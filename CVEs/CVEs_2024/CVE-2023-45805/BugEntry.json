{"buggy_code": ["from __future__ import annotations\n\nimport dataclasses\nimport fnmatch\nimport posixpath\nimport re\nimport sys\nimport warnings\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Generator, TypeVar, cast\n\nfrom pdm import termui\nfrom pdm.exceptions import CandidateInfoNotFound, CandidateNotFound, PackageWarning\nfrom pdm.models.candidates import Candidate, make_candidate\nfrom pdm.models.requirements import (\n    Requirement,\n    filter_requirements_with_extras,\n    parse_requirement,\n)\nfrom pdm.models.search import SearchResultParser\nfrom pdm.models.specifiers import PySpecSet\nfrom pdm.utils import (\n    cd,\n    normalize_name,\n    path_to_url,\n    url_to_path,\n    url_without_fragments,\n)\n\nif TYPE_CHECKING:\n    from typing import Any, Callable, Iterable, Mapping\n\n    from unearth import Link\n\n    from pdm._types import CandidateInfo, FileHash, RepositoryConfig, SearchResult\n    from pdm.environments import BaseEnvironment\n\n    CandidateKey = tuple[str, str | None, str | None, bool]\n\nALLOW_ALL_PYTHON = PySpecSet()\nT = TypeVar(\"T\", bound=\"BaseRepository\")\n\n\ndef cache_result(func: Callable[[T, Candidate], CandidateInfo]) -> Callable[[T, Candidate], CandidateInfo]:\n    @wraps(func)\n    def wrapper(self: T, candidate: Candidate) -> CandidateInfo:\n        result = func(self, candidate)\n        prepared = candidate.prepared\n        if prepared and prepared.should_cache():\n            self._candidate_info_cache.set(candidate, result)\n        return result\n\n    return wrapper\n\n\nclass BaseRepository:\n    \"\"\"A Repository acts as the source of packages and metadata.\"\"\"\n\n    def __init__(\n        self,\n        sources: list[RepositoryConfig],\n        environment: BaseEnvironment,\n        ignore_compatibility: bool = True,\n    ) -> None:\n        \"\"\"\n        :param sources: a list of sources to download packages from.\n        :param environment: the bound environment instance.\n        :param ignore_compatibility: if True, don't evaluate candidate against\n            the current environment.\n        \"\"\"\n        self.sources = sources\n        self.environment = environment\n        self.ignore_compatibility = ignore_compatibility\n        self._candidate_info_cache = environment.project.make_candidate_info_cache()\n        self._hash_cache = environment.project.make_hash_cache()\n        self.has_warnings = False\n\n    def get_filtered_sources(self, req: Requirement) -> list[RepositoryConfig]:\n        \"\"\"Get matching sources based on the index attribute.\"\"\"\n        return self.sources\n\n    def get_dependencies(self, candidate: Candidate) -> tuple[list[Requirement], PySpecSet, str]:\n        \"\"\"Get (dependencies, python_specifier, summary) of the candidate.\"\"\"\n        requires_python, summary = \"\", \"\"\n        requirements: list[str] = []\n        last_ext_info = None\n        for getter in self.dependency_generators():\n            try:\n                requirements, requires_python, summary = getter(candidate)\n            except CandidateInfoNotFound:\n                last_ext_info = sys.exc_info()\n                continue\n            break\n        else:\n            if last_ext_info is not None:\n                raise last_ext_info[1].with_traceback(last_ext_info[2])  # type: ignore[union-attr]\n        reqs: list[Requirement] = []\n        for line in requirements:\n            if line.startswith(\"-e \"):\n                reqs.append(parse_requirement(line[3:], True))\n            else:\n                reqs.append(parse_requirement(line))\n        if candidate.req.extras:\n            # XXX: If the requirement has extras, add the original candidate\n            # (without extras) as its dependency. This ensures the same package with\n            # different extras resolve to the same version.\n            self_req = dataclasses.replace(\n                candidate.req.as_pinned_version(candidate.version),\n                extras=None,\n                marker=None,\n            )\n            reqs.append(self_req)\n        # Store the metadata on the candidate for caching\n        candidate.requires_python = requires_python\n        candidate.summary = summary\n        if not self.ignore_compatibility:\n            pep508_env = self.environment.marker_environment\n            reqs = [req for req in reqs if not req.marker or req.marker.evaluate(pep508_env)]\n        return reqs, PySpecSet(requires_python), summary\n\n    def _find_candidates(self, requirement: Requirement) -> Iterable[Candidate]:\n        raise NotImplementedError\n\n    def is_this_package(self, requirement: Requirement) -> bool:\n        \"\"\"Whether the requirement is the same as this package\"\"\"\n        project = self.environment.project\n        return requirement.is_named and project.name is not None and requirement.key == normalize_name(project.name)\n\n    def make_this_candidate(self, requirement: Requirement) -> Candidate:\n        \"\"\"Make a candidate for this package.\n        In this case the finder will look for a candidate from the package sources\n        \"\"\"\n        from unearth import Link\n\n        project = self.environment.project\n        assert project.name\n        link = Link.from_path(project.root)\n        candidate = make_candidate(requirement, project.name, link=link)\n        candidate.prepare(self.environment).metadata\n        return candidate\n\n    def _should_ignore_package_warning(self, requirement: Requirement) -> bool:\n        ignore_settings = self.environment.project.pyproject.settings.get(\"ignore_package_warnings\", [])\n        package_name = requirement.key\n        assert package_name is not None\n        for pat in ignore_settings:\n            pat = re.sub(r\"[^A-Za-z0-9?*\\[\\]]+\", \"-\", pat).lower()\n            if fnmatch.fnmatch(package_name, pat):\n                return True\n        return False\n\n    def find_candidates(\n        self,\n        requirement: Requirement,\n        allow_prereleases: bool | None = None,\n        ignore_requires_python: bool = False,\n    ) -> Iterable[Candidate]:\n        \"\"\"Find candidates of the given NamedRequirement. Let it to be implemented in\n        subclasses.\n        \"\"\"\n        # `allow_prereleases` is None means leave it to specifier to decide whether to\n        # include prereleases\n        from unearth.utils import LazySequence\n\n        if self.is_this_package(requirement):\n            return [self.make_this_candidate(requirement)]\n        requires_python = requirement.requires_python & self.environment.python_requires\n        cans = LazySequence(self._find_candidates(requirement))\n        applicable_cans = LazySequence(\n            c\n            for c in cans\n            if requirement.specifier.contains(c.version, allow_prereleases)  # type: ignore[arg-type, union-attr]\n        )\n\n        def filter_candidates_with_requires_python(candidates: Iterable[Candidate]) -> Generator[Candidate, None, None]:\n            project_requires_python = self.environment.python_requires\n            if ignore_requires_python:\n                yield from candidates\n                return\n\n            def python_specifier(spec: str | PySpecSet) -> str:\n                if isinstance(spec, PySpecSet):\n                    spec = str(spec)\n                return \"all Python versions\" if not spec else f\"Python{spec}\"\n\n            for candidate in candidates:\n                if not requires_python.is_subset(candidate.requires_python):\n                    if self._should_ignore_package_warning(requirement):\n                        continue\n                    working_requires_python = project_requires_python & PySpecSet(candidate.requires_python)\n                    if working_requires_python.is_impossible:  # pragma: no cover\n                        continue\n                    warnings.warn(\n                        f\"Skipping {candidate.name}@{candidate.version} because it requires \"\n                        f\"{python_specifier(candidate.requires_python)} but the project claims to work with \"\n                        f\"{python_specifier(project_requires_python)}.\\nNarrow down the `requires-python` range to \"\n                        f'include this version. For example, \"{working_requires_python}\" should work.',\n                        PackageWarning,\n                        stacklevel=4,\n                    )\n                    self.has_warnings = True\n                else:\n                    yield candidate\n\n        applicable_cans_python_compatible = LazySequence(filter_candidates_with_requires_python(applicable_cans))\n        # Evaluate data-requires-python attr and discard incompatible candidates\n        # to reduce the number of candidates to resolve.\n        if applicable_cans_python_compatible:\n            applicable_cans = applicable_cans_python_compatible\n\n        if not applicable_cans:\n            termui.logger.debug(\"\\tCould not find any matching candidates.\")\n\n        if not applicable_cans and allow_prereleases is None:\n            # No non-pre-releases is found, force pre-releases now\n            applicable_cans = LazySequence(\n                c for c in cans if requirement.specifier.contains(c.version, True)  # type: ignore[arg-type, union-attr]\n            )\n            applicable_cans_python_compatible = LazySequence(filter_candidates_with_requires_python(applicable_cans))\n            if applicable_cans_python_compatible:\n                applicable_cans = applicable_cans_python_compatible\n\n            if not applicable_cans:\n                termui.logger.debug(\n                    \"\\tCould not find any matching candidates even when considering pre-releases.\",\n                )\n\n        def log_candidates(title: str, candidates: Iterable[Candidate], max_lines: int = 10) -> None:\n            termui.logger.debug(\"\\t\" + title)\n            logged_lines = set()\n            for can in candidates:\n                new_line = f\"\\t  {can!r}\"\n                if new_line not in logged_lines:\n                    logged_lines.add(new_line)\n                    if len(logged_lines) > max_lines:\n                        termui.logger.debug(\"\\t  ... [more]\")\n                        break\n                    else:\n                        termui.logger.debug(new_line)\n\n        if self.environment.project.core.ui.verbosity >= termui.Verbosity.DEBUG:\n            if applicable_cans:\n                log_candidates(\"Found matching candidates:\", applicable_cans)\n            elif cans:\n                log_candidates(\"Found but non-matching candidates:\", cans)\n\n        return applicable_cans\n\n    def _get_dependencies_from_cache(self, candidate: Candidate) -> CandidateInfo:\n        try:\n            result = self._candidate_info_cache.get(candidate)\n        except KeyError:\n            raise CandidateInfoNotFound(candidate) from None\n        return result\n\n    @cache_result\n    def _get_dependencies_from_metadata(self, candidate: Candidate) -> CandidateInfo:\n        prepared = candidate.prepare(self.environment)\n        deps = prepared.get_dependencies_from_metadata()\n        requires_python = candidate.requires_python\n        summary = prepared.metadata.metadata[\"Summary\"]\n        return deps, requires_python, summary\n\n    def _get_dependency_from_local_package(self, candidate: Candidate) -> CandidateInfo:\n        \"\"\"Adds the local package as a candidate only if the candidate\n        name is the same as the local package.\"\"\"\n        project = self.environment.project\n        if not project.name or candidate.name != project.name:\n            raise CandidateInfoNotFound(candidate) from None\n\n        reqs = project.pyproject.metadata.get(\"dependencies\", [])\n        extra_dependencies = project.pyproject.settings.get(\"dev-dependencies\", {}).copy()\n        extra_dependencies.update(project.pyproject.metadata.get(\"optional-dependencies\", {}))\n        if candidate.req.extras is not None:\n            reqs = sum(\n                (extra_dependencies.get(g, []) for g in candidate.req.extras),\n                [],\n            )\n\n        return (\n            reqs,\n            str(self.environment.python_requires),\n            project.pyproject.metadata.get(\"description\", \"UNKNOWN\"),\n        )\n\n    def _is_python_match(self, link: Link) -> bool:\n        from packaging.tags import Tag\n        from packaging.utils import parse_wheel_filename\n\n        def is_tag_match(tag: Tag, python_requires: PySpecSet) -> bool:\n            if tag.interpreter.startswith((\"cp\", \"py\")):\n                major, minor = tag.interpreter[2], tag.interpreter[3:]\n                if not minor:\n                    version = f\"{major}.0\"\n                else:\n                    version = f\"{major}.{minor}.0\"\n                if tag.abi == \"abi3\":\n                    spec = PySpecSet(f\">={version}\")  # cp37-abi3 is compatible with >=3.7\n                else:\n                    spec = PySpecSet(f\"~={version}\")  # cp37-cp37 is only compatible with 3.7.*\n                return not (spec & python_requires).is_impossible\n            else:\n                # we don't know about compatility for non-cpython implementations\n                # assume it is compatible\n                return True\n\n        if not link.is_wheel:\n            return True\n        python_requires = self.environment.python_requires\n        tags = parse_wheel_filename(link.filename)[-1]\n        result = any(is_tag_match(tag, python_requires) for tag in tags)\n        if not result:\n            termui.logger.debug(\n                \"Skipping %r because it is not compatible with %r\",\n                link,\n                python_requires,\n            )\n        return result\n\n    def get_hashes(self, candidate: Candidate) -> list[FileHash]:\n        \"\"\"Get hashes of all possible installable candidates\n        of a given package version.\n        \"\"\"\n        if (\n            candidate.req.is_vcs\n            or candidate.req.is_file_or_url\n            and candidate.req.is_local_dir  # type: ignore[attr-defined]\n        ):\n            return []\n        if candidate.hashes:\n            return candidate.hashes\n        req = candidate.req.as_pinned_version(candidate.version)\n        comes_from = candidate.link.comes_from if candidate.link else None\n        result: list[FileHash] = []\n        logged = False\n        respect_source_order = self.environment.project.pyproject.settings.get(\"resolution\", {}).get(\n            \"respect-source-order\", False\n        )\n        if req.is_named and respect_source_order and comes_from:\n            sources = [s for s in self.sources if comes_from.startswith(s.url)]\n        else:\n            sources = self.sources\n        with self.environment.get_finder(sources, self.ignore_compatibility) as finder:\n            if req.is_file_or_url:\n                this_link = cast(\"Link\", candidate.prepare(self.environment).link)\n                links: list[Link] = [this_link]\n            else:  # the req must be a named requirement\n                links = [package.link for package in finder.find_matches(req.as_line())]\n                if self.ignore_compatibility:\n                    links = [link for link in links if self._is_python_match(link)]\n            for link in links:\n                if not link or link.is_vcs or link.is_file and link.file_path.is_dir():\n                    # The links found can still be a local directory or vcs, skippping it.\n                    continue\n                if not logged:\n                    termui.logger.info(\"Fetching hashes for %s\", candidate)\n                    logged = True\n                result.append(\n                    {\n                        \"url\": link.url_without_fragment,\n                        \"file\": link.filename,\n                        \"hash\": self._hash_cache.get_hash(link, finder.session),\n                    }\n                )\n        return result\n\n    def dependency_generators(self) -> Iterable[Callable[[Candidate], CandidateInfo]]:\n        \"\"\"Return an iterable of getter functions to get dependencies, which will be\n        called one by one.\n        \"\"\"\n        raise NotImplementedError\n\n    def search(self, query: str) -> SearchResult:\n        \"\"\"Search package by name or summary.\n\n        :param query: query string\n        :returns: search result, a dictionary of name: package metadata\n        \"\"\"\n        raise NotImplementedError\n\n\nclass PyPIRepository(BaseRepository):\n    \"\"\"Get package and metadata from PyPI source.\"\"\"\n\n    DEFAULT_INDEX_URL = \"https://pypi.org\"\n\n    @cache_result\n    def _get_dependencies_from_json(self, candidate: Candidate) -> CandidateInfo:\n        if not candidate.name or not candidate.version:\n            # Only look for json api for named requirements.\n            raise CandidateInfoNotFound(candidate)\n        sources = self.get_filtered_sources(candidate.req)\n        url_prefixes = [\n            proc_url[:-7]  # Strip \"/simple\".\n            for proc_url in (raw_url.rstrip(\"/\") for raw_url in (source.url for source in sources) if raw_url)\n            if proc_url.endswith(\"/simple\")\n        ]\n        with self.environment.get_finder(sources) as finder:\n            session = finder.session\n            for prefix in url_prefixes:\n                json_url = f\"{prefix}/pypi/{candidate.name}/{candidate.version}/json\"\n                resp = session.get(json_url)\n                if not resp.ok:\n                    continue\n\n                info = resp.json()[\"info\"]\n\n                requires_python = info[\"requires_python\"] or \"\"\n                summary = info[\"summary\"] or \"\"\n                try:\n                    requirement_lines = info[\"requires_dist\"] or []\n                except KeyError:\n                    requirement_lines = info[\"requires\"] or []\n                requirements = filter_requirements_with_extras(\n                    cast(str, candidate.req.project_name),\n                    requirement_lines,\n                    candidate.req.extras or (),\n                )\n                return requirements, requires_python, summary\n        raise CandidateInfoNotFound(candidate)\n\n    def dependency_generators(self) -> Iterable[Callable[[Candidate], CandidateInfo]]:\n        yield self._get_dependencies_from_cache\n        yield self._get_dependency_from_local_package\n        if self.environment.project.config[\"pypi.json_api\"]:\n            yield self._get_dependencies_from_json\n        yield self._get_dependencies_from_metadata\n\n    def _find_candidates(self, requirement: Requirement) -> Iterable[Candidate]:\n        from unearth.utils import LazySequence\n\n        sources = self.get_filtered_sources(requirement)\n        with self.environment.get_finder(sources, self.ignore_compatibility) as finder:\n            cans = LazySequence(\n                Candidate.from_installation_candidate(c, requirement)\n                for c in finder.find_all_packages(requirement.project_name, allow_yanked=requirement.is_pinned)\n            )\n        if not cans:\n            raise CandidateNotFound(\n                f\"Unable to find candidates for {requirement.project_name}. There may \"\n                \"exist some issues with the package name or network condition.\"\n            )\n        return cans\n\n    def search(self, query: str) -> SearchResult:\n        pypi_simple = self.sources[0].url.rstrip(\"/\")  # type: ignore[union-attr]\n\n        if pypi_simple.endswith(\"/simple\"):\n            search_url = pypi_simple[:-6] + \"search\"\n        else:\n            search_url = pypi_simple + \"/search\"\n\n        with self.environment.get_finder() as finder:\n            session = finder.session\n            resp = session.get(search_url, params={\"q\": query})\n            if resp.status_code == 404:\n                self.environment.project.core.ui.echo(\n                    f\"{pypi_simple!r} doesn't support '/search' endpoint, fallback \"\n                    f\"to {self.DEFAULT_INDEX_URL!r} now.\\n\"\n                    \"This may take longer depending on your network condition.\",\n                    err=True,\n                    style=\"warning\",\n                )\n                resp = session.get(f\"{self.DEFAULT_INDEX_URL}/search\", params={\"q\": query})\n            parser = SearchResultParser()\n            resp.raise_for_status()\n            parser.feed(resp.text)\n            return parser.results\n\n\nclass LockedRepository(BaseRepository):\n    def __init__(\n        self,\n        lockfile: Mapping[str, Any],\n        sources: list[RepositoryConfig],\n        environment: BaseEnvironment,\n    ) -> None:\n        super().__init__(sources, environment, ignore_compatibility=False)\n        self.packages: dict[CandidateKey, Candidate] = {}\n        self.candidate_info: dict[CandidateKey, CandidateInfo] = {}\n        self._read_lockfile(lockfile)\n\n    @property\n    def all_candidates(self) -> dict[str, Candidate]:\n        return {can.req.identify(): can for can in self.packages.values()}\n\n    def _read_lockfile(self, lockfile: Mapping[str, Any]) -> None:\n        root = self.environment.project.root\n        with cd(root):\n            for package in lockfile.get(\"package\", []):\n                version = package.get(\"version\")\n                if version:\n                    package[\"version\"] = f\"=={version}\"\n                package_name = package.pop(\"name\")\n                req_dict = {\n                    k: v for k, v in package.items() if k not in (\"dependencies\", \"requires_python\", \"summary\", \"files\")\n                }\n                req = Requirement.from_req_dict(package_name, req_dict)\n                if req.is_file_or_url and req.path and not req.url:  # type: ignore[attr-defined]\n                    req.url = path_to_url(posixpath.join(root, req.path))  # type: ignore[attr-defined]\n                can = make_candidate(req, name=package_name, version=version)\n                can.hashes = package.get(\"files\", [])\n                can_id = self._identify_candidate(can)\n                self.packages[can_id] = can\n                candidate_info: CandidateInfo = (\n                    package.get(\"dependencies\", []),\n                    package.get(\"requires_python\", \"\"),\n                    package.get(\"summary\", \"\"),\n                )\n                self.candidate_info[can_id] = candidate_info\n\n    def _identify_candidate(self, candidate: Candidate) -> CandidateKey:\n        url: str | None = None\n        if candidate.link is not None:\n            url = candidate.link.url_without_fragment\n            url = self.environment.project.backend.expand_line(cast(str, url))\n            if url.startswith(\"file://\"):\n                path = posixpath.normpath(url_to_path(url))\n                url = path_to_url(path)\n        return (\n            candidate.identify(),\n            candidate.version if not url else None,\n            url,\n            candidate.req.editable,\n        )\n\n    def _get_dependencies_from_lockfile(self, candidate: Candidate) -> CandidateInfo:\n        return self.candidate_info[self._identify_candidate(candidate)]\n\n    def dependency_generators(self) -> Iterable[Callable[[Candidate], CandidateInfo]]:\n        return (\n            self._get_dependency_from_local_package,\n            self._get_dependencies_from_lockfile,\n        )\n\n    def _matching_keys(self, requirement: Requirement) -> Iterable[CandidateKey]:\n        from pdm.models.requirements import FileRequirement\n\n        for key in self.candidate_info:\n            can_req = self.packages[key].req\n            if requirement.name:\n                if key[0] != requirement.identify():\n                    continue\n            else:\n                assert isinstance(requirement, FileRequirement)\n                if not isinstance(can_req, FileRequirement):\n                    continue\n                if requirement.path and can_req.path:\n                    if requirement.path != can_req.path:\n                        continue\n                elif key[2] is not None and key[2] != url_without_fragments(requirement.url):\n                    continue\n\n            yield key\n\n    def find_candidates(\n        self,\n        requirement: Requirement,\n        allow_prereleases: bool | None = None,\n        ignore_requires_python: bool = False,\n    ) -> Iterable[Candidate]:\n        if self.is_this_package(requirement):\n            candidate = self.make_this_candidate(requirement)\n            if candidate is not None:\n                yield candidate\n                return\n        for key in self._matching_keys(requirement):\n            info = self.candidate_info[key]\n            if not PySpecSet(info[1]).contains(str(self.environment.interpreter.version), True):\n                continue\n            can = self.packages[key]\n            can.requires_python = info[1]\n            if not requirement.name:\n                # make sure can.identify() won't return a randomly-generated name\n                requirement.name = can.name\n            can.req = requirement\n            yield can\n\n    def get_hashes(self, candidate: Candidate) -> list[FileHash]:\n        return candidate.hashes\n"], "fixing_code": ["from __future__ import annotations\n\nimport dataclasses\nimport fnmatch\nimport posixpath\nimport re\nimport sys\nimport warnings\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Generator, TypeVar, cast\n\nfrom pdm import termui\nfrom pdm.exceptions import CandidateInfoNotFound, CandidateNotFound, PackageWarning, PdmException\nfrom pdm.models.candidates import Candidate, make_candidate\nfrom pdm.models.requirements import (\n    Requirement,\n    filter_requirements_with_extras,\n    parse_requirement,\n)\nfrom pdm.models.search import SearchResultParser\nfrom pdm.models.specifiers import PySpecSet\nfrom pdm.utils import (\n    cd,\n    normalize_name,\n    path_to_url,\n    url_to_path,\n    url_without_fragments,\n)\n\nif TYPE_CHECKING:\n    from typing import Any, Callable, Iterable, Mapping\n\n    from unearth import Link\n\n    from pdm._types import CandidateInfo, FileHash, RepositoryConfig, SearchResult\n    from pdm.environments import BaseEnvironment\n\n    CandidateKey = tuple[str, str | None, str | None, bool]\n\nALLOW_ALL_PYTHON = PySpecSet()\nT = TypeVar(\"T\", bound=\"BaseRepository\")\n\n\ndef cache_result(func: Callable[[T, Candidate], CandidateInfo]) -> Callable[[T, Candidate], CandidateInfo]:\n    @wraps(func)\n    def wrapper(self: T, candidate: Candidate) -> CandidateInfo:\n        result = func(self, candidate)\n        prepared = candidate.prepared\n        if prepared and prepared.should_cache():\n            self._candidate_info_cache.set(candidate, result)\n        return result\n\n    return wrapper\n\n\nclass BaseRepository:\n    \"\"\"A Repository acts as the source of packages and metadata.\"\"\"\n\n    def __init__(\n        self,\n        sources: list[RepositoryConfig],\n        environment: BaseEnvironment,\n        ignore_compatibility: bool = True,\n    ) -> None:\n        \"\"\"\n        :param sources: a list of sources to download packages from.\n        :param environment: the bound environment instance.\n        :param ignore_compatibility: if True, don't evaluate candidate against\n            the current environment.\n        \"\"\"\n        self.sources = sources\n        self.environment = environment\n        self.ignore_compatibility = ignore_compatibility\n        self._candidate_info_cache = environment.project.make_candidate_info_cache()\n        self._hash_cache = environment.project.make_hash_cache()\n        self.has_warnings = False\n\n    def get_filtered_sources(self, req: Requirement) -> list[RepositoryConfig]:\n        \"\"\"Get matching sources based on the index attribute.\"\"\"\n        return self.sources\n\n    def get_dependencies(self, candidate: Candidate) -> tuple[list[Requirement], PySpecSet, str]:\n        \"\"\"Get (dependencies, python_specifier, summary) of the candidate.\"\"\"\n        requires_python, summary = \"\", \"\"\n        requirements: list[str] = []\n        last_ext_info = None\n        for getter in self.dependency_generators():\n            try:\n                requirements, requires_python, summary = getter(candidate)\n            except CandidateInfoNotFound:\n                last_ext_info = sys.exc_info()\n                continue\n            break\n        else:\n            if last_ext_info is not None:\n                raise last_ext_info[1].with_traceback(last_ext_info[2])  # type: ignore[union-attr]\n        reqs: list[Requirement] = []\n        for line in requirements:\n            if line.startswith(\"-e \"):\n                reqs.append(parse_requirement(line[3:], True))\n            else:\n                reqs.append(parse_requirement(line))\n        if candidate.req.extras:\n            # XXX: If the requirement has extras, add the original candidate\n            # (without extras) as its dependency. This ensures the same package with\n            # different extras resolve to the same version.\n            self_req = dataclasses.replace(\n                candidate.req.as_pinned_version(candidate.version),\n                extras=None,\n                marker=None,\n            )\n            reqs.append(self_req)\n        # Store the metadata on the candidate for caching\n        candidate.requires_python = requires_python\n        candidate.summary = summary\n        if not self.ignore_compatibility:\n            pep508_env = self.environment.marker_environment\n            reqs = [req for req in reqs if not req.marker or req.marker.evaluate(pep508_env)]\n        return reqs, PySpecSet(requires_python), summary\n\n    def _find_candidates(self, requirement: Requirement) -> Iterable[Candidate]:\n        raise NotImplementedError\n\n    def is_this_package(self, requirement: Requirement) -> bool:\n        \"\"\"Whether the requirement is the same as this package\"\"\"\n        project = self.environment.project\n        return requirement.is_named and project.name is not None and requirement.key == normalize_name(project.name)\n\n    def make_this_candidate(self, requirement: Requirement) -> Candidate:\n        \"\"\"Make a candidate for this package.\n        In this case the finder will look for a candidate from the package sources\n        \"\"\"\n        from unearth import Link\n\n        project = self.environment.project\n        assert project.name\n        link = Link.from_path(project.root)\n        candidate = make_candidate(requirement, project.name, link=link)\n        candidate.prepare(self.environment).metadata\n        return candidate\n\n    def _should_ignore_package_warning(self, requirement: Requirement) -> bool:\n        ignore_settings = self.environment.project.pyproject.settings.get(\"ignore_package_warnings\", [])\n        package_name = requirement.key\n        assert package_name is not None\n        for pat in ignore_settings:\n            pat = re.sub(r\"[^A-Za-z0-9?*\\[\\]]+\", \"-\", pat).lower()\n            if fnmatch.fnmatch(package_name, pat):\n                return True\n        return False\n\n    def find_candidates(\n        self,\n        requirement: Requirement,\n        allow_prereleases: bool | None = None,\n        ignore_requires_python: bool = False,\n    ) -> Iterable[Candidate]:\n        \"\"\"Find candidates of the given NamedRequirement. Let it to be implemented in\n        subclasses.\n        \"\"\"\n        # `allow_prereleases` is None means leave it to specifier to decide whether to\n        # include prereleases\n        from unearth.utils import LazySequence\n\n        if self.is_this_package(requirement):\n            return [self.make_this_candidate(requirement)]\n        requires_python = requirement.requires_python & self.environment.python_requires\n        cans = LazySequence(self._find_candidates(requirement))\n        applicable_cans = LazySequence(\n            c\n            for c in cans\n            if requirement.specifier.contains(c.version, allow_prereleases)  # type: ignore[arg-type, union-attr]\n        )\n\n        def filter_candidates_with_requires_python(candidates: Iterable[Candidate]) -> Generator[Candidate, None, None]:\n            project_requires_python = self.environment.python_requires\n            if ignore_requires_python:\n                yield from candidates\n                return\n\n            def python_specifier(spec: str | PySpecSet) -> str:\n                if isinstance(spec, PySpecSet):\n                    spec = str(spec)\n                return \"all Python versions\" if not spec else f\"Python{spec}\"\n\n            for candidate in candidates:\n                if not requires_python.is_subset(candidate.requires_python):\n                    if self._should_ignore_package_warning(requirement):\n                        continue\n                    working_requires_python = project_requires_python & PySpecSet(candidate.requires_python)\n                    if working_requires_python.is_impossible:  # pragma: no cover\n                        continue\n                    warnings.warn(\n                        f\"Skipping {candidate.name}@{candidate.version} because it requires \"\n                        f\"{python_specifier(candidate.requires_python)} but the project claims to work with \"\n                        f\"{python_specifier(project_requires_python)}.\\nNarrow down the `requires-python` range to \"\n                        f'include this version. For example, \"{working_requires_python}\" should work.',\n                        PackageWarning,\n                        stacklevel=4,\n                    )\n                    self.has_warnings = True\n                else:\n                    yield candidate\n\n        applicable_cans_python_compatible = LazySequence(filter_candidates_with_requires_python(applicable_cans))\n        # Evaluate data-requires-python attr and discard incompatible candidates\n        # to reduce the number of candidates to resolve.\n        if applicable_cans_python_compatible:\n            applicable_cans = applicable_cans_python_compatible\n\n        if not applicable_cans:\n            termui.logger.debug(\"\\tCould not find any matching candidates.\")\n\n        if not applicable_cans and allow_prereleases is None:\n            # No non-pre-releases is found, force pre-releases now\n            applicable_cans = LazySequence(\n                c for c in cans if requirement.specifier.contains(c.version, True)  # type: ignore[arg-type, union-attr]\n            )\n            applicable_cans_python_compatible = LazySequence(filter_candidates_with_requires_python(applicable_cans))\n            if applicable_cans_python_compatible:\n                applicable_cans = applicable_cans_python_compatible\n\n            if not applicable_cans:\n                termui.logger.debug(\n                    \"\\tCould not find any matching candidates even when considering pre-releases.\",\n                )\n\n        def log_candidates(title: str, candidates: Iterable[Candidate], max_lines: int = 10) -> None:\n            termui.logger.debug(\"\\t\" + title)\n            logged_lines = set()\n            for can in candidates:\n                new_line = f\"\\t  {can!r}\"\n                if new_line not in logged_lines:\n                    logged_lines.add(new_line)\n                    if len(logged_lines) > max_lines:\n                        termui.logger.debug(\"\\t  ... [more]\")\n                        break\n                    else:\n                        termui.logger.debug(new_line)\n\n        if self.environment.project.core.ui.verbosity >= termui.Verbosity.DEBUG:\n            if applicable_cans:\n                log_candidates(\"Found matching candidates:\", applicable_cans)\n            elif cans:\n                log_candidates(\"Found but non-matching candidates:\", cans)\n\n        return applicable_cans\n\n    def _get_dependencies_from_cache(self, candidate: Candidate) -> CandidateInfo:\n        try:\n            result = self._candidate_info_cache.get(candidate)\n        except KeyError:\n            raise CandidateInfoNotFound(candidate) from None\n        return result\n\n    @cache_result\n    def _get_dependencies_from_metadata(self, candidate: Candidate) -> CandidateInfo:\n        prepared = candidate.prepare(self.environment)\n        deps = prepared.get_dependencies_from_metadata()\n        requires_python = candidate.requires_python\n        summary = prepared.metadata.metadata[\"Summary\"]\n        return deps, requires_python, summary\n\n    def _get_dependency_from_local_package(self, candidate: Candidate) -> CandidateInfo:\n        \"\"\"Adds the local package as a candidate only if the candidate\n        name is the same as the local package.\"\"\"\n        project = self.environment.project\n        if not project.name or candidate.name != project.name:\n            raise CandidateInfoNotFound(candidate) from None\n\n        reqs = project.pyproject.metadata.get(\"dependencies\", [])\n        extra_dependencies = project.pyproject.settings.get(\"dev-dependencies\", {}).copy()\n        extra_dependencies.update(project.pyproject.metadata.get(\"optional-dependencies\", {}))\n        if candidate.req.extras is not None:\n            reqs = sum(\n                (extra_dependencies.get(g, []) for g in candidate.req.extras),\n                [],\n            )\n\n        return (\n            reqs,\n            str(self.environment.python_requires),\n            project.pyproject.metadata.get(\"description\", \"UNKNOWN\"),\n        )\n\n    def _is_python_match(self, link: Link) -> bool:\n        from packaging.tags import Tag\n        from packaging.utils import parse_wheel_filename\n\n        def is_tag_match(tag: Tag, python_requires: PySpecSet) -> bool:\n            if tag.interpreter.startswith((\"cp\", \"py\")):\n                major, minor = tag.interpreter[2], tag.interpreter[3:]\n                if not minor:\n                    version = f\"{major}.0\"\n                else:\n                    version = f\"{major}.{minor}.0\"\n                if tag.abi == \"abi3\":\n                    spec = PySpecSet(f\">={version}\")  # cp37-abi3 is compatible with >=3.7\n                else:\n                    spec = PySpecSet(f\"~={version}\")  # cp37-cp37 is only compatible with 3.7.*\n                return not (spec & python_requires).is_impossible\n            else:\n                # we don't know about compatility for non-cpython implementations\n                # assume it is compatible\n                return True\n\n        if not link.is_wheel:\n            return True\n        python_requires = self.environment.python_requires\n        tags = parse_wheel_filename(link.filename)[-1]\n        result = any(is_tag_match(tag, python_requires) for tag in tags)\n        if not result:\n            termui.logger.debug(\n                \"Skipping %r because it is not compatible with %r\",\n                link,\n                python_requires,\n            )\n        return result\n\n    def get_hashes(self, candidate: Candidate) -> list[FileHash]:\n        \"\"\"Get hashes of all possible installable candidates\n        of a given package version.\n        \"\"\"\n        if (\n            candidate.req.is_vcs\n            or candidate.req.is_file_or_url\n            and candidate.req.is_local_dir  # type: ignore[attr-defined]\n        ):\n            return []\n        if candidate.hashes:\n            return candidate.hashes\n        req = candidate.req.as_pinned_version(candidate.version)\n        comes_from = candidate.link.comes_from if candidate.link else None\n        result: list[FileHash] = []\n        logged = False\n        respect_source_order = self.environment.project.pyproject.settings.get(\"resolution\", {}).get(\n            \"respect-source-order\", False\n        )\n        if req.is_named and respect_source_order and comes_from:\n            sources = [s for s in self.sources if comes_from.startswith(s.url)]\n        else:\n            sources = self.sources\n        with self.environment.get_finder(sources, self.ignore_compatibility) as finder:\n            if req.is_file_or_url:\n                this_link = cast(\"Link\", candidate.prepare(self.environment).link)\n                links: list[Link] = [this_link]\n            else:  # the req must be a named requirement\n                links = [package.link for package in finder.find_matches(req.as_line())]\n                if self.ignore_compatibility:\n                    links = [link for link in links if self._is_python_match(link)]\n            for link in links:\n                if not link or link.is_vcs or link.is_file and link.file_path.is_dir():\n                    # The links found can still be a local directory or vcs, skippping it.\n                    continue\n                if not logged:\n                    termui.logger.info(\"Fetching hashes for %s\", candidate)\n                    logged = True\n                result.append(\n                    {\n                        \"url\": link.url_without_fragment,\n                        \"file\": link.filename,\n                        \"hash\": self._hash_cache.get_hash(link, finder.session),\n                    }\n                )\n        return result\n\n    def dependency_generators(self) -> Iterable[Callable[[Candidate], CandidateInfo]]:\n        \"\"\"Return an iterable of getter functions to get dependencies, which will be\n        called one by one.\n        \"\"\"\n        raise NotImplementedError\n\n    def search(self, query: str) -> SearchResult:\n        \"\"\"Search package by name or summary.\n\n        :param query: query string\n        :returns: search result, a dictionary of name: package metadata\n        \"\"\"\n        raise NotImplementedError\n\n\nclass PyPIRepository(BaseRepository):\n    \"\"\"Get package and metadata from PyPI source.\"\"\"\n\n    DEFAULT_INDEX_URL = \"https://pypi.org\"\n\n    @cache_result\n    def _get_dependencies_from_json(self, candidate: Candidate) -> CandidateInfo:\n        if not candidate.name or not candidate.version:\n            # Only look for json api for named requirements.\n            raise CandidateInfoNotFound(candidate)\n        sources = self.get_filtered_sources(candidate.req)\n        url_prefixes = [\n            proc_url[:-7]  # Strip \"/simple\".\n            for proc_url in (raw_url.rstrip(\"/\") for raw_url in (source.url for source in sources) if raw_url)\n            if proc_url.endswith(\"/simple\")\n        ]\n        with self.environment.get_finder(sources) as finder:\n            session = finder.session\n            for prefix in url_prefixes:\n                json_url = f\"{prefix}/pypi/{candidate.name}/{candidate.version}/json\"\n                resp = session.get(json_url)\n                if not resp.ok:\n                    continue\n\n                info = resp.json()[\"info\"]\n\n                requires_python = info[\"requires_python\"] or \"\"\n                summary = info[\"summary\"] or \"\"\n                try:\n                    requirement_lines = info[\"requires_dist\"] or []\n                except KeyError:\n                    requirement_lines = info[\"requires\"] or []\n                requirements = filter_requirements_with_extras(\n                    cast(str, candidate.req.project_name),\n                    requirement_lines,\n                    candidate.req.extras or (),\n                )\n                return requirements, requires_python, summary\n        raise CandidateInfoNotFound(candidate)\n\n    def dependency_generators(self) -> Iterable[Callable[[Candidate], CandidateInfo]]:\n        yield self._get_dependencies_from_cache\n        yield self._get_dependency_from_local_package\n        if self.environment.project.config[\"pypi.json_api\"]:\n            yield self._get_dependencies_from_json\n        yield self._get_dependencies_from_metadata\n\n    def _find_candidates(self, requirement: Requirement) -> Iterable[Candidate]:\n        from unearth.utils import LazySequence\n\n        sources = self.get_filtered_sources(requirement)\n        with self.environment.get_finder(sources, self.ignore_compatibility) as finder:\n            cans = LazySequence(\n                Candidate.from_installation_candidate(c, requirement)\n                for c in finder.find_all_packages(requirement.project_name, allow_yanked=requirement.is_pinned)\n            )\n        if not cans:\n            raise CandidateNotFound(\n                f\"Unable to find candidates for {requirement.project_name}. There may \"\n                \"exist some issues with the package name or network condition.\"\n            )\n        return cans\n\n    def search(self, query: str) -> SearchResult:\n        pypi_simple = self.sources[0].url.rstrip(\"/\")  # type: ignore[union-attr]\n\n        if pypi_simple.endswith(\"/simple\"):\n            search_url = pypi_simple[:-6] + \"search\"\n        else:\n            search_url = pypi_simple + \"/search\"\n\n        with self.environment.get_finder() as finder:\n            session = finder.session\n            resp = session.get(search_url, params={\"q\": query})\n            if resp.status_code == 404:\n                self.environment.project.core.ui.echo(\n                    f\"{pypi_simple!r} doesn't support '/search' endpoint, fallback \"\n                    f\"to {self.DEFAULT_INDEX_URL!r} now.\\n\"\n                    \"This may take longer depending on your network condition.\",\n                    err=True,\n                    style=\"warning\",\n                )\n                resp = session.get(f\"{self.DEFAULT_INDEX_URL}/search\", params={\"q\": query})\n            parser = SearchResultParser()\n            resp.raise_for_status()\n            parser.feed(resp.text)\n            return parser.results\n\n\nclass LockedRepository(BaseRepository):\n    def __init__(\n        self,\n        lockfile: Mapping[str, Any],\n        sources: list[RepositoryConfig],\n        environment: BaseEnvironment,\n    ) -> None:\n        super().__init__(sources, environment, ignore_compatibility=False)\n        self.packages: dict[CandidateKey, Candidate] = {}\n        self.candidate_info: dict[CandidateKey, CandidateInfo] = {}\n        self._read_lockfile(lockfile)\n\n    @property\n    def all_candidates(self) -> dict[str, Candidate]:\n        return {can.req.identify(): can for can in self.packages.values()}\n\n    def _read_lockfile(self, lockfile: Mapping[str, Any]) -> None:\n        root = self.environment.project.root\n        static_urls = self.environment.project.lockfile.static_urls\n        with cd(root):\n            for package in lockfile.get(\"package\", []):\n                version = package.get(\"version\")\n                if version:\n                    package[\"version\"] = f\"=={version}\"\n                package_name = package.pop(\"name\")\n                req_dict = {\n                    k: v for k, v in package.items() if k not in (\"dependencies\", \"requires_python\", \"summary\", \"files\")\n                }\n                req = Requirement.from_req_dict(package_name, req_dict)\n                if req.is_file_or_url and req.path and not req.url:  # type: ignore[attr-defined]\n                    req.url = path_to_url(posixpath.join(root, req.path))  # type: ignore[attr-defined]\n                can = make_candidate(req, name=package_name, version=version)\n                can.hashes = package.get(\"files\", [])\n                if not static_urls and any(\"url\" in f for f in can.hashes):\n                    raise PdmException(\n                        \"Static URLs are not allowed in lockfile unless enabled by `pdm lock --static-urls`.\"\n                    )\n                can_id = self._identify_candidate(can)\n                self.packages[can_id] = can\n                candidate_info: CandidateInfo = (\n                    package.get(\"dependencies\", []),\n                    package.get(\"requires_python\", \"\"),\n                    package.get(\"summary\", \"\"),\n                )\n                self.candidate_info[can_id] = candidate_info\n\n    def _identify_candidate(self, candidate: Candidate) -> CandidateKey:\n        url: str | None = None\n        if candidate.link is not None:\n            url = candidate.link.url_without_fragment\n            url = self.environment.project.backend.expand_line(cast(str, url))\n            if url.startswith(\"file://\"):\n                path = posixpath.normpath(url_to_path(url))\n                url = path_to_url(path)\n        return (\n            candidate.identify(),\n            candidate.version if not url else None,\n            url,\n            candidate.req.editable,\n        )\n\n    def _get_dependencies_from_lockfile(self, candidate: Candidate) -> CandidateInfo:\n        return self.candidate_info[self._identify_candidate(candidate)]\n\n    def dependency_generators(self) -> Iterable[Callable[[Candidate], CandidateInfo]]:\n        return (\n            self._get_dependency_from_local_package,\n            self._get_dependencies_from_lockfile,\n        )\n\n    def _matching_keys(self, requirement: Requirement) -> Iterable[CandidateKey]:\n        from pdm.models.requirements import FileRequirement\n\n        for key in self.candidate_info:\n            can_req = self.packages[key].req\n            if requirement.name:\n                if key[0] != requirement.identify():\n                    continue\n            else:\n                assert isinstance(requirement, FileRequirement)\n                if not isinstance(can_req, FileRequirement):\n                    continue\n                if requirement.path and can_req.path:\n                    if requirement.path != can_req.path:\n                        continue\n                elif key[2] is not None and key[2] != url_without_fragments(requirement.url):\n                    continue\n\n            yield key\n\n    def find_candidates(\n        self,\n        requirement: Requirement,\n        allow_prereleases: bool | None = None,\n        ignore_requires_python: bool = False,\n    ) -> Iterable[Candidate]:\n        if self.is_this_package(requirement):\n            candidate = self.make_this_candidate(requirement)\n            if candidate is not None:\n                yield candidate\n                return\n        for key in self._matching_keys(requirement):\n            info = self.candidate_info[key]\n            if not PySpecSet(info[1]).contains(str(self.environment.interpreter.version), True):\n                continue\n            can = self.packages[key]\n            can.requires_python = info[1]\n            if not requirement.name:\n                # make sure can.identify() won't return a randomly-generated name\n                requirement.name = can.name\n            can.req = requirement\n            yield can\n\n    def get_hashes(self, candidate: Candidate) -> list[FileHash]:\n        return candidate.hashes\n"], "filenames": ["src/pdm/models/repositories.py"], "buggy_code_start_loc": [13], "buggy_code_end_loc": [502], "fixing_code_start_loc": [13], "fixing_code_end_loc": [508], "type": "NVD-CWE-noinfo", "message": "pdm is a Python package and dependency manager supporting the latest PEP standards. It's possible to craft a malicious `pdm.lock` file that could allow e.g. an insider or a malicious open source project to appear to depend on a trusted PyPI project, but actually install another project. A project `foo` can be targeted by creating the project `foo-2` and uploading the file `foo-2-2.tar.gz` to pypi.org. PyPI will see this as project `foo-2` version `2`, while PDM will see this as project `foo` version `2-2`. The version must only be `parseable as a version` and the filename must be a prefix of the project name, but it's not verified to match the version being installed. Version `2-2` is also not a valid normalized version per PEP 440. Matching the project name exactly (not just prefix) would fix the issue. When installing dependencies with PDM, what's actually installed could differ from what's listed in `pyproject.toml` (including arbitrary code execution on install). It could also be used for downgrade attacks by only changing the version. This issue has been addressed in commit `6853e2642df` which is included in release version `2.9.4`. Users are advised to upgrade. There are no known workarounds for this vulnerability.", "other": {"cve": {"id": "CVE-2023-45805", "sourceIdentifier": "security-advisories@github.com", "published": "2023-10-20T19:15:08.820", "lastModified": "2023-10-28T03:24:28.190", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "pdm is a Python package and dependency manager supporting the latest PEP standards. It's possible to craft a malicious `pdm.lock` file that could allow e.g. an insider or a malicious open source project to appear to depend on a trusted PyPI project, but actually install another project. A project `foo` can be targeted by creating the project `foo-2` and uploading the file `foo-2-2.tar.gz` to pypi.org. PyPI will see this as project `foo-2` version `2`, while PDM will see this as project `foo` version `2-2`. The version must only be `parseable as a version` and the filename must be a prefix of the project name, but it's not verified to match the version being installed. Version `2-2` is also not a valid normalized version per PEP 440. Matching the project name exactly (not just prefix) would fix the issue. When installing dependencies with PDM, what's actually installed could differ from what's listed in `pyproject.toml` (including arbitrary code execution on install). It could also be used for downgrade attacks by only changing the version. This issue has been addressed in commit `6853e2642df` which is included in release version `2.9.4`. Users are advised to upgrade. There are no known workarounds for this vulnerability."}, {"lang": "es", "value": "pdm es un administrador de dependencias y paquetes de Python que admite los \u00faltimos est\u00e1ndares PEP. Es posible crear un archivo `pdm.lock` malicioso que podr\u00eda permitir, por ejemplo, que un proyecto interno o de c\u00f3digo abierto malicioso parezca depender de un proyecto PyPI confiable, pero en realidad instale otro proyecto. Se puede apuntar a un proyecto `foo` creando el proyecto `foo-2` y cargando el archivo `foo-2-2.tar.gz` en pypi.org. PyPI ver\u00e1 esto como proyecto `foo-2` versi\u00f3n `2`, mientras que PDM ver\u00e1 esto como proyecto `foo` versi\u00f3n `2-2`. La versi\u00f3n solo debe ser \"analizable como versi\u00f3n\" y el nombre del archivo debe ser un prefijo del nombre del proyecto, pero no se verifica que coincida con la versi\u00f3n que se est\u00e1 instalando. La versi\u00f3n `2-2` tampoco es una versi\u00f3n normalizada v\u00e1lida seg\u00fan PEP 440. Hacer coincidir exactamente el nombre del proyecto (no solo el prefijo) solucionar\u00eda el problema. Al instalar dependencias con PDM, lo que realmente se instala puede diferir de lo que aparece en `pyproject.toml` (incluida la ejecuci\u00f3n de c\u00f3digo arbitrario durante la instalaci\u00f3n). Tambi\u00e9n podr\u00eda usarse para ataques de degradaci\u00f3n cambiando solo la versi\u00f3n. Este problema se solucion\u00f3 en el commit `6853e2642df` que se incluye en la versi\u00f3n `2.9.4`. Se recomienda a los usuarios que actualicen. No se conocen workarounds para esta vulnerabilidad."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:frostming:pdm:*:*:*:*:*:python:*:*", "versionStartIncluding": "2.0.0", "versionEndExcluding": "2.10.0", "matchCriteriaId": "678C6891-163D-40EC-AA3F-F6C62650F55B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:frostming:unearth:*:*:*:*:*:python:*:*", "versionEndExcluding": "0.11.2", "matchCriteriaId": "2D6377F2-F3DF-4E02-810F-11EF3C227B7E"}]}]}], "references": [{"url": "https://github.com/frostming/unearth/blob/eca170d9370ac5032f2e497ee9b1b63823d3fe0f/src/unearth/evaluator.py#L215-L229", "source": "security-advisories@github.com", "tags": ["Product"]}, {"url": "https://github.com/pdm-project/pdm/blob/45d1dfa47d4900c14a31b9bb761e4c46eb5c9442/src/pdm/models/candidates.py#L98-L99", "source": "security-advisories@github.com", "tags": ["Product"]}, {"url": "https://github.com/pdm-project/pdm/commit/6853e2642dfa281d4a9958fbc6c95b7e32d84831", "source": "security-advisories@github.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/pdm-project/pdm/security/advisories/GHSA-j44v-mmf2-xvm9", "source": "security-advisories@github.com", "tags": ["Exploit", "Vendor Advisory"]}, {"url": "https://peps.python.org/pep-0440/#post-release-spelling", "source": "security-advisories@github.com", "tags": ["Not Applicable"]}]}, "github_commit_url": "https://github.com/pdm-project/pdm/commit/6853e2642dfa281d4a9958fbc6c95b7e32d84831"}}