{"buggy_code": ["import abc\nimport asyncio\nimport collections\nimport re\nimport string\nimport zlib\nfrom contextlib import suppress\nfrom enum import IntEnum\nfrom typing import (\n    Any,\n    Generic,\n    List,\n    NamedTuple,\n    Optional,\n    Pattern,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom multidict import CIMultiDict, CIMultiDictProxy, istr\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .base_protocol import BaseProtocol\nfrom .helpers import NO_EXTENSIONS, BaseTimerContext\nfrom .http_exceptions import (\n    BadHttpMessage,\n    BadStatusLine,\n    ContentEncodingError,\n    ContentLengthError,\n    InvalidHeader,\n    LineTooLong,\n    TransferEncodingError,\n)\nfrom .http_writer import HttpVersion, HttpVersion10\nfrom .log import internal_logger\nfrom .streams import EMPTY_PAYLOAD, StreamReader\nfrom .typedefs import Final, RawHeaders\n\ntry:\n    import brotli\n\n    HAS_BROTLI = True\nexcept ImportError:  # pragma: no cover\n    HAS_BROTLI = False\n\n\n__all__ = (\n    \"HeadersParser\",\n    \"HttpParser\",\n    \"HttpRequestParser\",\n    \"HttpResponseParser\",\n    \"RawRequestMessage\",\n    \"RawResponseMessage\",\n)\n\nASCIISET: Final[Set[str]] = set(string.printable)\n\n# See https://tools.ietf.org/html/rfc7230#section-3.1.1\n# and https://tools.ietf.org/html/rfc7230#appendix-B\n#\n#     method = token\n#     tchar = \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\" / \"+\" / \"-\" / \".\" /\n#             \"^\" / \"_\" / \"`\" / \"|\" / \"~\" / DIGIT / ALPHA\n#     token = 1*tchar\nMETHRE: Final[Pattern[str]] = re.compile(r\"[!#$%&'*+\\-.^_`|~0-9A-Za-z]+\")\nVERSRE: Final[Pattern[str]] = re.compile(r\"HTTP/(\\d+).(\\d+)\")\nHDRRE: Final[Pattern[bytes]] = re.compile(rb\"[\\x00-\\x1F\\x7F()<>@,;:\\[\\]={} \\t\\\\\\\\\\\"]\")\n\n\nclass RawRequestMessage(NamedTuple):\n    method: str\n    path: str\n    version: HttpVersion\n    headers: \"CIMultiDictProxy[str]\"\n    raw_headers: RawHeaders\n    should_close: bool\n    compression: Optional[str]\n    upgrade: bool\n    chunked: bool\n    url: URL\n\n\nRawResponseMessage = collections.namedtuple(\n    \"RawResponseMessage\",\n    [\n        \"version\",\n        \"code\",\n        \"reason\",\n        \"headers\",\n        \"raw_headers\",\n        \"should_close\",\n        \"compression\",\n        \"upgrade\",\n        \"chunked\",\n    ],\n)\n\n\n_MsgT = TypeVar(\"_MsgT\", RawRequestMessage, RawResponseMessage)\n\n\nclass ParseState(IntEnum):\n\n    PARSE_NONE = 0\n    PARSE_LENGTH = 1\n    PARSE_CHUNKED = 2\n    PARSE_UNTIL_EOF = 3\n\n\nclass ChunkState(IntEnum):\n    PARSE_CHUNKED_SIZE = 0\n    PARSE_CHUNKED_CHUNK = 1\n    PARSE_CHUNKED_CHUNK_EOF = 2\n    PARSE_MAYBE_TRAILERS = 3\n    PARSE_TRAILERS = 4\n\n\nclass HeadersParser:\n    def __init__(\n        self,\n        max_line_size: int = 8190,\n        max_headers: int = 32768,\n        max_field_size: int = 8190,\n    ) -> None:\n        self.max_line_size = max_line_size\n        self.max_headers = max_headers\n        self.max_field_size = max_field_size\n\n    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\"CIMultiDictProxy[str]\", RawHeaders]:\n        headers: CIMultiDict[str] = CIMultiDict()\n        raw_headers = []\n\n        lines_idx = 1\n        line = lines[1]\n        line_count = len(lines)\n\n        while line:\n            # Parse initial header name : value pair.\n            try:\n                bname, bvalue = line.split(b\":\", 1)\n            except ValueError:\n                raise InvalidHeader(line) from None\n\n            bname = bname.strip(b\" \\t\")\n            bvalue = bvalue.lstrip()\n            if HDRRE.search(bname):\n                raise InvalidHeader(bname)\n            if len(bname) > self.max_field_size:\n                raise LineTooLong(\n                    \"request header name {}\".format(\n                        bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                    ),\n                    str(self.max_field_size),\n                    str(len(bname)),\n                )\n\n            header_length = len(bvalue)\n\n            # next line\n            lines_idx += 1\n            line = lines[lines_idx]\n\n            # consume continuation lines\n            continuation = line and line[0] in (32, 9)  # (' ', '\\t')\n\n            if continuation:\n                bvalue_lst = [bvalue]\n                while continuation:\n                    header_length += len(line)\n                    if header_length > self.max_field_size:\n                        raise LineTooLong(\n                            \"request header field {}\".format(\n                                bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                            ),\n                            str(self.max_field_size),\n                            str(header_length),\n                        )\n                    bvalue_lst.append(line)\n\n                    # next line\n                    lines_idx += 1\n                    if lines_idx < line_count:\n                        line = lines[lines_idx]\n                        if line:\n                            continuation = line[0] in (32, 9)  # (' ', '\\t')\n                    else:\n                        line = b\"\"\n                        break\n                bvalue = b\"\".join(bvalue_lst)\n            else:\n                if header_length > self.max_field_size:\n                    raise LineTooLong(\n                        \"request header field {}\".format(\n                            bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                        ),\n                        str(self.max_field_size),\n                        str(header_length),\n                    )\n\n            bvalue = bvalue.strip()\n            name = bname.decode(\"utf-8\", \"surrogateescape\")\n            value = bvalue.decode(\"utf-8\", \"surrogateescape\")\n\n            headers.add(name, value)\n            raw_headers.append((bname, bvalue))\n\n        return (CIMultiDictProxy(headers), tuple(raw_headers))\n\n\nclass HttpParser(abc.ABC, Generic[_MsgT]):\n    def __init__(\n        self,\n        protocol: Optional[BaseProtocol] = None,\n        loop: Optional[asyncio.AbstractEventLoop] = None,\n        limit: int = 2**16,\n        max_line_size: int = 8190,\n        max_headers: int = 32768,\n        max_field_size: int = 8190,\n        timer: Optional[BaseTimerContext] = None,\n        code: Optional[int] = None,\n        method: Optional[str] = None,\n        readall: bool = False,\n        payload_exception: Optional[Type[BaseException]] = None,\n        response_with_body: bool = True,\n        read_until_eof: bool = False,\n        auto_decompress: bool = True,\n    ) -> None:\n        self.protocol = protocol\n        self.loop = loop\n        self.max_line_size = max_line_size\n        self.max_headers = max_headers\n        self.max_field_size = max_field_size\n        self.timer = timer\n        self.code = code\n        self.method = method\n        self.readall = readall\n        self.payload_exception = payload_exception\n        self.response_with_body = response_with_body\n        self.read_until_eof = read_until_eof\n\n        self._lines: List[bytes] = []\n        self._tail = b\"\"\n        self._upgraded = False\n        self._payload = None\n        self._payload_parser: Optional[HttpPayloadParser] = None\n        self._auto_decompress = auto_decompress\n        self._limit = limit\n        self._headers_parser = HeadersParser(max_line_size, max_headers, max_field_size)\n\n    @abc.abstractmethod\n    def parse_message(self, lines: List[bytes]) -> _MsgT:\n        pass\n\n    def feed_eof(self) -> Optional[_MsgT]:\n        if self._payload_parser is not None:\n            self._payload_parser.feed_eof()\n            self._payload_parser = None\n        else:\n            # try to extract partial message\n            if self._tail:\n                self._lines.append(self._tail)\n\n            if self._lines:\n                if self._lines[-1] != \"\\r\\n\":\n                    self._lines.append(b\"\")\n                with suppress(Exception):\n                    return self.parse_message(self._lines)\n        return None\n\n    def feed_data(\n        self,\n        data: bytes,\n        SEP: bytes = b\"\\r\\n\",\n        EMPTY: bytes = b\"\",\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\n        METH_CONNECT: str = hdrs.METH_CONNECT,\n        SEC_WEBSOCKET_KEY1: istr = hdrs.SEC_WEBSOCKET_KEY1,\n    ) -> Tuple[List[Tuple[_MsgT, StreamReader]], bool, bytes]:\n\n        messages = []\n\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n\n        data_len = len(data)\n        start_pos = 0\n        loop = self.loop\n\n        while start_pos < data_len:\n\n            # read HTTP message (request/response line + headers), \\r\\n\\r\\n\n            # and split by lines\n            if self._payload_parser is None and not self._upgraded:\n                pos = data.find(SEP, start_pos)\n                # consume \\r\\n\n                if pos == start_pos and not self._lines:\n                    start_pos = pos + 2\n                    continue\n\n                if pos >= start_pos:\n                    # line found\n                    self._lines.append(data[start_pos:pos])\n                    start_pos = pos + 2\n\n                    # \\r\\n\\r\\n found\n                    if self._lines[-1] == EMPTY:\n                        try:\n                            msg: _MsgT = self.parse_message(self._lines)\n                        finally:\n                            self._lines.clear()\n\n                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            try:\n                                length = int(length_hdr)\n                            except ValueError:\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            if length < 0:\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return length\n\n                        length = get_content_length()\n                        # do not support old websocket spec\n                        if SEC_WEBSOCKET_KEY1 in msg.headers:\n                            raise InvalidHeader(SEC_WEBSOCKET_KEY1)\n\n                        self._upgraded = msg.upgrade\n\n                        method = getattr(msg, \"method\", self.method)\n\n                        assert self.protocol is not None\n                        # calculate payload\n                        if (\n                            (length is not None and length > 0)\n                            or msg.chunked\n                            and not msg.upgrade\n                        ):\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                readall=self.readall,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        elif method == METH_CONNECT:\n                            assert isinstance(msg, RawRequestMessage)\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            self._upgraded = True\n                            self._payload_parser = HttpPayloadParser(\n                                payload,\n                                method=msg.method,\n                                compression=msg.compression,\n                                readall=True,\n                                auto_decompress=self._auto_decompress,\n                            )\n                        else:\n                            if (\n                                getattr(msg, \"code\", 100) >= 199\n                                and length is None\n                                and self.read_until_eof\n                            ):\n                                payload = StreamReader(\n                                    self.protocol,\n                                    timer=self.timer,\n                                    loop=loop,\n                                    limit=self._limit,\n                                )\n                                payload_parser = HttpPayloadParser(\n                                    payload,\n                                    length=length,\n                                    chunked=msg.chunked,\n                                    method=method,\n                                    compression=msg.compression,\n                                    code=self.code,\n                                    readall=True,\n                                    response_with_body=self.response_with_body,\n                                    auto_decompress=self._auto_decompress,\n                                )\n                                if not payload_parser.done:\n                                    self._payload_parser = payload_parser\n                            else:\n                                payload = EMPTY_PAYLOAD\n\n                        messages.append((msg, payload))\n                else:\n                    self._tail = data[start_pos:]\n                    data = EMPTY\n                    break\n\n            # no parser, just store\n            elif self._payload_parser is None and self._upgraded:\n                assert not self._lines\n                break\n\n            # feed payload\n            elif data and start_pos < data_len:\n                assert not self._lines\n                assert self._payload_parser is not None\n                try:\n                    eof, data = self._payload_parser.feed_data(data[start_pos:])\n                except BaseException as exc:\n                    if self.payload_exception is not None:\n                        self._payload_parser.payload.set_exception(\n                            self.payload_exception(str(exc))\n                        )\n                    else:\n                        self._payload_parser.payload.set_exception(exc)\n\n                    eof = True\n                    data = b\"\"\n\n                if eof:\n                    start_pos = 0\n                    data_len = len(data)\n                    self._payload_parser = None\n                    continue\n            else:\n                break\n\n        if data and start_pos < data_len:\n            data = data[start_pos:]\n        else:\n            data = EMPTY\n\n        return messages, self._upgraded, data\n\n    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\n        \"CIMultiDictProxy[str]\", RawHeaders, Optional[bool], Optional[str], bool, bool\n    ]:\n        \"\"\"Parses RFC 5322 headers from a stream.\n\n        Line continuations are supported. Returns list of header name\n        and value pairs. Header name is in upper case.\n        \"\"\"\n        headers, raw_headers = self._headers_parser.parse_headers(lines)\n        close_conn = None\n        encoding = None\n        upgrade = False\n        chunked = False\n\n        # keep-alive\n        conn = headers.get(hdrs.CONNECTION)\n        if conn:\n            v = conn.lower()\n            if v == \"close\":\n                close_conn = True\n            elif v == \"keep-alive\":\n                close_conn = False\n            elif v == \"upgrade\":\n                upgrade = True\n\n        # encoding\n        enc = headers.get(hdrs.CONTENT_ENCODING)\n        if enc:\n            enc = enc.lower()\n            if enc in (\"gzip\", \"deflate\", \"br\"):\n                encoding = enc\n\n        # chunking\n        te = headers.get(hdrs.TRANSFER_ENCODING)\n        if te is not None:\n            if \"chunked\" == te.lower():\n                chunked = True\n            else:\n                raise BadHttpMessage(\"Request has invalid `Transfer-Encoding`\")\n\n            if hdrs.CONTENT_LENGTH in headers:\n                raise BadHttpMessage(\n                    \"Transfer-Encoding can't be present with Content-Length\",\n                )\n\n        return (headers, raw_headers, close_conn, encoding, upgrade, chunked)\n\n    def set_upgraded(self, val: bool) -> None:\n        \"\"\"Set connection upgraded (to websocket) mode.\n\n        :param bool val: new state.\n        \"\"\"\n        self._upgraded = val\n\n\nclass HttpRequestParser(HttpParser[RawRequestMessage]):\n    \"\"\"Read request status line.\n\n    Exception .http_exceptions.BadStatusLine\n    could be raised in case of any errors in status line.\n    Returns RawRequestMessage.\n    \"\"\"\n\n    def parse_message(self, lines: List[bytes]) -> RawRequestMessage:\n        # request line\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            method, path, version = line.split(None, 2)\n        except ValueError:\n            raise BadStatusLine(line) from None\n\n        if len(path) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(path))\n            )\n\n        # method\n        if not METHRE.match(method):\n            raise BadStatusLine(method)\n\n        # version\n        try:\n            if version.startswith(\"HTTP/\"):\n                n1, n2 = version[5:].split(\".\", 1)\n                version_o = HttpVersion(int(n1), int(n2))\n            else:\n                raise BadStatusLine(version)\n        except Exception:\n            raise BadStatusLine(version)\n\n        if method == \"CONNECT\":\n            # authority-form,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.3\n            url = URL.build(authority=path, encoded=True)\n        elif path.startswith(\"/\"):\n            # origin-form,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.1\n            path_part, _hash_separator, url_fragment = path.partition(\"#\")\n            path_part, _question_mark_separator, qs_part = path_part.partition(\"?\")\n\n            # NOTE: `yarl.URL.build()` is used to mimic what the Cython-based\n            # NOTE: parser does, otherwise it results into the same\n            # NOTE: HTTP Request-Line input producing different\n            # NOTE: `yarl.URL()` objects\n            url = URL.build(\n                path=path_part,\n                query_string=qs_part,\n                fragment=url_fragment,\n                encoded=True,\n            )\n        else:\n            # absolute-form for proxy maybe,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.2\n            url = URL(path, encoded=True)\n\n        # read headers\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n\n        if close is None:  # then the headers weren't set in the request\n            if version_o <= HttpVersion10:  # HTTP 1.0 must asks to not close\n                close = True\n            else:  # HTTP 1.1 must ask to close.\n                close = False\n\n        return RawRequestMessage(\n            method,\n            path,\n            version_o,\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n            url,\n        )\n\n\nclass HttpResponseParser(HttpParser[RawResponseMessage]):\n    \"\"\"Read response status line and headers.\n\n    BadStatusLine could be raised in case of any errors in status line.\n    Returns RawResponseMessage.\n    \"\"\"\n\n    def parse_message(self, lines: List[bytes]) -> RawResponseMessage:\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            version, status = line.split(None, 1)\n        except ValueError:\n            raise BadStatusLine(line) from None\n\n        try:\n            status, reason = status.split(None, 1)\n        except ValueError:\n            reason = \"\"\n\n        if len(reason) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(reason))\n            )\n\n        # version\n        match = VERSRE.match(version)\n        if match is None:\n            raise BadStatusLine(line)\n        version_o = HttpVersion(int(match.group(1)), int(match.group(2)))\n\n        # The status code is a three-digit number\n        try:\n            status_i = int(status)\n        except ValueError:\n            raise BadStatusLine(line) from None\n\n        if status_i > 999:\n            raise BadStatusLine(line)\n\n        # read headers\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n\n        if close is None:\n            close = version_o <= HttpVersion10\n\n        return RawResponseMessage(\n            version_o,\n            status_i,\n            reason.strip(),\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        )\n\n\nclass HttpPayloadParser:\n    def __init__(\n        self,\n        payload: StreamReader,\n        length: Optional[int] = None,\n        chunked: bool = False,\n        compression: Optional[str] = None,\n        code: Optional[int] = None,\n        method: Optional[str] = None,\n        readall: bool = False,\n        response_with_body: bool = True,\n        auto_decompress: bool = True,\n    ) -> None:\n        self._length = 0\n        self._type = ParseState.PARSE_NONE\n        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\n        self._chunk_size = 0\n        self._chunk_tail = b\"\"\n        self._auto_decompress = auto_decompress\n        self.done = False\n\n        # payload decompression wrapper\n        if response_with_body and compression and self._auto_decompress:\n            real_payload: Union[StreamReader, DeflateBuffer] = DeflateBuffer(\n                payload, compression\n            )\n        else:\n            real_payload = payload\n\n        # payload parser\n        if not response_with_body:\n            # don't parse payload if it's not expected to be received\n            self._type = ParseState.PARSE_NONE\n            real_payload.feed_eof()\n            self.done = True\n\n        elif chunked:\n            self._type = ParseState.PARSE_CHUNKED\n        elif length is not None:\n            self._type = ParseState.PARSE_LENGTH\n            self._length = length\n            if self._length == 0:\n                real_payload.feed_eof()\n                self.done = True\n        else:\n            if readall and code != 204:\n                self._type = ParseState.PARSE_UNTIL_EOF\n            elif method in (\"PUT\", \"POST\"):\n                internal_logger.warning(  # pragma: no cover\n                    \"Content-Length or Transfer-Encoding header is required\"\n                )\n                self._type = ParseState.PARSE_NONE\n                real_payload.feed_eof()\n                self.done = True\n\n        self.payload = real_payload\n\n    def feed_eof(self) -> None:\n        if self._type == ParseState.PARSE_UNTIL_EOF:\n            self.payload.feed_eof()\n        elif self._type == ParseState.PARSE_LENGTH:\n            raise ContentLengthError(\n                \"Not enough data for satisfy content length header.\"\n            )\n        elif self._type == ParseState.PARSE_CHUNKED:\n            raise TransferEncodingError(\n                \"Not enough data for satisfy transfer length header.\"\n            )\n\n    def feed_data(\n        self, chunk: bytes, SEP: bytes = b\"\\r\\n\", CHUNK_EXT: bytes = b\";\"\n    ) -> Tuple[bool, bytes]:\n        # Read specified amount of bytes\n        if self._type == ParseState.PARSE_LENGTH:\n            required = self._length\n            chunk_len = len(chunk)\n\n            if required >= chunk_len:\n                self._length = required - chunk_len\n                self.payload.feed_data(chunk, chunk_len)\n                if self._length == 0:\n                    self.payload.feed_eof()\n                    return True, b\"\"\n            else:\n                self._length = 0\n                self.payload.feed_data(chunk[:required], required)\n                self.payload.feed_eof()\n                return True, chunk[required:]\n\n        # Chunked transfer encoding parser\n        elif self._type == ParseState.PARSE_CHUNKED:\n            if self._chunk_tail:\n                chunk = self._chunk_tail + chunk\n                self._chunk_tail = b\"\"\n\n            while chunk:\n\n                # read next chunk size\n                if self._chunk == ChunkState.PARSE_CHUNKED_SIZE:\n                    pos = chunk.find(SEP)\n                    if pos >= 0:\n                        i = chunk.find(CHUNK_EXT, 0, pos)\n                        if i >= 0:\n                            size_b = chunk[:i]  # strip chunk-extensions\n                        else:\n                            size_b = chunk[:pos]\n\n                        try:\n                            size = int(bytes(size_b), 16)\n                        except ValueError:\n                            exc = TransferEncodingError(\n                                chunk[:pos].decode(\"ascii\", \"surrogateescape\")\n                            )\n                            self.payload.set_exception(exc)\n                            raise exc from None\n\n                        chunk = chunk[pos + 2 :]\n                        if size == 0:  # eof marker\n                            self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\n                        else:\n                            self._chunk = ChunkState.PARSE_CHUNKED_CHUNK\n                            self._chunk_size = size\n                            self.payload.begin_http_chunk_receiving()\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n                # read chunk and feed buffer\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK:\n                    required = self._chunk_size\n                    chunk_len = len(chunk)\n\n                    if required > chunk_len:\n                        self._chunk_size = required - chunk_len\n                        self.payload.feed_data(chunk, chunk_len)\n                        return False, b\"\"\n                    else:\n                        self._chunk_size = 0\n                        self.payload.feed_data(chunk[:required], required)\n                        chunk = chunk[required:]\n                        self._chunk = ChunkState.PARSE_CHUNKED_CHUNK_EOF\n                        self.payload.end_http_chunk_receiving()\n\n                # toss the CRLF at the end of the chunk\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK_EOF:\n                    if chunk[:2] == SEP:\n                        chunk = chunk[2:]\n                        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n                # if stream does not contain trailer, after 0\\r\\n\n                # we should get another \\r\\n otherwise\n                # trailers needs to be skiped until \\r\\n\\r\\n\n                if self._chunk == ChunkState.PARSE_MAYBE_TRAILERS:\n                    head = chunk[:2]\n                    if head == SEP:\n                        # end of stream\n                        self.payload.feed_eof()\n                        return True, chunk[2:]\n                    # Both CR and LF, or only LF may not be received yet. It is\n                    # expected that CRLF or LF will be shown at the very first\n                    # byte next time, otherwise trailers should come. The last\n                    # CRLF which marks the end of response might not be\n                    # contained in the same TCP segment which delivered the\n                    # size indicator.\n                    if not head:\n                        return False, b\"\"\n                    if head == SEP[:1]:\n                        self._chunk_tail = head\n                        return False, b\"\"\n                    self._chunk = ChunkState.PARSE_TRAILERS\n\n                # read and discard trailer up to the CRLF terminator\n                if self._chunk == ChunkState.PARSE_TRAILERS:\n                    pos = chunk.find(SEP)\n                    if pos >= 0:\n                        chunk = chunk[pos + 2 :]\n                        self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n        # Read all bytes until eof\n        elif self._type == ParseState.PARSE_UNTIL_EOF:\n            self.payload.feed_data(chunk, len(chunk))\n\n        return False, b\"\"\n\n\nclass DeflateBuffer:\n    \"\"\"DeflateStream decompress stream and feed data into specified stream.\"\"\"\n\n    decompressor: Any\n\n    def __init__(self, out: StreamReader, encoding: Optional[str]) -> None:\n        self.out = out\n        self.size = 0\n        self.encoding = encoding\n        self._started_decoding = False\n\n        if encoding == \"br\":\n            if not HAS_BROTLI:  # pragma: no cover\n                raise ContentEncodingError(\n                    \"Can not decode content-encoding: brotli (br). \"\n                    \"Please install `Brotli`\"\n                )\n\n            class BrotliDecoder:\n                # Supports both 'brotlipy' and 'Brotli' packages\n                # since they share an import name. The top branches\n                # are for 'brotlipy' and bottom branches for 'Brotli'\n                def __init__(self) -> None:\n                    self._obj = brotli.Decompressor()\n\n                def decompress(self, data: bytes) -> bytes:\n                    if hasattr(self._obj, \"decompress\"):\n                        return cast(bytes, self._obj.decompress(data))\n                    return cast(bytes, self._obj.process(data))\n\n                def flush(self) -> bytes:\n                    if hasattr(self._obj, \"flush\"):\n                        return cast(bytes, self._obj.flush())\n                    return b\"\"\n\n            self.decompressor = BrotliDecoder()\n        else:\n            zlib_mode = 16 + zlib.MAX_WBITS if encoding == \"gzip\" else zlib.MAX_WBITS\n            self.decompressor = zlib.decompressobj(wbits=zlib_mode)\n\n    def set_exception(self, exc: BaseException) -> None:\n        self.out.set_exception(exc)\n\n    def feed_data(self, chunk: bytes, size: int) -> None:\n        if not size:\n            return\n\n        self.size += size\n\n        # RFC1950\n        # bits 0..3 = CM = 0b1000 = 8 = \"deflate\"\n        # bits 4..7 = CINFO = 1..7 = windows size.\n        if (\n            not self._started_decoding\n            and self.encoding == \"deflate\"\n            and chunk[0] & 0xF != 8\n        ):\n            # Change the decoder to decompress incorrectly compressed data\n            # Actually we should issue a warning about non-RFC-compliant data.\n            self.decompressor = zlib.decompressobj(wbits=-zlib.MAX_WBITS)\n\n        try:\n            chunk = self.decompressor.decompress(chunk)\n        except Exception:\n            raise ContentEncodingError(\n                \"Can not decode content-encoding: %s\" % self.encoding\n            )\n\n        self._started_decoding = True\n\n        if chunk:\n            self.out.feed_data(chunk, len(chunk))\n\n    def feed_eof(self) -> None:\n        chunk = self.decompressor.flush()\n\n        if chunk or self.size > 0:\n            self.out.feed_data(chunk, len(chunk))\n            if self.encoding == \"deflate\" and not self.decompressor.eof:\n                raise ContentEncodingError(\"deflate\")\n\n        self.out.feed_eof()\n\n    def begin_http_chunk_receiving(self) -> None:\n        self.out.begin_http_chunk_receiving()\n\n    def end_http_chunk_receiving(self) -> None:\n        self.out.end_http_chunk_receiving()\n\n\nHttpRequestParserPy = HttpRequestParser\nHttpResponseParserPy = HttpResponseParser\nRawRequestMessagePy = RawRequestMessage\nRawResponseMessagePy = RawResponseMessage\n\ntry:\n    if not NO_EXTENSIONS:\n        from ._http_parser import (  # type: ignore[import,no-redef]\n            HttpRequestParser,\n            HttpResponseParser,\n            RawRequestMessage,\n            RawResponseMessage,\n        )\n\n        HttpRequestParserC = HttpRequestParser\n        HttpResponseParserC = HttpResponseParser\n        RawRequestMessageC = RawRequestMessage\n        RawResponseMessageC = RawResponseMessage\nexcept ImportError:  # pragma: no cover\n    pass\n", "# Tests for aiohttp/protocol.py\n\nimport asyncio\nimport re\nfrom typing import Any, List\nfrom unittest import mock\nfrom urllib.parse import quote\n\nimport pytest\nfrom multidict import CIMultiDict\nfrom yarl import URL\n\nimport aiohttp\nfrom aiohttp import http_exceptions, streams\nfrom aiohttp.http_parser import (\n    NO_EXTENSIONS,\n    DeflateBuffer,\n    HttpPayloadParser,\n    HttpRequestParserPy,\n    HttpResponseParserPy,\n)\n\ntry:\n    import brotli\nexcept ImportError:\n    brotli = None\n\n\nREQUEST_PARSERS = [HttpRequestParserPy]\nRESPONSE_PARSERS = [HttpResponseParserPy]\n\ntry:\n    from aiohttp.http_parser import HttpRequestParserC, HttpResponseParserC\n\n    REQUEST_PARSERS.append(HttpRequestParserC)\n    RESPONSE_PARSERS.append(HttpResponseParserC)\nexcept ImportError:  # pragma: no cover\n    pass\n\n\n@pytest.fixture\ndef protocol():\n    return mock.Mock()\n\n\ndef _gen_ids(parsers: List[Any]) -> List[str]:\n    return [\n        \"py-parser\" if parser.__module__ == \"aiohttp.http_parser\" else \"c-parser\"\n        for parser in parsers\n    ]\n\n\n@pytest.fixture(params=REQUEST_PARSERS, ids=_gen_ids(REQUEST_PARSERS))\ndef parser(loop: Any, protocol: Any, request: Any):\n    # Parser implementations\n    return request.param(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_headers=32768,\n        max_field_size=8190,\n    )\n\n\n@pytest.fixture(params=REQUEST_PARSERS, ids=_gen_ids(REQUEST_PARSERS))\ndef request_cls(request: Any):\n    # Request Parser class\n    return request.param\n\n\n@pytest.fixture(params=RESPONSE_PARSERS, ids=_gen_ids(RESPONSE_PARSERS))\ndef response(loop: Any, protocol: Any, request: Any):\n    # Parser implementations\n    return request.param(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_headers=32768,\n        max_field_size=8190,\n    )\n\n\n@pytest.fixture(params=RESPONSE_PARSERS, ids=_gen_ids(RESPONSE_PARSERS))\ndef response_cls(request: Any):\n    # Parser implementations\n    return request.param\n\n\n@pytest.fixture\ndef stream():\n    return mock.Mock()\n\n\n@pytest.mark.skipif(NO_EXTENSIONS, reason=\"Extentions available but not imported\")\ndef test_c_parser_loaded():\n    assert \"HttpRequestParserC\" in dir(aiohttp.http_parser)\n    assert \"HttpResponseParserC\" in dir(aiohttp.http_parser)\n    assert \"RawRequestMessageC\" in dir(aiohttp.http_parser)\n    assert \"RawResponseMessageC\" in dir(aiohttp.http_parser)\n\n\ndef test_parse_headers(parser: Any) -> None:\n    text = b\"\"\"GET /test HTTP/1.1\\r\ntest: line\\r\n continue\\r\ntest2: data\\r\n\\r\n\"\"\"\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 1\n    msg = messages[0][0]\n\n    assert list(msg.headers.items()) == [(\"test\", \"line continue\"), (\"test2\", \"data\")]\n    assert msg.raw_headers == ((b\"test\", b\"line continue\"), (b\"test2\", b\"data\"))\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n\n\n@pytest.mark.skipif(NO_EXTENSIONS, reason=\"Only tests C parser.\")\ndef test_invalid_character(loop: Any, protocol: Any, request: Any) -> None:\n    parser = HttpRequestParserC(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_field_size=8190,\n    )\n    text = b\"POST / HTTP/1.1\\r\\nHost: localhost:8080\\r\\nSet-Cookie: abc\\x01def\\r\\n\\r\\n\"\n    error_detail = re.escape(\n        r\"\"\":\n\n    b'Set-Cookie: abc\\x01def'\n                     ^\"\"\"\n    )\n    with pytest.raises(http_exceptions.BadHttpMessage, match=error_detail):\n        parser.feed_data(text)\n\n\n@pytest.mark.skipif(NO_EXTENSIONS, reason=\"Only tests C parser.\")\ndef test_invalid_linebreak(loop: Any, protocol: Any, request: Any) -> None:\n    parser = HttpRequestParserC(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_field_size=8190,\n    )\n    text = b\"GET /world HTTP/1.1\\r\\nHost: 127.0.0.1\\n\\r\\n\"\n    error_detail = re.escape(\n        r\"\"\":\n\n    b'Host: 127.0.0.1\\n'\n                     ^\"\"\"\n    )\n    with pytest.raises(http_exceptions.BadHttpMessage, match=error_detail):\n        parser.feed_data(text)\n\n\ndef test_parse(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 1\n    msg, _ = messages[0]\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert msg.method == \"GET\"\n    assert msg.path == \"/test\"\n    assert msg.version == (1, 1)\n\n\nasync def test_parse_body(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\nContent-Length: 4\\r\\n\\r\\nbody\"\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 1\n    _, payload = messages[0]\n    body = await payload.read(4)\n    assert body == b\"body\"\n\n\nasync def test_parse_body_with_CRLF(parser) -> None:\n    text = b\"\\r\\nGET /test HTTP/1.1\\r\\nContent-Length: 4\\r\\n\\r\\nbody\"\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 1\n    _, payload = messages[0]\n    body = await payload.read(4)\n    assert body == b\"body\"\n\n\ndef test_parse_delayed(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 0\n    assert not upgrade\n\n    messages, upgrade, tail = parser.feed_data(b\"\\r\\n\")\n    assert len(messages) == 1\n    msg = messages[0][0]\n    assert msg.method == \"GET\"\n\n\ndef test_headers_multi_feed(parser) -> None:\n    text1 = b\"GET /test HTTP/1.1\\r\\n\"\n    text2 = b\"test: line\\r\"\n    text3 = b\"\\n continue\\r\\n\\r\\n\"\n\n    messages, upgrade, tail = parser.feed_data(text1)\n    assert len(messages) == 0\n\n    messages, upgrade, tail = parser.feed_data(text2)\n    assert len(messages) == 0\n\n    messages, upgrade, tail = parser.feed_data(text3)\n    assert len(messages) == 1\n\n    msg = messages[0][0]\n    assert list(msg.headers.items()) == [(\"test\", \"line continue\")]\n    assert msg.raw_headers == ((b\"test\", b\"line continue\"),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n\n\ndef test_headers_split_field(parser) -> None:\n    text1 = b\"GET /test HTTP/1.1\\r\\n\"\n    text2 = b\"t\"\n    text3 = b\"es\"\n    text4 = b\"t: value\\r\\n\\r\\n\"\n\n    messages, upgrade, tail = parser.feed_data(text1)\n    messages, upgrade, tail = parser.feed_data(text2)\n    messages, upgrade, tail = parser.feed_data(text3)\n    assert len(messages) == 0\n    messages, upgrade, tail = parser.feed_data(text4)\n    assert len(messages) == 1\n\n    msg = messages[0][0]\n    assert list(msg.headers.items()) == [(\"test\", \"value\")]\n    assert msg.raw_headers == ((b\"test\", b\"value\"),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n\n\ndef test_parse_headers_multi(parser) -> None:\n    text = (\n        b\"GET /test HTTP/1.1\\r\\n\"\n        b\"Set-Cookie: c1=cookie1\\r\\n\"\n        b\"Set-Cookie: c2=cookie2\\r\\n\\r\\n\"\n    )\n\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 1\n    msg = messages[0][0]\n\n    assert list(msg.headers.items()) == [\n        (\"Set-Cookie\", \"c1=cookie1\"),\n        (\"Set-Cookie\", \"c2=cookie2\"),\n    ]\n    assert msg.raw_headers == (\n        (b\"Set-Cookie\", b\"c1=cookie1\"),\n        (b\"Set-Cookie\", b\"c2=cookie2\"),\n    )\n    assert not msg.should_close\n    assert msg.compression is None\n\n\ndef test_conn_default_1_0(parser) -> None:\n    text = b\"GET /test HTTP/1.0\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.should_close\n\n\ndef test_conn_default_1_1(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n\n\ndef test_conn_close(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"connection: close\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.should_close\n\n\ndef test_conn_close_1_0(parser) -> None:\n    text = b\"GET /test HTTP/1.0\\r\\n\" b\"connection: close\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.should_close\n\n\ndef test_conn_keep_alive_1_0(parser) -> None:\n    text = b\"GET /test HTTP/1.0\\r\\n\" b\"connection: keep-alive\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n\n\ndef test_conn_keep_alive_1_1(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"connection: keep-alive\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n\n\ndef test_conn_other_1_0(parser) -> None:\n    text = b\"GET /test HTTP/1.0\\r\\n\" b\"connection: test\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.should_close\n\n\ndef test_conn_other_1_1(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"connection: test\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n\n\ndef test_request_chunked(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg, payload = messages[0]\n    assert msg.chunked\n    assert not upgrade\n    assert isinstance(payload, streams.StreamReader)\n\n\ndef test_request_te_chunked_with_content_length(parser: Any) -> None:\n    text = (\n        b\"GET /test HTTP/1.1\\r\\n\"\n        b\"content-length: 1234\\r\\n\"\n        b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    )\n    with pytest.raises(\n        http_exceptions.BadHttpMessage,\n        match=\"Transfer-Encoding can't be present with Content-Length\",\n    ):\n        parser.feed_data(text)\n\n\ndef test_request_te_chunked123(parser: Any) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked123\\r\\n\\r\\n\"\n    with pytest.raises(\n        http_exceptions.BadHttpMessage,\n        match=\"Request has invalid `Transfer-Encoding`\",\n    ):\n        parser.feed_data(text)\n\n\ndef test_conn_upgrade(parser: Any) -> None:\n    text = (\n        b\"GET /test HTTP/1.1\\r\\n\"\n        b\"connection: upgrade\\r\\n\"\n        b\"upgrade: websocket\\r\\n\\r\\n\"\n    )\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n    assert msg.upgrade\n    assert upgrade\n\n\ndef test_compression_empty(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-encoding: \\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.compression is None\n\n\ndef test_compression_deflate(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-encoding: deflate\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.compression == \"deflate\"\n\n\ndef test_compression_gzip(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-encoding: gzip\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.compression == \"gzip\"\n\n\n@pytest.mark.skipif(brotli is None, reason=\"brotli is not installed\")\ndef test_compression_brotli(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-encoding: br\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.compression == \"br\"\n\n\ndef test_compression_unknown(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-encoding: compress\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.compression is None\n\n\ndef test_url_connect(parser: Any) -> None:\n    text = b\"CONNECT www.google.com HTTP/1.1\\r\\n\" b\"content-length: 0\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg, payload = messages[0]\n    assert upgrade\n    assert msg.url == URL.build(authority=\"www.google.com\")\n\n\ndef test_headers_connect(parser: Any) -> None:\n    text = b\"CONNECT www.google.com HTTP/1.1\\r\\n\" b\"content-length: 0\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg, payload = messages[0]\n    assert upgrade\n    assert isinstance(payload, streams.StreamReader)\n\n\ndef test_url_absolute(parser: Any) -> None:\n    text = (\n        b\"GET https://www.google.com/path/to.html HTTP/1.1\\r\\n\"\n        b\"content-length: 0\\r\\n\\r\\n\"\n    )\n    messages, upgrade, tail = parser.feed_data(text)\n    msg, payload = messages[0]\n    assert not upgrade\n    assert msg.method == \"GET\"\n    assert msg.url == URL(\"https://www.google.com/path/to.html\")\n\n\ndef test_headers_old_websocket_key1(parser: Any) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"SEC-WEBSOCKET-KEY1: line\\r\\n\\r\\n\"\n\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\ndef test_headers_content_length_err_1(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-length: line\\r\\n\\r\\n\"\n\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\ndef test_headers_content_length_err_2(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-length: -1\\r\\n\\r\\n\"\n\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\ndef test_invalid_header(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"test line\\r\\n\\r\\n\"\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\ndef test_invalid_name(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"test[]: line\\r\\n\\r\\n\"\n\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\n@pytest.mark.parametrize(\"size\", [40960, 8191])\ndef test_max_header_field_size(parser, size) -> None:\n    name = b\"t\" * size\n    text = b\"GET /test HTTP/1.1\\r\\n\" + name + b\":data\\r\\n\\r\\n\"\n\n    match = f\"400, message:\\n  Got more than 8190 bytes \\\\({size}\\\\) when reading\"\n    with pytest.raises(http_exceptions.LineTooLong, match=match):\n        parser.feed_data(text)\n\n\ndef test_max_header_field_size_under_limit(parser) -> None:\n    name = b\"t\" * 8190\n    text = b\"GET /test HTTP/1.1\\r\\n\" + name + b\":data\\r\\n\\r\\n\"\n\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.method == \"GET\"\n    assert msg.path == \"/test\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict({name.decode(): \"data\"})\n    assert msg.raw_headers == ((name, b\"data\"),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/test\")\n\n\n@pytest.mark.parametrize(\"size\", [40960, 8191])\ndef test_max_header_value_size(parser, size) -> None:\n    name = b\"t\" * size\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"data:\" + name + b\"\\r\\n\\r\\n\"\n\n    match = f\"400, message:\\n  Got more than 8190 bytes \\\\({size}\\\\) when reading\"\n    with pytest.raises(http_exceptions.LineTooLong, match=match):\n        parser.feed_data(text)\n\n\ndef test_max_header_value_size_under_limit(parser) -> None:\n    value = b\"A\" * 8190\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"data:\" + value + b\"\\r\\n\\r\\n\"\n\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.method == \"GET\"\n    assert msg.path == \"/test\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict({\"data\": value.decode()})\n    assert msg.raw_headers == ((b\"data\", value),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/test\")\n\n\n@pytest.mark.parametrize(\"size\", [40965, 8191])\ndef test_max_header_value_size_continuation(parser, size) -> None:\n    name = b\"T\" * (size - 5)\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"data: test\\r\\n \" + name + b\"\\r\\n\\r\\n\"\n\n    match = f\"400, message:\\n  Got more than 8190 bytes \\\\({size}\\\\) when reading\"\n    with pytest.raises(http_exceptions.LineTooLong, match=match):\n        parser.feed_data(text)\n\n\ndef test_max_header_value_size_continuation_under_limit(parser) -> None:\n    value = b\"A\" * 8185\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"data: test\\r\\n \" + value + b\"\\r\\n\\r\\n\"\n\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.method == \"GET\"\n    assert msg.path == \"/test\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict({\"data\": \"test \" + value.decode()})\n    assert msg.raw_headers == ((b\"data\", b\"test \" + value),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/test\")\n\n\ndef test_http_request_parser(parser) -> None:\n    text = b\"GET /path HTTP/1.1\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"/path\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict()\n    assert msg.raw_headers == ()\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/path\")\n\n\ndef test_http_request_bad_status_line(parser) -> None:\n    text = b\"getpath \\r\\n\\r\\n\"\n    with pytest.raises(http_exceptions.BadStatusLine):\n        parser.feed_data(text)\n\n\ndef test_http_request_upgrade(parser) -> None:\n    text = (\n        b\"GET /test HTTP/1.1\\r\\n\"\n        b\"connection: upgrade\\r\\n\"\n        b\"upgrade: websocket\\r\\n\\r\\n\"\n        b\"some raw data\"\n    )\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n    assert msg.upgrade\n    assert upgrade\n    assert tail == b\"some raw data\"\n\n\ndef test_http_request_parser_utf8(parser) -> None:\n    text = \"GET /path HTTP/1.1\\r\\nx-test:\u0442\u0435\u0441\u0442\\r\\n\\r\\n\".encode()\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"/path\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict([(\"X-TEST\", \"\u0442\u0435\u0441\u0442\")])\n    assert msg.raw_headers == ((b\"x-test\", \"\u0442\u0435\u0441\u0442\".encode()),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/path\")\n\n\ndef test_http_request_parser_non_utf8(parser) -> None:\n    text = \"GET /path HTTP/1.1\\r\\nx-test:\u0442\u0435\u0441\u0442\\r\\n\\r\\n\".encode(\"cp1251\")\n    msg = parser.feed_data(text)[0][0][0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"/path\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict(\n        [(\"X-TEST\", \"\u0442\u0435\u0441\u0442\".encode(\"cp1251\").decode(\"utf8\", \"surrogateescape\"))]\n    )\n    assert msg.raw_headers == ((b\"x-test\", \"\u0442\u0435\u0441\u0442\".encode(\"cp1251\")),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/path\")\n\n\ndef test_http_request_parser_two_slashes(parser) -> None:\n    text = b\"GET //path HTTP/1.1\\r\\n\\r\\n\"\n    msg = parser.feed_data(text)[0][0][0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"//path\"\n    assert msg.url.path == \"//path\"\n    assert msg.version == (1, 1)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n\n\ndef test_http_request_parser_bad_method(parser) -> None:\n    with pytest.raises(http_exceptions.BadStatusLine):\n        parser.feed_data(b'=\":<G>(e),[T];?\" /get HTTP/1.1\\r\\n\\r\\n')\n\n\ndef test_http_request_parser_bad_version(parser) -> None:\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(b\"GET //get HT/11\\r\\n\\r\\n\")\n\n\n@pytest.mark.parametrize(\"size\", [40965, 8191])\ndef test_http_request_max_status_line(parser, size) -> None:\n    path = b\"t\" * (size - 5)\n    match = f\"400, message:\\n  Got more than 8190 bytes \\\\({size}\\\\) when reading\"\n    with pytest.raises(http_exceptions.LineTooLong, match=match):\n        parser.feed_data(b\"GET /path\" + path + b\" HTTP/1.1\\r\\n\\r\\n\")\n\n\ndef test_http_request_max_status_line_under_limit(parser) -> None:\n    path = b\"t\" * (8190 - 5)\n    messages, upgraded, tail = parser.feed_data(\n        b\"GET /path\" + path + b\" HTTP/1.1\\r\\n\\r\\n\"\n    )\n    msg = messages[0][0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"/path\" + path.decode()\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict()\n    assert msg.raw_headers == ()\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/path\" + path.decode())\n\n\ndef test_http_response_parser_utf8(response) -> None:\n    text = \"HTTP/1.1 200 Ok\\r\\nx-test:\u0442\u0435\u0441\u0442\\r\\n\\r\\n\".encode()\n\n    messages, upgraded, tail = response.feed_data(text)\n    assert len(messages) == 1\n    msg = messages[0][0]\n\n    assert msg.version == (1, 1)\n    assert msg.code == 200\n    assert msg.reason == \"Ok\"\n    assert msg.headers == CIMultiDict([(\"X-TEST\", \"\u0442\u0435\u0441\u0442\")])\n    assert msg.raw_headers == ((b\"x-test\", \"\u0442\u0435\u0441\u0442\".encode()),)\n    assert not upgraded\n    assert not tail\n\n\n@pytest.mark.parametrize(\"size\", [40962, 8191])\ndef test_http_response_parser_bad_status_line_too_long(response, size) -> None:\n    reason = b\"t\" * (size - 2)\n    match = f\"400, message:\\n  Got more than 8190 bytes \\\\({size}\\\\) when reading\"\n    with pytest.raises(http_exceptions.LineTooLong, match=match):\n        response.feed_data(b\"HTTP/1.1 200 Ok\" + reason + b\"\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_status_line_under_limit(response) -> None:\n    reason = b\"O\" * 8190\n    messages, upgraded, tail = response.feed_data(\n        b\"HTTP/1.1 200 \" + reason + b\"\\r\\n\\r\\n\"\n    )\n    msg = messages[0][0]\n    assert msg.version == (1, 1)\n    assert msg.code == 200\n    assert msg.reason == reason.decode()\n\n\ndef test_http_response_parser_bad_version(response) -> None:\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        response.feed_data(b\"HT/11 200 Ok\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_no_reason(response) -> None:\n    msg = response.feed_data(b\"HTTP/1.1 200\\r\\n\\r\\n\")[0][0][0]\n\n    assert msg.version == (1, 1)\n    assert msg.code == 200\n    assert msg.reason == \"\"\n\n\ndef test_http_response_parser_lenient_headers(response) -> None:\n    messages, upgrade, tail = response.feed_data(\n        b\"HTTP/1.1 200 test\\r\\nFoo: abc\\x01def\\r\\n\\r\\n\"\n    )\n    msg = messages[0][0]\n\n    assert msg.headers[\"Foo\"] == \"abc\\x01def\"\n\n\n@pytest.mark.dev_mode\ndef test_http_response_parser_strict_headers(response) -> None:\n    if isinstance(response, HttpResponseParserPy):\n        pytest.xfail(\"Py parser is lenient. May update py-parser later.\")\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        response.feed_data(b\"HTTP/1.1 200 test\\r\\nFoo: abc\\x01def\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_bad(response) -> None:\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        response.feed_data(b\"HTT/1\\r\\n\\r\\n\")\n\n\n@pytest.mark.skipif(not NO_EXTENSIONS, reason=\"Behaviour has changed in C parser\")\ndef test_http_response_parser_code_under_100(response) -> None:\n    msg = response.feed_data(b\"HTTP/1.1 99 test\\r\\n\\r\\n\")[0][0][0]\n    assert msg.code == 99\n\n\ndef test_http_response_parser_code_above_999(response) -> None:\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        response.feed_data(b\"HTTP/1.1 9999 test\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_code_not_int(response) -> None:\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        response.feed_data(b\"HTTP/1.1 ttt test\\r\\n\\r\\n\")\n\n\ndef test_http_request_chunked_payload(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    assert msg.chunked\n    assert not payload.is_eof()\n    assert isinstance(payload, streams.StreamReader)\n\n    parser.feed_data(b\"4\\r\\ndata\\r\\n4\\r\\nline\\r\\n0\\r\\n\\r\\n\")\n\n    assert b\"dataline\" == b\"\".join(d for d in payload._buffer)\n    assert [4, 8] == payload._http_chunk_splits\n    assert payload.is_eof()\n\n\ndef test_http_request_chunked_payload_and_next_message(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    messages, upgraded, tail = parser.feed_data(\n        b\"4\\r\\ndata\\r\\n4\\r\\nline\\r\\n0\\r\\n\\r\\n\"\n        b\"POST /test2 HTTP/1.1\\r\\n\"\n        b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    )\n\n    assert b\"dataline\" == b\"\".join(d for d in payload._buffer)\n    assert [4, 8] == payload._http_chunk_splits\n    assert payload.is_eof()\n\n    assert len(messages) == 1\n    msg2, payload2 = messages[0]\n\n    assert msg2.method == \"POST\"\n    assert msg2.chunked\n    assert not payload2.is_eof()\n\n\ndef test_http_request_chunked_payload_chunks(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    parser.feed_data(b\"4\\r\\ndata\\r\")\n    parser.feed_data(b\"\\n4\")\n    parser.feed_data(b\"\\r\")\n    parser.feed_data(b\"\\n\")\n    parser.feed_data(b\"li\")\n    parser.feed_data(b\"ne\\r\\n0\\r\\n\")\n    parser.feed_data(b\"test: test\\r\\n\")\n\n    assert b\"dataline\" == b\"\".join(d for d in payload._buffer)\n    assert [4, 8] == payload._http_chunk_splits\n    assert not payload.is_eof()\n\n    parser.feed_data(b\"\\r\\n\")\n    assert b\"dataline\" == b\"\".join(d for d in payload._buffer)\n    assert [4, 8] == payload._http_chunk_splits\n    assert payload.is_eof()\n\n\ndef test_parse_chunked_payload_chunk_extension(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    parser.feed_data(b\"4;test\\r\\ndata\\r\\n4\\r\\nline\\r\\n0\\r\\ntest: test\\r\\n\\r\\n\")\n\n    assert b\"dataline\" == b\"\".join(d for d in payload._buffer)\n    assert [4, 8] == payload._http_chunk_splits\n    assert payload.is_eof()\n\n\ndef _test_parse_no_length_or_te_on_post(loop, protocol, request_cls):\n    parser = request_cls(protocol, loop, readall=True)\n    text = b\"POST /test HTTP/1.1\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    assert payload.is_eof()\n\n\ndef test_parse_payload_response_without_body(loop, protocol, response_cls) -> None:\n    parser = response_cls(protocol, loop, 2**16, response_with_body=False)\n    text = b\"HTTP/1.1 200 Ok\\r\\n\" b\"content-length: 10\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    assert payload.is_eof()\n\n\ndef test_parse_length_payload(response) -> None:\n    text = b\"HTTP/1.1 200 Ok\\r\\n\" b\"content-length: 4\\r\\n\\r\\n\"\n    msg, payload = response.feed_data(text)[0][0]\n    assert not payload.is_eof()\n\n    response.feed_data(b\"da\")\n    response.feed_data(b\"t\")\n    response.feed_data(b\"aHT\")\n\n    assert payload.is_eof()\n    assert b\"data\" == b\"\".join(d for d in payload._buffer)\n\n\ndef test_parse_no_length_payload(parser) -> None:\n    text = b\"PUT / HTTP/1.1\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n    assert payload.is_eof()\n\n\ndef test_partial_url(parser) -> None:\n    messages, upgrade, tail = parser.feed_data(b\"GET /te\")\n    assert len(messages) == 0\n    messages, upgrade, tail = parser.feed_data(b\"st HTTP/1.1\\r\\n\\r\\n\")\n    assert len(messages) == 1\n\n    msg, payload = messages[0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"/test\"\n    assert msg.version == (1, 1)\n    assert payload.is_eof()\n\n\n@pytest.mark.parametrize(\n    (\"uri\", \"path\", \"query\", \"fragment\"),\n    [\n        (\"/path%23frag\", \"/path#frag\", {}, \"\"),\n        (\"/path%2523frag\", \"/path%23frag\", {}, \"\"),\n        (\"/path?key=value%23frag\", \"/path\", {\"key\": \"value#frag\"}, \"\"),\n        (\"/path?key=value%2523frag\", \"/path\", {\"key\": \"value%23frag\"}, \"\"),\n        (\"/path#frag%20\", \"/path\", {}, \"frag \"),\n        (\"/path#frag%2520\", \"/path\", {}, \"frag%20\"),\n    ],\n)\ndef test_parse_uri_percent_encoded(parser, uri, path, query, fragment) -> None:\n    text = (f\"GET {uri} HTTP/1.1\\r\\n\\r\\n\").encode()\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n\n    assert msg.path == uri\n    assert msg.url == URL(uri)\n    assert msg.url.path == path\n    assert msg.url.query == query\n    assert msg.url.fragment == fragment\n\n\ndef test_parse_uri_utf8(parser) -> None:\n    if not isinstance(parser, HttpRequestParserPy):\n        pytest.xfail(\"Not valid HTTP. Maybe update py-parser to reject later.\")\n    text = (\"GET /\u043f\u0443\u0442\u044c?\u043a\u043b\u044e\u0447=\u0437\u043d\u0430\u0447#\u0444\u0440\u0430\u0433 HTTP/1.1\\r\\n\\r\\n\").encode()\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n\n    assert msg.path == \"/\u043f\u0443\u0442\u044c?\u043a\u043b\u044e\u0447=\u0437\u043d\u0430\u0447#\u0444\u0440\u0430\u0433\"\n    assert msg.url.path == \"/\u043f\u0443\u0442\u044c\"\n    assert msg.url.query == {\"\u043a\u043b\u044e\u0447\": \"\u0437\u043d\u0430\u0447\"}\n    assert msg.url.fragment == \"\u0444\u0440\u0430\u0433\"\n\n\ndef test_parse_uri_utf8_percent_encoded(parser) -> None:\n    text = (\n        \"GET %s HTTP/1.1\\r\\n\\r\\n\" % quote(\"/\u043f\u0443\u0442\u044c?\u043a\u043b\u044e\u0447=\u0437\u043d\u0430\u0447#\u0444\u0440\u0430\u0433\", safe=\"/?=#\")\n    ).encode()\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n\n    assert msg.path == quote(\"/\u043f\u0443\u0442\u044c?\u043a\u043b\u044e\u0447=\u0437\u043d\u0430\u0447#\u0444\u0440\u0430\u0433\", safe=\"/?=#\")\n    assert msg.url == URL(\"/\u043f\u0443\u0442\u044c?\u043a\u043b\u044e\u0447=\u0437\u043d\u0430\u0447#\u0444\u0440\u0430\u0433\")\n    assert msg.url.path == \"/\u043f\u0443\u0442\u044c\"\n    assert msg.url.query == {\"\u043a\u043b\u044e\u0447\": \"\u0437\u043d\u0430\u0447\"}\n    assert msg.url.fragment == \"\u0444\u0440\u0430\u0433\"\n\n\n@pytest.mark.skipif(\n    \"HttpRequestParserC\" not in dir(aiohttp.http_parser),\n    reason=\"C based HTTP parser not available\",\n)\ndef test_parse_bad_method_for_c_parser_raises(loop, protocol):\n    payload = b\"GET1 /test HTTP/1.1\\r\\n\\r\\n\"\n    parser = HttpRequestParserC(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_headers=32768,\n        max_field_size=8190,\n    )\n\n    with pytest.raises(aiohttp.http_exceptions.BadStatusLine):\n        messages, upgrade, tail = parser.feed_data(payload)\n\n\nclass TestParsePayload:\n    async def test_parse_eof_payload(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, readall=True)\n        p.feed_data(b\"data\")\n        p.feed_eof()\n\n        assert out.is_eof()\n        assert [(bytearray(b\"data\"), 4)] == list(out._buffer)\n\n    async def test_parse_no_body(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, method=\"PUT\")\n\n        assert out.is_eof()\n        assert p.done\n\n    async def test_parse_length_payload_eof(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n\n        p = HttpPayloadParser(out, length=4)\n        p.feed_data(b\"da\")\n\n        with pytest.raises(http_exceptions.ContentLengthError):\n            p.feed_eof()\n\n    async def test_parse_chunked_payload_size_error(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, chunked=True)\n        with pytest.raises(http_exceptions.TransferEncodingError):\n            p.feed_data(b\"blah\\r\\n\")\n        assert isinstance(out.exception(), http_exceptions.TransferEncodingError)\n\n    async def test_parse_chunked_payload_split_end(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\n\")\n        p.feed_data(b\"\\r\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_parse_chunked_payload_split_end2(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\n\\r\")\n        p.feed_data(b\"\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_parse_chunked_payload_split_end_trailers(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\n\")\n        p.feed_data(b\"Content-MD5: 912ec803b2ce49e4a541068d495ab570\\r\\n\")\n        p.feed_data(b\"\\r\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_parse_chunked_payload_split_end_trailers2(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\n\")\n        p.feed_data(b\"Content-MD5: 912ec803b2ce49e4a541068d495ab570\\r\\n\\r\")\n        p.feed_data(b\"\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_parse_chunked_payload_split_end_trailers3(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\nContent-MD5: \")\n        p.feed_data(b\"912ec803b2ce49e4a541068d495ab570\\r\\n\\r\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_parse_chunked_payload_split_end_trailers4(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\n\" b\"C\")\n        p.feed_data(b\"ontent-MD5: 912ec803b2ce49e4a541068d495ab570\\r\\n\\r\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_http_payload_parser_length(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=2)\n        eof, tail = p.feed_data(b\"1245\")\n        assert eof\n\n        assert b\"12\" == b\"\".join(d for d, _ in out._buffer)\n        assert b\"45\" == tail\n\n    async def test_http_payload_parser_deflate(self, stream) -> None:\n        # c=compressobj(wbits=15); b''.join([c.compress(b'data'), c.flush()])\n        COMPRESSED = b\"x\\x9cKI,I\\x04\\x00\\x04\\x00\\x01\\x9b\"\n\n        length = len(COMPRESSED)\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=length, compression=\"deflate\")\n        p.feed_data(COMPRESSED)\n        assert b\"data\" == b\"\".join(d for d, _ in out._buffer)\n        assert out.is_eof()\n\n    async def test_http_payload_parser_deflate_no_hdrs(self, stream: Any) -> None:\n        \"\"\"Tests incorrectly formed data (no zlib headers).\"\"\"\n        # c=compressobj(wbits=-15); b''.join([c.compress(b'data'), c.flush()])\n        COMPRESSED = b\"KI,I\\x04\\x00\"\n\n        length = len(COMPRESSED)\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=length, compression=\"deflate\")\n        p.feed_data(COMPRESSED)\n        assert b\"data\" == b\"\".join(d for d, _ in out._buffer)\n        assert out.is_eof()\n\n    async def test_http_payload_parser_deflate_light(self, stream) -> None:\n        # c=compressobj(wbits=9); b''.join([c.compress(b'data'), c.flush()])\n        COMPRESSED = b\"\\x18\\x95KI,I\\x04\\x00\\x04\\x00\\x01\\x9b\"\n\n        length = len(COMPRESSED)\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=length, compression=\"deflate\")\n        p.feed_data(COMPRESSED)\n        assert b\"data\" == b\"\".join(d for d, _ in out._buffer)\n        assert out.is_eof()\n\n    async def test_http_payload_parser_deflate_split(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, compression=\"deflate\", readall=True)\n        # Feeding one correct byte should be enough to choose exact\n        # deflate decompressor\n        p.feed_data(b\"x\", 1)\n        p.feed_data(b\"\\x9cKI,I\\x04\\x00\\x04\\x00\\x01\\x9b\", 11)\n        p.feed_eof()\n        assert b\"data\" == b\"\".join(d for d, _ in out._buffer)\n\n    async def test_http_payload_parser_deflate_split_err(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, compression=\"deflate\", readall=True)\n        # Feeding one wrong byte should be enough to choose exact\n        # deflate decompressor\n        p.feed_data(b\"K\", 1)\n        p.feed_data(b\"I,I\\x04\\x00\", 5)\n        p.feed_eof()\n        assert b\"data\" == b\"\".join(d for d, _ in out._buffer)\n\n    async def test_http_payload_parser_length_zero(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=0)\n        assert p.done\n        assert out.is_eof()\n\n    @pytest.mark.skipif(brotli is None, reason=\"brotli is not installed\")\n    async def test_http_payload_brotli(self, stream) -> None:\n        compressed = brotli.compress(b\"brotli data\")\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=len(compressed), compression=\"br\")\n        p.feed_data(compressed)\n        assert b\"brotli data\" == b\"\".join(d for d, _ in out._buffer)\n        assert out.is_eof()\n\n\nclass TestDeflateBuffer:\n    async def test_feed_data(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"deflate\")\n\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.decompress.return_value = b\"line\"\n\n        # First byte should be b'x' in order code not to change the decoder.\n        dbuf.feed_data(b\"xxxx\", 4)\n        assert [b\"line\"] == list(d for d, _ in buf._buffer)\n\n    async def test_feed_data_err(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"deflate\")\n\n        exc = ValueError()\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.decompress.side_effect = exc\n\n        with pytest.raises(http_exceptions.ContentEncodingError):\n            # Should be more than 4 bytes to trigger deflate FSM error.\n            # Should start with b'x', otherwise code switch mocked decoder.\n            dbuf.feed_data(b\"xsomedata\", 9)\n\n    async def test_feed_eof(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"deflate\")\n\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.flush.return_value = b\"line\"\n\n        dbuf.feed_eof()\n        assert [b\"line\"] == list(d for d, _ in buf._buffer)\n        assert buf._eof\n\n    async def test_feed_eof_err_deflate(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"deflate\")\n\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.flush.return_value = b\"line\"\n        dbuf.decompressor.eof = False\n\n        with pytest.raises(http_exceptions.ContentEncodingError):\n            dbuf.feed_eof()\n\n    async def test_feed_eof_no_err_gzip(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"gzip\")\n\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.flush.return_value = b\"line\"\n        dbuf.decompressor.eof = False\n\n        dbuf.feed_eof()\n        assert [b\"line\"] == list(d for d, _ in buf._buffer)\n\n    async def test_feed_eof_no_err_brotli(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"br\")\n\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.flush.return_value = b\"line\"\n        dbuf.decompressor.eof = False\n\n        dbuf.feed_eof()\n        assert [b\"line\"] == list(d for d, _ in buf._buffer)\n\n    async def test_empty_body(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"deflate\")\n        dbuf.feed_eof()\n\n        assert buf.at_eof()\n"], "fixing_code": ["import abc\nimport asyncio\nimport collections\nimport re\nimport string\nimport zlib\nfrom contextlib import suppress\nfrom enum import IntEnum\nfrom typing import (\n    Any,\n    Generic,\n    List,\n    NamedTuple,\n    Optional,\n    Pattern,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom multidict import CIMultiDict, CIMultiDictProxy, istr\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .base_protocol import BaseProtocol\nfrom .helpers import NO_EXTENSIONS, BaseTimerContext\nfrom .http_exceptions import (\n    BadHttpMessage,\n    BadStatusLine,\n    ContentEncodingError,\n    ContentLengthError,\n    InvalidHeader,\n    LineTooLong,\n    TransferEncodingError,\n)\nfrom .http_writer import HttpVersion, HttpVersion10\nfrom .log import internal_logger\nfrom .streams import EMPTY_PAYLOAD, StreamReader\nfrom .typedefs import Final, RawHeaders\n\ntry:\n    import brotli\n\n    HAS_BROTLI = True\nexcept ImportError:  # pragma: no cover\n    HAS_BROTLI = False\n\n\n__all__ = (\n    \"HeadersParser\",\n    \"HttpParser\",\n    \"HttpRequestParser\",\n    \"HttpResponseParser\",\n    \"RawRequestMessage\",\n    \"RawResponseMessage\",\n)\n\nASCIISET: Final[Set[str]] = set(string.printable)\n\n# See https://www.rfc-editor.org/rfc/rfc9110.html#name-overview\n# and https://www.rfc-editor.org/rfc/rfc9110.html#name-tokens\n#\n#     method = token\n#     tchar = \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\" / \"+\" / \"-\" / \".\" /\n#             \"^\" / \"_\" / \"`\" / \"|\" / \"~\" / DIGIT / ALPHA\n#     token = 1*tchar\nMETHRE: Final[Pattern[str]] = re.compile(r\"[!#$%&'*+\\-.^_`|~0-9A-Za-z]+\")\nVERSRE: Final[Pattern[str]] = re.compile(r\"HTTP/(\\d).(\\d)\")\nHDRRE: Final[Pattern[bytes]] = re.compile(rb\"[\\x00-\\x1F\\x7F()<>@,;:\\[\\]={} \\t\\\"\\\\]\")\n\n\nclass RawRequestMessage(NamedTuple):\n    method: str\n    path: str\n    version: HttpVersion\n    headers: \"CIMultiDictProxy[str]\"\n    raw_headers: RawHeaders\n    should_close: bool\n    compression: Optional[str]\n    upgrade: bool\n    chunked: bool\n    url: URL\n\n\nRawResponseMessage = collections.namedtuple(\n    \"RawResponseMessage\",\n    [\n        \"version\",\n        \"code\",\n        \"reason\",\n        \"headers\",\n        \"raw_headers\",\n        \"should_close\",\n        \"compression\",\n        \"upgrade\",\n        \"chunked\",\n    ],\n)\n\n\n_MsgT = TypeVar(\"_MsgT\", RawRequestMessage, RawResponseMessage)\n\n\nclass ParseState(IntEnum):\n\n    PARSE_NONE = 0\n    PARSE_LENGTH = 1\n    PARSE_CHUNKED = 2\n    PARSE_UNTIL_EOF = 3\n\n\nclass ChunkState(IntEnum):\n    PARSE_CHUNKED_SIZE = 0\n    PARSE_CHUNKED_CHUNK = 1\n    PARSE_CHUNKED_CHUNK_EOF = 2\n    PARSE_MAYBE_TRAILERS = 3\n    PARSE_TRAILERS = 4\n\n\nclass HeadersParser:\n    def __init__(\n        self,\n        max_line_size: int = 8190,\n        max_headers: int = 32768,\n        max_field_size: int = 8190,\n    ) -> None:\n        self.max_line_size = max_line_size\n        self.max_headers = max_headers\n        self.max_field_size = max_field_size\n\n    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\"CIMultiDictProxy[str]\", RawHeaders]:\n        headers: CIMultiDict[str] = CIMultiDict()\n        raw_headers = []\n\n        lines_idx = 1\n        line = lines[1]\n        line_count = len(lines)\n\n        while line:\n            # Parse initial header name : value pair.\n            try:\n                bname, bvalue = line.split(b\":\", 1)\n            except ValueError:\n                raise InvalidHeader(line) from None\n\n            # https://www.rfc-editor.org/rfc/rfc9112.html#section-5.1-2\n            if {bname[0], bname[-1]} & {32, 9}:  # {\" \", \"\\t\"}\n                raise InvalidHeader(line)\n\n            bvalue = bvalue.lstrip(b\" \\t\")\n            if HDRRE.search(bname):\n                raise InvalidHeader(bname)\n            if len(bname) > self.max_field_size:\n                raise LineTooLong(\n                    \"request header name {}\".format(\n                        bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                    ),\n                    str(self.max_field_size),\n                    str(len(bname)),\n                )\n\n            header_length = len(bvalue)\n\n            # next line\n            lines_idx += 1\n            line = lines[lines_idx]\n\n            # consume continuation lines\n            continuation = line and line[0] in (32, 9)  # (' ', '\\t')\n\n            # Deprecated: https://www.rfc-editor.org/rfc/rfc9112.html#name-obsolete-line-folding\n            if continuation:\n                bvalue_lst = [bvalue]\n                while continuation:\n                    header_length += len(line)\n                    if header_length > self.max_field_size:\n                        raise LineTooLong(\n                            \"request header field {}\".format(\n                                bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                            ),\n                            str(self.max_field_size),\n                            str(header_length),\n                        )\n                    bvalue_lst.append(line)\n\n                    # next line\n                    lines_idx += 1\n                    if lines_idx < line_count:\n                        line = lines[lines_idx]\n                        if line:\n                            continuation = line[0] in (32, 9)  # (' ', '\\t')\n                    else:\n                        line = b\"\"\n                        break\n                bvalue = b\"\".join(bvalue_lst)\n            else:\n                if header_length > self.max_field_size:\n                    raise LineTooLong(\n                        \"request header field {}\".format(\n                            bname.decode(\"utf8\", \"xmlcharrefreplace\")\n                        ),\n                        str(self.max_field_size),\n                        str(header_length),\n                    )\n\n            bvalue = bvalue.strip(b\" \\t\")\n            name = bname.decode(\"utf-8\", \"surrogateescape\")\n            value = bvalue.decode(\"utf-8\", \"surrogateescape\")\n\n            # https://www.rfc-editor.org/rfc/rfc9110.html#section-5.5-5\n            if \"\\n\" in value or \"\\r\" in value or \"\\x00\" in value:\n                raise InvalidHeader(bvalue)\n\n            headers.add(name, value)\n            raw_headers.append((bname, bvalue))\n\n        return (CIMultiDictProxy(headers), tuple(raw_headers))\n\n\nclass HttpParser(abc.ABC, Generic[_MsgT]):\n    def __init__(\n        self,\n        protocol: Optional[BaseProtocol] = None,\n        loop: Optional[asyncio.AbstractEventLoop] = None,\n        limit: int = 2**16,\n        max_line_size: int = 8190,\n        max_headers: int = 32768,\n        max_field_size: int = 8190,\n        timer: Optional[BaseTimerContext] = None,\n        code: Optional[int] = None,\n        method: Optional[str] = None,\n        readall: bool = False,\n        payload_exception: Optional[Type[BaseException]] = None,\n        response_with_body: bool = True,\n        read_until_eof: bool = False,\n        auto_decompress: bool = True,\n    ) -> None:\n        self.protocol = protocol\n        self.loop = loop\n        self.max_line_size = max_line_size\n        self.max_headers = max_headers\n        self.max_field_size = max_field_size\n        self.timer = timer\n        self.code = code\n        self.method = method\n        self.readall = readall\n        self.payload_exception = payload_exception\n        self.response_with_body = response_with_body\n        self.read_until_eof = read_until_eof\n\n        self._lines: List[bytes] = []\n        self._tail = b\"\"\n        self._upgraded = False\n        self._payload = None\n        self._payload_parser: Optional[HttpPayloadParser] = None\n        self._auto_decompress = auto_decompress\n        self._limit = limit\n        self._headers_parser = HeadersParser(max_line_size, max_headers, max_field_size)\n\n    @abc.abstractmethod\n    def parse_message(self, lines: List[bytes]) -> _MsgT:\n        pass\n\n    def feed_eof(self) -> Optional[_MsgT]:\n        if self._payload_parser is not None:\n            self._payload_parser.feed_eof()\n            self._payload_parser = None\n        else:\n            # try to extract partial message\n            if self._tail:\n                self._lines.append(self._tail)\n\n            if self._lines:\n                if self._lines[-1] != \"\\r\\n\":\n                    self._lines.append(b\"\")\n                with suppress(Exception):\n                    return self.parse_message(self._lines)\n        return None\n\n    def feed_data(\n        self,\n        data: bytes,\n        SEP: bytes = b\"\\r\\n\",\n        EMPTY: bytes = b\"\",\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\n        METH_CONNECT: str = hdrs.METH_CONNECT,\n        SEC_WEBSOCKET_KEY1: istr = hdrs.SEC_WEBSOCKET_KEY1,\n    ) -> Tuple[List[Tuple[_MsgT, StreamReader]], bool, bytes]:\n\n        messages = []\n\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n\n        data_len = len(data)\n        start_pos = 0\n        loop = self.loop\n\n        while start_pos < data_len:\n\n            # read HTTP message (request/response line + headers), \\r\\n\\r\\n\n            # and split by lines\n            if self._payload_parser is None and not self._upgraded:\n                pos = data.find(SEP, start_pos)\n                # consume \\r\\n\n                if pos == start_pos and not self._lines:\n                    start_pos = pos + 2\n                    continue\n\n                if pos >= start_pos:\n                    # line found\n                    self._lines.append(data[start_pos:pos])\n                    start_pos = pos + 2\n\n                    # \\r\\n\\r\\n found\n                    if self._lines[-1] == EMPTY:\n                        try:\n                            msg: _MsgT = self.parse_message(self._lines)\n                        finally:\n                            self._lines.clear()\n\n                        def get_content_length() -> Optional[int]:\n                            # payload length\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\n                            if length_hdr is None:\n                                return None\n\n                            # Shouldn't allow +/- or other number formats.\n                            # https://www.rfc-editor.org/rfc/rfc9110#section-8.6-2\n                            if not length_hdr.strip(\" \\t\").isdigit():\n                                raise InvalidHeader(CONTENT_LENGTH)\n\n                            return int(length_hdr)\n\n                        length = get_content_length()\n                        # do not support old websocket spec\n                        if SEC_WEBSOCKET_KEY1 in msg.headers:\n                            raise InvalidHeader(SEC_WEBSOCKET_KEY1)\n\n                        self._upgraded = msg.upgrade\n\n                        method = getattr(msg, \"method\", self.method)\n\n                        assert self.protocol is not None\n                        # calculate payload\n                        if (\n                            (length is not None and length > 0)\n                            or msg.chunked\n                            and not msg.upgrade\n                        ):\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            payload_parser = HttpPayloadParser(\n                                payload,\n                                length=length,\n                                chunked=msg.chunked,\n                                method=method,\n                                compression=msg.compression,\n                                code=self.code,\n                                readall=self.readall,\n                                response_with_body=self.response_with_body,\n                                auto_decompress=self._auto_decompress,\n                            )\n                            if not payload_parser.done:\n                                self._payload_parser = payload_parser\n                        elif method == METH_CONNECT:\n                            assert isinstance(msg, RawRequestMessage)\n                            payload = StreamReader(\n                                self.protocol,\n                                timer=self.timer,\n                                loop=loop,\n                                limit=self._limit,\n                            )\n                            self._upgraded = True\n                            self._payload_parser = HttpPayloadParser(\n                                payload,\n                                method=msg.method,\n                                compression=msg.compression,\n                                readall=True,\n                                auto_decompress=self._auto_decompress,\n                            )\n                        else:\n                            if (\n                                getattr(msg, \"code\", 100) >= 199\n                                and length is None\n                                and self.read_until_eof\n                            ):\n                                payload = StreamReader(\n                                    self.protocol,\n                                    timer=self.timer,\n                                    loop=loop,\n                                    limit=self._limit,\n                                )\n                                payload_parser = HttpPayloadParser(\n                                    payload,\n                                    length=length,\n                                    chunked=msg.chunked,\n                                    method=method,\n                                    compression=msg.compression,\n                                    code=self.code,\n                                    readall=True,\n                                    response_with_body=self.response_with_body,\n                                    auto_decompress=self._auto_decompress,\n                                )\n                                if not payload_parser.done:\n                                    self._payload_parser = payload_parser\n                            else:\n                                payload = EMPTY_PAYLOAD\n\n                        messages.append((msg, payload))\n                else:\n                    self._tail = data[start_pos:]\n                    data = EMPTY\n                    break\n\n            # no parser, just store\n            elif self._payload_parser is None and self._upgraded:\n                assert not self._lines\n                break\n\n            # feed payload\n            elif data and start_pos < data_len:\n                assert not self._lines\n                assert self._payload_parser is not None\n                try:\n                    eof, data = self._payload_parser.feed_data(data[start_pos:])\n                except BaseException as exc:\n                    if self.payload_exception is not None:\n                        self._payload_parser.payload.set_exception(\n                            self.payload_exception(str(exc))\n                        )\n                    else:\n                        self._payload_parser.payload.set_exception(exc)\n\n                    eof = True\n                    data = b\"\"\n\n                if eof:\n                    start_pos = 0\n                    data_len = len(data)\n                    self._payload_parser = None\n                    continue\n            else:\n                break\n\n        if data and start_pos < data_len:\n            data = data[start_pos:]\n        else:\n            data = EMPTY\n\n        return messages, self._upgraded, data\n\n    def parse_headers(\n        self, lines: List[bytes]\n    ) -> Tuple[\n        \"CIMultiDictProxy[str]\", RawHeaders, Optional[bool], Optional[str], bool, bool\n    ]:\n        \"\"\"Parses RFC 5322 headers from a stream.\n\n        Line continuations are supported. Returns list of header name\n        and value pairs. Header name is in upper case.\n        \"\"\"\n        headers, raw_headers = self._headers_parser.parse_headers(lines)\n        close_conn = None\n        encoding = None\n        upgrade = False\n        chunked = False\n\n        # https://www.rfc-editor.org/rfc/rfc9110.html#section-5.5-6\n        # https://www.rfc-editor.org/rfc/rfc9110.html#name-collected-abnf\n        singletons = (\n            hdrs.CONTENT_LENGTH,\n            hdrs.CONTENT_LOCATION,\n            hdrs.CONTENT_RANGE,\n            hdrs.CONTENT_TYPE,\n            hdrs.ETAG,\n            hdrs.HOST,\n            hdrs.MAX_FORWARDS,\n            hdrs.SERVER,\n            hdrs.TRANSFER_ENCODING,\n            hdrs.USER_AGENT,\n        )\n        bad_hdr = next((h for h in singletons if len(headers.getall(h, ())) > 1), None)\n        if bad_hdr is not None:\n            raise BadHttpMessage(f\"Duplicate '{bad_hdr}' header found.\")\n\n        # keep-alive\n        conn = headers.get(hdrs.CONNECTION)\n        if conn:\n            v = conn.lower()\n            if v == \"close\":\n                close_conn = True\n            elif v == \"keep-alive\":\n                close_conn = False\n            elif v == \"upgrade\":\n                upgrade = True\n\n        # encoding\n        enc = headers.get(hdrs.CONTENT_ENCODING)\n        if enc:\n            enc = enc.lower()\n            if enc in (\"gzip\", \"deflate\", \"br\"):\n                encoding = enc\n\n        # chunking\n        te = headers.get(hdrs.TRANSFER_ENCODING)\n        if te is not None:\n            if \"chunked\" == te.lower():\n                chunked = True\n            else:\n                raise BadHttpMessage(\"Request has invalid `Transfer-Encoding`\")\n\n            if hdrs.CONTENT_LENGTH in headers:\n                raise BadHttpMessage(\n                    \"Transfer-Encoding can't be present with Content-Length\",\n                )\n\n        return (headers, raw_headers, close_conn, encoding, upgrade, chunked)\n\n    def set_upgraded(self, val: bool) -> None:\n        \"\"\"Set connection upgraded (to websocket) mode.\n\n        :param bool val: new state.\n        \"\"\"\n        self._upgraded = val\n\n\nclass HttpRequestParser(HttpParser[RawRequestMessage]):\n    \"\"\"Read request status line.\n\n    Exception .http_exceptions.BadStatusLine\n    could be raised in case of any errors in status line.\n    Returns RawRequestMessage.\n    \"\"\"\n\n    def parse_message(self, lines: List[bytes]) -> RawRequestMessage:\n        # request line\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            method, path, version = line.split(maxsplit=2)\n        except ValueError:\n            raise BadStatusLine(line) from None\n\n        if len(path) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(path))\n            )\n\n        # method\n        if not METHRE.match(method):\n            raise BadStatusLine(method)\n\n        # version\n        match = VERSRE.match(version)\n        if match is None:\n            raise BadStatusLine(line)\n        version_o = HttpVersion(int(match.group(1)), int(match.group(2)))\n\n        if method == \"CONNECT\":\n            # authority-form,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.3\n            url = URL.build(authority=path, encoded=True)\n        elif path.startswith(\"/\"):\n            # origin-form,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.1\n            path_part, _hash_separator, url_fragment = path.partition(\"#\")\n            path_part, _question_mark_separator, qs_part = path_part.partition(\"?\")\n\n            # NOTE: `yarl.URL.build()` is used to mimic what the Cython-based\n            # NOTE: parser does, otherwise it results into the same\n            # NOTE: HTTP Request-Line input producing different\n            # NOTE: `yarl.URL()` objects\n            url = URL.build(\n                path=path_part,\n                query_string=qs_part,\n                fragment=url_fragment,\n                encoded=True,\n            )\n        else:\n            # absolute-form for proxy maybe,\n            # https://datatracker.ietf.org/doc/html/rfc7230#section-5.3.2\n            url = URL(path, encoded=True)\n\n        # read headers\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n\n        if close is None:  # then the headers weren't set in the request\n            if version_o <= HttpVersion10:  # HTTP 1.0 must asks to not close\n                close = True\n            else:  # HTTP 1.1 must ask to close.\n                close = False\n\n        return RawRequestMessage(\n            method,\n            path,\n            version_o,\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n            url,\n        )\n\n\nclass HttpResponseParser(HttpParser[RawResponseMessage]):\n    \"\"\"Read response status line and headers.\n\n    BadStatusLine could be raised in case of any errors in status line.\n    Returns RawResponseMessage.\n    \"\"\"\n\n    def parse_message(self, lines: List[bytes]) -> RawResponseMessage:\n        line = lines[0].decode(\"utf-8\", \"surrogateescape\")\n        try:\n            version, status = line.split(maxsplit=1)\n        except ValueError:\n            raise BadStatusLine(line) from None\n\n        try:\n            status, reason = status.split(maxsplit=1)\n        except ValueError:\n            reason = \"\"\n\n        if len(reason) > self.max_line_size:\n            raise LineTooLong(\n                \"Status line is too long\", str(self.max_line_size), str(len(reason))\n            )\n\n        # version\n        match = VERSRE.match(version)\n        if match is None:\n            raise BadStatusLine(line)\n        version_o = HttpVersion(int(match.group(1)), int(match.group(2)))\n\n        # The status code is a three-digit number\n        if len(status) != 3 or not status.isdigit():\n            raise BadStatusLine(line)\n        status_i = int(status)\n\n        # read headers\n        (\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        ) = self.parse_headers(lines)\n\n        if close is None:\n            close = version_o <= HttpVersion10\n\n        return RawResponseMessage(\n            version_o,\n            status_i,\n            reason.strip(),\n            headers,\n            raw_headers,\n            close,\n            compression,\n            upgrade,\n            chunked,\n        )\n\n\nclass HttpPayloadParser:\n    def __init__(\n        self,\n        payload: StreamReader,\n        length: Optional[int] = None,\n        chunked: bool = False,\n        compression: Optional[str] = None,\n        code: Optional[int] = None,\n        method: Optional[str] = None,\n        readall: bool = False,\n        response_with_body: bool = True,\n        auto_decompress: bool = True,\n    ) -> None:\n        self._length = 0\n        self._type = ParseState.PARSE_NONE\n        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\n        self._chunk_size = 0\n        self._chunk_tail = b\"\"\n        self._auto_decompress = auto_decompress\n        self.done = False\n\n        # payload decompression wrapper\n        if response_with_body and compression and self._auto_decompress:\n            real_payload: Union[StreamReader, DeflateBuffer] = DeflateBuffer(\n                payload, compression\n            )\n        else:\n            real_payload = payload\n\n        # payload parser\n        if not response_with_body:\n            # don't parse payload if it's not expected to be received\n            self._type = ParseState.PARSE_NONE\n            real_payload.feed_eof()\n            self.done = True\n\n        elif chunked:\n            self._type = ParseState.PARSE_CHUNKED\n        elif length is not None:\n            self._type = ParseState.PARSE_LENGTH\n            self._length = length\n            if self._length == 0:\n                real_payload.feed_eof()\n                self.done = True\n        else:\n            if readall and code != 204:\n                self._type = ParseState.PARSE_UNTIL_EOF\n            elif method in (\"PUT\", \"POST\"):\n                internal_logger.warning(  # pragma: no cover\n                    \"Content-Length or Transfer-Encoding header is required\"\n                )\n                self._type = ParseState.PARSE_NONE\n                real_payload.feed_eof()\n                self.done = True\n\n        self.payload = real_payload\n\n    def feed_eof(self) -> None:\n        if self._type == ParseState.PARSE_UNTIL_EOF:\n            self.payload.feed_eof()\n        elif self._type == ParseState.PARSE_LENGTH:\n            raise ContentLengthError(\n                \"Not enough data for satisfy content length header.\"\n            )\n        elif self._type == ParseState.PARSE_CHUNKED:\n            raise TransferEncodingError(\n                \"Not enough data for satisfy transfer length header.\"\n            )\n\n    def feed_data(\n        self, chunk: bytes, SEP: bytes = b\"\\r\\n\", CHUNK_EXT: bytes = b\";\"\n    ) -> Tuple[bool, bytes]:\n        # Read specified amount of bytes\n        if self._type == ParseState.PARSE_LENGTH:\n            required = self._length\n            chunk_len = len(chunk)\n\n            if required >= chunk_len:\n                self._length = required - chunk_len\n                self.payload.feed_data(chunk, chunk_len)\n                if self._length == 0:\n                    self.payload.feed_eof()\n                    return True, b\"\"\n            else:\n                self._length = 0\n                self.payload.feed_data(chunk[:required], required)\n                self.payload.feed_eof()\n                return True, chunk[required:]\n\n        # Chunked transfer encoding parser\n        elif self._type == ParseState.PARSE_CHUNKED:\n            if self._chunk_tail:\n                chunk = self._chunk_tail + chunk\n                self._chunk_tail = b\"\"\n\n            while chunk:\n\n                # read next chunk size\n                if self._chunk == ChunkState.PARSE_CHUNKED_SIZE:\n                    pos = chunk.find(SEP)\n                    if pos >= 0:\n                        i = chunk.find(CHUNK_EXT, 0, pos)\n                        if i >= 0:\n                            size_b = chunk[:i]  # strip chunk-extensions\n                        else:\n                            size_b = chunk[:pos]\n\n                        if not size_b.isdigit():\n                            exc = TransferEncodingError(\n                                chunk[:pos].decode(\"ascii\", \"surrogateescape\")\n                            )\n                            self.payload.set_exception(exc)\n                            raise exc\n                        size = int(bytes(size_b), 16)\n\n                        chunk = chunk[pos + 2 :]\n                        if size == 0:  # eof marker\n                            self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\n                        else:\n                            self._chunk = ChunkState.PARSE_CHUNKED_CHUNK\n                            self._chunk_size = size\n                            self.payload.begin_http_chunk_receiving()\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n                # read chunk and feed buffer\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK:\n                    required = self._chunk_size\n                    chunk_len = len(chunk)\n\n                    if required > chunk_len:\n                        self._chunk_size = required - chunk_len\n                        self.payload.feed_data(chunk, chunk_len)\n                        return False, b\"\"\n                    else:\n                        self._chunk_size = 0\n                        self.payload.feed_data(chunk[:required], required)\n                        chunk = chunk[required:]\n                        self._chunk = ChunkState.PARSE_CHUNKED_CHUNK_EOF\n                        self.payload.end_http_chunk_receiving()\n\n                # toss the CRLF at the end of the chunk\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK_EOF:\n                    if chunk[:2] == SEP:\n                        chunk = chunk[2:]\n                        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n                # if stream does not contain trailer, after 0\\r\\n\n                # we should get another \\r\\n otherwise\n                # trailers needs to be skiped until \\r\\n\\r\\n\n                if self._chunk == ChunkState.PARSE_MAYBE_TRAILERS:\n                    head = chunk[:2]\n                    if head == SEP:\n                        # end of stream\n                        self.payload.feed_eof()\n                        return True, chunk[2:]\n                    # Both CR and LF, or only LF may not be received yet. It is\n                    # expected that CRLF or LF will be shown at the very first\n                    # byte next time, otherwise trailers should come. The last\n                    # CRLF which marks the end of response might not be\n                    # contained in the same TCP segment which delivered the\n                    # size indicator.\n                    if not head:\n                        return False, b\"\"\n                    if head == SEP[:1]:\n                        self._chunk_tail = head\n                        return False, b\"\"\n                    self._chunk = ChunkState.PARSE_TRAILERS\n\n                # read and discard trailer up to the CRLF terminator\n                if self._chunk == ChunkState.PARSE_TRAILERS:\n                    pos = chunk.find(SEP)\n                    if pos >= 0:\n                        chunk = chunk[pos + 2 :]\n                        self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\n                    else:\n                        self._chunk_tail = chunk\n                        return False, b\"\"\n\n        # Read all bytes until eof\n        elif self._type == ParseState.PARSE_UNTIL_EOF:\n            self.payload.feed_data(chunk, len(chunk))\n\n        return False, b\"\"\n\n\nclass DeflateBuffer:\n    \"\"\"DeflateStream decompress stream and feed data into specified stream.\"\"\"\n\n    decompressor: Any\n\n    def __init__(self, out: StreamReader, encoding: Optional[str]) -> None:\n        self.out = out\n        self.size = 0\n        self.encoding = encoding\n        self._started_decoding = False\n\n        if encoding == \"br\":\n            if not HAS_BROTLI:  # pragma: no cover\n                raise ContentEncodingError(\n                    \"Can not decode content-encoding: brotli (br). \"\n                    \"Please install `Brotli`\"\n                )\n\n            class BrotliDecoder:\n                # Supports both 'brotlipy' and 'Brotli' packages\n                # since they share an import name. The top branches\n                # are for 'brotlipy' and bottom branches for 'Brotli'\n                def __init__(self) -> None:\n                    self._obj = brotli.Decompressor()\n\n                def decompress(self, data: bytes) -> bytes:\n                    if hasattr(self._obj, \"decompress\"):\n                        return cast(bytes, self._obj.decompress(data))\n                    return cast(bytes, self._obj.process(data))\n\n                def flush(self) -> bytes:\n                    if hasattr(self._obj, \"flush\"):\n                        return cast(bytes, self._obj.flush())\n                    return b\"\"\n\n            self.decompressor = BrotliDecoder()\n        else:\n            zlib_mode = 16 + zlib.MAX_WBITS if encoding == \"gzip\" else zlib.MAX_WBITS\n            self.decompressor = zlib.decompressobj(wbits=zlib_mode)\n\n    def set_exception(self, exc: BaseException) -> None:\n        self.out.set_exception(exc)\n\n    def feed_data(self, chunk: bytes, size: int) -> None:\n        if not size:\n            return\n\n        self.size += size\n\n        # RFC1950\n        # bits 0..3 = CM = 0b1000 = 8 = \"deflate\"\n        # bits 4..7 = CINFO = 1..7 = windows size.\n        if (\n            not self._started_decoding\n            and self.encoding == \"deflate\"\n            and chunk[0] & 0xF != 8\n        ):\n            # Change the decoder to decompress incorrectly compressed data\n            # Actually we should issue a warning about non-RFC-compliant data.\n            self.decompressor = zlib.decompressobj(wbits=-zlib.MAX_WBITS)\n\n        try:\n            chunk = self.decompressor.decompress(chunk)\n        except Exception:\n            raise ContentEncodingError(\n                \"Can not decode content-encoding: %s\" % self.encoding\n            )\n\n        self._started_decoding = True\n\n        if chunk:\n            self.out.feed_data(chunk, len(chunk))\n\n    def feed_eof(self) -> None:\n        chunk = self.decompressor.flush()\n\n        if chunk or self.size > 0:\n            self.out.feed_data(chunk, len(chunk))\n            if self.encoding == \"deflate\" and not self.decompressor.eof:\n                raise ContentEncodingError(\"deflate\")\n\n        self.out.feed_eof()\n\n    def begin_http_chunk_receiving(self) -> None:\n        self.out.begin_http_chunk_receiving()\n\n    def end_http_chunk_receiving(self) -> None:\n        self.out.end_http_chunk_receiving()\n\n\nHttpRequestParserPy = HttpRequestParser\nHttpResponseParserPy = HttpResponseParser\nRawRequestMessagePy = RawRequestMessage\nRawResponseMessagePy = RawResponseMessage\n\ntry:\n    if not NO_EXTENSIONS:\n        from ._http_parser import (  # type: ignore[import,no-redef]\n            HttpRequestParser,\n            HttpResponseParser,\n            RawRequestMessage,\n            RawResponseMessage,\n        )\n\n        HttpRequestParserC = HttpRequestParser\n        HttpResponseParserC = HttpResponseParser\n        RawRequestMessageC = RawRequestMessage\n        RawResponseMessageC = RawResponseMessage\nexcept ImportError:  # pragma: no cover\n    pass\n", "# Tests for aiohttp/protocol.py\n\nimport asyncio\nimport re\nfrom typing import Any, List\nfrom unittest import mock\nfrom urllib.parse import quote\n\nimport pytest\nfrom multidict import CIMultiDict\nfrom yarl import URL\n\nimport aiohttp\nfrom aiohttp import http_exceptions, streams\nfrom aiohttp.http_parser import (\n    NO_EXTENSIONS,\n    DeflateBuffer,\n    HttpPayloadParser,\n    HttpRequestParserPy,\n    HttpResponseParserPy,\n)\n\ntry:\n    import brotli\nexcept ImportError:\n    brotli = None\n\n\nREQUEST_PARSERS = [HttpRequestParserPy]\nRESPONSE_PARSERS = [HttpResponseParserPy]\n\ntry:\n    from aiohttp.http_parser import HttpRequestParserC, HttpResponseParserC\n\n    REQUEST_PARSERS.append(HttpRequestParserC)\n    RESPONSE_PARSERS.append(HttpResponseParserC)\nexcept ImportError:  # pragma: no cover\n    pass\n\n\n@pytest.fixture\ndef protocol():\n    return mock.Mock()\n\n\ndef _gen_ids(parsers: List[Any]) -> List[str]:\n    return [\n        \"py-parser\" if parser.__module__ == \"aiohttp.http_parser\" else \"c-parser\"\n        for parser in parsers\n    ]\n\n\n@pytest.fixture(params=REQUEST_PARSERS, ids=_gen_ids(REQUEST_PARSERS))\ndef parser(loop: Any, protocol: Any, request: Any):\n    # Parser implementations\n    return request.param(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_headers=32768,\n        max_field_size=8190,\n    )\n\n\n@pytest.fixture(params=REQUEST_PARSERS, ids=_gen_ids(REQUEST_PARSERS))\ndef request_cls(request: Any):\n    # Request Parser class\n    return request.param\n\n\n@pytest.fixture(params=RESPONSE_PARSERS, ids=_gen_ids(RESPONSE_PARSERS))\ndef response(loop: Any, protocol: Any, request: Any):\n    # Parser implementations\n    return request.param(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_headers=32768,\n        max_field_size=8190,\n    )\n\n\n@pytest.fixture(params=RESPONSE_PARSERS, ids=_gen_ids(RESPONSE_PARSERS))\ndef response_cls(request: Any):\n    # Parser implementations\n    return request.param\n\n\n@pytest.fixture\ndef stream():\n    return mock.Mock()\n\n\n@pytest.mark.skipif(NO_EXTENSIONS, reason=\"Extentions available but not imported\")\ndef test_c_parser_loaded():\n    assert \"HttpRequestParserC\" in dir(aiohttp.http_parser)\n    assert \"HttpResponseParserC\" in dir(aiohttp.http_parser)\n    assert \"RawRequestMessageC\" in dir(aiohttp.http_parser)\n    assert \"RawResponseMessageC\" in dir(aiohttp.http_parser)\n\n\ndef test_parse_headers(parser: Any) -> None:\n    text = b\"\"\"GET /test HTTP/1.1\\r\ntest: line\\r\n continue\\r\ntest2: data\\r\n\\r\n\"\"\"\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 1\n    msg = messages[0][0]\n\n    assert list(msg.headers.items()) == [(\"test\", \"line continue\"), (\"test2\", \"data\")]\n    assert msg.raw_headers == ((b\"test\", b\"line continue\"), (b\"test2\", b\"data\"))\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n\n\n@pytest.mark.skipif(NO_EXTENSIONS, reason=\"Only tests C parser.\")\ndef test_invalid_character(loop: Any, protocol: Any, request: Any) -> None:\n    parser = HttpRequestParserC(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_field_size=8190,\n    )\n    text = b\"POST / HTTP/1.1\\r\\nHost: localhost:8080\\r\\nSet-Cookie: abc\\x01def\\r\\n\\r\\n\"\n    error_detail = re.escape(\n        r\"\"\":\n\n    b'Set-Cookie: abc\\x01def'\n                     ^\"\"\"\n    )\n    with pytest.raises(http_exceptions.BadHttpMessage, match=error_detail):\n        parser.feed_data(text)\n\n\n@pytest.mark.skipif(NO_EXTENSIONS, reason=\"Only tests C parser.\")\ndef test_invalid_linebreak(loop: Any, protocol: Any, request: Any) -> None:\n    parser = HttpRequestParserC(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_field_size=8190,\n    )\n    text = b\"GET /world HTTP/1.1\\r\\nHost: 127.0.0.1\\n\\r\\n\"\n    error_detail = re.escape(\n        r\"\"\":\n\n    b'Host: 127.0.0.1\\n'\n                     ^\"\"\"\n    )\n    with pytest.raises(http_exceptions.BadHttpMessage, match=error_detail):\n        parser.feed_data(text)\n\n\ndef test_parse(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 1\n    msg, _ = messages[0]\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert msg.method == \"GET\"\n    assert msg.path == \"/test\"\n    assert msg.version == (1, 1)\n\n\nasync def test_parse_body(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\nContent-Length: 4\\r\\n\\r\\nbody\"\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 1\n    _, payload = messages[0]\n    body = await payload.read(4)\n    assert body == b\"body\"\n\n\nasync def test_parse_body_with_CRLF(parser) -> None:\n    text = b\"\\r\\nGET /test HTTP/1.1\\r\\nContent-Length: 4\\r\\n\\r\\nbody\"\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 1\n    _, payload = messages[0]\n    body = await payload.read(4)\n    assert body == b\"body\"\n\n\ndef test_parse_delayed(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 0\n    assert not upgrade\n\n    messages, upgrade, tail = parser.feed_data(b\"\\r\\n\")\n    assert len(messages) == 1\n    msg = messages[0][0]\n    assert msg.method == \"GET\"\n\n\ndef test_headers_multi_feed(parser) -> None:\n    text1 = b\"GET /test HTTP/1.1\\r\\n\"\n    text2 = b\"test: line\\r\"\n    text3 = b\"\\n continue\\r\\n\\r\\n\"\n\n    messages, upgrade, tail = parser.feed_data(text1)\n    assert len(messages) == 0\n\n    messages, upgrade, tail = parser.feed_data(text2)\n    assert len(messages) == 0\n\n    messages, upgrade, tail = parser.feed_data(text3)\n    assert len(messages) == 1\n\n    msg = messages[0][0]\n    assert list(msg.headers.items()) == [(\"test\", \"line continue\")]\n    assert msg.raw_headers == ((b\"test\", b\"line continue\"),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n\n\ndef test_headers_split_field(parser) -> None:\n    text1 = b\"GET /test HTTP/1.1\\r\\n\"\n    text2 = b\"t\"\n    text3 = b\"es\"\n    text4 = b\"t: value\\r\\n\\r\\n\"\n\n    messages, upgrade, tail = parser.feed_data(text1)\n    messages, upgrade, tail = parser.feed_data(text2)\n    messages, upgrade, tail = parser.feed_data(text3)\n    assert len(messages) == 0\n    messages, upgrade, tail = parser.feed_data(text4)\n    assert len(messages) == 1\n\n    msg = messages[0][0]\n    assert list(msg.headers.items()) == [(\"test\", \"value\")]\n    assert msg.raw_headers == ((b\"test\", b\"value\"),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n\n\ndef test_parse_headers_multi(parser) -> None:\n    text = (\n        b\"GET /test HTTP/1.1\\r\\n\"\n        b\"Set-Cookie: c1=cookie1\\r\\n\"\n        b\"Set-Cookie: c2=cookie2\\r\\n\\r\\n\"\n    )\n\n    messages, upgrade, tail = parser.feed_data(text)\n    assert len(messages) == 1\n    msg = messages[0][0]\n\n    assert list(msg.headers.items()) == [\n        (\"Set-Cookie\", \"c1=cookie1\"),\n        (\"Set-Cookie\", \"c2=cookie2\"),\n    ]\n    assert msg.raw_headers == (\n        (b\"Set-Cookie\", b\"c1=cookie1\"),\n        (b\"Set-Cookie\", b\"c2=cookie2\"),\n    )\n    assert not msg.should_close\n    assert msg.compression is None\n\n\ndef test_conn_default_1_0(parser) -> None:\n    text = b\"GET /test HTTP/1.0\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.should_close\n\n\ndef test_conn_default_1_1(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n\n\ndef test_conn_close(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"connection: close\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.should_close\n\n\ndef test_conn_close_1_0(parser) -> None:\n    text = b\"GET /test HTTP/1.0\\r\\n\" b\"connection: close\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.should_close\n\n\ndef test_conn_keep_alive_1_0(parser) -> None:\n    text = b\"GET /test HTTP/1.0\\r\\n\" b\"connection: keep-alive\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n\n\ndef test_conn_keep_alive_1_1(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"connection: keep-alive\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n\n\ndef test_conn_other_1_0(parser) -> None:\n    text = b\"GET /test HTTP/1.0\\r\\n\" b\"connection: test\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.should_close\n\n\ndef test_conn_other_1_1(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"connection: test\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n\n\ndef test_request_chunked(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg, payload = messages[0]\n    assert msg.chunked\n    assert not upgrade\n    assert isinstance(payload, streams.StreamReader)\n\n\ndef test_request_te_chunked_with_content_length(parser: Any) -> None:\n    text = (\n        b\"GET /test HTTP/1.1\\r\\n\"\n        b\"content-length: 1234\\r\\n\"\n        b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    )\n    with pytest.raises(\n        http_exceptions.BadHttpMessage,\n        match=\"Transfer-Encoding can't be present with Content-Length\",\n    ):\n        parser.feed_data(text)\n\n\ndef test_request_te_chunked123(parser: Any) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked123\\r\\n\\r\\n\"\n    with pytest.raises(\n        http_exceptions.BadHttpMessage,\n        match=\"Request has invalid `Transfer-Encoding`\",\n    ):\n        parser.feed_data(text)\n\n\ndef test_conn_upgrade(parser: Any) -> None:\n    text = (\n        b\"GET /test HTTP/1.1\\r\\n\"\n        b\"connection: upgrade\\r\\n\"\n        b\"upgrade: websocket\\r\\n\\r\\n\"\n    )\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n    assert msg.upgrade\n    assert upgrade\n\n\ndef test_compression_empty(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-encoding: \\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.compression is None\n\n\ndef test_compression_deflate(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-encoding: deflate\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.compression == \"deflate\"\n\n\ndef test_compression_gzip(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-encoding: gzip\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.compression == \"gzip\"\n\n\n@pytest.mark.skipif(brotli is None, reason=\"brotli is not installed\")\ndef test_compression_brotli(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-encoding: br\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.compression == \"br\"\n\n\ndef test_compression_unknown(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-encoding: compress\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.compression is None\n\n\ndef test_url_connect(parser: Any) -> None:\n    text = b\"CONNECT www.google.com HTTP/1.1\\r\\n\" b\"content-length: 0\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg, payload = messages[0]\n    assert upgrade\n    assert msg.url == URL.build(authority=\"www.google.com\")\n\n\ndef test_headers_connect(parser: Any) -> None:\n    text = b\"CONNECT www.google.com HTTP/1.1\\r\\n\" b\"content-length: 0\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg, payload = messages[0]\n    assert upgrade\n    assert isinstance(payload, streams.StreamReader)\n\n\ndef test_url_absolute(parser: Any) -> None:\n    text = (\n        b\"GET https://www.google.com/path/to.html HTTP/1.1\\r\\n\"\n        b\"content-length: 0\\r\\n\\r\\n\"\n    )\n    messages, upgrade, tail = parser.feed_data(text)\n    msg, payload = messages[0]\n    assert not upgrade\n    assert msg.method == \"GET\"\n    assert msg.url == URL(\"https://www.google.com/path/to.html\")\n\n\ndef test_headers_old_websocket_key1(parser: Any) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"SEC-WEBSOCKET-KEY1: line\\r\\n\\r\\n\"\n\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\ndef test_headers_content_length_err_1(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-length: line\\r\\n\\r\\n\"\n\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\ndef test_headers_content_length_err_2(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"content-length: -1\\r\\n\\r\\n\"\n\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\ndef test_invalid_header(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"test line\\r\\n\\r\\n\"\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\ndef test_invalid_name(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"test[]: line\\r\\n\\r\\n\"\n\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\ndef test_cve_2023_37276(parser: Any) -> None:\n    text = b\"\"\"POST / HTTP/1.1\\r\\nHost: localhost:8080\\r\\nX-Abc: \\rxTransfer-Encoding: chunked\\r\\n\\r\\n\"\"\"\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\n@pytest.mark.parametrize(\n    \"hdr\",\n    (\n        \"Content-Length: -5\",  # https://www.rfc-editor.org/rfc/rfc9110.html#name-content-length\n        \"Content-Length: +256\",\n        \"Foo: abc\\rdef\",  # https://www.rfc-editor.org/rfc/rfc9110.html#section-5.5-5\n        \"Bar: abc\\ndef\",\n        \"Baz: abc\\x00def\",\n        \"Foo : bar\",  # https://www.rfc-editor.org/rfc/rfc9112.html#section-5.1-2\n        \"Foo\\t: bar\",\n    ),\n)\ndef test_bad_headers(parser: Any, hdr: str) -> None:\n    text = f\"POST / HTTP/1.1\\r\\n{hdr}\\r\\n\\r\\n\".encode()\n    with pytest.raises(http_exceptions.InvalidHeader):\n        parser.feed_data(text)\n\n\ndef test_bad_chunked_py(loop: Any, protocol: Any) -> None:\n    \"\"\"Test that invalid chunked encoding doesn't allow content-length to be used.\"\"\"\n    parser = HttpRequestParserPy(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_field_size=8190,\n    )\n    text = (\n        b\"GET / HTTP/1.1\\r\\nHost: a\\r\\nTransfer-Encoding: chunked\\r\\n\\r\\n0_2e\\r\\n\\r\\n\"\n        + b\"GET / HTTP/1.1\\r\\nHost: a\\r\\nContent-Length: 5\\r\\n\\r\\n0\\r\\n\\r\\n\"\n    )\n    messages, upgrade, tail = parser.feed_data(text)\n    assert isinstance(messages[0][1].exception(), http_exceptions.TransferEncodingError)\n\n\n@pytest.mark.skipif(\n    \"HttpRequestParserC\" not in dir(aiohttp.http_parser),\n    reason=\"C based HTTP parser not available\",\n)\ndef test_bad_chunked_c(loop: Any, protocol: Any) -> None:\n    \"\"\"C parser behaves differently. Maybe we should align them later.\"\"\"\n    parser = HttpRequestParserC(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_field_size=8190,\n    )\n    text = (\n        b\"GET / HTTP/1.1\\r\\nHost: a\\r\\nTransfer-Encoding: chunked\\r\\n\\r\\n0_2e\\r\\n\\r\\n\"\n        + b\"GET / HTTP/1.1\\r\\nHost: a\\r\\nContent-Length: 5\\r\\n\\r\\n0\\r\\n\\r\\n\"\n    )\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\ndef test_whitespace_before_header(parser: Any) -> None:\n    text = b\"GET / HTTP/1.1\\r\\n\\tContent-Length: 1\\r\\n\\r\\nX\"\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(text)\n\n\n@pytest.mark.parametrize(\"size\", [40960, 8191])\ndef test_max_header_field_size(parser, size) -> None:\n    name = b\"t\" * size\n    text = b\"GET /test HTTP/1.1\\r\\n\" + name + b\":data\\r\\n\\r\\n\"\n\n    match = f\"400, message:\\n  Got more than 8190 bytes \\\\({size}\\\\) when reading\"\n    with pytest.raises(http_exceptions.LineTooLong, match=match):\n        parser.feed_data(text)\n\n\ndef test_max_header_field_size_under_limit(parser) -> None:\n    name = b\"t\" * 8190\n    text = b\"GET /test HTTP/1.1\\r\\n\" + name + b\":data\\r\\n\\r\\n\"\n\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.method == \"GET\"\n    assert msg.path == \"/test\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict({name.decode(): \"data\"})\n    assert msg.raw_headers == ((name, b\"data\"),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/test\")\n\n\n@pytest.mark.parametrize(\"size\", [40960, 8191])\ndef test_max_header_value_size(parser, size) -> None:\n    name = b\"t\" * size\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"data:\" + name + b\"\\r\\n\\r\\n\"\n\n    match = f\"400, message:\\n  Got more than 8190 bytes \\\\({size}\\\\) when reading\"\n    with pytest.raises(http_exceptions.LineTooLong, match=match):\n        parser.feed_data(text)\n\n\ndef test_max_header_value_size_under_limit(parser) -> None:\n    value = b\"A\" * 8190\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"data:\" + value + b\"\\r\\n\\r\\n\"\n\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.method == \"GET\"\n    assert msg.path == \"/test\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict({\"data\": value.decode()})\n    assert msg.raw_headers == ((b\"data\", value),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/test\")\n\n\n@pytest.mark.parametrize(\"size\", [40965, 8191])\ndef test_max_header_value_size_continuation(parser, size) -> None:\n    name = b\"T\" * (size - 5)\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"data: test\\r\\n \" + name + b\"\\r\\n\\r\\n\"\n\n    match = f\"400, message:\\n  Got more than 8190 bytes \\\\({size}\\\\) when reading\"\n    with pytest.raises(http_exceptions.LineTooLong, match=match):\n        parser.feed_data(text)\n\n\ndef test_max_header_value_size_continuation_under_limit(parser) -> None:\n    value = b\"A\" * 8185\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"data: test\\r\\n \" + value + b\"\\r\\n\\r\\n\"\n\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert msg.method == \"GET\"\n    assert msg.path == \"/test\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict({\"data\": \"test \" + value.decode()})\n    assert msg.raw_headers == ((b\"data\", b\"test \" + value),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/test\")\n\n\ndef test_http_request_parser(parser) -> None:\n    text = b\"GET /path HTTP/1.1\\r\\n\\r\\n\"\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"/path\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict()\n    assert msg.raw_headers == ()\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/path\")\n\n\ndef test_http_request_bad_status_line(parser) -> None:\n    text = b\"getpath \\r\\n\\r\\n\"\n    with pytest.raises(http_exceptions.BadStatusLine):\n        parser.feed_data(text)\n\n\ndef test_http_request_upgrade(parser) -> None:\n    text = (\n        b\"GET /test HTTP/1.1\\r\\n\"\n        b\"connection: upgrade\\r\\n\"\n        b\"upgrade: websocket\\r\\n\\r\\n\"\n        b\"some raw data\"\n    )\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n    assert not msg.should_close\n    assert msg.upgrade\n    assert upgrade\n    assert tail == b\"some raw data\"\n\n\ndef test_http_request_parser_utf8(parser) -> None:\n    text = \"GET /path HTTP/1.1\\r\\nx-test:\u0442\u0435\u0441\u0442\\r\\n\\r\\n\".encode()\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"/path\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict([(\"X-TEST\", \"\u0442\u0435\u0441\u0442\")])\n    assert msg.raw_headers == ((b\"x-test\", \"\u0442\u0435\u0441\u0442\".encode()),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/path\")\n\n\ndef test_http_request_parser_non_utf8(parser) -> None:\n    text = \"GET /path HTTP/1.1\\r\\nx-test:\u0442\u0435\u0441\u0442\\r\\n\\r\\n\".encode(\"cp1251\")\n    msg = parser.feed_data(text)[0][0][0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"/path\"\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict(\n        [(\"X-TEST\", \"\u0442\u0435\u0441\u0442\".encode(\"cp1251\").decode(\"utf8\", \"surrogateescape\"))]\n    )\n    assert msg.raw_headers == ((b\"x-test\", \"\u0442\u0435\u0441\u0442\".encode(\"cp1251\")),)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/path\")\n\n\ndef test_http_request_parser_two_slashes(parser) -> None:\n    text = b\"GET //path HTTP/1.1\\r\\n\\r\\n\"\n    msg = parser.feed_data(text)[0][0][0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"//path\"\n    assert msg.url.path == \"//path\"\n    assert msg.version == (1, 1)\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n\n\ndef test_http_request_parser_bad_method(parser) -> None:\n    with pytest.raises(http_exceptions.BadStatusLine):\n        parser.feed_data(b'=\":<G>(e),[T];?\" /get HTTP/1.1\\r\\n\\r\\n')\n\n\ndef test_http_request_parser_bad_version(parser) -> None:\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(b\"GET //get HT/11\\r\\n\\r\\n\")\n\n\ndef test_http_request_parser_bad_version_number(parser: Any) -> None:\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        parser.feed_data(b\"GET /test HTTP/12.3\\r\\n\\r\\n\")\n\n\n@pytest.mark.parametrize(\"size\", [40965, 8191])\ndef test_http_request_max_status_line(parser, size) -> None:\n    path = b\"t\" * (size - 5)\n    match = f\"400, message:\\n  Got more than 8190 bytes \\\\({size}\\\\) when reading\"\n    with pytest.raises(http_exceptions.LineTooLong, match=match):\n        parser.feed_data(b\"GET /path\" + path + b\" HTTP/1.1\\r\\n\\r\\n\")\n\n\ndef test_http_request_max_status_line_under_limit(parser) -> None:\n    path = b\"t\" * (8190 - 5)\n    messages, upgraded, tail = parser.feed_data(\n        b\"GET /path\" + path + b\" HTTP/1.1\\r\\n\\r\\n\"\n    )\n    msg = messages[0][0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"/path\" + path.decode()\n    assert msg.version == (1, 1)\n    assert msg.headers == CIMultiDict()\n    assert msg.raw_headers == ()\n    assert not msg.should_close\n    assert msg.compression is None\n    assert not msg.upgrade\n    assert not msg.chunked\n    assert msg.url == URL(\"/path\" + path.decode())\n\n\ndef test_http_response_parser_utf8(response) -> None:\n    text = \"HTTP/1.1 200 Ok\\r\\nx-test:\u0442\u0435\u0441\u0442\\r\\n\\r\\n\".encode()\n\n    messages, upgraded, tail = response.feed_data(text)\n    assert len(messages) == 1\n    msg = messages[0][0]\n\n    assert msg.version == (1, 1)\n    assert msg.code == 200\n    assert msg.reason == \"Ok\"\n    assert msg.headers == CIMultiDict([(\"X-TEST\", \"\u0442\u0435\u0441\u0442\")])\n    assert msg.raw_headers == ((b\"x-test\", \"\u0442\u0435\u0441\u0442\".encode()),)\n    assert not upgraded\n    assert not tail\n\n\n@pytest.mark.parametrize(\"size\", [40962, 8191])\ndef test_http_response_parser_bad_status_line_too_long(response, size) -> None:\n    reason = b\"t\" * (size - 2)\n    match = f\"400, message:\\n  Got more than 8190 bytes \\\\({size}\\\\) when reading\"\n    with pytest.raises(http_exceptions.LineTooLong, match=match):\n        response.feed_data(b\"HTTP/1.1 200 Ok\" + reason + b\"\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_status_line_under_limit(response) -> None:\n    reason = b\"O\" * 8190\n    messages, upgraded, tail = response.feed_data(\n        b\"HTTP/1.1 200 \" + reason + b\"\\r\\n\\r\\n\"\n    )\n    msg = messages[0][0]\n    assert msg.version == (1, 1)\n    assert msg.code == 200\n    assert msg.reason == reason.decode()\n\n\ndef test_http_response_parser_bad_version(response) -> None:\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        response.feed_data(b\"HT/11 200 Ok\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_bad_version_number(response) -> None:\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        response.feed_data(b\"HTTP/12.3 200 Ok\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_no_reason(response) -> None:\n    msg = response.feed_data(b\"HTTP/1.1 200\\r\\n\\r\\n\")[0][0][0]\n\n    assert msg.version == (1, 1)\n    assert msg.code == 200\n    assert msg.reason == \"\"\n\n\ndef test_http_response_parser_lenient_headers(response) -> None:\n    messages, upgrade, tail = response.feed_data(\n        b\"HTTP/1.1 200 test\\r\\nFoo: abc\\x01def\\r\\n\\r\\n\"\n    )\n    msg = messages[0][0]\n\n    assert msg.headers[\"Foo\"] == \"abc\\x01def\"\n\n\n@pytest.mark.dev_mode\ndef test_http_response_parser_strict_headers(response) -> None:\n    if isinstance(response, HttpResponseParserPy):\n        pytest.xfail(\"Py parser is lenient. May update py-parser later.\")\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        response.feed_data(b\"HTTP/1.1 200 test\\r\\nFoo: abc\\x01def\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_bad(response) -> None:\n    with pytest.raises(http_exceptions.BadHttpMessage):\n        response.feed_data(b\"HTT/1\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_code_under_100(response) -> None:\n    with pytest.raises(http_exceptions.BadStatusLine):\n        response.feed_data(b\"HTTP/1.1 99 test\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_code_above_999(response) -> None:\n    with pytest.raises(http_exceptions.BadStatusLine):\n        response.feed_data(b\"HTTP/1.1 9999 test\\r\\n\\r\\n\")\n\n\ndef test_http_response_parser_code_not_int(response) -> None:\n    with pytest.raises(http_exceptions.BadStatusLine):\n        response.feed_data(b\"HTTP/1.1 ttt test\\r\\n\\r\\n\")\n\n\ndef test_http_request_chunked_payload(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    assert msg.chunked\n    assert not payload.is_eof()\n    assert isinstance(payload, streams.StreamReader)\n\n    parser.feed_data(b\"4\\r\\ndata\\r\\n4\\r\\nline\\r\\n0\\r\\n\\r\\n\")\n\n    assert b\"dataline\" == b\"\".join(d for d in payload._buffer)\n    assert [4, 8] == payload._http_chunk_splits\n    assert payload.is_eof()\n\n\ndef test_http_request_chunked_payload_and_next_message(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    messages, upgraded, tail = parser.feed_data(\n        b\"4\\r\\ndata\\r\\n4\\r\\nline\\r\\n0\\r\\n\\r\\n\"\n        b\"POST /test2 HTTP/1.1\\r\\n\"\n        b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    )\n\n    assert b\"dataline\" == b\"\".join(d for d in payload._buffer)\n    assert [4, 8] == payload._http_chunk_splits\n    assert payload.is_eof()\n\n    assert len(messages) == 1\n    msg2, payload2 = messages[0]\n\n    assert msg2.method == \"POST\"\n    assert msg2.chunked\n    assert not payload2.is_eof()\n\n\ndef test_http_request_chunked_payload_chunks(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    parser.feed_data(b\"4\\r\\ndata\\r\")\n    parser.feed_data(b\"\\n4\")\n    parser.feed_data(b\"\\r\")\n    parser.feed_data(b\"\\n\")\n    parser.feed_data(b\"li\")\n    parser.feed_data(b\"ne\\r\\n0\\r\\n\")\n    parser.feed_data(b\"test: test\\r\\n\")\n\n    assert b\"dataline\" == b\"\".join(d for d in payload._buffer)\n    assert [4, 8] == payload._http_chunk_splits\n    assert not payload.is_eof()\n\n    parser.feed_data(b\"\\r\\n\")\n    assert b\"dataline\" == b\"\".join(d for d in payload._buffer)\n    assert [4, 8] == payload._http_chunk_splits\n    assert payload.is_eof()\n\n\ndef test_parse_chunked_payload_chunk_extension(parser) -> None:\n    text = b\"GET /test HTTP/1.1\\r\\n\" b\"transfer-encoding: chunked\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    parser.feed_data(b\"4;test\\r\\ndata\\r\\n4\\r\\nline\\r\\n0\\r\\ntest: test\\r\\n\\r\\n\")\n\n    assert b\"dataline\" == b\"\".join(d for d in payload._buffer)\n    assert [4, 8] == payload._http_chunk_splits\n    assert payload.is_eof()\n\n\ndef _test_parse_no_length_or_te_on_post(loop, protocol, request_cls):\n    parser = request_cls(protocol, loop, readall=True)\n    text = b\"POST /test HTTP/1.1\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    assert payload.is_eof()\n\n\ndef test_parse_payload_response_without_body(loop, protocol, response_cls) -> None:\n    parser = response_cls(protocol, loop, 2**16, response_with_body=False)\n    text = b\"HTTP/1.1 200 Ok\\r\\n\" b\"content-length: 10\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n\n    assert payload.is_eof()\n\n\ndef test_parse_length_payload(response) -> None:\n    text = b\"HTTP/1.1 200 Ok\\r\\n\" b\"content-length: 4\\r\\n\\r\\n\"\n    msg, payload = response.feed_data(text)[0][0]\n    assert not payload.is_eof()\n\n    response.feed_data(b\"da\")\n    response.feed_data(b\"t\")\n    response.feed_data(b\"aHT\")\n\n    assert payload.is_eof()\n    assert b\"data\" == b\"\".join(d for d in payload._buffer)\n\n\ndef test_parse_no_length_payload(parser) -> None:\n    text = b\"PUT / HTTP/1.1\\r\\n\\r\\n\"\n    msg, payload = parser.feed_data(text)[0][0]\n    assert payload.is_eof()\n\n\ndef test_partial_url(parser) -> None:\n    messages, upgrade, tail = parser.feed_data(b\"GET /te\")\n    assert len(messages) == 0\n    messages, upgrade, tail = parser.feed_data(b\"st HTTP/1.1\\r\\n\\r\\n\")\n    assert len(messages) == 1\n\n    msg, payload = messages[0]\n\n    assert msg.method == \"GET\"\n    assert msg.path == \"/test\"\n    assert msg.version == (1, 1)\n    assert payload.is_eof()\n\n\n@pytest.mark.parametrize(\n    (\"uri\", \"path\", \"query\", \"fragment\"),\n    [\n        (\"/path%23frag\", \"/path#frag\", {}, \"\"),\n        (\"/path%2523frag\", \"/path%23frag\", {}, \"\"),\n        (\"/path?key=value%23frag\", \"/path\", {\"key\": \"value#frag\"}, \"\"),\n        (\"/path?key=value%2523frag\", \"/path\", {\"key\": \"value%23frag\"}, \"\"),\n        (\"/path#frag%20\", \"/path\", {}, \"frag \"),\n        (\"/path#frag%2520\", \"/path\", {}, \"frag%20\"),\n    ],\n)\ndef test_parse_uri_percent_encoded(parser, uri, path, query, fragment) -> None:\n    text = (f\"GET {uri} HTTP/1.1\\r\\n\\r\\n\").encode()\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n\n    assert msg.path == uri\n    assert msg.url == URL(uri)\n    assert msg.url.path == path\n    assert msg.url.query == query\n    assert msg.url.fragment == fragment\n\n\ndef test_parse_uri_utf8(parser) -> None:\n    if not isinstance(parser, HttpRequestParserPy):\n        pytest.xfail(\"Not valid HTTP. Maybe update py-parser to reject later.\")\n    text = (\"GET /\u043f\u0443\u0442\u044c?\u043a\u043b\u044e\u0447=\u0437\u043d\u0430\u0447#\u0444\u0440\u0430\u0433 HTTP/1.1\\r\\n\\r\\n\").encode()\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n\n    assert msg.path == \"/\u043f\u0443\u0442\u044c?\u043a\u043b\u044e\u0447=\u0437\u043d\u0430\u0447#\u0444\u0440\u0430\u0433\"\n    assert msg.url.path == \"/\u043f\u0443\u0442\u044c\"\n    assert msg.url.query == {\"\u043a\u043b\u044e\u0447\": \"\u0437\u043d\u0430\u0447\"}\n    assert msg.url.fragment == \"\u0444\u0440\u0430\u0433\"\n\n\ndef test_parse_uri_utf8_percent_encoded(parser) -> None:\n    text = (\n        \"GET %s HTTP/1.1\\r\\n\\r\\n\" % quote(\"/\u043f\u0443\u0442\u044c?\u043a\u043b\u044e\u0447=\u0437\u043d\u0430\u0447#\u0444\u0440\u0430\u0433\", safe=\"/?=#\")\n    ).encode()\n    messages, upgrade, tail = parser.feed_data(text)\n    msg = messages[0][0]\n\n    assert msg.path == quote(\"/\u043f\u0443\u0442\u044c?\u043a\u043b\u044e\u0447=\u0437\u043d\u0430\u0447#\u0444\u0440\u0430\u0433\", safe=\"/?=#\")\n    assert msg.url == URL(\"/\u043f\u0443\u0442\u044c?\u043a\u043b\u044e\u0447=\u0437\u043d\u0430\u0447#\u0444\u0440\u0430\u0433\")\n    assert msg.url.path == \"/\u043f\u0443\u0442\u044c\"\n    assert msg.url.query == {\"\u043a\u043b\u044e\u0447\": \"\u0437\u043d\u0430\u0447\"}\n    assert msg.url.fragment == \"\u0444\u0440\u0430\u0433\"\n\n\n@pytest.mark.skipif(\n    \"HttpRequestParserC\" not in dir(aiohttp.http_parser),\n    reason=\"C based HTTP parser not available\",\n)\ndef test_parse_bad_method_for_c_parser_raises(loop, protocol):\n    payload = b\"GET1 /test HTTP/1.1\\r\\n\\r\\n\"\n    parser = HttpRequestParserC(\n        protocol,\n        loop,\n        2**16,\n        max_line_size=8190,\n        max_headers=32768,\n        max_field_size=8190,\n    )\n\n    with pytest.raises(aiohttp.http_exceptions.BadStatusLine):\n        messages, upgrade, tail = parser.feed_data(payload)\n\n\nclass TestParsePayload:\n    async def test_parse_eof_payload(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, readall=True)\n        p.feed_data(b\"data\")\n        p.feed_eof()\n\n        assert out.is_eof()\n        assert [(bytearray(b\"data\"), 4)] == list(out._buffer)\n\n    async def test_parse_no_body(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, method=\"PUT\")\n\n        assert out.is_eof()\n        assert p.done\n\n    async def test_parse_length_payload_eof(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n\n        p = HttpPayloadParser(out, length=4)\n        p.feed_data(b\"da\")\n\n        with pytest.raises(http_exceptions.ContentLengthError):\n            p.feed_eof()\n\n    async def test_parse_chunked_payload_size_error(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, chunked=True)\n        with pytest.raises(http_exceptions.TransferEncodingError):\n            p.feed_data(b\"blah\\r\\n\")\n        assert isinstance(out.exception(), http_exceptions.TransferEncodingError)\n\n    async def test_parse_chunked_payload_split_end(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\n\")\n        p.feed_data(b\"\\r\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_parse_chunked_payload_split_end2(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\n\\r\")\n        p.feed_data(b\"\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_parse_chunked_payload_split_end_trailers(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\n\")\n        p.feed_data(b\"Content-MD5: 912ec803b2ce49e4a541068d495ab570\\r\\n\")\n        p.feed_data(b\"\\r\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_parse_chunked_payload_split_end_trailers2(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\n\")\n        p.feed_data(b\"Content-MD5: 912ec803b2ce49e4a541068d495ab570\\r\\n\\r\")\n        p.feed_data(b\"\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_parse_chunked_payload_split_end_trailers3(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\nContent-MD5: \")\n        p.feed_data(b\"912ec803b2ce49e4a541068d495ab570\\r\\n\\r\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_parse_chunked_payload_split_end_trailers4(self, protocol) -> None:\n        out = aiohttp.StreamReader(protocol, 2**16, loop=None)\n        p = HttpPayloadParser(out, chunked=True)\n        p.feed_data(b\"4\\r\\nasdf\\r\\n0\\r\\n\" b\"C\")\n        p.feed_data(b\"ontent-MD5: 912ec803b2ce49e4a541068d495ab570\\r\\n\\r\\n\")\n\n        assert out.is_eof()\n        assert b\"asdf\" == b\"\".join(out._buffer)\n\n    async def test_http_payload_parser_length(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=2)\n        eof, tail = p.feed_data(b\"1245\")\n        assert eof\n\n        assert b\"12\" == b\"\".join(d for d, _ in out._buffer)\n        assert b\"45\" == tail\n\n    async def test_http_payload_parser_deflate(self, stream) -> None:\n        # c=compressobj(wbits=15); b''.join([c.compress(b'data'), c.flush()])\n        COMPRESSED = b\"x\\x9cKI,I\\x04\\x00\\x04\\x00\\x01\\x9b\"\n\n        length = len(COMPRESSED)\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=length, compression=\"deflate\")\n        p.feed_data(COMPRESSED)\n        assert b\"data\" == b\"\".join(d for d, _ in out._buffer)\n        assert out.is_eof()\n\n    async def test_http_payload_parser_deflate_no_hdrs(self, stream: Any) -> None:\n        \"\"\"Tests incorrectly formed data (no zlib headers).\"\"\"\n        # c=compressobj(wbits=-15); b''.join([c.compress(b'data'), c.flush()])\n        COMPRESSED = b\"KI,I\\x04\\x00\"\n\n        length = len(COMPRESSED)\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=length, compression=\"deflate\")\n        p.feed_data(COMPRESSED)\n        assert b\"data\" == b\"\".join(d for d, _ in out._buffer)\n        assert out.is_eof()\n\n    async def test_http_payload_parser_deflate_light(self, stream) -> None:\n        # c=compressobj(wbits=9); b''.join([c.compress(b'data'), c.flush()])\n        COMPRESSED = b\"\\x18\\x95KI,I\\x04\\x00\\x04\\x00\\x01\\x9b\"\n\n        length = len(COMPRESSED)\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=length, compression=\"deflate\")\n        p.feed_data(COMPRESSED)\n        assert b\"data\" == b\"\".join(d for d, _ in out._buffer)\n        assert out.is_eof()\n\n    async def test_http_payload_parser_deflate_split(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, compression=\"deflate\", readall=True)\n        # Feeding one correct byte should be enough to choose exact\n        # deflate decompressor\n        p.feed_data(b\"x\", 1)\n        p.feed_data(b\"\\x9cKI,I\\x04\\x00\\x04\\x00\\x01\\x9b\", 11)\n        p.feed_eof()\n        assert b\"data\" == b\"\".join(d for d, _ in out._buffer)\n\n    async def test_http_payload_parser_deflate_split_err(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, compression=\"deflate\", readall=True)\n        # Feeding one wrong byte should be enough to choose exact\n        # deflate decompressor\n        p.feed_data(b\"K\", 1)\n        p.feed_data(b\"I,I\\x04\\x00\", 5)\n        p.feed_eof()\n        assert b\"data\" == b\"\".join(d for d, _ in out._buffer)\n\n    async def test_http_payload_parser_length_zero(self, stream) -> None:\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=0)\n        assert p.done\n        assert out.is_eof()\n\n    @pytest.mark.skipif(brotli is None, reason=\"brotli is not installed\")\n    async def test_http_payload_brotli(self, stream) -> None:\n        compressed = brotli.compress(b\"brotli data\")\n        out = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        p = HttpPayloadParser(out, length=len(compressed), compression=\"br\")\n        p.feed_data(compressed)\n        assert b\"brotli data\" == b\"\".join(d for d, _ in out._buffer)\n        assert out.is_eof()\n\n\nclass TestDeflateBuffer:\n    async def test_feed_data(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"deflate\")\n\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.decompress.return_value = b\"line\"\n\n        # First byte should be b'x' in order code not to change the decoder.\n        dbuf.feed_data(b\"xxxx\", 4)\n        assert [b\"line\"] == list(d for d, _ in buf._buffer)\n\n    async def test_feed_data_err(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"deflate\")\n\n        exc = ValueError()\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.decompress.side_effect = exc\n\n        with pytest.raises(http_exceptions.ContentEncodingError):\n            # Should be more than 4 bytes to trigger deflate FSM error.\n            # Should start with b'x', otherwise code switch mocked decoder.\n            dbuf.feed_data(b\"xsomedata\", 9)\n\n    async def test_feed_eof(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"deflate\")\n\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.flush.return_value = b\"line\"\n\n        dbuf.feed_eof()\n        assert [b\"line\"] == list(d for d, _ in buf._buffer)\n        assert buf._eof\n\n    async def test_feed_eof_err_deflate(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"deflate\")\n\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.flush.return_value = b\"line\"\n        dbuf.decompressor.eof = False\n\n        with pytest.raises(http_exceptions.ContentEncodingError):\n            dbuf.feed_eof()\n\n    async def test_feed_eof_no_err_gzip(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"gzip\")\n\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.flush.return_value = b\"line\"\n        dbuf.decompressor.eof = False\n\n        dbuf.feed_eof()\n        assert [b\"line\"] == list(d for d, _ in buf._buffer)\n\n    async def test_feed_eof_no_err_brotli(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"br\")\n\n        dbuf.decompressor = mock.Mock()\n        dbuf.decompressor.flush.return_value = b\"line\"\n        dbuf.decompressor.eof = False\n\n        dbuf.feed_eof()\n        assert [b\"line\"] == list(d for d, _ in buf._buffer)\n\n    async def test_empty_body(self, stream) -> None:\n        buf = aiohttp.FlowControlDataQueue(\n            stream, 2**16, loop=asyncio.get_event_loop()\n        )\n        dbuf = DeflateBuffer(buf, \"deflate\")\n        dbuf.feed_eof()\n\n        assert buf.at_eof()\n"], "filenames": ["aiohttp/http_parser.py", "tests/test_http_parser.py"], "buggy_code_start_loc": [63, 467], "buggy_code_end_loc": [784, 759], "fixing_code_start_loc": [63, 468], "fixing_code_end_loc": [798, 836], "type": "CWE-444", "message": "aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. The HTTP parser in AIOHTTP has numerous problems with header parsing, which could lead to request smuggling. This parser is only used when AIOHTTP_NO_EXTENSIONS is enabled (or not using a prebuilt wheel). These bugs have been addressed in commit `d5c12ba89` which has been included in release version 3.8.6. Users are advised to upgrade. There are no known workarounds for these issues.", "other": {"cve": {"id": "CVE-2023-47627", "sourceIdentifier": "security-advisories@github.com", "published": "2023-11-14T21:15:12.820", "lastModified": "2024-02-05T07:15:08.850", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "aiohttp is an asynchronous HTTP client/server framework for asyncio and Python. The HTTP parser in AIOHTTP has numerous problems with header parsing, which could lead to request smuggling. This parser is only used when AIOHTTP_NO_EXTENSIONS is enabled (or not using a prebuilt wheel). These bugs have been addressed in commit `d5c12ba89` which has been included in release version 3.8.6. Users are advised to upgrade. There are no known workarounds for these issues."}, {"lang": "es", "value": "aiohttp es un framework cliente/servidor HTTP as\u00edncrono para asyncio y Python. El analizador HTTP en AIOHTTP tiene numerosos problemas con el an\u00e1lisis de encabezados, lo que podr\u00eda provocar contrabando de solicitudes. Este analizador solo se usa cuando AIOHTTP_NO_EXTENSIONS est\u00e1 habilitado (o no se usa una rueda predise\u00f1ada). Estos errores se solucionaron en el commit`d5c12ba89` que se incluy\u00f3 en la versi\u00f3n 3.8.6. Se recomienda a los usuarios que actualicen. No se conocen workarounds para estos problemas."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 3.9, "impactScore": 1.4}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-444"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:aiohttp:aiohttp:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.8.6", "matchCriteriaId": "CB42D548-24BA-4A11-9732-2BE87863BCF2"}]}]}], "references": [{"url": "https://github.com/aio-libs/aiohttp/commit/d5c12ba890557a575c313bb3017910d7616fce3d", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/aio-libs/aiohttp/security/advisories/GHSA-gfw2-4jvh-wgfg", "source": "security-advisories@github.com", "tags": ["Exploit", "Vendor Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FUSJVQ7OQ55RWL4XAX2F5EZ73N4ZSH6U/", "source": "security-advisories@github.com"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/VDKQ6HM3KNDU4OQI476ZWT4O7DMSIT35/", "source": "security-advisories@github.com"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/WQYQL6WV535EEKSNH7KRARLLMOW5WXDM/", "source": "security-advisories@github.com"}]}, "github_commit_url": "https://github.com/aio-libs/aiohttp/commit/d5c12ba890557a575c313bb3017910d7616fce3d"}}