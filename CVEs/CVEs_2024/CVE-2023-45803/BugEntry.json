{"buggy_code": ["version: 2\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3\"\n\npython:\n  install:\n    - requirements: docs/requirements.txt\n    - method: pip\n      path: .\n      extra_requirements:\n        - brotli\n        - secure\n        - socks\n        - zstd\n\nsphinx:\n  fail_on_warning: true\n", "2.0.6 (2023-10-02)\n==================\n\n* Added the ``Cookie`` header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via ``Retry.remove_headers_on_redirect``.\n\n2.0.5 (2023-09-20)\n==================\n\n- Allowed pyOpenSSL third-party module without any deprecation warning. (`#3126 <https://github.com/urllib3/urllib3/issues/3126>`__)\n- Fixed default ``blocksize`` of ``HTTPConnection`` classes to match high-level classes. Previously was 8KiB, now 16KiB. (`#3066 <https://github.com/urllib3/urllib3/issues/3066>`__)\n\n\n2.0.4 (2023-07-19)\n==================\n\n- Added support for union operators to ``HTTPHeaderDict`` (`#2254 <https://github.com/urllib3/urllib3/issues/2254>`__)\n- Added ``BaseHTTPResponse`` to ``urllib3.__all__`` (`#3078 <https://github.com/urllib3/urllib3/issues/3078>`__)\n- Fixed ``urllib3.connection.HTTPConnection`` to raise the ``http.client.connect`` audit event to have the same behavior as the standard library HTTP client (`#2757 <https://github.com/urllib3/urllib3/issues/2757>`__)\n- Relied on the standard library for checking hostnames in supported PyPy releases (`#3087 <https://github.com/urllib3/urllib3/issues/3087>`__)\n\n\n2.0.3 (2023-06-07)\n==================\n\n- Allowed alternative SSL libraries such as LibreSSL, while still issuing a warning as we cannot help users facing issues with implementations other than OpenSSL. (`#3020 <https://github.com/urllib3/urllib3/issues/3020>`__)\n- Deprecated URLs which don't have an explicit scheme (`#2950 <https://github.com/urllib3/urllib3/pull/2950>`_)\n- Fixed response decoding with Zstandard when compressed data is made of several frames. (`#3008 <https://github.com/urllib3/urllib3/issues/3008>`__)\n- Fixed ``assert_hostname=False`` to correctly skip hostname check. (`#3051 <https://github.com/urllib3/urllib3/issues/3051>`__)\n\n\n2.0.2 (2023-05-03)\n==================\n\n- Fixed ``HTTPResponse.stream()`` to continue yielding bytes if buffered decompressed data\n  was still available to be read even if the underlying socket is closed. This prevents\n  a compressed response from being truncated. (`#3009 <https://github.com/urllib3/urllib3/issues/3009>`__)\n\n\n2.0.1 (2023-04-30)\n==================\n\n- Fixed a socket leak when fingerprint or hostname verifications fail. (`#2991 <https://github.com/urllib3/urllib3/issues/2991>`__)\n- Fixed an error when ``HTTPResponse.read(0)`` was the first ``read`` call or when the internal response body buffer was otherwise empty. (`#2998 <https://github.com/urllib3/urllib3/issues/2998>`__)\n\n\n2.0.0 (2023-04-26)\n==================\n\nRead the `v2.0 migration guide <https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html>`__ for help upgrading to the latest version of urllib3.\n\nRemoved\n-------\n\n* Removed support for Python 2.7, 3.5, and 3.6 (`#883 <https://github.com/urllib3/urllib3/issues/883>`__, `#2336 <https://github.com/urllib3/urllib3/issues/2336>`__).\n* Removed fallback on certificate ``commonName`` in ``match_hostname()`` function.\n  This behavior was deprecated in May 2000 in RFC 2818. Instead only ``subjectAltName``\n  is used to verify the hostname by default. To enable verifying the hostname against\n  ``commonName`` use ``SSLContext.hostname_checks_common_name = True`` (`#2113 <https://github.com/urllib3/urllib3/issues/2113>`__).\n* Removed support for Python with an ``ssl`` module compiled with LibreSSL, CiscoSSL,\n  wolfSSL, and all other OpenSSL alternatives. Python is moving to require OpenSSL with PEP 644 (`#2168 <https://github.com/urllib3/urllib3/issues/2168>`__).\n* Removed support for OpenSSL versions earlier than 1.1.1 or that don't have SNI support.\n  When an incompatible OpenSSL version is detected an ``ImportError`` is raised (`#2168 <https://github.com/urllib3/urllib3/issues/2168>`__).\n* Removed the list of default ciphers for OpenSSL 1.1.1+ and SecureTransport as their own defaults are already secure (`#2082 <https://github.com/urllib3/urllib3/issues/2082>`__).\n* Removed ``urllib3.contrib.appengine.AppEngineManager`` and support for Google App Engine Standard Environment (`#2044 <https://github.com/urllib3/urllib3/issues/2044>`__).\n* Removed deprecated ``Retry`` options ``method_whitelist``, ``DEFAULT_REDIRECT_HEADERS_BLACKLIST`` (`#2086 <https://github.com/urllib3/urllib3/issues/2086>`__).\n* Removed ``urllib3.HTTPResponse.from_httplib`` (`#2648 <https://github.com/urllib3/urllib3/issues/2648>`__).\n* Removed default value of ``None`` for the ``request_context`` parameter of ``urllib3.PoolManager.connection_from_pool_key``. This change should have no effect on users as the default value of ``None`` was an invalid option and was never used (`#1897 <https://github.com/urllib3/urllib3/issues/1897>`__).\n* Removed the ``urllib3.request`` module. ``urllib3.request.RequestMethods`` has been made a private API.\n  This change was made to ensure that ``from urllib3 import request`` imported the top-level ``request()``\n  function instead of the ``urllib3.request`` module (`#2269 <https://github.com/urllib3/urllib3/issues/2269>`__).\n* Removed support for SSLv3.0 from the ``urllib3.contrib.pyopenssl`` even when support is available from the compiled OpenSSL library (`#2233 <https://github.com/urllib3/urllib3/issues/2233>`__).\n* Removed the deprecated ``urllib3.contrib.ntlmpool`` module (`#2339 <https://github.com/urllib3/urllib3/issues/2339>`__).\n* Removed ``DEFAULT_CIPHERS``, ``HAS_SNI``, ``USE_DEFAULT_SSLCONTEXT_CIPHERS``, from the private module ``urllib3.util.ssl_`` (`#2168 <https://github.com/urllib3/urllib3/issues/2168>`__).\n* Removed ``urllib3.exceptions.SNIMissingWarning`` (`#2168 <https://github.com/urllib3/urllib3/issues/2168>`__).\n* Removed the ``_prepare_conn`` method from ``HTTPConnectionPool``. Previously this was only used to call ``HTTPSConnection.set_cert()`` by ``HTTPSConnectionPool`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Removed ``tls_in_tls_required`` property from ``HTTPSConnection``. This is now determined from the ``scheme`` parameter in ``HTTPConnection.set_tunnel()`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Removed the ``strict`` parameter/attribute from ``HTTPConnection``, ``HTTPSConnection``, ``HTTPConnectionPool``, ``HTTPSConnectionPool``, and ``HTTPResponse`` (`#2064 <https://github.com/urllib3/urllib3/issues/2064>`__).\n\nDeprecated\n----------\n\n* Deprecated ``HTTPResponse.getheaders()`` and ``HTTPResponse.getheader()`` which will be removed in urllib3 v2.1.0. Instead use ``HTTPResponse.headers`` and ``HTTPResponse.headers.get(name, default)``. (`#1543 <https://github.com/urllib3/urllib3/issues/1543>`__, `#2814 <https://github.com/urllib3/urllib3/issues/2814>`__).\n* Deprecated ``urllib3.contrib.pyopenssl`` module which will be removed in urllib3 v2.1.0 (`#2691 <https://github.com/urllib3/urllib3/issues/2691>`__).\n* Deprecated ``urllib3.contrib.securetransport`` module which will be removed in urllib3 v2.1.0 (`#2692 <https://github.com/urllib3/urllib3/issues/2692>`__).\n* Deprecated ``ssl_version`` option in favor of ``ssl_minimum_version``. ``ssl_version`` will be removed in urllib3 v2.1.0 (`#2110 <https://github.com/urllib3/urllib3/issues/2110>`__).\n* Deprecated the ``strict`` parameter of ``PoolManager.connection_from_context()`` as it's not longer needed in Python 3.x. It will be removed in urllib3 v2.1.0 (`#2267 <https://github.com/urllib3/urllib3/issues/2267>`__)\n* Deprecated the ``NewConnectionError.pool`` attribute which will be removed in urllib3 v2.1.0 (`#2271 <https://github.com/urllib3/urllib3/issues/2271>`__).\n* Deprecated ``format_header_param_html5`` and ``format_header_param`` in favor of ``format_multipart_header_param`` (`#2257 <https://github.com/urllib3/urllib3/issues/2257>`__).\n* Deprecated ``RequestField.header_formatter`` parameter which will be removed in urllib3 v2.1.0 (`#2257 <https://github.com/urllib3/urllib3/issues/2257>`__).\n* Deprecated ``HTTPSConnection.set_cert()`` method. Instead pass parameters to the ``HTTPSConnection`` constructor (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Deprecated ``HTTPConnection.request_chunked()`` method which will be removed in urllib3 v2.1.0. Instead pass ``chunked=True`` to ``HTTPConnection.request()`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n\nAdded\n-----\n\n* Added top-level ``urllib3.request`` function which uses a preconfigured module-global ``PoolManager`` instance (`#2150 <https://github.com/urllib3/urllib3/issues/2150>`__).\n* Added the ``json`` parameter to ``urllib3.request()``, ``PoolManager.request()``, and ``ConnectionPool.request()`` methods to send JSON bodies in requests. Using this parameter will set the header ``Content-Type: application/json`` if ``Content-Type`` isn't already defined.\n  Added support for parsing JSON response bodies with ``HTTPResponse.json()`` method (`#2243 <https://github.com/urllib3/urllib3/issues/2243>`__).\n* Added type hints to the ``urllib3`` module (`#1897 <https://github.com/urllib3/urllib3/issues/1897>`__).\n* Added ``ssl_minimum_version`` and ``ssl_maximum_version`` options which set\n  ``SSLContext.minimum_version`` and ``SSLContext.maximum_version`` (`#2110 <https://github.com/urllib3/urllib3/issues/2110>`__).\n* Added support for Zstandard (RFC 8878) when ``zstandard`` 1.18.0 or later is installed.\n  Added the ``zstd`` extra which installs the ``zstandard`` package (`#1992 <https://github.com/urllib3/urllib3/issues/1992>`__).\n* Added ``urllib3.response.BaseHTTPResponse`` class. All future response classes will be subclasses of ``BaseHTTPResponse`` (`#2083 <https://github.com/urllib3/urllib3/issues/2083>`__).\n* Added ``FullPoolError`` which is raised when ``PoolManager(block=True)`` and a connection is returned to a full pool (`#2197 <https://github.com/urllib3/urllib3/issues/2197>`__).\n* Added ``HTTPHeaderDict`` to the top-level ``urllib3`` namespace (`#2216 <https://github.com/urllib3/urllib3/issues/2216>`__).\n* Added support for configuring header merging behavior with HTTPHeaderDict\n  When using a ``HTTPHeaderDict`` to provide headers for a request, by default duplicate\n  header values will be repeated. But if ``combine=True`` is passed into a call to\n  ``HTTPHeaderDict.add``, then the added header value will be merged in with an existing\n  value into a comma-separated list (``X-My-Header: foo, bar``) (`#2242 <https://github.com/urllib3/urllib3/issues/2242>`__).\n* Added ``NameResolutionError`` exception when a DNS error occurs (`#2305 <https://github.com/urllib3/urllib3/issues/2305>`__).\n* Added ``proxy_assert_hostname`` and ``proxy_assert_fingerprint`` kwargs to ``ProxyManager`` (`#2409 <https://github.com/urllib3/urllib3/issues/2409>`__).\n* Added a configurable ``backoff_max`` parameter to the ``Retry`` class.\n  If a custom ``backoff_max`` is provided to the ``Retry`` class, it\n  will replace the ``Retry.DEFAULT_BACKOFF_MAX`` (`#2494 <https://github.com/urllib3/urllib3/issues/2494>`__).\n* Added the ``authority`` property to the Url class as per RFC 3986 3.2. This property should be used in place of ``netloc`` for users who want to include the userinfo (auth) component of the URI (`#2520 <https://github.com/urllib3/urllib3/issues/2520>`__).\n* Added the ``scheme`` parameter to ``HTTPConnection.set_tunnel`` to configure the scheme of the origin being tunnelled to (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Added the ``is_closed``, ``is_connected`` and ``has_connected_to_proxy`` properties to ``HTTPConnection`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Added optional ``backoff_jitter`` parameter to ``Retry``. (`#2952 <https://github.com/urllib3/urllib3/issues/2952>`__)\n\nChanged\n-------\n\n* Changed ``urllib3.response.HTTPResponse.read`` to respect the semantics of ``io.BufferedIOBase`` regardless of compression. Specifically, this method:\n\n  * Only returns an empty bytes object to indicate EOF (that is, the response has been fully consumed).\n  * Never returns more bytes than requested.\n  * Can issue any number of system calls: zero, one or multiple.\n\n  If you want each ``urllib3.response.HTTPResponse.read`` call to issue a single system call, you need to disable decompression by setting ``decode_content=False`` (`#2128 <https://github.com/urllib3/urllib3/issues/2128>`__).\n* Changed ``urllib3.HTTPConnection.getresponse`` to return an instance of ``urllib3.HTTPResponse`` instead of ``http.client.HTTPResponse`` (`#2648 <https://github.com/urllib3/urllib3/issues/2648>`__).\n* Changed ``ssl_version`` to instead set the corresponding ``SSLContext.minimum_version``\n  and ``SSLContext.maximum_version`` values.  Regardless of ``ssl_version`` passed\n  ``SSLContext`` objects are now constructed using ``ssl.PROTOCOL_TLS_CLIENT`` (`#2110 <https://github.com/urllib3/urllib3/issues/2110>`__).\n* Changed default ``SSLContext.minimum_version`` to be ``TLSVersion.TLSv1_2`` in line with Python 3.10 (`#2373 <https://github.com/urllib3/urllib3/issues/2373>`__).\n* Changed ``ProxyError`` to wrap any connection error (timeout, TLS, DNS) that occurs when connecting to the proxy (`#2482 <https://github.com/urllib3/urllib3/pull/2482>`__).\n* Changed ``urllib3.util.create_urllib3_context`` to not override the system cipher suites with\n  a default value. The new default will be cipher suites configured by the operating system (`#2168 <https://github.com/urllib3/urllib3/issues/2168>`__).\n* Changed ``multipart/form-data`` header parameter formatting matches the WHATWG HTML Standard as of 2021-06-10. Control characters in filenames are no longer percent encoded (`#2257 <https://github.com/urllib3/urllib3/issues/2257>`__).\n* Changed the error raised when connecting via HTTPS when the ``ssl`` module isn't available from ``SSLError`` to ``ImportError`` (`#2589 <https://github.com/urllib3/urllib3/issues/2589>`__).\n* Changed ``HTTPConnection.request()`` to always use lowercase chunk boundaries when sending requests with ``Transfer-Encoding: chunked`` (`#2515 <https://github.com/urllib3/urllib3/issues/2515>`__).\n* Changed ``enforce_content_length`` default to True, preventing silent data loss when reading streamed responses (`#2514 <https://github.com/urllib3/urllib3/issues/2514>`__).\n* Changed internal implementation of ``HTTPHeaderDict`` to use ``dict`` instead of ``collections.OrderedDict`` for better performance (`#2080 <https://github.com/urllib3/urllib3/issues/2080>`__).\n* Changed the ``urllib3.contrib.pyopenssl`` module to wrap ``OpenSSL.SSL.Error`` with ``ssl.SSLError`` in ``PyOpenSSLContext.load_cert_chain`` (`#2628 <https://github.com/urllib3/urllib3/issues/2628>`__).\n* Changed usage of the deprecated ``socket.error`` to ``OSError`` (`#2120 <https://github.com/urllib3/urllib3/issues/2120>`__).\n* Changed all parameters in the ``HTTPConnection`` and ``HTTPSConnection`` constructors to be keyword-only except ``host`` and ``port`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Changed ``HTTPConnection.getresponse()`` to set the socket timeout from ``HTTPConnection.timeout`` value before reading\n  data from the socket. This previously was done manually by the ``HTTPConnectionPool`` calling ``HTTPConnection.sock.settimeout(...)`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Changed the ``_proxy_host`` property to ``_tunnel_host`` in ``HTTPConnectionPool`` to more closely match how the property is used (value in ``HTTPConnection.set_tunnel()``) (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Changed name of ``Retry.BACK0FF_MAX`` to be ``Retry.DEFAULT_BACKOFF_MAX``.\n* Changed TLS handshakes to use ``SSLContext.check_hostname`` when possible (`#2452 <https://github.com/urllib3/urllib3/pull/2452>`__).\n* Changed ``server_hostname`` to behave like other parameters only used by ``HTTPSConnectionPool`` (`#2537 <https://github.com/urllib3/urllib3/pull/2537>`__).\n* Changed the default ``blocksize`` to 16KB to match OpenSSL's default read amounts (`#2348 <https://github.com/urllib3/urllib3/pull/2348>`__).\n* Changed ``HTTPResponse.read()`` to raise an error when calling with ``decode_content=False`` after using ``decode_content=True`` to prevent data loss (`#2800 <https://github.com/urllib3/urllib3/issues/2800>`__).\n\nFixed\n-----\n\n* Fixed thread-safety issue where accessing a ``PoolManager`` with many distinct origins would cause connection pools to be closed while requests are in progress (`#1252 <https://github.com/urllib3/urllib3/issues/1252>`__).\n* Fixed an issue where an ``HTTPConnection`` instance would erroneously reuse the socket read timeout value from reading the previous response instead of a newly configured connect timeout.\n  Instead now if ``HTTPConnection.timeout`` is updated before sending the next request the new timeout value will be used (`#2645 <https://github.com/urllib3/urllib3/issues/2645>`__).\n* Fixed ``socket.error.errno`` when raised from pyOpenSSL's ``OpenSSL.SSL.SysCallError`` (`#2118 <https://github.com/urllib3/urllib3/issues/2118>`__).\n* Fixed the default value of ``HTTPSConnection.socket_options`` to match ``HTTPConnection`` (`#2213 <https://github.com/urllib3/urllib3/issues/2213>`__).\n* Fixed a bug where ``headers`` would be modified by the ``remove_headers_on_redirect`` feature (`#2272 <https://github.com/urllib3/urllib3/issues/2272>`__).\n* Fixed a reference cycle bug in ``urllib3.util.connection.create_connection()`` (`#2277 <https://github.com/urllib3/urllib3/issues/2277>`__).\n* Fixed a socket leak if ``HTTPConnection.connect()`` fails (`#2571 <https://github.com/urllib3/urllib3/pull/2571>`__).\n* Fixed ``urllib3.contrib.pyopenssl.WrappedSocket`` and ``urllib3.contrib.securetransport.WrappedSocket`` close methods (`#2970 <https://github.com/urllib3/urllib3/issues/2970>`__)\n\n1.26.17 (2023-10-02)\n====================\n\n* Added the ``Cookie`` header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via ``Retry.remove_headers_on_redirect``. (`#3139 <https://github.com/urllib3/urllib3/pull/3139>`_)\n\n1.26.16 (2023-05-23)\n====================\n\n* Fixed thread-safety issue where accessing a ``PoolManager`` with many distinct origins\n  would cause connection pools to be closed while requests are in progress (`#2954 <https://github.com/urllib3/urllib3/pull/2954>`_)\n\n1.26.15 (2023-03-10)\n====================\n\n* Fix socket timeout value when ``HTTPConnection`` is reused (`#2645 <https://github.com/urllib3/urllib3/issues/2645>`__)\n* Remove \"!\" character from the unreserved characters in IPv6 Zone ID parsing\n  (`#2899 <https://github.com/urllib3/urllib3/issues/2899>`__)\n* Fix IDNA handling of '\\x80' byte (`#2901 <https://github.com/urllib3/urllib3/issues/2901>`__)\n\n1.26.14 (2023-01-11)\n====================\n\n* Fixed parsing of port 0 (zero) returning None, instead of 0. (`#2850 <https://github.com/urllib3/urllib3/issues/2850>`__)\n* Removed deprecated getheaders() calls in contrib module. Fixed the type hint of ``PoolKey.key_retries`` by adding ``bool`` to the union. (`#2865 <https://github.com/urllib3/urllib3/issues/2865>`__)\n\n1.26.13 (2022-11-23)\n====================\n\n* Deprecated the ``HTTPResponse.getheaders()`` and ``HTTPResponse.getheader()`` methods.\n* Fixed an issue where parsing a URL with leading zeroes in the port would be rejected\n  even when the port number after removing the zeroes was valid.\n* Fixed a deprecation warning when using cryptography v39.0.0.\n* Removed the ``<4`` in the ``Requires-Python`` packaging metadata field.\n\n1.26.12 (2022-08-22)\n====================\n\n* Deprecated the `urllib3[secure]` extra and the `urllib3.contrib.pyopenssl` module.\n  Both will be removed in v2.x. See this `GitHub issue <https://github.com/urllib3/urllib3/issues/2680>`_\n  for justification and info on how to migrate.\n\n1.26.11 (2022-07-25)\n====================\n\n* Fixed an issue where reading more than 2 GiB in a call to ``HTTPResponse.read`` would\n  raise an ``OverflowError`` on Python 3.9 and earlier.\n\n1.26.10 (2022-07-07)\n====================\n\n* Removed support for Python 3.5\n* Fixed an issue where a ``ProxyError`` recommending configuring the proxy as HTTP\n  instead of HTTPS could appear even when an HTTPS proxy wasn't configured.\n\n1.26.9 (2022-03-16)\n===================\n\n* Changed ``urllib3[brotli]`` extra to favor installing Brotli libraries that are still\n  receiving updates like ``brotli`` and ``brotlicffi`` instead of ``brotlipy``.\n  This change does not impact behavior of urllib3, only which dependencies are installed.\n* Fixed a socket leaking when ``HTTPSConnection.connect()`` raises an exception.\n* Fixed ``server_hostname`` being forwarded from ``PoolManager`` to ``HTTPConnectionPool``\n  when requesting an HTTP URL. Should only be forwarded when requesting an HTTPS URL.\n\n1.26.8 (2022-01-07)\n===================\n\n* Added extra message to ``urllib3.exceptions.ProxyError`` when urllib3 detects that\n  a proxy is configured to use HTTPS but the proxy itself appears to only use HTTP.\n* Added a mention of the size of the connection pool when discarding a connection due to the pool being full.\n* Added explicit support for Python 3.11.\n* Deprecated the ``Retry.MAX_BACKOFF`` class property in favor of ``Retry.DEFAULT_MAX_BACKOFF``\n  to better match the rest of the default parameter names. ``Retry.MAX_BACKOFF`` is removed in v2.0.\n* Changed location of the vendored ``ssl.match_hostname`` function from ``urllib3.packages.ssl_match_hostname``\n  to ``urllib3.util.ssl_match_hostname`` to ensure Python 3.10+ compatibility after being repackaged\n  by downstream distributors.\n* Fixed absolute imports, all imports are now relative.\n\n\n1.26.7 (2021-09-22)\n===================\n\n* Fixed a bug with HTTPS hostname verification involving IP addresses and lack\n  of SNI. (Issue #2400)\n* Fixed a bug where IPv6 braces weren't stripped during certificate hostname\n  matching. (Issue #2240)\n\n\n1.26.6 (2021-06-25)\n===================\n\n* Deprecated the ``urllib3.contrib.ntlmpool`` module. urllib3 is not able to support\n  it properly due to `reasons listed in this issue <https://github.com/urllib3/urllib3/issues/2282>`_.\n  If you are a user of this module please leave a comment.\n* Changed ``HTTPConnection.request_chunked()`` to not erroneously emit multiple\n  ``Transfer-Encoding`` headers in the case that one is already specified.\n* Fixed typo in deprecation message to recommend ``Retry.DEFAULT_ALLOWED_METHODS``.\n\n\n1.26.5 (2021-05-26)\n===================\n\n* Fixed deprecation warnings emitted in Python 3.10.\n* Updated vendored ``six`` library to 1.16.0.\n* Improved performance of URL parser when splitting\n  the authority component.\n\n\n1.26.4 (2021-03-15)\n===================\n\n* Changed behavior of the default ``SSLContext`` when connecting to HTTPS proxy\n  during HTTPS requests. The default ``SSLContext`` now sets ``check_hostname=True``.\n\n\n1.26.3 (2021-01-26)\n===================\n\n* Fixed bytes and string comparison issue with headers (Pull #2141)\n\n* Changed ``ProxySchemeUnknown`` error message to be\n  more actionable if the user supplies a proxy URL without\n  a scheme. (Pull #2107)\n\n\n1.26.2 (2020-11-12)\n===================\n\n* Fixed an issue where ``wrap_socket`` and ``CERT_REQUIRED`` wouldn't\n  be imported properly on Python 2.7.8 and earlier (Pull #2052)\n\n\n1.26.1 (2020-11-11)\n===================\n\n* Fixed an issue where two ``User-Agent`` headers would be sent if a\n  ``User-Agent`` header key is passed as ``bytes`` (Pull #2047)\n\n\n1.26.0 (2020-11-10)\n===================\n\n* **NOTE: urllib3 v2.0 will drop support for Python 2**.\n  `Read more in the v2.0 Roadmap <https://urllib3.readthedocs.io/en/latest/v2-roadmap.html>`_.\n\n* Added support for HTTPS proxies contacting HTTPS servers (Pull #1923, Pull #1806)\n\n* Deprecated negotiating TLSv1 and TLSv1.1 by default. Users that\n  still wish to use TLS earlier than 1.2 without a deprecation warning\n  should opt-in explicitly by setting ``ssl_version=ssl.PROTOCOL_TLSv1_1`` (Pull #2002)\n  **Starting in urllib3 v2.0: Connections that receive a ``DeprecationWarning`` will fail**\n\n* Deprecated ``Retry`` options ``Retry.DEFAULT_METHOD_WHITELIST``, ``Retry.DEFAULT_REDIRECT_HEADERS_BLACKLIST``\n  and ``Retry(method_whitelist=...)`` in favor of ``Retry.DEFAULT_ALLOWED_METHODS``,\n  ``Retry.DEFAULT_REMOVE_HEADERS_ON_REDIRECT``, and ``Retry(allowed_methods=...)``\n  (Pull #2000) **Starting in urllib3 v2.0: Deprecated options will be removed**\n\n* Added default ``User-Agent`` header to every request (Pull #1750)\n\n* Added ``urllib3.util.SKIP_HEADER`` for skipping ``User-Agent``, ``Accept-Encoding``,\n  and ``Host`` headers from being automatically emitted with requests (Pull #2018)\n\n* Collapse ``transfer-encoding: chunked`` request data and framing into\n  the same ``socket.send()`` call (Pull #1906)\n\n* Send ``http/1.1`` ALPN identifier with every TLS handshake by default (Pull #1894)\n\n* Properly terminate SecureTransport connections when CA verification fails (Pull #1977)\n\n* Don't emit an ``SNIMissingWarning`` when passing ``server_hostname=None``\n  to SecureTransport (Pull #1903)\n\n* Disabled requesting TLSv1.2 session tickets as they weren't being used by urllib3 (Pull #1970)\n\n* Suppress ``BrokenPipeError`` when writing request body after the server\n  has closed the socket (Pull #1524)\n\n* Wrap ``ssl.SSLError`` that can be raised from reading a socket (e.g. \"bad MAC\")\n  into an ``urllib3.exceptions.SSLError`` (Pull #1939)\n\n\n1.25.11 (2020-10-19)\n====================\n\n* Fix retry backoff time parsed from ``Retry-After`` header when given\n  in the HTTP date format. The HTTP date was parsed as the local timezone\n  rather than accounting for the timezone in the HTTP date (typically\n  UTC) (Pull #1932, Pull #1935, Pull #1938, Pull #1949)\n\n* Fix issue where an error would be raised when the ``SSLKEYLOGFILE``\n  environment variable was set to the empty string. Now ``SSLContext.keylog_file``\n  is not set in this situation (Pull #2016)\n\n\n1.25.10 (2020-07-22)\n====================\n\n* Added support for ``SSLKEYLOGFILE`` environment variable for\n  logging TLS session keys with use with programs like\n  Wireshark for decrypting captured web traffic (Pull #1867)\n\n* Fixed loading of SecureTransport libraries on macOS Big Sur\n  due to the new dynamic linker cache (Pull #1905)\n\n* Collapse chunked request bodies data and framing into one\n  call to ``send()`` to reduce the number of TCP packets by 2-4x (Pull #1906)\n\n* Don't insert ``None`` into ``ConnectionPool`` if the pool\n  was empty when requesting a connection (Pull #1866)\n\n* Avoid ``hasattr`` call in ``BrotliDecoder.decompress()`` (Pull #1858)\n\n\n1.25.9 (2020-04-16)\n===================\n\n* Added ``InvalidProxyConfigurationWarning`` which is raised when\n  erroneously specifying an HTTPS proxy URL. urllib3 doesn't currently\n  support connecting to HTTPS proxies but will soon be able to\n  and we would like users to migrate properly without much breakage.\n\n  See `this GitHub issue <https://github.com/urllib3/urllib3/issues/1850>`_\n  for more information on how to fix your proxy config. (Pull #1851)\n\n* Drain connection after ``PoolManager`` redirect (Pull #1817)\n\n* Ensure ``load_verify_locations`` raises ``SSLError`` for all backends (Pull #1812)\n\n* Rename ``VerifiedHTTPSConnection`` to ``HTTPSConnection`` (Pull #1805)\n\n* Allow the CA certificate data to be passed as a string (Pull #1804)\n\n* Raise ``ValueError`` if method contains control characters (Pull #1800)\n\n* Add ``__repr__`` to ``Timeout`` (Pull #1795)\n\n\n1.25.8 (2020-01-20)\n===================\n\n* Drop support for EOL Python 3.4 (Pull #1774)\n\n* Optimize _encode_invalid_chars (Pull #1787)\n\n\n1.25.7 (2019-11-11)\n===================\n\n* Preserve ``chunked`` parameter on retries (Pull #1715, Pull #1734)\n\n* Allow unset ``SERVER_SOFTWARE`` in App Engine (Pull #1704, Issue #1470)\n\n* Fix issue where URL fragment was sent within the request target. (Pull #1732)\n\n* Fix issue where an empty query section in a URL would fail to parse. (Pull #1732)\n\n* Remove TLS 1.3 support in SecureTransport due to Apple removing support (Pull #1703)\n\n\n1.25.6 (2019-09-24)\n===================\n\n* Fix issue where tilde (``~``) characters were incorrectly\n  percent-encoded in the path. (Pull #1692)\n\n\n1.25.5 (2019-09-19)\n===================\n\n* Add mitigation for BPO-37428 affecting Python <3.7.4 and OpenSSL 1.1.1+ which\n  caused certificate verification to be enabled when using ``cert_reqs=CERT_NONE``.\n  (Issue #1682)\n\n\n1.25.4 (2019-09-19)\n===================\n\n* Propagate Retry-After header settings to subsequent retries. (Pull #1607)\n\n* Fix edge case where Retry-After header was still respected even when\n  explicitly opted out of. (Pull #1607)\n\n* Remove dependency on ``rfc3986`` for URL parsing.\n\n* Fix issue where URLs containing invalid characters within ``Url.auth`` would\n  raise an exception instead of percent-encoding those characters.\n\n* Add support for ``HTTPResponse.auto_close = False`` which makes HTTP responses\n  work well with BufferedReaders and other ``io`` module features. (Pull #1652)\n\n* Percent-encode invalid characters in URL for ``HTTPConnectionPool.request()`` (Pull #1673)\n\n\n1.25.3 (2019-05-23)\n===================\n\n* Change ``HTTPSConnection`` to load system CA certificates\n  when ``ca_certs``, ``ca_cert_dir``, and ``ssl_context`` are\n  unspecified. (Pull #1608, Issue #1603)\n\n* Upgrade bundled rfc3986 to v1.3.2. (Pull #1609, Issue #1605)\n\n\n1.25.2 (2019-04-28)\n===================\n\n* Change ``is_ipaddress`` to not detect IPvFuture addresses. (Pull #1583)\n\n* Change ``parse_url`` to percent-encode invalid characters within the\n  path, query, and target components. (Pull #1586)\n\n\n1.25.1 (2019-04-24)\n===================\n\n* Add support for Google's ``Brotli`` package. (Pull #1572, Pull #1579)\n\n* Upgrade bundled rfc3986 to v1.3.1 (Pull #1578)\n\n\n1.25 (2019-04-22)\n=================\n\n* Require and validate certificates by default when using HTTPS (Pull #1507)\n\n* Upgraded ``urllib3.utils.parse_url()`` to be RFC 3986 compliant. (Pull #1487)\n\n* Added support for ``key_password`` for ``HTTPSConnectionPool`` to use\n  encrypted ``key_file`` without creating your own ``SSLContext`` object. (Pull #1489)\n\n* Add TLSv1.3 support to CPython, pyOpenSSL, and SecureTransport ``SSLContext``\n  implementations. (Pull #1496)\n\n* Switched the default multipart header encoder from RFC 2231 to HTML 5 working draft. (Issue #303, Pull #1492)\n\n* Fixed issue where OpenSSL would block if an encrypted client private key was\n  given and no password was given. Instead an ``SSLError`` is raised. (Pull #1489)\n\n* Added support for Brotli content encoding. It is enabled automatically if\n  ``brotlipy`` package is installed which can be requested with\n  ``urllib3[brotli]`` extra. (Pull #1532)\n\n* Drop ciphers using DSS key exchange from default TLS cipher suites.\n  Improve default ciphers when using SecureTransport. (Pull #1496)\n\n* Implemented a more efficient ``HTTPResponse.__iter__()`` method. (Issue #1483)\n\n1.24.3 (2019-05-01)\n===================\n\n* Apply fix for CVE-2019-9740. (Pull #1591)\n\n1.24.2 (2019-04-17)\n===================\n\n* Don't load system certificates by default when any other ``ca_certs``, ``ca_certs_dir`` or\n  ``ssl_context`` parameters are specified.\n\n* Remove Authorization header regardless of case when redirecting to cross-site. (Issue #1510)\n\n* Add support for IPv6 addresses in subjectAltName section of certificates. (Issue #1269)\n\n\n1.24.1 (2018-11-02)\n===================\n\n* Remove quadratic behavior within ``GzipDecoder.decompress()`` (Issue #1467)\n\n* Restored functionality of ``ciphers`` parameter for ``create_urllib3_context()``. (Issue #1462)\n\n\n1.24 (2018-10-16)\n=================\n\n* Allow key_server_hostname to be specified when initializing a PoolManager to allow custom SNI to be overridden. (Pull #1449)\n\n* Test against Python 3.7 on AppVeyor. (Pull #1453)\n\n* Early-out ipv6 checks when running on App Engine. (Pull #1450)\n\n* Change ambiguous description of backoff_factor (Pull #1436)\n\n* Add ability to handle multiple Content-Encodings (Issue #1441 and Pull #1442)\n\n* Skip DNS names that can't be idna-decoded when using pyOpenSSL (Issue #1405).\n\n* Add a server_hostname parameter to HTTPSConnection which allows for\n  overriding the SNI hostname sent in the handshake. (Pull #1397)\n\n* Drop support for EOL Python 2.6 (Pull #1429 and Pull #1430)\n\n* Fixed bug where responses with header Content-Type: message/* erroneously\n  raised HeaderParsingError, resulting in a warning being logged. (Pull #1439)\n\n* Move urllib3 to src/urllib3 (Pull #1409)\n\n\n1.23 (2018-06-04)\n=================\n\n* Allow providing a list of headers to strip from requests when redirecting\n  to a different host. Defaults to the ``Authorization`` header. Different\n  headers can be set via ``Retry.remove_headers_on_redirect``. (Issue #1316)\n\n* Fix ``util.selectors._fileobj_to_fd`` to accept ``long`` (Issue #1247).\n\n* Dropped Python 3.3 support. (Pull #1242)\n\n* Put the connection back in the pool when calling stream() or read_chunked() on\n  a chunked HEAD response. (Issue #1234)\n\n* Fixed pyOpenSSL-specific ssl client authentication issue when clients\n  attempted to auth via certificate + chain (Issue #1060)\n\n* Add the port to the connectionpool connect print (Pull #1251)\n\n* Don't use the ``uuid`` module to create multipart data boundaries. (Pull #1380)\n\n* ``read_chunked()`` on a closed response returns no chunks. (Issue #1088)\n\n* Add Python 2.6 support to ``contrib.securetransport`` (Pull #1359)\n\n* Added support for auth info in url for SOCKS proxy (Pull #1363)\n\n\n1.22 (2017-07-20)\n=================\n\n* Fixed missing brackets in ``HTTP CONNECT`` when connecting to IPv6 address via\n  IPv6 proxy. (Issue #1222)\n\n* Made the connection pool retry on ``SSLError``.  The original ``SSLError``\n  is available on ``MaxRetryError.reason``. (Issue #1112)\n\n* Drain and release connection before recursing on retry/redirect.  Fixes\n  deadlocks with a blocking connectionpool. (Issue #1167)\n\n* Fixed compatibility for cookiejar. (Issue #1229)\n\n* pyopenssl: Use vendored version of ``six``. (Issue #1231)\n\n\n1.21.1 (2017-05-02)\n===================\n\n* Fixed SecureTransport issue that would cause long delays in response body\n  delivery. (Pull #1154)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``socket_options`` flag to the ``PoolManager``.  (Issue #1165)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``assert_hostname`` or ``assert_fingerprint`` flag to the ``PoolManager``.\n  (Pull #1157)\n\n\n1.21 (2017-04-25)\n=================\n\n* Improved performance of certain selector system calls on Python 3.5 and\n  later. (Pull #1095)\n\n* Resolved issue where the PyOpenSSL backend would not wrap SysCallError\n  exceptions appropriately when sending data. (Pull #1125)\n\n* Selectors now detects a monkey-patched select module after import for modules\n  that patch the select module like eventlet, greenlet. (Pull #1128)\n\n* Reduced memory consumption when streaming zlib-compressed responses\n  (as opposed to raw deflate streams). (Pull #1129)\n\n* Connection pools now use the entire request context when constructing the\n  pool key. (Pull #1016)\n\n* ``PoolManager.connection_from_*`` methods now accept a new keyword argument,\n  ``pool_kwargs``, which are merged with the existing ``connection_pool_kw``.\n  (Pull #1016)\n\n* Add retry counter for ``status_forcelist``. (Issue #1147)\n\n* Added ``contrib`` module for using SecureTransport on macOS:\n  ``urllib3.contrib.securetransport``.  (Pull #1122)\n\n* urllib3 now only normalizes the case of ``http://`` and ``https://`` schemes:\n  for schemes it does not recognise, it assumes they are case-sensitive and\n  leaves them unchanged.\n  (Issue #1080)\n\n\n1.20 (2017-01-19)\n=================\n\n* Added support for waiting for I/O using selectors other than select,\n  improving urllib3's behaviour with large numbers of concurrent connections.\n  (Pull #1001)\n\n* Updated the date for the system clock check. (Issue #1005)\n\n* ConnectionPools now correctly consider hostnames to be case-insensitive.\n  (Issue #1032)\n\n* Outdated versions of PyOpenSSL now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Pull #1063)\n\n* Outdated versions of cryptography now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Issue #1044)\n\n* Automatically attempt to rewind a file-like body object when a request is\n  retried or redirected. (Pull #1039)\n\n* Fix some bugs that occur when modules incautiously patch the queue module.\n  (Pull #1061)\n\n* Prevent retries from occurring on read timeouts for which the request method\n  was not in the method whitelist. (Issue #1059)\n\n* Changed the PyOpenSSL contrib module to lazily load idna to avoid\n  unnecessarily bloating the memory of programs that don't need it. (Pull\n  #1076)\n\n* Add support for IPv6 literals with zone identifiers. (Pull #1013)\n\n* Added support for socks5h:// and socks4a:// schemes when working with SOCKS\n  proxies, and controlled remote DNS appropriately. (Issue #1035)\n\n\n1.19.1 (2016-11-16)\n===================\n\n* Fixed AppEngine import that didn't function on Python 3.5. (Pull #1025)\n\n\n1.19 (2016-11-03)\n=================\n\n* urllib3 now respects Retry-After headers on 413, 429, and 503 responses when\n  using the default retry logic. (Pull #955)\n\n* Remove markers from setup.py to assist ancient setuptools versions. (Issue\n  #986)\n\n* Disallow superscripts and other integerish things in URL ports. (Issue #989)\n\n* Allow urllib3's HTTPResponse.stream() method to continue to work with\n  non-httplib underlying FPs. (Pull #990)\n\n* Empty filenames in multipart headers are now emitted as such, rather than\n  being suppressed. (Issue #1015)\n\n* Prefer user-supplied Host headers on chunked uploads. (Issue #1009)\n\n\n1.18.1 (2016-10-27)\n===================\n\n* CVE-2016-9015. Users who are using urllib3 version 1.17 or 1.18 along with\n  PyOpenSSL injection and OpenSSL 1.1.0 *must* upgrade to this version. This\n  release fixes a vulnerability whereby urllib3 in the above configuration\n  would silently fail to validate TLS certificates due to erroneously setting\n  invalid flags in OpenSSL's ``SSL_CTX_set_verify`` function. These erroneous\n  flags do not cause a problem in OpenSSL versions before 1.1.0, which\n  interprets the presence of any flag as requesting certificate validation.\n\n  There is no PR for this patch, as it was prepared for simultaneous disclosure\n  and release. The master branch received the same fix in Pull #1010.\n\n\n1.18 (2016-09-26)\n=================\n\n* Fixed incorrect message for IncompleteRead exception. (Pull #973)\n\n* Accept ``iPAddress`` subject alternative name fields in TLS certificates.\n  (Issue #258)\n\n* Fixed consistency of ``HTTPResponse.closed`` between Python 2 and 3.\n  (Issue #977)\n\n* Fixed handling of wildcard certificates when using PyOpenSSL. (Issue #979)\n\n\n1.17 (2016-09-06)\n=================\n\n* Accept ``SSLContext`` objects for use in SSL/TLS negotiation. (Issue #835)\n\n* ConnectionPool debug log now includes scheme, host, and port. (Issue #897)\n\n* Substantially refactored documentation. (Issue #887)\n\n* Used URLFetch default timeout on AppEngine, rather than hardcoding our own.\n  (Issue #858)\n\n* Normalize the scheme and host in the URL parser (Issue #833)\n\n* ``HTTPResponse`` contains the last ``Retry`` object, which now also\n  contains retries history. (Issue #848)\n\n* Timeout can no longer be set as boolean, and must be greater than zero.\n  (Pull #924)\n\n* Removed pyasn1 and ndg-httpsclient from dependencies used for PyOpenSSL. We\n  now use cryptography and idna, both of which are already dependencies of\n  PyOpenSSL. (Pull #930)\n\n* Fixed infinite loop in ``stream`` when amt=None. (Issue #928)\n\n* Try to use the operating system's certificates when we are using an\n  ``SSLContext``. (Pull #941)\n\n* Updated cipher suite list to allow ChaCha20+Poly1305. AES-GCM is preferred to\n  ChaCha20, but ChaCha20 is then preferred to everything else. (Pull #947)\n\n* Updated cipher suite list to remove 3DES-based cipher suites. (Pull #958)\n\n* Removed the cipher suite fallback to allow HIGH ciphers. (Pull #958)\n\n* Implemented ``length_remaining`` to determine remaining content\n  to be read. (Pull #949)\n\n* Implemented ``enforce_content_length`` to enable exceptions when\n  incomplete data chunks are received. (Pull #949)\n\n* Dropped connection start, dropped connection reset, redirect, forced retry,\n  and new HTTPS connection log levels to DEBUG, from INFO. (Pull #967)\n\n\n1.16 (2016-06-11)\n=================\n\n* Disable IPv6 DNS when IPv6 connections are not possible. (Issue #840)\n\n* Provide ``key_fn_by_scheme`` pool keying mechanism that can be\n  overridden. (Issue #830)\n\n* Normalize scheme and host to lowercase for pool keys, and include\n  ``source_address``. (Issue #830)\n\n* Cleaner exception chain in Python 3 for ``_make_request``.\n  (Issue #861)\n\n* Fixed installing ``urllib3[socks]`` extra. (Issue #864)\n\n* Fixed signature of ``ConnectionPool.close`` so it can actually safely be\n  called by subclasses. (Issue #873)\n\n* Retain ``release_conn`` state across retries. (Issues #651, #866)\n\n* Add customizable ``HTTPConnectionPool.ResponseCls``, which defaults to\n  ``HTTPResponse`` but can be replaced with a subclass. (Issue #879)\n\n\n1.15.1 (2016-04-11)\n===================\n\n* Fix packaging to include backports module. (Issue #841)\n\n\n1.15 (2016-04-06)\n=================\n\n* Added Retry(raise_on_status=False). (Issue #720)\n\n* Always use setuptools, no more distutils fallback. (Issue #785)\n\n* Dropped support for Python 3.2. (Issue #786)\n\n* Chunked transfer encoding when requesting with ``chunked=True``.\n  (Issue #790)\n\n* Fixed regression with IPv6 port parsing. (Issue #801)\n\n* Append SNIMissingWarning messages to allow users to specify it in\n  the PYTHONWARNINGS environment variable. (Issue #816)\n\n* Handle unicode headers in Py2. (Issue #818)\n\n* Log certificate when there is a hostname mismatch. (Issue #820)\n\n* Preserve order of request/response headers. (Issue #821)\n\n\n1.14 (2015-12-29)\n=================\n\n* contrib: SOCKS proxy support! (Issue #762)\n\n* Fixed AppEngine handling of transfer-encoding header and bug\n  in Timeout defaults checking. (Issue #763)\n\n\n1.13.1 (2015-12-18)\n===================\n\n* Fixed regression in IPv6 + SSL for match_hostname. (Issue #761)\n\n\n1.13 (2015-12-14)\n=================\n\n* Fixed ``pip install urllib3[secure]`` on modern pip. (Issue #706)\n\n* pyopenssl: Fixed SSL3_WRITE_PENDING error. (Issue #717)\n\n* pyopenssl: Support for TLSv1.1 and TLSv1.2. (Issue #696)\n\n* Close connections more defensively on exception. (Issue #734)\n\n* Adjusted ``read_chunked`` to handle gzipped, chunk-encoded bodies without\n  repeatedly flushing the decoder, to function better on Jython. (Issue #743)\n\n* Accept ``ca_cert_dir`` for SSL-related PoolManager configuration. (Issue #758)\n\n\n1.12 (2015-09-03)\n=================\n\n* Rely on ``six`` for importing ``httplib`` to work around\n  conflicts with other Python 3 shims. (Issue #688)\n\n* Add support for directories of certificate authorities, as supported by\n  OpenSSL. (Issue #701)\n\n* New exception: ``NewConnectionError``, raised when we fail to establish\n  a new connection, usually ``ECONNREFUSED`` socket error.\n\n\n1.11 (2015-07-21)\n=================\n\n* When ``ca_certs`` is given, ``cert_reqs`` defaults to\n  ``'CERT_REQUIRED'``. (Issue #650)\n\n* ``pip install urllib3[secure]`` will install Certifi and\n  PyOpenSSL as dependencies. (Issue #678)\n\n* Made ``HTTPHeaderDict`` usable as a ``headers`` input value\n  (Issues #632, #679)\n\n* Added `urllib3.contrib.appengine <https://urllib3.readthedocs.io/en/latest/contrib.html#google-app-engine>`_\n  which has an ``AppEngineManager`` for using ``URLFetch`` in a\n  Google AppEngine environment. (Issue #664)\n\n* Dev: Added test suite for AppEngine. (Issue #631)\n\n* Fix performance regression when using PyOpenSSL. (Issue #626)\n\n* Passing incorrect scheme (e.g. ``foo://``) will raise\n  ``ValueError`` instead of ``AssertionError`` (backwards\n  compatible for now, but please migrate). (Issue #640)\n\n* Fix pools not getting replenished when an error occurs during a\n  request using ``release_conn=False``. (Issue #644)\n\n* Fix pool-default headers not applying for url-encoded requests\n  like GET. (Issue #657)\n\n* log.warning in Python 3 when headers are skipped due to parsing\n  errors. (Issue #642)\n\n* Close and discard connections if an error occurs during read.\n  (Issue #660)\n\n* Fix host parsing for IPv6 proxies. (Issue #668)\n\n* Separate warning type SubjectAltNameWarning, now issued once\n  per host. (Issue #671)\n\n* Fix ``httplib.IncompleteRead`` not getting converted to\n  ``ProtocolError`` when using ``HTTPResponse.stream()``\n  (Issue #674)\n\n1.10.4 (2015-05-03)\n===================\n\n* Migrate tests to Tornado 4. (Issue #594)\n\n* Append default warning configuration rather than overwrite.\n  (Issue #603)\n\n* Fix streaming decoding regression. (Issue #595)\n\n* Fix chunked requests losing state across keep-alive connections.\n  (Issue #599)\n\n* Fix hanging when chunked HEAD response has no body. (Issue #605)\n\n\n1.10.3 (2015-04-21)\n===================\n\n* Emit ``InsecurePlatformWarning`` when SSLContext object is missing.\n  (Issue #558)\n\n* Fix regression of duplicate header keys being discarded.\n  (Issue #563)\n\n* ``Response.stream()`` returns a generator for chunked responses.\n  (Issue #560)\n\n* Set upper-bound timeout when waiting for a socket in PyOpenSSL.\n  (Issue #585)\n\n* Work on platforms without `ssl` module for plain HTTP requests.\n  (Issue #587)\n\n* Stop relying on the stdlib's default cipher list. (Issue #588)\n\n\n1.10.2 (2015-02-25)\n===================\n\n* Fix file descriptor leakage on retries. (Issue #548)\n\n* Removed RC4 from default cipher list. (Issue #551)\n\n* Header performance improvements. (Issue #544)\n\n* Fix PoolManager not obeying redirect retry settings. (Issue #553)\n\n\n1.10.1 (2015-02-10)\n===================\n\n* Pools can be used as context managers. (Issue #545)\n\n* Don't re-use connections which experienced an SSLError. (Issue #529)\n\n* Don't fail when gzip decoding an empty stream. (Issue #535)\n\n* Add sha256 support for fingerprint verification. (Issue #540)\n\n* Fixed handling of header values containing commas. (Issue #533)\n\n\n1.10 (2014-12-14)\n=================\n\n* Disabled SSLv3. (Issue #473)\n\n* Add ``Url.url`` property to return the composed url string. (Issue #394)\n\n* Fixed PyOpenSSL + gevent ``WantWriteError``. (Issue #412)\n\n* ``MaxRetryError.reason`` will always be an exception, not string.\n  (Issue #481)\n\n* Fixed SSL-related timeouts not being detected as timeouts. (Issue #492)\n\n* Py3: Use ``ssl.create_default_context()`` when available. (Issue #473)\n\n* Emit ``InsecureRequestWarning`` for *every* insecure HTTPS request.\n  (Issue #496)\n\n* Emit ``SecurityWarning`` when certificate has no ``subjectAltName``.\n  (Issue #499)\n\n* Close and discard sockets which experienced SSL-related errors.\n  (Issue #501)\n\n* Handle ``body`` param in ``.request(...)``. (Issue #513)\n\n* Respect timeout with HTTPS proxy. (Issue #505)\n\n* PyOpenSSL: Handle ZeroReturnError exception. (Issue #520)\n\n\n1.9.1 (2014-09-13)\n==================\n\n* Apply socket arguments before binding. (Issue #427)\n\n* More careful checks if fp-like object is closed. (Issue #435)\n\n* Fixed packaging issues of some development-related files not\n  getting included. (Issue #440)\n\n* Allow performing *only* fingerprint verification. (Issue #444)\n\n* Emit ``SecurityWarning`` if system clock is waaay off. (Issue #445)\n\n* Fixed PyOpenSSL compatibility with PyPy. (Issue #450)\n\n* Fixed ``BrokenPipeError`` and ``ConnectionError`` handling in Py3.\n  (Issue #443)\n\n\n\n1.9 (2014-07-04)\n================\n\n* Shuffled around development-related files. If you're maintaining a distro\n  package of urllib3, you may need to tweak things. (Issue #415)\n\n* Unverified HTTPS requests will trigger a warning on the first request. See\n  our new `security documentation\n  <https://urllib3.readthedocs.io/en/latest/security.html>`_ for details.\n  (Issue #426)\n\n* New retry logic and ``urllib3.util.retry.Retry`` configuration object.\n  (Issue #326)\n\n* All raised exceptions should now wrapped in a\n  ``urllib3.exceptions.HTTPException``-extending exception. (Issue #326)\n\n* All errors during a retry-enabled request should be wrapped in\n  ``urllib3.exceptions.MaxRetryError``, including timeout-related exceptions\n  which were previously exempt. Underlying error is accessible from the\n  ``.reason`` property. (Issue #326)\n\n* ``urllib3.exceptions.ConnectionError`` renamed to\n  ``urllib3.exceptions.ProtocolError``. (Issue #326)\n\n* Errors during response read (such as IncompleteRead) are now wrapped in\n  ``urllib3.exceptions.ProtocolError``. (Issue #418)\n\n* Requesting an empty host will raise ``urllib3.exceptions.LocationValueError``.\n  (Issue #417)\n\n* Catch read timeouts over SSL connections as\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #419)\n\n* Apply socket arguments before connecting. (Issue #427)\n\n\n1.8.3 (2014-06-23)\n==================\n\n* Fix TLS verification when using a proxy in Python 3.4.1. (Issue #385)\n\n* Add ``disable_cache`` option to ``urllib3.util.make_headers``. (Issue #393)\n\n* Wrap ``socket.timeout`` exception with\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #399)\n\n* Fixed proxy-related bug where connections were being reused incorrectly.\n  (Issues #366, #369)\n\n* Added ``socket_options`` keyword parameter which allows to define\n  ``setsockopt`` configuration of new sockets. (Issue #397)\n\n* Removed ``HTTPConnection.tcp_nodelay`` in favor of\n  ``HTTPConnection.default_socket_options``. (Issue #397)\n\n* Fixed ``TypeError`` bug in Python 2.6.4. (Issue #411)\n\n\n1.8.2 (2014-04-17)\n==================\n\n* Fix ``urllib3.util`` not being included in the package.\n\n\n1.8.1 (2014-04-17)\n==================\n\n* Fix AppEngine bug of HTTPS requests going out as HTTP. (Issue #356)\n\n* Don't install ``dummyserver`` into ``site-packages`` as it's only needed\n  for the test suite. (Issue #362)\n\n* Added support for specifying ``source_address``. (Issue #352)\n\n\n1.8 (2014-03-04)\n================\n\n* Improved url parsing in ``urllib3.util.parse_url`` (properly parse '@' in\n  username, and blank ports like 'hostname:').\n\n* New ``urllib3.connection`` module which contains all the HTTPConnection\n  objects.\n\n* Several ``urllib3.util.Timeout``-related fixes. Also changed constructor\n  signature to a more sensible order. [Backwards incompatible]\n  (Issues #252, #262, #263)\n\n* Use ``backports.ssl_match_hostname`` if it's installed. (Issue #274)\n\n* Added ``.tell()`` method to ``urllib3.response.HTTPResponse`` which\n  returns the number of bytes read so far. (Issue #277)\n\n* Support for platforms without threading. (Issue #289)\n\n* Expand default-port comparison in ``HTTPConnectionPool.is_same_host``\n  to allow a pool with no specified port to be considered equal to to an\n  HTTP/HTTPS url with port 80/443 explicitly provided. (Issue #305)\n\n* Improved default SSL/TLS settings to avoid vulnerabilities.\n  (Issue #309)\n\n* Fixed ``urllib3.poolmanager.ProxyManager`` not retrying on connect errors.\n  (Issue #310)\n\n* Disable Nagle's Algorithm on the socket for non-proxies. A subset of requests\n  will send the entire HTTP request ~200 milliseconds faster; however, some of\n  the resulting TCP packets will be smaller. (Issue #254)\n\n* Increased maximum number of SubjectAltNames in ``urllib3.contrib.pyopenssl``\n  from the default 64 to 1024 in a single certificate. (Issue #318)\n\n* Headers are now passed and stored as a custom\n  ``urllib3.collections_.HTTPHeaderDict`` object rather than a plain ``dict``.\n  (Issue #329, #333)\n\n* Headers no longer lose their case on Python 3. (Issue #236)\n\n* ``urllib3.contrib.pyopenssl`` now uses the operating system's default CA\n  certificates on inject. (Issue #332)\n\n* Requests with ``retries=False`` will immediately raise any exceptions without\n  wrapping them in ``MaxRetryError``. (Issue #348)\n\n* Fixed open socket leak with SSL-related failures. (Issue #344, #348)\n\n\n1.7.1 (2013-09-25)\n==================\n\n* Added granular timeout support with new ``urllib3.util.Timeout`` class.\n  (Issue #231)\n\n* Fixed Python 3.4 support. (Issue #238)\n\n\n1.7 (2013-08-14)\n================\n\n* More exceptions are now pickle-able, with tests. (Issue #174)\n\n* Fixed redirecting with relative URLs in Location header. (Issue #178)\n\n* Support for relative urls in ``Location: ...`` header. (Issue #179)\n\n* ``urllib3.response.HTTPResponse`` now inherits from ``io.IOBase`` for bonus\n  file-like functionality. (Issue #187)\n\n* Passing ``assert_hostname=False`` when creating a HTTPSConnectionPool will\n  skip hostname verification for SSL connections. (Issue #194)\n\n* New method ``urllib3.response.HTTPResponse.stream(...)`` which acts as a\n  generator wrapped around ``.read(...)``. (Issue #198)\n\n* IPv6 url parsing enforces brackets around the hostname. (Issue #199)\n\n* Fixed thread race condition in\n  ``urllib3.poolmanager.PoolManager.connection_from_host(...)`` (Issue #204)\n\n* ``ProxyManager`` requests now include non-default port in ``Host: ...``\n  header. (Issue #217)\n\n* Added HTTPS proxy support in ``ProxyManager``. (Issue #170 #139)\n\n* New ``RequestField`` object can be passed to the ``fields=...`` param which\n  can specify headers. (Issue #220)\n\n* Raise ``urllib3.exceptions.ProxyError`` when connecting to proxy fails.\n  (Issue #221)\n\n* Use international headers when posting file names. (Issue #119)\n\n* Improved IPv6 support. (Issue #203)\n\n\n1.6 (2013-04-25)\n================\n\n* Contrib: Optional SNI support for Py2 using PyOpenSSL. (Issue #156)\n\n* ``ProxyManager`` automatically adds ``Host: ...`` header if not given.\n\n* Improved SSL-related code. ``cert_req`` now optionally takes a string like\n  \"REQUIRED\" or \"NONE\". Same with ``ssl_version`` takes strings like \"SSLv23\"\n  The string values reflect the suffix of the respective constant variable.\n  (Issue #130)\n\n* Vendored ``socksipy`` now based on Anorov's fork which handles unexpectedly\n  closed proxy connections and larger read buffers. (Issue #135)\n\n* Ensure the connection is closed if no data is received, fixes connection leak\n  on some platforms. (Issue #133)\n\n* Added SNI support for SSL/TLS connections on Py32+. (Issue #89)\n\n* Tests fixed to be compatible with Py26 again. (Issue #125)\n\n* Added ability to choose SSL version by passing an ``ssl.PROTOCOL_*`` constant\n  to the ``ssl_version`` parameter of ``HTTPSConnectionPool``. (Issue #109)\n\n* Allow an explicit content type to be specified when encoding file fields.\n  (Issue #126)\n\n* Exceptions are now pickleable, with tests. (Issue #101)\n\n* Fixed default headers not getting passed in some cases. (Issue #99)\n\n* Treat \"content-encoding\" header value as case-insensitive, per RFC 2616\n  Section 3.5. (Issue #110)\n\n* \"Connection Refused\" SocketErrors will get retried rather than raised.\n  (Issue #92)\n\n* Updated vendored ``six``, no longer overrides the global ``six`` module\n  namespace. (Issue #113)\n\n* ``urllib3.exceptions.MaxRetryError`` contains a ``reason`` property holding\n  the exception that prompted the final retry. If ``reason is None`` then it\n  was due to a redirect. (Issue #92, #114)\n\n* Fixed ``PoolManager.urlopen()`` from not redirecting more than once.\n  (Issue #149)\n\n* Don't assume ``Content-Type: text/plain`` for multi-part encoding parameters\n  that are not files. (Issue #111)\n\n* Pass `strict` param down to ``httplib.HTTPConnection``. (Issue #122)\n\n* Added mechanism to verify SSL certificates by fingerprint (md5, sha1) or\n  against an arbitrary hostname (when connecting by IP or for misconfigured\n  servers). (Issue #140)\n\n* Streaming decompression support. (Issue #159)\n\n\n1.5 (2012-08-02)\n================\n\n* Added ``urllib3.add_stderr_logger()`` for quickly enabling STDERR debug\n  logging in urllib3.\n\n* Native full URL parsing (including auth, path, query, fragment) available in\n  ``urllib3.util.parse_url(url)``.\n\n* Built-in redirect will switch method to 'GET' if status code is 303.\n  (Issue #11)\n\n* ``urllib3.PoolManager`` strips the scheme and host before sending the request\n  uri. (Issue #8)\n\n* New ``urllib3.exceptions.DecodeError`` exception for when automatic decoding,\n  based on the Content-Type header, fails.\n\n* Fixed bug with pool depletion and leaking connections (Issue #76). Added\n  explicit connection closing on pool eviction. Added\n  ``urllib3.PoolManager.clear()``.\n\n* 99% -> 100% unit test coverage.\n\n\n1.4 (2012-06-16)\n================\n\n* Minor AppEngine-related fixes.\n\n* Switched from ``mimetools.choose_boundary`` to ``uuid.uuid4()``.\n\n* Improved url parsing. (Issue #73)\n\n* IPv6 url support. (Issue #72)\n\n\n1.3 (2012-03-25)\n================\n\n* Removed pre-1.0 deprecated API.\n\n* Refactored helpers into a ``urllib3.util`` submodule.\n\n* Fixed multipart encoding to support list-of-tuples for keys with multiple\n  values. (Issue #48)\n\n* Fixed multiple Set-Cookie headers in response not getting merged properly in\n  Python 3. (Issue #53)\n\n* AppEngine support with Py27. (Issue #61)\n\n* Minor ``encode_multipart_formdata`` fixes related to Python 3 strings vs\n  bytes.\n\n\n1.2.2 (2012-02-06)\n==================\n\n* Fixed packaging bug of not shipping ``test-requirements.txt``. (Issue #47)\n\n\n1.2.1 (2012-02-05)\n==================\n\n* Fixed another bug related to when ``ssl`` module is not available. (Issue #41)\n\n* Location parsing errors now raise ``urllib3.exceptions.LocationParseError``\n  which inherits from ``ValueError``.\n\n\n1.2 (2012-01-29)\n================\n\n* Added Python 3 support (tested on 3.2.2)\n\n* Dropped Python 2.5 support (tested on 2.6.7, 2.7.2)\n\n* Use ``select.poll`` instead of ``select.select`` for platforms that support\n  it.\n\n* Use ``Queue.LifoQueue`` instead of ``Queue.Queue`` for more aggressive\n  connection reusing. Configurable by overriding ``ConnectionPool.QueueCls``.\n\n* Fixed ``ImportError`` during install when ``ssl`` module is not available.\n  (Issue #41)\n\n* Fixed ``PoolManager`` redirects between schemes (such as HTTP -> HTTPS) not\n  completing properly. (Issue #28, uncovered by Issue #10 in v1.1)\n\n* Ported ``dummyserver`` to use ``tornado`` instead of ``webob`` +\n  ``eventlet``. Removed extraneous unsupported dummyserver testing backends.\n  Added socket-level tests.\n\n* More tests. Achievement Unlocked: 99% Coverage.\n\n\n1.1 (2012-01-07)\n================\n\n* Refactored ``dummyserver`` to its own root namespace module (used for\n  testing).\n\n* Added hostname verification for ``VerifiedHTTPSConnection`` by vendoring in\n  Py32's ``ssl_match_hostname``. (Issue #25)\n\n* Fixed cross-host HTTP redirects when using ``PoolManager``. (Issue #10)\n\n* Fixed ``decode_content`` being ignored when set through ``urlopen``. (Issue\n  #27)\n\n* Fixed timeout-related bugs. (Issues #17, #23)\n\n\n1.0.2 (2011-11-04)\n==================\n\n* Fixed typo in ``VerifiedHTTPSConnection`` which would only present as a bug if\n  you're using the object manually. (Thanks pyos)\n\n* Made RecentlyUsedContainer (and consequently PoolManager) more thread-safe by\n  wrapping the access log in a mutex. (Thanks @christer)\n\n* Made RecentlyUsedContainer more dict-like (corrected ``__delitem__`` and\n  ``__getitem__`` behaviour), with tests. Shouldn't affect core urllib3 code.\n\n\n1.0.1 (2011-10-10)\n==================\n\n* Fixed a bug where the same connection would get returned into the pool twice,\n  causing extraneous \"HttpConnectionPool is full\" log warnings.\n\n\n1.0 (2011-10-08)\n================\n\n* Added ``PoolManager`` with LRU expiration of connections (tested and\n  documented).\n* Added ``ProxyManager`` (needs tests, docs, and confirmation that it works\n  with HTTPS proxies).\n* Added optional partial-read support for responses when\n  ``preload_content=False``. You can now make requests and just read the headers\n  without loading the content.\n* Made response decoding optional (default on, same as before).\n* Added optional explicit boundary string for ``encode_multipart_formdata``.\n* Convenience request methods are now inherited from ``RequestMethods``. Old\n  helpers like ``get_url`` and ``post_url`` should be abandoned in favour of\n  the new ``request(method, url, ...)``.\n* Refactored code to be even more decoupled, reusable, and extendable.\n* License header added to ``.py`` files.\n* Embiggened the documentation: Lots of Sphinx-friendly docstrings in the code\n  and docs in ``docs/`` and on https://urllib3.readthedocs.io/.\n* Embettered all the things!\n* Started writing this file.\n\n\n0.4.1 (2011-07-17)\n==================\n\n* Minor bug fixes, code cleanup.\n\n\n0.4 (2011-03-01)\n================\n\n* Better unicode support.\n* Added ``VerifiedHTTPSConnection``.\n* Added ``NTLMConnectionPool`` in contrib.\n* Minor improvements.\n\n\n0.3.1 (2010-07-13)\n==================\n\n* Added ``assert_host_name`` optional parameter. Now compatible with proxies.\n\n\n0.3 (2009-12-10)\n================\n\n* Added HTTPS support.\n* Minor bug fixes.\n* Refactored, broken backwards compatibility with 0.2.\n* API to be treated as stable from this version forward.\n\n\n0.2 (2008-11-17)\n================\n\n* Added unit tests.\n* Bug fixes.\n\n\n0.1 (2008-11-16)\n================\n\n* First release.\n", "from __future__ import annotations\n\nimport collections\nimport contextlib\nimport gzip\nimport json\nimport logging\nimport sys\nimport typing\nimport zlib\nfrom datetime import datetime, timedelta, timezone\nfrom http.client import responses\nfrom io import BytesIO\nfrom urllib.parse import urlsplit\n\nfrom tornado import httputil\nfrom tornado.web import RequestHandler\n\nfrom urllib3.util.util import to_str\n\nlog = logging.getLogger(__name__)\n\n\nclass Response:\n    def __init__(\n        self,\n        body: str | bytes | typing.Sequence[str | bytes] = \"\",\n        status: str = \"200 OK\",\n        headers: typing.Sequence[tuple[str, str | bytes]] | None = None,\n        json: typing.Any | None = None,\n    ) -> None:\n        self.body = body\n        self.status = status\n        if json is not None:\n            self.headers = headers or [(\"Content-type\", \"application/json\")]\n            self.body = json\n        else:\n            self.headers = headers or [(\"Content-type\", \"text/plain\")]\n\n    def __call__(self, request_handler: RequestHandler) -> None:\n        status, reason = self.status.split(\" \", 1)\n        request_handler.set_status(int(status), reason)\n        for header, value in self.headers:\n            request_handler.add_header(header, value)\n\n        if isinstance(self.body, str):\n            request_handler.write(self.body.encode())\n        elif isinstance(self.body, bytes):\n            request_handler.write(self.body)\n        # chunked\n        else:\n            for item in self.body:\n                if not isinstance(item, bytes):\n                    item = item.encode(\"utf8\")\n                request_handler.write(item)\n                request_handler.flush()\n\n\nRETRY_TEST_NAMES: dict[str, int] = collections.defaultdict(int)\n\n\ndef request_params(request: httputil.HTTPServerRequest) -> dict[str, bytes]:\n    params = {}\n    for k, v in request.arguments.items():\n        params[k] = next(iter(v))\n    return params\n\n\nclass TestingApp(RequestHandler):\n    \"\"\"\n    Simple app that performs various operations, useful for testing an HTTP\n    library.\n\n    Given any path, it will attempt to load a corresponding local method if\n    it exists. Status code 200 indicates success, 400 indicates failure. Each\n    method has its own conditions for success/failure.\n    \"\"\"\n\n    def get(self) -> None:\n        \"\"\"Handle GET requests\"\"\"\n        self._call_method()\n\n    def post(self) -> None:\n        \"\"\"Handle POST requests\"\"\"\n        self._call_method()\n\n    def put(self) -> None:\n        \"\"\"Handle PUT requests\"\"\"\n        self._call_method()\n\n    def options(self) -> None:\n        \"\"\"Handle OPTIONS requests\"\"\"\n        self._call_method()\n\n    def head(self) -> None:\n        \"\"\"Handle HEAD requests\"\"\"\n        self._call_method()\n\n    def _call_method(self) -> None:\n        \"\"\"Call the correct method in this class based on the incoming URI\"\"\"\n        req = self.request\n\n        path = req.path[:]\n        if not path.startswith(\"/\"):\n            path = urlsplit(path).path\n\n        target = path[1:].split(\"/\", 1)[0]\n        method = getattr(self, target, self.index)\n\n        resp = method(req)\n\n        if dict(resp.headers).get(\"Connection\") == \"close\":\n            # FIXME: Can we kill the connection somehow?\n            pass\n\n        resp(self)\n\n    def index(self, _request: httputil.HTTPServerRequest) -> Response:\n        \"Render simple message\"\n        return Response(\"Dummy server!\")\n\n    def certificate(self, request: httputil.HTTPServerRequest) -> Response:\n        \"\"\"Return the requester's certificate.\"\"\"\n        cert = request.get_ssl_certificate()\n        assert isinstance(cert, dict)\n        subject = {}\n        if cert is not None:\n            subject = {k: v for (k, v) in [y for z in cert[\"subject\"] for y in z]}\n        return Response(json.dumps(subject))\n\n    def alpn_protocol(self, request: httputil.HTTPServerRequest) -> Response:\n        \"\"\"Return the selected ALPN protocol.\"\"\"\n        assert request.connection is not None\n        proto = request.connection.stream.socket.selected_alpn_protocol()  # type: ignore[attr-defined]\n        return Response(proto.encode(\"utf8\") if proto is not None else \"\")\n\n    def source_address(self, request: httputil.HTTPServerRequest) -> Response:\n        \"\"\"Return the requester's IP address.\"\"\"\n        return Response(request.remote_ip)  # type: ignore[arg-type]\n\n    def set_up(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        test_type = params.get(\"test_type\")\n        test_id = params.get(\"test_id\")\n        if test_id:\n            print(f\"\\nNew test {test_type!r}: {test_id!r}\")\n        else:\n            print(f\"\\nNew test {test_type!r}\")\n        return Response(\"Dummy server is ready!\")\n\n    def specific_method(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Confirm that the request matches the desired method type\"\n        params = request_params(request)\n        method = params.get(\"method\")\n        method_str = method.decode() if method else None\n\n        if request.method != method_str:\n            return Response(\n                f\"Wrong method: {method_str} != {request.method}\",\n                status=\"400 Bad Request\",\n            )\n        return Response()\n\n    def upload(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Confirm that the uploaded file conforms to specification\"\n        params = request_params(request)\n        # FIXME: This is a huge broken mess\n        param = params.get(\"upload_param\", b\"myfile\").decode(\"ascii\")\n        filename = params.get(\"upload_filename\", b\"\").decode(\"utf-8\")\n        size = int(params.get(\"upload_size\", \"0\"))\n        files_ = request.files.get(param)\n        assert files_ is not None\n\n        if len(files_) != 1:\n            return Response(\n                f\"Expected 1 file for '{param}', not {len(files_)}\",\n                status=\"400 Bad Request\",\n            )\n        file_ = files_[0]\n\n        data = file_[\"body\"]\n        if int(size) != len(data):\n            return Response(\n                f\"Wrong size: {int(size)} != {len(data)}\", status=\"400 Bad Request\"\n            )\n\n        got_filename = file_[\"filename\"]\n        if isinstance(got_filename, bytes):\n            got_filename = got_filename.decode(\"utf-8\")\n\n        # Tornado can leave the trailing \\n in place on the filename.\n        if filename != got_filename:\n            return Response(\n                f\"Wrong filename: {filename} != {file_.filename}\",\n                status=\"400 Bad Request\",\n            )\n\n        return Response()\n\n    def redirect(self, request: httputil.HTTPServerRequest) -> Response:  # type: ignore[override]\n        \"Perform a redirect to ``target``\"\n        params = request_params(request)\n        target = params.get(\"target\", \"/\")\n        status = params.get(\"status\", b\"303 See Other\").decode(\"latin-1\")\n        if len(status) == 3:\n            status = f\"{status} Redirect\"\n\n        headers = [(\"Location\", target)]\n        return Response(status=status, headers=headers)\n\n    def not_found(self, request: httputil.HTTPServerRequest) -> Response:\n        return Response(\"Not found\", status=\"404 Not Found\")\n\n    def multi_redirect(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Performs a redirect chain based on ``redirect_codes``\"\n        params = request_params(request)\n        codes = params.get(\"redirect_codes\", b\"200\").decode(\"utf-8\")\n        head, tail = codes.split(\",\", 1) if \",\" in codes else (codes, None)\n        assert head is not None\n        status = f\"{head} {responses[int(head)]}\"\n        if not tail:\n            return Response(\"Done redirecting\", status=status)\n\n        headers = [(\"Location\", f\"/multi_redirect?redirect_codes={tail}\")]\n        return Response(status=status, headers=headers)\n\n    def keepalive(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        if params.get(\"close\", b\"0\") == b\"1\":\n            headers = [(\"Connection\", \"close\")]\n            return Response(\"Closing\", headers=headers)\n\n        headers = [(\"Connection\", \"keep-alive\")]\n        return Response(\"Keeping alive\", headers=headers)\n\n    def echo_params(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        echod = sorted((to_str(k), to_str(v)) for k, v in params.items())\n        return Response(repr(echod))\n\n    def echo(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Echo back the params\"\n        if request.method == \"GET\":\n            return Response(request.query)\n\n        return Response(request.body)\n\n    def echo_json(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Echo back the JSON\"\n        return Response(json=request.body, headers=list(request.headers.items()))\n\n    def echo_uri(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Echo back the requested URI\"\n        assert request.uri is not None\n        return Response(request.uri)\n\n    def encodingrequest(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Check for UA accepting gzip/deflate encoding\"\n        data = b\"hello, world!\"\n        encoding = request.headers.get(\"Accept-Encoding\", \"\")\n        headers = None\n        if encoding == \"gzip\":\n            headers = [(\"Content-Encoding\", \"gzip\")]\n            file_ = BytesIO()\n            with contextlib.closing(\n                gzip.GzipFile(\"\", mode=\"w\", fileobj=file_)\n            ) as zipfile:\n                zipfile.write(data)\n            data = file_.getvalue()\n        elif encoding == \"deflate\":\n            headers = [(\"Content-Encoding\", \"deflate\")]\n            data = zlib.compress(data)\n        elif encoding == \"garbage-gzip\":\n            headers = [(\"Content-Encoding\", \"gzip\")]\n            data = b\"garbage\"\n        elif encoding == \"garbage-deflate\":\n            headers = [(\"Content-Encoding\", \"deflate\")]\n            data = b\"garbage\"\n        return Response(data, headers=headers)\n\n    def headers(self, request: httputil.HTTPServerRequest) -> Response:\n        return Response(json.dumps(dict(request.headers)))\n\n    def multi_headers(self, request: httputil.HTTPServerRequest) -> Response:\n        return Response(json.dumps({\"headers\": list(request.headers.get_all())}))\n\n    def successful_retry(self, request: httputil.HTTPServerRequest) -> Response:\n        \"\"\"Handler which will return an error and then success\n\n        It's not currently very flexible as the number of retries is hard-coded.\n        \"\"\"\n        test_name = request.headers.get(\"test-name\", None)\n        if not test_name:\n            return Response(\"test-name header not set\", status=\"400 Bad Request\")\n\n        RETRY_TEST_NAMES[test_name] += 1\n\n        if RETRY_TEST_NAMES[test_name] >= 2:\n            return Response(\"Retry successful!\")\n        else:\n            return Response(\"need to keep retrying!\", status=\"418 I'm A Teapot\")\n\n    def chunked(self, request: httputil.HTTPServerRequest) -> Response:\n        return Response([\"123\"] * 4)\n\n    def chunked_gzip(self, request: httputil.HTTPServerRequest) -> Response:\n        chunks = []\n        compressor = zlib.compressobj(6, zlib.DEFLATED, 16 + zlib.MAX_WBITS)\n\n        for uncompressed in [b\"123\"] * 4:\n            chunks.append(compressor.compress(uncompressed))\n\n        chunks.append(compressor.flush())\n\n        return Response(chunks, headers=[(\"Content-Encoding\", \"gzip\")])\n\n    def nbytes(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        length = int(params[\"length\"])\n        data = b\"1\" * length\n        return Response(data, headers=[(\"Content-Type\", \"application/octet-stream\")])\n\n    def status(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        status = params.get(\"status\", b\"200 OK\").decode(\"latin-1\")\n\n        return Response(status=status)\n\n    def retry_after(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        if datetime.now() - self.application.last_req < timedelta(seconds=1):  # type: ignore[attr-defined]\n            status = params.get(\"status\", b\"429 Too Many Requests\")\n            return Response(\n                status=status.decode(\"utf-8\"), headers=[(\"Retry-After\", \"1\")]\n            )\n\n        self.application.last_req = datetime.now()  # type: ignore[attr-defined]\n\n        return Response(status=\"200 OK\")\n\n    def redirect_after(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Perform a redirect to ``target``\"\n        params = request_params(request)\n        date = params.get(\"date\")\n        if date:\n            retry_after = str(\n                httputil.format_timestamp(\n                    datetime.fromtimestamp(float(date), tz=timezone.utc)\n                )\n            )\n        else:\n            retry_after = \"1\"\n        target = params.get(\"target\", \"/\")\n        headers = [(\"Location\", target), (\"Retry-After\", retry_after)]\n        return Response(status=\"303 See Other\", headers=headers)\n\n    def shutdown(self, request: httputil.HTTPServerRequest) -> typing.NoReturn:\n        sys.exit()\n", "from __future__ import annotations\n\nimport typing\nfrom collections import OrderedDict\nfrom enum import Enum, auto\nfrom threading import RLock\n\nif typing.TYPE_CHECKING:\n    # We can only import Protocol if TYPE_CHECKING because it's a development\n    # dependency, and is not available at runtime.\n    from typing import Protocol\n\n    class HasGettableStringKeys(Protocol):\n        def keys(self) -> typing.Iterator[str]:\n            ...\n\n        def __getitem__(self, key: str) -> str:\n            ...\n\n\n__all__ = [\"RecentlyUsedContainer\", \"HTTPHeaderDict\"]\n\n\n# Key type\n_KT = typing.TypeVar(\"_KT\")\n# Value type\n_VT = typing.TypeVar(\"_VT\")\n# Default type\n_DT = typing.TypeVar(\"_DT\")\n\nValidHTTPHeaderSource = typing.Union[\n    \"HTTPHeaderDict\",\n    typing.Mapping[str, str],\n    typing.Iterable[typing.Tuple[str, str]],\n    \"HasGettableStringKeys\",\n]\n\n\nclass _Sentinel(Enum):\n    not_passed = auto()\n\n\ndef ensure_can_construct_http_header_dict(\n    potential: object,\n) -> ValidHTTPHeaderSource | None:\n    if isinstance(potential, HTTPHeaderDict):\n        return potential\n    elif isinstance(potential, typing.Mapping):\n        # Full runtime checking of the contents of a Mapping is expensive, so for the\n        # purposes of typechecking, we assume that any Mapping is the right shape.\n        return typing.cast(typing.Mapping[str, str], potential)\n    elif isinstance(potential, typing.Iterable):\n        # Similarly to Mapping, full runtime checking of the contents of an Iterable is\n        # expensive, so for the purposes of typechecking, we assume that any Iterable\n        # is the right shape.\n        return typing.cast(typing.Iterable[typing.Tuple[str, str]], potential)\n    elif hasattr(potential, \"keys\") and hasattr(potential, \"__getitem__\"):\n        return typing.cast(\"HasGettableStringKeys\", potential)\n    else:\n        return None\n\n\nclass RecentlyUsedContainer(typing.Generic[_KT, _VT], typing.MutableMapping[_KT, _VT]):\n    \"\"\"\n    Provides a thread-safe dict-like container which maintains up to\n    ``maxsize`` keys while throwing away the least-recently-used keys beyond\n    ``maxsize``.\n\n    :param maxsize:\n        Maximum number of recent elements to retain.\n\n    :param dispose_func:\n        Every time an item is evicted from the container,\n        ``dispose_func(value)`` is called.  Callback which will get called\n    \"\"\"\n\n    _container: typing.OrderedDict[_KT, _VT]\n    _maxsize: int\n    dispose_func: typing.Callable[[_VT], None] | None\n    lock: RLock\n\n    def __init__(\n        self,\n        maxsize: int = 10,\n        dispose_func: typing.Callable[[_VT], None] | None = None,\n    ) -> None:\n        super().__init__()\n        self._maxsize = maxsize\n        self.dispose_func = dispose_func\n        self._container = OrderedDict()\n        self.lock = RLock()\n\n    def __getitem__(self, key: _KT) -> _VT:\n        # Re-insert the item, moving it to the end of the eviction line.\n        with self.lock:\n            item = self._container.pop(key)\n            self._container[key] = item\n            return item\n\n    def __setitem__(self, key: _KT, value: _VT) -> None:\n        evicted_item = None\n        with self.lock:\n            # Possibly evict the existing value of 'key'\n            try:\n                # If the key exists, we'll overwrite it, which won't change the\n                # size of the pool. Because accessing a key should move it to\n                # the end of the eviction line, we pop it out first.\n                evicted_item = key, self._container.pop(key)\n                self._container[key] = value\n            except KeyError:\n                # When the key does not exist, we insert the value first so that\n                # evicting works in all cases, including when self._maxsize is 0\n                self._container[key] = value\n                if len(self._container) > self._maxsize:\n                    # If we didn't evict an existing value, and we've hit our maximum\n                    # size, then we have to evict the least recently used item from\n                    # the beginning of the container.\n                    evicted_item = self._container.popitem(last=False)\n\n        # After releasing the lock on the pool, dispose of any evicted value.\n        if evicted_item is not None and self.dispose_func:\n            _, evicted_value = evicted_item\n            self.dispose_func(evicted_value)\n\n    def __delitem__(self, key: _KT) -> None:\n        with self.lock:\n            value = self._container.pop(key)\n\n        if self.dispose_func:\n            self.dispose_func(value)\n\n    def __len__(self) -> int:\n        with self.lock:\n            return len(self._container)\n\n    def __iter__(self) -> typing.NoReturn:\n        raise NotImplementedError(\n            \"Iteration over this class is unlikely to be threadsafe.\"\n        )\n\n    def clear(self) -> None:\n        with self.lock:\n            # Copy pointers to all values, then wipe the mapping\n            values = list(self._container.values())\n            self._container.clear()\n\n        if self.dispose_func:\n            for value in values:\n                self.dispose_func(value)\n\n    def keys(self) -> set[_KT]:  # type: ignore[override]\n        with self.lock:\n            return set(self._container.keys())\n\n\nclass HTTPHeaderDictItemView(typing.Set[typing.Tuple[str, str]]):\n    \"\"\"\n    HTTPHeaderDict is unusual for a Mapping[str, str] in that it has two modes of\n    address.\n\n    If we directly try to get an item with a particular name, we will get a string\n    back that is the concatenated version of all the values:\n\n    >>> d['X-Header-Name']\n    'Value1, Value2, Value3'\n\n    However, if we iterate over an HTTPHeaderDict's items, we will optionally combine\n    these values based on whether combine=True was called when building up the dictionary\n\n    >>> d = HTTPHeaderDict({\"A\": \"1\", \"B\": \"foo\"})\n    >>> d.add(\"A\", \"2\", combine=True)\n    >>> d.add(\"B\", \"bar\")\n    >>> list(d.items())\n    [\n        ('A', '1, 2'),\n        ('B', 'foo'),\n        ('B', 'bar'),\n    ]\n\n    This class conforms to the interface required by the MutableMapping ABC while\n    also giving us the nonstandard iteration behavior we want; items with duplicate\n    keys, ordered by time of first insertion.\n    \"\"\"\n\n    _headers: HTTPHeaderDict\n\n    def __init__(self, headers: HTTPHeaderDict) -> None:\n        self._headers = headers\n\n    def __len__(self) -> int:\n        return len(list(self._headers.iteritems()))\n\n    def __iter__(self) -> typing.Iterator[tuple[str, str]]:\n        return self._headers.iteritems()\n\n    def __contains__(self, item: object) -> bool:\n        if isinstance(item, tuple) and len(item) == 2:\n            passed_key, passed_val = item\n            if isinstance(passed_key, str) and isinstance(passed_val, str):\n                return self._headers._has_value_for_header(passed_key, passed_val)\n        return False\n\n\nclass HTTPHeaderDict(typing.MutableMapping[str, str]):\n    \"\"\"\n    :param headers:\n        An iterable of field-value pairs. Must not contain multiple field names\n        when compared case-insensitively.\n\n    :param kwargs:\n        Additional field-value pairs to pass in to ``dict.update``.\n\n    A ``dict`` like container for storing HTTP Headers.\n\n    Field names are stored and compared case-insensitively in compliance with\n    RFC 7230. Iteration provides the first case-sensitive key seen for each\n    case-insensitive pair.\n\n    Using ``__setitem__`` syntax overwrites fields that compare equal\n    case-insensitively in order to maintain ``dict``'s api. For fields that\n    compare equal, instead create a new ``HTTPHeaderDict`` and use ``.add``\n    in a loop.\n\n    If multiple fields that are equal case-insensitively are passed to the\n    constructor or ``.update``, the behavior is undefined and some will be\n    lost.\n\n    >>> headers = HTTPHeaderDict()\n    >>> headers.add('Set-Cookie', 'foo=bar')\n    >>> headers.add('set-cookie', 'baz=quxx')\n    >>> headers['content-length'] = '7'\n    >>> headers['SET-cookie']\n    'foo=bar, baz=quxx'\n    >>> headers['Content-Length']\n    '7'\n    \"\"\"\n\n    _container: typing.MutableMapping[str, list[str]]\n\n    def __init__(self, headers: ValidHTTPHeaderSource | None = None, **kwargs: str):\n        super().__init__()\n        self._container = {}  # 'dict' is insert-ordered\n        if headers is not None:\n            if isinstance(headers, HTTPHeaderDict):\n                self._copy_from(headers)\n            else:\n                self.extend(headers)\n        if kwargs:\n            self.extend(kwargs)\n\n    def __setitem__(self, key: str, val: str) -> None:\n        # avoid a bytes/str comparison by decoding before httplib\n        if isinstance(key, bytes):\n            key = key.decode(\"latin-1\")\n        self._container[key.lower()] = [key, val]\n\n    def __getitem__(self, key: str) -> str:\n        val = self._container[key.lower()]\n        return \", \".join(val[1:])\n\n    def __delitem__(self, key: str) -> None:\n        del self._container[key.lower()]\n\n    def __contains__(self, key: object) -> bool:\n        if isinstance(key, str):\n            return key.lower() in self._container\n        return False\n\n    def setdefault(self, key: str, default: str = \"\") -> str:\n        return super().setdefault(key, default)\n\n    def __eq__(self, other: object) -> bool:\n        maybe_constructable = ensure_can_construct_http_header_dict(other)\n        if maybe_constructable is None:\n            return False\n        else:\n            other_as_http_header_dict = type(self)(maybe_constructable)\n\n        return {k.lower(): v for k, v in self.itermerged()} == {\n            k.lower(): v for k, v in other_as_http_header_dict.itermerged()\n        }\n\n    def __ne__(self, other: object) -> bool:\n        return not self.__eq__(other)\n\n    def __len__(self) -> int:\n        return len(self._container)\n\n    def __iter__(self) -> typing.Iterator[str]:\n        # Only provide the originally cased names\n        for vals in self._container.values():\n            yield vals[0]\n\n    def discard(self, key: str) -> None:\n        try:\n            del self[key]\n        except KeyError:\n            pass\n\n    def add(self, key: str, val: str, *, combine: bool = False) -> None:\n        \"\"\"Adds a (name, value) pair, doesn't overwrite the value if it already\n        exists.\n\n        If this is called with combine=True, instead of adding a new header value\n        as a distinct item during iteration, this will instead append the value to\n        any existing header value with a comma. If no existing header value exists\n        for the key, then the value will simply be added, ignoring the combine parameter.\n\n        >>> headers = HTTPHeaderDict(foo='bar')\n        >>> headers.add('Foo', 'baz')\n        >>> headers['foo']\n        'bar, baz'\n        >>> list(headers.items())\n        [('foo', 'bar'), ('foo', 'baz')]\n        >>> headers.add('foo', 'quz', combine=True)\n        >>> list(headers.items())\n        [('foo', 'bar, baz, quz')]\n        \"\"\"\n        # avoid a bytes/str comparison by decoding before httplib\n        if isinstance(key, bytes):\n            key = key.decode(\"latin-1\")\n        key_lower = key.lower()\n        new_vals = [key, val]\n        # Keep the common case aka no item present as fast as possible\n        vals = self._container.setdefault(key_lower, new_vals)\n        if new_vals is not vals:\n            # if there are values here, then there is at least the initial\n            # key/value pair\n            assert len(vals) >= 2\n            if combine:\n                vals[-1] = vals[-1] + \", \" + val\n            else:\n                vals.append(val)\n\n    def extend(self, *args: ValidHTTPHeaderSource, **kwargs: str) -> None:\n        \"\"\"Generic import function for any type of header-like object.\n        Adapted version of MutableMapping.update in order to insert items\n        with self.add instead of self.__setitem__\n        \"\"\"\n        if len(args) > 1:\n            raise TypeError(\n                f\"extend() takes at most 1 positional arguments ({len(args)} given)\"\n            )\n        other = args[0] if len(args) >= 1 else ()\n\n        if isinstance(other, HTTPHeaderDict):\n            for key, val in other.iteritems():\n                self.add(key, val)\n        elif isinstance(other, typing.Mapping):\n            for key, val in other.items():\n                self.add(key, val)\n        elif isinstance(other, typing.Iterable):\n            other = typing.cast(typing.Iterable[typing.Tuple[str, str]], other)\n            for key, value in other:\n                self.add(key, value)\n        elif hasattr(other, \"keys\") and hasattr(other, \"__getitem__\"):\n            # THIS IS NOT A TYPESAFE BRANCH\n            # In this branch, the object has a `keys` attr but is not a Mapping or any of\n            # the other types indicated in the method signature. We do some stuff with\n            # it as though it partially implements the Mapping interface, but we're not\n            # doing that stuff safely AT ALL.\n            for key in other.keys():\n                self.add(key, other[key])\n\n        for key, value in kwargs.items():\n            self.add(key, value)\n\n    @typing.overload\n    def getlist(self, key: str) -> list[str]:\n        ...\n\n    @typing.overload\n    def getlist(self, key: str, default: _DT) -> list[str] | _DT:\n        ...\n\n    def getlist(\n        self, key: str, default: _Sentinel | _DT = _Sentinel.not_passed\n    ) -> list[str] | _DT:\n        \"\"\"Returns a list of all the values for the named field. Returns an\n        empty list if the key doesn't exist.\"\"\"\n        try:\n            vals = self._container[key.lower()]\n        except KeyError:\n            if default is _Sentinel.not_passed:\n                # _DT is unbound; empty list is instance of List[str]\n                return []\n            # _DT is bound; default is instance of _DT\n            return default\n        else:\n            # _DT may or may not be bound; vals[1:] is instance of List[str], which\n            # meets our external interface requirement of `Union[List[str], _DT]`.\n            return vals[1:]\n\n    # Backwards compatibility for httplib\n    getheaders = getlist\n    getallmatchingheaders = getlist\n    iget = getlist\n\n    # Backwards compatibility for http.cookiejar\n    get_all = getlist\n\n    def __repr__(self) -> str:\n        return f\"{type(self).__name__}({dict(self.itermerged())})\"\n\n    def _copy_from(self, other: HTTPHeaderDict) -> None:\n        for key in other:\n            val = other.getlist(key)\n            self._container[key.lower()] = [key, *val]\n\n    def copy(self) -> HTTPHeaderDict:\n        clone = type(self)()\n        clone._copy_from(self)\n        return clone\n\n    def iteritems(self) -> typing.Iterator[tuple[str, str]]:\n        \"\"\"Iterate over all header lines, including duplicate ones.\"\"\"\n        for key in self:\n            vals = self._container[key.lower()]\n            for val in vals[1:]:\n                yield vals[0], val\n\n    def itermerged(self) -> typing.Iterator[tuple[str, str]]:\n        \"\"\"Iterate over all headers, merging duplicate ones together.\"\"\"\n        for key in self:\n            val = self._container[key.lower()]\n            yield val[0], \", \".join(val[1:])\n\n    def items(self) -> HTTPHeaderDictItemView:  # type: ignore[override]\n        return HTTPHeaderDictItemView(self)\n\n    def _has_value_for_header(self, header_name: str, potential_value: str) -> bool:\n        if header_name in self:\n            return potential_value in self._container[header_name.lower()][1:]\n        return False\n\n    def __ior__(self, other: object) -> HTTPHeaderDict:\n        # Supports extending a header dict in-place using operator |=\n        # combining items with add instead of __setitem__\n        maybe_constructable = ensure_can_construct_http_header_dict(other)\n        if maybe_constructable is None:\n            return NotImplemented\n        self.extend(maybe_constructable)\n        return self\n\n    def __or__(self, other: object) -> HTTPHeaderDict:\n        # Supports merging header dicts using operator |\n        # combining items with add instead of __setitem__\n        maybe_constructable = ensure_can_construct_http_header_dict(other)\n        if maybe_constructable is None:\n            return NotImplemented\n        result = self.copy()\n        result.extend(maybe_constructable)\n        return result\n\n    def __ror__(self, other: object) -> HTTPHeaderDict:\n        # Supports merging header dicts using operator | when other is on left side\n        # combining items with add instead of __setitem__\n        maybe_constructable = ensure_can_construct_http_header_dict(other)\n        if maybe_constructable is None:\n            return NotImplemented\n        result = type(self)(maybe_constructable)\n        result.extend(self)\n        return result\n", "# This file is protected via CODEOWNERS\nfrom __future__ import annotations\n\n__version__ = \"2.0.6\"\n", "from __future__ import annotations\n\nimport errno\nimport logging\nimport queue\nimport sys\nimport typing\nimport warnings\nimport weakref\nfrom socket import timeout as SocketTimeout\nfrom types import TracebackType\n\nfrom ._base_connection import _TYPE_BODY\nfrom ._request_methods import RequestMethods\nfrom .connection import (\n    BaseSSLError,\n    BrokenPipeError,\n    DummyConnection,\n    HTTPConnection,\n    HTTPException,\n    HTTPSConnection,\n    ProxyConfig,\n    _wrap_proxy_error,\n)\nfrom .connection import port_by_scheme as port_by_scheme\nfrom .exceptions import (\n    ClosedPoolError,\n    EmptyPoolError,\n    FullPoolError,\n    HostChangedError,\n    InsecureRequestWarning,\n    LocationValueError,\n    MaxRetryError,\n    NewConnectionError,\n    ProtocolError,\n    ProxyError,\n    ReadTimeoutError,\n    SSLError,\n    TimeoutError,\n)\nfrom .response import BaseHTTPResponse\nfrom .util.connection import is_connection_dropped\nfrom .util.proxy import connection_requires_http_tunnel\nfrom .util.request import _TYPE_BODY_POSITION, set_file_position\nfrom .util.retry import Retry\nfrom .util.ssl_match_hostname import CertificateError\nfrom .util.timeout import _DEFAULT_TIMEOUT, _TYPE_DEFAULT, Timeout\nfrom .util.url import Url, _encode_target\nfrom .util.url import _normalize_host as normalize_host\nfrom .util.url import parse_url\nfrom .util.util import to_str\n\nif typing.TYPE_CHECKING:\n    import ssl\n    from typing import Literal\n\n    from ._base_connection import BaseHTTPConnection, BaseHTTPSConnection\n\nlog = logging.getLogger(__name__)\n\n_TYPE_TIMEOUT = typing.Union[Timeout, float, _TYPE_DEFAULT, None]\n\n_SelfT = typing.TypeVar(\"_SelfT\")\n\n\n# Pool objects\nclass ConnectionPool:\n    \"\"\"\n    Base class for all connection pools, such as\n    :class:`.HTTPConnectionPool` and :class:`.HTTPSConnectionPool`.\n\n    .. note::\n       ConnectionPool.urlopen() does not normalize or percent-encode target URIs\n       which is useful if your target server doesn't support percent-encoded\n       target URIs.\n    \"\"\"\n\n    scheme: str | None = None\n    QueueCls = queue.LifoQueue\n\n    def __init__(self, host: str, port: int | None = None) -> None:\n        if not host:\n            raise LocationValueError(\"No host specified.\")\n\n        self.host = _normalize_host(host, scheme=self.scheme)\n        self.port = port\n\n        # This property uses 'normalize_host()' (not '_normalize_host()')\n        # to avoid removing square braces around IPv6 addresses.\n        # This value is sent to `HTTPConnection.set_tunnel()` if called\n        # because square braces are required for HTTP CONNECT tunneling.\n        self._tunnel_host = normalize_host(host, scheme=self.scheme).lower()\n\n    def __str__(self) -> str:\n        return f\"{type(self).__name__}(host={self.host!r}, port={self.port!r})\"\n\n    def __enter__(self: _SelfT) -> _SelfT:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> Literal[False]:\n        self.close()\n        # Return False to re-raise any potential exceptions\n        return False\n\n    def close(self) -> None:\n        \"\"\"\n        Close all pooled connections and disable the pool.\n        \"\"\"\n\n\n# This is taken from http://hg.python.org/cpython/file/7aaba721ebc0/Lib/socket.py#l252\n_blocking_errnos = {errno.EAGAIN, errno.EWOULDBLOCK}\n\n\nclass HTTPConnectionPool(ConnectionPool, RequestMethods):\n    \"\"\"\n    Thread-safe connection pool for one host.\n\n    :param host:\n        Host used for this HTTP Connection (e.g. \"localhost\"), passed into\n        :class:`http.client.HTTPConnection`.\n\n    :param port:\n        Port used for this HTTP Connection (None is equivalent to 80), passed\n        into :class:`http.client.HTTPConnection`.\n\n    :param timeout:\n        Socket timeout in seconds for each individual connection. This can\n        be a float or integer, which sets the timeout for the HTTP request,\n        or an instance of :class:`urllib3.util.Timeout` which gives you more\n        fine-grained control over request timeouts. After the constructor has\n        been parsed, this is always a `urllib3.util.Timeout` object.\n\n    :param maxsize:\n        Number of connections to save that can be reused. More than 1 is useful\n        in multithreaded situations. If ``block`` is set to False, more\n        connections will be created but they will not be saved once they've\n        been used.\n\n    :param block:\n        If set to True, no more than ``maxsize`` connections will be used at\n        a time. When no free connections are available, the call will block\n        until a connection has been released. This is a useful side effect for\n        particular multithreaded situations where one does not want to use more\n        than maxsize connections per host to prevent flooding.\n\n    :param headers:\n        Headers to include with all requests, unless other headers are given\n        explicitly.\n\n    :param retries:\n        Retry configuration to use by default with requests in this pool.\n\n    :param _proxy:\n        Parsed proxy URL, should not be used directly, instead, see\n        :class:`urllib3.ProxyManager`\n\n    :param _proxy_headers:\n        A dictionary with proxy headers, should not be used directly,\n        instead, see :class:`urllib3.ProxyManager`\n\n    :param \\\\**conn_kw:\n        Additional parameters are used to create fresh :class:`urllib3.connection.HTTPConnection`,\n        :class:`urllib3.connection.HTTPSConnection` instances.\n    \"\"\"\n\n    scheme = \"http\"\n    ConnectionCls: (\n        type[BaseHTTPConnection] | type[BaseHTTPSConnection]\n    ) = HTTPConnection\n\n    def __init__(\n        self,\n        host: str,\n        port: int | None = None,\n        timeout: _TYPE_TIMEOUT | None = _DEFAULT_TIMEOUT,\n        maxsize: int = 1,\n        block: bool = False,\n        headers: typing.Mapping[str, str] | None = None,\n        retries: Retry | bool | int | None = None,\n        _proxy: Url | None = None,\n        _proxy_headers: typing.Mapping[str, str] | None = None,\n        _proxy_config: ProxyConfig | None = None,\n        **conn_kw: typing.Any,\n    ):\n        ConnectionPool.__init__(self, host, port)\n        RequestMethods.__init__(self, headers)\n\n        if not isinstance(timeout, Timeout):\n            timeout = Timeout.from_float(timeout)\n\n        if retries is None:\n            retries = Retry.DEFAULT\n\n        self.timeout = timeout\n        self.retries = retries\n\n        self.pool: queue.LifoQueue[typing.Any] | None = self.QueueCls(maxsize)\n        self.block = block\n\n        self.proxy = _proxy\n        self.proxy_headers = _proxy_headers or {}\n        self.proxy_config = _proxy_config\n\n        # Fill the queue up so that doing get() on it will block properly\n        for _ in range(maxsize):\n            self.pool.put(None)\n\n        # These are mostly for testing and debugging purposes.\n        self.num_connections = 0\n        self.num_requests = 0\n        self.conn_kw = conn_kw\n\n        if self.proxy:\n            # Enable Nagle's algorithm for proxies, to avoid packet fragmentation.\n            # We cannot know if the user has added default socket options, so we cannot replace the\n            # list.\n            self.conn_kw.setdefault(\"socket_options\", [])\n\n            self.conn_kw[\"proxy\"] = self.proxy\n            self.conn_kw[\"proxy_config\"] = self.proxy_config\n\n        # Do not pass 'self' as callback to 'finalize'.\n        # Then the 'finalize' would keep an endless living (leak) to self.\n        # By just passing a reference to the pool allows the garbage collector\n        # to free self if nobody else has a reference to it.\n        pool = self.pool\n\n        # Close all the HTTPConnections in the pool before the\n        # HTTPConnectionPool object is garbage collected.\n        weakref.finalize(self, _close_pool_connections, pool)\n\n    def _new_conn(self) -> BaseHTTPConnection:\n        \"\"\"\n        Return a fresh :class:`HTTPConnection`.\n        \"\"\"\n        self.num_connections += 1\n        log.debug(\n            \"Starting new HTTP connection (%d): %s:%s\",\n            self.num_connections,\n            self.host,\n            self.port or \"80\",\n        )\n\n        conn = self.ConnectionCls(\n            host=self.host,\n            port=self.port,\n            timeout=self.timeout.connect_timeout,\n            **self.conn_kw,\n        )\n        return conn\n\n    def _get_conn(self, timeout: float | None = None) -> BaseHTTPConnection:\n        \"\"\"\n        Get a connection. Will return a pooled connection if one is available.\n\n        If no connections are available and :prop:`.block` is ``False``, then a\n        fresh connection is returned.\n\n        :param timeout:\n            Seconds to wait before giving up and raising\n            :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and\n            :prop:`.block` is ``True``.\n        \"\"\"\n        conn = None\n\n        if self.pool is None:\n            raise ClosedPoolError(self, \"Pool is closed.\")\n\n        try:\n            conn = self.pool.get(block=self.block, timeout=timeout)\n\n        except AttributeError:  # self.pool is None\n            raise ClosedPoolError(self, \"Pool is closed.\") from None  # Defensive:\n\n        except queue.Empty:\n            if self.block:\n                raise EmptyPoolError(\n                    self,\n                    \"Pool is empty and a new connection can't be opened due to blocking mode.\",\n                ) from None\n            pass  # Oh well, we'll create a new connection then\n\n        # If this is a persistent connection, check if it got disconnected\n        if conn and is_connection_dropped(conn):\n            log.debug(\"Resetting dropped connection: %s\", self.host)\n            conn.close()\n\n        return conn or self._new_conn()\n\n    def _put_conn(self, conn: BaseHTTPConnection | None) -> None:\n        \"\"\"\n        Put a connection back into the pool.\n\n        :param conn:\n            Connection object for the current host and port as returned by\n            :meth:`._new_conn` or :meth:`._get_conn`.\n\n        If the pool is already full, the connection is closed and discarded\n        because we exceeded maxsize. If connections are discarded frequently,\n        then maxsize should be increased.\n\n        If the pool is closed, then the connection will be closed and discarded.\n        \"\"\"\n        if self.pool is not None:\n            try:\n                self.pool.put(conn, block=False)\n                return  # Everything is dandy, done.\n            except AttributeError:\n                # self.pool is None.\n                pass\n            except queue.Full:\n                # Connection never got put back into the pool, close it.\n                if conn:\n                    conn.close()\n\n                if self.block:\n                    # This should never happen if you got the conn from self._get_conn\n                    raise FullPoolError(\n                        self,\n                        \"Pool reached maximum size and no more connections are allowed.\",\n                    ) from None\n\n                log.warning(\n                    \"Connection pool is full, discarding connection: %s. Connection pool size: %s\",\n                    self.host,\n                    self.pool.qsize(),\n                )\n\n        # Connection never got put back into the pool, close it.\n        if conn:\n            conn.close()\n\n    def _validate_conn(self, conn: BaseHTTPConnection) -> None:\n        \"\"\"\n        Called right before a request is made, after the socket is created.\n        \"\"\"\n\n    def _prepare_proxy(self, conn: BaseHTTPConnection) -> None:\n        # Nothing to do for HTTP connections.\n        pass\n\n    def _get_timeout(self, timeout: _TYPE_TIMEOUT) -> Timeout:\n        \"\"\"Helper that always returns a :class:`urllib3.util.Timeout`\"\"\"\n        if timeout is _DEFAULT_TIMEOUT:\n            return self.timeout.clone()\n\n        if isinstance(timeout, Timeout):\n            return timeout.clone()\n        else:\n            # User passed us an int/float. This is for backwards compatibility,\n            # can be removed later\n            return Timeout.from_float(timeout)\n\n    def _raise_timeout(\n        self,\n        err: BaseSSLError | OSError | SocketTimeout,\n        url: str,\n        timeout_value: _TYPE_TIMEOUT | None,\n    ) -> None:\n        \"\"\"Is the error actually a timeout? Will raise a ReadTimeout or pass\"\"\"\n\n        if isinstance(err, SocketTimeout):\n            raise ReadTimeoutError(\n                self, url, f\"Read timed out. (read timeout={timeout_value})\"\n            ) from err\n\n        # See the above comment about EAGAIN in Python 3.\n        if hasattr(err, \"errno\") and err.errno in _blocking_errnos:\n            raise ReadTimeoutError(\n                self, url, f\"Read timed out. (read timeout={timeout_value})\"\n            ) from err\n\n    def _make_request(\n        self,\n        conn: BaseHTTPConnection,\n        method: str,\n        url: str,\n        body: _TYPE_BODY | None = None,\n        headers: typing.Mapping[str, str] | None = None,\n        retries: Retry | None = None,\n        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,\n        chunked: bool = False,\n        response_conn: BaseHTTPConnection | None = None,\n        preload_content: bool = True,\n        decode_content: bool = True,\n        enforce_content_length: bool = True,\n    ) -> BaseHTTPResponse:\n        \"\"\"\n        Perform a request on a given urllib connection object taken from our\n        pool.\n\n        :param conn:\n            a connection from one of our connection pools\n\n        :param method:\n            HTTP request method (such as GET, POST, PUT, etc.)\n\n        :param url:\n            The URL to perform the request on.\n\n        :param body:\n            Data to send in the request body, either :class:`str`, :class:`bytes`,\n            an iterable of :class:`str`/:class:`bytes`, or a file-like object.\n\n        :param headers:\n            Dictionary of custom headers to send, such as User-Agent,\n            If-None-Match, etc. If None, pool headers are used. If provided,\n            these headers completely replace any pool-specific headers.\n\n        :param retries:\n            Configure the number of retries to allow before raising a\n            :class:`~urllib3.exceptions.MaxRetryError` exception.\n\n            Pass ``None`` to retry until you receive a response. Pass a\n            :class:`~urllib3.util.retry.Retry` object for fine-grained control\n            over different types of retries.\n            Pass an integer number to retry connection errors that many times,\n            but no other types of errors. Pass zero to never retry.\n\n            If ``False``, then retries are disabled and any exception is raised\n            immediately. Also, instead of raising a MaxRetryError on redirects,\n            the redirect response will be returned.\n\n        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.\n\n        :param timeout:\n            If specified, overrides the default timeout for this one\n            request. It may be a float (in seconds) or an instance of\n            :class:`urllib3.util.Timeout`.\n\n        :param chunked:\n            If True, urllib3 will send the body using chunked transfer\n            encoding. Otherwise, urllib3 will send the body using the standard\n            content-length form. Defaults to False.\n\n        :param response_conn:\n            Set this to ``None`` if you will handle releasing the connection or\n            set the connection to have the response release it.\n\n        :param preload_content:\n          If True, the response's body will be preloaded during construction.\n\n        :param decode_content:\n            If True, will attempt to decode the body based on the\n            'content-encoding' header.\n\n        :param enforce_content_length:\n            Enforce content length checking. Body returned by server must match\n            value of Content-Length header, if present. Otherwise, raise error.\n        \"\"\"\n        self.num_requests += 1\n\n        timeout_obj = self._get_timeout(timeout)\n        timeout_obj.start_connect()\n        conn.timeout = Timeout.resolve_default_timeout(timeout_obj.connect_timeout)\n\n        try:\n            # Trigger any extra validation we need to do.\n            try:\n                self._validate_conn(conn)\n            except (SocketTimeout, BaseSSLError) as e:\n                self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n                raise\n\n        # _validate_conn() starts the connection to an HTTPS proxy\n        # so we need to wrap errors with 'ProxyError' here too.\n        except (\n            OSError,\n            NewConnectionError,\n            TimeoutError,\n            BaseSSLError,\n            CertificateError,\n            SSLError,\n        ) as e:\n            new_e: Exception = e\n            if isinstance(e, (BaseSSLError, CertificateError)):\n                new_e = SSLError(e)\n            # If the connection didn't successfully connect to it's proxy\n            # then there\n            if isinstance(\n                new_e, (OSError, NewConnectionError, TimeoutError, SSLError)\n            ) and (conn and conn.proxy and not conn.has_connected_to_proxy):\n                new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n            raise new_e\n\n        # conn.request() calls http.client.*.request, not the method in\n        # urllib3.request. It also calls makefile (recv) on the socket.\n        try:\n            conn.request(\n                method,\n                url,\n                body=body,\n                headers=headers,\n                chunked=chunked,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                enforce_content_length=enforce_content_length,\n            )\n\n        # We are swallowing BrokenPipeError (errno.EPIPE) since the server is\n        # legitimately able to close the connection after sending a valid response.\n        # With this behaviour, the received response is still readable.\n        except BrokenPipeError:\n            pass\n        except OSError as e:\n            # MacOS/Linux\n            # EPROTOTYPE is needed on macOS\n            # https://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug/\n            if e.errno != errno.EPROTOTYPE:\n                raise\n\n        # Reset the timeout for the recv() on the socket\n        read_timeout = timeout_obj.read_timeout\n\n        if not conn.is_closed:\n            # In Python 3 socket.py will catch EAGAIN and return None when you\n            # try and read into the file pointer created by http.client, which\n            # instead raises a BadStatusLine exception. Instead of catching\n            # the exception and assuming all BadStatusLine exceptions are read\n            # timeouts, check for a zero timeout before making the request.\n            if read_timeout == 0:\n                raise ReadTimeoutError(\n                    self, url, f\"Read timed out. (read timeout={read_timeout})\"\n                )\n            conn.timeout = read_timeout\n\n        # Receive the response from the server\n        try:\n            response = conn.getresponse()\n        except (BaseSSLError, OSError) as e:\n            self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n            raise\n\n        # Set properties that are used by the pooling layer.\n        response.retries = retries\n        response._connection = response_conn  # type: ignore[attr-defined]\n        response._pool = self  # type: ignore[attr-defined]\n\n        log.debug(\n            '%s://%s:%s \"%s %s %s\" %s %s',\n            self.scheme,\n            self.host,\n            self.port,\n            method,\n            url,\n            # HTTP version\n            conn._http_vsn_str,  # type: ignore[attr-defined]\n            response.status,\n            response.length_remaining,  # type: ignore[attr-defined]\n        )\n\n        return response\n\n    def close(self) -> None:\n        \"\"\"\n        Close all pooled connections and disable the pool.\n        \"\"\"\n        if self.pool is None:\n            return\n        # Disable access to the pool\n        old_pool, self.pool = self.pool, None\n\n        # Close all the HTTPConnections in the pool.\n        _close_pool_connections(old_pool)\n\n    def is_same_host(self, url: str) -> bool:\n        \"\"\"\n        Check if the given ``url`` is a member of the same host as this\n        connection pool.\n        \"\"\"\n        if url.startswith(\"/\"):\n            return True\n\n        # TODO: Add optional support for socket.gethostbyname checking.\n        scheme, _, host, port, *_ = parse_url(url)\n        scheme = scheme or \"http\"\n        if host is not None:\n            host = _normalize_host(host, scheme=scheme)\n\n        # Use explicit default port for comparison when none is given\n        if self.port and not port:\n            port = port_by_scheme.get(scheme)\n        elif not self.port and port == port_by_scheme.get(scheme):\n            port = None\n\n        return (scheme, host, port) == (self.scheme, self.host, self.port)\n\n    def urlopen(  # type: ignore[override]\n        self,\n        method: str,\n        url: str,\n        body: _TYPE_BODY | None = None,\n        headers: typing.Mapping[str, str] | None = None,\n        retries: Retry | bool | int | None = None,\n        redirect: bool = True,\n        assert_same_host: bool = True,\n        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,\n        pool_timeout: int | None = None,\n        release_conn: bool | None = None,\n        chunked: bool = False,\n        body_pos: _TYPE_BODY_POSITION | None = None,\n        preload_content: bool = True,\n        decode_content: bool = True,\n        **response_kw: typing.Any,\n    ) -> BaseHTTPResponse:\n        \"\"\"\n        Get a connection from the pool and perform an HTTP request. This is the\n        lowest level call for making a request, so you'll need to specify all\n        the raw details.\n\n        .. note::\n\n           More commonly, it's appropriate to use a convenience method\n           such as :meth:`request`.\n\n        .. note::\n\n           `release_conn` will only behave as expected if\n           `preload_content=False` because we want to make\n           `preload_content=False` the default behaviour someday soon without\n           breaking backwards compatibility.\n\n        :param method:\n            HTTP request method (such as GET, POST, PUT, etc.)\n\n        :param url:\n            The URL to perform the request on.\n\n        :param body:\n            Data to send in the request body, either :class:`str`, :class:`bytes`,\n            an iterable of :class:`str`/:class:`bytes`, or a file-like object.\n\n        :param headers:\n            Dictionary of custom headers to send, such as User-Agent,\n            If-None-Match, etc. If None, pool headers are used. If provided,\n            these headers completely replace any pool-specific headers.\n\n        :param retries:\n            Configure the number of retries to allow before raising a\n            :class:`~urllib3.exceptions.MaxRetryError` exception.\n\n            Pass ``None`` to retry until you receive a response. Pass a\n            :class:`~urllib3.util.retry.Retry` object for fine-grained control\n            over different types of retries.\n            Pass an integer number to retry connection errors that many times,\n            but no other types of errors. Pass zero to never retry.\n\n            If ``False``, then retries are disabled and any exception is raised\n            immediately. Also, instead of raising a MaxRetryError on redirects,\n            the redirect response will be returned.\n\n        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.\n\n        :param redirect:\n            If True, automatically handle redirects (status codes 301, 302,\n            303, 307, 308). Each redirect counts as a retry. Disabling retries\n            will disable redirect, too.\n\n        :param assert_same_host:\n            If ``True``, will make sure that the host of the pool requests is\n            consistent else will raise HostChangedError. When ``False``, you can\n            use the pool on an HTTP proxy and request foreign hosts.\n\n        :param timeout:\n            If specified, overrides the default timeout for this one\n            request. It may be a float (in seconds) or an instance of\n            :class:`urllib3.util.Timeout`.\n\n        :param pool_timeout:\n            If set and the pool is set to block=True, then this method will\n            block for ``pool_timeout`` seconds and raise EmptyPoolError if no\n            connection is available within the time period.\n\n        :param bool preload_content:\n            If True, the response's body will be preloaded into memory.\n\n        :param bool decode_content:\n            If True, will attempt to decode the body based on the\n            'content-encoding' header.\n\n        :param release_conn:\n            If False, then the urlopen call will not release the connection\n            back into the pool once a response is received (but will release if\n            you read the entire contents of the response such as when\n            `preload_content=True`). This is useful if you're not preloading\n            the response's content immediately. You will need to call\n            ``r.release_conn()`` on the response ``r`` to return the connection\n            back into the pool. If None, it takes the value of ``preload_content``\n            which defaults to ``True``.\n\n        :param bool chunked:\n            If True, urllib3 will send the body using chunked transfer\n            encoding. Otherwise, urllib3 will send the body using the standard\n            content-length form. Defaults to False.\n\n        :param int body_pos:\n            Position to seek to in file-like body in the event of a retry or\n            redirect. Typically this won't need to be set because urllib3 will\n            auto-populate the value when needed.\n        \"\"\"\n        parsed_url = parse_url(url)\n        destination_scheme = parsed_url.scheme\n\n        if headers is None:\n            headers = self.headers\n\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)\n\n        if release_conn is None:\n            release_conn = preload_content\n\n        # Check host\n        if assert_same_host and not self.is_same_host(url):\n            raise HostChangedError(self, url, retries)\n\n        # Ensure that the URL we're connecting to is properly encoded\n        if url.startswith(\"/\"):\n            url = to_str(_encode_target(url))\n        else:\n            url = to_str(parsed_url.url)\n\n        conn = None\n\n        # Track whether `conn` needs to be released before\n        # returning/raising/recursing. Update this variable if necessary, and\n        # leave `release_conn` constant throughout the function. That way, if\n        # the function recurses, the original value of `release_conn` will be\n        # passed down into the recursive call, and its value will be respected.\n        #\n        # See issue #651 [1] for details.\n        #\n        # [1] <https://github.com/urllib3/urllib3/issues/651>\n        release_this_conn = release_conn\n\n        http_tunnel_required = connection_requires_http_tunnel(\n            self.proxy, self.proxy_config, destination_scheme\n        )\n\n        # Merge the proxy headers. Only done when not using HTTP CONNECT. We\n        # have to copy the headers dict so we can safely change it without those\n        # changes being reflected in anyone else's copy.\n        if not http_tunnel_required:\n            headers = headers.copy()  # type: ignore[attr-defined]\n            headers.update(self.proxy_headers)  # type: ignore[union-attr]\n\n        # Must keep the exception bound to a separate variable or else Python 3\n        # complains about UnboundLocalError.\n        err = None\n\n        # Keep track of whether we cleanly exited the except block. This\n        # ensures we do proper cleanup in finally.\n        clean_exit = False\n\n        # Rewind body position, if needed. Record current position\n        # for future rewinds in the event of a redirect/retry.\n        body_pos = set_file_position(body, body_pos)\n\n        try:\n            # Request a connection from the queue.\n            timeout_obj = self._get_timeout(timeout)\n            conn = self._get_conn(timeout=pool_timeout)\n\n            conn.timeout = timeout_obj.connect_timeout  # type: ignore[assignment]\n\n            # Is this a closed/new connection that requires CONNECT tunnelling?\n            if self.proxy is not None and http_tunnel_required and conn.is_closed:\n                try:\n                    self._prepare_proxy(conn)\n                except (BaseSSLError, OSError, SocketTimeout) as e:\n                    self._raise_timeout(\n                        err=e, url=self.proxy.url, timeout_value=conn.timeout\n                    )\n                    raise\n\n            # If we're going to release the connection in ``finally:``, then\n            # the response doesn't need to know about the connection. Otherwise\n            # it will also try to release it and we'll have a double-release\n            # mess.\n            response_conn = conn if not release_conn else None\n\n            # Make the request on the HTTPConnection object\n            response = self._make_request(\n                conn,\n                method,\n                url,\n                timeout=timeout_obj,\n                body=body,\n                headers=headers,\n                chunked=chunked,\n                retries=retries,\n                response_conn=response_conn,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                **response_kw,\n            )\n\n            # Everything went great!\n            clean_exit = True\n\n        except EmptyPoolError:\n            # Didn't get a connection from the pool, no need to clean up\n            clean_exit = True\n            release_this_conn = False\n            raise\n\n        except (\n            TimeoutError,\n            HTTPException,\n            OSError,\n            ProtocolError,\n            BaseSSLError,\n            SSLError,\n            CertificateError,\n            ProxyError,\n        ) as e:\n            # Discard the connection for these exceptions. It will be\n            # replaced during the next _get_conn() call.\n            clean_exit = False\n            new_e: Exception = e\n            if isinstance(e, (BaseSSLError, CertificateError)):\n                new_e = SSLError(e)\n            if isinstance(\n                new_e,\n                (\n                    OSError,\n                    NewConnectionError,\n                    TimeoutError,\n                    SSLError,\n                    HTTPException,\n                ),\n            ) and (conn and conn.proxy and not conn.has_connected_to_proxy):\n                new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n            elif isinstance(new_e, (OSError, HTTPException)):\n                new_e = ProtocolError(\"Connection aborted.\", new_e)\n\n            retries = retries.increment(\n                method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n            )\n            retries.sleep()\n\n            # Keep track of the error for the retry warning.\n            err = e\n\n        finally:\n            if not clean_exit:\n                # We hit some kind of exception, handled or otherwise. We need\n                # to throw the connection away unless explicitly told not to.\n                # Close the connection, set the variable to None, and make sure\n                # we put the None back in the pool to avoid leaking it.\n                if conn:\n                    conn.close()\n                    conn = None\n                release_this_conn = True\n\n            if release_this_conn:\n                # Put the connection back to be reused. If the connection is\n                # expired then it will be None, which will get replaced with a\n                # fresh connection during _get_conn.\n                self._put_conn(conn)\n\n        if not conn:\n            # Try again\n            log.warning(\n                \"Retrying (%r) after connection broken by '%r': %s\", retries, err, url\n            )\n            return self.urlopen(\n                method,\n                url,\n                body,\n                headers,\n                retries,\n                redirect,\n                assert_same_host,\n                timeout=timeout,\n                pool_timeout=pool_timeout,\n                release_conn=release_conn,\n                chunked=chunked,\n                body_pos=body_pos,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                **response_kw,\n            )\n\n        # Handle redirect?\n        redirect_location = redirect and response.get_redirect_location()\n        if redirect_location:\n            if response.status == 303:\n                method = \"GET\"\n\n            try:\n                retries = retries.increment(method, url, response=response, _pool=self)\n            except MaxRetryError:\n                if retries.raise_on_redirect:\n                    response.drain_conn()\n                    raise\n                return response\n\n            response.drain_conn()\n            retries.sleep_for_retry(response)\n            log.debug(\"Redirecting %s -> %s\", url, redirect_location)\n            return self.urlopen(\n                method,\n                redirect_location,\n                body,\n                headers,\n                retries=retries,\n                redirect=redirect,\n                assert_same_host=assert_same_host,\n                timeout=timeout,\n                pool_timeout=pool_timeout,\n                release_conn=release_conn,\n                chunked=chunked,\n                body_pos=body_pos,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                **response_kw,\n            )\n\n        # Check if we should retry the HTTP response.\n        has_retry_after = bool(response.headers.get(\"Retry-After\"))\n        if retries.is_retry(method, response.status, has_retry_after):\n            try:\n                retries = retries.increment(method, url, response=response, _pool=self)\n            except MaxRetryError:\n                if retries.raise_on_status:\n                    response.drain_conn()\n                    raise\n                return response\n\n            response.drain_conn()\n            retries.sleep(response)\n            log.debug(\"Retry: %s\", url)\n            return self.urlopen(\n                method,\n                url,\n                body,\n                headers,\n                retries=retries,\n                redirect=redirect,\n                assert_same_host=assert_same_host,\n                timeout=timeout,\n                pool_timeout=pool_timeout,\n                release_conn=release_conn,\n                chunked=chunked,\n                body_pos=body_pos,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                **response_kw,\n            )\n\n        return response\n\n\nclass HTTPSConnectionPool(HTTPConnectionPool):\n    \"\"\"\n    Same as :class:`.HTTPConnectionPool`, but HTTPS.\n\n    :class:`.HTTPSConnection` uses one of ``assert_fingerprint``,\n    ``assert_hostname`` and ``host`` in this order to verify connections.\n    If ``assert_hostname`` is False, no verification is done.\n\n    The ``key_file``, ``cert_file``, ``cert_reqs``, ``ca_certs``,\n    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`\n    is available and are fed into :meth:`urllib3.util.ssl_wrap_socket` to upgrade\n    the connection socket into an SSL socket.\n    \"\"\"\n\n    scheme = \"https\"\n    ConnectionCls: type[BaseHTTPSConnection] = HTTPSConnection\n\n    def __init__(\n        self,\n        host: str,\n        port: int | None = None,\n        timeout: _TYPE_TIMEOUT | None = _DEFAULT_TIMEOUT,\n        maxsize: int = 1,\n        block: bool = False,\n        headers: typing.Mapping[str, str] | None = None,\n        retries: Retry | bool | int | None = None,\n        _proxy: Url | None = None,\n        _proxy_headers: typing.Mapping[str, str] | None = None,\n        key_file: str | None = None,\n        cert_file: str | None = None,\n        cert_reqs: int | str | None = None,\n        key_password: str | None = None,\n        ca_certs: str | None = None,\n        ssl_version: int | str | None = None,\n        ssl_minimum_version: ssl.TLSVersion | None = None,\n        ssl_maximum_version: ssl.TLSVersion | None = None,\n        assert_hostname: str | Literal[False] | None = None,\n        assert_fingerprint: str | None = None,\n        ca_cert_dir: str | None = None,\n        **conn_kw: typing.Any,\n    ) -> None:\n        super().__init__(\n            host,\n            port,\n            timeout,\n            maxsize,\n            block,\n            headers,\n            retries,\n            _proxy,\n            _proxy_headers,\n            **conn_kw,\n        )\n\n        self.key_file = key_file\n        self.cert_file = cert_file\n        self.cert_reqs = cert_reqs\n        self.key_password = key_password\n        self.ca_certs = ca_certs\n        self.ca_cert_dir = ca_cert_dir\n        self.ssl_version = ssl_version\n        self.ssl_minimum_version = ssl_minimum_version\n        self.ssl_maximum_version = ssl_maximum_version\n        self.assert_hostname = assert_hostname\n        self.assert_fingerprint = assert_fingerprint\n\n    def _prepare_proxy(self, conn: HTTPSConnection) -> None:  # type: ignore[override]\n        \"\"\"Establishes a tunnel connection through HTTP CONNECT.\"\"\"\n        if self.proxy and self.proxy.scheme == \"https\":\n            tunnel_scheme = \"https\"\n        else:\n            tunnel_scheme = \"http\"\n\n        conn.set_tunnel(\n            scheme=tunnel_scheme,\n            host=self._tunnel_host,\n            port=self.port,\n            headers=self.proxy_headers,\n        )\n        conn.connect()\n\n    def _new_conn(self) -> BaseHTTPSConnection:\n        \"\"\"\n        Return a fresh :class:`urllib3.connection.HTTPConnection`.\n        \"\"\"\n        self.num_connections += 1\n        log.debug(\n            \"Starting new HTTPS connection (%d): %s:%s\",\n            self.num_connections,\n            self.host,\n            self.port or \"443\",\n        )\n\n        if not self.ConnectionCls or self.ConnectionCls is DummyConnection:  # type: ignore[comparison-overlap]\n            raise ImportError(\n                \"Can't connect to HTTPS URL because the SSL module is not available.\"\n            )\n\n        actual_host: str = self.host\n        actual_port = self.port\n        if self.proxy is not None and self.proxy.host is not None:\n            actual_host = self.proxy.host\n            actual_port = self.proxy.port\n\n        return self.ConnectionCls(\n            host=actual_host,\n            port=actual_port,\n            timeout=self.timeout.connect_timeout,\n            cert_file=self.cert_file,\n            key_file=self.key_file,\n            key_password=self.key_password,\n            cert_reqs=self.cert_reqs,\n            ca_certs=self.ca_certs,\n            ca_cert_dir=self.ca_cert_dir,\n            assert_hostname=self.assert_hostname,\n            assert_fingerprint=self.assert_fingerprint,\n            ssl_version=self.ssl_version,\n            ssl_minimum_version=self.ssl_minimum_version,\n            ssl_maximum_version=self.ssl_maximum_version,\n            **self.conn_kw,\n        )\n\n    def _validate_conn(self, conn: BaseHTTPConnection) -> None:\n        \"\"\"\n        Called right before a request is made, after the socket is created.\n        \"\"\"\n        super()._validate_conn(conn)\n\n        # Force connect early to allow us to validate the connection.\n        if conn.is_closed:\n            conn.connect()\n\n        if not conn.is_verified:\n            warnings.warn(\n                (\n                    f\"Unverified HTTPS request is being made to host '{conn.host}'. \"\n                    \"Adding certificate verification is strongly advised. See: \"\n                    \"https://urllib3.readthedocs.io/en/latest/advanced-usage.html\"\n                    \"#tls-warnings\"\n                ),\n                InsecureRequestWarning,\n            )\n\n\ndef connection_from_url(url: str, **kw: typing.Any) -> HTTPConnectionPool:\n    \"\"\"\n    Given a url, return an :class:`.ConnectionPool` instance of its host.\n\n    This is a shortcut for not having to parse out the scheme, host, and port\n    of the url before creating an :class:`.ConnectionPool` instance.\n\n    :param url:\n        Absolute URL string that must include the scheme. Port is optional.\n\n    :param \\\\**kw:\n        Passes additional parameters to the constructor of the appropriate\n        :class:`.ConnectionPool`. Useful for specifying things like\n        timeout, maxsize, headers, etc.\n\n    Example::\n\n        >>> conn = connection_from_url('http://google.com/')\n        >>> r = conn.request('GET', '/')\n    \"\"\"\n    scheme, _, host, port, *_ = parse_url(url)\n    scheme = scheme or \"http\"\n    port = port or port_by_scheme.get(scheme, 80)\n    if scheme == \"https\":\n        return HTTPSConnectionPool(host, port=port, **kw)  # type: ignore[arg-type]\n    else:\n        return HTTPConnectionPool(host, port=port, **kw)  # type: ignore[arg-type]\n\n\n@typing.overload\ndef _normalize_host(host: None, scheme: str | None) -> None:\n    ...\n\n\n@typing.overload\ndef _normalize_host(host: str, scheme: str | None) -> str:\n    ...\n\n\ndef _normalize_host(host: str | None, scheme: str | None) -> str | None:\n    \"\"\"\n    Normalize hosts for comparisons and use with sockets.\n    \"\"\"\n\n    host = normalize_host(host, scheme)\n\n    # httplib doesn't like it when we include brackets in IPv6 addresses\n    # Specifically, if we include brackets but also pass the port then\n    # httplib crazily doubles up the square brackets on the Host header.\n    # Instead, we need to make sure we never pass ``None`` as the port.\n    # However, for backward compatibility reasons we can't actually\n    # *assert* that.  See http://bugs.python.org/issue28539\n    if host and host.startswith(\"[\") and host.endswith(\"]\"):\n        host = host[1:-1]\n    return host\n\n\ndef _url_from_pool(\n    pool: HTTPConnectionPool | HTTPSConnectionPool, path: str | None = None\n) -> str:\n    \"\"\"Returns the URL from a given connection pool. This is mainly used for testing and logging.\"\"\"\n    return Url(scheme=pool.scheme, host=pool.host, port=pool.port, path=path).url\n\n\ndef _close_pool_connections(pool: queue.LifoQueue[typing.Any]) -> None:\n    \"\"\"Drains a queue of connections and closes each one.\"\"\"\n    try:\n        while True:\n            conn = pool.get(block=False)\n            if conn:\n                conn.close()\n    except queue.Empty:\n        pass  # Done.\n", "from __future__ import annotations\n\nimport functools\nimport logging\nimport typing\nimport warnings\nfrom types import TracebackType\nfrom urllib.parse import urljoin\n\nfrom ._collections import RecentlyUsedContainer\nfrom ._request_methods import RequestMethods\nfrom .connection import ProxyConfig\nfrom .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, port_by_scheme\nfrom .exceptions import (\n    LocationValueError,\n    MaxRetryError,\n    ProxySchemeUnknown,\n    URLSchemeUnknown,\n)\nfrom .response import BaseHTTPResponse\nfrom .util.connection import _TYPE_SOCKET_OPTIONS\nfrom .util.proxy import connection_requires_http_tunnel\nfrom .util.retry import Retry\nfrom .util.timeout import Timeout\nfrom .util.url import Url, parse_url\n\nif typing.TYPE_CHECKING:\n    import ssl\n    from typing import Literal\n\n__all__ = [\"PoolManager\", \"ProxyManager\", \"proxy_from_url\"]\n\n\nlog = logging.getLogger(__name__)\n\nSSL_KEYWORDS = (\n    \"key_file\",\n    \"cert_file\",\n    \"cert_reqs\",\n    \"ca_certs\",\n    \"ssl_version\",\n    \"ssl_minimum_version\",\n    \"ssl_maximum_version\",\n    \"ca_cert_dir\",\n    \"ssl_context\",\n    \"key_password\",\n    \"server_hostname\",\n)\n# Default value for `blocksize` - a new parameter introduced to\n# http.client.HTTPConnection & http.client.HTTPSConnection in Python 3.7\n_DEFAULT_BLOCKSIZE = 16384\n\n_SelfT = typing.TypeVar(\"_SelfT\")\n\n\nclass PoolKey(typing.NamedTuple):\n    \"\"\"\n    All known keyword arguments that could be provided to the pool manager, its\n    pools, or the underlying connections.\n\n    All custom key schemes should include the fields in this key at a minimum.\n    \"\"\"\n\n    key_scheme: str\n    key_host: str\n    key_port: int | None\n    key_timeout: Timeout | float | int | None\n    key_retries: Retry | bool | int | None\n    key_block: bool | None\n    key_source_address: tuple[str, int] | None\n    key_key_file: str | None\n    key_key_password: str | None\n    key_cert_file: str | None\n    key_cert_reqs: str | None\n    key_ca_certs: str | None\n    key_ssl_version: int | str | None\n    key_ssl_minimum_version: ssl.TLSVersion | None\n    key_ssl_maximum_version: ssl.TLSVersion | None\n    key_ca_cert_dir: str | None\n    key_ssl_context: ssl.SSLContext | None\n    key_maxsize: int | None\n    key_headers: frozenset[tuple[str, str]] | None\n    key__proxy: Url | None\n    key__proxy_headers: frozenset[tuple[str, str]] | None\n    key__proxy_config: ProxyConfig | None\n    key_socket_options: _TYPE_SOCKET_OPTIONS | None\n    key__socks_options: frozenset[tuple[str, str]] | None\n    key_assert_hostname: bool | str | None\n    key_assert_fingerprint: str | None\n    key_server_hostname: str | None\n    key_blocksize: int | None\n\n\ndef _default_key_normalizer(\n    key_class: type[PoolKey], request_context: dict[str, typing.Any]\n) -> PoolKey:\n    \"\"\"\n    Create a pool key out of a request context dictionary.\n\n    According to RFC 3986, both the scheme and host are case-insensitive.\n    Therefore, this function normalizes both before constructing the pool\n    key for an HTTPS request. If you wish to change this behaviour, provide\n    alternate callables to ``key_fn_by_scheme``.\n\n    :param key_class:\n        The class to use when constructing the key. This should be a namedtuple\n        with the ``scheme`` and ``host`` keys at a minimum.\n    :type  key_class: namedtuple\n    :param request_context:\n        A dictionary-like object that contain the context for a request.\n    :type  request_context: dict\n\n    :return: A namedtuple that can be used as a connection pool key.\n    :rtype:  PoolKey\n    \"\"\"\n    # Since we mutate the dictionary, make a copy first\n    context = request_context.copy()\n    context[\"scheme\"] = context[\"scheme\"].lower()\n    context[\"host\"] = context[\"host\"].lower()\n\n    # These are both dictionaries and need to be transformed into frozensets\n    for key in (\"headers\", \"_proxy_headers\", \"_socks_options\"):\n        if key in context and context[key] is not None:\n            context[key] = frozenset(context[key].items())\n\n    # The socket_options key may be a list and needs to be transformed into a\n    # tuple.\n    socket_opts = context.get(\"socket_options\")\n    if socket_opts is not None:\n        context[\"socket_options\"] = tuple(socket_opts)\n\n    # Map the kwargs to the names in the namedtuple - this is necessary since\n    # namedtuples can't have fields starting with '_'.\n    for key in list(context.keys()):\n        context[\"key_\" + key] = context.pop(key)\n\n    # Default to ``None`` for keys missing from the context\n    for field in key_class._fields:\n        if field not in context:\n            context[field] = None\n\n    # Default key_blocksize to _DEFAULT_BLOCKSIZE if missing from the context\n    if context.get(\"key_blocksize\") is None:\n        context[\"key_blocksize\"] = _DEFAULT_BLOCKSIZE\n\n    return key_class(**context)\n\n\n#: A dictionary that maps a scheme to a callable that creates a pool key.\n#: This can be used to alter the way pool keys are constructed, if desired.\n#: Each PoolManager makes a copy of this dictionary so they can be configured\n#: globally here, or individually on the instance.\nkey_fn_by_scheme = {\n    \"http\": functools.partial(_default_key_normalizer, PoolKey),\n    \"https\": functools.partial(_default_key_normalizer, PoolKey),\n}\n\npool_classes_by_scheme = {\"http\": HTTPConnectionPool, \"https\": HTTPSConnectionPool}\n\n\nclass PoolManager(RequestMethods):\n    \"\"\"\n    Allows for arbitrary requests while transparently keeping track of\n    necessary connection pools for you.\n\n    :param num_pools:\n        Number of connection pools to cache before discarding the least\n        recently used pool.\n\n    :param headers:\n        Headers to include with all requests, unless other headers are given\n        explicitly.\n\n    :param \\\\**connection_pool_kw:\n        Additional parameters are used to create fresh\n        :class:`urllib3.connectionpool.ConnectionPool` instances.\n\n    Example:\n\n    .. code-block:: python\n\n        import urllib3\n\n        http = urllib3.PoolManager(num_pools=2)\n\n        resp1 = http.request(\"GET\", \"https://google.com/\")\n        resp2 = http.request(\"GET\", \"https://google.com/mail\")\n        resp3 = http.request(\"GET\", \"https://yahoo.com/\")\n\n        print(len(http.pools))\n        # 2\n\n    \"\"\"\n\n    proxy: Url | None = None\n    proxy_config: ProxyConfig | None = None\n\n    def __init__(\n        self,\n        num_pools: int = 10,\n        headers: typing.Mapping[str, str] | None = None,\n        **connection_pool_kw: typing.Any,\n    ) -> None:\n        super().__init__(headers)\n        self.connection_pool_kw = connection_pool_kw\n\n        self.pools: RecentlyUsedContainer[PoolKey, HTTPConnectionPool]\n        self.pools = RecentlyUsedContainer(num_pools)\n\n        # Locally set the pool classes and keys so other PoolManagers can\n        # override them.\n        self.pool_classes_by_scheme = pool_classes_by_scheme\n        self.key_fn_by_scheme = key_fn_by_scheme.copy()\n\n    def __enter__(self: _SelfT) -> _SelfT:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> Literal[False]:\n        self.clear()\n        # Return False to re-raise any potential exceptions\n        return False\n\n    def _new_pool(\n        self,\n        scheme: str,\n        host: str,\n        port: int,\n        request_context: dict[str, typing.Any] | None = None,\n    ) -> HTTPConnectionPool:\n        \"\"\"\n        Create a new :class:`urllib3.connectionpool.ConnectionPool` based on host, port, scheme, and\n        any additional pool keyword arguments.\n\n        If ``request_context`` is provided, it is provided as keyword arguments\n        to the pool class used. This method is used to actually create the\n        connection pools handed out by :meth:`connection_from_url` and\n        companion methods. It is intended to be overridden for customization.\n        \"\"\"\n        pool_cls: type[HTTPConnectionPool] = self.pool_classes_by_scheme[scheme]\n        if request_context is None:\n            request_context = self.connection_pool_kw.copy()\n\n        # Default blocksize to _DEFAULT_BLOCKSIZE if missing or explicitly\n        # set to 'None' in the request_context.\n        if request_context.get(\"blocksize\") is None:\n            request_context[\"blocksize\"] = _DEFAULT_BLOCKSIZE\n\n        # Although the context has everything necessary to create the pool,\n        # this function has historically only used the scheme, host, and port\n        # in the positional args. When an API change is acceptable these can\n        # be removed.\n        for key in (\"scheme\", \"host\", \"port\"):\n            request_context.pop(key, None)\n\n        if scheme == \"http\":\n            for kw in SSL_KEYWORDS:\n                request_context.pop(kw, None)\n\n        return pool_cls(host, port, **request_context)\n\n    def clear(self) -> None:\n        \"\"\"\n        Empty our store of pools and direct them all to close.\n\n        This will not affect in-flight connections, but they will not be\n        re-used after completion.\n        \"\"\"\n        self.pools.clear()\n\n    def connection_from_host(\n        self,\n        host: str | None,\n        port: int | None = None,\n        scheme: str | None = \"http\",\n        pool_kwargs: dict[str, typing.Any] | None = None,\n    ) -> HTTPConnectionPool:\n        \"\"\"\n        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the host, port, and scheme.\n\n        If ``port`` isn't given, it will be derived from the ``scheme`` using\n        ``urllib3.connectionpool.port_by_scheme``. If ``pool_kwargs`` is\n        provided, it is merged with the instance's ``connection_pool_kw``\n        variable and used to create the new connection pool, if one is\n        needed.\n        \"\"\"\n\n        if not host:\n            raise LocationValueError(\"No host specified.\")\n\n        request_context = self._merge_pool_kwargs(pool_kwargs)\n        request_context[\"scheme\"] = scheme or \"http\"\n        if not port:\n            port = port_by_scheme.get(request_context[\"scheme\"].lower(), 80)\n        request_context[\"port\"] = port\n        request_context[\"host\"] = host\n\n        return self.connection_from_context(request_context)\n\n    def connection_from_context(\n        self, request_context: dict[str, typing.Any]\n    ) -> HTTPConnectionPool:\n        \"\"\"\n        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the request context.\n\n        ``request_context`` must at least contain the ``scheme`` key and its\n        value must be a key in ``key_fn_by_scheme`` instance variable.\n        \"\"\"\n        if \"strict\" in request_context:\n            warnings.warn(\n                \"The 'strict' parameter is no longer needed on Python 3+. \"\n                \"This will raise an error in urllib3 v2.1.0.\",\n                DeprecationWarning,\n            )\n            request_context.pop(\"strict\")\n\n        scheme = request_context[\"scheme\"].lower()\n        pool_key_constructor = self.key_fn_by_scheme.get(scheme)\n        if not pool_key_constructor:\n            raise URLSchemeUnknown(scheme)\n        pool_key = pool_key_constructor(request_context)\n\n        return self.connection_from_pool_key(pool_key, request_context=request_context)\n\n    def connection_from_pool_key(\n        self, pool_key: PoolKey, request_context: dict[str, typing.Any]\n    ) -> HTTPConnectionPool:\n        \"\"\"\n        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the provided pool key.\n\n        ``pool_key`` should be a namedtuple that only contains immutable\n        objects. At a minimum it must have the ``scheme``, ``host``, and\n        ``port`` fields.\n        \"\"\"\n        with self.pools.lock:\n            # If the scheme, host, or port doesn't match existing open\n            # connections, open a new ConnectionPool.\n            pool = self.pools.get(pool_key)\n            if pool:\n                return pool\n\n            # Make a fresh ConnectionPool of the desired type\n            scheme = request_context[\"scheme\"]\n            host = request_context[\"host\"]\n            port = request_context[\"port\"]\n            pool = self._new_pool(scheme, host, port, request_context=request_context)\n            self.pools[pool_key] = pool\n\n        return pool\n\n    def connection_from_url(\n        self, url: str, pool_kwargs: dict[str, typing.Any] | None = None\n    ) -> HTTPConnectionPool:\n        \"\"\"\n        Similar to :func:`urllib3.connectionpool.connection_from_url`.\n\n        If ``pool_kwargs`` is not provided and a new pool needs to be\n        constructed, ``self.connection_pool_kw`` is used to initialize\n        the :class:`urllib3.connectionpool.ConnectionPool`. If ``pool_kwargs``\n        is provided, it is used instead. Note that if a new pool does not\n        need to be created for the request, the provided ``pool_kwargs`` are\n        not used.\n        \"\"\"\n        u = parse_url(url)\n        return self.connection_from_host(\n            u.host, port=u.port, scheme=u.scheme, pool_kwargs=pool_kwargs\n        )\n\n    def _merge_pool_kwargs(\n        self, override: dict[str, typing.Any] | None\n    ) -> dict[str, typing.Any]:\n        \"\"\"\n        Merge a dictionary of override values for self.connection_pool_kw.\n\n        This does not modify self.connection_pool_kw and returns a new dict.\n        Any keys in the override dictionary with a value of ``None`` are\n        removed from the merged dictionary.\n        \"\"\"\n        base_pool_kwargs = self.connection_pool_kw.copy()\n        if override:\n            for key, value in override.items():\n                if value is None:\n                    try:\n                        del base_pool_kwargs[key]\n                    except KeyError:\n                        pass\n                else:\n                    base_pool_kwargs[key] = value\n        return base_pool_kwargs\n\n    def _proxy_requires_url_absolute_form(self, parsed_url: Url) -> bool:\n        \"\"\"\n        Indicates if the proxy requires the complete destination URL in the\n        request.  Normally this is only needed when not using an HTTP CONNECT\n        tunnel.\n        \"\"\"\n        if self.proxy is None:\n            return False\n\n        return not connection_requires_http_tunnel(\n            self.proxy, self.proxy_config, parsed_url.scheme\n        )\n\n    def urlopen(  # type: ignore[override]\n        self, method: str, url: str, redirect: bool = True, **kw: typing.Any\n    ) -> BaseHTTPResponse:\n        \"\"\"\n        Same as :meth:`urllib3.HTTPConnectionPool.urlopen`\n        with custom cross-host redirect logic and only sends the request-uri\n        portion of the ``url``.\n\n        The given ``url`` parameter must be absolute, such that an appropriate\n        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.\n        \"\"\"\n        u = parse_url(url)\n\n        if u.scheme is None:\n            warnings.warn(\n                \"URLs without a scheme (ie 'https://') are deprecated and will raise an error \"\n                \"in a future version of urllib3. To avoid this DeprecationWarning ensure all URLs \"\n                \"start with 'https://' or 'http://'. Read more in this issue: \"\n                \"https://github.com/urllib3/urllib3/issues/2920\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n\n        kw[\"assert_same_host\"] = False\n        kw[\"redirect\"] = False\n\n        if \"headers\" not in kw:\n            kw[\"headers\"] = self.headers\n\n        if self._proxy_requires_url_absolute_form(u):\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n\n        # Support relative URLs for redirecting.\n        redirect_location = urljoin(url, redirect_location)\n\n        # RFC 7231, Section 6.4.4\n        if response.status == 303:\n            method = \"GET\"\n\n        retries = kw.get(\"retries\")\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n\n        # Strip headers marked as unsafe to forward to the redirected location.\n        # Check remove_headers_on_redirect to avoid a potential network call within\n        # conn.is_same_host() which may use socket.gethostbyname() in the future.\n        if retries.remove_headers_on_redirect and not conn.is_same_host(\n            redirect_location\n        ):\n            new_headers = kw[\"headers\"].copy()\n            for header in kw[\"headers\"]:\n                if header.lower() in retries.remove_headers_on_redirect:\n                    new_headers.pop(header, None)\n            kw[\"headers\"] = new_headers\n\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                response.drain_conn()\n                raise\n            return response\n\n        kw[\"retries\"] = retries\n        kw[\"redirect\"] = redirect\n\n        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n\n        response.drain_conn()\n        return self.urlopen(method, redirect_location, **kw)\n\n\nclass ProxyManager(PoolManager):\n    \"\"\"\n    Behaves just like :class:`PoolManager`, but sends all requests through\n    the defined proxy, using the CONNECT method for HTTPS URLs.\n\n    :param proxy_url:\n        The URL of the proxy to be used.\n\n    :param proxy_headers:\n        A dictionary containing headers that will be sent to the proxy. In case\n        of HTTP they are being sent with each request, while in the\n        HTTPS/CONNECT case they are sent only once. Could be used for proxy\n        authentication.\n\n    :param proxy_ssl_context:\n        The proxy SSL context is used to establish the TLS connection to the\n        proxy when using HTTPS proxies.\n\n    :param use_forwarding_for_https:\n        (Defaults to False) If set to True will forward requests to the HTTPS\n        proxy to be made on behalf of the client instead of creating a TLS\n        tunnel via the CONNECT method. **Enabling this flag means that request\n        and response headers and content will be visible from the HTTPS proxy**\n        whereas tunneling keeps request and response headers and content\n        private.  IP address, target hostname, SNI, and port are always visible\n        to an HTTPS proxy even when this flag is disabled.\n\n    :param proxy_assert_hostname:\n        The hostname of the certificate to verify against.\n\n    :param proxy_assert_fingerprint:\n        The fingerprint of the certificate to verify against.\n\n    Example:\n\n    .. code-block:: python\n\n        import urllib3\n\n        proxy = urllib3.ProxyManager(\"https://localhost:3128/\")\n\n        resp1 = proxy.request(\"GET\", \"https://google.com/\")\n        resp2 = proxy.request(\"GET\", \"https://httpbin.org/\")\n\n        print(len(proxy.pools))\n        # 1\n\n        resp3 = proxy.request(\"GET\", \"https://httpbin.org/\")\n        resp4 = proxy.request(\"GET\", \"https://twitter.com/\")\n\n        print(len(proxy.pools))\n        # 3\n\n    \"\"\"\n\n    def __init__(\n        self,\n        proxy_url: str,\n        num_pools: int = 10,\n        headers: typing.Mapping[str, str] | None = None,\n        proxy_headers: typing.Mapping[str, str] | None = None,\n        proxy_ssl_context: ssl.SSLContext | None = None,\n        use_forwarding_for_https: bool = False,\n        proxy_assert_hostname: None | str | Literal[False] = None,\n        proxy_assert_fingerprint: str | None = None,\n        **connection_pool_kw: typing.Any,\n    ) -> None:\n        if isinstance(proxy_url, HTTPConnectionPool):\n            str_proxy_url = f\"{proxy_url.scheme}://{proxy_url.host}:{proxy_url.port}\"\n        else:\n            str_proxy_url = proxy_url\n        proxy = parse_url(str_proxy_url)\n\n        if proxy.scheme not in (\"http\", \"https\"):\n            raise ProxySchemeUnknown(proxy.scheme)\n\n        if not proxy.port:\n            port = port_by_scheme.get(proxy.scheme, 80)\n            proxy = proxy._replace(port=port)\n\n        self.proxy = proxy\n        self.proxy_headers = proxy_headers or {}\n        self.proxy_ssl_context = proxy_ssl_context\n        self.proxy_config = ProxyConfig(\n            proxy_ssl_context,\n            use_forwarding_for_https,\n            proxy_assert_hostname,\n            proxy_assert_fingerprint,\n        )\n\n        connection_pool_kw[\"_proxy\"] = self.proxy\n        connection_pool_kw[\"_proxy_headers\"] = self.proxy_headers\n        connection_pool_kw[\"_proxy_config\"] = self.proxy_config\n\n        super().__init__(num_pools, headers, **connection_pool_kw)\n\n    def connection_from_host(\n        self,\n        host: str | None,\n        port: int | None = None,\n        scheme: str | None = \"http\",\n        pool_kwargs: dict[str, typing.Any] | None = None,\n    ) -> HTTPConnectionPool:\n        if scheme == \"https\":\n            return super().connection_from_host(\n                host, port, scheme, pool_kwargs=pool_kwargs\n            )\n\n        return super().connection_from_host(\n            self.proxy.host, self.proxy.port, self.proxy.scheme, pool_kwargs=pool_kwargs  # type: ignore[union-attr]\n        )\n\n    def _set_proxy_headers(\n        self, url: str, headers: typing.Mapping[str, str] | None = None\n    ) -> typing.Mapping[str, str]:\n        \"\"\"\n        Sets headers needed by proxies: specifically, the Accept and Host\n        headers. Only sets headers not provided by the user.\n        \"\"\"\n        headers_ = {\"Accept\": \"*/*\"}\n\n        netloc = parse_url(url).netloc\n        if netloc:\n            headers_[\"Host\"] = netloc\n\n        if headers:\n            headers_.update(headers)\n        return headers_\n\n    def urlopen(  # type: ignore[override]\n        self, method: str, url: str, redirect: bool = True, **kw: typing.Any\n    ) -> BaseHTTPResponse:\n        \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n        u = parse_url(url)\n        if not connection_requires_http_tunnel(self.proxy, self.proxy_config, u.scheme):\n            # For connections using HTTP CONNECT, httplib sets the necessary\n            # headers on the CONNECT to the proxy. If we're not using CONNECT,\n            # we'll definitely need to set 'Host' at the very least.\n            headers = kw.get(\"headers\", self.headers)\n            kw[\"headers\"] = self._set_proxy_headers(url, headers)\n\n        return super().urlopen(method, url, redirect=redirect, **kw)\n\n\ndef proxy_from_url(url: str, **kw: typing.Any) -> ProxyManager:\n    return ProxyManager(proxy_url=url, **kw)\n", "from __future__ import annotations\n\nimport io\nimport socket\nimport time\nimport typing\nimport warnings\nfrom test import LONG_TIMEOUT, SHORT_TIMEOUT\nfrom threading import Event\nfrom unittest import mock\nfrom urllib.parse import urlencode\n\nimport pytest\n\nfrom dummyserver.server import HAS_IPV6_AND_DNS, NoIPv6Warning\nfrom dummyserver.testcase import HTTPDummyServerTestCase, SocketDummyServerTestCase\nfrom urllib3 import HTTPConnectionPool, encode_multipart_formdata\nfrom urllib3._collections import HTTPHeaderDict\nfrom urllib3.connection import _get_default_user_agent\nfrom urllib3.exceptions import (\n    ConnectTimeoutError,\n    DecodeError,\n    EmptyPoolError,\n    MaxRetryError,\n    NameResolutionError,\n    NewConnectionError,\n    ReadTimeoutError,\n    UnrewindableBodyError,\n)\nfrom urllib3.fields import _TYPE_FIELD_VALUE_TUPLE\nfrom urllib3.util import SKIP_HEADER, SKIPPABLE_HEADERS\nfrom urllib3.util.retry import RequestHistory, Retry\nfrom urllib3.util.timeout import _TYPE_TIMEOUT, Timeout\n\nfrom .. import INVALID_SOURCE_ADDRESSES, TARPIT_HOST, VALID_SOURCE_ADDRESSES\nfrom ..port_helpers import find_unused_port\n\n\ndef wait_for_socket(ready_event: Event) -> None:\n    ready_event.wait()\n    ready_event.clear()\n\n\nclass TestConnectionPoolTimeouts(SocketDummyServerTestCase):\n    def test_timeout_float(self) -> None:\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=2)\n\n        with HTTPConnectionPool(self.host, self.port, retries=False) as pool:\n            wait_for_socket(ready_event)\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=SHORT_TIMEOUT)\n            block_event.set()  # Release block\n\n            # Shouldn't raise this time\n            wait_for_socket(ready_event)\n            block_event.set()  # Pre-release block\n            pool.request(\"GET\", \"/\", timeout=LONG_TIMEOUT)\n\n    def test_conn_closed(self) -> None:\n        block_event = Event()\n        self.start_basic_handler(block_send=block_event, num=1)\n\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=SHORT_TIMEOUT, retries=False\n        ) as pool:\n            conn = pool._get_conn()\n            pool._put_conn(conn)\n            try:\n                with pytest.raises(ReadTimeoutError):\n                    pool.urlopen(\"GET\", \"/\")\n                if not conn.is_closed:\n                    with pytest.raises(socket.error):\n                        conn.sock.recv(1024)  # type: ignore[attr-defined]\n            finally:\n                pool._put_conn(conn)\n\n            block_event.set()\n\n    def test_timeout(self) -> None:\n        # Requests should time out when expected\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=3)\n\n        # Pool-global timeout\n        short_timeout = Timeout(read=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=short_timeout, retries=False\n        ) as pool:\n            wait_for_socket(ready_event)\n            block_event.clear()\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n            block_event.set()  # Release request\n\n        # Request-specific timeouts should raise errors\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=short_timeout, retries=False\n        ) as pool:\n            wait_for_socket(ready_event)\n            now = time.time()\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=LONG_TIMEOUT)\n            delta = time.time() - now\n\n            message = \"timeout was pool-level SHORT_TIMEOUT rather than request-level LONG_TIMEOUT\"\n            assert delta >= LONG_TIMEOUT, message\n            block_event.set()  # Release request\n\n            # Timeout passed directly to request should raise a request timeout\n            wait_for_socket(ready_event)\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=SHORT_TIMEOUT)\n            block_event.set()  # Release request\n\n    def test_connect_timeout(self) -> None:\n        url = \"/\"\n        host, port = TARPIT_HOST, 80\n        timeout = Timeout(connect=SHORT_TIMEOUT)\n\n        # Pool-global timeout\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            with pytest.raises(ConnectTimeoutError):\n                pool._make_request(conn, \"GET\", url)\n\n            # Retries\n            retries = Retry(connect=0)\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", url, retries=retries)\n\n        # Request-specific connection timeouts\n        big_timeout = Timeout(read=LONG_TIMEOUT, connect=LONG_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=big_timeout, retries=False) as pool:\n            conn = pool._get_conn()\n            with pytest.raises(ConnectTimeoutError):\n                pool._make_request(conn, \"GET\", url, timeout=timeout)\n\n            pool._put_conn(conn)\n            with pytest.raises(ConnectTimeoutError):\n                pool.request(\"GET\", url, timeout=timeout)\n\n    def test_total_applies_connect(self) -> None:\n        host, port = TARPIT_HOST, 80\n\n        timeout = Timeout(total=None, connect=SHORT_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                with pytest.raises(ConnectTimeoutError):\n                    pool._make_request(conn, \"GET\", \"/\")\n            finally:\n                conn.close()\n\n        timeout = Timeout(connect=3, read=5, total=SHORT_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                with pytest.raises(ConnectTimeoutError):\n                    pool._make_request(conn, \"GET\", \"/\")\n            finally:\n                conn.close()\n\n    def test_total_timeout(self) -> None:\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=2)\n\n        wait_for_socket(ready_event)\n        # This will get the socket to raise an EAGAIN on the read\n        timeout = Timeout(connect=3, read=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=timeout, retries=False\n        ) as pool:\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n\n            block_event.set()\n            wait_for_socket(ready_event)\n            block_event.clear()\n\n        # The connect should succeed and this should hit the read timeout\n        timeout = Timeout(connect=3, read=5, total=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=timeout, retries=False\n        ) as pool:\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n\n    def test_create_connection_timeout(self) -> None:\n        self.start_basic_handler(block_send=Event(), num=0)  # needed for self.port\n\n        timeout = Timeout(connect=SHORT_TIMEOUT, total=LONG_TIMEOUT)\n        with HTTPConnectionPool(\n            TARPIT_HOST, self.port, timeout=timeout, retries=False\n        ) as pool:\n            conn = pool._new_conn()\n            with pytest.raises(ConnectTimeoutError):\n                conn.connect()\n\n\nclass TestConnectionPool(HTTPDummyServerTestCase):\n    def test_get(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/specific_method\", fields={\"method\": \"GET\"})\n            assert r.status == 200, r.data\n\n    def test_post_url(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/specific_method\", fields={\"method\": \"POST\"})\n            assert r.status == 200, r.data\n\n    def test_urlopen_put(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.urlopen(\"PUT\", \"/specific_method?method=PUT\")\n            assert r.status == 200, r.data\n\n    def test_wrong_specific_method(self) -> None:\n        # To make sure the dummy server is actually returning failed responses\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/specific_method\", fields={\"method\": \"POST\"})\n            assert r.status == 400, r.data\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/specific_method\", fields={\"method\": \"GET\"})\n            assert r.status == 400, r.data\n\n    def test_upload(self) -> None:\n        data = \"I'm in ur multipart form-data, hazing a cheezburgr\"\n        fields: dict[str, _TYPE_FIELD_VALUE_TUPLE] = {\n            \"upload_param\": \"filefield\",\n            \"upload_filename\": \"lolcat.txt\",\n            \"filefield\": (\"lolcat.txt\", data),\n        }\n        fields[\"upload_size\"] = len(data)  # type: ignore\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/upload\", fields=fields)\n            assert r.status == 200, r.data\n\n    def test_one_name_multiple_values(self) -> None:\n        fields = [(\"foo\", \"a\"), (\"foo\", \"b\")]\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # urlencode\n            r = pool.request(\"GET\", \"/echo\", fields=fields)\n            assert r.data == b\"foo=a&foo=b\"\n\n            # multipart\n            r = pool.request(\"POST\", \"/echo\", fields=fields)\n            assert r.data.count(b'name=\"foo\"') == 2\n\n    def test_request_method_body(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            body = b\"hi\"\n            r = pool.request(\"POST\", \"/echo\", body=body)\n            assert r.data == body\n\n            fields = [(\"hi\", \"hello\")]\n            with pytest.raises(TypeError):\n                pool.request(\"POST\", \"/echo\", body=body, fields=fields)\n\n    def test_unicode_upload(self) -> None:\n        fieldname = \"myfile\"\n        filename = \"\\xe2\\x99\\xa5.txt\"\n        data = \"\\xe2\\x99\\xa5\".encode()\n        size = len(data)\n\n        fields: dict[str, _TYPE_FIELD_VALUE_TUPLE] = {\n            \"upload_param\": fieldname,\n            \"upload_filename\": filename,\n            fieldname: (filename, data),\n        }\n        fields[\"upload_size\"] = size  # type: ignore\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/upload\", fields=fields)\n            assert r.status == 200, r.data\n\n    def test_nagle(self) -> None:\n        \"\"\"Test that connections have TCP_NODELAY turned on\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            conn = pool._get_conn()\n            try:\n                pool._make_request(conn, \"GET\", \"/\")\n                tcp_nodelay_setting = conn.sock.getsockopt(  # type: ignore[attr-defined]\n                    socket.IPPROTO_TCP, socket.TCP_NODELAY\n                )\n                assert tcp_nodelay_setting\n            finally:\n                conn.close()\n\n    @pytest.mark.parametrize(\n        \"socket_options\",\n        [\n            [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)],\n            ((socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1),),\n        ],\n    )\n    def test_socket_options(self, socket_options: tuple[int, int, int]) -> None:\n        \"\"\"Test that connections accept socket options.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries to\n        # connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(\n            self.host,\n            self.port,\n            socket_options=socket_options,\n        ) as pool:\n            # Get the socket of a new connection.\n            s = pool._new_conn()._new_conn()  # type: ignore[attr-defined]\n            try:\n                using_keepalive = (\n                    s.getsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE) > 0\n                )\n                assert using_keepalive\n            finally:\n                s.close()\n\n    @pytest.mark.parametrize(\"socket_options\", [None, []])\n    def test_disable_default_socket_options(\n        self, socket_options: list[int] | None\n    ) -> None:\n        \"\"\"Test that passing None or empty list disables all socket options.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(\n            self.host, self.port, socket_options=socket_options\n        ) as pool:\n            s = pool._new_conn()._new_conn()  # type: ignore[attr-defined]\n            try:\n                using_nagle = s.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY) == 0\n                assert using_nagle\n            finally:\n                s.close()\n\n    def test_defaults_are_applied(self) -> None:\n        \"\"\"Test that modifying the default socket options works.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # Get the HTTPConnection instance\n            conn = pool._new_conn()\n            try:\n                # Update the default socket options\n                assert conn.socket_options is not None\n                conn.socket_options += [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)]  # type: ignore[operator]\n                s = conn._new_conn()  # type: ignore[attr-defined]\n                nagle_disabled = (\n                    s.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY) > 0\n                )\n                using_keepalive = (\n                    s.getsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE) > 0\n                )\n                assert nagle_disabled\n                assert using_keepalive\n            finally:\n                conn.close()\n                s.close()\n\n    def test_connection_error_retries(self) -> None:\n        \"\"\"ECONNREFUSED error should raise a connection error, with retries\"\"\"\n        port = find_unused_port()\n        with HTTPConnectionPool(self.host, port) as pool:\n            with pytest.raises(MaxRetryError) as e:\n                pool.request(\"GET\", \"/\", retries=Retry(connect=3))\n            assert type(e.value.reason) == NewConnectionError\n\n    def test_timeout_success(self) -> None:\n        timeout = Timeout(connect=3, read=5, total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            pool.request(\"GET\", \"/\")\n            # This should not raise a \"Timeout already started\" error\n            pool.request(\"GET\", \"/\")\n\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            # This should also not raise a \"Timeout already started\" error\n            pool.request(\"GET\", \"/\")\n\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            pool.request(\"GET\", \"/\")\n\n    socket_timeout_reuse_testdata = pytest.mark.parametrize(\n        [\"timeout\", \"expect_settimeout_calls\"],\n        [\n            (1, (1, 1)),\n            (None, (None, None)),\n            (Timeout(read=4), (None, 4)),\n            (Timeout(read=4, connect=5), (5, 4)),\n            (Timeout(connect=6), (6, None)),\n        ],\n    )\n\n    @socket_timeout_reuse_testdata\n    def test_socket_timeout_updated_on_reuse_constructor(\n        self,\n        timeout: _TYPE_TIMEOUT,\n        expect_settimeout_calls: typing.Sequence[float | None],\n    ) -> None:\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            # Make a request to create a new connection.\n            pool.urlopen(\"GET\", \"/\")\n\n            # Grab the connection and mock the inner socket.\n            assert pool.pool is not None\n            conn = pool.pool.get_nowait()\n            conn_sock = mock.Mock(wraps=conn.sock)\n            conn.sock = conn_sock\n            pool._put_conn(conn)\n\n            # Assert that sock.settimeout() is called with the new connect timeout, then the read timeout.\n            pool.urlopen(\"GET\", \"/\", timeout=timeout)\n            conn_sock.settimeout.assert_has_calls(\n                [mock.call(x) for x in expect_settimeout_calls]\n            )\n\n    @socket_timeout_reuse_testdata\n    def test_socket_timeout_updated_on_reuse_parameter(\n        self,\n        timeout: _TYPE_TIMEOUT,\n        expect_settimeout_calls: typing.Sequence[float | None],\n    ) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # Make a request to create a new connection.\n            pool.urlopen(\"GET\", \"/\", timeout=LONG_TIMEOUT)\n\n            # Grab the connection and mock the inner socket.\n            assert pool.pool is not None\n            conn = pool.pool.get_nowait()\n            conn_sock = mock.Mock(wraps=conn.sock)\n            conn.sock = conn_sock\n            pool._put_conn(conn)\n\n            # Assert that sock.settimeout() is called with the new connect timeout, then the read timeout.\n            pool.urlopen(\"GET\", \"/\", timeout=timeout)\n            conn_sock.settimeout.assert_has_calls(\n                [mock.call(x) for x in expect_settimeout_calls]\n            )\n\n    def test_tunnel(self) -> None:\n        # note the actual httplib.py has no tests for this functionality\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                conn.set_tunnel(self.host, self.port)\n                with mock.patch.object(\n                    conn, \"_tunnel\", create=True, return_value=None\n                ) as conn_tunnel:\n                    pool._make_request(conn, \"GET\", \"/\")\n                conn_tunnel.assert_called_once_with()\n            finally:\n                conn.close()\n\n        # test that it's not called when tunnel is not set\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                with mock.patch.object(\n                    conn, \"_tunnel\", create=True, return_value=None\n                ) as conn_tunnel:\n                    pool._make_request(conn, \"GET\", \"/\")\n                assert not conn_tunnel.called\n            finally:\n                conn.close()\n\n    def test_redirect_relative_url_no_deprecation(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"error\", DeprecationWarning)\n                pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"})\n\n    def test_redirect(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, redirect=False)\n            assert r.status == 303\n\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"})\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_bad_connect(self) -> None:\n        with HTTPConnectionPool(\"badhost.invalid\", self.port) as pool:\n            with pytest.raises(MaxRetryError) as e:\n                pool.request(\"GET\", \"/\", retries=5)\n            assert type(e.value.reason) == NameResolutionError\n\n    def test_keepalive(self) -> None:\n        with HTTPConnectionPool(self.host, self.port, block=True, maxsize=1) as pool:\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n\n            assert r.status == 200\n            assert pool.num_connections == 1\n            assert pool.num_requests == 2\n\n    def test_keepalive_close(self) -> None:\n        with HTTPConnectionPool(\n            self.host, self.port, block=True, maxsize=1, timeout=2\n        ) as pool:\n            r = pool.request(\n                \"GET\", \"/keepalive?close=1\", retries=0, headers={\"Connection\": \"close\"}\n            )\n\n            assert pool.num_connections == 1\n\n            # The dummyserver will have responded with Connection:close,\n            # and httplib will properly cleanup the socket.\n\n            # We grab the HTTPConnection object straight from the Queue,\n            # because _get_conn() is where the check & reset occurs\n            assert pool.pool is not None\n            conn = pool.pool.get()\n            assert conn.sock is None\n            pool._put_conn(conn)\n\n            # Now with keep-alive\n            r = pool.request(\n                \"GET\",\n                \"/keepalive?close=0\",\n                retries=0,\n                headers={\"Connection\": \"keep-alive\"},\n            )\n\n            # The dummyserver responded with Connection:keep-alive, the connection\n            # persists.\n            conn = pool.pool.get()\n            assert conn.sock is not None\n            pool._put_conn(conn)\n\n            # Another request asking the server to close the connection. This one\n            # should get cleaned up for the next request.\n            r = pool.request(\n                \"GET\", \"/keepalive?close=1\", retries=0, headers={\"Connection\": \"close\"}\n            )\n\n            assert r.status == 200\n\n            conn = pool.pool.get()\n            assert conn.sock is None\n            pool._put_conn(conn)\n\n            # Next request\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n\n    def test_post_with_urlencode(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"banana\": \"hammock\", \"lol\": \"cat\"}\n            r = pool.request(\"POST\", \"/echo\", fields=data, encode_multipart=False)\n            assert r.data.decode(\"utf-8\") == urlencode(data)\n\n    def test_post_with_multipart(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"banana\": \"hammock\", \"lol\": \"cat\"}\n            r = pool.request(\"POST\", \"/echo\", fields=data, encode_multipart=True)\n            body = r.data.split(b\"\\r\\n\")\n\n            encoded_data = encode_multipart_formdata(data)[0]\n            expected_body = encoded_data.split(b\"\\r\\n\")\n\n            # TODO: Get rid of extra parsing stuff when you can specify\n            # a custom boundary to encode_multipart_formdata\n            \"\"\"\n            We need to loop the return lines because a timestamp is attached\n            from within encode_multipart_formdata. When the server echos back\n            the data, it has the timestamp from when the data was encoded, which\n            is not equivalent to when we run encode_multipart_formdata on\n            the data again.\n            \"\"\"\n            for i, line in enumerate(body):\n                if line.startswith(b\"--\"):\n                    continue\n\n                assert body[i] == expected_body[i]\n\n    def test_post_with_multipart__iter__(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"hello\": \"world\"}\n            r = pool.request(\n                \"POST\",\n                \"/echo\",\n                fields=data,\n                preload_content=False,\n                multipart_boundary=\"boundary\",\n                encode_multipart=True,\n            )\n\n            chunks = [chunk for chunk in r]\n            assert chunks == [\n                b\"--boundary\\r\\n\",\n                b'Content-Disposition: form-data; name=\"hello\"\\r\\n',\n                b\"\\r\\n\",\n                b\"world\\r\\n\",\n                b\"--boundary--\\r\\n\",\n            ]\n\n    def test_check_gzip(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\", \"/encodingrequest\", headers={\"accept-encoding\": \"gzip\"}\n            )\n            assert r.headers.get(\"content-encoding\") == \"gzip\"\n            assert r.data == b\"hello, world!\"\n\n    def test_check_deflate(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\", \"/encodingrequest\", headers={\"accept-encoding\": \"deflate\"}\n            )\n            assert r.headers.get(\"content-encoding\") == \"deflate\"\n            assert r.data == b\"hello, world!\"\n\n    def test_bad_decode(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with pytest.raises(DecodeError):\n                pool.request(\n                    \"GET\",\n                    \"/encodingrequest\",\n                    headers={\"accept-encoding\": \"garbage-deflate\"},\n                )\n\n            with pytest.raises(DecodeError):\n                pool.request(\n                    \"GET\",\n                    \"/encodingrequest\",\n                    headers={\"accept-encoding\": \"garbage-gzip\"},\n                )\n\n    def test_connection_count(self) -> None:\n        with HTTPConnectionPool(self.host, self.port, maxsize=1) as pool:\n            pool.request(\"GET\", \"/\")\n            pool.request(\"GET\", \"/\")\n            pool.request(\"GET\", \"/\")\n\n            assert pool.num_connections == 1\n            assert pool.num_requests == 3\n\n    def test_connection_count_bigpool(self) -> None:\n        with HTTPConnectionPool(self.host, self.port, maxsize=16) as http_pool:\n            http_pool.request(\"GET\", \"/\")\n            http_pool.request(\"GET\", \"/\")\n            http_pool.request(\"GET\", \"/\")\n\n            assert http_pool.num_connections == 1\n            assert http_pool.num_requests == 3\n\n    def test_partial_response(self) -> None:\n        with HTTPConnectionPool(self.host, self.port, maxsize=1) as pool:\n            req_data = {\"lol\": \"cat\"}\n            resp_data = urlencode(req_data).encode(\"utf-8\")\n\n            r = pool.request(\"GET\", \"/echo\", fields=req_data, preload_content=False)\n\n            assert r.read(5) == resp_data[:5]\n            assert r.read() == resp_data[5:]\n\n    def test_lazy_load_twice(self) -> None:\n        # This test is sad and confusing. Need to figure out what's\n        # going on with partial reads and socket reuse.\n\n        with HTTPConnectionPool(\n            self.host, self.port, block=True, maxsize=1, timeout=2\n        ) as pool:\n            payload_size = 1024 * 2\n            first_chunk = 512\n\n            boundary = \"foo\"\n\n            req_data = {\"count\": \"a\" * payload_size}\n            resp_data = encode_multipart_formdata(req_data, boundary=boundary)[0]\n\n            req2_data = {\"count\": \"b\" * payload_size}\n            resp2_data = encode_multipart_formdata(req2_data, boundary=boundary)[0]\n\n            r1 = pool.request(\n                \"POST\",\n                \"/echo\",\n                fields=req_data,\n                multipart_boundary=boundary,\n                preload_content=False,\n            )\n\n            assert r1.read(first_chunk) == resp_data[:first_chunk]\n\n            try:\n                r2 = pool.request(\n                    \"POST\",\n                    \"/echo\",\n                    fields=req2_data,\n                    multipart_boundary=boundary,\n                    preload_content=False,\n                    pool_timeout=0.001,\n                )\n\n                # This branch should generally bail here, but maybe someday it will\n                # work? Perhaps by some sort of magic. Consider it a TODO.\n\n                assert r2.read(first_chunk) == resp2_data[:first_chunk]\n\n                assert r1.read() == resp_data[first_chunk:]\n                assert r2.read() == resp2_data[first_chunk:]\n                assert pool.num_requests == 2\n\n            except EmptyPoolError:\n                assert r1.read() == resp_data[first_chunk:]\n                assert pool.num_requests == 1\n\n            assert pool.num_connections == 1\n\n    def test_for_double_release(self) -> None:\n        MAXSIZE = 5\n\n        # Check default state\n        with HTTPConnectionPool(self.host, self.port, maxsize=MAXSIZE) as pool:\n            assert pool.num_connections == 0\n            assert pool.pool is not None\n            assert pool.pool.qsize() == MAXSIZE\n\n            # Make an empty slot for testing\n            pool.pool.get()\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n            # Check state after simple request\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n            # Check state without release\n            pool.urlopen(\"GET\", \"/\", preload_content=False)\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            # Check state after read\n            pool.urlopen(\"GET\", \"/\").data\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n    def test_release_conn_parameter(self) -> None:\n        MAXSIZE = 5\n        with HTTPConnectionPool(self.host, self.port, maxsize=MAXSIZE) as pool:\n            assert pool.pool is not None\n            assert pool.pool.qsize() == MAXSIZE\n\n            # Make request without releasing connection\n            pool.request(\"GET\", \"/\", release_conn=False, preload_content=False)\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n    def test_dns_error(self) -> None:\n        with HTTPConnectionPool(\n            \"thishostdoesnotexist.invalid\", self.port, timeout=0.001\n        ) as pool:\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", \"/test\", retries=2)\n\n    @pytest.mark.parametrize(\"char\", [\" \", \"\\r\", \"\\n\", \"\\x00\"])\n    def test_invalid_method_not_allowed(self, char: str) -> None:\n        with pytest.raises(ValueError):\n            with HTTPConnectionPool(self.host, self.port) as pool:\n                pool.request(\"GET\" + char, \"/\")\n\n    def test_percent_encode_invalid_target_chars(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/echo_params?q=\\r&k=\\n \\n\")\n            assert r.data == b\"[('k', '\\\\n \\\\n'), ('q', '\\\\r')]\"\n\n    def test_source_address(self) -> None:\n        for addr, is_ipv6 in VALID_SOURCE_ADDRESSES:\n            if is_ipv6 and not HAS_IPV6_AND_DNS:\n                warnings.warn(\"No IPv6 support: skipping.\", NoIPv6Warning)\n                continue\n            with HTTPConnectionPool(\n                self.host, self.port, source_address=addr, retries=False\n            ) as pool:\n                r = pool.request(\"GET\", \"/source_address\")\n                assert r.data == addr[0].encode()\n\n    @pytest.mark.parametrize(\n        \"invalid_source_address, is_ipv6\", INVALID_SOURCE_ADDRESSES\n    )\n    def test_source_address_error(\n        self, invalid_source_address: tuple[str, int], is_ipv6: bool\n    ) -> None:\n        with HTTPConnectionPool(\n            self.host, self.port, source_address=invalid_source_address, retries=False\n        ) as pool:\n            if is_ipv6:\n                with pytest.raises(NameResolutionError):\n                    pool.request(\"GET\", f\"/source_address?{invalid_source_address}\")\n            else:\n                with pytest.raises(NewConnectionError):\n                    pool.request(\"GET\", f\"/source_address?{invalid_source_address}\")\n\n    def test_stream_keepalive(self) -> None:\n        x = 2\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            for _ in range(x):\n                response = pool.request(\n                    \"GET\",\n                    \"/chunked\",\n                    headers={\"Connection\": \"keep-alive\"},\n                    preload_content=False,\n                    retries=False,\n                )\n                for chunk in response.stream():\n                    assert chunk == b\"123\"\n\n            assert pool.num_connections == 1\n            assert pool.num_requests == x\n\n    def test_read_chunked_short_circuit(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\"GET\", \"/chunked\", preload_content=False)\n            response.read()\n            with pytest.raises(StopIteration):\n                next(response.read_chunked())\n\n    def test_read_chunked_on_closed_response(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\"GET\", \"/chunked\", preload_content=False)\n            response.close()\n            with pytest.raises(StopIteration):\n                next(response.read_chunked())\n\n    def test_chunked_gzip(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\n                \"GET\", \"/chunked_gzip\", preload_content=False, decode_content=True\n            )\n\n            assert b\"123\" * 4 == response.read()\n\n    def test_cleanup_on_connection_error(self) -> None:\n        \"\"\"\n        Test that connections are recycled to the pool on\n        connection errors where no http response is received.\n        \"\"\"\n        poolsize = 3\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=poolsize, block=True\n        ) as http:\n            assert http.pool is not None\n            assert http.pool.qsize() == poolsize\n\n            # force a connection error by supplying a non-existent\n            # url. We won't get a response for this  and so the\n            # conn won't be implicitly returned to the pool.\n            with pytest.raises(MaxRetryError):\n                http.request(\n                    \"GET\",\n                    \"/redirect\",\n                    fields={\"target\": \"/\"},\n                    release_conn=False,\n                    retries=0,\n                )\n\n            r = http.request(\n                \"GET\",\n                \"/redirect\",\n                fields={\"target\": \"/\"},\n                release_conn=False,\n                retries=1,\n            )\n            r.release_conn()\n\n            # the pool should still contain poolsize elements\n            assert http.pool.qsize() == http.pool.maxsize\n\n    def test_mixed_case_hostname(self) -> None:\n        with HTTPConnectionPool(\"LoCaLhOsT\", self.port) as pool:\n            response = pool.request(\"GET\", f\"http://LoCaLhOsT:{self.port}/\")\n            assert response.status == 200\n\n    def test_preserves_path_dot_segments(self) -> None:\n        \"\"\"ConnectionPool preserves dot segments in the URI\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\"GET\", \"/echo_uri/seg0/../seg2\")\n            assert response.data == b\"/echo_uri/seg0/../seg2\"\n\n    def test_default_user_agent_header(self) -> None:\n        \"\"\"ConnectionPool has a default user agent\"\"\"\n        default_ua = _get_default_user_agent()\n        custom_ua = \"I'm not a web scraper, what are you talking about?\"\n        custom_ua2 = \"Yet Another User Agent\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # Use default user agent if no user agent was specified.\n            r = pool.request(\"GET\", \"/headers\")\n            request_headers = r.json()\n            assert request_headers.get(\"User-Agent\") == _get_default_user_agent()\n\n            # Prefer the request user agent over the default.\n            headers = {\"UsEr-AGENt\": custom_ua}\n            r = pool.request(\"GET\", \"/headers\", headers=headers)\n            request_headers = r.json()\n            assert request_headers.get(\"User-Agent\") == custom_ua\n\n            # Do not modify pool headers when using the default user agent.\n            pool_headers = {\"foo\": \"bar\"}\n            pool.headers = pool_headers\n            r = pool.request(\"GET\", \"/headers\")\n            request_headers = r.json()\n            assert request_headers.get(\"User-Agent\") == default_ua\n            assert \"User-Agent\" not in pool_headers\n\n            pool.headers.update({\"User-Agent\": custom_ua2})\n            r = pool.request(\"GET\", \"/headers\")\n            request_headers = r.json()\n            assert request_headers.get(\"User-Agent\") == custom_ua2\n\n    @pytest.mark.parametrize(\n        \"headers\",\n        [\n            None,\n            {},\n            {\"User-Agent\": \"key\"},\n            {\"user-agent\": \"key\"},\n            {b\"uSeR-AgEnT\": b\"key\"},\n            {b\"user-agent\": \"key\"},\n        ],\n    )\n    @pytest.mark.parametrize(\"chunked\", [True, False])\n    def test_user_agent_header_not_sent_twice(\n        self, headers: dict[str, str] | None, chunked: bool\n    ) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/headers\", headers=headers, chunked=chunked)\n            request_headers = r.json()\n\n            if not headers:\n                assert request_headers[\"User-Agent\"].startswith(\"python-urllib3/\")\n                assert \"key\" not in request_headers[\"User-Agent\"]\n            else:\n                assert request_headers[\"User-Agent\"] == \"key\"\n\n    def test_no_user_agent_header(self) -> None:\n        \"\"\"ConnectionPool can suppress sending a user agent header\"\"\"\n        custom_ua = \"I'm not a web scraper, what are you talking about?\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # Suppress user agent in the request headers.\n            no_ua_headers = {\"User-Agent\": SKIP_HEADER}\n            r = pool.request(\"GET\", \"/headers\", headers=no_ua_headers)\n            request_headers = r.json()\n            assert \"User-Agent\" not in request_headers\n            assert no_ua_headers[\"User-Agent\"] == SKIP_HEADER\n\n            # Suppress user agent in the pool headers.\n            pool.headers = no_ua_headers\n            r = pool.request(\"GET\", \"/headers\")\n            request_headers = r.json()\n            assert \"User-Agent\" not in request_headers\n            assert no_ua_headers[\"User-Agent\"] == SKIP_HEADER\n\n            # Request headers override pool headers.\n            pool_headers = {\"User-Agent\": custom_ua}\n            pool.headers = pool_headers\n            r = pool.request(\"GET\", \"/headers\", headers=no_ua_headers)\n            request_headers = r.json()\n            assert \"User-Agent\" not in request_headers\n            assert no_ua_headers[\"User-Agent\"] == SKIP_HEADER\n            assert pool_headers.get(\"User-Agent\") == custom_ua\n\n    @pytest.mark.parametrize(\n        \"accept_encoding\",\n        [\n            \"Accept-Encoding\",\n            \"accept-encoding\",\n            b\"Accept-Encoding\",\n            b\"accept-encoding\",\n            None,\n        ],\n    )\n    @pytest.mark.parametrize(\"host\", [\"Host\", \"host\", b\"Host\", b\"host\", None])\n    @pytest.mark.parametrize(\n        \"user_agent\", [\"User-Agent\", \"user-agent\", b\"User-Agent\", b\"user-agent\", None]\n    )\n    @pytest.mark.parametrize(\"chunked\", [True, False])\n    def test_skip_header(\n        self,\n        accept_encoding: str | None,\n        host: str | None,\n        user_agent: str | None,\n        chunked: bool,\n    ) -> None:\n        headers = {}\n\n        if accept_encoding is not None:\n            headers[accept_encoding] = SKIP_HEADER\n        if host is not None:\n            headers[host] = SKIP_HEADER\n        if user_agent is not None:\n            headers[user_agent] = SKIP_HEADER\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/headers\", headers=headers, chunked=chunked)\n        request_headers = r.json()\n\n        if accept_encoding is None:\n            assert \"Accept-Encoding\" in request_headers\n        else:\n            assert accept_encoding not in request_headers\n        if host is None:\n            assert \"Host\" in request_headers\n        else:\n            assert host not in request_headers\n        if user_agent is None:\n            assert \"User-Agent\" in request_headers\n        else:\n            assert user_agent not in request_headers\n\n    @pytest.mark.parametrize(\"header\", [\"Content-Length\", \"content-length\"])\n    @pytest.mark.parametrize(\"chunked\", [True, False])\n    def test_skip_header_non_supported(self, header: str, chunked: bool) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with pytest.raises(\n                ValueError,\n                match=\"urllib3.util.SKIP_HEADER only supports 'Accept-Encoding', 'Host', 'User-Agent'\",\n            ) as e:\n                pool.request(\n                    \"GET\", \"/headers\", headers={header: SKIP_HEADER}, chunked=chunked\n                )\n            # Ensure that the error message stays up to date with 'SKIP_HEADER_SUPPORTED_HEADERS'\n            assert all(\n                (\"'\" + header.title() + \"'\") in str(e.value)\n                for header in SKIPPABLE_HEADERS\n            )\n\n    @pytest.mark.parametrize(\"chunked\", [True, False])\n    @pytest.mark.parametrize(\"pool_request\", [True, False])\n    @pytest.mark.parametrize(\"header_type\", [dict, HTTPHeaderDict])\n    def test_headers_not_modified_by_request(\n        self,\n        chunked: bool,\n        pool_request: bool,\n        header_type: type[dict[str, str] | HTTPHeaderDict],\n    ) -> None:\n        # Test that the .request*() methods of ConnectionPool and HTTPConnection\n        # don't modify the given 'headers' structure, instead they should\n        # make their own internal copies at request time.\n        headers = header_type()\n        headers[\"key\"] = \"val\"\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            pool.headers = headers\n            if pool_request:\n                pool.request(\"GET\", \"/headers\", chunked=chunked)\n            else:\n                conn = pool._get_conn()\n                conn.request(\"GET\", \"/headers\", chunked=chunked)\n\n            assert pool.headers == {\"key\": \"val\"}\n            assert isinstance(pool.headers, header_type)\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            if pool_request:\n                pool.request(\"GET\", \"/headers\", headers=headers, chunked=chunked)\n            else:\n                conn = pool._get_conn()\n                conn.request(\"GET\", \"/headers\", headers=headers, chunked=chunked)\n\n            assert headers == {\"key\": \"val\"}\n\n    def test_request_chunked_is_deprecated(\n        self,\n    ) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            conn = pool._get_conn()\n\n            with pytest.warns(DeprecationWarning) as w:\n                conn.request_chunked(\"GET\", \"/headers\")  # type: ignore[attr-defined]\n            assert len(w) == 1 and str(w[0].message) == (\n                \"HTTPConnection.request_chunked() is deprecated and will be removed in urllib3 v2.1.0. \"\n                \"Instead use HTTPConnection.request(..., chunked=True).\"\n            )\n\n            resp = conn.getresponse()\n            assert resp.status == 200\n            assert resp.json()[\"Transfer-Encoding\"] == \"chunked\"\n\n    def test_bytes_header(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"User-Agent\": \"test header\"}\n            r = pool.request(\"GET\", \"/headers\", headers=headers)\n            request_headers = r.json()\n            assert \"User-Agent\" in request_headers\n            assert request_headers[\"User-Agent\"] == \"test header\"\n\n    @pytest.mark.parametrize(\n        \"user_agent\", [\"Sch\u00f6nefeld/1.18.0\", \"Sch\u00f6nefeld/1.18.0\".encode(\"iso-8859-1\")]\n    )\n    def test_user_agent_non_ascii_user_agent(self, user_agent: str) -> None:\n        with HTTPConnectionPool(self.host, self.port, retries=False) as pool:\n            r = pool.urlopen(\n                \"GET\",\n                \"/headers\",\n                headers={\"User-Agent\": user_agent},\n            )\n            request_headers = r.json()\n            assert \"User-Agent\" in request_headers\n            assert request_headers[\"User-Agent\"] == \"Sch\u00f6nefeld/1.18.0\"\n\n\nclass TestRetry(HTTPDummyServerTestCase):\n    def test_max_retry(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, retries=0)\n\n    def test_disabled_retry(self) -> None:\n        \"\"\"Disabled retries should disable redirect handling.\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, retries=False)\n            assert r.status == 303\n\n            r = pool.request(\n                \"GET\",\n                \"/redirect\",\n                fields={\"target\": \"/\"},\n                retries=Retry(redirect=False),\n            )\n            assert r.status == 303\n\n        with HTTPConnectionPool(\n            \"thishostdoesnotexist.invalid\", self.port, timeout=0.001\n        ) as pool:\n            with pytest.raises(NameResolutionError):\n                pool.request(\"GET\", \"/test\", retries=False)\n\n    def test_read_retries(self) -> None:\n        \"\"\"Should retry for status codes in the forcelist\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(read=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_read_retries\"},\n                retries=retry,\n            )\n            assert resp.status == 200\n\n    def test_read_total_retries(self) -> None:\n        \"\"\"HTTP response w/ status code in the forcelist should be retried\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_read_total_retries\"}\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n    def test_retries_wrong_forcelist(self) -> None:\n        \"\"\"HTTP response w/ status code not in forcelist shouldn't be retried\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(total=1, status_forcelist=[202])\n            resp = pool.request(\n                \"GET\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_wrong_forcelist\"},\n                retries=retry,\n            )\n            assert resp.status == 418\n\n    def test_default_method_forcelist_retried(self) -> None:\n        \"\"\"urllib3 should retry methods in the default method forcelist\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"OPTIONS\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_default_forcelist\"},\n                retries=retry,\n            )\n            assert resp.status == 200\n\n    def test_retries_wrong_method_list(self) -> None:\n        \"\"\"Method not in our allowed list should not be retried, even if code matches\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_wrong_allowed_method\"}\n            retry = Retry(total=1, status_forcelist=[418], allowed_methods=[\"POST\"])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 418\n\n    def test_read_retries_unsuccessful(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_read_retries_unsuccessful\"}\n            resp = pool.request(\"GET\", \"/successful_retry\", headers=headers, retries=1)\n            assert resp.status == 418\n\n    def test_retry_reuse_safe(self) -> None:\n        \"\"\"It should be possible to reuse a Retry object across requests\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_retry_safe\"}\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n    def test_retry_return_in_response(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_retry_return_in_response\"}\n            retry = Retry(total=2, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n            assert resp.retries is not None\n            assert resp.retries.total == 1\n            assert resp.retries.history == (\n                RequestHistory(\"GET\", \"/successful_retry\", None, 418, None),\n            )\n\n    def test_retry_redirect_history(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            resp = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"})\n            assert resp.status == 200\n            assert resp.retries is not None\n            assert resp.retries.history == (\n                RequestHistory(\"GET\", \"/redirect?target=%2F\", None, 303, \"/\"),\n            )\n\n    def test_multi_redirect_history(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/multi_redirect\",\n                fields={\"redirect_codes\": \"303,302,200\"},\n                redirect=False,\n            )\n            assert r.status == 303\n            assert r.retries is not None\n            assert r.retries.history == tuple()\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/multi_redirect\",\n                retries=10,\n                fields={\"redirect_codes\": \"303,302,301,307,302,200\"},\n            )\n            assert r.status == 200\n            assert r.data == b\"Done redirecting\"\n\n            expected = [\n                (303, \"/multi_redirect?redirect_codes=302,301,307,302,200\"),\n                (302, \"/multi_redirect?redirect_codes=301,307,302,200\"),\n                (301, \"/multi_redirect?redirect_codes=307,302,200\"),\n                (307, \"/multi_redirect?redirect_codes=302,200\"),\n                (302, \"/multi_redirect?redirect_codes=200\"),\n            ]\n            assert r.retries is not None\n            actual = [\n                (history.status, history.redirect_location)\n                for history in r.retries.history\n            ]\n            assert actual == expected\n\n\nclass TestRetryAfter(HTTPDummyServerTestCase):\n    def test_retry_after(self) -> None:\n        # Request twice in a second to get a 429 response.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=False,\n            )\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=False,\n            )\n            assert r.status == 429\n\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=True,\n            )\n            assert r.status == 200\n\n            # Request twice in a second to get a 503 response.\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=False,\n            )\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=False,\n            )\n            assert r.status == 503\n\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=True,\n            )\n            assert r.status == 200\n\n            # Ignore Retry-After header on status which is not defined in\n            # Retry.RETRY_AFTER_STATUS_CODES.\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"418 I'm a teapot\"},\n                retries=True,\n            )\n            assert r.status == 418\n\n    def test_redirect_after(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect_after\", retries=False)\n            assert r.status == 303\n\n            t = time.time()\n            r = pool.request(\"GET\", \"/redirect_after\")\n            assert r.status == 200\n            delta = time.time() - t\n            assert delta >= 1\n\n            t = time.time()\n            timestamp = t + 2\n            r = pool.request(\"GET\", \"/redirect_after?date=\" + str(timestamp))\n            assert r.status == 200\n            delta = time.time() - t\n            assert delta >= 1\n\n            # Retry-After is past\n            t = time.time()\n            timestamp = t - 1\n            r = pool.request(\"GET\", \"/redirect_after?date=\" + str(timestamp))\n            delta = time.time() - t\n            assert r.status == 200\n            assert delta < 1\n\n\nclass TestFileBodiesOnRetryOrRedirect(HTTPDummyServerTestCase):\n    def test_retries_put_filehandle(self) -> None:\n        \"\"\"HTTP PUT retry with a file-like object should not timeout\"\"\"\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            retry = Retry(total=3, status_forcelist=[418])\n            # httplib reads in 8k chunks; use a larger content length\n            content_length = 65535\n            data = b\"A\" * content_length\n            uploaded_file = io.BytesIO(data)\n            headers = {\n                \"test-name\": \"test_retries_put_filehandle\",\n                \"Content-Length\": str(content_length),\n            }\n            resp = pool.urlopen(\n                \"PUT\",\n                \"/successful_retry\",\n                headers=headers,\n                retries=retry,\n                body=uploaded_file,\n                assert_same_host=False,\n                redirect=False,\n            )\n            assert resp.status == 200\n\n    def test_redirect_put_file(self) -> None:\n        \"\"\"PUT with file object should work with a redirection response\"\"\"\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            retry = Retry(total=3, status_forcelist=[418])\n            # httplib reads in 8k chunks; use a larger content length\n            content_length = 65535\n            data = b\"A\" * content_length\n            uploaded_file = io.BytesIO(data)\n            headers = {\n                \"test-name\": \"test_redirect_put_file\",\n                \"Content-Length\": str(content_length),\n            }\n            url = \"/redirect?target=/echo&status=307\"\n            resp = pool.urlopen(\n                \"PUT\",\n                url,\n                headers=headers,\n                retries=retry,\n                body=uploaded_file,\n                assert_same_host=False,\n                redirect=True,\n            )\n            assert resp.status == 200\n            assert resp.data == data\n\n    def test_redirect_with_failed_tell(self) -> None:\n        \"\"\"Abort request if failed to get a position from tell()\"\"\"\n\n        class BadTellObject(io.BytesIO):\n            def tell(self) -> typing.NoReturn:\n                raise OSError\n\n        body = BadTellObject(b\"the data\")\n        url = \"/redirect?target=/successful_retry\"\n        # httplib uses fileno if Content-Length isn't supplied,\n        # which is unsupported by BytesIO.\n        headers = {\"Content-Length\": \"8\"}\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            with pytest.raises(\n                UnrewindableBodyError, match=\"Unable to record file position for\"\n            ):\n                pool.urlopen(\"PUT\", url, headers=headers, body=body)\n\n\nclass TestRetryPoolSize(HTTPDummyServerTestCase):\n    def test_pool_size_retry(self) -> None:\n        retries = Retry(total=1, raise_on_status=False, status_forcelist=[404])\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=10, retries=retries, block=True\n        ) as pool:\n            pool.urlopen(\"GET\", \"/not_found\", preload_content=False)\n            assert pool.num_connections == 1\n\n\nclass TestRedirectPoolSize(HTTPDummyServerTestCase):\n    def test_pool_size_redirect(self) -> None:\n        retries = Retry(\n            total=1, raise_on_status=False, status_forcelist=[404], redirect=True\n        )\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=10, retries=retries, block=True\n        ) as pool:\n            pool.urlopen(\"GET\", \"/redirect\", preload_content=False)\n            assert pool.num_connections == 1\n", "from __future__ import annotations\n\nimport gzip\nimport typing\nfrom test import LONG_TIMEOUT\nfrom unittest import mock\n\nimport pytest\n\nfrom dummyserver.server import HAS_IPV6\nfrom dummyserver.testcase import HTTPDummyServerTestCase, IPv6HTTPDummyServerTestCase\nfrom urllib3 import HTTPHeaderDict, HTTPResponse, request\nfrom urllib3.connectionpool import port_by_scheme\nfrom urllib3.exceptions import MaxRetryError, URLSchemeUnknown\nfrom urllib3.poolmanager import PoolManager\nfrom urllib3.util.retry import Retry\n\n\nclass TestPoolManager(HTTPDummyServerTestCase):\n    @classmethod\n    def setup_class(cls) -> None:\n        super().setup_class()\n        cls.base_url = f\"http://{cls.host}:{cls.port}\"\n        cls.base_url_alt = f\"http://{cls.host_alt}:{cls.port}\"\n\n    def test_redirect(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url}/\"},\n                redirect=False,\n            )\n\n            assert r.status == 303\n\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url}/\"},\n            )\n\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_redirect_twice(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url}/redirect\"},\n                redirect=False,\n            )\n\n            assert r.status == 303\n\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url}/redirect?target={self.base_url}/\"},\n            )\n\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_redirect_to_relative_url(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": \"/redirect\"},\n                redirect=False,\n            )\n\n            assert r.status == 303\n\n            r = http.request(\n                \"GET\", f\"{self.base_url}/redirect\", fields={\"target\": \"/redirect\"}\n            )\n\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_cross_host_redirect(self) -> None:\n        with PoolManager() as http:\n            cross_host_location = f\"{self.base_url_alt}/echo?a=b\"\n            with pytest.raises(MaxRetryError):\n                http.request(\n                    \"GET\",\n                    f\"{self.base_url}/redirect\",\n                    fields={\"target\": cross_host_location},\n                    timeout=LONG_TIMEOUT,\n                    retries=0,\n                )\n\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/echo?a=b\"},\n                timeout=LONG_TIMEOUT,\n                retries=1,\n            )\n\n            assert isinstance(r, HTTPResponse)\n            assert r._pool is not None\n            assert r._pool.host == self.host_alt\n\n    def test_too_many_redirects(self) -> None:\n        with PoolManager() as http:\n            with pytest.raises(MaxRetryError):\n                http.request(\n                    \"GET\",\n                    f\"{self.base_url}/redirect\",\n                    fields={\n                        \"target\": f\"{self.base_url}/redirect?target={self.base_url}/\"\n                    },\n                    retries=1,\n                    preload_content=False,\n                )\n\n            with pytest.raises(MaxRetryError):\n                http.request(\n                    \"GET\",\n                    f\"{self.base_url}/redirect\",\n                    fields={\n                        \"target\": f\"{self.base_url}/redirect?target={self.base_url}/\"\n                    },\n                    retries=Retry(total=None, redirect=1),\n                    preload_content=False,\n                )\n\n            # Even with preload_content=False and raise on redirects, we reused the same\n            # connection\n            assert len(http.pools) == 1\n            pool = http.connection_from_host(self.host, self.port)\n            assert pool.num_connections == 1\n\n    def test_redirect_cross_host_remove_headers(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/headers\"},\n                headers={\"Authorization\": \"foo\", \"Cookie\": \"foo=bar\"},\n            )\n\n            assert r.status == 200\n\n            data = r.json()\n\n            assert \"Authorization\" not in data\n            assert \"Cookie\" not in data\n\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/headers\"},\n                headers={\"authorization\": \"foo\", \"cookie\": \"foo=bar\"},\n            )\n\n            assert r.status == 200\n\n            data = r.json()\n\n            assert \"authorization\" not in data\n            assert \"Authorization\" not in data\n            assert \"cookie\" not in data\n            assert \"Cookie\" not in data\n\n    def test_redirect_cross_host_no_remove_headers(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/headers\"},\n                headers={\"Authorization\": \"foo\", \"Cookie\": \"foo=bar\"},\n                retries=Retry(remove_headers_on_redirect=[]),\n            )\n\n            assert r.status == 200\n\n            data = r.json()\n\n            assert data[\"Authorization\"] == \"foo\"\n            assert data[\"Cookie\"] == \"foo=bar\"\n\n    def test_redirect_cross_host_set_removed_headers(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/headers\"},\n                headers={\n                    \"X-API-Secret\": \"foo\",\n                    \"Authorization\": \"bar\",\n                    \"Cookie\": \"foo=bar\",\n                },\n                retries=Retry(remove_headers_on_redirect=[\"X-API-Secret\"]),\n            )\n\n            assert r.status == 200\n\n            data = r.json()\n\n            assert \"X-API-Secret\" not in data\n            assert data[\"Authorization\"] == \"bar\"\n            assert data[\"Cookie\"] == \"foo=bar\"\n\n            headers = {\n                \"x-api-secret\": \"foo\",\n                \"authorization\": \"bar\",\n                \"cookie\": \"foo=bar\",\n            }\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/headers\"},\n                headers=headers,\n                retries=Retry(remove_headers_on_redirect=[\"X-API-Secret\"]),\n            )\n\n            assert r.status == 200\n\n            data = r.json()\n\n            assert \"x-api-secret\" not in data\n            assert \"X-API-Secret\" not in data\n            assert data[\"Authorization\"] == \"bar\"\n            assert data[\"Cookie\"] == \"foo=bar\"\n\n            # Ensure the header argument itself is not modified in-place.\n            assert headers == {\n                \"x-api-secret\": \"foo\",\n                \"authorization\": \"bar\",\n                \"cookie\": \"foo=bar\",\n            }\n\n    def test_redirect_without_preload_releases_connection(self) -> None:\n        with PoolManager(block=True, maxsize=2) as http:\n            r = http.request(\"GET\", f\"{self.base_url}/redirect\", preload_content=False)\n            assert isinstance(r, HTTPResponse)\n            assert r._pool is not None\n            assert r._pool.num_requests == 2\n            assert r._pool.num_connections == 1\n            assert len(http.pools) == 1\n\n    def test_unknown_scheme(self) -> None:\n        with PoolManager() as http:\n            unknown_scheme = \"unknown\"\n            unknown_scheme_url = f\"{unknown_scheme}://host\"\n            with pytest.raises(URLSchemeUnknown) as e:\n                r = http.request(\"GET\", unknown_scheme_url)\n            assert e.value.scheme == unknown_scheme\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": unknown_scheme_url},\n                redirect=False,\n            )\n            assert r.status == 303\n            assert r.headers.get(\"Location\") == unknown_scheme_url\n            with pytest.raises(URLSchemeUnknown) as e:\n                r = http.request(\n                    \"GET\",\n                    f\"{self.base_url}/redirect\",\n                    fields={\"target\": unknown_scheme_url},\n                )\n            assert e.value.scheme == unknown_scheme\n\n    def test_raise_on_redirect(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url}/redirect?target={self.base_url}/\"},\n                retries=Retry(total=None, redirect=1, raise_on_redirect=False),\n            )\n\n            assert r.status == 303\n\n    def test_raise_on_status(self) -> None:\n        with PoolManager() as http:\n            with pytest.raises(MaxRetryError):\n                # the default is to raise\n                r = http.request(\n                    \"GET\",\n                    f\"{self.base_url}/status\",\n                    fields={\"status\": \"500 Internal Server Error\"},\n                    retries=Retry(total=1, status_forcelist=range(500, 600)),\n                )\n\n            with pytest.raises(MaxRetryError):\n                # raise explicitly\n                r = http.request(\n                    \"GET\",\n                    f\"{self.base_url}/status\",\n                    fields={\"status\": \"500 Internal Server Error\"},\n                    retries=Retry(\n                        total=1, status_forcelist=range(500, 600), raise_on_status=True\n                    ),\n                )\n\n            # don't raise\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/status\",\n                fields={\"status\": \"500 Internal Server Error\"},\n                retries=Retry(\n                    total=1, status_forcelist=range(500, 600), raise_on_status=False\n                ),\n            )\n\n            assert r.status == 500\n\n    def test_missing_port(self) -> None:\n        # Can a URL that lacks an explicit port like ':80' succeed, or\n        # will all such URLs fail with an error?\n\n        with PoolManager() as http:\n            # By globally adjusting `port_by_scheme` we pretend for a moment\n            # that HTTP's default port is not 80, but is the port at which\n            # our test server happens to be listening.\n            port_by_scheme[\"http\"] = self.port\n            try:\n                r = http.request(\"GET\", f\"http://{self.host}/\", retries=0)\n            finally:\n                port_by_scheme[\"http\"] = 80\n\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_headers(self) -> None:\n        with PoolManager(headers={\"Foo\": \"bar\"}) as http:\n            r = http.request(\"GET\", f\"{self.base_url}/headers\")\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") == \"bar\"\n\n            r = http.request(\"POST\", f\"{self.base_url}/headers\")\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") == \"bar\"\n\n            r = http.request_encode_url(\"GET\", f\"{self.base_url}/headers\")\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") == \"bar\"\n\n            r = http.request_encode_body(\"POST\", f\"{self.base_url}/headers\")\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") == \"bar\"\n\n            r = http.request_encode_url(\n                \"GET\", f\"{self.base_url}/headers\", headers={\"Baz\": \"quux\"}\n            )\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") is None\n            assert returned_headers.get(\"Baz\") == \"quux\"\n\n            r = http.request_encode_body(\n                \"GET\", f\"{self.base_url}/headers\", headers={\"Baz\": \"quux\"}\n            )\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") is None\n            assert returned_headers.get(\"Baz\") == \"quux\"\n\n    def test_headers_http_header_dict(self) -> None:\n        # Test uses a list of headers to assert the order\n        # that headers are sent in the request too.\n\n        headers = HTTPHeaderDict()\n        headers.add(\"Foo\", \"bar\")\n        headers.add(\"Multi\", \"1\")\n        headers.add(\"Baz\", \"quux\")\n        headers.add(\"Multi\", \"2\")\n\n        with PoolManager(headers=headers) as http:\n            r = http.request(\"GET\", f\"{self.base_url}/multi_headers\")\n            returned_headers = r.json()[\"headers\"]\n            assert returned_headers[-4:] == [\n                [\"Foo\", \"bar\"],\n                [\"Multi\", \"1\"],\n                [\"Multi\", \"2\"],\n                [\"Baz\", \"quux\"],\n            ]\n\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/multi_headers\",\n                headers={\n                    **headers,\n                    \"Extra\": \"extra\",\n                    \"Foo\": \"new\",\n                },\n            )\n            returned_headers = r.json()[\"headers\"]\n            assert returned_headers[-4:] == [\n                [\"Foo\", \"new\"],\n                [\"Multi\", \"1, 2\"],\n                [\"Baz\", \"quux\"],\n                [\"Extra\", \"extra\"],\n            ]\n\n    def test_merge_headers_with_pool_manager_headers(self) -> None:\n        headers = HTTPHeaderDict()\n        headers.add(\"Cookie\", \"choc-chip\")\n        headers.add(\"Cookie\", \"oatmeal-raisin\")\n        orig = headers.copy()\n        added_headers = {\"Cookie\": \"tim-tam\"}\n\n        with PoolManager(headers=headers) as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/multi_headers\",\n                headers=typing.cast(HTTPHeaderDict, http.headers) | added_headers,\n            )\n            returned_headers = r.json()[\"headers\"]\n            assert returned_headers[-3:] == [\n                [\"Cookie\", \"choc-chip\"],\n                [\"Cookie\", \"oatmeal-raisin\"],\n                [\"Cookie\", \"tim-tam\"],\n            ]\n            # make sure the pool headers weren't modified\n            assert http.headers == orig\n\n    def test_headers_http_multi_header_multipart(self) -> None:\n        headers = HTTPHeaderDict()\n        headers.add(\"Multi\", \"1\")\n        headers.add(\"Multi\", \"2\")\n        old_headers = headers.copy()\n\n        with PoolManager(headers=headers) as http:\n            r = http.request(\n                \"POST\",\n                f\"{self.base_url}/multi_headers\",\n                fields={\"k\": \"v\"},\n                multipart_boundary=\"b\",\n                encode_multipart=True,\n            )\n            returned_headers = r.json()[\"headers\"]\n            assert returned_headers[4:] == [\n                [\"Multi\", \"1\"],\n                [\"Multi\", \"2\"],\n                [\"Content-Type\", \"multipart/form-data; boundary=b\"],\n            ]\n            # Assert that the previous headers weren't modified.\n            assert headers == old_headers\n\n            # Set a default value for the Content-Type\n            headers[\"Content-Type\"] = \"multipart/form-data; boundary=b; field=value\"\n            r = http.request(\n                \"POST\",\n                f\"{self.base_url}/multi_headers\",\n                fields={\"k\": \"v\"},\n                multipart_boundary=\"b\",\n                encode_multipart=True,\n            )\n            returned_headers = r.json()[\"headers\"]\n            assert returned_headers[4:] == [\n                [\"Multi\", \"1\"],\n                [\"Multi\", \"2\"],\n                # Uses the set value, not the one that would be generated.\n                [\"Content-Type\", \"multipart/form-data; boundary=b; field=value\"],\n            ]\n\n    def test_body(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\"POST\", f\"{self.base_url}/echo\", body=b\"test\")\n            assert r.data == b\"test\"\n\n    def test_http_with_ssl_keywords(self) -> None:\n        with PoolManager(ca_certs=\"REQUIRED\") as http:\n            r = http.request(\"GET\", f\"http://{self.host}:{self.port}/\")\n            assert r.status == 200\n\n    def test_http_with_server_hostname(self) -> None:\n        with PoolManager(server_hostname=\"example.com\") as http:\n            r = http.request(\"GET\", f\"http://{self.host}:{self.port}/\")\n            assert r.status == 200\n\n    def test_http_with_ca_cert_dir(self) -> None:\n        with PoolManager(ca_certs=\"REQUIRED\", ca_cert_dir=\"/nosuchdir\") as http:\n            r = http.request(\"GET\", f\"http://{self.host}:{self.port}/\")\n            assert r.status == 200\n\n    @pytest.mark.parametrize(\n        [\"target\", \"expected_target\"],\n        [\n            (\"/echo_uri?q=1#fragment\", b\"/echo_uri?q=1\"),\n            (\"/echo_uri?#\", b\"/echo_uri?\"),\n            (\"/echo_uri#?\", b\"/echo_uri\"),\n            (\"/echo_uri#?#\", b\"/echo_uri\"),\n            (\"/echo_uri??#\", b\"/echo_uri??\"),\n            (\"/echo_uri?%3f#\", b\"/echo_uri?%3F\"),\n            (\"/echo_uri?%3F#\", b\"/echo_uri?%3F\"),\n            (\"/echo_uri?[]\", b\"/echo_uri?%5B%5D\"),\n        ],\n    )\n    def test_encode_http_target(self, target: str, expected_target: bytes) -> None:\n        with PoolManager() as http:\n            url = f\"http://{self.host}:{self.port}{target}\"\n            r = http.request(\"GET\", url)\n            assert r.data == expected_target\n\n    def test_top_level_request(self) -> None:\n        r = request(\"GET\", f\"{self.base_url}/\")\n        assert r.status == 200\n        assert r.data == b\"Dummy server!\"\n\n    def test_top_level_request_without_keyword_args(self) -> None:\n        body = \"\"\n        with pytest.raises(TypeError):\n            request(\"GET\", f\"{self.base_url}/\", body)  # type: ignore[misc]\n\n    def test_top_level_request_with_body(self) -> None:\n        r = request(\"POST\", f\"{self.base_url}/echo\", body=b\"test\")\n        assert r.status == 200\n        assert r.data == b\"test\"\n\n    def test_top_level_request_with_preload_content(self) -> None:\n        r = request(\"GET\", f\"{self.base_url}/echo\", preload_content=False)\n        assert r.status == 200\n        assert r.connection is not None\n        r.data\n        assert r.connection is None\n\n    def test_top_level_request_with_decode_content(self) -> None:\n        r = request(\n            \"GET\",\n            f\"{self.base_url}/encodingrequest\",\n            headers={\"accept-encoding\": \"gzip\"},\n            decode_content=False,\n        )\n        assert r.status == 200\n        assert gzip.decompress(r.data) == b\"hello, world!\"\n\n        r = request(\n            \"GET\",\n            f\"{self.base_url}/encodingrequest\",\n            headers={\"accept-encoding\": \"gzip\"},\n            decode_content=True,\n        )\n        assert r.status == 200\n        assert r.data == b\"hello, world!\"\n\n    def test_top_level_request_with_redirect(self) -> None:\n        r = request(\n            \"GET\",\n            f\"{self.base_url}/redirect\",\n            fields={\"target\": f\"{self.base_url}/\"},\n            redirect=False,\n        )\n\n        assert r.status == 303\n\n        r = request(\n            \"GET\",\n            f\"{self.base_url}/redirect\",\n            fields={\"target\": f\"{self.base_url}/\"},\n            redirect=True,\n        )\n\n        assert r.status == 200\n        assert r.data == b\"Dummy server!\"\n\n    def test_top_level_request_with_retries(self) -> None:\n        r = request(\"GET\", f\"{self.base_url}/redirect\", retries=False)\n        assert r.status == 303\n\n        r = request(\"GET\", f\"{self.base_url}/redirect\", retries=3)\n        assert r.status == 200\n\n    def test_top_level_request_with_timeout(self) -> None:\n        with mock.patch(\"urllib3.poolmanager.RequestMethods.request\") as mockRequest:\n            mockRequest.return_value = HTTPResponse(status=200)\n\n            r = request(\"GET\", f\"{self.base_url}/redirect\", timeout=2.5)\n\n            assert r.status == 200\n\n            mockRequest.assert_called_with(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                body=None,\n                fields=None,\n                headers=None,\n                preload_content=True,\n                decode_content=True,\n                redirect=True,\n                retries=None,\n                timeout=2.5,\n                json=None,\n            )\n\n    @pytest.mark.parametrize(\n        \"headers\",\n        [\n            None,\n            {\"content-Type\": \"application/json\"},\n            {\"content-Type\": \"text/plain\"},\n            {\"attribute\": \"value\", \"CONTENT-TYPE\": \"application/json\"},\n            HTTPHeaderDict(cookie=\"foo, bar\"),\n        ],\n    )\n    def test_request_with_json(self, headers: HTTPHeaderDict) -> None:\n        body = {\"attribute\": \"value\"}\n        r = request(\n            method=\"POST\", url=f\"{self.base_url}/echo_json\", headers=headers, json=body\n        )\n        assert r.status == 200\n        assert r.json() == body\n        if headers is not None and \"application/json\" not in headers.values():\n            assert \"text/plain\" in r.headers[\"Content-Type\"].replace(\" \", \"\").split(\",\")\n        else:\n            assert \"application/json\" in r.headers[\"Content-Type\"].replace(\n                \" \", \"\"\n            ).split(\",\")\n\n    def test_top_level_request_with_json_with_httpheaderdict(self) -> None:\n        body = {\"attribute\": \"value\"}\n        header = HTTPHeaderDict(cookie=\"foo, bar\")\n        with PoolManager(headers=header) as http:\n            r = http.request(method=\"POST\", url=f\"{self.base_url}/echo_json\", json=body)\n            assert r.status == 200\n            assert r.json() == body\n            assert \"application/json\" in r.headers[\"Content-Type\"].replace(\n                \" \", \"\"\n            ).split(\",\")\n\n    def test_top_level_request_with_body_and_json(self) -> None:\n        match = \"request got values for both 'body' and 'json' parameters which are mutually exclusive\"\n        with pytest.raises(TypeError, match=match):\n            body = {\"attribute\": \"value\"}\n            request(method=\"POST\", url=f\"{self.base_url}/echo\", body=\"\", json=body)\n\n    def test_top_level_request_with_invalid_body(self) -> None:\n        class BadBody:\n            def __repr__(self) -> str:\n                return \"<BadBody>\"\n\n        with pytest.raises(TypeError) as e:\n            request(\n                method=\"POST\",\n                url=f\"{self.base_url}/echo\",\n                body=BadBody(),  # type: ignore[arg-type]\n            )\n        assert str(e.value) == (\n            \"'body' must be a bytes-like object, file-like \"\n            \"object, or iterable. Instead was <BadBody>\"\n        )\n\n\n@pytest.mark.skipif(not HAS_IPV6, reason=\"IPv6 is not supported on this system\")\nclass TestIPv6PoolManager(IPv6HTTPDummyServerTestCase):\n    @classmethod\n    def setup_class(cls) -> None:\n        super().setup_class()\n        cls.base_url = f\"http://[{cls.host}]:{cls.port}\"\n\n    def test_ipv6(self) -> None:\n        with PoolManager() as http:\n            http.request(\"GET\", self.base_url)\n"], "fixing_code": ["version: 2\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\n\npython:\n  install:\n    - requirements: docs/requirements.txt\n    - method: pip\n      path: .\n      extra_requirements:\n        - brotli\n        - secure\n        - socks\n        - zstd\n\nsphinx:\n  fail_on_warning: true\n", "2.0.7 (2023-10-17)\n==================\n\n* Made body stripped from HTTP requests changing the request method to GET after HTTP 303 \"See Other\" redirect responses.\n\n2.0.6 (2023-10-02)\n==================\n\n* Added the ``Cookie`` header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via ``Retry.remove_headers_on_redirect``.\n\n2.0.5 (2023-09-20)\n==================\n\n- Allowed pyOpenSSL third-party module without any deprecation warning. (`#3126 <https://github.com/urllib3/urllib3/issues/3126>`__)\n- Fixed default ``blocksize`` of ``HTTPConnection`` classes to match high-level classes. Previously was 8KiB, now 16KiB. (`#3066 <https://github.com/urllib3/urllib3/issues/3066>`__)\n\n\n2.0.4 (2023-07-19)\n==================\n\n- Added support for union operators to ``HTTPHeaderDict`` (`#2254 <https://github.com/urllib3/urllib3/issues/2254>`__)\n- Added ``BaseHTTPResponse`` to ``urllib3.__all__`` (`#3078 <https://github.com/urllib3/urllib3/issues/3078>`__)\n- Fixed ``urllib3.connection.HTTPConnection`` to raise the ``http.client.connect`` audit event to have the same behavior as the standard library HTTP client (`#2757 <https://github.com/urllib3/urllib3/issues/2757>`__)\n- Relied on the standard library for checking hostnames in supported PyPy releases (`#3087 <https://github.com/urllib3/urllib3/issues/3087>`__)\n\n\n2.0.3 (2023-06-07)\n==================\n\n- Allowed alternative SSL libraries such as LibreSSL, while still issuing a warning as we cannot help users facing issues with implementations other than OpenSSL. (`#3020 <https://github.com/urllib3/urllib3/issues/3020>`__)\n- Deprecated URLs which don't have an explicit scheme (`#2950 <https://github.com/urllib3/urllib3/pull/2950>`_)\n- Fixed response decoding with Zstandard when compressed data is made of several frames. (`#3008 <https://github.com/urllib3/urllib3/issues/3008>`__)\n- Fixed ``assert_hostname=False`` to correctly skip hostname check. (`#3051 <https://github.com/urllib3/urllib3/issues/3051>`__)\n\n\n2.0.2 (2023-05-03)\n==================\n\n- Fixed ``HTTPResponse.stream()`` to continue yielding bytes if buffered decompressed data\n  was still available to be read even if the underlying socket is closed. This prevents\n  a compressed response from being truncated. (`#3009 <https://github.com/urllib3/urllib3/issues/3009>`__)\n\n\n2.0.1 (2023-04-30)\n==================\n\n- Fixed a socket leak when fingerprint or hostname verifications fail. (`#2991 <https://github.com/urllib3/urllib3/issues/2991>`__)\n- Fixed an error when ``HTTPResponse.read(0)`` was the first ``read`` call or when the internal response body buffer was otherwise empty. (`#2998 <https://github.com/urllib3/urllib3/issues/2998>`__)\n\n\n2.0.0 (2023-04-26)\n==================\n\nRead the `v2.0 migration guide <https://urllib3.readthedocs.io/en/latest/v2-migration-guide.html>`__ for help upgrading to the latest version of urllib3.\n\nRemoved\n-------\n\n* Removed support for Python 2.7, 3.5, and 3.6 (`#883 <https://github.com/urllib3/urllib3/issues/883>`__, `#2336 <https://github.com/urllib3/urllib3/issues/2336>`__).\n* Removed fallback on certificate ``commonName`` in ``match_hostname()`` function.\n  This behavior was deprecated in May 2000 in RFC 2818. Instead only ``subjectAltName``\n  is used to verify the hostname by default. To enable verifying the hostname against\n  ``commonName`` use ``SSLContext.hostname_checks_common_name = True`` (`#2113 <https://github.com/urllib3/urllib3/issues/2113>`__).\n* Removed support for Python with an ``ssl`` module compiled with LibreSSL, CiscoSSL,\n  wolfSSL, and all other OpenSSL alternatives. Python is moving to require OpenSSL with PEP 644 (`#2168 <https://github.com/urllib3/urllib3/issues/2168>`__).\n* Removed support for OpenSSL versions earlier than 1.1.1 or that don't have SNI support.\n  When an incompatible OpenSSL version is detected an ``ImportError`` is raised (`#2168 <https://github.com/urllib3/urllib3/issues/2168>`__).\n* Removed the list of default ciphers for OpenSSL 1.1.1+ and SecureTransport as their own defaults are already secure (`#2082 <https://github.com/urllib3/urllib3/issues/2082>`__).\n* Removed ``urllib3.contrib.appengine.AppEngineManager`` and support for Google App Engine Standard Environment (`#2044 <https://github.com/urllib3/urllib3/issues/2044>`__).\n* Removed deprecated ``Retry`` options ``method_whitelist``, ``DEFAULT_REDIRECT_HEADERS_BLACKLIST`` (`#2086 <https://github.com/urllib3/urllib3/issues/2086>`__).\n* Removed ``urllib3.HTTPResponse.from_httplib`` (`#2648 <https://github.com/urllib3/urllib3/issues/2648>`__).\n* Removed default value of ``None`` for the ``request_context`` parameter of ``urllib3.PoolManager.connection_from_pool_key``. This change should have no effect on users as the default value of ``None`` was an invalid option and was never used (`#1897 <https://github.com/urllib3/urllib3/issues/1897>`__).\n* Removed the ``urllib3.request`` module. ``urllib3.request.RequestMethods`` has been made a private API.\n  This change was made to ensure that ``from urllib3 import request`` imported the top-level ``request()``\n  function instead of the ``urllib3.request`` module (`#2269 <https://github.com/urllib3/urllib3/issues/2269>`__).\n* Removed support for SSLv3.0 from the ``urllib3.contrib.pyopenssl`` even when support is available from the compiled OpenSSL library (`#2233 <https://github.com/urllib3/urllib3/issues/2233>`__).\n* Removed the deprecated ``urllib3.contrib.ntlmpool`` module (`#2339 <https://github.com/urllib3/urllib3/issues/2339>`__).\n* Removed ``DEFAULT_CIPHERS``, ``HAS_SNI``, ``USE_DEFAULT_SSLCONTEXT_CIPHERS``, from the private module ``urllib3.util.ssl_`` (`#2168 <https://github.com/urllib3/urllib3/issues/2168>`__).\n* Removed ``urllib3.exceptions.SNIMissingWarning`` (`#2168 <https://github.com/urllib3/urllib3/issues/2168>`__).\n* Removed the ``_prepare_conn`` method from ``HTTPConnectionPool``. Previously this was only used to call ``HTTPSConnection.set_cert()`` by ``HTTPSConnectionPool`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Removed ``tls_in_tls_required`` property from ``HTTPSConnection``. This is now determined from the ``scheme`` parameter in ``HTTPConnection.set_tunnel()`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Removed the ``strict`` parameter/attribute from ``HTTPConnection``, ``HTTPSConnection``, ``HTTPConnectionPool``, ``HTTPSConnectionPool``, and ``HTTPResponse`` (`#2064 <https://github.com/urllib3/urllib3/issues/2064>`__).\n\nDeprecated\n----------\n\n* Deprecated ``HTTPResponse.getheaders()`` and ``HTTPResponse.getheader()`` which will be removed in urllib3 v2.1.0. Instead use ``HTTPResponse.headers`` and ``HTTPResponse.headers.get(name, default)``. (`#1543 <https://github.com/urllib3/urllib3/issues/1543>`__, `#2814 <https://github.com/urllib3/urllib3/issues/2814>`__).\n* Deprecated ``urllib3.contrib.pyopenssl`` module which will be removed in urllib3 v2.1.0 (`#2691 <https://github.com/urllib3/urllib3/issues/2691>`__).\n* Deprecated ``urllib3.contrib.securetransport`` module which will be removed in urllib3 v2.1.0 (`#2692 <https://github.com/urllib3/urllib3/issues/2692>`__).\n* Deprecated ``ssl_version`` option in favor of ``ssl_minimum_version``. ``ssl_version`` will be removed in urllib3 v2.1.0 (`#2110 <https://github.com/urllib3/urllib3/issues/2110>`__).\n* Deprecated the ``strict`` parameter of ``PoolManager.connection_from_context()`` as it's not longer needed in Python 3.x. It will be removed in urllib3 v2.1.0 (`#2267 <https://github.com/urllib3/urllib3/issues/2267>`__)\n* Deprecated the ``NewConnectionError.pool`` attribute which will be removed in urllib3 v2.1.0 (`#2271 <https://github.com/urllib3/urllib3/issues/2271>`__).\n* Deprecated ``format_header_param_html5`` and ``format_header_param`` in favor of ``format_multipart_header_param`` (`#2257 <https://github.com/urllib3/urllib3/issues/2257>`__).\n* Deprecated ``RequestField.header_formatter`` parameter which will be removed in urllib3 v2.1.0 (`#2257 <https://github.com/urllib3/urllib3/issues/2257>`__).\n* Deprecated ``HTTPSConnection.set_cert()`` method. Instead pass parameters to the ``HTTPSConnection`` constructor (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Deprecated ``HTTPConnection.request_chunked()`` method which will be removed in urllib3 v2.1.0. Instead pass ``chunked=True`` to ``HTTPConnection.request()`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n\nAdded\n-----\n\n* Added top-level ``urllib3.request`` function which uses a preconfigured module-global ``PoolManager`` instance (`#2150 <https://github.com/urllib3/urllib3/issues/2150>`__).\n* Added the ``json`` parameter to ``urllib3.request()``, ``PoolManager.request()``, and ``ConnectionPool.request()`` methods to send JSON bodies in requests. Using this parameter will set the header ``Content-Type: application/json`` if ``Content-Type`` isn't already defined.\n  Added support for parsing JSON response bodies with ``HTTPResponse.json()`` method (`#2243 <https://github.com/urllib3/urllib3/issues/2243>`__).\n* Added type hints to the ``urllib3`` module (`#1897 <https://github.com/urllib3/urllib3/issues/1897>`__).\n* Added ``ssl_minimum_version`` and ``ssl_maximum_version`` options which set\n  ``SSLContext.minimum_version`` and ``SSLContext.maximum_version`` (`#2110 <https://github.com/urllib3/urllib3/issues/2110>`__).\n* Added support for Zstandard (RFC 8878) when ``zstandard`` 1.18.0 or later is installed.\n  Added the ``zstd`` extra which installs the ``zstandard`` package (`#1992 <https://github.com/urllib3/urllib3/issues/1992>`__).\n* Added ``urllib3.response.BaseHTTPResponse`` class. All future response classes will be subclasses of ``BaseHTTPResponse`` (`#2083 <https://github.com/urllib3/urllib3/issues/2083>`__).\n* Added ``FullPoolError`` which is raised when ``PoolManager(block=True)`` and a connection is returned to a full pool (`#2197 <https://github.com/urllib3/urllib3/issues/2197>`__).\n* Added ``HTTPHeaderDict`` to the top-level ``urllib3`` namespace (`#2216 <https://github.com/urllib3/urllib3/issues/2216>`__).\n* Added support for configuring header merging behavior with HTTPHeaderDict\n  When using a ``HTTPHeaderDict`` to provide headers for a request, by default duplicate\n  header values will be repeated. But if ``combine=True`` is passed into a call to\n  ``HTTPHeaderDict.add``, then the added header value will be merged in with an existing\n  value into a comma-separated list (``X-My-Header: foo, bar``) (`#2242 <https://github.com/urllib3/urllib3/issues/2242>`__).\n* Added ``NameResolutionError`` exception when a DNS error occurs (`#2305 <https://github.com/urllib3/urllib3/issues/2305>`__).\n* Added ``proxy_assert_hostname`` and ``proxy_assert_fingerprint`` kwargs to ``ProxyManager`` (`#2409 <https://github.com/urllib3/urllib3/issues/2409>`__).\n* Added a configurable ``backoff_max`` parameter to the ``Retry`` class.\n  If a custom ``backoff_max`` is provided to the ``Retry`` class, it\n  will replace the ``Retry.DEFAULT_BACKOFF_MAX`` (`#2494 <https://github.com/urllib3/urllib3/issues/2494>`__).\n* Added the ``authority`` property to the Url class as per RFC 3986 3.2. This property should be used in place of ``netloc`` for users who want to include the userinfo (auth) component of the URI (`#2520 <https://github.com/urllib3/urllib3/issues/2520>`__).\n* Added the ``scheme`` parameter to ``HTTPConnection.set_tunnel`` to configure the scheme of the origin being tunnelled to (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Added the ``is_closed``, ``is_connected`` and ``has_connected_to_proxy`` properties to ``HTTPConnection`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Added optional ``backoff_jitter`` parameter to ``Retry``. (`#2952 <https://github.com/urllib3/urllib3/issues/2952>`__)\n\nChanged\n-------\n\n* Changed ``urllib3.response.HTTPResponse.read`` to respect the semantics of ``io.BufferedIOBase`` regardless of compression. Specifically, this method:\n\n  * Only returns an empty bytes object to indicate EOF (that is, the response has been fully consumed).\n  * Never returns more bytes than requested.\n  * Can issue any number of system calls: zero, one or multiple.\n\n  If you want each ``urllib3.response.HTTPResponse.read`` call to issue a single system call, you need to disable decompression by setting ``decode_content=False`` (`#2128 <https://github.com/urllib3/urllib3/issues/2128>`__).\n* Changed ``urllib3.HTTPConnection.getresponse`` to return an instance of ``urllib3.HTTPResponse`` instead of ``http.client.HTTPResponse`` (`#2648 <https://github.com/urllib3/urllib3/issues/2648>`__).\n* Changed ``ssl_version`` to instead set the corresponding ``SSLContext.minimum_version``\n  and ``SSLContext.maximum_version`` values.  Regardless of ``ssl_version`` passed\n  ``SSLContext`` objects are now constructed using ``ssl.PROTOCOL_TLS_CLIENT`` (`#2110 <https://github.com/urllib3/urllib3/issues/2110>`__).\n* Changed default ``SSLContext.minimum_version`` to be ``TLSVersion.TLSv1_2`` in line with Python 3.10 (`#2373 <https://github.com/urllib3/urllib3/issues/2373>`__).\n* Changed ``ProxyError`` to wrap any connection error (timeout, TLS, DNS) that occurs when connecting to the proxy (`#2482 <https://github.com/urllib3/urllib3/pull/2482>`__).\n* Changed ``urllib3.util.create_urllib3_context`` to not override the system cipher suites with\n  a default value. The new default will be cipher suites configured by the operating system (`#2168 <https://github.com/urllib3/urllib3/issues/2168>`__).\n* Changed ``multipart/form-data`` header parameter formatting matches the WHATWG HTML Standard as of 2021-06-10. Control characters in filenames are no longer percent encoded (`#2257 <https://github.com/urllib3/urllib3/issues/2257>`__).\n* Changed the error raised when connecting via HTTPS when the ``ssl`` module isn't available from ``SSLError`` to ``ImportError`` (`#2589 <https://github.com/urllib3/urllib3/issues/2589>`__).\n* Changed ``HTTPConnection.request()`` to always use lowercase chunk boundaries when sending requests with ``Transfer-Encoding: chunked`` (`#2515 <https://github.com/urllib3/urllib3/issues/2515>`__).\n* Changed ``enforce_content_length`` default to True, preventing silent data loss when reading streamed responses (`#2514 <https://github.com/urllib3/urllib3/issues/2514>`__).\n* Changed internal implementation of ``HTTPHeaderDict`` to use ``dict`` instead of ``collections.OrderedDict`` for better performance (`#2080 <https://github.com/urllib3/urllib3/issues/2080>`__).\n* Changed the ``urllib3.contrib.pyopenssl`` module to wrap ``OpenSSL.SSL.Error`` with ``ssl.SSLError`` in ``PyOpenSSLContext.load_cert_chain`` (`#2628 <https://github.com/urllib3/urllib3/issues/2628>`__).\n* Changed usage of the deprecated ``socket.error`` to ``OSError`` (`#2120 <https://github.com/urllib3/urllib3/issues/2120>`__).\n* Changed all parameters in the ``HTTPConnection`` and ``HTTPSConnection`` constructors to be keyword-only except ``host`` and ``port`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Changed ``HTTPConnection.getresponse()`` to set the socket timeout from ``HTTPConnection.timeout`` value before reading\n  data from the socket. This previously was done manually by the ``HTTPConnectionPool`` calling ``HTTPConnection.sock.settimeout(...)`` (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Changed the ``_proxy_host`` property to ``_tunnel_host`` in ``HTTPConnectionPool`` to more closely match how the property is used (value in ``HTTPConnection.set_tunnel()``) (`#1985 <https://github.com/urllib3/urllib3/issues/1985>`__).\n* Changed name of ``Retry.BACK0FF_MAX`` to be ``Retry.DEFAULT_BACKOFF_MAX``.\n* Changed TLS handshakes to use ``SSLContext.check_hostname`` when possible (`#2452 <https://github.com/urllib3/urllib3/pull/2452>`__).\n* Changed ``server_hostname`` to behave like other parameters only used by ``HTTPSConnectionPool`` (`#2537 <https://github.com/urllib3/urllib3/pull/2537>`__).\n* Changed the default ``blocksize`` to 16KB to match OpenSSL's default read amounts (`#2348 <https://github.com/urllib3/urllib3/pull/2348>`__).\n* Changed ``HTTPResponse.read()`` to raise an error when calling with ``decode_content=False`` after using ``decode_content=True`` to prevent data loss (`#2800 <https://github.com/urllib3/urllib3/issues/2800>`__).\n\nFixed\n-----\n\n* Fixed thread-safety issue where accessing a ``PoolManager`` with many distinct origins would cause connection pools to be closed while requests are in progress (`#1252 <https://github.com/urllib3/urllib3/issues/1252>`__).\n* Fixed an issue where an ``HTTPConnection`` instance would erroneously reuse the socket read timeout value from reading the previous response instead of a newly configured connect timeout.\n  Instead now if ``HTTPConnection.timeout`` is updated before sending the next request the new timeout value will be used (`#2645 <https://github.com/urllib3/urllib3/issues/2645>`__).\n* Fixed ``socket.error.errno`` when raised from pyOpenSSL's ``OpenSSL.SSL.SysCallError`` (`#2118 <https://github.com/urllib3/urllib3/issues/2118>`__).\n* Fixed the default value of ``HTTPSConnection.socket_options`` to match ``HTTPConnection`` (`#2213 <https://github.com/urllib3/urllib3/issues/2213>`__).\n* Fixed a bug where ``headers`` would be modified by the ``remove_headers_on_redirect`` feature (`#2272 <https://github.com/urllib3/urllib3/issues/2272>`__).\n* Fixed a reference cycle bug in ``urllib3.util.connection.create_connection()`` (`#2277 <https://github.com/urllib3/urllib3/issues/2277>`__).\n* Fixed a socket leak if ``HTTPConnection.connect()`` fails (`#2571 <https://github.com/urllib3/urllib3/pull/2571>`__).\n* Fixed ``urllib3.contrib.pyopenssl.WrappedSocket`` and ``urllib3.contrib.securetransport.WrappedSocket`` close methods (`#2970 <https://github.com/urllib3/urllib3/issues/2970>`__)\n\n1.26.18 (2023-10-17)\n====================\n\n* Made body stripped from HTTP requests changing the request method to GET after HTTP 303 \"See Other\" redirect responses.\n\n1.26.17 (2023-10-02)\n====================\n\n* Added the ``Cookie`` header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via ``Retry.remove_headers_on_redirect``. (`#3139 <https://github.com/urllib3/urllib3/pull/3139>`_)\n\n1.26.16 (2023-05-23)\n====================\n\n* Fixed thread-safety issue where accessing a ``PoolManager`` with many distinct origins\n  would cause connection pools to be closed while requests are in progress (`#2954 <https://github.com/urllib3/urllib3/pull/2954>`_)\n\n1.26.15 (2023-03-10)\n====================\n\n* Fix socket timeout value when ``HTTPConnection`` is reused (`#2645 <https://github.com/urllib3/urllib3/issues/2645>`__)\n* Remove \"!\" character from the unreserved characters in IPv6 Zone ID parsing\n  (`#2899 <https://github.com/urllib3/urllib3/issues/2899>`__)\n* Fix IDNA handling of '\\x80' byte (`#2901 <https://github.com/urllib3/urllib3/issues/2901>`__)\n\n1.26.14 (2023-01-11)\n====================\n\n* Fixed parsing of port 0 (zero) returning None, instead of 0. (`#2850 <https://github.com/urllib3/urllib3/issues/2850>`__)\n* Removed deprecated getheaders() calls in contrib module. Fixed the type hint of ``PoolKey.key_retries`` by adding ``bool`` to the union. (`#2865 <https://github.com/urllib3/urllib3/issues/2865>`__)\n\n1.26.13 (2022-11-23)\n====================\n\n* Deprecated the ``HTTPResponse.getheaders()`` and ``HTTPResponse.getheader()`` methods.\n* Fixed an issue where parsing a URL with leading zeroes in the port would be rejected\n  even when the port number after removing the zeroes was valid.\n* Fixed a deprecation warning when using cryptography v39.0.0.\n* Removed the ``<4`` in the ``Requires-Python`` packaging metadata field.\n\n1.26.12 (2022-08-22)\n====================\n\n* Deprecated the `urllib3[secure]` extra and the `urllib3.contrib.pyopenssl` module.\n  Both will be removed in v2.x. See this `GitHub issue <https://github.com/urllib3/urllib3/issues/2680>`_\n  for justification and info on how to migrate.\n\n1.26.11 (2022-07-25)\n====================\n\n* Fixed an issue where reading more than 2 GiB in a call to ``HTTPResponse.read`` would\n  raise an ``OverflowError`` on Python 3.9 and earlier.\n\n1.26.10 (2022-07-07)\n====================\n\n* Removed support for Python 3.5\n* Fixed an issue where a ``ProxyError`` recommending configuring the proxy as HTTP\n  instead of HTTPS could appear even when an HTTPS proxy wasn't configured.\n\n1.26.9 (2022-03-16)\n===================\n\n* Changed ``urllib3[brotli]`` extra to favor installing Brotli libraries that are still\n  receiving updates like ``brotli`` and ``brotlicffi`` instead of ``brotlipy``.\n  This change does not impact behavior of urllib3, only which dependencies are installed.\n* Fixed a socket leaking when ``HTTPSConnection.connect()`` raises an exception.\n* Fixed ``server_hostname`` being forwarded from ``PoolManager`` to ``HTTPConnectionPool``\n  when requesting an HTTP URL. Should only be forwarded when requesting an HTTPS URL.\n\n1.26.8 (2022-01-07)\n===================\n\n* Added extra message to ``urllib3.exceptions.ProxyError`` when urllib3 detects that\n  a proxy is configured to use HTTPS but the proxy itself appears to only use HTTP.\n* Added a mention of the size of the connection pool when discarding a connection due to the pool being full.\n* Added explicit support for Python 3.11.\n* Deprecated the ``Retry.MAX_BACKOFF`` class property in favor of ``Retry.DEFAULT_MAX_BACKOFF``\n  to better match the rest of the default parameter names. ``Retry.MAX_BACKOFF`` is removed in v2.0.\n* Changed location of the vendored ``ssl.match_hostname`` function from ``urllib3.packages.ssl_match_hostname``\n  to ``urllib3.util.ssl_match_hostname`` to ensure Python 3.10+ compatibility after being repackaged\n  by downstream distributors.\n* Fixed absolute imports, all imports are now relative.\n\n\n1.26.7 (2021-09-22)\n===================\n\n* Fixed a bug with HTTPS hostname verification involving IP addresses and lack\n  of SNI. (Issue #2400)\n* Fixed a bug where IPv6 braces weren't stripped during certificate hostname\n  matching. (Issue #2240)\n\n\n1.26.6 (2021-06-25)\n===================\n\n* Deprecated the ``urllib3.contrib.ntlmpool`` module. urllib3 is not able to support\n  it properly due to `reasons listed in this issue <https://github.com/urllib3/urllib3/issues/2282>`_.\n  If you are a user of this module please leave a comment.\n* Changed ``HTTPConnection.request_chunked()`` to not erroneously emit multiple\n  ``Transfer-Encoding`` headers in the case that one is already specified.\n* Fixed typo in deprecation message to recommend ``Retry.DEFAULT_ALLOWED_METHODS``.\n\n\n1.26.5 (2021-05-26)\n===================\n\n* Fixed deprecation warnings emitted in Python 3.10.\n* Updated vendored ``six`` library to 1.16.0.\n* Improved performance of URL parser when splitting\n  the authority component.\n\n\n1.26.4 (2021-03-15)\n===================\n\n* Changed behavior of the default ``SSLContext`` when connecting to HTTPS proxy\n  during HTTPS requests. The default ``SSLContext`` now sets ``check_hostname=True``.\n\n\n1.26.3 (2021-01-26)\n===================\n\n* Fixed bytes and string comparison issue with headers (Pull #2141)\n\n* Changed ``ProxySchemeUnknown`` error message to be\n  more actionable if the user supplies a proxy URL without\n  a scheme. (Pull #2107)\n\n\n1.26.2 (2020-11-12)\n===================\n\n* Fixed an issue where ``wrap_socket`` and ``CERT_REQUIRED`` wouldn't\n  be imported properly on Python 2.7.8 and earlier (Pull #2052)\n\n\n1.26.1 (2020-11-11)\n===================\n\n* Fixed an issue where two ``User-Agent`` headers would be sent if a\n  ``User-Agent`` header key is passed as ``bytes`` (Pull #2047)\n\n\n1.26.0 (2020-11-10)\n===================\n\n* **NOTE: urllib3 v2.0 will drop support for Python 2**.\n  `Read more in the v2.0 Roadmap <https://urllib3.readthedocs.io/en/latest/v2-roadmap.html>`_.\n\n* Added support for HTTPS proxies contacting HTTPS servers (Pull #1923, Pull #1806)\n\n* Deprecated negotiating TLSv1 and TLSv1.1 by default. Users that\n  still wish to use TLS earlier than 1.2 without a deprecation warning\n  should opt-in explicitly by setting ``ssl_version=ssl.PROTOCOL_TLSv1_1`` (Pull #2002)\n  **Starting in urllib3 v2.0: Connections that receive a ``DeprecationWarning`` will fail**\n\n* Deprecated ``Retry`` options ``Retry.DEFAULT_METHOD_WHITELIST``, ``Retry.DEFAULT_REDIRECT_HEADERS_BLACKLIST``\n  and ``Retry(method_whitelist=...)`` in favor of ``Retry.DEFAULT_ALLOWED_METHODS``,\n  ``Retry.DEFAULT_REMOVE_HEADERS_ON_REDIRECT``, and ``Retry(allowed_methods=...)``\n  (Pull #2000) **Starting in urllib3 v2.0: Deprecated options will be removed**\n\n* Added default ``User-Agent`` header to every request (Pull #1750)\n\n* Added ``urllib3.util.SKIP_HEADER`` for skipping ``User-Agent``, ``Accept-Encoding``,\n  and ``Host`` headers from being automatically emitted with requests (Pull #2018)\n\n* Collapse ``transfer-encoding: chunked`` request data and framing into\n  the same ``socket.send()`` call (Pull #1906)\n\n* Send ``http/1.1`` ALPN identifier with every TLS handshake by default (Pull #1894)\n\n* Properly terminate SecureTransport connections when CA verification fails (Pull #1977)\n\n* Don't emit an ``SNIMissingWarning`` when passing ``server_hostname=None``\n  to SecureTransport (Pull #1903)\n\n* Disabled requesting TLSv1.2 session tickets as they weren't being used by urllib3 (Pull #1970)\n\n* Suppress ``BrokenPipeError`` when writing request body after the server\n  has closed the socket (Pull #1524)\n\n* Wrap ``ssl.SSLError`` that can be raised from reading a socket (e.g. \"bad MAC\")\n  into an ``urllib3.exceptions.SSLError`` (Pull #1939)\n\n\n1.25.11 (2020-10-19)\n====================\n\n* Fix retry backoff time parsed from ``Retry-After`` header when given\n  in the HTTP date format. The HTTP date was parsed as the local timezone\n  rather than accounting for the timezone in the HTTP date (typically\n  UTC) (Pull #1932, Pull #1935, Pull #1938, Pull #1949)\n\n* Fix issue where an error would be raised when the ``SSLKEYLOGFILE``\n  environment variable was set to the empty string. Now ``SSLContext.keylog_file``\n  is not set in this situation (Pull #2016)\n\n\n1.25.10 (2020-07-22)\n====================\n\n* Added support for ``SSLKEYLOGFILE`` environment variable for\n  logging TLS session keys with use with programs like\n  Wireshark for decrypting captured web traffic (Pull #1867)\n\n* Fixed loading of SecureTransport libraries on macOS Big Sur\n  due to the new dynamic linker cache (Pull #1905)\n\n* Collapse chunked request bodies data and framing into one\n  call to ``send()`` to reduce the number of TCP packets by 2-4x (Pull #1906)\n\n* Don't insert ``None`` into ``ConnectionPool`` if the pool\n  was empty when requesting a connection (Pull #1866)\n\n* Avoid ``hasattr`` call in ``BrotliDecoder.decompress()`` (Pull #1858)\n\n\n1.25.9 (2020-04-16)\n===================\n\n* Added ``InvalidProxyConfigurationWarning`` which is raised when\n  erroneously specifying an HTTPS proxy URL. urllib3 doesn't currently\n  support connecting to HTTPS proxies but will soon be able to\n  and we would like users to migrate properly without much breakage.\n\n  See `this GitHub issue <https://github.com/urllib3/urllib3/issues/1850>`_\n  for more information on how to fix your proxy config. (Pull #1851)\n\n* Drain connection after ``PoolManager`` redirect (Pull #1817)\n\n* Ensure ``load_verify_locations`` raises ``SSLError`` for all backends (Pull #1812)\n\n* Rename ``VerifiedHTTPSConnection`` to ``HTTPSConnection`` (Pull #1805)\n\n* Allow the CA certificate data to be passed as a string (Pull #1804)\n\n* Raise ``ValueError`` if method contains control characters (Pull #1800)\n\n* Add ``__repr__`` to ``Timeout`` (Pull #1795)\n\n\n1.25.8 (2020-01-20)\n===================\n\n* Drop support for EOL Python 3.4 (Pull #1774)\n\n* Optimize _encode_invalid_chars (Pull #1787)\n\n\n1.25.7 (2019-11-11)\n===================\n\n* Preserve ``chunked`` parameter on retries (Pull #1715, Pull #1734)\n\n* Allow unset ``SERVER_SOFTWARE`` in App Engine (Pull #1704, Issue #1470)\n\n* Fix issue where URL fragment was sent within the request target. (Pull #1732)\n\n* Fix issue where an empty query section in a URL would fail to parse. (Pull #1732)\n\n* Remove TLS 1.3 support in SecureTransport due to Apple removing support (Pull #1703)\n\n\n1.25.6 (2019-09-24)\n===================\n\n* Fix issue where tilde (``~``) characters were incorrectly\n  percent-encoded in the path. (Pull #1692)\n\n\n1.25.5 (2019-09-19)\n===================\n\n* Add mitigation for BPO-37428 affecting Python <3.7.4 and OpenSSL 1.1.1+ which\n  caused certificate verification to be enabled when using ``cert_reqs=CERT_NONE``.\n  (Issue #1682)\n\n\n1.25.4 (2019-09-19)\n===================\n\n* Propagate Retry-After header settings to subsequent retries. (Pull #1607)\n\n* Fix edge case where Retry-After header was still respected even when\n  explicitly opted out of. (Pull #1607)\n\n* Remove dependency on ``rfc3986`` for URL parsing.\n\n* Fix issue where URLs containing invalid characters within ``Url.auth`` would\n  raise an exception instead of percent-encoding those characters.\n\n* Add support for ``HTTPResponse.auto_close = False`` which makes HTTP responses\n  work well with BufferedReaders and other ``io`` module features. (Pull #1652)\n\n* Percent-encode invalid characters in URL for ``HTTPConnectionPool.request()`` (Pull #1673)\n\n\n1.25.3 (2019-05-23)\n===================\n\n* Change ``HTTPSConnection`` to load system CA certificates\n  when ``ca_certs``, ``ca_cert_dir``, and ``ssl_context`` are\n  unspecified. (Pull #1608, Issue #1603)\n\n* Upgrade bundled rfc3986 to v1.3.2. (Pull #1609, Issue #1605)\n\n\n1.25.2 (2019-04-28)\n===================\n\n* Change ``is_ipaddress`` to not detect IPvFuture addresses. (Pull #1583)\n\n* Change ``parse_url`` to percent-encode invalid characters within the\n  path, query, and target components. (Pull #1586)\n\n\n1.25.1 (2019-04-24)\n===================\n\n* Add support for Google's ``Brotli`` package. (Pull #1572, Pull #1579)\n\n* Upgrade bundled rfc3986 to v1.3.1 (Pull #1578)\n\n\n1.25 (2019-04-22)\n=================\n\n* Require and validate certificates by default when using HTTPS (Pull #1507)\n\n* Upgraded ``urllib3.utils.parse_url()`` to be RFC 3986 compliant. (Pull #1487)\n\n* Added support for ``key_password`` for ``HTTPSConnectionPool`` to use\n  encrypted ``key_file`` without creating your own ``SSLContext`` object. (Pull #1489)\n\n* Add TLSv1.3 support to CPython, pyOpenSSL, and SecureTransport ``SSLContext``\n  implementations. (Pull #1496)\n\n* Switched the default multipart header encoder from RFC 2231 to HTML 5 working draft. (Issue #303, Pull #1492)\n\n* Fixed issue where OpenSSL would block if an encrypted client private key was\n  given and no password was given. Instead an ``SSLError`` is raised. (Pull #1489)\n\n* Added support for Brotli content encoding. It is enabled automatically if\n  ``brotlipy`` package is installed which can be requested with\n  ``urllib3[brotli]`` extra. (Pull #1532)\n\n* Drop ciphers using DSS key exchange from default TLS cipher suites.\n  Improve default ciphers when using SecureTransport. (Pull #1496)\n\n* Implemented a more efficient ``HTTPResponse.__iter__()`` method. (Issue #1483)\n\n1.24.3 (2019-05-01)\n===================\n\n* Apply fix for CVE-2019-9740. (Pull #1591)\n\n1.24.2 (2019-04-17)\n===================\n\n* Don't load system certificates by default when any other ``ca_certs``, ``ca_certs_dir`` or\n  ``ssl_context`` parameters are specified.\n\n* Remove Authorization header regardless of case when redirecting to cross-site. (Issue #1510)\n\n* Add support for IPv6 addresses in subjectAltName section of certificates. (Issue #1269)\n\n\n1.24.1 (2018-11-02)\n===================\n\n* Remove quadratic behavior within ``GzipDecoder.decompress()`` (Issue #1467)\n\n* Restored functionality of ``ciphers`` parameter for ``create_urllib3_context()``. (Issue #1462)\n\n\n1.24 (2018-10-16)\n=================\n\n* Allow key_server_hostname to be specified when initializing a PoolManager to allow custom SNI to be overridden. (Pull #1449)\n\n* Test against Python 3.7 on AppVeyor. (Pull #1453)\n\n* Early-out ipv6 checks when running on App Engine. (Pull #1450)\n\n* Change ambiguous description of backoff_factor (Pull #1436)\n\n* Add ability to handle multiple Content-Encodings (Issue #1441 and Pull #1442)\n\n* Skip DNS names that can't be idna-decoded when using pyOpenSSL (Issue #1405).\n\n* Add a server_hostname parameter to HTTPSConnection which allows for\n  overriding the SNI hostname sent in the handshake. (Pull #1397)\n\n* Drop support for EOL Python 2.6 (Pull #1429 and Pull #1430)\n\n* Fixed bug where responses with header Content-Type: message/* erroneously\n  raised HeaderParsingError, resulting in a warning being logged. (Pull #1439)\n\n* Move urllib3 to src/urllib3 (Pull #1409)\n\n\n1.23 (2018-06-04)\n=================\n\n* Allow providing a list of headers to strip from requests when redirecting\n  to a different host. Defaults to the ``Authorization`` header. Different\n  headers can be set via ``Retry.remove_headers_on_redirect``. (Issue #1316)\n\n* Fix ``util.selectors._fileobj_to_fd`` to accept ``long`` (Issue #1247).\n\n* Dropped Python 3.3 support. (Pull #1242)\n\n* Put the connection back in the pool when calling stream() or read_chunked() on\n  a chunked HEAD response. (Issue #1234)\n\n* Fixed pyOpenSSL-specific ssl client authentication issue when clients\n  attempted to auth via certificate + chain (Issue #1060)\n\n* Add the port to the connectionpool connect print (Pull #1251)\n\n* Don't use the ``uuid`` module to create multipart data boundaries. (Pull #1380)\n\n* ``read_chunked()`` on a closed response returns no chunks. (Issue #1088)\n\n* Add Python 2.6 support to ``contrib.securetransport`` (Pull #1359)\n\n* Added support for auth info in url for SOCKS proxy (Pull #1363)\n\n\n1.22 (2017-07-20)\n=================\n\n* Fixed missing brackets in ``HTTP CONNECT`` when connecting to IPv6 address via\n  IPv6 proxy. (Issue #1222)\n\n* Made the connection pool retry on ``SSLError``.  The original ``SSLError``\n  is available on ``MaxRetryError.reason``. (Issue #1112)\n\n* Drain and release connection before recursing on retry/redirect.  Fixes\n  deadlocks with a blocking connectionpool. (Issue #1167)\n\n* Fixed compatibility for cookiejar. (Issue #1229)\n\n* pyopenssl: Use vendored version of ``six``. (Issue #1231)\n\n\n1.21.1 (2017-05-02)\n===================\n\n* Fixed SecureTransport issue that would cause long delays in response body\n  delivery. (Pull #1154)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``socket_options`` flag to the ``PoolManager``.  (Issue #1165)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``assert_hostname`` or ``assert_fingerprint`` flag to the ``PoolManager``.\n  (Pull #1157)\n\n\n1.21 (2017-04-25)\n=================\n\n* Improved performance of certain selector system calls on Python 3.5 and\n  later. (Pull #1095)\n\n* Resolved issue where the PyOpenSSL backend would not wrap SysCallError\n  exceptions appropriately when sending data. (Pull #1125)\n\n* Selectors now detects a monkey-patched select module after import for modules\n  that patch the select module like eventlet, greenlet. (Pull #1128)\n\n* Reduced memory consumption when streaming zlib-compressed responses\n  (as opposed to raw deflate streams). (Pull #1129)\n\n* Connection pools now use the entire request context when constructing the\n  pool key. (Pull #1016)\n\n* ``PoolManager.connection_from_*`` methods now accept a new keyword argument,\n  ``pool_kwargs``, which are merged with the existing ``connection_pool_kw``.\n  (Pull #1016)\n\n* Add retry counter for ``status_forcelist``. (Issue #1147)\n\n* Added ``contrib`` module for using SecureTransport on macOS:\n  ``urllib3.contrib.securetransport``.  (Pull #1122)\n\n* urllib3 now only normalizes the case of ``http://`` and ``https://`` schemes:\n  for schemes it does not recognise, it assumes they are case-sensitive and\n  leaves them unchanged.\n  (Issue #1080)\n\n\n1.20 (2017-01-19)\n=================\n\n* Added support for waiting for I/O using selectors other than select,\n  improving urllib3's behaviour with large numbers of concurrent connections.\n  (Pull #1001)\n\n* Updated the date for the system clock check. (Issue #1005)\n\n* ConnectionPools now correctly consider hostnames to be case-insensitive.\n  (Issue #1032)\n\n* Outdated versions of PyOpenSSL now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Pull #1063)\n\n* Outdated versions of cryptography now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Issue #1044)\n\n* Automatically attempt to rewind a file-like body object when a request is\n  retried or redirected. (Pull #1039)\n\n* Fix some bugs that occur when modules incautiously patch the queue module.\n  (Pull #1061)\n\n* Prevent retries from occurring on read timeouts for which the request method\n  was not in the method whitelist. (Issue #1059)\n\n* Changed the PyOpenSSL contrib module to lazily load idna to avoid\n  unnecessarily bloating the memory of programs that don't need it. (Pull\n  #1076)\n\n* Add support for IPv6 literals with zone identifiers. (Pull #1013)\n\n* Added support for socks5h:// and socks4a:// schemes when working with SOCKS\n  proxies, and controlled remote DNS appropriately. (Issue #1035)\n\n\n1.19.1 (2016-11-16)\n===================\n\n* Fixed AppEngine import that didn't function on Python 3.5. (Pull #1025)\n\n\n1.19 (2016-11-03)\n=================\n\n* urllib3 now respects Retry-After headers on 413, 429, and 503 responses when\n  using the default retry logic. (Pull #955)\n\n* Remove markers from setup.py to assist ancient setuptools versions. (Issue\n  #986)\n\n* Disallow superscripts and other integerish things in URL ports. (Issue #989)\n\n* Allow urllib3's HTTPResponse.stream() method to continue to work with\n  non-httplib underlying FPs. (Pull #990)\n\n* Empty filenames in multipart headers are now emitted as such, rather than\n  being suppressed. (Issue #1015)\n\n* Prefer user-supplied Host headers on chunked uploads. (Issue #1009)\n\n\n1.18.1 (2016-10-27)\n===================\n\n* CVE-2016-9015. Users who are using urllib3 version 1.17 or 1.18 along with\n  PyOpenSSL injection and OpenSSL 1.1.0 *must* upgrade to this version. This\n  release fixes a vulnerability whereby urllib3 in the above configuration\n  would silently fail to validate TLS certificates due to erroneously setting\n  invalid flags in OpenSSL's ``SSL_CTX_set_verify`` function. These erroneous\n  flags do not cause a problem in OpenSSL versions before 1.1.0, which\n  interprets the presence of any flag as requesting certificate validation.\n\n  There is no PR for this patch, as it was prepared for simultaneous disclosure\n  and release. The master branch received the same fix in Pull #1010.\n\n\n1.18 (2016-09-26)\n=================\n\n* Fixed incorrect message for IncompleteRead exception. (Pull #973)\n\n* Accept ``iPAddress`` subject alternative name fields in TLS certificates.\n  (Issue #258)\n\n* Fixed consistency of ``HTTPResponse.closed`` between Python 2 and 3.\n  (Issue #977)\n\n* Fixed handling of wildcard certificates when using PyOpenSSL. (Issue #979)\n\n\n1.17 (2016-09-06)\n=================\n\n* Accept ``SSLContext`` objects for use in SSL/TLS negotiation. (Issue #835)\n\n* ConnectionPool debug log now includes scheme, host, and port. (Issue #897)\n\n* Substantially refactored documentation. (Issue #887)\n\n* Used URLFetch default timeout on AppEngine, rather than hardcoding our own.\n  (Issue #858)\n\n* Normalize the scheme and host in the URL parser (Issue #833)\n\n* ``HTTPResponse`` contains the last ``Retry`` object, which now also\n  contains retries history. (Issue #848)\n\n* Timeout can no longer be set as boolean, and must be greater than zero.\n  (Pull #924)\n\n* Removed pyasn1 and ndg-httpsclient from dependencies used for PyOpenSSL. We\n  now use cryptography and idna, both of which are already dependencies of\n  PyOpenSSL. (Pull #930)\n\n* Fixed infinite loop in ``stream`` when amt=None. (Issue #928)\n\n* Try to use the operating system's certificates when we are using an\n  ``SSLContext``. (Pull #941)\n\n* Updated cipher suite list to allow ChaCha20+Poly1305. AES-GCM is preferred to\n  ChaCha20, but ChaCha20 is then preferred to everything else. (Pull #947)\n\n* Updated cipher suite list to remove 3DES-based cipher suites. (Pull #958)\n\n* Removed the cipher suite fallback to allow HIGH ciphers. (Pull #958)\n\n* Implemented ``length_remaining`` to determine remaining content\n  to be read. (Pull #949)\n\n* Implemented ``enforce_content_length`` to enable exceptions when\n  incomplete data chunks are received. (Pull #949)\n\n* Dropped connection start, dropped connection reset, redirect, forced retry,\n  and new HTTPS connection log levels to DEBUG, from INFO. (Pull #967)\n\n\n1.16 (2016-06-11)\n=================\n\n* Disable IPv6 DNS when IPv6 connections are not possible. (Issue #840)\n\n* Provide ``key_fn_by_scheme`` pool keying mechanism that can be\n  overridden. (Issue #830)\n\n* Normalize scheme and host to lowercase for pool keys, and include\n  ``source_address``. (Issue #830)\n\n* Cleaner exception chain in Python 3 for ``_make_request``.\n  (Issue #861)\n\n* Fixed installing ``urllib3[socks]`` extra. (Issue #864)\n\n* Fixed signature of ``ConnectionPool.close`` so it can actually safely be\n  called by subclasses. (Issue #873)\n\n* Retain ``release_conn`` state across retries. (Issues #651, #866)\n\n* Add customizable ``HTTPConnectionPool.ResponseCls``, which defaults to\n  ``HTTPResponse`` but can be replaced with a subclass. (Issue #879)\n\n\n1.15.1 (2016-04-11)\n===================\n\n* Fix packaging to include backports module. (Issue #841)\n\n\n1.15 (2016-04-06)\n=================\n\n* Added Retry(raise_on_status=False). (Issue #720)\n\n* Always use setuptools, no more distutils fallback. (Issue #785)\n\n* Dropped support for Python 3.2. (Issue #786)\n\n* Chunked transfer encoding when requesting with ``chunked=True``.\n  (Issue #790)\n\n* Fixed regression with IPv6 port parsing. (Issue #801)\n\n* Append SNIMissingWarning messages to allow users to specify it in\n  the PYTHONWARNINGS environment variable. (Issue #816)\n\n* Handle unicode headers in Py2. (Issue #818)\n\n* Log certificate when there is a hostname mismatch. (Issue #820)\n\n* Preserve order of request/response headers. (Issue #821)\n\n\n1.14 (2015-12-29)\n=================\n\n* contrib: SOCKS proxy support! (Issue #762)\n\n* Fixed AppEngine handling of transfer-encoding header and bug\n  in Timeout defaults checking. (Issue #763)\n\n\n1.13.1 (2015-12-18)\n===================\n\n* Fixed regression in IPv6 + SSL for match_hostname. (Issue #761)\n\n\n1.13 (2015-12-14)\n=================\n\n* Fixed ``pip install urllib3[secure]`` on modern pip. (Issue #706)\n\n* pyopenssl: Fixed SSL3_WRITE_PENDING error. (Issue #717)\n\n* pyopenssl: Support for TLSv1.1 and TLSv1.2. (Issue #696)\n\n* Close connections more defensively on exception. (Issue #734)\n\n* Adjusted ``read_chunked`` to handle gzipped, chunk-encoded bodies without\n  repeatedly flushing the decoder, to function better on Jython. (Issue #743)\n\n* Accept ``ca_cert_dir`` for SSL-related PoolManager configuration. (Issue #758)\n\n\n1.12 (2015-09-03)\n=================\n\n* Rely on ``six`` for importing ``httplib`` to work around\n  conflicts with other Python 3 shims. (Issue #688)\n\n* Add support for directories of certificate authorities, as supported by\n  OpenSSL. (Issue #701)\n\n* New exception: ``NewConnectionError``, raised when we fail to establish\n  a new connection, usually ``ECONNREFUSED`` socket error.\n\n\n1.11 (2015-07-21)\n=================\n\n* When ``ca_certs`` is given, ``cert_reqs`` defaults to\n  ``'CERT_REQUIRED'``. (Issue #650)\n\n* ``pip install urllib3[secure]`` will install Certifi and\n  PyOpenSSL as dependencies. (Issue #678)\n\n* Made ``HTTPHeaderDict`` usable as a ``headers`` input value\n  (Issues #632, #679)\n\n* Added `urllib3.contrib.appengine <https://urllib3.readthedocs.io/en/latest/contrib.html#google-app-engine>`_\n  which has an ``AppEngineManager`` for using ``URLFetch`` in a\n  Google AppEngine environment. (Issue #664)\n\n* Dev: Added test suite for AppEngine. (Issue #631)\n\n* Fix performance regression when using PyOpenSSL. (Issue #626)\n\n* Passing incorrect scheme (e.g. ``foo://``) will raise\n  ``ValueError`` instead of ``AssertionError`` (backwards\n  compatible for now, but please migrate). (Issue #640)\n\n* Fix pools not getting replenished when an error occurs during a\n  request using ``release_conn=False``. (Issue #644)\n\n* Fix pool-default headers not applying for url-encoded requests\n  like GET. (Issue #657)\n\n* log.warning in Python 3 when headers are skipped due to parsing\n  errors. (Issue #642)\n\n* Close and discard connections if an error occurs during read.\n  (Issue #660)\n\n* Fix host parsing for IPv6 proxies. (Issue #668)\n\n* Separate warning type SubjectAltNameWarning, now issued once\n  per host. (Issue #671)\n\n* Fix ``httplib.IncompleteRead`` not getting converted to\n  ``ProtocolError`` when using ``HTTPResponse.stream()``\n  (Issue #674)\n\n1.10.4 (2015-05-03)\n===================\n\n* Migrate tests to Tornado 4. (Issue #594)\n\n* Append default warning configuration rather than overwrite.\n  (Issue #603)\n\n* Fix streaming decoding regression. (Issue #595)\n\n* Fix chunked requests losing state across keep-alive connections.\n  (Issue #599)\n\n* Fix hanging when chunked HEAD response has no body. (Issue #605)\n\n\n1.10.3 (2015-04-21)\n===================\n\n* Emit ``InsecurePlatformWarning`` when SSLContext object is missing.\n  (Issue #558)\n\n* Fix regression of duplicate header keys being discarded.\n  (Issue #563)\n\n* ``Response.stream()`` returns a generator for chunked responses.\n  (Issue #560)\n\n* Set upper-bound timeout when waiting for a socket in PyOpenSSL.\n  (Issue #585)\n\n* Work on platforms without `ssl` module for plain HTTP requests.\n  (Issue #587)\n\n* Stop relying on the stdlib's default cipher list. (Issue #588)\n\n\n1.10.2 (2015-02-25)\n===================\n\n* Fix file descriptor leakage on retries. (Issue #548)\n\n* Removed RC4 from default cipher list. (Issue #551)\n\n* Header performance improvements. (Issue #544)\n\n* Fix PoolManager not obeying redirect retry settings. (Issue #553)\n\n\n1.10.1 (2015-02-10)\n===================\n\n* Pools can be used as context managers. (Issue #545)\n\n* Don't re-use connections which experienced an SSLError. (Issue #529)\n\n* Don't fail when gzip decoding an empty stream. (Issue #535)\n\n* Add sha256 support for fingerprint verification. (Issue #540)\n\n* Fixed handling of header values containing commas. (Issue #533)\n\n\n1.10 (2014-12-14)\n=================\n\n* Disabled SSLv3. (Issue #473)\n\n* Add ``Url.url`` property to return the composed url string. (Issue #394)\n\n* Fixed PyOpenSSL + gevent ``WantWriteError``. (Issue #412)\n\n* ``MaxRetryError.reason`` will always be an exception, not string.\n  (Issue #481)\n\n* Fixed SSL-related timeouts not being detected as timeouts. (Issue #492)\n\n* Py3: Use ``ssl.create_default_context()`` when available. (Issue #473)\n\n* Emit ``InsecureRequestWarning`` for *every* insecure HTTPS request.\n  (Issue #496)\n\n* Emit ``SecurityWarning`` when certificate has no ``subjectAltName``.\n  (Issue #499)\n\n* Close and discard sockets which experienced SSL-related errors.\n  (Issue #501)\n\n* Handle ``body`` param in ``.request(...)``. (Issue #513)\n\n* Respect timeout with HTTPS proxy. (Issue #505)\n\n* PyOpenSSL: Handle ZeroReturnError exception. (Issue #520)\n\n\n1.9.1 (2014-09-13)\n==================\n\n* Apply socket arguments before binding. (Issue #427)\n\n* More careful checks if fp-like object is closed. (Issue #435)\n\n* Fixed packaging issues of some development-related files not\n  getting included. (Issue #440)\n\n* Allow performing *only* fingerprint verification. (Issue #444)\n\n* Emit ``SecurityWarning`` if system clock is waaay off. (Issue #445)\n\n* Fixed PyOpenSSL compatibility with PyPy. (Issue #450)\n\n* Fixed ``BrokenPipeError`` and ``ConnectionError`` handling in Py3.\n  (Issue #443)\n\n\n\n1.9 (2014-07-04)\n================\n\n* Shuffled around development-related files. If you're maintaining a distro\n  package of urllib3, you may need to tweak things. (Issue #415)\n\n* Unverified HTTPS requests will trigger a warning on the first request. See\n  our new `security documentation\n  <https://urllib3.readthedocs.io/en/latest/security.html>`_ for details.\n  (Issue #426)\n\n* New retry logic and ``urllib3.util.retry.Retry`` configuration object.\n  (Issue #326)\n\n* All raised exceptions should now wrapped in a\n  ``urllib3.exceptions.HTTPException``-extending exception. (Issue #326)\n\n* All errors during a retry-enabled request should be wrapped in\n  ``urllib3.exceptions.MaxRetryError``, including timeout-related exceptions\n  which were previously exempt. Underlying error is accessible from the\n  ``.reason`` property. (Issue #326)\n\n* ``urllib3.exceptions.ConnectionError`` renamed to\n  ``urllib3.exceptions.ProtocolError``. (Issue #326)\n\n* Errors during response read (such as IncompleteRead) are now wrapped in\n  ``urllib3.exceptions.ProtocolError``. (Issue #418)\n\n* Requesting an empty host will raise ``urllib3.exceptions.LocationValueError``.\n  (Issue #417)\n\n* Catch read timeouts over SSL connections as\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #419)\n\n* Apply socket arguments before connecting. (Issue #427)\n\n\n1.8.3 (2014-06-23)\n==================\n\n* Fix TLS verification when using a proxy in Python 3.4.1. (Issue #385)\n\n* Add ``disable_cache`` option to ``urllib3.util.make_headers``. (Issue #393)\n\n* Wrap ``socket.timeout`` exception with\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #399)\n\n* Fixed proxy-related bug where connections were being reused incorrectly.\n  (Issues #366, #369)\n\n* Added ``socket_options`` keyword parameter which allows to define\n  ``setsockopt`` configuration of new sockets. (Issue #397)\n\n* Removed ``HTTPConnection.tcp_nodelay`` in favor of\n  ``HTTPConnection.default_socket_options``. (Issue #397)\n\n* Fixed ``TypeError`` bug in Python 2.6.4. (Issue #411)\n\n\n1.8.2 (2014-04-17)\n==================\n\n* Fix ``urllib3.util`` not being included in the package.\n\n\n1.8.1 (2014-04-17)\n==================\n\n* Fix AppEngine bug of HTTPS requests going out as HTTP. (Issue #356)\n\n* Don't install ``dummyserver`` into ``site-packages`` as it's only needed\n  for the test suite. (Issue #362)\n\n* Added support for specifying ``source_address``. (Issue #352)\n\n\n1.8 (2014-03-04)\n================\n\n* Improved url parsing in ``urllib3.util.parse_url`` (properly parse '@' in\n  username, and blank ports like 'hostname:').\n\n* New ``urllib3.connection`` module which contains all the HTTPConnection\n  objects.\n\n* Several ``urllib3.util.Timeout``-related fixes. Also changed constructor\n  signature to a more sensible order. [Backwards incompatible]\n  (Issues #252, #262, #263)\n\n* Use ``backports.ssl_match_hostname`` if it's installed. (Issue #274)\n\n* Added ``.tell()`` method to ``urllib3.response.HTTPResponse`` which\n  returns the number of bytes read so far. (Issue #277)\n\n* Support for platforms without threading. (Issue #289)\n\n* Expand default-port comparison in ``HTTPConnectionPool.is_same_host``\n  to allow a pool with no specified port to be considered equal to to an\n  HTTP/HTTPS url with port 80/443 explicitly provided. (Issue #305)\n\n* Improved default SSL/TLS settings to avoid vulnerabilities.\n  (Issue #309)\n\n* Fixed ``urllib3.poolmanager.ProxyManager`` not retrying on connect errors.\n  (Issue #310)\n\n* Disable Nagle's Algorithm on the socket for non-proxies. A subset of requests\n  will send the entire HTTP request ~200 milliseconds faster; however, some of\n  the resulting TCP packets will be smaller. (Issue #254)\n\n* Increased maximum number of SubjectAltNames in ``urllib3.contrib.pyopenssl``\n  from the default 64 to 1024 in a single certificate. (Issue #318)\n\n* Headers are now passed and stored as a custom\n  ``urllib3.collections_.HTTPHeaderDict`` object rather than a plain ``dict``.\n  (Issue #329, #333)\n\n* Headers no longer lose their case on Python 3. (Issue #236)\n\n* ``urllib3.contrib.pyopenssl`` now uses the operating system's default CA\n  certificates on inject. (Issue #332)\n\n* Requests with ``retries=False`` will immediately raise any exceptions without\n  wrapping them in ``MaxRetryError``. (Issue #348)\n\n* Fixed open socket leak with SSL-related failures. (Issue #344, #348)\n\n\n1.7.1 (2013-09-25)\n==================\n\n* Added granular timeout support with new ``urllib3.util.Timeout`` class.\n  (Issue #231)\n\n* Fixed Python 3.4 support. (Issue #238)\n\n\n1.7 (2013-08-14)\n================\n\n* More exceptions are now pickle-able, with tests. (Issue #174)\n\n* Fixed redirecting with relative URLs in Location header. (Issue #178)\n\n* Support for relative urls in ``Location: ...`` header. (Issue #179)\n\n* ``urllib3.response.HTTPResponse`` now inherits from ``io.IOBase`` for bonus\n  file-like functionality. (Issue #187)\n\n* Passing ``assert_hostname=False`` when creating a HTTPSConnectionPool will\n  skip hostname verification for SSL connections. (Issue #194)\n\n* New method ``urllib3.response.HTTPResponse.stream(...)`` which acts as a\n  generator wrapped around ``.read(...)``. (Issue #198)\n\n* IPv6 url parsing enforces brackets around the hostname. (Issue #199)\n\n* Fixed thread race condition in\n  ``urllib3.poolmanager.PoolManager.connection_from_host(...)`` (Issue #204)\n\n* ``ProxyManager`` requests now include non-default port in ``Host: ...``\n  header. (Issue #217)\n\n* Added HTTPS proxy support in ``ProxyManager``. (Issue #170 #139)\n\n* New ``RequestField`` object can be passed to the ``fields=...`` param which\n  can specify headers. (Issue #220)\n\n* Raise ``urllib3.exceptions.ProxyError`` when connecting to proxy fails.\n  (Issue #221)\n\n* Use international headers when posting file names. (Issue #119)\n\n* Improved IPv6 support. (Issue #203)\n\n\n1.6 (2013-04-25)\n================\n\n* Contrib: Optional SNI support for Py2 using PyOpenSSL. (Issue #156)\n\n* ``ProxyManager`` automatically adds ``Host: ...`` header if not given.\n\n* Improved SSL-related code. ``cert_req`` now optionally takes a string like\n  \"REQUIRED\" or \"NONE\". Same with ``ssl_version`` takes strings like \"SSLv23\"\n  The string values reflect the suffix of the respective constant variable.\n  (Issue #130)\n\n* Vendored ``socksipy`` now based on Anorov's fork which handles unexpectedly\n  closed proxy connections and larger read buffers. (Issue #135)\n\n* Ensure the connection is closed if no data is received, fixes connection leak\n  on some platforms. (Issue #133)\n\n* Added SNI support for SSL/TLS connections on Py32+. (Issue #89)\n\n* Tests fixed to be compatible with Py26 again. (Issue #125)\n\n* Added ability to choose SSL version by passing an ``ssl.PROTOCOL_*`` constant\n  to the ``ssl_version`` parameter of ``HTTPSConnectionPool``. (Issue #109)\n\n* Allow an explicit content type to be specified when encoding file fields.\n  (Issue #126)\n\n* Exceptions are now pickleable, with tests. (Issue #101)\n\n* Fixed default headers not getting passed in some cases. (Issue #99)\n\n* Treat \"content-encoding\" header value as case-insensitive, per RFC 2616\n  Section 3.5. (Issue #110)\n\n* \"Connection Refused\" SocketErrors will get retried rather than raised.\n  (Issue #92)\n\n* Updated vendored ``six``, no longer overrides the global ``six`` module\n  namespace. (Issue #113)\n\n* ``urllib3.exceptions.MaxRetryError`` contains a ``reason`` property holding\n  the exception that prompted the final retry. If ``reason is None`` then it\n  was due to a redirect. (Issue #92, #114)\n\n* Fixed ``PoolManager.urlopen()`` from not redirecting more than once.\n  (Issue #149)\n\n* Don't assume ``Content-Type: text/plain`` for multi-part encoding parameters\n  that are not files. (Issue #111)\n\n* Pass `strict` param down to ``httplib.HTTPConnection``. (Issue #122)\n\n* Added mechanism to verify SSL certificates by fingerprint (md5, sha1) or\n  against an arbitrary hostname (when connecting by IP or for misconfigured\n  servers). (Issue #140)\n\n* Streaming decompression support. (Issue #159)\n\n\n1.5 (2012-08-02)\n================\n\n* Added ``urllib3.add_stderr_logger()`` for quickly enabling STDERR debug\n  logging in urllib3.\n\n* Native full URL parsing (including auth, path, query, fragment) available in\n  ``urllib3.util.parse_url(url)``.\n\n* Built-in redirect will switch method to 'GET' if status code is 303.\n  (Issue #11)\n\n* ``urllib3.PoolManager`` strips the scheme and host before sending the request\n  uri. (Issue #8)\n\n* New ``urllib3.exceptions.DecodeError`` exception for when automatic decoding,\n  based on the Content-Type header, fails.\n\n* Fixed bug with pool depletion and leaking connections (Issue #76). Added\n  explicit connection closing on pool eviction. Added\n  ``urllib3.PoolManager.clear()``.\n\n* 99% -> 100% unit test coverage.\n\n\n1.4 (2012-06-16)\n================\n\n* Minor AppEngine-related fixes.\n\n* Switched from ``mimetools.choose_boundary`` to ``uuid.uuid4()``.\n\n* Improved url parsing. (Issue #73)\n\n* IPv6 url support. (Issue #72)\n\n\n1.3 (2012-03-25)\n================\n\n* Removed pre-1.0 deprecated API.\n\n* Refactored helpers into a ``urllib3.util`` submodule.\n\n* Fixed multipart encoding to support list-of-tuples for keys with multiple\n  values. (Issue #48)\n\n* Fixed multiple Set-Cookie headers in response not getting merged properly in\n  Python 3. (Issue #53)\n\n* AppEngine support with Py27. (Issue #61)\n\n* Minor ``encode_multipart_formdata`` fixes related to Python 3 strings vs\n  bytes.\n\n\n1.2.2 (2012-02-06)\n==================\n\n* Fixed packaging bug of not shipping ``test-requirements.txt``. (Issue #47)\n\n\n1.2.1 (2012-02-05)\n==================\n\n* Fixed another bug related to when ``ssl`` module is not available. (Issue #41)\n\n* Location parsing errors now raise ``urllib3.exceptions.LocationParseError``\n  which inherits from ``ValueError``.\n\n\n1.2 (2012-01-29)\n================\n\n* Added Python 3 support (tested on 3.2.2)\n\n* Dropped Python 2.5 support (tested on 2.6.7, 2.7.2)\n\n* Use ``select.poll`` instead of ``select.select`` for platforms that support\n  it.\n\n* Use ``Queue.LifoQueue`` instead of ``Queue.Queue`` for more aggressive\n  connection reusing. Configurable by overriding ``ConnectionPool.QueueCls``.\n\n* Fixed ``ImportError`` during install when ``ssl`` module is not available.\n  (Issue #41)\n\n* Fixed ``PoolManager`` redirects between schemes (such as HTTP -> HTTPS) not\n  completing properly. (Issue #28, uncovered by Issue #10 in v1.1)\n\n* Ported ``dummyserver`` to use ``tornado`` instead of ``webob`` +\n  ``eventlet``. Removed extraneous unsupported dummyserver testing backends.\n  Added socket-level tests.\n\n* More tests. Achievement Unlocked: 99% Coverage.\n\n\n1.1 (2012-01-07)\n================\n\n* Refactored ``dummyserver`` to its own root namespace module (used for\n  testing).\n\n* Added hostname verification for ``VerifiedHTTPSConnection`` by vendoring in\n  Py32's ``ssl_match_hostname``. (Issue #25)\n\n* Fixed cross-host HTTP redirects when using ``PoolManager``. (Issue #10)\n\n* Fixed ``decode_content`` being ignored when set through ``urlopen``. (Issue\n  #27)\n\n* Fixed timeout-related bugs. (Issues #17, #23)\n\n\n1.0.2 (2011-11-04)\n==================\n\n* Fixed typo in ``VerifiedHTTPSConnection`` which would only present as a bug if\n  you're using the object manually. (Thanks pyos)\n\n* Made RecentlyUsedContainer (and consequently PoolManager) more thread-safe by\n  wrapping the access log in a mutex. (Thanks @christer)\n\n* Made RecentlyUsedContainer more dict-like (corrected ``__delitem__`` and\n  ``__getitem__`` behaviour), with tests. Shouldn't affect core urllib3 code.\n\n\n1.0.1 (2011-10-10)\n==================\n\n* Fixed a bug where the same connection would get returned into the pool twice,\n  causing extraneous \"HttpConnectionPool is full\" log warnings.\n\n\n1.0 (2011-10-08)\n================\n\n* Added ``PoolManager`` with LRU expiration of connections (tested and\n  documented).\n* Added ``ProxyManager`` (needs tests, docs, and confirmation that it works\n  with HTTPS proxies).\n* Added optional partial-read support for responses when\n  ``preload_content=False``. You can now make requests and just read the headers\n  without loading the content.\n* Made response decoding optional (default on, same as before).\n* Added optional explicit boundary string for ``encode_multipart_formdata``.\n* Convenience request methods are now inherited from ``RequestMethods``. Old\n  helpers like ``get_url`` and ``post_url`` should be abandoned in favour of\n  the new ``request(method, url, ...)``.\n* Refactored code to be even more decoupled, reusable, and extendable.\n* License header added to ``.py`` files.\n* Embiggened the documentation: Lots of Sphinx-friendly docstrings in the code\n  and docs in ``docs/`` and on https://urllib3.readthedocs.io/.\n* Embettered all the things!\n* Started writing this file.\n\n\n0.4.1 (2011-07-17)\n==================\n\n* Minor bug fixes, code cleanup.\n\n\n0.4 (2011-03-01)\n================\n\n* Better unicode support.\n* Added ``VerifiedHTTPSConnection``.\n* Added ``NTLMConnectionPool`` in contrib.\n* Minor improvements.\n\n\n0.3.1 (2010-07-13)\n==================\n\n* Added ``assert_host_name`` optional parameter. Now compatible with proxies.\n\n\n0.3 (2009-12-10)\n================\n\n* Added HTTPS support.\n* Minor bug fixes.\n* Refactored, broken backwards compatibility with 0.2.\n* API to be treated as stable from this version forward.\n\n\n0.2 (2008-11-17)\n================\n\n* Added unit tests.\n* Bug fixes.\n\n\n0.1 (2008-11-16)\n================\n\n* First release.\n", "from __future__ import annotations\n\nimport collections\nimport contextlib\nimport gzip\nimport json\nimport logging\nimport sys\nimport typing\nimport zlib\nfrom datetime import datetime, timedelta, timezone\nfrom http.client import responses\nfrom io import BytesIO\nfrom urllib.parse import urlsplit\n\nfrom tornado import httputil\nfrom tornado.web import RequestHandler\n\nfrom urllib3.util.util import to_str\n\nlog = logging.getLogger(__name__)\n\n\nclass Response:\n    def __init__(\n        self,\n        body: str | bytes | typing.Sequence[str | bytes] = \"\",\n        status: str = \"200 OK\",\n        headers: typing.Sequence[tuple[str, str | bytes]] | None = None,\n        json: typing.Any | None = None,\n    ) -> None:\n        self.body = body\n        self.status = status\n        if json is not None:\n            self.headers = headers or [(\"Content-type\", \"application/json\")]\n            self.body = json\n        else:\n            self.headers = headers or [(\"Content-type\", \"text/plain\")]\n\n    def __call__(self, request_handler: RequestHandler) -> None:\n        status, reason = self.status.split(\" \", 1)\n        request_handler.set_status(int(status), reason)\n        for header, value in self.headers:\n            request_handler.add_header(header, value)\n\n        if isinstance(self.body, str):\n            request_handler.write(self.body.encode())\n        elif isinstance(self.body, bytes):\n            request_handler.write(self.body)\n        # chunked\n        else:\n            for item in self.body:\n                if not isinstance(item, bytes):\n                    item = item.encode(\"utf8\")\n                request_handler.write(item)\n                request_handler.flush()\n\n\nRETRY_TEST_NAMES: dict[str, int] = collections.defaultdict(int)\n\n\ndef request_params(request: httputil.HTTPServerRequest) -> dict[str, bytes]:\n    params = {}\n    for k, v in request.arguments.items():\n        params[k] = next(iter(v))\n    return params\n\n\nclass TestingApp(RequestHandler):\n    \"\"\"\n    Simple app that performs various operations, useful for testing an HTTP\n    library.\n\n    Given any path, it will attempt to load a corresponding local method if\n    it exists. Status code 200 indicates success, 400 indicates failure. Each\n    method has its own conditions for success/failure.\n    \"\"\"\n\n    def get(self) -> None:\n        \"\"\"Handle GET requests\"\"\"\n        self._call_method()\n\n    def post(self) -> None:\n        \"\"\"Handle POST requests\"\"\"\n        self._call_method()\n\n    def put(self) -> None:\n        \"\"\"Handle PUT requests\"\"\"\n        self._call_method()\n\n    def options(self) -> None:\n        \"\"\"Handle OPTIONS requests\"\"\"\n        self._call_method()\n\n    def head(self) -> None:\n        \"\"\"Handle HEAD requests\"\"\"\n        self._call_method()\n\n    def _call_method(self) -> None:\n        \"\"\"Call the correct method in this class based on the incoming URI\"\"\"\n        req = self.request\n\n        path = req.path[:]\n        if not path.startswith(\"/\"):\n            path = urlsplit(path).path\n\n        target = path[1:].split(\"/\", 1)[0]\n        method = getattr(self, target, self.index)\n\n        resp = method(req)\n\n        if dict(resp.headers).get(\"Connection\") == \"close\":\n            # FIXME: Can we kill the connection somehow?\n            pass\n\n        resp(self)\n\n    def index(self, _request: httputil.HTTPServerRequest) -> Response:\n        \"Render simple message\"\n        return Response(\"Dummy server!\")\n\n    def certificate(self, request: httputil.HTTPServerRequest) -> Response:\n        \"\"\"Return the requester's certificate.\"\"\"\n        cert = request.get_ssl_certificate()\n        assert isinstance(cert, dict)\n        subject = {}\n        if cert is not None:\n            subject = {k: v for (k, v) in [y for z in cert[\"subject\"] for y in z]}\n        return Response(json.dumps(subject))\n\n    def alpn_protocol(self, request: httputil.HTTPServerRequest) -> Response:\n        \"\"\"Return the selected ALPN protocol.\"\"\"\n        assert request.connection is not None\n        proto = request.connection.stream.socket.selected_alpn_protocol()  # type: ignore[attr-defined]\n        return Response(proto.encode(\"utf8\") if proto is not None else \"\")\n\n    def source_address(self, request: httputil.HTTPServerRequest) -> Response:\n        \"\"\"Return the requester's IP address.\"\"\"\n        return Response(request.remote_ip)  # type: ignore[arg-type]\n\n    def set_up(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        test_type = params.get(\"test_type\")\n        test_id = params.get(\"test_id\")\n        if test_id:\n            print(f\"\\nNew test {test_type!r}: {test_id!r}\")\n        else:\n            print(f\"\\nNew test {test_type!r}\")\n        return Response(\"Dummy server is ready!\")\n\n    def specific_method(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Confirm that the request matches the desired method type\"\n        params = request_params(request)\n        method = params.get(\"method\")\n        method_str = method.decode() if method else None\n\n        if request.method != method_str:\n            return Response(\n                f\"Wrong method: {method_str} != {request.method}\",\n                status=\"400 Bad Request\",\n            )\n        return Response()\n\n    def upload(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Confirm that the uploaded file conforms to specification\"\n        params = request_params(request)\n        # FIXME: This is a huge broken mess\n        param = params.get(\"upload_param\", b\"myfile\").decode(\"ascii\")\n        filename = params.get(\"upload_filename\", b\"\").decode(\"utf-8\")\n        size = int(params.get(\"upload_size\", \"0\"))\n        files_ = request.files.get(param)\n        assert files_ is not None\n\n        if len(files_) != 1:\n            return Response(\n                f\"Expected 1 file for '{param}', not {len(files_)}\",\n                status=\"400 Bad Request\",\n            )\n        file_ = files_[0]\n\n        data = file_[\"body\"]\n        if int(size) != len(data):\n            return Response(\n                f\"Wrong size: {int(size)} != {len(data)}\", status=\"400 Bad Request\"\n            )\n\n        got_filename = file_[\"filename\"]\n        if isinstance(got_filename, bytes):\n            got_filename = got_filename.decode(\"utf-8\")\n\n        # Tornado can leave the trailing \\n in place on the filename.\n        if filename != got_filename:\n            return Response(\n                f\"Wrong filename: {filename} != {file_.filename}\",\n                status=\"400 Bad Request\",\n            )\n\n        return Response()\n\n    def redirect(self, request: httputil.HTTPServerRequest) -> Response:  # type: ignore[override]\n        \"Perform a redirect to ``target``\"\n        params = request_params(request)\n        target = params.get(\"target\", \"/\")\n        status = params.get(\"status\", b\"303 See Other\").decode(\"latin-1\")\n        if len(status) == 3:\n            status = f\"{status} Redirect\"\n\n        headers = [(\"Location\", target)]\n        return Response(status=status, headers=headers)\n\n    def not_found(self, request: httputil.HTTPServerRequest) -> Response:\n        return Response(\"Not found\", status=\"404 Not Found\")\n\n    def multi_redirect(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Performs a redirect chain based on ``redirect_codes``\"\n        params = request_params(request)\n        codes = params.get(\"redirect_codes\", b\"200\").decode(\"utf-8\")\n        head, tail = codes.split(\",\", 1) if \",\" in codes else (codes, None)\n        assert head is not None\n        status = f\"{head} {responses[int(head)]}\"\n        if not tail:\n            return Response(\"Done redirecting\", status=status)\n\n        headers = [(\"Location\", f\"/multi_redirect?redirect_codes={tail}\")]\n        return Response(status=status, headers=headers)\n\n    def keepalive(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        if params.get(\"close\", b\"0\") == b\"1\":\n            headers = [(\"Connection\", \"close\")]\n            return Response(\"Closing\", headers=headers)\n\n        headers = [(\"Connection\", \"keep-alive\")]\n        return Response(\"Keeping alive\", headers=headers)\n\n    def echo_params(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        echod = sorted((to_str(k), to_str(v)) for k, v in params.items())\n        return Response(repr(echod))\n\n    def echo(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Echo back the params\"\n        if request.method == \"GET\":\n            return Response(request.query)\n\n        return Response(request.body)\n\n    def echo_json(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Echo back the JSON\"\n        return Response(json=request.body, headers=list(request.headers.items()))\n\n    def echo_uri(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Echo back the requested URI\"\n        assert request.uri is not None\n        return Response(request.uri)\n\n    def encodingrequest(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Check for UA accepting gzip/deflate encoding\"\n        data = b\"hello, world!\"\n        encoding = request.headers.get(\"Accept-Encoding\", \"\")\n        headers = None\n        if encoding == \"gzip\":\n            headers = [(\"Content-Encoding\", \"gzip\")]\n            file_ = BytesIO()\n            with contextlib.closing(\n                gzip.GzipFile(\"\", mode=\"w\", fileobj=file_)\n            ) as zipfile:\n                zipfile.write(data)\n            data = file_.getvalue()\n        elif encoding == \"deflate\":\n            headers = [(\"Content-Encoding\", \"deflate\")]\n            data = zlib.compress(data)\n        elif encoding == \"garbage-gzip\":\n            headers = [(\"Content-Encoding\", \"gzip\")]\n            data = b\"garbage\"\n        elif encoding == \"garbage-deflate\":\n            headers = [(\"Content-Encoding\", \"deflate\")]\n            data = b\"garbage\"\n        return Response(data, headers=headers)\n\n    def headers(self, request: httputil.HTTPServerRequest) -> Response:\n        return Response(json.dumps(dict(request.headers)))\n\n    def headers_and_params(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        return Response(\n            json.dumps({\"headers\": dict(request.headers), \"params\": params})\n        )\n\n    def multi_headers(self, request: httputil.HTTPServerRequest) -> Response:\n        return Response(json.dumps({\"headers\": list(request.headers.get_all())}))\n\n    def successful_retry(self, request: httputil.HTTPServerRequest) -> Response:\n        \"\"\"Handler which will return an error and then success\n\n        It's not currently very flexible as the number of retries is hard-coded.\n        \"\"\"\n        test_name = request.headers.get(\"test-name\", None)\n        if not test_name:\n            return Response(\"test-name header not set\", status=\"400 Bad Request\")\n\n        RETRY_TEST_NAMES[test_name] += 1\n\n        if RETRY_TEST_NAMES[test_name] >= 2:\n            return Response(\"Retry successful!\")\n        else:\n            return Response(\"need to keep retrying!\", status=\"418 I'm A Teapot\")\n\n    def chunked(self, request: httputil.HTTPServerRequest) -> Response:\n        return Response([\"123\"] * 4)\n\n    def chunked_gzip(self, request: httputil.HTTPServerRequest) -> Response:\n        chunks = []\n        compressor = zlib.compressobj(6, zlib.DEFLATED, 16 + zlib.MAX_WBITS)\n\n        for uncompressed in [b\"123\"] * 4:\n            chunks.append(compressor.compress(uncompressed))\n\n        chunks.append(compressor.flush())\n\n        return Response(chunks, headers=[(\"Content-Encoding\", \"gzip\")])\n\n    def nbytes(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        length = int(params[\"length\"])\n        data = b\"1\" * length\n        return Response(data, headers=[(\"Content-Type\", \"application/octet-stream\")])\n\n    def status(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        status = params.get(\"status\", b\"200 OK\").decode(\"latin-1\")\n\n        return Response(status=status)\n\n    def retry_after(self, request: httputil.HTTPServerRequest) -> Response:\n        params = request_params(request)\n        if datetime.now() - self.application.last_req < timedelta(seconds=1):  # type: ignore[attr-defined]\n            status = params.get(\"status\", b\"429 Too Many Requests\")\n            return Response(\n                status=status.decode(\"utf-8\"), headers=[(\"Retry-After\", \"1\")]\n            )\n\n        self.application.last_req = datetime.now()  # type: ignore[attr-defined]\n\n        return Response(status=\"200 OK\")\n\n    def redirect_after(self, request: httputil.HTTPServerRequest) -> Response:\n        \"Perform a redirect to ``target``\"\n        params = request_params(request)\n        date = params.get(\"date\")\n        if date:\n            retry_after = str(\n                httputil.format_timestamp(\n                    datetime.fromtimestamp(float(date), tz=timezone.utc)\n                )\n            )\n        else:\n            retry_after = \"1\"\n        target = params.get(\"target\", \"/\")\n        headers = [(\"Location\", target), (\"Retry-After\", retry_after)]\n        return Response(status=\"303 See Other\", headers=headers)\n\n    def shutdown(self, request: httputil.HTTPServerRequest) -> typing.NoReturn:\n        sys.exit()\n", "from __future__ import annotations\n\nimport typing\nfrom collections import OrderedDict\nfrom enum import Enum, auto\nfrom threading import RLock\n\nif typing.TYPE_CHECKING:\n    # We can only import Protocol if TYPE_CHECKING because it's a development\n    # dependency, and is not available at runtime.\n    from typing import Protocol\n\n    from typing_extensions import Self\n\n    class HasGettableStringKeys(Protocol):\n        def keys(self) -> typing.Iterator[str]:\n            ...\n\n        def __getitem__(self, key: str) -> str:\n            ...\n\n\n__all__ = [\"RecentlyUsedContainer\", \"HTTPHeaderDict\"]\n\n\n# Key type\n_KT = typing.TypeVar(\"_KT\")\n# Value type\n_VT = typing.TypeVar(\"_VT\")\n# Default type\n_DT = typing.TypeVar(\"_DT\")\n\nValidHTTPHeaderSource = typing.Union[\n    \"HTTPHeaderDict\",\n    typing.Mapping[str, str],\n    typing.Iterable[typing.Tuple[str, str]],\n    \"HasGettableStringKeys\",\n]\n\n\nclass _Sentinel(Enum):\n    not_passed = auto()\n\n\ndef ensure_can_construct_http_header_dict(\n    potential: object,\n) -> ValidHTTPHeaderSource | None:\n    if isinstance(potential, HTTPHeaderDict):\n        return potential\n    elif isinstance(potential, typing.Mapping):\n        # Full runtime checking of the contents of a Mapping is expensive, so for the\n        # purposes of typechecking, we assume that any Mapping is the right shape.\n        return typing.cast(typing.Mapping[str, str], potential)\n    elif isinstance(potential, typing.Iterable):\n        # Similarly to Mapping, full runtime checking of the contents of an Iterable is\n        # expensive, so for the purposes of typechecking, we assume that any Iterable\n        # is the right shape.\n        return typing.cast(typing.Iterable[typing.Tuple[str, str]], potential)\n    elif hasattr(potential, \"keys\") and hasattr(potential, \"__getitem__\"):\n        return typing.cast(\"HasGettableStringKeys\", potential)\n    else:\n        return None\n\n\nclass RecentlyUsedContainer(typing.Generic[_KT, _VT], typing.MutableMapping[_KT, _VT]):\n    \"\"\"\n    Provides a thread-safe dict-like container which maintains up to\n    ``maxsize`` keys while throwing away the least-recently-used keys beyond\n    ``maxsize``.\n\n    :param maxsize:\n        Maximum number of recent elements to retain.\n\n    :param dispose_func:\n        Every time an item is evicted from the container,\n        ``dispose_func(value)`` is called.  Callback which will get called\n    \"\"\"\n\n    _container: typing.OrderedDict[_KT, _VT]\n    _maxsize: int\n    dispose_func: typing.Callable[[_VT], None] | None\n    lock: RLock\n\n    def __init__(\n        self,\n        maxsize: int = 10,\n        dispose_func: typing.Callable[[_VT], None] | None = None,\n    ) -> None:\n        super().__init__()\n        self._maxsize = maxsize\n        self.dispose_func = dispose_func\n        self._container = OrderedDict()\n        self.lock = RLock()\n\n    def __getitem__(self, key: _KT) -> _VT:\n        # Re-insert the item, moving it to the end of the eviction line.\n        with self.lock:\n            item = self._container.pop(key)\n            self._container[key] = item\n            return item\n\n    def __setitem__(self, key: _KT, value: _VT) -> None:\n        evicted_item = None\n        with self.lock:\n            # Possibly evict the existing value of 'key'\n            try:\n                # If the key exists, we'll overwrite it, which won't change the\n                # size of the pool. Because accessing a key should move it to\n                # the end of the eviction line, we pop it out first.\n                evicted_item = key, self._container.pop(key)\n                self._container[key] = value\n            except KeyError:\n                # When the key does not exist, we insert the value first so that\n                # evicting works in all cases, including when self._maxsize is 0\n                self._container[key] = value\n                if len(self._container) > self._maxsize:\n                    # If we didn't evict an existing value, and we've hit our maximum\n                    # size, then we have to evict the least recently used item from\n                    # the beginning of the container.\n                    evicted_item = self._container.popitem(last=False)\n\n        # After releasing the lock on the pool, dispose of any evicted value.\n        if evicted_item is not None and self.dispose_func:\n            _, evicted_value = evicted_item\n            self.dispose_func(evicted_value)\n\n    def __delitem__(self, key: _KT) -> None:\n        with self.lock:\n            value = self._container.pop(key)\n\n        if self.dispose_func:\n            self.dispose_func(value)\n\n    def __len__(self) -> int:\n        with self.lock:\n            return len(self._container)\n\n    def __iter__(self) -> typing.NoReturn:\n        raise NotImplementedError(\n            \"Iteration over this class is unlikely to be threadsafe.\"\n        )\n\n    def clear(self) -> None:\n        with self.lock:\n            # Copy pointers to all values, then wipe the mapping\n            values = list(self._container.values())\n            self._container.clear()\n\n        if self.dispose_func:\n            for value in values:\n                self.dispose_func(value)\n\n    def keys(self) -> set[_KT]:  # type: ignore[override]\n        with self.lock:\n            return set(self._container.keys())\n\n\nclass HTTPHeaderDictItemView(typing.Set[typing.Tuple[str, str]]):\n    \"\"\"\n    HTTPHeaderDict is unusual for a Mapping[str, str] in that it has two modes of\n    address.\n\n    If we directly try to get an item with a particular name, we will get a string\n    back that is the concatenated version of all the values:\n\n    >>> d['X-Header-Name']\n    'Value1, Value2, Value3'\n\n    However, if we iterate over an HTTPHeaderDict's items, we will optionally combine\n    these values based on whether combine=True was called when building up the dictionary\n\n    >>> d = HTTPHeaderDict({\"A\": \"1\", \"B\": \"foo\"})\n    >>> d.add(\"A\", \"2\", combine=True)\n    >>> d.add(\"B\", \"bar\")\n    >>> list(d.items())\n    [\n        ('A', '1, 2'),\n        ('B', 'foo'),\n        ('B', 'bar'),\n    ]\n\n    This class conforms to the interface required by the MutableMapping ABC while\n    also giving us the nonstandard iteration behavior we want; items with duplicate\n    keys, ordered by time of first insertion.\n    \"\"\"\n\n    _headers: HTTPHeaderDict\n\n    def __init__(self, headers: HTTPHeaderDict) -> None:\n        self._headers = headers\n\n    def __len__(self) -> int:\n        return len(list(self._headers.iteritems()))\n\n    def __iter__(self) -> typing.Iterator[tuple[str, str]]:\n        return self._headers.iteritems()\n\n    def __contains__(self, item: object) -> bool:\n        if isinstance(item, tuple) and len(item) == 2:\n            passed_key, passed_val = item\n            if isinstance(passed_key, str) and isinstance(passed_val, str):\n                return self._headers._has_value_for_header(passed_key, passed_val)\n        return False\n\n\nclass HTTPHeaderDict(typing.MutableMapping[str, str]):\n    \"\"\"\n    :param headers:\n        An iterable of field-value pairs. Must not contain multiple field names\n        when compared case-insensitively.\n\n    :param kwargs:\n        Additional field-value pairs to pass in to ``dict.update``.\n\n    A ``dict`` like container for storing HTTP Headers.\n\n    Field names are stored and compared case-insensitively in compliance with\n    RFC 7230. Iteration provides the first case-sensitive key seen for each\n    case-insensitive pair.\n\n    Using ``__setitem__`` syntax overwrites fields that compare equal\n    case-insensitively in order to maintain ``dict``'s api. For fields that\n    compare equal, instead create a new ``HTTPHeaderDict`` and use ``.add``\n    in a loop.\n\n    If multiple fields that are equal case-insensitively are passed to the\n    constructor or ``.update``, the behavior is undefined and some will be\n    lost.\n\n    >>> headers = HTTPHeaderDict()\n    >>> headers.add('Set-Cookie', 'foo=bar')\n    >>> headers.add('set-cookie', 'baz=quxx')\n    >>> headers['content-length'] = '7'\n    >>> headers['SET-cookie']\n    'foo=bar, baz=quxx'\n    >>> headers['Content-Length']\n    '7'\n    \"\"\"\n\n    _container: typing.MutableMapping[str, list[str]]\n\n    def __init__(self, headers: ValidHTTPHeaderSource | None = None, **kwargs: str):\n        super().__init__()\n        self._container = {}  # 'dict' is insert-ordered\n        if headers is not None:\n            if isinstance(headers, HTTPHeaderDict):\n                self._copy_from(headers)\n            else:\n                self.extend(headers)\n        if kwargs:\n            self.extend(kwargs)\n\n    def __setitem__(self, key: str, val: str) -> None:\n        # avoid a bytes/str comparison by decoding before httplib\n        if isinstance(key, bytes):\n            key = key.decode(\"latin-1\")\n        self._container[key.lower()] = [key, val]\n\n    def __getitem__(self, key: str) -> str:\n        val = self._container[key.lower()]\n        return \", \".join(val[1:])\n\n    def __delitem__(self, key: str) -> None:\n        del self._container[key.lower()]\n\n    def __contains__(self, key: object) -> bool:\n        if isinstance(key, str):\n            return key.lower() in self._container\n        return False\n\n    def setdefault(self, key: str, default: str = \"\") -> str:\n        return super().setdefault(key, default)\n\n    def __eq__(self, other: object) -> bool:\n        maybe_constructable = ensure_can_construct_http_header_dict(other)\n        if maybe_constructable is None:\n            return False\n        else:\n            other_as_http_header_dict = type(self)(maybe_constructable)\n\n        return {k.lower(): v for k, v in self.itermerged()} == {\n            k.lower(): v for k, v in other_as_http_header_dict.itermerged()\n        }\n\n    def __ne__(self, other: object) -> bool:\n        return not self.__eq__(other)\n\n    def __len__(self) -> int:\n        return len(self._container)\n\n    def __iter__(self) -> typing.Iterator[str]:\n        # Only provide the originally cased names\n        for vals in self._container.values():\n            yield vals[0]\n\n    def discard(self, key: str) -> None:\n        try:\n            del self[key]\n        except KeyError:\n            pass\n\n    def add(self, key: str, val: str, *, combine: bool = False) -> None:\n        \"\"\"Adds a (name, value) pair, doesn't overwrite the value if it already\n        exists.\n\n        If this is called with combine=True, instead of adding a new header value\n        as a distinct item during iteration, this will instead append the value to\n        any existing header value with a comma. If no existing header value exists\n        for the key, then the value will simply be added, ignoring the combine parameter.\n\n        >>> headers = HTTPHeaderDict(foo='bar')\n        >>> headers.add('Foo', 'baz')\n        >>> headers['foo']\n        'bar, baz'\n        >>> list(headers.items())\n        [('foo', 'bar'), ('foo', 'baz')]\n        >>> headers.add('foo', 'quz', combine=True)\n        >>> list(headers.items())\n        [('foo', 'bar, baz, quz')]\n        \"\"\"\n        # avoid a bytes/str comparison by decoding before httplib\n        if isinstance(key, bytes):\n            key = key.decode(\"latin-1\")\n        key_lower = key.lower()\n        new_vals = [key, val]\n        # Keep the common case aka no item present as fast as possible\n        vals = self._container.setdefault(key_lower, new_vals)\n        if new_vals is not vals:\n            # if there are values here, then there is at least the initial\n            # key/value pair\n            assert len(vals) >= 2\n            if combine:\n                vals[-1] = vals[-1] + \", \" + val\n            else:\n                vals.append(val)\n\n    def extend(self, *args: ValidHTTPHeaderSource, **kwargs: str) -> None:\n        \"\"\"Generic import function for any type of header-like object.\n        Adapted version of MutableMapping.update in order to insert items\n        with self.add instead of self.__setitem__\n        \"\"\"\n        if len(args) > 1:\n            raise TypeError(\n                f\"extend() takes at most 1 positional arguments ({len(args)} given)\"\n            )\n        other = args[0] if len(args) >= 1 else ()\n\n        if isinstance(other, HTTPHeaderDict):\n            for key, val in other.iteritems():\n                self.add(key, val)\n        elif isinstance(other, typing.Mapping):\n            for key, val in other.items():\n                self.add(key, val)\n        elif isinstance(other, typing.Iterable):\n            other = typing.cast(typing.Iterable[typing.Tuple[str, str]], other)\n            for key, value in other:\n                self.add(key, value)\n        elif hasattr(other, \"keys\") and hasattr(other, \"__getitem__\"):\n            # THIS IS NOT A TYPESAFE BRANCH\n            # In this branch, the object has a `keys` attr but is not a Mapping or any of\n            # the other types indicated in the method signature. We do some stuff with\n            # it as though it partially implements the Mapping interface, but we're not\n            # doing that stuff safely AT ALL.\n            for key in other.keys():\n                self.add(key, other[key])\n\n        for key, value in kwargs.items():\n            self.add(key, value)\n\n    @typing.overload\n    def getlist(self, key: str) -> list[str]:\n        ...\n\n    @typing.overload\n    def getlist(self, key: str, default: _DT) -> list[str] | _DT:\n        ...\n\n    def getlist(\n        self, key: str, default: _Sentinel | _DT = _Sentinel.not_passed\n    ) -> list[str] | _DT:\n        \"\"\"Returns a list of all the values for the named field. Returns an\n        empty list if the key doesn't exist.\"\"\"\n        try:\n            vals = self._container[key.lower()]\n        except KeyError:\n            if default is _Sentinel.not_passed:\n                # _DT is unbound; empty list is instance of List[str]\n                return []\n            # _DT is bound; default is instance of _DT\n            return default\n        else:\n            # _DT may or may not be bound; vals[1:] is instance of List[str], which\n            # meets our external interface requirement of `Union[List[str], _DT]`.\n            return vals[1:]\n\n    def _prepare_for_method_change(self) -> Self:\n        \"\"\"\n        Remove content-specific header fields before changing the request\n        method to GET or HEAD according to RFC 9110, Section 15.4.\n        \"\"\"\n        content_specific_headers = [\n            \"Content-Encoding\",\n            \"Content-Language\",\n            \"Content-Location\",\n            \"Content-Type\",\n            \"Content-Length\",\n            \"Digest\",\n            \"Last-Modified\",\n        ]\n        for header in content_specific_headers:\n            self.discard(header)\n        return self\n\n    # Backwards compatibility for httplib\n    getheaders = getlist\n    getallmatchingheaders = getlist\n    iget = getlist\n\n    # Backwards compatibility for http.cookiejar\n    get_all = getlist\n\n    def __repr__(self) -> str:\n        return f\"{type(self).__name__}({dict(self.itermerged())})\"\n\n    def _copy_from(self, other: HTTPHeaderDict) -> None:\n        for key in other:\n            val = other.getlist(key)\n            self._container[key.lower()] = [key, *val]\n\n    def copy(self) -> HTTPHeaderDict:\n        clone = type(self)()\n        clone._copy_from(self)\n        return clone\n\n    def iteritems(self) -> typing.Iterator[tuple[str, str]]:\n        \"\"\"Iterate over all header lines, including duplicate ones.\"\"\"\n        for key in self:\n            vals = self._container[key.lower()]\n            for val in vals[1:]:\n                yield vals[0], val\n\n    def itermerged(self) -> typing.Iterator[tuple[str, str]]:\n        \"\"\"Iterate over all headers, merging duplicate ones together.\"\"\"\n        for key in self:\n            val = self._container[key.lower()]\n            yield val[0], \", \".join(val[1:])\n\n    def items(self) -> HTTPHeaderDictItemView:  # type: ignore[override]\n        return HTTPHeaderDictItemView(self)\n\n    def _has_value_for_header(self, header_name: str, potential_value: str) -> bool:\n        if header_name in self:\n            return potential_value in self._container[header_name.lower()][1:]\n        return False\n\n    def __ior__(self, other: object) -> HTTPHeaderDict:\n        # Supports extending a header dict in-place using operator |=\n        # combining items with add instead of __setitem__\n        maybe_constructable = ensure_can_construct_http_header_dict(other)\n        if maybe_constructable is None:\n            return NotImplemented\n        self.extend(maybe_constructable)\n        return self\n\n    def __or__(self, other: object) -> HTTPHeaderDict:\n        # Supports merging header dicts using operator |\n        # combining items with add instead of __setitem__\n        maybe_constructable = ensure_can_construct_http_header_dict(other)\n        if maybe_constructable is None:\n            return NotImplemented\n        result = self.copy()\n        result.extend(maybe_constructable)\n        return result\n\n    def __ror__(self, other: object) -> HTTPHeaderDict:\n        # Supports merging header dicts using operator | when other is on left side\n        # combining items with add instead of __setitem__\n        maybe_constructable = ensure_can_construct_http_header_dict(other)\n        if maybe_constructable is None:\n            return NotImplemented\n        result = type(self)(maybe_constructable)\n        result.extend(self)\n        return result\n", "# This file is protected via CODEOWNERS\nfrom __future__ import annotations\n\n__version__ = \"2.0.7\"\n", "from __future__ import annotations\n\nimport errno\nimport logging\nimport queue\nimport sys\nimport typing\nimport warnings\nimport weakref\nfrom socket import timeout as SocketTimeout\nfrom types import TracebackType\n\nfrom ._base_connection import _TYPE_BODY\nfrom ._collections import HTTPHeaderDict\nfrom ._request_methods import RequestMethods\nfrom .connection import (\n    BaseSSLError,\n    BrokenPipeError,\n    DummyConnection,\n    HTTPConnection,\n    HTTPException,\n    HTTPSConnection,\n    ProxyConfig,\n    _wrap_proxy_error,\n)\nfrom .connection import port_by_scheme as port_by_scheme\nfrom .exceptions import (\n    ClosedPoolError,\n    EmptyPoolError,\n    FullPoolError,\n    HostChangedError,\n    InsecureRequestWarning,\n    LocationValueError,\n    MaxRetryError,\n    NewConnectionError,\n    ProtocolError,\n    ProxyError,\n    ReadTimeoutError,\n    SSLError,\n    TimeoutError,\n)\nfrom .response import BaseHTTPResponse\nfrom .util.connection import is_connection_dropped\nfrom .util.proxy import connection_requires_http_tunnel\nfrom .util.request import _TYPE_BODY_POSITION, set_file_position\nfrom .util.retry import Retry\nfrom .util.ssl_match_hostname import CertificateError\nfrom .util.timeout import _DEFAULT_TIMEOUT, _TYPE_DEFAULT, Timeout\nfrom .util.url import Url, _encode_target\nfrom .util.url import _normalize_host as normalize_host\nfrom .util.url import parse_url\nfrom .util.util import to_str\n\nif typing.TYPE_CHECKING:\n    import ssl\n    from typing import Literal\n\n    from ._base_connection import BaseHTTPConnection, BaseHTTPSConnection\n\nlog = logging.getLogger(__name__)\n\n_TYPE_TIMEOUT = typing.Union[Timeout, float, _TYPE_DEFAULT, None]\n\n_SelfT = typing.TypeVar(\"_SelfT\")\n\n\n# Pool objects\nclass ConnectionPool:\n    \"\"\"\n    Base class for all connection pools, such as\n    :class:`.HTTPConnectionPool` and :class:`.HTTPSConnectionPool`.\n\n    .. note::\n       ConnectionPool.urlopen() does not normalize or percent-encode target URIs\n       which is useful if your target server doesn't support percent-encoded\n       target URIs.\n    \"\"\"\n\n    scheme: str | None = None\n    QueueCls = queue.LifoQueue\n\n    def __init__(self, host: str, port: int | None = None) -> None:\n        if not host:\n            raise LocationValueError(\"No host specified.\")\n\n        self.host = _normalize_host(host, scheme=self.scheme)\n        self.port = port\n\n        # This property uses 'normalize_host()' (not '_normalize_host()')\n        # to avoid removing square braces around IPv6 addresses.\n        # This value is sent to `HTTPConnection.set_tunnel()` if called\n        # because square braces are required for HTTP CONNECT tunneling.\n        self._tunnel_host = normalize_host(host, scheme=self.scheme).lower()\n\n    def __str__(self) -> str:\n        return f\"{type(self).__name__}(host={self.host!r}, port={self.port!r})\"\n\n    def __enter__(self: _SelfT) -> _SelfT:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> Literal[False]:\n        self.close()\n        # Return False to re-raise any potential exceptions\n        return False\n\n    def close(self) -> None:\n        \"\"\"\n        Close all pooled connections and disable the pool.\n        \"\"\"\n\n\n# This is taken from http://hg.python.org/cpython/file/7aaba721ebc0/Lib/socket.py#l252\n_blocking_errnos = {errno.EAGAIN, errno.EWOULDBLOCK}\n\n\nclass HTTPConnectionPool(ConnectionPool, RequestMethods):\n    \"\"\"\n    Thread-safe connection pool for one host.\n\n    :param host:\n        Host used for this HTTP Connection (e.g. \"localhost\"), passed into\n        :class:`http.client.HTTPConnection`.\n\n    :param port:\n        Port used for this HTTP Connection (None is equivalent to 80), passed\n        into :class:`http.client.HTTPConnection`.\n\n    :param timeout:\n        Socket timeout in seconds for each individual connection. This can\n        be a float or integer, which sets the timeout for the HTTP request,\n        or an instance of :class:`urllib3.util.Timeout` which gives you more\n        fine-grained control over request timeouts. After the constructor has\n        been parsed, this is always a `urllib3.util.Timeout` object.\n\n    :param maxsize:\n        Number of connections to save that can be reused. More than 1 is useful\n        in multithreaded situations. If ``block`` is set to False, more\n        connections will be created but they will not be saved once they've\n        been used.\n\n    :param block:\n        If set to True, no more than ``maxsize`` connections will be used at\n        a time. When no free connections are available, the call will block\n        until a connection has been released. This is a useful side effect for\n        particular multithreaded situations where one does not want to use more\n        than maxsize connections per host to prevent flooding.\n\n    :param headers:\n        Headers to include with all requests, unless other headers are given\n        explicitly.\n\n    :param retries:\n        Retry configuration to use by default with requests in this pool.\n\n    :param _proxy:\n        Parsed proxy URL, should not be used directly, instead, see\n        :class:`urllib3.ProxyManager`\n\n    :param _proxy_headers:\n        A dictionary with proxy headers, should not be used directly,\n        instead, see :class:`urllib3.ProxyManager`\n\n    :param \\\\**conn_kw:\n        Additional parameters are used to create fresh :class:`urllib3.connection.HTTPConnection`,\n        :class:`urllib3.connection.HTTPSConnection` instances.\n    \"\"\"\n\n    scheme = \"http\"\n    ConnectionCls: (\n        type[BaseHTTPConnection] | type[BaseHTTPSConnection]\n    ) = HTTPConnection\n\n    def __init__(\n        self,\n        host: str,\n        port: int | None = None,\n        timeout: _TYPE_TIMEOUT | None = _DEFAULT_TIMEOUT,\n        maxsize: int = 1,\n        block: bool = False,\n        headers: typing.Mapping[str, str] | None = None,\n        retries: Retry | bool | int | None = None,\n        _proxy: Url | None = None,\n        _proxy_headers: typing.Mapping[str, str] | None = None,\n        _proxy_config: ProxyConfig | None = None,\n        **conn_kw: typing.Any,\n    ):\n        ConnectionPool.__init__(self, host, port)\n        RequestMethods.__init__(self, headers)\n\n        if not isinstance(timeout, Timeout):\n            timeout = Timeout.from_float(timeout)\n\n        if retries is None:\n            retries = Retry.DEFAULT\n\n        self.timeout = timeout\n        self.retries = retries\n\n        self.pool: queue.LifoQueue[typing.Any] | None = self.QueueCls(maxsize)\n        self.block = block\n\n        self.proxy = _proxy\n        self.proxy_headers = _proxy_headers or {}\n        self.proxy_config = _proxy_config\n\n        # Fill the queue up so that doing get() on it will block properly\n        for _ in range(maxsize):\n            self.pool.put(None)\n\n        # These are mostly for testing and debugging purposes.\n        self.num_connections = 0\n        self.num_requests = 0\n        self.conn_kw = conn_kw\n\n        if self.proxy:\n            # Enable Nagle's algorithm for proxies, to avoid packet fragmentation.\n            # We cannot know if the user has added default socket options, so we cannot replace the\n            # list.\n            self.conn_kw.setdefault(\"socket_options\", [])\n\n            self.conn_kw[\"proxy\"] = self.proxy\n            self.conn_kw[\"proxy_config\"] = self.proxy_config\n\n        # Do not pass 'self' as callback to 'finalize'.\n        # Then the 'finalize' would keep an endless living (leak) to self.\n        # By just passing a reference to the pool allows the garbage collector\n        # to free self if nobody else has a reference to it.\n        pool = self.pool\n\n        # Close all the HTTPConnections in the pool before the\n        # HTTPConnectionPool object is garbage collected.\n        weakref.finalize(self, _close_pool_connections, pool)\n\n    def _new_conn(self) -> BaseHTTPConnection:\n        \"\"\"\n        Return a fresh :class:`HTTPConnection`.\n        \"\"\"\n        self.num_connections += 1\n        log.debug(\n            \"Starting new HTTP connection (%d): %s:%s\",\n            self.num_connections,\n            self.host,\n            self.port or \"80\",\n        )\n\n        conn = self.ConnectionCls(\n            host=self.host,\n            port=self.port,\n            timeout=self.timeout.connect_timeout,\n            **self.conn_kw,\n        )\n        return conn\n\n    def _get_conn(self, timeout: float | None = None) -> BaseHTTPConnection:\n        \"\"\"\n        Get a connection. Will return a pooled connection if one is available.\n\n        If no connections are available and :prop:`.block` is ``False``, then a\n        fresh connection is returned.\n\n        :param timeout:\n            Seconds to wait before giving up and raising\n            :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and\n            :prop:`.block` is ``True``.\n        \"\"\"\n        conn = None\n\n        if self.pool is None:\n            raise ClosedPoolError(self, \"Pool is closed.\")\n\n        try:\n            conn = self.pool.get(block=self.block, timeout=timeout)\n\n        except AttributeError:  # self.pool is None\n            raise ClosedPoolError(self, \"Pool is closed.\") from None  # Defensive:\n\n        except queue.Empty:\n            if self.block:\n                raise EmptyPoolError(\n                    self,\n                    \"Pool is empty and a new connection can't be opened due to blocking mode.\",\n                ) from None\n            pass  # Oh well, we'll create a new connection then\n\n        # If this is a persistent connection, check if it got disconnected\n        if conn and is_connection_dropped(conn):\n            log.debug(\"Resetting dropped connection: %s\", self.host)\n            conn.close()\n\n        return conn or self._new_conn()\n\n    def _put_conn(self, conn: BaseHTTPConnection | None) -> None:\n        \"\"\"\n        Put a connection back into the pool.\n\n        :param conn:\n            Connection object for the current host and port as returned by\n            :meth:`._new_conn` or :meth:`._get_conn`.\n\n        If the pool is already full, the connection is closed and discarded\n        because we exceeded maxsize. If connections are discarded frequently,\n        then maxsize should be increased.\n\n        If the pool is closed, then the connection will be closed and discarded.\n        \"\"\"\n        if self.pool is not None:\n            try:\n                self.pool.put(conn, block=False)\n                return  # Everything is dandy, done.\n            except AttributeError:\n                # self.pool is None.\n                pass\n            except queue.Full:\n                # Connection never got put back into the pool, close it.\n                if conn:\n                    conn.close()\n\n                if self.block:\n                    # This should never happen if you got the conn from self._get_conn\n                    raise FullPoolError(\n                        self,\n                        \"Pool reached maximum size and no more connections are allowed.\",\n                    ) from None\n\n                log.warning(\n                    \"Connection pool is full, discarding connection: %s. Connection pool size: %s\",\n                    self.host,\n                    self.pool.qsize(),\n                )\n\n        # Connection never got put back into the pool, close it.\n        if conn:\n            conn.close()\n\n    def _validate_conn(self, conn: BaseHTTPConnection) -> None:\n        \"\"\"\n        Called right before a request is made, after the socket is created.\n        \"\"\"\n\n    def _prepare_proxy(self, conn: BaseHTTPConnection) -> None:\n        # Nothing to do for HTTP connections.\n        pass\n\n    def _get_timeout(self, timeout: _TYPE_TIMEOUT) -> Timeout:\n        \"\"\"Helper that always returns a :class:`urllib3.util.Timeout`\"\"\"\n        if timeout is _DEFAULT_TIMEOUT:\n            return self.timeout.clone()\n\n        if isinstance(timeout, Timeout):\n            return timeout.clone()\n        else:\n            # User passed us an int/float. This is for backwards compatibility,\n            # can be removed later\n            return Timeout.from_float(timeout)\n\n    def _raise_timeout(\n        self,\n        err: BaseSSLError | OSError | SocketTimeout,\n        url: str,\n        timeout_value: _TYPE_TIMEOUT | None,\n    ) -> None:\n        \"\"\"Is the error actually a timeout? Will raise a ReadTimeout or pass\"\"\"\n\n        if isinstance(err, SocketTimeout):\n            raise ReadTimeoutError(\n                self, url, f\"Read timed out. (read timeout={timeout_value})\"\n            ) from err\n\n        # See the above comment about EAGAIN in Python 3.\n        if hasattr(err, \"errno\") and err.errno in _blocking_errnos:\n            raise ReadTimeoutError(\n                self, url, f\"Read timed out. (read timeout={timeout_value})\"\n            ) from err\n\n    def _make_request(\n        self,\n        conn: BaseHTTPConnection,\n        method: str,\n        url: str,\n        body: _TYPE_BODY | None = None,\n        headers: typing.Mapping[str, str] | None = None,\n        retries: Retry | None = None,\n        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,\n        chunked: bool = False,\n        response_conn: BaseHTTPConnection | None = None,\n        preload_content: bool = True,\n        decode_content: bool = True,\n        enforce_content_length: bool = True,\n    ) -> BaseHTTPResponse:\n        \"\"\"\n        Perform a request on a given urllib connection object taken from our\n        pool.\n\n        :param conn:\n            a connection from one of our connection pools\n\n        :param method:\n            HTTP request method (such as GET, POST, PUT, etc.)\n\n        :param url:\n            The URL to perform the request on.\n\n        :param body:\n            Data to send in the request body, either :class:`str`, :class:`bytes`,\n            an iterable of :class:`str`/:class:`bytes`, or a file-like object.\n\n        :param headers:\n            Dictionary of custom headers to send, such as User-Agent,\n            If-None-Match, etc. If None, pool headers are used. If provided,\n            these headers completely replace any pool-specific headers.\n\n        :param retries:\n            Configure the number of retries to allow before raising a\n            :class:`~urllib3.exceptions.MaxRetryError` exception.\n\n            Pass ``None`` to retry until you receive a response. Pass a\n            :class:`~urllib3.util.retry.Retry` object for fine-grained control\n            over different types of retries.\n            Pass an integer number to retry connection errors that many times,\n            but no other types of errors. Pass zero to never retry.\n\n            If ``False``, then retries are disabled and any exception is raised\n            immediately. Also, instead of raising a MaxRetryError on redirects,\n            the redirect response will be returned.\n\n        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.\n\n        :param timeout:\n            If specified, overrides the default timeout for this one\n            request. It may be a float (in seconds) or an instance of\n            :class:`urllib3.util.Timeout`.\n\n        :param chunked:\n            If True, urllib3 will send the body using chunked transfer\n            encoding. Otherwise, urllib3 will send the body using the standard\n            content-length form. Defaults to False.\n\n        :param response_conn:\n            Set this to ``None`` if you will handle releasing the connection or\n            set the connection to have the response release it.\n\n        :param preload_content:\n          If True, the response's body will be preloaded during construction.\n\n        :param decode_content:\n            If True, will attempt to decode the body based on the\n            'content-encoding' header.\n\n        :param enforce_content_length:\n            Enforce content length checking. Body returned by server must match\n            value of Content-Length header, if present. Otherwise, raise error.\n        \"\"\"\n        self.num_requests += 1\n\n        timeout_obj = self._get_timeout(timeout)\n        timeout_obj.start_connect()\n        conn.timeout = Timeout.resolve_default_timeout(timeout_obj.connect_timeout)\n\n        try:\n            # Trigger any extra validation we need to do.\n            try:\n                self._validate_conn(conn)\n            except (SocketTimeout, BaseSSLError) as e:\n                self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n                raise\n\n        # _validate_conn() starts the connection to an HTTPS proxy\n        # so we need to wrap errors with 'ProxyError' here too.\n        except (\n            OSError,\n            NewConnectionError,\n            TimeoutError,\n            BaseSSLError,\n            CertificateError,\n            SSLError,\n        ) as e:\n            new_e: Exception = e\n            if isinstance(e, (BaseSSLError, CertificateError)):\n                new_e = SSLError(e)\n            # If the connection didn't successfully connect to it's proxy\n            # then there\n            if isinstance(\n                new_e, (OSError, NewConnectionError, TimeoutError, SSLError)\n            ) and (conn and conn.proxy and not conn.has_connected_to_proxy):\n                new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n            raise new_e\n\n        # conn.request() calls http.client.*.request, not the method in\n        # urllib3.request. It also calls makefile (recv) on the socket.\n        try:\n            conn.request(\n                method,\n                url,\n                body=body,\n                headers=headers,\n                chunked=chunked,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                enforce_content_length=enforce_content_length,\n            )\n\n        # We are swallowing BrokenPipeError (errno.EPIPE) since the server is\n        # legitimately able to close the connection after sending a valid response.\n        # With this behaviour, the received response is still readable.\n        except BrokenPipeError:\n            pass\n        except OSError as e:\n            # MacOS/Linux\n            # EPROTOTYPE is needed on macOS\n            # https://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug/\n            if e.errno != errno.EPROTOTYPE:\n                raise\n\n        # Reset the timeout for the recv() on the socket\n        read_timeout = timeout_obj.read_timeout\n\n        if not conn.is_closed:\n            # In Python 3 socket.py will catch EAGAIN and return None when you\n            # try and read into the file pointer created by http.client, which\n            # instead raises a BadStatusLine exception. Instead of catching\n            # the exception and assuming all BadStatusLine exceptions are read\n            # timeouts, check for a zero timeout before making the request.\n            if read_timeout == 0:\n                raise ReadTimeoutError(\n                    self, url, f\"Read timed out. (read timeout={read_timeout})\"\n                )\n            conn.timeout = read_timeout\n\n        # Receive the response from the server\n        try:\n            response = conn.getresponse()\n        except (BaseSSLError, OSError) as e:\n            self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n            raise\n\n        # Set properties that are used by the pooling layer.\n        response.retries = retries\n        response._connection = response_conn  # type: ignore[attr-defined]\n        response._pool = self  # type: ignore[attr-defined]\n\n        log.debug(\n            '%s://%s:%s \"%s %s %s\" %s %s',\n            self.scheme,\n            self.host,\n            self.port,\n            method,\n            url,\n            # HTTP version\n            conn._http_vsn_str,  # type: ignore[attr-defined]\n            response.status,\n            response.length_remaining,  # type: ignore[attr-defined]\n        )\n\n        return response\n\n    def close(self) -> None:\n        \"\"\"\n        Close all pooled connections and disable the pool.\n        \"\"\"\n        if self.pool is None:\n            return\n        # Disable access to the pool\n        old_pool, self.pool = self.pool, None\n\n        # Close all the HTTPConnections in the pool.\n        _close_pool_connections(old_pool)\n\n    def is_same_host(self, url: str) -> bool:\n        \"\"\"\n        Check if the given ``url`` is a member of the same host as this\n        connection pool.\n        \"\"\"\n        if url.startswith(\"/\"):\n            return True\n\n        # TODO: Add optional support for socket.gethostbyname checking.\n        scheme, _, host, port, *_ = parse_url(url)\n        scheme = scheme or \"http\"\n        if host is not None:\n            host = _normalize_host(host, scheme=scheme)\n\n        # Use explicit default port for comparison when none is given\n        if self.port and not port:\n            port = port_by_scheme.get(scheme)\n        elif not self.port and port == port_by_scheme.get(scheme):\n            port = None\n\n        return (scheme, host, port) == (self.scheme, self.host, self.port)\n\n    def urlopen(  # type: ignore[override]\n        self,\n        method: str,\n        url: str,\n        body: _TYPE_BODY | None = None,\n        headers: typing.Mapping[str, str] | None = None,\n        retries: Retry | bool | int | None = None,\n        redirect: bool = True,\n        assert_same_host: bool = True,\n        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,\n        pool_timeout: int | None = None,\n        release_conn: bool | None = None,\n        chunked: bool = False,\n        body_pos: _TYPE_BODY_POSITION | None = None,\n        preload_content: bool = True,\n        decode_content: bool = True,\n        **response_kw: typing.Any,\n    ) -> BaseHTTPResponse:\n        \"\"\"\n        Get a connection from the pool and perform an HTTP request. This is the\n        lowest level call for making a request, so you'll need to specify all\n        the raw details.\n\n        .. note::\n\n           More commonly, it's appropriate to use a convenience method\n           such as :meth:`request`.\n\n        .. note::\n\n           `release_conn` will only behave as expected if\n           `preload_content=False` because we want to make\n           `preload_content=False` the default behaviour someday soon without\n           breaking backwards compatibility.\n\n        :param method:\n            HTTP request method (such as GET, POST, PUT, etc.)\n\n        :param url:\n            The URL to perform the request on.\n\n        :param body:\n            Data to send in the request body, either :class:`str`, :class:`bytes`,\n            an iterable of :class:`str`/:class:`bytes`, or a file-like object.\n\n        :param headers:\n            Dictionary of custom headers to send, such as User-Agent,\n            If-None-Match, etc. If None, pool headers are used. If provided,\n            these headers completely replace any pool-specific headers.\n\n        :param retries:\n            Configure the number of retries to allow before raising a\n            :class:`~urllib3.exceptions.MaxRetryError` exception.\n\n            Pass ``None`` to retry until you receive a response. Pass a\n            :class:`~urllib3.util.retry.Retry` object for fine-grained control\n            over different types of retries.\n            Pass an integer number to retry connection errors that many times,\n            but no other types of errors. Pass zero to never retry.\n\n            If ``False``, then retries are disabled and any exception is raised\n            immediately. Also, instead of raising a MaxRetryError on redirects,\n            the redirect response will be returned.\n\n        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.\n\n        :param redirect:\n            If True, automatically handle redirects (status codes 301, 302,\n            303, 307, 308). Each redirect counts as a retry. Disabling retries\n            will disable redirect, too.\n\n        :param assert_same_host:\n            If ``True``, will make sure that the host of the pool requests is\n            consistent else will raise HostChangedError. When ``False``, you can\n            use the pool on an HTTP proxy and request foreign hosts.\n\n        :param timeout:\n            If specified, overrides the default timeout for this one\n            request. It may be a float (in seconds) or an instance of\n            :class:`urllib3.util.Timeout`.\n\n        :param pool_timeout:\n            If set and the pool is set to block=True, then this method will\n            block for ``pool_timeout`` seconds and raise EmptyPoolError if no\n            connection is available within the time period.\n\n        :param bool preload_content:\n            If True, the response's body will be preloaded into memory.\n\n        :param bool decode_content:\n            If True, will attempt to decode the body based on the\n            'content-encoding' header.\n\n        :param release_conn:\n            If False, then the urlopen call will not release the connection\n            back into the pool once a response is received (but will release if\n            you read the entire contents of the response such as when\n            `preload_content=True`). This is useful if you're not preloading\n            the response's content immediately. You will need to call\n            ``r.release_conn()`` on the response ``r`` to return the connection\n            back into the pool. If None, it takes the value of ``preload_content``\n            which defaults to ``True``.\n\n        :param bool chunked:\n            If True, urllib3 will send the body using chunked transfer\n            encoding. Otherwise, urllib3 will send the body using the standard\n            content-length form. Defaults to False.\n\n        :param int body_pos:\n            Position to seek to in file-like body in the event of a retry or\n            redirect. Typically this won't need to be set because urllib3 will\n            auto-populate the value when needed.\n        \"\"\"\n        parsed_url = parse_url(url)\n        destination_scheme = parsed_url.scheme\n\n        if headers is None:\n            headers = self.headers\n\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)\n\n        if release_conn is None:\n            release_conn = preload_content\n\n        # Check host\n        if assert_same_host and not self.is_same_host(url):\n            raise HostChangedError(self, url, retries)\n\n        # Ensure that the URL we're connecting to is properly encoded\n        if url.startswith(\"/\"):\n            url = to_str(_encode_target(url))\n        else:\n            url = to_str(parsed_url.url)\n\n        conn = None\n\n        # Track whether `conn` needs to be released before\n        # returning/raising/recursing. Update this variable if necessary, and\n        # leave `release_conn` constant throughout the function. That way, if\n        # the function recurses, the original value of `release_conn` will be\n        # passed down into the recursive call, and its value will be respected.\n        #\n        # See issue #651 [1] for details.\n        #\n        # [1] <https://github.com/urllib3/urllib3/issues/651>\n        release_this_conn = release_conn\n\n        http_tunnel_required = connection_requires_http_tunnel(\n            self.proxy, self.proxy_config, destination_scheme\n        )\n\n        # Merge the proxy headers. Only done when not using HTTP CONNECT. We\n        # have to copy the headers dict so we can safely change it without those\n        # changes being reflected in anyone else's copy.\n        if not http_tunnel_required:\n            headers = headers.copy()  # type: ignore[attr-defined]\n            headers.update(self.proxy_headers)  # type: ignore[union-attr]\n\n        # Must keep the exception bound to a separate variable or else Python 3\n        # complains about UnboundLocalError.\n        err = None\n\n        # Keep track of whether we cleanly exited the except block. This\n        # ensures we do proper cleanup in finally.\n        clean_exit = False\n\n        # Rewind body position, if needed. Record current position\n        # for future rewinds in the event of a redirect/retry.\n        body_pos = set_file_position(body, body_pos)\n\n        try:\n            # Request a connection from the queue.\n            timeout_obj = self._get_timeout(timeout)\n            conn = self._get_conn(timeout=pool_timeout)\n\n            conn.timeout = timeout_obj.connect_timeout  # type: ignore[assignment]\n\n            # Is this a closed/new connection that requires CONNECT tunnelling?\n            if self.proxy is not None and http_tunnel_required and conn.is_closed:\n                try:\n                    self._prepare_proxy(conn)\n                except (BaseSSLError, OSError, SocketTimeout) as e:\n                    self._raise_timeout(\n                        err=e, url=self.proxy.url, timeout_value=conn.timeout\n                    )\n                    raise\n\n            # If we're going to release the connection in ``finally:``, then\n            # the response doesn't need to know about the connection. Otherwise\n            # it will also try to release it and we'll have a double-release\n            # mess.\n            response_conn = conn if not release_conn else None\n\n            # Make the request on the HTTPConnection object\n            response = self._make_request(\n                conn,\n                method,\n                url,\n                timeout=timeout_obj,\n                body=body,\n                headers=headers,\n                chunked=chunked,\n                retries=retries,\n                response_conn=response_conn,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                **response_kw,\n            )\n\n            # Everything went great!\n            clean_exit = True\n\n        except EmptyPoolError:\n            # Didn't get a connection from the pool, no need to clean up\n            clean_exit = True\n            release_this_conn = False\n            raise\n\n        except (\n            TimeoutError,\n            HTTPException,\n            OSError,\n            ProtocolError,\n            BaseSSLError,\n            SSLError,\n            CertificateError,\n            ProxyError,\n        ) as e:\n            # Discard the connection for these exceptions. It will be\n            # replaced during the next _get_conn() call.\n            clean_exit = False\n            new_e: Exception = e\n            if isinstance(e, (BaseSSLError, CertificateError)):\n                new_e = SSLError(e)\n            if isinstance(\n                new_e,\n                (\n                    OSError,\n                    NewConnectionError,\n                    TimeoutError,\n                    SSLError,\n                    HTTPException,\n                ),\n            ) and (conn and conn.proxy and not conn.has_connected_to_proxy):\n                new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n            elif isinstance(new_e, (OSError, HTTPException)):\n                new_e = ProtocolError(\"Connection aborted.\", new_e)\n\n            retries = retries.increment(\n                method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]\n            )\n            retries.sleep()\n\n            # Keep track of the error for the retry warning.\n            err = e\n\n        finally:\n            if not clean_exit:\n                # We hit some kind of exception, handled or otherwise. We need\n                # to throw the connection away unless explicitly told not to.\n                # Close the connection, set the variable to None, and make sure\n                # we put the None back in the pool to avoid leaking it.\n                if conn:\n                    conn.close()\n                    conn = None\n                release_this_conn = True\n\n            if release_this_conn:\n                # Put the connection back to be reused. If the connection is\n                # expired then it will be None, which will get replaced with a\n                # fresh connection during _get_conn.\n                self._put_conn(conn)\n\n        if not conn:\n            # Try again\n            log.warning(\n                \"Retrying (%r) after connection broken by '%r': %s\", retries, err, url\n            )\n            return self.urlopen(\n                method,\n                url,\n                body,\n                headers,\n                retries,\n                redirect,\n                assert_same_host,\n                timeout=timeout,\n                pool_timeout=pool_timeout,\n                release_conn=release_conn,\n                chunked=chunked,\n                body_pos=body_pos,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                **response_kw,\n            )\n\n        # Handle redirect?\n        redirect_location = redirect and response.get_redirect_location()\n        if redirect_location:\n            if response.status == 303:\n                # Change the method according to RFC 9110, Section 15.4.4.\n                method = \"GET\"\n                # And lose the body not to transfer anything sensitive.\n                body = None\n                headers = HTTPHeaderDict(headers)._prepare_for_method_change()\n\n            try:\n                retries = retries.increment(method, url, response=response, _pool=self)\n            except MaxRetryError:\n                if retries.raise_on_redirect:\n                    response.drain_conn()\n                    raise\n                return response\n\n            response.drain_conn()\n            retries.sleep_for_retry(response)\n            log.debug(\"Redirecting %s -> %s\", url, redirect_location)\n            return self.urlopen(\n                method,\n                redirect_location,\n                body,\n                headers,\n                retries=retries,\n                redirect=redirect,\n                assert_same_host=assert_same_host,\n                timeout=timeout,\n                pool_timeout=pool_timeout,\n                release_conn=release_conn,\n                chunked=chunked,\n                body_pos=body_pos,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                **response_kw,\n            )\n\n        # Check if we should retry the HTTP response.\n        has_retry_after = bool(response.headers.get(\"Retry-After\"))\n        if retries.is_retry(method, response.status, has_retry_after):\n            try:\n                retries = retries.increment(method, url, response=response, _pool=self)\n            except MaxRetryError:\n                if retries.raise_on_status:\n                    response.drain_conn()\n                    raise\n                return response\n\n            response.drain_conn()\n            retries.sleep(response)\n            log.debug(\"Retry: %s\", url)\n            return self.urlopen(\n                method,\n                url,\n                body,\n                headers,\n                retries=retries,\n                redirect=redirect,\n                assert_same_host=assert_same_host,\n                timeout=timeout,\n                pool_timeout=pool_timeout,\n                release_conn=release_conn,\n                chunked=chunked,\n                body_pos=body_pos,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                **response_kw,\n            )\n\n        return response\n\n\nclass HTTPSConnectionPool(HTTPConnectionPool):\n    \"\"\"\n    Same as :class:`.HTTPConnectionPool`, but HTTPS.\n\n    :class:`.HTTPSConnection` uses one of ``assert_fingerprint``,\n    ``assert_hostname`` and ``host`` in this order to verify connections.\n    If ``assert_hostname`` is False, no verification is done.\n\n    The ``key_file``, ``cert_file``, ``cert_reqs``, ``ca_certs``,\n    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`\n    is available and are fed into :meth:`urllib3.util.ssl_wrap_socket` to upgrade\n    the connection socket into an SSL socket.\n    \"\"\"\n\n    scheme = \"https\"\n    ConnectionCls: type[BaseHTTPSConnection] = HTTPSConnection\n\n    def __init__(\n        self,\n        host: str,\n        port: int | None = None,\n        timeout: _TYPE_TIMEOUT | None = _DEFAULT_TIMEOUT,\n        maxsize: int = 1,\n        block: bool = False,\n        headers: typing.Mapping[str, str] | None = None,\n        retries: Retry | bool | int | None = None,\n        _proxy: Url | None = None,\n        _proxy_headers: typing.Mapping[str, str] | None = None,\n        key_file: str | None = None,\n        cert_file: str | None = None,\n        cert_reqs: int | str | None = None,\n        key_password: str | None = None,\n        ca_certs: str | None = None,\n        ssl_version: int | str | None = None,\n        ssl_minimum_version: ssl.TLSVersion | None = None,\n        ssl_maximum_version: ssl.TLSVersion | None = None,\n        assert_hostname: str | Literal[False] | None = None,\n        assert_fingerprint: str | None = None,\n        ca_cert_dir: str | None = None,\n        **conn_kw: typing.Any,\n    ) -> None:\n        super().__init__(\n            host,\n            port,\n            timeout,\n            maxsize,\n            block,\n            headers,\n            retries,\n            _proxy,\n            _proxy_headers,\n            **conn_kw,\n        )\n\n        self.key_file = key_file\n        self.cert_file = cert_file\n        self.cert_reqs = cert_reqs\n        self.key_password = key_password\n        self.ca_certs = ca_certs\n        self.ca_cert_dir = ca_cert_dir\n        self.ssl_version = ssl_version\n        self.ssl_minimum_version = ssl_minimum_version\n        self.ssl_maximum_version = ssl_maximum_version\n        self.assert_hostname = assert_hostname\n        self.assert_fingerprint = assert_fingerprint\n\n    def _prepare_proxy(self, conn: HTTPSConnection) -> None:  # type: ignore[override]\n        \"\"\"Establishes a tunnel connection through HTTP CONNECT.\"\"\"\n        if self.proxy and self.proxy.scheme == \"https\":\n            tunnel_scheme = \"https\"\n        else:\n            tunnel_scheme = \"http\"\n\n        conn.set_tunnel(\n            scheme=tunnel_scheme,\n            host=self._tunnel_host,\n            port=self.port,\n            headers=self.proxy_headers,\n        )\n        conn.connect()\n\n    def _new_conn(self) -> BaseHTTPSConnection:\n        \"\"\"\n        Return a fresh :class:`urllib3.connection.HTTPConnection`.\n        \"\"\"\n        self.num_connections += 1\n        log.debug(\n            \"Starting new HTTPS connection (%d): %s:%s\",\n            self.num_connections,\n            self.host,\n            self.port or \"443\",\n        )\n\n        if not self.ConnectionCls or self.ConnectionCls is DummyConnection:  # type: ignore[comparison-overlap]\n            raise ImportError(\n                \"Can't connect to HTTPS URL because the SSL module is not available.\"\n            )\n\n        actual_host: str = self.host\n        actual_port = self.port\n        if self.proxy is not None and self.proxy.host is not None:\n            actual_host = self.proxy.host\n            actual_port = self.proxy.port\n\n        return self.ConnectionCls(\n            host=actual_host,\n            port=actual_port,\n            timeout=self.timeout.connect_timeout,\n            cert_file=self.cert_file,\n            key_file=self.key_file,\n            key_password=self.key_password,\n            cert_reqs=self.cert_reqs,\n            ca_certs=self.ca_certs,\n            ca_cert_dir=self.ca_cert_dir,\n            assert_hostname=self.assert_hostname,\n            assert_fingerprint=self.assert_fingerprint,\n            ssl_version=self.ssl_version,\n            ssl_minimum_version=self.ssl_minimum_version,\n            ssl_maximum_version=self.ssl_maximum_version,\n            **self.conn_kw,\n        )\n\n    def _validate_conn(self, conn: BaseHTTPConnection) -> None:\n        \"\"\"\n        Called right before a request is made, after the socket is created.\n        \"\"\"\n        super()._validate_conn(conn)\n\n        # Force connect early to allow us to validate the connection.\n        if conn.is_closed:\n            conn.connect()\n\n        if not conn.is_verified:\n            warnings.warn(\n                (\n                    f\"Unverified HTTPS request is being made to host '{conn.host}'. \"\n                    \"Adding certificate verification is strongly advised. See: \"\n                    \"https://urllib3.readthedocs.io/en/latest/advanced-usage.html\"\n                    \"#tls-warnings\"\n                ),\n                InsecureRequestWarning,\n            )\n\n\ndef connection_from_url(url: str, **kw: typing.Any) -> HTTPConnectionPool:\n    \"\"\"\n    Given a url, return an :class:`.ConnectionPool` instance of its host.\n\n    This is a shortcut for not having to parse out the scheme, host, and port\n    of the url before creating an :class:`.ConnectionPool` instance.\n\n    :param url:\n        Absolute URL string that must include the scheme. Port is optional.\n\n    :param \\\\**kw:\n        Passes additional parameters to the constructor of the appropriate\n        :class:`.ConnectionPool`. Useful for specifying things like\n        timeout, maxsize, headers, etc.\n\n    Example::\n\n        >>> conn = connection_from_url('http://google.com/')\n        >>> r = conn.request('GET', '/')\n    \"\"\"\n    scheme, _, host, port, *_ = parse_url(url)\n    scheme = scheme or \"http\"\n    port = port or port_by_scheme.get(scheme, 80)\n    if scheme == \"https\":\n        return HTTPSConnectionPool(host, port=port, **kw)  # type: ignore[arg-type]\n    else:\n        return HTTPConnectionPool(host, port=port, **kw)  # type: ignore[arg-type]\n\n\n@typing.overload\ndef _normalize_host(host: None, scheme: str | None) -> None:\n    ...\n\n\n@typing.overload\ndef _normalize_host(host: str, scheme: str | None) -> str:\n    ...\n\n\ndef _normalize_host(host: str | None, scheme: str | None) -> str | None:\n    \"\"\"\n    Normalize hosts for comparisons and use with sockets.\n    \"\"\"\n\n    host = normalize_host(host, scheme)\n\n    # httplib doesn't like it when we include brackets in IPv6 addresses\n    # Specifically, if we include brackets but also pass the port then\n    # httplib crazily doubles up the square brackets on the Host header.\n    # Instead, we need to make sure we never pass ``None`` as the port.\n    # However, for backward compatibility reasons we can't actually\n    # *assert* that.  See http://bugs.python.org/issue28539\n    if host and host.startswith(\"[\") and host.endswith(\"]\"):\n        host = host[1:-1]\n    return host\n\n\ndef _url_from_pool(\n    pool: HTTPConnectionPool | HTTPSConnectionPool, path: str | None = None\n) -> str:\n    \"\"\"Returns the URL from a given connection pool. This is mainly used for testing and logging.\"\"\"\n    return Url(scheme=pool.scheme, host=pool.host, port=pool.port, path=path).url\n\n\ndef _close_pool_connections(pool: queue.LifoQueue[typing.Any]) -> None:\n    \"\"\"Drains a queue of connections and closes each one.\"\"\"\n    try:\n        while True:\n            conn = pool.get(block=False)\n            if conn:\n                conn.close()\n    except queue.Empty:\n        pass  # Done.\n", "from __future__ import annotations\n\nimport functools\nimport logging\nimport typing\nimport warnings\nfrom types import TracebackType\nfrom urllib.parse import urljoin\n\nfrom ._collections import HTTPHeaderDict, RecentlyUsedContainer\nfrom ._request_methods import RequestMethods\nfrom .connection import ProxyConfig\nfrom .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, port_by_scheme\nfrom .exceptions import (\n    LocationValueError,\n    MaxRetryError,\n    ProxySchemeUnknown,\n    URLSchemeUnknown,\n)\nfrom .response import BaseHTTPResponse\nfrom .util.connection import _TYPE_SOCKET_OPTIONS\nfrom .util.proxy import connection_requires_http_tunnel\nfrom .util.retry import Retry\nfrom .util.timeout import Timeout\nfrom .util.url import Url, parse_url\n\nif typing.TYPE_CHECKING:\n    import ssl\n    from typing import Literal\n\n__all__ = [\"PoolManager\", \"ProxyManager\", \"proxy_from_url\"]\n\n\nlog = logging.getLogger(__name__)\n\nSSL_KEYWORDS = (\n    \"key_file\",\n    \"cert_file\",\n    \"cert_reqs\",\n    \"ca_certs\",\n    \"ssl_version\",\n    \"ssl_minimum_version\",\n    \"ssl_maximum_version\",\n    \"ca_cert_dir\",\n    \"ssl_context\",\n    \"key_password\",\n    \"server_hostname\",\n)\n# Default value for `blocksize` - a new parameter introduced to\n# http.client.HTTPConnection & http.client.HTTPSConnection in Python 3.7\n_DEFAULT_BLOCKSIZE = 16384\n\n_SelfT = typing.TypeVar(\"_SelfT\")\n\n\nclass PoolKey(typing.NamedTuple):\n    \"\"\"\n    All known keyword arguments that could be provided to the pool manager, its\n    pools, or the underlying connections.\n\n    All custom key schemes should include the fields in this key at a minimum.\n    \"\"\"\n\n    key_scheme: str\n    key_host: str\n    key_port: int | None\n    key_timeout: Timeout | float | int | None\n    key_retries: Retry | bool | int | None\n    key_block: bool | None\n    key_source_address: tuple[str, int] | None\n    key_key_file: str | None\n    key_key_password: str | None\n    key_cert_file: str | None\n    key_cert_reqs: str | None\n    key_ca_certs: str | None\n    key_ssl_version: int | str | None\n    key_ssl_minimum_version: ssl.TLSVersion | None\n    key_ssl_maximum_version: ssl.TLSVersion | None\n    key_ca_cert_dir: str | None\n    key_ssl_context: ssl.SSLContext | None\n    key_maxsize: int | None\n    key_headers: frozenset[tuple[str, str]] | None\n    key__proxy: Url | None\n    key__proxy_headers: frozenset[tuple[str, str]] | None\n    key__proxy_config: ProxyConfig | None\n    key_socket_options: _TYPE_SOCKET_OPTIONS | None\n    key__socks_options: frozenset[tuple[str, str]] | None\n    key_assert_hostname: bool | str | None\n    key_assert_fingerprint: str | None\n    key_server_hostname: str | None\n    key_blocksize: int | None\n\n\ndef _default_key_normalizer(\n    key_class: type[PoolKey], request_context: dict[str, typing.Any]\n) -> PoolKey:\n    \"\"\"\n    Create a pool key out of a request context dictionary.\n\n    According to RFC 3986, both the scheme and host are case-insensitive.\n    Therefore, this function normalizes both before constructing the pool\n    key for an HTTPS request. If you wish to change this behaviour, provide\n    alternate callables to ``key_fn_by_scheme``.\n\n    :param key_class:\n        The class to use when constructing the key. This should be a namedtuple\n        with the ``scheme`` and ``host`` keys at a minimum.\n    :type  key_class: namedtuple\n    :param request_context:\n        A dictionary-like object that contain the context for a request.\n    :type  request_context: dict\n\n    :return: A namedtuple that can be used as a connection pool key.\n    :rtype:  PoolKey\n    \"\"\"\n    # Since we mutate the dictionary, make a copy first\n    context = request_context.copy()\n    context[\"scheme\"] = context[\"scheme\"].lower()\n    context[\"host\"] = context[\"host\"].lower()\n\n    # These are both dictionaries and need to be transformed into frozensets\n    for key in (\"headers\", \"_proxy_headers\", \"_socks_options\"):\n        if key in context and context[key] is not None:\n            context[key] = frozenset(context[key].items())\n\n    # The socket_options key may be a list and needs to be transformed into a\n    # tuple.\n    socket_opts = context.get(\"socket_options\")\n    if socket_opts is not None:\n        context[\"socket_options\"] = tuple(socket_opts)\n\n    # Map the kwargs to the names in the namedtuple - this is necessary since\n    # namedtuples can't have fields starting with '_'.\n    for key in list(context.keys()):\n        context[\"key_\" + key] = context.pop(key)\n\n    # Default to ``None`` for keys missing from the context\n    for field in key_class._fields:\n        if field not in context:\n            context[field] = None\n\n    # Default key_blocksize to _DEFAULT_BLOCKSIZE if missing from the context\n    if context.get(\"key_blocksize\") is None:\n        context[\"key_blocksize\"] = _DEFAULT_BLOCKSIZE\n\n    return key_class(**context)\n\n\n#: A dictionary that maps a scheme to a callable that creates a pool key.\n#: This can be used to alter the way pool keys are constructed, if desired.\n#: Each PoolManager makes a copy of this dictionary so they can be configured\n#: globally here, or individually on the instance.\nkey_fn_by_scheme = {\n    \"http\": functools.partial(_default_key_normalizer, PoolKey),\n    \"https\": functools.partial(_default_key_normalizer, PoolKey),\n}\n\npool_classes_by_scheme = {\"http\": HTTPConnectionPool, \"https\": HTTPSConnectionPool}\n\n\nclass PoolManager(RequestMethods):\n    \"\"\"\n    Allows for arbitrary requests while transparently keeping track of\n    necessary connection pools for you.\n\n    :param num_pools:\n        Number of connection pools to cache before discarding the least\n        recently used pool.\n\n    :param headers:\n        Headers to include with all requests, unless other headers are given\n        explicitly.\n\n    :param \\\\**connection_pool_kw:\n        Additional parameters are used to create fresh\n        :class:`urllib3.connectionpool.ConnectionPool` instances.\n\n    Example:\n\n    .. code-block:: python\n\n        import urllib3\n\n        http = urllib3.PoolManager(num_pools=2)\n\n        resp1 = http.request(\"GET\", \"https://google.com/\")\n        resp2 = http.request(\"GET\", \"https://google.com/mail\")\n        resp3 = http.request(\"GET\", \"https://yahoo.com/\")\n\n        print(len(http.pools))\n        # 2\n\n    \"\"\"\n\n    proxy: Url | None = None\n    proxy_config: ProxyConfig | None = None\n\n    def __init__(\n        self,\n        num_pools: int = 10,\n        headers: typing.Mapping[str, str] | None = None,\n        **connection_pool_kw: typing.Any,\n    ) -> None:\n        super().__init__(headers)\n        self.connection_pool_kw = connection_pool_kw\n\n        self.pools: RecentlyUsedContainer[PoolKey, HTTPConnectionPool]\n        self.pools = RecentlyUsedContainer(num_pools)\n\n        # Locally set the pool classes and keys so other PoolManagers can\n        # override them.\n        self.pool_classes_by_scheme = pool_classes_by_scheme\n        self.key_fn_by_scheme = key_fn_by_scheme.copy()\n\n    def __enter__(self: _SelfT) -> _SelfT:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> Literal[False]:\n        self.clear()\n        # Return False to re-raise any potential exceptions\n        return False\n\n    def _new_pool(\n        self,\n        scheme: str,\n        host: str,\n        port: int,\n        request_context: dict[str, typing.Any] | None = None,\n    ) -> HTTPConnectionPool:\n        \"\"\"\n        Create a new :class:`urllib3.connectionpool.ConnectionPool` based on host, port, scheme, and\n        any additional pool keyword arguments.\n\n        If ``request_context`` is provided, it is provided as keyword arguments\n        to the pool class used. This method is used to actually create the\n        connection pools handed out by :meth:`connection_from_url` and\n        companion methods. It is intended to be overridden for customization.\n        \"\"\"\n        pool_cls: type[HTTPConnectionPool] = self.pool_classes_by_scheme[scheme]\n        if request_context is None:\n            request_context = self.connection_pool_kw.copy()\n\n        # Default blocksize to _DEFAULT_BLOCKSIZE if missing or explicitly\n        # set to 'None' in the request_context.\n        if request_context.get(\"blocksize\") is None:\n            request_context[\"blocksize\"] = _DEFAULT_BLOCKSIZE\n\n        # Although the context has everything necessary to create the pool,\n        # this function has historically only used the scheme, host, and port\n        # in the positional args. When an API change is acceptable these can\n        # be removed.\n        for key in (\"scheme\", \"host\", \"port\"):\n            request_context.pop(key, None)\n\n        if scheme == \"http\":\n            for kw in SSL_KEYWORDS:\n                request_context.pop(kw, None)\n\n        return pool_cls(host, port, **request_context)\n\n    def clear(self) -> None:\n        \"\"\"\n        Empty our store of pools and direct them all to close.\n\n        This will not affect in-flight connections, but they will not be\n        re-used after completion.\n        \"\"\"\n        self.pools.clear()\n\n    def connection_from_host(\n        self,\n        host: str | None,\n        port: int | None = None,\n        scheme: str | None = \"http\",\n        pool_kwargs: dict[str, typing.Any] | None = None,\n    ) -> HTTPConnectionPool:\n        \"\"\"\n        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the host, port, and scheme.\n\n        If ``port`` isn't given, it will be derived from the ``scheme`` using\n        ``urllib3.connectionpool.port_by_scheme``. If ``pool_kwargs`` is\n        provided, it is merged with the instance's ``connection_pool_kw``\n        variable and used to create the new connection pool, if one is\n        needed.\n        \"\"\"\n\n        if not host:\n            raise LocationValueError(\"No host specified.\")\n\n        request_context = self._merge_pool_kwargs(pool_kwargs)\n        request_context[\"scheme\"] = scheme or \"http\"\n        if not port:\n            port = port_by_scheme.get(request_context[\"scheme\"].lower(), 80)\n        request_context[\"port\"] = port\n        request_context[\"host\"] = host\n\n        return self.connection_from_context(request_context)\n\n    def connection_from_context(\n        self, request_context: dict[str, typing.Any]\n    ) -> HTTPConnectionPool:\n        \"\"\"\n        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the request context.\n\n        ``request_context`` must at least contain the ``scheme`` key and its\n        value must be a key in ``key_fn_by_scheme`` instance variable.\n        \"\"\"\n        if \"strict\" in request_context:\n            warnings.warn(\n                \"The 'strict' parameter is no longer needed on Python 3+. \"\n                \"This will raise an error in urllib3 v2.1.0.\",\n                DeprecationWarning,\n            )\n            request_context.pop(\"strict\")\n\n        scheme = request_context[\"scheme\"].lower()\n        pool_key_constructor = self.key_fn_by_scheme.get(scheme)\n        if not pool_key_constructor:\n            raise URLSchemeUnknown(scheme)\n        pool_key = pool_key_constructor(request_context)\n\n        return self.connection_from_pool_key(pool_key, request_context=request_context)\n\n    def connection_from_pool_key(\n        self, pool_key: PoolKey, request_context: dict[str, typing.Any]\n    ) -> HTTPConnectionPool:\n        \"\"\"\n        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the provided pool key.\n\n        ``pool_key`` should be a namedtuple that only contains immutable\n        objects. At a minimum it must have the ``scheme``, ``host``, and\n        ``port`` fields.\n        \"\"\"\n        with self.pools.lock:\n            # If the scheme, host, or port doesn't match existing open\n            # connections, open a new ConnectionPool.\n            pool = self.pools.get(pool_key)\n            if pool:\n                return pool\n\n            # Make a fresh ConnectionPool of the desired type\n            scheme = request_context[\"scheme\"]\n            host = request_context[\"host\"]\n            port = request_context[\"port\"]\n            pool = self._new_pool(scheme, host, port, request_context=request_context)\n            self.pools[pool_key] = pool\n\n        return pool\n\n    def connection_from_url(\n        self, url: str, pool_kwargs: dict[str, typing.Any] | None = None\n    ) -> HTTPConnectionPool:\n        \"\"\"\n        Similar to :func:`urllib3.connectionpool.connection_from_url`.\n\n        If ``pool_kwargs`` is not provided and a new pool needs to be\n        constructed, ``self.connection_pool_kw`` is used to initialize\n        the :class:`urllib3.connectionpool.ConnectionPool`. If ``pool_kwargs``\n        is provided, it is used instead. Note that if a new pool does not\n        need to be created for the request, the provided ``pool_kwargs`` are\n        not used.\n        \"\"\"\n        u = parse_url(url)\n        return self.connection_from_host(\n            u.host, port=u.port, scheme=u.scheme, pool_kwargs=pool_kwargs\n        )\n\n    def _merge_pool_kwargs(\n        self, override: dict[str, typing.Any] | None\n    ) -> dict[str, typing.Any]:\n        \"\"\"\n        Merge a dictionary of override values for self.connection_pool_kw.\n\n        This does not modify self.connection_pool_kw and returns a new dict.\n        Any keys in the override dictionary with a value of ``None`` are\n        removed from the merged dictionary.\n        \"\"\"\n        base_pool_kwargs = self.connection_pool_kw.copy()\n        if override:\n            for key, value in override.items():\n                if value is None:\n                    try:\n                        del base_pool_kwargs[key]\n                    except KeyError:\n                        pass\n                else:\n                    base_pool_kwargs[key] = value\n        return base_pool_kwargs\n\n    def _proxy_requires_url_absolute_form(self, parsed_url: Url) -> bool:\n        \"\"\"\n        Indicates if the proxy requires the complete destination URL in the\n        request.  Normally this is only needed when not using an HTTP CONNECT\n        tunnel.\n        \"\"\"\n        if self.proxy is None:\n            return False\n\n        return not connection_requires_http_tunnel(\n            self.proxy, self.proxy_config, parsed_url.scheme\n        )\n\n    def urlopen(  # type: ignore[override]\n        self, method: str, url: str, redirect: bool = True, **kw: typing.Any\n    ) -> BaseHTTPResponse:\n        \"\"\"\n        Same as :meth:`urllib3.HTTPConnectionPool.urlopen`\n        with custom cross-host redirect logic and only sends the request-uri\n        portion of the ``url``.\n\n        The given ``url`` parameter must be absolute, such that an appropriate\n        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.\n        \"\"\"\n        u = parse_url(url)\n\n        if u.scheme is None:\n            warnings.warn(\n                \"URLs without a scheme (ie 'https://') are deprecated and will raise an error \"\n                \"in a future version of urllib3. To avoid this DeprecationWarning ensure all URLs \"\n                \"start with 'https://' or 'http://'. Read more in this issue: \"\n                \"https://github.com/urllib3/urllib3/issues/2920\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n\n        kw[\"assert_same_host\"] = False\n        kw[\"redirect\"] = False\n\n        if \"headers\" not in kw:\n            kw[\"headers\"] = self.headers\n\n        if self._proxy_requires_url_absolute_form(u):\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n\n        # Support relative URLs for redirecting.\n        redirect_location = urljoin(url, redirect_location)\n\n        if response.status == 303:\n            # Change the method according to RFC 9110, Section 15.4.4.\n            method = \"GET\"\n            # And lose the body not to transfer anything sensitive.\n            kw[\"body\"] = None\n            kw[\"headers\"] = HTTPHeaderDict(kw[\"headers\"])._prepare_for_method_change()\n\n        retries = kw.get(\"retries\")\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n\n        # Strip headers marked as unsafe to forward to the redirected location.\n        # Check remove_headers_on_redirect to avoid a potential network call within\n        # conn.is_same_host() which may use socket.gethostbyname() in the future.\n        if retries.remove_headers_on_redirect and not conn.is_same_host(\n            redirect_location\n        ):\n            new_headers = kw[\"headers\"].copy()\n            for header in kw[\"headers\"]:\n                if header.lower() in retries.remove_headers_on_redirect:\n                    new_headers.pop(header, None)\n            kw[\"headers\"] = new_headers\n\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                response.drain_conn()\n                raise\n            return response\n\n        kw[\"retries\"] = retries\n        kw[\"redirect\"] = redirect\n\n        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n\n        response.drain_conn()\n        return self.urlopen(method, redirect_location, **kw)\n\n\nclass ProxyManager(PoolManager):\n    \"\"\"\n    Behaves just like :class:`PoolManager`, but sends all requests through\n    the defined proxy, using the CONNECT method for HTTPS URLs.\n\n    :param proxy_url:\n        The URL of the proxy to be used.\n\n    :param proxy_headers:\n        A dictionary containing headers that will be sent to the proxy. In case\n        of HTTP they are being sent with each request, while in the\n        HTTPS/CONNECT case they are sent only once. Could be used for proxy\n        authentication.\n\n    :param proxy_ssl_context:\n        The proxy SSL context is used to establish the TLS connection to the\n        proxy when using HTTPS proxies.\n\n    :param use_forwarding_for_https:\n        (Defaults to False) If set to True will forward requests to the HTTPS\n        proxy to be made on behalf of the client instead of creating a TLS\n        tunnel via the CONNECT method. **Enabling this flag means that request\n        and response headers and content will be visible from the HTTPS proxy**\n        whereas tunneling keeps request and response headers and content\n        private.  IP address, target hostname, SNI, and port are always visible\n        to an HTTPS proxy even when this flag is disabled.\n\n    :param proxy_assert_hostname:\n        The hostname of the certificate to verify against.\n\n    :param proxy_assert_fingerprint:\n        The fingerprint of the certificate to verify against.\n\n    Example:\n\n    .. code-block:: python\n\n        import urllib3\n\n        proxy = urllib3.ProxyManager(\"https://localhost:3128/\")\n\n        resp1 = proxy.request(\"GET\", \"https://google.com/\")\n        resp2 = proxy.request(\"GET\", \"https://httpbin.org/\")\n\n        print(len(proxy.pools))\n        # 1\n\n        resp3 = proxy.request(\"GET\", \"https://httpbin.org/\")\n        resp4 = proxy.request(\"GET\", \"https://twitter.com/\")\n\n        print(len(proxy.pools))\n        # 3\n\n    \"\"\"\n\n    def __init__(\n        self,\n        proxy_url: str,\n        num_pools: int = 10,\n        headers: typing.Mapping[str, str] | None = None,\n        proxy_headers: typing.Mapping[str, str] | None = None,\n        proxy_ssl_context: ssl.SSLContext | None = None,\n        use_forwarding_for_https: bool = False,\n        proxy_assert_hostname: None | str | Literal[False] = None,\n        proxy_assert_fingerprint: str | None = None,\n        **connection_pool_kw: typing.Any,\n    ) -> None:\n        if isinstance(proxy_url, HTTPConnectionPool):\n            str_proxy_url = f\"{proxy_url.scheme}://{proxy_url.host}:{proxy_url.port}\"\n        else:\n            str_proxy_url = proxy_url\n        proxy = parse_url(str_proxy_url)\n\n        if proxy.scheme not in (\"http\", \"https\"):\n            raise ProxySchemeUnknown(proxy.scheme)\n\n        if not proxy.port:\n            port = port_by_scheme.get(proxy.scheme, 80)\n            proxy = proxy._replace(port=port)\n\n        self.proxy = proxy\n        self.proxy_headers = proxy_headers or {}\n        self.proxy_ssl_context = proxy_ssl_context\n        self.proxy_config = ProxyConfig(\n            proxy_ssl_context,\n            use_forwarding_for_https,\n            proxy_assert_hostname,\n            proxy_assert_fingerprint,\n        )\n\n        connection_pool_kw[\"_proxy\"] = self.proxy\n        connection_pool_kw[\"_proxy_headers\"] = self.proxy_headers\n        connection_pool_kw[\"_proxy_config\"] = self.proxy_config\n\n        super().__init__(num_pools, headers, **connection_pool_kw)\n\n    def connection_from_host(\n        self,\n        host: str | None,\n        port: int | None = None,\n        scheme: str | None = \"http\",\n        pool_kwargs: dict[str, typing.Any] | None = None,\n    ) -> HTTPConnectionPool:\n        if scheme == \"https\":\n            return super().connection_from_host(\n                host, port, scheme, pool_kwargs=pool_kwargs\n            )\n\n        return super().connection_from_host(\n            self.proxy.host, self.proxy.port, self.proxy.scheme, pool_kwargs=pool_kwargs  # type: ignore[union-attr]\n        )\n\n    def _set_proxy_headers(\n        self, url: str, headers: typing.Mapping[str, str] | None = None\n    ) -> typing.Mapping[str, str]:\n        \"\"\"\n        Sets headers needed by proxies: specifically, the Accept and Host\n        headers. Only sets headers not provided by the user.\n        \"\"\"\n        headers_ = {\"Accept\": \"*/*\"}\n\n        netloc = parse_url(url).netloc\n        if netloc:\n            headers_[\"Host\"] = netloc\n\n        if headers:\n            headers_.update(headers)\n        return headers_\n\n    def urlopen(  # type: ignore[override]\n        self, method: str, url: str, redirect: bool = True, **kw: typing.Any\n    ) -> BaseHTTPResponse:\n        \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n        u = parse_url(url)\n        if not connection_requires_http_tunnel(self.proxy, self.proxy_config, u.scheme):\n            # For connections using HTTP CONNECT, httplib sets the necessary\n            # headers on the CONNECT to the proxy. If we're not using CONNECT,\n            # we'll definitely need to set 'Host' at the very least.\n            headers = kw.get(\"headers\", self.headers)\n            kw[\"headers\"] = self._set_proxy_headers(url, headers)\n\n        return super().urlopen(method, url, redirect=redirect, **kw)\n\n\ndef proxy_from_url(url: str, **kw: typing.Any) -> ProxyManager:\n    return ProxyManager(proxy_url=url, **kw)\n", "from __future__ import annotations\n\nimport io\nimport socket\nimport time\nimport typing\nimport warnings\nfrom test import LONG_TIMEOUT, SHORT_TIMEOUT\nfrom threading import Event\nfrom unittest import mock\nfrom urllib.parse import urlencode\n\nimport pytest\n\nfrom dummyserver.server import HAS_IPV6_AND_DNS, NoIPv6Warning\nfrom dummyserver.testcase import HTTPDummyServerTestCase, SocketDummyServerTestCase\nfrom urllib3 import HTTPConnectionPool, encode_multipart_formdata\nfrom urllib3._collections import HTTPHeaderDict\nfrom urllib3.connection import _get_default_user_agent\nfrom urllib3.exceptions import (\n    ConnectTimeoutError,\n    DecodeError,\n    EmptyPoolError,\n    MaxRetryError,\n    NameResolutionError,\n    NewConnectionError,\n    ReadTimeoutError,\n    UnrewindableBodyError,\n)\nfrom urllib3.fields import _TYPE_FIELD_VALUE_TUPLE\nfrom urllib3.util import SKIP_HEADER, SKIPPABLE_HEADERS\nfrom urllib3.util.retry import RequestHistory, Retry\nfrom urllib3.util.timeout import _TYPE_TIMEOUT, Timeout\n\nfrom .. import INVALID_SOURCE_ADDRESSES, TARPIT_HOST, VALID_SOURCE_ADDRESSES\nfrom ..port_helpers import find_unused_port\n\n\ndef wait_for_socket(ready_event: Event) -> None:\n    ready_event.wait()\n    ready_event.clear()\n\n\nclass TestConnectionPoolTimeouts(SocketDummyServerTestCase):\n    def test_timeout_float(self) -> None:\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=2)\n\n        with HTTPConnectionPool(self.host, self.port, retries=False) as pool:\n            wait_for_socket(ready_event)\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=SHORT_TIMEOUT)\n            block_event.set()  # Release block\n\n            # Shouldn't raise this time\n            wait_for_socket(ready_event)\n            block_event.set()  # Pre-release block\n            pool.request(\"GET\", \"/\", timeout=LONG_TIMEOUT)\n\n    def test_conn_closed(self) -> None:\n        block_event = Event()\n        self.start_basic_handler(block_send=block_event, num=1)\n\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=SHORT_TIMEOUT, retries=False\n        ) as pool:\n            conn = pool._get_conn()\n            pool._put_conn(conn)\n            try:\n                with pytest.raises(ReadTimeoutError):\n                    pool.urlopen(\"GET\", \"/\")\n                if not conn.is_closed:\n                    with pytest.raises(socket.error):\n                        conn.sock.recv(1024)  # type: ignore[attr-defined]\n            finally:\n                pool._put_conn(conn)\n\n            block_event.set()\n\n    def test_timeout(self) -> None:\n        # Requests should time out when expected\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=3)\n\n        # Pool-global timeout\n        short_timeout = Timeout(read=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=short_timeout, retries=False\n        ) as pool:\n            wait_for_socket(ready_event)\n            block_event.clear()\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n            block_event.set()  # Release request\n\n        # Request-specific timeouts should raise errors\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=short_timeout, retries=False\n        ) as pool:\n            wait_for_socket(ready_event)\n            now = time.time()\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=LONG_TIMEOUT)\n            delta = time.time() - now\n\n            message = \"timeout was pool-level SHORT_TIMEOUT rather than request-level LONG_TIMEOUT\"\n            assert delta >= LONG_TIMEOUT, message\n            block_event.set()  # Release request\n\n            # Timeout passed directly to request should raise a request timeout\n            wait_for_socket(ready_event)\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\", timeout=SHORT_TIMEOUT)\n            block_event.set()  # Release request\n\n    def test_connect_timeout(self) -> None:\n        url = \"/\"\n        host, port = TARPIT_HOST, 80\n        timeout = Timeout(connect=SHORT_TIMEOUT)\n\n        # Pool-global timeout\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            with pytest.raises(ConnectTimeoutError):\n                pool._make_request(conn, \"GET\", url)\n\n            # Retries\n            retries = Retry(connect=0)\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", url, retries=retries)\n\n        # Request-specific connection timeouts\n        big_timeout = Timeout(read=LONG_TIMEOUT, connect=LONG_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=big_timeout, retries=False) as pool:\n            conn = pool._get_conn()\n            with pytest.raises(ConnectTimeoutError):\n                pool._make_request(conn, \"GET\", url, timeout=timeout)\n\n            pool._put_conn(conn)\n            with pytest.raises(ConnectTimeoutError):\n                pool.request(\"GET\", url, timeout=timeout)\n\n    def test_total_applies_connect(self) -> None:\n        host, port = TARPIT_HOST, 80\n\n        timeout = Timeout(total=None, connect=SHORT_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                with pytest.raises(ConnectTimeoutError):\n                    pool._make_request(conn, \"GET\", \"/\")\n            finally:\n                conn.close()\n\n        timeout = Timeout(connect=3, read=5, total=SHORT_TIMEOUT)\n        with HTTPConnectionPool(host, port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                with pytest.raises(ConnectTimeoutError):\n                    pool._make_request(conn, \"GET\", \"/\")\n            finally:\n                conn.close()\n\n    def test_total_timeout(self) -> None:\n        block_event = Event()\n        ready_event = self.start_basic_handler(block_send=block_event, num=2)\n\n        wait_for_socket(ready_event)\n        # This will get the socket to raise an EAGAIN on the read\n        timeout = Timeout(connect=3, read=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=timeout, retries=False\n        ) as pool:\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n\n            block_event.set()\n            wait_for_socket(ready_event)\n            block_event.clear()\n\n        # The connect should succeed and this should hit the read timeout\n        timeout = Timeout(connect=3, read=5, total=SHORT_TIMEOUT)\n        with HTTPConnectionPool(\n            self.host, self.port, timeout=timeout, retries=False\n        ) as pool:\n            with pytest.raises(ReadTimeoutError):\n                pool.request(\"GET\", \"/\")\n\n    def test_create_connection_timeout(self) -> None:\n        self.start_basic_handler(block_send=Event(), num=0)  # needed for self.port\n\n        timeout = Timeout(connect=SHORT_TIMEOUT, total=LONG_TIMEOUT)\n        with HTTPConnectionPool(\n            TARPIT_HOST, self.port, timeout=timeout, retries=False\n        ) as pool:\n            conn = pool._new_conn()\n            with pytest.raises(ConnectTimeoutError):\n                conn.connect()\n\n\nclass TestConnectionPool(HTTPDummyServerTestCase):\n    def test_get(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/specific_method\", fields={\"method\": \"GET\"})\n            assert r.status == 200, r.data\n\n    def test_post_url(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/specific_method\", fields={\"method\": \"POST\"})\n            assert r.status == 200, r.data\n\n    def test_urlopen_put(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.urlopen(\"PUT\", \"/specific_method?method=PUT\")\n            assert r.status == 200, r.data\n\n    def test_wrong_specific_method(self) -> None:\n        # To make sure the dummy server is actually returning failed responses\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/specific_method\", fields={\"method\": \"POST\"})\n            assert r.status == 400, r.data\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/specific_method\", fields={\"method\": \"GET\"})\n            assert r.status == 400, r.data\n\n    def test_upload(self) -> None:\n        data = \"I'm in ur multipart form-data, hazing a cheezburgr\"\n        fields: dict[str, _TYPE_FIELD_VALUE_TUPLE] = {\n            \"upload_param\": \"filefield\",\n            \"upload_filename\": \"lolcat.txt\",\n            \"filefield\": (\"lolcat.txt\", data),\n        }\n        fields[\"upload_size\"] = len(data)  # type: ignore\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/upload\", fields=fields)\n            assert r.status == 200, r.data\n\n    def test_one_name_multiple_values(self) -> None:\n        fields = [(\"foo\", \"a\"), (\"foo\", \"b\")]\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # urlencode\n            r = pool.request(\"GET\", \"/echo\", fields=fields)\n            assert r.data == b\"foo=a&foo=b\"\n\n            # multipart\n            r = pool.request(\"POST\", \"/echo\", fields=fields)\n            assert r.data.count(b'name=\"foo\"') == 2\n\n    def test_request_method_body(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            body = b\"hi\"\n            r = pool.request(\"POST\", \"/echo\", body=body)\n            assert r.data == body\n\n            fields = [(\"hi\", \"hello\")]\n            with pytest.raises(TypeError):\n                pool.request(\"POST\", \"/echo\", body=body, fields=fields)\n\n    def test_unicode_upload(self) -> None:\n        fieldname = \"myfile\"\n        filename = \"\\xe2\\x99\\xa5.txt\"\n        data = \"\\xe2\\x99\\xa5\".encode()\n        size = len(data)\n\n        fields: dict[str, _TYPE_FIELD_VALUE_TUPLE] = {\n            \"upload_param\": fieldname,\n            \"upload_filename\": filename,\n            fieldname: (filename, data),\n        }\n        fields[\"upload_size\"] = size  # type: ignore\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"POST\", \"/upload\", fields=fields)\n            assert r.status == 200, r.data\n\n    def test_nagle(self) -> None:\n        \"\"\"Test that connections have TCP_NODELAY turned on\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            conn = pool._get_conn()\n            try:\n                pool._make_request(conn, \"GET\", \"/\")\n                tcp_nodelay_setting = conn.sock.getsockopt(  # type: ignore[attr-defined]\n                    socket.IPPROTO_TCP, socket.TCP_NODELAY\n                )\n                assert tcp_nodelay_setting\n            finally:\n                conn.close()\n\n    @pytest.mark.parametrize(\n        \"socket_options\",\n        [\n            [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)],\n            ((socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1),),\n        ],\n    )\n    def test_socket_options(self, socket_options: tuple[int, int, int]) -> None:\n        \"\"\"Test that connections accept socket options.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries to\n        # connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(\n            self.host,\n            self.port,\n            socket_options=socket_options,\n        ) as pool:\n            # Get the socket of a new connection.\n            s = pool._new_conn()._new_conn()  # type: ignore[attr-defined]\n            try:\n                using_keepalive = (\n                    s.getsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE) > 0\n                )\n                assert using_keepalive\n            finally:\n                s.close()\n\n    @pytest.mark.parametrize(\"socket_options\", [None, []])\n    def test_disable_default_socket_options(\n        self, socket_options: list[int] | None\n    ) -> None:\n        \"\"\"Test that passing None or empty list disables all socket options.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(\n            self.host, self.port, socket_options=socket_options\n        ) as pool:\n            s = pool._new_conn()._new_conn()  # type: ignore[attr-defined]\n            try:\n                using_nagle = s.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY) == 0\n                assert using_nagle\n            finally:\n                s.close()\n\n    def test_defaults_are_applied(self) -> None:\n        \"\"\"Test that modifying the default socket options works.\"\"\"\n        # This test needs to be here in order to be run. socket.create_connection actually tries\n        # to connect to the host provided so we need a dummyserver to be running.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # Get the HTTPConnection instance\n            conn = pool._new_conn()\n            try:\n                # Update the default socket options\n                assert conn.socket_options is not None\n                conn.socket_options += [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)]  # type: ignore[operator]\n                s = conn._new_conn()  # type: ignore[attr-defined]\n                nagle_disabled = (\n                    s.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY) > 0\n                )\n                using_keepalive = (\n                    s.getsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE) > 0\n                )\n                assert nagle_disabled\n                assert using_keepalive\n            finally:\n                conn.close()\n                s.close()\n\n    def test_connection_error_retries(self) -> None:\n        \"\"\"ECONNREFUSED error should raise a connection error, with retries\"\"\"\n        port = find_unused_port()\n        with HTTPConnectionPool(self.host, port) as pool:\n            with pytest.raises(MaxRetryError) as e:\n                pool.request(\"GET\", \"/\", retries=Retry(connect=3))\n            assert type(e.value.reason) == NewConnectionError\n\n    def test_timeout_success(self) -> None:\n        timeout = Timeout(connect=3, read=5, total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            pool.request(\"GET\", \"/\")\n            # This should not raise a \"Timeout already started\" error\n            pool.request(\"GET\", \"/\")\n\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            # This should also not raise a \"Timeout already started\" error\n            pool.request(\"GET\", \"/\")\n\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            pool.request(\"GET\", \"/\")\n\n    socket_timeout_reuse_testdata = pytest.mark.parametrize(\n        [\"timeout\", \"expect_settimeout_calls\"],\n        [\n            (1, (1, 1)),\n            (None, (None, None)),\n            (Timeout(read=4), (None, 4)),\n            (Timeout(read=4, connect=5), (5, 4)),\n            (Timeout(connect=6), (6, None)),\n        ],\n    )\n\n    @socket_timeout_reuse_testdata\n    def test_socket_timeout_updated_on_reuse_constructor(\n        self,\n        timeout: _TYPE_TIMEOUT,\n        expect_settimeout_calls: typing.Sequence[float | None],\n    ) -> None:\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            # Make a request to create a new connection.\n            pool.urlopen(\"GET\", \"/\")\n\n            # Grab the connection and mock the inner socket.\n            assert pool.pool is not None\n            conn = pool.pool.get_nowait()\n            conn_sock = mock.Mock(wraps=conn.sock)\n            conn.sock = conn_sock\n            pool._put_conn(conn)\n\n            # Assert that sock.settimeout() is called with the new connect timeout, then the read timeout.\n            pool.urlopen(\"GET\", \"/\", timeout=timeout)\n            conn_sock.settimeout.assert_has_calls(\n                [mock.call(x) for x in expect_settimeout_calls]\n            )\n\n    @socket_timeout_reuse_testdata\n    def test_socket_timeout_updated_on_reuse_parameter(\n        self,\n        timeout: _TYPE_TIMEOUT,\n        expect_settimeout_calls: typing.Sequence[float | None],\n    ) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # Make a request to create a new connection.\n            pool.urlopen(\"GET\", \"/\", timeout=LONG_TIMEOUT)\n\n            # Grab the connection and mock the inner socket.\n            assert pool.pool is not None\n            conn = pool.pool.get_nowait()\n            conn_sock = mock.Mock(wraps=conn.sock)\n            conn.sock = conn_sock\n            pool._put_conn(conn)\n\n            # Assert that sock.settimeout() is called with the new connect timeout, then the read timeout.\n            pool.urlopen(\"GET\", \"/\", timeout=timeout)\n            conn_sock.settimeout.assert_has_calls(\n                [mock.call(x) for x in expect_settimeout_calls]\n            )\n\n    def test_tunnel(self) -> None:\n        # note the actual httplib.py has no tests for this functionality\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                conn.set_tunnel(self.host, self.port)\n                with mock.patch.object(\n                    conn, \"_tunnel\", create=True, return_value=None\n                ) as conn_tunnel:\n                    pool._make_request(conn, \"GET\", \"/\")\n                conn_tunnel.assert_called_once_with()\n            finally:\n                conn.close()\n\n        # test that it's not called when tunnel is not set\n        timeout = Timeout(total=None)\n        with HTTPConnectionPool(self.host, self.port, timeout=timeout) as pool:\n            conn = pool._get_conn()\n            try:\n                with mock.patch.object(\n                    conn, \"_tunnel\", create=True, return_value=None\n                ) as conn_tunnel:\n                    pool._make_request(conn, \"GET\", \"/\")\n                assert not conn_tunnel.called\n            finally:\n                conn.close()\n\n    def test_redirect_relative_url_no_deprecation(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"error\", DeprecationWarning)\n                pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"})\n\n    def test_redirect(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, redirect=False)\n            assert r.status == 303\n\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"})\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_303_redirect_makes_request_lose_body(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\n                \"POST\",\n                \"/redirect\",\n                fields={\"target\": \"/headers_and_params\", \"status\": \"303 See Other\"},\n            )\n        data = response.json()\n        assert data[\"params\"] == {}\n        assert \"Content-Type\" not in HTTPHeaderDict(data[\"headers\"])\n\n    def test_bad_connect(self) -> None:\n        with HTTPConnectionPool(\"badhost.invalid\", self.port) as pool:\n            with pytest.raises(MaxRetryError) as e:\n                pool.request(\"GET\", \"/\", retries=5)\n            assert type(e.value.reason) == NameResolutionError\n\n    def test_keepalive(self) -> None:\n        with HTTPConnectionPool(self.host, self.port, block=True, maxsize=1) as pool:\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n\n            assert r.status == 200\n            assert pool.num_connections == 1\n            assert pool.num_requests == 2\n\n    def test_keepalive_close(self) -> None:\n        with HTTPConnectionPool(\n            self.host, self.port, block=True, maxsize=1, timeout=2\n        ) as pool:\n            r = pool.request(\n                \"GET\", \"/keepalive?close=1\", retries=0, headers={\"Connection\": \"close\"}\n            )\n\n            assert pool.num_connections == 1\n\n            # The dummyserver will have responded with Connection:close,\n            # and httplib will properly cleanup the socket.\n\n            # We grab the HTTPConnection object straight from the Queue,\n            # because _get_conn() is where the check & reset occurs\n            assert pool.pool is not None\n            conn = pool.pool.get()\n            assert conn.sock is None\n            pool._put_conn(conn)\n\n            # Now with keep-alive\n            r = pool.request(\n                \"GET\",\n                \"/keepalive?close=0\",\n                retries=0,\n                headers={\"Connection\": \"keep-alive\"},\n            )\n\n            # The dummyserver responded with Connection:keep-alive, the connection\n            # persists.\n            conn = pool.pool.get()\n            assert conn.sock is not None\n            pool._put_conn(conn)\n\n            # Another request asking the server to close the connection. This one\n            # should get cleaned up for the next request.\n            r = pool.request(\n                \"GET\", \"/keepalive?close=1\", retries=0, headers={\"Connection\": \"close\"}\n            )\n\n            assert r.status == 200\n\n            conn = pool.pool.get()\n            assert conn.sock is None\n            pool._put_conn(conn)\n\n            # Next request\n            r = pool.request(\"GET\", \"/keepalive?close=0\")\n\n    def test_post_with_urlencode(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"banana\": \"hammock\", \"lol\": \"cat\"}\n            r = pool.request(\"POST\", \"/echo\", fields=data, encode_multipart=False)\n            assert r.data.decode(\"utf-8\") == urlencode(data)\n\n    def test_post_with_multipart(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"banana\": \"hammock\", \"lol\": \"cat\"}\n            r = pool.request(\"POST\", \"/echo\", fields=data, encode_multipart=True)\n            body = r.data.split(b\"\\r\\n\")\n\n            encoded_data = encode_multipart_formdata(data)[0]\n            expected_body = encoded_data.split(b\"\\r\\n\")\n\n            # TODO: Get rid of extra parsing stuff when you can specify\n            # a custom boundary to encode_multipart_formdata\n            \"\"\"\n            We need to loop the return lines because a timestamp is attached\n            from within encode_multipart_formdata. When the server echos back\n            the data, it has the timestamp from when the data was encoded, which\n            is not equivalent to when we run encode_multipart_formdata on\n            the data again.\n            \"\"\"\n            for i, line in enumerate(body):\n                if line.startswith(b\"--\"):\n                    continue\n\n                assert body[i] == expected_body[i]\n\n    def test_post_with_multipart__iter__(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            data = {\"hello\": \"world\"}\n            r = pool.request(\n                \"POST\",\n                \"/echo\",\n                fields=data,\n                preload_content=False,\n                multipart_boundary=\"boundary\",\n                encode_multipart=True,\n            )\n\n            chunks = [chunk for chunk in r]\n            assert chunks == [\n                b\"--boundary\\r\\n\",\n                b'Content-Disposition: form-data; name=\"hello\"\\r\\n',\n                b\"\\r\\n\",\n                b\"world\\r\\n\",\n                b\"--boundary--\\r\\n\",\n            ]\n\n    def test_check_gzip(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\", \"/encodingrequest\", headers={\"accept-encoding\": \"gzip\"}\n            )\n            assert r.headers.get(\"content-encoding\") == \"gzip\"\n            assert r.data == b\"hello, world!\"\n\n    def test_check_deflate(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\", \"/encodingrequest\", headers={\"accept-encoding\": \"deflate\"}\n            )\n            assert r.headers.get(\"content-encoding\") == \"deflate\"\n            assert r.data == b\"hello, world!\"\n\n    def test_bad_decode(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with pytest.raises(DecodeError):\n                pool.request(\n                    \"GET\",\n                    \"/encodingrequest\",\n                    headers={\"accept-encoding\": \"garbage-deflate\"},\n                )\n\n            with pytest.raises(DecodeError):\n                pool.request(\n                    \"GET\",\n                    \"/encodingrequest\",\n                    headers={\"accept-encoding\": \"garbage-gzip\"},\n                )\n\n    def test_connection_count(self) -> None:\n        with HTTPConnectionPool(self.host, self.port, maxsize=1) as pool:\n            pool.request(\"GET\", \"/\")\n            pool.request(\"GET\", \"/\")\n            pool.request(\"GET\", \"/\")\n\n            assert pool.num_connections == 1\n            assert pool.num_requests == 3\n\n    def test_connection_count_bigpool(self) -> None:\n        with HTTPConnectionPool(self.host, self.port, maxsize=16) as http_pool:\n            http_pool.request(\"GET\", \"/\")\n            http_pool.request(\"GET\", \"/\")\n            http_pool.request(\"GET\", \"/\")\n\n            assert http_pool.num_connections == 1\n            assert http_pool.num_requests == 3\n\n    def test_partial_response(self) -> None:\n        with HTTPConnectionPool(self.host, self.port, maxsize=1) as pool:\n            req_data = {\"lol\": \"cat\"}\n            resp_data = urlencode(req_data).encode(\"utf-8\")\n\n            r = pool.request(\"GET\", \"/echo\", fields=req_data, preload_content=False)\n\n            assert r.read(5) == resp_data[:5]\n            assert r.read() == resp_data[5:]\n\n    def test_lazy_load_twice(self) -> None:\n        # This test is sad and confusing. Need to figure out what's\n        # going on with partial reads and socket reuse.\n\n        with HTTPConnectionPool(\n            self.host, self.port, block=True, maxsize=1, timeout=2\n        ) as pool:\n            payload_size = 1024 * 2\n            first_chunk = 512\n\n            boundary = \"foo\"\n\n            req_data = {\"count\": \"a\" * payload_size}\n            resp_data = encode_multipart_formdata(req_data, boundary=boundary)[0]\n\n            req2_data = {\"count\": \"b\" * payload_size}\n            resp2_data = encode_multipart_formdata(req2_data, boundary=boundary)[0]\n\n            r1 = pool.request(\n                \"POST\",\n                \"/echo\",\n                fields=req_data,\n                multipart_boundary=boundary,\n                preload_content=False,\n            )\n\n            assert r1.read(first_chunk) == resp_data[:first_chunk]\n\n            try:\n                r2 = pool.request(\n                    \"POST\",\n                    \"/echo\",\n                    fields=req2_data,\n                    multipart_boundary=boundary,\n                    preload_content=False,\n                    pool_timeout=0.001,\n                )\n\n                # This branch should generally bail here, but maybe someday it will\n                # work? Perhaps by some sort of magic. Consider it a TODO.\n\n                assert r2.read(first_chunk) == resp2_data[:first_chunk]\n\n                assert r1.read() == resp_data[first_chunk:]\n                assert r2.read() == resp2_data[first_chunk:]\n                assert pool.num_requests == 2\n\n            except EmptyPoolError:\n                assert r1.read() == resp_data[first_chunk:]\n                assert pool.num_requests == 1\n\n            assert pool.num_connections == 1\n\n    def test_for_double_release(self) -> None:\n        MAXSIZE = 5\n\n        # Check default state\n        with HTTPConnectionPool(self.host, self.port, maxsize=MAXSIZE) as pool:\n            assert pool.num_connections == 0\n            assert pool.pool is not None\n            assert pool.pool.qsize() == MAXSIZE\n\n            # Make an empty slot for testing\n            pool.pool.get()\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n            # Check state after simple request\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n            # Check state without release\n            pool.urlopen(\"GET\", \"/\", preload_content=False)\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            # Check state after read\n            pool.urlopen(\"GET\", \"/\").data\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n            pool.urlopen(\"GET\", \"/\")\n            assert pool.pool.qsize() == MAXSIZE - 2\n\n    def test_release_conn_parameter(self) -> None:\n        MAXSIZE = 5\n        with HTTPConnectionPool(self.host, self.port, maxsize=MAXSIZE) as pool:\n            assert pool.pool is not None\n            assert pool.pool.qsize() == MAXSIZE\n\n            # Make request without releasing connection\n            pool.request(\"GET\", \"/\", release_conn=False, preload_content=False)\n            assert pool.pool.qsize() == MAXSIZE - 1\n\n    def test_dns_error(self) -> None:\n        with HTTPConnectionPool(\n            \"thishostdoesnotexist.invalid\", self.port, timeout=0.001\n        ) as pool:\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", \"/test\", retries=2)\n\n    @pytest.mark.parametrize(\"char\", [\" \", \"\\r\", \"\\n\", \"\\x00\"])\n    def test_invalid_method_not_allowed(self, char: str) -> None:\n        with pytest.raises(ValueError):\n            with HTTPConnectionPool(self.host, self.port) as pool:\n                pool.request(\"GET\" + char, \"/\")\n\n    def test_percent_encode_invalid_target_chars(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/echo_params?q=\\r&k=\\n \\n\")\n            assert r.data == b\"[('k', '\\\\n \\\\n'), ('q', '\\\\r')]\"\n\n    def test_source_address(self) -> None:\n        for addr, is_ipv6 in VALID_SOURCE_ADDRESSES:\n            if is_ipv6 and not HAS_IPV6_AND_DNS:\n                warnings.warn(\"No IPv6 support: skipping.\", NoIPv6Warning)\n                continue\n            with HTTPConnectionPool(\n                self.host, self.port, source_address=addr, retries=False\n            ) as pool:\n                r = pool.request(\"GET\", \"/source_address\")\n                assert r.data == addr[0].encode()\n\n    @pytest.mark.parametrize(\n        \"invalid_source_address, is_ipv6\", INVALID_SOURCE_ADDRESSES\n    )\n    def test_source_address_error(\n        self, invalid_source_address: tuple[str, int], is_ipv6: bool\n    ) -> None:\n        with HTTPConnectionPool(\n            self.host, self.port, source_address=invalid_source_address, retries=False\n        ) as pool:\n            if is_ipv6:\n                with pytest.raises(NameResolutionError):\n                    pool.request(\"GET\", f\"/source_address?{invalid_source_address}\")\n            else:\n                with pytest.raises(NewConnectionError):\n                    pool.request(\"GET\", f\"/source_address?{invalid_source_address}\")\n\n    def test_stream_keepalive(self) -> None:\n        x = 2\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            for _ in range(x):\n                response = pool.request(\n                    \"GET\",\n                    \"/chunked\",\n                    headers={\"Connection\": \"keep-alive\"},\n                    preload_content=False,\n                    retries=False,\n                )\n                for chunk in response.stream():\n                    assert chunk == b\"123\"\n\n            assert pool.num_connections == 1\n            assert pool.num_requests == x\n\n    def test_read_chunked_short_circuit(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\"GET\", \"/chunked\", preload_content=False)\n            response.read()\n            with pytest.raises(StopIteration):\n                next(response.read_chunked())\n\n    def test_read_chunked_on_closed_response(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\"GET\", \"/chunked\", preload_content=False)\n            response.close()\n            with pytest.raises(StopIteration):\n                next(response.read_chunked())\n\n    def test_chunked_gzip(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\n                \"GET\", \"/chunked_gzip\", preload_content=False, decode_content=True\n            )\n\n            assert b\"123\" * 4 == response.read()\n\n    def test_cleanup_on_connection_error(self) -> None:\n        \"\"\"\n        Test that connections are recycled to the pool on\n        connection errors where no http response is received.\n        \"\"\"\n        poolsize = 3\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=poolsize, block=True\n        ) as http:\n            assert http.pool is not None\n            assert http.pool.qsize() == poolsize\n\n            # force a connection error by supplying a non-existent\n            # url. We won't get a response for this  and so the\n            # conn won't be implicitly returned to the pool.\n            with pytest.raises(MaxRetryError):\n                http.request(\n                    \"GET\",\n                    \"/redirect\",\n                    fields={\"target\": \"/\"},\n                    release_conn=False,\n                    retries=0,\n                )\n\n            r = http.request(\n                \"GET\",\n                \"/redirect\",\n                fields={\"target\": \"/\"},\n                release_conn=False,\n                retries=1,\n            )\n            r.release_conn()\n\n            # the pool should still contain poolsize elements\n            assert http.pool.qsize() == http.pool.maxsize\n\n    def test_mixed_case_hostname(self) -> None:\n        with HTTPConnectionPool(\"LoCaLhOsT\", self.port) as pool:\n            response = pool.request(\"GET\", f\"http://LoCaLhOsT:{self.port}/\")\n            assert response.status == 200\n\n    def test_preserves_path_dot_segments(self) -> None:\n        \"\"\"ConnectionPool preserves dot segments in the URI\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            response = pool.request(\"GET\", \"/echo_uri/seg0/../seg2\")\n            assert response.data == b\"/echo_uri/seg0/../seg2\"\n\n    def test_default_user_agent_header(self) -> None:\n        \"\"\"ConnectionPool has a default user agent\"\"\"\n        default_ua = _get_default_user_agent()\n        custom_ua = \"I'm not a web scraper, what are you talking about?\"\n        custom_ua2 = \"Yet Another User Agent\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # Use default user agent if no user agent was specified.\n            r = pool.request(\"GET\", \"/headers\")\n            request_headers = r.json()\n            assert request_headers.get(\"User-Agent\") == _get_default_user_agent()\n\n            # Prefer the request user agent over the default.\n            headers = {\"UsEr-AGENt\": custom_ua}\n            r = pool.request(\"GET\", \"/headers\", headers=headers)\n            request_headers = r.json()\n            assert request_headers.get(\"User-Agent\") == custom_ua\n\n            # Do not modify pool headers when using the default user agent.\n            pool_headers = {\"foo\": \"bar\"}\n            pool.headers = pool_headers\n            r = pool.request(\"GET\", \"/headers\")\n            request_headers = r.json()\n            assert request_headers.get(\"User-Agent\") == default_ua\n            assert \"User-Agent\" not in pool_headers\n\n            pool.headers.update({\"User-Agent\": custom_ua2})\n            r = pool.request(\"GET\", \"/headers\")\n            request_headers = r.json()\n            assert request_headers.get(\"User-Agent\") == custom_ua2\n\n    @pytest.mark.parametrize(\n        \"headers\",\n        [\n            None,\n            {},\n            {\"User-Agent\": \"key\"},\n            {\"user-agent\": \"key\"},\n            {b\"uSeR-AgEnT\": b\"key\"},\n            {b\"user-agent\": \"key\"},\n        ],\n    )\n    @pytest.mark.parametrize(\"chunked\", [True, False])\n    def test_user_agent_header_not_sent_twice(\n        self, headers: dict[str, str] | None, chunked: bool\n    ) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/headers\", headers=headers, chunked=chunked)\n            request_headers = r.json()\n\n            if not headers:\n                assert request_headers[\"User-Agent\"].startswith(\"python-urllib3/\")\n                assert \"key\" not in request_headers[\"User-Agent\"]\n            else:\n                assert request_headers[\"User-Agent\"] == \"key\"\n\n    def test_no_user_agent_header(self) -> None:\n        \"\"\"ConnectionPool can suppress sending a user agent header\"\"\"\n        custom_ua = \"I'm not a web scraper, what are you talking about?\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            # Suppress user agent in the request headers.\n            no_ua_headers = {\"User-Agent\": SKIP_HEADER}\n            r = pool.request(\"GET\", \"/headers\", headers=no_ua_headers)\n            request_headers = r.json()\n            assert \"User-Agent\" not in request_headers\n            assert no_ua_headers[\"User-Agent\"] == SKIP_HEADER\n\n            # Suppress user agent in the pool headers.\n            pool.headers = no_ua_headers\n            r = pool.request(\"GET\", \"/headers\")\n            request_headers = r.json()\n            assert \"User-Agent\" not in request_headers\n            assert no_ua_headers[\"User-Agent\"] == SKIP_HEADER\n\n            # Request headers override pool headers.\n            pool_headers = {\"User-Agent\": custom_ua}\n            pool.headers = pool_headers\n            r = pool.request(\"GET\", \"/headers\", headers=no_ua_headers)\n            request_headers = r.json()\n            assert \"User-Agent\" not in request_headers\n            assert no_ua_headers[\"User-Agent\"] == SKIP_HEADER\n            assert pool_headers.get(\"User-Agent\") == custom_ua\n\n    @pytest.mark.parametrize(\n        \"accept_encoding\",\n        [\n            \"Accept-Encoding\",\n            \"accept-encoding\",\n            b\"Accept-Encoding\",\n            b\"accept-encoding\",\n            None,\n        ],\n    )\n    @pytest.mark.parametrize(\"host\", [\"Host\", \"host\", b\"Host\", b\"host\", None])\n    @pytest.mark.parametrize(\n        \"user_agent\", [\"User-Agent\", \"user-agent\", b\"User-Agent\", b\"user-agent\", None]\n    )\n    @pytest.mark.parametrize(\"chunked\", [True, False])\n    def test_skip_header(\n        self,\n        accept_encoding: str | None,\n        host: str | None,\n        user_agent: str | None,\n        chunked: bool,\n    ) -> None:\n        headers = {}\n\n        if accept_encoding is not None:\n            headers[accept_encoding] = SKIP_HEADER\n        if host is not None:\n            headers[host] = SKIP_HEADER\n        if user_agent is not None:\n            headers[user_agent] = SKIP_HEADER\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/headers\", headers=headers, chunked=chunked)\n        request_headers = r.json()\n\n        if accept_encoding is None:\n            assert \"Accept-Encoding\" in request_headers\n        else:\n            assert accept_encoding not in request_headers\n        if host is None:\n            assert \"Host\" in request_headers\n        else:\n            assert host not in request_headers\n        if user_agent is None:\n            assert \"User-Agent\" in request_headers\n        else:\n            assert user_agent not in request_headers\n\n    @pytest.mark.parametrize(\"header\", [\"Content-Length\", \"content-length\"])\n    @pytest.mark.parametrize(\"chunked\", [True, False])\n    def test_skip_header_non_supported(self, header: str, chunked: bool) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with pytest.raises(\n                ValueError,\n                match=\"urllib3.util.SKIP_HEADER only supports 'Accept-Encoding', 'Host', 'User-Agent'\",\n            ) as e:\n                pool.request(\n                    \"GET\", \"/headers\", headers={header: SKIP_HEADER}, chunked=chunked\n                )\n            # Ensure that the error message stays up to date with 'SKIP_HEADER_SUPPORTED_HEADERS'\n            assert all(\n                (\"'\" + header.title() + \"'\") in str(e.value)\n                for header in SKIPPABLE_HEADERS\n            )\n\n    @pytest.mark.parametrize(\"chunked\", [True, False])\n    @pytest.mark.parametrize(\"pool_request\", [True, False])\n    @pytest.mark.parametrize(\"header_type\", [dict, HTTPHeaderDict])\n    def test_headers_not_modified_by_request(\n        self,\n        chunked: bool,\n        pool_request: bool,\n        header_type: type[dict[str, str] | HTTPHeaderDict],\n    ) -> None:\n        # Test that the .request*() methods of ConnectionPool and HTTPConnection\n        # don't modify the given 'headers' structure, instead they should\n        # make their own internal copies at request time.\n        headers = header_type()\n        headers[\"key\"] = \"val\"\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            pool.headers = headers\n            if pool_request:\n                pool.request(\"GET\", \"/headers\", chunked=chunked)\n            else:\n                conn = pool._get_conn()\n                conn.request(\"GET\", \"/headers\", chunked=chunked)\n\n            assert pool.headers == {\"key\": \"val\"}\n            assert isinstance(pool.headers, header_type)\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            if pool_request:\n                pool.request(\"GET\", \"/headers\", headers=headers, chunked=chunked)\n            else:\n                conn = pool._get_conn()\n                conn.request(\"GET\", \"/headers\", headers=headers, chunked=chunked)\n\n            assert headers == {\"key\": \"val\"}\n\n    def test_request_chunked_is_deprecated(\n        self,\n    ) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            conn = pool._get_conn()\n\n            with pytest.warns(DeprecationWarning) as w:\n                conn.request_chunked(\"GET\", \"/headers\")  # type: ignore[attr-defined]\n            assert len(w) == 1 and str(w[0].message) == (\n                \"HTTPConnection.request_chunked() is deprecated and will be removed in urllib3 v2.1.0. \"\n                \"Instead use HTTPConnection.request(..., chunked=True).\"\n            )\n\n            resp = conn.getresponse()\n            assert resp.status == 200\n            assert resp.json()[\"Transfer-Encoding\"] == \"chunked\"\n\n    def test_bytes_header(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"User-Agent\": \"test header\"}\n            r = pool.request(\"GET\", \"/headers\", headers=headers)\n            request_headers = r.json()\n            assert \"User-Agent\" in request_headers\n            assert request_headers[\"User-Agent\"] == \"test header\"\n\n    @pytest.mark.parametrize(\n        \"user_agent\", [\"Sch\u00f6nefeld/1.18.0\", \"Sch\u00f6nefeld/1.18.0\".encode(\"iso-8859-1\")]\n    )\n    def test_user_agent_non_ascii_user_agent(self, user_agent: str) -> None:\n        with HTTPConnectionPool(self.host, self.port, retries=False) as pool:\n            r = pool.urlopen(\n                \"GET\",\n                \"/headers\",\n                headers={\"User-Agent\": user_agent},\n            )\n            request_headers = r.json()\n            assert \"User-Agent\" in request_headers\n            assert request_headers[\"User-Agent\"] == \"Sch\u00f6nefeld/1.18.0\"\n\n\nclass TestRetry(HTTPDummyServerTestCase):\n    def test_max_retry(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            with pytest.raises(MaxRetryError):\n                pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, retries=0)\n\n    def test_disabled_retry(self) -> None:\n        \"\"\"Disabled retries should disable redirect handling.\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"}, retries=False)\n            assert r.status == 303\n\n            r = pool.request(\n                \"GET\",\n                \"/redirect\",\n                fields={\"target\": \"/\"},\n                retries=Retry(redirect=False),\n            )\n            assert r.status == 303\n\n        with HTTPConnectionPool(\n            \"thishostdoesnotexist.invalid\", self.port, timeout=0.001\n        ) as pool:\n            with pytest.raises(NameResolutionError):\n                pool.request(\"GET\", \"/test\", retries=False)\n\n    def test_read_retries(self) -> None:\n        \"\"\"Should retry for status codes in the forcelist\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(read=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_read_retries\"},\n                retries=retry,\n            )\n            assert resp.status == 200\n\n    def test_read_total_retries(self) -> None:\n        \"\"\"HTTP response w/ status code in the forcelist should be retried\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_read_total_retries\"}\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n    def test_retries_wrong_forcelist(self) -> None:\n        \"\"\"HTTP response w/ status code not in forcelist shouldn't be retried\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(total=1, status_forcelist=[202])\n            resp = pool.request(\n                \"GET\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_wrong_forcelist\"},\n                retries=retry,\n            )\n            assert resp.status == 418\n\n    def test_default_method_forcelist_retried(self) -> None:\n        \"\"\"urllib3 should retry methods in the default method forcelist\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"OPTIONS\",\n                \"/successful_retry\",\n                headers={\"test-name\": \"test_default_forcelist\"},\n                retries=retry,\n            )\n            assert resp.status == 200\n\n    def test_retries_wrong_method_list(self) -> None:\n        \"\"\"Method not in our allowed list should not be retried, even if code matches\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_wrong_allowed_method\"}\n            retry = Retry(total=1, status_forcelist=[418], allowed_methods=[\"POST\"])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 418\n\n    def test_read_retries_unsuccessful(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_read_retries_unsuccessful\"}\n            resp = pool.request(\"GET\", \"/successful_retry\", headers=headers, retries=1)\n            assert resp.status == 418\n\n    def test_retry_reuse_safe(self) -> None:\n        \"\"\"It should be possible to reuse a Retry object across requests\"\"\"\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_retry_safe\"}\n            retry = Retry(total=1, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n\n    def test_retry_return_in_response(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            headers = {\"test-name\": \"test_retry_return_in_response\"}\n            retry = Retry(total=2, status_forcelist=[418])\n            resp = pool.request(\n                \"GET\", \"/successful_retry\", headers=headers, retries=retry\n            )\n            assert resp.status == 200\n            assert resp.retries is not None\n            assert resp.retries.total == 1\n            assert resp.retries.history == (\n                RequestHistory(\"GET\", \"/successful_retry\", None, 418, None),\n            )\n\n    def test_retry_redirect_history(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            resp = pool.request(\"GET\", \"/redirect\", fields={\"target\": \"/\"})\n            assert resp.status == 200\n            assert resp.retries is not None\n            assert resp.retries.history == (\n                RequestHistory(\"GET\", \"/redirect?target=%2F\", None, 303, \"/\"),\n            )\n\n    def test_multi_redirect_history(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/multi_redirect\",\n                fields={\"redirect_codes\": \"303,302,200\"},\n                redirect=False,\n            )\n            assert r.status == 303\n            assert r.retries is not None\n            assert r.retries.history == tuple()\n\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/multi_redirect\",\n                retries=10,\n                fields={\"redirect_codes\": \"303,302,301,307,302,200\"},\n            )\n            assert r.status == 200\n            assert r.data == b\"Done redirecting\"\n\n            expected = [\n                (303, \"/multi_redirect?redirect_codes=302,301,307,302,200\"),\n                (302, \"/multi_redirect?redirect_codes=301,307,302,200\"),\n                (301, \"/multi_redirect?redirect_codes=307,302,200\"),\n                (307, \"/multi_redirect?redirect_codes=302,200\"),\n                (302, \"/multi_redirect?redirect_codes=200\"),\n            ]\n            assert r.retries is not None\n            actual = [\n                (history.status, history.redirect_location)\n                for history in r.retries.history\n            ]\n            assert actual == expected\n\n\nclass TestRetryAfter(HTTPDummyServerTestCase):\n    def test_retry_after(self) -> None:\n        # Request twice in a second to get a 429 response.\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=False,\n            )\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=False,\n            )\n            assert r.status == 429\n\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"429 Too Many Requests\"},\n                retries=True,\n            )\n            assert r.status == 200\n\n            # Request twice in a second to get a 503 response.\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=False,\n            )\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=False,\n            )\n            assert r.status == 503\n\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"503 Service Unavailable\"},\n                retries=True,\n            )\n            assert r.status == 200\n\n            # Ignore Retry-After header on status which is not defined in\n            # Retry.RETRY_AFTER_STATUS_CODES.\n            r = pool.request(\n                \"GET\",\n                \"/retry_after\",\n                fields={\"status\": \"418 I'm a teapot\"},\n                retries=True,\n            )\n            assert r.status == 418\n\n    def test_redirect_after(self) -> None:\n        with HTTPConnectionPool(self.host, self.port) as pool:\n            r = pool.request(\"GET\", \"/redirect_after\", retries=False)\n            assert r.status == 303\n\n            t = time.time()\n            r = pool.request(\"GET\", \"/redirect_after\")\n            assert r.status == 200\n            delta = time.time() - t\n            assert delta >= 1\n\n            t = time.time()\n            timestamp = t + 2\n            r = pool.request(\"GET\", \"/redirect_after?date=\" + str(timestamp))\n            assert r.status == 200\n            delta = time.time() - t\n            assert delta >= 1\n\n            # Retry-After is past\n            t = time.time()\n            timestamp = t - 1\n            r = pool.request(\"GET\", \"/redirect_after?date=\" + str(timestamp))\n            delta = time.time() - t\n            assert r.status == 200\n            assert delta < 1\n\n\nclass TestFileBodiesOnRetryOrRedirect(HTTPDummyServerTestCase):\n    def test_retries_put_filehandle(self) -> None:\n        \"\"\"HTTP PUT retry with a file-like object should not timeout\"\"\"\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            retry = Retry(total=3, status_forcelist=[418])\n            # httplib reads in 8k chunks; use a larger content length\n            content_length = 65535\n            data = b\"A\" * content_length\n            uploaded_file = io.BytesIO(data)\n            headers = {\n                \"test-name\": \"test_retries_put_filehandle\",\n                \"Content-Length\": str(content_length),\n            }\n            resp = pool.urlopen(\n                \"PUT\",\n                \"/successful_retry\",\n                headers=headers,\n                retries=retry,\n                body=uploaded_file,\n                assert_same_host=False,\n                redirect=False,\n            )\n            assert resp.status == 200\n\n    def test_redirect_put_file(self) -> None:\n        \"\"\"PUT with file object should work with a redirection response\"\"\"\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            retry = Retry(total=3, status_forcelist=[418])\n            # httplib reads in 8k chunks; use a larger content length\n            content_length = 65535\n            data = b\"A\" * content_length\n            uploaded_file = io.BytesIO(data)\n            headers = {\n                \"test-name\": \"test_redirect_put_file\",\n                \"Content-Length\": str(content_length),\n            }\n            url = \"/redirect?target=/echo&status=307\"\n            resp = pool.urlopen(\n                \"PUT\",\n                url,\n                headers=headers,\n                retries=retry,\n                body=uploaded_file,\n                assert_same_host=False,\n                redirect=True,\n            )\n            assert resp.status == 200\n            assert resp.data == data\n\n    def test_redirect_with_failed_tell(self) -> None:\n        \"\"\"Abort request if failed to get a position from tell()\"\"\"\n\n        class BadTellObject(io.BytesIO):\n            def tell(self) -> typing.NoReturn:\n                raise OSError\n\n        body = BadTellObject(b\"the data\")\n        url = \"/redirect?target=/successful_retry\"\n        # httplib uses fileno if Content-Length isn't supplied,\n        # which is unsupported by BytesIO.\n        headers = {\"Content-Length\": \"8\"}\n        with HTTPConnectionPool(self.host, self.port, timeout=0.1) as pool:\n            with pytest.raises(\n                UnrewindableBodyError, match=\"Unable to record file position for\"\n            ):\n                pool.urlopen(\"PUT\", url, headers=headers, body=body)\n\n\nclass TestRetryPoolSize(HTTPDummyServerTestCase):\n    def test_pool_size_retry(self) -> None:\n        retries = Retry(total=1, raise_on_status=False, status_forcelist=[404])\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=10, retries=retries, block=True\n        ) as pool:\n            pool.urlopen(\"GET\", \"/not_found\", preload_content=False)\n            assert pool.num_connections == 1\n\n\nclass TestRedirectPoolSize(HTTPDummyServerTestCase):\n    def test_pool_size_redirect(self) -> None:\n        retries = Retry(\n            total=1, raise_on_status=False, status_forcelist=[404], redirect=True\n        )\n        with HTTPConnectionPool(\n            self.host, self.port, maxsize=10, retries=retries, block=True\n        ) as pool:\n            pool.urlopen(\"GET\", \"/redirect\", preload_content=False)\n            assert pool.num_connections == 1\n", "from __future__ import annotations\n\nimport gzip\nimport typing\nfrom test import LONG_TIMEOUT\nfrom unittest import mock\n\nimport pytest\n\nfrom dummyserver.server import HAS_IPV6\nfrom dummyserver.testcase import HTTPDummyServerTestCase, IPv6HTTPDummyServerTestCase\nfrom urllib3 import HTTPHeaderDict, HTTPResponse, request\nfrom urllib3.connectionpool import port_by_scheme\nfrom urllib3.exceptions import MaxRetryError, URLSchemeUnknown\nfrom urllib3.poolmanager import PoolManager\nfrom urllib3.util.retry import Retry\n\n\nclass TestPoolManager(HTTPDummyServerTestCase):\n    @classmethod\n    def setup_class(cls) -> None:\n        super().setup_class()\n        cls.base_url = f\"http://{cls.host}:{cls.port}\"\n        cls.base_url_alt = f\"http://{cls.host_alt}:{cls.port}\"\n\n    def test_redirect(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url}/\"},\n                redirect=False,\n            )\n\n            assert r.status == 303\n\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url}/\"},\n            )\n\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_redirect_twice(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url}/redirect\"},\n                redirect=False,\n            )\n\n            assert r.status == 303\n\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url}/redirect?target={self.base_url}/\"},\n            )\n\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_redirect_to_relative_url(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": \"/redirect\"},\n                redirect=False,\n            )\n\n            assert r.status == 303\n\n            r = http.request(\n                \"GET\", f\"{self.base_url}/redirect\", fields={\"target\": \"/redirect\"}\n            )\n\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_cross_host_redirect(self) -> None:\n        with PoolManager() as http:\n            cross_host_location = f\"{self.base_url_alt}/echo?a=b\"\n            with pytest.raises(MaxRetryError):\n                http.request(\n                    \"GET\",\n                    f\"{self.base_url}/redirect\",\n                    fields={\"target\": cross_host_location},\n                    timeout=LONG_TIMEOUT,\n                    retries=0,\n                )\n\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/echo?a=b\"},\n                timeout=LONG_TIMEOUT,\n                retries=1,\n            )\n\n            assert isinstance(r, HTTPResponse)\n            assert r._pool is not None\n            assert r._pool.host == self.host_alt\n\n    def test_too_many_redirects(self) -> None:\n        with PoolManager() as http:\n            with pytest.raises(MaxRetryError):\n                http.request(\n                    \"GET\",\n                    f\"{self.base_url}/redirect\",\n                    fields={\n                        \"target\": f\"{self.base_url}/redirect?target={self.base_url}/\"\n                    },\n                    retries=1,\n                    preload_content=False,\n                )\n\n            with pytest.raises(MaxRetryError):\n                http.request(\n                    \"GET\",\n                    f\"{self.base_url}/redirect\",\n                    fields={\n                        \"target\": f\"{self.base_url}/redirect?target={self.base_url}/\"\n                    },\n                    retries=Retry(total=None, redirect=1),\n                    preload_content=False,\n                )\n\n            # Even with preload_content=False and raise on redirects, we reused the same\n            # connection\n            assert len(http.pools) == 1\n            pool = http.connection_from_host(self.host, self.port)\n            assert pool.num_connections == 1\n\n    def test_redirect_cross_host_remove_headers(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/headers\"},\n                headers={\"Authorization\": \"foo\", \"Cookie\": \"foo=bar\"},\n            )\n\n            assert r.status == 200\n\n            data = r.json()\n\n            assert \"Authorization\" not in data\n            assert \"Cookie\" not in data\n\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/headers\"},\n                headers={\"authorization\": \"foo\", \"cookie\": \"foo=bar\"},\n            )\n\n            assert r.status == 200\n\n            data = r.json()\n\n            assert \"authorization\" not in data\n            assert \"Authorization\" not in data\n            assert \"cookie\" not in data\n            assert \"Cookie\" not in data\n\n    def test_redirect_cross_host_no_remove_headers(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/headers\"},\n                headers={\"Authorization\": \"foo\", \"Cookie\": \"foo=bar\"},\n                retries=Retry(remove_headers_on_redirect=[]),\n            )\n\n            assert r.status == 200\n\n            data = r.json()\n\n            assert data[\"Authorization\"] == \"foo\"\n            assert data[\"Cookie\"] == \"foo=bar\"\n\n    def test_redirect_cross_host_set_removed_headers(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/headers\"},\n                headers={\n                    \"X-API-Secret\": \"foo\",\n                    \"Authorization\": \"bar\",\n                    \"Cookie\": \"foo=bar\",\n                },\n                retries=Retry(remove_headers_on_redirect=[\"X-API-Secret\"]),\n            )\n\n            assert r.status == 200\n\n            data = r.json()\n\n            assert \"X-API-Secret\" not in data\n            assert data[\"Authorization\"] == \"bar\"\n            assert data[\"Cookie\"] == \"foo=bar\"\n\n            headers = {\n                \"x-api-secret\": \"foo\",\n                \"authorization\": \"bar\",\n                \"cookie\": \"foo=bar\",\n            }\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url_alt}/headers\"},\n                headers=headers,\n                retries=Retry(remove_headers_on_redirect=[\"X-API-Secret\"]),\n            )\n\n            assert r.status == 200\n\n            data = r.json()\n\n            assert \"x-api-secret\" not in data\n            assert \"X-API-Secret\" not in data\n            assert data[\"Authorization\"] == \"bar\"\n            assert data[\"Cookie\"] == \"foo=bar\"\n\n            # Ensure the header argument itself is not modified in-place.\n            assert headers == {\n                \"x-api-secret\": \"foo\",\n                \"authorization\": \"bar\",\n                \"cookie\": \"foo=bar\",\n            }\n\n    def test_redirect_without_preload_releases_connection(self) -> None:\n        with PoolManager(block=True, maxsize=2) as http:\n            r = http.request(\"GET\", f\"{self.base_url}/redirect\", preload_content=False)\n            assert isinstance(r, HTTPResponse)\n            assert r._pool is not None\n            assert r._pool.num_requests == 2\n            assert r._pool.num_connections == 1\n            assert len(http.pools) == 1\n\n    def test_303_redirect_makes_request_lose_body(self) -> None:\n        with PoolManager() as http:\n            response = http.request(\n                \"POST\",\n                f\"{self.base_url}/redirect\",\n                fields={\n                    \"target\": f\"{self.base_url}/headers_and_params\",\n                    \"status\": \"303 See Other\",\n                },\n            )\n        data = response.json()\n        assert data[\"params\"] == {}\n        assert \"Content-Type\" not in HTTPHeaderDict(data[\"headers\"])\n\n    def test_unknown_scheme(self) -> None:\n        with PoolManager() as http:\n            unknown_scheme = \"unknown\"\n            unknown_scheme_url = f\"{unknown_scheme}://host\"\n            with pytest.raises(URLSchemeUnknown) as e:\n                r = http.request(\"GET\", unknown_scheme_url)\n            assert e.value.scheme == unknown_scheme\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": unknown_scheme_url},\n                redirect=False,\n            )\n            assert r.status == 303\n            assert r.headers.get(\"Location\") == unknown_scheme_url\n            with pytest.raises(URLSchemeUnknown) as e:\n                r = http.request(\n                    \"GET\",\n                    f\"{self.base_url}/redirect\",\n                    fields={\"target\": unknown_scheme_url},\n                )\n            assert e.value.scheme == unknown_scheme\n\n    def test_raise_on_redirect(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                fields={\"target\": f\"{self.base_url}/redirect?target={self.base_url}/\"},\n                retries=Retry(total=None, redirect=1, raise_on_redirect=False),\n            )\n\n            assert r.status == 303\n\n    def test_raise_on_status(self) -> None:\n        with PoolManager() as http:\n            with pytest.raises(MaxRetryError):\n                # the default is to raise\n                r = http.request(\n                    \"GET\",\n                    f\"{self.base_url}/status\",\n                    fields={\"status\": \"500 Internal Server Error\"},\n                    retries=Retry(total=1, status_forcelist=range(500, 600)),\n                )\n\n            with pytest.raises(MaxRetryError):\n                # raise explicitly\n                r = http.request(\n                    \"GET\",\n                    f\"{self.base_url}/status\",\n                    fields={\"status\": \"500 Internal Server Error\"},\n                    retries=Retry(\n                        total=1, status_forcelist=range(500, 600), raise_on_status=True\n                    ),\n                )\n\n            # don't raise\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/status\",\n                fields={\"status\": \"500 Internal Server Error\"},\n                retries=Retry(\n                    total=1, status_forcelist=range(500, 600), raise_on_status=False\n                ),\n            )\n\n            assert r.status == 500\n\n    def test_missing_port(self) -> None:\n        # Can a URL that lacks an explicit port like ':80' succeed, or\n        # will all such URLs fail with an error?\n\n        with PoolManager() as http:\n            # By globally adjusting `port_by_scheme` we pretend for a moment\n            # that HTTP's default port is not 80, but is the port at which\n            # our test server happens to be listening.\n            port_by_scheme[\"http\"] = self.port\n            try:\n                r = http.request(\"GET\", f\"http://{self.host}/\", retries=0)\n            finally:\n                port_by_scheme[\"http\"] = 80\n\n            assert r.status == 200\n            assert r.data == b\"Dummy server!\"\n\n    def test_headers(self) -> None:\n        with PoolManager(headers={\"Foo\": \"bar\"}) as http:\n            r = http.request(\"GET\", f\"{self.base_url}/headers\")\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") == \"bar\"\n\n            r = http.request(\"POST\", f\"{self.base_url}/headers\")\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") == \"bar\"\n\n            r = http.request_encode_url(\"GET\", f\"{self.base_url}/headers\")\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") == \"bar\"\n\n            r = http.request_encode_body(\"POST\", f\"{self.base_url}/headers\")\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") == \"bar\"\n\n            r = http.request_encode_url(\n                \"GET\", f\"{self.base_url}/headers\", headers={\"Baz\": \"quux\"}\n            )\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") is None\n            assert returned_headers.get(\"Baz\") == \"quux\"\n\n            r = http.request_encode_body(\n                \"GET\", f\"{self.base_url}/headers\", headers={\"Baz\": \"quux\"}\n            )\n            returned_headers = r.json()\n            assert returned_headers.get(\"Foo\") is None\n            assert returned_headers.get(\"Baz\") == \"quux\"\n\n    def test_headers_http_header_dict(self) -> None:\n        # Test uses a list of headers to assert the order\n        # that headers are sent in the request too.\n\n        headers = HTTPHeaderDict()\n        headers.add(\"Foo\", \"bar\")\n        headers.add(\"Multi\", \"1\")\n        headers.add(\"Baz\", \"quux\")\n        headers.add(\"Multi\", \"2\")\n\n        with PoolManager(headers=headers) as http:\n            r = http.request(\"GET\", f\"{self.base_url}/multi_headers\")\n            returned_headers = r.json()[\"headers\"]\n            assert returned_headers[-4:] == [\n                [\"Foo\", \"bar\"],\n                [\"Multi\", \"1\"],\n                [\"Multi\", \"2\"],\n                [\"Baz\", \"quux\"],\n            ]\n\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/multi_headers\",\n                headers={\n                    **headers,\n                    \"Extra\": \"extra\",\n                    \"Foo\": \"new\",\n                },\n            )\n            returned_headers = r.json()[\"headers\"]\n            assert returned_headers[-4:] == [\n                [\"Foo\", \"new\"],\n                [\"Multi\", \"1, 2\"],\n                [\"Baz\", \"quux\"],\n                [\"Extra\", \"extra\"],\n            ]\n\n    def test_merge_headers_with_pool_manager_headers(self) -> None:\n        headers = HTTPHeaderDict()\n        headers.add(\"Cookie\", \"choc-chip\")\n        headers.add(\"Cookie\", \"oatmeal-raisin\")\n        orig = headers.copy()\n        added_headers = {\"Cookie\": \"tim-tam\"}\n\n        with PoolManager(headers=headers) as http:\n            r = http.request(\n                \"GET\",\n                f\"{self.base_url}/multi_headers\",\n                headers=typing.cast(HTTPHeaderDict, http.headers) | added_headers,\n            )\n            returned_headers = r.json()[\"headers\"]\n            assert returned_headers[-3:] == [\n                [\"Cookie\", \"choc-chip\"],\n                [\"Cookie\", \"oatmeal-raisin\"],\n                [\"Cookie\", \"tim-tam\"],\n            ]\n            # make sure the pool headers weren't modified\n            assert http.headers == orig\n\n    def test_headers_http_multi_header_multipart(self) -> None:\n        headers = HTTPHeaderDict()\n        headers.add(\"Multi\", \"1\")\n        headers.add(\"Multi\", \"2\")\n        old_headers = headers.copy()\n\n        with PoolManager(headers=headers) as http:\n            r = http.request(\n                \"POST\",\n                f\"{self.base_url}/multi_headers\",\n                fields={\"k\": \"v\"},\n                multipart_boundary=\"b\",\n                encode_multipart=True,\n            )\n            returned_headers = r.json()[\"headers\"]\n            assert returned_headers[4:] == [\n                [\"Multi\", \"1\"],\n                [\"Multi\", \"2\"],\n                [\"Content-Type\", \"multipart/form-data; boundary=b\"],\n            ]\n            # Assert that the previous headers weren't modified.\n            assert headers == old_headers\n\n            # Set a default value for the Content-Type\n            headers[\"Content-Type\"] = \"multipart/form-data; boundary=b; field=value\"\n            r = http.request(\n                \"POST\",\n                f\"{self.base_url}/multi_headers\",\n                fields={\"k\": \"v\"},\n                multipart_boundary=\"b\",\n                encode_multipart=True,\n            )\n            returned_headers = r.json()[\"headers\"]\n            assert returned_headers[4:] == [\n                [\"Multi\", \"1\"],\n                [\"Multi\", \"2\"],\n                # Uses the set value, not the one that would be generated.\n                [\"Content-Type\", \"multipart/form-data; boundary=b; field=value\"],\n            ]\n\n    def test_body(self) -> None:\n        with PoolManager() as http:\n            r = http.request(\"POST\", f\"{self.base_url}/echo\", body=b\"test\")\n            assert r.data == b\"test\"\n\n    def test_http_with_ssl_keywords(self) -> None:\n        with PoolManager(ca_certs=\"REQUIRED\") as http:\n            r = http.request(\"GET\", f\"http://{self.host}:{self.port}/\")\n            assert r.status == 200\n\n    def test_http_with_server_hostname(self) -> None:\n        with PoolManager(server_hostname=\"example.com\") as http:\n            r = http.request(\"GET\", f\"http://{self.host}:{self.port}/\")\n            assert r.status == 200\n\n    def test_http_with_ca_cert_dir(self) -> None:\n        with PoolManager(ca_certs=\"REQUIRED\", ca_cert_dir=\"/nosuchdir\") as http:\n            r = http.request(\"GET\", f\"http://{self.host}:{self.port}/\")\n            assert r.status == 200\n\n    @pytest.mark.parametrize(\n        [\"target\", \"expected_target\"],\n        [\n            (\"/echo_uri?q=1#fragment\", b\"/echo_uri?q=1\"),\n            (\"/echo_uri?#\", b\"/echo_uri?\"),\n            (\"/echo_uri#?\", b\"/echo_uri\"),\n            (\"/echo_uri#?#\", b\"/echo_uri\"),\n            (\"/echo_uri??#\", b\"/echo_uri??\"),\n            (\"/echo_uri?%3f#\", b\"/echo_uri?%3F\"),\n            (\"/echo_uri?%3F#\", b\"/echo_uri?%3F\"),\n            (\"/echo_uri?[]\", b\"/echo_uri?%5B%5D\"),\n        ],\n    )\n    def test_encode_http_target(self, target: str, expected_target: bytes) -> None:\n        with PoolManager() as http:\n            url = f\"http://{self.host}:{self.port}{target}\"\n            r = http.request(\"GET\", url)\n            assert r.data == expected_target\n\n    def test_top_level_request(self) -> None:\n        r = request(\"GET\", f\"{self.base_url}/\")\n        assert r.status == 200\n        assert r.data == b\"Dummy server!\"\n\n    def test_top_level_request_without_keyword_args(self) -> None:\n        body = \"\"\n        with pytest.raises(TypeError):\n            request(\"GET\", f\"{self.base_url}/\", body)  # type: ignore[misc]\n\n    def test_top_level_request_with_body(self) -> None:\n        r = request(\"POST\", f\"{self.base_url}/echo\", body=b\"test\")\n        assert r.status == 200\n        assert r.data == b\"test\"\n\n    def test_top_level_request_with_preload_content(self) -> None:\n        r = request(\"GET\", f\"{self.base_url}/echo\", preload_content=False)\n        assert r.status == 200\n        assert r.connection is not None\n        r.data\n        assert r.connection is None\n\n    def test_top_level_request_with_decode_content(self) -> None:\n        r = request(\n            \"GET\",\n            f\"{self.base_url}/encodingrequest\",\n            headers={\"accept-encoding\": \"gzip\"},\n            decode_content=False,\n        )\n        assert r.status == 200\n        assert gzip.decompress(r.data) == b\"hello, world!\"\n\n        r = request(\n            \"GET\",\n            f\"{self.base_url}/encodingrequest\",\n            headers={\"accept-encoding\": \"gzip\"},\n            decode_content=True,\n        )\n        assert r.status == 200\n        assert r.data == b\"hello, world!\"\n\n    def test_top_level_request_with_redirect(self) -> None:\n        r = request(\n            \"GET\",\n            f\"{self.base_url}/redirect\",\n            fields={\"target\": f\"{self.base_url}/\"},\n            redirect=False,\n        )\n\n        assert r.status == 303\n\n        r = request(\n            \"GET\",\n            f\"{self.base_url}/redirect\",\n            fields={\"target\": f\"{self.base_url}/\"},\n            redirect=True,\n        )\n\n        assert r.status == 200\n        assert r.data == b\"Dummy server!\"\n\n    def test_top_level_request_with_retries(self) -> None:\n        r = request(\"GET\", f\"{self.base_url}/redirect\", retries=False)\n        assert r.status == 303\n\n        r = request(\"GET\", f\"{self.base_url}/redirect\", retries=3)\n        assert r.status == 200\n\n    def test_top_level_request_with_timeout(self) -> None:\n        with mock.patch(\"urllib3.poolmanager.RequestMethods.request\") as mockRequest:\n            mockRequest.return_value = HTTPResponse(status=200)\n\n            r = request(\"GET\", f\"{self.base_url}/redirect\", timeout=2.5)\n\n            assert r.status == 200\n\n            mockRequest.assert_called_with(\n                \"GET\",\n                f\"{self.base_url}/redirect\",\n                body=None,\n                fields=None,\n                headers=None,\n                preload_content=True,\n                decode_content=True,\n                redirect=True,\n                retries=None,\n                timeout=2.5,\n                json=None,\n            )\n\n    @pytest.mark.parametrize(\n        \"headers\",\n        [\n            None,\n            {\"content-Type\": \"application/json\"},\n            {\"content-Type\": \"text/plain\"},\n            {\"attribute\": \"value\", \"CONTENT-TYPE\": \"application/json\"},\n            HTTPHeaderDict(cookie=\"foo, bar\"),\n        ],\n    )\n    def test_request_with_json(self, headers: HTTPHeaderDict) -> None:\n        body = {\"attribute\": \"value\"}\n        r = request(\n            method=\"POST\", url=f\"{self.base_url}/echo_json\", headers=headers, json=body\n        )\n        assert r.status == 200\n        assert r.json() == body\n        if headers is not None and \"application/json\" not in headers.values():\n            assert \"text/plain\" in r.headers[\"Content-Type\"].replace(\" \", \"\").split(\",\")\n        else:\n            assert \"application/json\" in r.headers[\"Content-Type\"].replace(\n                \" \", \"\"\n            ).split(\",\")\n\n    def test_top_level_request_with_json_with_httpheaderdict(self) -> None:\n        body = {\"attribute\": \"value\"}\n        header = HTTPHeaderDict(cookie=\"foo, bar\")\n        with PoolManager(headers=header) as http:\n            r = http.request(method=\"POST\", url=f\"{self.base_url}/echo_json\", json=body)\n            assert r.status == 200\n            assert r.json() == body\n            assert \"application/json\" in r.headers[\"Content-Type\"].replace(\n                \" \", \"\"\n            ).split(\",\")\n\n    def test_top_level_request_with_body_and_json(self) -> None:\n        match = \"request got values for both 'body' and 'json' parameters which are mutually exclusive\"\n        with pytest.raises(TypeError, match=match):\n            body = {\"attribute\": \"value\"}\n            request(method=\"POST\", url=f\"{self.base_url}/echo\", body=\"\", json=body)\n\n    def test_top_level_request_with_invalid_body(self) -> None:\n        class BadBody:\n            def __repr__(self) -> str:\n                return \"<BadBody>\"\n\n        with pytest.raises(TypeError) as e:\n            request(\n                method=\"POST\",\n                url=f\"{self.base_url}/echo\",\n                body=BadBody(),  # type: ignore[arg-type]\n            )\n        assert str(e.value) == (\n            \"'body' must be a bytes-like object, file-like \"\n            \"object, or iterable. Instead was <BadBody>\"\n        )\n\n\n@pytest.mark.skipif(not HAS_IPV6, reason=\"IPv6 is not supported on this system\")\nclass TestIPv6PoolManager(IPv6HTTPDummyServerTestCase):\n    @classmethod\n    def setup_class(cls) -> None:\n        super().setup_class()\n        cls.base_url = f\"http://[{cls.host}]:{cls.port}\"\n\n    def test_ipv6(self) -> None:\n        with PoolManager() as http:\n            http.request(\"GET\", self.base_url)\n"], "filenames": [".readthedocs.yml", "CHANGES.rst", "dummyserver/handlers.py", "src/urllib3/_collections.py", "src/urllib3/_version.py", "src/urllib3/connectionpool.py", "src/urllib3/poolmanager.py", "test/with_dummyserver/test_connectionpool.py", "test/with_dummyserver/test_poolmanager.py"], "buggy_code_start_loc": [6, 0, 283, 11, 4, 13, 10, 481, 245], "buggy_code_end_loc": [7, 169, 283, 393, 5, 895, 453, 481, 245], "fixing_code_start_loc": [6, 1, 284, 12, 4, 14, 10, 482, 246], "fixing_code_end_loc": [7, 180, 290, 414, 5, 901, 457, 493, 260], "type": "CWE-200", "message": "urllib3 is a user-friendly HTTP client library for Python. urllib3 previously wouldn't remove the HTTP request body when an HTTP redirect response using status 301, 302, or 303 after the request had its method changed from one that could accept a request body (like `POST`) to `GET` as is required by HTTP RFCs. Although this behavior is not specified in the section for redirects, it can be inferred by piecing together information from different sections and we have observed the behavior in other major HTTP client implementations like curl and web browsers. Because the vulnerability requires a previously trusted service to become compromised in order to have an impact on confidentiality we believe the exploitability of this vulnerability is low. Additionally, many users aren't putting sensitive data in HTTP request bodies, if this is the case then this vulnerability isn't exploitable. Both of the following conditions must be true to be affected by this vulnerability: 1. Using urllib3 and submitting sensitive information in the HTTP request body (such as form data or JSON) and 2. The origin service is compromised and starts redirecting using 301, 302, or 303 to a malicious peer or the redirected-to service becomes compromised. This issue has been addressed in versions 1.26.18 and 2.0.7 and users are advised to update to resolve this issue. Users unable to update should disable redirects for services that aren't expecting to respond with redirects with `redirects=False` and disable automatic redirects with `redirects=False` and handle 301, 302, and 303 redirects manually by stripping the HTTP request body.\n", "other": {"cve": {"id": "CVE-2023-45803", "sourceIdentifier": "security-advisories@github.com", "published": "2023-10-17T20:15:10.070", "lastModified": "2023-11-03T22:15:11.693", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "urllib3 is a user-friendly HTTP client library for Python. urllib3 previously wouldn't remove the HTTP request body when an HTTP redirect response using status 301, 302, or 303 after the request had its method changed from one that could accept a request body (like `POST`) to `GET` as is required by HTTP RFCs. Although this behavior is not specified in the section for redirects, it can be inferred by piecing together information from different sections and we have observed the behavior in other major HTTP client implementations like curl and web browsers. Because the vulnerability requires a previously trusted service to become compromised in order to have an impact on confidentiality we believe the exploitability of this vulnerability is low. Additionally, many users aren't putting sensitive data in HTTP request bodies, if this is the case then this vulnerability isn't exploitable. Both of the following conditions must be true to be affected by this vulnerability: 1. Using urllib3 and submitting sensitive information in the HTTP request body (such as form data or JSON) and 2. The origin service is compromised and starts redirecting using 301, 302, or 303 to a malicious peer or the redirected-to service becomes compromised. This issue has been addressed in versions 1.26.18 and 2.0.7 and users are advised to update to resolve this issue. Users unable to update should disable redirects for services that aren't expecting to respond with redirects with `redirects=False` and disable automatic redirects with `redirects=False` and handle 301, 302, and 303 redirects manually by stripping the HTTP request body.\n"}, {"lang": "es", "value": "urllib3 es una librer\u00eda cliente HTTP f\u00e1cil de usar para Python. Anteriormente, urllib3 no eliminaba el cuerpo de la solicitud HTTP cuando una respuesta de redirecci\u00f3n HTTP usaba el estado 301, 302 o 303 despu\u00e9s de que la solicitud cambiara su m\u00e9todo de uno que pudiera aceptar un cuerpo de solicitud (como `POST`) a `GET` tal como est\u00e1. requerido por los RFC HTTP. Aunque este comportamiento no se especifica en la secci\u00f3n de redirecciones, se puede inferir reuniendo informaci\u00f3n de diferentes secciones y hemos observado el comportamiento en otras implementaciones importantes de clientes HTTP como curl y navegadores web. Debido a que la vulnerabilidad requiere que un servicio previamente confiable se vea comprometido para tener un impacto en la confidencialidad, creemos que la explotabilidad de esta vulnerabilidad es baja. Adem\u00e1s, muchos usuarios no colocan datos confidenciales en los cuerpos de solicitud HTTP; si este es el caso, entonces esta vulnerabilidad no es explotable. Las dos condiciones siguientes deben ser ciertas para verse afectado por esta vulnerabilidad: 1. Usar urllib3 y enviar informaci\u00f3n confidencial en el cuerpo de la solicitud HTTP (como datos de formulario o JSON) y 2. El servicio de origen est\u00e1 comprometido y comienza a redireccionar usando 301. 302 o 303 a un par malicioso o el servicio redirigido se ve comprometido. Este problema se solucion\u00f3 en las versiones 1.26.18 y 2.0.7 y se recomienda a los usuarios que actualicen para resolverlo. Los usuarios que no puedan actualizar deben deshabilitar los redireccionamientos para servicios que no esperan responder con redireccionamientos con `redirects=False` y deshabilitar los redireccionamientos autom\u00e1ticos con `redirects=False` y manejar los redireccionamientos 301, 302 y 303 manualmente eliminando el cuerpo de la solicitud HTTP."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:A/AC:H/PR:H/UI:N/S:U/C:H/I:N/A:N", "attackVector": "ADJACENT_NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.2, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 0.5, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:A/AC:H/PR:H/UI:N/S:U/C:H/I:N/A:N", "attackVector": "ADJACENT_NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.2, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 0.5, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-200"}]}, {"source": "nvd@nist.gov", "type": "Secondary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:python:urllib3:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.26.18", "matchCriteriaId": "3F2284A6-F467-4419-9AF7-9FFD133B04E5"}, {"vulnerable": true, "criteria": "cpe:2.3:a:python:urllib3:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.0.0", "versionEndExcluding": "2.0.7", "matchCriteriaId": "6A586164-F448-431C-8966-14E145A82BB5"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:38:*:*:*:*:*:*:*", "matchCriteriaId": "CC559B26-5DFC-4B7A-A27C-B77DE755DFF9"}]}]}], "references": [{"url": "https://github.com/urllib3/urllib3/commit/4e98d57809dacab1cbe625fddeec1a290c478ea9", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/urllib3/urllib3/security/advisories/GHSA-g4mx-q9vg-27p4", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/4R2Y5XK3WALSR3FNAGN7JBYV2B343ZKB/", "source": "security-advisories@github.com"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/5F5CUBAN5XMEBVBZPHFITBLMJV5FIJJ5/", "source": "security-advisories@github.com"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/PPDPLM6UUMN55ESPQWJFLLIZY4ZKCNRX/", "source": "security-advisories@github.com", "tags": ["Mailing List"]}, {"url": "https://www.rfc-editor.org/rfc/rfc9110.html#name-get", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/urllib3/urllib3/commit/4e98d57809dacab1cbe625fddeec1a290c478ea9"}}