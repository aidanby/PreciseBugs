{"buggy_code": ["/*\n *  ext4.h\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/include/linux/minix_fs.h\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n#ifndef _EXT4_H\n#define _EXT4_H\n\n#include <linux/types.h>\n#include <linux/blkdev.h>\n#include <linux/magic.h>\n#include <linux/jbd2.h>\n#include <linux/quota.h>\n#include <linux/rwsem.h>\n#include <linux/rbtree.h>\n#include <linux/seqlock.h>\n#include <linux/mutex.h>\n#include <linux/timer.h>\n#include <linux/wait.h>\n#include <linux/blockgroup_lock.h>\n#include <linux/percpu_counter.h>\n\n/*\n * The fourth extended filesystem constants/structures\n */\n\n/*\n * Define EXT4FS_DEBUG to produce debug messages\n */\n#undef EXT4FS_DEBUG\n\n/*\n * Debug code\n */\n#ifdef EXT4FS_DEBUG\n#define ext4_debug(f, a...)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tprintk(KERN_DEBUG \"EXT4-fs DEBUG (%s, %d): %s:\",\t\\\n\t\t\t__FILE__, __LINE__, __func__);\t\t\t\\\n\t\tprintk(KERN_DEBUG f, ## a);\t\t\t\t\\\n\t} while (0)\n#else\n#define ext4_debug(f, a...)\tdo {} while (0)\n#endif\n\n/* data type for block offset of block group */\ntypedef int ext4_grpblk_t;\n\n/* data type for filesystem-wide blocks number */\ntypedef unsigned long long ext4_fsblk_t;\n\n/* data type for file logical block number */\ntypedef __u32 ext4_lblk_t;\n\n/* data type for block group number */\ntypedef unsigned int ext4_group_t;\n\n/*\n * Flags used in mballoc's allocation_context flags field.  \n *\n * Also used to show what's going on for debugging purposes when the\n * flag field is exported via the traceport interface\n */\n\n/* prefer goal again. length */\n#define EXT4_MB_HINT_MERGE\t\t0x0001\n/* blocks already reserved */\n#define EXT4_MB_HINT_RESERVED\t\t0x0002\n/* metadata is being allocated */\n#define EXT4_MB_HINT_METADATA\t\t0x0004\n/* first blocks in the file */\n#define EXT4_MB_HINT_FIRST\t\t0x0008\n/* search for the best chunk */\n#define EXT4_MB_HINT_BEST\t\t0x0010\n/* data is being allocated */\n#define EXT4_MB_HINT_DATA\t\t0x0020\n/* don't preallocate (for tails) */\n#define EXT4_MB_HINT_NOPREALLOC\t\t0x0040\n/* allocate for locality group */\n#define EXT4_MB_HINT_GROUP_ALLOC\t0x0080\n/* allocate goal blocks or none */\n#define EXT4_MB_HINT_GOAL_ONLY\t\t0x0100\n/* goal is meaningful */\n#define EXT4_MB_HINT_TRY_GOAL\t\t0x0200\n/* blocks already pre-reserved by delayed allocation */\n#define EXT4_MB_DELALLOC_RESERVED\t0x0400\n/* We are doing stream allocation */\n#define EXT4_MB_STREAM_ALLOC\t\t0x0800\n\n\nstruct ext4_allocation_request {\n\t/* target inode for block we're allocating */\n\tstruct inode *inode;\n\t/* how many blocks we want to allocate */\n\tunsigned int len;\n\t/* logical block in target inode */\n\text4_lblk_t logical;\n\t/* the closest logical allocated block to the left */\n\text4_lblk_t lleft;\n\t/* the closest logical allocated block to the right */\n\text4_lblk_t lright;\n\t/* phys. target (a hint) */\n\text4_fsblk_t goal;\n\t/* phys. block for the closest logical allocated block to the left */\n\text4_fsblk_t pleft;\n\t/* phys. block for the closest logical allocated block to the right */\n\text4_fsblk_t pright;\n\t/* flags. see above EXT4_MB_HINT_* */\n\tunsigned int flags;\n};\n\n/*\n * For delayed allocation tracking\n */\nstruct mpage_da_data {\n\tstruct inode *inode;\n\tsector_t b_blocknr;\t\t/* start block number of extent */\n\tsize_t b_size;\t\t\t/* size of extent */\n\tunsigned long b_state;\t\t/* state of the extent */\n\tunsigned long first_page, next_page;\t/* extent of pages */\n\tstruct writeback_control *wbc;\n\tint io_done;\n\tint pages_written;\n\tint retval;\n};\n#define\tEXT4_IO_UNWRITTEN\t0x1\ntypedef struct ext4_io_end {\n\tstruct list_head\tlist;\t\t/* per-file finished AIO list */\n\tstruct inode\t\t*inode;\t\t/* file being written to */\n\tunsigned int\t\tflag;\t\t/* unwritten or not */\n\tint\t\t\terror;\t\t/* I/O error code */\n\tloff_t\t\t\toffset;\t\t/* offset in the file */\n\tssize_t\t\t\tsize;\t\t/* size of the extent */\n\tstruct work_struct\twork;\t\t/* data work queue */\n} ext4_io_end_t;\n\n/*\n * Special inodes numbers\n */\n#define\tEXT4_BAD_INO\t\t 1\t/* Bad blocks inode */\n#define EXT4_ROOT_INO\t\t 2\t/* Root inode */\n#define EXT4_BOOT_LOADER_INO\t 5\t/* Boot loader inode */\n#define EXT4_UNDEL_DIR_INO\t 6\t/* Undelete directory inode */\n#define EXT4_RESIZE_INO\t\t 7\t/* Reserved group descriptors inode */\n#define EXT4_JOURNAL_INO\t 8\t/* Journal inode */\n\n/* First non-reserved inode for old ext4 filesystems */\n#define EXT4_GOOD_OLD_FIRST_INO\t11\n\n/*\n * Maximal count of links to a file\n */\n#define EXT4_LINK_MAX\t\t65000\n\n/*\n * Macro-instructions used to manage several block sizes\n */\n#define EXT4_MIN_BLOCK_SIZE\t\t1024\n#define\tEXT4_MAX_BLOCK_SIZE\t\t65536\n#define EXT4_MIN_BLOCK_LOG_SIZE\t\t10\n#ifdef __KERNEL__\n# define EXT4_BLOCK_SIZE(s)\t\t((s)->s_blocksize)\n#else\n# define EXT4_BLOCK_SIZE(s)\t\t(EXT4_MIN_BLOCK_SIZE << (s)->s_log_block_size)\n#endif\n#define\tEXT4_ADDR_PER_BLOCK(s)\t\t(EXT4_BLOCK_SIZE(s) / sizeof(__u32))\n#ifdef __KERNEL__\n# define EXT4_BLOCK_SIZE_BITS(s)\t((s)->s_blocksize_bits)\n#else\n# define EXT4_BLOCK_SIZE_BITS(s)\t((s)->s_log_block_size + 10)\n#endif\n#ifdef __KERNEL__\n#define\tEXT4_ADDR_PER_BLOCK_BITS(s)\t(EXT4_SB(s)->s_addr_per_block_bits)\n#define EXT4_INODE_SIZE(s)\t\t(EXT4_SB(s)->s_inode_size)\n#define EXT4_FIRST_INO(s)\t\t(EXT4_SB(s)->s_first_ino)\n#else\n#define EXT4_INODE_SIZE(s)\t(((s)->s_rev_level == EXT4_GOOD_OLD_REV) ? \\\n\t\t\t\t EXT4_GOOD_OLD_INODE_SIZE : \\\n\t\t\t\t (s)->s_inode_size)\n#define EXT4_FIRST_INO(s)\t(((s)->s_rev_level == EXT4_GOOD_OLD_REV) ? \\\n\t\t\t\t EXT4_GOOD_OLD_FIRST_INO : \\\n\t\t\t\t (s)->s_first_ino)\n#endif\n#define EXT4_BLOCK_ALIGN(size, blkbits)\t\tALIGN((size), (1 << (blkbits)))\n\n/*\n * Structure of a blocks group descriptor\n */\nstruct ext4_group_desc\n{\n\t__le32\tbg_block_bitmap_lo;\t/* Blocks bitmap block */\n\t__le32\tbg_inode_bitmap_lo;\t/* Inodes bitmap block */\n\t__le32\tbg_inode_table_lo;\t/* Inodes table block */\n\t__le16\tbg_free_blocks_count_lo;/* Free blocks count */\n\t__le16\tbg_free_inodes_count_lo;/* Free inodes count */\n\t__le16\tbg_used_dirs_count_lo;\t/* Directories count */\n\t__le16\tbg_flags;\t\t/* EXT4_BG_flags (INODE_UNINIT, etc) */\n\t__u32\tbg_reserved[2];\t\t/* Likely block/inode bitmap checksum */\n\t__le16  bg_itable_unused_lo;\t/* Unused inodes count */\n\t__le16  bg_checksum;\t\t/* crc16(sb_uuid+group+desc) */\n\t__le32\tbg_block_bitmap_hi;\t/* Blocks bitmap block MSB */\n\t__le32\tbg_inode_bitmap_hi;\t/* Inodes bitmap block MSB */\n\t__le32\tbg_inode_table_hi;\t/* Inodes table block MSB */\n\t__le16\tbg_free_blocks_count_hi;/* Free blocks count MSB */\n\t__le16\tbg_free_inodes_count_hi;/* Free inodes count MSB */\n\t__le16\tbg_used_dirs_count_hi;\t/* Directories count MSB */\n\t__le16  bg_itable_unused_hi;    /* Unused inodes count MSB */\n\t__u32\tbg_reserved2[3];\n};\n\n/*\n * Structure of a flex block group info\n */\n\nstruct flex_groups {\n\tatomic_t free_inodes;\n\tatomic_t free_blocks;\n\tatomic_t used_dirs;\n};\n\n#define EXT4_BG_INODE_UNINIT\t0x0001 /* Inode table/bitmap not in use */\n#define EXT4_BG_BLOCK_UNINIT\t0x0002 /* Block bitmap not in use */\n#define EXT4_BG_INODE_ZEROED\t0x0004 /* On-disk itable initialized to zero */\n\n/*\n * Macro-instructions used to manage group descriptors\n */\n#define EXT4_MIN_DESC_SIZE\t\t32\n#define EXT4_MIN_DESC_SIZE_64BIT\t64\n#define\tEXT4_MAX_DESC_SIZE\t\tEXT4_MIN_BLOCK_SIZE\n#define EXT4_DESC_SIZE(s)\t\t(EXT4_SB(s)->s_desc_size)\n#ifdef __KERNEL__\n# define EXT4_BLOCKS_PER_GROUP(s)\t(EXT4_SB(s)->s_blocks_per_group)\n# define EXT4_DESC_PER_BLOCK(s)\t\t(EXT4_SB(s)->s_desc_per_block)\n# define EXT4_INODES_PER_GROUP(s)\t(EXT4_SB(s)->s_inodes_per_group)\n# define EXT4_DESC_PER_BLOCK_BITS(s)\t(EXT4_SB(s)->s_desc_per_block_bits)\n#else\n# define EXT4_BLOCKS_PER_GROUP(s)\t((s)->s_blocks_per_group)\n# define EXT4_DESC_PER_BLOCK(s)\t\t(EXT4_BLOCK_SIZE(s) / EXT4_DESC_SIZE(s))\n# define EXT4_INODES_PER_GROUP(s)\t((s)->s_inodes_per_group)\n#endif\n\n/*\n * Constants relative to the data blocks\n */\n#define\tEXT4_NDIR_BLOCKS\t\t12\n#define\tEXT4_IND_BLOCK\t\t\tEXT4_NDIR_BLOCKS\n#define\tEXT4_DIND_BLOCK\t\t\t(EXT4_IND_BLOCK + 1)\n#define\tEXT4_TIND_BLOCK\t\t\t(EXT4_DIND_BLOCK + 1)\n#define\tEXT4_N_BLOCKS\t\t\t(EXT4_TIND_BLOCK + 1)\n\n/*\n * Inode flags\n */\n#define\tEXT4_SECRM_FL\t\t\t0x00000001 /* Secure deletion */\n#define\tEXT4_UNRM_FL\t\t\t0x00000002 /* Undelete */\n#define\tEXT4_COMPR_FL\t\t\t0x00000004 /* Compress file */\n#define EXT4_SYNC_FL\t\t\t0x00000008 /* Synchronous updates */\n#define EXT4_IMMUTABLE_FL\t\t0x00000010 /* Immutable file */\n#define EXT4_APPEND_FL\t\t\t0x00000020 /* writes to file may only append */\n#define EXT4_NODUMP_FL\t\t\t0x00000040 /* do not dump file */\n#define EXT4_NOATIME_FL\t\t\t0x00000080 /* do not update atime */\n/* Reserved for compression usage... */\n#define EXT4_DIRTY_FL\t\t\t0x00000100\n#define EXT4_COMPRBLK_FL\t\t0x00000200 /* One or more compressed clusters */\n#define EXT4_NOCOMPR_FL\t\t\t0x00000400 /* Don't compress */\n#define EXT4_ECOMPR_FL\t\t\t0x00000800 /* Compression error */\n/* End compression flags --- maybe not all used */\n#define EXT4_INDEX_FL\t\t\t0x00001000 /* hash-indexed directory */\n#define EXT4_IMAGIC_FL\t\t\t0x00002000 /* AFS directory */\n#define EXT4_JOURNAL_DATA_FL\t\t0x00004000 /* file data should be journaled */\n#define EXT4_NOTAIL_FL\t\t\t0x00008000 /* file tail should not be merged */\n#define EXT4_DIRSYNC_FL\t\t\t0x00010000 /* dirsync behaviour (directories only) */\n#define EXT4_TOPDIR_FL\t\t\t0x00020000 /* Top of directory hierarchies*/\n#define EXT4_HUGE_FILE_FL               0x00040000 /* Set to each huge file */\n#define EXT4_EXTENTS_FL\t\t\t0x00080000 /* Inode uses extents */\n#define EXT4_EA_INODE_FL\t        0x00200000 /* Inode used for large EA */\n#define EXT4_EOFBLOCKS_FL\t\t0x00400000 /* Blocks allocated beyond EOF */\n#define EXT4_RESERVED_FL\t\t0x80000000 /* reserved for ext4 lib */\n\n#define EXT4_FL_USER_VISIBLE\t\t0x004BDFFF /* User visible flags */\n#define EXT4_FL_USER_MODIFIABLE\t\t0x004B80FF /* User modifiable flags */\n\n/* Flags that should be inherited by new inodes from their parent. */\n#define EXT4_FL_INHERITED (EXT4_SECRM_FL | EXT4_UNRM_FL | EXT4_COMPR_FL |\\\n\t\t\t   EXT4_SYNC_FL | EXT4_IMMUTABLE_FL | EXT4_APPEND_FL |\\\n\t\t\t   EXT4_NODUMP_FL | EXT4_NOATIME_FL |\\\n\t\t\t   EXT4_NOCOMPR_FL | EXT4_JOURNAL_DATA_FL |\\\n\t\t\t   EXT4_NOTAIL_FL | EXT4_DIRSYNC_FL)\n\n/* Flags that are appropriate for regular files (all but dir-specific ones). */\n#define EXT4_REG_FLMASK (~(EXT4_DIRSYNC_FL | EXT4_TOPDIR_FL))\n\n/* Flags that are appropriate for non-directories/regular files. */\n#define EXT4_OTHER_FLMASK (EXT4_NODUMP_FL | EXT4_NOATIME_FL)\n\n/* Mask out flags that are inappropriate for the given type of inode. */\nstatic inline __u32 ext4_mask_flags(umode_t mode, __u32 flags)\n{\n\tif (S_ISDIR(mode))\n\t\treturn flags;\n\telse if (S_ISREG(mode))\n\t\treturn flags & EXT4_REG_FLMASK;\n\telse\n\t\treturn flags & EXT4_OTHER_FLMASK;\n}\n\n/* Used to pass group descriptor data when online resize is done */\nstruct ext4_new_group_input {\n\t__u32 group;\t\t/* Group number for this data */\n\t__u64 block_bitmap;\t/* Absolute block number of block bitmap */\n\t__u64 inode_bitmap;\t/* Absolute block number of inode bitmap */\n\t__u64 inode_table;\t/* Absolute block number of inode table start */\n\t__u32 blocks_count;\t/* Total number of blocks in this group */\n\t__u16 reserved_blocks;\t/* Number of reserved blocks in this group */\n\t__u16 unused;\n};\n\n/* The struct ext4_new_group_input in kernel space, with free_blocks_count */\nstruct ext4_new_group_data {\n\t__u32 group;\n\t__u64 block_bitmap;\n\t__u64 inode_bitmap;\n\t__u64 inode_table;\n\t__u32 blocks_count;\n\t__u16 reserved_blocks;\n\t__u16 unused;\n\t__u32 free_blocks_count;\n};\n\n/*\n * Flags used by ext4_get_blocks()\n */\n\t/* Allocate any needed blocks and/or convert an unitialized\n\t   extent to be an initialized ext4 */\n#define EXT4_GET_BLOCKS_CREATE\t\t\t0x0001\n\t/* Request the creation of an unitialized extent */\n#define EXT4_GET_BLOCKS_UNINIT_EXT\t\t0x0002\n#define EXT4_GET_BLOCKS_CREATE_UNINIT_EXT\t(EXT4_GET_BLOCKS_UNINIT_EXT|\\\n\t\t\t\t\t\t EXT4_GET_BLOCKS_CREATE)\n\t/* Caller is from the delayed allocation writeout path,\n\t   so set the magic i_delalloc_reserve_flag after taking the \n\t   inode allocation semaphore for */\n#define EXT4_GET_BLOCKS_DELALLOC_RESERVE\t0x0004\n\t/* caller is from the direct IO path, request to creation of an\n\tunitialized extents if not allocated, split the uninitialized\n\textent if blocks has been preallocated already*/\n#define EXT4_GET_BLOCKS_PRE_IO\t\t\t0x0008\n#define EXT4_GET_BLOCKS_CONVERT\t\t\t0x0010\n#define EXT4_GET_BLOCKS_IO_CREATE_EXT\t\t(EXT4_GET_BLOCKS_PRE_IO|\\\n\t\t\t\t\t EXT4_GET_BLOCKS_CREATE_UNINIT_EXT)\n\t/* Convert extent to initialized after IO complete */\n#define EXT4_GET_BLOCKS_IO_CONVERT_EXT\t\t(EXT4_GET_BLOCKS_CONVERT|\\\n\t\t\t\t\t EXT4_GET_BLOCKS_IO_CREATE_EXT)\n\n/*\n * Flags used by ext4_free_blocks\n */\n#define EXT4_FREE_BLOCKS_METADATA\t0x0001\n#define EXT4_FREE_BLOCKS_FORGET\t\t0x0002\n#define EXT4_FREE_BLOCKS_VALIDATED\t0x0004\n\n/*\n * ioctl commands\n */\n#define\tEXT4_IOC_GETFLAGS\t\tFS_IOC_GETFLAGS\n#define\tEXT4_IOC_SETFLAGS\t\tFS_IOC_SETFLAGS\n#define\tEXT4_IOC_GETVERSION\t\t_IOR('f', 3, long)\n#define\tEXT4_IOC_SETVERSION\t\t_IOW('f', 4, long)\n#define\tEXT4_IOC_GETVERSION_OLD\t\tFS_IOC_GETVERSION\n#define\tEXT4_IOC_SETVERSION_OLD\t\tFS_IOC_SETVERSION\n#ifdef CONFIG_JBD2_DEBUG\n#define EXT4_IOC_WAIT_FOR_READONLY\t_IOR('f', 99, long)\n#endif\n#define EXT4_IOC_GETRSVSZ\t\t_IOR('f', 5, long)\n#define EXT4_IOC_SETRSVSZ\t\t_IOW('f', 6, long)\n#define EXT4_IOC_GROUP_EXTEND\t\t_IOW('f', 7, unsigned long)\n#define EXT4_IOC_GROUP_ADD\t\t_IOW('f', 8, struct ext4_new_group_input)\n#define EXT4_IOC_MIGRATE\t\t_IO('f', 9)\n /* note ioctl 10 reserved for an early version of the FIEMAP ioctl */\n /* note ioctl 11 reserved for filesystem-independent FIEMAP ioctl */\n#define EXT4_IOC_ALLOC_DA_BLKS\t\t_IO('f', 12)\n#define EXT4_IOC_MOVE_EXT\t\t_IOWR('f', 15, struct move_extent)\n\n/*\n * ioctl commands in 32 bit emulation\n */\n#define EXT4_IOC32_GETFLAGS\t\tFS_IOC32_GETFLAGS\n#define EXT4_IOC32_SETFLAGS\t\tFS_IOC32_SETFLAGS\n#define EXT4_IOC32_GETVERSION\t\t_IOR('f', 3, int)\n#define EXT4_IOC32_SETVERSION\t\t_IOW('f', 4, int)\n#define EXT4_IOC32_GETRSVSZ\t\t_IOR('f', 5, int)\n#define EXT4_IOC32_SETRSVSZ\t\t_IOW('f', 6, int)\n#define EXT4_IOC32_GROUP_EXTEND\t\t_IOW('f', 7, unsigned int)\n#ifdef CONFIG_JBD2_DEBUG\n#define EXT4_IOC32_WAIT_FOR_READONLY\t_IOR('f', 99, int)\n#endif\n#define EXT4_IOC32_GETVERSION_OLD\tFS_IOC32_GETVERSION\n#define EXT4_IOC32_SETVERSION_OLD\tFS_IOC32_SETVERSION\n\n\n/*\n *  Mount options\n */\nstruct ext4_mount_options {\n\tunsigned long s_mount_opt;\n\tuid_t s_resuid;\n\tgid_t s_resgid;\n\tunsigned long s_commit_interval;\n\tu32 s_min_batch_time, s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\tint s_jquota_fmt;\n\tchar *s_qf_names[MAXQUOTAS];\n#endif\n};\n\n/* Max physical block we can addres w/o extents */\n#define EXT4_MAX_BLOCK_FILE_PHYS\t0xFFFFFFFF\n\n/*\n * Structure of an inode on the disk\n */\nstruct ext4_inode {\n\t__le16\ti_mode;\t\t/* File mode */\n\t__le16\ti_uid;\t\t/* Low 16 bits of Owner Uid */\n\t__le32\ti_size_lo;\t/* Size in bytes */\n\t__le32\ti_atime;\t/* Access time */\n\t__le32\ti_ctime;\t/* Inode Change time */\n\t__le32\ti_mtime;\t/* Modification time */\n\t__le32\ti_dtime;\t/* Deletion Time */\n\t__le16\ti_gid;\t\t/* Low 16 bits of Group Id */\n\t__le16\ti_links_count;\t/* Links count */\n\t__le32\ti_blocks_lo;\t/* Blocks count */\n\t__le32\ti_flags;\t/* File flags */\n\tunion {\n\t\tstruct {\n\t\t\t__le32  l_i_version;\n\t\t} linux1;\n\t\tstruct {\n\t\t\t__u32  h_i_translator;\n\t\t} hurd1;\n\t\tstruct {\n\t\t\t__u32  m_i_reserved1;\n\t\t} masix1;\n\t} osd1;\t\t\t\t/* OS dependent 1 */\n\t__le32\ti_block[EXT4_N_BLOCKS];/* Pointers to blocks */\n\t__le32\ti_generation;\t/* File version (for NFS) */\n\t__le32\ti_file_acl_lo;\t/* File ACL */\n\t__le32\ti_size_high;\n\t__le32\ti_obso_faddr;\t/* Obsoleted fragment address */\n\tunion {\n\t\tstruct {\n\t\t\t__le16\tl_i_blocks_high; /* were l_i_reserved1 */\n\t\t\t__le16\tl_i_file_acl_high;\n\t\t\t__le16\tl_i_uid_high;\t/* these 2 fields */\n\t\t\t__le16\tl_i_gid_high;\t/* were reserved2[0] */\n\t\t\t__u32\tl_i_reserved2;\n\t\t} linux2;\n\t\tstruct {\n\t\t\t__le16\th_i_reserved1;\t/* Obsoleted fragment number/size which are removed in ext4 */\n\t\t\t__u16\th_i_mode_high;\n\t\t\t__u16\th_i_uid_high;\n\t\t\t__u16\th_i_gid_high;\n\t\t\t__u32\th_i_author;\n\t\t} hurd2;\n\t\tstruct {\n\t\t\t__le16\th_i_reserved1;\t/* Obsoleted fragment number/size which are removed in ext4 */\n\t\t\t__le16\tm_i_file_acl_high;\n\t\t\t__u32\tm_i_reserved2[2];\n\t\t} masix2;\n\t} osd2;\t\t\t\t/* OS dependent 2 */\n\t__le16\ti_extra_isize;\n\t__le16\ti_pad1;\n\t__le32  i_ctime_extra;  /* extra Change time      (nsec << 2 | epoch) */\n\t__le32  i_mtime_extra;  /* extra Modification time(nsec << 2 | epoch) */\n\t__le32  i_atime_extra;  /* extra Access time      (nsec << 2 | epoch) */\n\t__le32  i_crtime;       /* File Creation time */\n\t__le32  i_crtime_extra; /* extra FileCreationtime (nsec << 2 | epoch) */\n\t__le32  i_version_hi;\t/* high 32 bits for 64-bit version */\n};\n\nstruct move_extent {\n\t__u32 reserved;\t\t/* should be zero */\n\t__u32 donor_fd;\t\t/* donor file descriptor */\n\t__u64 orig_start;\t/* logical start offset in block for orig */\n\t__u64 donor_start;\t/* logical start offset in block for donor */\n\t__u64 len;\t\t/* block length to be moved */\n\t__u64 moved_len;\t/* moved block length */\n};\n\n#define EXT4_EPOCH_BITS 2\n#define EXT4_EPOCH_MASK ((1 << EXT4_EPOCH_BITS) - 1)\n#define EXT4_NSEC_MASK  (~0UL << EXT4_EPOCH_BITS)\n\n/*\n * Extended fields will fit into an inode if the filesystem was formatted\n * with large inodes (-I 256 or larger) and there are not currently any EAs\n * consuming all of the available space. For new inodes we always reserve\n * enough space for the kernel's known extended fields, but for inodes\n * created with an old kernel this might not have been the case. None of\n * the extended inode fields is critical for correct filesystem operation.\n * This macro checks if a certain field fits in the inode. Note that\n * inode-size = GOOD_OLD_INODE_SIZE + i_extra_isize\n */\n#define EXT4_FITS_IN_INODE(ext4_inode, einode, field)\t\\\n\t((offsetof(typeof(*ext4_inode), field) +\t\\\n\t  sizeof((ext4_inode)->field))\t\t\t\\\n\t<= (EXT4_GOOD_OLD_INODE_SIZE +\t\t\t\\\n\t    (einode)->i_extra_isize))\t\t\t\\\n\nstatic inline __le32 ext4_encode_extra_time(struct timespec *time)\n{\n       return cpu_to_le32((sizeof(time->tv_sec) > 4 ?\n\t\t\t   (time->tv_sec >> 32) & EXT4_EPOCH_MASK : 0) |\n                          ((time->tv_nsec << EXT4_EPOCH_BITS) & EXT4_NSEC_MASK));\n}\n\nstatic inline void ext4_decode_extra_time(struct timespec *time, __le32 extra)\n{\n       if (sizeof(time->tv_sec) > 4)\n\t       time->tv_sec |= (__u64)(le32_to_cpu(extra) & EXT4_EPOCH_MASK)\n\t\t\t       << 32;\n       time->tv_nsec = (le32_to_cpu(extra) & EXT4_NSEC_MASK) >> EXT4_EPOCH_BITS;\n}\n\n#define EXT4_INODE_SET_XTIME(xtime, inode, raw_inode)\t\t\t       \\\ndo {\t\t\t\t\t\t\t\t\t       \\\n\t(raw_inode)->xtime = cpu_to_le32((inode)->xtime.tv_sec);\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, EXT4_I(inode), xtime ## _extra))     \\\n\t\t(raw_inode)->xtime ## _extra =\t\t\t\t       \\\n\t\t\t\text4_encode_extra_time(&(inode)->xtime);       \\\n} while (0)\n\n#define EXT4_EINODE_SET_XTIME(xtime, einode, raw_inode)\t\t\t       \\\ndo {\t\t\t\t\t\t\t\t\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime))\t\t       \\\n\t\t(raw_inode)->xtime = cpu_to_le32((einode)->xtime.tv_sec);      \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime ## _extra))\t       \\\n\t\t(raw_inode)->xtime ## _extra =\t\t\t\t       \\\n\t\t\t\text4_encode_extra_time(&(einode)->xtime);      \\\n} while (0)\n\n#define EXT4_INODE_GET_XTIME(xtime, inode, raw_inode)\t\t\t       \\\ndo {\t\t\t\t\t\t\t\t\t       \\\n\t(inode)->xtime.tv_sec = (signed)le32_to_cpu((raw_inode)->xtime);       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, EXT4_I(inode), xtime ## _extra))     \\\n\t\text4_decode_extra_time(&(inode)->xtime,\t\t\t       \\\n\t\t\t\t       raw_inode->xtime ## _extra);\t       \\\n} while (0)\n\n#define EXT4_EINODE_GET_XTIME(xtime, einode, raw_inode)\t\t\t       \\\ndo {\t\t\t\t\t\t\t\t\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime))\t\t       \\\n\t\t(einode)->xtime.tv_sec = \t\t\t\t       \\\n\t\t\t(signed)le32_to_cpu((raw_inode)->xtime);\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime ## _extra))\t       \\\n\t\text4_decode_extra_time(&(einode)->xtime,\t\t       \\\n\t\t\t\t       raw_inode->xtime ## _extra);\t       \\\n} while (0)\n\n#define i_disk_version osd1.linux1.l_i_version\n\n#if defined(__KERNEL__) || defined(__linux__)\n#define i_reserved1\tosd1.linux1.l_i_reserved1\n#define i_file_acl_high\tosd2.linux2.l_i_file_acl_high\n#define i_blocks_high\tosd2.linux2.l_i_blocks_high\n#define i_uid_low\ti_uid\n#define i_gid_low\ti_gid\n#define i_uid_high\tosd2.linux2.l_i_uid_high\n#define i_gid_high\tosd2.linux2.l_i_gid_high\n#define i_reserved2\tosd2.linux2.l_i_reserved2\n\n#elif defined(__GNU__)\n\n#define i_translator\tosd1.hurd1.h_i_translator\n#define i_uid_high\tosd2.hurd2.h_i_uid_high\n#define i_gid_high\tosd2.hurd2.h_i_gid_high\n#define i_author\tosd2.hurd2.h_i_author\n\n#elif defined(__masix__)\n\n#define i_reserved1\tosd1.masix1.m_i_reserved1\n#define i_file_acl_high\tosd2.masix2.m_i_file_acl_high\n#define i_reserved2\tosd2.masix2.m_i_reserved2\n\n#endif /* defined(__KERNEL__) || defined(__linux__) */\n\n/*\n * storage for cached extent\n */\nstruct ext4_ext_cache {\n\text4_fsblk_t\tec_start;\n\text4_lblk_t\tec_block;\n\t__u32\t\tec_len; /* must be 32bit to return holes */\n\t__u32\t\tec_type;\n};\n\n/*\n * fourth extended file system inode data in memory\n */\nstruct ext4_inode_info {\n\t__le32\ti_data[15];\t/* unconverted */\n\t__u32\ti_flags;\n\text4_fsblk_t\ti_file_acl;\n\t__u32\ti_dtime;\n\n\t/*\n\t * i_block_group is the number of the block group which contains\n\t * this file's inode.  Constant across the lifetime of the inode,\n\t * it is ued for making block allocation decisions - we try to\n\t * place a file's data blocks near its inode block, and new inodes\n\t * near to their parent directory's inode.\n\t */\n\text4_group_t\ti_block_group;\n\tunsigned long\ti_state_flags;\t\t/* Dynamic state flags */\n\n\text4_lblk_t\t\ti_dir_start_lookup;\n#ifdef CONFIG_EXT4_FS_XATTR\n\t/*\n\t * Extended attributes can be read independently of the main file\n\t * data. Taking i_mutex even when reading would cause contention\n\t * between readers of EAs and writers of regular file data, so\n\t * instead we synchronize on xattr_sem when reading or changing\n\t * EAs.\n\t */\n\tstruct rw_semaphore xattr_sem;\n#endif\n\n\tstruct list_head i_orphan;\t/* unlinked but open inodes */\n\n\t/*\n\t * i_disksize keeps track of what the inode size is ON DISK, not\n\t * in memory.  During truncate, i_size is set to the new size by\n\t * the VFS prior to calling ext4_truncate(), but the filesystem won't\n\t * set i_disksize to 0 until the truncate is actually under way.\n\t *\n\t * The intent is that i_disksize always represents the blocks which\n\t * are used by this file.  This allows recovery to restart truncate\n\t * on orphans if we crash during truncate.  We actually write i_disksize\n\t * into the on-disk inode when writing inodes out, instead of i_size.\n\t *\n\t * The only time when i_disksize and i_size may be different is when\n\t * a truncate is in progress.  The only things which change i_disksize\n\t * are ext4_get_block (growth) and ext4_truncate (shrinkth).\n\t */\n\tloff_t\ti_disksize;\n\n\t/*\n\t * i_data_sem is for serialising ext4_truncate() against\n\t * ext4_getblock().  In the 2.4 ext2 design, great chunks of inode's\n\t * data tree are chopped off during truncate. We can't do that in\n\t * ext4 because whenever we perform intermediate commits during\n\t * truncate, the inode and all the metadata blocks *must* be in a\n\t * consistent state which allows truncation of the orphans to restart\n\t * during recovery.  Hence we must fix the get_block-vs-truncate race\n\t * by other means, so we have i_data_sem.\n\t */\n\tstruct rw_semaphore i_data_sem;\n\tstruct inode vfs_inode;\n\tstruct jbd2_inode jinode;\n\n\tstruct ext4_ext_cache i_cached_extent;\n\t/*\n\t * File creation time. Its function is same as that of\n\t * struct timespec i_{a,c,m}time in the generic inode.\n\t */\n\tstruct timespec i_crtime;\n\n\t/* mballoc */\n\tstruct list_head i_prealloc_list;\n\tspinlock_t i_prealloc_lock;\n\n\t/* ialloc */\n\text4_group_t\ti_last_alloc_group;\n\n\t/* allocation reservation info for delalloc */\n\tunsigned int i_reserved_data_blocks;\n\tunsigned int i_reserved_meta_blocks;\n\tunsigned int i_allocated_meta_blocks;\n\tunsigned short i_delalloc_reserved_flag;\n\tsector_t i_da_metadata_calc_last_lblock;\n\tint i_da_metadata_calc_len;\n\n\t/* on-disk additional length */\n\t__u16 i_extra_isize;\n\n\tspinlock_t i_block_reservation_lock;\n#ifdef CONFIG_QUOTA\n\t/* quota space reservation, managed internally by quota code */\n\tqsize_t i_reserved_quota;\n#endif\n\n\t/* completed IOs that might need unwritten extents handling */\n\tstruct list_head i_completed_io_list;\n\t/* current io_end structure for async DIO write*/\n\text4_io_end_t *cur_aio_dio;\n\n\t/*\n\t * Transactions that contain inode's metadata needed to complete\n\t * fsync and fdatasync, respectively.\n\t */\n\ttid_t i_sync_tid;\n\ttid_t i_datasync_tid;\n};\n\n/*\n * File system states\n */\n#define\tEXT4_VALID_FS\t\t\t0x0001\t/* Unmounted cleanly */\n#define\tEXT4_ERROR_FS\t\t\t0x0002\t/* Errors detected */\n#define\tEXT4_ORPHAN_FS\t\t\t0x0004\t/* Orphans being recovered */\n\n/*\n * Misc. filesystem flags\n */\n#define EXT2_FLAGS_SIGNED_HASH\t\t0x0001  /* Signed dirhash in use */\n#define EXT2_FLAGS_UNSIGNED_HASH\t0x0002  /* Unsigned dirhash in use */\n#define EXT2_FLAGS_TEST_FILESYS\t\t0x0004\t/* to test development code */\n\n/*\n * Mount flags\n */\n#define EXT4_MOUNT_OLDALLOC\t\t0x00002  /* Don't use the new Orlov allocator */\n#define EXT4_MOUNT_GRPID\t\t0x00004\t/* Create files with directory's group */\n#define EXT4_MOUNT_DEBUG\t\t0x00008\t/* Some debugging messages */\n#define EXT4_MOUNT_ERRORS_CONT\t\t0x00010\t/* Continue on errors */\n#define EXT4_MOUNT_ERRORS_RO\t\t0x00020\t/* Remount fs ro on errors */\n#define EXT4_MOUNT_ERRORS_PANIC\t\t0x00040\t/* Panic on errors */\n#define EXT4_MOUNT_MINIX_DF\t\t0x00080\t/* Mimics the Minix statfs */\n#define EXT4_MOUNT_NOLOAD\t\t0x00100\t/* Don't use existing journal*/\n#define EXT4_MOUNT_DATA_FLAGS\t\t0x00C00\t/* Mode for data writes: */\n#define EXT4_MOUNT_JOURNAL_DATA\t\t0x00400\t/* Write data to journal */\n#define EXT4_MOUNT_ORDERED_DATA\t\t0x00800\t/* Flush data before commit */\n#define EXT4_MOUNT_WRITEBACK_DATA\t0x00C00\t/* No data ordering */\n#define EXT4_MOUNT_UPDATE_JOURNAL\t0x01000\t/* Update the journal format */\n#define EXT4_MOUNT_NO_UID32\t\t0x02000  /* Disable 32-bit UIDs */\n#define EXT4_MOUNT_XATTR_USER\t\t0x04000\t/* Extended user attributes */\n#define EXT4_MOUNT_POSIX_ACL\t\t0x08000\t/* POSIX Access Control Lists */\n#define EXT4_MOUNT_NO_AUTO_DA_ALLOC\t0x10000\t/* No auto delalloc mapping */\n#define EXT4_MOUNT_BARRIER\t\t0x20000 /* Use block barriers */\n#define EXT4_MOUNT_NOBH\t\t\t0x40000 /* No bufferheads */\n#define EXT4_MOUNT_QUOTA\t\t0x80000 /* Some quota option set */\n#define EXT4_MOUNT_USRQUOTA\t\t0x100000 /* \"old\" user quota */\n#define EXT4_MOUNT_GRPQUOTA\t\t0x200000 /* \"old\" group quota */\n#define EXT4_MOUNT_JOURNAL_CHECKSUM\t0x800000 /* Journal checksums */\n#define EXT4_MOUNT_JOURNAL_ASYNC_COMMIT\t0x1000000 /* Journal Async Commit */\n#define EXT4_MOUNT_I_VERSION            0x2000000 /* i_version support */\n#define EXT4_MOUNT_DELALLOC\t\t0x8000000 /* Delalloc support */\n#define EXT4_MOUNT_DATA_ERR_ABORT\t0x10000000 /* Abort on file data write */\n#define EXT4_MOUNT_BLOCK_VALIDITY\t0x20000000 /* Block validity checking */\n#define EXT4_MOUNT_DISCARD\t\t0x40000000 /* Issue DISCARD requests */\n\n#define clear_opt(o, opt)\t\to &= ~EXT4_MOUNT_##opt\n#define set_opt(o, opt)\t\t\to |= EXT4_MOUNT_##opt\n#define test_opt(sb, opt)\t\t(EXT4_SB(sb)->s_mount_opt & \\\n\t\t\t\t\t EXT4_MOUNT_##opt)\n\n#define ext4_set_bit\t\t\text2_set_bit\n#define ext4_set_bit_atomic\t\text2_set_bit_atomic\n#define ext4_clear_bit\t\t\text2_clear_bit\n#define ext4_clear_bit_atomic\t\text2_clear_bit_atomic\n#define ext4_test_bit\t\t\text2_test_bit\n#define ext4_find_first_zero_bit\text2_find_first_zero_bit\n#define ext4_find_next_zero_bit\t\text2_find_next_zero_bit\n#define ext4_find_next_bit\t\text2_find_next_bit\n\n/*\n * Maximal mount counts between two filesystem checks\n */\n#define EXT4_DFL_MAX_MNT_COUNT\t\t20\t/* Allow 20 mounts */\n#define EXT4_DFL_CHECKINTERVAL\t\t0\t/* Don't use interval check */\n\n/*\n * Behaviour when detecting errors\n */\n#define EXT4_ERRORS_CONTINUE\t\t1\t/* Continue execution */\n#define EXT4_ERRORS_RO\t\t\t2\t/* Remount fs read-only */\n#define EXT4_ERRORS_PANIC\t\t3\t/* Panic */\n#define EXT4_ERRORS_DEFAULT\t\tEXT4_ERRORS_CONTINUE\n\n/*\n * Structure of the super block\n */\nstruct ext4_super_block {\n/*00*/\t__le32\ts_inodes_count;\t\t/* Inodes count */\n\t__le32\ts_blocks_count_lo;\t/* Blocks count */\n\t__le32\ts_r_blocks_count_lo;\t/* Reserved blocks count */\n\t__le32\ts_free_blocks_count_lo;\t/* Free blocks count */\n/*10*/\t__le32\ts_free_inodes_count;\t/* Free inodes count */\n\t__le32\ts_first_data_block;\t/* First Data Block */\n\t__le32\ts_log_block_size;\t/* Block size */\n\t__le32\ts_obso_log_frag_size;\t/* Obsoleted fragment size */\n/*20*/\t__le32\ts_blocks_per_group;\t/* # Blocks per group */\n\t__le32\ts_obso_frags_per_group;\t/* Obsoleted fragments per group */\n\t__le32\ts_inodes_per_group;\t/* # Inodes per group */\n\t__le32\ts_mtime;\t\t/* Mount time */\n/*30*/\t__le32\ts_wtime;\t\t/* Write time */\n\t__le16\ts_mnt_count;\t\t/* Mount count */\n\t__le16\ts_max_mnt_count;\t/* Maximal mount count */\n\t__le16\ts_magic;\t\t/* Magic signature */\n\t__le16\ts_state;\t\t/* File system state */\n\t__le16\ts_errors;\t\t/* Behaviour when detecting errors */\n\t__le16\ts_minor_rev_level;\t/* minor revision level */\n/*40*/\t__le32\ts_lastcheck;\t\t/* time of last check */\n\t__le32\ts_checkinterval;\t/* max. time between checks */\n\t__le32\ts_creator_os;\t\t/* OS */\n\t__le32\ts_rev_level;\t\t/* Revision level */\n/*50*/\t__le16\ts_def_resuid;\t\t/* Default uid for reserved blocks */\n\t__le16\ts_def_resgid;\t\t/* Default gid for reserved blocks */\n\t/*\n\t * These fields are for EXT4_DYNAMIC_REV superblocks only.\n\t *\n\t * Note: the difference between the compatible feature set and\n\t * the incompatible feature set is that if there is a bit set\n\t * in the incompatible feature set that the kernel doesn't\n\t * know about, it should refuse to mount the filesystem.\n\t *\n\t * e2fsck's requirements are more strict; if it doesn't know\n\t * about a feature in either the compatible or incompatible\n\t * feature set, it must abort and not try to meddle with\n\t * things it doesn't understand...\n\t */\n\t__le32\ts_first_ino;\t\t/* First non-reserved inode */\n\t__le16  s_inode_size;\t\t/* size of inode structure */\n\t__le16\ts_block_group_nr;\t/* block group # of this superblock */\n\t__le32\ts_feature_compat;\t/* compatible feature set */\n/*60*/\t__le32\ts_feature_incompat;\t/* incompatible feature set */\n\t__le32\ts_feature_ro_compat;\t/* readonly-compatible feature set */\n/*68*/\t__u8\ts_uuid[16];\t\t/* 128-bit uuid for volume */\n/*78*/\tchar\ts_volume_name[16];\t/* volume name */\n/*88*/\tchar\ts_last_mounted[64];\t/* directory where last mounted */\n/*C8*/\t__le32\ts_algorithm_usage_bitmap; /* For compression */\n\t/*\n\t * Performance hints.  Directory preallocation should only\n\t * happen if the EXT4_FEATURE_COMPAT_DIR_PREALLOC flag is on.\n\t */\n\t__u8\ts_prealloc_blocks;\t/* Nr of blocks to try to preallocate*/\n\t__u8\ts_prealloc_dir_blocks;\t/* Nr to preallocate for dirs */\n\t__le16\ts_reserved_gdt_blocks;\t/* Per group desc for online growth */\n\t/*\n\t * Journaling support valid if EXT4_FEATURE_COMPAT_HAS_JOURNAL set.\n\t */\n/*D0*/\t__u8\ts_journal_uuid[16];\t/* uuid of journal superblock */\n/*E0*/\t__le32\ts_journal_inum;\t\t/* inode number of journal file */\n\t__le32\ts_journal_dev;\t\t/* device number of journal file */\n\t__le32\ts_last_orphan;\t\t/* start of list of inodes to delete */\n\t__le32\ts_hash_seed[4];\t\t/* HTREE hash seed */\n\t__u8\ts_def_hash_version;\t/* Default hash version to use */\n\t__u8\ts_reserved_char_pad;\n\t__le16  s_desc_size;\t\t/* size of group descriptor */\n/*100*/\t__le32\ts_default_mount_opts;\n\t__le32\ts_first_meta_bg;\t/* First metablock block group */\n\t__le32\ts_mkfs_time;\t\t/* When the filesystem was created */\n\t__le32\ts_jnl_blocks[17];\t/* Backup of the journal inode */\n\t/* 64bit support valid if EXT4_FEATURE_COMPAT_64BIT */\n/*150*/\t__le32\ts_blocks_count_hi;\t/* Blocks count */\n\t__le32\ts_r_blocks_count_hi;\t/* Reserved blocks count */\n\t__le32\ts_free_blocks_count_hi;\t/* Free blocks count */\n\t__le16\ts_min_extra_isize;\t/* All inodes have at least # bytes */\n\t__le16\ts_want_extra_isize; \t/* New inodes should reserve # bytes */\n\t__le32\ts_flags;\t\t/* Miscellaneous flags */\n\t__le16  s_raid_stride;\t\t/* RAID stride */\n\t__le16  s_mmp_interval;         /* # seconds to wait in MMP checking */\n\t__le64  s_mmp_block;            /* Block for multi-mount protection */\n\t__le32  s_raid_stripe_width;    /* blocks on all data disks (N*stride)*/\n\t__u8\ts_log_groups_per_flex;  /* FLEX_BG group size */\n\t__u8\ts_reserved_char_pad2;\n\t__le16  s_reserved_pad;\n\t__le64\ts_kbytes_written;\t/* nr of lifetime kilobytes written */\n\t__u32   s_reserved[160];        /* Padding to the end of the block */\n};\n\n#ifdef __KERNEL__\n\n/*\n * run-time mount flags\n */\n#define EXT4_MF_MNTDIR_SAMPLED\t0x0001\n#define EXT4_MF_FS_ABORTED\t0x0002\t/* Fatal error detected */\n\n/*\n * fourth extended-fs super-block data in memory\n */\nstruct ext4_sb_info {\n\tunsigned long s_desc_size;\t/* Size of a group descriptor in bytes */\n\tunsigned long s_inodes_per_block;/* Number of inodes per block */\n\tunsigned long s_blocks_per_group;/* Number of blocks in a group */\n\tunsigned long s_inodes_per_group;/* Number of inodes in a group */\n\tunsigned long s_itb_per_group;\t/* Number of inode table blocks per group */\n\tunsigned long s_gdb_count;\t/* Number of group descriptor blocks */\n\tunsigned long s_desc_per_block;\t/* Number of group descriptors per block */\n\text4_group_t s_groups_count;\t/* Number of groups in the fs */\n\text4_group_t s_blockfile_groups;/* Groups acceptable for non-extent files */\n\tunsigned long s_overhead_last;  /* Last calculated overhead */\n\tunsigned long s_blocks_last;    /* Last seen block count */\n\tloff_t s_bitmap_maxbytes;\t/* max bytes for bitmap files */\n\tstruct buffer_head * s_sbh;\t/* Buffer containing the super block */\n\tstruct ext4_super_block *s_es;\t/* Pointer to the super block in the buffer */\n\tstruct buffer_head **s_group_desc;\n\tunsigned int s_mount_opt;\n\tunsigned int s_mount_flags;\n\text4_fsblk_t s_sb_block;\n\tuid_t s_resuid;\n\tgid_t s_resgid;\n\tunsigned short s_mount_state;\n\tunsigned short s_pad;\n\tint s_addr_per_block_bits;\n\tint s_desc_per_block_bits;\n\tint s_inode_size;\n\tint s_first_ino;\n\tunsigned int s_inode_readahead_blks;\n\tunsigned int s_inode_goal;\n\tspinlock_t s_next_gen_lock;\n\tu32 s_next_generation;\n\tu32 s_hash_seed[4];\n\tint s_def_hash_version;\n\tint s_hash_unsigned;\t/* 3 if hash should be signed, 0 if not */\n\tstruct percpu_counter s_freeblocks_counter;\n\tstruct percpu_counter s_freeinodes_counter;\n\tstruct percpu_counter s_dirs_counter;\n\tstruct percpu_counter s_dirtyblocks_counter;\n\tstruct blockgroup_lock *s_blockgroup_lock;\n\tstruct proc_dir_entry *s_proc;\n\tstruct kobject s_kobj;\n\tstruct completion s_kobj_unregister;\n\n\t/* Journaling */\n\tstruct inode *s_journal_inode;\n\tstruct journal_s *s_journal;\n\tstruct list_head s_orphan;\n\tstruct mutex s_orphan_lock;\n\tstruct mutex s_resize_lock;\n\tunsigned long s_commit_interval;\n\tu32 s_max_batch_time;\n\tu32 s_min_batch_time;\n\tstruct block_device *journal_bdev;\n#ifdef CONFIG_JBD2_DEBUG\n\tstruct timer_list turn_ro_timer;\t/* For turning read-only (crash simulation) */\n\twait_queue_head_t ro_wait_queue;\t/* For people waiting for the fs to go read-only */\n#endif\n#ifdef CONFIG_QUOTA\n\tchar *s_qf_names[MAXQUOTAS];\t\t/* Names of quota files with journalled quota */\n\tint s_jquota_fmt;\t\t\t/* Format of quota to use */\n#endif\n\tunsigned int s_want_extra_isize; /* New inodes should reserve # bytes */\n\tstruct rb_root system_blks;\n\n#ifdef EXTENTS_STATS\n\t/* ext4 extents stats */\n\tunsigned long s_ext_min;\n\tunsigned long s_ext_max;\n\tunsigned long s_depth_max;\n\tspinlock_t s_ext_stats_lock;\n\tunsigned long s_ext_blocks;\n\tunsigned long s_ext_extents;\n#endif\n\n\t/* for buddy allocator */\n\tstruct ext4_group_info ***s_group_info;\n\tstruct inode *s_buddy_cache;\n\tlong s_blocks_reserved;\n\tspinlock_t s_reserve_lock;\n\tspinlock_t s_md_lock;\n\ttid_t s_last_transaction;\n\tunsigned short *s_mb_offsets;\n\tunsigned int *s_mb_maxs;\n\n\t/* tunables */\n\tunsigned long s_stripe;\n\tunsigned int s_mb_stream_request;\n\tunsigned int s_mb_max_to_scan;\n\tunsigned int s_mb_min_to_scan;\n\tunsigned int s_mb_stats;\n\tunsigned int s_mb_order2_reqs;\n\tunsigned int s_mb_group_prealloc;\n\tunsigned int s_max_writeback_mb_bump;\n\t/* where last allocation was done - for stream allocation */\n\tunsigned long s_mb_last_group;\n\tunsigned long s_mb_last_start;\n\n\t/* stats for buddy allocator */\n\tspinlock_t s_mb_pa_lock;\n\tatomic_t s_bal_reqs;\t/* number of reqs with len > 1 */\n\tatomic_t s_bal_success;\t/* we found long enough chunks */\n\tatomic_t s_bal_allocated;\t/* in blocks */\n\tatomic_t s_bal_ex_scanned;\t/* total extents scanned */\n\tatomic_t s_bal_goals;\t/* goal hits */\n\tatomic_t s_bal_breaks;\t/* too long searches */\n\tatomic_t s_bal_2orders;\t/* 2^order hits */\n\tspinlock_t s_bal_lock;\n\tunsigned long s_mb_buddies_generated;\n\tunsigned long long s_mb_generation_time;\n\tatomic_t s_mb_lost_chunks;\n\tatomic_t s_mb_preallocated;\n\tatomic_t s_mb_discarded;\n\tatomic_t s_lock_busy;\n\n\t/* locality groups */\n\tstruct ext4_locality_group *s_locality_groups;\n\n\t/* for write statistics */\n\tunsigned long s_sectors_written_start;\n\tu64 s_kbytes_written;\n\n\tunsigned int s_log_groups_per_flex;\n\tstruct flex_groups *s_flex_groups;\n\n\t/* workqueue for dio unwritten */\n\tstruct workqueue_struct *dio_unwritten_wq;\n};\n\nstatic inline struct ext4_sb_info *EXT4_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\nstatic inline struct ext4_inode_info *EXT4_I(struct inode *inode)\n{\n\treturn container_of(inode, struct ext4_inode_info, vfs_inode);\n}\n\nstatic inline struct timespec ext4_current_time(struct inode *inode)\n{\n\treturn (inode->i_sb->s_time_gran < NSEC_PER_SEC) ?\n\t\tcurrent_fs_time(inode->i_sb) : CURRENT_TIME_SEC;\n}\n\nstatic inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)\n{\n\treturn ino == EXT4_ROOT_INO ||\n\t\tino == EXT4_JOURNAL_INO ||\n\t\tino == EXT4_RESIZE_INO ||\n\t\t(ino >= EXT4_FIRST_INO(sb) &&\n\t\t ino <= le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count));\n}\n\n/*\n * Inode dynamic state flags\n */\nenum {\n\tEXT4_STATE_JDATA,\t\t/* journaled data exists */\n\tEXT4_STATE_NEW,\t\t\t/* inode is newly created */\n\tEXT4_STATE_XATTR,\t\t/* has in-inode xattrs */\n\tEXT4_STATE_NO_EXPAND,\t\t/* No space for expansion */\n\tEXT4_STATE_DA_ALLOC_CLOSE,\t/* Alloc DA blks on close */\n\tEXT4_STATE_EXT_MIGRATE,\t\t/* Inode is migrating */\n\tEXT4_STATE_DIO_UNWRITTEN,\t/* need convert on dio done*/\n};\n\nstatic inline int ext4_test_inode_state(struct inode *inode, int bit)\n{\n\treturn test_bit(bit, &EXT4_I(inode)->i_state_flags);\n}\n\nstatic inline void ext4_set_inode_state(struct inode *inode, int bit)\n{\n\tset_bit(bit, &EXT4_I(inode)->i_state_flags);\n}\n\nstatic inline void ext4_clear_inode_state(struct inode *inode, int bit)\n{\n\tclear_bit(bit, &EXT4_I(inode)->i_state_flags);\n}\n#else\n/* Assume that user mode programs are passing in an ext4fs superblock, not\n * a kernel struct super_block.  This will allow us to call the feature-test\n * macros from user land. */\n#define EXT4_SB(sb)\t(sb)\n#endif\n\n#define NEXT_ORPHAN(inode) EXT4_I(inode)->i_dtime\n\n/*\n * Codes for operating systems\n */\n#define EXT4_OS_LINUX\t\t0\n#define EXT4_OS_HURD\t\t1\n#define EXT4_OS_MASIX\t\t2\n#define EXT4_OS_FREEBSD\t\t3\n#define EXT4_OS_LITES\t\t4\n\n/*\n * Revision levels\n */\n#define EXT4_GOOD_OLD_REV\t0\t/* The good old (original) format */\n#define EXT4_DYNAMIC_REV\t1\t/* V2 format w/ dynamic inode sizes */\n\n#define EXT4_CURRENT_REV\tEXT4_GOOD_OLD_REV\n#define EXT4_MAX_SUPP_REV\tEXT4_DYNAMIC_REV\n\n#define EXT4_GOOD_OLD_INODE_SIZE 128\n\n/*\n * Feature set definitions\n */\n\n#define EXT4_HAS_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\t((EXT4_SB(sb)->s_es->s_feature_compat & cpu_to_le32(mask)) != 0)\n#define EXT4_HAS_RO_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\t((EXT4_SB(sb)->s_es->s_feature_ro_compat & cpu_to_le32(mask)) != 0)\n#define EXT4_HAS_INCOMPAT_FEATURE(sb,mask)\t\t\t\\\n\t((EXT4_SB(sb)->s_es->s_feature_incompat & cpu_to_le32(mask)) != 0)\n#define EXT4_SET_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_compat |= cpu_to_le32(mask)\n#define EXT4_SET_RO_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_ro_compat |= cpu_to_le32(mask)\n#define EXT4_SET_INCOMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_incompat |= cpu_to_le32(mask)\n#define EXT4_CLEAR_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_compat &= ~cpu_to_le32(mask)\n#define EXT4_CLEAR_RO_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_ro_compat &= ~cpu_to_le32(mask)\n#define EXT4_CLEAR_INCOMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_incompat &= ~cpu_to_le32(mask)\n\n#define EXT4_FEATURE_COMPAT_DIR_PREALLOC\t0x0001\n#define EXT4_FEATURE_COMPAT_IMAGIC_INODES\t0x0002\n#define EXT4_FEATURE_COMPAT_HAS_JOURNAL\t\t0x0004\n#define EXT4_FEATURE_COMPAT_EXT_ATTR\t\t0x0008\n#define EXT4_FEATURE_COMPAT_RESIZE_INODE\t0x0010\n#define EXT4_FEATURE_COMPAT_DIR_INDEX\t\t0x0020\n\n#define EXT4_FEATURE_RO_COMPAT_SPARSE_SUPER\t0x0001\n#define EXT4_FEATURE_RO_COMPAT_LARGE_FILE\t0x0002\n#define EXT4_FEATURE_RO_COMPAT_BTREE_DIR\t0x0004\n#define EXT4_FEATURE_RO_COMPAT_HUGE_FILE        0x0008\n#define EXT4_FEATURE_RO_COMPAT_GDT_CSUM\t\t0x0010\n#define EXT4_FEATURE_RO_COMPAT_DIR_NLINK\t0x0020\n#define EXT4_FEATURE_RO_COMPAT_EXTRA_ISIZE\t0x0040\n\n#define EXT4_FEATURE_INCOMPAT_COMPRESSION\t0x0001\n#define EXT4_FEATURE_INCOMPAT_FILETYPE\t\t0x0002\n#define EXT4_FEATURE_INCOMPAT_RECOVER\t\t0x0004 /* Needs recovery */\n#define EXT4_FEATURE_INCOMPAT_JOURNAL_DEV\t0x0008 /* Journal device */\n#define EXT4_FEATURE_INCOMPAT_META_BG\t\t0x0010\n#define EXT4_FEATURE_INCOMPAT_EXTENTS\t\t0x0040 /* extents support */\n#define EXT4_FEATURE_INCOMPAT_64BIT\t\t0x0080\n#define EXT4_FEATURE_INCOMPAT_MMP               0x0100\n#define EXT4_FEATURE_INCOMPAT_FLEX_BG\t\t0x0200\n#define EXT4_FEATURE_INCOMPAT_EA_INODE\t\t0x0400 /* EA in inode */\n#define EXT4_FEATURE_INCOMPAT_DIRDATA\t\t0x1000 /* data in dirent */\n\n#define EXT4_FEATURE_COMPAT_SUPP\tEXT2_FEATURE_COMPAT_EXT_ATTR\n#define EXT4_FEATURE_INCOMPAT_SUPP\t(EXT4_FEATURE_INCOMPAT_FILETYPE| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_RECOVER| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_META_BG| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_EXTENTS| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_64BIT| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_FLEX_BG)\n#define EXT4_FEATURE_RO_COMPAT_SUPP\t(EXT4_FEATURE_RO_COMPAT_SPARSE_SUPER| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_LARGE_FILE| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_GDT_CSUM| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_DIR_NLINK | \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_EXTRA_ISIZE | \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_BTREE_DIR |\\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_HUGE_FILE)\n\n/*\n * Default values for user and/or group using reserved blocks\n */\n#define\tEXT4_DEF_RESUID\t\t0\n#define\tEXT4_DEF_RESGID\t\t0\n\n#define EXT4_DEF_INODE_READAHEAD_BLKS\t32\n\n/*\n * Default mount options\n */\n#define EXT4_DEFM_DEBUG\t\t0x0001\n#define EXT4_DEFM_BSDGROUPS\t0x0002\n#define EXT4_DEFM_XATTR_USER\t0x0004\n#define EXT4_DEFM_ACL\t\t0x0008\n#define EXT4_DEFM_UID16\t\t0x0010\n#define EXT4_DEFM_JMODE\t\t0x0060\n#define EXT4_DEFM_JMODE_DATA\t0x0020\n#define EXT4_DEFM_JMODE_ORDERED\t0x0040\n#define EXT4_DEFM_JMODE_WBACK\t0x0060\n\n/*\n * Default journal batch times\n */\n#define EXT4_DEF_MIN_BATCH_TIME\t0\n#define EXT4_DEF_MAX_BATCH_TIME\t15000 /* 15ms */\n\n/*\n * Minimum number of groups in a flexgroup before we separate out\n * directories into the first block group of a flexgroup\n */\n#define EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME\t4\n\n/*\n * Structure of a directory entry\n */\n#define EXT4_NAME_LEN 255\n\nstruct ext4_dir_entry {\n\t__le32\tinode;\t\t\t/* Inode number */\n\t__le16\trec_len;\t\t/* Directory entry length */\n\t__le16\tname_len;\t\t/* Name length */\n\tchar\tname[EXT4_NAME_LEN];\t/* File name */\n};\n\n/*\n * The new version of the directory entry.  Since EXT4 structures are\n * stored in intel byte order, and the name_len field could never be\n * bigger than 255 chars, it's safe to reclaim the extra byte for the\n * file_type field.\n */\nstruct ext4_dir_entry_2 {\n\t__le32\tinode;\t\t\t/* Inode number */\n\t__le16\trec_len;\t\t/* Directory entry length */\n\t__u8\tname_len;\t\t/* Name length */\n\t__u8\tfile_type;\n\tchar\tname[EXT4_NAME_LEN];\t/* File name */\n};\n\n/*\n * Ext4 directory file types.  Only the low 3 bits are used.  The\n * other bits are reserved for now.\n */\n#define EXT4_FT_UNKNOWN\t\t0\n#define EXT4_FT_REG_FILE\t1\n#define EXT4_FT_DIR\t\t2\n#define EXT4_FT_CHRDEV\t\t3\n#define EXT4_FT_BLKDEV\t\t4\n#define EXT4_FT_FIFO\t\t5\n#define EXT4_FT_SOCK\t\t6\n#define EXT4_FT_SYMLINK\t\t7\n\n#define EXT4_FT_MAX\t\t8\n\n/*\n * EXT4_DIR_PAD defines the directory entries boundaries\n *\n * NOTE: It must be a multiple of 4\n */\n#define EXT4_DIR_PAD\t\t\t4\n#define EXT4_DIR_ROUND\t\t\t(EXT4_DIR_PAD - 1)\n#define EXT4_DIR_REC_LEN(name_len)\t(((name_len) + 8 + EXT4_DIR_ROUND) & \\\n\t\t\t\t\t ~EXT4_DIR_ROUND)\n#define EXT4_MAX_REC_LEN\t\t((1<<16)-1)\n\n/*\n * Hash Tree Directory indexing\n * (c) Daniel Phillips, 2001\n */\n\n#define is_dx(dir) (EXT4_HAS_COMPAT_FEATURE(dir->i_sb, \\\n\t\t\t\t      EXT4_FEATURE_COMPAT_DIR_INDEX) && \\\n\t\t      (EXT4_I(dir)->i_flags & EXT4_INDEX_FL))\n#define EXT4_DIR_LINK_MAX(dir) (!is_dx(dir) && (dir)->i_nlink >= EXT4_LINK_MAX)\n#define EXT4_DIR_LINK_EMPTY(dir) ((dir)->i_nlink == 2 || (dir)->i_nlink == 1)\n\n/* Legal values for the dx_root hash_version field: */\n\n#define DX_HASH_LEGACY\t\t0\n#define DX_HASH_HALF_MD4\t1\n#define DX_HASH_TEA\t\t2\n#define DX_HASH_LEGACY_UNSIGNED\t3\n#define DX_HASH_HALF_MD4_UNSIGNED\t4\n#define DX_HASH_TEA_UNSIGNED\t\t5\n\n#ifdef __KERNEL__\n\n/* hash info structure used by the directory hash */\nstruct dx_hash_info\n{\n\tu32\t\thash;\n\tu32\t\tminor_hash;\n\tint\t\thash_version;\n\tu32\t\t*seed;\n};\n\n#define EXT4_HTREE_EOF\t0x7fffffff\n\n/*\n * Control parameters used by ext4_htree_next_block\n */\n#define HASH_NB_ALWAYS\t\t1\n\n\n/*\n * Describe an inode's exact location on disk and in memory\n */\nstruct ext4_iloc\n{\n\tstruct buffer_head *bh;\n\tunsigned long offset;\n\text4_group_t block_group;\n};\n\nstatic inline struct ext4_inode *ext4_raw_inode(struct ext4_iloc *iloc)\n{\n\treturn (struct ext4_inode *) (iloc->bh->b_data + iloc->offset);\n}\n\n/*\n * This structure is stuffed into the struct file's private_data field\n * for directories.  It is where we put information so that we can do\n * readdir operations in hash tree order.\n */\nstruct dir_private_info {\n\tstruct rb_root\troot;\n\tstruct rb_node\t*curr_node;\n\tstruct fname\t*extra_fname;\n\tloff_t\t\tlast_pos;\n\t__u32\t\tcurr_hash;\n\t__u32\t\tcurr_minor_hash;\n\t__u32\t\tnext_hash;\n};\n\n/* calculate the first block number of the group */\nstatic inline ext4_fsblk_t\next4_group_first_block_no(struct super_block *sb, ext4_group_t group_no)\n{\n\treturn group_no * (ext4_fsblk_t)EXT4_BLOCKS_PER_GROUP(sb) +\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_first_data_block);\n}\n\n/*\n * Special error return code only used by dx_probe() and its callers.\n */\n#define ERR_BAD_DX_DIR\t-75000\n\nvoid ext4_get_group_no_and_offset(struct super_block *sb, ext4_fsblk_t blocknr,\n\t\t\text4_group_t *blockgrpp, ext4_grpblk_t *offsetp);\n\nextern struct proc_dir_entry *ext4_proc_root;\n\n/*\n * Function prototypes\n */\n\n/*\n * Ok, these declarations are also in <linux/kernel.h> but none of the\n * ext4 source programs needs to include it so they are duplicated here.\n */\n# define NORET_TYPE\t/**/\n# define ATTRIB_NORET\t__attribute__((noreturn))\n# define NORET_AND\tnoreturn,\n\n/* bitmap.c */\nextern unsigned int ext4_count_free(struct buffer_head *, unsigned);\n\n/* balloc.c */\nextern unsigned int ext4_block_group(struct super_block *sb,\n\t\t\text4_fsblk_t blocknr);\nextern ext4_grpblk_t ext4_block_group_offset(struct super_block *sb,\n\t\t\text4_fsblk_t blocknr);\nextern int ext4_bg_has_super(struct super_block *sb, ext4_group_t group);\nextern unsigned long ext4_bg_num_gdb(struct super_block *sb,\n\t\t\text4_group_t group);\nextern ext4_fsblk_t ext4_new_meta_blocks(handle_t *handle, struct inode *inode,\n\t\t\text4_fsblk_t goal, unsigned long *count, int *errp);\nextern int ext4_claim_free_blocks(struct ext4_sb_info *sbi, s64 nblocks);\nextern int ext4_has_free_blocks(struct ext4_sb_info *sbi, s64 nblocks);\nextern void ext4_add_groupblocks(handle_t *handle, struct super_block *sb,\n\t\t\t\text4_fsblk_t block, unsigned long count);\nextern ext4_fsblk_t ext4_count_free_blocks(struct super_block *);\nextern void ext4_check_blocks_bitmap(struct super_block *);\nextern struct ext4_group_desc * ext4_get_group_desc(struct super_block * sb,\n\t\t\t\t\t\t    ext4_group_t block_group,\n\t\t\t\t\t\t    struct buffer_head ** bh);\nextern int ext4_should_retry_alloc(struct super_block *sb, int *retries);\nstruct buffer_head *ext4_read_block_bitmap(struct super_block *sb,\n\t\t\t\t      ext4_group_t block_group);\nextern unsigned ext4_init_block_bitmap(struct super_block *sb,\n\t\t\t\t       struct buffer_head *bh,\n\t\t\t\t       ext4_group_t group,\n\t\t\t\t       struct ext4_group_desc *desc);\n#define ext4_free_blocks_after_init(sb, group, desc)\t\t\t\\\n\t\text4_init_block_bitmap(sb, NULL, group, desc)\n\n/* dir.c */\nextern int ext4_check_dir_entry(const char *, struct inode *,\n\t\t\t\tstruct ext4_dir_entry_2 *,\n\t\t\t\tstruct buffer_head *, unsigned int);\nextern int ext4_htree_store_dirent(struct file *dir_file, __u32 hash,\n\t\t\t\t    __u32 minor_hash,\n\t\t\t\t    struct ext4_dir_entry_2 *dirent);\nextern void ext4_htree_free_dir_info(struct dir_private_info *p);\n\n/* fsync.c */\nextern int ext4_sync_file(struct file *, struct dentry *, int);\n\n/* hash.c */\nextern int ext4fs_dirhash(const char *name, int len, struct\n\t\t\t  dx_hash_info *hinfo);\n\n/* ialloc.c */\nextern struct inode *ext4_new_inode(handle_t *, struct inode *, int,\n\t\t\t\t    const struct qstr *qstr, __u32 goal);\nextern void ext4_free_inode(handle_t *, struct inode *);\nextern struct inode * ext4_orphan_get(struct super_block *, unsigned long);\nextern unsigned long ext4_count_free_inodes(struct super_block *);\nextern unsigned long ext4_count_dirs(struct super_block *);\nextern void ext4_check_inodes_bitmap(struct super_block *);\nextern unsigned ext4_init_inode_bitmap(struct super_block *sb,\n\t\t\t\t       struct buffer_head *bh,\n\t\t\t\t       ext4_group_t group,\n\t\t\t\t       struct ext4_group_desc *desc);\nextern void mark_bitmap_end(int start_bit, int end_bit, char *bitmap);\n\n/* mballoc.c */\nextern long ext4_mb_stats;\nextern long ext4_mb_max_to_scan;\nextern int ext4_mb_init(struct super_block *, int);\nextern int ext4_mb_release(struct super_block *);\nextern ext4_fsblk_t ext4_mb_new_blocks(handle_t *,\n\t\t\t\tstruct ext4_allocation_request *, int *);\nextern int ext4_mb_reserve_blocks(struct super_block *, int);\nextern void ext4_discard_preallocations(struct inode *);\nextern int __init init_ext4_mballoc(void);\nextern void exit_ext4_mballoc(void);\nextern void ext4_free_blocks(handle_t *handle, struct inode *inode,\n\t\t\t     struct buffer_head *bh, ext4_fsblk_t block,\n\t\t\t     unsigned long count, int flags);\nextern int ext4_mb_add_groupinfo(struct super_block *sb,\n\t\text4_group_t i, struct ext4_group_desc *desc);\nextern int ext4_mb_get_buddy_cache_lock(struct super_block *, ext4_group_t);\nextern void ext4_mb_put_buddy_cache_lock(struct super_block *,\n\t\t\t\t\t\text4_group_t, int);\n/* inode.c */\nstruct buffer_head *ext4_getblk(handle_t *, struct inode *,\n\t\t\t\t\t\text4_lblk_t, int, int *);\nstruct buffer_head *ext4_bread(handle_t *, struct inode *,\n\t\t\t\t\t\text4_lblk_t, int, int *);\nint ext4_get_block(struct inode *inode, sector_t iblock,\n\t\t\t\tstruct buffer_head *bh_result, int create);\n\nextern struct inode *ext4_iget(struct super_block *, unsigned long);\nextern int  ext4_write_inode(struct inode *, int);\nextern int  ext4_setattr(struct dentry *, struct iattr *);\nextern int  ext4_getattr(struct vfsmount *mnt, struct dentry *dentry,\n\t\t\t\tstruct kstat *stat);\nextern void ext4_delete_inode(struct inode *);\nextern int  ext4_sync_inode(handle_t *, struct inode *);\nextern void ext4_dirty_inode(struct inode *);\nextern int ext4_change_inode_journal_flag(struct inode *, int);\nextern int ext4_get_inode_loc(struct inode *, struct ext4_iloc *);\nextern int ext4_can_truncate(struct inode *inode);\nextern void ext4_truncate(struct inode *);\nextern int ext4_truncate_restart_trans(handle_t *, struct inode *, int nblocks);\nextern void ext4_set_inode_flags(struct inode *);\nextern void ext4_get_inode_flags(struct ext4_inode_info *);\nextern int ext4_alloc_da_blocks(struct inode *inode);\nextern void ext4_set_aops(struct inode *inode);\nextern int ext4_writepage_trans_blocks(struct inode *);\nextern int ext4_meta_trans_blocks(struct inode *, int nrblocks, int idxblocks);\nextern int ext4_chunk_trans_blocks(struct inode *, int nrblocks);\nextern int ext4_block_truncate_page(handle_t *handle,\n\t\tstruct address_space *mapping, loff_t from);\nextern int ext4_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);\nextern qsize_t *ext4_get_reserved_space(struct inode *inode);\nextern int flush_completed_IO(struct inode *inode);\nextern void ext4_da_update_reserve_space(struct inode *inode,\n\t\t\t\t\tint used, int quota_claim);\n/* ioctl.c */\nextern long ext4_ioctl(struct file *, unsigned int, unsigned long);\nextern long ext4_compat_ioctl(struct file *, unsigned int, unsigned long);\n\n/* migrate.c */\nextern int ext4_ext_migrate(struct inode *);\n\n/* namei.c */\nextern unsigned int ext4_rec_len_from_disk(__le16 dlen, unsigned blocksize);\nextern __le16 ext4_rec_len_to_disk(unsigned len, unsigned blocksize);\nextern int ext4_orphan_add(handle_t *, struct inode *);\nextern int ext4_orphan_del(handle_t *, struct inode *);\nextern int ext4_htree_fill_tree(struct file *dir_file, __u32 start_hash,\n\t\t\t\t__u32 start_minor_hash, __u32 *next_hash);\n\n/* resize.c */\nextern int ext4_group_add(struct super_block *sb,\n\t\t\t\tstruct ext4_new_group_data *input);\nextern int ext4_group_extend(struct super_block *sb,\n\t\t\t\tstruct ext4_super_block *es,\n\t\t\t\text4_fsblk_t n_blocks_count);\n\n/* super.c */\nextern void __ext4_error(struct super_block *, const char *, const char *, ...)\n\t__attribute__ ((format (printf, 3, 4)));\n#define ext4_error(sb, message...)\t__ext4_error(sb, __func__, ## message)\nextern void __ext4_std_error(struct super_block *, const char *, int);\nextern void ext4_abort(struct super_block *, const char *, const char *, ...)\n\t__attribute__ ((format (printf, 3, 4)));\nextern void __ext4_warning(struct super_block *, const char *,\n\t\t\t  const char *, ...)\n\t__attribute__ ((format (printf, 3, 4)));\n#define ext4_warning(sb, message...)\t__ext4_warning(sb, __func__, ## message)\nextern void ext4_msg(struct super_block *, const char *, const char *, ...)\n\t__attribute__ ((format (printf, 3, 4)));\nextern void ext4_grp_locked_error(struct super_block *, ext4_group_t,\n\t\t\t\tconst char *, const char *, ...)\n\t__attribute__ ((format (printf, 4, 5)));\nextern void ext4_update_dynamic_rev(struct super_block *sb);\nextern int ext4_update_compat_feature(handle_t *handle, struct super_block *sb,\n\t\t\t\t\t__u32 compat);\nextern int ext4_update_rocompat_feature(handle_t *handle,\n\t\t\t\t\tstruct super_block *sb,\t__u32 rocompat);\nextern int ext4_update_incompat_feature(handle_t *handle,\n\t\t\t\t\tstruct super_block *sb,\t__u32 incompat);\nextern ext4_fsblk_t ext4_block_bitmap(struct super_block *sb,\n\t\t\t\t      struct ext4_group_desc *bg);\nextern ext4_fsblk_t ext4_inode_bitmap(struct super_block *sb,\n\t\t\t\t      struct ext4_group_desc *bg);\nextern ext4_fsblk_t ext4_inode_table(struct super_block *sb,\n\t\t\t\t     struct ext4_group_desc *bg);\nextern __u32 ext4_free_blks_count(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg);\nextern __u32 ext4_free_inodes_count(struct super_block *sb,\n\t\t\t\t struct ext4_group_desc *bg);\nextern __u32 ext4_used_dirs_count(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg);\nextern __u32 ext4_itable_unused_count(struct super_block *sb,\n\t\t\t\t   struct ext4_group_desc *bg);\nextern void ext4_block_bitmap_set(struct super_block *sb,\n\t\t\t\t  struct ext4_group_desc *bg, ext4_fsblk_t blk);\nextern void ext4_inode_bitmap_set(struct super_block *sb,\n\t\t\t\t  struct ext4_group_desc *bg, ext4_fsblk_t blk);\nextern void ext4_inode_table_set(struct super_block *sb,\n\t\t\t\t struct ext4_group_desc *bg, ext4_fsblk_t blk);\nextern void ext4_free_blks_set(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg, __u32 count);\nextern void ext4_free_inodes_set(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg, __u32 count);\nextern void ext4_used_dirs_set(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg, __u32 count);\nextern void ext4_itable_unused_set(struct super_block *sb,\n\t\t\t\t   struct ext4_group_desc *bg, __u32 count);\nextern __le16 ext4_group_desc_csum(struct ext4_sb_info *sbi, __u32 group,\n\t\t\t\t   struct ext4_group_desc *gdp);\nextern int ext4_group_desc_csum_verify(struct ext4_sb_info *sbi, __u32 group,\n\t\t\t\t       struct ext4_group_desc *gdp);\n\nstatic inline ext4_fsblk_t ext4_blocks_count(struct ext4_super_block *es)\n{\n\treturn ((ext4_fsblk_t)le32_to_cpu(es->s_blocks_count_hi) << 32) |\n\t\tle32_to_cpu(es->s_blocks_count_lo);\n}\n\nstatic inline ext4_fsblk_t ext4_r_blocks_count(struct ext4_super_block *es)\n{\n\treturn ((ext4_fsblk_t)le32_to_cpu(es->s_r_blocks_count_hi) << 32) |\n\t\tle32_to_cpu(es->s_r_blocks_count_lo);\n}\n\nstatic inline ext4_fsblk_t ext4_free_blocks_count(struct ext4_super_block *es)\n{\n\treturn ((ext4_fsblk_t)le32_to_cpu(es->s_free_blocks_count_hi) << 32) |\n\t\tle32_to_cpu(es->s_free_blocks_count_lo);\n}\n\nstatic inline void ext4_blocks_count_set(struct ext4_super_block *es,\n\t\t\t\t\t ext4_fsblk_t blk)\n{\n\tes->s_blocks_count_lo = cpu_to_le32((u32)blk);\n\tes->s_blocks_count_hi = cpu_to_le32(blk >> 32);\n}\n\nstatic inline void ext4_free_blocks_count_set(struct ext4_super_block *es,\n\t\t\t\t\t      ext4_fsblk_t blk)\n{\n\tes->s_free_blocks_count_lo = cpu_to_le32((u32)blk);\n\tes->s_free_blocks_count_hi = cpu_to_le32(blk >> 32);\n}\n\nstatic inline void ext4_r_blocks_count_set(struct ext4_super_block *es,\n\t\t\t\t\t   ext4_fsblk_t blk)\n{\n\tes->s_r_blocks_count_lo = cpu_to_le32((u32)blk);\n\tes->s_r_blocks_count_hi = cpu_to_le32(blk >> 32);\n}\n\nstatic inline loff_t ext4_isize(struct ext4_inode *raw_inode)\n{\n\tif (S_ISREG(le16_to_cpu(raw_inode->i_mode)))\n\t\treturn ((loff_t)le32_to_cpu(raw_inode->i_size_high) << 32) |\n\t\t\tle32_to_cpu(raw_inode->i_size_lo);\n\telse\n\t\treturn (loff_t) le32_to_cpu(raw_inode->i_size_lo);\n}\n\nstatic inline void ext4_isize_set(struct ext4_inode *raw_inode, loff_t i_size)\n{\n\traw_inode->i_size_lo = cpu_to_le32(i_size);\n\traw_inode->i_size_high = cpu_to_le32(i_size >> 32);\n}\n\nstatic inline\nstruct ext4_group_info *ext4_get_group_info(struct super_block *sb,\n\t\t\t\t\t    ext4_group_t group)\n{\n\t struct ext4_group_info ***grp_info;\n\t long indexv, indexh;\n\t grp_info = EXT4_SB(sb)->s_group_info;\n\t indexv = group >> (EXT4_DESC_PER_BLOCK_BITS(sb));\n\t indexh = group & ((EXT4_DESC_PER_BLOCK(sb)) - 1);\n\t return grp_info[indexv][indexh];\n}\n\n/*\n * Reading s_groups_count requires using smp_rmb() afterwards.  See\n * the locking protocol documented in the comments of ext4_group_add()\n * in resize.c\n */\nstatic inline ext4_group_t ext4_get_groups_count(struct super_block *sb)\n{\n\text4_group_t\tngroups = EXT4_SB(sb)->s_groups_count;\n\n\tsmp_rmb();\n\treturn ngroups;\n}\n\nstatic inline ext4_group_t ext4_flex_group(struct ext4_sb_info *sbi,\n\t\t\t\t\t     ext4_group_t block_group)\n{\n\treturn block_group >> sbi->s_log_groups_per_flex;\n}\n\nstatic inline unsigned int ext4_flex_bg_size(struct ext4_sb_info *sbi)\n{\n\treturn 1 << sbi->s_log_groups_per_flex;\n}\n\n#define ext4_std_error(sb, errno)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif ((errno))\t\t\t\t\t\t\\\n\t\t__ext4_std_error((sb), __func__, (errno));\t\\\n} while (0)\n\n#ifdef CONFIG_SMP\n/* Each CPU can accumulate percpu_counter_batch blocks in their local\n * counters. So we need to make sure we have free blocks more\n * than percpu_counter_batch  * nr_cpu_ids. Also add a window of 4 times.\n */\n#define EXT4_FREEBLOCKS_WATERMARK (4 * (percpu_counter_batch * nr_cpu_ids))\n#else\n#define EXT4_FREEBLOCKS_WATERMARK 0\n#endif\n\nstatic inline void ext4_update_i_disksize(struct inode *inode, loff_t newsize)\n{\n\t/*\n\t * XXX: replace with spinlock if seen contended -bzzz\n\t */\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\tif (newsize > EXT4_I(inode)->i_disksize)\n\t\tEXT4_I(inode)->i_disksize = newsize;\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\treturn ;\n}\n\nstruct ext4_group_info {\n\tunsigned long   bb_state;\n\tstruct rb_root  bb_free_root;\n\text4_grpblk_t\tbb_first_free;\t/* first free block */\n\text4_grpblk_t\tbb_free;\t/* total free blocks */\n\text4_grpblk_t\tbb_fragments;\t/* nr of freespace fragments */\n\tstruct          list_head bb_prealloc_list;\n#ifdef DOUBLE_CHECK\n\tvoid            *bb_bitmap;\n#endif\n\tstruct rw_semaphore alloc_sem;\n\text4_grpblk_t\tbb_counters[];\t/* Nr of free power-of-two-block\n\t\t\t\t\t * regions, index is order.\n\t\t\t\t\t * bb_counters[3] = 5 means\n\t\t\t\t\t * 5 free 8-block regions. */\n};\n\n#define EXT4_GROUP_INFO_NEED_INIT_BIT\t0\n\n#define EXT4_MB_GRP_NEED_INIT(grp)\t\\\n\t(test_bit(EXT4_GROUP_INFO_NEED_INIT_BIT, &((grp)->bb_state)))\n\n#define EXT4_MAX_CONTENTION\t\t8\n#define EXT4_CONTENTION_THRESHOLD\t2\n\nstatic inline spinlock_t *ext4_group_lock_ptr(struct super_block *sb,\n\t\t\t\t\t      ext4_group_t group)\n{\n\treturn bgl_lock_ptr(EXT4_SB(sb)->s_blockgroup_lock, group);\n}\n\n/*\n * Returns true if the filesystem is busy enough that attempts to\n * access the block group locks has run into contention.\n */\nstatic inline int ext4_fs_is_busy(struct ext4_sb_info *sbi)\n{\n\treturn (atomic_read(&sbi->s_lock_busy) > EXT4_CONTENTION_THRESHOLD);\n}\n\nstatic inline void ext4_lock_group(struct super_block *sb, ext4_group_t group)\n{\n\tspinlock_t *lock = ext4_group_lock_ptr(sb, group);\n\tif (spin_trylock(lock))\n\t\t/*\n\t\t * We're able to grab the lock right away, so drop the\n\t\t * lock contention counter.\n\t\t */\n\t\tatomic_add_unless(&EXT4_SB(sb)->s_lock_busy, -1, 0);\n\telse {\n\t\t/*\n\t\t * The lock is busy, so bump the contention counter,\n\t\t * and then wait on the spin lock.\n\t\t */\n\t\tatomic_add_unless(&EXT4_SB(sb)->s_lock_busy, 1,\n\t\t\t\t  EXT4_MAX_CONTENTION);\n\t\tspin_lock(lock);\n\t}\n}\n\nstatic inline void ext4_unlock_group(struct super_block *sb,\n\t\t\t\t\text4_group_t group)\n{\n\tspin_unlock(ext4_group_lock_ptr(sb, group));\n}\n\n/*\n * Inodes and files operations\n */\n\n/* dir.c */\nextern const struct file_operations ext4_dir_operations;\n\n/* file.c */\nextern const struct inode_operations ext4_file_inode_operations;\nextern const struct file_operations ext4_file_operations;\n\n/* namei.c */\nextern const struct inode_operations ext4_dir_inode_operations;\nextern const struct inode_operations ext4_special_inode_operations;\nextern struct dentry *ext4_get_parent(struct dentry *child);\n\n/* symlink.c */\nextern const struct inode_operations ext4_symlink_inode_operations;\nextern const struct inode_operations ext4_fast_symlink_inode_operations;\n\n/* block_validity */\nextern void ext4_release_system_zone(struct super_block *sb);\nextern int ext4_setup_system_zone(struct super_block *sb);\nextern int __init init_ext4_system_zone(void);\nextern void exit_ext4_system_zone(void);\nextern int ext4_data_block_valid(struct ext4_sb_info *sbi,\n\t\t\t\t ext4_fsblk_t start_blk,\n\t\t\t\t unsigned int count);\n\n/* extents.c */\nextern int ext4_ext_tree_init(handle_t *handle, struct inode *);\nextern int ext4_ext_writepage_trans_blocks(struct inode *, int);\nextern int ext4_ext_index_trans_blocks(struct inode *inode, int nrblocks,\n\t\t\t\t       int chunk);\nextern int ext4_ext_get_blocks(handle_t *handle, struct inode *inode,\n\t\t\t       ext4_lblk_t iblock, unsigned int max_blocks,\n\t\t\t       struct buffer_head *bh_result, int flags);\nextern void ext4_ext_truncate(struct inode *);\nextern void ext4_ext_init(struct super_block *);\nextern void ext4_ext_release(struct super_block *);\nextern long ext4_fallocate(struct inode *inode, int mode, loff_t offset,\n\t\t\t  loff_t len);\nextern int ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,\n\t\t\t  ssize_t len);\nextern int ext4_get_blocks(handle_t *handle, struct inode *inode,\n\t\t\t   sector_t block, unsigned int max_blocks,\n\t\t\t   struct buffer_head *bh, int flags);\nextern int ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t\t__u64 start, __u64 len);\n/* move_extent.c */\nextern int ext4_move_extents(struct file *o_filp, struct file *d_filp,\n\t\t\t     __u64 start_orig, __u64 start_donor,\n\t\t\t     __u64 len, __u64 *moved_len);\n\n\n/*\n * Add new method to test wether block and inode bitmaps are properly\n * initialized. With uninit_bg reading the block from disk is not enough\n * to mark the bitmap uptodate. We need to also zero-out the bitmap\n */\n#define BH_BITMAP_UPTODATE BH_JBDPrivateStart\n\nstatic inline int bitmap_uptodate(struct buffer_head *bh)\n{\n\treturn (buffer_uptodate(bh) &&\n\t\t\ttest_bit(BH_BITMAP_UPTODATE, &(bh)->b_state));\n}\nstatic inline void set_bitmap_uptodate(struct buffer_head *bh)\n{\n\tset_bit(BH_BITMAP_UPTODATE, &(bh)->b_state);\n}\n\n#endif\t/* __KERNEL__ */\n\n#endif\t/* _EXT4_H */\n", "/*\n * ext4_jbd2.h\n *\n * Written by Stephen C. Tweedie <sct@redhat.com>, 1999\n *\n * Copyright 1998--1999 Red Hat corp --- All Rights Reserved\n *\n * This file is part of the Linux kernel and is made available under\n * the terms of the GNU General Public License, version 2, or at your\n * option, any later version, incorporated herein by reference.\n *\n * Ext4-specific journaling extensions.\n */\n\n#ifndef _EXT4_JBD2_H\n#define _EXT4_JBD2_H\n\n#include <linux/fs.h>\n#include <linux/jbd2.h>\n#include \"ext4.h\"\n\n#define EXT4_JOURNAL(inode)\t(EXT4_SB((inode)->i_sb)->s_journal)\n\n/* Define the number of blocks we need to account to a transaction to\n * modify one block of data.\n *\n * We may have to touch one inode, one bitmap buffer, up to three\n * indirection blocks, the group and superblock summaries, and the data\n * block to complete the transaction.\n *\n * For extents-enabled fs we may have to allocate and modify up to\n * 5 levels of tree + root which are stored in the inode. */\n\n#define EXT4_SINGLEDATA_TRANS_BLOCKS(sb)\t\t\t\t\\\n\t(EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)   \\\n\t ? 27U : 8U)\n\n/* Extended attribute operations touch at most two data buffers,\n * two bitmap buffers, and two group summaries, in addition to the inode\n * and the superblock, which are already accounted for. */\n\n#define EXT4_XATTR_TRANS_BLOCKS\t\t6U\n\n/* Define the minimum size for a transaction which modifies data.  This\n * needs to take into account the fact that we may end up modifying two\n * quota files too (one for the group, one for the user quota).  The\n * superblock only gets updated once, of course, so don't bother\n * counting that again for the quota updates. */\n\n#define EXT4_DATA_TRANS_BLOCKS(sb)\t(EXT4_SINGLEDATA_TRANS_BLOCKS(sb) + \\\n\t\t\t\t\t EXT4_XATTR_TRANS_BLOCKS - 2 + \\\n\t\t\t\t\t EXT4_MAXQUOTAS_TRANS_BLOCKS(sb))\n\n/*\n * Define the number of metadata blocks we need to account to modify data.\n *\n * This include super block, inode block, quota blocks and xattr blocks\n */\n#define EXT4_META_TRANS_BLOCKS(sb)\t(EXT4_XATTR_TRANS_BLOCKS + \\\n\t\t\t\t\tEXT4_MAXQUOTAS_TRANS_BLOCKS(sb))\n\n/* Delete operations potentially hit one directory's namespace plus an\n * entire inode, plus arbitrary amounts of bitmap/indirection data.  Be\n * generous.  We can grow the delete transaction later if necessary. */\n\n#define EXT4_DELETE_TRANS_BLOCKS(sb)\t(2 * EXT4_DATA_TRANS_BLOCKS(sb) + 64)\n\n/* Define an arbitrary limit for the amount of data we will anticipate\n * writing to any given transaction.  For unbounded transactions such as\n * write(2) and truncate(2) we can write more than this, but we always\n * start off at the maximum transaction size and grow the transaction\n * optimistically as we go. */\n\n#define EXT4_MAX_TRANS_DATA\t\t64U\n\n/* We break up a large truncate or write transaction once the handle's\n * buffer credits gets this low, we need either to extend the\n * transaction or to start a new one.  Reserve enough space here for\n * inode, bitmap, superblock, group and indirection updates for at least\n * one block, plus two quota updates.  Quota allocations are not\n * needed. */\n\n#define EXT4_RESERVE_TRANS_BLOCKS\t12U\n\n#define EXT4_INDEX_EXTRA_TRANS_BLOCKS\t8\n\n#ifdef CONFIG_QUOTA\n/* Amount of blocks needed for quota update - we know that the structure was\n * allocated so we need to update only inode+data */\n#define EXT4_QUOTA_TRANS_BLOCKS(sb) (test_opt(sb, QUOTA) ? 2 : 0)\n/* Amount of blocks needed for quota insert/delete - we do some block writes\n * but inode, sb and group updates are done only once */\n#define EXT4_QUOTA_INIT_BLOCKS(sb) (test_opt(sb, QUOTA) ? (DQUOT_INIT_ALLOC*\\\n\t\t(EXT4_SINGLEDATA_TRANS_BLOCKS(sb)-3)+3+DQUOT_INIT_REWRITE) : 0)\n\n#define EXT4_QUOTA_DEL_BLOCKS(sb) (test_opt(sb, QUOTA) ? (DQUOT_DEL_ALLOC*\\\n\t\t(EXT4_SINGLEDATA_TRANS_BLOCKS(sb)-3)+3+DQUOT_DEL_REWRITE) : 0)\n#else\n#define EXT4_QUOTA_TRANS_BLOCKS(sb) 0\n#define EXT4_QUOTA_INIT_BLOCKS(sb) 0\n#define EXT4_QUOTA_DEL_BLOCKS(sb) 0\n#endif\n#define EXT4_MAXQUOTAS_TRANS_BLOCKS(sb) (MAXQUOTAS*EXT4_QUOTA_TRANS_BLOCKS(sb))\n#define EXT4_MAXQUOTAS_INIT_BLOCKS(sb) (MAXQUOTAS*EXT4_QUOTA_INIT_BLOCKS(sb))\n#define EXT4_MAXQUOTAS_DEL_BLOCKS(sb) (MAXQUOTAS*EXT4_QUOTA_DEL_BLOCKS(sb))\n\nint\next4_mark_iloc_dirty(handle_t *handle,\n\t\t     struct inode *inode,\n\t\t     struct ext4_iloc *iloc);\n\n/*\n * On success, We end up with an outstanding reference count against\n * iloc->bh.  This _must_ be cleaned up later.\n */\n\nint ext4_reserve_inode_write(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_iloc *iloc);\n\nint ext4_mark_inode_dirty(handle_t *handle, struct inode *inode);\n\n/*\n * Wrapper functions with which ext4 calls into JBD.\n */\nvoid ext4_journal_abort_handle(const char *caller, const char *err_fn,\n\t\tstruct buffer_head *bh, handle_t *handle, int err);\n\nint __ext4_journal_get_undo_access(const char *where, handle_t *handle,\n\t\t\t\tstruct buffer_head *bh);\n\nint __ext4_journal_get_write_access(const char *where, handle_t *handle,\n\t\t\t\tstruct buffer_head *bh);\n\nint __ext4_forget(const char *where, handle_t *handle, int is_metadata,\n\t\t  struct inode *inode, struct buffer_head *bh,\n\t\t  ext4_fsblk_t blocknr);\n\nint __ext4_journal_get_create_access(const char *where,\n\t\t\t\thandle_t *handle, struct buffer_head *bh);\n\nint __ext4_handle_dirty_metadata(const char *where, handle_t *handle,\n\t\t\t\t struct inode *inode, struct buffer_head *bh);\n\n#define ext4_journal_get_undo_access(handle, bh) \\\n\t__ext4_journal_get_undo_access(__func__, (handle), (bh))\n#define ext4_journal_get_write_access(handle, bh) \\\n\t__ext4_journal_get_write_access(__func__, (handle), (bh))\n#define ext4_forget(handle, is_metadata, inode, bh, block_nr) \\\n\t__ext4_forget(__func__, (handle), (is_metadata), (inode), (bh),\\\n\t\t      (block_nr))\n#define ext4_journal_get_create_access(handle, bh) \\\n\t__ext4_journal_get_create_access(__func__, (handle), (bh))\n#define ext4_handle_dirty_metadata(handle, inode, bh) \\\n\t__ext4_handle_dirty_metadata(__func__, (handle), (inode), (bh))\n\nhandle_t *ext4_journal_start_sb(struct super_block *sb, int nblocks);\nint __ext4_journal_stop(const char *where, handle_t *handle);\n\n#define EXT4_NOJOURNAL_MAX_REF_COUNT ((unsigned long) 4096)\n\n/* Note:  Do not use this for NULL handles.  This is only to determine if\n * a properly allocated handle is using a journal or not. */\nstatic inline int ext4_handle_valid(handle_t *handle)\n{\n\tif ((unsigned long)handle < EXT4_NOJOURNAL_MAX_REF_COUNT)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic inline void ext4_handle_sync(handle_t *handle)\n{\n\tif (ext4_handle_valid(handle))\n\t\thandle->h_sync = 1;\n}\n\nstatic inline void ext4_handle_release_buffer(handle_t *handle,\n\t\t\t\t\t\tstruct buffer_head *bh)\n{\n\tif (ext4_handle_valid(handle))\n\t\tjbd2_journal_release_buffer(handle, bh);\n}\n\nstatic inline int ext4_handle_is_aborted(handle_t *handle)\n{\n\tif (ext4_handle_valid(handle))\n\t\treturn is_handle_aborted(handle);\n\treturn 0;\n}\n\nstatic inline int ext4_handle_has_enough_credits(handle_t *handle, int needed)\n{\n\tif (ext4_handle_valid(handle) && handle->h_buffer_credits < needed)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic inline void ext4_journal_release_buffer(handle_t *handle,\n\t\t\t\t\t\tstruct buffer_head *bh)\n{\n\tif (ext4_handle_valid(handle))\n\t\tjbd2_journal_release_buffer(handle, bh);\n}\n\nstatic inline handle_t *ext4_journal_start(struct inode *inode, int nblocks)\n{\n\treturn ext4_journal_start_sb(inode->i_sb, nblocks);\n}\n\n#define ext4_journal_stop(handle) \\\n\t__ext4_journal_stop(__func__, (handle))\n\nstatic inline handle_t *ext4_journal_current_handle(void)\n{\n\treturn journal_current_handle();\n}\n\nstatic inline int ext4_journal_extend(handle_t *handle, int nblocks)\n{\n\tif (ext4_handle_valid(handle))\n\t\treturn jbd2_journal_extend(handle, nblocks);\n\treturn 0;\n}\n\nstatic inline int ext4_journal_restart(handle_t *handle, int nblocks)\n{\n\tif (ext4_handle_valid(handle))\n\t\treturn jbd2_journal_restart(handle, nblocks);\n\treturn 0;\n}\n\nstatic inline int ext4_journal_blocks_per_page(struct inode *inode)\n{\n\tif (EXT4_JOURNAL(inode) != NULL)\n\t\treturn jbd2_journal_blocks_per_page(inode);\n\treturn 0;\n}\n\nstatic inline int ext4_journal_force_commit(journal_t *journal)\n{\n\tif (journal)\n\t\treturn jbd2_journal_force_commit(journal);\n\treturn 0;\n}\n\nstatic inline int ext4_jbd2_file_inode(handle_t *handle, struct inode *inode)\n{\n\tif (ext4_handle_valid(handle))\n\t\treturn jbd2_journal_file_inode(handle, &EXT4_I(inode)->jinode);\n\treturn 0;\n}\n\nstatic inline void ext4_update_inode_fsync_trans(handle_t *handle,\n\t\t\t\t\t\t struct inode *inode,\n\t\t\t\t\t\t int datasync)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\tif (ext4_handle_valid(handle)) {\n\t\tei->i_sync_tid = handle->h_transaction->t_tid;\n\t\tif (datasync)\n\t\t\tei->i_datasync_tid = handle->h_transaction->t_tid;\n\t}\n}\n\n/* super.c */\nint ext4_force_commit(struct super_block *sb);\n\nstatic inline int ext4_should_journal_data(struct inode *inode)\n{\n\tif (EXT4_JOURNAL(inode) == NULL)\n\t\treturn 0;\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn 1;\n\tif (test_opt(inode->i_sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)\n\t\treturn 1;\n\tif (EXT4_I(inode)->i_flags & EXT4_JOURNAL_DATA_FL)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline int ext4_should_order_data(struct inode *inode)\n{\n\tif (EXT4_JOURNAL(inode) == NULL)\n\t\treturn 0;\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn 0;\n\tif (EXT4_I(inode)->i_flags & EXT4_JOURNAL_DATA_FL)\n\t\treturn 0;\n\tif (test_opt(inode->i_sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline int ext4_should_writeback_data(struct inode *inode)\n{\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn 0;\n\tif (EXT4_JOURNAL(inode) == NULL)\n\t\treturn 1;\n\tif (EXT4_I(inode)->i_flags & EXT4_JOURNAL_DATA_FL)\n\t\treturn 0;\n\tif (test_opt(inode->i_sb, DATA_FLAGS) == EXT4_MOUNT_WRITEBACK_DATA)\n\t\treturn 1;\n\treturn 0;\n}\n\n#endif\t/* _EXT4_JBD2_H */\n", "/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n *\n * Architecture independence:\n *   Copyright (c) 2005, Bull S.A.\n *   Written by Pierre Peiffer <pierre.peiffer@bull.net>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n */\n\n/*\n * Extents support for EXT4\n *\n * TODO:\n *   - ext4*_error() should be used in some situations\n *   - analyze all BUG()/BUG_ON(), use -EIO where appropriate\n *   - smart tree reduction\n */\n\n#include <linux/module.h>\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/falloc.h>\n#include <asm/uaccess.h>\n#include <linux/fiemap.h>\n#include \"ext4_jbd2.h\"\n#include \"ext4_extents.h\"\n\n\n/*\n * ext_pblock:\n * combine low and high parts of physical block number into ext4_fsblk_t\n */\next4_fsblk_t ext_pblock(struct ext4_extent *ex)\n{\n\text4_fsblk_t block;\n\n\tblock = le32_to_cpu(ex->ee_start_lo);\n\tblock |= ((ext4_fsblk_t) le16_to_cpu(ex->ee_start_hi) << 31) << 1;\n\treturn block;\n}\n\n/*\n * idx_pblock:\n * combine low and high parts of a leaf physical block number into ext4_fsblk_t\n */\next4_fsblk_t idx_pblock(struct ext4_extent_idx *ix)\n{\n\text4_fsblk_t block;\n\n\tblock = le32_to_cpu(ix->ei_leaf_lo);\n\tblock |= ((ext4_fsblk_t) le16_to_cpu(ix->ei_leaf_hi) << 31) << 1;\n\treturn block;\n}\n\n/*\n * ext4_ext_store_pblock:\n * stores a large physical block number into an extent struct,\n * breaking it into parts\n */\nvoid ext4_ext_store_pblock(struct ext4_extent *ex, ext4_fsblk_t pb)\n{\n\tex->ee_start_lo = cpu_to_le32((unsigned long) (pb & 0xffffffff));\n\tex->ee_start_hi = cpu_to_le16((unsigned long) ((pb >> 31) >> 1) & 0xffff);\n}\n\n/*\n * ext4_idx_store_pblock:\n * stores a large physical block number into an index struct,\n * breaking it into parts\n */\nstatic void ext4_idx_store_pblock(struct ext4_extent_idx *ix, ext4_fsblk_t pb)\n{\n\tix->ei_leaf_lo = cpu_to_le32((unsigned long) (pb & 0xffffffff));\n\tix->ei_leaf_hi = cpu_to_le16((unsigned long) ((pb >> 31) >> 1) & 0xffff);\n}\n\nstatic int ext4_ext_truncate_extend_restart(handle_t *handle,\n\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t    int needed)\n{\n\tint err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (handle->h_buffer_credits > needed)\n\t\treturn 0;\n\terr = ext4_journal_extend(handle, needed);\n\tif (err <= 0)\n\t\treturn err;\n\terr = ext4_truncate_restart_trans(handle, inode, needed);\n\t/*\n\t * We have dropped i_data_sem so someone might have cached again\n\t * an extent we are going to truncate.\n\t */\n\text4_ext_invalidate_cache(inode);\n\n\treturn err;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n */\nstatic int ext4_ext_get_access(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\treturn ext4_journal_get_write_access(handle, path->p_bh);\n\t}\n\t/* path points to leaf/index in inode body */\n\t/* we use in-core data, no need to protect them */\n\treturn 0;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n *  - EIO\n */\nstatic int ext4_ext_dirty(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tint err;\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\terr = ext4_handle_dirty_metadata(handle, inode, path->p_bh);\n\t} else {\n\t\t/* path points to leaf/index in inode body */\n\t\terr = ext4_mark_inode_dirty(handle, inode);\n\t}\n\treturn err;\n}\n\nstatic ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      ext4_lblk_t block)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\text4_fsblk_t bg_start;\n\text4_fsblk_t last_block;\n\text4_grpblk_t colour;\n\text4_group_t block_group;\n\tint flex_size = ext4_flex_bg_size(EXT4_SB(inode->i_sb));\n\tint depth;\n\n\tif (path) {\n\t\tstruct ext4_extent *ex;\n\t\tdepth = path->p_depth;\n\n\t\t/* try to predict block placement */\n\t\tex = path[depth].p_ext;\n\t\tif (ex)\n\t\t\treturn ext_pblock(ex)+(block-le32_to_cpu(ex->ee_block));\n\n\t\t/* it looks like index is empty;\n\t\t * try to find starting block from index itself */\n\t\tif (path[depth].p_bh)\n\t\t\treturn path[depth].p_bh->b_blocknr;\n\t}\n\n\t/* OK. use inode's group */\n\tblock_group = ei->i_block_group;\n\tif (flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) {\n\t\t/*\n\t\t * If there are at least EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME\n\t\t * block groups per flexgroup, reserve the first block \n\t\t * group for directories and special files.  Regular \n\t\t * files will start at the second block group.  This\n\t\t * tends to speed up directory access and improves \n\t\t * fsck times.\n\t\t */\n\t\tblock_group &= ~(flex_size-1);\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\tblock_group++;\n\t}\n\tbg_start = (block_group * EXT4_BLOCKS_PER_GROUP(inode->i_sb)) +\n\t\tle32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_first_data_block);\n\tlast_block = ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es) - 1;\n\n\t/*\n\t * If we are doing delayed allocation, we don't need take\n\t * colour into account.\n\t */\n\tif (test_opt(inode->i_sb, DELALLOC))\n\t\treturn bg_start;\n\n\tif (bg_start + EXT4_BLOCKS_PER_GROUP(inode->i_sb) <= last_block)\n\t\tcolour = (current->pid % 16) *\n\t\t\t(EXT4_BLOCKS_PER_GROUP(inode->i_sb) / 16);\n\telse\n\t\tcolour = (current->pid % 16) * ((last_block - bg_start) / 16);\n\treturn bg_start + colour + block;\n}\n\n/*\n * Allocation for a meta data block\n */\nstatic ext4_fsblk_t\next4_ext_new_meta_block(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path,\n\t\t\tstruct ext4_extent *ex, int *err)\n{\n\text4_fsblk_t goal, newblock;\n\n\tgoal = ext4_ext_find_goal(inode, path, le32_to_cpu(ex->ee_block));\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, NULL, err);\n\treturn newblock;\n}\n\nstatic inline int ext4_ext_space_block(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 6)\n\t\t\tsize = 6;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_block_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent_idx);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 5)\n\t\t\tsize = 5;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 3)\n\t\t\tsize = 3;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent_idx);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 4)\n\t\t\tsize = 4;\n#endif\n\t}\n\treturn size;\n}\n\n/*\n * Calculate the number of metadata blocks needed\n * to allocate @blocks\n * Worse case is one block per extent\n */\nint ext4_ext_calc_metadata_amount(struct inode *inode, sector_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint idxs, num = 0;\n\n\tidxs = ((inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t/ sizeof(struct ext4_extent_idx));\n\n\t/*\n\t * If the new delayed allocation block is contiguous with the\n\t * previous da block, it can share index blocks with the\n\t * previous block, so we only need to allocate a new index\n\t * block every idxs leaf blocks.  At ldxs**2 blocks, we need\n\t * an additional index block, and at ldxs**3 blocks, yet\n\t * another index blocks.\n\t */\n\tif (ei->i_da_metadata_calc_len &&\n\t    ei->i_da_metadata_calc_last_lblock+1 == lblock) {\n\t\tif ((ei->i_da_metadata_calc_len % idxs) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {\n\t\t\tnum++;\n\t\t\tei->i_da_metadata_calc_len = 0;\n\t\t} else\n\t\t\tei->i_da_metadata_calc_len++;\n\t\tei->i_da_metadata_calc_last_lblock++;\n\t\treturn num;\n\t}\n\n\t/*\n\t * In the worst case we need a new set of index blocks at\n\t * every level of the inode's extent tree.\n\t */\n\tei->i_da_metadata_calc_len = 1;\n\tei->i_da_metadata_calc_last_lblock = lblock;\n\treturn ext_depth(inode) + 1;\n}\n\nstatic int\next4_ext_max_entries(struct inode *inode, int depth)\n{\n\tint max;\n\n\tif (depth == ext_depth(inode)) {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_root(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_root_idx(inode, 1);\n\t} else {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_block(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_block_idx(inode, 1);\n\t}\n\n\treturn max;\n}\n\nstatic int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)\n{\n\text4_fsblk_t block = ext_pblock(ext);\n\tint len = ext4_ext_get_actual_len(ext);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, len);\n}\n\nstatic int ext4_valid_extent_idx(struct inode *inode,\n\t\t\t\tstruct ext4_extent_idx *ext_idx)\n{\n\text4_fsblk_t block = idx_pblock(ext_idx);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, 1);\n}\n\nstatic int ext4_valid_extent_entries(struct inode *inode,\n\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\tint depth)\n{\n\tstruct ext4_extent *ext;\n\tstruct ext4_extent_idx *ext_idx;\n\tunsigned short entries;\n\tif (eh->eh_entries == 0)\n\t\treturn 1;\n\n\tentries = le16_to_cpu(eh->eh_entries);\n\n\tif (depth == 0) {\n\t\t/* leaf entries */\n\t\text = EXT_FIRST_EXTENT(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent(inode, ext))\n\t\t\t\treturn 0;\n\t\t\text++;\n\t\t\tentries--;\n\t\t}\n\t} else {\n\t\text_idx = EXT_FIRST_INDEX(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent_idx(inode, ext_idx))\n\t\t\t\treturn 0;\n\t\t\text_idx++;\n\t\t\tentries--;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __ext4_ext_check(const char *function, struct inode *inode,\n\t\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\t\tint depth)\n{\n\tconst char *error_msg;\n\tint max = 0;\n\n\tif (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) {\n\t\terror_msg = \"invalid magic\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_depth) != depth)) {\n\t\terror_msg = \"unexpected eh_depth\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(eh->eh_max == 0)) {\n\t\terror_msg = \"invalid eh_max\";\n\t\tgoto corrupted;\n\t}\n\tmax = ext4_ext_max_entries(inode, depth);\n\tif (unlikely(le16_to_cpu(eh->eh_max) > max)) {\n\t\terror_msg = \"too large eh_max\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) {\n\t\terror_msg = \"invalid eh_entries\";\n\t\tgoto corrupted;\n\t}\n\tif (!ext4_valid_extent_entries(inode, eh, depth)) {\n\t\terror_msg = \"invalid extent entries\";\n\t\tgoto corrupted;\n\t}\n\treturn 0;\n\ncorrupted:\n\t__ext4_error(inode->i_sb, function,\n\t\t\t\"bad header/extent in inode #%lu: %s - magic %x, \"\n\t\t\t\"entries %u, max %u(%u), depth %u(%u)\",\n\t\t\tinode->i_ino, error_msg, le16_to_cpu(eh->eh_magic),\n\t\t\tle16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max),\n\t\t\tmax, le16_to_cpu(eh->eh_depth), depth);\n\n\treturn -EIO;\n}\n\n#define ext4_ext_check(inode, eh, depth)\t\\\n\t__ext4_ext_check(__func__, inode, eh, depth)\n\nint ext4_ext_check_inode(struct inode *inode)\n{\n\treturn ext4_ext_check(inode, ext_inode_hdr(inode), ext_depth(inode));\n}\n\n#ifdef EXT_DEBUG\nstatic void ext4_ext_show_path(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint k, l = path->p_depth;\n\n\text_debug(\"path:\");\n\tfor (k = 0; k <= l; k++, path++) {\n\t\tif (path->p_idx) {\n\t\t  ext_debug(\"  %d->%llu\", le32_to_cpu(path->p_idx->ei_block),\n\t\t\t    idx_pblock(path->p_idx));\n\t\t} else if (path->p_ext) {\n\t\t\text_debug(\"  %d:[%d]%d:%llu \",\n\t\t\t\t  le32_to_cpu(path->p_ext->ee_block),\n\t\t\t\t  ext4_ext_is_uninitialized(path->p_ext),\n\t\t\t\t  ext4_ext_get_actual_len(path->p_ext),\n\t\t\t\t  ext_pblock(path->p_ext));\n\t\t} else\n\t\t\text_debug(\"  []\");\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_leaf(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex;\n\tint i;\n\n\tif (!path)\n\t\treturn;\n\n\teh = path[depth].p_hdr;\n\tex = EXT_FIRST_EXTENT(eh);\n\n\text_debug(\"Displaying leaf extents for inode %lu\\n\", inode->i_ino);\n\n\tfor (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ex++) {\n\t\text_debug(\"%d:[%d]%d:%llu \", le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex), ext_pblock(ex));\n\t}\n\text_debug(\"\\n\");\n}\n#else\n#define ext4_ext_show_path(inode, path)\n#define ext4_ext_show_leaf(inode, path)\n#endif\n\nvoid ext4_ext_drop_refs(struct ext4_ext_path *path)\n{\n\tint depth = path->p_depth;\n\tint i;\n\n\tfor (i = 0; i <= depth; i++, path++)\n\t\tif (path->p_bh) {\n\t\t\tbrelse(path->p_bh);\n\t\t\tpath->p_bh = NULL;\n\t\t}\n}\n\n/*\n * ext4_ext_binsearch_idx:\n * binary search for the closest index of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch_idx(struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent_idx *r, *l, *m;\n\n\n\text_debug(\"binsearch for %u(idx):  \", block);\n\n\tl = EXT_FIRST_INDEX(eh) + 1;\n\tr = EXT_LAST_INDEX(eh);\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ei_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ei_block),\n\t\t\t\tm, le32_to_cpu(m->ei_block),\n\t\t\t\tr, le32_to_cpu(r->ei_block));\n\t}\n\n\tpath->p_idx = l - 1;\n\text_debug(\"  -> %d->%lld \", le32_to_cpu(path->p_idx->ei_block),\n\t\t  idx_pblock(path->p_idx));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent_idx *chix, *ix;\n\t\tint k;\n\n\t\tchix = ix = EXT_FIRST_INDEX(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) {\n\t\t  if (k != 0 &&\n\t\t      le32_to_cpu(ix->ei_block) <= le32_to_cpu(ix[-1].ei_block)) {\n\t\t\t\tprintk(KERN_DEBUG \"k=%d, ix=0x%p, \"\n\t\t\t\t       \"first=0x%p\\n\", k,\n\t\t\t\t       ix, EXT_FIRST_INDEX(eh));\n\t\t\t\tprintk(KERN_DEBUG \"%u <= %u\\n\",\n\t\t\t\t       le32_to_cpu(ix->ei_block),\n\t\t\t\t       le32_to_cpu(ix[-1].ei_block));\n\t\t\t}\n\t\t\tBUG_ON(k && le32_to_cpu(ix->ei_block)\n\t\t\t\t\t   <= le32_to_cpu(ix[-1].ei_block));\n\t\t\tif (block < le32_to_cpu(ix->ei_block))\n\t\t\t\tbreak;\n\t\t\tchix = ix;\n\t\t}\n\t\tBUG_ON(chix != path->p_idx);\n\t}\n#endif\n\n}\n\n/*\n * ext4_ext_binsearch:\n * binary search for closest extent of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch(struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent *r, *l, *m;\n\n\tif (eh->eh_entries == 0) {\n\t\t/*\n\t\t * this leaf is empty:\n\t\t * we get such a leaf in split/add case\n\t\t */\n\t\treturn;\n\t}\n\n\text_debug(\"binsearch for %u:  \", block);\n\n\tl = EXT_FIRST_EXTENT(eh) + 1;\n\tr = EXT_LAST_EXTENT(eh);\n\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ee_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ee_block),\n\t\t\t\tm, le32_to_cpu(m->ee_block),\n\t\t\t\tr, le32_to_cpu(r->ee_block));\n\t}\n\n\tpath->p_ext = l - 1;\n\text_debug(\"  -> %d:%llu:[%d]%d \",\n\t\t\tle32_to_cpu(path->p_ext->ee_block),\n\t\t\text_pblock(path->p_ext),\n\t\t\text4_ext_is_uninitialized(path->p_ext),\n\t\t\text4_ext_get_actual_len(path->p_ext));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent *chex, *ex;\n\t\tint k;\n\n\t\tchex = ex = EXT_FIRST_EXTENT(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) {\n\t\t\tBUG_ON(k && le32_to_cpu(ex->ee_block)\n\t\t\t\t\t  <= le32_to_cpu(ex[-1].ee_block));\n\t\t\tif (block < le32_to_cpu(ex->ee_block))\n\t\t\t\tbreak;\n\t\t\tchex = ex;\n\t\t}\n\t\tBUG_ON(chex != path->p_ext);\n\t}\n#endif\n\n}\n\nint ext4_ext_tree_init(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_extent_header *eh;\n\n\teh = ext_inode_hdr(inode);\n\teh->eh_depth = 0;\n\teh->eh_entries = 0;\n\teh->eh_magic = EXT4_EXT_MAGIC;\n\teh->eh_max = cpu_to_le16(ext4_ext_space_root(inode, 0));\n\text4_mark_inode_dirty(handle, inode);\n\text4_ext_invalidate_cache(inode);\n\treturn 0;\n}\n\nstruct ext4_ext_path *\next4_ext_find_extent(struct inode *inode, ext4_lblk_t block,\n\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct buffer_head *bh;\n\tshort int depth, i, ppos = 0, alloc = 0;\n\n\teh = ext_inode_hdr(inode);\n\tdepth = ext_depth(inode);\n\n\t/* account possible depth increase */\n\tif (!path) {\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 2),\n\t\t\t\tGFP_NOFS);\n\t\tif (!path)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\talloc = 1;\n\t}\n\tpath[0].p_hdr = eh;\n\tpath[0].p_bh = NULL;\n\n\ti = depth;\n\t/* walk through the tree */\n\twhile (i) {\n\t\tint need_to_validate = 0;\n\n\t\text_debug(\"depth %d: num %d, max %d\\n\",\n\t\t\t  ppos, le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\n\t\text4_ext_binsearch_idx(inode, path + ppos, block);\n\t\tpath[ppos].p_block = idx_pblock(path[ppos].p_idx);\n\t\tpath[ppos].p_depth = i;\n\t\tpath[ppos].p_ext = NULL;\n\n\t\tbh = sb_getblk(inode->i_sb, path[ppos].p_block);\n\t\tif (unlikely(!bh))\n\t\t\tgoto err;\n\t\tif (!bh_uptodate_or_lock(bh)) {\n\t\t\tif (bh_submit_read(bh) < 0) {\n\t\t\t\tput_bh(bh);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\t/* validate the extent entries */\n\t\t\tneed_to_validate = 1;\n\t\t}\n\t\teh = ext_block_hdr(bh);\n\t\tppos++;\n\t\tBUG_ON(ppos > depth);\n\t\tpath[ppos].p_bh = bh;\n\t\tpath[ppos].p_hdr = eh;\n\t\ti--;\n\n\t\tif (need_to_validate && ext4_ext_check(inode, eh, i))\n\t\t\tgoto err;\n\t}\n\n\tpath[ppos].p_depth = i;\n\tpath[ppos].p_ext = NULL;\n\tpath[ppos].p_idx = NULL;\n\n\t/* find extent */\n\text4_ext_binsearch(inode, path + ppos, block);\n\t/* if not an empty leaf */\n\tif (path[ppos].p_ext)\n\t\tpath[ppos].p_block = ext_pblock(path[ppos].p_ext);\n\n\text4_ext_show_path(inode, path);\n\n\treturn path;\n\nerr:\n\text4_ext_drop_refs(path);\n\tif (alloc)\n\t\tkfree(path);\n\treturn ERR_PTR(-EIO);\n}\n\n/*\n * ext4_ext_insert_index:\n * insert new index [@logical;@ptr] into the block at @curp;\n * check where to insert: before @curp or after @curp\n */\nint ext4_ext_insert_index(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *curp,\n\t\t\t\tint logical, ext4_fsblk_t ptr)\n{\n\tstruct ext4_extent_idx *ix;\n\tint len, err;\n\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\treturn err;\n\n\tBUG_ON(logical == le32_to_cpu(curp->p_idx->ei_block));\n\tlen = EXT_MAX_INDEX(curp->p_hdr) - curp->p_idx;\n\tif (logical > le32_to_cpu(curp->p_idx->ei_block)) {\n\t\t/* insert after */\n\t\tif (curp->p_idx != EXT_LAST_INDEX(curp->p_hdr)) {\n\t\t\tlen = (len - 1) * sizeof(struct ext4_extent_idx);\n\t\t\tlen = len < 0 ? 0 : len;\n\t\t\text_debug(\"insert new index %d after: %llu. \"\n\t\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\t\tlogical, ptr, len,\n\t\t\t\t\t(curp->p_idx + 1), (curp->p_idx + 2));\n\t\t\tmemmove(curp->p_idx + 2, curp->p_idx + 1, len);\n\t\t}\n\t\tix = curp->p_idx + 1;\n\t} else {\n\t\t/* insert before */\n\t\tlen = len * sizeof(struct ext4_extent_idx);\n\t\tlen = len < 0 ? 0 : len;\n\t\text_debug(\"insert new index %d before: %llu. \"\n\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\tlogical, ptr, len,\n\t\t\t\tcurp->p_idx, (curp->p_idx + 1));\n\t\tmemmove(curp->p_idx + 1, curp->p_idx, len);\n\t\tix = curp->p_idx;\n\t}\n\n\tix->ei_block = cpu_to_le32(logical);\n\text4_idx_store_pblock(ix, ptr);\n\tle16_add_cpu(&curp->p_hdr->eh_entries, 1);\n\n\tBUG_ON(le16_to_cpu(curp->p_hdr->eh_entries)\n\t\t\t     > le16_to_cpu(curp->p_hdr->eh_max));\n\tBUG_ON(ix > EXT_LAST_INDEX(curp->p_hdr));\n\n\terr = ext4_ext_dirty(handle, inode, curp);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_split:\n * inserts new subtree into the path, using free index entry\n * at depth @at:\n * - allocates all needed blocks (new leaf and all intermediate index blocks)\n * - makes decision where to split\n * - moves remaining extents and index entries (right to the split point)\n *   into the newly allocated blocks\n * - initializes subtree\n */\nstatic int ext4_ext_split(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int at)\n{\n\tstruct buffer_head *bh = NULL;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tstruct ext4_extent *ex;\n\tint i = at, k, m, a;\n\text4_fsblk_t newblock, oldblock;\n\t__le32 border;\n\text4_fsblk_t *ablocks = NULL; /* array of allocated blocks */\n\tint err = 0;\n\n\t/* make decision: where to split? */\n\t/* FIXME: now decision is simplest: at current extent */\n\n\t/* if current leaf will be split, then we should use\n\t * border from split point */\n\tBUG_ON(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr));\n\tif (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\tborder = path[depth].p_ext[1].ee_block;\n\t\text_debug(\"leaf will be split.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\t  le32_to_cpu(border));\n\t} else {\n\t\tborder = newext->ee_block;\n\t\text_debug(\"leaf will be added.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\tle32_to_cpu(border));\n\t}\n\n\t/*\n\t * If error occurs, then we break processing\n\t * and mark filesystem read-only. index won't\n\t * be inserted and tree will be in consistent\n\t * state. Next mount will repair buffers too.\n\t */\n\n\t/*\n\t * Get array to track all allocated blocks.\n\t * We need this to handle errors and free blocks\n\t * upon them.\n\t */\n\tablocks = kzalloc(sizeof(ext4_fsblk_t) * depth, GFP_NOFS);\n\tif (!ablocks)\n\t\treturn -ENOMEM;\n\n\t/* allocate all needed blocks */\n\text_debug(\"allocate %d blocks for indexes/leaf\\n\", depth - at);\n\tfor (a = 0; a < depth - at; a++) {\n\t\tnewblock = ext4_ext_new_meta_block(handle, inode, path,\n\t\t\t\t\t\t   newext, &err);\n\t\tif (newblock == 0)\n\t\t\tgoto cleanup;\n\t\tablocks[a] = newblock;\n\t}\n\n\t/* initialize new leaf */\n\tnewblock = ablocks[--a];\n\tBUG_ON(newblock == 0);\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err)\n\t\tgoto cleanup;\n\n\tneh = ext_block_hdr(bh);\n\tneh->eh_entries = 0;\n\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tneh->eh_depth = 0;\n\tex = EXT_FIRST_EXTENT(neh);\n\n\t/* move remainder of path[depth] to the new leaf */\n\tBUG_ON(path[depth].p_hdr->eh_entries != path[depth].p_hdr->eh_max);\n\t/* start copy from next extent */\n\t/* TODO: we could do it by single memmove */\n\tm = 0;\n\tpath[depth].p_ext++;\n\twhile (path[depth].p_ext <=\n\t\t\tEXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\text_debug(\"move %d:%llu:[%d]%d in new leaf %llu\\n\",\n\t\t\t\tle32_to_cpu(path[depth].p_ext->ee_block),\n\t\t\t\text_pblock(path[depth].p_ext),\n\t\t\t\text4_ext_is_uninitialized(path[depth].p_ext),\n\t\t\t\text4_ext_get_actual_len(path[depth].p_ext),\n\t\t\t\tnewblock);\n\t\t/*memmove(ex++, path[depth].p_ext++,\n\t\t\t\tsizeof(struct ext4_extent));\n\t\tneh->eh_entries++;*/\n\t\tpath[depth].p_ext++;\n\t\tm++;\n\t}\n\tif (m) {\n\t\tmemmove(ex, path[depth].p_ext-m, sizeof(struct ext4_extent)*m);\n\t\tle16_add_cpu(&neh->eh_entries, m);\n\t}\n\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto cleanup;\n\tbrelse(bh);\n\tbh = NULL;\n\n\t/* correct old leaf */\n\tif (m) {\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tle16_add_cpu(&path[depth].p_hdr->eh_entries, -m);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t}\n\n\t/* create intermediate indexes */\n\tk = depth - at - 1;\n\tBUG_ON(k < 0);\n\tif (k)\n\t\text_debug(\"create %d intermediate indices\\n\", k);\n\t/* insert new index into current index block */\n\t/* current depth stored in i var */\n\ti = depth - 1;\n\twhile (k--) {\n\t\toldblock = newblock;\n\t\tnewblock = ablocks[--a];\n\t\tbh = sb_getblk(inode->i_sb, newblock);\n\t\tif (!bh) {\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tlock_buffer(bh);\n\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\tneh = ext_block_hdr(bh);\n\t\tneh->eh_entries = cpu_to_le16(1);\n\t\tneh->eh_magic = EXT4_EXT_MAGIC;\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\t\tneh->eh_depth = cpu_to_le16(depth - i);\n\t\tfidx = EXT_FIRST_INDEX(neh);\n\t\tfidx->ei_block = border;\n\t\text4_idx_store_pblock(fidx, oldblock);\n\n\t\text_debug(\"int.index at %d (block %llu): %u -> %llu\\n\",\n\t\t\t\ti, newblock, le32_to_cpu(border), oldblock);\n\t\t/* copy indexes */\n\t\tm = 0;\n\t\tpath[i].p_idx++;\n\n\t\text_debug(\"cur 0x%p, last 0x%p\\n\", path[i].p_idx,\n\t\t\t\tEXT_MAX_INDEX(path[i].p_hdr));\n\t\tBUG_ON(EXT_MAX_INDEX(path[i].p_hdr) !=\n\t\t\t\tEXT_LAST_INDEX(path[i].p_hdr));\n\t\twhile (path[i].p_idx <= EXT_MAX_INDEX(path[i].p_hdr)) {\n\t\t\text_debug(\"%d: move %d:%llu in new index %llu\\n\", i,\n\t\t\t\t\tle32_to_cpu(path[i].p_idx->ei_block),\n\t\t\t\t\tidx_pblock(path[i].p_idx),\n\t\t\t\t\tnewblock);\n\t\t\t/*memmove(++fidx, path[i].p_idx++,\n\t\t\t\t\tsizeof(struct ext4_extent_idx));\n\t\t\tneh->eh_entries++;\n\t\t\tBUG_ON(neh->eh_entries > neh->eh_max);*/\n\t\t\tpath[i].p_idx++;\n\t\t\tm++;\n\t\t}\n\t\tif (m) {\n\t\t\tmemmove(++fidx, path[i].p_idx - m,\n\t\t\t\tsizeof(struct ext4_extent_idx) * m);\n\t\t\tle16_add_cpu(&neh->eh_entries, m);\n\t\t}\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tbrelse(bh);\n\t\tbh = NULL;\n\n\t\t/* correct old index */\n\t\tif (m) {\n\t\t\terr = ext4_ext_get_access(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t\tle16_add_cpu(&path[i].p_hdr->eh_entries, -m);\n\t\t\terr = ext4_ext_dirty(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t}\n\n\t\ti--;\n\t}\n\n\t/* insert new index */\n\terr = ext4_ext_insert_index(handle, inode, path + at,\n\t\t\t\t    le32_to_cpu(border), newblock);\n\ncleanup:\n\tif (bh) {\n\t\tif (buffer_locked(bh))\n\t\t\tunlock_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\tif (err) {\n\t\t/* free all allocated blocks in error case */\n\t\tfor (i = 0; i < depth; i++) {\n\t\t\tif (!ablocks[i])\n\t\t\t\tcontinue;\n\t\t\text4_free_blocks(handle, inode, 0, ablocks[i], 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\t\t}\n\t}\n\tkfree(ablocks);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_grow_indepth:\n * implements tree growing procedure:\n * - allocates new block\n * - moves top-level data (index block or leaf) into the new block\n * - initializes new top-level, creating index that points to the\n *   just created block\n */\nstatic int ext4_ext_grow_indepth(handle_t *handle, struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tstruct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *curp = path;\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tstruct buffer_head *bh;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\n\tnewblock = ext4_ext_new_meta_block(handle, inode, path, newext, &err);\n\tif (newblock == 0)\n\t\treturn err;\n\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\text4_std_error(inode->i_sb, err);\n\t\treturn err;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err) {\n\t\tunlock_buffer(bh);\n\t\tgoto out;\n\t}\n\n\t/* move top-level index/leaf into new block */\n\tmemmove(bh->b_data, curp->p_hdr, sizeof(EXT4_I(inode)->i_data));\n\n\t/* set size of new block */\n\tneh = ext_block_hdr(bh);\n\t/* old root could have indexes or leaves\n\t * so calculate e_max right way */\n\tif (ext_depth(inode))\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\telse\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto out;\n\n\t/* create index in new top-level index: num,max,pointer */\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\tgoto out;\n\n\tcurp->p_hdr->eh_magic = EXT4_EXT_MAGIC;\n\tcurp->p_hdr->eh_max = cpu_to_le16(ext4_ext_space_root_idx(inode, 0));\n\tcurp->p_hdr->eh_entries = cpu_to_le16(1);\n\tcurp->p_idx = EXT_FIRST_INDEX(curp->p_hdr);\n\n\tif (path[0].p_hdr->eh_depth)\n\t\tcurp->p_idx->ei_block =\n\t\t\tEXT_FIRST_INDEX(path[0].p_hdr)->ei_block;\n\telse\n\t\tcurp->p_idx->ei_block =\n\t\t\tEXT_FIRST_EXTENT(path[0].p_hdr)->ee_block;\n\text4_idx_store_pblock(curp->p_idx, newblock);\n\n\tneh = ext_inode_hdr(inode);\n\tfidx = EXT_FIRST_INDEX(neh);\n\text_debug(\"new root: num %d(%d), lblock %d, ptr %llu\\n\",\n\t\t  le16_to_cpu(neh->eh_entries), le16_to_cpu(neh->eh_max),\n\t\t  le32_to_cpu(fidx->ei_block), idx_pblock(fidx));\n\n\tneh->eh_depth = cpu_to_le16(path->p_depth + 1);\n\terr = ext4_ext_dirty(handle, inode, curp);\nout:\n\tbrelse(bh);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_create_new_leaf:\n * finds empty index and adds new leaf.\n * if no free index is found, then it requests in-depth growing.\n */\nstatic int ext4_ext_create_new_leaf(handle_t *handle, struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tstruct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *curp;\n\tint depth, i, err = 0;\n\nrepeat:\n\ti = depth = ext_depth(inode);\n\n\t/* walk up to the tree and look for free index entry */\n\tcurp = path + depth;\n\twhile (i > 0 && !EXT_HAS_FREE_INDEX(curp)) {\n\t\ti--;\n\t\tcurp--;\n\t}\n\n\t/* we use already allocated block for index block,\n\t * so subsequent data blocks should be contiguous */\n\tif (EXT_HAS_FREE_INDEX(curp)) {\n\t\t/* if we found index with free entry, then use that\n\t\t * entry: create all needed subtree and add new leaf */\n\t\terr = ext4_ext_split(handle, inode, path, newext, i);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t    (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path))\n\t\t\terr = PTR_ERR(path);\n\t} else {\n\t\t/* tree is full, time to grow in depth */\n\t\terr = ext4_ext_grow_indepth(handle, inode, path, newext);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t   (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * only first (depth 0 -> 1) produces free space;\n\t\t * in all other cases we have to split the grown tree\n\t\t */\n\t\tdepth = ext_depth(inode);\n\t\tif (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) {\n\t\t\t/* now we need to split */\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * search the closest allocated block to the left for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nint\next4_ext_search_left(struct inode *inode, struct ext4_ext_path *path,\n\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\tint depth, ee_len;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tBUG_ON(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex);\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tBUG_ON(ix != EXT_FIRST_INDEX(path[depth].p_hdr));\n\t\t}\n\t\treturn 0;\n\t}\n\n\tBUG_ON(*logical < (le32_to_cpu(ex->ee_block) + ee_len));\n\n\t*logical = le32_to_cpu(ex->ee_block) + ee_len - 1;\n\t*phys = ext_pblock(ex) + ee_len - 1;\n\treturn 0;\n}\n\n/*\n * search the closest allocated block to the right for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nint\next4_ext_search_right(struct inode *inode, struct ext4_ext_path *path,\n\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t block;\n\tint depth;\t/* Note, NOT eh_depth; depth from top of tree */\n\tint ee_len;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tBUG_ON(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex);\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tBUG_ON(ix != EXT_FIRST_INDEX(path[depth].p_hdr));\n\t\t}\n\t\t*logical = le32_to_cpu(ex->ee_block);\n\t\t*phys = ext_pblock(ex);\n\t\treturn 0;\n\t}\n\n\tBUG_ON(*logical < (le32_to_cpu(ex->ee_block) + ee_len));\n\n\tif (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t/* next allocated block in this leaf */\n\t\tex++;\n\t\t*logical = le32_to_cpu(ex->ee_block);\n\t\t*phys = ext_pblock(ex);\n\t\treturn 0;\n\t}\n\n\t/* go up and search for index to the right */\n\twhile (--depth >= 0) {\n\t\tix = path[depth].p_idx;\n\t\tif (ix != EXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\tgoto got_index;\n\t}\n\n\t/* we've gone up to the root and found no index to the right */\n\treturn 0;\n\ngot_index:\n\t/* we've found index to the right, let's\n\t * follow it and find the closest allocated\n\t * block to the right */\n\tix++;\n\tblock = idx_pblock(ix);\n\twhile (++depth < path->p_depth) {\n\t\tbh = sb_bread(inode->i_sb, block);\n\t\tif (bh == NULL)\n\t\t\treturn -EIO;\n\t\teh = ext_block_hdr(bh);\n\t\t/* subtract from p_depth to get proper eh_depth */\n\t\tif (ext4_ext_check(inode, eh, path->p_depth - depth)) {\n\t\t\tput_bh(bh);\n\t\t\treturn -EIO;\n\t\t}\n\t\tix = EXT_FIRST_INDEX(eh);\n\t\tblock = idx_pblock(ix);\n\t\tput_bh(bh);\n\t}\n\n\tbh = sb_bread(inode->i_sb, block);\n\tif (bh == NULL)\n\t\treturn -EIO;\n\teh = ext_block_hdr(bh);\n\tif (ext4_ext_check(inode, eh, path->p_depth - depth)) {\n\t\tput_bh(bh);\n\t\treturn -EIO;\n\t}\n\tex = EXT_FIRST_EXTENT(eh);\n\t*logical = le32_to_cpu(ex->ee_block);\n\t*phys = ext_pblock(ex);\n\tput_bh(bh);\n\treturn 0;\n}\n\n/*\n * ext4_ext_next_allocated_block:\n * returns allocated block in subsequent extent or EXT_MAX_BLOCK.\n * NOTE: it considers block number from index entry as\n * allocated block. Thus, index entries have to be consistent\n * with leaves.\n */\nstatic ext4_lblk_t\next4_ext_next_allocated_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn EXT_MAX_BLOCK;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\t/* leaf */\n\t\t\tif (path[depth].p_ext !=\n\t\t\t\t\tEXT_LAST_EXTENT(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_ext[1].ee_block);\n\t\t} else {\n\t\t\t/* index */\n\t\t\tif (path[depth].p_idx !=\n\t\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\t}\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCK;\n}\n\n/*\n * ext4_ext_next_leaf_block:\n * returns first allocated block from next leaf or EXT_MAX_BLOCK\n */\nstatic ext4_lblk_t ext4_ext_next_leaf_block(struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\t/* zero-tree has no leaf blocks at all */\n\tif (depth == 0)\n\t\treturn EXT_MAX_BLOCK;\n\n\t/* go to index block */\n\tdepth--;\n\n\twhile (depth >= 0) {\n\t\tif (path[depth].p_idx !=\n\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\treturn (ext4_lblk_t)\n\t\t\t\tle32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCK;\n}\n\n/*\n * ext4_ext_correct_indexes:\n * if leaf gets modified and modified extent is first in the leaf,\n * then we have to correct all indexes above.\n * TODO: do we need to correct tree in all cases?\n */\nstatic int ext4_ext_correct_indexes(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\t__le32 border;\n\tint k, err = 0;\n\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tBUG_ON(ex == NULL);\n\tBUG_ON(eh == NULL);\n\n\tif (depth == 0) {\n\t\t/* there is no tree at all */\n\t\treturn 0;\n\t}\n\n\tif (ex != EXT_FIRST_EXTENT(eh)) {\n\t\t/* we correct tree if first leaf got modified only */\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: we need correction if border is smaller than current one\n\t */\n\tk = depth - 1;\n\tborder = path[depth].p_ext->ee_block;\n\terr = ext4_ext_get_access(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\tpath[k].p_idx->ei_block = border;\n\terr = ext4_ext_dirty(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\n\twhile (k--) {\n\t\t/* change all left-side indexes */\n\t\tif (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr))\n\t\t\tbreak;\n\t\terr = ext4_ext_get_access(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath[k].p_idx->ei_block = border;\n\t\terr = ext4_ext_dirty(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint\next4_can_extents_be_merged(struct inode *inode, struct ext4_extent *ex1,\n\t\t\t\tstruct ext4_extent *ex2)\n{\n\tunsigned short ext1_ee_len, ext2_ee_len, max_len;\n\n\t/*\n\t * Make sure that either both extents are uninitialized, or\n\t * both are _not_.\n\t */\n\tif (ext4_ext_is_uninitialized(ex1) ^ ext4_ext_is_uninitialized(ex2))\n\t\treturn 0;\n\n\tif (ext4_ext_is_uninitialized(ex1))\n\t\tmax_len = EXT_UNINIT_MAX_LEN;\n\telse\n\t\tmax_len = EXT_INIT_MAX_LEN;\n\n\text1_ee_len = ext4_ext_get_actual_len(ex1);\n\text2_ee_len = ext4_ext_get_actual_len(ex2);\n\n\tif (le32_to_cpu(ex1->ee_block) + ext1_ee_len !=\n\t\t\tle32_to_cpu(ex2->ee_block))\n\t\treturn 0;\n\n\t/*\n\t * To allow future support for preallocated extents to be added\n\t * as an RO_COMPAT feature, refuse to merge to extents if\n\t * this can result in the top bit of ee_len being set.\n\t */\n\tif (ext1_ee_len + ext2_ee_len > max_len)\n\t\treturn 0;\n#ifdef AGGRESSIVE_TEST\n\tif (ext1_ee_len >= 4)\n\t\treturn 0;\n#endif\n\n\tif (ext_pblock(ex1) + ext1_ee_len == ext_pblock(ex2))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function tries to merge the \"ex\" extent to the next extent in the tree.\n * It always tries to merge towards right. If you want to merge towards\n * left, pass \"ex - 1\" as argument instead of \"ex\".\n * Returns 0 if the extents (ex and ex+1) were _not_ merged and returns\n * 1 if they got merged.\n */\nint ext4_ext_try_to_merge(struct inode *inode,\n\t\t\t  struct ext4_ext_path *path,\n\t\t\t  struct ext4_extent *ex)\n{\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth, len;\n\tint merge_done = 0;\n\tint uninitialized = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\twhile (ex < EXT_LAST_EXTENT(eh)) {\n\t\tif (!ext4_can_extents_be_merged(inode, ex, ex + 1))\n\t\t\tbreak;\n\t\t/* merge with next extent! */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t+ ext4_ext_get_actual_len(ex + 1));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\n\t\tif (ex + 1 < EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = (EXT_LAST_EXTENT(eh) - ex - 1)\n\t\t\t\t* sizeof(struct ext4_extent);\n\t\t\tmemmove(ex + 1, ex + 2, len);\n\t\t}\n\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\tmerge_done = 1;\n\t\tWARN_ON(eh->eh_entries == 0);\n\t\tif (!eh->eh_entries)\n\t\t\text4_error(inode->i_sb,\n\t\t\t\t   \"inode#%lu, eh->eh_entries = 0!\",\n\t\t\t\t   inode->i_ino);\n\t}\n\n\treturn merge_done;\n}\n\n/*\n * check if a portion of the \"newext\" extent overlaps with an\n * existing extent.\n *\n * If there is an overlap discovered, it updates the length of the newext\n * such that there will be no overlap, and then returns 1.\n * If there is no overlap found, it returns 0.\n */\nunsigned int ext4_ext_check_overlap(struct inode *inode,\n\t\t\t\t    struct ext4_extent *newext,\n\t\t\t\t    struct ext4_ext_path *path)\n{\n\text4_lblk_t b1, b2;\n\tunsigned int depth, len1;\n\tunsigned int ret = 0;\n\n\tb1 = le32_to_cpu(newext->ee_block);\n\tlen1 = ext4_ext_get_actual_len(newext);\n\tdepth = ext_depth(inode);\n\tif (!path[depth].p_ext)\n\t\tgoto out;\n\tb2 = le32_to_cpu(path[depth].p_ext->ee_block);\n\n\t/*\n\t * get the next allocated block if the extent in the path\n\t * is before the requested block(s)\n\t */\n\tif (b2 < b1) {\n\t\tb2 = ext4_ext_next_allocated_block(path);\n\t\tif (b2 == EXT_MAX_BLOCK)\n\t\t\tgoto out;\n\t}\n\n\t/* check for wrap through zero on extent logical start block*/\n\tif (b1 + len1 < b1) {\n\t\tlen1 = EXT_MAX_BLOCK - b1;\n\t\tnewext->ee_len = cpu_to_le16(len1);\n\t\tret = 1;\n\t}\n\n\t/* check for overlap */\n\tif (b1 + len1 > b2) {\n\t\tnewext->ee_len = cpu_to_le16(b2 - b1);\n\t\tret = 1;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * ext4_ext_insert_extent:\n * tries to merge requsted extent into the existing extent or\n * inserts requested extent as new one into the tree,\n * creating new leaf in the no-space case.\n */\nint ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int flag)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tunsigned uninitialized = 0;\n\n\tBUG_ON(ext4_ext_get_actual_len(newext) == 0);\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tBUG_ON(path[depth].p_hdr == NULL);\n\n\t/* try to insert block into found extent and return */\n\tif (ex && (flag != EXT4_GET_BLOCKS_PRE_IO)\n\t\t&& ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\text_debug(\"append [%d]%d block to %d:[%d]%d (from %llu)\\n\",\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_is_uninitialized(ex),\n\t\t\t\text4_ext_get_actual_len(ex), ext_pblock(ex));\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t/*\n\t\t * ext4_can_extents_be_merged should have checked that either\n\t\t * both extents are uninitialized, or both aren't. Thus we\n\t\t * need to check only one of them here.\n\t\t */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\teh = path[depth].p_hdr;\n\t\tnearex = ex;\n\t\tgoto merge;\n\t}\n\nrepeat:\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = ext4_ext_next_leaf_block(inode, path);\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block)\n\t    && next != EXT_MAX_BLOCK) {\n\t\text_debug(\"next leaf block - %d\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_ext_find_extent(inode, next, NULL);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isnt full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto repeat;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\terr = ext4_ext_create_new_leaf(handle, inode, path, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %d:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tpath[depth].p_ext = EXT_FIRST_EXTENT(eh);\n\t} else if (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n/*\t\tBUG_ON(newext->ee_block == nearex->ee_block); */\n\t\tif (nearex != EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = EXT_MAX_EXTENT(eh) - nearex;\n\t\t\tlen = (len - 1) * sizeof(struct ext4_extent);\n\t\t\tlen = len < 0 ? 0 : len;\n\t\t\text_debug(\"insert %d:%llu:[%d]%d after: nearest 0x%p, \"\n\t\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\t\tmemmove(nearex + 2, nearex + 1, len);\n\t\t}\n\t\tpath[depth].p_ext = nearex + 1;\n\t} else {\n\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\tlen = (EXT_MAX_EXTENT(eh) - nearex) * sizeof(struct ext4_extent);\n\t\tlen = len < 0 ? 0 : len;\n\t\text_debug(\"insert %d:%llu:[%d]%d before: nearest 0x%p, \"\n\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\tmemmove(nearex + 1, nearex, len);\n\t\tpath[depth].p_ext = nearex;\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tnearex = path[depth].p_ext;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents to the right */\n\tif (flag != EXT4_GET_BLOCKS_PRE_IO)\n\t\text4_ext_try_to_merge(inode, path, nearex);\n\n\t/* try to merge extents to the left */\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\ncleanup:\n\tif (npath) {\n\t\text4_ext_drop_refs(npath);\n\t\tkfree(npath);\n\t}\n\text4_ext_invalidate_cache(inode);\n\treturn err;\n}\n\nint ext4_ext_walk_space(struct inode *inode, ext4_lblk_t block,\n\t\t\text4_lblk_t num, ext_prepare_callback func,\n\t\t\tvoid *cbdata)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_ext_cache cbex;\n\tstruct ext4_extent *ex;\n\text4_lblk_t next, start = 0, end = 0;\n\text4_lblk_t last = block + num;\n\tint depth, exists, err = 0;\n\n\tBUG_ON(func == NULL);\n\tBUG_ON(inode == NULL);\n\n\twhile (block < last && block != EXT_MAX_BLOCK) {\n\t\tnum = last - block;\n\t\t/* find extent for this block */\n\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\t\tpath = ext4_ext_find_extent(inode, block, path);\n\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tpath = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdepth = ext_depth(inode);\n\t\tBUG_ON(path[depth].p_hdr == NULL);\n\t\tex = path[depth].p_ext;\n\t\tnext = ext4_ext_next_allocated_block(path);\n\n\t\texists = 0;\n\t\tif (!ex) {\n\t\t\t/* there is no extent yet, so try to allocate\n\t\t\t * all requested space */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t} else if (le32_to_cpu(ex->ee_block) > block) {\n\t\t\t/* need to allocate space before found extent */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\t\t/* need to allocate space after found extent */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t\tif (end >= next)\n\t\t\t\tend = next;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)) {\n\t\t\t/*\n\t\t\t * some part of requested space is covered\n\t\t\t * by found extent\n\t\t\t */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block)\n\t\t\t\t+ ext4_ext_get_actual_len(ex);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t\texists = 1;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t\tBUG_ON(end <= start);\n\n\t\tif (!exists) {\n\t\t\tcbex.ec_block = start;\n\t\t\tcbex.ec_len = end - start;\n\t\t\tcbex.ec_start = 0;\n\t\t\tcbex.ec_type = EXT4_EXT_CACHE_GAP;\n\t\t} else {\n\t\t\tcbex.ec_block = le32_to_cpu(ex->ee_block);\n\t\t\tcbex.ec_len = ext4_ext_get_actual_len(ex);\n\t\t\tcbex.ec_start = ext_pblock(ex);\n\t\t\tcbex.ec_type = EXT4_EXT_CACHE_EXTENT;\n\t\t}\n\n\t\tBUG_ON(cbex.ec_len == 0);\n\t\terr = func(inode, path, &cbex, ex, cbdata);\n\t\text4_ext_drop_refs(path);\n\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t\tif (err == EXT_REPEAT)\n\t\t\tcontinue;\n\t\telse if (err == EXT_BREAK) {\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ext_depth(inode) != depth) {\n\t\t\t/* depth was changed. we have to realloc path */\n\t\t\tkfree(path);\n\t\t\tpath = NULL;\n\t\t}\n\n\t\tblock = cbex.ec_block + cbex.ec_len;\n\t}\n\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\n\treturn err;\n}\n\nstatic void\next4_ext_put_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t\t__u32 len, ext4_fsblk_t start, int type)\n{\n\tstruct ext4_ext_cache *cex;\n\tBUG_ON(len == 0);\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\tcex->ec_type = type;\n\tcex->ec_block = block;\n\tcex->ec_len = len;\n\tcex->ec_start = start;\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n}\n\n/*\n * ext4_ext_put_gap_in_cache:\n * calculate boundaries of the gap that the requested block fits into\n * and cache this gap\n */\nstatic void\next4_ext_put_gap_in_cache(struct inode *inode, struct ext4_ext_path *path,\n\t\t\t\text4_lblk_t block)\n{\n\tint depth = ext_depth(inode);\n\tunsigned long len;\n\text4_lblk_t lblock;\n\tstruct ext4_extent *ex;\n\n\tex = path[depth].p_ext;\n\tif (ex == NULL) {\n\t\t/* there is no extent yet, so gap is [0;-] */\n\t\tlblock = 0;\n\t\tlen = EXT_MAX_BLOCK;\n\t\text_debug(\"cache gap(whole file):\");\n\t} else if (block < le32_to_cpu(ex->ee_block)) {\n\t\tlblock = block;\n\t\tlen = le32_to_cpu(ex->ee_block) - block;\n\t\text_debug(\"cache gap(before): %u [%u:%u]\",\n\t\t\t\tblock,\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\t ext4_ext_get_actual_len(ex));\n\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\text4_lblk_t next;\n\t\tlblock = le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex);\n\n\t\tnext = ext4_ext_next_allocated_block(path);\n\t\text_debug(\"cache gap(after): [%u:%u] %u\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tblock);\n\t\tBUG_ON(next == lblock);\n\t\tlen = next - lblock;\n\t} else {\n\t\tlblock = len = 0;\n\t\tBUG();\n\t}\n\n\text_debug(\" -> %u:%lu\\n\", lblock, len);\n\text4_ext_put_in_cache(inode, lblock, len, 0, EXT4_EXT_CACHE_GAP);\n}\n\nstatic int\next4_ext_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t\tstruct ext4_extent *ex)\n{\n\tstruct ext4_ext_cache *cex;\n\tint ret = EXT4_EXT_CACHE_NO;\n\n\t/* \n\t * We borrow i_block_reservation_lock to protect i_cached_extent\n\t */\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\n\t/* has cache valid data? */\n\tif (cex->ec_type == EXT4_EXT_CACHE_NO)\n\t\tgoto errout;\n\n\tBUG_ON(cex->ec_type != EXT4_EXT_CACHE_GAP &&\n\t\t\tcex->ec_type != EXT4_EXT_CACHE_EXTENT);\n\tif (block >= cex->ec_block && block < cex->ec_block + cex->ec_len) {\n\t\tex->ee_block = cpu_to_le32(cex->ec_block);\n\t\text4_ext_store_pblock(ex, cex->ec_start);\n\t\tex->ee_len = cpu_to_le16(cex->ec_len);\n\t\text_debug(\"%u cached by %u:%u:%llu\\n\",\n\t\t\t\tblock,\n\t\t\t\tcex->ec_block, cex->ec_len, cex->ec_start);\n\t\tret = cex->ec_type;\n\t}\nerrout:\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\treturn ret;\n}\n\n/*\n * ext4_ext_rm_idx:\n * removes index from the index block.\n * It's used in truncate case only, thus all requests are for\n * last index in the block only.\n */\nstatic int ext4_ext_rm_idx(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path)\n{\n\tint err;\n\text4_fsblk_t leaf;\n\n\t/* free index block */\n\tpath--;\n\tleaf = idx_pblock(path->p_idx);\n\tBUG_ON(path->p_hdr->eh_entries == 0);\n\terr = ext4_ext_get_access(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\tle16_add_cpu(&path->p_hdr->eh_entries, -1);\n\terr = ext4_ext_dirty(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\text_debug(\"index is empty, remove it, free block %llu\\n\", leaf);\n\text4_free_blocks(handle, inode, 0, leaf, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n\treturn err;\n}\n\n/*\n * ext4_ext_calc_credits_for_single_extent:\n * This routine returns max. credits that needed to insert an extent\n * to the extent tree.\n * When pass the actual path, the caller should calculate credits\n * under i_data_sem.\n */\nint ext4_ext_calc_credits_for_single_extent(struct inode *inode, int nrblocks,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path) {\n\t\tint depth = ext_depth(inode);\n\t\tint ret = 0;\n\n\t\t/* probably there is space in leaf? */\n\t\tif (le16_to_cpu(path[depth].p_hdr->eh_entries)\n\t\t\t\t< le16_to_cpu(path[depth].p_hdr->eh_max)) {\n\n\t\t\t/*\n\t\t\t *  There are some space in the leaf tree, no\n\t\t\t *  need to account for leaf block credit\n\t\t\t *\n\t\t\t *  bitmaps and block group descriptor blocks\n\t\t\t *  and other metadat blocks still need to be\n\t\t\t *  accounted.\n\t\t\t */\n\t\t\t/* 1 bitmap, 1 block group descriptor */\n\t\t\tret = 2 + EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ext4_chunk_trans_blocks(inode, nrblocks);\n}\n\n/*\n * How many index/leaf blocks need to change/allocate to modify nrblocks?\n *\n * if nrblocks are fit in a single extent (chunk flag is 1), then\n * in the worse case, each tree level index/leaf need to be changed\n * if the tree split due to insert a new extent, then the old tree\n * index/leaf need to be updated too\n *\n * If the nrblocks are discontiguous, they could cause\n * the whole tree split more than once, but this is really rare.\n */\nint ext4_ext_index_trans_blocks(struct inode *inode, int nrblocks, int chunk)\n{\n\tint index;\n\tint depth = ext_depth(inode);\n\n\tif (chunk)\n\t\tindex = depth * 2;\n\telse\n\t\tindex = depth * 3;\n\n\treturn index;\n}\n\nstatic int ext4_remove_blocks(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_extent *ex,\n\t\t\t\text4_lblk_t from, ext4_lblk_t to)\n{\n\tunsigned short ee_len =  ext4_ext_get_actual_len(ex);\n\tint flags = EXT4_FREE_BLOCKS_FORGET;\n\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\tflags |= EXT4_FREE_BLOCKS_METADATA;\n#ifdef EXTENTS_STATS\n\t{\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\tspin_lock(&sbi->s_ext_stats_lock);\n\t\tsbi->s_ext_blocks += ee_len;\n\t\tsbi->s_ext_extents++;\n\t\tif (ee_len < sbi->s_ext_min)\n\t\t\tsbi->s_ext_min = ee_len;\n\t\tif (ee_len > sbi->s_ext_max)\n\t\t\tsbi->s_ext_max = ee_len;\n\t\tif (ext_depth(inode) > sbi->s_depth_max)\n\t\t\tsbi->s_depth_max = ext_depth(inode);\n\t\tspin_unlock(&sbi->s_ext_stats_lock);\n\t}\n#endif\n\tif (from >= le32_to_cpu(ex->ee_block)\n\t    && to == le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\t/* tail removal */\n\t\text4_lblk_t num;\n\t\text4_fsblk_t start;\n\n\t\tnum = le32_to_cpu(ex->ee_block) + ee_len - from;\n\t\tstart = ext_pblock(ex) + ee_len - num;\n\t\text_debug(\"free last %u blocks starting %llu\\n\", num, start);\n\t\text4_free_blocks(handle, inode, 0, start, num, flags);\n\t} else if (from == le32_to_cpu(ex->ee_block)\n\t\t   && to <= le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\tprintk(KERN_INFO \"strange request: removal %u-%u from %u:%u\\n\",\n\t\t\tfrom, to, le32_to_cpu(ex->ee_block), ee_len);\n\t} else {\n\t\tprintk(KERN_INFO \"strange request: removal(2) \"\n\t\t\t\t\"%u-%u from %u:%u\\n\",\n\t\t\t\tfrom, to, le32_to_cpu(ex->ee_block), ee_len);\n\t}\n\treturn 0;\n}\n\nstatic int\next4_ext_rm_leaf(handle_t *handle, struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t start)\n{\n\tint err = 0, correct_index = 0;\n\tint depth = ext_depth(inode), credits;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t a, b, block;\n\tunsigned num;\n\text4_lblk_t ex_ee_block;\n\tunsigned short ex_ee_len;\n\tunsigned uninitialized = 0;\n\tstruct ext4_extent *ex;\n\n\t/* the header must be checked already in ext4_ext_remove_space() */\n\text_debug(\"truncate since %u in leaf\\n\", start);\n\tif (!path[depth].p_hdr)\n\t\tpath[depth].p_hdr = ext_block_hdr(path[depth].p_bh);\n\teh = path[depth].p_hdr;\n\tBUG_ON(eh == NULL);\n\n\t/* find where to start removing */\n\tex = EXT_LAST_EXTENT(eh);\n\n\tex_ee_block = le32_to_cpu(ex->ee_block);\n\tex_ee_len = ext4_ext_get_actual_len(ex);\n\n\twhile (ex >= EXT_FIRST_EXTENT(eh) &&\n\t\t\tex_ee_block + ex_ee_len > start) {\n\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\telse\n\t\t\tuninitialized = 0;\n\n\t\text_debug(\"remove ext %u:[%d]%d\\n\", ex_ee_block,\n\t\t\t uninitialized, ex_ee_len);\n\t\tpath[depth].p_ext = ex;\n\n\t\ta = ex_ee_block > start ? ex_ee_block : start;\n\t\tb = ex_ee_block + ex_ee_len - 1 < EXT_MAX_BLOCK ?\n\t\t\tex_ee_block + ex_ee_len - 1 : EXT_MAX_BLOCK;\n\n\t\text_debug(\"  border %u:%u\\n\", a, b);\n\n\t\tif (a != ex_ee_block && b != ex_ee_block + ex_ee_len - 1) {\n\t\t\tblock = 0;\n\t\t\tnum = 0;\n\t\t\tBUG();\n\t\t} else if (a != ex_ee_block) {\n\t\t\t/* remove tail of the extent */\n\t\t\tblock = ex_ee_block;\n\t\t\tnum = a - block;\n\t\t} else if (b != ex_ee_block + ex_ee_len - 1) {\n\t\t\t/* remove head of the extent */\n\t\t\tblock = a;\n\t\t\tnum = b - a;\n\t\t\t/* there is no \"make a hole\" API yet */\n\t\t\tBUG();\n\t\t} else {\n\t\t\t/* remove whole extent: excellent! */\n\t\t\tblock = ex_ee_block;\n\t\t\tnum = 0;\n\t\t\tBUG_ON(a != ex_ee_block);\n\t\t\tBUG_ON(b != ex_ee_block + ex_ee_len - 1);\n\t\t}\n\n\t\t/*\n\t\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t\t * descriptor) for each block group; assume two block\n\t\t * groups plus ex_ee_len/blocks_per_block_group for\n\t\t * the worst case\n\t\t */\n\t\tcredits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));\n\t\tif (ex == EXT_FIRST_EXTENT(eh)) {\n\t\t\tcorrect_index = 1;\n\t\t\tcredits += (ext_depth(inode)) + 1;\n\t\t}\n\t\tcredits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);\n\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_remove_blocks(handle, inode, ex, a, b);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (num == 0) {\n\t\t\t/* this extent is removed; mark slot entirely unused */\n\t\t\text4_ext_store_pblock(ex, 0);\n\t\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\t}\n\n\t\tex->ee_block = cpu_to_le32(block);\n\t\tex->ee_len = cpu_to_le16(num);\n\t\t/*\n\t\t * Do not mark uninitialized if all the blocks in the\n\t\t * extent have been removed.\n\t\t */\n\t\tif (uninitialized && num)\n\t\t\text4_ext_mark_uninitialized(ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\text_debug(\"new extent: %u:%u:%llu\\n\", block, num,\n\t\t\t\text_pblock(ex));\n\t\tex--;\n\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t}\n\n\tif (correct_index && eh->eh_entries)\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\n\t/* if this leaf is free, then we should\n\t * remove it from index block above */\n\tif (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)\n\t\terr = ext4_ext_rm_idx(handle, inode, path + depth);\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_more_to_rm:\n * returns 1 if current index has to be freed (even partial)\n */\nstatic int\next4_ext_more_to_rm(struct ext4_ext_path *path)\n{\n\tBUG_ON(path->p_idx == NULL);\n\n\tif (path->p_idx < EXT_FIRST_INDEX(path->p_hdr))\n\t\treturn 0;\n\n\t/*\n\t * if truncate on deeper level happened, it wasn't partial,\n\t * so we have to consider current index for truncation\n\t */\n\tif (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tint depth = ext_depth(inode);\n\tstruct ext4_ext_path *path;\n\thandle_t *handle;\n\tint i = 0, err = 0;\n\n\text_debug(\"truncate since %u\\n\", start);\n\n\t/* probably first extent we're gonna free will be last in block */\n\thandle = ext4_journal_start(inode, depth + 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\text4_ext_invalidate_cache(inode);\n\n\t/*\n\t * We start scanning from right side, freeing all the blocks\n\t * after i_size and walking into the tree depth-wise.\n\t */\n\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 1), GFP_NOFS);\n\tif (path == NULL) {\n\t\text4_journal_stop(handle);\n\t\treturn -ENOMEM;\n\t}\n\tpath[0].p_hdr = ext_inode_hdr(inode);\n\tif (ext4_ext_check(inode, path[0].p_hdr, depth)) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\tpath[0].p_depth = depth;\n\n\twhile (i >= 0 && err == 0) {\n\t\tif (i == depth) {\n\t\t\t/* this is leaf block */\n\t\t\terr = ext4_ext_rm_leaf(handle, inode, path, start);\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* this is index block */\n\t\tif (!path[i].p_hdr) {\n\t\t\text_debug(\"initialize header\\n\");\n\t\t\tpath[i].p_hdr = ext_block_hdr(path[i].p_bh);\n\t\t}\n\n\t\tif (!path[i].p_idx) {\n\t\t\t/* this level hasn't been touched yet */\n\t\t\tpath[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;\n\t\t\text_debug(\"init index ptr: hdr 0x%p, num %d\\n\",\n\t\t\t\t  path[i].p_hdr,\n\t\t\t\t  le16_to_cpu(path[i].p_hdr->eh_entries));\n\t\t} else {\n\t\t\t/* we were already here, see at next index */\n\t\t\tpath[i].p_idx--;\n\t\t}\n\n\t\text_debug(\"level %d - index, first 0x%p, cur 0x%p\\n\",\n\t\t\t\ti, EXT_FIRST_INDEX(path[i].p_hdr),\n\t\t\t\tpath[i].p_idx);\n\t\tif (ext4_ext_more_to_rm(path + i)) {\n\t\t\tstruct buffer_head *bh;\n\t\t\t/* go to the next level */\n\t\t\text_debug(\"move to level %d (block %llu)\\n\",\n\t\t\t\t  i + 1, idx_pblock(path[i].p_idx));\n\t\t\tmemset(path + i + 1, 0, sizeof(*path));\n\t\t\tbh = sb_bread(sb, idx_pblock(path[i].p_idx));\n\t\t\tif (!bh) {\n\t\t\t\t/* should we reset i_size? */\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (WARN_ON(i + 1 > depth)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ext4_ext_check(inode, ext_block_hdr(bh),\n\t\t\t\t\t\t\tdepth - i - 1)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpath[i + 1].p_bh = bh;\n\n\t\t\t/* save actual number of indexes since this\n\t\t\t * number is changed at the next iteration */\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);\n\t\t\ti++;\n\t\t} else {\n\t\t\t/* we finished processing this index, go up */\n\t\t\tif (path[i].p_hdr->eh_entries == 0 && i > 0) {\n\t\t\t\t/* index is empty, remove it;\n\t\t\t\t * handle must be already prepared by the\n\t\t\t\t * truncatei_leaf() */\n\t\t\t\terr = ext4_ext_rm_idx(handle, inode, path + i);\n\t\t\t}\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\text_debug(\"return to level %d\\n\", i);\n\t\t}\n\t}\n\n\t/* TODO: flexible tree reduction should be here */\n\tif (path->p_hdr->eh_entries == 0) {\n\t\t/*\n\t\t * truncate to zero freed all the tree,\n\t\t * so we need to correct eh_depth\n\t\t */\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err == 0) {\n\t\t\text_inode_hdr(inode)->eh_depth = 0;\n\t\t\text_inode_hdr(inode)->eh_max =\n\t\t\t\tcpu_to_le16(ext4_ext_space_root(inode, 0));\n\t\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\t}\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\text4_journal_stop(handle);\n\n\treturn err;\n}\n\n/*\n * called at mount time\n */\nvoid ext4_ext_init(struct super_block *sb)\n{\n\t/*\n\t * possible initialization would be here\n\t */\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) {\n#if defined(AGGRESSIVE_TEST) || defined(CHECK_BINSEARCH) || defined(EXTENTS_STATS)\n\t\tprintk(KERN_INFO \"EXT4-fs: file extents enabled\");\n#ifdef AGGRESSIVE_TEST\n\t\tprintk(\", aggressive tests\");\n#endif\n#ifdef CHECK_BINSEARCH\n\t\tprintk(\", check binsearch\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tprintk(\", stats\");\n#endif\n\t\tprintk(\"\\n\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tspin_lock_init(&EXT4_SB(sb)->s_ext_stats_lock);\n\t\tEXT4_SB(sb)->s_ext_min = 1 << 30;\n\t\tEXT4_SB(sb)->s_ext_max = 0;\n#endif\n\t}\n}\n\n/*\n * called at umount time\n */\nvoid ext4_ext_release(struct super_block *sb)\n{\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS))\n\t\treturn;\n\n#ifdef EXTENTS_STATS\n\tif (EXT4_SB(sb)->s_ext_blocks && EXT4_SB(sb)->s_ext_extents) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\tprintk(KERN_ERR \"EXT4-fs: %lu blocks in %lu extents (%lu ave)\\n\",\n\t\t\tsbi->s_ext_blocks, sbi->s_ext_extents,\n\t\t\tsbi->s_ext_blocks / sbi->s_ext_extents);\n\t\tprintk(KERN_ERR \"EXT4-fs: extents: %lu min, %lu max, max depth %lu\\n\",\n\t\t\tsbi->s_ext_min, sbi->s_ext_max, sbi->s_depth_max);\n\t}\n#endif\n}\n\nstatic void bi_complete(struct bio *bio, int error)\n{\n\tcomplete((struct completion *)bio->bi_private);\n}\n\n/* FIXME!! we need to try to merge to left or right after zero-out  */\nstatic int ext4_ext_zeroout(struct inode *inode, struct ext4_extent *ex)\n{\n\tint ret = -EIO;\n\tstruct bio *bio;\n\tint blkbits, blocksize;\n\tsector_t ee_pblock;\n\tstruct completion event;\n\tunsigned int ee_len, len, done, offset;\n\n\n\tblkbits   = inode->i_blkbits;\n\tblocksize = inode->i_sb->s_blocksize;\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext_pblock(ex);\n\n\t/* convert ee_pblock to 512 byte sectors */\n\tee_pblock = ee_pblock << (blkbits - 9);\n\n\twhile (ee_len > 0) {\n\n\t\tif (ee_len > BIO_MAX_PAGES)\n\t\t\tlen = BIO_MAX_PAGES;\n\t\telse\n\t\t\tlen = ee_len;\n\n\t\tbio = bio_alloc(GFP_NOIO, len);\n\t\tbio->bi_sector = ee_pblock;\n\t\tbio->bi_bdev   = inode->i_sb->s_bdev;\n\n\t\tdone = 0;\n\t\toffset = 0;\n\t\twhile (done < len) {\n\t\t\tret = bio_add_page(bio, ZERO_PAGE(0),\n\t\t\t\t\t\t\tblocksize, offset);\n\t\t\tif (ret != blocksize) {\n\t\t\t\t/*\n\t\t\t\t * We can't add any more pages because of\n\t\t\t\t * hardware limitations.  Start a new bio.\n\t\t\t\t */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdone++;\n\t\t\toffset += blocksize;\n\t\t\tif (offset >= PAGE_CACHE_SIZE)\n\t\t\t\toffset = 0;\n\t\t}\n\n\t\tinit_completion(&event);\n\t\tbio->bi_private = &event;\n\t\tbio->bi_end_io = bi_complete;\n\t\tsubmit_bio(WRITE, bio);\n\t\twait_for_completion(&event);\n\n\t\tif (test_bit(BIO_UPTODATE, &bio->bi_flags))\n\t\t\tret = 0;\n\t\telse {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tbio_put(bio);\n\t\tee_len    -= done;\n\t\tee_pblock += done  << (blkbits - 9);\n\t}\n\treturn ret;\n}\n\n#define EXT4_EXT_ZERO_LEN 7\n/*\n * This function is called by ext4_ext_get_blocks() if someone tries to write\n * to an uninitialized extent. It may result in splitting the uninitialized\n * extent into multiple extents (upto three - one initialized and two\n * uninitialized).\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be initialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n */\nstatic int ext4_ext_convert_to_initialized(handle_t *handle,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\t\text4_lblk_t iblock,\n\t\t\t\t\t\tunsigned int max_blocks)\n{\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex1 = NULL;\n\tstruct ext4_extent *ex2 = NULL;\n\tstruct ext4_extent *ex3 = NULL;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t ee_block;\n\tunsigned int allocated, ee_len, depth;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\tint ret = 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tallocated = ee_len - (iblock - ee_block);\n\tnewblock = iblock - ee_block + ext_pblock(ex);\n\tex2 = ex;\n\torig_ex.ee_block = ex->ee_block;\n\torig_ex.ee_len   = cpu_to_le16(ee_len);\n\text4_ext_store_pblock(&orig_ex, ext_pblock(ex));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* If extent has less than 2*EXT4_EXT_ZERO_LEN zerout directly */\n\tif (ee_len <= 2*EXT4_EXT_ZERO_LEN) {\n\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_block = orig_ex.ee_block;\n\t\tex->ee_len   = orig_ex.ee_len;\n\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t/* zeroed the full extent */\n\t\treturn allocated;\n\t}\n\n\t/* ex1: ee_block to iblock - 1 : uninitialized */\n\tif (iblock > ee_block) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(iblock - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/*\n\t * for sanity, update the length of the ex2 extent before\n\t * we insert ex3, if ex1 is NULL. This is to avoid temporary\n\t * overlap of blocks.\n\t */\n\tif (!ex1 && allocated > max_blocks)\n\t\tex2->ee_len = cpu_to_le16(max_blocks);\n\t/* ex3: to ee_block + ee_len : uninitialised */\n\tif (allocated > max_blocks) {\n\t\tunsigned int newdepth;\n\t\t/* If extent has less than EXT4_EXT_ZERO_LEN zerout directly */\n\t\tif (allocated <= EXT4_EXT_ZERO_LEN) {\n\t\t\t/*\n\t\t\t * iblock == ee_block is handled by the zerouout\n\t\t\t * at the beginning.\n\t\t\t * Mark first half uninitialized.\n\t\t\t * Mark second half initialized and zero out the\n\t\t\t * initialized extent\n\t\t\t */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = cpu_to_le16(ee_len - allocated);\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\n\t\t\tex3 = &newex;\n\t\t\tex3->ee_block = cpu_to_le32(iblock);\n\t\t\text4_ext_store_pblock(ex3, newblock);\n\t\t\tex3->ee_len = cpu_to_le16(allocated);\n\t\t\terr = ext4_ext_insert_extent(handle, inode, path,\n\t\t\t\t\t\t\tex3, 0);\n\t\t\tif (err == -ENOSPC) {\n\t\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto fix_extent_len;\n\t\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t\t/* blocks available from iblock */\n\t\t\t\treturn allocated;\n\n\t\t\t} else if (err)\n\t\t\t\tgoto fix_extent_len;\n\n\t\t\t/*\n\t\t\t * We need to zero out the second half because\n\t\t\t * an fallocate request can update file size and\n\t\t\t * converting the second half to initialized extent\n\t\t\t * implies that we can leak some junk data to user\n\t\t\t * space.\n\t\t\t */\n\t\t\terr =  ext4_ext_zeroout(inode, ex3);\n\t\t\tif (err) {\n\t\t\t\t/*\n\t\t\t\t * We should actually mark the\n\t\t\t\t * second half as uninit and return error\n\t\t\t\t * Insert would have changed the extent\n\t\t\t\t */\n\t\t\t\tdepth = ext_depth(inode);\n\t\t\t\text4_ext_drop_refs(path);\n\t\t\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t\t\t\t\tiblock, path);\n\t\t\t\tif (IS_ERR(path)) {\n\t\t\t\t\terr = PTR_ERR(path);\n\t\t\t\t\treturn err;\n\t\t\t\t}\n\t\t\t\t/* get the second half extent details */\n\t\t\t\tex = path[depth].p_ext;\n\t\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t\t\tpath + depth);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\text4_ext_mark_uninitialized(ex);\n\t\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* zeroed the second half */\n\t\t\treturn allocated;\n\t\t}\n\t\tex3 = &newex;\n\t\tex3->ee_block = cpu_to_le32(iblock + max_blocks);\n\t\text4_ext_store_pblock(ex3, newblock + max_blocks);\n\t\tex3->ee_len = cpu_to_le16(allocated - max_blocks);\n\t\text4_ext_mark_uninitialized(ex3);\n\t\terr = ext4_ext_insert_extent(handle, inode, path, ex3, 0);\n\t\tif (err == -ENOSPC) {\n\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tif (err)\n\t\t\t\tgoto fix_extent_len;\n\t\t\t/* update the extent length and mark as initialized */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t/* zeroed the full extent */\n\t\t\t/* blocks available from iblock */\n\t\t\treturn allocated;\n\n\t\t} else if (err)\n\t\t\tgoto fix_extent_len;\n\t\t/*\n\t\t * The depth, and hence eh & ex might change\n\t\t * as part of the insert above.\n\t\t */\n\t\tnewdepth = ext_depth(inode);\n\t\t/*\n\t\t * update the extent length after successful insert of the\n\t\t * split extent\n\t\t */\n\t\torig_ex.ee_len = cpu_to_le16(ee_len -\n\t\t\t\t\t\text4_ext_get_actual_len(ex3));\n\t\tdepth = newdepth;\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode, iblock, path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\t\teh = path[depth].p_hdr;\n\t\tex = path[depth].p_ext;\n\t\tif (ex2 != &newex)\n\t\t\tex2 = ex;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tallocated = max_blocks;\n\n\t\t/* If extent has less than EXT4_EXT_ZERO_LEN and we are trying\n\t\t * to insert a extent in the middle zerout directly\n\t\t * otherwise give the extent a chance to merge to left\n\t\t */\n\t\tif (le16_to_cpu(orig_ex.ee_len) <= EXT4_EXT_ZERO_LEN &&\n\t\t\t\t\t\t\tiblock != ee_block) {\n\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tif (err)\n\t\t\t\tgoto fix_extent_len;\n\t\t\t/* update the extent length and mark as initialized */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t/* zero out the first half */\n\t\t\t/* blocks available from iblock */\n\t\t\treturn allocated;\n\t\t}\n\t}\n\t/*\n\t * If there was a change of depth as part of the\n\t * insertion of ex3 above, we need to update the length\n\t * of the ex1 extent again here\n\t */\n\tif (ex1 && ex1 != ex) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(iblock - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/* ex2: iblock to iblock + maxblocks-1 : initialised */\n\tex2->ee_block = cpu_to_le32(iblock);\n\text4_ext_store_pblock(ex2, newblock);\n\tex2->ee_len = cpu_to_le16(allocated);\n\tif (ex2 != ex)\n\t\tgoto insert;\n\t/*\n\t * New (initialized) extent starts from the first block\n\t * in the current extent. i.e., ex2 == ex\n\t * We have to see if it can be merged with the extent\n\t * on the left.\n\t */\n\tif (ex2 > EXT_FIRST_EXTENT(eh)) {\n\t\t/*\n\t\t * To merge left, pass \"ex2 - 1\" to try_to_merge(),\n\t\t * since it merges towards right _only_.\n\t\t */\n\t\tret = ext4_ext_try_to_merge(inode, path, ex2 - 1);\n\t\tif (ret) {\n\t\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tdepth = ext_depth(inode);\n\t\t\tex2--;\n\t\t}\n\t}\n\t/*\n\t * Try to Merge towards right. This might be required\n\t * only when the whole extent is being written to.\n\t * i.e. ex2 == ex and ex3 == NULL.\n\t */\n\tif (!ex3) {\n\t\tret = ext4_ext_try_to_merge(inode, path, ex2);\n\t\tif (ret) {\n\t\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tgoto out;\ninsert:\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, 0);\n\tif (err == -ENOSPC) {\n\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_block = orig_ex.ee_block;\n\t\tex->ee_len   = orig_ex.ee_len;\n\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t/* zero out the first half */\n\t\treturn allocated;\n\t} else if (err)\n\t\tgoto fix_extent_len;\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err ? err : allocated;\n\nfix_extent_len:\n\tex->ee_block = orig_ex.ee_block;\n\tex->ee_len   = orig_ex.ee_len;\n\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\text4_ext_mark_uninitialized(ex);\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}\n\n/*\n * This function is called by ext4_ext_get_blocks() from\n * ext4_get_blocks_dio_write() when DIO to write\n * to an uninitialized extent.\n *\n * Writing to an uninitized extent may result in splitting the uninitialized\n * extent into multiple /intialized unintialized extents (up to three)\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be uninitialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * One of more index blocks maybe needed if the extent tree grow after\n * the unintialized extent split. To prevent ENOSPC occur at the IO\n * complete, we need to split the uninitialized extent before DIO submit\n * the IO. The uninitilized extent called at this time will be split\n * into three uninitialized extent(at most). After IO complete, the part\n * being filled will be convert to initialized by the end_io callback function\n * via ext4_convert_unwritten_extents().\n *\n * Returns the size of uninitialized extent to be written on success.\n */\nstatic int ext4_split_unwritten_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\text4_lblk_t iblock,\n\t\t\t\t\tunsigned int max_blocks,\n\t\t\t\t\tint flags)\n{\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex1 = NULL;\n\tstruct ext4_extent *ex2 = NULL;\n\tstruct ext4_extent *ex3 = NULL;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t ee_block;\n\tunsigned int allocated, ee_len, depth;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\n\text_debug(\"ext4_split_unwritten_extents: inode %lu,\"\n\t\t  \"iblock %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t  (unsigned long long)iblock, max_blocks);\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tallocated = ee_len - (iblock - ee_block);\n\tnewblock = iblock - ee_block + ext_pblock(ex);\n\tex2 = ex;\n\torig_ex.ee_block = ex->ee_block;\n\torig_ex.ee_len   = cpu_to_le16(ee_len);\n\text4_ext_store_pblock(&orig_ex, ext_pblock(ex));\n\n\t/*\n \t * If the uninitialized extent begins at the same logical\n \t * block where the write begins, and the write completely\n \t * covers the extent, then we don't need to split it.\n \t */\n\tif ((iblock == ee_block) && (allocated <= max_blocks))\n\t\treturn allocated;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* ex1: ee_block to iblock - 1 : uninitialized */\n\tif (iblock > ee_block) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(iblock - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/*\n\t * for sanity, update the length of the ex2 extent before\n\t * we insert ex3, if ex1 is NULL. This is to avoid temporary\n\t * overlap of blocks.\n\t */\n\tif (!ex1 && allocated > max_blocks)\n\t\tex2->ee_len = cpu_to_le16(max_blocks);\n\t/* ex3: to ee_block + ee_len : uninitialised */\n\tif (allocated > max_blocks) {\n\t\tunsigned int newdepth;\n\t\tex3 = &newex;\n\t\tex3->ee_block = cpu_to_le32(iblock + max_blocks);\n\t\text4_ext_store_pblock(ex3, newblock + max_blocks);\n\t\tex3->ee_len = cpu_to_le16(allocated - max_blocks);\n\t\text4_ext_mark_uninitialized(ex3);\n\t\terr = ext4_ext_insert_extent(handle, inode, path, ex3, flags);\n\t\tif (err == -ENOSPC) {\n\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tif (err)\n\t\t\t\tgoto fix_extent_len;\n\t\t\t/* update the extent length and mark as initialized */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t/* zeroed the full extent */\n\t\t\t/* blocks available from iblock */\n\t\t\treturn allocated;\n\n\t\t} else if (err)\n\t\t\tgoto fix_extent_len;\n\t\t/*\n\t\t * The depth, and hence eh & ex might change\n\t\t * as part of the insert above.\n\t\t */\n\t\tnewdepth = ext_depth(inode);\n\t\t/*\n\t\t * update the extent length after successful insert of the\n\t\t * split extent\n\t\t */\n\t\torig_ex.ee_len = cpu_to_le16(ee_len -\n\t\t\t\t\t\text4_ext_get_actual_len(ex3));\n\t\tdepth = newdepth;\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode, iblock, path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\t\teh = path[depth].p_hdr;\n\t\tex = path[depth].p_ext;\n\t\tif (ex2 != &newex)\n\t\t\tex2 = ex;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tallocated = max_blocks;\n\t}\n\t/*\n\t * If there was a change of depth as part of the\n\t * insertion of ex3 above, we need to update the length\n\t * of the ex1 extent again here\n\t */\n\tif (ex1 && ex1 != ex) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(iblock - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/*\n\t * ex2: iblock to iblock + maxblocks-1 : to be direct IO written,\n\t * uninitialised still.\n\t */\n\tex2->ee_block = cpu_to_le32(iblock);\n\text4_ext_store_pblock(ex2, newblock);\n\tex2->ee_len = cpu_to_le16(allocated);\n\text4_ext_mark_uninitialized(ex2);\n\tif (ex2 != ex)\n\t\tgoto insert;\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\text_debug(\"out here\\n\");\n\tgoto out;\ninsert:\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC) {\n\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_block = orig_ex.ee_block;\n\t\tex->ee_len   = orig_ex.ee_len;\n\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t/* zero out the first half */\n\t\treturn allocated;\n\t} else if (err)\n\t\tgoto fix_extent_len;\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err ? err : allocated;\n\nfix_extent_len:\n\tex->ee_block = orig_ex.ee_block;\n\tex->ee_len   = orig_ex.ee_len;\n\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\text4_ext_mark_uninitialized(ex);\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}\nstatic int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t      struct inode *inode,\n\t\t\t\t\t      struct ext4_ext_path *path)\n{\n\tstruct ext4_extent *ex;\n\tstruct ext4_extent_header *eh;\n\tint depth;\n\tint err = 0;\n\tint ret = 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/*\n\t * We have to see if it can be merged with the extent\n\t * on the left.\n\t */\n\tif (ex > EXT_FIRST_EXTENT(eh)) {\n\t\t/*\n\t\t * To merge left, pass \"ex - 1\" to try_to_merge(),\n\t\t * since it merges towards right _only_.\n\t\t */\n\t\tret = ext4_ext_try_to_merge(inode, path, ex - 1);\n\t\tif (ret) {\n\t\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tdepth = ext_depth(inode);\n\t\t\tex--;\n\t\t}\n\t}\n\t/*\n\t * Try to Merge towards right.\n\t */\n\tret = ext4_ext_try_to_merge(inode, path, ex);\n\tif (ret) {\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tdepth = ext_depth(inode);\n\t}\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}\n\nstatic void unmap_underlying_metadata_blocks(struct block_device *bdev,\n\t\t\tsector_t block, int count)\n{\n\tint i;\n\tfor (i = 0; i < count; i++)\n                unmap_underlying_metadata(bdev, block + i);\n}\n\nstatic int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\text4_lblk_t iblock, unsigned int max_blocks,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, struct buffer_head *bh_result,\n\t\t\text4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = EXT4_I(inode)->cur_aio_dio;\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical\"\n\t\t  \"block %llu, max_blocks %u, flags %d, allocated %u\",\n\t\t  inode->i_ino, (unsigned long long)iblock, max_blocks,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif (flags == EXT4_GET_BLOCKS_PRE_IO) {\n\t\tret = ext4_split_unwritten_extents(handle,\n\t\t\t\t\t\tinode, path, iblock,\n\t\t\t\t\t\tmax_blocks, flags);\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to convertion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\tio->flag = EXT4_IO_UNWRITTEN;\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif (flags == EXT4_GET_BLOCKS_CONVERT) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0)\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tset_buffer_unwritten(bh_result);\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode,\n\t\t\t\t\t\tpath, iblock,\n\t\t\t\t\t\tmax_blocks);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tset_buffer_new(bh_result);\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > max_blocks) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + max_blocks,\n\t\t\t\t\tallocated - max_blocks);\n\t\tallocated = max_blocks;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\text4_da_update_reserve_space(inode, allocated, 0);\n\nmap_out:\n\tset_buffer_mapped(bh_result);\nout1:\n\tif (allocated > max_blocks)\n\t\tallocated = max_blocks;\n\text4_ext_show_leaf(inode, path);\n\tbh_result->b_bdev = inode->i_sb->s_bdev;\n\tbh_result->b_blocknr = newblock;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}\n/*\n * Block allocation/map/preallocation routine for extents based files\n *\n *\n * Need to be called with\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system block\n * (ie, create is zero). Otherwise down_write(&EXT4_I(inode)->i_data_sem)\n *\n * return > 0, number of of blocks already mapped/allocated\n *          if create == 0 and these are pre-allocated blocks\n *          \tbuffer head is unmapped\n *          otherwise blocks are mapped\n *\n * return = 0, if plain look up failed (blocks have not been allocated)\n *          buffer head is unmapped\n *\n * return < 0, error case.\n */\nint ext4_ext_get_blocks(handle_t *handle, struct inode *inode,\n\t\t\text4_lblk_t iblock,\n\t\t\tunsigned int max_blocks, struct buffer_head *bh_result,\n\t\t\tint flags)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent newex, *ex, *last_ex;\n\text4_fsblk_t newblock;\n\tint err = 0, depth, ret, cache_type;\n\tunsigned int allocated = 0;\n\tstruct ext4_allocation_request ar;\n\text4_io_end_t *io = EXT4_I(inode)->cur_aio_dio;\n\n\t__clear_bit(BH_New, &bh_result->b_state);\n\text_debug(\"blocks %u/%u requested for inode %lu\\n\",\n\t\t\tiblock, max_blocks, inode->i_ino);\n\n\t/* check in cache */\n\tcache_type = ext4_ext_in_cache(inode, iblock, &newex);\n\tif (cache_type) {\n\t\tif (cache_type == EXT4_EXT_CACHE_GAP) {\n\t\t\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t\t\t/*\n\t\t\t\t * block isn't allocated yet and\n\t\t\t\t * user doesn't want to allocate it\n\t\t\t\t */\n\t\t\t\tgoto out2;\n\t\t\t}\n\t\t\t/* we should allocate requested block */\n\t\t} else if (cache_type == EXT4_EXT_CACHE_EXTENT) {\n\t\t\t/* block is already allocated */\n\t\t\tnewblock = iblock\n\t\t\t\t   - le32_to_cpu(newex.ee_block)\n\t\t\t\t   + ext_pblock(&newex);\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ext4_ext_get_actual_len(&newex) -\n\t\t\t\t\t(iblock - le32_to_cpu(newex.ee_block));\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t}\n\n\t/* find extent for this block */\n\tpath = ext4_ext_find_extent(inode, iblock, NULL);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out2;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * consistent leaf must not be empty;\n\t * this situation is possible, though, _during_ tree modification;\n\t * this is why assert can't be put in ext4_ext_find_extent()\n\t */\n\tif (path[depth].p_ext == NULL && depth != 0) {\n\t\text4_error(inode->i_sb, \"bad extent address \"\n\t\t\t   \"inode: %lu, iblock: %d, depth: %d\",\n\t\t\t   inode->i_ino, iblock, depth);\n\t\terr = -EIO;\n\t\tgoto out2;\n\t}\n\teh = path[depth].p_hdr;\n\n\tex = path[depth].p_ext;\n\tif (ex) {\n\t\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\t\text4_fsblk_t ee_start = ext_pblock(ex);\n\t\tunsigned short ee_len;\n\n\t\t/*\n\t\t * Uninitialized extents are treated as holes, except that\n\t\t * we split out initialized portions during a write.\n\t\t */\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\t\t/* if found extent covers block, simply return it */\n\t\tif (iblock >= ee_block && iblock < ee_block + ee_len) {\n\t\t\tnewblock = iblock - ee_block + ee_start;\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ee_len - (iblock - ee_block);\n\t\t\text_debug(\"%u fit into %u:%d -> %llu\\n\", iblock,\n\t\t\t\t\tee_block, ee_len, newblock);\n\n\t\t\t/* Do not put uninitialized extent in the cache */\n\t\t\tif (!ext4_ext_is_uninitialized(ex)) {\n\t\t\t\text4_ext_put_in_cache(inode, ee_block,\n\t\t\t\t\t\t\tee_len, ee_start,\n\t\t\t\t\t\t\tEXT4_EXT_CACHE_EXTENT);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = ext4_ext_handle_uninitialized_extents(handle,\n\t\t\t\t\tinode, iblock, max_blocks, path,\n\t\t\t\t\tflags, allocated, bh_result, newblock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/*\n\t * requested block isn't allocated yet;\n\t * we couldn't try to create block if create flag is zero\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * put just found gap into cache to speed up\n\t\t * subsequent requests\n\t\t */\n\t\text4_ext_put_gap_in_cache(inode, path, iblock);\n\t\tgoto out2;\n\t}\n\t/*\n\t * Okay, we need to do block allocation.\n\t */\n\n\t/* find neighbour allocated blocks */\n\tar.lleft = iblock;\n\terr = ext4_ext_search_left(inode, path, &ar.lleft, &ar.pleft);\n\tif (err)\n\t\tgoto out2;\n\tar.lright = iblock;\n\terr = ext4_ext_search_right(inode, path, &ar.lright, &ar.pright);\n\tif (err)\n\t\tgoto out2;\n\n\t/*\n\t * See if request is beyond maximum number of blocks we can have in\n\t * a single extent. For an initialized extent this limit is\n\t * EXT_INIT_MAX_LEN and for an uninitialized extent this limit is\n\t * EXT_UNINIT_MAX_LEN.\n\t */\n\tif (max_blocks > EXT_INIT_MAX_LEN &&\n\t    !(flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmax_blocks = EXT_INIT_MAX_LEN;\n\telse if (max_blocks > EXT_UNINIT_MAX_LEN &&\n\t\t (flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmax_blocks = EXT_UNINIT_MAX_LEN;\n\n\t/* Check if we can really insert (iblock)::(iblock+max_blocks) extent */\n\tnewex.ee_block = cpu_to_le32(iblock);\n\tnewex.ee_len = cpu_to_le16(max_blocks);\n\terr = ext4_ext_check_overlap(inode, &newex, path);\n\tif (err)\n\t\tallocated = ext4_ext_get_actual_len(&newex);\n\telse\n\t\tallocated = max_blocks;\n\n\t/* allocate new block */\n\tar.inode = inode;\n\tar.goal = ext4_ext_find_goal(inode, path, iblock);\n\tar.logical = iblock;\n\tar.len = allocated;\n\tif (S_ISREG(inode->i_mode))\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\telse\n\t\t/* disable in-core preallocation for non-regular files */\n\t\tar.flags = 0;\n\tnewblock = ext4_mb_new_blocks(handle, &ar, &err);\n\tif (!newblock)\n\t\tgoto out2;\n\text_debug(\"allocate new block: goal %llu, found %llu/%u\\n\",\n\t\t  ar.goal, newblock, allocated);\n\n\t/* try to insert new extent into found leaf and return */\n\text4_ext_store_pblock(&newex, newblock);\n\tnewex.ee_len = cpu_to_le16(ar.len);\n\t/* Mark uninitialized */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT){\n\t\text4_ext_mark_uninitialized(&newex);\n\t\t/*\n\t\t * io_end structure was created for every async\n\t\t * direct IO write to the middle of the file.\n\t\t * To avoid unecessary convertion for every aio dio rewrite\n\t\t * to the mid of file, here we flag the IO that is really\n\t\t * need the convertion.\n\t\t * For non asycn direct IO case, flag the inode state\n\t\t * that we need to perform convertion when IO is done.\n\t\t */\n\t\tif (flags == EXT4_GET_BLOCKS_PRE_IO) {\n\t\t\tif (io)\n\t\t\t\tio->flag = EXT4_IO_UNWRITTEN;\n\t\t\telse\n\t\t\t\text4_set_inode_state(inode,\n\t\t\t\t\t\t     EXT4_STATE_DIO_UNWRITTEN);\n\t\t}\n\t}\n\n\tif (unlikely(EXT4_I(inode)->i_flags & EXT4_EOFBLOCKS_FL)) {\n\t\tif (eh->eh_entries) {\n\t\t\tlast_ex = EXT_LAST_EXTENT(eh);\n\t\t\tif (iblock + ar.len > le32_to_cpu(last_ex->ee_block)\n\t\t\t\t\t    + ext4_ext_get_actual_len(last_ex))\n\t\t\t\tEXT4_I(inode)->i_flags &= ~EXT4_EOFBLOCKS_FL;\n\t\t} else {\n\t\t\tWARN_ON(eh->eh_entries == 0);\n\t\t\text4_error(inode->i_sb, __func__,\n\t\t\t\t\"inode#%lu, eh->eh_entries = 0!\", inode->i_ino);\n\t\t\t}\n\t}\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err) {\n\t\t/* free data blocks we just allocated */\n\t\t/* not a good idea to call discard here directly,\n\t\t * but otherwise we'd need to call it every free() */\n\t\text4_discard_preallocations(inode);\n\t\text4_free_blocks(handle, inode, 0, ext_pblock(&newex),\n\t\t\t\t ext4_ext_get_actual_len(&newex), 0);\n\t\tgoto out2;\n\t}\n\n\t/* previous routine could use block we allocated */\n\tnewblock = ext_pblock(&newex);\n\tallocated = ext4_ext_get_actual_len(&newex);\n\tif (allocated > max_blocks)\n\t\tallocated = max_blocks;\n\tset_buffer_new(bh_result);\n\n\t/*\n\t * Update reserved blocks/metadata blocks after successful\n\t * block allocation which had been deferred till now.\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\text4_da_update_reserve_space(inode, allocated, 1);\n\n\t/*\n\t * Cache the extent and update transaction to commit on fdatasync only\n\t * when it is _not_ an uninitialized extent.\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_UNINIT_EXT) == 0) {\n\t\text4_ext_put_in_cache(inode, iblock, allocated, newblock,\n\t\t\t\t\t\tEXT4_EXT_CACHE_EXTENT);\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t} else\n\t\text4_update_inode_fsync_trans(handle, inode, 0);\nout:\n\tif (allocated > max_blocks)\n\t\tallocated = max_blocks;\n\text4_ext_show_leaf(inode, path);\n\tset_buffer_mapped(bh_result);\n\tbh_result->b_bdev = inode->i_sb->s_bdev;\n\tbh_result->b_blocknr = newblock;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}\n\nvoid ext4_ext_truncate(struct inode *inode)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t last_block;\n\thandle_t *handle;\n\tint err = 0;\n\n\t/*\n\t * probably first extent we're gonna free will be last in block\n\t */\n\terr = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, err);\n\tif (IS_ERR(handle))\n\t\treturn;\n\n\tif (inode->i_size & (sb->s_blocksize - 1))\n\t\text4_block_truncate_page(handle, mapping, inode->i_size);\n\n\tif (ext4_orphan_add(handle, inode))\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_ext_invalidate_cache(inode);\n\n\text4_discard_preallocations(inode);\n\n\t/*\n\t * TODO: optimization is possible here.\n\t * Probably we need not scan at all,\n\t * because page truncation is enough.\n\t */\n\n\t/* we have to know where to truncate from in crash case */\n\tEXT4_I(inode)->i_disksize = inode->i_size;\n\text4_mark_inode_dirty(handle, inode);\n\n\tlast_block = (inode->i_size + sb->s_blocksize - 1)\n\t\t\t>> EXT4_BLOCK_SIZE_BITS(sb);\n\terr = ext4_ext_remove_space(inode, last_block);\n\n\t/* In a multi-transaction truncate, we only make the final\n\t * transaction synchronous.\n\t */\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\nout_stop:\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\t/*\n\t * If this was a simple ftruncate() and the file will remain alive,\n\t * then we need to clear up the orphan record which we created above.\n\t * However, if this was a real unlink then we were called by\n\t * ext4_delete_inode(), and we allow that function to clean up the\n\t * orphan info for us.\n\t */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(handle, inode);\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\n}\n\nstatic void ext4_falloc_update_inode(struct inode *inode,\n\t\t\t\tint mode, loff_t new_size, int update_ctime)\n{\n\tstruct timespec now;\n\n\tif (update_ctime) {\n\t\tnow = current_fs_time(inode->i_sb);\n\t\tif (!timespec_equal(&inode->i_ctime, &now))\n\t\t\tinode->i_ctime = now;\n\t}\n\t/*\n\t * Update only when preallocation was requested beyond\n\t * the file size.\n\t */\n\tif (!(mode & FALLOC_FL_KEEP_SIZE)) {\n\t\tif (new_size > i_size_read(inode))\n\t\t\ti_size_write(inode, new_size);\n\t\tif (new_size > EXT4_I(inode)->i_disksize)\n\t\t\text4_update_i_disksize(inode, new_size);\n\t} else {\n\t\t/*\n\t\t * Mark that we allocate beyond EOF so the subsequent truncate\n\t\t * can proceed even if the new size is the same as i_size.\n\t\t */\n\t\tif (new_size > i_size_read(inode))\n\t\t\tEXT4_I(inode)->i_flags |= EXT4_EOFBLOCKS_FL;\n\t}\n\n}\n\n/*\n * preallocate space for a file. This implements ext4's fallocate inode\n * operation, which gets called from sys_fallocate system call.\n * For block-mapped files, posix_fallocate should fall back to the method\n * of writing zeroes to the required new blocks (the same behavior which is\n * expected for file systems which do not support fallocate() system call).\n */\nlong ext4_fallocate(struct inode *inode, int mode, loff_t offset, loff_t len)\n{\n\thandle_t *handle;\n\text4_lblk_t block;\n\tloff_t new_size;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint retries = 0;\n\tstruct buffer_head map_bh;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\t/*\n\t * currently supporting (pre)allocate mode for extent-based\n\t * files _only_\n\t */\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL))\n\t\treturn -EOPNOTSUPP;\n\n\t/* preallocation to directories is currently not supported */\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn -ENODEV;\n\n\tblock = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)\n\t\t\t\t\t\t\t- block;\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\tmutex_lock(&inode->i_mutex);\nretry:\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tblock = block + ret;\n\t\tmax_blocks = max_blocks - ret;\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tmap_bh.b_state = 0;\n\t\tret = ext4_get_blocks(handle, inode, block,\n\t\t\t\t      max_blocks, &map_bh,\n\t\t\t\t      EXT4_GET_BLOCKS_CREATE_UNINIT_EXT);\n\t\tif (ret <= 0) {\n#ifdef EXT4FS_DEBUG\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tprintk(KERN_ERR \"%s: ext4_ext_get_blocks \"\n\t\t\t\t    \"returned error inode#%lu, block=%u, \"\n\t\t\t\t    \"max_blocks=%u\", __func__,\n\t\t\t\t    inode->i_ino, block, max_blocks);\n#endif\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\t\tbreak;\n\t\t}\n\t\tif ((block + ret) >= (EXT4_BLOCK_ALIGN(offset + len,\n\t\t\t\t\t\tblkbits) >> blkbits))\n\t\t\tnew_size = offset + len;\n\t\telse\n\t\t\tnew_size = (block + ret) << blkbits;\n\n\t\text4_falloc_update_inode(inode, mode, new_size,\n\t\t\t\t\t\tbuffer_new(&map_bh));\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret2)\n\t\t\tbreak;\n\t}\n\tif (ret == -ENOSPC &&\n\t\t\text4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * This function convert a range of blocks to written extents\n * The caller of this function will pass the start offset and the size.\n * all unwritten extents within this range will be converted to\n * written extents.\n *\n * This function is called from the direct IO end io call back\n * function, to convert the fallocated extents after IO is completed.\n * Returns 0 on success.\n */\nint ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,\n\t\t\t\t    ssize_t len)\n{\n\thandle_t *handle;\n\text4_lblk_t block;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct buffer_head map_bh;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\tblock = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)\n\t\t\t\t\t\t\t- block;\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tblock = block + ret;\n\t\tmax_blocks = max_blocks - ret;\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tmap_bh.b_state = 0;\n\t\tret = ext4_get_blocks(handle, inode, block,\n\t\t\t\t      max_blocks, &map_bh,\n\t\t\t\t      EXT4_GET_BLOCKS_IO_CONVERT_EXT);\n\t\tif (ret <= 0) {\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tprintk(KERN_ERR \"%s: ext4_ext_get_blocks \"\n\t\t\t\t    \"returned error inode#%lu, block=%u, \"\n\t\t\t\t    \"max_blocks=%u\", __func__,\n\t\t\t\t    inode->i_ino, block, max_blocks);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret <= 0 || ret2 )\n\t\t\tbreak;\n\t}\n\treturn ret > 0 ? ret2 : ret;\n}\n/*\n * Callback function called for each extent to gather FIEMAP information.\n */\nstatic int ext4_ext_fiemap_cb(struct inode *inode, struct ext4_ext_path *path,\n\t\t       struct ext4_ext_cache *newex, struct ext4_extent *ex,\n\t\t       void *data)\n{\n\tstruct fiemap_extent_info *fieinfo = data;\n\tunsigned char blksize_bits = inode->i_sb->s_blocksize_bits;\n\t__u64\tlogical;\n\t__u64\tphysical;\n\t__u64\tlength;\n\t__u32\tflags = 0;\n\tint\terror;\n\n\tlogical =  (__u64)newex->ec_block << blksize_bits;\n\n\tif (newex->ec_type == EXT4_EXT_CACHE_GAP) {\n\t\tpgoff_t offset;\n\t\tstruct page *page;\n\t\tstruct buffer_head *bh = NULL;\n\n\t\toffset = logical >> PAGE_SHIFT;\n\t\tpage = find_get_page(inode->i_mapping, offset);\n\t\tif (!page || !page_has_buffers(page))\n\t\t\treturn EXT_CONTINUE;\n\n\t\tbh = page_buffers(page);\n\n\t\tif (!bh)\n\t\t\treturn EXT_CONTINUE;\n\n\t\tif (buffer_delay(bh)) {\n\t\t\tflags |= FIEMAP_EXTENT_DELALLOC;\n\t\t\tpage_cache_release(page);\n\t\t} else {\n\t\t\tpage_cache_release(page);\n\t\t\treturn EXT_CONTINUE;\n\t\t}\n\t}\n\n\tphysical = (__u64)newex->ec_start << blksize_bits;\n\tlength =   (__u64)newex->ec_len << blksize_bits;\n\n\tif (ex && ext4_ext_is_uninitialized(ex))\n\t\tflags |= FIEMAP_EXTENT_UNWRITTEN;\n\n\t/*\n\t * If this extent reaches EXT_MAX_BLOCK, it must be last.\n\t *\n\t * Or if ext4_ext_next_allocated_block is EXT_MAX_BLOCK,\n\t * this also indicates no more allocated blocks.\n\t *\n\t * XXX this might miss a single-block extent at EXT_MAX_BLOCK\n\t */\n\tif (ext4_ext_next_allocated_block(path) == EXT_MAX_BLOCK ||\n\t    newex->ec_block + newex->ec_len - 1 == EXT_MAX_BLOCK) {\n\t\tloff_t size = i_size_read(inode);\n\t\tloff_t bs = EXT4_BLOCK_SIZE(inode->i_sb);\n\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\t\tif ((flags & FIEMAP_EXTENT_DELALLOC) &&\n\t\t    logical+length > size)\n\t\t\tlength = (size - logical + bs - 1) & ~(bs-1);\n\t}\n\n\terror = fiemap_fill_next_extent(fieinfo, logical, physical,\n\t\t\t\t\tlength, flags);\n\tif (error < 0)\n\t\treturn error;\n\tif (error == 1)\n\t\treturn EXT_BREAK;\n\n\treturn EXT_CONTINUE;\n}\n\n/* fiemap flags we can handle specified here */\n#define EXT4_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)\n\nstatic int ext4_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\t__u64 physical = 0;\n\t__u64 length;\n\t__u32 flags = FIEMAP_EXTENT_LAST;\n\tint blockbits = inode->i_sb->s_blocksize_bits;\n\tint error = 0;\n\n\t/* in-inode? */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tstruct ext4_iloc iloc;\n\t\tint offset;\t/* offset of xattr in inode */\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error)\n\t\t\treturn error;\n\t\tphysical = iloc.bh->b_blocknr << blockbits;\n\t\toffset = EXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\tEXT4_I(inode)->i_extra_isize;\n\t\tphysical += offset;\n\t\tlength = EXT4_SB(inode->i_sb)->s_inode_size - offset;\n\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t} else { /* external block */\n\t\tphysical = EXT4_I(inode)->i_file_acl << blockbits;\n\t\tlength = inode->i_sb->s_blocksize;\n\t}\n\n\tif (physical)\n\t\terror = fiemap_fill_next_extent(fieinfo, 0, physical,\n\t\t\t\t\t\tlength, flags);\n\treturn (error < 0 ? error : 0);\n}\n\nint ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\text4_lblk_t start_blk;\n\tint error = 0;\n\n\t/* fallback to generic here if not in extents fmt */\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL))\n\t\treturn generic_block_fiemap(inode, fieinfo, start, len,\n\t\t\text4_get_block);\n\n\tif (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS))\n\t\treturn -EBADR;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\terror = ext4_xattr_fiemap(inode, fieinfo);\n\t} else {\n\t\text4_lblk_t len_blks;\n\t\t__u64 last_blk;\n\n\t\tstart_blk = start >> inode->i_sb->s_blocksize_bits;\n\t\tlast_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;\n\t\tif (last_blk >= EXT_MAX_BLOCK)\n\t\t\tlast_blk = EXT_MAX_BLOCK-1;\n\t\tlen_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;\n\n\t\t/*\n\t\t * Walk the extent tree gathering extent information.\n\t\t * ext4_ext_fiemap_cb will push extents back to user.\n\t\t */\n\t\terror = ext4_ext_walk_space(inode, start_blk, len_blks,\n\t\t\t\t\t  ext4_ext_fiemap_cb, fieinfo);\n\t}\n\n\treturn error;\n}\n\n", "/*\n *  linux/fs/ext4/inode.c\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/fs/minix/inode.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  Goal-directed block allocation by Stephen Tweedie\n *\t(sct@redhat.com), 1993, 1998\n *  Big-endian to little-endian byte-swapping/bitmaps by\n *        David S. Miller (davem@caip.rutgers.edu), 1995\n *  64-bit file support on 64-bit platforms by Jakub Jelinek\n *\t(jj@sunsite.ms.mff.cuni.cz)\n *\n *  Assorted race fixes, rewrite of ext4_get_block() by Al Viro, 2000\n */\n\n#include <linux/module.h>\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/buffer_head.h>\n#include <linux/writeback.h>\n#include <linux/pagevec.h>\n#include <linux/mpage.h>\n#include <linux/namei.h>\n#include <linux/uio.h>\n#include <linux/bio.h>\n#include <linux/workqueue.h>\n\n#include \"ext4_jbd2.h\"\n#include \"xattr.h\"\n#include \"acl.h\"\n#include \"ext4_extents.h\"\n\n#include <trace/events/ext4.h>\n\n#define MPAGE_DA_EXTENT_TAIL 0x01\n\nstatic inline int ext4_begin_ordered_truncate(struct inode *inode,\n\t\t\t\t\t      loff_t new_size)\n{\n\treturn jbd2_journal_begin_ordered_truncate(\n\t\t\t\t\tEXT4_SB(inode->i_sb)->s_journal,\n\t\t\t\t\t&EXT4_I(inode)->jinode,\n\t\t\t\t\tnew_size);\n}\n\nstatic void ext4_invalidatepage(struct page *page, unsigned long offset);\n\n/*\n * Test whether an inode is a fast symlink.\n */\nstatic int ext4_inode_is_fast_symlink(struct inode *inode)\n{\n\tint ea_blocks = EXT4_I(inode)->i_file_acl ?\n\t\t(inode->i_sb->s_blocksize >> 9) : 0;\n\n\treturn (S_ISLNK(inode->i_mode) && inode->i_blocks - ea_blocks == 0);\n}\n\n/*\n * Work out how many blocks we need to proceed with the next chunk of a\n * truncate transaction.\n */\nstatic unsigned long blocks_for_truncate(struct inode *inode)\n{\n\text4_lblk_t needed;\n\n\tneeded = inode->i_blocks >> (inode->i_sb->s_blocksize_bits - 9);\n\n\t/* Give ourselves just enough room to cope with inodes in which\n\t * i_blocks is corrupt: we've seen disk corruptions in the past\n\t * which resulted in random data in an inode which looked enough\n\t * like a regular file for ext4 to try to delete it.  Things\n\t * will go a bit crazy if that happens, but at least we should\n\t * try not to panic the whole kernel. */\n\tif (needed < 2)\n\t\tneeded = 2;\n\n\t/* But we need to bound the transaction so we don't overflow the\n\t * journal. */\n\tif (needed > EXT4_MAX_TRANS_DATA)\n\t\tneeded = EXT4_MAX_TRANS_DATA;\n\n\treturn EXT4_DATA_TRANS_BLOCKS(inode->i_sb) + needed;\n}\n\n/*\n * Truncate transactions can be complex and absolutely huge.  So we need to\n * be able to restart the transaction at a conventient checkpoint to make\n * sure we don't overflow the journal.\n *\n * start_transaction gets us a new handle for a truncate transaction,\n * and extend_transaction tries to extend the existing one a bit.  If\n * extend fails, we need to propagate the failure up and restart the\n * transaction in the top-level truncate loop. --sct\n */\nstatic handle_t *start_transaction(struct inode *inode)\n{\n\thandle_t *result;\n\n\tresult = ext4_journal_start(inode, blocks_for_truncate(inode));\n\tif (!IS_ERR(result))\n\t\treturn result;\n\n\text4_std_error(inode->i_sb, PTR_ERR(result));\n\treturn result;\n}\n\n/*\n * Try to extend this transaction for the purposes of truncation.\n *\n * Returns 0 if we managed to create more room.  If we can't create more\n * room, and the transaction must be restarted we return 1.\n */\nstatic int try_to_extend_transaction(handle_t *handle, struct inode *inode)\n{\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (ext4_handle_has_enough_credits(handle, EXT4_RESERVE_TRANS_BLOCKS+1))\n\t\treturn 0;\n\tif (!ext4_journal_extend(handle, blocks_for_truncate(inode)))\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * Restart the transaction associated with *handle.  This does a commit,\n * so before we call here everything must be consistently dirtied against\n * this transaction.\n */\nint ext4_truncate_restart_trans(handle_t *handle, struct inode *inode,\n\t\t\t\t int nblocks)\n{\n\tint ret;\n\n\t/*\n\t * Drop i_data_sem to avoid deadlock with ext4_get_blocks At this\n\t * moment, get_block can be called only for blocks inside i_size since\n\t * page cache has been already dropped and writes are blocked by\n\t * i_mutex. So we can safely drop the i_data_sem here.\n\t */\n\tBUG_ON(EXT4_JOURNAL(inode) == NULL);\n\tjbd_debug(2, \"restarting handle %p\\n\", handle);\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tret = ext4_journal_restart(handle, blocks_for_truncate(inode));\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\treturn ret;\n}\n\n/*\n * Called at the last iput() if i_nlink is zero.\n */\nvoid ext4_delete_inode(struct inode *inode)\n{\n\thandle_t *handle;\n\tint err;\n\n\tif (ext4_should_order_data(inode))\n\t\text4_begin_ordered_truncate(inode, 0);\n\ttruncate_inode_pages(&inode->i_data, 0);\n\n\tif (is_bad_inode(inode))\n\t\tgoto no_delete;\n\n\thandle = ext4_journal_start(inode, blocks_for_truncate(inode)+3);\n\tif (IS_ERR(handle)) {\n\t\text4_std_error(inode->i_sb, PTR_ERR(handle));\n\t\t/*\n\t\t * If we're going to skip the normal cleanup, we still need to\n\t\t * make sure that the in-core orphan linked list is properly\n\t\t * cleaned up.\n\t\t */\n\t\text4_orphan_del(NULL, inode);\n\t\tgoto no_delete;\n\t}\n\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tinode->i_size = 0;\n\terr = ext4_mark_inode_dirty(handle, inode);\n\tif (err) {\n\t\text4_warning(inode->i_sb,\n\t\t\t     \"couldn't mark inode dirty (err %d)\", err);\n\t\tgoto stop_handle;\n\t}\n\tif (inode->i_blocks)\n\t\text4_truncate(inode);\n\n\t/*\n\t * ext4_ext_truncate() doesn't reserve any slop when it\n\t * restarts journal transactions; therefore there may not be\n\t * enough credits left in the handle to remove the inode from\n\t * the orphan list and set the dtime field.\n\t */\n\tif (!ext4_handle_has_enough_credits(handle, 3)) {\n\t\terr = ext4_journal_extend(handle, 3);\n\t\tif (err > 0)\n\t\t\terr = ext4_journal_restart(handle, 3);\n\t\tif (err != 0) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"couldn't extend journal (err %d)\", err);\n\t\tstop_handle:\n\t\t\text4_journal_stop(handle);\n\t\t\tgoto no_delete;\n\t\t}\n\t}\n\n\t/*\n\t * Kill off the orphan record which ext4_truncate created.\n\t * AKPM: I think this can be inside the above `if'.\n\t * Note that ext4_orphan_del() has to be able to cope with the\n\t * deletion of a non-existent orphan - this is because we don't\n\t * know if ext4_truncate() actually created an orphan record.\n\t * (Well, we could do this if we need to, but heck - it works)\n\t */\n\text4_orphan_del(handle, inode);\n\tEXT4_I(inode)->i_dtime\t= get_seconds();\n\n\t/*\n\t * One subtle ordering requirement: if anything has gone wrong\n\t * (transaction abort, IO errors, whatever), then we can still\n\t * do these next steps (the fs will already have been marked as\n\t * having errors), but we can't free the inode if the mark_dirty\n\t * fails.\n\t */\n\tif (ext4_mark_inode_dirty(handle, inode))\n\t\t/* If that failed, just do the required in-core inode clear. */\n\t\tclear_inode(inode);\n\telse\n\t\text4_free_inode(handle, inode);\n\text4_journal_stop(handle);\n\treturn;\nno_delete:\n\tclear_inode(inode);\t/* We must guarantee clearing of inode... */\n}\n\ntypedef struct {\n\t__le32\t*p;\n\t__le32\tkey;\n\tstruct buffer_head *bh;\n} Indirect;\n\nstatic inline void add_chain(Indirect *p, struct buffer_head *bh, __le32 *v)\n{\n\tp->key = *(p->p = v);\n\tp->bh = bh;\n}\n\n/**\n *\text4_block_to_path - parse the block number into array of offsets\n *\t@inode: inode in question (we are only interested in its superblock)\n *\t@i_block: block number to be parsed\n *\t@offsets: array to store the offsets in\n *\t@boundary: set this non-zero if the referred-to block is likely to be\n *\t       followed (on disk) by an indirect block.\n *\n *\tTo store the locations of file's data ext4 uses a data structure common\n *\tfor UNIX filesystems - tree of pointers anchored in the inode, with\n *\tdata blocks at leaves and indirect blocks in intermediate nodes.\n *\tThis function translates the block number into path in that tree -\n *\treturn value is the path length and @offsets[n] is the offset of\n *\tpointer to (n+1)th node in the nth one. If @block is out of range\n *\t(negative or too large) warning is printed and zero returned.\n *\n *\tNote: function doesn't find node addresses, so no IO is needed. All\n *\twe need to know is the capacity of indirect blocks (taken from the\n *\tinode->i_sb).\n */\n\n/*\n * Portability note: the last comparison (check that we fit into triple\n * indirect block) is spelled differently, because otherwise on an\n * architecture with 32-bit longs and 8Kb pages we might get into trouble\n * if our filesystem had 8Kb blocks. We might use long long, but that would\n * kill us on x86. Oh, well, at least the sign propagation does not matter -\n * i_block would have to be negative in the very beginning, so we would not\n * get there at all.\n */\n\nstatic int ext4_block_to_path(struct inode *inode,\n\t\t\t      ext4_lblk_t i_block,\n\t\t\t      ext4_lblk_t offsets[4], int *boundary)\n{\n\tint ptrs = EXT4_ADDR_PER_BLOCK(inode->i_sb);\n\tint ptrs_bits = EXT4_ADDR_PER_BLOCK_BITS(inode->i_sb);\n\tconst long direct_blocks = EXT4_NDIR_BLOCKS,\n\t\tindirect_blocks = ptrs,\n\t\tdouble_blocks = (1 << (ptrs_bits * 2));\n\tint n = 0;\n\tint final = 0;\n\n\tif (i_block < direct_blocks) {\n\t\toffsets[n++] = i_block;\n\t\tfinal = direct_blocks;\n\t} else if ((i_block -= direct_blocks) < indirect_blocks) {\n\t\toffsets[n++] = EXT4_IND_BLOCK;\n\t\toffsets[n++] = i_block;\n\t\tfinal = ptrs;\n\t} else if ((i_block -= indirect_blocks) < double_blocks) {\n\t\toffsets[n++] = EXT4_DIND_BLOCK;\n\t\toffsets[n++] = i_block >> ptrs_bits;\n\t\toffsets[n++] = i_block & (ptrs - 1);\n\t\tfinal = ptrs;\n\t} else if (((i_block -= double_blocks) >> (ptrs_bits * 2)) < ptrs) {\n\t\toffsets[n++] = EXT4_TIND_BLOCK;\n\t\toffsets[n++] = i_block >> (ptrs_bits * 2);\n\t\toffsets[n++] = (i_block >> ptrs_bits) & (ptrs - 1);\n\t\toffsets[n++] = i_block & (ptrs - 1);\n\t\tfinal = ptrs;\n\t} else {\n\t\text4_warning(inode->i_sb, \"block %lu > max in inode %lu\",\n\t\t\t     i_block + direct_blocks +\n\t\t\t     indirect_blocks + double_blocks, inode->i_ino);\n\t}\n\tif (boundary)\n\t\t*boundary = final - 1 - (i_block & (ptrs - 1));\n\treturn n;\n}\n\nstatic int __ext4_check_blockref(const char *function, struct inode *inode,\n\t\t\t\t __le32 *p, unsigned int max)\n{\n\t__le32 *bref = p;\n\tunsigned int blk;\n\n\twhile (bref < p+max) {\n\t\tblk = le32_to_cpu(*bref++);\n\t\tif (blk &&\n\t\t    unlikely(!ext4_data_block_valid(EXT4_SB(inode->i_sb),\n\t\t\t\t\t\t    blk, 1))) {\n\t\t\t__ext4_error(inode->i_sb, function,\n\t\t\t\t   \"invalid block reference %u \"\n\t\t\t\t   \"in inode #%lu\", blk, inode->i_ino);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\treturn 0;\n}\n\n\n#define ext4_check_indirect_blockref(inode, bh)                         \\\n\t__ext4_check_blockref(__func__, inode, (__le32 *)(bh)->b_data,  \\\n\t\t\t      EXT4_ADDR_PER_BLOCK((inode)->i_sb))\n\n#define ext4_check_inode_blockref(inode)                                \\\n\t__ext4_check_blockref(__func__, inode, EXT4_I(inode)->i_data,   \\\n\t\t\t      EXT4_NDIR_BLOCKS)\n\n/**\n *\text4_get_branch - read the chain of indirect blocks leading to data\n *\t@inode: inode in question\n *\t@depth: depth of the chain (1 - direct pointer, etc.)\n *\t@offsets: offsets of pointers in inode/indirect blocks\n *\t@chain: place to store the result\n *\t@err: here we store the error value\n *\n *\tFunction fills the array of triples <key, p, bh> and returns %NULL\n *\tif everything went OK or the pointer to the last filled triple\n *\t(incomplete one) otherwise. Upon the return chain[i].key contains\n *\tthe number of (i+1)-th block in the chain (as it is stored in memory,\n *\ti.e. little-endian 32-bit), chain[i].p contains the address of that\n *\tnumber (it points into struct inode for i==0 and into the bh->b_data\n *\tfor i>0) and chain[i].bh points to the buffer_head of i-th indirect\n *\tblock for i>0 and NULL for i==0. In other words, it holds the block\n *\tnumbers of the chain, addresses they were taken from (and where we can\n *\tverify that chain did not change) and buffer_heads hosting these\n *\tnumbers.\n *\n *\tFunction stops when it stumbles upon zero pointer (absent block)\n *\t\t(pointer to last triple returned, *@err == 0)\n *\tor when it gets an IO error reading an indirect block\n *\t\t(ditto, *@err == -EIO)\n *\tor when it reads all @depth-1 indirect blocks successfully and finds\n *\tthe whole chain, all way to the data (returns %NULL, *err == 0).\n *\n *      Need to be called with\n *      down_read(&EXT4_I(inode)->i_data_sem)\n */\nstatic Indirect *ext4_get_branch(struct inode *inode, int depth,\n\t\t\t\t ext4_lblk_t  *offsets,\n\t\t\t\t Indirect chain[4], int *err)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tIndirect *p = chain;\n\tstruct buffer_head *bh;\n\n\t*err = 0;\n\t/* i_data is not going away, no lock needed */\n\tadd_chain(chain, NULL, EXT4_I(inode)->i_data + *offsets);\n\tif (!p->key)\n\t\tgoto no_block;\n\twhile (--depth) {\n\t\tbh = sb_getblk(sb, le32_to_cpu(p->key));\n\t\tif (unlikely(!bh))\n\t\t\tgoto failure;\n\n\t\tif (!bh_uptodate_or_lock(bh)) {\n\t\t\tif (bh_submit_read(bh) < 0) {\n\t\t\t\tput_bh(bh);\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t\t/* validate block references */\n\t\t\tif (ext4_check_indirect_blockref(inode, bh)) {\n\t\t\t\tput_bh(bh);\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t}\n\n\t\tadd_chain(++p, bh, (__le32 *)bh->b_data + *++offsets);\n\t\t/* Reader: end */\n\t\tif (!p->key)\n\t\t\tgoto no_block;\n\t}\n\treturn NULL;\n\nfailure:\n\t*err = -EIO;\nno_block:\n\treturn p;\n}\n\n/**\n *\text4_find_near - find a place for allocation with sufficient locality\n *\t@inode: owner\n *\t@ind: descriptor of indirect block.\n *\n *\tThis function returns the preferred place for block allocation.\n *\tIt is used when heuristic for sequential allocation fails.\n *\tRules are:\n *\t  + if there is a block to the left of our position - allocate near it.\n *\t  + if pointer will live in indirect block - allocate near that block.\n *\t  + if pointer will live in inode - allocate in the same\n *\t    cylinder group.\n *\n * In the latter case we colour the starting block by the callers PID to\n * prevent it from clashing with concurrent allocations for a different inode\n * in the same block group.   The PID is used here so that functionally related\n * files will be close-by on-disk.\n *\n *\tCaller must make sure that @ind is valid and will stay that way.\n */\nstatic ext4_fsblk_t ext4_find_near(struct inode *inode, Indirect *ind)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\t__le32 *start = ind->bh ? (__le32 *) ind->bh->b_data : ei->i_data;\n\t__le32 *p;\n\text4_fsblk_t bg_start;\n\text4_fsblk_t last_block;\n\text4_grpblk_t colour;\n\text4_group_t block_group;\n\tint flex_size = ext4_flex_bg_size(EXT4_SB(inode->i_sb));\n\n\t/* Try to find previous block */\n\tfor (p = ind->p - 1; p >= start; p--) {\n\t\tif (*p)\n\t\t\treturn le32_to_cpu(*p);\n\t}\n\n\t/* No such thing, so let's try location of indirect block */\n\tif (ind->bh)\n\t\treturn ind->bh->b_blocknr;\n\n\t/*\n\t * It is going to be referred to from the inode itself? OK, just put it\n\t * into the same cylinder group then.\n\t */\n\tblock_group = ei->i_block_group;\n\tif (flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) {\n\t\tblock_group &= ~(flex_size-1);\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\tblock_group++;\n\t}\n\tbg_start = ext4_group_first_block_no(inode->i_sb, block_group);\n\tlast_block = ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es) - 1;\n\n\t/*\n\t * If we are doing delayed allocation, we don't need take\n\t * colour into account.\n\t */\n\tif (test_opt(inode->i_sb, DELALLOC))\n\t\treturn bg_start;\n\n\tif (bg_start + EXT4_BLOCKS_PER_GROUP(inode->i_sb) <= last_block)\n\t\tcolour = (current->pid % 16) *\n\t\t\t(EXT4_BLOCKS_PER_GROUP(inode->i_sb) / 16);\n\telse\n\t\tcolour = (current->pid % 16) * ((last_block - bg_start) / 16);\n\treturn bg_start + colour;\n}\n\n/**\n *\text4_find_goal - find a preferred place for allocation.\n *\t@inode: owner\n *\t@block:  block we want\n *\t@partial: pointer to the last triple within a chain\n *\n *\tNormally this function find the preferred place for block allocation,\n *\treturns it.\n *\tBecause this is only used for non-extent files, we limit the block nr\n *\tto 32 bits.\n */\nstatic ext4_fsblk_t ext4_find_goal(struct inode *inode, ext4_lblk_t block,\n\t\t\t\t   Indirect *partial)\n{\n\text4_fsblk_t goal;\n\n\t/*\n\t * XXX need to get goal block from mballoc's data structures\n\t */\n\n\tgoal = ext4_find_near(inode, partial);\n\tgoal = goal & EXT4_MAX_BLOCK_FILE_PHYS;\n\treturn goal;\n}\n\n/**\n *\text4_blks_to_allocate: Look up the block map and count the number\n *\tof direct blocks need to be allocated for the given branch.\n *\n *\t@branch: chain of indirect blocks\n *\t@k: number of blocks need for indirect blocks\n *\t@blks: number of data blocks to be mapped.\n *\t@blocks_to_boundary:  the offset in the indirect block\n *\n *\treturn the total number of blocks to be allocate, including the\n *\tdirect and indirect blocks.\n */\nstatic int ext4_blks_to_allocate(Indirect *branch, int k, unsigned int blks,\n\t\t\t\t int blocks_to_boundary)\n{\n\tunsigned int count = 0;\n\n\t/*\n\t * Simple case, [t,d]Indirect block(s) has not allocated yet\n\t * then it's clear blocks on that path have not allocated\n\t */\n\tif (k > 0) {\n\t\t/* right now we don't handle cross boundary allocation */\n\t\tif (blks < blocks_to_boundary + 1)\n\t\t\tcount += blks;\n\t\telse\n\t\t\tcount += blocks_to_boundary + 1;\n\t\treturn count;\n\t}\n\n\tcount++;\n\twhile (count < blks && count <= blocks_to_boundary &&\n\t\tle32_to_cpu(*(branch[0].p + count)) == 0) {\n\t\tcount++;\n\t}\n\treturn count;\n}\n\n/**\n *\text4_alloc_blocks: multiple allocate blocks needed for a branch\n *\t@indirect_blks: the number of blocks need to allocate for indirect\n *\t\t\tblocks\n *\n *\t@new_blocks: on return it will store the new block numbers for\n *\tthe indirect blocks(if needed) and the first direct block,\n *\t@blks:\ton return it will store the total number of allocated\n *\t\tdirect blocks\n */\nstatic int ext4_alloc_blocks(handle_t *handle, struct inode *inode,\n\t\t\t     ext4_lblk_t iblock, ext4_fsblk_t goal,\n\t\t\t     int indirect_blks, int blks,\n\t\t\t     ext4_fsblk_t new_blocks[4], int *err)\n{\n\tstruct ext4_allocation_request ar;\n\tint target, i;\n\tunsigned long count = 0, blk_allocated = 0;\n\tint index = 0;\n\text4_fsblk_t current_block = 0;\n\tint ret = 0;\n\n\t/*\n\t * Here we try to allocate the requested multiple blocks at once,\n\t * on a best-effort basis.\n\t * To build a branch, we should allocate blocks for\n\t * the indirect blocks(if not allocated yet), and at least\n\t * the first direct block of this branch.  That's the\n\t * minimum number of blocks need to allocate(required)\n\t */\n\t/* first we try to allocate the indirect blocks */\n\ttarget = indirect_blks;\n\twhile (target > 0) {\n\t\tcount = target;\n\t\t/* allocating blocks for indirect blocks and direct blocks */\n\t\tcurrent_block = ext4_new_meta_blocks(handle, inode,\n\t\t\t\t\t\t\tgoal, &count, err);\n\t\tif (*err)\n\t\t\tgoto failed_out;\n\n\t\tBUG_ON(current_block + count > EXT4_MAX_BLOCK_FILE_PHYS);\n\n\t\ttarget -= count;\n\t\t/* allocate blocks for indirect blocks */\n\t\twhile (index < indirect_blks && count) {\n\t\t\tnew_blocks[index++] = current_block++;\n\t\t\tcount--;\n\t\t}\n\t\tif (count > 0) {\n\t\t\t/*\n\t\t\t * save the new block number\n\t\t\t * for the first direct block\n\t\t\t */\n\t\t\tnew_blocks[index] = current_block;\n\t\t\tprintk(KERN_INFO \"%s returned more blocks than \"\n\t\t\t\t\t\t\"requested\\n\", __func__);\n\t\t\tWARN_ON(1);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\ttarget = blks - count ;\n\tblk_allocated = count;\n\tif (!target)\n\t\tgoto allocated;\n\t/* Now allocate data blocks */\n\tmemset(&ar, 0, sizeof(ar));\n\tar.inode = inode;\n\tar.goal = goal;\n\tar.len = target;\n\tar.logical = iblock;\n\tif (S_ISREG(inode->i_mode))\n\t\t/* enable in-core preallocation only for regular files */\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\n\tcurrent_block = ext4_mb_new_blocks(handle, &ar, err);\n\tBUG_ON(current_block + ar.len > EXT4_MAX_BLOCK_FILE_PHYS);\n\n\tif (*err && (target == blks)) {\n\t\t/*\n\t\t * if the allocation failed and we didn't allocate\n\t\t * any blocks before\n\t\t */\n\t\tgoto failed_out;\n\t}\n\tif (!*err) {\n\t\tif (target == blks) {\n\t\t\t/*\n\t\t\t * save the new block number\n\t\t\t * for the first direct block\n\t\t\t */\n\t\t\tnew_blocks[index] = current_block;\n\t\t}\n\t\tblk_allocated += ar.len;\n\t}\nallocated:\n\t/* total number of blocks allocated for direct blocks */\n\tret = blk_allocated;\n\t*err = 0;\n\treturn ret;\nfailed_out:\n\tfor (i = 0; i < index; i++)\n\t\text4_free_blocks(handle, inode, 0, new_blocks[i], 1, 0);\n\treturn ret;\n}\n\n/**\n *\text4_alloc_branch - allocate and set up a chain of blocks.\n *\t@inode: owner\n *\t@indirect_blks: number of allocated indirect blocks\n *\t@blks: number of allocated direct blocks\n *\t@offsets: offsets (in the blocks) to store the pointers to next.\n *\t@branch: place to store the chain in.\n *\n *\tThis function allocates blocks, zeroes out all but the last one,\n *\tlinks them into chain and (if we are synchronous) writes them to disk.\n *\tIn other words, it prepares a branch that can be spliced onto the\n *\tinode. It stores the information about that chain in the branch[], in\n *\tthe same format as ext4_get_branch() would do. We are calling it after\n *\twe had read the existing part of chain and partial points to the last\n *\ttriple of that (one with zero ->key). Upon the exit we have the same\n *\tpicture as after the successful ext4_get_block(), except that in one\n *\tplace chain is disconnected - *branch->p is still zero (we did not\n *\tset the last link), but branch->key contains the number that should\n *\tbe placed into *branch->p to fill that gap.\n *\n *\tIf allocation fails we free all blocks we've allocated (and forget\n *\ttheir buffer_heads) and return the error value the from failed\n *\text4_alloc_block() (normally -ENOSPC). Otherwise we set the chain\n *\tas described above and return 0.\n */\nstatic int ext4_alloc_branch(handle_t *handle, struct inode *inode,\n\t\t\t     ext4_lblk_t iblock, int indirect_blks,\n\t\t\t     int *blks, ext4_fsblk_t goal,\n\t\t\t     ext4_lblk_t *offsets, Indirect *branch)\n{\n\tint blocksize = inode->i_sb->s_blocksize;\n\tint i, n = 0;\n\tint err = 0;\n\tstruct buffer_head *bh;\n\tint num;\n\text4_fsblk_t new_blocks[4];\n\text4_fsblk_t current_block;\n\n\tnum = ext4_alloc_blocks(handle, inode, iblock, goal, indirect_blks,\n\t\t\t\t*blks, new_blocks, &err);\n\tif (err)\n\t\treturn err;\n\n\tbranch[0].key = cpu_to_le32(new_blocks[0]);\n\t/*\n\t * metadata blocks and data blocks are allocated.\n\t */\n\tfor (n = 1; n <= indirect_blks;  n++) {\n\t\t/*\n\t\t * Get buffer_head for parent block, zero it out\n\t\t * and set the pointer to new one, then send\n\t\t * parent to disk.\n\t\t */\n\t\tbh = sb_getblk(inode->i_sb, new_blocks[n-1]);\n\t\tbranch[n].bh = bh;\n\t\tlock_buffer(bh);\n\t\tBUFFER_TRACE(bh, \"call get_create_access\");\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err) {\n\t\t\t/* Don't brelse(bh) here; it's done in\n\t\t\t * ext4_journal_forget() below */\n\t\t\tunlock_buffer(bh);\n\t\t\tgoto failed;\n\t\t}\n\n\t\tmemset(bh->b_data, 0, blocksize);\n\t\tbranch[n].p = (__le32 *) bh->b_data + offsets[n];\n\t\tbranch[n].key = cpu_to_le32(new_blocks[n]);\n\t\t*branch[n].p = branch[n].key;\n\t\tif (n == indirect_blks) {\n\t\t\tcurrent_block = new_blocks[n];\n\t\t\t/*\n\t\t\t * End of chain, update the last new metablock of\n\t\t\t * the chain to point to the new allocated\n\t\t\t * data blocks numbers\n\t\t\t */\n\t\t\tfor (i = 1; i < num; i++)\n\t\t\t\t*(branch[n].p + i) = cpu_to_le32(++current_block);\n\t\t}\n\t\tBUFFER_TRACE(bh, \"marking uptodate\");\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto failed;\n\t}\n\t*blks = num;\n\treturn err;\nfailed:\n\t/* Allocation failed, free what we already allocated */\n\text4_free_blocks(handle, inode, 0, new_blocks[0], 1, 0);\n\tfor (i = 1; i <= n ; i++) {\n\t\t/* \n\t\t * branch[i].bh is newly allocated, so there is no\n\t\t * need to revoke the block, which is why we don't\n\t\t * need to set EXT4_FREE_BLOCKS_METADATA.\n\t\t */\n\t\text4_free_blocks(handle, inode, 0, new_blocks[i], 1,\n\t\t\t\t EXT4_FREE_BLOCKS_FORGET);\n\t}\n\tfor (i = n+1; i < indirect_blks; i++)\n\t\text4_free_blocks(handle, inode, 0, new_blocks[i], 1, 0);\n\n\text4_free_blocks(handle, inode, 0, new_blocks[i], num, 0);\n\n\treturn err;\n}\n\n/**\n * ext4_splice_branch - splice the allocated branch onto inode.\n * @inode: owner\n * @block: (logical) number of block we are adding\n * @chain: chain of indirect blocks (with a missing link - see\n *\text4_alloc_branch)\n * @where: location of missing link\n * @num:   number of indirect blocks we are adding\n * @blks:  number of direct blocks we are adding\n *\n * This function fills the missing link and does all housekeeping needed in\n * inode (->i_blocks, etc.). In case of success we end up with the full\n * chain to new block and return 0.\n */\nstatic int ext4_splice_branch(handle_t *handle, struct inode *inode,\n\t\t\t      ext4_lblk_t block, Indirect *where, int num,\n\t\t\t      int blks)\n{\n\tint i;\n\tint err = 0;\n\text4_fsblk_t current_block;\n\n\t/*\n\t * If we're splicing into a [td]indirect block (as opposed to the\n\t * inode) then we need to get write access to the [td]indirect block\n\t * before the splice.\n\t */\n\tif (where->bh) {\n\t\tBUFFER_TRACE(where->bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, where->bh);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t}\n\t/* That's it */\n\n\t*where->p = where->key;\n\n\t/*\n\t * Update the host buffer_head or inode to point to more just allocated\n\t * direct blocks blocks\n\t */\n\tif (num == 0 && blks > 1) {\n\t\tcurrent_block = le32_to_cpu(where->key) + 1;\n\t\tfor (i = 1; i < blks; i++)\n\t\t\t*(where->p + i) = cpu_to_le32(current_block++);\n\t}\n\n\t/* We are done with atomic stuff, now do the rest of housekeeping */\n\t/* had we spliced it onto indirect block? */\n\tif (where->bh) {\n\t\t/*\n\t\t * If we spliced it onto an indirect block, we haven't\n\t\t * altered the inode.  Note however that if it is being spliced\n\t\t * onto an indirect block at the very end of the file (the\n\t\t * file is growing) then we *will* alter the inode to reflect\n\t\t * the new i_size.  But that is not done here - it is done in\n\t\t * generic_commit_write->__mark_inode_dirty->ext4_dirty_inode.\n\t\t */\n\t\tjbd_debug(5, \"splicing indirect only\\n\");\n\t\tBUFFER_TRACE(where->bh, \"call ext4_handle_dirty_metadata\");\n\t\terr = ext4_handle_dirty_metadata(handle, inode, where->bh);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t} else {\n\t\t/*\n\t\t * OK, we spliced it into the inode itself on a direct block.\n\t\t */\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tjbd_debug(5, \"splicing direct\\n\");\n\t}\n\treturn err;\n\nerr_out:\n\tfor (i = 1; i <= num; i++) {\n\t\t/* \n\t\t * branch[i].bh is newly allocated, so there is no\n\t\t * need to revoke the block, which is why we don't\n\t\t * need to set EXT4_FREE_BLOCKS_METADATA.\n\t\t */\n\t\text4_free_blocks(handle, inode, where[i].bh, 0, 1,\n\t\t\t\t EXT4_FREE_BLOCKS_FORGET);\n\t}\n\text4_free_blocks(handle, inode, 0, le32_to_cpu(where[num].key),\n\t\t\t blks, 0);\n\n\treturn err;\n}\n\n/*\n * The ext4_ind_get_blocks() function handles non-extents inodes\n * (i.e., using the traditional indirect/double-indirect i_blocks\n * scheme) for ext4_get_blocks().\n *\n * Allocation strategy is simple: if we have to allocate something, we will\n * have to go the whole way to leaf. So let's do it before attaching anything\n * to tree, set linkage between the newborn blocks, write them if sync is\n * required, recheck the path, free and repeat if check fails, otherwise\n * set the last missing link (that will protect us from any truncate-generated\n * removals - all blocks on the path are immune now) and possibly force the\n * write on the parent block.\n * That has a nice additional property: no special recovery from the failed\n * allocations is needed - we simply release blocks and do not touch anything\n * reachable from inode.\n *\n * `handle' can be NULL if create == 0.\n *\n * return > 0, # of blocks mapped or allocated.\n * return = 0, if plain lookup failed.\n * return < 0, error case.\n *\n * The ext4_ind_get_blocks() function should be called with\n * down_write(&EXT4_I(inode)->i_data_sem) if allocating filesystem\n * blocks (i.e., flags has EXT4_GET_BLOCKS_CREATE set) or\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system\n * blocks.\n */\nstatic int ext4_ind_get_blocks(handle_t *handle, struct inode *inode,\n\t\t\t       ext4_lblk_t iblock, unsigned int maxblocks,\n\t\t\t       struct buffer_head *bh_result,\n\t\t\t       int flags)\n{\n\tint err = -EIO;\n\text4_lblk_t offsets[4];\n\tIndirect chain[4];\n\tIndirect *partial;\n\text4_fsblk_t goal;\n\tint indirect_blks;\n\tint blocks_to_boundary = 0;\n\tint depth;\n\tint count = 0;\n\text4_fsblk_t first_block = 0;\n\n\tJ_ASSERT(!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL));\n\tJ_ASSERT(handle != NULL || (flags & EXT4_GET_BLOCKS_CREATE) == 0);\n\tdepth = ext4_block_to_path(inode, iblock, offsets,\n\t\t\t\t   &blocks_to_boundary);\n\n\tif (depth == 0)\n\t\tgoto out;\n\n\tpartial = ext4_get_branch(inode, depth, offsets, chain, &err);\n\n\t/* Simplest case - block found, no allocation needed */\n\tif (!partial) {\n\t\tfirst_block = le32_to_cpu(chain[depth - 1].key);\n\t\tclear_buffer_new(bh_result);\n\t\tcount++;\n\t\t/*map more blocks*/\n\t\twhile (count < maxblocks && count <= blocks_to_boundary) {\n\t\t\text4_fsblk_t blk;\n\n\t\t\tblk = le32_to_cpu(*(chain[depth-1].p + count));\n\n\t\t\tif (blk == first_block + count)\n\t\t\t\tcount++;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\t\tgoto got_it;\n\t}\n\n\t/* Next simple case - plain lookup or failed read of indirect block */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0 || err == -EIO)\n\t\tgoto cleanup;\n\n\t/*\n\t * Okay, we need to do block allocation.\n\t*/\n\tgoal = ext4_find_goal(inode, iblock, partial);\n\n\t/* the number of blocks need to allocate for [d,t]indirect blocks */\n\tindirect_blks = (chain + depth) - partial - 1;\n\n\t/*\n\t * Next look up the indirect map to count the totoal number of\n\t * direct blocks to allocate for this branch.\n\t */\n\tcount = ext4_blks_to_allocate(partial, indirect_blks,\n\t\t\t\t\tmaxblocks, blocks_to_boundary);\n\t/*\n\t * Block out ext4_truncate while we alter the tree\n\t */\n\terr = ext4_alloc_branch(handle, inode, iblock, indirect_blks,\n\t\t\t\t&count, goal,\n\t\t\t\toffsets + (partial - chain), partial);\n\n\t/*\n\t * The ext4_splice_branch call will free and forget any buffers\n\t * on the new chain if there is a failure, but that risks using\n\t * up transaction credits, especially for bitmaps where the\n\t * credits cannot be returned.  Can we handle this somehow?  We\n\t * may need to return -EAGAIN upwards in the worst case.  --sct\n\t */\n\tif (!err)\n\t\terr = ext4_splice_branch(handle, inode, iblock,\n\t\t\t\t\t partial, indirect_blks, count);\n\tif (err)\n\t\tgoto cleanup;\n\n\tset_buffer_new(bh_result);\n\n\text4_update_inode_fsync_trans(handle, inode, 1);\ngot_it:\n\tmap_bh(bh_result, inode->i_sb, le32_to_cpu(chain[depth-1].key));\n\tif (count > blocks_to_boundary)\n\t\tset_buffer_boundary(bh_result);\n\terr = count;\n\t/* Clean up and exit */\n\tpartial = chain + depth - 1;\t/* the whole chain */\ncleanup:\n\twhile (partial > chain) {\n\t\tBUFFER_TRACE(partial->bh, \"call brelse\");\n\t\tbrelse(partial->bh);\n\t\tpartial--;\n\t}\n\tBUFFER_TRACE(bh_result, \"returned\");\nout:\n\treturn err;\n}\n\n#ifdef CONFIG_QUOTA\nqsize_t *ext4_get_reserved_space(struct inode *inode)\n{\n\treturn &EXT4_I(inode)->i_reserved_quota;\n}\n#endif\n\n/*\n * Calculate the number of metadata blocks need to reserve\n * to allocate a new block at @lblocks for non extent file based file\n */\nstatic int ext4_indirect_calc_metadata_amount(struct inode *inode,\n\t\t\t\t\t      sector_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint dind_mask = EXT4_ADDR_PER_BLOCK(inode->i_sb) - 1;\n\tint blk_bits;\n\n\tif (lblock < EXT4_NDIR_BLOCKS)\n\t\treturn 0;\n\n\tlblock -= EXT4_NDIR_BLOCKS;\n\n\tif (ei->i_da_metadata_calc_len &&\n\t    (lblock & dind_mask) == ei->i_da_metadata_calc_last_lblock) {\n\t\tei->i_da_metadata_calc_len++;\n\t\treturn 0;\n\t}\n\tei->i_da_metadata_calc_last_lblock = lblock & dind_mask;\n\tei->i_da_metadata_calc_len = 1;\n\tblk_bits = roundup_pow_of_two(lblock + 1);\n\treturn (blk_bits / EXT4_ADDR_PER_BLOCK_BITS(inode->i_sb)) + 1;\n}\n\n/*\n * Calculate the number of metadata blocks need to reserve\n * to allocate a block located at @lblock\n */\nstatic int ext4_calc_metadata_amount(struct inode *inode, sector_t lblock)\n{\n\tif (EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL)\n\t\treturn ext4_ext_calc_metadata_amount(inode, lblock);\n\n\treturn ext4_indirect_calc_metadata_amount(inode, lblock);\n}\n\n/*\n * Called with i_data_sem down, which is important since we can call\n * ext4_discard_preallocations() from here.\n */\nvoid ext4_da_update_reserve_space(struct inode *inode,\n\t\t\t\t\tint used, int quota_claim)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint mdb_free = 0, allocated_meta_blocks = 0;\n\n\tspin_lock(&ei->i_block_reservation_lock);\n\ttrace_ext4_da_update_reserve_space(inode, used);\n\tif (unlikely(used > ei->i_reserved_data_blocks)) {\n\t\text4_msg(inode->i_sb, KERN_NOTICE, \"%s: ino %lu, used %d \"\n\t\t\t \"with only %d reserved data blocks\\n\",\n\t\t\t __func__, inode->i_ino, used,\n\t\t\t ei->i_reserved_data_blocks);\n\t\tWARN_ON(1);\n\t\tused = ei->i_reserved_data_blocks;\n\t}\n\n\t/* Update per-inode reservations */\n\tei->i_reserved_data_blocks -= used;\n\tused += ei->i_allocated_meta_blocks;\n\tei->i_reserved_meta_blocks -= ei->i_allocated_meta_blocks;\n\tallocated_meta_blocks = ei->i_allocated_meta_blocks;\n\tei->i_allocated_meta_blocks = 0;\n\tpercpu_counter_sub(&sbi->s_dirtyblocks_counter, used);\n\n\tif (ei->i_reserved_data_blocks == 0) {\n\t\t/*\n\t\t * We can release all of the reserved metadata blocks\n\t\t * only when we have written all of the delayed\n\t\t * allocation blocks.\n\t\t */\n\t\tmdb_free = ei->i_reserved_meta_blocks;\n\t\tei->i_reserved_meta_blocks = 0;\n\t\tei->i_da_metadata_calc_len = 0;\n\t\tpercpu_counter_sub(&sbi->s_dirtyblocks_counter, mdb_free);\n\t}\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\t/* Update quota subsystem */\n\tif (quota_claim) {\n\t\tvfs_dq_claim_block(inode, used);\n\t\tif (mdb_free)\n\t\t\tvfs_dq_release_reservation_block(inode, mdb_free);\n\t} else {\n\t\t/*\n\t\t * We did fallocate with an offset that is already delayed\n\t\t * allocated. So on delayed allocated writeback we should\n\t\t * not update the quota for allocated blocks. But then\n\t\t * converting an fallocate region to initialized region would\n\t\t * have caused a metadata allocation. So claim quota for\n\t\t * that\n\t\t */\n\t\tif (allocated_meta_blocks)\n\t\t\tvfs_dq_claim_block(inode, allocated_meta_blocks);\n\t\tvfs_dq_release_reservation_block(inode, mdb_free + used);\n\t}\n\n\t/*\n\t * If we have done all the pending block allocations and if\n\t * there aren't any writers on the inode, we can discard the\n\t * inode's preallocations.\n\t */\n\tif ((ei->i_reserved_data_blocks == 0) &&\n\t    (atomic_read(&inode->i_writecount) == 0))\n\t\text4_discard_preallocations(inode);\n}\n\nstatic int check_block_validity(struct inode *inode, const char *msg,\n\t\t\t\tsector_t logical, sector_t phys, int len)\n{\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), phys, len)) {\n\t\t__ext4_error(inode->i_sb, msg,\n\t\t\t   \"inode #%lu logical block %llu mapped to %llu \"\n\t\t\t   \"(size %d)\", inode->i_ino,\n\t\t\t   (unsigned long long) logical,\n\t\t\t   (unsigned long long) phys, len);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\n/*\n * Return the number of contiguous dirty pages in a given inode\n * starting at page frame idx.\n */\nstatic pgoff_t ext4_num_dirty_pages(struct inode *inode, pgoff_t idx,\n\t\t\t\t    unsigned int max_pages)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tpgoff_t\tindex;\n\tstruct pagevec pvec;\n\tpgoff_t num = 0;\n\tint i, nr_pages, done = 0;\n\n\tif (max_pages == 0)\n\t\treturn 0;\n\tpagevec_init(&pvec, 0);\n\twhile (!done) {\n\t\tindex = idx;\n\t\tnr_pages = pagevec_lookup_tag(&pvec, mapping, &index,\n\t\t\t\t\t      PAGECACHE_TAG_DIRTY,\n\t\t\t\t\t      (pgoff_t)PAGEVEC_SIZE);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\t\t\tstruct buffer_head *bh, *head;\n\n\t\t\tlock_page(page);\n\t\t\tif (unlikely(page->mapping != mapping) ||\n\t\t\t    !PageDirty(page) ||\n\t\t\t    PageWriteback(page) ||\n\t\t\t    page->index != idx) {\n\t\t\t\tdone = 1;\n\t\t\t\tunlock_page(page);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (page_has_buffers(page)) {\n\t\t\t\tbh = head = page_buffers(page);\n\t\t\t\tdo {\n\t\t\t\t\tif (!buffer_delay(bh) &&\n\t\t\t\t\t    !buffer_unwritten(bh))\n\t\t\t\t\t\tdone = 1;\n\t\t\t\t\tbh = bh->b_this_page;\n\t\t\t\t} while (!done && (bh != head));\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t\tif (done)\n\t\t\t\tbreak;\n\t\t\tidx++;\n\t\t\tnum++;\n\t\t\tif (num >= max_pages)\n\t\t\t\tbreak;\n\t\t}\n\t\tpagevec_release(&pvec);\n\t}\n\treturn num;\n}\n\n/*\n * The ext4_get_blocks() function tries to look up the requested blocks,\n * and returns if the blocks are already mapped.\n *\n * Otherwise it takes the write lock of the i_data_sem and allocate blocks\n * and store the allocated blocks in the result buffer head and mark it\n * mapped.\n *\n * If file type is extents based, it will call ext4_ext_get_blocks(),\n * Otherwise, call with ext4_ind_get_blocks() to handle indirect mapping\n * based files\n *\n * On success, it returns the number of blocks being mapped or allocate.\n * if create==0 and the blocks are pre-allocated and uninitialized block,\n * the result buffer head is unmapped. If the create ==1, it will make sure\n * the buffer head is mapped.\n *\n * It returns 0 if plain look up failed (blocks have not been allocated), in\n * that casem, buffer head is unmapped\n *\n * It returns the error in case of allocation failure.\n */\nint ext4_get_blocks(handle_t *handle, struct inode *inode, sector_t block,\n\t\t    unsigned int max_blocks, struct buffer_head *bh,\n\t\t    int flags)\n{\n\tint retval;\n\n\tclear_buffer_mapped(bh);\n\tclear_buffer_unwritten(bh);\n\n\text_debug(\"ext4_get_blocks(): inode %lu, flag %d, max_blocks %u,\"\n\t\t  \"logical block %lu\\n\", inode->i_ino, flags, max_blocks,\n\t\t  (unsigned long)block);\n\t/*\n\t * Try to see if we can get the block without requesting a new\n\t * file system block.\n\t */\n\tdown_read((&EXT4_I(inode)->i_data_sem));\n\tif (EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL) {\n\t\tretval =  ext4_ext_get_blocks(handle, inode, block, max_blocks,\n\t\t\t\tbh, 0);\n\t} else {\n\t\tretval = ext4_ind_get_blocks(handle, inode, block, max_blocks,\n\t\t\t\t\t     bh, 0);\n\t}\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\n\tif (retval > 0 && buffer_mapped(bh)) {\n\t\tint ret = check_block_validity(inode, \"file system corruption\",\n\t\t\t\t\t       block, bh->b_blocknr, retval);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\n\t/* If it is only a block(s) look up */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0)\n\t\treturn retval;\n\n\t/*\n\t * Returns if the blocks have already allocated\n\t *\n\t * Note that if blocks have been preallocated\n\t * ext4_ext_get_block() returns th create = 0\n\t * with buffer head unmapped.\n\t */\n\tif (retval > 0 && buffer_mapped(bh))\n\t\treturn retval;\n\n\t/*\n\t * When we call get_blocks without the create flag, the\n\t * BH_Unwritten flag could have gotten set if the blocks\n\t * requested were part of a uninitialized extent.  We need to\n\t * clear this flag now that we are committed to convert all or\n\t * part of the uninitialized extent to be an initialized\n\t * extent.  This is because we need to avoid the combination\n\t * of BH_Unwritten and BH_Mapped flags being simultaneously\n\t * set on the buffer_head.\n\t */\n\tclear_buffer_unwritten(bh);\n\n\t/*\n\t * New blocks allocate and/or writing to uninitialized extent\n\t * will possibly result in updating i_data, so we take\n\t * the write lock of i_data_sem, and call get_blocks()\n\t * with create == 1 flag.\n\t */\n\tdown_write((&EXT4_I(inode)->i_data_sem));\n\n\t/*\n\t * if the caller is from delayed allocation writeout path\n\t * we have already reserved fs blocks for allocation\n\t * let the underlying get_block() function know to\n\t * avoid double accounting\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tEXT4_I(inode)->i_delalloc_reserved_flag = 1;\n\t/*\n\t * We need to check for EXT4 here because migrate\n\t * could have changed the inode type in between\n\t */\n\tif (EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL) {\n\t\tretval =  ext4_ext_get_blocks(handle, inode, block, max_blocks,\n\t\t\t\t\t      bh, flags);\n\t} else {\n\t\tretval = ext4_ind_get_blocks(handle, inode, block,\n\t\t\t\t\t     max_blocks, bh, flags);\n\n\t\tif (retval > 0 && buffer_new(bh)) {\n\t\t\t/*\n\t\t\t * We allocated new blocks which will result in\n\t\t\t * i_data's format changing.  Force the migrate\n\t\t\t * to fail by clearing migrate flags\n\t\t\t */\n\t\t\text4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);\n\t\t}\n\n\t\t/*\n\t\t * Update reserved blocks/metadata blocks after successful\n\t\t * block allocation which had been deferred till now. We don't\n\t\t * support fallocate for non extent files. So we can update\n\t\t * reserve space here.\n\t\t */\n\t\tif ((retval > 0) &&\n\t\t\t(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE))\n\t\t\text4_da_update_reserve_space(inode, retval, 1);\n\t}\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tEXT4_I(inode)->i_delalloc_reserved_flag = 0;\n\n\tup_write((&EXT4_I(inode)->i_data_sem));\n\tif (retval > 0 && buffer_mapped(bh)) {\n\t\tint ret = check_block_validity(inode, \"file system \"\n\t\t\t\t\t       \"corruption after allocation\",\n\t\t\t\t\t       block, bh->b_blocknr, retval);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\treturn retval;\n}\n\n/* Maximum number of blocks we map for direct IO at once. */\n#define DIO_MAX_BLOCKS 4096\n\nint ext4_get_block(struct inode *inode, sector_t iblock,\n\t\t   struct buffer_head *bh_result, int create)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tint ret = 0, started = 0;\n\tunsigned max_blocks = bh_result->b_size >> inode->i_blkbits;\n\tint dio_credits;\n\n\tif (create && !handle) {\n\t\t/* Direct IO write... */\n\t\tif (max_blocks > DIO_MAX_BLOCKS)\n\t\t\tmax_blocks = DIO_MAX_BLOCKS;\n\t\tdio_credits = ext4_chunk_trans_blocks(inode, max_blocks);\n\t\thandle = ext4_journal_start(inode, dio_credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tgoto out;\n\t\t}\n\t\tstarted = 1;\n\t}\n\n\tret = ext4_get_blocks(handle, inode, iblock, max_blocks, bh_result,\n\t\t\t      create ? EXT4_GET_BLOCKS_CREATE : 0);\n\tif (ret > 0) {\n\t\tbh_result->b_size = (ret << inode->i_blkbits);\n\t\tret = 0;\n\t}\n\tif (started)\n\t\text4_journal_stop(handle);\nout:\n\treturn ret;\n}\n\n/*\n * `handle' can be NULL if create is zero\n */\nstruct buffer_head *ext4_getblk(handle_t *handle, struct inode *inode,\n\t\t\t\text4_lblk_t block, int create, int *errp)\n{\n\tstruct buffer_head dummy;\n\tint fatal = 0, err;\n\tint flags = 0;\n\n\tJ_ASSERT(handle != NULL || create == 0);\n\n\tdummy.b_state = 0;\n\tdummy.b_blocknr = -1000;\n\tbuffer_trace_init(&dummy.b_history);\n\tif (create)\n\t\tflags |= EXT4_GET_BLOCKS_CREATE;\n\terr = ext4_get_blocks(handle, inode, block, 1, &dummy, flags);\n\t/*\n\t * ext4_get_blocks() returns number of blocks mapped. 0 in\n\t * case of a HOLE.\n\t */\n\tif (err > 0) {\n\t\tif (err > 1)\n\t\t\tWARN_ON(1);\n\t\terr = 0;\n\t}\n\t*errp = err;\n\tif (!err && buffer_mapped(&dummy)) {\n\t\tstruct buffer_head *bh;\n\t\tbh = sb_getblk(inode->i_sb, dummy.b_blocknr);\n\t\tif (!bh) {\n\t\t\t*errp = -EIO;\n\t\t\tgoto err;\n\t\t}\n\t\tif (buffer_new(&dummy)) {\n\t\t\tJ_ASSERT(create != 0);\n\t\t\tJ_ASSERT(handle != NULL);\n\n\t\t\t/*\n\t\t\t * Now that we do not always journal data, we should\n\t\t\t * keep in mind whether this should always journal the\n\t\t\t * new buffer as metadata.  For now, regular file\n\t\t\t * writes use ext4_get_block instead, so it's not a\n\t\t\t * problem.\n\t\t\t */\n\t\t\tlock_buffer(bh);\n\t\t\tBUFFER_TRACE(bh, \"call get_create_access\");\n\t\t\tfatal = ext4_journal_get_create_access(handle, bh);\n\t\t\tif (!fatal && !buffer_uptodate(bh)) {\n\t\t\t\tmemset(bh->b_data, 0, inode->i_sb->s_blocksize);\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t}\n\t\t\tunlock_buffer(bh);\n\t\t\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\t\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\t\tif (!fatal)\n\t\t\t\tfatal = err;\n\t\t} else {\n\t\t\tBUFFER_TRACE(bh, \"not a new buffer\");\n\t\t}\n\t\tif (fatal) {\n\t\t\t*errp = fatal;\n\t\t\tbrelse(bh);\n\t\t\tbh = NULL;\n\t\t}\n\t\treturn bh;\n\t}\nerr:\n\treturn NULL;\n}\n\nstruct buffer_head *ext4_bread(handle_t *handle, struct inode *inode,\n\t\t\t       ext4_lblk_t block, int create, int *err)\n{\n\tstruct buffer_head *bh;\n\n\tbh = ext4_getblk(handle, inode, block, create, err);\n\tif (!bh)\n\t\treturn bh;\n\tif (buffer_uptodate(bh))\n\t\treturn bh;\n\tll_rw_block(READ_META, 1, &bh);\n\twait_on_buffer(bh);\n\tif (buffer_uptodate(bh))\n\t\treturn bh;\n\tput_bh(bh);\n\t*err = -EIO;\n\treturn NULL;\n}\n\nstatic int walk_page_buffers(handle_t *handle,\n\t\t\t     struct buffer_head *head,\n\t\t\t     unsigned from,\n\t\t\t     unsigned to,\n\t\t\t     int *partial,\n\t\t\t     int (*fn)(handle_t *handle,\n\t\t\t\t       struct buffer_head *bh))\n{\n\tstruct buffer_head *bh;\n\tunsigned block_start, block_end;\n\tunsigned blocksize = head->b_size;\n\tint err, ret = 0;\n\tstruct buffer_head *next;\n\n\tfor (bh = head, block_start = 0;\n\t     ret == 0 && (bh != head || !block_start);\n\t     block_start = block_end, bh = next) {\n\t\tnext = bh->b_this_page;\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end <= from || block_start >= to) {\n\t\t\tif (partial && !buffer_uptodate(bh))\n\t\t\t\t*partial = 1;\n\t\t\tcontinue;\n\t\t}\n\t\terr = (*fn)(handle, bh);\n\t\tif (!ret)\n\t\t\tret = err;\n\t}\n\treturn ret;\n}\n\n/*\n * To preserve ordering, it is essential that the hole instantiation and\n * the data write be encapsulated in a single transaction.  We cannot\n * close off a transaction and start a new one between the ext4_get_block()\n * and the commit_write().  So doing the jbd2_journal_start at the start of\n * prepare_write() is the right place.\n *\n * Also, this function can nest inside ext4_writepage() ->\n * block_write_full_page(). In that case, we *know* that ext4_writepage()\n * has generated enough buffer credits to do the whole page.  So we won't\n * block on the journal in that case, which is good, because the caller may\n * be PF_MEMALLOC.\n *\n * By accident, ext4 can be reentered when a transaction is open via\n * quota file writes.  If we were to commit the transaction while thus\n * reentered, there can be a deadlock - we would be holding a quota\n * lock, and the commit would never complete if another thread had a\n * transaction open and was blocking on the quota lock - a ranking\n * violation.\n *\n * So what we do is to rely on the fact that jbd2_journal_stop/journal_start\n * will _not_ run commit under these circumstances because handle->h_ref\n * is elevated.  We'll still have enough credits for the tiny quotafile\n * write.\n */\nstatic int do_journal_get_write_access(handle_t *handle,\n\t\t\t\t       struct buffer_head *bh)\n{\n\tif (!buffer_mapped(bh) || buffer_freed(bh))\n\t\treturn 0;\n\treturn ext4_journal_get_write_access(handle, bh);\n}\n\n/*\n * Truncate blocks that were not used by write. We have to truncate the\n * pagecache as well so that corresponding buffers get properly unmapped.\n */\nstatic void ext4_truncate_failed_write(struct inode *inode)\n{\n\ttruncate_inode_pages(inode->i_mapping, inode->i_size);\n\text4_truncate(inode);\n}\n\nstatic int ext4_write_begin(struct file *file, struct address_space *mapping,\n\t\t\t    loff_t pos, unsigned len, unsigned flags,\n\t\t\t    struct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tint ret, needed_blocks;\n\thandle_t *handle;\n\tint retries = 0;\n\tstruct page *page;\n\tpgoff_t index;\n\tunsigned from, to;\n\n\ttrace_ext4_write_begin(inode, pos, len, flags);\n\t/*\n\t * Reserve one block more for addition to orphan list in case\n\t * we allocate blocks but write fails for some reason\n\t */\n\tneeded_blocks = ext4_writepage_trans_blocks(inode) + 1;\n\tindex = pos >> PAGE_CACHE_SHIFT;\n\tfrom = pos & (PAGE_CACHE_SIZE - 1);\n\tto = from + len;\n\nretry:\n\thandle = ext4_journal_start(inode, needed_blocks);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out;\n\t}\n\n\t/* We cannot recurse into the filesystem as the transaction is already\n\t * started */\n\tflags |= AOP_FLAG_NOFS;\n\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page) {\n\t\text4_journal_stop(handle);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\t*pagep = page;\n\n\tret = block_write_begin(file, mapping, pos, len, flags, pagep, fsdata,\n\t\t\t\text4_get_block);\n\n\tif (!ret && ext4_should_journal_data(inode)) {\n\t\tret = walk_page_buffers(handle, page_buffers(page),\n\t\t\t\tfrom, to, NULL, do_journal_get_write_access);\n\t}\n\n\tif (ret) {\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\t/*\n\t\t * block_write_begin may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again. Don't need\n\t\t * i_size_read because we hold i_mutex.\n\t\t *\n\t\t * Add inode to orphan list in case we crash before\n\t\t * truncate finishes\n\t\t */\n\t\tif (pos + len > inode->i_size && ext4_can_truncate(inode))\n\t\t\text4_orphan_add(handle, inode);\n\n\t\text4_journal_stop(handle);\n\t\tif (pos + len > inode->i_size) {\n\t\t\text4_truncate_failed_write(inode);\n\t\t\t/*\n\t\t\t * If truncate failed early the inode might\n\t\t\t * still be on the orphan list; we need to\n\t\t\t * make sure the inode is removed from the\n\t\t\t * orphan list in that case.\n\t\t\t */\n\t\t\tif (inode->i_nlink)\n\t\t\t\text4_orphan_del(NULL, inode);\n\t\t}\n\t}\n\n\tif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry;\nout:\n\treturn ret;\n}\n\n/* For write_end() in data=journal mode */\nstatic int write_end_fn(handle_t *handle, struct buffer_head *bh)\n{\n\tif (!buffer_mapped(bh) || buffer_freed(bh))\n\t\treturn 0;\n\tset_buffer_uptodate(bh);\n\treturn ext4_handle_dirty_metadata(handle, NULL, bh);\n}\n\nstatic int ext4_generic_write_end(struct file *file,\n\t\t\t\t  struct address_space *mapping,\n\t\t\t\t  loff_t pos, unsigned len, unsigned copied,\n\t\t\t\t  struct page *page, void *fsdata)\n{\n\tint i_size_changed = 0;\n\tstruct inode *inode = mapping->host;\n\thandle_t *handle = ext4_journal_current_handle();\n\n\tcopied = block_write_end(file, mapping, pos, len, copied, page, fsdata);\n\n\t/*\n\t * No need to use i_size_read() here, the i_size\n\t * cannot change under us because we hold i_mutex.\n\t *\n\t * But it's important to update i_size while still holding page lock:\n\t * page writeout could otherwise come in and zero beyond i_size.\n\t */\n\tif (pos + copied > inode->i_size) {\n\t\ti_size_write(inode, pos + copied);\n\t\ti_size_changed = 1;\n\t}\n\n\tif (pos + copied >  EXT4_I(inode)->i_disksize) {\n\t\t/* We need to mark inode dirty even if\n\t\t * new_i_size is less that inode->i_size\n\t\t * bu greater than i_disksize.(hint delalloc)\n\t\t */\n\t\text4_update_i_disksize(inode, (pos + copied));\n\t\ti_size_changed = 1;\n\t}\n\tunlock_page(page);\n\tpage_cache_release(page);\n\n\t/*\n\t * Don't mark the inode dirty under page lock. First, it unnecessarily\n\t * makes the holding time of page lock longer. Second, it forces lock\n\t * ordering of page lock and transaction start for journaling\n\t * filesystems.\n\t */\n\tif (i_size_changed)\n\t\text4_mark_inode_dirty(handle, inode);\n\n\treturn copied;\n}\n\n/*\n * We need to pick up the new inode size which generic_commit_write gave us\n * `file' can be NULL - eg, when called from page_symlink().\n *\n * ext4 never places buffers on inode->i_mapping->private_list.  metadata\n * buffers are managed internally.\n */\nstatic int ext4_ordered_write_end(struct file *file,\n\t\t\t\t  struct address_space *mapping,\n\t\t\t\t  loff_t pos, unsigned len, unsigned copied,\n\t\t\t\t  struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tint ret = 0, ret2;\n\n\ttrace_ext4_ordered_write_end(inode, pos, len, copied);\n\tret = ext4_jbd2_file_inode(handle, inode);\n\n\tif (ret == 0) {\n\t\tret2 = ext4_generic_write_end(file, mapping, pos, len, copied,\n\t\t\t\t\t\t\tpage, fsdata);\n\t\tcopied = ret2;\n\t\tif (pos + len > inode->i_size && ext4_can_truncate(inode))\n\t\t\t/* if we have allocated more blocks and copied\n\t\t\t * less. We will have blocks allocated outside\n\t\t\t * inode->i_size. So truncate them\n\t\t\t */\n\t\t\text4_orphan_add(handle, inode);\n\t\tif (ret2 < 0)\n\t\t\tret = ret2;\n\t}\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\tif (pos + len > inode->i_size) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\n\treturn ret ? ret : copied;\n}\n\nstatic int ext4_writeback_write_end(struct file *file,\n\t\t\t\t    struct address_space *mapping,\n\t\t\t\t    loff_t pos, unsigned len, unsigned copied,\n\t\t\t\t    struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tint ret = 0, ret2;\n\n\ttrace_ext4_writeback_write_end(inode, pos, len, copied);\n\tret2 = ext4_generic_write_end(file, mapping, pos, len, copied,\n\t\t\t\t\t\t\tpage, fsdata);\n\tcopied = ret2;\n\tif (pos + len > inode->i_size && ext4_can_truncate(inode))\n\t\t/* if we have allocated more blocks and copied\n\t\t * less. We will have blocks allocated outside\n\t\t * inode->i_size. So truncate them\n\t\t */\n\t\text4_orphan_add(handle, inode);\n\n\tif (ret2 < 0)\n\t\tret = ret2;\n\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\tif (pos + len > inode->i_size) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\treturn ret ? ret : copied;\n}\n\nstatic int ext4_journalled_write_end(struct file *file,\n\t\t\t\t     struct address_space *mapping,\n\t\t\t\t     loff_t pos, unsigned len, unsigned copied,\n\t\t\t\t     struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tint ret = 0, ret2;\n\tint partial = 0;\n\tunsigned from, to;\n\tloff_t new_i_size;\n\n\ttrace_ext4_journalled_write_end(inode, pos, len, copied);\n\tfrom = pos & (PAGE_CACHE_SIZE - 1);\n\tto = from + len;\n\n\tif (copied < len) {\n\t\tif (!PageUptodate(page))\n\t\t\tcopied = 0;\n\t\tpage_zero_new_buffers(page, from+copied, to);\n\t}\n\n\tret = walk_page_buffers(handle, page_buffers(page), from,\n\t\t\t\tto, &partial, write_end_fn);\n\tif (!partial)\n\t\tSetPageUptodate(page);\n\tnew_i_size = pos + copied;\n\tif (new_i_size > inode->i_size)\n\t\ti_size_write(inode, pos+copied);\n\text4_set_inode_state(inode, EXT4_STATE_JDATA);\n\tif (new_i_size > EXT4_I(inode)->i_disksize) {\n\t\text4_update_i_disksize(inode, new_i_size);\n\t\tret2 = ext4_mark_inode_dirty(handle, inode);\n\t\tif (!ret)\n\t\t\tret = ret2;\n\t}\n\n\tunlock_page(page);\n\tpage_cache_release(page);\n\tif (pos + len > inode->i_size && ext4_can_truncate(inode))\n\t\t/* if we have allocated more blocks and copied\n\t\t * less. We will have blocks allocated outside\n\t\t * inode->i_size. So truncate them\n\t\t */\n\t\text4_orphan_add(handle, inode);\n\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\tif (pos + len > inode->i_size) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\treturn ret ? ret : copied;\n}\n\n/*\n * Reserve a single block located at lblock\n */\nstatic int ext4_da_reserve_space(struct inode *inode, sector_t lblock)\n{\n\tint retries = 0;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tunsigned long md_needed, md_reserved;\n\n\t/*\n\t * recalculate the amount of metadata blocks to reserve\n\t * in order to allocate nrblocks\n\t * worse case is one extent per block\n\t */\nrepeat:\n\tspin_lock(&ei->i_block_reservation_lock);\n\tmd_reserved = ei->i_reserved_meta_blocks;\n\tmd_needed = ext4_calc_metadata_amount(inode, lblock);\n\ttrace_ext4_da_reserve_space(inode, md_needed);\n\tspin_unlock(&ei->i_block_reservation_lock);\n\n\t/*\n\t * Make quota reservation here to prevent quota overflow\n\t * later. Real quota accounting is done at pages writeout\n\t * time.\n\t */\n\tif (vfs_dq_reserve_block(inode, md_needed + 1))\n\t\treturn -EDQUOT;\n\n\tif (ext4_claim_free_blocks(sbi, md_needed + 1)) {\n\t\tvfs_dq_release_reservation_block(inode, md_needed + 1);\n\t\tif (ext4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\t\tyield();\n\t\t\tgoto repeat;\n\t\t}\n\t\treturn -ENOSPC;\n\t}\n\tspin_lock(&ei->i_block_reservation_lock);\n\tei->i_reserved_data_blocks++;\n\tei->i_reserved_meta_blocks += md_needed;\n\tspin_unlock(&ei->i_block_reservation_lock);\n\n\treturn 0;       /* success */\n}\n\nstatic void ext4_da_release_space(struct inode *inode, int to_free)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\tif (!to_free)\n\t\treturn;\t\t/* Nothing to release, exit */\n\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\tif (unlikely(to_free > ei->i_reserved_data_blocks)) {\n\t\t/*\n\t\t * if there aren't enough reserved blocks, then the\n\t\t * counter is messed up somewhere.  Since this\n\t\t * function is called from invalidate page, it's\n\t\t * harmless to return without any action.\n\t\t */\n\t\text4_msg(inode->i_sb, KERN_NOTICE, \"ext4_da_release_space: \"\n\t\t\t \"ino %lu, to_free %d with only %d reserved \"\n\t\t\t \"data blocks\\n\", inode->i_ino, to_free,\n\t\t\t ei->i_reserved_data_blocks);\n\t\tWARN_ON(1);\n\t\tto_free = ei->i_reserved_data_blocks;\n\t}\n\tei->i_reserved_data_blocks -= to_free;\n\n\tif (ei->i_reserved_data_blocks == 0) {\n\t\t/*\n\t\t * We can release all of the reserved metadata blocks\n\t\t * only when we have written all of the delayed\n\t\t * allocation blocks.\n\t\t */\n\t\tto_free += ei->i_reserved_meta_blocks;\n\t\tei->i_reserved_meta_blocks = 0;\n\t\tei->i_da_metadata_calc_len = 0;\n\t}\n\n\t/* update fs dirty blocks counter */\n\tpercpu_counter_sub(&sbi->s_dirtyblocks_counter, to_free);\n\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\tvfs_dq_release_reservation_block(inode, to_free);\n}\n\nstatic void ext4_da_page_release_reservation(struct page *page,\n\t\t\t\t\t     unsigned long offset)\n{\n\tint to_release = 0;\n\tstruct buffer_head *head, *bh;\n\tunsigned int curr_off = 0;\n\n\thead = page_buffers(page);\n\tbh = head;\n\tdo {\n\t\tunsigned int next_off = curr_off + bh->b_size;\n\n\t\tif ((offset <= curr_off) && (buffer_delay(bh))) {\n\t\t\tto_release++;\n\t\t\tclear_buffer_delay(bh);\n\t\t}\n\t\tcurr_off = next_off;\n\t} while ((bh = bh->b_this_page) != head);\n\text4_da_release_space(page->mapping->host, to_release);\n}\n\n/*\n * Delayed allocation stuff\n */\n\n/*\n * mpage_da_submit_io - walks through extent of pages and try to write\n * them with writepage() call back\n *\n * @mpd->inode: inode\n * @mpd->first_page: first page of the extent\n * @mpd->next_page: page after the last page of the extent\n *\n * By the time mpage_da_submit_io() is called we expect all blocks\n * to be allocated. this may be wrong if allocation failed.\n *\n * As pages are already locked by write_cache_pages(), we can't use it\n */\nstatic int mpage_da_submit_io(struct mpage_da_data *mpd)\n{\n\tlong pages_skipped;\n\tstruct pagevec pvec;\n\tunsigned long index, end;\n\tint ret = 0, err, nr_pages, i;\n\tstruct inode *inode = mpd->inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\tBUG_ON(mpd->next_page <= mpd->first_page);\n\t/*\n\t * We need to start from the first_page to the next_page - 1\n\t * to make sure we also write the mapped dirty buffer_heads.\n\t * If we look at mpd->b_blocknr we would only be looking\n\t * at the currently mapped buffer_heads.\n\t */\n\tindex = mpd->first_page;\n\tend = mpd->next_page - 1;\n\n\tpagevec_init(&pvec, 0);\n\twhile (index <= end) {\n\t\tnr_pages = pagevec_lookup(&pvec, mapping, index, PAGEVEC_SIZE);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tindex = page->index;\n\t\t\tif (index > end)\n\t\t\t\tbreak;\n\t\t\tindex++;\n\n\t\t\tBUG_ON(!PageLocked(page));\n\t\t\tBUG_ON(PageWriteback(page));\n\n\t\t\tpages_skipped = mpd->wbc->pages_skipped;\n\t\t\terr = mapping->a_ops->writepage(page, mpd->wbc);\n\t\t\tif (!err && (pages_skipped == mpd->wbc->pages_skipped))\n\t\t\t\t/*\n\t\t\t\t * have successfully written the page\n\t\t\t\t * without skipping the same\n\t\t\t\t */\n\t\t\t\tmpd->pages_written++;\n\t\t\t/*\n\t\t\t * In error case, we have to continue because\n\t\t\t * remaining pages are still locked\n\t\t\t * XXX: unlock and re-dirty them?\n\t\t\t */\n\t\t\tif (ret == 0)\n\t\t\t\tret = err;\n\t\t}\n\t\tpagevec_release(&pvec);\n\t}\n\treturn ret;\n}\n\n/*\n * mpage_put_bnr_to_bhs - walk blocks and assign them actual numbers\n *\n * @mpd->inode - inode to walk through\n * @exbh->b_blocknr - first block on a disk\n * @exbh->b_size - amount of space in bytes\n * @logical - first logical block to start assignment with\n *\n * the function goes through all passed space and put actual disk\n * block numbers into buffer heads, dropping BH_Delay and BH_Unwritten\n */\nstatic void mpage_put_bnr_to_bhs(struct mpage_da_data *mpd, sector_t logical,\n\t\t\t\t struct buffer_head *exbh)\n{\n\tstruct inode *inode = mpd->inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\tint blocks = exbh->b_size >> inode->i_blkbits;\n\tsector_t pblock = exbh->b_blocknr, cur_logical;\n\tstruct buffer_head *head, *bh;\n\tpgoff_t index, end;\n\tstruct pagevec pvec;\n\tint nr_pages, i;\n\n\tindex = logical >> (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\tend = (logical + blocks - 1) >> (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\tcur_logical = index << (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\n\tpagevec_init(&pvec, 0);\n\n\twhile (index <= end) {\n\t\t/* XXX: optimize tail */\n\t\tnr_pages = pagevec_lookup(&pvec, mapping, index, PAGEVEC_SIZE);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tindex = page->index;\n\t\t\tif (index > end)\n\t\t\t\tbreak;\n\t\t\tindex++;\n\n\t\t\tBUG_ON(!PageLocked(page));\n\t\t\tBUG_ON(PageWriteback(page));\n\t\t\tBUG_ON(!page_has_buffers(page));\n\n\t\t\tbh = page_buffers(page);\n\t\t\thead = bh;\n\n\t\t\t/* skip blocks out of the range */\n\t\t\tdo {\n\t\t\t\tif (cur_logical >= logical)\n\t\t\t\t\tbreak;\n\t\t\t\tcur_logical++;\n\t\t\t} while ((bh = bh->b_this_page) != head);\n\n\t\t\tdo {\n\t\t\t\tif (cur_logical >= logical + blocks)\n\t\t\t\t\tbreak;\n\n\t\t\t\tif (buffer_delay(bh) ||\n\t\t\t\t\t\tbuffer_unwritten(bh)) {\n\n\t\t\t\t\tBUG_ON(bh->b_bdev != inode->i_sb->s_bdev);\n\n\t\t\t\t\tif (buffer_delay(bh)) {\n\t\t\t\t\t\tclear_buffer_delay(bh);\n\t\t\t\t\t\tbh->b_blocknr = pblock;\n\t\t\t\t\t} else {\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * unwritten already should have\n\t\t\t\t\t\t * blocknr assigned. Verify that\n\t\t\t\t\t\t */\n\t\t\t\t\t\tclear_buffer_unwritten(bh);\n\t\t\t\t\t\tBUG_ON(bh->b_blocknr != pblock);\n\t\t\t\t\t}\n\n\t\t\t\t} else if (buffer_mapped(bh))\n\t\t\t\t\tBUG_ON(bh->b_blocknr != pblock);\n\n\t\t\t\tcur_logical++;\n\t\t\t\tpblock++;\n\t\t\t} while ((bh = bh->b_this_page) != head);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t}\n}\n\n\n/*\n * __unmap_underlying_blocks - just a helper function to unmap\n * set of blocks described by @bh\n */\nstatic inline void __unmap_underlying_blocks(struct inode *inode,\n\t\t\t\t\t     struct buffer_head *bh)\n{\n\tstruct block_device *bdev = inode->i_sb->s_bdev;\n\tint blocks, i;\n\n\tblocks = bh->b_size >> inode->i_blkbits;\n\tfor (i = 0; i < blocks; i++)\n\t\tunmap_underlying_metadata(bdev, bh->b_blocknr + i);\n}\n\nstatic void ext4_da_block_invalidatepages(struct mpage_da_data *mpd,\n\t\t\t\t\tsector_t logical, long blk_cnt)\n{\n\tint nr_pages, i;\n\tpgoff_t index, end;\n\tstruct pagevec pvec;\n\tstruct inode *inode = mpd->inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\tindex = logical >> (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\tend   = (logical + blk_cnt - 1) >>\n\t\t\t\t(PAGE_CACHE_SHIFT - inode->i_blkbits);\n\twhile (index <= end) {\n\t\tnr_pages = pagevec_lookup(&pvec, mapping, index, PAGEVEC_SIZE);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\t\t\tindex = page->index;\n\t\t\tif (index > end)\n\t\t\t\tbreak;\n\t\t\tindex++;\n\n\t\t\tBUG_ON(!PageLocked(page));\n\t\t\tBUG_ON(PageWriteback(page));\n\t\t\tblock_invalidatepage(page, 0);\n\t\t\tClearPageUptodate(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\n\treturn;\n}\n\nstatic void ext4_print_free_blocks(struct inode *inode)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tprintk(KERN_CRIT \"Total free blocks count %lld\\n\",\n\t       ext4_count_free_blocks(inode->i_sb));\n\tprintk(KERN_CRIT \"Free/Dirty block details\\n\");\n\tprintk(KERN_CRIT \"free_blocks=%lld\\n\",\n\t       (long long) percpu_counter_sum(&sbi->s_freeblocks_counter));\n\tprintk(KERN_CRIT \"dirty_blocks=%lld\\n\",\n\t       (long long) percpu_counter_sum(&sbi->s_dirtyblocks_counter));\n\tprintk(KERN_CRIT \"Block reservation details\\n\");\n\tprintk(KERN_CRIT \"i_reserved_data_blocks=%u\\n\",\n\t       EXT4_I(inode)->i_reserved_data_blocks);\n\tprintk(KERN_CRIT \"i_reserved_meta_blocks=%u\\n\",\n\t       EXT4_I(inode)->i_reserved_meta_blocks);\n\treturn;\n}\n\n/*\n * mpage_da_map_blocks - go through given space\n *\n * @mpd - bh describing space\n *\n * The function skips space we know is already mapped to disk blocks.\n *\n */\nstatic int mpage_da_map_blocks(struct mpage_da_data *mpd)\n{\n\tint err, blks, get_blocks_flags;\n\tstruct buffer_head new;\n\tsector_t next = mpd->b_blocknr;\n\tunsigned max_blocks = mpd->b_size >> mpd->inode->i_blkbits;\n\tloff_t disksize = EXT4_I(mpd->inode)->i_disksize;\n\thandle_t *handle = NULL;\n\n\t/*\n\t * We consider only non-mapped and non-allocated blocks\n\t */\n\tif ((mpd->b_state  & (1 << BH_Mapped)) &&\n\t\t!(mpd->b_state & (1 << BH_Delay)) &&\n\t\t!(mpd->b_state & (1 << BH_Unwritten)))\n\t\treturn 0;\n\n\t/*\n\t * If we didn't accumulate anything to write simply return\n\t */\n\tif (!mpd->b_size)\n\t\treturn 0;\n\n\thandle = ext4_journal_current_handle();\n\tBUG_ON(!handle);\n\n\t/*\n\t * Call ext4_get_blocks() to allocate any delayed allocation\n\t * blocks, or to convert an uninitialized extent to be\n\t * initialized (in the case where we have written into\n\t * one or more preallocated blocks).\n\t *\n\t * We pass in the magic EXT4_GET_BLOCKS_DELALLOC_RESERVE to\n\t * indicate that we are on the delayed allocation path.  This\n\t * affects functions in many different parts of the allocation\n\t * call path.  This flag exists primarily because we don't\n\t * want to change *many* call functions, so ext4_get_blocks()\n\t * will set the magic i_delalloc_reserved_flag once the\n\t * inode's allocation semaphore is taken.\n\t *\n\t * If the blocks in questions were delalloc blocks, set\n\t * EXT4_GET_BLOCKS_DELALLOC_RESERVE so the delalloc accounting\n\t * variables are updated after the blocks have been allocated.\n\t */\n\tnew.b_state = 0;\n\tget_blocks_flags = EXT4_GET_BLOCKS_CREATE;\n\tif (mpd->b_state & (1 << BH_Delay))\n\t\tget_blocks_flags |= EXT4_GET_BLOCKS_DELALLOC_RESERVE;\n\n\tblks = ext4_get_blocks(handle, mpd->inode, next, max_blocks,\n\t\t\t       &new, get_blocks_flags);\n\tif (blks < 0) {\n\t\terr = blks;\n\t\t/*\n\t\t * If get block returns with error we simply\n\t\t * return. Later writepage will redirty the page and\n\t\t * writepages will find the dirty page again\n\t\t */\n\t\tif (err == -EAGAIN)\n\t\t\treturn 0;\n\n\t\tif (err == -ENOSPC &&\n\t\t    ext4_count_free_blocks(mpd->inode->i_sb)) {\n\t\t\tmpd->retval = err;\n\t\t\treturn 0;\n\t\t}\n\n\t\t/*\n\t\t * get block failure will cause us to loop in\n\t\t * writepages, because a_ops->writepage won't be able\n\t\t * to make progress. The page will be redirtied by\n\t\t * writepage and writepages will again try to write\n\t\t * the same.\n\t\t */\n\t\text4_msg(mpd->inode->i_sb, KERN_CRIT,\n\t\t\t \"delayed block allocation failed for inode %lu at \"\n\t\t\t \"logical offset %llu with max blocks %zd with \"\n\t\t\t \"error %d\\n\", mpd->inode->i_ino,\n\t\t\t (unsigned long long) next,\n\t\t\t mpd->b_size >> mpd->inode->i_blkbits, err);\n\t\tprintk(KERN_CRIT \"This should not happen!!  \"\n\t\t       \"Data will be lost\\n\");\n\t\tif (err == -ENOSPC) {\n\t\t\text4_print_free_blocks(mpd->inode);\n\t\t}\n\t\t/* invalidate all the pages */\n\t\text4_da_block_invalidatepages(mpd, next,\n\t\t\t\tmpd->b_size >> mpd->inode->i_blkbits);\n\t\treturn err;\n\t}\n\tBUG_ON(blks == 0);\n\n\tnew.b_size = (blks << mpd->inode->i_blkbits);\n\n\tif (buffer_new(&new))\n\t\t__unmap_underlying_blocks(mpd->inode, &new);\n\n\t/*\n\t * If blocks are delayed marked, we need to\n\t * put actual blocknr and drop delayed bit\n\t */\n\tif ((mpd->b_state & (1 << BH_Delay)) ||\n\t    (mpd->b_state & (1 << BH_Unwritten)))\n\t\tmpage_put_bnr_to_bhs(mpd, next, &new);\n\n\tif (ext4_should_order_data(mpd->inode)) {\n\t\terr = ext4_jbd2_file_inode(handle, mpd->inode);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/*\n\t * Update on-disk size along with block allocation.\n\t */\n\tdisksize = ((loff_t) next + blks) << mpd->inode->i_blkbits;\n\tif (disksize > i_size_read(mpd->inode))\n\t\tdisksize = i_size_read(mpd->inode);\n\tif (disksize > EXT4_I(mpd->inode)->i_disksize) {\n\t\text4_update_i_disksize(mpd->inode, disksize);\n\t\treturn ext4_mark_inode_dirty(handle, mpd->inode);\n\t}\n\n\treturn 0;\n}\n\n#define BH_FLAGS ((1 << BH_Uptodate) | (1 << BH_Mapped) | \\\n\t\t(1 << BH_Delay) | (1 << BH_Unwritten))\n\n/*\n * mpage_add_bh_to_extent - try to add one more block to extent of blocks\n *\n * @mpd->lbh - extent of blocks\n * @logical - logical number of the block in the file\n * @bh - bh of the block (used to access block's state)\n *\n * the function is used to collect contig. blocks in same state\n */\nstatic void mpage_add_bh_to_extent(struct mpage_da_data *mpd,\n\t\t\t\t   sector_t logical, size_t b_size,\n\t\t\t\t   unsigned long b_state)\n{\n\tsector_t next;\n\tint nrblocks = mpd->b_size >> mpd->inode->i_blkbits;\n\n\t/* check if thereserved journal credits might overflow */\n\tif (!(EXT4_I(mpd->inode)->i_flags & EXT4_EXTENTS_FL)) {\n\t\tif (nrblocks >= EXT4_MAX_TRANS_DATA) {\n\t\t\t/*\n\t\t\t * With non-extent format we are limited by the journal\n\t\t\t * credit available.  Total credit needed to insert\n\t\t\t * nrblocks contiguous blocks is dependent on the\n\t\t\t * nrblocks.  So limit nrblocks.\n\t\t\t */\n\t\t\tgoto flush_it;\n\t\t} else if ((nrblocks + (b_size >> mpd->inode->i_blkbits)) >\n\t\t\t\tEXT4_MAX_TRANS_DATA) {\n\t\t\t/*\n\t\t\t * Adding the new buffer_head would make it cross the\n\t\t\t * allowed limit for which we have journal credit\n\t\t\t * reserved. So limit the new bh->b_size\n\t\t\t */\n\t\t\tb_size = (EXT4_MAX_TRANS_DATA - nrblocks) <<\n\t\t\t\t\t\tmpd->inode->i_blkbits;\n\t\t\t/* we will do mpage_da_submit_io in the next loop */\n\t\t}\n\t}\n\t/*\n\t * First block in the extent\n\t */\n\tif (mpd->b_size == 0) {\n\t\tmpd->b_blocknr = logical;\n\t\tmpd->b_size = b_size;\n\t\tmpd->b_state = b_state & BH_FLAGS;\n\t\treturn;\n\t}\n\n\tnext = mpd->b_blocknr + nrblocks;\n\t/*\n\t * Can we merge the block to our big extent?\n\t */\n\tif (logical == next && (b_state & BH_FLAGS) == mpd->b_state) {\n\t\tmpd->b_size += b_size;\n\t\treturn;\n\t}\n\nflush_it:\n\t/*\n\t * We couldn't merge the block to our extent, so we\n\t * need to flush current  extent and start new one\n\t */\n\tif (mpage_da_map_blocks(mpd) == 0)\n\t\tmpage_da_submit_io(mpd);\n\tmpd->io_done = 1;\n\treturn;\n}\n\nstatic int ext4_bh_delay_or_unwritten(handle_t *handle, struct buffer_head *bh)\n{\n\treturn (buffer_delay(bh) || buffer_unwritten(bh)) && buffer_dirty(bh);\n}\n\n/*\n * __mpage_da_writepage - finds extent of pages and blocks\n *\n * @page: page to consider\n * @wbc: not used, we just follow rules\n * @data: context\n *\n * The function finds extents of pages and scan them for all blocks.\n */\nstatic int __mpage_da_writepage(struct page *page,\n\t\t\t\tstruct writeback_control *wbc, void *data)\n{\n\tstruct mpage_da_data *mpd = data;\n\tstruct inode *inode = mpd->inode;\n\tstruct buffer_head *bh, *head;\n\tsector_t logical;\n\n\tif (mpd->io_done) {\n\t\t/*\n\t\t * Rest of the page in the page_vec\n\t\t * redirty then and skip then. We will\n\t\t * try to write them again after\n\t\t * starting a new transaction\n\t\t */\n\t\tredirty_page_for_writepage(wbc, page);\n\t\tunlock_page(page);\n\t\treturn MPAGE_DA_EXTENT_TAIL;\n\t}\n\t/*\n\t * Can we merge this page to current extent?\n\t */\n\tif (mpd->next_page != page->index) {\n\t\t/*\n\t\t * Nope, we can't. So, we map non-allocated blocks\n\t\t * and start IO on them using writepage()\n\t\t */\n\t\tif (mpd->next_page != mpd->first_page) {\n\t\t\tif (mpage_da_map_blocks(mpd) == 0)\n\t\t\t\tmpage_da_submit_io(mpd);\n\t\t\t/*\n\t\t\t * skip rest of the page in the page_vec\n\t\t\t */\n\t\t\tmpd->io_done = 1;\n\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\tunlock_page(page);\n\t\t\treturn MPAGE_DA_EXTENT_TAIL;\n\t\t}\n\n\t\t/*\n\t\t * Start next extent of pages ...\n\t\t */\n\t\tmpd->first_page = page->index;\n\n\t\t/*\n\t\t * ... and blocks\n\t\t */\n\t\tmpd->b_size = 0;\n\t\tmpd->b_state = 0;\n\t\tmpd->b_blocknr = 0;\n\t}\n\n\tmpd->next_page = page->index + 1;\n\tlogical = (sector_t) page->index <<\n\t\t  (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\n\tif (!page_has_buffers(page)) {\n\t\tmpage_add_bh_to_extent(mpd, logical, PAGE_CACHE_SIZE,\n\t\t\t\t       (1 << BH_Dirty) | (1 << BH_Uptodate));\n\t\tif (mpd->io_done)\n\t\t\treturn MPAGE_DA_EXTENT_TAIL;\n\t} else {\n\t\t/*\n\t\t * Page with regular buffer heads, just add all dirty ones\n\t\t */\n\t\thead = page_buffers(page);\n\t\tbh = head;\n\t\tdo {\n\t\t\tBUG_ON(buffer_locked(bh));\n\t\t\t/*\n\t\t\t * We need to try to allocate\n\t\t\t * unmapped blocks in the same page.\n\t\t\t * Otherwise we won't make progress\n\t\t\t * with the page in ext4_writepage\n\t\t\t */\n\t\t\tif (ext4_bh_delay_or_unwritten(NULL, bh)) {\n\t\t\t\tmpage_add_bh_to_extent(mpd, logical,\n\t\t\t\t\t\t       bh->b_size,\n\t\t\t\t\t\t       bh->b_state);\n\t\t\t\tif (mpd->io_done)\n\t\t\t\t\treturn MPAGE_DA_EXTENT_TAIL;\n\t\t\t} else if (buffer_dirty(bh) && (buffer_mapped(bh))) {\n\t\t\t\t/*\n\t\t\t\t * mapped dirty buffer. We need to update\n\t\t\t\t * the b_state because we look at\n\t\t\t\t * b_state in mpage_da_map_blocks. We don't\n\t\t\t\t * update b_size because if we find an\n\t\t\t\t * unmapped buffer_head later we need to\n\t\t\t\t * use the b_state flag of that buffer_head.\n\t\t\t\t */\n\t\t\t\tif (mpd->b_size == 0)\n\t\t\t\t\tmpd->b_state = bh->b_state & BH_FLAGS;\n\t\t\t}\n\t\t\tlogical++;\n\t\t} while ((bh = bh->b_this_page) != head);\n\t}\n\n\treturn 0;\n}\n\n/*\n * This is a special get_blocks_t callback which is used by\n * ext4_da_write_begin().  It will either return mapped block or\n * reserve space for a single block.\n *\n * For delayed buffer_head we have BH_Mapped, BH_New, BH_Delay set.\n * We also have b_blocknr = -1 and b_bdev initialized properly\n *\n * For unwritten buffer_head we have BH_Mapped, BH_New, BH_Unwritten set.\n * We also have b_blocknr = physicalblock mapping unwritten extent and b_bdev\n * initialized properly.\n */\nstatic int ext4_da_get_block_prep(struct inode *inode, sector_t iblock,\n\t\t\t\t  struct buffer_head *bh_result, int create)\n{\n\tint ret = 0;\n\tsector_t invalid_block = ~((sector_t) 0xffff);\n\n\tif (invalid_block < ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es))\n\t\tinvalid_block = ~0;\n\n\tBUG_ON(create == 0);\n\tBUG_ON(bh_result->b_size != inode->i_sb->s_blocksize);\n\n\t/*\n\t * first, we need to know whether the block is allocated already\n\t * preallocated blocks are unmapped but should treated\n\t * the same as allocated blocks.\n\t */\n\tret = ext4_get_blocks(NULL, inode, iblock, 1,  bh_result, 0);\n\tif ((ret == 0) && !buffer_delay(bh_result)) {\n\t\t/* the block isn't (pre)allocated yet, let's reserve space */\n\t\t/*\n\t\t * XXX: __block_prepare_write() unmaps passed block,\n\t\t * is it OK?\n\t\t */\n\t\tret = ext4_da_reserve_space(inode, iblock);\n\t\tif (ret)\n\t\t\t/* not enough space to reserve */\n\t\t\treturn ret;\n\n\t\tmap_bh(bh_result, inode->i_sb, invalid_block);\n\t\tset_buffer_new(bh_result);\n\t\tset_buffer_delay(bh_result);\n\t} else if (ret > 0) {\n\t\tbh_result->b_size = (ret << inode->i_blkbits);\n\t\tif (buffer_unwritten(bh_result)) {\n\t\t\t/* A delayed write to unwritten bh should\n\t\t\t * be marked new and mapped.  Mapped ensures\n\t\t\t * that we don't do get_block multiple times\n\t\t\t * when we write to the same offset and new\n\t\t\t * ensures that we do proper zero out for\n\t\t\t * partial write.\n\t\t\t */\n\t\t\tset_buffer_new(bh_result);\n\t\t\tset_buffer_mapped(bh_result);\n\t\t}\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n/*\n * This function is used as a standard get_block_t calback function\n * when there is no desire to allocate any blocks.  It is used as a\n * callback function for block_prepare_write(), nobh_writepage(), and\n * block_write_full_page().  These functions should only try to map a\n * single block at a time.\n *\n * Since this function doesn't do block allocations even if the caller\n * requests it by passing in create=1, it is critically important that\n * any caller checks to make sure that any buffer heads are returned\n * by this function are either all already mapped or marked for\n * delayed allocation before calling nobh_writepage() or\n * block_write_full_page().  Otherwise, b_blocknr could be left\n * unitialized, and the page write functions will be taken by\n * surprise.\n */\nstatic int noalloc_get_block_write(struct inode *inode, sector_t iblock,\n\t\t\t\t   struct buffer_head *bh_result, int create)\n{\n\tint ret = 0;\n\tunsigned max_blocks = bh_result->b_size >> inode->i_blkbits;\n\n\tBUG_ON(bh_result->b_size != inode->i_sb->s_blocksize);\n\n\t/*\n\t * we don't want to do block allocation in writepage\n\t * so call get_block_wrap with create = 0\n\t */\n\tret = ext4_get_blocks(NULL, inode, iblock, max_blocks, bh_result, 0);\n\tif (ret > 0) {\n\t\tbh_result->b_size = (ret << inode->i_blkbits);\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nstatic int bget_one(handle_t *handle, struct buffer_head *bh)\n{\n\tget_bh(bh);\n\treturn 0;\n}\n\nstatic int bput_one(handle_t *handle, struct buffer_head *bh)\n{\n\tput_bh(bh);\n\treturn 0;\n}\n\nstatic int __ext4_journalled_writepage(struct page *page,\n\t\t\t\t       unsigned int len)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct buffer_head *page_bufs;\n\thandle_t *handle = NULL;\n\tint ret = 0;\n\tint err;\n\n\tpage_bufs = page_buffers(page);\n\tBUG_ON(!page_bufs);\n\twalk_page_buffers(handle, page_bufs, 0, len, NULL, bget_one);\n\t/* As soon as we unlock the page, it can go away, but we have\n\t * references to buffers so we are safe */\n\tunlock_page(page);\n\n\thandle = ext4_journal_start(inode, ext4_writepage_trans_blocks(inode));\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out;\n\t}\n\n\tret = walk_page_buffers(handle, page_bufs, 0, len, NULL,\n\t\t\t\tdo_journal_get_write_access);\n\n\terr = walk_page_buffers(handle, page_bufs, 0, len, NULL,\n\t\t\t\twrite_end_fn);\n\tif (ret == 0)\n\t\tret = err;\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\n\twalk_page_buffers(handle, page_bufs, 0, len, NULL, bput_one);\n\text4_set_inode_state(inode, EXT4_STATE_JDATA);\nout:\n\treturn ret;\n}\n\n/*\n * Note that we don't need to start a transaction unless we're journaling data\n * because we should have holes filled from ext4_page_mkwrite(). We even don't\n * need to file the inode to the transaction's list in ordered mode because if\n * we are writing back data added by write(), the inode is already there and if\n * we are writing back data modified via mmap(), noone guarantees in which\n * transaction the data will hit the disk. In case we are journaling data, we\n * cannot start transaction directly because transaction start ranks above page\n * lock so we have to do some magic.\n *\n * This function can get called via...\n *   - ext4_da_writepages after taking page lock (have journal handle)\n *   - journal_submit_inode_data_buffers (no journal handle)\n *   - shrink_page_list via pdflush (no journal handle)\n *   - grab_page_cache when doing write_begin (have journal handle)\n *\n * We don't do any block allocation in this function. If we have page with\n * multiple blocks we need to write those buffer_heads that are mapped. This\n * is important for mmaped based write. So if we do with blocksize 1K\n * truncate(f, 1024);\n * a = mmap(f, 0, 4096);\n * a[0] = 'a';\n * truncate(f, 4096);\n * we have in the page first buffer_head mapped via page_mkwrite call back\n * but other bufer_heads would be unmapped but dirty(dirty done via the\n * do_wp_page). So writepage should write the first block. If we modify\n * the mmap area beyond 1024 we will again get a page_fault and the\n * page_mkwrite callback will do the block allocation and mark the\n * buffer_heads mapped.\n *\n * We redirty the page if we have any buffer_heads that is either delay or\n * unwritten in the page.\n *\n * We can get recursively called as show below.\n *\n *\text4_writepage() -> kmalloc() -> __alloc_pages() -> page_launder() ->\n *\t\text4_writepage()\n *\n * But since we don't do any block allocation we should not deadlock.\n * Page also have the dirty flag cleared so we don't get recurive page_lock.\n */\nstatic int ext4_writepage(struct page *page,\n\t\t\t  struct writeback_control *wbc)\n{\n\tint ret = 0;\n\tloff_t size;\n\tunsigned int len;\n\tstruct buffer_head *page_bufs;\n\tstruct inode *inode = page->mapping->host;\n\n\ttrace_ext4_writepage(inode, page);\n\tsize = i_size_read(inode);\n\tif (page->index == size >> PAGE_CACHE_SHIFT)\n\t\tlen = size & ~PAGE_CACHE_MASK;\n\telse\n\t\tlen = PAGE_CACHE_SIZE;\n\n\tif (page_has_buffers(page)) {\n\t\tpage_bufs = page_buffers(page);\n\t\tif (walk_page_buffers(NULL, page_bufs, 0, len, NULL,\n\t\t\t\t\text4_bh_delay_or_unwritten)) {\n\t\t\t/*\n\t\t\t * We don't want to do  block allocation\n\t\t\t * So redirty the page and return\n\t\t\t * We may reach here when we do a journal commit\n\t\t\t * via journal_submit_inode_data_buffers.\n\t\t\t * If we don't have mapping block we just ignore\n\t\t\t * them. We can also reach here via shrink_page_list\n\t\t\t */\n\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\tunlock_page(page);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * The test for page_has_buffers() is subtle:\n\t\t * We know the page is dirty but it lost buffers. That means\n\t\t * that at some moment in time after write_begin()/write_end()\n\t\t * has been called all buffers have been clean and thus they\n\t\t * must have been written at least once. So they are all\n\t\t * mapped and we can happily proceed with mapping them\n\t\t * and writing the page.\n\t\t *\n\t\t * Try to initialize the buffer_heads and check whether\n\t\t * all are mapped and non delay. We don't want to\n\t\t * do block allocation here.\n\t\t */\n\t\tret = block_prepare_write(page, 0, len,\n\t\t\t\t\t  noalloc_get_block_write);\n\t\tif (!ret) {\n\t\t\tpage_bufs = page_buffers(page);\n\t\t\t/* check whether all are mapped and non delay */\n\t\t\tif (walk_page_buffers(NULL, page_bufs, 0, len, NULL,\n\t\t\t\t\t\text4_bh_delay_or_unwritten)) {\n\t\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\t\tunlock_page(page);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * We can't do block allocation here\n\t\t\t * so just redity the page and unlock\n\t\t\t * and return\n\t\t\t */\n\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\tunlock_page(page);\n\t\t\treturn 0;\n\t\t}\n\t\t/* now mark the buffer_heads as dirty and uptodate */\n\t\tblock_commit_write(page, 0, len);\n\t}\n\n\tif (PageChecked(page) && ext4_should_journal_data(inode)) {\n\t\t/*\n\t\t * It's mmapped pagecache.  Add buffers and journal it.  There\n\t\t * doesn't seem much point in redirtying the page here.\n\t\t */\n\t\tClearPageChecked(page);\n\t\treturn __ext4_journalled_writepage(page, len);\n\t}\n\n\tif (test_opt(inode->i_sb, NOBH) && ext4_should_writeback_data(inode))\n\t\tret = nobh_writepage(page, noalloc_get_block_write, wbc);\n\telse\n\t\tret = block_write_full_page(page, noalloc_get_block_write,\n\t\t\t\t\t    wbc);\n\n\treturn ret;\n}\n\n/*\n * This is called via ext4_da_writepages() to\n * calulate the total number of credits to reserve to fit\n * a single extent allocation into a single transaction,\n * ext4_da_writpeages() will loop calling this before\n * the block allocation.\n */\n\nstatic int ext4_da_writepages_trans_blocks(struct inode *inode)\n{\n\tint max_blocks = EXT4_I(inode)->i_reserved_data_blocks;\n\n\t/*\n\t * With non-extent format the journal credit needed to\n\t * insert nrblocks contiguous block is dependent on\n\t * number of contiguous block. So we will limit\n\t * number of contiguous block to a sane value\n\t */\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL) &&\n\t    (max_blocks > EXT4_MAX_TRANS_DATA))\n\t\tmax_blocks = EXT4_MAX_TRANS_DATA;\n\n\treturn ext4_chunk_trans_blocks(inode, max_blocks);\n}\n\nstatic int ext4_da_writepages(struct address_space *mapping,\n\t\t\t      struct writeback_control *wbc)\n{\n\tpgoff_t\tindex;\n\tint range_whole = 0;\n\thandle_t *handle = NULL;\n\tstruct mpage_da_data mpd;\n\tstruct inode *inode = mapping->host;\n\tint no_nrwrite_index_update;\n\tint pages_written = 0;\n\tlong pages_skipped;\n\tunsigned int max_pages;\n\tint range_cyclic, cycled = 1, io_done = 0;\n\tint needed_blocks, ret = 0;\n\tlong desired_nr_to_write, nr_to_writebump = 0;\n\tloff_t range_start = wbc->range_start;\n\tstruct ext4_sb_info *sbi = EXT4_SB(mapping->host->i_sb);\n\n\ttrace_ext4_da_writepages(inode, wbc);\n\n\t/*\n\t * No pages to write? This is mainly a kludge to avoid starting\n\t * a transaction for special inodes like journal inode on last iput()\n\t * because that could violate lock ordering on umount\n\t */\n\tif (!mapping->nrpages || !mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\treturn 0;\n\n\t/*\n\t * If the filesystem has aborted, it is read-only, so return\n\t * right away instead of dumping stack traces later on that\n\t * will obscure the real source of the problem.  We test\n\t * EXT4_MF_FS_ABORTED instead of sb->s_flag's MS_RDONLY because\n\t * the latter could be true if the filesystem is mounted\n\t * read-only, and in that case, ext4_da_writepages should\n\t * *never* be called, so if that ever happens, we would want\n\t * the stack trace.\n\t */\n\tif (unlikely(sbi->s_mount_flags & EXT4_MF_FS_ABORTED))\n\t\treturn -EROFS;\n\n\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\trange_whole = 1;\n\n\trange_cyclic = wbc->range_cyclic;\n\tif (wbc->range_cyclic) {\n\t\tindex = mapping->writeback_index;\n\t\tif (index)\n\t\t\tcycled = 0;\n\t\twbc->range_start = index << PAGE_CACHE_SHIFT;\n\t\twbc->range_end  = LLONG_MAX;\n\t\twbc->range_cyclic = 0;\n\t} else\n\t\tindex = wbc->range_start >> PAGE_CACHE_SHIFT;\n\n\t/*\n\t * This works around two forms of stupidity.  The first is in\n\t * the writeback code, which caps the maximum number of pages\n\t * written to be 1024 pages.  This is wrong on multiple\n\t * levels; different architectues have a different page size,\n\t * which changes the maximum amount of data which gets\n\t * written.  Secondly, 4 megabytes is way too small.  XFS\n\t * forces this value to be 16 megabytes by multiplying\n\t * nr_to_write parameter by four, and then relies on its\n\t * allocator to allocate larger extents to make them\n\t * contiguous.  Unfortunately this brings us to the second\n\t * stupidity, which is that ext4's mballoc code only allocates\n\t * at most 2048 blocks.  So we force contiguous writes up to\n\t * the number of dirty blocks in the inode, or\n\t * sbi->max_writeback_mb_bump whichever is smaller.\n\t */\n\tmax_pages = sbi->s_max_writeback_mb_bump << (20 - PAGE_CACHE_SHIFT);\n\tif (!range_cyclic && range_whole)\n\t\tdesired_nr_to_write = wbc->nr_to_write * 8;\n\telse\n\t\tdesired_nr_to_write = ext4_num_dirty_pages(inode, index,\n\t\t\t\t\t\t\t   max_pages);\n\tif (desired_nr_to_write > max_pages)\n\t\tdesired_nr_to_write = max_pages;\n\n\tif (wbc->nr_to_write < desired_nr_to_write) {\n\t\tnr_to_writebump = desired_nr_to_write - wbc->nr_to_write;\n\t\twbc->nr_to_write = desired_nr_to_write;\n\t}\n\n\tmpd.wbc = wbc;\n\tmpd.inode = mapping->host;\n\n\t/*\n\t * we don't want write_cache_pages to update\n\t * nr_to_write and writeback_index\n\t */\n\tno_nrwrite_index_update = wbc->no_nrwrite_index_update;\n\twbc->no_nrwrite_index_update = 1;\n\tpages_skipped = wbc->pages_skipped;\n\nretry:\n\twhile (!ret && wbc->nr_to_write > 0) {\n\n\t\t/*\n\t\t * we  insert one extent at a time. So we need\n\t\t * credit needed for single extent allocation.\n\t\t * journalled mode is currently not supported\n\t\t * by delalloc\n\t\t */\n\t\tBUG_ON(ext4_should_journal_data(inode));\n\t\tneeded_blocks = ext4_da_writepages_trans_blocks(inode);\n\n\t\t/* start a new transaction*/\n\t\thandle = ext4_journal_start(inode, needed_blocks);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\text4_msg(inode->i_sb, KERN_CRIT, \"%s: jbd2_start: \"\n\t\t\t       \"%ld pages, ino %lu; err %d\\n\", __func__,\n\t\t\t\twbc->nr_to_write, inode->i_ino, ret);\n\t\t\tgoto out_writepages;\n\t\t}\n\n\t\t/*\n\t\t * Now call __mpage_da_writepage to find the next\n\t\t * contiguous region of logical blocks that need\n\t\t * blocks to be allocated by ext4.  We don't actually\n\t\t * submit the blocks for I/O here, even though\n\t\t * write_cache_pages thinks it will, and will set the\n\t\t * pages as clean for write before calling\n\t\t * __mpage_da_writepage().\n\t\t */\n\t\tmpd.b_size = 0;\n\t\tmpd.b_state = 0;\n\t\tmpd.b_blocknr = 0;\n\t\tmpd.first_page = 0;\n\t\tmpd.next_page = 0;\n\t\tmpd.io_done = 0;\n\t\tmpd.pages_written = 0;\n\t\tmpd.retval = 0;\n\t\tret = write_cache_pages(mapping, wbc, __mpage_da_writepage,\n\t\t\t\t\t&mpd);\n\t\t/*\n\t\t * If we have a contiguous extent of pages and we\n\t\t * haven't done the I/O yet, map the blocks and submit\n\t\t * them for I/O.\n\t\t */\n\t\tif (!mpd.io_done && mpd.next_page != mpd.first_page) {\n\t\t\tif (mpage_da_map_blocks(&mpd) == 0)\n\t\t\t\tmpage_da_submit_io(&mpd);\n\t\t\tmpd.io_done = 1;\n\t\t\tret = MPAGE_DA_EXTENT_TAIL;\n\t\t}\n\t\ttrace_ext4_da_write_pages(inode, &mpd);\n\t\twbc->nr_to_write -= mpd.pages_written;\n\n\t\text4_journal_stop(handle);\n\n\t\tif ((mpd.retval == -ENOSPC) && sbi->s_journal) {\n\t\t\t/* commit the transaction which would\n\t\t\t * free blocks released in the transaction\n\t\t\t * and try again\n\t\t\t */\n\t\t\tjbd2_journal_force_commit_nested(sbi->s_journal);\n\t\t\twbc->pages_skipped = pages_skipped;\n\t\t\tret = 0;\n\t\t} else if (ret == MPAGE_DA_EXTENT_TAIL) {\n\t\t\t/*\n\t\t\t * got one extent now try with\n\t\t\t * rest of the pages\n\t\t\t */\n\t\t\tpages_written += mpd.pages_written;\n\t\t\twbc->pages_skipped = pages_skipped;\n\t\t\tret = 0;\n\t\t\tio_done = 1;\n\t\t} else if (wbc->nr_to_write)\n\t\t\t/*\n\t\t\t * There is no more writeout needed\n\t\t\t * or we requested for a noblocking writeout\n\t\t\t * and we found the device congested\n\t\t\t */\n\t\t\tbreak;\n\t}\n\tif (!io_done && !cycled) {\n\t\tcycled = 1;\n\t\tindex = 0;\n\t\twbc->range_start = index << PAGE_CACHE_SHIFT;\n\t\twbc->range_end  = mapping->writeback_index - 1;\n\t\tgoto retry;\n\t}\n\tif (pages_skipped != wbc->pages_skipped)\n\t\text4_msg(inode->i_sb, KERN_CRIT,\n\t\t\t \"This should not happen leaving %s \"\n\t\t\t \"with nr_to_write = %ld ret = %d\\n\",\n\t\t\t __func__, wbc->nr_to_write, ret);\n\n\t/* Update index */\n\tindex += pages_written;\n\twbc->range_cyclic = range_cyclic;\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\t/*\n\t\t * set the writeback_index so that range_cyclic\n\t\t * mode will write it back later\n\t\t */\n\t\tmapping->writeback_index = index;\n\nout_writepages:\n\tif (!no_nrwrite_index_update)\n\t\twbc->no_nrwrite_index_update = 0;\n\twbc->nr_to_write -= nr_to_writebump;\n\twbc->range_start = range_start;\n\ttrace_ext4_da_writepages_result(inode, wbc, ret, pages_written);\n\treturn ret;\n}\n\n#define FALL_BACK_TO_NONDELALLOC 1\nstatic int ext4_nonda_switch(struct super_block *sb)\n{\n\ts64 free_blocks, dirty_blocks;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\t/*\n\t * switch to non delalloc mode if we are running low\n\t * on free block. The free block accounting via percpu\n\t * counters can get slightly wrong with percpu_counter_batch getting\n\t * accumulated on each CPU without updating global counters\n\t * Delalloc need an accurate free block accounting. So switch\n\t * to non delalloc when we are near to error range.\n\t */\n\tfree_blocks  = percpu_counter_read_positive(&sbi->s_freeblocks_counter);\n\tdirty_blocks = percpu_counter_read_positive(&sbi->s_dirtyblocks_counter);\n\tif (2 * free_blocks < 3 * dirty_blocks ||\n\t\tfree_blocks < (dirty_blocks + EXT4_FREEBLOCKS_WATERMARK)) {\n\t\t/*\n\t\t * free block count is less than 150% of dirty blocks\n\t\t * or free blocks is less than watermark\n\t\t */\n\t\treturn 1;\n\t}\n\t/*\n\t * Even if we don't switch but are nearing capacity,\n\t * start pushing delalloc when 1/2 of free blocks are dirty.\n\t */\n\tif (free_blocks < 2 * dirty_blocks)\n\t\twriteback_inodes_sb_if_idle(sb);\n\n\treturn 0;\n}\n\nstatic int ext4_da_write_begin(struct file *file, struct address_space *mapping,\n\t\t\t       loff_t pos, unsigned len, unsigned flags,\n\t\t\t       struct page **pagep, void **fsdata)\n{\n\tint ret, retries = 0, quota_retries = 0;\n\tstruct page *page;\n\tpgoff_t index;\n\tunsigned from, to;\n\tstruct inode *inode = mapping->host;\n\thandle_t *handle;\n\n\tindex = pos >> PAGE_CACHE_SHIFT;\n\tfrom = pos & (PAGE_CACHE_SIZE - 1);\n\tto = from + len;\n\n\tif (ext4_nonda_switch(inode->i_sb)) {\n\t\t*fsdata = (void *)FALL_BACK_TO_NONDELALLOC;\n\t\treturn ext4_write_begin(file, mapping, pos,\n\t\t\t\t\tlen, flags, pagep, fsdata);\n\t}\n\t*fsdata = (void *)0;\n\ttrace_ext4_da_write_begin(inode, pos, len, flags);\nretry:\n\t/*\n\t * With delayed allocation, we don't log the i_disksize update\n\t * if there is delayed block allocation. But we still need\n\t * to journalling the i_disksize update if writes to the end\n\t * of file which has an already mapped buffer.\n\t */\n\thandle = ext4_journal_start(inode, 1);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out;\n\t}\n\t/* We cannot recurse into the filesystem as the transaction is already\n\t * started */\n\tflags |= AOP_FLAG_NOFS;\n\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page) {\n\t\text4_journal_stop(handle);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\t*pagep = page;\n\n\tret = block_write_begin(file, mapping, pos, len, flags, pagep, fsdata,\n\t\t\t\text4_da_get_block_prep);\n\tif (ret < 0) {\n\t\tunlock_page(page);\n\t\text4_journal_stop(handle);\n\t\tpage_cache_release(page);\n\t\t/*\n\t\t * block_write_begin may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again. Don't need\n\t\t * i_size_read because we hold i_mutex.\n\t\t */\n\t\tif (pos + len > inode->i_size)\n\t\t\text4_truncate_failed_write(inode);\n\t}\n\n\tif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry;\n\n\tif ((ret == -EDQUOT) &&\n\t    EXT4_I(inode)->i_reserved_meta_blocks &&\n\t    (quota_retries++ < 3)) {\n\t\t/*\n\t\t * Since we often over-estimate the number of meta\n\t\t * data blocks required, we may sometimes get a\n\t\t * spurios out of quota error even though there would\n\t\t * be enough space once we write the data blocks and\n\t\t * find out how many meta data blocks were _really_\n\t\t * required.  So try forcing the inode write to see if\n\t\t * that helps.\n\t\t */\n\t\twrite_inode_now(inode, (quota_retries == 3));\n\t\tgoto retry;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * Check if we should update i_disksize\n * when write to the end of file but not require block allocation\n */\nstatic int ext4_da_should_update_i_disksize(struct page *page,\n\t\t\t\t\t    unsigned long offset)\n{\n\tstruct buffer_head *bh;\n\tstruct inode *inode = page->mapping->host;\n\tunsigned int idx;\n\tint i;\n\n\tbh = page_buffers(page);\n\tidx = offset >> inode->i_blkbits;\n\n\tfor (i = 0; i < idx; i++)\n\t\tbh = bh->b_this_page;\n\n\tif (!buffer_mapped(bh) || (buffer_delay(bh)) || buffer_unwritten(bh))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int ext4_da_write_end(struct file *file,\n\t\t\t     struct address_space *mapping,\n\t\t\t     loff_t pos, unsigned len, unsigned copied,\n\t\t\t     struct page *page, void *fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tint ret = 0, ret2;\n\thandle_t *handle = ext4_journal_current_handle();\n\tloff_t new_i_size;\n\tunsigned long start, end;\n\tint write_mode = (int)(unsigned long)fsdata;\n\n\tif (write_mode == FALL_BACK_TO_NONDELALLOC) {\n\t\tif (ext4_should_order_data(inode)) {\n\t\t\treturn ext4_ordered_write_end(file, mapping, pos,\n\t\t\t\t\tlen, copied, page, fsdata);\n\t\t} else if (ext4_should_writeback_data(inode)) {\n\t\t\treturn ext4_writeback_write_end(file, mapping, pos,\n\t\t\t\t\tlen, copied, page, fsdata);\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t}\n\n\ttrace_ext4_da_write_end(inode, pos, len, copied);\n\tstart = pos & (PAGE_CACHE_SIZE - 1);\n\tend = start + copied - 1;\n\n\t/*\n\t * generic_write_end() will run mark_inode_dirty() if i_size\n\t * changes.  So let's piggyback the i_disksize mark_inode_dirty\n\t * into that.\n\t */\n\n\tnew_i_size = pos + copied;\n\tif (new_i_size > EXT4_I(inode)->i_disksize) {\n\t\tif (ext4_da_should_update_i_disksize(page, end)) {\n\t\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tif (new_i_size > EXT4_I(inode)->i_disksize) {\n\t\t\t\t/*\n\t\t\t\t * Updating i_disksize when extending file\n\t\t\t\t * without needing block allocation\n\t\t\t\t */\n\t\t\t\tif (ext4_should_order_data(inode))\n\t\t\t\t\tret = ext4_jbd2_file_inode(handle,\n\t\t\t\t\t\t\t\t   inode);\n\n\t\t\t\tEXT4_I(inode)->i_disksize = new_i_size;\n\t\t\t}\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\t/* We need to mark inode dirty even if\n\t\t\t * new_i_size is less that inode->i_size\n\t\t\t * bu greater than i_disksize.(hint delalloc)\n\t\t\t */\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t}\n\t}\n\tret2 = generic_write_end(file, mapping, pos, len, copied,\n\t\t\t\t\t\t\tpage, fsdata);\n\tcopied = ret2;\n\tif (ret2 < 0)\n\t\tret = ret2;\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\treturn ret ? ret : copied;\n}\n\nstatic void ext4_da_invalidatepage(struct page *page, unsigned long offset)\n{\n\t/*\n\t * Drop reserved blocks\n\t */\n\tBUG_ON(!PageLocked(page));\n\tif (!page_has_buffers(page))\n\t\tgoto out;\n\n\text4_da_page_release_reservation(page, offset);\n\nout:\n\text4_invalidatepage(page, offset);\n\n\treturn;\n}\n\n/*\n * Force all delayed allocation blocks to be allocated for a given inode.\n */\nint ext4_alloc_da_blocks(struct inode *inode)\n{\n\ttrace_ext4_alloc_da_blocks(inode);\n\n\tif (!EXT4_I(inode)->i_reserved_data_blocks &&\n\t    !EXT4_I(inode)->i_reserved_meta_blocks)\n\t\treturn 0;\n\n\t/*\n\t * We do something simple for now.  The filemap_flush() will\n\t * also start triggering a write of the data blocks, which is\n\t * not strictly speaking necessary (and for users of\n\t * laptop_mode, not even desirable).  However, to do otherwise\n\t * would require replicating code paths in:\n\t *\n\t * ext4_da_writepages() ->\n\t *    write_cache_pages() ---> (via passed in callback function)\n\t *        __mpage_da_writepage() -->\n\t *           mpage_add_bh_to_extent()\n\t *           mpage_da_map_blocks()\n\t *\n\t * The problem is that write_cache_pages(), located in\n\t * mm/page-writeback.c, marks pages clean in preparation for\n\t * doing I/O, which is not desirable if we're not planning on\n\t * doing I/O at all.\n\t *\n\t * We could call write_cache_pages(), and then redirty all of\n\t * the pages by calling redirty_page_for_writeback() but that\n\t * would be ugly in the extreme.  So instead we would need to\n\t * replicate parts of the code in the above functions,\n\t * simplifying them becuase we wouldn't actually intend to\n\t * write out the pages, but rather only collect contiguous\n\t * logical block extents, call the multi-block allocator, and\n\t * then update the buffer heads with the block allocations.\n\t *\n\t * For now, though, we'll cheat by calling filemap_flush(),\n\t * which will map the blocks, and start the I/O, but not\n\t * actually wait for the I/O to complete.\n\t */\n\treturn filemap_flush(inode->i_mapping);\n}\n\n/*\n * bmap() is special.  It gets used by applications such as lilo and by\n * the swapper to find the on-disk block of a specific piece of data.\n *\n * Naturally, this is dangerous if the block concerned is still in the\n * journal.  If somebody makes a swapfile on an ext4 data-journaling\n * filesystem and enables swap, then they may get a nasty shock when the\n * data getting swapped to that swapfile suddenly gets overwritten by\n * the original zero's written out previously to the journal and\n * awaiting writeback in the kernel's buffer cache.\n *\n * So, if we see any bmap calls here on a modified, data-journaled file,\n * take extra steps to flush any blocks which might be in the cache.\n */\nstatic sector_t ext4_bmap(struct address_space *mapping, sector_t block)\n{\n\tstruct inode *inode = mapping->host;\n\tjournal_t *journal;\n\tint err;\n\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) &&\n\t\t\ttest_opt(inode->i_sb, DELALLOC)) {\n\t\t/*\n\t\t * With delalloc we want to sync the file\n\t\t * so that we can make sure we allocate\n\t\t * blocks for file\n\t\t */\n\t\tfilemap_write_and_wait(mapping);\n\t}\n\n\tif (EXT4_JOURNAL(inode) &&\n\t    ext4_test_inode_state(inode, EXT4_STATE_JDATA)) {\n\t\t/*\n\t\t * This is a REALLY heavyweight approach, but the use of\n\t\t * bmap on dirty files is expected to be extremely rare:\n\t\t * only if we run lilo or swapon on a freshly made file\n\t\t * do we expect this to happen.\n\t\t *\n\t\t * (bmap requires CAP_SYS_RAWIO so this does not\n\t\t * represent an unprivileged user DOS attack --- we'd be\n\t\t * in trouble if mortal users could trigger this path at\n\t\t * will.)\n\t\t *\n\t\t * NB. EXT4_STATE_JDATA is not set on files other than\n\t\t * regular files.  If somebody wants to bmap a directory\n\t\t * or symlink and gets confused because the buffer\n\t\t * hasn't yet been flushed to disk, they deserve\n\t\t * everything they get.\n\t\t */\n\n\t\text4_clear_inode_state(inode, EXT4_STATE_JDATA);\n\t\tjournal = EXT4_JOURNAL(inode);\n\t\tjbd2_journal_lock_updates(journal);\n\t\terr = jbd2_journal_flush(journal);\n\t\tjbd2_journal_unlock_updates(journal);\n\n\t\tif (err)\n\t\t\treturn 0;\n\t}\n\n\treturn generic_block_bmap(mapping, block, ext4_get_block);\n}\n\nstatic int ext4_readpage(struct file *file, struct page *page)\n{\n\treturn mpage_readpage(page, ext4_get_block);\n}\n\nstatic int\next4_readpages(struct file *file, struct address_space *mapping,\n\t\tstruct list_head *pages, unsigned nr_pages)\n{\n\treturn mpage_readpages(mapping, pages, nr_pages, ext4_get_block);\n}\n\nstatic void ext4_invalidatepage(struct page *page, unsigned long offset)\n{\n\tjournal_t *journal = EXT4_JOURNAL(page->mapping->host);\n\n\t/*\n\t * If it's a full truncate we just forget about the pending dirtying\n\t */\n\tif (offset == 0)\n\t\tClearPageChecked(page);\n\n\tif (journal)\n\t\tjbd2_journal_invalidatepage(journal, page, offset);\n\telse\n\t\tblock_invalidatepage(page, offset);\n}\n\nstatic int ext4_releasepage(struct page *page, gfp_t wait)\n{\n\tjournal_t *journal = EXT4_JOURNAL(page->mapping->host);\n\n\tWARN_ON(PageChecked(page));\n\tif (!page_has_buffers(page))\n\t\treturn 0;\n\tif (journal)\n\t\treturn jbd2_journal_try_to_free_buffers(journal, page, wait);\n\telse\n\t\treturn try_to_free_buffers(page);\n}\n\n/*\n * O_DIRECT for ext3 (or indirect map) based files\n *\n * If the O_DIRECT write will extend the file then add this inode to the\n * orphan list.  So recovery will truncate it back to the original size\n * if the machine crashes during the write.\n *\n * If the O_DIRECT write is intantiating holes inside i_size and the machine\n * crashes then stale disk data _may_ be exposed inside the file. But current\n * VFS code falls back into buffered path in that case so we are safe.\n */\nstatic ssize_t ext4_ind_direct_IO(int rw, struct kiocb *iocb,\n\t\t\t      const struct iovec *iov, loff_t offset,\n\t\t\t      unsigned long nr_segs)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\thandle_t *handle;\n\tssize_t ret;\n\tint orphan = 0;\n\tsize_t count = iov_length(iov, nr_segs);\n\tint retries = 0;\n\n\tif (rw == WRITE) {\n\t\tloff_t final_size = offset + count;\n\n\t\tif (final_size > inode->i_size) {\n\t\t\t/* Credits for sb + inode write */\n\t\t\thandle = ext4_journal_start(inode, 2);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\tret = PTR_ERR(handle);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = ext4_orphan_add(handle, inode);\n\t\t\tif (ret) {\n\t\t\t\text4_journal_stop(handle);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\torphan = 1;\n\t\t\tei->i_disksize = inode->i_size;\n\t\t\text4_journal_stop(handle);\n\t\t}\n\t}\n\nretry:\n\tret = blockdev_direct_IO(rw, iocb, inode, inode->i_sb->s_bdev, iov,\n\t\t\t\t offset, nr_segs,\n\t\t\t\t ext4_get_block, NULL);\n\tif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry;\n\n\tif (orphan) {\n\t\tint err;\n\n\t\t/* Credits for sb + inode write */\n\t\thandle = ext4_journal_start(inode, 2);\n\t\tif (IS_ERR(handle)) {\n\t\t\t/* This is really bad luck. We've written the data\n\t\t\t * but cannot extend i_size. Bail out and pretend\n\t\t\t * the write failed... */\n\t\t\tret = PTR_ERR(handle);\n\t\t\tif (inode->i_nlink)\n\t\t\t\text4_orphan_del(NULL, inode);\n\n\t\t\tgoto out;\n\t\t}\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(handle, inode);\n\t\tif (ret > 0) {\n\t\t\tloff_t end = offset + ret;\n\t\t\tif (end > inode->i_size) {\n\t\t\t\tei->i_disksize = end;\n\t\t\t\ti_size_write(inode, end);\n\t\t\t\t/*\n\t\t\t\t * We're going to return a positive `ret'\n\t\t\t\t * here due to non-zero-length I/O, so there's\n\t\t\t\t * no way of reporting error returns from\n\t\t\t\t * ext4_mark_inode_dirty() to userspace.  So\n\t\t\t\t * ignore it.\n\t\t\t\t */\n\t\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\t}\n\t\t}\n\t\terr = ext4_journal_stop(handle);\n\t\tif (ret == 0)\n\t\t\tret = err;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int ext4_get_block_write(struct inode *inode, sector_t iblock,\n\t\t   struct buffer_head *bh_result, int create)\n{\n\thandle_t *handle = NULL;\n\tint ret = 0;\n\tunsigned max_blocks = bh_result->b_size >> inode->i_blkbits;\n\tint dio_credits;\n\n\text4_debug(\"ext4_get_block_write: inode %lu, create flag %d\\n\",\n\t\t   inode->i_ino, create);\n\t/*\n\t * ext4_get_block in prepare for a DIO write or buffer write.\n\t * We allocate an uinitialized extent if blocks haven't been allocated.\n\t * The extent will be converted to initialized after IO complete.\n\t */\n\tcreate = EXT4_GET_BLOCKS_IO_CREATE_EXT;\n\n\tif (max_blocks > DIO_MAX_BLOCKS)\n\t\tmax_blocks = DIO_MAX_BLOCKS;\n\tdio_credits = ext4_chunk_trans_blocks(inode, max_blocks);\n\thandle = ext4_journal_start(inode, dio_credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out;\n\t}\n\tret = ext4_get_blocks(handle, inode, iblock, max_blocks, bh_result,\n\t\t\t      create);\n\tif (ret > 0) {\n\t\tbh_result->b_size = (ret << inode->i_blkbits);\n\t\tret = 0;\n\t}\n\text4_journal_stop(handle);\nout:\n\treturn ret;\n}\n\nstatic void ext4_free_io_end(ext4_io_end_t *io)\n{\n\tBUG_ON(!io);\n\tiput(io->inode);\n\tkfree(io);\n}\n\nstatic void dump_completed_IO(struct inode * inode)\n{\n#ifdef\tEXT4_DEBUG\n\tstruct list_head *cur, *before, *after;\n\text4_io_end_t *io, *io0, *io1;\n\n\tif (list_empty(&EXT4_I(inode)->i_completed_io_list)){\n\t\text4_debug(\"inode %lu completed_io list is empty\\n\", inode->i_ino);\n\t\treturn;\n\t}\n\n\text4_debug(\"Dump inode %lu completed_io list \\n\", inode->i_ino);\n\tlist_for_each_entry(io, &EXT4_I(inode)->i_completed_io_list, list){\n\t\tcur = &io->list;\n\t\tbefore = cur->prev;\n\t\tio0 = container_of(before, ext4_io_end_t, list);\n\t\tafter = cur->next;\n\t\tio1 = container_of(after, ext4_io_end_t, list);\n\n\t\text4_debug(\"io 0x%p from inode %lu,prev 0x%p,next 0x%p\\n\",\n\t\t\t    io, inode->i_ino, io0, io1);\n\t}\n#endif\n}\n\n/*\n * check a range of space and convert unwritten extents to written.\n */\nstatic int ext4_end_io_nolock(ext4_io_end_t *io)\n{\n\tstruct inode *inode = io->inode;\n\tloff_t offset = io->offset;\n\tssize_t size = io->size;\n\tint ret = 0;\n\n\text4_debug(\"ext4_end_io_nolock: io 0x%p from inode %lu,list->next 0x%p,\"\n\t\t   \"list->prev 0x%p\\n\",\n\t           io, inode->i_ino, io->list.next, io->list.prev);\n\n\tif (list_empty(&io->list))\n\t\treturn ret;\n\n\tif (io->flag != EXT4_IO_UNWRITTEN)\n\t\treturn ret;\n\n\tif (offset + size <= i_size_read(inode))\n\t\tret = ext4_convert_unwritten_extents(inode, offset, size);\n\n\tif (ret < 0) {\n\t\tprintk(KERN_EMERG \"%s: failed to convert unwritten\"\n\t\t\t\"extents to written extents, error is %d\"\n\t\t\t\" io is still on inode %lu aio dio list\\n\",\n                       __func__, ret, inode->i_ino);\n\t\treturn ret;\n\t}\n\n\t/* clear the DIO AIO unwritten flag */\n\tio->flag = 0;\n\treturn ret;\n}\n\n/*\n * work on completed aio dio IO, to convert unwritten extents to extents\n */\nstatic void ext4_end_io_work(struct work_struct *work)\n{\n\text4_io_end_t *io  = container_of(work, ext4_io_end_t, work);\n\tstruct inode *inode = io->inode;\n\tint ret = 0;\n\n\tmutex_lock(&inode->i_mutex);\n\tret = ext4_end_io_nolock(io);\n\tif (ret >= 0) {\n\t\tif (!list_empty(&io->list))\n\t\t\tlist_del_init(&io->list);\n\t\text4_free_io_end(io);\n\t}\n\tmutex_unlock(&inode->i_mutex);\n}\n\n/*\n * This function is called from ext4_sync_file().\n *\n * When IO is completed, the work to convert unwritten extents to\n * written is queued on workqueue but may not get immediately\n * scheduled. When fsync is called, we need to ensure the\n * conversion is complete before fsync returns.\n * The inode keeps track of a list of pending/completed IO that\n * might needs to do the conversion. This function walks through\n * the list and convert the related unwritten extents for completed IO\n * to written.\n * The function return the number of pending IOs on success.\n */\nint flush_completed_IO(struct inode *inode)\n{\n\text4_io_end_t *io;\n\tint ret = 0;\n\tint ret2 = 0;\n\n\tif (list_empty(&EXT4_I(inode)->i_completed_io_list))\n\t\treturn ret;\n\n\tdump_completed_IO(inode);\n\twhile (!list_empty(&EXT4_I(inode)->i_completed_io_list)){\n\t\tio = list_entry(EXT4_I(inode)->i_completed_io_list.next,\n\t\t\t\text4_io_end_t, list);\n\t\t/*\n\t\t * Calling ext4_end_io_nolock() to convert completed\n\t\t * IO to written.\n\t\t *\n\t\t * When ext4_sync_file() is called, run_queue() may already\n\t\t * about to flush the work corresponding to this io structure.\n\t\t * It will be upset if it founds the io structure related\n\t\t * to the work-to-be schedule is freed.\n\t\t *\n\t\t * Thus we need to keep the io structure still valid here after\n\t\t * convertion finished. The io structure has a flag to\n\t\t * avoid double converting from both fsync and background work\n\t\t * queue work.\n\t\t */\n\t\tret = ext4_end_io_nolock(io);\n\t\tif (ret < 0)\n\t\t\tret2 = ret;\n\t\telse\n\t\t\tlist_del_init(&io->list);\n\t}\n\treturn (ret2 < 0) ? ret2 : 0;\n}\n\nstatic ext4_io_end_t *ext4_init_io_end (struct inode *inode)\n{\n\text4_io_end_t *io = NULL;\n\n\tio = kmalloc(sizeof(*io), GFP_NOFS);\n\n\tif (io) {\n\t\tigrab(inode);\n\t\tio->inode = inode;\n\t\tio->flag = 0;\n\t\tio->offset = 0;\n\t\tio->size = 0;\n\t\tio->error = 0;\n\t\tINIT_WORK(&io->work, ext4_end_io_work);\n\t\tINIT_LIST_HEAD(&io->list);\n\t}\n\n\treturn io;\n}\n\nstatic void ext4_end_io_dio(struct kiocb *iocb, loff_t offset,\n\t\t\t    ssize_t size, void *private)\n{\n        ext4_io_end_t *io_end = iocb->private;\n\tstruct workqueue_struct *wq;\n\n\t/* if not async direct IO or dio with 0 bytes write, just return */\n\tif (!io_end || !size)\n\t\treturn;\n\n\text_debug(\"ext4_end_io_dio(): io_end 0x%p\"\n\t\t  \"for inode %lu, iocb 0x%p, offset %llu, size %llu\\n\",\n \t\t  iocb->private, io_end->inode->i_ino, iocb, offset,\n\t\t  size);\n\n\t/* if not aio dio with unwritten extents, just free io and return */\n\tif (io_end->flag != EXT4_IO_UNWRITTEN){\n\t\text4_free_io_end(io_end);\n\t\tiocb->private = NULL;\n\t\treturn;\n\t}\n\n\tio_end->offset = offset;\n\tio_end->size = size;\n\twq = EXT4_SB(io_end->inode->i_sb)->dio_unwritten_wq;\n\n\t/* queue the work to convert unwritten extents to written */\n\tqueue_work(wq, &io_end->work);\n\n\t/* Add the io_end to per-inode completed aio dio list*/\n\tlist_add_tail(&io_end->list,\n\t\t &EXT4_I(io_end->inode)->i_completed_io_list);\n\tiocb->private = NULL;\n}\n\n/*\n * For ext4 extent files, ext4 will do direct-io write to holes,\n * preallocated extents, and those write extend the file, no need to\n * fall back to buffered IO.\n *\n * For holes, we fallocate those blocks, mark them as unintialized\n * If those blocks were preallocated, we mark sure they are splited, but\n * still keep the range to write as unintialized.\n *\n * The unwrritten extents will be converted to written when DIO is completed.\n * For async direct IO, since the IO may still pending when return, we\n * set up an end_io call back function, which will do the convertion\n * when async direct IO completed.\n *\n * If the O_DIRECT write will extend the file then add this inode to the\n * orphan list.  So recovery will truncate it back to the original size\n * if the machine crashes during the write.\n *\n */\nstatic ssize_t ext4_ext_direct_IO(int rw, struct kiocb *iocb,\n\t\t\t      const struct iovec *iov, loff_t offset,\n\t\t\t      unsigned long nr_segs)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\tssize_t ret;\n\tsize_t count = iov_length(iov, nr_segs);\n\n\tloff_t final_size = offset + count;\n\tif (rw == WRITE && final_size <= inode->i_size) {\n\t\t/*\n \t\t * We could direct write to holes and fallocate.\n\t\t *\n \t\t * Allocated blocks to fill the hole are marked as uninitialized\n \t\t * to prevent paralel buffered read to expose the stale data\n \t\t * before DIO complete the data IO.\n\t\t *\n \t\t * As to previously fallocated extents, ext4 get_block\n \t\t * will just simply mark the buffer mapped but still\n \t\t * keep the extents uninitialized.\n \t\t *\n\t\t * for non AIO case, we will convert those unwritten extents\n\t\t * to written after return back from blockdev_direct_IO.\n\t\t *\n\t\t * for async DIO, the conversion needs to be defered when\n\t\t * the IO is completed. The ext4 end_io callback function\n\t\t * will be called to take care of the conversion work.\n\t\t * Here for async case, we allocate an io_end structure to\n\t\t * hook to the iocb.\n \t\t */\n\t\tiocb->private = NULL;\n\t\tEXT4_I(inode)->cur_aio_dio = NULL;\n\t\tif (!is_sync_kiocb(iocb)) {\n\t\t\tiocb->private = ext4_init_io_end(inode);\n\t\t\tif (!iocb->private)\n\t\t\t\treturn -ENOMEM;\n\t\t\t/*\n\t\t\t * we save the io structure for current async\n\t\t\t * direct IO, so that later ext4_get_blocks()\n\t\t\t * could flag the io structure whether there\n\t\t\t * is a unwritten extents needs to be converted\n\t\t\t * when IO is completed.\n\t\t\t */\n\t\t\tEXT4_I(inode)->cur_aio_dio = iocb->private;\n\t\t}\n\n\t\tret = blockdev_direct_IO(rw, iocb, inode,\n\t\t\t\t\t inode->i_sb->s_bdev, iov,\n\t\t\t\t\t offset, nr_segs,\n\t\t\t\t\t ext4_get_block_write,\n\t\t\t\t\t ext4_end_io_dio);\n\t\tif (iocb->private)\n\t\t\tEXT4_I(inode)->cur_aio_dio = NULL;\n\t\t/*\n\t\t * The io_end structure takes a reference to the inode,\n\t\t * that structure needs to be destroyed and the\n\t\t * reference to the inode need to be dropped, when IO is\n\t\t * complete, even with 0 byte write, or failed.\n\t\t *\n\t\t * In the successful AIO DIO case, the io_end structure will be\n\t\t * desctroyed and the reference to the inode will be dropped\n\t\t * after the end_io call back function is called.\n\t\t *\n\t\t * In the case there is 0 byte write, or error case, since\n\t\t * VFS direct IO won't invoke the end_io call back function,\n\t\t * we need to free the end_io structure here.\n\t\t */\n\t\tif (ret != -EIOCBQUEUED && ret <= 0 && iocb->private) {\n\t\t\text4_free_io_end(iocb->private);\n\t\t\tiocb->private = NULL;\n\t\t} else if (ret > 0 && ext4_test_inode_state(inode,\n\t\t\t\t\t\tEXT4_STATE_DIO_UNWRITTEN)) {\n\t\t\tint err;\n\t\t\t/*\n\t\t\t * for non AIO case, since the IO is already\n\t\t\t * completed, we could do the convertion right here\n\t\t\t */\n\t\t\terr = ext4_convert_unwritten_extents(inode,\n\t\t\t\t\t\t\t     offset, ret);\n\t\t\tif (err < 0)\n\t\t\t\tret = err;\n\t\t\text4_clear_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\t}\n\t\treturn ret;\n\t}\n\n\t/* for write the the end of file case, we fall back to old way */\n\treturn ext4_ind_direct_IO(rw, iocb, iov, offset, nr_segs);\n}\n\nstatic ssize_t ext4_direct_IO(int rw, struct kiocb *iocb,\n\t\t\t      const struct iovec *iov, loff_t offset,\n\t\t\t      unsigned long nr_segs)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\n\tif (EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL)\n\t\treturn ext4_ext_direct_IO(rw, iocb, iov, offset, nr_segs);\n\n\treturn ext4_ind_direct_IO(rw, iocb, iov, offset, nr_segs);\n}\n\n/*\n * Pages can be marked dirty completely asynchronously from ext4's journalling\n * activity.  By filemap_sync_pte(), try_to_unmap_one(), etc.  We cannot do\n * much here because ->set_page_dirty is called under VFS locks.  The page is\n * not necessarily locked.\n *\n * We cannot just dirty the page and leave attached buffers clean, because the\n * buffers' dirty state is \"definitive\".  We cannot just set the buffers dirty\n * or jbddirty because all the journalling code will explode.\n *\n * So what we do is to mark the page \"pending dirty\" and next time writepage\n * is called, propagate that into the buffers appropriately.\n */\nstatic int ext4_journalled_set_page_dirty(struct page *page)\n{\n\tSetPageChecked(page);\n\treturn __set_page_dirty_nobuffers(page);\n}\n\nstatic const struct address_space_operations ext4_ordered_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.sync_page\t\t= block_sync_page,\n\t.write_begin\t\t= ext4_write_begin,\n\t.write_end\t\t= ext4_ordered_write_end,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= ext4_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_writeback_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.sync_page\t\t= block_sync_page,\n\t.write_begin\t\t= ext4_write_begin,\n\t.write_end\t\t= ext4_writeback_write_end,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= ext4_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_journalled_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.sync_page\t\t= block_sync_page,\n\t.write_begin\t\t= ext4_write_begin,\n\t.write_end\t\t= ext4_journalled_write_end,\n\t.set_page_dirty\t\t= ext4_journalled_set_page_dirty,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_da_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.writepages\t\t= ext4_da_writepages,\n\t.sync_page\t\t= block_sync_page,\n\t.write_begin\t\t= ext4_da_write_begin,\n\t.write_end\t\t= ext4_da_write_end,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_da_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= ext4_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nvoid ext4_set_aops(struct inode *inode)\n{\n\tif (ext4_should_order_data(inode) &&\n\t\ttest_opt(inode->i_sb, DELALLOC))\n\t\tinode->i_mapping->a_ops = &ext4_da_aops;\n\telse if (ext4_should_order_data(inode))\n\t\tinode->i_mapping->a_ops = &ext4_ordered_aops;\n\telse if (ext4_should_writeback_data(inode) &&\n\t\t test_opt(inode->i_sb, DELALLOC))\n\t\tinode->i_mapping->a_ops = &ext4_da_aops;\n\telse if (ext4_should_writeback_data(inode))\n\t\tinode->i_mapping->a_ops = &ext4_writeback_aops;\n\telse\n\t\tinode->i_mapping->a_ops = &ext4_journalled_aops;\n}\n\n/*\n * ext4_block_truncate_page() zeroes out a mapping from file offset `from'\n * up to the end of the block which corresponds to `from'.\n * This required during truncate. We need to physically zero the tail end\n * of that block so it doesn't yield old data if the file is later grown.\n */\nint ext4_block_truncate_page(handle_t *handle,\n\t\tstruct address_space *mapping, loff_t from)\n{\n\text4_fsblk_t index = from >> PAGE_CACHE_SHIFT;\n\tunsigned offset = from & (PAGE_CACHE_SIZE-1);\n\tunsigned blocksize, length, pos;\n\text4_lblk_t iblock;\n\tstruct inode *inode = mapping->host;\n\tstruct buffer_head *bh;\n\tstruct page *page;\n\tint err = 0;\n\n\tpage = find_or_create_page(mapping, from >> PAGE_CACHE_SHIFT,\n\t\t\t\t   mapping_gfp_mask(mapping) & ~__GFP_FS);\n\tif (!page)\n\t\treturn -EINVAL;\n\n\tblocksize = inode->i_sb->s_blocksize;\n\tlength = blocksize - (offset & (blocksize - 1));\n\tiblock = index << (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits);\n\n\t/*\n\t * For \"nobh\" option,  we can only work if we don't need to\n\t * read-in the page - otherwise we create buffers to do the IO.\n\t */\n\tif (!page_has_buffers(page) && test_opt(inode->i_sb, NOBH) &&\n\t     ext4_should_writeback_data(inode) && PageUptodate(page)) {\n\t\tzero_user(page, offset, length);\n\t\tset_page_dirty(page);\n\t\tgoto unlock;\n\t}\n\n\tif (!page_has_buffers(page))\n\t\tcreate_empty_buffers(page, blocksize, 0);\n\n\t/* Find the buffer that contains \"offset\" */\n\tbh = page_buffers(page);\n\tpos = blocksize;\n\twhile (offset >= pos) {\n\t\tbh = bh->b_this_page;\n\t\tiblock++;\n\t\tpos += blocksize;\n\t}\n\n\terr = 0;\n\tif (buffer_freed(bh)) {\n\t\tBUFFER_TRACE(bh, \"freed: skip\");\n\t\tgoto unlock;\n\t}\n\n\tif (!buffer_mapped(bh)) {\n\t\tBUFFER_TRACE(bh, \"unmapped\");\n\t\text4_get_block(inode, iblock, bh, 0);\n\t\t/* unmapped? It's a hole - nothing to do */\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tBUFFER_TRACE(bh, \"still unmapped\");\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\t/* Ok, it's mapped. Make sure it's up-to-date */\n\tif (PageUptodate(page))\n\t\tset_buffer_uptodate(bh);\n\n\tif (!buffer_uptodate(bh)) {\n\t\terr = -EIO;\n\t\tll_rw_block(READ, 1, &bh);\n\t\twait_on_buffer(bh);\n\t\t/* Uhhuh. Read error. Complain and punt. */\n\t\tif (!buffer_uptodate(bh))\n\t\t\tgoto unlock;\n\t}\n\n\tif (ext4_should_journal_data(inode)) {\n\t\tBUFFER_TRACE(bh, \"get write access\");\n\t\terr = ext4_journal_get_write_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t}\n\n\tzero_user(page, offset, length);\n\n\tBUFFER_TRACE(bh, \"zeroed end of block\");\n\n\terr = 0;\n\tif (ext4_should_journal_data(inode)) {\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t} else {\n\t\tif (ext4_should_order_data(inode))\n\t\t\terr = ext4_jbd2_file_inode(handle, inode);\n\t\tmark_buffer_dirty(bh);\n\t}\n\nunlock:\n\tunlock_page(page);\n\tpage_cache_release(page);\n\treturn err;\n}\n\n/*\n * Probably it should be a library function... search for first non-zero word\n * or memcmp with zero_page, whatever is better for particular architecture.\n * Linus?\n */\nstatic inline int all_zeroes(__le32 *p, __le32 *q)\n{\n\twhile (p < q)\n\t\tif (*p++)\n\t\t\treturn 0;\n\treturn 1;\n}\n\n/**\n *\text4_find_shared - find the indirect blocks for partial truncation.\n *\t@inode:\t  inode in question\n *\t@depth:\t  depth of the affected branch\n *\t@offsets: offsets of pointers in that branch (see ext4_block_to_path)\n *\t@chain:\t  place to store the pointers to partial indirect blocks\n *\t@top:\t  place to the (detached) top of branch\n *\n *\tThis is a helper function used by ext4_truncate().\n *\n *\tWhen we do truncate() we may have to clean the ends of several\n *\tindirect blocks but leave the blocks themselves alive. Block is\n *\tpartially truncated if some data below the new i_size is refered\n *\tfrom it (and it is on the path to the first completely truncated\n *\tdata block, indeed).  We have to free the top of that path along\n *\twith everything to the right of the path. Since no allocation\n *\tpast the truncation point is possible until ext4_truncate()\n *\tfinishes, we may safely do the latter, but top of branch may\n *\trequire special attention - pageout below the truncation point\n *\tmight try to populate it.\n *\n *\tWe atomically detach the top of branch from the tree, store the\n *\tblock number of its root in *@top, pointers to buffer_heads of\n *\tpartially truncated blocks - in @chain[].bh and pointers to\n *\ttheir last elements that should not be removed - in\n *\t@chain[].p. Return value is the pointer to last filled element\n *\tof @chain.\n *\n *\tThe work left to caller to do the actual freeing of subtrees:\n *\t\ta) free the subtree starting from *@top\n *\t\tb) free the subtrees whose roots are stored in\n *\t\t\t(@chain[i].p+1 .. end of @chain[i].bh->b_data)\n *\t\tc) free the subtrees growing from the inode past the @chain[0].\n *\t\t\t(no partially truncated stuff there).  */\n\nstatic Indirect *ext4_find_shared(struct inode *inode, int depth,\n\t\t\t\t  ext4_lblk_t offsets[4], Indirect chain[4],\n\t\t\t\t  __le32 *top)\n{\n\tIndirect *partial, *p;\n\tint k, err;\n\n\t*top = 0;\n\t/* Make k index the deepest non-null offset + 1 */\n\tfor (k = depth; k > 1 && !offsets[k-1]; k--)\n\t\t;\n\tpartial = ext4_get_branch(inode, k, offsets, chain, &err);\n\t/* Writer: pointers */\n\tif (!partial)\n\t\tpartial = chain + k-1;\n\t/*\n\t * If the branch acquired continuation since we've looked at it -\n\t * fine, it should all survive and (new) top doesn't belong to us.\n\t */\n\tif (!partial->key && *partial->p)\n\t\t/* Writer: end */\n\t\tgoto no_top;\n\tfor (p = partial; (p > chain) && all_zeroes((__le32 *) p->bh->b_data, p->p); p--)\n\t\t;\n\t/*\n\t * OK, we've found the last block that must survive. The rest of our\n\t * branch should be detached before unlocking. However, if that rest\n\t * of branch is all ours and does not grow immediately from the inode\n\t * it's easier to cheat and just decrement partial->p.\n\t */\n\tif (p == chain + k - 1 && p > chain) {\n\t\tp->p--;\n\t} else {\n\t\t*top = *p->p;\n\t\t/* Nope, don't do this in ext4.  Must leave the tree intact */\n#if 0\n\t\t*p->p = 0;\n#endif\n\t}\n\t/* Writer: end */\n\n\twhile (partial > p) {\n\t\tbrelse(partial->bh);\n\t\tpartial--;\n\t}\nno_top:\n\treturn partial;\n}\n\n/*\n * Zero a number of block pointers in either an inode or an indirect block.\n * If we restart the transaction we must again get write access to the\n * indirect block for further modification.\n *\n * We release `count' blocks on disk, but (last - first) may be greater\n * than `count' because there can be holes in there.\n */\nstatic int ext4_clear_blocks(handle_t *handle, struct inode *inode,\n\t\t\t     struct buffer_head *bh,\n\t\t\t     ext4_fsblk_t block_to_free,\n\t\t\t     unsigned long count, __le32 *first,\n\t\t\t     __le32 *last)\n{\n\t__le32 *p;\n\tint\tflags = EXT4_FREE_BLOCKS_FORGET | EXT4_FREE_BLOCKS_VALIDATED;\n\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\tflags |= EXT4_FREE_BLOCKS_METADATA;\n\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), block_to_free,\n\t\t\t\t   count)) {\n\t\text4_error(inode->i_sb, \"inode #%lu: \"\n\t\t\t   \"attempt to clear blocks %llu len %lu, invalid\",\n\t\t\t   inode->i_ino, (unsigned long long) block_to_free,\n\t\t\t   count);\n\t\treturn 1;\n\t}\n\n\tif (try_to_extend_transaction(handle, inode)) {\n\t\tif (bh) {\n\t\t\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\t\t\text4_handle_dirty_metadata(handle, inode, bh);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\text4_truncate_restart_trans(handle, inode,\n\t\t\t\t\t    blocks_for_truncate(inode));\n\t\tif (bh) {\n\t\t\tBUFFER_TRACE(bh, \"retaking write access\");\n\t\t\text4_journal_get_write_access(handle, bh);\n\t\t}\n\t}\n\n\tfor (p = first; p < last; p++)\n\t\t*p = 0;\n\n\text4_free_blocks(handle, inode, 0, block_to_free, count, flags);\n\treturn 0;\n}\n\n/**\n * ext4_free_data - free a list of data blocks\n * @handle:\thandle for this transaction\n * @inode:\tinode we are dealing with\n * @this_bh:\tindirect buffer_head which contains *@first and *@last\n * @first:\tarray of block numbers\n * @last:\tpoints immediately past the end of array\n *\n * We are freeing all blocks refered from that array (numbers are stored as\n * little-endian 32-bit) and updating @inode->i_blocks appropriately.\n *\n * We accumulate contiguous runs of blocks to free.  Conveniently, if these\n * blocks are contiguous then releasing them at one time will only affect one\n * or two bitmap blocks (+ group descriptor(s) and superblock) and we won't\n * actually use a lot of journal space.\n *\n * @this_bh will be %NULL if @first and @last point into the inode's direct\n * block pointers.\n */\nstatic void ext4_free_data(handle_t *handle, struct inode *inode,\n\t\t\t   struct buffer_head *this_bh,\n\t\t\t   __le32 *first, __le32 *last)\n{\n\text4_fsblk_t block_to_free = 0;    /* Starting block # of a run */\n\tunsigned long count = 0;\t    /* Number of blocks in the run */\n\t__le32 *block_to_free_p = NULL;\t    /* Pointer into inode/ind\n\t\t\t\t\t       corresponding to\n\t\t\t\t\t       block_to_free */\n\text4_fsblk_t nr;\t\t    /* Current block # */\n\t__le32 *p;\t\t\t    /* Pointer into inode/ind\n\t\t\t\t\t       for current block */\n\tint err;\n\n\tif (this_bh) {\t\t\t\t/* For indirect block */\n\t\tBUFFER_TRACE(this_bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, this_bh);\n\t\t/* Important: if we can't update the indirect pointers\n\t\t * to the blocks, we can't free them. */\n\t\tif (err)\n\t\t\treturn;\n\t}\n\n\tfor (p = first; p < last; p++) {\n\t\tnr = le32_to_cpu(*p);\n\t\tif (nr) {\n\t\t\t/* accumulate blocks to free if they're contiguous */\n\t\t\tif (count == 0) {\n\t\t\t\tblock_to_free = nr;\n\t\t\t\tblock_to_free_p = p;\n\t\t\t\tcount = 1;\n\t\t\t} else if (nr == block_to_free + count) {\n\t\t\t\tcount++;\n\t\t\t} else {\n\t\t\t\tif (ext4_clear_blocks(handle, inode, this_bh,\n\t\t\t\t\t\t      block_to_free, count,\n\t\t\t\t\t\t      block_to_free_p, p))\n\t\t\t\t\tbreak;\n\t\t\t\tblock_to_free = nr;\n\t\t\t\tblock_to_free_p = p;\n\t\t\t\tcount = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (count > 0)\n\t\text4_clear_blocks(handle, inode, this_bh, block_to_free,\n\t\t\t\t  count, block_to_free_p, p);\n\n\tif (this_bh) {\n\t\tBUFFER_TRACE(this_bh, \"call ext4_handle_dirty_metadata\");\n\n\t\t/*\n\t\t * The buffer head should have an attached journal head at this\n\t\t * point. However, if the data is corrupted and an indirect\n\t\t * block pointed to itself, it would have been detached when\n\t\t * the block was cleared. Check for this instead of OOPSing.\n\t\t */\n\t\tif ((EXT4_JOURNAL(inode) == NULL) || bh2jh(this_bh))\n\t\t\text4_handle_dirty_metadata(handle, inode, this_bh);\n\t\telse\n\t\t\text4_error(inode->i_sb,\n\t\t\t\t   \"circular indirect block detected, \"\n\t\t\t\t   \"inode=%lu, block=%llu\",\n\t\t\t\t   inode->i_ino,\n\t\t\t\t   (unsigned long long) this_bh->b_blocknr);\n\t}\n}\n\n/**\n *\text4_free_branches - free an array of branches\n *\t@handle: JBD handle for this transaction\n *\t@inode:\tinode we are dealing with\n *\t@parent_bh: the buffer_head which contains *@first and *@last\n *\t@first:\tarray of block numbers\n *\t@last:\tpointer immediately past the end of array\n *\t@depth:\tdepth of the branches to free\n *\n *\tWe are freeing all blocks refered from these branches (numbers are\n *\tstored as little-endian 32-bit) and updating @inode->i_blocks\n *\tappropriately.\n */\nstatic void ext4_free_branches(handle_t *handle, struct inode *inode,\n\t\t\t       struct buffer_head *parent_bh,\n\t\t\t       __le32 *first, __le32 *last, int depth)\n{\n\text4_fsblk_t nr;\n\t__le32 *p;\n\n\tif (ext4_handle_is_aborted(handle))\n\t\treturn;\n\n\tif (depth--) {\n\t\tstruct buffer_head *bh;\n\t\tint addr_per_block = EXT4_ADDR_PER_BLOCK(inode->i_sb);\n\t\tp = last;\n\t\twhile (--p >= first) {\n\t\t\tnr = le32_to_cpu(*p);\n\t\t\tif (!nr)\n\t\t\t\tcontinue;\t\t/* A hole */\n\n\t\t\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb),\n\t\t\t\t\t\t   nr, 1)) {\n\t\t\t\text4_error(inode->i_sb,\n\t\t\t\t\t   \"indirect mapped block in inode \"\n\t\t\t\t\t   \"#%lu invalid (level %d, blk #%lu)\",\n\t\t\t\t\t   inode->i_ino, depth,\n\t\t\t\t\t   (unsigned long) nr);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Go read the buffer for the next level down */\n\t\t\tbh = sb_bread(inode->i_sb, nr);\n\n\t\t\t/*\n\t\t\t * A read failure? Report error and clear slot\n\t\t\t * (should be rare).\n\t\t\t */\n\t\t\tif (!bh) {\n\t\t\t\text4_error(inode->i_sb,\n\t\t\t\t\t   \"Read failure, inode=%lu, block=%llu\",\n\t\t\t\t\t   inode->i_ino, nr);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* This zaps the entire block.  Bottom up. */\n\t\t\tBUFFER_TRACE(bh, \"free child branches\");\n\t\t\text4_free_branches(handle, inode, bh,\n\t\t\t\t\t(__le32 *) bh->b_data,\n\t\t\t\t\t(__le32 *) bh->b_data + addr_per_block,\n\t\t\t\t\tdepth);\n\n\t\t\t/*\n\t\t\t * We've probably journalled the indirect block several\n\t\t\t * times during the truncate.  But it's no longer\n\t\t\t * needed and we now drop it from the transaction via\n\t\t\t * jbd2_journal_revoke().\n\t\t\t *\n\t\t\t * That's easy if it's exclusively part of this\n\t\t\t * transaction.  But if it's part of the committing\n\t\t\t * transaction then jbd2_journal_forget() will simply\n\t\t\t * brelse() it.  That means that if the underlying\n\t\t\t * block is reallocated in ext4_get_block(),\n\t\t\t * unmap_underlying_metadata() will find this block\n\t\t\t * and will try to get rid of it.  damn, damn.\n\t\t\t *\n\t\t\t * If this block has already been committed to the\n\t\t\t * journal, a revoke record will be written.  And\n\t\t\t * revoke records must be emitted *before* clearing\n\t\t\t * this block's bit in the bitmaps.\n\t\t\t */\n\t\t\text4_forget(handle, 1, inode, bh, bh->b_blocknr);\n\n\t\t\t/*\n\t\t\t * Everything below this this pointer has been\n\t\t\t * released.  Now let this top-of-subtree go.\n\t\t\t *\n\t\t\t * We want the freeing of this indirect block to be\n\t\t\t * atomic in the journal with the updating of the\n\t\t\t * bitmap block which owns it.  So make some room in\n\t\t\t * the journal.\n\t\t\t *\n\t\t\t * We zero the parent pointer *after* freeing its\n\t\t\t * pointee in the bitmaps, so if extend_transaction()\n\t\t\t * for some reason fails to put the bitmap changes and\n\t\t\t * the release into the same transaction, recovery\n\t\t\t * will merely complain about releasing a free block,\n\t\t\t * rather than leaking blocks.\n\t\t\t */\n\t\t\tif (ext4_handle_is_aborted(handle))\n\t\t\t\treturn;\n\t\t\tif (try_to_extend_transaction(handle, inode)) {\n\t\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\t\text4_truncate_restart_trans(handle, inode,\n\t\t\t\t\t    blocks_for_truncate(inode));\n\t\t\t}\n\n\t\t\text4_free_blocks(handle, inode, 0, nr, 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\n\t\t\tif (parent_bh) {\n\t\t\t\t/*\n\t\t\t\t * The block which we have just freed is\n\t\t\t\t * pointed to by an indirect block: journal it\n\t\t\t\t */\n\t\t\t\tBUFFER_TRACE(parent_bh, \"get_write_access\");\n\t\t\t\tif (!ext4_journal_get_write_access(handle,\n\t\t\t\t\t\t\t\t   parent_bh)){\n\t\t\t\t\t*p = 0;\n\t\t\t\t\tBUFFER_TRACE(parent_bh,\n\t\t\t\t\t\"call ext4_handle_dirty_metadata\");\n\t\t\t\t\text4_handle_dirty_metadata(handle,\n\t\t\t\t\t\t\t\t   inode,\n\t\t\t\t\t\t\t\t   parent_bh);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\t/* We have reached the bottom of the tree. */\n\t\tBUFFER_TRACE(parent_bh, \"free data blocks\");\n\t\text4_free_data(handle, inode, parent_bh, first, last);\n\t}\n}\n\nint ext4_can_truncate(struct inode *inode)\n{\n\tif (IS_APPEND(inode) || IS_IMMUTABLE(inode))\n\t\treturn 0;\n\tif (S_ISREG(inode->i_mode))\n\t\treturn 1;\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn 1;\n\tif (S_ISLNK(inode->i_mode))\n\t\treturn !ext4_inode_is_fast_symlink(inode);\n\treturn 0;\n}\n\n/*\n * ext4_truncate()\n *\n * We block out ext4_get_block() block instantiations across the entire\n * transaction, and VFS/VM ensures that ext4_truncate() cannot run\n * simultaneously on behalf of the same inode.\n *\n * As we work through the truncate and commmit bits of it to the journal there\n * is one core, guiding principle: the file's tree must always be consistent on\n * disk.  We must be able to restart the truncate after a crash.\n *\n * The file's tree may be transiently inconsistent in memory (although it\n * probably isn't), but whenever we close off and commit a journal transaction,\n * the contents of (the filesystem + the journal) must be consistent and\n * restartable.  It's pretty simple, really: bottom up, right to left (although\n * left-to-right works OK too).\n *\n * Note that at recovery time, journal replay occurs *before* the restart of\n * truncate against the orphan inode list.\n *\n * The committed inode has the new, desired i_size (which is the same as\n * i_disksize in this case).  After a crash, ext4_orphan_cleanup() will see\n * that this inode's truncate did not complete and it will again call\n * ext4_truncate() to have another go.  So there will be instantiated blocks\n * to the right of the truncation point in a crashed ext4 filesystem.  But\n * that's fine - as long as they are linked from the inode, the post-crash\n * ext4_truncate() run will find them and release them.\n */\nvoid ext4_truncate(struct inode *inode)\n{\n\thandle_t *handle;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\t__le32 *i_data = ei->i_data;\n\tint addr_per_block = EXT4_ADDR_PER_BLOCK(inode->i_sb);\n\tstruct address_space *mapping = inode->i_mapping;\n\text4_lblk_t offsets[4];\n\tIndirect chain[4];\n\tIndirect *partial;\n\t__le32 nr = 0;\n\tint n;\n\text4_lblk_t last_block;\n\tunsigned blocksize = inode->i_sb->s_blocksize;\n\n\tif (!ext4_can_truncate(inode))\n\t\treturn;\n\n\tEXT4_I(inode)->i_flags &= ~EXT4_EOFBLOCKS_FL;\n\n\tif (inode->i_size == 0 && !test_opt(inode->i_sb, NO_AUTO_DA_ALLOC))\n\t\text4_set_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE);\n\n\tif (EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL) {\n\t\text4_ext_truncate(inode);\n\t\treturn;\n\t}\n\n\thandle = start_transaction(inode);\n\tif (IS_ERR(handle))\n\t\treturn;\t\t/* AKPM: return what? */\n\n\tlast_block = (inode->i_size + blocksize-1)\n\t\t\t\t\t>> EXT4_BLOCK_SIZE_BITS(inode->i_sb);\n\n\tif (inode->i_size & (blocksize - 1))\n\t\tif (ext4_block_truncate_page(handle, mapping, inode->i_size))\n\t\t\tgoto out_stop;\n\n\tn = ext4_block_to_path(inode, last_block, offsets, NULL);\n\tif (n == 0)\n\t\tgoto out_stop;\t/* error */\n\n\t/*\n\t * OK.  This truncate is going to happen.  We add the inode to the\n\t * orphan list, so that if this truncate spans multiple transactions,\n\t * and we crash, we will resume the truncate when the filesystem\n\t * recovers.  It also marks the inode dirty, to catch the new size.\n\t *\n\t * Implication: the file must always be in a sane, consistent\n\t * truncatable state while each transaction commits.\n\t */\n\tif (ext4_orphan_add(handle, inode))\n\t\tgoto out_stop;\n\n\t/*\n\t * From here we block out all ext4_get_block() callers who want to\n\t * modify the block allocation tree.\n\t */\n\tdown_write(&ei->i_data_sem);\n\n\text4_discard_preallocations(inode);\n\n\t/*\n\t * The orphan list entry will now protect us from any crash which\n\t * occurs before the truncate completes, so it is now safe to propagate\n\t * the new, shorter inode size (held for now in i_size) into the\n\t * on-disk inode. We do this via i_disksize, which is the value which\n\t * ext4 *really* writes onto the disk inode.\n\t */\n\tei->i_disksize = inode->i_size;\n\n\tif (n == 1) {\t\t/* direct blocks */\n\t\text4_free_data(handle, inode, NULL, i_data+offsets[0],\n\t\t\t       i_data + EXT4_NDIR_BLOCKS);\n\t\tgoto do_indirects;\n\t}\n\n\tpartial = ext4_find_shared(inode, n, offsets, chain, &nr);\n\t/* Kill the top of shared branch (not detached) */\n\tif (nr) {\n\t\tif (partial == chain) {\n\t\t\t/* Shared branch grows from the inode */\n\t\t\text4_free_branches(handle, inode, NULL,\n\t\t\t\t\t   &nr, &nr+1, (chain+n-1) - partial);\n\t\t\t*partial->p = 0;\n\t\t\t/*\n\t\t\t * We mark the inode dirty prior to restart,\n\t\t\t * and prior to stop.  No need for it here.\n\t\t\t */\n\t\t} else {\n\t\t\t/* Shared branch grows from an indirect block */\n\t\t\tBUFFER_TRACE(partial->bh, \"get_write_access\");\n\t\t\text4_free_branches(handle, inode, partial->bh,\n\t\t\t\t\tpartial->p,\n\t\t\t\t\tpartial->p+1, (chain+n-1) - partial);\n\t\t}\n\t}\n\t/* Clear the ends of indirect blocks on the shared branch */\n\twhile (partial > chain) {\n\t\text4_free_branches(handle, inode, partial->bh, partial->p + 1,\n\t\t\t\t   (__le32*)partial->bh->b_data+addr_per_block,\n\t\t\t\t   (chain+n-1) - partial);\n\t\tBUFFER_TRACE(partial->bh, \"call brelse\");\n\t\tbrelse(partial->bh);\n\t\tpartial--;\n\t}\ndo_indirects:\n\t/* Kill the remaining (whole) subtrees */\n\tswitch (offsets[0]) {\n\tdefault:\n\t\tnr = i_data[EXT4_IND_BLOCK];\n\t\tif (nr) {\n\t\t\text4_free_branches(handle, inode, NULL, &nr, &nr+1, 1);\n\t\t\ti_data[EXT4_IND_BLOCK] = 0;\n\t\t}\n\tcase EXT4_IND_BLOCK:\n\t\tnr = i_data[EXT4_DIND_BLOCK];\n\t\tif (nr) {\n\t\t\text4_free_branches(handle, inode, NULL, &nr, &nr+1, 2);\n\t\t\ti_data[EXT4_DIND_BLOCK] = 0;\n\t\t}\n\tcase EXT4_DIND_BLOCK:\n\t\tnr = i_data[EXT4_TIND_BLOCK];\n\t\tif (nr) {\n\t\t\text4_free_branches(handle, inode, NULL, &nr, &nr+1, 3);\n\t\t\ti_data[EXT4_TIND_BLOCK] = 0;\n\t\t}\n\tcase EXT4_TIND_BLOCK:\n\t\t;\n\t}\n\n\tup_write(&ei->i_data_sem);\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\n\t/*\n\t * In a multi-transaction truncate, we only make the final transaction\n\t * synchronous\n\t */\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\nout_stop:\n\t/*\n\t * If this was a simple ftruncate(), and the file will remain alive\n\t * then we need to clear up the orphan record which we created above.\n\t * However, if this was a real unlink then we were called by\n\t * ext4_delete_inode(), and we allow that function to clean up the\n\t * orphan info for us.\n\t */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(handle, inode);\n\n\text4_journal_stop(handle);\n}\n\n/*\n * ext4_get_inode_loc returns with an extra refcount against the inode's\n * underlying buffer_head on success. If 'in_mem' is true, we have all\n * data in memory that is needed to recreate the on-disk version of this\n * inode.\n */\nstatic int __ext4_get_inode_loc(struct inode *inode,\n\t\t\t\tstruct ext4_iloc *iloc, int in_mem)\n{\n\tstruct ext4_group_desc\t*gdp;\n\tstruct buffer_head\t*bh;\n\tstruct super_block\t*sb = inode->i_sb;\n\text4_fsblk_t\t\tblock;\n\tint\t\t\tinodes_per_block, inode_offset;\n\n\tiloc->bh = NULL;\n\tif (!ext4_valid_inum(sb, inode->i_ino))\n\t\treturn -EIO;\n\n\tiloc->block_group = (inode->i_ino - 1) / EXT4_INODES_PER_GROUP(sb);\n\tgdp = ext4_get_group_desc(sb, iloc->block_group, NULL);\n\tif (!gdp)\n\t\treturn -EIO;\n\n\t/*\n\t * Figure out the offset within the block group inode table\n\t */\n\tinodes_per_block = (EXT4_BLOCK_SIZE(sb) / EXT4_INODE_SIZE(sb));\n\tinode_offset = ((inode->i_ino - 1) %\n\t\t\tEXT4_INODES_PER_GROUP(sb));\n\tblock = ext4_inode_table(sb, gdp) + (inode_offset / inodes_per_block);\n\tiloc->offset = (inode_offset % inodes_per_block) * EXT4_INODE_SIZE(sb);\n\n\tbh = sb_getblk(sb, block);\n\tif (!bh) {\n\t\text4_error(sb, \"unable to read inode block - \"\n\t\t\t   \"inode=%lu, block=%llu\", inode->i_ino, block);\n\t\treturn -EIO;\n\t}\n\tif (!buffer_uptodate(bh)) {\n\t\tlock_buffer(bh);\n\n\t\t/*\n\t\t * If the buffer has the write error flag, we have failed\n\t\t * to write out another inode in the same block.  In this\n\t\t * case, we don't have to read the block because we may\n\t\t * read the old inode data successfully.\n\t\t */\n\t\tif (buffer_write_io_error(bh) && !buffer_uptodate(bh))\n\t\t\tset_buffer_uptodate(bh);\n\n\t\tif (buffer_uptodate(bh)) {\n\t\t\t/* someone brought it uptodate while we waited */\n\t\t\tunlock_buffer(bh);\n\t\t\tgoto has_buffer;\n\t\t}\n\n\t\t/*\n\t\t * If we have all information of the inode in memory and this\n\t\t * is the only valid inode in the block, we need not read the\n\t\t * block.\n\t\t */\n\t\tif (in_mem) {\n\t\t\tstruct buffer_head *bitmap_bh;\n\t\t\tint i, start;\n\n\t\t\tstart = inode_offset & ~(inodes_per_block - 1);\n\n\t\t\t/* Is the inode bitmap in cache? */\n\t\t\tbitmap_bh = sb_getblk(sb, ext4_inode_bitmap(sb, gdp));\n\t\t\tif (!bitmap_bh)\n\t\t\t\tgoto make_io;\n\n\t\t\t/*\n\t\t\t * If the inode bitmap isn't in cache then the\n\t\t\t * optimisation may end up performing two reads instead\n\t\t\t * of one, so skip it.\n\t\t\t */\n\t\t\tif (!buffer_uptodate(bitmap_bh)) {\n\t\t\t\tbrelse(bitmap_bh);\n\t\t\t\tgoto make_io;\n\t\t\t}\n\t\t\tfor (i = start; i < start + inodes_per_block; i++) {\n\t\t\t\tif (i == inode_offset)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (ext4_test_bit(i, bitmap_bh->b_data))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbrelse(bitmap_bh);\n\t\t\tif (i == start + inodes_per_block) {\n\t\t\t\t/* all other inodes are free, so skip I/O */\n\t\t\t\tmemset(bh->b_data, 0, bh->b_size);\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\tunlock_buffer(bh);\n\t\t\t\tgoto has_buffer;\n\t\t\t}\n\t\t}\n\nmake_io:\n\t\t/*\n\t\t * If we need to do any I/O, try to pre-readahead extra\n\t\t * blocks from the inode table.\n\t\t */\n\t\tif (EXT4_SB(sb)->s_inode_readahead_blks) {\n\t\t\text4_fsblk_t b, end, table;\n\t\t\tunsigned num;\n\n\t\t\ttable = ext4_inode_table(sb, gdp);\n\t\t\t/* s_inode_readahead_blks is always a power of 2 */\n\t\t\tb = block & ~(EXT4_SB(sb)->s_inode_readahead_blks-1);\n\t\t\tif (table > b)\n\t\t\t\tb = table;\n\t\t\tend = b + EXT4_SB(sb)->s_inode_readahead_blks;\n\t\t\tnum = EXT4_INODES_PER_GROUP(sb);\n\t\t\tif (EXT4_HAS_RO_COMPAT_FEATURE(sb,\n\t\t\t\t       EXT4_FEATURE_RO_COMPAT_GDT_CSUM))\n\t\t\t\tnum -= ext4_itable_unused_count(sb, gdp);\n\t\t\ttable += num / inodes_per_block;\n\t\t\tif (end > table)\n\t\t\t\tend = table;\n\t\t\twhile (b <= end)\n\t\t\t\tsb_breadahead(sb, b++);\n\t\t}\n\n\t\t/*\n\t\t * There are other valid inodes in the buffer, this inode\n\t\t * has in-inode xattrs, or we don't have this inode in memory.\n\t\t * Read the block from disk.\n\t\t */\n\t\tget_bh(bh);\n\t\tbh->b_end_io = end_buffer_read_sync;\n\t\tsubmit_bh(READ_META, bh);\n\t\twait_on_buffer(bh);\n\t\tif (!buffer_uptodate(bh)) {\n\t\t\text4_error(sb, \"unable to read inode block - inode=%lu,\"\n\t\t\t\t   \" block=%llu\", inode->i_ino, block);\n\t\t\tbrelse(bh);\n\t\t\treturn -EIO;\n\t\t}\n\t}\nhas_buffer:\n\tiloc->bh = bh;\n\treturn 0;\n}\n\nint ext4_get_inode_loc(struct inode *inode, struct ext4_iloc *iloc)\n{\n\t/* We have all inode data except xattrs in memory here. */\n\treturn __ext4_get_inode_loc(inode, iloc,\n\t\t!ext4_test_inode_state(inode, EXT4_STATE_XATTR));\n}\n\nvoid ext4_set_inode_flags(struct inode *inode)\n{\n\tunsigned int flags = EXT4_I(inode)->i_flags;\n\n\tinode->i_flags &= ~(S_SYNC|S_APPEND|S_IMMUTABLE|S_NOATIME|S_DIRSYNC);\n\tif (flags & EXT4_SYNC_FL)\n\t\tinode->i_flags |= S_SYNC;\n\tif (flags & EXT4_APPEND_FL)\n\t\tinode->i_flags |= S_APPEND;\n\tif (flags & EXT4_IMMUTABLE_FL)\n\t\tinode->i_flags |= S_IMMUTABLE;\n\tif (flags & EXT4_NOATIME_FL)\n\t\tinode->i_flags |= S_NOATIME;\n\tif (flags & EXT4_DIRSYNC_FL)\n\t\tinode->i_flags |= S_DIRSYNC;\n}\n\n/* Propagate flags from i_flags to EXT4_I(inode)->i_flags */\nvoid ext4_get_inode_flags(struct ext4_inode_info *ei)\n{\n\tunsigned int flags = ei->vfs_inode.i_flags;\n\n\tei->i_flags &= ~(EXT4_SYNC_FL|EXT4_APPEND_FL|\n\t\t\tEXT4_IMMUTABLE_FL|EXT4_NOATIME_FL|EXT4_DIRSYNC_FL);\n\tif (flags & S_SYNC)\n\t\tei->i_flags |= EXT4_SYNC_FL;\n\tif (flags & S_APPEND)\n\t\tei->i_flags |= EXT4_APPEND_FL;\n\tif (flags & S_IMMUTABLE)\n\t\tei->i_flags |= EXT4_IMMUTABLE_FL;\n\tif (flags & S_NOATIME)\n\t\tei->i_flags |= EXT4_NOATIME_FL;\n\tif (flags & S_DIRSYNC)\n\t\tei->i_flags |= EXT4_DIRSYNC_FL;\n}\n\nstatic blkcnt_t ext4_inode_blocks(struct ext4_inode *raw_inode,\n\t\t\t\t  struct ext4_inode_info *ei)\n{\n\tblkcnt_t i_blocks ;\n\tstruct inode *inode = &(ei->vfs_inode);\n\tstruct super_block *sb = inode->i_sb;\n\n\tif (EXT4_HAS_RO_COMPAT_FEATURE(sb,\n\t\t\t\tEXT4_FEATURE_RO_COMPAT_HUGE_FILE)) {\n\t\t/* we are using combined 48 bit field */\n\t\ti_blocks = ((u64)le16_to_cpu(raw_inode->i_blocks_high)) << 32 |\n\t\t\t\t\tle32_to_cpu(raw_inode->i_blocks_lo);\n\t\tif (ei->i_flags & EXT4_HUGE_FILE_FL) {\n\t\t\t/* i_blocks represent file system block size */\n\t\t\treturn i_blocks  << (inode->i_blkbits - 9);\n\t\t} else {\n\t\t\treturn i_blocks;\n\t\t}\n\t} else {\n\t\treturn le32_to_cpu(raw_inode->i_blocks_lo);\n\t}\n}\n\nstruct inode *ext4_iget(struct super_block *sb, unsigned long ino)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei;\n\tstruct inode *inode;\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\tlong ret;\n\tint block;\n\n\tinode = iget_locked(sb, ino);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (!(inode->i_state & I_NEW))\n\t\treturn inode;\n\n\tei = EXT4_I(inode);\n\tiloc.bh = 0;\n\n\tret = __ext4_get_inode_loc(inode, &iloc, 0);\n\tif (ret < 0)\n\t\tgoto bad_inode;\n\traw_inode = ext4_raw_inode(&iloc);\n\tinode->i_mode = le16_to_cpu(raw_inode->i_mode);\n\tinode->i_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);\n\tinode->i_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\tinode->i_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;\n\t\tinode->i_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;\n\t}\n\tinode->i_nlink = le16_to_cpu(raw_inode->i_links_count);\n\n\tei->i_state_flags = 0;\n\tei->i_dir_start_lookup = 0;\n\tei->i_dtime = le32_to_cpu(raw_inode->i_dtime);\n\t/* We now have enough fields to check if the inode was active or not.\n\t * This is needed because nfsd might try to access dead inodes\n\t * the test is that same one that e2fsck uses\n\t * NeilBrown 1999oct15\n\t */\n\tif (inode->i_nlink == 0) {\n\t\tif (inode->i_mode == 0 ||\n\t\t    !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS)) {\n\t\t\t/* this inode is deleted */\n\t\t\tret = -ESTALE;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\t/* The only unlinked inodes we let through here have\n\t\t * valid i_mode and are being read by the orphan\n\t\t * recovery code: that's fine, we're about to complete\n\t\t * the process of deleting those. */\n\t}\n\tei->i_flags = le32_to_cpu(raw_inode->i_flags);\n\tinode->i_blocks = ext4_inode_blocks(raw_inode, ei);\n\tei->i_file_acl = le32_to_cpu(raw_inode->i_file_acl_lo);\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT))\n\t\tei->i_file_acl |=\n\t\t\t((__u64)le16_to_cpu(raw_inode->i_file_acl_high)) << 32;\n\tinode->i_size = ext4_isize(raw_inode);\n\tei->i_disksize = inode->i_size;\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tinode->i_generation = le32_to_cpu(raw_inode->i_generation);\n\tei->i_block_group = iloc.block_group;\n\tei->i_last_alloc_group = ~0;\n\t/*\n\t * NOTE! The in-memory inode i_data array is in little-endian order\n\t * even on big-endian machines: we do NOT byteswap the block numbers!\n\t */\n\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\tei->i_data[block] = raw_inode->i_block[block];\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\n\t/*\n\t * Set transaction id's of transactions that have to be committed\n\t * to finish f[data]sync. We set them to currently running transaction\n\t * as we cannot be sure that the inode or some of its metadata isn't\n\t * part of the transaction - the inode could have been reclaimed and\n\t * now it is reread from disk.\n\t */\n\tif (journal) {\n\t\ttransaction_t *transaction;\n\t\ttid_t tid;\n\n\t\tspin_lock(&journal->j_state_lock);\n\t\tif (journal->j_running_transaction)\n\t\t\ttransaction = journal->j_running_transaction;\n\t\telse\n\t\t\ttransaction = journal->j_committing_transaction;\n\t\tif (transaction)\n\t\t\ttid = transaction->t_tid;\n\t\telse\n\t\t\ttid = journal->j_commit_sequence;\n\t\tspin_unlock(&journal->j_state_lock);\n\t\tei->i_sync_tid = tid;\n\t\tei->i_datasync_tid = tid;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tei->i_extra_isize = le16_to_cpu(raw_inode->i_extra_isize);\n\t\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize >\n\t\t    EXT4_INODE_SIZE(inode->i_sb)) {\n\t\t\tret = -EIO;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\tif (ei->i_extra_isize == 0) {\n\t\t\t/* The extra space is currently unused. Use it. */\n\t\t\tei->i_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t    EXT4_GOOD_OLD_INODE_SIZE;\n\t\t} else {\n\t\t\t__le32 *magic = (void *)raw_inode +\n\t\t\t\t\tEXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\t\tei->i_extra_isize;\n\t\t\tif (*magic == cpu_to_le32(EXT4_XATTR_MAGIC))\n\t\t\t\text4_set_inode_state(inode, EXT4_STATE_XATTR);\n\t\t}\n\t} else\n\t\tei->i_extra_isize = 0;\n\n\tEXT4_INODE_GET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_GET_XTIME(i_crtime, ei, raw_inode);\n\n\tinode->i_version = le32_to_cpu(raw_inode->i_disk_version);\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\tinode->i_version |=\n\t\t\t(__u64)(le32_to_cpu(raw_inode->i_version_hi)) << 32;\n\t}\n\n\tret = 0;\n\tif (ei->i_file_acl &&\n\t    !ext4_data_block_valid(EXT4_SB(sb), ei->i_file_acl, 1)) {\n\t\text4_error(sb, \"bad extended attribute block %llu inode #%lu\",\n\t\t\t   ei->i_file_acl, inode->i_ino);\n\t\tret = -EIO;\n\t\tgoto bad_inode;\n\t} else if (ei->i_flags & EXT4_EXTENTS_FL) {\n\t\tif (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t    (S_ISLNK(inode->i_mode) &&\n\t\t     !ext4_inode_is_fast_symlink(inode)))\n\t\t\t/* Validate extent which is part of inode */\n\t\t\tret = ext4_ext_check_inode(inode);\n\t} else if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t   (S_ISLNK(inode->i_mode) &&\n\t\t    !ext4_inode_is_fast_symlink(inode))) {\n\t\t/* Validate block references which are part of inode */\n\t\tret = ext4_check_inode_blockref(inode);\n\t}\n\tif (ret)\n\t\tgoto bad_inode;\n\n\tif (S_ISREG(inode->i_mode)) {\n\t\tinode->i_op = &ext4_file_inode_operations;\n\t\tinode->i_fop = &ext4_file_operations;\n\t\text4_set_aops(inode);\n\t} else if (S_ISDIR(inode->i_mode)) {\n\t\tinode->i_op = &ext4_dir_inode_operations;\n\t\tinode->i_fop = &ext4_dir_operations;\n\t} else if (S_ISLNK(inode->i_mode)) {\n\t\tif (ext4_inode_is_fast_symlink(inode)) {\n\t\t\tinode->i_op = &ext4_fast_symlink_inode_operations;\n\t\t\tnd_terminate_link(ei->i_data, inode->i_size,\n\t\t\t\tsizeof(ei->i_data) - 1);\n\t\t} else {\n\t\t\tinode->i_op = &ext4_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t}\n\t} else if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) ||\n\t      S_ISFIFO(inode->i_mode) || S_ISSOCK(inode->i_mode)) {\n\t\tinode->i_op = &ext4_special_inode_operations;\n\t\tif (raw_inode->i_block[0])\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   old_decode_dev(le32_to_cpu(raw_inode->i_block[0])));\n\t\telse\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   new_decode_dev(le32_to_cpu(raw_inode->i_block[1])));\n\t} else {\n\t\tret = -EIO;\n\t\text4_error(inode->i_sb, \"bogus i_mode (%o) for inode=%lu\",\n\t\t\t   inode->i_mode, inode->i_ino);\n\t\tgoto bad_inode;\n\t}\n\tbrelse(iloc.bh);\n\text4_set_inode_flags(inode);\n\tunlock_new_inode(inode);\n\treturn inode;\n\nbad_inode:\n\tbrelse(iloc.bh);\n\tiget_failed(inode);\n\treturn ERR_PTR(ret);\n}\n\nstatic int ext4_inode_blocks_set(handle_t *handle,\n\t\t\t\tstruct ext4_inode *raw_inode,\n\t\t\t\tstruct ext4_inode_info *ei)\n{\n\tstruct inode *inode = &(ei->vfs_inode);\n\tu64 i_blocks = inode->i_blocks;\n\tstruct super_block *sb = inode->i_sb;\n\n\tif (i_blocks <= ~0U) {\n\t\t/*\n\t\t * i_blocks can be represnted in a 32 bit variable\n\t\t * as multiple of 512 bytes\n\t\t */\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = 0;\n\t\tei->i_flags &= ~EXT4_HUGE_FILE_FL;\n\t\treturn 0;\n\t}\n\tif (!EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_HUGE_FILE))\n\t\treturn -EFBIG;\n\n\tif (i_blocks <= 0xffffffffffffULL) {\n\t\t/*\n\t\t * i_blocks can be represented in a 48 bit variable\n\t\t * as multiple of 512 bytes\n\t\t */\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = cpu_to_le16(i_blocks >> 32);\n\t\tei->i_flags &= ~EXT4_HUGE_FILE_FL;\n\t} else {\n\t\tei->i_flags |= EXT4_HUGE_FILE_FL;\n\t\t/* i_block is stored in file system block size */\n\t\ti_blocks = i_blocks >> (inode->i_blkbits - 9);\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = cpu_to_le16(i_blocks >> 32);\n\t}\n\treturn 0;\n}\n\n/*\n * Post the struct inode info into an on-disk inode location in the\n * buffer-cache.  This gobbles the caller's reference to the\n * buffer_head in the inode location struct.\n *\n * The caller must have write access to iloc->bh.\n */\nstatic int ext4_do_update_inode(handle_t *handle,\n\t\t\t\tstruct inode *inode,\n\t\t\t\tstruct ext4_iloc *iloc)\n{\n\tstruct ext4_inode *raw_inode = ext4_raw_inode(iloc);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct buffer_head *bh = iloc->bh;\n\tint err = 0, rc, block;\n\n\t/* For fields not not tracking in the in-memory inode,\n\t * initialise them to zero for new inodes. */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_NEW))\n\t\tmemset(raw_inode, 0, EXT4_SB(inode->i_sb)->s_inode_size);\n\n\text4_get_inode_flags(ei);\n\traw_inode->i_mode = cpu_to_le16(inode->i_mode);\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\traw_inode->i_uid_low = cpu_to_le16(low_16_bits(inode->i_uid));\n\t\traw_inode->i_gid_low = cpu_to_le16(low_16_bits(inode->i_gid));\n/*\n * Fix up interoperability with old kernels. Otherwise, old inodes get\n * re-used with the upper 16 bits of the uid/gid intact\n */\n\t\tif (!ei->i_dtime) {\n\t\t\traw_inode->i_uid_high =\n\t\t\t\tcpu_to_le16(high_16_bits(inode->i_uid));\n\t\t\traw_inode->i_gid_high =\n\t\t\t\tcpu_to_le16(high_16_bits(inode->i_gid));\n\t\t} else {\n\t\t\traw_inode->i_uid_high = 0;\n\t\t\traw_inode->i_gid_high = 0;\n\t\t}\n\t} else {\n\t\traw_inode->i_uid_low =\n\t\t\tcpu_to_le16(fs_high2lowuid(inode->i_uid));\n\t\traw_inode->i_gid_low =\n\t\t\tcpu_to_le16(fs_high2lowgid(inode->i_gid));\n\t\traw_inode->i_uid_high = 0;\n\t\traw_inode->i_gid_high = 0;\n\t}\n\traw_inode->i_links_count = cpu_to_le16(inode->i_nlink);\n\n\tEXT4_INODE_SET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_SET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_SET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_SET_XTIME(i_crtime, ei, raw_inode);\n\n\tif (ext4_inode_blocks_set(handle, raw_inode, ei))\n\t\tgoto out_brelse;\n\traw_inode->i_dtime = cpu_to_le32(ei->i_dtime);\n\traw_inode->i_flags = cpu_to_le32(ei->i_flags);\n\tif (EXT4_SB(inode->i_sb)->s_es->s_creator_os !=\n\t    cpu_to_le32(EXT4_OS_HURD))\n\t\traw_inode->i_file_acl_high =\n\t\t\tcpu_to_le16(ei->i_file_acl >> 32);\n\traw_inode->i_file_acl_lo = cpu_to_le32(ei->i_file_acl);\n\text4_isize_set(raw_inode, ei->i_disksize);\n\tif (ei->i_disksize > 0x7fffffffULL) {\n\t\tstruct super_block *sb = inode->i_sb;\n\t\tif (!EXT4_HAS_RO_COMPAT_FEATURE(sb,\n\t\t\t\tEXT4_FEATURE_RO_COMPAT_LARGE_FILE) ||\n\t\t\t\tEXT4_SB(sb)->s_es->s_rev_level ==\n\t\t\t\tcpu_to_le32(EXT4_GOOD_OLD_REV)) {\n\t\t\t/* If this is the first large file\n\t\t\t * created, add a flag to the superblock.\n\t\t\t */\n\t\t\terr = ext4_journal_get_write_access(handle,\n\t\t\t\t\tEXT4_SB(sb)->s_sbh);\n\t\t\tif (err)\n\t\t\t\tgoto out_brelse;\n\t\t\text4_update_dynamic_rev(sb);\n\t\t\tEXT4_SET_RO_COMPAT_FEATURE(sb,\n\t\t\t\t\tEXT4_FEATURE_RO_COMPAT_LARGE_FILE);\n\t\t\tsb->s_dirt = 1;\n\t\t\text4_handle_sync(handle);\n\t\t\terr = ext4_handle_dirty_metadata(handle, NULL,\n\t\t\t\t\tEXT4_SB(sb)->s_sbh);\n\t\t}\n\t}\n\traw_inode->i_generation = cpu_to_le32(inode->i_generation);\n\tif (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode)) {\n\t\tif (old_valid_dev(inode->i_rdev)) {\n\t\t\traw_inode->i_block[0] =\n\t\t\t\tcpu_to_le32(old_encode_dev(inode->i_rdev));\n\t\t\traw_inode->i_block[1] = 0;\n\t\t} else {\n\t\t\traw_inode->i_block[0] = 0;\n\t\t\traw_inode->i_block[1] =\n\t\t\t\tcpu_to_le32(new_encode_dev(inode->i_rdev));\n\t\t\traw_inode->i_block[2] = 0;\n\t\t}\n\t} else\n\t\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\t\traw_inode->i_block[block] = ei->i_data[block];\n\n\traw_inode->i_disk_version = cpu_to_le32(inode->i_version);\n\tif (ei->i_extra_isize) {\n\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\traw_inode->i_version_hi =\n\t\t\tcpu_to_le32(inode->i_version >> 32);\n\t\traw_inode->i_extra_isize = cpu_to_le16(ei->i_extra_isize);\n\t}\n\n\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\trc = ext4_handle_dirty_metadata(handle, NULL, bh);\n\tif (!err)\n\t\terr = rc;\n\text4_clear_inode_state(inode, EXT4_STATE_NEW);\n\n\text4_update_inode_fsync_trans(handle, inode, 0);\nout_brelse:\n\tbrelse(bh);\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}\n\n/*\n * ext4_write_inode()\n *\n * We are called from a few places:\n *\n * - Within generic_file_write() for O_SYNC files.\n *   Here, there will be no transaction running. We wait for any running\n *   trasnaction to commit.\n *\n * - Within sys_sync(), kupdate and such.\n *   We wait on commit, if tol to.\n *\n * - Within prune_icache() (PF_MEMALLOC == true)\n *   Here we simply return.  We can't afford to block kswapd on the\n *   journal commit.\n *\n * In all cases it is actually safe for us to return without doing anything,\n * because the inode has been copied into a raw inode buffer in\n * ext4_mark_inode_dirty().  This is a correctness thing for O_SYNC and for\n * knfsd.\n *\n * Note that we are absolutely dependent upon all inode dirtiers doing the\n * right thing: they *must* call mark_inode_dirty() after dirtying info in\n * which we are interested.\n *\n * It would be a bug for them to not do this.  The code:\n *\n *\tmark_inode_dirty(inode)\n *\tstuff();\n *\tinode->i_size = expr;\n *\n * is in error because a kswapd-driven write_inode() could occur while\n * `stuff()' is running, and the new i_size will be lost.  Plus the inode\n * will no longer be on the superblock's dirty inode list.\n */\nint ext4_write_inode(struct inode *inode, int wait)\n{\n\tint err;\n\n\tif (current->flags & PF_MEMALLOC)\n\t\treturn 0;\n\n\tif (EXT4_SB(inode->i_sb)->s_journal) {\n\t\tif (ext4_journal_current_handle()) {\n\t\t\tjbd_debug(1, \"called recursively, non-PF_MEMALLOC!\\n\");\n\t\t\tdump_stack();\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (!wait)\n\t\t\treturn 0;\n\n\t\terr = ext4_force_commit(inode->i_sb);\n\t} else {\n\t\tstruct ext4_iloc iloc;\n\n\t\terr = ext4_get_inode_loc(inode, &iloc);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (wait)\n\t\t\tsync_dirty_buffer(iloc.bh);\n\t\tif (buffer_req(iloc.bh) && !buffer_uptodate(iloc.bh)) {\n\t\t\text4_error(inode->i_sb, \"IO error syncing inode, \"\n\t\t\t\t   \"inode=%lu, block=%llu\", inode->i_ino,\n\t\t\t\t   (unsigned long long)iloc.bh->b_blocknr);\n\t\t\terr = -EIO;\n\t\t}\n\t}\n\treturn err;\n}\n\n/*\n * ext4_setattr()\n *\n * Called from notify_change.\n *\n * We want to trap VFS attempts to truncate the file as soon as\n * possible.  In particular, we want to make sure that when the VFS\n * shrinks i_size, we put the inode on the orphan list and modify\n * i_disksize immediately, so that during the subsequent flushing of\n * dirty pages and freeing of disk blocks, we can guarantee that any\n * commit will leave the blocks being flushed in an unused state on\n * disk.  (On recovery, the inode will get truncated and the blocks will\n * be freed, so we have a strong guarantee that no future commit will\n * leave these blocks visible to the user.)\n *\n * Another thing we have to assure is that if we are in ordered mode\n * and inode is still attached to the committing transaction, we must\n * we start writeout of all the dirty pages which are being truncated.\n * This way we are sure that all the data written in the previous\n * transaction are already on disk (truncate waits for pages under\n * writeback).\n *\n * Called with inode->i_mutex down.\n */\nint ext4_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error, rc = 0;\n\tconst unsigned int ia_valid = attr->ia_valid;\n\n\terror = inode_change_ok(inode, attr);\n\tif (error)\n\t\treturn error;\n\n\tif ((ia_valid & ATTR_UID && attr->ia_uid != inode->i_uid) ||\n\t\t(ia_valid & ATTR_GID && attr->ia_gid != inode->i_gid)) {\n\t\thandle_t *handle;\n\n\t\t/* (user+group)*(old+new) structure, inode write (sb,\n\t\t * inode block, ? - but truncate inode update has it) */\n\t\thandle = ext4_journal_start(inode, (EXT4_MAXQUOTAS_INIT_BLOCKS(inode->i_sb)+\n\t\t\t\t\tEXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb))+3);\n\t\tif (IS_ERR(handle)) {\n\t\t\terror = PTR_ERR(handle);\n\t\t\tgoto err_out;\n\t\t}\n\t\terror = vfs_dq_transfer(inode, attr) ? -EDQUOT : 0;\n\t\tif (error) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn error;\n\t\t}\n\t\t/* Update corresponding info in inode so that everything is in\n\t\t * one transaction */\n\t\tif (attr->ia_valid & ATTR_UID)\n\t\t\tinode->i_uid = attr->ia_uid;\n\t\tif (attr->ia_valid & ATTR_GID)\n\t\t\tinode->i_gid = attr->ia_gid;\n\t\terror = ext4_mark_inode_dirty(handle, inode);\n\t\text4_journal_stop(handle);\n\t}\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL)) {\n\t\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t\t\tif (attr->ia_size > sbi->s_bitmap_maxbytes) {\n\t\t\t\terror = -EFBIG;\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (S_ISREG(inode->i_mode) &&\n\t    attr->ia_valid & ATTR_SIZE &&\n\t    (attr->ia_size < inode->i_size ||\n\t     (EXT4_I(inode)->i_flags & EXT4_EOFBLOCKS_FL))) {\n\t\thandle_t *handle;\n\n\t\thandle = ext4_journal_start(inode, 3);\n\t\tif (IS_ERR(handle)) {\n\t\t\terror = PTR_ERR(handle);\n\t\t\tgoto err_out;\n\t\t}\n\n\t\terror = ext4_orphan_add(handle, inode);\n\t\tEXT4_I(inode)->i_disksize = attr->ia_size;\n\t\trc = ext4_mark_inode_dirty(handle, inode);\n\t\tif (!error)\n\t\t\terror = rc;\n\t\text4_journal_stop(handle);\n\n\t\tif (ext4_should_order_data(inode)) {\n\t\t\terror = ext4_begin_ordered_truncate(inode,\n\t\t\t\t\t\t\t    attr->ia_size);\n\t\t\tif (error) {\n\t\t\t\t/* Do as much error cleanup as possible */\n\t\t\t\thandle = ext4_journal_start(inode, 3);\n\t\t\t\tif (IS_ERR(handle)) {\n\t\t\t\t\text4_orphan_del(NULL, inode);\n\t\t\t\t\tgoto err_out;\n\t\t\t\t}\n\t\t\t\text4_orphan_del(handle, inode);\n\t\t\t\text4_journal_stop(handle);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\t\t/* ext4_truncate will clear the flag */\n\t\tif ((EXT4_I(inode)->i_flags & EXT4_EOFBLOCKS_FL))\n\t\t\text4_truncate(inode);\n\t}\n\n\trc = inode_setattr(inode, attr);\n\n\t/* If inode_setattr's call to ext4_truncate failed to get a\n\t * transaction handle at all, we need to clean up the in-core\n\t * orphan list manually. */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(NULL, inode);\n\n\tif (!rc && (ia_valid & ATTR_MODE))\n\t\trc = ext4_acl_chmod(inode);\n\nerr_out:\n\text4_std_error(inode->i_sb, error);\n\tif (!error)\n\t\terror = rc;\n\treturn error;\n}\n\nint ext4_getattr(struct vfsmount *mnt, struct dentry *dentry,\n\t\t struct kstat *stat)\n{\n\tstruct inode *inode;\n\tunsigned long delalloc_blocks;\n\n\tinode = dentry->d_inode;\n\tgeneric_fillattr(inode, stat);\n\n\t/*\n\t * We can't update i_blocks if the block allocation is delayed\n\t * otherwise in the case of system crash before the real block\n\t * allocation is done, we will have i_blocks inconsistent with\n\t * on-disk file blocks.\n\t * We always keep i_blocks updated together with real\n\t * allocation. But to not confuse with user, stat\n\t * will return the blocks that include the delayed allocation\n\t * blocks for this file.\n\t */\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tdelalloc_blocks = EXT4_I(inode)->i_reserved_data_blocks;\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\tstat->blocks += (delalloc_blocks << inode->i_sb->s_blocksize_bits)>>9;\n\treturn 0;\n}\n\nstatic int ext4_indirect_trans_blocks(struct inode *inode, int nrblocks,\n\t\t\t\t      int chunk)\n{\n\tint indirects;\n\n\t/* if nrblocks are contiguous */\n\tif (chunk) {\n\t\t/*\n\t\t * With N contiguous data blocks, it need at most\n\t\t * N/EXT4_ADDR_PER_BLOCK(inode->i_sb) indirect blocks\n\t\t * 2 dindirect blocks\n\t\t * 1 tindirect block\n\t\t */\n\t\tindirects = nrblocks / EXT4_ADDR_PER_BLOCK(inode->i_sb);\n\t\treturn indirects + 3;\n\t}\n\t/*\n\t * if nrblocks are not contiguous, worse case, each block touch\n\t * a indirect block, and each indirect block touch a double indirect\n\t * block, plus a triple indirect block\n\t */\n\tindirects = nrblocks * 2 + 1;\n\treturn indirects;\n}\n\nstatic int ext4_index_trans_blocks(struct inode *inode, int nrblocks, int chunk)\n{\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL))\n\t\treturn ext4_indirect_trans_blocks(inode, nrblocks, chunk);\n\treturn ext4_ext_index_trans_blocks(inode, nrblocks, chunk);\n}\n\n/*\n * Account for index blocks, block groups bitmaps and block group\n * descriptor blocks if modify datablocks and index blocks\n * worse case, the indexs blocks spread over different block groups\n *\n * If datablocks are discontiguous, they are possible to spread over\n * different block groups too. If they are contiuguous, with flexbg,\n * they could still across block group boundary.\n *\n * Also account for superblock, inode, quota and xattr blocks\n */\nint ext4_meta_trans_blocks(struct inode *inode, int nrblocks, int chunk)\n{\n\text4_group_t groups, ngroups = ext4_get_groups_count(inode->i_sb);\n\tint gdpblocks;\n\tint idxblocks;\n\tint ret = 0;\n\n\t/*\n\t * How many index blocks need to touch to modify nrblocks?\n\t * The \"Chunk\" flag indicating whether the nrblocks is\n\t * physically contiguous on disk\n\t *\n\t * For Direct IO and fallocate, they calls get_block to allocate\n\t * one single extent at a time, so they could set the \"Chunk\" flag\n\t */\n\tidxblocks = ext4_index_trans_blocks(inode, nrblocks, chunk);\n\n\tret = idxblocks;\n\n\t/*\n\t * Now let's see how many group bitmaps and group descriptors need\n\t * to account\n\t */\n\tgroups = idxblocks;\n\tif (chunk)\n\t\tgroups += 1;\n\telse\n\t\tgroups += nrblocks;\n\n\tgdpblocks = groups;\n\tif (groups > ngroups)\n\t\tgroups = ngroups;\n\tif (groups > EXT4_SB(inode->i_sb)->s_gdb_count)\n\t\tgdpblocks = EXT4_SB(inode->i_sb)->s_gdb_count;\n\n\t/* bitmaps and block group descriptor blocks */\n\tret += groups + gdpblocks;\n\n\t/* Blocks for super block, inode, quota and xattr blocks */\n\tret += EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\n\treturn ret;\n}\n\n/*\n * Calulate the total number of credits to reserve to fit\n * the modification of a single pages into a single transaction,\n * which may include multiple chunks of block allocations.\n *\n * This could be called via ext4_write_begin()\n *\n * We need to consider the worse case, when\n * one new block per extent.\n */\nint ext4_writepage_trans_blocks(struct inode *inode)\n{\n\tint bpp = ext4_journal_blocks_per_page(inode);\n\tint ret;\n\n\tret = ext4_meta_trans_blocks(inode, bpp, 0);\n\n\t/* Account for data blocks for journalled mode */\n\tif (ext4_should_journal_data(inode))\n\t\tret += bpp;\n\treturn ret;\n}\n\n/*\n * Calculate the journal credits for a chunk of data modification.\n *\n * This is called from DIO, fallocate or whoever calling\n * ext4_get_blocks() to map/allocate a chunk of contiguous disk blocks.\n *\n * journal buffers for data blocks are not included here, as DIO\n * and fallocate do no need to journal data buffers.\n */\nint ext4_chunk_trans_blocks(struct inode *inode, int nrblocks)\n{\n\treturn ext4_meta_trans_blocks(inode, nrblocks, 1);\n}\n\n/*\n * The caller must have previously called ext4_reserve_inode_write().\n * Give this, we know that the caller already has write access to iloc->bh.\n */\nint ext4_mark_iloc_dirty(handle_t *handle,\n\t\t\t struct inode *inode, struct ext4_iloc *iloc)\n{\n\tint err = 0;\n\n\tif (test_opt(inode->i_sb, I_VERSION))\n\t\tinode_inc_iversion(inode);\n\n\t/* the do_update_inode consumes one bh->b_count */\n\tget_bh(iloc->bh);\n\n\t/* ext4_do_update_inode() does jbd2_journal_dirty_metadata */\n\terr = ext4_do_update_inode(handle, inode, iloc);\n\tput_bh(iloc->bh);\n\treturn err;\n}\n\n/*\n * On success, We end up with an outstanding reference count against\n * iloc->bh.  This _must_ be cleaned up later.\n */\n\nint\next4_reserve_inode_write(handle_t *handle, struct inode *inode,\n\t\t\t struct ext4_iloc *iloc)\n{\n\tint err;\n\n\terr = ext4_get_inode_loc(inode, iloc);\n\tif (!err) {\n\t\tBUFFER_TRACE(iloc->bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, iloc->bh);\n\t\tif (err) {\n\t\t\tbrelse(iloc->bh);\n\t\t\tiloc->bh = NULL;\n\t\t}\n\t}\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}\n\n/*\n * Expand an inode by new_extra_isize bytes.\n * Returns 0 on success or negative error number on failure.\n */\nstatic int ext4_expand_extra_isize(struct inode *inode,\n\t\t\t\t   unsigned int new_extra_isize,\n\t\t\t\t   struct ext4_iloc iloc,\n\t\t\t\t   handle_t *handle)\n{\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_xattr_entry *entry;\n\n\tif (EXT4_I(inode)->i_extra_isize >= new_extra_isize)\n\t\treturn 0;\n\n\traw_inode = ext4_raw_inode(&iloc);\n\n\theader = IHDR(inode, raw_inode);\n\tentry = IFIRST(header);\n\n\t/* No extended attributes present */\n\tif (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) ||\n\t    header->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {\n\t\tmemset((void *)raw_inode + EXT4_GOOD_OLD_INODE_SIZE, 0,\n\t\t\tnew_extra_isize);\n\t\tEXT4_I(inode)->i_extra_isize = new_extra_isize;\n\t\treturn 0;\n\t}\n\n\t/* try to expand with EAs present */\n\treturn ext4_expand_extra_isize_ea(inode, new_extra_isize,\n\t\t\t\t\t  raw_inode, handle);\n}\n\n/*\n * What we do here is to mark the in-core inode as clean with respect to inode\n * dirtiness (it may still be data-dirty).\n * This means that the in-core inode may be reaped by prune_icache\n * without having to perform any I/O.  This is a very good thing,\n * because *any* task may call prune_icache - even ones which\n * have a transaction open against a different journal.\n *\n * Is this cheating?  Not really.  Sure, we haven't written the\n * inode out, but prune_icache isn't a user-visible syncing function.\n * Whenever the user wants stuff synced (sys_sync, sys_msync, sys_fsync)\n * we start and wait on commits.\n *\n * Is this efficient/effective?  Well, we're being nice to the system\n * by cleaning up our inodes proactively so they can be reaped\n * without I/O.  But we are potentially leaving up to five seconds'\n * worth of inodes floating about which prune_icache wants us to\n * write out.  One way to fix that would be to get prune_icache()\n * to do a write_super() to free up some memory.  It has the desired\n * effect.\n */\nint ext4_mark_inode_dirty(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstatic unsigned int mnt_count;\n\tint err, ret;\n\n\tmight_sleep();\n\terr = ext4_reserve_inode_write(handle, inode, &iloc);\n\tif (ext4_handle_valid(handle) &&\n\t    EXT4_I(inode)->i_extra_isize < sbi->s_want_extra_isize &&\n\t    !ext4_test_inode_state(inode, EXT4_STATE_NO_EXPAND)) {\n\t\t/*\n\t\t * We need extra buffer credits since we may write into EA block\n\t\t * with this same handle. If journal_extend fails, then it will\n\t\t * only result in a minor loss of functionality for that inode.\n\t\t * If this is felt to be critical, then e2fsck should be run to\n\t\t * force a large enough s_min_extra_isize.\n\t\t */\n\t\tif ((jbd2_journal_extend(handle,\n\t\t\t     EXT4_DATA_TRANS_BLOCKS(inode->i_sb))) == 0) {\n\t\t\tret = ext4_expand_extra_isize(inode,\n\t\t\t\t\t\t      sbi->s_want_extra_isize,\n\t\t\t\t\t\t      iloc, handle);\n\t\t\tif (ret) {\n\t\t\t\text4_set_inode_state(inode,\n\t\t\t\t\t\t     EXT4_STATE_NO_EXPAND);\n\t\t\t\tif (mnt_count !=\n\t\t\t\t\tle16_to_cpu(sbi->s_es->s_mnt_count)) {\n\t\t\t\t\text4_warning(inode->i_sb,\n\t\t\t\t\t\"Unable to expand inode %lu. Delete\"\n\t\t\t\t\t\" some EAs or run e2fsck.\",\n\t\t\t\t\tinode->i_ino);\n\t\t\t\t\tmnt_count =\n\t\t\t\t\t  le16_to_cpu(sbi->s_es->s_mnt_count);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif (!err)\n\t\terr = ext4_mark_iloc_dirty(handle, inode, &iloc);\n\treturn err;\n}\n\n/*\n * ext4_dirty_inode() is called from __mark_inode_dirty()\n *\n * We're really interested in the case where a file is being extended.\n * i_size has been changed by generic_commit_write() and we thus need\n * to include the updated inode in the current transaction.\n *\n * Also, vfs_dq_alloc_block() will always dirty the inode when blocks\n * are allocated to the file.\n *\n * If the inode is marked synchronous, we don't honour that here - doing\n * so would cause a commit on atime updates, which we don't bother doing.\n * We handle synchronous inodes at the highest possible level.\n */\nvoid ext4_dirty_inode(struct inode *inode)\n{\n\thandle_t *handle;\n\n\thandle = ext4_journal_start(inode, 2);\n\tif (IS_ERR(handle))\n\t\tgoto out;\n\n\text4_mark_inode_dirty(handle, inode);\n\n\text4_journal_stop(handle);\nout:\n\treturn;\n}\n\n#if 0\n/*\n * Bind an inode's backing buffer_head into this transaction, to prevent\n * it from being flushed to disk early.  Unlike\n * ext4_reserve_inode_write, this leaves behind no bh reference and\n * returns no iloc structure, so the caller needs to repeat the iloc\n * lookup to mark the inode dirty later.\n */\nstatic int ext4_pin_inode(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_iloc iloc;\n\n\tint err = 0;\n\tif (handle) {\n\t\terr = ext4_get_inode_loc(inode, &iloc);\n\t\tif (!err) {\n\t\t\tBUFFER_TRACE(iloc.bh, \"get_write_access\");\n\t\t\terr = jbd2_journal_get_write_access(handle, iloc.bh);\n\t\t\tif (!err)\n\t\t\t\terr = ext4_handle_dirty_metadata(handle,\n\t\t\t\t\t\t\t\t NULL,\n\t\t\t\t\t\t\t\t iloc.bh);\n\t\t\tbrelse(iloc.bh);\n\t\t}\n\t}\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}\n#endif\n\nint ext4_change_inode_journal_flag(struct inode *inode, int val)\n{\n\tjournal_t *journal;\n\thandle_t *handle;\n\tint err;\n\n\t/*\n\t * We have to be very careful here: changing a data block's\n\t * journaling status dynamically is dangerous.  If we write a\n\t * data block to the journal, change the status and then delete\n\t * that block, we risk forgetting to revoke the old log record\n\t * from the journal and so a subsequent replay can corrupt data.\n\t * So, first we make sure that the journal is empty and that\n\t * nobody is changing anything.\n\t */\n\n\tjournal = EXT4_JOURNAL(inode);\n\tif (!journal)\n\t\treturn 0;\n\tif (is_journal_aborted(journal))\n\t\treturn -EROFS;\n\n\tjbd2_journal_lock_updates(journal);\n\tjbd2_journal_flush(journal);\n\n\t/*\n\t * OK, there are no updates running now, and all cached data is\n\t * synced to disk.  We are now in a completely consistent state\n\t * which doesn't have anything in the journal, and we know that\n\t * no filesystem updates are running, so it is safe to modify\n\t * the inode's in-core data-journaling state flag now.\n\t */\n\n\tif (val)\n\t\tEXT4_I(inode)->i_flags |= EXT4_JOURNAL_DATA_FL;\n\telse\n\t\tEXT4_I(inode)->i_flags &= ~EXT4_JOURNAL_DATA_FL;\n\text4_set_aops(inode);\n\n\tjbd2_journal_unlock_updates(journal);\n\n\t/* Finally we can mark the inode as dirty. */\n\n\thandle = ext4_journal_start(inode, 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\terr = ext4_mark_inode_dirty(handle, inode);\n\text4_handle_sync(handle);\n\text4_journal_stop(handle);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\nstatic int ext4_bh_unmapped(handle_t *handle, struct buffer_head *bh)\n{\n\treturn !buffer_mapped(bh);\n}\n\nint ext4_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct page *page = vmf->page;\n\tloff_t size;\n\tunsigned long len;\n\tint ret = -EINVAL;\n\tvoid *fsdata;\n\tstruct file *file = vma->vm_file;\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\t/*\n\t * Get i_alloc_sem to stop truncates messing with the inode. We cannot\n\t * get i_mutex because we are already holding mmap_sem.\n\t */\n\tdown_read(&inode->i_alloc_sem);\n\tsize = i_size_read(inode);\n\tif (page->mapping != mapping || size <= page_offset(page)\n\t    || !PageUptodate(page)) {\n\t\t/* page got truncated from under us? */\n\t\tgoto out_unlock;\n\t}\n\tret = 0;\n\tif (PageMappedToDisk(page))\n\t\tgoto out_unlock;\n\n\tif (page->index == size >> PAGE_CACHE_SHIFT)\n\t\tlen = size & ~PAGE_CACHE_MASK;\n\telse\n\t\tlen = PAGE_CACHE_SIZE;\n\n\tlock_page(page);\n\t/*\n\t * return if we have all the buffers mapped. This avoid\n\t * the need to call write_begin/write_end which does a\n\t * journal_start/journal_stop which can block and take\n\t * long time\n\t */\n\tif (page_has_buffers(page)) {\n\t\tif (!walk_page_buffers(NULL, page_buffers(page), 0, len, NULL,\n\t\t\t\t\text4_bh_unmapped)) {\n\t\t\tunlock_page(page);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\tunlock_page(page);\n\t/*\n\t * OK, we need to fill the hole... Do write_begin write_end\n\t * to do block allocation/reservation.We are not holding\n\t * inode.i__mutex here. That allow * parallel write_begin,\n\t * write_end call. lock_page prevent this from happening\n\t * on the same page though\n\t */\n\tret = mapping->a_ops->write_begin(file, mapping, page_offset(page),\n\t\t\tlen, AOP_FLAG_UNINTERRUPTIBLE, &page, &fsdata);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\tret = mapping->a_ops->write_end(file, mapping, page_offset(page),\n\t\t\tlen, len, page, fsdata);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\tret = 0;\nout_unlock:\n\tif (ret)\n\t\tret = VM_FAULT_SIGBUS;\n\tup_read(&inode->i_alloc_sem);\n\treturn ret;\n}\n", "/*\n *  linux/fs/ext4/super.c\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/fs/minix/inode.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  Big-endian to little-endian byte-swapping/bitmaps by\n *        David S. Miller (davem@caip.rutgers.edu), 1995\n */\n\n#include <linux/module.h>\n#include <linux/string.h>\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/vmalloc.h>\n#include <linux/jbd2.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/blkdev.h>\n#include <linux/parser.h>\n#include <linux/smp_lock.h>\n#include <linux/buffer_head.h>\n#include <linux/exportfs.h>\n#include <linux/vfs.h>\n#include <linux/random.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/quotaops.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/ctype.h>\n#include <linux/log2.h>\n#include <linux/crc16.h>\n#include <asm/uaccess.h>\n\n#include \"ext4.h\"\n#include \"ext4_jbd2.h\"\n#include \"xattr.h\"\n#include \"acl.h\"\n#include \"mballoc.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/ext4.h>\n\nstruct proc_dir_entry *ext4_proc_root;\nstatic struct kset *ext4_kset;\n\nstatic int ext4_load_journal(struct super_block *, struct ext4_super_block *,\n\t\t\t     unsigned long journal_devnum);\nstatic int ext4_commit_super(struct super_block *sb, int sync);\nstatic void ext4_mark_recovery_complete(struct super_block *sb,\n\t\t\t\t\tstruct ext4_super_block *es);\nstatic void ext4_clear_journal_err(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es);\nstatic int ext4_sync_fs(struct super_block *sb, int wait);\nstatic const char *ext4_decode_error(struct super_block *sb, int errno,\n\t\t\t\t     char nbuf[16]);\nstatic int ext4_remount(struct super_block *sb, int *flags, char *data);\nstatic int ext4_statfs(struct dentry *dentry, struct kstatfs *buf);\nstatic int ext4_unfreeze(struct super_block *sb);\nstatic void ext4_write_super(struct super_block *sb);\nstatic int ext4_freeze(struct super_block *sb);\n\n\next4_fsblk_t ext4_block_bitmap(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_block_bitmap_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_block_bitmap_hi) << 32 : 0);\n}\n\next4_fsblk_t ext4_inode_bitmap(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_inode_bitmap_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_inode_bitmap_hi) << 32 : 0);\n}\n\next4_fsblk_t ext4_inode_table(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_inode_table_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_inode_table_hi) << 32 : 0);\n}\n\n__u32 ext4_free_blks_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_free_blocks_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_free_blocks_count_hi) << 16 : 0);\n}\n\n__u32 ext4_free_inodes_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_free_inodes_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_free_inodes_count_hi) << 16 : 0);\n}\n\n__u32 ext4_used_dirs_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_used_dirs_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_used_dirs_count_hi) << 16 : 0);\n}\n\n__u32 ext4_itable_unused_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_itable_unused_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_itable_unused_hi) << 16 : 0);\n}\n\nvoid ext4_block_bitmap_set(struct super_block *sb,\n\t\t\t   struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_block_bitmap_lo = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_block_bitmap_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_inode_bitmap_set(struct super_block *sb,\n\t\t\t   struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_inode_bitmap_lo  = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_inode_bitmap_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_inode_table_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_inode_table_lo = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_inode_table_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_free_blks_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_free_blocks_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_free_blocks_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_free_inodes_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_free_inodes_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_free_inodes_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_used_dirs_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_used_dirs_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_used_dirs_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_itable_unused_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_itable_unused_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_itable_unused_hi = cpu_to_le16(count >> 16);\n}\n\n\n/* Just increment the non-pointer handle value */\nstatic handle_t *ext4_get_nojournal(void)\n{\n\thandle_t *handle = current->journal_info;\n\tunsigned long ref_cnt = (unsigned long)handle;\n\n\tBUG_ON(ref_cnt >= EXT4_NOJOURNAL_MAX_REF_COUNT);\n\n\tref_cnt++;\n\thandle = (handle_t *)ref_cnt;\n\n\tcurrent->journal_info = handle;\n\treturn handle;\n}\n\n\n/* Decrement the non-pointer handle value */\nstatic void ext4_put_nojournal(handle_t *handle)\n{\n\tunsigned long ref_cnt = (unsigned long)handle;\n\n\tBUG_ON(ref_cnt == 0);\n\n\tref_cnt--;\n\thandle = (handle_t *)ref_cnt;\n\n\tcurrent->journal_info = handle;\n}\n\n/*\n * Wrappers for jbd2_journal_start/end.\n *\n * The only special thing we need to do here is to make sure that all\n * journal_end calls result in the superblock being marked dirty, so\n * that sync() will call the filesystem's write_super callback if\n * appropriate.\n */\nhandle_t *ext4_journal_start_sb(struct super_block *sb, int nblocks)\n{\n\tjournal_t *journal;\n\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn ERR_PTR(-EROFS);\n\n\t/* Special case here: if the journal has aborted behind our\n\t * backs (eg. EIO in the commit thread), then we still need to\n\t * take the FS itself readonly cleanly. */\n\tjournal = EXT4_SB(sb)->s_journal;\n\tif (journal) {\n\t\tif (is_journal_aborted(journal)) {\n\t\t\text4_abort(sb, __func__, \"Detected aborted journal\");\n\t\t\treturn ERR_PTR(-EROFS);\n\t\t}\n\t\treturn jbd2_journal_start(journal, nblocks);\n\t}\n\treturn ext4_get_nojournal();\n}\n\n/*\n * The only special thing we need to do here is to make sure that all\n * jbd2_journal_stop calls result in the superblock being marked dirty, so\n * that sync() will call the filesystem's write_super callback if\n * appropriate.\n */\nint __ext4_journal_stop(const char *where, handle_t *handle)\n{\n\tstruct super_block *sb;\n\tint err;\n\tint rc;\n\n\tif (!ext4_handle_valid(handle)) {\n\t\text4_put_nojournal(handle);\n\t\treturn 0;\n\t}\n\tsb = handle->h_transaction->t_journal->j_private;\n\terr = handle->h_err;\n\trc = jbd2_journal_stop(handle);\n\n\tif (!err)\n\t\terr = rc;\n\tif (err)\n\t\t__ext4_std_error(sb, where, err);\n\treturn err;\n}\n\nvoid ext4_journal_abort_handle(const char *caller, const char *err_fn,\n\t\tstruct buffer_head *bh, handle_t *handle, int err)\n{\n\tchar nbuf[16];\n\tconst char *errstr = ext4_decode_error(NULL, err, nbuf);\n\n\tBUG_ON(!ext4_handle_valid(handle));\n\n\tif (bh)\n\t\tBUFFER_TRACE(bh, \"abort\");\n\n\tif (!handle->h_err)\n\t\thandle->h_err = err;\n\n\tif (is_handle_aborted(handle))\n\t\treturn;\n\n\tprintk(KERN_ERR \"%s: aborting transaction: %s in %s\\n\",\n\t       caller, errstr, err_fn);\n\n\tjbd2_journal_abort_handle(handle);\n}\n\n/* Deal with the reporting of failure conditions on a filesystem such as\n * inconsistencies detected or read IO failures.\n *\n * On ext2, we can store the error state of the filesystem in the\n * superblock.  That is not possible on ext4, because we may have other\n * write ordering constraints on the superblock which prevent us from\n * writing it out straight away; and given that the journal is about to\n * be aborted, we can't rely on the current, or future, transactions to\n * write out the superblock safely.\n *\n * We'll just use the jbd2_journal_abort() error code to record an error in\n * the journal instead.  On recovery, the journal will compain about\n * that error until we've noted it down and cleared it.\n */\n\nstatic void ext4_handle_error(struct super_block *sb)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn;\n\n\tif (!test_opt(sb, ERRORS_CONT)) {\n\t\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\t\tEXT4_SB(sb)->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\t\tif (journal)\n\t\t\tjbd2_journal_abort(journal, -EIO);\n\t}\n\tif (test_opt(sb, ERRORS_RO)) {\n\t\text4_msg(sb, KERN_CRIT, \"Remounting filesystem read-only\");\n\t\tsb->s_flags |= MS_RDONLY;\n\t}\n\text4_commit_super(sb, 1);\n\tif (test_opt(sb, ERRORS_PANIC))\n\t\tpanic(\"EXT4-fs (device %s): panic forced after error\\n\",\n\t\t\tsb->s_id);\n}\n\nvoid __ext4_error(struct super_block *sb, const char *function,\n\t\tconst char *fmt, ...)\n{\n\tva_list args;\n\n\tva_start(args, fmt);\n\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s: \", sb->s_id, function);\n\tvprintk(fmt, args);\n\tprintk(\"\\n\");\n\tva_end(args);\n\n\text4_handle_error(sb);\n}\n\nstatic const char *ext4_decode_error(struct super_block *sb, int errno,\n\t\t\t\t     char nbuf[16])\n{\n\tchar *errstr = NULL;\n\n\tswitch (errno) {\n\tcase -EIO:\n\t\terrstr = \"IO failure\";\n\t\tbreak;\n\tcase -ENOMEM:\n\t\terrstr = \"Out of memory\";\n\t\tbreak;\n\tcase -EROFS:\n\t\tif (!sb || (EXT4_SB(sb)->s_journal &&\n\t\t\t    EXT4_SB(sb)->s_journal->j_flags & JBD2_ABORT))\n\t\t\terrstr = \"Journal has aborted\";\n\t\telse\n\t\t\terrstr = \"Readonly filesystem\";\n\t\tbreak;\n\tdefault:\n\t\t/* If the caller passed in an extra buffer for unknown\n\t\t * errors, textualise them now.  Else we just return\n\t\t * NULL. */\n\t\tif (nbuf) {\n\t\t\t/* Check for truncated error codes... */\n\t\t\tif (snprintf(nbuf, 16, \"error %d\", -errno) >= 0)\n\t\t\t\terrstr = nbuf;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn errstr;\n}\n\n/* __ext4_std_error decodes expected errors from journaling functions\n * automatically and invokes the appropriate error response.  */\n\nvoid __ext4_std_error(struct super_block *sb, const char *function, int errno)\n{\n\tchar nbuf[16];\n\tconst char *errstr;\n\n\t/* Special case: if the error is EROFS, and we're not already\n\t * inside a transaction, then there's really no point in logging\n\t * an error. */\n\tif (errno == -EROFS && journal_current_handle() == NULL &&\n\t    (sb->s_flags & MS_RDONLY))\n\t\treturn;\n\n\terrstr = ext4_decode_error(sb, errno, nbuf);\n\tprintk(KERN_CRIT \"EXT4-fs error (device %s) in %s: %s\\n\",\n\t       sb->s_id, function, errstr);\n\n\text4_handle_error(sb);\n}\n\n/*\n * ext4_abort is a much stronger failure handler than ext4_error.  The\n * abort function may be used to deal with unrecoverable failures such\n * as journal IO errors or ENOMEM at a critical moment in log management.\n *\n * We unconditionally force the filesystem into an ABORT|READONLY state,\n * unless the error response on the fs has been set to panic in which\n * case we take the easy way out and panic immediately.\n */\n\nvoid ext4_abort(struct super_block *sb, const char *function,\n\t\tconst char *fmt, ...)\n{\n\tva_list args;\n\n\tva_start(args, fmt);\n\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s: \", sb->s_id, function);\n\tvprintk(fmt, args);\n\tprintk(\"\\n\");\n\tva_end(args);\n\n\tif (test_opt(sb, ERRORS_PANIC))\n\t\tpanic(\"EXT4-fs panic from previous error\\n\");\n\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn;\n\n\text4_msg(sb, KERN_CRIT, \"Remounting filesystem read-only\");\n\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\tsb->s_flags |= MS_RDONLY;\n\tEXT4_SB(sb)->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\tif (EXT4_SB(sb)->s_journal)\n\t\tjbd2_journal_abort(EXT4_SB(sb)->s_journal, -EIO);\n}\n\nvoid ext4_msg (struct super_block * sb, const char *prefix,\n\t\t   const char *fmt, ...)\n{\n\tva_list args;\n\n\tva_start(args, fmt);\n\tprintk(\"%sEXT4-fs (%s): \", prefix, sb->s_id);\n\tvprintk(fmt, args);\n\tprintk(\"\\n\");\n\tva_end(args);\n}\n\nvoid __ext4_warning(struct super_block *sb, const char *function,\n\t\t  const char *fmt, ...)\n{\n\tva_list args;\n\n\tva_start(args, fmt);\n\tprintk(KERN_WARNING \"EXT4-fs warning (device %s): %s: \",\n\t       sb->s_id, function);\n\tvprintk(fmt, args);\n\tprintk(\"\\n\");\n\tva_end(args);\n}\n\nvoid ext4_grp_locked_error(struct super_block *sb, ext4_group_t grp,\n\t\t\t   const char *function, const char *fmt, ...)\n__releases(bitlock)\n__acquires(bitlock)\n{\n\tva_list args;\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tva_start(args, fmt);\n\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s: \", sb->s_id, function);\n\tvprintk(fmt, args);\n\tprintk(\"\\n\");\n\tva_end(args);\n\n\tif (test_opt(sb, ERRORS_CONT)) {\n\t\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\t\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\t\text4_commit_super(sb, 0);\n\t\treturn;\n\t}\n\text4_unlock_group(sb, grp);\n\text4_handle_error(sb);\n\t/*\n\t * We only get here in the ERRORS_RO case; relocking the group\n\t * may be dangerous, but nothing bad will happen since the\n\t * filesystem will have already been marked read/only and the\n\t * journal has been aborted.  We return 1 as a hint to callers\n\t * who might what to use the return value from\n\t * ext4_grp_locked_error() to distinguish beween the\n\t * ERRORS_CONT and ERRORS_RO case, and perhaps return more\n\t * aggressively from the ext4 function in question, with a\n\t * more appropriate error code.\n\t */\n\text4_lock_group(sb, grp);\n\treturn;\n}\n\nvoid ext4_update_dynamic_rev(struct super_block *sb)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tif (le32_to_cpu(es->s_rev_level) > EXT4_GOOD_OLD_REV)\n\t\treturn;\n\n\text4_warning(sb,\n\t\t     \"updating to rev %d because of new feature flag, \"\n\t\t     \"running e2fsck is recommended\",\n\t\t     EXT4_DYNAMIC_REV);\n\n\tes->s_first_ino = cpu_to_le32(EXT4_GOOD_OLD_FIRST_INO);\n\tes->s_inode_size = cpu_to_le16(EXT4_GOOD_OLD_INODE_SIZE);\n\tes->s_rev_level = cpu_to_le32(EXT4_DYNAMIC_REV);\n\t/* leave es->s_feature_*compat flags alone */\n\t/* es->s_uuid will be set by e2fsck if empty */\n\n\t/*\n\t * The rest of the superblock fields should be zero, and if not it\n\t * means they are likely already in use, so leave them alone.  We\n\t * can leave it up to e2fsck to clean up any inconsistencies there.\n\t */\n}\n\n/*\n * Open the external journal device\n */\nstatic struct block_device *ext4_blkdev_get(dev_t dev, struct super_block *sb)\n{\n\tstruct block_device *bdev;\n\tchar b[BDEVNAME_SIZE];\n\n\tbdev = open_by_devnum(dev, FMODE_READ|FMODE_WRITE);\n\tif (IS_ERR(bdev))\n\t\tgoto fail;\n\treturn bdev;\n\nfail:\n\text4_msg(sb, KERN_ERR, \"failed to open journal device %s: %ld\",\n\t\t\t__bdevname(dev, b), PTR_ERR(bdev));\n\treturn NULL;\n}\n\n/*\n * Release the journal device\n */\nstatic int ext4_blkdev_put(struct block_device *bdev)\n{\n\tbd_release(bdev);\n\treturn blkdev_put(bdev, FMODE_READ|FMODE_WRITE);\n}\n\nstatic int ext4_blkdev_remove(struct ext4_sb_info *sbi)\n{\n\tstruct block_device *bdev;\n\tint ret = -ENODEV;\n\n\tbdev = sbi->journal_bdev;\n\tif (bdev) {\n\t\tret = ext4_blkdev_put(bdev);\n\t\tsbi->journal_bdev = NULL;\n\t}\n\treturn ret;\n}\n\nstatic inline struct inode *orphan_list_entry(struct list_head *l)\n{\n\treturn &list_entry(l, struct ext4_inode_info, i_orphan)->vfs_inode;\n}\n\nstatic void dump_orphan_list(struct super_block *sb, struct ext4_sb_info *sbi)\n{\n\tstruct list_head *l;\n\n\text4_msg(sb, KERN_ERR, \"sb orphan head is %d\",\n\t\t le32_to_cpu(sbi->s_es->s_last_orphan));\n\n\tprintk(KERN_ERR \"sb_info orphan list:\\n\");\n\tlist_for_each(l, &sbi->s_orphan) {\n\t\tstruct inode *inode = orphan_list_entry(l);\n\t\tprintk(KERN_ERR \"  \"\n\t\t       \"inode %s:%lu at %p: mode %o, nlink %d, next %d\\n\",\n\t\t       inode->i_sb->s_id, inode->i_ino, inode,\n\t\t       inode->i_mode, inode->i_nlink,\n\t\t       NEXT_ORPHAN(inode));\n\t}\n}\n\nstatic void ext4_put_super(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tint i, err;\n\n\tflush_workqueue(sbi->dio_unwritten_wq);\n\tdestroy_workqueue(sbi->dio_unwritten_wq);\n\n\tlock_super(sb);\n\tlock_kernel();\n\tif (sb->s_dirt)\n\t\text4_commit_super(sb, 1);\n\n\tif (sbi->s_journal) {\n\t\terr = jbd2_journal_destroy(sbi->s_journal);\n\t\tsbi->s_journal = NULL;\n\t\tif (err < 0)\n\t\t\text4_abort(sb, __func__,\n\t\t\t\t   \"Couldn't clean up the journal\");\n\t}\n\n\text4_release_system_zone(sb);\n\text4_mb_release(sb);\n\text4_ext_release(sb);\n\text4_xattr_put_super(sb);\n\n\tif (!(sb->s_flags & MS_RDONLY)) {\n\t\tEXT4_CLEAR_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);\n\t\tes->s_state = cpu_to_le16(sbi->s_mount_state);\n\t\text4_commit_super(sb, 1);\n\t}\n\tif (sbi->s_proc) {\n\t\tremove_proc_entry(sb->s_id, ext4_proc_root);\n\t}\n\tkobject_del(&sbi->s_kobj);\n\n\tfor (i = 0; i < sbi->s_gdb_count; i++)\n\t\tbrelse(sbi->s_group_desc[i]);\n\tkfree(sbi->s_group_desc);\n\tif (is_vmalloc_addr(sbi->s_flex_groups))\n\t\tvfree(sbi->s_flex_groups);\n\telse\n\t\tkfree(sbi->s_flex_groups);\n\tpercpu_counter_destroy(&sbi->s_freeblocks_counter);\n\tpercpu_counter_destroy(&sbi->s_freeinodes_counter);\n\tpercpu_counter_destroy(&sbi->s_dirs_counter);\n\tpercpu_counter_destroy(&sbi->s_dirtyblocks_counter);\n\tbrelse(sbi->s_sbh);\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(sbi->s_qf_names[i]);\n#endif\n\n\t/* Debugging code just in case the in-memory inode orphan list\n\t * isn't empty.  The on-disk one can be non-empty if we've\n\t * detected an error and taken the fs readonly, but the\n\t * in-memory list had better be clean by this point. */\n\tif (!list_empty(&sbi->s_orphan))\n\t\tdump_orphan_list(sb, sbi);\n\tJ_ASSERT(list_empty(&sbi->s_orphan));\n\n\tinvalidate_bdev(sb->s_bdev);\n\tif (sbi->journal_bdev && sbi->journal_bdev != sb->s_bdev) {\n\t\t/*\n\t\t * Invalidate the journal device's buffers.  We don't want them\n\t\t * floating about in memory - the physical journal device may\n\t\t * hotswapped, and it breaks the `ro-after' testing code.\n\t\t */\n\t\tsync_blockdev(sbi->journal_bdev);\n\t\tinvalidate_bdev(sbi->journal_bdev);\n\t\text4_blkdev_remove(sbi);\n\t}\n\tsb->s_fs_info = NULL;\n\t/*\n\t * Now that we are completely done shutting down the\n\t * superblock, we need to actually destroy the kobject.\n\t */\n\tunlock_kernel();\n\tunlock_super(sb);\n\tkobject_put(&sbi->s_kobj);\n\twait_for_completion(&sbi->s_kobj_unregister);\n\tkfree(sbi->s_blockgroup_lock);\n\tkfree(sbi);\n}\n\nstatic struct kmem_cache *ext4_inode_cachep;\n\n/*\n * Called inside transaction, so use GFP_NOFS\n */\nstatic struct inode *ext4_alloc_inode(struct super_block *sb)\n{\n\tstruct ext4_inode_info *ei;\n\n\tei = kmem_cache_alloc(ext4_inode_cachep, GFP_NOFS);\n\tif (!ei)\n\t\treturn NULL;\n\n\tei->vfs_inode.i_version = 1;\n\tei->vfs_inode.i_data.writeback_index = 0;\n\tmemset(&ei->i_cached_extent, 0, sizeof(struct ext4_ext_cache));\n\tINIT_LIST_HEAD(&ei->i_prealloc_list);\n\tspin_lock_init(&ei->i_prealloc_lock);\n\t/*\n\t * Note:  We can be called before EXT4_SB(sb)->s_journal is set,\n\t * therefore it can be null here.  Don't check it, just initialize\n\t * jinode.\n\t */\n\tjbd2_journal_init_jbd_inode(&ei->jinode, &ei->vfs_inode);\n\tei->i_reserved_data_blocks = 0;\n\tei->i_reserved_meta_blocks = 0;\n\tei->i_allocated_meta_blocks = 0;\n\tei->i_da_metadata_calc_len = 0;\n\tei->i_delalloc_reserved_flag = 0;\n\tspin_lock_init(&(ei->i_block_reservation_lock));\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tINIT_LIST_HEAD(&ei->i_completed_io_list);\n\tei->cur_aio_dio = NULL;\n\tei->i_sync_tid = 0;\n\tei->i_datasync_tid = 0;\n\n\treturn &ei->vfs_inode;\n}\n\nstatic void ext4_destroy_inode(struct inode *inode)\n{\n\tif (!list_empty(&(EXT4_I(inode)->i_orphan))) {\n\t\text4_msg(inode->i_sb, KERN_ERR,\n\t\t\t \"Inode %lu (%p): orphan list check failed!\",\n\t\t\t inode->i_ino, EXT4_I(inode));\n\t\tprint_hex_dump(KERN_INFO, \"\", DUMP_PREFIX_ADDRESS, 16, 4,\n\t\t\t\tEXT4_I(inode), sizeof(struct ext4_inode_info),\n\t\t\t\ttrue);\n\t\tdump_stack();\n\t}\n\tkmem_cache_free(ext4_inode_cachep, EXT4_I(inode));\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct ext4_inode_info *ei = (struct ext4_inode_info *) foo;\n\n\tINIT_LIST_HEAD(&ei->i_orphan);\n#ifdef CONFIG_EXT4_FS_XATTR\n\tinit_rwsem(&ei->xattr_sem);\n#endif\n\tinit_rwsem(&ei->i_data_sem);\n\tinode_init_once(&ei->vfs_inode);\n}\n\nstatic int init_inodecache(void)\n{\n\text4_inode_cachep = kmem_cache_create(\"ext4_inode_cache\",\n\t\t\t\t\t     sizeof(struct ext4_inode_info),\n\t\t\t\t\t     0, (SLAB_RECLAIM_ACCOUNT|\n\t\t\t\t\t\tSLAB_MEM_SPREAD),\n\t\t\t\t\t     init_once);\n\tif (ext4_inode_cachep == NULL)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void destroy_inodecache(void)\n{\n\tkmem_cache_destroy(ext4_inode_cachep);\n}\n\nstatic void ext4_clear_inode(struct inode *inode)\n{\n\text4_discard_preallocations(inode);\n\tif (EXT4_JOURNAL(inode))\n\t\tjbd2_journal_release_jbd_inode(EXT4_SB(inode->i_sb)->s_journal,\n\t\t\t\t       &EXT4_I(inode)->jinode);\n}\n\nstatic inline void ext4_show_quota_options(struct seq_file *seq,\n\t\t\t\t\t   struct super_block *sb)\n{\n#if defined(CONFIG_QUOTA)\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (sbi->s_jquota_fmt) {\n\t\tchar *fmtname = \"\";\n\n\t\tswitch (sbi->s_jquota_fmt) {\n\t\tcase QFMT_VFS_OLD:\n\t\t\tfmtname = \"vfsold\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V0:\n\t\t\tfmtname = \"vfsv0\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V1:\n\t\t\tfmtname = \"vfsv1\";\n\t\t\tbreak;\n\t\t}\n\t\tseq_printf(seq, \",jqfmt=%s\", fmtname);\n\t}\n\n\tif (sbi->s_qf_names[USRQUOTA])\n\t\tseq_printf(seq, \",usrjquota=%s\", sbi->s_qf_names[USRQUOTA]);\n\n\tif (sbi->s_qf_names[GRPQUOTA])\n\t\tseq_printf(seq, \",grpjquota=%s\", sbi->s_qf_names[GRPQUOTA]);\n\n\tif (test_opt(sb, USRQUOTA))\n\t\tseq_puts(seq, \",usrquota\");\n\n\tif (test_opt(sb, GRPQUOTA))\n\t\tseq_puts(seq, \",grpquota\");\n#endif\n}\n\n/*\n * Show an option if\n *  - it's set to a non-default value OR\n *  - if the per-sb default is different from the global default\n */\nstatic int ext4_show_options(struct seq_file *seq, struct vfsmount *vfs)\n{\n\tint def_errors;\n\tunsigned long def_mount_opts;\n\tstruct super_block *sb = vfs->mnt_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\n\tdef_mount_opts = le32_to_cpu(es->s_default_mount_opts);\n\tdef_errors     = le16_to_cpu(es->s_errors);\n\n\tif (sbi->s_sb_block != 1)\n\t\tseq_printf(seq, \",sb=%llu\", sbi->s_sb_block);\n\tif (test_opt(sb, MINIX_DF))\n\t\tseq_puts(seq, \",minixdf\");\n\tif (test_opt(sb, GRPID) && !(def_mount_opts & EXT4_DEFM_BSDGROUPS))\n\t\tseq_puts(seq, \",grpid\");\n\tif (!test_opt(sb, GRPID) && (def_mount_opts & EXT4_DEFM_BSDGROUPS))\n\t\tseq_puts(seq, \",nogrpid\");\n\tif (sbi->s_resuid != EXT4_DEF_RESUID ||\n\t    le16_to_cpu(es->s_def_resuid) != EXT4_DEF_RESUID) {\n\t\tseq_printf(seq, \",resuid=%u\", sbi->s_resuid);\n\t}\n\tif (sbi->s_resgid != EXT4_DEF_RESGID ||\n\t    le16_to_cpu(es->s_def_resgid) != EXT4_DEF_RESGID) {\n\t\tseq_printf(seq, \",resgid=%u\", sbi->s_resgid);\n\t}\n\tif (test_opt(sb, ERRORS_RO)) {\n\t\tif (def_errors == EXT4_ERRORS_PANIC ||\n\t\t    def_errors == EXT4_ERRORS_CONTINUE) {\n\t\t\tseq_puts(seq, \",errors=remount-ro\");\n\t\t}\n\t}\n\tif (test_opt(sb, ERRORS_CONT) && def_errors != EXT4_ERRORS_CONTINUE)\n\t\tseq_puts(seq, \",errors=continue\");\n\tif (test_opt(sb, ERRORS_PANIC) && def_errors != EXT4_ERRORS_PANIC)\n\t\tseq_puts(seq, \",errors=panic\");\n\tif (test_opt(sb, NO_UID32) && !(def_mount_opts & EXT4_DEFM_UID16))\n\t\tseq_puts(seq, \",nouid32\");\n\tif (test_opt(sb, DEBUG) && !(def_mount_opts & EXT4_DEFM_DEBUG))\n\t\tseq_puts(seq, \",debug\");\n\tif (test_opt(sb, OLDALLOC))\n\t\tseq_puts(seq, \",oldalloc\");\n#ifdef CONFIG_EXT4_FS_XATTR\n\tif (test_opt(sb, XATTR_USER) &&\n\t\t!(def_mount_opts & EXT4_DEFM_XATTR_USER))\n\t\tseq_puts(seq, \",user_xattr\");\n\tif (!test_opt(sb, XATTR_USER) &&\n\t    (def_mount_opts & EXT4_DEFM_XATTR_USER)) {\n\t\tseq_puts(seq, \",nouser_xattr\");\n\t}\n#endif\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\tif (test_opt(sb, POSIX_ACL) && !(def_mount_opts & EXT4_DEFM_ACL))\n\t\tseq_puts(seq, \",acl\");\n\tif (!test_opt(sb, POSIX_ACL) && (def_mount_opts & EXT4_DEFM_ACL))\n\t\tseq_puts(seq, \",noacl\");\n#endif\n\tif (sbi->s_commit_interval != JBD2_DEFAULT_MAX_COMMIT_AGE*HZ) {\n\t\tseq_printf(seq, \",commit=%u\",\n\t\t\t   (unsigned) (sbi->s_commit_interval / HZ));\n\t}\n\tif (sbi->s_min_batch_time != EXT4_DEF_MIN_BATCH_TIME) {\n\t\tseq_printf(seq, \",min_batch_time=%u\",\n\t\t\t   (unsigned) sbi->s_min_batch_time);\n\t}\n\tif (sbi->s_max_batch_time != EXT4_DEF_MAX_BATCH_TIME) {\n\t\tseq_printf(seq, \",max_batch_time=%u\",\n\t\t\t   (unsigned) sbi->s_min_batch_time);\n\t}\n\n\t/*\n\t * We're changing the default of barrier mount option, so\n\t * let's always display its mount state so it's clear what its\n\t * status is.\n\t */\n\tseq_puts(seq, \",barrier=\");\n\tseq_puts(seq, test_opt(sb, BARRIER) ? \"1\" : \"0\");\n\tif (test_opt(sb, JOURNAL_ASYNC_COMMIT))\n\t\tseq_puts(seq, \",journal_async_commit\");\n\tif (test_opt(sb, NOBH))\n\t\tseq_puts(seq, \",nobh\");\n\tif (test_opt(sb, I_VERSION))\n\t\tseq_puts(seq, \",i_version\");\n\tif (!test_opt(sb, DELALLOC))\n\t\tseq_puts(seq, \",nodelalloc\");\n\n\n\tif (sbi->s_stripe)\n\t\tseq_printf(seq, \",stripe=%lu\", sbi->s_stripe);\n\t/*\n\t * journal mode get enabled in different ways\n\t * So just print the value even if we didn't specify it\n\t */\n\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)\n\t\tseq_puts(seq, \",data=journal\");\n\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA)\n\t\tseq_puts(seq, \",data=ordered\");\n\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_WRITEBACK_DATA)\n\t\tseq_puts(seq, \",data=writeback\");\n\n\tif (sbi->s_inode_readahead_blks != EXT4_DEF_INODE_READAHEAD_BLKS)\n\t\tseq_printf(seq, \",inode_readahead_blks=%u\",\n\t\t\t   sbi->s_inode_readahead_blks);\n\n\tif (test_opt(sb, DATA_ERR_ABORT))\n\t\tseq_puts(seq, \",data_err=abort\");\n\n\tif (test_opt(sb, NO_AUTO_DA_ALLOC))\n\t\tseq_puts(seq, \",noauto_da_alloc\");\n\n\tif (test_opt(sb, DISCARD))\n\t\tseq_puts(seq, \",discard\");\n\n\tif (test_opt(sb, NOLOAD))\n\t\tseq_puts(seq, \",norecovery\");\n\n\text4_show_quota_options(seq, sb);\n\n\treturn 0;\n}\n\nstatic struct inode *ext4_nfs_get_inode(struct super_block *sb,\n\t\t\t\t\tu64 ino, u32 generation)\n{\n\tstruct inode *inode;\n\n\tif (ino < EXT4_FIRST_INO(sb) && ino != EXT4_ROOT_INO)\n\t\treturn ERR_PTR(-ESTALE);\n\tif (ino > le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count))\n\t\treturn ERR_PTR(-ESTALE);\n\n\t/* iget isn't really right if the inode is currently unallocated!!\n\t *\n\t * ext4_read_inode will return a bad_inode if the inode had been\n\t * deleted, so we should be safe.\n\t *\n\t * Currently we don't know the generation for parent directory, so\n\t * a generation of 0 means \"accept any\"\n\t */\n\tinode = ext4_iget(sb, ino);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\tif (generation && inode->i_generation != generation) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ESTALE);\n\t}\n\n\treturn inode;\n}\n\nstatic struct dentry *ext4_fh_to_dentry(struct super_block *sb, struct fid *fid,\n\t\t\t\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_dentry(sb, fid, fh_len, fh_type,\n\t\t\t\t    ext4_nfs_get_inode);\n}\n\nstatic struct dentry *ext4_fh_to_parent(struct super_block *sb, struct fid *fid,\n\t\t\t\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_parent(sb, fid, fh_len, fh_type,\n\t\t\t\t    ext4_nfs_get_inode);\n}\n\n/*\n * Try to release metadata pages (indirect blocks, directories) which are\n * mapped via the block device.  Since these pages could have journal heads\n * which would prevent try_to_free_buffers() from freeing them, we must use\n * jbd2 layer's try_to_free_buffers() function to release them.\n */\nstatic int bdev_try_to_free_page(struct super_block *sb, struct page *page,\n\t\t\t\t gfp_t wait)\n{\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\tWARN_ON(PageChecked(page));\n\tif (!page_has_buffers(page))\n\t\treturn 0;\n\tif (journal)\n\t\treturn jbd2_journal_try_to_free_buffers(journal, page,\n\t\t\t\t\t\t\twait & ~__GFP_WAIT);\n\treturn try_to_free_buffers(page);\n}\n\n#ifdef CONFIG_QUOTA\n#define QTYPE2NAME(t) ((t) == USRQUOTA ? \"user\" : \"group\")\n#define QTYPE2MOPT(on, t) ((t) == USRQUOTA?((on)##USRJQUOTA):((on)##GRPJQUOTA))\n\nstatic int ext4_write_dquot(struct dquot *dquot);\nstatic int ext4_acquire_dquot(struct dquot *dquot);\nstatic int ext4_release_dquot(struct dquot *dquot);\nstatic int ext4_mark_dquot_dirty(struct dquot *dquot);\nstatic int ext4_write_info(struct super_block *sb, int type);\nstatic int ext4_quota_on(struct super_block *sb, int type, int format_id,\n\t\t\t\tchar *path, int remount);\nstatic int ext4_quota_on_mount(struct super_block *sb, int type);\nstatic ssize_t ext4_quota_read(struct super_block *sb, int type, char *data,\n\t\t\t       size_t len, loff_t off);\nstatic ssize_t ext4_quota_write(struct super_block *sb, int type,\n\t\t\t\tconst char *data, size_t len, loff_t off);\n\nstatic const struct dquot_operations ext4_quota_operations = {\n\t.initialize\t= dquot_initialize,\n\t.drop\t\t= dquot_drop,\n\t.alloc_space\t= dquot_alloc_space,\n\t.reserve_space\t= dquot_reserve_space,\n\t.claim_space\t= dquot_claim_space,\n\t.release_rsv\t= dquot_release_reserved_space,\n#ifdef CONFIG_QUOTA\n\t.get_reserved_space = ext4_get_reserved_space,\n#endif\n\t.alloc_inode\t= dquot_alloc_inode,\n\t.free_space\t= dquot_free_space,\n\t.free_inode\t= dquot_free_inode,\n\t.transfer\t= dquot_transfer,\n\t.write_dquot\t= ext4_write_dquot,\n\t.acquire_dquot\t= ext4_acquire_dquot,\n\t.release_dquot\t= ext4_release_dquot,\n\t.mark_dirty\t= ext4_mark_dquot_dirty,\n\t.write_info\t= ext4_write_info,\n\t.alloc_dquot\t= dquot_alloc,\n\t.destroy_dquot\t= dquot_destroy,\n};\n\nstatic const struct quotactl_ops ext4_qctl_operations = {\n\t.quota_on\t= ext4_quota_on,\n\t.quota_off\t= vfs_quota_off,\n\t.quota_sync\t= vfs_quota_sync,\n\t.get_info\t= vfs_get_dqinfo,\n\t.set_info\t= vfs_set_dqinfo,\n\t.get_dqblk\t= vfs_get_dqblk,\n\t.set_dqblk\t= vfs_set_dqblk\n};\n#endif\n\nstatic const struct super_operations ext4_sops = {\n\t.alloc_inode\t= ext4_alloc_inode,\n\t.destroy_inode\t= ext4_destroy_inode,\n\t.write_inode\t= ext4_write_inode,\n\t.dirty_inode\t= ext4_dirty_inode,\n\t.delete_inode\t= ext4_delete_inode,\n\t.put_super\t= ext4_put_super,\n\t.sync_fs\t= ext4_sync_fs,\n\t.freeze_fs\t= ext4_freeze,\n\t.unfreeze_fs\t= ext4_unfreeze,\n\t.statfs\t\t= ext4_statfs,\n\t.remount_fs\t= ext4_remount,\n\t.clear_inode\t= ext4_clear_inode,\n\t.show_options\t= ext4_show_options,\n#ifdef CONFIG_QUOTA\n\t.quota_read\t= ext4_quota_read,\n\t.quota_write\t= ext4_quota_write,\n#endif\n\t.bdev_try_to_free_page = bdev_try_to_free_page,\n};\n\nstatic const struct super_operations ext4_nojournal_sops = {\n\t.alloc_inode\t= ext4_alloc_inode,\n\t.destroy_inode\t= ext4_destroy_inode,\n\t.write_inode\t= ext4_write_inode,\n\t.dirty_inode\t= ext4_dirty_inode,\n\t.delete_inode\t= ext4_delete_inode,\n\t.write_super\t= ext4_write_super,\n\t.put_super\t= ext4_put_super,\n\t.statfs\t\t= ext4_statfs,\n\t.remount_fs\t= ext4_remount,\n\t.clear_inode\t= ext4_clear_inode,\n\t.show_options\t= ext4_show_options,\n#ifdef CONFIG_QUOTA\n\t.quota_read\t= ext4_quota_read,\n\t.quota_write\t= ext4_quota_write,\n#endif\n\t.bdev_try_to_free_page = bdev_try_to_free_page,\n};\n\nstatic const struct export_operations ext4_export_ops = {\n\t.fh_to_dentry = ext4_fh_to_dentry,\n\t.fh_to_parent = ext4_fh_to_parent,\n\t.get_parent = ext4_get_parent,\n};\n\nenum {\n\tOpt_bsd_df, Opt_minix_df, Opt_grpid, Opt_nogrpid,\n\tOpt_resgid, Opt_resuid, Opt_sb, Opt_err_cont, Opt_err_panic, Opt_err_ro,\n\tOpt_nouid32, Opt_debug, Opt_oldalloc, Opt_orlov,\n\tOpt_user_xattr, Opt_nouser_xattr, Opt_acl, Opt_noacl,\n\tOpt_auto_da_alloc, Opt_noauto_da_alloc, Opt_noload, Opt_nobh, Opt_bh,\n\tOpt_commit, Opt_min_batch_time, Opt_max_batch_time,\n\tOpt_journal_update, Opt_journal_dev,\n\tOpt_journal_checksum, Opt_journal_async_commit,\n\tOpt_abort, Opt_data_journal, Opt_data_ordered, Opt_data_writeback,\n\tOpt_data_err_abort, Opt_data_err_ignore,\n\tOpt_usrjquota, Opt_grpjquota, Opt_offusrjquota, Opt_offgrpjquota,\n\tOpt_jqfmt_vfsold, Opt_jqfmt_vfsv0, Opt_jqfmt_vfsv1, Opt_quota,\n\tOpt_noquota, Opt_ignore, Opt_barrier, Opt_nobarrier, Opt_err,\n\tOpt_resize, Opt_usrquota, Opt_grpquota, Opt_i_version,\n\tOpt_stripe, Opt_delalloc, Opt_nodelalloc,\n\tOpt_block_validity, Opt_noblock_validity,\n\tOpt_inode_readahead_blks, Opt_journal_ioprio,\n\tOpt_discard, Opt_nodiscard,\n};\n\nstatic const match_table_t tokens = {\n\t{Opt_bsd_df, \"bsddf\"},\n\t{Opt_minix_df, \"minixdf\"},\n\t{Opt_grpid, \"grpid\"},\n\t{Opt_grpid, \"bsdgroups\"},\n\t{Opt_nogrpid, \"nogrpid\"},\n\t{Opt_nogrpid, \"sysvgroups\"},\n\t{Opt_resgid, \"resgid=%u\"},\n\t{Opt_resuid, \"resuid=%u\"},\n\t{Opt_sb, \"sb=%u\"},\n\t{Opt_err_cont, \"errors=continue\"},\n\t{Opt_err_panic, \"errors=panic\"},\n\t{Opt_err_ro, \"errors=remount-ro\"},\n\t{Opt_nouid32, \"nouid32\"},\n\t{Opt_debug, \"debug\"},\n\t{Opt_oldalloc, \"oldalloc\"},\n\t{Opt_orlov, \"orlov\"},\n\t{Opt_user_xattr, \"user_xattr\"},\n\t{Opt_nouser_xattr, \"nouser_xattr\"},\n\t{Opt_acl, \"acl\"},\n\t{Opt_noacl, \"noacl\"},\n\t{Opt_noload, \"noload\"},\n\t{Opt_noload, \"norecovery\"},\n\t{Opt_nobh, \"nobh\"},\n\t{Opt_bh, \"bh\"},\n\t{Opt_commit, \"commit=%u\"},\n\t{Opt_min_batch_time, \"min_batch_time=%u\"},\n\t{Opt_max_batch_time, \"max_batch_time=%u\"},\n\t{Opt_journal_update, \"journal=update\"},\n\t{Opt_journal_dev, \"journal_dev=%u\"},\n\t{Opt_journal_checksum, \"journal_checksum\"},\n\t{Opt_journal_async_commit, \"journal_async_commit\"},\n\t{Opt_abort, \"abort\"},\n\t{Opt_data_journal, \"data=journal\"},\n\t{Opt_data_ordered, \"data=ordered\"},\n\t{Opt_data_writeback, \"data=writeback\"},\n\t{Opt_data_err_abort, \"data_err=abort\"},\n\t{Opt_data_err_ignore, \"data_err=ignore\"},\n\t{Opt_offusrjquota, \"usrjquota=\"},\n\t{Opt_usrjquota, \"usrjquota=%s\"},\n\t{Opt_offgrpjquota, \"grpjquota=\"},\n\t{Opt_grpjquota, \"grpjquota=%s\"},\n\t{Opt_jqfmt_vfsold, \"jqfmt=vfsold\"},\n\t{Opt_jqfmt_vfsv0, \"jqfmt=vfsv0\"},\n\t{Opt_jqfmt_vfsv1, \"jqfmt=vfsv1\"},\n\t{Opt_grpquota, \"grpquota\"},\n\t{Opt_noquota, \"noquota\"},\n\t{Opt_quota, \"quota\"},\n\t{Opt_usrquota, \"usrquota\"},\n\t{Opt_barrier, \"barrier=%u\"},\n\t{Opt_barrier, \"barrier\"},\n\t{Opt_nobarrier, \"nobarrier\"},\n\t{Opt_i_version, \"i_version\"},\n\t{Opt_stripe, \"stripe=%u\"},\n\t{Opt_resize, \"resize\"},\n\t{Opt_delalloc, \"delalloc\"},\n\t{Opt_nodelalloc, \"nodelalloc\"},\n\t{Opt_block_validity, \"block_validity\"},\n\t{Opt_noblock_validity, \"noblock_validity\"},\n\t{Opt_inode_readahead_blks, \"inode_readahead_blks=%u\"},\n\t{Opt_journal_ioprio, \"journal_ioprio=%u\"},\n\t{Opt_auto_da_alloc, \"auto_da_alloc=%u\"},\n\t{Opt_auto_da_alloc, \"auto_da_alloc\"},\n\t{Opt_noauto_da_alloc, \"noauto_da_alloc\"},\n\t{Opt_discard, \"discard\"},\n\t{Opt_nodiscard, \"nodiscard\"},\n\t{Opt_err, NULL},\n};\n\nstatic ext4_fsblk_t get_sb_block(void **data)\n{\n\text4_fsblk_t\tsb_block;\n\tchar\t\t*options = (char *) *data;\n\n\tif (!options || strncmp(options, \"sb=\", 3) != 0)\n\t\treturn 1;\t/* Default location */\n\n\toptions += 3;\n\t/* TODO: use simple_strtoll with >32bit ext4 */\n\tsb_block = simple_strtoul(options, &options, 0);\n\tif (*options && *options != ',') {\n\t\tprintk(KERN_ERR \"EXT4-fs: Invalid sb specification: %s\\n\",\n\t\t       (char *) *data);\n\t\treturn 1;\n\t}\n\tif (*options == ',')\n\t\toptions++;\n\t*data = (void *) options;\n\n\treturn sb_block;\n}\n\n#define DEFAULT_JOURNAL_IOPRIO (IOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE, 3))\nstatic char deprecated_msg[] = \"Mount option \\\"%s\\\" will be removed by %s\\n\"\n\t\"Contact linux-ext4@vger.kernel.org if you think we should keep it.\\n\";\n\n#ifdef CONFIG_QUOTA\nstatic int set_qf_name(struct super_block *sb, int qtype, substring_t *args)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *qname;\n\n\tif (sb_any_quota_loaded(sb) &&\n\t\t!sbi->s_qf_names[qtype]) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Cannot change journaled \"\n\t\t\t\"quota options when quota turned on\");\n\t\treturn 0;\n\t}\n\tqname = match_strdup(args);\n\tif (!qname) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Not enough memory for storing quotafile name\");\n\t\treturn 0;\n\t}\n\tif (sbi->s_qf_names[qtype] &&\n\t\tstrcmp(sbi->s_qf_names[qtype], qname)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"%s quota file already specified\", QTYPE2NAME(qtype));\n\t\tkfree(qname);\n\t\treturn 0;\n\t}\n\tsbi->s_qf_names[qtype] = qname;\n\tif (strchr(sbi->s_qf_names[qtype], '/')) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"quotafile must be on filesystem root\");\n\t\tkfree(sbi->s_qf_names[qtype]);\n\t\tsbi->s_qf_names[qtype] = NULL;\n\t\treturn 0;\n\t}\n\tset_opt(sbi->s_mount_opt, QUOTA);\n\treturn 1;\n}\n\nstatic int clear_qf_name(struct super_block *sb, int qtype)\n{\n\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (sb_any_quota_loaded(sb) &&\n\t\tsbi->s_qf_names[qtype]) {\n\t\text4_msg(sb, KERN_ERR, \"Cannot change journaled quota options\"\n\t\t\t\" when quota turned on\");\n\t\treturn 0;\n\t}\n\t/*\n\t * The space will be released later when all options are confirmed\n\t * to be correct\n\t */\n\tsbi->s_qf_names[qtype] = NULL;\n\treturn 1;\n}\n#endif\n\nstatic int parse_options(char *options, struct super_block *sb,\n\t\t\t unsigned long *journal_devnum,\n\t\t\t unsigned int *journal_ioprio,\n\t\t\t ext4_fsblk_t *n_blocks_count, int is_remount)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *p;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint data_opt = 0;\n\tint option;\n#ifdef CONFIG_QUOTA\n\tint qfmt;\n#endif\n\n\tif (!options)\n\t\treturn 1;\n\n\twhile ((p = strsep(&options, \",\")) != NULL) {\n\t\tint token;\n\t\tif (!*p)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Initialize args struct so we know whether arg was\n\t\t * found; some options take optional arguments.\n\t\t */\n\t\targs[0].to = args[0].from = 0;\n\t\ttoken = match_token(p, tokens, args);\n\t\tswitch (token) {\n\t\tcase Opt_bsd_df:\n\t\t\text4_msg(sb, KERN_WARNING, deprecated_msg, p, \"2.6.38\");\n\t\t\tclear_opt(sbi->s_mount_opt, MINIX_DF);\n\t\t\tbreak;\n\t\tcase Opt_minix_df:\n\t\t\text4_msg(sb, KERN_WARNING, deprecated_msg, p, \"2.6.38\");\n\t\t\tset_opt(sbi->s_mount_opt, MINIX_DF);\n\n\t\t\tbreak;\n\t\tcase Opt_grpid:\n\t\t\text4_msg(sb, KERN_WARNING, deprecated_msg, p, \"2.6.38\");\n\t\t\tset_opt(sbi->s_mount_opt, GRPID);\n\n\t\t\tbreak;\n\t\tcase Opt_nogrpid:\n\t\t\text4_msg(sb, KERN_WARNING, deprecated_msg, p, \"2.6.38\");\n\t\t\tclear_opt(sbi->s_mount_opt, GRPID);\n\n\t\t\tbreak;\n\t\tcase Opt_resuid:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tsbi->s_resuid = option;\n\t\t\tbreak;\n\t\tcase Opt_resgid:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tsbi->s_resgid = option;\n\t\t\tbreak;\n\t\tcase Opt_sb:\n\t\t\t/* handled by get_sb_block() instead of here */\n\t\t\t/* *sb_block = match_int(&args[0]); */\n\t\t\tbreak;\n\t\tcase Opt_err_panic:\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_CONT);\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_RO);\n\t\t\tset_opt(sbi->s_mount_opt, ERRORS_PANIC);\n\t\t\tbreak;\n\t\tcase Opt_err_ro:\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_CONT);\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_PANIC);\n\t\t\tset_opt(sbi->s_mount_opt, ERRORS_RO);\n\t\t\tbreak;\n\t\tcase Opt_err_cont:\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_RO);\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_PANIC);\n\t\t\tset_opt(sbi->s_mount_opt, ERRORS_CONT);\n\t\t\tbreak;\n\t\tcase Opt_nouid32:\n\t\t\tset_opt(sbi->s_mount_opt, NO_UID32);\n\t\t\tbreak;\n\t\tcase Opt_debug:\n\t\t\tset_opt(sbi->s_mount_opt, DEBUG);\n\t\t\tbreak;\n\t\tcase Opt_oldalloc:\n\t\t\tset_opt(sbi->s_mount_opt, OLDALLOC);\n\t\t\tbreak;\n\t\tcase Opt_orlov:\n\t\t\tclear_opt(sbi->s_mount_opt, OLDALLOC);\n\t\t\tbreak;\n#ifdef CONFIG_EXT4_FS_XATTR\n\t\tcase Opt_user_xattr:\n\t\t\tset_opt(sbi->s_mount_opt, XATTR_USER);\n\t\t\tbreak;\n\t\tcase Opt_nouser_xattr:\n\t\t\tclear_opt(sbi->s_mount_opt, XATTR_USER);\n\t\t\tbreak;\n#else\n\t\tcase Opt_user_xattr:\n\t\tcase Opt_nouser_xattr:\n\t\t\text4_msg(sb, KERN_ERR, \"(no)user_xattr options not supported\");\n\t\t\tbreak;\n#endif\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\t\tcase Opt_acl:\n\t\t\tset_opt(sbi->s_mount_opt, POSIX_ACL);\n\t\t\tbreak;\n\t\tcase Opt_noacl:\n\t\t\tclear_opt(sbi->s_mount_opt, POSIX_ACL);\n\t\t\tbreak;\n#else\n\t\tcase Opt_acl:\n\t\tcase Opt_noacl:\n\t\t\text4_msg(sb, KERN_ERR, \"(no)acl options not supported\");\n\t\t\tbreak;\n#endif\n\t\tcase Opt_journal_update:\n\t\t\t/* @@@ FIXME */\n\t\t\t/* Eventually we will want to be able to create\n\t\t\t   a journal file here.  For now, only allow the\n\t\t\t   user to specify an existing inode to be the\n\t\t\t   journal file. */\n\t\t\tif (is_remount) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t \"Cannot specify journal on remount\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tset_opt(sbi->s_mount_opt, UPDATE_JOURNAL);\n\t\t\tbreak;\n\t\tcase Opt_journal_dev:\n\t\t\tif (is_remount) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\"Cannot specify journal on remount\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\t*journal_devnum = option;\n\t\t\tbreak;\n\t\tcase Opt_journal_checksum:\n\t\t\tset_opt(sbi->s_mount_opt, JOURNAL_CHECKSUM);\n\t\t\tbreak;\n\t\tcase Opt_journal_async_commit:\n\t\t\tset_opt(sbi->s_mount_opt, JOURNAL_ASYNC_COMMIT);\n\t\t\tset_opt(sbi->s_mount_opt, JOURNAL_CHECKSUM);\n\t\t\tbreak;\n\t\tcase Opt_noload:\n\t\t\tset_opt(sbi->s_mount_opt, NOLOAD);\n\t\t\tbreak;\n\t\tcase Opt_commit:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0)\n\t\t\t\treturn 0;\n\t\t\tif (option == 0)\n\t\t\t\toption = JBD2_DEFAULT_MAX_COMMIT_AGE;\n\t\t\tsbi->s_commit_interval = HZ * option;\n\t\t\tbreak;\n\t\tcase Opt_max_batch_time:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0)\n\t\t\t\treturn 0;\n\t\t\tif (option == 0)\n\t\t\t\toption = EXT4_DEF_MAX_BATCH_TIME;\n\t\t\tsbi->s_max_batch_time = option;\n\t\t\tbreak;\n\t\tcase Opt_min_batch_time:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0)\n\t\t\t\treturn 0;\n\t\t\tsbi->s_min_batch_time = option;\n\t\t\tbreak;\n\t\tcase Opt_data_journal:\n\t\t\tdata_opt = EXT4_MOUNT_JOURNAL_DATA;\n\t\t\tgoto datacheck;\n\t\tcase Opt_data_ordered:\n\t\t\tdata_opt = EXT4_MOUNT_ORDERED_DATA;\n\t\t\tgoto datacheck;\n\t\tcase Opt_data_writeback:\n\t\t\tdata_opt = EXT4_MOUNT_WRITEBACK_DATA;\n\t\tdatacheck:\n\t\t\tif (is_remount) {\n\t\t\t\tif (test_opt(sb, DATA_FLAGS) != data_opt) {\n\t\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\t\"Cannot change data mode on remount\");\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tclear_opt(sbi->s_mount_opt, DATA_FLAGS);\n\t\t\t\tsbi->s_mount_opt |= data_opt;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Opt_data_err_abort:\n\t\t\tset_opt(sbi->s_mount_opt, DATA_ERR_ABORT);\n\t\t\tbreak;\n\t\tcase Opt_data_err_ignore:\n\t\t\tclear_opt(sbi->s_mount_opt, DATA_ERR_ABORT);\n\t\t\tbreak;\n#ifdef CONFIG_QUOTA\n\t\tcase Opt_usrjquota:\n\t\t\tif (!set_qf_name(sb, USRQUOTA, &args[0]))\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tcase Opt_grpjquota:\n\t\t\tif (!set_qf_name(sb, GRPQUOTA, &args[0]))\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tcase Opt_offusrjquota:\n\t\t\tif (!clear_qf_name(sb, USRQUOTA))\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tcase Opt_offgrpjquota:\n\t\t\tif (!clear_qf_name(sb, GRPQUOTA))\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\n\t\tcase Opt_jqfmt_vfsold:\n\t\t\tqfmt = QFMT_VFS_OLD;\n\t\t\tgoto set_qf_format;\n\t\tcase Opt_jqfmt_vfsv0:\n\t\t\tqfmt = QFMT_VFS_V0;\n\t\t\tgoto set_qf_format;\n\t\tcase Opt_jqfmt_vfsv1:\n\t\t\tqfmt = QFMT_VFS_V1;\nset_qf_format:\n\t\t\tif (sb_any_quota_loaded(sb) &&\n\t\t\t    sbi->s_jquota_fmt != qfmt) {\n\t\t\t\text4_msg(sb, KERN_ERR, \"Cannot change \"\n\t\t\t\t\t\"journaled quota options when \"\n\t\t\t\t\t\"quota turned on\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tsbi->s_jquota_fmt = qfmt;\n\t\t\tbreak;\n\t\tcase Opt_quota:\n\t\tcase Opt_usrquota:\n\t\t\tset_opt(sbi->s_mount_opt, QUOTA);\n\t\t\tset_opt(sbi->s_mount_opt, USRQUOTA);\n\t\t\tbreak;\n\t\tcase Opt_grpquota:\n\t\t\tset_opt(sbi->s_mount_opt, QUOTA);\n\t\t\tset_opt(sbi->s_mount_opt, GRPQUOTA);\n\t\t\tbreak;\n\t\tcase Opt_noquota:\n\t\t\tif (sb_any_quota_loaded(sb)) {\n\t\t\t\text4_msg(sb, KERN_ERR, \"Cannot change quota \"\n\t\t\t\t\t\"options when quota turned on\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tclear_opt(sbi->s_mount_opt, QUOTA);\n\t\t\tclear_opt(sbi->s_mount_opt, USRQUOTA);\n\t\t\tclear_opt(sbi->s_mount_opt, GRPQUOTA);\n\t\t\tbreak;\n#else\n\t\tcase Opt_quota:\n\t\tcase Opt_usrquota:\n\t\tcase Opt_grpquota:\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\"quota options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_usrjquota:\n\t\tcase Opt_grpjquota:\n\t\tcase Opt_offusrjquota:\n\t\tcase Opt_offgrpjquota:\n\t\tcase Opt_jqfmt_vfsold:\n\t\tcase Opt_jqfmt_vfsv0:\n\t\tcase Opt_jqfmt_vfsv1:\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\"journaled quota options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_noquota:\n\t\t\tbreak;\n#endif\n\t\tcase Opt_abort:\n\t\t\tsbi->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\t\t\tbreak;\n\t\tcase Opt_nobarrier:\n\t\t\tclear_opt(sbi->s_mount_opt, BARRIER);\n\t\t\tbreak;\n\t\tcase Opt_barrier:\n\t\t\tif (args[0].from) {\n\t\t\t\tif (match_int(&args[0], &option))\n\t\t\t\t\treturn 0;\n\t\t\t} else\n\t\t\t\toption = 1;\t/* No argument, default to 1 */\n\t\t\tif (option)\n\t\t\t\tset_opt(sbi->s_mount_opt, BARRIER);\n\t\t\telse\n\t\t\t\tclear_opt(sbi->s_mount_opt, BARRIER);\n\t\t\tbreak;\n\t\tcase Opt_ignore:\n\t\t\tbreak;\n\t\tcase Opt_resize:\n\t\t\tif (!is_remount) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\"resize option only available \"\n\t\t\t\t\t\"for remount\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tif (match_int(&args[0], &option) != 0)\n\t\t\t\treturn 0;\n\t\t\t*n_blocks_count = option;\n\t\t\tbreak;\n\t\tcase Opt_nobh:\n\t\t\tset_opt(sbi->s_mount_opt, NOBH);\n\t\t\tbreak;\n\t\tcase Opt_bh:\n\t\t\tclear_opt(sbi->s_mount_opt, NOBH);\n\t\t\tbreak;\n\t\tcase Opt_i_version:\n\t\t\tset_opt(sbi->s_mount_opt, I_VERSION);\n\t\t\tsb->s_flags |= MS_I_VERSION;\n\t\t\tbreak;\n\t\tcase Opt_nodelalloc:\n\t\t\tclear_opt(sbi->s_mount_opt, DELALLOC);\n\t\t\tbreak;\n\t\tcase Opt_stripe:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0)\n\t\t\t\treturn 0;\n\t\t\tsbi->s_stripe = option;\n\t\t\tbreak;\n\t\tcase Opt_delalloc:\n\t\t\tset_opt(sbi->s_mount_opt, DELALLOC);\n\t\t\tbreak;\n\t\tcase Opt_block_validity:\n\t\t\tset_opt(sbi->s_mount_opt, BLOCK_VALIDITY);\n\t\t\tbreak;\n\t\tcase Opt_noblock_validity:\n\t\t\tclear_opt(sbi->s_mount_opt, BLOCK_VALIDITY);\n\t\t\tbreak;\n\t\tcase Opt_inode_readahead_blks:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0 || option > (1 << 30))\n\t\t\t\treturn 0;\n\t\t\tif (!is_power_of_2(option)) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t \"EXT4-fs: inode_readahead_blks\"\n\t\t\t\t\t \" must be a power of 2\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tsbi->s_inode_readahead_blks = option;\n\t\t\tbreak;\n\t\tcase Opt_journal_ioprio:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0 || option > 7)\n\t\t\t\tbreak;\n\t\t\t*journal_ioprio = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE,\n\t\t\t\t\t\t\t    option);\n\t\t\tbreak;\n\t\tcase Opt_noauto_da_alloc:\n\t\t\tset_opt(sbi->s_mount_opt,NO_AUTO_DA_ALLOC);\n\t\t\tbreak;\n\t\tcase Opt_auto_da_alloc:\n\t\t\tif (args[0].from) {\n\t\t\t\tif (match_int(&args[0], &option))\n\t\t\t\t\treturn 0;\n\t\t\t} else\n\t\t\t\toption = 1;\t/* No argument, default to 1 */\n\t\t\tif (option)\n\t\t\t\tclear_opt(sbi->s_mount_opt, NO_AUTO_DA_ALLOC);\n\t\t\telse\n\t\t\t\tset_opt(sbi->s_mount_opt,NO_AUTO_DA_ALLOC);\n\t\t\tbreak;\n\t\tcase Opt_discard:\n\t\t\tset_opt(sbi->s_mount_opt, DISCARD);\n\t\t\tbreak;\n\t\tcase Opt_nodiscard:\n\t\t\tclear_opt(sbi->s_mount_opt, DISCARD);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"Unrecognized mount option \\\"%s\\\" \"\n\t\t\t       \"or missing value\", p);\n\t\t\treturn 0;\n\t\t}\n\t}\n#ifdef CONFIG_QUOTA\n\tif (sbi->s_qf_names[USRQUOTA] || sbi->s_qf_names[GRPQUOTA]) {\n\t\tif (test_opt(sb, USRQUOTA) && sbi->s_qf_names[USRQUOTA])\n\t\t\tclear_opt(sbi->s_mount_opt, USRQUOTA);\n\n\t\tif (test_opt(sb, GRPQUOTA) && sbi->s_qf_names[GRPQUOTA])\n\t\t\tclear_opt(sbi->s_mount_opt, GRPQUOTA);\n\n\t\tif (test_opt(sb, GRPQUOTA) || test_opt(sb, USRQUOTA)) {\n\t\t\text4_msg(sb, KERN_ERR, \"old and new quota \"\n\t\t\t\t\t\"format mixing\");\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!sbi->s_jquota_fmt) {\n\t\t\text4_msg(sb, KERN_ERR, \"journaled quota format \"\n\t\t\t\t\t\"not specified\");\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tif (sbi->s_jquota_fmt) {\n\t\t\text4_msg(sb, KERN_ERR, \"journaled quota format \"\n\t\t\t\t\t\"specified with no journaling \"\n\t\t\t\t\t\"enabled\");\n\t\t\treturn 0;\n\t\t}\n\t}\n#endif\n\treturn 1;\n}\n\nstatic int ext4_setup_super(struct super_block *sb, struct ext4_super_block *es,\n\t\t\t    int read_only)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tint res = 0;\n\n\tif (le32_to_cpu(es->s_rev_level) > EXT4_MAX_SUPP_REV) {\n\t\text4_msg(sb, KERN_ERR, \"revision level too high, \"\n\t\t\t \"forcing read-only mode\");\n\t\tres = MS_RDONLY;\n\t}\n\tif (read_only)\n\t\treturn res;\n\tif (!(sbi->s_mount_state & EXT4_VALID_FS))\n\t\text4_msg(sb, KERN_WARNING, \"warning: mounting unchecked fs, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if ((sbi->s_mount_state & EXT4_ERROR_FS))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: mounting fs with errors, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if ((__s16) le16_to_cpu(es->s_max_mnt_count) >= 0 &&\n\t\t le16_to_cpu(es->s_mnt_count) >=\n\t\t (unsigned short) (__s16) le16_to_cpu(es->s_max_mnt_count))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: maximal mount count reached, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if (le32_to_cpu(es->s_checkinterval) &&\n\t\t(le32_to_cpu(es->s_lastcheck) +\n\t\t\tle32_to_cpu(es->s_checkinterval) <= get_seconds()))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: checktime reached, \"\n\t\t\t \"running e2fsck is recommended\");\n\tif (!sbi->s_journal)\n\t\tes->s_state &= cpu_to_le16(~EXT4_VALID_FS);\n\tif (!(__s16) le16_to_cpu(es->s_max_mnt_count))\n\t\tes->s_max_mnt_count = cpu_to_le16(EXT4_DFL_MAX_MNT_COUNT);\n\tle16_add_cpu(&es->s_mnt_count, 1);\n\tes->s_mtime = cpu_to_le32(get_seconds());\n\text4_update_dynamic_rev(sb);\n\tif (sbi->s_journal)\n\t\tEXT4_SET_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);\n\n\text4_commit_super(sb, 1);\n\tif (test_opt(sb, DEBUG))\n\t\tprintk(KERN_INFO \"[EXT4 FS bs=%lu, gc=%u, \"\n\t\t\t\t\"bpg=%lu, ipg=%lu, mo=%04x]\\n\",\n\t\t\tsb->s_blocksize,\n\t\t\tsbi->s_groups_count,\n\t\t\tEXT4_BLOCKS_PER_GROUP(sb),\n\t\t\tEXT4_INODES_PER_GROUP(sb),\n\t\t\tsbi->s_mount_opt);\n\n\treturn res;\n}\n\nstatic int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tint groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\tif (groups_per_flex < 2) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = kzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\tsbi->s_flex_groups = vmalloc(size);\n\t\tif (sbi->s_flex_groups)\n\t\t\tmemset(sbi->s_flex_groups, 0, size);\n\t}\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for \"\n\t\t\t\t\"%u flex groups\", flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_blks_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_blocks);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}\n\n__le16 ext4_group_desc_csum(struct ext4_sb_info *sbi, __u32 block_group,\n\t\t\t    struct ext4_group_desc *gdp)\n{\n\t__u16 crc = 0;\n\n\tif (sbi->s_es->s_feature_ro_compat &\n\t    cpu_to_le32(EXT4_FEATURE_RO_COMPAT_GDT_CSUM)) {\n\t\tint offset = offsetof(struct ext4_group_desc, bg_checksum);\n\t\t__le32 le_group = cpu_to_le32(block_group);\n\n\t\tcrc = crc16(~0, sbi->s_es->s_uuid, sizeof(sbi->s_es->s_uuid));\n\t\tcrc = crc16(crc, (__u8 *)&le_group, sizeof(le_group));\n\t\tcrc = crc16(crc, (__u8 *)gdp, offset);\n\t\toffset += sizeof(gdp->bg_checksum); /* skip checksum */\n\t\t/* for checksum of struct ext4_group_desc do the rest...*/\n\t\tif ((sbi->s_es->s_feature_incompat &\n\t\t     cpu_to_le32(EXT4_FEATURE_INCOMPAT_64BIT)) &&\n\t\t    offset < le16_to_cpu(sbi->s_es->s_desc_size))\n\t\t\tcrc = crc16(crc, (__u8 *)gdp + offset,\n\t\t\t\t    le16_to_cpu(sbi->s_es->s_desc_size) -\n\t\t\t\t\toffset);\n\t}\n\n\treturn cpu_to_le16(crc);\n}\n\nint ext4_group_desc_csum_verify(struct ext4_sb_info *sbi, __u32 block_group,\n\t\t\t\tstruct ext4_group_desc *gdp)\n{\n\tif ((sbi->s_es->s_feature_ro_compat &\n\t     cpu_to_le32(EXT4_FEATURE_RO_COMPAT_GDT_CSUM)) &&\n\t    (gdp->bg_checksum != ext4_group_desc_csum(sbi, block_group, gdp)))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/* Called at mount-time, super-block is locked */\nstatic int ext4_check_descriptors(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t first_block = le32_to_cpu(sbi->s_es->s_first_data_block);\n\text4_fsblk_t last_block;\n\text4_fsblk_t block_bitmap;\n\text4_fsblk_t inode_bitmap;\n\text4_fsblk_t inode_table;\n\tint flexbg_flag = 0;\n\text4_group_t i;\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG))\n\t\tflexbg_flag = 1;\n\n\text4_debug(\"Checking group descriptors\");\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tif (i == sbi->s_groups_count - 1 || flexbg_flag)\n\t\t\tlast_block = ext4_blocks_count(sbi->s_es) - 1;\n\t\telse\n\t\t\tlast_block = first_block +\n\t\t\t\t(EXT4_BLOCKS_PER_GROUP(sb) - 1);\n\n\t\tblock_bitmap = ext4_block_bitmap(sb, gdp);\n\t\tif (block_bitmap < first_block || block_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Block bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, block_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_bitmap = ext4_inode_bitmap(sb, gdp);\n\t\tif (inode_bitmap < first_block || inode_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_table = ext4_inode_table(sb, gdp);\n\t\tif (inode_table < first_block ||\n\t\t    inode_table + sbi->s_itb_per_group - 1 > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode table for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_table);\n\t\t\treturn 0;\n\t\t}\n\t\text4_lock_group(sb, i);\n\t\tif (!ext4_group_desc_csum_verify(sbi, i, gdp)) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Checksum for group %u failed (%u!=%u)\",\n\t\t\t\t i, le16_to_cpu(ext4_group_desc_csum(sbi, i,\n\t\t\t\t     gdp)), le16_to_cpu(gdp->bg_checksum));\n\t\t\tif (!(sb->s_flags & MS_RDONLY)) {\n\t\t\t\text4_unlock_group(sb, i);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, i);\n\t\tif (!flexbg_flag)\n\t\t\tfirst_block += EXT4_BLOCKS_PER_GROUP(sb);\n\t}\n\n\text4_free_blocks_count_set(sbi->s_es, ext4_count_free_blocks(sb));\n\tsbi->s_es->s_free_inodes_count =cpu_to_le32(ext4_count_free_inodes(sb));\n\treturn 1;\n}\n\n/* ext4_orphan_cleanup() walks a singly-linked list of inodes (starting at\n * the superblock) which were deleted from all directories, but held open by\n * a process at the time of a crash.  We walk the list and try to delete these\n * inodes at recovery time (only with a read-write filesystem).\n *\n * In order to keep the orphan inode chain consistent during traversal (in\n * case of crash during recovery), we link each inode into the superblock\n * orphan list_head and handle it the same way as an inode deletion during\n * normal operation (which journals the operations for us).\n *\n * We only do an iget() and an iput() on each inode, which is very safe if we\n * accidentally point at an in-use or already deleted inode.  The worst that\n * can happen in this case is that we get a \"bit already cleared\" message from\n * ext4_free_inode().  The only reason we would point at a wrong inode is if\n * e2fsck was run on this filesystem, and it must have already done the orphan\n * inode cleanup for us, so we can safely abort without any further action.\n */\nstatic void ext4_orphan_cleanup(struct super_block *sb,\n\t\t\t\tstruct ext4_super_block *es)\n{\n\tunsigned int s_flags = sb->s_flags;\n\tint nr_orphans = 0, nr_truncates = 0;\n#ifdef CONFIG_QUOTA\n\tint i;\n#endif\n\tif (!es->s_last_orphan) {\n\t\tjbd_debug(4, \"no orphan inodes to clean up\\n\");\n\t\treturn;\n\t}\n\n\tif (bdev_read_only(sb->s_bdev)) {\n\t\text4_msg(sb, KERN_ERR, \"write access \"\n\t\t\t\"unavailable, skipping orphan cleanup\");\n\t\treturn;\n\t}\n\n\tif (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) {\n\t\tif (es->s_last_orphan)\n\t\t\tjbd_debug(1, \"Errors on filesystem, \"\n\t\t\t\t  \"clearing orphan list.\\n\");\n\t\tes->s_last_orphan = 0;\n\t\tjbd_debug(1, \"Skipping orphan recovery on fs with errors.\\n\");\n\t\treturn;\n\t}\n\n\tif (s_flags & MS_RDONLY) {\n\t\text4_msg(sb, KERN_INFO, \"orphan cleanup on readonly fs\");\n\t\tsb->s_flags &= ~MS_RDONLY;\n\t}\n#ifdef CONFIG_QUOTA\n\t/* Needed for iput() to work correctly and not trash data */\n\tsb->s_flags |= MS_ACTIVE;\n\t/* Turn on quotas so that they are updated correctly */\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tif (EXT4_SB(sb)->s_qf_names[i]) {\n\t\t\tint ret = ext4_quota_on_mount(sb, i);\n\t\t\tif (ret < 0)\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\"Cannot turn on journaled \"\n\t\t\t\t\t\"quota: error %d\", ret);\n\t\t}\n\t}\n#endif\n\n\twhile (es->s_last_orphan) {\n\t\tstruct inode *inode;\n\n\t\tinode = ext4_orphan_get(sb, le32_to_cpu(es->s_last_orphan));\n\t\tif (IS_ERR(inode)) {\n\t\t\tes->s_last_orphan = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_add(&EXT4_I(inode)->i_orphan, &EXT4_SB(sb)->s_orphan);\n\t\tvfs_dq_init(inode);\n\t\tif (inode->i_nlink) {\n\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\"%s: truncating inode %lu to %lld bytes\",\n\t\t\t\t__func__, inode->i_ino, inode->i_size);\n\t\t\tjbd_debug(2, \"truncating inode %lu to %lld bytes\\n\",\n\t\t\t\t  inode->i_ino, inode->i_size);\n\t\t\text4_truncate(inode);\n\t\t\tnr_truncates++;\n\t\t} else {\n\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\"%s: deleting unreferenced inode %lu\",\n\t\t\t\t__func__, inode->i_ino);\n\t\t\tjbd_debug(2, \"deleting unreferenced inode %lu\\n\",\n\t\t\t\t  inode->i_ino);\n\t\t\tnr_orphans++;\n\t\t}\n\t\tiput(inode);  /* The delete magic happens here! */\n\t}\n\n#define PLURAL(x) (x), ((x) == 1) ? \"\" : \"s\"\n\n\tif (nr_orphans)\n\t\text4_msg(sb, KERN_INFO, \"%d orphan inode%s deleted\",\n\t\t       PLURAL(nr_orphans));\n\tif (nr_truncates)\n\t\text4_msg(sb, KERN_INFO, \"%d truncate%s cleaned up\",\n\t\t       PLURAL(nr_truncates));\n#ifdef CONFIG_QUOTA\n\t/* Turn quotas off */\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tif (sb_dqopt(sb)->files[i])\n\t\t\tvfs_quota_off(sb, i, 0);\n\t}\n#endif\n\tsb->s_flags = s_flags; /* Restore MS_RDONLY status */\n}\n\n/*\n * Maximal extent format file size.\n * Resulting logical blkno at s_maxbytes must fit in our on-disk\n * extent format containers, within a sector_t, and within i_blocks\n * in the vfs.  ext4 inode has 48 bits of i_block in fsblock units,\n * so that won't be a limiting factor.\n *\n * Note, this does *not* consider any metadata overhead for vfs i_blocks.\n */\nstatic loff_t ext4_max_size(int blkbits, int has_huge_files)\n{\n\tloff_t res;\n\tloff_t upper_limit = MAX_LFS_FILESIZE;\n\n\t/* small i_blocks in vfs inode? */\n\tif (!has_huge_files || sizeof(blkcnt_t) < sizeof(u64)) {\n\t\t/*\n\t\t * CONFIG_LBDAF is not enabled implies the inode\n\t\t * i_block represent total blocks in 512 bytes\n\t\t * 32 == size of vfs inode i_blocks * 8\n\t\t */\n\t\tupper_limit = (1LL << 32) - 1;\n\n\t\t/* total blocks in file system block size */\n\t\tupper_limit >>= (blkbits - 9);\n\t\tupper_limit <<= blkbits;\n\t}\n\n\t/* 32-bit extent-start container, ee_block */\n\tres = 1LL << 32;\n\tres <<= blkbits;\n\tres -= 1;\n\n\t/* Sanity check against vm- & vfs- imposed limits */\n\tif (res > upper_limit)\n\t\tres = upper_limit;\n\n\treturn res;\n}\n\n/*\n * Maximal bitmap file size.  There is a direct, and {,double-,triple-}indirect\n * block limit, and also a limit of (2^48 - 1) 512-byte sectors in i_blocks.\n * We need to be 1 filesystem block less than the 2^48 sector limit.\n */\nstatic loff_t ext4_max_bitmap_size(int bits, int has_huge_files)\n{\n\tloff_t res = EXT4_NDIR_BLOCKS;\n\tint meta_blocks;\n\tloff_t upper_limit;\n\t/* This is calculated to be the largest file size for a dense, block\n\t * mapped file such that the file's total number of 512-byte sectors,\n\t * including data and all indirect blocks, does not exceed (2^48 - 1).\n\t *\n\t * __u32 i_blocks_lo and _u16 i_blocks_high represent the total\n\t * number of 512-byte sectors of the file.\n\t */\n\n\tif (!has_huge_files || sizeof(blkcnt_t) < sizeof(u64)) {\n\t\t/*\n\t\t * !has_huge_files or CONFIG_LBDAF not enabled implies that\n\t\t * the inode i_block field represents total file blocks in\n\t\t * 2^32 512-byte sectors == size of vfs inode i_blocks * 8\n\t\t */\n\t\tupper_limit = (1LL << 32) - 1;\n\n\t\t/* total blocks in file system block size */\n\t\tupper_limit >>= (bits - 9);\n\n\t} else {\n\t\t/*\n\t\t * We use 48 bit ext4_inode i_blocks\n\t\t * With EXT4_HUGE_FILE_FL set the i_blocks\n\t\t * represent total number of blocks in\n\t\t * file system block size\n\t\t */\n\t\tupper_limit = (1LL << 48) - 1;\n\n\t}\n\n\t/* indirect blocks */\n\tmeta_blocks = 1;\n\t/* double indirect blocks */\n\tmeta_blocks += 1 + (1LL << (bits-2));\n\t/* tripple indirect blocks */\n\tmeta_blocks += 1 + (1LL << (bits-2)) + (1LL << (2*(bits-2)));\n\n\tupper_limit -= meta_blocks;\n\tupper_limit <<= bits;\n\n\tres += 1LL << (bits-2);\n\tres += 1LL << (2*(bits-2));\n\tres += 1LL << (3*(bits-2));\n\tres <<= bits;\n\tif (res > upper_limit)\n\t\tres = upper_limit;\n\n\tif (res > MAX_LFS_FILESIZE)\n\t\tres = MAX_LFS_FILESIZE;\n\n\treturn res;\n}\n\nstatic ext4_fsblk_t descriptor_loc(struct super_block *sb,\n\t\t\t\t   ext4_fsblk_t logical_sb_block, int nr)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_group_t bg, first_meta_bg;\n\tint has_super = 0;\n\n\tfirst_meta_bg = le32_to_cpu(sbi->s_es->s_first_meta_bg);\n\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_META_BG) ||\n\t    nr < first_meta_bg)\n\t\treturn logical_sb_block + nr + 1;\n\tbg = sbi->s_desc_per_block * nr;\n\tif (ext4_bg_has_super(sb, bg))\n\t\thas_super = 1;\n\n\treturn (has_super + ext4_group_first_block_no(sb, bg));\n}\n\n/**\n * ext4_get_stripe_size: Get the stripe size.\n * @sbi: In memory super block info\n *\n * If we have specified it via mount option, then\n * use the mount option value. If the value specified at mount time is\n * greater than the blocks per group use the super block value.\n * If the super block value is greater than blocks per group return 0.\n * Allocator needs it be less than blocks per group.\n *\n */\nstatic unsigned long ext4_get_stripe_size(struct ext4_sb_info *sbi)\n{\n\tunsigned long stride = le16_to_cpu(sbi->s_es->s_raid_stride);\n\tunsigned long stripe_width =\n\t\t\tle32_to_cpu(sbi->s_es->s_raid_stripe_width);\n\n\tif (sbi->s_stripe && sbi->s_stripe <= sbi->s_blocks_per_group)\n\t\treturn sbi->s_stripe;\n\n\tif (stripe_width <= sbi->s_blocks_per_group)\n\t\treturn stripe_width;\n\n\tif (stride <= sbi->s_blocks_per_group)\n\t\treturn stride;\n\n\treturn 0;\n}\n\n/* sysfs supprt */\n\nstruct ext4_attr {\n\tstruct attribute attr;\n\tssize_t (*show)(struct ext4_attr *, struct ext4_sb_info *, char *);\n\tssize_t (*store)(struct ext4_attr *, struct ext4_sb_info *, \n\t\t\t const char *, size_t);\n\tint offset;\n};\n\nstatic int parse_strtoul(const char *buf,\n\t\tunsigned long max, unsigned long *value)\n{\n\tchar *endp;\n\n\t*value = simple_strtoul(skip_spaces(buf), &endp, 0);\n\tendp = skip_spaces(endp);\n\tif (*endp || *value > max)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic ssize_t delayed_allocation_blocks_show(struct ext4_attr *a,\n\t\t\t\t\t      struct ext4_sb_info *sbi,\n\t\t\t\t\t      char *buf)\n{\n\treturn snprintf(buf, PAGE_SIZE, \"%llu\\n\",\n\t\t\t(s64) percpu_counter_sum(&sbi->s_dirtyblocks_counter));\n}\n\nstatic ssize_t session_write_kbytes_show(struct ext4_attr *a,\n\t\t\t\t\t struct ext4_sb_info *sbi, char *buf)\n{\n\tstruct super_block *sb = sbi->s_buddy_cache->i_sb;\n\n\treturn snprintf(buf, PAGE_SIZE, \"%lu\\n\",\n\t\t\t(part_stat_read(sb->s_bdev->bd_part, sectors[1]) -\n\t\t\t sbi->s_sectors_written_start) >> 1);\n}\n\nstatic ssize_t lifetime_write_kbytes_show(struct ext4_attr *a,\n\t\t\t\t\t  struct ext4_sb_info *sbi, char *buf)\n{\n\tstruct super_block *sb = sbi->s_buddy_cache->i_sb;\n\n\treturn snprintf(buf, PAGE_SIZE, \"%llu\\n\",\n\t\t\t(unsigned long long)(sbi->s_kbytes_written +\n\t\t\t((part_stat_read(sb->s_bdev->bd_part, sectors[1]) -\n\t\t\t  EXT4_SB(sb)->s_sectors_written_start) >> 1)));\n}\n\nstatic ssize_t inode_readahead_blks_store(struct ext4_attr *a,\n\t\t\t\t\t  struct ext4_sb_info *sbi,\n\t\t\t\t\t  const char *buf, size_t count)\n{\n\tunsigned long t;\n\n\tif (parse_strtoul(buf, 0x40000000, &t))\n\t\treturn -EINVAL;\n\n\tif (!is_power_of_2(t))\n\t\treturn -EINVAL;\n\n\tsbi->s_inode_readahead_blks = t;\n\treturn count;\n}\n\nstatic ssize_t sbi_ui_show(struct ext4_attr *a,\n\t\t\t   struct ext4_sb_info *sbi, char *buf)\n{\n\tunsigned int *ui = (unsigned int *) (((char *) sbi) + a->offset);\n\n\treturn snprintf(buf, PAGE_SIZE, \"%u\\n\", *ui);\n}\n\nstatic ssize_t sbi_ui_store(struct ext4_attr *a,\n\t\t\t    struct ext4_sb_info *sbi,\n\t\t\t    const char *buf, size_t count)\n{\n\tunsigned int *ui = (unsigned int *) (((char *) sbi) + a->offset);\n\tunsigned long t;\n\n\tif (parse_strtoul(buf, 0xffffffff, &t))\n\t\treturn -EINVAL;\n\t*ui = t;\n\treturn count;\n}\n\n#define EXT4_ATTR_OFFSET(_name,_mode,_show,_store,_elname) \\\nstatic struct ext4_attr ext4_attr_##_name = {\t\t\t\\\n\t.attr = {.name = __stringify(_name), .mode = _mode },\t\\\n\t.show\t= _show,\t\t\t\t\t\\\n\t.store\t= _store,\t\t\t\t\t\\\n\t.offset = offsetof(struct ext4_sb_info, _elname),\t\\\n}\n#define EXT4_ATTR(name, mode, show, store) \\\nstatic struct ext4_attr ext4_attr_##name = __ATTR(name, mode, show, store)\n\n#define EXT4_RO_ATTR(name) EXT4_ATTR(name, 0444, name##_show, NULL)\n#define EXT4_RW_ATTR(name) EXT4_ATTR(name, 0644, name##_show, name##_store)\n#define EXT4_RW_ATTR_SBI_UI(name, elname)\t\\\n\tEXT4_ATTR_OFFSET(name, 0644, sbi_ui_show, sbi_ui_store, elname)\n#define ATTR_LIST(name) &ext4_attr_##name.attr\n\nEXT4_RO_ATTR(delayed_allocation_blocks);\nEXT4_RO_ATTR(session_write_kbytes);\nEXT4_RO_ATTR(lifetime_write_kbytes);\nEXT4_ATTR_OFFSET(inode_readahead_blks, 0644, sbi_ui_show,\n\t\t inode_readahead_blks_store, s_inode_readahead_blks);\nEXT4_RW_ATTR_SBI_UI(inode_goal, s_inode_goal);\nEXT4_RW_ATTR_SBI_UI(mb_stats, s_mb_stats);\nEXT4_RW_ATTR_SBI_UI(mb_max_to_scan, s_mb_max_to_scan);\nEXT4_RW_ATTR_SBI_UI(mb_min_to_scan, s_mb_min_to_scan);\nEXT4_RW_ATTR_SBI_UI(mb_order2_req, s_mb_order2_reqs);\nEXT4_RW_ATTR_SBI_UI(mb_stream_req, s_mb_stream_request);\nEXT4_RW_ATTR_SBI_UI(mb_group_prealloc, s_mb_group_prealloc);\nEXT4_RW_ATTR_SBI_UI(max_writeback_mb_bump, s_max_writeback_mb_bump);\n\nstatic struct attribute *ext4_attrs[] = {\n\tATTR_LIST(delayed_allocation_blocks),\n\tATTR_LIST(session_write_kbytes),\n\tATTR_LIST(lifetime_write_kbytes),\n\tATTR_LIST(inode_readahead_blks),\n\tATTR_LIST(inode_goal),\n\tATTR_LIST(mb_stats),\n\tATTR_LIST(mb_max_to_scan),\n\tATTR_LIST(mb_min_to_scan),\n\tATTR_LIST(mb_order2_req),\n\tATTR_LIST(mb_stream_req),\n\tATTR_LIST(mb_group_prealloc),\n\tATTR_LIST(max_writeback_mb_bump),\n\tNULL,\n};\n\nstatic ssize_t ext4_attr_show(struct kobject *kobj,\n\t\t\t      struct attribute *attr, char *buf)\n{\n\tstruct ext4_sb_info *sbi = container_of(kobj, struct ext4_sb_info,\n\t\t\t\t\t\ts_kobj);\n\tstruct ext4_attr *a = container_of(attr, struct ext4_attr, attr);\n\n\treturn a->show ? a->show(a, sbi, buf) : 0;\n}\n\nstatic ssize_t ext4_attr_store(struct kobject *kobj,\n\t\t\t       struct attribute *attr,\n\t\t\t       const char *buf, size_t len)\n{\n\tstruct ext4_sb_info *sbi = container_of(kobj, struct ext4_sb_info,\n\t\t\t\t\t\ts_kobj);\n\tstruct ext4_attr *a = container_of(attr, struct ext4_attr, attr);\n\n\treturn a->store ? a->store(a, sbi, buf, len) : 0;\n}\n\nstatic void ext4_sb_release(struct kobject *kobj)\n{\n\tstruct ext4_sb_info *sbi = container_of(kobj, struct ext4_sb_info,\n\t\t\t\t\t\ts_kobj);\n\tcomplete(&sbi->s_kobj_unregister);\n}\n\n\nstatic struct sysfs_ops ext4_attr_ops = {\n\t.show\t= ext4_attr_show,\n\t.store\t= ext4_attr_store,\n};\n\nstatic struct kobj_type ext4_ktype = {\n\t.default_attrs\t= ext4_attrs,\n\t.sysfs_ops\t= &ext4_attr_ops,\n\t.release\t= ext4_sb_release,\n};\n\n/*\n * Check whether this filesystem can be mounted based on\n * the features present and the RDONLY/RDWR mount requested.\n * Returns 1 if this filesystem can be mounted as requested,\n * 0 if it cannot be.\n */\nstatic int ext4_feature_set_ok(struct super_block *sb, int readonly)\n{\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, ~EXT4_FEATURE_INCOMPAT_SUPP)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Couldn't mount because of \"\n\t\t\t\"unsupported optional features (%x)\",\n\t\t\t(le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_incompat) &\n\t\t\t~EXT4_FEATURE_INCOMPAT_SUPP));\n\t\treturn 0;\n\t}\n\n\tif (readonly)\n\t\treturn 1;\n\n\t/* Check that feature set is OK for a read-write mount */\n\tif (EXT4_HAS_RO_COMPAT_FEATURE(sb, ~EXT4_FEATURE_RO_COMPAT_SUPP)) {\n\t\text4_msg(sb, KERN_ERR, \"couldn't mount RDWR because of \"\n\t\t\t \"unsupported optional features (%x)\",\n\t\t\t (le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_ro_compat) &\n\t\t\t\t~EXT4_FEATURE_RO_COMPAT_SUPP));\n\t\treturn 0;\n\t}\n\t/*\n\t * Large file size enabled file system can only be mounted\n\t * read-write on 32-bit systems if kernel is built with CONFIG_LBDAF\n\t */\n\tif (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_HUGE_FILE)) {\n\t\tif (sizeof(blkcnt_t) < sizeof(u64)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Filesystem with huge files \"\n\t\t\t\t \"cannot be mounted RDWR without \"\n\t\t\t\t \"CONFIG_LBDAF\");\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int ext4_fill_super(struct super_block *sb, void *data, int silent)\n\t\t\t\t__releases(kernel_lock)\n\t\t\t\t__acquires(kernel_lock)\n{\n\tstruct buffer_head *bh;\n\tstruct ext4_super_block *es = NULL;\n\tstruct ext4_sb_info *sbi;\n\text4_fsblk_t block;\n\text4_fsblk_t sb_block = get_sb_block(&data);\n\text4_fsblk_t logical_sb_block;\n\tunsigned long offset = 0;\n\tunsigned long journal_devnum = 0;\n\tunsigned long def_mount_opts;\n\tstruct inode *root;\n\tchar *cp;\n\tconst char *descr;\n\tint ret = -EINVAL;\n\tint blocksize;\n\tunsigned int db_count;\n\tunsigned int i;\n\tint needs_recovery, has_huge_files;\n\t__u64 blocks_count;\n\tint err;\n\tunsigned int journal_ioprio = DEFAULT_JOURNAL_IOPRIO;\n\n\tsbi = kzalloc(sizeof(*sbi), GFP_KERNEL);\n\tif (!sbi)\n\t\treturn -ENOMEM;\n\n\tsbi->s_blockgroup_lock =\n\t\tkzalloc(sizeof(struct blockgroup_lock), GFP_KERNEL);\n\tif (!sbi->s_blockgroup_lock) {\n\t\tkfree(sbi);\n\t\treturn -ENOMEM;\n\t}\n\tsb->s_fs_info = sbi;\n\tsbi->s_mount_opt = 0;\n\tsbi->s_resuid = EXT4_DEF_RESUID;\n\tsbi->s_resgid = EXT4_DEF_RESGID;\n\tsbi->s_inode_readahead_blks = EXT4_DEF_INODE_READAHEAD_BLKS;\n\tsbi->s_sb_block = sb_block;\n\tsbi->s_sectors_written_start = part_stat_read(sb->s_bdev->bd_part,\n\t\t\t\t\t\t      sectors[1]);\n\n\tunlock_kernel();\n\n\t/* Cleanup superblock name */\n\tfor (cp = sb->s_id; (cp = strchr(cp, '/'));)\n\t\t*cp = '!';\n\n\tblocksize = sb_min_blocksize(sb, EXT4_MIN_BLOCK_SIZE);\n\tif (!blocksize) {\n\t\text4_msg(sb, KERN_ERR, \"unable to set blocksize\");\n\t\tgoto out_fail;\n\t}\n\n\t/*\n\t * The ext4 superblock will not be buffer aligned for other than 1kB\n\t * block sizes.  We need to calculate the offset from buffer start.\n\t */\n\tif (blocksize != EXT4_MIN_BLOCK_SIZE) {\n\t\tlogical_sb_block = sb_block * EXT4_MIN_BLOCK_SIZE;\n\t\toffset = do_div(logical_sb_block, blocksize);\n\t} else {\n\t\tlogical_sb_block = sb_block;\n\t}\n\n\tif (!(bh = sb_bread(sb, logical_sb_block))) {\n\t\text4_msg(sb, KERN_ERR, \"unable to read superblock\");\n\t\tgoto out_fail;\n\t}\n\t/*\n\t * Note: s_es must be initialized as soon as possible because\n\t *       some ext4 macro-instructions depend on its value\n\t */\n\tes = (struct ext4_super_block *) (((char *)bh->b_data) + offset);\n\tsbi->s_es = es;\n\tsb->s_magic = le16_to_cpu(es->s_magic);\n\tif (sb->s_magic != EXT4_SUPER_MAGIC)\n\t\tgoto cantfind_ext4;\n\tsbi->s_kbytes_written = le64_to_cpu(es->s_kbytes_written);\n\n\t/* Set defaults before we parse the mount options */\n\tdef_mount_opts = le32_to_cpu(es->s_default_mount_opts);\n\tif (def_mount_opts & EXT4_DEFM_DEBUG)\n\t\tset_opt(sbi->s_mount_opt, DEBUG);\n\tif (def_mount_opts & EXT4_DEFM_BSDGROUPS) {\n\t\text4_msg(sb, KERN_WARNING, deprecated_msg, \"bsdgroups\",\n\t\t\t\"2.6.38\");\n\t\tset_opt(sbi->s_mount_opt, GRPID);\n\t}\n\tif (def_mount_opts & EXT4_DEFM_UID16)\n\t\tset_opt(sbi->s_mount_opt, NO_UID32);\n#ifdef CONFIG_EXT4_FS_XATTR\n\tif (def_mount_opts & EXT4_DEFM_XATTR_USER)\n\t\tset_opt(sbi->s_mount_opt, XATTR_USER);\n#endif\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\tif (def_mount_opts & EXT4_DEFM_ACL)\n\t\tset_opt(sbi->s_mount_opt, POSIX_ACL);\n#endif\n\tif ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_DATA)\n\t\tset_opt(sbi->s_mount_opt, JOURNAL_DATA);\n\telse if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_ORDERED)\n\t\tset_opt(sbi->s_mount_opt, ORDERED_DATA);\n\telse if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_WBACK)\n\t\tset_opt(sbi->s_mount_opt, WRITEBACK_DATA);\n\n\tif (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_PANIC)\n\t\tset_opt(sbi->s_mount_opt, ERRORS_PANIC);\n\telse if (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_CONTINUE)\n\t\tset_opt(sbi->s_mount_opt, ERRORS_CONT);\n\telse\n\t\tset_opt(sbi->s_mount_opt, ERRORS_RO);\n\n\tsbi->s_resuid = le16_to_cpu(es->s_def_resuid);\n\tsbi->s_resgid = le16_to_cpu(es->s_def_resgid);\n\tsbi->s_commit_interval = JBD2_DEFAULT_MAX_COMMIT_AGE * HZ;\n\tsbi->s_min_batch_time = EXT4_DEF_MIN_BATCH_TIME;\n\tsbi->s_max_batch_time = EXT4_DEF_MAX_BATCH_TIME;\n\n\tset_opt(sbi->s_mount_opt, BARRIER);\n\n\t/*\n\t * enable delayed allocation by default\n\t * Use -o nodelalloc to turn it off\n\t */\n\tset_opt(sbi->s_mount_opt, DELALLOC);\n\n\tif (!parse_options((char *) data, sb, &journal_devnum,\n\t\t\t   &journal_ioprio, NULL, 0))\n\t\tgoto failed_mount;\n\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sb, POSIX_ACL) ? MS_POSIXACL : 0);\n\n\tif (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV &&\n\t    (EXT4_HAS_COMPAT_FEATURE(sb, ~0U) ||\n\t     EXT4_HAS_RO_COMPAT_FEATURE(sb, ~0U) ||\n\t     EXT4_HAS_INCOMPAT_FEATURE(sb, ~0U)))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t       \"feature flags set on rev 0 fs, \"\n\t\t       \"running e2fsck is recommended\");\n\n\t/*\n\t * Check feature flags regardless of the revision level, since we\n\t * previously didn't change the revision level when setting the flags,\n\t * so there is a chance incompat flags are set on a rev 0 filesystem.\n\t */\n\tif (!ext4_feature_set_ok(sb, (sb->s_flags & MS_RDONLY)))\n\t\tgoto failed_mount;\n\n\tblocksize = BLOCK_SIZE << le32_to_cpu(es->s_log_block_size);\n\n\tif (blocksize < EXT4_MIN_BLOCK_SIZE ||\n\t    blocksize > EXT4_MAX_BLOCK_SIZE) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t       \"Unsupported filesystem blocksize %d\", blocksize);\n\t\tgoto failed_mount;\n\t}\n\n\tif (sb->s_blocksize != blocksize) {\n\t\t/* Validate the filesystem blocksize */\n\t\tif (!sb_set_blocksize(sb, blocksize)) {\n\t\t\text4_msg(sb, KERN_ERR, \"bad block size %d\",\n\t\t\t\t\tblocksize);\n\t\t\tgoto failed_mount;\n\t\t}\n\n\t\tbrelse(bh);\n\t\tlogical_sb_block = sb_block * EXT4_MIN_BLOCK_SIZE;\n\t\toffset = do_div(logical_sb_block, blocksize);\n\t\tbh = sb_bread(sb, logical_sb_block);\n\t\tif (!bh) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"Can't read superblock on 2nd try\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tes = (struct ext4_super_block *)(((char *)bh->b_data) + offset);\n\t\tsbi->s_es = es;\n\t\tif (es->s_magic != cpu_to_le16(EXT4_SUPER_MAGIC)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"Magic mismatch, very weird!\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\thas_huge_files = EXT4_HAS_RO_COMPAT_FEATURE(sb,\n\t\t\t\tEXT4_FEATURE_RO_COMPAT_HUGE_FILE);\n\tsbi->s_bitmap_maxbytes = ext4_max_bitmap_size(sb->s_blocksize_bits,\n\t\t\t\t\t\t      has_huge_files);\n\tsb->s_maxbytes = ext4_max_size(sb->s_blocksize_bits, has_huge_files);\n\n\tif (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV) {\n\t\tsbi->s_inode_size = EXT4_GOOD_OLD_INODE_SIZE;\n\t\tsbi->s_first_ino = EXT4_GOOD_OLD_FIRST_INO;\n\t} else {\n\t\tsbi->s_inode_size = le16_to_cpu(es->s_inode_size);\n\t\tsbi->s_first_ino = le32_to_cpu(es->s_first_ino);\n\t\tif ((sbi->s_inode_size < EXT4_GOOD_OLD_INODE_SIZE) ||\n\t\t    (!is_power_of_2(sbi->s_inode_size)) ||\n\t\t    (sbi->s_inode_size > blocksize)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unsupported inode size: %d\",\n\t\t\t       sbi->s_inode_size);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE)\n\t\t\tsb->s_time_gran = 1 << (EXT4_EPOCH_BITS - 2);\n\t}\n\n\tsbi->s_desc_size = le16_to_cpu(es->s_desc_size);\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT)) {\n\t\tif (sbi->s_desc_size < EXT4_MIN_DESC_SIZE_64BIT ||\n\t\t    sbi->s_desc_size > EXT4_MAX_DESC_SIZE ||\n\t\t    !is_power_of_2(sbi->s_desc_size)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unsupported descriptor size %lu\",\n\t\t\t       sbi->s_desc_size);\n\t\t\tgoto failed_mount;\n\t\t}\n\t} else\n\t\tsbi->s_desc_size = EXT4_MIN_DESC_SIZE;\n\n\tsbi->s_blocks_per_group = le32_to_cpu(es->s_blocks_per_group);\n\tsbi->s_inodes_per_group = le32_to_cpu(es->s_inodes_per_group);\n\tif (EXT4_INODE_SIZE(sb) == 0 || EXT4_INODES_PER_GROUP(sb) == 0)\n\t\tgoto cantfind_ext4;\n\n\tsbi->s_inodes_per_block = blocksize / EXT4_INODE_SIZE(sb);\n\tif (sbi->s_inodes_per_block == 0)\n\t\tgoto cantfind_ext4;\n\tsbi->s_itb_per_group = sbi->s_inodes_per_group /\n\t\t\t\t\tsbi->s_inodes_per_block;\n\tsbi->s_desc_per_block = blocksize / EXT4_DESC_SIZE(sb);\n\tsbi->s_sbh = bh;\n\tsbi->s_mount_state = le16_to_cpu(es->s_state);\n\tsbi->s_addr_per_block_bits = ilog2(EXT4_ADDR_PER_BLOCK(sb));\n\tsbi->s_desc_per_block_bits = ilog2(EXT4_DESC_PER_BLOCK(sb));\n\n\tfor (i = 0; i < 4; i++)\n\t\tsbi->s_hash_seed[i] = le32_to_cpu(es->s_hash_seed[i]);\n\tsbi->s_def_hash_version = es->s_def_hash_version;\n\ti = le32_to_cpu(es->s_flags);\n\tif (i & EXT2_FLAGS_UNSIGNED_HASH)\n\t\tsbi->s_hash_unsigned = 3;\n\telse if ((i & EXT2_FLAGS_SIGNED_HASH) == 0) {\n#ifdef __CHAR_UNSIGNED__\n\t\tes->s_flags |= cpu_to_le32(EXT2_FLAGS_UNSIGNED_HASH);\n\t\tsbi->s_hash_unsigned = 3;\n#else\n\t\tes->s_flags |= cpu_to_le32(EXT2_FLAGS_SIGNED_HASH);\n#endif\n\t\tsb->s_dirt = 1;\n\t}\n\n\tif (sbi->s_blocks_per_group > blocksize * 8) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t       \"#blocks per group too big: %lu\",\n\t\t       sbi->s_blocks_per_group);\n\t\tgoto failed_mount;\n\t}\n\tif (sbi->s_inodes_per_group > blocksize * 8) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t       \"#inodes per group too big: %lu\",\n\t\t       sbi->s_inodes_per_group);\n\t\tgoto failed_mount;\n\t}\n\n\t/*\n\t * Test whether we have more sectors than will fit in sector_t,\n\t * and whether the max offset is addressable by the page cache.\n\t */\n\tif ((ext4_blocks_count(es) >\n\t     (sector_t)(~0ULL) >> (sb->s_blocksize_bits - 9)) ||\n\t    (ext4_blocks_count(es) >\n\t     (pgoff_t)(~0ULL) >> (PAGE_CACHE_SHIFT - sb->s_blocksize_bits))) {\n\t\text4_msg(sb, KERN_ERR, \"filesystem\"\n\t\t\t \" too large to mount safely on this system\");\n\t\tif (sizeof(sector_t) < 8)\n\t\t\text4_msg(sb, KERN_WARNING, \"CONFIG_LBDAF not enabled\");\n\t\tret = -EFBIG;\n\t\tgoto failed_mount;\n\t}\n\n\tif (EXT4_BLOCKS_PER_GROUP(sb) == 0)\n\t\tgoto cantfind_ext4;\n\n\t/* check blocks count against device size */\n\tblocks_count = sb->s_bdev->bd_inode->i_size >> sb->s_blocksize_bits;\n\tif (blocks_count && ext4_blocks_count(es) > blocks_count) {\n\t\text4_msg(sb, KERN_WARNING, \"bad geometry: block count %llu \"\n\t\t       \"exceeds size of device (%llu blocks)\",\n\t\t       ext4_blocks_count(es), blocks_count);\n\t\tgoto failed_mount;\n\t}\n\n\t/*\n\t * It makes no sense for the first data block to be beyond the end\n\t * of the filesystem.\n\t */\n\tif (le32_to_cpu(es->s_first_data_block) >= ext4_blocks_count(es)) {\n                ext4_msg(sb, KERN_WARNING, \"bad geometry: first data\"\n\t\t\t \"block %u is beyond end of filesystem (%llu)\",\n\t\t\t le32_to_cpu(es->s_first_data_block),\n\t\t\t ext4_blocks_count(es));\n\t\tgoto failed_mount;\n\t}\n\tblocks_count = (ext4_blocks_count(es) -\n\t\t\tle32_to_cpu(es->s_first_data_block) +\n\t\t\tEXT4_BLOCKS_PER_GROUP(sb) - 1);\n\tdo_div(blocks_count, EXT4_BLOCKS_PER_GROUP(sb));\n\tif (blocks_count > ((uint64_t)1<<32) - EXT4_DESC_PER_BLOCK(sb)) {\n\t\text4_msg(sb, KERN_WARNING, \"groups count too large: %u \"\n\t\t       \"(block count %llu, first data block %u, \"\n\t\t       \"blocks per group %lu)\", sbi->s_groups_count,\n\t\t       ext4_blocks_count(es),\n\t\t       le32_to_cpu(es->s_first_data_block),\n\t\t       EXT4_BLOCKS_PER_GROUP(sb));\n\t\tgoto failed_mount;\n\t}\n\tsbi->s_groups_count = blocks_count;\n\tsbi->s_blockfile_groups = min_t(ext4_group_t, sbi->s_groups_count,\n\t\t\t(EXT4_MAX_BLOCK_FILE_PHYS / EXT4_BLOCKS_PER_GROUP(sb)));\n\tdb_count = (sbi->s_groups_count + EXT4_DESC_PER_BLOCK(sb) - 1) /\n\t\t   EXT4_DESC_PER_BLOCK(sb);\n\tsbi->s_group_desc = kmalloc(db_count * sizeof(struct buffer_head *),\n\t\t\t\t    GFP_KERNEL);\n\tif (sbi->s_group_desc == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory\");\n\t\tgoto failed_mount;\n\t}\n\n#ifdef CONFIG_PROC_FS\n\tif (ext4_proc_root)\n\t\tsbi->s_proc = proc_mkdir(sb->s_id, ext4_proc_root);\n#endif\n\n\tbgl_lock_init(sbi->s_blockgroup_lock);\n\n\tfor (i = 0; i < db_count; i++) {\n\t\tblock = descriptor_loc(sb, logical_sb_block, i);\n\t\tsbi->s_group_desc[i] = sb_bread(sb, block);\n\t\tif (!sbi->s_group_desc[i]) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"can't read group descriptor %d\", i);\n\t\t\tdb_count = i;\n\t\t\tgoto failed_mount2;\n\t\t}\n\t}\n\tif (!ext4_check_descriptors(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"group descriptors corrupted!\");\n\t\tgoto failed_mount2;\n\t}\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG))\n\t\tif (!ext4_fill_flex_info(sb)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unable to initialize \"\n\t\t\t       \"flex_bg meta info!\");\n\t\t\tgoto failed_mount2;\n\t\t}\n\n\tsbi->s_gdb_count = db_count;\n\tget_random_bytes(&sbi->s_next_generation, sizeof(u32));\n\tspin_lock_init(&sbi->s_next_gen_lock);\n\n\terr = percpu_counter_init(&sbi->s_freeblocks_counter,\n\t\t\text4_count_free_blocks(sb));\n\tif (!err) {\n\t\terr = percpu_counter_init(&sbi->s_freeinodes_counter,\n\t\t\t\text4_count_free_inodes(sb));\n\t}\n\tif (!err) {\n\t\terr = percpu_counter_init(&sbi->s_dirs_counter,\n\t\t\t\text4_count_dirs(sb));\n\t}\n\tif (!err) {\n\t\terr = percpu_counter_init(&sbi->s_dirtyblocks_counter, 0);\n\t}\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"insufficient memory\");\n\t\tgoto failed_mount3;\n\t}\n\n\tsbi->s_stripe = ext4_get_stripe_size(sbi);\n\tsbi->s_max_writeback_mb_bump = 128;\n\n\t/*\n\t * set up enough so that it can read an inode\n\t */\n\tif (!test_opt(sb, NOLOAD) &&\n\t    EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL))\n\t\tsb->s_op = &ext4_sops;\n\telse\n\t\tsb->s_op = &ext4_nojournal_sops;\n\tsb->s_export_op = &ext4_export_ops;\n\tsb->s_xattr = ext4_xattr_handlers;\n#ifdef CONFIG_QUOTA\n\tsb->s_qcop = &ext4_qctl_operations;\n\tsb->dq_op = &ext4_quota_operations;\n#endif\n\tINIT_LIST_HEAD(&sbi->s_orphan); /* unlinked but open files */\n\tmutex_init(&sbi->s_orphan_lock);\n\tmutex_init(&sbi->s_resize_lock);\n\n\tsb->s_root = NULL;\n\n\tneeds_recovery = (es->s_last_orphan != 0 ||\n\t\t\t  EXT4_HAS_INCOMPAT_FEATURE(sb,\n\t\t\t\t    EXT4_FEATURE_INCOMPAT_RECOVER));\n\n\t/*\n\t * The first inode we look at is the journal inode.  Don't try\n\t * root first: it may be modified in the journal!\n\t */\n\tif (!test_opt(sb, NOLOAD) &&\n\t    EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL)) {\n\t\tif (ext4_load_journal(sb, es, journal_devnum))\n\t\t\tgoto failed_mount3;\n\t} else if (test_opt(sb, NOLOAD) && !(sb->s_flags & MS_RDONLY) &&\n\t      EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER)) {\n\t\text4_msg(sb, KERN_ERR, \"required journal recovery \"\n\t\t       \"suppressed and not mounted read-only\");\n\t\tgoto failed_mount4;\n\t} else {\n\t\tclear_opt(sbi->s_mount_opt, DATA_FLAGS);\n\t\tset_opt(sbi->s_mount_opt, WRITEBACK_DATA);\n\t\tsbi->s_journal = NULL;\n\t\tneeds_recovery = 0;\n\t\tgoto no_journal;\n\t}\n\n\tif (ext4_blocks_count(es) > 0xffffffffULL &&\n\t    !jbd2_journal_set_features(EXT4_SB(sb)->s_journal, 0, 0,\n\t\t\t\t       JBD2_FEATURE_INCOMPAT_64BIT)) {\n\t\text4_msg(sb, KERN_ERR, \"Failed to set 64-bit journal feature\");\n\t\tgoto failed_mount4;\n\t}\n\n\tif (test_opt(sb, JOURNAL_ASYNC_COMMIT)) {\n\t\tjbd2_journal_set_features(sbi->s_journal,\n\t\t\t\tJBD2_FEATURE_COMPAT_CHECKSUM, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT);\n\t} else if (test_opt(sb, JOURNAL_CHECKSUM)) {\n\t\tjbd2_journal_set_features(sbi->s_journal,\n\t\t\t\tJBD2_FEATURE_COMPAT_CHECKSUM, 0, 0);\n\t\tjbd2_journal_clear_features(sbi->s_journal, 0, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT);\n\t} else {\n\t\tjbd2_journal_clear_features(sbi->s_journal,\n\t\t\t\tJBD2_FEATURE_COMPAT_CHECKSUM, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT);\n\t}\n\n\t/* We have now updated the journal if required, so we can\n\t * validate the data journaling mode. */\n\tswitch (test_opt(sb, DATA_FLAGS)) {\n\tcase 0:\n\t\t/* No mode set, assume a default based on the journal\n\t\t * capabilities: ORDERED_DATA if the journal can\n\t\t * cope, else JOURNAL_DATA\n\t\t */\n\t\tif (jbd2_journal_check_available_features\n\t\t    (sbi->s_journal, 0, 0, JBD2_FEATURE_INCOMPAT_REVOKE))\n\t\t\tset_opt(sbi->s_mount_opt, ORDERED_DATA);\n\t\telse\n\t\t\tset_opt(sbi->s_mount_opt, JOURNAL_DATA);\n\t\tbreak;\n\n\tcase EXT4_MOUNT_ORDERED_DATA:\n\tcase EXT4_MOUNT_WRITEBACK_DATA:\n\t\tif (!jbd2_journal_check_available_features\n\t\t    (sbi->s_journal, 0, 0, JBD2_FEATURE_INCOMPAT_REVOKE)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Journal does not support \"\n\t\t\t       \"requested data journaling mode\");\n\t\t\tgoto failed_mount4;\n\t\t}\n\tdefault:\n\t\tbreak;\n\t}\n\tset_task_ioprio(sbi->s_journal->j_task, journal_ioprio);\n\nno_journal:\n\n\tif (test_opt(sb, NOBH)) {\n\t\tif (!(test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_WRITEBACK_DATA)) {\n\t\t\text4_msg(sb, KERN_WARNING, \"Ignoring nobh option - \"\n\t\t\t\t\"its supported only with writeback mode\");\n\t\t\tclear_opt(sbi->s_mount_opt, NOBH);\n\t\t}\n\t}\n\tEXT4_SB(sb)->dio_unwritten_wq = create_workqueue(\"ext4-dio-unwritten\");\n\tif (!EXT4_SB(sb)->dio_unwritten_wq) {\n\t\tprintk(KERN_ERR \"EXT4-fs: failed to create DIO workqueue\\n\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\t/*\n\t * The jbd2_journal_load will have done any necessary log recovery,\n\t * so we can safely mount the rest of the filesystem now.\n\t */\n\n\troot = ext4_iget(sb, EXT4_ROOT_INO);\n\tif (IS_ERR(root)) {\n\t\text4_msg(sb, KERN_ERR, \"get root inode failed\");\n\t\tret = PTR_ERR(root);\n\t\tgoto failed_mount4;\n\t}\n\tif (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) {\n\t\tiput(root);\n\t\text4_msg(sb, KERN_ERR, \"corrupt root inode, run e2fsck\");\n\t\tgoto failed_mount4;\n\t}\n\tsb->s_root = d_alloc_root(root);\n\tif (!sb->s_root) {\n\t\text4_msg(sb, KERN_ERR, \"get root dentry failed\");\n\t\tiput(root);\n\t\tret = -ENOMEM;\n\t\tgoto failed_mount4;\n\t}\n\n\text4_setup_super(sb, es, sb->s_flags & MS_RDONLY);\n\n\t/* determine the minimum size of new large inodes, if present */\n\tif (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\t     EXT4_GOOD_OLD_INODE_SIZE;\n\t\tif (EXT4_HAS_RO_COMPAT_FEATURE(sb,\n\t\t\t\t       EXT4_FEATURE_RO_COMPAT_EXTRA_ISIZE)) {\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_want_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_want_extra_isize);\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_min_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_min_extra_isize);\n\t\t}\n\t}\n\t/* Check if enough inode space is available */\n\tif (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >\n\t\t\t\t\t\t\tsbi->s_inode_size) {\n\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\t       EXT4_GOOD_OLD_INODE_SIZE;\n\t\text4_msg(sb, KERN_INFO, \"required extra inode space not\"\n\t\t\t \"available\");\n\t}\n\n\tif (test_opt(sb, DELALLOC) &&\n\t    (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)) {\n\t\text4_msg(sb, KERN_WARNING, \"Ignoring delalloc option - \"\n\t\t\t \"requested data journaling mode\");\n\t\tclear_opt(sbi->s_mount_opt, DELALLOC);\n\t}\n\n\terr = ext4_setup_system_zone(sb);\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"failed to initialize system \"\n\t\t\t \"zone (%d)\\n\", err);\n\t\tgoto failed_mount4;\n\t}\n\n\text4_ext_init(sb);\n\terr = ext4_mb_init(sb, needs_recovery);\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"failed to initalize mballoc (%d)\",\n\t\t\t err);\n\t\tgoto failed_mount4;\n\t}\n\n\tsbi->s_kobj.kset = ext4_kset;\n\tinit_completion(&sbi->s_kobj_unregister);\n\terr = kobject_init_and_add(&sbi->s_kobj, &ext4_ktype, NULL,\n\t\t\t\t   \"%s\", sb->s_id);\n\tif (err) {\n\t\text4_mb_release(sb);\n\t\text4_ext_release(sb);\n\t\tgoto failed_mount4;\n\t};\n\n\tEXT4_SB(sb)->s_mount_state |= EXT4_ORPHAN_FS;\n\text4_orphan_cleanup(sb, es);\n\tEXT4_SB(sb)->s_mount_state &= ~EXT4_ORPHAN_FS;\n\tif (needs_recovery) {\n\t\text4_msg(sb, KERN_INFO, \"recovery complete\");\n\t\text4_mark_recovery_complete(sb, es);\n\t}\n\tif (EXT4_SB(sb)->s_journal) {\n\t\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)\n\t\t\tdescr = \" journalled data mode\";\n\t\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA)\n\t\t\tdescr = \" ordered data mode\";\n\t\telse\n\t\t\tdescr = \" writeback data mode\";\n\t} else\n\t\tdescr = \"out journal\";\n\n\text4_msg(sb, KERN_INFO, \"mounted filesystem with%s\", descr);\n\n\tlock_kernel();\n\treturn 0;\n\ncantfind_ext4:\n\tif (!silent)\n\t\text4_msg(sb, KERN_ERR, \"VFS: Can't find ext4 filesystem\");\n\tgoto failed_mount;\n\nfailed_mount4:\n\text4_msg(sb, KERN_ERR, \"mount failed\");\n\tdestroy_workqueue(EXT4_SB(sb)->dio_unwritten_wq);\nfailed_mount_wq:\n\text4_release_system_zone(sb);\n\tif (sbi->s_journal) {\n\t\tjbd2_journal_destroy(sbi->s_journal);\n\t\tsbi->s_journal = NULL;\n\t}\nfailed_mount3:\n\tif (sbi->s_flex_groups) {\n\t\tif (is_vmalloc_addr(sbi->s_flex_groups))\n\t\t\tvfree(sbi->s_flex_groups);\n\t\telse\n\t\t\tkfree(sbi->s_flex_groups);\n\t}\n\tpercpu_counter_destroy(&sbi->s_freeblocks_counter);\n\tpercpu_counter_destroy(&sbi->s_freeinodes_counter);\n\tpercpu_counter_destroy(&sbi->s_dirs_counter);\n\tpercpu_counter_destroy(&sbi->s_dirtyblocks_counter);\nfailed_mount2:\n\tfor (i = 0; i < db_count; i++)\n\t\tbrelse(sbi->s_group_desc[i]);\n\tkfree(sbi->s_group_desc);\nfailed_mount:\n\tif (sbi->s_proc) {\n\t\tremove_proc_entry(sb->s_id, ext4_proc_root);\n\t}\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(sbi->s_qf_names[i]);\n#endif\n\text4_blkdev_remove(sbi);\n\tbrelse(bh);\nout_fail:\n\tsb->s_fs_info = NULL;\n\tkfree(sbi->s_blockgroup_lock);\n\tkfree(sbi);\n\tlock_kernel();\n\treturn ret;\n}\n\n/*\n * Setup any per-fs journal parameters now.  We'll do this both on\n * initial mount, once the journal has been initialised but before we've\n * done any recovery; and again on any subsequent remount.\n */\nstatic void ext4_init_journal_params(struct super_block *sb, journal_t *journal)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tjournal->j_commit_interval = sbi->s_commit_interval;\n\tjournal->j_min_batch_time = sbi->s_min_batch_time;\n\tjournal->j_max_batch_time = sbi->s_max_batch_time;\n\n\tspin_lock(&journal->j_state_lock);\n\tif (test_opt(sb, BARRIER))\n\t\tjournal->j_flags |= JBD2_BARRIER;\n\telse\n\t\tjournal->j_flags &= ~JBD2_BARRIER;\n\tif (test_opt(sb, DATA_ERR_ABORT))\n\t\tjournal->j_flags |= JBD2_ABORT_ON_SYNCDATA_ERR;\n\telse\n\t\tjournal->j_flags &= ~JBD2_ABORT_ON_SYNCDATA_ERR;\n\tspin_unlock(&journal->j_state_lock);\n}\n\nstatic journal_t *ext4_get_journal(struct super_block *sb,\n\t\t\t\t   unsigned int journal_inum)\n{\n\tstruct inode *journal_inode;\n\tjournal_t *journal;\n\n\tBUG_ON(!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL));\n\n\t/* First, test for the existence of a valid inode on disk.  Bad\n\t * things happen if we iget() an unused inode, as the subsequent\n\t * iput() will try to delete it. */\n\n\tjournal_inode = ext4_iget(sb, journal_inum);\n\tif (IS_ERR(journal_inode)) {\n\t\text4_msg(sb, KERN_ERR, \"no journal found\");\n\t\treturn NULL;\n\t}\n\tif (!journal_inode->i_nlink) {\n\t\tmake_bad_inode(journal_inode);\n\t\tiput(journal_inode);\n\t\text4_msg(sb, KERN_ERR, \"journal inode is deleted\");\n\t\treturn NULL;\n\t}\n\n\tjbd_debug(2, \"Journal inode found at %p: %lld bytes\\n\",\n\t\t  journal_inode, journal_inode->i_size);\n\tif (!S_ISREG(journal_inode->i_mode)) {\n\t\text4_msg(sb, KERN_ERR, \"invalid journal inode\");\n\t\tiput(journal_inode);\n\t\treturn NULL;\n\t}\n\n\tjournal = jbd2_journal_init_inode(journal_inode);\n\tif (!journal) {\n\t\text4_msg(sb, KERN_ERR, \"Could not load journal inode\");\n\t\tiput(journal_inode);\n\t\treturn NULL;\n\t}\n\tjournal->j_private = sb;\n\text4_init_journal_params(sb, journal);\n\treturn journal;\n}\n\nstatic journal_t *ext4_get_dev_journal(struct super_block *sb,\n\t\t\t\t       dev_t j_dev)\n{\n\tstruct buffer_head *bh;\n\tjournal_t *journal;\n\text4_fsblk_t start;\n\text4_fsblk_t len;\n\tint hblock, blocksize;\n\text4_fsblk_t sb_block;\n\tunsigned long offset;\n\tstruct ext4_super_block *es;\n\tstruct block_device *bdev;\n\n\tBUG_ON(!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL));\n\n\tbdev = ext4_blkdev_get(j_dev, sb);\n\tif (bdev == NULL)\n\t\treturn NULL;\n\n\tif (bd_claim(bdev, sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"failed to claim external journal device\");\n\t\tblkdev_put(bdev, FMODE_READ|FMODE_WRITE);\n\t\treturn NULL;\n\t}\n\n\tblocksize = sb->s_blocksize;\n\thblock = bdev_logical_block_size(bdev);\n\tif (blocksize < hblock) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"blocksize too small for journal device\");\n\t\tgoto out_bdev;\n\t}\n\n\tsb_block = EXT4_MIN_BLOCK_SIZE / blocksize;\n\toffset = EXT4_MIN_BLOCK_SIZE % blocksize;\n\tset_blocksize(bdev, blocksize);\n\tif (!(bh = __bread(bdev, sb_block, blocksize))) {\n\t\text4_msg(sb, KERN_ERR, \"couldn't read superblock of \"\n\t\t       \"external journal\");\n\t\tgoto out_bdev;\n\t}\n\n\tes = (struct ext4_super_block *) (((char *)bh->b_data) + offset);\n\tif ((le16_to_cpu(es->s_magic) != EXT4_SUPER_MAGIC) ||\n\t    !(le32_to_cpu(es->s_feature_incompat) &\n\t      EXT4_FEATURE_INCOMPAT_JOURNAL_DEV)) {\n\t\text4_msg(sb, KERN_ERR, \"external journal has \"\n\t\t\t\t\t\"bad superblock\");\n\t\tbrelse(bh);\n\t\tgoto out_bdev;\n\t}\n\n\tif (memcmp(EXT4_SB(sb)->s_es->s_journal_uuid, es->s_uuid, 16)) {\n\t\text4_msg(sb, KERN_ERR, \"journal UUID does not match\");\n\t\tbrelse(bh);\n\t\tgoto out_bdev;\n\t}\n\n\tlen = ext4_blocks_count(es);\n\tstart = sb_block + 1;\n\tbrelse(bh);\t/* we're done with the superblock */\n\n\tjournal = jbd2_journal_init_dev(bdev, sb->s_bdev,\n\t\t\t\t\tstart, len, blocksize);\n\tif (!journal) {\n\t\text4_msg(sb, KERN_ERR, \"failed to create device journal\");\n\t\tgoto out_bdev;\n\t}\n\tjournal->j_private = sb;\n\tll_rw_block(READ, 1, &journal->j_sb_buffer);\n\twait_on_buffer(journal->j_sb_buffer);\n\tif (!buffer_uptodate(journal->j_sb_buffer)) {\n\t\text4_msg(sb, KERN_ERR, \"I/O error on journal device\");\n\t\tgoto out_journal;\n\t}\n\tif (be32_to_cpu(journal->j_superblock->s_nr_users) != 1) {\n\t\text4_msg(sb, KERN_ERR, \"External journal has more than one \"\n\t\t\t\t\t\"user (unsupported) - %d\",\n\t\t\tbe32_to_cpu(journal->j_superblock->s_nr_users));\n\t\tgoto out_journal;\n\t}\n\tEXT4_SB(sb)->journal_bdev = bdev;\n\text4_init_journal_params(sb, journal);\n\treturn journal;\n\nout_journal:\n\tjbd2_journal_destroy(journal);\nout_bdev:\n\text4_blkdev_put(bdev);\n\treturn NULL;\n}\n\nstatic int ext4_load_journal(struct super_block *sb,\n\t\t\t     struct ext4_super_block *es,\n\t\t\t     unsigned long journal_devnum)\n{\n\tjournal_t *journal;\n\tunsigned int journal_inum = le32_to_cpu(es->s_journal_inum);\n\tdev_t journal_dev;\n\tint err = 0;\n\tint really_read_only;\n\n\tBUG_ON(!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL));\n\n\tif (journal_devnum &&\n\t    journal_devnum != le32_to_cpu(es->s_journal_dev)) {\n\t\text4_msg(sb, KERN_INFO, \"external journal device major/minor \"\n\t\t\t\"numbers have changed\");\n\t\tjournal_dev = new_decode_dev(journal_devnum);\n\t} else\n\t\tjournal_dev = new_decode_dev(le32_to_cpu(es->s_journal_dev));\n\n\treally_read_only = bdev_read_only(sb->s_bdev);\n\n\t/*\n\t * Are we loading a blank journal or performing recovery after a\n\t * crash?  For recovery, we need to check in advance whether we\n\t * can get read-write access to the device.\n\t */\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER)) {\n\t\tif (sb->s_flags & MS_RDONLY) {\n\t\t\text4_msg(sb, KERN_INFO, \"INFO: recovery \"\n\t\t\t\t\t\"required on readonly filesystem\");\n\t\t\tif (really_read_only) {\n\t\t\t\text4_msg(sb, KERN_ERR, \"write access \"\n\t\t\t\t\t\"unavailable, cannot proceed\");\n\t\t\t\treturn -EROFS;\n\t\t\t}\n\t\t\text4_msg(sb, KERN_INFO, \"write access will \"\n\t\t\t       \"be enabled during recovery\");\n\t\t}\n\t}\n\n\tif (journal_inum && journal_dev) {\n\t\text4_msg(sb, KERN_ERR, \"filesystem has both journal \"\n\t\t       \"and inode journals!\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (journal_inum) {\n\t\tif (!(journal = ext4_get_journal(sb, journal_inum)))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (!(journal = ext4_get_dev_journal(sb, journal_dev)))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!(journal->j_flags & JBD2_BARRIER))\n\t\text4_msg(sb, KERN_INFO, \"barriers disabled\");\n\n\tif (!really_read_only && test_opt(sb, UPDATE_JOURNAL)) {\n\t\terr = jbd2_journal_update_format(journal);\n\t\tif (err)  {\n\t\t\text4_msg(sb, KERN_ERR, \"error updating journal\");\n\t\t\tjbd2_journal_destroy(journal);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER))\n\t\terr = jbd2_journal_wipe(journal, !really_read_only);\n\tif (!err)\n\t\terr = jbd2_journal_load(journal);\n\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"error loading journal\");\n\t\tjbd2_journal_destroy(journal);\n\t\treturn err;\n\t}\n\n\tEXT4_SB(sb)->s_journal = journal;\n\text4_clear_journal_err(sb, es);\n\n\tif (journal_devnum &&\n\t    journal_devnum != le32_to_cpu(es->s_journal_dev)) {\n\t\tes->s_journal_dev = cpu_to_le32(journal_devnum);\n\n\t\t/* Make sure we flush the recovery flag to disk. */\n\t\text4_commit_super(sb, 1);\n\t}\n\n\treturn 0;\n}\n\nstatic int ext4_commit_super(struct super_block *sb, int sync)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\tstruct buffer_head *sbh = EXT4_SB(sb)->s_sbh;\n\tint error = 0;\n\n\tif (!sbh)\n\t\treturn error;\n\tif (buffer_write_io_error(sbh)) {\n\t\t/*\n\t\t * Oh, dear.  A previous attempt to write the\n\t\t * superblock failed.  This could happen because the\n\t\t * USB device was yanked out.  Or it could happen to\n\t\t * be a transient write error and maybe the block will\n\t\t * be remapped.  Nothing we can do but to retry the\n\t\t * write and hope for the best.\n\t\t */\n\t\text4_msg(sb, KERN_ERR, \"previous I/O error to \"\n\t\t       \"superblock detected\");\n\t\tclear_buffer_write_io_error(sbh);\n\t\tset_buffer_uptodate(sbh);\n\t}\n\t/*\n\t * If the file system is mounted read-only, don't update the\n\t * superblock write time.  This avoids updating the superblock\n\t * write time when we are mounting the root file system\n\t * read/only but we need to replay the journal; at that point,\n\t * for people who are east of GMT and who make their clock\n\t * tick in localtime for Windows bug-for-bug compatibility,\n\t * the clock is set in the future, and this will cause e2fsck\n\t * to complain and force a full file system check.\n\t */\n\tif (!(sb->s_flags & MS_RDONLY))\n\t\tes->s_wtime = cpu_to_le32(get_seconds());\n\tes->s_kbytes_written =\n\t\tcpu_to_le64(EXT4_SB(sb)->s_kbytes_written + \n\t\t\t    ((part_stat_read(sb->s_bdev->bd_part, sectors[1]) -\n\t\t\t      EXT4_SB(sb)->s_sectors_written_start) >> 1));\n\text4_free_blocks_count_set(es, percpu_counter_sum_positive(\n\t\t\t\t\t&EXT4_SB(sb)->s_freeblocks_counter));\n\tes->s_free_inodes_count = cpu_to_le32(percpu_counter_sum_positive(\n\t\t\t\t\t&EXT4_SB(sb)->s_freeinodes_counter));\n\tsb->s_dirt = 0;\n\tBUFFER_TRACE(sbh, \"marking dirty\");\n\tmark_buffer_dirty(sbh);\n\tif (sync) {\n\t\terror = sync_dirty_buffer(sbh);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\terror = buffer_write_io_error(sbh);\n\t\tif (error) {\n\t\t\text4_msg(sb, KERN_ERR, \"I/O error while writing \"\n\t\t\t       \"superblock\");\n\t\t\tclear_buffer_write_io_error(sbh);\n\t\t\tset_buffer_uptodate(sbh);\n\t\t}\n\t}\n\treturn error;\n}\n\n/*\n * Have we just finished recovery?  If so, and if we are mounting (or\n * remounting) the filesystem readonly, then we will end up with a\n * consistent fs on disk.  Record that fact.\n */\nstatic void ext4_mark_recovery_complete(struct super_block *sb,\n\t\t\t\t\tstruct ext4_super_block *es)\n{\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\tif (!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL)) {\n\t\tBUG_ON(journal != NULL);\n\t\treturn;\n\t}\n\tjbd2_journal_lock_updates(journal);\n\tif (jbd2_journal_flush(journal) < 0)\n\t\tgoto out;\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER) &&\n\t    sb->s_flags & MS_RDONLY) {\n\t\tEXT4_CLEAR_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);\n\t\text4_commit_super(sb, 1);\n\t}\n\nout:\n\tjbd2_journal_unlock_updates(journal);\n}\n\n/*\n * If we are mounting (or read-write remounting) a filesystem whose journal\n * has recorded an error from a previous lifetime, move that error to the\n * main filesystem now.\n */\nstatic void ext4_clear_journal_err(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es)\n{\n\tjournal_t *journal;\n\tint j_errno;\n\tconst char *errstr;\n\n\tBUG_ON(!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL));\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\n\t/*\n\t * Now check for any error status which may have been recorded in the\n\t * journal by a prior ext4_error() or ext4_abort()\n\t */\n\n\tj_errno = jbd2_journal_errno(journal);\n\tif (j_errno) {\n\t\tchar nbuf[16];\n\n\t\terrstr = ext4_decode_error(sb, j_errno, nbuf);\n\t\text4_warning(sb, \"Filesystem error recorded \"\n\t\t\t     \"from previous mount: %s\", errstr);\n\t\text4_warning(sb, \"Marking fs in need of filesystem check.\");\n\n\t\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\t\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\t\text4_commit_super(sb, 1);\n\n\t\tjbd2_journal_clear_err(journal);\n\t}\n}\n\n/*\n * Force the running and committing transactions to commit,\n * and wait on the commit.\n */\nint ext4_force_commit(struct super_block *sb)\n{\n\tjournal_t *journal;\n\tint ret = 0;\n\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn 0;\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\tif (journal)\n\t\tret = ext4_journal_force_commit(journal);\n\n\treturn ret;\n}\n\nstatic void ext4_write_super(struct super_block *sb)\n{\n\tlock_super(sb);\n\text4_commit_super(sb, 1);\n\tunlock_super(sb);\n}\n\nstatic int ext4_sync_fs(struct super_block *sb, int wait)\n{\n\tint ret = 0;\n\ttid_t target;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\ttrace_ext4_sync_fs(sb, wait);\n\tflush_workqueue(sbi->dio_unwritten_wq);\n\tif (jbd2_journal_start_commit(sbi->s_journal, &target)) {\n\t\tif (wait)\n\t\t\tjbd2_log_wait_commit(sbi->s_journal, target);\n\t}\n\treturn ret;\n}\n\n/*\n * LVM calls this function before a (read-only) snapshot is created.  This\n * gives us a chance to flush the journal completely and mark the fs clean.\n */\nstatic int ext4_freeze(struct super_block *sb)\n{\n\tint error = 0;\n\tjournal_t *journal;\n\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn 0;\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\n\t/* Now we set up the journal barrier. */\n\tjbd2_journal_lock_updates(journal);\n\n\t/*\n\t * Don't clear the needs_recovery flag if we failed to flush\n\t * the journal.\n\t */\n\terror = jbd2_journal_flush(journal);\n\tif (error < 0) {\n\tout:\n\t\tjbd2_journal_unlock_updates(journal);\n\t\treturn error;\n\t}\n\n\t/* Journal blocked and flushed, clear needs_recovery flag. */\n\tEXT4_CLEAR_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);\n\terror = ext4_commit_super(sb, 1);\n\tif (error)\n\t\tgoto out;\n\treturn 0;\n}\n\n/*\n * Called by LVM after the snapshot is done.  We need to reset the RECOVER\n * flag here, even though the filesystem is not technically dirty yet.\n */\nstatic int ext4_unfreeze(struct super_block *sb)\n{\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn 0;\n\n\tlock_super(sb);\n\t/* Reset the needs_recovery flag before the fs is unlocked. */\n\tEXT4_SET_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);\n\text4_commit_super(sb, 1);\n\tunlock_super(sb);\n\tjbd2_journal_unlock_updates(EXT4_SB(sb)->s_journal);\n\treturn 0;\n}\n\nstatic int ext4_remount(struct super_block *sb, int *flags, char *data)\n{\n\tstruct ext4_super_block *es;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t n_blocks_count = 0;\n\tunsigned long old_sb_flags;\n\tstruct ext4_mount_options old_opts;\n\text4_group_t g;\n\tunsigned int journal_ioprio = DEFAULT_JOURNAL_IOPRIO;\n\tint err;\n#ifdef CONFIG_QUOTA\n\tint i;\n#endif\n\n\tlock_kernel();\n\n\t/* Store the original options */\n\tlock_super(sb);\n\told_sb_flags = sb->s_flags;\n\told_opts.s_mount_opt = sbi->s_mount_opt;\n\told_opts.s_resuid = sbi->s_resuid;\n\told_opts.s_resgid = sbi->s_resgid;\n\told_opts.s_commit_interval = sbi->s_commit_interval;\n\told_opts.s_min_batch_time = sbi->s_min_batch_time;\n\told_opts.s_max_batch_time = sbi->s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\told_opts.s_jquota_fmt = sbi->s_jquota_fmt;\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\told_opts.s_qf_names[i] = sbi->s_qf_names[i];\n#endif\n\tif (sbi->s_journal && sbi->s_journal->j_task->io_context)\n\t\tjournal_ioprio = sbi->s_journal->j_task->io_context->ioprio;\n\n\t/*\n\t * Allow the \"check\" option to be passed as a remount option.\n\t */\n\tif (!parse_options(data, sb, NULL, &journal_ioprio,\n\t\t\t   &n_blocks_count, 1)) {\n\t\terr = -EINVAL;\n\t\tgoto restore_opts;\n\t}\n\n\tif (sbi->s_mount_flags & EXT4_MF_FS_ABORTED)\n\t\text4_abort(sb, __func__, \"Abort forced by user\");\n\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sb, POSIX_ACL) ? MS_POSIXACL : 0);\n\n\tes = sbi->s_es;\n\n\tif (sbi->s_journal) {\n\t\text4_init_journal_params(sb, sbi->s_journal);\n\t\tset_task_ioprio(sbi->s_journal->j_task, journal_ioprio);\n\t}\n\n\tif ((*flags & MS_RDONLY) != (sb->s_flags & MS_RDONLY) ||\n\t\tn_blocks_count > ext4_blocks_count(es)) {\n\t\tif (sbi->s_mount_flags & EXT4_MF_FS_ABORTED) {\n\t\t\terr = -EROFS;\n\t\t\tgoto restore_opts;\n\t\t}\n\n\t\tif (*flags & MS_RDONLY) {\n\t\t\t/*\n\t\t\t * First of all, the unconditional stuff we have to do\n\t\t\t * to disable replay of the journal when we next remount\n\t\t\t */\n\t\t\tsb->s_flags |= MS_RDONLY;\n\n\t\t\t/*\n\t\t\t * OK, test if we are remounting a valid rw partition\n\t\t\t * readonly, and if so set the rdonly flag and then\n\t\t\t * mark the partition as valid again.\n\t\t\t */\n\t\t\tif (!(es->s_state & cpu_to_le16(EXT4_VALID_FS)) &&\n\t\t\t    (sbi->s_mount_state & EXT4_VALID_FS))\n\t\t\t\tes->s_state = cpu_to_le16(sbi->s_mount_state);\n\n\t\t\tif (sbi->s_journal)\n\t\t\t\text4_mark_recovery_complete(sb, es);\n\t\t} else {\n\t\t\t/* Make sure we can mount this feature set readwrite */\n\t\t\tif (!ext4_feature_set_ok(sb, 0)) {\n\t\t\t\terr = -EROFS;\n\t\t\t\tgoto restore_opts;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Make sure the group descriptor checksums\n\t\t\t * are sane.  If they aren't, refuse to remount r/w.\n\t\t\t */\n\t\t\tfor (g = 0; g < sbi->s_groups_count; g++) {\n\t\t\t\tstruct ext4_group_desc *gdp =\n\t\t\t\t\text4_get_group_desc(sb, g, NULL);\n\n\t\t\t\tif (!ext4_group_desc_csum_verify(sbi, g, gdp)) {\n\t\t\t\t\text4_msg(sb, KERN_ERR,\n\t       \"ext4_remount: Checksum for group %u failed (%u!=%u)\",\n\t\tg, le16_to_cpu(ext4_group_desc_csum(sbi, g, gdp)),\n\t\t\t\t\t       le16_to_cpu(gdp->bg_checksum));\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto restore_opts;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If we have an unprocessed orphan list hanging\n\t\t\t * around from a previously readonly bdev mount,\n\t\t\t * require a full umount/remount for now.\n\t\t\t */\n\t\t\tif (es->s_last_orphan) {\n\t\t\t\text4_msg(sb, KERN_WARNING, \"Couldn't \"\n\t\t\t\t       \"remount RDWR because of unprocessed \"\n\t\t\t\t       \"orphan inode list.  Please \"\n\t\t\t\t       \"umount/remount instead\");\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto restore_opts;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Mounting a RDONLY partition read-write, so reread\n\t\t\t * and store the current valid flag.  (It may have\n\t\t\t * been changed by e2fsck since we originally mounted\n\t\t\t * the partition.)\n\t\t\t */\n\t\t\tif (sbi->s_journal)\n\t\t\t\text4_clear_journal_err(sb, es);\n\t\t\tsbi->s_mount_state = le16_to_cpu(es->s_state);\n\t\t\tif ((err = ext4_group_extend(sb, es, n_blocks_count)))\n\t\t\t\tgoto restore_opts;\n\t\t\tif (!ext4_setup_super(sb, es, 0))\n\t\t\t\tsb->s_flags &= ~MS_RDONLY;\n\t\t}\n\t}\n\text4_setup_system_zone(sb);\n\tif (sbi->s_journal == NULL)\n\t\text4_commit_super(sb, 1);\n\n#ifdef CONFIG_QUOTA\n\t/* Release old quota file names */\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tif (old_opts.s_qf_names[i] &&\n\t\t    old_opts.s_qf_names[i] != sbi->s_qf_names[i])\n\t\t\tkfree(old_opts.s_qf_names[i]);\n#endif\n\tunlock_super(sb);\n\tunlock_kernel();\n\treturn 0;\n\nrestore_opts:\n\tsb->s_flags = old_sb_flags;\n\tsbi->s_mount_opt = old_opts.s_mount_opt;\n\tsbi->s_resuid = old_opts.s_resuid;\n\tsbi->s_resgid = old_opts.s_resgid;\n\tsbi->s_commit_interval = old_opts.s_commit_interval;\n\tsbi->s_min_batch_time = old_opts.s_min_batch_time;\n\tsbi->s_max_batch_time = old_opts.s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\tsbi->s_jquota_fmt = old_opts.s_jquota_fmt;\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tif (sbi->s_qf_names[i] &&\n\t\t    old_opts.s_qf_names[i] != sbi->s_qf_names[i])\n\t\t\tkfree(sbi->s_qf_names[i]);\n\t\tsbi->s_qf_names[i] = old_opts.s_qf_names[i];\n\t}\n#endif\n\tunlock_super(sb);\n\tunlock_kernel();\n\treturn err;\n}\n\nstatic int ext4_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct super_block *sb = dentry->d_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tu64 fsid;\n\n\tif (test_opt(sb, MINIX_DF)) {\n\t\tsbi->s_overhead_last = 0;\n\t} else if (sbi->s_blocks_last != ext4_blocks_count(es)) {\n\t\text4_group_t i, ngroups = ext4_get_groups_count(sb);\n\t\text4_fsblk_t overhead = 0;\n\n\t\t/*\n\t\t * Compute the overhead (FS structures).  This is constant\n\t\t * for a given filesystem unless the number of block groups\n\t\t * changes so we cache the previous value until it does.\n\t\t */\n\n\t\t/*\n\t\t * All of the blocks before first_data_block are\n\t\t * overhead\n\t\t */\n\t\toverhead = le32_to_cpu(es->s_first_data_block);\n\n\t\t/*\n\t\t * Add the overhead attributed to the superblock and\n\t\t * block group descriptors.  If the sparse superblocks\n\t\t * feature is turned on, then not all groups have this.\n\t\t */\n\t\tfor (i = 0; i < ngroups; i++) {\n\t\t\toverhead += ext4_bg_has_super(sb, i) +\n\t\t\t\text4_bg_num_gdb(sb, i);\n\t\t\tcond_resched();\n\t\t}\n\n\t\t/*\n\t\t * Every block group has an inode bitmap, a block\n\t\t * bitmap, and an inode table.\n\t\t */\n\t\toverhead += ngroups * (2 + sbi->s_itb_per_group);\n\t\tsbi->s_overhead_last = overhead;\n\t\tsmp_wmb();\n\t\tsbi->s_blocks_last = ext4_blocks_count(es);\n\t}\n\n\tbuf->f_type = EXT4_SUPER_MAGIC;\n\tbuf->f_bsize = sb->s_blocksize;\n\tbuf->f_blocks = ext4_blocks_count(es) - sbi->s_overhead_last;\n\tbuf->f_bfree = percpu_counter_sum_positive(&sbi->s_freeblocks_counter) -\n\t\t       percpu_counter_sum_positive(&sbi->s_dirtyblocks_counter);\n\tbuf->f_bavail = buf->f_bfree - ext4_r_blocks_count(es);\n\tif (buf->f_bfree < ext4_r_blocks_count(es))\n\t\tbuf->f_bavail = 0;\n\tbuf->f_files = le32_to_cpu(es->s_inodes_count);\n\tbuf->f_ffree = percpu_counter_sum_positive(&sbi->s_freeinodes_counter);\n\tbuf->f_namelen = EXT4_NAME_LEN;\n\tfsid = le64_to_cpup((void *)es->s_uuid) ^\n\t       le64_to_cpup((void *)es->s_uuid + sizeof(u64));\n\tbuf->f_fsid.val[0] = fsid & 0xFFFFFFFFUL;\n\tbuf->f_fsid.val[1] = (fsid >> 32) & 0xFFFFFFFFUL;\n\n\treturn 0;\n}\n\n/* Helper function for writing quotas on sync - we need to start transaction\n * before quota file is locked for write. Otherwise the are possible deadlocks:\n * Process 1                         Process 2\n * ext4_create()                     quota_sync()\n *   jbd2_journal_start()                  write_dquot()\n *   vfs_dq_init()                         down(dqio_mutex)\n *     down(dqio_mutex)                    jbd2_journal_start()\n *\n */\n\n#ifdef CONFIG_QUOTA\n\nstatic inline struct inode *dquot_to_inode(struct dquot *dquot)\n{\n\treturn sb_dqopt(dquot->dq_sb)->files[dquot->dq_type];\n}\n\nstatic int ext4_write_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\tstruct inode *inode;\n\n\tinode = dquot_to_inode(dquot);\n\thandle = ext4_journal_start(inode,\n\t\t\t\t    EXT4_QUOTA_TRANS_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_commit(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_acquire_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\thandle = ext4_journal_start(dquot_to_inode(dquot),\n\t\t\t\t    EXT4_QUOTA_INIT_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_acquire(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_release_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\thandle = ext4_journal_start(dquot_to_inode(dquot),\n\t\t\t\t    EXT4_QUOTA_DEL_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle)) {\n\t\t/* Release dquot anyway to avoid endless cycle in dqput() */\n\t\tdquot_release(dquot);\n\t\treturn PTR_ERR(handle);\n\t}\n\tret = dquot_release(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_mark_dquot_dirty(struct dquot *dquot)\n{\n\t/* Are we journaling quotas? */\n\tif (EXT4_SB(dquot->dq_sb)->s_qf_names[USRQUOTA] ||\n\t    EXT4_SB(dquot->dq_sb)->s_qf_names[GRPQUOTA]) {\n\t\tdquot_mark_dquot_dirty(dquot);\n\t\treturn ext4_write_dquot(dquot);\n\t} else {\n\t\treturn dquot_mark_dquot_dirty(dquot);\n\t}\n}\n\nstatic int ext4_write_info(struct super_block *sb, int type)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\t/* Data block + inode block */\n\thandle = ext4_journal_start(sb->s_root->d_inode, 2);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_commit_info(sb, type);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\n/*\n * Turn on quotas during mount time - we need to find\n * the quota file and such...\n */\nstatic int ext4_quota_on_mount(struct super_block *sb, int type)\n{\n\treturn vfs_quota_on_mount(sb, EXT4_SB(sb)->s_qf_names[type],\n\t\t\t\t  EXT4_SB(sb)->s_jquota_fmt, type);\n}\n\n/*\n * Standard function to be called on quota_on\n */\nstatic int ext4_quota_on(struct super_block *sb, int type, int format_id,\n\t\t\t char *name, int remount)\n{\n\tint err;\n\tstruct path path;\n\n\tif (!test_opt(sb, QUOTA))\n\t\treturn -EINVAL;\n\t/* When remounting, no checks are needed and in fact, name is NULL */\n\tif (remount)\n\t\treturn vfs_quota_on(sb, type, format_id, name, remount);\n\n\terr = kern_path(name, LOOKUP_FOLLOW, &path);\n\tif (err)\n\t\treturn err;\n\n\t/* Quotafile not on the same filesystem? */\n\tif (path.mnt->mnt_sb != sb) {\n\t\tpath_put(&path);\n\t\treturn -EXDEV;\n\t}\n\t/* Journaling quota? */\n\tif (EXT4_SB(sb)->s_qf_names[type]) {\n\t\t/* Quotafile not in fs root? */\n\t\tif (path.dentry->d_parent != sb->s_root)\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t\"Quota file not on filesystem root. \"\n\t\t\t\t\"Journaled quota will not work\");\n\t}\n\n\t/*\n\t * When we journal data on quota file, we have to flush journal to see\n\t * all updates to the file when we bypass pagecache...\n\t */\n\tif (EXT4_SB(sb)->s_journal &&\n\t    ext4_should_journal_data(path.dentry->d_inode)) {\n\t\t/*\n\t\t * We don't need to lock updates but journal_flush() could\n\t\t * otherwise be livelocked...\n\t\t */\n\t\tjbd2_journal_lock_updates(EXT4_SB(sb)->s_journal);\n\t\terr = jbd2_journal_flush(EXT4_SB(sb)->s_journal);\n\t\tjbd2_journal_unlock_updates(EXT4_SB(sb)->s_journal);\n\t\tif (err) {\n\t\t\tpath_put(&path);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\terr = vfs_quota_on_path(sb, type, format_id, &path);\n\tpath_put(&path);\n\treturn err;\n}\n\n/* Read data from quotafile - avoid pagecache and such because we cannot afford\n * acquiring the locks... As quota files are never truncated and quota code\n * itself serializes the operations (and noone else should touch the files)\n * we don't have to be afraid of races */\nstatic ssize_t ext4_quota_read(struct super_block *sb, int type, char *data,\n\t\t\t       size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\text4_lblk_t blk = off >> EXT4_BLOCK_SIZE_BITS(sb);\n\tint err = 0;\n\tint offset = off & (sb->s_blocksize - 1);\n\tint tocopy;\n\tsize_t toread;\n\tstruct buffer_head *bh;\n\tloff_t i_size = i_size_read(inode);\n\n\tif (off > i_size)\n\t\treturn 0;\n\tif (off+len > i_size)\n\t\tlen = i_size-off;\n\ttoread = len;\n\twhile (toread > 0) {\n\t\ttocopy = sb->s_blocksize - offset < toread ?\n\t\t\t\tsb->s_blocksize - offset : toread;\n\t\tbh = ext4_bread(NULL, inode, blk, 0, &err);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (!bh)\t/* A hole? */\n\t\t\tmemset(data, 0, tocopy);\n\t\telse\n\t\t\tmemcpy(data, bh->b_data+offset, tocopy);\n\t\tbrelse(bh);\n\t\toffset = 0;\n\t\ttoread -= tocopy;\n\t\tdata += tocopy;\n\t\tblk++;\n\t}\n\treturn len;\n}\n\n/* Write to quotafile (we know the transaction is already started and has\n * enough credits) */\nstatic ssize_t ext4_quota_write(struct super_block *sb, int type,\n\t\t\t\tconst char *data, size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\text4_lblk_t blk = off >> EXT4_BLOCK_SIZE_BITS(sb);\n\tint err = 0;\n\tint offset = off & (sb->s_blocksize - 1);\n\tint tocopy;\n\tint journal_quota = EXT4_SB(sb)->s_qf_names[type] != NULL;\n\tsize_t towrite = len;\n\tstruct buffer_head *bh;\n\thandle_t *handle = journal_current_handle();\n\n\tif (EXT4_SB(sb)->s_journal && !handle) {\n\t\text4_msg(sb, KERN_WARNING, \"Quota write (off=%llu, len=%llu)\"\n\t\t\t\" cancelled because transaction is not started\",\n\t\t\t(unsigned long long)off, (unsigned long long)len);\n\t\treturn -EIO;\n\t}\n\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_QUOTA);\n\twhile (towrite > 0) {\n\t\ttocopy = sb->s_blocksize - offset < towrite ?\n\t\t\t\tsb->s_blocksize - offset : towrite;\n\t\tbh = ext4_bread(handle, inode, blk, 1, &err);\n\t\tif (!bh)\n\t\t\tgoto out;\n\t\tif (journal_quota) {\n\t\t\terr = ext4_journal_get_write_access(handle, bh);\n\t\t\tif (err) {\n\t\t\t\tbrelse(bh);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tlock_buffer(bh);\n\t\tmemcpy(bh->b_data+offset, data, tocopy);\n\t\tflush_dcache_page(bh->b_page);\n\t\tunlock_buffer(bh);\n\t\tif (journal_quota)\n\t\t\terr = ext4_handle_dirty_metadata(handle, NULL, bh);\n\t\telse {\n\t\t\t/* Always do at least ordered writes for quotas */\n\t\t\terr = ext4_jbd2_file_inode(handle, inode);\n\t\t\tmark_buffer_dirty(bh);\n\t\t}\n\t\tbrelse(bh);\n\t\tif (err)\n\t\t\tgoto out;\n\t\toffset = 0;\n\t\ttowrite -= tocopy;\n\t\tdata += tocopy;\n\t\tblk++;\n\t}\nout:\n\tif (len == towrite) {\n\t\tmutex_unlock(&inode->i_mutex);\n\t\treturn err;\n\t}\n\tif (inode->i_size < off+len-towrite) {\n\t\ti_size_write(inode, off+len-towrite);\n\t\tEXT4_I(inode)->i_disksize = inode->i_size;\n\t}\n\tinode->i_mtime = inode->i_ctime = CURRENT_TIME;\n\text4_mark_inode_dirty(handle, inode);\n\tmutex_unlock(&inode->i_mutex);\n\treturn len - towrite;\n}\n\n#endif\n\nstatic int ext4_get_sb(struct file_system_type *fs_type, int flags,\n\t\t       const char *dev_name, void *data, struct vfsmount *mnt)\n{\n\treturn get_sb_bdev(fs_type, flags, dev_name, data, ext4_fill_super,mnt);\n}\n\n#if !defined(CONTIG_EXT2_FS) && !defined(CONFIG_EXT2_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT23)\nstatic struct file_system_type ext2_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext2\",\n\t.get_sb\t\t= ext4_get_sb,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\n\nstatic inline void register_as_ext2(void)\n{\n\tint err = register_filesystem(&ext2_fs_type);\n\tif (err)\n\t\tprintk(KERN_WARNING\n\t\t       \"EXT4-fs: Unable to register as ext2 (%d)\\n\", err);\n}\n\nstatic inline void unregister_as_ext2(void)\n{\n\tunregister_filesystem(&ext2_fs_type);\n}\nMODULE_ALIAS(\"ext2\");\n#else\nstatic inline void register_as_ext2(void) { }\nstatic inline void unregister_as_ext2(void) { }\n#endif\n\n#if !defined(CONTIG_EXT3_FS) && !defined(CONFIG_EXT3_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT23)\nstatic struct file_system_type ext3_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext3\",\n\t.get_sb\t\t= ext4_get_sb,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\n\nstatic inline void register_as_ext3(void)\n{\n\tint err = register_filesystem(&ext3_fs_type);\n\tif (err)\n\t\tprintk(KERN_WARNING\n\t\t       \"EXT4-fs: Unable to register as ext3 (%d)\\n\", err);\n}\n\nstatic inline void unregister_as_ext3(void)\n{\n\tunregister_filesystem(&ext3_fs_type);\n}\nMODULE_ALIAS(\"ext3\");\n#else\nstatic inline void register_as_ext3(void) { }\nstatic inline void unregister_as_ext3(void) { }\n#endif\n\nstatic struct file_system_type ext4_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext4\",\n\t.get_sb\t\t= ext4_get_sb,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\n\nstatic int __init init_ext4_fs(void)\n{\n\tint err;\n\n\terr = init_ext4_system_zone();\n\tif (err)\n\t\treturn err;\n\text4_kset = kset_create_and_add(\"ext4\", NULL, fs_kobj);\n\tif (!ext4_kset)\n\t\tgoto out4;\n\text4_proc_root = proc_mkdir(\"fs/ext4\", NULL);\n\terr = init_ext4_mballoc();\n\tif (err)\n\t\tgoto out3;\n\n\terr = init_ext4_xattr();\n\tif (err)\n\t\tgoto out2;\n\terr = init_inodecache();\n\tif (err)\n\t\tgoto out1;\n\tregister_as_ext2();\n\tregister_as_ext3();\n\terr = register_filesystem(&ext4_fs_type);\n\tif (err)\n\t\tgoto out;\n\treturn 0;\nout:\n\tunregister_as_ext2();\n\tunregister_as_ext3();\n\tdestroy_inodecache();\nout1:\n\texit_ext4_xattr();\nout2:\n\texit_ext4_mballoc();\nout3:\n\tremove_proc_entry(\"fs/ext4\", NULL);\n\tkset_unregister(ext4_kset);\nout4:\n\texit_ext4_system_zone();\n\treturn err;\n}\n\nstatic void __exit exit_ext4_fs(void)\n{\n\tunregister_as_ext2();\n\tunregister_as_ext3();\n\tunregister_filesystem(&ext4_fs_type);\n\tdestroy_inodecache();\n\texit_ext4_xattr();\n\texit_ext4_mballoc();\n\tremove_proc_entry(\"fs/ext4\", NULL);\n\tkset_unregister(ext4_kset);\n\texit_ext4_system_zone();\n}\n\nMODULE_AUTHOR(\"Remy Card, Stephen Tweedie, Andrew Morton, Andreas Dilger, Theodore Ts'o and others\");\nMODULE_DESCRIPTION(\"Fourth Extended Filesystem\");\nMODULE_LICENSE(\"GPL\");\nmodule_init(init_ext4_fs)\nmodule_exit(exit_ext4_fs)\n"], "fixing_code": ["/*\n *  ext4.h\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/include/linux/minix_fs.h\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n#ifndef _EXT4_H\n#define _EXT4_H\n\n#include <linux/types.h>\n#include <linux/blkdev.h>\n#include <linux/magic.h>\n#include <linux/jbd2.h>\n#include <linux/quota.h>\n#include <linux/rwsem.h>\n#include <linux/rbtree.h>\n#include <linux/seqlock.h>\n#include <linux/mutex.h>\n#include <linux/timer.h>\n#include <linux/wait.h>\n#include <linux/blockgroup_lock.h>\n#include <linux/percpu_counter.h>\n\n/*\n * The fourth extended filesystem constants/structures\n */\n\n/*\n * Define EXT4FS_DEBUG to produce debug messages\n */\n#undef EXT4FS_DEBUG\n\n/*\n * Debug code\n */\n#ifdef EXT4FS_DEBUG\n#define ext4_debug(f, a...)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tprintk(KERN_DEBUG \"EXT4-fs DEBUG (%s, %d): %s:\",\t\\\n\t\t\t__FILE__, __LINE__, __func__);\t\t\t\\\n\t\tprintk(KERN_DEBUG f, ## a);\t\t\t\t\\\n\t} while (0)\n#else\n#define ext4_debug(f, a...)\tdo {} while (0)\n#endif\n\n/* data type for block offset of block group */\ntypedef int ext4_grpblk_t;\n\n/* data type for filesystem-wide blocks number */\ntypedef unsigned long long ext4_fsblk_t;\n\n/* data type for file logical block number */\ntypedef __u32 ext4_lblk_t;\n\n/* data type for block group number */\ntypedef unsigned int ext4_group_t;\n\n/*\n * Flags used in mballoc's allocation_context flags field.  \n *\n * Also used to show what's going on for debugging purposes when the\n * flag field is exported via the traceport interface\n */\n\n/* prefer goal again. length */\n#define EXT4_MB_HINT_MERGE\t\t0x0001\n/* blocks already reserved */\n#define EXT4_MB_HINT_RESERVED\t\t0x0002\n/* metadata is being allocated */\n#define EXT4_MB_HINT_METADATA\t\t0x0004\n/* first blocks in the file */\n#define EXT4_MB_HINT_FIRST\t\t0x0008\n/* search for the best chunk */\n#define EXT4_MB_HINT_BEST\t\t0x0010\n/* data is being allocated */\n#define EXT4_MB_HINT_DATA\t\t0x0020\n/* don't preallocate (for tails) */\n#define EXT4_MB_HINT_NOPREALLOC\t\t0x0040\n/* allocate for locality group */\n#define EXT4_MB_HINT_GROUP_ALLOC\t0x0080\n/* allocate goal blocks or none */\n#define EXT4_MB_HINT_GOAL_ONLY\t\t0x0100\n/* goal is meaningful */\n#define EXT4_MB_HINT_TRY_GOAL\t\t0x0200\n/* blocks already pre-reserved by delayed allocation */\n#define EXT4_MB_DELALLOC_RESERVED\t0x0400\n/* We are doing stream allocation */\n#define EXT4_MB_STREAM_ALLOC\t\t0x0800\n\n\nstruct ext4_allocation_request {\n\t/* target inode for block we're allocating */\n\tstruct inode *inode;\n\t/* how many blocks we want to allocate */\n\tunsigned int len;\n\t/* logical block in target inode */\n\text4_lblk_t logical;\n\t/* the closest logical allocated block to the left */\n\text4_lblk_t lleft;\n\t/* the closest logical allocated block to the right */\n\text4_lblk_t lright;\n\t/* phys. target (a hint) */\n\text4_fsblk_t goal;\n\t/* phys. block for the closest logical allocated block to the left */\n\text4_fsblk_t pleft;\n\t/* phys. block for the closest logical allocated block to the right */\n\text4_fsblk_t pright;\n\t/* flags. see above EXT4_MB_HINT_* */\n\tunsigned int flags;\n};\n\n/*\n * For delayed allocation tracking\n */\nstruct mpage_da_data {\n\tstruct inode *inode;\n\tsector_t b_blocknr;\t\t/* start block number of extent */\n\tsize_t b_size;\t\t\t/* size of extent */\n\tunsigned long b_state;\t\t/* state of the extent */\n\tunsigned long first_page, next_page;\t/* extent of pages */\n\tstruct writeback_control *wbc;\n\tint io_done;\n\tint pages_written;\n\tint retval;\n};\n#define\tEXT4_IO_UNWRITTEN\t0x1\ntypedef struct ext4_io_end {\n\tstruct list_head\tlist;\t\t/* per-file finished AIO list */\n\tstruct inode\t\t*inode;\t\t/* file being written to */\n\tunsigned int\t\tflag;\t\t/* unwritten or not */\n\tstruct page\t\t*page;\t\t/* page struct for buffer write */\n\tloff_t\t\t\toffset;\t\t/* offset in the file */\n\tssize_t\t\t\tsize;\t\t/* size of the extent */\n\tstruct work_struct\twork;\t\t/* data work queue */\n} ext4_io_end_t;\n\n/*\n * Special inodes numbers\n */\n#define\tEXT4_BAD_INO\t\t 1\t/* Bad blocks inode */\n#define EXT4_ROOT_INO\t\t 2\t/* Root inode */\n#define EXT4_BOOT_LOADER_INO\t 5\t/* Boot loader inode */\n#define EXT4_UNDEL_DIR_INO\t 6\t/* Undelete directory inode */\n#define EXT4_RESIZE_INO\t\t 7\t/* Reserved group descriptors inode */\n#define EXT4_JOURNAL_INO\t 8\t/* Journal inode */\n\n/* First non-reserved inode for old ext4 filesystems */\n#define EXT4_GOOD_OLD_FIRST_INO\t11\n\n/*\n * Maximal count of links to a file\n */\n#define EXT4_LINK_MAX\t\t65000\n\n/*\n * Macro-instructions used to manage several block sizes\n */\n#define EXT4_MIN_BLOCK_SIZE\t\t1024\n#define\tEXT4_MAX_BLOCK_SIZE\t\t65536\n#define EXT4_MIN_BLOCK_LOG_SIZE\t\t10\n#ifdef __KERNEL__\n# define EXT4_BLOCK_SIZE(s)\t\t((s)->s_blocksize)\n#else\n# define EXT4_BLOCK_SIZE(s)\t\t(EXT4_MIN_BLOCK_SIZE << (s)->s_log_block_size)\n#endif\n#define\tEXT4_ADDR_PER_BLOCK(s)\t\t(EXT4_BLOCK_SIZE(s) / sizeof(__u32))\n#ifdef __KERNEL__\n# define EXT4_BLOCK_SIZE_BITS(s)\t((s)->s_blocksize_bits)\n#else\n# define EXT4_BLOCK_SIZE_BITS(s)\t((s)->s_log_block_size + 10)\n#endif\n#ifdef __KERNEL__\n#define\tEXT4_ADDR_PER_BLOCK_BITS(s)\t(EXT4_SB(s)->s_addr_per_block_bits)\n#define EXT4_INODE_SIZE(s)\t\t(EXT4_SB(s)->s_inode_size)\n#define EXT4_FIRST_INO(s)\t\t(EXT4_SB(s)->s_first_ino)\n#else\n#define EXT4_INODE_SIZE(s)\t(((s)->s_rev_level == EXT4_GOOD_OLD_REV) ? \\\n\t\t\t\t EXT4_GOOD_OLD_INODE_SIZE : \\\n\t\t\t\t (s)->s_inode_size)\n#define EXT4_FIRST_INO(s)\t(((s)->s_rev_level == EXT4_GOOD_OLD_REV) ? \\\n\t\t\t\t EXT4_GOOD_OLD_FIRST_INO : \\\n\t\t\t\t (s)->s_first_ino)\n#endif\n#define EXT4_BLOCK_ALIGN(size, blkbits)\t\tALIGN((size), (1 << (blkbits)))\n\n/*\n * Structure of a blocks group descriptor\n */\nstruct ext4_group_desc\n{\n\t__le32\tbg_block_bitmap_lo;\t/* Blocks bitmap block */\n\t__le32\tbg_inode_bitmap_lo;\t/* Inodes bitmap block */\n\t__le32\tbg_inode_table_lo;\t/* Inodes table block */\n\t__le16\tbg_free_blocks_count_lo;/* Free blocks count */\n\t__le16\tbg_free_inodes_count_lo;/* Free inodes count */\n\t__le16\tbg_used_dirs_count_lo;\t/* Directories count */\n\t__le16\tbg_flags;\t\t/* EXT4_BG_flags (INODE_UNINIT, etc) */\n\t__u32\tbg_reserved[2];\t\t/* Likely block/inode bitmap checksum */\n\t__le16  bg_itable_unused_lo;\t/* Unused inodes count */\n\t__le16  bg_checksum;\t\t/* crc16(sb_uuid+group+desc) */\n\t__le32\tbg_block_bitmap_hi;\t/* Blocks bitmap block MSB */\n\t__le32\tbg_inode_bitmap_hi;\t/* Inodes bitmap block MSB */\n\t__le32\tbg_inode_table_hi;\t/* Inodes table block MSB */\n\t__le16\tbg_free_blocks_count_hi;/* Free blocks count MSB */\n\t__le16\tbg_free_inodes_count_hi;/* Free inodes count MSB */\n\t__le16\tbg_used_dirs_count_hi;\t/* Directories count MSB */\n\t__le16  bg_itable_unused_hi;    /* Unused inodes count MSB */\n\t__u32\tbg_reserved2[3];\n};\n\n/*\n * Structure of a flex block group info\n */\n\nstruct flex_groups {\n\tatomic_t free_inodes;\n\tatomic_t free_blocks;\n\tatomic_t used_dirs;\n};\n\n#define EXT4_BG_INODE_UNINIT\t0x0001 /* Inode table/bitmap not in use */\n#define EXT4_BG_BLOCK_UNINIT\t0x0002 /* Block bitmap not in use */\n#define EXT4_BG_INODE_ZEROED\t0x0004 /* On-disk itable initialized to zero */\n\n/*\n * Macro-instructions used to manage group descriptors\n */\n#define EXT4_MIN_DESC_SIZE\t\t32\n#define EXT4_MIN_DESC_SIZE_64BIT\t64\n#define\tEXT4_MAX_DESC_SIZE\t\tEXT4_MIN_BLOCK_SIZE\n#define EXT4_DESC_SIZE(s)\t\t(EXT4_SB(s)->s_desc_size)\n#ifdef __KERNEL__\n# define EXT4_BLOCKS_PER_GROUP(s)\t(EXT4_SB(s)->s_blocks_per_group)\n# define EXT4_DESC_PER_BLOCK(s)\t\t(EXT4_SB(s)->s_desc_per_block)\n# define EXT4_INODES_PER_GROUP(s)\t(EXT4_SB(s)->s_inodes_per_group)\n# define EXT4_DESC_PER_BLOCK_BITS(s)\t(EXT4_SB(s)->s_desc_per_block_bits)\n#else\n# define EXT4_BLOCKS_PER_GROUP(s)\t((s)->s_blocks_per_group)\n# define EXT4_DESC_PER_BLOCK(s)\t\t(EXT4_BLOCK_SIZE(s) / EXT4_DESC_SIZE(s))\n# define EXT4_INODES_PER_GROUP(s)\t((s)->s_inodes_per_group)\n#endif\n\n/*\n * Constants relative to the data blocks\n */\n#define\tEXT4_NDIR_BLOCKS\t\t12\n#define\tEXT4_IND_BLOCK\t\t\tEXT4_NDIR_BLOCKS\n#define\tEXT4_DIND_BLOCK\t\t\t(EXT4_IND_BLOCK + 1)\n#define\tEXT4_TIND_BLOCK\t\t\t(EXT4_DIND_BLOCK + 1)\n#define\tEXT4_N_BLOCKS\t\t\t(EXT4_TIND_BLOCK + 1)\n\n/*\n * Inode flags\n */\n#define\tEXT4_SECRM_FL\t\t\t0x00000001 /* Secure deletion */\n#define\tEXT4_UNRM_FL\t\t\t0x00000002 /* Undelete */\n#define\tEXT4_COMPR_FL\t\t\t0x00000004 /* Compress file */\n#define EXT4_SYNC_FL\t\t\t0x00000008 /* Synchronous updates */\n#define EXT4_IMMUTABLE_FL\t\t0x00000010 /* Immutable file */\n#define EXT4_APPEND_FL\t\t\t0x00000020 /* writes to file may only append */\n#define EXT4_NODUMP_FL\t\t\t0x00000040 /* do not dump file */\n#define EXT4_NOATIME_FL\t\t\t0x00000080 /* do not update atime */\n/* Reserved for compression usage... */\n#define EXT4_DIRTY_FL\t\t\t0x00000100\n#define EXT4_COMPRBLK_FL\t\t0x00000200 /* One or more compressed clusters */\n#define EXT4_NOCOMPR_FL\t\t\t0x00000400 /* Don't compress */\n#define EXT4_ECOMPR_FL\t\t\t0x00000800 /* Compression error */\n/* End compression flags --- maybe not all used */\n#define EXT4_INDEX_FL\t\t\t0x00001000 /* hash-indexed directory */\n#define EXT4_IMAGIC_FL\t\t\t0x00002000 /* AFS directory */\n#define EXT4_JOURNAL_DATA_FL\t\t0x00004000 /* file data should be journaled */\n#define EXT4_NOTAIL_FL\t\t\t0x00008000 /* file tail should not be merged */\n#define EXT4_DIRSYNC_FL\t\t\t0x00010000 /* dirsync behaviour (directories only) */\n#define EXT4_TOPDIR_FL\t\t\t0x00020000 /* Top of directory hierarchies*/\n#define EXT4_HUGE_FILE_FL               0x00040000 /* Set to each huge file */\n#define EXT4_EXTENTS_FL\t\t\t0x00080000 /* Inode uses extents */\n#define EXT4_EA_INODE_FL\t        0x00200000 /* Inode used for large EA */\n#define EXT4_EOFBLOCKS_FL\t\t0x00400000 /* Blocks allocated beyond EOF */\n#define EXT4_RESERVED_FL\t\t0x80000000 /* reserved for ext4 lib */\n\n#define EXT4_FL_USER_VISIBLE\t\t0x004BDFFF /* User visible flags */\n#define EXT4_FL_USER_MODIFIABLE\t\t0x004B80FF /* User modifiable flags */\n\n/* Flags that should be inherited by new inodes from their parent. */\n#define EXT4_FL_INHERITED (EXT4_SECRM_FL | EXT4_UNRM_FL | EXT4_COMPR_FL |\\\n\t\t\t   EXT4_SYNC_FL | EXT4_IMMUTABLE_FL | EXT4_APPEND_FL |\\\n\t\t\t   EXT4_NODUMP_FL | EXT4_NOATIME_FL |\\\n\t\t\t   EXT4_NOCOMPR_FL | EXT4_JOURNAL_DATA_FL |\\\n\t\t\t   EXT4_NOTAIL_FL | EXT4_DIRSYNC_FL)\n\n/* Flags that are appropriate for regular files (all but dir-specific ones). */\n#define EXT4_REG_FLMASK (~(EXT4_DIRSYNC_FL | EXT4_TOPDIR_FL))\n\n/* Flags that are appropriate for non-directories/regular files. */\n#define EXT4_OTHER_FLMASK (EXT4_NODUMP_FL | EXT4_NOATIME_FL)\n\n/* Mask out flags that are inappropriate for the given type of inode. */\nstatic inline __u32 ext4_mask_flags(umode_t mode, __u32 flags)\n{\n\tif (S_ISDIR(mode))\n\t\treturn flags;\n\telse if (S_ISREG(mode))\n\t\treturn flags & EXT4_REG_FLMASK;\n\telse\n\t\treturn flags & EXT4_OTHER_FLMASK;\n}\n\n/* Used to pass group descriptor data when online resize is done */\nstruct ext4_new_group_input {\n\t__u32 group;\t\t/* Group number for this data */\n\t__u64 block_bitmap;\t/* Absolute block number of block bitmap */\n\t__u64 inode_bitmap;\t/* Absolute block number of inode bitmap */\n\t__u64 inode_table;\t/* Absolute block number of inode table start */\n\t__u32 blocks_count;\t/* Total number of blocks in this group */\n\t__u16 reserved_blocks;\t/* Number of reserved blocks in this group */\n\t__u16 unused;\n};\n\n/* The struct ext4_new_group_input in kernel space, with free_blocks_count */\nstruct ext4_new_group_data {\n\t__u32 group;\n\t__u64 block_bitmap;\n\t__u64 inode_bitmap;\n\t__u64 inode_table;\n\t__u32 blocks_count;\n\t__u16 reserved_blocks;\n\t__u16 unused;\n\t__u32 free_blocks_count;\n};\n\n/*\n * Flags used by ext4_get_blocks()\n */\n\t/* Allocate any needed blocks and/or convert an unitialized\n\t   extent to be an initialized ext4 */\n#define EXT4_GET_BLOCKS_CREATE\t\t\t0x0001\n\t/* Request the creation of an unitialized extent */\n#define EXT4_GET_BLOCKS_UNINIT_EXT\t\t0x0002\n#define EXT4_GET_BLOCKS_CREATE_UNINIT_EXT\t(EXT4_GET_BLOCKS_UNINIT_EXT|\\\n\t\t\t\t\t\t EXT4_GET_BLOCKS_CREATE)\n\t/* Caller is from the delayed allocation writeout path,\n\t   so set the magic i_delalloc_reserve_flag after taking the \n\t   inode allocation semaphore for */\n#define EXT4_GET_BLOCKS_DELALLOC_RESERVE\t0x0004\n\t/* caller is from the direct IO path, request to creation of an\n\tunitialized extents if not allocated, split the uninitialized\n\textent if blocks has been preallocated already*/\n#define EXT4_GET_BLOCKS_PRE_IO\t\t\t0x0008\n#define EXT4_GET_BLOCKS_CONVERT\t\t\t0x0010\n#define EXT4_GET_BLOCKS_IO_CREATE_EXT\t\t(EXT4_GET_BLOCKS_PRE_IO|\\\n\t\t\t\t\t EXT4_GET_BLOCKS_CREATE_UNINIT_EXT)\n\t/* Convert extent to initialized after IO complete */\n#define EXT4_GET_BLOCKS_IO_CONVERT_EXT\t\t(EXT4_GET_BLOCKS_CONVERT|\\\n\t\t\t\t\t EXT4_GET_BLOCKS_CREATE_UNINIT_EXT)\n\n/*\n * Flags used by ext4_free_blocks\n */\n#define EXT4_FREE_BLOCKS_METADATA\t0x0001\n#define EXT4_FREE_BLOCKS_FORGET\t\t0x0002\n#define EXT4_FREE_BLOCKS_VALIDATED\t0x0004\n\n/*\n * ioctl commands\n */\n#define\tEXT4_IOC_GETFLAGS\t\tFS_IOC_GETFLAGS\n#define\tEXT4_IOC_SETFLAGS\t\tFS_IOC_SETFLAGS\n#define\tEXT4_IOC_GETVERSION\t\t_IOR('f', 3, long)\n#define\tEXT4_IOC_SETVERSION\t\t_IOW('f', 4, long)\n#define\tEXT4_IOC_GETVERSION_OLD\t\tFS_IOC_GETVERSION\n#define\tEXT4_IOC_SETVERSION_OLD\t\tFS_IOC_SETVERSION\n#ifdef CONFIG_JBD2_DEBUG\n#define EXT4_IOC_WAIT_FOR_READONLY\t_IOR('f', 99, long)\n#endif\n#define EXT4_IOC_GETRSVSZ\t\t_IOR('f', 5, long)\n#define EXT4_IOC_SETRSVSZ\t\t_IOW('f', 6, long)\n#define EXT4_IOC_GROUP_EXTEND\t\t_IOW('f', 7, unsigned long)\n#define EXT4_IOC_GROUP_ADD\t\t_IOW('f', 8, struct ext4_new_group_input)\n#define EXT4_IOC_MIGRATE\t\t_IO('f', 9)\n /* note ioctl 10 reserved for an early version of the FIEMAP ioctl */\n /* note ioctl 11 reserved for filesystem-independent FIEMAP ioctl */\n#define EXT4_IOC_ALLOC_DA_BLKS\t\t_IO('f', 12)\n#define EXT4_IOC_MOVE_EXT\t\t_IOWR('f', 15, struct move_extent)\n\n/*\n * ioctl commands in 32 bit emulation\n */\n#define EXT4_IOC32_GETFLAGS\t\tFS_IOC32_GETFLAGS\n#define EXT4_IOC32_SETFLAGS\t\tFS_IOC32_SETFLAGS\n#define EXT4_IOC32_GETVERSION\t\t_IOR('f', 3, int)\n#define EXT4_IOC32_SETVERSION\t\t_IOW('f', 4, int)\n#define EXT4_IOC32_GETRSVSZ\t\t_IOR('f', 5, int)\n#define EXT4_IOC32_SETRSVSZ\t\t_IOW('f', 6, int)\n#define EXT4_IOC32_GROUP_EXTEND\t\t_IOW('f', 7, unsigned int)\n#ifdef CONFIG_JBD2_DEBUG\n#define EXT4_IOC32_WAIT_FOR_READONLY\t_IOR('f', 99, int)\n#endif\n#define EXT4_IOC32_GETVERSION_OLD\tFS_IOC32_GETVERSION\n#define EXT4_IOC32_SETVERSION_OLD\tFS_IOC32_SETVERSION\n\n\n/*\n *  Mount options\n */\nstruct ext4_mount_options {\n\tunsigned long s_mount_opt;\n\tuid_t s_resuid;\n\tgid_t s_resgid;\n\tunsigned long s_commit_interval;\n\tu32 s_min_batch_time, s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\tint s_jquota_fmt;\n\tchar *s_qf_names[MAXQUOTAS];\n#endif\n};\n\n/* Max physical block we can addres w/o extents */\n#define EXT4_MAX_BLOCK_FILE_PHYS\t0xFFFFFFFF\n\n/*\n * Structure of an inode on the disk\n */\nstruct ext4_inode {\n\t__le16\ti_mode;\t\t/* File mode */\n\t__le16\ti_uid;\t\t/* Low 16 bits of Owner Uid */\n\t__le32\ti_size_lo;\t/* Size in bytes */\n\t__le32\ti_atime;\t/* Access time */\n\t__le32\ti_ctime;\t/* Inode Change time */\n\t__le32\ti_mtime;\t/* Modification time */\n\t__le32\ti_dtime;\t/* Deletion Time */\n\t__le16\ti_gid;\t\t/* Low 16 bits of Group Id */\n\t__le16\ti_links_count;\t/* Links count */\n\t__le32\ti_blocks_lo;\t/* Blocks count */\n\t__le32\ti_flags;\t/* File flags */\n\tunion {\n\t\tstruct {\n\t\t\t__le32  l_i_version;\n\t\t} linux1;\n\t\tstruct {\n\t\t\t__u32  h_i_translator;\n\t\t} hurd1;\n\t\tstruct {\n\t\t\t__u32  m_i_reserved1;\n\t\t} masix1;\n\t} osd1;\t\t\t\t/* OS dependent 1 */\n\t__le32\ti_block[EXT4_N_BLOCKS];/* Pointers to blocks */\n\t__le32\ti_generation;\t/* File version (for NFS) */\n\t__le32\ti_file_acl_lo;\t/* File ACL */\n\t__le32\ti_size_high;\n\t__le32\ti_obso_faddr;\t/* Obsoleted fragment address */\n\tunion {\n\t\tstruct {\n\t\t\t__le16\tl_i_blocks_high; /* were l_i_reserved1 */\n\t\t\t__le16\tl_i_file_acl_high;\n\t\t\t__le16\tl_i_uid_high;\t/* these 2 fields */\n\t\t\t__le16\tl_i_gid_high;\t/* were reserved2[0] */\n\t\t\t__u32\tl_i_reserved2;\n\t\t} linux2;\n\t\tstruct {\n\t\t\t__le16\th_i_reserved1;\t/* Obsoleted fragment number/size which are removed in ext4 */\n\t\t\t__u16\th_i_mode_high;\n\t\t\t__u16\th_i_uid_high;\n\t\t\t__u16\th_i_gid_high;\n\t\t\t__u32\th_i_author;\n\t\t} hurd2;\n\t\tstruct {\n\t\t\t__le16\th_i_reserved1;\t/* Obsoleted fragment number/size which are removed in ext4 */\n\t\t\t__le16\tm_i_file_acl_high;\n\t\t\t__u32\tm_i_reserved2[2];\n\t\t} masix2;\n\t} osd2;\t\t\t\t/* OS dependent 2 */\n\t__le16\ti_extra_isize;\n\t__le16\ti_pad1;\n\t__le32  i_ctime_extra;  /* extra Change time      (nsec << 2 | epoch) */\n\t__le32  i_mtime_extra;  /* extra Modification time(nsec << 2 | epoch) */\n\t__le32  i_atime_extra;  /* extra Access time      (nsec << 2 | epoch) */\n\t__le32  i_crtime;       /* File Creation time */\n\t__le32  i_crtime_extra; /* extra FileCreationtime (nsec << 2 | epoch) */\n\t__le32  i_version_hi;\t/* high 32 bits for 64-bit version */\n};\n\nstruct move_extent {\n\t__u32 reserved;\t\t/* should be zero */\n\t__u32 donor_fd;\t\t/* donor file descriptor */\n\t__u64 orig_start;\t/* logical start offset in block for orig */\n\t__u64 donor_start;\t/* logical start offset in block for donor */\n\t__u64 len;\t\t/* block length to be moved */\n\t__u64 moved_len;\t/* moved block length */\n};\n\n#define EXT4_EPOCH_BITS 2\n#define EXT4_EPOCH_MASK ((1 << EXT4_EPOCH_BITS) - 1)\n#define EXT4_NSEC_MASK  (~0UL << EXT4_EPOCH_BITS)\n\n/*\n * Extended fields will fit into an inode if the filesystem was formatted\n * with large inodes (-I 256 or larger) and there are not currently any EAs\n * consuming all of the available space. For new inodes we always reserve\n * enough space for the kernel's known extended fields, but for inodes\n * created with an old kernel this might not have been the case. None of\n * the extended inode fields is critical for correct filesystem operation.\n * This macro checks if a certain field fits in the inode. Note that\n * inode-size = GOOD_OLD_INODE_SIZE + i_extra_isize\n */\n#define EXT4_FITS_IN_INODE(ext4_inode, einode, field)\t\\\n\t((offsetof(typeof(*ext4_inode), field) +\t\\\n\t  sizeof((ext4_inode)->field))\t\t\t\\\n\t<= (EXT4_GOOD_OLD_INODE_SIZE +\t\t\t\\\n\t    (einode)->i_extra_isize))\t\t\t\\\n\nstatic inline __le32 ext4_encode_extra_time(struct timespec *time)\n{\n       return cpu_to_le32((sizeof(time->tv_sec) > 4 ?\n\t\t\t   (time->tv_sec >> 32) & EXT4_EPOCH_MASK : 0) |\n                          ((time->tv_nsec << EXT4_EPOCH_BITS) & EXT4_NSEC_MASK));\n}\n\nstatic inline void ext4_decode_extra_time(struct timespec *time, __le32 extra)\n{\n       if (sizeof(time->tv_sec) > 4)\n\t       time->tv_sec |= (__u64)(le32_to_cpu(extra) & EXT4_EPOCH_MASK)\n\t\t\t       << 32;\n       time->tv_nsec = (le32_to_cpu(extra) & EXT4_NSEC_MASK) >> EXT4_EPOCH_BITS;\n}\n\n#define EXT4_INODE_SET_XTIME(xtime, inode, raw_inode)\t\t\t       \\\ndo {\t\t\t\t\t\t\t\t\t       \\\n\t(raw_inode)->xtime = cpu_to_le32((inode)->xtime.tv_sec);\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, EXT4_I(inode), xtime ## _extra))     \\\n\t\t(raw_inode)->xtime ## _extra =\t\t\t\t       \\\n\t\t\t\text4_encode_extra_time(&(inode)->xtime);       \\\n} while (0)\n\n#define EXT4_EINODE_SET_XTIME(xtime, einode, raw_inode)\t\t\t       \\\ndo {\t\t\t\t\t\t\t\t\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime))\t\t       \\\n\t\t(raw_inode)->xtime = cpu_to_le32((einode)->xtime.tv_sec);      \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime ## _extra))\t       \\\n\t\t(raw_inode)->xtime ## _extra =\t\t\t\t       \\\n\t\t\t\text4_encode_extra_time(&(einode)->xtime);      \\\n} while (0)\n\n#define EXT4_INODE_GET_XTIME(xtime, inode, raw_inode)\t\t\t       \\\ndo {\t\t\t\t\t\t\t\t\t       \\\n\t(inode)->xtime.tv_sec = (signed)le32_to_cpu((raw_inode)->xtime);       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, EXT4_I(inode), xtime ## _extra))     \\\n\t\text4_decode_extra_time(&(inode)->xtime,\t\t\t       \\\n\t\t\t\t       raw_inode->xtime ## _extra);\t       \\\n} while (0)\n\n#define EXT4_EINODE_GET_XTIME(xtime, einode, raw_inode)\t\t\t       \\\ndo {\t\t\t\t\t\t\t\t\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime))\t\t       \\\n\t\t(einode)->xtime.tv_sec = \t\t\t\t       \\\n\t\t\t(signed)le32_to_cpu((raw_inode)->xtime);\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime ## _extra))\t       \\\n\t\text4_decode_extra_time(&(einode)->xtime,\t\t       \\\n\t\t\t\t       raw_inode->xtime ## _extra);\t       \\\n} while (0)\n\n#define i_disk_version osd1.linux1.l_i_version\n\n#if defined(__KERNEL__) || defined(__linux__)\n#define i_reserved1\tosd1.linux1.l_i_reserved1\n#define i_file_acl_high\tosd2.linux2.l_i_file_acl_high\n#define i_blocks_high\tosd2.linux2.l_i_blocks_high\n#define i_uid_low\ti_uid\n#define i_gid_low\ti_gid\n#define i_uid_high\tosd2.linux2.l_i_uid_high\n#define i_gid_high\tosd2.linux2.l_i_gid_high\n#define i_reserved2\tosd2.linux2.l_i_reserved2\n\n#elif defined(__GNU__)\n\n#define i_translator\tosd1.hurd1.h_i_translator\n#define i_uid_high\tosd2.hurd2.h_i_uid_high\n#define i_gid_high\tosd2.hurd2.h_i_gid_high\n#define i_author\tosd2.hurd2.h_i_author\n\n#elif defined(__masix__)\n\n#define i_reserved1\tosd1.masix1.m_i_reserved1\n#define i_file_acl_high\tosd2.masix2.m_i_file_acl_high\n#define i_reserved2\tosd2.masix2.m_i_reserved2\n\n#endif /* defined(__KERNEL__) || defined(__linux__) */\n\n/*\n * storage for cached extent\n */\nstruct ext4_ext_cache {\n\text4_fsblk_t\tec_start;\n\text4_lblk_t\tec_block;\n\t__u32\t\tec_len; /* must be 32bit to return holes */\n\t__u32\t\tec_type;\n};\n\n/*\n * fourth extended file system inode data in memory\n */\nstruct ext4_inode_info {\n\t__le32\ti_data[15];\t/* unconverted */\n\t__u32\ti_flags;\n\text4_fsblk_t\ti_file_acl;\n\t__u32\ti_dtime;\n\n\t/*\n\t * i_block_group is the number of the block group which contains\n\t * this file's inode.  Constant across the lifetime of the inode,\n\t * it is ued for making block allocation decisions - we try to\n\t * place a file's data blocks near its inode block, and new inodes\n\t * near to their parent directory's inode.\n\t */\n\text4_group_t\ti_block_group;\n\tunsigned long\ti_state_flags;\t\t/* Dynamic state flags */\n\n\text4_lblk_t\t\ti_dir_start_lookup;\n#ifdef CONFIG_EXT4_FS_XATTR\n\t/*\n\t * Extended attributes can be read independently of the main file\n\t * data. Taking i_mutex even when reading would cause contention\n\t * between readers of EAs and writers of regular file data, so\n\t * instead we synchronize on xattr_sem when reading or changing\n\t * EAs.\n\t */\n\tstruct rw_semaphore xattr_sem;\n#endif\n\n\tstruct list_head i_orphan;\t/* unlinked but open inodes */\n\n\t/*\n\t * i_disksize keeps track of what the inode size is ON DISK, not\n\t * in memory.  During truncate, i_size is set to the new size by\n\t * the VFS prior to calling ext4_truncate(), but the filesystem won't\n\t * set i_disksize to 0 until the truncate is actually under way.\n\t *\n\t * The intent is that i_disksize always represents the blocks which\n\t * are used by this file.  This allows recovery to restart truncate\n\t * on orphans if we crash during truncate.  We actually write i_disksize\n\t * into the on-disk inode when writing inodes out, instead of i_size.\n\t *\n\t * The only time when i_disksize and i_size may be different is when\n\t * a truncate is in progress.  The only things which change i_disksize\n\t * are ext4_get_block (growth) and ext4_truncate (shrinkth).\n\t */\n\tloff_t\ti_disksize;\n\n\t/*\n\t * i_data_sem is for serialising ext4_truncate() against\n\t * ext4_getblock().  In the 2.4 ext2 design, great chunks of inode's\n\t * data tree are chopped off during truncate. We can't do that in\n\t * ext4 because whenever we perform intermediate commits during\n\t * truncate, the inode and all the metadata blocks *must* be in a\n\t * consistent state which allows truncation of the orphans to restart\n\t * during recovery.  Hence we must fix the get_block-vs-truncate race\n\t * by other means, so we have i_data_sem.\n\t */\n\tstruct rw_semaphore i_data_sem;\n\tstruct inode vfs_inode;\n\tstruct jbd2_inode jinode;\n\n\tstruct ext4_ext_cache i_cached_extent;\n\t/*\n\t * File creation time. Its function is same as that of\n\t * struct timespec i_{a,c,m}time in the generic inode.\n\t */\n\tstruct timespec i_crtime;\n\n\t/* mballoc */\n\tstruct list_head i_prealloc_list;\n\tspinlock_t i_prealloc_lock;\n\n\t/* ialloc */\n\text4_group_t\ti_last_alloc_group;\n\n\t/* allocation reservation info for delalloc */\n\tunsigned int i_reserved_data_blocks;\n\tunsigned int i_reserved_meta_blocks;\n\tunsigned int i_allocated_meta_blocks;\n\tunsigned short i_delalloc_reserved_flag;\n\tsector_t i_da_metadata_calc_last_lblock;\n\tint i_da_metadata_calc_len;\n\n\t/* on-disk additional length */\n\t__u16 i_extra_isize;\n\n\tspinlock_t i_block_reservation_lock;\n#ifdef CONFIG_QUOTA\n\t/* quota space reservation, managed internally by quota code */\n\tqsize_t i_reserved_quota;\n#endif\n\n\t/* completed IOs that might need unwritten extents handling */\n\tstruct list_head i_completed_io_list;\n\tspinlock_t i_completed_io_lock;\n\t/* current io_end structure for async DIO write*/\n\text4_io_end_t *cur_aio_dio;\n\n\t/*\n\t * Transactions that contain inode's metadata needed to complete\n\t * fsync and fdatasync, respectively.\n\t */\n\ttid_t i_sync_tid;\n\ttid_t i_datasync_tid;\n};\n\n/*\n * File system states\n */\n#define\tEXT4_VALID_FS\t\t\t0x0001\t/* Unmounted cleanly */\n#define\tEXT4_ERROR_FS\t\t\t0x0002\t/* Errors detected */\n#define\tEXT4_ORPHAN_FS\t\t\t0x0004\t/* Orphans being recovered */\n\n/*\n * Misc. filesystem flags\n */\n#define EXT2_FLAGS_SIGNED_HASH\t\t0x0001  /* Signed dirhash in use */\n#define EXT2_FLAGS_UNSIGNED_HASH\t0x0002  /* Unsigned dirhash in use */\n#define EXT2_FLAGS_TEST_FILESYS\t\t0x0004\t/* to test development code */\n\n/*\n * Mount flags\n */\n#define EXT4_MOUNT_OLDALLOC\t\t0x00002  /* Don't use the new Orlov allocator */\n#define EXT4_MOUNT_GRPID\t\t0x00004\t/* Create files with directory's group */\n#define EXT4_MOUNT_DEBUG\t\t0x00008\t/* Some debugging messages */\n#define EXT4_MOUNT_ERRORS_CONT\t\t0x00010\t/* Continue on errors */\n#define EXT4_MOUNT_ERRORS_RO\t\t0x00020\t/* Remount fs ro on errors */\n#define EXT4_MOUNT_ERRORS_PANIC\t\t0x00040\t/* Panic on errors */\n#define EXT4_MOUNT_MINIX_DF\t\t0x00080\t/* Mimics the Minix statfs */\n#define EXT4_MOUNT_NOLOAD\t\t0x00100\t/* Don't use existing journal*/\n#define EXT4_MOUNT_DATA_FLAGS\t\t0x00C00\t/* Mode for data writes: */\n#define EXT4_MOUNT_JOURNAL_DATA\t\t0x00400\t/* Write data to journal */\n#define EXT4_MOUNT_ORDERED_DATA\t\t0x00800\t/* Flush data before commit */\n#define EXT4_MOUNT_WRITEBACK_DATA\t0x00C00\t/* No data ordering */\n#define EXT4_MOUNT_UPDATE_JOURNAL\t0x01000\t/* Update the journal format */\n#define EXT4_MOUNT_NO_UID32\t\t0x02000  /* Disable 32-bit UIDs */\n#define EXT4_MOUNT_XATTR_USER\t\t0x04000\t/* Extended user attributes */\n#define EXT4_MOUNT_POSIX_ACL\t\t0x08000\t/* POSIX Access Control Lists */\n#define EXT4_MOUNT_NO_AUTO_DA_ALLOC\t0x10000\t/* No auto delalloc mapping */\n#define EXT4_MOUNT_BARRIER\t\t0x20000 /* Use block barriers */\n#define EXT4_MOUNT_NOBH\t\t\t0x40000 /* No bufferheads */\n#define EXT4_MOUNT_QUOTA\t\t0x80000 /* Some quota option set */\n#define EXT4_MOUNT_USRQUOTA\t\t0x100000 /* \"old\" user quota */\n#define EXT4_MOUNT_GRPQUOTA\t\t0x200000 /* \"old\" group quota */\n#define EXT4_MOUNT_DIOREAD_NOLOCK\t0x400000 /* Enable support for dio read nolocking */\n#define EXT4_MOUNT_JOURNAL_CHECKSUM\t0x800000 /* Journal checksums */\n#define EXT4_MOUNT_JOURNAL_ASYNC_COMMIT\t0x1000000 /* Journal Async Commit */\n#define EXT4_MOUNT_I_VERSION            0x2000000 /* i_version support */\n#define EXT4_MOUNT_DELALLOC\t\t0x8000000 /* Delalloc support */\n#define EXT4_MOUNT_DATA_ERR_ABORT\t0x10000000 /* Abort on file data write */\n#define EXT4_MOUNT_BLOCK_VALIDITY\t0x20000000 /* Block validity checking */\n#define EXT4_MOUNT_DISCARD\t\t0x40000000 /* Issue DISCARD requests */\n\n#define clear_opt(o, opt)\t\to &= ~EXT4_MOUNT_##opt\n#define set_opt(o, opt)\t\t\to |= EXT4_MOUNT_##opt\n#define test_opt(sb, opt)\t\t(EXT4_SB(sb)->s_mount_opt & \\\n\t\t\t\t\t EXT4_MOUNT_##opt)\n\n#define ext4_set_bit\t\t\text2_set_bit\n#define ext4_set_bit_atomic\t\text2_set_bit_atomic\n#define ext4_clear_bit\t\t\text2_clear_bit\n#define ext4_clear_bit_atomic\t\text2_clear_bit_atomic\n#define ext4_test_bit\t\t\text2_test_bit\n#define ext4_find_first_zero_bit\text2_find_first_zero_bit\n#define ext4_find_next_zero_bit\t\text2_find_next_zero_bit\n#define ext4_find_next_bit\t\text2_find_next_bit\n\n/*\n * Maximal mount counts between two filesystem checks\n */\n#define EXT4_DFL_MAX_MNT_COUNT\t\t20\t/* Allow 20 mounts */\n#define EXT4_DFL_CHECKINTERVAL\t\t0\t/* Don't use interval check */\n\n/*\n * Behaviour when detecting errors\n */\n#define EXT4_ERRORS_CONTINUE\t\t1\t/* Continue execution */\n#define EXT4_ERRORS_RO\t\t\t2\t/* Remount fs read-only */\n#define EXT4_ERRORS_PANIC\t\t3\t/* Panic */\n#define EXT4_ERRORS_DEFAULT\t\tEXT4_ERRORS_CONTINUE\n\n/*\n * Structure of the super block\n */\nstruct ext4_super_block {\n/*00*/\t__le32\ts_inodes_count;\t\t/* Inodes count */\n\t__le32\ts_blocks_count_lo;\t/* Blocks count */\n\t__le32\ts_r_blocks_count_lo;\t/* Reserved blocks count */\n\t__le32\ts_free_blocks_count_lo;\t/* Free blocks count */\n/*10*/\t__le32\ts_free_inodes_count;\t/* Free inodes count */\n\t__le32\ts_first_data_block;\t/* First Data Block */\n\t__le32\ts_log_block_size;\t/* Block size */\n\t__le32\ts_obso_log_frag_size;\t/* Obsoleted fragment size */\n/*20*/\t__le32\ts_blocks_per_group;\t/* # Blocks per group */\n\t__le32\ts_obso_frags_per_group;\t/* Obsoleted fragments per group */\n\t__le32\ts_inodes_per_group;\t/* # Inodes per group */\n\t__le32\ts_mtime;\t\t/* Mount time */\n/*30*/\t__le32\ts_wtime;\t\t/* Write time */\n\t__le16\ts_mnt_count;\t\t/* Mount count */\n\t__le16\ts_max_mnt_count;\t/* Maximal mount count */\n\t__le16\ts_magic;\t\t/* Magic signature */\n\t__le16\ts_state;\t\t/* File system state */\n\t__le16\ts_errors;\t\t/* Behaviour when detecting errors */\n\t__le16\ts_minor_rev_level;\t/* minor revision level */\n/*40*/\t__le32\ts_lastcheck;\t\t/* time of last check */\n\t__le32\ts_checkinterval;\t/* max. time between checks */\n\t__le32\ts_creator_os;\t\t/* OS */\n\t__le32\ts_rev_level;\t\t/* Revision level */\n/*50*/\t__le16\ts_def_resuid;\t\t/* Default uid for reserved blocks */\n\t__le16\ts_def_resgid;\t\t/* Default gid for reserved blocks */\n\t/*\n\t * These fields are for EXT4_DYNAMIC_REV superblocks only.\n\t *\n\t * Note: the difference between the compatible feature set and\n\t * the incompatible feature set is that if there is a bit set\n\t * in the incompatible feature set that the kernel doesn't\n\t * know about, it should refuse to mount the filesystem.\n\t *\n\t * e2fsck's requirements are more strict; if it doesn't know\n\t * about a feature in either the compatible or incompatible\n\t * feature set, it must abort and not try to meddle with\n\t * things it doesn't understand...\n\t */\n\t__le32\ts_first_ino;\t\t/* First non-reserved inode */\n\t__le16  s_inode_size;\t\t/* size of inode structure */\n\t__le16\ts_block_group_nr;\t/* block group # of this superblock */\n\t__le32\ts_feature_compat;\t/* compatible feature set */\n/*60*/\t__le32\ts_feature_incompat;\t/* incompatible feature set */\n\t__le32\ts_feature_ro_compat;\t/* readonly-compatible feature set */\n/*68*/\t__u8\ts_uuid[16];\t\t/* 128-bit uuid for volume */\n/*78*/\tchar\ts_volume_name[16];\t/* volume name */\n/*88*/\tchar\ts_last_mounted[64];\t/* directory where last mounted */\n/*C8*/\t__le32\ts_algorithm_usage_bitmap; /* For compression */\n\t/*\n\t * Performance hints.  Directory preallocation should only\n\t * happen if the EXT4_FEATURE_COMPAT_DIR_PREALLOC flag is on.\n\t */\n\t__u8\ts_prealloc_blocks;\t/* Nr of blocks to try to preallocate*/\n\t__u8\ts_prealloc_dir_blocks;\t/* Nr to preallocate for dirs */\n\t__le16\ts_reserved_gdt_blocks;\t/* Per group desc for online growth */\n\t/*\n\t * Journaling support valid if EXT4_FEATURE_COMPAT_HAS_JOURNAL set.\n\t */\n/*D0*/\t__u8\ts_journal_uuid[16];\t/* uuid of journal superblock */\n/*E0*/\t__le32\ts_journal_inum;\t\t/* inode number of journal file */\n\t__le32\ts_journal_dev;\t\t/* device number of journal file */\n\t__le32\ts_last_orphan;\t\t/* start of list of inodes to delete */\n\t__le32\ts_hash_seed[4];\t\t/* HTREE hash seed */\n\t__u8\ts_def_hash_version;\t/* Default hash version to use */\n\t__u8\ts_reserved_char_pad;\n\t__le16  s_desc_size;\t\t/* size of group descriptor */\n/*100*/\t__le32\ts_default_mount_opts;\n\t__le32\ts_first_meta_bg;\t/* First metablock block group */\n\t__le32\ts_mkfs_time;\t\t/* When the filesystem was created */\n\t__le32\ts_jnl_blocks[17];\t/* Backup of the journal inode */\n\t/* 64bit support valid if EXT4_FEATURE_COMPAT_64BIT */\n/*150*/\t__le32\ts_blocks_count_hi;\t/* Blocks count */\n\t__le32\ts_r_blocks_count_hi;\t/* Reserved blocks count */\n\t__le32\ts_free_blocks_count_hi;\t/* Free blocks count */\n\t__le16\ts_min_extra_isize;\t/* All inodes have at least # bytes */\n\t__le16\ts_want_extra_isize; \t/* New inodes should reserve # bytes */\n\t__le32\ts_flags;\t\t/* Miscellaneous flags */\n\t__le16  s_raid_stride;\t\t/* RAID stride */\n\t__le16  s_mmp_interval;         /* # seconds to wait in MMP checking */\n\t__le64  s_mmp_block;            /* Block for multi-mount protection */\n\t__le32  s_raid_stripe_width;    /* blocks on all data disks (N*stride)*/\n\t__u8\ts_log_groups_per_flex;  /* FLEX_BG group size */\n\t__u8\ts_reserved_char_pad2;\n\t__le16  s_reserved_pad;\n\t__le64\ts_kbytes_written;\t/* nr of lifetime kilobytes written */\n\t__u32   s_reserved[160];        /* Padding to the end of the block */\n};\n\n#ifdef __KERNEL__\n\n/*\n * run-time mount flags\n */\n#define EXT4_MF_MNTDIR_SAMPLED\t0x0001\n#define EXT4_MF_FS_ABORTED\t0x0002\t/* Fatal error detected */\n\n/*\n * fourth extended-fs super-block data in memory\n */\nstruct ext4_sb_info {\n\tunsigned long s_desc_size;\t/* Size of a group descriptor in bytes */\n\tunsigned long s_inodes_per_block;/* Number of inodes per block */\n\tunsigned long s_blocks_per_group;/* Number of blocks in a group */\n\tunsigned long s_inodes_per_group;/* Number of inodes in a group */\n\tunsigned long s_itb_per_group;\t/* Number of inode table blocks per group */\n\tunsigned long s_gdb_count;\t/* Number of group descriptor blocks */\n\tunsigned long s_desc_per_block;\t/* Number of group descriptors per block */\n\text4_group_t s_groups_count;\t/* Number of groups in the fs */\n\text4_group_t s_blockfile_groups;/* Groups acceptable for non-extent files */\n\tunsigned long s_overhead_last;  /* Last calculated overhead */\n\tunsigned long s_blocks_last;    /* Last seen block count */\n\tloff_t s_bitmap_maxbytes;\t/* max bytes for bitmap files */\n\tstruct buffer_head * s_sbh;\t/* Buffer containing the super block */\n\tstruct ext4_super_block *s_es;\t/* Pointer to the super block in the buffer */\n\tstruct buffer_head **s_group_desc;\n\tunsigned int s_mount_opt;\n\tunsigned int s_mount_flags;\n\text4_fsblk_t s_sb_block;\n\tuid_t s_resuid;\n\tgid_t s_resgid;\n\tunsigned short s_mount_state;\n\tunsigned short s_pad;\n\tint s_addr_per_block_bits;\n\tint s_desc_per_block_bits;\n\tint s_inode_size;\n\tint s_first_ino;\n\tunsigned int s_inode_readahead_blks;\n\tunsigned int s_inode_goal;\n\tspinlock_t s_next_gen_lock;\n\tu32 s_next_generation;\n\tu32 s_hash_seed[4];\n\tint s_def_hash_version;\n\tint s_hash_unsigned;\t/* 3 if hash should be signed, 0 if not */\n\tstruct percpu_counter s_freeblocks_counter;\n\tstruct percpu_counter s_freeinodes_counter;\n\tstruct percpu_counter s_dirs_counter;\n\tstruct percpu_counter s_dirtyblocks_counter;\n\tstruct blockgroup_lock *s_blockgroup_lock;\n\tstruct proc_dir_entry *s_proc;\n\tstruct kobject s_kobj;\n\tstruct completion s_kobj_unregister;\n\n\t/* Journaling */\n\tstruct inode *s_journal_inode;\n\tstruct journal_s *s_journal;\n\tstruct list_head s_orphan;\n\tstruct mutex s_orphan_lock;\n\tstruct mutex s_resize_lock;\n\tunsigned long s_commit_interval;\n\tu32 s_max_batch_time;\n\tu32 s_min_batch_time;\n\tstruct block_device *journal_bdev;\n#ifdef CONFIG_JBD2_DEBUG\n\tstruct timer_list turn_ro_timer;\t/* For turning read-only (crash simulation) */\n\twait_queue_head_t ro_wait_queue;\t/* For people waiting for the fs to go read-only */\n#endif\n#ifdef CONFIG_QUOTA\n\tchar *s_qf_names[MAXQUOTAS];\t\t/* Names of quota files with journalled quota */\n\tint s_jquota_fmt;\t\t\t/* Format of quota to use */\n#endif\n\tunsigned int s_want_extra_isize; /* New inodes should reserve # bytes */\n\tstruct rb_root system_blks;\n\n#ifdef EXTENTS_STATS\n\t/* ext4 extents stats */\n\tunsigned long s_ext_min;\n\tunsigned long s_ext_max;\n\tunsigned long s_depth_max;\n\tspinlock_t s_ext_stats_lock;\n\tunsigned long s_ext_blocks;\n\tunsigned long s_ext_extents;\n#endif\n\n\t/* for buddy allocator */\n\tstruct ext4_group_info ***s_group_info;\n\tstruct inode *s_buddy_cache;\n\tlong s_blocks_reserved;\n\tspinlock_t s_reserve_lock;\n\tspinlock_t s_md_lock;\n\ttid_t s_last_transaction;\n\tunsigned short *s_mb_offsets;\n\tunsigned int *s_mb_maxs;\n\n\t/* tunables */\n\tunsigned long s_stripe;\n\tunsigned int s_mb_stream_request;\n\tunsigned int s_mb_max_to_scan;\n\tunsigned int s_mb_min_to_scan;\n\tunsigned int s_mb_stats;\n\tunsigned int s_mb_order2_reqs;\n\tunsigned int s_mb_group_prealloc;\n\tunsigned int s_max_writeback_mb_bump;\n\t/* where last allocation was done - for stream allocation */\n\tunsigned long s_mb_last_group;\n\tunsigned long s_mb_last_start;\n\n\t/* stats for buddy allocator */\n\tspinlock_t s_mb_pa_lock;\n\tatomic_t s_bal_reqs;\t/* number of reqs with len > 1 */\n\tatomic_t s_bal_success;\t/* we found long enough chunks */\n\tatomic_t s_bal_allocated;\t/* in blocks */\n\tatomic_t s_bal_ex_scanned;\t/* total extents scanned */\n\tatomic_t s_bal_goals;\t/* goal hits */\n\tatomic_t s_bal_breaks;\t/* too long searches */\n\tatomic_t s_bal_2orders;\t/* 2^order hits */\n\tspinlock_t s_bal_lock;\n\tunsigned long s_mb_buddies_generated;\n\tunsigned long long s_mb_generation_time;\n\tatomic_t s_mb_lost_chunks;\n\tatomic_t s_mb_preallocated;\n\tatomic_t s_mb_discarded;\n\tatomic_t s_lock_busy;\n\n\t/* locality groups */\n\tstruct ext4_locality_group *s_locality_groups;\n\n\t/* for write statistics */\n\tunsigned long s_sectors_written_start;\n\tu64 s_kbytes_written;\n\n\tunsigned int s_log_groups_per_flex;\n\tstruct flex_groups *s_flex_groups;\n\n\t/* workqueue for dio unwritten */\n\tstruct workqueue_struct *dio_unwritten_wq;\n};\n\nstatic inline struct ext4_sb_info *EXT4_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\nstatic inline struct ext4_inode_info *EXT4_I(struct inode *inode)\n{\n\treturn container_of(inode, struct ext4_inode_info, vfs_inode);\n}\n\nstatic inline struct timespec ext4_current_time(struct inode *inode)\n{\n\treturn (inode->i_sb->s_time_gran < NSEC_PER_SEC) ?\n\t\tcurrent_fs_time(inode->i_sb) : CURRENT_TIME_SEC;\n}\n\nstatic inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)\n{\n\treturn ino == EXT4_ROOT_INO ||\n\t\tino == EXT4_JOURNAL_INO ||\n\t\tino == EXT4_RESIZE_INO ||\n\t\t(ino >= EXT4_FIRST_INO(sb) &&\n\t\t ino <= le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count));\n}\n\n/*\n * Inode dynamic state flags\n */\nenum {\n\tEXT4_STATE_JDATA,\t\t/* journaled data exists */\n\tEXT4_STATE_NEW,\t\t\t/* inode is newly created */\n\tEXT4_STATE_XATTR,\t\t/* has in-inode xattrs */\n\tEXT4_STATE_NO_EXPAND,\t\t/* No space for expansion */\n\tEXT4_STATE_DA_ALLOC_CLOSE,\t/* Alloc DA blks on close */\n\tEXT4_STATE_EXT_MIGRATE,\t\t/* Inode is migrating */\n\tEXT4_STATE_DIO_UNWRITTEN,\t/* need convert on dio done*/\n};\n\nstatic inline int ext4_test_inode_state(struct inode *inode, int bit)\n{\n\treturn test_bit(bit, &EXT4_I(inode)->i_state_flags);\n}\n\nstatic inline void ext4_set_inode_state(struct inode *inode, int bit)\n{\n\tset_bit(bit, &EXT4_I(inode)->i_state_flags);\n}\n\nstatic inline void ext4_clear_inode_state(struct inode *inode, int bit)\n{\n\tclear_bit(bit, &EXT4_I(inode)->i_state_flags);\n}\n#else\n/* Assume that user mode programs are passing in an ext4fs superblock, not\n * a kernel struct super_block.  This will allow us to call the feature-test\n * macros from user land. */\n#define EXT4_SB(sb)\t(sb)\n#endif\n\n#define NEXT_ORPHAN(inode) EXT4_I(inode)->i_dtime\n\n/*\n * Codes for operating systems\n */\n#define EXT4_OS_LINUX\t\t0\n#define EXT4_OS_HURD\t\t1\n#define EXT4_OS_MASIX\t\t2\n#define EXT4_OS_FREEBSD\t\t3\n#define EXT4_OS_LITES\t\t4\n\n/*\n * Revision levels\n */\n#define EXT4_GOOD_OLD_REV\t0\t/* The good old (original) format */\n#define EXT4_DYNAMIC_REV\t1\t/* V2 format w/ dynamic inode sizes */\n\n#define EXT4_CURRENT_REV\tEXT4_GOOD_OLD_REV\n#define EXT4_MAX_SUPP_REV\tEXT4_DYNAMIC_REV\n\n#define EXT4_GOOD_OLD_INODE_SIZE 128\n\n/*\n * Feature set definitions\n */\n\n#define EXT4_HAS_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\t((EXT4_SB(sb)->s_es->s_feature_compat & cpu_to_le32(mask)) != 0)\n#define EXT4_HAS_RO_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\t((EXT4_SB(sb)->s_es->s_feature_ro_compat & cpu_to_le32(mask)) != 0)\n#define EXT4_HAS_INCOMPAT_FEATURE(sb,mask)\t\t\t\\\n\t((EXT4_SB(sb)->s_es->s_feature_incompat & cpu_to_le32(mask)) != 0)\n#define EXT4_SET_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_compat |= cpu_to_le32(mask)\n#define EXT4_SET_RO_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_ro_compat |= cpu_to_le32(mask)\n#define EXT4_SET_INCOMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_incompat |= cpu_to_le32(mask)\n#define EXT4_CLEAR_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_compat &= ~cpu_to_le32(mask)\n#define EXT4_CLEAR_RO_COMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_ro_compat &= ~cpu_to_le32(mask)\n#define EXT4_CLEAR_INCOMPAT_FEATURE(sb,mask)\t\t\t\\\n\tEXT4_SB(sb)->s_es->s_feature_incompat &= ~cpu_to_le32(mask)\n\n#define EXT4_FEATURE_COMPAT_DIR_PREALLOC\t0x0001\n#define EXT4_FEATURE_COMPAT_IMAGIC_INODES\t0x0002\n#define EXT4_FEATURE_COMPAT_HAS_JOURNAL\t\t0x0004\n#define EXT4_FEATURE_COMPAT_EXT_ATTR\t\t0x0008\n#define EXT4_FEATURE_COMPAT_RESIZE_INODE\t0x0010\n#define EXT4_FEATURE_COMPAT_DIR_INDEX\t\t0x0020\n\n#define EXT4_FEATURE_RO_COMPAT_SPARSE_SUPER\t0x0001\n#define EXT4_FEATURE_RO_COMPAT_LARGE_FILE\t0x0002\n#define EXT4_FEATURE_RO_COMPAT_BTREE_DIR\t0x0004\n#define EXT4_FEATURE_RO_COMPAT_HUGE_FILE        0x0008\n#define EXT4_FEATURE_RO_COMPAT_GDT_CSUM\t\t0x0010\n#define EXT4_FEATURE_RO_COMPAT_DIR_NLINK\t0x0020\n#define EXT4_FEATURE_RO_COMPAT_EXTRA_ISIZE\t0x0040\n\n#define EXT4_FEATURE_INCOMPAT_COMPRESSION\t0x0001\n#define EXT4_FEATURE_INCOMPAT_FILETYPE\t\t0x0002\n#define EXT4_FEATURE_INCOMPAT_RECOVER\t\t0x0004 /* Needs recovery */\n#define EXT4_FEATURE_INCOMPAT_JOURNAL_DEV\t0x0008 /* Journal device */\n#define EXT4_FEATURE_INCOMPAT_META_BG\t\t0x0010\n#define EXT4_FEATURE_INCOMPAT_EXTENTS\t\t0x0040 /* extents support */\n#define EXT4_FEATURE_INCOMPAT_64BIT\t\t0x0080\n#define EXT4_FEATURE_INCOMPAT_MMP               0x0100\n#define EXT4_FEATURE_INCOMPAT_FLEX_BG\t\t0x0200\n#define EXT4_FEATURE_INCOMPAT_EA_INODE\t\t0x0400 /* EA in inode */\n#define EXT4_FEATURE_INCOMPAT_DIRDATA\t\t0x1000 /* data in dirent */\n\n#define EXT4_FEATURE_COMPAT_SUPP\tEXT2_FEATURE_COMPAT_EXT_ATTR\n#define EXT4_FEATURE_INCOMPAT_SUPP\t(EXT4_FEATURE_INCOMPAT_FILETYPE| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_RECOVER| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_META_BG| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_EXTENTS| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_64BIT| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_FLEX_BG)\n#define EXT4_FEATURE_RO_COMPAT_SUPP\t(EXT4_FEATURE_RO_COMPAT_SPARSE_SUPER| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_LARGE_FILE| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_GDT_CSUM| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_DIR_NLINK | \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_EXTRA_ISIZE | \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_BTREE_DIR |\\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_HUGE_FILE)\n\n/*\n * Default values for user and/or group using reserved blocks\n */\n#define\tEXT4_DEF_RESUID\t\t0\n#define\tEXT4_DEF_RESGID\t\t0\n\n#define EXT4_DEF_INODE_READAHEAD_BLKS\t32\n\n/*\n * Default mount options\n */\n#define EXT4_DEFM_DEBUG\t\t0x0001\n#define EXT4_DEFM_BSDGROUPS\t0x0002\n#define EXT4_DEFM_XATTR_USER\t0x0004\n#define EXT4_DEFM_ACL\t\t0x0008\n#define EXT4_DEFM_UID16\t\t0x0010\n#define EXT4_DEFM_JMODE\t\t0x0060\n#define EXT4_DEFM_JMODE_DATA\t0x0020\n#define EXT4_DEFM_JMODE_ORDERED\t0x0040\n#define EXT4_DEFM_JMODE_WBACK\t0x0060\n\n/*\n * Default journal batch times\n */\n#define EXT4_DEF_MIN_BATCH_TIME\t0\n#define EXT4_DEF_MAX_BATCH_TIME\t15000 /* 15ms */\n\n/*\n * Minimum number of groups in a flexgroup before we separate out\n * directories into the first block group of a flexgroup\n */\n#define EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME\t4\n\n/*\n * Structure of a directory entry\n */\n#define EXT4_NAME_LEN 255\n\nstruct ext4_dir_entry {\n\t__le32\tinode;\t\t\t/* Inode number */\n\t__le16\trec_len;\t\t/* Directory entry length */\n\t__le16\tname_len;\t\t/* Name length */\n\tchar\tname[EXT4_NAME_LEN];\t/* File name */\n};\n\n/*\n * The new version of the directory entry.  Since EXT4 structures are\n * stored in intel byte order, and the name_len field could never be\n * bigger than 255 chars, it's safe to reclaim the extra byte for the\n * file_type field.\n */\nstruct ext4_dir_entry_2 {\n\t__le32\tinode;\t\t\t/* Inode number */\n\t__le16\trec_len;\t\t/* Directory entry length */\n\t__u8\tname_len;\t\t/* Name length */\n\t__u8\tfile_type;\n\tchar\tname[EXT4_NAME_LEN];\t/* File name */\n};\n\n/*\n * Ext4 directory file types.  Only the low 3 bits are used.  The\n * other bits are reserved for now.\n */\n#define EXT4_FT_UNKNOWN\t\t0\n#define EXT4_FT_REG_FILE\t1\n#define EXT4_FT_DIR\t\t2\n#define EXT4_FT_CHRDEV\t\t3\n#define EXT4_FT_BLKDEV\t\t4\n#define EXT4_FT_FIFO\t\t5\n#define EXT4_FT_SOCK\t\t6\n#define EXT4_FT_SYMLINK\t\t7\n\n#define EXT4_FT_MAX\t\t8\n\n/*\n * EXT4_DIR_PAD defines the directory entries boundaries\n *\n * NOTE: It must be a multiple of 4\n */\n#define EXT4_DIR_PAD\t\t\t4\n#define EXT4_DIR_ROUND\t\t\t(EXT4_DIR_PAD - 1)\n#define EXT4_DIR_REC_LEN(name_len)\t(((name_len) + 8 + EXT4_DIR_ROUND) & \\\n\t\t\t\t\t ~EXT4_DIR_ROUND)\n#define EXT4_MAX_REC_LEN\t\t((1<<16)-1)\n\n/*\n * Hash Tree Directory indexing\n * (c) Daniel Phillips, 2001\n */\n\n#define is_dx(dir) (EXT4_HAS_COMPAT_FEATURE(dir->i_sb, \\\n\t\t\t\t      EXT4_FEATURE_COMPAT_DIR_INDEX) && \\\n\t\t      (EXT4_I(dir)->i_flags & EXT4_INDEX_FL))\n#define EXT4_DIR_LINK_MAX(dir) (!is_dx(dir) && (dir)->i_nlink >= EXT4_LINK_MAX)\n#define EXT4_DIR_LINK_EMPTY(dir) ((dir)->i_nlink == 2 || (dir)->i_nlink == 1)\n\n/* Legal values for the dx_root hash_version field: */\n\n#define DX_HASH_LEGACY\t\t0\n#define DX_HASH_HALF_MD4\t1\n#define DX_HASH_TEA\t\t2\n#define DX_HASH_LEGACY_UNSIGNED\t3\n#define DX_HASH_HALF_MD4_UNSIGNED\t4\n#define DX_HASH_TEA_UNSIGNED\t\t5\n\n#ifdef __KERNEL__\n\n/* hash info structure used by the directory hash */\nstruct dx_hash_info\n{\n\tu32\t\thash;\n\tu32\t\tminor_hash;\n\tint\t\thash_version;\n\tu32\t\t*seed;\n};\n\n#define EXT4_HTREE_EOF\t0x7fffffff\n\n/*\n * Control parameters used by ext4_htree_next_block\n */\n#define HASH_NB_ALWAYS\t\t1\n\n\n/*\n * Describe an inode's exact location on disk and in memory\n */\nstruct ext4_iloc\n{\n\tstruct buffer_head *bh;\n\tunsigned long offset;\n\text4_group_t block_group;\n};\n\nstatic inline struct ext4_inode *ext4_raw_inode(struct ext4_iloc *iloc)\n{\n\treturn (struct ext4_inode *) (iloc->bh->b_data + iloc->offset);\n}\n\n/*\n * This structure is stuffed into the struct file's private_data field\n * for directories.  It is where we put information so that we can do\n * readdir operations in hash tree order.\n */\nstruct dir_private_info {\n\tstruct rb_root\troot;\n\tstruct rb_node\t*curr_node;\n\tstruct fname\t*extra_fname;\n\tloff_t\t\tlast_pos;\n\t__u32\t\tcurr_hash;\n\t__u32\t\tcurr_minor_hash;\n\t__u32\t\tnext_hash;\n};\n\n/* calculate the first block number of the group */\nstatic inline ext4_fsblk_t\next4_group_first_block_no(struct super_block *sb, ext4_group_t group_no)\n{\n\treturn group_no * (ext4_fsblk_t)EXT4_BLOCKS_PER_GROUP(sb) +\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_first_data_block);\n}\n\n/*\n * Special error return code only used by dx_probe() and its callers.\n */\n#define ERR_BAD_DX_DIR\t-75000\n\nvoid ext4_get_group_no_and_offset(struct super_block *sb, ext4_fsblk_t blocknr,\n\t\t\text4_group_t *blockgrpp, ext4_grpblk_t *offsetp);\n\nextern struct proc_dir_entry *ext4_proc_root;\n\n/*\n * Function prototypes\n */\n\n/*\n * Ok, these declarations are also in <linux/kernel.h> but none of the\n * ext4 source programs needs to include it so they are duplicated here.\n */\n# define NORET_TYPE\t/**/\n# define ATTRIB_NORET\t__attribute__((noreturn))\n# define NORET_AND\tnoreturn,\n\n/* bitmap.c */\nextern unsigned int ext4_count_free(struct buffer_head *, unsigned);\n\n/* balloc.c */\nextern unsigned int ext4_block_group(struct super_block *sb,\n\t\t\text4_fsblk_t blocknr);\nextern ext4_grpblk_t ext4_block_group_offset(struct super_block *sb,\n\t\t\text4_fsblk_t blocknr);\nextern int ext4_bg_has_super(struct super_block *sb, ext4_group_t group);\nextern unsigned long ext4_bg_num_gdb(struct super_block *sb,\n\t\t\text4_group_t group);\nextern ext4_fsblk_t ext4_new_meta_blocks(handle_t *handle, struct inode *inode,\n\t\t\text4_fsblk_t goal, unsigned long *count, int *errp);\nextern int ext4_claim_free_blocks(struct ext4_sb_info *sbi, s64 nblocks);\nextern int ext4_has_free_blocks(struct ext4_sb_info *sbi, s64 nblocks);\nextern void ext4_add_groupblocks(handle_t *handle, struct super_block *sb,\n\t\t\t\text4_fsblk_t block, unsigned long count);\nextern ext4_fsblk_t ext4_count_free_blocks(struct super_block *);\nextern void ext4_check_blocks_bitmap(struct super_block *);\nextern struct ext4_group_desc * ext4_get_group_desc(struct super_block * sb,\n\t\t\t\t\t\t    ext4_group_t block_group,\n\t\t\t\t\t\t    struct buffer_head ** bh);\nextern int ext4_should_retry_alloc(struct super_block *sb, int *retries);\nstruct buffer_head *ext4_read_block_bitmap(struct super_block *sb,\n\t\t\t\t      ext4_group_t block_group);\nextern unsigned ext4_init_block_bitmap(struct super_block *sb,\n\t\t\t\t       struct buffer_head *bh,\n\t\t\t\t       ext4_group_t group,\n\t\t\t\t       struct ext4_group_desc *desc);\n#define ext4_free_blocks_after_init(sb, group, desc)\t\t\t\\\n\t\text4_init_block_bitmap(sb, NULL, group, desc)\n\n/* dir.c */\nextern int ext4_check_dir_entry(const char *, struct inode *,\n\t\t\t\tstruct ext4_dir_entry_2 *,\n\t\t\t\tstruct buffer_head *, unsigned int);\nextern int ext4_htree_store_dirent(struct file *dir_file, __u32 hash,\n\t\t\t\t    __u32 minor_hash,\n\t\t\t\t    struct ext4_dir_entry_2 *dirent);\nextern void ext4_htree_free_dir_info(struct dir_private_info *p);\n\n/* fsync.c */\nextern int ext4_sync_file(struct file *, struct dentry *, int);\n\n/* hash.c */\nextern int ext4fs_dirhash(const char *name, int len, struct\n\t\t\t  dx_hash_info *hinfo);\n\n/* ialloc.c */\nextern struct inode *ext4_new_inode(handle_t *, struct inode *, int,\n\t\t\t\t    const struct qstr *qstr, __u32 goal);\nextern void ext4_free_inode(handle_t *, struct inode *);\nextern struct inode * ext4_orphan_get(struct super_block *, unsigned long);\nextern unsigned long ext4_count_free_inodes(struct super_block *);\nextern unsigned long ext4_count_dirs(struct super_block *);\nextern void ext4_check_inodes_bitmap(struct super_block *);\nextern unsigned ext4_init_inode_bitmap(struct super_block *sb,\n\t\t\t\t       struct buffer_head *bh,\n\t\t\t\t       ext4_group_t group,\n\t\t\t\t       struct ext4_group_desc *desc);\nextern void mark_bitmap_end(int start_bit, int end_bit, char *bitmap);\n\n/* mballoc.c */\nextern long ext4_mb_stats;\nextern long ext4_mb_max_to_scan;\nextern int ext4_mb_init(struct super_block *, int);\nextern int ext4_mb_release(struct super_block *);\nextern ext4_fsblk_t ext4_mb_new_blocks(handle_t *,\n\t\t\t\tstruct ext4_allocation_request *, int *);\nextern int ext4_mb_reserve_blocks(struct super_block *, int);\nextern void ext4_discard_preallocations(struct inode *);\nextern int __init init_ext4_mballoc(void);\nextern void exit_ext4_mballoc(void);\nextern void ext4_free_blocks(handle_t *handle, struct inode *inode,\n\t\t\t     struct buffer_head *bh, ext4_fsblk_t block,\n\t\t\t     unsigned long count, int flags);\nextern int ext4_mb_add_groupinfo(struct super_block *sb,\n\t\text4_group_t i, struct ext4_group_desc *desc);\nextern int ext4_mb_get_buddy_cache_lock(struct super_block *, ext4_group_t);\nextern void ext4_mb_put_buddy_cache_lock(struct super_block *,\n\t\t\t\t\t\text4_group_t, int);\n/* inode.c */\nstruct buffer_head *ext4_getblk(handle_t *, struct inode *,\n\t\t\t\t\t\text4_lblk_t, int, int *);\nstruct buffer_head *ext4_bread(handle_t *, struct inode *,\n\t\t\t\t\t\text4_lblk_t, int, int *);\nint ext4_get_block(struct inode *inode, sector_t iblock,\n\t\t\t\tstruct buffer_head *bh_result, int create);\n\nextern struct inode *ext4_iget(struct super_block *, unsigned long);\nextern int  ext4_write_inode(struct inode *, int);\nextern int  ext4_setattr(struct dentry *, struct iattr *);\nextern int  ext4_getattr(struct vfsmount *mnt, struct dentry *dentry,\n\t\t\t\tstruct kstat *stat);\nextern void ext4_delete_inode(struct inode *);\nextern int  ext4_sync_inode(handle_t *, struct inode *);\nextern void ext4_dirty_inode(struct inode *);\nextern int ext4_change_inode_journal_flag(struct inode *, int);\nextern int ext4_get_inode_loc(struct inode *, struct ext4_iloc *);\nextern int ext4_can_truncate(struct inode *inode);\nextern void ext4_truncate(struct inode *);\nextern int ext4_truncate_restart_trans(handle_t *, struct inode *, int nblocks);\nextern void ext4_set_inode_flags(struct inode *);\nextern void ext4_get_inode_flags(struct ext4_inode_info *);\nextern int ext4_alloc_da_blocks(struct inode *inode);\nextern void ext4_set_aops(struct inode *inode);\nextern int ext4_writepage_trans_blocks(struct inode *);\nextern int ext4_meta_trans_blocks(struct inode *, int nrblocks, int idxblocks);\nextern int ext4_chunk_trans_blocks(struct inode *, int nrblocks);\nextern int ext4_block_truncate_page(handle_t *handle,\n\t\tstruct address_space *mapping, loff_t from);\nextern int ext4_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);\nextern qsize_t *ext4_get_reserved_space(struct inode *inode);\nextern int flush_completed_IO(struct inode *inode);\nextern void ext4_da_update_reserve_space(struct inode *inode,\n\t\t\t\t\tint used, int quota_claim);\n/* ioctl.c */\nextern long ext4_ioctl(struct file *, unsigned int, unsigned long);\nextern long ext4_compat_ioctl(struct file *, unsigned int, unsigned long);\n\n/* migrate.c */\nextern int ext4_ext_migrate(struct inode *);\n\n/* namei.c */\nextern unsigned int ext4_rec_len_from_disk(__le16 dlen, unsigned blocksize);\nextern __le16 ext4_rec_len_to_disk(unsigned len, unsigned blocksize);\nextern int ext4_orphan_add(handle_t *, struct inode *);\nextern int ext4_orphan_del(handle_t *, struct inode *);\nextern int ext4_htree_fill_tree(struct file *dir_file, __u32 start_hash,\n\t\t\t\t__u32 start_minor_hash, __u32 *next_hash);\n\n/* resize.c */\nextern int ext4_group_add(struct super_block *sb,\n\t\t\t\tstruct ext4_new_group_data *input);\nextern int ext4_group_extend(struct super_block *sb,\n\t\t\t\tstruct ext4_super_block *es,\n\t\t\t\text4_fsblk_t n_blocks_count);\n\n/* super.c */\nextern void __ext4_error(struct super_block *, const char *, const char *, ...)\n\t__attribute__ ((format (printf, 3, 4)));\n#define ext4_error(sb, message...)\t__ext4_error(sb, __func__, ## message)\nextern void __ext4_std_error(struct super_block *, const char *, int);\nextern void ext4_abort(struct super_block *, const char *, const char *, ...)\n\t__attribute__ ((format (printf, 3, 4)));\nextern void __ext4_warning(struct super_block *, const char *,\n\t\t\t  const char *, ...)\n\t__attribute__ ((format (printf, 3, 4)));\n#define ext4_warning(sb, message...)\t__ext4_warning(sb, __func__, ## message)\nextern void ext4_msg(struct super_block *, const char *, const char *, ...)\n\t__attribute__ ((format (printf, 3, 4)));\nextern void ext4_grp_locked_error(struct super_block *, ext4_group_t,\n\t\t\t\tconst char *, const char *, ...)\n\t__attribute__ ((format (printf, 4, 5)));\nextern void ext4_update_dynamic_rev(struct super_block *sb);\nextern int ext4_update_compat_feature(handle_t *handle, struct super_block *sb,\n\t\t\t\t\t__u32 compat);\nextern int ext4_update_rocompat_feature(handle_t *handle,\n\t\t\t\t\tstruct super_block *sb,\t__u32 rocompat);\nextern int ext4_update_incompat_feature(handle_t *handle,\n\t\t\t\t\tstruct super_block *sb,\t__u32 incompat);\nextern ext4_fsblk_t ext4_block_bitmap(struct super_block *sb,\n\t\t\t\t      struct ext4_group_desc *bg);\nextern ext4_fsblk_t ext4_inode_bitmap(struct super_block *sb,\n\t\t\t\t      struct ext4_group_desc *bg);\nextern ext4_fsblk_t ext4_inode_table(struct super_block *sb,\n\t\t\t\t     struct ext4_group_desc *bg);\nextern __u32 ext4_free_blks_count(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg);\nextern __u32 ext4_free_inodes_count(struct super_block *sb,\n\t\t\t\t struct ext4_group_desc *bg);\nextern __u32 ext4_used_dirs_count(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg);\nextern __u32 ext4_itable_unused_count(struct super_block *sb,\n\t\t\t\t   struct ext4_group_desc *bg);\nextern void ext4_block_bitmap_set(struct super_block *sb,\n\t\t\t\t  struct ext4_group_desc *bg, ext4_fsblk_t blk);\nextern void ext4_inode_bitmap_set(struct super_block *sb,\n\t\t\t\t  struct ext4_group_desc *bg, ext4_fsblk_t blk);\nextern void ext4_inode_table_set(struct super_block *sb,\n\t\t\t\t struct ext4_group_desc *bg, ext4_fsblk_t blk);\nextern void ext4_free_blks_set(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg, __u32 count);\nextern void ext4_free_inodes_set(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg, __u32 count);\nextern void ext4_used_dirs_set(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg, __u32 count);\nextern void ext4_itable_unused_set(struct super_block *sb,\n\t\t\t\t   struct ext4_group_desc *bg, __u32 count);\nextern __le16 ext4_group_desc_csum(struct ext4_sb_info *sbi, __u32 group,\n\t\t\t\t   struct ext4_group_desc *gdp);\nextern int ext4_group_desc_csum_verify(struct ext4_sb_info *sbi, __u32 group,\n\t\t\t\t       struct ext4_group_desc *gdp);\n\nstatic inline ext4_fsblk_t ext4_blocks_count(struct ext4_super_block *es)\n{\n\treturn ((ext4_fsblk_t)le32_to_cpu(es->s_blocks_count_hi) << 32) |\n\t\tle32_to_cpu(es->s_blocks_count_lo);\n}\n\nstatic inline ext4_fsblk_t ext4_r_blocks_count(struct ext4_super_block *es)\n{\n\treturn ((ext4_fsblk_t)le32_to_cpu(es->s_r_blocks_count_hi) << 32) |\n\t\tle32_to_cpu(es->s_r_blocks_count_lo);\n}\n\nstatic inline ext4_fsblk_t ext4_free_blocks_count(struct ext4_super_block *es)\n{\n\treturn ((ext4_fsblk_t)le32_to_cpu(es->s_free_blocks_count_hi) << 32) |\n\t\tle32_to_cpu(es->s_free_blocks_count_lo);\n}\n\nstatic inline void ext4_blocks_count_set(struct ext4_super_block *es,\n\t\t\t\t\t ext4_fsblk_t blk)\n{\n\tes->s_blocks_count_lo = cpu_to_le32((u32)blk);\n\tes->s_blocks_count_hi = cpu_to_le32(blk >> 32);\n}\n\nstatic inline void ext4_free_blocks_count_set(struct ext4_super_block *es,\n\t\t\t\t\t      ext4_fsblk_t blk)\n{\n\tes->s_free_blocks_count_lo = cpu_to_le32((u32)blk);\n\tes->s_free_blocks_count_hi = cpu_to_le32(blk >> 32);\n}\n\nstatic inline void ext4_r_blocks_count_set(struct ext4_super_block *es,\n\t\t\t\t\t   ext4_fsblk_t blk)\n{\n\tes->s_r_blocks_count_lo = cpu_to_le32((u32)blk);\n\tes->s_r_blocks_count_hi = cpu_to_le32(blk >> 32);\n}\n\nstatic inline loff_t ext4_isize(struct ext4_inode *raw_inode)\n{\n\tif (S_ISREG(le16_to_cpu(raw_inode->i_mode)))\n\t\treturn ((loff_t)le32_to_cpu(raw_inode->i_size_high) << 32) |\n\t\t\tle32_to_cpu(raw_inode->i_size_lo);\n\telse\n\t\treturn (loff_t) le32_to_cpu(raw_inode->i_size_lo);\n}\n\nstatic inline void ext4_isize_set(struct ext4_inode *raw_inode, loff_t i_size)\n{\n\traw_inode->i_size_lo = cpu_to_le32(i_size);\n\traw_inode->i_size_high = cpu_to_le32(i_size >> 32);\n}\n\nstatic inline\nstruct ext4_group_info *ext4_get_group_info(struct super_block *sb,\n\t\t\t\t\t    ext4_group_t group)\n{\n\t struct ext4_group_info ***grp_info;\n\t long indexv, indexh;\n\t grp_info = EXT4_SB(sb)->s_group_info;\n\t indexv = group >> (EXT4_DESC_PER_BLOCK_BITS(sb));\n\t indexh = group & ((EXT4_DESC_PER_BLOCK(sb)) - 1);\n\t return grp_info[indexv][indexh];\n}\n\n/*\n * Reading s_groups_count requires using smp_rmb() afterwards.  See\n * the locking protocol documented in the comments of ext4_group_add()\n * in resize.c\n */\nstatic inline ext4_group_t ext4_get_groups_count(struct super_block *sb)\n{\n\text4_group_t\tngroups = EXT4_SB(sb)->s_groups_count;\n\n\tsmp_rmb();\n\treturn ngroups;\n}\n\nstatic inline ext4_group_t ext4_flex_group(struct ext4_sb_info *sbi,\n\t\t\t\t\t     ext4_group_t block_group)\n{\n\treturn block_group >> sbi->s_log_groups_per_flex;\n}\n\nstatic inline unsigned int ext4_flex_bg_size(struct ext4_sb_info *sbi)\n{\n\treturn 1 << sbi->s_log_groups_per_flex;\n}\n\n#define ext4_std_error(sb, errno)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif ((errno))\t\t\t\t\t\t\\\n\t\t__ext4_std_error((sb), __func__, (errno));\t\\\n} while (0)\n\n#ifdef CONFIG_SMP\n/* Each CPU can accumulate percpu_counter_batch blocks in their local\n * counters. So we need to make sure we have free blocks more\n * than percpu_counter_batch  * nr_cpu_ids. Also add a window of 4 times.\n */\n#define EXT4_FREEBLOCKS_WATERMARK (4 * (percpu_counter_batch * nr_cpu_ids))\n#else\n#define EXT4_FREEBLOCKS_WATERMARK 0\n#endif\n\nstatic inline void ext4_update_i_disksize(struct inode *inode, loff_t newsize)\n{\n\t/*\n\t * XXX: replace with spinlock if seen contended -bzzz\n\t */\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\tif (newsize > EXT4_I(inode)->i_disksize)\n\t\tEXT4_I(inode)->i_disksize = newsize;\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\treturn ;\n}\n\nstruct ext4_group_info {\n\tunsigned long   bb_state;\n\tstruct rb_root  bb_free_root;\n\text4_grpblk_t\tbb_first_free;\t/* first free block */\n\text4_grpblk_t\tbb_free;\t/* total free blocks */\n\text4_grpblk_t\tbb_fragments;\t/* nr of freespace fragments */\n\tstruct          list_head bb_prealloc_list;\n#ifdef DOUBLE_CHECK\n\tvoid            *bb_bitmap;\n#endif\n\tstruct rw_semaphore alloc_sem;\n\text4_grpblk_t\tbb_counters[];\t/* Nr of free power-of-two-block\n\t\t\t\t\t * regions, index is order.\n\t\t\t\t\t * bb_counters[3] = 5 means\n\t\t\t\t\t * 5 free 8-block regions. */\n};\n\n#define EXT4_GROUP_INFO_NEED_INIT_BIT\t0\n\n#define EXT4_MB_GRP_NEED_INIT(grp)\t\\\n\t(test_bit(EXT4_GROUP_INFO_NEED_INIT_BIT, &((grp)->bb_state)))\n\n#define EXT4_MAX_CONTENTION\t\t8\n#define EXT4_CONTENTION_THRESHOLD\t2\n\nstatic inline spinlock_t *ext4_group_lock_ptr(struct super_block *sb,\n\t\t\t\t\t      ext4_group_t group)\n{\n\treturn bgl_lock_ptr(EXT4_SB(sb)->s_blockgroup_lock, group);\n}\n\n/*\n * Returns true if the filesystem is busy enough that attempts to\n * access the block group locks has run into contention.\n */\nstatic inline int ext4_fs_is_busy(struct ext4_sb_info *sbi)\n{\n\treturn (atomic_read(&sbi->s_lock_busy) > EXT4_CONTENTION_THRESHOLD);\n}\n\nstatic inline void ext4_lock_group(struct super_block *sb, ext4_group_t group)\n{\n\tspinlock_t *lock = ext4_group_lock_ptr(sb, group);\n\tif (spin_trylock(lock))\n\t\t/*\n\t\t * We're able to grab the lock right away, so drop the\n\t\t * lock contention counter.\n\t\t */\n\t\tatomic_add_unless(&EXT4_SB(sb)->s_lock_busy, -1, 0);\n\telse {\n\t\t/*\n\t\t * The lock is busy, so bump the contention counter,\n\t\t * and then wait on the spin lock.\n\t\t */\n\t\tatomic_add_unless(&EXT4_SB(sb)->s_lock_busy, 1,\n\t\t\t\t  EXT4_MAX_CONTENTION);\n\t\tspin_lock(lock);\n\t}\n}\n\nstatic inline void ext4_unlock_group(struct super_block *sb,\n\t\t\t\t\text4_group_t group)\n{\n\tspin_unlock(ext4_group_lock_ptr(sb, group));\n}\n\n/*\n * Inodes and files operations\n */\n\n/* dir.c */\nextern const struct file_operations ext4_dir_operations;\n\n/* file.c */\nextern const struct inode_operations ext4_file_inode_operations;\nextern const struct file_operations ext4_file_operations;\n\n/* namei.c */\nextern const struct inode_operations ext4_dir_inode_operations;\nextern const struct inode_operations ext4_special_inode_operations;\nextern struct dentry *ext4_get_parent(struct dentry *child);\n\n/* symlink.c */\nextern const struct inode_operations ext4_symlink_inode_operations;\nextern const struct inode_operations ext4_fast_symlink_inode_operations;\n\n/* block_validity */\nextern void ext4_release_system_zone(struct super_block *sb);\nextern int ext4_setup_system_zone(struct super_block *sb);\nextern int __init init_ext4_system_zone(void);\nextern void exit_ext4_system_zone(void);\nextern int ext4_data_block_valid(struct ext4_sb_info *sbi,\n\t\t\t\t ext4_fsblk_t start_blk,\n\t\t\t\t unsigned int count);\n\n/* extents.c */\nextern int ext4_ext_tree_init(handle_t *handle, struct inode *);\nextern int ext4_ext_writepage_trans_blocks(struct inode *, int);\nextern int ext4_ext_index_trans_blocks(struct inode *inode, int nrblocks,\n\t\t\t\t       int chunk);\nextern int ext4_ext_get_blocks(handle_t *handle, struct inode *inode,\n\t\t\t       ext4_lblk_t iblock, unsigned int max_blocks,\n\t\t\t       struct buffer_head *bh_result, int flags);\nextern void ext4_ext_truncate(struct inode *);\nextern void ext4_ext_init(struct super_block *);\nextern void ext4_ext_release(struct super_block *);\nextern long ext4_fallocate(struct inode *inode, int mode, loff_t offset,\n\t\t\t  loff_t len);\nextern int ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,\n\t\t\t  ssize_t len);\nextern int ext4_get_blocks(handle_t *handle, struct inode *inode,\n\t\t\t   sector_t block, unsigned int max_blocks,\n\t\t\t   struct buffer_head *bh, int flags);\nextern int ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t\t__u64 start, __u64 len);\n/* move_extent.c */\nextern int ext4_move_extents(struct file *o_filp, struct file *d_filp,\n\t\t\t     __u64 start_orig, __u64 start_donor,\n\t\t\t     __u64 len, __u64 *moved_len);\n\n\n/* BH_Uninit flag: blocks are allocated but uninitialized on disk */\nenum ext4_state_bits {\n\tBH_Uninit\t/* blocks are allocated but uninitialized on disk */\n\t  = BH_JBDPrivateStart,\n};\n\nBUFFER_FNS(Uninit, uninit)\nTAS_BUFFER_FNS(Uninit, uninit)\n\n/*\n * Add new method to test wether block and inode bitmaps are properly\n * initialized. With uninit_bg reading the block from disk is not enough\n * to mark the bitmap uptodate. We need to also zero-out the bitmap\n */\n#define BH_BITMAP_UPTODATE BH_JBDPrivateStart\n\nstatic inline int bitmap_uptodate(struct buffer_head *bh)\n{\n\treturn (buffer_uptodate(bh) &&\n\t\t\ttest_bit(BH_BITMAP_UPTODATE, &(bh)->b_state));\n}\nstatic inline void set_bitmap_uptodate(struct buffer_head *bh)\n{\n\tset_bit(BH_BITMAP_UPTODATE, &(bh)->b_state);\n}\n\n#endif\t/* __KERNEL__ */\n\n#endif\t/* _EXT4_H */\n", "/*\n * ext4_jbd2.h\n *\n * Written by Stephen C. Tweedie <sct@redhat.com>, 1999\n *\n * Copyright 1998--1999 Red Hat corp --- All Rights Reserved\n *\n * This file is part of the Linux kernel and is made available under\n * the terms of the GNU General Public License, version 2, or at your\n * option, any later version, incorporated herein by reference.\n *\n * Ext4-specific journaling extensions.\n */\n\n#ifndef _EXT4_JBD2_H\n#define _EXT4_JBD2_H\n\n#include <linux/fs.h>\n#include <linux/jbd2.h>\n#include \"ext4.h\"\n\n#define EXT4_JOURNAL(inode)\t(EXT4_SB((inode)->i_sb)->s_journal)\n\n/* Define the number of blocks we need to account to a transaction to\n * modify one block of data.\n *\n * We may have to touch one inode, one bitmap buffer, up to three\n * indirection blocks, the group and superblock summaries, and the data\n * block to complete the transaction.\n *\n * For extents-enabled fs we may have to allocate and modify up to\n * 5 levels of tree + root which are stored in the inode. */\n\n#define EXT4_SINGLEDATA_TRANS_BLOCKS(sb)\t\t\t\t\\\n\t(EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)   \\\n\t ? 27U : 8U)\n\n/* Extended attribute operations touch at most two data buffers,\n * two bitmap buffers, and two group summaries, in addition to the inode\n * and the superblock, which are already accounted for. */\n\n#define EXT4_XATTR_TRANS_BLOCKS\t\t6U\n\n/* Define the minimum size for a transaction which modifies data.  This\n * needs to take into account the fact that we may end up modifying two\n * quota files too (one for the group, one for the user quota).  The\n * superblock only gets updated once, of course, so don't bother\n * counting that again for the quota updates. */\n\n#define EXT4_DATA_TRANS_BLOCKS(sb)\t(EXT4_SINGLEDATA_TRANS_BLOCKS(sb) + \\\n\t\t\t\t\t EXT4_XATTR_TRANS_BLOCKS - 2 + \\\n\t\t\t\t\t EXT4_MAXQUOTAS_TRANS_BLOCKS(sb))\n\n/*\n * Define the number of metadata blocks we need to account to modify data.\n *\n * This include super block, inode block, quota blocks and xattr blocks\n */\n#define EXT4_META_TRANS_BLOCKS(sb)\t(EXT4_XATTR_TRANS_BLOCKS + \\\n\t\t\t\t\tEXT4_MAXQUOTAS_TRANS_BLOCKS(sb))\n\n/* Delete operations potentially hit one directory's namespace plus an\n * entire inode, plus arbitrary amounts of bitmap/indirection data.  Be\n * generous.  We can grow the delete transaction later if necessary. */\n\n#define EXT4_DELETE_TRANS_BLOCKS(sb)\t(2 * EXT4_DATA_TRANS_BLOCKS(sb) + 64)\n\n/* Define an arbitrary limit for the amount of data we will anticipate\n * writing to any given transaction.  For unbounded transactions such as\n * write(2) and truncate(2) we can write more than this, but we always\n * start off at the maximum transaction size and grow the transaction\n * optimistically as we go. */\n\n#define EXT4_MAX_TRANS_DATA\t\t64U\n\n/* We break up a large truncate or write transaction once the handle's\n * buffer credits gets this low, we need either to extend the\n * transaction or to start a new one.  Reserve enough space here for\n * inode, bitmap, superblock, group and indirection updates for at least\n * one block, plus two quota updates.  Quota allocations are not\n * needed. */\n\n#define EXT4_RESERVE_TRANS_BLOCKS\t12U\n\n#define EXT4_INDEX_EXTRA_TRANS_BLOCKS\t8\n\n#ifdef CONFIG_QUOTA\n/* Amount of blocks needed for quota update - we know that the structure was\n * allocated so we need to update only inode+data */\n#define EXT4_QUOTA_TRANS_BLOCKS(sb) (test_opt(sb, QUOTA) ? 2 : 0)\n/* Amount of blocks needed for quota insert/delete - we do some block writes\n * but inode, sb and group updates are done only once */\n#define EXT4_QUOTA_INIT_BLOCKS(sb) (test_opt(sb, QUOTA) ? (DQUOT_INIT_ALLOC*\\\n\t\t(EXT4_SINGLEDATA_TRANS_BLOCKS(sb)-3)+3+DQUOT_INIT_REWRITE) : 0)\n\n#define EXT4_QUOTA_DEL_BLOCKS(sb) (test_opt(sb, QUOTA) ? (DQUOT_DEL_ALLOC*\\\n\t\t(EXT4_SINGLEDATA_TRANS_BLOCKS(sb)-3)+3+DQUOT_DEL_REWRITE) : 0)\n#else\n#define EXT4_QUOTA_TRANS_BLOCKS(sb) 0\n#define EXT4_QUOTA_INIT_BLOCKS(sb) 0\n#define EXT4_QUOTA_DEL_BLOCKS(sb) 0\n#endif\n#define EXT4_MAXQUOTAS_TRANS_BLOCKS(sb) (MAXQUOTAS*EXT4_QUOTA_TRANS_BLOCKS(sb))\n#define EXT4_MAXQUOTAS_INIT_BLOCKS(sb) (MAXQUOTAS*EXT4_QUOTA_INIT_BLOCKS(sb))\n#define EXT4_MAXQUOTAS_DEL_BLOCKS(sb) (MAXQUOTAS*EXT4_QUOTA_DEL_BLOCKS(sb))\n\nint\next4_mark_iloc_dirty(handle_t *handle,\n\t\t     struct inode *inode,\n\t\t     struct ext4_iloc *iloc);\n\n/*\n * On success, We end up with an outstanding reference count against\n * iloc->bh.  This _must_ be cleaned up later.\n */\n\nint ext4_reserve_inode_write(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_iloc *iloc);\n\nint ext4_mark_inode_dirty(handle_t *handle, struct inode *inode);\n\n/*\n * Wrapper functions with which ext4 calls into JBD.\n */\nvoid ext4_journal_abort_handle(const char *caller, const char *err_fn,\n\t\tstruct buffer_head *bh, handle_t *handle, int err);\n\nint __ext4_journal_get_undo_access(const char *where, handle_t *handle,\n\t\t\t\tstruct buffer_head *bh);\n\nint __ext4_journal_get_write_access(const char *where, handle_t *handle,\n\t\t\t\tstruct buffer_head *bh);\n\nint __ext4_forget(const char *where, handle_t *handle, int is_metadata,\n\t\t  struct inode *inode, struct buffer_head *bh,\n\t\t  ext4_fsblk_t blocknr);\n\nint __ext4_journal_get_create_access(const char *where,\n\t\t\t\thandle_t *handle, struct buffer_head *bh);\n\nint __ext4_handle_dirty_metadata(const char *where, handle_t *handle,\n\t\t\t\t struct inode *inode, struct buffer_head *bh);\n\n#define ext4_journal_get_undo_access(handle, bh) \\\n\t__ext4_journal_get_undo_access(__func__, (handle), (bh))\n#define ext4_journal_get_write_access(handle, bh) \\\n\t__ext4_journal_get_write_access(__func__, (handle), (bh))\n#define ext4_forget(handle, is_metadata, inode, bh, block_nr) \\\n\t__ext4_forget(__func__, (handle), (is_metadata), (inode), (bh),\\\n\t\t      (block_nr))\n#define ext4_journal_get_create_access(handle, bh) \\\n\t__ext4_journal_get_create_access(__func__, (handle), (bh))\n#define ext4_handle_dirty_metadata(handle, inode, bh) \\\n\t__ext4_handle_dirty_metadata(__func__, (handle), (inode), (bh))\n\nhandle_t *ext4_journal_start_sb(struct super_block *sb, int nblocks);\nint __ext4_journal_stop(const char *where, handle_t *handle);\n\n#define EXT4_NOJOURNAL_MAX_REF_COUNT ((unsigned long) 4096)\n\n/* Note:  Do not use this for NULL handles.  This is only to determine if\n * a properly allocated handle is using a journal or not. */\nstatic inline int ext4_handle_valid(handle_t *handle)\n{\n\tif ((unsigned long)handle < EXT4_NOJOURNAL_MAX_REF_COUNT)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic inline void ext4_handle_sync(handle_t *handle)\n{\n\tif (ext4_handle_valid(handle))\n\t\thandle->h_sync = 1;\n}\n\nstatic inline void ext4_handle_release_buffer(handle_t *handle,\n\t\t\t\t\t\tstruct buffer_head *bh)\n{\n\tif (ext4_handle_valid(handle))\n\t\tjbd2_journal_release_buffer(handle, bh);\n}\n\nstatic inline int ext4_handle_is_aborted(handle_t *handle)\n{\n\tif (ext4_handle_valid(handle))\n\t\treturn is_handle_aborted(handle);\n\treturn 0;\n}\n\nstatic inline int ext4_handle_has_enough_credits(handle_t *handle, int needed)\n{\n\tif (ext4_handle_valid(handle) && handle->h_buffer_credits < needed)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic inline void ext4_journal_release_buffer(handle_t *handle,\n\t\t\t\t\t\tstruct buffer_head *bh)\n{\n\tif (ext4_handle_valid(handle))\n\t\tjbd2_journal_release_buffer(handle, bh);\n}\n\nstatic inline handle_t *ext4_journal_start(struct inode *inode, int nblocks)\n{\n\treturn ext4_journal_start_sb(inode->i_sb, nblocks);\n}\n\n#define ext4_journal_stop(handle) \\\n\t__ext4_journal_stop(__func__, (handle))\n\nstatic inline handle_t *ext4_journal_current_handle(void)\n{\n\treturn journal_current_handle();\n}\n\nstatic inline int ext4_journal_extend(handle_t *handle, int nblocks)\n{\n\tif (ext4_handle_valid(handle))\n\t\treturn jbd2_journal_extend(handle, nblocks);\n\treturn 0;\n}\n\nstatic inline int ext4_journal_restart(handle_t *handle, int nblocks)\n{\n\tif (ext4_handle_valid(handle))\n\t\treturn jbd2_journal_restart(handle, nblocks);\n\treturn 0;\n}\n\nstatic inline int ext4_journal_blocks_per_page(struct inode *inode)\n{\n\tif (EXT4_JOURNAL(inode) != NULL)\n\t\treturn jbd2_journal_blocks_per_page(inode);\n\treturn 0;\n}\n\nstatic inline int ext4_journal_force_commit(journal_t *journal)\n{\n\tif (journal)\n\t\treturn jbd2_journal_force_commit(journal);\n\treturn 0;\n}\n\nstatic inline int ext4_jbd2_file_inode(handle_t *handle, struct inode *inode)\n{\n\tif (ext4_handle_valid(handle))\n\t\treturn jbd2_journal_file_inode(handle, &EXT4_I(inode)->jinode);\n\treturn 0;\n}\n\nstatic inline void ext4_update_inode_fsync_trans(handle_t *handle,\n\t\t\t\t\t\t struct inode *inode,\n\t\t\t\t\t\t int datasync)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\tif (ext4_handle_valid(handle)) {\n\t\tei->i_sync_tid = handle->h_transaction->t_tid;\n\t\tif (datasync)\n\t\t\tei->i_datasync_tid = handle->h_transaction->t_tid;\n\t}\n}\n\n/* super.c */\nint ext4_force_commit(struct super_block *sb);\n\nstatic inline int ext4_should_journal_data(struct inode *inode)\n{\n\tif (EXT4_JOURNAL(inode) == NULL)\n\t\treturn 0;\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn 1;\n\tif (test_opt(inode->i_sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)\n\t\treturn 1;\n\tif (EXT4_I(inode)->i_flags & EXT4_JOURNAL_DATA_FL)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline int ext4_should_order_data(struct inode *inode)\n{\n\tif (EXT4_JOURNAL(inode) == NULL)\n\t\treturn 0;\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn 0;\n\tif (EXT4_I(inode)->i_flags & EXT4_JOURNAL_DATA_FL)\n\t\treturn 0;\n\tif (test_opt(inode->i_sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline int ext4_should_writeback_data(struct inode *inode)\n{\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn 0;\n\tif (EXT4_JOURNAL(inode) == NULL)\n\t\treturn 1;\n\tif (EXT4_I(inode)->i_flags & EXT4_JOURNAL_DATA_FL)\n\t\treturn 0;\n\tif (test_opt(inode->i_sb, DATA_FLAGS) == EXT4_MOUNT_WRITEBACK_DATA)\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function controls whether or not we should try to go down the\n * dioread_nolock code paths, which makes it safe to avoid taking\n * i_mutex for direct I/O reads.  This only works for extent-based\n * files, and it doesn't work for nobh or if data journaling is\n * enabled, since the dioread_nolock code uses b_private to pass\n * information back to the I/O completion handler, and this conflicts\n * with the jbd's use of b_private.\n */\nstatic inline int ext4_should_dioread_nolock(struct inode *inode)\n{\n\tif (!test_opt(inode->i_sb, DIOREAD_NOLOCK))\n\t\treturn 0;\n\tif (test_opt(inode->i_sb, NOBH))\n\t\treturn 0;\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn 0;\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL))\n\t\treturn 0;\n\tif (ext4_should_journal_data(inode))\n\t\treturn 0;\n\treturn 1;\n}\n\n#endif\t/* _EXT4_JBD2_H */\n", "/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n *\n * Architecture independence:\n *   Copyright (c) 2005, Bull S.A.\n *   Written by Pierre Peiffer <pierre.peiffer@bull.net>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n */\n\n/*\n * Extents support for EXT4\n *\n * TODO:\n *   - ext4*_error() should be used in some situations\n *   - analyze all BUG()/BUG_ON(), use -EIO where appropriate\n *   - smart tree reduction\n */\n\n#include <linux/module.h>\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/falloc.h>\n#include <asm/uaccess.h>\n#include <linux/fiemap.h>\n#include \"ext4_jbd2.h\"\n#include \"ext4_extents.h\"\n\n\n/*\n * ext_pblock:\n * combine low and high parts of physical block number into ext4_fsblk_t\n */\next4_fsblk_t ext_pblock(struct ext4_extent *ex)\n{\n\text4_fsblk_t block;\n\n\tblock = le32_to_cpu(ex->ee_start_lo);\n\tblock |= ((ext4_fsblk_t) le16_to_cpu(ex->ee_start_hi) << 31) << 1;\n\treturn block;\n}\n\n/*\n * idx_pblock:\n * combine low and high parts of a leaf physical block number into ext4_fsblk_t\n */\next4_fsblk_t idx_pblock(struct ext4_extent_idx *ix)\n{\n\text4_fsblk_t block;\n\n\tblock = le32_to_cpu(ix->ei_leaf_lo);\n\tblock |= ((ext4_fsblk_t) le16_to_cpu(ix->ei_leaf_hi) << 31) << 1;\n\treturn block;\n}\n\n/*\n * ext4_ext_store_pblock:\n * stores a large physical block number into an extent struct,\n * breaking it into parts\n */\nvoid ext4_ext_store_pblock(struct ext4_extent *ex, ext4_fsblk_t pb)\n{\n\tex->ee_start_lo = cpu_to_le32((unsigned long) (pb & 0xffffffff));\n\tex->ee_start_hi = cpu_to_le16((unsigned long) ((pb >> 31) >> 1) & 0xffff);\n}\n\n/*\n * ext4_idx_store_pblock:\n * stores a large physical block number into an index struct,\n * breaking it into parts\n */\nstatic void ext4_idx_store_pblock(struct ext4_extent_idx *ix, ext4_fsblk_t pb)\n{\n\tix->ei_leaf_lo = cpu_to_le32((unsigned long) (pb & 0xffffffff));\n\tix->ei_leaf_hi = cpu_to_le16((unsigned long) ((pb >> 31) >> 1) & 0xffff);\n}\n\nstatic int ext4_ext_truncate_extend_restart(handle_t *handle,\n\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t    int needed)\n{\n\tint err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (handle->h_buffer_credits > needed)\n\t\treturn 0;\n\terr = ext4_journal_extend(handle, needed);\n\tif (err <= 0)\n\t\treturn err;\n\terr = ext4_truncate_restart_trans(handle, inode, needed);\n\t/*\n\t * We have dropped i_data_sem so someone might have cached again\n\t * an extent we are going to truncate.\n\t */\n\text4_ext_invalidate_cache(inode);\n\n\treturn err;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n */\nstatic int ext4_ext_get_access(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\treturn ext4_journal_get_write_access(handle, path->p_bh);\n\t}\n\t/* path points to leaf/index in inode body */\n\t/* we use in-core data, no need to protect them */\n\treturn 0;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n *  - EIO\n */\nstatic int ext4_ext_dirty(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tint err;\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\terr = ext4_handle_dirty_metadata(handle, inode, path->p_bh);\n\t} else {\n\t\t/* path points to leaf/index in inode body */\n\t\terr = ext4_mark_inode_dirty(handle, inode);\n\t}\n\treturn err;\n}\n\nstatic ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      ext4_lblk_t block)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\text4_fsblk_t bg_start;\n\text4_fsblk_t last_block;\n\text4_grpblk_t colour;\n\text4_group_t block_group;\n\tint flex_size = ext4_flex_bg_size(EXT4_SB(inode->i_sb));\n\tint depth;\n\n\tif (path) {\n\t\tstruct ext4_extent *ex;\n\t\tdepth = path->p_depth;\n\n\t\t/* try to predict block placement */\n\t\tex = path[depth].p_ext;\n\t\tif (ex)\n\t\t\treturn ext_pblock(ex)+(block-le32_to_cpu(ex->ee_block));\n\n\t\t/* it looks like index is empty;\n\t\t * try to find starting block from index itself */\n\t\tif (path[depth].p_bh)\n\t\t\treturn path[depth].p_bh->b_blocknr;\n\t}\n\n\t/* OK. use inode's group */\n\tblock_group = ei->i_block_group;\n\tif (flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) {\n\t\t/*\n\t\t * If there are at least EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME\n\t\t * block groups per flexgroup, reserve the first block \n\t\t * group for directories and special files.  Regular \n\t\t * files will start at the second block group.  This\n\t\t * tends to speed up directory access and improves \n\t\t * fsck times.\n\t\t */\n\t\tblock_group &= ~(flex_size-1);\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\tblock_group++;\n\t}\n\tbg_start = (block_group * EXT4_BLOCKS_PER_GROUP(inode->i_sb)) +\n\t\tle32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_first_data_block);\n\tlast_block = ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es) - 1;\n\n\t/*\n\t * If we are doing delayed allocation, we don't need take\n\t * colour into account.\n\t */\n\tif (test_opt(inode->i_sb, DELALLOC))\n\t\treturn bg_start;\n\n\tif (bg_start + EXT4_BLOCKS_PER_GROUP(inode->i_sb) <= last_block)\n\t\tcolour = (current->pid % 16) *\n\t\t\t(EXT4_BLOCKS_PER_GROUP(inode->i_sb) / 16);\n\telse\n\t\tcolour = (current->pid % 16) * ((last_block - bg_start) / 16);\n\treturn bg_start + colour + block;\n}\n\n/*\n * Allocation for a meta data block\n */\nstatic ext4_fsblk_t\next4_ext_new_meta_block(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path,\n\t\t\tstruct ext4_extent *ex, int *err)\n{\n\text4_fsblk_t goal, newblock;\n\n\tgoal = ext4_ext_find_goal(inode, path, le32_to_cpu(ex->ee_block));\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, NULL, err);\n\treturn newblock;\n}\n\nstatic inline int ext4_ext_space_block(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 6)\n\t\t\tsize = 6;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_block_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent_idx);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 5)\n\t\t\tsize = 5;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 3)\n\t\t\tsize = 3;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent_idx);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 4)\n\t\t\tsize = 4;\n#endif\n\t}\n\treturn size;\n}\n\n/*\n * Calculate the number of metadata blocks needed\n * to allocate @blocks\n * Worse case is one block per extent\n */\nint ext4_ext_calc_metadata_amount(struct inode *inode, sector_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint idxs, num = 0;\n\n\tidxs = ((inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t/ sizeof(struct ext4_extent_idx));\n\n\t/*\n\t * If the new delayed allocation block is contiguous with the\n\t * previous da block, it can share index blocks with the\n\t * previous block, so we only need to allocate a new index\n\t * block every idxs leaf blocks.  At ldxs**2 blocks, we need\n\t * an additional index block, and at ldxs**3 blocks, yet\n\t * another index blocks.\n\t */\n\tif (ei->i_da_metadata_calc_len &&\n\t    ei->i_da_metadata_calc_last_lblock+1 == lblock) {\n\t\tif ((ei->i_da_metadata_calc_len % idxs) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {\n\t\t\tnum++;\n\t\t\tei->i_da_metadata_calc_len = 0;\n\t\t} else\n\t\t\tei->i_da_metadata_calc_len++;\n\t\tei->i_da_metadata_calc_last_lblock++;\n\t\treturn num;\n\t}\n\n\t/*\n\t * In the worst case we need a new set of index blocks at\n\t * every level of the inode's extent tree.\n\t */\n\tei->i_da_metadata_calc_len = 1;\n\tei->i_da_metadata_calc_last_lblock = lblock;\n\treturn ext_depth(inode) + 1;\n}\n\nstatic int\next4_ext_max_entries(struct inode *inode, int depth)\n{\n\tint max;\n\n\tif (depth == ext_depth(inode)) {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_root(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_root_idx(inode, 1);\n\t} else {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_block(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_block_idx(inode, 1);\n\t}\n\n\treturn max;\n}\n\nstatic int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)\n{\n\text4_fsblk_t block = ext_pblock(ext);\n\tint len = ext4_ext_get_actual_len(ext);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, len);\n}\n\nstatic int ext4_valid_extent_idx(struct inode *inode,\n\t\t\t\tstruct ext4_extent_idx *ext_idx)\n{\n\text4_fsblk_t block = idx_pblock(ext_idx);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, 1);\n}\n\nstatic int ext4_valid_extent_entries(struct inode *inode,\n\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\tint depth)\n{\n\tstruct ext4_extent *ext;\n\tstruct ext4_extent_idx *ext_idx;\n\tunsigned short entries;\n\tif (eh->eh_entries == 0)\n\t\treturn 1;\n\n\tentries = le16_to_cpu(eh->eh_entries);\n\n\tif (depth == 0) {\n\t\t/* leaf entries */\n\t\text = EXT_FIRST_EXTENT(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent(inode, ext))\n\t\t\t\treturn 0;\n\t\t\text++;\n\t\t\tentries--;\n\t\t}\n\t} else {\n\t\text_idx = EXT_FIRST_INDEX(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent_idx(inode, ext_idx))\n\t\t\t\treturn 0;\n\t\t\text_idx++;\n\t\t\tentries--;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __ext4_ext_check(const char *function, struct inode *inode,\n\t\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\t\tint depth)\n{\n\tconst char *error_msg;\n\tint max = 0;\n\n\tif (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) {\n\t\terror_msg = \"invalid magic\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_depth) != depth)) {\n\t\terror_msg = \"unexpected eh_depth\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(eh->eh_max == 0)) {\n\t\terror_msg = \"invalid eh_max\";\n\t\tgoto corrupted;\n\t}\n\tmax = ext4_ext_max_entries(inode, depth);\n\tif (unlikely(le16_to_cpu(eh->eh_max) > max)) {\n\t\terror_msg = \"too large eh_max\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) {\n\t\terror_msg = \"invalid eh_entries\";\n\t\tgoto corrupted;\n\t}\n\tif (!ext4_valid_extent_entries(inode, eh, depth)) {\n\t\terror_msg = \"invalid extent entries\";\n\t\tgoto corrupted;\n\t}\n\treturn 0;\n\ncorrupted:\n\t__ext4_error(inode->i_sb, function,\n\t\t\t\"bad header/extent in inode #%lu: %s - magic %x, \"\n\t\t\t\"entries %u, max %u(%u), depth %u(%u)\",\n\t\t\tinode->i_ino, error_msg, le16_to_cpu(eh->eh_magic),\n\t\t\tle16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max),\n\t\t\tmax, le16_to_cpu(eh->eh_depth), depth);\n\n\treturn -EIO;\n}\n\n#define ext4_ext_check(inode, eh, depth)\t\\\n\t__ext4_ext_check(__func__, inode, eh, depth)\n\nint ext4_ext_check_inode(struct inode *inode)\n{\n\treturn ext4_ext_check(inode, ext_inode_hdr(inode), ext_depth(inode));\n}\n\n#ifdef EXT_DEBUG\nstatic void ext4_ext_show_path(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint k, l = path->p_depth;\n\n\text_debug(\"path:\");\n\tfor (k = 0; k <= l; k++, path++) {\n\t\tif (path->p_idx) {\n\t\t  ext_debug(\"  %d->%llu\", le32_to_cpu(path->p_idx->ei_block),\n\t\t\t    idx_pblock(path->p_idx));\n\t\t} else if (path->p_ext) {\n\t\t\text_debug(\"  %d:[%d]%d:%llu \",\n\t\t\t\t  le32_to_cpu(path->p_ext->ee_block),\n\t\t\t\t  ext4_ext_is_uninitialized(path->p_ext),\n\t\t\t\t  ext4_ext_get_actual_len(path->p_ext),\n\t\t\t\t  ext_pblock(path->p_ext));\n\t\t} else\n\t\t\text_debug(\"  []\");\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_leaf(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex;\n\tint i;\n\n\tif (!path)\n\t\treturn;\n\n\teh = path[depth].p_hdr;\n\tex = EXT_FIRST_EXTENT(eh);\n\n\text_debug(\"Displaying leaf extents for inode %lu\\n\", inode->i_ino);\n\n\tfor (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ex++) {\n\t\text_debug(\"%d:[%d]%d:%llu \", le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex), ext_pblock(ex));\n\t}\n\text_debug(\"\\n\");\n}\n#else\n#define ext4_ext_show_path(inode, path)\n#define ext4_ext_show_leaf(inode, path)\n#endif\n\nvoid ext4_ext_drop_refs(struct ext4_ext_path *path)\n{\n\tint depth = path->p_depth;\n\tint i;\n\n\tfor (i = 0; i <= depth; i++, path++)\n\t\tif (path->p_bh) {\n\t\t\tbrelse(path->p_bh);\n\t\t\tpath->p_bh = NULL;\n\t\t}\n}\n\n/*\n * ext4_ext_binsearch_idx:\n * binary search for the closest index of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch_idx(struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent_idx *r, *l, *m;\n\n\n\text_debug(\"binsearch for %u(idx):  \", block);\n\n\tl = EXT_FIRST_INDEX(eh) + 1;\n\tr = EXT_LAST_INDEX(eh);\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ei_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ei_block),\n\t\t\t\tm, le32_to_cpu(m->ei_block),\n\t\t\t\tr, le32_to_cpu(r->ei_block));\n\t}\n\n\tpath->p_idx = l - 1;\n\text_debug(\"  -> %d->%lld \", le32_to_cpu(path->p_idx->ei_block),\n\t\t  idx_pblock(path->p_idx));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent_idx *chix, *ix;\n\t\tint k;\n\n\t\tchix = ix = EXT_FIRST_INDEX(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) {\n\t\t  if (k != 0 &&\n\t\t      le32_to_cpu(ix->ei_block) <= le32_to_cpu(ix[-1].ei_block)) {\n\t\t\t\tprintk(KERN_DEBUG \"k=%d, ix=0x%p, \"\n\t\t\t\t       \"first=0x%p\\n\", k,\n\t\t\t\t       ix, EXT_FIRST_INDEX(eh));\n\t\t\t\tprintk(KERN_DEBUG \"%u <= %u\\n\",\n\t\t\t\t       le32_to_cpu(ix->ei_block),\n\t\t\t\t       le32_to_cpu(ix[-1].ei_block));\n\t\t\t}\n\t\t\tBUG_ON(k && le32_to_cpu(ix->ei_block)\n\t\t\t\t\t   <= le32_to_cpu(ix[-1].ei_block));\n\t\t\tif (block < le32_to_cpu(ix->ei_block))\n\t\t\t\tbreak;\n\t\t\tchix = ix;\n\t\t}\n\t\tBUG_ON(chix != path->p_idx);\n\t}\n#endif\n\n}\n\n/*\n * ext4_ext_binsearch:\n * binary search for closest extent of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch(struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent *r, *l, *m;\n\n\tif (eh->eh_entries == 0) {\n\t\t/*\n\t\t * this leaf is empty:\n\t\t * we get such a leaf in split/add case\n\t\t */\n\t\treturn;\n\t}\n\n\text_debug(\"binsearch for %u:  \", block);\n\n\tl = EXT_FIRST_EXTENT(eh) + 1;\n\tr = EXT_LAST_EXTENT(eh);\n\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ee_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ee_block),\n\t\t\t\tm, le32_to_cpu(m->ee_block),\n\t\t\t\tr, le32_to_cpu(r->ee_block));\n\t}\n\n\tpath->p_ext = l - 1;\n\text_debug(\"  -> %d:%llu:[%d]%d \",\n\t\t\tle32_to_cpu(path->p_ext->ee_block),\n\t\t\text_pblock(path->p_ext),\n\t\t\text4_ext_is_uninitialized(path->p_ext),\n\t\t\text4_ext_get_actual_len(path->p_ext));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent *chex, *ex;\n\t\tint k;\n\n\t\tchex = ex = EXT_FIRST_EXTENT(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) {\n\t\t\tBUG_ON(k && le32_to_cpu(ex->ee_block)\n\t\t\t\t\t  <= le32_to_cpu(ex[-1].ee_block));\n\t\t\tif (block < le32_to_cpu(ex->ee_block))\n\t\t\t\tbreak;\n\t\t\tchex = ex;\n\t\t}\n\t\tBUG_ON(chex != path->p_ext);\n\t}\n#endif\n\n}\n\nint ext4_ext_tree_init(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_extent_header *eh;\n\n\teh = ext_inode_hdr(inode);\n\teh->eh_depth = 0;\n\teh->eh_entries = 0;\n\teh->eh_magic = EXT4_EXT_MAGIC;\n\teh->eh_max = cpu_to_le16(ext4_ext_space_root(inode, 0));\n\text4_mark_inode_dirty(handle, inode);\n\text4_ext_invalidate_cache(inode);\n\treturn 0;\n}\n\nstruct ext4_ext_path *\next4_ext_find_extent(struct inode *inode, ext4_lblk_t block,\n\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct buffer_head *bh;\n\tshort int depth, i, ppos = 0, alloc = 0;\n\n\teh = ext_inode_hdr(inode);\n\tdepth = ext_depth(inode);\n\n\t/* account possible depth increase */\n\tif (!path) {\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 2),\n\t\t\t\tGFP_NOFS);\n\t\tif (!path)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\talloc = 1;\n\t}\n\tpath[0].p_hdr = eh;\n\tpath[0].p_bh = NULL;\n\n\ti = depth;\n\t/* walk through the tree */\n\twhile (i) {\n\t\tint need_to_validate = 0;\n\n\t\text_debug(\"depth %d: num %d, max %d\\n\",\n\t\t\t  ppos, le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\n\t\text4_ext_binsearch_idx(inode, path + ppos, block);\n\t\tpath[ppos].p_block = idx_pblock(path[ppos].p_idx);\n\t\tpath[ppos].p_depth = i;\n\t\tpath[ppos].p_ext = NULL;\n\n\t\tbh = sb_getblk(inode->i_sb, path[ppos].p_block);\n\t\tif (unlikely(!bh))\n\t\t\tgoto err;\n\t\tif (!bh_uptodate_or_lock(bh)) {\n\t\t\tif (bh_submit_read(bh) < 0) {\n\t\t\t\tput_bh(bh);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\t/* validate the extent entries */\n\t\t\tneed_to_validate = 1;\n\t\t}\n\t\teh = ext_block_hdr(bh);\n\t\tppos++;\n\t\tBUG_ON(ppos > depth);\n\t\tpath[ppos].p_bh = bh;\n\t\tpath[ppos].p_hdr = eh;\n\t\ti--;\n\n\t\tif (need_to_validate && ext4_ext_check(inode, eh, i))\n\t\t\tgoto err;\n\t}\n\n\tpath[ppos].p_depth = i;\n\tpath[ppos].p_ext = NULL;\n\tpath[ppos].p_idx = NULL;\n\n\t/* find extent */\n\text4_ext_binsearch(inode, path + ppos, block);\n\t/* if not an empty leaf */\n\tif (path[ppos].p_ext)\n\t\tpath[ppos].p_block = ext_pblock(path[ppos].p_ext);\n\n\text4_ext_show_path(inode, path);\n\n\treturn path;\n\nerr:\n\text4_ext_drop_refs(path);\n\tif (alloc)\n\t\tkfree(path);\n\treturn ERR_PTR(-EIO);\n}\n\n/*\n * ext4_ext_insert_index:\n * insert new index [@logical;@ptr] into the block at @curp;\n * check where to insert: before @curp or after @curp\n */\nint ext4_ext_insert_index(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *curp,\n\t\t\t\tint logical, ext4_fsblk_t ptr)\n{\n\tstruct ext4_extent_idx *ix;\n\tint len, err;\n\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\treturn err;\n\n\tBUG_ON(logical == le32_to_cpu(curp->p_idx->ei_block));\n\tlen = EXT_MAX_INDEX(curp->p_hdr) - curp->p_idx;\n\tif (logical > le32_to_cpu(curp->p_idx->ei_block)) {\n\t\t/* insert after */\n\t\tif (curp->p_idx != EXT_LAST_INDEX(curp->p_hdr)) {\n\t\t\tlen = (len - 1) * sizeof(struct ext4_extent_idx);\n\t\t\tlen = len < 0 ? 0 : len;\n\t\t\text_debug(\"insert new index %d after: %llu. \"\n\t\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\t\tlogical, ptr, len,\n\t\t\t\t\t(curp->p_idx + 1), (curp->p_idx + 2));\n\t\t\tmemmove(curp->p_idx + 2, curp->p_idx + 1, len);\n\t\t}\n\t\tix = curp->p_idx + 1;\n\t} else {\n\t\t/* insert before */\n\t\tlen = len * sizeof(struct ext4_extent_idx);\n\t\tlen = len < 0 ? 0 : len;\n\t\text_debug(\"insert new index %d before: %llu. \"\n\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\tlogical, ptr, len,\n\t\t\t\tcurp->p_idx, (curp->p_idx + 1));\n\t\tmemmove(curp->p_idx + 1, curp->p_idx, len);\n\t\tix = curp->p_idx;\n\t}\n\n\tix->ei_block = cpu_to_le32(logical);\n\text4_idx_store_pblock(ix, ptr);\n\tle16_add_cpu(&curp->p_hdr->eh_entries, 1);\n\n\tBUG_ON(le16_to_cpu(curp->p_hdr->eh_entries)\n\t\t\t     > le16_to_cpu(curp->p_hdr->eh_max));\n\tBUG_ON(ix > EXT_LAST_INDEX(curp->p_hdr));\n\n\terr = ext4_ext_dirty(handle, inode, curp);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_split:\n * inserts new subtree into the path, using free index entry\n * at depth @at:\n * - allocates all needed blocks (new leaf and all intermediate index blocks)\n * - makes decision where to split\n * - moves remaining extents and index entries (right to the split point)\n *   into the newly allocated blocks\n * - initializes subtree\n */\nstatic int ext4_ext_split(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int at)\n{\n\tstruct buffer_head *bh = NULL;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tstruct ext4_extent *ex;\n\tint i = at, k, m, a;\n\text4_fsblk_t newblock, oldblock;\n\t__le32 border;\n\text4_fsblk_t *ablocks = NULL; /* array of allocated blocks */\n\tint err = 0;\n\n\t/* make decision: where to split? */\n\t/* FIXME: now decision is simplest: at current extent */\n\n\t/* if current leaf will be split, then we should use\n\t * border from split point */\n\tBUG_ON(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr));\n\tif (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\tborder = path[depth].p_ext[1].ee_block;\n\t\text_debug(\"leaf will be split.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\t  le32_to_cpu(border));\n\t} else {\n\t\tborder = newext->ee_block;\n\t\text_debug(\"leaf will be added.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\tle32_to_cpu(border));\n\t}\n\n\t/*\n\t * If error occurs, then we break processing\n\t * and mark filesystem read-only. index won't\n\t * be inserted and tree will be in consistent\n\t * state. Next mount will repair buffers too.\n\t */\n\n\t/*\n\t * Get array to track all allocated blocks.\n\t * We need this to handle errors and free blocks\n\t * upon them.\n\t */\n\tablocks = kzalloc(sizeof(ext4_fsblk_t) * depth, GFP_NOFS);\n\tif (!ablocks)\n\t\treturn -ENOMEM;\n\n\t/* allocate all needed blocks */\n\text_debug(\"allocate %d blocks for indexes/leaf\\n\", depth - at);\n\tfor (a = 0; a < depth - at; a++) {\n\t\tnewblock = ext4_ext_new_meta_block(handle, inode, path,\n\t\t\t\t\t\t   newext, &err);\n\t\tif (newblock == 0)\n\t\t\tgoto cleanup;\n\t\tablocks[a] = newblock;\n\t}\n\n\t/* initialize new leaf */\n\tnewblock = ablocks[--a];\n\tBUG_ON(newblock == 0);\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err)\n\t\tgoto cleanup;\n\n\tneh = ext_block_hdr(bh);\n\tneh->eh_entries = 0;\n\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tneh->eh_depth = 0;\n\tex = EXT_FIRST_EXTENT(neh);\n\n\t/* move remainder of path[depth] to the new leaf */\n\tBUG_ON(path[depth].p_hdr->eh_entries != path[depth].p_hdr->eh_max);\n\t/* start copy from next extent */\n\t/* TODO: we could do it by single memmove */\n\tm = 0;\n\tpath[depth].p_ext++;\n\twhile (path[depth].p_ext <=\n\t\t\tEXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\text_debug(\"move %d:%llu:[%d]%d in new leaf %llu\\n\",\n\t\t\t\tle32_to_cpu(path[depth].p_ext->ee_block),\n\t\t\t\text_pblock(path[depth].p_ext),\n\t\t\t\text4_ext_is_uninitialized(path[depth].p_ext),\n\t\t\t\text4_ext_get_actual_len(path[depth].p_ext),\n\t\t\t\tnewblock);\n\t\t/*memmove(ex++, path[depth].p_ext++,\n\t\t\t\tsizeof(struct ext4_extent));\n\t\tneh->eh_entries++;*/\n\t\tpath[depth].p_ext++;\n\t\tm++;\n\t}\n\tif (m) {\n\t\tmemmove(ex, path[depth].p_ext-m, sizeof(struct ext4_extent)*m);\n\t\tle16_add_cpu(&neh->eh_entries, m);\n\t}\n\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto cleanup;\n\tbrelse(bh);\n\tbh = NULL;\n\n\t/* correct old leaf */\n\tif (m) {\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tle16_add_cpu(&path[depth].p_hdr->eh_entries, -m);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t}\n\n\t/* create intermediate indexes */\n\tk = depth - at - 1;\n\tBUG_ON(k < 0);\n\tif (k)\n\t\text_debug(\"create %d intermediate indices\\n\", k);\n\t/* insert new index into current index block */\n\t/* current depth stored in i var */\n\ti = depth - 1;\n\twhile (k--) {\n\t\toldblock = newblock;\n\t\tnewblock = ablocks[--a];\n\t\tbh = sb_getblk(inode->i_sb, newblock);\n\t\tif (!bh) {\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tlock_buffer(bh);\n\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\tneh = ext_block_hdr(bh);\n\t\tneh->eh_entries = cpu_to_le16(1);\n\t\tneh->eh_magic = EXT4_EXT_MAGIC;\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\t\tneh->eh_depth = cpu_to_le16(depth - i);\n\t\tfidx = EXT_FIRST_INDEX(neh);\n\t\tfidx->ei_block = border;\n\t\text4_idx_store_pblock(fidx, oldblock);\n\n\t\text_debug(\"int.index at %d (block %llu): %u -> %llu\\n\",\n\t\t\t\ti, newblock, le32_to_cpu(border), oldblock);\n\t\t/* copy indexes */\n\t\tm = 0;\n\t\tpath[i].p_idx++;\n\n\t\text_debug(\"cur 0x%p, last 0x%p\\n\", path[i].p_idx,\n\t\t\t\tEXT_MAX_INDEX(path[i].p_hdr));\n\t\tBUG_ON(EXT_MAX_INDEX(path[i].p_hdr) !=\n\t\t\t\tEXT_LAST_INDEX(path[i].p_hdr));\n\t\twhile (path[i].p_idx <= EXT_MAX_INDEX(path[i].p_hdr)) {\n\t\t\text_debug(\"%d: move %d:%llu in new index %llu\\n\", i,\n\t\t\t\t\tle32_to_cpu(path[i].p_idx->ei_block),\n\t\t\t\t\tidx_pblock(path[i].p_idx),\n\t\t\t\t\tnewblock);\n\t\t\t/*memmove(++fidx, path[i].p_idx++,\n\t\t\t\t\tsizeof(struct ext4_extent_idx));\n\t\t\tneh->eh_entries++;\n\t\t\tBUG_ON(neh->eh_entries > neh->eh_max);*/\n\t\t\tpath[i].p_idx++;\n\t\t\tm++;\n\t\t}\n\t\tif (m) {\n\t\t\tmemmove(++fidx, path[i].p_idx - m,\n\t\t\t\tsizeof(struct ext4_extent_idx) * m);\n\t\t\tle16_add_cpu(&neh->eh_entries, m);\n\t\t}\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tbrelse(bh);\n\t\tbh = NULL;\n\n\t\t/* correct old index */\n\t\tif (m) {\n\t\t\terr = ext4_ext_get_access(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t\tle16_add_cpu(&path[i].p_hdr->eh_entries, -m);\n\t\t\terr = ext4_ext_dirty(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t}\n\n\t\ti--;\n\t}\n\n\t/* insert new index */\n\terr = ext4_ext_insert_index(handle, inode, path + at,\n\t\t\t\t    le32_to_cpu(border), newblock);\n\ncleanup:\n\tif (bh) {\n\t\tif (buffer_locked(bh))\n\t\t\tunlock_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\tif (err) {\n\t\t/* free all allocated blocks in error case */\n\t\tfor (i = 0; i < depth; i++) {\n\t\t\tif (!ablocks[i])\n\t\t\t\tcontinue;\n\t\t\text4_free_blocks(handle, inode, 0, ablocks[i], 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\t\t}\n\t}\n\tkfree(ablocks);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_grow_indepth:\n * implements tree growing procedure:\n * - allocates new block\n * - moves top-level data (index block or leaf) into the new block\n * - initializes new top-level, creating index that points to the\n *   just created block\n */\nstatic int ext4_ext_grow_indepth(handle_t *handle, struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tstruct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *curp = path;\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tstruct buffer_head *bh;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\n\tnewblock = ext4_ext_new_meta_block(handle, inode, path, newext, &err);\n\tif (newblock == 0)\n\t\treturn err;\n\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\text4_std_error(inode->i_sb, err);\n\t\treturn err;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err) {\n\t\tunlock_buffer(bh);\n\t\tgoto out;\n\t}\n\n\t/* move top-level index/leaf into new block */\n\tmemmove(bh->b_data, curp->p_hdr, sizeof(EXT4_I(inode)->i_data));\n\n\t/* set size of new block */\n\tneh = ext_block_hdr(bh);\n\t/* old root could have indexes or leaves\n\t * so calculate e_max right way */\n\tif (ext_depth(inode))\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\telse\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto out;\n\n\t/* create index in new top-level index: num,max,pointer */\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\tgoto out;\n\n\tcurp->p_hdr->eh_magic = EXT4_EXT_MAGIC;\n\tcurp->p_hdr->eh_max = cpu_to_le16(ext4_ext_space_root_idx(inode, 0));\n\tcurp->p_hdr->eh_entries = cpu_to_le16(1);\n\tcurp->p_idx = EXT_FIRST_INDEX(curp->p_hdr);\n\n\tif (path[0].p_hdr->eh_depth)\n\t\tcurp->p_idx->ei_block =\n\t\t\tEXT_FIRST_INDEX(path[0].p_hdr)->ei_block;\n\telse\n\t\tcurp->p_idx->ei_block =\n\t\t\tEXT_FIRST_EXTENT(path[0].p_hdr)->ee_block;\n\text4_idx_store_pblock(curp->p_idx, newblock);\n\n\tneh = ext_inode_hdr(inode);\n\tfidx = EXT_FIRST_INDEX(neh);\n\text_debug(\"new root: num %d(%d), lblock %d, ptr %llu\\n\",\n\t\t  le16_to_cpu(neh->eh_entries), le16_to_cpu(neh->eh_max),\n\t\t  le32_to_cpu(fidx->ei_block), idx_pblock(fidx));\n\n\tneh->eh_depth = cpu_to_le16(path->p_depth + 1);\n\terr = ext4_ext_dirty(handle, inode, curp);\nout:\n\tbrelse(bh);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_create_new_leaf:\n * finds empty index and adds new leaf.\n * if no free index is found, then it requests in-depth growing.\n */\nstatic int ext4_ext_create_new_leaf(handle_t *handle, struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tstruct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *curp;\n\tint depth, i, err = 0;\n\nrepeat:\n\ti = depth = ext_depth(inode);\n\n\t/* walk up to the tree and look for free index entry */\n\tcurp = path + depth;\n\twhile (i > 0 && !EXT_HAS_FREE_INDEX(curp)) {\n\t\ti--;\n\t\tcurp--;\n\t}\n\n\t/* we use already allocated block for index block,\n\t * so subsequent data blocks should be contiguous */\n\tif (EXT_HAS_FREE_INDEX(curp)) {\n\t\t/* if we found index with free entry, then use that\n\t\t * entry: create all needed subtree and add new leaf */\n\t\terr = ext4_ext_split(handle, inode, path, newext, i);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t    (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path))\n\t\t\terr = PTR_ERR(path);\n\t} else {\n\t\t/* tree is full, time to grow in depth */\n\t\terr = ext4_ext_grow_indepth(handle, inode, path, newext);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t   (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * only first (depth 0 -> 1) produces free space;\n\t\t * in all other cases we have to split the grown tree\n\t\t */\n\t\tdepth = ext_depth(inode);\n\t\tif (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) {\n\t\t\t/* now we need to split */\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * search the closest allocated block to the left for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nint\next4_ext_search_left(struct inode *inode, struct ext4_ext_path *path,\n\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\tint depth, ee_len;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tBUG_ON(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex);\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tBUG_ON(ix != EXT_FIRST_INDEX(path[depth].p_hdr));\n\t\t}\n\t\treturn 0;\n\t}\n\n\tBUG_ON(*logical < (le32_to_cpu(ex->ee_block) + ee_len));\n\n\t*logical = le32_to_cpu(ex->ee_block) + ee_len - 1;\n\t*phys = ext_pblock(ex) + ee_len - 1;\n\treturn 0;\n}\n\n/*\n * search the closest allocated block to the right for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nint\next4_ext_search_right(struct inode *inode, struct ext4_ext_path *path,\n\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t block;\n\tint depth;\t/* Note, NOT eh_depth; depth from top of tree */\n\tint ee_len;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tBUG_ON(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex);\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tBUG_ON(ix != EXT_FIRST_INDEX(path[depth].p_hdr));\n\t\t}\n\t\t*logical = le32_to_cpu(ex->ee_block);\n\t\t*phys = ext_pblock(ex);\n\t\treturn 0;\n\t}\n\n\tBUG_ON(*logical < (le32_to_cpu(ex->ee_block) + ee_len));\n\n\tif (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t/* next allocated block in this leaf */\n\t\tex++;\n\t\t*logical = le32_to_cpu(ex->ee_block);\n\t\t*phys = ext_pblock(ex);\n\t\treturn 0;\n\t}\n\n\t/* go up and search for index to the right */\n\twhile (--depth >= 0) {\n\t\tix = path[depth].p_idx;\n\t\tif (ix != EXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\tgoto got_index;\n\t}\n\n\t/* we've gone up to the root and found no index to the right */\n\treturn 0;\n\ngot_index:\n\t/* we've found index to the right, let's\n\t * follow it and find the closest allocated\n\t * block to the right */\n\tix++;\n\tblock = idx_pblock(ix);\n\twhile (++depth < path->p_depth) {\n\t\tbh = sb_bread(inode->i_sb, block);\n\t\tif (bh == NULL)\n\t\t\treturn -EIO;\n\t\teh = ext_block_hdr(bh);\n\t\t/* subtract from p_depth to get proper eh_depth */\n\t\tif (ext4_ext_check(inode, eh, path->p_depth - depth)) {\n\t\t\tput_bh(bh);\n\t\t\treturn -EIO;\n\t\t}\n\t\tix = EXT_FIRST_INDEX(eh);\n\t\tblock = idx_pblock(ix);\n\t\tput_bh(bh);\n\t}\n\n\tbh = sb_bread(inode->i_sb, block);\n\tif (bh == NULL)\n\t\treturn -EIO;\n\teh = ext_block_hdr(bh);\n\tif (ext4_ext_check(inode, eh, path->p_depth - depth)) {\n\t\tput_bh(bh);\n\t\treturn -EIO;\n\t}\n\tex = EXT_FIRST_EXTENT(eh);\n\t*logical = le32_to_cpu(ex->ee_block);\n\t*phys = ext_pblock(ex);\n\tput_bh(bh);\n\treturn 0;\n}\n\n/*\n * ext4_ext_next_allocated_block:\n * returns allocated block in subsequent extent or EXT_MAX_BLOCK.\n * NOTE: it considers block number from index entry as\n * allocated block. Thus, index entries have to be consistent\n * with leaves.\n */\nstatic ext4_lblk_t\next4_ext_next_allocated_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn EXT_MAX_BLOCK;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\t/* leaf */\n\t\t\tif (path[depth].p_ext !=\n\t\t\t\t\tEXT_LAST_EXTENT(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_ext[1].ee_block);\n\t\t} else {\n\t\t\t/* index */\n\t\t\tif (path[depth].p_idx !=\n\t\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\t}\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCK;\n}\n\n/*\n * ext4_ext_next_leaf_block:\n * returns first allocated block from next leaf or EXT_MAX_BLOCK\n */\nstatic ext4_lblk_t ext4_ext_next_leaf_block(struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\t/* zero-tree has no leaf blocks at all */\n\tif (depth == 0)\n\t\treturn EXT_MAX_BLOCK;\n\n\t/* go to index block */\n\tdepth--;\n\n\twhile (depth >= 0) {\n\t\tif (path[depth].p_idx !=\n\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\treturn (ext4_lblk_t)\n\t\t\t\tle32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCK;\n}\n\n/*\n * ext4_ext_correct_indexes:\n * if leaf gets modified and modified extent is first in the leaf,\n * then we have to correct all indexes above.\n * TODO: do we need to correct tree in all cases?\n */\nstatic int ext4_ext_correct_indexes(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\t__le32 border;\n\tint k, err = 0;\n\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tBUG_ON(ex == NULL);\n\tBUG_ON(eh == NULL);\n\n\tif (depth == 0) {\n\t\t/* there is no tree at all */\n\t\treturn 0;\n\t}\n\n\tif (ex != EXT_FIRST_EXTENT(eh)) {\n\t\t/* we correct tree if first leaf got modified only */\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: we need correction if border is smaller than current one\n\t */\n\tk = depth - 1;\n\tborder = path[depth].p_ext->ee_block;\n\terr = ext4_ext_get_access(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\tpath[k].p_idx->ei_block = border;\n\terr = ext4_ext_dirty(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\n\twhile (k--) {\n\t\t/* change all left-side indexes */\n\t\tif (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr))\n\t\t\tbreak;\n\t\terr = ext4_ext_get_access(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath[k].p_idx->ei_block = border;\n\t\terr = ext4_ext_dirty(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint\next4_can_extents_be_merged(struct inode *inode, struct ext4_extent *ex1,\n\t\t\t\tstruct ext4_extent *ex2)\n{\n\tunsigned short ext1_ee_len, ext2_ee_len, max_len;\n\n\t/*\n\t * Make sure that either both extents are uninitialized, or\n\t * both are _not_.\n\t */\n\tif (ext4_ext_is_uninitialized(ex1) ^ ext4_ext_is_uninitialized(ex2))\n\t\treturn 0;\n\n\tif (ext4_ext_is_uninitialized(ex1))\n\t\tmax_len = EXT_UNINIT_MAX_LEN;\n\telse\n\t\tmax_len = EXT_INIT_MAX_LEN;\n\n\text1_ee_len = ext4_ext_get_actual_len(ex1);\n\text2_ee_len = ext4_ext_get_actual_len(ex2);\n\n\tif (le32_to_cpu(ex1->ee_block) + ext1_ee_len !=\n\t\t\tle32_to_cpu(ex2->ee_block))\n\t\treturn 0;\n\n\t/*\n\t * To allow future support for preallocated extents to be added\n\t * as an RO_COMPAT feature, refuse to merge to extents if\n\t * this can result in the top bit of ee_len being set.\n\t */\n\tif (ext1_ee_len + ext2_ee_len > max_len)\n\t\treturn 0;\n#ifdef AGGRESSIVE_TEST\n\tif (ext1_ee_len >= 4)\n\t\treturn 0;\n#endif\n\n\tif (ext_pblock(ex1) + ext1_ee_len == ext_pblock(ex2))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function tries to merge the \"ex\" extent to the next extent in the tree.\n * It always tries to merge towards right. If you want to merge towards\n * left, pass \"ex - 1\" as argument instead of \"ex\".\n * Returns 0 if the extents (ex and ex+1) were _not_ merged and returns\n * 1 if they got merged.\n */\nint ext4_ext_try_to_merge(struct inode *inode,\n\t\t\t  struct ext4_ext_path *path,\n\t\t\t  struct ext4_extent *ex)\n{\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth, len;\n\tint merge_done = 0;\n\tint uninitialized = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\twhile (ex < EXT_LAST_EXTENT(eh)) {\n\t\tif (!ext4_can_extents_be_merged(inode, ex, ex + 1))\n\t\t\tbreak;\n\t\t/* merge with next extent! */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t+ ext4_ext_get_actual_len(ex + 1));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\n\t\tif (ex + 1 < EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = (EXT_LAST_EXTENT(eh) - ex - 1)\n\t\t\t\t* sizeof(struct ext4_extent);\n\t\t\tmemmove(ex + 1, ex + 2, len);\n\t\t}\n\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\tmerge_done = 1;\n\t\tWARN_ON(eh->eh_entries == 0);\n\t\tif (!eh->eh_entries)\n\t\t\text4_error(inode->i_sb,\n\t\t\t\t   \"inode#%lu, eh->eh_entries = 0!\",\n\t\t\t\t   inode->i_ino);\n\t}\n\n\treturn merge_done;\n}\n\n/*\n * check if a portion of the \"newext\" extent overlaps with an\n * existing extent.\n *\n * If there is an overlap discovered, it updates the length of the newext\n * such that there will be no overlap, and then returns 1.\n * If there is no overlap found, it returns 0.\n */\nunsigned int ext4_ext_check_overlap(struct inode *inode,\n\t\t\t\t    struct ext4_extent *newext,\n\t\t\t\t    struct ext4_ext_path *path)\n{\n\text4_lblk_t b1, b2;\n\tunsigned int depth, len1;\n\tunsigned int ret = 0;\n\n\tb1 = le32_to_cpu(newext->ee_block);\n\tlen1 = ext4_ext_get_actual_len(newext);\n\tdepth = ext_depth(inode);\n\tif (!path[depth].p_ext)\n\t\tgoto out;\n\tb2 = le32_to_cpu(path[depth].p_ext->ee_block);\n\n\t/*\n\t * get the next allocated block if the extent in the path\n\t * is before the requested block(s)\n\t */\n\tif (b2 < b1) {\n\t\tb2 = ext4_ext_next_allocated_block(path);\n\t\tif (b2 == EXT_MAX_BLOCK)\n\t\t\tgoto out;\n\t}\n\n\t/* check for wrap through zero on extent logical start block*/\n\tif (b1 + len1 < b1) {\n\t\tlen1 = EXT_MAX_BLOCK - b1;\n\t\tnewext->ee_len = cpu_to_le16(len1);\n\t\tret = 1;\n\t}\n\n\t/* check for overlap */\n\tif (b1 + len1 > b2) {\n\t\tnewext->ee_len = cpu_to_le16(b2 - b1);\n\t\tret = 1;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * ext4_ext_insert_extent:\n * tries to merge requsted extent into the existing extent or\n * inserts requested extent as new one into the tree,\n * creating new leaf in the no-space case.\n */\nint ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int flag)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tunsigned uninitialized = 0;\n\n\tBUG_ON(ext4_ext_get_actual_len(newext) == 0);\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tBUG_ON(path[depth].p_hdr == NULL);\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(flag & EXT4_GET_BLOCKS_PRE_IO)\n\t\t&& ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\text_debug(\"append [%d]%d block to %d:[%d]%d (from %llu)\\n\",\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_is_uninitialized(ex),\n\t\t\t\text4_ext_get_actual_len(ex), ext_pblock(ex));\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t/*\n\t\t * ext4_can_extents_be_merged should have checked that either\n\t\t * both extents are uninitialized, or both aren't. Thus we\n\t\t * need to check only one of them here.\n\t\t */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\teh = path[depth].p_hdr;\n\t\tnearex = ex;\n\t\tgoto merge;\n\t}\n\nrepeat:\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = ext4_ext_next_leaf_block(inode, path);\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block)\n\t    && next != EXT_MAX_BLOCK) {\n\t\text_debug(\"next leaf block - %d\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_ext_find_extent(inode, next, NULL);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isnt full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto repeat;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\terr = ext4_ext_create_new_leaf(handle, inode, path, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %d:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tpath[depth].p_ext = EXT_FIRST_EXTENT(eh);\n\t} else if (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n/*\t\tBUG_ON(newext->ee_block == nearex->ee_block); */\n\t\tif (nearex != EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = EXT_MAX_EXTENT(eh) - nearex;\n\t\t\tlen = (len - 1) * sizeof(struct ext4_extent);\n\t\t\tlen = len < 0 ? 0 : len;\n\t\t\text_debug(\"insert %d:%llu:[%d]%d after: nearest 0x%p, \"\n\t\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\t\tmemmove(nearex + 2, nearex + 1, len);\n\t\t}\n\t\tpath[depth].p_ext = nearex + 1;\n\t} else {\n\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\tlen = (EXT_MAX_EXTENT(eh) - nearex) * sizeof(struct ext4_extent);\n\t\tlen = len < 0 ? 0 : len;\n\t\text_debug(\"insert %d:%llu:[%d]%d before: nearest 0x%p, \"\n\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\tmemmove(nearex + 1, nearex, len);\n\t\tpath[depth].p_ext = nearex;\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tnearex = path[depth].p_ext;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents to the right */\n\tif (!(flag & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(inode, path, nearex);\n\n\t/* try to merge extents to the left */\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\ncleanup:\n\tif (npath) {\n\t\text4_ext_drop_refs(npath);\n\t\tkfree(npath);\n\t}\n\text4_ext_invalidate_cache(inode);\n\treturn err;\n}\n\nint ext4_ext_walk_space(struct inode *inode, ext4_lblk_t block,\n\t\t\text4_lblk_t num, ext_prepare_callback func,\n\t\t\tvoid *cbdata)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_ext_cache cbex;\n\tstruct ext4_extent *ex;\n\text4_lblk_t next, start = 0, end = 0;\n\text4_lblk_t last = block + num;\n\tint depth, exists, err = 0;\n\n\tBUG_ON(func == NULL);\n\tBUG_ON(inode == NULL);\n\n\twhile (block < last && block != EXT_MAX_BLOCK) {\n\t\tnum = last - block;\n\t\t/* find extent for this block */\n\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\t\tpath = ext4_ext_find_extent(inode, block, path);\n\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tpath = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdepth = ext_depth(inode);\n\t\tBUG_ON(path[depth].p_hdr == NULL);\n\t\tex = path[depth].p_ext;\n\t\tnext = ext4_ext_next_allocated_block(path);\n\n\t\texists = 0;\n\t\tif (!ex) {\n\t\t\t/* there is no extent yet, so try to allocate\n\t\t\t * all requested space */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t} else if (le32_to_cpu(ex->ee_block) > block) {\n\t\t\t/* need to allocate space before found extent */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\t\t/* need to allocate space after found extent */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t\tif (end >= next)\n\t\t\t\tend = next;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)) {\n\t\t\t/*\n\t\t\t * some part of requested space is covered\n\t\t\t * by found extent\n\t\t\t */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block)\n\t\t\t\t+ ext4_ext_get_actual_len(ex);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t\texists = 1;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t\tBUG_ON(end <= start);\n\n\t\tif (!exists) {\n\t\t\tcbex.ec_block = start;\n\t\t\tcbex.ec_len = end - start;\n\t\t\tcbex.ec_start = 0;\n\t\t\tcbex.ec_type = EXT4_EXT_CACHE_GAP;\n\t\t} else {\n\t\t\tcbex.ec_block = le32_to_cpu(ex->ee_block);\n\t\t\tcbex.ec_len = ext4_ext_get_actual_len(ex);\n\t\t\tcbex.ec_start = ext_pblock(ex);\n\t\t\tcbex.ec_type = EXT4_EXT_CACHE_EXTENT;\n\t\t}\n\n\t\tBUG_ON(cbex.ec_len == 0);\n\t\terr = func(inode, path, &cbex, ex, cbdata);\n\t\text4_ext_drop_refs(path);\n\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t\tif (err == EXT_REPEAT)\n\t\t\tcontinue;\n\t\telse if (err == EXT_BREAK) {\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ext_depth(inode) != depth) {\n\t\t\t/* depth was changed. we have to realloc path */\n\t\t\tkfree(path);\n\t\t\tpath = NULL;\n\t\t}\n\n\t\tblock = cbex.ec_block + cbex.ec_len;\n\t}\n\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\n\treturn err;\n}\n\nstatic void\next4_ext_put_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t\t__u32 len, ext4_fsblk_t start, int type)\n{\n\tstruct ext4_ext_cache *cex;\n\tBUG_ON(len == 0);\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\tcex->ec_type = type;\n\tcex->ec_block = block;\n\tcex->ec_len = len;\n\tcex->ec_start = start;\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n}\n\n/*\n * ext4_ext_put_gap_in_cache:\n * calculate boundaries of the gap that the requested block fits into\n * and cache this gap\n */\nstatic void\next4_ext_put_gap_in_cache(struct inode *inode, struct ext4_ext_path *path,\n\t\t\t\text4_lblk_t block)\n{\n\tint depth = ext_depth(inode);\n\tunsigned long len;\n\text4_lblk_t lblock;\n\tstruct ext4_extent *ex;\n\n\tex = path[depth].p_ext;\n\tif (ex == NULL) {\n\t\t/* there is no extent yet, so gap is [0;-] */\n\t\tlblock = 0;\n\t\tlen = EXT_MAX_BLOCK;\n\t\text_debug(\"cache gap(whole file):\");\n\t} else if (block < le32_to_cpu(ex->ee_block)) {\n\t\tlblock = block;\n\t\tlen = le32_to_cpu(ex->ee_block) - block;\n\t\text_debug(\"cache gap(before): %u [%u:%u]\",\n\t\t\t\tblock,\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\t ext4_ext_get_actual_len(ex));\n\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\text4_lblk_t next;\n\t\tlblock = le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex);\n\n\t\tnext = ext4_ext_next_allocated_block(path);\n\t\text_debug(\"cache gap(after): [%u:%u] %u\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tblock);\n\t\tBUG_ON(next == lblock);\n\t\tlen = next - lblock;\n\t} else {\n\t\tlblock = len = 0;\n\t\tBUG();\n\t}\n\n\text_debug(\" -> %u:%lu\\n\", lblock, len);\n\text4_ext_put_in_cache(inode, lblock, len, 0, EXT4_EXT_CACHE_GAP);\n}\n\nstatic int\next4_ext_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t\tstruct ext4_extent *ex)\n{\n\tstruct ext4_ext_cache *cex;\n\tint ret = EXT4_EXT_CACHE_NO;\n\n\t/* \n\t * We borrow i_block_reservation_lock to protect i_cached_extent\n\t */\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\n\t/* has cache valid data? */\n\tif (cex->ec_type == EXT4_EXT_CACHE_NO)\n\t\tgoto errout;\n\n\tBUG_ON(cex->ec_type != EXT4_EXT_CACHE_GAP &&\n\t\t\tcex->ec_type != EXT4_EXT_CACHE_EXTENT);\n\tif (block >= cex->ec_block && block < cex->ec_block + cex->ec_len) {\n\t\tex->ee_block = cpu_to_le32(cex->ec_block);\n\t\text4_ext_store_pblock(ex, cex->ec_start);\n\t\tex->ee_len = cpu_to_le16(cex->ec_len);\n\t\text_debug(\"%u cached by %u:%u:%llu\\n\",\n\t\t\t\tblock,\n\t\t\t\tcex->ec_block, cex->ec_len, cex->ec_start);\n\t\tret = cex->ec_type;\n\t}\nerrout:\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\treturn ret;\n}\n\n/*\n * ext4_ext_rm_idx:\n * removes index from the index block.\n * It's used in truncate case only, thus all requests are for\n * last index in the block only.\n */\nstatic int ext4_ext_rm_idx(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path)\n{\n\tint err;\n\text4_fsblk_t leaf;\n\n\t/* free index block */\n\tpath--;\n\tleaf = idx_pblock(path->p_idx);\n\tBUG_ON(path->p_hdr->eh_entries == 0);\n\terr = ext4_ext_get_access(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\tle16_add_cpu(&path->p_hdr->eh_entries, -1);\n\terr = ext4_ext_dirty(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\text_debug(\"index is empty, remove it, free block %llu\\n\", leaf);\n\text4_free_blocks(handle, inode, 0, leaf, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n\treturn err;\n}\n\n/*\n * ext4_ext_calc_credits_for_single_extent:\n * This routine returns max. credits that needed to insert an extent\n * to the extent tree.\n * When pass the actual path, the caller should calculate credits\n * under i_data_sem.\n */\nint ext4_ext_calc_credits_for_single_extent(struct inode *inode, int nrblocks,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path) {\n\t\tint depth = ext_depth(inode);\n\t\tint ret = 0;\n\n\t\t/* probably there is space in leaf? */\n\t\tif (le16_to_cpu(path[depth].p_hdr->eh_entries)\n\t\t\t\t< le16_to_cpu(path[depth].p_hdr->eh_max)) {\n\n\t\t\t/*\n\t\t\t *  There are some space in the leaf tree, no\n\t\t\t *  need to account for leaf block credit\n\t\t\t *\n\t\t\t *  bitmaps and block group descriptor blocks\n\t\t\t *  and other metadat blocks still need to be\n\t\t\t *  accounted.\n\t\t\t */\n\t\t\t/* 1 bitmap, 1 block group descriptor */\n\t\t\tret = 2 + EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ext4_chunk_trans_blocks(inode, nrblocks);\n}\n\n/*\n * How many index/leaf blocks need to change/allocate to modify nrblocks?\n *\n * if nrblocks are fit in a single extent (chunk flag is 1), then\n * in the worse case, each tree level index/leaf need to be changed\n * if the tree split due to insert a new extent, then the old tree\n * index/leaf need to be updated too\n *\n * If the nrblocks are discontiguous, they could cause\n * the whole tree split more than once, but this is really rare.\n */\nint ext4_ext_index_trans_blocks(struct inode *inode, int nrblocks, int chunk)\n{\n\tint index;\n\tint depth = ext_depth(inode);\n\n\tif (chunk)\n\t\tindex = depth * 2;\n\telse\n\t\tindex = depth * 3;\n\n\treturn index;\n}\n\nstatic int ext4_remove_blocks(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_extent *ex,\n\t\t\t\text4_lblk_t from, ext4_lblk_t to)\n{\n\tunsigned short ee_len =  ext4_ext_get_actual_len(ex);\n\tint flags = EXT4_FREE_BLOCKS_FORGET;\n\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\tflags |= EXT4_FREE_BLOCKS_METADATA;\n#ifdef EXTENTS_STATS\n\t{\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\tspin_lock(&sbi->s_ext_stats_lock);\n\t\tsbi->s_ext_blocks += ee_len;\n\t\tsbi->s_ext_extents++;\n\t\tif (ee_len < sbi->s_ext_min)\n\t\t\tsbi->s_ext_min = ee_len;\n\t\tif (ee_len > sbi->s_ext_max)\n\t\t\tsbi->s_ext_max = ee_len;\n\t\tif (ext_depth(inode) > sbi->s_depth_max)\n\t\t\tsbi->s_depth_max = ext_depth(inode);\n\t\tspin_unlock(&sbi->s_ext_stats_lock);\n\t}\n#endif\n\tif (from >= le32_to_cpu(ex->ee_block)\n\t    && to == le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\t/* tail removal */\n\t\text4_lblk_t num;\n\t\text4_fsblk_t start;\n\n\t\tnum = le32_to_cpu(ex->ee_block) + ee_len - from;\n\t\tstart = ext_pblock(ex) + ee_len - num;\n\t\text_debug(\"free last %u blocks starting %llu\\n\", num, start);\n\t\text4_free_blocks(handle, inode, 0, start, num, flags);\n\t} else if (from == le32_to_cpu(ex->ee_block)\n\t\t   && to <= le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\tprintk(KERN_INFO \"strange request: removal %u-%u from %u:%u\\n\",\n\t\t\tfrom, to, le32_to_cpu(ex->ee_block), ee_len);\n\t} else {\n\t\tprintk(KERN_INFO \"strange request: removal(2) \"\n\t\t\t\t\"%u-%u from %u:%u\\n\",\n\t\t\t\tfrom, to, le32_to_cpu(ex->ee_block), ee_len);\n\t}\n\treturn 0;\n}\n\nstatic int\next4_ext_rm_leaf(handle_t *handle, struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t start)\n{\n\tint err = 0, correct_index = 0;\n\tint depth = ext_depth(inode), credits;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t a, b, block;\n\tunsigned num;\n\text4_lblk_t ex_ee_block;\n\tunsigned short ex_ee_len;\n\tunsigned uninitialized = 0;\n\tstruct ext4_extent *ex;\n\n\t/* the header must be checked already in ext4_ext_remove_space() */\n\text_debug(\"truncate since %u in leaf\\n\", start);\n\tif (!path[depth].p_hdr)\n\t\tpath[depth].p_hdr = ext_block_hdr(path[depth].p_bh);\n\teh = path[depth].p_hdr;\n\tBUG_ON(eh == NULL);\n\n\t/* find where to start removing */\n\tex = EXT_LAST_EXTENT(eh);\n\n\tex_ee_block = le32_to_cpu(ex->ee_block);\n\tex_ee_len = ext4_ext_get_actual_len(ex);\n\n\twhile (ex >= EXT_FIRST_EXTENT(eh) &&\n\t\t\tex_ee_block + ex_ee_len > start) {\n\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\telse\n\t\t\tuninitialized = 0;\n\n\t\text_debug(\"remove ext %u:[%d]%d\\n\", ex_ee_block,\n\t\t\t uninitialized, ex_ee_len);\n\t\tpath[depth].p_ext = ex;\n\n\t\ta = ex_ee_block > start ? ex_ee_block : start;\n\t\tb = ex_ee_block + ex_ee_len - 1 < EXT_MAX_BLOCK ?\n\t\t\tex_ee_block + ex_ee_len - 1 : EXT_MAX_BLOCK;\n\n\t\text_debug(\"  border %u:%u\\n\", a, b);\n\n\t\tif (a != ex_ee_block && b != ex_ee_block + ex_ee_len - 1) {\n\t\t\tblock = 0;\n\t\t\tnum = 0;\n\t\t\tBUG();\n\t\t} else if (a != ex_ee_block) {\n\t\t\t/* remove tail of the extent */\n\t\t\tblock = ex_ee_block;\n\t\t\tnum = a - block;\n\t\t} else if (b != ex_ee_block + ex_ee_len - 1) {\n\t\t\t/* remove head of the extent */\n\t\t\tblock = a;\n\t\t\tnum = b - a;\n\t\t\t/* there is no \"make a hole\" API yet */\n\t\t\tBUG();\n\t\t} else {\n\t\t\t/* remove whole extent: excellent! */\n\t\t\tblock = ex_ee_block;\n\t\t\tnum = 0;\n\t\t\tBUG_ON(a != ex_ee_block);\n\t\t\tBUG_ON(b != ex_ee_block + ex_ee_len - 1);\n\t\t}\n\n\t\t/*\n\t\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t\t * descriptor) for each block group; assume two block\n\t\t * groups plus ex_ee_len/blocks_per_block_group for\n\t\t * the worst case\n\t\t */\n\t\tcredits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));\n\t\tif (ex == EXT_FIRST_EXTENT(eh)) {\n\t\t\tcorrect_index = 1;\n\t\t\tcredits += (ext_depth(inode)) + 1;\n\t\t}\n\t\tcredits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);\n\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_remove_blocks(handle, inode, ex, a, b);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (num == 0) {\n\t\t\t/* this extent is removed; mark slot entirely unused */\n\t\t\text4_ext_store_pblock(ex, 0);\n\t\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\t}\n\n\t\tex->ee_block = cpu_to_le32(block);\n\t\tex->ee_len = cpu_to_le16(num);\n\t\t/*\n\t\t * Do not mark uninitialized if all the blocks in the\n\t\t * extent have been removed.\n\t\t */\n\t\tif (uninitialized && num)\n\t\t\text4_ext_mark_uninitialized(ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\text_debug(\"new extent: %u:%u:%llu\\n\", block, num,\n\t\t\t\text_pblock(ex));\n\t\tex--;\n\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t}\n\n\tif (correct_index && eh->eh_entries)\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\n\t/* if this leaf is free, then we should\n\t * remove it from index block above */\n\tif (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)\n\t\terr = ext4_ext_rm_idx(handle, inode, path + depth);\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_more_to_rm:\n * returns 1 if current index has to be freed (even partial)\n */\nstatic int\next4_ext_more_to_rm(struct ext4_ext_path *path)\n{\n\tBUG_ON(path->p_idx == NULL);\n\n\tif (path->p_idx < EXT_FIRST_INDEX(path->p_hdr))\n\t\treturn 0;\n\n\t/*\n\t * if truncate on deeper level happened, it wasn't partial,\n\t * so we have to consider current index for truncation\n\t */\n\tif (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tint depth = ext_depth(inode);\n\tstruct ext4_ext_path *path;\n\thandle_t *handle;\n\tint i = 0, err = 0;\n\n\text_debug(\"truncate since %u\\n\", start);\n\n\t/* probably first extent we're gonna free will be last in block */\n\thandle = ext4_journal_start(inode, depth + 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\text4_ext_invalidate_cache(inode);\n\n\t/*\n\t * We start scanning from right side, freeing all the blocks\n\t * after i_size and walking into the tree depth-wise.\n\t */\n\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 1), GFP_NOFS);\n\tif (path == NULL) {\n\t\text4_journal_stop(handle);\n\t\treturn -ENOMEM;\n\t}\n\tpath[0].p_hdr = ext_inode_hdr(inode);\n\tif (ext4_ext_check(inode, path[0].p_hdr, depth)) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\tpath[0].p_depth = depth;\n\n\twhile (i >= 0 && err == 0) {\n\t\tif (i == depth) {\n\t\t\t/* this is leaf block */\n\t\t\terr = ext4_ext_rm_leaf(handle, inode, path, start);\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* this is index block */\n\t\tif (!path[i].p_hdr) {\n\t\t\text_debug(\"initialize header\\n\");\n\t\t\tpath[i].p_hdr = ext_block_hdr(path[i].p_bh);\n\t\t}\n\n\t\tif (!path[i].p_idx) {\n\t\t\t/* this level hasn't been touched yet */\n\t\t\tpath[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;\n\t\t\text_debug(\"init index ptr: hdr 0x%p, num %d\\n\",\n\t\t\t\t  path[i].p_hdr,\n\t\t\t\t  le16_to_cpu(path[i].p_hdr->eh_entries));\n\t\t} else {\n\t\t\t/* we were already here, see at next index */\n\t\t\tpath[i].p_idx--;\n\t\t}\n\n\t\text_debug(\"level %d - index, first 0x%p, cur 0x%p\\n\",\n\t\t\t\ti, EXT_FIRST_INDEX(path[i].p_hdr),\n\t\t\t\tpath[i].p_idx);\n\t\tif (ext4_ext_more_to_rm(path + i)) {\n\t\t\tstruct buffer_head *bh;\n\t\t\t/* go to the next level */\n\t\t\text_debug(\"move to level %d (block %llu)\\n\",\n\t\t\t\t  i + 1, idx_pblock(path[i].p_idx));\n\t\t\tmemset(path + i + 1, 0, sizeof(*path));\n\t\t\tbh = sb_bread(sb, idx_pblock(path[i].p_idx));\n\t\t\tif (!bh) {\n\t\t\t\t/* should we reset i_size? */\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (WARN_ON(i + 1 > depth)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ext4_ext_check(inode, ext_block_hdr(bh),\n\t\t\t\t\t\t\tdepth - i - 1)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpath[i + 1].p_bh = bh;\n\n\t\t\t/* save actual number of indexes since this\n\t\t\t * number is changed at the next iteration */\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);\n\t\t\ti++;\n\t\t} else {\n\t\t\t/* we finished processing this index, go up */\n\t\t\tif (path[i].p_hdr->eh_entries == 0 && i > 0) {\n\t\t\t\t/* index is empty, remove it;\n\t\t\t\t * handle must be already prepared by the\n\t\t\t\t * truncatei_leaf() */\n\t\t\t\terr = ext4_ext_rm_idx(handle, inode, path + i);\n\t\t\t}\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\text_debug(\"return to level %d\\n\", i);\n\t\t}\n\t}\n\n\t/* TODO: flexible tree reduction should be here */\n\tif (path->p_hdr->eh_entries == 0) {\n\t\t/*\n\t\t * truncate to zero freed all the tree,\n\t\t * so we need to correct eh_depth\n\t\t */\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err == 0) {\n\t\t\text_inode_hdr(inode)->eh_depth = 0;\n\t\t\text_inode_hdr(inode)->eh_max =\n\t\t\t\tcpu_to_le16(ext4_ext_space_root(inode, 0));\n\t\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\t}\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\text4_journal_stop(handle);\n\n\treturn err;\n}\n\n/*\n * called at mount time\n */\nvoid ext4_ext_init(struct super_block *sb)\n{\n\t/*\n\t * possible initialization would be here\n\t */\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) {\n#if defined(AGGRESSIVE_TEST) || defined(CHECK_BINSEARCH) || defined(EXTENTS_STATS)\n\t\tprintk(KERN_INFO \"EXT4-fs: file extents enabled\");\n#ifdef AGGRESSIVE_TEST\n\t\tprintk(\", aggressive tests\");\n#endif\n#ifdef CHECK_BINSEARCH\n\t\tprintk(\", check binsearch\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tprintk(\", stats\");\n#endif\n\t\tprintk(\"\\n\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tspin_lock_init(&EXT4_SB(sb)->s_ext_stats_lock);\n\t\tEXT4_SB(sb)->s_ext_min = 1 << 30;\n\t\tEXT4_SB(sb)->s_ext_max = 0;\n#endif\n\t}\n}\n\n/*\n * called at umount time\n */\nvoid ext4_ext_release(struct super_block *sb)\n{\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS))\n\t\treturn;\n\n#ifdef EXTENTS_STATS\n\tif (EXT4_SB(sb)->s_ext_blocks && EXT4_SB(sb)->s_ext_extents) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\tprintk(KERN_ERR \"EXT4-fs: %lu blocks in %lu extents (%lu ave)\\n\",\n\t\t\tsbi->s_ext_blocks, sbi->s_ext_extents,\n\t\t\tsbi->s_ext_blocks / sbi->s_ext_extents);\n\t\tprintk(KERN_ERR \"EXT4-fs: extents: %lu min, %lu max, max depth %lu\\n\",\n\t\t\tsbi->s_ext_min, sbi->s_ext_max, sbi->s_depth_max);\n\t}\n#endif\n}\n\nstatic void bi_complete(struct bio *bio, int error)\n{\n\tcomplete((struct completion *)bio->bi_private);\n}\n\n/* FIXME!! we need to try to merge to left or right after zero-out  */\nstatic int ext4_ext_zeroout(struct inode *inode, struct ext4_extent *ex)\n{\n\tint ret = -EIO;\n\tstruct bio *bio;\n\tint blkbits, blocksize;\n\tsector_t ee_pblock;\n\tstruct completion event;\n\tunsigned int ee_len, len, done, offset;\n\n\n\tblkbits   = inode->i_blkbits;\n\tblocksize = inode->i_sb->s_blocksize;\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext_pblock(ex);\n\n\t/* convert ee_pblock to 512 byte sectors */\n\tee_pblock = ee_pblock << (blkbits - 9);\n\n\twhile (ee_len > 0) {\n\n\t\tif (ee_len > BIO_MAX_PAGES)\n\t\t\tlen = BIO_MAX_PAGES;\n\t\telse\n\t\t\tlen = ee_len;\n\n\t\tbio = bio_alloc(GFP_NOIO, len);\n\t\tbio->bi_sector = ee_pblock;\n\t\tbio->bi_bdev   = inode->i_sb->s_bdev;\n\n\t\tdone = 0;\n\t\toffset = 0;\n\t\twhile (done < len) {\n\t\t\tret = bio_add_page(bio, ZERO_PAGE(0),\n\t\t\t\t\t\t\tblocksize, offset);\n\t\t\tif (ret != blocksize) {\n\t\t\t\t/*\n\t\t\t\t * We can't add any more pages because of\n\t\t\t\t * hardware limitations.  Start a new bio.\n\t\t\t\t */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdone++;\n\t\t\toffset += blocksize;\n\t\t\tif (offset >= PAGE_CACHE_SIZE)\n\t\t\t\toffset = 0;\n\t\t}\n\n\t\tinit_completion(&event);\n\t\tbio->bi_private = &event;\n\t\tbio->bi_end_io = bi_complete;\n\t\tsubmit_bio(WRITE, bio);\n\t\twait_for_completion(&event);\n\n\t\tif (test_bit(BIO_UPTODATE, &bio->bi_flags))\n\t\t\tret = 0;\n\t\telse {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tbio_put(bio);\n\t\tee_len    -= done;\n\t\tee_pblock += done  << (blkbits - 9);\n\t}\n\treturn ret;\n}\n\n#define EXT4_EXT_ZERO_LEN 7\n/*\n * This function is called by ext4_ext_get_blocks() if someone tries to write\n * to an uninitialized extent. It may result in splitting the uninitialized\n * extent into multiple extents (upto three - one initialized and two\n * uninitialized).\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be initialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n */\nstatic int ext4_ext_convert_to_initialized(handle_t *handle,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\t\text4_lblk_t iblock,\n\t\t\t\t\t\tunsigned int max_blocks)\n{\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex1 = NULL;\n\tstruct ext4_extent *ex2 = NULL;\n\tstruct ext4_extent *ex3 = NULL;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t ee_block;\n\tunsigned int allocated, ee_len, depth;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\tint ret = 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tallocated = ee_len - (iblock - ee_block);\n\tnewblock = iblock - ee_block + ext_pblock(ex);\n\tex2 = ex;\n\torig_ex.ee_block = ex->ee_block;\n\torig_ex.ee_len   = cpu_to_le16(ee_len);\n\text4_ext_store_pblock(&orig_ex, ext_pblock(ex));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* If extent has less than 2*EXT4_EXT_ZERO_LEN zerout directly */\n\tif (ee_len <= 2*EXT4_EXT_ZERO_LEN) {\n\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_block = orig_ex.ee_block;\n\t\tex->ee_len   = orig_ex.ee_len;\n\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t/* zeroed the full extent */\n\t\treturn allocated;\n\t}\n\n\t/* ex1: ee_block to iblock - 1 : uninitialized */\n\tif (iblock > ee_block) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(iblock - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/*\n\t * for sanity, update the length of the ex2 extent before\n\t * we insert ex3, if ex1 is NULL. This is to avoid temporary\n\t * overlap of blocks.\n\t */\n\tif (!ex1 && allocated > max_blocks)\n\t\tex2->ee_len = cpu_to_le16(max_blocks);\n\t/* ex3: to ee_block + ee_len : uninitialised */\n\tif (allocated > max_blocks) {\n\t\tunsigned int newdepth;\n\t\t/* If extent has less than EXT4_EXT_ZERO_LEN zerout directly */\n\t\tif (allocated <= EXT4_EXT_ZERO_LEN) {\n\t\t\t/*\n\t\t\t * iblock == ee_block is handled by the zerouout\n\t\t\t * at the beginning.\n\t\t\t * Mark first half uninitialized.\n\t\t\t * Mark second half initialized and zero out the\n\t\t\t * initialized extent\n\t\t\t */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = cpu_to_le16(ee_len - allocated);\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\n\t\t\tex3 = &newex;\n\t\t\tex3->ee_block = cpu_to_le32(iblock);\n\t\t\text4_ext_store_pblock(ex3, newblock);\n\t\t\tex3->ee_len = cpu_to_le16(allocated);\n\t\t\terr = ext4_ext_insert_extent(handle, inode, path,\n\t\t\t\t\t\t\tex3, 0);\n\t\t\tif (err == -ENOSPC) {\n\t\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto fix_extent_len;\n\t\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t\t/* blocks available from iblock */\n\t\t\t\treturn allocated;\n\n\t\t\t} else if (err)\n\t\t\t\tgoto fix_extent_len;\n\n\t\t\t/*\n\t\t\t * We need to zero out the second half because\n\t\t\t * an fallocate request can update file size and\n\t\t\t * converting the second half to initialized extent\n\t\t\t * implies that we can leak some junk data to user\n\t\t\t * space.\n\t\t\t */\n\t\t\terr =  ext4_ext_zeroout(inode, ex3);\n\t\t\tif (err) {\n\t\t\t\t/*\n\t\t\t\t * We should actually mark the\n\t\t\t\t * second half as uninit and return error\n\t\t\t\t * Insert would have changed the extent\n\t\t\t\t */\n\t\t\t\tdepth = ext_depth(inode);\n\t\t\t\text4_ext_drop_refs(path);\n\t\t\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t\t\t\t\tiblock, path);\n\t\t\t\tif (IS_ERR(path)) {\n\t\t\t\t\terr = PTR_ERR(path);\n\t\t\t\t\treturn err;\n\t\t\t\t}\n\t\t\t\t/* get the second half extent details */\n\t\t\t\tex = path[depth].p_ext;\n\t\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t\t\tpath + depth);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\text4_ext_mark_uninitialized(ex);\n\t\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* zeroed the second half */\n\t\t\treturn allocated;\n\t\t}\n\t\tex3 = &newex;\n\t\tex3->ee_block = cpu_to_le32(iblock + max_blocks);\n\t\text4_ext_store_pblock(ex3, newblock + max_blocks);\n\t\tex3->ee_len = cpu_to_le16(allocated - max_blocks);\n\t\text4_ext_mark_uninitialized(ex3);\n\t\terr = ext4_ext_insert_extent(handle, inode, path, ex3, 0);\n\t\tif (err == -ENOSPC) {\n\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tif (err)\n\t\t\t\tgoto fix_extent_len;\n\t\t\t/* update the extent length and mark as initialized */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t/* zeroed the full extent */\n\t\t\t/* blocks available from iblock */\n\t\t\treturn allocated;\n\n\t\t} else if (err)\n\t\t\tgoto fix_extent_len;\n\t\t/*\n\t\t * The depth, and hence eh & ex might change\n\t\t * as part of the insert above.\n\t\t */\n\t\tnewdepth = ext_depth(inode);\n\t\t/*\n\t\t * update the extent length after successful insert of the\n\t\t * split extent\n\t\t */\n\t\torig_ex.ee_len = cpu_to_le16(ee_len -\n\t\t\t\t\t\text4_ext_get_actual_len(ex3));\n\t\tdepth = newdepth;\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode, iblock, path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\t\teh = path[depth].p_hdr;\n\t\tex = path[depth].p_ext;\n\t\tif (ex2 != &newex)\n\t\t\tex2 = ex;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tallocated = max_blocks;\n\n\t\t/* If extent has less than EXT4_EXT_ZERO_LEN and we are trying\n\t\t * to insert a extent in the middle zerout directly\n\t\t * otherwise give the extent a chance to merge to left\n\t\t */\n\t\tif (le16_to_cpu(orig_ex.ee_len) <= EXT4_EXT_ZERO_LEN &&\n\t\t\t\t\t\t\tiblock != ee_block) {\n\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tif (err)\n\t\t\t\tgoto fix_extent_len;\n\t\t\t/* update the extent length and mark as initialized */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t/* zero out the first half */\n\t\t\t/* blocks available from iblock */\n\t\t\treturn allocated;\n\t\t}\n\t}\n\t/*\n\t * If there was a change of depth as part of the\n\t * insertion of ex3 above, we need to update the length\n\t * of the ex1 extent again here\n\t */\n\tif (ex1 && ex1 != ex) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(iblock - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/* ex2: iblock to iblock + maxblocks-1 : initialised */\n\tex2->ee_block = cpu_to_le32(iblock);\n\text4_ext_store_pblock(ex2, newblock);\n\tex2->ee_len = cpu_to_le16(allocated);\n\tif (ex2 != ex)\n\t\tgoto insert;\n\t/*\n\t * New (initialized) extent starts from the first block\n\t * in the current extent. i.e., ex2 == ex\n\t * We have to see if it can be merged with the extent\n\t * on the left.\n\t */\n\tif (ex2 > EXT_FIRST_EXTENT(eh)) {\n\t\t/*\n\t\t * To merge left, pass \"ex2 - 1\" to try_to_merge(),\n\t\t * since it merges towards right _only_.\n\t\t */\n\t\tret = ext4_ext_try_to_merge(inode, path, ex2 - 1);\n\t\tif (ret) {\n\t\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tdepth = ext_depth(inode);\n\t\t\tex2--;\n\t\t}\n\t}\n\t/*\n\t * Try to Merge towards right. This might be required\n\t * only when the whole extent is being written to.\n\t * i.e. ex2 == ex and ex3 == NULL.\n\t */\n\tif (!ex3) {\n\t\tret = ext4_ext_try_to_merge(inode, path, ex2);\n\t\tif (ret) {\n\t\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tgoto out;\ninsert:\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, 0);\n\tif (err == -ENOSPC) {\n\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_block = orig_ex.ee_block;\n\t\tex->ee_len   = orig_ex.ee_len;\n\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t/* zero out the first half */\n\t\treturn allocated;\n\t} else if (err)\n\t\tgoto fix_extent_len;\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err ? err : allocated;\n\nfix_extent_len:\n\tex->ee_block = orig_ex.ee_block;\n\tex->ee_len   = orig_ex.ee_len;\n\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\text4_ext_mark_uninitialized(ex);\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}\n\n/*\n * This function is called by ext4_ext_get_blocks() from\n * ext4_get_blocks_dio_write() when DIO to write\n * to an uninitialized extent.\n *\n * Writing to an uninitized extent may result in splitting the uninitialized\n * extent into multiple /intialized unintialized extents (up to three)\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be uninitialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * One of more index blocks maybe needed if the extent tree grow after\n * the unintialized extent split. To prevent ENOSPC occur at the IO\n * complete, we need to split the uninitialized extent before DIO submit\n * the IO. The uninitilized extent called at this time will be split\n * into three uninitialized extent(at most). After IO complete, the part\n * being filled will be convert to initialized by the end_io callback function\n * via ext4_convert_unwritten_extents().\n *\n * Returns the size of uninitialized extent to be written on success.\n */\nstatic int ext4_split_unwritten_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\text4_lblk_t iblock,\n\t\t\t\t\tunsigned int max_blocks,\n\t\t\t\t\tint flags)\n{\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex1 = NULL;\n\tstruct ext4_extent *ex2 = NULL;\n\tstruct ext4_extent *ex3 = NULL;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t ee_block;\n\tunsigned int allocated, ee_len, depth;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\n\text_debug(\"ext4_split_unwritten_extents: inode %lu,\"\n\t\t  \"iblock %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t  (unsigned long long)iblock, max_blocks);\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tallocated = ee_len - (iblock - ee_block);\n\tnewblock = iblock - ee_block + ext_pblock(ex);\n\tex2 = ex;\n\torig_ex.ee_block = ex->ee_block;\n\torig_ex.ee_len   = cpu_to_le16(ee_len);\n\text4_ext_store_pblock(&orig_ex, ext_pblock(ex));\n\n\t/*\n \t * If the uninitialized extent begins at the same logical\n \t * block where the write begins, and the write completely\n \t * covers the extent, then we don't need to split it.\n \t */\n\tif ((iblock == ee_block) && (allocated <= max_blocks))\n\t\treturn allocated;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* ex1: ee_block to iblock - 1 : uninitialized */\n\tif (iblock > ee_block) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(iblock - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/*\n\t * for sanity, update the length of the ex2 extent before\n\t * we insert ex3, if ex1 is NULL. This is to avoid temporary\n\t * overlap of blocks.\n\t */\n\tif (!ex1 && allocated > max_blocks)\n\t\tex2->ee_len = cpu_to_le16(max_blocks);\n\t/* ex3: to ee_block + ee_len : uninitialised */\n\tif (allocated > max_blocks) {\n\t\tunsigned int newdepth;\n\t\tex3 = &newex;\n\t\tex3->ee_block = cpu_to_le32(iblock + max_blocks);\n\t\text4_ext_store_pblock(ex3, newblock + max_blocks);\n\t\tex3->ee_len = cpu_to_le16(allocated - max_blocks);\n\t\text4_ext_mark_uninitialized(ex3);\n\t\terr = ext4_ext_insert_extent(handle, inode, path, ex3, flags);\n\t\tif (err == -ENOSPC) {\n\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tif (err)\n\t\t\t\tgoto fix_extent_len;\n\t\t\t/* update the extent length and mark as initialized */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t/* zeroed the full extent */\n\t\t\t/* blocks available from iblock */\n\t\t\treturn allocated;\n\n\t\t} else if (err)\n\t\t\tgoto fix_extent_len;\n\t\t/*\n\t\t * The depth, and hence eh & ex might change\n\t\t * as part of the insert above.\n\t\t */\n\t\tnewdepth = ext_depth(inode);\n\t\t/*\n\t\t * update the extent length after successful insert of the\n\t\t * split extent\n\t\t */\n\t\torig_ex.ee_len = cpu_to_le16(ee_len -\n\t\t\t\t\t\text4_ext_get_actual_len(ex3));\n\t\tdepth = newdepth;\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode, iblock, path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\t\teh = path[depth].p_hdr;\n\t\tex = path[depth].p_ext;\n\t\tif (ex2 != &newex)\n\t\t\tex2 = ex;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tallocated = max_blocks;\n\t}\n\t/*\n\t * If there was a change of depth as part of the\n\t * insertion of ex3 above, we need to update the length\n\t * of the ex1 extent again here\n\t */\n\tif (ex1 && ex1 != ex) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(iblock - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/*\n\t * ex2: iblock to iblock + maxblocks-1 : to be direct IO written,\n\t * uninitialised still.\n\t */\n\tex2->ee_block = cpu_to_le32(iblock);\n\text4_ext_store_pblock(ex2, newblock);\n\tex2->ee_len = cpu_to_le16(allocated);\n\text4_ext_mark_uninitialized(ex2);\n\tif (ex2 != ex)\n\t\tgoto insert;\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\text_debug(\"out here\\n\");\n\tgoto out;\ninsert:\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC) {\n\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_block = orig_ex.ee_block;\n\t\tex->ee_len   = orig_ex.ee_len;\n\t\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t/* zero out the first half */\n\t\treturn allocated;\n\t} else if (err)\n\t\tgoto fix_extent_len;\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err ? err : allocated;\n\nfix_extent_len:\n\tex->ee_block = orig_ex.ee_block;\n\tex->ee_len   = orig_ex.ee_len;\n\text4_ext_store_pblock(ex, ext_pblock(&orig_ex));\n\text4_ext_mark_uninitialized(ex);\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}\nstatic int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t      struct inode *inode,\n\t\t\t\t\t      struct ext4_ext_path *path)\n{\n\tstruct ext4_extent *ex;\n\tstruct ext4_extent_header *eh;\n\tint depth;\n\tint err = 0;\n\tint ret = 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/*\n\t * We have to see if it can be merged with the extent\n\t * on the left.\n\t */\n\tif (ex > EXT_FIRST_EXTENT(eh)) {\n\t\t/*\n\t\t * To merge left, pass \"ex - 1\" to try_to_merge(),\n\t\t * since it merges towards right _only_.\n\t\t */\n\t\tret = ext4_ext_try_to_merge(inode, path, ex - 1);\n\t\tif (ret) {\n\t\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tdepth = ext_depth(inode);\n\t\t\tex--;\n\t\t}\n\t}\n\t/*\n\t * Try to Merge towards right.\n\t */\n\tret = ext4_ext_try_to_merge(inode, path, ex);\n\tif (ret) {\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tdepth = ext_depth(inode);\n\t}\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}\n\nstatic void unmap_underlying_metadata_blocks(struct block_device *bdev,\n\t\t\tsector_t block, int count)\n{\n\tint i;\n\tfor (i = 0; i < count; i++)\n                unmap_underlying_metadata(bdev, block + i);\n}\n\nstatic int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\text4_lblk_t iblock, unsigned int max_blocks,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, struct buffer_head *bh_result,\n\t\t\text4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = EXT4_I(inode)->cur_aio_dio;\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical\"\n\t\t  \"block %llu, max_blocks %u, flags %d, allocated %u\",\n\t\t  inode->i_ino, (unsigned long long)iblock, max_blocks,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle,\n\t\t\t\t\t\tinode, path, iblock,\n\t\t\t\t\t\tmax_blocks, flags);\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to convertion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\tio->flag = EXT4_IO_UNWRITTEN;\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tset_buffer_uninit(bh_result);\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0)\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tset_buffer_unwritten(bh_result);\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode,\n\t\t\t\t\t\tpath, iblock,\n\t\t\t\t\t\tmax_blocks);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tset_buffer_new(bh_result);\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > max_blocks) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + max_blocks,\n\t\t\t\t\tallocated - max_blocks);\n\t\tallocated = max_blocks;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\text4_da_update_reserve_space(inode, allocated, 0);\n\nmap_out:\n\tset_buffer_mapped(bh_result);\nout1:\n\tif (allocated > max_blocks)\n\t\tallocated = max_blocks;\n\text4_ext_show_leaf(inode, path);\n\tbh_result->b_bdev = inode->i_sb->s_bdev;\n\tbh_result->b_blocknr = newblock;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}\n/*\n * Block allocation/map/preallocation routine for extents based files\n *\n *\n * Need to be called with\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system block\n * (ie, create is zero). Otherwise down_write(&EXT4_I(inode)->i_data_sem)\n *\n * return > 0, number of of blocks already mapped/allocated\n *          if create == 0 and these are pre-allocated blocks\n *          \tbuffer head is unmapped\n *          otherwise blocks are mapped\n *\n * return = 0, if plain look up failed (blocks have not been allocated)\n *          buffer head is unmapped\n *\n * return < 0, error case.\n */\nint ext4_ext_get_blocks(handle_t *handle, struct inode *inode,\n\t\t\text4_lblk_t iblock,\n\t\t\tunsigned int max_blocks, struct buffer_head *bh_result,\n\t\t\tint flags)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent newex, *ex, *last_ex;\n\text4_fsblk_t newblock;\n\tint err = 0, depth, ret, cache_type;\n\tunsigned int allocated = 0;\n\tstruct ext4_allocation_request ar;\n\text4_io_end_t *io = EXT4_I(inode)->cur_aio_dio;\n\n\t__clear_bit(BH_New, &bh_result->b_state);\n\text_debug(\"blocks %u/%u requested for inode %lu\\n\",\n\t\t\tiblock, max_blocks, inode->i_ino);\n\n\t/* check in cache */\n\tcache_type = ext4_ext_in_cache(inode, iblock, &newex);\n\tif (cache_type) {\n\t\tif (cache_type == EXT4_EXT_CACHE_GAP) {\n\t\t\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t\t\t/*\n\t\t\t\t * block isn't allocated yet and\n\t\t\t\t * user doesn't want to allocate it\n\t\t\t\t */\n\t\t\t\tgoto out2;\n\t\t\t}\n\t\t\t/* we should allocate requested block */\n\t\t} else if (cache_type == EXT4_EXT_CACHE_EXTENT) {\n\t\t\t/* block is already allocated */\n\t\t\tnewblock = iblock\n\t\t\t\t   - le32_to_cpu(newex.ee_block)\n\t\t\t\t   + ext_pblock(&newex);\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ext4_ext_get_actual_len(&newex) -\n\t\t\t\t\t(iblock - le32_to_cpu(newex.ee_block));\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t}\n\n\t/* find extent for this block */\n\tpath = ext4_ext_find_extent(inode, iblock, NULL);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out2;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * consistent leaf must not be empty;\n\t * this situation is possible, though, _during_ tree modification;\n\t * this is why assert can't be put in ext4_ext_find_extent()\n\t */\n\tif (path[depth].p_ext == NULL && depth != 0) {\n\t\text4_error(inode->i_sb, \"bad extent address \"\n\t\t\t   \"inode: %lu, iblock: %d, depth: %d\",\n\t\t\t   inode->i_ino, iblock, depth);\n\t\terr = -EIO;\n\t\tgoto out2;\n\t}\n\teh = path[depth].p_hdr;\n\n\tex = path[depth].p_ext;\n\tif (ex) {\n\t\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\t\text4_fsblk_t ee_start = ext_pblock(ex);\n\t\tunsigned short ee_len;\n\n\t\t/*\n\t\t * Uninitialized extents are treated as holes, except that\n\t\t * we split out initialized portions during a write.\n\t\t */\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\t\t/* if found extent covers block, simply return it */\n\t\tif (iblock >= ee_block && iblock < ee_block + ee_len) {\n\t\t\tnewblock = iblock - ee_block + ee_start;\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ee_len - (iblock - ee_block);\n\t\t\text_debug(\"%u fit into %u:%d -> %llu\\n\", iblock,\n\t\t\t\t\tee_block, ee_len, newblock);\n\n\t\t\t/* Do not put uninitialized extent in the cache */\n\t\t\tif (!ext4_ext_is_uninitialized(ex)) {\n\t\t\t\text4_ext_put_in_cache(inode, ee_block,\n\t\t\t\t\t\t\tee_len, ee_start,\n\t\t\t\t\t\t\tEXT4_EXT_CACHE_EXTENT);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = ext4_ext_handle_uninitialized_extents(handle,\n\t\t\t\t\tinode, iblock, max_blocks, path,\n\t\t\t\t\tflags, allocated, bh_result, newblock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/*\n\t * requested block isn't allocated yet;\n\t * we couldn't try to create block if create flag is zero\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * put just found gap into cache to speed up\n\t\t * subsequent requests\n\t\t */\n\t\text4_ext_put_gap_in_cache(inode, path, iblock);\n\t\tgoto out2;\n\t}\n\t/*\n\t * Okay, we need to do block allocation.\n\t */\n\n\t/* find neighbour allocated blocks */\n\tar.lleft = iblock;\n\terr = ext4_ext_search_left(inode, path, &ar.lleft, &ar.pleft);\n\tif (err)\n\t\tgoto out2;\n\tar.lright = iblock;\n\terr = ext4_ext_search_right(inode, path, &ar.lright, &ar.pright);\n\tif (err)\n\t\tgoto out2;\n\n\t/*\n\t * See if request is beyond maximum number of blocks we can have in\n\t * a single extent. For an initialized extent this limit is\n\t * EXT_INIT_MAX_LEN and for an uninitialized extent this limit is\n\t * EXT_UNINIT_MAX_LEN.\n\t */\n\tif (max_blocks > EXT_INIT_MAX_LEN &&\n\t    !(flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmax_blocks = EXT_INIT_MAX_LEN;\n\telse if (max_blocks > EXT_UNINIT_MAX_LEN &&\n\t\t (flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmax_blocks = EXT_UNINIT_MAX_LEN;\n\n\t/* Check if we can really insert (iblock)::(iblock+max_blocks) extent */\n\tnewex.ee_block = cpu_to_le32(iblock);\n\tnewex.ee_len = cpu_to_le16(max_blocks);\n\terr = ext4_ext_check_overlap(inode, &newex, path);\n\tif (err)\n\t\tallocated = ext4_ext_get_actual_len(&newex);\n\telse\n\t\tallocated = max_blocks;\n\n\t/* allocate new block */\n\tar.inode = inode;\n\tar.goal = ext4_ext_find_goal(inode, path, iblock);\n\tar.logical = iblock;\n\tar.len = allocated;\n\tif (S_ISREG(inode->i_mode))\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\telse\n\t\t/* disable in-core preallocation for non-regular files */\n\t\tar.flags = 0;\n\tnewblock = ext4_mb_new_blocks(handle, &ar, &err);\n\tif (!newblock)\n\t\tgoto out2;\n\text_debug(\"allocate new block: goal %llu, found %llu/%u\\n\",\n\t\t  ar.goal, newblock, allocated);\n\n\t/* try to insert new extent into found leaf and return */\n\text4_ext_store_pblock(&newex, newblock);\n\tnewex.ee_len = cpu_to_le16(ar.len);\n\t/* Mark uninitialized */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT){\n\t\text4_ext_mark_uninitialized(&newex);\n\t\t/*\n\t\t * io_end structure was created for every IO write to an\n\t\t * uninitialized extent. To avoid unecessary conversion,\n\t\t * here we flag the IO that really needs the conversion.\n\t\t * For non asycn direct IO case, flag the inode state\n\t\t * that we need to perform convertion when IO is done.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\t\tif (io)\n\t\t\t\tio->flag = EXT4_IO_UNWRITTEN;\n\t\t\telse\n\t\t\t\text4_set_inode_state(inode,\n\t\t\t\t\t\t     EXT4_STATE_DIO_UNWRITTEN);\n\t\t}\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tset_buffer_uninit(bh_result);\n\t}\n\n\tif (unlikely(EXT4_I(inode)->i_flags & EXT4_EOFBLOCKS_FL)) {\n\t\tif (eh->eh_entries) {\n\t\t\tlast_ex = EXT_LAST_EXTENT(eh);\n\t\t\tif (iblock + ar.len > le32_to_cpu(last_ex->ee_block)\n\t\t\t\t\t    + ext4_ext_get_actual_len(last_ex))\n\t\t\t\tEXT4_I(inode)->i_flags &= ~EXT4_EOFBLOCKS_FL;\n\t\t} else {\n\t\t\tWARN_ON(eh->eh_entries == 0);\n\t\t\text4_error(inode->i_sb, __func__,\n\t\t\t\t\"inode#%lu, eh->eh_entries = 0!\", inode->i_ino);\n\t\t\t}\n\t}\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err) {\n\t\t/* free data blocks we just allocated */\n\t\t/* not a good idea to call discard here directly,\n\t\t * but otherwise we'd need to call it every free() */\n\t\text4_discard_preallocations(inode);\n\t\text4_free_blocks(handle, inode, 0, ext_pblock(&newex),\n\t\t\t\t ext4_ext_get_actual_len(&newex), 0);\n\t\tgoto out2;\n\t}\n\n\t/* previous routine could use block we allocated */\n\tnewblock = ext_pblock(&newex);\n\tallocated = ext4_ext_get_actual_len(&newex);\n\tif (allocated > max_blocks)\n\t\tallocated = max_blocks;\n\tset_buffer_new(bh_result);\n\n\t/*\n\t * Update reserved blocks/metadata blocks after successful\n\t * block allocation which had been deferred till now.\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\text4_da_update_reserve_space(inode, allocated, 1);\n\n\t/*\n\t * Cache the extent and update transaction to commit on fdatasync only\n\t * when it is _not_ an uninitialized extent.\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_UNINIT_EXT) == 0) {\n\t\text4_ext_put_in_cache(inode, iblock, allocated, newblock,\n\t\t\t\t\t\tEXT4_EXT_CACHE_EXTENT);\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t} else\n\t\text4_update_inode_fsync_trans(handle, inode, 0);\nout:\n\tif (allocated > max_blocks)\n\t\tallocated = max_blocks;\n\text4_ext_show_leaf(inode, path);\n\tset_buffer_mapped(bh_result);\n\tbh_result->b_bdev = inode->i_sb->s_bdev;\n\tbh_result->b_blocknr = newblock;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}\n\nvoid ext4_ext_truncate(struct inode *inode)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t last_block;\n\thandle_t *handle;\n\tint err = 0;\n\n\t/*\n\t * probably first extent we're gonna free will be last in block\n\t */\n\terr = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, err);\n\tif (IS_ERR(handle))\n\t\treturn;\n\n\tif (inode->i_size & (sb->s_blocksize - 1))\n\t\text4_block_truncate_page(handle, mapping, inode->i_size);\n\n\tif (ext4_orphan_add(handle, inode))\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_ext_invalidate_cache(inode);\n\n\text4_discard_preallocations(inode);\n\n\t/*\n\t * TODO: optimization is possible here.\n\t * Probably we need not scan at all,\n\t * because page truncation is enough.\n\t */\n\n\t/* we have to know where to truncate from in crash case */\n\tEXT4_I(inode)->i_disksize = inode->i_size;\n\text4_mark_inode_dirty(handle, inode);\n\n\tlast_block = (inode->i_size + sb->s_blocksize - 1)\n\t\t\t>> EXT4_BLOCK_SIZE_BITS(sb);\n\terr = ext4_ext_remove_space(inode, last_block);\n\n\t/* In a multi-transaction truncate, we only make the final\n\t * transaction synchronous.\n\t */\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\nout_stop:\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\t/*\n\t * If this was a simple ftruncate() and the file will remain alive,\n\t * then we need to clear up the orphan record which we created above.\n\t * However, if this was a real unlink then we were called by\n\t * ext4_delete_inode(), and we allow that function to clean up the\n\t * orphan info for us.\n\t */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(handle, inode);\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\n}\n\nstatic void ext4_falloc_update_inode(struct inode *inode,\n\t\t\t\tint mode, loff_t new_size, int update_ctime)\n{\n\tstruct timespec now;\n\n\tif (update_ctime) {\n\t\tnow = current_fs_time(inode->i_sb);\n\t\tif (!timespec_equal(&inode->i_ctime, &now))\n\t\t\tinode->i_ctime = now;\n\t}\n\t/*\n\t * Update only when preallocation was requested beyond\n\t * the file size.\n\t */\n\tif (!(mode & FALLOC_FL_KEEP_SIZE)) {\n\t\tif (new_size > i_size_read(inode))\n\t\t\ti_size_write(inode, new_size);\n\t\tif (new_size > EXT4_I(inode)->i_disksize)\n\t\t\text4_update_i_disksize(inode, new_size);\n\t} else {\n\t\t/*\n\t\t * Mark that we allocate beyond EOF so the subsequent truncate\n\t\t * can proceed even if the new size is the same as i_size.\n\t\t */\n\t\tif (new_size > i_size_read(inode))\n\t\t\tEXT4_I(inode)->i_flags |= EXT4_EOFBLOCKS_FL;\n\t}\n\n}\n\n/*\n * preallocate space for a file. This implements ext4's fallocate inode\n * operation, which gets called from sys_fallocate system call.\n * For block-mapped files, posix_fallocate should fall back to the method\n * of writing zeroes to the required new blocks (the same behavior which is\n * expected for file systems which do not support fallocate() system call).\n */\nlong ext4_fallocate(struct inode *inode, int mode, loff_t offset, loff_t len)\n{\n\thandle_t *handle;\n\text4_lblk_t block;\n\tloff_t new_size;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint retries = 0;\n\tstruct buffer_head map_bh;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\t/*\n\t * currently supporting (pre)allocate mode for extent-based\n\t * files _only_\n\t */\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL))\n\t\treturn -EOPNOTSUPP;\n\n\t/* preallocation to directories is currently not supported */\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn -ENODEV;\n\n\tblock = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)\n\t\t\t\t\t\t\t- block;\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\tmutex_lock(&inode->i_mutex);\nretry:\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tblock = block + ret;\n\t\tmax_blocks = max_blocks - ret;\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tmap_bh.b_state = 0;\n\t\tret = ext4_get_blocks(handle, inode, block,\n\t\t\t\t      max_blocks, &map_bh,\n\t\t\t\t      EXT4_GET_BLOCKS_CREATE_UNINIT_EXT);\n\t\tif (ret <= 0) {\n#ifdef EXT4FS_DEBUG\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tprintk(KERN_ERR \"%s: ext4_ext_get_blocks \"\n\t\t\t\t    \"returned error inode#%lu, block=%u, \"\n\t\t\t\t    \"max_blocks=%u\", __func__,\n\t\t\t\t    inode->i_ino, block, max_blocks);\n#endif\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\t\tbreak;\n\t\t}\n\t\tif ((block + ret) >= (EXT4_BLOCK_ALIGN(offset + len,\n\t\t\t\t\t\tblkbits) >> blkbits))\n\t\t\tnew_size = offset + len;\n\t\telse\n\t\t\tnew_size = (block + ret) << blkbits;\n\n\t\text4_falloc_update_inode(inode, mode, new_size,\n\t\t\t\t\t\tbuffer_new(&map_bh));\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret2)\n\t\t\tbreak;\n\t}\n\tif (ret == -ENOSPC &&\n\t\t\text4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * This function convert a range of blocks to written extents\n * The caller of this function will pass the start offset and the size.\n * all unwritten extents within this range will be converted to\n * written extents.\n *\n * This function is called from the direct IO end io call back\n * function, to convert the fallocated extents after IO is completed.\n * Returns 0 on success.\n */\nint ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,\n\t\t\t\t    ssize_t len)\n{\n\thandle_t *handle;\n\text4_lblk_t block;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct buffer_head map_bh;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\tblock = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)\n\t\t\t\t\t\t\t- block;\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tblock = block + ret;\n\t\tmax_blocks = max_blocks - ret;\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tmap_bh.b_state = 0;\n\t\tret = ext4_get_blocks(handle, inode, block,\n\t\t\t\t      max_blocks, &map_bh,\n\t\t\t\t      EXT4_GET_BLOCKS_IO_CONVERT_EXT);\n\t\tif (ret <= 0) {\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tprintk(KERN_ERR \"%s: ext4_ext_get_blocks \"\n\t\t\t\t    \"returned error inode#%lu, block=%u, \"\n\t\t\t\t    \"max_blocks=%u\", __func__,\n\t\t\t\t    inode->i_ino, block, max_blocks);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret <= 0 || ret2 )\n\t\t\tbreak;\n\t}\n\treturn ret > 0 ? ret2 : ret;\n}\n/*\n * Callback function called for each extent to gather FIEMAP information.\n */\nstatic int ext4_ext_fiemap_cb(struct inode *inode, struct ext4_ext_path *path,\n\t\t       struct ext4_ext_cache *newex, struct ext4_extent *ex,\n\t\t       void *data)\n{\n\tstruct fiemap_extent_info *fieinfo = data;\n\tunsigned char blksize_bits = inode->i_sb->s_blocksize_bits;\n\t__u64\tlogical;\n\t__u64\tphysical;\n\t__u64\tlength;\n\t__u32\tflags = 0;\n\tint\terror;\n\n\tlogical =  (__u64)newex->ec_block << blksize_bits;\n\n\tif (newex->ec_type == EXT4_EXT_CACHE_GAP) {\n\t\tpgoff_t offset;\n\t\tstruct page *page;\n\t\tstruct buffer_head *bh = NULL;\n\n\t\toffset = logical >> PAGE_SHIFT;\n\t\tpage = find_get_page(inode->i_mapping, offset);\n\t\tif (!page || !page_has_buffers(page))\n\t\t\treturn EXT_CONTINUE;\n\n\t\tbh = page_buffers(page);\n\n\t\tif (!bh)\n\t\t\treturn EXT_CONTINUE;\n\n\t\tif (buffer_delay(bh)) {\n\t\t\tflags |= FIEMAP_EXTENT_DELALLOC;\n\t\t\tpage_cache_release(page);\n\t\t} else {\n\t\t\tpage_cache_release(page);\n\t\t\treturn EXT_CONTINUE;\n\t\t}\n\t}\n\n\tphysical = (__u64)newex->ec_start << blksize_bits;\n\tlength =   (__u64)newex->ec_len << blksize_bits;\n\n\tif (ex && ext4_ext_is_uninitialized(ex))\n\t\tflags |= FIEMAP_EXTENT_UNWRITTEN;\n\n\t/*\n\t * If this extent reaches EXT_MAX_BLOCK, it must be last.\n\t *\n\t * Or if ext4_ext_next_allocated_block is EXT_MAX_BLOCK,\n\t * this also indicates no more allocated blocks.\n\t *\n\t * XXX this might miss a single-block extent at EXT_MAX_BLOCK\n\t */\n\tif (ext4_ext_next_allocated_block(path) == EXT_MAX_BLOCK ||\n\t    newex->ec_block + newex->ec_len - 1 == EXT_MAX_BLOCK) {\n\t\tloff_t size = i_size_read(inode);\n\t\tloff_t bs = EXT4_BLOCK_SIZE(inode->i_sb);\n\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\t\tif ((flags & FIEMAP_EXTENT_DELALLOC) &&\n\t\t    logical+length > size)\n\t\t\tlength = (size - logical + bs - 1) & ~(bs-1);\n\t}\n\n\terror = fiemap_fill_next_extent(fieinfo, logical, physical,\n\t\t\t\t\tlength, flags);\n\tif (error < 0)\n\t\treturn error;\n\tif (error == 1)\n\t\treturn EXT_BREAK;\n\n\treturn EXT_CONTINUE;\n}\n\n/* fiemap flags we can handle specified here */\n#define EXT4_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)\n\nstatic int ext4_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\t__u64 physical = 0;\n\t__u64 length;\n\t__u32 flags = FIEMAP_EXTENT_LAST;\n\tint blockbits = inode->i_sb->s_blocksize_bits;\n\tint error = 0;\n\n\t/* in-inode? */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tstruct ext4_iloc iloc;\n\t\tint offset;\t/* offset of xattr in inode */\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error)\n\t\t\treturn error;\n\t\tphysical = iloc.bh->b_blocknr << blockbits;\n\t\toffset = EXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\tEXT4_I(inode)->i_extra_isize;\n\t\tphysical += offset;\n\t\tlength = EXT4_SB(inode->i_sb)->s_inode_size - offset;\n\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t} else { /* external block */\n\t\tphysical = EXT4_I(inode)->i_file_acl << blockbits;\n\t\tlength = inode->i_sb->s_blocksize;\n\t}\n\n\tif (physical)\n\t\terror = fiemap_fill_next_extent(fieinfo, 0, physical,\n\t\t\t\t\t\tlength, flags);\n\treturn (error < 0 ? error : 0);\n}\n\nint ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\text4_lblk_t start_blk;\n\tint error = 0;\n\n\t/* fallback to generic here if not in extents fmt */\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL))\n\t\treturn generic_block_fiemap(inode, fieinfo, start, len,\n\t\t\text4_get_block);\n\n\tif (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS))\n\t\treturn -EBADR;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\terror = ext4_xattr_fiemap(inode, fieinfo);\n\t} else {\n\t\text4_lblk_t len_blks;\n\t\t__u64 last_blk;\n\n\t\tstart_blk = start >> inode->i_sb->s_blocksize_bits;\n\t\tlast_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;\n\t\tif (last_blk >= EXT_MAX_BLOCK)\n\t\t\tlast_blk = EXT_MAX_BLOCK-1;\n\t\tlen_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;\n\n\t\t/*\n\t\t * Walk the extent tree gathering extent information.\n\t\t * ext4_ext_fiemap_cb will push extents back to user.\n\t\t */\n\t\terror = ext4_ext_walk_space(inode, start_blk, len_blks,\n\t\t\t\t\t  ext4_ext_fiemap_cb, fieinfo);\n\t}\n\n\treturn error;\n}\n\n", "/*\n *  linux/fs/ext4/inode.c\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/fs/minix/inode.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  Goal-directed block allocation by Stephen Tweedie\n *\t(sct@redhat.com), 1993, 1998\n *  Big-endian to little-endian byte-swapping/bitmaps by\n *        David S. Miller (davem@caip.rutgers.edu), 1995\n *  64-bit file support on 64-bit platforms by Jakub Jelinek\n *\t(jj@sunsite.ms.mff.cuni.cz)\n *\n *  Assorted race fixes, rewrite of ext4_get_block() by Al Viro, 2000\n */\n\n#include <linux/module.h>\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/buffer_head.h>\n#include <linux/writeback.h>\n#include <linux/pagevec.h>\n#include <linux/mpage.h>\n#include <linux/namei.h>\n#include <linux/uio.h>\n#include <linux/bio.h>\n#include <linux/workqueue.h>\n#include <linux/kernel.h>\n\n#include \"ext4_jbd2.h\"\n#include \"xattr.h\"\n#include \"acl.h\"\n#include \"ext4_extents.h\"\n\n#include <trace/events/ext4.h>\n\n#define MPAGE_DA_EXTENT_TAIL 0x01\n\nstatic inline int ext4_begin_ordered_truncate(struct inode *inode,\n\t\t\t\t\t      loff_t new_size)\n{\n\treturn jbd2_journal_begin_ordered_truncate(\n\t\t\t\t\tEXT4_SB(inode->i_sb)->s_journal,\n\t\t\t\t\t&EXT4_I(inode)->jinode,\n\t\t\t\t\tnew_size);\n}\n\nstatic void ext4_invalidatepage(struct page *page, unsigned long offset);\n\n/*\n * Test whether an inode is a fast symlink.\n */\nstatic int ext4_inode_is_fast_symlink(struct inode *inode)\n{\n\tint ea_blocks = EXT4_I(inode)->i_file_acl ?\n\t\t(inode->i_sb->s_blocksize >> 9) : 0;\n\n\treturn (S_ISLNK(inode->i_mode) && inode->i_blocks - ea_blocks == 0);\n}\n\n/*\n * Work out how many blocks we need to proceed with the next chunk of a\n * truncate transaction.\n */\nstatic unsigned long blocks_for_truncate(struct inode *inode)\n{\n\text4_lblk_t needed;\n\n\tneeded = inode->i_blocks >> (inode->i_sb->s_blocksize_bits - 9);\n\n\t/* Give ourselves just enough room to cope with inodes in which\n\t * i_blocks is corrupt: we've seen disk corruptions in the past\n\t * which resulted in random data in an inode which looked enough\n\t * like a regular file for ext4 to try to delete it.  Things\n\t * will go a bit crazy if that happens, but at least we should\n\t * try not to panic the whole kernel. */\n\tif (needed < 2)\n\t\tneeded = 2;\n\n\t/* But we need to bound the transaction so we don't overflow the\n\t * journal. */\n\tif (needed > EXT4_MAX_TRANS_DATA)\n\t\tneeded = EXT4_MAX_TRANS_DATA;\n\n\treturn EXT4_DATA_TRANS_BLOCKS(inode->i_sb) + needed;\n}\n\n/*\n * Truncate transactions can be complex and absolutely huge.  So we need to\n * be able to restart the transaction at a conventient checkpoint to make\n * sure we don't overflow the journal.\n *\n * start_transaction gets us a new handle for a truncate transaction,\n * and extend_transaction tries to extend the existing one a bit.  If\n * extend fails, we need to propagate the failure up and restart the\n * transaction in the top-level truncate loop. --sct\n */\nstatic handle_t *start_transaction(struct inode *inode)\n{\n\thandle_t *result;\n\n\tresult = ext4_journal_start(inode, blocks_for_truncate(inode));\n\tif (!IS_ERR(result))\n\t\treturn result;\n\n\text4_std_error(inode->i_sb, PTR_ERR(result));\n\treturn result;\n}\n\n/*\n * Try to extend this transaction for the purposes of truncation.\n *\n * Returns 0 if we managed to create more room.  If we can't create more\n * room, and the transaction must be restarted we return 1.\n */\nstatic int try_to_extend_transaction(handle_t *handle, struct inode *inode)\n{\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (ext4_handle_has_enough_credits(handle, EXT4_RESERVE_TRANS_BLOCKS+1))\n\t\treturn 0;\n\tif (!ext4_journal_extend(handle, blocks_for_truncate(inode)))\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * Restart the transaction associated with *handle.  This does a commit,\n * so before we call here everything must be consistently dirtied against\n * this transaction.\n */\nint ext4_truncate_restart_trans(handle_t *handle, struct inode *inode,\n\t\t\t\t int nblocks)\n{\n\tint ret;\n\n\t/*\n\t * Drop i_data_sem to avoid deadlock with ext4_get_blocks At this\n\t * moment, get_block can be called only for blocks inside i_size since\n\t * page cache has been already dropped and writes are blocked by\n\t * i_mutex. So we can safely drop the i_data_sem here.\n\t */\n\tBUG_ON(EXT4_JOURNAL(inode) == NULL);\n\tjbd_debug(2, \"restarting handle %p\\n\", handle);\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tret = ext4_journal_restart(handle, blocks_for_truncate(inode));\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\treturn ret;\n}\n\n/*\n * Called at the last iput() if i_nlink is zero.\n */\nvoid ext4_delete_inode(struct inode *inode)\n{\n\thandle_t *handle;\n\tint err;\n\n\tif (ext4_should_order_data(inode))\n\t\text4_begin_ordered_truncate(inode, 0);\n\ttruncate_inode_pages(&inode->i_data, 0);\n\n\tif (is_bad_inode(inode))\n\t\tgoto no_delete;\n\n\thandle = ext4_journal_start(inode, blocks_for_truncate(inode)+3);\n\tif (IS_ERR(handle)) {\n\t\text4_std_error(inode->i_sb, PTR_ERR(handle));\n\t\t/*\n\t\t * If we're going to skip the normal cleanup, we still need to\n\t\t * make sure that the in-core orphan linked list is properly\n\t\t * cleaned up.\n\t\t */\n\t\text4_orphan_del(NULL, inode);\n\t\tgoto no_delete;\n\t}\n\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tinode->i_size = 0;\n\terr = ext4_mark_inode_dirty(handle, inode);\n\tif (err) {\n\t\text4_warning(inode->i_sb,\n\t\t\t     \"couldn't mark inode dirty (err %d)\", err);\n\t\tgoto stop_handle;\n\t}\n\tif (inode->i_blocks)\n\t\text4_truncate(inode);\n\n\t/*\n\t * ext4_ext_truncate() doesn't reserve any slop when it\n\t * restarts journal transactions; therefore there may not be\n\t * enough credits left in the handle to remove the inode from\n\t * the orphan list and set the dtime field.\n\t */\n\tif (!ext4_handle_has_enough_credits(handle, 3)) {\n\t\terr = ext4_journal_extend(handle, 3);\n\t\tif (err > 0)\n\t\t\terr = ext4_journal_restart(handle, 3);\n\t\tif (err != 0) {\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"couldn't extend journal (err %d)\", err);\n\t\tstop_handle:\n\t\t\text4_journal_stop(handle);\n\t\t\tgoto no_delete;\n\t\t}\n\t}\n\n\t/*\n\t * Kill off the orphan record which ext4_truncate created.\n\t * AKPM: I think this can be inside the above `if'.\n\t * Note that ext4_orphan_del() has to be able to cope with the\n\t * deletion of a non-existent orphan - this is because we don't\n\t * know if ext4_truncate() actually created an orphan record.\n\t * (Well, we could do this if we need to, but heck - it works)\n\t */\n\text4_orphan_del(handle, inode);\n\tEXT4_I(inode)->i_dtime\t= get_seconds();\n\n\t/*\n\t * One subtle ordering requirement: if anything has gone wrong\n\t * (transaction abort, IO errors, whatever), then we can still\n\t * do these next steps (the fs will already have been marked as\n\t * having errors), but we can't free the inode if the mark_dirty\n\t * fails.\n\t */\n\tif (ext4_mark_inode_dirty(handle, inode))\n\t\t/* If that failed, just do the required in-core inode clear. */\n\t\tclear_inode(inode);\n\telse\n\t\text4_free_inode(handle, inode);\n\text4_journal_stop(handle);\n\treturn;\nno_delete:\n\tclear_inode(inode);\t/* We must guarantee clearing of inode... */\n}\n\ntypedef struct {\n\t__le32\t*p;\n\t__le32\tkey;\n\tstruct buffer_head *bh;\n} Indirect;\n\nstatic inline void add_chain(Indirect *p, struct buffer_head *bh, __le32 *v)\n{\n\tp->key = *(p->p = v);\n\tp->bh = bh;\n}\n\n/**\n *\text4_block_to_path - parse the block number into array of offsets\n *\t@inode: inode in question (we are only interested in its superblock)\n *\t@i_block: block number to be parsed\n *\t@offsets: array to store the offsets in\n *\t@boundary: set this non-zero if the referred-to block is likely to be\n *\t       followed (on disk) by an indirect block.\n *\n *\tTo store the locations of file's data ext4 uses a data structure common\n *\tfor UNIX filesystems - tree of pointers anchored in the inode, with\n *\tdata blocks at leaves and indirect blocks in intermediate nodes.\n *\tThis function translates the block number into path in that tree -\n *\treturn value is the path length and @offsets[n] is the offset of\n *\tpointer to (n+1)th node in the nth one. If @block is out of range\n *\t(negative or too large) warning is printed and zero returned.\n *\n *\tNote: function doesn't find node addresses, so no IO is needed. All\n *\twe need to know is the capacity of indirect blocks (taken from the\n *\tinode->i_sb).\n */\n\n/*\n * Portability note: the last comparison (check that we fit into triple\n * indirect block) is spelled differently, because otherwise on an\n * architecture with 32-bit longs and 8Kb pages we might get into trouble\n * if our filesystem had 8Kb blocks. We might use long long, but that would\n * kill us on x86. Oh, well, at least the sign propagation does not matter -\n * i_block would have to be negative in the very beginning, so we would not\n * get there at all.\n */\n\nstatic int ext4_block_to_path(struct inode *inode,\n\t\t\t      ext4_lblk_t i_block,\n\t\t\t      ext4_lblk_t offsets[4], int *boundary)\n{\n\tint ptrs = EXT4_ADDR_PER_BLOCK(inode->i_sb);\n\tint ptrs_bits = EXT4_ADDR_PER_BLOCK_BITS(inode->i_sb);\n\tconst long direct_blocks = EXT4_NDIR_BLOCKS,\n\t\tindirect_blocks = ptrs,\n\t\tdouble_blocks = (1 << (ptrs_bits * 2));\n\tint n = 0;\n\tint final = 0;\n\n\tif (i_block < direct_blocks) {\n\t\toffsets[n++] = i_block;\n\t\tfinal = direct_blocks;\n\t} else if ((i_block -= direct_blocks) < indirect_blocks) {\n\t\toffsets[n++] = EXT4_IND_BLOCK;\n\t\toffsets[n++] = i_block;\n\t\tfinal = ptrs;\n\t} else if ((i_block -= indirect_blocks) < double_blocks) {\n\t\toffsets[n++] = EXT4_DIND_BLOCK;\n\t\toffsets[n++] = i_block >> ptrs_bits;\n\t\toffsets[n++] = i_block & (ptrs - 1);\n\t\tfinal = ptrs;\n\t} else if (((i_block -= double_blocks) >> (ptrs_bits * 2)) < ptrs) {\n\t\toffsets[n++] = EXT4_TIND_BLOCK;\n\t\toffsets[n++] = i_block >> (ptrs_bits * 2);\n\t\toffsets[n++] = (i_block >> ptrs_bits) & (ptrs - 1);\n\t\toffsets[n++] = i_block & (ptrs - 1);\n\t\tfinal = ptrs;\n\t} else {\n\t\text4_warning(inode->i_sb, \"block %lu > max in inode %lu\",\n\t\t\t     i_block + direct_blocks +\n\t\t\t     indirect_blocks + double_blocks, inode->i_ino);\n\t}\n\tif (boundary)\n\t\t*boundary = final - 1 - (i_block & (ptrs - 1));\n\treturn n;\n}\n\nstatic int __ext4_check_blockref(const char *function, struct inode *inode,\n\t\t\t\t __le32 *p, unsigned int max)\n{\n\t__le32 *bref = p;\n\tunsigned int blk;\n\n\twhile (bref < p+max) {\n\t\tblk = le32_to_cpu(*bref++);\n\t\tif (blk &&\n\t\t    unlikely(!ext4_data_block_valid(EXT4_SB(inode->i_sb),\n\t\t\t\t\t\t    blk, 1))) {\n\t\t\t__ext4_error(inode->i_sb, function,\n\t\t\t\t   \"invalid block reference %u \"\n\t\t\t\t   \"in inode #%lu\", blk, inode->i_ino);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\treturn 0;\n}\n\n\n#define ext4_check_indirect_blockref(inode, bh)                         \\\n\t__ext4_check_blockref(__func__, inode, (__le32 *)(bh)->b_data,  \\\n\t\t\t      EXT4_ADDR_PER_BLOCK((inode)->i_sb))\n\n#define ext4_check_inode_blockref(inode)                                \\\n\t__ext4_check_blockref(__func__, inode, EXT4_I(inode)->i_data,   \\\n\t\t\t      EXT4_NDIR_BLOCKS)\n\n/**\n *\text4_get_branch - read the chain of indirect blocks leading to data\n *\t@inode: inode in question\n *\t@depth: depth of the chain (1 - direct pointer, etc.)\n *\t@offsets: offsets of pointers in inode/indirect blocks\n *\t@chain: place to store the result\n *\t@err: here we store the error value\n *\n *\tFunction fills the array of triples <key, p, bh> and returns %NULL\n *\tif everything went OK or the pointer to the last filled triple\n *\t(incomplete one) otherwise. Upon the return chain[i].key contains\n *\tthe number of (i+1)-th block in the chain (as it is stored in memory,\n *\ti.e. little-endian 32-bit), chain[i].p contains the address of that\n *\tnumber (it points into struct inode for i==0 and into the bh->b_data\n *\tfor i>0) and chain[i].bh points to the buffer_head of i-th indirect\n *\tblock for i>0 and NULL for i==0. In other words, it holds the block\n *\tnumbers of the chain, addresses they were taken from (and where we can\n *\tverify that chain did not change) and buffer_heads hosting these\n *\tnumbers.\n *\n *\tFunction stops when it stumbles upon zero pointer (absent block)\n *\t\t(pointer to last triple returned, *@err == 0)\n *\tor when it gets an IO error reading an indirect block\n *\t\t(ditto, *@err == -EIO)\n *\tor when it reads all @depth-1 indirect blocks successfully and finds\n *\tthe whole chain, all way to the data (returns %NULL, *err == 0).\n *\n *      Need to be called with\n *      down_read(&EXT4_I(inode)->i_data_sem)\n */\nstatic Indirect *ext4_get_branch(struct inode *inode, int depth,\n\t\t\t\t ext4_lblk_t  *offsets,\n\t\t\t\t Indirect chain[4], int *err)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tIndirect *p = chain;\n\tstruct buffer_head *bh;\n\n\t*err = 0;\n\t/* i_data is not going away, no lock needed */\n\tadd_chain(chain, NULL, EXT4_I(inode)->i_data + *offsets);\n\tif (!p->key)\n\t\tgoto no_block;\n\twhile (--depth) {\n\t\tbh = sb_getblk(sb, le32_to_cpu(p->key));\n\t\tif (unlikely(!bh))\n\t\t\tgoto failure;\n\n\t\tif (!bh_uptodate_or_lock(bh)) {\n\t\t\tif (bh_submit_read(bh) < 0) {\n\t\t\t\tput_bh(bh);\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t\t/* validate block references */\n\t\t\tif (ext4_check_indirect_blockref(inode, bh)) {\n\t\t\t\tput_bh(bh);\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t}\n\n\t\tadd_chain(++p, bh, (__le32 *)bh->b_data + *++offsets);\n\t\t/* Reader: end */\n\t\tif (!p->key)\n\t\t\tgoto no_block;\n\t}\n\treturn NULL;\n\nfailure:\n\t*err = -EIO;\nno_block:\n\treturn p;\n}\n\n/**\n *\text4_find_near - find a place for allocation with sufficient locality\n *\t@inode: owner\n *\t@ind: descriptor of indirect block.\n *\n *\tThis function returns the preferred place for block allocation.\n *\tIt is used when heuristic for sequential allocation fails.\n *\tRules are:\n *\t  + if there is a block to the left of our position - allocate near it.\n *\t  + if pointer will live in indirect block - allocate near that block.\n *\t  + if pointer will live in inode - allocate in the same\n *\t    cylinder group.\n *\n * In the latter case we colour the starting block by the callers PID to\n * prevent it from clashing with concurrent allocations for a different inode\n * in the same block group.   The PID is used here so that functionally related\n * files will be close-by on-disk.\n *\n *\tCaller must make sure that @ind is valid and will stay that way.\n */\nstatic ext4_fsblk_t ext4_find_near(struct inode *inode, Indirect *ind)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\t__le32 *start = ind->bh ? (__le32 *) ind->bh->b_data : ei->i_data;\n\t__le32 *p;\n\text4_fsblk_t bg_start;\n\text4_fsblk_t last_block;\n\text4_grpblk_t colour;\n\text4_group_t block_group;\n\tint flex_size = ext4_flex_bg_size(EXT4_SB(inode->i_sb));\n\n\t/* Try to find previous block */\n\tfor (p = ind->p - 1; p >= start; p--) {\n\t\tif (*p)\n\t\t\treturn le32_to_cpu(*p);\n\t}\n\n\t/* No such thing, so let's try location of indirect block */\n\tif (ind->bh)\n\t\treturn ind->bh->b_blocknr;\n\n\t/*\n\t * It is going to be referred to from the inode itself? OK, just put it\n\t * into the same cylinder group then.\n\t */\n\tblock_group = ei->i_block_group;\n\tif (flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) {\n\t\tblock_group &= ~(flex_size-1);\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\tblock_group++;\n\t}\n\tbg_start = ext4_group_first_block_no(inode->i_sb, block_group);\n\tlast_block = ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es) - 1;\n\n\t/*\n\t * If we are doing delayed allocation, we don't need take\n\t * colour into account.\n\t */\n\tif (test_opt(inode->i_sb, DELALLOC))\n\t\treturn bg_start;\n\n\tif (bg_start + EXT4_BLOCKS_PER_GROUP(inode->i_sb) <= last_block)\n\t\tcolour = (current->pid % 16) *\n\t\t\t(EXT4_BLOCKS_PER_GROUP(inode->i_sb) / 16);\n\telse\n\t\tcolour = (current->pid % 16) * ((last_block - bg_start) / 16);\n\treturn bg_start + colour;\n}\n\n/**\n *\text4_find_goal - find a preferred place for allocation.\n *\t@inode: owner\n *\t@block:  block we want\n *\t@partial: pointer to the last triple within a chain\n *\n *\tNormally this function find the preferred place for block allocation,\n *\treturns it.\n *\tBecause this is only used for non-extent files, we limit the block nr\n *\tto 32 bits.\n */\nstatic ext4_fsblk_t ext4_find_goal(struct inode *inode, ext4_lblk_t block,\n\t\t\t\t   Indirect *partial)\n{\n\text4_fsblk_t goal;\n\n\t/*\n\t * XXX need to get goal block from mballoc's data structures\n\t */\n\n\tgoal = ext4_find_near(inode, partial);\n\tgoal = goal & EXT4_MAX_BLOCK_FILE_PHYS;\n\treturn goal;\n}\n\n/**\n *\text4_blks_to_allocate: Look up the block map and count the number\n *\tof direct blocks need to be allocated for the given branch.\n *\n *\t@branch: chain of indirect blocks\n *\t@k: number of blocks need for indirect blocks\n *\t@blks: number of data blocks to be mapped.\n *\t@blocks_to_boundary:  the offset in the indirect block\n *\n *\treturn the total number of blocks to be allocate, including the\n *\tdirect and indirect blocks.\n */\nstatic int ext4_blks_to_allocate(Indirect *branch, int k, unsigned int blks,\n\t\t\t\t int blocks_to_boundary)\n{\n\tunsigned int count = 0;\n\n\t/*\n\t * Simple case, [t,d]Indirect block(s) has not allocated yet\n\t * then it's clear blocks on that path have not allocated\n\t */\n\tif (k > 0) {\n\t\t/* right now we don't handle cross boundary allocation */\n\t\tif (blks < blocks_to_boundary + 1)\n\t\t\tcount += blks;\n\t\telse\n\t\t\tcount += blocks_to_boundary + 1;\n\t\treturn count;\n\t}\n\n\tcount++;\n\twhile (count < blks && count <= blocks_to_boundary &&\n\t\tle32_to_cpu(*(branch[0].p + count)) == 0) {\n\t\tcount++;\n\t}\n\treturn count;\n}\n\n/**\n *\text4_alloc_blocks: multiple allocate blocks needed for a branch\n *\t@indirect_blks: the number of blocks need to allocate for indirect\n *\t\t\tblocks\n *\n *\t@new_blocks: on return it will store the new block numbers for\n *\tthe indirect blocks(if needed) and the first direct block,\n *\t@blks:\ton return it will store the total number of allocated\n *\t\tdirect blocks\n */\nstatic int ext4_alloc_blocks(handle_t *handle, struct inode *inode,\n\t\t\t     ext4_lblk_t iblock, ext4_fsblk_t goal,\n\t\t\t     int indirect_blks, int blks,\n\t\t\t     ext4_fsblk_t new_blocks[4], int *err)\n{\n\tstruct ext4_allocation_request ar;\n\tint target, i;\n\tunsigned long count = 0, blk_allocated = 0;\n\tint index = 0;\n\text4_fsblk_t current_block = 0;\n\tint ret = 0;\n\n\t/*\n\t * Here we try to allocate the requested multiple blocks at once,\n\t * on a best-effort basis.\n\t * To build a branch, we should allocate blocks for\n\t * the indirect blocks(if not allocated yet), and at least\n\t * the first direct block of this branch.  That's the\n\t * minimum number of blocks need to allocate(required)\n\t */\n\t/* first we try to allocate the indirect blocks */\n\ttarget = indirect_blks;\n\twhile (target > 0) {\n\t\tcount = target;\n\t\t/* allocating blocks for indirect blocks and direct blocks */\n\t\tcurrent_block = ext4_new_meta_blocks(handle, inode,\n\t\t\t\t\t\t\tgoal, &count, err);\n\t\tif (*err)\n\t\t\tgoto failed_out;\n\n\t\tBUG_ON(current_block + count > EXT4_MAX_BLOCK_FILE_PHYS);\n\n\t\ttarget -= count;\n\t\t/* allocate blocks for indirect blocks */\n\t\twhile (index < indirect_blks && count) {\n\t\t\tnew_blocks[index++] = current_block++;\n\t\t\tcount--;\n\t\t}\n\t\tif (count > 0) {\n\t\t\t/*\n\t\t\t * save the new block number\n\t\t\t * for the first direct block\n\t\t\t */\n\t\t\tnew_blocks[index] = current_block;\n\t\t\tprintk(KERN_INFO \"%s returned more blocks than \"\n\t\t\t\t\t\t\"requested\\n\", __func__);\n\t\t\tWARN_ON(1);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\ttarget = blks - count ;\n\tblk_allocated = count;\n\tif (!target)\n\t\tgoto allocated;\n\t/* Now allocate data blocks */\n\tmemset(&ar, 0, sizeof(ar));\n\tar.inode = inode;\n\tar.goal = goal;\n\tar.len = target;\n\tar.logical = iblock;\n\tif (S_ISREG(inode->i_mode))\n\t\t/* enable in-core preallocation only for regular files */\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\n\tcurrent_block = ext4_mb_new_blocks(handle, &ar, err);\n\tBUG_ON(current_block + ar.len > EXT4_MAX_BLOCK_FILE_PHYS);\n\n\tif (*err && (target == blks)) {\n\t\t/*\n\t\t * if the allocation failed and we didn't allocate\n\t\t * any blocks before\n\t\t */\n\t\tgoto failed_out;\n\t}\n\tif (!*err) {\n\t\tif (target == blks) {\n\t\t\t/*\n\t\t\t * save the new block number\n\t\t\t * for the first direct block\n\t\t\t */\n\t\t\tnew_blocks[index] = current_block;\n\t\t}\n\t\tblk_allocated += ar.len;\n\t}\nallocated:\n\t/* total number of blocks allocated for direct blocks */\n\tret = blk_allocated;\n\t*err = 0;\n\treturn ret;\nfailed_out:\n\tfor (i = 0; i < index; i++)\n\t\text4_free_blocks(handle, inode, 0, new_blocks[i], 1, 0);\n\treturn ret;\n}\n\n/**\n *\text4_alloc_branch - allocate and set up a chain of blocks.\n *\t@inode: owner\n *\t@indirect_blks: number of allocated indirect blocks\n *\t@blks: number of allocated direct blocks\n *\t@offsets: offsets (in the blocks) to store the pointers to next.\n *\t@branch: place to store the chain in.\n *\n *\tThis function allocates blocks, zeroes out all but the last one,\n *\tlinks them into chain and (if we are synchronous) writes them to disk.\n *\tIn other words, it prepares a branch that can be spliced onto the\n *\tinode. It stores the information about that chain in the branch[], in\n *\tthe same format as ext4_get_branch() would do. We are calling it after\n *\twe had read the existing part of chain and partial points to the last\n *\ttriple of that (one with zero ->key). Upon the exit we have the same\n *\tpicture as after the successful ext4_get_block(), except that in one\n *\tplace chain is disconnected - *branch->p is still zero (we did not\n *\tset the last link), but branch->key contains the number that should\n *\tbe placed into *branch->p to fill that gap.\n *\n *\tIf allocation fails we free all blocks we've allocated (and forget\n *\ttheir buffer_heads) and return the error value the from failed\n *\text4_alloc_block() (normally -ENOSPC). Otherwise we set the chain\n *\tas described above and return 0.\n */\nstatic int ext4_alloc_branch(handle_t *handle, struct inode *inode,\n\t\t\t     ext4_lblk_t iblock, int indirect_blks,\n\t\t\t     int *blks, ext4_fsblk_t goal,\n\t\t\t     ext4_lblk_t *offsets, Indirect *branch)\n{\n\tint blocksize = inode->i_sb->s_blocksize;\n\tint i, n = 0;\n\tint err = 0;\n\tstruct buffer_head *bh;\n\tint num;\n\text4_fsblk_t new_blocks[4];\n\text4_fsblk_t current_block;\n\n\tnum = ext4_alloc_blocks(handle, inode, iblock, goal, indirect_blks,\n\t\t\t\t*blks, new_blocks, &err);\n\tif (err)\n\t\treturn err;\n\n\tbranch[0].key = cpu_to_le32(new_blocks[0]);\n\t/*\n\t * metadata blocks and data blocks are allocated.\n\t */\n\tfor (n = 1; n <= indirect_blks;  n++) {\n\t\t/*\n\t\t * Get buffer_head for parent block, zero it out\n\t\t * and set the pointer to new one, then send\n\t\t * parent to disk.\n\t\t */\n\t\tbh = sb_getblk(inode->i_sb, new_blocks[n-1]);\n\t\tbranch[n].bh = bh;\n\t\tlock_buffer(bh);\n\t\tBUFFER_TRACE(bh, \"call get_create_access\");\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err) {\n\t\t\t/* Don't brelse(bh) here; it's done in\n\t\t\t * ext4_journal_forget() below */\n\t\t\tunlock_buffer(bh);\n\t\t\tgoto failed;\n\t\t}\n\n\t\tmemset(bh->b_data, 0, blocksize);\n\t\tbranch[n].p = (__le32 *) bh->b_data + offsets[n];\n\t\tbranch[n].key = cpu_to_le32(new_blocks[n]);\n\t\t*branch[n].p = branch[n].key;\n\t\tif (n == indirect_blks) {\n\t\t\tcurrent_block = new_blocks[n];\n\t\t\t/*\n\t\t\t * End of chain, update the last new metablock of\n\t\t\t * the chain to point to the new allocated\n\t\t\t * data blocks numbers\n\t\t\t */\n\t\t\tfor (i = 1; i < num; i++)\n\t\t\t\t*(branch[n].p + i) = cpu_to_le32(++current_block);\n\t\t}\n\t\tBUFFER_TRACE(bh, \"marking uptodate\");\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto failed;\n\t}\n\t*blks = num;\n\treturn err;\nfailed:\n\t/* Allocation failed, free what we already allocated */\n\text4_free_blocks(handle, inode, 0, new_blocks[0], 1, 0);\n\tfor (i = 1; i <= n ; i++) {\n\t\t/* \n\t\t * branch[i].bh is newly allocated, so there is no\n\t\t * need to revoke the block, which is why we don't\n\t\t * need to set EXT4_FREE_BLOCKS_METADATA.\n\t\t */\n\t\text4_free_blocks(handle, inode, 0, new_blocks[i], 1,\n\t\t\t\t EXT4_FREE_BLOCKS_FORGET);\n\t}\n\tfor (i = n+1; i < indirect_blks; i++)\n\t\text4_free_blocks(handle, inode, 0, new_blocks[i], 1, 0);\n\n\text4_free_blocks(handle, inode, 0, new_blocks[i], num, 0);\n\n\treturn err;\n}\n\n/**\n * ext4_splice_branch - splice the allocated branch onto inode.\n * @inode: owner\n * @block: (logical) number of block we are adding\n * @chain: chain of indirect blocks (with a missing link - see\n *\text4_alloc_branch)\n * @where: location of missing link\n * @num:   number of indirect blocks we are adding\n * @blks:  number of direct blocks we are adding\n *\n * This function fills the missing link and does all housekeeping needed in\n * inode (->i_blocks, etc.). In case of success we end up with the full\n * chain to new block and return 0.\n */\nstatic int ext4_splice_branch(handle_t *handle, struct inode *inode,\n\t\t\t      ext4_lblk_t block, Indirect *where, int num,\n\t\t\t      int blks)\n{\n\tint i;\n\tint err = 0;\n\text4_fsblk_t current_block;\n\n\t/*\n\t * If we're splicing into a [td]indirect block (as opposed to the\n\t * inode) then we need to get write access to the [td]indirect block\n\t * before the splice.\n\t */\n\tif (where->bh) {\n\t\tBUFFER_TRACE(where->bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, where->bh);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t}\n\t/* That's it */\n\n\t*where->p = where->key;\n\n\t/*\n\t * Update the host buffer_head or inode to point to more just allocated\n\t * direct blocks blocks\n\t */\n\tif (num == 0 && blks > 1) {\n\t\tcurrent_block = le32_to_cpu(where->key) + 1;\n\t\tfor (i = 1; i < blks; i++)\n\t\t\t*(where->p + i) = cpu_to_le32(current_block++);\n\t}\n\n\t/* We are done with atomic stuff, now do the rest of housekeeping */\n\t/* had we spliced it onto indirect block? */\n\tif (where->bh) {\n\t\t/*\n\t\t * If we spliced it onto an indirect block, we haven't\n\t\t * altered the inode.  Note however that if it is being spliced\n\t\t * onto an indirect block at the very end of the file (the\n\t\t * file is growing) then we *will* alter the inode to reflect\n\t\t * the new i_size.  But that is not done here - it is done in\n\t\t * generic_commit_write->__mark_inode_dirty->ext4_dirty_inode.\n\t\t */\n\t\tjbd_debug(5, \"splicing indirect only\\n\");\n\t\tBUFFER_TRACE(where->bh, \"call ext4_handle_dirty_metadata\");\n\t\terr = ext4_handle_dirty_metadata(handle, inode, where->bh);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t} else {\n\t\t/*\n\t\t * OK, we spliced it into the inode itself on a direct block.\n\t\t */\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tjbd_debug(5, \"splicing direct\\n\");\n\t}\n\treturn err;\n\nerr_out:\n\tfor (i = 1; i <= num; i++) {\n\t\t/* \n\t\t * branch[i].bh is newly allocated, so there is no\n\t\t * need to revoke the block, which is why we don't\n\t\t * need to set EXT4_FREE_BLOCKS_METADATA.\n\t\t */\n\t\text4_free_blocks(handle, inode, where[i].bh, 0, 1,\n\t\t\t\t EXT4_FREE_BLOCKS_FORGET);\n\t}\n\text4_free_blocks(handle, inode, 0, le32_to_cpu(where[num].key),\n\t\t\t blks, 0);\n\n\treturn err;\n}\n\n/*\n * The ext4_ind_get_blocks() function handles non-extents inodes\n * (i.e., using the traditional indirect/double-indirect i_blocks\n * scheme) for ext4_get_blocks().\n *\n * Allocation strategy is simple: if we have to allocate something, we will\n * have to go the whole way to leaf. So let's do it before attaching anything\n * to tree, set linkage between the newborn blocks, write them if sync is\n * required, recheck the path, free and repeat if check fails, otherwise\n * set the last missing link (that will protect us from any truncate-generated\n * removals - all blocks on the path are immune now) and possibly force the\n * write on the parent block.\n * That has a nice additional property: no special recovery from the failed\n * allocations is needed - we simply release blocks and do not touch anything\n * reachable from inode.\n *\n * `handle' can be NULL if create == 0.\n *\n * return > 0, # of blocks mapped or allocated.\n * return = 0, if plain lookup failed.\n * return < 0, error case.\n *\n * The ext4_ind_get_blocks() function should be called with\n * down_write(&EXT4_I(inode)->i_data_sem) if allocating filesystem\n * blocks (i.e., flags has EXT4_GET_BLOCKS_CREATE set) or\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system\n * blocks.\n */\nstatic int ext4_ind_get_blocks(handle_t *handle, struct inode *inode,\n\t\t\t       ext4_lblk_t iblock, unsigned int maxblocks,\n\t\t\t       struct buffer_head *bh_result,\n\t\t\t       int flags)\n{\n\tint err = -EIO;\n\text4_lblk_t offsets[4];\n\tIndirect chain[4];\n\tIndirect *partial;\n\text4_fsblk_t goal;\n\tint indirect_blks;\n\tint blocks_to_boundary = 0;\n\tint depth;\n\tint count = 0;\n\text4_fsblk_t first_block = 0;\n\n\tJ_ASSERT(!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL));\n\tJ_ASSERT(handle != NULL || (flags & EXT4_GET_BLOCKS_CREATE) == 0);\n\tdepth = ext4_block_to_path(inode, iblock, offsets,\n\t\t\t\t   &blocks_to_boundary);\n\n\tif (depth == 0)\n\t\tgoto out;\n\n\tpartial = ext4_get_branch(inode, depth, offsets, chain, &err);\n\n\t/* Simplest case - block found, no allocation needed */\n\tif (!partial) {\n\t\tfirst_block = le32_to_cpu(chain[depth - 1].key);\n\t\tclear_buffer_new(bh_result);\n\t\tcount++;\n\t\t/*map more blocks*/\n\t\twhile (count < maxblocks && count <= blocks_to_boundary) {\n\t\t\text4_fsblk_t blk;\n\n\t\t\tblk = le32_to_cpu(*(chain[depth-1].p + count));\n\n\t\t\tif (blk == first_block + count)\n\t\t\t\tcount++;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\t\tgoto got_it;\n\t}\n\n\t/* Next simple case - plain lookup or failed read of indirect block */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0 || err == -EIO)\n\t\tgoto cleanup;\n\n\t/*\n\t * Okay, we need to do block allocation.\n\t*/\n\tgoal = ext4_find_goal(inode, iblock, partial);\n\n\t/* the number of blocks need to allocate for [d,t]indirect blocks */\n\tindirect_blks = (chain + depth) - partial - 1;\n\n\t/*\n\t * Next look up the indirect map to count the totoal number of\n\t * direct blocks to allocate for this branch.\n\t */\n\tcount = ext4_blks_to_allocate(partial, indirect_blks,\n\t\t\t\t\tmaxblocks, blocks_to_boundary);\n\t/*\n\t * Block out ext4_truncate while we alter the tree\n\t */\n\terr = ext4_alloc_branch(handle, inode, iblock, indirect_blks,\n\t\t\t\t&count, goal,\n\t\t\t\toffsets + (partial - chain), partial);\n\n\t/*\n\t * The ext4_splice_branch call will free and forget any buffers\n\t * on the new chain if there is a failure, but that risks using\n\t * up transaction credits, especially for bitmaps where the\n\t * credits cannot be returned.  Can we handle this somehow?  We\n\t * may need to return -EAGAIN upwards in the worst case.  --sct\n\t */\n\tif (!err)\n\t\terr = ext4_splice_branch(handle, inode, iblock,\n\t\t\t\t\t partial, indirect_blks, count);\n\tif (err)\n\t\tgoto cleanup;\n\n\tset_buffer_new(bh_result);\n\n\text4_update_inode_fsync_trans(handle, inode, 1);\ngot_it:\n\tmap_bh(bh_result, inode->i_sb, le32_to_cpu(chain[depth-1].key));\n\tif (count > blocks_to_boundary)\n\t\tset_buffer_boundary(bh_result);\n\terr = count;\n\t/* Clean up and exit */\n\tpartial = chain + depth - 1;\t/* the whole chain */\ncleanup:\n\twhile (partial > chain) {\n\t\tBUFFER_TRACE(partial->bh, \"call brelse\");\n\t\tbrelse(partial->bh);\n\t\tpartial--;\n\t}\n\tBUFFER_TRACE(bh_result, \"returned\");\nout:\n\treturn err;\n}\n\n#ifdef CONFIG_QUOTA\nqsize_t *ext4_get_reserved_space(struct inode *inode)\n{\n\treturn &EXT4_I(inode)->i_reserved_quota;\n}\n#endif\n\n/*\n * Calculate the number of metadata blocks need to reserve\n * to allocate a new block at @lblocks for non extent file based file\n */\nstatic int ext4_indirect_calc_metadata_amount(struct inode *inode,\n\t\t\t\t\t      sector_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint dind_mask = EXT4_ADDR_PER_BLOCK(inode->i_sb) - 1;\n\tint blk_bits;\n\n\tif (lblock < EXT4_NDIR_BLOCKS)\n\t\treturn 0;\n\n\tlblock -= EXT4_NDIR_BLOCKS;\n\n\tif (ei->i_da_metadata_calc_len &&\n\t    (lblock & dind_mask) == ei->i_da_metadata_calc_last_lblock) {\n\t\tei->i_da_metadata_calc_len++;\n\t\treturn 0;\n\t}\n\tei->i_da_metadata_calc_last_lblock = lblock & dind_mask;\n\tei->i_da_metadata_calc_len = 1;\n\tblk_bits = roundup_pow_of_two(lblock + 1);\n\treturn (blk_bits / EXT4_ADDR_PER_BLOCK_BITS(inode->i_sb)) + 1;\n}\n\n/*\n * Calculate the number of metadata blocks need to reserve\n * to allocate a block located at @lblock\n */\nstatic int ext4_calc_metadata_amount(struct inode *inode, sector_t lblock)\n{\n\tif (EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL)\n\t\treturn ext4_ext_calc_metadata_amount(inode, lblock);\n\n\treturn ext4_indirect_calc_metadata_amount(inode, lblock);\n}\n\n/*\n * Called with i_data_sem down, which is important since we can call\n * ext4_discard_preallocations() from here.\n */\nvoid ext4_da_update_reserve_space(struct inode *inode,\n\t\t\t\t\tint used, int quota_claim)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint mdb_free = 0, allocated_meta_blocks = 0;\n\n\tspin_lock(&ei->i_block_reservation_lock);\n\ttrace_ext4_da_update_reserve_space(inode, used);\n\tif (unlikely(used > ei->i_reserved_data_blocks)) {\n\t\text4_msg(inode->i_sb, KERN_NOTICE, \"%s: ino %lu, used %d \"\n\t\t\t \"with only %d reserved data blocks\\n\",\n\t\t\t __func__, inode->i_ino, used,\n\t\t\t ei->i_reserved_data_blocks);\n\t\tWARN_ON(1);\n\t\tused = ei->i_reserved_data_blocks;\n\t}\n\n\t/* Update per-inode reservations */\n\tei->i_reserved_data_blocks -= used;\n\tused += ei->i_allocated_meta_blocks;\n\tei->i_reserved_meta_blocks -= ei->i_allocated_meta_blocks;\n\tallocated_meta_blocks = ei->i_allocated_meta_blocks;\n\tei->i_allocated_meta_blocks = 0;\n\tpercpu_counter_sub(&sbi->s_dirtyblocks_counter, used);\n\n\tif (ei->i_reserved_data_blocks == 0) {\n\t\t/*\n\t\t * We can release all of the reserved metadata blocks\n\t\t * only when we have written all of the delayed\n\t\t * allocation blocks.\n\t\t */\n\t\tmdb_free = ei->i_reserved_meta_blocks;\n\t\tei->i_reserved_meta_blocks = 0;\n\t\tei->i_da_metadata_calc_len = 0;\n\t\tpercpu_counter_sub(&sbi->s_dirtyblocks_counter, mdb_free);\n\t}\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\t/* Update quota subsystem */\n\tif (quota_claim) {\n\t\tvfs_dq_claim_block(inode, used);\n\t\tif (mdb_free)\n\t\t\tvfs_dq_release_reservation_block(inode, mdb_free);\n\t} else {\n\t\t/*\n\t\t * We did fallocate with an offset that is already delayed\n\t\t * allocated. So on delayed allocated writeback we should\n\t\t * not update the quota for allocated blocks. But then\n\t\t * converting an fallocate region to initialized region would\n\t\t * have caused a metadata allocation. So claim quota for\n\t\t * that\n\t\t */\n\t\tif (allocated_meta_blocks)\n\t\t\tvfs_dq_claim_block(inode, allocated_meta_blocks);\n\t\tvfs_dq_release_reservation_block(inode, mdb_free + used);\n\t}\n\n\t/*\n\t * If we have done all the pending block allocations and if\n\t * there aren't any writers on the inode, we can discard the\n\t * inode's preallocations.\n\t */\n\tif ((ei->i_reserved_data_blocks == 0) &&\n\t    (atomic_read(&inode->i_writecount) == 0))\n\t\text4_discard_preallocations(inode);\n}\n\nstatic int check_block_validity(struct inode *inode, const char *msg,\n\t\t\t\tsector_t logical, sector_t phys, int len)\n{\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), phys, len)) {\n\t\t__ext4_error(inode->i_sb, msg,\n\t\t\t   \"inode #%lu logical block %llu mapped to %llu \"\n\t\t\t   \"(size %d)\", inode->i_ino,\n\t\t\t   (unsigned long long) logical,\n\t\t\t   (unsigned long long) phys, len);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\n/*\n * Return the number of contiguous dirty pages in a given inode\n * starting at page frame idx.\n */\nstatic pgoff_t ext4_num_dirty_pages(struct inode *inode, pgoff_t idx,\n\t\t\t\t    unsigned int max_pages)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tpgoff_t\tindex;\n\tstruct pagevec pvec;\n\tpgoff_t num = 0;\n\tint i, nr_pages, done = 0;\n\n\tif (max_pages == 0)\n\t\treturn 0;\n\tpagevec_init(&pvec, 0);\n\twhile (!done) {\n\t\tindex = idx;\n\t\tnr_pages = pagevec_lookup_tag(&pvec, mapping, &index,\n\t\t\t\t\t      PAGECACHE_TAG_DIRTY,\n\t\t\t\t\t      (pgoff_t)PAGEVEC_SIZE);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\t\t\tstruct buffer_head *bh, *head;\n\n\t\t\tlock_page(page);\n\t\t\tif (unlikely(page->mapping != mapping) ||\n\t\t\t    !PageDirty(page) ||\n\t\t\t    PageWriteback(page) ||\n\t\t\t    page->index != idx) {\n\t\t\t\tdone = 1;\n\t\t\t\tunlock_page(page);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (page_has_buffers(page)) {\n\t\t\t\tbh = head = page_buffers(page);\n\t\t\t\tdo {\n\t\t\t\t\tif (!buffer_delay(bh) &&\n\t\t\t\t\t    !buffer_unwritten(bh))\n\t\t\t\t\t\tdone = 1;\n\t\t\t\t\tbh = bh->b_this_page;\n\t\t\t\t} while (!done && (bh != head));\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t\tif (done)\n\t\t\t\tbreak;\n\t\t\tidx++;\n\t\t\tnum++;\n\t\t\tif (num >= max_pages)\n\t\t\t\tbreak;\n\t\t}\n\t\tpagevec_release(&pvec);\n\t}\n\treturn num;\n}\n\n/*\n * The ext4_get_blocks() function tries to look up the requested blocks,\n * and returns if the blocks are already mapped.\n *\n * Otherwise it takes the write lock of the i_data_sem and allocate blocks\n * and store the allocated blocks in the result buffer head and mark it\n * mapped.\n *\n * If file type is extents based, it will call ext4_ext_get_blocks(),\n * Otherwise, call with ext4_ind_get_blocks() to handle indirect mapping\n * based files\n *\n * On success, it returns the number of blocks being mapped or allocate.\n * if create==0 and the blocks are pre-allocated and uninitialized block,\n * the result buffer head is unmapped. If the create ==1, it will make sure\n * the buffer head is mapped.\n *\n * It returns 0 if plain look up failed (blocks have not been allocated), in\n * that casem, buffer head is unmapped\n *\n * It returns the error in case of allocation failure.\n */\nint ext4_get_blocks(handle_t *handle, struct inode *inode, sector_t block,\n\t\t    unsigned int max_blocks, struct buffer_head *bh,\n\t\t    int flags)\n{\n\tint retval;\n\n\tclear_buffer_mapped(bh);\n\tclear_buffer_unwritten(bh);\n\n\text_debug(\"ext4_get_blocks(): inode %lu, flag %d, max_blocks %u,\"\n\t\t  \"logical block %lu\\n\", inode->i_ino, flags, max_blocks,\n\t\t  (unsigned long)block);\n\t/*\n\t * Try to see if we can get the block without requesting a new\n\t * file system block.\n\t */\n\tdown_read((&EXT4_I(inode)->i_data_sem));\n\tif (EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL) {\n\t\tretval =  ext4_ext_get_blocks(handle, inode, block, max_blocks,\n\t\t\t\tbh, 0);\n\t} else {\n\t\tretval = ext4_ind_get_blocks(handle, inode, block, max_blocks,\n\t\t\t\t\t     bh, 0);\n\t}\n\tup_read((&EXT4_I(inode)->i_data_sem));\n\n\tif (retval > 0 && buffer_mapped(bh)) {\n\t\tint ret = check_block_validity(inode, \"file system corruption\",\n\t\t\t\t\t       block, bh->b_blocknr, retval);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\n\t/* If it is only a block(s) look up */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0)\n\t\treturn retval;\n\n\t/*\n\t * Returns if the blocks have already allocated\n\t *\n\t * Note that if blocks have been preallocated\n\t * ext4_ext_get_block() returns th create = 0\n\t * with buffer head unmapped.\n\t */\n\tif (retval > 0 && buffer_mapped(bh))\n\t\treturn retval;\n\n\t/*\n\t * When we call get_blocks without the create flag, the\n\t * BH_Unwritten flag could have gotten set if the blocks\n\t * requested were part of a uninitialized extent.  We need to\n\t * clear this flag now that we are committed to convert all or\n\t * part of the uninitialized extent to be an initialized\n\t * extent.  This is because we need to avoid the combination\n\t * of BH_Unwritten and BH_Mapped flags being simultaneously\n\t * set on the buffer_head.\n\t */\n\tclear_buffer_unwritten(bh);\n\n\t/*\n\t * New blocks allocate and/or writing to uninitialized extent\n\t * will possibly result in updating i_data, so we take\n\t * the write lock of i_data_sem, and call get_blocks()\n\t * with create == 1 flag.\n\t */\n\tdown_write((&EXT4_I(inode)->i_data_sem));\n\n\t/*\n\t * if the caller is from delayed allocation writeout path\n\t * we have already reserved fs blocks for allocation\n\t * let the underlying get_block() function know to\n\t * avoid double accounting\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tEXT4_I(inode)->i_delalloc_reserved_flag = 1;\n\t/*\n\t * We need to check for EXT4 here because migrate\n\t * could have changed the inode type in between\n\t */\n\tif (EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL) {\n\t\tretval =  ext4_ext_get_blocks(handle, inode, block, max_blocks,\n\t\t\t\t\t      bh, flags);\n\t} else {\n\t\tretval = ext4_ind_get_blocks(handle, inode, block,\n\t\t\t\t\t     max_blocks, bh, flags);\n\n\t\tif (retval > 0 && buffer_new(bh)) {\n\t\t\t/*\n\t\t\t * We allocated new blocks which will result in\n\t\t\t * i_data's format changing.  Force the migrate\n\t\t\t * to fail by clearing migrate flags\n\t\t\t */\n\t\t\text4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);\n\t\t}\n\n\t\t/*\n\t\t * Update reserved blocks/metadata blocks after successful\n\t\t * block allocation which had been deferred till now. We don't\n\t\t * support fallocate for non extent files. So we can update\n\t\t * reserve space here.\n\t\t */\n\t\tif ((retval > 0) &&\n\t\t\t(flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE))\n\t\t\text4_da_update_reserve_space(inode, retval, 1);\n\t}\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tEXT4_I(inode)->i_delalloc_reserved_flag = 0;\n\n\tup_write((&EXT4_I(inode)->i_data_sem));\n\tif (retval > 0 && buffer_mapped(bh)) {\n\t\tint ret = check_block_validity(inode, \"file system \"\n\t\t\t\t\t       \"corruption after allocation\",\n\t\t\t\t\t       block, bh->b_blocknr, retval);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\treturn retval;\n}\n\n/* Maximum number of blocks we map for direct IO at once. */\n#define DIO_MAX_BLOCKS 4096\n\nint ext4_get_block(struct inode *inode, sector_t iblock,\n\t\t   struct buffer_head *bh_result, int create)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tint ret = 0, started = 0;\n\tunsigned max_blocks = bh_result->b_size >> inode->i_blkbits;\n\tint dio_credits;\n\n\tif (create && !handle) {\n\t\t/* Direct IO write... */\n\t\tif (max_blocks > DIO_MAX_BLOCKS)\n\t\t\tmax_blocks = DIO_MAX_BLOCKS;\n\t\tdio_credits = ext4_chunk_trans_blocks(inode, max_blocks);\n\t\thandle = ext4_journal_start(inode, dio_credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tgoto out;\n\t\t}\n\t\tstarted = 1;\n\t}\n\n\tret = ext4_get_blocks(handle, inode, iblock, max_blocks, bh_result,\n\t\t\t      create ? EXT4_GET_BLOCKS_CREATE : 0);\n\tif (ret > 0) {\n\t\tbh_result->b_size = (ret << inode->i_blkbits);\n\t\tret = 0;\n\t}\n\tif (started)\n\t\text4_journal_stop(handle);\nout:\n\treturn ret;\n}\n\n/*\n * `handle' can be NULL if create is zero\n */\nstruct buffer_head *ext4_getblk(handle_t *handle, struct inode *inode,\n\t\t\t\text4_lblk_t block, int create, int *errp)\n{\n\tstruct buffer_head dummy;\n\tint fatal = 0, err;\n\tint flags = 0;\n\n\tJ_ASSERT(handle != NULL || create == 0);\n\n\tdummy.b_state = 0;\n\tdummy.b_blocknr = -1000;\n\tbuffer_trace_init(&dummy.b_history);\n\tif (create)\n\t\tflags |= EXT4_GET_BLOCKS_CREATE;\n\terr = ext4_get_blocks(handle, inode, block, 1, &dummy, flags);\n\t/*\n\t * ext4_get_blocks() returns number of blocks mapped. 0 in\n\t * case of a HOLE.\n\t */\n\tif (err > 0) {\n\t\tif (err > 1)\n\t\t\tWARN_ON(1);\n\t\terr = 0;\n\t}\n\t*errp = err;\n\tif (!err && buffer_mapped(&dummy)) {\n\t\tstruct buffer_head *bh;\n\t\tbh = sb_getblk(inode->i_sb, dummy.b_blocknr);\n\t\tif (!bh) {\n\t\t\t*errp = -EIO;\n\t\t\tgoto err;\n\t\t}\n\t\tif (buffer_new(&dummy)) {\n\t\t\tJ_ASSERT(create != 0);\n\t\t\tJ_ASSERT(handle != NULL);\n\n\t\t\t/*\n\t\t\t * Now that we do not always journal data, we should\n\t\t\t * keep in mind whether this should always journal the\n\t\t\t * new buffer as metadata.  For now, regular file\n\t\t\t * writes use ext4_get_block instead, so it's not a\n\t\t\t * problem.\n\t\t\t */\n\t\t\tlock_buffer(bh);\n\t\t\tBUFFER_TRACE(bh, \"call get_create_access\");\n\t\t\tfatal = ext4_journal_get_create_access(handle, bh);\n\t\t\tif (!fatal && !buffer_uptodate(bh)) {\n\t\t\t\tmemset(bh->b_data, 0, inode->i_sb->s_blocksize);\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t}\n\t\t\tunlock_buffer(bh);\n\t\t\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\t\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\t\tif (!fatal)\n\t\t\t\tfatal = err;\n\t\t} else {\n\t\t\tBUFFER_TRACE(bh, \"not a new buffer\");\n\t\t}\n\t\tif (fatal) {\n\t\t\t*errp = fatal;\n\t\t\tbrelse(bh);\n\t\t\tbh = NULL;\n\t\t}\n\t\treturn bh;\n\t}\nerr:\n\treturn NULL;\n}\n\nstruct buffer_head *ext4_bread(handle_t *handle, struct inode *inode,\n\t\t\t       ext4_lblk_t block, int create, int *err)\n{\n\tstruct buffer_head *bh;\n\n\tbh = ext4_getblk(handle, inode, block, create, err);\n\tif (!bh)\n\t\treturn bh;\n\tif (buffer_uptodate(bh))\n\t\treturn bh;\n\tll_rw_block(READ_META, 1, &bh);\n\twait_on_buffer(bh);\n\tif (buffer_uptodate(bh))\n\t\treturn bh;\n\tput_bh(bh);\n\t*err = -EIO;\n\treturn NULL;\n}\n\nstatic int walk_page_buffers(handle_t *handle,\n\t\t\t     struct buffer_head *head,\n\t\t\t     unsigned from,\n\t\t\t     unsigned to,\n\t\t\t     int *partial,\n\t\t\t     int (*fn)(handle_t *handle,\n\t\t\t\t       struct buffer_head *bh))\n{\n\tstruct buffer_head *bh;\n\tunsigned block_start, block_end;\n\tunsigned blocksize = head->b_size;\n\tint err, ret = 0;\n\tstruct buffer_head *next;\n\n\tfor (bh = head, block_start = 0;\n\t     ret == 0 && (bh != head || !block_start);\n\t     block_start = block_end, bh = next) {\n\t\tnext = bh->b_this_page;\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end <= from || block_start >= to) {\n\t\t\tif (partial && !buffer_uptodate(bh))\n\t\t\t\t*partial = 1;\n\t\t\tcontinue;\n\t\t}\n\t\terr = (*fn)(handle, bh);\n\t\tif (!ret)\n\t\t\tret = err;\n\t}\n\treturn ret;\n}\n\n/*\n * To preserve ordering, it is essential that the hole instantiation and\n * the data write be encapsulated in a single transaction.  We cannot\n * close off a transaction and start a new one between the ext4_get_block()\n * and the commit_write().  So doing the jbd2_journal_start at the start of\n * prepare_write() is the right place.\n *\n * Also, this function can nest inside ext4_writepage() ->\n * block_write_full_page(). In that case, we *know* that ext4_writepage()\n * has generated enough buffer credits to do the whole page.  So we won't\n * block on the journal in that case, which is good, because the caller may\n * be PF_MEMALLOC.\n *\n * By accident, ext4 can be reentered when a transaction is open via\n * quota file writes.  If we were to commit the transaction while thus\n * reentered, there can be a deadlock - we would be holding a quota\n * lock, and the commit would never complete if another thread had a\n * transaction open and was blocking on the quota lock - a ranking\n * violation.\n *\n * So what we do is to rely on the fact that jbd2_journal_stop/journal_start\n * will _not_ run commit under these circumstances because handle->h_ref\n * is elevated.  We'll still have enough credits for the tiny quotafile\n * write.\n */\nstatic int do_journal_get_write_access(handle_t *handle,\n\t\t\t\t       struct buffer_head *bh)\n{\n\tif (!buffer_mapped(bh) || buffer_freed(bh))\n\t\treturn 0;\n\treturn ext4_journal_get_write_access(handle, bh);\n}\n\n/*\n * Truncate blocks that were not used by write. We have to truncate the\n * pagecache as well so that corresponding buffers get properly unmapped.\n */\nstatic void ext4_truncate_failed_write(struct inode *inode)\n{\n\ttruncate_inode_pages(inode->i_mapping, inode->i_size);\n\text4_truncate(inode);\n}\n\nstatic int ext4_get_block_write(struct inode *inode, sector_t iblock,\n\t\t   struct buffer_head *bh_result, int create);\nstatic int ext4_write_begin(struct file *file, struct address_space *mapping,\n\t\t\t    loff_t pos, unsigned len, unsigned flags,\n\t\t\t    struct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tint ret, needed_blocks;\n\thandle_t *handle;\n\tint retries = 0;\n\tstruct page *page;\n\tpgoff_t index;\n\tunsigned from, to;\n\n\ttrace_ext4_write_begin(inode, pos, len, flags);\n\t/*\n\t * Reserve one block more for addition to orphan list in case\n\t * we allocate blocks but write fails for some reason\n\t */\n\tneeded_blocks = ext4_writepage_trans_blocks(inode) + 1;\n\tindex = pos >> PAGE_CACHE_SHIFT;\n\tfrom = pos & (PAGE_CACHE_SIZE - 1);\n\tto = from + len;\n\nretry:\n\thandle = ext4_journal_start(inode, needed_blocks);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out;\n\t}\n\n\t/* We cannot recurse into the filesystem as the transaction is already\n\t * started */\n\tflags |= AOP_FLAG_NOFS;\n\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page) {\n\t\text4_journal_stop(handle);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\t*pagep = page;\n\n\tif (ext4_should_dioread_nolock(inode))\n\t\tret = block_write_begin(file, mapping, pos, len, flags, pagep,\n\t\t\t\tfsdata, ext4_get_block_write);\n\telse\n\t\tret = block_write_begin(file, mapping, pos, len, flags, pagep,\n\t\t\t\tfsdata, ext4_get_block);\n\n\tif (!ret && ext4_should_journal_data(inode)) {\n\t\tret = walk_page_buffers(handle, page_buffers(page),\n\t\t\t\tfrom, to, NULL, do_journal_get_write_access);\n\t}\n\n\tif (ret) {\n\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\t/*\n\t\t * block_write_begin may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again. Don't need\n\t\t * i_size_read because we hold i_mutex.\n\t\t *\n\t\t * Add inode to orphan list in case we crash before\n\t\t * truncate finishes\n\t\t */\n\t\tif (pos + len > inode->i_size && ext4_can_truncate(inode))\n\t\t\text4_orphan_add(handle, inode);\n\n\t\text4_journal_stop(handle);\n\t\tif (pos + len > inode->i_size) {\n\t\t\text4_truncate_failed_write(inode);\n\t\t\t/*\n\t\t\t * If truncate failed early the inode might\n\t\t\t * still be on the orphan list; we need to\n\t\t\t * make sure the inode is removed from the\n\t\t\t * orphan list in that case.\n\t\t\t */\n\t\t\tif (inode->i_nlink)\n\t\t\t\text4_orphan_del(NULL, inode);\n\t\t}\n\t}\n\n\tif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry;\nout:\n\treturn ret;\n}\n\n/* For write_end() in data=journal mode */\nstatic int write_end_fn(handle_t *handle, struct buffer_head *bh)\n{\n\tif (!buffer_mapped(bh) || buffer_freed(bh))\n\t\treturn 0;\n\tset_buffer_uptodate(bh);\n\treturn ext4_handle_dirty_metadata(handle, NULL, bh);\n}\n\nstatic int ext4_generic_write_end(struct file *file,\n\t\t\t\t  struct address_space *mapping,\n\t\t\t\t  loff_t pos, unsigned len, unsigned copied,\n\t\t\t\t  struct page *page, void *fsdata)\n{\n\tint i_size_changed = 0;\n\tstruct inode *inode = mapping->host;\n\thandle_t *handle = ext4_journal_current_handle();\n\n\tcopied = block_write_end(file, mapping, pos, len, copied, page, fsdata);\n\n\t/*\n\t * No need to use i_size_read() here, the i_size\n\t * cannot change under us because we hold i_mutex.\n\t *\n\t * But it's important to update i_size while still holding page lock:\n\t * page writeout could otherwise come in and zero beyond i_size.\n\t */\n\tif (pos + copied > inode->i_size) {\n\t\ti_size_write(inode, pos + copied);\n\t\ti_size_changed = 1;\n\t}\n\n\tif (pos + copied >  EXT4_I(inode)->i_disksize) {\n\t\t/* We need to mark inode dirty even if\n\t\t * new_i_size is less that inode->i_size\n\t\t * bu greater than i_disksize.(hint delalloc)\n\t\t */\n\t\text4_update_i_disksize(inode, (pos + copied));\n\t\ti_size_changed = 1;\n\t}\n\tunlock_page(page);\n\tpage_cache_release(page);\n\n\t/*\n\t * Don't mark the inode dirty under page lock. First, it unnecessarily\n\t * makes the holding time of page lock longer. Second, it forces lock\n\t * ordering of page lock and transaction start for journaling\n\t * filesystems.\n\t */\n\tif (i_size_changed)\n\t\text4_mark_inode_dirty(handle, inode);\n\n\treturn copied;\n}\n\n/*\n * We need to pick up the new inode size which generic_commit_write gave us\n * `file' can be NULL - eg, when called from page_symlink().\n *\n * ext4 never places buffers on inode->i_mapping->private_list.  metadata\n * buffers are managed internally.\n */\nstatic int ext4_ordered_write_end(struct file *file,\n\t\t\t\t  struct address_space *mapping,\n\t\t\t\t  loff_t pos, unsigned len, unsigned copied,\n\t\t\t\t  struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tint ret = 0, ret2;\n\n\ttrace_ext4_ordered_write_end(inode, pos, len, copied);\n\tret = ext4_jbd2_file_inode(handle, inode);\n\n\tif (ret == 0) {\n\t\tret2 = ext4_generic_write_end(file, mapping, pos, len, copied,\n\t\t\t\t\t\t\tpage, fsdata);\n\t\tcopied = ret2;\n\t\tif (pos + len > inode->i_size && ext4_can_truncate(inode))\n\t\t\t/* if we have allocated more blocks and copied\n\t\t\t * less. We will have blocks allocated outside\n\t\t\t * inode->i_size. So truncate them\n\t\t\t */\n\t\t\text4_orphan_add(handle, inode);\n\t\tif (ret2 < 0)\n\t\t\tret = ret2;\n\t}\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\tif (pos + len > inode->i_size) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\n\treturn ret ? ret : copied;\n}\n\nstatic int ext4_writeback_write_end(struct file *file,\n\t\t\t\t    struct address_space *mapping,\n\t\t\t\t    loff_t pos, unsigned len, unsigned copied,\n\t\t\t\t    struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tint ret = 0, ret2;\n\n\ttrace_ext4_writeback_write_end(inode, pos, len, copied);\n\tret2 = ext4_generic_write_end(file, mapping, pos, len, copied,\n\t\t\t\t\t\t\tpage, fsdata);\n\tcopied = ret2;\n\tif (pos + len > inode->i_size && ext4_can_truncate(inode))\n\t\t/* if we have allocated more blocks and copied\n\t\t * less. We will have blocks allocated outside\n\t\t * inode->i_size. So truncate them\n\t\t */\n\t\text4_orphan_add(handle, inode);\n\n\tif (ret2 < 0)\n\t\tret = ret2;\n\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\tif (pos + len > inode->i_size) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\treturn ret ? ret : copied;\n}\n\nstatic int ext4_journalled_write_end(struct file *file,\n\t\t\t\t     struct address_space *mapping,\n\t\t\t\t     loff_t pos, unsigned len, unsigned copied,\n\t\t\t\t     struct page *page, void *fsdata)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tstruct inode *inode = mapping->host;\n\tint ret = 0, ret2;\n\tint partial = 0;\n\tunsigned from, to;\n\tloff_t new_i_size;\n\n\ttrace_ext4_journalled_write_end(inode, pos, len, copied);\n\tfrom = pos & (PAGE_CACHE_SIZE - 1);\n\tto = from + len;\n\n\tif (copied < len) {\n\t\tif (!PageUptodate(page))\n\t\t\tcopied = 0;\n\t\tpage_zero_new_buffers(page, from+copied, to);\n\t}\n\n\tret = walk_page_buffers(handle, page_buffers(page), from,\n\t\t\t\tto, &partial, write_end_fn);\n\tif (!partial)\n\t\tSetPageUptodate(page);\n\tnew_i_size = pos + copied;\n\tif (new_i_size > inode->i_size)\n\t\ti_size_write(inode, pos+copied);\n\text4_set_inode_state(inode, EXT4_STATE_JDATA);\n\tif (new_i_size > EXT4_I(inode)->i_disksize) {\n\t\text4_update_i_disksize(inode, new_i_size);\n\t\tret2 = ext4_mark_inode_dirty(handle, inode);\n\t\tif (!ret)\n\t\t\tret = ret2;\n\t}\n\n\tunlock_page(page);\n\tpage_cache_release(page);\n\tif (pos + len > inode->i_size && ext4_can_truncate(inode))\n\t\t/* if we have allocated more blocks and copied\n\t\t * less. We will have blocks allocated outside\n\t\t * inode->i_size. So truncate them\n\t\t */\n\t\text4_orphan_add(handle, inode);\n\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\tif (pos + len > inode->i_size) {\n\t\text4_truncate_failed_write(inode);\n\t\t/*\n\t\t * If truncate failed early the inode might still be\n\t\t * on the orphan list; we need to make sure the inode\n\t\t * is removed from the orphan list in that case.\n\t\t */\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(NULL, inode);\n\t}\n\n\treturn ret ? ret : copied;\n}\n\n/*\n * Reserve a single block located at lblock\n */\nstatic int ext4_da_reserve_space(struct inode *inode, sector_t lblock)\n{\n\tint retries = 0;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tunsigned long md_needed, md_reserved;\n\n\t/*\n\t * recalculate the amount of metadata blocks to reserve\n\t * in order to allocate nrblocks\n\t * worse case is one extent per block\n\t */\nrepeat:\n\tspin_lock(&ei->i_block_reservation_lock);\n\tmd_reserved = ei->i_reserved_meta_blocks;\n\tmd_needed = ext4_calc_metadata_amount(inode, lblock);\n\ttrace_ext4_da_reserve_space(inode, md_needed);\n\tspin_unlock(&ei->i_block_reservation_lock);\n\n\t/*\n\t * Make quota reservation here to prevent quota overflow\n\t * later. Real quota accounting is done at pages writeout\n\t * time.\n\t */\n\tif (vfs_dq_reserve_block(inode, md_needed + 1))\n\t\treturn -EDQUOT;\n\n\tif (ext4_claim_free_blocks(sbi, md_needed + 1)) {\n\t\tvfs_dq_release_reservation_block(inode, md_needed + 1);\n\t\tif (ext4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\t\tyield();\n\t\t\tgoto repeat;\n\t\t}\n\t\treturn -ENOSPC;\n\t}\n\tspin_lock(&ei->i_block_reservation_lock);\n\tei->i_reserved_data_blocks++;\n\tei->i_reserved_meta_blocks += md_needed;\n\tspin_unlock(&ei->i_block_reservation_lock);\n\n\treturn 0;       /* success */\n}\n\nstatic void ext4_da_release_space(struct inode *inode, int to_free)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\n\tif (!to_free)\n\t\treturn;\t\t/* Nothing to release, exit */\n\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\tif (unlikely(to_free > ei->i_reserved_data_blocks)) {\n\t\t/*\n\t\t * if there aren't enough reserved blocks, then the\n\t\t * counter is messed up somewhere.  Since this\n\t\t * function is called from invalidate page, it's\n\t\t * harmless to return without any action.\n\t\t */\n\t\text4_msg(inode->i_sb, KERN_NOTICE, \"ext4_da_release_space: \"\n\t\t\t \"ino %lu, to_free %d with only %d reserved \"\n\t\t\t \"data blocks\\n\", inode->i_ino, to_free,\n\t\t\t ei->i_reserved_data_blocks);\n\t\tWARN_ON(1);\n\t\tto_free = ei->i_reserved_data_blocks;\n\t}\n\tei->i_reserved_data_blocks -= to_free;\n\n\tif (ei->i_reserved_data_blocks == 0) {\n\t\t/*\n\t\t * We can release all of the reserved metadata blocks\n\t\t * only when we have written all of the delayed\n\t\t * allocation blocks.\n\t\t */\n\t\tto_free += ei->i_reserved_meta_blocks;\n\t\tei->i_reserved_meta_blocks = 0;\n\t\tei->i_da_metadata_calc_len = 0;\n\t}\n\n\t/* update fs dirty blocks counter */\n\tpercpu_counter_sub(&sbi->s_dirtyblocks_counter, to_free);\n\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\tvfs_dq_release_reservation_block(inode, to_free);\n}\n\nstatic void ext4_da_page_release_reservation(struct page *page,\n\t\t\t\t\t     unsigned long offset)\n{\n\tint to_release = 0;\n\tstruct buffer_head *head, *bh;\n\tunsigned int curr_off = 0;\n\n\thead = page_buffers(page);\n\tbh = head;\n\tdo {\n\t\tunsigned int next_off = curr_off + bh->b_size;\n\n\t\tif ((offset <= curr_off) && (buffer_delay(bh))) {\n\t\t\tto_release++;\n\t\t\tclear_buffer_delay(bh);\n\t\t}\n\t\tcurr_off = next_off;\n\t} while ((bh = bh->b_this_page) != head);\n\text4_da_release_space(page->mapping->host, to_release);\n}\n\n/*\n * Delayed allocation stuff\n */\n\n/*\n * mpage_da_submit_io - walks through extent of pages and try to write\n * them with writepage() call back\n *\n * @mpd->inode: inode\n * @mpd->first_page: first page of the extent\n * @mpd->next_page: page after the last page of the extent\n *\n * By the time mpage_da_submit_io() is called we expect all blocks\n * to be allocated. this may be wrong if allocation failed.\n *\n * As pages are already locked by write_cache_pages(), we can't use it\n */\nstatic int mpage_da_submit_io(struct mpage_da_data *mpd)\n{\n\tlong pages_skipped;\n\tstruct pagevec pvec;\n\tunsigned long index, end;\n\tint ret = 0, err, nr_pages, i;\n\tstruct inode *inode = mpd->inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\tBUG_ON(mpd->next_page <= mpd->first_page);\n\t/*\n\t * We need to start from the first_page to the next_page - 1\n\t * to make sure we also write the mapped dirty buffer_heads.\n\t * If we look at mpd->b_blocknr we would only be looking\n\t * at the currently mapped buffer_heads.\n\t */\n\tindex = mpd->first_page;\n\tend = mpd->next_page - 1;\n\n\tpagevec_init(&pvec, 0);\n\twhile (index <= end) {\n\t\tnr_pages = pagevec_lookup(&pvec, mapping, index, PAGEVEC_SIZE);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tindex = page->index;\n\t\t\tif (index > end)\n\t\t\t\tbreak;\n\t\t\tindex++;\n\n\t\t\tBUG_ON(!PageLocked(page));\n\t\t\tBUG_ON(PageWriteback(page));\n\n\t\t\tpages_skipped = mpd->wbc->pages_skipped;\n\t\t\terr = mapping->a_ops->writepage(page, mpd->wbc);\n\t\t\tif (!err && (pages_skipped == mpd->wbc->pages_skipped))\n\t\t\t\t/*\n\t\t\t\t * have successfully written the page\n\t\t\t\t * without skipping the same\n\t\t\t\t */\n\t\t\t\tmpd->pages_written++;\n\t\t\t/*\n\t\t\t * In error case, we have to continue because\n\t\t\t * remaining pages are still locked\n\t\t\t * XXX: unlock and re-dirty them?\n\t\t\t */\n\t\t\tif (ret == 0)\n\t\t\t\tret = err;\n\t\t}\n\t\tpagevec_release(&pvec);\n\t}\n\treturn ret;\n}\n\n/*\n * mpage_put_bnr_to_bhs - walk blocks and assign them actual numbers\n *\n * @mpd->inode - inode to walk through\n * @exbh->b_blocknr - first block on a disk\n * @exbh->b_size - amount of space in bytes\n * @logical - first logical block to start assignment with\n *\n * the function goes through all passed space and put actual disk\n * block numbers into buffer heads, dropping BH_Delay and BH_Unwritten\n */\nstatic void mpage_put_bnr_to_bhs(struct mpage_da_data *mpd, sector_t logical,\n\t\t\t\t struct buffer_head *exbh)\n{\n\tstruct inode *inode = mpd->inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\tint blocks = exbh->b_size >> inode->i_blkbits;\n\tsector_t pblock = exbh->b_blocknr, cur_logical;\n\tstruct buffer_head *head, *bh;\n\tpgoff_t index, end;\n\tstruct pagevec pvec;\n\tint nr_pages, i;\n\n\tindex = logical >> (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\tend = (logical + blocks - 1) >> (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\tcur_logical = index << (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\n\tpagevec_init(&pvec, 0);\n\n\twhile (index <= end) {\n\t\t/* XXX: optimize tail */\n\t\tnr_pages = pagevec_lookup(&pvec, mapping, index, PAGEVEC_SIZE);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tindex = page->index;\n\t\t\tif (index > end)\n\t\t\t\tbreak;\n\t\t\tindex++;\n\n\t\t\tBUG_ON(!PageLocked(page));\n\t\t\tBUG_ON(PageWriteback(page));\n\t\t\tBUG_ON(!page_has_buffers(page));\n\n\t\t\tbh = page_buffers(page);\n\t\t\thead = bh;\n\n\t\t\t/* skip blocks out of the range */\n\t\t\tdo {\n\t\t\t\tif (cur_logical >= logical)\n\t\t\t\t\tbreak;\n\t\t\t\tcur_logical++;\n\t\t\t} while ((bh = bh->b_this_page) != head);\n\n\t\t\tdo {\n\t\t\t\tif (cur_logical >= logical + blocks)\n\t\t\t\t\tbreak;\n\n\t\t\t\tif (buffer_delay(bh) ||\n\t\t\t\t\t\tbuffer_unwritten(bh)) {\n\n\t\t\t\t\tBUG_ON(bh->b_bdev != inode->i_sb->s_bdev);\n\n\t\t\t\t\tif (buffer_delay(bh)) {\n\t\t\t\t\t\tclear_buffer_delay(bh);\n\t\t\t\t\t\tbh->b_blocknr = pblock;\n\t\t\t\t\t} else {\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * unwritten already should have\n\t\t\t\t\t\t * blocknr assigned. Verify that\n\t\t\t\t\t\t */\n\t\t\t\t\t\tclear_buffer_unwritten(bh);\n\t\t\t\t\t\tBUG_ON(bh->b_blocknr != pblock);\n\t\t\t\t\t}\n\n\t\t\t\t} else if (buffer_mapped(bh))\n\t\t\t\t\tBUG_ON(bh->b_blocknr != pblock);\n\n\t\t\t\tif (buffer_uninit(exbh))\n\t\t\t\t\tset_buffer_uninit(bh);\n\t\t\t\tcur_logical++;\n\t\t\t\tpblock++;\n\t\t\t} while ((bh = bh->b_this_page) != head);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t}\n}\n\n\n/*\n * __unmap_underlying_blocks - just a helper function to unmap\n * set of blocks described by @bh\n */\nstatic inline void __unmap_underlying_blocks(struct inode *inode,\n\t\t\t\t\t     struct buffer_head *bh)\n{\n\tstruct block_device *bdev = inode->i_sb->s_bdev;\n\tint blocks, i;\n\n\tblocks = bh->b_size >> inode->i_blkbits;\n\tfor (i = 0; i < blocks; i++)\n\t\tunmap_underlying_metadata(bdev, bh->b_blocknr + i);\n}\n\nstatic void ext4_da_block_invalidatepages(struct mpage_da_data *mpd,\n\t\t\t\t\tsector_t logical, long blk_cnt)\n{\n\tint nr_pages, i;\n\tpgoff_t index, end;\n\tstruct pagevec pvec;\n\tstruct inode *inode = mpd->inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\tindex = logical >> (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\tend   = (logical + blk_cnt - 1) >>\n\t\t\t\t(PAGE_CACHE_SHIFT - inode->i_blkbits);\n\twhile (index <= end) {\n\t\tnr_pages = pagevec_lookup(&pvec, mapping, index, PAGEVEC_SIZE);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\t\t\tindex = page->index;\n\t\t\tif (index > end)\n\t\t\t\tbreak;\n\t\t\tindex++;\n\n\t\t\tBUG_ON(!PageLocked(page));\n\t\t\tBUG_ON(PageWriteback(page));\n\t\t\tblock_invalidatepage(page, 0);\n\t\t\tClearPageUptodate(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\n\treturn;\n}\n\nstatic void ext4_print_free_blocks(struct inode *inode)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tprintk(KERN_CRIT \"Total free blocks count %lld\\n\",\n\t       ext4_count_free_blocks(inode->i_sb));\n\tprintk(KERN_CRIT \"Free/Dirty block details\\n\");\n\tprintk(KERN_CRIT \"free_blocks=%lld\\n\",\n\t       (long long) percpu_counter_sum(&sbi->s_freeblocks_counter));\n\tprintk(KERN_CRIT \"dirty_blocks=%lld\\n\",\n\t       (long long) percpu_counter_sum(&sbi->s_dirtyblocks_counter));\n\tprintk(KERN_CRIT \"Block reservation details\\n\");\n\tprintk(KERN_CRIT \"i_reserved_data_blocks=%u\\n\",\n\t       EXT4_I(inode)->i_reserved_data_blocks);\n\tprintk(KERN_CRIT \"i_reserved_meta_blocks=%u\\n\",\n\t       EXT4_I(inode)->i_reserved_meta_blocks);\n\treturn;\n}\n\n/*\n * mpage_da_map_blocks - go through given space\n *\n * @mpd - bh describing space\n *\n * The function skips space we know is already mapped to disk blocks.\n *\n */\nstatic int mpage_da_map_blocks(struct mpage_da_data *mpd)\n{\n\tint err, blks, get_blocks_flags;\n\tstruct buffer_head new;\n\tsector_t next = mpd->b_blocknr;\n\tunsigned max_blocks = mpd->b_size >> mpd->inode->i_blkbits;\n\tloff_t disksize = EXT4_I(mpd->inode)->i_disksize;\n\thandle_t *handle = NULL;\n\n\t/*\n\t * We consider only non-mapped and non-allocated blocks\n\t */\n\tif ((mpd->b_state  & (1 << BH_Mapped)) &&\n\t\t!(mpd->b_state & (1 << BH_Delay)) &&\n\t\t!(mpd->b_state & (1 << BH_Unwritten)))\n\t\treturn 0;\n\n\t/*\n\t * If we didn't accumulate anything to write simply return\n\t */\n\tif (!mpd->b_size)\n\t\treturn 0;\n\n\thandle = ext4_journal_current_handle();\n\tBUG_ON(!handle);\n\n\t/*\n\t * Call ext4_get_blocks() to allocate any delayed allocation\n\t * blocks, or to convert an uninitialized extent to be\n\t * initialized (in the case where we have written into\n\t * one or more preallocated blocks).\n\t *\n\t * We pass in the magic EXT4_GET_BLOCKS_DELALLOC_RESERVE to\n\t * indicate that we are on the delayed allocation path.  This\n\t * affects functions in many different parts of the allocation\n\t * call path.  This flag exists primarily because we don't\n\t * want to change *many* call functions, so ext4_get_blocks()\n\t * will set the magic i_delalloc_reserved_flag once the\n\t * inode's allocation semaphore is taken.\n\t *\n\t * If the blocks in questions were delalloc blocks, set\n\t * EXT4_GET_BLOCKS_DELALLOC_RESERVE so the delalloc accounting\n\t * variables are updated after the blocks have been allocated.\n\t */\n\tnew.b_state = 0;\n\tget_blocks_flags = EXT4_GET_BLOCKS_CREATE;\n\tif (ext4_should_dioread_nolock(mpd->inode))\n\t\tget_blocks_flags |= EXT4_GET_BLOCKS_IO_CREATE_EXT;\n\tif (mpd->b_state & (1 << BH_Delay))\n\t\tget_blocks_flags |= EXT4_GET_BLOCKS_DELALLOC_RESERVE;\n\n\tblks = ext4_get_blocks(handle, mpd->inode, next, max_blocks,\n\t\t\t       &new, get_blocks_flags);\n\tif (blks < 0) {\n\t\terr = blks;\n\t\t/*\n\t\t * If get block returns with error we simply\n\t\t * return. Later writepage will redirty the page and\n\t\t * writepages will find the dirty page again\n\t\t */\n\t\tif (err == -EAGAIN)\n\t\t\treturn 0;\n\n\t\tif (err == -ENOSPC &&\n\t\t    ext4_count_free_blocks(mpd->inode->i_sb)) {\n\t\t\tmpd->retval = err;\n\t\t\treturn 0;\n\t\t}\n\n\t\t/*\n\t\t * get block failure will cause us to loop in\n\t\t * writepages, because a_ops->writepage won't be able\n\t\t * to make progress. The page will be redirtied by\n\t\t * writepage and writepages will again try to write\n\t\t * the same.\n\t\t */\n\t\text4_msg(mpd->inode->i_sb, KERN_CRIT,\n\t\t\t \"delayed block allocation failed for inode %lu at \"\n\t\t\t \"logical offset %llu with max blocks %zd with \"\n\t\t\t \"error %d\\n\", mpd->inode->i_ino,\n\t\t\t (unsigned long long) next,\n\t\t\t mpd->b_size >> mpd->inode->i_blkbits, err);\n\t\tprintk(KERN_CRIT \"This should not happen!!  \"\n\t\t       \"Data will be lost\\n\");\n\t\tif (err == -ENOSPC) {\n\t\t\text4_print_free_blocks(mpd->inode);\n\t\t}\n\t\t/* invalidate all the pages */\n\t\text4_da_block_invalidatepages(mpd, next,\n\t\t\t\tmpd->b_size >> mpd->inode->i_blkbits);\n\t\treturn err;\n\t}\n\tBUG_ON(blks == 0);\n\n\tnew.b_size = (blks << mpd->inode->i_blkbits);\n\n\tif (buffer_new(&new))\n\t\t__unmap_underlying_blocks(mpd->inode, &new);\n\n\t/*\n\t * If blocks are delayed marked, we need to\n\t * put actual blocknr and drop delayed bit\n\t */\n\tif ((mpd->b_state & (1 << BH_Delay)) ||\n\t    (mpd->b_state & (1 << BH_Unwritten)))\n\t\tmpage_put_bnr_to_bhs(mpd, next, &new);\n\n\tif (ext4_should_order_data(mpd->inode)) {\n\t\terr = ext4_jbd2_file_inode(handle, mpd->inode);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/*\n\t * Update on-disk size along with block allocation.\n\t */\n\tdisksize = ((loff_t) next + blks) << mpd->inode->i_blkbits;\n\tif (disksize > i_size_read(mpd->inode))\n\t\tdisksize = i_size_read(mpd->inode);\n\tif (disksize > EXT4_I(mpd->inode)->i_disksize) {\n\t\text4_update_i_disksize(mpd->inode, disksize);\n\t\treturn ext4_mark_inode_dirty(handle, mpd->inode);\n\t}\n\n\treturn 0;\n}\n\n#define BH_FLAGS ((1 << BH_Uptodate) | (1 << BH_Mapped) | \\\n\t\t(1 << BH_Delay) | (1 << BH_Unwritten))\n\n/*\n * mpage_add_bh_to_extent - try to add one more block to extent of blocks\n *\n * @mpd->lbh - extent of blocks\n * @logical - logical number of the block in the file\n * @bh - bh of the block (used to access block's state)\n *\n * the function is used to collect contig. blocks in same state\n */\nstatic void mpage_add_bh_to_extent(struct mpage_da_data *mpd,\n\t\t\t\t   sector_t logical, size_t b_size,\n\t\t\t\t   unsigned long b_state)\n{\n\tsector_t next;\n\tint nrblocks = mpd->b_size >> mpd->inode->i_blkbits;\n\n\t/* check if thereserved journal credits might overflow */\n\tif (!(EXT4_I(mpd->inode)->i_flags & EXT4_EXTENTS_FL)) {\n\t\tif (nrblocks >= EXT4_MAX_TRANS_DATA) {\n\t\t\t/*\n\t\t\t * With non-extent format we are limited by the journal\n\t\t\t * credit available.  Total credit needed to insert\n\t\t\t * nrblocks contiguous blocks is dependent on the\n\t\t\t * nrblocks.  So limit nrblocks.\n\t\t\t */\n\t\t\tgoto flush_it;\n\t\t} else if ((nrblocks + (b_size >> mpd->inode->i_blkbits)) >\n\t\t\t\tEXT4_MAX_TRANS_DATA) {\n\t\t\t/*\n\t\t\t * Adding the new buffer_head would make it cross the\n\t\t\t * allowed limit for which we have journal credit\n\t\t\t * reserved. So limit the new bh->b_size\n\t\t\t */\n\t\t\tb_size = (EXT4_MAX_TRANS_DATA - nrblocks) <<\n\t\t\t\t\t\tmpd->inode->i_blkbits;\n\t\t\t/* we will do mpage_da_submit_io in the next loop */\n\t\t}\n\t}\n\t/*\n\t * First block in the extent\n\t */\n\tif (mpd->b_size == 0) {\n\t\tmpd->b_blocknr = logical;\n\t\tmpd->b_size = b_size;\n\t\tmpd->b_state = b_state & BH_FLAGS;\n\t\treturn;\n\t}\n\n\tnext = mpd->b_blocknr + nrblocks;\n\t/*\n\t * Can we merge the block to our big extent?\n\t */\n\tif (logical == next && (b_state & BH_FLAGS) == mpd->b_state) {\n\t\tmpd->b_size += b_size;\n\t\treturn;\n\t}\n\nflush_it:\n\t/*\n\t * We couldn't merge the block to our extent, so we\n\t * need to flush current  extent and start new one\n\t */\n\tif (mpage_da_map_blocks(mpd) == 0)\n\t\tmpage_da_submit_io(mpd);\n\tmpd->io_done = 1;\n\treturn;\n}\n\nstatic int ext4_bh_delay_or_unwritten(handle_t *handle, struct buffer_head *bh)\n{\n\treturn (buffer_delay(bh) || buffer_unwritten(bh)) && buffer_dirty(bh);\n}\n\n/*\n * __mpage_da_writepage - finds extent of pages and blocks\n *\n * @page: page to consider\n * @wbc: not used, we just follow rules\n * @data: context\n *\n * The function finds extents of pages and scan them for all blocks.\n */\nstatic int __mpage_da_writepage(struct page *page,\n\t\t\t\tstruct writeback_control *wbc, void *data)\n{\n\tstruct mpage_da_data *mpd = data;\n\tstruct inode *inode = mpd->inode;\n\tstruct buffer_head *bh, *head;\n\tsector_t logical;\n\n\tif (mpd->io_done) {\n\t\t/*\n\t\t * Rest of the page in the page_vec\n\t\t * redirty then and skip then. We will\n\t\t * try to write them again after\n\t\t * starting a new transaction\n\t\t */\n\t\tredirty_page_for_writepage(wbc, page);\n\t\tunlock_page(page);\n\t\treturn MPAGE_DA_EXTENT_TAIL;\n\t}\n\t/*\n\t * Can we merge this page to current extent?\n\t */\n\tif (mpd->next_page != page->index) {\n\t\t/*\n\t\t * Nope, we can't. So, we map non-allocated blocks\n\t\t * and start IO on them using writepage()\n\t\t */\n\t\tif (mpd->next_page != mpd->first_page) {\n\t\t\tif (mpage_da_map_blocks(mpd) == 0)\n\t\t\t\tmpage_da_submit_io(mpd);\n\t\t\t/*\n\t\t\t * skip rest of the page in the page_vec\n\t\t\t */\n\t\t\tmpd->io_done = 1;\n\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\tunlock_page(page);\n\t\t\treturn MPAGE_DA_EXTENT_TAIL;\n\t\t}\n\n\t\t/*\n\t\t * Start next extent of pages ...\n\t\t */\n\t\tmpd->first_page = page->index;\n\n\t\t/*\n\t\t * ... and blocks\n\t\t */\n\t\tmpd->b_size = 0;\n\t\tmpd->b_state = 0;\n\t\tmpd->b_blocknr = 0;\n\t}\n\n\tmpd->next_page = page->index + 1;\n\tlogical = (sector_t) page->index <<\n\t\t  (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\n\tif (!page_has_buffers(page)) {\n\t\tmpage_add_bh_to_extent(mpd, logical, PAGE_CACHE_SIZE,\n\t\t\t\t       (1 << BH_Dirty) | (1 << BH_Uptodate));\n\t\tif (mpd->io_done)\n\t\t\treturn MPAGE_DA_EXTENT_TAIL;\n\t} else {\n\t\t/*\n\t\t * Page with regular buffer heads, just add all dirty ones\n\t\t */\n\t\thead = page_buffers(page);\n\t\tbh = head;\n\t\tdo {\n\t\t\tBUG_ON(buffer_locked(bh));\n\t\t\t/*\n\t\t\t * We need to try to allocate\n\t\t\t * unmapped blocks in the same page.\n\t\t\t * Otherwise we won't make progress\n\t\t\t * with the page in ext4_writepage\n\t\t\t */\n\t\t\tif (ext4_bh_delay_or_unwritten(NULL, bh)) {\n\t\t\t\tmpage_add_bh_to_extent(mpd, logical,\n\t\t\t\t\t\t       bh->b_size,\n\t\t\t\t\t\t       bh->b_state);\n\t\t\t\tif (mpd->io_done)\n\t\t\t\t\treturn MPAGE_DA_EXTENT_TAIL;\n\t\t\t} else if (buffer_dirty(bh) && (buffer_mapped(bh))) {\n\t\t\t\t/*\n\t\t\t\t * mapped dirty buffer. We need to update\n\t\t\t\t * the b_state because we look at\n\t\t\t\t * b_state in mpage_da_map_blocks. We don't\n\t\t\t\t * update b_size because if we find an\n\t\t\t\t * unmapped buffer_head later we need to\n\t\t\t\t * use the b_state flag of that buffer_head.\n\t\t\t\t */\n\t\t\t\tif (mpd->b_size == 0)\n\t\t\t\t\tmpd->b_state = bh->b_state & BH_FLAGS;\n\t\t\t}\n\t\t\tlogical++;\n\t\t} while ((bh = bh->b_this_page) != head);\n\t}\n\n\treturn 0;\n}\n\n/*\n * This is a special get_blocks_t callback which is used by\n * ext4_da_write_begin().  It will either return mapped block or\n * reserve space for a single block.\n *\n * For delayed buffer_head we have BH_Mapped, BH_New, BH_Delay set.\n * We also have b_blocknr = -1 and b_bdev initialized properly\n *\n * For unwritten buffer_head we have BH_Mapped, BH_New, BH_Unwritten set.\n * We also have b_blocknr = physicalblock mapping unwritten extent and b_bdev\n * initialized properly.\n */\nstatic int ext4_da_get_block_prep(struct inode *inode, sector_t iblock,\n\t\t\t\t  struct buffer_head *bh_result, int create)\n{\n\tint ret = 0;\n\tsector_t invalid_block = ~((sector_t) 0xffff);\n\n\tif (invalid_block < ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es))\n\t\tinvalid_block = ~0;\n\n\tBUG_ON(create == 0);\n\tBUG_ON(bh_result->b_size != inode->i_sb->s_blocksize);\n\n\t/*\n\t * first, we need to know whether the block is allocated already\n\t * preallocated blocks are unmapped but should treated\n\t * the same as allocated blocks.\n\t */\n\tret = ext4_get_blocks(NULL, inode, iblock, 1,  bh_result, 0);\n\tif ((ret == 0) && !buffer_delay(bh_result)) {\n\t\t/* the block isn't (pre)allocated yet, let's reserve space */\n\t\t/*\n\t\t * XXX: __block_prepare_write() unmaps passed block,\n\t\t * is it OK?\n\t\t */\n\t\tret = ext4_da_reserve_space(inode, iblock);\n\t\tif (ret)\n\t\t\t/* not enough space to reserve */\n\t\t\treturn ret;\n\n\t\tmap_bh(bh_result, inode->i_sb, invalid_block);\n\t\tset_buffer_new(bh_result);\n\t\tset_buffer_delay(bh_result);\n\t} else if (ret > 0) {\n\t\tbh_result->b_size = (ret << inode->i_blkbits);\n\t\tif (buffer_unwritten(bh_result)) {\n\t\t\t/* A delayed write to unwritten bh should\n\t\t\t * be marked new and mapped.  Mapped ensures\n\t\t\t * that we don't do get_block multiple times\n\t\t\t * when we write to the same offset and new\n\t\t\t * ensures that we do proper zero out for\n\t\t\t * partial write.\n\t\t\t */\n\t\t\tset_buffer_new(bh_result);\n\t\t\tset_buffer_mapped(bh_result);\n\t\t}\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n/*\n * This function is used as a standard get_block_t calback function\n * when there is no desire to allocate any blocks.  It is used as a\n * callback function for block_prepare_write(), nobh_writepage(), and\n * block_write_full_page().  These functions should only try to map a\n * single block at a time.\n *\n * Since this function doesn't do block allocations even if the caller\n * requests it by passing in create=1, it is critically important that\n * any caller checks to make sure that any buffer heads are returned\n * by this function are either all already mapped or marked for\n * delayed allocation before calling nobh_writepage() or\n * block_write_full_page().  Otherwise, b_blocknr could be left\n * unitialized, and the page write functions will be taken by\n * surprise.\n */\nstatic int noalloc_get_block_write(struct inode *inode, sector_t iblock,\n\t\t\t\t   struct buffer_head *bh_result, int create)\n{\n\tint ret = 0;\n\tunsigned max_blocks = bh_result->b_size >> inode->i_blkbits;\n\n\tBUG_ON(bh_result->b_size != inode->i_sb->s_blocksize);\n\n\t/*\n\t * we don't want to do block allocation in writepage\n\t * so call get_block_wrap with create = 0\n\t */\n\tret = ext4_get_blocks(NULL, inode, iblock, max_blocks, bh_result, 0);\n\tif (ret > 0) {\n\t\tbh_result->b_size = (ret << inode->i_blkbits);\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nstatic int bget_one(handle_t *handle, struct buffer_head *bh)\n{\n\tget_bh(bh);\n\treturn 0;\n}\n\nstatic int bput_one(handle_t *handle, struct buffer_head *bh)\n{\n\tput_bh(bh);\n\treturn 0;\n}\n\nstatic int __ext4_journalled_writepage(struct page *page,\n\t\t\t\t       unsigned int len)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct buffer_head *page_bufs;\n\thandle_t *handle = NULL;\n\tint ret = 0;\n\tint err;\n\n\tpage_bufs = page_buffers(page);\n\tBUG_ON(!page_bufs);\n\twalk_page_buffers(handle, page_bufs, 0, len, NULL, bget_one);\n\t/* As soon as we unlock the page, it can go away, but we have\n\t * references to buffers so we are safe */\n\tunlock_page(page);\n\n\thandle = ext4_journal_start(inode, ext4_writepage_trans_blocks(inode));\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out;\n\t}\n\n\tret = walk_page_buffers(handle, page_bufs, 0, len, NULL,\n\t\t\t\tdo_journal_get_write_access);\n\n\terr = walk_page_buffers(handle, page_bufs, 0, len, NULL,\n\t\t\t\twrite_end_fn);\n\tif (ret == 0)\n\t\tret = err;\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\n\twalk_page_buffers(handle, page_bufs, 0, len, NULL, bput_one);\n\text4_set_inode_state(inode, EXT4_STATE_JDATA);\nout:\n\treturn ret;\n}\n\nstatic int ext4_set_bh_endio(struct buffer_head *bh, struct inode *inode);\nstatic void ext4_end_io_buffer_write(struct buffer_head *bh, int uptodate);\n\n/*\n * Note that we don't need to start a transaction unless we're journaling data\n * because we should have holes filled from ext4_page_mkwrite(). We even don't\n * need to file the inode to the transaction's list in ordered mode because if\n * we are writing back data added by write(), the inode is already there and if\n * we are writing back data modified via mmap(), noone guarantees in which\n * transaction the data will hit the disk. In case we are journaling data, we\n * cannot start transaction directly because transaction start ranks above page\n * lock so we have to do some magic.\n *\n * This function can get called via...\n *   - ext4_da_writepages after taking page lock (have journal handle)\n *   - journal_submit_inode_data_buffers (no journal handle)\n *   - shrink_page_list via pdflush (no journal handle)\n *   - grab_page_cache when doing write_begin (have journal handle)\n *\n * We don't do any block allocation in this function. If we have page with\n * multiple blocks we need to write those buffer_heads that are mapped. This\n * is important for mmaped based write. So if we do with blocksize 1K\n * truncate(f, 1024);\n * a = mmap(f, 0, 4096);\n * a[0] = 'a';\n * truncate(f, 4096);\n * we have in the page first buffer_head mapped via page_mkwrite call back\n * but other bufer_heads would be unmapped but dirty(dirty done via the\n * do_wp_page). So writepage should write the first block. If we modify\n * the mmap area beyond 1024 we will again get a page_fault and the\n * page_mkwrite callback will do the block allocation and mark the\n * buffer_heads mapped.\n *\n * We redirty the page if we have any buffer_heads that is either delay or\n * unwritten in the page.\n *\n * We can get recursively called as show below.\n *\n *\text4_writepage() -> kmalloc() -> __alloc_pages() -> page_launder() ->\n *\t\text4_writepage()\n *\n * But since we don't do any block allocation we should not deadlock.\n * Page also have the dirty flag cleared so we don't get recurive page_lock.\n */\nstatic int ext4_writepage(struct page *page,\n\t\t\t  struct writeback_control *wbc)\n{\n\tint ret = 0;\n\tloff_t size;\n\tunsigned int len;\n\tstruct buffer_head *page_bufs = NULL;\n\tstruct inode *inode = page->mapping->host;\n\n\ttrace_ext4_writepage(inode, page);\n\tsize = i_size_read(inode);\n\tif (page->index == size >> PAGE_CACHE_SHIFT)\n\t\tlen = size & ~PAGE_CACHE_MASK;\n\telse\n\t\tlen = PAGE_CACHE_SIZE;\n\n\tif (page_has_buffers(page)) {\n\t\tpage_bufs = page_buffers(page);\n\t\tif (walk_page_buffers(NULL, page_bufs, 0, len, NULL,\n\t\t\t\t\text4_bh_delay_or_unwritten)) {\n\t\t\t/*\n\t\t\t * We don't want to do  block allocation\n\t\t\t * So redirty the page and return\n\t\t\t * We may reach here when we do a journal commit\n\t\t\t * via journal_submit_inode_data_buffers.\n\t\t\t * If we don't have mapping block we just ignore\n\t\t\t * them. We can also reach here via shrink_page_list\n\t\t\t */\n\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\tunlock_page(page);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * The test for page_has_buffers() is subtle:\n\t\t * We know the page is dirty but it lost buffers. That means\n\t\t * that at some moment in time after write_begin()/write_end()\n\t\t * has been called all buffers have been clean and thus they\n\t\t * must have been written at least once. So they are all\n\t\t * mapped and we can happily proceed with mapping them\n\t\t * and writing the page.\n\t\t *\n\t\t * Try to initialize the buffer_heads and check whether\n\t\t * all are mapped and non delay. We don't want to\n\t\t * do block allocation here.\n\t\t */\n\t\tret = block_prepare_write(page, 0, len,\n\t\t\t\t\t  noalloc_get_block_write);\n\t\tif (!ret) {\n\t\t\tpage_bufs = page_buffers(page);\n\t\t\t/* check whether all are mapped and non delay */\n\t\t\tif (walk_page_buffers(NULL, page_bufs, 0, len, NULL,\n\t\t\t\t\t\text4_bh_delay_or_unwritten)) {\n\t\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\t\tunlock_page(page);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t} else {\n\t\t\t/*\n\t\t\t * We can't do block allocation here\n\t\t\t * so just redity the page and unlock\n\t\t\t * and return\n\t\t\t */\n\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\tunlock_page(page);\n\t\t\treturn 0;\n\t\t}\n\t\t/* now mark the buffer_heads as dirty and uptodate */\n\t\tblock_commit_write(page, 0, len);\n\t}\n\n\tif (PageChecked(page) && ext4_should_journal_data(inode)) {\n\t\t/*\n\t\t * It's mmapped pagecache.  Add buffers and journal it.  There\n\t\t * doesn't seem much point in redirtying the page here.\n\t\t */\n\t\tClearPageChecked(page);\n\t\treturn __ext4_journalled_writepage(page, len);\n\t}\n\n\tif (test_opt(inode->i_sb, NOBH) && ext4_should_writeback_data(inode))\n\t\tret = nobh_writepage(page, noalloc_get_block_write, wbc);\n\telse if (page_bufs && buffer_uninit(page_bufs)) {\n\t\text4_set_bh_endio(page_bufs, inode);\n\t\tret = block_write_full_page_endio(page, noalloc_get_block_write,\n\t\t\t\t\t    wbc, ext4_end_io_buffer_write);\n\t} else\n\t\tret = block_write_full_page(page, noalloc_get_block_write,\n\t\t\t\t\t    wbc);\n\n\treturn ret;\n}\n\n/*\n * This is called via ext4_da_writepages() to\n * calulate the total number of credits to reserve to fit\n * a single extent allocation into a single transaction,\n * ext4_da_writpeages() will loop calling this before\n * the block allocation.\n */\n\nstatic int ext4_da_writepages_trans_blocks(struct inode *inode)\n{\n\tint max_blocks = EXT4_I(inode)->i_reserved_data_blocks;\n\n\t/*\n\t * With non-extent format the journal credit needed to\n\t * insert nrblocks contiguous block is dependent on\n\t * number of contiguous block. So we will limit\n\t * number of contiguous block to a sane value\n\t */\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL) &&\n\t    (max_blocks > EXT4_MAX_TRANS_DATA))\n\t\tmax_blocks = EXT4_MAX_TRANS_DATA;\n\n\treturn ext4_chunk_trans_blocks(inode, max_blocks);\n}\n\nstatic int ext4_da_writepages(struct address_space *mapping,\n\t\t\t      struct writeback_control *wbc)\n{\n\tpgoff_t\tindex;\n\tint range_whole = 0;\n\thandle_t *handle = NULL;\n\tstruct mpage_da_data mpd;\n\tstruct inode *inode = mapping->host;\n\tint no_nrwrite_index_update;\n\tint pages_written = 0;\n\tlong pages_skipped;\n\tunsigned int max_pages;\n\tint range_cyclic, cycled = 1, io_done = 0;\n\tint needed_blocks, ret = 0;\n\tlong desired_nr_to_write, nr_to_writebump = 0;\n\tloff_t range_start = wbc->range_start;\n\tstruct ext4_sb_info *sbi = EXT4_SB(mapping->host->i_sb);\n\n\ttrace_ext4_da_writepages(inode, wbc);\n\n\t/*\n\t * No pages to write? This is mainly a kludge to avoid starting\n\t * a transaction for special inodes like journal inode on last iput()\n\t * because that could violate lock ordering on umount\n\t */\n\tif (!mapping->nrpages || !mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\treturn 0;\n\n\t/*\n\t * If the filesystem has aborted, it is read-only, so return\n\t * right away instead of dumping stack traces later on that\n\t * will obscure the real source of the problem.  We test\n\t * EXT4_MF_FS_ABORTED instead of sb->s_flag's MS_RDONLY because\n\t * the latter could be true if the filesystem is mounted\n\t * read-only, and in that case, ext4_da_writepages should\n\t * *never* be called, so if that ever happens, we would want\n\t * the stack trace.\n\t */\n\tif (unlikely(sbi->s_mount_flags & EXT4_MF_FS_ABORTED))\n\t\treturn -EROFS;\n\n\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\trange_whole = 1;\n\n\trange_cyclic = wbc->range_cyclic;\n\tif (wbc->range_cyclic) {\n\t\tindex = mapping->writeback_index;\n\t\tif (index)\n\t\t\tcycled = 0;\n\t\twbc->range_start = index << PAGE_CACHE_SHIFT;\n\t\twbc->range_end  = LLONG_MAX;\n\t\twbc->range_cyclic = 0;\n\t} else\n\t\tindex = wbc->range_start >> PAGE_CACHE_SHIFT;\n\n\t/*\n\t * This works around two forms of stupidity.  The first is in\n\t * the writeback code, which caps the maximum number of pages\n\t * written to be 1024 pages.  This is wrong on multiple\n\t * levels; different architectues have a different page size,\n\t * which changes the maximum amount of data which gets\n\t * written.  Secondly, 4 megabytes is way too small.  XFS\n\t * forces this value to be 16 megabytes by multiplying\n\t * nr_to_write parameter by four, and then relies on its\n\t * allocator to allocate larger extents to make them\n\t * contiguous.  Unfortunately this brings us to the second\n\t * stupidity, which is that ext4's mballoc code only allocates\n\t * at most 2048 blocks.  So we force contiguous writes up to\n\t * the number of dirty blocks in the inode, or\n\t * sbi->max_writeback_mb_bump whichever is smaller.\n\t */\n\tmax_pages = sbi->s_max_writeback_mb_bump << (20 - PAGE_CACHE_SHIFT);\n\tif (!range_cyclic && range_whole)\n\t\tdesired_nr_to_write = wbc->nr_to_write * 8;\n\telse\n\t\tdesired_nr_to_write = ext4_num_dirty_pages(inode, index,\n\t\t\t\t\t\t\t   max_pages);\n\tif (desired_nr_to_write > max_pages)\n\t\tdesired_nr_to_write = max_pages;\n\n\tif (wbc->nr_to_write < desired_nr_to_write) {\n\t\tnr_to_writebump = desired_nr_to_write - wbc->nr_to_write;\n\t\twbc->nr_to_write = desired_nr_to_write;\n\t}\n\n\tmpd.wbc = wbc;\n\tmpd.inode = mapping->host;\n\n\t/*\n\t * we don't want write_cache_pages to update\n\t * nr_to_write and writeback_index\n\t */\n\tno_nrwrite_index_update = wbc->no_nrwrite_index_update;\n\twbc->no_nrwrite_index_update = 1;\n\tpages_skipped = wbc->pages_skipped;\n\nretry:\n\twhile (!ret && wbc->nr_to_write > 0) {\n\n\t\t/*\n\t\t * we  insert one extent at a time. So we need\n\t\t * credit needed for single extent allocation.\n\t\t * journalled mode is currently not supported\n\t\t * by delalloc\n\t\t */\n\t\tBUG_ON(ext4_should_journal_data(inode));\n\t\tneeded_blocks = ext4_da_writepages_trans_blocks(inode);\n\n\t\t/* start a new transaction*/\n\t\thandle = ext4_journal_start(inode, needed_blocks);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\text4_msg(inode->i_sb, KERN_CRIT, \"%s: jbd2_start: \"\n\t\t\t       \"%ld pages, ino %lu; err %d\\n\", __func__,\n\t\t\t\twbc->nr_to_write, inode->i_ino, ret);\n\t\t\tgoto out_writepages;\n\t\t}\n\n\t\t/*\n\t\t * Now call __mpage_da_writepage to find the next\n\t\t * contiguous region of logical blocks that need\n\t\t * blocks to be allocated by ext4.  We don't actually\n\t\t * submit the blocks for I/O here, even though\n\t\t * write_cache_pages thinks it will, and will set the\n\t\t * pages as clean for write before calling\n\t\t * __mpage_da_writepage().\n\t\t */\n\t\tmpd.b_size = 0;\n\t\tmpd.b_state = 0;\n\t\tmpd.b_blocknr = 0;\n\t\tmpd.first_page = 0;\n\t\tmpd.next_page = 0;\n\t\tmpd.io_done = 0;\n\t\tmpd.pages_written = 0;\n\t\tmpd.retval = 0;\n\t\tret = write_cache_pages(mapping, wbc, __mpage_da_writepage,\n\t\t\t\t\t&mpd);\n\t\t/*\n\t\t * If we have a contiguous extent of pages and we\n\t\t * haven't done the I/O yet, map the blocks and submit\n\t\t * them for I/O.\n\t\t */\n\t\tif (!mpd.io_done && mpd.next_page != mpd.first_page) {\n\t\t\tif (mpage_da_map_blocks(&mpd) == 0)\n\t\t\t\tmpage_da_submit_io(&mpd);\n\t\t\tmpd.io_done = 1;\n\t\t\tret = MPAGE_DA_EXTENT_TAIL;\n\t\t}\n\t\ttrace_ext4_da_write_pages(inode, &mpd);\n\t\twbc->nr_to_write -= mpd.pages_written;\n\n\t\text4_journal_stop(handle);\n\n\t\tif ((mpd.retval == -ENOSPC) && sbi->s_journal) {\n\t\t\t/* commit the transaction which would\n\t\t\t * free blocks released in the transaction\n\t\t\t * and try again\n\t\t\t */\n\t\t\tjbd2_journal_force_commit_nested(sbi->s_journal);\n\t\t\twbc->pages_skipped = pages_skipped;\n\t\t\tret = 0;\n\t\t} else if (ret == MPAGE_DA_EXTENT_TAIL) {\n\t\t\t/*\n\t\t\t * got one extent now try with\n\t\t\t * rest of the pages\n\t\t\t */\n\t\t\tpages_written += mpd.pages_written;\n\t\t\twbc->pages_skipped = pages_skipped;\n\t\t\tret = 0;\n\t\t\tio_done = 1;\n\t\t} else if (wbc->nr_to_write)\n\t\t\t/*\n\t\t\t * There is no more writeout needed\n\t\t\t * or we requested for a noblocking writeout\n\t\t\t * and we found the device congested\n\t\t\t */\n\t\t\tbreak;\n\t}\n\tif (!io_done && !cycled) {\n\t\tcycled = 1;\n\t\tindex = 0;\n\t\twbc->range_start = index << PAGE_CACHE_SHIFT;\n\t\twbc->range_end  = mapping->writeback_index - 1;\n\t\tgoto retry;\n\t}\n\tif (pages_skipped != wbc->pages_skipped)\n\t\text4_msg(inode->i_sb, KERN_CRIT,\n\t\t\t \"This should not happen leaving %s \"\n\t\t\t \"with nr_to_write = %ld ret = %d\\n\",\n\t\t\t __func__, wbc->nr_to_write, ret);\n\n\t/* Update index */\n\tindex += pages_written;\n\twbc->range_cyclic = range_cyclic;\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\t/*\n\t\t * set the writeback_index so that range_cyclic\n\t\t * mode will write it back later\n\t\t */\n\t\tmapping->writeback_index = index;\n\nout_writepages:\n\tif (!no_nrwrite_index_update)\n\t\twbc->no_nrwrite_index_update = 0;\n\twbc->nr_to_write -= nr_to_writebump;\n\twbc->range_start = range_start;\n\ttrace_ext4_da_writepages_result(inode, wbc, ret, pages_written);\n\treturn ret;\n}\n\n#define FALL_BACK_TO_NONDELALLOC 1\nstatic int ext4_nonda_switch(struct super_block *sb)\n{\n\ts64 free_blocks, dirty_blocks;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\t/*\n\t * switch to non delalloc mode if we are running low\n\t * on free block. The free block accounting via percpu\n\t * counters can get slightly wrong with percpu_counter_batch getting\n\t * accumulated on each CPU without updating global counters\n\t * Delalloc need an accurate free block accounting. So switch\n\t * to non delalloc when we are near to error range.\n\t */\n\tfree_blocks  = percpu_counter_read_positive(&sbi->s_freeblocks_counter);\n\tdirty_blocks = percpu_counter_read_positive(&sbi->s_dirtyblocks_counter);\n\tif (2 * free_blocks < 3 * dirty_blocks ||\n\t\tfree_blocks < (dirty_blocks + EXT4_FREEBLOCKS_WATERMARK)) {\n\t\t/*\n\t\t * free block count is less than 150% of dirty blocks\n\t\t * or free blocks is less than watermark\n\t\t */\n\t\treturn 1;\n\t}\n\t/*\n\t * Even if we don't switch but are nearing capacity,\n\t * start pushing delalloc when 1/2 of free blocks are dirty.\n\t */\n\tif (free_blocks < 2 * dirty_blocks)\n\t\twriteback_inodes_sb_if_idle(sb);\n\n\treturn 0;\n}\n\nstatic int ext4_da_write_begin(struct file *file, struct address_space *mapping,\n\t\t\t       loff_t pos, unsigned len, unsigned flags,\n\t\t\t       struct page **pagep, void **fsdata)\n{\n\tint ret, retries = 0, quota_retries = 0;\n\tstruct page *page;\n\tpgoff_t index;\n\tunsigned from, to;\n\tstruct inode *inode = mapping->host;\n\thandle_t *handle;\n\n\tindex = pos >> PAGE_CACHE_SHIFT;\n\tfrom = pos & (PAGE_CACHE_SIZE - 1);\n\tto = from + len;\n\n\tif (ext4_nonda_switch(inode->i_sb)) {\n\t\t*fsdata = (void *)FALL_BACK_TO_NONDELALLOC;\n\t\treturn ext4_write_begin(file, mapping, pos,\n\t\t\t\t\tlen, flags, pagep, fsdata);\n\t}\n\t*fsdata = (void *)0;\n\ttrace_ext4_da_write_begin(inode, pos, len, flags);\nretry:\n\t/*\n\t * With delayed allocation, we don't log the i_disksize update\n\t * if there is delayed block allocation. But we still need\n\t * to journalling the i_disksize update if writes to the end\n\t * of file which has an already mapped buffer.\n\t */\n\thandle = ext4_journal_start(inode, 1);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out;\n\t}\n\t/* We cannot recurse into the filesystem as the transaction is already\n\t * started */\n\tflags |= AOP_FLAG_NOFS;\n\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page) {\n\t\text4_journal_stop(handle);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\t*pagep = page;\n\n\tret = block_write_begin(file, mapping, pos, len, flags, pagep, fsdata,\n\t\t\t\text4_da_get_block_prep);\n\tif (ret < 0) {\n\t\tunlock_page(page);\n\t\text4_journal_stop(handle);\n\t\tpage_cache_release(page);\n\t\t/*\n\t\t * block_write_begin may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again. Don't need\n\t\t * i_size_read because we hold i_mutex.\n\t\t */\n\t\tif (pos + len > inode->i_size)\n\t\t\text4_truncate_failed_write(inode);\n\t}\n\n\tif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry;\n\n\tif ((ret == -EDQUOT) &&\n\t    EXT4_I(inode)->i_reserved_meta_blocks &&\n\t    (quota_retries++ < 3)) {\n\t\t/*\n\t\t * Since we often over-estimate the number of meta\n\t\t * data blocks required, we may sometimes get a\n\t\t * spurios out of quota error even though there would\n\t\t * be enough space once we write the data blocks and\n\t\t * find out how many meta data blocks were _really_\n\t\t * required.  So try forcing the inode write to see if\n\t\t * that helps.\n\t\t */\n\t\twrite_inode_now(inode, (quota_retries == 3));\n\t\tgoto retry;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * Check if we should update i_disksize\n * when write to the end of file but not require block allocation\n */\nstatic int ext4_da_should_update_i_disksize(struct page *page,\n\t\t\t\t\t    unsigned long offset)\n{\n\tstruct buffer_head *bh;\n\tstruct inode *inode = page->mapping->host;\n\tunsigned int idx;\n\tint i;\n\n\tbh = page_buffers(page);\n\tidx = offset >> inode->i_blkbits;\n\n\tfor (i = 0; i < idx; i++)\n\t\tbh = bh->b_this_page;\n\n\tif (!buffer_mapped(bh) || (buffer_delay(bh)) || buffer_unwritten(bh))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int ext4_da_write_end(struct file *file,\n\t\t\t     struct address_space *mapping,\n\t\t\t     loff_t pos, unsigned len, unsigned copied,\n\t\t\t     struct page *page, void *fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tint ret = 0, ret2;\n\thandle_t *handle = ext4_journal_current_handle();\n\tloff_t new_i_size;\n\tunsigned long start, end;\n\tint write_mode = (int)(unsigned long)fsdata;\n\n\tif (write_mode == FALL_BACK_TO_NONDELALLOC) {\n\t\tif (ext4_should_order_data(inode)) {\n\t\t\treturn ext4_ordered_write_end(file, mapping, pos,\n\t\t\t\t\tlen, copied, page, fsdata);\n\t\t} else if (ext4_should_writeback_data(inode)) {\n\t\t\treturn ext4_writeback_write_end(file, mapping, pos,\n\t\t\t\t\tlen, copied, page, fsdata);\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t}\n\n\ttrace_ext4_da_write_end(inode, pos, len, copied);\n\tstart = pos & (PAGE_CACHE_SIZE - 1);\n\tend = start + copied - 1;\n\n\t/*\n\t * generic_write_end() will run mark_inode_dirty() if i_size\n\t * changes.  So let's piggyback the i_disksize mark_inode_dirty\n\t * into that.\n\t */\n\n\tnew_i_size = pos + copied;\n\tif (new_i_size > EXT4_I(inode)->i_disksize) {\n\t\tif (ext4_da_should_update_i_disksize(page, end)) {\n\t\t\tdown_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tif (new_i_size > EXT4_I(inode)->i_disksize) {\n\t\t\t\t/*\n\t\t\t\t * Updating i_disksize when extending file\n\t\t\t\t * without needing block allocation\n\t\t\t\t */\n\t\t\t\tif (ext4_should_order_data(inode))\n\t\t\t\t\tret = ext4_jbd2_file_inode(handle,\n\t\t\t\t\t\t\t\t   inode);\n\n\t\t\t\tEXT4_I(inode)->i_disksize = new_i_size;\n\t\t\t}\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\t/* We need to mark inode dirty even if\n\t\t\t * new_i_size is less that inode->i_size\n\t\t\t * bu greater than i_disksize.(hint delalloc)\n\t\t\t */\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t}\n\t}\n\tret2 = generic_write_end(file, mapping, pos, len, copied,\n\t\t\t\t\t\t\tpage, fsdata);\n\tcopied = ret2;\n\tif (ret2 < 0)\n\t\tret = ret2;\n\tret2 = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = ret2;\n\n\treturn ret ? ret : copied;\n}\n\nstatic void ext4_da_invalidatepage(struct page *page, unsigned long offset)\n{\n\t/*\n\t * Drop reserved blocks\n\t */\n\tBUG_ON(!PageLocked(page));\n\tif (!page_has_buffers(page))\n\t\tgoto out;\n\n\text4_da_page_release_reservation(page, offset);\n\nout:\n\text4_invalidatepage(page, offset);\n\n\treturn;\n}\n\n/*\n * Force all delayed allocation blocks to be allocated for a given inode.\n */\nint ext4_alloc_da_blocks(struct inode *inode)\n{\n\ttrace_ext4_alloc_da_blocks(inode);\n\n\tif (!EXT4_I(inode)->i_reserved_data_blocks &&\n\t    !EXT4_I(inode)->i_reserved_meta_blocks)\n\t\treturn 0;\n\n\t/*\n\t * We do something simple for now.  The filemap_flush() will\n\t * also start triggering a write of the data blocks, which is\n\t * not strictly speaking necessary (and for users of\n\t * laptop_mode, not even desirable).  However, to do otherwise\n\t * would require replicating code paths in:\n\t *\n\t * ext4_da_writepages() ->\n\t *    write_cache_pages() ---> (via passed in callback function)\n\t *        __mpage_da_writepage() -->\n\t *           mpage_add_bh_to_extent()\n\t *           mpage_da_map_blocks()\n\t *\n\t * The problem is that write_cache_pages(), located in\n\t * mm/page-writeback.c, marks pages clean in preparation for\n\t * doing I/O, which is not desirable if we're not planning on\n\t * doing I/O at all.\n\t *\n\t * We could call write_cache_pages(), and then redirty all of\n\t * the pages by calling redirty_page_for_writeback() but that\n\t * would be ugly in the extreme.  So instead we would need to\n\t * replicate parts of the code in the above functions,\n\t * simplifying them becuase we wouldn't actually intend to\n\t * write out the pages, but rather only collect contiguous\n\t * logical block extents, call the multi-block allocator, and\n\t * then update the buffer heads with the block allocations.\n\t *\n\t * For now, though, we'll cheat by calling filemap_flush(),\n\t * which will map the blocks, and start the I/O, but not\n\t * actually wait for the I/O to complete.\n\t */\n\treturn filemap_flush(inode->i_mapping);\n}\n\n/*\n * bmap() is special.  It gets used by applications such as lilo and by\n * the swapper to find the on-disk block of a specific piece of data.\n *\n * Naturally, this is dangerous if the block concerned is still in the\n * journal.  If somebody makes a swapfile on an ext4 data-journaling\n * filesystem and enables swap, then they may get a nasty shock when the\n * data getting swapped to that swapfile suddenly gets overwritten by\n * the original zero's written out previously to the journal and\n * awaiting writeback in the kernel's buffer cache.\n *\n * So, if we see any bmap calls here on a modified, data-journaled file,\n * take extra steps to flush any blocks which might be in the cache.\n */\nstatic sector_t ext4_bmap(struct address_space *mapping, sector_t block)\n{\n\tstruct inode *inode = mapping->host;\n\tjournal_t *journal;\n\tint err;\n\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) &&\n\t\t\ttest_opt(inode->i_sb, DELALLOC)) {\n\t\t/*\n\t\t * With delalloc we want to sync the file\n\t\t * so that we can make sure we allocate\n\t\t * blocks for file\n\t\t */\n\t\tfilemap_write_and_wait(mapping);\n\t}\n\n\tif (EXT4_JOURNAL(inode) &&\n\t    ext4_test_inode_state(inode, EXT4_STATE_JDATA)) {\n\t\t/*\n\t\t * This is a REALLY heavyweight approach, but the use of\n\t\t * bmap on dirty files is expected to be extremely rare:\n\t\t * only if we run lilo or swapon on a freshly made file\n\t\t * do we expect this to happen.\n\t\t *\n\t\t * (bmap requires CAP_SYS_RAWIO so this does not\n\t\t * represent an unprivileged user DOS attack --- we'd be\n\t\t * in trouble if mortal users could trigger this path at\n\t\t * will.)\n\t\t *\n\t\t * NB. EXT4_STATE_JDATA is not set on files other than\n\t\t * regular files.  If somebody wants to bmap a directory\n\t\t * or symlink and gets confused because the buffer\n\t\t * hasn't yet been flushed to disk, they deserve\n\t\t * everything they get.\n\t\t */\n\n\t\text4_clear_inode_state(inode, EXT4_STATE_JDATA);\n\t\tjournal = EXT4_JOURNAL(inode);\n\t\tjbd2_journal_lock_updates(journal);\n\t\terr = jbd2_journal_flush(journal);\n\t\tjbd2_journal_unlock_updates(journal);\n\n\t\tif (err)\n\t\t\treturn 0;\n\t}\n\n\treturn generic_block_bmap(mapping, block, ext4_get_block);\n}\n\nstatic int ext4_readpage(struct file *file, struct page *page)\n{\n\treturn mpage_readpage(page, ext4_get_block);\n}\n\nstatic int\next4_readpages(struct file *file, struct address_space *mapping,\n\t\tstruct list_head *pages, unsigned nr_pages)\n{\n\treturn mpage_readpages(mapping, pages, nr_pages, ext4_get_block);\n}\n\nstatic void ext4_free_io_end(ext4_io_end_t *io)\n{\n\tBUG_ON(!io);\n\tif (io->page)\n\t\tput_page(io->page);\n\tiput(io->inode);\n\tkfree(io);\n}\n\nstatic void ext4_invalidatepage_free_endio(struct page *page, unsigned long offset)\n{\n\tstruct buffer_head *head, *bh;\n\tunsigned int curr_off = 0;\n\n\tif (!page_has_buffers(page))\n\t\treturn;\n\thead = bh = page_buffers(page);\n\tdo {\n\t\tif (offset <= curr_off && test_clear_buffer_uninit(bh)\n\t\t\t\t\t&& bh->b_private) {\n\t\t\text4_free_io_end(bh->b_private);\n\t\t\tbh->b_private = NULL;\n\t\t\tbh->b_end_io = NULL;\n\t\t}\n\t\tcurr_off = curr_off + bh->b_size;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n}\n\nstatic void ext4_invalidatepage(struct page *page, unsigned long offset)\n{\n\tjournal_t *journal = EXT4_JOURNAL(page->mapping->host);\n\n\t/*\n\t * free any io_end structure allocated for buffers to be discarded\n\t */\n\tif (ext4_should_dioread_nolock(page->mapping->host))\n\t\text4_invalidatepage_free_endio(page, offset);\n\t/*\n\t * If it's a full truncate we just forget about the pending dirtying\n\t */\n\tif (offset == 0)\n\t\tClearPageChecked(page);\n\n\tif (journal)\n\t\tjbd2_journal_invalidatepage(journal, page, offset);\n\telse\n\t\tblock_invalidatepage(page, offset);\n}\n\nstatic int ext4_releasepage(struct page *page, gfp_t wait)\n{\n\tjournal_t *journal = EXT4_JOURNAL(page->mapping->host);\n\n\tWARN_ON(PageChecked(page));\n\tif (!page_has_buffers(page))\n\t\treturn 0;\n\tif (journal)\n\t\treturn jbd2_journal_try_to_free_buffers(journal, page, wait);\n\telse\n\t\treturn try_to_free_buffers(page);\n}\n\n/*\n * O_DIRECT for ext3 (or indirect map) based files\n *\n * If the O_DIRECT write will extend the file then add this inode to the\n * orphan list.  So recovery will truncate it back to the original size\n * if the machine crashes during the write.\n *\n * If the O_DIRECT write is intantiating holes inside i_size and the machine\n * crashes then stale disk data _may_ be exposed inside the file. But current\n * VFS code falls back into buffered path in that case so we are safe.\n */\nstatic ssize_t ext4_ind_direct_IO(int rw, struct kiocb *iocb,\n\t\t\t      const struct iovec *iov, loff_t offset,\n\t\t\t      unsigned long nr_segs)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\thandle_t *handle;\n\tssize_t ret;\n\tint orphan = 0;\n\tsize_t count = iov_length(iov, nr_segs);\n\tint retries = 0;\n\n\tif (rw == WRITE) {\n\t\tloff_t final_size = offset + count;\n\n\t\tif (final_size > inode->i_size) {\n\t\t\t/* Credits for sb + inode write */\n\t\t\thandle = ext4_journal_start(inode, 2);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\tret = PTR_ERR(handle);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = ext4_orphan_add(handle, inode);\n\t\t\tif (ret) {\n\t\t\t\text4_journal_stop(handle);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\torphan = 1;\n\t\t\tei->i_disksize = inode->i_size;\n\t\t\text4_journal_stop(handle);\n\t\t}\n\t}\n\nretry:\n\tret = blockdev_direct_IO(rw, iocb, inode, inode->i_sb->s_bdev, iov,\n\t\t\t\t offset, nr_segs,\n\t\t\t\t ext4_get_block, NULL);\n\tif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\n\t\tgoto retry;\n\n\tif (orphan) {\n\t\tint err;\n\n\t\t/* Credits for sb + inode write */\n\t\thandle = ext4_journal_start(inode, 2);\n\t\tif (IS_ERR(handle)) {\n\t\t\t/* This is really bad luck. We've written the data\n\t\t\t * but cannot extend i_size. Bail out and pretend\n\t\t\t * the write failed... */\n\t\t\tret = PTR_ERR(handle);\n\t\t\tif (inode->i_nlink)\n\t\t\t\text4_orphan_del(NULL, inode);\n\n\t\t\tgoto out;\n\t\t}\n\t\tif (inode->i_nlink)\n\t\t\text4_orphan_del(handle, inode);\n\t\tif (ret > 0) {\n\t\t\tloff_t end = offset + ret;\n\t\t\tif (end > inode->i_size) {\n\t\t\t\tei->i_disksize = end;\n\t\t\t\ti_size_write(inode, end);\n\t\t\t\t/*\n\t\t\t\t * We're going to return a positive `ret'\n\t\t\t\t * here due to non-zero-length I/O, so there's\n\t\t\t\t * no way of reporting error returns from\n\t\t\t\t * ext4_mark_inode_dirty() to userspace.  So\n\t\t\t\t * ignore it.\n\t\t\t\t */\n\t\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\t}\n\t\t}\n\t\terr = ext4_journal_stop(handle);\n\t\tif (ret == 0)\n\t\t\tret = err;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int ext4_get_block_write(struct inode *inode, sector_t iblock,\n\t\t   struct buffer_head *bh_result, int create)\n{\n\thandle_t *handle = ext4_journal_current_handle();\n\tint ret = 0;\n\tunsigned max_blocks = bh_result->b_size >> inode->i_blkbits;\n\tint dio_credits;\n\tint started = 0;\n\n\text4_debug(\"ext4_get_block_write: inode %lu, create flag %d\\n\",\n\t\t   inode->i_ino, create);\n\t/*\n\t * ext4_get_block in prepare for a DIO write or buffer write.\n\t * We allocate an uinitialized extent if blocks haven't been allocated.\n\t * The extent will be converted to initialized after IO complete.\n\t */\n\tcreate = EXT4_GET_BLOCKS_IO_CREATE_EXT;\n\n\tif (!handle) {\n\t\tif (max_blocks > DIO_MAX_BLOCKS)\n\t\t\tmax_blocks = DIO_MAX_BLOCKS;\n\t\tdio_credits = ext4_chunk_trans_blocks(inode, max_blocks);\n\t\thandle = ext4_journal_start(inode, dio_credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tgoto out;\n\t\t}\n\t\tstarted = 1;\n\t}\n\n\tret = ext4_get_blocks(handle, inode, iblock, max_blocks, bh_result,\n\t\t\t      create);\n\tif (ret > 0) {\n\t\tbh_result->b_size = (ret << inode->i_blkbits);\n\t\tret = 0;\n\t}\n\tif (started)\n\t\text4_journal_stop(handle);\nout:\n\treturn ret;\n}\n\nstatic void dump_completed_IO(struct inode * inode)\n{\n#ifdef\tEXT4_DEBUG\n\tstruct list_head *cur, *before, *after;\n\text4_io_end_t *io, *io0, *io1;\n\tunsigned long flags;\n\n\tif (list_empty(&EXT4_I(inode)->i_completed_io_list)){\n\t\text4_debug(\"inode %lu completed_io list is empty\\n\", inode->i_ino);\n\t\treturn;\n\t}\n\n\text4_debug(\"Dump inode %lu completed_io list \\n\", inode->i_ino);\n\tspin_lock_irqsave(&EXT4_I(inode)->i_completed_io_lock, flags);\n\tlist_for_each_entry(io, &EXT4_I(inode)->i_completed_io_list, list){\n\t\tcur = &io->list;\n\t\tbefore = cur->prev;\n\t\tio0 = container_of(before, ext4_io_end_t, list);\n\t\tafter = cur->next;\n\t\tio1 = container_of(after, ext4_io_end_t, list);\n\n\t\text4_debug(\"io 0x%p from inode %lu,prev 0x%p,next 0x%p\\n\",\n\t\t\t    io, inode->i_ino, io0, io1);\n\t}\n\tspin_unlock_irqrestore(&EXT4_I(inode)->i_completed_io_lock, flags);\n#endif\n}\n\n/*\n * check a range of space and convert unwritten extents to written.\n */\nstatic int ext4_end_io_nolock(ext4_io_end_t *io)\n{\n\tstruct inode *inode = io->inode;\n\tloff_t offset = io->offset;\n\tssize_t size = io->size;\n\tint ret = 0;\n\n\text4_debug(\"ext4_end_io_nolock: io 0x%p from inode %lu,list->next 0x%p,\"\n\t\t   \"list->prev 0x%p\\n\",\n\t           io, inode->i_ino, io->list.next, io->list.prev);\n\n\tif (list_empty(&io->list))\n\t\treturn ret;\n\n\tif (io->flag != EXT4_IO_UNWRITTEN)\n\t\treturn ret;\n\n\tret = ext4_convert_unwritten_extents(inode, offset, size);\n\tif (ret < 0) {\n\t\tprintk(KERN_EMERG \"%s: failed to convert unwritten\"\n\t\t\t\"extents to written extents, error is %d\"\n\t\t\t\" io is still on inode %lu aio dio list\\n\",\n                       __func__, ret, inode->i_ino);\n\t\treturn ret;\n\t}\n\n\t/* clear the DIO AIO unwritten flag */\n\tio->flag = 0;\n\treturn ret;\n}\n\n/*\n * work on completed aio dio IO, to convert unwritten extents to extents\n */\nstatic void ext4_end_io_work(struct work_struct *work)\n{\n\text4_io_end_t\t\t*io = container_of(work, ext4_io_end_t, work);\n\tstruct inode\t\t*inode = io->inode;\n\tstruct ext4_inode_info\t*ei = EXT4_I(inode);\n\tunsigned long\t\tflags;\n\tint\t\t\tret;\n\n\tmutex_lock(&inode->i_mutex);\n\tret = ext4_end_io_nolock(io);\n\tif (ret < 0) {\n\t\tmutex_unlock(&inode->i_mutex);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&ei->i_completed_io_lock, flags);\n\tif (!list_empty(&io->list))\n\t\tlist_del_init(&io->list);\n\tspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\n\tmutex_unlock(&inode->i_mutex);\n\text4_free_io_end(io);\n}\n\n/*\n * This function is called from ext4_sync_file().\n *\n * When IO is completed, the work to convert unwritten extents to\n * written is queued on workqueue but may not get immediately\n * scheduled. When fsync is called, we need to ensure the\n * conversion is complete before fsync returns.\n * The inode keeps track of a list of pending/completed IO that\n * might needs to do the conversion. This function walks through\n * the list and convert the related unwritten extents for completed IO\n * to written.\n * The function return the number of pending IOs on success.\n */\nint flush_completed_IO(struct inode *inode)\n{\n\text4_io_end_t *io;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tunsigned long flags;\n\tint ret = 0;\n\tint ret2 = 0;\n\n\tif (list_empty(&ei->i_completed_io_list))\n\t\treturn ret;\n\n\tdump_completed_IO(inode);\n\tspin_lock_irqsave(&ei->i_completed_io_lock, flags);\n\twhile (!list_empty(&ei->i_completed_io_list)){\n\t\tio = list_entry(ei->i_completed_io_list.next,\n\t\t\t\text4_io_end_t, list);\n\t\t/*\n\t\t * Calling ext4_end_io_nolock() to convert completed\n\t\t * IO to written.\n\t\t *\n\t\t * When ext4_sync_file() is called, run_queue() may already\n\t\t * about to flush the work corresponding to this io structure.\n\t\t * It will be upset if it founds the io structure related\n\t\t * to the work-to-be schedule is freed.\n\t\t *\n\t\t * Thus we need to keep the io structure still valid here after\n\t\t * convertion finished. The io structure has a flag to\n\t\t * avoid double converting from both fsync and background work\n\t\t * queue work.\n\t\t */\n\t\tspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\n\t\tret = ext4_end_io_nolock(io);\n\t\tspin_lock_irqsave(&ei->i_completed_io_lock, flags);\n\t\tif (ret < 0)\n\t\t\tret2 = ret;\n\t\telse\n\t\t\tlist_del_init(&io->list);\n\t}\n\tspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\n\treturn (ret2 < 0) ? ret2 : 0;\n}\n\nstatic ext4_io_end_t *ext4_init_io_end (struct inode *inode, gfp_t flags)\n{\n\text4_io_end_t *io = NULL;\n\n\tio = kmalloc(sizeof(*io), flags);\n\n\tif (io) {\n\t\tigrab(inode);\n\t\tio->inode = inode;\n\t\tio->flag = 0;\n\t\tio->offset = 0;\n\t\tio->size = 0;\n\t\tio->page = NULL;\n\t\tINIT_WORK(&io->work, ext4_end_io_work);\n\t\tINIT_LIST_HEAD(&io->list);\n\t}\n\n\treturn io;\n}\n\nstatic void ext4_end_io_dio(struct kiocb *iocb, loff_t offset,\n\t\t\t    ssize_t size, void *private)\n{\n        ext4_io_end_t *io_end = iocb->private;\n\tstruct workqueue_struct *wq;\n\tunsigned long flags;\n\tstruct ext4_inode_info *ei;\n\n\t/* if not async direct IO or dio with 0 bytes write, just return */\n\tif (!io_end || !size)\n\t\treturn;\n\n\text_debug(\"ext4_end_io_dio(): io_end 0x%p\"\n\t\t  \"for inode %lu, iocb 0x%p, offset %llu, size %llu\\n\",\n \t\t  iocb->private, io_end->inode->i_ino, iocb, offset,\n\t\t  size);\n\n\t/* if not aio dio with unwritten extents, just free io and return */\n\tif (io_end->flag != EXT4_IO_UNWRITTEN){\n\t\text4_free_io_end(io_end);\n\t\tiocb->private = NULL;\n\t\treturn;\n\t}\n\n\tio_end->offset = offset;\n\tio_end->size = size;\n\tio_end->flag = EXT4_IO_UNWRITTEN;\n\twq = EXT4_SB(io_end->inode->i_sb)->dio_unwritten_wq;\n\n\t/* queue the work to convert unwritten extents to written */\n\tqueue_work(wq, &io_end->work);\n\n\t/* Add the io_end to per-inode completed aio dio list*/\n\tei = EXT4_I(io_end->inode);\n\tspin_lock_irqsave(&ei->i_completed_io_lock, flags);\n\tlist_add_tail(&io_end->list, &ei->i_completed_io_list);\n\tspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\n\tiocb->private = NULL;\n}\n\nstatic void ext4_end_io_buffer_write(struct buffer_head *bh, int uptodate)\n{\n\text4_io_end_t *io_end = bh->b_private;\n\tstruct workqueue_struct *wq;\n\tstruct inode *inode;\n\tunsigned long flags;\n\n\tif (!test_clear_buffer_uninit(bh) || !io_end)\n\t\tgoto out;\n\n\tif (!(io_end->inode->i_sb->s_flags & MS_ACTIVE)) {\n\t\tprintk(\"sb umounted, discard end_io request for inode %lu\\n\",\n\t\t\tio_end->inode->i_ino);\n\t\text4_free_io_end(io_end);\n\t\tgoto out;\n\t}\n\n\tio_end->flag = EXT4_IO_UNWRITTEN;\n\tinode = io_end->inode;\n\n\t/* Add the io_end to per-inode completed io list*/\n\tspin_lock_irqsave(&EXT4_I(inode)->i_completed_io_lock, flags);\n\tlist_add_tail(&io_end->list, &EXT4_I(inode)->i_completed_io_list);\n\tspin_unlock_irqrestore(&EXT4_I(inode)->i_completed_io_lock, flags);\n\n\twq = EXT4_SB(inode->i_sb)->dio_unwritten_wq;\n\t/* queue the work to convert unwritten extents to written */\n\tqueue_work(wq, &io_end->work);\nout:\n\tbh->b_private = NULL;\n\tbh->b_end_io = NULL;\n\tclear_buffer_uninit(bh);\n\tend_buffer_async_write(bh, uptodate);\n}\n\nstatic int ext4_set_bh_endio(struct buffer_head *bh, struct inode *inode)\n{\n\text4_io_end_t *io_end;\n\tstruct page *page = bh->b_page;\n\tloff_t offset = (sector_t)page->index << PAGE_CACHE_SHIFT;\n\tsize_t size = bh->b_size;\n\nretry:\n\tio_end = ext4_init_io_end(inode, GFP_ATOMIC);\n\tif (!io_end) {\n\t\tif (printk_ratelimit())\n\t\t\tprintk(KERN_WARNING \"%s: allocation fail\\n\", __func__);\n\t\tschedule();\n\t\tgoto retry;\n\t}\n\tio_end->offset = offset;\n\tio_end->size = size;\n\t/*\n\t * We need to hold a reference to the page to make sure it\n\t * doesn't get evicted before ext4_end_io_work() has a chance\n\t * to convert the extent from written to unwritten.\n\t */\n\tio_end->page = page;\n\tget_page(io_end->page);\n\n\tbh->b_private = io_end;\n\tbh->b_end_io = ext4_end_io_buffer_write;\n\treturn 0;\n}\n\n/*\n * For ext4 extent files, ext4 will do direct-io write to holes,\n * preallocated extents, and those write extend the file, no need to\n * fall back to buffered IO.\n *\n * For holes, we fallocate those blocks, mark them as unintialized\n * If those blocks were preallocated, we mark sure they are splited, but\n * still keep the range to write as unintialized.\n *\n * The unwrritten extents will be converted to written when DIO is completed.\n * For async direct IO, since the IO may still pending when return, we\n * set up an end_io call back function, which will do the convertion\n * when async direct IO completed.\n *\n * If the O_DIRECT write will extend the file then add this inode to the\n * orphan list.  So recovery will truncate it back to the original size\n * if the machine crashes during the write.\n *\n */\nstatic ssize_t ext4_ext_direct_IO(int rw, struct kiocb *iocb,\n\t\t\t      const struct iovec *iov, loff_t offset,\n\t\t\t      unsigned long nr_segs)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\tssize_t ret;\n\tsize_t count = iov_length(iov, nr_segs);\n\n\tloff_t final_size = offset + count;\n\tif (rw == WRITE && final_size <= inode->i_size) {\n\t\t/*\n \t\t * We could direct write to holes and fallocate.\n\t\t *\n \t\t * Allocated blocks to fill the hole are marked as uninitialized\n \t\t * to prevent paralel buffered read to expose the stale data\n \t\t * before DIO complete the data IO.\n\t\t *\n \t\t * As to previously fallocated extents, ext4 get_block\n \t\t * will just simply mark the buffer mapped but still\n \t\t * keep the extents uninitialized.\n \t\t *\n\t\t * for non AIO case, we will convert those unwritten extents\n\t\t * to written after return back from blockdev_direct_IO.\n\t\t *\n\t\t * for async DIO, the conversion needs to be defered when\n\t\t * the IO is completed. The ext4 end_io callback function\n\t\t * will be called to take care of the conversion work.\n\t\t * Here for async case, we allocate an io_end structure to\n\t\t * hook to the iocb.\n \t\t */\n\t\tiocb->private = NULL;\n\t\tEXT4_I(inode)->cur_aio_dio = NULL;\n\t\tif (!is_sync_kiocb(iocb)) {\n\t\t\tiocb->private = ext4_init_io_end(inode, GFP_NOFS);\n\t\t\tif (!iocb->private)\n\t\t\t\treturn -ENOMEM;\n\t\t\t/*\n\t\t\t * we save the io structure for current async\n\t\t\t * direct IO, so that later ext4_get_blocks()\n\t\t\t * could flag the io structure whether there\n\t\t\t * is a unwritten extents needs to be converted\n\t\t\t * when IO is completed.\n\t\t\t */\n\t\t\tEXT4_I(inode)->cur_aio_dio = iocb->private;\n\t\t}\n\n\t\tret = blockdev_direct_IO(rw, iocb, inode,\n\t\t\t\t\t inode->i_sb->s_bdev, iov,\n\t\t\t\t\t offset, nr_segs,\n\t\t\t\t\t ext4_get_block_write,\n\t\t\t\t\t ext4_end_io_dio);\n\t\tif (iocb->private)\n\t\t\tEXT4_I(inode)->cur_aio_dio = NULL;\n\t\t/*\n\t\t * The io_end structure takes a reference to the inode,\n\t\t * that structure needs to be destroyed and the\n\t\t * reference to the inode need to be dropped, when IO is\n\t\t * complete, even with 0 byte write, or failed.\n\t\t *\n\t\t * In the successful AIO DIO case, the io_end structure will be\n\t\t * desctroyed and the reference to the inode will be dropped\n\t\t * after the end_io call back function is called.\n\t\t *\n\t\t * In the case there is 0 byte write, or error case, since\n\t\t * VFS direct IO won't invoke the end_io call back function,\n\t\t * we need to free the end_io structure here.\n\t\t */\n\t\tif (ret != -EIOCBQUEUED && ret <= 0 && iocb->private) {\n\t\t\text4_free_io_end(iocb->private);\n\t\t\tiocb->private = NULL;\n\t\t} else if (ret > 0 && ext4_test_inode_state(inode,\n\t\t\t\t\t\tEXT4_STATE_DIO_UNWRITTEN)) {\n\t\t\tint err;\n\t\t\t/*\n\t\t\t * for non AIO case, since the IO is already\n\t\t\t * completed, we could do the convertion right here\n\t\t\t */\n\t\t\terr = ext4_convert_unwritten_extents(inode,\n\t\t\t\t\t\t\t     offset, ret);\n\t\t\tif (err < 0)\n\t\t\t\tret = err;\n\t\t\text4_clear_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\t}\n\t\treturn ret;\n\t}\n\n\t/* for write the the end of file case, we fall back to old way */\n\treturn ext4_ind_direct_IO(rw, iocb, iov, offset, nr_segs);\n}\n\nstatic ssize_t ext4_direct_IO(int rw, struct kiocb *iocb,\n\t\t\t      const struct iovec *iov, loff_t offset,\n\t\t\t      unsigned long nr_segs)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\n\tif (EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL)\n\t\treturn ext4_ext_direct_IO(rw, iocb, iov, offset, nr_segs);\n\n\treturn ext4_ind_direct_IO(rw, iocb, iov, offset, nr_segs);\n}\n\n/*\n * Pages can be marked dirty completely asynchronously from ext4's journalling\n * activity.  By filemap_sync_pte(), try_to_unmap_one(), etc.  We cannot do\n * much here because ->set_page_dirty is called under VFS locks.  The page is\n * not necessarily locked.\n *\n * We cannot just dirty the page and leave attached buffers clean, because the\n * buffers' dirty state is \"definitive\".  We cannot just set the buffers dirty\n * or jbddirty because all the journalling code will explode.\n *\n * So what we do is to mark the page \"pending dirty\" and next time writepage\n * is called, propagate that into the buffers appropriately.\n */\nstatic int ext4_journalled_set_page_dirty(struct page *page)\n{\n\tSetPageChecked(page);\n\treturn __set_page_dirty_nobuffers(page);\n}\n\nstatic const struct address_space_operations ext4_ordered_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.sync_page\t\t= block_sync_page,\n\t.write_begin\t\t= ext4_write_begin,\n\t.write_end\t\t= ext4_ordered_write_end,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= ext4_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_writeback_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.sync_page\t\t= block_sync_page,\n\t.write_begin\t\t= ext4_write_begin,\n\t.write_end\t\t= ext4_writeback_write_end,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= ext4_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_journalled_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.sync_page\t\t= block_sync_page,\n\t.write_begin\t\t= ext4_write_begin,\n\t.write_end\t\t= ext4_journalled_write_end,\n\t.set_page_dirty\t\t= ext4_journalled_set_page_dirty,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nstatic const struct address_space_operations ext4_da_aops = {\n\t.readpage\t\t= ext4_readpage,\n\t.readpages\t\t= ext4_readpages,\n\t.writepage\t\t= ext4_writepage,\n\t.writepages\t\t= ext4_da_writepages,\n\t.sync_page\t\t= block_sync_page,\n\t.write_begin\t\t= ext4_da_write_begin,\n\t.write_end\t\t= ext4_da_write_end,\n\t.bmap\t\t\t= ext4_bmap,\n\t.invalidatepage\t\t= ext4_da_invalidatepage,\n\t.releasepage\t\t= ext4_releasepage,\n\t.direct_IO\t\t= ext4_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n\nvoid ext4_set_aops(struct inode *inode)\n{\n\tif (ext4_should_order_data(inode) &&\n\t\ttest_opt(inode->i_sb, DELALLOC))\n\t\tinode->i_mapping->a_ops = &ext4_da_aops;\n\telse if (ext4_should_order_data(inode))\n\t\tinode->i_mapping->a_ops = &ext4_ordered_aops;\n\telse if (ext4_should_writeback_data(inode) &&\n\t\t test_opt(inode->i_sb, DELALLOC))\n\t\tinode->i_mapping->a_ops = &ext4_da_aops;\n\telse if (ext4_should_writeback_data(inode))\n\t\tinode->i_mapping->a_ops = &ext4_writeback_aops;\n\telse\n\t\tinode->i_mapping->a_ops = &ext4_journalled_aops;\n}\n\n/*\n * ext4_block_truncate_page() zeroes out a mapping from file offset `from'\n * up to the end of the block which corresponds to `from'.\n * This required during truncate. We need to physically zero the tail end\n * of that block so it doesn't yield old data if the file is later grown.\n */\nint ext4_block_truncate_page(handle_t *handle,\n\t\tstruct address_space *mapping, loff_t from)\n{\n\text4_fsblk_t index = from >> PAGE_CACHE_SHIFT;\n\tunsigned offset = from & (PAGE_CACHE_SIZE-1);\n\tunsigned blocksize, length, pos;\n\text4_lblk_t iblock;\n\tstruct inode *inode = mapping->host;\n\tstruct buffer_head *bh;\n\tstruct page *page;\n\tint err = 0;\n\n\tpage = find_or_create_page(mapping, from >> PAGE_CACHE_SHIFT,\n\t\t\t\t   mapping_gfp_mask(mapping) & ~__GFP_FS);\n\tif (!page)\n\t\treturn -EINVAL;\n\n\tblocksize = inode->i_sb->s_blocksize;\n\tlength = blocksize - (offset & (blocksize - 1));\n\tiblock = index << (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits);\n\n\t/*\n\t * For \"nobh\" option,  we can only work if we don't need to\n\t * read-in the page - otherwise we create buffers to do the IO.\n\t */\n\tif (!page_has_buffers(page) && test_opt(inode->i_sb, NOBH) &&\n\t     ext4_should_writeback_data(inode) && PageUptodate(page)) {\n\t\tzero_user(page, offset, length);\n\t\tset_page_dirty(page);\n\t\tgoto unlock;\n\t}\n\n\tif (!page_has_buffers(page))\n\t\tcreate_empty_buffers(page, blocksize, 0);\n\n\t/* Find the buffer that contains \"offset\" */\n\tbh = page_buffers(page);\n\tpos = blocksize;\n\twhile (offset >= pos) {\n\t\tbh = bh->b_this_page;\n\t\tiblock++;\n\t\tpos += blocksize;\n\t}\n\n\terr = 0;\n\tif (buffer_freed(bh)) {\n\t\tBUFFER_TRACE(bh, \"freed: skip\");\n\t\tgoto unlock;\n\t}\n\n\tif (!buffer_mapped(bh)) {\n\t\tBUFFER_TRACE(bh, \"unmapped\");\n\t\text4_get_block(inode, iblock, bh, 0);\n\t\t/* unmapped? It's a hole - nothing to do */\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tBUFFER_TRACE(bh, \"still unmapped\");\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\t/* Ok, it's mapped. Make sure it's up-to-date */\n\tif (PageUptodate(page))\n\t\tset_buffer_uptodate(bh);\n\n\tif (!buffer_uptodate(bh)) {\n\t\terr = -EIO;\n\t\tll_rw_block(READ, 1, &bh);\n\t\twait_on_buffer(bh);\n\t\t/* Uhhuh. Read error. Complain and punt. */\n\t\tif (!buffer_uptodate(bh))\n\t\t\tgoto unlock;\n\t}\n\n\tif (ext4_should_journal_data(inode)) {\n\t\tBUFFER_TRACE(bh, \"get write access\");\n\t\terr = ext4_journal_get_write_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t}\n\n\tzero_user(page, offset, length);\n\n\tBUFFER_TRACE(bh, \"zeroed end of block\");\n\n\terr = 0;\n\tif (ext4_should_journal_data(inode)) {\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t} else {\n\t\tif (ext4_should_order_data(inode))\n\t\t\terr = ext4_jbd2_file_inode(handle, inode);\n\t\tmark_buffer_dirty(bh);\n\t}\n\nunlock:\n\tunlock_page(page);\n\tpage_cache_release(page);\n\treturn err;\n}\n\n/*\n * Probably it should be a library function... search for first non-zero word\n * or memcmp with zero_page, whatever is better for particular architecture.\n * Linus?\n */\nstatic inline int all_zeroes(__le32 *p, __le32 *q)\n{\n\twhile (p < q)\n\t\tif (*p++)\n\t\t\treturn 0;\n\treturn 1;\n}\n\n/**\n *\text4_find_shared - find the indirect blocks for partial truncation.\n *\t@inode:\t  inode in question\n *\t@depth:\t  depth of the affected branch\n *\t@offsets: offsets of pointers in that branch (see ext4_block_to_path)\n *\t@chain:\t  place to store the pointers to partial indirect blocks\n *\t@top:\t  place to the (detached) top of branch\n *\n *\tThis is a helper function used by ext4_truncate().\n *\n *\tWhen we do truncate() we may have to clean the ends of several\n *\tindirect blocks but leave the blocks themselves alive. Block is\n *\tpartially truncated if some data below the new i_size is refered\n *\tfrom it (and it is on the path to the first completely truncated\n *\tdata block, indeed).  We have to free the top of that path along\n *\twith everything to the right of the path. Since no allocation\n *\tpast the truncation point is possible until ext4_truncate()\n *\tfinishes, we may safely do the latter, but top of branch may\n *\trequire special attention - pageout below the truncation point\n *\tmight try to populate it.\n *\n *\tWe atomically detach the top of branch from the tree, store the\n *\tblock number of its root in *@top, pointers to buffer_heads of\n *\tpartially truncated blocks - in @chain[].bh and pointers to\n *\ttheir last elements that should not be removed - in\n *\t@chain[].p. Return value is the pointer to last filled element\n *\tof @chain.\n *\n *\tThe work left to caller to do the actual freeing of subtrees:\n *\t\ta) free the subtree starting from *@top\n *\t\tb) free the subtrees whose roots are stored in\n *\t\t\t(@chain[i].p+1 .. end of @chain[i].bh->b_data)\n *\t\tc) free the subtrees growing from the inode past the @chain[0].\n *\t\t\t(no partially truncated stuff there).  */\n\nstatic Indirect *ext4_find_shared(struct inode *inode, int depth,\n\t\t\t\t  ext4_lblk_t offsets[4], Indirect chain[4],\n\t\t\t\t  __le32 *top)\n{\n\tIndirect *partial, *p;\n\tint k, err;\n\n\t*top = 0;\n\t/* Make k index the deepest non-null offset + 1 */\n\tfor (k = depth; k > 1 && !offsets[k-1]; k--)\n\t\t;\n\tpartial = ext4_get_branch(inode, k, offsets, chain, &err);\n\t/* Writer: pointers */\n\tif (!partial)\n\t\tpartial = chain + k-1;\n\t/*\n\t * If the branch acquired continuation since we've looked at it -\n\t * fine, it should all survive and (new) top doesn't belong to us.\n\t */\n\tif (!partial->key && *partial->p)\n\t\t/* Writer: end */\n\t\tgoto no_top;\n\tfor (p = partial; (p > chain) && all_zeroes((__le32 *) p->bh->b_data, p->p); p--)\n\t\t;\n\t/*\n\t * OK, we've found the last block that must survive. The rest of our\n\t * branch should be detached before unlocking. However, if that rest\n\t * of branch is all ours and does not grow immediately from the inode\n\t * it's easier to cheat and just decrement partial->p.\n\t */\n\tif (p == chain + k - 1 && p > chain) {\n\t\tp->p--;\n\t} else {\n\t\t*top = *p->p;\n\t\t/* Nope, don't do this in ext4.  Must leave the tree intact */\n#if 0\n\t\t*p->p = 0;\n#endif\n\t}\n\t/* Writer: end */\n\n\twhile (partial > p) {\n\t\tbrelse(partial->bh);\n\t\tpartial--;\n\t}\nno_top:\n\treturn partial;\n}\n\n/*\n * Zero a number of block pointers in either an inode or an indirect block.\n * If we restart the transaction we must again get write access to the\n * indirect block for further modification.\n *\n * We release `count' blocks on disk, but (last - first) may be greater\n * than `count' because there can be holes in there.\n */\nstatic int ext4_clear_blocks(handle_t *handle, struct inode *inode,\n\t\t\t     struct buffer_head *bh,\n\t\t\t     ext4_fsblk_t block_to_free,\n\t\t\t     unsigned long count, __le32 *first,\n\t\t\t     __le32 *last)\n{\n\t__le32 *p;\n\tint\tflags = EXT4_FREE_BLOCKS_FORGET | EXT4_FREE_BLOCKS_VALIDATED;\n\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\tflags |= EXT4_FREE_BLOCKS_METADATA;\n\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), block_to_free,\n\t\t\t\t   count)) {\n\t\text4_error(inode->i_sb, \"inode #%lu: \"\n\t\t\t   \"attempt to clear blocks %llu len %lu, invalid\",\n\t\t\t   inode->i_ino, (unsigned long long) block_to_free,\n\t\t\t   count);\n\t\treturn 1;\n\t}\n\n\tif (try_to_extend_transaction(handle, inode)) {\n\t\tif (bh) {\n\t\t\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\t\t\text4_handle_dirty_metadata(handle, inode, bh);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\text4_truncate_restart_trans(handle, inode,\n\t\t\t\t\t    blocks_for_truncate(inode));\n\t\tif (bh) {\n\t\t\tBUFFER_TRACE(bh, \"retaking write access\");\n\t\t\text4_journal_get_write_access(handle, bh);\n\t\t}\n\t}\n\n\tfor (p = first; p < last; p++)\n\t\t*p = 0;\n\n\text4_free_blocks(handle, inode, 0, block_to_free, count, flags);\n\treturn 0;\n}\n\n/**\n * ext4_free_data - free a list of data blocks\n * @handle:\thandle for this transaction\n * @inode:\tinode we are dealing with\n * @this_bh:\tindirect buffer_head which contains *@first and *@last\n * @first:\tarray of block numbers\n * @last:\tpoints immediately past the end of array\n *\n * We are freeing all blocks refered from that array (numbers are stored as\n * little-endian 32-bit) and updating @inode->i_blocks appropriately.\n *\n * We accumulate contiguous runs of blocks to free.  Conveniently, if these\n * blocks are contiguous then releasing them at one time will only affect one\n * or two bitmap blocks (+ group descriptor(s) and superblock) and we won't\n * actually use a lot of journal space.\n *\n * @this_bh will be %NULL if @first and @last point into the inode's direct\n * block pointers.\n */\nstatic void ext4_free_data(handle_t *handle, struct inode *inode,\n\t\t\t   struct buffer_head *this_bh,\n\t\t\t   __le32 *first, __le32 *last)\n{\n\text4_fsblk_t block_to_free = 0;    /* Starting block # of a run */\n\tunsigned long count = 0;\t    /* Number of blocks in the run */\n\t__le32 *block_to_free_p = NULL;\t    /* Pointer into inode/ind\n\t\t\t\t\t       corresponding to\n\t\t\t\t\t       block_to_free */\n\text4_fsblk_t nr;\t\t    /* Current block # */\n\t__le32 *p;\t\t\t    /* Pointer into inode/ind\n\t\t\t\t\t       for current block */\n\tint err;\n\n\tif (this_bh) {\t\t\t\t/* For indirect block */\n\t\tBUFFER_TRACE(this_bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, this_bh);\n\t\t/* Important: if we can't update the indirect pointers\n\t\t * to the blocks, we can't free them. */\n\t\tif (err)\n\t\t\treturn;\n\t}\n\n\tfor (p = first; p < last; p++) {\n\t\tnr = le32_to_cpu(*p);\n\t\tif (nr) {\n\t\t\t/* accumulate blocks to free if they're contiguous */\n\t\t\tif (count == 0) {\n\t\t\t\tblock_to_free = nr;\n\t\t\t\tblock_to_free_p = p;\n\t\t\t\tcount = 1;\n\t\t\t} else if (nr == block_to_free + count) {\n\t\t\t\tcount++;\n\t\t\t} else {\n\t\t\t\tif (ext4_clear_blocks(handle, inode, this_bh,\n\t\t\t\t\t\t      block_to_free, count,\n\t\t\t\t\t\t      block_to_free_p, p))\n\t\t\t\t\tbreak;\n\t\t\t\tblock_to_free = nr;\n\t\t\t\tblock_to_free_p = p;\n\t\t\t\tcount = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (count > 0)\n\t\text4_clear_blocks(handle, inode, this_bh, block_to_free,\n\t\t\t\t  count, block_to_free_p, p);\n\n\tif (this_bh) {\n\t\tBUFFER_TRACE(this_bh, \"call ext4_handle_dirty_metadata\");\n\n\t\t/*\n\t\t * The buffer head should have an attached journal head at this\n\t\t * point. However, if the data is corrupted and an indirect\n\t\t * block pointed to itself, it would have been detached when\n\t\t * the block was cleared. Check for this instead of OOPSing.\n\t\t */\n\t\tif ((EXT4_JOURNAL(inode) == NULL) || bh2jh(this_bh))\n\t\t\text4_handle_dirty_metadata(handle, inode, this_bh);\n\t\telse\n\t\t\text4_error(inode->i_sb,\n\t\t\t\t   \"circular indirect block detected, \"\n\t\t\t\t   \"inode=%lu, block=%llu\",\n\t\t\t\t   inode->i_ino,\n\t\t\t\t   (unsigned long long) this_bh->b_blocknr);\n\t}\n}\n\n/**\n *\text4_free_branches - free an array of branches\n *\t@handle: JBD handle for this transaction\n *\t@inode:\tinode we are dealing with\n *\t@parent_bh: the buffer_head which contains *@first and *@last\n *\t@first:\tarray of block numbers\n *\t@last:\tpointer immediately past the end of array\n *\t@depth:\tdepth of the branches to free\n *\n *\tWe are freeing all blocks refered from these branches (numbers are\n *\tstored as little-endian 32-bit) and updating @inode->i_blocks\n *\tappropriately.\n */\nstatic void ext4_free_branches(handle_t *handle, struct inode *inode,\n\t\t\t       struct buffer_head *parent_bh,\n\t\t\t       __le32 *first, __le32 *last, int depth)\n{\n\text4_fsblk_t nr;\n\t__le32 *p;\n\n\tif (ext4_handle_is_aborted(handle))\n\t\treturn;\n\n\tif (depth--) {\n\t\tstruct buffer_head *bh;\n\t\tint addr_per_block = EXT4_ADDR_PER_BLOCK(inode->i_sb);\n\t\tp = last;\n\t\twhile (--p >= first) {\n\t\t\tnr = le32_to_cpu(*p);\n\t\t\tif (!nr)\n\t\t\t\tcontinue;\t\t/* A hole */\n\n\t\t\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb),\n\t\t\t\t\t\t   nr, 1)) {\n\t\t\t\text4_error(inode->i_sb,\n\t\t\t\t\t   \"indirect mapped block in inode \"\n\t\t\t\t\t   \"#%lu invalid (level %d, blk #%lu)\",\n\t\t\t\t\t   inode->i_ino, depth,\n\t\t\t\t\t   (unsigned long) nr);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Go read the buffer for the next level down */\n\t\t\tbh = sb_bread(inode->i_sb, nr);\n\n\t\t\t/*\n\t\t\t * A read failure? Report error and clear slot\n\t\t\t * (should be rare).\n\t\t\t */\n\t\t\tif (!bh) {\n\t\t\t\text4_error(inode->i_sb,\n\t\t\t\t\t   \"Read failure, inode=%lu, block=%llu\",\n\t\t\t\t\t   inode->i_ino, nr);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* This zaps the entire block.  Bottom up. */\n\t\t\tBUFFER_TRACE(bh, \"free child branches\");\n\t\t\text4_free_branches(handle, inode, bh,\n\t\t\t\t\t(__le32 *) bh->b_data,\n\t\t\t\t\t(__le32 *) bh->b_data + addr_per_block,\n\t\t\t\t\tdepth);\n\n\t\t\t/*\n\t\t\t * We've probably journalled the indirect block several\n\t\t\t * times during the truncate.  But it's no longer\n\t\t\t * needed and we now drop it from the transaction via\n\t\t\t * jbd2_journal_revoke().\n\t\t\t *\n\t\t\t * That's easy if it's exclusively part of this\n\t\t\t * transaction.  But if it's part of the committing\n\t\t\t * transaction then jbd2_journal_forget() will simply\n\t\t\t * brelse() it.  That means that if the underlying\n\t\t\t * block is reallocated in ext4_get_block(),\n\t\t\t * unmap_underlying_metadata() will find this block\n\t\t\t * and will try to get rid of it.  damn, damn.\n\t\t\t *\n\t\t\t * If this block has already been committed to the\n\t\t\t * journal, a revoke record will be written.  And\n\t\t\t * revoke records must be emitted *before* clearing\n\t\t\t * this block's bit in the bitmaps.\n\t\t\t */\n\t\t\text4_forget(handle, 1, inode, bh, bh->b_blocknr);\n\n\t\t\t/*\n\t\t\t * Everything below this this pointer has been\n\t\t\t * released.  Now let this top-of-subtree go.\n\t\t\t *\n\t\t\t * We want the freeing of this indirect block to be\n\t\t\t * atomic in the journal with the updating of the\n\t\t\t * bitmap block which owns it.  So make some room in\n\t\t\t * the journal.\n\t\t\t *\n\t\t\t * We zero the parent pointer *after* freeing its\n\t\t\t * pointee in the bitmaps, so if extend_transaction()\n\t\t\t * for some reason fails to put the bitmap changes and\n\t\t\t * the release into the same transaction, recovery\n\t\t\t * will merely complain about releasing a free block,\n\t\t\t * rather than leaking blocks.\n\t\t\t */\n\t\t\tif (ext4_handle_is_aborted(handle))\n\t\t\t\treturn;\n\t\t\tif (try_to_extend_transaction(handle, inode)) {\n\t\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\t\text4_truncate_restart_trans(handle, inode,\n\t\t\t\t\t    blocks_for_truncate(inode));\n\t\t\t}\n\n\t\t\text4_free_blocks(handle, inode, 0, nr, 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\n\t\t\tif (parent_bh) {\n\t\t\t\t/*\n\t\t\t\t * The block which we have just freed is\n\t\t\t\t * pointed to by an indirect block: journal it\n\t\t\t\t */\n\t\t\t\tBUFFER_TRACE(parent_bh, \"get_write_access\");\n\t\t\t\tif (!ext4_journal_get_write_access(handle,\n\t\t\t\t\t\t\t\t   parent_bh)){\n\t\t\t\t\t*p = 0;\n\t\t\t\t\tBUFFER_TRACE(parent_bh,\n\t\t\t\t\t\"call ext4_handle_dirty_metadata\");\n\t\t\t\t\text4_handle_dirty_metadata(handle,\n\t\t\t\t\t\t\t\t   inode,\n\t\t\t\t\t\t\t\t   parent_bh);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\t/* We have reached the bottom of the tree. */\n\t\tBUFFER_TRACE(parent_bh, \"free data blocks\");\n\t\text4_free_data(handle, inode, parent_bh, first, last);\n\t}\n}\n\nint ext4_can_truncate(struct inode *inode)\n{\n\tif (IS_APPEND(inode) || IS_IMMUTABLE(inode))\n\t\treturn 0;\n\tif (S_ISREG(inode->i_mode))\n\t\treturn 1;\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn 1;\n\tif (S_ISLNK(inode->i_mode))\n\t\treturn !ext4_inode_is_fast_symlink(inode);\n\treturn 0;\n}\n\n/*\n * ext4_truncate()\n *\n * We block out ext4_get_block() block instantiations across the entire\n * transaction, and VFS/VM ensures that ext4_truncate() cannot run\n * simultaneously on behalf of the same inode.\n *\n * As we work through the truncate and commmit bits of it to the journal there\n * is one core, guiding principle: the file's tree must always be consistent on\n * disk.  We must be able to restart the truncate after a crash.\n *\n * The file's tree may be transiently inconsistent in memory (although it\n * probably isn't), but whenever we close off and commit a journal transaction,\n * the contents of (the filesystem + the journal) must be consistent and\n * restartable.  It's pretty simple, really: bottom up, right to left (although\n * left-to-right works OK too).\n *\n * Note that at recovery time, journal replay occurs *before* the restart of\n * truncate against the orphan inode list.\n *\n * The committed inode has the new, desired i_size (which is the same as\n * i_disksize in this case).  After a crash, ext4_orphan_cleanup() will see\n * that this inode's truncate did not complete and it will again call\n * ext4_truncate() to have another go.  So there will be instantiated blocks\n * to the right of the truncation point in a crashed ext4 filesystem.  But\n * that's fine - as long as they are linked from the inode, the post-crash\n * ext4_truncate() run will find them and release them.\n */\nvoid ext4_truncate(struct inode *inode)\n{\n\thandle_t *handle;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\t__le32 *i_data = ei->i_data;\n\tint addr_per_block = EXT4_ADDR_PER_BLOCK(inode->i_sb);\n\tstruct address_space *mapping = inode->i_mapping;\n\text4_lblk_t offsets[4];\n\tIndirect chain[4];\n\tIndirect *partial;\n\t__le32 nr = 0;\n\tint n;\n\text4_lblk_t last_block;\n\tunsigned blocksize = inode->i_sb->s_blocksize;\n\n\tif (!ext4_can_truncate(inode))\n\t\treturn;\n\n\tEXT4_I(inode)->i_flags &= ~EXT4_EOFBLOCKS_FL;\n\n\tif (inode->i_size == 0 && !test_opt(inode->i_sb, NO_AUTO_DA_ALLOC))\n\t\text4_set_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE);\n\n\tif (EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL) {\n\t\text4_ext_truncate(inode);\n\t\treturn;\n\t}\n\n\thandle = start_transaction(inode);\n\tif (IS_ERR(handle))\n\t\treturn;\t\t/* AKPM: return what? */\n\n\tlast_block = (inode->i_size + blocksize-1)\n\t\t\t\t\t>> EXT4_BLOCK_SIZE_BITS(inode->i_sb);\n\n\tif (inode->i_size & (blocksize - 1))\n\t\tif (ext4_block_truncate_page(handle, mapping, inode->i_size))\n\t\t\tgoto out_stop;\n\n\tn = ext4_block_to_path(inode, last_block, offsets, NULL);\n\tif (n == 0)\n\t\tgoto out_stop;\t/* error */\n\n\t/*\n\t * OK.  This truncate is going to happen.  We add the inode to the\n\t * orphan list, so that if this truncate spans multiple transactions,\n\t * and we crash, we will resume the truncate when the filesystem\n\t * recovers.  It also marks the inode dirty, to catch the new size.\n\t *\n\t * Implication: the file must always be in a sane, consistent\n\t * truncatable state while each transaction commits.\n\t */\n\tif (ext4_orphan_add(handle, inode))\n\t\tgoto out_stop;\n\n\t/*\n\t * From here we block out all ext4_get_block() callers who want to\n\t * modify the block allocation tree.\n\t */\n\tdown_write(&ei->i_data_sem);\n\n\text4_discard_preallocations(inode);\n\n\t/*\n\t * The orphan list entry will now protect us from any crash which\n\t * occurs before the truncate completes, so it is now safe to propagate\n\t * the new, shorter inode size (held for now in i_size) into the\n\t * on-disk inode. We do this via i_disksize, which is the value which\n\t * ext4 *really* writes onto the disk inode.\n\t */\n\tei->i_disksize = inode->i_size;\n\n\tif (n == 1) {\t\t/* direct blocks */\n\t\text4_free_data(handle, inode, NULL, i_data+offsets[0],\n\t\t\t       i_data + EXT4_NDIR_BLOCKS);\n\t\tgoto do_indirects;\n\t}\n\n\tpartial = ext4_find_shared(inode, n, offsets, chain, &nr);\n\t/* Kill the top of shared branch (not detached) */\n\tif (nr) {\n\t\tif (partial == chain) {\n\t\t\t/* Shared branch grows from the inode */\n\t\t\text4_free_branches(handle, inode, NULL,\n\t\t\t\t\t   &nr, &nr+1, (chain+n-1) - partial);\n\t\t\t*partial->p = 0;\n\t\t\t/*\n\t\t\t * We mark the inode dirty prior to restart,\n\t\t\t * and prior to stop.  No need for it here.\n\t\t\t */\n\t\t} else {\n\t\t\t/* Shared branch grows from an indirect block */\n\t\t\tBUFFER_TRACE(partial->bh, \"get_write_access\");\n\t\t\text4_free_branches(handle, inode, partial->bh,\n\t\t\t\t\tpartial->p,\n\t\t\t\t\tpartial->p+1, (chain+n-1) - partial);\n\t\t}\n\t}\n\t/* Clear the ends of indirect blocks on the shared branch */\n\twhile (partial > chain) {\n\t\text4_free_branches(handle, inode, partial->bh, partial->p + 1,\n\t\t\t\t   (__le32*)partial->bh->b_data+addr_per_block,\n\t\t\t\t   (chain+n-1) - partial);\n\t\tBUFFER_TRACE(partial->bh, \"call brelse\");\n\t\tbrelse(partial->bh);\n\t\tpartial--;\n\t}\ndo_indirects:\n\t/* Kill the remaining (whole) subtrees */\n\tswitch (offsets[0]) {\n\tdefault:\n\t\tnr = i_data[EXT4_IND_BLOCK];\n\t\tif (nr) {\n\t\t\text4_free_branches(handle, inode, NULL, &nr, &nr+1, 1);\n\t\t\ti_data[EXT4_IND_BLOCK] = 0;\n\t\t}\n\tcase EXT4_IND_BLOCK:\n\t\tnr = i_data[EXT4_DIND_BLOCK];\n\t\tif (nr) {\n\t\t\text4_free_branches(handle, inode, NULL, &nr, &nr+1, 2);\n\t\t\ti_data[EXT4_DIND_BLOCK] = 0;\n\t\t}\n\tcase EXT4_DIND_BLOCK:\n\t\tnr = i_data[EXT4_TIND_BLOCK];\n\t\tif (nr) {\n\t\t\text4_free_branches(handle, inode, NULL, &nr, &nr+1, 3);\n\t\t\ti_data[EXT4_TIND_BLOCK] = 0;\n\t\t}\n\tcase EXT4_TIND_BLOCK:\n\t\t;\n\t}\n\n\tup_write(&ei->i_data_sem);\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\n\t/*\n\t * In a multi-transaction truncate, we only make the final transaction\n\t * synchronous\n\t */\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\nout_stop:\n\t/*\n\t * If this was a simple ftruncate(), and the file will remain alive\n\t * then we need to clear up the orphan record which we created above.\n\t * However, if this was a real unlink then we were called by\n\t * ext4_delete_inode(), and we allow that function to clean up the\n\t * orphan info for us.\n\t */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(handle, inode);\n\n\text4_journal_stop(handle);\n}\n\n/*\n * ext4_get_inode_loc returns with an extra refcount against the inode's\n * underlying buffer_head on success. If 'in_mem' is true, we have all\n * data in memory that is needed to recreate the on-disk version of this\n * inode.\n */\nstatic int __ext4_get_inode_loc(struct inode *inode,\n\t\t\t\tstruct ext4_iloc *iloc, int in_mem)\n{\n\tstruct ext4_group_desc\t*gdp;\n\tstruct buffer_head\t*bh;\n\tstruct super_block\t*sb = inode->i_sb;\n\text4_fsblk_t\t\tblock;\n\tint\t\t\tinodes_per_block, inode_offset;\n\n\tiloc->bh = NULL;\n\tif (!ext4_valid_inum(sb, inode->i_ino))\n\t\treturn -EIO;\n\n\tiloc->block_group = (inode->i_ino - 1) / EXT4_INODES_PER_GROUP(sb);\n\tgdp = ext4_get_group_desc(sb, iloc->block_group, NULL);\n\tif (!gdp)\n\t\treturn -EIO;\n\n\t/*\n\t * Figure out the offset within the block group inode table\n\t */\n\tinodes_per_block = (EXT4_BLOCK_SIZE(sb) / EXT4_INODE_SIZE(sb));\n\tinode_offset = ((inode->i_ino - 1) %\n\t\t\tEXT4_INODES_PER_GROUP(sb));\n\tblock = ext4_inode_table(sb, gdp) + (inode_offset / inodes_per_block);\n\tiloc->offset = (inode_offset % inodes_per_block) * EXT4_INODE_SIZE(sb);\n\n\tbh = sb_getblk(sb, block);\n\tif (!bh) {\n\t\text4_error(sb, \"unable to read inode block - \"\n\t\t\t   \"inode=%lu, block=%llu\", inode->i_ino, block);\n\t\treturn -EIO;\n\t}\n\tif (!buffer_uptodate(bh)) {\n\t\tlock_buffer(bh);\n\n\t\t/*\n\t\t * If the buffer has the write error flag, we have failed\n\t\t * to write out another inode in the same block.  In this\n\t\t * case, we don't have to read the block because we may\n\t\t * read the old inode data successfully.\n\t\t */\n\t\tif (buffer_write_io_error(bh) && !buffer_uptodate(bh))\n\t\t\tset_buffer_uptodate(bh);\n\n\t\tif (buffer_uptodate(bh)) {\n\t\t\t/* someone brought it uptodate while we waited */\n\t\t\tunlock_buffer(bh);\n\t\t\tgoto has_buffer;\n\t\t}\n\n\t\t/*\n\t\t * If we have all information of the inode in memory and this\n\t\t * is the only valid inode in the block, we need not read the\n\t\t * block.\n\t\t */\n\t\tif (in_mem) {\n\t\t\tstruct buffer_head *bitmap_bh;\n\t\t\tint i, start;\n\n\t\t\tstart = inode_offset & ~(inodes_per_block - 1);\n\n\t\t\t/* Is the inode bitmap in cache? */\n\t\t\tbitmap_bh = sb_getblk(sb, ext4_inode_bitmap(sb, gdp));\n\t\t\tif (!bitmap_bh)\n\t\t\t\tgoto make_io;\n\n\t\t\t/*\n\t\t\t * If the inode bitmap isn't in cache then the\n\t\t\t * optimisation may end up performing two reads instead\n\t\t\t * of one, so skip it.\n\t\t\t */\n\t\t\tif (!buffer_uptodate(bitmap_bh)) {\n\t\t\t\tbrelse(bitmap_bh);\n\t\t\t\tgoto make_io;\n\t\t\t}\n\t\t\tfor (i = start; i < start + inodes_per_block; i++) {\n\t\t\t\tif (i == inode_offset)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (ext4_test_bit(i, bitmap_bh->b_data))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbrelse(bitmap_bh);\n\t\t\tif (i == start + inodes_per_block) {\n\t\t\t\t/* all other inodes are free, so skip I/O */\n\t\t\t\tmemset(bh->b_data, 0, bh->b_size);\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\tunlock_buffer(bh);\n\t\t\t\tgoto has_buffer;\n\t\t\t}\n\t\t}\n\nmake_io:\n\t\t/*\n\t\t * If we need to do any I/O, try to pre-readahead extra\n\t\t * blocks from the inode table.\n\t\t */\n\t\tif (EXT4_SB(sb)->s_inode_readahead_blks) {\n\t\t\text4_fsblk_t b, end, table;\n\t\t\tunsigned num;\n\n\t\t\ttable = ext4_inode_table(sb, gdp);\n\t\t\t/* s_inode_readahead_blks is always a power of 2 */\n\t\t\tb = block & ~(EXT4_SB(sb)->s_inode_readahead_blks-1);\n\t\t\tif (table > b)\n\t\t\t\tb = table;\n\t\t\tend = b + EXT4_SB(sb)->s_inode_readahead_blks;\n\t\t\tnum = EXT4_INODES_PER_GROUP(sb);\n\t\t\tif (EXT4_HAS_RO_COMPAT_FEATURE(sb,\n\t\t\t\t       EXT4_FEATURE_RO_COMPAT_GDT_CSUM))\n\t\t\t\tnum -= ext4_itable_unused_count(sb, gdp);\n\t\t\ttable += num / inodes_per_block;\n\t\t\tif (end > table)\n\t\t\t\tend = table;\n\t\t\twhile (b <= end)\n\t\t\t\tsb_breadahead(sb, b++);\n\t\t}\n\n\t\t/*\n\t\t * There are other valid inodes in the buffer, this inode\n\t\t * has in-inode xattrs, or we don't have this inode in memory.\n\t\t * Read the block from disk.\n\t\t */\n\t\tget_bh(bh);\n\t\tbh->b_end_io = end_buffer_read_sync;\n\t\tsubmit_bh(READ_META, bh);\n\t\twait_on_buffer(bh);\n\t\tif (!buffer_uptodate(bh)) {\n\t\t\text4_error(sb, \"unable to read inode block - inode=%lu,\"\n\t\t\t\t   \" block=%llu\", inode->i_ino, block);\n\t\t\tbrelse(bh);\n\t\t\treturn -EIO;\n\t\t}\n\t}\nhas_buffer:\n\tiloc->bh = bh;\n\treturn 0;\n}\n\nint ext4_get_inode_loc(struct inode *inode, struct ext4_iloc *iloc)\n{\n\t/* We have all inode data except xattrs in memory here. */\n\treturn __ext4_get_inode_loc(inode, iloc,\n\t\t!ext4_test_inode_state(inode, EXT4_STATE_XATTR));\n}\n\nvoid ext4_set_inode_flags(struct inode *inode)\n{\n\tunsigned int flags = EXT4_I(inode)->i_flags;\n\n\tinode->i_flags &= ~(S_SYNC|S_APPEND|S_IMMUTABLE|S_NOATIME|S_DIRSYNC);\n\tif (flags & EXT4_SYNC_FL)\n\t\tinode->i_flags |= S_SYNC;\n\tif (flags & EXT4_APPEND_FL)\n\t\tinode->i_flags |= S_APPEND;\n\tif (flags & EXT4_IMMUTABLE_FL)\n\t\tinode->i_flags |= S_IMMUTABLE;\n\tif (flags & EXT4_NOATIME_FL)\n\t\tinode->i_flags |= S_NOATIME;\n\tif (flags & EXT4_DIRSYNC_FL)\n\t\tinode->i_flags |= S_DIRSYNC;\n}\n\n/* Propagate flags from i_flags to EXT4_I(inode)->i_flags */\nvoid ext4_get_inode_flags(struct ext4_inode_info *ei)\n{\n\tunsigned int flags = ei->vfs_inode.i_flags;\n\n\tei->i_flags &= ~(EXT4_SYNC_FL|EXT4_APPEND_FL|\n\t\t\tEXT4_IMMUTABLE_FL|EXT4_NOATIME_FL|EXT4_DIRSYNC_FL);\n\tif (flags & S_SYNC)\n\t\tei->i_flags |= EXT4_SYNC_FL;\n\tif (flags & S_APPEND)\n\t\tei->i_flags |= EXT4_APPEND_FL;\n\tif (flags & S_IMMUTABLE)\n\t\tei->i_flags |= EXT4_IMMUTABLE_FL;\n\tif (flags & S_NOATIME)\n\t\tei->i_flags |= EXT4_NOATIME_FL;\n\tif (flags & S_DIRSYNC)\n\t\tei->i_flags |= EXT4_DIRSYNC_FL;\n}\n\nstatic blkcnt_t ext4_inode_blocks(struct ext4_inode *raw_inode,\n\t\t\t\t  struct ext4_inode_info *ei)\n{\n\tblkcnt_t i_blocks ;\n\tstruct inode *inode = &(ei->vfs_inode);\n\tstruct super_block *sb = inode->i_sb;\n\n\tif (EXT4_HAS_RO_COMPAT_FEATURE(sb,\n\t\t\t\tEXT4_FEATURE_RO_COMPAT_HUGE_FILE)) {\n\t\t/* we are using combined 48 bit field */\n\t\ti_blocks = ((u64)le16_to_cpu(raw_inode->i_blocks_high)) << 32 |\n\t\t\t\t\tle32_to_cpu(raw_inode->i_blocks_lo);\n\t\tif (ei->i_flags & EXT4_HUGE_FILE_FL) {\n\t\t\t/* i_blocks represent file system block size */\n\t\t\treturn i_blocks  << (inode->i_blkbits - 9);\n\t\t} else {\n\t\t\treturn i_blocks;\n\t\t}\n\t} else {\n\t\treturn le32_to_cpu(raw_inode->i_blocks_lo);\n\t}\n}\n\nstruct inode *ext4_iget(struct super_block *sb, unsigned long ino)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei;\n\tstruct inode *inode;\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\tlong ret;\n\tint block;\n\n\tinode = iget_locked(sb, ino);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (!(inode->i_state & I_NEW))\n\t\treturn inode;\n\n\tei = EXT4_I(inode);\n\tiloc.bh = 0;\n\n\tret = __ext4_get_inode_loc(inode, &iloc, 0);\n\tif (ret < 0)\n\t\tgoto bad_inode;\n\traw_inode = ext4_raw_inode(&iloc);\n\tinode->i_mode = le16_to_cpu(raw_inode->i_mode);\n\tinode->i_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);\n\tinode->i_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\tinode->i_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;\n\t\tinode->i_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;\n\t}\n\tinode->i_nlink = le16_to_cpu(raw_inode->i_links_count);\n\n\tei->i_state_flags = 0;\n\tei->i_dir_start_lookup = 0;\n\tei->i_dtime = le32_to_cpu(raw_inode->i_dtime);\n\t/* We now have enough fields to check if the inode was active or not.\n\t * This is needed because nfsd might try to access dead inodes\n\t * the test is that same one that e2fsck uses\n\t * NeilBrown 1999oct15\n\t */\n\tif (inode->i_nlink == 0) {\n\t\tif (inode->i_mode == 0 ||\n\t\t    !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS)) {\n\t\t\t/* this inode is deleted */\n\t\t\tret = -ESTALE;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\t/* The only unlinked inodes we let through here have\n\t\t * valid i_mode and are being read by the orphan\n\t\t * recovery code: that's fine, we're about to complete\n\t\t * the process of deleting those. */\n\t}\n\tei->i_flags = le32_to_cpu(raw_inode->i_flags);\n\tinode->i_blocks = ext4_inode_blocks(raw_inode, ei);\n\tei->i_file_acl = le32_to_cpu(raw_inode->i_file_acl_lo);\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT))\n\t\tei->i_file_acl |=\n\t\t\t((__u64)le16_to_cpu(raw_inode->i_file_acl_high)) << 32;\n\tinode->i_size = ext4_isize(raw_inode);\n\tei->i_disksize = inode->i_size;\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tinode->i_generation = le32_to_cpu(raw_inode->i_generation);\n\tei->i_block_group = iloc.block_group;\n\tei->i_last_alloc_group = ~0;\n\t/*\n\t * NOTE! The in-memory inode i_data array is in little-endian order\n\t * even on big-endian machines: we do NOT byteswap the block numbers!\n\t */\n\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\tei->i_data[block] = raw_inode->i_block[block];\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\n\t/*\n\t * Set transaction id's of transactions that have to be committed\n\t * to finish f[data]sync. We set them to currently running transaction\n\t * as we cannot be sure that the inode or some of its metadata isn't\n\t * part of the transaction - the inode could have been reclaimed and\n\t * now it is reread from disk.\n\t */\n\tif (journal) {\n\t\ttransaction_t *transaction;\n\t\ttid_t tid;\n\n\t\tspin_lock(&journal->j_state_lock);\n\t\tif (journal->j_running_transaction)\n\t\t\ttransaction = journal->j_running_transaction;\n\t\telse\n\t\t\ttransaction = journal->j_committing_transaction;\n\t\tif (transaction)\n\t\t\ttid = transaction->t_tid;\n\t\telse\n\t\t\ttid = journal->j_commit_sequence;\n\t\tspin_unlock(&journal->j_state_lock);\n\t\tei->i_sync_tid = tid;\n\t\tei->i_datasync_tid = tid;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tei->i_extra_isize = le16_to_cpu(raw_inode->i_extra_isize);\n\t\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize >\n\t\t    EXT4_INODE_SIZE(inode->i_sb)) {\n\t\t\tret = -EIO;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\tif (ei->i_extra_isize == 0) {\n\t\t\t/* The extra space is currently unused. Use it. */\n\t\t\tei->i_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t    EXT4_GOOD_OLD_INODE_SIZE;\n\t\t} else {\n\t\t\t__le32 *magic = (void *)raw_inode +\n\t\t\t\t\tEXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\t\tei->i_extra_isize;\n\t\t\tif (*magic == cpu_to_le32(EXT4_XATTR_MAGIC))\n\t\t\t\text4_set_inode_state(inode, EXT4_STATE_XATTR);\n\t\t}\n\t} else\n\t\tei->i_extra_isize = 0;\n\n\tEXT4_INODE_GET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_GET_XTIME(i_crtime, ei, raw_inode);\n\n\tinode->i_version = le32_to_cpu(raw_inode->i_disk_version);\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\tinode->i_version |=\n\t\t\t(__u64)(le32_to_cpu(raw_inode->i_version_hi)) << 32;\n\t}\n\n\tret = 0;\n\tif (ei->i_file_acl &&\n\t    !ext4_data_block_valid(EXT4_SB(sb), ei->i_file_acl, 1)) {\n\t\text4_error(sb, \"bad extended attribute block %llu inode #%lu\",\n\t\t\t   ei->i_file_acl, inode->i_ino);\n\t\tret = -EIO;\n\t\tgoto bad_inode;\n\t} else if (ei->i_flags & EXT4_EXTENTS_FL) {\n\t\tif (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t    (S_ISLNK(inode->i_mode) &&\n\t\t     !ext4_inode_is_fast_symlink(inode)))\n\t\t\t/* Validate extent which is part of inode */\n\t\t\tret = ext4_ext_check_inode(inode);\n\t} else if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t   (S_ISLNK(inode->i_mode) &&\n\t\t    !ext4_inode_is_fast_symlink(inode))) {\n\t\t/* Validate block references which are part of inode */\n\t\tret = ext4_check_inode_blockref(inode);\n\t}\n\tif (ret)\n\t\tgoto bad_inode;\n\n\tif (S_ISREG(inode->i_mode)) {\n\t\tinode->i_op = &ext4_file_inode_operations;\n\t\tinode->i_fop = &ext4_file_operations;\n\t\text4_set_aops(inode);\n\t} else if (S_ISDIR(inode->i_mode)) {\n\t\tinode->i_op = &ext4_dir_inode_operations;\n\t\tinode->i_fop = &ext4_dir_operations;\n\t} else if (S_ISLNK(inode->i_mode)) {\n\t\tif (ext4_inode_is_fast_symlink(inode)) {\n\t\t\tinode->i_op = &ext4_fast_symlink_inode_operations;\n\t\t\tnd_terminate_link(ei->i_data, inode->i_size,\n\t\t\t\tsizeof(ei->i_data) - 1);\n\t\t} else {\n\t\t\tinode->i_op = &ext4_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t}\n\t} else if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) ||\n\t      S_ISFIFO(inode->i_mode) || S_ISSOCK(inode->i_mode)) {\n\t\tinode->i_op = &ext4_special_inode_operations;\n\t\tif (raw_inode->i_block[0])\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   old_decode_dev(le32_to_cpu(raw_inode->i_block[0])));\n\t\telse\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   new_decode_dev(le32_to_cpu(raw_inode->i_block[1])));\n\t} else {\n\t\tret = -EIO;\n\t\text4_error(inode->i_sb, \"bogus i_mode (%o) for inode=%lu\",\n\t\t\t   inode->i_mode, inode->i_ino);\n\t\tgoto bad_inode;\n\t}\n\tbrelse(iloc.bh);\n\text4_set_inode_flags(inode);\n\tunlock_new_inode(inode);\n\treturn inode;\n\nbad_inode:\n\tbrelse(iloc.bh);\n\tiget_failed(inode);\n\treturn ERR_PTR(ret);\n}\n\nstatic int ext4_inode_blocks_set(handle_t *handle,\n\t\t\t\tstruct ext4_inode *raw_inode,\n\t\t\t\tstruct ext4_inode_info *ei)\n{\n\tstruct inode *inode = &(ei->vfs_inode);\n\tu64 i_blocks = inode->i_blocks;\n\tstruct super_block *sb = inode->i_sb;\n\n\tif (i_blocks <= ~0U) {\n\t\t/*\n\t\t * i_blocks can be represnted in a 32 bit variable\n\t\t * as multiple of 512 bytes\n\t\t */\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = 0;\n\t\tei->i_flags &= ~EXT4_HUGE_FILE_FL;\n\t\treturn 0;\n\t}\n\tif (!EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_HUGE_FILE))\n\t\treturn -EFBIG;\n\n\tif (i_blocks <= 0xffffffffffffULL) {\n\t\t/*\n\t\t * i_blocks can be represented in a 48 bit variable\n\t\t * as multiple of 512 bytes\n\t\t */\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = cpu_to_le16(i_blocks >> 32);\n\t\tei->i_flags &= ~EXT4_HUGE_FILE_FL;\n\t} else {\n\t\tei->i_flags |= EXT4_HUGE_FILE_FL;\n\t\t/* i_block is stored in file system block size */\n\t\ti_blocks = i_blocks >> (inode->i_blkbits - 9);\n\t\traw_inode->i_blocks_lo   = cpu_to_le32(i_blocks);\n\t\traw_inode->i_blocks_high = cpu_to_le16(i_blocks >> 32);\n\t}\n\treturn 0;\n}\n\n/*\n * Post the struct inode info into an on-disk inode location in the\n * buffer-cache.  This gobbles the caller's reference to the\n * buffer_head in the inode location struct.\n *\n * The caller must have write access to iloc->bh.\n */\nstatic int ext4_do_update_inode(handle_t *handle,\n\t\t\t\tstruct inode *inode,\n\t\t\t\tstruct ext4_iloc *iloc)\n{\n\tstruct ext4_inode *raw_inode = ext4_raw_inode(iloc);\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct buffer_head *bh = iloc->bh;\n\tint err = 0, rc, block;\n\n\t/* For fields not not tracking in the in-memory inode,\n\t * initialise them to zero for new inodes. */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_NEW))\n\t\tmemset(raw_inode, 0, EXT4_SB(inode->i_sb)->s_inode_size);\n\n\text4_get_inode_flags(ei);\n\traw_inode->i_mode = cpu_to_le16(inode->i_mode);\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\traw_inode->i_uid_low = cpu_to_le16(low_16_bits(inode->i_uid));\n\t\traw_inode->i_gid_low = cpu_to_le16(low_16_bits(inode->i_gid));\n/*\n * Fix up interoperability with old kernels. Otherwise, old inodes get\n * re-used with the upper 16 bits of the uid/gid intact\n */\n\t\tif (!ei->i_dtime) {\n\t\t\traw_inode->i_uid_high =\n\t\t\t\tcpu_to_le16(high_16_bits(inode->i_uid));\n\t\t\traw_inode->i_gid_high =\n\t\t\t\tcpu_to_le16(high_16_bits(inode->i_gid));\n\t\t} else {\n\t\t\traw_inode->i_uid_high = 0;\n\t\t\traw_inode->i_gid_high = 0;\n\t\t}\n\t} else {\n\t\traw_inode->i_uid_low =\n\t\t\tcpu_to_le16(fs_high2lowuid(inode->i_uid));\n\t\traw_inode->i_gid_low =\n\t\t\tcpu_to_le16(fs_high2lowgid(inode->i_gid));\n\t\traw_inode->i_uid_high = 0;\n\t\traw_inode->i_gid_high = 0;\n\t}\n\traw_inode->i_links_count = cpu_to_le16(inode->i_nlink);\n\n\tEXT4_INODE_SET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_SET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_SET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_SET_XTIME(i_crtime, ei, raw_inode);\n\n\tif (ext4_inode_blocks_set(handle, raw_inode, ei))\n\t\tgoto out_brelse;\n\traw_inode->i_dtime = cpu_to_le32(ei->i_dtime);\n\traw_inode->i_flags = cpu_to_le32(ei->i_flags);\n\tif (EXT4_SB(inode->i_sb)->s_es->s_creator_os !=\n\t    cpu_to_le32(EXT4_OS_HURD))\n\t\traw_inode->i_file_acl_high =\n\t\t\tcpu_to_le16(ei->i_file_acl >> 32);\n\traw_inode->i_file_acl_lo = cpu_to_le32(ei->i_file_acl);\n\text4_isize_set(raw_inode, ei->i_disksize);\n\tif (ei->i_disksize > 0x7fffffffULL) {\n\t\tstruct super_block *sb = inode->i_sb;\n\t\tif (!EXT4_HAS_RO_COMPAT_FEATURE(sb,\n\t\t\t\tEXT4_FEATURE_RO_COMPAT_LARGE_FILE) ||\n\t\t\t\tEXT4_SB(sb)->s_es->s_rev_level ==\n\t\t\t\tcpu_to_le32(EXT4_GOOD_OLD_REV)) {\n\t\t\t/* If this is the first large file\n\t\t\t * created, add a flag to the superblock.\n\t\t\t */\n\t\t\terr = ext4_journal_get_write_access(handle,\n\t\t\t\t\tEXT4_SB(sb)->s_sbh);\n\t\t\tif (err)\n\t\t\t\tgoto out_brelse;\n\t\t\text4_update_dynamic_rev(sb);\n\t\t\tEXT4_SET_RO_COMPAT_FEATURE(sb,\n\t\t\t\t\tEXT4_FEATURE_RO_COMPAT_LARGE_FILE);\n\t\t\tsb->s_dirt = 1;\n\t\t\text4_handle_sync(handle);\n\t\t\terr = ext4_handle_dirty_metadata(handle, NULL,\n\t\t\t\t\tEXT4_SB(sb)->s_sbh);\n\t\t}\n\t}\n\traw_inode->i_generation = cpu_to_le32(inode->i_generation);\n\tif (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode)) {\n\t\tif (old_valid_dev(inode->i_rdev)) {\n\t\t\traw_inode->i_block[0] =\n\t\t\t\tcpu_to_le32(old_encode_dev(inode->i_rdev));\n\t\t\traw_inode->i_block[1] = 0;\n\t\t} else {\n\t\t\traw_inode->i_block[0] = 0;\n\t\t\traw_inode->i_block[1] =\n\t\t\t\tcpu_to_le32(new_encode_dev(inode->i_rdev));\n\t\t\traw_inode->i_block[2] = 0;\n\t\t}\n\t} else\n\t\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\t\traw_inode->i_block[block] = ei->i_data[block];\n\n\traw_inode->i_disk_version = cpu_to_le32(inode->i_version);\n\tif (ei->i_extra_isize) {\n\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\traw_inode->i_version_hi =\n\t\t\tcpu_to_le32(inode->i_version >> 32);\n\t\traw_inode->i_extra_isize = cpu_to_le16(ei->i_extra_isize);\n\t}\n\n\tBUFFER_TRACE(bh, \"call ext4_handle_dirty_metadata\");\n\trc = ext4_handle_dirty_metadata(handle, NULL, bh);\n\tif (!err)\n\t\terr = rc;\n\text4_clear_inode_state(inode, EXT4_STATE_NEW);\n\n\text4_update_inode_fsync_trans(handle, inode, 0);\nout_brelse:\n\tbrelse(bh);\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}\n\n/*\n * ext4_write_inode()\n *\n * We are called from a few places:\n *\n * - Within generic_file_write() for O_SYNC files.\n *   Here, there will be no transaction running. We wait for any running\n *   trasnaction to commit.\n *\n * - Within sys_sync(), kupdate and such.\n *   We wait on commit, if tol to.\n *\n * - Within prune_icache() (PF_MEMALLOC == true)\n *   Here we simply return.  We can't afford to block kswapd on the\n *   journal commit.\n *\n * In all cases it is actually safe for us to return without doing anything,\n * because the inode has been copied into a raw inode buffer in\n * ext4_mark_inode_dirty().  This is a correctness thing for O_SYNC and for\n * knfsd.\n *\n * Note that we are absolutely dependent upon all inode dirtiers doing the\n * right thing: they *must* call mark_inode_dirty() after dirtying info in\n * which we are interested.\n *\n * It would be a bug for them to not do this.  The code:\n *\n *\tmark_inode_dirty(inode)\n *\tstuff();\n *\tinode->i_size = expr;\n *\n * is in error because a kswapd-driven write_inode() could occur while\n * `stuff()' is running, and the new i_size will be lost.  Plus the inode\n * will no longer be on the superblock's dirty inode list.\n */\nint ext4_write_inode(struct inode *inode, int wait)\n{\n\tint err;\n\n\tif (current->flags & PF_MEMALLOC)\n\t\treturn 0;\n\n\tif (EXT4_SB(inode->i_sb)->s_journal) {\n\t\tif (ext4_journal_current_handle()) {\n\t\t\tjbd_debug(1, \"called recursively, non-PF_MEMALLOC!\\n\");\n\t\t\tdump_stack();\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (!wait)\n\t\t\treturn 0;\n\n\t\terr = ext4_force_commit(inode->i_sb);\n\t} else {\n\t\tstruct ext4_iloc iloc;\n\n\t\terr = ext4_get_inode_loc(inode, &iloc);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (wait)\n\t\t\tsync_dirty_buffer(iloc.bh);\n\t\tif (buffer_req(iloc.bh) && !buffer_uptodate(iloc.bh)) {\n\t\t\text4_error(inode->i_sb, \"IO error syncing inode, \"\n\t\t\t\t   \"inode=%lu, block=%llu\", inode->i_ino,\n\t\t\t\t   (unsigned long long)iloc.bh->b_blocknr);\n\t\t\terr = -EIO;\n\t\t}\n\t}\n\treturn err;\n}\n\n/*\n * ext4_setattr()\n *\n * Called from notify_change.\n *\n * We want to trap VFS attempts to truncate the file as soon as\n * possible.  In particular, we want to make sure that when the VFS\n * shrinks i_size, we put the inode on the orphan list and modify\n * i_disksize immediately, so that during the subsequent flushing of\n * dirty pages and freeing of disk blocks, we can guarantee that any\n * commit will leave the blocks being flushed in an unused state on\n * disk.  (On recovery, the inode will get truncated and the blocks will\n * be freed, so we have a strong guarantee that no future commit will\n * leave these blocks visible to the user.)\n *\n * Another thing we have to assure is that if we are in ordered mode\n * and inode is still attached to the committing transaction, we must\n * we start writeout of all the dirty pages which are being truncated.\n * This way we are sure that all the data written in the previous\n * transaction are already on disk (truncate waits for pages under\n * writeback).\n *\n * Called with inode->i_mutex down.\n */\nint ext4_setattr(struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error, rc = 0;\n\tconst unsigned int ia_valid = attr->ia_valid;\n\n\terror = inode_change_ok(inode, attr);\n\tif (error)\n\t\treturn error;\n\n\tif ((ia_valid & ATTR_UID && attr->ia_uid != inode->i_uid) ||\n\t\t(ia_valid & ATTR_GID && attr->ia_gid != inode->i_gid)) {\n\t\thandle_t *handle;\n\n\t\t/* (user+group)*(old+new) structure, inode write (sb,\n\t\t * inode block, ? - but truncate inode update has it) */\n\t\thandle = ext4_journal_start(inode, (EXT4_MAXQUOTAS_INIT_BLOCKS(inode->i_sb)+\n\t\t\t\t\tEXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb))+3);\n\t\tif (IS_ERR(handle)) {\n\t\t\terror = PTR_ERR(handle);\n\t\t\tgoto err_out;\n\t\t}\n\t\terror = vfs_dq_transfer(inode, attr) ? -EDQUOT : 0;\n\t\tif (error) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn error;\n\t\t}\n\t\t/* Update corresponding info in inode so that everything is in\n\t\t * one transaction */\n\t\tif (attr->ia_valid & ATTR_UID)\n\t\t\tinode->i_uid = attr->ia_uid;\n\t\tif (attr->ia_valid & ATTR_GID)\n\t\t\tinode->i_gid = attr->ia_gid;\n\t\terror = ext4_mark_inode_dirty(handle, inode);\n\t\text4_journal_stop(handle);\n\t}\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL)) {\n\t\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\n\t\t\tif (attr->ia_size > sbi->s_bitmap_maxbytes) {\n\t\t\t\terror = -EFBIG;\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (S_ISREG(inode->i_mode) &&\n\t    attr->ia_valid & ATTR_SIZE &&\n\t    (attr->ia_size < inode->i_size ||\n\t     (EXT4_I(inode)->i_flags & EXT4_EOFBLOCKS_FL))) {\n\t\thandle_t *handle;\n\n\t\thandle = ext4_journal_start(inode, 3);\n\t\tif (IS_ERR(handle)) {\n\t\t\terror = PTR_ERR(handle);\n\t\t\tgoto err_out;\n\t\t}\n\n\t\terror = ext4_orphan_add(handle, inode);\n\t\tEXT4_I(inode)->i_disksize = attr->ia_size;\n\t\trc = ext4_mark_inode_dirty(handle, inode);\n\t\tif (!error)\n\t\t\terror = rc;\n\t\text4_journal_stop(handle);\n\n\t\tif (ext4_should_order_data(inode)) {\n\t\t\terror = ext4_begin_ordered_truncate(inode,\n\t\t\t\t\t\t\t    attr->ia_size);\n\t\t\tif (error) {\n\t\t\t\t/* Do as much error cleanup as possible */\n\t\t\t\thandle = ext4_journal_start(inode, 3);\n\t\t\t\tif (IS_ERR(handle)) {\n\t\t\t\t\text4_orphan_del(NULL, inode);\n\t\t\t\t\tgoto err_out;\n\t\t\t\t}\n\t\t\t\text4_orphan_del(handle, inode);\n\t\t\t\text4_journal_stop(handle);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\t\t/* ext4_truncate will clear the flag */\n\t\tif ((EXT4_I(inode)->i_flags & EXT4_EOFBLOCKS_FL))\n\t\t\text4_truncate(inode);\n\t}\n\n\trc = inode_setattr(inode, attr);\n\n\t/* If inode_setattr's call to ext4_truncate failed to get a\n\t * transaction handle at all, we need to clean up the in-core\n\t * orphan list manually. */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(NULL, inode);\n\n\tif (!rc && (ia_valid & ATTR_MODE))\n\t\trc = ext4_acl_chmod(inode);\n\nerr_out:\n\text4_std_error(inode->i_sb, error);\n\tif (!error)\n\t\terror = rc;\n\treturn error;\n}\n\nint ext4_getattr(struct vfsmount *mnt, struct dentry *dentry,\n\t\t struct kstat *stat)\n{\n\tstruct inode *inode;\n\tunsigned long delalloc_blocks;\n\n\tinode = dentry->d_inode;\n\tgeneric_fillattr(inode, stat);\n\n\t/*\n\t * We can't update i_blocks if the block allocation is delayed\n\t * otherwise in the case of system crash before the real block\n\t * allocation is done, we will have i_blocks inconsistent with\n\t * on-disk file blocks.\n\t * We always keep i_blocks updated together with real\n\t * allocation. But to not confuse with user, stat\n\t * will return the blocks that include the delayed allocation\n\t * blocks for this file.\n\t */\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tdelalloc_blocks = EXT4_I(inode)->i_reserved_data_blocks;\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\n\tstat->blocks += (delalloc_blocks << inode->i_sb->s_blocksize_bits)>>9;\n\treturn 0;\n}\n\nstatic int ext4_indirect_trans_blocks(struct inode *inode, int nrblocks,\n\t\t\t\t      int chunk)\n{\n\tint indirects;\n\n\t/* if nrblocks are contiguous */\n\tif (chunk) {\n\t\t/*\n\t\t * With N contiguous data blocks, it need at most\n\t\t * N/EXT4_ADDR_PER_BLOCK(inode->i_sb) indirect blocks\n\t\t * 2 dindirect blocks\n\t\t * 1 tindirect block\n\t\t */\n\t\tindirects = nrblocks / EXT4_ADDR_PER_BLOCK(inode->i_sb);\n\t\treturn indirects + 3;\n\t}\n\t/*\n\t * if nrblocks are not contiguous, worse case, each block touch\n\t * a indirect block, and each indirect block touch a double indirect\n\t * block, plus a triple indirect block\n\t */\n\tindirects = nrblocks * 2 + 1;\n\treturn indirects;\n}\n\nstatic int ext4_index_trans_blocks(struct inode *inode, int nrblocks, int chunk)\n{\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL))\n\t\treturn ext4_indirect_trans_blocks(inode, nrblocks, chunk);\n\treturn ext4_ext_index_trans_blocks(inode, nrblocks, chunk);\n}\n\n/*\n * Account for index blocks, block groups bitmaps and block group\n * descriptor blocks if modify datablocks and index blocks\n * worse case, the indexs blocks spread over different block groups\n *\n * If datablocks are discontiguous, they are possible to spread over\n * different block groups too. If they are contiuguous, with flexbg,\n * they could still across block group boundary.\n *\n * Also account for superblock, inode, quota and xattr blocks\n */\nint ext4_meta_trans_blocks(struct inode *inode, int nrblocks, int chunk)\n{\n\text4_group_t groups, ngroups = ext4_get_groups_count(inode->i_sb);\n\tint gdpblocks;\n\tint idxblocks;\n\tint ret = 0;\n\n\t/*\n\t * How many index blocks need to touch to modify nrblocks?\n\t * The \"Chunk\" flag indicating whether the nrblocks is\n\t * physically contiguous on disk\n\t *\n\t * For Direct IO and fallocate, they calls get_block to allocate\n\t * one single extent at a time, so they could set the \"Chunk\" flag\n\t */\n\tidxblocks = ext4_index_trans_blocks(inode, nrblocks, chunk);\n\n\tret = idxblocks;\n\n\t/*\n\t * Now let's see how many group bitmaps and group descriptors need\n\t * to account\n\t */\n\tgroups = idxblocks;\n\tif (chunk)\n\t\tgroups += 1;\n\telse\n\t\tgroups += nrblocks;\n\n\tgdpblocks = groups;\n\tif (groups > ngroups)\n\t\tgroups = ngroups;\n\tif (groups > EXT4_SB(inode->i_sb)->s_gdb_count)\n\t\tgdpblocks = EXT4_SB(inode->i_sb)->s_gdb_count;\n\n\t/* bitmaps and block group descriptor blocks */\n\tret += groups + gdpblocks;\n\n\t/* Blocks for super block, inode, quota and xattr blocks */\n\tret += EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\n\treturn ret;\n}\n\n/*\n * Calulate the total number of credits to reserve to fit\n * the modification of a single pages into a single transaction,\n * which may include multiple chunks of block allocations.\n *\n * This could be called via ext4_write_begin()\n *\n * We need to consider the worse case, when\n * one new block per extent.\n */\nint ext4_writepage_trans_blocks(struct inode *inode)\n{\n\tint bpp = ext4_journal_blocks_per_page(inode);\n\tint ret;\n\n\tret = ext4_meta_trans_blocks(inode, bpp, 0);\n\n\t/* Account for data blocks for journalled mode */\n\tif (ext4_should_journal_data(inode))\n\t\tret += bpp;\n\treturn ret;\n}\n\n/*\n * Calculate the journal credits for a chunk of data modification.\n *\n * This is called from DIO, fallocate or whoever calling\n * ext4_get_blocks() to map/allocate a chunk of contiguous disk blocks.\n *\n * journal buffers for data blocks are not included here, as DIO\n * and fallocate do no need to journal data buffers.\n */\nint ext4_chunk_trans_blocks(struct inode *inode, int nrblocks)\n{\n\treturn ext4_meta_trans_blocks(inode, nrblocks, 1);\n}\n\n/*\n * The caller must have previously called ext4_reserve_inode_write().\n * Give this, we know that the caller already has write access to iloc->bh.\n */\nint ext4_mark_iloc_dirty(handle_t *handle,\n\t\t\t struct inode *inode, struct ext4_iloc *iloc)\n{\n\tint err = 0;\n\n\tif (test_opt(inode->i_sb, I_VERSION))\n\t\tinode_inc_iversion(inode);\n\n\t/* the do_update_inode consumes one bh->b_count */\n\tget_bh(iloc->bh);\n\n\t/* ext4_do_update_inode() does jbd2_journal_dirty_metadata */\n\terr = ext4_do_update_inode(handle, inode, iloc);\n\tput_bh(iloc->bh);\n\treturn err;\n}\n\n/*\n * On success, We end up with an outstanding reference count against\n * iloc->bh.  This _must_ be cleaned up later.\n */\n\nint\next4_reserve_inode_write(handle_t *handle, struct inode *inode,\n\t\t\t struct ext4_iloc *iloc)\n{\n\tint err;\n\n\terr = ext4_get_inode_loc(inode, iloc);\n\tif (!err) {\n\t\tBUFFER_TRACE(iloc->bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, iloc->bh);\n\t\tif (err) {\n\t\t\tbrelse(iloc->bh);\n\t\t\tiloc->bh = NULL;\n\t\t}\n\t}\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}\n\n/*\n * Expand an inode by new_extra_isize bytes.\n * Returns 0 on success or negative error number on failure.\n */\nstatic int ext4_expand_extra_isize(struct inode *inode,\n\t\t\t\t   unsigned int new_extra_isize,\n\t\t\t\t   struct ext4_iloc iloc,\n\t\t\t\t   handle_t *handle)\n{\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_xattr_entry *entry;\n\n\tif (EXT4_I(inode)->i_extra_isize >= new_extra_isize)\n\t\treturn 0;\n\n\traw_inode = ext4_raw_inode(&iloc);\n\n\theader = IHDR(inode, raw_inode);\n\tentry = IFIRST(header);\n\n\t/* No extended attributes present */\n\tif (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) ||\n\t    header->h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {\n\t\tmemset((void *)raw_inode + EXT4_GOOD_OLD_INODE_SIZE, 0,\n\t\t\tnew_extra_isize);\n\t\tEXT4_I(inode)->i_extra_isize = new_extra_isize;\n\t\treturn 0;\n\t}\n\n\t/* try to expand with EAs present */\n\treturn ext4_expand_extra_isize_ea(inode, new_extra_isize,\n\t\t\t\t\t  raw_inode, handle);\n}\n\n/*\n * What we do here is to mark the in-core inode as clean with respect to inode\n * dirtiness (it may still be data-dirty).\n * This means that the in-core inode may be reaped by prune_icache\n * without having to perform any I/O.  This is a very good thing,\n * because *any* task may call prune_icache - even ones which\n * have a transaction open against a different journal.\n *\n * Is this cheating?  Not really.  Sure, we haven't written the\n * inode out, but prune_icache isn't a user-visible syncing function.\n * Whenever the user wants stuff synced (sys_sync, sys_msync, sys_fsync)\n * we start and wait on commits.\n *\n * Is this efficient/effective?  Well, we're being nice to the system\n * by cleaning up our inodes proactively so they can be reaped\n * without I/O.  But we are potentially leaving up to five seconds'\n * worth of inodes floating about which prune_icache wants us to\n * write out.  One way to fix that would be to get prune_icache()\n * to do a write_super() to free up some memory.  It has the desired\n * effect.\n */\nint ext4_mark_inode_dirty(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstatic unsigned int mnt_count;\n\tint err, ret;\n\n\tmight_sleep();\n\terr = ext4_reserve_inode_write(handle, inode, &iloc);\n\tif (ext4_handle_valid(handle) &&\n\t    EXT4_I(inode)->i_extra_isize < sbi->s_want_extra_isize &&\n\t    !ext4_test_inode_state(inode, EXT4_STATE_NO_EXPAND)) {\n\t\t/*\n\t\t * We need extra buffer credits since we may write into EA block\n\t\t * with this same handle. If journal_extend fails, then it will\n\t\t * only result in a minor loss of functionality for that inode.\n\t\t * If this is felt to be critical, then e2fsck should be run to\n\t\t * force a large enough s_min_extra_isize.\n\t\t */\n\t\tif ((jbd2_journal_extend(handle,\n\t\t\t     EXT4_DATA_TRANS_BLOCKS(inode->i_sb))) == 0) {\n\t\t\tret = ext4_expand_extra_isize(inode,\n\t\t\t\t\t\t      sbi->s_want_extra_isize,\n\t\t\t\t\t\t      iloc, handle);\n\t\t\tif (ret) {\n\t\t\t\text4_set_inode_state(inode,\n\t\t\t\t\t\t     EXT4_STATE_NO_EXPAND);\n\t\t\t\tif (mnt_count !=\n\t\t\t\t\tle16_to_cpu(sbi->s_es->s_mnt_count)) {\n\t\t\t\t\text4_warning(inode->i_sb,\n\t\t\t\t\t\"Unable to expand inode %lu. Delete\"\n\t\t\t\t\t\" some EAs or run e2fsck.\",\n\t\t\t\t\tinode->i_ino);\n\t\t\t\t\tmnt_count =\n\t\t\t\t\t  le16_to_cpu(sbi->s_es->s_mnt_count);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif (!err)\n\t\terr = ext4_mark_iloc_dirty(handle, inode, &iloc);\n\treturn err;\n}\n\n/*\n * ext4_dirty_inode() is called from __mark_inode_dirty()\n *\n * We're really interested in the case where a file is being extended.\n * i_size has been changed by generic_commit_write() and we thus need\n * to include the updated inode in the current transaction.\n *\n * Also, vfs_dq_alloc_block() will always dirty the inode when blocks\n * are allocated to the file.\n *\n * If the inode is marked synchronous, we don't honour that here - doing\n * so would cause a commit on atime updates, which we don't bother doing.\n * We handle synchronous inodes at the highest possible level.\n */\nvoid ext4_dirty_inode(struct inode *inode)\n{\n\thandle_t *handle;\n\n\thandle = ext4_journal_start(inode, 2);\n\tif (IS_ERR(handle))\n\t\tgoto out;\n\n\text4_mark_inode_dirty(handle, inode);\n\n\text4_journal_stop(handle);\nout:\n\treturn;\n}\n\n#if 0\n/*\n * Bind an inode's backing buffer_head into this transaction, to prevent\n * it from being flushed to disk early.  Unlike\n * ext4_reserve_inode_write, this leaves behind no bh reference and\n * returns no iloc structure, so the caller needs to repeat the iloc\n * lookup to mark the inode dirty later.\n */\nstatic int ext4_pin_inode(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_iloc iloc;\n\n\tint err = 0;\n\tif (handle) {\n\t\terr = ext4_get_inode_loc(inode, &iloc);\n\t\tif (!err) {\n\t\t\tBUFFER_TRACE(iloc.bh, \"get_write_access\");\n\t\t\terr = jbd2_journal_get_write_access(handle, iloc.bh);\n\t\t\tif (!err)\n\t\t\t\terr = ext4_handle_dirty_metadata(handle,\n\t\t\t\t\t\t\t\t NULL,\n\t\t\t\t\t\t\t\t iloc.bh);\n\t\t\tbrelse(iloc.bh);\n\t\t}\n\t}\n\text4_std_error(inode->i_sb, err);\n\treturn err;\n}\n#endif\n\nint ext4_change_inode_journal_flag(struct inode *inode, int val)\n{\n\tjournal_t *journal;\n\thandle_t *handle;\n\tint err;\n\n\t/*\n\t * We have to be very careful here: changing a data block's\n\t * journaling status dynamically is dangerous.  If we write a\n\t * data block to the journal, change the status and then delete\n\t * that block, we risk forgetting to revoke the old log record\n\t * from the journal and so a subsequent replay can corrupt data.\n\t * So, first we make sure that the journal is empty and that\n\t * nobody is changing anything.\n\t */\n\n\tjournal = EXT4_JOURNAL(inode);\n\tif (!journal)\n\t\treturn 0;\n\tif (is_journal_aborted(journal))\n\t\treturn -EROFS;\n\n\tjbd2_journal_lock_updates(journal);\n\tjbd2_journal_flush(journal);\n\n\t/*\n\t * OK, there are no updates running now, and all cached data is\n\t * synced to disk.  We are now in a completely consistent state\n\t * which doesn't have anything in the journal, and we know that\n\t * no filesystem updates are running, so it is safe to modify\n\t * the inode's in-core data-journaling state flag now.\n\t */\n\n\tif (val)\n\t\tEXT4_I(inode)->i_flags |= EXT4_JOURNAL_DATA_FL;\n\telse\n\t\tEXT4_I(inode)->i_flags &= ~EXT4_JOURNAL_DATA_FL;\n\text4_set_aops(inode);\n\n\tjbd2_journal_unlock_updates(journal);\n\n\t/* Finally we can mark the inode as dirty. */\n\n\thandle = ext4_journal_start(inode, 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\n\terr = ext4_mark_inode_dirty(handle, inode);\n\text4_handle_sync(handle);\n\text4_journal_stop(handle);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\nstatic int ext4_bh_unmapped(handle_t *handle, struct buffer_head *bh)\n{\n\treturn !buffer_mapped(bh);\n}\n\nint ext4_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tstruct page *page = vmf->page;\n\tloff_t size;\n\tunsigned long len;\n\tint ret = -EINVAL;\n\tvoid *fsdata;\n\tstruct file *file = vma->vm_file;\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\t/*\n\t * Get i_alloc_sem to stop truncates messing with the inode. We cannot\n\t * get i_mutex because we are already holding mmap_sem.\n\t */\n\tdown_read(&inode->i_alloc_sem);\n\tsize = i_size_read(inode);\n\tif (page->mapping != mapping || size <= page_offset(page)\n\t    || !PageUptodate(page)) {\n\t\t/* page got truncated from under us? */\n\t\tgoto out_unlock;\n\t}\n\tret = 0;\n\tif (PageMappedToDisk(page))\n\t\tgoto out_unlock;\n\n\tif (page->index == size >> PAGE_CACHE_SHIFT)\n\t\tlen = size & ~PAGE_CACHE_MASK;\n\telse\n\t\tlen = PAGE_CACHE_SIZE;\n\n\tlock_page(page);\n\t/*\n\t * return if we have all the buffers mapped. This avoid\n\t * the need to call write_begin/write_end which does a\n\t * journal_start/journal_stop which can block and take\n\t * long time\n\t */\n\tif (page_has_buffers(page)) {\n\t\tif (!walk_page_buffers(NULL, page_buffers(page), 0, len, NULL,\n\t\t\t\t\text4_bh_unmapped)) {\n\t\t\tunlock_page(page);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\tunlock_page(page);\n\t/*\n\t * OK, we need to fill the hole... Do write_begin write_end\n\t * to do block allocation/reservation.We are not holding\n\t * inode.i__mutex here. That allow * parallel write_begin,\n\t * write_end call. lock_page prevent this from happening\n\t * on the same page though\n\t */\n\tret = mapping->a_ops->write_begin(file, mapping, page_offset(page),\n\t\t\tlen, AOP_FLAG_UNINTERRUPTIBLE, &page, &fsdata);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\tret = mapping->a_ops->write_end(file, mapping, page_offset(page),\n\t\t\tlen, len, page, fsdata);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\tret = 0;\nout_unlock:\n\tif (ret)\n\t\tret = VM_FAULT_SIGBUS;\n\tup_read(&inode->i_alloc_sem);\n\treturn ret;\n}\n", "/*\n *  linux/fs/ext4/super.c\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/fs/minix/inode.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  Big-endian to little-endian byte-swapping/bitmaps by\n *        David S. Miller (davem@caip.rutgers.edu), 1995\n */\n\n#include <linux/module.h>\n#include <linux/string.h>\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/vmalloc.h>\n#include <linux/jbd2.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/blkdev.h>\n#include <linux/parser.h>\n#include <linux/smp_lock.h>\n#include <linux/buffer_head.h>\n#include <linux/exportfs.h>\n#include <linux/vfs.h>\n#include <linux/random.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/quotaops.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/ctype.h>\n#include <linux/log2.h>\n#include <linux/crc16.h>\n#include <asm/uaccess.h>\n\n#include \"ext4.h\"\n#include \"ext4_jbd2.h\"\n#include \"xattr.h\"\n#include \"acl.h\"\n#include \"mballoc.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/ext4.h>\n\nstruct proc_dir_entry *ext4_proc_root;\nstatic struct kset *ext4_kset;\n\nstatic int ext4_load_journal(struct super_block *, struct ext4_super_block *,\n\t\t\t     unsigned long journal_devnum);\nstatic int ext4_commit_super(struct super_block *sb, int sync);\nstatic void ext4_mark_recovery_complete(struct super_block *sb,\n\t\t\t\t\tstruct ext4_super_block *es);\nstatic void ext4_clear_journal_err(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es);\nstatic int ext4_sync_fs(struct super_block *sb, int wait);\nstatic const char *ext4_decode_error(struct super_block *sb, int errno,\n\t\t\t\t     char nbuf[16]);\nstatic int ext4_remount(struct super_block *sb, int *flags, char *data);\nstatic int ext4_statfs(struct dentry *dentry, struct kstatfs *buf);\nstatic int ext4_unfreeze(struct super_block *sb);\nstatic void ext4_write_super(struct super_block *sb);\nstatic int ext4_freeze(struct super_block *sb);\n\n\next4_fsblk_t ext4_block_bitmap(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_block_bitmap_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_block_bitmap_hi) << 32 : 0);\n}\n\next4_fsblk_t ext4_inode_bitmap(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_inode_bitmap_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_inode_bitmap_hi) << 32 : 0);\n}\n\next4_fsblk_t ext4_inode_table(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le32_to_cpu(bg->bg_inode_table_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_inode_table_hi) << 32 : 0);\n}\n\n__u32 ext4_free_blks_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_free_blocks_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_free_blocks_count_hi) << 16 : 0);\n}\n\n__u32 ext4_free_inodes_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_free_inodes_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_free_inodes_count_hi) << 16 : 0);\n}\n\n__u32 ext4_used_dirs_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_used_dirs_count_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_used_dirs_count_hi) << 16 : 0);\n}\n\n__u32 ext4_itable_unused_count(struct super_block *sb,\n\t\t\t      struct ext4_group_desc *bg)\n{\n\treturn le16_to_cpu(bg->bg_itable_unused_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (__u32)le16_to_cpu(bg->bg_itable_unused_hi) << 16 : 0);\n}\n\nvoid ext4_block_bitmap_set(struct super_block *sb,\n\t\t\t   struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_block_bitmap_lo = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_block_bitmap_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_inode_bitmap_set(struct super_block *sb,\n\t\t\t   struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_inode_bitmap_lo  = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_inode_bitmap_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_inode_table_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, ext4_fsblk_t blk)\n{\n\tbg->bg_inode_table_lo = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_inode_table_hi = cpu_to_le32(blk >> 32);\n}\n\nvoid ext4_free_blks_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_free_blocks_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_free_blocks_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_free_inodes_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_free_inodes_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_free_inodes_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_used_dirs_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_used_dirs_count_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_used_dirs_count_hi = cpu_to_le16(count >> 16);\n}\n\nvoid ext4_itable_unused_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count)\n{\n\tbg->bg_itable_unused_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_itable_unused_hi = cpu_to_le16(count >> 16);\n}\n\n\n/* Just increment the non-pointer handle value */\nstatic handle_t *ext4_get_nojournal(void)\n{\n\thandle_t *handle = current->journal_info;\n\tunsigned long ref_cnt = (unsigned long)handle;\n\n\tBUG_ON(ref_cnt >= EXT4_NOJOURNAL_MAX_REF_COUNT);\n\n\tref_cnt++;\n\thandle = (handle_t *)ref_cnt;\n\n\tcurrent->journal_info = handle;\n\treturn handle;\n}\n\n\n/* Decrement the non-pointer handle value */\nstatic void ext4_put_nojournal(handle_t *handle)\n{\n\tunsigned long ref_cnt = (unsigned long)handle;\n\n\tBUG_ON(ref_cnt == 0);\n\n\tref_cnt--;\n\thandle = (handle_t *)ref_cnt;\n\n\tcurrent->journal_info = handle;\n}\n\n/*\n * Wrappers for jbd2_journal_start/end.\n *\n * The only special thing we need to do here is to make sure that all\n * journal_end calls result in the superblock being marked dirty, so\n * that sync() will call the filesystem's write_super callback if\n * appropriate.\n */\nhandle_t *ext4_journal_start_sb(struct super_block *sb, int nblocks)\n{\n\tjournal_t *journal;\n\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn ERR_PTR(-EROFS);\n\n\t/* Special case here: if the journal has aborted behind our\n\t * backs (eg. EIO in the commit thread), then we still need to\n\t * take the FS itself readonly cleanly. */\n\tjournal = EXT4_SB(sb)->s_journal;\n\tif (journal) {\n\t\tif (is_journal_aborted(journal)) {\n\t\t\text4_abort(sb, __func__, \"Detected aborted journal\");\n\t\t\treturn ERR_PTR(-EROFS);\n\t\t}\n\t\treturn jbd2_journal_start(journal, nblocks);\n\t}\n\treturn ext4_get_nojournal();\n}\n\n/*\n * The only special thing we need to do here is to make sure that all\n * jbd2_journal_stop calls result in the superblock being marked dirty, so\n * that sync() will call the filesystem's write_super callback if\n * appropriate.\n */\nint __ext4_journal_stop(const char *where, handle_t *handle)\n{\n\tstruct super_block *sb;\n\tint err;\n\tint rc;\n\n\tif (!ext4_handle_valid(handle)) {\n\t\text4_put_nojournal(handle);\n\t\treturn 0;\n\t}\n\tsb = handle->h_transaction->t_journal->j_private;\n\terr = handle->h_err;\n\trc = jbd2_journal_stop(handle);\n\n\tif (!err)\n\t\terr = rc;\n\tif (err)\n\t\t__ext4_std_error(sb, where, err);\n\treturn err;\n}\n\nvoid ext4_journal_abort_handle(const char *caller, const char *err_fn,\n\t\tstruct buffer_head *bh, handle_t *handle, int err)\n{\n\tchar nbuf[16];\n\tconst char *errstr = ext4_decode_error(NULL, err, nbuf);\n\n\tBUG_ON(!ext4_handle_valid(handle));\n\n\tif (bh)\n\t\tBUFFER_TRACE(bh, \"abort\");\n\n\tif (!handle->h_err)\n\t\thandle->h_err = err;\n\n\tif (is_handle_aborted(handle))\n\t\treturn;\n\n\tprintk(KERN_ERR \"%s: aborting transaction: %s in %s\\n\",\n\t       caller, errstr, err_fn);\n\n\tjbd2_journal_abort_handle(handle);\n}\n\n/* Deal with the reporting of failure conditions on a filesystem such as\n * inconsistencies detected or read IO failures.\n *\n * On ext2, we can store the error state of the filesystem in the\n * superblock.  That is not possible on ext4, because we may have other\n * write ordering constraints on the superblock which prevent us from\n * writing it out straight away; and given that the journal is about to\n * be aborted, we can't rely on the current, or future, transactions to\n * write out the superblock safely.\n *\n * We'll just use the jbd2_journal_abort() error code to record an error in\n * the journal instead.  On recovery, the journal will compain about\n * that error until we've noted it down and cleared it.\n */\n\nstatic void ext4_handle_error(struct super_block *sb)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn;\n\n\tif (!test_opt(sb, ERRORS_CONT)) {\n\t\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\t\tEXT4_SB(sb)->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\t\tif (journal)\n\t\t\tjbd2_journal_abort(journal, -EIO);\n\t}\n\tif (test_opt(sb, ERRORS_RO)) {\n\t\text4_msg(sb, KERN_CRIT, \"Remounting filesystem read-only\");\n\t\tsb->s_flags |= MS_RDONLY;\n\t}\n\text4_commit_super(sb, 1);\n\tif (test_opt(sb, ERRORS_PANIC))\n\t\tpanic(\"EXT4-fs (device %s): panic forced after error\\n\",\n\t\t\tsb->s_id);\n}\n\nvoid __ext4_error(struct super_block *sb, const char *function,\n\t\tconst char *fmt, ...)\n{\n\tva_list args;\n\n\tva_start(args, fmt);\n\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s: \", sb->s_id, function);\n\tvprintk(fmt, args);\n\tprintk(\"\\n\");\n\tva_end(args);\n\n\text4_handle_error(sb);\n}\n\nstatic const char *ext4_decode_error(struct super_block *sb, int errno,\n\t\t\t\t     char nbuf[16])\n{\n\tchar *errstr = NULL;\n\n\tswitch (errno) {\n\tcase -EIO:\n\t\terrstr = \"IO failure\";\n\t\tbreak;\n\tcase -ENOMEM:\n\t\terrstr = \"Out of memory\";\n\t\tbreak;\n\tcase -EROFS:\n\t\tif (!sb || (EXT4_SB(sb)->s_journal &&\n\t\t\t    EXT4_SB(sb)->s_journal->j_flags & JBD2_ABORT))\n\t\t\terrstr = \"Journal has aborted\";\n\t\telse\n\t\t\terrstr = \"Readonly filesystem\";\n\t\tbreak;\n\tdefault:\n\t\t/* If the caller passed in an extra buffer for unknown\n\t\t * errors, textualise them now.  Else we just return\n\t\t * NULL. */\n\t\tif (nbuf) {\n\t\t\t/* Check for truncated error codes... */\n\t\t\tif (snprintf(nbuf, 16, \"error %d\", -errno) >= 0)\n\t\t\t\terrstr = nbuf;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn errstr;\n}\n\n/* __ext4_std_error decodes expected errors from journaling functions\n * automatically and invokes the appropriate error response.  */\n\nvoid __ext4_std_error(struct super_block *sb, const char *function, int errno)\n{\n\tchar nbuf[16];\n\tconst char *errstr;\n\n\t/* Special case: if the error is EROFS, and we're not already\n\t * inside a transaction, then there's really no point in logging\n\t * an error. */\n\tif (errno == -EROFS && journal_current_handle() == NULL &&\n\t    (sb->s_flags & MS_RDONLY))\n\t\treturn;\n\n\terrstr = ext4_decode_error(sb, errno, nbuf);\n\tprintk(KERN_CRIT \"EXT4-fs error (device %s) in %s: %s\\n\",\n\t       sb->s_id, function, errstr);\n\n\text4_handle_error(sb);\n}\n\n/*\n * ext4_abort is a much stronger failure handler than ext4_error.  The\n * abort function may be used to deal with unrecoverable failures such\n * as journal IO errors or ENOMEM at a critical moment in log management.\n *\n * We unconditionally force the filesystem into an ABORT|READONLY state,\n * unless the error response on the fs has been set to panic in which\n * case we take the easy way out and panic immediately.\n */\n\nvoid ext4_abort(struct super_block *sb, const char *function,\n\t\tconst char *fmt, ...)\n{\n\tva_list args;\n\n\tva_start(args, fmt);\n\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s: \", sb->s_id, function);\n\tvprintk(fmt, args);\n\tprintk(\"\\n\");\n\tva_end(args);\n\n\tif (test_opt(sb, ERRORS_PANIC))\n\t\tpanic(\"EXT4-fs panic from previous error\\n\");\n\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn;\n\n\text4_msg(sb, KERN_CRIT, \"Remounting filesystem read-only\");\n\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\tsb->s_flags |= MS_RDONLY;\n\tEXT4_SB(sb)->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\tif (EXT4_SB(sb)->s_journal)\n\t\tjbd2_journal_abort(EXT4_SB(sb)->s_journal, -EIO);\n}\n\nvoid ext4_msg (struct super_block * sb, const char *prefix,\n\t\t   const char *fmt, ...)\n{\n\tva_list args;\n\n\tva_start(args, fmt);\n\tprintk(\"%sEXT4-fs (%s): \", prefix, sb->s_id);\n\tvprintk(fmt, args);\n\tprintk(\"\\n\");\n\tva_end(args);\n}\n\nvoid __ext4_warning(struct super_block *sb, const char *function,\n\t\t  const char *fmt, ...)\n{\n\tva_list args;\n\n\tva_start(args, fmt);\n\tprintk(KERN_WARNING \"EXT4-fs warning (device %s): %s: \",\n\t       sb->s_id, function);\n\tvprintk(fmt, args);\n\tprintk(\"\\n\");\n\tva_end(args);\n}\n\nvoid ext4_grp_locked_error(struct super_block *sb, ext4_group_t grp,\n\t\t\t   const char *function, const char *fmt, ...)\n__releases(bitlock)\n__acquires(bitlock)\n{\n\tva_list args;\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tva_start(args, fmt);\n\tprintk(KERN_CRIT \"EXT4-fs error (device %s): %s: \", sb->s_id, function);\n\tvprintk(fmt, args);\n\tprintk(\"\\n\");\n\tva_end(args);\n\n\tif (test_opt(sb, ERRORS_CONT)) {\n\t\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\t\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\t\text4_commit_super(sb, 0);\n\t\treturn;\n\t}\n\text4_unlock_group(sb, grp);\n\text4_handle_error(sb);\n\t/*\n\t * We only get here in the ERRORS_RO case; relocking the group\n\t * may be dangerous, but nothing bad will happen since the\n\t * filesystem will have already been marked read/only and the\n\t * journal has been aborted.  We return 1 as a hint to callers\n\t * who might what to use the return value from\n\t * ext4_grp_locked_error() to distinguish beween the\n\t * ERRORS_CONT and ERRORS_RO case, and perhaps return more\n\t * aggressively from the ext4 function in question, with a\n\t * more appropriate error code.\n\t */\n\text4_lock_group(sb, grp);\n\treturn;\n}\n\nvoid ext4_update_dynamic_rev(struct super_block *sb)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tif (le32_to_cpu(es->s_rev_level) > EXT4_GOOD_OLD_REV)\n\t\treturn;\n\n\text4_warning(sb,\n\t\t     \"updating to rev %d because of new feature flag, \"\n\t\t     \"running e2fsck is recommended\",\n\t\t     EXT4_DYNAMIC_REV);\n\n\tes->s_first_ino = cpu_to_le32(EXT4_GOOD_OLD_FIRST_INO);\n\tes->s_inode_size = cpu_to_le16(EXT4_GOOD_OLD_INODE_SIZE);\n\tes->s_rev_level = cpu_to_le32(EXT4_DYNAMIC_REV);\n\t/* leave es->s_feature_*compat flags alone */\n\t/* es->s_uuid will be set by e2fsck if empty */\n\n\t/*\n\t * The rest of the superblock fields should be zero, and if not it\n\t * means they are likely already in use, so leave them alone.  We\n\t * can leave it up to e2fsck to clean up any inconsistencies there.\n\t */\n}\n\n/*\n * Open the external journal device\n */\nstatic struct block_device *ext4_blkdev_get(dev_t dev, struct super_block *sb)\n{\n\tstruct block_device *bdev;\n\tchar b[BDEVNAME_SIZE];\n\n\tbdev = open_by_devnum(dev, FMODE_READ|FMODE_WRITE);\n\tif (IS_ERR(bdev))\n\t\tgoto fail;\n\treturn bdev;\n\nfail:\n\text4_msg(sb, KERN_ERR, \"failed to open journal device %s: %ld\",\n\t\t\t__bdevname(dev, b), PTR_ERR(bdev));\n\treturn NULL;\n}\n\n/*\n * Release the journal device\n */\nstatic int ext4_blkdev_put(struct block_device *bdev)\n{\n\tbd_release(bdev);\n\treturn blkdev_put(bdev, FMODE_READ|FMODE_WRITE);\n}\n\nstatic int ext4_blkdev_remove(struct ext4_sb_info *sbi)\n{\n\tstruct block_device *bdev;\n\tint ret = -ENODEV;\n\n\tbdev = sbi->journal_bdev;\n\tif (bdev) {\n\t\tret = ext4_blkdev_put(bdev);\n\t\tsbi->journal_bdev = NULL;\n\t}\n\treturn ret;\n}\n\nstatic inline struct inode *orphan_list_entry(struct list_head *l)\n{\n\treturn &list_entry(l, struct ext4_inode_info, i_orphan)->vfs_inode;\n}\n\nstatic void dump_orphan_list(struct super_block *sb, struct ext4_sb_info *sbi)\n{\n\tstruct list_head *l;\n\n\text4_msg(sb, KERN_ERR, \"sb orphan head is %d\",\n\t\t le32_to_cpu(sbi->s_es->s_last_orphan));\n\n\tprintk(KERN_ERR \"sb_info orphan list:\\n\");\n\tlist_for_each(l, &sbi->s_orphan) {\n\t\tstruct inode *inode = orphan_list_entry(l);\n\t\tprintk(KERN_ERR \"  \"\n\t\t       \"inode %s:%lu at %p: mode %o, nlink %d, next %d\\n\",\n\t\t       inode->i_sb->s_id, inode->i_ino, inode,\n\t\t       inode->i_mode, inode->i_nlink,\n\t\t       NEXT_ORPHAN(inode));\n\t}\n}\n\nstatic void ext4_put_super(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tint i, err;\n\n\tflush_workqueue(sbi->dio_unwritten_wq);\n\tdestroy_workqueue(sbi->dio_unwritten_wq);\n\n\tlock_super(sb);\n\tlock_kernel();\n\tif (sb->s_dirt)\n\t\text4_commit_super(sb, 1);\n\n\tif (sbi->s_journal) {\n\t\terr = jbd2_journal_destroy(sbi->s_journal);\n\t\tsbi->s_journal = NULL;\n\t\tif (err < 0)\n\t\t\text4_abort(sb, __func__,\n\t\t\t\t   \"Couldn't clean up the journal\");\n\t}\n\n\text4_release_system_zone(sb);\n\text4_mb_release(sb);\n\text4_ext_release(sb);\n\text4_xattr_put_super(sb);\n\n\tif (!(sb->s_flags & MS_RDONLY)) {\n\t\tEXT4_CLEAR_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);\n\t\tes->s_state = cpu_to_le16(sbi->s_mount_state);\n\t\text4_commit_super(sb, 1);\n\t}\n\tif (sbi->s_proc) {\n\t\tremove_proc_entry(sb->s_id, ext4_proc_root);\n\t}\n\tkobject_del(&sbi->s_kobj);\n\n\tfor (i = 0; i < sbi->s_gdb_count; i++)\n\t\tbrelse(sbi->s_group_desc[i]);\n\tkfree(sbi->s_group_desc);\n\tif (is_vmalloc_addr(sbi->s_flex_groups))\n\t\tvfree(sbi->s_flex_groups);\n\telse\n\t\tkfree(sbi->s_flex_groups);\n\tpercpu_counter_destroy(&sbi->s_freeblocks_counter);\n\tpercpu_counter_destroy(&sbi->s_freeinodes_counter);\n\tpercpu_counter_destroy(&sbi->s_dirs_counter);\n\tpercpu_counter_destroy(&sbi->s_dirtyblocks_counter);\n\tbrelse(sbi->s_sbh);\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(sbi->s_qf_names[i]);\n#endif\n\n\t/* Debugging code just in case the in-memory inode orphan list\n\t * isn't empty.  The on-disk one can be non-empty if we've\n\t * detected an error and taken the fs readonly, but the\n\t * in-memory list had better be clean by this point. */\n\tif (!list_empty(&sbi->s_orphan))\n\t\tdump_orphan_list(sb, sbi);\n\tJ_ASSERT(list_empty(&sbi->s_orphan));\n\n\tinvalidate_bdev(sb->s_bdev);\n\tif (sbi->journal_bdev && sbi->journal_bdev != sb->s_bdev) {\n\t\t/*\n\t\t * Invalidate the journal device's buffers.  We don't want them\n\t\t * floating about in memory - the physical journal device may\n\t\t * hotswapped, and it breaks the `ro-after' testing code.\n\t\t */\n\t\tsync_blockdev(sbi->journal_bdev);\n\t\tinvalidate_bdev(sbi->journal_bdev);\n\t\text4_blkdev_remove(sbi);\n\t}\n\tsb->s_fs_info = NULL;\n\t/*\n\t * Now that we are completely done shutting down the\n\t * superblock, we need to actually destroy the kobject.\n\t */\n\tunlock_kernel();\n\tunlock_super(sb);\n\tkobject_put(&sbi->s_kobj);\n\twait_for_completion(&sbi->s_kobj_unregister);\n\tkfree(sbi->s_blockgroup_lock);\n\tkfree(sbi);\n}\n\nstatic struct kmem_cache *ext4_inode_cachep;\n\n/*\n * Called inside transaction, so use GFP_NOFS\n */\nstatic struct inode *ext4_alloc_inode(struct super_block *sb)\n{\n\tstruct ext4_inode_info *ei;\n\n\tei = kmem_cache_alloc(ext4_inode_cachep, GFP_NOFS);\n\tif (!ei)\n\t\treturn NULL;\n\n\tei->vfs_inode.i_version = 1;\n\tei->vfs_inode.i_data.writeback_index = 0;\n\tmemset(&ei->i_cached_extent, 0, sizeof(struct ext4_ext_cache));\n\tINIT_LIST_HEAD(&ei->i_prealloc_list);\n\tspin_lock_init(&ei->i_prealloc_lock);\n\t/*\n\t * Note:  We can be called before EXT4_SB(sb)->s_journal is set,\n\t * therefore it can be null here.  Don't check it, just initialize\n\t * jinode.\n\t */\n\tjbd2_journal_init_jbd_inode(&ei->jinode, &ei->vfs_inode);\n\tei->i_reserved_data_blocks = 0;\n\tei->i_reserved_meta_blocks = 0;\n\tei->i_allocated_meta_blocks = 0;\n\tei->i_da_metadata_calc_len = 0;\n\tei->i_delalloc_reserved_flag = 0;\n\tspin_lock_init(&(ei->i_block_reservation_lock));\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tINIT_LIST_HEAD(&ei->i_completed_io_list);\n\tspin_lock_init(&ei->i_completed_io_lock);\n\tei->cur_aio_dio = NULL;\n\tei->i_sync_tid = 0;\n\tei->i_datasync_tid = 0;\n\n\treturn &ei->vfs_inode;\n}\n\nstatic void ext4_destroy_inode(struct inode *inode)\n{\n\tif (!list_empty(&(EXT4_I(inode)->i_orphan))) {\n\t\text4_msg(inode->i_sb, KERN_ERR,\n\t\t\t \"Inode %lu (%p): orphan list check failed!\",\n\t\t\t inode->i_ino, EXT4_I(inode));\n\t\tprint_hex_dump(KERN_INFO, \"\", DUMP_PREFIX_ADDRESS, 16, 4,\n\t\t\t\tEXT4_I(inode), sizeof(struct ext4_inode_info),\n\t\t\t\ttrue);\n\t\tdump_stack();\n\t}\n\tkmem_cache_free(ext4_inode_cachep, EXT4_I(inode));\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct ext4_inode_info *ei = (struct ext4_inode_info *) foo;\n\n\tINIT_LIST_HEAD(&ei->i_orphan);\n#ifdef CONFIG_EXT4_FS_XATTR\n\tinit_rwsem(&ei->xattr_sem);\n#endif\n\tinit_rwsem(&ei->i_data_sem);\n\tinode_init_once(&ei->vfs_inode);\n}\n\nstatic int init_inodecache(void)\n{\n\text4_inode_cachep = kmem_cache_create(\"ext4_inode_cache\",\n\t\t\t\t\t     sizeof(struct ext4_inode_info),\n\t\t\t\t\t     0, (SLAB_RECLAIM_ACCOUNT|\n\t\t\t\t\t\tSLAB_MEM_SPREAD),\n\t\t\t\t\t     init_once);\n\tif (ext4_inode_cachep == NULL)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void destroy_inodecache(void)\n{\n\tkmem_cache_destroy(ext4_inode_cachep);\n}\n\nstatic void ext4_clear_inode(struct inode *inode)\n{\n\text4_discard_preallocations(inode);\n\tif (EXT4_JOURNAL(inode))\n\t\tjbd2_journal_release_jbd_inode(EXT4_SB(inode->i_sb)->s_journal,\n\t\t\t\t       &EXT4_I(inode)->jinode);\n}\n\nstatic inline void ext4_show_quota_options(struct seq_file *seq,\n\t\t\t\t\t   struct super_block *sb)\n{\n#if defined(CONFIG_QUOTA)\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (sbi->s_jquota_fmt) {\n\t\tchar *fmtname = \"\";\n\n\t\tswitch (sbi->s_jquota_fmt) {\n\t\tcase QFMT_VFS_OLD:\n\t\t\tfmtname = \"vfsold\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V0:\n\t\t\tfmtname = \"vfsv0\";\n\t\t\tbreak;\n\t\tcase QFMT_VFS_V1:\n\t\t\tfmtname = \"vfsv1\";\n\t\t\tbreak;\n\t\t}\n\t\tseq_printf(seq, \",jqfmt=%s\", fmtname);\n\t}\n\n\tif (sbi->s_qf_names[USRQUOTA])\n\t\tseq_printf(seq, \",usrjquota=%s\", sbi->s_qf_names[USRQUOTA]);\n\n\tif (sbi->s_qf_names[GRPQUOTA])\n\t\tseq_printf(seq, \",grpjquota=%s\", sbi->s_qf_names[GRPQUOTA]);\n\n\tif (test_opt(sb, USRQUOTA))\n\t\tseq_puts(seq, \",usrquota\");\n\n\tif (test_opt(sb, GRPQUOTA))\n\t\tseq_puts(seq, \",grpquota\");\n#endif\n}\n\n/*\n * Show an option if\n *  - it's set to a non-default value OR\n *  - if the per-sb default is different from the global default\n */\nstatic int ext4_show_options(struct seq_file *seq, struct vfsmount *vfs)\n{\n\tint def_errors;\n\tunsigned long def_mount_opts;\n\tstruct super_block *sb = vfs->mnt_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\n\tdef_mount_opts = le32_to_cpu(es->s_default_mount_opts);\n\tdef_errors     = le16_to_cpu(es->s_errors);\n\n\tif (sbi->s_sb_block != 1)\n\t\tseq_printf(seq, \",sb=%llu\", sbi->s_sb_block);\n\tif (test_opt(sb, MINIX_DF))\n\t\tseq_puts(seq, \",minixdf\");\n\tif (test_opt(sb, GRPID) && !(def_mount_opts & EXT4_DEFM_BSDGROUPS))\n\t\tseq_puts(seq, \",grpid\");\n\tif (!test_opt(sb, GRPID) && (def_mount_opts & EXT4_DEFM_BSDGROUPS))\n\t\tseq_puts(seq, \",nogrpid\");\n\tif (sbi->s_resuid != EXT4_DEF_RESUID ||\n\t    le16_to_cpu(es->s_def_resuid) != EXT4_DEF_RESUID) {\n\t\tseq_printf(seq, \",resuid=%u\", sbi->s_resuid);\n\t}\n\tif (sbi->s_resgid != EXT4_DEF_RESGID ||\n\t    le16_to_cpu(es->s_def_resgid) != EXT4_DEF_RESGID) {\n\t\tseq_printf(seq, \",resgid=%u\", sbi->s_resgid);\n\t}\n\tif (test_opt(sb, ERRORS_RO)) {\n\t\tif (def_errors == EXT4_ERRORS_PANIC ||\n\t\t    def_errors == EXT4_ERRORS_CONTINUE) {\n\t\t\tseq_puts(seq, \",errors=remount-ro\");\n\t\t}\n\t}\n\tif (test_opt(sb, ERRORS_CONT) && def_errors != EXT4_ERRORS_CONTINUE)\n\t\tseq_puts(seq, \",errors=continue\");\n\tif (test_opt(sb, ERRORS_PANIC) && def_errors != EXT4_ERRORS_PANIC)\n\t\tseq_puts(seq, \",errors=panic\");\n\tif (test_opt(sb, NO_UID32) && !(def_mount_opts & EXT4_DEFM_UID16))\n\t\tseq_puts(seq, \",nouid32\");\n\tif (test_opt(sb, DEBUG) && !(def_mount_opts & EXT4_DEFM_DEBUG))\n\t\tseq_puts(seq, \",debug\");\n\tif (test_opt(sb, OLDALLOC))\n\t\tseq_puts(seq, \",oldalloc\");\n#ifdef CONFIG_EXT4_FS_XATTR\n\tif (test_opt(sb, XATTR_USER) &&\n\t\t!(def_mount_opts & EXT4_DEFM_XATTR_USER))\n\t\tseq_puts(seq, \",user_xattr\");\n\tif (!test_opt(sb, XATTR_USER) &&\n\t    (def_mount_opts & EXT4_DEFM_XATTR_USER)) {\n\t\tseq_puts(seq, \",nouser_xattr\");\n\t}\n#endif\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\tif (test_opt(sb, POSIX_ACL) && !(def_mount_opts & EXT4_DEFM_ACL))\n\t\tseq_puts(seq, \",acl\");\n\tif (!test_opt(sb, POSIX_ACL) && (def_mount_opts & EXT4_DEFM_ACL))\n\t\tseq_puts(seq, \",noacl\");\n#endif\n\tif (sbi->s_commit_interval != JBD2_DEFAULT_MAX_COMMIT_AGE*HZ) {\n\t\tseq_printf(seq, \",commit=%u\",\n\t\t\t   (unsigned) (sbi->s_commit_interval / HZ));\n\t}\n\tif (sbi->s_min_batch_time != EXT4_DEF_MIN_BATCH_TIME) {\n\t\tseq_printf(seq, \",min_batch_time=%u\",\n\t\t\t   (unsigned) sbi->s_min_batch_time);\n\t}\n\tif (sbi->s_max_batch_time != EXT4_DEF_MAX_BATCH_TIME) {\n\t\tseq_printf(seq, \",max_batch_time=%u\",\n\t\t\t   (unsigned) sbi->s_min_batch_time);\n\t}\n\n\t/*\n\t * We're changing the default of barrier mount option, so\n\t * let's always display its mount state so it's clear what its\n\t * status is.\n\t */\n\tseq_puts(seq, \",barrier=\");\n\tseq_puts(seq, test_opt(sb, BARRIER) ? \"1\" : \"0\");\n\tif (test_opt(sb, JOURNAL_ASYNC_COMMIT))\n\t\tseq_puts(seq, \",journal_async_commit\");\n\tif (test_opt(sb, NOBH))\n\t\tseq_puts(seq, \",nobh\");\n\tif (test_opt(sb, I_VERSION))\n\t\tseq_puts(seq, \",i_version\");\n\tif (!test_opt(sb, DELALLOC))\n\t\tseq_puts(seq, \",nodelalloc\");\n\n\n\tif (sbi->s_stripe)\n\t\tseq_printf(seq, \",stripe=%lu\", sbi->s_stripe);\n\t/*\n\t * journal mode get enabled in different ways\n\t * So just print the value even if we didn't specify it\n\t */\n\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)\n\t\tseq_puts(seq, \",data=journal\");\n\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA)\n\t\tseq_puts(seq, \",data=ordered\");\n\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_WRITEBACK_DATA)\n\t\tseq_puts(seq, \",data=writeback\");\n\n\tif (sbi->s_inode_readahead_blks != EXT4_DEF_INODE_READAHEAD_BLKS)\n\t\tseq_printf(seq, \",inode_readahead_blks=%u\",\n\t\t\t   sbi->s_inode_readahead_blks);\n\n\tif (test_opt(sb, DATA_ERR_ABORT))\n\t\tseq_puts(seq, \",data_err=abort\");\n\n\tif (test_opt(sb, NO_AUTO_DA_ALLOC))\n\t\tseq_puts(seq, \",noauto_da_alloc\");\n\n\tif (test_opt(sb, DISCARD))\n\t\tseq_puts(seq, \",discard\");\n\n\tif (test_opt(sb, NOLOAD))\n\t\tseq_puts(seq, \",norecovery\");\n\n\tif (test_opt(sb, DIOREAD_NOLOCK))\n\t\tseq_puts(seq, \",dioread_nolock\");\n\n\text4_show_quota_options(seq, sb);\n\n\treturn 0;\n}\n\nstatic struct inode *ext4_nfs_get_inode(struct super_block *sb,\n\t\t\t\t\tu64 ino, u32 generation)\n{\n\tstruct inode *inode;\n\n\tif (ino < EXT4_FIRST_INO(sb) && ino != EXT4_ROOT_INO)\n\t\treturn ERR_PTR(-ESTALE);\n\tif (ino > le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count))\n\t\treturn ERR_PTR(-ESTALE);\n\n\t/* iget isn't really right if the inode is currently unallocated!!\n\t *\n\t * ext4_read_inode will return a bad_inode if the inode had been\n\t * deleted, so we should be safe.\n\t *\n\t * Currently we don't know the generation for parent directory, so\n\t * a generation of 0 means \"accept any\"\n\t */\n\tinode = ext4_iget(sb, ino);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\tif (generation && inode->i_generation != generation) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ESTALE);\n\t}\n\n\treturn inode;\n}\n\nstatic struct dentry *ext4_fh_to_dentry(struct super_block *sb, struct fid *fid,\n\t\t\t\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_dentry(sb, fid, fh_len, fh_type,\n\t\t\t\t    ext4_nfs_get_inode);\n}\n\nstatic struct dentry *ext4_fh_to_parent(struct super_block *sb, struct fid *fid,\n\t\t\t\t\tint fh_len, int fh_type)\n{\n\treturn generic_fh_to_parent(sb, fid, fh_len, fh_type,\n\t\t\t\t    ext4_nfs_get_inode);\n}\n\n/*\n * Try to release metadata pages (indirect blocks, directories) which are\n * mapped via the block device.  Since these pages could have journal heads\n * which would prevent try_to_free_buffers() from freeing them, we must use\n * jbd2 layer's try_to_free_buffers() function to release them.\n */\nstatic int bdev_try_to_free_page(struct super_block *sb, struct page *page,\n\t\t\t\t gfp_t wait)\n{\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\tWARN_ON(PageChecked(page));\n\tif (!page_has_buffers(page))\n\t\treturn 0;\n\tif (journal)\n\t\treturn jbd2_journal_try_to_free_buffers(journal, page,\n\t\t\t\t\t\t\twait & ~__GFP_WAIT);\n\treturn try_to_free_buffers(page);\n}\n\n#ifdef CONFIG_QUOTA\n#define QTYPE2NAME(t) ((t) == USRQUOTA ? \"user\" : \"group\")\n#define QTYPE2MOPT(on, t) ((t) == USRQUOTA?((on)##USRJQUOTA):((on)##GRPJQUOTA))\n\nstatic int ext4_write_dquot(struct dquot *dquot);\nstatic int ext4_acquire_dquot(struct dquot *dquot);\nstatic int ext4_release_dquot(struct dquot *dquot);\nstatic int ext4_mark_dquot_dirty(struct dquot *dquot);\nstatic int ext4_write_info(struct super_block *sb, int type);\nstatic int ext4_quota_on(struct super_block *sb, int type, int format_id,\n\t\t\t\tchar *path, int remount);\nstatic int ext4_quota_on_mount(struct super_block *sb, int type);\nstatic ssize_t ext4_quota_read(struct super_block *sb, int type, char *data,\n\t\t\t       size_t len, loff_t off);\nstatic ssize_t ext4_quota_write(struct super_block *sb, int type,\n\t\t\t\tconst char *data, size_t len, loff_t off);\n\nstatic const struct dquot_operations ext4_quota_operations = {\n\t.initialize\t= dquot_initialize,\n\t.drop\t\t= dquot_drop,\n\t.alloc_space\t= dquot_alloc_space,\n\t.reserve_space\t= dquot_reserve_space,\n\t.claim_space\t= dquot_claim_space,\n\t.release_rsv\t= dquot_release_reserved_space,\n#ifdef CONFIG_QUOTA\n\t.get_reserved_space = ext4_get_reserved_space,\n#endif\n\t.alloc_inode\t= dquot_alloc_inode,\n\t.free_space\t= dquot_free_space,\n\t.free_inode\t= dquot_free_inode,\n\t.transfer\t= dquot_transfer,\n\t.write_dquot\t= ext4_write_dquot,\n\t.acquire_dquot\t= ext4_acquire_dquot,\n\t.release_dquot\t= ext4_release_dquot,\n\t.mark_dirty\t= ext4_mark_dquot_dirty,\n\t.write_info\t= ext4_write_info,\n\t.alloc_dquot\t= dquot_alloc,\n\t.destroy_dquot\t= dquot_destroy,\n};\n\nstatic const struct quotactl_ops ext4_qctl_operations = {\n\t.quota_on\t= ext4_quota_on,\n\t.quota_off\t= vfs_quota_off,\n\t.quota_sync\t= vfs_quota_sync,\n\t.get_info\t= vfs_get_dqinfo,\n\t.set_info\t= vfs_set_dqinfo,\n\t.get_dqblk\t= vfs_get_dqblk,\n\t.set_dqblk\t= vfs_set_dqblk\n};\n#endif\n\nstatic const struct super_operations ext4_sops = {\n\t.alloc_inode\t= ext4_alloc_inode,\n\t.destroy_inode\t= ext4_destroy_inode,\n\t.write_inode\t= ext4_write_inode,\n\t.dirty_inode\t= ext4_dirty_inode,\n\t.delete_inode\t= ext4_delete_inode,\n\t.put_super\t= ext4_put_super,\n\t.sync_fs\t= ext4_sync_fs,\n\t.freeze_fs\t= ext4_freeze,\n\t.unfreeze_fs\t= ext4_unfreeze,\n\t.statfs\t\t= ext4_statfs,\n\t.remount_fs\t= ext4_remount,\n\t.clear_inode\t= ext4_clear_inode,\n\t.show_options\t= ext4_show_options,\n#ifdef CONFIG_QUOTA\n\t.quota_read\t= ext4_quota_read,\n\t.quota_write\t= ext4_quota_write,\n#endif\n\t.bdev_try_to_free_page = bdev_try_to_free_page,\n};\n\nstatic const struct super_operations ext4_nojournal_sops = {\n\t.alloc_inode\t= ext4_alloc_inode,\n\t.destroy_inode\t= ext4_destroy_inode,\n\t.write_inode\t= ext4_write_inode,\n\t.dirty_inode\t= ext4_dirty_inode,\n\t.delete_inode\t= ext4_delete_inode,\n\t.write_super\t= ext4_write_super,\n\t.put_super\t= ext4_put_super,\n\t.statfs\t\t= ext4_statfs,\n\t.remount_fs\t= ext4_remount,\n\t.clear_inode\t= ext4_clear_inode,\n\t.show_options\t= ext4_show_options,\n#ifdef CONFIG_QUOTA\n\t.quota_read\t= ext4_quota_read,\n\t.quota_write\t= ext4_quota_write,\n#endif\n\t.bdev_try_to_free_page = bdev_try_to_free_page,\n};\n\nstatic const struct export_operations ext4_export_ops = {\n\t.fh_to_dentry = ext4_fh_to_dentry,\n\t.fh_to_parent = ext4_fh_to_parent,\n\t.get_parent = ext4_get_parent,\n};\n\nenum {\n\tOpt_bsd_df, Opt_minix_df, Opt_grpid, Opt_nogrpid,\n\tOpt_resgid, Opt_resuid, Opt_sb, Opt_err_cont, Opt_err_panic, Opt_err_ro,\n\tOpt_nouid32, Opt_debug, Opt_oldalloc, Opt_orlov,\n\tOpt_user_xattr, Opt_nouser_xattr, Opt_acl, Opt_noacl,\n\tOpt_auto_da_alloc, Opt_noauto_da_alloc, Opt_noload, Opt_nobh, Opt_bh,\n\tOpt_commit, Opt_min_batch_time, Opt_max_batch_time,\n\tOpt_journal_update, Opt_journal_dev,\n\tOpt_journal_checksum, Opt_journal_async_commit,\n\tOpt_abort, Opt_data_journal, Opt_data_ordered, Opt_data_writeback,\n\tOpt_data_err_abort, Opt_data_err_ignore,\n\tOpt_usrjquota, Opt_grpjquota, Opt_offusrjquota, Opt_offgrpjquota,\n\tOpt_jqfmt_vfsold, Opt_jqfmt_vfsv0, Opt_jqfmt_vfsv1, Opt_quota,\n\tOpt_noquota, Opt_ignore, Opt_barrier, Opt_nobarrier, Opt_err,\n\tOpt_resize, Opt_usrquota, Opt_grpquota, Opt_i_version,\n\tOpt_stripe, Opt_delalloc, Opt_nodelalloc,\n\tOpt_block_validity, Opt_noblock_validity,\n\tOpt_inode_readahead_blks, Opt_journal_ioprio,\n\tOpt_dioread_nolock, Opt_dioread_lock,\n\tOpt_discard, Opt_nodiscard,\n};\n\nstatic const match_table_t tokens = {\n\t{Opt_bsd_df, \"bsddf\"},\n\t{Opt_minix_df, \"minixdf\"},\n\t{Opt_grpid, \"grpid\"},\n\t{Opt_grpid, \"bsdgroups\"},\n\t{Opt_nogrpid, \"nogrpid\"},\n\t{Opt_nogrpid, \"sysvgroups\"},\n\t{Opt_resgid, \"resgid=%u\"},\n\t{Opt_resuid, \"resuid=%u\"},\n\t{Opt_sb, \"sb=%u\"},\n\t{Opt_err_cont, \"errors=continue\"},\n\t{Opt_err_panic, \"errors=panic\"},\n\t{Opt_err_ro, \"errors=remount-ro\"},\n\t{Opt_nouid32, \"nouid32\"},\n\t{Opt_debug, \"debug\"},\n\t{Opt_oldalloc, \"oldalloc\"},\n\t{Opt_orlov, \"orlov\"},\n\t{Opt_user_xattr, \"user_xattr\"},\n\t{Opt_nouser_xattr, \"nouser_xattr\"},\n\t{Opt_acl, \"acl\"},\n\t{Opt_noacl, \"noacl\"},\n\t{Opt_noload, \"noload\"},\n\t{Opt_noload, \"norecovery\"},\n\t{Opt_nobh, \"nobh\"},\n\t{Opt_bh, \"bh\"},\n\t{Opt_commit, \"commit=%u\"},\n\t{Opt_min_batch_time, \"min_batch_time=%u\"},\n\t{Opt_max_batch_time, \"max_batch_time=%u\"},\n\t{Opt_journal_update, \"journal=update\"},\n\t{Opt_journal_dev, \"journal_dev=%u\"},\n\t{Opt_journal_checksum, \"journal_checksum\"},\n\t{Opt_journal_async_commit, \"journal_async_commit\"},\n\t{Opt_abort, \"abort\"},\n\t{Opt_data_journal, \"data=journal\"},\n\t{Opt_data_ordered, \"data=ordered\"},\n\t{Opt_data_writeback, \"data=writeback\"},\n\t{Opt_data_err_abort, \"data_err=abort\"},\n\t{Opt_data_err_ignore, \"data_err=ignore\"},\n\t{Opt_offusrjquota, \"usrjquota=\"},\n\t{Opt_usrjquota, \"usrjquota=%s\"},\n\t{Opt_offgrpjquota, \"grpjquota=\"},\n\t{Opt_grpjquota, \"grpjquota=%s\"},\n\t{Opt_jqfmt_vfsold, \"jqfmt=vfsold\"},\n\t{Opt_jqfmt_vfsv0, \"jqfmt=vfsv0\"},\n\t{Opt_jqfmt_vfsv1, \"jqfmt=vfsv1\"},\n\t{Opt_grpquota, \"grpquota\"},\n\t{Opt_noquota, \"noquota\"},\n\t{Opt_quota, \"quota\"},\n\t{Opt_usrquota, \"usrquota\"},\n\t{Opt_barrier, \"barrier=%u\"},\n\t{Opt_barrier, \"barrier\"},\n\t{Opt_nobarrier, \"nobarrier\"},\n\t{Opt_i_version, \"i_version\"},\n\t{Opt_stripe, \"stripe=%u\"},\n\t{Opt_resize, \"resize\"},\n\t{Opt_delalloc, \"delalloc\"},\n\t{Opt_nodelalloc, \"nodelalloc\"},\n\t{Opt_block_validity, \"block_validity\"},\n\t{Opt_noblock_validity, \"noblock_validity\"},\n\t{Opt_inode_readahead_blks, \"inode_readahead_blks=%u\"},\n\t{Opt_journal_ioprio, \"journal_ioprio=%u\"},\n\t{Opt_auto_da_alloc, \"auto_da_alloc=%u\"},\n\t{Opt_auto_da_alloc, \"auto_da_alloc\"},\n\t{Opt_noauto_da_alloc, \"noauto_da_alloc\"},\n\t{Opt_dioread_nolock, \"dioread_nolock\"},\n\t{Opt_dioread_lock, \"dioread_lock\"},\n\t{Opt_discard, \"discard\"},\n\t{Opt_nodiscard, \"nodiscard\"},\n\t{Opt_err, NULL},\n};\n\nstatic ext4_fsblk_t get_sb_block(void **data)\n{\n\text4_fsblk_t\tsb_block;\n\tchar\t\t*options = (char *) *data;\n\n\tif (!options || strncmp(options, \"sb=\", 3) != 0)\n\t\treturn 1;\t/* Default location */\n\n\toptions += 3;\n\t/* TODO: use simple_strtoll with >32bit ext4 */\n\tsb_block = simple_strtoul(options, &options, 0);\n\tif (*options && *options != ',') {\n\t\tprintk(KERN_ERR \"EXT4-fs: Invalid sb specification: %s\\n\",\n\t\t       (char *) *data);\n\t\treturn 1;\n\t}\n\tif (*options == ',')\n\t\toptions++;\n\t*data = (void *) options;\n\n\treturn sb_block;\n}\n\n#define DEFAULT_JOURNAL_IOPRIO (IOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE, 3))\nstatic char deprecated_msg[] = \"Mount option \\\"%s\\\" will be removed by %s\\n\"\n\t\"Contact linux-ext4@vger.kernel.org if you think we should keep it.\\n\";\n\n#ifdef CONFIG_QUOTA\nstatic int set_qf_name(struct super_block *sb, int qtype, substring_t *args)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *qname;\n\n\tif (sb_any_quota_loaded(sb) &&\n\t\t!sbi->s_qf_names[qtype]) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Cannot change journaled \"\n\t\t\t\"quota options when quota turned on\");\n\t\treturn 0;\n\t}\n\tqname = match_strdup(args);\n\tif (!qname) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Not enough memory for storing quotafile name\");\n\t\treturn 0;\n\t}\n\tif (sbi->s_qf_names[qtype] &&\n\t\tstrcmp(sbi->s_qf_names[qtype], qname)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"%s quota file already specified\", QTYPE2NAME(qtype));\n\t\tkfree(qname);\n\t\treturn 0;\n\t}\n\tsbi->s_qf_names[qtype] = qname;\n\tif (strchr(sbi->s_qf_names[qtype], '/')) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"quotafile must be on filesystem root\");\n\t\tkfree(sbi->s_qf_names[qtype]);\n\t\tsbi->s_qf_names[qtype] = NULL;\n\t\treturn 0;\n\t}\n\tset_opt(sbi->s_mount_opt, QUOTA);\n\treturn 1;\n}\n\nstatic int clear_qf_name(struct super_block *sb, int qtype)\n{\n\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (sb_any_quota_loaded(sb) &&\n\t\tsbi->s_qf_names[qtype]) {\n\t\text4_msg(sb, KERN_ERR, \"Cannot change journaled quota options\"\n\t\t\t\" when quota turned on\");\n\t\treturn 0;\n\t}\n\t/*\n\t * The space will be released later when all options are confirmed\n\t * to be correct\n\t */\n\tsbi->s_qf_names[qtype] = NULL;\n\treturn 1;\n}\n#endif\n\nstatic int parse_options(char *options, struct super_block *sb,\n\t\t\t unsigned long *journal_devnum,\n\t\t\t unsigned int *journal_ioprio,\n\t\t\t ext4_fsblk_t *n_blocks_count, int is_remount)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tchar *p;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint data_opt = 0;\n\tint option;\n#ifdef CONFIG_QUOTA\n\tint qfmt;\n#endif\n\n\tif (!options)\n\t\treturn 1;\n\n\twhile ((p = strsep(&options, \",\")) != NULL) {\n\t\tint token;\n\t\tif (!*p)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Initialize args struct so we know whether arg was\n\t\t * found; some options take optional arguments.\n\t\t */\n\t\targs[0].to = args[0].from = 0;\n\t\ttoken = match_token(p, tokens, args);\n\t\tswitch (token) {\n\t\tcase Opt_bsd_df:\n\t\t\text4_msg(sb, KERN_WARNING, deprecated_msg, p, \"2.6.38\");\n\t\t\tclear_opt(sbi->s_mount_opt, MINIX_DF);\n\t\t\tbreak;\n\t\tcase Opt_minix_df:\n\t\t\text4_msg(sb, KERN_WARNING, deprecated_msg, p, \"2.6.38\");\n\t\t\tset_opt(sbi->s_mount_opt, MINIX_DF);\n\n\t\t\tbreak;\n\t\tcase Opt_grpid:\n\t\t\text4_msg(sb, KERN_WARNING, deprecated_msg, p, \"2.6.38\");\n\t\t\tset_opt(sbi->s_mount_opt, GRPID);\n\n\t\t\tbreak;\n\t\tcase Opt_nogrpid:\n\t\t\text4_msg(sb, KERN_WARNING, deprecated_msg, p, \"2.6.38\");\n\t\t\tclear_opt(sbi->s_mount_opt, GRPID);\n\n\t\t\tbreak;\n\t\tcase Opt_resuid:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tsbi->s_resuid = option;\n\t\t\tbreak;\n\t\tcase Opt_resgid:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tsbi->s_resgid = option;\n\t\t\tbreak;\n\t\tcase Opt_sb:\n\t\t\t/* handled by get_sb_block() instead of here */\n\t\t\t/* *sb_block = match_int(&args[0]); */\n\t\t\tbreak;\n\t\tcase Opt_err_panic:\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_CONT);\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_RO);\n\t\t\tset_opt(sbi->s_mount_opt, ERRORS_PANIC);\n\t\t\tbreak;\n\t\tcase Opt_err_ro:\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_CONT);\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_PANIC);\n\t\t\tset_opt(sbi->s_mount_opt, ERRORS_RO);\n\t\t\tbreak;\n\t\tcase Opt_err_cont:\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_RO);\n\t\t\tclear_opt(sbi->s_mount_opt, ERRORS_PANIC);\n\t\t\tset_opt(sbi->s_mount_opt, ERRORS_CONT);\n\t\t\tbreak;\n\t\tcase Opt_nouid32:\n\t\t\tset_opt(sbi->s_mount_opt, NO_UID32);\n\t\t\tbreak;\n\t\tcase Opt_debug:\n\t\t\tset_opt(sbi->s_mount_opt, DEBUG);\n\t\t\tbreak;\n\t\tcase Opt_oldalloc:\n\t\t\tset_opt(sbi->s_mount_opt, OLDALLOC);\n\t\t\tbreak;\n\t\tcase Opt_orlov:\n\t\t\tclear_opt(sbi->s_mount_opt, OLDALLOC);\n\t\t\tbreak;\n#ifdef CONFIG_EXT4_FS_XATTR\n\t\tcase Opt_user_xattr:\n\t\t\tset_opt(sbi->s_mount_opt, XATTR_USER);\n\t\t\tbreak;\n\t\tcase Opt_nouser_xattr:\n\t\t\tclear_opt(sbi->s_mount_opt, XATTR_USER);\n\t\t\tbreak;\n#else\n\t\tcase Opt_user_xattr:\n\t\tcase Opt_nouser_xattr:\n\t\t\text4_msg(sb, KERN_ERR, \"(no)user_xattr options not supported\");\n\t\t\tbreak;\n#endif\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\t\tcase Opt_acl:\n\t\t\tset_opt(sbi->s_mount_opt, POSIX_ACL);\n\t\t\tbreak;\n\t\tcase Opt_noacl:\n\t\t\tclear_opt(sbi->s_mount_opt, POSIX_ACL);\n\t\t\tbreak;\n#else\n\t\tcase Opt_acl:\n\t\tcase Opt_noacl:\n\t\t\text4_msg(sb, KERN_ERR, \"(no)acl options not supported\");\n\t\t\tbreak;\n#endif\n\t\tcase Opt_journal_update:\n\t\t\t/* @@@ FIXME */\n\t\t\t/* Eventually we will want to be able to create\n\t\t\t   a journal file here.  For now, only allow the\n\t\t\t   user to specify an existing inode to be the\n\t\t\t   journal file. */\n\t\t\tif (is_remount) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t \"Cannot specify journal on remount\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tset_opt(sbi->s_mount_opt, UPDATE_JOURNAL);\n\t\t\tbreak;\n\t\tcase Opt_journal_dev:\n\t\t\tif (is_remount) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\"Cannot specify journal on remount\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\t*journal_devnum = option;\n\t\t\tbreak;\n\t\tcase Opt_journal_checksum:\n\t\t\tset_opt(sbi->s_mount_opt, JOURNAL_CHECKSUM);\n\t\t\tbreak;\n\t\tcase Opt_journal_async_commit:\n\t\t\tset_opt(sbi->s_mount_opt, JOURNAL_ASYNC_COMMIT);\n\t\t\tset_opt(sbi->s_mount_opt, JOURNAL_CHECKSUM);\n\t\t\tbreak;\n\t\tcase Opt_noload:\n\t\t\tset_opt(sbi->s_mount_opt, NOLOAD);\n\t\t\tbreak;\n\t\tcase Opt_commit:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0)\n\t\t\t\treturn 0;\n\t\t\tif (option == 0)\n\t\t\t\toption = JBD2_DEFAULT_MAX_COMMIT_AGE;\n\t\t\tsbi->s_commit_interval = HZ * option;\n\t\t\tbreak;\n\t\tcase Opt_max_batch_time:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0)\n\t\t\t\treturn 0;\n\t\t\tif (option == 0)\n\t\t\t\toption = EXT4_DEF_MAX_BATCH_TIME;\n\t\t\tsbi->s_max_batch_time = option;\n\t\t\tbreak;\n\t\tcase Opt_min_batch_time:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0)\n\t\t\t\treturn 0;\n\t\t\tsbi->s_min_batch_time = option;\n\t\t\tbreak;\n\t\tcase Opt_data_journal:\n\t\t\tdata_opt = EXT4_MOUNT_JOURNAL_DATA;\n\t\t\tgoto datacheck;\n\t\tcase Opt_data_ordered:\n\t\t\tdata_opt = EXT4_MOUNT_ORDERED_DATA;\n\t\t\tgoto datacheck;\n\t\tcase Opt_data_writeback:\n\t\t\tdata_opt = EXT4_MOUNT_WRITEBACK_DATA;\n\t\tdatacheck:\n\t\t\tif (is_remount) {\n\t\t\t\tif (test_opt(sb, DATA_FLAGS) != data_opt) {\n\t\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\t\"Cannot change data mode on remount\");\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tclear_opt(sbi->s_mount_opt, DATA_FLAGS);\n\t\t\t\tsbi->s_mount_opt |= data_opt;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Opt_data_err_abort:\n\t\t\tset_opt(sbi->s_mount_opt, DATA_ERR_ABORT);\n\t\t\tbreak;\n\t\tcase Opt_data_err_ignore:\n\t\t\tclear_opt(sbi->s_mount_opt, DATA_ERR_ABORT);\n\t\t\tbreak;\n#ifdef CONFIG_QUOTA\n\t\tcase Opt_usrjquota:\n\t\t\tif (!set_qf_name(sb, USRQUOTA, &args[0]))\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tcase Opt_grpjquota:\n\t\t\tif (!set_qf_name(sb, GRPQUOTA, &args[0]))\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tcase Opt_offusrjquota:\n\t\t\tif (!clear_qf_name(sb, USRQUOTA))\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tcase Opt_offgrpjquota:\n\t\t\tif (!clear_qf_name(sb, GRPQUOTA))\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\n\t\tcase Opt_jqfmt_vfsold:\n\t\t\tqfmt = QFMT_VFS_OLD;\n\t\t\tgoto set_qf_format;\n\t\tcase Opt_jqfmt_vfsv0:\n\t\t\tqfmt = QFMT_VFS_V0;\n\t\t\tgoto set_qf_format;\n\t\tcase Opt_jqfmt_vfsv1:\n\t\t\tqfmt = QFMT_VFS_V1;\nset_qf_format:\n\t\t\tif (sb_any_quota_loaded(sb) &&\n\t\t\t    sbi->s_jquota_fmt != qfmt) {\n\t\t\t\text4_msg(sb, KERN_ERR, \"Cannot change \"\n\t\t\t\t\t\"journaled quota options when \"\n\t\t\t\t\t\"quota turned on\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tsbi->s_jquota_fmt = qfmt;\n\t\t\tbreak;\n\t\tcase Opt_quota:\n\t\tcase Opt_usrquota:\n\t\t\tset_opt(sbi->s_mount_opt, QUOTA);\n\t\t\tset_opt(sbi->s_mount_opt, USRQUOTA);\n\t\t\tbreak;\n\t\tcase Opt_grpquota:\n\t\t\tset_opt(sbi->s_mount_opt, QUOTA);\n\t\t\tset_opt(sbi->s_mount_opt, GRPQUOTA);\n\t\t\tbreak;\n\t\tcase Opt_noquota:\n\t\t\tif (sb_any_quota_loaded(sb)) {\n\t\t\t\text4_msg(sb, KERN_ERR, \"Cannot change quota \"\n\t\t\t\t\t\"options when quota turned on\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tclear_opt(sbi->s_mount_opt, QUOTA);\n\t\t\tclear_opt(sbi->s_mount_opt, USRQUOTA);\n\t\t\tclear_opt(sbi->s_mount_opt, GRPQUOTA);\n\t\t\tbreak;\n#else\n\t\tcase Opt_quota:\n\t\tcase Opt_usrquota:\n\t\tcase Opt_grpquota:\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\"quota options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_usrjquota:\n\t\tcase Opt_grpjquota:\n\t\tcase Opt_offusrjquota:\n\t\tcase Opt_offgrpjquota:\n\t\tcase Opt_jqfmt_vfsold:\n\t\tcase Opt_jqfmt_vfsv0:\n\t\tcase Opt_jqfmt_vfsv1:\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\"journaled quota options not supported\");\n\t\t\tbreak;\n\t\tcase Opt_noquota:\n\t\t\tbreak;\n#endif\n\t\tcase Opt_abort:\n\t\t\tsbi->s_mount_flags |= EXT4_MF_FS_ABORTED;\n\t\t\tbreak;\n\t\tcase Opt_nobarrier:\n\t\t\tclear_opt(sbi->s_mount_opt, BARRIER);\n\t\t\tbreak;\n\t\tcase Opt_barrier:\n\t\t\tif (args[0].from) {\n\t\t\t\tif (match_int(&args[0], &option))\n\t\t\t\t\treturn 0;\n\t\t\t} else\n\t\t\t\toption = 1;\t/* No argument, default to 1 */\n\t\t\tif (option)\n\t\t\t\tset_opt(sbi->s_mount_opt, BARRIER);\n\t\t\telse\n\t\t\t\tclear_opt(sbi->s_mount_opt, BARRIER);\n\t\t\tbreak;\n\t\tcase Opt_ignore:\n\t\t\tbreak;\n\t\tcase Opt_resize:\n\t\t\tif (!is_remount) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\"resize option only available \"\n\t\t\t\t\t\"for remount\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tif (match_int(&args[0], &option) != 0)\n\t\t\t\treturn 0;\n\t\t\t*n_blocks_count = option;\n\t\t\tbreak;\n\t\tcase Opt_nobh:\n\t\t\tset_opt(sbi->s_mount_opt, NOBH);\n\t\t\tbreak;\n\t\tcase Opt_bh:\n\t\t\tclear_opt(sbi->s_mount_opt, NOBH);\n\t\t\tbreak;\n\t\tcase Opt_i_version:\n\t\t\tset_opt(sbi->s_mount_opt, I_VERSION);\n\t\t\tsb->s_flags |= MS_I_VERSION;\n\t\t\tbreak;\n\t\tcase Opt_nodelalloc:\n\t\t\tclear_opt(sbi->s_mount_opt, DELALLOC);\n\t\t\tbreak;\n\t\tcase Opt_stripe:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0)\n\t\t\t\treturn 0;\n\t\t\tsbi->s_stripe = option;\n\t\t\tbreak;\n\t\tcase Opt_delalloc:\n\t\t\tset_opt(sbi->s_mount_opt, DELALLOC);\n\t\t\tbreak;\n\t\tcase Opt_block_validity:\n\t\t\tset_opt(sbi->s_mount_opt, BLOCK_VALIDITY);\n\t\t\tbreak;\n\t\tcase Opt_noblock_validity:\n\t\t\tclear_opt(sbi->s_mount_opt, BLOCK_VALIDITY);\n\t\t\tbreak;\n\t\tcase Opt_inode_readahead_blks:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0 || option > (1 << 30))\n\t\t\t\treturn 0;\n\t\t\tif (!is_power_of_2(option)) {\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t \"EXT4-fs: inode_readahead_blks\"\n\t\t\t\t\t \" must be a power of 2\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tsbi->s_inode_readahead_blks = option;\n\t\t\tbreak;\n\t\tcase Opt_journal_ioprio:\n\t\t\tif (match_int(&args[0], &option))\n\t\t\t\treturn 0;\n\t\t\tif (option < 0 || option > 7)\n\t\t\t\tbreak;\n\t\t\t*journal_ioprio = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE,\n\t\t\t\t\t\t\t    option);\n\t\t\tbreak;\n\t\tcase Opt_noauto_da_alloc:\n\t\t\tset_opt(sbi->s_mount_opt,NO_AUTO_DA_ALLOC);\n\t\t\tbreak;\n\t\tcase Opt_auto_da_alloc:\n\t\t\tif (args[0].from) {\n\t\t\t\tif (match_int(&args[0], &option))\n\t\t\t\t\treturn 0;\n\t\t\t} else\n\t\t\t\toption = 1;\t/* No argument, default to 1 */\n\t\t\tif (option)\n\t\t\t\tclear_opt(sbi->s_mount_opt, NO_AUTO_DA_ALLOC);\n\t\t\telse\n\t\t\t\tset_opt(sbi->s_mount_opt,NO_AUTO_DA_ALLOC);\n\t\t\tbreak;\n\t\tcase Opt_discard:\n\t\t\tset_opt(sbi->s_mount_opt, DISCARD);\n\t\t\tbreak;\n\t\tcase Opt_nodiscard:\n\t\t\tclear_opt(sbi->s_mount_opt, DISCARD);\n\t\t\tbreak;\n\t\tcase Opt_dioread_nolock:\n\t\t\tset_opt(sbi->s_mount_opt, DIOREAD_NOLOCK);\n\t\t\tbreak;\n\t\tcase Opt_dioread_lock:\n\t\t\tclear_opt(sbi->s_mount_opt, DIOREAD_NOLOCK);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"Unrecognized mount option \\\"%s\\\" \"\n\t\t\t       \"or missing value\", p);\n\t\t\treturn 0;\n\t\t}\n\t}\n#ifdef CONFIG_QUOTA\n\tif (sbi->s_qf_names[USRQUOTA] || sbi->s_qf_names[GRPQUOTA]) {\n\t\tif (test_opt(sb, USRQUOTA) && sbi->s_qf_names[USRQUOTA])\n\t\t\tclear_opt(sbi->s_mount_opt, USRQUOTA);\n\n\t\tif (test_opt(sb, GRPQUOTA) && sbi->s_qf_names[GRPQUOTA])\n\t\t\tclear_opt(sbi->s_mount_opt, GRPQUOTA);\n\n\t\tif (test_opt(sb, GRPQUOTA) || test_opt(sb, USRQUOTA)) {\n\t\t\text4_msg(sb, KERN_ERR, \"old and new quota \"\n\t\t\t\t\t\"format mixing\");\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!sbi->s_jquota_fmt) {\n\t\t\text4_msg(sb, KERN_ERR, \"journaled quota format \"\n\t\t\t\t\t\"not specified\");\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tif (sbi->s_jquota_fmt) {\n\t\t\text4_msg(sb, KERN_ERR, \"journaled quota format \"\n\t\t\t\t\t\"specified with no journaling \"\n\t\t\t\t\t\"enabled\");\n\t\t\treturn 0;\n\t\t}\n\t}\n#endif\n\treturn 1;\n}\n\nstatic int ext4_setup_super(struct super_block *sb, struct ext4_super_block *es,\n\t\t\t    int read_only)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tint res = 0;\n\n\tif (le32_to_cpu(es->s_rev_level) > EXT4_MAX_SUPP_REV) {\n\t\text4_msg(sb, KERN_ERR, \"revision level too high, \"\n\t\t\t \"forcing read-only mode\");\n\t\tres = MS_RDONLY;\n\t}\n\tif (read_only)\n\t\treturn res;\n\tif (!(sbi->s_mount_state & EXT4_VALID_FS))\n\t\text4_msg(sb, KERN_WARNING, \"warning: mounting unchecked fs, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if ((sbi->s_mount_state & EXT4_ERROR_FS))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: mounting fs with errors, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if ((__s16) le16_to_cpu(es->s_max_mnt_count) >= 0 &&\n\t\t le16_to_cpu(es->s_mnt_count) >=\n\t\t (unsigned short) (__s16) le16_to_cpu(es->s_max_mnt_count))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: maximal mount count reached, \"\n\t\t\t \"running e2fsck is recommended\");\n\telse if (le32_to_cpu(es->s_checkinterval) &&\n\t\t(le32_to_cpu(es->s_lastcheck) +\n\t\t\tle32_to_cpu(es->s_checkinterval) <= get_seconds()))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t\t \"warning: checktime reached, \"\n\t\t\t \"running e2fsck is recommended\");\n\tif (!sbi->s_journal)\n\t\tes->s_state &= cpu_to_le16(~EXT4_VALID_FS);\n\tif (!(__s16) le16_to_cpu(es->s_max_mnt_count))\n\t\tes->s_max_mnt_count = cpu_to_le16(EXT4_DFL_MAX_MNT_COUNT);\n\tle16_add_cpu(&es->s_mnt_count, 1);\n\tes->s_mtime = cpu_to_le32(get_seconds());\n\text4_update_dynamic_rev(sb);\n\tif (sbi->s_journal)\n\t\tEXT4_SET_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);\n\n\text4_commit_super(sb, 1);\n\tif (test_opt(sb, DEBUG))\n\t\tprintk(KERN_INFO \"[EXT4 FS bs=%lu, gc=%u, \"\n\t\t\t\t\"bpg=%lu, ipg=%lu, mo=%04x]\\n\",\n\t\t\tsb->s_blocksize,\n\t\t\tsbi->s_groups_count,\n\t\t\tEXT4_BLOCKS_PER_GROUP(sb),\n\t\t\tEXT4_INODES_PER_GROUP(sb),\n\t\t\tsbi->s_mount_opt);\n\n\treturn res;\n}\n\nstatic int ext4_fill_flex_info(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp = NULL;\n\text4_group_t flex_group_count;\n\text4_group_t flex_group;\n\tint groups_per_flex = 0;\n\tsize_t size;\n\tint i;\n\n\tsbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;\n\tgroups_per_flex = 1 << sbi->s_log_groups_per_flex;\n\n\tif (groups_per_flex < 2) {\n\t\tsbi->s_log_groups_per_flex = 0;\n\t\treturn 1;\n\t}\n\n\t/* We allocate both existing and potentially added groups */\n\tflex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +\n\t\t\t((le16_to_cpu(sbi->s_es->s_reserved_gdt_blocks) + 1) <<\n\t\t\t      EXT4_DESC_PER_BLOCK_BITS(sb))) / groups_per_flex;\n\tsize = flex_group_count * sizeof(struct flex_groups);\n\tsbi->s_flex_groups = kzalloc(size, GFP_KERNEL);\n\tif (sbi->s_flex_groups == NULL) {\n\t\tsbi->s_flex_groups = vmalloc(size);\n\t\tif (sbi->s_flex_groups)\n\t\t\tmemset(sbi->s_flex_groups, 0, size);\n\t}\n\tif (sbi->s_flex_groups == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory for \"\n\t\t\t\t\"%u flex groups\", flex_group_count);\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tflex_group = ext4_flex_group(sbi, i);\n\t\tatomic_add(ext4_free_inodes_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_inodes);\n\t\tatomic_add(ext4_free_blks_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].free_blocks);\n\t\tatomic_add(ext4_used_dirs_count(sb, gdp),\n\t\t\t   &sbi->s_flex_groups[flex_group].used_dirs);\n\t}\n\n\treturn 1;\nfailed:\n\treturn 0;\n}\n\n__le16 ext4_group_desc_csum(struct ext4_sb_info *sbi, __u32 block_group,\n\t\t\t    struct ext4_group_desc *gdp)\n{\n\t__u16 crc = 0;\n\n\tif (sbi->s_es->s_feature_ro_compat &\n\t    cpu_to_le32(EXT4_FEATURE_RO_COMPAT_GDT_CSUM)) {\n\t\tint offset = offsetof(struct ext4_group_desc, bg_checksum);\n\t\t__le32 le_group = cpu_to_le32(block_group);\n\n\t\tcrc = crc16(~0, sbi->s_es->s_uuid, sizeof(sbi->s_es->s_uuid));\n\t\tcrc = crc16(crc, (__u8 *)&le_group, sizeof(le_group));\n\t\tcrc = crc16(crc, (__u8 *)gdp, offset);\n\t\toffset += sizeof(gdp->bg_checksum); /* skip checksum */\n\t\t/* for checksum of struct ext4_group_desc do the rest...*/\n\t\tif ((sbi->s_es->s_feature_incompat &\n\t\t     cpu_to_le32(EXT4_FEATURE_INCOMPAT_64BIT)) &&\n\t\t    offset < le16_to_cpu(sbi->s_es->s_desc_size))\n\t\t\tcrc = crc16(crc, (__u8 *)gdp + offset,\n\t\t\t\t    le16_to_cpu(sbi->s_es->s_desc_size) -\n\t\t\t\t\toffset);\n\t}\n\n\treturn cpu_to_le16(crc);\n}\n\nint ext4_group_desc_csum_verify(struct ext4_sb_info *sbi, __u32 block_group,\n\t\t\t\tstruct ext4_group_desc *gdp)\n{\n\tif ((sbi->s_es->s_feature_ro_compat &\n\t     cpu_to_le32(EXT4_FEATURE_RO_COMPAT_GDT_CSUM)) &&\n\t    (gdp->bg_checksum != ext4_group_desc_csum(sbi, block_group, gdp)))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/* Called at mount-time, super-block is locked */\nstatic int ext4_check_descriptors(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t first_block = le32_to_cpu(sbi->s_es->s_first_data_block);\n\text4_fsblk_t last_block;\n\text4_fsblk_t block_bitmap;\n\text4_fsblk_t inode_bitmap;\n\text4_fsblk_t inode_table;\n\tint flexbg_flag = 0;\n\text4_group_t i;\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG))\n\t\tflexbg_flag = 1;\n\n\text4_debug(\"Checking group descriptors\");\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tif (i == sbi->s_groups_count - 1 || flexbg_flag)\n\t\t\tlast_block = ext4_blocks_count(sbi->s_es) - 1;\n\t\telse\n\t\t\tlast_block = first_block +\n\t\t\t\t(EXT4_BLOCKS_PER_GROUP(sb) - 1);\n\n\t\tblock_bitmap = ext4_block_bitmap(sb, gdp);\n\t\tif (block_bitmap < first_block || block_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Block bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, block_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_bitmap = ext4_inode_bitmap(sb, gdp);\n\t\tif (inode_bitmap < first_block || inode_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_table = ext4_inode_table(sb, gdp);\n\t\tif (inode_table < first_block ||\n\t\t    inode_table + sbi->s_itb_per_group - 1 > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode table for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_table);\n\t\t\treturn 0;\n\t\t}\n\t\text4_lock_group(sb, i);\n\t\tif (!ext4_group_desc_csum_verify(sbi, i, gdp)) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Checksum for group %u failed (%u!=%u)\",\n\t\t\t\t i, le16_to_cpu(ext4_group_desc_csum(sbi, i,\n\t\t\t\t     gdp)), le16_to_cpu(gdp->bg_checksum));\n\t\t\tif (!(sb->s_flags & MS_RDONLY)) {\n\t\t\t\text4_unlock_group(sb, i);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, i);\n\t\tif (!flexbg_flag)\n\t\t\tfirst_block += EXT4_BLOCKS_PER_GROUP(sb);\n\t}\n\n\text4_free_blocks_count_set(sbi->s_es, ext4_count_free_blocks(sb));\n\tsbi->s_es->s_free_inodes_count =cpu_to_le32(ext4_count_free_inodes(sb));\n\treturn 1;\n}\n\n/* ext4_orphan_cleanup() walks a singly-linked list of inodes (starting at\n * the superblock) which were deleted from all directories, but held open by\n * a process at the time of a crash.  We walk the list and try to delete these\n * inodes at recovery time (only with a read-write filesystem).\n *\n * In order to keep the orphan inode chain consistent during traversal (in\n * case of crash during recovery), we link each inode into the superblock\n * orphan list_head and handle it the same way as an inode deletion during\n * normal operation (which journals the operations for us).\n *\n * We only do an iget() and an iput() on each inode, which is very safe if we\n * accidentally point at an in-use or already deleted inode.  The worst that\n * can happen in this case is that we get a \"bit already cleared\" message from\n * ext4_free_inode().  The only reason we would point at a wrong inode is if\n * e2fsck was run on this filesystem, and it must have already done the orphan\n * inode cleanup for us, so we can safely abort without any further action.\n */\nstatic void ext4_orphan_cleanup(struct super_block *sb,\n\t\t\t\tstruct ext4_super_block *es)\n{\n\tunsigned int s_flags = sb->s_flags;\n\tint nr_orphans = 0, nr_truncates = 0;\n#ifdef CONFIG_QUOTA\n\tint i;\n#endif\n\tif (!es->s_last_orphan) {\n\t\tjbd_debug(4, \"no orphan inodes to clean up\\n\");\n\t\treturn;\n\t}\n\n\tif (bdev_read_only(sb->s_bdev)) {\n\t\text4_msg(sb, KERN_ERR, \"write access \"\n\t\t\t\"unavailable, skipping orphan cleanup\");\n\t\treturn;\n\t}\n\n\tif (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) {\n\t\tif (es->s_last_orphan)\n\t\t\tjbd_debug(1, \"Errors on filesystem, \"\n\t\t\t\t  \"clearing orphan list.\\n\");\n\t\tes->s_last_orphan = 0;\n\t\tjbd_debug(1, \"Skipping orphan recovery on fs with errors.\\n\");\n\t\treturn;\n\t}\n\n\tif (s_flags & MS_RDONLY) {\n\t\text4_msg(sb, KERN_INFO, \"orphan cleanup on readonly fs\");\n\t\tsb->s_flags &= ~MS_RDONLY;\n\t}\n#ifdef CONFIG_QUOTA\n\t/* Needed for iput() to work correctly and not trash data */\n\tsb->s_flags |= MS_ACTIVE;\n\t/* Turn on quotas so that they are updated correctly */\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tif (EXT4_SB(sb)->s_qf_names[i]) {\n\t\t\tint ret = ext4_quota_on_mount(sb, i);\n\t\t\tif (ret < 0)\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\"Cannot turn on journaled \"\n\t\t\t\t\t\"quota: error %d\", ret);\n\t\t}\n\t}\n#endif\n\n\twhile (es->s_last_orphan) {\n\t\tstruct inode *inode;\n\n\t\tinode = ext4_orphan_get(sb, le32_to_cpu(es->s_last_orphan));\n\t\tif (IS_ERR(inode)) {\n\t\t\tes->s_last_orphan = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_add(&EXT4_I(inode)->i_orphan, &EXT4_SB(sb)->s_orphan);\n\t\tvfs_dq_init(inode);\n\t\tif (inode->i_nlink) {\n\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\"%s: truncating inode %lu to %lld bytes\",\n\t\t\t\t__func__, inode->i_ino, inode->i_size);\n\t\t\tjbd_debug(2, \"truncating inode %lu to %lld bytes\\n\",\n\t\t\t\t  inode->i_ino, inode->i_size);\n\t\t\text4_truncate(inode);\n\t\t\tnr_truncates++;\n\t\t} else {\n\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\"%s: deleting unreferenced inode %lu\",\n\t\t\t\t__func__, inode->i_ino);\n\t\t\tjbd_debug(2, \"deleting unreferenced inode %lu\\n\",\n\t\t\t\t  inode->i_ino);\n\t\t\tnr_orphans++;\n\t\t}\n\t\tiput(inode);  /* The delete magic happens here! */\n\t}\n\n#define PLURAL(x) (x), ((x) == 1) ? \"\" : \"s\"\n\n\tif (nr_orphans)\n\t\text4_msg(sb, KERN_INFO, \"%d orphan inode%s deleted\",\n\t\t       PLURAL(nr_orphans));\n\tif (nr_truncates)\n\t\text4_msg(sb, KERN_INFO, \"%d truncate%s cleaned up\",\n\t\t       PLURAL(nr_truncates));\n#ifdef CONFIG_QUOTA\n\t/* Turn quotas off */\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tif (sb_dqopt(sb)->files[i])\n\t\t\tvfs_quota_off(sb, i, 0);\n\t}\n#endif\n\tsb->s_flags = s_flags; /* Restore MS_RDONLY status */\n}\n\n/*\n * Maximal extent format file size.\n * Resulting logical blkno at s_maxbytes must fit in our on-disk\n * extent format containers, within a sector_t, and within i_blocks\n * in the vfs.  ext4 inode has 48 bits of i_block in fsblock units,\n * so that won't be a limiting factor.\n *\n * Note, this does *not* consider any metadata overhead for vfs i_blocks.\n */\nstatic loff_t ext4_max_size(int blkbits, int has_huge_files)\n{\n\tloff_t res;\n\tloff_t upper_limit = MAX_LFS_FILESIZE;\n\n\t/* small i_blocks in vfs inode? */\n\tif (!has_huge_files || sizeof(blkcnt_t) < sizeof(u64)) {\n\t\t/*\n\t\t * CONFIG_LBDAF is not enabled implies the inode\n\t\t * i_block represent total blocks in 512 bytes\n\t\t * 32 == size of vfs inode i_blocks * 8\n\t\t */\n\t\tupper_limit = (1LL << 32) - 1;\n\n\t\t/* total blocks in file system block size */\n\t\tupper_limit >>= (blkbits - 9);\n\t\tupper_limit <<= blkbits;\n\t}\n\n\t/* 32-bit extent-start container, ee_block */\n\tres = 1LL << 32;\n\tres <<= blkbits;\n\tres -= 1;\n\n\t/* Sanity check against vm- & vfs- imposed limits */\n\tif (res > upper_limit)\n\t\tres = upper_limit;\n\n\treturn res;\n}\n\n/*\n * Maximal bitmap file size.  There is a direct, and {,double-,triple-}indirect\n * block limit, and also a limit of (2^48 - 1) 512-byte sectors in i_blocks.\n * We need to be 1 filesystem block less than the 2^48 sector limit.\n */\nstatic loff_t ext4_max_bitmap_size(int bits, int has_huge_files)\n{\n\tloff_t res = EXT4_NDIR_BLOCKS;\n\tint meta_blocks;\n\tloff_t upper_limit;\n\t/* This is calculated to be the largest file size for a dense, block\n\t * mapped file such that the file's total number of 512-byte sectors,\n\t * including data and all indirect blocks, does not exceed (2^48 - 1).\n\t *\n\t * __u32 i_blocks_lo and _u16 i_blocks_high represent the total\n\t * number of 512-byte sectors of the file.\n\t */\n\n\tif (!has_huge_files || sizeof(blkcnt_t) < sizeof(u64)) {\n\t\t/*\n\t\t * !has_huge_files or CONFIG_LBDAF not enabled implies that\n\t\t * the inode i_block field represents total file blocks in\n\t\t * 2^32 512-byte sectors == size of vfs inode i_blocks * 8\n\t\t */\n\t\tupper_limit = (1LL << 32) - 1;\n\n\t\t/* total blocks in file system block size */\n\t\tupper_limit >>= (bits - 9);\n\n\t} else {\n\t\t/*\n\t\t * We use 48 bit ext4_inode i_blocks\n\t\t * With EXT4_HUGE_FILE_FL set the i_blocks\n\t\t * represent total number of blocks in\n\t\t * file system block size\n\t\t */\n\t\tupper_limit = (1LL << 48) - 1;\n\n\t}\n\n\t/* indirect blocks */\n\tmeta_blocks = 1;\n\t/* double indirect blocks */\n\tmeta_blocks += 1 + (1LL << (bits-2));\n\t/* tripple indirect blocks */\n\tmeta_blocks += 1 + (1LL << (bits-2)) + (1LL << (2*(bits-2)));\n\n\tupper_limit -= meta_blocks;\n\tupper_limit <<= bits;\n\n\tres += 1LL << (bits-2);\n\tres += 1LL << (2*(bits-2));\n\tres += 1LL << (3*(bits-2));\n\tres <<= bits;\n\tif (res > upper_limit)\n\t\tres = upper_limit;\n\n\tif (res > MAX_LFS_FILESIZE)\n\t\tres = MAX_LFS_FILESIZE;\n\n\treturn res;\n}\n\nstatic ext4_fsblk_t descriptor_loc(struct super_block *sb,\n\t\t\t\t   ext4_fsblk_t logical_sb_block, int nr)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_group_t bg, first_meta_bg;\n\tint has_super = 0;\n\n\tfirst_meta_bg = le32_to_cpu(sbi->s_es->s_first_meta_bg);\n\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_META_BG) ||\n\t    nr < first_meta_bg)\n\t\treturn logical_sb_block + nr + 1;\n\tbg = sbi->s_desc_per_block * nr;\n\tif (ext4_bg_has_super(sb, bg))\n\t\thas_super = 1;\n\n\treturn (has_super + ext4_group_first_block_no(sb, bg));\n}\n\n/**\n * ext4_get_stripe_size: Get the stripe size.\n * @sbi: In memory super block info\n *\n * If we have specified it via mount option, then\n * use the mount option value. If the value specified at mount time is\n * greater than the blocks per group use the super block value.\n * If the super block value is greater than blocks per group return 0.\n * Allocator needs it be less than blocks per group.\n *\n */\nstatic unsigned long ext4_get_stripe_size(struct ext4_sb_info *sbi)\n{\n\tunsigned long stride = le16_to_cpu(sbi->s_es->s_raid_stride);\n\tunsigned long stripe_width =\n\t\t\tle32_to_cpu(sbi->s_es->s_raid_stripe_width);\n\n\tif (sbi->s_stripe && sbi->s_stripe <= sbi->s_blocks_per_group)\n\t\treturn sbi->s_stripe;\n\n\tif (stripe_width <= sbi->s_blocks_per_group)\n\t\treturn stripe_width;\n\n\tif (stride <= sbi->s_blocks_per_group)\n\t\treturn stride;\n\n\treturn 0;\n}\n\n/* sysfs supprt */\n\nstruct ext4_attr {\n\tstruct attribute attr;\n\tssize_t (*show)(struct ext4_attr *, struct ext4_sb_info *, char *);\n\tssize_t (*store)(struct ext4_attr *, struct ext4_sb_info *, \n\t\t\t const char *, size_t);\n\tint offset;\n};\n\nstatic int parse_strtoul(const char *buf,\n\t\tunsigned long max, unsigned long *value)\n{\n\tchar *endp;\n\n\t*value = simple_strtoul(skip_spaces(buf), &endp, 0);\n\tendp = skip_spaces(endp);\n\tif (*endp || *value > max)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic ssize_t delayed_allocation_blocks_show(struct ext4_attr *a,\n\t\t\t\t\t      struct ext4_sb_info *sbi,\n\t\t\t\t\t      char *buf)\n{\n\treturn snprintf(buf, PAGE_SIZE, \"%llu\\n\",\n\t\t\t(s64) percpu_counter_sum(&sbi->s_dirtyblocks_counter));\n}\n\nstatic ssize_t session_write_kbytes_show(struct ext4_attr *a,\n\t\t\t\t\t struct ext4_sb_info *sbi, char *buf)\n{\n\tstruct super_block *sb = sbi->s_buddy_cache->i_sb;\n\n\treturn snprintf(buf, PAGE_SIZE, \"%lu\\n\",\n\t\t\t(part_stat_read(sb->s_bdev->bd_part, sectors[1]) -\n\t\t\t sbi->s_sectors_written_start) >> 1);\n}\n\nstatic ssize_t lifetime_write_kbytes_show(struct ext4_attr *a,\n\t\t\t\t\t  struct ext4_sb_info *sbi, char *buf)\n{\n\tstruct super_block *sb = sbi->s_buddy_cache->i_sb;\n\n\treturn snprintf(buf, PAGE_SIZE, \"%llu\\n\",\n\t\t\t(unsigned long long)(sbi->s_kbytes_written +\n\t\t\t((part_stat_read(sb->s_bdev->bd_part, sectors[1]) -\n\t\t\t  EXT4_SB(sb)->s_sectors_written_start) >> 1)));\n}\n\nstatic ssize_t inode_readahead_blks_store(struct ext4_attr *a,\n\t\t\t\t\t  struct ext4_sb_info *sbi,\n\t\t\t\t\t  const char *buf, size_t count)\n{\n\tunsigned long t;\n\n\tif (parse_strtoul(buf, 0x40000000, &t))\n\t\treturn -EINVAL;\n\n\tif (!is_power_of_2(t))\n\t\treturn -EINVAL;\n\n\tsbi->s_inode_readahead_blks = t;\n\treturn count;\n}\n\nstatic ssize_t sbi_ui_show(struct ext4_attr *a,\n\t\t\t   struct ext4_sb_info *sbi, char *buf)\n{\n\tunsigned int *ui = (unsigned int *) (((char *) sbi) + a->offset);\n\n\treturn snprintf(buf, PAGE_SIZE, \"%u\\n\", *ui);\n}\n\nstatic ssize_t sbi_ui_store(struct ext4_attr *a,\n\t\t\t    struct ext4_sb_info *sbi,\n\t\t\t    const char *buf, size_t count)\n{\n\tunsigned int *ui = (unsigned int *) (((char *) sbi) + a->offset);\n\tunsigned long t;\n\n\tif (parse_strtoul(buf, 0xffffffff, &t))\n\t\treturn -EINVAL;\n\t*ui = t;\n\treturn count;\n}\n\n#define EXT4_ATTR_OFFSET(_name,_mode,_show,_store,_elname) \\\nstatic struct ext4_attr ext4_attr_##_name = {\t\t\t\\\n\t.attr = {.name = __stringify(_name), .mode = _mode },\t\\\n\t.show\t= _show,\t\t\t\t\t\\\n\t.store\t= _store,\t\t\t\t\t\\\n\t.offset = offsetof(struct ext4_sb_info, _elname),\t\\\n}\n#define EXT4_ATTR(name, mode, show, store) \\\nstatic struct ext4_attr ext4_attr_##name = __ATTR(name, mode, show, store)\n\n#define EXT4_RO_ATTR(name) EXT4_ATTR(name, 0444, name##_show, NULL)\n#define EXT4_RW_ATTR(name) EXT4_ATTR(name, 0644, name##_show, name##_store)\n#define EXT4_RW_ATTR_SBI_UI(name, elname)\t\\\n\tEXT4_ATTR_OFFSET(name, 0644, sbi_ui_show, sbi_ui_store, elname)\n#define ATTR_LIST(name) &ext4_attr_##name.attr\n\nEXT4_RO_ATTR(delayed_allocation_blocks);\nEXT4_RO_ATTR(session_write_kbytes);\nEXT4_RO_ATTR(lifetime_write_kbytes);\nEXT4_ATTR_OFFSET(inode_readahead_blks, 0644, sbi_ui_show,\n\t\t inode_readahead_blks_store, s_inode_readahead_blks);\nEXT4_RW_ATTR_SBI_UI(inode_goal, s_inode_goal);\nEXT4_RW_ATTR_SBI_UI(mb_stats, s_mb_stats);\nEXT4_RW_ATTR_SBI_UI(mb_max_to_scan, s_mb_max_to_scan);\nEXT4_RW_ATTR_SBI_UI(mb_min_to_scan, s_mb_min_to_scan);\nEXT4_RW_ATTR_SBI_UI(mb_order2_req, s_mb_order2_reqs);\nEXT4_RW_ATTR_SBI_UI(mb_stream_req, s_mb_stream_request);\nEXT4_RW_ATTR_SBI_UI(mb_group_prealloc, s_mb_group_prealloc);\nEXT4_RW_ATTR_SBI_UI(max_writeback_mb_bump, s_max_writeback_mb_bump);\n\nstatic struct attribute *ext4_attrs[] = {\n\tATTR_LIST(delayed_allocation_blocks),\n\tATTR_LIST(session_write_kbytes),\n\tATTR_LIST(lifetime_write_kbytes),\n\tATTR_LIST(inode_readahead_blks),\n\tATTR_LIST(inode_goal),\n\tATTR_LIST(mb_stats),\n\tATTR_LIST(mb_max_to_scan),\n\tATTR_LIST(mb_min_to_scan),\n\tATTR_LIST(mb_order2_req),\n\tATTR_LIST(mb_stream_req),\n\tATTR_LIST(mb_group_prealloc),\n\tATTR_LIST(max_writeback_mb_bump),\n\tNULL,\n};\n\nstatic ssize_t ext4_attr_show(struct kobject *kobj,\n\t\t\t      struct attribute *attr, char *buf)\n{\n\tstruct ext4_sb_info *sbi = container_of(kobj, struct ext4_sb_info,\n\t\t\t\t\t\ts_kobj);\n\tstruct ext4_attr *a = container_of(attr, struct ext4_attr, attr);\n\n\treturn a->show ? a->show(a, sbi, buf) : 0;\n}\n\nstatic ssize_t ext4_attr_store(struct kobject *kobj,\n\t\t\t       struct attribute *attr,\n\t\t\t       const char *buf, size_t len)\n{\n\tstruct ext4_sb_info *sbi = container_of(kobj, struct ext4_sb_info,\n\t\t\t\t\t\ts_kobj);\n\tstruct ext4_attr *a = container_of(attr, struct ext4_attr, attr);\n\n\treturn a->store ? a->store(a, sbi, buf, len) : 0;\n}\n\nstatic void ext4_sb_release(struct kobject *kobj)\n{\n\tstruct ext4_sb_info *sbi = container_of(kobj, struct ext4_sb_info,\n\t\t\t\t\t\ts_kobj);\n\tcomplete(&sbi->s_kobj_unregister);\n}\n\n\nstatic struct sysfs_ops ext4_attr_ops = {\n\t.show\t= ext4_attr_show,\n\t.store\t= ext4_attr_store,\n};\n\nstatic struct kobj_type ext4_ktype = {\n\t.default_attrs\t= ext4_attrs,\n\t.sysfs_ops\t= &ext4_attr_ops,\n\t.release\t= ext4_sb_release,\n};\n\n/*\n * Check whether this filesystem can be mounted based on\n * the features present and the RDONLY/RDWR mount requested.\n * Returns 1 if this filesystem can be mounted as requested,\n * 0 if it cannot be.\n */\nstatic int ext4_feature_set_ok(struct super_block *sb, int readonly)\n{\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, ~EXT4_FEATURE_INCOMPAT_SUPP)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Couldn't mount because of \"\n\t\t\t\"unsupported optional features (%x)\",\n\t\t\t(le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_incompat) &\n\t\t\t~EXT4_FEATURE_INCOMPAT_SUPP));\n\t\treturn 0;\n\t}\n\n\tif (readonly)\n\t\treturn 1;\n\n\t/* Check that feature set is OK for a read-write mount */\n\tif (EXT4_HAS_RO_COMPAT_FEATURE(sb, ~EXT4_FEATURE_RO_COMPAT_SUPP)) {\n\t\text4_msg(sb, KERN_ERR, \"couldn't mount RDWR because of \"\n\t\t\t \"unsupported optional features (%x)\",\n\t\t\t (le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_ro_compat) &\n\t\t\t\t~EXT4_FEATURE_RO_COMPAT_SUPP));\n\t\treturn 0;\n\t}\n\t/*\n\t * Large file size enabled file system can only be mounted\n\t * read-write on 32-bit systems if kernel is built with CONFIG_LBDAF\n\t */\n\tif (EXT4_HAS_RO_COMPAT_FEATURE(sb, EXT4_FEATURE_RO_COMPAT_HUGE_FILE)) {\n\t\tif (sizeof(blkcnt_t) < sizeof(u64)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Filesystem with huge files \"\n\t\t\t\t \"cannot be mounted RDWR without \"\n\t\t\t\t \"CONFIG_LBDAF\");\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int ext4_fill_super(struct super_block *sb, void *data, int silent)\n\t\t\t\t__releases(kernel_lock)\n\t\t\t\t__acquires(kernel_lock)\n{\n\tstruct buffer_head *bh;\n\tstruct ext4_super_block *es = NULL;\n\tstruct ext4_sb_info *sbi;\n\text4_fsblk_t block;\n\text4_fsblk_t sb_block = get_sb_block(&data);\n\text4_fsblk_t logical_sb_block;\n\tunsigned long offset = 0;\n\tunsigned long journal_devnum = 0;\n\tunsigned long def_mount_opts;\n\tstruct inode *root;\n\tchar *cp;\n\tconst char *descr;\n\tint ret = -EINVAL;\n\tint blocksize;\n\tunsigned int db_count;\n\tunsigned int i;\n\tint needs_recovery, has_huge_files;\n\t__u64 blocks_count;\n\tint err;\n\tunsigned int journal_ioprio = DEFAULT_JOURNAL_IOPRIO;\n\n\tsbi = kzalloc(sizeof(*sbi), GFP_KERNEL);\n\tif (!sbi)\n\t\treturn -ENOMEM;\n\n\tsbi->s_blockgroup_lock =\n\t\tkzalloc(sizeof(struct blockgroup_lock), GFP_KERNEL);\n\tif (!sbi->s_blockgroup_lock) {\n\t\tkfree(sbi);\n\t\treturn -ENOMEM;\n\t}\n\tsb->s_fs_info = sbi;\n\tsbi->s_mount_opt = 0;\n\tsbi->s_resuid = EXT4_DEF_RESUID;\n\tsbi->s_resgid = EXT4_DEF_RESGID;\n\tsbi->s_inode_readahead_blks = EXT4_DEF_INODE_READAHEAD_BLKS;\n\tsbi->s_sb_block = sb_block;\n\tsbi->s_sectors_written_start = part_stat_read(sb->s_bdev->bd_part,\n\t\t\t\t\t\t      sectors[1]);\n\n\tunlock_kernel();\n\n\t/* Cleanup superblock name */\n\tfor (cp = sb->s_id; (cp = strchr(cp, '/'));)\n\t\t*cp = '!';\n\n\tblocksize = sb_min_blocksize(sb, EXT4_MIN_BLOCK_SIZE);\n\tif (!blocksize) {\n\t\text4_msg(sb, KERN_ERR, \"unable to set blocksize\");\n\t\tgoto out_fail;\n\t}\n\n\t/*\n\t * The ext4 superblock will not be buffer aligned for other than 1kB\n\t * block sizes.  We need to calculate the offset from buffer start.\n\t */\n\tif (blocksize != EXT4_MIN_BLOCK_SIZE) {\n\t\tlogical_sb_block = sb_block * EXT4_MIN_BLOCK_SIZE;\n\t\toffset = do_div(logical_sb_block, blocksize);\n\t} else {\n\t\tlogical_sb_block = sb_block;\n\t}\n\n\tif (!(bh = sb_bread(sb, logical_sb_block))) {\n\t\text4_msg(sb, KERN_ERR, \"unable to read superblock\");\n\t\tgoto out_fail;\n\t}\n\t/*\n\t * Note: s_es must be initialized as soon as possible because\n\t *       some ext4 macro-instructions depend on its value\n\t */\n\tes = (struct ext4_super_block *) (((char *)bh->b_data) + offset);\n\tsbi->s_es = es;\n\tsb->s_magic = le16_to_cpu(es->s_magic);\n\tif (sb->s_magic != EXT4_SUPER_MAGIC)\n\t\tgoto cantfind_ext4;\n\tsbi->s_kbytes_written = le64_to_cpu(es->s_kbytes_written);\n\n\t/* Set defaults before we parse the mount options */\n\tdef_mount_opts = le32_to_cpu(es->s_default_mount_opts);\n\tif (def_mount_opts & EXT4_DEFM_DEBUG)\n\t\tset_opt(sbi->s_mount_opt, DEBUG);\n\tif (def_mount_opts & EXT4_DEFM_BSDGROUPS) {\n\t\text4_msg(sb, KERN_WARNING, deprecated_msg, \"bsdgroups\",\n\t\t\t\"2.6.38\");\n\t\tset_opt(sbi->s_mount_opt, GRPID);\n\t}\n\tif (def_mount_opts & EXT4_DEFM_UID16)\n\t\tset_opt(sbi->s_mount_opt, NO_UID32);\n#ifdef CONFIG_EXT4_FS_XATTR\n\tif (def_mount_opts & EXT4_DEFM_XATTR_USER)\n\t\tset_opt(sbi->s_mount_opt, XATTR_USER);\n#endif\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\tif (def_mount_opts & EXT4_DEFM_ACL)\n\t\tset_opt(sbi->s_mount_opt, POSIX_ACL);\n#endif\n\tif ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_DATA)\n\t\tset_opt(sbi->s_mount_opt, JOURNAL_DATA);\n\telse if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_ORDERED)\n\t\tset_opt(sbi->s_mount_opt, ORDERED_DATA);\n\telse if ((def_mount_opts & EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_WBACK)\n\t\tset_opt(sbi->s_mount_opt, WRITEBACK_DATA);\n\n\tif (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_PANIC)\n\t\tset_opt(sbi->s_mount_opt, ERRORS_PANIC);\n\telse if (le16_to_cpu(sbi->s_es->s_errors) == EXT4_ERRORS_CONTINUE)\n\t\tset_opt(sbi->s_mount_opt, ERRORS_CONT);\n\telse\n\t\tset_opt(sbi->s_mount_opt, ERRORS_RO);\n\n\tsbi->s_resuid = le16_to_cpu(es->s_def_resuid);\n\tsbi->s_resgid = le16_to_cpu(es->s_def_resgid);\n\tsbi->s_commit_interval = JBD2_DEFAULT_MAX_COMMIT_AGE * HZ;\n\tsbi->s_min_batch_time = EXT4_DEF_MIN_BATCH_TIME;\n\tsbi->s_max_batch_time = EXT4_DEF_MAX_BATCH_TIME;\n\n\tset_opt(sbi->s_mount_opt, BARRIER);\n\n\t/*\n\t * enable delayed allocation by default\n\t * Use -o nodelalloc to turn it off\n\t */\n\tset_opt(sbi->s_mount_opt, DELALLOC);\n\n\tif (!parse_options((char *) data, sb, &journal_devnum,\n\t\t\t   &journal_ioprio, NULL, 0))\n\t\tgoto failed_mount;\n\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sb, POSIX_ACL) ? MS_POSIXACL : 0);\n\n\tif (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV &&\n\t    (EXT4_HAS_COMPAT_FEATURE(sb, ~0U) ||\n\t     EXT4_HAS_RO_COMPAT_FEATURE(sb, ~0U) ||\n\t     EXT4_HAS_INCOMPAT_FEATURE(sb, ~0U)))\n\t\text4_msg(sb, KERN_WARNING,\n\t\t       \"feature flags set on rev 0 fs, \"\n\t\t       \"running e2fsck is recommended\");\n\n\t/*\n\t * Check feature flags regardless of the revision level, since we\n\t * previously didn't change the revision level when setting the flags,\n\t * so there is a chance incompat flags are set on a rev 0 filesystem.\n\t */\n\tif (!ext4_feature_set_ok(sb, (sb->s_flags & MS_RDONLY)))\n\t\tgoto failed_mount;\n\n\tblocksize = BLOCK_SIZE << le32_to_cpu(es->s_log_block_size);\n\n\tif (blocksize < EXT4_MIN_BLOCK_SIZE ||\n\t    blocksize > EXT4_MAX_BLOCK_SIZE) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t       \"Unsupported filesystem blocksize %d\", blocksize);\n\t\tgoto failed_mount;\n\t}\n\n\tif (sb->s_blocksize != blocksize) {\n\t\t/* Validate the filesystem blocksize */\n\t\tif (!sb_set_blocksize(sb, blocksize)) {\n\t\t\text4_msg(sb, KERN_ERR, \"bad block size %d\",\n\t\t\t\t\tblocksize);\n\t\t\tgoto failed_mount;\n\t\t}\n\n\t\tbrelse(bh);\n\t\tlogical_sb_block = sb_block * EXT4_MIN_BLOCK_SIZE;\n\t\toffset = do_div(logical_sb_block, blocksize);\n\t\tbh = sb_bread(sb, logical_sb_block);\n\t\tif (!bh) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"Can't read superblock on 2nd try\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tes = (struct ext4_super_block *)(((char *)bh->b_data) + offset);\n\t\tsbi->s_es = es;\n\t\tif (es->s_magic != cpu_to_le16(EXT4_SUPER_MAGIC)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"Magic mismatch, very weird!\");\n\t\t\tgoto failed_mount;\n\t\t}\n\t}\n\n\thas_huge_files = EXT4_HAS_RO_COMPAT_FEATURE(sb,\n\t\t\t\tEXT4_FEATURE_RO_COMPAT_HUGE_FILE);\n\tsbi->s_bitmap_maxbytes = ext4_max_bitmap_size(sb->s_blocksize_bits,\n\t\t\t\t\t\t      has_huge_files);\n\tsb->s_maxbytes = ext4_max_size(sb->s_blocksize_bits, has_huge_files);\n\n\tif (le32_to_cpu(es->s_rev_level) == EXT4_GOOD_OLD_REV) {\n\t\tsbi->s_inode_size = EXT4_GOOD_OLD_INODE_SIZE;\n\t\tsbi->s_first_ino = EXT4_GOOD_OLD_FIRST_INO;\n\t} else {\n\t\tsbi->s_inode_size = le16_to_cpu(es->s_inode_size);\n\t\tsbi->s_first_ino = le32_to_cpu(es->s_first_ino);\n\t\tif ((sbi->s_inode_size < EXT4_GOOD_OLD_INODE_SIZE) ||\n\t\t    (!is_power_of_2(sbi->s_inode_size)) ||\n\t\t    (sbi->s_inode_size > blocksize)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unsupported inode size: %d\",\n\t\t\t       sbi->s_inode_size);\n\t\t\tgoto failed_mount;\n\t\t}\n\t\tif (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE)\n\t\t\tsb->s_time_gran = 1 << (EXT4_EPOCH_BITS - 2);\n\t}\n\n\tsbi->s_desc_size = le16_to_cpu(es->s_desc_size);\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_64BIT)) {\n\t\tif (sbi->s_desc_size < EXT4_MIN_DESC_SIZE_64BIT ||\n\t\t    sbi->s_desc_size > EXT4_MAX_DESC_SIZE ||\n\t\t    !is_power_of_2(sbi->s_desc_size)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unsupported descriptor size %lu\",\n\t\t\t       sbi->s_desc_size);\n\t\t\tgoto failed_mount;\n\t\t}\n\t} else\n\t\tsbi->s_desc_size = EXT4_MIN_DESC_SIZE;\n\n\tsbi->s_blocks_per_group = le32_to_cpu(es->s_blocks_per_group);\n\tsbi->s_inodes_per_group = le32_to_cpu(es->s_inodes_per_group);\n\tif (EXT4_INODE_SIZE(sb) == 0 || EXT4_INODES_PER_GROUP(sb) == 0)\n\t\tgoto cantfind_ext4;\n\n\tsbi->s_inodes_per_block = blocksize / EXT4_INODE_SIZE(sb);\n\tif (sbi->s_inodes_per_block == 0)\n\t\tgoto cantfind_ext4;\n\tsbi->s_itb_per_group = sbi->s_inodes_per_group /\n\t\t\t\t\tsbi->s_inodes_per_block;\n\tsbi->s_desc_per_block = blocksize / EXT4_DESC_SIZE(sb);\n\tsbi->s_sbh = bh;\n\tsbi->s_mount_state = le16_to_cpu(es->s_state);\n\tsbi->s_addr_per_block_bits = ilog2(EXT4_ADDR_PER_BLOCK(sb));\n\tsbi->s_desc_per_block_bits = ilog2(EXT4_DESC_PER_BLOCK(sb));\n\n\tfor (i = 0; i < 4; i++)\n\t\tsbi->s_hash_seed[i] = le32_to_cpu(es->s_hash_seed[i]);\n\tsbi->s_def_hash_version = es->s_def_hash_version;\n\ti = le32_to_cpu(es->s_flags);\n\tif (i & EXT2_FLAGS_UNSIGNED_HASH)\n\t\tsbi->s_hash_unsigned = 3;\n\telse if ((i & EXT2_FLAGS_SIGNED_HASH) == 0) {\n#ifdef __CHAR_UNSIGNED__\n\t\tes->s_flags |= cpu_to_le32(EXT2_FLAGS_UNSIGNED_HASH);\n\t\tsbi->s_hash_unsigned = 3;\n#else\n\t\tes->s_flags |= cpu_to_le32(EXT2_FLAGS_SIGNED_HASH);\n#endif\n\t\tsb->s_dirt = 1;\n\t}\n\n\tif (sbi->s_blocks_per_group > blocksize * 8) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t       \"#blocks per group too big: %lu\",\n\t\t       sbi->s_blocks_per_group);\n\t\tgoto failed_mount;\n\t}\n\tif (sbi->s_inodes_per_group > blocksize * 8) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t       \"#inodes per group too big: %lu\",\n\t\t       sbi->s_inodes_per_group);\n\t\tgoto failed_mount;\n\t}\n\n\t/*\n\t * Test whether we have more sectors than will fit in sector_t,\n\t * and whether the max offset is addressable by the page cache.\n\t */\n\tif ((ext4_blocks_count(es) >\n\t     (sector_t)(~0ULL) >> (sb->s_blocksize_bits - 9)) ||\n\t    (ext4_blocks_count(es) >\n\t     (pgoff_t)(~0ULL) >> (PAGE_CACHE_SHIFT - sb->s_blocksize_bits))) {\n\t\text4_msg(sb, KERN_ERR, \"filesystem\"\n\t\t\t \" too large to mount safely on this system\");\n\t\tif (sizeof(sector_t) < 8)\n\t\t\text4_msg(sb, KERN_WARNING, \"CONFIG_LBDAF not enabled\");\n\t\tret = -EFBIG;\n\t\tgoto failed_mount;\n\t}\n\n\tif (EXT4_BLOCKS_PER_GROUP(sb) == 0)\n\t\tgoto cantfind_ext4;\n\n\t/* check blocks count against device size */\n\tblocks_count = sb->s_bdev->bd_inode->i_size >> sb->s_blocksize_bits;\n\tif (blocks_count && ext4_blocks_count(es) > blocks_count) {\n\t\text4_msg(sb, KERN_WARNING, \"bad geometry: block count %llu \"\n\t\t       \"exceeds size of device (%llu blocks)\",\n\t\t       ext4_blocks_count(es), blocks_count);\n\t\tgoto failed_mount;\n\t}\n\n\t/*\n\t * It makes no sense for the first data block to be beyond the end\n\t * of the filesystem.\n\t */\n\tif (le32_to_cpu(es->s_first_data_block) >= ext4_blocks_count(es)) {\n                ext4_msg(sb, KERN_WARNING, \"bad geometry: first data\"\n\t\t\t \"block %u is beyond end of filesystem (%llu)\",\n\t\t\t le32_to_cpu(es->s_first_data_block),\n\t\t\t ext4_blocks_count(es));\n\t\tgoto failed_mount;\n\t}\n\tblocks_count = (ext4_blocks_count(es) -\n\t\t\tle32_to_cpu(es->s_first_data_block) +\n\t\t\tEXT4_BLOCKS_PER_GROUP(sb) - 1);\n\tdo_div(blocks_count, EXT4_BLOCKS_PER_GROUP(sb));\n\tif (blocks_count > ((uint64_t)1<<32) - EXT4_DESC_PER_BLOCK(sb)) {\n\t\text4_msg(sb, KERN_WARNING, \"groups count too large: %u \"\n\t\t       \"(block count %llu, first data block %u, \"\n\t\t       \"blocks per group %lu)\", sbi->s_groups_count,\n\t\t       ext4_blocks_count(es),\n\t\t       le32_to_cpu(es->s_first_data_block),\n\t\t       EXT4_BLOCKS_PER_GROUP(sb));\n\t\tgoto failed_mount;\n\t}\n\tsbi->s_groups_count = blocks_count;\n\tsbi->s_blockfile_groups = min_t(ext4_group_t, sbi->s_groups_count,\n\t\t\t(EXT4_MAX_BLOCK_FILE_PHYS / EXT4_BLOCKS_PER_GROUP(sb)));\n\tdb_count = (sbi->s_groups_count + EXT4_DESC_PER_BLOCK(sb) - 1) /\n\t\t   EXT4_DESC_PER_BLOCK(sb);\n\tsbi->s_group_desc = kmalloc(db_count * sizeof(struct buffer_head *),\n\t\t\t\t    GFP_KERNEL);\n\tif (sbi->s_group_desc == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"not enough memory\");\n\t\tgoto failed_mount;\n\t}\n\n#ifdef CONFIG_PROC_FS\n\tif (ext4_proc_root)\n\t\tsbi->s_proc = proc_mkdir(sb->s_id, ext4_proc_root);\n#endif\n\n\tbgl_lock_init(sbi->s_blockgroup_lock);\n\n\tfor (i = 0; i < db_count; i++) {\n\t\tblock = descriptor_loc(sb, logical_sb_block, i);\n\t\tsbi->s_group_desc[i] = sb_bread(sb, block);\n\t\tif (!sbi->s_group_desc[i]) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"can't read group descriptor %d\", i);\n\t\t\tdb_count = i;\n\t\t\tgoto failed_mount2;\n\t\t}\n\t}\n\tif (!ext4_check_descriptors(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"group descriptors corrupted!\");\n\t\tgoto failed_mount2;\n\t}\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG))\n\t\tif (!ext4_fill_flex_info(sb)) {\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t       \"unable to initialize \"\n\t\t\t       \"flex_bg meta info!\");\n\t\t\tgoto failed_mount2;\n\t\t}\n\n\tsbi->s_gdb_count = db_count;\n\tget_random_bytes(&sbi->s_next_generation, sizeof(u32));\n\tspin_lock_init(&sbi->s_next_gen_lock);\n\n\terr = percpu_counter_init(&sbi->s_freeblocks_counter,\n\t\t\text4_count_free_blocks(sb));\n\tif (!err) {\n\t\terr = percpu_counter_init(&sbi->s_freeinodes_counter,\n\t\t\t\text4_count_free_inodes(sb));\n\t}\n\tif (!err) {\n\t\terr = percpu_counter_init(&sbi->s_dirs_counter,\n\t\t\t\text4_count_dirs(sb));\n\t}\n\tif (!err) {\n\t\terr = percpu_counter_init(&sbi->s_dirtyblocks_counter, 0);\n\t}\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"insufficient memory\");\n\t\tgoto failed_mount3;\n\t}\n\n\tsbi->s_stripe = ext4_get_stripe_size(sbi);\n\tsbi->s_max_writeback_mb_bump = 128;\n\n\t/*\n\t * set up enough so that it can read an inode\n\t */\n\tif (!test_opt(sb, NOLOAD) &&\n\t    EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL))\n\t\tsb->s_op = &ext4_sops;\n\telse\n\t\tsb->s_op = &ext4_nojournal_sops;\n\tsb->s_export_op = &ext4_export_ops;\n\tsb->s_xattr = ext4_xattr_handlers;\n#ifdef CONFIG_QUOTA\n\tsb->s_qcop = &ext4_qctl_operations;\n\tsb->dq_op = &ext4_quota_operations;\n#endif\n\tINIT_LIST_HEAD(&sbi->s_orphan); /* unlinked but open files */\n\tmutex_init(&sbi->s_orphan_lock);\n\tmutex_init(&sbi->s_resize_lock);\n\n\tsb->s_root = NULL;\n\n\tneeds_recovery = (es->s_last_orphan != 0 ||\n\t\t\t  EXT4_HAS_INCOMPAT_FEATURE(sb,\n\t\t\t\t    EXT4_FEATURE_INCOMPAT_RECOVER));\n\n\t/*\n\t * The first inode we look at is the journal inode.  Don't try\n\t * root first: it may be modified in the journal!\n\t */\n\tif (!test_opt(sb, NOLOAD) &&\n\t    EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL)) {\n\t\tif (ext4_load_journal(sb, es, journal_devnum))\n\t\t\tgoto failed_mount3;\n\t} else if (test_opt(sb, NOLOAD) && !(sb->s_flags & MS_RDONLY) &&\n\t      EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER)) {\n\t\text4_msg(sb, KERN_ERR, \"required journal recovery \"\n\t\t       \"suppressed and not mounted read-only\");\n\t\tgoto failed_mount_wq;\n\t} else {\n\t\tclear_opt(sbi->s_mount_opt, DATA_FLAGS);\n\t\tset_opt(sbi->s_mount_opt, WRITEBACK_DATA);\n\t\tsbi->s_journal = NULL;\n\t\tneeds_recovery = 0;\n\t\tgoto no_journal;\n\t}\n\n\tif (ext4_blocks_count(es) > 0xffffffffULL &&\n\t    !jbd2_journal_set_features(EXT4_SB(sb)->s_journal, 0, 0,\n\t\t\t\t       JBD2_FEATURE_INCOMPAT_64BIT)) {\n\t\text4_msg(sb, KERN_ERR, \"Failed to set 64-bit journal feature\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\tif (test_opt(sb, JOURNAL_ASYNC_COMMIT)) {\n\t\tjbd2_journal_set_features(sbi->s_journal,\n\t\t\t\tJBD2_FEATURE_COMPAT_CHECKSUM, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT);\n\t} else if (test_opt(sb, JOURNAL_CHECKSUM)) {\n\t\tjbd2_journal_set_features(sbi->s_journal,\n\t\t\t\tJBD2_FEATURE_COMPAT_CHECKSUM, 0, 0);\n\t\tjbd2_journal_clear_features(sbi->s_journal, 0, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT);\n\t} else {\n\t\tjbd2_journal_clear_features(sbi->s_journal,\n\t\t\t\tJBD2_FEATURE_COMPAT_CHECKSUM, 0,\n\t\t\t\tJBD2_FEATURE_INCOMPAT_ASYNC_COMMIT);\n\t}\n\n\t/* We have now updated the journal if required, so we can\n\t * validate the data journaling mode. */\n\tswitch (test_opt(sb, DATA_FLAGS)) {\n\tcase 0:\n\t\t/* No mode set, assume a default based on the journal\n\t\t * capabilities: ORDERED_DATA if the journal can\n\t\t * cope, else JOURNAL_DATA\n\t\t */\n\t\tif (jbd2_journal_check_available_features\n\t\t    (sbi->s_journal, 0, 0, JBD2_FEATURE_INCOMPAT_REVOKE))\n\t\t\tset_opt(sbi->s_mount_opt, ORDERED_DATA);\n\t\telse\n\t\t\tset_opt(sbi->s_mount_opt, JOURNAL_DATA);\n\t\tbreak;\n\n\tcase EXT4_MOUNT_ORDERED_DATA:\n\tcase EXT4_MOUNT_WRITEBACK_DATA:\n\t\tif (!jbd2_journal_check_available_features\n\t\t    (sbi->s_journal, 0, 0, JBD2_FEATURE_INCOMPAT_REVOKE)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Journal does not support \"\n\t\t\t       \"requested data journaling mode\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\tdefault:\n\t\tbreak;\n\t}\n\tset_task_ioprio(sbi->s_journal->j_task, journal_ioprio);\n\nno_journal:\n\tif (test_opt(sb, NOBH)) {\n\t\tif (!(test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_WRITEBACK_DATA)) {\n\t\t\text4_msg(sb, KERN_WARNING, \"Ignoring nobh option - \"\n\t\t\t\t\"its supported only with writeback mode\");\n\t\t\tclear_opt(sbi->s_mount_opt, NOBH);\n\t\t}\n\t\tif (test_opt(sb, DIOREAD_NOLOCK)) {\n\t\t\text4_msg(sb, KERN_WARNING, \"dioread_nolock option is \"\n\t\t\t\t\"not supported with nobh mode\");\n\t\t\tgoto failed_mount_wq;\n\t\t}\n\t}\n\tEXT4_SB(sb)->dio_unwritten_wq = create_workqueue(\"ext4-dio-unwritten\");\n\tif (!EXT4_SB(sb)->dio_unwritten_wq) {\n\t\tprintk(KERN_ERR \"EXT4-fs: failed to create DIO workqueue\\n\");\n\t\tgoto failed_mount_wq;\n\t}\n\n\t/*\n\t * The jbd2_journal_load will have done any necessary log recovery,\n\t * so we can safely mount the rest of the filesystem now.\n\t */\n\n\troot = ext4_iget(sb, EXT4_ROOT_INO);\n\tif (IS_ERR(root)) {\n\t\text4_msg(sb, KERN_ERR, \"get root inode failed\");\n\t\tret = PTR_ERR(root);\n\t\tgoto failed_mount4;\n\t}\n\tif (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) {\n\t\tiput(root);\n\t\text4_msg(sb, KERN_ERR, \"corrupt root inode, run e2fsck\");\n\t\tgoto failed_mount4;\n\t}\n\tsb->s_root = d_alloc_root(root);\n\tif (!sb->s_root) {\n\t\text4_msg(sb, KERN_ERR, \"get root dentry failed\");\n\t\tiput(root);\n\t\tret = -ENOMEM;\n\t\tgoto failed_mount4;\n\t}\n\n\text4_setup_super(sb, es, sb->s_flags & MS_RDONLY);\n\n\t/* determine the minimum size of new large inodes, if present */\n\tif (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\t     EXT4_GOOD_OLD_INODE_SIZE;\n\t\tif (EXT4_HAS_RO_COMPAT_FEATURE(sb,\n\t\t\t\t       EXT4_FEATURE_RO_COMPAT_EXTRA_ISIZE)) {\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_want_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_want_extra_isize);\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_min_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_min_extra_isize);\n\t\t}\n\t}\n\t/* Check if enough inode space is available */\n\tif (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >\n\t\t\t\t\t\t\tsbi->s_inode_size) {\n\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\t       EXT4_GOOD_OLD_INODE_SIZE;\n\t\text4_msg(sb, KERN_INFO, \"required extra inode space not\"\n\t\t\t \"available\");\n\t}\n\n\tif (test_opt(sb, DELALLOC) &&\n\t    (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)) {\n\t\text4_msg(sb, KERN_WARNING, \"Ignoring delalloc option - \"\n\t\t\t \"requested data journaling mode\");\n\t\tclear_opt(sbi->s_mount_opt, DELALLOC);\n\t}\n\tif (test_opt(sb, DIOREAD_NOLOCK)) {\n\t\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA) {\n\t\t\text4_msg(sb, KERN_WARNING, \"Ignoring dioread_nolock \"\n\t\t\t\t\"option - requested data journaling mode\");\n\t\t\tclear_opt(sbi->s_mount_opt, DIOREAD_NOLOCK);\n\t\t}\n\t\tif (sb->s_blocksize < PAGE_SIZE) {\n\t\t\text4_msg(sb, KERN_WARNING, \"Ignoring dioread_nolock \"\n\t\t\t\t\"option - block size is too small\");\n\t\t\tclear_opt(sbi->s_mount_opt, DIOREAD_NOLOCK);\n\t\t}\n\t}\n\n\terr = ext4_setup_system_zone(sb);\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"failed to initialize system \"\n\t\t\t \"zone (%d)\\n\", err);\n\t\tgoto failed_mount4;\n\t}\n\n\text4_ext_init(sb);\n\terr = ext4_mb_init(sb, needs_recovery);\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"failed to initalize mballoc (%d)\",\n\t\t\t err);\n\t\tgoto failed_mount4;\n\t}\n\n\tsbi->s_kobj.kset = ext4_kset;\n\tinit_completion(&sbi->s_kobj_unregister);\n\terr = kobject_init_and_add(&sbi->s_kobj, &ext4_ktype, NULL,\n\t\t\t\t   \"%s\", sb->s_id);\n\tif (err) {\n\t\text4_mb_release(sb);\n\t\text4_ext_release(sb);\n\t\tgoto failed_mount4;\n\t};\n\n\tEXT4_SB(sb)->s_mount_state |= EXT4_ORPHAN_FS;\n\text4_orphan_cleanup(sb, es);\n\tEXT4_SB(sb)->s_mount_state &= ~EXT4_ORPHAN_FS;\n\tif (needs_recovery) {\n\t\text4_msg(sb, KERN_INFO, \"recovery complete\");\n\t\text4_mark_recovery_complete(sb, es);\n\t}\n\tif (EXT4_SB(sb)->s_journal) {\n\t\tif (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_JOURNAL_DATA)\n\t\t\tdescr = \" journalled data mode\";\n\t\telse if (test_opt(sb, DATA_FLAGS) == EXT4_MOUNT_ORDERED_DATA)\n\t\t\tdescr = \" ordered data mode\";\n\t\telse\n\t\t\tdescr = \" writeback data mode\";\n\t} else\n\t\tdescr = \"out journal\";\n\n\text4_msg(sb, KERN_INFO, \"mounted filesystem with%s\", descr);\n\n\tlock_kernel();\n\treturn 0;\n\ncantfind_ext4:\n\tif (!silent)\n\t\text4_msg(sb, KERN_ERR, \"VFS: Can't find ext4 filesystem\");\n\tgoto failed_mount;\n\nfailed_mount4:\n\text4_msg(sb, KERN_ERR, \"mount failed\");\n\tdestroy_workqueue(EXT4_SB(sb)->dio_unwritten_wq);\nfailed_mount_wq:\n\text4_release_system_zone(sb);\n\tif (sbi->s_journal) {\n\t\tjbd2_journal_destroy(sbi->s_journal);\n\t\tsbi->s_journal = NULL;\n\t}\nfailed_mount3:\n\tif (sbi->s_flex_groups) {\n\t\tif (is_vmalloc_addr(sbi->s_flex_groups))\n\t\t\tvfree(sbi->s_flex_groups);\n\t\telse\n\t\t\tkfree(sbi->s_flex_groups);\n\t}\n\tpercpu_counter_destroy(&sbi->s_freeblocks_counter);\n\tpercpu_counter_destroy(&sbi->s_freeinodes_counter);\n\tpercpu_counter_destroy(&sbi->s_dirs_counter);\n\tpercpu_counter_destroy(&sbi->s_dirtyblocks_counter);\nfailed_mount2:\n\tfor (i = 0; i < db_count; i++)\n\t\tbrelse(sbi->s_group_desc[i]);\n\tkfree(sbi->s_group_desc);\nfailed_mount:\n\tif (sbi->s_proc) {\n\t\tremove_proc_entry(sb->s_id, ext4_proc_root);\n\t}\n#ifdef CONFIG_QUOTA\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tkfree(sbi->s_qf_names[i]);\n#endif\n\text4_blkdev_remove(sbi);\n\tbrelse(bh);\nout_fail:\n\tsb->s_fs_info = NULL;\n\tkfree(sbi->s_blockgroup_lock);\n\tkfree(sbi);\n\tlock_kernel();\n\treturn ret;\n}\n\n/*\n * Setup any per-fs journal parameters now.  We'll do this both on\n * initial mount, once the journal has been initialised but before we've\n * done any recovery; and again on any subsequent remount.\n */\nstatic void ext4_init_journal_params(struct super_block *sb, journal_t *journal)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tjournal->j_commit_interval = sbi->s_commit_interval;\n\tjournal->j_min_batch_time = sbi->s_min_batch_time;\n\tjournal->j_max_batch_time = sbi->s_max_batch_time;\n\n\tspin_lock(&journal->j_state_lock);\n\tif (test_opt(sb, BARRIER))\n\t\tjournal->j_flags |= JBD2_BARRIER;\n\telse\n\t\tjournal->j_flags &= ~JBD2_BARRIER;\n\tif (test_opt(sb, DATA_ERR_ABORT))\n\t\tjournal->j_flags |= JBD2_ABORT_ON_SYNCDATA_ERR;\n\telse\n\t\tjournal->j_flags &= ~JBD2_ABORT_ON_SYNCDATA_ERR;\n\tspin_unlock(&journal->j_state_lock);\n}\n\nstatic journal_t *ext4_get_journal(struct super_block *sb,\n\t\t\t\t   unsigned int journal_inum)\n{\n\tstruct inode *journal_inode;\n\tjournal_t *journal;\n\n\tBUG_ON(!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL));\n\n\t/* First, test for the existence of a valid inode on disk.  Bad\n\t * things happen if we iget() an unused inode, as the subsequent\n\t * iput() will try to delete it. */\n\n\tjournal_inode = ext4_iget(sb, journal_inum);\n\tif (IS_ERR(journal_inode)) {\n\t\text4_msg(sb, KERN_ERR, \"no journal found\");\n\t\treturn NULL;\n\t}\n\tif (!journal_inode->i_nlink) {\n\t\tmake_bad_inode(journal_inode);\n\t\tiput(journal_inode);\n\t\text4_msg(sb, KERN_ERR, \"journal inode is deleted\");\n\t\treturn NULL;\n\t}\n\n\tjbd_debug(2, \"Journal inode found at %p: %lld bytes\\n\",\n\t\t  journal_inode, journal_inode->i_size);\n\tif (!S_ISREG(journal_inode->i_mode)) {\n\t\text4_msg(sb, KERN_ERR, \"invalid journal inode\");\n\t\tiput(journal_inode);\n\t\treturn NULL;\n\t}\n\n\tjournal = jbd2_journal_init_inode(journal_inode);\n\tif (!journal) {\n\t\text4_msg(sb, KERN_ERR, \"Could not load journal inode\");\n\t\tiput(journal_inode);\n\t\treturn NULL;\n\t}\n\tjournal->j_private = sb;\n\text4_init_journal_params(sb, journal);\n\treturn journal;\n}\n\nstatic journal_t *ext4_get_dev_journal(struct super_block *sb,\n\t\t\t\t       dev_t j_dev)\n{\n\tstruct buffer_head *bh;\n\tjournal_t *journal;\n\text4_fsblk_t start;\n\text4_fsblk_t len;\n\tint hblock, blocksize;\n\text4_fsblk_t sb_block;\n\tunsigned long offset;\n\tstruct ext4_super_block *es;\n\tstruct block_device *bdev;\n\n\tBUG_ON(!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL));\n\n\tbdev = ext4_blkdev_get(j_dev, sb);\n\tif (bdev == NULL)\n\t\treturn NULL;\n\n\tif (bd_claim(bdev, sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"failed to claim external journal device\");\n\t\tblkdev_put(bdev, FMODE_READ|FMODE_WRITE);\n\t\treturn NULL;\n\t}\n\n\tblocksize = sb->s_blocksize;\n\thblock = bdev_logical_block_size(bdev);\n\tif (blocksize < hblock) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"blocksize too small for journal device\");\n\t\tgoto out_bdev;\n\t}\n\n\tsb_block = EXT4_MIN_BLOCK_SIZE / blocksize;\n\toffset = EXT4_MIN_BLOCK_SIZE % blocksize;\n\tset_blocksize(bdev, blocksize);\n\tif (!(bh = __bread(bdev, sb_block, blocksize))) {\n\t\text4_msg(sb, KERN_ERR, \"couldn't read superblock of \"\n\t\t       \"external journal\");\n\t\tgoto out_bdev;\n\t}\n\n\tes = (struct ext4_super_block *) (((char *)bh->b_data) + offset);\n\tif ((le16_to_cpu(es->s_magic) != EXT4_SUPER_MAGIC) ||\n\t    !(le32_to_cpu(es->s_feature_incompat) &\n\t      EXT4_FEATURE_INCOMPAT_JOURNAL_DEV)) {\n\t\text4_msg(sb, KERN_ERR, \"external journal has \"\n\t\t\t\t\t\"bad superblock\");\n\t\tbrelse(bh);\n\t\tgoto out_bdev;\n\t}\n\n\tif (memcmp(EXT4_SB(sb)->s_es->s_journal_uuid, es->s_uuid, 16)) {\n\t\text4_msg(sb, KERN_ERR, \"journal UUID does not match\");\n\t\tbrelse(bh);\n\t\tgoto out_bdev;\n\t}\n\n\tlen = ext4_blocks_count(es);\n\tstart = sb_block + 1;\n\tbrelse(bh);\t/* we're done with the superblock */\n\n\tjournal = jbd2_journal_init_dev(bdev, sb->s_bdev,\n\t\t\t\t\tstart, len, blocksize);\n\tif (!journal) {\n\t\text4_msg(sb, KERN_ERR, \"failed to create device journal\");\n\t\tgoto out_bdev;\n\t}\n\tjournal->j_private = sb;\n\tll_rw_block(READ, 1, &journal->j_sb_buffer);\n\twait_on_buffer(journal->j_sb_buffer);\n\tif (!buffer_uptodate(journal->j_sb_buffer)) {\n\t\text4_msg(sb, KERN_ERR, \"I/O error on journal device\");\n\t\tgoto out_journal;\n\t}\n\tif (be32_to_cpu(journal->j_superblock->s_nr_users) != 1) {\n\t\text4_msg(sb, KERN_ERR, \"External journal has more than one \"\n\t\t\t\t\t\"user (unsupported) - %d\",\n\t\t\tbe32_to_cpu(journal->j_superblock->s_nr_users));\n\t\tgoto out_journal;\n\t}\n\tEXT4_SB(sb)->journal_bdev = bdev;\n\text4_init_journal_params(sb, journal);\n\treturn journal;\n\nout_journal:\n\tjbd2_journal_destroy(journal);\nout_bdev:\n\text4_blkdev_put(bdev);\n\treturn NULL;\n}\n\nstatic int ext4_load_journal(struct super_block *sb,\n\t\t\t     struct ext4_super_block *es,\n\t\t\t     unsigned long journal_devnum)\n{\n\tjournal_t *journal;\n\tunsigned int journal_inum = le32_to_cpu(es->s_journal_inum);\n\tdev_t journal_dev;\n\tint err = 0;\n\tint really_read_only;\n\n\tBUG_ON(!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL));\n\n\tif (journal_devnum &&\n\t    journal_devnum != le32_to_cpu(es->s_journal_dev)) {\n\t\text4_msg(sb, KERN_INFO, \"external journal device major/minor \"\n\t\t\t\"numbers have changed\");\n\t\tjournal_dev = new_decode_dev(journal_devnum);\n\t} else\n\t\tjournal_dev = new_decode_dev(le32_to_cpu(es->s_journal_dev));\n\n\treally_read_only = bdev_read_only(sb->s_bdev);\n\n\t/*\n\t * Are we loading a blank journal or performing recovery after a\n\t * crash?  For recovery, we need to check in advance whether we\n\t * can get read-write access to the device.\n\t */\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER)) {\n\t\tif (sb->s_flags & MS_RDONLY) {\n\t\t\text4_msg(sb, KERN_INFO, \"INFO: recovery \"\n\t\t\t\t\t\"required on readonly filesystem\");\n\t\t\tif (really_read_only) {\n\t\t\t\text4_msg(sb, KERN_ERR, \"write access \"\n\t\t\t\t\t\"unavailable, cannot proceed\");\n\t\t\t\treturn -EROFS;\n\t\t\t}\n\t\t\text4_msg(sb, KERN_INFO, \"write access will \"\n\t\t\t       \"be enabled during recovery\");\n\t\t}\n\t}\n\n\tif (journal_inum && journal_dev) {\n\t\text4_msg(sb, KERN_ERR, \"filesystem has both journal \"\n\t\t       \"and inode journals!\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (journal_inum) {\n\t\tif (!(journal = ext4_get_journal(sb, journal_inum)))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (!(journal = ext4_get_dev_journal(sb, journal_dev)))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!(journal->j_flags & JBD2_BARRIER))\n\t\text4_msg(sb, KERN_INFO, \"barriers disabled\");\n\n\tif (!really_read_only && test_opt(sb, UPDATE_JOURNAL)) {\n\t\terr = jbd2_journal_update_format(journal);\n\t\tif (err)  {\n\t\t\text4_msg(sb, KERN_ERR, \"error updating journal\");\n\t\t\tjbd2_journal_destroy(journal);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER))\n\t\terr = jbd2_journal_wipe(journal, !really_read_only);\n\tif (!err)\n\t\terr = jbd2_journal_load(journal);\n\n\tif (err) {\n\t\text4_msg(sb, KERN_ERR, \"error loading journal\");\n\t\tjbd2_journal_destroy(journal);\n\t\treturn err;\n\t}\n\n\tEXT4_SB(sb)->s_journal = journal;\n\text4_clear_journal_err(sb, es);\n\n\tif (journal_devnum &&\n\t    journal_devnum != le32_to_cpu(es->s_journal_dev)) {\n\t\tes->s_journal_dev = cpu_to_le32(journal_devnum);\n\n\t\t/* Make sure we flush the recovery flag to disk. */\n\t\text4_commit_super(sb, 1);\n\t}\n\n\treturn 0;\n}\n\nstatic int ext4_commit_super(struct super_block *sb, int sync)\n{\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\tstruct buffer_head *sbh = EXT4_SB(sb)->s_sbh;\n\tint error = 0;\n\n\tif (!sbh)\n\t\treturn error;\n\tif (buffer_write_io_error(sbh)) {\n\t\t/*\n\t\t * Oh, dear.  A previous attempt to write the\n\t\t * superblock failed.  This could happen because the\n\t\t * USB device was yanked out.  Or it could happen to\n\t\t * be a transient write error and maybe the block will\n\t\t * be remapped.  Nothing we can do but to retry the\n\t\t * write and hope for the best.\n\t\t */\n\t\text4_msg(sb, KERN_ERR, \"previous I/O error to \"\n\t\t       \"superblock detected\");\n\t\tclear_buffer_write_io_error(sbh);\n\t\tset_buffer_uptodate(sbh);\n\t}\n\t/*\n\t * If the file system is mounted read-only, don't update the\n\t * superblock write time.  This avoids updating the superblock\n\t * write time when we are mounting the root file system\n\t * read/only but we need to replay the journal; at that point,\n\t * for people who are east of GMT and who make their clock\n\t * tick in localtime for Windows bug-for-bug compatibility,\n\t * the clock is set in the future, and this will cause e2fsck\n\t * to complain and force a full file system check.\n\t */\n\tif (!(sb->s_flags & MS_RDONLY))\n\t\tes->s_wtime = cpu_to_le32(get_seconds());\n\tes->s_kbytes_written =\n\t\tcpu_to_le64(EXT4_SB(sb)->s_kbytes_written + \n\t\t\t    ((part_stat_read(sb->s_bdev->bd_part, sectors[1]) -\n\t\t\t      EXT4_SB(sb)->s_sectors_written_start) >> 1));\n\text4_free_blocks_count_set(es, percpu_counter_sum_positive(\n\t\t\t\t\t&EXT4_SB(sb)->s_freeblocks_counter));\n\tes->s_free_inodes_count = cpu_to_le32(percpu_counter_sum_positive(\n\t\t\t\t\t&EXT4_SB(sb)->s_freeinodes_counter));\n\tsb->s_dirt = 0;\n\tBUFFER_TRACE(sbh, \"marking dirty\");\n\tmark_buffer_dirty(sbh);\n\tif (sync) {\n\t\terror = sync_dirty_buffer(sbh);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\terror = buffer_write_io_error(sbh);\n\t\tif (error) {\n\t\t\text4_msg(sb, KERN_ERR, \"I/O error while writing \"\n\t\t\t       \"superblock\");\n\t\t\tclear_buffer_write_io_error(sbh);\n\t\t\tset_buffer_uptodate(sbh);\n\t\t}\n\t}\n\treturn error;\n}\n\n/*\n * Have we just finished recovery?  If so, and if we are mounting (or\n * remounting) the filesystem readonly, then we will end up with a\n * consistent fs on disk.  Record that fact.\n */\nstatic void ext4_mark_recovery_complete(struct super_block *sb,\n\t\t\t\t\tstruct ext4_super_block *es)\n{\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\n\tif (!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL)) {\n\t\tBUG_ON(journal != NULL);\n\t\treturn;\n\t}\n\tjbd2_journal_lock_updates(journal);\n\tif (jbd2_journal_flush(journal) < 0)\n\t\tgoto out;\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER) &&\n\t    sb->s_flags & MS_RDONLY) {\n\t\tEXT4_CLEAR_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);\n\t\text4_commit_super(sb, 1);\n\t}\n\nout:\n\tjbd2_journal_unlock_updates(journal);\n}\n\n/*\n * If we are mounting (or read-write remounting) a filesystem whose journal\n * has recorded an error from a previous lifetime, move that error to the\n * main filesystem now.\n */\nstatic void ext4_clear_journal_err(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es)\n{\n\tjournal_t *journal;\n\tint j_errno;\n\tconst char *errstr;\n\n\tBUG_ON(!EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL));\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\n\t/*\n\t * Now check for any error status which may have been recorded in the\n\t * journal by a prior ext4_error() or ext4_abort()\n\t */\n\n\tj_errno = jbd2_journal_errno(journal);\n\tif (j_errno) {\n\t\tchar nbuf[16];\n\n\t\terrstr = ext4_decode_error(sb, j_errno, nbuf);\n\t\text4_warning(sb, \"Filesystem error recorded \"\n\t\t\t     \"from previous mount: %s\", errstr);\n\t\text4_warning(sb, \"Marking fs in need of filesystem check.\");\n\n\t\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\t\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\t\text4_commit_super(sb, 1);\n\n\t\tjbd2_journal_clear_err(journal);\n\t}\n}\n\n/*\n * Force the running and committing transactions to commit,\n * and wait on the commit.\n */\nint ext4_force_commit(struct super_block *sb)\n{\n\tjournal_t *journal;\n\tint ret = 0;\n\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn 0;\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\tif (journal)\n\t\tret = ext4_journal_force_commit(journal);\n\n\treturn ret;\n}\n\nstatic void ext4_write_super(struct super_block *sb)\n{\n\tlock_super(sb);\n\text4_commit_super(sb, 1);\n\tunlock_super(sb);\n}\n\nstatic int ext4_sync_fs(struct super_block *sb, int wait)\n{\n\tint ret = 0;\n\ttid_t target;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\ttrace_ext4_sync_fs(sb, wait);\n\tflush_workqueue(sbi->dio_unwritten_wq);\n\tif (jbd2_journal_start_commit(sbi->s_journal, &target)) {\n\t\tif (wait)\n\t\t\tjbd2_log_wait_commit(sbi->s_journal, target);\n\t}\n\treturn ret;\n}\n\n/*\n * LVM calls this function before a (read-only) snapshot is created.  This\n * gives us a chance to flush the journal completely and mark the fs clean.\n */\nstatic int ext4_freeze(struct super_block *sb)\n{\n\tint error = 0;\n\tjournal_t *journal;\n\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn 0;\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\n\t/* Now we set up the journal barrier. */\n\tjbd2_journal_lock_updates(journal);\n\n\t/*\n\t * Don't clear the needs_recovery flag if we failed to flush\n\t * the journal.\n\t */\n\terror = jbd2_journal_flush(journal);\n\tif (error < 0) {\n\tout:\n\t\tjbd2_journal_unlock_updates(journal);\n\t\treturn error;\n\t}\n\n\t/* Journal blocked and flushed, clear needs_recovery flag. */\n\tEXT4_CLEAR_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);\n\terror = ext4_commit_super(sb, 1);\n\tif (error)\n\t\tgoto out;\n\treturn 0;\n}\n\n/*\n * Called by LVM after the snapshot is done.  We need to reset the RECOVER\n * flag here, even though the filesystem is not technically dirty yet.\n */\nstatic int ext4_unfreeze(struct super_block *sb)\n{\n\tif (sb->s_flags & MS_RDONLY)\n\t\treturn 0;\n\n\tlock_super(sb);\n\t/* Reset the needs_recovery flag before the fs is unlocked. */\n\tEXT4_SET_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);\n\text4_commit_super(sb, 1);\n\tunlock_super(sb);\n\tjbd2_journal_unlock_updates(EXT4_SB(sb)->s_journal);\n\treturn 0;\n}\n\nstatic int ext4_remount(struct super_block *sb, int *flags, char *data)\n{\n\tstruct ext4_super_block *es;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t n_blocks_count = 0;\n\tunsigned long old_sb_flags;\n\tstruct ext4_mount_options old_opts;\n\text4_group_t g;\n\tunsigned int journal_ioprio = DEFAULT_JOURNAL_IOPRIO;\n\tint err;\n#ifdef CONFIG_QUOTA\n\tint i;\n#endif\n\n\tlock_kernel();\n\n\t/* Store the original options */\n\tlock_super(sb);\n\told_sb_flags = sb->s_flags;\n\told_opts.s_mount_opt = sbi->s_mount_opt;\n\told_opts.s_resuid = sbi->s_resuid;\n\told_opts.s_resgid = sbi->s_resgid;\n\told_opts.s_commit_interval = sbi->s_commit_interval;\n\told_opts.s_min_batch_time = sbi->s_min_batch_time;\n\told_opts.s_max_batch_time = sbi->s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\told_opts.s_jquota_fmt = sbi->s_jquota_fmt;\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\told_opts.s_qf_names[i] = sbi->s_qf_names[i];\n#endif\n\tif (sbi->s_journal && sbi->s_journal->j_task->io_context)\n\t\tjournal_ioprio = sbi->s_journal->j_task->io_context->ioprio;\n\n\t/*\n\t * Allow the \"check\" option to be passed as a remount option.\n\t */\n\tif (!parse_options(data, sb, NULL, &journal_ioprio,\n\t\t\t   &n_blocks_count, 1)) {\n\t\terr = -EINVAL;\n\t\tgoto restore_opts;\n\t}\n\n\tif (sbi->s_mount_flags & EXT4_MF_FS_ABORTED)\n\t\text4_abort(sb, __func__, \"Abort forced by user\");\n\n\tsb->s_flags = (sb->s_flags & ~MS_POSIXACL) |\n\t\t(test_opt(sb, POSIX_ACL) ? MS_POSIXACL : 0);\n\n\tes = sbi->s_es;\n\n\tif (sbi->s_journal) {\n\t\text4_init_journal_params(sb, sbi->s_journal);\n\t\tset_task_ioprio(sbi->s_journal->j_task, journal_ioprio);\n\t}\n\n\tif ((*flags & MS_RDONLY) != (sb->s_flags & MS_RDONLY) ||\n\t\tn_blocks_count > ext4_blocks_count(es)) {\n\t\tif (sbi->s_mount_flags & EXT4_MF_FS_ABORTED) {\n\t\t\terr = -EROFS;\n\t\t\tgoto restore_opts;\n\t\t}\n\n\t\tif (*flags & MS_RDONLY) {\n\t\t\t/*\n\t\t\t * First of all, the unconditional stuff we have to do\n\t\t\t * to disable replay of the journal when we next remount\n\t\t\t */\n\t\t\tsb->s_flags |= MS_RDONLY;\n\n\t\t\t/*\n\t\t\t * OK, test if we are remounting a valid rw partition\n\t\t\t * readonly, and if so set the rdonly flag and then\n\t\t\t * mark the partition as valid again.\n\t\t\t */\n\t\t\tif (!(es->s_state & cpu_to_le16(EXT4_VALID_FS)) &&\n\t\t\t    (sbi->s_mount_state & EXT4_VALID_FS))\n\t\t\t\tes->s_state = cpu_to_le16(sbi->s_mount_state);\n\n\t\t\tif (sbi->s_journal)\n\t\t\t\text4_mark_recovery_complete(sb, es);\n\t\t} else {\n\t\t\t/* Make sure we can mount this feature set readwrite */\n\t\t\tif (!ext4_feature_set_ok(sb, 0)) {\n\t\t\t\terr = -EROFS;\n\t\t\t\tgoto restore_opts;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Make sure the group descriptor checksums\n\t\t\t * are sane.  If they aren't, refuse to remount r/w.\n\t\t\t */\n\t\t\tfor (g = 0; g < sbi->s_groups_count; g++) {\n\t\t\t\tstruct ext4_group_desc *gdp =\n\t\t\t\t\text4_get_group_desc(sb, g, NULL);\n\n\t\t\t\tif (!ext4_group_desc_csum_verify(sbi, g, gdp)) {\n\t\t\t\t\text4_msg(sb, KERN_ERR,\n\t       \"ext4_remount: Checksum for group %u failed (%u!=%u)\",\n\t\tg, le16_to_cpu(ext4_group_desc_csum(sbi, g, gdp)),\n\t\t\t\t\t       le16_to_cpu(gdp->bg_checksum));\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto restore_opts;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If we have an unprocessed orphan list hanging\n\t\t\t * around from a previously readonly bdev mount,\n\t\t\t * require a full umount/remount for now.\n\t\t\t */\n\t\t\tif (es->s_last_orphan) {\n\t\t\t\text4_msg(sb, KERN_WARNING, \"Couldn't \"\n\t\t\t\t       \"remount RDWR because of unprocessed \"\n\t\t\t\t       \"orphan inode list.  Please \"\n\t\t\t\t       \"umount/remount instead\");\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto restore_opts;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Mounting a RDONLY partition read-write, so reread\n\t\t\t * and store the current valid flag.  (It may have\n\t\t\t * been changed by e2fsck since we originally mounted\n\t\t\t * the partition.)\n\t\t\t */\n\t\t\tif (sbi->s_journal)\n\t\t\t\text4_clear_journal_err(sb, es);\n\t\t\tsbi->s_mount_state = le16_to_cpu(es->s_state);\n\t\t\tif ((err = ext4_group_extend(sb, es, n_blocks_count)))\n\t\t\t\tgoto restore_opts;\n\t\t\tif (!ext4_setup_super(sb, es, 0))\n\t\t\t\tsb->s_flags &= ~MS_RDONLY;\n\t\t}\n\t}\n\text4_setup_system_zone(sb);\n\tif (sbi->s_journal == NULL)\n\t\text4_commit_super(sb, 1);\n\n#ifdef CONFIG_QUOTA\n\t/* Release old quota file names */\n\tfor (i = 0; i < MAXQUOTAS; i++)\n\t\tif (old_opts.s_qf_names[i] &&\n\t\t    old_opts.s_qf_names[i] != sbi->s_qf_names[i])\n\t\t\tkfree(old_opts.s_qf_names[i]);\n#endif\n\tunlock_super(sb);\n\tunlock_kernel();\n\treturn 0;\n\nrestore_opts:\n\tsb->s_flags = old_sb_flags;\n\tsbi->s_mount_opt = old_opts.s_mount_opt;\n\tsbi->s_resuid = old_opts.s_resuid;\n\tsbi->s_resgid = old_opts.s_resgid;\n\tsbi->s_commit_interval = old_opts.s_commit_interval;\n\tsbi->s_min_batch_time = old_opts.s_min_batch_time;\n\tsbi->s_max_batch_time = old_opts.s_max_batch_time;\n#ifdef CONFIG_QUOTA\n\tsbi->s_jquota_fmt = old_opts.s_jquota_fmt;\n\tfor (i = 0; i < MAXQUOTAS; i++) {\n\t\tif (sbi->s_qf_names[i] &&\n\t\t    old_opts.s_qf_names[i] != sbi->s_qf_names[i])\n\t\t\tkfree(sbi->s_qf_names[i]);\n\t\tsbi->s_qf_names[i] = old_opts.s_qf_names[i];\n\t}\n#endif\n\tunlock_super(sb);\n\tunlock_kernel();\n\treturn err;\n}\n\nstatic int ext4_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct super_block *sb = dentry->d_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tu64 fsid;\n\n\tif (test_opt(sb, MINIX_DF)) {\n\t\tsbi->s_overhead_last = 0;\n\t} else if (sbi->s_blocks_last != ext4_blocks_count(es)) {\n\t\text4_group_t i, ngroups = ext4_get_groups_count(sb);\n\t\text4_fsblk_t overhead = 0;\n\n\t\t/*\n\t\t * Compute the overhead (FS structures).  This is constant\n\t\t * for a given filesystem unless the number of block groups\n\t\t * changes so we cache the previous value until it does.\n\t\t */\n\n\t\t/*\n\t\t * All of the blocks before first_data_block are\n\t\t * overhead\n\t\t */\n\t\toverhead = le32_to_cpu(es->s_first_data_block);\n\n\t\t/*\n\t\t * Add the overhead attributed to the superblock and\n\t\t * block group descriptors.  If the sparse superblocks\n\t\t * feature is turned on, then not all groups have this.\n\t\t */\n\t\tfor (i = 0; i < ngroups; i++) {\n\t\t\toverhead += ext4_bg_has_super(sb, i) +\n\t\t\t\text4_bg_num_gdb(sb, i);\n\t\t\tcond_resched();\n\t\t}\n\n\t\t/*\n\t\t * Every block group has an inode bitmap, a block\n\t\t * bitmap, and an inode table.\n\t\t */\n\t\toverhead += ngroups * (2 + sbi->s_itb_per_group);\n\t\tsbi->s_overhead_last = overhead;\n\t\tsmp_wmb();\n\t\tsbi->s_blocks_last = ext4_blocks_count(es);\n\t}\n\n\tbuf->f_type = EXT4_SUPER_MAGIC;\n\tbuf->f_bsize = sb->s_blocksize;\n\tbuf->f_blocks = ext4_blocks_count(es) - sbi->s_overhead_last;\n\tbuf->f_bfree = percpu_counter_sum_positive(&sbi->s_freeblocks_counter) -\n\t\t       percpu_counter_sum_positive(&sbi->s_dirtyblocks_counter);\n\tbuf->f_bavail = buf->f_bfree - ext4_r_blocks_count(es);\n\tif (buf->f_bfree < ext4_r_blocks_count(es))\n\t\tbuf->f_bavail = 0;\n\tbuf->f_files = le32_to_cpu(es->s_inodes_count);\n\tbuf->f_ffree = percpu_counter_sum_positive(&sbi->s_freeinodes_counter);\n\tbuf->f_namelen = EXT4_NAME_LEN;\n\tfsid = le64_to_cpup((void *)es->s_uuid) ^\n\t       le64_to_cpup((void *)es->s_uuid + sizeof(u64));\n\tbuf->f_fsid.val[0] = fsid & 0xFFFFFFFFUL;\n\tbuf->f_fsid.val[1] = (fsid >> 32) & 0xFFFFFFFFUL;\n\n\treturn 0;\n}\n\n/* Helper function for writing quotas on sync - we need to start transaction\n * before quota file is locked for write. Otherwise the are possible deadlocks:\n * Process 1                         Process 2\n * ext4_create()                     quota_sync()\n *   jbd2_journal_start()                  write_dquot()\n *   vfs_dq_init()                         down(dqio_mutex)\n *     down(dqio_mutex)                    jbd2_journal_start()\n *\n */\n\n#ifdef CONFIG_QUOTA\n\nstatic inline struct inode *dquot_to_inode(struct dquot *dquot)\n{\n\treturn sb_dqopt(dquot->dq_sb)->files[dquot->dq_type];\n}\n\nstatic int ext4_write_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\tstruct inode *inode;\n\n\tinode = dquot_to_inode(dquot);\n\thandle = ext4_journal_start(inode,\n\t\t\t\t    EXT4_QUOTA_TRANS_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_commit(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_acquire_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\thandle = ext4_journal_start(dquot_to_inode(dquot),\n\t\t\t\t    EXT4_QUOTA_INIT_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_acquire(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_release_dquot(struct dquot *dquot)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\thandle = ext4_journal_start(dquot_to_inode(dquot),\n\t\t\t\t    EXT4_QUOTA_DEL_BLOCKS(dquot->dq_sb));\n\tif (IS_ERR(handle)) {\n\t\t/* Release dquot anyway to avoid endless cycle in dqput() */\n\t\tdquot_release(dquot);\n\t\treturn PTR_ERR(handle);\n\t}\n\tret = dquot_release(dquot);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int ext4_mark_dquot_dirty(struct dquot *dquot)\n{\n\t/* Are we journaling quotas? */\n\tif (EXT4_SB(dquot->dq_sb)->s_qf_names[USRQUOTA] ||\n\t    EXT4_SB(dquot->dq_sb)->s_qf_names[GRPQUOTA]) {\n\t\tdquot_mark_dquot_dirty(dquot);\n\t\treturn ext4_write_dquot(dquot);\n\t} else {\n\t\treturn dquot_mark_dquot_dirty(dquot);\n\t}\n}\n\nstatic int ext4_write_info(struct super_block *sb, int type)\n{\n\tint ret, err;\n\thandle_t *handle;\n\n\t/* Data block + inode block */\n\thandle = ext4_journal_start(sb->s_root->d_inode, 2);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\tret = dquot_commit_info(sb, type);\n\terr = ext4_journal_stop(handle);\n\tif (!ret)\n\t\tret = err;\n\treturn ret;\n}\n\n/*\n * Turn on quotas during mount time - we need to find\n * the quota file and such...\n */\nstatic int ext4_quota_on_mount(struct super_block *sb, int type)\n{\n\treturn vfs_quota_on_mount(sb, EXT4_SB(sb)->s_qf_names[type],\n\t\t\t\t  EXT4_SB(sb)->s_jquota_fmt, type);\n}\n\n/*\n * Standard function to be called on quota_on\n */\nstatic int ext4_quota_on(struct super_block *sb, int type, int format_id,\n\t\t\t char *name, int remount)\n{\n\tint err;\n\tstruct path path;\n\n\tif (!test_opt(sb, QUOTA))\n\t\treturn -EINVAL;\n\t/* When remounting, no checks are needed and in fact, name is NULL */\n\tif (remount)\n\t\treturn vfs_quota_on(sb, type, format_id, name, remount);\n\n\terr = kern_path(name, LOOKUP_FOLLOW, &path);\n\tif (err)\n\t\treturn err;\n\n\t/* Quotafile not on the same filesystem? */\n\tif (path.mnt->mnt_sb != sb) {\n\t\tpath_put(&path);\n\t\treturn -EXDEV;\n\t}\n\t/* Journaling quota? */\n\tif (EXT4_SB(sb)->s_qf_names[type]) {\n\t\t/* Quotafile not in fs root? */\n\t\tif (path.dentry->d_parent != sb->s_root)\n\t\t\text4_msg(sb, KERN_WARNING,\n\t\t\t\t\"Quota file not on filesystem root. \"\n\t\t\t\t\"Journaled quota will not work\");\n\t}\n\n\t/*\n\t * When we journal data on quota file, we have to flush journal to see\n\t * all updates to the file when we bypass pagecache...\n\t */\n\tif (EXT4_SB(sb)->s_journal &&\n\t    ext4_should_journal_data(path.dentry->d_inode)) {\n\t\t/*\n\t\t * We don't need to lock updates but journal_flush() could\n\t\t * otherwise be livelocked...\n\t\t */\n\t\tjbd2_journal_lock_updates(EXT4_SB(sb)->s_journal);\n\t\terr = jbd2_journal_flush(EXT4_SB(sb)->s_journal);\n\t\tjbd2_journal_unlock_updates(EXT4_SB(sb)->s_journal);\n\t\tif (err) {\n\t\t\tpath_put(&path);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\terr = vfs_quota_on_path(sb, type, format_id, &path);\n\tpath_put(&path);\n\treturn err;\n}\n\n/* Read data from quotafile - avoid pagecache and such because we cannot afford\n * acquiring the locks... As quota files are never truncated and quota code\n * itself serializes the operations (and noone else should touch the files)\n * we don't have to be afraid of races */\nstatic ssize_t ext4_quota_read(struct super_block *sb, int type, char *data,\n\t\t\t       size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\text4_lblk_t blk = off >> EXT4_BLOCK_SIZE_BITS(sb);\n\tint err = 0;\n\tint offset = off & (sb->s_blocksize - 1);\n\tint tocopy;\n\tsize_t toread;\n\tstruct buffer_head *bh;\n\tloff_t i_size = i_size_read(inode);\n\n\tif (off > i_size)\n\t\treturn 0;\n\tif (off+len > i_size)\n\t\tlen = i_size-off;\n\ttoread = len;\n\twhile (toread > 0) {\n\t\ttocopy = sb->s_blocksize - offset < toread ?\n\t\t\t\tsb->s_blocksize - offset : toread;\n\t\tbh = ext4_bread(NULL, inode, blk, 0, &err);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (!bh)\t/* A hole? */\n\t\t\tmemset(data, 0, tocopy);\n\t\telse\n\t\t\tmemcpy(data, bh->b_data+offset, tocopy);\n\t\tbrelse(bh);\n\t\toffset = 0;\n\t\ttoread -= tocopy;\n\t\tdata += tocopy;\n\t\tblk++;\n\t}\n\treturn len;\n}\n\n/* Write to quotafile (we know the transaction is already started and has\n * enough credits) */\nstatic ssize_t ext4_quota_write(struct super_block *sb, int type,\n\t\t\t\tconst char *data, size_t len, loff_t off)\n{\n\tstruct inode *inode = sb_dqopt(sb)->files[type];\n\text4_lblk_t blk = off >> EXT4_BLOCK_SIZE_BITS(sb);\n\tint err = 0;\n\tint offset = off & (sb->s_blocksize - 1);\n\tint tocopy;\n\tint journal_quota = EXT4_SB(sb)->s_qf_names[type] != NULL;\n\tsize_t towrite = len;\n\tstruct buffer_head *bh;\n\thandle_t *handle = journal_current_handle();\n\n\tif (EXT4_SB(sb)->s_journal && !handle) {\n\t\text4_msg(sb, KERN_WARNING, \"Quota write (off=%llu, len=%llu)\"\n\t\t\t\" cancelled because transaction is not started\",\n\t\t\t(unsigned long long)off, (unsigned long long)len);\n\t\treturn -EIO;\n\t}\n\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_QUOTA);\n\twhile (towrite > 0) {\n\t\ttocopy = sb->s_blocksize - offset < towrite ?\n\t\t\t\tsb->s_blocksize - offset : towrite;\n\t\tbh = ext4_bread(handle, inode, blk, 1, &err);\n\t\tif (!bh)\n\t\t\tgoto out;\n\t\tif (journal_quota) {\n\t\t\terr = ext4_journal_get_write_access(handle, bh);\n\t\t\tif (err) {\n\t\t\t\tbrelse(bh);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tlock_buffer(bh);\n\t\tmemcpy(bh->b_data+offset, data, tocopy);\n\t\tflush_dcache_page(bh->b_page);\n\t\tunlock_buffer(bh);\n\t\tif (journal_quota)\n\t\t\terr = ext4_handle_dirty_metadata(handle, NULL, bh);\n\t\telse {\n\t\t\t/* Always do at least ordered writes for quotas */\n\t\t\terr = ext4_jbd2_file_inode(handle, inode);\n\t\t\tmark_buffer_dirty(bh);\n\t\t}\n\t\tbrelse(bh);\n\t\tif (err)\n\t\t\tgoto out;\n\t\toffset = 0;\n\t\ttowrite -= tocopy;\n\t\tdata += tocopy;\n\t\tblk++;\n\t}\nout:\n\tif (len == towrite) {\n\t\tmutex_unlock(&inode->i_mutex);\n\t\treturn err;\n\t}\n\tif (inode->i_size < off+len-towrite) {\n\t\ti_size_write(inode, off+len-towrite);\n\t\tEXT4_I(inode)->i_disksize = inode->i_size;\n\t}\n\tinode->i_mtime = inode->i_ctime = CURRENT_TIME;\n\text4_mark_inode_dirty(handle, inode);\n\tmutex_unlock(&inode->i_mutex);\n\treturn len - towrite;\n}\n\n#endif\n\nstatic int ext4_get_sb(struct file_system_type *fs_type, int flags,\n\t\t       const char *dev_name, void *data, struct vfsmount *mnt)\n{\n\treturn get_sb_bdev(fs_type, flags, dev_name, data, ext4_fill_super,mnt);\n}\n\n#if !defined(CONTIG_EXT2_FS) && !defined(CONFIG_EXT2_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT23)\nstatic struct file_system_type ext2_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext2\",\n\t.get_sb\t\t= ext4_get_sb,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\n\nstatic inline void register_as_ext2(void)\n{\n\tint err = register_filesystem(&ext2_fs_type);\n\tif (err)\n\t\tprintk(KERN_WARNING\n\t\t       \"EXT4-fs: Unable to register as ext2 (%d)\\n\", err);\n}\n\nstatic inline void unregister_as_ext2(void)\n{\n\tunregister_filesystem(&ext2_fs_type);\n}\nMODULE_ALIAS(\"ext2\");\n#else\nstatic inline void register_as_ext2(void) { }\nstatic inline void unregister_as_ext2(void) { }\n#endif\n\n#if !defined(CONTIG_EXT3_FS) && !defined(CONFIG_EXT3_FS_MODULE) && defined(CONFIG_EXT4_USE_FOR_EXT23)\nstatic struct file_system_type ext3_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext3\",\n\t.get_sb\t\t= ext4_get_sb,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\n\nstatic inline void register_as_ext3(void)\n{\n\tint err = register_filesystem(&ext3_fs_type);\n\tif (err)\n\t\tprintk(KERN_WARNING\n\t\t       \"EXT4-fs: Unable to register as ext3 (%d)\\n\", err);\n}\n\nstatic inline void unregister_as_ext3(void)\n{\n\tunregister_filesystem(&ext3_fs_type);\n}\nMODULE_ALIAS(\"ext3\");\n#else\nstatic inline void register_as_ext3(void) { }\nstatic inline void unregister_as_ext3(void) { }\n#endif\n\nstatic struct file_system_type ext4_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"ext4\",\n\t.get_sb\t\t= ext4_get_sb,\n\t.kill_sb\t= kill_block_super,\n\t.fs_flags\t= FS_REQUIRES_DEV,\n};\n\nstatic int __init init_ext4_fs(void)\n{\n\tint err;\n\n\terr = init_ext4_system_zone();\n\tif (err)\n\t\treturn err;\n\text4_kset = kset_create_and_add(\"ext4\", NULL, fs_kobj);\n\tif (!ext4_kset)\n\t\tgoto out4;\n\text4_proc_root = proc_mkdir(\"fs/ext4\", NULL);\n\terr = init_ext4_mballoc();\n\tif (err)\n\t\tgoto out3;\n\n\terr = init_ext4_xattr();\n\tif (err)\n\t\tgoto out2;\n\terr = init_inodecache();\n\tif (err)\n\t\tgoto out1;\n\tregister_as_ext2();\n\tregister_as_ext3();\n\terr = register_filesystem(&ext4_fs_type);\n\tif (err)\n\t\tgoto out;\n\treturn 0;\nout:\n\tunregister_as_ext2();\n\tunregister_as_ext3();\n\tdestroy_inodecache();\nout1:\n\texit_ext4_xattr();\nout2:\n\texit_ext4_mballoc();\nout3:\n\tremove_proc_entry(\"fs/ext4\", NULL);\n\tkset_unregister(ext4_kset);\nout4:\n\texit_ext4_system_zone();\n\treturn err;\n}\n\nstatic void __exit exit_ext4_fs(void)\n{\n\tunregister_as_ext2();\n\tunregister_as_ext3();\n\tunregister_filesystem(&ext4_fs_type);\n\tdestroy_inodecache();\n\texit_ext4_xattr();\n\texit_ext4_mballoc();\n\tremove_proc_entry(\"fs/ext4\", NULL);\n\tkset_unregister(ext4_kset);\n\texit_ext4_system_zone();\n}\n\nMODULE_AUTHOR(\"Remy Card, Stephen Tweedie, Andrew Morton, Andreas Dilger, Theodore Ts'o and others\");\nMODULE_DESCRIPTION(\"Fourth Extended Filesystem\");\nMODULE_LICENSE(\"GPL\");\nmodule_init(init_ext4_fs)\nmodule_exit(exit_ext4_fs)\n"], "filenames": ["fs/ext4/ext4.h", "fs/ext4/ext4_jbd2.h", "fs/ext4/extents.c", "fs/ext4/inode.c", "fs/ext4/super.c"], "buggy_code_start_loc": [141, 306, 1622, 40, 711], "buggy_code_end_loc": [1783, 306, 3368, 3752, 2927], "fixing_code_start_loc": [141, 307, 1622, 41, 712], "fixing_code_end_loc": [1795, 331, 3371, 3887, 2957], "type": "NVD-CWE-Other", "message": "The ext4 implementation in the Linux kernel before 2.6.34 does not properly track the initialization of certain data structures, which allows physically proximate attackers to cause a denial of service (NULL pointer dereference and panic) via a crafted USB device, related to the ext4_fill_super function.", "other": {"cve": {"id": "CVE-2015-8324", "sourceIdentifier": "secalert@redhat.com", "published": "2016-05-02T10:59:18.577", "lastModified": "2023-02-13T00:55:09.680", "vulnStatus": "Modified", "evaluatorComment": "<a href=\"http://cwe.mitre.org/data/definitions/476.html\">CWE-476: NULL Pointer Dereference</a>", "descriptions": [{"lang": "en", "value": "The ext4 implementation in the Linux kernel before 2.6.34 does not properly track the initialization of certain data structures, which allows physically proximate attackers to cause a denial of service (NULL pointer dereference and panic) via a crafted USB device, related to the ext4_fill_super function."}, {"lang": "es", "value": "La implementaci\u00f3n de ext4 en el kernel de Linux en versiones anteriores a 2.6.34 no rastrea correctamente la inicalizaci\u00f3n de determinadas estructuras de datos, lo que permite a atacantes f\u00edsicamente pr\u00f3ximos provocar una denegaci\u00f3n de servicio (referencia a puntero NULL y p\u00e1nico) a trav\u00e9s de un dispositivo USB manipulado, relacionado con la funci\u00f3n ext4_fill_super."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:P/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "PHYSICAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.6, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 0.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.6.33.20", "matchCriteriaId": "B8187F7E-B31E-4C1C-8EB3-384193B7D7EE"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=744692dc059845b2a3022119871846e74d4f6e11", "source": "secalert@redhat.com", "tags": ["Vendor Advisory"]}, {"url": "http://mirror.linux.org.au/linux/kernel/v2.6/ChangeLog-2.6.34", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-0855.html", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2015/11/23/2", "source": "secalert@redhat.com"}, {"url": "http://www.oracle.com/technetwork/topics/security/linuxbulletinapr2016-2952096.html", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1267261", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/744692dc059845b2a3022119871846e74d4f6e11", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/744692dc059845b2a3022119871846e74d4f6e11"}}