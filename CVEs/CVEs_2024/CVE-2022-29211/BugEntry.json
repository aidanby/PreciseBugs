{"buggy_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/histogram_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\n\ntemplate <typename T, typename Tout>\nstruct HistogramFixedWidthFunctor<CPUDevice, T, Tout> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        const typename TTypes<T, 1>::ConstTensor& value_range,\n                        int32_t nbins, typename TTypes<Tout, 1>::Tensor& out) {\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    Tensor index_to_bin_tensor;\n\n    TF_RETURN_IF_ERROR(context->forward_input_or_allocate_temp(\n        {0}, DataTypeToEnum<int32>::value, TensorShape({values.size()}),\n        &index_to_bin_tensor));\n    auto index_to_bin = index_to_bin_tensor.flat<int32>();\n\n    const double step = static_cast<double>(value_range(1) - value_range(0)) /\n                        static_cast<double>(nbins);\n    const double nbins_minus_1 = static_cast<double>(nbins - 1);\n\n    // The calculation is done by finding the slot of each value in `values`.\n    // With [a, b]:\n    //   step = (b - a) / nbins\n    //   (x - a) / step\n    // , then the entries are mapped to output.\n\n    // Bug fix: Switch the order of cwiseMin and int32-casting to avoid\n    // producing a negative index when casting an big int64 number to int32\n    index_to_bin.device(d) =\n        ((values.cwiseMax(value_range(0)) - values.constant(value_range(0)))\n             .template cast<double>() /\n         step)\n            .cwiseMin(nbins_minus_1)\n            .template cast<int32>();\n\n    out.setZero();\n    for (int32_t i = 0; i < index_to_bin.size(); i++) {\n      out(index_to_bin(i)) += Tout(1);\n    }\n    return Status::OK();\n  }\n};\n\n}  // namespace functor\n\ntemplate <typename Device, typename T, typename Tout>\nclass HistogramFixedWidthOp : public OpKernel {\n public:\n  explicit HistogramFixedWidthOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& values_tensor = ctx->input(0);\n    const Tensor& value_range_tensor = ctx->input(1);\n    const Tensor& nbins_tensor = ctx->input(2);\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(value_range_tensor.shape()),\n                errors::InvalidArgument(\"value_range should be a vector.\"));\n    OP_REQUIRES(ctx, (value_range_tensor.shape().num_elements() == 2),\n                errors::InvalidArgument(\n                    \"value_range should be a vector of 2 elements.\"));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(nbins_tensor.shape()),\n                errors::InvalidArgument(\"nbins should be a scalar.\"));\n\n    const auto values = values_tensor.flat<T>();\n    const auto value_range = value_range_tensor.flat<T>();\n    const auto nbins = nbins_tensor.scalar<int32>()();\n\n    OP_REQUIRES(\n        ctx, (value_range(0) < value_range(1)),\n        errors::InvalidArgument(\"value_range should satisfy value_range[0] < \"\n                                \"value_range[1], but got '[\",\n                                value_range(0), \", \", value_range(1), \"]'\"));\n    OP_REQUIRES(\n        ctx, (nbins > 0),\n        errors::InvalidArgument(\"nbins should be a positive number, but got '\",\n                                nbins, \"'\"));\n\n    Tensor* out_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({nbins}), &out_tensor));\n    auto out = out_tensor->flat<Tout>();\n\n    OP_REQUIRES_OK(\n        ctx, functor::HistogramFixedWidthFunctor<Device, T, Tout>::Compute(\n                 ctx, values, value_range, nbins, out));\n  }\n};\n\n#define REGISTER_KERNELS(type)                                           \\\n  REGISTER_KERNEL_BUILDER(Name(\"HistogramFixedWidth\")                    \\\n                              .Device(DEVICE_CPU)                        \\\n                              .TypeConstraint<type>(\"T\")                 \\\n                              .TypeConstraint<int32>(\"dtype\"),           \\\n                          HistogramFixedWidthOp<CPUDevice, type, int32>) \\\n  REGISTER_KERNEL_BUILDER(Name(\"HistogramFixedWidth\")                    \\\n                              .Device(DEVICE_CPU)                        \\\n                              .TypeConstraint<type>(\"T\")                 \\\n                              .TypeConstraint<int64_t>(\"dtype\"),         \\\n                          HistogramFixedWidthOp<CPUDevice, type, int64>)\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_KERNELS(type)                                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"HistogramFixedWidth\")          \\\n                              .Device(DEVICE_GPU)              \\\n                              .HostMemory(\"value_range\")       \\\n                              .HostMemory(\"nbins\")             \\\n                              .TypeConstraint<type>(\"T\")       \\\n                              .TypeConstraint<int32>(\"dtype\"), \\\n                          HistogramFixedWidthOp<GPUDevice, type, int32>)\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // end namespace tensorflow\n"], "fixing_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/histogram_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\n\ntemplate <typename T, typename Tout>\nstruct HistogramFixedWidthFunctor<CPUDevice, T, Tout> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        const typename TTypes<T, 1>::ConstTensor& value_range,\n                        int32_t nbins, typename TTypes<Tout, 1>::Tensor& out) {\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    Tensor index_to_bin_tensor;\n\n    TF_RETURN_IF_ERROR(context->forward_input_or_allocate_temp(\n        {0}, DataTypeToEnum<int32>::value, TensorShape({values.size()}),\n        &index_to_bin_tensor));\n    auto index_to_bin = index_to_bin_tensor.flat<int32>();\n\n    const double step = static_cast<double>(value_range(1) - value_range(0)) /\n                        static_cast<double>(nbins);\n    const double nbins_minus_1 = static_cast<double>(nbins - 1);\n\n    // We cannot handle NANs in the algorithm below (due to the case to int32)\n    const Eigen::Tensor<int32, 1, 1> nans_tensor =\n        values.isnan().template cast<int32>();\n    const Eigen::Tensor<int32, 0, 1> reduced_tensor = nans_tensor.sum();\n    const int num_nans = reduced_tensor(0);\n    if (num_nans > 0) {\n      return errors::InvalidArgument(\"Histogram values must not contain NaN\");\n    }\n\n    // The calculation is done by finding the slot of each value in `values`.\n    // With [a, b]:\n    //   step = (b - a) / nbins\n    //   (x - a) / step\n    // , then the entries are mapped to output.\n\n    // Bug fix: Switch the order of cwiseMin and int32-casting to avoid\n    // producing a negative index when casting an big int64 number to int32\n    index_to_bin.device(d) =\n        ((values.cwiseMax(value_range(0)) - values.constant(value_range(0)))\n             .template cast<double>() /\n         step)\n            .cwiseMin(nbins_minus_1)\n            .template cast<int32>();\n\n    out.setZero();\n    for (int32_t i = 0; i < index_to_bin.size(); i++) {\n      out(index_to_bin(i)) += Tout(1);\n    }\n    return Status::OK();\n  }\n};\n\n}  // namespace functor\n\ntemplate <typename Device, typename T, typename Tout>\nclass HistogramFixedWidthOp : public OpKernel {\n public:\n  explicit HistogramFixedWidthOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& values_tensor = ctx->input(0);\n    const Tensor& value_range_tensor = ctx->input(1);\n    const Tensor& nbins_tensor = ctx->input(2);\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(value_range_tensor.shape()),\n                errors::InvalidArgument(\"value_range should be a vector.\"));\n    OP_REQUIRES(ctx, (value_range_tensor.shape().num_elements() == 2),\n                errors::InvalidArgument(\n                    \"value_range should be a vector of 2 elements.\"));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(nbins_tensor.shape()),\n                errors::InvalidArgument(\"nbins should be a scalar.\"));\n\n    const auto values = values_tensor.flat<T>();\n    const auto value_range = value_range_tensor.flat<T>();\n    const auto nbins = nbins_tensor.scalar<int32>()();\n\n    OP_REQUIRES(\n        ctx, value_range(0) < value_range(1),\n        errors::InvalidArgument(\"value_range should satisfy value_range[0] < \"\n                                \"value_range[1], but got '[\",\n                                value_range(0), \", \", value_range(1), \"]'\"));\n    OP_REQUIRES(\n        ctx, nbins > 0,\n        errors::InvalidArgument(\"nbins should be a positive number, but got '\",\n                                nbins, \"'\"));\n\n    Tensor* out_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({nbins}), &out_tensor));\n    auto out = out_tensor->flat<Tout>();\n\n    OP_REQUIRES_OK(\n        ctx, functor::HistogramFixedWidthFunctor<Device, T, Tout>::Compute(\n                 ctx, values, value_range, nbins, out));\n  }\n};\n\n#define REGISTER_KERNELS(type)                                           \\\n  REGISTER_KERNEL_BUILDER(Name(\"HistogramFixedWidth\")                    \\\n                              .Device(DEVICE_CPU)                        \\\n                              .TypeConstraint<type>(\"T\")                 \\\n                              .TypeConstraint<int32>(\"dtype\"),           \\\n                          HistogramFixedWidthOp<CPUDevice, type, int32>) \\\n  REGISTER_KERNEL_BUILDER(Name(\"HistogramFixedWidth\")                    \\\n                              .Device(DEVICE_CPU)                        \\\n                              .TypeConstraint<type>(\"T\")                 \\\n                              .TypeConstraint<int64_t>(\"dtype\"),         \\\n                          HistogramFixedWidthOp<CPUDevice, type, int64>)\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_KERNELS(type)                                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"HistogramFixedWidth\")          \\\n                              .Device(DEVICE_GPU)              \\\n                              .HostMemory(\"value_range\")       \\\n                              .HostMemory(\"nbins\")             \\\n                              .TypeConstraint<type>(\"T\")       \\\n                              .TypeConstraint<int32>(\"dtype\"), \\\n                          HistogramFixedWidthOp<GPUDevice, type, int32>)\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // end namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/histogram_op.cc"], "buggy_code_start_loc": [52], "buggy_code_end_loc": [107], "fixing_code_start_loc": [53], "fixing_code_end_loc": [116], "type": "CWE-20", "message": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.histogram_fixed_width` is vulnerable to a crash when the values array contain `Not a Number` (`NaN`) elements. The implementation assumes that all floating point operations are defined and then converts a floating point result to an integer index. If `values` contains `NaN` then the result of the division is still `NaN` and the cast to `int32` would result in a crash. This only occurs on the CPU implementation. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.", "other": {"cve": {"id": "CVE-2022-29211", "sourceIdentifier": "security-advisories@github.com", "published": "2022-05-21T00:15:11.650", "lastModified": "2022-06-03T15:02:31.067", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.histogram_fixed_width` is vulnerable to a crash when the values array contain `Not a Number` (`NaN`) elements. The implementation assumes that all floating point operations are defined and then converts a floating point result to an integer index. If `values` contains `NaN` then the result of the division is still `NaN` and the cast to `int32` would result in a crash. This only occurs on the CPU implementation. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En versiones anteriores a 2.9.0, 2.8.1, 2.7.2 y 2.6.4, la implementaci\u00f3n de \"tf.histogram_fixed_width\" es vulnerable a un fallo cuando el array de valores contiene elementos \"Not a Number\" (\"NaN\"). La implementaci\u00f3n asume que todas las operaciones de punto flotante est\u00e1n definidas y entonces convierte un resultado de punto flotante a un \u00edndice entero. Si \"values\" contiene \"NaN\" entonces el resultado de la divisi\u00f3n sigue siendo \"NaN\" y la conversi\u00f3n a \"int32\" resultar\u00eda en un bloqueo. Esto s\u00f3lo ocurre en la implementaci\u00f3n de la CPU. Las versiones 2.9.0, 2.8.1, 2.7.2 y 2.6.4 contienen un parche para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.4", "matchCriteriaId": "D9359D32-D090-44CF-AC43-2046084A28BB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.7.0", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C4DFBF2D-5283-42F6-8800-D653BFA5CE82"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "A58EDA5C-66D6-46F1-962E-60AFB7C784A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "89522760-C2DF-400D-9624-626D8F160CBA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:-:*:*:*:*:*:*", "matchCriteriaId": "E9EA1898-ACAA-4699-8BAE-54D62C1819FB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "130DE3C9-6842-456F-A259-BF8FF8457217"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "BBF2FCEF-989C-409D-9F4C-81418C65B972"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.9.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "9CFB1CFC-579D-4647-A472-6DE8BE1951DE"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.9.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "F3F3F37E-D27F-4060-830C-0AFF16150777"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/histogram_op.cc", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/blob/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/histogram_op.cc#L35-L74", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/e57fd691c7b0fd00ea3bfe43444f30c1969748b5", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/issues/45770", "source": "security-advisories@github.com", "tags": ["Exploit", "Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.6.4", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.7.2", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.8.1", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.9.0", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-xrp2-fhq4-4q3w", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/e57fd691c7b0fd00ea3bfe43444f30c1969748b5"}}