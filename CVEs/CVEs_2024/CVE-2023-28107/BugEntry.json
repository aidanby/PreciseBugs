{"buggy_code": ["# frozen_string_literal: true\n\nrequire \"backup_restore\"\nrequire \"backup_restore/backup_store\"\n\nclass Admin::BackupsController < Admin::AdminController\n  include ExternalUploadHelpers\n\n  before_action :ensure_backups_enabled\n  skip_before_action :check_xhr, only: %i[index show logs check_backup_chunk upload_backup_chunk]\n\n  def index\n    respond_to do |format|\n      format.html do\n        store_preloaded(\"operations_status\", MultiJson.dump(BackupRestore.operations_status))\n        store_preloaded(\"logs\", MultiJson.dump(BackupRestore.logs))\n        render \"default/empty\"\n      end\n\n      format.json do\n        store = BackupRestore::BackupStore.create\n\n        begin\n          render_serialized(store.files, BackupFileSerializer)\n        rescue BackupRestore::BackupStore::StorageError => e\n          render_json_error(e)\n        end\n      end\n    end\n  end\n\n  def status\n    render_json_dump(BackupRestore.operations_status)\n  end\n\n  def create\n    opts = {\n      publish_to_message_bus: true,\n      with_uploads: params.fetch(:with_uploads) == \"true\",\n      client_id: params[:client_id],\n    }\n    BackupRestore.backup!(current_user.id, opts)\n  rescue BackupRestore::OperationRunningError\n    render_error(\"backup.operation_already_running\")\n  else\n    StaffActionLogger.new(current_user).log_backup_create\n    render json: success_json\n  end\n\n  def cancel\n    BackupRestore.cancel!\n  rescue BackupRestore::OperationRunningError\n    render_error(\"backup.operation_already_running\")\n  else\n    render json: success_json\n  end\n\n  def email\n    store = BackupRestore::BackupStore.create\n\n    if store.file(params.fetch(:id)).present?\n      Jobs.enqueue(\n        :download_backup_email,\n        user_id: current_user.id,\n        backup_file_path: url_for(controller: \"backups\", action: \"show\"),\n      )\n\n      render body: nil\n    else\n      render body: nil, status: 404\n    end\n  end\n\n  def show\n    if !EmailBackupToken.compare(current_user.id, params.fetch(:token))\n      @error = I18n.t(\"download_backup_mailer.no_token\")\n      return render layout: \"no_ember\", status: 422, formats: [:html]\n    end\n\n    store = BackupRestore::BackupStore.create\n\n    if backup = store.file(params.fetch(:id), include_download_source: true)\n      EmailBackupToken.del(current_user.id)\n      StaffActionLogger.new(current_user).log_backup_download(backup)\n\n      if store.remote?\n        redirect_to backup.source, allow_other_host: true\n      else\n        headers[\"Content-Length\"] = File.size(backup.source).to_s\n        send_file backup.source\n      end\n    else\n      render body: nil, status: 404\n    end\n  end\n\n  def destroy\n    store = BackupRestore::BackupStore.create\n\n    if backup = store.file(params.fetch(:id))\n      StaffActionLogger.new(current_user).log_backup_destroy(backup)\n      store.delete_file(backup.filename)\n      render body: nil\n    else\n      render body: nil, status: 404\n    end\n  end\n\n  def logs\n    store_preloaded(\"operations_status\", MultiJson.dump(BackupRestore.operations_status))\n    store_preloaded(\"logs\", MultiJson.dump(BackupRestore.logs))\n    render \"default/empty\"\n  end\n\n  def restore\n    opts = {\n      filename: params.fetch(:id),\n      client_id: params.fetch(:client_id),\n      publish_to_message_bus: true,\n    }\n    BackupRestore.restore!(current_user.id, opts)\n  rescue BackupRestore::OperationRunningError\n    render_error(\"backup.operation_already_running\")\n  else\n    render json: success_json\n  end\n\n  def rollback\n    BackupRestore.rollback!\n  rescue BackupRestore::OperationRunningError\n    render_error(\"backup.operation_already_running\")\n  else\n    render json: success_json\n  end\n\n  def readonly\n    enable = params.fetch(:enable).to_s == \"true\"\n    readonly_mode_key = Discourse::USER_READONLY_MODE_KEY\n\n    if enable\n      Discourse.enable_readonly_mode(readonly_mode_key)\n    else\n      Discourse.disable_readonly_mode(readonly_mode_key)\n    end\n\n    StaffActionLogger.new(current_user).log_change_readonly_mode(enable)\n\n    render body: nil\n  end\n\n  def check_backup_chunk\n    identifier = params.fetch(:resumableIdentifier)\n    filename = params.fetch(:resumableFilename)\n    chunk_number = params.fetch(:resumableChunkNumber)\n    current_chunk_size = params.fetch(:resumableCurrentChunkSize).to_i\n\n    raise Discourse::InvalidParameters.new(:resumableIdentifier) unless valid_filename?(identifier)\n\n    # path to chunk file\n    chunk = BackupRestore::LocalBackupStore.chunk_path(identifier, filename, chunk_number)\n    # check chunk upload status\n    status = HandleChunkUpload.check_chunk(chunk, current_chunk_size: current_chunk_size)\n\n    render body: nil, status: status\n  end\n\n  def upload_backup_chunk\n    filename = params.fetch(:resumableFilename)\n    total_size = params.fetch(:resumableTotalSize).to_i\n    identifier = params.fetch(:resumableIdentifier)\n\n    raise Discourse::InvalidParameters.new(:resumableIdentifier) unless valid_filename?(identifier)\n    unless valid_extension?(filename)\n      return render status: 415, plain: I18n.t(\"backup.backup_file_should_be_tar_gz\")\n    end\n    unless has_enough_space_on_disk?(total_size)\n      return render status: 415, plain: I18n.t(\"backup.not_enough_space_on_disk\")\n    end\n    unless valid_filename?(filename)\n      return render status: 415, plain: I18n.t(\"backup.invalid_filename\")\n    end\n\n    file = params.fetch(:file)\n    chunk_number = params.fetch(:resumableChunkNumber).to_i\n    chunk_size = params.fetch(:resumableChunkSize).to_i\n    current_chunk_size = params.fetch(:resumableCurrentChunkSize).to_i\n    previous_chunk_number = chunk_number - 1\n\n    chunk = BackupRestore::LocalBackupStore.chunk_path(identifier, filename, chunk_number)\n    HandleChunkUpload.upload_chunk(chunk, file: file)\n\n    # when all chunks are uploaded\n    uploaded_file_size = previous_chunk_number * chunk_size\n    if uploaded_file_size + current_chunk_size >= total_size\n      # merge all the chunks in a background thread\n      Jobs.enqueue_in(\n        5.seconds,\n        :backup_chunks_merger,\n        filename: filename,\n        identifier: identifier,\n        chunks: chunk_number,\n      )\n    end\n\n    render body: nil\n  end\n\n  def create_upload_url\n    params.require(:filename)\n    filename = params.fetch(:filename)\n\n    unless valid_extension?(filename)\n      return render_json_error(I18n.t(\"backup.backup_file_should_be_tar_gz\"))\n    end\n    return render_json_error(I18n.t(\"backup.invalid_filename\")) unless valid_filename?(filename)\n\n    store = BackupRestore::BackupStore.create\n\n    begin\n      upload_url = store.generate_upload_url(filename)\n    rescue BackupRestore::BackupStore::BackupFileExists\n      return render_json_error(I18n.t(\"backup.file_exists\"))\n    rescue BackupRestore::BackupStore::StorageError => e\n      return render_json_error(e)\n    end\n\n    render json: success_json.merge(url: upload_url)\n  end\n\n  private\n\n  def has_enough_space_on_disk?(size)\n    DiskSpace.free(\"#{Rails.root}/public/backups\") > size\n  end\n\n  def ensure_backups_enabled\n    raise Discourse::InvalidAccess.new unless SiteSetting.enable_backups?\n  end\n\n  def valid_extension?(filename)\n    /\\.(tar\\.gz|t?gz)\\z/i =~ filename\n  end\n\n  def valid_filename?(filename)\n    !!(/\\A[a-zA-Z0-9\\._\\-]+\\z/ =~ filename)\n  end\n\n  def render_error(message_key)\n    render json: failed_json.merge(message: I18n.t(message_key))\n  end\n\n  def validate_before_create_multipart(file_name:, file_size:, upload_type:)\n    unless valid_extension?(file_name)\n      raise ExternalUploadHelpers::ExternalUploadValidationError.new(\n              I18n.t(\"backup.backup_file_should_be_tar_gz\"),\n            )\n    end\n    unless valid_filename?(file_name)\n      raise ExternalUploadHelpers::ExternalUploadValidationError.new(\n              I18n.t(\"backup.invalid_filename\"),\n            )\n    end\n  end\n\n  def self.serialize_upload(_upload)\n    {} # noop, the backup does not create an upload record\n  end\n\n  def create_direct_multipart_upload\n    begin\n      yield\n    rescue BackupRestore::BackupStore::StorageError => err\n      message =\n        debug_upload_error(\n          err,\n          I18n.t(\"upload.create_multipart_failure\", additional_detail: err.message),\n        )\n      raise ExternalUploadHelpers::ExternalUploadValidationError.new(message)\n    rescue BackupRestore::BackupStore::BackupFileExists\n      raise ExternalUploadHelpers::ExternalUploadValidationError.new(I18n.t(\"backup.file_exists\"))\n    end\n  end\nend\n", "# frozen_string_literal: true\n\nrequire \"mini_mime\"\nrequire \"file_store/s3_store\"\n\nmodule BackupRestore\n  class Backuper\n    attr_reader :success\n\n    def initialize(user_id, opts = {})\n      @user_id = user_id\n      @client_id = opts[:client_id]\n      @publish_to_message_bus = opts[:publish_to_message_bus] || false\n      @with_uploads = opts[:with_uploads] == false ? false : include_uploads?\n      @filename_override = opts[:filename]\n      @ticket = opts[:ticket]\n\n      initialize_state\n    end\n\n    def run\n      ensure_no_operation_is_running\n      ensure_we_have_a_user\n\n      log \"[STARTED]\"\n      log \"'#{@user.username}' has started the backup!\"\n\n      mark_backup_as_running\n\n      listen_for_shutdown_signal\n\n      ensure_directory_exists(@tmp_directory)\n      ensure_directory_exists(@archive_directory)\n\n      update_metadata\n      dump_public_schema\n\n      log \"Finalizing backup...\"\n\n      @with_uploads ? create_archive : move_dump_backup\n      upload_archive\n\n      after_create_hook\n    rescue SystemExit\n      log \"Backup process was cancelled!\"\n    rescue Exception => ex\n      log \"EXCEPTION: \" + ex.message\n      log ex.backtrace.join(\"\\n\")\n      @success = false\n    else\n      @success = true\n      @backup_filename\n    ensure\n      delete_old\n      clean_up\n      notify_user\n      log \"Finished!\"\n      publish_completion(@success)\n    end\n\n    protected\n\n    def ensure_no_operation_is_running\n      raise BackupRestore::OperationRunningError if BackupRestore.is_operation_running?\n    end\n\n    def ensure_we_have_a_user\n      raise Discourse::InvalidParameters.new(:user_id) unless @user\n    end\n\n    def get_parameterized_title\n      SiteSetting.title.parameterize.presence || \"discourse\"\n    end\n\n    def initialize_state\n      @success = false\n      @user = User.find_by(id: @user_id)\n      @logs = []\n      @store = BackupRestore::BackupStore.create\n      @current_db = RailsMultisite::ConnectionManagement.current_db\n      @timestamp = Time.now.strftime(\"%Y-%m-%d-%H%M%S\")\n      @tmp_directory = File.join(Rails.root, \"tmp\", \"backups\", @current_db, @timestamp)\n      @dump_filename = File.join(@tmp_directory, BackupRestore::DUMP_FILE)\n      @archive_directory = BackupRestore::LocalBackupStore.base_directory(db: @current_db)\n      filename = @filename_override || \"#{get_parameterized_title}-#{@timestamp}\"\n      @archive_basename =\n        File.join(\n          @archive_directory,\n          \"#{filename}-#{BackupRestore::VERSION_PREFIX}#{BackupRestore.current_version}\",\n        )\n\n      @backup_filename =\n        if @with_uploads\n          \"#{File.basename(@archive_basename)}.tar.gz\"\n        else\n          \"#{File.basename(@archive_basename)}.sql.gz\"\n        end\n    end\n\n    def listen_for_shutdown_signal\n      BackupRestore.clear_shutdown_signal!\n\n      Thread.new do\n        Thread.current.name = \"shutdown_wait\"\n\n        RailsMultisite::ConnectionManagement.with_connection(@current_db) do\n          while BackupRestore.is_operation_running?\n            exit if BackupRestore.should_shutdown?\n            sleep 0.1\n          end\n        end\n      end\n    end\n\n    def mark_backup_as_running\n      log \"Marking backup as running...\"\n      BackupRestore.mark_as_running!\n    end\n\n    def update_metadata\n      log \"Updating metadata...\"\n      BackupMetadata.delete_all\n      BackupMetadata.create!(name: \"base_url\", value: Discourse.base_url)\n      BackupMetadata.create!(name: \"cdn_url\", value: Discourse.asset_host)\n      BackupMetadata.create!(\n        name: \"s3_base_url\",\n        value: SiteSetting.Upload.enable_s3_uploads ? SiteSetting.Upload.s3_base_url : nil,\n      )\n      BackupMetadata.create!(\n        name: \"s3_cdn_url\",\n        value: SiteSetting.Upload.enable_s3_uploads ? SiteSetting.Upload.s3_cdn_url : nil,\n      )\n      BackupMetadata.create!(\n        name: \"db_name\",\n        value: RailsMultisite::ConnectionManagement.current_db,\n      )\n      BackupMetadata.create!(name: \"multisite\", value: Rails.configuration.multisite)\n    end\n\n    def dump_public_schema\n      log \"Dumping the public schema of the database...\"\n\n      logs = Queue.new\n      pg_dump_running = true\n\n      Thread.new do\n        RailsMultisite::ConnectionManagement.establish_connection(db: @current_db)\n        while pg_dump_running\n          message = logs.pop.strip\n          log(message) unless message.blank?\n        end\n      end\n\n      IO.popen(\"#{pg_dump_command} 2>&1\") do |pipe|\n        begin\n          while line = pipe.readline\n            logs << line\n          end\n        rescue EOFError\n          # finished reading...\n        ensure\n          pg_dump_running = false\n          logs << \"\"\n        end\n      end\n\n      raise \"pg_dump failed\" unless $?.success?\n    end\n\n    def pg_dump_command\n      db_conf = BackupRestore.database_configuration\n\n      password_argument = \"PGPASSWORD='#{db_conf.password}'\" if db_conf.password.present?\n      host_argument = \"--host=#{db_conf.host}\" if db_conf.host.present?\n      port_argument = \"--port=#{db_conf.port}\" if db_conf.port.present?\n      username_argument = \"--username=#{db_conf.username}\" if db_conf.username.present?\n\n      [\n        password_argument, # pass the password to pg_dump (if any)\n        \"pg_dump\", # the pg_dump command\n        \"--schema=public\", # only public schema\n        \"-T public.pg_*\", # exclude tables and views whose name starts with \"pg_\"\n        \"--file='#{@dump_filename}'\", # output to the dump.sql file\n        \"--no-owner\", # do not output commands to set ownership of objects\n        \"--no-privileges\", # prevent dumping of access privileges\n        \"--verbose\", # specifies verbose mode\n        \"--compress=4\", # Compression level of 4\n        host_argument, # the hostname to connect to (if any)\n        port_argument, # the port to connect to (if any)\n        username_argument, # the username to connect as (if any)\n        db_conf.database, # the name of the database to dump\n      ].join(\" \")\n    end\n\n    def move_dump_backup\n      log \"Finalizing database dump file: #{@backup_filename}\"\n\n      archive_filename = File.join(@archive_directory, @backup_filename)\n\n      Discourse::Utils.execute_command(\n        \"mv\",\n        @dump_filename,\n        archive_filename,\n        failure_message: \"Failed to move database dump file.\",\n      )\n\n      remove_tmp_directory\n    end\n\n    def create_archive\n      log \"Creating archive: #{@backup_filename}\"\n\n      tar_filename = \"#{@archive_basename}.tar\"\n\n      log \"Making sure archive does not already exist...\"\n      Discourse::Utils.execute_command(\"rm\", \"-f\", tar_filename)\n      Discourse::Utils.execute_command(\"rm\", \"-f\", \"#{tar_filename}.gz\")\n\n      log \"Creating empty archive...\"\n      Discourse::Utils.execute_command(\n        \"tar\",\n        \"--create\",\n        \"--file\",\n        tar_filename,\n        \"--files-from\",\n        \"/dev/null\",\n      )\n\n      log \"Archiving data dump...\"\n      Discourse::Utils.execute_command(\n        \"tar\",\n        \"--append\",\n        \"--dereference\",\n        \"--file\",\n        tar_filename,\n        File.basename(@dump_filename),\n        failure_message: \"Failed to archive data dump.\",\n        chdir: File.dirname(@dump_filename),\n      )\n\n      add_local_uploads_to_archive(tar_filename)\n      add_remote_uploads_to_archive(tar_filename) if SiteSetting.Upload.enable_s3_uploads\n\n      remove_tmp_directory\n\n      log \"Gzipping archive, this may take a while...\"\n      Discourse::Utils.execute_command(\n        \"gzip\",\n        \"-#{SiteSetting.backup_gzip_compression_level_for_uploads}\",\n        tar_filename,\n        failure_message: \"Failed to gzip archive.\",\n      )\n    end\n\n    def include_uploads?\n      has_local_uploads? || SiteSetting.include_s3_uploads_in_backups\n    end\n\n    def local_uploads_directory\n      @local_uploads_directory ||= File.join(Rails.root, \"public\", Discourse.store.upload_path)\n    end\n\n    def has_local_uploads?\n      File.directory?(local_uploads_directory) && !Dir.empty?(local_uploads_directory)\n    end\n\n    def add_local_uploads_to_archive(tar_filename)\n      log \"Archiving uploads...\"\n\n      if has_local_uploads?\n        upload_directory = Discourse.store.upload_path\n\n        if SiteSetting.include_thumbnails_in_backups\n          exclude_optimized = \"\"\n        else\n          optimized_path = File.join(upload_directory, \"optimized\")\n          exclude_optimized = \"--exclude=#{optimized_path}\"\n        end\n\n        Discourse::Utils.execute_command(\n          \"tar\",\n          \"--append\",\n          \"--dereference\",\n          exclude_optimized,\n          \"--file\",\n          tar_filename,\n          upload_directory,\n          failure_message: \"Failed to archive uploads.\",\n          success_status_codes: [0, 1],\n          chdir: File.join(Rails.root, \"public\"),\n        )\n      else\n        log \"No local uploads found. Skipping archiving of local uploads...\"\n      end\n    end\n\n    def add_remote_uploads_to_archive(tar_filename)\n      if !SiteSetting.include_s3_uploads_in_backups\n        log \"Skipping uploads stored on S3.\"\n        return\n      end\n\n      log \"Downloading uploads from S3. This may take a while...\"\n\n      store = FileStore::S3Store.new\n      upload_directory = Discourse.store.upload_path\n      count = 0\n\n      Upload.find_each do |upload|\n        next if upload.local?\n        filename = File.join(@tmp_directory, upload_directory, store.get_path_for_upload(upload))\n\n        begin\n          FileUtils.mkdir_p(File.dirname(filename))\n          store.download_file(upload, filename)\n        rescue StandardError => ex\n          log \"Failed to download file with upload ID #{upload.id} from S3\", ex\n        end\n\n        count += 1\n        log \"#{count} files have already been downloaded. Still downloading...\" if count % 500 == 0\n      end\n\n      log \"Appending uploads to archive...\"\n      Discourse::Utils.execute_command(\n        \"tar\",\n        \"--append\",\n        \"--file\",\n        tar_filename,\n        upload_directory,\n        failure_message: \"Failed to append uploads to archive.\",\n        success_status_codes: [0, 1],\n        chdir: @tmp_directory,\n      )\n\n      log \"No uploads found on S3. Skipping archiving of uploads stored on S3...\" if count == 0\n    end\n\n    def upload_archive\n      return unless @store.remote?\n\n      log \"Uploading archive...\"\n      content_type = MiniMime.lookup_by_filename(@backup_filename).content_type\n      archive_path = File.join(@archive_directory, @backup_filename)\n      @store.upload_file(@backup_filename, archive_path, content_type)\n    end\n\n    def after_create_hook\n      log \"Executing the after_create_hook for the backup...\"\n      DiscourseEvent.trigger(:backup_created)\n    end\n\n    def delete_old\n      return if Rails.env.development?\n\n      log \"Deleting old backups...\"\n      @store.delete_old\n    rescue => ex\n      log \"Something went wrong while deleting old backups.\", ex\n    end\n\n    def notify_user\n      return if @success && @user.id == Discourse::SYSTEM_USER_ID\n\n      log \"Notifying '#{@user.username}' of the end of the backup...\"\n      status = @success ? :backup_succeeded : :backup_failed\n\n      logs = Discourse::Utils.logs_markdown(@logs, user: @user)\n      post = SystemMessage.create_from_system_user(@user, status, logs: logs)\n\n      post.topic.invite_group(@user, Group[:admins]) if @user.id == Discourse::SYSTEM_USER_ID\n    rescue => ex\n      log \"Something went wrong while notifying user.\", ex\n    end\n\n    def clean_up\n      log \"Cleaning stuff up...\"\n      delete_uploaded_archive\n      remove_tar_leftovers\n      mark_backup_as_not_running\n      refresh_disk_space\n    end\n\n    def delete_uploaded_archive\n      return unless @store.remote?\n\n      archive_path = File.join(@archive_directory, @backup_filename)\n\n      if File.exist?(archive_path)\n        log \"Removing archive from local storage...\"\n        File.delete(archive_path)\n      end\n    rescue => ex\n      log \"Something went wrong while deleting uploaded archive from local storage.\", ex\n    end\n\n    def refresh_disk_space\n      log \"Refreshing disk stats...\"\n      @store.reset_cache\n    rescue => ex\n      log \"Something went wrong while refreshing disk stats.\", ex\n    end\n\n    def remove_tar_leftovers\n      log \"Removing '.tar' leftovers...\"\n      Dir[\"#{@archive_directory}/*.tar\"].each { |filename| File.delete(filename) }\n    rescue => ex\n      log \"Something went wrong while removing '.tar' leftovers.\", ex\n    end\n\n    def remove_tmp_directory\n      log \"Removing tmp '#{@tmp_directory}' directory...\"\n      FileUtils.rm_rf(@tmp_directory) if Dir[@tmp_directory].present?\n    rescue => ex\n      log \"Something went wrong while removing the following tmp directory: #{@tmp_directory}\", ex\n    end\n\n    def mark_backup_as_not_running\n      log \"Marking backup as finished...\"\n      BackupRestore.mark_as_not_running!\n    rescue => ex\n      log \"Something went wrong while marking backup as finished.\", ex\n    end\n\n    def ensure_directory_exists(directory)\n      log \"Making sure '#{directory}' exists...\"\n      FileUtils.mkdir_p(directory)\n    end\n\n    def log(message, ex = nil)\n      timestamp = Time.now.strftime(\"%Y-%m-%d %H:%M:%S\")\n      puts(message) if !Rails.env.test?\n      publish_log(message, timestamp)\n      save_log(message, timestamp)\n      Rails.logger.error(\"#{ex}\\n\" + ex.backtrace.join(\"\\n\")) if ex\n    end\n\n    def publish_log(message, timestamp)\n      return unless @publish_to_message_bus\n      data = { timestamp: timestamp, operation: \"backup\", message: message }\n      MessageBus.publish(\n        BackupRestore::LOGS_CHANNEL,\n        data,\n        user_ids: [@user_id],\n        client_ids: [@client_id],\n      )\n    end\n\n    def save_log(message, timestamp)\n      @logs << \"[#{timestamp}] #{message}\"\n    end\n\n    def publish_completion(success)\n      if success\n        log(\"[SUCCESS]\")\n        DiscourseEvent.trigger(:backup_complete, logs: @logs, ticket: @ticket)\n      else\n        log(\"[FAILED]\")\n        DiscourseEvent.trigger(:backup_failed, logs: @logs, ticket: @ticket)\n      end\n    end\n  end\nend\n", "# frozen_string_literal: true\n\nRSpec.describe BackupRestore::Backuper do\n  it \"returns a non-empty parameterized title when site title contains unicode\" do\n    SiteSetting.title = \"\u0194\"\n    backuper = BackupRestore::Backuper.new(Discourse.system_user.id)\n\n    expect(backuper.send(:get_parameterized_title)).to eq(\"discourse\")\n  end\n\n  it \"returns a valid parameterized site title\" do\n    SiteSetting.title = \"Coding Horror\"\n    backuper = BackupRestore::Backuper.new(Discourse.system_user.id)\n\n    expect(backuper.send(:get_parameterized_title)).to eq(\"coding-horror\")\n  end\n\n  describe \"#notify_user\" do\n    before { freeze_time Time.zone.parse(\"2010-01-01 12:00\") }\n\n    it \"includes logs if short\" do\n      SiteSetting.max_export_file_size_kb = 1\n      SiteSetting.export_authorized_extensions = \"tar.gz\"\n\n      silence_stdout do\n        backuper = BackupRestore::Backuper.new(Discourse.system_user.id)\n\n        expect { backuper.send(:notify_user) }.to change { Topic.private_messages.count }.by(\n          1,\n        ).and not_change { Upload.count }\n      end\n\n      expect(Topic.last.first_post.raw).to include(\n        \"```text\\n[2010-01-01 12:00:00] Notifying 'system' of the end of the backup...\\n```\",\n      )\n    end\n\n    it \"include upload if log is long\" do\n      SiteSetting.max_post_length = 250\n\n      silence_stdout do\n        backuper = BackupRestore::Backuper.new(Discourse.system_user.id)\n\n        expect { backuper.send(:notify_user) }.to change { Topic.private_messages.count }.by(\n          1,\n        ).and change { Upload.where(original_filename: \"log.txt.zip\").count }.by(1)\n      end\n\n      expect(Topic.last.first_post.raw).to include(\"[log.txt.zip|attachment]\")\n    end\n\n    it \"includes trimmed logs if log is long and upload cannot be saved\" do\n      SiteSetting.max_post_length = 348\n      SiteSetting.max_export_file_size_kb = 1\n      SiteSetting.export_authorized_extensions = \"tar.gz\"\n\n      silence_stdout do\n        backuper = BackupRestore::Backuper.new(Discourse.system_user.id)\n\n        1.upto(10).each { |i| backuper.send(:log, \"Line #{i}\") }\n\n        expect { backuper.send(:notify_user) }.to change { Topic.private_messages.count }.by(\n          1,\n        ).and not_change { Upload.count }\n      end\n\n      expect(Topic.last.first_post.raw).to include(\n        \"```text\\n...\\n[2010-01-01 12:00:00] Line 10\\n[2010-01-01 12:00:00] Notifying 'system' of the end of the backup...\\n```\",\n      )\n    end\n  end\nend\n", "# frozen_string_literal: true\n\nRSpec.describe Admin::BackupsController do\n  fab!(:admin) { Fabricate(:admin) }\n  fab!(:moderator) { Fabricate(:moderator) }\n  fab!(:user) { Fabricate(:user) }\n\n  let(:backup_filename) { \"2014-02-10-065935.tar.gz\" }\n  let(:backup_filename2) { \"2014-02-11-065935.tar.gz\" }\n\n  def create_backup_files(*filenames)\n    @paths =\n      filenames.map do |filename|\n        path = backup_path(filename)\n        File.open(path, \"w\") { |f| f.write(\"test backup\") }\n        path\n      end\n  end\n\n  def backup_path(filename)\n    File.join(BackupRestore::LocalBackupStore.base_directory, filename)\n  end\n\n  def map_preloaded\n    controller\n      .instance_variable_get(\"@preloaded\")\n      .map { |key, value| [key, JSON.parse(value)] }\n      .to_h\n  end\n\n  before { SiteSetting.backup_location = BackupLocationSiteSetting::LOCAL }\n\n  after do\n    Discourse.redis.flushdb\n\n    @paths&.each { |path| File.delete(path) if File.exist?(path) }\n    @paths = nil\n  end\n\n  describe \"#index\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"raises an error when backups are disabled\" do\n        SiteSetting.enable_backups = false\n        get \"/admin/backups.json\"\n        expect(response.status).to eq(403)\n      end\n\n      context \"with html format\" do\n        it \"preloads important data\" do\n          get \"/admin/backups.html\"\n          expect(response.status).to eq(200)\n\n          preloaded = map_preloaded\n          expect(preloaded[\"operations_status\"].symbolize_keys).to eq(\n            BackupRestore.operations_status,\n          )\n          expect(preloaded[\"logs\"].size).to eq(BackupRestore.logs.size)\n        end\n      end\n\n      context \"with json format\" do\n        it \"returns a list of all the backups\" do\n          begin\n            create_backup_files(backup_filename, backup_filename2)\n\n            get \"/admin/backups.json\"\n            expect(response.status).to eq(200)\n\n            filenames = response.parsed_body.map { |backup| backup[\"filename\"] }\n            expect(filenames).to include(backup_filename)\n            expect(filenames).to include(backup_filename2)\n          end\n        end\n      end\n    end\n\n    shared_examples \"backups inaccessible\" do\n      it \"denies access with a 404 response\" do\n        get \"/admin/backups.html\"\n\n        expect(response.status).to eq(404)\n\n        get \"/admin/backups.json\"\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backups inaccessible\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backups inaccessible\"\n    end\n  end\n\n  describe \"#status\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"returns the current backups status\" do\n        get \"/admin/backups/status.json\"\n        expect(response.body).to eq(BackupRestore.operations_status.to_json)\n        expect(response.status).to eq(200)\n      end\n    end\n\n    shared_examples \"status inaccessible\" do\n      it \"denies access with a 404 response\" do\n        get \"/admin/backups/status.json\"\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"status inaccessible\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"status inaccessible\"\n    end\n  end\n\n  describe \"#create\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"starts a backup\" do\n        BackupRestore.expects(:backup!).with(\n          admin.id,\n          { publish_to_message_bus: true, with_uploads: false, client_id: \"foo\" },\n        )\n\n        post \"/admin/backups.json\", params: { with_uploads: false, client_id: \"foo\" }\n\n        expect(response.status).to eq(200)\n      end\n    end\n\n    shared_examples \"backups creation not allowed\" do\n      it \"prevents backups creation with a 404 response\" do\n        post \"/admin/backups.json\", params: { with_uploads: false, client_id: \"foo\" }\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backups creation not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backups creation not allowed\"\n    end\n  end\n\n  describe \"#show\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"uses send_file to transmit the backup\" do\n        begin\n          token = EmailBackupToken.set(admin.id)\n          create_backup_files(backup_filename)\n\n          expect do\n            get \"/admin/backups/#{backup_filename}.json\", params: { token: token }\n          end.to change {\n            UserHistory.where(action: UserHistory.actions[:backup_download]).count\n          }.by(1)\n\n          expect(response.headers[\"Content-Length\"]).to eq(\"11\")\n          expect(response.headers[\"Content-Disposition\"]).to match(/attachment; filename/)\n        end\n      end\n\n      it \"returns 422 when token is bad\" do\n        begin\n          get \"/admin/backups/#{backup_filename}.json\", params: { token: \"bad_value\" }\n\n          expect(response.status).to eq(422)\n          expect(response.headers[\"Content-Disposition\"]).not_to match(/attachment; filename/)\n          expect(response.body).to include(I18n.t(\"download_backup_mailer.no_token\"))\n        end\n      end\n\n      it \"returns 404 when the backup does not exist\" do\n        token = EmailBackupToken.set(admin.id)\n        get \"/admin/backups/#{backup_filename}.json\", params: { token: token }\n\n        expect(response.status).to eq(404)\n      end\n    end\n\n    shared_examples \"backup inaccessible\" do\n      it \"denies access with a 404 response\" do\n        begin\n          token = EmailBackupToken.set(admin.id)\n          create_backup_files(backup_filename)\n\n          expect do\n            get \"/admin/backups/#{backup_filename}.json\", params: { token: token }\n          end.not_to change {\n            UserHistory.where(action: UserHistory.actions[:backup_download]).count\n          }\n\n          expect(response.status).to eq(404)\n          expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n          expect(response.headers[\"Content-Disposition\"]).not_to match(/attachment; filename/)\n        end\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup inaccessible\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup inaccessible\"\n    end\n  end\n\n  describe \"#destroy\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"removes the backup if found\" do\n        begin\n          path = backup_path(backup_filename)\n          create_backup_files(backup_filename)\n          expect(File.exist?(path)).to eq(true)\n\n          expect do delete \"/admin/backups/#{backup_filename}.json\" end.to change {\n            UserHistory.where(action: UserHistory.actions[:backup_destroy]).count\n          }.by(1)\n\n          expect(response.status).to eq(200)\n          expect(File.exist?(path)).to eq(false)\n        end\n      end\n\n      it \"doesn't remove the backup if not found\" do\n        delete \"/admin/backups/#{backup_filename}.json\"\n        expect(response.status).to eq(404)\n      end\n    end\n\n    shared_examples \"backup deletion not allowed\" do\n      it \"prevents deletion with a 404 response\" do\n        begin\n          path = backup_path(backup_filename)\n          create_backup_files(backup_filename)\n          expect(File.exist?(path)).to eq(true)\n\n          expect do delete \"/admin/backups/#{backup_filename}.json\" end.not_to change {\n            UserHistory.where(action: UserHistory.actions[:backup_destroy]).count\n          }\n\n          expect(response.status).to eq(404)\n          expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n          expect(File.exist?(path)).to eq(true)\n        end\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup deletion not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup deletion not allowed\"\n    end\n  end\n\n  describe \"#logs\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"preloads important data\" do\n        get \"/admin/backups/logs.html\"\n        expect(response.status).to eq(200)\n\n        preloaded = map_preloaded\n\n        expect(preloaded[\"operations_status\"].symbolize_keys).to eq(BackupRestore.operations_status)\n        expect(preloaded[\"logs\"].size).to eq(BackupRestore.logs.size)\n      end\n    end\n\n    shared_examples \"backup logs inaccessible\" do\n      it \"denies access with a 404 response\" do\n        get \"/admin/backups/logs.html\"\n\n        expect(response.status).to eq(404)\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup logs inaccessible\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup logs inaccessible\"\n    end\n  end\n\n  describe \"#restore\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"starts a restore\" do\n        BackupRestore.expects(:restore!).with(\n          admin.id,\n          { filename: backup_filename, publish_to_message_bus: true, client_id: \"foo\" },\n        )\n\n        post \"/admin/backups/#{backup_filename}/restore.json\", params: { client_id: \"foo\" }\n\n        expect(response.status).to eq(200)\n      end\n    end\n\n    shared_examples \"backup restoration not allowed\" do\n      it \"prevents restoration with a 404 response\" do\n        post \"/admin/backups/#{backup_filename}/restore.json\", params: { client_id: \"foo\" }\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup restoration not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup restoration not allowed\"\n    end\n  end\n\n  describe \"#readonly\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"enables readonly mode\" do\n        expect(Discourse.readonly_mode?).to eq(false)\n\n        expect { put \"/admin/backups/readonly.json\", params: { enable: true } }.to change {\n          UserHistory.where(\n            action: UserHistory.actions[:change_readonly_mode],\n            new_value: \"t\",\n          ).count\n        }.by(1)\n\n        expect(Discourse.readonly_mode?).to eq(true)\n        expect(response.status).to eq(200)\n      end\n\n      it \"disables readonly mode\" do\n        Discourse.enable_readonly_mode(Discourse::USER_READONLY_MODE_KEY)\n        expect(Discourse.readonly_mode?).to eq(true)\n\n        expect { put \"/admin/backups/readonly.json\", params: { enable: false } }.to change {\n          UserHistory.where(\n            action: UserHistory.actions[:change_readonly_mode],\n            new_value: \"f\",\n          ).count\n        }.by(1)\n\n        expect(response.status).to eq(200)\n        expect(Discourse.readonly_mode?).to eq(false)\n      end\n    end\n\n    shared_examples \"enabling readonly mode not allowed\" do\n      it \"prevents enabling readonly mode with a 404 response\" do\n        expect(Discourse.readonly_mode?).to eq(false)\n\n        expect do put \"/admin/backups/readonly.json\", params: { enable: true } end.not_to change {\n          UserHistory.where(\n            action: UserHistory.actions[:change_readonly_mode],\n            new_value: \"t\",\n          ).count\n        }\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n        expect(Discourse.readonly_mode?).to eq(false)\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"enabling readonly mode not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"enabling readonly mode not allowed\"\n    end\n  end\n\n  describe \"#upload_backup_chunk\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      describe \"when filename contains invalid characters\" do\n        it \"should raise an error\" do\n          [\"\u7070\u8272.tar.gz\", '; echo \\'haha\\'.tar.gz'].each do |invalid_filename|\n            described_class.any_instance.expects(:has_enough_space_on_disk?).returns(true)\n\n            post \"/admin/backups/upload\",\n                 params: {\n                   resumableFilename: invalid_filename,\n                   resumableTotalSize: 1,\n                   resumableIdentifier: \"test\",\n                 }\n\n            expect(response.status).to eq(415)\n            expect(response.body).to eq(I18n.t(\"backup.invalid_filename\"))\n          end\n        end\n      end\n\n      describe \"when resumableIdentifier is invalid\" do\n        it \"should raise an error\" do\n          filename = \"test_site-0123456789.tar.gz\"\n          @paths = [backup_path(File.join(\"tmp\", \"test\", \"#{filename}.part1\"))]\n\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: 1,\n                 resumableIdentifier: \"../test\",\n                 resumableChunkNumber: \"1\",\n                 resumableChunkSize: \"1\",\n                 resumableCurrentChunkSize: \"1\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n\n          expect(response.status).to eq(400)\n        end\n      end\n\n      describe \"when filename is valid\" do\n        it \"should upload the file successfully\" do\n          freeze_time\n          described_class.any_instance.expects(:has_enough_space_on_disk?).returns(true)\n\n          filename = \"test_Site-0123456789.tar.gz\"\n\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: 1,\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"1\",\n                 resumableChunkSize: \"1\",\n                 resumableCurrentChunkSize: \"1\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          expect_job_enqueued(\n            job: :backup_chunks_merger,\n            args: {\n              filename: filename,\n              identifier: \"test\",\n              chunks: 1,\n            },\n            at: 5.seconds.from_now,\n          )\n\n          expect(response.status).to eq(200)\n          expect(response.body).to eq(\"\")\n        end\n      end\n\n      describe \"completing an upload by enqueuing backup_chunks_merger\" do\n        let(:filename) { \"test_Site-0123456789.tar.gz\" }\n\n        it \"works with a single chunk\" do\n          freeze_time\n          described_class.any_instance.expects(:has_enough_space_on_disk?).returns(true)\n\n          # 2MB file, 2MB chunks = 1x 2MB chunk\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"2097152\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"1\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"2097152\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          expect_job_enqueued(\n            job: :backup_chunks_merger,\n            args: {\n              filename: filename,\n              identifier: \"test\",\n              chunks: 1,\n            },\n            at: 5.seconds.from_now,\n          )\n        end\n\n        it \"works with multiple chunks when the final chunk is chunk_size + remainder\" do\n          freeze_time\n          described_class.any_instance.expects(:has_enough_space_on_disk?).twice.returns(true)\n\n          # 5MB file, 2MB chunks = 1x 2MB chunk + 1x 3MB chunk with resumable.js\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"5242880\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"1\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"2097152\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"5242880\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"2\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"3145728\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          expect_job_enqueued(\n            job: :backup_chunks_merger,\n            args: {\n              filename: filename,\n              identifier: \"test\",\n              chunks: 2,\n            },\n            at: 5.seconds.from_now,\n          )\n        end\n\n        it \"works with multiple chunks when the final chunk is just the remaninder\" do\n          freeze_time\n          described_class.any_instance.expects(:has_enough_space_on_disk?).times(3).returns(true)\n\n          # 5MB file, 2MB chunks = 2x 2MB chunk + 1x 1MB chunk with uppy.js\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"5242880\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"1\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"2097152\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"5242880\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"2\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"2097152\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"5242880\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"3\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"1048576\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          expect_job_enqueued(\n            job: :backup_chunks_merger,\n            args: {\n              filename: filename,\n              identifier: \"test\",\n              chunks: 3,\n            },\n            at: 5.seconds.from_now,\n          )\n        end\n      end\n    end\n\n    shared_examples \"uploading backup chunk not allowed\" do\n      it \"prevents uploading of backup chunk with a 404 response\" do\n        freeze_time\n        filename = \"test_Site-0123456789.tar.gz\"\n\n        post \"/admin/backups/upload.json\",\n             params: {\n               resumableFilename: filename,\n               resumableTotalSize: 1,\n               resumableIdentifier: \"test\",\n               resumableChunkNumber: \"1\",\n               resumableChunkSize: \"1\",\n               resumableCurrentChunkSize: \"1\",\n               file: fixture_file_upload(Tempfile.new),\n             }\n\n        expect_not_enqueued_with(\n          job: :backup_chunks_merger,\n          args: {\n            filename: filename,\n            identifier: \"test\",\n            chunks: 1,\n          },\n          at: 5.seconds.from_now,\n        )\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"uploading backup chunk not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"uploading backup chunk not allowed\"\n    end\n  end\n\n  describe \"#check_backup_chunk\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      describe \"when resumableIdentifier is invalid\" do\n        it \"should raise an error\" do\n          get \"/admin/backups/upload\",\n              params: {\n                resumableidentifier: \"../some_file\",\n                resumablefilename: \"test_site-0123456789.tar.gz\",\n                resumablechunknumber: \"1\",\n                resumablecurrentchunksize: \"1\",\n              }\n\n          expect(response.status).to eq(400)\n        end\n      end\n    end\n\n    shared_examples \"checking backup chunk not allowed\" do\n      it \"denies access with a 404 response\" do\n        get \"/admin/backups/upload\",\n            params: {\n              resumableidentifier: \"../some_file\",\n              resumablefilename: \"test_site-0123456789.tar.gz\",\n              resumablechunknumber: \"1\",\n              resumablecurrentchunksize: \"1\",\n            }\n\n        expect(response.status).to eq(404)\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"checking backup chunk not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"checking backup chunk not allowed\"\n    end\n  end\n\n  describe \"#rollback\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"should rollback the restore\" do\n        BackupRestore.expects(:rollback!)\n\n        post \"/admin/backups/rollback.json\"\n\n        expect(response.status).to eq(200)\n      end\n\n      it \"should not allow rollback via a GET request\" do\n        get \"/admin/backups/rollback.json\"\n        expect(response.status).to eq(404)\n      end\n    end\n\n    shared_examples \"backup rollback not allowed\" do\n      it \"prevents rollbacks with a 404 response\" do\n        post \"/admin/backups/rollback.json\"\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup rollback not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup rollback not allowed\"\n    end\n  end\n\n  describe \"#cancel\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"should cancel an backup\" do\n        BackupRestore.expects(:cancel!)\n\n        delete \"/admin/backups/cancel.json\"\n\n        expect(response.status).to eq(200)\n      end\n\n      it \"should not allow cancel via a GET request\" do\n        get \"/admin/backups/cancel.json\"\n        expect(response.status).to eq(404)\n      end\n    end\n\n    shared_examples \"backup cancellation not allowed\" do\n      it \"prevents cancellation with a 404 response\" do\n        delete \"/admin/backups/cancel.json\"\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup cancellation not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup cancellation not allowed\"\n    end\n  end\n\n  describe \"#email\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"enqueues email job\" do\n        # might as well test this here if we really want www.example.com\n        SiteSetting.force_hostname = \"www.example.com\"\n\n        create_backup_files(backup_filename)\n\n        expect { put \"/admin/backups/#{backup_filename}.json\" }.to change {\n          Jobs::DownloadBackupEmail.jobs.size\n        }.by(1)\n\n        job_args = Jobs::DownloadBackupEmail.jobs.last[\"args\"].first\n        expect(job_args[\"user_id\"]).to eq(admin.id)\n        expect(job_args[\"backup_file_path\"]).to eq(\n          \"http://www.example.com/admin/backups/#{backup_filename}\",\n        )\n\n        expect(response.status).to eq(200)\n      end\n\n      it \"returns 404 when the backup does not exist\" do\n        put \"/admin/backups/#{backup_filename}.json\"\n\n        expect(response).to be_not_found\n      end\n    end\n\n    shared_examples \"backup emails not allowed\" do\n      it \"prevents sending backup emails with a 404 response\" do\n        SiteSetting.force_hostname = \"www.example.com\"\n        create_backup_files(backup_filename)\n\n        expect do put \"/admin/backups/#{backup_filename}.json\" end.not_to change {\n          Jobs::DownloadBackupEmail.jobs.size\n        }\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup emails not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup emails not allowed\"\n    end\n  end\n\n  describe \"S3 multipart uploads\" do\n    let(:upload_type) { \"backup\" }\n    let(:test_bucket_prefix) { \"test_#{ENV[\"TEST_ENV_NUMBER\"].presence || \"0\"}\" }\n    let(:backup_file_exists_response) { { status: 404 } }\n    let(:mock_multipart_upload_id) do\n      \"ibZBv_75gd9r8lH_gqXatLdxMVpAlj6CFTR.OwyF3953YdwbcQnMA2BLGn8Lx12fQNICtMw5KyteFeHw.Sjng--\"\n    end\n\n    before do\n      setup_s3\n      SiteSetting.enable_direct_s3_uploads = true\n      SiteSetting.s3_backup_bucket = \"s3-backup-bucket\"\n      SiteSetting.backup_location = BackupLocationSiteSetting::S3\n      stub_request(:head, \"https://s3-backup-bucket.s3.us-west-1.amazonaws.com/\").to_return(\n        status: 200,\n        body: \"\",\n        headers: {\n        },\n      )\n      stub_request(\n        :head,\n        \"https://s3-backup-bucket.s3.us-west-1.amazonaws.com/default/test.tar.gz\",\n      ).to_return(backup_file_exists_response)\n    end\n\n    shared_examples \"multipart uploads not allowed\" do\n      it \"prevents multipart uploads with a 404 response\" do\n        post \"/admin/backups/create-multipart.json\",\n             params: {\n               file_name: \"test.tar.gz\",\n               upload_type: upload_type,\n               file_size: 4098,\n             }\n        expect(response.status).to eq(404)\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"multipart uploads not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"multipart uploads not allowed\"\n    end\n\n    context \"when the user is admin\" do\n      before { sign_in(admin) }\n\n      def stub_create_multipart_backup_request\n        BackupRestore::S3BackupStore\n          .any_instance\n          .stubs(:temporary_upload_path)\n          .returns(\n            \"temp/default/#{test_bucket_prefix}/28fccf8259bbe75b873a2bd2564b778c/2u98j832nx93272x947823.gz\",\n          )\n        create_multipart_result = <<~XML\n        <?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\n        <InitiateMultipartUploadResult>\n           <Bucket>s3-backup-bucket</Bucket>\n           <Key>temp/default/#{test_bucket_prefix}/28fccf8259bbe75b873a2bd2564b778c/2u98j832nx93272x947823.gz</Key>\n           <UploadId>#{mock_multipart_upload_id}</UploadId>\n        </InitiateMultipartUploadResult>\n        XML\n        stub_request(\n          :post,\n          \"https://s3-backup-bucket.s3.us-west-1.amazonaws.com/temp/default/#{test_bucket_prefix}/28fccf8259bbe75b873a2bd2564b778c/2u98j832nx93272x947823.gz?uploads\",\n        ).to_return(status: 200, body: create_multipart_result)\n      end\n\n      it \"creates the multipart upload\" do\n        stub_create_multipart_backup_request\n        post \"/admin/backups/create-multipart.json\",\n             params: {\n               file_name: \"test.tar.gz\",\n               upload_type: upload_type,\n               file_size: 4098,\n             }\n        expect(response.status).to eq(200)\n        result = response.parsed_body\n\n        external_upload_stub =\n          ExternalUploadStub.where(\n            unique_identifier: result[\"unique_identifier\"],\n            original_filename: \"test.tar.gz\",\n            created_by: admin,\n            upload_type: upload_type,\n            key: result[\"key\"],\n            multipart: true,\n          )\n        expect(external_upload_stub.exists?).to eq(true)\n      end\n\n      context \"when backup of same filename already exists\" do\n        let(:backup_file_exists_response) { { status: 200, body: \"\" } }\n\n        it \"throws an error\" do\n          post \"/admin/backups/create-multipart.json\",\n               params: {\n                 file_name: \"test.tar.gz\",\n                 upload_type: upload_type,\n                 file_size: 4098,\n               }\n          expect(response.status).to eq(422)\n          expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"backup.file_exists\"))\n        end\n      end\n\n      context \"when filename is invalid\" do\n        it \"throws an error\" do\n          post \"/admin/backups/create-multipart.json\",\n               params: {\n                 file_name: \"blah $$##.tar.gz\",\n                 upload_type: upload_type,\n                 file_size: 4098,\n               }\n          expect(response.status).to eq(422)\n          expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"backup.invalid_filename\"))\n        end\n      end\n\n      context \"when extension is invalid\" do\n        it \"throws an error\" do\n          post \"/admin/backups/create-multipart.json\",\n               params: {\n                 file_name: \"test.png\",\n                 upload_type: upload_type,\n                 file_size: 4098,\n               }\n          expect(response.status).to eq(422)\n          expect(response.parsed_body[\"errors\"]).to include(\n            I18n.t(\"backup.backup_file_should_be_tar_gz\"),\n          )\n        end\n      end\n    end\n  end\nend\n"], "fixing_code": ["# frozen_string_literal: true\n\nrequire \"backup_restore\"\nrequire \"backup_restore/backup_store\"\n\nclass Admin::BackupsController < Admin::AdminController\n  include ExternalUploadHelpers\n\n  before_action :ensure_backups_enabled\n  skip_before_action :check_xhr, only: %i[index show logs check_backup_chunk upload_backup_chunk]\n\n  def index\n    respond_to do |format|\n      format.html do\n        store_preloaded(\"operations_status\", MultiJson.dump(BackupRestore.operations_status))\n        store_preloaded(\"logs\", MultiJson.dump(BackupRestore.logs))\n        render \"default/empty\"\n      end\n\n      format.json do\n        store = BackupRestore::BackupStore.create\n\n        begin\n          render_serialized(store.files, BackupFileSerializer)\n        rescue BackupRestore::BackupStore::StorageError => e\n          render_json_error(e)\n        end\n      end\n    end\n  end\n\n  def status\n    render_json_dump(BackupRestore.operations_status)\n  end\n\n  def create\n    RateLimiter.new(\n      current_user,\n      \"max-backups-per-minute\",\n      1,\n      1.minute,\n      apply_limit_to_staff: true,\n    ).performed!\n\n    opts = {\n      publish_to_message_bus: true,\n      with_uploads: params.fetch(:with_uploads) == \"true\",\n      client_id: params[:client_id],\n    }\n    BackupRestore.backup!(current_user.id, opts)\n  rescue BackupRestore::OperationRunningError\n    render_error(\"backup.operation_already_running\")\n  else\n    StaffActionLogger.new(current_user).log_backup_create\n    render json: success_json\n  end\n\n  def cancel\n    BackupRestore.cancel!\n  rescue BackupRestore::OperationRunningError\n    render_error(\"backup.operation_already_running\")\n  else\n    render json: success_json\n  end\n\n  def email\n    store = BackupRestore::BackupStore.create\n\n    if store.file(params.fetch(:id)).present?\n      Jobs.enqueue(\n        :download_backup_email,\n        user_id: current_user.id,\n        backup_file_path: url_for(controller: \"backups\", action: \"show\"),\n      )\n\n      render body: nil\n    else\n      render body: nil, status: 404\n    end\n  end\n\n  def show\n    if !EmailBackupToken.compare(current_user.id, params.fetch(:token))\n      @error = I18n.t(\"download_backup_mailer.no_token\")\n      return render layout: \"no_ember\", status: 422, formats: [:html]\n    end\n\n    store = BackupRestore::BackupStore.create\n\n    if backup = store.file(params.fetch(:id), include_download_source: true)\n      EmailBackupToken.del(current_user.id)\n      StaffActionLogger.new(current_user).log_backup_download(backup)\n\n      if store.remote?\n        redirect_to backup.source, allow_other_host: true\n      else\n        headers[\"Content-Length\"] = File.size(backup.source).to_s\n        send_file backup.source\n      end\n    else\n      render body: nil, status: 404\n    end\n  end\n\n  def destroy\n    store = BackupRestore::BackupStore.create\n\n    if backup = store.file(params.fetch(:id))\n      StaffActionLogger.new(current_user).log_backup_destroy(backup)\n      store.delete_file(backup.filename)\n      render body: nil\n    else\n      render body: nil, status: 404\n    end\n  end\n\n  def logs\n    store_preloaded(\"operations_status\", MultiJson.dump(BackupRestore.operations_status))\n    store_preloaded(\"logs\", MultiJson.dump(BackupRestore.logs))\n    render \"default/empty\"\n  end\n\n  def restore\n    opts = {\n      filename: params.fetch(:id),\n      client_id: params.fetch(:client_id),\n      publish_to_message_bus: true,\n    }\n    BackupRestore.restore!(current_user.id, opts)\n  rescue BackupRestore::OperationRunningError\n    render_error(\"backup.operation_already_running\")\n  else\n    render json: success_json\n  end\n\n  def rollback\n    BackupRestore.rollback!\n  rescue BackupRestore::OperationRunningError\n    render_error(\"backup.operation_already_running\")\n  else\n    render json: success_json\n  end\n\n  def readonly\n    enable = params.fetch(:enable).to_s == \"true\"\n    readonly_mode_key = Discourse::USER_READONLY_MODE_KEY\n\n    if enable\n      Discourse.enable_readonly_mode(readonly_mode_key)\n    else\n      Discourse.disable_readonly_mode(readonly_mode_key)\n    end\n\n    StaffActionLogger.new(current_user).log_change_readonly_mode(enable)\n\n    render body: nil\n  end\n\n  def check_backup_chunk\n    identifier = params.fetch(:resumableIdentifier)\n    filename = params.fetch(:resumableFilename)\n    chunk_number = params.fetch(:resumableChunkNumber)\n    current_chunk_size = params.fetch(:resumableCurrentChunkSize).to_i\n\n    raise Discourse::InvalidParameters.new(:resumableIdentifier) unless valid_filename?(identifier)\n\n    # path to chunk file\n    chunk = BackupRestore::LocalBackupStore.chunk_path(identifier, filename, chunk_number)\n    # check chunk upload status\n    status = HandleChunkUpload.check_chunk(chunk, current_chunk_size: current_chunk_size)\n\n    render body: nil, status: status\n  end\n\n  def upload_backup_chunk\n    filename = params.fetch(:resumableFilename)\n    total_size = params.fetch(:resumableTotalSize).to_i\n    identifier = params.fetch(:resumableIdentifier)\n\n    raise Discourse::InvalidParameters.new(:resumableIdentifier) unless valid_filename?(identifier)\n    unless valid_extension?(filename)\n      return render status: 415, plain: I18n.t(\"backup.backup_file_should_be_tar_gz\")\n    end\n    unless has_enough_space_on_disk?(total_size)\n      return render status: 415, plain: I18n.t(\"backup.not_enough_space_on_disk\")\n    end\n    unless valid_filename?(filename)\n      return render status: 415, plain: I18n.t(\"backup.invalid_filename\")\n    end\n\n    file = params.fetch(:file)\n    chunk_number = params.fetch(:resumableChunkNumber).to_i\n    chunk_size = params.fetch(:resumableChunkSize).to_i\n    current_chunk_size = params.fetch(:resumableCurrentChunkSize).to_i\n    previous_chunk_number = chunk_number - 1\n\n    chunk = BackupRestore::LocalBackupStore.chunk_path(identifier, filename, chunk_number)\n    HandleChunkUpload.upload_chunk(chunk, file: file)\n\n    # when all chunks are uploaded\n    uploaded_file_size = previous_chunk_number * chunk_size\n    if uploaded_file_size + current_chunk_size >= total_size\n      # merge all the chunks in a background thread\n      Jobs.enqueue_in(\n        5.seconds,\n        :backup_chunks_merger,\n        filename: filename,\n        identifier: identifier,\n        chunks: chunk_number,\n      )\n    end\n\n    render body: nil\n  end\n\n  def create_upload_url\n    params.require(:filename)\n    filename = params.fetch(:filename)\n\n    unless valid_extension?(filename)\n      return render_json_error(I18n.t(\"backup.backup_file_should_be_tar_gz\"))\n    end\n    return render_json_error(I18n.t(\"backup.invalid_filename\")) unless valid_filename?(filename)\n\n    store = BackupRestore::BackupStore.create\n\n    begin\n      upload_url = store.generate_upload_url(filename)\n    rescue BackupRestore::BackupStore::BackupFileExists\n      return render_json_error(I18n.t(\"backup.file_exists\"))\n    rescue BackupRestore::BackupStore::StorageError => e\n      return render_json_error(e)\n    end\n\n    render json: success_json.merge(url: upload_url)\n  end\n\n  private\n\n  def has_enough_space_on_disk?(size)\n    DiskSpace.free(\"#{Rails.root}/public/backups\") > size\n  end\n\n  def ensure_backups_enabled\n    raise Discourse::InvalidAccess.new unless SiteSetting.enable_backups?\n  end\n\n  def valid_extension?(filename)\n    /\\.(tar\\.gz|t?gz)\\z/i =~ filename\n  end\n\n  def valid_filename?(filename)\n    !!(/\\A[a-zA-Z0-9\\._\\-]+\\z/ =~ filename)\n  end\n\n  def render_error(message_key)\n    render json: failed_json.merge(message: I18n.t(message_key))\n  end\n\n  def validate_before_create_multipart(file_name:, file_size:, upload_type:)\n    unless valid_extension?(file_name)\n      raise ExternalUploadHelpers::ExternalUploadValidationError.new(\n              I18n.t(\"backup.backup_file_should_be_tar_gz\"),\n            )\n    end\n    unless valid_filename?(file_name)\n      raise ExternalUploadHelpers::ExternalUploadValidationError.new(\n              I18n.t(\"backup.invalid_filename\"),\n            )\n    end\n  end\n\n  def self.serialize_upload(_upload)\n    {} # noop, the backup does not create an upload record\n  end\n\n  def create_direct_multipart_upload\n    begin\n      yield\n    rescue BackupRestore::BackupStore::StorageError => err\n      message =\n        debug_upload_error(\n          err,\n          I18n.t(\"upload.create_multipart_failure\", additional_detail: err.message),\n        )\n      raise ExternalUploadHelpers::ExternalUploadValidationError.new(message)\n    rescue BackupRestore::BackupStore::BackupFileExists\n      raise ExternalUploadHelpers::ExternalUploadValidationError.new(I18n.t(\"backup.file_exists\"))\n    end\n  end\nend\n", "# frozen_string_literal: true\n\nrequire \"mini_mime\"\nrequire \"file_store/s3_store\"\n\nmodule BackupRestore\n  class Backuper\n    attr_reader :success, :store\n\n    def initialize(user_id, opts = {})\n      @user_id = user_id\n      @client_id = opts[:client_id]\n      @publish_to_message_bus = opts[:publish_to_message_bus] || false\n      @with_uploads = opts[:with_uploads] == false ? false : include_uploads?\n      @filename_override = opts[:filename]\n      @ticket = opts[:ticket]\n\n      initialize_state\n    end\n\n    def run\n      ensure_no_operation_is_running\n      ensure_we_have_a_user\n\n      log \"[STARTED]\"\n      log \"'#{@user.username}' has started the backup!\"\n\n      mark_backup_as_running\n\n      listen_for_shutdown_signal\n\n      ensure_directory_exists(@tmp_directory)\n      ensure_directory_exists(@archive_directory)\n\n      update_metadata\n      dump_public_schema\n\n      log \"Finalizing backup...\"\n\n      @with_uploads ? create_archive : move_dump_backup\n      upload_archive\n\n      after_create_hook\n    rescue SystemExit\n      log \"Backup process was cancelled!\"\n    rescue Exception => ex\n      log \"EXCEPTION: \" + ex.message\n      log ex.backtrace.join(\"\\n\")\n    else\n      @success = true\n      @backup_filename\n    ensure\n      delete_old\n      clean_up\n      notify_user\n      log \"Finished!\"\n      publish_completion\n    end\n\n    protected\n\n    def ensure_no_operation_is_running\n      raise BackupRestore::OperationRunningError if BackupRestore.is_operation_running?\n    end\n\n    def ensure_we_have_a_user\n      raise Discourse::InvalidParameters.new(:user_id) unless @user\n    end\n\n    def get_parameterized_title\n      SiteSetting.title.parameterize.presence || \"discourse\"\n    end\n\n    def initialize_state\n      @success = false\n      @user = User.find_by(id: @user_id)\n      @logs = []\n      @store = BackupRestore::BackupStore.create\n      @current_db = RailsMultisite::ConnectionManagement.current_db\n      @timestamp = Time.now.strftime(\"%Y-%m-%d-%H%M%S\")\n      @tmp_directory = File.join(Rails.root, \"tmp\", \"backups\", @current_db, @timestamp)\n      @dump_filename = File.join(@tmp_directory, BackupRestore::DUMP_FILE)\n      @archive_directory = BackupRestore::LocalBackupStore.base_directory(db: @current_db)\n      filename = @filename_override || \"#{get_parameterized_title}-#{@timestamp}\"\n      @archive_basename =\n        File.join(\n          @archive_directory,\n          \"#{filename}-#{BackupRestore::VERSION_PREFIX}#{BackupRestore.current_version}\",\n        )\n\n      @backup_filename =\n        if @with_uploads\n          \"#{File.basename(@archive_basename)}.tar.gz\"\n        else\n          \"#{File.basename(@archive_basename)}.sql.gz\"\n        end\n    end\n\n    def listen_for_shutdown_signal\n      BackupRestore.clear_shutdown_signal!\n\n      Thread.new do\n        Thread.current.name = \"shutdown_wait\"\n\n        RailsMultisite::ConnectionManagement.with_connection(@current_db) do\n          while BackupRestore.is_operation_running?\n            exit if BackupRestore.should_shutdown?\n            sleep 0.1\n          end\n        end\n      end\n    end\n\n    def mark_backup_as_running\n      log \"Marking backup as running...\"\n      BackupRestore.mark_as_running!\n    end\n\n    def update_metadata\n      log \"Updating metadata...\"\n      BackupMetadata.delete_all\n      BackupMetadata.create!(name: \"base_url\", value: Discourse.base_url)\n      BackupMetadata.create!(name: \"cdn_url\", value: Discourse.asset_host)\n      BackupMetadata.create!(\n        name: \"s3_base_url\",\n        value: SiteSetting.Upload.enable_s3_uploads ? SiteSetting.Upload.s3_base_url : nil,\n      )\n      BackupMetadata.create!(\n        name: \"s3_cdn_url\",\n        value: SiteSetting.Upload.enable_s3_uploads ? SiteSetting.Upload.s3_cdn_url : nil,\n      )\n      BackupMetadata.create!(\n        name: \"db_name\",\n        value: RailsMultisite::ConnectionManagement.current_db,\n      )\n      BackupMetadata.create!(name: \"multisite\", value: Rails.configuration.multisite)\n    end\n\n    def dump_public_schema\n      log \"Dumping the public schema of the database...\"\n\n      logs = Queue.new\n      pg_dump_running = true\n\n      Thread.new do\n        RailsMultisite::ConnectionManagement.establish_connection(db: @current_db)\n        while pg_dump_running\n          message = logs.pop.strip\n          log(message) unless message.blank?\n        end\n      end\n\n      IO.popen(\"#{pg_dump_command} 2>&1\") do |pipe|\n        begin\n          while line = pipe.readline\n            logs << line\n          end\n        rescue EOFError\n          # finished reading...\n        ensure\n          pg_dump_running = false\n          logs << \"\"\n        end\n      end\n\n      raise \"pg_dump failed\" unless $?.success?\n    end\n\n    def pg_dump_command\n      db_conf = BackupRestore.database_configuration\n\n      password_argument = \"PGPASSWORD='#{db_conf.password}'\" if db_conf.password.present?\n      host_argument = \"--host=#{db_conf.host}\" if db_conf.host.present?\n      port_argument = \"--port=#{db_conf.port}\" if db_conf.port.present?\n      username_argument = \"--username=#{db_conf.username}\" if db_conf.username.present?\n\n      [\n        password_argument, # pass the password to pg_dump (if any)\n        \"pg_dump\", # the pg_dump command\n        \"--schema=public\", # only public schema\n        \"-T public.pg_*\", # exclude tables and views whose name starts with \"pg_\"\n        \"--file='#{@dump_filename}'\", # output to the dump.sql file\n        \"--no-owner\", # do not output commands to set ownership of objects\n        \"--no-privileges\", # prevent dumping of access privileges\n        \"--verbose\", # specifies verbose mode\n        \"--compress=4\", # Compression level of 4\n        host_argument, # the hostname to connect to (if any)\n        port_argument, # the port to connect to (if any)\n        username_argument, # the username to connect as (if any)\n        db_conf.database, # the name of the database to dump\n      ].join(\" \")\n    end\n\n    def move_dump_backup\n      log \"Finalizing database dump file: #{@backup_filename}\"\n\n      archive_filename = File.join(@archive_directory, @backup_filename)\n\n      Discourse::Utils.execute_command(\n        \"mv\",\n        @dump_filename,\n        archive_filename,\n        failure_message: \"Failed to move database dump file.\",\n      )\n\n      remove_tmp_directory\n    end\n\n    def create_archive\n      log \"Creating archive: #{@backup_filename}\"\n\n      tar_filename = \"#{@archive_basename}.tar\"\n\n      log \"Making sure archive does not already exist...\"\n      Discourse::Utils.execute_command(\"rm\", \"-f\", tar_filename)\n      Discourse::Utils.execute_command(\"rm\", \"-f\", \"#{tar_filename}.gz\")\n\n      log \"Creating empty archive...\"\n      Discourse::Utils.execute_command(\n        \"tar\",\n        \"--create\",\n        \"--file\",\n        tar_filename,\n        \"--files-from\",\n        \"/dev/null\",\n      )\n\n      log \"Archiving data dump...\"\n      Discourse::Utils.execute_command(\n        \"tar\",\n        \"--append\",\n        \"--dereference\",\n        \"--file\",\n        tar_filename,\n        File.basename(@dump_filename),\n        failure_message: \"Failed to archive data dump.\",\n        chdir: File.dirname(@dump_filename),\n      )\n\n      add_local_uploads_to_archive(tar_filename)\n      add_remote_uploads_to_archive(tar_filename) if SiteSetting.Upload.enable_s3_uploads\n\n      remove_tmp_directory\n\n      log \"Gzipping archive, this may take a while...\"\n      Discourse::Utils.execute_command(\n        \"gzip\",\n        \"-#{SiteSetting.backup_gzip_compression_level_for_uploads}\",\n        tar_filename,\n        failure_message: \"Failed to gzip archive.\",\n      )\n    end\n\n    def include_uploads?\n      has_local_uploads? || SiteSetting.include_s3_uploads_in_backups\n    end\n\n    def local_uploads_directory\n      @local_uploads_directory ||= File.join(Rails.root, \"public\", Discourse.store.upload_path)\n    end\n\n    def has_local_uploads?\n      File.directory?(local_uploads_directory) && !Dir.empty?(local_uploads_directory)\n    end\n\n    def add_local_uploads_to_archive(tar_filename)\n      log \"Archiving uploads...\"\n\n      if has_local_uploads?\n        upload_directory = Discourse.store.upload_path\n\n        if SiteSetting.include_thumbnails_in_backups\n          exclude_optimized = \"\"\n        else\n          optimized_path = File.join(upload_directory, \"optimized\")\n          exclude_optimized = \"--exclude=#{optimized_path}\"\n        end\n\n        Discourse::Utils.execute_command(\n          \"tar\",\n          \"--append\",\n          \"--dereference\",\n          exclude_optimized,\n          \"--file\",\n          tar_filename,\n          upload_directory,\n          failure_message: \"Failed to archive uploads.\",\n          success_status_codes: [0, 1],\n          chdir: File.join(Rails.root, \"public\"),\n        )\n      else\n        log \"No local uploads found. Skipping archiving of local uploads...\"\n      end\n    end\n\n    def add_remote_uploads_to_archive(tar_filename)\n      if !SiteSetting.include_s3_uploads_in_backups\n        log \"Skipping uploads stored on S3.\"\n        return\n      end\n\n      log \"Downloading uploads from S3. This may take a while...\"\n\n      store = FileStore::S3Store.new\n      upload_directory = Discourse.store.upload_path\n      count = 0\n\n      Upload.find_each do |upload|\n        next if upload.local?\n        filename = File.join(@tmp_directory, upload_directory, store.get_path_for_upload(upload))\n\n        begin\n          FileUtils.mkdir_p(File.dirname(filename))\n          store.download_file(upload, filename)\n        rescue StandardError => ex\n          log \"Failed to download file with upload ID #{upload.id} from S3\", ex\n        end\n\n        count += 1\n        log \"#{count} files have already been downloaded. Still downloading...\" if count % 500 == 0\n      end\n\n      log \"Appending uploads to archive...\"\n      Discourse::Utils.execute_command(\n        \"tar\",\n        \"--append\",\n        \"--file\",\n        tar_filename,\n        upload_directory,\n        failure_message: \"Failed to append uploads to archive.\",\n        success_status_codes: [0, 1],\n        chdir: @tmp_directory,\n      )\n\n      log \"No uploads found on S3. Skipping archiving of uploads stored on S3...\" if count == 0\n    end\n\n    def upload_archive\n      return unless store.remote?\n\n      log \"Uploading archive...\"\n      content_type = MiniMime.lookup_by_filename(@backup_filename).content_type\n      archive_path = File.join(@archive_directory, @backup_filename)\n      store.upload_file(@backup_filename, archive_path, content_type)\n    end\n\n    def after_create_hook\n      log \"Executing the after_create_hook for the backup...\"\n      DiscourseEvent.trigger(:backup_created)\n    end\n\n    def delete_old\n      return if Rails.env.development?\n\n      log \"Deleting old backups...\"\n      store.delete_old\n    rescue => ex\n      log \"Something went wrong while deleting old backups.\", ex\n    end\n\n    def notify_user\n      return if success && @user.id == Discourse::SYSTEM_USER_ID\n\n      log \"Notifying '#{@user.username}' of the end of the backup...\"\n      status = success ? :backup_succeeded : :backup_failed\n\n      logs = Discourse::Utils.logs_markdown(@logs, user: @user)\n      post = SystemMessage.create_from_system_user(@user, status, logs: logs)\n\n      post.topic.invite_group(@user, Group[:admins]) if @user.id == Discourse::SYSTEM_USER_ID\n    rescue => ex\n      log \"Something went wrong while notifying user.\", ex\n    end\n\n    def clean_up\n      log \"Cleaning stuff up...\"\n      delete_uploaded_archive\n      remove_tar_leftovers\n      mark_backup_as_not_running\n      refresh_disk_space if success\n    end\n\n    def delete_uploaded_archive\n      return unless store.remote?\n\n      archive_path = File.join(@archive_directory, @backup_filename)\n\n      if File.exist?(archive_path)\n        log \"Removing archive from local storage...\"\n        File.delete(archive_path)\n      end\n    rescue => ex\n      log \"Something went wrong while deleting uploaded archive from local storage.\", ex\n    end\n\n    def refresh_disk_space\n      log \"Refreshing disk stats...\"\n      store.reset_cache\n    rescue => ex\n      log \"Something went wrong while refreshing disk stats.\", ex\n    end\n\n    def remove_tar_leftovers\n      log \"Removing '.tar' leftovers...\"\n      Dir[\"#{@archive_directory}/*.tar\"].each { |filename| File.delete(filename) }\n    rescue => ex\n      log \"Something went wrong while removing '.tar' leftovers.\", ex\n    end\n\n    def remove_tmp_directory\n      log \"Removing tmp '#{@tmp_directory}' directory...\"\n      FileUtils.rm_rf(@tmp_directory) if Dir[@tmp_directory].present?\n    rescue => ex\n      log \"Something went wrong while removing the following tmp directory: #{@tmp_directory}\", ex\n    end\n\n    def mark_backup_as_not_running\n      log \"Marking backup as finished...\"\n      BackupRestore.mark_as_not_running!\n    rescue => ex\n      log \"Something went wrong while marking backup as finished.\", ex\n    end\n\n    def ensure_directory_exists(directory)\n      log \"Making sure '#{directory}' exists...\"\n      FileUtils.mkdir_p(directory)\n    end\n\n    def log(message, ex = nil)\n      timestamp = Time.now.strftime(\"%Y-%m-%d %H:%M:%S\")\n      puts(message) if !Rails.env.test?\n      publish_log(message, timestamp)\n      save_log(message, timestamp)\n      Rails.logger.error(\"#{ex}\\n\" + ex.backtrace.join(\"\\n\")) if ex\n    end\n\n    def publish_log(message, timestamp)\n      return unless @publish_to_message_bus\n      data = { timestamp: timestamp, operation: \"backup\", message: message }\n      MessageBus.publish(\n        BackupRestore::LOGS_CHANNEL,\n        data,\n        user_ids: [@user_id],\n        client_ids: [@client_id],\n      )\n    end\n\n    def save_log(message, timestamp)\n      @logs << \"[#{timestamp}] #{message}\"\n    end\n\n    def publish_completion\n      if success\n        log(\"[SUCCESS]\")\n        DiscourseEvent.trigger(:backup_complete, logs: @logs, ticket: @ticket)\n      else\n        log(\"[FAILED]\")\n        DiscourseEvent.trigger(:backup_failed, logs: @logs, ticket: @ticket)\n      end\n    end\n  end\nend\n", "# frozen_string_literal: true\n\nRSpec.describe BackupRestore::Backuper do\n  describe \"#get_parameterized_title\" do\n    it \"returns a non-empty parameterized title when site title contains unicode\" do\n      SiteSetting.title = \"\u0194\"\n      backuper = BackupRestore::Backuper.new(Discourse.system_user.id)\n\n      expect(backuper.send(:get_parameterized_title)).to eq(\"discourse\")\n    end\n\n    it \"returns a valid parameterized site title\" do\n      SiteSetting.title = \"Coding Horror\"\n      backuper = BackupRestore::Backuper.new(Discourse.system_user.id)\n\n      expect(backuper.send(:get_parameterized_title)).to eq(\"coding-horror\")\n    end\n  end\n\n  describe \"#notify_user\" do\n    before { freeze_time Time.zone.parse(\"2010-01-01 12:00\") }\n\n    it \"includes logs if short\" do\n      SiteSetting.max_export_file_size_kb = 1\n      SiteSetting.export_authorized_extensions = \"tar.gz\"\n\n      silence_stdout do\n        backuper = BackupRestore::Backuper.new(Discourse.system_user.id)\n\n        expect { backuper.send(:notify_user) }.to change { Topic.private_messages.count }.by(\n          1,\n        ).and not_change { Upload.count }\n      end\n\n      expect(Topic.last.first_post.raw).to include(\n        \"```text\\n[2010-01-01 12:00:00] Notifying 'system' of the end of the backup...\\n```\",\n      )\n    end\n\n    it \"include upload if log is long\" do\n      SiteSetting.max_post_length = 250\n\n      silence_stdout do\n        backuper = BackupRestore::Backuper.new(Discourse.system_user.id)\n\n        expect { backuper.send(:notify_user) }.to change { Topic.private_messages.count }.by(\n          1,\n        ).and change { Upload.where(original_filename: \"log.txt.zip\").count }.by(1)\n      end\n\n      expect(Topic.last.first_post.raw).to include(\"[log.txt.zip|attachment]\")\n    end\n\n    it \"includes trimmed logs if log is long and upload cannot be saved\" do\n      SiteSetting.max_post_length = 348\n      SiteSetting.max_export_file_size_kb = 1\n      SiteSetting.export_authorized_extensions = \"tar.gz\"\n\n      silence_stdout do\n        backuper = BackupRestore::Backuper.new(Discourse.system_user.id)\n\n        1.upto(10).each { |i| backuper.send(:log, \"Line #{i}\") }\n\n        expect { backuper.send(:notify_user) }.to change { Topic.private_messages.count }.by(\n          1,\n        ).and not_change { Upload.count }\n      end\n\n      expect(Topic.last.first_post.raw).to include(\n        \"```text\\n...\\n[2010-01-01 12:00:00] Line 10\\n[2010-01-01 12:00:00] Notifying 'system' of the end of the backup...\\n```\",\n      )\n    end\n  end\n\n  describe \"#run\" do\n    subject(:run) { backup.run }\n\n    let(:backup) { described_class.new(user.id) }\n    let(:user) { Discourse.system_user }\n    let(:store) { backup.store }\n\n    before { backup.stubs(:success).returns(success) }\n\n    context \"when the result isn't successful\" do\n      let(:success) { false }\n\n      it \"doesn't refresh disk stats\" do\n        store.expects(:reset_cache).never\n        run\n      end\n    end\n\n    context \"when the result is successful\" do\n      let(:success) { true }\n\n      it \"refreshes disk stats\" do\n        store.expects(:reset_cache)\n        run\n      end\n    end\n  end\nend\n", "# frozen_string_literal: true\n\nRSpec.describe Admin::BackupsController do\n  fab!(:admin) { Fabricate(:admin) }\n  fab!(:moderator) { Fabricate(:moderator) }\n  fab!(:user) { Fabricate(:user) }\n\n  let(:backup_filename) { \"2014-02-10-065935.tar.gz\" }\n  let(:backup_filename2) { \"2014-02-11-065935.tar.gz\" }\n\n  def create_backup_files(*filenames)\n    @paths =\n      filenames.map do |filename|\n        path = backup_path(filename)\n        File.open(path, \"w\") { |f| f.write(\"test backup\") }\n        path\n      end\n  end\n\n  def backup_path(filename)\n    File.join(BackupRestore::LocalBackupStore.base_directory, filename)\n  end\n\n  def map_preloaded\n    controller\n      .instance_variable_get(\"@preloaded\")\n      .map { |key, value| [key, JSON.parse(value)] }\n      .to_h\n  end\n\n  before { SiteSetting.backup_location = BackupLocationSiteSetting::LOCAL }\n\n  after do\n    Discourse.redis.flushdb\n\n    @paths&.each { |path| File.delete(path) if File.exist?(path) }\n    @paths = nil\n  end\n\n  describe \"#index\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"raises an error when backups are disabled\" do\n        SiteSetting.enable_backups = false\n        get \"/admin/backups.json\"\n        expect(response.status).to eq(403)\n      end\n\n      context \"with html format\" do\n        it \"preloads important data\" do\n          get \"/admin/backups.html\"\n          expect(response.status).to eq(200)\n\n          preloaded = map_preloaded\n          expect(preloaded[\"operations_status\"].symbolize_keys).to eq(\n            BackupRestore.operations_status,\n          )\n          expect(preloaded[\"logs\"].size).to eq(BackupRestore.logs.size)\n        end\n      end\n\n      context \"with json format\" do\n        it \"returns a list of all the backups\" do\n          begin\n            create_backup_files(backup_filename, backup_filename2)\n\n            get \"/admin/backups.json\"\n            expect(response.status).to eq(200)\n\n            filenames = response.parsed_body.map { |backup| backup[\"filename\"] }\n            expect(filenames).to include(backup_filename)\n            expect(filenames).to include(backup_filename2)\n          end\n        end\n      end\n    end\n\n    shared_examples \"backups inaccessible\" do\n      it \"denies access with a 404 response\" do\n        get \"/admin/backups.html\"\n\n        expect(response.status).to eq(404)\n\n        get \"/admin/backups.json\"\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backups inaccessible\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backups inaccessible\"\n    end\n  end\n\n  describe \"#status\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"returns the current backups status\" do\n        get \"/admin/backups/status.json\"\n        expect(response.body).to eq(BackupRestore.operations_status.to_json)\n        expect(response.status).to eq(200)\n      end\n    end\n\n    shared_examples \"status inaccessible\" do\n      it \"denies access with a 404 response\" do\n        get \"/admin/backups/status.json\"\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"status inaccessible\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"status inaccessible\"\n    end\n  end\n\n  describe \"#create\" do\n    context \"when logged in as an admin\" do\n      before do\n        sign_in(admin)\n        BackupRestore.stubs(:backup!)\n      end\n\n      it \"starts a backup\" do\n        BackupRestore.expects(:backup!).with(\n          admin.id,\n          { publish_to_message_bus: true, with_uploads: false, client_id: \"foo\" },\n        )\n\n        post \"/admin/backups.json\", params: { with_uploads: false, client_id: \"foo\" }\n\n        expect(response.status).to eq(200)\n      end\n\n      context \"with rate limiting enabled\" do\n        before do\n          RateLimiter.clear_all!\n          RateLimiter.enable\n        end\n\n        after { RateLimiter.disable }\n\n        it \"is rate limited\" do\n          post \"/admin/backups.json\", params: { with_uploads: false, client_id: \"foo\" }\n          post \"/admin/backups.json\", params: { with_uploads: false, client_id: \"foo\" }\n\n          expect(response).to have_http_status :too_many_requests\n        end\n      end\n    end\n\n    shared_examples \"backups creation not allowed\" do\n      it \"prevents backups creation with a 404 response\" do\n        post \"/admin/backups.json\", params: { with_uploads: false, client_id: \"foo\" }\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backups creation not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backups creation not allowed\"\n    end\n  end\n\n  describe \"#show\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"uses send_file to transmit the backup\" do\n        begin\n          token = EmailBackupToken.set(admin.id)\n          create_backup_files(backup_filename)\n\n          expect do\n            get \"/admin/backups/#{backup_filename}.json\", params: { token: token }\n          end.to change {\n            UserHistory.where(action: UserHistory.actions[:backup_download]).count\n          }.by(1)\n\n          expect(response.headers[\"Content-Length\"]).to eq(\"11\")\n          expect(response.headers[\"Content-Disposition\"]).to match(/attachment; filename/)\n        end\n      end\n\n      it \"returns 422 when token is bad\" do\n        begin\n          get \"/admin/backups/#{backup_filename}.json\", params: { token: \"bad_value\" }\n\n          expect(response.status).to eq(422)\n          expect(response.headers[\"Content-Disposition\"]).not_to match(/attachment; filename/)\n          expect(response.body).to include(I18n.t(\"download_backup_mailer.no_token\"))\n        end\n      end\n\n      it \"returns 404 when the backup does not exist\" do\n        token = EmailBackupToken.set(admin.id)\n        get \"/admin/backups/#{backup_filename}.json\", params: { token: token }\n\n        expect(response.status).to eq(404)\n      end\n    end\n\n    shared_examples \"backup inaccessible\" do\n      it \"denies access with a 404 response\" do\n        begin\n          token = EmailBackupToken.set(admin.id)\n          create_backup_files(backup_filename)\n\n          expect do\n            get \"/admin/backups/#{backup_filename}.json\", params: { token: token }\n          end.not_to change {\n            UserHistory.where(action: UserHistory.actions[:backup_download]).count\n          }\n\n          expect(response.status).to eq(404)\n          expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n          expect(response.headers[\"Content-Disposition\"]).not_to match(/attachment; filename/)\n        end\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup inaccessible\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup inaccessible\"\n    end\n  end\n\n  describe \"#destroy\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"removes the backup if found\" do\n        begin\n          path = backup_path(backup_filename)\n          create_backup_files(backup_filename)\n          expect(File.exist?(path)).to eq(true)\n\n          expect do delete \"/admin/backups/#{backup_filename}.json\" end.to change {\n            UserHistory.where(action: UserHistory.actions[:backup_destroy]).count\n          }.by(1)\n\n          expect(response.status).to eq(200)\n          expect(File.exist?(path)).to eq(false)\n        end\n      end\n\n      it \"doesn't remove the backup if not found\" do\n        delete \"/admin/backups/#{backup_filename}.json\"\n        expect(response.status).to eq(404)\n      end\n    end\n\n    shared_examples \"backup deletion not allowed\" do\n      it \"prevents deletion with a 404 response\" do\n        begin\n          path = backup_path(backup_filename)\n          create_backup_files(backup_filename)\n          expect(File.exist?(path)).to eq(true)\n\n          expect do delete \"/admin/backups/#{backup_filename}.json\" end.not_to change {\n            UserHistory.where(action: UserHistory.actions[:backup_destroy]).count\n          }\n\n          expect(response.status).to eq(404)\n          expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n          expect(File.exist?(path)).to eq(true)\n        end\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup deletion not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup deletion not allowed\"\n    end\n  end\n\n  describe \"#logs\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"preloads important data\" do\n        get \"/admin/backups/logs.html\"\n        expect(response.status).to eq(200)\n\n        preloaded = map_preloaded\n\n        expect(preloaded[\"operations_status\"].symbolize_keys).to eq(BackupRestore.operations_status)\n        expect(preloaded[\"logs\"].size).to eq(BackupRestore.logs.size)\n      end\n    end\n\n    shared_examples \"backup logs inaccessible\" do\n      it \"denies access with a 404 response\" do\n        get \"/admin/backups/logs.html\"\n\n        expect(response.status).to eq(404)\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup logs inaccessible\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup logs inaccessible\"\n    end\n  end\n\n  describe \"#restore\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"starts a restore\" do\n        BackupRestore.expects(:restore!).with(\n          admin.id,\n          { filename: backup_filename, publish_to_message_bus: true, client_id: \"foo\" },\n        )\n\n        post \"/admin/backups/#{backup_filename}/restore.json\", params: { client_id: \"foo\" }\n\n        expect(response.status).to eq(200)\n      end\n    end\n\n    shared_examples \"backup restoration not allowed\" do\n      it \"prevents restoration with a 404 response\" do\n        post \"/admin/backups/#{backup_filename}/restore.json\", params: { client_id: \"foo\" }\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup restoration not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup restoration not allowed\"\n    end\n  end\n\n  describe \"#readonly\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"enables readonly mode\" do\n        expect(Discourse.readonly_mode?).to eq(false)\n\n        expect { put \"/admin/backups/readonly.json\", params: { enable: true } }.to change {\n          UserHistory.where(\n            action: UserHistory.actions[:change_readonly_mode],\n            new_value: \"t\",\n          ).count\n        }.by(1)\n\n        expect(Discourse.readonly_mode?).to eq(true)\n        expect(response.status).to eq(200)\n      end\n\n      it \"disables readonly mode\" do\n        Discourse.enable_readonly_mode(Discourse::USER_READONLY_MODE_KEY)\n        expect(Discourse.readonly_mode?).to eq(true)\n\n        expect { put \"/admin/backups/readonly.json\", params: { enable: false } }.to change {\n          UserHistory.where(\n            action: UserHistory.actions[:change_readonly_mode],\n            new_value: \"f\",\n          ).count\n        }.by(1)\n\n        expect(response.status).to eq(200)\n        expect(Discourse.readonly_mode?).to eq(false)\n      end\n    end\n\n    shared_examples \"enabling readonly mode not allowed\" do\n      it \"prevents enabling readonly mode with a 404 response\" do\n        expect(Discourse.readonly_mode?).to eq(false)\n\n        expect do put \"/admin/backups/readonly.json\", params: { enable: true } end.not_to change {\n          UserHistory.where(\n            action: UserHistory.actions[:change_readonly_mode],\n            new_value: \"t\",\n          ).count\n        }\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n        expect(Discourse.readonly_mode?).to eq(false)\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"enabling readonly mode not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"enabling readonly mode not allowed\"\n    end\n  end\n\n  describe \"#upload_backup_chunk\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      describe \"when filename contains invalid characters\" do\n        it \"should raise an error\" do\n          [\"\u7070\u8272.tar.gz\", '; echo \\'haha\\'.tar.gz'].each do |invalid_filename|\n            described_class.any_instance.expects(:has_enough_space_on_disk?).returns(true)\n\n            post \"/admin/backups/upload\",\n                 params: {\n                   resumableFilename: invalid_filename,\n                   resumableTotalSize: 1,\n                   resumableIdentifier: \"test\",\n                 }\n\n            expect(response.status).to eq(415)\n            expect(response.body).to eq(I18n.t(\"backup.invalid_filename\"))\n          end\n        end\n      end\n\n      describe \"when resumableIdentifier is invalid\" do\n        it \"should raise an error\" do\n          filename = \"test_site-0123456789.tar.gz\"\n          @paths = [backup_path(File.join(\"tmp\", \"test\", \"#{filename}.part1\"))]\n\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: 1,\n                 resumableIdentifier: \"../test\",\n                 resumableChunkNumber: \"1\",\n                 resumableChunkSize: \"1\",\n                 resumableCurrentChunkSize: \"1\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n\n          expect(response.status).to eq(400)\n        end\n      end\n\n      describe \"when filename is valid\" do\n        it \"should upload the file successfully\" do\n          freeze_time\n          described_class.any_instance.expects(:has_enough_space_on_disk?).returns(true)\n\n          filename = \"test_Site-0123456789.tar.gz\"\n\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: 1,\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"1\",\n                 resumableChunkSize: \"1\",\n                 resumableCurrentChunkSize: \"1\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          expect_job_enqueued(\n            job: :backup_chunks_merger,\n            args: {\n              filename: filename,\n              identifier: \"test\",\n              chunks: 1,\n            },\n            at: 5.seconds.from_now,\n          )\n\n          expect(response.status).to eq(200)\n          expect(response.body).to eq(\"\")\n        end\n      end\n\n      describe \"completing an upload by enqueuing backup_chunks_merger\" do\n        let(:filename) { \"test_Site-0123456789.tar.gz\" }\n\n        it \"works with a single chunk\" do\n          freeze_time\n          described_class.any_instance.expects(:has_enough_space_on_disk?).returns(true)\n\n          # 2MB file, 2MB chunks = 1x 2MB chunk\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"2097152\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"1\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"2097152\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          expect_job_enqueued(\n            job: :backup_chunks_merger,\n            args: {\n              filename: filename,\n              identifier: \"test\",\n              chunks: 1,\n            },\n            at: 5.seconds.from_now,\n          )\n        end\n\n        it \"works with multiple chunks when the final chunk is chunk_size + remainder\" do\n          freeze_time\n          described_class.any_instance.expects(:has_enough_space_on_disk?).twice.returns(true)\n\n          # 5MB file, 2MB chunks = 1x 2MB chunk + 1x 3MB chunk with resumable.js\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"5242880\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"1\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"2097152\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"5242880\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"2\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"3145728\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          expect_job_enqueued(\n            job: :backup_chunks_merger,\n            args: {\n              filename: filename,\n              identifier: \"test\",\n              chunks: 2,\n            },\n            at: 5.seconds.from_now,\n          )\n        end\n\n        it \"works with multiple chunks when the final chunk is just the remaninder\" do\n          freeze_time\n          described_class.any_instance.expects(:has_enough_space_on_disk?).times(3).returns(true)\n\n          # 5MB file, 2MB chunks = 2x 2MB chunk + 1x 1MB chunk with uppy.js\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"5242880\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"1\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"2097152\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"5242880\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"2\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"2097152\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          post \"/admin/backups/upload.json\",\n               params: {\n                 resumableFilename: filename,\n                 resumableTotalSize: \"5242880\",\n                 resumableIdentifier: \"test\",\n                 resumableChunkNumber: \"3\",\n                 resumableChunkSize: \"2097152\",\n                 resumableCurrentChunkSize: \"1048576\",\n                 file: fixture_file_upload(Tempfile.new),\n               }\n          expect_job_enqueued(\n            job: :backup_chunks_merger,\n            args: {\n              filename: filename,\n              identifier: \"test\",\n              chunks: 3,\n            },\n            at: 5.seconds.from_now,\n          )\n        end\n      end\n    end\n\n    shared_examples \"uploading backup chunk not allowed\" do\n      it \"prevents uploading of backup chunk with a 404 response\" do\n        freeze_time\n        filename = \"test_Site-0123456789.tar.gz\"\n\n        post \"/admin/backups/upload.json\",\n             params: {\n               resumableFilename: filename,\n               resumableTotalSize: 1,\n               resumableIdentifier: \"test\",\n               resumableChunkNumber: \"1\",\n               resumableChunkSize: \"1\",\n               resumableCurrentChunkSize: \"1\",\n               file: fixture_file_upload(Tempfile.new),\n             }\n\n        expect_not_enqueued_with(\n          job: :backup_chunks_merger,\n          args: {\n            filename: filename,\n            identifier: \"test\",\n            chunks: 1,\n          },\n          at: 5.seconds.from_now,\n        )\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"uploading backup chunk not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"uploading backup chunk not allowed\"\n    end\n  end\n\n  describe \"#check_backup_chunk\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      describe \"when resumableIdentifier is invalid\" do\n        it \"should raise an error\" do\n          get \"/admin/backups/upload\",\n              params: {\n                resumableidentifier: \"../some_file\",\n                resumablefilename: \"test_site-0123456789.tar.gz\",\n                resumablechunknumber: \"1\",\n                resumablecurrentchunksize: \"1\",\n              }\n\n          expect(response.status).to eq(400)\n        end\n      end\n    end\n\n    shared_examples \"checking backup chunk not allowed\" do\n      it \"denies access with a 404 response\" do\n        get \"/admin/backups/upload\",\n            params: {\n              resumableidentifier: \"../some_file\",\n              resumablefilename: \"test_site-0123456789.tar.gz\",\n              resumablechunknumber: \"1\",\n              resumablecurrentchunksize: \"1\",\n            }\n\n        expect(response.status).to eq(404)\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"checking backup chunk not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"checking backup chunk not allowed\"\n    end\n  end\n\n  describe \"#rollback\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"should rollback the restore\" do\n        BackupRestore.expects(:rollback!)\n\n        post \"/admin/backups/rollback.json\"\n\n        expect(response.status).to eq(200)\n      end\n\n      it \"should not allow rollback via a GET request\" do\n        get \"/admin/backups/rollback.json\"\n        expect(response.status).to eq(404)\n      end\n    end\n\n    shared_examples \"backup rollback not allowed\" do\n      it \"prevents rollbacks with a 404 response\" do\n        post \"/admin/backups/rollback.json\"\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup rollback not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup rollback not allowed\"\n    end\n  end\n\n  describe \"#cancel\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"should cancel an backup\" do\n        BackupRestore.expects(:cancel!)\n\n        delete \"/admin/backups/cancel.json\"\n\n        expect(response.status).to eq(200)\n      end\n\n      it \"should not allow cancel via a GET request\" do\n        get \"/admin/backups/cancel.json\"\n        expect(response.status).to eq(404)\n      end\n    end\n\n    shared_examples \"backup cancellation not allowed\" do\n      it \"prevents cancellation with a 404 response\" do\n        delete \"/admin/backups/cancel.json\"\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup cancellation not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup cancellation not allowed\"\n    end\n  end\n\n  describe \"#email\" do\n    context \"when logged in as an admin\" do\n      before { sign_in(admin) }\n\n      it \"enqueues email job\" do\n        # might as well test this here if we really want www.example.com\n        SiteSetting.force_hostname = \"www.example.com\"\n\n        create_backup_files(backup_filename)\n\n        expect { put \"/admin/backups/#{backup_filename}.json\" }.to change {\n          Jobs::DownloadBackupEmail.jobs.size\n        }.by(1)\n\n        job_args = Jobs::DownloadBackupEmail.jobs.last[\"args\"].first\n        expect(job_args[\"user_id\"]).to eq(admin.id)\n        expect(job_args[\"backup_file_path\"]).to eq(\n          \"http://www.example.com/admin/backups/#{backup_filename}\",\n        )\n\n        expect(response.status).to eq(200)\n      end\n\n      it \"returns 404 when the backup does not exist\" do\n        put \"/admin/backups/#{backup_filename}.json\"\n\n        expect(response).to be_not_found\n      end\n    end\n\n    shared_examples \"backup emails not allowed\" do\n      it \"prevents sending backup emails with a 404 response\" do\n        SiteSetting.force_hostname = \"www.example.com\"\n        create_backup_files(backup_filename)\n\n        expect do put \"/admin/backups/#{backup_filename}.json\" end.not_to change {\n          Jobs::DownloadBackupEmail.jobs.size\n        }\n\n        expect(response.status).to eq(404)\n        expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"not_found\"))\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"backup emails not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"backup emails not allowed\"\n    end\n  end\n\n  describe \"S3 multipart uploads\" do\n    let(:upload_type) { \"backup\" }\n    let(:test_bucket_prefix) { \"test_#{ENV[\"TEST_ENV_NUMBER\"].presence || \"0\"}\" }\n    let(:backup_file_exists_response) { { status: 404 } }\n    let(:mock_multipart_upload_id) do\n      \"ibZBv_75gd9r8lH_gqXatLdxMVpAlj6CFTR.OwyF3953YdwbcQnMA2BLGn8Lx12fQNICtMw5KyteFeHw.Sjng--\"\n    end\n\n    before do\n      setup_s3\n      SiteSetting.enable_direct_s3_uploads = true\n      SiteSetting.s3_backup_bucket = \"s3-backup-bucket\"\n      SiteSetting.backup_location = BackupLocationSiteSetting::S3\n      stub_request(:head, \"https://s3-backup-bucket.s3.us-west-1.amazonaws.com/\").to_return(\n        status: 200,\n        body: \"\",\n        headers: {\n        },\n      )\n      stub_request(\n        :head,\n        \"https://s3-backup-bucket.s3.us-west-1.amazonaws.com/default/test.tar.gz\",\n      ).to_return(backup_file_exists_response)\n    end\n\n    shared_examples \"multipart uploads not allowed\" do\n      it \"prevents multipart uploads with a 404 response\" do\n        post \"/admin/backups/create-multipart.json\",\n             params: {\n               file_name: \"test.tar.gz\",\n               upload_type: upload_type,\n               file_size: 4098,\n             }\n        expect(response.status).to eq(404)\n      end\n    end\n\n    context \"when logged in as a moderator\" do\n      before { sign_in(moderator) }\n\n      include_examples \"multipart uploads not allowed\"\n    end\n\n    context \"when logged in as a non-staff user\" do\n      before { sign_in(user) }\n\n      include_examples \"multipart uploads not allowed\"\n    end\n\n    context \"when the user is admin\" do\n      before { sign_in(admin) }\n\n      def stub_create_multipart_backup_request\n        BackupRestore::S3BackupStore\n          .any_instance\n          .stubs(:temporary_upload_path)\n          .returns(\n            \"temp/default/#{test_bucket_prefix}/28fccf8259bbe75b873a2bd2564b778c/2u98j832nx93272x947823.gz\",\n          )\n        create_multipart_result = <<~XML\n        <?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\n        <InitiateMultipartUploadResult>\n           <Bucket>s3-backup-bucket</Bucket>\n           <Key>temp/default/#{test_bucket_prefix}/28fccf8259bbe75b873a2bd2564b778c/2u98j832nx93272x947823.gz</Key>\n           <UploadId>#{mock_multipart_upload_id}</UploadId>\n        </InitiateMultipartUploadResult>\n        XML\n        stub_request(\n          :post,\n          \"https://s3-backup-bucket.s3.us-west-1.amazonaws.com/temp/default/#{test_bucket_prefix}/28fccf8259bbe75b873a2bd2564b778c/2u98j832nx93272x947823.gz?uploads\",\n        ).to_return(status: 200, body: create_multipart_result)\n      end\n\n      it \"creates the multipart upload\" do\n        stub_create_multipart_backup_request\n        post \"/admin/backups/create-multipart.json\",\n             params: {\n               file_name: \"test.tar.gz\",\n               upload_type: upload_type,\n               file_size: 4098,\n             }\n        expect(response.status).to eq(200)\n        result = response.parsed_body\n\n        external_upload_stub =\n          ExternalUploadStub.where(\n            unique_identifier: result[\"unique_identifier\"],\n            original_filename: \"test.tar.gz\",\n            created_by: admin,\n            upload_type: upload_type,\n            key: result[\"key\"],\n            multipart: true,\n          )\n        expect(external_upload_stub.exists?).to eq(true)\n      end\n\n      context \"when backup of same filename already exists\" do\n        let(:backup_file_exists_response) { { status: 200, body: \"\" } }\n\n        it \"throws an error\" do\n          post \"/admin/backups/create-multipart.json\",\n               params: {\n                 file_name: \"test.tar.gz\",\n                 upload_type: upload_type,\n                 file_size: 4098,\n               }\n          expect(response.status).to eq(422)\n          expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"backup.file_exists\"))\n        end\n      end\n\n      context \"when filename is invalid\" do\n        it \"throws an error\" do\n          post \"/admin/backups/create-multipart.json\",\n               params: {\n                 file_name: \"blah $$##.tar.gz\",\n                 upload_type: upload_type,\n                 file_size: 4098,\n               }\n          expect(response.status).to eq(422)\n          expect(response.parsed_body[\"errors\"]).to include(I18n.t(\"backup.invalid_filename\"))\n        end\n      end\n\n      context \"when extension is invalid\" do\n        it \"throws an error\" do\n          post \"/admin/backups/create-multipart.json\",\n               params: {\n                 file_name: \"test.png\",\n                 upload_type: upload_type,\n                 file_size: 4098,\n               }\n          expect(response.status).to eq(422)\n          expect(response.parsed_body[\"errors\"]).to include(\n            I18n.t(\"backup.backup_file_should_be_tar_gz\"),\n          )\n        end\n      end\n    end\n  end\nend\n"], "filenames": ["app/controllers/admin/backups_controller.rb", "lib/backup_restore/backuper.rb", "spec/lib/backup_restore/backuper_spec.rb", "spec/requests/admin/backups_controller_spec.rb"], "buggy_code_start_loc": [36, 8, 4, 140], "buggy_code_end_loc": [36, 454, 71, 150], "fixing_code_start_loc": [37, 8, 4, 140], "fixing_code_end_loc": [45, 453, 102, 170], "type": "CWE-770", "message": "Discourse is an open-source discussion platform. Prior to version 3.0.2 of the `stable` branch and version 3.1.0.beta3 of the `beta` and `tests-passed` branches, a user logged as an administrator can request backups multiple times, which will eat up all the connections to the DB. If this is done on a site using multisite, then it can affect the whole cluster. The vulnerability is patched in version 3.0.2 of the `stable` branch and version 3.1.0.beta3 of the `beta` and `tests-passed` branches. There are no known workarounds.", "other": {"cve": {"id": "CVE-2023-28107", "sourceIdentifier": "security-advisories@github.com", "published": "2023-03-17T17:15:11.517", "lastModified": "2023-03-23T20:53:57.360", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Discourse is an open-source discussion platform. Prior to version 3.0.2 of the `stable` branch and version 3.1.0.beta3 of the `beta` and `tests-passed` branches, a user logged as an administrator can request backups multiple times, which will eat up all the connections to the DB. If this is done on a site using multisite, then it can affect the whole cluster. The vulnerability is patched in version 3.0.2 of the `stable` branch and version 3.1.0.beta3 of the `beta` and `tests-passed` branches. There are no known workarounds."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.2, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:H/UI:R/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 0.9, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-770"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:discourse:discourse:*:*:*:*:stable:*:*:*", "versionEndIncluding": "3.0.1", "matchCriteriaId": "618BD7ED-B602-46C3-AFDA-55544B4E6264"}, {"vulnerable": true, "criteria": "cpe:2.3:a:discourse:discourse:*:*:*:*:beta:*:*:*", "versionEndExcluding": "3.1.0", "matchCriteriaId": "D3C08972-822D-4657-9B6F-02BC692B7C6E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:discourse:discourse:3.1.0:beta1:*:*:beta:*:*:*", "matchCriteriaId": "B9BBED17-A6BA-4F17-8814-8D8521F28375"}, {"vulnerable": true, "criteria": "cpe:2.3:a:discourse:discourse:3.1.0:beta2:*:*:beta:*:*:*", "matchCriteriaId": "888B8ECF-EBE0-4821-82F6-B0026E95E407"}]}]}], "references": [{"url": "https://github.com/discourse/discourse/commit/0bd64788d2b4680c04fbef76314a24884d65fed9", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/discourse/discourse/commit/78a3efa7104eed6dd3ed7a06a71e2705337d9e61", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/discourse/discourse/pull/20700", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/discourse/discourse/pull/20701", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/discourse/discourse/security/advisories/GHSA-cp7c-fm4c-6xxx", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/discourse/discourse/commit/0bd64788d2b4680c04fbef76314a24884d65fed9"}}