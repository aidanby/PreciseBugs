{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * derived from drivers/kvm/kvm_main.c\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright (C) 2008 Qumranet, Inc.\n * Copyright IBM Corporation, 2008\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Avi Kivity   <avi@qumranet.com>\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *   Amit Shah    <amit.shah@qumranet.com>\n *   Ben-Ami Yassour <benami@il.ibm.com>\n */\n\n#include <linux/kvm_host.h>\n#include \"irq.h\"\n#include \"ioapic.h\"\n#include \"mmu.h\"\n#include \"i8254.h\"\n#include \"tss.h\"\n#include \"kvm_cache_regs.h\"\n#include \"kvm_emulate.h\"\n#include \"x86.h\"\n#include \"cpuid.h\"\n#include \"pmu.h\"\n#include \"hyperv.h\"\n#include \"lapic.h\"\n#include \"xen.h\"\n#include \"smm.h\"\n\n#include <linux/clocksource.h>\n#include <linux/interrupt.h>\n#include <linux/kvm.h>\n#include <linux/fs.h>\n#include <linux/vmalloc.h>\n#include <linux/export.h>\n#include <linux/moduleparam.h>\n#include <linux/mman.h>\n#include <linux/highmem.h>\n#include <linux/iommu.h>\n#include <linux/cpufreq.h>\n#include <linux/user-return-notifier.h>\n#include <linux/srcu.h>\n#include <linux/slab.h>\n#include <linux/perf_event.h>\n#include <linux/uaccess.h>\n#include <linux/hash.h>\n#include <linux/pci.h>\n#include <linux/timekeeper_internal.h>\n#include <linux/pvclock_gtod.h>\n#include <linux/kvm_irqfd.h>\n#include <linux/irqbypass.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/isolation.h>\n#include <linux/mem_encrypt.h>\n#include <linux/entry-kvm.h>\n#include <linux/suspend.h>\n\n#include <trace/events/kvm.h>\n\n#include <asm/debugreg.h>\n#include <asm/msr.h>\n#include <asm/desc.h>\n#include <asm/mce.h>\n#include <asm/pkru.h>\n#include <linux/kernel_stat.h>\n#include <asm/fpu/api.h>\n#include <asm/fpu/xcr.h>\n#include <asm/fpu/xstate.h>\n#include <asm/pvclock.h>\n#include <asm/div64.h>\n#include <asm/irq_remapping.h>\n#include <asm/mshyperv.h>\n#include <asm/hypervisor.h>\n#include <asm/tlbflush.h>\n#include <asm/intel_pt.h>\n#include <asm/emulate_prefix.h>\n#include <asm/sgx.h>\n#include <clocksource/hyperv_timer.h>\n\n#define CREATE_TRACE_POINTS\n#include \"trace.h\"\n\n#define MAX_IO_MSRS 256\n#define KVM_MAX_MCE_BANKS 32\n\nstruct kvm_caps kvm_caps __read_mostly = {\n\t.supported_mce_cap = MCG_CTL_P | MCG_SER_P,\n};\nEXPORT_SYMBOL_GPL(kvm_caps);\n\n#define  ERR_PTR_USR(e)  ((void __user *)ERR_PTR(e))\n\n#define emul_to_vcpu(ctxt) \\\n\t((struct kvm_vcpu *)(ctxt)->vcpu)\n\n/* EFER defaults:\n * - enable syscall per default because its emulated by KVM\n * - enable LME and LMA per default on 64 bit KVM\n */\n#ifdef CONFIG_X86_64\nstatic\nu64 __read_mostly efer_reserved_bits = ~((u64)(EFER_SCE | EFER_LME | EFER_LMA));\n#else\nstatic u64 __read_mostly efer_reserved_bits = ~((u64)EFER_SCE);\n#endif\n\nstatic u64 __read_mostly cr4_reserved_bits = CR4_RESERVED_BITS;\n\n#define KVM_EXIT_HYPERCALL_VALID_MASK (1 << KVM_HC_MAP_GPA_RANGE)\n\n#define KVM_CAP_PMU_VALID_MASK KVM_PMU_CAP_DISABLE\n\n#define KVM_X2APIC_API_VALID_FLAGS (KVM_X2APIC_API_USE_32BIT_IDS | \\\n                                    KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)\n\nstatic void update_cr8_intercept(struct kvm_vcpu *vcpu);\nstatic void process_nmi(struct kvm_vcpu *vcpu);\nstatic void __kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);\nstatic void store_regs(struct kvm_vcpu *vcpu);\nstatic int sync_regs(struct kvm_vcpu *vcpu);\nstatic int kvm_vcpu_do_singlestep(struct kvm_vcpu *vcpu);\n\nstatic int __set_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2);\nstatic void __get_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2);\n\nstruct kvm_x86_ops kvm_x86_ops __read_mostly;\n\n#define KVM_X86_OP(func)\t\t\t\t\t     \\\n\tDEFINE_STATIC_CALL_NULL(kvm_x86_##func,\t\t\t     \\\n\t\t\t\t*(((struct kvm_x86_ops *)0)->func));\n#define KVM_X86_OP_OPTIONAL KVM_X86_OP\n#define KVM_X86_OP_OPTIONAL_RET0 KVM_X86_OP\n#include <asm/kvm-x86-ops.h>\nEXPORT_STATIC_CALL_GPL(kvm_x86_get_cs_db_l_bits);\nEXPORT_STATIC_CALL_GPL(kvm_x86_cache_reg);\n\nstatic bool __read_mostly ignore_msrs = 0;\nmodule_param(ignore_msrs, bool, S_IRUGO | S_IWUSR);\n\nbool __read_mostly report_ignored_msrs = true;\nmodule_param(report_ignored_msrs, bool, S_IRUGO | S_IWUSR);\nEXPORT_SYMBOL_GPL(report_ignored_msrs);\n\nunsigned int min_timer_period_us = 200;\nmodule_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);\n\nstatic bool __read_mostly kvmclock_periodic_sync = true;\nmodule_param(kvmclock_periodic_sync, bool, S_IRUGO);\n\n/* tsc tolerance in parts per million - default to 1/2 of the NTP threshold */\nstatic u32 __read_mostly tsc_tolerance_ppm = 250;\nmodule_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);\n\n/*\n * lapic timer advance (tscdeadline mode only) in nanoseconds.  '-1' enables\n * adaptive tuning starting from default advancement of 1000ns.  '0' disables\n * advancement entirely.  Any other value is used as-is and disables adaptive\n * tuning, i.e. allows privileged userspace to set an exact advancement time.\n */\nstatic int __read_mostly lapic_timer_advance_ns = -1;\nmodule_param(lapic_timer_advance_ns, int, S_IRUGO | S_IWUSR);\n\nstatic bool __read_mostly vector_hashing = true;\nmodule_param(vector_hashing, bool, S_IRUGO);\n\nbool __read_mostly enable_vmware_backdoor = false;\nmodule_param(enable_vmware_backdoor, bool, S_IRUGO);\nEXPORT_SYMBOL_GPL(enable_vmware_backdoor);\n\n/*\n * Flags to manipulate forced emulation behavior (any non-zero value will\n * enable forced emulation).\n */\n#define KVM_FEP_CLEAR_RFLAGS_RF\tBIT(1)\nstatic int __read_mostly force_emulation_prefix;\nmodule_param(force_emulation_prefix, int, 0644);\n\nint __read_mostly pi_inject_timer = -1;\nmodule_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);\n\n/* Enable/disable PMU virtualization */\nbool __read_mostly enable_pmu = true;\nEXPORT_SYMBOL_GPL(enable_pmu);\nmodule_param(enable_pmu, bool, 0444);\n\nbool __read_mostly eager_page_split = true;\nmodule_param(eager_page_split, bool, 0644);\n\n/* Enable/disable SMT_RSB bug mitigation */\nbool __read_mostly mitigate_smt_rsb;\nmodule_param(mitigate_smt_rsb, bool, 0444);\n\n/*\n * Restoring the host value for MSRs that are only consumed when running in\n * usermode, e.g. SYSCALL MSRs and TSC_AUX, can be deferred until the CPU\n * returns to userspace, i.e. the kernel can run with the guest's value.\n */\n#define KVM_MAX_NR_USER_RETURN_MSRS 16\n\nstruct kvm_user_return_msrs {\n\tstruct user_return_notifier urn;\n\tbool registered;\n\tstruct kvm_user_return_msr_values {\n\t\tu64 host;\n\t\tu64 curr;\n\t} values[KVM_MAX_NR_USER_RETURN_MSRS];\n};\n\nu32 __read_mostly kvm_nr_uret_msrs;\nEXPORT_SYMBOL_GPL(kvm_nr_uret_msrs);\nstatic u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];\nstatic struct kvm_user_return_msrs __percpu *user_return_msrs;\n\n#define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \\\n\t\t\t\t| XFEATURE_MASK_YMM | XFEATURE_MASK_BNDREGS \\\n\t\t\t\t| XFEATURE_MASK_BNDCSR | XFEATURE_MASK_AVX512 \\\n\t\t\t\t| XFEATURE_MASK_PKRU | XFEATURE_MASK_XTILE)\n\nu64 __read_mostly host_efer;\nEXPORT_SYMBOL_GPL(host_efer);\n\nbool __read_mostly allow_smaller_maxphyaddr = 0;\nEXPORT_SYMBOL_GPL(allow_smaller_maxphyaddr);\n\nbool __read_mostly enable_apicv = true;\nEXPORT_SYMBOL_GPL(enable_apicv);\n\nu64 __read_mostly host_xss;\nEXPORT_SYMBOL_GPL(host_xss);\n\nconst struct _kvm_stats_desc kvm_vm_stats_desc[] = {\n\tKVM_GENERIC_VM_STATS(),\n\tSTATS_DESC_COUNTER(VM, mmu_shadow_zapped),\n\tSTATS_DESC_COUNTER(VM, mmu_pte_write),\n\tSTATS_DESC_COUNTER(VM, mmu_pde_zapped),\n\tSTATS_DESC_COUNTER(VM, mmu_flooded),\n\tSTATS_DESC_COUNTER(VM, mmu_recycled),\n\tSTATS_DESC_COUNTER(VM, mmu_cache_miss),\n\tSTATS_DESC_ICOUNTER(VM, mmu_unsync),\n\tSTATS_DESC_ICOUNTER(VM, pages_4k),\n\tSTATS_DESC_ICOUNTER(VM, pages_2m),\n\tSTATS_DESC_ICOUNTER(VM, pages_1g),\n\tSTATS_DESC_ICOUNTER(VM, nx_lpage_splits),\n\tSTATS_DESC_PCOUNTER(VM, max_mmu_rmap_size),\n\tSTATS_DESC_PCOUNTER(VM, max_mmu_page_hash_collisions)\n};\n\nconst struct kvm_stats_header kvm_vm_stats_header = {\n\t.name_size = KVM_STATS_NAME_SIZE,\n\t.num_desc = ARRAY_SIZE(kvm_vm_stats_desc),\n\t.id_offset = sizeof(struct kvm_stats_header),\n\t.desc_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE,\n\t.data_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE +\n\t\t       sizeof(kvm_vm_stats_desc),\n};\n\nconst struct _kvm_stats_desc kvm_vcpu_stats_desc[] = {\n\tKVM_GENERIC_VCPU_STATS(),\n\tSTATS_DESC_COUNTER(VCPU, pf_taken),\n\tSTATS_DESC_COUNTER(VCPU, pf_fixed),\n\tSTATS_DESC_COUNTER(VCPU, pf_emulate),\n\tSTATS_DESC_COUNTER(VCPU, pf_spurious),\n\tSTATS_DESC_COUNTER(VCPU, pf_fast),\n\tSTATS_DESC_COUNTER(VCPU, pf_mmio_spte_created),\n\tSTATS_DESC_COUNTER(VCPU, pf_guest),\n\tSTATS_DESC_COUNTER(VCPU, tlb_flush),\n\tSTATS_DESC_COUNTER(VCPU, invlpg),\n\tSTATS_DESC_COUNTER(VCPU, exits),\n\tSTATS_DESC_COUNTER(VCPU, io_exits),\n\tSTATS_DESC_COUNTER(VCPU, mmio_exits),\n\tSTATS_DESC_COUNTER(VCPU, signal_exits),\n\tSTATS_DESC_COUNTER(VCPU, irq_window_exits),\n\tSTATS_DESC_COUNTER(VCPU, nmi_window_exits),\n\tSTATS_DESC_COUNTER(VCPU, l1d_flush),\n\tSTATS_DESC_COUNTER(VCPU, halt_exits),\n\tSTATS_DESC_COUNTER(VCPU, request_irq_exits),\n\tSTATS_DESC_COUNTER(VCPU, irq_exits),\n\tSTATS_DESC_COUNTER(VCPU, host_state_reload),\n\tSTATS_DESC_COUNTER(VCPU, fpu_reload),\n\tSTATS_DESC_COUNTER(VCPU, insn_emulation),\n\tSTATS_DESC_COUNTER(VCPU, insn_emulation_fail),\n\tSTATS_DESC_COUNTER(VCPU, hypercalls),\n\tSTATS_DESC_COUNTER(VCPU, irq_injections),\n\tSTATS_DESC_COUNTER(VCPU, nmi_injections),\n\tSTATS_DESC_COUNTER(VCPU, req_event),\n\tSTATS_DESC_COUNTER(VCPU, nested_run),\n\tSTATS_DESC_COUNTER(VCPU, directed_yield_attempted),\n\tSTATS_DESC_COUNTER(VCPU, directed_yield_successful),\n\tSTATS_DESC_COUNTER(VCPU, preemption_reported),\n\tSTATS_DESC_COUNTER(VCPU, preemption_other),\n\tSTATS_DESC_IBOOLEAN(VCPU, guest_mode),\n\tSTATS_DESC_COUNTER(VCPU, notify_window_exits),\n};\n\nconst struct kvm_stats_header kvm_vcpu_stats_header = {\n\t.name_size = KVM_STATS_NAME_SIZE,\n\t.num_desc = ARRAY_SIZE(kvm_vcpu_stats_desc),\n\t.id_offset = sizeof(struct kvm_stats_header),\n\t.desc_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE,\n\t.data_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE +\n\t\t       sizeof(kvm_vcpu_stats_desc),\n};\n\nu64 __read_mostly host_xcr0;\n\nstatic struct kmem_cache *x86_emulator_cache;\n\n/*\n * When called, it means the previous get/set msr reached an invalid msr.\n * Return true if we want to ignore/silent this failed msr access.\n */\nstatic bool kvm_msr_ignored_check(u32 msr, u64 data, bool write)\n{\n\tconst char *op = write ? \"wrmsr\" : \"rdmsr\";\n\n\tif (ignore_msrs) {\n\t\tif (report_ignored_msrs)\n\t\t\tkvm_pr_unimpl(\"ignored %s: 0x%x data 0x%llx\\n\",\n\t\t\t\t      op, msr, data);\n\t\t/* Mask the error */\n\t\treturn true;\n\t} else {\n\t\tkvm_debug_ratelimited(\"unhandled %s: 0x%x data 0x%llx\\n\",\n\t\t\t\t      op, msr, data);\n\t\treturn false;\n\t}\n}\n\nstatic struct kmem_cache *kvm_alloc_emulator_cache(void)\n{\n\tunsigned int useroffset = offsetof(struct x86_emulate_ctxt, src);\n\tunsigned int size = sizeof(struct x86_emulate_ctxt);\n\n\treturn kmem_cache_create_usercopy(\"x86_emulator\", size,\n\t\t\t\t\t  __alignof__(struct x86_emulate_ctxt),\n\t\t\t\t\t  SLAB_ACCOUNT, useroffset,\n\t\t\t\t\t  size - useroffset, NULL);\n}\n\nstatic int emulator_fix_hypercall(struct x86_emulate_ctxt *ctxt);\n\nstatic inline void kvm_async_pf_hash_reset(struct kvm_vcpu *vcpu)\n{\n\tint i;\n\tfor (i = 0; i < ASYNC_PF_PER_VCPU; i++)\n\t\tvcpu->arch.apf.gfns[i] = ~0;\n}\n\nstatic void kvm_on_user_return(struct user_return_notifier *urn)\n{\n\tunsigned slot;\n\tstruct kvm_user_return_msrs *msrs\n\t\t= container_of(urn, struct kvm_user_return_msrs, urn);\n\tstruct kvm_user_return_msr_values *values;\n\tunsigned long flags;\n\n\t/*\n\t * Disabling irqs at this point since the following code could be\n\t * interrupted and executed through kvm_arch_hardware_disable()\n\t */\n\tlocal_irq_save(flags);\n\tif (msrs->registered) {\n\t\tmsrs->registered = false;\n\t\tuser_return_notifier_unregister(urn);\n\t}\n\tlocal_irq_restore(flags);\n\tfor (slot = 0; slot < kvm_nr_uret_msrs; ++slot) {\n\t\tvalues = &msrs->values[slot];\n\t\tif (values->host != values->curr) {\n\t\t\twrmsrl(kvm_uret_msrs_list[slot], values->host);\n\t\t\tvalues->curr = values->host;\n\t\t}\n\t}\n}\n\nstatic int kvm_probe_user_return_msr(u32 msr)\n{\n\tu64 val;\n\tint ret;\n\n\tpreempt_disable();\n\tret = rdmsrl_safe(msr, &val);\n\tif (ret)\n\t\tgoto out;\n\tret = wrmsrl_safe(msr, val);\nout:\n\tpreempt_enable();\n\treturn ret;\n}\n\nint kvm_add_user_return_msr(u32 msr)\n{\n\tBUG_ON(kvm_nr_uret_msrs >= KVM_MAX_NR_USER_RETURN_MSRS);\n\n\tif (kvm_probe_user_return_msr(msr))\n\t\treturn -1;\n\n\tkvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;\n\treturn kvm_nr_uret_msrs++;\n}\nEXPORT_SYMBOL_GPL(kvm_add_user_return_msr);\n\nint kvm_find_user_return_msr(u32 msr)\n{\n\tint i;\n\n\tfor (i = 0; i < kvm_nr_uret_msrs; ++i) {\n\t\tif (kvm_uret_msrs_list[i] == msr)\n\t\t\treturn i;\n\t}\n\treturn -1;\n}\nEXPORT_SYMBOL_GPL(kvm_find_user_return_msr);\n\nstatic void kvm_user_return_msr_cpu_online(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);\n\tu64 value;\n\tint i;\n\n\tfor (i = 0; i < kvm_nr_uret_msrs; ++i) {\n\t\trdmsrl_safe(kvm_uret_msrs_list[i], &value);\n\t\tmsrs->values[i].host = value;\n\t\tmsrs->values[i].curr = value;\n\t}\n}\n\nint kvm_set_user_return_msr(unsigned slot, u64 value, u64 mask)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);\n\tint err;\n\n\tvalue = (value & mask) | (msrs->values[slot].host & ~mask);\n\tif (value == msrs->values[slot].curr)\n\t\treturn 0;\n\terr = wrmsrl_safe(kvm_uret_msrs_list[slot], value);\n\tif (err)\n\t\treturn 1;\n\n\tmsrs->values[slot].curr = value;\n\tif (!msrs->registered) {\n\t\tmsrs->urn.on_user_return = kvm_on_user_return;\n\t\tuser_return_notifier_register(&msrs->urn);\n\t\tmsrs->registered = true;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_user_return_msr);\n\nstatic void drop_user_return_notifiers(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);\n\n\tif (msrs->registered)\n\t\tkvm_on_user_return(&msrs->urn);\n}\n\nu64 kvm_get_apic_base(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.apic_base;\n}\n\nenum lapic_mode kvm_get_apic_mode(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_apic_mode(kvm_get_apic_base(vcpu));\n}\nEXPORT_SYMBOL_GPL(kvm_get_apic_mode);\n\nint kvm_set_apic_base(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tenum lapic_mode old_mode = kvm_get_apic_mode(vcpu);\n\tenum lapic_mode new_mode = kvm_apic_mode(msr_info->data);\n\tu64 reserved_bits = kvm_vcpu_reserved_gpa_bits_raw(vcpu) | 0x2ff |\n\t\t(guest_cpuid_has(vcpu, X86_FEATURE_X2APIC) ? 0 : X2APIC_ENABLE);\n\n\tif ((msr_info->data & reserved_bits) != 0 || new_mode == LAPIC_MODE_INVALID)\n\t\treturn 1;\n\tif (!msr_info->host_initiated) {\n\t\tif (old_mode == LAPIC_MODE_X2APIC && new_mode == LAPIC_MODE_XAPIC)\n\t\t\treturn 1;\n\t\tif (old_mode == LAPIC_MODE_DISABLED && new_mode == LAPIC_MODE_X2APIC)\n\t\t\treturn 1;\n\t}\n\n\tkvm_lapic_set_base(vcpu, msr_info->data);\n\tkvm_recalculate_apic_map(vcpu->kvm);\n\treturn 0;\n}\n\n/*\n * Handle a fault on a hardware virtualization (VMX or SVM) instruction.\n *\n * Hardware virtualization extension instructions may fault if a reboot turns\n * off virtualization while processes are running.  Usually after catching the\n * fault we just panic; during reboot instead the instruction is ignored.\n */\nnoinstr void kvm_spurious_fault(void)\n{\n\t/* Fault while not rebooting.  We want the trace. */\n\tBUG_ON(!kvm_rebooting);\n}\nEXPORT_SYMBOL_GPL(kvm_spurious_fault);\n\n#define EXCPT_BENIGN\t\t0\n#define EXCPT_CONTRIBUTORY\t1\n#define EXCPT_PF\t\t2\n\nstatic int exception_class(int vector)\n{\n\tswitch (vector) {\n\tcase PF_VECTOR:\n\t\treturn EXCPT_PF;\n\tcase DE_VECTOR:\n\tcase TS_VECTOR:\n\tcase NP_VECTOR:\n\tcase SS_VECTOR:\n\tcase GP_VECTOR:\n\t\treturn EXCPT_CONTRIBUTORY;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn EXCPT_BENIGN;\n}\n\n#define EXCPT_FAULT\t\t0\n#define EXCPT_TRAP\t\t1\n#define EXCPT_ABORT\t\t2\n#define EXCPT_INTERRUPT\t\t3\n#define EXCPT_DB\t\t4\n\nstatic int exception_type(int vector)\n{\n\tunsigned int mask;\n\n\tif (WARN_ON(vector > 31 || vector == NMI_VECTOR))\n\t\treturn EXCPT_INTERRUPT;\n\n\tmask = 1 << vector;\n\n\t/*\n\t * #DBs can be trap-like or fault-like, the caller must check other CPU\n\t * state, e.g. DR6, to determine whether a #DB is a trap or fault.\n\t */\n\tif (mask & (1 << DB_VECTOR))\n\t\treturn EXCPT_DB;\n\n\tif (mask & ((1 << BP_VECTOR) | (1 << OF_VECTOR)))\n\t\treturn EXCPT_TRAP;\n\n\tif (mask & ((1 << DF_VECTOR) | (1 << MC_VECTOR)))\n\t\treturn EXCPT_ABORT;\n\n\t/* Reserved exceptions will result in fault */\n\treturn EXCPT_FAULT;\n}\n\nvoid kvm_deliver_exception_payload(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct kvm_queued_exception *ex)\n{\n\tif (!ex->has_payload)\n\t\treturn;\n\n\tswitch (ex->vector) {\n\tcase DB_VECTOR:\n\t\t/*\n\t\t * \"Certain debug exceptions may clear bit 0-3.  The\n\t\t * remaining contents of the DR6 register are never\n\t\t * cleared by the processor\".\n\t\t */\n\t\tvcpu->arch.dr6 &= ~DR_TRAP_BITS;\n\t\t/*\n\t\t * In order to reflect the #DB exception payload in guest\n\t\t * dr6, three components need to be considered: active low\n\t\t * bit, FIXED_1 bits and active high bits (e.g. DR6_BD,\n\t\t * DR6_BS and DR6_BT)\n\t\t * DR6_ACTIVE_LOW contains the FIXED_1 and active low bits.\n\t\t * In the target guest dr6:\n\t\t * FIXED_1 bits should always be set.\n\t\t * Active low bits should be cleared if 1-setting in payload.\n\t\t * Active high bits should be set if 1-setting in payload.\n\t\t *\n\t\t * Note, the payload is compatible with the pending debug\n\t\t * exceptions/exit qualification under VMX, that active_low bits\n\t\t * are active high in payload.\n\t\t * So they need to be flipped for DR6.\n\t\t */\n\t\tvcpu->arch.dr6 |= DR6_ACTIVE_LOW;\n\t\tvcpu->arch.dr6 |= ex->payload;\n\t\tvcpu->arch.dr6 ^= ex->payload & DR6_ACTIVE_LOW;\n\n\t\t/*\n\t\t * The #DB payload is defined as compatible with the 'pending\n\t\t * debug exceptions' field under VMX, not DR6. While bit 12 is\n\t\t * defined in the 'pending debug exceptions' field (enabled\n\t\t * breakpoint), it is reserved and must be zero in DR6.\n\t\t */\n\t\tvcpu->arch.dr6 &= ~BIT(12);\n\t\tbreak;\n\tcase PF_VECTOR:\n\t\tvcpu->arch.cr2 = ex->payload;\n\t\tbreak;\n\t}\n\n\tex->has_payload = false;\n\tex->payload = 0;\n}\nEXPORT_SYMBOL_GPL(kvm_deliver_exception_payload);\n\nstatic void kvm_queue_exception_vmexit(struct kvm_vcpu *vcpu, unsigned int vector,\n\t\t\t\t       bool has_error_code, u32 error_code,\n\t\t\t\t       bool has_payload, unsigned long payload)\n{\n\tstruct kvm_queued_exception *ex = &vcpu->arch.exception_vmexit;\n\n\tex->vector = vector;\n\tex->injected = false;\n\tex->pending = true;\n\tex->has_error_code = has_error_code;\n\tex->error_code = error_code;\n\tex->has_payload = has_payload;\n\tex->payload = payload;\n}\n\n/* Forcibly leave the nested mode in cases like a vCPU reset */\nstatic void kvm_leave_nested(struct kvm_vcpu *vcpu)\n{\n\tkvm_x86_ops.nested_ops->leave_nested(vcpu);\n}\n\nstatic void kvm_multiple_exception(struct kvm_vcpu *vcpu,\n\t\tunsigned nr, bool has_error, u32 error_code,\n\t        bool has_payload, unsigned long payload, bool reinject)\n{\n\tu32 prev_nr;\n\tint class1, class2;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\t/*\n\t * If the exception is destined for L2 and isn't being reinjected,\n\t * morph it to a VM-Exit if L1 wants to intercept the exception.  A\n\t * previously injected exception is not checked because it was checked\n\t * when it was original queued, and re-checking is incorrect if _L1_\n\t * injected the exception, in which case it's exempt from interception.\n\t */\n\tif (!reinject && is_guest_mode(vcpu) &&\n\t    kvm_x86_ops.nested_ops->is_exception_vmexit(vcpu, nr, error_code)) {\n\t\tkvm_queue_exception_vmexit(vcpu, nr, has_error, error_code,\n\t\t\t\t\t   has_payload, payload);\n\t\treturn;\n\t}\n\n\tif (!vcpu->arch.exception.pending && !vcpu->arch.exception.injected) {\n\tqueue:\n\t\tif (reinject) {\n\t\t\t/*\n\t\t\t * On VM-Entry, an exception can be pending if and only\n\t\t\t * if event injection was blocked by nested_run_pending.\n\t\t\t * In that case, however, vcpu_enter_guest() requests an\n\t\t\t * immediate exit, and the guest shouldn't proceed far\n\t\t\t * enough to need reinjection.\n\t\t\t */\n\t\t\tWARN_ON_ONCE(kvm_is_exception_pending(vcpu));\n\t\t\tvcpu->arch.exception.injected = true;\n\t\t\tif (WARN_ON_ONCE(has_payload)) {\n\t\t\t\t/*\n\t\t\t\t * A reinjected event has already\n\t\t\t\t * delivered its payload.\n\t\t\t\t */\n\t\t\t\thas_payload = false;\n\t\t\t\tpayload = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tvcpu->arch.exception.pending = true;\n\t\t\tvcpu->arch.exception.injected = false;\n\t\t}\n\t\tvcpu->arch.exception.has_error_code = has_error;\n\t\tvcpu->arch.exception.vector = nr;\n\t\tvcpu->arch.exception.error_code = error_code;\n\t\tvcpu->arch.exception.has_payload = has_payload;\n\t\tvcpu->arch.exception.payload = payload;\n\t\tif (!is_guest_mode(vcpu))\n\t\t\tkvm_deliver_exception_payload(vcpu,\n\t\t\t\t\t\t      &vcpu->arch.exception);\n\t\treturn;\n\t}\n\n\t/* to check exception */\n\tprev_nr = vcpu->arch.exception.vector;\n\tif (prev_nr == DF_VECTOR) {\n\t\t/* triple fault -> shutdown */\n\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\treturn;\n\t}\n\tclass1 = exception_class(prev_nr);\n\tclass2 = exception_class(nr);\n\tif ((class1 == EXCPT_CONTRIBUTORY && class2 == EXCPT_CONTRIBUTORY) ||\n\t    (class1 == EXCPT_PF && class2 != EXCPT_BENIGN)) {\n\t\t/*\n\t\t * Synthesize #DF.  Clear the previously injected or pending\n\t\t * exception so as not to incorrectly trigger shutdown.\n\t\t */\n\t\tvcpu->arch.exception.injected = false;\n\t\tvcpu->arch.exception.pending = false;\n\n\t\tkvm_queue_exception_e(vcpu, DF_VECTOR, 0);\n\t} else {\n\t\t/* replace previous exception with a new one in a hope\n\t\t   that instruction re-execution will regenerate lost\n\t\t   exception */\n\t\tgoto queue;\n\t}\n}\n\nvoid kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr)\n{\n\tkvm_multiple_exception(vcpu, nr, false, 0, false, 0, false);\n}\nEXPORT_SYMBOL_GPL(kvm_queue_exception);\n\nvoid kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr)\n{\n\tkvm_multiple_exception(vcpu, nr, false, 0, false, 0, true);\n}\nEXPORT_SYMBOL_GPL(kvm_requeue_exception);\n\nvoid kvm_queue_exception_p(struct kvm_vcpu *vcpu, unsigned nr,\n\t\t\t   unsigned long payload)\n{\n\tkvm_multiple_exception(vcpu, nr, false, 0, true, payload, false);\n}\nEXPORT_SYMBOL_GPL(kvm_queue_exception_p);\n\nstatic void kvm_queue_exception_e_p(struct kvm_vcpu *vcpu, unsigned nr,\n\t\t\t\t    u32 error_code, unsigned long payload)\n{\n\tkvm_multiple_exception(vcpu, nr, true, error_code,\n\t\t\t       true, payload, false);\n}\n\nint kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err)\n{\n\tif (err)\n\t\tkvm_inject_gp(vcpu, 0);\n\telse\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(kvm_complete_insn_gp);\n\nstatic int complete_emulated_insn_gp(struct kvm_vcpu *vcpu, int err)\n{\n\tif (err) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\treturn kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE | EMULTYPE_SKIP |\n\t\t\t\t       EMULTYPE_COMPLETE_USER_EXIT);\n}\n\nvoid kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault)\n{\n\t++vcpu->stat.pf_guest;\n\n\t/*\n\t * Async #PF in L2 is always forwarded to L1 as a VM-Exit regardless of\n\t * whether or not L1 wants to intercept \"regular\" #PF.\n\t */\n\tif (is_guest_mode(vcpu) && fault->async_page_fault)\n\t\tkvm_queue_exception_vmexit(vcpu, PF_VECTOR,\n\t\t\t\t\t   true, fault->error_code,\n\t\t\t\t\t   true, fault->address);\n\telse\n\t\tkvm_queue_exception_e_p(vcpu, PF_VECTOR, fault->error_code,\n\t\t\t\t\tfault->address);\n}\n\nvoid kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct x86_exception *fault)\n{\n\tstruct kvm_mmu *fault_mmu;\n\tWARN_ON_ONCE(fault->vector != PF_VECTOR);\n\n\tfault_mmu = fault->nested_page_fault ? vcpu->arch.mmu :\n\t\t\t\t\t       vcpu->arch.walk_mmu;\n\n\t/*\n\t * Invalidate the TLB entry for the faulting address, if it exists,\n\t * else the access will fault indefinitely (and to emulate hardware).\n\t */\n\tif ((fault->error_code & PFERR_PRESENT_MASK) &&\n\t    !(fault->error_code & PFERR_RSVD_MASK))\n\t\tkvm_mmu_invalidate_gva(vcpu, fault_mmu, fault->address,\n\t\t\t\t       fault_mmu->root.hpa);\n\n\tfault_mmu->inject_page_fault(vcpu, fault);\n}\nEXPORT_SYMBOL_GPL(kvm_inject_emulated_page_fault);\n\nvoid kvm_inject_nmi(struct kvm_vcpu *vcpu)\n{\n\tatomic_inc(&vcpu->arch.nmi_queued);\n\tkvm_make_request(KVM_REQ_NMI, vcpu);\n}\n\nvoid kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code)\n{\n\tkvm_multiple_exception(vcpu, nr, true, error_code, false, 0, false);\n}\nEXPORT_SYMBOL_GPL(kvm_queue_exception_e);\n\nvoid kvm_requeue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code)\n{\n\tkvm_multiple_exception(vcpu, nr, true, error_code, false, 0, true);\n}\nEXPORT_SYMBOL_GPL(kvm_requeue_exception_e);\n\n/*\n * Checks if cpl <= required_cpl; if true, return true.  Otherwise queue\n * a #GP and return false.\n */\nbool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl)\n{\n\tif (static_call(kvm_x86_get_cpl)(vcpu) <= required_cpl)\n\t\treturn true;\n\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\treturn false;\n}\n\nbool kvm_require_dr(struct kvm_vcpu *vcpu, int dr)\n{\n\tif ((dr != 4 && dr != 5) || !kvm_read_cr4_bits(vcpu, X86_CR4_DE))\n\t\treturn true;\n\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(kvm_require_dr);\n\nstatic inline u64 pdptr_rsvd_bits(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.reserved_gpa_bits | rsvd_bits(5, 8) | rsvd_bits(1, 2);\n}\n\n/*\n * Load the pae pdptrs.  Return 1 if they are all valid, 0 otherwise.\n */\nint load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tgfn_t pdpt_gfn = cr3 >> PAGE_SHIFT;\n\tgpa_t real_gpa;\n\tint i;\n\tint ret;\n\tu64 pdpte[ARRAY_SIZE(mmu->pdptrs)];\n\n\t/*\n\t * If the MMU is nested, CR3 holds an L2 GPA and needs to be translated\n\t * to an L1 GPA.\n\t */\n\treal_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn),\n\t\t\t\t     PFERR_USER_MASK | PFERR_WRITE_MASK, NULL);\n\tif (real_gpa == INVALID_GPA)\n\t\treturn 0;\n\n\t/* Note the offset, PDPTRs are 32 byte aligned when using PAE paging. */\n\tret = kvm_vcpu_read_guest_page(vcpu, gpa_to_gfn(real_gpa), pdpte,\n\t\t\t\t       cr3 & GENMASK(11, 5), sizeof(pdpte));\n\tif (ret < 0)\n\t\treturn 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(pdpte); ++i) {\n\t\tif ((pdpte[i] & PT_PRESENT_MASK) &&\n\t\t    (pdpte[i] & pdptr_rsvd_bits(vcpu))) {\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * Marking VCPU_EXREG_PDPTR dirty doesn't work for !tdp_enabled.\n\t * Shadow page roots need to be reconstructed instead.\n\t */\n\tif (!tdp_enabled && memcmp(mmu->pdptrs, pdpte, sizeof(mmu->pdptrs)))\n\t\tkvm_mmu_free_roots(vcpu->kvm, mmu, KVM_MMU_ROOT_CURRENT);\n\n\tmemcpy(mmu->pdptrs, pdpte, sizeof(mmu->pdptrs));\n\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_PDPTR);\n\tkvm_make_request(KVM_REQ_LOAD_MMU_PGD, vcpu);\n\tvcpu->arch.pdptrs_from_userspace = false;\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(load_pdptrs);\n\nvoid kvm_post_set_cr0(struct kvm_vcpu *vcpu, unsigned long old_cr0, unsigned long cr0)\n{\n\tif ((cr0 ^ old_cr0) & X86_CR0_PG) {\n\t\tkvm_clear_async_pf_completion_queue(vcpu);\n\t\tkvm_async_pf_hash_reset(vcpu);\n\n\t\t/*\n\t\t * Clearing CR0.PG is defined to flush the TLB from the guest's\n\t\t * perspective.\n\t\t */\n\t\tif (!(cr0 & X86_CR0_PG))\n\t\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t}\n\n\tif ((cr0 ^ old_cr0) & KVM_MMU_CR0_ROLE_BITS)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\tif (((cr0 ^ old_cr0) & X86_CR0_CD) &&\n\t    kvm_arch_has_noncoherent_dma(vcpu->kvm) &&\n\t    !kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))\n\t\tkvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);\n}\nEXPORT_SYMBOL_GPL(kvm_post_set_cr0);\n\nint kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)\n{\n\tunsigned long old_cr0 = kvm_read_cr0(vcpu);\n\n\tcr0 |= X86_CR0_ET;\n\n#ifdef CONFIG_X86_64\n\tif (cr0 & 0xffffffff00000000UL)\n\t\treturn 1;\n#endif\n\n\tcr0 &= ~CR0_RESERVED_BITS;\n\n\tif ((cr0 & X86_CR0_NW) && !(cr0 & X86_CR0_CD))\n\t\treturn 1;\n\n\tif ((cr0 & X86_CR0_PG) && !(cr0 & X86_CR0_PE))\n\t\treturn 1;\n\n#ifdef CONFIG_X86_64\n\tif ((vcpu->arch.efer & EFER_LME) && !is_paging(vcpu) &&\n\t    (cr0 & X86_CR0_PG)) {\n\t\tint cs_db, cs_l;\n\n\t\tif (!is_pae(vcpu))\n\t\t\treturn 1;\n\t\tstatic_call(kvm_x86_get_cs_db_l_bits)(vcpu, &cs_db, &cs_l);\n\t\tif (cs_l)\n\t\t\treturn 1;\n\t}\n#endif\n\tif (!(vcpu->arch.efer & EFER_LME) && (cr0 & X86_CR0_PG) &&\n\t    is_pae(vcpu) && ((cr0 ^ old_cr0) & X86_CR0_PDPTR_BITS) &&\n\t    !load_pdptrs(vcpu, kvm_read_cr3(vcpu)))\n\t\treturn 1;\n\n\tif (!(cr0 & X86_CR0_PG) &&\n\t    (is_64_bit_mode(vcpu) || kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE)))\n\t\treturn 1;\n\n\tstatic_call(kvm_x86_set_cr0)(vcpu, cr0);\n\n\tkvm_post_set_cr0(vcpu, old_cr0, cr0);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr0);\n\nvoid kvm_lmsw(struct kvm_vcpu *vcpu, unsigned long msw)\n{\n\t(void)kvm_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~0x0eul) | (msw & 0x0f));\n}\nEXPORT_SYMBOL_GPL(kvm_lmsw);\n\nvoid kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn;\n\n\tif (kvm_read_cr4_bits(vcpu, X86_CR4_OSXSAVE)) {\n\n\t\tif (vcpu->arch.xcr0 != host_xcr0)\n\t\t\txsetbv(XCR_XFEATURE_ENABLED_MASK, vcpu->arch.xcr0);\n\n\t\tif (vcpu->arch.xsaves_enabled &&\n\t\t    vcpu->arch.ia32_xss != host_xss)\n\t\t\twrmsrl(MSR_IA32_XSS, vcpu->arch.ia32_xss);\n\t}\n\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\tif (static_cpu_has(X86_FEATURE_PKU) &&\n\t    vcpu->arch.pkru != vcpu->arch.host_pkru &&\n\t    ((vcpu->arch.xcr0 & XFEATURE_MASK_PKRU) ||\n\t     kvm_read_cr4_bits(vcpu, X86_CR4_PKE)))\n\t\twrite_pkru(vcpu->arch.pkru);\n#endif /* CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS */\n}\nEXPORT_SYMBOL_GPL(kvm_load_guest_xsave_state);\n\nvoid kvm_load_host_xsave_state(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn;\n\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\tif (static_cpu_has(X86_FEATURE_PKU) &&\n\t    ((vcpu->arch.xcr0 & XFEATURE_MASK_PKRU) ||\n\t     kvm_read_cr4_bits(vcpu, X86_CR4_PKE))) {\n\t\tvcpu->arch.pkru = rdpkru();\n\t\tif (vcpu->arch.pkru != vcpu->arch.host_pkru)\n\t\t\twrite_pkru(vcpu->arch.host_pkru);\n\t}\n#endif /* CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS */\n\n\tif (kvm_read_cr4_bits(vcpu, X86_CR4_OSXSAVE)) {\n\n\t\tif (vcpu->arch.xcr0 != host_xcr0)\n\t\t\txsetbv(XCR_XFEATURE_ENABLED_MASK, host_xcr0);\n\n\t\tif (vcpu->arch.xsaves_enabled &&\n\t\t    vcpu->arch.ia32_xss != host_xss)\n\t\t\twrmsrl(MSR_IA32_XSS, host_xss);\n\t}\n\n}\nEXPORT_SYMBOL_GPL(kvm_load_host_xsave_state);\n\n#ifdef CONFIG_X86_64\nstatic inline u64 kvm_guest_supported_xfd(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.guest_supported_xcr0 & XFEATURE_MASK_USER_DYNAMIC;\n}\n#endif\n\nstatic int __kvm_set_xcr(struct kvm_vcpu *vcpu, u32 index, u64 xcr)\n{\n\tu64 xcr0 = xcr;\n\tu64 old_xcr0 = vcpu->arch.xcr0;\n\tu64 valid_bits;\n\n\t/* Only support XCR_XFEATURE_ENABLED_MASK(xcr0) now  */\n\tif (index != XCR_XFEATURE_ENABLED_MASK)\n\t\treturn 1;\n\tif (!(xcr0 & XFEATURE_MASK_FP))\n\t\treturn 1;\n\tif ((xcr0 & XFEATURE_MASK_YMM) && !(xcr0 & XFEATURE_MASK_SSE))\n\t\treturn 1;\n\n\t/*\n\t * Do not allow the guest to set bits that we do not support\n\t * saving.  However, xcr0 bit 0 is always set, even if the\n\t * emulated CPU does not support XSAVE (see kvm_vcpu_reset()).\n\t */\n\tvalid_bits = vcpu->arch.guest_supported_xcr0 | XFEATURE_MASK_FP;\n\tif (xcr0 & ~valid_bits)\n\t\treturn 1;\n\n\tif ((!(xcr0 & XFEATURE_MASK_BNDREGS)) !=\n\t    (!(xcr0 & XFEATURE_MASK_BNDCSR)))\n\t\treturn 1;\n\n\tif (xcr0 & XFEATURE_MASK_AVX512) {\n\t\tif (!(xcr0 & XFEATURE_MASK_YMM))\n\t\t\treturn 1;\n\t\tif ((xcr0 & XFEATURE_MASK_AVX512) != XFEATURE_MASK_AVX512)\n\t\t\treturn 1;\n\t}\n\n\tif ((xcr0 & XFEATURE_MASK_XTILE) &&\n\t    ((xcr0 & XFEATURE_MASK_XTILE) != XFEATURE_MASK_XTILE))\n\t\treturn 1;\n\n\tvcpu->arch.xcr0 = xcr0;\n\n\tif ((xcr0 ^ old_xcr0) & XFEATURE_MASK_EXTEND)\n\t\tkvm_update_cpuid_runtime(vcpu);\n\treturn 0;\n}\n\nint kvm_emulate_xsetbv(struct kvm_vcpu *vcpu)\n{\n\t/* Note, #UD due to CR4.OSXSAVE=0 has priority over the intercept. */\n\tif (static_call(kvm_x86_get_cpl)(vcpu) != 0 ||\n\t    __kvm_set_xcr(vcpu, kvm_rcx_read(vcpu), kvm_read_edx_eax(vcpu))) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_xsetbv);\n\nbool __kvm_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\tif (cr4 & cr4_reserved_bits)\n\t\treturn false;\n\n\tif (cr4 & vcpu->arch.cr4_guest_rsvd_bits)\n\t\treturn false;\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(__kvm_is_valid_cr4);\n\nstatic bool kvm_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\treturn __kvm_is_valid_cr4(vcpu, cr4) &&\n\t       static_call(kvm_x86_is_valid_cr4)(vcpu, cr4);\n}\n\nvoid kvm_post_set_cr4(struct kvm_vcpu *vcpu, unsigned long old_cr4, unsigned long cr4)\n{\n\tif ((cr4 ^ old_cr4) & KVM_MMU_CR4_ROLE_BITS)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\t/*\n\t * If CR4.PCIDE is changed 0 -> 1, there is no need to flush the TLB\n\t * according to the SDM; however, stale prev_roots could be reused\n\t * incorrectly in the future after a MOV to CR3 with NOFLUSH=1, so we\n\t * free them all.  This is *not* a superset of KVM_REQ_TLB_FLUSH_GUEST\n\t * or KVM_REQ_TLB_FLUSH_CURRENT, because the hardware TLB is not flushed,\n\t * so fall through.\n\t */\n\tif (!tdp_enabled &&\n\t    (cr4 & X86_CR4_PCIDE) && !(old_cr4 & X86_CR4_PCIDE))\n\t\tkvm_mmu_unload(vcpu);\n\n\t/*\n\t * The TLB has to be flushed for all PCIDs if any of the following\n\t * (architecturally required) changes happen:\n\t * - CR4.PCIDE is changed from 1 to 0\n\t * - CR4.PGE is toggled\n\t *\n\t * This is a superset of KVM_REQ_TLB_FLUSH_CURRENT.\n\t */\n\tif (((cr4 ^ old_cr4) & X86_CR4_PGE) ||\n\t    (!(cr4 & X86_CR4_PCIDE) && (old_cr4 & X86_CR4_PCIDE)))\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\n\t/*\n\t * The TLB has to be flushed for the current PCID if any of the\n\t * following (architecturally required) changes happen:\n\t * - CR4.SMEP is changed from 0 to 1\n\t * - CR4.PAE is toggled\n\t */\n\telse if (((cr4 ^ old_cr4) & X86_CR4_PAE) ||\n\t\t ((cr4 & X86_CR4_SMEP) && !(old_cr4 & X86_CR4_SMEP)))\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n\n}\nEXPORT_SYMBOL_GPL(kvm_post_set_cr4);\n\nint kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\tunsigned long old_cr4 = kvm_read_cr4(vcpu);\n\n\tif (!kvm_is_valid_cr4(vcpu, cr4))\n\t\treturn 1;\n\n\tif (is_long_mode(vcpu)) {\n\t\tif (!(cr4 & X86_CR4_PAE))\n\t\t\treturn 1;\n\t\tif ((cr4 ^ old_cr4) & X86_CR4_LA57)\n\t\t\treturn 1;\n\t} else if (is_paging(vcpu) && (cr4 & X86_CR4_PAE)\n\t\t   && ((cr4 ^ old_cr4) & X86_CR4_PDPTR_BITS)\n\t\t   && !load_pdptrs(vcpu, kvm_read_cr3(vcpu)))\n\t\treturn 1;\n\n\tif ((cr4 & X86_CR4_PCIDE) && !(old_cr4 & X86_CR4_PCIDE)) {\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_PCID))\n\t\t\treturn 1;\n\n\t\t/* PCID can not be enabled when cr3[11:0]!=000H or EFER.LMA=0 */\n\t\tif ((kvm_read_cr3(vcpu) & X86_CR3_PCID_MASK) || !is_long_mode(vcpu))\n\t\t\treturn 1;\n\t}\n\n\tstatic_call(kvm_x86_set_cr4)(vcpu, cr4);\n\n\tkvm_post_set_cr4(vcpu, old_cr4, cr4);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr4);\n\nstatic void kvm_invalidate_pcid(struct kvm_vcpu *vcpu, unsigned long pcid)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tunsigned long roots_to_free = 0;\n\tint i;\n\n\t/*\n\t * MOV CR3 and INVPCID are usually not intercepted when using TDP, but\n\t * this is reachable when running EPT=1 and unrestricted_guest=0,  and\n\t * also via the emulator.  KVM's TDP page tables are not in the scope of\n\t * the invalidation, but the guest's TLB entries need to be flushed as\n\t * the CPU may have cached entries in its TLB for the target PCID.\n\t */\n\tif (unlikely(tdp_enabled)) {\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t\treturn;\n\t}\n\n\t/*\n\t * If neither the current CR3 nor any of the prev_roots use the given\n\t * PCID, then nothing needs to be done here because a resync will\n\t * happen anyway before switching to any other CR3.\n\t */\n\tif (kvm_get_active_pcid(vcpu) == pcid) {\n\t\tkvm_make_request(KVM_REQ_MMU_SYNC, vcpu);\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n\t}\n\n\t/*\n\t * If PCID is disabled, there is no need to free prev_roots even if the\n\t * PCIDs for them are also 0, because MOV to CR3 always flushes the TLB\n\t * with PCIDE=0.\n\t */\n\tif (!kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE))\n\t\treturn;\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)\n\t\tif (kvm_get_pcid(vcpu, mmu->prev_roots[i].pgd) == pcid)\n\t\t\troots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);\n\n\tkvm_mmu_free_roots(vcpu->kvm, mmu, roots_to_free);\n}\n\nint kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)\n{\n\tbool skip_tlb_flush = false;\n\tunsigned long pcid = 0;\n#ifdef CONFIG_X86_64\n\tbool pcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);\n\n\tif (pcid_enabled) {\n\t\tskip_tlb_flush = cr3 & X86_CR3_PCID_NOFLUSH;\n\t\tcr3 &= ~X86_CR3_PCID_NOFLUSH;\n\t\tpcid = cr3 & X86_CR3_PCID_MASK;\n\t}\n#endif\n\n\t/* PDPTRs are always reloaded for PAE paging. */\n\tif (cr3 == kvm_read_cr3(vcpu) && !is_pae_paging(vcpu))\n\t\tgoto handle_tlb_flush;\n\n\t/*\n\t * Do not condition the GPA check on long mode, this helper is used to\n\t * stuff CR3, e.g. for RSM emulation, and there is no guarantee that\n\t * the current vCPU mode is accurate.\n\t */\n\tif (kvm_vcpu_is_illegal_gpa(vcpu, cr3))\n\t\treturn 1;\n\n\tif (is_pae_paging(vcpu) && !load_pdptrs(vcpu, cr3))\n\t\treturn 1;\n\n\tif (cr3 != kvm_read_cr3(vcpu))\n\t\tkvm_mmu_new_pgd(vcpu, cr3);\n\n\tvcpu->arch.cr3 = cr3;\n\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_CR3);\n\t/* Do not call post_set_cr3, we do not get here for confidential guests.  */\n\nhandle_tlb_flush:\n\t/*\n\t * A load of CR3 that flushes the TLB flushes only the current PCID,\n\t * even if PCID is disabled, in which case PCID=0 is flushed.  It's a\n\t * moot point in the end because _disabling_ PCID will flush all PCIDs,\n\t * and it's impossible to use a non-zero PCID when PCID is disabled,\n\t * i.e. only PCID=0 can be relevant.\n\t */\n\tif (!skip_tlb_flush)\n\t\tkvm_invalidate_pcid(vcpu, pcid);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr3);\n\nint kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8)\n{\n\tif (cr8 & CR8_RESERVED_BITS)\n\t\treturn 1;\n\tif (lapic_in_kernel(vcpu))\n\t\tkvm_lapic_set_tpr(vcpu, cr8);\n\telse\n\t\tvcpu->arch.cr8 = cr8;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr8);\n\nunsigned long kvm_get_cr8(struct kvm_vcpu *vcpu)\n{\n\tif (lapic_in_kernel(vcpu))\n\t\treturn kvm_lapic_get_cr8(vcpu);\n\telse\n\t\treturn vcpu->arch.cr8;\n}\nEXPORT_SYMBOL_GPL(kvm_get_cr8);\n\nstatic void kvm_update_dr0123(struct kvm_vcpu *vcpu)\n{\n\tint i;\n\n\tif (!(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP)) {\n\t\tfor (i = 0; i < KVM_NR_DB_REGS; i++)\n\t\t\tvcpu->arch.eff_db[i] = vcpu->arch.db[i];\n\t}\n}\n\nvoid kvm_update_dr7(struct kvm_vcpu *vcpu)\n{\n\tunsigned long dr7;\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP)\n\t\tdr7 = vcpu->arch.guest_debug_dr7;\n\telse\n\t\tdr7 = vcpu->arch.dr7;\n\tstatic_call(kvm_x86_set_dr7)(vcpu, dr7);\n\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_BP_ENABLED;\n\tif (dr7 & DR7_BP_EN_MASK)\n\t\tvcpu->arch.switch_db_regs |= KVM_DEBUGREG_BP_ENABLED;\n}\nEXPORT_SYMBOL_GPL(kvm_update_dr7);\n\nstatic u64 kvm_dr6_fixed(struct kvm_vcpu *vcpu)\n{\n\tu64 fixed = DR6_FIXED_1;\n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_RTM))\n\t\tfixed |= DR6_RTM;\n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_BUS_LOCK_DETECT))\n\t\tfixed |= DR6_BUS_LOCK;\n\treturn fixed;\n}\n\nint kvm_set_dr(struct kvm_vcpu *vcpu, int dr, unsigned long val)\n{\n\tsize_t size = ARRAY_SIZE(vcpu->arch.db);\n\n\tswitch (dr) {\n\tcase 0 ... 3:\n\t\tvcpu->arch.db[array_index_nospec(dr, size)] = val;\n\t\tif (!(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP))\n\t\t\tvcpu->arch.eff_db[dr] = val;\n\t\tbreak;\n\tcase 4:\n\tcase 6:\n\t\tif (!kvm_dr6_valid(val))\n\t\t\treturn 1; /* #GP */\n\t\tvcpu->arch.dr6 = (val & DR6_VOLATILE) | kvm_dr6_fixed(vcpu);\n\t\tbreak;\n\tcase 5:\n\tdefault: /* 7 */\n\t\tif (!kvm_dr7_valid(val))\n\t\t\treturn 1; /* #GP */\n\t\tvcpu->arch.dr7 = (val & DR7_VOLATILE) | DR7_FIXED_1;\n\t\tkvm_update_dr7(vcpu);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_dr);\n\nvoid kvm_get_dr(struct kvm_vcpu *vcpu, int dr, unsigned long *val)\n{\n\tsize_t size = ARRAY_SIZE(vcpu->arch.db);\n\n\tswitch (dr) {\n\tcase 0 ... 3:\n\t\t*val = vcpu->arch.db[array_index_nospec(dr, size)];\n\t\tbreak;\n\tcase 4:\n\tcase 6:\n\t\t*val = vcpu->arch.dr6;\n\t\tbreak;\n\tcase 5:\n\tdefault: /* 7 */\n\t\t*val = vcpu->arch.dr7;\n\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_get_dr);\n\nint kvm_emulate_rdpmc(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = kvm_rcx_read(vcpu);\n\tu64 data;\n\n\tif (kvm_pmu_rdpmc(vcpu, ecx, &data)) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tkvm_rax_write(vcpu, (u32)data);\n\tkvm_rdx_write(vcpu, data >> 32);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_rdpmc);\n\n/*\n * List of msr numbers which we expose to userspace through KVM_GET_MSRS\n * and KVM_SET_MSRS, and KVM_GET_MSR_INDEX_LIST.\n *\n * The three MSR lists(msrs_to_save, emulated_msrs, msr_based_features)\n * extract the supported MSRs from the related const lists.\n * msrs_to_save is selected from the msrs_to_save_all to reflect the\n * capabilities of the host cpu. This capabilities test skips MSRs that are\n * kvm-specific. Those are put in emulated_msrs_all; filtering of emulated_msrs\n * may depend on host virtualization features rather than host cpu features.\n */\n\nstatic const u32 msrs_to_save_all[] = {\n\tMSR_IA32_SYSENTER_CS, MSR_IA32_SYSENTER_ESP, MSR_IA32_SYSENTER_EIP,\n\tMSR_STAR,\n#ifdef CONFIG_X86_64\n\tMSR_CSTAR, MSR_KERNEL_GS_BASE, MSR_SYSCALL_MASK, MSR_LSTAR,\n#endif\n\tMSR_IA32_TSC, MSR_IA32_CR_PAT, MSR_VM_HSAVE_PA,\n\tMSR_IA32_FEAT_CTL, MSR_IA32_BNDCFGS, MSR_TSC_AUX,\n\tMSR_IA32_SPEC_CTRL,\n\tMSR_IA32_RTIT_CTL, MSR_IA32_RTIT_STATUS, MSR_IA32_RTIT_CR3_MATCH,\n\tMSR_IA32_RTIT_OUTPUT_BASE, MSR_IA32_RTIT_OUTPUT_MASK,\n\tMSR_IA32_RTIT_ADDR0_A, MSR_IA32_RTIT_ADDR0_B,\n\tMSR_IA32_RTIT_ADDR1_A, MSR_IA32_RTIT_ADDR1_B,\n\tMSR_IA32_RTIT_ADDR2_A, MSR_IA32_RTIT_ADDR2_B,\n\tMSR_IA32_RTIT_ADDR3_A, MSR_IA32_RTIT_ADDR3_B,\n\tMSR_IA32_UMWAIT_CONTROL,\n\n\tMSR_ARCH_PERFMON_FIXED_CTR0, MSR_ARCH_PERFMON_FIXED_CTR1,\n\tMSR_ARCH_PERFMON_FIXED_CTR0 + 2,\n\tMSR_CORE_PERF_FIXED_CTR_CTRL, MSR_CORE_PERF_GLOBAL_STATUS,\n\tMSR_CORE_PERF_GLOBAL_CTRL, MSR_CORE_PERF_GLOBAL_OVF_CTRL,\n\tMSR_IA32_PEBS_ENABLE, MSR_IA32_DS_AREA, MSR_PEBS_DATA_CFG,\n\n\t/* This part of MSRs should match KVM_INTEL_PMC_MAX_GENERIC. */\n\tMSR_ARCH_PERFMON_PERFCTR0, MSR_ARCH_PERFMON_PERFCTR1,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 2, MSR_ARCH_PERFMON_PERFCTR0 + 3,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 4, MSR_ARCH_PERFMON_PERFCTR0 + 5,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 6, MSR_ARCH_PERFMON_PERFCTR0 + 7,\n\tMSR_ARCH_PERFMON_EVENTSEL0, MSR_ARCH_PERFMON_EVENTSEL1,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 2, MSR_ARCH_PERFMON_EVENTSEL0 + 3,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 4, MSR_ARCH_PERFMON_EVENTSEL0 + 5,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 6, MSR_ARCH_PERFMON_EVENTSEL0 + 7,\n\n\tMSR_K7_EVNTSEL0, MSR_K7_EVNTSEL1, MSR_K7_EVNTSEL2, MSR_K7_EVNTSEL3,\n\tMSR_K7_PERFCTR0, MSR_K7_PERFCTR1, MSR_K7_PERFCTR2, MSR_K7_PERFCTR3,\n\n\t/* This part of MSRs should match KVM_AMD_PMC_MAX_GENERIC. */\n\tMSR_F15H_PERF_CTL0, MSR_F15H_PERF_CTL1, MSR_F15H_PERF_CTL2,\n\tMSR_F15H_PERF_CTL3, MSR_F15H_PERF_CTL4, MSR_F15H_PERF_CTL5,\n\tMSR_F15H_PERF_CTR0, MSR_F15H_PERF_CTR1, MSR_F15H_PERF_CTR2,\n\tMSR_F15H_PERF_CTR3, MSR_F15H_PERF_CTR4, MSR_F15H_PERF_CTR5,\n\n\tMSR_IA32_XFD, MSR_IA32_XFD_ERR,\n};\n\nstatic u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_all)];\nstatic unsigned num_msrs_to_save;\n\nstatic const u32 emulated_msrs_all[] = {\n\tMSR_KVM_SYSTEM_TIME, MSR_KVM_WALL_CLOCK,\n\tMSR_KVM_SYSTEM_TIME_NEW, MSR_KVM_WALL_CLOCK_NEW,\n\tHV_X64_MSR_GUEST_OS_ID, HV_X64_MSR_HYPERCALL,\n\tHV_X64_MSR_TIME_REF_COUNT, HV_X64_MSR_REFERENCE_TSC,\n\tHV_X64_MSR_TSC_FREQUENCY, HV_X64_MSR_APIC_FREQUENCY,\n\tHV_X64_MSR_CRASH_P0, HV_X64_MSR_CRASH_P1, HV_X64_MSR_CRASH_P2,\n\tHV_X64_MSR_CRASH_P3, HV_X64_MSR_CRASH_P4, HV_X64_MSR_CRASH_CTL,\n\tHV_X64_MSR_RESET,\n\tHV_X64_MSR_VP_INDEX,\n\tHV_X64_MSR_VP_RUNTIME,\n\tHV_X64_MSR_SCONTROL,\n\tHV_X64_MSR_STIMER0_CONFIG,\n\tHV_X64_MSR_VP_ASSIST_PAGE,\n\tHV_X64_MSR_REENLIGHTENMENT_CONTROL, HV_X64_MSR_TSC_EMULATION_CONTROL,\n\tHV_X64_MSR_TSC_EMULATION_STATUS,\n\tHV_X64_MSR_SYNDBG_OPTIONS,\n\tHV_X64_MSR_SYNDBG_CONTROL, HV_X64_MSR_SYNDBG_STATUS,\n\tHV_X64_MSR_SYNDBG_SEND_BUFFER, HV_X64_MSR_SYNDBG_RECV_BUFFER,\n\tHV_X64_MSR_SYNDBG_PENDING_BUFFER,\n\n\tMSR_KVM_ASYNC_PF_EN, MSR_KVM_STEAL_TIME,\n\tMSR_KVM_PV_EOI_EN, MSR_KVM_ASYNC_PF_INT, MSR_KVM_ASYNC_PF_ACK,\n\n\tMSR_IA32_TSC_ADJUST,\n\tMSR_IA32_TSC_DEADLINE,\n\tMSR_IA32_ARCH_CAPABILITIES,\n\tMSR_IA32_PERF_CAPABILITIES,\n\tMSR_IA32_MISC_ENABLE,\n\tMSR_IA32_MCG_STATUS,\n\tMSR_IA32_MCG_CTL,\n\tMSR_IA32_MCG_EXT_CTL,\n\tMSR_IA32_SMBASE,\n\tMSR_SMI_COUNT,\n\tMSR_PLATFORM_INFO,\n\tMSR_MISC_FEATURES_ENABLES,\n\tMSR_AMD64_VIRT_SPEC_CTRL,\n\tMSR_AMD64_TSC_RATIO,\n\tMSR_IA32_POWER_CTL,\n\tMSR_IA32_UCODE_REV,\n\n\t/*\n\t * The following list leaves out MSRs whose values are determined\n\t * by arch/x86/kvm/vmx/nested.c based on CPUID or other MSRs.\n\t * We always support the \"true\" VMX control MSRs, even if the host\n\t * processor does not, so I am putting these registers here rather\n\t * than in msrs_to_save_all.\n\t */\n\tMSR_IA32_VMX_BASIC,\n\tMSR_IA32_VMX_TRUE_PINBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_PROCBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_EXIT_CTLS,\n\tMSR_IA32_VMX_TRUE_ENTRY_CTLS,\n\tMSR_IA32_VMX_MISC,\n\tMSR_IA32_VMX_CR0_FIXED0,\n\tMSR_IA32_VMX_CR4_FIXED0,\n\tMSR_IA32_VMX_VMCS_ENUM,\n\tMSR_IA32_VMX_PROCBASED_CTLS2,\n\tMSR_IA32_VMX_EPT_VPID_CAP,\n\tMSR_IA32_VMX_VMFUNC,\n\n\tMSR_K7_HWCR,\n\tMSR_KVM_POLL_CONTROL,\n};\n\nstatic u32 emulated_msrs[ARRAY_SIZE(emulated_msrs_all)];\nstatic unsigned num_emulated_msrs;\n\n/*\n * List of msr numbers which are used to expose MSR-based features that\n * can be used by a hypervisor to validate requested CPU features.\n */\nstatic const u32 msr_based_features_all[] = {\n\tMSR_IA32_VMX_BASIC,\n\tMSR_IA32_VMX_TRUE_PINBASED_CTLS,\n\tMSR_IA32_VMX_PINBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_PROCBASED_CTLS,\n\tMSR_IA32_VMX_PROCBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_EXIT_CTLS,\n\tMSR_IA32_VMX_EXIT_CTLS,\n\tMSR_IA32_VMX_TRUE_ENTRY_CTLS,\n\tMSR_IA32_VMX_ENTRY_CTLS,\n\tMSR_IA32_VMX_MISC,\n\tMSR_IA32_VMX_CR0_FIXED0,\n\tMSR_IA32_VMX_CR0_FIXED1,\n\tMSR_IA32_VMX_CR4_FIXED0,\n\tMSR_IA32_VMX_CR4_FIXED1,\n\tMSR_IA32_VMX_VMCS_ENUM,\n\tMSR_IA32_VMX_PROCBASED_CTLS2,\n\tMSR_IA32_VMX_EPT_VPID_CAP,\n\tMSR_IA32_VMX_VMFUNC,\n\n\tMSR_AMD64_DE_CFG,\n\tMSR_IA32_UCODE_REV,\n\tMSR_IA32_ARCH_CAPABILITIES,\n\tMSR_IA32_PERF_CAPABILITIES,\n};\n\nstatic u32 msr_based_features[ARRAY_SIZE(msr_based_features_all)];\nstatic unsigned int num_msr_based_features;\n\n/*\n * Some IA32_ARCH_CAPABILITIES bits have dependencies on MSRs that KVM\n * does not yet virtualize. These include:\n *   10 - MISC_PACKAGE_CTRLS\n *   11 - ENERGY_FILTERING_CTL\n *   12 - DOITM\n *   18 - FB_CLEAR_CTRL\n *   21 - XAPIC_DISABLE_STATUS\n *   23 - OVERCLOCKING_STATUS\n */\n\n#define KVM_SUPPORTED_ARCH_CAP \\\n\t(ARCH_CAP_RDCL_NO | ARCH_CAP_IBRS_ALL | ARCH_CAP_RSBA | \\\n\t ARCH_CAP_SKIP_VMENTRY_L1DFLUSH | ARCH_CAP_SSB_NO | ARCH_CAP_MDS_NO | \\\n\t ARCH_CAP_PSCHANGE_MC_NO | ARCH_CAP_TSX_CTRL_MSR | ARCH_CAP_TAA_NO | \\\n\t ARCH_CAP_SBDR_SSDP_NO | ARCH_CAP_FBSDP_NO | ARCH_CAP_PSDP_NO | \\\n\t ARCH_CAP_FB_CLEAR | ARCH_CAP_RRSBA | ARCH_CAP_PBRSB_NO)\n\nstatic u64 kvm_get_arch_capabilities(void)\n{\n\tu64 data = 0;\n\n\tif (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES)) {\n\t\trdmsrl(MSR_IA32_ARCH_CAPABILITIES, data);\n\t\tdata &= KVM_SUPPORTED_ARCH_CAP;\n\t}\n\n\t/*\n\t * If nx_huge_pages is enabled, KVM's shadow paging will ensure that\n\t * the nested hypervisor runs with NX huge pages.  If it is not,\n\t * L1 is anyway vulnerable to ITLB_MULTIHIT exploits from other\n\t * L1 guests, so it need not worry about its own (L2) guests.\n\t */\n\tdata |= ARCH_CAP_PSCHANGE_MC_NO;\n\n\t/*\n\t * If we're doing cache flushes (either \"always\" or \"cond\")\n\t * we will do one whenever the guest does a vmlaunch/vmresume.\n\t * If an outer hypervisor is doing the cache flush for us\n\t * (VMENTER_L1D_FLUSH_NESTED_VM), we can safely pass that\n\t * capability to the guest too, and if EPT is disabled we're not\n\t * vulnerable.  Overall, only VMENTER_L1D_FLUSH_NEVER will\n\t * require a nested hypervisor to do a flush of its own.\n\t */\n\tif (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)\n\t\tdata |= ARCH_CAP_SKIP_VMENTRY_L1DFLUSH;\n\n\tif (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))\n\t\tdata |= ARCH_CAP_RDCL_NO;\n\tif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\tdata |= ARCH_CAP_SSB_NO;\n\tif (!boot_cpu_has_bug(X86_BUG_MDS))\n\t\tdata |= ARCH_CAP_MDS_NO;\n\n\tif (!boot_cpu_has(X86_FEATURE_RTM)) {\n\t\t/*\n\t\t * If RTM=0 because the kernel has disabled TSX, the host might\n\t\t * have TAA_NO or TSX_CTRL.  Clear TAA_NO (the guest sees RTM=0\n\t\t * and therefore knows that there cannot be TAA) but keep\n\t\t * TSX_CTRL: some buggy userspaces leave it set on tsx=on hosts,\n\t\t * and we want to allow migrating those guests to tsx=off hosts.\n\t\t */\n\t\tdata &= ~ARCH_CAP_TAA_NO;\n\t} else if (!boot_cpu_has_bug(X86_BUG_TAA)) {\n\t\tdata |= ARCH_CAP_TAA_NO;\n\t} else {\n\t\t/*\n\t\t * Nothing to do here; we emulate TSX_CTRL if present on the\n\t\t * host so the guest can choose between disabling TSX or\n\t\t * using VERW to clear CPU buffers.\n\t\t */\n\t}\n\n\treturn data;\n}\n\nstatic int kvm_get_msr_feature(struct kvm_msr_entry *msr)\n{\n\tswitch (msr->index) {\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tmsr->data = kvm_get_arch_capabilities();\n\t\tbreak;\n\tcase MSR_IA32_PERF_CAPABILITIES:\n\t\tmsr->data = kvm_caps.supported_perf_cap;\n\t\tbreak;\n\tcase MSR_IA32_UCODE_REV:\n\t\trdmsrl_safe(msr->index, &msr->data);\n\t\tbreak;\n\tdefault:\n\t\treturn static_call(kvm_x86_get_msr_feature)(msr);\n\t}\n\treturn 0;\n}\n\nstatic int do_get_msr_feature(struct kvm_vcpu *vcpu, unsigned index, u64 *data)\n{\n\tstruct kvm_msr_entry msr;\n\tint r;\n\n\tmsr.index = index;\n\tr = kvm_get_msr_feature(&msr);\n\n\tif (r == KVM_MSR_RET_INVALID) {\n\t\t/* Unconditionally clear the output for simplicity */\n\t\t*data = 0;\n\t\tif (kvm_msr_ignored_check(index, 0, false))\n\t\t\tr = 0;\n\t}\n\n\tif (r)\n\t\treturn r;\n\n\t*data = msr.data;\n\n\treturn 0;\n}\n\nstatic bool __kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer)\n{\n\tif (efer & EFER_FFXSR && !guest_cpuid_has(vcpu, X86_FEATURE_FXSR_OPT))\n\t\treturn false;\n\n\tif (efer & EFER_SVME && !guest_cpuid_has(vcpu, X86_FEATURE_SVM))\n\t\treturn false;\n\n\tif (efer & (EFER_LME | EFER_LMA) &&\n\t    !guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\treturn false;\n\n\tif (efer & EFER_NX && !guest_cpuid_has(vcpu, X86_FEATURE_NX))\n\t\treturn false;\n\n\treturn true;\n\n}\nbool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer)\n{\n\tif (efer & efer_reserved_bits)\n\t\treturn false;\n\n\treturn __kvm_valid_efer(vcpu, efer);\n}\nEXPORT_SYMBOL_GPL(kvm_valid_efer);\n\nstatic int set_efer(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tu64 old_efer = vcpu->arch.efer;\n\tu64 efer = msr_info->data;\n\tint r;\n\n\tif (efer & efer_reserved_bits)\n\t\treturn 1;\n\n\tif (!msr_info->host_initiated) {\n\t\tif (!__kvm_valid_efer(vcpu, efer))\n\t\t\treturn 1;\n\n\t\tif (is_paging(vcpu) &&\n\t\t    (vcpu->arch.efer & EFER_LME) != (efer & EFER_LME))\n\t\t\treturn 1;\n\t}\n\n\tefer &= ~EFER_LMA;\n\tefer |= vcpu->arch.efer & EFER_LMA;\n\n\tr = static_call(kvm_x86_set_efer)(vcpu, efer);\n\tif (r) {\n\t\tWARN_ON(r > 0);\n\t\treturn r;\n\t}\n\n\tif ((efer ^ old_efer) & KVM_MMU_EFER_ROLE_BITS)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\treturn 0;\n}\n\nvoid kvm_enable_efer_bits(u64 mask)\n{\n       efer_reserved_bits &= ~mask;\n}\nEXPORT_SYMBOL_GPL(kvm_enable_efer_bits);\n\nbool kvm_msr_allowed(struct kvm_vcpu *vcpu, u32 index, u32 type)\n{\n\tstruct kvm_x86_msr_filter *msr_filter;\n\tstruct msr_bitmap_range *ranges;\n\tstruct kvm *kvm = vcpu->kvm;\n\tbool allowed;\n\tint idx;\n\tu32 i;\n\n\t/* x2APIC MSRs do not support filtering. */\n\tif (index >= 0x800 && index <= 0x8ff)\n\t\treturn true;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\n\tmsr_filter = srcu_dereference(kvm->arch.msr_filter, &kvm->srcu);\n\tif (!msr_filter) {\n\t\tallowed = true;\n\t\tgoto out;\n\t}\n\n\tallowed = msr_filter->default_allow;\n\tranges = msr_filter->ranges;\n\n\tfor (i = 0; i < msr_filter->count; i++) {\n\t\tu32 start = ranges[i].base;\n\t\tu32 end = start + ranges[i].nmsrs;\n\t\tu32 flags = ranges[i].flags;\n\t\tunsigned long *bitmap = ranges[i].bitmap;\n\n\t\tif ((index >= start) && (index < end) && (flags & type)) {\n\t\t\tallowed = !!test_bit(index - start, bitmap);\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn allowed;\n}\nEXPORT_SYMBOL_GPL(kvm_msr_allowed);\n\n/*\n * Write @data into the MSR specified by @index.  Select MSR specific fault\n * checks are bypassed if @host_initiated is %true.\n * Returns 0 on success, non-0 otherwise.\n * Assumes vcpu_load() was already called.\n */\nstatic int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,\n\t\t\t bool host_initiated)\n{\n\tstruct msr_data msr;\n\n\tswitch (index) {\n\tcase MSR_FS_BASE:\n\tcase MSR_GS_BASE:\n\tcase MSR_KERNEL_GS_BASE:\n\tcase MSR_CSTAR:\n\tcase MSR_LSTAR:\n\t\tif (is_noncanonical_address(data, vcpu))\n\t\t\treturn 1;\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\t/*\n\t\t * IA32_SYSENTER_ESP and IA32_SYSENTER_EIP cause #GP if\n\t\t * non-canonical address is written on Intel but not on\n\t\t * AMD (which ignores the top 32-bits, because it does\n\t\t * not implement 64-bit SYSENTER).\n\t\t *\n\t\t * 64-bit code should hence be able to write a non-canonical\n\t\t * value on AMD.  Making the address canonical ensures that\n\t\t * vmentry does not fail on Intel after writing a non-canonical\n\t\t * value, and that something deterministic happens if the guest\n\t\t * invokes 64-bit SYSENTER.\n\t\t */\n\t\tdata = __canonical_address(data, vcpu_virt_addr_bits(vcpu));\n\t\tbreak;\n\tcase MSR_TSC_AUX:\n\t\tif (!kvm_is_supported_user_return_msr(MSR_TSC_AUX))\n\t\t\treturn 1;\n\n\t\tif (!host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP) &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDPID))\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * Per Intel's SDM, bits 63:32 are reserved, but AMD's APM has\n\t\t * incomplete and conflicting architectural behavior.  Current\n\t\t * AMD CPUs completely ignore bits 63:32, i.e. they aren't\n\t\t * reserved and always read as zeros.  Enforce Intel's reserved\n\t\t * bits check if and only if the guest CPU is Intel, and clear\n\t\t * the bits in all other cases.  This ensures cross-vendor\n\t\t * migration will provide consistent behavior for the guest.\n\t\t */\n\t\tif (guest_cpuid_is_intel(vcpu) && (data >> 32) != 0)\n\t\t\treturn 1;\n\n\t\tdata = (u32)data;\n\t\tbreak;\n\t}\n\n\tmsr.data = data;\n\tmsr.index = index;\n\tmsr.host_initiated = host_initiated;\n\n\treturn static_call(kvm_x86_set_msr)(vcpu, &msr);\n}\n\nstatic int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,\n\t\t\t\t     u32 index, u64 data, bool host_initiated)\n{\n\tint ret = __kvm_set_msr(vcpu, index, data, host_initiated);\n\n\tif (ret == KVM_MSR_RET_INVALID)\n\t\tif (kvm_msr_ignored_check(index, data, true))\n\t\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * Read the MSR specified by @index into @data.  Select MSR specific fault\n * checks are bypassed if @host_initiated is %true.\n * Returns 0 on success, non-0 otherwise.\n * Assumes vcpu_load() was already called.\n */\nint __kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data,\n\t\t  bool host_initiated)\n{\n\tstruct msr_data msr;\n\tint ret;\n\n\tswitch (index) {\n\tcase MSR_TSC_AUX:\n\t\tif (!kvm_is_supported_user_return_msr(MSR_TSC_AUX))\n\t\t\treturn 1;\n\n\t\tif (!host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP) &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDPID))\n\t\t\treturn 1;\n\t\tbreak;\n\t}\n\n\tmsr.index = index;\n\tmsr.host_initiated = host_initiated;\n\n\tret = static_call(kvm_x86_get_msr)(vcpu, &msr);\n\tif (!ret)\n\t\t*data = msr.data;\n\treturn ret;\n}\n\nstatic int kvm_get_msr_ignored_check(struct kvm_vcpu *vcpu,\n\t\t\t\t     u32 index, u64 *data, bool host_initiated)\n{\n\tint ret = __kvm_get_msr(vcpu, index, data, host_initiated);\n\n\tif (ret == KVM_MSR_RET_INVALID) {\n\t\t/* Unconditionally clear *data for simplicity */\n\t\t*data = 0;\n\t\tif (kvm_msr_ignored_check(index, 0, false))\n\t\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\nstatic int kvm_get_msr_with_filter(struct kvm_vcpu *vcpu, u32 index, u64 *data)\n{\n\tif (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_READ))\n\t\treturn KVM_MSR_RET_FILTERED;\n\treturn kvm_get_msr_ignored_check(vcpu, index, data, false);\n}\n\nstatic int kvm_set_msr_with_filter(struct kvm_vcpu *vcpu, u32 index, u64 data)\n{\n\tif (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_WRITE))\n\t\treturn KVM_MSR_RET_FILTERED;\n\treturn kvm_set_msr_ignored_check(vcpu, index, data, false);\n}\n\nint kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data)\n{\n\treturn kvm_get_msr_ignored_check(vcpu, index, data, false);\n}\nEXPORT_SYMBOL_GPL(kvm_get_msr);\n\nint kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data)\n{\n\treturn kvm_set_msr_ignored_check(vcpu, index, data, false);\n}\nEXPORT_SYMBOL_GPL(kvm_set_msr);\n\nstatic void complete_userspace_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tif (!vcpu->run->msr.error) {\n\t\tkvm_rax_write(vcpu, (u32)vcpu->run->msr.data);\n\t\tkvm_rdx_write(vcpu, vcpu->run->msr.data >> 32);\n\t}\n}\n\nstatic int complete_emulated_msr_access(struct kvm_vcpu *vcpu)\n{\n\treturn complete_emulated_insn_gp(vcpu, vcpu->run->msr.error);\n}\n\nstatic int complete_emulated_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tcomplete_userspace_rdmsr(vcpu);\n\treturn complete_emulated_msr_access(vcpu);\n}\n\nstatic int complete_fast_msr_access(struct kvm_vcpu *vcpu)\n{\n\treturn static_call(kvm_x86_complete_emulated_msr)(vcpu, vcpu->run->msr.error);\n}\n\nstatic int complete_fast_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tcomplete_userspace_rdmsr(vcpu);\n\treturn complete_fast_msr_access(vcpu);\n}\n\nstatic u64 kvm_msr_reason(int r)\n{\n\tswitch (r) {\n\tcase KVM_MSR_RET_INVALID:\n\t\treturn KVM_MSR_EXIT_REASON_UNKNOWN;\n\tcase KVM_MSR_RET_FILTERED:\n\t\treturn KVM_MSR_EXIT_REASON_FILTER;\n\tdefault:\n\t\treturn KVM_MSR_EXIT_REASON_INVAL;\n\t}\n}\n\nstatic int kvm_msr_user_space(struct kvm_vcpu *vcpu, u32 index,\n\t\t\t      u32 exit_reason, u64 data,\n\t\t\t      int (*completion)(struct kvm_vcpu *vcpu),\n\t\t\t      int r)\n{\n\tu64 msr_reason = kvm_msr_reason(r);\n\n\t/* Check if the user wanted to know about this MSR fault */\n\tif (!(vcpu->kvm->arch.user_space_msr_mask & msr_reason))\n\t\treturn 0;\n\n\tvcpu->run->exit_reason = exit_reason;\n\tvcpu->run->msr.error = 0;\n\tmemset(vcpu->run->msr.pad, 0, sizeof(vcpu->run->msr.pad));\n\tvcpu->run->msr.reason = msr_reason;\n\tvcpu->run->msr.index = index;\n\tvcpu->run->msr.data = data;\n\tvcpu->arch.complete_userspace_io = completion;\n\n\treturn 1;\n}\n\nint kvm_emulate_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = kvm_rcx_read(vcpu);\n\tu64 data;\n\tint r;\n\n\tr = kvm_get_msr_with_filter(vcpu, ecx, &data);\n\n\tif (!r) {\n\t\ttrace_kvm_msr_read(ecx, data);\n\n\t\tkvm_rax_write(vcpu, data & -1u);\n\t\tkvm_rdx_write(vcpu, (data >> 32) & -1u);\n\t} else {\n\t\t/* MSR read failed? See if we should ask user space */\n\t\tif (kvm_msr_user_space(vcpu, ecx, KVM_EXIT_X86_RDMSR, 0,\n\t\t\t\t       complete_fast_rdmsr, r))\n\t\t\treturn 0;\n\t\ttrace_kvm_msr_read_ex(ecx);\n\t}\n\n\treturn static_call(kvm_x86_complete_emulated_msr)(vcpu, r);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_rdmsr);\n\nint kvm_emulate_wrmsr(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = kvm_rcx_read(vcpu);\n\tu64 data = kvm_read_edx_eax(vcpu);\n\tint r;\n\n\tr = kvm_set_msr_with_filter(vcpu, ecx, data);\n\n\tif (!r) {\n\t\ttrace_kvm_msr_write(ecx, data);\n\t} else {\n\t\t/* MSR write failed? See if we should ask user space */\n\t\tif (kvm_msr_user_space(vcpu, ecx, KVM_EXIT_X86_WRMSR, data,\n\t\t\t\t       complete_fast_msr_access, r))\n\t\t\treturn 0;\n\t\t/* Signal all other negative errors to userspace */\n\t\tif (r < 0)\n\t\t\treturn r;\n\t\ttrace_kvm_msr_write_ex(ecx, data);\n\t}\n\n\treturn static_call(kvm_x86_complete_emulated_msr)(vcpu, r);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_wrmsr);\n\nint kvm_emulate_as_nop(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nint kvm_emulate_invd(struct kvm_vcpu *vcpu)\n{\n\t/* Treat an INVD instruction as a NOP and just skip it. */\n\treturn kvm_emulate_as_nop(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_invd);\n\nint kvm_handle_invalid_op(struct kvm_vcpu *vcpu)\n{\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(kvm_handle_invalid_op);\n\n\nstatic int kvm_emulate_monitor_mwait(struct kvm_vcpu *vcpu, const char *insn)\n{\n\tif (!kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_MWAIT_NEVER_UD_FAULTS) &&\n\t    !guest_cpuid_has(vcpu, X86_FEATURE_MWAIT))\n\t\treturn kvm_handle_invalid_op(vcpu);\n\n\tpr_warn_once(\"kvm: %s instruction emulated as NOP!\\n\", insn);\n\treturn kvm_emulate_as_nop(vcpu);\n}\nint kvm_emulate_mwait(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_monitor_mwait(vcpu, \"MWAIT\");\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_mwait);\n\nint kvm_emulate_monitor(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_monitor_mwait(vcpu, \"MONITOR\");\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_monitor);\n\nstatic inline bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)\n{\n\txfer_to_guest_mode_prepare();\n\treturn vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||\n\t\txfer_to_guest_mode_work_pending();\n}\n\n/*\n * The fast path for frequent and performance sensitive wrmsr emulation,\n * i.e. the sending of IPI, sending IPI early in the VM-Exit flow reduces\n * the latency of virtual IPI by avoiding the expensive bits of transitioning\n * from guest to host, e.g. reacquiring KVM's SRCU lock. In contrast to the\n * other cases which must be called after interrupts are enabled on the host.\n */\nstatic int handle_fastpath_set_x2apic_icr_irqoff(struct kvm_vcpu *vcpu, u64 data)\n{\n\tif (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(vcpu->arch.apic))\n\t\treturn 1;\n\n\tif (((data & APIC_SHORT_MASK) == APIC_DEST_NOSHORT) &&\n\t    ((data & APIC_DEST_MASK) == APIC_DEST_PHYSICAL) &&\n\t    ((data & APIC_MODE_MASK) == APIC_DM_FIXED) &&\n\t    ((u32)(data >> 32) != X2APIC_BROADCAST))\n\t\treturn kvm_x2apic_icr_write(vcpu->arch.apic, data);\n\n\treturn 1;\n}\n\nstatic int handle_fastpath_set_tscdeadline(struct kvm_vcpu *vcpu, u64 data)\n{\n\tif (!kvm_can_use_hv_timer(vcpu))\n\t\treturn 1;\n\n\tkvm_set_lapic_tscdeadline_msr(vcpu, data);\n\treturn 0;\n}\n\nfastpath_t handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu)\n{\n\tu32 msr = kvm_rcx_read(vcpu);\n\tu64 data;\n\tfastpath_t ret = EXIT_FASTPATH_NONE;\n\n\tswitch (msr) {\n\tcase APIC_BASE_MSR + (APIC_ICR >> 4):\n\t\tdata = kvm_read_edx_eax(vcpu);\n\t\tif (!handle_fastpath_set_x2apic_icr_irqoff(vcpu, data)) {\n\t\t\tkvm_skip_emulated_instruction(vcpu);\n\t\t\tret = EXIT_FASTPATH_EXIT_HANDLED;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_TSC_DEADLINE:\n\t\tdata = kvm_read_edx_eax(vcpu);\n\t\tif (!handle_fastpath_set_tscdeadline(vcpu, data)) {\n\t\t\tkvm_skip_emulated_instruction(vcpu);\n\t\t\tret = EXIT_FASTPATH_REENTER_GUEST;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (ret != EXIT_FASTPATH_NONE)\n\t\ttrace_kvm_msr_write(msr, data);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(handle_fastpath_set_msr_irqoff);\n\n/*\n * Adapt set_msr() to msr_io()'s calling convention\n */\nstatic int do_get_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)\n{\n\treturn kvm_get_msr_ignored_check(vcpu, index, data, true);\n}\n\nstatic int do_set_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)\n{\n\treturn kvm_set_msr_ignored_check(vcpu, index, *data, true);\n}\n\n#ifdef CONFIG_X86_64\nstruct pvclock_clock {\n\tint vclock_mode;\n\tu64 cycle_last;\n\tu64 mask;\n\tu32 mult;\n\tu32 shift;\n\tu64 base_cycles;\n\tu64 offset;\n};\n\nstruct pvclock_gtod_data {\n\tseqcount_t\tseq;\n\n\tstruct pvclock_clock clock; /* extract of a clocksource struct */\n\tstruct pvclock_clock raw_clock; /* extract of a clocksource struct */\n\n\tktime_t\t\toffs_boot;\n\tu64\t\twall_time_sec;\n};\n\nstatic struct pvclock_gtod_data pvclock_gtod_data;\n\nstatic void update_pvclock_gtod(struct timekeeper *tk)\n{\n\tstruct pvclock_gtod_data *vdata = &pvclock_gtod_data;\n\n\twrite_seqcount_begin(&vdata->seq);\n\n\t/* copy pvclock gtod data */\n\tvdata->clock.vclock_mode\t= tk->tkr_mono.clock->vdso_clock_mode;\n\tvdata->clock.cycle_last\t\t= tk->tkr_mono.cycle_last;\n\tvdata->clock.mask\t\t= tk->tkr_mono.mask;\n\tvdata->clock.mult\t\t= tk->tkr_mono.mult;\n\tvdata->clock.shift\t\t= tk->tkr_mono.shift;\n\tvdata->clock.base_cycles\t= tk->tkr_mono.xtime_nsec;\n\tvdata->clock.offset\t\t= tk->tkr_mono.base;\n\n\tvdata->raw_clock.vclock_mode\t= tk->tkr_raw.clock->vdso_clock_mode;\n\tvdata->raw_clock.cycle_last\t= tk->tkr_raw.cycle_last;\n\tvdata->raw_clock.mask\t\t= tk->tkr_raw.mask;\n\tvdata->raw_clock.mult\t\t= tk->tkr_raw.mult;\n\tvdata->raw_clock.shift\t\t= tk->tkr_raw.shift;\n\tvdata->raw_clock.base_cycles\t= tk->tkr_raw.xtime_nsec;\n\tvdata->raw_clock.offset\t\t= tk->tkr_raw.base;\n\n\tvdata->wall_time_sec            = tk->xtime_sec;\n\n\tvdata->offs_boot\t\t= tk->offs_boot;\n\n\twrite_seqcount_end(&vdata->seq);\n}\n\nstatic s64 get_kvmclock_base_ns(void)\n{\n\t/* Count up from boot time, but with the frequency of the raw clock.  */\n\treturn ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));\n}\n#else\nstatic s64 get_kvmclock_base_ns(void)\n{\n\t/* Master clock not used, so we can just use CLOCK_BOOTTIME.  */\n\treturn ktime_get_boottime_ns();\n}\n#endif\n\nstatic void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)\n{\n\tint version;\n\tint r;\n\tstruct pvclock_wall_clock wc;\n\tu32 wc_sec_hi;\n\tu64 wall_nsec;\n\n\tif (!wall_clock)\n\t\treturn;\n\n\tr = kvm_read_guest(kvm, wall_clock, &version, sizeof(version));\n\tif (r)\n\t\treturn;\n\n\tif (version & 1)\n\t\t++version;  /* first time write, random junk */\n\n\t++version;\n\n\tif (kvm_write_guest(kvm, wall_clock, &version, sizeof(version)))\n\t\treturn;\n\n\t/*\n\t * The guest calculates current wall clock time by adding\n\t * system time (updated by kvm_guest_time_update below) to the\n\t * wall clock specified here.  We do the reverse here.\n\t */\n\twall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);\n\n\twc.nsec = do_div(wall_nsec, 1000000000);\n\twc.sec = (u32)wall_nsec; /* overflow in 2106 guest time */\n\twc.version = version;\n\n\tkvm_write_guest(kvm, wall_clock, &wc, sizeof(wc));\n\n\tif (sec_hi_ofs) {\n\t\twc_sec_hi = wall_nsec >> 32;\n\t\tkvm_write_guest(kvm, wall_clock + sec_hi_ofs,\n\t\t\t\t&wc_sec_hi, sizeof(wc_sec_hi));\n\t}\n\n\tversion++;\n\tkvm_write_guest(kvm, wall_clock, &version, sizeof(version));\n}\n\nstatic void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,\n\t\t\t\t  bool old_msr, bool host_initiated)\n{\n\tstruct kvm_arch *ka = &vcpu->kvm->arch;\n\n\tif (vcpu->vcpu_id == 0 && !host_initiated) {\n\t\tif (ka->boot_vcpu_runs_old_kvmclock != old_msr)\n\t\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\n\t\tka->boot_vcpu_runs_old_kvmclock = old_msr;\n\t}\n\n\tvcpu->arch.time = system_time;\n\tkvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);\n\n\t/* we verify if the enable bit is set... */\n\tif (system_time & 1)\n\t\tkvm_gpc_activate(&vcpu->arch.pv_time, system_time & ~1ULL,\n\t\t\t\t sizeof(struct pvclock_vcpu_time_info));\n\telse\n\t\tkvm_gpc_deactivate(&vcpu->arch.pv_time);\n\n\treturn;\n}\n\nstatic uint32_t div_frac(uint32_t dividend, uint32_t divisor)\n{\n\tdo_shl32_div32(dividend, divisor);\n\treturn dividend;\n}\n\nstatic void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,\n\t\t\t       s8 *pshift, u32 *pmultiplier)\n{\n\tuint64_t scaled64;\n\tint32_t  shift = 0;\n\tuint64_t tps64;\n\tuint32_t tps32;\n\n\ttps64 = base_hz;\n\tscaled64 = scaled_hz;\n\twhile (tps64 > scaled64*2 || tps64 & 0xffffffff00000000ULL) {\n\t\ttps64 >>= 1;\n\t\tshift--;\n\t}\n\n\ttps32 = (uint32_t)tps64;\n\twhile (tps32 <= scaled64 || scaled64 & 0xffffffff00000000ULL) {\n\t\tif (scaled64 & 0xffffffff00000000ULL || tps32 & 0x80000000)\n\t\t\tscaled64 >>= 1;\n\t\telse\n\t\t\ttps32 <<= 1;\n\t\tshift++;\n\t}\n\n\t*pshift = shift;\n\t*pmultiplier = div_frac(scaled64, tps32);\n}\n\n#ifdef CONFIG_X86_64\nstatic atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);\n#endif\n\nstatic DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);\nstatic unsigned long max_tsc_khz;\n\nstatic u32 adjust_tsc_khz(u32 khz, s32 ppm)\n{\n\tu64 v = (u64)khz * (1000000 + ppm);\n\tdo_div(v, 1000000);\n\treturn v;\n}\n\nstatic void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier);\n\nstatic int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)\n{\n\tu64 ratio;\n\n\t/* Guest TSC same frequency as host TSC? */\n\tif (!scale) {\n\t\tkvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);\n\t\treturn 0;\n\t}\n\n\t/* TSC scaling supported? */\n\tif (!kvm_caps.has_tsc_control) {\n\t\tif (user_tsc_khz > tsc_khz) {\n\t\t\tvcpu->arch.tsc_catchup = 1;\n\t\t\tvcpu->arch.tsc_always_catchup = 1;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tpr_warn_ratelimited(\"user requested TSC rate below hardware speed\\n\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\t/* TSC scaling required  - calculate ratio */\n\tratio = mul_u64_u32_div(1ULL << kvm_caps.tsc_scaling_ratio_frac_bits,\n\t\t\t\tuser_tsc_khz, tsc_khz);\n\n\tif (ratio == 0 || ratio >= kvm_caps.max_tsc_scaling_ratio) {\n\t\tpr_warn_ratelimited(\"Invalid TSC scaling ratio - virtual-tsc-khz=%u\\n\",\n\t\t\t            user_tsc_khz);\n\t\treturn -1;\n\t}\n\n\tkvm_vcpu_write_tsc_multiplier(vcpu, ratio);\n\treturn 0;\n}\n\nstatic int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)\n{\n\tu32 thresh_lo, thresh_hi;\n\tint use_scaling = 0;\n\n\t/* tsc_khz can be zero if TSC calibration fails */\n\tif (user_tsc_khz == 0) {\n\t\t/* set tsc_scaling_ratio to a safe value */\n\t\tkvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);\n\t\treturn -1;\n\t}\n\n\t/* Compute a scale to convert nanoseconds in TSC cycles */\n\tkvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,\n\t\t\t   &vcpu->arch.virtual_tsc_shift,\n\t\t\t   &vcpu->arch.virtual_tsc_mult);\n\tvcpu->arch.virtual_tsc_khz = user_tsc_khz;\n\n\t/*\n\t * Compute the variation in TSC rate which is acceptable\n\t * within the range of tolerance and decide if the\n\t * rate being applied is within that bounds of the hardware\n\t * rate.  If so, no scaling or compensation need be done.\n\t */\n\tthresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);\n\tthresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);\n\tif (user_tsc_khz < thresh_lo || user_tsc_khz > thresh_hi) {\n\t\tpr_debug(\"kvm: requested TSC rate %u falls outside tolerance [%u,%u]\\n\", user_tsc_khz, thresh_lo, thresh_hi);\n\t\tuse_scaling = 1;\n\t}\n\treturn set_tsc_khz(vcpu, user_tsc_khz, use_scaling);\n}\n\nstatic u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)\n{\n\tu64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,\n\t\t\t\t      vcpu->arch.virtual_tsc_mult,\n\t\t\t\t      vcpu->arch.virtual_tsc_shift);\n\ttsc += vcpu->arch.this_tsc_write;\n\treturn tsc;\n}\n\n#ifdef CONFIG_X86_64\nstatic inline int gtod_is_based_on_tsc(int mode)\n{\n\treturn mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;\n}\n#endif\n\nstatic void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)\n{\n#ifdef CONFIG_X86_64\n\tbool vcpus_matched;\n\tstruct kvm_arch *ka = &vcpu->kvm->arch;\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\n\tvcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==\n\t\t\t atomic_read(&vcpu->kvm->online_vcpus));\n\n\t/*\n\t * Once the masterclock is enabled, always perform request in\n\t * order to update it.\n\t *\n\t * In order to enable masterclock, the host clocksource must be TSC\n\t * and the vcpus need to have matched TSCs.  When that happens,\n\t * perform request to enable masterclock.\n\t */\n\tif (ka->use_master_clock ||\n\t    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))\n\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\n\ttrace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,\n\t\t\t    atomic_read(&vcpu->kvm->online_vcpus),\n\t\t            ka->use_master_clock, gtod->clock.vclock_mode);\n#endif\n}\n\n/*\n * Multiply tsc by a fixed point number represented by ratio.\n *\n * The most significant 64-N bits (mult) of ratio represent the\n * integral part of the fixed point number; the remaining N bits\n * (frac) represent the fractional part, ie. ratio represents a fixed\n * point number (mult + frac * 2^(-N)).\n *\n * N equals to kvm_caps.tsc_scaling_ratio_frac_bits.\n */\nstatic inline u64 __scale_tsc(u64 ratio, u64 tsc)\n{\n\treturn mul_u64_u64_shr(tsc, ratio, kvm_caps.tsc_scaling_ratio_frac_bits);\n}\n\nu64 kvm_scale_tsc(u64 tsc, u64 ratio)\n{\n\tu64 _tsc = tsc;\n\n\tif (ratio != kvm_caps.default_tsc_scaling_ratio)\n\t\t_tsc = __scale_tsc(ratio, tsc);\n\n\treturn _tsc;\n}\n\nstatic u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)\n{\n\tu64 tsc;\n\n\ttsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio);\n\n\treturn target_tsc - tsc;\n}\n\nu64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)\n{\n\treturn vcpu->arch.l1_tsc_offset +\n\t\tkvm_scale_tsc(host_tsc, vcpu->arch.l1_tsc_scaling_ratio);\n}\nEXPORT_SYMBOL_GPL(kvm_read_l1_tsc);\n\nu64 kvm_calc_nested_tsc_offset(u64 l1_offset, u64 l2_offset, u64 l2_multiplier)\n{\n\tu64 nested_offset;\n\n\tif (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)\n\t\tnested_offset = l1_offset;\n\telse\n\t\tnested_offset = mul_s64_u64_shr((s64) l1_offset, l2_multiplier,\n\t\t\t\t\t\tkvm_caps.tsc_scaling_ratio_frac_bits);\n\n\tnested_offset += l2_offset;\n\treturn nested_offset;\n}\nEXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_offset);\n\nu64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)\n{\n\tif (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)\n\t\treturn mul_u64_u64_shr(l1_multiplier, l2_multiplier,\n\t\t\t\t       kvm_caps.tsc_scaling_ratio_frac_bits);\n\n\treturn l1_multiplier;\n}\nEXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_multiplier);\n\nstatic void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)\n{\n\ttrace_kvm_write_tsc_offset(vcpu->vcpu_id,\n\t\t\t\t   vcpu->arch.l1_tsc_offset,\n\t\t\t\t   l1_offset);\n\n\tvcpu->arch.l1_tsc_offset = l1_offset;\n\n\t/*\n\t * If we are here because L1 chose not to trap WRMSR to TSC then\n\t * according to the spec this should set L1's TSC (as opposed to\n\t * setting L1's offset for L2).\n\t */\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(\n\t\t\tl1_offset,\n\t\t\tstatic_call(kvm_x86_get_l2_tsc_offset)(vcpu),\n\t\t\tstatic_call(kvm_x86_get_l2_tsc_multiplier)(vcpu));\n\telse\n\t\tvcpu->arch.tsc_offset = l1_offset;\n\n\tstatic_call(kvm_x86_write_tsc_offset)(vcpu, vcpu->arch.tsc_offset);\n}\n\nstatic void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier)\n{\n\tvcpu->arch.l1_tsc_scaling_ratio = l1_multiplier;\n\n\t/* Userspace is changing the multiplier while L2 is active */\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.tsc_scaling_ratio = kvm_calc_nested_tsc_multiplier(\n\t\t\tl1_multiplier,\n\t\t\tstatic_call(kvm_x86_get_l2_tsc_multiplier)(vcpu));\n\telse\n\t\tvcpu->arch.tsc_scaling_ratio = l1_multiplier;\n\n\tif (kvm_caps.has_tsc_control)\n\t\tstatic_call(kvm_x86_write_tsc_multiplier)(\n\t\t\tvcpu, vcpu->arch.tsc_scaling_ratio);\n}\n\nstatic inline bool kvm_check_tsc_unstable(void)\n{\n#ifdef CONFIG_X86_64\n\t/*\n\t * TSC is marked unstable when we're running on Hyper-V,\n\t * 'TSC page' clocksource is good.\n\t */\n\tif (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)\n\t\treturn false;\n#endif\n\treturn check_tsc_unstable();\n}\n\n/*\n * Infers attempts to synchronize the guest's tsc from host writes. Sets the\n * offset for the vcpu and tracks the TSC matching generation that the vcpu\n * participates in.\n */\nstatic void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,\n\t\t\t\t  u64 ns, bool matched)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\n\tlockdep_assert_held(&kvm->arch.tsc_write_lock);\n\n\t/*\n\t * We also track th most recent recorded KHZ, write and time to\n\t * allow the matching interval to be extended at each write.\n\t */\n\tkvm->arch.last_tsc_nsec = ns;\n\tkvm->arch.last_tsc_write = tsc;\n\tkvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;\n\tkvm->arch.last_tsc_offset = offset;\n\n\tvcpu->arch.last_guest_tsc = tsc;\n\n\tkvm_vcpu_write_tsc_offset(vcpu, offset);\n\n\tif (!matched) {\n\t\t/*\n\t\t * We split periods of matched TSC writes into generations.\n\t\t * For each generation, we track the original measured\n\t\t * nanosecond time, offset, and write, so if TSCs are in\n\t\t * sync, we can match exact offset, and if not, we can match\n\t\t * exact software computation in compute_guest_tsc()\n\t\t *\n\t\t * These values are tracked in kvm->arch.cur_xxx variables.\n\t\t */\n\t\tkvm->arch.cur_tsc_generation++;\n\t\tkvm->arch.cur_tsc_nsec = ns;\n\t\tkvm->arch.cur_tsc_write = tsc;\n\t\tkvm->arch.cur_tsc_offset = offset;\n\t\tkvm->arch.nr_vcpus_matched_tsc = 0;\n\t} else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {\n\t\tkvm->arch.nr_vcpus_matched_tsc++;\n\t}\n\n\t/* Keep track of which generation this VCPU has synchronized to */\n\tvcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;\n\tvcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;\n\tvcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;\n\n\tkvm_track_tsc_matching(vcpu);\n}\n\nstatic void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\tu64 offset, ns, elapsed;\n\tunsigned long flags;\n\tbool matched = false;\n\tbool synchronizing = false;\n\n\traw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);\n\toffset = kvm_compute_l1_tsc_offset(vcpu, data);\n\tns = get_kvmclock_base_ns();\n\telapsed = ns - kvm->arch.last_tsc_nsec;\n\n\tif (vcpu->arch.virtual_tsc_khz) {\n\t\tif (data == 0) {\n\t\t\t/*\n\t\t\t * detection of vcpu initialization -- need to sync\n\t\t\t * with other vCPUs. This particularly helps to keep\n\t\t\t * kvm_clock stable after CPU hotplug\n\t\t\t */\n\t\t\tsynchronizing = true;\n\t\t} else {\n\t\t\tu64 tsc_exp = kvm->arch.last_tsc_write +\n\t\t\t\t\t\tnsec_to_cycles(vcpu, elapsed);\n\t\t\tu64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;\n\t\t\t/*\n\t\t\t * Special case: TSC write with a small delta (1 second)\n\t\t\t * of virtual cycle time against real time is\n\t\t\t * interpreted as an attempt to synchronize the CPU.\n\t\t\t */\n\t\t\tsynchronizing = data < tsc_exp + tsc_hz &&\n\t\t\t\t\tdata + tsc_hz > tsc_exp;\n\t\t}\n\t}\n\n\t/*\n\t * For a reliable TSC, we can match TSC offsets, and for an unstable\n\t * TSC, we add elapsed time in this computation.  We could let the\n\t * compensation code attempt to catch up if we fall behind, but\n\t * it's better to try to match offsets from the beginning.\n         */\n\tif (synchronizing &&\n\t    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {\n\t\tif (!kvm_check_tsc_unstable()) {\n\t\t\toffset = kvm->arch.cur_tsc_offset;\n\t\t} else {\n\t\t\tu64 delta = nsec_to_cycles(vcpu, elapsed);\n\t\t\tdata += delta;\n\t\t\toffset = kvm_compute_l1_tsc_offset(vcpu, data);\n\t\t}\n\t\tmatched = true;\n\t}\n\n\t__kvm_synchronize_tsc(vcpu, offset, data, ns, matched);\n\traw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);\n}\n\nstatic inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   s64 adjustment)\n{\n\tu64 tsc_offset = vcpu->arch.l1_tsc_offset;\n\tkvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);\n}\n\nstatic inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)\n{\n\tif (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)\n\t\tWARN_ON(adjustment < 0);\n\tadjustment = kvm_scale_tsc((u64) adjustment,\n\t\t\t\t   vcpu->arch.l1_tsc_scaling_ratio);\n\tadjust_tsc_offset_guest(vcpu, adjustment);\n}\n\n#ifdef CONFIG_X86_64\n\nstatic u64 read_tsc(void)\n{\n\tu64 ret = (u64)rdtsc_ordered();\n\tu64 last = pvclock_gtod_data.clock.cycle_last;\n\n\tif (likely(ret >= last))\n\t\treturn ret;\n\n\t/*\n\t * GCC likes to generate cmov here, but this branch is extremely\n\t * predictable (it's just a function of time and the likely is\n\t * very likely) and there's a data dependence, so force GCC\n\t * to generate a branch instead.  I don't barrier() because\n\t * we don't actually need a barrier, and if this function\n\t * ever gets inlined it will generate worse code.\n\t */\n\tasm volatile (\"\");\n\treturn last;\n}\n\nstatic inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,\n\t\t\t  int *mode)\n{\n\tlong v;\n\tu64 tsc_pg_val;\n\n\tswitch (clock->vclock_mode) {\n\tcase VDSO_CLOCKMODE_HVCLOCK:\n\t\ttsc_pg_val = hv_read_tsc_page_tsc(hv_get_tsc_page(),\n\t\t\t\t\t\t  tsc_timestamp);\n\t\tif (tsc_pg_val != U64_MAX) {\n\t\t\t/* TSC page valid */\n\t\t\t*mode = VDSO_CLOCKMODE_HVCLOCK;\n\t\t\tv = (tsc_pg_val - clock->cycle_last) &\n\t\t\t\tclock->mask;\n\t\t} else {\n\t\t\t/* TSC page invalid */\n\t\t\t*mode = VDSO_CLOCKMODE_NONE;\n\t\t}\n\t\tbreak;\n\tcase VDSO_CLOCKMODE_TSC:\n\t\t*mode = VDSO_CLOCKMODE_TSC;\n\t\t*tsc_timestamp = read_tsc();\n\t\tv = (*tsc_timestamp - clock->cycle_last) &\n\t\t\tclock->mask;\n\t\tbreak;\n\tdefault:\n\t\t*mode = VDSO_CLOCKMODE_NONE;\n\t}\n\n\tif (*mode == VDSO_CLOCKMODE_NONE)\n\t\t*tsc_timestamp = v = 0;\n\n\treturn v * clock->mult;\n}\n\nstatic int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)\n{\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\tunsigned long seq;\n\tint mode;\n\tu64 ns;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&gtod->seq);\n\t\tns = gtod->raw_clock.base_cycles;\n\t\tns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);\n\t\tns >>= gtod->raw_clock.shift;\n\t\tns += ktime_to_ns(ktime_add(gtod->raw_clock.offset, gtod->offs_boot));\n\t} while (unlikely(read_seqcount_retry(&gtod->seq, seq)));\n\t*t = ns;\n\n\treturn mode;\n}\n\nstatic int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)\n{\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\tunsigned long seq;\n\tint mode;\n\tu64 ns;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&gtod->seq);\n\t\tts->tv_sec = gtod->wall_time_sec;\n\t\tns = gtod->clock.base_cycles;\n\t\tns += vgettsc(&gtod->clock, tsc_timestamp, &mode);\n\t\tns >>= gtod->clock.shift;\n\t} while (unlikely(read_seqcount_retry(&gtod->seq, seq)));\n\n\tts->tv_sec += __iter_div_u64_rem(ns, NSEC_PER_SEC, &ns);\n\tts->tv_nsec = ns;\n\n\treturn mode;\n}\n\n/* returns true if host is using TSC based clocksource */\nstatic bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)\n{\n\t/* checked again under seqlock below */\n\tif (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))\n\t\treturn false;\n\n\treturn gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,\n\t\t\t\t\t\t      tsc_timestamp));\n}\n\n/* returns true if host is using TSC based clocksource */\nstatic bool kvm_get_walltime_and_clockread(struct timespec64 *ts,\n\t\t\t\t\t   u64 *tsc_timestamp)\n{\n\t/* checked again under seqlock below */\n\tif (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))\n\t\treturn false;\n\n\treturn gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));\n}\n#endif\n\n/*\n *\n * Assuming a stable TSC across physical CPUS, and a stable TSC\n * across virtual CPUs, the following condition is possible.\n * Each numbered line represents an event visible to both\n * CPUs at the next numbered event.\n *\n * \"timespecX\" represents host monotonic time. \"tscX\" represents\n * RDTSC value.\n *\n * \t\tVCPU0 on CPU0\t\t|\tVCPU1 on CPU1\n *\n * 1.  read timespec0,tsc0\n * 2.\t\t\t\t\t| timespec1 = timespec0 + N\n * \t\t\t\t\t| tsc1 = tsc0 + M\n * 3. transition to guest\t\t| transition to guest\n * 4. ret0 = timespec0 + (rdtsc - tsc0) |\n * 5.\t\t\t\t        | ret1 = timespec1 + (rdtsc - tsc1)\n * \t\t\t\t        | ret1 = timespec0 + N + (rdtsc - (tsc0 + M))\n *\n * Since ret0 update is visible to VCPU1 at time 5, to obey monotonicity:\n *\n * \t- ret0 < ret1\n *\t- timespec0 + (rdtsc - tsc0) < timespec0 + N + (rdtsc - (tsc0 + M))\n *\t\t...\n *\t- 0 < N - M => M < N\n *\n * That is, when timespec0 != timespec1, M < N. Unfortunately that is not\n * always the case (the difference between two distinct xtime instances\n * might be smaller then the difference between corresponding TSC reads,\n * when updating guest vcpus pvclock areas).\n *\n * To avoid that problem, do not allow visibility of distinct\n * system_timestamp/tsc_timestamp values simultaneously: use a master\n * copy of host monotonic time values. Update that master copy\n * in lockstep.\n *\n * Rely on synchronization of host TSCs and guest TSCs for monotonicity.\n *\n */\n\nstatic void pvclock_update_vm_gtod_copy(struct kvm *kvm)\n{\n#ifdef CONFIG_X86_64\n\tstruct kvm_arch *ka = &kvm->arch;\n\tint vclock_mode;\n\tbool host_tsc_clocksource, vcpus_matched;\n\n\tlockdep_assert_held(&kvm->arch.tsc_write_lock);\n\tvcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==\n\t\t\tatomic_read(&kvm->online_vcpus));\n\n\t/*\n\t * If the host uses TSC clock, then passthrough TSC as stable\n\t * to the guest.\n\t */\n\thost_tsc_clocksource = kvm_get_time_and_clockread(\n\t\t\t\t\t&ka->master_kernel_ns,\n\t\t\t\t\t&ka->master_cycle_now);\n\n\tka->use_master_clock = host_tsc_clocksource && vcpus_matched\n\t\t\t\t&& !ka->backwards_tsc_observed\n\t\t\t\t&& !ka->boot_vcpu_runs_old_kvmclock;\n\n\tif (ka->use_master_clock)\n\t\tatomic_set(&kvm_guest_has_master_clock, 1);\n\n\tvclock_mode = pvclock_gtod_data.clock.vclock_mode;\n\ttrace_kvm_update_master_clock(ka->use_master_clock, vclock_mode,\n\t\t\t\t\tvcpus_matched);\n#endif\n}\n\nstatic void kvm_make_mclock_inprogress_request(struct kvm *kvm)\n{\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);\n}\n\nstatic void __kvm_start_pvclock_update(struct kvm *kvm)\n{\n\traw_spin_lock_irq(&kvm->arch.tsc_write_lock);\n\twrite_seqcount_begin(&kvm->arch.pvclock_sc);\n}\n\nstatic void kvm_start_pvclock_update(struct kvm *kvm)\n{\n\tkvm_make_mclock_inprogress_request(kvm);\n\n\t/* no guest entries from this point */\n\t__kvm_start_pvclock_update(kvm);\n}\n\nstatic void kvm_end_pvclock_update(struct kvm *kvm)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\n\twrite_seqcount_end(&ka->pvclock_sc);\n\traw_spin_unlock_irq(&ka->tsc_write_lock);\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\n\t/* guest entries allowed */\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);\n}\n\nstatic void kvm_update_masterclock(struct kvm *kvm)\n{\n\tkvm_hv_request_tsc_page_update(kvm);\n\tkvm_start_pvclock_update(kvm);\n\tpvclock_update_vm_gtod_copy(kvm);\n\tkvm_end_pvclock_update(kvm);\n}\n\n/*\n * Use the kernel's tsc_khz directly if the TSC is constant, otherwise use KVM's\n * per-CPU value (which may be zero if a CPU is going offline).  Note, tsc_khz\n * can change during boot even if the TSC is constant, as it's possible for KVM\n * to be loaded before TSC calibration completes.  Ideally, KVM would get a\n * notification when calibration completes, but practically speaking calibration\n * will complete before userspace is alive enough to create VMs.\n */\nstatic unsigned long get_cpu_tsc_khz(void)\n{\n\tif (static_cpu_has(X86_FEATURE_CONSTANT_TSC))\n\t\treturn tsc_khz;\n\telse\n\t\treturn __this_cpu_read(cpu_tsc_khz);\n}\n\n/* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */\nstatic void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\tstruct pvclock_vcpu_time_info hv_clock;\n\n\t/* both __this_cpu_read() and rdtsc() should be on the same cpu */\n\tget_cpu();\n\n\tdata->flags = 0;\n\tif (ka->use_master_clock &&\n\t    (static_cpu_has(X86_FEATURE_CONSTANT_TSC) || __this_cpu_read(cpu_tsc_khz))) {\n#ifdef CONFIG_X86_64\n\t\tstruct timespec64 ts;\n\n\t\tif (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {\n\t\t\tdata->realtime = ts.tv_nsec + NSEC_PER_SEC * ts.tv_sec;\n\t\t\tdata->flags |= KVM_CLOCK_REALTIME | KVM_CLOCK_HOST_TSC;\n\t\t} else\n#endif\n\t\tdata->host_tsc = rdtsc();\n\n\t\tdata->flags |= KVM_CLOCK_TSC_STABLE;\n\t\thv_clock.tsc_timestamp = ka->master_cycle_now;\n\t\thv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;\n\t\tkvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,\n\t\t\t\t   &hv_clock.tsc_shift,\n\t\t\t\t   &hv_clock.tsc_to_system_mul);\n\t\tdata->clock = __pvclock_read_cycles(&hv_clock, data->host_tsc);\n\t} else {\n\t\tdata->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;\n\t}\n\n\tput_cpu();\n}\n\nstatic void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\tunsigned seq;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&ka->pvclock_sc);\n\t\t__get_kvmclock(kvm, data);\n\t} while (read_seqcount_retry(&ka->pvclock_sc, seq));\n}\n\nu64 get_kvmclock_ns(struct kvm *kvm)\n{\n\tstruct kvm_clock_data data;\n\n\tget_kvmclock(kvm, &data);\n\treturn data.clock;\n}\n\nstatic void kvm_setup_guest_pvclock(struct kvm_vcpu *v,\n\t\t\t\t    struct gfn_to_pfn_cache *gpc,\n\t\t\t\t    unsigned int offset)\n{\n\tstruct kvm_vcpu_arch *vcpu = &v->arch;\n\tstruct pvclock_vcpu_time_info *guest_hv_clock;\n\tunsigned long flags;\n\n\tread_lock_irqsave(&gpc->lock, flags);\n\twhile (!kvm_gpc_check(gpc, offset + sizeof(*guest_hv_clock))) {\n\t\tread_unlock_irqrestore(&gpc->lock, flags);\n\n\t\tif (kvm_gpc_refresh(gpc, offset + sizeof(*guest_hv_clock)))\n\t\t\treturn;\n\n\t\tread_lock_irqsave(&gpc->lock, flags);\n\t}\n\n\tguest_hv_clock = (void *)(gpc->khva + offset);\n\n\t/*\n\t * This VCPU is paused, but it's legal for a guest to read another\n\t * VCPU's kvmclock, so we really have to follow the specification where\n\t * it says that version is odd if data is being modified, and even after\n\t * it is consistent.\n\t */\n\n\tguest_hv_clock->version = vcpu->hv_clock.version = (guest_hv_clock->version + 1) | 1;\n\tsmp_wmb();\n\n\t/* retain PVCLOCK_GUEST_STOPPED if set in guest copy */\n\tvcpu->hv_clock.flags |= (guest_hv_clock->flags & PVCLOCK_GUEST_STOPPED);\n\n\tif (vcpu->pvclock_set_guest_stopped_request) {\n\t\tvcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;\n\t\tvcpu->pvclock_set_guest_stopped_request = false;\n\t}\n\n\tmemcpy(guest_hv_clock, &vcpu->hv_clock, sizeof(*guest_hv_clock));\n\tsmp_wmb();\n\n\tguest_hv_clock->version = ++vcpu->hv_clock.version;\n\n\tmark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);\n\tread_unlock_irqrestore(&gpc->lock, flags);\n\n\ttrace_kvm_pvclock_update(v->vcpu_id, &vcpu->hv_clock);\n}\n\nstatic int kvm_guest_time_update(struct kvm_vcpu *v)\n{\n\tunsigned long flags, tgt_tsc_khz;\n\tunsigned seq;\n\tstruct kvm_vcpu_arch *vcpu = &v->arch;\n\tstruct kvm_arch *ka = &v->kvm->arch;\n\ts64 kernel_ns;\n\tu64 tsc_timestamp, host_tsc;\n\tu8 pvclock_flags;\n\tbool use_master_clock;\n\n\tkernel_ns = 0;\n\thost_tsc = 0;\n\n\t/*\n\t * If the host uses TSC clock, then passthrough TSC as stable\n\t * to the guest.\n\t */\n\tdo {\n\t\tseq = read_seqcount_begin(&ka->pvclock_sc);\n\t\tuse_master_clock = ka->use_master_clock;\n\t\tif (use_master_clock) {\n\t\t\thost_tsc = ka->master_cycle_now;\n\t\t\tkernel_ns = ka->master_kernel_ns;\n\t\t}\n\t} while (read_seqcount_retry(&ka->pvclock_sc, seq));\n\n\t/* Keep irq disabled to prevent changes to the clock */\n\tlocal_irq_save(flags);\n\ttgt_tsc_khz = get_cpu_tsc_khz();\n\tif (unlikely(tgt_tsc_khz == 0)) {\n\t\tlocal_irq_restore(flags);\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, v);\n\t\treturn 1;\n\t}\n\tif (!use_master_clock) {\n\t\thost_tsc = rdtsc();\n\t\tkernel_ns = get_kvmclock_base_ns();\n\t}\n\n\ttsc_timestamp = kvm_read_l1_tsc(v, host_tsc);\n\n\t/*\n\t * We may have to catch up the TSC to match elapsed wall clock\n\t * time for two reasons, even if kvmclock is used.\n\t *   1) CPU could have been running below the maximum TSC rate\n\t *   2) Broken TSC compensation resets the base at each VCPU\n\t *      entry to avoid unknown leaps of TSC even when running\n\t *      again on the same CPU.  This may cause apparent elapsed\n\t *      time to disappear, and the guest to stand still or run\n\t *\tvery slowly.\n\t */\n\tif (vcpu->tsc_catchup) {\n\t\tu64 tsc = compute_guest_tsc(v, kernel_ns);\n\t\tif (tsc > tsc_timestamp) {\n\t\t\tadjust_tsc_offset_guest(v, tsc - tsc_timestamp);\n\t\t\ttsc_timestamp = tsc;\n\t\t}\n\t}\n\n\tlocal_irq_restore(flags);\n\n\t/* With all the info we got, fill in the values */\n\n\tif (kvm_caps.has_tsc_control)\n\t\ttgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz,\n\t\t\t\t\t    v->arch.l1_tsc_scaling_ratio);\n\n\tif (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {\n\t\tkvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,\n\t\t\t\t   &vcpu->hv_clock.tsc_shift,\n\t\t\t\t   &vcpu->hv_clock.tsc_to_system_mul);\n\t\tvcpu->hw_tsc_khz = tgt_tsc_khz;\n\t}\n\n\tvcpu->hv_clock.tsc_timestamp = tsc_timestamp;\n\tvcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;\n\tvcpu->last_guest_tsc = tsc_timestamp;\n\n\t/* If the host uses TSC clocksource, then it is stable */\n\tpvclock_flags = 0;\n\tif (use_master_clock)\n\t\tpvclock_flags |= PVCLOCK_TSC_STABLE_BIT;\n\n\tvcpu->hv_clock.flags = pvclock_flags;\n\n\tif (vcpu->pv_time.active)\n\t\tkvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);\n\tif (vcpu->xen.vcpu_info_cache.active)\n\t\tkvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_info_cache,\n\t\t\t\t\toffsetof(struct compat_vcpu_info, time));\n\tif (vcpu->xen.vcpu_time_info_cache.active)\n\t\tkvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_time_info_cache, 0);\n\tkvm_hv_setup_tsc_page(v->kvm, &vcpu->hv_clock);\n\treturn 0;\n}\n\n/*\n * kvmclock updates which are isolated to a given vcpu, such as\n * vcpu->cpu migration, should not allow system_timestamp from\n * the rest of the vcpus to remain static. Otherwise ntp frequency\n * correction applies to one vcpu's system_timestamp but not\n * the others.\n *\n * So in those cases, request a kvmclock update for all vcpus.\n * We need to rate-limit these requests though, as they can\n * considerably slow guests that have a large number of vcpus.\n * The time for a remote vcpu to update its kvmclock is bound\n * by the delay we use to rate-limit the updates.\n */\n\n#define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)\n\nstatic void kvmclock_update_fn(struct work_struct *work)\n{\n\tunsigned long i;\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct kvm_arch *ka = container_of(dwork, struct kvm_arch,\n\t\t\t\t\t   kvmclock_update_work);\n\tstruct kvm *kvm = container_of(ka, struct kvm, arch);\n\tstruct kvm_vcpu *vcpu;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t}\n}\n\nstatic void kvm_gen_kvmclock_update(struct kvm_vcpu *v)\n{\n\tstruct kvm *kvm = v->kvm;\n\n\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, v);\n\tschedule_delayed_work(&kvm->arch.kvmclock_update_work,\n\t\t\t\t\tKVMCLOCK_UPDATE_DELAY);\n}\n\n#define KVMCLOCK_SYNC_PERIOD (300 * HZ)\n\nstatic void kvmclock_sync_fn(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct kvm_arch *ka = container_of(dwork, struct kvm_arch,\n\t\t\t\t\t   kvmclock_sync_work);\n\tstruct kvm *kvm = container_of(ka, struct kvm, arch);\n\n\tif (!kvmclock_periodic_sync)\n\t\treturn;\n\n\tschedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);\n\tschedule_delayed_work(&kvm->arch.kvmclock_sync_work,\n\t\t\t\t\tKVMCLOCK_SYNC_PERIOD);\n}\n\n/* These helpers are safe iff @msr is known to be an MCx bank MSR. */\nstatic bool is_mci_control_msr(u32 msr)\n{\n\treturn (msr & 3) == 0;\n}\nstatic bool is_mci_status_msr(u32 msr)\n{\n\treturn (msr & 3) == 1;\n}\n\n/*\n * On AMD, HWCR[McStatusWrEn] controls whether setting MCi_STATUS results in #GP.\n */\nstatic bool can_set_mci_status(struct kvm_vcpu *vcpu)\n{\n\t/* McStatusWrEn enabled? */\n\tif (guest_cpuid_is_amd_or_hygon(vcpu))\n\t\treturn !!(vcpu->arch.msr_hwcr & BIT_ULL(18));\n\n\treturn false;\n}\n\nstatic int set_msr_mce(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\tunsigned bank_num = mcg_cap & 0xff;\n\tu32 msr = msr_info->index;\n\tu64 data = msr_info->data;\n\tu32 offset, last_msr;\n\n\tswitch (msr) {\n\tcase MSR_IA32_MCG_STATUS:\n\t\tvcpu->arch.mcg_status = data;\n\t\tbreak;\n\tcase MSR_IA32_MCG_CTL:\n\t\tif (!(mcg_cap & MCG_CTL_P) &&\n\t\t    (data || !msr_info->host_initiated))\n\t\t\treturn 1;\n\t\tif (data != 0 && data != ~(u64)0)\n\t\t\treturn 1;\n\t\tvcpu->arch.mcg_ctl = data;\n\t\tbreak;\n\tcase MSR_IA32_MC0_CTL2 ... MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) - 1:\n\t\tlast_msr = MSR_IA32_MCx_CTL2(bank_num) - 1;\n\t\tif (msr > last_msr)\n\t\t\treturn 1;\n\n\t\tif (!(mcg_cap & MCG_CMCI_P) && (data || !msr_info->host_initiated))\n\t\t\treturn 1;\n\t\t/* An attempt to write a 1 to a reserved bit raises #GP */\n\t\tif (data & ~(MCI_CTL2_CMCI_EN | MCI_CTL2_CMCI_THRESHOLD_MASK))\n\t\t\treturn 1;\n\t\toffset = array_index_nospec(msr - MSR_IA32_MC0_CTL2,\n\t\t\t\t\t    last_msr + 1 - MSR_IA32_MC0_CTL2);\n\t\tvcpu->arch.mci_ctl2_banks[offset] = data;\n\t\tbreak;\n\tcase MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:\n\t\tlast_msr = MSR_IA32_MCx_CTL(bank_num) - 1;\n\t\tif (msr > last_msr)\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * Only 0 or all 1s can be written to IA32_MCi_CTL, all other\n\t\t * values are architecturally undefined.  But, some Linux\n\t\t * kernels clear bit 10 in bank 4 to workaround a BIOS/GART TLB\n\t\t * issue on AMD K8s, allow bit 10 to be clear when setting all\n\t\t * other bits in order to avoid an uncaught #GP in the guest.\n\t\t *\n\t\t * UNIXWARE clears bit 0 of MC1_CTL to ignore correctable,\n\t\t * single-bit ECC data errors.\n\t\t */\n\t\tif (is_mci_control_msr(msr) &&\n\t\t    data != 0 && (data | (1 << 10) | 1) != ~(u64)0)\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * All CPUs allow writing 0 to MCi_STATUS MSRs to clear the MSR.\n\t\t * AMD-based CPUs allow non-zero values, but if and only if\n\t\t * HWCR[McStatusWrEn] is set.\n\t\t */\n\t\tif (!msr_info->host_initiated && is_mci_status_msr(msr) &&\n\t\t    data != 0 && !can_set_mci_status(vcpu))\n\t\t\treturn 1;\n\n\t\toffset = array_index_nospec(msr - MSR_IA32_MC0_CTL,\n\t\t\t\t\t    last_msr + 1 - MSR_IA32_MC0_CTL);\n\t\tvcpu->arch.mce_banks[offset] = data;\n\t\tbreak;\n\tdefault:\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline bool kvm_pv_async_pf_enabled(struct kvm_vcpu *vcpu)\n{\n\tu64 mask = KVM_ASYNC_PF_ENABLED | KVM_ASYNC_PF_DELIVERY_AS_INT;\n\n\treturn (vcpu->arch.apf.msr_en_val & mask) == mask;\n}\n\nstatic int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)\n{\n\tgpa_t gpa = data & ~0x3f;\n\n\t/* Bits 4:5 are reserved, Should be zero */\n\tif (data & 0x30)\n\t\treturn 1;\n\n\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_VMEXIT) &&\n\t    (data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT))\n\t\treturn 1;\n\n\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT) &&\n\t    (data & KVM_ASYNC_PF_DELIVERY_AS_INT))\n\t\treturn 1;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn data ? 1 : 0;\n\n\tvcpu->arch.apf.msr_en_val = data;\n\n\tif (!kvm_pv_async_pf_enabled(vcpu)) {\n\t\tkvm_clear_async_pf_completion_queue(vcpu);\n\t\tkvm_async_pf_hash_reset(vcpu);\n\t\treturn 0;\n\t}\n\n\tif (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apf.data, gpa,\n\t\t\t\t\tsizeof(u64)))\n\t\treturn 1;\n\n\tvcpu->arch.apf.send_user_only = !(data & KVM_ASYNC_PF_SEND_ALWAYS);\n\tvcpu->arch.apf.delivery_as_pf_vmexit = data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;\n\n\tkvm_async_pf_wakeup_all(vcpu);\n\n\treturn 0;\n}\n\nstatic int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)\n{\n\t/* Bits 8-63 are reserved */\n\tif (data >> 8)\n\t\treturn 1;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 1;\n\n\tvcpu->arch.apf.msr_int_val = data;\n\n\tvcpu->arch.apf.vec = data & KVM_ASYNC_PF_VEC_MASK;\n\n\treturn 0;\n}\n\nstatic void kvmclock_reset(struct kvm_vcpu *vcpu)\n{\n\tkvm_gpc_deactivate(&vcpu->arch.pv_time);\n\tvcpu->arch.time = 0;\n}\n\nstatic void kvm_vcpu_flush_tlb_all(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.tlb_flush;\n\tstatic_call(kvm_x86_flush_tlb_all)(vcpu);\n\n\t/* Flushing all ASIDs flushes the current ASID... */\n\tkvm_clear_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n}\n\nstatic void kvm_vcpu_flush_tlb_guest(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.tlb_flush;\n\n\tif (!tdp_enabled) {\n\t\t/*\n\t\t * A TLB flush on behalf of the guest is equivalent to\n\t\t * INVPCID(all), toggling CR4.PGE, etc., which requires\n\t\t * a forced sync of the shadow page tables.  Ensure all the\n\t\t * roots are synced and the guest TLB in hardware is clean.\n\t\t */\n\t\tkvm_mmu_sync_roots(vcpu);\n\t\tkvm_mmu_sync_prev_roots(vcpu);\n\t}\n\n\tstatic_call(kvm_x86_flush_tlb_guest)(vcpu);\n\n\t/*\n\t * Flushing all \"guest\" TLB is always a superset of Hyper-V's fine\n\t * grained flushing.\n\t */\n\tkvm_hv_vcpu_purge_flush_tlb(vcpu);\n}\n\n\nstatic inline void kvm_vcpu_flush_tlb_current(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.tlb_flush;\n\tstatic_call(kvm_x86_flush_tlb_current)(vcpu);\n}\n\n/*\n * Service \"local\" TLB flush requests, which are specific to the current MMU\n * context.  In addition to the generic event handling in vcpu_enter_guest(),\n * TLB flushes that are targeted at an MMU context also need to be serviced\n * prior before nested VM-Enter/VM-Exit.\n */\nvoid kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_check_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu))\n\t\tkvm_vcpu_flush_tlb_current(vcpu);\n\n\tif (kvm_check_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu))\n\t\tkvm_vcpu_flush_tlb_guest(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_service_local_tlb_flush_requests);\n\nstatic void record_steal_time(struct kvm_vcpu *vcpu)\n{\n\tstruct gfn_to_hva_cache *ghc = &vcpu->arch.st.cache;\n\tstruct kvm_steal_time __user *st;\n\tstruct kvm_memslots *slots;\n\tgpa_t gpa = vcpu->arch.st.msr_val & KVM_STEAL_VALID_BITS;\n\tu64 steal;\n\tu32 version;\n\n\tif (kvm_xen_msr_enabled(vcpu->kvm)) {\n\t\tkvm_xen_runstate_set_running(vcpu);\n\t\treturn;\n\t}\n\n\tif (!(vcpu->arch.st.msr_val & KVM_MSR_ENABLED))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(current->mm != vcpu->kvm->mm))\n\t\treturn;\n\n\tslots = kvm_memslots(vcpu->kvm);\n\n\tif (unlikely(slots->generation != ghc->generation ||\n\t\t     gpa != ghc->gpa ||\n\t\t     kvm_is_error_hva(ghc->hva) || !ghc->memslot)) {\n\t\t/* We rely on the fact that it fits in a single page. */\n\t\tBUILD_BUG_ON((sizeof(*st) - 1) & KVM_STEAL_VALID_BITS);\n\n\t\tif (kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, gpa, sizeof(*st)) ||\n\t\t    kvm_is_error_hva(ghc->hva) || !ghc->memslot)\n\t\t\treturn;\n\t}\n\n\tst = (struct kvm_steal_time __user *)ghc->hva;\n\t/*\n\t * Doing a TLB flush here, on the guest's behalf, can avoid\n\t * expensive IPIs.\n\t */\n\tif (guest_pv_has(vcpu, KVM_FEATURE_PV_TLB_FLUSH)) {\n\t\tu8 st_preempted = 0;\n\t\tint err = -EFAULT;\n\n\t\tif (!user_access_begin(st, sizeof(*st)))\n\t\t\treturn;\n\n\t\tasm volatile(\"1: xchgb %0, %2\\n\"\n\t\t\t     \"xor %1, %1\\n\"\n\t\t\t     \"2:\\n\"\n\t\t\t     _ASM_EXTABLE_UA(1b, 2b)\n\t\t\t     : \"+q\" (st_preempted),\n\t\t\t       \"+&r\" (err),\n\t\t\t       \"+m\" (st->preempted));\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tuser_access_end();\n\n\t\tvcpu->arch.st.preempted = 0;\n\n\t\ttrace_kvm_pv_tlb_flush(vcpu->vcpu_id,\n\t\t\t\t       st_preempted & KVM_VCPU_FLUSH_TLB);\n\t\tif (st_preempted & KVM_VCPU_FLUSH_TLB)\n\t\t\tkvm_vcpu_flush_tlb_guest(vcpu);\n\n\t\tif (!user_access_begin(st, sizeof(*st)))\n\t\t\tgoto dirty;\n\t} else {\n\t\tif (!user_access_begin(st, sizeof(*st)))\n\t\t\treturn;\n\n\t\tunsafe_put_user(0, &st->preempted, out);\n\t\tvcpu->arch.st.preempted = 0;\n\t}\n\n\tunsafe_get_user(version, &st->version, out);\n\tif (version & 1)\n\t\tversion += 1;  /* first time write, random junk */\n\n\tversion += 1;\n\tunsafe_put_user(version, &st->version, out);\n\n\tsmp_wmb();\n\n\tunsafe_get_user(steal, &st->steal, out);\n\tsteal += current->sched_info.run_delay -\n\t\tvcpu->arch.st.last_steal;\n\tvcpu->arch.st.last_steal = current->sched_info.run_delay;\n\tunsafe_put_user(steal, &st->steal, out);\n\n\tversion += 1;\n\tunsafe_put_user(version, &st->version, out);\n\n out:\n\tuser_access_end();\n dirty:\n\tmark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));\n}\n\nint kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tbool pr = false;\n\tu32 msr = msr_info->index;\n\tu64 data = msr_info->data;\n\n\tif (msr && msr == vcpu->kvm->arch.xen_hvm_config.msr)\n\t\treturn kvm_xen_write_hypercall_page(vcpu, data);\n\n\tswitch (msr) {\n\tcase MSR_AMD64_NB_CFG:\n\tcase MSR_IA32_UCODE_WRITE:\n\tcase MSR_VM_HSAVE_PA:\n\tcase MSR_AMD64_PATCH_LOADER:\n\tcase MSR_AMD64_BU_CFG2:\n\tcase MSR_AMD64_DC_CFG:\n\tcase MSR_F15H_EX_CFG:\n\t\tbreak;\n\n\tcase MSR_IA32_UCODE_REV:\n\t\tif (msr_info->host_initiated)\n\t\t\tvcpu->arch.microcode_version = data;\n\t\tbreak;\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvcpu->arch.arch_capabilities = data;\n\t\tbreak;\n\tcase MSR_IA32_PERF_CAPABILITIES:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tif (data & ~kvm_caps.supported_perf_cap)\n\t\t\treturn 1;\n\n\t\tvcpu->arch.perf_capabilities = data;\n\t\tkvm_pmu_refresh(vcpu);\n\t\treturn 0;\n\tcase MSR_EFER:\n\t\treturn set_efer(vcpu, msr_info);\n\tcase MSR_K7_HWCR:\n\t\tdata &= ~(u64)0x40;\t/* ignore flush filter disable */\n\t\tdata &= ~(u64)0x100;\t/* ignore ignne emulation enable */\n\t\tdata &= ~(u64)0x8;\t/* ignore TLB cache disable */\n\n\t\t/* Handle McStatusWrEn */\n\t\tif (data == BIT_ULL(18)) {\n\t\t\tvcpu->arch.msr_hwcr = data;\n\t\t} else if (data != 0) {\n\t\t\tvcpu_unimpl(vcpu, \"unimplemented HWCR wrmsr: 0x%llx\\n\",\n\t\t\t\t    data);\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\tcase MSR_FAM10H_MMIO_CONF_BASE:\n\t\tif (data != 0) {\n\t\t\tvcpu_unimpl(vcpu, \"unimplemented MMIO_CONF_BASE wrmsr: \"\n\t\t\t\t    \"0x%llx\\n\", data);\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\tcase 0x200 ... MSR_IA32_MC0_CTL2 - 1:\n\tcase MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) ... 0x2ff:\n\t\treturn kvm_mtrr_set_msr(vcpu, msr, data);\n\tcase MSR_IA32_APICBASE:\n\t\treturn kvm_set_apic_base(vcpu, msr_info);\n\tcase APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:\n\t\treturn kvm_x2apic_msr_write(vcpu, msr, data);\n\tcase MSR_IA32_TSC_DEADLINE:\n\t\tkvm_set_lapic_tscdeadline_msr(vcpu, data);\n\t\tbreak;\n\tcase MSR_IA32_TSC_ADJUST:\n\t\tif (guest_cpuid_has(vcpu, X86_FEATURE_TSC_ADJUST)) {\n\t\t\tif (!msr_info->host_initiated) {\n\t\t\t\ts64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;\n\t\t\t\tadjust_tsc_offset_guest(vcpu, adj);\n\t\t\t\t/* Before back to guest, tsc_timestamp must be adjusted\n\t\t\t\t * as well, otherwise guest's percpu pvclock time could jump.\n\t\t\t\t */\n\t\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\t\t}\n\t\t\tvcpu->arch.ia32_tsc_adjust_msr = data;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_MISC_ENABLE: {\n\t\tu64 old_val = vcpu->arch.ia32_misc_enable_msr;\n\n\t\tif (!msr_info->host_initiated) {\n\t\t\t/* RO bits */\n\t\t\tif ((old_val ^ data) & MSR_IA32_MISC_ENABLE_PMU_RO_MASK)\n\t\t\t\treturn 1;\n\n\t\t\t/* R bits, i.e. writes are ignored, but don't fault. */\n\t\t\tdata = data & ~MSR_IA32_MISC_ENABLE_EMON;\n\t\t\tdata |= old_val & MSR_IA32_MISC_ENABLE_EMON;\n\t\t}\n\n\t\tif (!kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_MISC_ENABLE_NO_MWAIT) &&\n\t\t    ((old_val ^ data)  & MSR_IA32_MISC_ENABLE_MWAIT)) {\n\t\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_XMM3))\n\t\t\t\treturn 1;\n\t\t\tvcpu->arch.ia32_misc_enable_msr = data;\n\t\t\tkvm_update_cpuid_runtime(vcpu);\n\t\t} else {\n\t\t\tvcpu->arch.ia32_misc_enable_msr = data;\n\t\t}\n\t\tbreak;\n\t}\n\tcase MSR_IA32_SMBASE:\n\t\tif (!IS_ENABLED(CONFIG_KVM_SMM) || !msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvcpu->arch.smbase = data;\n\t\tbreak;\n\tcase MSR_IA32_POWER_CTL:\n\t\tvcpu->arch.msr_ia32_power_ctl = data;\n\t\tbreak;\n\tcase MSR_IA32_TSC:\n\t\tif (msr_info->host_initiated) {\n\t\t\tkvm_synchronize_tsc(vcpu, data);\n\t\t} else {\n\t\t\tu64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;\n\t\t\tadjust_tsc_offset_guest(vcpu, adj);\n\t\t\tvcpu->arch.ia32_tsc_adjust_msr += adj;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_XSS:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XSAVES))\n\t\t\treturn 1;\n\t\t/*\n\t\t * KVM supports exposing PT to the guest, but does not support\n\t\t * IA32_XSS[bit 8]. Guests have to use RDMSR/WRMSR rather than\n\t\t * XSAVES/XRSTORS to save/restore PT MSRs.\n\t\t */\n\t\tif (data & ~kvm_caps.supported_xss)\n\t\t\treturn 1;\n\t\tvcpu->arch.ia32_xss = data;\n\t\tkvm_update_cpuid_runtime(vcpu);\n\t\tbreak;\n\tcase MSR_SMI_COUNT:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvcpu->arch.smi_count = data;\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tvcpu->kvm->arch.wall_clock = data;\n\t\tkvm_write_wall_clock(vcpu->kvm, data, 0);\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tvcpu->kvm->arch.wall_clock = data;\n\t\tkvm_write_wall_clock(vcpu->kvm, data, 0);\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tkvm_write_system_time(vcpu, data, false, msr_info->host_initiated);\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tkvm_write_system_time(vcpu, data, true,  msr_info->host_initiated);\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF))\n\t\t\treturn 1;\n\n\t\tif (kvm_pv_enable_async_pf(vcpu, data))\n\t\t\treturn 1;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_INT:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))\n\t\t\treturn 1;\n\n\t\tif (kvm_pv_enable_async_pf_int(vcpu, data))\n\t\t\treturn 1;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_ACK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))\n\t\t\treturn 1;\n\t\tif (data & 0x1) {\n\t\t\tvcpu->arch.apf.pageready_pending = false;\n\t\t\tkvm_check_async_pf_completion(vcpu);\n\t\t}\n\t\tbreak;\n\tcase MSR_KVM_STEAL_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_STEAL_TIME))\n\t\t\treturn 1;\n\n\t\tif (unlikely(!sched_info_on()))\n\t\t\treturn 1;\n\n\t\tif (data & KVM_STEAL_RESERVED_MASK)\n\t\t\treturn 1;\n\n\t\tvcpu->arch.st.msr_val = data;\n\n\t\tif (!(data & KVM_MSR_ENABLED))\n\t\t\tbreak;\n\n\t\tkvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);\n\n\t\tbreak;\n\tcase MSR_KVM_PV_EOI_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_EOI))\n\t\t\treturn 1;\n\n\t\tif (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))\n\t\t\treturn 1;\n\t\tbreak;\n\n\tcase MSR_KVM_POLL_CONTROL:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_POLL_CONTROL))\n\t\t\treturn 1;\n\n\t\t/* only enable bit supported */\n\t\tif (data & (-1ULL << 1))\n\t\t\treturn 1;\n\n\t\tvcpu->arch.msr_kvm_poll_control = data;\n\t\tbreak;\n\n\tcase MSR_IA32_MCG_CTL:\n\tcase MSR_IA32_MCG_STATUS:\n\tcase MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:\n\tcase MSR_IA32_MC0_CTL2 ... MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) - 1:\n\t\treturn set_msr_mce(vcpu, msr_info);\n\n\tcase MSR_K7_PERFCTR0 ... MSR_K7_PERFCTR3:\n\tcase MSR_P6_PERFCTR0 ... MSR_P6_PERFCTR1:\n\t\tpr = true;\n\t\tfallthrough;\n\tcase MSR_K7_EVNTSEL0 ... MSR_K7_EVNTSEL3:\n\tcase MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr))\n\t\t\treturn kvm_pmu_set_msr(vcpu, msr_info);\n\n\t\tif (pr || data != 0)\n\t\t\tvcpu_unimpl(vcpu, \"disabled perfctr wrmsr: \"\n\t\t\t\t    \"0x%x data 0x%llx\\n\", msr, data);\n\t\tbreak;\n\tcase MSR_K7_CLK_CTL:\n\t\t/*\n\t\t * Ignore all writes to this no longer documented MSR.\n\t\t * Writes are only relevant for old K7 processors,\n\t\t * all pre-dating SVM, but a recommended workaround from\n\t\t * AMD for these chips. It is possible to specify the\n\t\t * affected processor models on the command line, hence\n\t\t * the need to ignore the workaround.\n\t\t */\n\t\tbreak;\n\tcase HV_X64_MSR_GUEST_OS_ID ... HV_X64_MSR_SINT15:\n\tcase HV_X64_MSR_SYNDBG_CONTROL ... HV_X64_MSR_SYNDBG_PENDING_BUFFER:\n\tcase HV_X64_MSR_SYNDBG_OPTIONS:\n\tcase HV_X64_MSR_CRASH_P0 ... HV_X64_MSR_CRASH_P4:\n\tcase HV_X64_MSR_CRASH_CTL:\n\tcase HV_X64_MSR_STIMER0_CONFIG ... HV_X64_MSR_STIMER3_COUNT:\n\tcase HV_X64_MSR_REENLIGHTENMENT_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_STATUS:\n\t\treturn kvm_hv_set_msr_common(vcpu, msr, data,\n\t\t\t\t\t     msr_info->host_initiated);\n\tcase MSR_IA32_BBL_CR_CTL3:\n\t\t/* Drop writes to this legacy MSR -- see rdmsr\n\t\t * counterpart for further detail.\n\t\t */\n\t\tif (report_ignored_msrs)\n\t\t\tvcpu_unimpl(vcpu, \"ignored wrmsr: 0x%x data 0x%llx\\n\",\n\t\t\t\tmsr, data);\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_ID_LENGTH:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tvcpu->arch.osvw.length = data;\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_STATUS:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tvcpu->arch.osvw.status = data;\n\t\tbreak;\n\tcase MSR_PLATFORM_INFO:\n\t\tif (!msr_info->host_initiated ||\n\t\t    (!(data & MSR_PLATFORM_INFO_CPUID_FAULT) &&\n\t\t     cpuid_fault_enabled(vcpu)))\n\t\t\treturn 1;\n\t\tvcpu->arch.msr_platform_info = data;\n\t\tbreak;\n\tcase MSR_MISC_FEATURES_ENABLES:\n\t\tif (data & ~MSR_MISC_FEATURES_ENABLES_CPUID_FAULT ||\n\t\t    (data & MSR_MISC_FEATURES_ENABLES_CPUID_FAULT &&\n\t\t     !supports_cpuid_fault(vcpu)))\n\t\t\treturn 1;\n\t\tvcpu->arch.msr_misc_features_enables = data;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_IA32_XFD:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XFD))\n\t\t\treturn 1;\n\n\t\tif (data & ~kvm_guest_supported_xfd(vcpu))\n\t\t\treturn 1;\n\n\t\tfpu_update_guest_xfd(&vcpu->arch.guest_fpu, data);\n\t\tbreak;\n\tcase MSR_IA32_XFD_ERR:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XFD))\n\t\t\treturn 1;\n\n\t\tif (data & ~kvm_guest_supported_xfd(vcpu))\n\t\t\treturn 1;\n\n\t\tvcpu->arch.guest_fpu.xfd_err = data;\n\t\tbreak;\n#endif\n\tcase MSR_IA32_PEBS_ENABLE:\n\tcase MSR_IA32_DS_AREA:\n\tcase MSR_PEBS_DATA_CFG:\n\tcase MSR_F15H_PERF_CTL0 ... MSR_F15H_PERF_CTR5:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr))\n\t\t\treturn kvm_pmu_set_msr(vcpu, msr_info);\n\t\t/*\n\t\t * Userspace is allowed to write '0' to MSRs that KVM reports\n\t\t * as to-be-saved, even if an MSRs isn't fully supported.\n\t\t */\n\t\treturn !msr_info->host_initiated || data;\n\tdefault:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr))\n\t\t\treturn kvm_pmu_set_msr(vcpu, msr_info);\n\t\treturn KVM_MSR_RET_INVALID;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_msr_common);\n\nstatic int get_msr_mce(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata, bool host)\n{\n\tu64 data;\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\tunsigned bank_num = mcg_cap & 0xff;\n\tu32 offset, last_msr;\n\n\tswitch (msr) {\n\tcase MSR_IA32_P5_MC_ADDR:\n\tcase MSR_IA32_P5_MC_TYPE:\n\t\tdata = 0;\n\t\tbreak;\n\tcase MSR_IA32_MCG_CAP:\n\t\tdata = vcpu->arch.mcg_cap;\n\t\tbreak;\n\tcase MSR_IA32_MCG_CTL:\n\t\tif (!(mcg_cap & MCG_CTL_P) && !host)\n\t\t\treturn 1;\n\t\tdata = vcpu->arch.mcg_ctl;\n\t\tbreak;\n\tcase MSR_IA32_MCG_STATUS:\n\t\tdata = vcpu->arch.mcg_status;\n\t\tbreak;\n\tcase MSR_IA32_MC0_CTL2 ... MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) - 1:\n\t\tlast_msr = MSR_IA32_MCx_CTL2(bank_num) - 1;\n\t\tif (msr > last_msr)\n\t\t\treturn 1;\n\n\t\tif (!(mcg_cap & MCG_CMCI_P) && !host)\n\t\t\treturn 1;\n\t\toffset = array_index_nospec(msr - MSR_IA32_MC0_CTL2,\n\t\t\t\t\t    last_msr + 1 - MSR_IA32_MC0_CTL2);\n\t\tdata = vcpu->arch.mci_ctl2_banks[offset];\n\t\tbreak;\n\tcase MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:\n\t\tlast_msr = MSR_IA32_MCx_CTL(bank_num) - 1;\n\t\tif (msr > last_msr)\n\t\t\treturn 1;\n\n\t\toffset = array_index_nospec(msr - MSR_IA32_MC0_CTL,\n\t\t\t\t\t    last_msr + 1 - MSR_IA32_MC0_CTL);\n\t\tdata = vcpu->arch.mce_banks[offset];\n\t\tbreak;\n\tdefault:\n\t\treturn 1;\n\t}\n\t*pdata = data;\n\treturn 0;\n}\n\nint kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tswitch (msr_info->index) {\n\tcase MSR_IA32_PLATFORM_ID:\n\tcase MSR_IA32_EBL_CR_POWERON:\n\tcase MSR_IA32_LASTBRANCHFROMIP:\n\tcase MSR_IA32_LASTBRANCHTOIP:\n\tcase MSR_IA32_LASTINTFROMIP:\n\tcase MSR_IA32_LASTINTTOIP:\n\tcase MSR_AMD64_SYSCFG:\n\tcase MSR_K8_TSEG_ADDR:\n\tcase MSR_K8_TSEG_MASK:\n\tcase MSR_VM_HSAVE_PA:\n\tcase MSR_K8_INT_PENDING_MSG:\n\tcase MSR_AMD64_NB_CFG:\n\tcase MSR_FAM10H_MMIO_CONF_BASE:\n\tcase MSR_AMD64_BU_CFG2:\n\tcase MSR_IA32_PERF_CTL:\n\tcase MSR_AMD64_DC_CFG:\n\tcase MSR_F15H_EX_CFG:\n\t/*\n\t * Intel Sandy Bridge CPUs must support the RAPL (running average power\n\t * limit) MSRs. Just return 0, as we do not want to expose the host\n\t * data here. Do not conditionalize this on CPUID, as KVM does not do\n\t * so for existing CPU-specific MSRs.\n\t */\n\tcase MSR_RAPL_POWER_UNIT:\n\tcase MSR_PP0_ENERGY_STATUS:\t/* Power plane 0 (core) */\n\tcase MSR_PP1_ENERGY_STATUS:\t/* Power plane 1 (graphics uncore) */\n\tcase MSR_PKG_ENERGY_STATUS:\t/* Total package */\n\tcase MSR_DRAM_ENERGY_STATUS:\t/* DRAM controller */\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_IA32_PEBS_ENABLE:\n\tcase MSR_IA32_DS_AREA:\n\tcase MSR_PEBS_DATA_CFG:\n\tcase MSR_F15H_PERF_CTL0 ... MSR_F15H_PERF_CTR5:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr_info->index))\n\t\t\treturn kvm_pmu_get_msr(vcpu, msr_info);\n\t\t/*\n\t\t * Userspace is allowed to read MSRs that KVM reports as\n\t\t * to-be-saved, even if an MSR isn't fully supported.\n\t\t */\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_K7_EVNTSEL0 ... MSR_K7_EVNTSEL3:\n\tcase MSR_K7_PERFCTR0 ... MSR_K7_PERFCTR3:\n\tcase MSR_P6_PERFCTR0 ... MSR_P6_PERFCTR1:\n\tcase MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr_info->index))\n\t\t\treturn kvm_pmu_get_msr(vcpu, msr_info);\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_IA32_UCODE_REV:\n\t\tmsr_info->data = vcpu->arch.microcode_version;\n\t\tbreak;\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_ARCH_CAPABILITIES))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.arch_capabilities;\n\t\tbreak;\n\tcase MSR_IA32_PERF_CAPABILITIES:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_PDCM))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.perf_capabilities;\n\t\tbreak;\n\tcase MSR_IA32_POWER_CTL:\n\t\tmsr_info->data = vcpu->arch.msr_ia32_power_ctl;\n\t\tbreak;\n\tcase MSR_IA32_TSC: {\n\t\t/*\n\t\t * Intel SDM states that MSR_IA32_TSC read adds the TSC offset\n\t\t * even when not intercepted. AMD manual doesn't explicitly\n\t\t * state this but appears to behave the same.\n\t\t *\n\t\t * On userspace reads and writes, however, we unconditionally\n\t\t * return L1's TSC value to ensure backwards-compatible\n\t\t * behavior for migration.\n\t\t */\n\t\tu64 offset, ratio;\n\n\t\tif (msr_info->host_initiated) {\n\t\t\toffset = vcpu->arch.l1_tsc_offset;\n\t\t\tratio = vcpu->arch.l1_tsc_scaling_ratio;\n\t\t} else {\n\t\t\toffset = vcpu->arch.tsc_offset;\n\t\t\tratio = vcpu->arch.tsc_scaling_ratio;\n\t\t}\n\n\t\tmsr_info->data = kvm_scale_tsc(rdtsc(), ratio) + offset;\n\t\tbreak;\n\t}\n\tcase MSR_MTRRcap:\n\tcase 0x200 ... MSR_IA32_MC0_CTL2 - 1:\n\tcase MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) ... 0x2ff:\n\t\treturn kvm_mtrr_get_msr(vcpu, msr_info->index, &msr_info->data);\n\tcase 0xcd: /* fsb frequency */\n\t\tmsr_info->data = 3;\n\t\tbreak;\n\t\t/*\n\t\t * MSR_EBC_FREQUENCY_ID\n\t\t * Conservative value valid for even the basic CPU models.\n\t\t * Models 0,1: 000 in bits 23:21 indicating a bus speed of\n\t\t * 100MHz, model 2 000 in bits 18:16 indicating 100MHz,\n\t\t * and 266MHz for model 3, or 4. Set Core Clock\n\t\t * Frequency to System Bus Frequency Ratio to 1 (bits\n\t\t * 31:24) even though these are only valid for CPU\n\t\t * models > 2, however guests may end up dividing or\n\t\t * multiplying by zero otherwise.\n\t\t */\n\tcase MSR_EBC_FREQUENCY_ID:\n\t\tmsr_info->data = 1 << 24;\n\t\tbreak;\n\tcase MSR_IA32_APICBASE:\n\t\tmsr_info->data = kvm_get_apic_base(vcpu);\n\t\tbreak;\n\tcase APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:\n\t\treturn kvm_x2apic_msr_read(vcpu, msr_info->index, &msr_info->data);\n\tcase MSR_IA32_TSC_DEADLINE:\n\t\tmsr_info->data = kvm_get_lapic_tscdeadline_msr(vcpu);\n\t\tbreak;\n\tcase MSR_IA32_TSC_ADJUST:\n\t\tmsr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;\n\t\tbreak;\n\tcase MSR_IA32_MISC_ENABLE:\n\t\tmsr_info->data = vcpu->arch.ia32_misc_enable_msr;\n\t\tbreak;\n\tcase MSR_IA32_SMBASE:\n\t\tif (!IS_ENABLED(CONFIG_KVM_SMM) || !msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.smbase;\n\t\tbreak;\n\tcase MSR_SMI_COUNT:\n\t\tmsr_info->data = vcpu->arch.smi_count;\n\t\tbreak;\n\tcase MSR_IA32_PERF_STATUS:\n\t\t/* TSC increment by tick */\n\t\tmsr_info->data = 1000ULL;\n\t\t/* CPU multiplier */\n\t\tmsr_info->data |= (((uint64_t)4ULL) << 40);\n\t\tbreak;\n\tcase MSR_EFER:\n\t\tmsr_info->data = vcpu->arch.efer;\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->kvm->arch.wall_clock;\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->kvm->arch.wall_clock;\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.time;\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.time;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.apf.msr_en_val;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_INT:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.apf.msr_int_val;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_ACK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_KVM_STEAL_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_STEAL_TIME))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.st.msr_val;\n\t\tbreak;\n\tcase MSR_KVM_PV_EOI_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_EOI))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.pv_eoi.msr_val;\n\t\tbreak;\n\tcase MSR_KVM_POLL_CONTROL:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_POLL_CONTROL))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.msr_kvm_poll_control;\n\t\tbreak;\n\tcase MSR_IA32_P5_MC_ADDR:\n\tcase MSR_IA32_P5_MC_TYPE:\n\tcase MSR_IA32_MCG_CAP:\n\tcase MSR_IA32_MCG_CTL:\n\tcase MSR_IA32_MCG_STATUS:\n\tcase MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:\n\tcase MSR_IA32_MC0_CTL2 ... MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) - 1:\n\t\treturn get_msr_mce(vcpu, msr_info->index, &msr_info->data,\n\t\t\t\t   msr_info->host_initiated);\n\tcase MSR_IA32_XSS:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XSAVES))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.ia32_xss;\n\t\tbreak;\n\tcase MSR_K7_CLK_CTL:\n\t\t/*\n\t\t * Provide expected ramp-up count for K7. All other\n\t\t * are set to zero, indicating minimum divisors for\n\t\t * every field.\n\t\t *\n\t\t * This prevents guest kernels on AMD host with CPU\n\t\t * type 6, model 8 and higher from exploding due to\n\t\t * the rdmsr failing.\n\t\t */\n\t\tmsr_info->data = 0x20000000;\n\t\tbreak;\n\tcase HV_X64_MSR_GUEST_OS_ID ... HV_X64_MSR_SINT15:\n\tcase HV_X64_MSR_SYNDBG_CONTROL ... HV_X64_MSR_SYNDBG_PENDING_BUFFER:\n\tcase HV_X64_MSR_SYNDBG_OPTIONS:\n\tcase HV_X64_MSR_CRASH_P0 ... HV_X64_MSR_CRASH_P4:\n\tcase HV_X64_MSR_CRASH_CTL:\n\tcase HV_X64_MSR_STIMER0_CONFIG ... HV_X64_MSR_STIMER3_COUNT:\n\tcase HV_X64_MSR_REENLIGHTENMENT_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_STATUS:\n\t\treturn kvm_hv_get_msr_common(vcpu,\n\t\t\t\t\t     msr_info->index, &msr_info->data,\n\t\t\t\t\t     msr_info->host_initiated);\n\tcase MSR_IA32_BBL_CR_CTL3:\n\t\t/* This legacy MSR exists but isn't fully documented in current\n\t\t * silicon.  It is however accessed by winxp in very narrow\n\t\t * scenarios where it sets bit #19, itself documented as\n\t\t * a \"reserved\" bit.  Best effort attempt to source coherent\n\t\t * read data here should the balance of the register be\n\t\t * interpreted by the guest:\n\t\t *\n\t\t * L2 cache control register 3: 64GB range, 256KB size,\n\t\t * enabled, latency 0x1, configured\n\t\t */\n\t\tmsr_info->data = 0xbe702111;\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_ID_LENGTH:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.osvw.length;\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_STATUS:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.osvw.status;\n\t\tbreak;\n\tcase MSR_PLATFORM_INFO:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !vcpu->kvm->arch.guest_can_read_msr_platform_info)\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.msr_platform_info;\n\t\tbreak;\n\tcase MSR_MISC_FEATURES_ENABLES:\n\t\tmsr_info->data = vcpu->arch.msr_misc_features_enables;\n\t\tbreak;\n\tcase MSR_K7_HWCR:\n\t\tmsr_info->data = vcpu->arch.msr_hwcr;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_IA32_XFD:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XFD))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.guest_fpu.fpstate->xfd;\n\t\tbreak;\n\tcase MSR_IA32_XFD_ERR:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XFD))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.guest_fpu.xfd_err;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr_info->index))\n\t\t\treturn kvm_pmu_get_msr(vcpu, msr_info);\n\t\treturn KVM_MSR_RET_INVALID;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_get_msr_common);\n\n/*\n * Read or write a bunch of msrs. All parameters are kernel addresses.\n *\n * @return number of msrs set successfully.\n */\nstatic int __msr_io(struct kvm_vcpu *vcpu, struct kvm_msrs *msrs,\n\t\t    struct kvm_msr_entry *entries,\n\t\t    int (*do_msr)(struct kvm_vcpu *vcpu,\n\t\t\t\t  unsigned index, u64 *data))\n{\n\tint i;\n\n\tfor (i = 0; i < msrs->nmsrs; ++i)\n\t\tif (do_msr(vcpu, entries[i].index, &entries[i].data))\n\t\t\tbreak;\n\n\treturn i;\n}\n\n/*\n * Read or write a bunch of msrs. Parameters are user addresses.\n *\n * @return number of msrs set successfully.\n */\nstatic int msr_io(struct kvm_vcpu *vcpu, struct kvm_msrs __user *user_msrs,\n\t\t  int (*do_msr)(struct kvm_vcpu *vcpu,\n\t\t\t\tunsigned index, u64 *data),\n\t\t  int writeback)\n{\n\tstruct kvm_msrs msrs;\n\tstruct kvm_msr_entry *entries;\n\tint r, n;\n\tunsigned size;\n\n\tr = -EFAULT;\n\tif (copy_from_user(&msrs, user_msrs, sizeof(msrs)))\n\t\tgoto out;\n\n\tr = -E2BIG;\n\tif (msrs.nmsrs >= MAX_IO_MSRS)\n\t\tgoto out;\n\n\tsize = sizeof(struct kvm_msr_entry) * msrs.nmsrs;\n\tentries = memdup_user(user_msrs->entries, size);\n\tif (IS_ERR(entries)) {\n\t\tr = PTR_ERR(entries);\n\t\tgoto out;\n\t}\n\n\tr = n = __msr_io(vcpu, &msrs, entries, do_msr);\n\tif (r < 0)\n\t\tgoto out_free;\n\n\tr = -EFAULT;\n\tif (writeback && copy_to_user(user_msrs->entries, entries, size))\n\t\tgoto out_free;\n\n\tr = n;\n\nout_free:\n\tkfree(entries);\nout:\n\treturn r;\n}\n\nstatic inline bool kvm_can_mwait_in_guest(void)\n{\n\treturn boot_cpu_has(X86_FEATURE_MWAIT) &&\n\t\t!boot_cpu_has_bug(X86_BUG_MONITOR) &&\n\t\tboot_cpu_has(X86_FEATURE_ARAT);\n}\n\nstatic int kvm_ioctl_get_supported_hv_cpuid(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    struct kvm_cpuid2 __user *cpuid_arg)\n{\n\tstruct kvm_cpuid2 cpuid;\n\tint r;\n\n\tr = -EFAULT;\n\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\treturn r;\n\n\tr = kvm_get_hv_cpuid(vcpu, &cpuid, cpuid_arg->entries);\n\tif (r)\n\t\treturn r;\n\n\tr = -EFAULT;\n\tif (copy_to_user(cpuid_arg, &cpuid, sizeof(cpuid)))\n\t\treturn r;\n\n\treturn 0;\n}\n\nint kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)\n{\n\tint r = 0;\n\n\tswitch (ext) {\n\tcase KVM_CAP_IRQCHIP:\n\tcase KVM_CAP_HLT:\n\tcase KVM_CAP_MMU_SHADOW_CACHE_CONTROL:\n\tcase KVM_CAP_SET_TSS_ADDR:\n\tcase KVM_CAP_EXT_CPUID:\n\tcase KVM_CAP_EXT_EMUL_CPUID:\n\tcase KVM_CAP_CLOCKSOURCE:\n\tcase KVM_CAP_PIT:\n\tcase KVM_CAP_NOP_IO_DELAY:\n\tcase KVM_CAP_MP_STATE:\n\tcase KVM_CAP_SYNC_MMU:\n\tcase KVM_CAP_USER_NMI:\n\tcase KVM_CAP_REINJECT_CONTROL:\n\tcase KVM_CAP_IRQ_INJECT_STATUS:\n\tcase KVM_CAP_IOEVENTFD:\n\tcase KVM_CAP_IOEVENTFD_NO_LENGTH:\n\tcase KVM_CAP_PIT2:\n\tcase KVM_CAP_PIT_STATE2:\n\tcase KVM_CAP_SET_IDENTITY_MAP_ADDR:\n\tcase KVM_CAP_VCPU_EVENTS:\n\tcase KVM_CAP_HYPERV:\n\tcase KVM_CAP_HYPERV_VAPIC:\n\tcase KVM_CAP_HYPERV_SPIN:\n\tcase KVM_CAP_HYPERV_SYNIC:\n\tcase KVM_CAP_HYPERV_SYNIC2:\n\tcase KVM_CAP_HYPERV_VP_INDEX:\n\tcase KVM_CAP_HYPERV_EVENTFD:\n\tcase KVM_CAP_HYPERV_TLBFLUSH:\n\tcase KVM_CAP_HYPERV_SEND_IPI:\n\tcase KVM_CAP_HYPERV_CPUID:\n\tcase KVM_CAP_HYPERV_ENFORCE_CPUID:\n\tcase KVM_CAP_SYS_HYPERV_CPUID:\n\tcase KVM_CAP_PCI_SEGMENT:\n\tcase KVM_CAP_DEBUGREGS:\n\tcase KVM_CAP_X86_ROBUST_SINGLESTEP:\n\tcase KVM_CAP_XSAVE:\n\tcase KVM_CAP_ASYNC_PF:\n\tcase KVM_CAP_ASYNC_PF_INT:\n\tcase KVM_CAP_GET_TSC_KHZ:\n\tcase KVM_CAP_KVMCLOCK_CTRL:\n\tcase KVM_CAP_READONLY_MEM:\n\tcase KVM_CAP_HYPERV_TIME:\n\tcase KVM_CAP_IOAPIC_POLARITY_IGNORED:\n\tcase KVM_CAP_TSC_DEADLINE_TIMER:\n\tcase KVM_CAP_DISABLE_QUIRKS:\n\tcase KVM_CAP_SET_BOOT_CPU_ID:\n \tcase KVM_CAP_SPLIT_IRQCHIP:\n\tcase KVM_CAP_IMMEDIATE_EXIT:\n\tcase KVM_CAP_PMU_EVENT_FILTER:\n\tcase KVM_CAP_GET_MSR_FEATURES:\n\tcase KVM_CAP_MSR_PLATFORM_INFO:\n\tcase KVM_CAP_EXCEPTION_PAYLOAD:\n\tcase KVM_CAP_X86_TRIPLE_FAULT_EVENT:\n\tcase KVM_CAP_SET_GUEST_DEBUG:\n\tcase KVM_CAP_LAST_CPU:\n\tcase KVM_CAP_X86_USER_SPACE_MSR:\n\tcase KVM_CAP_X86_MSR_FILTER:\n\tcase KVM_CAP_ENFORCE_PV_FEATURE_CPUID:\n#ifdef CONFIG_X86_SGX_KVM\n\tcase KVM_CAP_SGX_ATTRIBUTE:\n#endif\n\tcase KVM_CAP_VM_COPY_ENC_CONTEXT_FROM:\n\tcase KVM_CAP_VM_MOVE_ENC_CONTEXT_FROM:\n\tcase KVM_CAP_SREGS2:\n\tcase KVM_CAP_EXIT_ON_EMULATION_FAILURE:\n\tcase KVM_CAP_VCPU_ATTRIBUTES:\n\tcase KVM_CAP_SYS_ATTRIBUTES:\n\tcase KVM_CAP_VAPIC:\n\tcase KVM_CAP_ENABLE_CAP:\n\tcase KVM_CAP_VM_DISABLE_NX_HUGE_PAGES:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_EXIT_HYPERCALL:\n\t\tr = KVM_EXIT_HYPERCALL_VALID_MASK;\n\t\tbreak;\n\tcase KVM_CAP_SET_GUEST_DEBUG2:\n\t\treturn KVM_GUESTDBG_VALID_MASK;\n#ifdef CONFIG_KVM_XEN\n\tcase KVM_CAP_XEN_HVM:\n\t\tr = KVM_XEN_HVM_CONFIG_HYPERCALL_MSR |\n\t\t    KVM_XEN_HVM_CONFIG_INTERCEPT_HCALL |\n\t\t    KVM_XEN_HVM_CONFIG_SHARED_INFO |\n\t\t    KVM_XEN_HVM_CONFIG_EVTCHN_2LEVEL |\n\t\t    KVM_XEN_HVM_CONFIG_EVTCHN_SEND;\n\t\tif (sched_info_on())\n\t\t\tr |= KVM_XEN_HVM_CONFIG_RUNSTATE |\n\t\t\t     KVM_XEN_HVM_CONFIG_RUNSTATE_UPDATE_FLAG;\n\t\tbreak;\n#endif\n\tcase KVM_CAP_SYNC_REGS:\n\t\tr = KVM_SYNC_X86_VALID_FIELDS;\n\t\tbreak;\n\tcase KVM_CAP_ADJUST_CLOCK:\n\t\tr = KVM_CLOCK_VALID_FLAGS;\n\t\tbreak;\n\tcase KVM_CAP_X86_DISABLE_EXITS:\n\t\tr = KVM_X86_DISABLE_EXITS_PAUSE;\n\n\t\tif (!mitigate_smt_rsb) {\n\t\t\tr |= KVM_X86_DISABLE_EXITS_HLT |\n\t\t\t     KVM_X86_DISABLE_EXITS_CSTATE;\n\n\t\t\tif (kvm_can_mwait_in_guest())\n\t\t\t\tr |= KVM_X86_DISABLE_EXITS_MWAIT;\n\t\t}\n\t\tbreak;\n\tcase KVM_CAP_X86_SMM:\n\t\tif (!IS_ENABLED(CONFIG_KVM_SMM))\n\t\t\tbreak;\n\n\t\t/* SMBASE is usually relocated above 1M on modern chipsets,\n\t\t * and SMM handlers might indeed rely on 4G segment limits,\n\t\t * so do not report SMM to be available if real mode is\n\t\t * emulated via vm86 mode.  Still, do not go to great lengths\n\t\t * to avoid userspace's usage of the feature, because it is a\n\t\t * fringe case that is not enabled except via specific settings\n\t\t * of the module parameters.\n\t\t */\n\t\tr = static_call(kvm_x86_has_emulated_msr)(kvm, MSR_IA32_SMBASE);\n\t\tbreak;\n\tcase KVM_CAP_NR_VCPUS:\n\t\tr = min_t(unsigned int, num_online_cpus(), KVM_MAX_VCPUS);\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPUS:\n\t\tr = KVM_MAX_VCPUS;\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPU_ID:\n\t\tr = KVM_MAX_VCPU_IDS;\n\t\tbreak;\n\tcase KVM_CAP_PV_MMU:\t/* obsolete */\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_MCE:\n\t\tr = KVM_MAX_MCE_BANKS;\n\t\tbreak;\n\tcase KVM_CAP_XCRS:\n\t\tr = boot_cpu_has(X86_FEATURE_XSAVE);\n\t\tbreak;\n\tcase KVM_CAP_TSC_CONTROL:\n\tcase KVM_CAP_VM_TSC_CONTROL:\n\t\tr = kvm_caps.has_tsc_control;\n\t\tbreak;\n\tcase KVM_CAP_X2APIC_API:\n\t\tr = KVM_X2APIC_API_VALID_FLAGS;\n\t\tbreak;\n\tcase KVM_CAP_NESTED_STATE:\n\t\tr = kvm_x86_ops.nested_ops->get_state ?\n\t\t\tkvm_x86_ops.nested_ops->get_state(NULL, NULL, 0) : 0;\n\t\tbreak;\n\tcase KVM_CAP_HYPERV_DIRECT_TLBFLUSH:\n\t\tr = kvm_x86_ops.enable_l2_tlb_flush != NULL;\n\t\tbreak;\n\tcase KVM_CAP_HYPERV_ENLIGHTENED_VMCS:\n\t\tr = kvm_x86_ops.nested_ops->enable_evmcs != NULL;\n\t\tbreak;\n\tcase KVM_CAP_SMALLER_MAXPHYADDR:\n\t\tr = (int) allow_smaller_maxphyaddr;\n\t\tbreak;\n\tcase KVM_CAP_STEAL_TIME:\n\t\tr = sched_info_on();\n\t\tbreak;\n\tcase KVM_CAP_X86_BUS_LOCK_EXIT:\n\t\tif (kvm_caps.has_bus_lock_exit)\n\t\t\tr = KVM_BUS_LOCK_DETECTION_OFF |\n\t\t\t    KVM_BUS_LOCK_DETECTION_EXIT;\n\t\telse\n\t\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_XSAVE2: {\n\t\tu64 guest_perm = xstate_get_guest_group_perm();\n\n\t\tr = xstate_required_size(kvm_caps.supported_xcr0 & guest_perm, false);\n\t\tif (r < sizeof(struct kvm_xsave))\n\t\t\tr = sizeof(struct kvm_xsave);\n\t\tbreak;\n\t}\n\tcase KVM_CAP_PMU_CAPABILITY:\n\t\tr = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;\n\t\tbreak;\n\tcase KVM_CAP_DISABLE_QUIRKS2:\n\t\tr = KVM_X86_VALID_QUIRKS;\n\t\tbreak;\n\tcase KVM_CAP_X86_NOTIFY_VMEXIT:\n\t\tr = kvm_caps.has_notify_vmexit;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn r;\n}\n\nstatic inline void __user *kvm_get_attr_addr(struct kvm_device_attr *attr)\n{\n\tvoid __user *uaddr = (void __user*)(unsigned long)attr->addr;\n\n\tif ((u64)(unsigned long)uaddr != attr->addr)\n\t\treturn ERR_PTR_USR(-EFAULT);\n\treturn uaddr;\n}\n\nstatic int kvm_x86_dev_get_attr(struct kvm_device_attr *attr)\n{\n\tu64 __user *uaddr = kvm_get_attr_addr(attr);\n\n\tif (attr->group)\n\t\treturn -ENXIO;\n\n\tif (IS_ERR(uaddr))\n\t\treturn PTR_ERR(uaddr);\n\n\tswitch (attr->attr) {\n\tcase KVM_X86_XCOMP_GUEST_SUPP:\n\t\tif (put_user(kvm_caps.supported_xcr0, uaddr))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENXIO;\n\t\tbreak;\n\t}\n}\n\nstatic int kvm_x86_dev_has_attr(struct kvm_device_attr *attr)\n{\n\tif (attr->group)\n\t\treturn -ENXIO;\n\n\tswitch (attr->attr) {\n\tcase KVM_X86_XCOMP_GUEST_SUPP:\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENXIO;\n\t}\n}\n\nlong kvm_arch_dev_ioctl(struct file *filp,\n\t\t\tunsigned int ioctl, unsigned long arg)\n{\n\tvoid __user *argp = (void __user *)arg;\n\tlong r;\n\n\tswitch (ioctl) {\n\tcase KVM_GET_MSR_INDEX_LIST: {\n\t\tstruct kvm_msr_list __user *user_msr_list = argp;\n\t\tstruct kvm_msr_list msr_list;\n\t\tunsigned n;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&msr_list, user_msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tn = msr_list.nmsrs;\n\t\tmsr_list.nmsrs = num_msrs_to_save + num_emulated_msrs;\n\t\tif (copy_to_user(user_msr_list, &msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tr = -E2BIG;\n\t\tif (n < msr_list.nmsrs)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(user_msr_list->indices, &msrs_to_save,\n\t\t\t\t num_msrs_to_save * sizeof(u32)))\n\t\t\tgoto out;\n\t\tif (copy_to_user(user_msr_list->indices + num_msrs_to_save,\n\t\t\t\t &emulated_msrs,\n\t\t\t\t num_emulated_msrs * sizeof(u32)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_SUPPORTED_CPUID:\n\tcase KVM_GET_EMULATED_CPUID: {\n\t\tstruct kvm_cpuid2 __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid2 cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\n\t\tr = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries,\n\t\t\t\t\t    ioctl);\n\t\tif (r)\n\t\t\tgoto out;\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(cpuid_arg, &cpuid, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_X86_GET_MCE_CAP_SUPPORTED:\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &kvm_caps.supported_mce_cap,\n\t\t\t\t sizeof(kvm_caps.supported_mce_cap)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_GET_MSR_FEATURE_INDEX_LIST: {\n\t\tstruct kvm_msr_list __user *user_msr_list = argp;\n\t\tstruct kvm_msr_list msr_list;\n\t\tunsigned int n;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&msr_list, user_msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tn = msr_list.nmsrs;\n\t\tmsr_list.nmsrs = num_msr_based_features;\n\t\tif (copy_to_user(user_msr_list, &msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tr = -E2BIG;\n\t\tif (n < msr_list.nmsrs)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(user_msr_list->indices, &msr_based_features,\n\t\t\t\t num_msr_based_features * sizeof(u32)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_MSRS:\n\t\tr = msr_io(NULL, argp, do_get_msr_feature, 1);\n\t\tbreak;\n\tcase KVM_GET_SUPPORTED_HV_CPUID:\n\t\tr = kvm_ioctl_get_supported_hv_cpuid(NULL, argp);\n\t\tbreak;\n\tcase KVM_GET_DEVICE_ATTR: {\n\t\tstruct kvm_device_attr attr;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))\n\t\t\tbreak;\n\t\tr = kvm_x86_dev_get_attr(&attr);\n\t\tbreak;\n\t}\n\tcase KVM_HAS_DEVICE_ATTR: {\n\t\tstruct kvm_device_attr attr;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))\n\t\t\tbreak;\n\t\tr = kvm_x86_dev_has_attr(&attr);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\nout:\n\treturn r;\n}\n\nstatic void wbinvd_ipi(void *garbage)\n{\n\twbinvd();\n}\n\nstatic bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_arch_has_noncoherent_dma(vcpu->kvm);\n}\n\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\t/* Address WBINVD may be executed by guest */\n\tif (need_emulate_wbinvd(vcpu)) {\n\t\tif (static_call(kvm_x86_has_wbinvd_exit)())\n\t\t\tcpumask_set_cpu(cpu, vcpu->arch.wbinvd_dirty_mask);\n\t\telse if (vcpu->cpu != -1 && vcpu->cpu != cpu)\n\t\t\tsmp_call_function_single(vcpu->cpu,\n\t\t\t\t\twbinvd_ipi, NULL, 1);\n\t}\n\n\tstatic_call(kvm_x86_vcpu_load)(vcpu, cpu);\n\n\t/* Save host pkru register if supported */\n\tvcpu->arch.host_pkru = read_pkru();\n\n\t/* Apply any externally detected TSC adjustments (due to suspend) */\n\tif (unlikely(vcpu->arch.tsc_offset_adjustment)) {\n\t\tadjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);\n\t\tvcpu->arch.tsc_offset_adjustment = 0;\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t}\n\n\tif (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {\n\t\ts64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :\n\t\t\t\trdtsc() - vcpu->arch.last_host_tsc;\n\t\tif (tsc_delta < 0)\n\t\t\tmark_tsc_unstable(\"KVM discovered backwards TSC\");\n\n\t\tif (kvm_check_tsc_unstable()) {\n\t\t\tu64 offset = kvm_compute_l1_tsc_offset(vcpu,\n\t\t\t\t\t\tvcpu->arch.last_guest_tsc);\n\t\t\tkvm_vcpu_write_tsc_offset(vcpu, offset);\n\t\t\tvcpu->arch.tsc_catchup = 1;\n\t\t}\n\n\t\tif (kvm_lapic_hv_timer_in_use(vcpu))\n\t\t\tkvm_lapic_restart_hv_timer(vcpu);\n\n\t\t/*\n\t\t * On a host with synchronized TSC, there is no need to update\n\t\t * kvmclock on vcpu->cpu migration\n\t\t */\n\t\tif (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)\n\t\t\tkvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);\n\t\tif (vcpu->cpu != cpu)\n\t\t\tkvm_make_request(KVM_REQ_MIGRATE_TIMER, vcpu);\n\t\tvcpu->cpu = cpu;\n\t}\n\n\tkvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);\n}\n\nstatic void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)\n{\n\tstruct gfn_to_hva_cache *ghc = &vcpu->arch.st.cache;\n\tstruct kvm_steal_time __user *st;\n\tstruct kvm_memslots *slots;\n\tstatic const u8 preempted = KVM_VCPU_PREEMPTED;\n\tgpa_t gpa = vcpu->arch.st.msr_val & KVM_STEAL_VALID_BITS;\n\n\t/*\n\t * The vCPU can be marked preempted if and only if the VM-Exit was on\n\t * an instruction boundary and will not trigger guest emulation of any\n\t * kind (see vcpu_run).  Vendor specific code controls (conservatively)\n\t * when this is true, for example allowing the vCPU to be marked\n\t * preempted if and only if the VM-Exit was due to a host interrupt.\n\t */\n\tif (!vcpu->arch.at_instruction_boundary) {\n\t\tvcpu->stat.preemption_other++;\n\t\treturn;\n\t}\n\n\tvcpu->stat.preemption_reported++;\n\tif (!(vcpu->arch.st.msr_val & KVM_MSR_ENABLED))\n\t\treturn;\n\n\tif (vcpu->arch.st.preempted)\n\t\treturn;\n\n\t/* This happens on process exit */\n\tif (unlikely(current->mm != vcpu->kvm->mm))\n\t\treturn;\n\n\tslots = kvm_memslots(vcpu->kvm);\n\n\tif (unlikely(slots->generation != ghc->generation ||\n\t\t     gpa != ghc->gpa ||\n\t\t     kvm_is_error_hva(ghc->hva) || !ghc->memslot))\n\t\treturn;\n\n\tst = (struct kvm_steal_time __user *)ghc->hva;\n\tBUILD_BUG_ON(sizeof(st->preempted) != sizeof(preempted));\n\n\tif (!copy_to_user_nofault(&st->preempted, &preempted, sizeof(preempted)))\n\t\tvcpu->arch.st.preempted = KVM_VCPU_PREEMPTED;\n\n\tmark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));\n}\n\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tint idx;\n\n\tif (vcpu->preempted) {\n\t\tif (!vcpu->arch.guest_state_protected)\n\t\t\tvcpu->arch.preempted_in_kernel = !static_call(kvm_x86_get_cpl)(vcpu);\n\n\t\t/*\n\t\t * Take the srcu lock as memslots will be accessed to check the gfn\n\t\t * cache generation against the memslots generation.\n\t\t */\n\t\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tif (kvm_xen_msr_enabled(vcpu->kvm))\n\t\t\tkvm_xen_runstate_set_preempted(vcpu);\n\t\telse\n\t\t\tkvm_steal_time_set_preempted(vcpu);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t}\n\n\tstatic_call(kvm_x86_vcpu_put)(vcpu);\n\tvcpu->arch.last_host_tsc = rdtsc();\n}\n\nstatic int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_lapic_state *s)\n{\n\tstatic_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);\n\n\treturn kvm_apic_get_state(vcpu, s);\n}\n\nstatic int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_lapic_state *s)\n{\n\tint r;\n\n\tr = kvm_apic_set_state(vcpu, s);\n\tif (r)\n\t\treturn r;\n\tupdate_cr8_intercept(vcpu);\n\n\treturn 0;\n}\n\nstatic int kvm_cpu_accept_dm_intr(struct kvm_vcpu *vcpu)\n{\n\t/*\n\t * We can accept userspace's request for interrupt injection\n\t * as long as we have a place to store the interrupt number.\n\t * The actual injection will happen when the CPU is able to\n\t * deliver the interrupt.\n\t */\n\tif (kvm_cpu_has_extint(vcpu))\n\t\treturn false;\n\n\t/* Acknowledging ExtINT does not happen if LINT0 is masked.  */\n\treturn (!lapic_in_kernel(vcpu) ||\n\t\tkvm_apic_accept_pic_intr(vcpu));\n}\n\nstatic int kvm_vcpu_ready_for_interrupt_injection(struct kvm_vcpu *vcpu)\n{\n\t/*\n\t * Do not cause an interrupt window exit if an exception\n\t * is pending or an event needs reinjection; userspace\n\t * might want to inject the interrupt manually using KVM_SET_REGS\n\t * or KVM_SET_SREGS.  For that to work, we must be at an\n\t * instruction boundary and with no events half-injected.\n\t */\n\treturn (kvm_arch_interrupt_allowed(vcpu) &&\n\t\tkvm_cpu_accept_dm_intr(vcpu) &&\n\t\t!kvm_event_needs_reinjection(vcpu) &&\n\t\t!kvm_is_exception_pending(vcpu));\n}\n\nstatic int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_interrupt *irq)\n{\n\tif (irq->irq >= KVM_NR_INTERRUPTS)\n\t\treturn -EINVAL;\n\n\tif (!irqchip_in_kernel(vcpu->kvm)) {\n\t\tkvm_queue_interrupt(vcpu, irq->irq, false);\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * With in-kernel LAPIC, we only use this to inject EXTINT, so\n\t * fail for in-kernel 8259.\n\t */\n\tif (pic_in_kernel(vcpu->kvm))\n\t\treturn -ENXIO;\n\n\tif (vcpu->arch.pending_external_vector != -1)\n\t\treturn -EEXIST;\n\n\tvcpu->arch.pending_external_vector = irq->irq;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_nmi(struct kvm_vcpu *vcpu)\n{\n\tkvm_inject_nmi(vcpu);\n\n\treturn 0;\n}\n\nstatic int vcpu_ioctl_tpr_access_reporting(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   struct kvm_tpr_access_ctl *tac)\n{\n\tif (tac->flags)\n\t\treturn -EINVAL;\n\tvcpu->arch.tpr_access_reporting = !!tac->enabled;\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_x86_setup_mce(struct kvm_vcpu *vcpu,\n\t\t\t\t\tu64 mcg_cap)\n{\n\tint r;\n\tunsigned bank_num = mcg_cap & 0xff, bank;\n\n\tr = -EINVAL;\n\tif (!bank_num || bank_num > KVM_MAX_MCE_BANKS)\n\t\tgoto out;\n\tif (mcg_cap & ~(kvm_caps.supported_mce_cap | 0xff | 0xff0000))\n\t\tgoto out;\n\tr = 0;\n\tvcpu->arch.mcg_cap = mcg_cap;\n\t/* Init IA32_MCG_CTL to all 1s */\n\tif (mcg_cap & MCG_CTL_P)\n\t\tvcpu->arch.mcg_ctl = ~(u64)0;\n\t/* Init IA32_MCi_CTL to all 1s, IA32_MCi_CTL2 to all 0s */\n\tfor (bank = 0; bank < bank_num; bank++) {\n\t\tvcpu->arch.mce_banks[bank*4] = ~(u64)0;\n\t\tif (mcg_cap & MCG_CMCI_P)\n\t\t\tvcpu->arch.mci_ctl2_banks[bank] = 0;\n\t}\n\n\tkvm_apic_after_set_mcg_cap(vcpu);\n\n\tstatic_call(kvm_x86_setup_mce)(vcpu);\nout:\n\treturn r;\n}\n\n/*\n * Validate this is an UCNA (uncorrectable no action) error by checking the\n * MCG_STATUS and MCi_STATUS registers:\n * - none of the bits for Machine Check Exceptions are set\n * - both the VAL (valid) and UC (uncorrectable) bits are set\n * MCI_STATUS_PCC - Processor Context Corrupted\n * MCI_STATUS_S - Signaled as a Machine Check Exception\n * MCI_STATUS_AR - Software recoverable Action Required\n */\nstatic bool is_ucna(struct kvm_x86_mce *mce)\n{\n\treturn\t!mce->mcg_status &&\n\t\t!(mce->status & (MCI_STATUS_PCC | MCI_STATUS_S | MCI_STATUS_AR)) &&\n\t\t(mce->status & MCI_STATUS_VAL) &&\n\t\t(mce->status & MCI_STATUS_UC);\n}\n\nstatic int kvm_vcpu_x86_set_ucna(struct kvm_vcpu *vcpu, struct kvm_x86_mce *mce, u64* banks)\n{\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\n\tbanks[1] = mce->status;\n\tbanks[2] = mce->addr;\n\tbanks[3] = mce->misc;\n\tvcpu->arch.mcg_status = mce->mcg_status;\n\n\tif (!(mcg_cap & MCG_CMCI_P) ||\n\t    !(vcpu->arch.mci_ctl2_banks[mce->bank] & MCI_CTL2_CMCI_EN))\n\t\treturn 0;\n\n\tif (lapic_in_kernel(vcpu))\n\t\tkvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);\n\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_mce(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct kvm_x86_mce *mce)\n{\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\tunsigned bank_num = mcg_cap & 0xff;\n\tu64 *banks = vcpu->arch.mce_banks;\n\n\tif (mce->bank >= bank_num || !(mce->status & MCI_STATUS_VAL))\n\t\treturn -EINVAL;\n\n\tbanks += array_index_nospec(4 * mce->bank, 4 * bank_num);\n\n\tif (is_ucna(mce))\n\t\treturn kvm_vcpu_x86_set_ucna(vcpu, mce, banks);\n\n\t/*\n\t * if IA32_MCG_CTL is not all 1s, the uncorrected error\n\t * reporting is disabled\n\t */\n\tif ((mce->status & MCI_STATUS_UC) && (mcg_cap & MCG_CTL_P) &&\n\t    vcpu->arch.mcg_ctl != ~(u64)0)\n\t\treturn 0;\n\t/*\n\t * if IA32_MCi_CTL is not all 1s, the uncorrected error\n\t * reporting is disabled for the bank\n\t */\n\tif ((mce->status & MCI_STATUS_UC) && banks[0] != ~(u64)0)\n\t\treturn 0;\n\tif (mce->status & MCI_STATUS_UC) {\n\t\tif ((vcpu->arch.mcg_status & MCG_STATUS_MCIP) ||\n\t\t    !kvm_read_cr4_bits(vcpu, X86_CR4_MCE)) {\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\t\treturn 0;\n\t\t}\n\t\tif (banks[1] & MCI_STATUS_VAL)\n\t\t\tmce->status |= MCI_STATUS_OVER;\n\t\tbanks[2] = mce->addr;\n\t\tbanks[3] = mce->misc;\n\t\tvcpu->arch.mcg_status = mce->mcg_status;\n\t\tbanks[1] = mce->status;\n\t\tkvm_queue_exception(vcpu, MC_VECTOR);\n\t} else if (!(banks[1] & MCI_STATUS_VAL)\n\t\t   || !(banks[1] & MCI_STATUS_UC)) {\n\t\tif (banks[1] & MCI_STATUS_VAL)\n\t\t\tmce->status |= MCI_STATUS_OVER;\n\t\tbanks[2] = mce->addr;\n\t\tbanks[3] = mce->misc;\n\t\tbanks[1] = mce->status;\n\t} else\n\t\tbanks[1] |= MCI_STATUS_OVER;\n\treturn 0;\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_vcpu_events(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       struct kvm_vcpu_events *events)\n{\n\tstruct kvm_queued_exception *ex;\n\n\tprocess_nmi(vcpu);\n\n#ifdef CONFIG_KVM_SMM\n\tif (kvm_check_request(KVM_REQ_SMI, vcpu))\n\t\tprocess_smi(vcpu);\n#endif\n\n\t/*\n\t * KVM's ABI only allows for one exception to be migrated.  Luckily,\n\t * the only time there can be two queued exceptions is if there's a\n\t * non-exiting _injected_ exception, and a pending exiting exception.\n\t * In that case, ignore the VM-Exiting exception as it's an extension\n\t * of the injected exception.\n\t */\n\tif (vcpu->arch.exception_vmexit.pending &&\n\t    !vcpu->arch.exception.pending &&\n\t    !vcpu->arch.exception.injected)\n\t\tex = &vcpu->arch.exception_vmexit;\n\telse\n\t\tex = &vcpu->arch.exception;\n\n\t/*\n\t * In guest mode, payload delivery should be deferred if the exception\n\t * will be intercepted by L1, e.g. KVM should not modifying CR2 if L1\n\t * intercepts #PF, ditto for DR6 and #DBs.  If the per-VM capability,\n\t * KVM_CAP_EXCEPTION_PAYLOAD, is not set, userspace may or may not\n\t * propagate the payload and so it cannot be safely deferred.  Deliver\n\t * the payload if the capability hasn't been requested.\n\t */\n\tif (!vcpu->kvm->arch.exception_payload_enabled &&\n\t    ex->pending && ex->has_payload)\n\t\tkvm_deliver_exception_payload(vcpu, ex);\n\n\tmemset(events, 0, sizeof(*events));\n\n\t/*\n\t * The API doesn't provide the instruction length for software\n\t * exceptions, so don't report them. As long as the guest RIP\n\t * isn't advanced, we should expect to encounter the exception\n\t * again.\n\t */\n\tif (!kvm_exception_is_soft(ex->vector)) {\n\t\tevents->exception.injected = ex->injected;\n\t\tevents->exception.pending = ex->pending;\n\t\t/*\n\t\t * For ABI compatibility, deliberately conflate\n\t\t * pending and injected exceptions when\n\t\t * KVM_CAP_EXCEPTION_PAYLOAD isn't enabled.\n\t\t */\n\t\tif (!vcpu->kvm->arch.exception_payload_enabled)\n\t\t\tevents->exception.injected |= ex->pending;\n\t}\n\tevents->exception.nr = ex->vector;\n\tevents->exception.has_error_code = ex->has_error_code;\n\tevents->exception.error_code = ex->error_code;\n\tevents->exception_has_payload = ex->has_payload;\n\tevents->exception_payload = ex->payload;\n\n\tevents->interrupt.injected =\n\t\tvcpu->arch.interrupt.injected && !vcpu->arch.interrupt.soft;\n\tevents->interrupt.nr = vcpu->arch.interrupt.nr;\n\tevents->interrupt.shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);\n\n\tevents->nmi.injected = vcpu->arch.nmi_injected;\n\tevents->nmi.pending = vcpu->arch.nmi_pending != 0;\n\tevents->nmi.masked = static_call(kvm_x86_get_nmi_mask)(vcpu);\n\n\t/* events->sipi_vector is never valid when reporting to user space */\n\n#ifdef CONFIG_KVM_SMM\n\tevents->smi.smm = is_smm(vcpu);\n\tevents->smi.pending = vcpu->arch.smi_pending;\n\tevents->smi.smm_inside_nmi =\n\t\t!!(vcpu->arch.hflags & HF_SMM_INSIDE_NMI_MASK);\n#endif\n\tevents->smi.latched_init = kvm_lapic_latched_init(vcpu);\n\n\tevents->flags = (KVM_VCPUEVENT_VALID_NMI_PENDING\n\t\t\t | KVM_VCPUEVENT_VALID_SHADOW\n\t\t\t | KVM_VCPUEVENT_VALID_SMM);\n\tif (vcpu->kvm->arch.exception_payload_enabled)\n\t\tevents->flags |= KVM_VCPUEVENT_VALID_PAYLOAD;\n\tif (vcpu->kvm->arch.triple_fault_event) {\n\t\tevents->triple_fault.pending = kvm_test_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\tevents->flags |= KVM_VCPUEVENT_VALID_TRIPLE_FAULT;\n\t}\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,\n\t\t\t\t\t      struct kvm_vcpu_events *events)\n{\n\tif (events->flags & ~(KVM_VCPUEVENT_VALID_NMI_PENDING\n\t\t\t      | KVM_VCPUEVENT_VALID_SIPI_VECTOR\n\t\t\t      | KVM_VCPUEVENT_VALID_SHADOW\n\t\t\t      | KVM_VCPUEVENT_VALID_SMM\n\t\t\t      | KVM_VCPUEVENT_VALID_PAYLOAD\n\t\t\t      | KVM_VCPUEVENT_VALID_TRIPLE_FAULT))\n\t\treturn -EINVAL;\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_PAYLOAD) {\n\t\tif (!vcpu->kvm->arch.exception_payload_enabled)\n\t\t\treturn -EINVAL;\n\t\tif (events->exception.pending)\n\t\t\tevents->exception.injected = 0;\n\t\telse\n\t\t\tevents->exception_has_payload = 0;\n\t} else {\n\t\tevents->exception.pending = 0;\n\t\tevents->exception_has_payload = 0;\n\t}\n\n\tif ((events->exception.injected || events->exception.pending) &&\n\t    (events->exception.nr > 31 || events->exception.nr == NMI_VECTOR))\n\t\treturn -EINVAL;\n\n\t/* INITs are latched while in SMM */\n\tif (events->flags & KVM_VCPUEVENT_VALID_SMM &&\n\t    (events->smi.smm || events->smi.pending) &&\n\t    vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED)\n\t\treturn -EINVAL;\n\n\tprocess_nmi(vcpu);\n\n\t/*\n\t * Flag that userspace is stuffing an exception, the next KVM_RUN will\n\t * morph the exception to a VM-Exit if appropriate.  Do this only for\n\t * pending exceptions, already-injected exceptions are not subject to\n\t * intercpetion.  Note, userspace that conflates pending and injected\n\t * is hosed, and will incorrectly convert an injected exception into a\n\t * pending exception, which in turn may cause a spurious VM-Exit.\n\t */\n\tvcpu->arch.exception_from_userspace = events->exception.pending;\n\n\tvcpu->arch.exception_vmexit.pending = false;\n\n\tvcpu->arch.exception.injected = events->exception.injected;\n\tvcpu->arch.exception.pending = events->exception.pending;\n\tvcpu->arch.exception.vector = events->exception.nr;\n\tvcpu->arch.exception.has_error_code = events->exception.has_error_code;\n\tvcpu->arch.exception.error_code = events->exception.error_code;\n\tvcpu->arch.exception.has_payload = events->exception_has_payload;\n\tvcpu->arch.exception.payload = events->exception_payload;\n\n\tvcpu->arch.interrupt.injected = events->interrupt.injected;\n\tvcpu->arch.interrupt.nr = events->interrupt.nr;\n\tvcpu->arch.interrupt.soft = events->interrupt.soft;\n\tif (events->flags & KVM_VCPUEVENT_VALID_SHADOW)\n\t\tstatic_call(kvm_x86_set_interrupt_shadow)(vcpu,\n\t\t\t\t\t\tevents->interrupt.shadow);\n\n\tvcpu->arch.nmi_injected = events->nmi.injected;\n\tif (events->flags & KVM_VCPUEVENT_VALID_NMI_PENDING)\n\t\tvcpu->arch.nmi_pending = events->nmi.pending;\n\tstatic_call(kvm_x86_set_nmi_mask)(vcpu, events->nmi.masked);\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_SIPI_VECTOR &&\n\t    lapic_in_kernel(vcpu))\n\t\tvcpu->arch.apic->sipi_vector = events->sipi_vector;\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_SMM) {\n#ifdef CONFIG_KVM_SMM\n\t\tif (!!(vcpu->arch.hflags & HF_SMM_MASK) != events->smi.smm) {\n\t\t\tkvm_leave_nested(vcpu);\n\t\t\tkvm_smm_changed(vcpu, events->smi.smm);\n\t\t}\n\n\t\tvcpu->arch.smi_pending = events->smi.pending;\n\n\t\tif (events->smi.smm) {\n\t\t\tif (events->smi.smm_inside_nmi)\n\t\t\t\tvcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;\n\t\t\telse\n\t\t\t\tvcpu->arch.hflags &= ~HF_SMM_INSIDE_NMI_MASK;\n\t\t}\n\n#else\n\t\tif (events->smi.smm || events->smi.pending ||\n\t\t    events->smi.smm_inside_nmi)\n\t\t\treturn -EINVAL;\n#endif\n\n\t\tif (lapic_in_kernel(vcpu)) {\n\t\t\tif (events->smi.latched_init)\n\t\t\t\tset_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);\n\t\t\telse\n\t\t\t\tclear_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);\n\t\t}\n\t}\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_TRIPLE_FAULT) {\n\t\tif (!vcpu->kvm->arch.triple_fault_event)\n\t\t\treturn -EINVAL;\n\t\tif (events->triple_fault.pending)\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\telse\n\t\t\tkvm_clear_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t}\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\treturn 0;\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_debugregs(struct kvm_vcpu *vcpu,\n\t\t\t\t\t     struct kvm_debugregs *dbgregs)\n{\n\tunsigned long val;\n\n\tmemcpy(dbgregs->db, vcpu->arch.db, sizeof(vcpu->arch.db));\n\tkvm_get_dr(vcpu, 6, &val);\n\tdbgregs->dr6 = val;\n\tdbgregs->dr7 = vcpu->arch.dr7;\n\tdbgregs->flags = 0;\n\tmemset(&dbgregs->reserved, 0, sizeof(dbgregs->reserved));\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_debugregs(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    struct kvm_debugregs *dbgregs)\n{\n\tif (dbgregs->flags)\n\t\treturn -EINVAL;\n\n\tif (!kvm_dr6_valid(dbgregs->dr6))\n\t\treturn -EINVAL;\n\tif (!kvm_dr7_valid(dbgregs->dr7))\n\t\treturn -EINVAL;\n\n\tmemcpy(vcpu->arch.db, dbgregs->db, sizeof(vcpu->arch.db));\n\tkvm_update_dr0123(vcpu);\n\tvcpu->arch.dr6 = dbgregs->dr6;\n\tvcpu->arch.dr7 = dbgregs->dr7;\n\tkvm_update_dr7(vcpu);\n\n\treturn 0;\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_xsave(struct kvm_vcpu *vcpu,\n\t\t\t\t\t struct kvm_xsave *guest_xsave)\n{\n\tif (fpstate_is_confidential(&vcpu->arch.guest_fpu))\n\t\treturn;\n\n\tfpu_copy_guest_fpstate_to_uabi(&vcpu->arch.guest_fpu,\n\t\t\t\t       guest_xsave->region,\n\t\t\t\t       sizeof(guest_xsave->region),\n\t\t\t\t       vcpu->arch.pkru);\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_xsave2(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  u8 *state, unsigned int size)\n{\n\tif (fpstate_is_confidential(&vcpu->arch.guest_fpu))\n\t\treturn;\n\n\tfpu_copy_guest_fpstate_to_uabi(&vcpu->arch.guest_fpu,\n\t\t\t\t       state, size, vcpu->arch.pkru);\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_xsave(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_xsave *guest_xsave)\n{\n\tif (fpstate_is_confidential(&vcpu->arch.guest_fpu))\n\t\treturn 0;\n\n\treturn fpu_copy_uabi_to_guest_fpstate(&vcpu->arch.guest_fpu,\n\t\t\t\t\t      guest_xsave->region,\n\t\t\t\t\t      kvm_caps.supported_xcr0,\n\t\t\t\t\t      &vcpu->arch.pkru);\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_xcrs(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_xcrs *guest_xcrs)\n{\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE)) {\n\t\tguest_xcrs->nr_xcrs = 0;\n\t\treturn;\n\t}\n\n\tguest_xcrs->nr_xcrs = 1;\n\tguest_xcrs->flags = 0;\n\tguest_xcrs->xcrs[0].xcr = XCR_XFEATURE_ENABLED_MASK;\n\tguest_xcrs->xcrs[0].value = vcpu->arch.xcr0;\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_xcrs(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct kvm_xcrs *guest_xcrs)\n{\n\tint i, r = 0;\n\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE))\n\t\treturn -EINVAL;\n\n\tif (guest_xcrs->nr_xcrs > KVM_MAX_XCRS || guest_xcrs->flags)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < guest_xcrs->nr_xcrs; i++)\n\t\t/* Only support XCR0 currently */\n\t\tif (guest_xcrs->xcrs[i].xcr == XCR_XFEATURE_ENABLED_MASK) {\n\t\t\tr = __kvm_set_xcr(vcpu, XCR_XFEATURE_ENABLED_MASK,\n\t\t\t\tguest_xcrs->xcrs[i].value);\n\t\t\tbreak;\n\t\t}\n\tif (r)\n\t\tr = -EINVAL;\n\treturn r;\n}\n\n/*\n * kvm_set_guest_paused() indicates to the guest kernel that it has been\n * stopped by the hypervisor.  This function will be called from the host only.\n * EINVAL is returned when the host attempts to set the flag for a guest that\n * does not support pv clocks.\n */\nstatic int kvm_set_guest_paused(struct kvm_vcpu *vcpu)\n{\n\tif (!vcpu->arch.pv_time.active)\n\t\treturn -EINVAL;\n\tvcpu->arch.pvclock_set_guest_stopped_request = true;\n\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\treturn 0;\n}\n\nstatic int kvm_arch_tsc_has_attr(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_device_attr *attr)\n{\n\tint r;\n\n\tswitch (attr->attr) {\n\tcase KVM_VCPU_TSC_OFFSET:\n\t\tr = 0;\n\t\tbreak;\n\tdefault:\n\t\tr = -ENXIO;\n\t}\n\n\treturn r;\n}\n\nstatic int kvm_arch_tsc_get_attr(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_device_attr *attr)\n{\n\tu64 __user *uaddr = kvm_get_attr_addr(attr);\n\tint r;\n\n\tif (IS_ERR(uaddr))\n\t\treturn PTR_ERR(uaddr);\n\n\tswitch (attr->attr) {\n\tcase KVM_VCPU_TSC_OFFSET:\n\t\tr = -EFAULT;\n\t\tif (put_user(vcpu->arch.l1_tsc_offset, uaddr))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\tdefault:\n\t\tr = -ENXIO;\n\t}\n\n\treturn r;\n}\n\nstatic int kvm_arch_tsc_set_attr(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_device_attr *attr)\n{\n\tu64 __user *uaddr = kvm_get_attr_addr(attr);\n\tstruct kvm *kvm = vcpu->kvm;\n\tint r;\n\n\tif (IS_ERR(uaddr))\n\t\treturn PTR_ERR(uaddr);\n\n\tswitch (attr->attr) {\n\tcase KVM_VCPU_TSC_OFFSET: {\n\t\tu64 offset, tsc, ns;\n\t\tunsigned long flags;\n\t\tbool matched;\n\n\t\tr = -EFAULT;\n\t\tif (get_user(offset, uaddr))\n\t\t\tbreak;\n\n\t\traw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);\n\n\t\tmatched = (vcpu->arch.virtual_tsc_khz &&\n\t\t\t   kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&\n\t\t\t   kvm->arch.last_tsc_offset == offset);\n\n\t\ttsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio) + offset;\n\t\tns = get_kvmclock_base_ns();\n\n\t\t__kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched);\n\t\traw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);\n\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = -ENXIO;\n\t}\n\n\treturn r;\n}\n\nstatic int kvm_vcpu_ioctl_device_attr(struct kvm_vcpu *vcpu,\n\t\t\t\t      unsigned int ioctl,\n\t\t\t\t      void __user *argp)\n{\n\tstruct kvm_device_attr attr;\n\tint r;\n\n\tif (copy_from_user(&attr, argp, sizeof(attr)))\n\t\treturn -EFAULT;\n\n\tif (attr.group != KVM_VCPU_TSC_CTRL)\n\t\treturn -ENXIO;\n\n\tswitch (ioctl) {\n\tcase KVM_HAS_DEVICE_ATTR:\n\t\tr = kvm_arch_tsc_has_attr(vcpu, &attr);\n\t\tbreak;\n\tcase KVM_GET_DEVICE_ATTR:\n\t\tr = kvm_arch_tsc_get_attr(vcpu, &attr);\n\t\tbreak;\n\tcase KVM_SET_DEVICE_ATTR:\n\t\tr = kvm_arch_tsc_set_attr(vcpu, &attr);\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nstatic int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_enable_cap *cap)\n{\n\tint r;\n\tuint16_t vmcs_version;\n\tvoid __user *user_ptr;\n\n\tif (cap->flags)\n\t\treturn -EINVAL;\n\n\tswitch (cap->cap) {\n\tcase KVM_CAP_HYPERV_SYNIC2:\n\t\tif (cap->args[0])\n\t\t\treturn -EINVAL;\n\t\tfallthrough;\n\n\tcase KVM_CAP_HYPERV_SYNIC:\n\t\tif (!irqchip_in_kernel(vcpu->kvm))\n\t\t\treturn -EINVAL;\n\t\treturn kvm_hv_activate_synic(vcpu, cap->cap ==\n\t\t\t\t\t     KVM_CAP_HYPERV_SYNIC2);\n\tcase KVM_CAP_HYPERV_ENLIGHTENED_VMCS:\n\t\tif (!kvm_x86_ops.nested_ops->enable_evmcs)\n\t\t\treturn -ENOTTY;\n\t\tr = kvm_x86_ops.nested_ops->enable_evmcs(vcpu, &vmcs_version);\n\t\tif (!r) {\n\t\t\tuser_ptr = (void __user *)(uintptr_t)cap->args[0];\n\t\t\tif (copy_to_user(user_ptr, &vmcs_version,\n\t\t\t\t\t sizeof(vmcs_version)))\n\t\t\t\tr = -EFAULT;\n\t\t}\n\t\treturn r;\n\tcase KVM_CAP_HYPERV_DIRECT_TLBFLUSH:\n\t\tif (!kvm_x86_ops.enable_l2_tlb_flush)\n\t\t\treturn -ENOTTY;\n\n\t\treturn static_call(kvm_x86_enable_l2_tlb_flush)(vcpu);\n\n\tcase KVM_CAP_HYPERV_ENFORCE_CPUID:\n\t\treturn kvm_hv_set_enforce_cpuid(vcpu, cap->args[0]);\n\n\tcase KVM_CAP_ENFORCE_PV_FEATURE_CPUID:\n\t\tvcpu->arch.pv_cpuid.enforce = cap->args[0];\n\t\tif (vcpu->arch.pv_cpuid.enforce)\n\t\t\tkvm_update_pv_runtime(vcpu);\n\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nlong kvm_arch_vcpu_ioctl(struct file *filp,\n\t\t\t unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\tunion {\n\t\tstruct kvm_sregs2 *sregs2;\n\t\tstruct kvm_lapic_state *lapic;\n\t\tstruct kvm_xsave *xsave;\n\t\tstruct kvm_xcrs *xcrs;\n\t\tvoid *buffer;\n\t} u;\n\n\tvcpu_load(vcpu);\n\n\tu.buffer = NULL;\n\tswitch (ioctl) {\n\tcase KVM_GET_LAPIC: {\n\t\tr = -EINVAL;\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tu.lapic = kzalloc(sizeof(struct kvm_lapic_state),\n\t\t\t\tGFP_KERNEL_ACCOUNT);\n\n\t\tr = -ENOMEM;\n\t\tif (!u.lapic)\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_get_lapic(vcpu, u.lapic);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.lapic, sizeof(struct kvm_lapic_state)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_LAPIC: {\n\t\tr = -EINVAL;\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tu.lapic = memdup_user(argp, sizeof(*u.lapic));\n\t\tif (IS_ERR(u.lapic)) {\n\t\t\tr = PTR_ERR(u.lapic);\n\t\t\tgoto out_nofree;\n\t\t}\n\n\t\tr = kvm_vcpu_ioctl_set_lapic(vcpu, u.lapic);\n\t\tbreak;\n\t}\n\tcase KVM_INTERRUPT: {\n\t\tstruct kvm_interrupt irq;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&irq, argp, sizeof(irq)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_interrupt(vcpu, &irq);\n\t\tbreak;\n\t}\n\tcase KVM_NMI: {\n\t\tr = kvm_vcpu_ioctl_nmi(vcpu);\n\t\tbreak;\n\t}\n\tcase KVM_SMI: {\n\t\tr = kvm_inject_smi(vcpu);\n\t\tbreak;\n\t}\n\tcase KVM_SET_CPUID: {\n\t\tstruct kvm_cpuid __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_set_cpuid(vcpu, &cpuid, cpuid_arg->entries);\n\t\tbreak;\n\t}\n\tcase KVM_SET_CPUID2: {\n\t\tstruct kvm_cpuid2 __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid2 cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_set_cpuid2(vcpu, &cpuid,\n\t\t\t\t\t      cpuid_arg->entries);\n\t\tbreak;\n\t}\n\tcase KVM_GET_CPUID2: {\n\t\tstruct kvm_cpuid2 __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid2 cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_get_cpuid2(vcpu, &cpuid,\n\t\t\t\t\t      cpuid_arg->entries);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(cpuid_arg, &cpuid, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_MSRS: {\n\t\tint idx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = msr_io(vcpu, argp, do_get_msr, 1);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_SET_MSRS: {\n\t\tint idx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = msr_io(vcpu, argp, do_set_msr, 0);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_TPR_ACCESS_REPORTING: {\n\t\tstruct kvm_tpr_access_ctl tac;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&tac, argp, sizeof(tac)))\n\t\t\tgoto out;\n\t\tr = vcpu_ioctl_tpr_access_reporting(vcpu, &tac);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &tac, sizeof(tac)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t};\n\tcase KVM_SET_VAPIC_ADDR: {\n\t\tstruct kvm_vapic_addr va;\n\t\tint idx;\n\n\t\tr = -EINVAL;\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&va, argp, sizeof(va)))\n\t\t\tgoto out;\n\t\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = kvm_lapic_set_vapic_addr(vcpu, va.vapic_addr);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_X86_SETUP_MCE: {\n\t\tu64 mcg_cap;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&mcg_cap, argp, sizeof(mcg_cap)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_x86_setup_mce(vcpu, mcg_cap);\n\t\tbreak;\n\t}\n\tcase KVM_X86_SET_MCE: {\n\t\tstruct kvm_x86_mce mce;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&mce, argp, sizeof(mce)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_x86_set_mce(vcpu, &mce);\n\t\tbreak;\n\t}\n\tcase KVM_GET_VCPU_EVENTS: {\n\t\tstruct kvm_vcpu_events events;\n\n\t\tkvm_vcpu_ioctl_x86_get_vcpu_events(vcpu, &events);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &events, sizeof(struct kvm_vcpu_events)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_VCPU_EVENTS: {\n\t\tstruct kvm_vcpu_events events;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&events, argp, sizeof(struct kvm_vcpu_events)))\n\t\t\tbreak;\n\n\t\tr = kvm_vcpu_ioctl_x86_set_vcpu_events(vcpu, &events);\n\t\tbreak;\n\t}\n\tcase KVM_GET_DEBUGREGS: {\n\t\tstruct kvm_debugregs dbgregs;\n\n\t\tkvm_vcpu_ioctl_x86_get_debugregs(vcpu, &dbgregs);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &dbgregs,\n\t\t\t\t sizeof(struct kvm_debugregs)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_DEBUGREGS: {\n\t\tstruct kvm_debugregs dbgregs;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&dbgregs, argp,\n\t\t\t\t   sizeof(struct kvm_debugregs)))\n\t\t\tbreak;\n\n\t\tr = kvm_vcpu_ioctl_x86_set_debugregs(vcpu, &dbgregs);\n\t\tbreak;\n\t}\n\tcase KVM_GET_XSAVE: {\n\t\tr = -EINVAL;\n\t\tif (vcpu->arch.guest_fpu.uabi_size > sizeof(struct kvm_xsave))\n\t\t\tbreak;\n\n\t\tu.xsave = kzalloc(sizeof(struct kvm_xsave), GFP_KERNEL_ACCOUNT);\n\t\tr = -ENOMEM;\n\t\tif (!u.xsave)\n\t\t\tbreak;\n\n\t\tkvm_vcpu_ioctl_x86_get_xsave(vcpu, u.xsave);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.xsave, sizeof(struct kvm_xsave)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_XSAVE: {\n\t\tint size = vcpu->arch.guest_fpu.uabi_size;\n\n\t\tu.xsave = memdup_user(argp, size);\n\t\tif (IS_ERR(u.xsave)) {\n\t\t\tr = PTR_ERR(u.xsave);\n\t\t\tgoto out_nofree;\n\t\t}\n\n\t\tr = kvm_vcpu_ioctl_x86_set_xsave(vcpu, u.xsave);\n\t\tbreak;\n\t}\n\n\tcase KVM_GET_XSAVE2: {\n\t\tint size = vcpu->arch.guest_fpu.uabi_size;\n\n\t\tu.xsave = kzalloc(size, GFP_KERNEL_ACCOUNT);\n\t\tr = -ENOMEM;\n\t\tif (!u.xsave)\n\t\t\tbreak;\n\n\t\tkvm_vcpu_ioctl_x86_get_xsave2(vcpu, u.buffer, size);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.xsave, size))\n\t\t\tbreak;\n\n\t\tr = 0;\n\t\tbreak;\n\t}\n\n\tcase KVM_GET_XCRS: {\n\t\tu.xcrs = kzalloc(sizeof(struct kvm_xcrs), GFP_KERNEL_ACCOUNT);\n\t\tr = -ENOMEM;\n\t\tif (!u.xcrs)\n\t\t\tbreak;\n\n\t\tkvm_vcpu_ioctl_x86_get_xcrs(vcpu, u.xcrs);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.xcrs,\n\t\t\t\t sizeof(struct kvm_xcrs)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_XCRS: {\n\t\tu.xcrs = memdup_user(argp, sizeof(*u.xcrs));\n\t\tif (IS_ERR(u.xcrs)) {\n\t\t\tr = PTR_ERR(u.xcrs);\n\t\t\tgoto out_nofree;\n\t\t}\n\n\t\tr = kvm_vcpu_ioctl_x86_set_xcrs(vcpu, u.xcrs);\n\t\tbreak;\n\t}\n\tcase KVM_SET_TSC_KHZ: {\n\t\tu32 user_tsc_khz;\n\n\t\tr = -EINVAL;\n\t\tuser_tsc_khz = (u32)arg;\n\n\t\tif (kvm_caps.has_tsc_control &&\n\t\t    user_tsc_khz >= kvm_caps.max_guest_tsc_khz)\n\t\t\tgoto out;\n\n\t\tif (user_tsc_khz == 0)\n\t\t\tuser_tsc_khz = tsc_khz;\n\n\t\tif (!kvm_set_tsc_khz(vcpu, user_tsc_khz))\n\t\t\tr = 0;\n\n\t\tgoto out;\n\t}\n\tcase KVM_GET_TSC_KHZ: {\n\t\tr = vcpu->arch.virtual_tsc_khz;\n\t\tgoto out;\n\t}\n\tcase KVM_KVMCLOCK_CTRL: {\n\t\tr = kvm_set_guest_paused(vcpu);\n\t\tgoto out;\n\t}\n\tcase KVM_ENABLE_CAP: {\n\t\tstruct kvm_enable_cap cap;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cap, argp, sizeof(cap)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);\n\t\tbreak;\n\t}\n\tcase KVM_GET_NESTED_STATE: {\n\t\tstruct kvm_nested_state __user *user_kvm_nested_state = argp;\n\t\tu32 user_data_size;\n\n\t\tr = -EINVAL;\n\t\tif (!kvm_x86_ops.nested_ops->get_state)\n\t\t\tbreak;\n\n\t\tBUILD_BUG_ON(sizeof(user_data_size) != sizeof(user_kvm_nested_state->size));\n\t\tr = -EFAULT;\n\t\tif (get_user(user_data_size, &user_kvm_nested_state->size))\n\t\t\tbreak;\n\n\t\tr = kvm_x86_ops.nested_ops->get_state(vcpu, user_kvm_nested_state,\n\t\t\t\t\t\t     user_data_size);\n\t\tif (r < 0)\n\t\t\tbreak;\n\n\t\tif (r > user_data_size) {\n\t\t\tif (put_user(r, &user_kvm_nested_state->size))\n\t\t\t\tr = -EFAULT;\n\t\t\telse\n\t\t\t\tr = -E2BIG;\n\t\t\tbreak;\n\t\t}\n\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_NESTED_STATE: {\n\t\tstruct kvm_nested_state __user *user_kvm_nested_state = argp;\n\t\tstruct kvm_nested_state kvm_state;\n\t\tint idx;\n\n\t\tr = -EINVAL;\n\t\tif (!kvm_x86_ops.nested_ops->set_state)\n\t\t\tbreak;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&kvm_state, user_kvm_nested_state, sizeof(kvm_state)))\n\t\t\tbreak;\n\n\t\tr = -EINVAL;\n\t\tif (kvm_state.size < sizeof(kvm_state))\n\t\t\tbreak;\n\n\t\tif (kvm_state.flags &\n\t\t    ~(KVM_STATE_NESTED_RUN_PENDING | KVM_STATE_NESTED_GUEST_MODE\n\t\t      | KVM_STATE_NESTED_EVMCS | KVM_STATE_NESTED_MTF_PENDING\n\t\t      | KVM_STATE_NESTED_GIF_SET))\n\t\t\tbreak;\n\n\t\t/* nested_run_pending implies guest_mode.  */\n\t\tif ((kvm_state.flags & KVM_STATE_NESTED_RUN_PENDING)\n\t\t    && !(kvm_state.flags & KVM_STATE_NESTED_GUEST_MODE))\n\t\t\tbreak;\n\n\t\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = kvm_x86_ops.nested_ops->set_state(vcpu, user_kvm_nested_state, &kvm_state);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_GET_SUPPORTED_HV_CPUID:\n\t\tr = kvm_ioctl_get_supported_hv_cpuid(vcpu, argp);\n\t\tbreak;\n#ifdef CONFIG_KVM_XEN\n\tcase KVM_XEN_VCPU_GET_ATTR: {\n\t\tstruct kvm_xen_vcpu_attr xva;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xva, argp, sizeof(xva)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_vcpu_get_attr(vcpu, &xva);\n\t\tif (!r && copy_to_user(argp, &xva, sizeof(xva)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase KVM_XEN_VCPU_SET_ATTR: {\n\t\tstruct kvm_xen_vcpu_attr xva;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xva, argp, sizeof(xva)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_vcpu_set_attr(vcpu, &xva);\n\t\tbreak;\n\t}\n#endif\n\tcase KVM_GET_SREGS2: {\n\t\tu.sregs2 = kzalloc(sizeof(struct kvm_sregs2), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!u.sregs2)\n\t\t\tgoto out;\n\t\t__get_sregs2(vcpu, u.sregs2);\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.sregs2, sizeof(struct kvm_sregs2)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_SREGS2: {\n\t\tu.sregs2 = memdup_user(argp, sizeof(struct kvm_sregs2));\n\t\tif (IS_ERR(u.sregs2)) {\n\t\t\tr = PTR_ERR(u.sregs2);\n\t\t\tu.sregs2 = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tr = __set_sregs2(vcpu, u.sregs2);\n\t\tbreak;\n\t}\n\tcase KVM_HAS_DEVICE_ATTR:\n\tcase KVM_GET_DEVICE_ATTR:\n\tcase KVM_SET_DEVICE_ATTR:\n\t\tr = kvm_vcpu_ioctl_device_attr(vcpu, ioctl, argp);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t}\nout:\n\tkfree(u.buffer);\nout_nofree:\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\nvm_fault_t kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)\n{\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic int kvm_vm_ioctl_set_tss_addr(struct kvm *kvm, unsigned long addr)\n{\n\tint ret;\n\n\tif (addr > (unsigned int)(-3 * PAGE_SIZE))\n\t\treturn -EINVAL;\n\tret = static_call(kvm_x86_set_tss_addr)(kvm, addr);\n\treturn ret;\n}\n\nstatic int kvm_vm_ioctl_set_identity_map_addr(struct kvm *kvm,\n\t\t\t\t\t      u64 ident_addr)\n{\n\treturn static_call(kvm_x86_set_identity_map_addr)(kvm, ident_addr);\n}\n\nstatic int kvm_vm_ioctl_set_nr_mmu_pages(struct kvm *kvm,\n\t\t\t\t\t unsigned long kvm_nr_mmu_pages)\n{\n\tif (kvm_nr_mmu_pages < KVM_MIN_ALLOC_MMU_PAGES)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tkvm_mmu_change_mmu_pages(kvm, kvm_nr_mmu_pages);\n\tkvm->arch.n_requested_mmu_pages = kvm_nr_mmu_pages;\n\n\tmutex_unlock(&kvm->slots_lock);\n\treturn 0;\n}\n\nstatic unsigned long kvm_vm_ioctl_get_nr_mmu_pages(struct kvm *kvm)\n{\n\treturn kvm->arch.n_max_mmu_pages;\n}\n\nstatic int kvm_vm_ioctl_get_irqchip(struct kvm *kvm, struct kvm_irqchip *chip)\n{\n\tstruct kvm_pic *pic = kvm->arch.vpic;\n\tint r;\n\n\tr = 0;\n\tswitch (chip->chip_id) {\n\tcase KVM_IRQCHIP_PIC_MASTER:\n\t\tmemcpy(&chip->chip.pic, &pic->pics[0],\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tbreak;\n\tcase KVM_IRQCHIP_PIC_SLAVE:\n\t\tmemcpy(&chip->chip.pic, &pic->pics[1],\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tbreak;\n\tcase KVM_IRQCHIP_IOAPIC:\n\t\tkvm_get_ioapic(kvm, &chip->chip.ioapic);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn r;\n}\n\nstatic int kvm_vm_ioctl_set_irqchip(struct kvm *kvm, struct kvm_irqchip *chip)\n{\n\tstruct kvm_pic *pic = kvm->arch.vpic;\n\tint r;\n\n\tr = 0;\n\tswitch (chip->chip_id) {\n\tcase KVM_IRQCHIP_PIC_MASTER:\n\t\tspin_lock(&pic->lock);\n\t\tmemcpy(&pic->pics[0], &chip->chip.pic,\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tspin_unlock(&pic->lock);\n\t\tbreak;\n\tcase KVM_IRQCHIP_PIC_SLAVE:\n\t\tspin_lock(&pic->lock);\n\t\tmemcpy(&pic->pics[1], &chip->chip.pic,\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tspin_unlock(&pic->lock);\n\t\tbreak;\n\tcase KVM_IRQCHIP_IOAPIC:\n\t\tkvm_set_ioapic(kvm, &chip->chip.ioapic);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\tkvm_pic_update_irq(pic);\n\treturn r;\n}\n\nstatic int kvm_vm_ioctl_get_pit(struct kvm *kvm, struct kvm_pit_state *ps)\n{\n\tstruct kvm_kpit_state *kps = &kvm->arch.vpit->pit_state;\n\n\tBUILD_BUG_ON(sizeof(*ps) != sizeof(kps->channels));\n\n\tmutex_lock(&kps->lock);\n\tmemcpy(ps, &kps->channels, sizeof(*ps));\n\tmutex_unlock(&kps->lock);\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_set_pit(struct kvm *kvm, struct kvm_pit_state *ps)\n{\n\tint i;\n\tstruct kvm_pit *pit = kvm->arch.vpit;\n\n\tmutex_lock(&pit->pit_state.lock);\n\tmemcpy(&pit->pit_state.channels, ps, sizeof(*ps));\n\tfor (i = 0; i < 3; i++)\n\t\tkvm_pit_load_count(pit, i, ps->channels[i].count, 0);\n\tmutex_unlock(&pit->pit_state.lock);\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_get_pit2(struct kvm *kvm, struct kvm_pit_state2 *ps)\n{\n\tmutex_lock(&kvm->arch.vpit->pit_state.lock);\n\tmemcpy(ps->channels, &kvm->arch.vpit->pit_state.channels,\n\t\tsizeof(ps->channels));\n\tps->flags = kvm->arch.vpit->pit_state.flags;\n\tmutex_unlock(&kvm->arch.vpit->pit_state.lock);\n\tmemset(&ps->reserved, 0, sizeof(ps->reserved));\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_set_pit2(struct kvm *kvm, struct kvm_pit_state2 *ps)\n{\n\tint start = 0;\n\tint i;\n\tu32 prev_legacy, cur_legacy;\n\tstruct kvm_pit *pit = kvm->arch.vpit;\n\n\tmutex_lock(&pit->pit_state.lock);\n\tprev_legacy = pit->pit_state.flags & KVM_PIT_FLAGS_HPET_LEGACY;\n\tcur_legacy = ps->flags & KVM_PIT_FLAGS_HPET_LEGACY;\n\tif (!prev_legacy && cur_legacy)\n\t\tstart = 1;\n\tmemcpy(&pit->pit_state.channels, &ps->channels,\n\t       sizeof(pit->pit_state.channels));\n\tpit->pit_state.flags = ps->flags;\n\tfor (i = 0; i < 3; i++)\n\t\tkvm_pit_load_count(pit, i, pit->pit_state.channels[i].count,\n\t\t\t\t   start && i == 0);\n\tmutex_unlock(&pit->pit_state.lock);\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_reinject(struct kvm *kvm,\n\t\t\t\t struct kvm_reinject_control *control)\n{\n\tstruct kvm_pit *pit = kvm->arch.vpit;\n\n\t/* pit->pit_state.lock was overloaded to prevent userspace from getting\n\t * an inconsistent state after running multiple KVM_REINJECT_CONTROL\n\t * ioctls in parallel.  Use a separate lock if that ioctl isn't rare.\n\t */\n\tmutex_lock(&pit->pit_state.lock);\n\tkvm_pit_set_reinject(pit, control->pit_reinject);\n\tmutex_unlock(&pit->pit_state.lock);\n\n\treturn 0;\n}\n\nvoid kvm_arch_sync_dirty_log(struct kvm *kvm, struct kvm_memory_slot *memslot)\n{\n\n\t/*\n\t * Flush all CPUs' dirty log buffers to the  dirty_bitmap.  Called\n\t * before reporting dirty_bitmap to userspace.  KVM flushes the buffers\n\t * on all VM-Exits, thus we only need to kick running vCPUs to force a\n\t * VM-Exit.\n\t */\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nint kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,\n\t\t\tbool line_status)\n{\n\tif (!irqchip_in_kernel(kvm))\n\t\treturn -ENXIO;\n\n\tirq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,\n\t\t\t\t\tirq_event->irq, irq_event->level,\n\t\t\t\t\tline_status);\n\treturn 0;\n}\n\nint kvm_vm_ioctl_enable_cap(struct kvm *kvm,\n\t\t\t    struct kvm_enable_cap *cap)\n{\n\tint r;\n\n\tif (cap->flags)\n\t\treturn -EINVAL;\n\n\tswitch (cap->cap) {\n\tcase KVM_CAP_DISABLE_QUIRKS2:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_X86_VALID_QUIRKS)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase KVM_CAP_DISABLE_QUIRKS:\n\t\tkvm->arch.disabled_quirks = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_SPLIT_IRQCHIP: {\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] > MAX_NR_RESERVED_IOAPIC_PINS)\n\t\t\tgoto split_irqchip_unlock;\n\t\tr = -EEXIST;\n\t\tif (irqchip_in_kernel(kvm))\n\t\t\tgoto split_irqchip_unlock;\n\t\tif (kvm->created_vcpus)\n\t\t\tgoto split_irqchip_unlock;\n\t\tr = kvm_setup_empty_irq_routing(kvm);\n\t\tif (r)\n\t\t\tgoto split_irqchip_unlock;\n\t\t/* Pairs with irqchip_in_kernel. */\n\t\tsmp_wmb();\n\t\tkvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;\n\t\tkvm->arch.nr_reserved_ioapic_pins = cap->args[0];\n\t\tkvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);\n\t\tr = 0;\nsplit_irqchip_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_CAP_X2APIC_API:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_X2APIC_API_VALID_FLAGS)\n\t\t\tbreak;\n\n\t\tif (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS)\n\t\t\tkvm->arch.x2apic_format = true;\n\t\tif (cap->args[0] & KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)\n\t\t\tkvm->arch.x2apic_broadcast_quirk_disabled = true;\n\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_X86_DISABLE_EXITS:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_X86_DISABLE_VALID_EXITS)\n\t\t\tbreak;\n\n\t\tif (cap->args[0] & KVM_X86_DISABLE_EXITS_PAUSE)\n\t\t\tkvm->arch.pause_in_guest = true;\n\n#define SMT_RSB_MSG \"This processor is affected by the Cross-Thread Return Predictions vulnerability. \" \\\n\t\t    \"KVM_CAP_X86_DISABLE_EXITS should only be used with SMT disabled or trusted guests.\"\n\n\t\tif (!mitigate_smt_rsb) {\n\t\t\tif (boot_cpu_has_bug(X86_BUG_SMT_RSB) && cpu_smt_possible() &&\n\t\t\t    (cap->args[0] & ~KVM_X86_DISABLE_EXITS_PAUSE))\n\t\t\t\tpr_warn_once(SMT_RSB_MSG);\n\n\t\t\tif ((cap->args[0] & KVM_X86_DISABLE_EXITS_MWAIT) &&\n\t\t\t    kvm_can_mwait_in_guest())\n\t\t\t\tkvm->arch.mwait_in_guest = true;\n\t\t\tif (cap->args[0] & KVM_X86_DISABLE_EXITS_HLT)\n\t\t\t\tkvm->arch.hlt_in_guest = true;\n\t\t\tif (cap->args[0] & KVM_X86_DISABLE_EXITS_CSTATE)\n\t\t\t\tkvm->arch.cstate_in_guest = true;\n\t\t}\n\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_MSR_PLATFORM_INFO:\n\t\tkvm->arch.guest_can_read_msr_platform_info = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_EXCEPTION_PAYLOAD:\n\t\tkvm->arch.exception_payload_enabled = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_X86_TRIPLE_FAULT_EVENT:\n\t\tkvm->arch.triple_fault_event = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_X86_USER_SPACE_MSR:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_MSR_EXIT_REASON_VALID_MASK)\n\t\t\tbreak;\n\t\tkvm->arch.user_space_msr_mask = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_X86_BUS_LOCK_EXIT:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_BUS_LOCK_DETECTION_VALID_MODE)\n\t\t\tbreak;\n\n\t\tif ((cap->args[0] & KVM_BUS_LOCK_DETECTION_OFF) &&\n\t\t    (cap->args[0] & KVM_BUS_LOCK_DETECTION_EXIT))\n\t\t\tbreak;\n\n\t\tif (kvm_caps.has_bus_lock_exit &&\n\t\t    cap->args[0] & KVM_BUS_LOCK_DETECTION_EXIT)\n\t\t\tkvm->arch.bus_lock_detection_enabled = true;\n\t\tr = 0;\n\t\tbreak;\n#ifdef CONFIG_X86_SGX_KVM\n\tcase KVM_CAP_SGX_ATTRIBUTE: {\n\t\tunsigned long allowed_attributes = 0;\n\n\t\tr = sgx_set_attribute(&allowed_attributes, cap->args[0]);\n\t\tif (r)\n\t\t\tbreak;\n\n\t\t/* KVM only supports the PROVISIONKEY privileged attribute. */\n\t\tif ((allowed_attributes & SGX_ATTR_PROVISIONKEY) &&\n\t\t    !(allowed_attributes & ~SGX_ATTR_PROVISIONKEY))\n\t\t\tkvm->arch.sgx_provisioning_allowed = true;\n\t\telse\n\t\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n#endif\n\tcase KVM_CAP_VM_COPY_ENC_CONTEXT_FROM:\n\t\tr = -EINVAL;\n\t\tif (!kvm_x86_ops.vm_copy_enc_context_from)\n\t\t\tbreak;\n\n\t\tr = static_call(kvm_x86_vm_copy_enc_context_from)(kvm, cap->args[0]);\n\t\tbreak;\n\tcase KVM_CAP_VM_MOVE_ENC_CONTEXT_FROM:\n\t\tr = -EINVAL;\n\t\tif (!kvm_x86_ops.vm_move_enc_context_from)\n\t\t\tbreak;\n\n\t\tr = static_call(kvm_x86_vm_move_enc_context_from)(kvm, cap->args[0]);\n\t\tbreak;\n\tcase KVM_CAP_EXIT_HYPERCALL:\n\t\tif (cap->args[0] & ~KVM_EXIT_HYPERCALL_VALID_MASK) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tkvm->arch.hypercall_exit_enabled = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_EXIT_ON_EMULATION_FAILURE:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~1)\n\t\t\tbreak;\n\t\tkvm->arch.exit_on_emulation_error = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_PMU_CAPABILITY:\n\t\tr = -EINVAL;\n\t\tif (!enable_pmu || (cap->args[0] & ~KVM_CAP_PMU_VALID_MASK))\n\t\t\tbreak;\n\n\t\tmutex_lock(&kvm->lock);\n\t\tif (!kvm->created_vcpus) {\n\t\t\tkvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);\n\t\t\tr = 0;\n\t\t}\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPU_ID:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] > KVM_MAX_VCPU_IDS)\n\t\t\tbreak;\n\n\t\tmutex_lock(&kvm->lock);\n\t\tif (kvm->arch.max_vcpu_ids == cap->args[0]) {\n\t\t\tr = 0;\n\t\t} else if (!kvm->arch.max_vcpu_ids) {\n\t\t\tkvm->arch.max_vcpu_ids = cap->args[0];\n\t\t\tr = 0;\n\t\t}\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tcase KVM_CAP_X86_NOTIFY_VMEXIT:\n\t\tr = -EINVAL;\n\t\tif ((u32)cap->args[0] & ~KVM_X86_NOTIFY_VMEXIT_VALID_BITS)\n\t\t\tbreak;\n\t\tif (!kvm_caps.has_notify_vmexit)\n\t\t\tbreak;\n\t\tif (!((u32)cap->args[0] & KVM_X86_NOTIFY_VMEXIT_ENABLED))\n\t\t\tbreak;\n\t\tmutex_lock(&kvm->lock);\n\t\tif (!kvm->created_vcpus) {\n\t\t\tkvm->arch.notify_window = cap->args[0] >> 32;\n\t\t\tkvm->arch.notify_vmexit_flags = (u32)cap->args[0];\n\t\t\tr = 0;\n\t\t}\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tcase KVM_CAP_VM_DISABLE_NX_HUGE_PAGES:\n\t\tr = -EINVAL;\n\n\t\t/*\n\t\t * Since the risk of disabling NX hugepages is a guest crashing\n\t\t * the system, ensure the userspace process has permission to\n\t\t * reboot the system.\n\t\t *\n\t\t * Note that unlike the reboot() syscall, the process must have\n\t\t * this capability in the root namespace because exposing\n\t\t * /dev/kvm into a container does not limit the scope of the\n\t\t * iTLB multihit bug to that container. In other words,\n\t\t * this must use capable(), not ns_capable().\n\t\t */\n\t\tif (!capable(CAP_SYS_BOOT)) {\n\t\t\tr = -EPERM;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cap->args[0])\n\t\t\tbreak;\n\n\t\tmutex_lock(&kvm->lock);\n\t\tif (!kvm->created_vcpus) {\n\t\t\tkvm->arch.disable_nx_huge_pages = true;\n\t\t\tr = 0;\n\t\t}\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn r;\n}\n\nstatic struct kvm_x86_msr_filter *kvm_alloc_msr_filter(bool default_allow)\n{\n\tstruct kvm_x86_msr_filter *msr_filter;\n\n\tmsr_filter = kzalloc(sizeof(*msr_filter), GFP_KERNEL_ACCOUNT);\n\tif (!msr_filter)\n\t\treturn NULL;\n\n\tmsr_filter->default_allow = default_allow;\n\treturn msr_filter;\n}\n\nstatic void kvm_free_msr_filter(struct kvm_x86_msr_filter *msr_filter)\n{\n\tu32 i;\n\n\tif (!msr_filter)\n\t\treturn;\n\n\tfor (i = 0; i < msr_filter->count; i++)\n\t\tkfree(msr_filter->ranges[i].bitmap);\n\n\tkfree(msr_filter);\n}\n\nstatic int kvm_add_msr_filter(struct kvm_x86_msr_filter *msr_filter,\n\t\t\t      struct kvm_msr_filter_range *user_range)\n{\n\tunsigned long *bitmap = NULL;\n\tsize_t bitmap_size;\n\n\tif (!user_range->nmsrs)\n\t\treturn 0;\n\n\tif (user_range->flags & ~KVM_MSR_FILTER_RANGE_VALID_MASK)\n\t\treturn -EINVAL;\n\n\tif (!user_range->flags)\n\t\treturn -EINVAL;\n\n\tbitmap_size = BITS_TO_LONGS(user_range->nmsrs) * sizeof(long);\n\tif (!bitmap_size || bitmap_size > KVM_MSR_FILTER_MAX_BITMAP_SIZE)\n\t\treturn -EINVAL;\n\n\tbitmap = memdup_user((__user u8*)user_range->bitmap, bitmap_size);\n\tif (IS_ERR(bitmap))\n\t\treturn PTR_ERR(bitmap);\n\n\tmsr_filter->ranges[msr_filter->count] = (struct msr_bitmap_range) {\n\t\t.flags = user_range->flags,\n\t\t.base = user_range->base,\n\t\t.nmsrs = user_range->nmsrs,\n\t\t.bitmap = bitmap,\n\t};\n\n\tmsr_filter->count++;\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_set_msr_filter(struct kvm *kvm,\n\t\t\t\t       struct kvm_msr_filter *filter)\n{\n\tstruct kvm_x86_msr_filter *new_filter, *old_filter;\n\tbool default_allow;\n\tbool empty = true;\n\tint r = 0;\n\tu32 i;\n\n\tif (filter->flags & ~KVM_MSR_FILTER_VALID_MASK)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < ARRAY_SIZE(filter->ranges); i++)\n\t\tempty &= !filter->ranges[i].nmsrs;\n\n\tdefault_allow = !(filter->flags & KVM_MSR_FILTER_DEFAULT_DENY);\n\tif (empty && !default_allow)\n\t\treturn -EINVAL;\n\n\tnew_filter = kvm_alloc_msr_filter(default_allow);\n\tif (!new_filter)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < ARRAY_SIZE(filter->ranges); i++) {\n\t\tr = kvm_add_msr_filter(new_filter, &filter->ranges[i]);\n\t\tif (r) {\n\t\t\tkvm_free_msr_filter(new_filter);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tmutex_lock(&kvm->lock);\n\n\t/* The per-VM filter is protected by kvm->lock... */\n\told_filter = srcu_dereference_check(kvm->arch.msr_filter, &kvm->srcu, 1);\n\n\trcu_assign_pointer(kvm->arch.msr_filter, new_filter);\n\tsynchronize_srcu(&kvm->srcu);\n\n\tkvm_free_msr_filter(old_filter);\n\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_MSR_FILTER_CHANGED);\n\tmutex_unlock(&kvm->lock);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_KVM_COMPAT\n/* for KVM_X86_SET_MSR_FILTER */\nstruct kvm_msr_filter_range_compat {\n\t__u32 flags;\n\t__u32 nmsrs;\n\t__u32 base;\n\t__u32 bitmap;\n};\n\nstruct kvm_msr_filter_compat {\n\t__u32 flags;\n\tstruct kvm_msr_filter_range_compat ranges[KVM_MSR_FILTER_MAX_RANGES];\n};\n\n#define KVM_X86_SET_MSR_FILTER_COMPAT _IOW(KVMIO, 0xc6, struct kvm_msr_filter_compat)\n\nlong kvm_arch_vm_compat_ioctl(struct file *filp, unsigned int ioctl,\n\t\t\t      unsigned long arg)\n{\n\tvoid __user *argp = (void __user *)arg;\n\tstruct kvm *kvm = filp->private_data;\n\tlong r = -ENOTTY;\n\n\tswitch (ioctl) {\n\tcase KVM_X86_SET_MSR_FILTER_COMPAT: {\n\t\tstruct kvm_msr_filter __user *user_msr_filter = argp;\n\t\tstruct kvm_msr_filter_compat filter_compat;\n\t\tstruct kvm_msr_filter filter;\n\t\tint i;\n\n\t\tif (copy_from_user(&filter_compat, user_msr_filter,\n\t\t\t\t   sizeof(filter_compat)))\n\t\t\treturn -EFAULT;\n\n\t\tfilter.flags = filter_compat.flags;\n\t\tfor (i = 0; i < ARRAY_SIZE(filter.ranges); i++) {\n\t\t\tstruct kvm_msr_filter_range_compat *cr;\n\n\t\t\tcr = &filter_compat.ranges[i];\n\t\t\tfilter.ranges[i] = (struct kvm_msr_filter_range) {\n\t\t\t\t.flags = cr->flags,\n\t\t\t\t.nmsrs = cr->nmsrs,\n\t\t\t\t.base = cr->base,\n\t\t\t\t.bitmap = (__u8 *)(ulong)cr->bitmap,\n\t\t\t};\n\t\t}\n\n\t\tr = kvm_vm_ioctl_set_msr_filter(kvm, &filter);\n\t\tbreak;\n\t}\n\t}\n\n\treturn r;\n}\n#endif\n\n#ifdef CONFIG_HAVE_KVM_PM_NOTIFIER\nstatic int kvm_arch_suspend_notifier(struct kvm *kvm)\n{\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\tint ret = 0;\n\n\tmutex_lock(&kvm->lock);\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tif (!vcpu->arch.pv_time.active)\n\t\t\tcontinue;\n\n\t\tret = kvm_set_guest_paused(vcpu);\n\t\tif (ret) {\n\t\t\tkvm_err(\"Failed to pause guest VCPU%d: %d\\n\",\n\t\t\t\tvcpu->vcpu_id, ret);\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&kvm->lock);\n\n\treturn ret ? NOTIFY_BAD : NOTIFY_DONE;\n}\n\nint kvm_arch_pm_notifier(struct kvm *kvm, unsigned long state)\n{\n\tswitch (state) {\n\tcase PM_HIBERNATION_PREPARE:\n\tcase PM_SUSPEND_PREPARE:\n\t\treturn kvm_arch_suspend_notifier(kvm);\n\t}\n\n\treturn NOTIFY_DONE;\n}\n#endif /* CONFIG_HAVE_KVM_PM_NOTIFIER */\n\nstatic int kvm_vm_ioctl_get_clock(struct kvm *kvm, void __user *argp)\n{\n\tstruct kvm_clock_data data = { 0 };\n\n\tget_kvmclock(kvm, &data);\n\tif (copy_to_user(argp, &data, sizeof(data)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\tstruct kvm_clock_data data;\n\tu64 now_raw_ns;\n\n\tif (copy_from_user(&data, argp, sizeof(data)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Only KVM_CLOCK_REALTIME is used, but allow passing the\n\t * result of KVM_GET_CLOCK back to KVM_SET_CLOCK.\n\t */\n\tif (data.flags & ~KVM_CLOCK_VALID_FLAGS)\n\t\treturn -EINVAL;\n\n\tkvm_hv_request_tsc_page_update(kvm);\n\tkvm_start_pvclock_update(kvm);\n\tpvclock_update_vm_gtod_copy(kvm);\n\n\t/*\n\t * This pairs with kvm_guest_time_update(): when masterclock is\n\t * in use, we use master_kernel_ns + kvmclock_offset to set\n\t * unsigned 'system_time' so if we use get_kvmclock_ns() (which\n\t * is slightly ahead) here we risk going negative on unsigned\n\t * 'system_time' when 'data.clock' is very small.\n\t */\n\tif (data.flags & KVM_CLOCK_REALTIME) {\n\t\tu64 now_real_ns = ktime_get_real_ns();\n\n\t\t/*\n\t\t * Avoid stepping the kvmclock backwards.\n\t\t */\n\t\tif (now_real_ns > data.realtime)\n\t\t\tdata.clock += now_real_ns - data.realtime;\n\t}\n\n\tif (ka->use_master_clock)\n\t\tnow_raw_ns = ka->master_kernel_ns;\n\telse\n\t\tnow_raw_ns = get_kvmclock_base_ns();\n\tka->kvmclock_offset = data.clock - now_raw_ns;\n\tkvm_end_pvclock_update(kvm);\n\treturn 0;\n}\n\nlong kvm_arch_vm_ioctl(struct file *filp,\n\t\t       unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r = -ENOTTY;\n\t/*\n\t * This union makes it completely explicit to gcc-3.x\n\t * that these two variables' stack usage should be\n\t * combined, not added together.\n\t */\n\tunion {\n\t\tstruct kvm_pit_state ps;\n\t\tstruct kvm_pit_state2 ps2;\n\t\tstruct kvm_pit_config pit_config;\n\t} u;\n\n\tswitch (ioctl) {\n\tcase KVM_SET_TSS_ADDR:\n\t\tr = kvm_vm_ioctl_set_tss_addr(kvm, arg);\n\t\tbreak;\n\tcase KVM_SET_IDENTITY_MAP_ADDR: {\n\t\tu64 ident_addr;\n\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -EINVAL;\n\t\tif (kvm->created_vcpus)\n\t\t\tgoto set_identity_unlock;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&ident_addr, argp, sizeof(ident_addr)))\n\t\t\tgoto set_identity_unlock;\n\t\tr = kvm_vm_ioctl_set_identity_map_addr(kvm, ident_addr);\nset_identity_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_SET_NR_MMU_PAGES:\n\t\tr = kvm_vm_ioctl_set_nr_mmu_pages(kvm, arg);\n\t\tbreak;\n\tcase KVM_GET_NR_MMU_PAGES:\n\t\tr = kvm_vm_ioctl_get_nr_mmu_pages(kvm);\n\t\tbreak;\n\tcase KVM_CREATE_IRQCHIP: {\n\t\tmutex_lock(&kvm->lock);\n\n\t\tr = -EEXIST;\n\t\tif (irqchip_in_kernel(kvm))\n\t\t\tgoto create_irqchip_unlock;\n\n\t\tr = -EINVAL;\n\t\tif (kvm->created_vcpus)\n\t\t\tgoto create_irqchip_unlock;\n\n\t\tr = kvm_pic_init(kvm);\n\t\tif (r)\n\t\t\tgoto create_irqchip_unlock;\n\n\t\tr = kvm_ioapic_init(kvm);\n\t\tif (r) {\n\t\t\tkvm_pic_destroy(kvm);\n\t\t\tgoto create_irqchip_unlock;\n\t\t}\n\n\t\tr = kvm_setup_default_irq_routing(kvm);\n\t\tif (r) {\n\t\t\tkvm_ioapic_destroy(kvm);\n\t\t\tkvm_pic_destroy(kvm);\n\t\t\tgoto create_irqchip_unlock;\n\t\t}\n\t\t/* Write kvm->irq_routing before enabling irqchip_in_kernel. */\n\t\tsmp_wmb();\n\t\tkvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;\n\t\tkvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);\n\tcreate_irqchip_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_CREATE_PIT:\n\t\tu.pit_config.flags = KVM_PIT_SPEAKER_DUMMY;\n\t\tgoto create_pit;\n\tcase KVM_CREATE_PIT2:\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.pit_config, argp,\n\t\t\t\t   sizeof(struct kvm_pit_config)))\n\t\t\tgoto out;\n\tcreate_pit:\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -EEXIST;\n\t\tif (kvm->arch.vpit)\n\t\t\tgoto create_pit_unlock;\n\t\tr = -ENOMEM;\n\t\tkvm->arch.vpit = kvm_create_pit(kvm, u.pit_config.flags);\n\t\tif (kvm->arch.vpit)\n\t\t\tr = 0;\n\tcreate_pit_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tcase KVM_GET_IRQCHIP: {\n\t\t/* 0: PIC master, 1: PIC slave, 2: IOAPIC */\n\t\tstruct kvm_irqchip *chip;\n\n\t\tchip = memdup_user(argp, sizeof(*chip));\n\t\tif (IS_ERR(chip)) {\n\t\t\tr = PTR_ERR(chip);\n\t\t\tgoto out;\n\t\t}\n\n\t\tr = -ENXIO;\n\t\tif (!irqchip_kernel(kvm))\n\t\t\tgoto get_irqchip_out;\n\t\tr = kvm_vm_ioctl_get_irqchip(kvm, chip);\n\t\tif (r)\n\t\t\tgoto get_irqchip_out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, chip, sizeof(*chip)))\n\t\t\tgoto get_irqchip_out;\n\t\tr = 0;\n\tget_irqchip_out:\n\t\tkfree(chip);\n\t\tbreak;\n\t}\n\tcase KVM_SET_IRQCHIP: {\n\t\t/* 0: PIC master, 1: PIC slave, 2: IOAPIC */\n\t\tstruct kvm_irqchip *chip;\n\n\t\tchip = memdup_user(argp, sizeof(*chip));\n\t\tif (IS_ERR(chip)) {\n\t\t\tr = PTR_ERR(chip);\n\t\t\tgoto out;\n\t\t}\n\n\t\tr = -ENXIO;\n\t\tif (!irqchip_kernel(kvm))\n\t\t\tgoto set_irqchip_out;\n\t\tr = kvm_vm_ioctl_set_irqchip(kvm, chip);\n\tset_irqchip_out:\n\t\tkfree(chip);\n\t\tbreak;\n\t}\n\tcase KVM_GET_PIT: {\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.ps, argp, sizeof(struct kvm_pit_state)))\n\t\t\tgoto out;\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_get_pit(kvm, &u.ps);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &u.ps, sizeof(struct kvm_pit_state)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_PIT: {\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.ps, argp, sizeof(u.ps)))\n\t\t\tgoto out;\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto set_pit_out;\n\t\tr = kvm_vm_ioctl_set_pit(kvm, &u.ps);\nset_pit_out:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_GET_PIT2: {\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_get_pit2(kvm, &u.ps2);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &u.ps2, sizeof(u.ps2)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_PIT2: {\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.ps2, argp, sizeof(u.ps2)))\n\t\t\tgoto out;\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto set_pit2_out;\n\t\tr = kvm_vm_ioctl_set_pit2(kvm, &u.ps2);\nset_pit2_out:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_REINJECT_CONTROL: {\n\t\tstruct kvm_reinject_control control;\n\t\tr =  -EFAULT;\n\t\tif (copy_from_user(&control, argp, sizeof(control)))\n\t\t\tgoto out;\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_reinject(kvm, &control);\n\t\tbreak;\n\t}\n\tcase KVM_SET_BOOT_CPU_ID:\n\t\tr = 0;\n\t\tmutex_lock(&kvm->lock);\n\t\tif (kvm->created_vcpus)\n\t\t\tr = -EBUSY;\n\t\telse\n\t\t\tkvm->arch.bsp_vcpu_id = arg;\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n#ifdef CONFIG_KVM_XEN\n\tcase KVM_XEN_HVM_CONFIG: {\n\t\tstruct kvm_xen_hvm_config xhc;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xhc, argp, sizeof(xhc)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_hvm_config(kvm, &xhc);\n\t\tbreak;\n\t}\n\tcase KVM_XEN_HVM_GET_ATTR: {\n\t\tstruct kvm_xen_hvm_attr xha;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xha, argp, sizeof(xha)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_hvm_get_attr(kvm, &xha);\n\t\tif (!r && copy_to_user(argp, &xha, sizeof(xha)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase KVM_XEN_HVM_SET_ATTR: {\n\t\tstruct kvm_xen_hvm_attr xha;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xha, argp, sizeof(xha)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_hvm_set_attr(kvm, &xha);\n\t\tbreak;\n\t}\n\tcase KVM_XEN_HVM_EVTCHN_SEND: {\n\t\tstruct kvm_irq_routing_xen_evtchn uxe;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&uxe, argp, sizeof(uxe)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_hvm_evtchn_send(kvm, &uxe);\n\t\tbreak;\n\t}\n#endif\n\tcase KVM_SET_CLOCK:\n\t\tr = kvm_vm_ioctl_set_clock(kvm, argp);\n\t\tbreak;\n\tcase KVM_GET_CLOCK:\n\t\tr = kvm_vm_ioctl_get_clock(kvm, argp);\n\t\tbreak;\n\tcase KVM_SET_TSC_KHZ: {\n\t\tu32 user_tsc_khz;\n\n\t\tr = -EINVAL;\n\t\tuser_tsc_khz = (u32)arg;\n\n\t\tif (kvm_caps.has_tsc_control &&\n\t\t    user_tsc_khz >= kvm_caps.max_guest_tsc_khz)\n\t\t\tgoto out;\n\n\t\tif (user_tsc_khz == 0)\n\t\t\tuser_tsc_khz = tsc_khz;\n\n\t\tWRITE_ONCE(kvm->arch.default_tsc_khz, user_tsc_khz);\n\t\tr = 0;\n\n\t\tgoto out;\n\t}\n\tcase KVM_GET_TSC_KHZ: {\n\t\tr = READ_ONCE(kvm->arch.default_tsc_khz);\n\t\tgoto out;\n\t}\n\tcase KVM_MEMORY_ENCRYPT_OP: {\n\t\tr = -ENOTTY;\n\t\tif (!kvm_x86_ops.mem_enc_ioctl)\n\t\t\tgoto out;\n\n\t\tr = static_call(kvm_x86_mem_enc_ioctl)(kvm, argp);\n\t\tbreak;\n\t}\n\tcase KVM_MEMORY_ENCRYPT_REG_REGION: {\n\t\tstruct kvm_enc_region region;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&region, argp, sizeof(region)))\n\t\t\tgoto out;\n\n\t\tr = -ENOTTY;\n\t\tif (!kvm_x86_ops.mem_enc_register_region)\n\t\t\tgoto out;\n\n\t\tr = static_call(kvm_x86_mem_enc_register_region)(kvm, &region);\n\t\tbreak;\n\t}\n\tcase KVM_MEMORY_ENCRYPT_UNREG_REGION: {\n\t\tstruct kvm_enc_region region;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&region, argp, sizeof(region)))\n\t\t\tgoto out;\n\n\t\tr = -ENOTTY;\n\t\tif (!kvm_x86_ops.mem_enc_unregister_region)\n\t\t\tgoto out;\n\n\t\tr = static_call(kvm_x86_mem_enc_unregister_region)(kvm, &region);\n\t\tbreak;\n\t}\n\tcase KVM_HYPERV_EVENTFD: {\n\t\tstruct kvm_hyperv_eventfd hvevfd;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&hvevfd, argp, sizeof(hvevfd)))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_hv_eventfd(kvm, &hvevfd);\n\t\tbreak;\n\t}\n\tcase KVM_SET_PMU_EVENT_FILTER:\n\t\tr = kvm_vm_ioctl_set_pmu_event_filter(kvm, argp);\n\t\tbreak;\n\tcase KVM_X86_SET_MSR_FILTER: {\n\t\tstruct kvm_msr_filter __user *user_msr_filter = argp;\n\t\tstruct kvm_msr_filter filter;\n\n\t\tif (copy_from_user(&filter, user_msr_filter, sizeof(filter)))\n\t\t\treturn -EFAULT;\n\n\t\tr = kvm_vm_ioctl_set_msr_filter(kvm, &filter);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = -ENOTTY;\n\t}\nout:\n\treturn r;\n}\n\nstatic void kvm_init_msr_list(void)\n{\n\tu32 dummy[2];\n\tunsigned i;\n\n\tBUILD_BUG_ON_MSG(KVM_PMC_MAX_FIXED != 3,\n\t\t\t \"Please update the fixed PMCs in msrs_to_saved_all[]\");\n\n\tnum_msrs_to_save = 0;\n\tnum_emulated_msrs = 0;\n\tnum_msr_based_features = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(msrs_to_save_all); i++) {\n\t\tif (rdmsr_safe(msrs_to_save_all[i], &dummy[0], &dummy[1]) < 0)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Even MSRs that are valid in the host may not be exposed\n\t\t * to the guests in some cases.\n\t\t */\n\t\tswitch (msrs_to_save_all[i]) {\n\t\tcase MSR_IA32_BNDCFGS:\n\t\t\tif (!kvm_mpx_supported())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_TSC_AUX:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_RDTSCP) &&\n\t\t\t    !kvm_cpu_cap_has(X86_FEATURE_RDPID))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_UMWAIT_CONTROL:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_WAITPKG))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_CTL:\n\t\tcase MSR_IA32_RTIT_STATUS:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_CR3_MATCH:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT) ||\n\t\t\t    !intel_pt_validate_hw_cap(PT_CAP_cr3_filtering))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_OUTPUT_BASE:\n\t\tcase MSR_IA32_RTIT_OUTPUT_MASK:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT) ||\n\t\t\t\t(!intel_pt_validate_hw_cap(PT_CAP_topa_output) &&\n\t\t\t\t !intel_pt_validate_hw_cap(PT_CAP_single_range_output)))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT) ||\n\t\t\t\tmsrs_to_save_all[i] - MSR_IA32_RTIT_ADDR0_A >=\n\t\t\t\tintel_pt_validate_hw_cap(PT_CAP_num_address_ranges) * 2)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_ARCH_PERFMON_PERFCTR0 ... MSR_ARCH_PERFMON_PERFCTR_MAX:\n\t\t\tif (msrs_to_save_all[i] - MSR_ARCH_PERFMON_PERFCTR0 >=\n\t\t\t    min(KVM_INTEL_PMC_MAX_GENERIC, kvm_pmu_cap.num_counters_gp))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_ARCH_PERFMON_EVENTSEL0 ... MSR_ARCH_PERFMON_EVENTSEL_MAX:\n\t\t\tif (msrs_to_save_all[i] - MSR_ARCH_PERFMON_EVENTSEL0 >=\n\t\t\t    min(KVM_INTEL_PMC_MAX_GENERIC, kvm_pmu_cap.num_counters_gp))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_XFD:\n\t\tcase MSR_IA32_XFD_ERR:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_XFD))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tmsrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(emulated_msrs_all); i++) {\n\t\tif (!static_call(kvm_x86_has_emulated_msr)(NULL, emulated_msrs_all[i]))\n\t\t\tcontinue;\n\n\t\temulated_msrs[num_emulated_msrs++] = emulated_msrs_all[i];\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(msr_based_features_all); i++) {\n\t\tstruct kvm_msr_entry msr;\n\n\t\tmsr.index = msr_based_features_all[i];\n\t\tif (kvm_get_msr_feature(&msr))\n\t\t\tcontinue;\n\n\t\tmsr_based_features[num_msr_based_features++] = msr_based_features_all[i];\n\t}\n}\n\nstatic int vcpu_mmio_write(struct kvm_vcpu *vcpu, gpa_t addr, int len,\n\t\t\t   const void *v)\n{\n\tint handled = 0;\n\tint n;\n\n\tdo {\n\t\tn = min(len, 8);\n\t\tif (!(lapic_in_kernel(vcpu) &&\n\t\t      !kvm_iodevice_write(vcpu, &vcpu->arch.apic->dev, addr, n, v))\n\t\t    && kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, n, v))\n\t\t\tbreak;\n\t\thandled += n;\n\t\taddr += n;\n\t\tlen -= n;\n\t\tv += n;\n\t} while (len);\n\n\treturn handled;\n}\n\nstatic int vcpu_mmio_read(struct kvm_vcpu *vcpu, gpa_t addr, int len, void *v)\n{\n\tint handled = 0;\n\tint n;\n\n\tdo {\n\t\tn = min(len, 8);\n\t\tif (!(lapic_in_kernel(vcpu) &&\n\t\t      !kvm_iodevice_read(vcpu, &vcpu->arch.apic->dev,\n\t\t\t\t\t addr, n, v))\n\t\t    && kvm_io_bus_read(vcpu, KVM_MMIO_BUS, addr, n, v))\n\t\t\tbreak;\n\t\ttrace_kvm_mmio(KVM_TRACE_MMIO_READ, n, addr, v);\n\t\thandled += n;\n\t\taddr += n;\n\t\tlen -= n;\n\t\tv += n;\n\t} while (len);\n\n\treturn handled;\n}\n\nvoid kvm_set_segment(struct kvm_vcpu *vcpu,\n\t\t     struct kvm_segment *var, int seg)\n{\n\tstatic_call(kvm_x86_set_segment)(vcpu, var, seg);\n}\n\nvoid kvm_get_segment(struct kvm_vcpu *vcpu,\n\t\t     struct kvm_segment *var, int seg)\n{\n\tstatic_call(kvm_x86_get_segment)(vcpu, var, seg);\n}\n\ngpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u64 access,\n\t\t\t   struct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tgpa_t t_gpa;\n\n\tBUG_ON(!mmu_is_nested(vcpu));\n\n\t/* NPT walks are always user-walks */\n\taccess |= PFERR_USER_MASK;\n\tt_gpa  = mmu->gva_to_gpa(vcpu, mmu, gpa, access, exception);\n\n\treturn t_gpa;\n}\n\ngpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t      struct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tu64 access = (static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\treturn mmu->gva_to_gpa(vcpu, mmu, gva, access, exception);\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_gva_to_gpa_read);\n\ngpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t       struct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tu64 access = (static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\taccess |= PFERR_WRITE_MASK;\n\treturn mmu->gva_to_gpa(vcpu, mmu, gva, access, exception);\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_gva_to_gpa_write);\n\n/* uses this to access any guest's mapped memory without checking CPL */\ngpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t\tstruct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\treturn mmu->gva_to_gpa(vcpu, mmu, gva, 0, exception);\n}\n\nstatic int kvm_read_guest_virt_helper(gva_t addr, void *val, unsigned int bytes,\n\t\t\t\t      struct kvm_vcpu *vcpu, u64 access,\n\t\t\t\t      struct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tvoid *data = val;\n\tint r = X86EMUL_CONTINUE;\n\n\twhile (bytes) {\n\t\tgpa_t gpa = mmu->gva_to_gpa(vcpu, mmu, addr, access, exception);\n\t\tunsigned offset = addr & (PAGE_SIZE-1);\n\t\tunsigned toread = min(bytes, (unsigned)PAGE_SIZE - offset);\n\t\tint ret;\n\n\t\tif (gpa == INVALID_GPA)\n\t\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t\tret = kvm_vcpu_read_guest_page(vcpu, gpa >> PAGE_SHIFT, data,\n\t\t\t\t\t       offset, toread);\n\t\tif (ret < 0) {\n\t\t\tr = X86EMUL_IO_NEEDED;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbytes -= toread;\n\t\tdata += toread;\n\t\taddr += toread;\n\t}\nout:\n\treturn r;\n}\n\n/* used for instruction fetching */\nstatic int kvm_fetch_guest_virt(struct x86_emulate_ctxt *ctxt,\n\t\t\t\tgva_t addr, void *val, unsigned int bytes,\n\t\t\t\tstruct x86_exception *exception)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tu64 access = (static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\tunsigned offset;\n\tint ret;\n\n\t/* Inline kvm_read_guest_virt_helper for speed.  */\n\tgpa_t gpa = mmu->gva_to_gpa(vcpu, mmu, addr, access|PFERR_FETCH_MASK,\n\t\t\t\t    exception);\n\tif (unlikely(gpa == INVALID_GPA))\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\n\toffset = addr & (PAGE_SIZE-1);\n\tif (WARN_ON(offset + bytes > PAGE_SIZE))\n\t\tbytes = (unsigned)PAGE_SIZE - offset;\n\tret = kvm_vcpu_read_guest_page(vcpu, gpa >> PAGE_SHIFT, val,\n\t\t\t\t       offset, bytes);\n\tif (unlikely(ret < 0))\n\t\treturn X86EMUL_IO_NEEDED;\n\n\treturn X86EMUL_CONTINUE;\n}\n\nint kvm_read_guest_virt(struct kvm_vcpu *vcpu,\n\t\t\t       gva_t addr, void *val, unsigned int bytes,\n\t\t\t       struct x86_exception *exception)\n{\n\tu64 access = (static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\n\t/*\n\t * FIXME: this should call handle_emulation_failure if X86EMUL_IO_NEEDED\n\t * is returned, but our callers are not ready for that and they blindly\n\t * call kvm_inject_page_fault.  Ensure that they at least do not leak\n\t * uninitialized kernel stack memory into cr2 and error code.\n\t */\n\tmemset(exception, 0, sizeof(*exception));\n\treturn kvm_read_guest_virt_helper(addr, val, bytes, vcpu, access,\n\t\t\t\t\t  exception);\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest_virt);\n\nstatic int emulator_read_std(struct x86_emulate_ctxt *ctxt,\n\t\t\t     gva_t addr, void *val, unsigned int bytes,\n\t\t\t     struct x86_exception *exception, bool system)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tu64 access = 0;\n\n\tif (system)\n\t\taccess |= PFERR_IMPLICIT_ACCESS;\n\telse if (static_call(kvm_x86_get_cpl)(vcpu) == 3)\n\t\taccess |= PFERR_USER_MASK;\n\n\treturn kvm_read_guest_virt_helper(addr, val, bytes, vcpu, access, exception);\n}\n\nstatic int kvm_write_guest_virt_helper(gva_t addr, void *val, unsigned int bytes,\n\t\t\t\t      struct kvm_vcpu *vcpu, u64 access,\n\t\t\t\t      struct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tvoid *data = val;\n\tint r = X86EMUL_CONTINUE;\n\n\twhile (bytes) {\n\t\tgpa_t gpa = mmu->gva_to_gpa(vcpu, mmu, addr, access, exception);\n\t\tunsigned offset = addr & (PAGE_SIZE-1);\n\t\tunsigned towrite = min(bytes, (unsigned)PAGE_SIZE - offset);\n\t\tint ret;\n\n\t\tif (gpa == INVALID_GPA)\n\t\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t\tret = kvm_vcpu_write_guest(vcpu, gpa, data, towrite);\n\t\tif (ret < 0) {\n\t\t\tr = X86EMUL_IO_NEEDED;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbytes -= towrite;\n\t\tdata += towrite;\n\t\taddr += towrite;\n\t}\nout:\n\treturn r;\n}\n\nstatic int emulator_write_std(struct x86_emulate_ctxt *ctxt, gva_t addr, void *val,\n\t\t\t      unsigned int bytes, struct x86_exception *exception,\n\t\t\t      bool system)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tu64 access = PFERR_WRITE_MASK;\n\n\tif (system)\n\t\taccess |= PFERR_IMPLICIT_ACCESS;\n\telse if (static_call(kvm_x86_get_cpl)(vcpu) == 3)\n\t\taccess |= PFERR_USER_MASK;\n\n\treturn kvm_write_guest_virt_helper(addr, val, bytes, vcpu,\n\t\t\t\t\t   access, exception);\n}\n\nint kvm_write_guest_virt_system(struct kvm_vcpu *vcpu, gva_t addr, void *val,\n\t\t\t\tunsigned int bytes, struct x86_exception *exception)\n{\n\t/* kvm_write_guest_virt_system can pull in tons of pages. */\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\treturn kvm_write_guest_virt_helper(addr, val, bytes, vcpu,\n\t\t\t\t\t   PFERR_WRITE_MASK, exception);\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_virt_system);\n\nstatic int kvm_can_emulate_insn(struct kvm_vcpu *vcpu, int emul_type,\n\t\t\t\tvoid *insn, int insn_len)\n{\n\treturn static_call(kvm_x86_can_emulate_instruction)(vcpu, emul_type,\n\t\t\t\t\t\t\t    insn, insn_len);\n}\n\nint handle_ud(struct kvm_vcpu *vcpu)\n{\n\tstatic const char kvm_emulate_prefix[] = { __KVM_EMULATE_PREFIX };\n\tint fep_flags = READ_ONCE(force_emulation_prefix);\n\tint emul_type = EMULTYPE_TRAP_UD;\n\tchar sig[5]; /* ud2; .ascii \"kvm\" */\n\tstruct x86_exception e;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emul_type, NULL, 0)))\n\t\treturn 1;\n\n\tif (fep_flags &&\n\t    kvm_read_guest_virt(vcpu, kvm_get_linear_rip(vcpu),\n\t\t\t\tsig, sizeof(sig), &e) == 0 &&\n\t    memcmp(sig, kvm_emulate_prefix, sizeof(sig)) == 0) {\n\t\tif (fep_flags & KVM_FEP_CLEAR_RFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, kvm_get_rflags(vcpu) & ~X86_EFLAGS_RF);\n\t\tkvm_rip_write(vcpu, kvm_rip_read(vcpu) + sizeof(sig));\n\t\temul_type = EMULTYPE_TRAP_UD_FORCED;\n\t}\n\n\treturn kvm_emulate_instruction(vcpu, emul_type);\n}\nEXPORT_SYMBOL_GPL(handle_ud);\n\nstatic int vcpu_is_mmio_gpa(struct kvm_vcpu *vcpu, unsigned long gva,\n\t\t\t    gpa_t gpa, bool write)\n{\n\t/* For APIC access vmexit */\n\tif ((gpa & PAGE_MASK) == APIC_DEFAULT_PHYS_BASE)\n\t\treturn 1;\n\n\tif (vcpu_match_mmio_gpa(vcpu, gpa)) {\n\t\ttrace_vcpu_match_mmio(gva, gpa, write, true);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int vcpu_mmio_gva_to_gpa(struct kvm_vcpu *vcpu, unsigned long gva,\n\t\t\t\tgpa_t *gpa, struct x86_exception *exception,\n\t\t\t\tbool write)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tu64 access = ((static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0)\n\t\t| (write ? PFERR_WRITE_MASK : 0);\n\n\t/*\n\t * currently PKRU is only applied to ept enabled guest so\n\t * there is no pkey in EPT page table for L1 guest or EPT\n\t * shadow page table for L2 guest.\n\t */\n\tif (vcpu_match_mmio_gva(vcpu, gva) && (!is_paging(vcpu) ||\n\t    !permission_fault(vcpu, vcpu->arch.walk_mmu,\n\t\t\t      vcpu->arch.mmio_access, 0, access))) {\n\t\t*gpa = vcpu->arch.mmio_gfn << PAGE_SHIFT |\n\t\t\t\t\t(gva & (PAGE_SIZE - 1));\n\t\ttrace_vcpu_match_mmio(gva, *gpa, write, false);\n\t\treturn 1;\n\t}\n\n\t*gpa = mmu->gva_to_gpa(vcpu, mmu, gva, access, exception);\n\n\tif (*gpa == INVALID_GPA)\n\t\treturn -1;\n\n\treturn vcpu_is_mmio_gpa(vcpu, gva, *gpa, write);\n}\n\nint emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\tconst void *val, int bytes)\n{\n\tint ret;\n\n\tret = kvm_vcpu_write_guest(vcpu, gpa, val, bytes);\n\tif (ret < 0)\n\t\treturn 0;\n\tkvm_page_track_write(vcpu, gpa, val, bytes);\n\treturn 1;\n}\n\nstruct read_write_emulator_ops {\n\tint (*read_write_prepare)(struct kvm_vcpu *vcpu, void *val,\n\t\t\t\t  int bytes);\n\tint (*read_write_emulate)(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t\t  void *val, int bytes);\n\tint (*read_write_mmio)(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t       int bytes, void *val);\n\tint (*read_write_exit_mmio)(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t\t    void *val, int bytes);\n\tbool write;\n};\n\nstatic int read_prepare(struct kvm_vcpu *vcpu, void *val, int bytes)\n{\n\tif (vcpu->mmio_read_completed) {\n\t\ttrace_kvm_mmio(KVM_TRACE_MMIO_READ, bytes,\n\t\t\t       vcpu->mmio_fragments[0].gpa, val);\n\t\tvcpu->mmio_read_completed = 0;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int read_emulate(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\tvoid *val, int bytes)\n{\n\treturn !kvm_vcpu_read_guest(vcpu, gpa, val, bytes);\n}\n\nstatic int write_emulate(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t void *val, int bytes)\n{\n\treturn emulator_write_phys(vcpu, gpa, val, bytes);\n}\n\nstatic int write_mmio(struct kvm_vcpu *vcpu, gpa_t gpa, int bytes, void *val)\n{\n\ttrace_kvm_mmio(KVM_TRACE_MMIO_WRITE, bytes, gpa, val);\n\treturn vcpu_mmio_write(vcpu, gpa, bytes, val);\n}\n\nstatic int read_exit_mmio(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t  void *val, int bytes)\n{\n\ttrace_kvm_mmio(KVM_TRACE_MMIO_READ_UNSATISFIED, bytes, gpa, NULL);\n\treturn X86EMUL_IO_NEEDED;\n}\n\nstatic int write_exit_mmio(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t   void *val, int bytes)\n{\n\tstruct kvm_mmio_fragment *frag = &vcpu->mmio_fragments[0];\n\n\tmemcpy(vcpu->run->mmio.data, frag->data, min(8u, frag->len));\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic const struct read_write_emulator_ops read_emultor = {\n\t.read_write_prepare = read_prepare,\n\t.read_write_emulate = read_emulate,\n\t.read_write_mmio = vcpu_mmio_read,\n\t.read_write_exit_mmio = read_exit_mmio,\n};\n\nstatic const struct read_write_emulator_ops write_emultor = {\n\t.read_write_emulate = write_emulate,\n\t.read_write_mmio = write_mmio,\n\t.read_write_exit_mmio = write_exit_mmio,\n\t.write = true,\n};\n\nstatic int emulator_read_write_onepage(unsigned long addr, void *val,\n\t\t\t\t       unsigned int bytes,\n\t\t\t\t       struct x86_exception *exception,\n\t\t\t\t       struct kvm_vcpu *vcpu,\n\t\t\t\t       const struct read_write_emulator_ops *ops)\n{\n\tgpa_t gpa;\n\tint handled, ret;\n\tbool write = ops->write;\n\tstruct kvm_mmio_fragment *frag;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\n\t/*\n\t * If the exit was due to a NPF we may already have a GPA.\n\t * If the GPA is present, use it to avoid the GVA to GPA table walk.\n\t * Note, this cannot be used on string operations since string\n\t * operation using rep will only have the initial GPA from the NPF\n\t * occurred.\n\t */\n\tif (ctxt->gpa_available && emulator_can_use_gpa(ctxt) &&\n\t    (addr & ~PAGE_MASK) == (ctxt->gpa_val & ~PAGE_MASK)) {\n\t\tgpa = ctxt->gpa_val;\n\t\tret = vcpu_is_mmio_gpa(vcpu, addr, gpa, write);\n\t} else {\n\t\tret = vcpu_mmio_gva_to_gpa(vcpu, addr, &gpa, exception, write);\n\t\tif (ret < 0)\n\t\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\tif (!ret && ops->read_write_emulate(vcpu, gpa, val, bytes))\n\t\treturn X86EMUL_CONTINUE;\n\n\t/*\n\t * Is this MMIO handled locally?\n\t */\n\thandled = ops->read_write_mmio(vcpu, gpa, bytes, val);\n\tif (handled == bytes)\n\t\treturn X86EMUL_CONTINUE;\n\n\tgpa += handled;\n\tbytes -= handled;\n\tval += handled;\n\n\tWARN_ON(vcpu->mmio_nr_fragments >= KVM_MAX_MMIO_FRAGMENTS);\n\tfrag = &vcpu->mmio_fragments[vcpu->mmio_nr_fragments++];\n\tfrag->gpa = gpa;\n\tfrag->data = val;\n\tfrag->len = bytes;\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic int emulator_read_write(struct x86_emulate_ctxt *ctxt,\n\t\t\tunsigned long addr,\n\t\t\tvoid *val, unsigned int bytes,\n\t\t\tstruct x86_exception *exception,\n\t\t\tconst struct read_write_emulator_ops *ops)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tgpa_t gpa;\n\tint rc;\n\n\tif (ops->read_write_prepare &&\n\t\t  ops->read_write_prepare(vcpu, val, bytes))\n\t\treturn X86EMUL_CONTINUE;\n\n\tvcpu->mmio_nr_fragments = 0;\n\n\t/* Crossing a page boundary? */\n\tif (((addr + bytes - 1) ^ addr) & PAGE_MASK) {\n\t\tint now;\n\n\t\tnow = -addr & ~PAGE_MASK;\n\t\trc = emulator_read_write_onepage(addr, val, now, exception,\n\t\t\t\t\t\t vcpu, ops);\n\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t\taddr += now;\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\taddr = (u32)addr;\n\t\tval += now;\n\t\tbytes -= now;\n\t}\n\n\trc = emulator_read_write_onepage(addr, val, bytes, exception,\n\t\t\t\t\t vcpu, ops);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (!vcpu->mmio_nr_fragments)\n\t\treturn rc;\n\n\tgpa = vcpu->mmio_fragments[0].gpa;\n\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_cur_fragment = 0;\n\n\tvcpu->run->mmio.len = min(8u, vcpu->mmio_fragments[0].len);\n\tvcpu->run->mmio.is_write = vcpu->mmio_is_write = ops->write;\n\tvcpu->run->exit_reason = KVM_EXIT_MMIO;\n\tvcpu->run->mmio.phys_addr = gpa;\n\n\treturn ops->read_write_exit_mmio(vcpu, gpa, val, bytes);\n}\n\nstatic int emulator_read_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  unsigned long addr,\n\t\t\t\t  void *val,\n\t\t\t\t  unsigned int bytes,\n\t\t\t\t  struct x86_exception *exception)\n{\n\treturn emulator_read_write(ctxt, addr, val, bytes,\n\t\t\t\t   exception, &read_emultor);\n}\n\nstatic int emulator_write_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t    unsigned long addr,\n\t\t\t    const void *val,\n\t\t\t    unsigned int bytes,\n\t\t\t    struct x86_exception *exception)\n{\n\treturn emulator_read_write(ctxt, addr, (void *)val, bytes,\n\t\t\t\t   exception, &write_emultor);\n}\n\n#define emulator_try_cmpxchg_user(t, ptr, old, new) \\\n\t(__try_cmpxchg_user((t __user *)(ptr), (t *)(old), *(t *)(new), efault ## t))\n\nstatic int emulator_cmpxchg_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t     unsigned long addr,\n\t\t\t\t     const void *old,\n\t\t\t\t     const void *new,\n\t\t\t\t     unsigned int bytes,\n\t\t\t\t     struct x86_exception *exception)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tu64 page_line_mask;\n\tunsigned long hva;\n\tgpa_t gpa;\n\tint r;\n\n\t/* guests cmpxchg8b have to be emulated atomically */\n\tif (bytes > 8 || (bytes & (bytes - 1)))\n\t\tgoto emul_write;\n\n\tgpa = kvm_mmu_gva_to_gpa_write(vcpu, addr, NULL);\n\n\tif (gpa == INVALID_GPA ||\n\t    (gpa & PAGE_MASK) == APIC_DEFAULT_PHYS_BASE)\n\t\tgoto emul_write;\n\n\t/*\n\t * Emulate the atomic as a straight write to avoid #AC if SLD is\n\t * enabled in the host and the access splits a cache line.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_SPLIT_LOCK_DETECT))\n\t\tpage_line_mask = ~(cache_line_size() - 1);\n\telse\n\t\tpage_line_mask = PAGE_MASK;\n\n\tif (((gpa + bytes - 1) & page_line_mask) != (gpa & page_line_mask))\n\t\tgoto emul_write;\n\n\thva = kvm_vcpu_gfn_to_hva(vcpu, gpa_to_gfn(gpa));\n\tif (kvm_is_error_hva(hva))\n\t\tgoto emul_write;\n\n\thva += offset_in_page(gpa);\n\n\tswitch (bytes) {\n\tcase 1:\n\t\tr = emulator_try_cmpxchg_user(u8, hva, old, new);\n\t\tbreak;\n\tcase 2:\n\t\tr = emulator_try_cmpxchg_user(u16, hva, old, new);\n\t\tbreak;\n\tcase 4:\n\t\tr = emulator_try_cmpxchg_user(u32, hva, old, new);\n\t\tbreak;\n\tcase 8:\n\t\tr = emulator_try_cmpxchg_user(u64, hva, old, new);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tif (r < 0)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\tif (r)\n\t\treturn X86EMUL_CMPXCHG_FAILED;\n\n\tkvm_page_track_write(vcpu, gpa, new, bytes);\n\n\treturn X86EMUL_CONTINUE;\n\nemul_write:\n\tprintk_once(KERN_WARNING \"kvm: emulating exchange as write\\n\");\n\n\treturn emulator_write_emulated(ctxt, addr, new, bytes, exception);\n}\n\nstatic int emulator_pio_in_out(struct kvm_vcpu *vcpu, int size,\n\t\t\t       unsigned short port, void *data,\n\t\t\t       unsigned int count, bool in)\n{\n\tunsigned i;\n\tint r;\n\n\tWARN_ON_ONCE(vcpu->arch.pio.count);\n\tfor (i = 0; i < count; i++) {\n\t\tif (in)\n\t\t\tr = kvm_io_bus_read(vcpu, KVM_PIO_BUS, port, size, data);\n\t\telse\n\t\t\tr = kvm_io_bus_write(vcpu, KVM_PIO_BUS, port, size, data);\n\n\t\tif (r) {\n\t\t\tif (i == 0)\n\t\t\t\tgoto userspace_io;\n\n\t\t\t/*\n\t\t\t * Userspace must have unregistered the device while PIO\n\t\t\t * was running.  Drop writes / read as 0.\n\t\t\t */\n\t\t\tif (in)\n\t\t\t\tmemset(data, 0, size * (count - i));\n\t\t\tbreak;\n\t\t}\n\n\t\tdata += size;\n\t}\n\treturn 1;\n\nuserspace_io:\n\tvcpu->arch.pio.port = port;\n\tvcpu->arch.pio.in = in;\n\tvcpu->arch.pio.count = count;\n\tvcpu->arch.pio.size = size;\n\n\tif (in)\n\t\tmemset(vcpu->arch.pio_data, 0, size * count);\n\telse\n\t\tmemcpy(vcpu->arch.pio_data, data, size * count);\n\n\tvcpu->run->exit_reason = KVM_EXIT_IO;\n\tvcpu->run->io.direction = in ? KVM_EXIT_IO_IN : KVM_EXIT_IO_OUT;\n\tvcpu->run->io.size = size;\n\tvcpu->run->io.data_offset = KVM_PIO_PAGE_OFFSET * PAGE_SIZE;\n\tvcpu->run->io.count = count;\n\tvcpu->run->io.port = port;\n\treturn 0;\n}\n\nstatic int emulator_pio_in(struct kvm_vcpu *vcpu, int size,\n      \t\t\t   unsigned short port, void *val, unsigned int count)\n{\n\tint r = emulator_pio_in_out(vcpu, size, port, val, count, true);\n\tif (r)\n\t\ttrace_kvm_pio(KVM_PIO_IN, port, size, count, val);\n\n\treturn r;\n}\n\nstatic void complete_emulator_pio_in(struct kvm_vcpu *vcpu, void *val)\n{\n\tint size = vcpu->arch.pio.size;\n\tunsigned int count = vcpu->arch.pio.count;\n\tmemcpy(val, vcpu->arch.pio_data, size * count);\n\ttrace_kvm_pio(KVM_PIO_IN, vcpu->arch.pio.port, size, count, vcpu->arch.pio_data);\n\tvcpu->arch.pio.count = 0;\n}\n\nstatic int emulator_pio_in_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t    int size, unsigned short port, void *val,\n\t\t\t\t    unsigned int count)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tif (vcpu->arch.pio.count) {\n\t\t/*\n\t\t * Complete a previous iteration that required userspace I/O.\n\t\t * Note, @count isn't guaranteed to match pio.count as userspace\n\t\t * can modify ECX before rerunning the vCPU.  Ignore any such\n\t\t * shenanigans as KVM doesn't support modifying the rep count,\n\t\t * and the emulator ensures @count doesn't overflow the buffer.\n\t\t */\n\t\tcomplete_emulator_pio_in(vcpu, val);\n\t\treturn 1;\n\t}\n\n\treturn emulator_pio_in(vcpu, size, port, val, count);\n}\n\nstatic int emulator_pio_out(struct kvm_vcpu *vcpu, int size,\n\t\t\t    unsigned short port, const void *val,\n\t\t\t    unsigned int count)\n{\n\ttrace_kvm_pio(KVM_PIO_OUT, port, size, count, val);\n\treturn emulator_pio_in_out(vcpu, size, port, (void *)val, count, false);\n}\n\nstatic int emulator_pio_out_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t     int size, unsigned short port,\n\t\t\t\t     const void *val, unsigned int count)\n{\n\treturn emulator_pio_out(emul_to_vcpu(ctxt), size, port, val, count);\n}\n\nstatic unsigned long get_segment_base(struct kvm_vcpu *vcpu, int seg)\n{\n\treturn static_call(kvm_x86_get_segment_base)(vcpu, seg);\n}\n\nstatic void emulator_invlpg(struct x86_emulate_ctxt *ctxt, ulong address)\n{\n\tkvm_mmu_invlpg(emul_to_vcpu(ctxt), address);\n}\n\nstatic int kvm_emulate_wbinvd_noskip(struct kvm_vcpu *vcpu)\n{\n\tif (!need_emulate_wbinvd(vcpu))\n\t\treturn X86EMUL_CONTINUE;\n\n\tif (static_call(kvm_x86_has_wbinvd_exit)()) {\n\t\tint cpu = get_cpu();\n\n\t\tcpumask_set_cpu(cpu, vcpu->arch.wbinvd_dirty_mask);\n\t\ton_each_cpu_mask(vcpu->arch.wbinvd_dirty_mask,\n\t\t\t\twbinvd_ipi, NULL, 1);\n\t\tput_cpu();\n\t\tcpumask_clear(vcpu->arch.wbinvd_dirty_mask);\n\t} else\n\t\twbinvd();\n\treturn X86EMUL_CONTINUE;\n}\n\nint kvm_emulate_wbinvd(struct kvm_vcpu *vcpu)\n{\n\tkvm_emulate_wbinvd_noskip(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_wbinvd);\n\n\n\nstatic void emulator_wbinvd(struct x86_emulate_ctxt *ctxt)\n{\n\tkvm_emulate_wbinvd_noskip(emul_to_vcpu(ctxt));\n}\n\nstatic void emulator_get_dr(struct x86_emulate_ctxt *ctxt, int dr,\n\t\t\t    unsigned long *dest)\n{\n\tkvm_get_dr(emul_to_vcpu(ctxt), dr, dest);\n}\n\nstatic int emulator_set_dr(struct x86_emulate_ctxt *ctxt, int dr,\n\t\t\t   unsigned long value)\n{\n\n\treturn kvm_set_dr(emul_to_vcpu(ctxt), dr, value);\n}\n\nstatic u64 mk_cr_64(u64 curr_cr, u32 new_val)\n{\n\treturn (curr_cr & ~((1ULL << 32) - 1)) | new_val;\n}\n\nstatic unsigned long emulator_get_cr(struct x86_emulate_ctxt *ctxt, int cr)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tunsigned long value;\n\n\tswitch (cr) {\n\tcase 0:\n\t\tvalue = kvm_read_cr0(vcpu);\n\t\tbreak;\n\tcase 2:\n\t\tvalue = vcpu->arch.cr2;\n\t\tbreak;\n\tcase 3:\n\t\tvalue = kvm_read_cr3(vcpu);\n\t\tbreak;\n\tcase 4:\n\t\tvalue = kvm_read_cr4(vcpu);\n\t\tbreak;\n\tcase 8:\n\t\tvalue = kvm_get_cr8(vcpu);\n\t\tbreak;\n\tdefault:\n\t\tkvm_err(\"%s: unexpected cr %u\\n\", __func__, cr);\n\t\treturn 0;\n\t}\n\n\treturn value;\n}\n\nstatic int emulator_set_cr(struct x86_emulate_ctxt *ctxt, int cr, ulong val)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tint res = 0;\n\n\tswitch (cr) {\n\tcase 0:\n\t\tres = kvm_set_cr0(vcpu, mk_cr_64(kvm_read_cr0(vcpu), val));\n\t\tbreak;\n\tcase 2:\n\t\tvcpu->arch.cr2 = val;\n\t\tbreak;\n\tcase 3:\n\t\tres = kvm_set_cr3(vcpu, val);\n\t\tbreak;\n\tcase 4:\n\t\tres = kvm_set_cr4(vcpu, mk_cr_64(kvm_read_cr4(vcpu), val));\n\t\tbreak;\n\tcase 8:\n\t\tres = kvm_set_cr8(vcpu, val);\n\t\tbreak;\n\tdefault:\n\t\tkvm_err(\"%s: unexpected cr %u\\n\", __func__, cr);\n\t\tres = -1;\n\t}\n\n\treturn res;\n}\n\nstatic int emulator_get_cpl(struct x86_emulate_ctxt *ctxt)\n{\n\treturn static_call(kvm_x86_get_cpl)(emul_to_vcpu(ctxt));\n}\n\nstatic void emulator_get_gdt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tstatic_call(kvm_x86_get_gdt)(emul_to_vcpu(ctxt), dt);\n}\n\nstatic void emulator_get_idt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tstatic_call(kvm_x86_get_idt)(emul_to_vcpu(ctxt), dt);\n}\n\nstatic void emulator_set_gdt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tstatic_call(kvm_x86_set_gdt)(emul_to_vcpu(ctxt), dt);\n}\n\nstatic void emulator_set_idt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tstatic_call(kvm_x86_set_idt)(emul_to_vcpu(ctxt), dt);\n}\n\nstatic unsigned long emulator_get_cached_segment_base(\n\tstruct x86_emulate_ctxt *ctxt, int seg)\n{\n\treturn get_segment_base(emul_to_vcpu(ctxt), seg);\n}\n\nstatic bool emulator_get_segment(struct x86_emulate_ctxt *ctxt, u16 *selector,\n\t\t\t\t struct desc_struct *desc, u32 *base3,\n\t\t\t\t int seg)\n{\n\tstruct kvm_segment var;\n\n\tkvm_get_segment(emul_to_vcpu(ctxt), &var, seg);\n\t*selector = var.selector;\n\n\tif (var.unusable) {\n\t\tmemset(desc, 0, sizeof(*desc));\n\t\tif (base3)\n\t\t\t*base3 = 0;\n\t\treturn false;\n\t}\n\n\tif (var.g)\n\t\tvar.limit >>= 12;\n\tset_desc_limit(desc, var.limit);\n\tset_desc_base(desc, (unsigned long)var.base);\n#ifdef CONFIG_X86_64\n\tif (base3)\n\t\t*base3 = var.base >> 32;\n#endif\n\tdesc->type = var.type;\n\tdesc->s = var.s;\n\tdesc->dpl = var.dpl;\n\tdesc->p = var.present;\n\tdesc->avl = var.avl;\n\tdesc->l = var.l;\n\tdesc->d = var.db;\n\tdesc->g = var.g;\n\n\treturn true;\n}\n\nstatic void emulator_set_segment(struct x86_emulate_ctxt *ctxt, u16 selector,\n\t\t\t\t struct desc_struct *desc, u32 base3,\n\t\t\t\t int seg)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tstruct kvm_segment var;\n\n\tvar.selector = selector;\n\tvar.base = get_desc_base(desc);\n#ifdef CONFIG_X86_64\n\tvar.base |= ((u64)base3) << 32;\n#endif\n\tvar.limit = get_desc_limit(desc);\n\tif (desc->g)\n\t\tvar.limit = (var.limit << 12) | 0xfff;\n\tvar.type = desc->type;\n\tvar.dpl = desc->dpl;\n\tvar.db = desc->d;\n\tvar.s = desc->s;\n\tvar.l = desc->l;\n\tvar.g = desc->g;\n\tvar.avl = desc->avl;\n\tvar.present = desc->p;\n\tvar.unusable = !var.present;\n\tvar.padding = 0;\n\n\tkvm_set_segment(vcpu, &var, seg);\n\treturn;\n}\n\nstatic int emulator_get_msr_with_filter(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\tu32 msr_index, u64 *pdata)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tint r;\n\n\tr = kvm_get_msr_with_filter(vcpu, msr_index, pdata);\n\tif (r < 0)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tif (r) {\n\t\tif (kvm_msr_user_space(vcpu, msr_index, KVM_EXIT_X86_RDMSR, 0,\n\t\t\t\t       complete_emulated_rdmsr, r))\n\t\t\treturn X86EMUL_IO_NEEDED;\n\n\t\ttrace_kvm_msr_read_ex(msr_index);\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\ttrace_kvm_msr_read(msr_index, *pdata);\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic int emulator_set_msr_with_filter(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\tu32 msr_index, u64 data)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tint r;\n\n\tr = kvm_set_msr_with_filter(vcpu, msr_index, data);\n\tif (r < 0)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tif (r) {\n\t\tif (kvm_msr_user_space(vcpu, msr_index, KVM_EXIT_X86_WRMSR, data,\n\t\t\t\t       complete_emulated_msr_access, r))\n\t\t\treturn X86EMUL_IO_NEEDED;\n\n\t\ttrace_kvm_msr_write_ex(msr_index, data);\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\ttrace_kvm_msr_write(msr_index, data);\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic int emulator_get_msr(struct x86_emulate_ctxt *ctxt,\n\t\t\t    u32 msr_index, u64 *pdata)\n{\n\treturn kvm_get_msr(emul_to_vcpu(ctxt), msr_index, pdata);\n}\n\nstatic int emulator_check_pmc(struct x86_emulate_ctxt *ctxt,\n\t\t\t      u32 pmc)\n{\n\tif (kvm_pmu_is_valid_rdpmc_ecx(emul_to_vcpu(ctxt), pmc))\n\t\treturn 0;\n\treturn -EINVAL;\n}\n\nstatic int emulator_read_pmc(struct x86_emulate_ctxt *ctxt,\n\t\t\t     u32 pmc, u64 *pdata)\n{\n\treturn kvm_pmu_rdpmc(emul_to_vcpu(ctxt), pmc, pdata);\n}\n\nstatic void emulator_halt(struct x86_emulate_ctxt *ctxt)\n{\n\temul_to_vcpu(ctxt)->arch.halt_request = 1;\n}\n\nstatic int emulator_intercept(struct x86_emulate_ctxt *ctxt,\n\t\t\t      struct x86_instruction_info *info,\n\t\t\t      enum x86_intercept_stage stage)\n{\n\treturn static_call(kvm_x86_check_intercept)(emul_to_vcpu(ctxt), info, stage,\n\t\t\t\t\t    &ctxt->exception);\n}\n\nstatic bool emulator_get_cpuid(struct x86_emulate_ctxt *ctxt,\n\t\t\t      u32 *eax, u32 *ebx, u32 *ecx, u32 *edx,\n\t\t\t      bool exact_only)\n{\n\treturn kvm_cpuid(emul_to_vcpu(ctxt), eax, ebx, ecx, edx, exact_only);\n}\n\nstatic bool emulator_guest_has_long_mode(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_LM);\n}\n\nstatic bool emulator_guest_has_movbe(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_MOVBE);\n}\n\nstatic bool emulator_guest_has_fxsr(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_FXSR);\n}\n\nstatic bool emulator_guest_has_rdpid(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_RDPID);\n}\n\nstatic ulong emulator_read_gpr(struct x86_emulate_ctxt *ctxt, unsigned reg)\n{\n\treturn kvm_register_read_raw(emul_to_vcpu(ctxt), reg);\n}\n\nstatic void emulator_write_gpr(struct x86_emulate_ctxt *ctxt, unsigned reg, ulong val)\n{\n\tkvm_register_write_raw(emul_to_vcpu(ctxt), reg, val);\n}\n\nstatic void emulator_set_nmi_mask(struct x86_emulate_ctxt *ctxt, bool masked)\n{\n\tstatic_call(kvm_x86_set_nmi_mask)(emul_to_vcpu(ctxt), masked);\n}\n\nstatic unsigned emulator_get_hflags(struct x86_emulate_ctxt *ctxt)\n{\n\treturn emul_to_vcpu(ctxt)->arch.hflags;\n}\n\n#ifndef CONFIG_KVM_SMM\nstatic int emulator_leave_smm(struct x86_emulate_ctxt *ctxt)\n{\n\tWARN_ON_ONCE(1);\n\treturn X86EMUL_UNHANDLEABLE;\n}\n#endif\n\nstatic void emulator_triple_fault(struct x86_emulate_ctxt *ctxt)\n{\n\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, emul_to_vcpu(ctxt));\n}\n\nstatic int emulator_set_xcr(struct x86_emulate_ctxt *ctxt, u32 index, u64 xcr)\n{\n\treturn __kvm_set_xcr(emul_to_vcpu(ctxt), index, xcr);\n}\n\nstatic void emulator_vm_bugged(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct kvm *kvm = emul_to_vcpu(ctxt)->kvm;\n\n\tif (!kvm->vm_bugged)\n\t\tkvm_vm_bugged(kvm);\n}\n\nstatic const struct x86_emulate_ops emulate_ops = {\n\t.vm_bugged           = emulator_vm_bugged,\n\t.read_gpr            = emulator_read_gpr,\n\t.write_gpr           = emulator_write_gpr,\n\t.read_std            = emulator_read_std,\n\t.write_std           = emulator_write_std,\n\t.fetch               = kvm_fetch_guest_virt,\n\t.read_emulated       = emulator_read_emulated,\n\t.write_emulated      = emulator_write_emulated,\n\t.cmpxchg_emulated    = emulator_cmpxchg_emulated,\n\t.invlpg              = emulator_invlpg,\n\t.pio_in_emulated     = emulator_pio_in_emulated,\n\t.pio_out_emulated    = emulator_pio_out_emulated,\n\t.get_segment         = emulator_get_segment,\n\t.set_segment         = emulator_set_segment,\n\t.get_cached_segment_base = emulator_get_cached_segment_base,\n\t.get_gdt             = emulator_get_gdt,\n\t.get_idt\t     = emulator_get_idt,\n\t.set_gdt             = emulator_set_gdt,\n\t.set_idt\t     = emulator_set_idt,\n\t.get_cr              = emulator_get_cr,\n\t.set_cr              = emulator_set_cr,\n\t.cpl                 = emulator_get_cpl,\n\t.get_dr              = emulator_get_dr,\n\t.set_dr              = emulator_set_dr,\n\t.set_msr_with_filter = emulator_set_msr_with_filter,\n\t.get_msr_with_filter = emulator_get_msr_with_filter,\n\t.get_msr             = emulator_get_msr,\n\t.check_pmc\t     = emulator_check_pmc,\n\t.read_pmc            = emulator_read_pmc,\n\t.halt                = emulator_halt,\n\t.wbinvd              = emulator_wbinvd,\n\t.fix_hypercall       = emulator_fix_hypercall,\n\t.intercept           = emulator_intercept,\n\t.get_cpuid           = emulator_get_cpuid,\n\t.guest_has_long_mode = emulator_guest_has_long_mode,\n\t.guest_has_movbe     = emulator_guest_has_movbe,\n\t.guest_has_fxsr      = emulator_guest_has_fxsr,\n\t.guest_has_rdpid     = emulator_guest_has_rdpid,\n\t.set_nmi_mask        = emulator_set_nmi_mask,\n\t.get_hflags          = emulator_get_hflags,\n\t.leave_smm           = emulator_leave_smm,\n\t.triple_fault        = emulator_triple_fault,\n\t.set_xcr             = emulator_set_xcr,\n};\n\nstatic void toggle_interruptibility(struct kvm_vcpu *vcpu, u32 mask)\n{\n\tu32 int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);\n\t/*\n\t * an sti; sti; sequence only disable interrupts for the first\n\t * instruction. So, if the last instruction, be it emulated or\n\t * not, left the system with the INT_STI flag enabled, it\n\t * means that the last instruction is an sti. We should not\n\t * leave the flag on in this case. The same goes for mov ss\n\t */\n\tif (int_shadow & mask)\n\t\tmask = 0;\n\tif (unlikely(int_shadow || mask)) {\n\t\tstatic_call(kvm_x86_set_interrupt_shadow)(vcpu, mask);\n\t\tif (!mask)\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t}\n}\n\nstatic void inject_emulated_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\n\tif (ctxt->exception.vector == PF_VECTOR)\n\t\tkvm_inject_emulated_page_fault(vcpu, &ctxt->exception);\n\telse if (ctxt->exception.error_code_valid)\n\t\tkvm_queue_exception_e(vcpu, ctxt->exception.vector,\n\t\t\t\t      ctxt->exception.error_code);\n\telse\n\t\tkvm_queue_exception(vcpu, ctxt->exception.vector);\n}\n\nstatic struct x86_emulate_ctxt *alloc_emulate_ctxt(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt;\n\n\tctxt = kmem_cache_zalloc(x86_emulator_cache, GFP_KERNEL_ACCOUNT);\n\tif (!ctxt) {\n\t\tpr_err(\"kvm: failed to allocate vcpu's emulator\\n\");\n\t\treturn NULL;\n\t}\n\n\tctxt->vcpu = vcpu;\n\tctxt->ops = &emulate_ops;\n\tvcpu->arch.emulate_ctxt = ctxt;\n\n\treturn ctxt;\n}\n\nstatic void init_emulate_ctxt(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint cs_db, cs_l;\n\n\tstatic_call(kvm_x86_get_cs_db_l_bits)(vcpu, &cs_db, &cs_l);\n\n\tctxt->gpa_available = false;\n\tctxt->eflags = kvm_get_rflags(vcpu);\n\tctxt->tf = (ctxt->eflags & X86_EFLAGS_TF) != 0;\n\n\tctxt->eip = kvm_rip_read(vcpu);\n\tctxt->mode = (!is_protmode(vcpu))\t\t? X86EMUL_MODE_REAL :\n\t\t     (ctxt->eflags & X86_EFLAGS_VM)\t? X86EMUL_MODE_VM86 :\n\t\t     (cs_l && is_long_mode(vcpu))\t? X86EMUL_MODE_PROT64 :\n\t\t     cs_db\t\t\t\t? X86EMUL_MODE_PROT32 :\n\t\t\t\t\t\t\t  X86EMUL_MODE_PROT16;\n\tBUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);\n\n\tctxt->interruptibility = 0;\n\tctxt->have_exception = false;\n\tctxt->exception.vector = -1;\n\tctxt->perm_ok = false;\n\n\tinit_decode_cache(ctxt);\n\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n}\n\nvoid kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint ret;\n\n\tinit_emulate_ctxt(vcpu);\n\n\tctxt->op_bytes = 2;\n\tctxt->ad_bytes = 2;\n\tctxt->_eip = ctxt->eip + inc_eip;\n\tret = emulate_int_real(ctxt, irq);\n\n\tif (ret != X86EMUL_CONTINUE) {\n\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t} else {\n\t\tctxt->eip = ctxt->_eip;\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tkvm_set_rflags(vcpu, ctxt->eflags);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_inject_realmode_interrupt);\n\nstatic void prepare_emulation_failure_exit(struct kvm_vcpu *vcpu, u64 *data,\n\t\t\t\t\t   u8 ndata, u8 *insn_bytes, u8 insn_size)\n{\n\tstruct kvm_run *run = vcpu->run;\n\tu64 info[5];\n\tu8 info_start;\n\n\t/*\n\t * Zero the whole array used to retrieve the exit info, as casting to\n\t * u32 for select entries will leave some chunks uninitialized.\n\t */\n\tmemset(&info, 0, sizeof(info));\n\n\tstatic_call(kvm_x86_get_exit_info)(vcpu, (u32 *)&info[0], &info[1],\n\t\t\t\t\t   &info[2], (u32 *)&info[3],\n\t\t\t\t\t   (u32 *)&info[4]);\n\n\trun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\trun->emulation_failure.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\n\t/*\n\t * There's currently space for 13 entries, but 5 are used for the exit\n\t * reason and info.  Restrict to 4 to reduce the maintenance burden\n\t * when expanding kvm_run.emulation_failure in the future.\n\t */\n\tif (WARN_ON_ONCE(ndata > 4))\n\t\tndata = 4;\n\n\t/* Always include the flags as a 'data' entry. */\n\tinfo_start = 1;\n\trun->emulation_failure.flags = 0;\n\n\tif (insn_size) {\n\t\tBUILD_BUG_ON((sizeof(run->emulation_failure.insn_size) +\n\t\t\t      sizeof(run->emulation_failure.insn_bytes) != 16));\n\t\tinfo_start += 2;\n\t\trun->emulation_failure.flags |=\n\t\t\tKVM_INTERNAL_ERROR_EMULATION_FLAG_INSTRUCTION_BYTES;\n\t\trun->emulation_failure.insn_size = insn_size;\n\t\tmemset(run->emulation_failure.insn_bytes, 0x90,\n\t\t       sizeof(run->emulation_failure.insn_bytes));\n\t\tmemcpy(run->emulation_failure.insn_bytes, insn_bytes, insn_size);\n\t}\n\n\tmemcpy(&run->internal.data[info_start], info, sizeof(info));\n\tmemcpy(&run->internal.data[info_start + ARRAY_SIZE(info)], data,\n\t       ndata * sizeof(data[0]));\n\n\trun->emulation_failure.ndata = info_start + ARRAY_SIZE(info) + ndata;\n}\n\nstatic void prepare_emulation_ctxt_failure_exit(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\n\tprepare_emulation_failure_exit(vcpu, NULL, 0, ctxt->fetch.data,\n\t\t\t\t       ctxt->fetch.end - ctxt->fetch.data);\n}\n\nvoid __kvm_prepare_emulation_failure_exit(struct kvm_vcpu *vcpu, u64 *data,\n\t\t\t\t\t  u8 ndata)\n{\n\tprepare_emulation_failure_exit(vcpu, data, ndata, NULL, 0);\n}\nEXPORT_SYMBOL_GPL(__kvm_prepare_emulation_failure_exit);\n\nvoid kvm_prepare_emulation_failure_exit(struct kvm_vcpu *vcpu)\n{\n\t__kvm_prepare_emulation_failure_exit(vcpu, NULL, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_prepare_emulation_failure_exit);\n\nstatic int handle_emulation_failure(struct kvm_vcpu *vcpu, int emulation_type)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\n\t++vcpu->stat.insn_emulation_fail;\n\ttrace_kvm_emulate_insn_failed(vcpu);\n\n\tif (emulation_type & EMULTYPE_VMWARE_GP) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\tif (kvm->arch.exit_on_emulation_error ||\n\t    (emulation_type & EMULTYPE_SKIP)) {\n\t\tprepare_emulation_ctxt_failure_exit(vcpu);\n\t\treturn 0;\n\t}\n\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\n\tif (!is_guest_mode(vcpu) && static_call(kvm_x86_get_cpl)(vcpu) == 0) {\n\t\tprepare_emulation_ctxt_failure_exit(vcpu);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic bool reexecute_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t\t  bool write_fault_to_shadow_pgtable,\n\t\t\t\t  int emulation_type)\n{\n\tgpa_t gpa = cr2_or_gpa;\n\tkvm_pfn_t pfn;\n\n\tif (!(emulation_type & EMULTYPE_ALLOW_RETRY_PF))\n\t\treturn false;\n\n\tif (WARN_ON_ONCE(is_guest_mode(vcpu)) ||\n\t    WARN_ON_ONCE(!(emulation_type & EMULTYPE_PF)))\n\t\treturn false;\n\n\tif (!vcpu->arch.mmu->root_role.direct) {\n\t\t/*\n\t\t * Write permission should be allowed since only\n\t\t * write access need to be emulated.\n\t\t */\n\t\tgpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2_or_gpa, NULL);\n\n\t\t/*\n\t\t * If the mapping is invalid in guest, let cpu retry\n\t\t * it to generate fault.\n\t\t */\n\t\tif (gpa == INVALID_GPA)\n\t\t\treturn true;\n\t}\n\n\t/*\n\t * Do not retry the unhandleable instruction if it faults on the\n\t * readonly host memory, otherwise it will goto a infinite loop:\n\t * retry instruction -> write #PF -> emulation fail -> retry\n\t * instruction -> ...\n\t */\n\tpfn = gfn_to_pfn(vcpu->kvm, gpa_to_gfn(gpa));\n\n\t/*\n\t * If the instruction failed on the error pfn, it can not be fixed,\n\t * report the error to userspace.\n\t */\n\tif (is_error_noslot_pfn(pfn))\n\t\treturn false;\n\n\tkvm_release_pfn_clean(pfn);\n\n\t/* The instructions are well-emulated on direct mmu. */\n\tif (vcpu->arch.mmu->root_role.direct) {\n\t\tunsigned int indirect_shadow_pages;\n\n\t\twrite_lock(&vcpu->kvm->mmu_lock);\n\t\tindirect_shadow_pages = vcpu->kvm->arch.indirect_shadow_pages;\n\t\twrite_unlock(&vcpu->kvm->mmu_lock);\n\n\t\tif (indirect_shadow_pages)\n\t\t\tkvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\t\treturn true;\n\t}\n\n\t/*\n\t * if emulation was due to access to shadowed page table\n\t * and it failed try to unshadow page and re-enter the\n\t * guest to let CPU execute the instruction.\n\t */\n\tkvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\t/*\n\t * If the access faults on its page table, it can not\n\t * be fixed by unprotecting shadow page and it should\n\t * be reported to userspace.\n\t */\n\treturn !write_fault_to_shadow_pgtable;\n}\n\nstatic bool retry_instruction(struct x86_emulate_ctxt *ctxt,\n\t\t\t      gpa_t cr2_or_gpa,  int emulation_type)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tunsigned long last_retry_eip, last_retry_addr, gpa = cr2_or_gpa;\n\n\tlast_retry_eip = vcpu->arch.last_retry_eip;\n\tlast_retry_addr = vcpu->arch.last_retry_addr;\n\n\t/*\n\t * If the emulation is caused by #PF and it is non-page_table\n\t * writing instruction, it means the VM-EXIT is caused by shadow\n\t * page protected, we can zap the shadow page and retry this\n\t * instruction directly.\n\t *\n\t * Note: if the guest uses a non-page-table modifying instruction\n\t * on the PDE that points to the instruction, then we will unmap\n\t * the instruction and go to an infinite loop. So, we cache the\n\t * last retried eip and the last fault address, if we meet the eip\n\t * and the address again, we can break out of the potential infinite\n\t * loop.\n\t */\n\tvcpu->arch.last_retry_eip = vcpu->arch.last_retry_addr = 0;\n\n\tif (!(emulation_type & EMULTYPE_ALLOW_RETRY_PF))\n\t\treturn false;\n\n\tif (WARN_ON_ONCE(is_guest_mode(vcpu)) ||\n\t    WARN_ON_ONCE(!(emulation_type & EMULTYPE_PF)))\n\t\treturn false;\n\n\tif (x86_page_table_writing_insn(ctxt))\n\t\treturn false;\n\n\tif (ctxt->eip == last_retry_eip && last_retry_addr == cr2_or_gpa)\n\t\treturn false;\n\n\tvcpu->arch.last_retry_eip = ctxt->eip;\n\tvcpu->arch.last_retry_addr = cr2_or_gpa;\n\n\tif (!vcpu->arch.mmu->root_role.direct)\n\t\tgpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2_or_gpa, NULL);\n\n\tkvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\treturn true;\n}\n\nstatic int complete_emulated_mmio(struct kvm_vcpu *vcpu);\nstatic int complete_emulated_pio(struct kvm_vcpu *vcpu);\n\nstatic int kvm_vcpu_check_hw_bp(unsigned long addr, u32 type, u32 dr7,\n\t\t\t\tunsigned long *db)\n{\n\tu32 dr6 = 0;\n\tint i;\n\tu32 enable, rwlen;\n\n\tenable = dr7;\n\trwlen = dr7 >> 16;\n\tfor (i = 0; i < 4; i++, enable >>= 2, rwlen >>= 4)\n\t\tif ((enable & 3) && (rwlen & 15) == type && db[i] == addr)\n\t\t\tdr6 |= (1 << i);\n\treturn dr6;\n}\n\nstatic int kvm_vcpu_do_singlestep(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *kvm_run = vcpu->run;\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP) {\n\t\tkvm_run->debug.arch.dr6 = DR6_BS | DR6_ACTIVE_LOW;\n\t\tkvm_run->debug.arch.pc = kvm_get_linear_rip(vcpu);\n\t\tkvm_run->debug.arch.exception = DB_VECTOR;\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\treturn 0;\n\t}\n\tkvm_queue_exception_p(vcpu, DB_VECTOR, DR6_BS);\n\treturn 1;\n}\n\nint kvm_skip_emulated_instruction(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\tint r;\n\n\tr = static_call(kvm_x86_skip_emulated_instruction)(vcpu);\n\tif (unlikely(!r))\n\t\treturn 0;\n\n\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\n\t/*\n\t * rflags is the old, \"raw\" value of the flags.  The new value has\n\t * not been saved yet.\n\t *\n\t * This is correct even for TF set by the guest, because \"the\n\t * processor will not generate this exception after the instruction\n\t * that sets the TF flag\".\n\t */\n\tif (unlikely(rflags & X86_EFLAGS_TF))\n\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_skip_emulated_instruction);\n\nstatic bool kvm_is_code_breakpoint_inhibited(struct kvm_vcpu *vcpu)\n{\n\tu32 shadow;\n\n\tif (kvm_get_rflags(vcpu) & X86_EFLAGS_RF)\n\t\treturn true;\n\n\t/*\n\t * Intel CPUs inhibit code #DBs when MOV/POP SS blocking is active,\n\t * but AMD CPUs do not.  MOV/POP SS blocking is rare, check that first\n\t * to avoid the relatively expensive CPUID lookup.\n\t */\n\tshadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);\n\treturn (shadow & KVM_X86_SHADOW_INT_MOV_SS) &&\n\t       guest_cpuid_is_intel(vcpu);\n}\n\nstatic bool kvm_vcpu_check_code_breakpoint(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   int emulation_type, int *r)\n{\n\tWARN_ON_ONCE(emulation_type & EMULTYPE_NO_DECODE);\n\n\t/*\n\t * Do not check for code breakpoints if hardware has already done the\n\t * checks, as inferred from the emulation type.  On NO_DECODE and SKIP,\n\t * the instruction has passed all exception checks, and all intercepted\n\t * exceptions that trigger emulation have lower priority than code\n\t * breakpoints, i.e. the fact that the intercepted exception occurred\n\t * means any code breakpoints have already been serviced.\n\t *\n\t * Note, KVM needs to check for code #DBs on EMULTYPE_TRAP_UD_FORCED as\n\t * hardware has checked the RIP of the magic prefix, but not the RIP of\n\t * the instruction being emulated.  The intent of forced emulation is\n\t * to behave as if KVM intercepted the instruction without an exception\n\t * and without a prefix.\n\t */\n\tif (emulation_type & (EMULTYPE_NO_DECODE | EMULTYPE_SKIP |\n\t\t\t      EMULTYPE_TRAP_UD | EMULTYPE_VMWARE_GP | EMULTYPE_PF))\n\t\treturn false;\n\n\tif (unlikely(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) &&\n\t    (vcpu->arch.guest_debug_dr7 & DR7_BP_EN_MASK)) {\n\t\tstruct kvm_run *kvm_run = vcpu->run;\n\t\tunsigned long eip = kvm_get_linear_rip(vcpu);\n\t\tu32 dr6 = kvm_vcpu_check_hw_bp(eip, 0,\n\t\t\t\t\t   vcpu->arch.guest_debug_dr7,\n\t\t\t\t\t   vcpu->arch.eff_db);\n\n\t\tif (dr6 != 0) {\n\t\t\tkvm_run->debug.arch.dr6 = dr6 | DR6_ACTIVE_LOW;\n\t\t\tkvm_run->debug.arch.pc = eip;\n\t\t\tkvm_run->debug.arch.exception = DB_VECTOR;\n\t\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\t\t*r = 0;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (unlikely(vcpu->arch.dr7 & DR7_BP_EN_MASK) &&\n\t    !kvm_is_code_breakpoint_inhibited(vcpu)) {\n\t\tunsigned long eip = kvm_get_linear_rip(vcpu);\n\t\tu32 dr6 = kvm_vcpu_check_hw_bp(eip, 0,\n\t\t\t\t\t   vcpu->arch.dr7,\n\t\t\t\t\t   vcpu->arch.db);\n\n\t\tif (dr6 != 0) {\n\t\t\tkvm_queue_exception_p(vcpu, DB_VECTOR, dr6);\n\t\t\t*r = 1;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic bool is_vmware_backdoor_opcode(struct x86_emulate_ctxt *ctxt)\n{\n\tswitch (ctxt->opcode_len) {\n\tcase 1:\n\t\tswitch (ctxt->b) {\n\t\tcase 0xe4:\t/* IN */\n\t\tcase 0xe5:\n\t\tcase 0xec:\n\t\tcase 0xed:\n\t\tcase 0xe6:\t/* OUT */\n\t\tcase 0xe7:\n\t\tcase 0xee:\n\t\tcase 0xef:\n\t\tcase 0x6c:\t/* INS */\n\t\tcase 0x6d:\n\t\tcase 0x6e:\t/* OUTS */\n\t\tcase 0x6f:\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\tcase 2:\n\t\tswitch (ctxt->b) {\n\t\tcase 0x33:\t/* RDPMC */\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\n/*\n * Decode an instruction for emulation.  The caller is responsible for handling\n * code breakpoints.  Note, manually detecting code breakpoints is unnecessary\n * (and wrong) when emulating on an intercepted fault-like exception[*], as\n * code breakpoints have higher priority and thus have already been done by\n * hardware.\n *\n * [*] Except #MC, which is higher priority, but KVM should never emulate in\n *     response to a machine check.\n */\nint x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,\n\t\t\t\t    void *insn, int insn_len)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint r;\n\n\tinit_emulate_ctxt(vcpu);\n\n\tr = x86_decode_insn(ctxt, insn, insn_len, emulation_type);\n\n\ttrace_kvm_emulate_insn_start(vcpu);\n\t++vcpu->stat.insn_emulation;\n\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);\n\nint x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\t/*\n\t\t * Return immediately if RIP hits a code breakpoint, such #DBs\n\t\t * are fault-like and are higher priority than any faults on\n\t\t * the code fetch itself.\n\t\t */\n\t\tif (kvm_vcpu_check_code_breakpoint(vcpu, emulation_type, &r))\n\t\t\treturn r;\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\n\t\t\tif (ctxt->have_exception &&\n\t\t\t    !(emulation_type & EMULTYPE_SKIP)) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tinject_emulated_exception(vcpu);\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\n\t\t/*\n\t\t * Note, EXCPT_DB is assumed to be fault-like as the emulator\n\t\t * only supports code breakpoints and general detect #DB, both\n\t\t * of which are fault-like.\n\t\t */\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}\n\nint kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)\n{\n\treturn x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_instruction);\n\nint kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,\n\t\t\t\t\tvoid *insn, int insn_len)\n{\n\treturn x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_instruction_from_buffer);\n\nstatic int complete_fast_pio_out_port_0x7e(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.pio.count = 0;\n\treturn 1;\n}\n\nstatic int complete_fast_pio_out(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.pio.count = 0;\n\n\tif (unlikely(!kvm_is_linear_rip(vcpu, vcpu->arch.pio.linear_rip)))\n\t\treturn 1;\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size,\n\t\t\t    unsigned short port)\n{\n\tunsigned long val = kvm_rax_read(vcpu);\n\tint ret = emulator_pio_out(vcpu, size, port, &val, 1);\n\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Workaround userspace that relies on old KVM behavior of %rip being\n\t * incremented prior to exiting to userspace to handle \"OUT 0x7e\".\n\t */\n\tif (port == 0x7e &&\n\t    kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_OUT_7E_INC_RIP)) {\n\t\tvcpu->arch.complete_userspace_io =\n\t\t\tcomplete_fast_pio_out_port_0x7e;\n\t\tkvm_skip_emulated_instruction(vcpu);\n\t} else {\n\t\tvcpu->arch.pio.linear_rip = kvm_get_linear_rip(vcpu);\n\t\tvcpu->arch.complete_userspace_io = complete_fast_pio_out;\n\t}\n\treturn 0;\n}\n\nstatic int complete_fast_pio_in(struct kvm_vcpu *vcpu)\n{\n\tunsigned long val;\n\n\t/* We should only ever be called with arch.pio.count equal to 1 */\n\tBUG_ON(vcpu->arch.pio.count != 1);\n\n\tif (unlikely(!kvm_is_linear_rip(vcpu, vcpu->arch.pio.linear_rip))) {\n\t\tvcpu->arch.pio.count = 0;\n\t\treturn 1;\n\t}\n\n\t/* For size less than 4 we merge, else we zero extend */\n\tval = (vcpu->arch.pio.size < 4) ? kvm_rax_read(vcpu) : 0;\n\n\tcomplete_emulator_pio_in(vcpu, &val);\n\tkvm_rax_write(vcpu, val);\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int kvm_fast_pio_in(struct kvm_vcpu *vcpu, int size,\n\t\t\t   unsigned short port)\n{\n\tunsigned long val;\n\tint ret;\n\n\t/* For size less than 4 we merge, else we zero extend */\n\tval = (size < 4) ? kvm_rax_read(vcpu) : 0;\n\n\tret = emulator_pio_in(vcpu, size, port, &val, 1);\n\tif (ret) {\n\t\tkvm_rax_write(vcpu, val);\n\t\treturn ret;\n\t}\n\n\tvcpu->arch.pio.linear_rip = kvm_get_linear_rip(vcpu);\n\tvcpu->arch.complete_userspace_io = complete_fast_pio_in;\n\n\treturn 0;\n}\n\nint kvm_fast_pio(struct kvm_vcpu *vcpu, int size, unsigned short port, int in)\n{\n\tint ret;\n\n\tif (in)\n\t\tret = kvm_fast_pio_in(vcpu, size, port);\n\telse\n\t\tret = kvm_fast_pio_out(vcpu, size, port);\n\treturn ret && kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_fast_pio);\n\nstatic int kvmclock_cpu_down_prep(unsigned int cpu)\n{\n\t__this_cpu_write(cpu_tsc_khz, 0);\n\treturn 0;\n}\n\nstatic void tsc_khz_changed(void *data)\n{\n\tstruct cpufreq_freqs *freq = data;\n\tunsigned long khz = 0;\n\n\tWARN_ON_ONCE(boot_cpu_has(X86_FEATURE_CONSTANT_TSC));\n\n\tif (data)\n\t\tkhz = freq->new;\n\telse\n\t\tkhz = cpufreq_quick_get(raw_smp_processor_id());\n\tif (!khz)\n\t\tkhz = tsc_khz;\n\t__this_cpu_write(cpu_tsc_khz, khz);\n}\n\n#ifdef CONFIG_X86_64\nstatic void kvm_hyperv_tsc_notifier(void)\n{\n\tstruct kvm *kvm;\n\tint cpu;\n\n\tmutex_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\tkvm_make_mclock_inprogress_request(kvm);\n\n\t/* no guest entries from this point */\n\thyperv_stop_tsc_emulation();\n\n\t/* TSC frequency always matches when on Hyper-V */\n\tif (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {\n\t\tfor_each_present_cpu(cpu)\n\t\t\tper_cpu(cpu_tsc_khz, cpu) = tsc_khz;\n\t}\n\tkvm_caps.max_guest_tsc_khz = tsc_khz;\n\n\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\t__kvm_start_pvclock_update(kvm);\n\t\tpvclock_update_vm_gtod_copy(kvm);\n\t\tkvm_end_pvclock_update(kvm);\n\t}\n\n\tmutex_unlock(&kvm_lock);\n}\n#endif\n\nstatic void __kvmclock_cpufreq_notifier(struct cpufreq_freqs *freq, int cpu)\n{\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint send_ipi = 0;\n\tunsigned long i;\n\n\t/*\n\t * We allow guests to temporarily run on slowing clocks,\n\t * provided we notify them after, or to run on accelerating\n\t * clocks, provided we notify them before.  Thus time never\n\t * goes backwards.\n\t *\n\t * However, we have a problem.  We can't atomically update\n\t * the frequency of a given CPU from this function; it is\n\t * merely a notifier, which can be called from any CPU.\n\t * Changing the TSC frequency at arbitrary points in time\n\t * requires a recomputation of local variables related to\n\t * the TSC for each VCPU.  We must flag these local variables\n\t * to be updated and be sure the update takes place with the\n\t * new frequency before any guests proceed.\n\t *\n\t * Unfortunately, the combination of hotplug CPU and frequency\n\t * change creates an intractable locking scenario; the order\n\t * of when these callouts happen is undefined with respect to\n\t * CPU hotplug, and they can race with each other.  As such,\n\t * merely setting per_cpu(cpu_tsc_khz) = X during a hotadd is\n\t * undefined; you can actually have a CPU frequency change take\n\t * place in between the computation of X and the setting of the\n\t * variable.  To protect against this problem, all updates of\n\t * the per_cpu tsc_khz variable are done in an interrupt\n\t * protected IPI, and all callers wishing to update the value\n\t * must wait for a synchronous IPI to complete (which is trivial\n\t * if the caller is on the CPU already).  This establishes the\n\t * necessary total order on variable updates.\n\t *\n\t * Note that because a guest time update may take place\n\t * anytime after the setting of the VCPU's request bit, the\n\t * correct TSC value must be set before the request.  However,\n\t * to ensure the update actually makes it to any guest which\n\t * starts running in hardware virtualization between the set\n\t * and the acquisition of the spinlock, we must also ping the\n\t * CPU after setting the request bit.\n\t *\n\t */\n\n\tsmp_call_function_single(cpu, tsc_khz_changed, freq, 1);\n\n\tmutex_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (vcpu->cpu != cpu)\n\t\t\t\tcontinue;\n\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\t\tif (vcpu->cpu != raw_smp_processor_id())\n\t\t\t\tsend_ipi = 1;\n\t\t}\n\t}\n\tmutex_unlock(&kvm_lock);\n\n\tif (freq->old < freq->new && send_ipi) {\n\t\t/*\n\t\t * We upscale the frequency.  Must make the guest\n\t\t * doesn't see old kvmclock values while running with\n\t\t * the new frequency, otherwise we risk the guest sees\n\t\t * time go backwards.\n\t\t *\n\t\t * In case we update the frequency for another cpu\n\t\t * (which might be in guest context) send an interrupt\n\t\t * to kick the cpu out of guest context.  Next time\n\t\t * guest context is entered kvmclock will be updated,\n\t\t * so the guest will not see stale values.\n\t\t */\n\t\tsmp_call_function_single(cpu, tsc_khz_changed, freq, 1);\n\t}\n}\n\nstatic int kvmclock_cpufreq_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t     void *data)\n{\n\tstruct cpufreq_freqs *freq = data;\n\tint cpu;\n\n\tif (val == CPUFREQ_PRECHANGE && freq->old > freq->new)\n\t\treturn 0;\n\tif (val == CPUFREQ_POSTCHANGE && freq->old < freq->new)\n\t\treturn 0;\n\n\tfor_each_cpu(cpu, freq->policy->cpus)\n\t\t__kvmclock_cpufreq_notifier(freq, cpu);\n\n\treturn 0;\n}\n\nstatic struct notifier_block kvmclock_cpufreq_notifier_block = {\n\t.notifier_call  = kvmclock_cpufreq_notifier\n};\n\nstatic int kvmclock_cpu_online(unsigned int cpu)\n{\n\ttsc_khz_changed(NULL);\n\treturn 0;\n}\n\nstatic void kvm_timer_init(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {\n\t\tmax_tsc_khz = tsc_khz;\n\n\t\tif (IS_ENABLED(CONFIG_CPU_FREQ)) {\n\t\t\tstruct cpufreq_policy *policy;\n\t\t\tint cpu;\n\n\t\t\tcpu = get_cpu();\n\t\t\tpolicy = cpufreq_cpu_get(cpu);\n\t\t\tif (policy) {\n\t\t\t\tif (policy->cpuinfo.max_freq)\n\t\t\t\t\tmax_tsc_khz = policy->cpuinfo.max_freq;\n\t\t\t\tcpufreq_cpu_put(policy);\n\t\t\t}\n\t\t\tput_cpu();\n\t\t}\n\t\tcpufreq_register_notifier(&kvmclock_cpufreq_notifier_block,\n\t\t\t\t\t  CPUFREQ_TRANSITION_NOTIFIER);\n\n\t\tcpuhp_setup_state(CPUHP_AP_X86_KVM_CLK_ONLINE, \"x86/kvm/clk:online\",\n\t\t\t\t  kvmclock_cpu_online, kvmclock_cpu_down_prep);\n\t}\n}\n\n#ifdef CONFIG_X86_64\nstatic void pvclock_gtod_update_fn(struct work_struct *work)\n{\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\n\tmutex_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\tatomic_set(&kvm_guest_has_master_clock, 0);\n\tmutex_unlock(&kvm_lock);\n}\n\nstatic DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);\n\n/*\n * Indirection to move queue_work() out of the tk_core.seq write held\n * region to prevent possible deadlocks against time accessors which\n * are invoked with work related locks held.\n */\nstatic void pvclock_irq_work_fn(struct irq_work *w)\n{\n\tqueue_work(system_long_wq, &pvclock_gtod_work);\n}\n\nstatic DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);\n\n/*\n * Notification about pvclock gtod data update.\n */\nstatic int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,\n\t\t\t       void *priv)\n{\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\tstruct timekeeper *tk = priv;\n\n\tupdate_pvclock_gtod(tk);\n\n\t/*\n\t * Disable master clock if host does not trust, or does not use,\n\t * TSC based clocksource. Delegate queue_work() to irq_work as\n\t * this is invoked with tk_core.seq write held.\n\t */\n\tif (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&\n\t    atomic_read(&kvm_guest_has_master_clock) != 0)\n\t\tirq_work_queue(&pvclock_irq_work);\n\treturn 0;\n}\n\nstatic struct notifier_block pvclock_gtod_notifier = {\n\t.notifier_call = pvclock_gtod_notify,\n};\n#endif\n\nint kvm_arch_init(void *opaque)\n{\n\tstruct kvm_x86_init_ops *ops = opaque;\n\tu64 host_pat;\n\tint r;\n\n\tif (kvm_x86_ops.hardware_enable) {\n\t\tpr_err(\"kvm: already loaded vendor module '%s'\\n\", kvm_x86_ops.name);\n\t\treturn -EEXIST;\n\t}\n\n\tif (!ops->cpu_has_kvm_support()) {\n\t\tpr_err_ratelimited(\"kvm: no hardware support for '%s'\\n\",\n\t\t\t\t   ops->runtime_ops->name);\n\t\treturn -EOPNOTSUPP;\n\t}\n\tif (ops->disabled_by_bios()) {\n\t\tpr_err_ratelimited(\"kvm: support for '%s' disabled by bios\\n\",\n\t\t\t\t   ops->runtime_ops->name);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/*\n\t * KVM explicitly assumes that the guest has an FPU and\n\t * FXSAVE/FXRSTOR. For example, the KVM_GET_FPU explicitly casts the\n\t * vCPU's FPU state as a fxregs_state struct.\n\t */\n\tif (!boot_cpu_has(X86_FEATURE_FPU) || !boot_cpu_has(X86_FEATURE_FXSR)) {\n\t\tprintk(KERN_ERR \"kvm: inadequate fpu\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && !boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {\n\t\tpr_err(\"RT requires X86_FEATURE_CONSTANT_TSC\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/*\n\t * KVM assumes that PAT entry '0' encodes WB memtype and simply zeroes\n\t * the PAT bits in SPTEs.  Bail if PAT[0] is programmed to something\n\t * other than WB.  Note, EPT doesn't utilize the PAT, but don't bother\n\t * with an exception.  PAT[0] is set to WB on RESET and also by the\n\t * kernel, i.e. failure indicates a kernel bug or broken firmware.\n\t */\n\tif (rdmsrl_safe(MSR_IA32_CR_PAT, &host_pat) ||\n\t    (host_pat & GENMASK(2, 0)) != 6) {\n\t\tpr_err(\"kvm: host PAT[0] is not WB\\n\");\n\t\treturn -EIO;\n\t}\n\n\tx86_emulator_cache = kvm_alloc_emulator_cache();\n\tif (!x86_emulator_cache) {\n\t\tpr_err(\"kvm: failed to allocate cache for x86 emulator\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tuser_return_msrs = alloc_percpu(struct kvm_user_return_msrs);\n\tif (!user_return_msrs) {\n\t\tprintk(KERN_ERR \"kvm: failed to allocate percpu kvm_user_return_msrs\\n\");\n\t\tr = -ENOMEM;\n\t\tgoto out_free_x86_emulator_cache;\n\t}\n\tkvm_nr_uret_msrs = 0;\n\n\tr = kvm_mmu_vendor_module_init();\n\tif (r)\n\t\tgoto out_free_percpu;\n\n\tkvm_timer_init();\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVE)) {\n\t\thost_xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);\n\t\tkvm_caps.supported_xcr0 = host_xcr0 & KVM_SUPPORTED_XCR0;\n\t}\n\n\tif (pi_inject_timer == -1)\n\t\tpi_inject_timer = housekeeping_enabled(HK_TYPE_TIMER);\n#ifdef CONFIG_X86_64\n\tpvclock_gtod_register_notifier(&pvclock_gtod_notifier);\n\n\tif (hypervisor_is_type(X86_HYPER_MS_HYPERV))\n\t\tset_hv_tscchange_cb(kvm_hyperv_tsc_notifier);\n#endif\n\n\treturn 0;\n\nout_free_percpu:\n\tfree_percpu(user_return_msrs);\nout_free_x86_emulator_cache:\n\tkmem_cache_destroy(x86_emulator_cache);\n\treturn r;\n}\n\nvoid kvm_arch_exit(void)\n{\n#ifdef CONFIG_X86_64\n\tif (hypervisor_is_type(X86_HYPER_MS_HYPERV))\n\t\tclear_hv_tscchange_cb();\n#endif\n\tkvm_lapic_exit();\n\n\tif (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {\n\t\tcpufreq_unregister_notifier(&kvmclock_cpufreq_notifier_block,\n\t\t\t\t\t    CPUFREQ_TRANSITION_NOTIFIER);\n\t\tcpuhp_remove_state_nocalls(CPUHP_AP_X86_KVM_CLK_ONLINE);\n\t}\n#ifdef CONFIG_X86_64\n\tpvclock_gtod_unregister_notifier(&pvclock_gtod_notifier);\n\tirq_work_sync(&pvclock_irq_work);\n\tcancel_work_sync(&pvclock_gtod_work);\n#endif\n\tkvm_x86_ops.hardware_enable = NULL;\n\tkvm_mmu_vendor_module_exit();\n\tfree_percpu(user_return_msrs);\n\tkmem_cache_destroy(x86_emulator_cache);\n#ifdef CONFIG_KVM_XEN\n\tstatic_key_deferred_flush(&kvm_xen_enabled);\n\tWARN_ON(static_branch_unlikely(&kvm_xen_enabled.key));\n#endif\n}\n\nstatic int __kvm_emulate_halt(struct kvm_vcpu *vcpu, int state, int reason)\n{\n\t/*\n\t * The vCPU has halted, e.g. executed HLT.  Update the run state if the\n\t * local APIC is in-kernel, the run loop will detect the non-runnable\n\t * state and halt the vCPU.  Exit to userspace if the local APIC is\n\t * managed by userspace, in which case userspace is responsible for\n\t * handling wake events.\n\t */\n\t++vcpu->stat.halt_exits;\n\tif (lapic_in_kernel(vcpu)) {\n\t\tvcpu->arch.mp_state = state;\n\t\treturn 1;\n\t} else {\n\t\tvcpu->run->exit_reason = reason;\n\t\treturn 0;\n\t}\n}\n\nint kvm_emulate_halt_noskip(struct kvm_vcpu *vcpu)\n{\n\treturn __kvm_emulate_halt(vcpu, KVM_MP_STATE_HALTED, KVM_EXIT_HLT);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_halt_noskip);\n\nint kvm_emulate_halt(struct kvm_vcpu *vcpu)\n{\n\tint ret = kvm_skip_emulated_instruction(vcpu);\n\t/*\n\t * TODO: we might be squashing a GUESTDBG_SINGLESTEP-triggered\n\t * KVM_EXIT_DEBUG here.\n\t */\n\treturn kvm_emulate_halt_noskip(vcpu) && ret;\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_halt);\n\nint kvm_emulate_ap_reset_hold(struct kvm_vcpu *vcpu)\n{\n\tint ret = kvm_skip_emulated_instruction(vcpu);\n\n\treturn __kvm_emulate_halt(vcpu, KVM_MP_STATE_AP_RESET_HOLD,\n\t\t\t\t\tKVM_EXIT_AP_RESET_HOLD) && ret;\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_ap_reset_hold);\n\n#ifdef CONFIG_X86_64\nstatic int kvm_pv_clock_pairing(struct kvm_vcpu *vcpu, gpa_t paddr,\n\t\t\t        unsigned long clock_type)\n{\n\tstruct kvm_clock_pairing clock_pairing;\n\tstruct timespec64 ts;\n\tu64 cycle;\n\tint ret;\n\n\tif (clock_type != KVM_CLOCK_PAIRING_WALLCLOCK)\n\t\treturn -KVM_EOPNOTSUPP;\n\n\t/*\n\t * When tsc is in permanent catchup mode guests won't be able to use\n\t * pvclock_read_retry loop to get consistent view of pvclock\n\t */\n\tif (vcpu->arch.tsc_always_catchup)\n\t\treturn -KVM_EOPNOTSUPP;\n\n\tif (!kvm_get_walltime_and_clockread(&ts, &cycle))\n\t\treturn -KVM_EOPNOTSUPP;\n\n\tclock_pairing.sec = ts.tv_sec;\n\tclock_pairing.nsec = ts.tv_nsec;\n\tclock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);\n\tclock_pairing.flags = 0;\n\tmemset(&clock_pairing.pad, 0, sizeof(clock_pairing.pad));\n\n\tret = 0;\n\tif (kvm_write_guest(vcpu->kvm, paddr, &clock_pairing,\n\t\t\t    sizeof(struct kvm_clock_pairing)))\n\t\tret = -KVM_EFAULT;\n\n\treturn ret;\n}\n#endif\n\n/*\n * kvm_pv_kick_cpu_op:  Kick a vcpu.\n *\n * @apicid - apicid of vcpu to be kicked.\n */\nstatic void kvm_pv_kick_cpu_op(struct kvm *kvm, int apicid)\n{\n\t/*\n\t * All other fields are unused for APIC_DM_REMRD, but may be consumed by\n\t * common code, e.g. for tracing. Defer initialization to the compiler.\n\t */\n\tstruct kvm_lapic_irq lapic_irq = {\n\t\t.delivery_mode = APIC_DM_REMRD,\n\t\t.dest_mode = APIC_DEST_PHYSICAL,\n\t\t.shorthand = APIC_DEST_NOSHORT,\n\t\t.dest_id = apicid,\n\t};\n\n\tkvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);\n}\n\nbool kvm_apicv_activated(struct kvm *kvm)\n{\n\treturn (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);\n}\nEXPORT_SYMBOL_GPL(kvm_apicv_activated);\n\nbool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu)\n{\n\tulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);\n\tulong vcpu_reasons = static_call(kvm_x86_vcpu_get_apicv_inhibit_reasons)(vcpu);\n\n\treturn (vm_reasons | vcpu_reasons) == 0;\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_apicv_activated);\n\nstatic void set_or_clear_apicv_inhibit(unsigned long *inhibits,\n\t\t\t\t       enum kvm_apicv_inhibit reason, bool set)\n{\n\tif (set)\n\t\t__set_bit(reason, inhibits);\n\telse\n\t\t__clear_bit(reason, inhibits);\n\n\ttrace_kvm_apicv_inhibit_changed(reason, set, *inhibits);\n}\n\nstatic void kvm_apicv_init(struct kvm *kvm)\n{\n\tunsigned long *inhibits = &kvm->arch.apicv_inhibit_reasons;\n\n\tinit_rwsem(&kvm->arch.apicv_update_lock);\n\n\tset_or_clear_apicv_inhibit(inhibits, APICV_INHIBIT_REASON_ABSENT, true);\n\n\tif (!enable_apicv)\n\t\tset_or_clear_apicv_inhibit(inhibits,\n\t\t\t\t\t   APICV_INHIBIT_REASON_DISABLE, true);\n}\n\nstatic void kvm_sched_yield(struct kvm_vcpu *vcpu, unsigned long dest_id)\n{\n\tstruct kvm_vcpu *target = NULL;\n\tstruct kvm_apic_map *map;\n\n\tvcpu->stat.directed_yield_attempted++;\n\n\tif (single_task_running())\n\t\tgoto no_yield;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(vcpu->kvm->arch.apic_map);\n\n\tif (likely(map) && dest_id <= map->max_apic_id && map->phys_map[dest_id])\n\t\ttarget = map->phys_map[dest_id]->vcpu;\n\n\trcu_read_unlock();\n\n\tif (!target || !READ_ONCE(target->ready))\n\t\tgoto no_yield;\n\n\t/* Ignore requests to yield to self */\n\tif (vcpu == target)\n\t\tgoto no_yield;\n\n\tif (kvm_vcpu_yield_to(target) <= 0)\n\t\tgoto no_yield;\n\n\tvcpu->stat.directed_yield_successful++;\n\nno_yield:\n\treturn;\n}\n\nstatic int complete_hypercall_exit(struct kvm_vcpu *vcpu)\n{\n\tu64 ret = vcpu->run->hypercall.ret;\n\n\tif (!is_64_bit_mode(vcpu))\n\t\tret = (u32)ret;\n\tkvm_rax_write(vcpu, ret);\n\t++vcpu->stat.hypercalls;\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nint kvm_emulate_hypercall(struct kvm_vcpu *vcpu)\n{\n\tunsigned long nr, a0, a1, a2, a3, ret;\n\tint op_64_bit;\n\n\tif (kvm_xen_hypercall_enabled(vcpu->kvm))\n\t\treturn kvm_xen_hypercall(vcpu);\n\n\tif (kvm_hv_hypercall_enabled(vcpu))\n\t\treturn kvm_hv_hypercall(vcpu);\n\n\tnr = kvm_rax_read(vcpu);\n\ta0 = kvm_rbx_read(vcpu);\n\ta1 = kvm_rcx_read(vcpu);\n\ta2 = kvm_rdx_read(vcpu);\n\ta3 = kvm_rsi_read(vcpu);\n\n\ttrace_kvm_hypercall(nr, a0, a1, a2, a3);\n\n\top_64_bit = is_64_bit_hypercall(vcpu);\n\tif (!op_64_bit) {\n\t\tnr &= 0xFFFFFFFF;\n\t\ta0 &= 0xFFFFFFFF;\n\t\ta1 &= 0xFFFFFFFF;\n\t\ta2 &= 0xFFFFFFFF;\n\t\ta3 &= 0xFFFFFFFF;\n\t}\n\n\tif (static_call(kvm_x86_get_cpl)(vcpu) != 0) {\n\t\tret = -KVM_EPERM;\n\t\tgoto out;\n\t}\n\n\tret = -KVM_ENOSYS;\n\n\tswitch (nr) {\n\tcase KVM_HC_VAPIC_POLL_IRQ:\n\t\tret = 0;\n\t\tbreak;\n\tcase KVM_HC_KICK_CPU:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_UNHALT))\n\t\t\tbreak;\n\n\t\tkvm_pv_kick_cpu_op(vcpu->kvm, a1);\n\t\tkvm_sched_yield(vcpu, a1);\n\t\tret = 0;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase KVM_HC_CLOCK_PAIRING:\n\t\tret = kvm_pv_clock_pairing(vcpu, a0, a1);\n\t\tbreak;\n#endif\n\tcase KVM_HC_SEND_IPI:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_SEND_IPI))\n\t\t\tbreak;\n\n\t\tret = kvm_pv_send_ipi(vcpu->kvm, a0, a1, a2, a3, op_64_bit);\n\t\tbreak;\n\tcase KVM_HC_SCHED_YIELD:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_SCHED_YIELD))\n\t\t\tbreak;\n\n\t\tkvm_sched_yield(vcpu, a0);\n\t\tret = 0;\n\t\tbreak;\n\tcase KVM_HC_MAP_GPA_RANGE: {\n\t\tu64 gpa = a0, npages = a1, attrs = a2;\n\n\t\tret = -KVM_ENOSYS;\n\t\tif (!(vcpu->kvm->arch.hypercall_exit_enabled & (1 << KVM_HC_MAP_GPA_RANGE)))\n\t\t\tbreak;\n\n\t\tif (!PAGE_ALIGNED(gpa) || !npages ||\n\t\t    gpa_to_gfn(gpa) + npages <= gpa_to_gfn(gpa)) {\n\t\t\tret = -KVM_EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tvcpu->run->exit_reason        = KVM_EXIT_HYPERCALL;\n\t\tvcpu->run->hypercall.nr       = KVM_HC_MAP_GPA_RANGE;\n\t\tvcpu->run->hypercall.args[0]  = gpa;\n\t\tvcpu->run->hypercall.args[1]  = npages;\n\t\tvcpu->run->hypercall.args[2]  = attrs;\n\t\tvcpu->run->hypercall.longmode = op_64_bit;\n\t\tvcpu->arch.complete_userspace_io = complete_hypercall_exit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\tret = -KVM_ENOSYS;\n\t\tbreak;\n\t}\nout:\n\tif (!op_64_bit)\n\t\tret = (u32)ret;\n\tkvm_rax_write(vcpu, ret);\n\n\t++vcpu->stat.hypercalls;\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_hypercall);\n\nstatic int emulator_fix_hypercall(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tchar instruction[3];\n\tunsigned long rip = kvm_rip_read(vcpu);\n\n\t/*\n\t * If the quirk is disabled, synthesize a #UD and let the guest pick up\n\t * the pieces.\n\t */\n\tif (!kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_FIX_HYPERCALL_INSN)) {\n\t\tctxt->exception.error_code_valid = false;\n\t\tctxt->exception.vector = UD_VECTOR;\n\t\tctxt->have_exception = true;\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\tstatic_call(kvm_x86_patch_hypercall)(vcpu, instruction);\n\n\treturn emulator_write_emulated(ctxt, rip, instruction, 3,\n\t\t&ctxt->exception);\n}\n\nstatic int dm_request_for_irq_injection(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->run->request_interrupt_window &&\n\t\tlikely(!pic_in_kernel(vcpu->kvm));\n}\n\n/* Called within kvm->srcu read side.  */\nstatic void post_kvm_run_save(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *kvm_run = vcpu->run;\n\n\tkvm_run->if_flag = static_call(kvm_x86_get_if_flag)(vcpu);\n\tkvm_run->cr8 = kvm_get_cr8(vcpu);\n\tkvm_run->apic_base = kvm_get_apic_base(vcpu);\n\n\tkvm_run->ready_for_interrupt_injection =\n\t\tpic_in_kernel(vcpu->kvm) ||\n\t\tkvm_vcpu_ready_for_interrupt_injection(vcpu);\n\n\tif (is_smm(vcpu))\n\t\tkvm_run->flags |= KVM_RUN_X86_SMM;\n}\n\nstatic void update_cr8_intercept(struct kvm_vcpu *vcpu)\n{\n\tint max_irr, tpr;\n\n\tif (!kvm_x86_ops.update_cr8_intercept)\n\t\treturn;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tif (vcpu->arch.apic->apicv_active)\n\t\treturn;\n\n\tif (!vcpu->arch.apic->vapic_addr)\n\t\tmax_irr = kvm_lapic_find_highest_irr(vcpu);\n\telse\n\t\tmax_irr = -1;\n\n\tif (max_irr != -1)\n\t\tmax_irr >>= 4;\n\n\ttpr = kvm_lapic_get_cr8(vcpu);\n\n\tstatic_call(kvm_x86_update_cr8_intercept)(vcpu, tpr, max_irr);\n}\n\n\nint kvm_check_nested_events(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_test_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {\n\t\tkvm_x86_ops.nested_ops->triple_fault(vcpu);\n\t\treturn 1;\n\t}\n\n\treturn kvm_x86_ops.nested_ops->check_events(vcpu);\n}\n\nstatic void kvm_inject_exception(struct kvm_vcpu *vcpu)\n{\n\ttrace_kvm_inj_exception(vcpu->arch.exception.vector,\n\t\t\t\tvcpu->arch.exception.has_error_code,\n\t\t\t\tvcpu->arch.exception.error_code,\n\t\t\t\tvcpu->arch.exception.injected);\n\n\tif (vcpu->arch.exception.error_code && !is_protmode(vcpu))\n\t\tvcpu->arch.exception.error_code = false;\n\tstatic_call(kvm_x86_inject_exception)(vcpu);\n}\n\n/*\n * Check for any event (interrupt or exception) that is ready to be injected,\n * and if there is at least one event, inject the event with the highest\n * priority.  This handles both \"pending\" events, i.e. events that have never\n * been injected into the guest, and \"injected\" events, i.e. events that were\n * injected as part of a previous VM-Enter, but weren't successfully delivered\n * and need to be re-injected.\n *\n * Note, this is not guaranteed to be invoked on a guest instruction boundary,\n * i.e. doesn't guarantee that there's an event window in the guest.  KVM must\n * be able to inject exceptions in the \"middle\" of an instruction, and so must\n * also be able to re-inject NMIs and IRQs in the middle of an instruction.\n * I.e. for exceptions and re-injected events, NOT invoking this on instruction\n * boundaries is necessary and correct.\n *\n * For simplicity, KVM uses a single path to inject all events (except events\n * that are injected directly from L1 to L2) and doesn't explicitly track\n * instruction boundaries for asynchronous events.  However, because VM-Exits\n * that can occur during instruction execution typically result in KVM skipping\n * the instruction or injecting an exception, e.g. instruction and exception\n * intercepts, and because pending exceptions have higher priority than pending\n * interrupts, KVM still honors instruction boundaries in most scenarios.\n *\n * But, if a VM-Exit occurs during instruction execution, and KVM does NOT skip\n * the instruction or inject an exception, then KVM can incorrecty inject a new\n * asynchrounous event if the event became pending after the CPU fetched the\n * instruction (in the guest).  E.g. if a page fault (#PF, #NPF, EPT violation)\n * occurs and is resolved by KVM, a coincident NMI, SMI, IRQ, etc... can be\n * injected on the restarted instruction instead of being deferred until the\n * instruction completes.\n *\n * In practice, this virtualization hole is unlikely to be observed by the\n * guest, and even less likely to cause functional problems.  To detect the\n * hole, the guest would have to trigger an event on a side effect of an early\n * phase of instruction execution, e.g. on the instruction fetch from memory.\n * And for it to be a functional problem, the guest would need to depend on the\n * ordering between that side effect, the instruction completing, _and_ the\n * delivery of the asynchronous event.\n */\nstatic int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,\n\t\t\t\t       bool *req_immediate_exit)\n{\n\tbool can_inject;\n\tint r;\n\n\t/*\n\t * Process nested events first, as nested VM-Exit supercedes event\n\t * re-injection.  If there's an event queued for re-injection, it will\n\t * be saved into the appropriate vmc{b,s}12 fields on nested VM-Exit.\n\t */\n\tif (is_guest_mode(vcpu))\n\t\tr = kvm_check_nested_events(vcpu);\n\telse\n\t\tr = 0;\n\n\t/*\n\t * Re-inject exceptions and events *especially* if immediate entry+exit\n\t * to/from L2 is needed, as any event that has already been injected\n\t * into L2 needs to complete its lifecycle before injecting a new event.\n\t *\n\t * Don't re-inject an NMI or interrupt if there is a pending exception.\n\t * This collision arises if an exception occurred while vectoring the\n\t * injected event, KVM intercepted said exception, and KVM ultimately\n\t * determined the fault belongs to the guest and queues the exception\n\t * for injection back into the guest.\n\t *\n\t * \"Injected\" interrupts can also collide with pending exceptions if\n\t * userspace ignores the \"ready for injection\" flag and blindly queues\n\t * an interrupt.  In that case, prioritizing the exception is correct,\n\t * as the exception \"occurred\" before the exit to userspace.  Trap-like\n\t * exceptions, e.g. most #DBs, have higher priority than interrupts.\n\t * And while fault-like exceptions, e.g. #GP and #PF, are the lowest\n\t * priority, they're only generated (pended) during instruction\n\t * execution, and interrupts are recognized at instruction boundaries.\n\t * Thus a pending fault-like exception means the fault occurred on the\n\t * *previous* instruction and must be serviced prior to recognizing any\n\t * new events in order to fully complete the previous instruction.\n\t */\n\tif (vcpu->arch.exception.injected)\n\t\tkvm_inject_exception(vcpu);\n\telse if (kvm_is_exception_pending(vcpu))\n\t\t; /* see above */\n\telse if (vcpu->arch.nmi_injected)\n\t\tstatic_call(kvm_x86_inject_nmi)(vcpu);\n\telse if (vcpu->arch.interrupt.injected)\n\t\tstatic_call(kvm_x86_inject_irq)(vcpu, true);\n\n\t/*\n\t * Exceptions that morph to VM-Exits are handled above, and pending\n\t * exceptions on top of injected exceptions that do not VM-Exit should\n\t * either morph to #DF or, sadly, override the injected exception.\n\t */\n\tWARN_ON_ONCE(vcpu->arch.exception.injected &&\n\t\t     vcpu->arch.exception.pending);\n\n\t/*\n\t * Bail if immediate entry+exit to/from the guest is needed to complete\n\t * nested VM-Enter or event re-injection so that a different pending\n\t * event can be serviced (or if KVM needs to exit to userspace).\n\t *\n\t * Otherwise, continue processing events even if VM-Exit occurred.  The\n\t * VM-Exit will have cleared exceptions that were meant for L2, but\n\t * there may now be events that can be injected into L1.\n\t */\n\tif (r < 0)\n\t\tgoto out;\n\n\t/*\n\t * A pending exception VM-Exit should either result in nested VM-Exit\n\t * or force an immediate re-entry and exit to/from L2, and exception\n\t * VM-Exits cannot be injected (flag should _never_ be set).\n\t */\n\tWARN_ON_ONCE(vcpu->arch.exception_vmexit.injected ||\n\t\t     vcpu->arch.exception_vmexit.pending);\n\n\t/*\n\t * New events, other than exceptions, cannot be injected if KVM needs\n\t * to re-inject a previous event.  See above comments on re-injecting\n\t * for why pending exceptions get priority.\n\t */\n\tcan_inject = !kvm_event_needs_reinjection(vcpu);\n\n\tif (vcpu->arch.exception.pending) {\n\t\t/*\n\t\t * Fault-class exceptions, except #DBs, set RF=1 in the RFLAGS\n\t\t * value pushed on the stack.  Trap-like exception and all #DBs\n\t\t * leave RF as-is (KVM follows Intel's behavior in this regard;\n\t\t * AMD states that code breakpoint #DBs excplitly clear RF=0).\n\t\t *\n\t\t * Note, most versions of Intel's SDM and AMD's APM incorrectly\n\t\t * describe the behavior of General Detect #DBs, which are\n\t\t * fault-like.  They do _not_ set RF, a la code breakpoints.\n\t\t */\n\t\tif (exception_type(vcpu->arch.exception.vector) == EXCPT_FAULT)\n\t\t\t__kvm_set_rflags(vcpu, kvm_get_rflags(vcpu) |\n\t\t\t\t\t     X86_EFLAGS_RF);\n\n\t\tif (vcpu->arch.exception.vector == DB_VECTOR) {\n\t\t\tkvm_deliver_exception_payload(vcpu, &vcpu->arch.exception);\n\t\t\tif (vcpu->arch.dr7 & DR7_GD) {\n\t\t\t\tvcpu->arch.dr7 &= ~DR7_GD;\n\t\t\t\tkvm_update_dr7(vcpu);\n\t\t\t}\n\t\t}\n\n\t\tkvm_inject_exception(vcpu);\n\n\t\tvcpu->arch.exception.pending = false;\n\t\tvcpu->arch.exception.injected = true;\n\n\t\tcan_inject = false;\n\t}\n\n\t/* Don't inject interrupts if the user asked to avoid doing so */\n\tif (vcpu->guest_debug & KVM_GUESTDBG_BLOCKIRQ)\n\t\treturn 0;\n\n\t/*\n\t * Finally, inject interrupt events.  If an event cannot be injected\n\t * due to architectural conditions (e.g. IF=0) a window-open exit\n\t * will re-request KVM_REQ_EVENT.  Sometimes however an event is pending\n\t * and can architecturally be injected, but we cannot do it right now:\n\t * an interrupt could have arrived just now and we have to inject it\n\t * as a vmexit, or there could already an event in the queue, which is\n\t * indicated by can_inject.  In that case we request an immediate exit\n\t * in order to make progress and get back here for another iteration.\n\t * The kvm_x86_ops hooks communicate this by returning -EBUSY.\n\t */\n#ifdef CONFIG_KVM_SMM\n\tif (vcpu->arch.smi_pending) {\n\t\tr = can_inject ? static_call(kvm_x86_smi_allowed)(vcpu, true) : -EBUSY;\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t\tif (r) {\n\t\t\tvcpu->arch.smi_pending = false;\n\t\t\t++vcpu->arch.smi_count;\n\t\t\tenter_smm(vcpu);\n\t\t\tcan_inject = false;\n\t\t} else\n\t\t\tstatic_call(kvm_x86_enable_smi_window)(vcpu);\n\t}\n#endif\n\n\tif (vcpu->arch.nmi_pending) {\n\t\tr = can_inject ? static_call(kvm_x86_nmi_allowed)(vcpu, true) : -EBUSY;\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t\tif (r) {\n\t\t\t--vcpu->arch.nmi_pending;\n\t\t\tvcpu->arch.nmi_injected = true;\n\t\t\tstatic_call(kvm_x86_inject_nmi)(vcpu);\n\t\t\tcan_inject = false;\n\t\t\tWARN_ON(static_call(kvm_x86_nmi_allowed)(vcpu, true) < 0);\n\t\t}\n\t\tif (vcpu->arch.nmi_pending)\n\t\t\tstatic_call(kvm_x86_enable_nmi_window)(vcpu);\n\t}\n\n\tif (kvm_cpu_has_injectable_intr(vcpu)) {\n\t\tr = can_inject ? static_call(kvm_x86_interrupt_allowed)(vcpu, true) : -EBUSY;\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t\tif (r) {\n\t\t\tkvm_queue_interrupt(vcpu, kvm_cpu_get_interrupt(vcpu), false);\n\t\t\tstatic_call(kvm_x86_inject_irq)(vcpu, false);\n\t\t\tWARN_ON(static_call(kvm_x86_interrupt_allowed)(vcpu, true) < 0);\n\t\t}\n\t\tif (kvm_cpu_has_injectable_intr(vcpu))\n\t\t\tstatic_call(kvm_x86_enable_irq_window)(vcpu);\n\t}\n\n\tif (is_guest_mode(vcpu) &&\n\t    kvm_x86_ops.nested_ops->has_events &&\n\t    kvm_x86_ops.nested_ops->has_events(vcpu))\n\t\t*req_immediate_exit = true;\n\n\t/*\n\t * KVM must never queue a new exception while injecting an event; KVM\n\t * is done emulating and should only propagate the to-be-injected event\n\t * to the VMCS/VMCB.  Queueing a new exception can put the vCPU into an\n\t * infinite loop as KVM will bail from VM-Enter to inject the pending\n\t * exception and start the cycle all over.\n\t *\n\t * Exempt triple faults as they have special handling and won't put the\n\t * vCPU into an infinite loop.  Triple fault can be queued when running\n\t * VMX without unrestricted guest, as that requires KVM to emulate Real\n\t * Mode events (see kvm_inject_realmode_interrupt()).\n\t */\n\tWARN_ON_ONCE(vcpu->arch.exception.pending ||\n\t\t     vcpu->arch.exception_vmexit.pending);\n\treturn 0;\n\nout:\n\tif (r == -EBUSY) {\n\t\t*req_immediate_exit = true;\n\t\tr = 0;\n\t}\n\treturn r;\n}\n\nstatic void process_nmi(struct kvm_vcpu *vcpu)\n{\n\tunsigned limit = 2;\n\n\t/*\n\t * x86 is limited to one NMI running, and one NMI pending after it.\n\t * If an NMI is already in progress, limit further NMIs to just one.\n\t * Otherwise, allow two (and we'll inject the first one immediately).\n\t */\n\tif (static_call(kvm_x86_get_nmi_mask)(vcpu) || vcpu->arch.nmi_injected)\n\t\tlimit = 1;\n\n\tvcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);\n\tvcpu->arch.nmi_pending = min(vcpu->arch.nmi_pending, limit);\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\n\nvoid kvm_make_scan_ioapic_request_mask(struct kvm *kvm,\n\t\t\t\t       unsigned long *vcpu_bitmap)\n{\n\tkvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);\n}\n\nvoid kvm_make_scan_ioapic_request(struct kvm *kvm)\n{\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);\n}\n\nvoid kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tbool activate;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tdown_read(&vcpu->kvm->arch.apicv_update_lock);\n\tpreempt_disable();\n\n\t/* Do not activate APICV when APIC is disabled */\n\tactivate = kvm_vcpu_apicv_activated(vcpu) &&\n\t\t   (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED);\n\n\tif (apic->apicv_active == activate)\n\t\tgoto out;\n\n\tapic->apicv_active = activate;\n\tkvm_apic_update_apicv(vcpu);\n\tstatic_call(kvm_x86_refresh_apicv_exec_ctrl)(vcpu);\n\n\t/*\n\t * When APICv gets disabled, we may still have injected interrupts\n\t * pending. At the same time, KVM_REQ_EVENT may not be set as APICv was\n\t * still active when the interrupt got accepted. Make sure\n\t * kvm_check_and_inject_events() is called to check for that.\n\t */\n\tif (!apic->apicv_active)\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\nout:\n\tpreempt_enable();\n\tup_read(&vcpu->kvm->arch.apicv_update_lock);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_update_apicv);\n\nvoid __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,\n\t\t\t\t      enum kvm_apicv_inhibit reason, bool set)\n{\n\tunsigned long old, new;\n\n\tlockdep_assert_held_write(&kvm->arch.apicv_update_lock);\n\n\tif (!static_call(kvm_x86_check_apicv_inhibit_reasons)(reason))\n\t\treturn;\n\n\told = new = kvm->arch.apicv_inhibit_reasons;\n\n\tset_or_clear_apicv_inhibit(&new, reason, set);\n\n\tif (!!old != !!new) {\n\t\t/*\n\t\t * Kick all vCPUs before setting apicv_inhibit_reasons to avoid\n\t\t * false positives in the sanity check WARN in svm_vcpu_run().\n\t\t * This task will wait for all vCPUs to ack the kick IRQ before\n\t\t * updating apicv_inhibit_reasons, and all other vCPUs will\n\t\t * block on acquiring apicv_update_lock so that vCPUs can't\n\t\t * redo svm_vcpu_run() without seeing the new inhibit state.\n\t\t *\n\t\t * Note, holding apicv_update_lock and taking it in the read\n\t\t * side (handling the request) also prevents other vCPUs from\n\t\t * servicing the request with a stale apicv_inhibit_reasons.\n\t\t */\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);\n\t\tkvm->arch.apicv_inhibit_reasons = new;\n\t\tif (new) {\n\t\t\tunsigned long gfn = gpa_to_gfn(APIC_DEFAULT_PHYS_BASE);\n\t\t\tint idx = srcu_read_lock(&kvm->srcu);\n\n\t\t\tkvm_zap_gfn_range(kvm, gfn, gfn+1);\n\t\t\tsrcu_read_unlock(&kvm->srcu, idx);\n\t\t}\n\t} else {\n\t\tkvm->arch.apicv_inhibit_reasons = new;\n\t}\n}\n\nvoid kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,\n\t\t\t\t    enum kvm_apicv_inhibit reason, bool set)\n{\n\tif (!enable_apicv)\n\t\treturn;\n\n\tdown_write(&kvm->arch.apicv_update_lock);\n\t__kvm_set_or_clear_apicv_inhibit(kvm, reason, set);\n\tup_write(&kvm->arch.apicv_update_lock);\n}\nEXPORT_SYMBOL_GPL(kvm_set_or_clear_apicv_inhibit);\n\nstatic void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_apic_present(vcpu))\n\t\treturn;\n\n\tbitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);\n\n\tif (irqchip_split(vcpu->kvm))\n\t\tkvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);\n\telse {\n\t\tstatic_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);\n\t\tif (ioapic_in_kernel(vcpu->kvm))\n\t\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);\n\t}\n\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.load_eoi_exitmap_pending = true;\n\telse\n\t\tkvm_make_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu);\n}\n\nstatic void vcpu_load_eoi_exitmap(struct kvm_vcpu *vcpu)\n{\n\tu64 eoi_exit_bitmap[4];\n\n\tif (!kvm_apic_hw_enabled(vcpu->arch.apic))\n\t\treturn;\n\n\tif (to_hv_vcpu(vcpu)) {\n\t\tbitmap_or((ulong *)eoi_exit_bitmap,\n\t\t\t  vcpu->arch.ioapic_handled_vectors,\n\t\t\t  to_hv_synic(vcpu)->vec_bitmap, 256);\n\t\tstatic_call_cond(kvm_x86_load_eoi_exitmap)(vcpu, eoi_exit_bitmap);\n\t\treturn;\n\t}\n\n\tstatic_call_cond(kvm_x86_load_eoi_exitmap)(\n\t\tvcpu, (u64 *)vcpu->arch.ioapic_handled_vectors);\n}\n\nvoid kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,\n\t\t\t\t\t    unsigned long start, unsigned long end)\n{\n\tunsigned long apic_address;\n\n\t/*\n\t * The physical address of apic access page is stored in the VMCS.\n\t * Update it when it becomes invalid.\n\t */\n\tapic_address = gfn_to_hva(kvm, APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT);\n\tif (start <= apic_address && apic_address < end)\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);\n}\n\nvoid kvm_arch_guest_memory_reclaimed(struct kvm *kvm)\n{\n\tstatic_call_cond(kvm_x86_guest_memory_reclaimed)(kvm);\n}\n\nstatic void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tstatic_call_cond(kvm_x86_set_apic_access_page_addr)(vcpu);\n}\n\nvoid __kvm_request_immediate_exit(struct kvm_vcpu *vcpu)\n{\n\tsmp_send_reschedule(vcpu->cpu);\n}\nEXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);\n\n/*\n * Called within kvm->srcu read side.\n * Returns 1 to let vcpu_run() continue the guest execution loop without\n * exiting to the userspace.  Otherwise, the value will be returned to the\n * userspace.\n */\nstatic int vcpu_enter_guest(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\tbool req_int_win =\n\t\tdm_request_for_irq_injection(vcpu) &&\n\t\tkvm_cpu_accept_dm_intr(vcpu);\n\tfastpath_t exit_fastpath;\n\n\tbool req_immediate_exit = false;\n\n\tif (kvm_request_pending(vcpu)) {\n\t\tif (kvm_check_request(KVM_REQ_VM_DEAD, vcpu)) {\n\t\t\tr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (kvm_dirty_ring_check_request(vcpu)) {\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (kvm_check_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu)) {\n\t\t\tif (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {\n\t\t\t\tr = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_MMU_FREE_OBSOLETE_ROOTS, vcpu))\n\t\t\tkvm_mmu_free_obsolete_roots(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))\n\t\t\t__kvm_migrate_timers(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))\n\t\t\tkvm_update_masterclock(vcpu->kvm);\n\t\tif (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))\n\t\t\tkvm_gen_kvmclock_update(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {\n\t\t\tr = kvm_guest_time_update(vcpu);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_MMU_SYNC, vcpu))\n\t\t\tkvm_mmu_sync_roots(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_LOAD_MMU_PGD, vcpu))\n\t\t\tkvm_mmu_load_pgd(vcpu);\n\n\t\t/*\n\t\t * Note, the order matters here, as flushing \"all\" TLB entries\n\t\t * also flushes the \"current\" TLB entries, i.e. servicing the\n\t\t * flush \"all\" will clear any request to flush \"current\".\n\t\t */\n\t\tif (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu))\n\t\t\tkvm_vcpu_flush_tlb_all(vcpu);\n\n\t\tkvm_service_local_tlb_flush_requests(vcpu);\n\n\t\t/*\n\t\t * Fall back to a \"full\" guest flush if Hyper-V's precise\n\t\t * flushing fails.  Note, Hyper-V's flushing is per-vCPU, but\n\t\t * the flushes are considered \"remote\" and not \"local\" because\n\t\t * the requests can be initiated from other vCPUs.\n\t\t */\n\t\tif (kvm_check_request(KVM_REQ_HV_TLB_FLUSH, vcpu) &&\n\t\t    kvm_hv_vcpu_flush_tlb(vcpu))\n\t\t\tkvm_vcpu_flush_tlb_guest(vcpu);\n\n\t\tif (kvm_check_request(KVM_REQ_REPORT_TPR_ACCESS, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_TPR_ACCESS;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_test_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {\n\t\t\tif (is_guest_mode(vcpu))\n\t\t\t\tkvm_x86_ops.nested_ops->triple_fault(vcpu);\n\n\t\t\tif (kvm_check_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {\n\t\t\t\tvcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;\n\t\t\t\tvcpu->mmio_needed = 0;\n\t\t\t\tr = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {\n\t\t\t/* Page is swapped out. Do synthetic halt */\n\t\t\tvcpu->arch.apf.halted = true;\n\t\t\tr = 1;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_STEAL_UPDATE, vcpu))\n\t\t\trecord_steal_time(vcpu);\n#ifdef CONFIG_KVM_SMM\n\t\tif (kvm_check_request(KVM_REQ_SMI, vcpu))\n\t\t\tprocess_smi(vcpu);\n#endif\n\t\tif (kvm_check_request(KVM_REQ_NMI, vcpu))\n\t\t\tprocess_nmi(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_PMU, vcpu))\n\t\t\tkvm_pmu_handle_event(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_PMI, vcpu))\n\t\t\tkvm_pmu_deliver_pmi(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {\n\t\t\tBUG_ON(vcpu->arch.pending_ioapic_eoi > 255);\n\t\t\tif (test_bit(vcpu->arch.pending_ioapic_eoi,\n\t\t\t\t     vcpu->arch.ioapic_handled_vectors)) {\n\t\t\t\tvcpu->run->exit_reason = KVM_EXIT_IOAPIC_EOI;\n\t\t\t\tvcpu->run->eoi.vector =\n\t\t\t\t\t\tvcpu->arch.pending_ioapic_eoi;\n\t\t\t\tr = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_SCAN_IOAPIC, vcpu))\n\t\t\tvcpu_scan_ioapic(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu))\n\t\t\tvcpu_load_eoi_exitmap(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))\n\t\t\tkvm_vcpu_reload_apic_access_page(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_HV_CRASH, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_SYSTEM_EVENT;\n\t\t\tvcpu->run->system_event.type = KVM_SYSTEM_EVENT_CRASH;\n\t\t\tvcpu->run->system_event.ndata = 0;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_HV_RESET, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_SYSTEM_EVENT;\n\t\t\tvcpu->run->system_event.type = KVM_SYSTEM_EVENT_RESET;\n\t\t\tvcpu->run->system_event.ndata = 0;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_HV_EXIT, vcpu)) {\n\t\t\tstruct kvm_vcpu_hv *hv_vcpu = to_hv_vcpu(vcpu);\n\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_HYPERV;\n\t\t\tvcpu->run->hyperv = hv_vcpu->exit;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * KVM_REQ_HV_STIMER has to be processed after\n\t\t * KVM_REQ_CLOCK_UPDATE, because Hyper-V SynIC timers\n\t\t * depend on the guest clock being up-to-date\n\t\t */\n\t\tif (kvm_check_request(KVM_REQ_HV_STIMER, vcpu))\n\t\t\tkvm_hv_process_stimers(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))\n\t\t\tkvm_vcpu_update_apicv(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_APF_READY, vcpu))\n\t\t\tkvm_check_async_pf_completion(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MSR_FILTER_CHANGED, vcpu))\n\t\t\tstatic_call(kvm_x86_msr_filter_changed)(vcpu);\n\n\t\tif (kvm_check_request(KVM_REQ_UPDATE_CPU_DIRTY_LOGGING, vcpu))\n\t\t\tstatic_call(kvm_x86_update_cpu_dirty_logging)(vcpu);\n\t}\n\n\tif (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win ||\n\t    kvm_xen_has_interrupt(vcpu)) {\n\t\t++vcpu->stat.req_event;\n\t\tr = kvm_apic_accept_events(vcpu);\n\t\tif (r < 0) {\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {\n\t\t\tr = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tr = kvm_check_and_inject_events(vcpu, &req_immediate_exit);\n\t\tif (r < 0) {\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (req_int_win)\n\t\t\tstatic_call(kvm_x86_enable_irq_window)(vcpu);\n\n\t\tif (kvm_lapic_enabled(vcpu)) {\n\t\t\tupdate_cr8_intercept(vcpu);\n\t\t\tkvm_lapic_sync_to_vapic(vcpu);\n\t\t}\n\t}\n\n\tr = kvm_mmu_reload(vcpu);\n\tif (unlikely(r)) {\n\t\tgoto cancel_injection;\n\t}\n\n\tpreempt_disable();\n\n\tstatic_call(kvm_x86_prepare_switch_to_guest)(vcpu);\n\n\t/*\n\t * Disable IRQs before setting IN_GUEST_MODE.  Posted interrupt\n\t * IPI are then delayed after guest entry, which ensures that they\n\t * result in virtual interrupt delivery.\n\t */\n\tlocal_irq_disable();\n\n\t/* Store vcpu->apicv_active before vcpu->mode.  */\n\tsmp_store_release(&vcpu->mode, IN_GUEST_MODE);\n\n\tkvm_vcpu_srcu_read_unlock(vcpu);\n\n\t/*\n\t * 1) We should set ->mode before checking ->requests.  Please see\n\t * the comment in kvm_vcpu_exiting_guest_mode().\n\t *\n\t * 2) For APICv, we should set ->mode before checking PID.ON. This\n\t * pairs with the memory barrier implicit in pi_test_and_set_on\n\t * (see vmx_deliver_posted_interrupt).\n\t *\n\t * 3) This also orders the write to mode from any reads to the page\n\t * tables done while the VCPU is running.  Please see the comment\n\t * in kvm_flush_remote_tlbs.\n\t */\n\tsmp_mb__after_srcu_read_unlock();\n\n\t/*\n\t * Process pending posted interrupts to handle the case where the\n\t * notification IRQ arrived in the host, or was never sent (because the\n\t * target vCPU wasn't running).  Do this regardless of the vCPU's APICv\n\t * status, KVM doesn't update assigned devices when APICv is inhibited,\n\t * i.e. they can post interrupts even if APICv is temporarily disabled.\n\t */\n\tif (kvm_lapic_enabled(vcpu))\n\t\tstatic_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);\n\n\tif (kvm_vcpu_exit_request(vcpu)) {\n\t\tvcpu->mode = OUTSIDE_GUEST_MODE;\n\t\tsmp_wmb();\n\t\tlocal_irq_enable();\n\t\tpreempt_enable();\n\t\tkvm_vcpu_srcu_read_lock(vcpu);\n\t\tr = 1;\n\t\tgoto cancel_injection;\n\t}\n\n\tif (req_immediate_exit) {\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tstatic_call(kvm_x86_request_immediate_exit)(vcpu);\n\t}\n\n\tfpregs_assert_state_consistent();\n\tif (test_thread_flag(TIF_NEED_FPU_LOAD))\n\t\tswitch_fpu_return();\n\n\tif (vcpu->arch.guest_fpu.xfd_err)\n\t\twrmsrl(MSR_IA32_XFD_ERR, vcpu->arch.guest_fpu.xfd_err);\n\n\tif (unlikely(vcpu->arch.switch_db_regs)) {\n\t\tset_debugreg(0, 7);\n\t\tset_debugreg(vcpu->arch.eff_db[0], 0);\n\t\tset_debugreg(vcpu->arch.eff_db[1], 1);\n\t\tset_debugreg(vcpu->arch.eff_db[2], 2);\n\t\tset_debugreg(vcpu->arch.eff_db[3], 3);\n\t} else if (unlikely(hw_breakpoint_active())) {\n\t\tset_debugreg(0, 7);\n\t}\n\n\tguest_timing_enter_irqoff();\n\n\tfor (;;) {\n\t\t/*\n\t\t * Assert that vCPU vs. VM APICv state is consistent.  An APICv\n\t\t * update must kick and wait for all vCPUs before toggling the\n\t\t * per-VM state, and responsing vCPUs must wait for the update\n\t\t * to complete before servicing KVM_REQ_APICV_UPDATE.\n\t\t */\n\t\tWARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&\n\t\t\t     (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));\n\n\t\texit_fastpath = static_call(kvm_x86_vcpu_run)(vcpu);\n\t\tif (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))\n\t\t\tbreak;\n\n\t\tif (kvm_lapic_enabled(vcpu))\n\t\t\tstatic_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);\n\n\t\tif (unlikely(kvm_vcpu_exit_request(vcpu))) {\n\t\t\texit_fastpath = EXIT_FASTPATH_EXIT_HANDLED;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * Do this here before restoring debug registers on the host.  And\n\t * since we do this before handling the vmexit, a DR access vmexit\n\t * can (a) read the correct value of the debug registers, (b) set\n\t * KVM_DEBUGREG_WONT_EXIT again.\n\t */\n\tif (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)) {\n\t\tWARN_ON(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP);\n\t\tstatic_call(kvm_x86_sync_dirty_debug_regs)(vcpu);\n\t\tkvm_update_dr0123(vcpu);\n\t\tkvm_update_dr7(vcpu);\n\t}\n\n\t/*\n\t * If the guest has used debug registers, at least dr7\n\t * will be disabled while returning to the host.\n\t * If we don't have active breakpoints in the host, we don't\n\t * care about the messed up debug address registers. But if\n\t * we have some of them active, restore the old state.\n\t */\n\tif (hw_breakpoint_active())\n\t\thw_breakpoint_restore();\n\n\tvcpu->arch.last_vmentry_cpu = vcpu->cpu;\n\tvcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\n\tvcpu->mode = OUTSIDE_GUEST_MODE;\n\tsmp_wmb();\n\n\t/*\n\t * Sync xfd before calling handle_exit_irqoff() which may\n\t * rely on the fact that guest_fpu::xfd is up-to-date (e.g.\n\t * in #NM irqoff handler).\n\t */\n\tif (vcpu->arch.xfd_no_write_intercept)\n\t\tfpu_sync_guest_vmexit_xfd_state();\n\n\tstatic_call(kvm_x86_handle_exit_irqoff)(vcpu);\n\n\tif (vcpu->arch.guest_fpu.xfd_err)\n\t\twrmsrl(MSR_IA32_XFD_ERR, 0);\n\n\t/*\n\t * Consume any pending interrupts, including the possible source of\n\t * VM-Exit on SVM and any ticks that occur between VM-Exit and now.\n\t * An instruction is required after local_irq_enable() to fully unblock\n\t * interrupts on processors that implement an interrupt shadow, the\n\t * stat.exits increment will do nicely.\n\t */\n\tkvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);\n\tlocal_irq_enable();\n\t++vcpu->stat.exits;\n\tlocal_irq_disable();\n\tkvm_after_interrupt(vcpu);\n\n\t/*\n\t * Wait until after servicing IRQs to account guest time so that any\n\t * ticks that occurred while running the guest are properly accounted\n\t * to the guest.  Waiting until IRQs are enabled degrades the accuracy\n\t * of accounting via context tracking, but the loss of accuracy is\n\t * acceptable for all known use cases.\n\t */\n\tguest_timing_exit_irqoff();\n\n\tlocal_irq_enable();\n\tpreempt_enable();\n\n\tkvm_vcpu_srcu_read_lock(vcpu);\n\n\t/*\n\t * Profile KVM exit RIPs:\n\t */\n\tif (unlikely(prof_on == KVM_PROFILING)) {\n\t\tunsigned long rip = kvm_rip_read(vcpu);\n\t\tprofile_hit(KVM_PROFILING, (void *)rip);\n\t}\n\n\tif (unlikely(vcpu->arch.tsc_always_catchup))\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\n\tif (vcpu->arch.apic_attention)\n\t\tkvm_lapic_sync_from_vapic(vcpu);\n\n\tr = static_call(kvm_x86_handle_exit)(vcpu, exit_fastpath);\n\treturn r;\n\ncancel_injection:\n\tif (req_immediate_exit)\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\tstatic_call(kvm_x86_cancel_injection)(vcpu);\n\tif (unlikely(vcpu->arch.apic_attention))\n\t\tkvm_lapic_sync_from_vapic(vcpu);\nout:\n\treturn r;\n}\n\n/* Called within kvm->srcu read side.  */\nstatic inline int vcpu_block(struct kvm_vcpu *vcpu)\n{\n\tbool hv_timer;\n\n\tif (!kvm_arch_vcpu_runnable(vcpu)) {\n\t\t/*\n\t\t * Switch to the software timer before halt-polling/blocking as\n\t\t * the guest's timer may be a break event for the vCPU, and the\n\t\t * hypervisor timer runs only when the CPU is in guest mode.\n\t\t * Switch before halt-polling so that KVM recognizes an expired\n\t\t * timer before blocking.\n\t\t */\n\t\thv_timer = kvm_lapic_hv_timer_in_use(vcpu);\n\t\tif (hv_timer)\n\t\t\tkvm_lapic_switch_to_sw_timer(vcpu);\n\n\t\tkvm_vcpu_srcu_read_unlock(vcpu);\n\t\tif (vcpu->arch.mp_state == KVM_MP_STATE_HALTED)\n\t\t\tkvm_vcpu_halt(vcpu);\n\t\telse\n\t\t\tkvm_vcpu_block(vcpu);\n\t\tkvm_vcpu_srcu_read_lock(vcpu);\n\n\t\tif (hv_timer)\n\t\t\tkvm_lapic_switch_to_hv_timer(vcpu);\n\n\t\t/*\n\t\t * If the vCPU is not runnable, a signal or another host event\n\t\t * of some kind is pending; service it without changing the\n\t\t * vCPU's activity state.\n\t\t */\n\t\tif (!kvm_arch_vcpu_runnable(vcpu))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * Evaluate nested events before exiting the halted state.  This allows\n\t * the halt state to be recorded properly in the VMCS12's activity\n\t * state field (AMD does not have a similar field and a VM-Exit always\n\t * causes a spurious wakeup from HLT).\n\t */\n\tif (is_guest_mode(vcpu)) {\n\t\tif (kvm_check_nested_events(vcpu) < 0)\n\t\t\treturn 0;\n\t}\n\n\tif (kvm_apic_accept_events(vcpu) < 0)\n\t\treturn 0;\n\tswitch(vcpu->arch.mp_state) {\n\tcase KVM_MP_STATE_HALTED:\n\tcase KVM_MP_STATE_AP_RESET_HOLD:\n\t\tvcpu->arch.pv.pv_unhalted = false;\n\t\tvcpu->arch.mp_state =\n\t\t\tKVM_MP_STATE_RUNNABLE;\n\t\tfallthrough;\n\tcase KVM_MP_STATE_RUNNABLE:\n\t\tvcpu->arch.apf.halted = false;\n\t\tbreak;\n\tcase KVM_MP_STATE_INIT_RECEIVED:\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n\treturn 1;\n}\n\nstatic inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)\n{\n\treturn (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&\n\t\t!vcpu->arch.apf.halted);\n}\n\n/* Called within kvm->srcu read side.  */\nstatic int vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\tfor (;;) {\n\t\t/*\n\t\t * If another guest vCPU requests a PV TLB flush in the middle\n\t\t * of instruction emulation, the rest of the emulation could\n\t\t * use a stale page translation. Assume that any code after\n\t\t * this point can start executing an instruction.\n\t\t */\n\t\tvcpu->arch.at_instruction_boundary = false;\n\t\tif (kvm_vcpu_running(vcpu)) {\n\t\t\tr = vcpu_enter_guest(vcpu);\n\t\t} else {\n\t\t\tr = vcpu_block(vcpu);\n\t\t}\n\n\t\tif (r <= 0)\n\t\t\tbreak;\n\n\t\tkvm_clear_request(KVM_REQ_UNBLOCK, vcpu);\n\t\tif (kvm_xen_has_pending_events(vcpu))\n\t\t\tkvm_xen_inject_pending_events(vcpu);\n\n\t\tif (kvm_cpu_has_pending_timer(vcpu))\n\t\t\tkvm_inject_pending_timer_irqs(vcpu);\n\n\t\tif (dm_request_for_irq_injection(vcpu) &&\n\t\t\tkvm_vcpu_ready_for_interrupt_injection(vcpu)) {\n\t\t\tr = 0;\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_IRQ_WINDOW_OPEN;\n\t\t\t++vcpu->stat.request_irq_exits;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (__xfer_to_guest_mode_work_pending()) {\n\t\t\tkvm_vcpu_srcu_read_unlock(vcpu);\n\t\t\tr = xfer_to_guest_mode_handle_work(vcpu);\n\t\t\tkvm_vcpu_srcu_read_lock(vcpu);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn r;\n}\n\nstatic inline int complete_emulated_io(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE);\n}\n\nstatic int complete_emulated_pio(struct kvm_vcpu *vcpu)\n{\n\tBUG_ON(!vcpu->arch.pio.count);\n\n\treturn complete_emulated_io(vcpu);\n}\n\n/*\n * Implements the following, as a state machine:\n *\n * read:\n *   for each fragment\n *     for each mmio piece in the fragment\n *       write gpa, len\n *       exit\n *       copy data\n *   execute insn\n *\n * write:\n *   for each fragment\n *     for each mmio piece in the fragment\n *       write gpa, len\n *       copy data\n *       exit\n */\nstatic int complete_emulated_mmio(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *run = vcpu->run;\n\tstruct kvm_mmio_fragment *frag;\n\tunsigned len;\n\n\tBUG_ON(!vcpu->mmio_needed);\n\n\t/* Complete previous fragment */\n\tfrag = &vcpu->mmio_fragments[vcpu->mmio_cur_fragment];\n\tlen = min(8u, frag->len);\n\tif (!vcpu->mmio_is_write)\n\t\tmemcpy(frag->data, run->mmio.data, len);\n\n\tif (frag->len <= 8) {\n\t\t/* Switch to the next fragment. */\n\t\tfrag++;\n\t\tvcpu->mmio_cur_fragment++;\n\t} else {\n\t\t/* Go forward to the next mmio piece. */\n\t\tfrag->data += len;\n\t\tfrag->gpa += len;\n\t\tfrag->len -= len;\n\t}\n\n\tif (vcpu->mmio_cur_fragment >= vcpu->mmio_nr_fragments) {\n\t\tvcpu->mmio_needed = 0;\n\n\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\tif (vcpu->mmio_is_write)\n\t\t\treturn 1;\n\t\tvcpu->mmio_read_completed = 1;\n\t\treturn complete_emulated_io(vcpu);\n\t}\n\n\trun->exit_reason = KVM_EXIT_MMIO;\n\trun->mmio.phys_addr = frag->gpa;\n\tif (vcpu->mmio_is_write)\n\t\tmemcpy(run->mmio.data, frag->data, min(8u, frag->len));\n\trun->mmio.len = min(8u, frag->len);\n\trun->mmio.is_write = vcpu->mmio_is_write;\n\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\treturn 0;\n}\n\n/* Swap (qemu) user FPU context for the guest FPU context. */\nstatic void kvm_load_guest_fpu(struct kvm_vcpu *vcpu)\n{\n\t/* Exclude PKRU, it's restored separately immediately after VM-Exit. */\n\tfpu_swap_kvm_fpstate(&vcpu->arch.guest_fpu, true);\n\ttrace_kvm_fpu(1);\n}\n\n/* When vcpu_run ends, restore user space FPU context. */\nstatic void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)\n{\n\tfpu_swap_kvm_fpstate(&vcpu->arch.guest_fpu, false);\n\t++vcpu->stat.fpu_reload;\n\ttrace_kvm_fpu(0);\n}\n\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_queued_exception *ex = &vcpu->arch.exception;\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tint r;\n\n\tvcpu_load(vcpu);\n\tkvm_sigset_activate(vcpu);\n\tkvm_run->flags = 0;\n\tkvm_load_guest_fpu(vcpu);\n\n\tkvm_vcpu_srcu_read_lock(vcpu);\n\tif (unlikely(vcpu->arch.mp_state == KVM_MP_STATE_UNINITIALIZED)) {\n\t\tif (kvm_run->immediate_exit) {\n\t\t\tr = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * It should be impossible for the hypervisor timer to be in\n\t\t * use before KVM has ever run the vCPU.\n\t\t */\n\t\tWARN_ON_ONCE(kvm_lapic_hv_timer_in_use(vcpu));\n\n\t\tkvm_vcpu_srcu_read_unlock(vcpu);\n\t\tkvm_vcpu_block(vcpu);\n\t\tkvm_vcpu_srcu_read_lock(vcpu);\n\n\t\tif (kvm_apic_accept_events(vcpu) < 0) {\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tr = -EAGAIN;\n\t\tif (signal_pending(current)) {\n\t\t\tr = -EINTR;\n\t\t\tkvm_run->exit_reason = KVM_EXIT_INTR;\n\t\t\t++vcpu->stat.signal_exits;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif ((kvm_run->kvm_valid_regs & ~KVM_SYNC_X86_VALID_FIELDS) ||\n\t    (kvm_run->kvm_dirty_regs & ~KVM_SYNC_X86_VALID_FIELDS)) {\n\t\tr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (kvm_run->kvm_dirty_regs) {\n\t\tr = sync_regs(vcpu);\n\t\tif (r != 0)\n\t\t\tgoto out;\n\t}\n\n\t/* re-sync apic's tpr */\n\tif (!lapic_in_kernel(vcpu)) {\n\t\tif (kvm_set_cr8(vcpu, kvm_run->cr8) != 0) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * If userspace set a pending exception and L2 is active, convert it to\n\t * a pending VM-Exit if L1 wants to intercept the exception.\n\t */\n\tif (vcpu->arch.exception_from_userspace && is_guest_mode(vcpu) &&\n\t    kvm_x86_ops.nested_ops->is_exception_vmexit(vcpu, ex->vector,\n\t\t\t\t\t\t\tex->error_code)) {\n\t\tkvm_queue_exception_vmexit(vcpu, ex->vector,\n\t\t\t\t\t   ex->has_error_code, ex->error_code,\n\t\t\t\t\t   ex->has_payload, ex->payload);\n\t\tex->injected = false;\n\t\tex->pending = false;\n\t}\n\tvcpu->arch.exception_from_userspace = false;\n\n\tif (unlikely(vcpu->arch.complete_userspace_io)) {\n\t\tint (*cui)(struct kvm_vcpu *) = vcpu->arch.complete_userspace_io;\n\t\tvcpu->arch.complete_userspace_io = NULL;\n\t\tr = cui(vcpu);\n\t\tif (r <= 0)\n\t\t\tgoto out;\n\t} else {\n\t\tWARN_ON_ONCE(vcpu->arch.pio.count);\n\t\tWARN_ON_ONCE(vcpu->mmio_needed);\n\t}\n\n\tif (kvm_run->immediate_exit) {\n\t\tr = -EINTR;\n\t\tgoto out;\n\t}\n\n\tr = static_call(kvm_x86_vcpu_pre_run)(vcpu);\n\tif (r <= 0)\n\t\tgoto out;\n\n\tr = vcpu_run(vcpu);\n\nout:\n\tkvm_put_guest_fpu(vcpu);\n\tif (kvm_run->kvm_valid_regs)\n\t\tstore_regs(vcpu);\n\tpost_kvm_run_save(vcpu);\n\tkvm_vcpu_srcu_read_unlock(vcpu);\n\n\tkvm_sigset_deactivate(vcpu);\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\nstatic void __get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tif (vcpu->arch.emulate_regs_need_sync_to_vcpu) {\n\t\t/*\n\t\t * We are here if userspace calls get_regs() in the middle of\n\t\t * instruction emulation. Registers state needs to be copied\n\t\t * back from emulation context to vcpu. Userspace shouldn't do\n\t\t * that usually, but some bad designed PV devices (vmware\n\t\t * backdoor interface) need this to work\n\t\t */\n\t\temulator_writeback_register_cache(vcpu->arch.emulate_ctxt);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t}\n\tregs->rax = kvm_rax_read(vcpu);\n\tregs->rbx = kvm_rbx_read(vcpu);\n\tregs->rcx = kvm_rcx_read(vcpu);\n\tregs->rdx = kvm_rdx_read(vcpu);\n\tregs->rsi = kvm_rsi_read(vcpu);\n\tregs->rdi = kvm_rdi_read(vcpu);\n\tregs->rsp = kvm_rsp_read(vcpu);\n\tregs->rbp = kvm_rbp_read(vcpu);\n#ifdef CONFIG_X86_64\n\tregs->r8 = kvm_r8_read(vcpu);\n\tregs->r9 = kvm_r9_read(vcpu);\n\tregs->r10 = kvm_r10_read(vcpu);\n\tregs->r11 = kvm_r11_read(vcpu);\n\tregs->r12 = kvm_r12_read(vcpu);\n\tregs->r13 = kvm_r13_read(vcpu);\n\tregs->r14 = kvm_r14_read(vcpu);\n\tregs->r15 = kvm_r15_read(vcpu);\n#endif\n\n\tregs->rip = kvm_rip_read(vcpu);\n\tregs->rflags = kvm_get_rflags(vcpu);\n}\n\nint kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tvcpu_load(vcpu);\n\t__get_regs(vcpu, regs);\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nstatic void __set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tvcpu->arch.emulate_regs_need_sync_from_vcpu = true;\n\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\n\tkvm_rax_write(vcpu, regs->rax);\n\tkvm_rbx_write(vcpu, regs->rbx);\n\tkvm_rcx_write(vcpu, regs->rcx);\n\tkvm_rdx_write(vcpu, regs->rdx);\n\tkvm_rsi_write(vcpu, regs->rsi);\n\tkvm_rdi_write(vcpu, regs->rdi);\n\tkvm_rsp_write(vcpu, regs->rsp);\n\tkvm_rbp_write(vcpu, regs->rbp);\n#ifdef CONFIG_X86_64\n\tkvm_r8_write(vcpu, regs->r8);\n\tkvm_r9_write(vcpu, regs->r9);\n\tkvm_r10_write(vcpu, regs->r10);\n\tkvm_r11_write(vcpu, regs->r11);\n\tkvm_r12_write(vcpu, regs->r12);\n\tkvm_r13_write(vcpu, regs->r13);\n\tkvm_r14_write(vcpu, regs->r14);\n\tkvm_r15_write(vcpu, regs->r15);\n#endif\n\n\tkvm_rip_write(vcpu, regs->rip);\n\tkvm_set_rflags(vcpu, regs->rflags | X86_EFLAGS_FIXED);\n\n\tvcpu->arch.exception.pending = false;\n\tvcpu->arch.exception_vmexit.pending = false;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\n\nint kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tvcpu_load(vcpu);\n\t__set_regs(vcpu, regs);\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nstatic void __get_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\tstruct desc_ptr dt;\n\n\tif (vcpu->arch.guest_state_protected)\n\t\tgoto skip_protected_regs;\n\n\tkvm_get_segment(vcpu, &sregs->cs, VCPU_SREG_CS);\n\tkvm_get_segment(vcpu, &sregs->ds, VCPU_SREG_DS);\n\tkvm_get_segment(vcpu, &sregs->es, VCPU_SREG_ES);\n\tkvm_get_segment(vcpu, &sregs->fs, VCPU_SREG_FS);\n\tkvm_get_segment(vcpu, &sregs->gs, VCPU_SREG_GS);\n\tkvm_get_segment(vcpu, &sregs->ss, VCPU_SREG_SS);\n\n\tkvm_get_segment(vcpu, &sregs->tr, VCPU_SREG_TR);\n\tkvm_get_segment(vcpu, &sregs->ldt, VCPU_SREG_LDTR);\n\n\tstatic_call(kvm_x86_get_idt)(vcpu, &dt);\n\tsregs->idt.limit = dt.size;\n\tsregs->idt.base = dt.address;\n\tstatic_call(kvm_x86_get_gdt)(vcpu, &dt);\n\tsregs->gdt.limit = dt.size;\n\tsregs->gdt.base = dt.address;\n\n\tsregs->cr2 = vcpu->arch.cr2;\n\tsregs->cr3 = kvm_read_cr3(vcpu);\n\nskip_protected_regs:\n\tsregs->cr0 = kvm_read_cr0(vcpu);\n\tsregs->cr4 = kvm_read_cr4(vcpu);\n\tsregs->cr8 = kvm_get_cr8(vcpu);\n\tsregs->efer = vcpu->arch.efer;\n\tsregs->apic_base = kvm_get_apic_base(vcpu);\n}\n\nstatic void __get_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\t__get_sregs_common(vcpu, sregs);\n\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn;\n\n\tif (vcpu->arch.interrupt.injected && !vcpu->arch.interrupt.soft)\n\t\tset_bit(vcpu->arch.interrupt.nr,\n\t\t\t(unsigned long *)sregs->interrupt_bitmap);\n}\n\nstatic void __get_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2)\n{\n\tint i;\n\n\t__get_sregs_common(vcpu, (struct kvm_sregs *)sregs2);\n\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn;\n\n\tif (is_pae_paging(vcpu)) {\n\t\tfor (i = 0 ; i < 4 ; i++)\n\t\t\tsregs2->pdptrs[i] = kvm_pdptr_read(vcpu, i);\n\t\tsregs2->flags |= KVM_SREGS2_FLAGS_PDPTRS_VALID;\n\t}\n}\n\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs)\n{\n\tvcpu_load(vcpu);\n\t__get_sregs(vcpu, sregs);\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state)\n{\n\tint r;\n\n\tvcpu_load(vcpu);\n\tif (kvm_mpx_supported())\n\t\tkvm_load_guest_fpu(vcpu);\n\n\tr = kvm_apic_accept_events(vcpu);\n\tif (r < 0)\n\t\tgoto out;\n\tr = 0;\n\n\tif ((vcpu->arch.mp_state == KVM_MP_STATE_HALTED ||\n\t     vcpu->arch.mp_state == KVM_MP_STATE_AP_RESET_HOLD) &&\n\t    vcpu->arch.pv.pv_unhalted)\n\t\tmp_state->mp_state = KVM_MP_STATE_RUNNABLE;\n\telse\n\t\tmp_state->mp_state = vcpu->arch.mp_state;\n\nout:\n\tif (kvm_mpx_supported())\n\t\tkvm_put_guest_fpu(vcpu);\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\nint kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state)\n{\n\tint ret = -EINVAL;\n\n\tvcpu_load(vcpu);\n\n\tswitch (mp_state->mp_state) {\n\tcase KVM_MP_STATE_UNINITIALIZED:\n\tcase KVM_MP_STATE_HALTED:\n\tcase KVM_MP_STATE_AP_RESET_HOLD:\n\tcase KVM_MP_STATE_INIT_RECEIVED:\n\tcase KVM_MP_STATE_SIPI_RECEIVED:\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tbreak;\n\n\tcase KVM_MP_STATE_RUNNABLE:\n\t\tbreak;\n\n\tdefault:\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Pending INITs are reported using KVM_SET_VCPU_EVENTS, disallow\n\t * forcing the guest into INIT/SIPI if those events are supposed to be\n\t * blocked.  KVM prioritizes SMI over INIT, so reject INIT/SIPI state\n\t * if an SMI is pending as well.\n\t */\n\tif ((!kvm_apic_init_sipi_allowed(vcpu) || vcpu->arch.smi_pending) &&\n\t    (mp_state->mp_state == KVM_MP_STATE_SIPI_RECEIVED ||\n\t     mp_state->mp_state == KVM_MP_STATE_INIT_RECEIVED))\n\t\tgoto out;\n\n\tif (mp_state->mp_state == KVM_MP_STATE_SIPI_RECEIVED) {\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;\n\t\tset_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);\n\t} else\n\t\tvcpu->arch.mp_state = mp_state->mp_state;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tret = 0;\nout:\n\tvcpu_put(vcpu);\n\treturn ret;\n}\n\nint kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int idt_index,\n\t\t    int reason, bool has_error_code, u32 error_code)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint ret;\n\n\tinit_emulate_ctxt(vcpu);\n\n\tret = emulator_task_switch(ctxt, tss_selector, idt_index, reason,\n\t\t\t\t   has_error_code, error_code);\n\tif (ret) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn 0;\n\t}\n\n\tkvm_rip_write(vcpu, ctxt->eip);\n\tkvm_set_rflags(vcpu, ctxt->eflags);\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(kvm_task_switch);\n\nstatic bool kvm_is_valid_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\tif ((sregs->efer & EFER_LME) && (sregs->cr0 & X86_CR0_PG)) {\n\t\t/*\n\t\t * When EFER.LME and CR0.PG are set, the processor is in\n\t\t * 64-bit mode (though maybe in a 32-bit code segment).\n\t\t * CR4.PAE and EFER.LMA must be set.\n\t\t */\n\t\tif (!(sregs->cr4 & X86_CR4_PAE) || !(sregs->efer & EFER_LMA))\n\t\t\treturn false;\n\t\tif (kvm_vcpu_is_illegal_gpa(vcpu, sregs->cr3))\n\t\t\treturn false;\n\t} else {\n\t\t/*\n\t\t * Not in 64-bit mode: EFER.LMA is clear and the code\n\t\t * segment cannot be 64-bit.\n\t\t */\n\t\tif (sregs->efer & EFER_LMA || sregs->cs.l)\n\t\t\treturn false;\n\t}\n\n\treturn kvm_is_valid_cr4(vcpu, sregs->cr4);\n}\n\nstatic int __set_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs,\n\t\tint *mmu_reset_needed, bool update_pdptrs)\n{\n\tstruct msr_data apic_base_msr;\n\tint idx;\n\tstruct desc_ptr dt;\n\n\tif (!kvm_is_valid_sregs(vcpu, sregs))\n\t\treturn -EINVAL;\n\n\tapic_base_msr.data = sregs->apic_base;\n\tapic_base_msr.host_initiated = true;\n\tif (kvm_set_apic_base(vcpu, &apic_base_msr))\n\t\treturn -EINVAL;\n\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn 0;\n\n\tdt.size = sregs->idt.limit;\n\tdt.address = sregs->idt.base;\n\tstatic_call(kvm_x86_set_idt)(vcpu, &dt);\n\tdt.size = sregs->gdt.limit;\n\tdt.address = sregs->gdt.base;\n\tstatic_call(kvm_x86_set_gdt)(vcpu, &dt);\n\n\tvcpu->arch.cr2 = sregs->cr2;\n\t*mmu_reset_needed |= kvm_read_cr3(vcpu) != sregs->cr3;\n\tvcpu->arch.cr3 = sregs->cr3;\n\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_CR3);\n\tstatic_call_cond(kvm_x86_post_set_cr3)(vcpu, sregs->cr3);\n\n\tkvm_set_cr8(vcpu, sregs->cr8);\n\n\t*mmu_reset_needed |= vcpu->arch.efer != sregs->efer;\n\tstatic_call(kvm_x86_set_efer)(vcpu, sregs->efer);\n\n\t*mmu_reset_needed |= kvm_read_cr0(vcpu) != sregs->cr0;\n\tstatic_call(kvm_x86_set_cr0)(vcpu, sregs->cr0);\n\tvcpu->arch.cr0 = sregs->cr0;\n\n\t*mmu_reset_needed |= kvm_read_cr4(vcpu) != sregs->cr4;\n\tstatic_call(kvm_x86_set_cr4)(vcpu, sregs->cr4);\n\n\tif (update_pdptrs) {\n\t\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tif (is_pae_paging(vcpu)) {\n\t\t\tload_pdptrs(vcpu, kvm_read_cr3(vcpu));\n\t\t\t*mmu_reset_needed = 1;\n\t\t}\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t}\n\n\tkvm_set_segment(vcpu, &sregs->cs, VCPU_SREG_CS);\n\tkvm_set_segment(vcpu, &sregs->ds, VCPU_SREG_DS);\n\tkvm_set_segment(vcpu, &sregs->es, VCPU_SREG_ES);\n\tkvm_set_segment(vcpu, &sregs->fs, VCPU_SREG_FS);\n\tkvm_set_segment(vcpu, &sregs->gs, VCPU_SREG_GS);\n\tkvm_set_segment(vcpu, &sregs->ss, VCPU_SREG_SS);\n\n\tkvm_set_segment(vcpu, &sregs->tr, VCPU_SREG_TR);\n\tkvm_set_segment(vcpu, &sregs->ldt, VCPU_SREG_LDTR);\n\n\tupdate_cr8_intercept(vcpu);\n\n\t/* Older userspace won't unhalt the vcpu on reset. */\n\tif (kvm_vcpu_is_bsp(vcpu) && kvm_rip_read(vcpu) == 0xfff0 &&\n\t    sregs->cs.selector == 0xf000 && sregs->cs.base == 0xffff0000 &&\n\t    !is_protmode(vcpu))\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\n\treturn 0;\n}\n\nstatic int __set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\tint pending_vec, max_bits;\n\tint mmu_reset_needed = 0;\n\tint ret = __set_sregs_common(vcpu, sregs, &mmu_reset_needed, true);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (mmu_reset_needed)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\tmax_bits = KVM_NR_INTERRUPTS;\n\tpending_vec = find_first_bit(\n\t\t(const unsigned long *)sregs->interrupt_bitmap, max_bits);\n\n\tif (pending_vec < max_bits) {\n\t\tkvm_queue_interrupt(vcpu, pending_vec, false);\n\t\tpr_debug(\"Set back pending irq %d\\n\", pending_vec);\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t}\n\treturn 0;\n}\n\nstatic int __set_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2)\n{\n\tint mmu_reset_needed = 0;\n\tbool valid_pdptrs = sregs2->flags & KVM_SREGS2_FLAGS_PDPTRS_VALID;\n\tbool pae = (sregs2->cr0 & X86_CR0_PG) && (sregs2->cr4 & X86_CR4_PAE) &&\n\t\t!(sregs2->efer & EFER_LMA);\n\tint i, ret;\n\n\tif (sregs2->flags & ~KVM_SREGS2_FLAGS_PDPTRS_VALID)\n\t\treturn -EINVAL;\n\n\tif (valid_pdptrs && (!pae || vcpu->arch.guest_state_protected))\n\t\treturn -EINVAL;\n\n\tret = __set_sregs_common(vcpu, (struct kvm_sregs *)sregs2,\n\t\t\t\t &mmu_reset_needed, !valid_pdptrs);\n\tif (ret)\n\t\treturn ret;\n\n\tif (valid_pdptrs) {\n\t\tfor (i = 0; i < 4 ; i++)\n\t\t\tkvm_pdptr_write(vcpu, i, sregs2->pdptrs[i]);\n\n\t\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_PDPTR);\n\t\tmmu_reset_needed = 1;\n\t\tvcpu->arch.pdptrs_from_userspace = true;\n\t}\n\tif (mmu_reset_needed)\n\t\tkvm_mmu_reset_context(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs)\n{\n\tint ret;\n\n\tvcpu_load(vcpu);\n\tret = __set_sregs(vcpu, sregs);\n\tvcpu_put(vcpu);\n\treturn ret;\n}\n\nstatic void kvm_arch_vcpu_guestdbg_update_apicv_inhibit(struct kvm *kvm)\n{\n\tbool set = false;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\n\tif (!enable_apicv)\n\t\treturn;\n\n\tdown_write(&kvm->arch.apicv_update_lock);\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_BLOCKIRQ) {\n\t\t\tset = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\t__kvm_set_or_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_BLOCKIRQ, set);\n\tup_write(&kvm->arch.apicv_update_lock);\n}\n\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_guest_debug *dbg)\n{\n\tunsigned long rflags;\n\tint i, r;\n\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn -EINVAL;\n\n\tvcpu_load(vcpu);\n\n\tif (dbg->control & (KVM_GUESTDBG_INJECT_DB | KVM_GUESTDBG_INJECT_BP)) {\n\t\tr = -EBUSY;\n\t\tif (kvm_is_exception_pending(vcpu))\n\t\t\tgoto out;\n\t\tif (dbg->control & KVM_GUESTDBG_INJECT_DB)\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\telse\n\t\t\tkvm_queue_exception(vcpu, BP_VECTOR);\n\t}\n\n\t/*\n\t * Read rflags as long as potentially injected trace flags are still\n\t * filtered out.\n\t */\n\trflags = kvm_get_rflags(vcpu);\n\n\tvcpu->guest_debug = dbg->control;\n\tif (!(vcpu->guest_debug & KVM_GUESTDBG_ENABLE))\n\t\tvcpu->guest_debug = 0;\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {\n\t\tfor (i = 0; i < KVM_NR_DB_REGS; ++i)\n\t\t\tvcpu->arch.eff_db[i] = dbg->arch.debugreg[i];\n\t\tvcpu->arch.guest_debug_dr7 = dbg->arch.debugreg[7];\n\t} else {\n\t\tfor (i = 0; i < KVM_NR_DB_REGS; i++)\n\t\t\tvcpu->arch.eff_db[i] = vcpu->arch.db[i];\n\t}\n\tkvm_update_dr7(vcpu);\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\tvcpu->arch.singlestep_rip = kvm_get_linear_rip(vcpu);\n\n\t/*\n\t * Trigger an rflags update that will inject or remove the trace\n\t * flags.\n\t */\n\tkvm_set_rflags(vcpu, rflags);\n\n\tstatic_call(kvm_x86_update_exception_bitmap)(vcpu);\n\n\tkvm_arch_vcpu_guestdbg_update_apicv_inhibit(vcpu->kvm);\n\n\tr = 0;\n\nout:\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\n/*\n * Translate a guest virtual address to a guest physical address.\n */\nint kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_translation *tr)\n{\n\tunsigned long vaddr = tr->linear_address;\n\tgpa_t gpa;\n\tint idx;\n\n\tvcpu_load(vcpu);\n\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\tgpa = kvm_mmu_gva_to_gpa_system(vcpu, vaddr, NULL);\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\ttr->physical_address = gpa;\n\ttr->valid = gpa != INVALID_GPA;\n\ttr->writeable = 1;\n\ttr->usermode = 0;\n\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\n{\n\tstruct fxregs_state *fxsave;\n\n\tif (fpstate_is_confidential(&vcpu->arch.guest_fpu))\n\t\treturn 0;\n\n\tvcpu_load(vcpu);\n\n\tfxsave = &vcpu->arch.guest_fpu.fpstate->regs.fxsave;\n\tmemcpy(fpu->fpr, fxsave->st_space, 128);\n\tfpu->fcw = fxsave->cwd;\n\tfpu->fsw = fxsave->swd;\n\tfpu->ftwx = fxsave->twd;\n\tfpu->last_opcode = fxsave->fop;\n\tfpu->last_ip = fxsave->rip;\n\tfpu->last_dp = fxsave->rdp;\n\tmemcpy(fpu->xmm, fxsave->xmm_space, sizeof(fxsave->xmm_space));\n\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\n{\n\tstruct fxregs_state *fxsave;\n\n\tif (fpstate_is_confidential(&vcpu->arch.guest_fpu))\n\t\treturn 0;\n\n\tvcpu_load(vcpu);\n\n\tfxsave = &vcpu->arch.guest_fpu.fpstate->regs.fxsave;\n\n\tmemcpy(fxsave->st_space, fpu->fpr, 128);\n\tfxsave->cwd = fpu->fcw;\n\tfxsave->swd = fpu->fsw;\n\tfxsave->twd = fpu->ftwx;\n\tfxsave->fop = fpu->last_opcode;\n\tfxsave->rip = fpu->last_ip;\n\tfxsave->rdp = fpu->last_dp;\n\tmemcpy(fxsave->xmm_space, fpu->xmm, sizeof(fxsave->xmm_space));\n\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nstatic void store_regs(struct kvm_vcpu *vcpu)\n{\n\tBUILD_BUG_ON(sizeof(struct kvm_sync_regs) > SYNC_REGS_SIZE_BYTES);\n\n\tif (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_REGS)\n\t\t__get_regs(vcpu, &vcpu->run->s.regs.regs);\n\n\tif (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_SREGS)\n\t\t__get_sregs(vcpu, &vcpu->run->s.regs.sregs);\n\n\tif (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_EVENTS)\n\t\tkvm_vcpu_ioctl_x86_get_vcpu_events(\n\t\t\t\tvcpu, &vcpu->run->s.regs.events);\n}\n\nstatic int sync_regs(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_REGS) {\n\t\t__set_regs(vcpu, &vcpu->run->s.regs.regs);\n\t\tvcpu->run->kvm_dirty_regs &= ~KVM_SYNC_X86_REGS;\n\t}\n\tif (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_SREGS) {\n\t\tif (__set_sregs(vcpu, &vcpu->run->s.regs.sregs))\n\t\t\treturn -EINVAL;\n\t\tvcpu->run->kvm_dirty_regs &= ~KVM_SYNC_X86_SREGS;\n\t}\n\tif (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_EVENTS) {\n\t\tif (kvm_vcpu_ioctl_x86_set_vcpu_events(\n\t\t\t\tvcpu, &vcpu->run->s.regs.events))\n\t\t\treturn -EINVAL;\n\t\tvcpu->run->kvm_dirty_regs &= ~KVM_SYNC_X86_EVENTS;\n\t}\n\n\treturn 0;\n}\n\nint kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)\n{\n\tif (kvm_check_tsc_unstable() && kvm->created_vcpus)\n\t\tpr_warn_once(\"kvm: SMP vm created on host with unstable TSC; \"\n\t\t\t     \"guest TSC will not be reliable\\n\");\n\n\tif (!kvm->arch.max_vcpu_ids)\n\t\tkvm->arch.max_vcpu_ids = KVM_MAX_VCPU_IDS;\n\n\tif (id >= kvm->arch.max_vcpu_ids)\n\t\treturn -EINVAL;\n\n\treturn static_call(kvm_x86_vcpu_precreate)(kvm);\n}\n\nint kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)\n{\n\tstruct page *page;\n\tint r;\n\n\tvcpu->arch.last_vmentry_cpu = -1;\n\tvcpu->arch.regs_avail = ~0;\n\tvcpu->arch.regs_dirty = ~0;\n\n\tkvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm, vcpu, KVM_HOST_USES_PFN);\n\n\tif (!irqchip_in_kernel(vcpu->kvm) || kvm_vcpu_is_reset_bsp(vcpu))\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\telse\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;\n\n\tr = kvm_mmu_create(vcpu);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (irqchip_in_kernel(vcpu->kvm)) {\n\t\tr = kvm_create_lapic(vcpu, lapic_timer_advance_ns);\n\t\tif (r < 0)\n\t\t\tgoto fail_mmu_destroy;\n\n\t\t/*\n\t\t * Defer evaluating inhibits until the vCPU is first run, as\n\t\t * this vCPU will not get notified of any changes until this\n\t\t * vCPU is visible to other vCPUs (marked online and added to\n\t\t * the set of vCPUs).  Opportunistically mark APICv active as\n\t\t * VMX in particularly is highly unlikely to have inhibits.\n\t\t * Ignore the current per-VM APICv state so that vCPU creation\n\t\t * is guaranteed to run with a deterministic value, the request\n\t\t * will ensure the vCPU gets the correct state before VM-Entry.\n\t\t */\n\t\tif (enable_apicv) {\n\t\t\tvcpu->arch.apic->apicv_active = true;\n\t\t\tkvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);\n\t\t}\n\t} else\n\t\tstatic_branch_inc(&kvm_has_noapic_vcpu);\n\n\tr = -ENOMEM;\n\n\tpage = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\tif (!page)\n\t\tgoto fail_free_lapic;\n\tvcpu->arch.pio_data = page_address(page);\n\n\tvcpu->arch.mce_banks = kcalloc(KVM_MAX_MCE_BANKS * 4, sizeof(u64),\n\t\t\t\t       GFP_KERNEL_ACCOUNT);\n\tvcpu->arch.mci_ctl2_banks = kcalloc(KVM_MAX_MCE_BANKS, sizeof(u64),\n\t\t\t\t\t    GFP_KERNEL_ACCOUNT);\n\tif (!vcpu->arch.mce_banks || !vcpu->arch.mci_ctl2_banks)\n\t\tgoto fail_free_mce_banks;\n\tvcpu->arch.mcg_cap = KVM_MAX_MCE_BANKS;\n\n\tif (!zalloc_cpumask_var(&vcpu->arch.wbinvd_dirty_mask,\n\t\t\t\tGFP_KERNEL_ACCOUNT))\n\t\tgoto fail_free_mce_banks;\n\n\tif (!alloc_emulate_ctxt(vcpu))\n\t\tgoto free_wbinvd_dirty_mask;\n\n\tif (!fpu_alloc_guest_fpstate(&vcpu->arch.guest_fpu)) {\n\t\tpr_err(\"kvm: failed to allocate vcpu's fpu\\n\");\n\t\tgoto free_emulate_ctxt;\n\t}\n\n\tvcpu->arch.maxphyaddr = cpuid_query_maxphyaddr(vcpu);\n\tvcpu->arch.reserved_gpa_bits = kvm_vcpu_reserved_gpa_bits_raw(vcpu);\n\n\tvcpu->arch.pat = MSR_IA32_CR_PAT_DEFAULT;\n\n\tkvm_async_pf_hash_reset(vcpu);\n\n\tvcpu->arch.perf_capabilities = kvm_caps.supported_perf_cap;\n\tkvm_pmu_init(vcpu);\n\n\tvcpu->arch.pending_external_vector = -1;\n\tvcpu->arch.preempted_in_kernel = false;\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\tvcpu->arch.hv_root_tdp = INVALID_PAGE;\n#endif\n\n\tr = static_call(kvm_x86_vcpu_create)(vcpu);\n\tif (r)\n\t\tgoto free_guest_fpu;\n\n\tvcpu->arch.arch_capabilities = kvm_get_arch_capabilities();\n\tvcpu->arch.msr_platform_info = MSR_PLATFORM_INFO_CPUID_FAULT;\n\tkvm_xen_init_vcpu(vcpu);\n\tkvm_vcpu_mtrr_init(vcpu);\n\tvcpu_load(vcpu);\n\tkvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);\n\tkvm_vcpu_reset(vcpu, false);\n\tkvm_init_mmu(vcpu);\n\tvcpu_put(vcpu);\n\treturn 0;\n\nfree_guest_fpu:\n\tfpu_free_guest_fpstate(&vcpu->arch.guest_fpu);\nfree_emulate_ctxt:\n\tkmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);\nfree_wbinvd_dirty_mask:\n\tfree_cpumask_var(vcpu->arch.wbinvd_dirty_mask);\nfail_free_mce_banks:\n\tkfree(vcpu->arch.mce_banks);\n\tkfree(vcpu->arch.mci_ctl2_banks);\n\tfree_page((unsigned long)vcpu->arch.pio_data);\nfail_free_lapic:\n\tkvm_free_lapic(vcpu);\nfail_mmu_destroy:\n\tkvm_mmu_destroy(vcpu);\n\treturn r;\n}\n\nvoid kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\n\tif (mutex_lock_killable(&vcpu->mutex))\n\t\treturn;\n\tvcpu_load(vcpu);\n\tkvm_synchronize_tsc(vcpu, 0);\n\tvcpu_put(vcpu);\n\n\t/* poll control enabled by default */\n\tvcpu->arch.msr_kvm_poll_control = 1;\n\n\tmutex_unlock(&vcpu->mutex);\n\n\tif (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)\n\t\tschedule_delayed_work(&kvm->arch.kvmclock_sync_work,\n\t\t\t\t\t\tKVMCLOCK_SYNC_PERIOD);\n}\n\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)\n{\n\tint idx;\n\n\tkvmclock_reset(vcpu);\n\n\tstatic_call(kvm_x86_vcpu_free)(vcpu);\n\n\tkmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);\n\tfree_cpumask_var(vcpu->arch.wbinvd_dirty_mask);\n\tfpu_free_guest_fpstate(&vcpu->arch.guest_fpu);\n\n\tkvm_xen_destroy_vcpu(vcpu);\n\tkvm_hv_vcpu_uninit(vcpu);\n\tkvm_pmu_destroy(vcpu);\n\tkfree(vcpu->arch.mce_banks);\n\tkfree(vcpu->arch.mci_ctl2_banks);\n\tkvm_free_lapic(vcpu);\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\tkvm_mmu_destroy(vcpu);\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\tfree_page((unsigned long)vcpu->arch.pio_data);\n\tkvfree(vcpu->arch.cpuid_entries);\n\tif (!lapic_in_kernel(vcpu))\n\t\tstatic_branch_dec(&kvm_has_noapic_vcpu);\n}\n\nvoid kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct kvm_cpuid_entry2 *cpuid_0x1;\n\tunsigned long old_cr0 = kvm_read_cr0(vcpu);\n\tunsigned long new_cr0;\n\n\t/*\n\t * Several of the \"set\" flows, e.g. ->set_cr0(), read other registers\n\t * to handle side effects.  RESET emulation hits those flows and relies\n\t * on emulated/virtualized registers, including those that are loaded\n\t * into hardware, to be zeroed at vCPU creation.  Use CRs as a sentinel\n\t * to detect improper or missing initialization.\n\t */\n\tWARN_ON_ONCE(!init_event &&\n\t\t     (old_cr0 || kvm_read_cr3(vcpu) || kvm_read_cr4(vcpu)));\n\n\t/*\n\t * SVM doesn't unconditionally VM-Exit on INIT and SHUTDOWN, thus it's\n\t * possible to INIT the vCPU while L2 is active.  Force the vCPU back\n\t * into L1 as EFER.SVME is cleared on INIT (along with all other EFER\n\t * bits), i.e. virtualization is disabled.\n\t */\n\tif (is_guest_mode(vcpu))\n\t\tkvm_leave_nested(vcpu);\n\n\tkvm_lapic_reset(vcpu, init_event);\n\n\tWARN_ON_ONCE(is_guest_mode(vcpu) || is_smm(vcpu));\n\tvcpu->arch.hflags = 0;\n\n\tvcpu->arch.smi_pending = 0;\n\tvcpu->arch.smi_count = 0;\n\tatomic_set(&vcpu->arch.nmi_queued, 0);\n\tvcpu->arch.nmi_pending = 0;\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_interrupt_queue(vcpu);\n\tkvm_clear_exception_queue(vcpu);\n\n\tmemset(vcpu->arch.db, 0, sizeof(vcpu->arch.db));\n\tkvm_update_dr0123(vcpu);\n\tvcpu->arch.dr6 = DR6_ACTIVE_LOW;\n\tvcpu->arch.dr7 = DR7_FIXED_1;\n\tkvm_update_dr7(vcpu);\n\n\tvcpu->arch.cr2 = 0;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\tvcpu->arch.apf.msr_en_val = 0;\n\tvcpu->arch.apf.msr_int_val = 0;\n\tvcpu->arch.st.msr_val = 0;\n\n\tkvmclock_reset(vcpu);\n\n\tkvm_clear_async_pf_completion_queue(vcpu);\n\tkvm_async_pf_hash_reset(vcpu);\n\tvcpu->arch.apf.halted = false;\n\n\tif (vcpu->arch.guest_fpu.fpstate && kvm_mpx_supported()) {\n\t\tstruct fpstate *fpstate = vcpu->arch.guest_fpu.fpstate;\n\n\t\t/*\n\t\t * All paths that lead to INIT are required to load the guest's\n\t\t * FPU state (because most paths are buried in KVM_RUN).\n\t\t */\n\t\tif (init_event)\n\t\t\tkvm_put_guest_fpu(vcpu);\n\n\t\tfpstate_clear_xstate_component(fpstate, XFEATURE_BNDREGS);\n\t\tfpstate_clear_xstate_component(fpstate, XFEATURE_BNDCSR);\n\n\t\tif (init_event)\n\t\t\tkvm_load_guest_fpu(vcpu);\n\t}\n\n\tif (!init_event) {\n\t\tkvm_pmu_reset(vcpu);\n\t\tvcpu->arch.smbase = 0x30000;\n\n\t\tvcpu->arch.msr_misc_features_enables = 0;\n\t\tvcpu->arch.ia32_misc_enable_msr = MSR_IA32_MISC_ENABLE_PEBS_UNAVAIL |\n\t\t\t\t\t\t  MSR_IA32_MISC_ENABLE_BTS_UNAVAIL;\n\n\t\t__kvm_set_xcr(vcpu, 0, XFEATURE_MASK_FP);\n\t\t__kvm_set_msr(vcpu, MSR_IA32_XSS, 0, true);\n\t}\n\n\t/* All GPRs except RDX (handled below) are zeroed on RESET/INIT. */\n\tmemset(vcpu->arch.regs, 0, sizeof(vcpu->arch.regs));\n\tkvm_register_mark_dirty(vcpu, VCPU_REGS_RSP);\n\n\t/*\n\t * Fall back to KVM's default Family/Model/Stepping of 0x600 (P6/Athlon)\n\t * if no CPUID match is found.  Note, it's impossible to get a match at\n\t * RESET since KVM emulates RESET before exposing the vCPU to userspace,\n\t * i.e. it's impossible for kvm_find_cpuid_entry() to find a valid entry\n\t * on RESET.  But, go through the motions in case that's ever remedied.\n\t */\n\tcpuid_0x1 = kvm_find_cpuid_entry(vcpu, 1);\n\tkvm_rdx_write(vcpu, cpuid_0x1 ? cpuid_0x1->eax : 0x600);\n\n\tstatic_call(kvm_x86_vcpu_reset)(vcpu, init_event);\n\n\tkvm_set_rflags(vcpu, X86_EFLAGS_FIXED);\n\tkvm_rip_write(vcpu, 0xfff0);\n\n\tvcpu->arch.cr3 = 0;\n\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_CR3);\n\n\t/*\n\t * CR0.CD/NW are set on RESET, preserved on INIT.  Note, some versions\n\t * of Intel's SDM list CD/NW as being set on INIT, but they contradict\n\t * (or qualify) that with a footnote stating that CD/NW are preserved.\n\t */\n\tnew_cr0 = X86_CR0_ET;\n\tif (init_event)\n\t\tnew_cr0 |= (old_cr0 & (X86_CR0_NW | X86_CR0_CD));\n\telse\n\t\tnew_cr0 |= X86_CR0_NW | X86_CR0_CD;\n\n\tstatic_call(kvm_x86_set_cr0)(vcpu, new_cr0);\n\tstatic_call(kvm_x86_set_cr4)(vcpu, 0);\n\tstatic_call(kvm_x86_set_efer)(vcpu, 0);\n\tstatic_call(kvm_x86_update_exception_bitmap)(vcpu);\n\n\t/*\n\t * On the standard CR0/CR4/EFER modification paths, there are several\n\t * complex conditions determining whether the MMU has to be reset and/or\n\t * which PCIDs have to be flushed.  However, CR0.WP and the paging-related\n\t * bits in CR4 and EFER are irrelevant if CR0.PG was '0'; and a reset+flush\n\t * is needed anyway if CR0.PG was '1' (which can only happen for INIT, as\n\t * CR0 will be '0' prior to RESET).  So we only need to check CR0.PG here.\n\t */\n\tif (old_cr0 & X86_CR0_PG) {\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t\tkvm_mmu_reset_context(vcpu);\n\t}\n\n\t/*\n\t * Intel's SDM states that all TLB entries are flushed on INIT.  AMD's\n\t * APM states the TLBs are untouched by INIT, but it also states that\n\t * the TLBs are flushed on \"External initialization of the processor.\"\n\t * Flush the guest TLB regardless of vendor, there is no meaningful\n\t * benefit in relying on the guest to flush the TLB immediately after\n\t * INIT.  A spurious TLB flush is benign and likely negligible from a\n\t * performance perspective.\n\t */\n\tif (init_event)\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_reset);\n\nvoid kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)\n{\n\tstruct kvm_segment cs;\n\n\tkvm_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\tcs.selector = vector << 8;\n\tcs.base = vector << 12;\n\tkvm_set_segment(vcpu, &cs, VCPU_SREG_CS);\n\tkvm_rip_write(vcpu, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_deliver_sipi_vector);\n\nint kvm_arch_hardware_enable(void)\n{\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\tint ret;\n\tu64 local_tsc;\n\tu64 max_tsc = 0;\n\tbool stable, backwards_tsc = false;\n\n\tkvm_user_return_msr_cpu_online();\n\tret = static_call(kvm_x86_hardware_enable)();\n\tif (ret != 0)\n\t\treturn ret;\n\n\tlocal_tsc = rdtsc();\n\tstable = !kvm_check_tsc_unstable();\n\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (!stable && vcpu->cpu == smp_processor_id())\n\t\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\t\tif (stable && vcpu->arch.last_host_tsc > local_tsc) {\n\t\t\t\tbackwards_tsc = true;\n\t\t\t\tif (vcpu->arch.last_host_tsc > max_tsc)\n\t\t\t\t\tmax_tsc = vcpu->arch.last_host_tsc;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Sometimes, even reliable TSCs go backwards.  This happens on\n\t * platforms that reset TSC during suspend or hibernate actions, but\n\t * maintain synchronization.  We must compensate.  Fortunately, we can\n\t * detect that condition here, which happens early in CPU bringup,\n\t * before any KVM threads can be running.  Unfortunately, we can't\n\t * bring the TSCs fully up to date with real time, as we aren't yet far\n\t * enough into CPU bringup that we know how much real time has actually\n\t * elapsed; our helper function, ktime_get_boottime_ns() will be using boot\n\t * variables that haven't been updated yet.\n\t *\n\t * So we simply find the maximum observed TSC above, then record the\n\t * adjustment to TSC in each VCPU.  When the VCPU later gets loaded,\n\t * the adjustment will be applied.  Note that we accumulate\n\t * adjustments, in case multiple suspend cycles happen before some VCPU\n\t * gets a chance to run again.  In the event that no KVM threads get a\n\t * chance to run, we will miss the entire elapsed period, as we'll have\n\t * reset last_host_tsc, so VCPUs will not have the TSC adjusted and may\n\t * loose cycle time.  This isn't too big a deal, since the loss will be\n\t * uniform across all VCPUs (not to mention the scenario is extremely\n\t * unlikely). It is possible that a second hibernate recovery happens\n\t * much faster than a first, causing the observed TSC here to be\n\t * smaller; this would require additional padding adjustment, which is\n\t * why we set last_host_tsc to the local tsc observed here.\n\t *\n\t * N.B. - this code below runs only on platforms with reliable TSC,\n\t * as that is the only way backwards_tsc is set above.  Also note\n\t * that this runs for ALL vcpus, which is not a bug; all VCPUs should\n\t * have the same delta_cyc adjustment applied if backwards_tsc\n\t * is detected.  Note further, this adjustment is only done once,\n\t * as we reset last_host_tsc on all VCPUs to stop this from being\n\t * called multiple times (one for each physical CPU bringup).\n\t *\n\t * Platforms with unreliable TSCs don't have to deal with this, they\n\t * will be compensated by the logic in vcpu_load, which sets the TSC to\n\t * catchup mode.  This will catchup all VCPUs to real time, but cannot\n\t * guarantee that they stay in perfect synchronization.\n\t */\n\tif (backwards_tsc) {\n\t\tu64 delta_cyc = max_tsc - local_tsc;\n\t\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\t\tkvm->arch.backwards_tsc_observed = true;\n\t\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\t\tvcpu->arch.tsc_offset_adjustment += delta_cyc;\n\t\t\t\tvcpu->arch.last_host_tsc = local_tsc;\n\t\t\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We have to disable TSC offset matching.. if you were\n\t\t\t * booting a VM while issuing an S4 host suspend....\n\t\t\t * you may have some problem.  Solving this issue is\n\t\t\t * left as an exercise to the reader.\n\t\t\t */\n\t\t\tkvm->arch.last_tsc_nsec = 0;\n\t\t\tkvm->arch.last_tsc_write = 0;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nvoid kvm_arch_hardware_disable(void)\n{\n\tstatic_call(kvm_x86_hardware_disable)();\n\tdrop_user_return_notifiers();\n}\n\nstatic inline void kvm_ops_update(struct kvm_x86_init_ops *ops)\n{\n\tmemcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));\n\n#define __KVM_X86_OP(func) \\\n\tstatic_call_update(kvm_x86_##func, kvm_x86_ops.func);\n#define KVM_X86_OP(func) \\\n\tWARN_ON(!kvm_x86_ops.func); __KVM_X86_OP(func)\n#define KVM_X86_OP_OPTIONAL __KVM_X86_OP\n#define KVM_X86_OP_OPTIONAL_RET0(func) \\\n\tstatic_call_update(kvm_x86_##func, (void *)kvm_x86_ops.func ? : \\\n\t\t\t\t\t   (void *)__static_call_return0);\n#include <asm/kvm-x86-ops.h>\n#undef __KVM_X86_OP\n\n\tkvm_pmu_ops_update(ops->pmu_ops);\n}\n\nint kvm_arch_hardware_setup(void *opaque)\n{\n\tstruct kvm_x86_init_ops *ops = opaque;\n\tint r;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES))\n\t\trdmsrl(MSR_IA32_XSS, host_xss);\n\n\tkvm_init_pmu_capability();\n\n\tr = ops->hardware_setup();\n\tif (r != 0)\n\t\treturn r;\n\n\tkvm_ops_update(ops);\n\n\tkvm_register_perf_callbacks(ops->handle_intel_pt_intr);\n\n\tif (!kvm_cpu_cap_has(X86_FEATURE_XSAVES))\n\t\tkvm_caps.supported_xss = 0;\n\n#define __kvm_cpu_cap_has(UNUSED_, f) kvm_cpu_cap_has(f)\n\tcr4_reserved_bits = __cr4_reserved_bits(__kvm_cpu_cap_has, UNUSED_);\n#undef __kvm_cpu_cap_has\n\n\tif (kvm_caps.has_tsc_control) {\n\t\t/*\n\t\t * Make sure the user can only configure tsc_khz values that\n\t\t * fit into a signed integer.\n\t\t * A min value is not calculated because it will always\n\t\t * be 1 on all machines.\n\t\t */\n\t\tu64 max = min(0x7fffffffULL,\n\t\t\t      __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));\n\t\tkvm_caps.max_guest_tsc_khz = max;\n\t}\n\tkvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;\n\tkvm_init_msr_list();\n\treturn 0;\n}\n\nvoid kvm_arch_hardware_unsetup(void)\n{\n\tkvm_unregister_perf_callbacks();\n\n\tstatic_call(kvm_x86_hardware_unsetup)();\n}\n\nint kvm_arch_check_processor_compat(void *opaque)\n{\n\tstruct cpuinfo_x86 *c = &cpu_data(smp_processor_id());\n\tstruct kvm_x86_init_ops *ops = opaque;\n\n\tWARN_ON(!irqs_disabled());\n\n\tif (__cr4_reserved_bits(cpu_has, c) !=\n\t    __cr4_reserved_bits(cpu_has, &boot_cpu_data))\n\t\treturn -EIO;\n\n\treturn ops->check_processor_compatibility();\n}\n\nbool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->kvm->arch.bsp_vcpu_id == vcpu->vcpu_id;\n}\n\nbool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)\n{\n\treturn (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;\n}\n\n__read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);\nEXPORT_SYMBOL_GPL(kvm_has_noapic_vcpu);\n\nvoid kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\tif (pmu->version && unlikely(pmu->event_count)) {\n\t\tpmu->need_cleanup = true;\n\t\tkvm_make_request(KVM_REQ_PMU, vcpu);\n\t}\n\tstatic_call(kvm_x86_sched_in)(vcpu, cpu);\n}\n\nvoid kvm_arch_free_vm(struct kvm *kvm)\n{\n\tkfree(to_kvm_hv(kvm)->hv_pa_pg);\n\t__kvm_arch_free_vm(kvm);\n}\n\n\nint kvm_arch_init_vm(struct kvm *kvm, unsigned long type)\n{\n\tint ret;\n\tunsigned long flags;\n\n\tif (type)\n\t\treturn -EINVAL;\n\n\tret = kvm_page_track_init(kvm);\n\tif (ret)\n\t\tgoto out;\n\n\tret = kvm_mmu_init_vm(kvm);\n\tif (ret)\n\t\tgoto out_page_track;\n\n\tret = static_call(kvm_x86_vm_init)(kvm);\n\tif (ret)\n\t\tgoto out_uninit_mmu;\n\n\tINIT_HLIST_HEAD(&kvm->arch.mask_notifier_list);\n\tINIT_LIST_HEAD(&kvm->arch.assigned_dev_head);\n\tatomic_set(&kvm->arch.noncoherent_dma_count, 0);\n\n\t/* Reserve bit 0 of irq_sources_bitmap for userspace irq source */\n\tset_bit(KVM_USERSPACE_IRQ_SOURCE_ID, &kvm->arch.irq_sources_bitmap);\n\t/* Reserve bit 1 of irq_sources_bitmap for irqfd-resampler */\n\tset_bit(KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t&kvm->arch.irq_sources_bitmap);\n\n\traw_spin_lock_init(&kvm->arch.tsc_write_lock);\n\tmutex_init(&kvm->arch.apic_map_lock);\n\tseqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);\n\tkvm->arch.kvmclock_offset = -get_kvmclock_base_ns();\n\n\traw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);\n\tpvclock_update_vm_gtod_copy(kvm);\n\traw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);\n\n\tkvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;\n\tkvm->arch.guest_can_read_msr_platform_info = true;\n\tkvm->arch.enable_pmu = enable_pmu;\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\tspin_lock_init(&kvm->arch.hv_root_tdp_lock);\n\tkvm->arch.hv_root_tdp = INVALID_PAGE;\n#endif\n\n\tINIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);\n\tINIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);\n\n\tkvm_apicv_init(kvm);\n\tkvm_hv_init_vm(kvm);\n\tkvm_xen_init_vm(kvm);\n\n\treturn 0;\n\nout_uninit_mmu:\n\tkvm_mmu_uninit_vm(kvm);\nout_page_track:\n\tkvm_page_track_cleanup(kvm);\nout:\n\treturn ret;\n}\n\nint kvm_arch_post_init_vm(struct kvm *kvm)\n{\n\treturn kvm_mmu_post_init_vm(kvm);\n}\n\nstatic void kvm_unload_vcpu_mmu(struct kvm_vcpu *vcpu)\n{\n\tvcpu_load(vcpu);\n\tkvm_mmu_unload(vcpu);\n\tvcpu_put(vcpu);\n}\n\nstatic void kvm_unload_vcpu_mmus(struct kvm *kvm)\n{\n\tunsigned long i;\n\tstruct kvm_vcpu *vcpu;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tkvm_clear_async_pf_completion_queue(vcpu);\n\t\tkvm_unload_vcpu_mmu(vcpu);\n\t}\n}\n\nvoid kvm_arch_sync_events(struct kvm *kvm)\n{\n\tcancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);\n\tcancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);\n\tkvm_free_pit(kvm);\n}\n\n/**\n * __x86_set_memory_region: Setup KVM internal memory slot\n *\n * @kvm: the kvm pointer to the VM.\n * @id: the slot ID to setup.\n * @gpa: the GPA to install the slot (unused when @size == 0).\n * @size: the size of the slot. Set to zero to uninstall a slot.\n *\n * This function helps to setup a KVM internal memory slot.  Specify\n * @size > 0 to install a new slot, while @size == 0 to uninstall a\n * slot.  The return code can be one of the following:\n *\n *   HVA:           on success (uninstall will return a bogus HVA)\n *   -errno:        on error\n *\n * The caller should always use IS_ERR() to check the return value\n * before use.  Note, the KVM internal memory slots are guaranteed to\n * remain valid and unchanged until the VM is destroyed, i.e., the\n * GPA->HVA translation will not change.  However, the HVA is a user\n * address, i.e. its accessibility is not guaranteed, and must be\n * accessed via __copy_{to,from}_user().\n */\nvoid __user * __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa,\n\t\t\t\t      u32 size)\n{\n\tint i, r;\n\tunsigned long hva, old_npages;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tstruct kvm_memory_slot *slot;\n\n\t/* Called with kvm->slots_lock held.  */\n\tif (WARN_ON(id >= KVM_MEM_SLOTS_NUM))\n\t\treturn ERR_PTR_USR(-EINVAL);\n\n\tslot = id_to_memslot(slots, id);\n\tif (size) {\n\t\tif (slot && slot->npages)\n\t\t\treturn ERR_PTR_USR(-EEXIST);\n\n\t\t/*\n\t\t * MAP_SHARED to prevent internal slot pages from being moved\n\t\t * by fork()/COW.\n\t\t */\n\t\thva = vm_mmap(NULL, 0, size, PROT_READ | PROT_WRITE,\n\t\t\t      MAP_SHARED | MAP_ANONYMOUS, 0);\n\t\tif (IS_ERR((void *)hva))\n\t\t\treturn (void __user *)hva;\n\t} else {\n\t\tif (!slot || !slot->npages)\n\t\t\treturn NULL;\n\n\t\told_npages = slot->npages;\n\t\thva = slot->userspace_addr;\n\t}\n\n\tfor (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {\n\t\tstruct kvm_userspace_memory_region m;\n\n\t\tm.slot = id | (i << 16);\n\t\tm.flags = 0;\n\t\tm.guest_phys_addr = gpa;\n\t\tm.userspace_addr = hva;\n\t\tm.memory_size = size;\n\t\tr = __kvm_set_memory_region(kvm, &m);\n\t\tif (r < 0)\n\t\t\treturn ERR_PTR_USR(r);\n\t}\n\n\tif (!size)\n\t\tvm_munmap(hva, old_npages * PAGE_SIZE);\n\n\treturn (void __user *)hva;\n}\nEXPORT_SYMBOL_GPL(__x86_set_memory_region);\n\nvoid kvm_arch_pre_destroy_vm(struct kvm *kvm)\n{\n\tkvm_mmu_pre_destroy_vm(kvm);\n}\n\nvoid kvm_arch_destroy_vm(struct kvm *kvm)\n{\n\tif (current->mm == kvm->mm) {\n\t\t/*\n\t\t * Free memory regions allocated on behalf of userspace,\n\t\t * unless the memory map has changed due to process exit\n\t\t * or fd copying.\n\t\t */\n\t\tmutex_lock(&kvm->slots_lock);\n\t\t__x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,\n\t\t\t\t\t0, 0);\n\t\t__x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,\n\t\t\t\t\t0, 0);\n\t\t__x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);\n\t\tmutex_unlock(&kvm->slots_lock);\n\t}\n\tkvm_unload_vcpu_mmus(kvm);\n\tstatic_call_cond(kvm_x86_vm_destroy)(kvm);\n\tkvm_free_msr_filter(srcu_dereference_check(kvm->arch.msr_filter, &kvm->srcu, 1));\n\tkvm_pic_destroy(kvm);\n\tkvm_ioapic_destroy(kvm);\n\tkvm_destroy_vcpus(kvm);\n\tkvfree(rcu_dereference_check(kvm->arch.apic_map, 1));\n\tkfree(srcu_dereference_check(kvm->arch.pmu_event_filter, &kvm->srcu, 1));\n\tkvm_mmu_uninit_vm(kvm);\n\tkvm_page_track_cleanup(kvm);\n\tkvm_xen_destroy_vm(kvm);\n\tkvm_hv_destroy_vm(kvm);\n}\n\nstatic void memslot_rmap_free(struct kvm_memory_slot *slot)\n{\n\tint i;\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tkvfree(slot->arch.rmap[i]);\n\t\tslot->arch.rmap[i] = NULL;\n\t}\n}\n\nvoid kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *slot)\n{\n\tint i;\n\n\tmemslot_rmap_free(slot);\n\n\tfor (i = 1; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tkvfree(slot->arch.lpage_info[i - 1]);\n\t\tslot->arch.lpage_info[i - 1] = NULL;\n\t}\n\n\tkvm_page_track_free_memslot(slot);\n}\n\nint memslot_rmap_alloc(struct kvm_memory_slot *slot, unsigned long npages)\n{\n\tconst int sz = sizeof(*slot->arch.rmap[0]);\n\tint i;\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tint level = i + 1;\n\t\tint lpages = __kvm_mmu_slot_lpages(slot, npages, level);\n\n\t\tif (slot->arch.rmap[i])\n\t\t\tcontinue;\n\n\t\tslot->arch.rmap[i] = __vcalloc(lpages, sz, GFP_KERNEL_ACCOUNT);\n\t\tif (!slot->arch.rmap[i]) {\n\t\t\tmemslot_rmap_free(slot);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int kvm_alloc_memslot_metadata(struct kvm *kvm,\n\t\t\t\t      struct kvm_memory_slot *slot)\n{\n\tunsigned long npages = slot->npages;\n\tint i, r;\n\n\t/*\n\t * Clear out the previous array pointers for the KVM_MR_MOVE case.  The\n\t * old arrays will be freed by __kvm_set_memory_region() if installing\n\t * the new memslot is successful.\n\t */\n\tmemset(&slot->arch, 0, sizeof(slot->arch));\n\n\tif (kvm_memslots_have_rmaps(kvm)) {\n\t\tr = memslot_rmap_alloc(slot, npages);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 1; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tstruct kvm_lpage_info *linfo;\n\t\tunsigned long ugfn;\n\t\tint lpages;\n\t\tint level = i + 1;\n\n\t\tlpages = __kvm_mmu_slot_lpages(slot, npages, level);\n\n\t\tlinfo = __vcalloc(lpages, sizeof(*linfo), GFP_KERNEL_ACCOUNT);\n\t\tif (!linfo)\n\t\t\tgoto out_free;\n\n\t\tslot->arch.lpage_info[i - 1] = linfo;\n\n\t\tif (slot->base_gfn & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tlinfo[0].disallow_lpage = 1;\n\t\tif ((slot->base_gfn + npages) & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tlinfo[lpages - 1].disallow_lpage = 1;\n\t\tugfn = slot->userspace_addr >> PAGE_SHIFT;\n\t\t/*\n\t\t * If the gfn and userspace address are not aligned wrt each\n\t\t * other, disable large page support for this slot.\n\t\t */\n\t\tif ((slot->base_gfn ^ ugfn) & (KVM_PAGES_PER_HPAGE(level) - 1)) {\n\t\t\tunsigned long j;\n\n\t\t\tfor (j = 0; j < lpages; ++j)\n\t\t\t\tlinfo[j].disallow_lpage = 1;\n\t\t}\n\t}\n\n\tif (kvm_page_track_create_memslot(kvm, slot, npages))\n\t\tgoto out_free;\n\n\treturn 0;\n\nout_free:\n\tmemslot_rmap_free(slot);\n\n\tfor (i = 1; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tkvfree(slot->arch.lpage_info[i - 1]);\n\t\tslot->arch.lpage_info[i - 1] = NULL;\n\t}\n\treturn -ENOMEM;\n}\n\nvoid kvm_arch_memslots_updated(struct kvm *kvm, u64 gen)\n{\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\n\t/*\n\t * memslots->generation has been incremented.\n\t * mmio generation may have reached its maximum value.\n\t */\n\tkvm_mmu_invalidate_mmio_sptes(kvm, gen);\n\n\t/* Force re-initialization of steal_time cache */\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\n\t\t\t\t   const struct kvm_memory_slot *old,\n\t\t\t\t   struct kvm_memory_slot *new,\n\t\t\t\t   enum kvm_mr_change change)\n{\n\tif (change == KVM_MR_CREATE || change == KVM_MR_MOVE) {\n\t\tif ((new->base_gfn + new->npages - 1) > kvm_mmu_max_gfn())\n\t\t\treturn -EINVAL;\n\n\t\treturn kvm_alloc_memslot_metadata(kvm, new);\n\t}\n\n\tif (change == KVM_MR_FLAGS_ONLY)\n\t\tmemcpy(&new->arch, &old->arch, sizeof(old->arch));\n\telse if (WARN_ON_ONCE(change != KVM_MR_DELETE))\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\n\nstatic void kvm_mmu_update_cpu_dirty_logging(struct kvm *kvm, bool enable)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\n\tif (!kvm_x86_ops.cpu_dirty_log_size)\n\t\treturn;\n\n\tif ((enable && ++ka->cpu_dirty_logging_count == 1) ||\n\t    (!enable && --ka->cpu_dirty_logging_count == 0))\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_UPDATE_CPU_DIRTY_LOGGING);\n\n\tWARN_ON_ONCE(ka->cpu_dirty_logging_count < 0);\n}\n\nstatic void kvm_mmu_slot_apply_flags(struct kvm *kvm,\n\t\t\t\t     struct kvm_memory_slot *old,\n\t\t\t\t     const struct kvm_memory_slot *new,\n\t\t\t\t     enum kvm_mr_change change)\n{\n\tu32 old_flags = old ? old->flags : 0;\n\tu32 new_flags = new ? new->flags : 0;\n\tbool log_dirty_pages = new_flags & KVM_MEM_LOG_DIRTY_PAGES;\n\n\t/*\n\t * Update CPU dirty logging if dirty logging is being toggled.  This\n\t * applies to all operations.\n\t */\n\tif ((old_flags ^ new_flags) & KVM_MEM_LOG_DIRTY_PAGES)\n\t\tkvm_mmu_update_cpu_dirty_logging(kvm, log_dirty_pages);\n\n\t/*\n\t * Nothing more to do for RO slots (which can't be dirtied and can't be\n\t * made writable) or CREATE/MOVE/DELETE of a slot.\n\t *\n\t * For a memslot with dirty logging disabled:\n\t * CREATE:      No dirty mappings will already exist.\n\t * MOVE/DELETE: The old mappings will already have been cleaned up by\n\t *\t\tkvm_arch_flush_shadow_memslot()\n\t *\n\t * For a memslot with dirty logging enabled:\n\t * CREATE:      No shadow pages exist, thus nothing to write-protect\n\t *\t\tand no dirty bits to clear.\n\t * MOVE/DELETE: The old mappings will already have been cleaned up by\n\t *\t\tkvm_arch_flush_shadow_memslot().\n\t */\n\tif ((change != KVM_MR_FLAGS_ONLY) || (new_flags & KVM_MEM_READONLY))\n\t\treturn;\n\n\t/*\n\t * READONLY and non-flags changes were filtered out above, and the only\n\t * other flag is LOG_DIRTY_PAGES, i.e. something is wrong if dirty\n\t * logging isn't being toggled on or off.\n\t */\n\tif (WARN_ON_ONCE(!((old_flags ^ new_flags) & KVM_MEM_LOG_DIRTY_PAGES)))\n\t\treturn;\n\n\tif (!log_dirty_pages) {\n\t\t/*\n\t\t * Dirty logging tracks sptes in 4k granularity, meaning that\n\t\t * large sptes have to be split.  If live migration succeeds,\n\t\t * the guest in the source machine will be destroyed and large\n\t\t * sptes will be created in the destination.  However, if the\n\t\t * guest continues to run in the source machine (for example if\n\t\t * live migration fails), small sptes will remain around and\n\t\t * cause bad performance.\n\t\t *\n\t\t * Scan sptes if dirty logging has been stopped, dropping those\n\t\t * which can be collapsed into a single large-page spte.  Later\n\t\t * page faults will create the large-page sptes.\n\t\t */\n\t\tkvm_mmu_zap_collapsible_sptes(kvm, new);\n\t} else {\n\t\t/*\n\t\t * Initially-all-set does not require write protecting any page,\n\t\t * because they're all assumed to be dirty.\n\t\t */\n\t\tif (kvm_dirty_log_manual_protect_and_init_set(kvm))\n\t\t\treturn;\n\n\t\tif (READ_ONCE(eager_page_split))\n\t\t\tkvm_mmu_slot_try_split_huge_pages(kvm, new, PG_LEVEL_4K);\n\n\t\tif (kvm_x86_ops.cpu_dirty_log_size) {\n\t\t\tkvm_mmu_slot_leaf_clear_dirty(kvm, new);\n\t\t\tkvm_mmu_slot_remove_write_access(kvm, new, PG_LEVEL_2M);\n\t\t} else {\n\t\t\tkvm_mmu_slot_remove_write_access(kvm, new, PG_LEVEL_4K);\n\t\t}\n\n\t\t/*\n\t\t * Unconditionally flush the TLBs after enabling dirty logging.\n\t\t * A flush is almost always going to be necessary (see below),\n\t\t * and unconditionally flushing allows the helpers to omit\n\t\t * the subtly complex checks when removing write access.\n\t\t *\n\t\t * Do the flush outside of mmu_lock to reduce the amount of\n\t\t * time mmu_lock is held.  Flushing after dropping mmu_lock is\n\t\t * safe as KVM only needs to guarantee the slot is fully\n\t\t * write-protected before returning to userspace, i.e. before\n\t\t * userspace can consume the dirty status.\n\t\t *\n\t\t * Flushing outside of mmu_lock requires KVM to be careful when\n\t\t * making decisions based on writable status of an SPTE, e.g. a\n\t\t * !writable SPTE doesn't guarantee a CPU can't perform writes.\n\t\t *\n\t\t * Specifically, KVM also write-protects guest page tables to\n\t\t * monitor changes when using shadow paging, and must guarantee\n\t\t * no CPUs can write to those page before mmu_lock is dropped.\n\t\t * Because CPUs may have stale TLB entries at this point, a\n\t\t * !writable SPTE doesn't guarantee CPUs can't perform writes.\n\t\t *\n\t\t * KVM also allows making SPTES writable outside of mmu_lock,\n\t\t * e.g. to allow dirty logging without taking mmu_lock.\n\t\t *\n\t\t * To handle these scenarios, KVM uses a separate software-only\n\t\t * bit (MMU-writable) to track if a SPTE is !writable due to\n\t\t * a guest page table being write-protected (KVM clears the\n\t\t * MMU-writable flag when write-protecting for shadow paging).\n\t\t *\n\t\t * The use of MMU-writable is also the primary motivation for\n\t\t * the unconditional flush.  Because KVM must guarantee that a\n\t\t * CPU doesn't contain stale, writable TLB entries for a\n\t\t * !MMU-writable SPTE, KVM must flush if it encounters any\n\t\t * MMU-writable SPTE regardless of whether the actual hardware\n\t\t * writable bit was set.  I.e. KVM is almost guaranteed to need\n\t\t * to flush, while unconditionally flushing allows the \"remove\n\t\t * write access\" helpers to ignore MMU-writable entirely.\n\t\t *\n\t\t * See is_writable_pte() for more details (the case involving\n\t\t * access-tracked SPTEs is particularly relevant).\n\t\t */\n\t\tkvm_arch_flush_remote_tlbs_memslot(kvm, new);\n\t}\n}\n\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\n\t\t\t\tstruct kvm_memory_slot *old,\n\t\t\t\tconst struct kvm_memory_slot *new,\n\t\t\t\tenum kvm_mr_change change)\n{\n\tif (!kvm->arch.n_requested_mmu_pages &&\n\t    (change == KVM_MR_CREATE || change == KVM_MR_DELETE)) {\n\t\tunsigned long nr_mmu_pages;\n\n\t\tnr_mmu_pages = kvm->nr_memslot_pages / KVM_MEMSLOT_PAGES_TO_MMU_PAGES_RATIO;\n\t\tnr_mmu_pages = max(nr_mmu_pages, KVM_MIN_ALLOC_MMU_PAGES);\n\t\tkvm_mmu_change_mmu_pages(kvm, nr_mmu_pages);\n\t}\n\n\tkvm_mmu_slot_apply_flags(kvm, old, new, change);\n\n\t/* Free the arrays associated with the old memslot. */\n\tif (change == KVM_MR_MOVE)\n\t\tkvm_arch_free_memslot(kvm, old);\n}\n\nvoid kvm_arch_flush_shadow_all(struct kvm *kvm)\n{\n\tkvm_mmu_zap_all(kvm);\n}\n\nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot)\n{\n\tkvm_page_track_flush_slot(kvm, slot);\n}\n\nstatic inline bool kvm_guest_apic_has_interrupt(struct kvm_vcpu *vcpu)\n{\n\treturn (is_guest_mode(vcpu) &&\n\t\tstatic_call(kvm_x86_guest_apic_has_interrupt)(vcpu));\n}\n\nstatic inline bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)\n{\n\tif (!list_empty_careful(&vcpu->async_pf.done))\n\t\treturn true;\n\n\tif (kvm_apic_has_pending_init_or_sipi(vcpu) &&\n\t    kvm_apic_init_sipi_allowed(vcpu))\n\t\treturn true;\n\n\tif (vcpu->arch.pv.pv_unhalted)\n\t\treturn true;\n\n\tif (kvm_is_exception_pending(vcpu))\n\t\treturn true;\n\n\tif (kvm_test_request(KVM_REQ_NMI, vcpu) ||\n\t    (vcpu->arch.nmi_pending &&\n\t     static_call(kvm_x86_nmi_allowed)(vcpu, false)))\n\t\treturn true;\n\n#ifdef CONFIG_KVM_SMM\n\tif (kvm_test_request(KVM_REQ_SMI, vcpu) ||\n\t    (vcpu->arch.smi_pending &&\n\t     static_call(kvm_x86_smi_allowed)(vcpu, false)))\n\t\treturn true;\n#endif\n\n\tif (kvm_arch_interrupt_allowed(vcpu) &&\n\t    (kvm_cpu_has_interrupt(vcpu) ||\n\t    kvm_guest_apic_has_interrupt(vcpu)))\n\t\treturn true;\n\n\tif (kvm_hv_has_stimer_pending(vcpu))\n\t\treturn true;\n\n\tif (is_guest_mode(vcpu) &&\n\t    kvm_x86_ops.nested_ops->has_events &&\n\t    kvm_x86_ops.nested_ops->has_events(vcpu))\n\t\treturn true;\n\n\tif (kvm_xen_has_pending_events(vcpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nint kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_vcpu_running(vcpu) || kvm_vcpu_has_events(vcpu);\n}\n\nbool kvm_arch_dy_has_pending_interrupt(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_vcpu_apicv_active(vcpu) &&\n\t    static_call(kvm_x86_dy_apicv_has_pending_interrupt)(vcpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nbool kvm_arch_dy_runnable(struct kvm_vcpu *vcpu)\n{\n\tif (READ_ONCE(vcpu->arch.pv.pv_unhalted))\n\t\treturn true;\n\n\tif (kvm_test_request(KVM_REQ_NMI, vcpu) ||\n#ifdef CONFIG_KVM_SMM\n\t\tkvm_test_request(KVM_REQ_SMI, vcpu) ||\n#endif\n\t\t kvm_test_request(KVM_REQ_EVENT, vcpu))\n\t\treturn true;\n\n\treturn kvm_arch_dy_has_pending_interrupt(vcpu);\n}\n\nbool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn true;\n\n\treturn vcpu->arch.preempted_in_kernel;\n}\n\nunsigned long kvm_arch_vcpu_get_ip(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_rip_read(vcpu);\n}\n\nint kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;\n}\n\nint kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu)\n{\n\treturn static_call(kvm_x86_interrupt_allowed)(vcpu, false);\n}\n\nunsigned long kvm_get_linear_rip(struct kvm_vcpu *vcpu)\n{\n\t/* Can't read the RIP when guest state is protected, just return 0 */\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn 0;\n\n\tif (is_64_bit_mode(vcpu))\n\t\treturn kvm_rip_read(vcpu);\n\treturn (u32)(get_segment_base(vcpu, VCPU_SREG_CS) +\n\t\t     kvm_rip_read(vcpu));\n}\nEXPORT_SYMBOL_GPL(kvm_get_linear_rip);\n\nbool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip)\n{\n\treturn kvm_get_linear_rip(vcpu) == linear_rip;\n}\nEXPORT_SYMBOL_GPL(kvm_is_linear_rip);\n\nunsigned long kvm_get_rflags(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rflags;\n\n\trflags = static_call(kvm_x86_get_rflags)(vcpu);\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\trflags &= ~X86_EFLAGS_TF;\n\treturn rflags;\n}\nEXPORT_SYMBOL_GPL(kvm_get_rflags);\n\nstatic void __kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\n{\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP &&\n\t    kvm_is_linear_rip(vcpu, vcpu->arch.singlestep_rip))\n\t\trflags |= X86_EFLAGS_TF;\n\tstatic_call(kvm_x86_set_rflags)(vcpu, rflags);\n}\n\nvoid kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\n{\n\t__kvm_set_rflags(vcpu, rflags);\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_set_rflags);\n\nstatic inline u32 kvm_async_pf_hash_fn(gfn_t gfn)\n{\n\tBUILD_BUG_ON(!is_power_of_2(ASYNC_PF_PER_VCPU));\n\n\treturn hash_32(gfn & 0xffffffff, order_base_2(ASYNC_PF_PER_VCPU));\n}\n\nstatic inline u32 kvm_async_pf_next_probe(u32 key)\n{\n\treturn (key + 1) & (ASYNC_PF_PER_VCPU - 1);\n}\n\nstatic void kvm_add_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\tu32 key = kvm_async_pf_hash_fn(gfn);\n\n\twhile (vcpu->arch.apf.gfns[key] != ~0)\n\t\tkey = kvm_async_pf_next_probe(key);\n\n\tvcpu->arch.apf.gfns[key] = gfn;\n}\n\nstatic u32 kvm_async_pf_gfn_slot(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\tint i;\n\tu32 key = kvm_async_pf_hash_fn(gfn);\n\n\tfor (i = 0; i < ASYNC_PF_PER_VCPU &&\n\t\t     (vcpu->arch.apf.gfns[key] != gfn &&\n\t\t      vcpu->arch.apf.gfns[key] != ~0); i++)\n\t\tkey = kvm_async_pf_next_probe(key);\n\n\treturn key;\n}\n\nbool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\treturn vcpu->arch.apf.gfns[kvm_async_pf_gfn_slot(vcpu, gfn)] == gfn;\n}\n\nstatic void kvm_del_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\tu32 i, j, k;\n\n\ti = j = kvm_async_pf_gfn_slot(vcpu, gfn);\n\n\tif (WARN_ON_ONCE(vcpu->arch.apf.gfns[i] != gfn))\n\t\treturn;\n\n\twhile (true) {\n\t\tvcpu->arch.apf.gfns[i] = ~0;\n\t\tdo {\n\t\t\tj = kvm_async_pf_next_probe(j);\n\t\t\tif (vcpu->arch.apf.gfns[j] == ~0)\n\t\t\t\treturn;\n\t\t\tk = kvm_async_pf_hash_fn(vcpu->arch.apf.gfns[j]);\n\t\t\t/*\n\t\t\t * k lies cyclically in ]i,j]\n\t\t\t * |    i.k.j |\n\t\t\t * |....j i.k.| or  |.k..j i...|\n\t\t\t */\n\t\t} while ((i <= j) ? (i < k && k <= j) : (i < k || k <= j));\n\t\tvcpu->arch.apf.gfns[i] = vcpu->arch.apf.gfns[j];\n\t\ti = j;\n\t}\n}\n\nstatic inline int apf_put_user_notpresent(struct kvm_vcpu *vcpu)\n{\n\tu32 reason = KVM_PV_REASON_PAGE_NOT_PRESENT;\n\n\treturn kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apf.data, &reason,\n\t\t\t\t      sizeof(reason));\n}\n\nstatic inline int apf_put_user_ready(struct kvm_vcpu *vcpu, u32 token)\n{\n\tunsigned int offset = offsetof(struct kvm_vcpu_pv_apf_data, token);\n\n\treturn kvm_write_guest_offset_cached(vcpu->kvm, &vcpu->arch.apf.data,\n\t\t\t\t\t     &token, offset, sizeof(token));\n}\n\nstatic inline bool apf_pageready_slot_free(struct kvm_vcpu *vcpu)\n{\n\tunsigned int offset = offsetof(struct kvm_vcpu_pv_apf_data, token);\n\tu32 val;\n\n\tif (kvm_read_guest_offset_cached(vcpu->kvm, &vcpu->arch.apf.data,\n\t\t\t\t\t &val, offset, sizeof(val)))\n\t\treturn false;\n\n\treturn !val;\n}\n\nstatic bool kvm_can_deliver_async_pf(struct kvm_vcpu *vcpu)\n{\n\n\tif (!kvm_pv_async_pf_enabled(vcpu))\n\t\treturn false;\n\n\tif (vcpu->arch.apf.send_user_only &&\n\t    static_call(kvm_x86_get_cpl)(vcpu) == 0)\n\t\treturn false;\n\n\tif (is_guest_mode(vcpu)) {\n\t\t/*\n\t\t * L1 needs to opt into the special #PF vmexits that are\n\t\t * used to deliver async page faults.\n\t\t */\n\t\treturn vcpu->arch.apf.delivery_as_pf_vmexit;\n\t} else {\n\t\t/*\n\t\t * Play it safe in case the guest temporarily disables paging.\n\t\t * The real mode IDT in particular is unlikely to have a #PF\n\t\t * exception setup.\n\t\t */\n\t\treturn is_paging(vcpu);\n\t}\n}\n\nbool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)\n{\n\tif (unlikely(!lapic_in_kernel(vcpu) ||\n\t\t     kvm_event_needs_reinjection(vcpu) ||\n\t\t     kvm_is_exception_pending(vcpu)))\n\t\treturn false;\n\n\tif (kvm_hlt_in_guest(vcpu->kvm) && !kvm_can_deliver_async_pf(vcpu))\n\t\treturn false;\n\n\t/*\n\t * If interrupts are off we cannot even use an artificial\n\t * halt state.\n\t */\n\treturn kvm_arch_interrupt_allowed(vcpu);\n}\n\nbool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_async_pf *work)\n{\n\tstruct x86_exception fault;\n\n\ttrace_kvm_async_pf_not_present(work->arch.token, work->cr2_or_gpa);\n\tkvm_add_async_pf_gfn(vcpu, work->arch.gfn);\n\n\tif (kvm_can_deliver_async_pf(vcpu) &&\n\t    !apf_put_user_notpresent(vcpu)) {\n\t\tfault.vector = PF_VECTOR;\n\t\tfault.error_code_valid = true;\n\t\tfault.error_code = 0;\n\t\tfault.nested_page_fault = false;\n\t\tfault.address = work->arch.token;\n\t\tfault.async_page_fault = true;\n\t\tkvm_inject_page_fault(vcpu, &fault);\n\t\treturn true;\n\t} else {\n\t\t/*\n\t\t * It is not possible to deliver a paravirtualized asynchronous\n\t\t * page fault, but putting the guest in an artificial halt state\n\t\t * can be beneficial nevertheless: if an interrupt arrives, we\n\t\t * can deliver it timely and perhaps the guest will schedule\n\t\t * another process.  When the instruction that triggered a page\n\t\t * fault is retried, hopefully the page will be ready in the host.\n\t\t */\n\t\tkvm_make_request(KVM_REQ_APF_HALT, vcpu);\n\t\treturn false;\n\t}\n}\n\nvoid kvm_arch_async_page_present(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_async_pf *work)\n{\n\tstruct kvm_lapic_irq irq = {\n\t\t.delivery_mode = APIC_DM_FIXED,\n\t\t.vector = vcpu->arch.apf.vec\n\t};\n\n\tif (work->wakeup_all)\n\t\twork->arch.token = ~0; /* broadcast wakeup */\n\telse\n\t\tkvm_del_async_pf_gfn(vcpu, work->arch.gfn);\n\ttrace_kvm_async_pf_ready(work->arch.token, work->cr2_or_gpa);\n\n\tif ((work->wakeup_all || work->notpresent_injected) &&\n\t    kvm_pv_async_pf_enabled(vcpu) &&\n\t    !apf_put_user_ready(vcpu, work->arch.token)) {\n\t\tvcpu->arch.apf.pageready_pending = true;\n\t\tkvm_apic_set_irq(vcpu, &irq, NULL);\n\t}\n\n\tvcpu->arch.apf.halted = false;\n\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n}\n\nvoid kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu)\n{\n\tkvm_make_request(KVM_REQ_APF_READY, vcpu);\n\tif (!vcpu->arch.apf.pageready_pending)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nbool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_pv_async_pf_enabled(vcpu))\n\t\treturn true;\n\telse\n\t\treturn kvm_lapic_enabled(vcpu) && apf_pageready_slot_free(vcpu);\n}\n\nvoid kvm_arch_start_assignment(struct kvm *kvm)\n{\n\tif (atomic_inc_return(&kvm->arch.assigned_device_count) == 1)\n\t\tstatic_call_cond(kvm_x86_pi_start_assignment)(kvm);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_start_assignment);\n\nvoid kvm_arch_end_assignment(struct kvm *kvm)\n{\n\tatomic_dec(&kvm->arch.assigned_device_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_end_assignment);\n\nbool noinstr kvm_arch_has_assigned_device(struct kvm *kvm)\n{\n\treturn arch_atomic_read(&kvm->arch.assigned_device_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_has_assigned_device);\n\nvoid kvm_arch_register_noncoherent_dma(struct kvm *kvm)\n{\n\tatomic_inc(&kvm->arch.noncoherent_dma_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_register_noncoherent_dma);\n\nvoid kvm_arch_unregister_noncoherent_dma(struct kvm *kvm)\n{\n\tatomic_dec(&kvm->arch.noncoherent_dma_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_unregister_noncoherent_dma);\n\nbool kvm_arch_has_noncoherent_dma(struct kvm *kvm)\n{\n\treturn atomic_read(&kvm->arch.noncoherent_dma_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_has_noncoherent_dma);\n\nbool kvm_arch_has_irq_bypass(void)\n{\n\treturn true;\n}\n\nint kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,\n\t\t\t\t      struct irq_bypass_producer *prod)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(cons, struct kvm_kernel_irqfd, consumer);\n\tint ret;\n\n\tirqfd->producer = prod;\n\tkvm_arch_start_assignment(irqfd->kvm);\n\tret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm,\n\t\t\t\t\t prod->irq, irqfd->gsi, 1);\n\n\tif (ret)\n\t\tkvm_arch_end_assignment(irqfd->kvm);\n\n\treturn ret;\n}\n\nvoid kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,\n\t\t\t\t      struct irq_bypass_producer *prod)\n{\n\tint ret;\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(cons, struct kvm_kernel_irqfd, consumer);\n\n\tWARN_ON(irqfd->producer != prod);\n\tirqfd->producer = NULL;\n\n\t/*\n\t * When producer of consumer is unregistered, we change back to\n\t * remapped mode, so we can re-use the current implementation\n\t * when the irq is masked/disabled or the consumer side (KVM\n\t * int this case doesn't want to receive the interrupts.\n\t*/\n\tret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);\n\tif (ret)\n\t\tprintk(KERN_INFO \"irq bypass consumer (token %p) unregistration\"\n\t\t       \" fails: %d\\n\", irqfd->consumer.token, ret);\n\n\tkvm_arch_end_assignment(irqfd->kvm);\n}\n\nint kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,\n\t\t\t\t   uint32_t guest_irq, bool set)\n{\n\treturn static_call(kvm_x86_pi_update_irte)(kvm, host_irq, guest_irq, set);\n}\n\nbool kvm_arch_irqfd_route_changed(struct kvm_kernel_irq_routing_entry *old,\n\t\t\t\t  struct kvm_kernel_irq_routing_entry *new)\n{\n\tif (new->type != KVM_IRQ_ROUTING_MSI)\n\t\treturn true;\n\n\treturn !!memcmp(&old->msi, &new->msi, sizeof(new->msi));\n}\n\nbool kvm_vector_hashing_enabled(void)\n{\n\treturn vector_hashing;\n}\n\nbool kvm_arch_no_poll(struct kvm_vcpu *vcpu)\n{\n\treturn (vcpu->arch.msr_kvm_poll_control & 1) == 0;\n}\nEXPORT_SYMBOL_GPL(kvm_arch_no_poll);\n\n\nint kvm_spec_ctrl_test_value(u64 value)\n{\n\t/*\n\t * test that setting IA32_SPEC_CTRL to given value\n\t * is allowed by the host processor\n\t */\n\n\tu64 saved_value;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tlocal_irq_save(flags);\n\n\tif (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))\n\t\tret = 1;\n\telse if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))\n\t\tret = 1;\n\telse\n\t\twrmsrl(MSR_IA32_SPEC_CTRL, saved_value);\n\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kvm_spec_ctrl_test_value);\n\nvoid kvm_fixup_and_inject_pf_error(struct kvm_vcpu *vcpu, gva_t gva, u16 error_code)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tstruct x86_exception fault;\n\tu64 access = error_code &\n\t\t(PFERR_WRITE_MASK | PFERR_FETCH_MASK | PFERR_USER_MASK);\n\n\tif (!(error_code & PFERR_PRESENT_MASK) ||\n\t    mmu->gva_to_gpa(vcpu, mmu, gva, access, &fault) != INVALID_GPA) {\n\t\t/*\n\t\t * If vcpu->arch.walk_mmu->gva_to_gpa succeeded, the page\n\t\t * tables probably do not match the TLB.  Just proceed\n\t\t * with the error code that the processor gave.\n\t\t */\n\t\tfault.vector = PF_VECTOR;\n\t\tfault.error_code_valid = true;\n\t\tfault.error_code = error_code;\n\t\tfault.nested_page_fault = false;\n\t\tfault.address = gva;\n\t\tfault.async_page_fault = false;\n\t}\n\tvcpu->arch.walk_mmu->inject_page_fault(vcpu, &fault);\n}\nEXPORT_SYMBOL_GPL(kvm_fixup_and_inject_pf_error);\n\n/*\n * Handles kvm_read/write_guest_virt*() result and either injects #PF or returns\n * KVM_EXIT_INTERNAL_ERROR for cases not currently handled by KVM. Return value\n * indicates whether exit to userspace is needed.\n */\nint kvm_handle_memory_failure(struct kvm_vcpu *vcpu, int r,\n\t\t\t      struct x86_exception *e)\n{\n\tif (r == X86EMUL_PROPAGATE_FAULT) {\n\t\tif (KVM_BUG_ON(!e, vcpu->kvm))\n\t\t\treturn -EIO;\n\n\t\tkvm_inject_emulated_page_fault(vcpu, e);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * In case kvm_read/write_guest_virt*() failed with X86EMUL_IO_NEEDED\n\t * while handling a VMX instruction KVM could've handled the request\n\t * correctly by exiting to userspace and performing I/O but there\n\t * doesn't seem to be a real use-case behind such requests, just return\n\t * KVM_EXIT_INTERNAL_ERROR for now.\n\t */\n\tkvm_prepare_emulation_failure_exit(vcpu);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_handle_memory_failure);\n\nint kvm_handle_invpcid(struct kvm_vcpu *vcpu, unsigned long type, gva_t gva)\n{\n\tbool pcid_enabled;\n\tstruct x86_exception e;\n\tstruct {\n\t\tu64 pcid;\n\t\tu64 gla;\n\t} operand;\n\tint r;\n\n\tr = kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e);\n\tif (r != X86EMUL_CONTINUE)\n\t\treturn kvm_handle_memory_failure(vcpu, r, &e);\n\n\tif (operand.pcid >> 12 != 0) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tpcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);\n\n\tswitch (type) {\n\tcase INVPCID_TYPE_INDIV_ADDR:\n\t\tif ((!pcid_enabled && (operand.pcid != 0)) ||\n\t\t    is_noncanonical_address(operand.gla, vcpu)) {\n\t\t\tkvm_inject_gp(vcpu, 0);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_mmu_invpcid_gva(vcpu, operand.gla, operand.pcid);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tcase INVPCID_TYPE_SINGLE_CTXT:\n\t\tif (!pcid_enabled && (operand.pcid != 0)) {\n\t\t\tkvm_inject_gp(vcpu, 0);\n\t\t\treturn 1;\n\t\t}\n\n\t\tkvm_invalidate_pcid(vcpu, operand.pcid);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tcase INVPCID_TYPE_ALL_NON_GLOBAL:\n\t\t/*\n\t\t * Currently, KVM doesn't mark global entries in the shadow\n\t\t * page tables, so a non-global flush just degenerates to a\n\t\t * global flush. If needed, we could optimize this later by\n\t\t * keeping track of global entries in shadow page tables.\n\t\t */\n\n\t\tfallthrough;\n\tcase INVPCID_TYPE_ALL_INCL_GLOBAL:\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tdefault:\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_handle_invpcid);\n\nstatic int complete_sev_es_emulated_mmio(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *run = vcpu->run;\n\tstruct kvm_mmio_fragment *frag;\n\tunsigned int len;\n\n\tBUG_ON(!vcpu->mmio_needed);\n\n\t/* Complete previous fragment */\n\tfrag = &vcpu->mmio_fragments[vcpu->mmio_cur_fragment];\n\tlen = min(8u, frag->len);\n\tif (!vcpu->mmio_is_write)\n\t\tmemcpy(frag->data, run->mmio.data, len);\n\n\tif (frag->len <= 8) {\n\t\t/* Switch to the next fragment. */\n\t\tfrag++;\n\t\tvcpu->mmio_cur_fragment++;\n\t} else {\n\t\t/* Go forward to the next mmio piece. */\n\t\tfrag->data += len;\n\t\tfrag->gpa += len;\n\t\tfrag->len -= len;\n\t}\n\n\tif (vcpu->mmio_cur_fragment >= vcpu->mmio_nr_fragments) {\n\t\tvcpu->mmio_needed = 0;\n\n\t\t// VMG change, at this point, we're always done\n\t\t// RIP has already been advanced\n\t\treturn 1;\n\t}\n\n\t// More MMIO is needed\n\trun->mmio.phys_addr = frag->gpa;\n\trun->mmio.len = min(8u, frag->len);\n\trun->mmio.is_write = vcpu->mmio_is_write;\n\tif (run->mmio.is_write)\n\t\tmemcpy(run->mmio.data, frag->data, min(8u, frag->len));\n\trun->exit_reason = KVM_EXIT_MMIO;\n\n\tvcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;\n\n\treturn 0;\n}\n\nint kvm_sev_es_mmio_write(struct kvm_vcpu *vcpu, gpa_t gpa, unsigned int bytes,\n\t\t\t  void *data)\n{\n\tint handled;\n\tstruct kvm_mmio_fragment *frag;\n\n\tif (!data)\n\t\treturn -EINVAL;\n\n\thandled = write_emultor.read_write_mmio(vcpu, gpa, bytes, data);\n\tif (handled == bytes)\n\t\treturn 1;\n\n\tbytes -= handled;\n\tgpa += handled;\n\tdata += handled;\n\n\t/*TODO: Check if need to increment number of frags */\n\tfrag = vcpu->mmio_fragments;\n\tvcpu->mmio_nr_fragments = 1;\n\tfrag->len = bytes;\n\tfrag->gpa = gpa;\n\tfrag->data = data;\n\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_cur_fragment = 0;\n\n\tvcpu->run->mmio.phys_addr = gpa;\n\tvcpu->run->mmio.len = min(8u, frag->len);\n\tvcpu->run->mmio.is_write = 1;\n\tmemcpy(vcpu->run->mmio.data, frag->data, min(8u, frag->len));\n\tvcpu->run->exit_reason = KVM_EXIT_MMIO;\n\n\tvcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_sev_es_mmio_write);\n\nint kvm_sev_es_mmio_read(struct kvm_vcpu *vcpu, gpa_t gpa, unsigned int bytes,\n\t\t\t void *data)\n{\n\tint handled;\n\tstruct kvm_mmio_fragment *frag;\n\n\tif (!data)\n\t\treturn -EINVAL;\n\n\thandled = read_emultor.read_write_mmio(vcpu, gpa, bytes, data);\n\tif (handled == bytes)\n\t\treturn 1;\n\n\tbytes -= handled;\n\tgpa += handled;\n\tdata += handled;\n\n\t/*TODO: Check if need to increment number of frags */\n\tfrag = vcpu->mmio_fragments;\n\tvcpu->mmio_nr_fragments = 1;\n\tfrag->len = bytes;\n\tfrag->gpa = gpa;\n\tfrag->data = data;\n\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_cur_fragment = 0;\n\n\tvcpu->run->mmio.phys_addr = gpa;\n\tvcpu->run->mmio.len = min(8u, frag->len);\n\tvcpu->run->mmio.is_write = 0;\n\tvcpu->run->exit_reason = KVM_EXIT_MMIO;\n\n\tvcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_sev_es_mmio_read);\n\nstatic void advance_sev_es_emulated_pio(struct kvm_vcpu *vcpu, unsigned count, int size)\n{\n\tvcpu->arch.sev_pio_count -= count;\n\tvcpu->arch.sev_pio_data += count * size;\n}\n\nstatic int kvm_sev_es_outs(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t   unsigned int port);\n\nstatic int complete_sev_es_emulated_outs(struct kvm_vcpu *vcpu)\n{\n\tint size = vcpu->arch.pio.size;\n\tint port = vcpu->arch.pio.port;\n\n\tvcpu->arch.pio.count = 0;\n\tif (vcpu->arch.sev_pio_count)\n\t\treturn kvm_sev_es_outs(vcpu, size, port);\n\treturn 1;\n}\n\nstatic int kvm_sev_es_outs(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t   unsigned int port)\n{\n\tfor (;;) {\n\t\tunsigned int count =\n\t\t\tmin_t(unsigned int, PAGE_SIZE / size, vcpu->arch.sev_pio_count);\n\t\tint ret = emulator_pio_out(vcpu, size, port, vcpu->arch.sev_pio_data, count);\n\n\t\t/* memcpy done already by emulator_pio_out.  */\n\t\tadvance_sev_es_emulated_pio(vcpu, count, size);\n\t\tif (!ret)\n\t\t\tbreak;\n\n\t\t/* Emulation done by the kernel.  */\n\t\tif (!vcpu->arch.sev_pio_count)\n\t\t\treturn 1;\n\t}\n\n\tvcpu->arch.complete_userspace_io = complete_sev_es_emulated_outs;\n\treturn 0;\n}\n\nstatic int kvm_sev_es_ins(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t  unsigned int port);\n\nstatic int complete_sev_es_emulated_ins(struct kvm_vcpu *vcpu)\n{\n\tunsigned count = vcpu->arch.pio.count;\n\tint size = vcpu->arch.pio.size;\n\tint port = vcpu->arch.pio.port;\n\n\tcomplete_emulator_pio_in(vcpu, vcpu->arch.sev_pio_data);\n\tadvance_sev_es_emulated_pio(vcpu, count, size);\n\tif (vcpu->arch.sev_pio_count)\n\t\treturn kvm_sev_es_ins(vcpu, size, port);\n\treturn 1;\n}\n\nstatic int kvm_sev_es_ins(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t  unsigned int port)\n{\n\tfor (;;) {\n\t\tunsigned int count =\n\t\t\tmin_t(unsigned int, PAGE_SIZE / size, vcpu->arch.sev_pio_count);\n\t\tif (!emulator_pio_in(vcpu, size, port, vcpu->arch.sev_pio_data, count))\n\t\t\tbreak;\n\n\t\t/* Emulation done by the kernel.  */\n\t\tadvance_sev_es_emulated_pio(vcpu, count, size);\n\t\tif (!vcpu->arch.sev_pio_count)\n\t\t\treturn 1;\n\t}\n\n\tvcpu->arch.complete_userspace_io = complete_sev_es_emulated_ins;\n\treturn 0;\n}\n\nint kvm_sev_es_string_io(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t unsigned int port, void *data,  unsigned int count,\n\t\t\t int in)\n{\n\tvcpu->arch.sev_pio_data = data;\n\tvcpu->arch.sev_pio_count = count;\n\treturn in ? kvm_sev_es_ins(vcpu, size, port)\n\t\t  : kvm_sev_es_outs(vcpu, size, port);\n}\nEXPORT_SYMBOL_GPL(kvm_sev_es_string_io);\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_entry);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_exit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_fast_mmio);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_inj_virq);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_page_fault);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_msr);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_cr);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmenter);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmexit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmexit_inject);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_intr_vmexit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmenter_failed);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_invlpga);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_skinit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_intercepts);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_write_tsc_offset);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_ple_window_update);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_pml_full);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_pi_irte_update);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_unaccelerated_access);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_incomplete_ipi);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_ga_log);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_kick_vcpu_slowpath);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_doorbell);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_apicv_accept_irq);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_enter);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_exit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_msr_protocol_enter);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_msr_protocol_exit);\n\nstatic int __init kvm_x86_init(void)\n{\n\tkvm_mmu_x86_module_init();\n\tmitigate_smt_rsb &= boot_cpu_has_bug(X86_BUG_SMT_RSB) && cpu_smt_possible();\n\treturn 0;\n}\nmodule_init(kvm_x86_init);\n\nstatic void __exit kvm_x86_exit(void)\n{\n\t/*\n\t * If module_init() is implemented, module_exit() must also be\n\t * implemented to allow module unload.\n\t */\n}\nmodule_exit(kvm_x86_exit);\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * derived from drivers/kvm/kvm_main.c\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright (C) 2008 Qumranet, Inc.\n * Copyright IBM Corporation, 2008\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Avi Kivity   <avi@qumranet.com>\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *   Amit Shah    <amit.shah@qumranet.com>\n *   Ben-Ami Yassour <benami@il.ibm.com>\n */\n\n#include <linux/kvm_host.h>\n#include \"irq.h\"\n#include \"ioapic.h\"\n#include \"mmu.h\"\n#include \"i8254.h\"\n#include \"tss.h\"\n#include \"kvm_cache_regs.h\"\n#include \"kvm_emulate.h\"\n#include \"x86.h\"\n#include \"cpuid.h\"\n#include \"pmu.h\"\n#include \"hyperv.h\"\n#include \"lapic.h\"\n#include \"xen.h\"\n#include \"smm.h\"\n\n#include <linux/clocksource.h>\n#include <linux/interrupt.h>\n#include <linux/kvm.h>\n#include <linux/fs.h>\n#include <linux/vmalloc.h>\n#include <linux/export.h>\n#include <linux/moduleparam.h>\n#include <linux/mman.h>\n#include <linux/highmem.h>\n#include <linux/iommu.h>\n#include <linux/cpufreq.h>\n#include <linux/user-return-notifier.h>\n#include <linux/srcu.h>\n#include <linux/slab.h>\n#include <linux/perf_event.h>\n#include <linux/uaccess.h>\n#include <linux/hash.h>\n#include <linux/pci.h>\n#include <linux/timekeeper_internal.h>\n#include <linux/pvclock_gtod.h>\n#include <linux/kvm_irqfd.h>\n#include <linux/irqbypass.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/isolation.h>\n#include <linux/mem_encrypt.h>\n#include <linux/entry-kvm.h>\n#include <linux/suspend.h>\n\n#include <trace/events/kvm.h>\n\n#include <asm/debugreg.h>\n#include <asm/msr.h>\n#include <asm/desc.h>\n#include <asm/mce.h>\n#include <asm/pkru.h>\n#include <linux/kernel_stat.h>\n#include <asm/fpu/api.h>\n#include <asm/fpu/xcr.h>\n#include <asm/fpu/xstate.h>\n#include <asm/pvclock.h>\n#include <asm/div64.h>\n#include <asm/irq_remapping.h>\n#include <asm/mshyperv.h>\n#include <asm/hypervisor.h>\n#include <asm/tlbflush.h>\n#include <asm/intel_pt.h>\n#include <asm/emulate_prefix.h>\n#include <asm/sgx.h>\n#include <clocksource/hyperv_timer.h>\n\n#define CREATE_TRACE_POINTS\n#include \"trace.h\"\n\n#define MAX_IO_MSRS 256\n#define KVM_MAX_MCE_BANKS 32\n\nstruct kvm_caps kvm_caps __read_mostly = {\n\t.supported_mce_cap = MCG_CTL_P | MCG_SER_P,\n};\nEXPORT_SYMBOL_GPL(kvm_caps);\n\n#define  ERR_PTR_USR(e)  ((void __user *)ERR_PTR(e))\n\n#define emul_to_vcpu(ctxt) \\\n\t((struct kvm_vcpu *)(ctxt)->vcpu)\n\n/* EFER defaults:\n * - enable syscall per default because its emulated by KVM\n * - enable LME and LMA per default on 64 bit KVM\n */\n#ifdef CONFIG_X86_64\nstatic\nu64 __read_mostly efer_reserved_bits = ~((u64)(EFER_SCE | EFER_LME | EFER_LMA));\n#else\nstatic u64 __read_mostly efer_reserved_bits = ~((u64)EFER_SCE);\n#endif\n\nstatic u64 __read_mostly cr4_reserved_bits = CR4_RESERVED_BITS;\n\n#define KVM_EXIT_HYPERCALL_VALID_MASK (1 << KVM_HC_MAP_GPA_RANGE)\n\n#define KVM_CAP_PMU_VALID_MASK KVM_PMU_CAP_DISABLE\n\n#define KVM_X2APIC_API_VALID_FLAGS (KVM_X2APIC_API_USE_32BIT_IDS | \\\n                                    KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)\n\nstatic void update_cr8_intercept(struct kvm_vcpu *vcpu);\nstatic void process_nmi(struct kvm_vcpu *vcpu);\nstatic void __kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);\nstatic void store_regs(struct kvm_vcpu *vcpu);\nstatic int sync_regs(struct kvm_vcpu *vcpu);\nstatic int kvm_vcpu_do_singlestep(struct kvm_vcpu *vcpu);\n\nstatic int __set_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2);\nstatic void __get_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2);\n\nstruct kvm_x86_ops kvm_x86_ops __read_mostly;\n\n#define KVM_X86_OP(func)\t\t\t\t\t     \\\n\tDEFINE_STATIC_CALL_NULL(kvm_x86_##func,\t\t\t     \\\n\t\t\t\t*(((struct kvm_x86_ops *)0)->func));\n#define KVM_X86_OP_OPTIONAL KVM_X86_OP\n#define KVM_X86_OP_OPTIONAL_RET0 KVM_X86_OP\n#include <asm/kvm-x86-ops.h>\nEXPORT_STATIC_CALL_GPL(kvm_x86_get_cs_db_l_bits);\nEXPORT_STATIC_CALL_GPL(kvm_x86_cache_reg);\n\nstatic bool __read_mostly ignore_msrs = 0;\nmodule_param(ignore_msrs, bool, S_IRUGO | S_IWUSR);\n\nbool __read_mostly report_ignored_msrs = true;\nmodule_param(report_ignored_msrs, bool, S_IRUGO | S_IWUSR);\nEXPORT_SYMBOL_GPL(report_ignored_msrs);\n\nunsigned int min_timer_period_us = 200;\nmodule_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);\n\nstatic bool __read_mostly kvmclock_periodic_sync = true;\nmodule_param(kvmclock_periodic_sync, bool, S_IRUGO);\n\n/* tsc tolerance in parts per million - default to 1/2 of the NTP threshold */\nstatic u32 __read_mostly tsc_tolerance_ppm = 250;\nmodule_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);\n\n/*\n * lapic timer advance (tscdeadline mode only) in nanoseconds.  '-1' enables\n * adaptive tuning starting from default advancement of 1000ns.  '0' disables\n * advancement entirely.  Any other value is used as-is and disables adaptive\n * tuning, i.e. allows privileged userspace to set an exact advancement time.\n */\nstatic int __read_mostly lapic_timer_advance_ns = -1;\nmodule_param(lapic_timer_advance_ns, int, S_IRUGO | S_IWUSR);\n\nstatic bool __read_mostly vector_hashing = true;\nmodule_param(vector_hashing, bool, S_IRUGO);\n\nbool __read_mostly enable_vmware_backdoor = false;\nmodule_param(enable_vmware_backdoor, bool, S_IRUGO);\nEXPORT_SYMBOL_GPL(enable_vmware_backdoor);\n\n/*\n * Flags to manipulate forced emulation behavior (any non-zero value will\n * enable forced emulation).\n */\n#define KVM_FEP_CLEAR_RFLAGS_RF\tBIT(1)\nstatic int __read_mostly force_emulation_prefix;\nmodule_param(force_emulation_prefix, int, 0644);\n\nint __read_mostly pi_inject_timer = -1;\nmodule_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);\n\n/* Enable/disable PMU virtualization */\nbool __read_mostly enable_pmu = true;\nEXPORT_SYMBOL_GPL(enable_pmu);\nmodule_param(enable_pmu, bool, 0444);\n\nbool __read_mostly eager_page_split = true;\nmodule_param(eager_page_split, bool, 0644);\n\n/* Enable/disable SMT_RSB bug mitigation */\nbool __read_mostly mitigate_smt_rsb;\nmodule_param(mitigate_smt_rsb, bool, 0444);\n\n/*\n * Restoring the host value for MSRs that are only consumed when running in\n * usermode, e.g. SYSCALL MSRs and TSC_AUX, can be deferred until the CPU\n * returns to userspace, i.e. the kernel can run with the guest's value.\n */\n#define KVM_MAX_NR_USER_RETURN_MSRS 16\n\nstruct kvm_user_return_msrs {\n\tstruct user_return_notifier urn;\n\tbool registered;\n\tstruct kvm_user_return_msr_values {\n\t\tu64 host;\n\t\tu64 curr;\n\t} values[KVM_MAX_NR_USER_RETURN_MSRS];\n};\n\nu32 __read_mostly kvm_nr_uret_msrs;\nEXPORT_SYMBOL_GPL(kvm_nr_uret_msrs);\nstatic u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];\nstatic struct kvm_user_return_msrs __percpu *user_return_msrs;\n\n#define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \\\n\t\t\t\t| XFEATURE_MASK_YMM | XFEATURE_MASK_BNDREGS \\\n\t\t\t\t| XFEATURE_MASK_BNDCSR | XFEATURE_MASK_AVX512 \\\n\t\t\t\t| XFEATURE_MASK_PKRU | XFEATURE_MASK_XTILE)\n\nu64 __read_mostly host_efer;\nEXPORT_SYMBOL_GPL(host_efer);\n\nbool __read_mostly allow_smaller_maxphyaddr = 0;\nEXPORT_SYMBOL_GPL(allow_smaller_maxphyaddr);\n\nbool __read_mostly enable_apicv = true;\nEXPORT_SYMBOL_GPL(enable_apicv);\n\nu64 __read_mostly host_xss;\nEXPORT_SYMBOL_GPL(host_xss);\n\nconst struct _kvm_stats_desc kvm_vm_stats_desc[] = {\n\tKVM_GENERIC_VM_STATS(),\n\tSTATS_DESC_COUNTER(VM, mmu_shadow_zapped),\n\tSTATS_DESC_COUNTER(VM, mmu_pte_write),\n\tSTATS_DESC_COUNTER(VM, mmu_pde_zapped),\n\tSTATS_DESC_COUNTER(VM, mmu_flooded),\n\tSTATS_DESC_COUNTER(VM, mmu_recycled),\n\tSTATS_DESC_COUNTER(VM, mmu_cache_miss),\n\tSTATS_DESC_ICOUNTER(VM, mmu_unsync),\n\tSTATS_DESC_ICOUNTER(VM, pages_4k),\n\tSTATS_DESC_ICOUNTER(VM, pages_2m),\n\tSTATS_DESC_ICOUNTER(VM, pages_1g),\n\tSTATS_DESC_ICOUNTER(VM, nx_lpage_splits),\n\tSTATS_DESC_PCOUNTER(VM, max_mmu_rmap_size),\n\tSTATS_DESC_PCOUNTER(VM, max_mmu_page_hash_collisions)\n};\n\nconst struct kvm_stats_header kvm_vm_stats_header = {\n\t.name_size = KVM_STATS_NAME_SIZE,\n\t.num_desc = ARRAY_SIZE(kvm_vm_stats_desc),\n\t.id_offset = sizeof(struct kvm_stats_header),\n\t.desc_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE,\n\t.data_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE +\n\t\t       sizeof(kvm_vm_stats_desc),\n};\n\nconst struct _kvm_stats_desc kvm_vcpu_stats_desc[] = {\n\tKVM_GENERIC_VCPU_STATS(),\n\tSTATS_DESC_COUNTER(VCPU, pf_taken),\n\tSTATS_DESC_COUNTER(VCPU, pf_fixed),\n\tSTATS_DESC_COUNTER(VCPU, pf_emulate),\n\tSTATS_DESC_COUNTER(VCPU, pf_spurious),\n\tSTATS_DESC_COUNTER(VCPU, pf_fast),\n\tSTATS_DESC_COUNTER(VCPU, pf_mmio_spte_created),\n\tSTATS_DESC_COUNTER(VCPU, pf_guest),\n\tSTATS_DESC_COUNTER(VCPU, tlb_flush),\n\tSTATS_DESC_COUNTER(VCPU, invlpg),\n\tSTATS_DESC_COUNTER(VCPU, exits),\n\tSTATS_DESC_COUNTER(VCPU, io_exits),\n\tSTATS_DESC_COUNTER(VCPU, mmio_exits),\n\tSTATS_DESC_COUNTER(VCPU, signal_exits),\n\tSTATS_DESC_COUNTER(VCPU, irq_window_exits),\n\tSTATS_DESC_COUNTER(VCPU, nmi_window_exits),\n\tSTATS_DESC_COUNTER(VCPU, l1d_flush),\n\tSTATS_DESC_COUNTER(VCPU, halt_exits),\n\tSTATS_DESC_COUNTER(VCPU, request_irq_exits),\n\tSTATS_DESC_COUNTER(VCPU, irq_exits),\n\tSTATS_DESC_COUNTER(VCPU, host_state_reload),\n\tSTATS_DESC_COUNTER(VCPU, fpu_reload),\n\tSTATS_DESC_COUNTER(VCPU, insn_emulation),\n\tSTATS_DESC_COUNTER(VCPU, insn_emulation_fail),\n\tSTATS_DESC_COUNTER(VCPU, hypercalls),\n\tSTATS_DESC_COUNTER(VCPU, irq_injections),\n\tSTATS_DESC_COUNTER(VCPU, nmi_injections),\n\tSTATS_DESC_COUNTER(VCPU, req_event),\n\tSTATS_DESC_COUNTER(VCPU, nested_run),\n\tSTATS_DESC_COUNTER(VCPU, directed_yield_attempted),\n\tSTATS_DESC_COUNTER(VCPU, directed_yield_successful),\n\tSTATS_DESC_COUNTER(VCPU, preemption_reported),\n\tSTATS_DESC_COUNTER(VCPU, preemption_other),\n\tSTATS_DESC_IBOOLEAN(VCPU, guest_mode),\n\tSTATS_DESC_COUNTER(VCPU, notify_window_exits),\n};\n\nconst struct kvm_stats_header kvm_vcpu_stats_header = {\n\t.name_size = KVM_STATS_NAME_SIZE,\n\t.num_desc = ARRAY_SIZE(kvm_vcpu_stats_desc),\n\t.id_offset = sizeof(struct kvm_stats_header),\n\t.desc_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE,\n\t.data_offset = sizeof(struct kvm_stats_header) + KVM_STATS_NAME_SIZE +\n\t\t       sizeof(kvm_vcpu_stats_desc),\n};\n\nu64 __read_mostly host_xcr0;\n\nstatic struct kmem_cache *x86_emulator_cache;\n\n/*\n * When called, it means the previous get/set msr reached an invalid msr.\n * Return true if we want to ignore/silent this failed msr access.\n */\nstatic bool kvm_msr_ignored_check(u32 msr, u64 data, bool write)\n{\n\tconst char *op = write ? \"wrmsr\" : \"rdmsr\";\n\n\tif (ignore_msrs) {\n\t\tif (report_ignored_msrs)\n\t\t\tkvm_pr_unimpl(\"ignored %s: 0x%x data 0x%llx\\n\",\n\t\t\t\t      op, msr, data);\n\t\t/* Mask the error */\n\t\treturn true;\n\t} else {\n\t\tkvm_debug_ratelimited(\"unhandled %s: 0x%x data 0x%llx\\n\",\n\t\t\t\t      op, msr, data);\n\t\treturn false;\n\t}\n}\n\nstatic struct kmem_cache *kvm_alloc_emulator_cache(void)\n{\n\tunsigned int useroffset = offsetof(struct x86_emulate_ctxt, src);\n\tunsigned int size = sizeof(struct x86_emulate_ctxt);\n\n\treturn kmem_cache_create_usercopy(\"x86_emulator\", size,\n\t\t\t\t\t  __alignof__(struct x86_emulate_ctxt),\n\t\t\t\t\t  SLAB_ACCOUNT, useroffset,\n\t\t\t\t\t  size - useroffset, NULL);\n}\n\nstatic int emulator_fix_hypercall(struct x86_emulate_ctxt *ctxt);\n\nstatic inline void kvm_async_pf_hash_reset(struct kvm_vcpu *vcpu)\n{\n\tint i;\n\tfor (i = 0; i < ASYNC_PF_PER_VCPU; i++)\n\t\tvcpu->arch.apf.gfns[i] = ~0;\n}\n\nstatic void kvm_on_user_return(struct user_return_notifier *urn)\n{\n\tunsigned slot;\n\tstruct kvm_user_return_msrs *msrs\n\t\t= container_of(urn, struct kvm_user_return_msrs, urn);\n\tstruct kvm_user_return_msr_values *values;\n\tunsigned long flags;\n\n\t/*\n\t * Disabling irqs at this point since the following code could be\n\t * interrupted and executed through kvm_arch_hardware_disable()\n\t */\n\tlocal_irq_save(flags);\n\tif (msrs->registered) {\n\t\tmsrs->registered = false;\n\t\tuser_return_notifier_unregister(urn);\n\t}\n\tlocal_irq_restore(flags);\n\tfor (slot = 0; slot < kvm_nr_uret_msrs; ++slot) {\n\t\tvalues = &msrs->values[slot];\n\t\tif (values->host != values->curr) {\n\t\t\twrmsrl(kvm_uret_msrs_list[slot], values->host);\n\t\t\tvalues->curr = values->host;\n\t\t}\n\t}\n}\n\nstatic int kvm_probe_user_return_msr(u32 msr)\n{\n\tu64 val;\n\tint ret;\n\n\tpreempt_disable();\n\tret = rdmsrl_safe(msr, &val);\n\tif (ret)\n\t\tgoto out;\n\tret = wrmsrl_safe(msr, val);\nout:\n\tpreempt_enable();\n\treturn ret;\n}\n\nint kvm_add_user_return_msr(u32 msr)\n{\n\tBUG_ON(kvm_nr_uret_msrs >= KVM_MAX_NR_USER_RETURN_MSRS);\n\n\tif (kvm_probe_user_return_msr(msr))\n\t\treturn -1;\n\n\tkvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;\n\treturn kvm_nr_uret_msrs++;\n}\nEXPORT_SYMBOL_GPL(kvm_add_user_return_msr);\n\nint kvm_find_user_return_msr(u32 msr)\n{\n\tint i;\n\n\tfor (i = 0; i < kvm_nr_uret_msrs; ++i) {\n\t\tif (kvm_uret_msrs_list[i] == msr)\n\t\t\treturn i;\n\t}\n\treturn -1;\n}\nEXPORT_SYMBOL_GPL(kvm_find_user_return_msr);\n\nstatic void kvm_user_return_msr_cpu_online(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);\n\tu64 value;\n\tint i;\n\n\tfor (i = 0; i < kvm_nr_uret_msrs; ++i) {\n\t\trdmsrl_safe(kvm_uret_msrs_list[i], &value);\n\t\tmsrs->values[i].host = value;\n\t\tmsrs->values[i].curr = value;\n\t}\n}\n\nint kvm_set_user_return_msr(unsigned slot, u64 value, u64 mask)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);\n\tint err;\n\n\tvalue = (value & mask) | (msrs->values[slot].host & ~mask);\n\tif (value == msrs->values[slot].curr)\n\t\treturn 0;\n\terr = wrmsrl_safe(kvm_uret_msrs_list[slot], value);\n\tif (err)\n\t\treturn 1;\n\n\tmsrs->values[slot].curr = value;\n\tif (!msrs->registered) {\n\t\tmsrs->urn.on_user_return = kvm_on_user_return;\n\t\tuser_return_notifier_register(&msrs->urn);\n\t\tmsrs->registered = true;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_user_return_msr);\n\nstatic void drop_user_return_notifiers(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);\n\n\tif (msrs->registered)\n\t\tkvm_on_user_return(&msrs->urn);\n}\n\nu64 kvm_get_apic_base(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.apic_base;\n}\n\nenum lapic_mode kvm_get_apic_mode(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_apic_mode(kvm_get_apic_base(vcpu));\n}\nEXPORT_SYMBOL_GPL(kvm_get_apic_mode);\n\nint kvm_set_apic_base(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tenum lapic_mode old_mode = kvm_get_apic_mode(vcpu);\n\tenum lapic_mode new_mode = kvm_apic_mode(msr_info->data);\n\tu64 reserved_bits = kvm_vcpu_reserved_gpa_bits_raw(vcpu) | 0x2ff |\n\t\t(guest_cpuid_has(vcpu, X86_FEATURE_X2APIC) ? 0 : X2APIC_ENABLE);\n\n\tif ((msr_info->data & reserved_bits) != 0 || new_mode == LAPIC_MODE_INVALID)\n\t\treturn 1;\n\tif (!msr_info->host_initiated) {\n\t\tif (old_mode == LAPIC_MODE_X2APIC && new_mode == LAPIC_MODE_XAPIC)\n\t\t\treturn 1;\n\t\tif (old_mode == LAPIC_MODE_DISABLED && new_mode == LAPIC_MODE_X2APIC)\n\t\t\treturn 1;\n\t}\n\n\tkvm_lapic_set_base(vcpu, msr_info->data);\n\tkvm_recalculate_apic_map(vcpu->kvm);\n\treturn 0;\n}\n\n/*\n * Handle a fault on a hardware virtualization (VMX or SVM) instruction.\n *\n * Hardware virtualization extension instructions may fault if a reboot turns\n * off virtualization while processes are running.  Usually after catching the\n * fault we just panic; during reboot instead the instruction is ignored.\n */\nnoinstr void kvm_spurious_fault(void)\n{\n\t/* Fault while not rebooting.  We want the trace. */\n\tBUG_ON(!kvm_rebooting);\n}\nEXPORT_SYMBOL_GPL(kvm_spurious_fault);\n\n#define EXCPT_BENIGN\t\t0\n#define EXCPT_CONTRIBUTORY\t1\n#define EXCPT_PF\t\t2\n\nstatic int exception_class(int vector)\n{\n\tswitch (vector) {\n\tcase PF_VECTOR:\n\t\treturn EXCPT_PF;\n\tcase DE_VECTOR:\n\tcase TS_VECTOR:\n\tcase NP_VECTOR:\n\tcase SS_VECTOR:\n\tcase GP_VECTOR:\n\t\treturn EXCPT_CONTRIBUTORY;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn EXCPT_BENIGN;\n}\n\n#define EXCPT_FAULT\t\t0\n#define EXCPT_TRAP\t\t1\n#define EXCPT_ABORT\t\t2\n#define EXCPT_INTERRUPT\t\t3\n#define EXCPT_DB\t\t4\n\nstatic int exception_type(int vector)\n{\n\tunsigned int mask;\n\n\tif (WARN_ON(vector > 31 || vector == NMI_VECTOR))\n\t\treturn EXCPT_INTERRUPT;\n\n\tmask = 1 << vector;\n\n\t/*\n\t * #DBs can be trap-like or fault-like, the caller must check other CPU\n\t * state, e.g. DR6, to determine whether a #DB is a trap or fault.\n\t */\n\tif (mask & (1 << DB_VECTOR))\n\t\treturn EXCPT_DB;\n\n\tif (mask & ((1 << BP_VECTOR) | (1 << OF_VECTOR)))\n\t\treturn EXCPT_TRAP;\n\n\tif (mask & ((1 << DF_VECTOR) | (1 << MC_VECTOR)))\n\t\treturn EXCPT_ABORT;\n\n\t/* Reserved exceptions will result in fault */\n\treturn EXCPT_FAULT;\n}\n\nvoid kvm_deliver_exception_payload(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct kvm_queued_exception *ex)\n{\n\tif (!ex->has_payload)\n\t\treturn;\n\n\tswitch (ex->vector) {\n\tcase DB_VECTOR:\n\t\t/*\n\t\t * \"Certain debug exceptions may clear bit 0-3.  The\n\t\t * remaining contents of the DR6 register are never\n\t\t * cleared by the processor\".\n\t\t */\n\t\tvcpu->arch.dr6 &= ~DR_TRAP_BITS;\n\t\t/*\n\t\t * In order to reflect the #DB exception payload in guest\n\t\t * dr6, three components need to be considered: active low\n\t\t * bit, FIXED_1 bits and active high bits (e.g. DR6_BD,\n\t\t * DR6_BS and DR6_BT)\n\t\t * DR6_ACTIVE_LOW contains the FIXED_1 and active low bits.\n\t\t * In the target guest dr6:\n\t\t * FIXED_1 bits should always be set.\n\t\t * Active low bits should be cleared if 1-setting in payload.\n\t\t * Active high bits should be set if 1-setting in payload.\n\t\t *\n\t\t * Note, the payload is compatible with the pending debug\n\t\t * exceptions/exit qualification under VMX, that active_low bits\n\t\t * are active high in payload.\n\t\t * So they need to be flipped for DR6.\n\t\t */\n\t\tvcpu->arch.dr6 |= DR6_ACTIVE_LOW;\n\t\tvcpu->arch.dr6 |= ex->payload;\n\t\tvcpu->arch.dr6 ^= ex->payload & DR6_ACTIVE_LOW;\n\n\t\t/*\n\t\t * The #DB payload is defined as compatible with the 'pending\n\t\t * debug exceptions' field under VMX, not DR6. While bit 12 is\n\t\t * defined in the 'pending debug exceptions' field (enabled\n\t\t * breakpoint), it is reserved and must be zero in DR6.\n\t\t */\n\t\tvcpu->arch.dr6 &= ~BIT(12);\n\t\tbreak;\n\tcase PF_VECTOR:\n\t\tvcpu->arch.cr2 = ex->payload;\n\t\tbreak;\n\t}\n\n\tex->has_payload = false;\n\tex->payload = 0;\n}\nEXPORT_SYMBOL_GPL(kvm_deliver_exception_payload);\n\nstatic void kvm_queue_exception_vmexit(struct kvm_vcpu *vcpu, unsigned int vector,\n\t\t\t\t       bool has_error_code, u32 error_code,\n\t\t\t\t       bool has_payload, unsigned long payload)\n{\n\tstruct kvm_queued_exception *ex = &vcpu->arch.exception_vmexit;\n\n\tex->vector = vector;\n\tex->injected = false;\n\tex->pending = true;\n\tex->has_error_code = has_error_code;\n\tex->error_code = error_code;\n\tex->has_payload = has_payload;\n\tex->payload = payload;\n}\n\n/* Forcibly leave the nested mode in cases like a vCPU reset */\nstatic void kvm_leave_nested(struct kvm_vcpu *vcpu)\n{\n\tkvm_x86_ops.nested_ops->leave_nested(vcpu);\n}\n\nstatic void kvm_multiple_exception(struct kvm_vcpu *vcpu,\n\t\tunsigned nr, bool has_error, u32 error_code,\n\t        bool has_payload, unsigned long payload, bool reinject)\n{\n\tu32 prev_nr;\n\tint class1, class2;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\t/*\n\t * If the exception is destined for L2 and isn't being reinjected,\n\t * morph it to a VM-Exit if L1 wants to intercept the exception.  A\n\t * previously injected exception is not checked because it was checked\n\t * when it was original queued, and re-checking is incorrect if _L1_\n\t * injected the exception, in which case it's exempt from interception.\n\t */\n\tif (!reinject && is_guest_mode(vcpu) &&\n\t    kvm_x86_ops.nested_ops->is_exception_vmexit(vcpu, nr, error_code)) {\n\t\tkvm_queue_exception_vmexit(vcpu, nr, has_error, error_code,\n\t\t\t\t\t   has_payload, payload);\n\t\treturn;\n\t}\n\n\tif (!vcpu->arch.exception.pending && !vcpu->arch.exception.injected) {\n\tqueue:\n\t\tif (reinject) {\n\t\t\t/*\n\t\t\t * On VM-Entry, an exception can be pending if and only\n\t\t\t * if event injection was blocked by nested_run_pending.\n\t\t\t * In that case, however, vcpu_enter_guest() requests an\n\t\t\t * immediate exit, and the guest shouldn't proceed far\n\t\t\t * enough to need reinjection.\n\t\t\t */\n\t\t\tWARN_ON_ONCE(kvm_is_exception_pending(vcpu));\n\t\t\tvcpu->arch.exception.injected = true;\n\t\t\tif (WARN_ON_ONCE(has_payload)) {\n\t\t\t\t/*\n\t\t\t\t * A reinjected event has already\n\t\t\t\t * delivered its payload.\n\t\t\t\t */\n\t\t\t\thas_payload = false;\n\t\t\t\tpayload = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tvcpu->arch.exception.pending = true;\n\t\t\tvcpu->arch.exception.injected = false;\n\t\t}\n\t\tvcpu->arch.exception.has_error_code = has_error;\n\t\tvcpu->arch.exception.vector = nr;\n\t\tvcpu->arch.exception.error_code = error_code;\n\t\tvcpu->arch.exception.has_payload = has_payload;\n\t\tvcpu->arch.exception.payload = payload;\n\t\tif (!is_guest_mode(vcpu))\n\t\t\tkvm_deliver_exception_payload(vcpu,\n\t\t\t\t\t\t      &vcpu->arch.exception);\n\t\treturn;\n\t}\n\n\t/* to check exception */\n\tprev_nr = vcpu->arch.exception.vector;\n\tif (prev_nr == DF_VECTOR) {\n\t\t/* triple fault -> shutdown */\n\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\treturn;\n\t}\n\tclass1 = exception_class(prev_nr);\n\tclass2 = exception_class(nr);\n\tif ((class1 == EXCPT_CONTRIBUTORY && class2 == EXCPT_CONTRIBUTORY) ||\n\t    (class1 == EXCPT_PF && class2 != EXCPT_BENIGN)) {\n\t\t/*\n\t\t * Synthesize #DF.  Clear the previously injected or pending\n\t\t * exception so as not to incorrectly trigger shutdown.\n\t\t */\n\t\tvcpu->arch.exception.injected = false;\n\t\tvcpu->arch.exception.pending = false;\n\n\t\tkvm_queue_exception_e(vcpu, DF_VECTOR, 0);\n\t} else {\n\t\t/* replace previous exception with a new one in a hope\n\t\t   that instruction re-execution will regenerate lost\n\t\t   exception */\n\t\tgoto queue;\n\t}\n}\n\nvoid kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr)\n{\n\tkvm_multiple_exception(vcpu, nr, false, 0, false, 0, false);\n}\nEXPORT_SYMBOL_GPL(kvm_queue_exception);\n\nvoid kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr)\n{\n\tkvm_multiple_exception(vcpu, nr, false, 0, false, 0, true);\n}\nEXPORT_SYMBOL_GPL(kvm_requeue_exception);\n\nvoid kvm_queue_exception_p(struct kvm_vcpu *vcpu, unsigned nr,\n\t\t\t   unsigned long payload)\n{\n\tkvm_multiple_exception(vcpu, nr, false, 0, true, payload, false);\n}\nEXPORT_SYMBOL_GPL(kvm_queue_exception_p);\n\nstatic void kvm_queue_exception_e_p(struct kvm_vcpu *vcpu, unsigned nr,\n\t\t\t\t    u32 error_code, unsigned long payload)\n{\n\tkvm_multiple_exception(vcpu, nr, true, error_code,\n\t\t\t       true, payload, false);\n}\n\nint kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err)\n{\n\tif (err)\n\t\tkvm_inject_gp(vcpu, 0);\n\telse\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(kvm_complete_insn_gp);\n\nstatic int complete_emulated_insn_gp(struct kvm_vcpu *vcpu, int err)\n{\n\tif (err) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\treturn kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE | EMULTYPE_SKIP |\n\t\t\t\t       EMULTYPE_COMPLETE_USER_EXIT);\n}\n\nvoid kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault)\n{\n\t++vcpu->stat.pf_guest;\n\n\t/*\n\t * Async #PF in L2 is always forwarded to L1 as a VM-Exit regardless of\n\t * whether or not L1 wants to intercept \"regular\" #PF.\n\t */\n\tif (is_guest_mode(vcpu) && fault->async_page_fault)\n\t\tkvm_queue_exception_vmexit(vcpu, PF_VECTOR,\n\t\t\t\t\t   true, fault->error_code,\n\t\t\t\t\t   true, fault->address);\n\telse\n\t\tkvm_queue_exception_e_p(vcpu, PF_VECTOR, fault->error_code,\n\t\t\t\t\tfault->address);\n}\n\nvoid kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct x86_exception *fault)\n{\n\tstruct kvm_mmu *fault_mmu;\n\tWARN_ON_ONCE(fault->vector != PF_VECTOR);\n\n\tfault_mmu = fault->nested_page_fault ? vcpu->arch.mmu :\n\t\t\t\t\t       vcpu->arch.walk_mmu;\n\n\t/*\n\t * Invalidate the TLB entry for the faulting address, if it exists,\n\t * else the access will fault indefinitely (and to emulate hardware).\n\t */\n\tif ((fault->error_code & PFERR_PRESENT_MASK) &&\n\t    !(fault->error_code & PFERR_RSVD_MASK))\n\t\tkvm_mmu_invalidate_gva(vcpu, fault_mmu, fault->address,\n\t\t\t\t       fault_mmu->root.hpa);\n\n\tfault_mmu->inject_page_fault(vcpu, fault);\n}\nEXPORT_SYMBOL_GPL(kvm_inject_emulated_page_fault);\n\nvoid kvm_inject_nmi(struct kvm_vcpu *vcpu)\n{\n\tatomic_inc(&vcpu->arch.nmi_queued);\n\tkvm_make_request(KVM_REQ_NMI, vcpu);\n}\n\nvoid kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code)\n{\n\tkvm_multiple_exception(vcpu, nr, true, error_code, false, 0, false);\n}\nEXPORT_SYMBOL_GPL(kvm_queue_exception_e);\n\nvoid kvm_requeue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code)\n{\n\tkvm_multiple_exception(vcpu, nr, true, error_code, false, 0, true);\n}\nEXPORT_SYMBOL_GPL(kvm_requeue_exception_e);\n\n/*\n * Checks if cpl <= required_cpl; if true, return true.  Otherwise queue\n * a #GP and return false.\n */\nbool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl)\n{\n\tif (static_call(kvm_x86_get_cpl)(vcpu) <= required_cpl)\n\t\treturn true;\n\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\treturn false;\n}\n\nbool kvm_require_dr(struct kvm_vcpu *vcpu, int dr)\n{\n\tif ((dr != 4 && dr != 5) || !kvm_read_cr4_bits(vcpu, X86_CR4_DE))\n\t\treturn true;\n\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(kvm_require_dr);\n\nstatic inline u64 pdptr_rsvd_bits(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.reserved_gpa_bits | rsvd_bits(5, 8) | rsvd_bits(1, 2);\n}\n\n/*\n * Load the pae pdptrs.  Return 1 if they are all valid, 0 otherwise.\n */\nint load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tgfn_t pdpt_gfn = cr3 >> PAGE_SHIFT;\n\tgpa_t real_gpa;\n\tint i;\n\tint ret;\n\tu64 pdpte[ARRAY_SIZE(mmu->pdptrs)];\n\n\t/*\n\t * If the MMU is nested, CR3 holds an L2 GPA and needs to be translated\n\t * to an L1 GPA.\n\t */\n\treal_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn),\n\t\t\t\t     PFERR_USER_MASK | PFERR_WRITE_MASK, NULL);\n\tif (real_gpa == INVALID_GPA)\n\t\treturn 0;\n\n\t/* Note the offset, PDPTRs are 32 byte aligned when using PAE paging. */\n\tret = kvm_vcpu_read_guest_page(vcpu, gpa_to_gfn(real_gpa), pdpte,\n\t\t\t\t       cr3 & GENMASK(11, 5), sizeof(pdpte));\n\tif (ret < 0)\n\t\treturn 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(pdpte); ++i) {\n\t\tif ((pdpte[i] & PT_PRESENT_MASK) &&\n\t\t    (pdpte[i] & pdptr_rsvd_bits(vcpu))) {\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * Marking VCPU_EXREG_PDPTR dirty doesn't work for !tdp_enabled.\n\t * Shadow page roots need to be reconstructed instead.\n\t */\n\tif (!tdp_enabled && memcmp(mmu->pdptrs, pdpte, sizeof(mmu->pdptrs)))\n\t\tkvm_mmu_free_roots(vcpu->kvm, mmu, KVM_MMU_ROOT_CURRENT);\n\n\tmemcpy(mmu->pdptrs, pdpte, sizeof(mmu->pdptrs));\n\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_PDPTR);\n\tkvm_make_request(KVM_REQ_LOAD_MMU_PGD, vcpu);\n\tvcpu->arch.pdptrs_from_userspace = false;\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(load_pdptrs);\n\nvoid kvm_post_set_cr0(struct kvm_vcpu *vcpu, unsigned long old_cr0, unsigned long cr0)\n{\n\tif ((cr0 ^ old_cr0) & X86_CR0_PG) {\n\t\tkvm_clear_async_pf_completion_queue(vcpu);\n\t\tkvm_async_pf_hash_reset(vcpu);\n\n\t\t/*\n\t\t * Clearing CR0.PG is defined to flush the TLB from the guest's\n\t\t * perspective.\n\t\t */\n\t\tif (!(cr0 & X86_CR0_PG))\n\t\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t}\n\n\tif ((cr0 ^ old_cr0) & KVM_MMU_CR0_ROLE_BITS)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\tif (((cr0 ^ old_cr0) & X86_CR0_CD) &&\n\t    kvm_arch_has_noncoherent_dma(vcpu->kvm) &&\n\t    !kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))\n\t\tkvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);\n}\nEXPORT_SYMBOL_GPL(kvm_post_set_cr0);\n\nint kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)\n{\n\tunsigned long old_cr0 = kvm_read_cr0(vcpu);\n\n\tcr0 |= X86_CR0_ET;\n\n#ifdef CONFIG_X86_64\n\tif (cr0 & 0xffffffff00000000UL)\n\t\treturn 1;\n#endif\n\n\tcr0 &= ~CR0_RESERVED_BITS;\n\n\tif ((cr0 & X86_CR0_NW) && !(cr0 & X86_CR0_CD))\n\t\treturn 1;\n\n\tif ((cr0 & X86_CR0_PG) && !(cr0 & X86_CR0_PE))\n\t\treturn 1;\n\n#ifdef CONFIG_X86_64\n\tif ((vcpu->arch.efer & EFER_LME) && !is_paging(vcpu) &&\n\t    (cr0 & X86_CR0_PG)) {\n\t\tint cs_db, cs_l;\n\n\t\tif (!is_pae(vcpu))\n\t\t\treturn 1;\n\t\tstatic_call(kvm_x86_get_cs_db_l_bits)(vcpu, &cs_db, &cs_l);\n\t\tif (cs_l)\n\t\t\treturn 1;\n\t}\n#endif\n\tif (!(vcpu->arch.efer & EFER_LME) && (cr0 & X86_CR0_PG) &&\n\t    is_pae(vcpu) && ((cr0 ^ old_cr0) & X86_CR0_PDPTR_BITS) &&\n\t    !load_pdptrs(vcpu, kvm_read_cr3(vcpu)))\n\t\treturn 1;\n\n\tif (!(cr0 & X86_CR0_PG) &&\n\t    (is_64_bit_mode(vcpu) || kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE)))\n\t\treturn 1;\n\n\tstatic_call(kvm_x86_set_cr0)(vcpu, cr0);\n\n\tkvm_post_set_cr0(vcpu, old_cr0, cr0);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr0);\n\nvoid kvm_lmsw(struct kvm_vcpu *vcpu, unsigned long msw)\n{\n\t(void)kvm_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~0x0eul) | (msw & 0x0f));\n}\nEXPORT_SYMBOL_GPL(kvm_lmsw);\n\nvoid kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn;\n\n\tif (kvm_read_cr4_bits(vcpu, X86_CR4_OSXSAVE)) {\n\n\t\tif (vcpu->arch.xcr0 != host_xcr0)\n\t\t\txsetbv(XCR_XFEATURE_ENABLED_MASK, vcpu->arch.xcr0);\n\n\t\tif (vcpu->arch.xsaves_enabled &&\n\t\t    vcpu->arch.ia32_xss != host_xss)\n\t\t\twrmsrl(MSR_IA32_XSS, vcpu->arch.ia32_xss);\n\t}\n\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\tif (static_cpu_has(X86_FEATURE_PKU) &&\n\t    vcpu->arch.pkru != vcpu->arch.host_pkru &&\n\t    ((vcpu->arch.xcr0 & XFEATURE_MASK_PKRU) ||\n\t     kvm_read_cr4_bits(vcpu, X86_CR4_PKE)))\n\t\twrite_pkru(vcpu->arch.pkru);\n#endif /* CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS */\n}\nEXPORT_SYMBOL_GPL(kvm_load_guest_xsave_state);\n\nvoid kvm_load_host_xsave_state(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn;\n\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\tif (static_cpu_has(X86_FEATURE_PKU) &&\n\t    ((vcpu->arch.xcr0 & XFEATURE_MASK_PKRU) ||\n\t     kvm_read_cr4_bits(vcpu, X86_CR4_PKE))) {\n\t\tvcpu->arch.pkru = rdpkru();\n\t\tif (vcpu->arch.pkru != vcpu->arch.host_pkru)\n\t\t\twrite_pkru(vcpu->arch.host_pkru);\n\t}\n#endif /* CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS */\n\n\tif (kvm_read_cr4_bits(vcpu, X86_CR4_OSXSAVE)) {\n\n\t\tif (vcpu->arch.xcr0 != host_xcr0)\n\t\t\txsetbv(XCR_XFEATURE_ENABLED_MASK, host_xcr0);\n\n\t\tif (vcpu->arch.xsaves_enabled &&\n\t\t    vcpu->arch.ia32_xss != host_xss)\n\t\t\twrmsrl(MSR_IA32_XSS, host_xss);\n\t}\n\n}\nEXPORT_SYMBOL_GPL(kvm_load_host_xsave_state);\n\n#ifdef CONFIG_X86_64\nstatic inline u64 kvm_guest_supported_xfd(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.guest_supported_xcr0 & XFEATURE_MASK_USER_DYNAMIC;\n}\n#endif\n\nstatic int __kvm_set_xcr(struct kvm_vcpu *vcpu, u32 index, u64 xcr)\n{\n\tu64 xcr0 = xcr;\n\tu64 old_xcr0 = vcpu->arch.xcr0;\n\tu64 valid_bits;\n\n\t/* Only support XCR_XFEATURE_ENABLED_MASK(xcr0) now  */\n\tif (index != XCR_XFEATURE_ENABLED_MASK)\n\t\treturn 1;\n\tif (!(xcr0 & XFEATURE_MASK_FP))\n\t\treturn 1;\n\tif ((xcr0 & XFEATURE_MASK_YMM) && !(xcr0 & XFEATURE_MASK_SSE))\n\t\treturn 1;\n\n\t/*\n\t * Do not allow the guest to set bits that we do not support\n\t * saving.  However, xcr0 bit 0 is always set, even if the\n\t * emulated CPU does not support XSAVE (see kvm_vcpu_reset()).\n\t */\n\tvalid_bits = vcpu->arch.guest_supported_xcr0 | XFEATURE_MASK_FP;\n\tif (xcr0 & ~valid_bits)\n\t\treturn 1;\n\n\tif ((!(xcr0 & XFEATURE_MASK_BNDREGS)) !=\n\t    (!(xcr0 & XFEATURE_MASK_BNDCSR)))\n\t\treturn 1;\n\n\tif (xcr0 & XFEATURE_MASK_AVX512) {\n\t\tif (!(xcr0 & XFEATURE_MASK_YMM))\n\t\t\treturn 1;\n\t\tif ((xcr0 & XFEATURE_MASK_AVX512) != XFEATURE_MASK_AVX512)\n\t\t\treturn 1;\n\t}\n\n\tif ((xcr0 & XFEATURE_MASK_XTILE) &&\n\t    ((xcr0 & XFEATURE_MASK_XTILE) != XFEATURE_MASK_XTILE))\n\t\treturn 1;\n\n\tvcpu->arch.xcr0 = xcr0;\n\n\tif ((xcr0 ^ old_xcr0) & XFEATURE_MASK_EXTEND)\n\t\tkvm_update_cpuid_runtime(vcpu);\n\treturn 0;\n}\n\nint kvm_emulate_xsetbv(struct kvm_vcpu *vcpu)\n{\n\t/* Note, #UD due to CR4.OSXSAVE=0 has priority over the intercept. */\n\tif (static_call(kvm_x86_get_cpl)(vcpu) != 0 ||\n\t    __kvm_set_xcr(vcpu, kvm_rcx_read(vcpu), kvm_read_edx_eax(vcpu))) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_xsetbv);\n\nbool __kvm_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\tif (cr4 & cr4_reserved_bits)\n\t\treturn false;\n\n\tif (cr4 & vcpu->arch.cr4_guest_rsvd_bits)\n\t\treturn false;\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(__kvm_is_valid_cr4);\n\nstatic bool kvm_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\treturn __kvm_is_valid_cr4(vcpu, cr4) &&\n\t       static_call(kvm_x86_is_valid_cr4)(vcpu, cr4);\n}\n\nvoid kvm_post_set_cr4(struct kvm_vcpu *vcpu, unsigned long old_cr4, unsigned long cr4)\n{\n\tif ((cr4 ^ old_cr4) & KVM_MMU_CR4_ROLE_BITS)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\t/*\n\t * If CR4.PCIDE is changed 0 -> 1, there is no need to flush the TLB\n\t * according to the SDM; however, stale prev_roots could be reused\n\t * incorrectly in the future after a MOV to CR3 with NOFLUSH=1, so we\n\t * free them all.  This is *not* a superset of KVM_REQ_TLB_FLUSH_GUEST\n\t * or KVM_REQ_TLB_FLUSH_CURRENT, because the hardware TLB is not flushed,\n\t * so fall through.\n\t */\n\tif (!tdp_enabled &&\n\t    (cr4 & X86_CR4_PCIDE) && !(old_cr4 & X86_CR4_PCIDE))\n\t\tkvm_mmu_unload(vcpu);\n\n\t/*\n\t * The TLB has to be flushed for all PCIDs if any of the following\n\t * (architecturally required) changes happen:\n\t * - CR4.PCIDE is changed from 1 to 0\n\t * - CR4.PGE is toggled\n\t *\n\t * This is a superset of KVM_REQ_TLB_FLUSH_CURRENT.\n\t */\n\tif (((cr4 ^ old_cr4) & X86_CR4_PGE) ||\n\t    (!(cr4 & X86_CR4_PCIDE) && (old_cr4 & X86_CR4_PCIDE)))\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\n\t/*\n\t * The TLB has to be flushed for the current PCID if any of the\n\t * following (architecturally required) changes happen:\n\t * - CR4.SMEP is changed from 0 to 1\n\t * - CR4.PAE is toggled\n\t */\n\telse if (((cr4 ^ old_cr4) & X86_CR4_PAE) ||\n\t\t ((cr4 & X86_CR4_SMEP) && !(old_cr4 & X86_CR4_SMEP)))\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n\n}\nEXPORT_SYMBOL_GPL(kvm_post_set_cr4);\n\nint kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\tunsigned long old_cr4 = kvm_read_cr4(vcpu);\n\n\tif (!kvm_is_valid_cr4(vcpu, cr4))\n\t\treturn 1;\n\n\tif (is_long_mode(vcpu)) {\n\t\tif (!(cr4 & X86_CR4_PAE))\n\t\t\treturn 1;\n\t\tif ((cr4 ^ old_cr4) & X86_CR4_LA57)\n\t\t\treturn 1;\n\t} else if (is_paging(vcpu) && (cr4 & X86_CR4_PAE)\n\t\t   && ((cr4 ^ old_cr4) & X86_CR4_PDPTR_BITS)\n\t\t   && !load_pdptrs(vcpu, kvm_read_cr3(vcpu)))\n\t\treturn 1;\n\n\tif ((cr4 & X86_CR4_PCIDE) && !(old_cr4 & X86_CR4_PCIDE)) {\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_PCID))\n\t\t\treturn 1;\n\n\t\t/* PCID can not be enabled when cr3[11:0]!=000H or EFER.LMA=0 */\n\t\tif ((kvm_read_cr3(vcpu) & X86_CR3_PCID_MASK) || !is_long_mode(vcpu))\n\t\t\treturn 1;\n\t}\n\n\tstatic_call(kvm_x86_set_cr4)(vcpu, cr4);\n\n\tkvm_post_set_cr4(vcpu, old_cr4, cr4);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr4);\n\nstatic void kvm_invalidate_pcid(struct kvm_vcpu *vcpu, unsigned long pcid)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tunsigned long roots_to_free = 0;\n\tint i;\n\n\t/*\n\t * MOV CR3 and INVPCID are usually not intercepted when using TDP, but\n\t * this is reachable when running EPT=1 and unrestricted_guest=0,  and\n\t * also via the emulator.  KVM's TDP page tables are not in the scope of\n\t * the invalidation, but the guest's TLB entries need to be flushed as\n\t * the CPU may have cached entries in its TLB for the target PCID.\n\t */\n\tif (unlikely(tdp_enabled)) {\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t\treturn;\n\t}\n\n\t/*\n\t * If neither the current CR3 nor any of the prev_roots use the given\n\t * PCID, then nothing needs to be done here because a resync will\n\t * happen anyway before switching to any other CR3.\n\t */\n\tif (kvm_get_active_pcid(vcpu) == pcid) {\n\t\tkvm_make_request(KVM_REQ_MMU_SYNC, vcpu);\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n\t}\n\n\t/*\n\t * If PCID is disabled, there is no need to free prev_roots even if the\n\t * PCIDs for them are also 0, because MOV to CR3 always flushes the TLB\n\t * with PCIDE=0.\n\t */\n\tif (!kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE))\n\t\treturn;\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)\n\t\tif (kvm_get_pcid(vcpu, mmu->prev_roots[i].pgd) == pcid)\n\t\t\troots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);\n\n\tkvm_mmu_free_roots(vcpu->kvm, mmu, roots_to_free);\n}\n\nint kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)\n{\n\tbool skip_tlb_flush = false;\n\tunsigned long pcid = 0;\n#ifdef CONFIG_X86_64\n\tbool pcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);\n\n\tif (pcid_enabled) {\n\t\tskip_tlb_flush = cr3 & X86_CR3_PCID_NOFLUSH;\n\t\tcr3 &= ~X86_CR3_PCID_NOFLUSH;\n\t\tpcid = cr3 & X86_CR3_PCID_MASK;\n\t}\n#endif\n\n\t/* PDPTRs are always reloaded for PAE paging. */\n\tif (cr3 == kvm_read_cr3(vcpu) && !is_pae_paging(vcpu))\n\t\tgoto handle_tlb_flush;\n\n\t/*\n\t * Do not condition the GPA check on long mode, this helper is used to\n\t * stuff CR3, e.g. for RSM emulation, and there is no guarantee that\n\t * the current vCPU mode is accurate.\n\t */\n\tif (kvm_vcpu_is_illegal_gpa(vcpu, cr3))\n\t\treturn 1;\n\n\tif (is_pae_paging(vcpu) && !load_pdptrs(vcpu, cr3))\n\t\treturn 1;\n\n\tif (cr3 != kvm_read_cr3(vcpu))\n\t\tkvm_mmu_new_pgd(vcpu, cr3);\n\n\tvcpu->arch.cr3 = cr3;\n\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_CR3);\n\t/* Do not call post_set_cr3, we do not get here for confidential guests.  */\n\nhandle_tlb_flush:\n\t/*\n\t * A load of CR3 that flushes the TLB flushes only the current PCID,\n\t * even if PCID is disabled, in which case PCID=0 is flushed.  It's a\n\t * moot point in the end because _disabling_ PCID will flush all PCIDs,\n\t * and it's impossible to use a non-zero PCID when PCID is disabled,\n\t * i.e. only PCID=0 can be relevant.\n\t */\n\tif (!skip_tlb_flush)\n\t\tkvm_invalidate_pcid(vcpu, pcid);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr3);\n\nint kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8)\n{\n\tif (cr8 & CR8_RESERVED_BITS)\n\t\treturn 1;\n\tif (lapic_in_kernel(vcpu))\n\t\tkvm_lapic_set_tpr(vcpu, cr8);\n\telse\n\t\tvcpu->arch.cr8 = cr8;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr8);\n\nunsigned long kvm_get_cr8(struct kvm_vcpu *vcpu)\n{\n\tif (lapic_in_kernel(vcpu))\n\t\treturn kvm_lapic_get_cr8(vcpu);\n\telse\n\t\treturn vcpu->arch.cr8;\n}\nEXPORT_SYMBOL_GPL(kvm_get_cr8);\n\nstatic void kvm_update_dr0123(struct kvm_vcpu *vcpu)\n{\n\tint i;\n\n\tif (!(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP)) {\n\t\tfor (i = 0; i < KVM_NR_DB_REGS; i++)\n\t\t\tvcpu->arch.eff_db[i] = vcpu->arch.db[i];\n\t}\n}\n\nvoid kvm_update_dr7(struct kvm_vcpu *vcpu)\n{\n\tunsigned long dr7;\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP)\n\t\tdr7 = vcpu->arch.guest_debug_dr7;\n\telse\n\t\tdr7 = vcpu->arch.dr7;\n\tstatic_call(kvm_x86_set_dr7)(vcpu, dr7);\n\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_BP_ENABLED;\n\tif (dr7 & DR7_BP_EN_MASK)\n\t\tvcpu->arch.switch_db_regs |= KVM_DEBUGREG_BP_ENABLED;\n}\nEXPORT_SYMBOL_GPL(kvm_update_dr7);\n\nstatic u64 kvm_dr6_fixed(struct kvm_vcpu *vcpu)\n{\n\tu64 fixed = DR6_FIXED_1;\n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_RTM))\n\t\tfixed |= DR6_RTM;\n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_BUS_LOCK_DETECT))\n\t\tfixed |= DR6_BUS_LOCK;\n\treturn fixed;\n}\n\nint kvm_set_dr(struct kvm_vcpu *vcpu, int dr, unsigned long val)\n{\n\tsize_t size = ARRAY_SIZE(vcpu->arch.db);\n\n\tswitch (dr) {\n\tcase 0 ... 3:\n\t\tvcpu->arch.db[array_index_nospec(dr, size)] = val;\n\t\tif (!(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP))\n\t\t\tvcpu->arch.eff_db[dr] = val;\n\t\tbreak;\n\tcase 4:\n\tcase 6:\n\t\tif (!kvm_dr6_valid(val))\n\t\t\treturn 1; /* #GP */\n\t\tvcpu->arch.dr6 = (val & DR6_VOLATILE) | kvm_dr6_fixed(vcpu);\n\t\tbreak;\n\tcase 5:\n\tdefault: /* 7 */\n\t\tif (!kvm_dr7_valid(val))\n\t\t\treturn 1; /* #GP */\n\t\tvcpu->arch.dr7 = (val & DR7_VOLATILE) | DR7_FIXED_1;\n\t\tkvm_update_dr7(vcpu);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_dr);\n\nvoid kvm_get_dr(struct kvm_vcpu *vcpu, int dr, unsigned long *val)\n{\n\tsize_t size = ARRAY_SIZE(vcpu->arch.db);\n\n\tswitch (dr) {\n\tcase 0 ... 3:\n\t\t*val = vcpu->arch.db[array_index_nospec(dr, size)];\n\t\tbreak;\n\tcase 4:\n\tcase 6:\n\t\t*val = vcpu->arch.dr6;\n\t\tbreak;\n\tcase 5:\n\tdefault: /* 7 */\n\t\t*val = vcpu->arch.dr7;\n\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_get_dr);\n\nint kvm_emulate_rdpmc(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = kvm_rcx_read(vcpu);\n\tu64 data;\n\n\tif (kvm_pmu_rdpmc(vcpu, ecx, &data)) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tkvm_rax_write(vcpu, (u32)data);\n\tkvm_rdx_write(vcpu, data >> 32);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_rdpmc);\n\n/*\n * List of msr numbers which we expose to userspace through KVM_GET_MSRS\n * and KVM_SET_MSRS, and KVM_GET_MSR_INDEX_LIST.\n *\n * The three MSR lists(msrs_to_save, emulated_msrs, msr_based_features)\n * extract the supported MSRs from the related const lists.\n * msrs_to_save is selected from the msrs_to_save_all to reflect the\n * capabilities of the host cpu. This capabilities test skips MSRs that are\n * kvm-specific. Those are put in emulated_msrs_all; filtering of emulated_msrs\n * may depend on host virtualization features rather than host cpu features.\n */\n\nstatic const u32 msrs_to_save_all[] = {\n\tMSR_IA32_SYSENTER_CS, MSR_IA32_SYSENTER_ESP, MSR_IA32_SYSENTER_EIP,\n\tMSR_STAR,\n#ifdef CONFIG_X86_64\n\tMSR_CSTAR, MSR_KERNEL_GS_BASE, MSR_SYSCALL_MASK, MSR_LSTAR,\n#endif\n\tMSR_IA32_TSC, MSR_IA32_CR_PAT, MSR_VM_HSAVE_PA,\n\tMSR_IA32_FEAT_CTL, MSR_IA32_BNDCFGS, MSR_TSC_AUX,\n\tMSR_IA32_SPEC_CTRL,\n\tMSR_IA32_RTIT_CTL, MSR_IA32_RTIT_STATUS, MSR_IA32_RTIT_CR3_MATCH,\n\tMSR_IA32_RTIT_OUTPUT_BASE, MSR_IA32_RTIT_OUTPUT_MASK,\n\tMSR_IA32_RTIT_ADDR0_A, MSR_IA32_RTIT_ADDR0_B,\n\tMSR_IA32_RTIT_ADDR1_A, MSR_IA32_RTIT_ADDR1_B,\n\tMSR_IA32_RTIT_ADDR2_A, MSR_IA32_RTIT_ADDR2_B,\n\tMSR_IA32_RTIT_ADDR3_A, MSR_IA32_RTIT_ADDR3_B,\n\tMSR_IA32_UMWAIT_CONTROL,\n\n\tMSR_ARCH_PERFMON_FIXED_CTR0, MSR_ARCH_PERFMON_FIXED_CTR1,\n\tMSR_ARCH_PERFMON_FIXED_CTR0 + 2,\n\tMSR_CORE_PERF_FIXED_CTR_CTRL, MSR_CORE_PERF_GLOBAL_STATUS,\n\tMSR_CORE_PERF_GLOBAL_CTRL, MSR_CORE_PERF_GLOBAL_OVF_CTRL,\n\tMSR_IA32_PEBS_ENABLE, MSR_IA32_DS_AREA, MSR_PEBS_DATA_CFG,\n\n\t/* This part of MSRs should match KVM_INTEL_PMC_MAX_GENERIC. */\n\tMSR_ARCH_PERFMON_PERFCTR0, MSR_ARCH_PERFMON_PERFCTR1,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 2, MSR_ARCH_PERFMON_PERFCTR0 + 3,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 4, MSR_ARCH_PERFMON_PERFCTR0 + 5,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 6, MSR_ARCH_PERFMON_PERFCTR0 + 7,\n\tMSR_ARCH_PERFMON_EVENTSEL0, MSR_ARCH_PERFMON_EVENTSEL1,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 2, MSR_ARCH_PERFMON_EVENTSEL0 + 3,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 4, MSR_ARCH_PERFMON_EVENTSEL0 + 5,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 6, MSR_ARCH_PERFMON_EVENTSEL0 + 7,\n\n\tMSR_K7_EVNTSEL0, MSR_K7_EVNTSEL1, MSR_K7_EVNTSEL2, MSR_K7_EVNTSEL3,\n\tMSR_K7_PERFCTR0, MSR_K7_PERFCTR1, MSR_K7_PERFCTR2, MSR_K7_PERFCTR3,\n\n\t/* This part of MSRs should match KVM_AMD_PMC_MAX_GENERIC. */\n\tMSR_F15H_PERF_CTL0, MSR_F15H_PERF_CTL1, MSR_F15H_PERF_CTL2,\n\tMSR_F15H_PERF_CTL3, MSR_F15H_PERF_CTL4, MSR_F15H_PERF_CTL5,\n\tMSR_F15H_PERF_CTR0, MSR_F15H_PERF_CTR1, MSR_F15H_PERF_CTR2,\n\tMSR_F15H_PERF_CTR3, MSR_F15H_PERF_CTR4, MSR_F15H_PERF_CTR5,\n\n\tMSR_IA32_XFD, MSR_IA32_XFD_ERR,\n};\n\nstatic u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_all)];\nstatic unsigned num_msrs_to_save;\n\nstatic const u32 emulated_msrs_all[] = {\n\tMSR_KVM_SYSTEM_TIME, MSR_KVM_WALL_CLOCK,\n\tMSR_KVM_SYSTEM_TIME_NEW, MSR_KVM_WALL_CLOCK_NEW,\n\tHV_X64_MSR_GUEST_OS_ID, HV_X64_MSR_HYPERCALL,\n\tHV_X64_MSR_TIME_REF_COUNT, HV_X64_MSR_REFERENCE_TSC,\n\tHV_X64_MSR_TSC_FREQUENCY, HV_X64_MSR_APIC_FREQUENCY,\n\tHV_X64_MSR_CRASH_P0, HV_X64_MSR_CRASH_P1, HV_X64_MSR_CRASH_P2,\n\tHV_X64_MSR_CRASH_P3, HV_X64_MSR_CRASH_P4, HV_X64_MSR_CRASH_CTL,\n\tHV_X64_MSR_RESET,\n\tHV_X64_MSR_VP_INDEX,\n\tHV_X64_MSR_VP_RUNTIME,\n\tHV_X64_MSR_SCONTROL,\n\tHV_X64_MSR_STIMER0_CONFIG,\n\tHV_X64_MSR_VP_ASSIST_PAGE,\n\tHV_X64_MSR_REENLIGHTENMENT_CONTROL, HV_X64_MSR_TSC_EMULATION_CONTROL,\n\tHV_X64_MSR_TSC_EMULATION_STATUS,\n\tHV_X64_MSR_SYNDBG_OPTIONS,\n\tHV_X64_MSR_SYNDBG_CONTROL, HV_X64_MSR_SYNDBG_STATUS,\n\tHV_X64_MSR_SYNDBG_SEND_BUFFER, HV_X64_MSR_SYNDBG_RECV_BUFFER,\n\tHV_X64_MSR_SYNDBG_PENDING_BUFFER,\n\n\tMSR_KVM_ASYNC_PF_EN, MSR_KVM_STEAL_TIME,\n\tMSR_KVM_PV_EOI_EN, MSR_KVM_ASYNC_PF_INT, MSR_KVM_ASYNC_PF_ACK,\n\n\tMSR_IA32_TSC_ADJUST,\n\tMSR_IA32_TSC_DEADLINE,\n\tMSR_IA32_ARCH_CAPABILITIES,\n\tMSR_IA32_PERF_CAPABILITIES,\n\tMSR_IA32_MISC_ENABLE,\n\tMSR_IA32_MCG_STATUS,\n\tMSR_IA32_MCG_CTL,\n\tMSR_IA32_MCG_EXT_CTL,\n\tMSR_IA32_SMBASE,\n\tMSR_SMI_COUNT,\n\tMSR_PLATFORM_INFO,\n\tMSR_MISC_FEATURES_ENABLES,\n\tMSR_AMD64_VIRT_SPEC_CTRL,\n\tMSR_AMD64_TSC_RATIO,\n\tMSR_IA32_POWER_CTL,\n\tMSR_IA32_UCODE_REV,\n\n\t/*\n\t * The following list leaves out MSRs whose values are determined\n\t * by arch/x86/kvm/vmx/nested.c based on CPUID or other MSRs.\n\t * We always support the \"true\" VMX control MSRs, even if the host\n\t * processor does not, so I am putting these registers here rather\n\t * than in msrs_to_save_all.\n\t */\n\tMSR_IA32_VMX_BASIC,\n\tMSR_IA32_VMX_TRUE_PINBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_PROCBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_EXIT_CTLS,\n\tMSR_IA32_VMX_TRUE_ENTRY_CTLS,\n\tMSR_IA32_VMX_MISC,\n\tMSR_IA32_VMX_CR0_FIXED0,\n\tMSR_IA32_VMX_CR4_FIXED0,\n\tMSR_IA32_VMX_VMCS_ENUM,\n\tMSR_IA32_VMX_PROCBASED_CTLS2,\n\tMSR_IA32_VMX_EPT_VPID_CAP,\n\tMSR_IA32_VMX_VMFUNC,\n\n\tMSR_K7_HWCR,\n\tMSR_KVM_POLL_CONTROL,\n};\n\nstatic u32 emulated_msrs[ARRAY_SIZE(emulated_msrs_all)];\nstatic unsigned num_emulated_msrs;\n\n/*\n * List of msr numbers which are used to expose MSR-based features that\n * can be used by a hypervisor to validate requested CPU features.\n */\nstatic const u32 msr_based_features_all[] = {\n\tMSR_IA32_VMX_BASIC,\n\tMSR_IA32_VMX_TRUE_PINBASED_CTLS,\n\tMSR_IA32_VMX_PINBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_PROCBASED_CTLS,\n\tMSR_IA32_VMX_PROCBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_EXIT_CTLS,\n\tMSR_IA32_VMX_EXIT_CTLS,\n\tMSR_IA32_VMX_TRUE_ENTRY_CTLS,\n\tMSR_IA32_VMX_ENTRY_CTLS,\n\tMSR_IA32_VMX_MISC,\n\tMSR_IA32_VMX_CR0_FIXED0,\n\tMSR_IA32_VMX_CR0_FIXED1,\n\tMSR_IA32_VMX_CR4_FIXED0,\n\tMSR_IA32_VMX_CR4_FIXED1,\n\tMSR_IA32_VMX_VMCS_ENUM,\n\tMSR_IA32_VMX_PROCBASED_CTLS2,\n\tMSR_IA32_VMX_EPT_VPID_CAP,\n\tMSR_IA32_VMX_VMFUNC,\n\n\tMSR_AMD64_DE_CFG,\n\tMSR_IA32_UCODE_REV,\n\tMSR_IA32_ARCH_CAPABILITIES,\n\tMSR_IA32_PERF_CAPABILITIES,\n};\n\nstatic u32 msr_based_features[ARRAY_SIZE(msr_based_features_all)];\nstatic unsigned int num_msr_based_features;\n\n/*\n * Some IA32_ARCH_CAPABILITIES bits have dependencies on MSRs that KVM\n * does not yet virtualize. These include:\n *   10 - MISC_PACKAGE_CTRLS\n *   11 - ENERGY_FILTERING_CTL\n *   12 - DOITM\n *   18 - FB_CLEAR_CTRL\n *   21 - XAPIC_DISABLE_STATUS\n *   23 - OVERCLOCKING_STATUS\n */\n\n#define KVM_SUPPORTED_ARCH_CAP \\\n\t(ARCH_CAP_RDCL_NO | ARCH_CAP_IBRS_ALL | ARCH_CAP_RSBA | \\\n\t ARCH_CAP_SKIP_VMENTRY_L1DFLUSH | ARCH_CAP_SSB_NO | ARCH_CAP_MDS_NO | \\\n\t ARCH_CAP_PSCHANGE_MC_NO | ARCH_CAP_TSX_CTRL_MSR | ARCH_CAP_TAA_NO | \\\n\t ARCH_CAP_SBDR_SSDP_NO | ARCH_CAP_FBSDP_NO | ARCH_CAP_PSDP_NO | \\\n\t ARCH_CAP_FB_CLEAR | ARCH_CAP_RRSBA | ARCH_CAP_PBRSB_NO)\n\nstatic u64 kvm_get_arch_capabilities(void)\n{\n\tu64 data = 0;\n\n\tif (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES)) {\n\t\trdmsrl(MSR_IA32_ARCH_CAPABILITIES, data);\n\t\tdata &= KVM_SUPPORTED_ARCH_CAP;\n\t}\n\n\t/*\n\t * If nx_huge_pages is enabled, KVM's shadow paging will ensure that\n\t * the nested hypervisor runs with NX huge pages.  If it is not,\n\t * L1 is anyway vulnerable to ITLB_MULTIHIT exploits from other\n\t * L1 guests, so it need not worry about its own (L2) guests.\n\t */\n\tdata |= ARCH_CAP_PSCHANGE_MC_NO;\n\n\t/*\n\t * If we're doing cache flushes (either \"always\" or \"cond\")\n\t * we will do one whenever the guest does a vmlaunch/vmresume.\n\t * If an outer hypervisor is doing the cache flush for us\n\t * (VMENTER_L1D_FLUSH_NESTED_VM), we can safely pass that\n\t * capability to the guest too, and if EPT is disabled we're not\n\t * vulnerable.  Overall, only VMENTER_L1D_FLUSH_NEVER will\n\t * require a nested hypervisor to do a flush of its own.\n\t */\n\tif (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)\n\t\tdata |= ARCH_CAP_SKIP_VMENTRY_L1DFLUSH;\n\n\tif (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))\n\t\tdata |= ARCH_CAP_RDCL_NO;\n\tif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\tdata |= ARCH_CAP_SSB_NO;\n\tif (!boot_cpu_has_bug(X86_BUG_MDS))\n\t\tdata |= ARCH_CAP_MDS_NO;\n\n\tif (!boot_cpu_has(X86_FEATURE_RTM)) {\n\t\t/*\n\t\t * If RTM=0 because the kernel has disabled TSX, the host might\n\t\t * have TAA_NO or TSX_CTRL.  Clear TAA_NO (the guest sees RTM=0\n\t\t * and therefore knows that there cannot be TAA) but keep\n\t\t * TSX_CTRL: some buggy userspaces leave it set on tsx=on hosts,\n\t\t * and we want to allow migrating those guests to tsx=off hosts.\n\t\t */\n\t\tdata &= ~ARCH_CAP_TAA_NO;\n\t} else if (!boot_cpu_has_bug(X86_BUG_TAA)) {\n\t\tdata |= ARCH_CAP_TAA_NO;\n\t} else {\n\t\t/*\n\t\t * Nothing to do here; we emulate TSX_CTRL if present on the\n\t\t * host so the guest can choose between disabling TSX or\n\t\t * using VERW to clear CPU buffers.\n\t\t */\n\t}\n\n\treturn data;\n}\n\nstatic int kvm_get_msr_feature(struct kvm_msr_entry *msr)\n{\n\tswitch (msr->index) {\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tmsr->data = kvm_get_arch_capabilities();\n\t\tbreak;\n\tcase MSR_IA32_PERF_CAPABILITIES:\n\t\tmsr->data = kvm_caps.supported_perf_cap;\n\t\tbreak;\n\tcase MSR_IA32_UCODE_REV:\n\t\trdmsrl_safe(msr->index, &msr->data);\n\t\tbreak;\n\tdefault:\n\t\treturn static_call(kvm_x86_get_msr_feature)(msr);\n\t}\n\treturn 0;\n}\n\nstatic int do_get_msr_feature(struct kvm_vcpu *vcpu, unsigned index, u64 *data)\n{\n\tstruct kvm_msr_entry msr;\n\tint r;\n\n\tmsr.index = index;\n\tr = kvm_get_msr_feature(&msr);\n\n\tif (r == KVM_MSR_RET_INVALID) {\n\t\t/* Unconditionally clear the output for simplicity */\n\t\t*data = 0;\n\t\tif (kvm_msr_ignored_check(index, 0, false))\n\t\t\tr = 0;\n\t}\n\n\tif (r)\n\t\treturn r;\n\n\t*data = msr.data;\n\n\treturn 0;\n}\n\nstatic bool __kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer)\n{\n\tif (efer & EFER_FFXSR && !guest_cpuid_has(vcpu, X86_FEATURE_FXSR_OPT))\n\t\treturn false;\n\n\tif (efer & EFER_SVME && !guest_cpuid_has(vcpu, X86_FEATURE_SVM))\n\t\treturn false;\n\n\tif (efer & (EFER_LME | EFER_LMA) &&\n\t    !guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\treturn false;\n\n\tif (efer & EFER_NX && !guest_cpuid_has(vcpu, X86_FEATURE_NX))\n\t\treturn false;\n\n\treturn true;\n\n}\nbool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer)\n{\n\tif (efer & efer_reserved_bits)\n\t\treturn false;\n\n\treturn __kvm_valid_efer(vcpu, efer);\n}\nEXPORT_SYMBOL_GPL(kvm_valid_efer);\n\nstatic int set_efer(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tu64 old_efer = vcpu->arch.efer;\n\tu64 efer = msr_info->data;\n\tint r;\n\n\tif (efer & efer_reserved_bits)\n\t\treturn 1;\n\n\tif (!msr_info->host_initiated) {\n\t\tif (!__kvm_valid_efer(vcpu, efer))\n\t\t\treturn 1;\n\n\t\tif (is_paging(vcpu) &&\n\t\t    (vcpu->arch.efer & EFER_LME) != (efer & EFER_LME))\n\t\t\treturn 1;\n\t}\n\n\tefer &= ~EFER_LMA;\n\tefer |= vcpu->arch.efer & EFER_LMA;\n\n\tr = static_call(kvm_x86_set_efer)(vcpu, efer);\n\tif (r) {\n\t\tWARN_ON(r > 0);\n\t\treturn r;\n\t}\n\n\tif ((efer ^ old_efer) & KVM_MMU_EFER_ROLE_BITS)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\treturn 0;\n}\n\nvoid kvm_enable_efer_bits(u64 mask)\n{\n       efer_reserved_bits &= ~mask;\n}\nEXPORT_SYMBOL_GPL(kvm_enable_efer_bits);\n\nbool kvm_msr_allowed(struct kvm_vcpu *vcpu, u32 index, u32 type)\n{\n\tstruct kvm_x86_msr_filter *msr_filter;\n\tstruct msr_bitmap_range *ranges;\n\tstruct kvm *kvm = vcpu->kvm;\n\tbool allowed;\n\tint idx;\n\tu32 i;\n\n\t/* x2APIC MSRs do not support filtering. */\n\tif (index >= 0x800 && index <= 0x8ff)\n\t\treturn true;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\n\tmsr_filter = srcu_dereference(kvm->arch.msr_filter, &kvm->srcu);\n\tif (!msr_filter) {\n\t\tallowed = true;\n\t\tgoto out;\n\t}\n\n\tallowed = msr_filter->default_allow;\n\tranges = msr_filter->ranges;\n\n\tfor (i = 0; i < msr_filter->count; i++) {\n\t\tu32 start = ranges[i].base;\n\t\tu32 end = start + ranges[i].nmsrs;\n\t\tu32 flags = ranges[i].flags;\n\t\tunsigned long *bitmap = ranges[i].bitmap;\n\n\t\tif ((index >= start) && (index < end) && (flags & type)) {\n\t\t\tallowed = !!test_bit(index - start, bitmap);\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn allowed;\n}\nEXPORT_SYMBOL_GPL(kvm_msr_allowed);\n\n/*\n * Write @data into the MSR specified by @index.  Select MSR specific fault\n * checks are bypassed if @host_initiated is %true.\n * Returns 0 on success, non-0 otherwise.\n * Assumes vcpu_load() was already called.\n */\nstatic int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,\n\t\t\t bool host_initiated)\n{\n\tstruct msr_data msr;\n\n\tswitch (index) {\n\tcase MSR_FS_BASE:\n\tcase MSR_GS_BASE:\n\tcase MSR_KERNEL_GS_BASE:\n\tcase MSR_CSTAR:\n\tcase MSR_LSTAR:\n\t\tif (is_noncanonical_address(data, vcpu))\n\t\t\treturn 1;\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\t/*\n\t\t * IA32_SYSENTER_ESP and IA32_SYSENTER_EIP cause #GP if\n\t\t * non-canonical address is written on Intel but not on\n\t\t * AMD (which ignores the top 32-bits, because it does\n\t\t * not implement 64-bit SYSENTER).\n\t\t *\n\t\t * 64-bit code should hence be able to write a non-canonical\n\t\t * value on AMD.  Making the address canonical ensures that\n\t\t * vmentry does not fail on Intel after writing a non-canonical\n\t\t * value, and that something deterministic happens if the guest\n\t\t * invokes 64-bit SYSENTER.\n\t\t */\n\t\tdata = __canonical_address(data, vcpu_virt_addr_bits(vcpu));\n\t\tbreak;\n\tcase MSR_TSC_AUX:\n\t\tif (!kvm_is_supported_user_return_msr(MSR_TSC_AUX))\n\t\t\treturn 1;\n\n\t\tif (!host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP) &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDPID))\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * Per Intel's SDM, bits 63:32 are reserved, but AMD's APM has\n\t\t * incomplete and conflicting architectural behavior.  Current\n\t\t * AMD CPUs completely ignore bits 63:32, i.e. they aren't\n\t\t * reserved and always read as zeros.  Enforce Intel's reserved\n\t\t * bits check if and only if the guest CPU is Intel, and clear\n\t\t * the bits in all other cases.  This ensures cross-vendor\n\t\t * migration will provide consistent behavior for the guest.\n\t\t */\n\t\tif (guest_cpuid_is_intel(vcpu) && (data >> 32) != 0)\n\t\t\treturn 1;\n\n\t\tdata = (u32)data;\n\t\tbreak;\n\t}\n\n\tmsr.data = data;\n\tmsr.index = index;\n\tmsr.host_initiated = host_initiated;\n\n\treturn static_call(kvm_x86_set_msr)(vcpu, &msr);\n}\n\nstatic int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,\n\t\t\t\t     u32 index, u64 data, bool host_initiated)\n{\n\tint ret = __kvm_set_msr(vcpu, index, data, host_initiated);\n\n\tif (ret == KVM_MSR_RET_INVALID)\n\t\tif (kvm_msr_ignored_check(index, data, true))\n\t\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * Read the MSR specified by @index into @data.  Select MSR specific fault\n * checks are bypassed if @host_initiated is %true.\n * Returns 0 on success, non-0 otherwise.\n * Assumes vcpu_load() was already called.\n */\nint __kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data,\n\t\t  bool host_initiated)\n{\n\tstruct msr_data msr;\n\tint ret;\n\n\tswitch (index) {\n\tcase MSR_TSC_AUX:\n\t\tif (!kvm_is_supported_user_return_msr(MSR_TSC_AUX))\n\t\t\treturn 1;\n\n\t\tif (!host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP) &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDPID))\n\t\t\treturn 1;\n\t\tbreak;\n\t}\n\n\tmsr.index = index;\n\tmsr.host_initiated = host_initiated;\n\n\tret = static_call(kvm_x86_get_msr)(vcpu, &msr);\n\tif (!ret)\n\t\t*data = msr.data;\n\treturn ret;\n}\n\nstatic int kvm_get_msr_ignored_check(struct kvm_vcpu *vcpu,\n\t\t\t\t     u32 index, u64 *data, bool host_initiated)\n{\n\tint ret = __kvm_get_msr(vcpu, index, data, host_initiated);\n\n\tif (ret == KVM_MSR_RET_INVALID) {\n\t\t/* Unconditionally clear *data for simplicity */\n\t\t*data = 0;\n\t\tif (kvm_msr_ignored_check(index, 0, false))\n\t\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\nstatic int kvm_get_msr_with_filter(struct kvm_vcpu *vcpu, u32 index, u64 *data)\n{\n\tif (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_READ))\n\t\treturn KVM_MSR_RET_FILTERED;\n\treturn kvm_get_msr_ignored_check(vcpu, index, data, false);\n}\n\nstatic int kvm_set_msr_with_filter(struct kvm_vcpu *vcpu, u32 index, u64 data)\n{\n\tif (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_WRITE))\n\t\treturn KVM_MSR_RET_FILTERED;\n\treturn kvm_set_msr_ignored_check(vcpu, index, data, false);\n}\n\nint kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data)\n{\n\treturn kvm_get_msr_ignored_check(vcpu, index, data, false);\n}\nEXPORT_SYMBOL_GPL(kvm_get_msr);\n\nint kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data)\n{\n\treturn kvm_set_msr_ignored_check(vcpu, index, data, false);\n}\nEXPORT_SYMBOL_GPL(kvm_set_msr);\n\nstatic void complete_userspace_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tif (!vcpu->run->msr.error) {\n\t\tkvm_rax_write(vcpu, (u32)vcpu->run->msr.data);\n\t\tkvm_rdx_write(vcpu, vcpu->run->msr.data >> 32);\n\t}\n}\n\nstatic int complete_emulated_msr_access(struct kvm_vcpu *vcpu)\n{\n\treturn complete_emulated_insn_gp(vcpu, vcpu->run->msr.error);\n}\n\nstatic int complete_emulated_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tcomplete_userspace_rdmsr(vcpu);\n\treturn complete_emulated_msr_access(vcpu);\n}\n\nstatic int complete_fast_msr_access(struct kvm_vcpu *vcpu)\n{\n\treturn static_call(kvm_x86_complete_emulated_msr)(vcpu, vcpu->run->msr.error);\n}\n\nstatic int complete_fast_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tcomplete_userspace_rdmsr(vcpu);\n\treturn complete_fast_msr_access(vcpu);\n}\n\nstatic u64 kvm_msr_reason(int r)\n{\n\tswitch (r) {\n\tcase KVM_MSR_RET_INVALID:\n\t\treturn KVM_MSR_EXIT_REASON_UNKNOWN;\n\tcase KVM_MSR_RET_FILTERED:\n\t\treturn KVM_MSR_EXIT_REASON_FILTER;\n\tdefault:\n\t\treturn KVM_MSR_EXIT_REASON_INVAL;\n\t}\n}\n\nstatic int kvm_msr_user_space(struct kvm_vcpu *vcpu, u32 index,\n\t\t\t      u32 exit_reason, u64 data,\n\t\t\t      int (*completion)(struct kvm_vcpu *vcpu),\n\t\t\t      int r)\n{\n\tu64 msr_reason = kvm_msr_reason(r);\n\n\t/* Check if the user wanted to know about this MSR fault */\n\tif (!(vcpu->kvm->arch.user_space_msr_mask & msr_reason))\n\t\treturn 0;\n\n\tvcpu->run->exit_reason = exit_reason;\n\tvcpu->run->msr.error = 0;\n\tmemset(vcpu->run->msr.pad, 0, sizeof(vcpu->run->msr.pad));\n\tvcpu->run->msr.reason = msr_reason;\n\tvcpu->run->msr.index = index;\n\tvcpu->run->msr.data = data;\n\tvcpu->arch.complete_userspace_io = completion;\n\n\treturn 1;\n}\n\nint kvm_emulate_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = kvm_rcx_read(vcpu);\n\tu64 data;\n\tint r;\n\n\tr = kvm_get_msr_with_filter(vcpu, ecx, &data);\n\n\tif (!r) {\n\t\ttrace_kvm_msr_read(ecx, data);\n\n\t\tkvm_rax_write(vcpu, data & -1u);\n\t\tkvm_rdx_write(vcpu, (data >> 32) & -1u);\n\t} else {\n\t\t/* MSR read failed? See if we should ask user space */\n\t\tif (kvm_msr_user_space(vcpu, ecx, KVM_EXIT_X86_RDMSR, 0,\n\t\t\t\t       complete_fast_rdmsr, r))\n\t\t\treturn 0;\n\t\ttrace_kvm_msr_read_ex(ecx);\n\t}\n\n\treturn static_call(kvm_x86_complete_emulated_msr)(vcpu, r);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_rdmsr);\n\nint kvm_emulate_wrmsr(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = kvm_rcx_read(vcpu);\n\tu64 data = kvm_read_edx_eax(vcpu);\n\tint r;\n\n\tr = kvm_set_msr_with_filter(vcpu, ecx, data);\n\n\tif (!r) {\n\t\ttrace_kvm_msr_write(ecx, data);\n\t} else {\n\t\t/* MSR write failed? See if we should ask user space */\n\t\tif (kvm_msr_user_space(vcpu, ecx, KVM_EXIT_X86_WRMSR, data,\n\t\t\t\t       complete_fast_msr_access, r))\n\t\t\treturn 0;\n\t\t/* Signal all other negative errors to userspace */\n\t\tif (r < 0)\n\t\t\treturn r;\n\t\ttrace_kvm_msr_write_ex(ecx, data);\n\t}\n\n\treturn static_call(kvm_x86_complete_emulated_msr)(vcpu, r);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_wrmsr);\n\nint kvm_emulate_as_nop(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nint kvm_emulate_invd(struct kvm_vcpu *vcpu)\n{\n\t/* Treat an INVD instruction as a NOP and just skip it. */\n\treturn kvm_emulate_as_nop(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_invd);\n\nint kvm_handle_invalid_op(struct kvm_vcpu *vcpu)\n{\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(kvm_handle_invalid_op);\n\n\nstatic int kvm_emulate_monitor_mwait(struct kvm_vcpu *vcpu, const char *insn)\n{\n\tif (!kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_MWAIT_NEVER_UD_FAULTS) &&\n\t    !guest_cpuid_has(vcpu, X86_FEATURE_MWAIT))\n\t\treturn kvm_handle_invalid_op(vcpu);\n\n\tpr_warn_once(\"kvm: %s instruction emulated as NOP!\\n\", insn);\n\treturn kvm_emulate_as_nop(vcpu);\n}\nint kvm_emulate_mwait(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_monitor_mwait(vcpu, \"MWAIT\");\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_mwait);\n\nint kvm_emulate_monitor(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_monitor_mwait(vcpu, \"MONITOR\");\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_monitor);\n\nstatic inline bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)\n{\n\txfer_to_guest_mode_prepare();\n\treturn vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||\n\t\txfer_to_guest_mode_work_pending();\n}\n\n/*\n * The fast path for frequent and performance sensitive wrmsr emulation,\n * i.e. the sending of IPI, sending IPI early in the VM-Exit flow reduces\n * the latency of virtual IPI by avoiding the expensive bits of transitioning\n * from guest to host, e.g. reacquiring KVM's SRCU lock. In contrast to the\n * other cases which must be called after interrupts are enabled on the host.\n */\nstatic int handle_fastpath_set_x2apic_icr_irqoff(struct kvm_vcpu *vcpu, u64 data)\n{\n\tif (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(vcpu->arch.apic))\n\t\treturn 1;\n\n\tif (((data & APIC_SHORT_MASK) == APIC_DEST_NOSHORT) &&\n\t    ((data & APIC_DEST_MASK) == APIC_DEST_PHYSICAL) &&\n\t    ((data & APIC_MODE_MASK) == APIC_DM_FIXED) &&\n\t    ((u32)(data >> 32) != X2APIC_BROADCAST))\n\t\treturn kvm_x2apic_icr_write(vcpu->arch.apic, data);\n\n\treturn 1;\n}\n\nstatic int handle_fastpath_set_tscdeadline(struct kvm_vcpu *vcpu, u64 data)\n{\n\tif (!kvm_can_use_hv_timer(vcpu))\n\t\treturn 1;\n\n\tkvm_set_lapic_tscdeadline_msr(vcpu, data);\n\treturn 0;\n}\n\nfastpath_t handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu)\n{\n\tu32 msr = kvm_rcx_read(vcpu);\n\tu64 data;\n\tfastpath_t ret = EXIT_FASTPATH_NONE;\n\n\tswitch (msr) {\n\tcase APIC_BASE_MSR + (APIC_ICR >> 4):\n\t\tdata = kvm_read_edx_eax(vcpu);\n\t\tif (!handle_fastpath_set_x2apic_icr_irqoff(vcpu, data)) {\n\t\t\tkvm_skip_emulated_instruction(vcpu);\n\t\t\tret = EXIT_FASTPATH_EXIT_HANDLED;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_TSC_DEADLINE:\n\t\tdata = kvm_read_edx_eax(vcpu);\n\t\tif (!handle_fastpath_set_tscdeadline(vcpu, data)) {\n\t\t\tkvm_skip_emulated_instruction(vcpu);\n\t\t\tret = EXIT_FASTPATH_REENTER_GUEST;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (ret != EXIT_FASTPATH_NONE)\n\t\ttrace_kvm_msr_write(msr, data);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(handle_fastpath_set_msr_irqoff);\n\n/*\n * Adapt set_msr() to msr_io()'s calling convention\n */\nstatic int do_get_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)\n{\n\treturn kvm_get_msr_ignored_check(vcpu, index, data, true);\n}\n\nstatic int do_set_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)\n{\n\treturn kvm_set_msr_ignored_check(vcpu, index, *data, true);\n}\n\n#ifdef CONFIG_X86_64\nstruct pvclock_clock {\n\tint vclock_mode;\n\tu64 cycle_last;\n\tu64 mask;\n\tu32 mult;\n\tu32 shift;\n\tu64 base_cycles;\n\tu64 offset;\n};\n\nstruct pvclock_gtod_data {\n\tseqcount_t\tseq;\n\n\tstruct pvclock_clock clock; /* extract of a clocksource struct */\n\tstruct pvclock_clock raw_clock; /* extract of a clocksource struct */\n\n\tktime_t\t\toffs_boot;\n\tu64\t\twall_time_sec;\n};\n\nstatic struct pvclock_gtod_data pvclock_gtod_data;\n\nstatic void update_pvclock_gtod(struct timekeeper *tk)\n{\n\tstruct pvclock_gtod_data *vdata = &pvclock_gtod_data;\n\n\twrite_seqcount_begin(&vdata->seq);\n\n\t/* copy pvclock gtod data */\n\tvdata->clock.vclock_mode\t= tk->tkr_mono.clock->vdso_clock_mode;\n\tvdata->clock.cycle_last\t\t= tk->tkr_mono.cycle_last;\n\tvdata->clock.mask\t\t= tk->tkr_mono.mask;\n\tvdata->clock.mult\t\t= tk->tkr_mono.mult;\n\tvdata->clock.shift\t\t= tk->tkr_mono.shift;\n\tvdata->clock.base_cycles\t= tk->tkr_mono.xtime_nsec;\n\tvdata->clock.offset\t\t= tk->tkr_mono.base;\n\n\tvdata->raw_clock.vclock_mode\t= tk->tkr_raw.clock->vdso_clock_mode;\n\tvdata->raw_clock.cycle_last\t= tk->tkr_raw.cycle_last;\n\tvdata->raw_clock.mask\t\t= tk->tkr_raw.mask;\n\tvdata->raw_clock.mult\t\t= tk->tkr_raw.mult;\n\tvdata->raw_clock.shift\t\t= tk->tkr_raw.shift;\n\tvdata->raw_clock.base_cycles\t= tk->tkr_raw.xtime_nsec;\n\tvdata->raw_clock.offset\t\t= tk->tkr_raw.base;\n\n\tvdata->wall_time_sec            = tk->xtime_sec;\n\n\tvdata->offs_boot\t\t= tk->offs_boot;\n\n\twrite_seqcount_end(&vdata->seq);\n}\n\nstatic s64 get_kvmclock_base_ns(void)\n{\n\t/* Count up from boot time, but with the frequency of the raw clock.  */\n\treturn ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));\n}\n#else\nstatic s64 get_kvmclock_base_ns(void)\n{\n\t/* Master clock not used, so we can just use CLOCK_BOOTTIME.  */\n\treturn ktime_get_boottime_ns();\n}\n#endif\n\nstatic void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)\n{\n\tint version;\n\tint r;\n\tstruct pvclock_wall_clock wc;\n\tu32 wc_sec_hi;\n\tu64 wall_nsec;\n\n\tif (!wall_clock)\n\t\treturn;\n\n\tr = kvm_read_guest(kvm, wall_clock, &version, sizeof(version));\n\tif (r)\n\t\treturn;\n\n\tif (version & 1)\n\t\t++version;  /* first time write, random junk */\n\n\t++version;\n\n\tif (kvm_write_guest(kvm, wall_clock, &version, sizeof(version)))\n\t\treturn;\n\n\t/*\n\t * The guest calculates current wall clock time by adding\n\t * system time (updated by kvm_guest_time_update below) to the\n\t * wall clock specified here.  We do the reverse here.\n\t */\n\twall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);\n\n\twc.nsec = do_div(wall_nsec, 1000000000);\n\twc.sec = (u32)wall_nsec; /* overflow in 2106 guest time */\n\twc.version = version;\n\n\tkvm_write_guest(kvm, wall_clock, &wc, sizeof(wc));\n\n\tif (sec_hi_ofs) {\n\t\twc_sec_hi = wall_nsec >> 32;\n\t\tkvm_write_guest(kvm, wall_clock + sec_hi_ofs,\n\t\t\t\t&wc_sec_hi, sizeof(wc_sec_hi));\n\t}\n\n\tversion++;\n\tkvm_write_guest(kvm, wall_clock, &version, sizeof(version));\n}\n\nstatic void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,\n\t\t\t\t  bool old_msr, bool host_initiated)\n{\n\tstruct kvm_arch *ka = &vcpu->kvm->arch;\n\n\tif (vcpu->vcpu_id == 0 && !host_initiated) {\n\t\tif (ka->boot_vcpu_runs_old_kvmclock != old_msr)\n\t\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\n\t\tka->boot_vcpu_runs_old_kvmclock = old_msr;\n\t}\n\n\tvcpu->arch.time = system_time;\n\tkvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);\n\n\t/* we verify if the enable bit is set... */\n\tif (system_time & 1)\n\t\tkvm_gpc_activate(&vcpu->arch.pv_time, system_time & ~1ULL,\n\t\t\t\t sizeof(struct pvclock_vcpu_time_info));\n\telse\n\t\tkvm_gpc_deactivate(&vcpu->arch.pv_time);\n\n\treturn;\n}\n\nstatic uint32_t div_frac(uint32_t dividend, uint32_t divisor)\n{\n\tdo_shl32_div32(dividend, divisor);\n\treturn dividend;\n}\n\nstatic void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,\n\t\t\t       s8 *pshift, u32 *pmultiplier)\n{\n\tuint64_t scaled64;\n\tint32_t  shift = 0;\n\tuint64_t tps64;\n\tuint32_t tps32;\n\n\ttps64 = base_hz;\n\tscaled64 = scaled_hz;\n\twhile (tps64 > scaled64*2 || tps64 & 0xffffffff00000000ULL) {\n\t\ttps64 >>= 1;\n\t\tshift--;\n\t}\n\n\ttps32 = (uint32_t)tps64;\n\twhile (tps32 <= scaled64 || scaled64 & 0xffffffff00000000ULL) {\n\t\tif (scaled64 & 0xffffffff00000000ULL || tps32 & 0x80000000)\n\t\t\tscaled64 >>= 1;\n\t\telse\n\t\t\ttps32 <<= 1;\n\t\tshift++;\n\t}\n\n\t*pshift = shift;\n\t*pmultiplier = div_frac(scaled64, tps32);\n}\n\n#ifdef CONFIG_X86_64\nstatic atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);\n#endif\n\nstatic DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);\nstatic unsigned long max_tsc_khz;\n\nstatic u32 adjust_tsc_khz(u32 khz, s32 ppm)\n{\n\tu64 v = (u64)khz * (1000000 + ppm);\n\tdo_div(v, 1000000);\n\treturn v;\n}\n\nstatic void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier);\n\nstatic int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)\n{\n\tu64 ratio;\n\n\t/* Guest TSC same frequency as host TSC? */\n\tif (!scale) {\n\t\tkvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);\n\t\treturn 0;\n\t}\n\n\t/* TSC scaling supported? */\n\tif (!kvm_caps.has_tsc_control) {\n\t\tif (user_tsc_khz > tsc_khz) {\n\t\t\tvcpu->arch.tsc_catchup = 1;\n\t\t\tvcpu->arch.tsc_always_catchup = 1;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tpr_warn_ratelimited(\"user requested TSC rate below hardware speed\\n\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\t/* TSC scaling required  - calculate ratio */\n\tratio = mul_u64_u32_div(1ULL << kvm_caps.tsc_scaling_ratio_frac_bits,\n\t\t\t\tuser_tsc_khz, tsc_khz);\n\n\tif (ratio == 0 || ratio >= kvm_caps.max_tsc_scaling_ratio) {\n\t\tpr_warn_ratelimited(\"Invalid TSC scaling ratio - virtual-tsc-khz=%u\\n\",\n\t\t\t            user_tsc_khz);\n\t\treturn -1;\n\t}\n\n\tkvm_vcpu_write_tsc_multiplier(vcpu, ratio);\n\treturn 0;\n}\n\nstatic int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)\n{\n\tu32 thresh_lo, thresh_hi;\n\tint use_scaling = 0;\n\n\t/* tsc_khz can be zero if TSC calibration fails */\n\tif (user_tsc_khz == 0) {\n\t\t/* set tsc_scaling_ratio to a safe value */\n\t\tkvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);\n\t\treturn -1;\n\t}\n\n\t/* Compute a scale to convert nanoseconds in TSC cycles */\n\tkvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,\n\t\t\t   &vcpu->arch.virtual_tsc_shift,\n\t\t\t   &vcpu->arch.virtual_tsc_mult);\n\tvcpu->arch.virtual_tsc_khz = user_tsc_khz;\n\n\t/*\n\t * Compute the variation in TSC rate which is acceptable\n\t * within the range of tolerance and decide if the\n\t * rate being applied is within that bounds of the hardware\n\t * rate.  If so, no scaling or compensation need be done.\n\t */\n\tthresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);\n\tthresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);\n\tif (user_tsc_khz < thresh_lo || user_tsc_khz > thresh_hi) {\n\t\tpr_debug(\"kvm: requested TSC rate %u falls outside tolerance [%u,%u]\\n\", user_tsc_khz, thresh_lo, thresh_hi);\n\t\tuse_scaling = 1;\n\t}\n\treturn set_tsc_khz(vcpu, user_tsc_khz, use_scaling);\n}\n\nstatic u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)\n{\n\tu64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,\n\t\t\t\t      vcpu->arch.virtual_tsc_mult,\n\t\t\t\t      vcpu->arch.virtual_tsc_shift);\n\ttsc += vcpu->arch.this_tsc_write;\n\treturn tsc;\n}\n\n#ifdef CONFIG_X86_64\nstatic inline int gtod_is_based_on_tsc(int mode)\n{\n\treturn mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;\n}\n#endif\n\nstatic void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)\n{\n#ifdef CONFIG_X86_64\n\tbool vcpus_matched;\n\tstruct kvm_arch *ka = &vcpu->kvm->arch;\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\n\tvcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==\n\t\t\t atomic_read(&vcpu->kvm->online_vcpus));\n\n\t/*\n\t * Once the masterclock is enabled, always perform request in\n\t * order to update it.\n\t *\n\t * In order to enable masterclock, the host clocksource must be TSC\n\t * and the vcpus need to have matched TSCs.  When that happens,\n\t * perform request to enable masterclock.\n\t */\n\tif (ka->use_master_clock ||\n\t    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))\n\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\n\ttrace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,\n\t\t\t    atomic_read(&vcpu->kvm->online_vcpus),\n\t\t            ka->use_master_clock, gtod->clock.vclock_mode);\n#endif\n}\n\n/*\n * Multiply tsc by a fixed point number represented by ratio.\n *\n * The most significant 64-N bits (mult) of ratio represent the\n * integral part of the fixed point number; the remaining N bits\n * (frac) represent the fractional part, ie. ratio represents a fixed\n * point number (mult + frac * 2^(-N)).\n *\n * N equals to kvm_caps.tsc_scaling_ratio_frac_bits.\n */\nstatic inline u64 __scale_tsc(u64 ratio, u64 tsc)\n{\n\treturn mul_u64_u64_shr(tsc, ratio, kvm_caps.tsc_scaling_ratio_frac_bits);\n}\n\nu64 kvm_scale_tsc(u64 tsc, u64 ratio)\n{\n\tu64 _tsc = tsc;\n\n\tif (ratio != kvm_caps.default_tsc_scaling_ratio)\n\t\t_tsc = __scale_tsc(ratio, tsc);\n\n\treturn _tsc;\n}\n\nstatic u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)\n{\n\tu64 tsc;\n\n\ttsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio);\n\n\treturn target_tsc - tsc;\n}\n\nu64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)\n{\n\treturn vcpu->arch.l1_tsc_offset +\n\t\tkvm_scale_tsc(host_tsc, vcpu->arch.l1_tsc_scaling_ratio);\n}\nEXPORT_SYMBOL_GPL(kvm_read_l1_tsc);\n\nu64 kvm_calc_nested_tsc_offset(u64 l1_offset, u64 l2_offset, u64 l2_multiplier)\n{\n\tu64 nested_offset;\n\n\tif (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)\n\t\tnested_offset = l1_offset;\n\telse\n\t\tnested_offset = mul_s64_u64_shr((s64) l1_offset, l2_multiplier,\n\t\t\t\t\t\tkvm_caps.tsc_scaling_ratio_frac_bits);\n\n\tnested_offset += l2_offset;\n\treturn nested_offset;\n}\nEXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_offset);\n\nu64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)\n{\n\tif (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)\n\t\treturn mul_u64_u64_shr(l1_multiplier, l2_multiplier,\n\t\t\t\t       kvm_caps.tsc_scaling_ratio_frac_bits);\n\n\treturn l1_multiplier;\n}\nEXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_multiplier);\n\nstatic void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)\n{\n\ttrace_kvm_write_tsc_offset(vcpu->vcpu_id,\n\t\t\t\t   vcpu->arch.l1_tsc_offset,\n\t\t\t\t   l1_offset);\n\n\tvcpu->arch.l1_tsc_offset = l1_offset;\n\n\t/*\n\t * If we are here because L1 chose not to trap WRMSR to TSC then\n\t * according to the spec this should set L1's TSC (as opposed to\n\t * setting L1's offset for L2).\n\t */\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(\n\t\t\tl1_offset,\n\t\t\tstatic_call(kvm_x86_get_l2_tsc_offset)(vcpu),\n\t\t\tstatic_call(kvm_x86_get_l2_tsc_multiplier)(vcpu));\n\telse\n\t\tvcpu->arch.tsc_offset = l1_offset;\n\n\tstatic_call(kvm_x86_write_tsc_offset)(vcpu, vcpu->arch.tsc_offset);\n}\n\nstatic void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier)\n{\n\tvcpu->arch.l1_tsc_scaling_ratio = l1_multiplier;\n\n\t/* Userspace is changing the multiplier while L2 is active */\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.tsc_scaling_ratio = kvm_calc_nested_tsc_multiplier(\n\t\t\tl1_multiplier,\n\t\t\tstatic_call(kvm_x86_get_l2_tsc_multiplier)(vcpu));\n\telse\n\t\tvcpu->arch.tsc_scaling_ratio = l1_multiplier;\n\n\tif (kvm_caps.has_tsc_control)\n\t\tstatic_call(kvm_x86_write_tsc_multiplier)(\n\t\t\tvcpu, vcpu->arch.tsc_scaling_ratio);\n}\n\nstatic inline bool kvm_check_tsc_unstable(void)\n{\n#ifdef CONFIG_X86_64\n\t/*\n\t * TSC is marked unstable when we're running on Hyper-V,\n\t * 'TSC page' clocksource is good.\n\t */\n\tif (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)\n\t\treturn false;\n#endif\n\treturn check_tsc_unstable();\n}\n\n/*\n * Infers attempts to synchronize the guest's tsc from host writes. Sets the\n * offset for the vcpu and tracks the TSC matching generation that the vcpu\n * participates in.\n */\nstatic void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,\n\t\t\t\t  u64 ns, bool matched)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\n\tlockdep_assert_held(&kvm->arch.tsc_write_lock);\n\n\t/*\n\t * We also track th most recent recorded KHZ, write and time to\n\t * allow the matching interval to be extended at each write.\n\t */\n\tkvm->arch.last_tsc_nsec = ns;\n\tkvm->arch.last_tsc_write = tsc;\n\tkvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;\n\tkvm->arch.last_tsc_offset = offset;\n\n\tvcpu->arch.last_guest_tsc = tsc;\n\n\tkvm_vcpu_write_tsc_offset(vcpu, offset);\n\n\tif (!matched) {\n\t\t/*\n\t\t * We split periods of matched TSC writes into generations.\n\t\t * For each generation, we track the original measured\n\t\t * nanosecond time, offset, and write, so if TSCs are in\n\t\t * sync, we can match exact offset, and if not, we can match\n\t\t * exact software computation in compute_guest_tsc()\n\t\t *\n\t\t * These values are tracked in kvm->arch.cur_xxx variables.\n\t\t */\n\t\tkvm->arch.cur_tsc_generation++;\n\t\tkvm->arch.cur_tsc_nsec = ns;\n\t\tkvm->arch.cur_tsc_write = tsc;\n\t\tkvm->arch.cur_tsc_offset = offset;\n\t\tkvm->arch.nr_vcpus_matched_tsc = 0;\n\t} else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {\n\t\tkvm->arch.nr_vcpus_matched_tsc++;\n\t}\n\n\t/* Keep track of which generation this VCPU has synchronized to */\n\tvcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;\n\tvcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;\n\tvcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;\n\n\tkvm_track_tsc_matching(vcpu);\n}\n\nstatic void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\tu64 offset, ns, elapsed;\n\tunsigned long flags;\n\tbool matched = false;\n\tbool synchronizing = false;\n\n\traw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);\n\toffset = kvm_compute_l1_tsc_offset(vcpu, data);\n\tns = get_kvmclock_base_ns();\n\telapsed = ns - kvm->arch.last_tsc_nsec;\n\n\tif (vcpu->arch.virtual_tsc_khz) {\n\t\tif (data == 0) {\n\t\t\t/*\n\t\t\t * detection of vcpu initialization -- need to sync\n\t\t\t * with other vCPUs. This particularly helps to keep\n\t\t\t * kvm_clock stable after CPU hotplug\n\t\t\t */\n\t\t\tsynchronizing = true;\n\t\t} else {\n\t\t\tu64 tsc_exp = kvm->arch.last_tsc_write +\n\t\t\t\t\t\tnsec_to_cycles(vcpu, elapsed);\n\t\t\tu64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;\n\t\t\t/*\n\t\t\t * Special case: TSC write with a small delta (1 second)\n\t\t\t * of virtual cycle time against real time is\n\t\t\t * interpreted as an attempt to synchronize the CPU.\n\t\t\t */\n\t\t\tsynchronizing = data < tsc_exp + tsc_hz &&\n\t\t\t\t\tdata + tsc_hz > tsc_exp;\n\t\t}\n\t}\n\n\t/*\n\t * For a reliable TSC, we can match TSC offsets, and for an unstable\n\t * TSC, we add elapsed time in this computation.  We could let the\n\t * compensation code attempt to catch up if we fall behind, but\n\t * it's better to try to match offsets from the beginning.\n         */\n\tif (synchronizing &&\n\t    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {\n\t\tif (!kvm_check_tsc_unstable()) {\n\t\t\toffset = kvm->arch.cur_tsc_offset;\n\t\t} else {\n\t\t\tu64 delta = nsec_to_cycles(vcpu, elapsed);\n\t\t\tdata += delta;\n\t\t\toffset = kvm_compute_l1_tsc_offset(vcpu, data);\n\t\t}\n\t\tmatched = true;\n\t}\n\n\t__kvm_synchronize_tsc(vcpu, offset, data, ns, matched);\n\traw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);\n}\n\nstatic inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   s64 adjustment)\n{\n\tu64 tsc_offset = vcpu->arch.l1_tsc_offset;\n\tkvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);\n}\n\nstatic inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)\n{\n\tif (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)\n\t\tWARN_ON(adjustment < 0);\n\tadjustment = kvm_scale_tsc((u64) adjustment,\n\t\t\t\t   vcpu->arch.l1_tsc_scaling_ratio);\n\tadjust_tsc_offset_guest(vcpu, adjustment);\n}\n\n#ifdef CONFIG_X86_64\n\nstatic u64 read_tsc(void)\n{\n\tu64 ret = (u64)rdtsc_ordered();\n\tu64 last = pvclock_gtod_data.clock.cycle_last;\n\n\tif (likely(ret >= last))\n\t\treturn ret;\n\n\t/*\n\t * GCC likes to generate cmov here, but this branch is extremely\n\t * predictable (it's just a function of time and the likely is\n\t * very likely) and there's a data dependence, so force GCC\n\t * to generate a branch instead.  I don't barrier() because\n\t * we don't actually need a barrier, and if this function\n\t * ever gets inlined it will generate worse code.\n\t */\n\tasm volatile (\"\");\n\treturn last;\n}\n\nstatic inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,\n\t\t\t  int *mode)\n{\n\tlong v;\n\tu64 tsc_pg_val;\n\n\tswitch (clock->vclock_mode) {\n\tcase VDSO_CLOCKMODE_HVCLOCK:\n\t\ttsc_pg_val = hv_read_tsc_page_tsc(hv_get_tsc_page(),\n\t\t\t\t\t\t  tsc_timestamp);\n\t\tif (tsc_pg_val != U64_MAX) {\n\t\t\t/* TSC page valid */\n\t\t\t*mode = VDSO_CLOCKMODE_HVCLOCK;\n\t\t\tv = (tsc_pg_val - clock->cycle_last) &\n\t\t\t\tclock->mask;\n\t\t} else {\n\t\t\t/* TSC page invalid */\n\t\t\t*mode = VDSO_CLOCKMODE_NONE;\n\t\t}\n\t\tbreak;\n\tcase VDSO_CLOCKMODE_TSC:\n\t\t*mode = VDSO_CLOCKMODE_TSC;\n\t\t*tsc_timestamp = read_tsc();\n\t\tv = (*tsc_timestamp - clock->cycle_last) &\n\t\t\tclock->mask;\n\t\tbreak;\n\tdefault:\n\t\t*mode = VDSO_CLOCKMODE_NONE;\n\t}\n\n\tif (*mode == VDSO_CLOCKMODE_NONE)\n\t\t*tsc_timestamp = v = 0;\n\n\treturn v * clock->mult;\n}\n\nstatic int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)\n{\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\tunsigned long seq;\n\tint mode;\n\tu64 ns;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&gtod->seq);\n\t\tns = gtod->raw_clock.base_cycles;\n\t\tns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);\n\t\tns >>= gtod->raw_clock.shift;\n\t\tns += ktime_to_ns(ktime_add(gtod->raw_clock.offset, gtod->offs_boot));\n\t} while (unlikely(read_seqcount_retry(&gtod->seq, seq)));\n\t*t = ns;\n\n\treturn mode;\n}\n\nstatic int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)\n{\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\tunsigned long seq;\n\tint mode;\n\tu64 ns;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&gtod->seq);\n\t\tts->tv_sec = gtod->wall_time_sec;\n\t\tns = gtod->clock.base_cycles;\n\t\tns += vgettsc(&gtod->clock, tsc_timestamp, &mode);\n\t\tns >>= gtod->clock.shift;\n\t} while (unlikely(read_seqcount_retry(&gtod->seq, seq)));\n\n\tts->tv_sec += __iter_div_u64_rem(ns, NSEC_PER_SEC, &ns);\n\tts->tv_nsec = ns;\n\n\treturn mode;\n}\n\n/* returns true if host is using TSC based clocksource */\nstatic bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)\n{\n\t/* checked again under seqlock below */\n\tif (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))\n\t\treturn false;\n\n\treturn gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,\n\t\t\t\t\t\t      tsc_timestamp));\n}\n\n/* returns true if host is using TSC based clocksource */\nstatic bool kvm_get_walltime_and_clockread(struct timespec64 *ts,\n\t\t\t\t\t   u64 *tsc_timestamp)\n{\n\t/* checked again under seqlock below */\n\tif (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))\n\t\treturn false;\n\n\treturn gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));\n}\n#endif\n\n/*\n *\n * Assuming a stable TSC across physical CPUS, and a stable TSC\n * across virtual CPUs, the following condition is possible.\n * Each numbered line represents an event visible to both\n * CPUs at the next numbered event.\n *\n * \"timespecX\" represents host monotonic time. \"tscX\" represents\n * RDTSC value.\n *\n * \t\tVCPU0 on CPU0\t\t|\tVCPU1 on CPU1\n *\n * 1.  read timespec0,tsc0\n * 2.\t\t\t\t\t| timespec1 = timespec0 + N\n * \t\t\t\t\t| tsc1 = tsc0 + M\n * 3. transition to guest\t\t| transition to guest\n * 4. ret0 = timespec0 + (rdtsc - tsc0) |\n * 5.\t\t\t\t        | ret1 = timespec1 + (rdtsc - tsc1)\n * \t\t\t\t        | ret1 = timespec0 + N + (rdtsc - (tsc0 + M))\n *\n * Since ret0 update is visible to VCPU1 at time 5, to obey monotonicity:\n *\n * \t- ret0 < ret1\n *\t- timespec0 + (rdtsc - tsc0) < timespec0 + N + (rdtsc - (tsc0 + M))\n *\t\t...\n *\t- 0 < N - M => M < N\n *\n * That is, when timespec0 != timespec1, M < N. Unfortunately that is not\n * always the case (the difference between two distinct xtime instances\n * might be smaller then the difference between corresponding TSC reads,\n * when updating guest vcpus pvclock areas).\n *\n * To avoid that problem, do not allow visibility of distinct\n * system_timestamp/tsc_timestamp values simultaneously: use a master\n * copy of host monotonic time values. Update that master copy\n * in lockstep.\n *\n * Rely on synchronization of host TSCs and guest TSCs for monotonicity.\n *\n */\n\nstatic void pvclock_update_vm_gtod_copy(struct kvm *kvm)\n{\n#ifdef CONFIG_X86_64\n\tstruct kvm_arch *ka = &kvm->arch;\n\tint vclock_mode;\n\tbool host_tsc_clocksource, vcpus_matched;\n\n\tlockdep_assert_held(&kvm->arch.tsc_write_lock);\n\tvcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==\n\t\t\tatomic_read(&kvm->online_vcpus));\n\n\t/*\n\t * If the host uses TSC clock, then passthrough TSC as stable\n\t * to the guest.\n\t */\n\thost_tsc_clocksource = kvm_get_time_and_clockread(\n\t\t\t\t\t&ka->master_kernel_ns,\n\t\t\t\t\t&ka->master_cycle_now);\n\n\tka->use_master_clock = host_tsc_clocksource && vcpus_matched\n\t\t\t\t&& !ka->backwards_tsc_observed\n\t\t\t\t&& !ka->boot_vcpu_runs_old_kvmclock;\n\n\tif (ka->use_master_clock)\n\t\tatomic_set(&kvm_guest_has_master_clock, 1);\n\n\tvclock_mode = pvclock_gtod_data.clock.vclock_mode;\n\ttrace_kvm_update_master_clock(ka->use_master_clock, vclock_mode,\n\t\t\t\t\tvcpus_matched);\n#endif\n}\n\nstatic void kvm_make_mclock_inprogress_request(struct kvm *kvm)\n{\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);\n}\n\nstatic void __kvm_start_pvclock_update(struct kvm *kvm)\n{\n\traw_spin_lock_irq(&kvm->arch.tsc_write_lock);\n\twrite_seqcount_begin(&kvm->arch.pvclock_sc);\n}\n\nstatic void kvm_start_pvclock_update(struct kvm *kvm)\n{\n\tkvm_make_mclock_inprogress_request(kvm);\n\n\t/* no guest entries from this point */\n\t__kvm_start_pvclock_update(kvm);\n}\n\nstatic void kvm_end_pvclock_update(struct kvm *kvm)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\n\twrite_seqcount_end(&ka->pvclock_sc);\n\traw_spin_unlock_irq(&ka->tsc_write_lock);\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\n\t/* guest entries allowed */\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);\n}\n\nstatic void kvm_update_masterclock(struct kvm *kvm)\n{\n\tkvm_hv_request_tsc_page_update(kvm);\n\tkvm_start_pvclock_update(kvm);\n\tpvclock_update_vm_gtod_copy(kvm);\n\tkvm_end_pvclock_update(kvm);\n}\n\n/*\n * Use the kernel's tsc_khz directly if the TSC is constant, otherwise use KVM's\n * per-CPU value (which may be zero if a CPU is going offline).  Note, tsc_khz\n * can change during boot even if the TSC is constant, as it's possible for KVM\n * to be loaded before TSC calibration completes.  Ideally, KVM would get a\n * notification when calibration completes, but practically speaking calibration\n * will complete before userspace is alive enough to create VMs.\n */\nstatic unsigned long get_cpu_tsc_khz(void)\n{\n\tif (static_cpu_has(X86_FEATURE_CONSTANT_TSC))\n\t\treturn tsc_khz;\n\telse\n\t\treturn __this_cpu_read(cpu_tsc_khz);\n}\n\n/* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */\nstatic void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\tstruct pvclock_vcpu_time_info hv_clock;\n\n\t/* both __this_cpu_read() and rdtsc() should be on the same cpu */\n\tget_cpu();\n\n\tdata->flags = 0;\n\tif (ka->use_master_clock &&\n\t    (static_cpu_has(X86_FEATURE_CONSTANT_TSC) || __this_cpu_read(cpu_tsc_khz))) {\n#ifdef CONFIG_X86_64\n\t\tstruct timespec64 ts;\n\n\t\tif (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {\n\t\t\tdata->realtime = ts.tv_nsec + NSEC_PER_SEC * ts.tv_sec;\n\t\t\tdata->flags |= KVM_CLOCK_REALTIME | KVM_CLOCK_HOST_TSC;\n\t\t} else\n#endif\n\t\tdata->host_tsc = rdtsc();\n\n\t\tdata->flags |= KVM_CLOCK_TSC_STABLE;\n\t\thv_clock.tsc_timestamp = ka->master_cycle_now;\n\t\thv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;\n\t\tkvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,\n\t\t\t\t   &hv_clock.tsc_shift,\n\t\t\t\t   &hv_clock.tsc_to_system_mul);\n\t\tdata->clock = __pvclock_read_cycles(&hv_clock, data->host_tsc);\n\t} else {\n\t\tdata->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;\n\t}\n\n\tput_cpu();\n}\n\nstatic void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\tunsigned seq;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&ka->pvclock_sc);\n\t\t__get_kvmclock(kvm, data);\n\t} while (read_seqcount_retry(&ka->pvclock_sc, seq));\n}\n\nu64 get_kvmclock_ns(struct kvm *kvm)\n{\n\tstruct kvm_clock_data data;\n\n\tget_kvmclock(kvm, &data);\n\treturn data.clock;\n}\n\nstatic void kvm_setup_guest_pvclock(struct kvm_vcpu *v,\n\t\t\t\t    struct gfn_to_pfn_cache *gpc,\n\t\t\t\t    unsigned int offset)\n{\n\tstruct kvm_vcpu_arch *vcpu = &v->arch;\n\tstruct pvclock_vcpu_time_info *guest_hv_clock;\n\tunsigned long flags;\n\n\tread_lock_irqsave(&gpc->lock, flags);\n\twhile (!kvm_gpc_check(gpc, offset + sizeof(*guest_hv_clock))) {\n\t\tread_unlock_irqrestore(&gpc->lock, flags);\n\n\t\tif (kvm_gpc_refresh(gpc, offset + sizeof(*guest_hv_clock)))\n\t\t\treturn;\n\n\t\tread_lock_irqsave(&gpc->lock, flags);\n\t}\n\n\tguest_hv_clock = (void *)(gpc->khva + offset);\n\n\t/*\n\t * This VCPU is paused, but it's legal for a guest to read another\n\t * VCPU's kvmclock, so we really have to follow the specification where\n\t * it says that version is odd if data is being modified, and even after\n\t * it is consistent.\n\t */\n\n\tguest_hv_clock->version = vcpu->hv_clock.version = (guest_hv_clock->version + 1) | 1;\n\tsmp_wmb();\n\n\t/* retain PVCLOCK_GUEST_STOPPED if set in guest copy */\n\tvcpu->hv_clock.flags |= (guest_hv_clock->flags & PVCLOCK_GUEST_STOPPED);\n\n\tif (vcpu->pvclock_set_guest_stopped_request) {\n\t\tvcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;\n\t\tvcpu->pvclock_set_guest_stopped_request = false;\n\t}\n\n\tmemcpy(guest_hv_clock, &vcpu->hv_clock, sizeof(*guest_hv_clock));\n\tsmp_wmb();\n\n\tguest_hv_clock->version = ++vcpu->hv_clock.version;\n\n\tmark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);\n\tread_unlock_irqrestore(&gpc->lock, flags);\n\n\ttrace_kvm_pvclock_update(v->vcpu_id, &vcpu->hv_clock);\n}\n\nstatic int kvm_guest_time_update(struct kvm_vcpu *v)\n{\n\tunsigned long flags, tgt_tsc_khz;\n\tunsigned seq;\n\tstruct kvm_vcpu_arch *vcpu = &v->arch;\n\tstruct kvm_arch *ka = &v->kvm->arch;\n\ts64 kernel_ns;\n\tu64 tsc_timestamp, host_tsc;\n\tu8 pvclock_flags;\n\tbool use_master_clock;\n\n\tkernel_ns = 0;\n\thost_tsc = 0;\n\n\t/*\n\t * If the host uses TSC clock, then passthrough TSC as stable\n\t * to the guest.\n\t */\n\tdo {\n\t\tseq = read_seqcount_begin(&ka->pvclock_sc);\n\t\tuse_master_clock = ka->use_master_clock;\n\t\tif (use_master_clock) {\n\t\t\thost_tsc = ka->master_cycle_now;\n\t\t\tkernel_ns = ka->master_kernel_ns;\n\t\t}\n\t} while (read_seqcount_retry(&ka->pvclock_sc, seq));\n\n\t/* Keep irq disabled to prevent changes to the clock */\n\tlocal_irq_save(flags);\n\ttgt_tsc_khz = get_cpu_tsc_khz();\n\tif (unlikely(tgt_tsc_khz == 0)) {\n\t\tlocal_irq_restore(flags);\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, v);\n\t\treturn 1;\n\t}\n\tif (!use_master_clock) {\n\t\thost_tsc = rdtsc();\n\t\tkernel_ns = get_kvmclock_base_ns();\n\t}\n\n\ttsc_timestamp = kvm_read_l1_tsc(v, host_tsc);\n\n\t/*\n\t * We may have to catch up the TSC to match elapsed wall clock\n\t * time for two reasons, even if kvmclock is used.\n\t *   1) CPU could have been running below the maximum TSC rate\n\t *   2) Broken TSC compensation resets the base at each VCPU\n\t *      entry to avoid unknown leaps of TSC even when running\n\t *      again on the same CPU.  This may cause apparent elapsed\n\t *      time to disappear, and the guest to stand still or run\n\t *\tvery slowly.\n\t */\n\tif (vcpu->tsc_catchup) {\n\t\tu64 tsc = compute_guest_tsc(v, kernel_ns);\n\t\tif (tsc > tsc_timestamp) {\n\t\t\tadjust_tsc_offset_guest(v, tsc - tsc_timestamp);\n\t\t\ttsc_timestamp = tsc;\n\t\t}\n\t}\n\n\tlocal_irq_restore(flags);\n\n\t/* With all the info we got, fill in the values */\n\n\tif (kvm_caps.has_tsc_control)\n\t\ttgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz,\n\t\t\t\t\t    v->arch.l1_tsc_scaling_ratio);\n\n\tif (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {\n\t\tkvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,\n\t\t\t\t   &vcpu->hv_clock.tsc_shift,\n\t\t\t\t   &vcpu->hv_clock.tsc_to_system_mul);\n\t\tvcpu->hw_tsc_khz = tgt_tsc_khz;\n\t}\n\n\tvcpu->hv_clock.tsc_timestamp = tsc_timestamp;\n\tvcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;\n\tvcpu->last_guest_tsc = tsc_timestamp;\n\n\t/* If the host uses TSC clocksource, then it is stable */\n\tpvclock_flags = 0;\n\tif (use_master_clock)\n\t\tpvclock_flags |= PVCLOCK_TSC_STABLE_BIT;\n\n\tvcpu->hv_clock.flags = pvclock_flags;\n\n\tif (vcpu->pv_time.active)\n\t\tkvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);\n\tif (vcpu->xen.vcpu_info_cache.active)\n\t\tkvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_info_cache,\n\t\t\t\t\toffsetof(struct compat_vcpu_info, time));\n\tif (vcpu->xen.vcpu_time_info_cache.active)\n\t\tkvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_time_info_cache, 0);\n\tkvm_hv_setup_tsc_page(v->kvm, &vcpu->hv_clock);\n\treturn 0;\n}\n\n/*\n * kvmclock updates which are isolated to a given vcpu, such as\n * vcpu->cpu migration, should not allow system_timestamp from\n * the rest of the vcpus to remain static. Otherwise ntp frequency\n * correction applies to one vcpu's system_timestamp but not\n * the others.\n *\n * So in those cases, request a kvmclock update for all vcpus.\n * We need to rate-limit these requests though, as they can\n * considerably slow guests that have a large number of vcpus.\n * The time for a remote vcpu to update its kvmclock is bound\n * by the delay we use to rate-limit the updates.\n */\n\n#define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)\n\nstatic void kvmclock_update_fn(struct work_struct *work)\n{\n\tunsigned long i;\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct kvm_arch *ka = container_of(dwork, struct kvm_arch,\n\t\t\t\t\t   kvmclock_update_work);\n\tstruct kvm *kvm = container_of(ka, struct kvm, arch);\n\tstruct kvm_vcpu *vcpu;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t}\n}\n\nstatic void kvm_gen_kvmclock_update(struct kvm_vcpu *v)\n{\n\tstruct kvm *kvm = v->kvm;\n\n\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, v);\n\tschedule_delayed_work(&kvm->arch.kvmclock_update_work,\n\t\t\t\t\tKVMCLOCK_UPDATE_DELAY);\n}\n\n#define KVMCLOCK_SYNC_PERIOD (300 * HZ)\n\nstatic void kvmclock_sync_fn(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct kvm_arch *ka = container_of(dwork, struct kvm_arch,\n\t\t\t\t\t   kvmclock_sync_work);\n\tstruct kvm *kvm = container_of(ka, struct kvm, arch);\n\n\tif (!kvmclock_periodic_sync)\n\t\treturn;\n\n\tschedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);\n\tschedule_delayed_work(&kvm->arch.kvmclock_sync_work,\n\t\t\t\t\tKVMCLOCK_SYNC_PERIOD);\n}\n\n/* These helpers are safe iff @msr is known to be an MCx bank MSR. */\nstatic bool is_mci_control_msr(u32 msr)\n{\n\treturn (msr & 3) == 0;\n}\nstatic bool is_mci_status_msr(u32 msr)\n{\n\treturn (msr & 3) == 1;\n}\n\n/*\n * On AMD, HWCR[McStatusWrEn] controls whether setting MCi_STATUS results in #GP.\n */\nstatic bool can_set_mci_status(struct kvm_vcpu *vcpu)\n{\n\t/* McStatusWrEn enabled? */\n\tif (guest_cpuid_is_amd_or_hygon(vcpu))\n\t\treturn !!(vcpu->arch.msr_hwcr & BIT_ULL(18));\n\n\treturn false;\n}\n\nstatic int set_msr_mce(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\tunsigned bank_num = mcg_cap & 0xff;\n\tu32 msr = msr_info->index;\n\tu64 data = msr_info->data;\n\tu32 offset, last_msr;\n\n\tswitch (msr) {\n\tcase MSR_IA32_MCG_STATUS:\n\t\tvcpu->arch.mcg_status = data;\n\t\tbreak;\n\tcase MSR_IA32_MCG_CTL:\n\t\tif (!(mcg_cap & MCG_CTL_P) &&\n\t\t    (data || !msr_info->host_initiated))\n\t\t\treturn 1;\n\t\tif (data != 0 && data != ~(u64)0)\n\t\t\treturn 1;\n\t\tvcpu->arch.mcg_ctl = data;\n\t\tbreak;\n\tcase MSR_IA32_MC0_CTL2 ... MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) - 1:\n\t\tlast_msr = MSR_IA32_MCx_CTL2(bank_num) - 1;\n\t\tif (msr > last_msr)\n\t\t\treturn 1;\n\n\t\tif (!(mcg_cap & MCG_CMCI_P) && (data || !msr_info->host_initiated))\n\t\t\treturn 1;\n\t\t/* An attempt to write a 1 to a reserved bit raises #GP */\n\t\tif (data & ~(MCI_CTL2_CMCI_EN | MCI_CTL2_CMCI_THRESHOLD_MASK))\n\t\t\treturn 1;\n\t\toffset = array_index_nospec(msr - MSR_IA32_MC0_CTL2,\n\t\t\t\t\t    last_msr + 1 - MSR_IA32_MC0_CTL2);\n\t\tvcpu->arch.mci_ctl2_banks[offset] = data;\n\t\tbreak;\n\tcase MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:\n\t\tlast_msr = MSR_IA32_MCx_CTL(bank_num) - 1;\n\t\tif (msr > last_msr)\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * Only 0 or all 1s can be written to IA32_MCi_CTL, all other\n\t\t * values are architecturally undefined.  But, some Linux\n\t\t * kernels clear bit 10 in bank 4 to workaround a BIOS/GART TLB\n\t\t * issue on AMD K8s, allow bit 10 to be clear when setting all\n\t\t * other bits in order to avoid an uncaught #GP in the guest.\n\t\t *\n\t\t * UNIXWARE clears bit 0 of MC1_CTL to ignore correctable,\n\t\t * single-bit ECC data errors.\n\t\t */\n\t\tif (is_mci_control_msr(msr) &&\n\t\t    data != 0 && (data | (1 << 10) | 1) != ~(u64)0)\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * All CPUs allow writing 0 to MCi_STATUS MSRs to clear the MSR.\n\t\t * AMD-based CPUs allow non-zero values, but if and only if\n\t\t * HWCR[McStatusWrEn] is set.\n\t\t */\n\t\tif (!msr_info->host_initiated && is_mci_status_msr(msr) &&\n\t\t    data != 0 && !can_set_mci_status(vcpu))\n\t\t\treturn 1;\n\n\t\toffset = array_index_nospec(msr - MSR_IA32_MC0_CTL,\n\t\t\t\t\t    last_msr + 1 - MSR_IA32_MC0_CTL);\n\t\tvcpu->arch.mce_banks[offset] = data;\n\t\tbreak;\n\tdefault:\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline bool kvm_pv_async_pf_enabled(struct kvm_vcpu *vcpu)\n{\n\tu64 mask = KVM_ASYNC_PF_ENABLED | KVM_ASYNC_PF_DELIVERY_AS_INT;\n\n\treturn (vcpu->arch.apf.msr_en_val & mask) == mask;\n}\n\nstatic int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)\n{\n\tgpa_t gpa = data & ~0x3f;\n\n\t/* Bits 4:5 are reserved, Should be zero */\n\tif (data & 0x30)\n\t\treturn 1;\n\n\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_VMEXIT) &&\n\t    (data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT))\n\t\treturn 1;\n\n\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT) &&\n\t    (data & KVM_ASYNC_PF_DELIVERY_AS_INT))\n\t\treturn 1;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn data ? 1 : 0;\n\n\tvcpu->arch.apf.msr_en_val = data;\n\n\tif (!kvm_pv_async_pf_enabled(vcpu)) {\n\t\tkvm_clear_async_pf_completion_queue(vcpu);\n\t\tkvm_async_pf_hash_reset(vcpu);\n\t\treturn 0;\n\t}\n\n\tif (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apf.data, gpa,\n\t\t\t\t\tsizeof(u64)))\n\t\treturn 1;\n\n\tvcpu->arch.apf.send_user_only = !(data & KVM_ASYNC_PF_SEND_ALWAYS);\n\tvcpu->arch.apf.delivery_as_pf_vmexit = data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;\n\n\tkvm_async_pf_wakeup_all(vcpu);\n\n\treturn 0;\n}\n\nstatic int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)\n{\n\t/* Bits 8-63 are reserved */\n\tif (data >> 8)\n\t\treturn 1;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 1;\n\n\tvcpu->arch.apf.msr_int_val = data;\n\n\tvcpu->arch.apf.vec = data & KVM_ASYNC_PF_VEC_MASK;\n\n\treturn 0;\n}\n\nstatic void kvmclock_reset(struct kvm_vcpu *vcpu)\n{\n\tkvm_gpc_deactivate(&vcpu->arch.pv_time);\n\tvcpu->arch.time = 0;\n}\n\nstatic void kvm_vcpu_flush_tlb_all(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.tlb_flush;\n\tstatic_call(kvm_x86_flush_tlb_all)(vcpu);\n\n\t/* Flushing all ASIDs flushes the current ASID... */\n\tkvm_clear_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n}\n\nstatic void kvm_vcpu_flush_tlb_guest(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.tlb_flush;\n\n\tif (!tdp_enabled) {\n\t\t/*\n\t\t * A TLB flush on behalf of the guest is equivalent to\n\t\t * INVPCID(all), toggling CR4.PGE, etc., which requires\n\t\t * a forced sync of the shadow page tables.  Ensure all the\n\t\t * roots are synced and the guest TLB in hardware is clean.\n\t\t */\n\t\tkvm_mmu_sync_roots(vcpu);\n\t\tkvm_mmu_sync_prev_roots(vcpu);\n\t}\n\n\tstatic_call(kvm_x86_flush_tlb_guest)(vcpu);\n\n\t/*\n\t * Flushing all \"guest\" TLB is always a superset of Hyper-V's fine\n\t * grained flushing.\n\t */\n\tkvm_hv_vcpu_purge_flush_tlb(vcpu);\n}\n\n\nstatic inline void kvm_vcpu_flush_tlb_current(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.tlb_flush;\n\tstatic_call(kvm_x86_flush_tlb_current)(vcpu);\n}\n\n/*\n * Service \"local\" TLB flush requests, which are specific to the current MMU\n * context.  In addition to the generic event handling in vcpu_enter_guest(),\n * TLB flushes that are targeted at an MMU context also need to be serviced\n * prior before nested VM-Enter/VM-Exit.\n */\nvoid kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_check_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu))\n\t\tkvm_vcpu_flush_tlb_current(vcpu);\n\n\tif (kvm_check_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu))\n\t\tkvm_vcpu_flush_tlb_guest(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_service_local_tlb_flush_requests);\n\nstatic void record_steal_time(struct kvm_vcpu *vcpu)\n{\n\tstruct gfn_to_hva_cache *ghc = &vcpu->arch.st.cache;\n\tstruct kvm_steal_time __user *st;\n\tstruct kvm_memslots *slots;\n\tgpa_t gpa = vcpu->arch.st.msr_val & KVM_STEAL_VALID_BITS;\n\tu64 steal;\n\tu32 version;\n\n\tif (kvm_xen_msr_enabled(vcpu->kvm)) {\n\t\tkvm_xen_runstate_set_running(vcpu);\n\t\treturn;\n\t}\n\n\tif (!(vcpu->arch.st.msr_val & KVM_MSR_ENABLED))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(current->mm != vcpu->kvm->mm))\n\t\treturn;\n\n\tslots = kvm_memslots(vcpu->kvm);\n\n\tif (unlikely(slots->generation != ghc->generation ||\n\t\t     gpa != ghc->gpa ||\n\t\t     kvm_is_error_hva(ghc->hva) || !ghc->memslot)) {\n\t\t/* We rely on the fact that it fits in a single page. */\n\t\tBUILD_BUG_ON((sizeof(*st) - 1) & KVM_STEAL_VALID_BITS);\n\n\t\tif (kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, gpa, sizeof(*st)) ||\n\t\t    kvm_is_error_hva(ghc->hva) || !ghc->memslot)\n\t\t\treturn;\n\t}\n\n\tst = (struct kvm_steal_time __user *)ghc->hva;\n\t/*\n\t * Doing a TLB flush here, on the guest's behalf, can avoid\n\t * expensive IPIs.\n\t */\n\tif (guest_pv_has(vcpu, KVM_FEATURE_PV_TLB_FLUSH)) {\n\t\tu8 st_preempted = 0;\n\t\tint err = -EFAULT;\n\n\t\tif (!user_access_begin(st, sizeof(*st)))\n\t\t\treturn;\n\n\t\tasm volatile(\"1: xchgb %0, %2\\n\"\n\t\t\t     \"xor %1, %1\\n\"\n\t\t\t     \"2:\\n\"\n\t\t\t     _ASM_EXTABLE_UA(1b, 2b)\n\t\t\t     : \"+q\" (st_preempted),\n\t\t\t       \"+&r\" (err),\n\t\t\t       \"+m\" (st->preempted));\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tuser_access_end();\n\n\t\tvcpu->arch.st.preempted = 0;\n\n\t\ttrace_kvm_pv_tlb_flush(vcpu->vcpu_id,\n\t\t\t\t       st_preempted & KVM_VCPU_FLUSH_TLB);\n\t\tif (st_preempted & KVM_VCPU_FLUSH_TLB)\n\t\t\tkvm_vcpu_flush_tlb_guest(vcpu);\n\n\t\tif (!user_access_begin(st, sizeof(*st)))\n\t\t\tgoto dirty;\n\t} else {\n\t\tif (!user_access_begin(st, sizeof(*st)))\n\t\t\treturn;\n\n\t\tunsafe_put_user(0, &st->preempted, out);\n\t\tvcpu->arch.st.preempted = 0;\n\t}\n\n\tunsafe_get_user(version, &st->version, out);\n\tif (version & 1)\n\t\tversion += 1;  /* first time write, random junk */\n\n\tversion += 1;\n\tunsafe_put_user(version, &st->version, out);\n\n\tsmp_wmb();\n\n\tunsafe_get_user(steal, &st->steal, out);\n\tsteal += current->sched_info.run_delay -\n\t\tvcpu->arch.st.last_steal;\n\tvcpu->arch.st.last_steal = current->sched_info.run_delay;\n\tunsafe_put_user(steal, &st->steal, out);\n\n\tversion += 1;\n\tunsafe_put_user(version, &st->version, out);\n\n out:\n\tuser_access_end();\n dirty:\n\tmark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));\n}\n\nint kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tbool pr = false;\n\tu32 msr = msr_info->index;\n\tu64 data = msr_info->data;\n\n\tif (msr && msr == vcpu->kvm->arch.xen_hvm_config.msr)\n\t\treturn kvm_xen_write_hypercall_page(vcpu, data);\n\n\tswitch (msr) {\n\tcase MSR_AMD64_NB_CFG:\n\tcase MSR_IA32_UCODE_WRITE:\n\tcase MSR_VM_HSAVE_PA:\n\tcase MSR_AMD64_PATCH_LOADER:\n\tcase MSR_AMD64_BU_CFG2:\n\tcase MSR_AMD64_DC_CFG:\n\tcase MSR_F15H_EX_CFG:\n\t\tbreak;\n\n\tcase MSR_IA32_UCODE_REV:\n\t\tif (msr_info->host_initiated)\n\t\t\tvcpu->arch.microcode_version = data;\n\t\tbreak;\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvcpu->arch.arch_capabilities = data;\n\t\tbreak;\n\tcase MSR_IA32_PERF_CAPABILITIES:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tif (data & ~kvm_caps.supported_perf_cap)\n\t\t\treturn 1;\n\n\t\tvcpu->arch.perf_capabilities = data;\n\t\tkvm_pmu_refresh(vcpu);\n\t\treturn 0;\n\tcase MSR_EFER:\n\t\treturn set_efer(vcpu, msr_info);\n\tcase MSR_K7_HWCR:\n\t\tdata &= ~(u64)0x40;\t/* ignore flush filter disable */\n\t\tdata &= ~(u64)0x100;\t/* ignore ignne emulation enable */\n\t\tdata &= ~(u64)0x8;\t/* ignore TLB cache disable */\n\n\t\t/* Handle McStatusWrEn */\n\t\tif (data == BIT_ULL(18)) {\n\t\t\tvcpu->arch.msr_hwcr = data;\n\t\t} else if (data != 0) {\n\t\t\tvcpu_unimpl(vcpu, \"unimplemented HWCR wrmsr: 0x%llx\\n\",\n\t\t\t\t    data);\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\tcase MSR_FAM10H_MMIO_CONF_BASE:\n\t\tif (data != 0) {\n\t\t\tvcpu_unimpl(vcpu, \"unimplemented MMIO_CONF_BASE wrmsr: \"\n\t\t\t\t    \"0x%llx\\n\", data);\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\tcase 0x200 ... MSR_IA32_MC0_CTL2 - 1:\n\tcase MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) ... 0x2ff:\n\t\treturn kvm_mtrr_set_msr(vcpu, msr, data);\n\tcase MSR_IA32_APICBASE:\n\t\treturn kvm_set_apic_base(vcpu, msr_info);\n\tcase APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:\n\t\treturn kvm_x2apic_msr_write(vcpu, msr, data);\n\tcase MSR_IA32_TSC_DEADLINE:\n\t\tkvm_set_lapic_tscdeadline_msr(vcpu, data);\n\t\tbreak;\n\tcase MSR_IA32_TSC_ADJUST:\n\t\tif (guest_cpuid_has(vcpu, X86_FEATURE_TSC_ADJUST)) {\n\t\t\tif (!msr_info->host_initiated) {\n\t\t\t\ts64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;\n\t\t\t\tadjust_tsc_offset_guest(vcpu, adj);\n\t\t\t\t/* Before back to guest, tsc_timestamp must be adjusted\n\t\t\t\t * as well, otherwise guest's percpu pvclock time could jump.\n\t\t\t\t */\n\t\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\t\t}\n\t\t\tvcpu->arch.ia32_tsc_adjust_msr = data;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_MISC_ENABLE: {\n\t\tu64 old_val = vcpu->arch.ia32_misc_enable_msr;\n\n\t\tif (!msr_info->host_initiated) {\n\t\t\t/* RO bits */\n\t\t\tif ((old_val ^ data) & MSR_IA32_MISC_ENABLE_PMU_RO_MASK)\n\t\t\t\treturn 1;\n\n\t\t\t/* R bits, i.e. writes are ignored, but don't fault. */\n\t\t\tdata = data & ~MSR_IA32_MISC_ENABLE_EMON;\n\t\t\tdata |= old_val & MSR_IA32_MISC_ENABLE_EMON;\n\t\t}\n\n\t\tif (!kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_MISC_ENABLE_NO_MWAIT) &&\n\t\t    ((old_val ^ data)  & MSR_IA32_MISC_ENABLE_MWAIT)) {\n\t\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_XMM3))\n\t\t\t\treturn 1;\n\t\t\tvcpu->arch.ia32_misc_enable_msr = data;\n\t\t\tkvm_update_cpuid_runtime(vcpu);\n\t\t} else {\n\t\t\tvcpu->arch.ia32_misc_enable_msr = data;\n\t\t}\n\t\tbreak;\n\t}\n\tcase MSR_IA32_SMBASE:\n\t\tif (!IS_ENABLED(CONFIG_KVM_SMM) || !msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvcpu->arch.smbase = data;\n\t\tbreak;\n\tcase MSR_IA32_POWER_CTL:\n\t\tvcpu->arch.msr_ia32_power_ctl = data;\n\t\tbreak;\n\tcase MSR_IA32_TSC:\n\t\tif (msr_info->host_initiated) {\n\t\t\tkvm_synchronize_tsc(vcpu, data);\n\t\t} else {\n\t\t\tu64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;\n\t\t\tadjust_tsc_offset_guest(vcpu, adj);\n\t\t\tvcpu->arch.ia32_tsc_adjust_msr += adj;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_XSS:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XSAVES))\n\t\t\treturn 1;\n\t\t/*\n\t\t * KVM supports exposing PT to the guest, but does not support\n\t\t * IA32_XSS[bit 8]. Guests have to use RDMSR/WRMSR rather than\n\t\t * XSAVES/XRSTORS to save/restore PT MSRs.\n\t\t */\n\t\tif (data & ~kvm_caps.supported_xss)\n\t\t\treturn 1;\n\t\tvcpu->arch.ia32_xss = data;\n\t\tkvm_update_cpuid_runtime(vcpu);\n\t\tbreak;\n\tcase MSR_SMI_COUNT:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvcpu->arch.smi_count = data;\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tvcpu->kvm->arch.wall_clock = data;\n\t\tkvm_write_wall_clock(vcpu->kvm, data, 0);\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tvcpu->kvm->arch.wall_clock = data;\n\t\tkvm_write_wall_clock(vcpu->kvm, data, 0);\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tkvm_write_system_time(vcpu, data, false, msr_info->host_initiated);\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tkvm_write_system_time(vcpu, data, true,  msr_info->host_initiated);\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF))\n\t\t\treturn 1;\n\n\t\tif (kvm_pv_enable_async_pf(vcpu, data))\n\t\t\treturn 1;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_INT:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))\n\t\t\treturn 1;\n\n\t\tif (kvm_pv_enable_async_pf_int(vcpu, data))\n\t\t\treturn 1;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_ACK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))\n\t\t\treturn 1;\n\t\tif (data & 0x1) {\n\t\t\tvcpu->arch.apf.pageready_pending = false;\n\t\t\tkvm_check_async_pf_completion(vcpu);\n\t\t}\n\t\tbreak;\n\tcase MSR_KVM_STEAL_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_STEAL_TIME))\n\t\t\treturn 1;\n\n\t\tif (unlikely(!sched_info_on()))\n\t\t\treturn 1;\n\n\t\tif (data & KVM_STEAL_RESERVED_MASK)\n\t\t\treturn 1;\n\n\t\tvcpu->arch.st.msr_val = data;\n\n\t\tif (!(data & KVM_MSR_ENABLED))\n\t\t\tbreak;\n\n\t\tkvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);\n\n\t\tbreak;\n\tcase MSR_KVM_PV_EOI_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_EOI))\n\t\t\treturn 1;\n\n\t\tif (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))\n\t\t\treturn 1;\n\t\tbreak;\n\n\tcase MSR_KVM_POLL_CONTROL:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_POLL_CONTROL))\n\t\t\treturn 1;\n\n\t\t/* only enable bit supported */\n\t\tif (data & (-1ULL << 1))\n\t\t\treturn 1;\n\n\t\tvcpu->arch.msr_kvm_poll_control = data;\n\t\tbreak;\n\n\tcase MSR_IA32_MCG_CTL:\n\tcase MSR_IA32_MCG_STATUS:\n\tcase MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:\n\tcase MSR_IA32_MC0_CTL2 ... MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) - 1:\n\t\treturn set_msr_mce(vcpu, msr_info);\n\n\tcase MSR_K7_PERFCTR0 ... MSR_K7_PERFCTR3:\n\tcase MSR_P6_PERFCTR0 ... MSR_P6_PERFCTR1:\n\t\tpr = true;\n\t\tfallthrough;\n\tcase MSR_K7_EVNTSEL0 ... MSR_K7_EVNTSEL3:\n\tcase MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr))\n\t\t\treturn kvm_pmu_set_msr(vcpu, msr_info);\n\n\t\tif (pr || data != 0)\n\t\t\tvcpu_unimpl(vcpu, \"disabled perfctr wrmsr: \"\n\t\t\t\t    \"0x%x data 0x%llx\\n\", msr, data);\n\t\tbreak;\n\tcase MSR_K7_CLK_CTL:\n\t\t/*\n\t\t * Ignore all writes to this no longer documented MSR.\n\t\t * Writes are only relevant for old K7 processors,\n\t\t * all pre-dating SVM, but a recommended workaround from\n\t\t * AMD for these chips. It is possible to specify the\n\t\t * affected processor models on the command line, hence\n\t\t * the need to ignore the workaround.\n\t\t */\n\t\tbreak;\n\tcase HV_X64_MSR_GUEST_OS_ID ... HV_X64_MSR_SINT15:\n\tcase HV_X64_MSR_SYNDBG_CONTROL ... HV_X64_MSR_SYNDBG_PENDING_BUFFER:\n\tcase HV_X64_MSR_SYNDBG_OPTIONS:\n\tcase HV_X64_MSR_CRASH_P0 ... HV_X64_MSR_CRASH_P4:\n\tcase HV_X64_MSR_CRASH_CTL:\n\tcase HV_X64_MSR_STIMER0_CONFIG ... HV_X64_MSR_STIMER3_COUNT:\n\tcase HV_X64_MSR_REENLIGHTENMENT_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_STATUS:\n\t\treturn kvm_hv_set_msr_common(vcpu, msr, data,\n\t\t\t\t\t     msr_info->host_initiated);\n\tcase MSR_IA32_BBL_CR_CTL3:\n\t\t/* Drop writes to this legacy MSR -- see rdmsr\n\t\t * counterpart for further detail.\n\t\t */\n\t\tif (report_ignored_msrs)\n\t\t\tvcpu_unimpl(vcpu, \"ignored wrmsr: 0x%x data 0x%llx\\n\",\n\t\t\t\tmsr, data);\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_ID_LENGTH:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tvcpu->arch.osvw.length = data;\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_STATUS:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tvcpu->arch.osvw.status = data;\n\t\tbreak;\n\tcase MSR_PLATFORM_INFO:\n\t\tif (!msr_info->host_initiated ||\n\t\t    (!(data & MSR_PLATFORM_INFO_CPUID_FAULT) &&\n\t\t     cpuid_fault_enabled(vcpu)))\n\t\t\treturn 1;\n\t\tvcpu->arch.msr_platform_info = data;\n\t\tbreak;\n\tcase MSR_MISC_FEATURES_ENABLES:\n\t\tif (data & ~MSR_MISC_FEATURES_ENABLES_CPUID_FAULT ||\n\t\t    (data & MSR_MISC_FEATURES_ENABLES_CPUID_FAULT &&\n\t\t     !supports_cpuid_fault(vcpu)))\n\t\t\treturn 1;\n\t\tvcpu->arch.msr_misc_features_enables = data;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_IA32_XFD:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XFD))\n\t\t\treturn 1;\n\n\t\tif (data & ~kvm_guest_supported_xfd(vcpu))\n\t\t\treturn 1;\n\n\t\tfpu_update_guest_xfd(&vcpu->arch.guest_fpu, data);\n\t\tbreak;\n\tcase MSR_IA32_XFD_ERR:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XFD))\n\t\t\treturn 1;\n\n\t\tif (data & ~kvm_guest_supported_xfd(vcpu))\n\t\t\treturn 1;\n\n\t\tvcpu->arch.guest_fpu.xfd_err = data;\n\t\tbreak;\n#endif\n\tcase MSR_IA32_PEBS_ENABLE:\n\tcase MSR_IA32_DS_AREA:\n\tcase MSR_PEBS_DATA_CFG:\n\tcase MSR_F15H_PERF_CTL0 ... MSR_F15H_PERF_CTR5:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr))\n\t\t\treturn kvm_pmu_set_msr(vcpu, msr_info);\n\t\t/*\n\t\t * Userspace is allowed to write '0' to MSRs that KVM reports\n\t\t * as to-be-saved, even if an MSRs isn't fully supported.\n\t\t */\n\t\treturn !msr_info->host_initiated || data;\n\tdefault:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr))\n\t\t\treturn kvm_pmu_set_msr(vcpu, msr_info);\n\t\treturn KVM_MSR_RET_INVALID;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_msr_common);\n\nstatic int get_msr_mce(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata, bool host)\n{\n\tu64 data;\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\tunsigned bank_num = mcg_cap & 0xff;\n\tu32 offset, last_msr;\n\n\tswitch (msr) {\n\tcase MSR_IA32_P5_MC_ADDR:\n\tcase MSR_IA32_P5_MC_TYPE:\n\t\tdata = 0;\n\t\tbreak;\n\tcase MSR_IA32_MCG_CAP:\n\t\tdata = vcpu->arch.mcg_cap;\n\t\tbreak;\n\tcase MSR_IA32_MCG_CTL:\n\t\tif (!(mcg_cap & MCG_CTL_P) && !host)\n\t\t\treturn 1;\n\t\tdata = vcpu->arch.mcg_ctl;\n\t\tbreak;\n\tcase MSR_IA32_MCG_STATUS:\n\t\tdata = vcpu->arch.mcg_status;\n\t\tbreak;\n\tcase MSR_IA32_MC0_CTL2 ... MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) - 1:\n\t\tlast_msr = MSR_IA32_MCx_CTL2(bank_num) - 1;\n\t\tif (msr > last_msr)\n\t\t\treturn 1;\n\n\t\tif (!(mcg_cap & MCG_CMCI_P) && !host)\n\t\t\treturn 1;\n\t\toffset = array_index_nospec(msr - MSR_IA32_MC0_CTL2,\n\t\t\t\t\t    last_msr + 1 - MSR_IA32_MC0_CTL2);\n\t\tdata = vcpu->arch.mci_ctl2_banks[offset];\n\t\tbreak;\n\tcase MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:\n\t\tlast_msr = MSR_IA32_MCx_CTL(bank_num) - 1;\n\t\tif (msr > last_msr)\n\t\t\treturn 1;\n\n\t\toffset = array_index_nospec(msr - MSR_IA32_MC0_CTL,\n\t\t\t\t\t    last_msr + 1 - MSR_IA32_MC0_CTL);\n\t\tdata = vcpu->arch.mce_banks[offset];\n\t\tbreak;\n\tdefault:\n\t\treturn 1;\n\t}\n\t*pdata = data;\n\treturn 0;\n}\n\nint kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tswitch (msr_info->index) {\n\tcase MSR_IA32_PLATFORM_ID:\n\tcase MSR_IA32_EBL_CR_POWERON:\n\tcase MSR_IA32_LASTBRANCHFROMIP:\n\tcase MSR_IA32_LASTBRANCHTOIP:\n\tcase MSR_IA32_LASTINTFROMIP:\n\tcase MSR_IA32_LASTINTTOIP:\n\tcase MSR_AMD64_SYSCFG:\n\tcase MSR_K8_TSEG_ADDR:\n\tcase MSR_K8_TSEG_MASK:\n\tcase MSR_VM_HSAVE_PA:\n\tcase MSR_K8_INT_PENDING_MSG:\n\tcase MSR_AMD64_NB_CFG:\n\tcase MSR_FAM10H_MMIO_CONF_BASE:\n\tcase MSR_AMD64_BU_CFG2:\n\tcase MSR_IA32_PERF_CTL:\n\tcase MSR_AMD64_DC_CFG:\n\tcase MSR_F15H_EX_CFG:\n\t/*\n\t * Intel Sandy Bridge CPUs must support the RAPL (running average power\n\t * limit) MSRs. Just return 0, as we do not want to expose the host\n\t * data here. Do not conditionalize this on CPUID, as KVM does not do\n\t * so for existing CPU-specific MSRs.\n\t */\n\tcase MSR_RAPL_POWER_UNIT:\n\tcase MSR_PP0_ENERGY_STATUS:\t/* Power plane 0 (core) */\n\tcase MSR_PP1_ENERGY_STATUS:\t/* Power plane 1 (graphics uncore) */\n\tcase MSR_PKG_ENERGY_STATUS:\t/* Total package */\n\tcase MSR_DRAM_ENERGY_STATUS:\t/* DRAM controller */\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_IA32_PEBS_ENABLE:\n\tcase MSR_IA32_DS_AREA:\n\tcase MSR_PEBS_DATA_CFG:\n\tcase MSR_F15H_PERF_CTL0 ... MSR_F15H_PERF_CTR5:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr_info->index))\n\t\t\treturn kvm_pmu_get_msr(vcpu, msr_info);\n\t\t/*\n\t\t * Userspace is allowed to read MSRs that KVM reports as\n\t\t * to-be-saved, even if an MSR isn't fully supported.\n\t\t */\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_K7_EVNTSEL0 ... MSR_K7_EVNTSEL3:\n\tcase MSR_K7_PERFCTR0 ... MSR_K7_PERFCTR3:\n\tcase MSR_P6_PERFCTR0 ... MSR_P6_PERFCTR1:\n\tcase MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr_info->index))\n\t\t\treturn kvm_pmu_get_msr(vcpu, msr_info);\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_IA32_UCODE_REV:\n\t\tmsr_info->data = vcpu->arch.microcode_version;\n\t\tbreak;\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_ARCH_CAPABILITIES))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.arch_capabilities;\n\t\tbreak;\n\tcase MSR_IA32_PERF_CAPABILITIES:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_PDCM))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.perf_capabilities;\n\t\tbreak;\n\tcase MSR_IA32_POWER_CTL:\n\t\tmsr_info->data = vcpu->arch.msr_ia32_power_ctl;\n\t\tbreak;\n\tcase MSR_IA32_TSC: {\n\t\t/*\n\t\t * Intel SDM states that MSR_IA32_TSC read adds the TSC offset\n\t\t * even when not intercepted. AMD manual doesn't explicitly\n\t\t * state this but appears to behave the same.\n\t\t *\n\t\t * On userspace reads and writes, however, we unconditionally\n\t\t * return L1's TSC value to ensure backwards-compatible\n\t\t * behavior for migration.\n\t\t */\n\t\tu64 offset, ratio;\n\n\t\tif (msr_info->host_initiated) {\n\t\t\toffset = vcpu->arch.l1_tsc_offset;\n\t\t\tratio = vcpu->arch.l1_tsc_scaling_ratio;\n\t\t} else {\n\t\t\toffset = vcpu->arch.tsc_offset;\n\t\t\tratio = vcpu->arch.tsc_scaling_ratio;\n\t\t}\n\n\t\tmsr_info->data = kvm_scale_tsc(rdtsc(), ratio) + offset;\n\t\tbreak;\n\t}\n\tcase MSR_MTRRcap:\n\tcase 0x200 ... MSR_IA32_MC0_CTL2 - 1:\n\tcase MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) ... 0x2ff:\n\t\treturn kvm_mtrr_get_msr(vcpu, msr_info->index, &msr_info->data);\n\tcase 0xcd: /* fsb frequency */\n\t\tmsr_info->data = 3;\n\t\tbreak;\n\t\t/*\n\t\t * MSR_EBC_FREQUENCY_ID\n\t\t * Conservative value valid for even the basic CPU models.\n\t\t * Models 0,1: 000 in bits 23:21 indicating a bus speed of\n\t\t * 100MHz, model 2 000 in bits 18:16 indicating 100MHz,\n\t\t * and 266MHz for model 3, or 4. Set Core Clock\n\t\t * Frequency to System Bus Frequency Ratio to 1 (bits\n\t\t * 31:24) even though these are only valid for CPU\n\t\t * models > 2, however guests may end up dividing or\n\t\t * multiplying by zero otherwise.\n\t\t */\n\tcase MSR_EBC_FREQUENCY_ID:\n\t\tmsr_info->data = 1 << 24;\n\t\tbreak;\n\tcase MSR_IA32_APICBASE:\n\t\tmsr_info->data = kvm_get_apic_base(vcpu);\n\t\tbreak;\n\tcase APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:\n\t\treturn kvm_x2apic_msr_read(vcpu, msr_info->index, &msr_info->data);\n\tcase MSR_IA32_TSC_DEADLINE:\n\t\tmsr_info->data = kvm_get_lapic_tscdeadline_msr(vcpu);\n\t\tbreak;\n\tcase MSR_IA32_TSC_ADJUST:\n\t\tmsr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;\n\t\tbreak;\n\tcase MSR_IA32_MISC_ENABLE:\n\t\tmsr_info->data = vcpu->arch.ia32_misc_enable_msr;\n\t\tbreak;\n\tcase MSR_IA32_SMBASE:\n\t\tif (!IS_ENABLED(CONFIG_KVM_SMM) || !msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.smbase;\n\t\tbreak;\n\tcase MSR_SMI_COUNT:\n\t\tmsr_info->data = vcpu->arch.smi_count;\n\t\tbreak;\n\tcase MSR_IA32_PERF_STATUS:\n\t\t/* TSC increment by tick */\n\t\tmsr_info->data = 1000ULL;\n\t\t/* CPU multiplier */\n\t\tmsr_info->data |= (((uint64_t)4ULL) << 40);\n\t\tbreak;\n\tcase MSR_EFER:\n\t\tmsr_info->data = vcpu->arch.efer;\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->kvm->arch.wall_clock;\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->kvm->arch.wall_clock;\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.time;\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.time;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.apf.msr_en_val;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_INT:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.apf.msr_int_val;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_ACK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_KVM_STEAL_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_STEAL_TIME))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.st.msr_val;\n\t\tbreak;\n\tcase MSR_KVM_PV_EOI_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_EOI))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.pv_eoi.msr_val;\n\t\tbreak;\n\tcase MSR_KVM_POLL_CONTROL:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_POLL_CONTROL))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.msr_kvm_poll_control;\n\t\tbreak;\n\tcase MSR_IA32_P5_MC_ADDR:\n\tcase MSR_IA32_P5_MC_TYPE:\n\tcase MSR_IA32_MCG_CAP:\n\tcase MSR_IA32_MCG_CTL:\n\tcase MSR_IA32_MCG_STATUS:\n\tcase MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:\n\tcase MSR_IA32_MC0_CTL2 ... MSR_IA32_MCx_CTL2(KVM_MAX_MCE_BANKS) - 1:\n\t\treturn get_msr_mce(vcpu, msr_info->index, &msr_info->data,\n\t\t\t\t   msr_info->host_initiated);\n\tcase MSR_IA32_XSS:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XSAVES))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.ia32_xss;\n\t\tbreak;\n\tcase MSR_K7_CLK_CTL:\n\t\t/*\n\t\t * Provide expected ramp-up count for K7. All other\n\t\t * are set to zero, indicating minimum divisors for\n\t\t * every field.\n\t\t *\n\t\t * This prevents guest kernels on AMD host with CPU\n\t\t * type 6, model 8 and higher from exploding due to\n\t\t * the rdmsr failing.\n\t\t */\n\t\tmsr_info->data = 0x20000000;\n\t\tbreak;\n\tcase HV_X64_MSR_GUEST_OS_ID ... HV_X64_MSR_SINT15:\n\tcase HV_X64_MSR_SYNDBG_CONTROL ... HV_X64_MSR_SYNDBG_PENDING_BUFFER:\n\tcase HV_X64_MSR_SYNDBG_OPTIONS:\n\tcase HV_X64_MSR_CRASH_P0 ... HV_X64_MSR_CRASH_P4:\n\tcase HV_X64_MSR_CRASH_CTL:\n\tcase HV_X64_MSR_STIMER0_CONFIG ... HV_X64_MSR_STIMER3_COUNT:\n\tcase HV_X64_MSR_REENLIGHTENMENT_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_STATUS:\n\t\treturn kvm_hv_get_msr_common(vcpu,\n\t\t\t\t\t     msr_info->index, &msr_info->data,\n\t\t\t\t\t     msr_info->host_initiated);\n\tcase MSR_IA32_BBL_CR_CTL3:\n\t\t/* This legacy MSR exists but isn't fully documented in current\n\t\t * silicon.  It is however accessed by winxp in very narrow\n\t\t * scenarios where it sets bit #19, itself documented as\n\t\t * a \"reserved\" bit.  Best effort attempt to source coherent\n\t\t * read data here should the balance of the register be\n\t\t * interpreted by the guest:\n\t\t *\n\t\t * L2 cache control register 3: 64GB range, 256KB size,\n\t\t * enabled, latency 0x1, configured\n\t\t */\n\t\tmsr_info->data = 0xbe702111;\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_ID_LENGTH:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.osvw.length;\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_STATUS:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.osvw.status;\n\t\tbreak;\n\tcase MSR_PLATFORM_INFO:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !vcpu->kvm->arch.guest_can_read_msr_platform_info)\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.msr_platform_info;\n\t\tbreak;\n\tcase MSR_MISC_FEATURES_ENABLES:\n\t\tmsr_info->data = vcpu->arch.msr_misc_features_enables;\n\t\tbreak;\n\tcase MSR_K7_HWCR:\n\t\tmsr_info->data = vcpu->arch.msr_hwcr;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_IA32_XFD:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XFD))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.guest_fpu.fpstate->xfd;\n\t\tbreak;\n\tcase MSR_IA32_XFD_ERR:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XFD))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.guest_fpu.xfd_err;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr_info->index))\n\t\t\treturn kvm_pmu_get_msr(vcpu, msr_info);\n\t\treturn KVM_MSR_RET_INVALID;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_get_msr_common);\n\n/*\n * Read or write a bunch of msrs. All parameters are kernel addresses.\n *\n * @return number of msrs set successfully.\n */\nstatic int __msr_io(struct kvm_vcpu *vcpu, struct kvm_msrs *msrs,\n\t\t    struct kvm_msr_entry *entries,\n\t\t    int (*do_msr)(struct kvm_vcpu *vcpu,\n\t\t\t\t  unsigned index, u64 *data))\n{\n\tint i;\n\n\tfor (i = 0; i < msrs->nmsrs; ++i)\n\t\tif (do_msr(vcpu, entries[i].index, &entries[i].data))\n\t\t\tbreak;\n\n\treturn i;\n}\n\n/*\n * Read or write a bunch of msrs. Parameters are user addresses.\n *\n * @return number of msrs set successfully.\n */\nstatic int msr_io(struct kvm_vcpu *vcpu, struct kvm_msrs __user *user_msrs,\n\t\t  int (*do_msr)(struct kvm_vcpu *vcpu,\n\t\t\t\tunsigned index, u64 *data),\n\t\t  int writeback)\n{\n\tstruct kvm_msrs msrs;\n\tstruct kvm_msr_entry *entries;\n\tint r, n;\n\tunsigned size;\n\n\tr = -EFAULT;\n\tif (copy_from_user(&msrs, user_msrs, sizeof(msrs)))\n\t\tgoto out;\n\n\tr = -E2BIG;\n\tif (msrs.nmsrs >= MAX_IO_MSRS)\n\t\tgoto out;\n\n\tsize = sizeof(struct kvm_msr_entry) * msrs.nmsrs;\n\tentries = memdup_user(user_msrs->entries, size);\n\tif (IS_ERR(entries)) {\n\t\tr = PTR_ERR(entries);\n\t\tgoto out;\n\t}\n\n\tr = n = __msr_io(vcpu, &msrs, entries, do_msr);\n\tif (r < 0)\n\t\tgoto out_free;\n\n\tr = -EFAULT;\n\tif (writeback && copy_to_user(user_msrs->entries, entries, size))\n\t\tgoto out_free;\n\n\tr = n;\n\nout_free:\n\tkfree(entries);\nout:\n\treturn r;\n}\n\nstatic inline bool kvm_can_mwait_in_guest(void)\n{\n\treturn boot_cpu_has(X86_FEATURE_MWAIT) &&\n\t\t!boot_cpu_has_bug(X86_BUG_MONITOR) &&\n\t\tboot_cpu_has(X86_FEATURE_ARAT);\n}\n\nstatic int kvm_ioctl_get_supported_hv_cpuid(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    struct kvm_cpuid2 __user *cpuid_arg)\n{\n\tstruct kvm_cpuid2 cpuid;\n\tint r;\n\n\tr = -EFAULT;\n\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\treturn r;\n\n\tr = kvm_get_hv_cpuid(vcpu, &cpuid, cpuid_arg->entries);\n\tif (r)\n\t\treturn r;\n\n\tr = -EFAULT;\n\tif (copy_to_user(cpuid_arg, &cpuid, sizeof(cpuid)))\n\t\treturn r;\n\n\treturn 0;\n}\n\nint kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)\n{\n\tint r = 0;\n\n\tswitch (ext) {\n\tcase KVM_CAP_IRQCHIP:\n\tcase KVM_CAP_HLT:\n\tcase KVM_CAP_MMU_SHADOW_CACHE_CONTROL:\n\tcase KVM_CAP_SET_TSS_ADDR:\n\tcase KVM_CAP_EXT_CPUID:\n\tcase KVM_CAP_EXT_EMUL_CPUID:\n\tcase KVM_CAP_CLOCKSOURCE:\n\tcase KVM_CAP_PIT:\n\tcase KVM_CAP_NOP_IO_DELAY:\n\tcase KVM_CAP_MP_STATE:\n\tcase KVM_CAP_SYNC_MMU:\n\tcase KVM_CAP_USER_NMI:\n\tcase KVM_CAP_REINJECT_CONTROL:\n\tcase KVM_CAP_IRQ_INJECT_STATUS:\n\tcase KVM_CAP_IOEVENTFD:\n\tcase KVM_CAP_IOEVENTFD_NO_LENGTH:\n\tcase KVM_CAP_PIT2:\n\tcase KVM_CAP_PIT_STATE2:\n\tcase KVM_CAP_SET_IDENTITY_MAP_ADDR:\n\tcase KVM_CAP_VCPU_EVENTS:\n\tcase KVM_CAP_HYPERV:\n\tcase KVM_CAP_HYPERV_VAPIC:\n\tcase KVM_CAP_HYPERV_SPIN:\n\tcase KVM_CAP_HYPERV_SYNIC:\n\tcase KVM_CAP_HYPERV_SYNIC2:\n\tcase KVM_CAP_HYPERV_VP_INDEX:\n\tcase KVM_CAP_HYPERV_EVENTFD:\n\tcase KVM_CAP_HYPERV_TLBFLUSH:\n\tcase KVM_CAP_HYPERV_SEND_IPI:\n\tcase KVM_CAP_HYPERV_CPUID:\n\tcase KVM_CAP_HYPERV_ENFORCE_CPUID:\n\tcase KVM_CAP_SYS_HYPERV_CPUID:\n\tcase KVM_CAP_PCI_SEGMENT:\n\tcase KVM_CAP_DEBUGREGS:\n\tcase KVM_CAP_X86_ROBUST_SINGLESTEP:\n\tcase KVM_CAP_XSAVE:\n\tcase KVM_CAP_ASYNC_PF:\n\tcase KVM_CAP_ASYNC_PF_INT:\n\tcase KVM_CAP_GET_TSC_KHZ:\n\tcase KVM_CAP_KVMCLOCK_CTRL:\n\tcase KVM_CAP_READONLY_MEM:\n\tcase KVM_CAP_HYPERV_TIME:\n\tcase KVM_CAP_IOAPIC_POLARITY_IGNORED:\n\tcase KVM_CAP_TSC_DEADLINE_TIMER:\n\tcase KVM_CAP_DISABLE_QUIRKS:\n\tcase KVM_CAP_SET_BOOT_CPU_ID:\n \tcase KVM_CAP_SPLIT_IRQCHIP:\n\tcase KVM_CAP_IMMEDIATE_EXIT:\n\tcase KVM_CAP_PMU_EVENT_FILTER:\n\tcase KVM_CAP_GET_MSR_FEATURES:\n\tcase KVM_CAP_MSR_PLATFORM_INFO:\n\tcase KVM_CAP_EXCEPTION_PAYLOAD:\n\tcase KVM_CAP_X86_TRIPLE_FAULT_EVENT:\n\tcase KVM_CAP_SET_GUEST_DEBUG:\n\tcase KVM_CAP_LAST_CPU:\n\tcase KVM_CAP_X86_USER_SPACE_MSR:\n\tcase KVM_CAP_X86_MSR_FILTER:\n\tcase KVM_CAP_ENFORCE_PV_FEATURE_CPUID:\n#ifdef CONFIG_X86_SGX_KVM\n\tcase KVM_CAP_SGX_ATTRIBUTE:\n#endif\n\tcase KVM_CAP_VM_COPY_ENC_CONTEXT_FROM:\n\tcase KVM_CAP_VM_MOVE_ENC_CONTEXT_FROM:\n\tcase KVM_CAP_SREGS2:\n\tcase KVM_CAP_EXIT_ON_EMULATION_FAILURE:\n\tcase KVM_CAP_VCPU_ATTRIBUTES:\n\tcase KVM_CAP_SYS_ATTRIBUTES:\n\tcase KVM_CAP_VAPIC:\n\tcase KVM_CAP_ENABLE_CAP:\n\tcase KVM_CAP_VM_DISABLE_NX_HUGE_PAGES:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_EXIT_HYPERCALL:\n\t\tr = KVM_EXIT_HYPERCALL_VALID_MASK;\n\t\tbreak;\n\tcase KVM_CAP_SET_GUEST_DEBUG2:\n\t\treturn KVM_GUESTDBG_VALID_MASK;\n#ifdef CONFIG_KVM_XEN\n\tcase KVM_CAP_XEN_HVM:\n\t\tr = KVM_XEN_HVM_CONFIG_HYPERCALL_MSR |\n\t\t    KVM_XEN_HVM_CONFIG_INTERCEPT_HCALL |\n\t\t    KVM_XEN_HVM_CONFIG_SHARED_INFO |\n\t\t    KVM_XEN_HVM_CONFIG_EVTCHN_2LEVEL |\n\t\t    KVM_XEN_HVM_CONFIG_EVTCHN_SEND;\n\t\tif (sched_info_on())\n\t\t\tr |= KVM_XEN_HVM_CONFIG_RUNSTATE |\n\t\t\t     KVM_XEN_HVM_CONFIG_RUNSTATE_UPDATE_FLAG;\n\t\tbreak;\n#endif\n\tcase KVM_CAP_SYNC_REGS:\n\t\tr = KVM_SYNC_X86_VALID_FIELDS;\n\t\tbreak;\n\tcase KVM_CAP_ADJUST_CLOCK:\n\t\tr = KVM_CLOCK_VALID_FLAGS;\n\t\tbreak;\n\tcase KVM_CAP_X86_DISABLE_EXITS:\n\t\tr = KVM_X86_DISABLE_EXITS_PAUSE;\n\n\t\tif (!mitigate_smt_rsb) {\n\t\t\tr |= KVM_X86_DISABLE_EXITS_HLT |\n\t\t\t     KVM_X86_DISABLE_EXITS_CSTATE;\n\n\t\t\tif (kvm_can_mwait_in_guest())\n\t\t\t\tr |= KVM_X86_DISABLE_EXITS_MWAIT;\n\t\t}\n\t\tbreak;\n\tcase KVM_CAP_X86_SMM:\n\t\tif (!IS_ENABLED(CONFIG_KVM_SMM))\n\t\t\tbreak;\n\n\t\t/* SMBASE is usually relocated above 1M on modern chipsets,\n\t\t * and SMM handlers might indeed rely on 4G segment limits,\n\t\t * so do not report SMM to be available if real mode is\n\t\t * emulated via vm86 mode.  Still, do not go to great lengths\n\t\t * to avoid userspace's usage of the feature, because it is a\n\t\t * fringe case that is not enabled except via specific settings\n\t\t * of the module parameters.\n\t\t */\n\t\tr = static_call(kvm_x86_has_emulated_msr)(kvm, MSR_IA32_SMBASE);\n\t\tbreak;\n\tcase KVM_CAP_NR_VCPUS:\n\t\tr = min_t(unsigned int, num_online_cpus(), KVM_MAX_VCPUS);\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPUS:\n\t\tr = KVM_MAX_VCPUS;\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPU_ID:\n\t\tr = KVM_MAX_VCPU_IDS;\n\t\tbreak;\n\tcase KVM_CAP_PV_MMU:\t/* obsolete */\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_MCE:\n\t\tr = KVM_MAX_MCE_BANKS;\n\t\tbreak;\n\tcase KVM_CAP_XCRS:\n\t\tr = boot_cpu_has(X86_FEATURE_XSAVE);\n\t\tbreak;\n\tcase KVM_CAP_TSC_CONTROL:\n\tcase KVM_CAP_VM_TSC_CONTROL:\n\t\tr = kvm_caps.has_tsc_control;\n\t\tbreak;\n\tcase KVM_CAP_X2APIC_API:\n\t\tr = KVM_X2APIC_API_VALID_FLAGS;\n\t\tbreak;\n\tcase KVM_CAP_NESTED_STATE:\n\t\tr = kvm_x86_ops.nested_ops->get_state ?\n\t\t\tkvm_x86_ops.nested_ops->get_state(NULL, NULL, 0) : 0;\n\t\tbreak;\n\tcase KVM_CAP_HYPERV_DIRECT_TLBFLUSH:\n\t\tr = kvm_x86_ops.enable_l2_tlb_flush != NULL;\n\t\tbreak;\n\tcase KVM_CAP_HYPERV_ENLIGHTENED_VMCS:\n\t\tr = kvm_x86_ops.nested_ops->enable_evmcs != NULL;\n\t\tbreak;\n\tcase KVM_CAP_SMALLER_MAXPHYADDR:\n\t\tr = (int) allow_smaller_maxphyaddr;\n\t\tbreak;\n\tcase KVM_CAP_STEAL_TIME:\n\t\tr = sched_info_on();\n\t\tbreak;\n\tcase KVM_CAP_X86_BUS_LOCK_EXIT:\n\t\tif (kvm_caps.has_bus_lock_exit)\n\t\t\tr = KVM_BUS_LOCK_DETECTION_OFF |\n\t\t\t    KVM_BUS_LOCK_DETECTION_EXIT;\n\t\telse\n\t\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_XSAVE2: {\n\t\tu64 guest_perm = xstate_get_guest_group_perm();\n\n\t\tr = xstate_required_size(kvm_caps.supported_xcr0 & guest_perm, false);\n\t\tif (r < sizeof(struct kvm_xsave))\n\t\t\tr = sizeof(struct kvm_xsave);\n\t\tbreak;\n\t}\n\tcase KVM_CAP_PMU_CAPABILITY:\n\t\tr = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;\n\t\tbreak;\n\tcase KVM_CAP_DISABLE_QUIRKS2:\n\t\tr = KVM_X86_VALID_QUIRKS;\n\t\tbreak;\n\tcase KVM_CAP_X86_NOTIFY_VMEXIT:\n\t\tr = kvm_caps.has_notify_vmexit;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn r;\n}\n\nstatic inline void __user *kvm_get_attr_addr(struct kvm_device_attr *attr)\n{\n\tvoid __user *uaddr = (void __user*)(unsigned long)attr->addr;\n\n\tif ((u64)(unsigned long)uaddr != attr->addr)\n\t\treturn ERR_PTR_USR(-EFAULT);\n\treturn uaddr;\n}\n\nstatic int kvm_x86_dev_get_attr(struct kvm_device_attr *attr)\n{\n\tu64 __user *uaddr = kvm_get_attr_addr(attr);\n\n\tif (attr->group)\n\t\treturn -ENXIO;\n\n\tif (IS_ERR(uaddr))\n\t\treturn PTR_ERR(uaddr);\n\n\tswitch (attr->attr) {\n\tcase KVM_X86_XCOMP_GUEST_SUPP:\n\t\tif (put_user(kvm_caps.supported_xcr0, uaddr))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENXIO;\n\t\tbreak;\n\t}\n}\n\nstatic int kvm_x86_dev_has_attr(struct kvm_device_attr *attr)\n{\n\tif (attr->group)\n\t\treturn -ENXIO;\n\n\tswitch (attr->attr) {\n\tcase KVM_X86_XCOMP_GUEST_SUPP:\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENXIO;\n\t}\n}\n\nlong kvm_arch_dev_ioctl(struct file *filp,\n\t\t\tunsigned int ioctl, unsigned long arg)\n{\n\tvoid __user *argp = (void __user *)arg;\n\tlong r;\n\n\tswitch (ioctl) {\n\tcase KVM_GET_MSR_INDEX_LIST: {\n\t\tstruct kvm_msr_list __user *user_msr_list = argp;\n\t\tstruct kvm_msr_list msr_list;\n\t\tunsigned n;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&msr_list, user_msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tn = msr_list.nmsrs;\n\t\tmsr_list.nmsrs = num_msrs_to_save + num_emulated_msrs;\n\t\tif (copy_to_user(user_msr_list, &msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tr = -E2BIG;\n\t\tif (n < msr_list.nmsrs)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(user_msr_list->indices, &msrs_to_save,\n\t\t\t\t num_msrs_to_save * sizeof(u32)))\n\t\t\tgoto out;\n\t\tif (copy_to_user(user_msr_list->indices + num_msrs_to_save,\n\t\t\t\t &emulated_msrs,\n\t\t\t\t num_emulated_msrs * sizeof(u32)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_SUPPORTED_CPUID:\n\tcase KVM_GET_EMULATED_CPUID: {\n\t\tstruct kvm_cpuid2 __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid2 cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\n\t\tr = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries,\n\t\t\t\t\t    ioctl);\n\t\tif (r)\n\t\t\tgoto out;\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(cpuid_arg, &cpuid, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_X86_GET_MCE_CAP_SUPPORTED:\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &kvm_caps.supported_mce_cap,\n\t\t\t\t sizeof(kvm_caps.supported_mce_cap)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_GET_MSR_FEATURE_INDEX_LIST: {\n\t\tstruct kvm_msr_list __user *user_msr_list = argp;\n\t\tstruct kvm_msr_list msr_list;\n\t\tunsigned int n;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&msr_list, user_msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tn = msr_list.nmsrs;\n\t\tmsr_list.nmsrs = num_msr_based_features;\n\t\tif (copy_to_user(user_msr_list, &msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tr = -E2BIG;\n\t\tif (n < msr_list.nmsrs)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(user_msr_list->indices, &msr_based_features,\n\t\t\t\t num_msr_based_features * sizeof(u32)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_MSRS:\n\t\tr = msr_io(NULL, argp, do_get_msr_feature, 1);\n\t\tbreak;\n\tcase KVM_GET_SUPPORTED_HV_CPUID:\n\t\tr = kvm_ioctl_get_supported_hv_cpuid(NULL, argp);\n\t\tbreak;\n\tcase KVM_GET_DEVICE_ATTR: {\n\t\tstruct kvm_device_attr attr;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))\n\t\t\tbreak;\n\t\tr = kvm_x86_dev_get_attr(&attr);\n\t\tbreak;\n\t}\n\tcase KVM_HAS_DEVICE_ATTR: {\n\t\tstruct kvm_device_attr attr;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))\n\t\t\tbreak;\n\t\tr = kvm_x86_dev_has_attr(&attr);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\nout:\n\treturn r;\n}\n\nstatic void wbinvd_ipi(void *garbage)\n{\n\twbinvd();\n}\n\nstatic bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_arch_has_noncoherent_dma(vcpu->kvm);\n}\n\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\t/* Address WBINVD may be executed by guest */\n\tif (need_emulate_wbinvd(vcpu)) {\n\t\tif (static_call(kvm_x86_has_wbinvd_exit)())\n\t\t\tcpumask_set_cpu(cpu, vcpu->arch.wbinvd_dirty_mask);\n\t\telse if (vcpu->cpu != -1 && vcpu->cpu != cpu)\n\t\t\tsmp_call_function_single(vcpu->cpu,\n\t\t\t\t\twbinvd_ipi, NULL, 1);\n\t}\n\n\tstatic_call(kvm_x86_vcpu_load)(vcpu, cpu);\n\n\t/* Save host pkru register if supported */\n\tvcpu->arch.host_pkru = read_pkru();\n\n\t/* Apply any externally detected TSC adjustments (due to suspend) */\n\tif (unlikely(vcpu->arch.tsc_offset_adjustment)) {\n\t\tadjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);\n\t\tvcpu->arch.tsc_offset_adjustment = 0;\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t}\n\n\tif (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {\n\t\ts64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :\n\t\t\t\trdtsc() - vcpu->arch.last_host_tsc;\n\t\tif (tsc_delta < 0)\n\t\t\tmark_tsc_unstable(\"KVM discovered backwards TSC\");\n\n\t\tif (kvm_check_tsc_unstable()) {\n\t\t\tu64 offset = kvm_compute_l1_tsc_offset(vcpu,\n\t\t\t\t\t\tvcpu->arch.last_guest_tsc);\n\t\t\tkvm_vcpu_write_tsc_offset(vcpu, offset);\n\t\t\tvcpu->arch.tsc_catchup = 1;\n\t\t}\n\n\t\tif (kvm_lapic_hv_timer_in_use(vcpu))\n\t\t\tkvm_lapic_restart_hv_timer(vcpu);\n\n\t\t/*\n\t\t * On a host with synchronized TSC, there is no need to update\n\t\t * kvmclock on vcpu->cpu migration\n\t\t */\n\t\tif (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)\n\t\t\tkvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);\n\t\tif (vcpu->cpu != cpu)\n\t\t\tkvm_make_request(KVM_REQ_MIGRATE_TIMER, vcpu);\n\t\tvcpu->cpu = cpu;\n\t}\n\n\tkvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);\n}\n\nstatic void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)\n{\n\tstruct gfn_to_hva_cache *ghc = &vcpu->arch.st.cache;\n\tstruct kvm_steal_time __user *st;\n\tstruct kvm_memslots *slots;\n\tstatic const u8 preempted = KVM_VCPU_PREEMPTED;\n\tgpa_t gpa = vcpu->arch.st.msr_val & KVM_STEAL_VALID_BITS;\n\n\t/*\n\t * The vCPU can be marked preempted if and only if the VM-Exit was on\n\t * an instruction boundary and will not trigger guest emulation of any\n\t * kind (see vcpu_run).  Vendor specific code controls (conservatively)\n\t * when this is true, for example allowing the vCPU to be marked\n\t * preempted if and only if the VM-Exit was due to a host interrupt.\n\t */\n\tif (!vcpu->arch.at_instruction_boundary) {\n\t\tvcpu->stat.preemption_other++;\n\t\treturn;\n\t}\n\n\tvcpu->stat.preemption_reported++;\n\tif (!(vcpu->arch.st.msr_val & KVM_MSR_ENABLED))\n\t\treturn;\n\n\tif (vcpu->arch.st.preempted)\n\t\treturn;\n\n\t/* This happens on process exit */\n\tif (unlikely(current->mm != vcpu->kvm->mm))\n\t\treturn;\n\n\tslots = kvm_memslots(vcpu->kvm);\n\n\tif (unlikely(slots->generation != ghc->generation ||\n\t\t     gpa != ghc->gpa ||\n\t\t     kvm_is_error_hva(ghc->hva) || !ghc->memslot))\n\t\treturn;\n\n\tst = (struct kvm_steal_time __user *)ghc->hva;\n\tBUILD_BUG_ON(sizeof(st->preempted) != sizeof(preempted));\n\n\tif (!copy_to_user_nofault(&st->preempted, &preempted, sizeof(preempted)))\n\t\tvcpu->arch.st.preempted = KVM_VCPU_PREEMPTED;\n\n\tmark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));\n}\n\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tint idx;\n\n\tif (vcpu->preempted) {\n\t\tif (!vcpu->arch.guest_state_protected)\n\t\t\tvcpu->arch.preempted_in_kernel = !static_call(kvm_x86_get_cpl)(vcpu);\n\n\t\t/*\n\t\t * Take the srcu lock as memslots will be accessed to check the gfn\n\t\t * cache generation against the memslots generation.\n\t\t */\n\t\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tif (kvm_xen_msr_enabled(vcpu->kvm))\n\t\t\tkvm_xen_runstate_set_preempted(vcpu);\n\t\telse\n\t\t\tkvm_steal_time_set_preempted(vcpu);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t}\n\n\tstatic_call(kvm_x86_vcpu_put)(vcpu);\n\tvcpu->arch.last_host_tsc = rdtsc();\n}\n\nstatic int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_lapic_state *s)\n{\n\tstatic_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);\n\n\treturn kvm_apic_get_state(vcpu, s);\n}\n\nstatic int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_lapic_state *s)\n{\n\tint r;\n\n\tr = kvm_apic_set_state(vcpu, s);\n\tif (r)\n\t\treturn r;\n\tupdate_cr8_intercept(vcpu);\n\n\treturn 0;\n}\n\nstatic int kvm_cpu_accept_dm_intr(struct kvm_vcpu *vcpu)\n{\n\t/*\n\t * We can accept userspace's request for interrupt injection\n\t * as long as we have a place to store the interrupt number.\n\t * The actual injection will happen when the CPU is able to\n\t * deliver the interrupt.\n\t */\n\tif (kvm_cpu_has_extint(vcpu))\n\t\treturn false;\n\n\t/* Acknowledging ExtINT does not happen if LINT0 is masked.  */\n\treturn (!lapic_in_kernel(vcpu) ||\n\t\tkvm_apic_accept_pic_intr(vcpu));\n}\n\nstatic int kvm_vcpu_ready_for_interrupt_injection(struct kvm_vcpu *vcpu)\n{\n\t/*\n\t * Do not cause an interrupt window exit if an exception\n\t * is pending or an event needs reinjection; userspace\n\t * might want to inject the interrupt manually using KVM_SET_REGS\n\t * or KVM_SET_SREGS.  For that to work, we must be at an\n\t * instruction boundary and with no events half-injected.\n\t */\n\treturn (kvm_arch_interrupt_allowed(vcpu) &&\n\t\tkvm_cpu_accept_dm_intr(vcpu) &&\n\t\t!kvm_event_needs_reinjection(vcpu) &&\n\t\t!kvm_is_exception_pending(vcpu));\n}\n\nstatic int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_interrupt *irq)\n{\n\tif (irq->irq >= KVM_NR_INTERRUPTS)\n\t\treturn -EINVAL;\n\n\tif (!irqchip_in_kernel(vcpu->kvm)) {\n\t\tkvm_queue_interrupt(vcpu, irq->irq, false);\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * With in-kernel LAPIC, we only use this to inject EXTINT, so\n\t * fail for in-kernel 8259.\n\t */\n\tif (pic_in_kernel(vcpu->kvm))\n\t\treturn -ENXIO;\n\n\tif (vcpu->arch.pending_external_vector != -1)\n\t\treturn -EEXIST;\n\n\tvcpu->arch.pending_external_vector = irq->irq;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_nmi(struct kvm_vcpu *vcpu)\n{\n\tkvm_inject_nmi(vcpu);\n\n\treturn 0;\n}\n\nstatic int vcpu_ioctl_tpr_access_reporting(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   struct kvm_tpr_access_ctl *tac)\n{\n\tif (tac->flags)\n\t\treturn -EINVAL;\n\tvcpu->arch.tpr_access_reporting = !!tac->enabled;\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_x86_setup_mce(struct kvm_vcpu *vcpu,\n\t\t\t\t\tu64 mcg_cap)\n{\n\tint r;\n\tunsigned bank_num = mcg_cap & 0xff, bank;\n\n\tr = -EINVAL;\n\tif (!bank_num || bank_num > KVM_MAX_MCE_BANKS)\n\t\tgoto out;\n\tif (mcg_cap & ~(kvm_caps.supported_mce_cap | 0xff | 0xff0000))\n\t\tgoto out;\n\tr = 0;\n\tvcpu->arch.mcg_cap = mcg_cap;\n\t/* Init IA32_MCG_CTL to all 1s */\n\tif (mcg_cap & MCG_CTL_P)\n\t\tvcpu->arch.mcg_ctl = ~(u64)0;\n\t/* Init IA32_MCi_CTL to all 1s, IA32_MCi_CTL2 to all 0s */\n\tfor (bank = 0; bank < bank_num; bank++) {\n\t\tvcpu->arch.mce_banks[bank*4] = ~(u64)0;\n\t\tif (mcg_cap & MCG_CMCI_P)\n\t\t\tvcpu->arch.mci_ctl2_banks[bank] = 0;\n\t}\n\n\tkvm_apic_after_set_mcg_cap(vcpu);\n\n\tstatic_call(kvm_x86_setup_mce)(vcpu);\nout:\n\treturn r;\n}\n\n/*\n * Validate this is an UCNA (uncorrectable no action) error by checking the\n * MCG_STATUS and MCi_STATUS registers:\n * - none of the bits for Machine Check Exceptions are set\n * - both the VAL (valid) and UC (uncorrectable) bits are set\n * MCI_STATUS_PCC - Processor Context Corrupted\n * MCI_STATUS_S - Signaled as a Machine Check Exception\n * MCI_STATUS_AR - Software recoverable Action Required\n */\nstatic bool is_ucna(struct kvm_x86_mce *mce)\n{\n\treturn\t!mce->mcg_status &&\n\t\t!(mce->status & (MCI_STATUS_PCC | MCI_STATUS_S | MCI_STATUS_AR)) &&\n\t\t(mce->status & MCI_STATUS_VAL) &&\n\t\t(mce->status & MCI_STATUS_UC);\n}\n\nstatic int kvm_vcpu_x86_set_ucna(struct kvm_vcpu *vcpu, struct kvm_x86_mce *mce, u64* banks)\n{\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\n\tbanks[1] = mce->status;\n\tbanks[2] = mce->addr;\n\tbanks[3] = mce->misc;\n\tvcpu->arch.mcg_status = mce->mcg_status;\n\n\tif (!(mcg_cap & MCG_CMCI_P) ||\n\t    !(vcpu->arch.mci_ctl2_banks[mce->bank] & MCI_CTL2_CMCI_EN))\n\t\treturn 0;\n\n\tif (lapic_in_kernel(vcpu))\n\t\tkvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);\n\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_mce(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct kvm_x86_mce *mce)\n{\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\tunsigned bank_num = mcg_cap & 0xff;\n\tu64 *banks = vcpu->arch.mce_banks;\n\n\tif (mce->bank >= bank_num || !(mce->status & MCI_STATUS_VAL))\n\t\treturn -EINVAL;\n\n\tbanks += array_index_nospec(4 * mce->bank, 4 * bank_num);\n\n\tif (is_ucna(mce))\n\t\treturn kvm_vcpu_x86_set_ucna(vcpu, mce, banks);\n\n\t/*\n\t * if IA32_MCG_CTL is not all 1s, the uncorrected error\n\t * reporting is disabled\n\t */\n\tif ((mce->status & MCI_STATUS_UC) && (mcg_cap & MCG_CTL_P) &&\n\t    vcpu->arch.mcg_ctl != ~(u64)0)\n\t\treturn 0;\n\t/*\n\t * if IA32_MCi_CTL is not all 1s, the uncorrected error\n\t * reporting is disabled for the bank\n\t */\n\tif ((mce->status & MCI_STATUS_UC) && banks[0] != ~(u64)0)\n\t\treturn 0;\n\tif (mce->status & MCI_STATUS_UC) {\n\t\tif ((vcpu->arch.mcg_status & MCG_STATUS_MCIP) ||\n\t\t    !kvm_read_cr4_bits(vcpu, X86_CR4_MCE)) {\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\t\treturn 0;\n\t\t}\n\t\tif (banks[1] & MCI_STATUS_VAL)\n\t\t\tmce->status |= MCI_STATUS_OVER;\n\t\tbanks[2] = mce->addr;\n\t\tbanks[3] = mce->misc;\n\t\tvcpu->arch.mcg_status = mce->mcg_status;\n\t\tbanks[1] = mce->status;\n\t\tkvm_queue_exception(vcpu, MC_VECTOR);\n\t} else if (!(banks[1] & MCI_STATUS_VAL)\n\t\t   || !(banks[1] & MCI_STATUS_UC)) {\n\t\tif (banks[1] & MCI_STATUS_VAL)\n\t\t\tmce->status |= MCI_STATUS_OVER;\n\t\tbanks[2] = mce->addr;\n\t\tbanks[3] = mce->misc;\n\t\tbanks[1] = mce->status;\n\t} else\n\t\tbanks[1] |= MCI_STATUS_OVER;\n\treturn 0;\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_vcpu_events(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       struct kvm_vcpu_events *events)\n{\n\tstruct kvm_queued_exception *ex;\n\n\tprocess_nmi(vcpu);\n\n#ifdef CONFIG_KVM_SMM\n\tif (kvm_check_request(KVM_REQ_SMI, vcpu))\n\t\tprocess_smi(vcpu);\n#endif\n\n\t/*\n\t * KVM's ABI only allows for one exception to be migrated.  Luckily,\n\t * the only time there can be two queued exceptions is if there's a\n\t * non-exiting _injected_ exception, and a pending exiting exception.\n\t * In that case, ignore the VM-Exiting exception as it's an extension\n\t * of the injected exception.\n\t */\n\tif (vcpu->arch.exception_vmexit.pending &&\n\t    !vcpu->arch.exception.pending &&\n\t    !vcpu->arch.exception.injected)\n\t\tex = &vcpu->arch.exception_vmexit;\n\telse\n\t\tex = &vcpu->arch.exception;\n\n\t/*\n\t * In guest mode, payload delivery should be deferred if the exception\n\t * will be intercepted by L1, e.g. KVM should not modifying CR2 if L1\n\t * intercepts #PF, ditto for DR6 and #DBs.  If the per-VM capability,\n\t * KVM_CAP_EXCEPTION_PAYLOAD, is not set, userspace may or may not\n\t * propagate the payload and so it cannot be safely deferred.  Deliver\n\t * the payload if the capability hasn't been requested.\n\t */\n\tif (!vcpu->kvm->arch.exception_payload_enabled &&\n\t    ex->pending && ex->has_payload)\n\t\tkvm_deliver_exception_payload(vcpu, ex);\n\n\tmemset(events, 0, sizeof(*events));\n\n\t/*\n\t * The API doesn't provide the instruction length for software\n\t * exceptions, so don't report them. As long as the guest RIP\n\t * isn't advanced, we should expect to encounter the exception\n\t * again.\n\t */\n\tif (!kvm_exception_is_soft(ex->vector)) {\n\t\tevents->exception.injected = ex->injected;\n\t\tevents->exception.pending = ex->pending;\n\t\t/*\n\t\t * For ABI compatibility, deliberately conflate\n\t\t * pending and injected exceptions when\n\t\t * KVM_CAP_EXCEPTION_PAYLOAD isn't enabled.\n\t\t */\n\t\tif (!vcpu->kvm->arch.exception_payload_enabled)\n\t\t\tevents->exception.injected |= ex->pending;\n\t}\n\tevents->exception.nr = ex->vector;\n\tevents->exception.has_error_code = ex->has_error_code;\n\tevents->exception.error_code = ex->error_code;\n\tevents->exception_has_payload = ex->has_payload;\n\tevents->exception_payload = ex->payload;\n\n\tevents->interrupt.injected =\n\t\tvcpu->arch.interrupt.injected && !vcpu->arch.interrupt.soft;\n\tevents->interrupt.nr = vcpu->arch.interrupt.nr;\n\tevents->interrupt.shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);\n\n\tevents->nmi.injected = vcpu->arch.nmi_injected;\n\tevents->nmi.pending = vcpu->arch.nmi_pending != 0;\n\tevents->nmi.masked = static_call(kvm_x86_get_nmi_mask)(vcpu);\n\n\t/* events->sipi_vector is never valid when reporting to user space */\n\n#ifdef CONFIG_KVM_SMM\n\tevents->smi.smm = is_smm(vcpu);\n\tevents->smi.pending = vcpu->arch.smi_pending;\n\tevents->smi.smm_inside_nmi =\n\t\t!!(vcpu->arch.hflags & HF_SMM_INSIDE_NMI_MASK);\n#endif\n\tevents->smi.latched_init = kvm_lapic_latched_init(vcpu);\n\n\tevents->flags = (KVM_VCPUEVENT_VALID_NMI_PENDING\n\t\t\t | KVM_VCPUEVENT_VALID_SHADOW\n\t\t\t | KVM_VCPUEVENT_VALID_SMM);\n\tif (vcpu->kvm->arch.exception_payload_enabled)\n\t\tevents->flags |= KVM_VCPUEVENT_VALID_PAYLOAD;\n\tif (vcpu->kvm->arch.triple_fault_event) {\n\t\tevents->triple_fault.pending = kvm_test_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\tevents->flags |= KVM_VCPUEVENT_VALID_TRIPLE_FAULT;\n\t}\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,\n\t\t\t\t\t      struct kvm_vcpu_events *events)\n{\n\tif (events->flags & ~(KVM_VCPUEVENT_VALID_NMI_PENDING\n\t\t\t      | KVM_VCPUEVENT_VALID_SIPI_VECTOR\n\t\t\t      | KVM_VCPUEVENT_VALID_SHADOW\n\t\t\t      | KVM_VCPUEVENT_VALID_SMM\n\t\t\t      | KVM_VCPUEVENT_VALID_PAYLOAD\n\t\t\t      | KVM_VCPUEVENT_VALID_TRIPLE_FAULT))\n\t\treturn -EINVAL;\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_PAYLOAD) {\n\t\tif (!vcpu->kvm->arch.exception_payload_enabled)\n\t\t\treturn -EINVAL;\n\t\tif (events->exception.pending)\n\t\t\tevents->exception.injected = 0;\n\t\telse\n\t\t\tevents->exception_has_payload = 0;\n\t} else {\n\t\tevents->exception.pending = 0;\n\t\tevents->exception_has_payload = 0;\n\t}\n\n\tif ((events->exception.injected || events->exception.pending) &&\n\t    (events->exception.nr > 31 || events->exception.nr == NMI_VECTOR))\n\t\treturn -EINVAL;\n\n\t/* INITs are latched while in SMM */\n\tif (events->flags & KVM_VCPUEVENT_VALID_SMM &&\n\t    (events->smi.smm || events->smi.pending) &&\n\t    vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED)\n\t\treturn -EINVAL;\n\n\tprocess_nmi(vcpu);\n\n\t/*\n\t * Flag that userspace is stuffing an exception, the next KVM_RUN will\n\t * morph the exception to a VM-Exit if appropriate.  Do this only for\n\t * pending exceptions, already-injected exceptions are not subject to\n\t * intercpetion.  Note, userspace that conflates pending and injected\n\t * is hosed, and will incorrectly convert an injected exception into a\n\t * pending exception, which in turn may cause a spurious VM-Exit.\n\t */\n\tvcpu->arch.exception_from_userspace = events->exception.pending;\n\n\tvcpu->arch.exception_vmexit.pending = false;\n\n\tvcpu->arch.exception.injected = events->exception.injected;\n\tvcpu->arch.exception.pending = events->exception.pending;\n\tvcpu->arch.exception.vector = events->exception.nr;\n\tvcpu->arch.exception.has_error_code = events->exception.has_error_code;\n\tvcpu->arch.exception.error_code = events->exception.error_code;\n\tvcpu->arch.exception.has_payload = events->exception_has_payload;\n\tvcpu->arch.exception.payload = events->exception_payload;\n\n\tvcpu->arch.interrupt.injected = events->interrupt.injected;\n\tvcpu->arch.interrupt.nr = events->interrupt.nr;\n\tvcpu->arch.interrupt.soft = events->interrupt.soft;\n\tif (events->flags & KVM_VCPUEVENT_VALID_SHADOW)\n\t\tstatic_call(kvm_x86_set_interrupt_shadow)(vcpu,\n\t\t\t\t\t\tevents->interrupt.shadow);\n\n\tvcpu->arch.nmi_injected = events->nmi.injected;\n\tif (events->flags & KVM_VCPUEVENT_VALID_NMI_PENDING)\n\t\tvcpu->arch.nmi_pending = events->nmi.pending;\n\tstatic_call(kvm_x86_set_nmi_mask)(vcpu, events->nmi.masked);\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_SIPI_VECTOR &&\n\t    lapic_in_kernel(vcpu))\n\t\tvcpu->arch.apic->sipi_vector = events->sipi_vector;\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_SMM) {\n#ifdef CONFIG_KVM_SMM\n\t\tif (!!(vcpu->arch.hflags & HF_SMM_MASK) != events->smi.smm) {\n\t\t\tkvm_leave_nested(vcpu);\n\t\t\tkvm_smm_changed(vcpu, events->smi.smm);\n\t\t}\n\n\t\tvcpu->arch.smi_pending = events->smi.pending;\n\n\t\tif (events->smi.smm) {\n\t\t\tif (events->smi.smm_inside_nmi)\n\t\t\t\tvcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;\n\t\t\telse\n\t\t\t\tvcpu->arch.hflags &= ~HF_SMM_INSIDE_NMI_MASK;\n\t\t}\n\n#else\n\t\tif (events->smi.smm || events->smi.pending ||\n\t\t    events->smi.smm_inside_nmi)\n\t\t\treturn -EINVAL;\n#endif\n\n\t\tif (lapic_in_kernel(vcpu)) {\n\t\t\tif (events->smi.latched_init)\n\t\t\t\tset_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);\n\t\t\telse\n\t\t\t\tclear_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);\n\t\t}\n\t}\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_TRIPLE_FAULT) {\n\t\tif (!vcpu->kvm->arch.triple_fault_event)\n\t\t\treturn -EINVAL;\n\t\tif (events->triple_fault.pending)\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\telse\n\t\t\tkvm_clear_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t}\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\treturn 0;\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_debugregs(struct kvm_vcpu *vcpu,\n\t\t\t\t\t     struct kvm_debugregs *dbgregs)\n{\n\tunsigned long val;\n\n\tmemset(dbgregs, 0, sizeof(*dbgregs));\n\tmemcpy(dbgregs->db, vcpu->arch.db, sizeof(vcpu->arch.db));\n\tkvm_get_dr(vcpu, 6, &val);\n\tdbgregs->dr6 = val;\n\tdbgregs->dr7 = vcpu->arch.dr7;\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_debugregs(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    struct kvm_debugregs *dbgregs)\n{\n\tif (dbgregs->flags)\n\t\treturn -EINVAL;\n\n\tif (!kvm_dr6_valid(dbgregs->dr6))\n\t\treturn -EINVAL;\n\tif (!kvm_dr7_valid(dbgregs->dr7))\n\t\treturn -EINVAL;\n\n\tmemcpy(vcpu->arch.db, dbgregs->db, sizeof(vcpu->arch.db));\n\tkvm_update_dr0123(vcpu);\n\tvcpu->arch.dr6 = dbgregs->dr6;\n\tvcpu->arch.dr7 = dbgregs->dr7;\n\tkvm_update_dr7(vcpu);\n\n\treturn 0;\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_xsave(struct kvm_vcpu *vcpu,\n\t\t\t\t\t struct kvm_xsave *guest_xsave)\n{\n\tif (fpstate_is_confidential(&vcpu->arch.guest_fpu))\n\t\treturn;\n\n\tfpu_copy_guest_fpstate_to_uabi(&vcpu->arch.guest_fpu,\n\t\t\t\t       guest_xsave->region,\n\t\t\t\t       sizeof(guest_xsave->region),\n\t\t\t\t       vcpu->arch.pkru);\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_xsave2(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  u8 *state, unsigned int size)\n{\n\tif (fpstate_is_confidential(&vcpu->arch.guest_fpu))\n\t\treturn;\n\n\tfpu_copy_guest_fpstate_to_uabi(&vcpu->arch.guest_fpu,\n\t\t\t\t       state, size, vcpu->arch.pkru);\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_xsave(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_xsave *guest_xsave)\n{\n\tif (fpstate_is_confidential(&vcpu->arch.guest_fpu))\n\t\treturn 0;\n\n\treturn fpu_copy_uabi_to_guest_fpstate(&vcpu->arch.guest_fpu,\n\t\t\t\t\t      guest_xsave->region,\n\t\t\t\t\t      kvm_caps.supported_xcr0,\n\t\t\t\t\t      &vcpu->arch.pkru);\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_xcrs(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_xcrs *guest_xcrs)\n{\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE)) {\n\t\tguest_xcrs->nr_xcrs = 0;\n\t\treturn;\n\t}\n\n\tguest_xcrs->nr_xcrs = 1;\n\tguest_xcrs->flags = 0;\n\tguest_xcrs->xcrs[0].xcr = XCR_XFEATURE_ENABLED_MASK;\n\tguest_xcrs->xcrs[0].value = vcpu->arch.xcr0;\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_xcrs(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct kvm_xcrs *guest_xcrs)\n{\n\tint i, r = 0;\n\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE))\n\t\treturn -EINVAL;\n\n\tif (guest_xcrs->nr_xcrs > KVM_MAX_XCRS || guest_xcrs->flags)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < guest_xcrs->nr_xcrs; i++)\n\t\t/* Only support XCR0 currently */\n\t\tif (guest_xcrs->xcrs[i].xcr == XCR_XFEATURE_ENABLED_MASK) {\n\t\t\tr = __kvm_set_xcr(vcpu, XCR_XFEATURE_ENABLED_MASK,\n\t\t\t\tguest_xcrs->xcrs[i].value);\n\t\t\tbreak;\n\t\t}\n\tif (r)\n\t\tr = -EINVAL;\n\treturn r;\n}\n\n/*\n * kvm_set_guest_paused() indicates to the guest kernel that it has been\n * stopped by the hypervisor.  This function will be called from the host only.\n * EINVAL is returned when the host attempts to set the flag for a guest that\n * does not support pv clocks.\n */\nstatic int kvm_set_guest_paused(struct kvm_vcpu *vcpu)\n{\n\tif (!vcpu->arch.pv_time.active)\n\t\treturn -EINVAL;\n\tvcpu->arch.pvclock_set_guest_stopped_request = true;\n\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\treturn 0;\n}\n\nstatic int kvm_arch_tsc_has_attr(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_device_attr *attr)\n{\n\tint r;\n\n\tswitch (attr->attr) {\n\tcase KVM_VCPU_TSC_OFFSET:\n\t\tr = 0;\n\t\tbreak;\n\tdefault:\n\t\tr = -ENXIO;\n\t}\n\n\treturn r;\n}\n\nstatic int kvm_arch_tsc_get_attr(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_device_attr *attr)\n{\n\tu64 __user *uaddr = kvm_get_attr_addr(attr);\n\tint r;\n\n\tif (IS_ERR(uaddr))\n\t\treturn PTR_ERR(uaddr);\n\n\tswitch (attr->attr) {\n\tcase KVM_VCPU_TSC_OFFSET:\n\t\tr = -EFAULT;\n\t\tif (put_user(vcpu->arch.l1_tsc_offset, uaddr))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\tdefault:\n\t\tr = -ENXIO;\n\t}\n\n\treturn r;\n}\n\nstatic int kvm_arch_tsc_set_attr(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_device_attr *attr)\n{\n\tu64 __user *uaddr = kvm_get_attr_addr(attr);\n\tstruct kvm *kvm = vcpu->kvm;\n\tint r;\n\n\tif (IS_ERR(uaddr))\n\t\treturn PTR_ERR(uaddr);\n\n\tswitch (attr->attr) {\n\tcase KVM_VCPU_TSC_OFFSET: {\n\t\tu64 offset, tsc, ns;\n\t\tunsigned long flags;\n\t\tbool matched;\n\n\t\tr = -EFAULT;\n\t\tif (get_user(offset, uaddr))\n\t\t\tbreak;\n\n\t\traw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);\n\n\t\tmatched = (vcpu->arch.virtual_tsc_khz &&\n\t\t\t   kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&\n\t\t\t   kvm->arch.last_tsc_offset == offset);\n\n\t\ttsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio) + offset;\n\t\tns = get_kvmclock_base_ns();\n\n\t\t__kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched);\n\t\traw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);\n\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = -ENXIO;\n\t}\n\n\treturn r;\n}\n\nstatic int kvm_vcpu_ioctl_device_attr(struct kvm_vcpu *vcpu,\n\t\t\t\t      unsigned int ioctl,\n\t\t\t\t      void __user *argp)\n{\n\tstruct kvm_device_attr attr;\n\tint r;\n\n\tif (copy_from_user(&attr, argp, sizeof(attr)))\n\t\treturn -EFAULT;\n\n\tif (attr.group != KVM_VCPU_TSC_CTRL)\n\t\treturn -ENXIO;\n\n\tswitch (ioctl) {\n\tcase KVM_HAS_DEVICE_ATTR:\n\t\tr = kvm_arch_tsc_has_attr(vcpu, &attr);\n\t\tbreak;\n\tcase KVM_GET_DEVICE_ATTR:\n\t\tr = kvm_arch_tsc_get_attr(vcpu, &attr);\n\t\tbreak;\n\tcase KVM_SET_DEVICE_ATTR:\n\t\tr = kvm_arch_tsc_set_attr(vcpu, &attr);\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nstatic int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_enable_cap *cap)\n{\n\tint r;\n\tuint16_t vmcs_version;\n\tvoid __user *user_ptr;\n\n\tif (cap->flags)\n\t\treturn -EINVAL;\n\n\tswitch (cap->cap) {\n\tcase KVM_CAP_HYPERV_SYNIC2:\n\t\tif (cap->args[0])\n\t\t\treturn -EINVAL;\n\t\tfallthrough;\n\n\tcase KVM_CAP_HYPERV_SYNIC:\n\t\tif (!irqchip_in_kernel(vcpu->kvm))\n\t\t\treturn -EINVAL;\n\t\treturn kvm_hv_activate_synic(vcpu, cap->cap ==\n\t\t\t\t\t     KVM_CAP_HYPERV_SYNIC2);\n\tcase KVM_CAP_HYPERV_ENLIGHTENED_VMCS:\n\t\tif (!kvm_x86_ops.nested_ops->enable_evmcs)\n\t\t\treturn -ENOTTY;\n\t\tr = kvm_x86_ops.nested_ops->enable_evmcs(vcpu, &vmcs_version);\n\t\tif (!r) {\n\t\t\tuser_ptr = (void __user *)(uintptr_t)cap->args[0];\n\t\t\tif (copy_to_user(user_ptr, &vmcs_version,\n\t\t\t\t\t sizeof(vmcs_version)))\n\t\t\t\tr = -EFAULT;\n\t\t}\n\t\treturn r;\n\tcase KVM_CAP_HYPERV_DIRECT_TLBFLUSH:\n\t\tif (!kvm_x86_ops.enable_l2_tlb_flush)\n\t\t\treturn -ENOTTY;\n\n\t\treturn static_call(kvm_x86_enable_l2_tlb_flush)(vcpu);\n\n\tcase KVM_CAP_HYPERV_ENFORCE_CPUID:\n\t\treturn kvm_hv_set_enforce_cpuid(vcpu, cap->args[0]);\n\n\tcase KVM_CAP_ENFORCE_PV_FEATURE_CPUID:\n\t\tvcpu->arch.pv_cpuid.enforce = cap->args[0];\n\t\tif (vcpu->arch.pv_cpuid.enforce)\n\t\t\tkvm_update_pv_runtime(vcpu);\n\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nlong kvm_arch_vcpu_ioctl(struct file *filp,\n\t\t\t unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\tunion {\n\t\tstruct kvm_sregs2 *sregs2;\n\t\tstruct kvm_lapic_state *lapic;\n\t\tstruct kvm_xsave *xsave;\n\t\tstruct kvm_xcrs *xcrs;\n\t\tvoid *buffer;\n\t} u;\n\n\tvcpu_load(vcpu);\n\n\tu.buffer = NULL;\n\tswitch (ioctl) {\n\tcase KVM_GET_LAPIC: {\n\t\tr = -EINVAL;\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tu.lapic = kzalloc(sizeof(struct kvm_lapic_state),\n\t\t\t\tGFP_KERNEL_ACCOUNT);\n\n\t\tr = -ENOMEM;\n\t\tif (!u.lapic)\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_get_lapic(vcpu, u.lapic);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.lapic, sizeof(struct kvm_lapic_state)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_LAPIC: {\n\t\tr = -EINVAL;\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tu.lapic = memdup_user(argp, sizeof(*u.lapic));\n\t\tif (IS_ERR(u.lapic)) {\n\t\t\tr = PTR_ERR(u.lapic);\n\t\t\tgoto out_nofree;\n\t\t}\n\n\t\tr = kvm_vcpu_ioctl_set_lapic(vcpu, u.lapic);\n\t\tbreak;\n\t}\n\tcase KVM_INTERRUPT: {\n\t\tstruct kvm_interrupt irq;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&irq, argp, sizeof(irq)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_interrupt(vcpu, &irq);\n\t\tbreak;\n\t}\n\tcase KVM_NMI: {\n\t\tr = kvm_vcpu_ioctl_nmi(vcpu);\n\t\tbreak;\n\t}\n\tcase KVM_SMI: {\n\t\tr = kvm_inject_smi(vcpu);\n\t\tbreak;\n\t}\n\tcase KVM_SET_CPUID: {\n\t\tstruct kvm_cpuid __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_set_cpuid(vcpu, &cpuid, cpuid_arg->entries);\n\t\tbreak;\n\t}\n\tcase KVM_SET_CPUID2: {\n\t\tstruct kvm_cpuid2 __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid2 cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_set_cpuid2(vcpu, &cpuid,\n\t\t\t\t\t      cpuid_arg->entries);\n\t\tbreak;\n\t}\n\tcase KVM_GET_CPUID2: {\n\t\tstruct kvm_cpuid2 __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid2 cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_get_cpuid2(vcpu, &cpuid,\n\t\t\t\t\t      cpuid_arg->entries);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(cpuid_arg, &cpuid, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_MSRS: {\n\t\tint idx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = msr_io(vcpu, argp, do_get_msr, 1);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_SET_MSRS: {\n\t\tint idx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = msr_io(vcpu, argp, do_set_msr, 0);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_TPR_ACCESS_REPORTING: {\n\t\tstruct kvm_tpr_access_ctl tac;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&tac, argp, sizeof(tac)))\n\t\t\tgoto out;\n\t\tr = vcpu_ioctl_tpr_access_reporting(vcpu, &tac);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &tac, sizeof(tac)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t};\n\tcase KVM_SET_VAPIC_ADDR: {\n\t\tstruct kvm_vapic_addr va;\n\t\tint idx;\n\n\t\tr = -EINVAL;\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&va, argp, sizeof(va)))\n\t\t\tgoto out;\n\t\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = kvm_lapic_set_vapic_addr(vcpu, va.vapic_addr);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_X86_SETUP_MCE: {\n\t\tu64 mcg_cap;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&mcg_cap, argp, sizeof(mcg_cap)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_x86_setup_mce(vcpu, mcg_cap);\n\t\tbreak;\n\t}\n\tcase KVM_X86_SET_MCE: {\n\t\tstruct kvm_x86_mce mce;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&mce, argp, sizeof(mce)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_x86_set_mce(vcpu, &mce);\n\t\tbreak;\n\t}\n\tcase KVM_GET_VCPU_EVENTS: {\n\t\tstruct kvm_vcpu_events events;\n\n\t\tkvm_vcpu_ioctl_x86_get_vcpu_events(vcpu, &events);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &events, sizeof(struct kvm_vcpu_events)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_VCPU_EVENTS: {\n\t\tstruct kvm_vcpu_events events;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&events, argp, sizeof(struct kvm_vcpu_events)))\n\t\t\tbreak;\n\n\t\tr = kvm_vcpu_ioctl_x86_set_vcpu_events(vcpu, &events);\n\t\tbreak;\n\t}\n\tcase KVM_GET_DEBUGREGS: {\n\t\tstruct kvm_debugregs dbgregs;\n\n\t\tkvm_vcpu_ioctl_x86_get_debugregs(vcpu, &dbgregs);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &dbgregs,\n\t\t\t\t sizeof(struct kvm_debugregs)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_DEBUGREGS: {\n\t\tstruct kvm_debugregs dbgregs;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&dbgregs, argp,\n\t\t\t\t   sizeof(struct kvm_debugregs)))\n\t\t\tbreak;\n\n\t\tr = kvm_vcpu_ioctl_x86_set_debugregs(vcpu, &dbgregs);\n\t\tbreak;\n\t}\n\tcase KVM_GET_XSAVE: {\n\t\tr = -EINVAL;\n\t\tif (vcpu->arch.guest_fpu.uabi_size > sizeof(struct kvm_xsave))\n\t\t\tbreak;\n\n\t\tu.xsave = kzalloc(sizeof(struct kvm_xsave), GFP_KERNEL_ACCOUNT);\n\t\tr = -ENOMEM;\n\t\tif (!u.xsave)\n\t\t\tbreak;\n\n\t\tkvm_vcpu_ioctl_x86_get_xsave(vcpu, u.xsave);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.xsave, sizeof(struct kvm_xsave)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_XSAVE: {\n\t\tint size = vcpu->arch.guest_fpu.uabi_size;\n\n\t\tu.xsave = memdup_user(argp, size);\n\t\tif (IS_ERR(u.xsave)) {\n\t\t\tr = PTR_ERR(u.xsave);\n\t\t\tgoto out_nofree;\n\t\t}\n\n\t\tr = kvm_vcpu_ioctl_x86_set_xsave(vcpu, u.xsave);\n\t\tbreak;\n\t}\n\n\tcase KVM_GET_XSAVE2: {\n\t\tint size = vcpu->arch.guest_fpu.uabi_size;\n\n\t\tu.xsave = kzalloc(size, GFP_KERNEL_ACCOUNT);\n\t\tr = -ENOMEM;\n\t\tif (!u.xsave)\n\t\t\tbreak;\n\n\t\tkvm_vcpu_ioctl_x86_get_xsave2(vcpu, u.buffer, size);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.xsave, size))\n\t\t\tbreak;\n\n\t\tr = 0;\n\t\tbreak;\n\t}\n\n\tcase KVM_GET_XCRS: {\n\t\tu.xcrs = kzalloc(sizeof(struct kvm_xcrs), GFP_KERNEL_ACCOUNT);\n\t\tr = -ENOMEM;\n\t\tif (!u.xcrs)\n\t\t\tbreak;\n\n\t\tkvm_vcpu_ioctl_x86_get_xcrs(vcpu, u.xcrs);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.xcrs,\n\t\t\t\t sizeof(struct kvm_xcrs)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_XCRS: {\n\t\tu.xcrs = memdup_user(argp, sizeof(*u.xcrs));\n\t\tif (IS_ERR(u.xcrs)) {\n\t\t\tr = PTR_ERR(u.xcrs);\n\t\t\tgoto out_nofree;\n\t\t}\n\n\t\tr = kvm_vcpu_ioctl_x86_set_xcrs(vcpu, u.xcrs);\n\t\tbreak;\n\t}\n\tcase KVM_SET_TSC_KHZ: {\n\t\tu32 user_tsc_khz;\n\n\t\tr = -EINVAL;\n\t\tuser_tsc_khz = (u32)arg;\n\n\t\tif (kvm_caps.has_tsc_control &&\n\t\t    user_tsc_khz >= kvm_caps.max_guest_tsc_khz)\n\t\t\tgoto out;\n\n\t\tif (user_tsc_khz == 0)\n\t\t\tuser_tsc_khz = tsc_khz;\n\n\t\tif (!kvm_set_tsc_khz(vcpu, user_tsc_khz))\n\t\t\tr = 0;\n\n\t\tgoto out;\n\t}\n\tcase KVM_GET_TSC_KHZ: {\n\t\tr = vcpu->arch.virtual_tsc_khz;\n\t\tgoto out;\n\t}\n\tcase KVM_KVMCLOCK_CTRL: {\n\t\tr = kvm_set_guest_paused(vcpu);\n\t\tgoto out;\n\t}\n\tcase KVM_ENABLE_CAP: {\n\t\tstruct kvm_enable_cap cap;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cap, argp, sizeof(cap)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);\n\t\tbreak;\n\t}\n\tcase KVM_GET_NESTED_STATE: {\n\t\tstruct kvm_nested_state __user *user_kvm_nested_state = argp;\n\t\tu32 user_data_size;\n\n\t\tr = -EINVAL;\n\t\tif (!kvm_x86_ops.nested_ops->get_state)\n\t\t\tbreak;\n\n\t\tBUILD_BUG_ON(sizeof(user_data_size) != sizeof(user_kvm_nested_state->size));\n\t\tr = -EFAULT;\n\t\tif (get_user(user_data_size, &user_kvm_nested_state->size))\n\t\t\tbreak;\n\n\t\tr = kvm_x86_ops.nested_ops->get_state(vcpu, user_kvm_nested_state,\n\t\t\t\t\t\t     user_data_size);\n\t\tif (r < 0)\n\t\t\tbreak;\n\n\t\tif (r > user_data_size) {\n\t\t\tif (put_user(r, &user_kvm_nested_state->size))\n\t\t\t\tr = -EFAULT;\n\t\t\telse\n\t\t\t\tr = -E2BIG;\n\t\t\tbreak;\n\t\t}\n\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_NESTED_STATE: {\n\t\tstruct kvm_nested_state __user *user_kvm_nested_state = argp;\n\t\tstruct kvm_nested_state kvm_state;\n\t\tint idx;\n\n\t\tr = -EINVAL;\n\t\tif (!kvm_x86_ops.nested_ops->set_state)\n\t\t\tbreak;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&kvm_state, user_kvm_nested_state, sizeof(kvm_state)))\n\t\t\tbreak;\n\n\t\tr = -EINVAL;\n\t\tif (kvm_state.size < sizeof(kvm_state))\n\t\t\tbreak;\n\n\t\tif (kvm_state.flags &\n\t\t    ~(KVM_STATE_NESTED_RUN_PENDING | KVM_STATE_NESTED_GUEST_MODE\n\t\t      | KVM_STATE_NESTED_EVMCS | KVM_STATE_NESTED_MTF_PENDING\n\t\t      | KVM_STATE_NESTED_GIF_SET))\n\t\t\tbreak;\n\n\t\t/* nested_run_pending implies guest_mode.  */\n\t\tif ((kvm_state.flags & KVM_STATE_NESTED_RUN_PENDING)\n\t\t    && !(kvm_state.flags & KVM_STATE_NESTED_GUEST_MODE))\n\t\t\tbreak;\n\n\t\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = kvm_x86_ops.nested_ops->set_state(vcpu, user_kvm_nested_state, &kvm_state);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_GET_SUPPORTED_HV_CPUID:\n\t\tr = kvm_ioctl_get_supported_hv_cpuid(vcpu, argp);\n\t\tbreak;\n#ifdef CONFIG_KVM_XEN\n\tcase KVM_XEN_VCPU_GET_ATTR: {\n\t\tstruct kvm_xen_vcpu_attr xva;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xva, argp, sizeof(xva)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_vcpu_get_attr(vcpu, &xva);\n\t\tif (!r && copy_to_user(argp, &xva, sizeof(xva)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase KVM_XEN_VCPU_SET_ATTR: {\n\t\tstruct kvm_xen_vcpu_attr xva;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xva, argp, sizeof(xva)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_vcpu_set_attr(vcpu, &xva);\n\t\tbreak;\n\t}\n#endif\n\tcase KVM_GET_SREGS2: {\n\t\tu.sregs2 = kzalloc(sizeof(struct kvm_sregs2), GFP_KERNEL);\n\t\tr = -ENOMEM;\n\t\tif (!u.sregs2)\n\t\t\tgoto out;\n\t\t__get_sregs2(vcpu, u.sregs2);\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.sregs2, sizeof(struct kvm_sregs2)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_SREGS2: {\n\t\tu.sregs2 = memdup_user(argp, sizeof(struct kvm_sregs2));\n\t\tif (IS_ERR(u.sregs2)) {\n\t\t\tr = PTR_ERR(u.sregs2);\n\t\t\tu.sregs2 = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tr = __set_sregs2(vcpu, u.sregs2);\n\t\tbreak;\n\t}\n\tcase KVM_HAS_DEVICE_ATTR:\n\tcase KVM_GET_DEVICE_ATTR:\n\tcase KVM_SET_DEVICE_ATTR:\n\t\tr = kvm_vcpu_ioctl_device_attr(vcpu, ioctl, argp);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t}\nout:\n\tkfree(u.buffer);\nout_nofree:\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\nvm_fault_t kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)\n{\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic int kvm_vm_ioctl_set_tss_addr(struct kvm *kvm, unsigned long addr)\n{\n\tint ret;\n\n\tif (addr > (unsigned int)(-3 * PAGE_SIZE))\n\t\treturn -EINVAL;\n\tret = static_call(kvm_x86_set_tss_addr)(kvm, addr);\n\treturn ret;\n}\n\nstatic int kvm_vm_ioctl_set_identity_map_addr(struct kvm *kvm,\n\t\t\t\t\t      u64 ident_addr)\n{\n\treturn static_call(kvm_x86_set_identity_map_addr)(kvm, ident_addr);\n}\n\nstatic int kvm_vm_ioctl_set_nr_mmu_pages(struct kvm *kvm,\n\t\t\t\t\t unsigned long kvm_nr_mmu_pages)\n{\n\tif (kvm_nr_mmu_pages < KVM_MIN_ALLOC_MMU_PAGES)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tkvm_mmu_change_mmu_pages(kvm, kvm_nr_mmu_pages);\n\tkvm->arch.n_requested_mmu_pages = kvm_nr_mmu_pages;\n\n\tmutex_unlock(&kvm->slots_lock);\n\treturn 0;\n}\n\nstatic unsigned long kvm_vm_ioctl_get_nr_mmu_pages(struct kvm *kvm)\n{\n\treturn kvm->arch.n_max_mmu_pages;\n}\n\nstatic int kvm_vm_ioctl_get_irqchip(struct kvm *kvm, struct kvm_irqchip *chip)\n{\n\tstruct kvm_pic *pic = kvm->arch.vpic;\n\tint r;\n\n\tr = 0;\n\tswitch (chip->chip_id) {\n\tcase KVM_IRQCHIP_PIC_MASTER:\n\t\tmemcpy(&chip->chip.pic, &pic->pics[0],\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tbreak;\n\tcase KVM_IRQCHIP_PIC_SLAVE:\n\t\tmemcpy(&chip->chip.pic, &pic->pics[1],\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tbreak;\n\tcase KVM_IRQCHIP_IOAPIC:\n\t\tkvm_get_ioapic(kvm, &chip->chip.ioapic);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn r;\n}\n\nstatic int kvm_vm_ioctl_set_irqchip(struct kvm *kvm, struct kvm_irqchip *chip)\n{\n\tstruct kvm_pic *pic = kvm->arch.vpic;\n\tint r;\n\n\tr = 0;\n\tswitch (chip->chip_id) {\n\tcase KVM_IRQCHIP_PIC_MASTER:\n\t\tspin_lock(&pic->lock);\n\t\tmemcpy(&pic->pics[0], &chip->chip.pic,\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tspin_unlock(&pic->lock);\n\t\tbreak;\n\tcase KVM_IRQCHIP_PIC_SLAVE:\n\t\tspin_lock(&pic->lock);\n\t\tmemcpy(&pic->pics[1], &chip->chip.pic,\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tspin_unlock(&pic->lock);\n\t\tbreak;\n\tcase KVM_IRQCHIP_IOAPIC:\n\t\tkvm_set_ioapic(kvm, &chip->chip.ioapic);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\tkvm_pic_update_irq(pic);\n\treturn r;\n}\n\nstatic int kvm_vm_ioctl_get_pit(struct kvm *kvm, struct kvm_pit_state *ps)\n{\n\tstruct kvm_kpit_state *kps = &kvm->arch.vpit->pit_state;\n\n\tBUILD_BUG_ON(sizeof(*ps) != sizeof(kps->channels));\n\n\tmutex_lock(&kps->lock);\n\tmemcpy(ps, &kps->channels, sizeof(*ps));\n\tmutex_unlock(&kps->lock);\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_set_pit(struct kvm *kvm, struct kvm_pit_state *ps)\n{\n\tint i;\n\tstruct kvm_pit *pit = kvm->arch.vpit;\n\n\tmutex_lock(&pit->pit_state.lock);\n\tmemcpy(&pit->pit_state.channels, ps, sizeof(*ps));\n\tfor (i = 0; i < 3; i++)\n\t\tkvm_pit_load_count(pit, i, ps->channels[i].count, 0);\n\tmutex_unlock(&pit->pit_state.lock);\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_get_pit2(struct kvm *kvm, struct kvm_pit_state2 *ps)\n{\n\tmutex_lock(&kvm->arch.vpit->pit_state.lock);\n\tmemcpy(ps->channels, &kvm->arch.vpit->pit_state.channels,\n\t\tsizeof(ps->channels));\n\tps->flags = kvm->arch.vpit->pit_state.flags;\n\tmutex_unlock(&kvm->arch.vpit->pit_state.lock);\n\tmemset(&ps->reserved, 0, sizeof(ps->reserved));\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_set_pit2(struct kvm *kvm, struct kvm_pit_state2 *ps)\n{\n\tint start = 0;\n\tint i;\n\tu32 prev_legacy, cur_legacy;\n\tstruct kvm_pit *pit = kvm->arch.vpit;\n\n\tmutex_lock(&pit->pit_state.lock);\n\tprev_legacy = pit->pit_state.flags & KVM_PIT_FLAGS_HPET_LEGACY;\n\tcur_legacy = ps->flags & KVM_PIT_FLAGS_HPET_LEGACY;\n\tif (!prev_legacy && cur_legacy)\n\t\tstart = 1;\n\tmemcpy(&pit->pit_state.channels, &ps->channels,\n\t       sizeof(pit->pit_state.channels));\n\tpit->pit_state.flags = ps->flags;\n\tfor (i = 0; i < 3; i++)\n\t\tkvm_pit_load_count(pit, i, pit->pit_state.channels[i].count,\n\t\t\t\t   start && i == 0);\n\tmutex_unlock(&pit->pit_state.lock);\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_reinject(struct kvm *kvm,\n\t\t\t\t struct kvm_reinject_control *control)\n{\n\tstruct kvm_pit *pit = kvm->arch.vpit;\n\n\t/* pit->pit_state.lock was overloaded to prevent userspace from getting\n\t * an inconsistent state after running multiple KVM_REINJECT_CONTROL\n\t * ioctls in parallel.  Use a separate lock if that ioctl isn't rare.\n\t */\n\tmutex_lock(&pit->pit_state.lock);\n\tkvm_pit_set_reinject(pit, control->pit_reinject);\n\tmutex_unlock(&pit->pit_state.lock);\n\n\treturn 0;\n}\n\nvoid kvm_arch_sync_dirty_log(struct kvm *kvm, struct kvm_memory_slot *memslot)\n{\n\n\t/*\n\t * Flush all CPUs' dirty log buffers to the  dirty_bitmap.  Called\n\t * before reporting dirty_bitmap to userspace.  KVM flushes the buffers\n\t * on all VM-Exits, thus we only need to kick running vCPUs to force a\n\t * VM-Exit.\n\t */\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nint kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,\n\t\t\tbool line_status)\n{\n\tif (!irqchip_in_kernel(kvm))\n\t\treturn -ENXIO;\n\n\tirq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,\n\t\t\t\t\tirq_event->irq, irq_event->level,\n\t\t\t\t\tline_status);\n\treturn 0;\n}\n\nint kvm_vm_ioctl_enable_cap(struct kvm *kvm,\n\t\t\t    struct kvm_enable_cap *cap)\n{\n\tint r;\n\n\tif (cap->flags)\n\t\treturn -EINVAL;\n\n\tswitch (cap->cap) {\n\tcase KVM_CAP_DISABLE_QUIRKS2:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_X86_VALID_QUIRKS)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase KVM_CAP_DISABLE_QUIRKS:\n\t\tkvm->arch.disabled_quirks = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_SPLIT_IRQCHIP: {\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] > MAX_NR_RESERVED_IOAPIC_PINS)\n\t\t\tgoto split_irqchip_unlock;\n\t\tr = -EEXIST;\n\t\tif (irqchip_in_kernel(kvm))\n\t\t\tgoto split_irqchip_unlock;\n\t\tif (kvm->created_vcpus)\n\t\t\tgoto split_irqchip_unlock;\n\t\tr = kvm_setup_empty_irq_routing(kvm);\n\t\tif (r)\n\t\t\tgoto split_irqchip_unlock;\n\t\t/* Pairs with irqchip_in_kernel. */\n\t\tsmp_wmb();\n\t\tkvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;\n\t\tkvm->arch.nr_reserved_ioapic_pins = cap->args[0];\n\t\tkvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);\n\t\tr = 0;\nsplit_irqchip_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_CAP_X2APIC_API:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_X2APIC_API_VALID_FLAGS)\n\t\t\tbreak;\n\n\t\tif (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS)\n\t\t\tkvm->arch.x2apic_format = true;\n\t\tif (cap->args[0] & KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)\n\t\t\tkvm->arch.x2apic_broadcast_quirk_disabled = true;\n\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_X86_DISABLE_EXITS:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_X86_DISABLE_VALID_EXITS)\n\t\t\tbreak;\n\n\t\tif (cap->args[0] & KVM_X86_DISABLE_EXITS_PAUSE)\n\t\t\tkvm->arch.pause_in_guest = true;\n\n#define SMT_RSB_MSG \"This processor is affected by the Cross-Thread Return Predictions vulnerability. \" \\\n\t\t    \"KVM_CAP_X86_DISABLE_EXITS should only be used with SMT disabled or trusted guests.\"\n\n\t\tif (!mitigate_smt_rsb) {\n\t\t\tif (boot_cpu_has_bug(X86_BUG_SMT_RSB) && cpu_smt_possible() &&\n\t\t\t    (cap->args[0] & ~KVM_X86_DISABLE_EXITS_PAUSE))\n\t\t\t\tpr_warn_once(SMT_RSB_MSG);\n\n\t\t\tif ((cap->args[0] & KVM_X86_DISABLE_EXITS_MWAIT) &&\n\t\t\t    kvm_can_mwait_in_guest())\n\t\t\t\tkvm->arch.mwait_in_guest = true;\n\t\t\tif (cap->args[0] & KVM_X86_DISABLE_EXITS_HLT)\n\t\t\t\tkvm->arch.hlt_in_guest = true;\n\t\t\tif (cap->args[0] & KVM_X86_DISABLE_EXITS_CSTATE)\n\t\t\t\tkvm->arch.cstate_in_guest = true;\n\t\t}\n\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_MSR_PLATFORM_INFO:\n\t\tkvm->arch.guest_can_read_msr_platform_info = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_EXCEPTION_PAYLOAD:\n\t\tkvm->arch.exception_payload_enabled = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_X86_TRIPLE_FAULT_EVENT:\n\t\tkvm->arch.triple_fault_event = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_X86_USER_SPACE_MSR:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_MSR_EXIT_REASON_VALID_MASK)\n\t\t\tbreak;\n\t\tkvm->arch.user_space_msr_mask = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_X86_BUS_LOCK_EXIT:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_BUS_LOCK_DETECTION_VALID_MODE)\n\t\t\tbreak;\n\n\t\tif ((cap->args[0] & KVM_BUS_LOCK_DETECTION_OFF) &&\n\t\t    (cap->args[0] & KVM_BUS_LOCK_DETECTION_EXIT))\n\t\t\tbreak;\n\n\t\tif (kvm_caps.has_bus_lock_exit &&\n\t\t    cap->args[0] & KVM_BUS_LOCK_DETECTION_EXIT)\n\t\t\tkvm->arch.bus_lock_detection_enabled = true;\n\t\tr = 0;\n\t\tbreak;\n#ifdef CONFIG_X86_SGX_KVM\n\tcase KVM_CAP_SGX_ATTRIBUTE: {\n\t\tunsigned long allowed_attributes = 0;\n\n\t\tr = sgx_set_attribute(&allowed_attributes, cap->args[0]);\n\t\tif (r)\n\t\t\tbreak;\n\n\t\t/* KVM only supports the PROVISIONKEY privileged attribute. */\n\t\tif ((allowed_attributes & SGX_ATTR_PROVISIONKEY) &&\n\t\t    !(allowed_attributes & ~SGX_ATTR_PROVISIONKEY))\n\t\t\tkvm->arch.sgx_provisioning_allowed = true;\n\t\telse\n\t\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n#endif\n\tcase KVM_CAP_VM_COPY_ENC_CONTEXT_FROM:\n\t\tr = -EINVAL;\n\t\tif (!kvm_x86_ops.vm_copy_enc_context_from)\n\t\t\tbreak;\n\n\t\tr = static_call(kvm_x86_vm_copy_enc_context_from)(kvm, cap->args[0]);\n\t\tbreak;\n\tcase KVM_CAP_VM_MOVE_ENC_CONTEXT_FROM:\n\t\tr = -EINVAL;\n\t\tif (!kvm_x86_ops.vm_move_enc_context_from)\n\t\t\tbreak;\n\n\t\tr = static_call(kvm_x86_vm_move_enc_context_from)(kvm, cap->args[0]);\n\t\tbreak;\n\tcase KVM_CAP_EXIT_HYPERCALL:\n\t\tif (cap->args[0] & ~KVM_EXIT_HYPERCALL_VALID_MASK) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tkvm->arch.hypercall_exit_enabled = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_EXIT_ON_EMULATION_FAILURE:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~1)\n\t\t\tbreak;\n\t\tkvm->arch.exit_on_emulation_error = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_PMU_CAPABILITY:\n\t\tr = -EINVAL;\n\t\tif (!enable_pmu || (cap->args[0] & ~KVM_CAP_PMU_VALID_MASK))\n\t\t\tbreak;\n\n\t\tmutex_lock(&kvm->lock);\n\t\tif (!kvm->created_vcpus) {\n\t\t\tkvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);\n\t\t\tr = 0;\n\t\t}\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPU_ID:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] > KVM_MAX_VCPU_IDS)\n\t\t\tbreak;\n\n\t\tmutex_lock(&kvm->lock);\n\t\tif (kvm->arch.max_vcpu_ids == cap->args[0]) {\n\t\t\tr = 0;\n\t\t} else if (!kvm->arch.max_vcpu_ids) {\n\t\t\tkvm->arch.max_vcpu_ids = cap->args[0];\n\t\t\tr = 0;\n\t\t}\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tcase KVM_CAP_X86_NOTIFY_VMEXIT:\n\t\tr = -EINVAL;\n\t\tif ((u32)cap->args[0] & ~KVM_X86_NOTIFY_VMEXIT_VALID_BITS)\n\t\t\tbreak;\n\t\tif (!kvm_caps.has_notify_vmexit)\n\t\t\tbreak;\n\t\tif (!((u32)cap->args[0] & KVM_X86_NOTIFY_VMEXIT_ENABLED))\n\t\t\tbreak;\n\t\tmutex_lock(&kvm->lock);\n\t\tif (!kvm->created_vcpus) {\n\t\t\tkvm->arch.notify_window = cap->args[0] >> 32;\n\t\t\tkvm->arch.notify_vmexit_flags = (u32)cap->args[0];\n\t\t\tr = 0;\n\t\t}\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tcase KVM_CAP_VM_DISABLE_NX_HUGE_PAGES:\n\t\tr = -EINVAL;\n\n\t\t/*\n\t\t * Since the risk of disabling NX hugepages is a guest crashing\n\t\t * the system, ensure the userspace process has permission to\n\t\t * reboot the system.\n\t\t *\n\t\t * Note that unlike the reboot() syscall, the process must have\n\t\t * this capability in the root namespace because exposing\n\t\t * /dev/kvm into a container does not limit the scope of the\n\t\t * iTLB multihit bug to that container. In other words,\n\t\t * this must use capable(), not ns_capable().\n\t\t */\n\t\tif (!capable(CAP_SYS_BOOT)) {\n\t\t\tr = -EPERM;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cap->args[0])\n\t\t\tbreak;\n\n\t\tmutex_lock(&kvm->lock);\n\t\tif (!kvm->created_vcpus) {\n\t\t\tkvm->arch.disable_nx_huge_pages = true;\n\t\t\tr = 0;\n\t\t}\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn r;\n}\n\nstatic struct kvm_x86_msr_filter *kvm_alloc_msr_filter(bool default_allow)\n{\n\tstruct kvm_x86_msr_filter *msr_filter;\n\n\tmsr_filter = kzalloc(sizeof(*msr_filter), GFP_KERNEL_ACCOUNT);\n\tif (!msr_filter)\n\t\treturn NULL;\n\n\tmsr_filter->default_allow = default_allow;\n\treturn msr_filter;\n}\n\nstatic void kvm_free_msr_filter(struct kvm_x86_msr_filter *msr_filter)\n{\n\tu32 i;\n\n\tif (!msr_filter)\n\t\treturn;\n\n\tfor (i = 0; i < msr_filter->count; i++)\n\t\tkfree(msr_filter->ranges[i].bitmap);\n\n\tkfree(msr_filter);\n}\n\nstatic int kvm_add_msr_filter(struct kvm_x86_msr_filter *msr_filter,\n\t\t\t      struct kvm_msr_filter_range *user_range)\n{\n\tunsigned long *bitmap = NULL;\n\tsize_t bitmap_size;\n\n\tif (!user_range->nmsrs)\n\t\treturn 0;\n\n\tif (user_range->flags & ~KVM_MSR_FILTER_RANGE_VALID_MASK)\n\t\treturn -EINVAL;\n\n\tif (!user_range->flags)\n\t\treturn -EINVAL;\n\n\tbitmap_size = BITS_TO_LONGS(user_range->nmsrs) * sizeof(long);\n\tif (!bitmap_size || bitmap_size > KVM_MSR_FILTER_MAX_BITMAP_SIZE)\n\t\treturn -EINVAL;\n\n\tbitmap = memdup_user((__user u8*)user_range->bitmap, bitmap_size);\n\tif (IS_ERR(bitmap))\n\t\treturn PTR_ERR(bitmap);\n\n\tmsr_filter->ranges[msr_filter->count] = (struct msr_bitmap_range) {\n\t\t.flags = user_range->flags,\n\t\t.base = user_range->base,\n\t\t.nmsrs = user_range->nmsrs,\n\t\t.bitmap = bitmap,\n\t};\n\n\tmsr_filter->count++;\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_set_msr_filter(struct kvm *kvm,\n\t\t\t\t       struct kvm_msr_filter *filter)\n{\n\tstruct kvm_x86_msr_filter *new_filter, *old_filter;\n\tbool default_allow;\n\tbool empty = true;\n\tint r = 0;\n\tu32 i;\n\n\tif (filter->flags & ~KVM_MSR_FILTER_VALID_MASK)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < ARRAY_SIZE(filter->ranges); i++)\n\t\tempty &= !filter->ranges[i].nmsrs;\n\n\tdefault_allow = !(filter->flags & KVM_MSR_FILTER_DEFAULT_DENY);\n\tif (empty && !default_allow)\n\t\treturn -EINVAL;\n\n\tnew_filter = kvm_alloc_msr_filter(default_allow);\n\tif (!new_filter)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < ARRAY_SIZE(filter->ranges); i++) {\n\t\tr = kvm_add_msr_filter(new_filter, &filter->ranges[i]);\n\t\tif (r) {\n\t\t\tkvm_free_msr_filter(new_filter);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tmutex_lock(&kvm->lock);\n\n\t/* The per-VM filter is protected by kvm->lock... */\n\told_filter = srcu_dereference_check(kvm->arch.msr_filter, &kvm->srcu, 1);\n\n\trcu_assign_pointer(kvm->arch.msr_filter, new_filter);\n\tsynchronize_srcu(&kvm->srcu);\n\n\tkvm_free_msr_filter(old_filter);\n\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_MSR_FILTER_CHANGED);\n\tmutex_unlock(&kvm->lock);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_KVM_COMPAT\n/* for KVM_X86_SET_MSR_FILTER */\nstruct kvm_msr_filter_range_compat {\n\t__u32 flags;\n\t__u32 nmsrs;\n\t__u32 base;\n\t__u32 bitmap;\n};\n\nstruct kvm_msr_filter_compat {\n\t__u32 flags;\n\tstruct kvm_msr_filter_range_compat ranges[KVM_MSR_FILTER_MAX_RANGES];\n};\n\n#define KVM_X86_SET_MSR_FILTER_COMPAT _IOW(KVMIO, 0xc6, struct kvm_msr_filter_compat)\n\nlong kvm_arch_vm_compat_ioctl(struct file *filp, unsigned int ioctl,\n\t\t\t      unsigned long arg)\n{\n\tvoid __user *argp = (void __user *)arg;\n\tstruct kvm *kvm = filp->private_data;\n\tlong r = -ENOTTY;\n\n\tswitch (ioctl) {\n\tcase KVM_X86_SET_MSR_FILTER_COMPAT: {\n\t\tstruct kvm_msr_filter __user *user_msr_filter = argp;\n\t\tstruct kvm_msr_filter_compat filter_compat;\n\t\tstruct kvm_msr_filter filter;\n\t\tint i;\n\n\t\tif (copy_from_user(&filter_compat, user_msr_filter,\n\t\t\t\t   sizeof(filter_compat)))\n\t\t\treturn -EFAULT;\n\n\t\tfilter.flags = filter_compat.flags;\n\t\tfor (i = 0; i < ARRAY_SIZE(filter.ranges); i++) {\n\t\t\tstruct kvm_msr_filter_range_compat *cr;\n\n\t\t\tcr = &filter_compat.ranges[i];\n\t\t\tfilter.ranges[i] = (struct kvm_msr_filter_range) {\n\t\t\t\t.flags = cr->flags,\n\t\t\t\t.nmsrs = cr->nmsrs,\n\t\t\t\t.base = cr->base,\n\t\t\t\t.bitmap = (__u8 *)(ulong)cr->bitmap,\n\t\t\t};\n\t\t}\n\n\t\tr = kvm_vm_ioctl_set_msr_filter(kvm, &filter);\n\t\tbreak;\n\t}\n\t}\n\n\treturn r;\n}\n#endif\n\n#ifdef CONFIG_HAVE_KVM_PM_NOTIFIER\nstatic int kvm_arch_suspend_notifier(struct kvm *kvm)\n{\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\tint ret = 0;\n\n\tmutex_lock(&kvm->lock);\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tif (!vcpu->arch.pv_time.active)\n\t\t\tcontinue;\n\n\t\tret = kvm_set_guest_paused(vcpu);\n\t\tif (ret) {\n\t\t\tkvm_err(\"Failed to pause guest VCPU%d: %d\\n\",\n\t\t\t\tvcpu->vcpu_id, ret);\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&kvm->lock);\n\n\treturn ret ? NOTIFY_BAD : NOTIFY_DONE;\n}\n\nint kvm_arch_pm_notifier(struct kvm *kvm, unsigned long state)\n{\n\tswitch (state) {\n\tcase PM_HIBERNATION_PREPARE:\n\tcase PM_SUSPEND_PREPARE:\n\t\treturn kvm_arch_suspend_notifier(kvm);\n\t}\n\n\treturn NOTIFY_DONE;\n}\n#endif /* CONFIG_HAVE_KVM_PM_NOTIFIER */\n\nstatic int kvm_vm_ioctl_get_clock(struct kvm *kvm, void __user *argp)\n{\n\tstruct kvm_clock_data data = { 0 };\n\n\tget_kvmclock(kvm, &data);\n\tif (copy_to_user(argp, &data, sizeof(data)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\tstruct kvm_clock_data data;\n\tu64 now_raw_ns;\n\n\tif (copy_from_user(&data, argp, sizeof(data)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Only KVM_CLOCK_REALTIME is used, but allow passing the\n\t * result of KVM_GET_CLOCK back to KVM_SET_CLOCK.\n\t */\n\tif (data.flags & ~KVM_CLOCK_VALID_FLAGS)\n\t\treturn -EINVAL;\n\n\tkvm_hv_request_tsc_page_update(kvm);\n\tkvm_start_pvclock_update(kvm);\n\tpvclock_update_vm_gtod_copy(kvm);\n\n\t/*\n\t * This pairs with kvm_guest_time_update(): when masterclock is\n\t * in use, we use master_kernel_ns + kvmclock_offset to set\n\t * unsigned 'system_time' so if we use get_kvmclock_ns() (which\n\t * is slightly ahead) here we risk going negative on unsigned\n\t * 'system_time' when 'data.clock' is very small.\n\t */\n\tif (data.flags & KVM_CLOCK_REALTIME) {\n\t\tu64 now_real_ns = ktime_get_real_ns();\n\n\t\t/*\n\t\t * Avoid stepping the kvmclock backwards.\n\t\t */\n\t\tif (now_real_ns > data.realtime)\n\t\t\tdata.clock += now_real_ns - data.realtime;\n\t}\n\n\tif (ka->use_master_clock)\n\t\tnow_raw_ns = ka->master_kernel_ns;\n\telse\n\t\tnow_raw_ns = get_kvmclock_base_ns();\n\tka->kvmclock_offset = data.clock - now_raw_ns;\n\tkvm_end_pvclock_update(kvm);\n\treturn 0;\n}\n\nlong kvm_arch_vm_ioctl(struct file *filp,\n\t\t       unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r = -ENOTTY;\n\t/*\n\t * This union makes it completely explicit to gcc-3.x\n\t * that these two variables' stack usage should be\n\t * combined, not added together.\n\t */\n\tunion {\n\t\tstruct kvm_pit_state ps;\n\t\tstruct kvm_pit_state2 ps2;\n\t\tstruct kvm_pit_config pit_config;\n\t} u;\n\n\tswitch (ioctl) {\n\tcase KVM_SET_TSS_ADDR:\n\t\tr = kvm_vm_ioctl_set_tss_addr(kvm, arg);\n\t\tbreak;\n\tcase KVM_SET_IDENTITY_MAP_ADDR: {\n\t\tu64 ident_addr;\n\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -EINVAL;\n\t\tif (kvm->created_vcpus)\n\t\t\tgoto set_identity_unlock;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&ident_addr, argp, sizeof(ident_addr)))\n\t\t\tgoto set_identity_unlock;\n\t\tr = kvm_vm_ioctl_set_identity_map_addr(kvm, ident_addr);\nset_identity_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_SET_NR_MMU_PAGES:\n\t\tr = kvm_vm_ioctl_set_nr_mmu_pages(kvm, arg);\n\t\tbreak;\n\tcase KVM_GET_NR_MMU_PAGES:\n\t\tr = kvm_vm_ioctl_get_nr_mmu_pages(kvm);\n\t\tbreak;\n\tcase KVM_CREATE_IRQCHIP: {\n\t\tmutex_lock(&kvm->lock);\n\n\t\tr = -EEXIST;\n\t\tif (irqchip_in_kernel(kvm))\n\t\t\tgoto create_irqchip_unlock;\n\n\t\tr = -EINVAL;\n\t\tif (kvm->created_vcpus)\n\t\t\tgoto create_irqchip_unlock;\n\n\t\tr = kvm_pic_init(kvm);\n\t\tif (r)\n\t\t\tgoto create_irqchip_unlock;\n\n\t\tr = kvm_ioapic_init(kvm);\n\t\tif (r) {\n\t\t\tkvm_pic_destroy(kvm);\n\t\t\tgoto create_irqchip_unlock;\n\t\t}\n\n\t\tr = kvm_setup_default_irq_routing(kvm);\n\t\tif (r) {\n\t\t\tkvm_ioapic_destroy(kvm);\n\t\t\tkvm_pic_destroy(kvm);\n\t\t\tgoto create_irqchip_unlock;\n\t\t}\n\t\t/* Write kvm->irq_routing before enabling irqchip_in_kernel. */\n\t\tsmp_wmb();\n\t\tkvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;\n\t\tkvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);\n\tcreate_irqchip_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_CREATE_PIT:\n\t\tu.pit_config.flags = KVM_PIT_SPEAKER_DUMMY;\n\t\tgoto create_pit;\n\tcase KVM_CREATE_PIT2:\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.pit_config, argp,\n\t\t\t\t   sizeof(struct kvm_pit_config)))\n\t\t\tgoto out;\n\tcreate_pit:\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -EEXIST;\n\t\tif (kvm->arch.vpit)\n\t\t\tgoto create_pit_unlock;\n\t\tr = -ENOMEM;\n\t\tkvm->arch.vpit = kvm_create_pit(kvm, u.pit_config.flags);\n\t\tif (kvm->arch.vpit)\n\t\t\tr = 0;\n\tcreate_pit_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tcase KVM_GET_IRQCHIP: {\n\t\t/* 0: PIC master, 1: PIC slave, 2: IOAPIC */\n\t\tstruct kvm_irqchip *chip;\n\n\t\tchip = memdup_user(argp, sizeof(*chip));\n\t\tif (IS_ERR(chip)) {\n\t\t\tr = PTR_ERR(chip);\n\t\t\tgoto out;\n\t\t}\n\n\t\tr = -ENXIO;\n\t\tif (!irqchip_kernel(kvm))\n\t\t\tgoto get_irqchip_out;\n\t\tr = kvm_vm_ioctl_get_irqchip(kvm, chip);\n\t\tif (r)\n\t\t\tgoto get_irqchip_out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, chip, sizeof(*chip)))\n\t\t\tgoto get_irqchip_out;\n\t\tr = 0;\n\tget_irqchip_out:\n\t\tkfree(chip);\n\t\tbreak;\n\t}\n\tcase KVM_SET_IRQCHIP: {\n\t\t/* 0: PIC master, 1: PIC slave, 2: IOAPIC */\n\t\tstruct kvm_irqchip *chip;\n\n\t\tchip = memdup_user(argp, sizeof(*chip));\n\t\tif (IS_ERR(chip)) {\n\t\t\tr = PTR_ERR(chip);\n\t\t\tgoto out;\n\t\t}\n\n\t\tr = -ENXIO;\n\t\tif (!irqchip_kernel(kvm))\n\t\t\tgoto set_irqchip_out;\n\t\tr = kvm_vm_ioctl_set_irqchip(kvm, chip);\n\tset_irqchip_out:\n\t\tkfree(chip);\n\t\tbreak;\n\t}\n\tcase KVM_GET_PIT: {\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.ps, argp, sizeof(struct kvm_pit_state)))\n\t\t\tgoto out;\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_get_pit(kvm, &u.ps);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &u.ps, sizeof(struct kvm_pit_state)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_PIT: {\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.ps, argp, sizeof(u.ps)))\n\t\t\tgoto out;\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto set_pit_out;\n\t\tr = kvm_vm_ioctl_set_pit(kvm, &u.ps);\nset_pit_out:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_GET_PIT2: {\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_get_pit2(kvm, &u.ps2);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &u.ps2, sizeof(u.ps2)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_PIT2: {\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.ps2, argp, sizeof(u.ps2)))\n\t\t\tgoto out;\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto set_pit2_out;\n\t\tr = kvm_vm_ioctl_set_pit2(kvm, &u.ps2);\nset_pit2_out:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_REINJECT_CONTROL: {\n\t\tstruct kvm_reinject_control control;\n\t\tr =  -EFAULT;\n\t\tif (copy_from_user(&control, argp, sizeof(control)))\n\t\t\tgoto out;\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_reinject(kvm, &control);\n\t\tbreak;\n\t}\n\tcase KVM_SET_BOOT_CPU_ID:\n\t\tr = 0;\n\t\tmutex_lock(&kvm->lock);\n\t\tif (kvm->created_vcpus)\n\t\t\tr = -EBUSY;\n\t\telse\n\t\t\tkvm->arch.bsp_vcpu_id = arg;\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n#ifdef CONFIG_KVM_XEN\n\tcase KVM_XEN_HVM_CONFIG: {\n\t\tstruct kvm_xen_hvm_config xhc;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xhc, argp, sizeof(xhc)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_hvm_config(kvm, &xhc);\n\t\tbreak;\n\t}\n\tcase KVM_XEN_HVM_GET_ATTR: {\n\t\tstruct kvm_xen_hvm_attr xha;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xha, argp, sizeof(xha)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_hvm_get_attr(kvm, &xha);\n\t\tif (!r && copy_to_user(argp, &xha, sizeof(xha)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase KVM_XEN_HVM_SET_ATTR: {\n\t\tstruct kvm_xen_hvm_attr xha;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xha, argp, sizeof(xha)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_hvm_set_attr(kvm, &xha);\n\t\tbreak;\n\t}\n\tcase KVM_XEN_HVM_EVTCHN_SEND: {\n\t\tstruct kvm_irq_routing_xen_evtchn uxe;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&uxe, argp, sizeof(uxe)))\n\t\t\tgoto out;\n\t\tr = kvm_xen_hvm_evtchn_send(kvm, &uxe);\n\t\tbreak;\n\t}\n#endif\n\tcase KVM_SET_CLOCK:\n\t\tr = kvm_vm_ioctl_set_clock(kvm, argp);\n\t\tbreak;\n\tcase KVM_GET_CLOCK:\n\t\tr = kvm_vm_ioctl_get_clock(kvm, argp);\n\t\tbreak;\n\tcase KVM_SET_TSC_KHZ: {\n\t\tu32 user_tsc_khz;\n\n\t\tr = -EINVAL;\n\t\tuser_tsc_khz = (u32)arg;\n\n\t\tif (kvm_caps.has_tsc_control &&\n\t\t    user_tsc_khz >= kvm_caps.max_guest_tsc_khz)\n\t\t\tgoto out;\n\n\t\tif (user_tsc_khz == 0)\n\t\t\tuser_tsc_khz = tsc_khz;\n\n\t\tWRITE_ONCE(kvm->arch.default_tsc_khz, user_tsc_khz);\n\t\tr = 0;\n\n\t\tgoto out;\n\t}\n\tcase KVM_GET_TSC_KHZ: {\n\t\tr = READ_ONCE(kvm->arch.default_tsc_khz);\n\t\tgoto out;\n\t}\n\tcase KVM_MEMORY_ENCRYPT_OP: {\n\t\tr = -ENOTTY;\n\t\tif (!kvm_x86_ops.mem_enc_ioctl)\n\t\t\tgoto out;\n\n\t\tr = static_call(kvm_x86_mem_enc_ioctl)(kvm, argp);\n\t\tbreak;\n\t}\n\tcase KVM_MEMORY_ENCRYPT_REG_REGION: {\n\t\tstruct kvm_enc_region region;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&region, argp, sizeof(region)))\n\t\t\tgoto out;\n\n\t\tr = -ENOTTY;\n\t\tif (!kvm_x86_ops.mem_enc_register_region)\n\t\t\tgoto out;\n\n\t\tr = static_call(kvm_x86_mem_enc_register_region)(kvm, &region);\n\t\tbreak;\n\t}\n\tcase KVM_MEMORY_ENCRYPT_UNREG_REGION: {\n\t\tstruct kvm_enc_region region;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&region, argp, sizeof(region)))\n\t\t\tgoto out;\n\n\t\tr = -ENOTTY;\n\t\tif (!kvm_x86_ops.mem_enc_unregister_region)\n\t\t\tgoto out;\n\n\t\tr = static_call(kvm_x86_mem_enc_unregister_region)(kvm, &region);\n\t\tbreak;\n\t}\n\tcase KVM_HYPERV_EVENTFD: {\n\t\tstruct kvm_hyperv_eventfd hvevfd;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&hvevfd, argp, sizeof(hvevfd)))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_hv_eventfd(kvm, &hvevfd);\n\t\tbreak;\n\t}\n\tcase KVM_SET_PMU_EVENT_FILTER:\n\t\tr = kvm_vm_ioctl_set_pmu_event_filter(kvm, argp);\n\t\tbreak;\n\tcase KVM_X86_SET_MSR_FILTER: {\n\t\tstruct kvm_msr_filter __user *user_msr_filter = argp;\n\t\tstruct kvm_msr_filter filter;\n\n\t\tif (copy_from_user(&filter, user_msr_filter, sizeof(filter)))\n\t\t\treturn -EFAULT;\n\n\t\tr = kvm_vm_ioctl_set_msr_filter(kvm, &filter);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = -ENOTTY;\n\t}\nout:\n\treturn r;\n}\n\nstatic void kvm_init_msr_list(void)\n{\n\tu32 dummy[2];\n\tunsigned i;\n\n\tBUILD_BUG_ON_MSG(KVM_PMC_MAX_FIXED != 3,\n\t\t\t \"Please update the fixed PMCs in msrs_to_saved_all[]\");\n\n\tnum_msrs_to_save = 0;\n\tnum_emulated_msrs = 0;\n\tnum_msr_based_features = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(msrs_to_save_all); i++) {\n\t\tif (rdmsr_safe(msrs_to_save_all[i], &dummy[0], &dummy[1]) < 0)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Even MSRs that are valid in the host may not be exposed\n\t\t * to the guests in some cases.\n\t\t */\n\t\tswitch (msrs_to_save_all[i]) {\n\t\tcase MSR_IA32_BNDCFGS:\n\t\t\tif (!kvm_mpx_supported())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_TSC_AUX:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_RDTSCP) &&\n\t\t\t    !kvm_cpu_cap_has(X86_FEATURE_RDPID))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_UMWAIT_CONTROL:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_WAITPKG))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_CTL:\n\t\tcase MSR_IA32_RTIT_STATUS:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_CR3_MATCH:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT) ||\n\t\t\t    !intel_pt_validate_hw_cap(PT_CAP_cr3_filtering))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_OUTPUT_BASE:\n\t\tcase MSR_IA32_RTIT_OUTPUT_MASK:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT) ||\n\t\t\t\t(!intel_pt_validate_hw_cap(PT_CAP_topa_output) &&\n\t\t\t\t !intel_pt_validate_hw_cap(PT_CAP_single_range_output)))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT) ||\n\t\t\t\tmsrs_to_save_all[i] - MSR_IA32_RTIT_ADDR0_A >=\n\t\t\t\tintel_pt_validate_hw_cap(PT_CAP_num_address_ranges) * 2)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_ARCH_PERFMON_PERFCTR0 ... MSR_ARCH_PERFMON_PERFCTR_MAX:\n\t\t\tif (msrs_to_save_all[i] - MSR_ARCH_PERFMON_PERFCTR0 >=\n\t\t\t    min(KVM_INTEL_PMC_MAX_GENERIC, kvm_pmu_cap.num_counters_gp))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_ARCH_PERFMON_EVENTSEL0 ... MSR_ARCH_PERFMON_EVENTSEL_MAX:\n\t\t\tif (msrs_to_save_all[i] - MSR_ARCH_PERFMON_EVENTSEL0 >=\n\t\t\t    min(KVM_INTEL_PMC_MAX_GENERIC, kvm_pmu_cap.num_counters_gp))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_XFD:\n\t\tcase MSR_IA32_XFD_ERR:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_XFD))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tmsrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(emulated_msrs_all); i++) {\n\t\tif (!static_call(kvm_x86_has_emulated_msr)(NULL, emulated_msrs_all[i]))\n\t\t\tcontinue;\n\n\t\temulated_msrs[num_emulated_msrs++] = emulated_msrs_all[i];\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(msr_based_features_all); i++) {\n\t\tstruct kvm_msr_entry msr;\n\n\t\tmsr.index = msr_based_features_all[i];\n\t\tif (kvm_get_msr_feature(&msr))\n\t\t\tcontinue;\n\n\t\tmsr_based_features[num_msr_based_features++] = msr_based_features_all[i];\n\t}\n}\n\nstatic int vcpu_mmio_write(struct kvm_vcpu *vcpu, gpa_t addr, int len,\n\t\t\t   const void *v)\n{\n\tint handled = 0;\n\tint n;\n\n\tdo {\n\t\tn = min(len, 8);\n\t\tif (!(lapic_in_kernel(vcpu) &&\n\t\t      !kvm_iodevice_write(vcpu, &vcpu->arch.apic->dev, addr, n, v))\n\t\t    && kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, n, v))\n\t\t\tbreak;\n\t\thandled += n;\n\t\taddr += n;\n\t\tlen -= n;\n\t\tv += n;\n\t} while (len);\n\n\treturn handled;\n}\n\nstatic int vcpu_mmio_read(struct kvm_vcpu *vcpu, gpa_t addr, int len, void *v)\n{\n\tint handled = 0;\n\tint n;\n\n\tdo {\n\t\tn = min(len, 8);\n\t\tif (!(lapic_in_kernel(vcpu) &&\n\t\t      !kvm_iodevice_read(vcpu, &vcpu->arch.apic->dev,\n\t\t\t\t\t addr, n, v))\n\t\t    && kvm_io_bus_read(vcpu, KVM_MMIO_BUS, addr, n, v))\n\t\t\tbreak;\n\t\ttrace_kvm_mmio(KVM_TRACE_MMIO_READ, n, addr, v);\n\t\thandled += n;\n\t\taddr += n;\n\t\tlen -= n;\n\t\tv += n;\n\t} while (len);\n\n\treturn handled;\n}\n\nvoid kvm_set_segment(struct kvm_vcpu *vcpu,\n\t\t     struct kvm_segment *var, int seg)\n{\n\tstatic_call(kvm_x86_set_segment)(vcpu, var, seg);\n}\n\nvoid kvm_get_segment(struct kvm_vcpu *vcpu,\n\t\t     struct kvm_segment *var, int seg)\n{\n\tstatic_call(kvm_x86_get_segment)(vcpu, var, seg);\n}\n\ngpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u64 access,\n\t\t\t   struct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tgpa_t t_gpa;\n\n\tBUG_ON(!mmu_is_nested(vcpu));\n\n\t/* NPT walks are always user-walks */\n\taccess |= PFERR_USER_MASK;\n\tt_gpa  = mmu->gva_to_gpa(vcpu, mmu, gpa, access, exception);\n\n\treturn t_gpa;\n}\n\ngpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t      struct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tu64 access = (static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\treturn mmu->gva_to_gpa(vcpu, mmu, gva, access, exception);\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_gva_to_gpa_read);\n\ngpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t       struct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tu64 access = (static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\taccess |= PFERR_WRITE_MASK;\n\treturn mmu->gva_to_gpa(vcpu, mmu, gva, access, exception);\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_gva_to_gpa_write);\n\n/* uses this to access any guest's mapped memory without checking CPL */\ngpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t\tstruct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\treturn mmu->gva_to_gpa(vcpu, mmu, gva, 0, exception);\n}\n\nstatic int kvm_read_guest_virt_helper(gva_t addr, void *val, unsigned int bytes,\n\t\t\t\t      struct kvm_vcpu *vcpu, u64 access,\n\t\t\t\t      struct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tvoid *data = val;\n\tint r = X86EMUL_CONTINUE;\n\n\twhile (bytes) {\n\t\tgpa_t gpa = mmu->gva_to_gpa(vcpu, mmu, addr, access, exception);\n\t\tunsigned offset = addr & (PAGE_SIZE-1);\n\t\tunsigned toread = min(bytes, (unsigned)PAGE_SIZE - offset);\n\t\tint ret;\n\n\t\tif (gpa == INVALID_GPA)\n\t\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t\tret = kvm_vcpu_read_guest_page(vcpu, gpa >> PAGE_SHIFT, data,\n\t\t\t\t\t       offset, toread);\n\t\tif (ret < 0) {\n\t\t\tr = X86EMUL_IO_NEEDED;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbytes -= toread;\n\t\tdata += toread;\n\t\taddr += toread;\n\t}\nout:\n\treturn r;\n}\n\n/* used for instruction fetching */\nstatic int kvm_fetch_guest_virt(struct x86_emulate_ctxt *ctxt,\n\t\t\t\tgva_t addr, void *val, unsigned int bytes,\n\t\t\t\tstruct x86_exception *exception)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tu64 access = (static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\tunsigned offset;\n\tint ret;\n\n\t/* Inline kvm_read_guest_virt_helper for speed.  */\n\tgpa_t gpa = mmu->gva_to_gpa(vcpu, mmu, addr, access|PFERR_FETCH_MASK,\n\t\t\t\t    exception);\n\tif (unlikely(gpa == INVALID_GPA))\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\n\toffset = addr & (PAGE_SIZE-1);\n\tif (WARN_ON(offset + bytes > PAGE_SIZE))\n\t\tbytes = (unsigned)PAGE_SIZE - offset;\n\tret = kvm_vcpu_read_guest_page(vcpu, gpa >> PAGE_SHIFT, val,\n\t\t\t\t       offset, bytes);\n\tif (unlikely(ret < 0))\n\t\treturn X86EMUL_IO_NEEDED;\n\n\treturn X86EMUL_CONTINUE;\n}\n\nint kvm_read_guest_virt(struct kvm_vcpu *vcpu,\n\t\t\t       gva_t addr, void *val, unsigned int bytes,\n\t\t\t       struct x86_exception *exception)\n{\n\tu64 access = (static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\n\t/*\n\t * FIXME: this should call handle_emulation_failure if X86EMUL_IO_NEEDED\n\t * is returned, but our callers are not ready for that and they blindly\n\t * call kvm_inject_page_fault.  Ensure that they at least do not leak\n\t * uninitialized kernel stack memory into cr2 and error code.\n\t */\n\tmemset(exception, 0, sizeof(*exception));\n\treturn kvm_read_guest_virt_helper(addr, val, bytes, vcpu, access,\n\t\t\t\t\t  exception);\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest_virt);\n\nstatic int emulator_read_std(struct x86_emulate_ctxt *ctxt,\n\t\t\t     gva_t addr, void *val, unsigned int bytes,\n\t\t\t     struct x86_exception *exception, bool system)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tu64 access = 0;\n\n\tif (system)\n\t\taccess |= PFERR_IMPLICIT_ACCESS;\n\telse if (static_call(kvm_x86_get_cpl)(vcpu) == 3)\n\t\taccess |= PFERR_USER_MASK;\n\n\treturn kvm_read_guest_virt_helper(addr, val, bytes, vcpu, access, exception);\n}\n\nstatic int kvm_write_guest_virt_helper(gva_t addr, void *val, unsigned int bytes,\n\t\t\t\t      struct kvm_vcpu *vcpu, u64 access,\n\t\t\t\t      struct x86_exception *exception)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tvoid *data = val;\n\tint r = X86EMUL_CONTINUE;\n\n\twhile (bytes) {\n\t\tgpa_t gpa = mmu->gva_to_gpa(vcpu, mmu, addr, access, exception);\n\t\tunsigned offset = addr & (PAGE_SIZE-1);\n\t\tunsigned towrite = min(bytes, (unsigned)PAGE_SIZE - offset);\n\t\tint ret;\n\n\t\tif (gpa == INVALID_GPA)\n\t\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t\tret = kvm_vcpu_write_guest(vcpu, gpa, data, towrite);\n\t\tif (ret < 0) {\n\t\t\tr = X86EMUL_IO_NEEDED;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbytes -= towrite;\n\t\tdata += towrite;\n\t\taddr += towrite;\n\t}\nout:\n\treturn r;\n}\n\nstatic int emulator_write_std(struct x86_emulate_ctxt *ctxt, gva_t addr, void *val,\n\t\t\t      unsigned int bytes, struct x86_exception *exception,\n\t\t\t      bool system)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tu64 access = PFERR_WRITE_MASK;\n\n\tif (system)\n\t\taccess |= PFERR_IMPLICIT_ACCESS;\n\telse if (static_call(kvm_x86_get_cpl)(vcpu) == 3)\n\t\taccess |= PFERR_USER_MASK;\n\n\treturn kvm_write_guest_virt_helper(addr, val, bytes, vcpu,\n\t\t\t\t\t   access, exception);\n}\n\nint kvm_write_guest_virt_system(struct kvm_vcpu *vcpu, gva_t addr, void *val,\n\t\t\t\tunsigned int bytes, struct x86_exception *exception)\n{\n\t/* kvm_write_guest_virt_system can pull in tons of pages. */\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\treturn kvm_write_guest_virt_helper(addr, val, bytes, vcpu,\n\t\t\t\t\t   PFERR_WRITE_MASK, exception);\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_virt_system);\n\nstatic int kvm_can_emulate_insn(struct kvm_vcpu *vcpu, int emul_type,\n\t\t\t\tvoid *insn, int insn_len)\n{\n\treturn static_call(kvm_x86_can_emulate_instruction)(vcpu, emul_type,\n\t\t\t\t\t\t\t    insn, insn_len);\n}\n\nint handle_ud(struct kvm_vcpu *vcpu)\n{\n\tstatic const char kvm_emulate_prefix[] = { __KVM_EMULATE_PREFIX };\n\tint fep_flags = READ_ONCE(force_emulation_prefix);\n\tint emul_type = EMULTYPE_TRAP_UD;\n\tchar sig[5]; /* ud2; .ascii \"kvm\" */\n\tstruct x86_exception e;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emul_type, NULL, 0)))\n\t\treturn 1;\n\n\tif (fep_flags &&\n\t    kvm_read_guest_virt(vcpu, kvm_get_linear_rip(vcpu),\n\t\t\t\tsig, sizeof(sig), &e) == 0 &&\n\t    memcmp(sig, kvm_emulate_prefix, sizeof(sig)) == 0) {\n\t\tif (fep_flags & KVM_FEP_CLEAR_RFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, kvm_get_rflags(vcpu) & ~X86_EFLAGS_RF);\n\t\tkvm_rip_write(vcpu, kvm_rip_read(vcpu) + sizeof(sig));\n\t\temul_type = EMULTYPE_TRAP_UD_FORCED;\n\t}\n\n\treturn kvm_emulate_instruction(vcpu, emul_type);\n}\nEXPORT_SYMBOL_GPL(handle_ud);\n\nstatic int vcpu_is_mmio_gpa(struct kvm_vcpu *vcpu, unsigned long gva,\n\t\t\t    gpa_t gpa, bool write)\n{\n\t/* For APIC access vmexit */\n\tif ((gpa & PAGE_MASK) == APIC_DEFAULT_PHYS_BASE)\n\t\treturn 1;\n\n\tif (vcpu_match_mmio_gpa(vcpu, gpa)) {\n\t\ttrace_vcpu_match_mmio(gva, gpa, write, true);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int vcpu_mmio_gva_to_gpa(struct kvm_vcpu *vcpu, unsigned long gva,\n\t\t\t\tgpa_t *gpa, struct x86_exception *exception,\n\t\t\t\tbool write)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tu64 access = ((static_call(kvm_x86_get_cpl)(vcpu) == 3) ? PFERR_USER_MASK : 0)\n\t\t| (write ? PFERR_WRITE_MASK : 0);\n\n\t/*\n\t * currently PKRU is only applied to ept enabled guest so\n\t * there is no pkey in EPT page table for L1 guest or EPT\n\t * shadow page table for L2 guest.\n\t */\n\tif (vcpu_match_mmio_gva(vcpu, gva) && (!is_paging(vcpu) ||\n\t    !permission_fault(vcpu, vcpu->arch.walk_mmu,\n\t\t\t      vcpu->arch.mmio_access, 0, access))) {\n\t\t*gpa = vcpu->arch.mmio_gfn << PAGE_SHIFT |\n\t\t\t\t\t(gva & (PAGE_SIZE - 1));\n\t\ttrace_vcpu_match_mmio(gva, *gpa, write, false);\n\t\treturn 1;\n\t}\n\n\t*gpa = mmu->gva_to_gpa(vcpu, mmu, gva, access, exception);\n\n\tif (*gpa == INVALID_GPA)\n\t\treturn -1;\n\n\treturn vcpu_is_mmio_gpa(vcpu, gva, *gpa, write);\n}\n\nint emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\tconst void *val, int bytes)\n{\n\tint ret;\n\n\tret = kvm_vcpu_write_guest(vcpu, gpa, val, bytes);\n\tif (ret < 0)\n\t\treturn 0;\n\tkvm_page_track_write(vcpu, gpa, val, bytes);\n\treturn 1;\n}\n\nstruct read_write_emulator_ops {\n\tint (*read_write_prepare)(struct kvm_vcpu *vcpu, void *val,\n\t\t\t\t  int bytes);\n\tint (*read_write_emulate)(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t\t  void *val, int bytes);\n\tint (*read_write_mmio)(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t       int bytes, void *val);\n\tint (*read_write_exit_mmio)(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t\t    void *val, int bytes);\n\tbool write;\n};\n\nstatic int read_prepare(struct kvm_vcpu *vcpu, void *val, int bytes)\n{\n\tif (vcpu->mmio_read_completed) {\n\t\ttrace_kvm_mmio(KVM_TRACE_MMIO_READ, bytes,\n\t\t\t       vcpu->mmio_fragments[0].gpa, val);\n\t\tvcpu->mmio_read_completed = 0;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int read_emulate(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\tvoid *val, int bytes)\n{\n\treturn !kvm_vcpu_read_guest(vcpu, gpa, val, bytes);\n}\n\nstatic int write_emulate(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t void *val, int bytes)\n{\n\treturn emulator_write_phys(vcpu, gpa, val, bytes);\n}\n\nstatic int write_mmio(struct kvm_vcpu *vcpu, gpa_t gpa, int bytes, void *val)\n{\n\ttrace_kvm_mmio(KVM_TRACE_MMIO_WRITE, bytes, gpa, val);\n\treturn vcpu_mmio_write(vcpu, gpa, bytes, val);\n}\n\nstatic int read_exit_mmio(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t  void *val, int bytes)\n{\n\ttrace_kvm_mmio(KVM_TRACE_MMIO_READ_UNSATISFIED, bytes, gpa, NULL);\n\treturn X86EMUL_IO_NEEDED;\n}\n\nstatic int write_exit_mmio(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t   void *val, int bytes)\n{\n\tstruct kvm_mmio_fragment *frag = &vcpu->mmio_fragments[0];\n\n\tmemcpy(vcpu->run->mmio.data, frag->data, min(8u, frag->len));\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic const struct read_write_emulator_ops read_emultor = {\n\t.read_write_prepare = read_prepare,\n\t.read_write_emulate = read_emulate,\n\t.read_write_mmio = vcpu_mmio_read,\n\t.read_write_exit_mmio = read_exit_mmio,\n};\n\nstatic const struct read_write_emulator_ops write_emultor = {\n\t.read_write_emulate = write_emulate,\n\t.read_write_mmio = write_mmio,\n\t.read_write_exit_mmio = write_exit_mmio,\n\t.write = true,\n};\n\nstatic int emulator_read_write_onepage(unsigned long addr, void *val,\n\t\t\t\t       unsigned int bytes,\n\t\t\t\t       struct x86_exception *exception,\n\t\t\t\t       struct kvm_vcpu *vcpu,\n\t\t\t\t       const struct read_write_emulator_ops *ops)\n{\n\tgpa_t gpa;\n\tint handled, ret;\n\tbool write = ops->write;\n\tstruct kvm_mmio_fragment *frag;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\n\t/*\n\t * If the exit was due to a NPF we may already have a GPA.\n\t * If the GPA is present, use it to avoid the GVA to GPA table walk.\n\t * Note, this cannot be used on string operations since string\n\t * operation using rep will only have the initial GPA from the NPF\n\t * occurred.\n\t */\n\tif (ctxt->gpa_available && emulator_can_use_gpa(ctxt) &&\n\t    (addr & ~PAGE_MASK) == (ctxt->gpa_val & ~PAGE_MASK)) {\n\t\tgpa = ctxt->gpa_val;\n\t\tret = vcpu_is_mmio_gpa(vcpu, addr, gpa, write);\n\t} else {\n\t\tret = vcpu_mmio_gva_to_gpa(vcpu, addr, &gpa, exception, write);\n\t\tif (ret < 0)\n\t\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\tif (!ret && ops->read_write_emulate(vcpu, gpa, val, bytes))\n\t\treturn X86EMUL_CONTINUE;\n\n\t/*\n\t * Is this MMIO handled locally?\n\t */\n\thandled = ops->read_write_mmio(vcpu, gpa, bytes, val);\n\tif (handled == bytes)\n\t\treturn X86EMUL_CONTINUE;\n\n\tgpa += handled;\n\tbytes -= handled;\n\tval += handled;\n\n\tWARN_ON(vcpu->mmio_nr_fragments >= KVM_MAX_MMIO_FRAGMENTS);\n\tfrag = &vcpu->mmio_fragments[vcpu->mmio_nr_fragments++];\n\tfrag->gpa = gpa;\n\tfrag->data = val;\n\tfrag->len = bytes;\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic int emulator_read_write(struct x86_emulate_ctxt *ctxt,\n\t\t\tunsigned long addr,\n\t\t\tvoid *val, unsigned int bytes,\n\t\t\tstruct x86_exception *exception,\n\t\t\tconst struct read_write_emulator_ops *ops)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tgpa_t gpa;\n\tint rc;\n\n\tif (ops->read_write_prepare &&\n\t\t  ops->read_write_prepare(vcpu, val, bytes))\n\t\treturn X86EMUL_CONTINUE;\n\n\tvcpu->mmio_nr_fragments = 0;\n\n\t/* Crossing a page boundary? */\n\tif (((addr + bytes - 1) ^ addr) & PAGE_MASK) {\n\t\tint now;\n\n\t\tnow = -addr & ~PAGE_MASK;\n\t\trc = emulator_read_write_onepage(addr, val, now, exception,\n\t\t\t\t\t\t vcpu, ops);\n\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t\taddr += now;\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\taddr = (u32)addr;\n\t\tval += now;\n\t\tbytes -= now;\n\t}\n\n\trc = emulator_read_write_onepage(addr, val, bytes, exception,\n\t\t\t\t\t vcpu, ops);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (!vcpu->mmio_nr_fragments)\n\t\treturn rc;\n\n\tgpa = vcpu->mmio_fragments[0].gpa;\n\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_cur_fragment = 0;\n\n\tvcpu->run->mmio.len = min(8u, vcpu->mmio_fragments[0].len);\n\tvcpu->run->mmio.is_write = vcpu->mmio_is_write = ops->write;\n\tvcpu->run->exit_reason = KVM_EXIT_MMIO;\n\tvcpu->run->mmio.phys_addr = gpa;\n\n\treturn ops->read_write_exit_mmio(vcpu, gpa, val, bytes);\n}\n\nstatic int emulator_read_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  unsigned long addr,\n\t\t\t\t  void *val,\n\t\t\t\t  unsigned int bytes,\n\t\t\t\t  struct x86_exception *exception)\n{\n\treturn emulator_read_write(ctxt, addr, val, bytes,\n\t\t\t\t   exception, &read_emultor);\n}\n\nstatic int emulator_write_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t    unsigned long addr,\n\t\t\t    const void *val,\n\t\t\t    unsigned int bytes,\n\t\t\t    struct x86_exception *exception)\n{\n\treturn emulator_read_write(ctxt, addr, (void *)val, bytes,\n\t\t\t\t   exception, &write_emultor);\n}\n\n#define emulator_try_cmpxchg_user(t, ptr, old, new) \\\n\t(__try_cmpxchg_user((t __user *)(ptr), (t *)(old), *(t *)(new), efault ## t))\n\nstatic int emulator_cmpxchg_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t     unsigned long addr,\n\t\t\t\t     const void *old,\n\t\t\t\t     const void *new,\n\t\t\t\t     unsigned int bytes,\n\t\t\t\t     struct x86_exception *exception)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tu64 page_line_mask;\n\tunsigned long hva;\n\tgpa_t gpa;\n\tint r;\n\n\t/* guests cmpxchg8b have to be emulated atomically */\n\tif (bytes > 8 || (bytes & (bytes - 1)))\n\t\tgoto emul_write;\n\n\tgpa = kvm_mmu_gva_to_gpa_write(vcpu, addr, NULL);\n\n\tif (gpa == INVALID_GPA ||\n\t    (gpa & PAGE_MASK) == APIC_DEFAULT_PHYS_BASE)\n\t\tgoto emul_write;\n\n\t/*\n\t * Emulate the atomic as a straight write to avoid #AC if SLD is\n\t * enabled in the host and the access splits a cache line.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_SPLIT_LOCK_DETECT))\n\t\tpage_line_mask = ~(cache_line_size() - 1);\n\telse\n\t\tpage_line_mask = PAGE_MASK;\n\n\tif (((gpa + bytes - 1) & page_line_mask) != (gpa & page_line_mask))\n\t\tgoto emul_write;\n\n\thva = kvm_vcpu_gfn_to_hva(vcpu, gpa_to_gfn(gpa));\n\tif (kvm_is_error_hva(hva))\n\t\tgoto emul_write;\n\n\thva += offset_in_page(gpa);\n\n\tswitch (bytes) {\n\tcase 1:\n\t\tr = emulator_try_cmpxchg_user(u8, hva, old, new);\n\t\tbreak;\n\tcase 2:\n\t\tr = emulator_try_cmpxchg_user(u16, hva, old, new);\n\t\tbreak;\n\tcase 4:\n\t\tr = emulator_try_cmpxchg_user(u32, hva, old, new);\n\t\tbreak;\n\tcase 8:\n\t\tr = emulator_try_cmpxchg_user(u64, hva, old, new);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tif (r < 0)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\tif (r)\n\t\treturn X86EMUL_CMPXCHG_FAILED;\n\n\tkvm_page_track_write(vcpu, gpa, new, bytes);\n\n\treturn X86EMUL_CONTINUE;\n\nemul_write:\n\tprintk_once(KERN_WARNING \"kvm: emulating exchange as write\\n\");\n\n\treturn emulator_write_emulated(ctxt, addr, new, bytes, exception);\n}\n\nstatic int emulator_pio_in_out(struct kvm_vcpu *vcpu, int size,\n\t\t\t       unsigned short port, void *data,\n\t\t\t       unsigned int count, bool in)\n{\n\tunsigned i;\n\tint r;\n\n\tWARN_ON_ONCE(vcpu->arch.pio.count);\n\tfor (i = 0; i < count; i++) {\n\t\tif (in)\n\t\t\tr = kvm_io_bus_read(vcpu, KVM_PIO_BUS, port, size, data);\n\t\telse\n\t\t\tr = kvm_io_bus_write(vcpu, KVM_PIO_BUS, port, size, data);\n\n\t\tif (r) {\n\t\t\tif (i == 0)\n\t\t\t\tgoto userspace_io;\n\n\t\t\t/*\n\t\t\t * Userspace must have unregistered the device while PIO\n\t\t\t * was running.  Drop writes / read as 0.\n\t\t\t */\n\t\t\tif (in)\n\t\t\t\tmemset(data, 0, size * (count - i));\n\t\t\tbreak;\n\t\t}\n\n\t\tdata += size;\n\t}\n\treturn 1;\n\nuserspace_io:\n\tvcpu->arch.pio.port = port;\n\tvcpu->arch.pio.in = in;\n\tvcpu->arch.pio.count = count;\n\tvcpu->arch.pio.size = size;\n\n\tif (in)\n\t\tmemset(vcpu->arch.pio_data, 0, size * count);\n\telse\n\t\tmemcpy(vcpu->arch.pio_data, data, size * count);\n\n\tvcpu->run->exit_reason = KVM_EXIT_IO;\n\tvcpu->run->io.direction = in ? KVM_EXIT_IO_IN : KVM_EXIT_IO_OUT;\n\tvcpu->run->io.size = size;\n\tvcpu->run->io.data_offset = KVM_PIO_PAGE_OFFSET * PAGE_SIZE;\n\tvcpu->run->io.count = count;\n\tvcpu->run->io.port = port;\n\treturn 0;\n}\n\nstatic int emulator_pio_in(struct kvm_vcpu *vcpu, int size,\n      \t\t\t   unsigned short port, void *val, unsigned int count)\n{\n\tint r = emulator_pio_in_out(vcpu, size, port, val, count, true);\n\tif (r)\n\t\ttrace_kvm_pio(KVM_PIO_IN, port, size, count, val);\n\n\treturn r;\n}\n\nstatic void complete_emulator_pio_in(struct kvm_vcpu *vcpu, void *val)\n{\n\tint size = vcpu->arch.pio.size;\n\tunsigned int count = vcpu->arch.pio.count;\n\tmemcpy(val, vcpu->arch.pio_data, size * count);\n\ttrace_kvm_pio(KVM_PIO_IN, vcpu->arch.pio.port, size, count, vcpu->arch.pio_data);\n\tvcpu->arch.pio.count = 0;\n}\n\nstatic int emulator_pio_in_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t    int size, unsigned short port, void *val,\n\t\t\t\t    unsigned int count)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tif (vcpu->arch.pio.count) {\n\t\t/*\n\t\t * Complete a previous iteration that required userspace I/O.\n\t\t * Note, @count isn't guaranteed to match pio.count as userspace\n\t\t * can modify ECX before rerunning the vCPU.  Ignore any such\n\t\t * shenanigans as KVM doesn't support modifying the rep count,\n\t\t * and the emulator ensures @count doesn't overflow the buffer.\n\t\t */\n\t\tcomplete_emulator_pio_in(vcpu, val);\n\t\treturn 1;\n\t}\n\n\treturn emulator_pio_in(vcpu, size, port, val, count);\n}\n\nstatic int emulator_pio_out(struct kvm_vcpu *vcpu, int size,\n\t\t\t    unsigned short port, const void *val,\n\t\t\t    unsigned int count)\n{\n\ttrace_kvm_pio(KVM_PIO_OUT, port, size, count, val);\n\treturn emulator_pio_in_out(vcpu, size, port, (void *)val, count, false);\n}\n\nstatic int emulator_pio_out_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t     int size, unsigned short port,\n\t\t\t\t     const void *val, unsigned int count)\n{\n\treturn emulator_pio_out(emul_to_vcpu(ctxt), size, port, val, count);\n}\n\nstatic unsigned long get_segment_base(struct kvm_vcpu *vcpu, int seg)\n{\n\treturn static_call(kvm_x86_get_segment_base)(vcpu, seg);\n}\n\nstatic void emulator_invlpg(struct x86_emulate_ctxt *ctxt, ulong address)\n{\n\tkvm_mmu_invlpg(emul_to_vcpu(ctxt), address);\n}\n\nstatic int kvm_emulate_wbinvd_noskip(struct kvm_vcpu *vcpu)\n{\n\tif (!need_emulate_wbinvd(vcpu))\n\t\treturn X86EMUL_CONTINUE;\n\n\tif (static_call(kvm_x86_has_wbinvd_exit)()) {\n\t\tint cpu = get_cpu();\n\n\t\tcpumask_set_cpu(cpu, vcpu->arch.wbinvd_dirty_mask);\n\t\ton_each_cpu_mask(vcpu->arch.wbinvd_dirty_mask,\n\t\t\t\twbinvd_ipi, NULL, 1);\n\t\tput_cpu();\n\t\tcpumask_clear(vcpu->arch.wbinvd_dirty_mask);\n\t} else\n\t\twbinvd();\n\treturn X86EMUL_CONTINUE;\n}\n\nint kvm_emulate_wbinvd(struct kvm_vcpu *vcpu)\n{\n\tkvm_emulate_wbinvd_noskip(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_wbinvd);\n\n\n\nstatic void emulator_wbinvd(struct x86_emulate_ctxt *ctxt)\n{\n\tkvm_emulate_wbinvd_noskip(emul_to_vcpu(ctxt));\n}\n\nstatic void emulator_get_dr(struct x86_emulate_ctxt *ctxt, int dr,\n\t\t\t    unsigned long *dest)\n{\n\tkvm_get_dr(emul_to_vcpu(ctxt), dr, dest);\n}\n\nstatic int emulator_set_dr(struct x86_emulate_ctxt *ctxt, int dr,\n\t\t\t   unsigned long value)\n{\n\n\treturn kvm_set_dr(emul_to_vcpu(ctxt), dr, value);\n}\n\nstatic u64 mk_cr_64(u64 curr_cr, u32 new_val)\n{\n\treturn (curr_cr & ~((1ULL << 32) - 1)) | new_val;\n}\n\nstatic unsigned long emulator_get_cr(struct x86_emulate_ctxt *ctxt, int cr)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tunsigned long value;\n\n\tswitch (cr) {\n\tcase 0:\n\t\tvalue = kvm_read_cr0(vcpu);\n\t\tbreak;\n\tcase 2:\n\t\tvalue = vcpu->arch.cr2;\n\t\tbreak;\n\tcase 3:\n\t\tvalue = kvm_read_cr3(vcpu);\n\t\tbreak;\n\tcase 4:\n\t\tvalue = kvm_read_cr4(vcpu);\n\t\tbreak;\n\tcase 8:\n\t\tvalue = kvm_get_cr8(vcpu);\n\t\tbreak;\n\tdefault:\n\t\tkvm_err(\"%s: unexpected cr %u\\n\", __func__, cr);\n\t\treturn 0;\n\t}\n\n\treturn value;\n}\n\nstatic int emulator_set_cr(struct x86_emulate_ctxt *ctxt, int cr, ulong val)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tint res = 0;\n\n\tswitch (cr) {\n\tcase 0:\n\t\tres = kvm_set_cr0(vcpu, mk_cr_64(kvm_read_cr0(vcpu), val));\n\t\tbreak;\n\tcase 2:\n\t\tvcpu->arch.cr2 = val;\n\t\tbreak;\n\tcase 3:\n\t\tres = kvm_set_cr3(vcpu, val);\n\t\tbreak;\n\tcase 4:\n\t\tres = kvm_set_cr4(vcpu, mk_cr_64(kvm_read_cr4(vcpu), val));\n\t\tbreak;\n\tcase 8:\n\t\tres = kvm_set_cr8(vcpu, val);\n\t\tbreak;\n\tdefault:\n\t\tkvm_err(\"%s: unexpected cr %u\\n\", __func__, cr);\n\t\tres = -1;\n\t}\n\n\treturn res;\n}\n\nstatic int emulator_get_cpl(struct x86_emulate_ctxt *ctxt)\n{\n\treturn static_call(kvm_x86_get_cpl)(emul_to_vcpu(ctxt));\n}\n\nstatic void emulator_get_gdt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tstatic_call(kvm_x86_get_gdt)(emul_to_vcpu(ctxt), dt);\n}\n\nstatic void emulator_get_idt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tstatic_call(kvm_x86_get_idt)(emul_to_vcpu(ctxt), dt);\n}\n\nstatic void emulator_set_gdt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tstatic_call(kvm_x86_set_gdt)(emul_to_vcpu(ctxt), dt);\n}\n\nstatic void emulator_set_idt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tstatic_call(kvm_x86_set_idt)(emul_to_vcpu(ctxt), dt);\n}\n\nstatic unsigned long emulator_get_cached_segment_base(\n\tstruct x86_emulate_ctxt *ctxt, int seg)\n{\n\treturn get_segment_base(emul_to_vcpu(ctxt), seg);\n}\n\nstatic bool emulator_get_segment(struct x86_emulate_ctxt *ctxt, u16 *selector,\n\t\t\t\t struct desc_struct *desc, u32 *base3,\n\t\t\t\t int seg)\n{\n\tstruct kvm_segment var;\n\n\tkvm_get_segment(emul_to_vcpu(ctxt), &var, seg);\n\t*selector = var.selector;\n\n\tif (var.unusable) {\n\t\tmemset(desc, 0, sizeof(*desc));\n\t\tif (base3)\n\t\t\t*base3 = 0;\n\t\treturn false;\n\t}\n\n\tif (var.g)\n\t\tvar.limit >>= 12;\n\tset_desc_limit(desc, var.limit);\n\tset_desc_base(desc, (unsigned long)var.base);\n#ifdef CONFIG_X86_64\n\tif (base3)\n\t\t*base3 = var.base >> 32;\n#endif\n\tdesc->type = var.type;\n\tdesc->s = var.s;\n\tdesc->dpl = var.dpl;\n\tdesc->p = var.present;\n\tdesc->avl = var.avl;\n\tdesc->l = var.l;\n\tdesc->d = var.db;\n\tdesc->g = var.g;\n\n\treturn true;\n}\n\nstatic void emulator_set_segment(struct x86_emulate_ctxt *ctxt, u16 selector,\n\t\t\t\t struct desc_struct *desc, u32 base3,\n\t\t\t\t int seg)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tstruct kvm_segment var;\n\n\tvar.selector = selector;\n\tvar.base = get_desc_base(desc);\n#ifdef CONFIG_X86_64\n\tvar.base |= ((u64)base3) << 32;\n#endif\n\tvar.limit = get_desc_limit(desc);\n\tif (desc->g)\n\t\tvar.limit = (var.limit << 12) | 0xfff;\n\tvar.type = desc->type;\n\tvar.dpl = desc->dpl;\n\tvar.db = desc->d;\n\tvar.s = desc->s;\n\tvar.l = desc->l;\n\tvar.g = desc->g;\n\tvar.avl = desc->avl;\n\tvar.present = desc->p;\n\tvar.unusable = !var.present;\n\tvar.padding = 0;\n\n\tkvm_set_segment(vcpu, &var, seg);\n\treturn;\n}\n\nstatic int emulator_get_msr_with_filter(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\tu32 msr_index, u64 *pdata)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tint r;\n\n\tr = kvm_get_msr_with_filter(vcpu, msr_index, pdata);\n\tif (r < 0)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tif (r) {\n\t\tif (kvm_msr_user_space(vcpu, msr_index, KVM_EXIT_X86_RDMSR, 0,\n\t\t\t\t       complete_emulated_rdmsr, r))\n\t\t\treturn X86EMUL_IO_NEEDED;\n\n\t\ttrace_kvm_msr_read_ex(msr_index);\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\ttrace_kvm_msr_read(msr_index, *pdata);\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic int emulator_set_msr_with_filter(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\tu32 msr_index, u64 data)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tint r;\n\n\tr = kvm_set_msr_with_filter(vcpu, msr_index, data);\n\tif (r < 0)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tif (r) {\n\t\tif (kvm_msr_user_space(vcpu, msr_index, KVM_EXIT_X86_WRMSR, data,\n\t\t\t\t       complete_emulated_msr_access, r))\n\t\t\treturn X86EMUL_IO_NEEDED;\n\n\t\ttrace_kvm_msr_write_ex(msr_index, data);\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\ttrace_kvm_msr_write(msr_index, data);\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic int emulator_get_msr(struct x86_emulate_ctxt *ctxt,\n\t\t\t    u32 msr_index, u64 *pdata)\n{\n\treturn kvm_get_msr(emul_to_vcpu(ctxt), msr_index, pdata);\n}\n\nstatic int emulator_check_pmc(struct x86_emulate_ctxt *ctxt,\n\t\t\t      u32 pmc)\n{\n\tif (kvm_pmu_is_valid_rdpmc_ecx(emul_to_vcpu(ctxt), pmc))\n\t\treturn 0;\n\treturn -EINVAL;\n}\n\nstatic int emulator_read_pmc(struct x86_emulate_ctxt *ctxt,\n\t\t\t     u32 pmc, u64 *pdata)\n{\n\treturn kvm_pmu_rdpmc(emul_to_vcpu(ctxt), pmc, pdata);\n}\n\nstatic void emulator_halt(struct x86_emulate_ctxt *ctxt)\n{\n\temul_to_vcpu(ctxt)->arch.halt_request = 1;\n}\n\nstatic int emulator_intercept(struct x86_emulate_ctxt *ctxt,\n\t\t\t      struct x86_instruction_info *info,\n\t\t\t      enum x86_intercept_stage stage)\n{\n\treturn static_call(kvm_x86_check_intercept)(emul_to_vcpu(ctxt), info, stage,\n\t\t\t\t\t    &ctxt->exception);\n}\n\nstatic bool emulator_get_cpuid(struct x86_emulate_ctxt *ctxt,\n\t\t\t      u32 *eax, u32 *ebx, u32 *ecx, u32 *edx,\n\t\t\t      bool exact_only)\n{\n\treturn kvm_cpuid(emul_to_vcpu(ctxt), eax, ebx, ecx, edx, exact_only);\n}\n\nstatic bool emulator_guest_has_long_mode(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_LM);\n}\n\nstatic bool emulator_guest_has_movbe(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_MOVBE);\n}\n\nstatic bool emulator_guest_has_fxsr(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_FXSR);\n}\n\nstatic bool emulator_guest_has_rdpid(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_RDPID);\n}\n\nstatic ulong emulator_read_gpr(struct x86_emulate_ctxt *ctxt, unsigned reg)\n{\n\treturn kvm_register_read_raw(emul_to_vcpu(ctxt), reg);\n}\n\nstatic void emulator_write_gpr(struct x86_emulate_ctxt *ctxt, unsigned reg, ulong val)\n{\n\tkvm_register_write_raw(emul_to_vcpu(ctxt), reg, val);\n}\n\nstatic void emulator_set_nmi_mask(struct x86_emulate_ctxt *ctxt, bool masked)\n{\n\tstatic_call(kvm_x86_set_nmi_mask)(emul_to_vcpu(ctxt), masked);\n}\n\nstatic unsigned emulator_get_hflags(struct x86_emulate_ctxt *ctxt)\n{\n\treturn emul_to_vcpu(ctxt)->arch.hflags;\n}\n\n#ifndef CONFIG_KVM_SMM\nstatic int emulator_leave_smm(struct x86_emulate_ctxt *ctxt)\n{\n\tWARN_ON_ONCE(1);\n\treturn X86EMUL_UNHANDLEABLE;\n}\n#endif\n\nstatic void emulator_triple_fault(struct x86_emulate_ctxt *ctxt)\n{\n\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, emul_to_vcpu(ctxt));\n}\n\nstatic int emulator_set_xcr(struct x86_emulate_ctxt *ctxt, u32 index, u64 xcr)\n{\n\treturn __kvm_set_xcr(emul_to_vcpu(ctxt), index, xcr);\n}\n\nstatic void emulator_vm_bugged(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct kvm *kvm = emul_to_vcpu(ctxt)->kvm;\n\n\tif (!kvm->vm_bugged)\n\t\tkvm_vm_bugged(kvm);\n}\n\nstatic const struct x86_emulate_ops emulate_ops = {\n\t.vm_bugged           = emulator_vm_bugged,\n\t.read_gpr            = emulator_read_gpr,\n\t.write_gpr           = emulator_write_gpr,\n\t.read_std            = emulator_read_std,\n\t.write_std           = emulator_write_std,\n\t.fetch               = kvm_fetch_guest_virt,\n\t.read_emulated       = emulator_read_emulated,\n\t.write_emulated      = emulator_write_emulated,\n\t.cmpxchg_emulated    = emulator_cmpxchg_emulated,\n\t.invlpg              = emulator_invlpg,\n\t.pio_in_emulated     = emulator_pio_in_emulated,\n\t.pio_out_emulated    = emulator_pio_out_emulated,\n\t.get_segment         = emulator_get_segment,\n\t.set_segment         = emulator_set_segment,\n\t.get_cached_segment_base = emulator_get_cached_segment_base,\n\t.get_gdt             = emulator_get_gdt,\n\t.get_idt\t     = emulator_get_idt,\n\t.set_gdt             = emulator_set_gdt,\n\t.set_idt\t     = emulator_set_idt,\n\t.get_cr              = emulator_get_cr,\n\t.set_cr              = emulator_set_cr,\n\t.cpl                 = emulator_get_cpl,\n\t.get_dr              = emulator_get_dr,\n\t.set_dr              = emulator_set_dr,\n\t.set_msr_with_filter = emulator_set_msr_with_filter,\n\t.get_msr_with_filter = emulator_get_msr_with_filter,\n\t.get_msr             = emulator_get_msr,\n\t.check_pmc\t     = emulator_check_pmc,\n\t.read_pmc            = emulator_read_pmc,\n\t.halt                = emulator_halt,\n\t.wbinvd              = emulator_wbinvd,\n\t.fix_hypercall       = emulator_fix_hypercall,\n\t.intercept           = emulator_intercept,\n\t.get_cpuid           = emulator_get_cpuid,\n\t.guest_has_long_mode = emulator_guest_has_long_mode,\n\t.guest_has_movbe     = emulator_guest_has_movbe,\n\t.guest_has_fxsr      = emulator_guest_has_fxsr,\n\t.guest_has_rdpid     = emulator_guest_has_rdpid,\n\t.set_nmi_mask        = emulator_set_nmi_mask,\n\t.get_hflags          = emulator_get_hflags,\n\t.leave_smm           = emulator_leave_smm,\n\t.triple_fault        = emulator_triple_fault,\n\t.set_xcr             = emulator_set_xcr,\n};\n\nstatic void toggle_interruptibility(struct kvm_vcpu *vcpu, u32 mask)\n{\n\tu32 int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);\n\t/*\n\t * an sti; sti; sequence only disable interrupts for the first\n\t * instruction. So, if the last instruction, be it emulated or\n\t * not, left the system with the INT_STI flag enabled, it\n\t * means that the last instruction is an sti. We should not\n\t * leave the flag on in this case. The same goes for mov ss\n\t */\n\tif (int_shadow & mask)\n\t\tmask = 0;\n\tif (unlikely(int_shadow || mask)) {\n\t\tstatic_call(kvm_x86_set_interrupt_shadow)(vcpu, mask);\n\t\tif (!mask)\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t}\n}\n\nstatic void inject_emulated_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\n\tif (ctxt->exception.vector == PF_VECTOR)\n\t\tkvm_inject_emulated_page_fault(vcpu, &ctxt->exception);\n\telse if (ctxt->exception.error_code_valid)\n\t\tkvm_queue_exception_e(vcpu, ctxt->exception.vector,\n\t\t\t\t      ctxt->exception.error_code);\n\telse\n\t\tkvm_queue_exception(vcpu, ctxt->exception.vector);\n}\n\nstatic struct x86_emulate_ctxt *alloc_emulate_ctxt(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt;\n\n\tctxt = kmem_cache_zalloc(x86_emulator_cache, GFP_KERNEL_ACCOUNT);\n\tif (!ctxt) {\n\t\tpr_err(\"kvm: failed to allocate vcpu's emulator\\n\");\n\t\treturn NULL;\n\t}\n\n\tctxt->vcpu = vcpu;\n\tctxt->ops = &emulate_ops;\n\tvcpu->arch.emulate_ctxt = ctxt;\n\n\treturn ctxt;\n}\n\nstatic void init_emulate_ctxt(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint cs_db, cs_l;\n\n\tstatic_call(kvm_x86_get_cs_db_l_bits)(vcpu, &cs_db, &cs_l);\n\n\tctxt->gpa_available = false;\n\tctxt->eflags = kvm_get_rflags(vcpu);\n\tctxt->tf = (ctxt->eflags & X86_EFLAGS_TF) != 0;\n\n\tctxt->eip = kvm_rip_read(vcpu);\n\tctxt->mode = (!is_protmode(vcpu))\t\t? X86EMUL_MODE_REAL :\n\t\t     (ctxt->eflags & X86_EFLAGS_VM)\t? X86EMUL_MODE_VM86 :\n\t\t     (cs_l && is_long_mode(vcpu))\t? X86EMUL_MODE_PROT64 :\n\t\t     cs_db\t\t\t\t? X86EMUL_MODE_PROT32 :\n\t\t\t\t\t\t\t  X86EMUL_MODE_PROT16;\n\tBUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);\n\n\tctxt->interruptibility = 0;\n\tctxt->have_exception = false;\n\tctxt->exception.vector = -1;\n\tctxt->perm_ok = false;\n\n\tinit_decode_cache(ctxt);\n\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n}\n\nvoid kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint ret;\n\n\tinit_emulate_ctxt(vcpu);\n\n\tctxt->op_bytes = 2;\n\tctxt->ad_bytes = 2;\n\tctxt->_eip = ctxt->eip + inc_eip;\n\tret = emulate_int_real(ctxt, irq);\n\n\tif (ret != X86EMUL_CONTINUE) {\n\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t} else {\n\t\tctxt->eip = ctxt->_eip;\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tkvm_set_rflags(vcpu, ctxt->eflags);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_inject_realmode_interrupt);\n\nstatic void prepare_emulation_failure_exit(struct kvm_vcpu *vcpu, u64 *data,\n\t\t\t\t\t   u8 ndata, u8 *insn_bytes, u8 insn_size)\n{\n\tstruct kvm_run *run = vcpu->run;\n\tu64 info[5];\n\tu8 info_start;\n\n\t/*\n\t * Zero the whole array used to retrieve the exit info, as casting to\n\t * u32 for select entries will leave some chunks uninitialized.\n\t */\n\tmemset(&info, 0, sizeof(info));\n\n\tstatic_call(kvm_x86_get_exit_info)(vcpu, (u32 *)&info[0], &info[1],\n\t\t\t\t\t   &info[2], (u32 *)&info[3],\n\t\t\t\t\t   (u32 *)&info[4]);\n\n\trun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\trun->emulation_failure.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\n\t/*\n\t * There's currently space for 13 entries, but 5 are used for the exit\n\t * reason and info.  Restrict to 4 to reduce the maintenance burden\n\t * when expanding kvm_run.emulation_failure in the future.\n\t */\n\tif (WARN_ON_ONCE(ndata > 4))\n\t\tndata = 4;\n\n\t/* Always include the flags as a 'data' entry. */\n\tinfo_start = 1;\n\trun->emulation_failure.flags = 0;\n\n\tif (insn_size) {\n\t\tBUILD_BUG_ON((sizeof(run->emulation_failure.insn_size) +\n\t\t\t      sizeof(run->emulation_failure.insn_bytes) != 16));\n\t\tinfo_start += 2;\n\t\trun->emulation_failure.flags |=\n\t\t\tKVM_INTERNAL_ERROR_EMULATION_FLAG_INSTRUCTION_BYTES;\n\t\trun->emulation_failure.insn_size = insn_size;\n\t\tmemset(run->emulation_failure.insn_bytes, 0x90,\n\t\t       sizeof(run->emulation_failure.insn_bytes));\n\t\tmemcpy(run->emulation_failure.insn_bytes, insn_bytes, insn_size);\n\t}\n\n\tmemcpy(&run->internal.data[info_start], info, sizeof(info));\n\tmemcpy(&run->internal.data[info_start + ARRAY_SIZE(info)], data,\n\t       ndata * sizeof(data[0]));\n\n\trun->emulation_failure.ndata = info_start + ARRAY_SIZE(info) + ndata;\n}\n\nstatic void prepare_emulation_ctxt_failure_exit(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\n\tprepare_emulation_failure_exit(vcpu, NULL, 0, ctxt->fetch.data,\n\t\t\t\t       ctxt->fetch.end - ctxt->fetch.data);\n}\n\nvoid __kvm_prepare_emulation_failure_exit(struct kvm_vcpu *vcpu, u64 *data,\n\t\t\t\t\t  u8 ndata)\n{\n\tprepare_emulation_failure_exit(vcpu, data, ndata, NULL, 0);\n}\nEXPORT_SYMBOL_GPL(__kvm_prepare_emulation_failure_exit);\n\nvoid kvm_prepare_emulation_failure_exit(struct kvm_vcpu *vcpu)\n{\n\t__kvm_prepare_emulation_failure_exit(vcpu, NULL, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_prepare_emulation_failure_exit);\n\nstatic int handle_emulation_failure(struct kvm_vcpu *vcpu, int emulation_type)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\n\t++vcpu->stat.insn_emulation_fail;\n\ttrace_kvm_emulate_insn_failed(vcpu);\n\n\tif (emulation_type & EMULTYPE_VMWARE_GP) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\tif (kvm->arch.exit_on_emulation_error ||\n\t    (emulation_type & EMULTYPE_SKIP)) {\n\t\tprepare_emulation_ctxt_failure_exit(vcpu);\n\t\treturn 0;\n\t}\n\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\n\tif (!is_guest_mode(vcpu) && static_call(kvm_x86_get_cpl)(vcpu) == 0) {\n\t\tprepare_emulation_ctxt_failure_exit(vcpu);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic bool reexecute_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t\t  bool write_fault_to_shadow_pgtable,\n\t\t\t\t  int emulation_type)\n{\n\tgpa_t gpa = cr2_or_gpa;\n\tkvm_pfn_t pfn;\n\n\tif (!(emulation_type & EMULTYPE_ALLOW_RETRY_PF))\n\t\treturn false;\n\n\tif (WARN_ON_ONCE(is_guest_mode(vcpu)) ||\n\t    WARN_ON_ONCE(!(emulation_type & EMULTYPE_PF)))\n\t\treturn false;\n\n\tif (!vcpu->arch.mmu->root_role.direct) {\n\t\t/*\n\t\t * Write permission should be allowed since only\n\t\t * write access need to be emulated.\n\t\t */\n\t\tgpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2_or_gpa, NULL);\n\n\t\t/*\n\t\t * If the mapping is invalid in guest, let cpu retry\n\t\t * it to generate fault.\n\t\t */\n\t\tif (gpa == INVALID_GPA)\n\t\t\treturn true;\n\t}\n\n\t/*\n\t * Do not retry the unhandleable instruction if it faults on the\n\t * readonly host memory, otherwise it will goto a infinite loop:\n\t * retry instruction -> write #PF -> emulation fail -> retry\n\t * instruction -> ...\n\t */\n\tpfn = gfn_to_pfn(vcpu->kvm, gpa_to_gfn(gpa));\n\n\t/*\n\t * If the instruction failed on the error pfn, it can not be fixed,\n\t * report the error to userspace.\n\t */\n\tif (is_error_noslot_pfn(pfn))\n\t\treturn false;\n\n\tkvm_release_pfn_clean(pfn);\n\n\t/* The instructions are well-emulated on direct mmu. */\n\tif (vcpu->arch.mmu->root_role.direct) {\n\t\tunsigned int indirect_shadow_pages;\n\n\t\twrite_lock(&vcpu->kvm->mmu_lock);\n\t\tindirect_shadow_pages = vcpu->kvm->arch.indirect_shadow_pages;\n\t\twrite_unlock(&vcpu->kvm->mmu_lock);\n\n\t\tif (indirect_shadow_pages)\n\t\t\tkvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\t\treturn true;\n\t}\n\n\t/*\n\t * if emulation was due to access to shadowed page table\n\t * and it failed try to unshadow page and re-enter the\n\t * guest to let CPU execute the instruction.\n\t */\n\tkvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\t/*\n\t * If the access faults on its page table, it can not\n\t * be fixed by unprotecting shadow page and it should\n\t * be reported to userspace.\n\t */\n\treturn !write_fault_to_shadow_pgtable;\n}\n\nstatic bool retry_instruction(struct x86_emulate_ctxt *ctxt,\n\t\t\t      gpa_t cr2_or_gpa,  int emulation_type)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tunsigned long last_retry_eip, last_retry_addr, gpa = cr2_or_gpa;\n\n\tlast_retry_eip = vcpu->arch.last_retry_eip;\n\tlast_retry_addr = vcpu->arch.last_retry_addr;\n\n\t/*\n\t * If the emulation is caused by #PF and it is non-page_table\n\t * writing instruction, it means the VM-EXIT is caused by shadow\n\t * page protected, we can zap the shadow page and retry this\n\t * instruction directly.\n\t *\n\t * Note: if the guest uses a non-page-table modifying instruction\n\t * on the PDE that points to the instruction, then we will unmap\n\t * the instruction and go to an infinite loop. So, we cache the\n\t * last retried eip and the last fault address, if we meet the eip\n\t * and the address again, we can break out of the potential infinite\n\t * loop.\n\t */\n\tvcpu->arch.last_retry_eip = vcpu->arch.last_retry_addr = 0;\n\n\tif (!(emulation_type & EMULTYPE_ALLOW_RETRY_PF))\n\t\treturn false;\n\n\tif (WARN_ON_ONCE(is_guest_mode(vcpu)) ||\n\t    WARN_ON_ONCE(!(emulation_type & EMULTYPE_PF)))\n\t\treturn false;\n\n\tif (x86_page_table_writing_insn(ctxt))\n\t\treturn false;\n\n\tif (ctxt->eip == last_retry_eip && last_retry_addr == cr2_or_gpa)\n\t\treturn false;\n\n\tvcpu->arch.last_retry_eip = ctxt->eip;\n\tvcpu->arch.last_retry_addr = cr2_or_gpa;\n\n\tif (!vcpu->arch.mmu->root_role.direct)\n\t\tgpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2_or_gpa, NULL);\n\n\tkvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\treturn true;\n}\n\nstatic int complete_emulated_mmio(struct kvm_vcpu *vcpu);\nstatic int complete_emulated_pio(struct kvm_vcpu *vcpu);\n\nstatic int kvm_vcpu_check_hw_bp(unsigned long addr, u32 type, u32 dr7,\n\t\t\t\tunsigned long *db)\n{\n\tu32 dr6 = 0;\n\tint i;\n\tu32 enable, rwlen;\n\n\tenable = dr7;\n\trwlen = dr7 >> 16;\n\tfor (i = 0; i < 4; i++, enable >>= 2, rwlen >>= 4)\n\t\tif ((enable & 3) && (rwlen & 15) == type && db[i] == addr)\n\t\t\tdr6 |= (1 << i);\n\treturn dr6;\n}\n\nstatic int kvm_vcpu_do_singlestep(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *kvm_run = vcpu->run;\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP) {\n\t\tkvm_run->debug.arch.dr6 = DR6_BS | DR6_ACTIVE_LOW;\n\t\tkvm_run->debug.arch.pc = kvm_get_linear_rip(vcpu);\n\t\tkvm_run->debug.arch.exception = DB_VECTOR;\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\treturn 0;\n\t}\n\tkvm_queue_exception_p(vcpu, DB_VECTOR, DR6_BS);\n\treturn 1;\n}\n\nint kvm_skip_emulated_instruction(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\tint r;\n\n\tr = static_call(kvm_x86_skip_emulated_instruction)(vcpu);\n\tif (unlikely(!r))\n\t\treturn 0;\n\n\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\n\t/*\n\t * rflags is the old, \"raw\" value of the flags.  The new value has\n\t * not been saved yet.\n\t *\n\t * This is correct even for TF set by the guest, because \"the\n\t * processor will not generate this exception after the instruction\n\t * that sets the TF flag\".\n\t */\n\tif (unlikely(rflags & X86_EFLAGS_TF))\n\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_skip_emulated_instruction);\n\nstatic bool kvm_is_code_breakpoint_inhibited(struct kvm_vcpu *vcpu)\n{\n\tu32 shadow;\n\n\tif (kvm_get_rflags(vcpu) & X86_EFLAGS_RF)\n\t\treturn true;\n\n\t/*\n\t * Intel CPUs inhibit code #DBs when MOV/POP SS blocking is active,\n\t * but AMD CPUs do not.  MOV/POP SS blocking is rare, check that first\n\t * to avoid the relatively expensive CPUID lookup.\n\t */\n\tshadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);\n\treturn (shadow & KVM_X86_SHADOW_INT_MOV_SS) &&\n\t       guest_cpuid_is_intel(vcpu);\n}\n\nstatic bool kvm_vcpu_check_code_breakpoint(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   int emulation_type, int *r)\n{\n\tWARN_ON_ONCE(emulation_type & EMULTYPE_NO_DECODE);\n\n\t/*\n\t * Do not check for code breakpoints if hardware has already done the\n\t * checks, as inferred from the emulation type.  On NO_DECODE and SKIP,\n\t * the instruction has passed all exception checks, and all intercepted\n\t * exceptions that trigger emulation have lower priority than code\n\t * breakpoints, i.e. the fact that the intercepted exception occurred\n\t * means any code breakpoints have already been serviced.\n\t *\n\t * Note, KVM needs to check for code #DBs on EMULTYPE_TRAP_UD_FORCED as\n\t * hardware has checked the RIP of the magic prefix, but not the RIP of\n\t * the instruction being emulated.  The intent of forced emulation is\n\t * to behave as if KVM intercepted the instruction without an exception\n\t * and without a prefix.\n\t */\n\tif (emulation_type & (EMULTYPE_NO_DECODE | EMULTYPE_SKIP |\n\t\t\t      EMULTYPE_TRAP_UD | EMULTYPE_VMWARE_GP | EMULTYPE_PF))\n\t\treturn false;\n\n\tif (unlikely(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) &&\n\t    (vcpu->arch.guest_debug_dr7 & DR7_BP_EN_MASK)) {\n\t\tstruct kvm_run *kvm_run = vcpu->run;\n\t\tunsigned long eip = kvm_get_linear_rip(vcpu);\n\t\tu32 dr6 = kvm_vcpu_check_hw_bp(eip, 0,\n\t\t\t\t\t   vcpu->arch.guest_debug_dr7,\n\t\t\t\t\t   vcpu->arch.eff_db);\n\n\t\tif (dr6 != 0) {\n\t\t\tkvm_run->debug.arch.dr6 = dr6 | DR6_ACTIVE_LOW;\n\t\t\tkvm_run->debug.arch.pc = eip;\n\t\t\tkvm_run->debug.arch.exception = DB_VECTOR;\n\t\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\t\t*r = 0;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (unlikely(vcpu->arch.dr7 & DR7_BP_EN_MASK) &&\n\t    !kvm_is_code_breakpoint_inhibited(vcpu)) {\n\t\tunsigned long eip = kvm_get_linear_rip(vcpu);\n\t\tu32 dr6 = kvm_vcpu_check_hw_bp(eip, 0,\n\t\t\t\t\t   vcpu->arch.dr7,\n\t\t\t\t\t   vcpu->arch.db);\n\n\t\tif (dr6 != 0) {\n\t\t\tkvm_queue_exception_p(vcpu, DB_VECTOR, dr6);\n\t\t\t*r = 1;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic bool is_vmware_backdoor_opcode(struct x86_emulate_ctxt *ctxt)\n{\n\tswitch (ctxt->opcode_len) {\n\tcase 1:\n\t\tswitch (ctxt->b) {\n\t\tcase 0xe4:\t/* IN */\n\t\tcase 0xe5:\n\t\tcase 0xec:\n\t\tcase 0xed:\n\t\tcase 0xe6:\t/* OUT */\n\t\tcase 0xe7:\n\t\tcase 0xee:\n\t\tcase 0xef:\n\t\tcase 0x6c:\t/* INS */\n\t\tcase 0x6d:\n\t\tcase 0x6e:\t/* OUTS */\n\t\tcase 0x6f:\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\tcase 2:\n\t\tswitch (ctxt->b) {\n\t\tcase 0x33:\t/* RDPMC */\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\n/*\n * Decode an instruction for emulation.  The caller is responsible for handling\n * code breakpoints.  Note, manually detecting code breakpoints is unnecessary\n * (and wrong) when emulating on an intercepted fault-like exception[*], as\n * code breakpoints have higher priority and thus have already been done by\n * hardware.\n *\n * [*] Except #MC, which is higher priority, but KVM should never emulate in\n *     response to a machine check.\n */\nint x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,\n\t\t\t\t    void *insn, int insn_len)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint r;\n\n\tinit_emulate_ctxt(vcpu);\n\n\tr = x86_decode_insn(ctxt, insn, insn_len, emulation_type);\n\n\ttrace_kvm_emulate_insn_start(vcpu);\n\t++vcpu->stat.insn_emulation;\n\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);\n\nint x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\t/*\n\t\t * Return immediately if RIP hits a code breakpoint, such #DBs\n\t\t * are fault-like and are higher priority than any faults on\n\t\t * the code fetch itself.\n\t\t */\n\t\tif (kvm_vcpu_check_code_breakpoint(vcpu, emulation_type, &r))\n\t\t\treturn r;\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\n\t\t\tif (ctxt->have_exception &&\n\t\t\t    !(emulation_type & EMULTYPE_SKIP)) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tinject_emulated_exception(vcpu);\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\n\t\t/*\n\t\t * Note, EXCPT_DB is assumed to be fault-like as the emulator\n\t\t * only supports code breakpoints and general detect #DB, both\n\t\t * of which are fault-like.\n\t\t */\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}\n\nint kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)\n{\n\treturn x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_instruction);\n\nint kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,\n\t\t\t\t\tvoid *insn, int insn_len)\n{\n\treturn x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_instruction_from_buffer);\n\nstatic int complete_fast_pio_out_port_0x7e(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.pio.count = 0;\n\treturn 1;\n}\n\nstatic int complete_fast_pio_out(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.pio.count = 0;\n\n\tif (unlikely(!kvm_is_linear_rip(vcpu, vcpu->arch.pio.linear_rip)))\n\t\treturn 1;\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size,\n\t\t\t    unsigned short port)\n{\n\tunsigned long val = kvm_rax_read(vcpu);\n\tint ret = emulator_pio_out(vcpu, size, port, &val, 1);\n\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Workaround userspace that relies on old KVM behavior of %rip being\n\t * incremented prior to exiting to userspace to handle \"OUT 0x7e\".\n\t */\n\tif (port == 0x7e &&\n\t    kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_OUT_7E_INC_RIP)) {\n\t\tvcpu->arch.complete_userspace_io =\n\t\t\tcomplete_fast_pio_out_port_0x7e;\n\t\tkvm_skip_emulated_instruction(vcpu);\n\t} else {\n\t\tvcpu->arch.pio.linear_rip = kvm_get_linear_rip(vcpu);\n\t\tvcpu->arch.complete_userspace_io = complete_fast_pio_out;\n\t}\n\treturn 0;\n}\n\nstatic int complete_fast_pio_in(struct kvm_vcpu *vcpu)\n{\n\tunsigned long val;\n\n\t/* We should only ever be called with arch.pio.count equal to 1 */\n\tBUG_ON(vcpu->arch.pio.count != 1);\n\n\tif (unlikely(!kvm_is_linear_rip(vcpu, vcpu->arch.pio.linear_rip))) {\n\t\tvcpu->arch.pio.count = 0;\n\t\treturn 1;\n\t}\n\n\t/* For size less than 4 we merge, else we zero extend */\n\tval = (vcpu->arch.pio.size < 4) ? kvm_rax_read(vcpu) : 0;\n\n\tcomplete_emulator_pio_in(vcpu, &val);\n\tkvm_rax_write(vcpu, val);\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int kvm_fast_pio_in(struct kvm_vcpu *vcpu, int size,\n\t\t\t   unsigned short port)\n{\n\tunsigned long val;\n\tint ret;\n\n\t/* For size less than 4 we merge, else we zero extend */\n\tval = (size < 4) ? kvm_rax_read(vcpu) : 0;\n\n\tret = emulator_pio_in(vcpu, size, port, &val, 1);\n\tif (ret) {\n\t\tkvm_rax_write(vcpu, val);\n\t\treturn ret;\n\t}\n\n\tvcpu->arch.pio.linear_rip = kvm_get_linear_rip(vcpu);\n\tvcpu->arch.complete_userspace_io = complete_fast_pio_in;\n\n\treturn 0;\n}\n\nint kvm_fast_pio(struct kvm_vcpu *vcpu, int size, unsigned short port, int in)\n{\n\tint ret;\n\n\tif (in)\n\t\tret = kvm_fast_pio_in(vcpu, size, port);\n\telse\n\t\tret = kvm_fast_pio_out(vcpu, size, port);\n\treturn ret && kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_fast_pio);\n\nstatic int kvmclock_cpu_down_prep(unsigned int cpu)\n{\n\t__this_cpu_write(cpu_tsc_khz, 0);\n\treturn 0;\n}\n\nstatic void tsc_khz_changed(void *data)\n{\n\tstruct cpufreq_freqs *freq = data;\n\tunsigned long khz = 0;\n\n\tWARN_ON_ONCE(boot_cpu_has(X86_FEATURE_CONSTANT_TSC));\n\n\tif (data)\n\t\tkhz = freq->new;\n\telse\n\t\tkhz = cpufreq_quick_get(raw_smp_processor_id());\n\tif (!khz)\n\t\tkhz = tsc_khz;\n\t__this_cpu_write(cpu_tsc_khz, khz);\n}\n\n#ifdef CONFIG_X86_64\nstatic void kvm_hyperv_tsc_notifier(void)\n{\n\tstruct kvm *kvm;\n\tint cpu;\n\n\tmutex_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\tkvm_make_mclock_inprogress_request(kvm);\n\n\t/* no guest entries from this point */\n\thyperv_stop_tsc_emulation();\n\n\t/* TSC frequency always matches when on Hyper-V */\n\tif (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {\n\t\tfor_each_present_cpu(cpu)\n\t\t\tper_cpu(cpu_tsc_khz, cpu) = tsc_khz;\n\t}\n\tkvm_caps.max_guest_tsc_khz = tsc_khz;\n\n\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\t__kvm_start_pvclock_update(kvm);\n\t\tpvclock_update_vm_gtod_copy(kvm);\n\t\tkvm_end_pvclock_update(kvm);\n\t}\n\n\tmutex_unlock(&kvm_lock);\n}\n#endif\n\nstatic void __kvmclock_cpufreq_notifier(struct cpufreq_freqs *freq, int cpu)\n{\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint send_ipi = 0;\n\tunsigned long i;\n\n\t/*\n\t * We allow guests to temporarily run on slowing clocks,\n\t * provided we notify them after, or to run on accelerating\n\t * clocks, provided we notify them before.  Thus time never\n\t * goes backwards.\n\t *\n\t * However, we have a problem.  We can't atomically update\n\t * the frequency of a given CPU from this function; it is\n\t * merely a notifier, which can be called from any CPU.\n\t * Changing the TSC frequency at arbitrary points in time\n\t * requires a recomputation of local variables related to\n\t * the TSC for each VCPU.  We must flag these local variables\n\t * to be updated and be sure the update takes place with the\n\t * new frequency before any guests proceed.\n\t *\n\t * Unfortunately, the combination of hotplug CPU and frequency\n\t * change creates an intractable locking scenario; the order\n\t * of when these callouts happen is undefined with respect to\n\t * CPU hotplug, and they can race with each other.  As such,\n\t * merely setting per_cpu(cpu_tsc_khz) = X during a hotadd is\n\t * undefined; you can actually have a CPU frequency change take\n\t * place in between the computation of X and the setting of the\n\t * variable.  To protect against this problem, all updates of\n\t * the per_cpu tsc_khz variable are done in an interrupt\n\t * protected IPI, and all callers wishing to update the value\n\t * must wait for a synchronous IPI to complete (which is trivial\n\t * if the caller is on the CPU already).  This establishes the\n\t * necessary total order on variable updates.\n\t *\n\t * Note that because a guest time update may take place\n\t * anytime after the setting of the VCPU's request bit, the\n\t * correct TSC value must be set before the request.  However,\n\t * to ensure the update actually makes it to any guest which\n\t * starts running in hardware virtualization between the set\n\t * and the acquisition of the spinlock, we must also ping the\n\t * CPU after setting the request bit.\n\t *\n\t */\n\n\tsmp_call_function_single(cpu, tsc_khz_changed, freq, 1);\n\n\tmutex_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (vcpu->cpu != cpu)\n\t\t\t\tcontinue;\n\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\t\tif (vcpu->cpu != raw_smp_processor_id())\n\t\t\t\tsend_ipi = 1;\n\t\t}\n\t}\n\tmutex_unlock(&kvm_lock);\n\n\tif (freq->old < freq->new && send_ipi) {\n\t\t/*\n\t\t * We upscale the frequency.  Must make the guest\n\t\t * doesn't see old kvmclock values while running with\n\t\t * the new frequency, otherwise we risk the guest sees\n\t\t * time go backwards.\n\t\t *\n\t\t * In case we update the frequency for another cpu\n\t\t * (which might be in guest context) send an interrupt\n\t\t * to kick the cpu out of guest context.  Next time\n\t\t * guest context is entered kvmclock will be updated,\n\t\t * so the guest will not see stale values.\n\t\t */\n\t\tsmp_call_function_single(cpu, tsc_khz_changed, freq, 1);\n\t}\n}\n\nstatic int kvmclock_cpufreq_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t     void *data)\n{\n\tstruct cpufreq_freqs *freq = data;\n\tint cpu;\n\n\tif (val == CPUFREQ_PRECHANGE && freq->old > freq->new)\n\t\treturn 0;\n\tif (val == CPUFREQ_POSTCHANGE && freq->old < freq->new)\n\t\treturn 0;\n\n\tfor_each_cpu(cpu, freq->policy->cpus)\n\t\t__kvmclock_cpufreq_notifier(freq, cpu);\n\n\treturn 0;\n}\n\nstatic struct notifier_block kvmclock_cpufreq_notifier_block = {\n\t.notifier_call  = kvmclock_cpufreq_notifier\n};\n\nstatic int kvmclock_cpu_online(unsigned int cpu)\n{\n\ttsc_khz_changed(NULL);\n\treturn 0;\n}\n\nstatic void kvm_timer_init(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {\n\t\tmax_tsc_khz = tsc_khz;\n\n\t\tif (IS_ENABLED(CONFIG_CPU_FREQ)) {\n\t\t\tstruct cpufreq_policy *policy;\n\t\t\tint cpu;\n\n\t\t\tcpu = get_cpu();\n\t\t\tpolicy = cpufreq_cpu_get(cpu);\n\t\t\tif (policy) {\n\t\t\t\tif (policy->cpuinfo.max_freq)\n\t\t\t\t\tmax_tsc_khz = policy->cpuinfo.max_freq;\n\t\t\t\tcpufreq_cpu_put(policy);\n\t\t\t}\n\t\t\tput_cpu();\n\t\t}\n\t\tcpufreq_register_notifier(&kvmclock_cpufreq_notifier_block,\n\t\t\t\t\t  CPUFREQ_TRANSITION_NOTIFIER);\n\n\t\tcpuhp_setup_state(CPUHP_AP_X86_KVM_CLK_ONLINE, \"x86/kvm/clk:online\",\n\t\t\t\t  kvmclock_cpu_online, kvmclock_cpu_down_prep);\n\t}\n}\n\n#ifdef CONFIG_X86_64\nstatic void pvclock_gtod_update_fn(struct work_struct *work)\n{\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\n\tmutex_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\tatomic_set(&kvm_guest_has_master_clock, 0);\n\tmutex_unlock(&kvm_lock);\n}\n\nstatic DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);\n\n/*\n * Indirection to move queue_work() out of the tk_core.seq write held\n * region to prevent possible deadlocks against time accessors which\n * are invoked with work related locks held.\n */\nstatic void pvclock_irq_work_fn(struct irq_work *w)\n{\n\tqueue_work(system_long_wq, &pvclock_gtod_work);\n}\n\nstatic DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);\n\n/*\n * Notification about pvclock gtod data update.\n */\nstatic int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,\n\t\t\t       void *priv)\n{\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\tstruct timekeeper *tk = priv;\n\n\tupdate_pvclock_gtod(tk);\n\n\t/*\n\t * Disable master clock if host does not trust, or does not use,\n\t * TSC based clocksource. Delegate queue_work() to irq_work as\n\t * this is invoked with tk_core.seq write held.\n\t */\n\tif (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&\n\t    atomic_read(&kvm_guest_has_master_clock) != 0)\n\t\tirq_work_queue(&pvclock_irq_work);\n\treturn 0;\n}\n\nstatic struct notifier_block pvclock_gtod_notifier = {\n\t.notifier_call = pvclock_gtod_notify,\n};\n#endif\n\nint kvm_arch_init(void *opaque)\n{\n\tstruct kvm_x86_init_ops *ops = opaque;\n\tu64 host_pat;\n\tint r;\n\n\tif (kvm_x86_ops.hardware_enable) {\n\t\tpr_err(\"kvm: already loaded vendor module '%s'\\n\", kvm_x86_ops.name);\n\t\treturn -EEXIST;\n\t}\n\n\tif (!ops->cpu_has_kvm_support()) {\n\t\tpr_err_ratelimited(\"kvm: no hardware support for '%s'\\n\",\n\t\t\t\t   ops->runtime_ops->name);\n\t\treturn -EOPNOTSUPP;\n\t}\n\tif (ops->disabled_by_bios()) {\n\t\tpr_err_ratelimited(\"kvm: support for '%s' disabled by bios\\n\",\n\t\t\t\t   ops->runtime_ops->name);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/*\n\t * KVM explicitly assumes that the guest has an FPU and\n\t * FXSAVE/FXRSTOR. For example, the KVM_GET_FPU explicitly casts the\n\t * vCPU's FPU state as a fxregs_state struct.\n\t */\n\tif (!boot_cpu_has(X86_FEATURE_FPU) || !boot_cpu_has(X86_FEATURE_FXSR)) {\n\t\tprintk(KERN_ERR \"kvm: inadequate fpu\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && !boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {\n\t\tpr_err(\"RT requires X86_FEATURE_CONSTANT_TSC\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/*\n\t * KVM assumes that PAT entry '0' encodes WB memtype and simply zeroes\n\t * the PAT bits in SPTEs.  Bail if PAT[0] is programmed to something\n\t * other than WB.  Note, EPT doesn't utilize the PAT, but don't bother\n\t * with an exception.  PAT[0] is set to WB on RESET and also by the\n\t * kernel, i.e. failure indicates a kernel bug or broken firmware.\n\t */\n\tif (rdmsrl_safe(MSR_IA32_CR_PAT, &host_pat) ||\n\t    (host_pat & GENMASK(2, 0)) != 6) {\n\t\tpr_err(\"kvm: host PAT[0] is not WB\\n\");\n\t\treturn -EIO;\n\t}\n\n\tx86_emulator_cache = kvm_alloc_emulator_cache();\n\tif (!x86_emulator_cache) {\n\t\tpr_err(\"kvm: failed to allocate cache for x86 emulator\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tuser_return_msrs = alloc_percpu(struct kvm_user_return_msrs);\n\tif (!user_return_msrs) {\n\t\tprintk(KERN_ERR \"kvm: failed to allocate percpu kvm_user_return_msrs\\n\");\n\t\tr = -ENOMEM;\n\t\tgoto out_free_x86_emulator_cache;\n\t}\n\tkvm_nr_uret_msrs = 0;\n\n\tr = kvm_mmu_vendor_module_init();\n\tif (r)\n\t\tgoto out_free_percpu;\n\n\tkvm_timer_init();\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVE)) {\n\t\thost_xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);\n\t\tkvm_caps.supported_xcr0 = host_xcr0 & KVM_SUPPORTED_XCR0;\n\t}\n\n\tif (pi_inject_timer == -1)\n\t\tpi_inject_timer = housekeeping_enabled(HK_TYPE_TIMER);\n#ifdef CONFIG_X86_64\n\tpvclock_gtod_register_notifier(&pvclock_gtod_notifier);\n\n\tif (hypervisor_is_type(X86_HYPER_MS_HYPERV))\n\t\tset_hv_tscchange_cb(kvm_hyperv_tsc_notifier);\n#endif\n\n\treturn 0;\n\nout_free_percpu:\n\tfree_percpu(user_return_msrs);\nout_free_x86_emulator_cache:\n\tkmem_cache_destroy(x86_emulator_cache);\n\treturn r;\n}\n\nvoid kvm_arch_exit(void)\n{\n#ifdef CONFIG_X86_64\n\tif (hypervisor_is_type(X86_HYPER_MS_HYPERV))\n\t\tclear_hv_tscchange_cb();\n#endif\n\tkvm_lapic_exit();\n\n\tif (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {\n\t\tcpufreq_unregister_notifier(&kvmclock_cpufreq_notifier_block,\n\t\t\t\t\t    CPUFREQ_TRANSITION_NOTIFIER);\n\t\tcpuhp_remove_state_nocalls(CPUHP_AP_X86_KVM_CLK_ONLINE);\n\t}\n#ifdef CONFIG_X86_64\n\tpvclock_gtod_unregister_notifier(&pvclock_gtod_notifier);\n\tirq_work_sync(&pvclock_irq_work);\n\tcancel_work_sync(&pvclock_gtod_work);\n#endif\n\tkvm_x86_ops.hardware_enable = NULL;\n\tkvm_mmu_vendor_module_exit();\n\tfree_percpu(user_return_msrs);\n\tkmem_cache_destroy(x86_emulator_cache);\n#ifdef CONFIG_KVM_XEN\n\tstatic_key_deferred_flush(&kvm_xen_enabled);\n\tWARN_ON(static_branch_unlikely(&kvm_xen_enabled.key));\n#endif\n}\n\nstatic int __kvm_emulate_halt(struct kvm_vcpu *vcpu, int state, int reason)\n{\n\t/*\n\t * The vCPU has halted, e.g. executed HLT.  Update the run state if the\n\t * local APIC is in-kernel, the run loop will detect the non-runnable\n\t * state and halt the vCPU.  Exit to userspace if the local APIC is\n\t * managed by userspace, in which case userspace is responsible for\n\t * handling wake events.\n\t */\n\t++vcpu->stat.halt_exits;\n\tif (lapic_in_kernel(vcpu)) {\n\t\tvcpu->arch.mp_state = state;\n\t\treturn 1;\n\t} else {\n\t\tvcpu->run->exit_reason = reason;\n\t\treturn 0;\n\t}\n}\n\nint kvm_emulate_halt_noskip(struct kvm_vcpu *vcpu)\n{\n\treturn __kvm_emulate_halt(vcpu, KVM_MP_STATE_HALTED, KVM_EXIT_HLT);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_halt_noskip);\n\nint kvm_emulate_halt(struct kvm_vcpu *vcpu)\n{\n\tint ret = kvm_skip_emulated_instruction(vcpu);\n\t/*\n\t * TODO: we might be squashing a GUESTDBG_SINGLESTEP-triggered\n\t * KVM_EXIT_DEBUG here.\n\t */\n\treturn kvm_emulate_halt_noskip(vcpu) && ret;\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_halt);\n\nint kvm_emulate_ap_reset_hold(struct kvm_vcpu *vcpu)\n{\n\tint ret = kvm_skip_emulated_instruction(vcpu);\n\n\treturn __kvm_emulate_halt(vcpu, KVM_MP_STATE_AP_RESET_HOLD,\n\t\t\t\t\tKVM_EXIT_AP_RESET_HOLD) && ret;\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_ap_reset_hold);\n\n#ifdef CONFIG_X86_64\nstatic int kvm_pv_clock_pairing(struct kvm_vcpu *vcpu, gpa_t paddr,\n\t\t\t        unsigned long clock_type)\n{\n\tstruct kvm_clock_pairing clock_pairing;\n\tstruct timespec64 ts;\n\tu64 cycle;\n\tint ret;\n\n\tif (clock_type != KVM_CLOCK_PAIRING_WALLCLOCK)\n\t\treturn -KVM_EOPNOTSUPP;\n\n\t/*\n\t * When tsc is in permanent catchup mode guests won't be able to use\n\t * pvclock_read_retry loop to get consistent view of pvclock\n\t */\n\tif (vcpu->arch.tsc_always_catchup)\n\t\treturn -KVM_EOPNOTSUPP;\n\n\tif (!kvm_get_walltime_and_clockread(&ts, &cycle))\n\t\treturn -KVM_EOPNOTSUPP;\n\n\tclock_pairing.sec = ts.tv_sec;\n\tclock_pairing.nsec = ts.tv_nsec;\n\tclock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);\n\tclock_pairing.flags = 0;\n\tmemset(&clock_pairing.pad, 0, sizeof(clock_pairing.pad));\n\n\tret = 0;\n\tif (kvm_write_guest(vcpu->kvm, paddr, &clock_pairing,\n\t\t\t    sizeof(struct kvm_clock_pairing)))\n\t\tret = -KVM_EFAULT;\n\n\treturn ret;\n}\n#endif\n\n/*\n * kvm_pv_kick_cpu_op:  Kick a vcpu.\n *\n * @apicid - apicid of vcpu to be kicked.\n */\nstatic void kvm_pv_kick_cpu_op(struct kvm *kvm, int apicid)\n{\n\t/*\n\t * All other fields are unused for APIC_DM_REMRD, but may be consumed by\n\t * common code, e.g. for tracing. Defer initialization to the compiler.\n\t */\n\tstruct kvm_lapic_irq lapic_irq = {\n\t\t.delivery_mode = APIC_DM_REMRD,\n\t\t.dest_mode = APIC_DEST_PHYSICAL,\n\t\t.shorthand = APIC_DEST_NOSHORT,\n\t\t.dest_id = apicid,\n\t};\n\n\tkvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);\n}\n\nbool kvm_apicv_activated(struct kvm *kvm)\n{\n\treturn (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);\n}\nEXPORT_SYMBOL_GPL(kvm_apicv_activated);\n\nbool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu)\n{\n\tulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);\n\tulong vcpu_reasons = static_call(kvm_x86_vcpu_get_apicv_inhibit_reasons)(vcpu);\n\n\treturn (vm_reasons | vcpu_reasons) == 0;\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_apicv_activated);\n\nstatic void set_or_clear_apicv_inhibit(unsigned long *inhibits,\n\t\t\t\t       enum kvm_apicv_inhibit reason, bool set)\n{\n\tif (set)\n\t\t__set_bit(reason, inhibits);\n\telse\n\t\t__clear_bit(reason, inhibits);\n\n\ttrace_kvm_apicv_inhibit_changed(reason, set, *inhibits);\n}\n\nstatic void kvm_apicv_init(struct kvm *kvm)\n{\n\tunsigned long *inhibits = &kvm->arch.apicv_inhibit_reasons;\n\n\tinit_rwsem(&kvm->arch.apicv_update_lock);\n\n\tset_or_clear_apicv_inhibit(inhibits, APICV_INHIBIT_REASON_ABSENT, true);\n\n\tif (!enable_apicv)\n\t\tset_or_clear_apicv_inhibit(inhibits,\n\t\t\t\t\t   APICV_INHIBIT_REASON_DISABLE, true);\n}\n\nstatic void kvm_sched_yield(struct kvm_vcpu *vcpu, unsigned long dest_id)\n{\n\tstruct kvm_vcpu *target = NULL;\n\tstruct kvm_apic_map *map;\n\n\tvcpu->stat.directed_yield_attempted++;\n\n\tif (single_task_running())\n\t\tgoto no_yield;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(vcpu->kvm->arch.apic_map);\n\n\tif (likely(map) && dest_id <= map->max_apic_id && map->phys_map[dest_id])\n\t\ttarget = map->phys_map[dest_id]->vcpu;\n\n\trcu_read_unlock();\n\n\tif (!target || !READ_ONCE(target->ready))\n\t\tgoto no_yield;\n\n\t/* Ignore requests to yield to self */\n\tif (vcpu == target)\n\t\tgoto no_yield;\n\n\tif (kvm_vcpu_yield_to(target) <= 0)\n\t\tgoto no_yield;\n\n\tvcpu->stat.directed_yield_successful++;\n\nno_yield:\n\treturn;\n}\n\nstatic int complete_hypercall_exit(struct kvm_vcpu *vcpu)\n{\n\tu64 ret = vcpu->run->hypercall.ret;\n\n\tif (!is_64_bit_mode(vcpu))\n\t\tret = (u32)ret;\n\tkvm_rax_write(vcpu, ret);\n\t++vcpu->stat.hypercalls;\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nint kvm_emulate_hypercall(struct kvm_vcpu *vcpu)\n{\n\tunsigned long nr, a0, a1, a2, a3, ret;\n\tint op_64_bit;\n\n\tif (kvm_xen_hypercall_enabled(vcpu->kvm))\n\t\treturn kvm_xen_hypercall(vcpu);\n\n\tif (kvm_hv_hypercall_enabled(vcpu))\n\t\treturn kvm_hv_hypercall(vcpu);\n\n\tnr = kvm_rax_read(vcpu);\n\ta0 = kvm_rbx_read(vcpu);\n\ta1 = kvm_rcx_read(vcpu);\n\ta2 = kvm_rdx_read(vcpu);\n\ta3 = kvm_rsi_read(vcpu);\n\n\ttrace_kvm_hypercall(nr, a0, a1, a2, a3);\n\n\top_64_bit = is_64_bit_hypercall(vcpu);\n\tif (!op_64_bit) {\n\t\tnr &= 0xFFFFFFFF;\n\t\ta0 &= 0xFFFFFFFF;\n\t\ta1 &= 0xFFFFFFFF;\n\t\ta2 &= 0xFFFFFFFF;\n\t\ta3 &= 0xFFFFFFFF;\n\t}\n\n\tif (static_call(kvm_x86_get_cpl)(vcpu) != 0) {\n\t\tret = -KVM_EPERM;\n\t\tgoto out;\n\t}\n\n\tret = -KVM_ENOSYS;\n\n\tswitch (nr) {\n\tcase KVM_HC_VAPIC_POLL_IRQ:\n\t\tret = 0;\n\t\tbreak;\n\tcase KVM_HC_KICK_CPU:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_UNHALT))\n\t\t\tbreak;\n\n\t\tkvm_pv_kick_cpu_op(vcpu->kvm, a1);\n\t\tkvm_sched_yield(vcpu, a1);\n\t\tret = 0;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase KVM_HC_CLOCK_PAIRING:\n\t\tret = kvm_pv_clock_pairing(vcpu, a0, a1);\n\t\tbreak;\n#endif\n\tcase KVM_HC_SEND_IPI:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_SEND_IPI))\n\t\t\tbreak;\n\n\t\tret = kvm_pv_send_ipi(vcpu->kvm, a0, a1, a2, a3, op_64_bit);\n\t\tbreak;\n\tcase KVM_HC_SCHED_YIELD:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_SCHED_YIELD))\n\t\t\tbreak;\n\n\t\tkvm_sched_yield(vcpu, a0);\n\t\tret = 0;\n\t\tbreak;\n\tcase KVM_HC_MAP_GPA_RANGE: {\n\t\tu64 gpa = a0, npages = a1, attrs = a2;\n\n\t\tret = -KVM_ENOSYS;\n\t\tif (!(vcpu->kvm->arch.hypercall_exit_enabled & (1 << KVM_HC_MAP_GPA_RANGE)))\n\t\t\tbreak;\n\n\t\tif (!PAGE_ALIGNED(gpa) || !npages ||\n\t\t    gpa_to_gfn(gpa) + npages <= gpa_to_gfn(gpa)) {\n\t\t\tret = -KVM_EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tvcpu->run->exit_reason        = KVM_EXIT_HYPERCALL;\n\t\tvcpu->run->hypercall.nr       = KVM_HC_MAP_GPA_RANGE;\n\t\tvcpu->run->hypercall.args[0]  = gpa;\n\t\tvcpu->run->hypercall.args[1]  = npages;\n\t\tvcpu->run->hypercall.args[2]  = attrs;\n\t\tvcpu->run->hypercall.longmode = op_64_bit;\n\t\tvcpu->arch.complete_userspace_io = complete_hypercall_exit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\tret = -KVM_ENOSYS;\n\t\tbreak;\n\t}\nout:\n\tif (!op_64_bit)\n\t\tret = (u32)ret;\n\tkvm_rax_write(vcpu, ret);\n\n\t++vcpu->stat.hypercalls;\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_hypercall);\n\nstatic int emulator_fix_hypercall(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tchar instruction[3];\n\tunsigned long rip = kvm_rip_read(vcpu);\n\n\t/*\n\t * If the quirk is disabled, synthesize a #UD and let the guest pick up\n\t * the pieces.\n\t */\n\tif (!kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_FIX_HYPERCALL_INSN)) {\n\t\tctxt->exception.error_code_valid = false;\n\t\tctxt->exception.vector = UD_VECTOR;\n\t\tctxt->have_exception = true;\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\tstatic_call(kvm_x86_patch_hypercall)(vcpu, instruction);\n\n\treturn emulator_write_emulated(ctxt, rip, instruction, 3,\n\t\t&ctxt->exception);\n}\n\nstatic int dm_request_for_irq_injection(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->run->request_interrupt_window &&\n\t\tlikely(!pic_in_kernel(vcpu->kvm));\n}\n\n/* Called within kvm->srcu read side.  */\nstatic void post_kvm_run_save(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *kvm_run = vcpu->run;\n\n\tkvm_run->if_flag = static_call(kvm_x86_get_if_flag)(vcpu);\n\tkvm_run->cr8 = kvm_get_cr8(vcpu);\n\tkvm_run->apic_base = kvm_get_apic_base(vcpu);\n\n\tkvm_run->ready_for_interrupt_injection =\n\t\tpic_in_kernel(vcpu->kvm) ||\n\t\tkvm_vcpu_ready_for_interrupt_injection(vcpu);\n\n\tif (is_smm(vcpu))\n\t\tkvm_run->flags |= KVM_RUN_X86_SMM;\n}\n\nstatic void update_cr8_intercept(struct kvm_vcpu *vcpu)\n{\n\tint max_irr, tpr;\n\n\tif (!kvm_x86_ops.update_cr8_intercept)\n\t\treturn;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tif (vcpu->arch.apic->apicv_active)\n\t\treturn;\n\n\tif (!vcpu->arch.apic->vapic_addr)\n\t\tmax_irr = kvm_lapic_find_highest_irr(vcpu);\n\telse\n\t\tmax_irr = -1;\n\n\tif (max_irr != -1)\n\t\tmax_irr >>= 4;\n\n\ttpr = kvm_lapic_get_cr8(vcpu);\n\n\tstatic_call(kvm_x86_update_cr8_intercept)(vcpu, tpr, max_irr);\n}\n\n\nint kvm_check_nested_events(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_test_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {\n\t\tkvm_x86_ops.nested_ops->triple_fault(vcpu);\n\t\treturn 1;\n\t}\n\n\treturn kvm_x86_ops.nested_ops->check_events(vcpu);\n}\n\nstatic void kvm_inject_exception(struct kvm_vcpu *vcpu)\n{\n\ttrace_kvm_inj_exception(vcpu->arch.exception.vector,\n\t\t\t\tvcpu->arch.exception.has_error_code,\n\t\t\t\tvcpu->arch.exception.error_code,\n\t\t\t\tvcpu->arch.exception.injected);\n\n\tif (vcpu->arch.exception.error_code && !is_protmode(vcpu))\n\t\tvcpu->arch.exception.error_code = false;\n\tstatic_call(kvm_x86_inject_exception)(vcpu);\n}\n\n/*\n * Check for any event (interrupt or exception) that is ready to be injected,\n * and if there is at least one event, inject the event with the highest\n * priority.  This handles both \"pending\" events, i.e. events that have never\n * been injected into the guest, and \"injected\" events, i.e. events that were\n * injected as part of a previous VM-Enter, but weren't successfully delivered\n * and need to be re-injected.\n *\n * Note, this is not guaranteed to be invoked on a guest instruction boundary,\n * i.e. doesn't guarantee that there's an event window in the guest.  KVM must\n * be able to inject exceptions in the \"middle\" of an instruction, and so must\n * also be able to re-inject NMIs and IRQs in the middle of an instruction.\n * I.e. for exceptions and re-injected events, NOT invoking this on instruction\n * boundaries is necessary and correct.\n *\n * For simplicity, KVM uses a single path to inject all events (except events\n * that are injected directly from L1 to L2) and doesn't explicitly track\n * instruction boundaries for asynchronous events.  However, because VM-Exits\n * that can occur during instruction execution typically result in KVM skipping\n * the instruction or injecting an exception, e.g. instruction and exception\n * intercepts, and because pending exceptions have higher priority than pending\n * interrupts, KVM still honors instruction boundaries in most scenarios.\n *\n * But, if a VM-Exit occurs during instruction execution, and KVM does NOT skip\n * the instruction or inject an exception, then KVM can incorrecty inject a new\n * asynchrounous event if the event became pending after the CPU fetched the\n * instruction (in the guest).  E.g. if a page fault (#PF, #NPF, EPT violation)\n * occurs and is resolved by KVM, a coincident NMI, SMI, IRQ, etc... can be\n * injected on the restarted instruction instead of being deferred until the\n * instruction completes.\n *\n * In practice, this virtualization hole is unlikely to be observed by the\n * guest, and even less likely to cause functional problems.  To detect the\n * hole, the guest would have to trigger an event on a side effect of an early\n * phase of instruction execution, e.g. on the instruction fetch from memory.\n * And for it to be a functional problem, the guest would need to depend on the\n * ordering between that side effect, the instruction completing, _and_ the\n * delivery of the asynchronous event.\n */\nstatic int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,\n\t\t\t\t       bool *req_immediate_exit)\n{\n\tbool can_inject;\n\tint r;\n\n\t/*\n\t * Process nested events first, as nested VM-Exit supercedes event\n\t * re-injection.  If there's an event queued for re-injection, it will\n\t * be saved into the appropriate vmc{b,s}12 fields on nested VM-Exit.\n\t */\n\tif (is_guest_mode(vcpu))\n\t\tr = kvm_check_nested_events(vcpu);\n\telse\n\t\tr = 0;\n\n\t/*\n\t * Re-inject exceptions and events *especially* if immediate entry+exit\n\t * to/from L2 is needed, as any event that has already been injected\n\t * into L2 needs to complete its lifecycle before injecting a new event.\n\t *\n\t * Don't re-inject an NMI or interrupt if there is a pending exception.\n\t * This collision arises if an exception occurred while vectoring the\n\t * injected event, KVM intercepted said exception, and KVM ultimately\n\t * determined the fault belongs to the guest and queues the exception\n\t * for injection back into the guest.\n\t *\n\t * \"Injected\" interrupts can also collide with pending exceptions if\n\t * userspace ignores the \"ready for injection\" flag and blindly queues\n\t * an interrupt.  In that case, prioritizing the exception is correct,\n\t * as the exception \"occurred\" before the exit to userspace.  Trap-like\n\t * exceptions, e.g. most #DBs, have higher priority than interrupts.\n\t * And while fault-like exceptions, e.g. #GP and #PF, are the lowest\n\t * priority, they're only generated (pended) during instruction\n\t * execution, and interrupts are recognized at instruction boundaries.\n\t * Thus a pending fault-like exception means the fault occurred on the\n\t * *previous* instruction and must be serviced prior to recognizing any\n\t * new events in order to fully complete the previous instruction.\n\t */\n\tif (vcpu->arch.exception.injected)\n\t\tkvm_inject_exception(vcpu);\n\telse if (kvm_is_exception_pending(vcpu))\n\t\t; /* see above */\n\telse if (vcpu->arch.nmi_injected)\n\t\tstatic_call(kvm_x86_inject_nmi)(vcpu);\n\telse if (vcpu->arch.interrupt.injected)\n\t\tstatic_call(kvm_x86_inject_irq)(vcpu, true);\n\n\t/*\n\t * Exceptions that morph to VM-Exits are handled above, and pending\n\t * exceptions on top of injected exceptions that do not VM-Exit should\n\t * either morph to #DF or, sadly, override the injected exception.\n\t */\n\tWARN_ON_ONCE(vcpu->arch.exception.injected &&\n\t\t     vcpu->arch.exception.pending);\n\n\t/*\n\t * Bail if immediate entry+exit to/from the guest is needed to complete\n\t * nested VM-Enter or event re-injection so that a different pending\n\t * event can be serviced (or if KVM needs to exit to userspace).\n\t *\n\t * Otherwise, continue processing events even if VM-Exit occurred.  The\n\t * VM-Exit will have cleared exceptions that were meant for L2, but\n\t * there may now be events that can be injected into L1.\n\t */\n\tif (r < 0)\n\t\tgoto out;\n\n\t/*\n\t * A pending exception VM-Exit should either result in nested VM-Exit\n\t * or force an immediate re-entry and exit to/from L2, and exception\n\t * VM-Exits cannot be injected (flag should _never_ be set).\n\t */\n\tWARN_ON_ONCE(vcpu->arch.exception_vmexit.injected ||\n\t\t     vcpu->arch.exception_vmexit.pending);\n\n\t/*\n\t * New events, other than exceptions, cannot be injected if KVM needs\n\t * to re-inject a previous event.  See above comments on re-injecting\n\t * for why pending exceptions get priority.\n\t */\n\tcan_inject = !kvm_event_needs_reinjection(vcpu);\n\n\tif (vcpu->arch.exception.pending) {\n\t\t/*\n\t\t * Fault-class exceptions, except #DBs, set RF=1 in the RFLAGS\n\t\t * value pushed on the stack.  Trap-like exception and all #DBs\n\t\t * leave RF as-is (KVM follows Intel's behavior in this regard;\n\t\t * AMD states that code breakpoint #DBs excplitly clear RF=0).\n\t\t *\n\t\t * Note, most versions of Intel's SDM and AMD's APM incorrectly\n\t\t * describe the behavior of General Detect #DBs, which are\n\t\t * fault-like.  They do _not_ set RF, a la code breakpoints.\n\t\t */\n\t\tif (exception_type(vcpu->arch.exception.vector) == EXCPT_FAULT)\n\t\t\t__kvm_set_rflags(vcpu, kvm_get_rflags(vcpu) |\n\t\t\t\t\t     X86_EFLAGS_RF);\n\n\t\tif (vcpu->arch.exception.vector == DB_VECTOR) {\n\t\t\tkvm_deliver_exception_payload(vcpu, &vcpu->arch.exception);\n\t\t\tif (vcpu->arch.dr7 & DR7_GD) {\n\t\t\t\tvcpu->arch.dr7 &= ~DR7_GD;\n\t\t\t\tkvm_update_dr7(vcpu);\n\t\t\t}\n\t\t}\n\n\t\tkvm_inject_exception(vcpu);\n\n\t\tvcpu->arch.exception.pending = false;\n\t\tvcpu->arch.exception.injected = true;\n\n\t\tcan_inject = false;\n\t}\n\n\t/* Don't inject interrupts if the user asked to avoid doing so */\n\tif (vcpu->guest_debug & KVM_GUESTDBG_BLOCKIRQ)\n\t\treturn 0;\n\n\t/*\n\t * Finally, inject interrupt events.  If an event cannot be injected\n\t * due to architectural conditions (e.g. IF=0) a window-open exit\n\t * will re-request KVM_REQ_EVENT.  Sometimes however an event is pending\n\t * and can architecturally be injected, but we cannot do it right now:\n\t * an interrupt could have arrived just now and we have to inject it\n\t * as a vmexit, or there could already an event in the queue, which is\n\t * indicated by can_inject.  In that case we request an immediate exit\n\t * in order to make progress and get back here for another iteration.\n\t * The kvm_x86_ops hooks communicate this by returning -EBUSY.\n\t */\n#ifdef CONFIG_KVM_SMM\n\tif (vcpu->arch.smi_pending) {\n\t\tr = can_inject ? static_call(kvm_x86_smi_allowed)(vcpu, true) : -EBUSY;\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t\tif (r) {\n\t\t\tvcpu->arch.smi_pending = false;\n\t\t\t++vcpu->arch.smi_count;\n\t\t\tenter_smm(vcpu);\n\t\t\tcan_inject = false;\n\t\t} else\n\t\t\tstatic_call(kvm_x86_enable_smi_window)(vcpu);\n\t}\n#endif\n\n\tif (vcpu->arch.nmi_pending) {\n\t\tr = can_inject ? static_call(kvm_x86_nmi_allowed)(vcpu, true) : -EBUSY;\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t\tif (r) {\n\t\t\t--vcpu->arch.nmi_pending;\n\t\t\tvcpu->arch.nmi_injected = true;\n\t\t\tstatic_call(kvm_x86_inject_nmi)(vcpu);\n\t\t\tcan_inject = false;\n\t\t\tWARN_ON(static_call(kvm_x86_nmi_allowed)(vcpu, true) < 0);\n\t\t}\n\t\tif (vcpu->arch.nmi_pending)\n\t\t\tstatic_call(kvm_x86_enable_nmi_window)(vcpu);\n\t}\n\n\tif (kvm_cpu_has_injectable_intr(vcpu)) {\n\t\tr = can_inject ? static_call(kvm_x86_interrupt_allowed)(vcpu, true) : -EBUSY;\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t\tif (r) {\n\t\t\tkvm_queue_interrupt(vcpu, kvm_cpu_get_interrupt(vcpu), false);\n\t\t\tstatic_call(kvm_x86_inject_irq)(vcpu, false);\n\t\t\tWARN_ON(static_call(kvm_x86_interrupt_allowed)(vcpu, true) < 0);\n\t\t}\n\t\tif (kvm_cpu_has_injectable_intr(vcpu))\n\t\t\tstatic_call(kvm_x86_enable_irq_window)(vcpu);\n\t}\n\n\tif (is_guest_mode(vcpu) &&\n\t    kvm_x86_ops.nested_ops->has_events &&\n\t    kvm_x86_ops.nested_ops->has_events(vcpu))\n\t\t*req_immediate_exit = true;\n\n\t/*\n\t * KVM must never queue a new exception while injecting an event; KVM\n\t * is done emulating and should only propagate the to-be-injected event\n\t * to the VMCS/VMCB.  Queueing a new exception can put the vCPU into an\n\t * infinite loop as KVM will bail from VM-Enter to inject the pending\n\t * exception and start the cycle all over.\n\t *\n\t * Exempt triple faults as they have special handling and won't put the\n\t * vCPU into an infinite loop.  Triple fault can be queued when running\n\t * VMX without unrestricted guest, as that requires KVM to emulate Real\n\t * Mode events (see kvm_inject_realmode_interrupt()).\n\t */\n\tWARN_ON_ONCE(vcpu->arch.exception.pending ||\n\t\t     vcpu->arch.exception_vmexit.pending);\n\treturn 0;\n\nout:\n\tif (r == -EBUSY) {\n\t\t*req_immediate_exit = true;\n\t\tr = 0;\n\t}\n\treturn r;\n}\n\nstatic void process_nmi(struct kvm_vcpu *vcpu)\n{\n\tunsigned limit = 2;\n\n\t/*\n\t * x86 is limited to one NMI running, and one NMI pending after it.\n\t * If an NMI is already in progress, limit further NMIs to just one.\n\t * Otherwise, allow two (and we'll inject the first one immediately).\n\t */\n\tif (static_call(kvm_x86_get_nmi_mask)(vcpu) || vcpu->arch.nmi_injected)\n\t\tlimit = 1;\n\n\tvcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);\n\tvcpu->arch.nmi_pending = min(vcpu->arch.nmi_pending, limit);\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\n\nvoid kvm_make_scan_ioapic_request_mask(struct kvm *kvm,\n\t\t\t\t       unsigned long *vcpu_bitmap)\n{\n\tkvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);\n}\n\nvoid kvm_make_scan_ioapic_request(struct kvm *kvm)\n{\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);\n}\n\nvoid kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tbool activate;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tdown_read(&vcpu->kvm->arch.apicv_update_lock);\n\tpreempt_disable();\n\n\t/* Do not activate APICV when APIC is disabled */\n\tactivate = kvm_vcpu_apicv_activated(vcpu) &&\n\t\t   (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED);\n\n\tif (apic->apicv_active == activate)\n\t\tgoto out;\n\n\tapic->apicv_active = activate;\n\tkvm_apic_update_apicv(vcpu);\n\tstatic_call(kvm_x86_refresh_apicv_exec_ctrl)(vcpu);\n\n\t/*\n\t * When APICv gets disabled, we may still have injected interrupts\n\t * pending. At the same time, KVM_REQ_EVENT may not be set as APICv was\n\t * still active when the interrupt got accepted. Make sure\n\t * kvm_check_and_inject_events() is called to check for that.\n\t */\n\tif (!apic->apicv_active)\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\nout:\n\tpreempt_enable();\n\tup_read(&vcpu->kvm->arch.apicv_update_lock);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_update_apicv);\n\nvoid __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,\n\t\t\t\t      enum kvm_apicv_inhibit reason, bool set)\n{\n\tunsigned long old, new;\n\n\tlockdep_assert_held_write(&kvm->arch.apicv_update_lock);\n\n\tif (!static_call(kvm_x86_check_apicv_inhibit_reasons)(reason))\n\t\treturn;\n\n\told = new = kvm->arch.apicv_inhibit_reasons;\n\n\tset_or_clear_apicv_inhibit(&new, reason, set);\n\n\tif (!!old != !!new) {\n\t\t/*\n\t\t * Kick all vCPUs before setting apicv_inhibit_reasons to avoid\n\t\t * false positives in the sanity check WARN in svm_vcpu_run().\n\t\t * This task will wait for all vCPUs to ack the kick IRQ before\n\t\t * updating apicv_inhibit_reasons, and all other vCPUs will\n\t\t * block on acquiring apicv_update_lock so that vCPUs can't\n\t\t * redo svm_vcpu_run() without seeing the new inhibit state.\n\t\t *\n\t\t * Note, holding apicv_update_lock and taking it in the read\n\t\t * side (handling the request) also prevents other vCPUs from\n\t\t * servicing the request with a stale apicv_inhibit_reasons.\n\t\t */\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);\n\t\tkvm->arch.apicv_inhibit_reasons = new;\n\t\tif (new) {\n\t\t\tunsigned long gfn = gpa_to_gfn(APIC_DEFAULT_PHYS_BASE);\n\t\t\tint idx = srcu_read_lock(&kvm->srcu);\n\n\t\t\tkvm_zap_gfn_range(kvm, gfn, gfn+1);\n\t\t\tsrcu_read_unlock(&kvm->srcu, idx);\n\t\t}\n\t} else {\n\t\tkvm->arch.apicv_inhibit_reasons = new;\n\t}\n}\n\nvoid kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,\n\t\t\t\t    enum kvm_apicv_inhibit reason, bool set)\n{\n\tif (!enable_apicv)\n\t\treturn;\n\n\tdown_write(&kvm->arch.apicv_update_lock);\n\t__kvm_set_or_clear_apicv_inhibit(kvm, reason, set);\n\tup_write(&kvm->arch.apicv_update_lock);\n}\nEXPORT_SYMBOL_GPL(kvm_set_or_clear_apicv_inhibit);\n\nstatic void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_apic_present(vcpu))\n\t\treturn;\n\n\tbitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);\n\n\tif (irqchip_split(vcpu->kvm))\n\t\tkvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);\n\telse {\n\t\tstatic_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);\n\t\tif (ioapic_in_kernel(vcpu->kvm))\n\t\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);\n\t}\n\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.load_eoi_exitmap_pending = true;\n\telse\n\t\tkvm_make_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu);\n}\n\nstatic void vcpu_load_eoi_exitmap(struct kvm_vcpu *vcpu)\n{\n\tu64 eoi_exit_bitmap[4];\n\n\tif (!kvm_apic_hw_enabled(vcpu->arch.apic))\n\t\treturn;\n\n\tif (to_hv_vcpu(vcpu)) {\n\t\tbitmap_or((ulong *)eoi_exit_bitmap,\n\t\t\t  vcpu->arch.ioapic_handled_vectors,\n\t\t\t  to_hv_synic(vcpu)->vec_bitmap, 256);\n\t\tstatic_call_cond(kvm_x86_load_eoi_exitmap)(vcpu, eoi_exit_bitmap);\n\t\treturn;\n\t}\n\n\tstatic_call_cond(kvm_x86_load_eoi_exitmap)(\n\t\tvcpu, (u64 *)vcpu->arch.ioapic_handled_vectors);\n}\n\nvoid kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,\n\t\t\t\t\t    unsigned long start, unsigned long end)\n{\n\tunsigned long apic_address;\n\n\t/*\n\t * The physical address of apic access page is stored in the VMCS.\n\t * Update it when it becomes invalid.\n\t */\n\tapic_address = gfn_to_hva(kvm, APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT);\n\tif (start <= apic_address && apic_address < end)\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);\n}\n\nvoid kvm_arch_guest_memory_reclaimed(struct kvm *kvm)\n{\n\tstatic_call_cond(kvm_x86_guest_memory_reclaimed)(kvm);\n}\n\nstatic void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tstatic_call_cond(kvm_x86_set_apic_access_page_addr)(vcpu);\n}\n\nvoid __kvm_request_immediate_exit(struct kvm_vcpu *vcpu)\n{\n\tsmp_send_reschedule(vcpu->cpu);\n}\nEXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);\n\n/*\n * Called within kvm->srcu read side.\n * Returns 1 to let vcpu_run() continue the guest execution loop without\n * exiting to the userspace.  Otherwise, the value will be returned to the\n * userspace.\n */\nstatic int vcpu_enter_guest(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\tbool req_int_win =\n\t\tdm_request_for_irq_injection(vcpu) &&\n\t\tkvm_cpu_accept_dm_intr(vcpu);\n\tfastpath_t exit_fastpath;\n\n\tbool req_immediate_exit = false;\n\n\tif (kvm_request_pending(vcpu)) {\n\t\tif (kvm_check_request(KVM_REQ_VM_DEAD, vcpu)) {\n\t\t\tr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (kvm_dirty_ring_check_request(vcpu)) {\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (kvm_check_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu)) {\n\t\t\tif (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {\n\t\t\t\tr = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_MMU_FREE_OBSOLETE_ROOTS, vcpu))\n\t\t\tkvm_mmu_free_obsolete_roots(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))\n\t\t\t__kvm_migrate_timers(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))\n\t\t\tkvm_update_masterclock(vcpu->kvm);\n\t\tif (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))\n\t\t\tkvm_gen_kvmclock_update(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {\n\t\t\tr = kvm_guest_time_update(vcpu);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_MMU_SYNC, vcpu))\n\t\t\tkvm_mmu_sync_roots(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_LOAD_MMU_PGD, vcpu))\n\t\t\tkvm_mmu_load_pgd(vcpu);\n\n\t\t/*\n\t\t * Note, the order matters here, as flushing \"all\" TLB entries\n\t\t * also flushes the \"current\" TLB entries, i.e. servicing the\n\t\t * flush \"all\" will clear any request to flush \"current\".\n\t\t */\n\t\tif (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu))\n\t\t\tkvm_vcpu_flush_tlb_all(vcpu);\n\n\t\tkvm_service_local_tlb_flush_requests(vcpu);\n\n\t\t/*\n\t\t * Fall back to a \"full\" guest flush if Hyper-V's precise\n\t\t * flushing fails.  Note, Hyper-V's flushing is per-vCPU, but\n\t\t * the flushes are considered \"remote\" and not \"local\" because\n\t\t * the requests can be initiated from other vCPUs.\n\t\t */\n\t\tif (kvm_check_request(KVM_REQ_HV_TLB_FLUSH, vcpu) &&\n\t\t    kvm_hv_vcpu_flush_tlb(vcpu))\n\t\t\tkvm_vcpu_flush_tlb_guest(vcpu);\n\n\t\tif (kvm_check_request(KVM_REQ_REPORT_TPR_ACCESS, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_TPR_ACCESS;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_test_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {\n\t\t\tif (is_guest_mode(vcpu))\n\t\t\t\tkvm_x86_ops.nested_ops->triple_fault(vcpu);\n\n\t\t\tif (kvm_check_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {\n\t\t\t\tvcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;\n\t\t\t\tvcpu->mmio_needed = 0;\n\t\t\t\tr = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {\n\t\t\t/* Page is swapped out. Do synthetic halt */\n\t\t\tvcpu->arch.apf.halted = true;\n\t\t\tr = 1;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_STEAL_UPDATE, vcpu))\n\t\t\trecord_steal_time(vcpu);\n#ifdef CONFIG_KVM_SMM\n\t\tif (kvm_check_request(KVM_REQ_SMI, vcpu))\n\t\t\tprocess_smi(vcpu);\n#endif\n\t\tif (kvm_check_request(KVM_REQ_NMI, vcpu))\n\t\t\tprocess_nmi(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_PMU, vcpu))\n\t\t\tkvm_pmu_handle_event(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_PMI, vcpu))\n\t\t\tkvm_pmu_deliver_pmi(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {\n\t\t\tBUG_ON(vcpu->arch.pending_ioapic_eoi > 255);\n\t\t\tif (test_bit(vcpu->arch.pending_ioapic_eoi,\n\t\t\t\t     vcpu->arch.ioapic_handled_vectors)) {\n\t\t\t\tvcpu->run->exit_reason = KVM_EXIT_IOAPIC_EOI;\n\t\t\t\tvcpu->run->eoi.vector =\n\t\t\t\t\t\tvcpu->arch.pending_ioapic_eoi;\n\t\t\t\tr = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_SCAN_IOAPIC, vcpu))\n\t\t\tvcpu_scan_ioapic(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu))\n\t\t\tvcpu_load_eoi_exitmap(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))\n\t\t\tkvm_vcpu_reload_apic_access_page(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_HV_CRASH, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_SYSTEM_EVENT;\n\t\t\tvcpu->run->system_event.type = KVM_SYSTEM_EVENT_CRASH;\n\t\t\tvcpu->run->system_event.ndata = 0;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_HV_RESET, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_SYSTEM_EVENT;\n\t\t\tvcpu->run->system_event.type = KVM_SYSTEM_EVENT_RESET;\n\t\t\tvcpu->run->system_event.ndata = 0;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_HV_EXIT, vcpu)) {\n\t\t\tstruct kvm_vcpu_hv *hv_vcpu = to_hv_vcpu(vcpu);\n\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_HYPERV;\n\t\t\tvcpu->run->hyperv = hv_vcpu->exit;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * KVM_REQ_HV_STIMER has to be processed after\n\t\t * KVM_REQ_CLOCK_UPDATE, because Hyper-V SynIC timers\n\t\t * depend on the guest clock being up-to-date\n\t\t */\n\t\tif (kvm_check_request(KVM_REQ_HV_STIMER, vcpu))\n\t\t\tkvm_hv_process_stimers(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))\n\t\t\tkvm_vcpu_update_apicv(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_APF_READY, vcpu))\n\t\t\tkvm_check_async_pf_completion(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MSR_FILTER_CHANGED, vcpu))\n\t\t\tstatic_call(kvm_x86_msr_filter_changed)(vcpu);\n\n\t\tif (kvm_check_request(KVM_REQ_UPDATE_CPU_DIRTY_LOGGING, vcpu))\n\t\t\tstatic_call(kvm_x86_update_cpu_dirty_logging)(vcpu);\n\t}\n\n\tif (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win ||\n\t    kvm_xen_has_interrupt(vcpu)) {\n\t\t++vcpu->stat.req_event;\n\t\tr = kvm_apic_accept_events(vcpu);\n\t\tif (r < 0) {\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {\n\t\t\tr = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tr = kvm_check_and_inject_events(vcpu, &req_immediate_exit);\n\t\tif (r < 0) {\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (req_int_win)\n\t\t\tstatic_call(kvm_x86_enable_irq_window)(vcpu);\n\n\t\tif (kvm_lapic_enabled(vcpu)) {\n\t\t\tupdate_cr8_intercept(vcpu);\n\t\t\tkvm_lapic_sync_to_vapic(vcpu);\n\t\t}\n\t}\n\n\tr = kvm_mmu_reload(vcpu);\n\tif (unlikely(r)) {\n\t\tgoto cancel_injection;\n\t}\n\n\tpreempt_disable();\n\n\tstatic_call(kvm_x86_prepare_switch_to_guest)(vcpu);\n\n\t/*\n\t * Disable IRQs before setting IN_GUEST_MODE.  Posted interrupt\n\t * IPI are then delayed after guest entry, which ensures that they\n\t * result in virtual interrupt delivery.\n\t */\n\tlocal_irq_disable();\n\n\t/* Store vcpu->apicv_active before vcpu->mode.  */\n\tsmp_store_release(&vcpu->mode, IN_GUEST_MODE);\n\n\tkvm_vcpu_srcu_read_unlock(vcpu);\n\n\t/*\n\t * 1) We should set ->mode before checking ->requests.  Please see\n\t * the comment in kvm_vcpu_exiting_guest_mode().\n\t *\n\t * 2) For APICv, we should set ->mode before checking PID.ON. This\n\t * pairs with the memory barrier implicit in pi_test_and_set_on\n\t * (see vmx_deliver_posted_interrupt).\n\t *\n\t * 3) This also orders the write to mode from any reads to the page\n\t * tables done while the VCPU is running.  Please see the comment\n\t * in kvm_flush_remote_tlbs.\n\t */\n\tsmp_mb__after_srcu_read_unlock();\n\n\t/*\n\t * Process pending posted interrupts to handle the case where the\n\t * notification IRQ arrived in the host, or was never sent (because the\n\t * target vCPU wasn't running).  Do this regardless of the vCPU's APICv\n\t * status, KVM doesn't update assigned devices when APICv is inhibited,\n\t * i.e. they can post interrupts even if APICv is temporarily disabled.\n\t */\n\tif (kvm_lapic_enabled(vcpu))\n\t\tstatic_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);\n\n\tif (kvm_vcpu_exit_request(vcpu)) {\n\t\tvcpu->mode = OUTSIDE_GUEST_MODE;\n\t\tsmp_wmb();\n\t\tlocal_irq_enable();\n\t\tpreempt_enable();\n\t\tkvm_vcpu_srcu_read_lock(vcpu);\n\t\tr = 1;\n\t\tgoto cancel_injection;\n\t}\n\n\tif (req_immediate_exit) {\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tstatic_call(kvm_x86_request_immediate_exit)(vcpu);\n\t}\n\n\tfpregs_assert_state_consistent();\n\tif (test_thread_flag(TIF_NEED_FPU_LOAD))\n\t\tswitch_fpu_return();\n\n\tif (vcpu->arch.guest_fpu.xfd_err)\n\t\twrmsrl(MSR_IA32_XFD_ERR, vcpu->arch.guest_fpu.xfd_err);\n\n\tif (unlikely(vcpu->arch.switch_db_regs)) {\n\t\tset_debugreg(0, 7);\n\t\tset_debugreg(vcpu->arch.eff_db[0], 0);\n\t\tset_debugreg(vcpu->arch.eff_db[1], 1);\n\t\tset_debugreg(vcpu->arch.eff_db[2], 2);\n\t\tset_debugreg(vcpu->arch.eff_db[3], 3);\n\t} else if (unlikely(hw_breakpoint_active())) {\n\t\tset_debugreg(0, 7);\n\t}\n\n\tguest_timing_enter_irqoff();\n\n\tfor (;;) {\n\t\t/*\n\t\t * Assert that vCPU vs. VM APICv state is consistent.  An APICv\n\t\t * update must kick and wait for all vCPUs before toggling the\n\t\t * per-VM state, and responsing vCPUs must wait for the update\n\t\t * to complete before servicing KVM_REQ_APICV_UPDATE.\n\t\t */\n\t\tWARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&\n\t\t\t     (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));\n\n\t\texit_fastpath = static_call(kvm_x86_vcpu_run)(vcpu);\n\t\tif (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))\n\t\t\tbreak;\n\n\t\tif (kvm_lapic_enabled(vcpu))\n\t\t\tstatic_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);\n\n\t\tif (unlikely(kvm_vcpu_exit_request(vcpu))) {\n\t\t\texit_fastpath = EXIT_FASTPATH_EXIT_HANDLED;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * Do this here before restoring debug registers on the host.  And\n\t * since we do this before handling the vmexit, a DR access vmexit\n\t * can (a) read the correct value of the debug registers, (b) set\n\t * KVM_DEBUGREG_WONT_EXIT again.\n\t */\n\tif (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)) {\n\t\tWARN_ON(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP);\n\t\tstatic_call(kvm_x86_sync_dirty_debug_regs)(vcpu);\n\t\tkvm_update_dr0123(vcpu);\n\t\tkvm_update_dr7(vcpu);\n\t}\n\n\t/*\n\t * If the guest has used debug registers, at least dr7\n\t * will be disabled while returning to the host.\n\t * If we don't have active breakpoints in the host, we don't\n\t * care about the messed up debug address registers. But if\n\t * we have some of them active, restore the old state.\n\t */\n\tif (hw_breakpoint_active())\n\t\thw_breakpoint_restore();\n\n\tvcpu->arch.last_vmentry_cpu = vcpu->cpu;\n\tvcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\n\tvcpu->mode = OUTSIDE_GUEST_MODE;\n\tsmp_wmb();\n\n\t/*\n\t * Sync xfd before calling handle_exit_irqoff() which may\n\t * rely on the fact that guest_fpu::xfd is up-to-date (e.g.\n\t * in #NM irqoff handler).\n\t */\n\tif (vcpu->arch.xfd_no_write_intercept)\n\t\tfpu_sync_guest_vmexit_xfd_state();\n\n\tstatic_call(kvm_x86_handle_exit_irqoff)(vcpu);\n\n\tif (vcpu->arch.guest_fpu.xfd_err)\n\t\twrmsrl(MSR_IA32_XFD_ERR, 0);\n\n\t/*\n\t * Consume any pending interrupts, including the possible source of\n\t * VM-Exit on SVM and any ticks that occur between VM-Exit and now.\n\t * An instruction is required after local_irq_enable() to fully unblock\n\t * interrupts on processors that implement an interrupt shadow, the\n\t * stat.exits increment will do nicely.\n\t */\n\tkvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);\n\tlocal_irq_enable();\n\t++vcpu->stat.exits;\n\tlocal_irq_disable();\n\tkvm_after_interrupt(vcpu);\n\n\t/*\n\t * Wait until after servicing IRQs to account guest time so that any\n\t * ticks that occurred while running the guest are properly accounted\n\t * to the guest.  Waiting until IRQs are enabled degrades the accuracy\n\t * of accounting via context tracking, but the loss of accuracy is\n\t * acceptable for all known use cases.\n\t */\n\tguest_timing_exit_irqoff();\n\n\tlocal_irq_enable();\n\tpreempt_enable();\n\n\tkvm_vcpu_srcu_read_lock(vcpu);\n\n\t/*\n\t * Profile KVM exit RIPs:\n\t */\n\tif (unlikely(prof_on == KVM_PROFILING)) {\n\t\tunsigned long rip = kvm_rip_read(vcpu);\n\t\tprofile_hit(KVM_PROFILING, (void *)rip);\n\t}\n\n\tif (unlikely(vcpu->arch.tsc_always_catchup))\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\n\tif (vcpu->arch.apic_attention)\n\t\tkvm_lapic_sync_from_vapic(vcpu);\n\n\tr = static_call(kvm_x86_handle_exit)(vcpu, exit_fastpath);\n\treturn r;\n\ncancel_injection:\n\tif (req_immediate_exit)\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\tstatic_call(kvm_x86_cancel_injection)(vcpu);\n\tif (unlikely(vcpu->arch.apic_attention))\n\t\tkvm_lapic_sync_from_vapic(vcpu);\nout:\n\treturn r;\n}\n\n/* Called within kvm->srcu read side.  */\nstatic inline int vcpu_block(struct kvm_vcpu *vcpu)\n{\n\tbool hv_timer;\n\n\tif (!kvm_arch_vcpu_runnable(vcpu)) {\n\t\t/*\n\t\t * Switch to the software timer before halt-polling/blocking as\n\t\t * the guest's timer may be a break event for the vCPU, and the\n\t\t * hypervisor timer runs only when the CPU is in guest mode.\n\t\t * Switch before halt-polling so that KVM recognizes an expired\n\t\t * timer before blocking.\n\t\t */\n\t\thv_timer = kvm_lapic_hv_timer_in_use(vcpu);\n\t\tif (hv_timer)\n\t\t\tkvm_lapic_switch_to_sw_timer(vcpu);\n\n\t\tkvm_vcpu_srcu_read_unlock(vcpu);\n\t\tif (vcpu->arch.mp_state == KVM_MP_STATE_HALTED)\n\t\t\tkvm_vcpu_halt(vcpu);\n\t\telse\n\t\t\tkvm_vcpu_block(vcpu);\n\t\tkvm_vcpu_srcu_read_lock(vcpu);\n\n\t\tif (hv_timer)\n\t\t\tkvm_lapic_switch_to_hv_timer(vcpu);\n\n\t\t/*\n\t\t * If the vCPU is not runnable, a signal or another host event\n\t\t * of some kind is pending; service it without changing the\n\t\t * vCPU's activity state.\n\t\t */\n\t\tif (!kvm_arch_vcpu_runnable(vcpu))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * Evaluate nested events before exiting the halted state.  This allows\n\t * the halt state to be recorded properly in the VMCS12's activity\n\t * state field (AMD does not have a similar field and a VM-Exit always\n\t * causes a spurious wakeup from HLT).\n\t */\n\tif (is_guest_mode(vcpu)) {\n\t\tif (kvm_check_nested_events(vcpu) < 0)\n\t\t\treturn 0;\n\t}\n\n\tif (kvm_apic_accept_events(vcpu) < 0)\n\t\treturn 0;\n\tswitch(vcpu->arch.mp_state) {\n\tcase KVM_MP_STATE_HALTED:\n\tcase KVM_MP_STATE_AP_RESET_HOLD:\n\t\tvcpu->arch.pv.pv_unhalted = false;\n\t\tvcpu->arch.mp_state =\n\t\t\tKVM_MP_STATE_RUNNABLE;\n\t\tfallthrough;\n\tcase KVM_MP_STATE_RUNNABLE:\n\t\tvcpu->arch.apf.halted = false;\n\t\tbreak;\n\tcase KVM_MP_STATE_INIT_RECEIVED:\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n\treturn 1;\n}\n\nstatic inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)\n{\n\treturn (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&\n\t\t!vcpu->arch.apf.halted);\n}\n\n/* Called within kvm->srcu read side.  */\nstatic int vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\tfor (;;) {\n\t\t/*\n\t\t * If another guest vCPU requests a PV TLB flush in the middle\n\t\t * of instruction emulation, the rest of the emulation could\n\t\t * use a stale page translation. Assume that any code after\n\t\t * this point can start executing an instruction.\n\t\t */\n\t\tvcpu->arch.at_instruction_boundary = false;\n\t\tif (kvm_vcpu_running(vcpu)) {\n\t\t\tr = vcpu_enter_guest(vcpu);\n\t\t} else {\n\t\t\tr = vcpu_block(vcpu);\n\t\t}\n\n\t\tif (r <= 0)\n\t\t\tbreak;\n\n\t\tkvm_clear_request(KVM_REQ_UNBLOCK, vcpu);\n\t\tif (kvm_xen_has_pending_events(vcpu))\n\t\t\tkvm_xen_inject_pending_events(vcpu);\n\n\t\tif (kvm_cpu_has_pending_timer(vcpu))\n\t\t\tkvm_inject_pending_timer_irqs(vcpu);\n\n\t\tif (dm_request_for_irq_injection(vcpu) &&\n\t\t\tkvm_vcpu_ready_for_interrupt_injection(vcpu)) {\n\t\t\tr = 0;\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_IRQ_WINDOW_OPEN;\n\t\t\t++vcpu->stat.request_irq_exits;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (__xfer_to_guest_mode_work_pending()) {\n\t\t\tkvm_vcpu_srcu_read_unlock(vcpu);\n\t\t\tr = xfer_to_guest_mode_handle_work(vcpu);\n\t\t\tkvm_vcpu_srcu_read_lock(vcpu);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn r;\n}\n\nstatic inline int complete_emulated_io(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE);\n}\n\nstatic int complete_emulated_pio(struct kvm_vcpu *vcpu)\n{\n\tBUG_ON(!vcpu->arch.pio.count);\n\n\treturn complete_emulated_io(vcpu);\n}\n\n/*\n * Implements the following, as a state machine:\n *\n * read:\n *   for each fragment\n *     for each mmio piece in the fragment\n *       write gpa, len\n *       exit\n *       copy data\n *   execute insn\n *\n * write:\n *   for each fragment\n *     for each mmio piece in the fragment\n *       write gpa, len\n *       copy data\n *       exit\n */\nstatic int complete_emulated_mmio(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *run = vcpu->run;\n\tstruct kvm_mmio_fragment *frag;\n\tunsigned len;\n\n\tBUG_ON(!vcpu->mmio_needed);\n\n\t/* Complete previous fragment */\n\tfrag = &vcpu->mmio_fragments[vcpu->mmio_cur_fragment];\n\tlen = min(8u, frag->len);\n\tif (!vcpu->mmio_is_write)\n\t\tmemcpy(frag->data, run->mmio.data, len);\n\n\tif (frag->len <= 8) {\n\t\t/* Switch to the next fragment. */\n\t\tfrag++;\n\t\tvcpu->mmio_cur_fragment++;\n\t} else {\n\t\t/* Go forward to the next mmio piece. */\n\t\tfrag->data += len;\n\t\tfrag->gpa += len;\n\t\tfrag->len -= len;\n\t}\n\n\tif (vcpu->mmio_cur_fragment >= vcpu->mmio_nr_fragments) {\n\t\tvcpu->mmio_needed = 0;\n\n\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\tif (vcpu->mmio_is_write)\n\t\t\treturn 1;\n\t\tvcpu->mmio_read_completed = 1;\n\t\treturn complete_emulated_io(vcpu);\n\t}\n\n\trun->exit_reason = KVM_EXIT_MMIO;\n\trun->mmio.phys_addr = frag->gpa;\n\tif (vcpu->mmio_is_write)\n\t\tmemcpy(run->mmio.data, frag->data, min(8u, frag->len));\n\trun->mmio.len = min(8u, frag->len);\n\trun->mmio.is_write = vcpu->mmio_is_write;\n\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\treturn 0;\n}\n\n/* Swap (qemu) user FPU context for the guest FPU context. */\nstatic void kvm_load_guest_fpu(struct kvm_vcpu *vcpu)\n{\n\t/* Exclude PKRU, it's restored separately immediately after VM-Exit. */\n\tfpu_swap_kvm_fpstate(&vcpu->arch.guest_fpu, true);\n\ttrace_kvm_fpu(1);\n}\n\n/* When vcpu_run ends, restore user space FPU context. */\nstatic void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)\n{\n\tfpu_swap_kvm_fpstate(&vcpu->arch.guest_fpu, false);\n\t++vcpu->stat.fpu_reload;\n\ttrace_kvm_fpu(0);\n}\n\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_queued_exception *ex = &vcpu->arch.exception;\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tint r;\n\n\tvcpu_load(vcpu);\n\tkvm_sigset_activate(vcpu);\n\tkvm_run->flags = 0;\n\tkvm_load_guest_fpu(vcpu);\n\n\tkvm_vcpu_srcu_read_lock(vcpu);\n\tif (unlikely(vcpu->arch.mp_state == KVM_MP_STATE_UNINITIALIZED)) {\n\t\tif (kvm_run->immediate_exit) {\n\t\t\tr = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * It should be impossible for the hypervisor timer to be in\n\t\t * use before KVM has ever run the vCPU.\n\t\t */\n\t\tWARN_ON_ONCE(kvm_lapic_hv_timer_in_use(vcpu));\n\n\t\tkvm_vcpu_srcu_read_unlock(vcpu);\n\t\tkvm_vcpu_block(vcpu);\n\t\tkvm_vcpu_srcu_read_lock(vcpu);\n\n\t\tif (kvm_apic_accept_events(vcpu) < 0) {\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tr = -EAGAIN;\n\t\tif (signal_pending(current)) {\n\t\t\tr = -EINTR;\n\t\t\tkvm_run->exit_reason = KVM_EXIT_INTR;\n\t\t\t++vcpu->stat.signal_exits;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif ((kvm_run->kvm_valid_regs & ~KVM_SYNC_X86_VALID_FIELDS) ||\n\t    (kvm_run->kvm_dirty_regs & ~KVM_SYNC_X86_VALID_FIELDS)) {\n\t\tr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (kvm_run->kvm_dirty_regs) {\n\t\tr = sync_regs(vcpu);\n\t\tif (r != 0)\n\t\t\tgoto out;\n\t}\n\n\t/* re-sync apic's tpr */\n\tif (!lapic_in_kernel(vcpu)) {\n\t\tif (kvm_set_cr8(vcpu, kvm_run->cr8) != 0) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * If userspace set a pending exception and L2 is active, convert it to\n\t * a pending VM-Exit if L1 wants to intercept the exception.\n\t */\n\tif (vcpu->arch.exception_from_userspace && is_guest_mode(vcpu) &&\n\t    kvm_x86_ops.nested_ops->is_exception_vmexit(vcpu, ex->vector,\n\t\t\t\t\t\t\tex->error_code)) {\n\t\tkvm_queue_exception_vmexit(vcpu, ex->vector,\n\t\t\t\t\t   ex->has_error_code, ex->error_code,\n\t\t\t\t\t   ex->has_payload, ex->payload);\n\t\tex->injected = false;\n\t\tex->pending = false;\n\t}\n\tvcpu->arch.exception_from_userspace = false;\n\n\tif (unlikely(vcpu->arch.complete_userspace_io)) {\n\t\tint (*cui)(struct kvm_vcpu *) = vcpu->arch.complete_userspace_io;\n\t\tvcpu->arch.complete_userspace_io = NULL;\n\t\tr = cui(vcpu);\n\t\tif (r <= 0)\n\t\t\tgoto out;\n\t} else {\n\t\tWARN_ON_ONCE(vcpu->arch.pio.count);\n\t\tWARN_ON_ONCE(vcpu->mmio_needed);\n\t}\n\n\tif (kvm_run->immediate_exit) {\n\t\tr = -EINTR;\n\t\tgoto out;\n\t}\n\n\tr = static_call(kvm_x86_vcpu_pre_run)(vcpu);\n\tif (r <= 0)\n\t\tgoto out;\n\n\tr = vcpu_run(vcpu);\n\nout:\n\tkvm_put_guest_fpu(vcpu);\n\tif (kvm_run->kvm_valid_regs)\n\t\tstore_regs(vcpu);\n\tpost_kvm_run_save(vcpu);\n\tkvm_vcpu_srcu_read_unlock(vcpu);\n\n\tkvm_sigset_deactivate(vcpu);\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\nstatic void __get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tif (vcpu->arch.emulate_regs_need_sync_to_vcpu) {\n\t\t/*\n\t\t * We are here if userspace calls get_regs() in the middle of\n\t\t * instruction emulation. Registers state needs to be copied\n\t\t * back from emulation context to vcpu. Userspace shouldn't do\n\t\t * that usually, but some bad designed PV devices (vmware\n\t\t * backdoor interface) need this to work\n\t\t */\n\t\temulator_writeback_register_cache(vcpu->arch.emulate_ctxt);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t}\n\tregs->rax = kvm_rax_read(vcpu);\n\tregs->rbx = kvm_rbx_read(vcpu);\n\tregs->rcx = kvm_rcx_read(vcpu);\n\tregs->rdx = kvm_rdx_read(vcpu);\n\tregs->rsi = kvm_rsi_read(vcpu);\n\tregs->rdi = kvm_rdi_read(vcpu);\n\tregs->rsp = kvm_rsp_read(vcpu);\n\tregs->rbp = kvm_rbp_read(vcpu);\n#ifdef CONFIG_X86_64\n\tregs->r8 = kvm_r8_read(vcpu);\n\tregs->r9 = kvm_r9_read(vcpu);\n\tregs->r10 = kvm_r10_read(vcpu);\n\tregs->r11 = kvm_r11_read(vcpu);\n\tregs->r12 = kvm_r12_read(vcpu);\n\tregs->r13 = kvm_r13_read(vcpu);\n\tregs->r14 = kvm_r14_read(vcpu);\n\tregs->r15 = kvm_r15_read(vcpu);\n#endif\n\n\tregs->rip = kvm_rip_read(vcpu);\n\tregs->rflags = kvm_get_rflags(vcpu);\n}\n\nint kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tvcpu_load(vcpu);\n\t__get_regs(vcpu, regs);\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nstatic void __set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tvcpu->arch.emulate_regs_need_sync_from_vcpu = true;\n\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\n\tkvm_rax_write(vcpu, regs->rax);\n\tkvm_rbx_write(vcpu, regs->rbx);\n\tkvm_rcx_write(vcpu, regs->rcx);\n\tkvm_rdx_write(vcpu, regs->rdx);\n\tkvm_rsi_write(vcpu, regs->rsi);\n\tkvm_rdi_write(vcpu, regs->rdi);\n\tkvm_rsp_write(vcpu, regs->rsp);\n\tkvm_rbp_write(vcpu, regs->rbp);\n#ifdef CONFIG_X86_64\n\tkvm_r8_write(vcpu, regs->r8);\n\tkvm_r9_write(vcpu, regs->r9);\n\tkvm_r10_write(vcpu, regs->r10);\n\tkvm_r11_write(vcpu, regs->r11);\n\tkvm_r12_write(vcpu, regs->r12);\n\tkvm_r13_write(vcpu, regs->r13);\n\tkvm_r14_write(vcpu, regs->r14);\n\tkvm_r15_write(vcpu, regs->r15);\n#endif\n\n\tkvm_rip_write(vcpu, regs->rip);\n\tkvm_set_rflags(vcpu, regs->rflags | X86_EFLAGS_FIXED);\n\n\tvcpu->arch.exception.pending = false;\n\tvcpu->arch.exception_vmexit.pending = false;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\n\nint kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tvcpu_load(vcpu);\n\t__set_regs(vcpu, regs);\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nstatic void __get_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\tstruct desc_ptr dt;\n\n\tif (vcpu->arch.guest_state_protected)\n\t\tgoto skip_protected_regs;\n\n\tkvm_get_segment(vcpu, &sregs->cs, VCPU_SREG_CS);\n\tkvm_get_segment(vcpu, &sregs->ds, VCPU_SREG_DS);\n\tkvm_get_segment(vcpu, &sregs->es, VCPU_SREG_ES);\n\tkvm_get_segment(vcpu, &sregs->fs, VCPU_SREG_FS);\n\tkvm_get_segment(vcpu, &sregs->gs, VCPU_SREG_GS);\n\tkvm_get_segment(vcpu, &sregs->ss, VCPU_SREG_SS);\n\n\tkvm_get_segment(vcpu, &sregs->tr, VCPU_SREG_TR);\n\tkvm_get_segment(vcpu, &sregs->ldt, VCPU_SREG_LDTR);\n\n\tstatic_call(kvm_x86_get_idt)(vcpu, &dt);\n\tsregs->idt.limit = dt.size;\n\tsregs->idt.base = dt.address;\n\tstatic_call(kvm_x86_get_gdt)(vcpu, &dt);\n\tsregs->gdt.limit = dt.size;\n\tsregs->gdt.base = dt.address;\n\n\tsregs->cr2 = vcpu->arch.cr2;\n\tsregs->cr3 = kvm_read_cr3(vcpu);\n\nskip_protected_regs:\n\tsregs->cr0 = kvm_read_cr0(vcpu);\n\tsregs->cr4 = kvm_read_cr4(vcpu);\n\tsregs->cr8 = kvm_get_cr8(vcpu);\n\tsregs->efer = vcpu->arch.efer;\n\tsregs->apic_base = kvm_get_apic_base(vcpu);\n}\n\nstatic void __get_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\t__get_sregs_common(vcpu, sregs);\n\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn;\n\n\tif (vcpu->arch.interrupt.injected && !vcpu->arch.interrupt.soft)\n\t\tset_bit(vcpu->arch.interrupt.nr,\n\t\t\t(unsigned long *)sregs->interrupt_bitmap);\n}\n\nstatic void __get_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2)\n{\n\tint i;\n\n\t__get_sregs_common(vcpu, (struct kvm_sregs *)sregs2);\n\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn;\n\n\tif (is_pae_paging(vcpu)) {\n\t\tfor (i = 0 ; i < 4 ; i++)\n\t\t\tsregs2->pdptrs[i] = kvm_pdptr_read(vcpu, i);\n\t\tsregs2->flags |= KVM_SREGS2_FLAGS_PDPTRS_VALID;\n\t}\n}\n\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs)\n{\n\tvcpu_load(vcpu);\n\t__get_sregs(vcpu, sregs);\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state)\n{\n\tint r;\n\n\tvcpu_load(vcpu);\n\tif (kvm_mpx_supported())\n\t\tkvm_load_guest_fpu(vcpu);\n\n\tr = kvm_apic_accept_events(vcpu);\n\tif (r < 0)\n\t\tgoto out;\n\tr = 0;\n\n\tif ((vcpu->arch.mp_state == KVM_MP_STATE_HALTED ||\n\t     vcpu->arch.mp_state == KVM_MP_STATE_AP_RESET_HOLD) &&\n\t    vcpu->arch.pv.pv_unhalted)\n\t\tmp_state->mp_state = KVM_MP_STATE_RUNNABLE;\n\telse\n\t\tmp_state->mp_state = vcpu->arch.mp_state;\n\nout:\n\tif (kvm_mpx_supported())\n\t\tkvm_put_guest_fpu(vcpu);\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\nint kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state)\n{\n\tint ret = -EINVAL;\n\n\tvcpu_load(vcpu);\n\n\tswitch (mp_state->mp_state) {\n\tcase KVM_MP_STATE_UNINITIALIZED:\n\tcase KVM_MP_STATE_HALTED:\n\tcase KVM_MP_STATE_AP_RESET_HOLD:\n\tcase KVM_MP_STATE_INIT_RECEIVED:\n\tcase KVM_MP_STATE_SIPI_RECEIVED:\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tbreak;\n\n\tcase KVM_MP_STATE_RUNNABLE:\n\t\tbreak;\n\n\tdefault:\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Pending INITs are reported using KVM_SET_VCPU_EVENTS, disallow\n\t * forcing the guest into INIT/SIPI if those events are supposed to be\n\t * blocked.  KVM prioritizes SMI over INIT, so reject INIT/SIPI state\n\t * if an SMI is pending as well.\n\t */\n\tif ((!kvm_apic_init_sipi_allowed(vcpu) || vcpu->arch.smi_pending) &&\n\t    (mp_state->mp_state == KVM_MP_STATE_SIPI_RECEIVED ||\n\t     mp_state->mp_state == KVM_MP_STATE_INIT_RECEIVED))\n\t\tgoto out;\n\n\tif (mp_state->mp_state == KVM_MP_STATE_SIPI_RECEIVED) {\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;\n\t\tset_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);\n\t} else\n\t\tvcpu->arch.mp_state = mp_state->mp_state;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tret = 0;\nout:\n\tvcpu_put(vcpu);\n\treturn ret;\n}\n\nint kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int idt_index,\n\t\t    int reason, bool has_error_code, u32 error_code)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint ret;\n\n\tinit_emulate_ctxt(vcpu);\n\n\tret = emulator_task_switch(ctxt, tss_selector, idt_index, reason,\n\t\t\t\t   has_error_code, error_code);\n\tif (ret) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn 0;\n\t}\n\n\tkvm_rip_write(vcpu, ctxt->eip);\n\tkvm_set_rflags(vcpu, ctxt->eflags);\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(kvm_task_switch);\n\nstatic bool kvm_is_valid_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\tif ((sregs->efer & EFER_LME) && (sregs->cr0 & X86_CR0_PG)) {\n\t\t/*\n\t\t * When EFER.LME and CR0.PG are set, the processor is in\n\t\t * 64-bit mode (though maybe in a 32-bit code segment).\n\t\t * CR4.PAE and EFER.LMA must be set.\n\t\t */\n\t\tif (!(sregs->cr4 & X86_CR4_PAE) || !(sregs->efer & EFER_LMA))\n\t\t\treturn false;\n\t\tif (kvm_vcpu_is_illegal_gpa(vcpu, sregs->cr3))\n\t\t\treturn false;\n\t} else {\n\t\t/*\n\t\t * Not in 64-bit mode: EFER.LMA is clear and the code\n\t\t * segment cannot be 64-bit.\n\t\t */\n\t\tif (sregs->efer & EFER_LMA || sregs->cs.l)\n\t\t\treturn false;\n\t}\n\n\treturn kvm_is_valid_cr4(vcpu, sregs->cr4);\n}\n\nstatic int __set_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs,\n\t\tint *mmu_reset_needed, bool update_pdptrs)\n{\n\tstruct msr_data apic_base_msr;\n\tint idx;\n\tstruct desc_ptr dt;\n\n\tif (!kvm_is_valid_sregs(vcpu, sregs))\n\t\treturn -EINVAL;\n\n\tapic_base_msr.data = sregs->apic_base;\n\tapic_base_msr.host_initiated = true;\n\tif (kvm_set_apic_base(vcpu, &apic_base_msr))\n\t\treturn -EINVAL;\n\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn 0;\n\n\tdt.size = sregs->idt.limit;\n\tdt.address = sregs->idt.base;\n\tstatic_call(kvm_x86_set_idt)(vcpu, &dt);\n\tdt.size = sregs->gdt.limit;\n\tdt.address = sregs->gdt.base;\n\tstatic_call(kvm_x86_set_gdt)(vcpu, &dt);\n\n\tvcpu->arch.cr2 = sregs->cr2;\n\t*mmu_reset_needed |= kvm_read_cr3(vcpu) != sregs->cr3;\n\tvcpu->arch.cr3 = sregs->cr3;\n\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_CR3);\n\tstatic_call_cond(kvm_x86_post_set_cr3)(vcpu, sregs->cr3);\n\n\tkvm_set_cr8(vcpu, sregs->cr8);\n\n\t*mmu_reset_needed |= vcpu->arch.efer != sregs->efer;\n\tstatic_call(kvm_x86_set_efer)(vcpu, sregs->efer);\n\n\t*mmu_reset_needed |= kvm_read_cr0(vcpu) != sregs->cr0;\n\tstatic_call(kvm_x86_set_cr0)(vcpu, sregs->cr0);\n\tvcpu->arch.cr0 = sregs->cr0;\n\n\t*mmu_reset_needed |= kvm_read_cr4(vcpu) != sregs->cr4;\n\tstatic_call(kvm_x86_set_cr4)(vcpu, sregs->cr4);\n\n\tif (update_pdptrs) {\n\t\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tif (is_pae_paging(vcpu)) {\n\t\t\tload_pdptrs(vcpu, kvm_read_cr3(vcpu));\n\t\t\t*mmu_reset_needed = 1;\n\t\t}\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t}\n\n\tkvm_set_segment(vcpu, &sregs->cs, VCPU_SREG_CS);\n\tkvm_set_segment(vcpu, &sregs->ds, VCPU_SREG_DS);\n\tkvm_set_segment(vcpu, &sregs->es, VCPU_SREG_ES);\n\tkvm_set_segment(vcpu, &sregs->fs, VCPU_SREG_FS);\n\tkvm_set_segment(vcpu, &sregs->gs, VCPU_SREG_GS);\n\tkvm_set_segment(vcpu, &sregs->ss, VCPU_SREG_SS);\n\n\tkvm_set_segment(vcpu, &sregs->tr, VCPU_SREG_TR);\n\tkvm_set_segment(vcpu, &sregs->ldt, VCPU_SREG_LDTR);\n\n\tupdate_cr8_intercept(vcpu);\n\n\t/* Older userspace won't unhalt the vcpu on reset. */\n\tif (kvm_vcpu_is_bsp(vcpu) && kvm_rip_read(vcpu) == 0xfff0 &&\n\t    sregs->cs.selector == 0xf000 && sregs->cs.base == 0xffff0000 &&\n\t    !is_protmode(vcpu))\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\n\treturn 0;\n}\n\nstatic int __set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\tint pending_vec, max_bits;\n\tint mmu_reset_needed = 0;\n\tint ret = __set_sregs_common(vcpu, sregs, &mmu_reset_needed, true);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (mmu_reset_needed)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\tmax_bits = KVM_NR_INTERRUPTS;\n\tpending_vec = find_first_bit(\n\t\t(const unsigned long *)sregs->interrupt_bitmap, max_bits);\n\n\tif (pending_vec < max_bits) {\n\t\tkvm_queue_interrupt(vcpu, pending_vec, false);\n\t\tpr_debug(\"Set back pending irq %d\\n\", pending_vec);\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t}\n\treturn 0;\n}\n\nstatic int __set_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2)\n{\n\tint mmu_reset_needed = 0;\n\tbool valid_pdptrs = sregs2->flags & KVM_SREGS2_FLAGS_PDPTRS_VALID;\n\tbool pae = (sregs2->cr0 & X86_CR0_PG) && (sregs2->cr4 & X86_CR4_PAE) &&\n\t\t!(sregs2->efer & EFER_LMA);\n\tint i, ret;\n\n\tif (sregs2->flags & ~KVM_SREGS2_FLAGS_PDPTRS_VALID)\n\t\treturn -EINVAL;\n\n\tif (valid_pdptrs && (!pae || vcpu->arch.guest_state_protected))\n\t\treturn -EINVAL;\n\n\tret = __set_sregs_common(vcpu, (struct kvm_sregs *)sregs2,\n\t\t\t\t &mmu_reset_needed, !valid_pdptrs);\n\tif (ret)\n\t\treturn ret;\n\n\tif (valid_pdptrs) {\n\t\tfor (i = 0; i < 4 ; i++)\n\t\t\tkvm_pdptr_write(vcpu, i, sregs2->pdptrs[i]);\n\n\t\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_PDPTR);\n\t\tmmu_reset_needed = 1;\n\t\tvcpu->arch.pdptrs_from_userspace = true;\n\t}\n\tif (mmu_reset_needed)\n\t\tkvm_mmu_reset_context(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs)\n{\n\tint ret;\n\n\tvcpu_load(vcpu);\n\tret = __set_sregs(vcpu, sregs);\n\tvcpu_put(vcpu);\n\treturn ret;\n}\n\nstatic void kvm_arch_vcpu_guestdbg_update_apicv_inhibit(struct kvm *kvm)\n{\n\tbool set = false;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\n\tif (!enable_apicv)\n\t\treturn;\n\n\tdown_write(&kvm->arch.apicv_update_lock);\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_BLOCKIRQ) {\n\t\t\tset = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\t__kvm_set_or_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_BLOCKIRQ, set);\n\tup_write(&kvm->arch.apicv_update_lock);\n}\n\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_guest_debug *dbg)\n{\n\tunsigned long rflags;\n\tint i, r;\n\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn -EINVAL;\n\n\tvcpu_load(vcpu);\n\n\tif (dbg->control & (KVM_GUESTDBG_INJECT_DB | KVM_GUESTDBG_INJECT_BP)) {\n\t\tr = -EBUSY;\n\t\tif (kvm_is_exception_pending(vcpu))\n\t\t\tgoto out;\n\t\tif (dbg->control & KVM_GUESTDBG_INJECT_DB)\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\telse\n\t\t\tkvm_queue_exception(vcpu, BP_VECTOR);\n\t}\n\n\t/*\n\t * Read rflags as long as potentially injected trace flags are still\n\t * filtered out.\n\t */\n\trflags = kvm_get_rflags(vcpu);\n\n\tvcpu->guest_debug = dbg->control;\n\tif (!(vcpu->guest_debug & KVM_GUESTDBG_ENABLE))\n\t\tvcpu->guest_debug = 0;\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {\n\t\tfor (i = 0; i < KVM_NR_DB_REGS; ++i)\n\t\t\tvcpu->arch.eff_db[i] = dbg->arch.debugreg[i];\n\t\tvcpu->arch.guest_debug_dr7 = dbg->arch.debugreg[7];\n\t} else {\n\t\tfor (i = 0; i < KVM_NR_DB_REGS; i++)\n\t\t\tvcpu->arch.eff_db[i] = vcpu->arch.db[i];\n\t}\n\tkvm_update_dr7(vcpu);\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\tvcpu->arch.singlestep_rip = kvm_get_linear_rip(vcpu);\n\n\t/*\n\t * Trigger an rflags update that will inject or remove the trace\n\t * flags.\n\t */\n\tkvm_set_rflags(vcpu, rflags);\n\n\tstatic_call(kvm_x86_update_exception_bitmap)(vcpu);\n\n\tkvm_arch_vcpu_guestdbg_update_apicv_inhibit(vcpu->kvm);\n\n\tr = 0;\n\nout:\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\n/*\n * Translate a guest virtual address to a guest physical address.\n */\nint kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_translation *tr)\n{\n\tunsigned long vaddr = tr->linear_address;\n\tgpa_t gpa;\n\tint idx;\n\n\tvcpu_load(vcpu);\n\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\tgpa = kvm_mmu_gva_to_gpa_system(vcpu, vaddr, NULL);\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\ttr->physical_address = gpa;\n\ttr->valid = gpa != INVALID_GPA;\n\ttr->writeable = 1;\n\ttr->usermode = 0;\n\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\n{\n\tstruct fxregs_state *fxsave;\n\n\tif (fpstate_is_confidential(&vcpu->arch.guest_fpu))\n\t\treturn 0;\n\n\tvcpu_load(vcpu);\n\n\tfxsave = &vcpu->arch.guest_fpu.fpstate->regs.fxsave;\n\tmemcpy(fpu->fpr, fxsave->st_space, 128);\n\tfpu->fcw = fxsave->cwd;\n\tfpu->fsw = fxsave->swd;\n\tfpu->ftwx = fxsave->twd;\n\tfpu->last_opcode = fxsave->fop;\n\tfpu->last_ip = fxsave->rip;\n\tfpu->last_dp = fxsave->rdp;\n\tmemcpy(fpu->xmm, fxsave->xmm_space, sizeof(fxsave->xmm_space));\n\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\n{\n\tstruct fxregs_state *fxsave;\n\n\tif (fpstate_is_confidential(&vcpu->arch.guest_fpu))\n\t\treturn 0;\n\n\tvcpu_load(vcpu);\n\n\tfxsave = &vcpu->arch.guest_fpu.fpstate->regs.fxsave;\n\n\tmemcpy(fxsave->st_space, fpu->fpr, 128);\n\tfxsave->cwd = fpu->fcw;\n\tfxsave->swd = fpu->fsw;\n\tfxsave->twd = fpu->ftwx;\n\tfxsave->fop = fpu->last_opcode;\n\tfxsave->rip = fpu->last_ip;\n\tfxsave->rdp = fpu->last_dp;\n\tmemcpy(fxsave->xmm_space, fpu->xmm, sizeof(fxsave->xmm_space));\n\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nstatic void store_regs(struct kvm_vcpu *vcpu)\n{\n\tBUILD_BUG_ON(sizeof(struct kvm_sync_regs) > SYNC_REGS_SIZE_BYTES);\n\n\tif (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_REGS)\n\t\t__get_regs(vcpu, &vcpu->run->s.regs.regs);\n\n\tif (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_SREGS)\n\t\t__get_sregs(vcpu, &vcpu->run->s.regs.sregs);\n\n\tif (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_EVENTS)\n\t\tkvm_vcpu_ioctl_x86_get_vcpu_events(\n\t\t\t\tvcpu, &vcpu->run->s.regs.events);\n}\n\nstatic int sync_regs(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_REGS) {\n\t\t__set_regs(vcpu, &vcpu->run->s.regs.regs);\n\t\tvcpu->run->kvm_dirty_regs &= ~KVM_SYNC_X86_REGS;\n\t}\n\tif (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_SREGS) {\n\t\tif (__set_sregs(vcpu, &vcpu->run->s.regs.sregs))\n\t\t\treturn -EINVAL;\n\t\tvcpu->run->kvm_dirty_regs &= ~KVM_SYNC_X86_SREGS;\n\t}\n\tif (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_EVENTS) {\n\t\tif (kvm_vcpu_ioctl_x86_set_vcpu_events(\n\t\t\t\tvcpu, &vcpu->run->s.regs.events))\n\t\t\treturn -EINVAL;\n\t\tvcpu->run->kvm_dirty_regs &= ~KVM_SYNC_X86_EVENTS;\n\t}\n\n\treturn 0;\n}\n\nint kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)\n{\n\tif (kvm_check_tsc_unstable() && kvm->created_vcpus)\n\t\tpr_warn_once(\"kvm: SMP vm created on host with unstable TSC; \"\n\t\t\t     \"guest TSC will not be reliable\\n\");\n\n\tif (!kvm->arch.max_vcpu_ids)\n\t\tkvm->arch.max_vcpu_ids = KVM_MAX_VCPU_IDS;\n\n\tif (id >= kvm->arch.max_vcpu_ids)\n\t\treturn -EINVAL;\n\n\treturn static_call(kvm_x86_vcpu_precreate)(kvm);\n}\n\nint kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)\n{\n\tstruct page *page;\n\tint r;\n\n\tvcpu->arch.last_vmentry_cpu = -1;\n\tvcpu->arch.regs_avail = ~0;\n\tvcpu->arch.regs_dirty = ~0;\n\n\tkvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm, vcpu, KVM_HOST_USES_PFN);\n\n\tif (!irqchip_in_kernel(vcpu->kvm) || kvm_vcpu_is_reset_bsp(vcpu))\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\telse\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;\n\n\tr = kvm_mmu_create(vcpu);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (irqchip_in_kernel(vcpu->kvm)) {\n\t\tr = kvm_create_lapic(vcpu, lapic_timer_advance_ns);\n\t\tif (r < 0)\n\t\t\tgoto fail_mmu_destroy;\n\n\t\t/*\n\t\t * Defer evaluating inhibits until the vCPU is first run, as\n\t\t * this vCPU will not get notified of any changes until this\n\t\t * vCPU is visible to other vCPUs (marked online and added to\n\t\t * the set of vCPUs).  Opportunistically mark APICv active as\n\t\t * VMX in particularly is highly unlikely to have inhibits.\n\t\t * Ignore the current per-VM APICv state so that vCPU creation\n\t\t * is guaranteed to run with a deterministic value, the request\n\t\t * will ensure the vCPU gets the correct state before VM-Entry.\n\t\t */\n\t\tif (enable_apicv) {\n\t\t\tvcpu->arch.apic->apicv_active = true;\n\t\t\tkvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);\n\t\t}\n\t} else\n\t\tstatic_branch_inc(&kvm_has_noapic_vcpu);\n\n\tr = -ENOMEM;\n\n\tpage = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\tif (!page)\n\t\tgoto fail_free_lapic;\n\tvcpu->arch.pio_data = page_address(page);\n\n\tvcpu->arch.mce_banks = kcalloc(KVM_MAX_MCE_BANKS * 4, sizeof(u64),\n\t\t\t\t       GFP_KERNEL_ACCOUNT);\n\tvcpu->arch.mci_ctl2_banks = kcalloc(KVM_MAX_MCE_BANKS, sizeof(u64),\n\t\t\t\t\t    GFP_KERNEL_ACCOUNT);\n\tif (!vcpu->arch.mce_banks || !vcpu->arch.mci_ctl2_banks)\n\t\tgoto fail_free_mce_banks;\n\tvcpu->arch.mcg_cap = KVM_MAX_MCE_BANKS;\n\n\tif (!zalloc_cpumask_var(&vcpu->arch.wbinvd_dirty_mask,\n\t\t\t\tGFP_KERNEL_ACCOUNT))\n\t\tgoto fail_free_mce_banks;\n\n\tif (!alloc_emulate_ctxt(vcpu))\n\t\tgoto free_wbinvd_dirty_mask;\n\n\tif (!fpu_alloc_guest_fpstate(&vcpu->arch.guest_fpu)) {\n\t\tpr_err(\"kvm: failed to allocate vcpu's fpu\\n\");\n\t\tgoto free_emulate_ctxt;\n\t}\n\n\tvcpu->arch.maxphyaddr = cpuid_query_maxphyaddr(vcpu);\n\tvcpu->arch.reserved_gpa_bits = kvm_vcpu_reserved_gpa_bits_raw(vcpu);\n\n\tvcpu->arch.pat = MSR_IA32_CR_PAT_DEFAULT;\n\n\tkvm_async_pf_hash_reset(vcpu);\n\n\tvcpu->arch.perf_capabilities = kvm_caps.supported_perf_cap;\n\tkvm_pmu_init(vcpu);\n\n\tvcpu->arch.pending_external_vector = -1;\n\tvcpu->arch.preempted_in_kernel = false;\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\tvcpu->arch.hv_root_tdp = INVALID_PAGE;\n#endif\n\n\tr = static_call(kvm_x86_vcpu_create)(vcpu);\n\tif (r)\n\t\tgoto free_guest_fpu;\n\n\tvcpu->arch.arch_capabilities = kvm_get_arch_capabilities();\n\tvcpu->arch.msr_platform_info = MSR_PLATFORM_INFO_CPUID_FAULT;\n\tkvm_xen_init_vcpu(vcpu);\n\tkvm_vcpu_mtrr_init(vcpu);\n\tvcpu_load(vcpu);\n\tkvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);\n\tkvm_vcpu_reset(vcpu, false);\n\tkvm_init_mmu(vcpu);\n\tvcpu_put(vcpu);\n\treturn 0;\n\nfree_guest_fpu:\n\tfpu_free_guest_fpstate(&vcpu->arch.guest_fpu);\nfree_emulate_ctxt:\n\tkmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);\nfree_wbinvd_dirty_mask:\n\tfree_cpumask_var(vcpu->arch.wbinvd_dirty_mask);\nfail_free_mce_banks:\n\tkfree(vcpu->arch.mce_banks);\n\tkfree(vcpu->arch.mci_ctl2_banks);\n\tfree_page((unsigned long)vcpu->arch.pio_data);\nfail_free_lapic:\n\tkvm_free_lapic(vcpu);\nfail_mmu_destroy:\n\tkvm_mmu_destroy(vcpu);\n\treturn r;\n}\n\nvoid kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\n\tif (mutex_lock_killable(&vcpu->mutex))\n\t\treturn;\n\tvcpu_load(vcpu);\n\tkvm_synchronize_tsc(vcpu, 0);\n\tvcpu_put(vcpu);\n\n\t/* poll control enabled by default */\n\tvcpu->arch.msr_kvm_poll_control = 1;\n\n\tmutex_unlock(&vcpu->mutex);\n\n\tif (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)\n\t\tschedule_delayed_work(&kvm->arch.kvmclock_sync_work,\n\t\t\t\t\t\tKVMCLOCK_SYNC_PERIOD);\n}\n\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)\n{\n\tint idx;\n\n\tkvmclock_reset(vcpu);\n\n\tstatic_call(kvm_x86_vcpu_free)(vcpu);\n\n\tkmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);\n\tfree_cpumask_var(vcpu->arch.wbinvd_dirty_mask);\n\tfpu_free_guest_fpstate(&vcpu->arch.guest_fpu);\n\n\tkvm_xen_destroy_vcpu(vcpu);\n\tkvm_hv_vcpu_uninit(vcpu);\n\tkvm_pmu_destroy(vcpu);\n\tkfree(vcpu->arch.mce_banks);\n\tkfree(vcpu->arch.mci_ctl2_banks);\n\tkvm_free_lapic(vcpu);\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\tkvm_mmu_destroy(vcpu);\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\tfree_page((unsigned long)vcpu->arch.pio_data);\n\tkvfree(vcpu->arch.cpuid_entries);\n\tif (!lapic_in_kernel(vcpu))\n\t\tstatic_branch_dec(&kvm_has_noapic_vcpu);\n}\n\nvoid kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct kvm_cpuid_entry2 *cpuid_0x1;\n\tunsigned long old_cr0 = kvm_read_cr0(vcpu);\n\tunsigned long new_cr0;\n\n\t/*\n\t * Several of the \"set\" flows, e.g. ->set_cr0(), read other registers\n\t * to handle side effects.  RESET emulation hits those flows and relies\n\t * on emulated/virtualized registers, including those that are loaded\n\t * into hardware, to be zeroed at vCPU creation.  Use CRs as a sentinel\n\t * to detect improper or missing initialization.\n\t */\n\tWARN_ON_ONCE(!init_event &&\n\t\t     (old_cr0 || kvm_read_cr3(vcpu) || kvm_read_cr4(vcpu)));\n\n\t/*\n\t * SVM doesn't unconditionally VM-Exit on INIT and SHUTDOWN, thus it's\n\t * possible to INIT the vCPU while L2 is active.  Force the vCPU back\n\t * into L1 as EFER.SVME is cleared on INIT (along with all other EFER\n\t * bits), i.e. virtualization is disabled.\n\t */\n\tif (is_guest_mode(vcpu))\n\t\tkvm_leave_nested(vcpu);\n\n\tkvm_lapic_reset(vcpu, init_event);\n\n\tWARN_ON_ONCE(is_guest_mode(vcpu) || is_smm(vcpu));\n\tvcpu->arch.hflags = 0;\n\n\tvcpu->arch.smi_pending = 0;\n\tvcpu->arch.smi_count = 0;\n\tatomic_set(&vcpu->arch.nmi_queued, 0);\n\tvcpu->arch.nmi_pending = 0;\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_interrupt_queue(vcpu);\n\tkvm_clear_exception_queue(vcpu);\n\n\tmemset(vcpu->arch.db, 0, sizeof(vcpu->arch.db));\n\tkvm_update_dr0123(vcpu);\n\tvcpu->arch.dr6 = DR6_ACTIVE_LOW;\n\tvcpu->arch.dr7 = DR7_FIXED_1;\n\tkvm_update_dr7(vcpu);\n\n\tvcpu->arch.cr2 = 0;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\tvcpu->arch.apf.msr_en_val = 0;\n\tvcpu->arch.apf.msr_int_val = 0;\n\tvcpu->arch.st.msr_val = 0;\n\n\tkvmclock_reset(vcpu);\n\n\tkvm_clear_async_pf_completion_queue(vcpu);\n\tkvm_async_pf_hash_reset(vcpu);\n\tvcpu->arch.apf.halted = false;\n\n\tif (vcpu->arch.guest_fpu.fpstate && kvm_mpx_supported()) {\n\t\tstruct fpstate *fpstate = vcpu->arch.guest_fpu.fpstate;\n\n\t\t/*\n\t\t * All paths that lead to INIT are required to load the guest's\n\t\t * FPU state (because most paths are buried in KVM_RUN).\n\t\t */\n\t\tif (init_event)\n\t\t\tkvm_put_guest_fpu(vcpu);\n\n\t\tfpstate_clear_xstate_component(fpstate, XFEATURE_BNDREGS);\n\t\tfpstate_clear_xstate_component(fpstate, XFEATURE_BNDCSR);\n\n\t\tif (init_event)\n\t\t\tkvm_load_guest_fpu(vcpu);\n\t}\n\n\tif (!init_event) {\n\t\tkvm_pmu_reset(vcpu);\n\t\tvcpu->arch.smbase = 0x30000;\n\n\t\tvcpu->arch.msr_misc_features_enables = 0;\n\t\tvcpu->arch.ia32_misc_enable_msr = MSR_IA32_MISC_ENABLE_PEBS_UNAVAIL |\n\t\t\t\t\t\t  MSR_IA32_MISC_ENABLE_BTS_UNAVAIL;\n\n\t\t__kvm_set_xcr(vcpu, 0, XFEATURE_MASK_FP);\n\t\t__kvm_set_msr(vcpu, MSR_IA32_XSS, 0, true);\n\t}\n\n\t/* All GPRs except RDX (handled below) are zeroed on RESET/INIT. */\n\tmemset(vcpu->arch.regs, 0, sizeof(vcpu->arch.regs));\n\tkvm_register_mark_dirty(vcpu, VCPU_REGS_RSP);\n\n\t/*\n\t * Fall back to KVM's default Family/Model/Stepping of 0x600 (P6/Athlon)\n\t * if no CPUID match is found.  Note, it's impossible to get a match at\n\t * RESET since KVM emulates RESET before exposing the vCPU to userspace,\n\t * i.e. it's impossible for kvm_find_cpuid_entry() to find a valid entry\n\t * on RESET.  But, go through the motions in case that's ever remedied.\n\t */\n\tcpuid_0x1 = kvm_find_cpuid_entry(vcpu, 1);\n\tkvm_rdx_write(vcpu, cpuid_0x1 ? cpuid_0x1->eax : 0x600);\n\n\tstatic_call(kvm_x86_vcpu_reset)(vcpu, init_event);\n\n\tkvm_set_rflags(vcpu, X86_EFLAGS_FIXED);\n\tkvm_rip_write(vcpu, 0xfff0);\n\n\tvcpu->arch.cr3 = 0;\n\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_CR3);\n\n\t/*\n\t * CR0.CD/NW are set on RESET, preserved on INIT.  Note, some versions\n\t * of Intel's SDM list CD/NW as being set on INIT, but they contradict\n\t * (or qualify) that with a footnote stating that CD/NW are preserved.\n\t */\n\tnew_cr0 = X86_CR0_ET;\n\tif (init_event)\n\t\tnew_cr0 |= (old_cr0 & (X86_CR0_NW | X86_CR0_CD));\n\telse\n\t\tnew_cr0 |= X86_CR0_NW | X86_CR0_CD;\n\n\tstatic_call(kvm_x86_set_cr0)(vcpu, new_cr0);\n\tstatic_call(kvm_x86_set_cr4)(vcpu, 0);\n\tstatic_call(kvm_x86_set_efer)(vcpu, 0);\n\tstatic_call(kvm_x86_update_exception_bitmap)(vcpu);\n\n\t/*\n\t * On the standard CR0/CR4/EFER modification paths, there are several\n\t * complex conditions determining whether the MMU has to be reset and/or\n\t * which PCIDs have to be flushed.  However, CR0.WP and the paging-related\n\t * bits in CR4 and EFER are irrelevant if CR0.PG was '0'; and a reset+flush\n\t * is needed anyway if CR0.PG was '1' (which can only happen for INIT, as\n\t * CR0 will be '0' prior to RESET).  So we only need to check CR0.PG here.\n\t */\n\tif (old_cr0 & X86_CR0_PG) {\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t\tkvm_mmu_reset_context(vcpu);\n\t}\n\n\t/*\n\t * Intel's SDM states that all TLB entries are flushed on INIT.  AMD's\n\t * APM states the TLBs are untouched by INIT, but it also states that\n\t * the TLBs are flushed on \"External initialization of the processor.\"\n\t * Flush the guest TLB regardless of vendor, there is no meaningful\n\t * benefit in relying on the guest to flush the TLB immediately after\n\t * INIT.  A spurious TLB flush is benign and likely negligible from a\n\t * performance perspective.\n\t */\n\tif (init_event)\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_reset);\n\nvoid kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)\n{\n\tstruct kvm_segment cs;\n\n\tkvm_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\tcs.selector = vector << 8;\n\tcs.base = vector << 12;\n\tkvm_set_segment(vcpu, &cs, VCPU_SREG_CS);\n\tkvm_rip_write(vcpu, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_deliver_sipi_vector);\n\nint kvm_arch_hardware_enable(void)\n{\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\tint ret;\n\tu64 local_tsc;\n\tu64 max_tsc = 0;\n\tbool stable, backwards_tsc = false;\n\n\tkvm_user_return_msr_cpu_online();\n\tret = static_call(kvm_x86_hardware_enable)();\n\tif (ret != 0)\n\t\treturn ret;\n\n\tlocal_tsc = rdtsc();\n\tstable = !kvm_check_tsc_unstable();\n\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (!stable && vcpu->cpu == smp_processor_id())\n\t\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\t\tif (stable && vcpu->arch.last_host_tsc > local_tsc) {\n\t\t\t\tbackwards_tsc = true;\n\t\t\t\tif (vcpu->arch.last_host_tsc > max_tsc)\n\t\t\t\t\tmax_tsc = vcpu->arch.last_host_tsc;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Sometimes, even reliable TSCs go backwards.  This happens on\n\t * platforms that reset TSC during suspend or hibernate actions, but\n\t * maintain synchronization.  We must compensate.  Fortunately, we can\n\t * detect that condition here, which happens early in CPU bringup,\n\t * before any KVM threads can be running.  Unfortunately, we can't\n\t * bring the TSCs fully up to date with real time, as we aren't yet far\n\t * enough into CPU bringup that we know how much real time has actually\n\t * elapsed; our helper function, ktime_get_boottime_ns() will be using boot\n\t * variables that haven't been updated yet.\n\t *\n\t * So we simply find the maximum observed TSC above, then record the\n\t * adjustment to TSC in each VCPU.  When the VCPU later gets loaded,\n\t * the adjustment will be applied.  Note that we accumulate\n\t * adjustments, in case multiple suspend cycles happen before some VCPU\n\t * gets a chance to run again.  In the event that no KVM threads get a\n\t * chance to run, we will miss the entire elapsed period, as we'll have\n\t * reset last_host_tsc, so VCPUs will not have the TSC adjusted and may\n\t * loose cycle time.  This isn't too big a deal, since the loss will be\n\t * uniform across all VCPUs (not to mention the scenario is extremely\n\t * unlikely). It is possible that a second hibernate recovery happens\n\t * much faster than a first, causing the observed TSC here to be\n\t * smaller; this would require additional padding adjustment, which is\n\t * why we set last_host_tsc to the local tsc observed here.\n\t *\n\t * N.B. - this code below runs only on platforms with reliable TSC,\n\t * as that is the only way backwards_tsc is set above.  Also note\n\t * that this runs for ALL vcpus, which is not a bug; all VCPUs should\n\t * have the same delta_cyc adjustment applied if backwards_tsc\n\t * is detected.  Note further, this adjustment is only done once,\n\t * as we reset last_host_tsc on all VCPUs to stop this from being\n\t * called multiple times (one for each physical CPU bringup).\n\t *\n\t * Platforms with unreliable TSCs don't have to deal with this, they\n\t * will be compensated by the logic in vcpu_load, which sets the TSC to\n\t * catchup mode.  This will catchup all VCPUs to real time, but cannot\n\t * guarantee that they stay in perfect synchronization.\n\t */\n\tif (backwards_tsc) {\n\t\tu64 delta_cyc = max_tsc - local_tsc;\n\t\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\t\tkvm->arch.backwards_tsc_observed = true;\n\t\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\t\tvcpu->arch.tsc_offset_adjustment += delta_cyc;\n\t\t\t\tvcpu->arch.last_host_tsc = local_tsc;\n\t\t\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We have to disable TSC offset matching.. if you were\n\t\t\t * booting a VM while issuing an S4 host suspend....\n\t\t\t * you may have some problem.  Solving this issue is\n\t\t\t * left as an exercise to the reader.\n\t\t\t */\n\t\t\tkvm->arch.last_tsc_nsec = 0;\n\t\t\tkvm->arch.last_tsc_write = 0;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nvoid kvm_arch_hardware_disable(void)\n{\n\tstatic_call(kvm_x86_hardware_disable)();\n\tdrop_user_return_notifiers();\n}\n\nstatic inline void kvm_ops_update(struct kvm_x86_init_ops *ops)\n{\n\tmemcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));\n\n#define __KVM_X86_OP(func) \\\n\tstatic_call_update(kvm_x86_##func, kvm_x86_ops.func);\n#define KVM_X86_OP(func) \\\n\tWARN_ON(!kvm_x86_ops.func); __KVM_X86_OP(func)\n#define KVM_X86_OP_OPTIONAL __KVM_X86_OP\n#define KVM_X86_OP_OPTIONAL_RET0(func) \\\n\tstatic_call_update(kvm_x86_##func, (void *)kvm_x86_ops.func ? : \\\n\t\t\t\t\t   (void *)__static_call_return0);\n#include <asm/kvm-x86-ops.h>\n#undef __KVM_X86_OP\n\n\tkvm_pmu_ops_update(ops->pmu_ops);\n}\n\nint kvm_arch_hardware_setup(void *opaque)\n{\n\tstruct kvm_x86_init_ops *ops = opaque;\n\tint r;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES))\n\t\trdmsrl(MSR_IA32_XSS, host_xss);\n\n\tkvm_init_pmu_capability();\n\n\tr = ops->hardware_setup();\n\tif (r != 0)\n\t\treturn r;\n\n\tkvm_ops_update(ops);\n\n\tkvm_register_perf_callbacks(ops->handle_intel_pt_intr);\n\n\tif (!kvm_cpu_cap_has(X86_FEATURE_XSAVES))\n\t\tkvm_caps.supported_xss = 0;\n\n#define __kvm_cpu_cap_has(UNUSED_, f) kvm_cpu_cap_has(f)\n\tcr4_reserved_bits = __cr4_reserved_bits(__kvm_cpu_cap_has, UNUSED_);\n#undef __kvm_cpu_cap_has\n\n\tif (kvm_caps.has_tsc_control) {\n\t\t/*\n\t\t * Make sure the user can only configure tsc_khz values that\n\t\t * fit into a signed integer.\n\t\t * A min value is not calculated because it will always\n\t\t * be 1 on all machines.\n\t\t */\n\t\tu64 max = min(0x7fffffffULL,\n\t\t\t      __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));\n\t\tkvm_caps.max_guest_tsc_khz = max;\n\t}\n\tkvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;\n\tkvm_init_msr_list();\n\treturn 0;\n}\n\nvoid kvm_arch_hardware_unsetup(void)\n{\n\tkvm_unregister_perf_callbacks();\n\n\tstatic_call(kvm_x86_hardware_unsetup)();\n}\n\nint kvm_arch_check_processor_compat(void *opaque)\n{\n\tstruct cpuinfo_x86 *c = &cpu_data(smp_processor_id());\n\tstruct kvm_x86_init_ops *ops = opaque;\n\n\tWARN_ON(!irqs_disabled());\n\n\tif (__cr4_reserved_bits(cpu_has, c) !=\n\t    __cr4_reserved_bits(cpu_has, &boot_cpu_data))\n\t\treturn -EIO;\n\n\treturn ops->check_processor_compatibility();\n}\n\nbool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->kvm->arch.bsp_vcpu_id == vcpu->vcpu_id;\n}\n\nbool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)\n{\n\treturn (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;\n}\n\n__read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);\nEXPORT_SYMBOL_GPL(kvm_has_noapic_vcpu);\n\nvoid kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\tif (pmu->version && unlikely(pmu->event_count)) {\n\t\tpmu->need_cleanup = true;\n\t\tkvm_make_request(KVM_REQ_PMU, vcpu);\n\t}\n\tstatic_call(kvm_x86_sched_in)(vcpu, cpu);\n}\n\nvoid kvm_arch_free_vm(struct kvm *kvm)\n{\n\tkfree(to_kvm_hv(kvm)->hv_pa_pg);\n\t__kvm_arch_free_vm(kvm);\n}\n\n\nint kvm_arch_init_vm(struct kvm *kvm, unsigned long type)\n{\n\tint ret;\n\tunsigned long flags;\n\n\tif (type)\n\t\treturn -EINVAL;\n\n\tret = kvm_page_track_init(kvm);\n\tif (ret)\n\t\tgoto out;\n\n\tret = kvm_mmu_init_vm(kvm);\n\tif (ret)\n\t\tgoto out_page_track;\n\n\tret = static_call(kvm_x86_vm_init)(kvm);\n\tif (ret)\n\t\tgoto out_uninit_mmu;\n\n\tINIT_HLIST_HEAD(&kvm->arch.mask_notifier_list);\n\tINIT_LIST_HEAD(&kvm->arch.assigned_dev_head);\n\tatomic_set(&kvm->arch.noncoherent_dma_count, 0);\n\n\t/* Reserve bit 0 of irq_sources_bitmap for userspace irq source */\n\tset_bit(KVM_USERSPACE_IRQ_SOURCE_ID, &kvm->arch.irq_sources_bitmap);\n\t/* Reserve bit 1 of irq_sources_bitmap for irqfd-resampler */\n\tset_bit(KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t&kvm->arch.irq_sources_bitmap);\n\n\traw_spin_lock_init(&kvm->arch.tsc_write_lock);\n\tmutex_init(&kvm->arch.apic_map_lock);\n\tseqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);\n\tkvm->arch.kvmclock_offset = -get_kvmclock_base_ns();\n\n\traw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);\n\tpvclock_update_vm_gtod_copy(kvm);\n\traw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);\n\n\tkvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;\n\tkvm->arch.guest_can_read_msr_platform_info = true;\n\tkvm->arch.enable_pmu = enable_pmu;\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\tspin_lock_init(&kvm->arch.hv_root_tdp_lock);\n\tkvm->arch.hv_root_tdp = INVALID_PAGE;\n#endif\n\n\tINIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);\n\tINIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);\n\n\tkvm_apicv_init(kvm);\n\tkvm_hv_init_vm(kvm);\n\tkvm_xen_init_vm(kvm);\n\n\treturn 0;\n\nout_uninit_mmu:\n\tkvm_mmu_uninit_vm(kvm);\nout_page_track:\n\tkvm_page_track_cleanup(kvm);\nout:\n\treturn ret;\n}\n\nint kvm_arch_post_init_vm(struct kvm *kvm)\n{\n\treturn kvm_mmu_post_init_vm(kvm);\n}\n\nstatic void kvm_unload_vcpu_mmu(struct kvm_vcpu *vcpu)\n{\n\tvcpu_load(vcpu);\n\tkvm_mmu_unload(vcpu);\n\tvcpu_put(vcpu);\n}\n\nstatic void kvm_unload_vcpu_mmus(struct kvm *kvm)\n{\n\tunsigned long i;\n\tstruct kvm_vcpu *vcpu;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tkvm_clear_async_pf_completion_queue(vcpu);\n\t\tkvm_unload_vcpu_mmu(vcpu);\n\t}\n}\n\nvoid kvm_arch_sync_events(struct kvm *kvm)\n{\n\tcancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);\n\tcancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);\n\tkvm_free_pit(kvm);\n}\n\n/**\n * __x86_set_memory_region: Setup KVM internal memory slot\n *\n * @kvm: the kvm pointer to the VM.\n * @id: the slot ID to setup.\n * @gpa: the GPA to install the slot (unused when @size == 0).\n * @size: the size of the slot. Set to zero to uninstall a slot.\n *\n * This function helps to setup a KVM internal memory slot.  Specify\n * @size > 0 to install a new slot, while @size == 0 to uninstall a\n * slot.  The return code can be one of the following:\n *\n *   HVA:           on success (uninstall will return a bogus HVA)\n *   -errno:        on error\n *\n * The caller should always use IS_ERR() to check the return value\n * before use.  Note, the KVM internal memory slots are guaranteed to\n * remain valid and unchanged until the VM is destroyed, i.e., the\n * GPA->HVA translation will not change.  However, the HVA is a user\n * address, i.e. its accessibility is not guaranteed, and must be\n * accessed via __copy_{to,from}_user().\n */\nvoid __user * __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa,\n\t\t\t\t      u32 size)\n{\n\tint i, r;\n\tunsigned long hva, old_npages;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tstruct kvm_memory_slot *slot;\n\n\t/* Called with kvm->slots_lock held.  */\n\tif (WARN_ON(id >= KVM_MEM_SLOTS_NUM))\n\t\treturn ERR_PTR_USR(-EINVAL);\n\n\tslot = id_to_memslot(slots, id);\n\tif (size) {\n\t\tif (slot && slot->npages)\n\t\t\treturn ERR_PTR_USR(-EEXIST);\n\n\t\t/*\n\t\t * MAP_SHARED to prevent internal slot pages from being moved\n\t\t * by fork()/COW.\n\t\t */\n\t\thva = vm_mmap(NULL, 0, size, PROT_READ | PROT_WRITE,\n\t\t\t      MAP_SHARED | MAP_ANONYMOUS, 0);\n\t\tif (IS_ERR((void *)hva))\n\t\t\treturn (void __user *)hva;\n\t} else {\n\t\tif (!slot || !slot->npages)\n\t\t\treturn NULL;\n\n\t\told_npages = slot->npages;\n\t\thva = slot->userspace_addr;\n\t}\n\n\tfor (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {\n\t\tstruct kvm_userspace_memory_region m;\n\n\t\tm.slot = id | (i << 16);\n\t\tm.flags = 0;\n\t\tm.guest_phys_addr = gpa;\n\t\tm.userspace_addr = hva;\n\t\tm.memory_size = size;\n\t\tr = __kvm_set_memory_region(kvm, &m);\n\t\tif (r < 0)\n\t\t\treturn ERR_PTR_USR(r);\n\t}\n\n\tif (!size)\n\t\tvm_munmap(hva, old_npages * PAGE_SIZE);\n\n\treturn (void __user *)hva;\n}\nEXPORT_SYMBOL_GPL(__x86_set_memory_region);\n\nvoid kvm_arch_pre_destroy_vm(struct kvm *kvm)\n{\n\tkvm_mmu_pre_destroy_vm(kvm);\n}\n\nvoid kvm_arch_destroy_vm(struct kvm *kvm)\n{\n\tif (current->mm == kvm->mm) {\n\t\t/*\n\t\t * Free memory regions allocated on behalf of userspace,\n\t\t * unless the memory map has changed due to process exit\n\t\t * or fd copying.\n\t\t */\n\t\tmutex_lock(&kvm->slots_lock);\n\t\t__x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,\n\t\t\t\t\t0, 0);\n\t\t__x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,\n\t\t\t\t\t0, 0);\n\t\t__x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);\n\t\tmutex_unlock(&kvm->slots_lock);\n\t}\n\tkvm_unload_vcpu_mmus(kvm);\n\tstatic_call_cond(kvm_x86_vm_destroy)(kvm);\n\tkvm_free_msr_filter(srcu_dereference_check(kvm->arch.msr_filter, &kvm->srcu, 1));\n\tkvm_pic_destroy(kvm);\n\tkvm_ioapic_destroy(kvm);\n\tkvm_destroy_vcpus(kvm);\n\tkvfree(rcu_dereference_check(kvm->arch.apic_map, 1));\n\tkfree(srcu_dereference_check(kvm->arch.pmu_event_filter, &kvm->srcu, 1));\n\tkvm_mmu_uninit_vm(kvm);\n\tkvm_page_track_cleanup(kvm);\n\tkvm_xen_destroy_vm(kvm);\n\tkvm_hv_destroy_vm(kvm);\n}\n\nstatic void memslot_rmap_free(struct kvm_memory_slot *slot)\n{\n\tint i;\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tkvfree(slot->arch.rmap[i]);\n\t\tslot->arch.rmap[i] = NULL;\n\t}\n}\n\nvoid kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *slot)\n{\n\tint i;\n\n\tmemslot_rmap_free(slot);\n\n\tfor (i = 1; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tkvfree(slot->arch.lpage_info[i - 1]);\n\t\tslot->arch.lpage_info[i - 1] = NULL;\n\t}\n\n\tkvm_page_track_free_memslot(slot);\n}\n\nint memslot_rmap_alloc(struct kvm_memory_slot *slot, unsigned long npages)\n{\n\tconst int sz = sizeof(*slot->arch.rmap[0]);\n\tint i;\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tint level = i + 1;\n\t\tint lpages = __kvm_mmu_slot_lpages(slot, npages, level);\n\n\t\tif (slot->arch.rmap[i])\n\t\t\tcontinue;\n\n\t\tslot->arch.rmap[i] = __vcalloc(lpages, sz, GFP_KERNEL_ACCOUNT);\n\t\tif (!slot->arch.rmap[i]) {\n\t\t\tmemslot_rmap_free(slot);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int kvm_alloc_memslot_metadata(struct kvm *kvm,\n\t\t\t\t      struct kvm_memory_slot *slot)\n{\n\tunsigned long npages = slot->npages;\n\tint i, r;\n\n\t/*\n\t * Clear out the previous array pointers for the KVM_MR_MOVE case.  The\n\t * old arrays will be freed by __kvm_set_memory_region() if installing\n\t * the new memslot is successful.\n\t */\n\tmemset(&slot->arch, 0, sizeof(slot->arch));\n\n\tif (kvm_memslots_have_rmaps(kvm)) {\n\t\tr = memslot_rmap_alloc(slot, npages);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 1; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tstruct kvm_lpage_info *linfo;\n\t\tunsigned long ugfn;\n\t\tint lpages;\n\t\tint level = i + 1;\n\n\t\tlpages = __kvm_mmu_slot_lpages(slot, npages, level);\n\n\t\tlinfo = __vcalloc(lpages, sizeof(*linfo), GFP_KERNEL_ACCOUNT);\n\t\tif (!linfo)\n\t\t\tgoto out_free;\n\n\t\tslot->arch.lpage_info[i - 1] = linfo;\n\n\t\tif (slot->base_gfn & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tlinfo[0].disallow_lpage = 1;\n\t\tif ((slot->base_gfn + npages) & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tlinfo[lpages - 1].disallow_lpage = 1;\n\t\tugfn = slot->userspace_addr >> PAGE_SHIFT;\n\t\t/*\n\t\t * If the gfn and userspace address are not aligned wrt each\n\t\t * other, disable large page support for this slot.\n\t\t */\n\t\tif ((slot->base_gfn ^ ugfn) & (KVM_PAGES_PER_HPAGE(level) - 1)) {\n\t\t\tunsigned long j;\n\n\t\t\tfor (j = 0; j < lpages; ++j)\n\t\t\t\tlinfo[j].disallow_lpage = 1;\n\t\t}\n\t}\n\n\tif (kvm_page_track_create_memslot(kvm, slot, npages))\n\t\tgoto out_free;\n\n\treturn 0;\n\nout_free:\n\tmemslot_rmap_free(slot);\n\n\tfor (i = 1; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tkvfree(slot->arch.lpage_info[i - 1]);\n\t\tslot->arch.lpage_info[i - 1] = NULL;\n\t}\n\treturn -ENOMEM;\n}\n\nvoid kvm_arch_memslots_updated(struct kvm *kvm, u64 gen)\n{\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\n\t/*\n\t * memslots->generation has been incremented.\n\t * mmio generation may have reached its maximum value.\n\t */\n\tkvm_mmu_invalidate_mmio_sptes(kvm, gen);\n\n\t/* Force re-initialization of steal_time cache */\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\n\t\t\t\t   const struct kvm_memory_slot *old,\n\t\t\t\t   struct kvm_memory_slot *new,\n\t\t\t\t   enum kvm_mr_change change)\n{\n\tif (change == KVM_MR_CREATE || change == KVM_MR_MOVE) {\n\t\tif ((new->base_gfn + new->npages - 1) > kvm_mmu_max_gfn())\n\t\t\treturn -EINVAL;\n\n\t\treturn kvm_alloc_memslot_metadata(kvm, new);\n\t}\n\n\tif (change == KVM_MR_FLAGS_ONLY)\n\t\tmemcpy(&new->arch, &old->arch, sizeof(old->arch));\n\telse if (WARN_ON_ONCE(change != KVM_MR_DELETE))\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\n\nstatic void kvm_mmu_update_cpu_dirty_logging(struct kvm *kvm, bool enable)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\n\tif (!kvm_x86_ops.cpu_dirty_log_size)\n\t\treturn;\n\n\tif ((enable && ++ka->cpu_dirty_logging_count == 1) ||\n\t    (!enable && --ka->cpu_dirty_logging_count == 0))\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_UPDATE_CPU_DIRTY_LOGGING);\n\n\tWARN_ON_ONCE(ka->cpu_dirty_logging_count < 0);\n}\n\nstatic void kvm_mmu_slot_apply_flags(struct kvm *kvm,\n\t\t\t\t     struct kvm_memory_slot *old,\n\t\t\t\t     const struct kvm_memory_slot *new,\n\t\t\t\t     enum kvm_mr_change change)\n{\n\tu32 old_flags = old ? old->flags : 0;\n\tu32 new_flags = new ? new->flags : 0;\n\tbool log_dirty_pages = new_flags & KVM_MEM_LOG_DIRTY_PAGES;\n\n\t/*\n\t * Update CPU dirty logging if dirty logging is being toggled.  This\n\t * applies to all operations.\n\t */\n\tif ((old_flags ^ new_flags) & KVM_MEM_LOG_DIRTY_PAGES)\n\t\tkvm_mmu_update_cpu_dirty_logging(kvm, log_dirty_pages);\n\n\t/*\n\t * Nothing more to do for RO slots (which can't be dirtied and can't be\n\t * made writable) or CREATE/MOVE/DELETE of a slot.\n\t *\n\t * For a memslot with dirty logging disabled:\n\t * CREATE:      No dirty mappings will already exist.\n\t * MOVE/DELETE: The old mappings will already have been cleaned up by\n\t *\t\tkvm_arch_flush_shadow_memslot()\n\t *\n\t * For a memslot with dirty logging enabled:\n\t * CREATE:      No shadow pages exist, thus nothing to write-protect\n\t *\t\tand no dirty bits to clear.\n\t * MOVE/DELETE: The old mappings will already have been cleaned up by\n\t *\t\tkvm_arch_flush_shadow_memslot().\n\t */\n\tif ((change != KVM_MR_FLAGS_ONLY) || (new_flags & KVM_MEM_READONLY))\n\t\treturn;\n\n\t/*\n\t * READONLY and non-flags changes were filtered out above, and the only\n\t * other flag is LOG_DIRTY_PAGES, i.e. something is wrong if dirty\n\t * logging isn't being toggled on or off.\n\t */\n\tif (WARN_ON_ONCE(!((old_flags ^ new_flags) & KVM_MEM_LOG_DIRTY_PAGES)))\n\t\treturn;\n\n\tif (!log_dirty_pages) {\n\t\t/*\n\t\t * Dirty logging tracks sptes in 4k granularity, meaning that\n\t\t * large sptes have to be split.  If live migration succeeds,\n\t\t * the guest in the source machine will be destroyed and large\n\t\t * sptes will be created in the destination.  However, if the\n\t\t * guest continues to run in the source machine (for example if\n\t\t * live migration fails), small sptes will remain around and\n\t\t * cause bad performance.\n\t\t *\n\t\t * Scan sptes if dirty logging has been stopped, dropping those\n\t\t * which can be collapsed into a single large-page spte.  Later\n\t\t * page faults will create the large-page sptes.\n\t\t */\n\t\tkvm_mmu_zap_collapsible_sptes(kvm, new);\n\t} else {\n\t\t/*\n\t\t * Initially-all-set does not require write protecting any page,\n\t\t * because they're all assumed to be dirty.\n\t\t */\n\t\tif (kvm_dirty_log_manual_protect_and_init_set(kvm))\n\t\t\treturn;\n\n\t\tif (READ_ONCE(eager_page_split))\n\t\t\tkvm_mmu_slot_try_split_huge_pages(kvm, new, PG_LEVEL_4K);\n\n\t\tif (kvm_x86_ops.cpu_dirty_log_size) {\n\t\t\tkvm_mmu_slot_leaf_clear_dirty(kvm, new);\n\t\t\tkvm_mmu_slot_remove_write_access(kvm, new, PG_LEVEL_2M);\n\t\t} else {\n\t\t\tkvm_mmu_slot_remove_write_access(kvm, new, PG_LEVEL_4K);\n\t\t}\n\n\t\t/*\n\t\t * Unconditionally flush the TLBs after enabling dirty logging.\n\t\t * A flush is almost always going to be necessary (see below),\n\t\t * and unconditionally flushing allows the helpers to omit\n\t\t * the subtly complex checks when removing write access.\n\t\t *\n\t\t * Do the flush outside of mmu_lock to reduce the amount of\n\t\t * time mmu_lock is held.  Flushing after dropping mmu_lock is\n\t\t * safe as KVM only needs to guarantee the slot is fully\n\t\t * write-protected before returning to userspace, i.e. before\n\t\t * userspace can consume the dirty status.\n\t\t *\n\t\t * Flushing outside of mmu_lock requires KVM to be careful when\n\t\t * making decisions based on writable status of an SPTE, e.g. a\n\t\t * !writable SPTE doesn't guarantee a CPU can't perform writes.\n\t\t *\n\t\t * Specifically, KVM also write-protects guest page tables to\n\t\t * monitor changes when using shadow paging, and must guarantee\n\t\t * no CPUs can write to those page before mmu_lock is dropped.\n\t\t * Because CPUs may have stale TLB entries at this point, a\n\t\t * !writable SPTE doesn't guarantee CPUs can't perform writes.\n\t\t *\n\t\t * KVM also allows making SPTES writable outside of mmu_lock,\n\t\t * e.g. to allow dirty logging without taking mmu_lock.\n\t\t *\n\t\t * To handle these scenarios, KVM uses a separate software-only\n\t\t * bit (MMU-writable) to track if a SPTE is !writable due to\n\t\t * a guest page table being write-protected (KVM clears the\n\t\t * MMU-writable flag when write-protecting for shadow paging).\n\t\t *\n\t\t * The use of MMU-writable is also the primary motivation for\n\t\t * the unconditional flush.  Because KVM must guarantee that a\n\t\t * CPU doesn't contain stale, writable TLB entries for a\n\t\t * !MMU-writable SPTE, KVM must flush if it encounters any\n\t\t * MMU-writable SPTE regardless of whether the actual hardware\n\t\t * writable bit was set.  I.e. KVM is almost guaranteed to need\n\t\t * to flush, while unconditionally flushing allows the \"remove\n\t\t * write access\" helpers to ignore MMU-writable entirely.\n\t\t *\n\t\t * See is_writable_pte() for more details (the case involving\n\t\t * access-tracked SPTEs is particularly relevant).\n\t\t */\n\t\tkvm_arch_flush_remote_tlbs_memslot(kvm, new);\n\t}\n}\n\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\n\t\t\t\tstruct kvm_memory_slot *old,\n\t\t\t\tconst struct kvm_memory_slot *new,\n\t\t\t\tenum kvm_mr_change change)\n{\n\tif (!kvm->arch.n_requested_mmu_pages &&\n\t    (change == KVM_MR_CREATE || change == KVM_MR_DELETE)) {\n\t\tunsigned long nr_mmu_pages;\n\n\t\tnr_mmu_pages = kvm->nr_memslot_pages / KVM_MEMSLOT_PAGES_TO_MMU_PAGES_RATIO;\n\t\tnr_mmu_pages = max(nr_mmu_pages, KVM_MIN_ALLOC_MMU_PAGES);\n\t\tkvm_mmu_change_mmu_pages(kvm, nr_mmu_pages);\n\t}\n\n\tkvm_mmu_slot_apply_flags(kvm, old, new, change);\n\n\t/* Free the arrays associated with the old memslot. */\n\tif (change == KVM_MR_MOVE)\n\t\tkvm_arch_free_memslot(kvm, old);\n}\n\nvoid kvm_arch_flush_shadow_all(struct kvm *kvm)\n{\n\tkvm_mmu_zap_all(kvm);\n}\n\nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot)\n{\n\tkvm_page_track_flush_slot(kvm, slot);\n}\n\nstatic inline bool kvm_guest_apic_has_interrupt(struct kvm_vcpu *vcpu)\n{\n\treturn (is_guest_mode(vcpu) &&\n\t\tstatic_call(kvm_x86_guest_apic_has_interrupt)(vcpu));\n}\n\nstatic inline bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)\n{\n\tif (!list_empty_careful(&vcpu->async_pf.done))\n\t\treturn true;\n\n\tif (kvm_apic_has_pending_init_or_sipi(vcpu) &&\n\t    kvm_apic_init_sipi_allowed(vcpu))\n\t\treturn true;\n\n\tif (vcpu->arch.pv.pv_unhalted)\n\t\treturn true;\n\n\tif (kvm_is_exception_pending(vcpu))\n\t\treturn true;\n\n\tif (kvm_test_request(KVM_REQ_NMI, vcpu) ||\n\t    (vcpu->arch.nmi_pending &&\n\t     static_call(kvm_x86_nmi_allowed)(vcpu, false)))\n\t\treturn true;\n\n#ifdef CONFIG_KVM_SMM\n\tif (kvm_test_request(KVM_REQ_SMI, vcpu) ||\n\t    (vcpu->arch.smi_pending &&\n\t     static_call(kvm_x86_smi_allowed)(vcpu, false)))\n\t\treturn true;\n#endif\n\n\tif (kvm_arch_interrupt_allowed(vcpu) &&\n\t    (kvm_cpu_has_interrupt(vcpu) ||\n\t    kvm_guest_apic_has_interrupt(vcpu)))\n\t\treturn true;\n\n\tif (kvm_hv_has_stimer_pending(vcpu))\n\t\treturn true;\n\n\tif (is_guest_mode(vcpu) &&\n\t    kvm_x86_ops.nested_ops->has_events &&\n\t    kvm_x86_ops.nested_ops->has_events(vcpu))\n\t\treturn true;\n\n\tif (kvm_xen_has_pending_events(vcpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nint kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_vcpu_running(vcpu) || kvm_vcpu_has_events(vcpu);\n}\n\nbool kvm_arch_dy_has_pending_interrupt(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_vcpu_apicv_active(vcpu) &&\n\t    static_call(kvm_x86_dy_apicv_has_pending_interrupt)(vcpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nbool kvm_arch_dy_runnable(struct kvm_vcpu *vcpu)\n{\n\tif (READ_ONCE(vcpu->arch.pv.pv_unhalted))\n\t\treturn true;\n\n\tif (kvm_test_request(KVM_REQ_NMI, vcpu) ||\n#ifdef CONFIG_KVM_SMM\n\t\tkvm_test_request(KVM_REQ_SMI, vcpu) ||\n#endif\n\t\t kvm_test_request(KVM_REQ_EVENT, vcpu))\n\t\treturn true;\n\n\treturn kvm_arch_dy_has_pending_interrupt(vcpu);\n}\n\nbool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn true;\n\n\treturn vcpu->arch.preempted_in_kernel;\n}\n\nunsigned long kvm_arch_vcpu_get_ip(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_rip_read(vcpu);\n}\n\nint kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;\n}\n\nint kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu)\n{\n\treturn static_call(kvm_x86_interrupt_allowed)(vcpu, false);\n}\n\nunsigned long kvm_get_linear_rip(struct kvm_vcpu *vcpu)\n{\n\t/* Can't read the RIP when guest state is protected, just return 0 */\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn 0;\n\n\tif (is_64_bit_mode(vcpu))\n\t\treturn kvm_rip_read(vcpu);\n\treturn (u32)(get_segment_base(vcpu, VCPU_SREG_CS) +\n\t\t     kvm_rip_read(vcpu));\n}\nEXPORT_SYMBOL_GPL(kvm_get_linear_rip);\n\nbool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip)\n{\n\treturn kvm_get_linear_rip(vcpu) == linear_rip;\n}\nEXPORT_SYMBOL_GPL(kvm_is_linear_rip);\n\nunsigned long kvm_get_rflags(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rflags;\n\n\trflags = static_call(kvm_x86_get_rflags)(vcpu);\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\trflags &= ~X86_EFLAGS_TF;\n\treturn rflags;\n}\nEXPORT_SYMBOL_GPL(kvm_get_rflags);\n\nstatic void __kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\n{\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP &&\n\t    kvm_is_linear_rip(vcpu, vcpu->arch.singlestep_rip))\n\t\trflags |= X86_EFLAGS_TF;\n\tstatic_call(kvm_x86_set_rflags)(vcpu, rflags);\n}\n\nvoid kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\n{\n\t__kvm_set_rflags(vcpu, rflags);\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_set_rflags);\n\nstatic inline u32 kvm_async_pf_hash_fn(gfn_t gfn)\n{\n\tBUILD_BUG_ON(!is_power_of_2(ASYNC_PF_PER_VCPU));\n\n\treturn hash_32(gfn & 0xffffffff, order_base_2(ASYNC_PF_PER_VCPU));\n}\n\nstatic inline u32 kvm_async_pf_next_probe(u32 key)\n{\n\treturn (key + 1) & (ASYNC_PF_PER_VCPU - 1);\n}\n\nstatic void kvm_add_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\tu32 key = kvm_async_pf_hash_fn(gfn);\n\n\twhile (vcpu->arch.apf.gfns[key] != ~0)\n\t\tkey = kvm_async_pf_next_probe(key);\n\n\tvcpu->arch.apf.gfns[key] = gfn;\n}\n\nstatic u32 kvm_async_pf_gfn_slot(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\tint i;\n\tu32 key = kvm_async_pf_hash_fn(gfn);\n\n\tfor (i = 0; i < ASYNC_PF_PER_VCPU &&\n\t\t     (vcpu->arch.apf.gfns[key] != gfn &&\n\t\t      vcpu->arch.apf.gfns[key] != ~0); i++)\n\t\tkey = kvm_async_pf_next_probe(key);\n\n\treturn key;\n}\n\nbool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\treturn vcpu->arch.apf.gfns[kvm_async_pf_gfn_slot(vcpu, gfn)] == gfn;\n}\n\nstatic void kvm_del_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\tu32 i, j, k;\n\n\ti = j = kvm_async_pf_gfn_slot(vcpu, gfn);\n\n\tif (WARN_ON_ONCE(vcpu->arch.apf.gfns[i] != gfn))\n\t\treturn;\n\n\twhile (true) {\n\t\tvcpu->arch.apf.gfns[i] = ~0;\n\t\tdo {\n\t\t\tj = kvm_async_pf_next_probe(j);\n\t\t\tif (vcpu->arch.apf.gfns[j] == ~0)\n\t\t\t\treturn;\n\t\t\tk = kvm_async_pf_hash_fn(vcpu->arch.apf.gfns[j]);\n\t\t\t/*\n\t\t\t * k lies cyclically in ]i,j]\n\t\t\t * |    i.k.j |\n\t\t\t * |....j i.k.| or  |.k..j i...|\n\t\t\t */\n\t\t} while ((i <= j) ? (i < k && k <= j) : (i < k || k <= j));\n\t\tvcpu->arch.apf.gfns[i] = vcpu->arch.apf.gfns[j];\n\t\ti = j;\n\t}\n}\n\nstatic inline int apf_put_user_notpresent(struct kvm_vcpu *vcpu)\n{\n\tu32 reason = KVM_PV_REASON_PAGE_NOT_PRESENT;\n\n\treturn kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apf.data, &reason,\n\t\t\t\t      sizeof(reason));\n}\n\nstatic inline int apf_put_user_ready(struct kvm_vcpu *vcpu, u32 token)\n{\n\tunsigned int offset = offsetof(struct kvm_vcpu_pv_apf_data, token);\n\n\treturn kvm_write_guest_offset_cached(vcpu->kvm, &vcpu->arch.apf.data,\n\t\t\t\t\t     &token, offset, sizeof(token));\n}\n\nstatic inline bool apf_pageready_slot_free(struct kvm_vcpu *vcpu)\n{\n\tunsigned int offset = offsetof(struct kvm_vcpu_pv_apf_data, token);\n\tu32 val;\n\n\tif (kvm_read_guest_offset_cached(vcpu->kvm, &vcpu->arch.apf.data,\n\t\t\t\t\t &val, offset, sizeof(val)))\n\t\treturn false;\n\n\treturn !val;\n}\n\nstatic bool kvm_can_deliver_async_pf(struct kvm_vcpu *vcpu)\n{\n\n\tif (!kvm_pv_async_pf_enabled(vcpu))\n\t\treturn false;\n\n\tif (vcpu->arch.apf.send_user_only &&\n\t    static_call(kvm_x86_get_cpl)(vcpu) == 0)\n\t\treturn false;\n\n\tif (is_guest_mode(vcpu)) {\n\t\t/*\n\t\t * L1 needs to opt into the special #PF vmexits that are\n\t\t * used to deliver async page faults.\n\t\t */\n\t\treturn vcpu->arch.apf.delivery_as_pf_vmexit;\n\t} else {\n\t\t/*\n\t\t * Play it safe in case the guest temporarily disables paging.\n\t\t * The real mode IDT in particular is unlikely to have a #PF\n\t\t * exception setup.\n\t\t */\n\t\treturn is_paging(vcpu);\n\t}\n}\n\nbool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)\n{\n\tif (unlikely(!lapic_in_kernel(vcpu) ||\n\t\t     kvm_event_needs_reinjection(vcpu) ||\n\t\t     kvm_is_exception_pending(vcpu)))\n\t\treturn false;\n\n\tif (kvm_hlt_in_guest(vcpu->kvm) && !kvm_can_deliver_async_pf(vcpu))\n\t\treturn false;\n\n\t/*\n\t * If interrupts are off we cannot even use an artificial\n\t * halt state.\n\t */\n\treturn kvm_arch_interrupt_allowed(vcpu);\n}\n\nbool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_async_pf *work)\n{\n\tstruct x86_exception fault;\n\n\ttrace_kvm_async_pf_not_present(work->arch.token, work->cr2_or_gpa);\n\tkvm_add_async_pf_gfn(vcpu, work->arch.gfn);\n\n\tif (kvm_can_deliver_async_pf(vcpu) &&\n\t    !apf_put_user_notpresent(vcpu)) {\n\t\tfault.vector = PF_VECTOR;\n\t\tfault.error_code_valid = true;\n\t\tfault.error_code = 0;\n\t\tfault.nested_page_fault = false;\n\t\tfault.address = work->arch.token;\n\t\tfault.async_page_fault = true;\n\t\tkvm_inject_page_fault(vcpu, &fault);\n\t\treturn true;\n\t} else {\n\t\t/*\n\t\t * It is not possible to deliver a paravirtualized asynchronous\n\t\t * page fault, but putting the guest in an artificial halt state\n\t\t * can be beneficial nevertheless: if an interrupt arrives, we\n\t\t * can deliver it timely and perhaps the guest will schedule\n\t\t * another process.  When the instruction that triggered a page\n\t\t * fault is retried, hopefully the page will be ready in the host.\n\t\t */\n\t\tkvm_make_request(KVM_REQ_APF_HALT, vcpu);\n\t\treturn false;\n\t}\n}\n\nvoid kvm_arch_async_page_present(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_async_pf *work)\n{\n\tstruct kvm_lapic_irq irq = {\n\t\t.delivery_mode = APIC_DM_FIXED,\n\t\t.vector = vcpu->arch.apf.vec\n\t};\n\n\tif (work->wakeup_all)\n\t\twork->arch.token = ~0; /* broadcast wakeup */\n\telse\n\t\tkvm_del_async_pf_gfn(vcpu, work->arch.gfn);\n\ttrace_kvm_async_pf_ready(work->arch.token, work->cr2_or_gpa);\n\n\tif ((work->wakeup_all || work->notpresent_injected) &&\n\t    kvm_pv_async_pf_enabled(vcpu) &&\n\t    !apf_put_user_ready(vcpu, work->arch.token)) {\n\t\tvcpu->arch.apf.pageready_pending = true;\n\t\tkvm_apic_set_irq(vcpu, &irq, NULL);\n\t}\n\n\tvcpu->arch.apf.halted = false;\n\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n}\n\nvoid kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu)\n{\n\tkvm_make_request(KVM_REQ_APF_READY, vcpu);\n\tif (!vcpu->arch.apf.pageready_pending)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nbool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_pv_async_pf_enabled(vcpu))\n\t\treturn true;\n\telse\n\t\treturn kvm_lapic_enabled(vcpu) && apf_pageready_slot_free(vcpu);\n}\n\nvoid kvm_arch_start_assignment(struct kvm *kvm)\n{\n\tif (atomic_inc_return(&kvm->arch.assigned_device_count) == 1)\n\t\tstatic_call_cond(kvm_x86_pi_start_assignment)(kvm);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_start_assignment);\n\nvoid kvm_arch_end_assignment(struct kvm *kvm)\n{\n\tatomic_dec(&kvm->arch.assigned_device_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_end_assignment);\n\nbool noinstr kvm_arch_has_assigned_device(struct kvm *kvm)\n{\n\treturn arch_atomic_read(&kvm->arch.assigned_device_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_has_assigned_device);\n\nvoid kvm_arch_register_noncoherent_dma(struct kvm *kvm)\n{\n\tatomic_inc(&kvm->arch.noncoherent_dma_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_register_noncoherent_dma);\n\nvoid kvm_arch_unregister_noncoherent_dma(struct kvm *kvm)\n{\n\tatomic_dec(&kvm->arch.noncoherent_dma_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_unregister_noncoherent_dma);\n\nbool kvm_arch_has_noncoherent_dma(struct kvm *kvm)\n{\n\treturn atomic_read(&kvm->arch.noncoherent_dma_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_has_noncoherent_dma);\n\nbool kvm_arch_has_irq_bypass(void)\n{\n\treturn true;\n}\n\nint kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,\n\t\t\t\t      struct irq_bypass_producer *prod)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(cons, struct kvm_kernel_irqfd, consumer);\n\tint ret;\n\n\tirqfd->producer = prod;\n\tkvm_arch_start_assignment(irqfd->kvm);\n\tret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm,\n\t\t\t\t\t prod->irq, irqfd->gsi, 1);\n\n\tif (ret)\n\t\tkvm_arch_end_assignment(irqfd->kvm);\n\n\treturn ret;\n}\n\nvoid kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,\n\t\t\t\t      struct irq_bypass_producer *prod)\n{\n\tint ret;\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(cons, struct kvm_kernel_irqfd, consumer);\n\n\tWARN_ON(irqfd->producer != prod);\n\tirqfd->producer = NULL;\n\n\t/*\n\t * When producer of consumer is unregistered, we change back to\n\t * remapped mode, so we can re-use the current implementation\n\t * when the irq is masked/disabled or the consumer side (KVM\n\t * int this case doesn't want to receive the interrupts.\n\t*/\n\tret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);\n\tif (ret)\n\t\tprintk(KERN_INFO \"irq bypass consumer (token %p) unregistration\"\n\t\t       \" fails: %d\\n\", irqfd->consumer.token, ret);\n\n\tkvm_arch_end_assignment(irqfd->kvm);\n}\n\nint kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,\n\t\t\t\t   uint32_t guest_irq, bool set)\n{\n\treturn static_call(kvm_x86_pi_update_irte)(kvm, host_irq, guest_irq, set);\n}\n\nbool kvm_arch_irqfd_route_changed(struct kvm_kernel_irq_routing_entry *old,\n\t\t\t\t  struct kvm_kernel_irq_routing_entry *new)\n{\n\tif (new->type != KVM_IRQ_ROUTING_MSI)\n\t\treturn true;\n\n\treturn !!memcmp(&old->msi, &new->msi, sizeof(new->msi));\n}\n\nbool kvm_vector_hashing_enabled(void)\n{\n\treturn vector_hashing;\n}\n\nbool kvm_arch_no_poll(struct kvm_vcpu *vcpu)\n{\n\treturn (vcpu->arch.msr_kvm_poll_control & 1) == 0;\n}\nEXPORT_SYMBOL_GPL(kvm_arch_no_poll);\n\n\nint kvm_spec_ctrl_test_value(u64 value)\n{\n\t/*\n\t * test that setting IA32_SPEC_CTRL to given value\n\t * is allowed by the host processor\n\t */\n\n\tu64 saved_value;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tlocal_irq_save(flags);\n\n\tif (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))\n\t\tret = 1;\n\telse if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))\n\t\tret = 1;\n\telse\n\t\twrmsrl(MSR_IA32_SPEC_CTRL, saved_value);\n\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kvm_spec_ctrl_test_value);\n\nvoid kvm_fixup_and_inject_pf_error(struct kvm_vcpu *vcpu, gva_t gva, u16 error_code)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\tstruct x86_exception fault;\n\tu64 access = error_code &\n\t\t(PFERR_WRITE_MASK | PFERR_FETCH_MASK | PFERR_USER_MASK);\n\n\tif (!(error_code & PFERR_PRESENT_MASK) ||\n\t    mmu->gva_to_gpa(vcpu, mmu, gva, access, &fault) != INVALID_GPA) {\n\t\t/*\n\t\t * If vcpu->arch.walk_mmu->gva_to_gpa succeeded, the page\n\t\t * tables probably do not match the TLB.  Just proceed\n\t\t * with the error code that the processor gave.\n\t\t */\n\t\tfault.vector = PF_VECTOR;\n\t\tfault.error_code_valid = true;\n\t\tfault.error_code = error_code;\n\t\tfault.nested_page_fault = false;\n\t\tfault.address = gva;\n\t\tfault.async_page_fault = false;\n\t}\n\tvcpu->arch.walk_mmu->inject_page_fault(vcpu, &fault);\n}\nEXPORT_SYMBOL_GPL(kvm_fixup_and_inject_pf_error);\n\n/*\n * Handles kvm_read/write_guest_virt*() result and either injects #PF or returns\n * KVM_EXIT_INTERNAL_ERROR for cases not currently handled by KVM. Return value\n * indicates whether exit to userspace is needed.\n */\nint kvm_handle_memory_failure(struct kvm_vcpu *vcpu, int r,\n\t\t\t      struct x86_exception *e)\n{\n\tif (r == X86EMUL_PROPAGATE_FAULT) {\n\t\tif (KVM_BUG_ON(!e, vcpu->kvm))\n\t\t\treturn -EIO;\n\n\t\tkvm_inject_emulated_page_fault(vcpu, e);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * In case kvm_read/write_guest_virt*() failed with X86EMUL_IO_NEEDED\n\t * while handling a VMX instruction KVM could've handled the request\n\t * correctly by exiting to userspace and performing I/O but there\n\t * doesn't seem to be a real use-case behind such requests, just return\n\t * KVM_EXIT_INTERNAL_ERROR for now.\n\t */\n\tkvm_prepare_emulation_failure_exit(vcpu);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_handle_memory_failure);\n\nint kvm_handle_invpcid(struct kvm_vcpu *vcpu, unsigned long type, gva_t gva)\n{\n\tbool pcid_enabled;\n\tstruct x86_exception e;\n\tstruct {\n\t\tu64 pcid;\n\t\tu64 gla;\n\t} operand;\n\tint r;\n\n\tr = kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e);\n\tif (r != X86EMUL_CONTINUE)\n\t\treturn kvm_handle_memory_failure(vcpu, r, &e);\n\n\tif (operand.pcid >> 12 != 0) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tpcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);\n\n\tswitch (type) {\n\tcase INVPCID_TYPE_INDIV_ADDR:\n\t\tif ((!pcid_enabled && (operand.pcid != 0)) ||\n\t\t    is_noncanonical_address(operand.gla, vcpu)) {\n\t\t\tkvm_inject_gp(vcpu, 0);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_mmu_invpcid_gva(vcpu, operand.gla, operand.pcid);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tcase INVPCID_TYPE_SINGLE_CTXT:\n\t\tif (!pcid_enabled && (operand.pcid != 0)) {\n\t\t\tkvm_inject_gp(vcpu, 0);\n\t\t\treturn 1;\n\t\t}\n\n\t\tkvm_invalidate_pcid(vcpu, operand.pcid);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tcase INVPCID_TYPE_ALL_NON_GLOBAL:\n\t\t/*\n\t\t * Currently, KVM doesn't mark global entries in the shadow\n\t\t * page tables, so a non-global flush just degenerates to a\n\t\t * global flush. If needed, we could optimize this later by\n\t\t * keeping track of global entries in shadow page tables.\n\t\t */\n\n\t\tfallthrough;\n\tcase INVPCID_TYPE_ALL_INCL_GLOBAL:\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tdefault:\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_handle_invpcid);\n\nstatic int complete_sev_es_emulated_mmio(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *run = vcpu->run;\n\tstruct kvm_mmio_fragment *frag;\n\tunsigned int len;\n\n\tBUG_ON(!vcpu->mmio_needed);\n\n\t/* Complete previous fragment */\n\tfrag = &vcpu->mmio_fragments[vcpu->mmio_cur_fragment];\n\tlen = min(8u, frag->len);\n\tif (!vcpu->mmio_is_write)\n\t\tmemcpy(frag->data, run->mmio.data, len);\n\n\tif (frag->len <= 8) {\n\t\t/* Switch to the next fragment. */\n\t\tfrag++;\n\t\tvcpu->mmio_cur_fragment++;\n\t} else {\n\t\t/* Go forward to the next mmio piece. */\n\t\tfrag->data += len;\n\t\tfrag->gpa += len;\n\t\tfrag->len -= len;\n\t}\n\n\tif (vcpu->mmio_cur_fragment >= vcpu->mmio_nr_fragments) {\n\t\tvcpu->mmio_needed = 0;\n\n\t\t// VMG change, at this point, we're always done\n\t\t// RIP has already been advanced\n\t\treturn 1;\n\t}\n\n\t// More MMIO is needed\n\trun->mmio.phys_addr = frag->gpa;\n\trun->mmio.len = min(8u, frag->len);\n\trun->mmio.is_write = vcpu->mmio_is_write;\n\tif (run->mmio.is_write)\n\t\tmemcpy(run->mmio.data, frag->data, min(8u, frag->len));\n\trun->exit_reason = KVM_EXIT_MMIO;\n\n\tvcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;\n\n\treturn 0;\n}\n\nint kvm_sev_es_mmio_write(struct kvm_vcpu *vcpu, gpa_t gpa, unsigned int bytes,\n\t\t\t  void *data)\n{\n\tint handled;\n\tstruct kvm_mmio_fragment *frag;\n\n\tif (!data)\n\t\treturn -EINVAL;\n\n\thandled = write_emultor.read_write_mmio(vcpu, gpa, bytes, data);\n\tif (handled == bytes)\n\t\treturn 1;\n\n\tbytes -= handled;\n\tgpa += handled;\n\tdata += handled;\n\n\t/*TODO: Check if need to increment number of frags */\n\tfrag = vcpu->mmio_fragments;\n\tvcpu->mmio_nr_fragments = 1;\n\tfrag->len = bytes;\n\tfrag->gpa = gpa;\n\tfrag->data = data;\n\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_cur_fragment = 0;\n\n\tvcpu->run->mmio.phys_addr = gpa;\n\tvcpu->run->mmio.len = min(8u, frag->len);\n\tvcpu->run->mmio.is_write = 1;\n\tmemcpy(vcpu->run->mmio.data, frag->data, min(8u, frag->len));\n\tvcpu->run->exit_reason = KVM_EXIT_MMIO;\n\n\tvcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_sev_es_mmio_write);\n\nint kvm_sev_es_mmio_read(struct kvm_vcpu *vcpu, gpa_t gpa, unsigned int bytes,\n\t\t\t void *data)\n{\n\tint handled;\n\tstruct kvm_mmio_fragment *frag;\n\n\tif (!data)\n\t\treturn -EINVAL;\n\n\thandled = read_emultor.read_write_mmio(vcpu, gpa, bytes, data);\n\tif (handled == bytes)\n\t\treturn 1;\n\n\tbytes -= handled;\n\tgpa += handled;\n\tdata += handled;\n\n\t/*TODO: Check if need to increment number of frags */\n\tfrag = vcpu->mmio_fragments;\n\tvcpu->mmio_nr_fragments = 1;\n\tfrag->len = bytes;\n\tfrag->gpa = gpa;\n\tfrag->data = data;\n\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_cur_fragment = 0;\n\n\tvcpu->run->mmio.phys_addr = gpa;\n\tvcpu->run->mmio.len = min(8u, frag->len);\n\tvcpu->run->mmio.is_write = 0;\n\tvcpu->run->exit_reason = KVM_EXIT_MMIO;\n\n\tvcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_sev_es_mmio_read);\n\nstatic void advance_sev_es_emulated_pio(struct kvm_vcpu *vcpu, unsigned count, int size)\n{\n\tvcpu->arch.sev_pio_count -= count;\n\tvcpu->arch.sev_pio_data += count * size;\n}\n\nstatic int kvm_sev_es_outs(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t   unsigned int port);\n\nstatic int complete_sev_es_emulated_outs(struct kvm_vcpu *vcpu)\n{\n\tint size = vcpu->arch.pio.size;\n\tint port = vcpu->arch.pio.port;\n\n\tvcpu->arch.pio.count = 0;\n\tif (vcpu->arch.sev_pio_count)\n\t\treturn kvm_sev_es_outs(vcpu, size, port);\n\treturn 1;\n}\n\nstatic int kvm_sev_es_outs(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t   unsigned int port)\n{\n\tfor (;;) {\n\t\tunsigned int count =\n\t\t\tmin_t(unsigned int, PAGE_SIZE / size, vcpu->arch.sev_pio_count);\n\t\tint ret = emulator_pio_out(vcpu, size, port, vcpu->arch.sev_pio_data, count);\n\n\t\t/* memcpy done already by emulator_pio_out.  */\n\t\tadvance_sev_es_emulated_pio(vcpu, count, size);\n\t\tif (!ret)\n\t\t\tbreak;\n\n\t\t/* Emulation done by the kernel.  */\n\t\tif (!vcpu->arch.sev_pio_count)\n\t\t\treturn 1;\n\t}\n\n\tvcpu->arch.complete_userspace_io = complete_sev_es_emulated_outs;\n\treturn 0;\n}\n\nstatic int kvm_sev_es_ins(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t  unsigned int port);\n\nstatic int complete_sev_es_emulated_ins(struct kvm_vcpu *vcpu)\n{\n\tunsigned count = vcpu->arch.pio.count;\n\tint size = vcpu->arch.pio.size;\n\tint port = vcpu->arch.pio.port;\n\n\tcomplete_emulator_pio_in(vcpu, vcpu->arch.sev_pio_data);\n\tadvance_sev_es_emulated_pio(vcpu, count, size);\n\tif (vcpu->arch.sev_pio_count)\n\t\treturn kvm_sev_es_ins(vcpu, size, port);\n\treturn 1;\n}\n\nstatic int kvm_sev_es_ins(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t  unsigned int port)\n{\n\tfor (;;) {\n\t\tunsigned int count =\n\t\t\tmin_t(unsigned int, PAGE_SIZE / size, vcpu->arch.sev_pio_count);\n\t\tif (!emulator_pio_in(vcpu, size, port, vcpu->arch.sev_pio_data, count))\n\t\t\tbreak;\n\n\t\t/* Emulation done by the kernel.  */\n\t\tadvance_sev_es_emulated_pio(vcpu, count, size);\n\t\tif (!vcpu->arch.sev_pio_count)\n\t\t\treturn 1;\n\t}\n\n\tvcpu->arch.complete_userspace_io = complete_sev_es_emulated_ins;\n\treturn 0;\n}\n\nint kvm_sev_es_string_io(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t unsigned int port, void *data,  unsigned int count,\n\t\t\t int in)\n{\n\tvcpu->arch.sev_pio_data = data;\n\tvcpu->arch.sev_pio_count = count;\n\treturn in ? kvm_sev_es_ins(vcpu, size, port)\n\t\t  : kvm_sev_es_outs(vcpu, size, port);\n}\nEXPORT_SYMBOL_GPL(kvm_sev_es_string_io);\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_entry);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_exit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_fast_mmio);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_inj_virq);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_page_fault);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_msr);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_cr);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmenter);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmexit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmexit_inject);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_intr_vmexit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmenter_failed);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_invlpga);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_skinit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_intercepts);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_write_tsc_offset);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_ple_window_update);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_pml_full);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_pi_irte_update);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_unaccelerated_access);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_incomplete_ipi);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_ga_log);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_kick_vcpu_slowpath);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_doorbell);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_apicv_accept_irq);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_enter);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_exit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_msr_protocol_enter);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_vmgexit_msr_protocol_exit);\n\nstatic int __init kvm_x86_init(void)\n{\n\tkvm_mmu_x86_module_init();\n\tmitigate_smt_rsb &= boot_cpu_has_bug(X86_BUG_SMT_RSB) && cpu_smt_possible();\n\treturn 0;\n}\nmodule_init(kvm_x86_init);\n\nstatic void __exit kvm_x86_exit(void)\n{\n\t/*\n\t * If module_init() is implemented, module_exit() must also be\n\t * implemented to allow module unload.\n\t */\n}\nmodule_exit(kvm_x86_exit);\n"], "filenames": ["arch/x86/kvm/x86.c"], "buggy_code_start_loc": [5265], "buggy_code_end_loc": [5272], "fixing_code_start_loc": [5266], "fixing_code_end_loc": [5270], "type": "CWE-665", "message": "A flaw was found in KVM. When calling the KVM_GET_DEBUGREGS ioctl, on 32-bit systems, there might be some uninitialized portions of the kvm_debugregs structure that could be copied to userspace, causing an information leak.", "other": {"cve": {"id": "CVE-2023-1513", "sourceIdentifier": "secalert@redhat.com", "published": "2023-03-23T21:15:19.427", "lastModified": "2023-05-03T14:15:24.243", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "A flaw was found in KVM. When calling the KVM_GET_DEBUGREGS ioctl, on 32-bit systems, there might be some uninitialized portions of the kvm_debugregs structure that could be copied to userspace, causing an information leak."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:L/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 3.3, "baseSeverity": "LOW"}, "exploitabilityScore": 1.8, "impactScore": 1.4}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-665"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-665"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "6.2", "matchCriteriaId": "108695B6-7133-4B6C-80AF-0F66880FE858"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:37:*:*:*:*:*:*:*", "matchCriteriaId": "E30D0E6F-4AE8-4284-8716-991DFA48CC5D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "142AD0DD-4CF3-4D74-9442-459CE3347E3A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "7F6FB57C-2BC7-487C-96DD-132683AEB35D"}]}]}], "references": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2179892", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/2c10b61421a28e95a46ab489fd56c0f442ff6952", "source": "secalert@redhat.com", "tags": ["Patch"]}, {"url": "https://lists.debian.org/debian-lts-announce/2023/05/msg00005.html", "source": "secalert@redhat.com"}, {"url": "https://lists.debian.org/debian-lts-announce/2023/05/msg00006.html", "source": "secalert@redhat.com"}, {"url": "https://lore.kernel.org/kvm/20230214103304.3689213-1-gregkh@linuxfoundation.org/", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/2c10b61421a28e95a46ab489fd56c0f442ff6952"}}