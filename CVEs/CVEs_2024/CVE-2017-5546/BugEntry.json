{"buggy_code": ["/*\n * linux/mm/slab.c\n * Written by Mark Hemment, 1996/97.\n * (markhe@nextd.demon.co.uk)\n *\n * kmem_cache_destroy() + some cleanup - 1999 Andrea Arcangeli\n *\n * Major cleanup, different bufctl logic, per-cpu arrays\n *\t(c) 2000 Manfred Spraul\n *\n * Cleanup, make the head arrays unconditional, preparation for NUMA\n * \t(c) 2002 Manfred Spraul\n *\n * An implementation of the Slab Allocator as described in outline in;\n *\tUNIX Internals: The New Frontiers by Uresh Vahalia\n *\tPub: Prentice Hall\tISBN 0-13-101908-2\n * or with a little more detail in;\n *\tThe Slab Allocator: An Object-Caching Kernel Memory Allocator\n *\tJeff Bonwick (Sun Microsystems).\n *\tPresented at: USENIX Summer 1994 Technical Conference\n *\n * The memory is organized in caches, one cache for each object type.\n * (e.g. inode_cache, dentry_cache, buffer_head, vm_area_struct)\n * Each cache consists out of many slabs (they are small (usually one\n * page long) and always contiguous), and each slab contains multiple\n * initialized objects.\n *\n * This means, that your constructor is used only for newly allocated\n * slabs and you must pass objects with the same initializations to\n * kmem_cache_free.\n *\n * Each cache can only support one memory type (GFP_DMA, GFP_HIGHMEM,\n * normal). If you need a special memory type, then must create a new\n * cache for that memory type.\n *\n * In order to reduce fragmentation, the slabs are sorted in 3 groups:\n *   full slabs with 0 free objects\n *   partial slabs\n *   empty slabs with no allocated objects\n *\n * If partial slabs exist, then new allocations come from these slabs,\n * otherwise from empty slabs or new slabs are allocated.\n *\n * kmem_cache_destroy() CAN CRASH if you try to allocate from the cache\n * during kmem_cache_destroy(). The caller must prevent concurrent allocs.\n *\n * Each cache has a short per-cpu head array, most allocs\n * and frees go into that array, and if that array overflows, then 1/2\n * of the entries in the array are given back into the global cache.\n * The head array is strictly LIFO and should improve the cache hit rates.\n * On SMP, it additionally reduces the spinlock operations.\n *\n * The c_cpuarray may not be read with enabled local interrupts -\n * it's changed with a smp_call_function().\n *\n * SMP synchronization:\n *  constructors and destructors are called without any locking.\n *  Several members in struct kmem_cache and struct slab never change, they\n *\tare accessed without any locking.\n *  The per-cpu arrays are never accessed from the wrong cpu, no locking,\n *  \tand local interrupts are disabled so slab code is preempt-safe.\n *  The non-constant members are protected with a per-cache irq spinlock.\n *\n * Many thanks to Mark Hemment, who wrote another per-cpu slab patch\n * in 2000 - many ideas in the current implementation are derived from\n * his patch.\n *\n * Further notes from the original documentation:\n *\n * 11 April '97.  Started multi-threading - markhe\n *\tThe global cache-chain is protected by the mutex 'slab_mutex'.\n *\tThe sem is only needed when accessing/extending the cache-chain, which\n *\tcan never happen inside an interrupt (kmem_cache_create(),\n *\tkmem_cache_shrink() and kmem_cache_reap()).\n *\n *\tAt present, each engine can be growing a cache.  This should be blocked.\n *\n * 15 March 2005. NUMA slab allocator.\n *\tShai Fultheim <shai@scalex86.org>.\n *\tShobhit Dayal <shobhit@calsoftinc.com>\n *\tAlok N Kataria <alokk@calsoftinc.com>\n *\tChristoph Lameter <christoph@lameter.com>\n *\n *\tModified the slab allocator to be node aware on NUMA systems.\n *\tEach node has its own list of partial, free and full slabs.\n *\tAll object allocations for a node occur from node specific slab lists.\n */\n\n#include\t<linux/slab.h>\n#include\t<linux/mm.h>\n#include\t<linux/poison.h>\n#include\t<linux/swap.h>\n#include\t<linux/cache.h>\n#include\t<linux/interrupt.h>\n#include\t<linux/init.h>\n#include\t<linux/compiler.h>\n#include\t<linux/cpuset.h>\n#include\t<linux/proc_fs.h>\n#include\t<linux/seq_file.h>\n#include\t<linux/notifier.h>\n#include\t<linux/kallsyms.h>\n#include\t<linux/cpu.h>\n#include\t<linux/sysctl.h>\n#include\t<linux/module.h>\n#include\t<linux/rcupdate.h>\n#include\t<linux/string.h>\n#include\t<linux/uaccess.h>\n#include\t<linux/nodemask.h>\n#include\t<linux/kmemleak.h>\n#include\t<linux/mempolicy.h>\n#include\t<linux/mutex.h>\n#include\t<linux/fault-inject.h>\n#include\t<linux/rtmutex.h>\n#include\t<linux/reciprocal_div.h>\n#include\t<linux/debugobjects.h>\n#include\t<linux/kmemcheck.h>\n#include\t<linux/memory.h>\n#include\t<linux/prefetch.h>\n\n#include\t<net/sock.h>\n\n#include\t<asm/cacheflush.h>\n#include\t<asm/tlbflush.h>\n#include\t<asm/page.h>\n\n#include <trace/events/kmem.h>\n\n#include\t\"internal.h\"\n\n#include\t\"slab.h\"\n\n/*\n * DEBUG\t- 1 for kmem_cache_create() to honour; SLAB_RED_ZONE & SLAB_POISON.\n *\t\t  0 for faster, smaller code (especially in the critical paths).\n *\n * STATS\t- 1 to collect stats for /proc/slabinfo.\n *\t\t  0 for faster, smaller code (especially in the critical paths).\n *\n * FORCED_DEBUG\t- 1 enables SLAB_RED_ZONE and SLAB_POISON (if possible)\n */\n\n#ifdef CONFIG_DEBUG_SLAB\n#define\tDEBUG\t\t1\n#define\tSTATS\t\t1\n#define\tFORCED_DEBUG\t1\n#else\n#define\tDEBUG\t\t0\n#define\tSTATS\t\t0\n#define\tFORCED_DEBUG\t0\n#endif\n\n/* Shouldn't this be in a header file somewhere? */\n#define\tBYTES_PER_WORD\t\tsizeof(void *)\n#define\tREDZONE_ALIGN\t\tmax(BYTES_PER_WORD, __alignof__(unsigned long long))\n\n#ifndef ARCH_KMALLOC_FLAGS\n#define ARCH_KMALLOC_FLAGS SLAB_HWCACHE_ALIGN\n#endif\n\n#define FREELIST_BYTE_INDEX (((PAGE_SIZE >> BITS_PER_BYTE) \\\n\t\t\t\t<= SLAB_OBJ_MIN_SIZE) ? 1 : 0)\n\n#if FREELIST_BYTE_INDEX\ntypedef unsigned char freelist_idx_t;\n#else\ntypedef unsigned short freelist_idx_t;\n#endif\n\n#define SLAB_OBJ_MAX_NUM ((1 << sizeof(freelist_idx_t) * BITS_PER_BYTE) - 1)\n\n/*\n * struct array_cache\n *\n * Purpose:\n * - LIFO ordering, to hand out cache-warm objects from _alloc\n * - reduce the number of linked list operations\n * - reduce spinlock operations\n *\n * The limit is stored in the per-cpu structure to reduce the data cache\n * footprint.\n *\n */\nstruct array_cache {\n\tunsigned int avail;\n\tunsigned int limit;\n\tunsigned int batchcount;\n\tunsigned int touched;\n\tvoid *entry[];\t/*\n\t\t\t * Must have this definition in here for the proper\n\t\t\t * alignment of array_cache. Also simplifies accessing\n\t\t\t * the entries.\n\t\t\t */\n};\n\nstruct alien_cache {\n\tspinlock_t lock;\n\tstruct array_cache ac;\n};\n\n/*\n * Need this for bootstrapping a per node allocator.\n */\n#define NUM_INIT_LISTS (2 * MAX_NUMNODES)\nstatic struct kmem_cache_node __initdata init_kmem_cache_node[NUM_INIT_LISTS];\n#define\tCACHE_CACHE 0\n#define\tSIZE_NODE (MAX_NUMNODES)\n\nstatic int drain_freelist(struct kmem_cache *cache,\n\t\t\tstruct kmem_cache_node *n, int tofree);\nstatic void free_block(struct kmem_cache *cachep, void **objpp, int len,\n\t\t\tint node, struct list_head *list);\nstatic void slabs_destroy(struct kmem_cache *cachep, struct list_head *list);\nstatic int enable_cpucache(struct kmem_cache *cachep, gfp_t gfp);\nstatic void cache_reap(struct work_struct *unused);\n\nstatic inline void fixup_objfreelist_debug(struct kmem_cache *cachep,\n\t\t\t\t\t\tvoid **list);\nstatic inline void fixup_slab_list(struct kmem_cache *cachep,\n\t\t\t\tstruct kmem_cache_node *n, struct page *page,\n\t\t\t\tvoid **list);\nstatic int slab_early_init = 1;\n\n#define INDEX_NODE kmalloc_index(sizeof(struct kmem_cache_node))\n\nstatic void kmem_cache_node_init(struct kmem_cache_node *parent)\n{\n\tINIT_LIST_HEAD(&parent->slabs_full);\n\tINIT_LIST_HEAD(&parent->slabs_partial);\n\tINIT_LIST_HEAD(&parent->slabs_free);\n\tparent->total_slabs = 0;\n\tparent->free_slabs = 0;\n\tparent->shared = NULL;\n\tparent->alien = NULL;\n\tparent->colour_next = 0;\n\tspin_lock_init(&parent->list_lock);\n\tparent->free_objects = 0;\n\tparent->free_touched = 0;\n}\n\n#define MAKE_LIST(cachep, listp, slab, nodeid)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tINIT_LIST_HEAD(listp);\t\t\t\t\t\\\n\t\tlist_splice(&get_node(cachep, nodeid)->slab, listp);\t\\\n\t} while (0)\n\n#define\tMAKE_ALL_LISTS(cachep, ptr, nodeid)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\tMAKE_LIST((cachep), (&(ptr)->slabs_full), slabs_full, nodeid);\t\\\n\tMAKE_LIST((cachep), (&(ptr)->slabs_partial), slabs_partial, nodeid); \\\n\tMAKE_LIST((cachep), (&(ptr)->slabs_free), slabs_free, nodeid);\t\\\n\t} while (0)\n\n#define CFLGS_OBJFREELIST_SLAB\t(0x40000000UL)\n#define CFLGS_OFF_SLAB\t\t(0x80000000UL)\n#define\tOBJFREELIST_SLAB(x)\t((x)->flags & CFLGS_OBJFREELIST_SLAB)\n#define\tOFF_SLAB(x)\t((x)->flags & CFLGS_OFF_SLAB)\n\n#define BATCHREFILL_LIMIT\t16\n/*\n * Optimization question: fewer reaps means less probability for unnessary\n * cpucache drain/refill cycles.\n *\n * OTOH the cpuarrays can contain lots of objects,\n * which could lock up otherwise freeable slabs.\n */\n#define REAPTIMEOUT_AC\t\t(2*HZ)\n#define REAPTIMEOUT_NODE\t(4*HZ)\n\n#if STATS\n#define\tSTATS_INC_ACTIVE(x)\t((x)->num_active++)\n#define\tSTATS_DEC_ACTIVE(x)\t((x)->num_active--)\n#define\tSTATS_INC_ALLOCED(x)\t((x)->num_allocations++)\n#define\tSTATS_INC_GROWN(x)\t((x)->grown++)\n#define\tSTATS_ADD_REAPED(x,y)\t((x)->reaped += (y))\n#define\tSTATS_SET_HIGH(x)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif ((x)->num_active > (x)->high_mark)\t\t\t\\\n\t\t\t(x)->high_mark = (x)->num_active;\t\t\\\n\t} while (0)\n#define\tSTATS_INC_ERR(x)\t((x)->errors++)\n#define\tSTATS_INC_NODEALLOCS(x)\t((x)->node_allocs++)\n#define\tSTATS_INC_NODEFREES(x)\t((x)->node_frees++)\n#define STATS_INC_ACOVERFLOW(x)   ((x)->node_overflow++)\n#define\tSTATS_SET_FREEABLE(x, i)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif ((x)->max_freeable < i)\t\t\t\t\\\n\t\t\t(x)->max_freeable = i;\t\t\t\t\\\n\t} while (0)\n#define STATS_INC_ALLOCHIT(x)\tatomic_inc(&(x)->allochit)\n#define STATS_INC_ALLOCMISS(x)\tatomic_inc(&(x)->allocmiss)\n#define STATS_INC_FREEHIT(x)\tatomic_inc(&(x)->freehit)\n#define STATS_INC_FREEMISS(x)\tatomic_inc(&(x)->freemiss)\n#else\n#define\tSTATS_INC_ACTIVE(x)\tdo { } while (0)\n#define\tSTATS_DEC_ACTIVE(x)\tdo { } while (0)\n#define\tSTATS_INC_ALLOCED(x)\tdo { } while (0)\n#define\tSTATS_INC_GROWN(x)\tdo { } while (0)\n#define\tSTATS_ADD_REAPED(x,y)\tdo { (void)(y); } while (0)\n#define\tSTATS_SET_HIGH(x)\tdo { } while (0)\n#define\tSTATS_INC_ERR(x)\tdo { } while (0)\n#define\tSTATS_INC_NODEALLOCS(x)\tdo { } while (0)\n#define\tSTATS_INC_NODEFREES(x)\tdo { } while (0)\n#define STATS_INC_ACOVERFLOW(x)   do { } while (0)\n#define\tSTATS_SET_FREEABLE(x, i) do { } while (0)\n#define STATS_INC_ALLOCHIT(x)\tdo { } while (0)\n#define STATS_INC_ALLOCMISS(x)\tdo { } while (0)\n#define STATS_INC_FREEHIT(x)\tdo { } while (0)\n#define STATS_INC_FREEMISS(x)\tdo { } while (0)\n#endif\n\n#if DEBUG\n\n/*\n * memory layout of objects:\n * 0\t\t: objp\n * 0 .. cachep->obj_offset - BYTES_PER_WORD - 1: padding. This ensures that\n * \t\tthe end of an object is aligned with the end of the real\n * \t\tallocation. Catches writes behind the end of the allocation.\n * cachep->obj_offset - BYTES_PER_WORD .. cachep->obj_offset - 1:\n * \t\tredzone word.\n * cachep->obj_offset: The real object.\n * cachep->size - 2* BYTES_PER_WORD: redzone word [BYTES_PER_WORD long]\n * cachep->size - 1* BYTES_PER_WORD: last caller address\n *\t\t\t\t\t[BYTES_PER_WORD long]\n */\nstatic int obj_offset(struct kmem_cache *cachep)\n{\n\treturn cachep->obj_offset;\n}\n\nstatic unsigned long long *dbg_redzone1(struct kmem_cache *cachep, void *objp)\n{\n\tBUG_ON(!(cachep->flags & SLAB_RED_ZONE));\n\treturn (unsigned long long*) (objp + obj_offset(cachep) -\n\t\t\t\t      sizeof(unsigned long long));\n}\n\nstatic unsigned long long *dbg_redzone2(struct kmem_cache *cachep, void *objp)\n{\n\tBUG_ON(!(cachep->flags & SLAB_RED_ZONE));\n\tif (cachep->flags & SLAB_STORE_USER)\n\t\treturn (unsigned long long *)(objp + cachep->size -\n\t\t\t\t\t      sizeof(unsigned long long) -\n\t\t\t\t\t      REDZONE_ALIGN);\n\treturn (unsigned long long *) (objp + cachep->size -\n\t\t\t\t       sizeof(unsigned long long));\n}\n\nstatic void **dbg_userword(struct kmem_cache *cachep, void *objp)\n{\n\tBUG_ON(!(cachep->flags & SLAB_STORE_USER));\n\treturn (void **)(objp + cachep->size - BYTES_PER_WORD);\n}\n\n#else\n\n#define obj_offset(x)\t\t\t0\n#define dbg_redzone1(cachep, objp)\t({BUG(); (unsigned long long *)NULL;})\n#define dbg_redzone2(cachep, objp)\t({BUG(); (unsigned long long *)NULL;})\n#define dbg_userword(cachep, objp)\t({BUG(); (void **)NULL;})\n\n#endif\n\n#ifdef CONFIG_DEBUG_SLAB_LEAK\n\nstatic inline bool is_store_user_clean(struct kmem_cache *cachep)\n{\n\treturn atomic_read(&cachep->store_user_clean) == 1;\n}\n\nstatic inline void set_store_user_clean(struct kmem_cache *cachep)\n{\n\tatomic_set(&cachep->store_user_clean, 1);\n}\n\nstatic inline void set_store_user_dirty(struct kmem_cache *cachep)\n{\n\tif (is_store_user_clean(cachep))\n\t\tatomic_set(&cachep->store_user_clean, 0);\n}\n\n#else\nstatic inline void set_store_user_dirty(struct kmem_cache *cachep) {}\n\n#endif\n\n/*\n * Do not go above this order unless 0 objects fit into the slab or\n * overridden on the command line.\n */\n#define\tSLAB_MAX_ORDER_HI\t1\n#define\tSLAB_MAX_ORDER_LO\t0\nstatic int slab_max_order = SLAB_MAX_ORDER_LO;\nstatic bool slab_max_order_set __initdata;\n\nstatic inline struct kmem_cache *virt_to_cache(const void *obj)\n{\n\tstruct page *page = virt_to_head_page(obj);\n\treturn page->slab_cache;\n}\n\nstatic inline void *index_to_obj(struct kmem_cache *cache, struct page *page,\n\t\t\t\t unsigned int idx)\n{\n\treturn page->s_mem + cache->size * idx;\n}\n\n/*\n * We want to avoid an expensive divide : (offset / cache->size)\n *   Using the fact that size is a constant for a particular cache,\n *   we can replace (offset / cache->size) by\n *   reciprocal_divide(offset, cache->reciprocal_buffer_size)\n */\nstatic inline unsigned int obj_to_index(const struct kmem_cache *cache,\n\t\t\t\t\tconst struct page *page, void *obj)\n{\n\tu32 offset = (obj - page->s_mem);\n\treturn reciprocal_divide(offset, cache->reciprocal_buffer_size);\n}\n\n#define BOOT_CPUCACHE_ENTRIES\t1\n/* internal cache of cache description objs */\nstatic struct kmem_cache kmem_cache_boot = {\n\t.batchcount = 1,\n\t.limit = BOOT_CPUCACHE_ENTRIES,\n\t.shared = 1,\n\t.size = sizeof(struct kmem_cache),\n\t.name = \"kmem_cache\",\n};\n\nstatic DEFINE_PER_CPU(struct delayed_work, slab_reap_work);\n\nstatic inline struct array_cache *cpu_cache_get(struct kmem_cache *cachep)\n{\n\treturn this_cpu_ptr(cachep->cpu_cache);\n}\n\n/*\n * Calculate the number of objects and left-over bytes for a given buffer size.\n */\nstatic unsigned int cache_estimate(unsigned long gfporder, size_t buffer_size,\n\t\tunsigned long flags, size_t *left_over)\n{\n\tunsigned int num;\n\tsize_t slab_size = PAGE_SIZE << gfporder;\n\n\t/*\n\t * The slab management structure can be either off the slab or\n\t * on it. For the latter case, the memory allocated for a\n\t * slab is used for:\n\t *\n\t * - @buffer_size bytes for each object\n\t * - One freelist_idx_t for each object\n\t *\n\t * We don't need to consider alignment of freelist because\n\t * freelist will be at the end of slab page. The objects will be\n\t * at the correct alignment.\n\t *\n\t * If the slab management structure is off the slab, then the\n\t * alignment will already be calculated into the size. Because\n\t * the slabs are all pages aligned, the objects will be at the\n\t * correct alignment when allocated.\n\t */\n\tif (flags & (CFLGS_OBJFREELIST_SLAB | CFLGS_OFF_SLAB)) {\n\t\tnum = slab_size / buffer_size;\n\t\t*left_over = slab_size % buffer_size;\n\t} else {\n\t\tnum = slab_size / (buffer_size + sizeof(freelist_idx_t));\n\t\t*left_over = slab_size %\n\t\t\t(buffer_size + sizeof(freelist_idx_t));\n\t}\n\n\treturn num;\n}\n\n#if DEBUG\n#define slab_error(cachep, msg) __slab_error(__func__, cachep, msg)\n\nstatic void __slab_error(const char *function, struct kmem_cache *cachep,\n\t\t\tchar *msg)\n{\n\tpr_err(\"slab error in %s(): cache `%s': %s\\n\",\n\t       function, cachep->name, msg);\n\tdump_stack();\n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n}\n#endif\n\n/*\n * By default on NUMA we use alien caches to stage the freeing of\n * objects allocated from other nodes. This causes massive memory\n * inefficiencies when using fake NUMA setup to split memory into a\n * large number of small nodes, so it can be disabled on the command\n * line\n  */\n\nstatic int use_alien_caches __read_mostly = 1;\nstatic int __init noaliencache_setup(char *s)\n{\n\tuse_alien_caches = 0;\n\treturn 1;\n}\n__setup(\"noaliencache\", noaliencache_setup);\n\nstatic int __init slab_max_order_setup(char *str)\n{\n\tget_option(&str, &slab_max_order);\n\tslab_max_order = slab_max_order < 0 ? 0 :\n\t\t\t\tmin(slab_max_order, MAX_ORDER - 1);\n\tslab_max_order_set = true;\n\n\treturn 1;\n}\n__setup(\"slab_max_order=\", slab_max_order_setup);\n\n#ifdef CONFIG_NUMA\n/*\n * Special reaping functions for NUMA systems called from cache_reap().\n * These take care of doing round robin flushing of alien caches (containing\n * objects freed on different nodes from which they were allocated) and the\n * flushing of remote pcps by calling drain_node_pages.\n */\nstatic DEFINE_PER_CPU(unsigned long, slab_reap_node);\n\nstatic void init_reap_node(int cpu)\n{\n\tper_cpu(slab_reap_node, cpu) = next_node_in(cpu_to_mem(cpu),\n\t\t\t\t\t\t    node_online_map);\n}\n\nstatic void next_reap_node(void)\n{\n\tint node = __this_cpu_read(slab_reap_node);\n\n\tnode = next_node_in(node, node_online_map);\n\t__this_cpu_write(slab_reap_node, node);\n}\n\n#else\n#define init_reap_node(cpu) do { } while (0)\n#define next_reap_node(void) do { } while (0)\n#endif\n\n/*\n * Initiate the reap timer running on the target CPU.  We run at around 1 to 2Hz\n * via the workqueue/eventd.\n * Add the CPU number into the expiration time to minimize the possibility of\n * the CPUs getting into lockstep and contending for the global cache chain\n * lock.\n */\nstatic void start_cpu_timer(int cpu)\n{\n\tstruct delayed_work *reap_work = &per_cpu(slab_reap_work, cpu);\n\n\tif (reap_work->work.func == NULL) {\n\t\tinit_reap_node(cpu);\n\t\tINIT_DEFERRABLE_WORK(reap_work, cache_reap);\n\t\tschedule_delayed_work_on(cpu, reap_work,\n\t\t\t\t\t__round_jiffies_relative(HZ, cpu));\n\t}\n}\n\nstatic void init_arraycache(struct array_cache *ac, int limit, int batch)\n{\n\t/*\n\t * The array_cache structures contain pointers to free object.\n\t * However, when such objects are allocated or transferred to another\n\t * cache the pointers are not cleared and they could be counted as\n\t * valid references during a kmemleak scan. Therefore, kmemleak must\n\t * not scan such objects.\n\t */\n\tkmemleak_no_scan(ac);\n\tif (ac) {\n\t\tac->avail = 0;\n\t\tac->limit = limit;\n\t\tac->batchcount = batch;\n\t\tac->touched = 0;\n\t}\n}\n\nstatic struct array_cache *alloc_arraycache(int node, int entries,\n\t\t\t\t\t    int batchcount, gfp_t gfp)\n{\n\tsize_t memsize = sizeof(void *) * entries + sizeof(struct array_cache);\n\tstruct array_cache *ac = NULL;\n\n\tac = kmalloc_node(memsize, gfp, node);\n\tinit_arraycache(ac, entries, batchcount);\n\treturn ac;\n}\n\nstatic noinline void cache_free_pfmemalloc(struct kmem_cache *cachep,\n\t\t\t\t\tstruct page *page, void *objp)\n{\n\tstruct kmem_cache_node *n;\n\tint page_node;\n\tLIST_HEAD(list);\n\n\tpage_node = page_to_nid(page);\n\tn = get_node(cachep, page_node);\n\n\tspin_lock(&n->list_lock);\n\tfree_block(cachep, &objp, 1, page_node, &list);\n\tspin_unlock(&n->list_lock);\n\n\tslabs_destroy(cachep, &list);\n}\n\n/*\n * Transfer objects in one arraycache to another.\n * Locking must be handled by the caller.\n *\n * Return the number of entries transferred.\n */\nstatic int transfer_objects(struct array_cache *to,\n\t\tstruct array_cache *from, unsigned int max)\n{\n\t/* Figure out how many entries to transfer */\n\tint nr = min3(from->avail, max, to->limit - to->avail);\n\n\tif (!nr)\n\t\treturn 0;\n\n\tmemcpy(to->entry + to->avail, from->entry + from->avail -nr,\n\t\t\tsizeof(void *) *nr);\n\n\tfrom->avail -= nr;\n\tto->avail += nr;\n\treturn nr;\n}\n\n#ifndef CONFIG_NUMA\n\n#define drain_alien_cache(cachep, alien) do { } while (0)\n#define reap_alien(cachep, n) do { } while (0)\n\nstatic inline struct alien_cache **alloc_alien_cache(int node,\n\t\t\t\t\t\tint limit, gfp_t gfp)\n{\n\treturn NULL;\n}\n\nstatic inline void free_alien_cache(struct alien_cache **ac_ptr)\n{\n}\n\nstatic inline int cache_free_alien(struct kmem_cache *cachep, void *objp)\n{\n\treturn 0;\n}\n\nstatic inline void *alternate_node_alloc(struct kmem_cache *cachep,\n\t\tgfp_t flags)\n{\n\treturn NULL;\n}\n\nstatic inline void *____cache_alloc_node(struct kmem_cache *cachep,\n\t\t gfp_t flags, int nodeid)\n{\n\treturn NULL;\n}\n\nstatic inline gfp_t gfp_exact_node(gfp_t flags)\n{\n\treturn flags & ~__GFP_NOFAIL;\n}\n\n#else\t/* CONFIG_NUMA */\n\nstatic void *____cache_alloc_node(struct kmem_cache *, gfp_t, int);\nstatic void *alternate_node_alloc(struct kmem_cache *, gfp_t);\n\nstatic struct alien_cache *__alloc_alien_cache(int node, int entries,\n\t\t\t\t\t\tint batch, gfp_t gfp)\n{\n\tsize_t memsize = sizeof(void *) * entries + sizeof(struct alien_cache);\n\tstruct alien_cache *alc = NULL;\n\n\talc = kmalloc_node(memsize, gfp, node);\n\tinit_arraycache(&alc->ac, entries, batch);\n\tspin_lock_init(&alc->lock);\n\treturn alc;\n}\n\nstatic struct alien_cache **alloc_alien_cache(int node, int limit, gfp_t gfp)\n{\n\tstruct alien_cache **alc_ptr;\n\tsize_t memsize = sizeof(void *) * nr_node_ids;\n\tint i;\n\n\tif (limit > 1)\n\t\tlimit = 12;\n\talc_ptr = kzalloc_node(memsize, gfp, node);\n\tif (!alc_ptr)\n\t\treturn NULL;\n\n\tfor_each_node(i) {\n\t\tif (i == node || !node_online(i))\n\t\t\tcontinue;\n\t\talc_ptr[i] = __alloc_alien_cache(node, limit, 0xbaadf00d, gfp);\n\t\tif (!alc_ptr[i]) {\n\t\t\tfor (i--; i >= 0; i--)\n\t\t\t\tkfree(alc_ptr[i]);\n\t\t\tkfree(alc_ptr);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\treturn alc_ptr;\n}\n\nstatic void free_alien_cache(struct alien_cache **alc_ptr)\n{\n\tint i;\n\n\tif (!alc_ptr)\n\t\treturn;\n\tfor_each_node(i)\n\t    kfree(alc_ptr[i]);\n\tkfree(alc_ptr);\n}\n\nstatic void __drain_alien_cache(struct kmem_cache *cachep,\n\t\t\t\tstruct array_cache *ac, int node,\n\t\t\t\tstruct list_head *list)\n{\n\tstruct kmem_cache_node *n = get_node(cachep, node);\n\n\tif (ac->avail) {\n\t\tspin_lock(&n->list_lock);\n\t\t/*\n\t\t * Stuff objects into the remote nodes shared array first.\n\t\t * That way we could avoid the overhead of putting the objects\n\t\t * into the free lists and getting them back later.\n\t\t */\n\t\tif (n->shared)\n\t\t\ttransfer_objects(n->shared, ac, ac->limit);\n\n\t\tfree_block(cachep, ac->entry, ac->avail, node, list);\n\t\tac->avail = 0;\n\t\tspin_unlock(&n->list_lock);\n\t}\n}\n\n/*\n * Called from cache_reap() to regularly drain alien caches round robin.\n */\nstatic void reap_alien(struct kmem_cache *cachep, struct kmem_cache_node *n)\n{\n\tint node = __this_cpu_read(slab_reap_node);\n\n\tif (n->alien) {\n\t\tstruct alien_cache *alc = n->alien[node];\n\t\tstruct array_cache *ac;\n\n\t\tif (alc) {\n\t\t\tac = &alc->ac;\n\t\t\tif (ac->avail && spin_trylock_irq(&alc->lock)) {\n\t\t\t\tLIST_HEAD(list);\n\n\t\t\t\t__drain_alien_cache(cachep, ac, node, &list);\n\t\t\t\tspin_unlock_irq(&alc->lock);\n\t\t\t\tslabs_destroy(cachep, &list);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void drain_alien_cache(struct kmem_cache *cachep,\n\t\t\t\tstruct alien_cache **alien)\n{\n\tint i = 0;\n\tstruct alien_cache *alc;\n\tstruct array_cache *ac;\n\tunsigned long flags;\n\n\tfor_each_online_node(i) {\n\t\talc = alien[i];\n\t\tif (alc) {\n\t\t\tLIST_HEAD(list);\n\n\t\t\tac = &alc->ac;\n\t\t\tspin_lock_irqsave(&alc->lock, flags);\n\t\t\t__drain_alien_cache(cachep, ac, i, &list);\n\t\t\tspin_unlock_irqrestore(&alc->lock, flags);\n\t\t\tslabs_destroy(cachep, &list);\n\t\t}\n\t}\n}\n\nstatic int __cache_free_alien(struct kmem_cache *cachep, void *objp,\n\t\t\t\tint node, int page_node)\n{\n\tstruct kmem_cache_node *n;\n\tstruct alien_cache *alien = NULL;\n\tstruct array_cache *ac;\n\tLIST_HEAD(list);\n\n\tn = get_node(cachep, node);\n\tSTATS_INC_NODEFREES(cachep);\n\tif (n->alien && n->alien[page_node]) {\n\t\talien = n->alien[page_node];\n\t\tac = &alien->ac;\n\t\tspin_lock(&alien->lock);\n\t\tif (unlikely(ac->avail == ac->limit)) {\n\t\t\tSTATS_INC_ACOVERFLOW(cachep);\n\t\t\t__drain_alien_cache(cachep, ac, page_node, &list);\n\t\t}\n\t\tac->entry[ac->avail++] = objp;\n\t\tspin_unlock(&alien->lock);\n\t\tslabs_destroy(cachep, &list);\n\t} else {\n\t\tn = get_node(cachep, page_node);\n\t\tspin_lock(&n->list_lock);\n\t\tfree_block(cachep, &objp, 1, page_node, &list);\n\t\tspin_unlock(&n->list_lock);\n\t\tslabs_destroy(cachep, &list);\n\t}\n\treturn 1;\n}\n\nstatic inline int cache_free_alien(struct kmem_cache *cachep, void *objp)\n{\n\tint page_node = page_to_nid(virt_to_page(objp));\n\tint node = numa_mem_id();\n\t/*\n\t * Make sure we are not freeing a object from another node to the array\n\t * cache on this cpu.\n\t */\n\tif (likely(node == page_node))\n\t\treturn 0;\n\n\treturn __cache_free_alien(cachep, objp, node, page_node);\n}\n\n/*\n * Construct gfp mask to allocate from a specific node but do not reclaim or\n * warn about failures.\n */\nstatic inline gfp_t gfp_exact_node(gfp_t flags)\n{\n\treturn (flags | __GFP_THISNODE | __GFP_NOWARN) & ~(__GFP_RECLAIM|__GFP_NOFAIL);\n}\n#endif\n\nstatic int init_cache_node(struct kmem_cache *cachep, int node, gfp_t gfp)\n{\n\tstruct kmem_cache_node *n;\n\n\t/*\n\t * Set up the kmem_cache_node for cpu before we can\n\t * begin anything. Make sure some other cpu on this\n\t * node has not already allocated this\n\t */\n\tn = get_node(cachep, node);\n\tif (n) {\n\t\tspin_lock_irq(&n->list_lock);\n\t\tn->free_limit = (1 + nr_cpus_node(node)) * cachep->batchcount +\n\t\t\t\tcachep->num;\n\t\tspin_unlock_irq(&n->list_lock);\n\n\t\treturn 0;\n\t}\n\n\tn = kmalloc_node(sizeof(struct kmem_cache_node), gfp, node);\n\tif (!n)\n\t\treturn -ENOMEM;\n\n\tkmem_cache_node_init(n);\n\tn->next_reap = jiffies + REAPTIMEOUT_NODE +\n\t\t    ((unsigned long)cachep) % REAPTIMEOUT_NODE;\n\n\tn->free_limit =\n\t\t(1 + nr_cpus_node(node)) * cachep->batchcount + cachep->num;\n\n\t/*\n\t * The kmem_cache_nodes don't come and go as CPUs\n\t * come and go.  slab_mutex is sufficient\n\t * protection here.\n\t */\n\tcachep->node[node] = n;\n\n\treturn 0;\n}\n\n#if (defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)) || defined(CONFIG_SMP)\n/*\n * Allocates and initializes node for a node on each slab cache, used for\n * either memory or cpu hotplug.  If memory is being hot-added, the kmem_cache_node\n * will be allocated off-node since memory is not yet online for the new node.\n * When hotplugging memory or a cpu, existing node are not replaced if\n * already in use.\n *\n * Must hold slab_mutex.\n */\nstatic int init_cache_node_node(int node)\n{\n\tint ret;\n\tstruct kmem_cache *cachep;\n\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tret = init_cache_node(cachep, node, GFP_KERNEL);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic int setup_kmem_cache_node(struct kmem_cache *cachep,\n\t\t\t\tint node, gfp_t gfp, bool force_change)\n{\n\tint ret = -ENOMEM;\n\tstruct kmem_cache_node *n;\n\tstruct array_cache *old_shared = NULL;\n\tstruct array_cache *new_shared = NULL;\n\tstruct alien_cache **new_alien = NULL;\n\tLIST_HEAD(list);\n\n\tif (use_alien_caches) {\n\t\tnew_alien = alloc_alien_cache(node, cachep->limit, gfp);\n\t\tif (!new_alien)\n\t\t\tgoto fail;\n\t}\n\n\tif (cachep->shared) {\n\t\tnew_shared = alloc_arraycache(node,\n\t\t\tcachep->shared * cachep->batchcount, 0xbaadf00d, gfp);\n\t\tif (!new_shared)\n\t\t\tgoto fail;\n\t}\n\n\tret = init_cache_node(cachep, node, gfp);\n\tif (ret)\n\t\tgoto fail;\n\n\tn = get_node(cachep, node);\n\tspin_lock_irq(&n->list_lock);\n\tif (n->shared && force_change) {\n\t\tfree_block(cachep, n->shared->entry,\n\t\t\t\tn->shared->avail, node, &list);\n\t\tn->shared->avail = 0;\n\t}\n\n\tif (!n->shared || force_change) {\n\t\told_shared = n->shared;\n\t\tn->shared = new_shared;\n\t\tnew_shared = NULL;\n\t}\n\n\tif (!n->alien) {\n\t\tn->alien = new_alien;\n\t\tnew_alien = NULL;\n\t}\n\n\tspin_unlock_irq(&n->list_lock);\n\tslabs_destroy(cachep, &list);\n\n\t/*\n\t * To protect lockless access to n->shared during irq disabled context.\n\t * If n->shared isn't NULL in irq disabled context, accessing to it is\n\t * guaranteed to be valid until irq is re-enabled, because it will be\n\t * freed after synchronize_sched().\n\t */\n\tif (old_shared && force_change)\n\t\tsynchronize_sched();\n\nfail:\n\tkfree(old_shared);\n\tkfree(new_shared);\n\tfree_alien_cache(new_alien);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_SMP\n\nstatic void cpuup_canceled(long cpu)\n{\n\tstruct kmem_cache *cachep;\n\tstruct kmem_cache_node *n = NULL;\n\tint node = cpu_to_mem(cpu);\n\tconst struct cpumask *mask = cpumask_of_node(node);\n\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tstruct array_cache *nc;\n\t\tstruct array_cache *shared;\n\t\tstruct alien_cache **alien;\n\t\tLIST_HEAD(list);\n\n\t\tn = get_node(cachep, node);\n\t\tif (!n)\n\t\t\tcontinue;\n\n\t\tspin_lock_irq(&n->list_lock);\n\n\t\t/* Free limit for this kmem_cache_node */\n\t\tn->free_limit -= cachep->batchcount;\n\n\t\t/* cpu is dead; no one can alloc from it. */\n\t\tnc = per_cpu_ptr(cachep->cpu_cache, cpu);\n\t\tif (nc) {\n\t\t\tfree_block(cachep, nc->entry, nc->avail, node, &list);\n\t\t\tnc->avail = 0;\n\t\t}\n\n\t\tif (!cpumask_empty(mask)) {\n\t\t\tspin_unlock_irq(&n->list_lock);\n\t\t\tgoto free_slab;\n\t\t}\n\n\t\tshared = n->shared;\n\t\tif (shared) {\n\t\t\tfree_block(cachep, shared->entry,\n\t\t\t\t   shared->avail, node, &list);\n\t\t\tn->shared = NULL;\n\t\t}\n\n\t\talien = n->alien;\n\t\tn->alien = NULL;\n\n\t\tspin_unlock_irq(&n->list_lock);\n\n\t\tkfree(shared);\n\t\tif (alien) {\n\t\t\tdrain_alien_cache(cachep, alien);\n\t\t\tfree_alien_cache(alien);\n\t\t}\n\nfree_slab:\n\t\tslabs_destroy(cachep, &list);\n\t}\n\t/*\n\t * In the previous loop, all the objects were freed to\n\t * the respective cache's slabs,  now we can go ahead and\n\t * shrink each nodelist to its limit.\n\t */\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tn = get_node(cachep, node);\n\t\tif (!n)\n\t\t\tcontinue;\n\t\tdrain_freelist(cachep, n, INT_MAX);\n\t}\n}\n\nstatic int cpuup_prepare(long cpu)\n{\n\tstruct kmem_cache *cachep;\n\tint node = cpu_to_mem(cpu);\n\tint err;\n\n\t/*\n\t * We need to do this right in the beginning since\n\t * alloc_arraycache's are going to use this list.\n\t * kmalloc_node allows us to add the slab to the right\n\t * kmem_cache_node and not this cpu's kmem_cache_node\n\t */\n\terr = init_cache_node_node(node);\n\tif (err < 0)\n\t\tgoto bad;\n\n\t/*\n\t * Now we can go ahead with allocating the shared arrays and\n\t * array caches\n\t */\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\terr = setup_kmem_cache_node(cachep, node, GFP_KERNEL, false);\n\t\tif (err)\n\t\t\tgoto bad;\n\t}\n\n\treturn 0;\nbad:\n\tcpuup_canceled(cpu);\n\treturn -ENOMEM;\n}\n\nint slab_prepare_cpu(unsigned int cpu)\n{\n\tint err;\n\n\tmutex_lock(&slab_mutex);\n\terr = cpuup_prepare(cpu);\n\tmutex_unlock(&slab_mutex);\n\treturn err;\n}\n\n/*\n * This is called for a failed online attempt and for a successful\n * offline.\n *\n * Even if all the cpus of a node are down, we don't free the\n * kmem_list3 of any cache. This to avoid a race between cpu_down, and\n * a kmalloc allocation from another cpu for memory from the node of\n * the cpu going down.  The list3 structure is usually allocated from\n * kmem_cache_create() and gets destroyed at kmem_cache_destroy().\n */\nint slab_dead_cpu(unsigned int cpu)\n{\n\tmutex_lock(&slab_mutex);\n\tcpuup_canceled(cpu);\n\tmutex_unlock(&slab_mutex);\n\treturn 0;\n}\n#endif\n\nstatic int slab_online_cpu(unsigned int cpu)\n{\n\tstart_cpu_timer(cpu);\n\treturn 0;\n}\n\nstatic int slab_offline_cpu(unsigned int cpu)\n{\n\t/*\n\t * Shutdown cache reaper. Note that the slab_mutex is held so\n\t * that if cache_reap() is invoked it cannot do anything\n\t * expensive but will only modify reap_work and reschedule the\n\t * timer.\n\t */\n\tcancel_delayed_work_sync(&per_cpu(slab_reap_work, cpu));\n\t/* Now the cache_reaper is guaranteed to be not running. */\n\tper_cpu(slab_reap_work, cpu).work.func = NULL;\n\treturn 0;\n}\n\n#if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\n/*\n * Drains freelist for a node on each slab cache, used for memory hot-remove.\n * Returns -EBUSY if all objects cannot be drained so that the node is not\n * removed.\n *\n * Must hold slab_mutex.\n */\nstatic int __meminit drain_cache_node_node(int node)\n{\n\tstruct kmem_cache *cachep;\n\tint ret = 0;\n\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tstruct kmem_cache_node *n;\n\n\t\tn = get_node(cachep, node);\n\t\tif (!n)\n\t\t\tcontinue;\n\n\t\tdrain_freelist(cachep, n, INT_MAX);\n\n\t\tif (!list_empty(&n->slabs_full) ||\n\t\t    !list_empty(&n->slabs_partial)) {\n\t\t\tret = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int __meminit slab_memory_callback(struct notifier_block *self,\n\t\t\t\t\tunsigned long action, void *arg)\n{\n\tstruct memory_notify *mnb = arg;\n\tint ret = 0;\n\tint nid;\n\n\tnid = mnb->status_change_nid;\n\tif (nid < 0)\n\t\tgoto out;\n\n\tswitch (action) {\n\tcase MEM_GOING_ONLINE:\n\t\tmutex_lock(&slab_mutex);\n\t\tret = init_cache_node_node(nid);\n\t\tmutex_unlock(&slab_mutex);\n\t\tbreak;\n\tcase MEM_GOING_OFFLINE:\n\t\tmutex_lock(&slab_mutex);\n\t\tret = drain_cache_node_node(nid);\n\t\tmutex_unlock(&slab_mutex);\n\t\tbreak;\n\tcase MEM_ONLINE:\n\tcase MEM_OFFLINE:\n\tcase MEM_CANCEL_ONLINE:\n\tcase MEM_CANCEL_OFFLINE:\n\t\tbreak;\n\t}\nout:\n\treturn notifier_from_errno(ret);\n}\n#endif /* CONFIG_NUMA && CONFIG_MEMORY_HOTPLUG */\n\n/*\n * swap the static kmem_cache_node with kmalloced memory\n */\nstatic void __init init_list(struct kmem_cache *cachep, struct kmem_cache_node *list,\n\t\t\t\tint nodeid)\n{\n\tstruct kmem_cache_node *ptr;\n\n\tptr = kmalloc_node(sizeof(struct kmem_cache_node), GFP_NOWAIT, nodeid);\n\tBUG_ON(!ptr);\n\n\tmemcpy(ptr, list, sizeof(struct kmem_cache_node));\n\t/*\n\t * Do not assume that spinlocks can be initialized via memcpy:\n\t */\n\tspin_lock_init(&ptr->list_lock);\n\n\tMAKE_ALL_LISTS(cachep, ptr, nodeid);\n\tcachep->node[nodeid] = ptr;\n}\n\n/*\n * For setting up all the kmem_cache_node for cache whose buffer_size is same as\n * size of kmem_cache_node.\n */\nstatic void __init set_up_node(struct kmem_cache *cachep, int index)\n{\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tcachep->node[node] = &init_kmem_cache_node[index + node];\n\t\tcachep->node[node]->next_reap = jiffies +\n\t\t    REAPTIMEOUT_NODE +\n\t\t    ((unsigned long)cachep) % REAPTIMEOUT_NODE;\n\t}\n}\n\n/*\n * Initialisation.  Called after the page allocator have been initialised and\n * before smp_init().\n */\nvoid __init kmem_cache_init(void)\n{\n\tint i;\n\n\tBUILD_BUG_ON(sizeof(((struct page *)NULL)->lru) <\n\t\t\t\t\tsizeof(struct rcu_head));\n\tkmem_cache = &kmem_cache_boot;\n\n\tif (!IS_ENABLED(CONFIG_NUMA) || num_possible_nodes() == 1)\n\t\tuse_alien_caches = 0;\n\n\tfor (i = 0; i < NUM_INIT_LISTS; i++)\n\t\tkmem_cache_node_init(&init_kmem_cache_node[i]);\n\n\t/*\n\t * Fragmentation resistance on low memory - only use bigger\n\t * page orders on machines with more than 32MB of memory if\n\t * not overridden on the command line.\n\t */\n\tif (!slab_max_order_set && totalram_pages > (32 << 20) >> PAGE_SHIFT)\n\t\tslab_max_order = SLAB_MAX_ORDER_HI;\n\n\t/* Bootstrap is tricky, because several objects are allocated\n\t * from caches that do not exist yet:\n\t * 1) initialize the kmem_cache cache: it contains the struct\n\t *    kmem_cache structures of all caches, except kmem_cache itself:\n\t *    kmem_cache is statically allocated.\n\t *    Initially an __init data area is used for the head array and the\n\t *    kmem_cache_node structures, it's replaced with a kmalloc allocated\n\t *    array at the end of the bootstrap.\n\t * 2) Create the first kmalloc cache.\n\t *    The struct kmem_cache for the new cache is allocated normally.\n\t *    An __init data area is used for the head array.\n\t * 3) Create the remaining kmalloc caches, with minimally sized\n\t *    head arrays.\n\t * 4) Replace the __init data head arrays for kmem_cache and the first\n\t *    kmalloc cache with kmalloc allocated arrays.\n\t * 5) Replace the __init data for kmem_cache_node for kmem_cache and\n\t *    the other cache's with kmalloc allocated memory.\n\t * 6) Resize the head arrays of the kmalloc caches to their final sizes.\n\t */\n\n\t/* 1) create the kmem_cache */\n\n\t/*\n\t * struct kmem_cache size depends on nr_node_ids & nr_cpu_ids\n\t */\n\tcreate_boot_cache(kmem_cache, \"kmem_cache\",\n\t\toffsetof(struct kmem_cache, node) +\n\t\t\t\t  nr_node_ids * sizeof(struct kmem_cache_node *),\n\t\t\t\t  SLAB_HWCACHE_ALIGN);\n\tlist_add(&kmem_cache->list, &slab_caches);\n\tslab_state = PARTIAL;\n\n\t/*\n\t * Initialize the caches that provide memory for the  kmem_cache_node\n\t * structures first.  Without this, further allocations will bug.\n\t */\n\tkmalloc_caches[INDEX_NODE] = create_kmalloc_cache(\"kmalloc-node\",\n\t\t\t\tkmalloc_size(INDEX_NODE), ARCH_KMALLOC_FLAGS);\n\tslab_state = PARTIAL_NODE;\n\tsetup_kmalloc_cache_index_table();\n\n\tslab_early_init = 0;\n\n\t/* 5) Replace the bootstrap kmem_cache_node */\n\t{\n\t\tint nid;\n\n\t\tfor_each_online_node(nid) {\n\t\t\tinit_list(kmem_cache, &init_kmem_cache_node[CACHE_CACHE + nid], nid);\n\n\t\t\tinit_list(kmalloc_caches[INDEX_NODE],\n\t\t\t\t\t  &init_kmem_cache_node[SIZE_NODE + nid], nid);\n\t\t}\n\t}\n\n\tcreate_kmalloc_caches(ARCH_KMALLOC_FLAGS);\n}\n\nvoid __init kmem_cache_init_late(void)\n{\n\tstruct kmem_cache *cachep;\n\n\tslab_state = UP;\n\n\t/* 6) resize the head arrays to their final sizes */\n\tmutex_lock(&slab_mutex);\n\tlist_for_each_entry(cachep, &slab_caches, list)\n\t\tif (enable_cpucache(cachep, GFP_NOWAIT))\n\t\t\tBUG();\n\tmutex_unlock(&slab_mutex);\n\n\t/* Done! */\n\tslab_state = FULL;\n\n#ifdef CONFIG_NUMA\n\t/*\n\t * Register a memory hotplug callback that initializes and frees\n\t * node.\n\t */\n\thotplug_memory_notifier(slab_memory_callback, SLAB_CALLBACK_PRI);\n#endif\n\n\t/*\n\t * The reap timers are started later, with a module init call: That part\n\t * of the kernel is not yet operational.\n\t */\n}\n\nstatic int __init cpucache_init(void)\n{\n\tint ret;\n\n\t/*\n\t * Register the timers that return unneeded pages to the page allocator\n\t */\n\tret = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, \"SLAB online\",\n\t\t\t\tslab_online_cpu, slab_offline_cpu);\n\tWARN_ON(ret < 0);\n\n\t/* Done! */\n\tslab_state = FULL;\n\treturn 0;\n}\n__initcall(cpucache_init);\n\nstatic noinline void\nslab_out_of_memory(struct kmem_cache *cachep, gfp_t gfpflags, int nodeid)\n{\n#if DEBUG\n\tstruct kmem_cache_node *n;\n\tunsigned long flags;\n\tint node;\n\tstatic DEFINE_RATELIMIT_STATE(slab_oom_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\n\tif ((gfpflags & __GFP_NOWARN) || !__ratelimit(&slab_oom_rs))\n\t\treturn;\n\n\tpr_warn(\"SLAB: Unable to allocate memory on node %d, gfp=%#x(%pGg)\\n\",\n\t\tnodeid, gfpflags, &gfpflags);\n\tpr_warn(\"  cache: %s, object size: %d, order: %d\\n\",\n\t\tcachep->name, cachep->size, cachep->gfporder);\n\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tunsigned long total_slabs, free_slabs, free_objs;\n\n\t\tspin_lock_irqsave(&n->list_lock, flags);\n\t\ttotal_slabs = n->total_slabs;\n\t\tfree_slabs = n->free_slabs;\n\t\tfree_objs = n->free_objects;\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\n\t\tpr_warn(\"  node %d: slabs: %ld/%ld, objs: %ld/%ld\\n\",\n\t\t\tnode, total_slabs - free_slabs, total_slabs,\n\t\t\t(total_slabs * cachep->num) - free_objs,\n\t\t\ttotal_slabs * cachep->num);\n\t}\n#endif\n}\n\n/*\n * Interface to system's page allocator. No need to hold the\n * kmem_cache_node ->list_lock.\n *\n * If we requested dmaable memory, we will get it. Even if we\n * did not request dmaable memory, we might get it, but that\n * would be relatively rare and ignorable.\n */\nstatic struct page *kmem_getpages(struct kmem_cache *cachep, gfp_t flags,\n\t\t\t\t\t\t\t\tint nodeid)\n{\n\tstruct page *page;\n\tint nr_pages;\n\n\tflags |= cachep->allocflags;\n\tif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\n\t\tflags |= __GFP_RECLAIMABLE;\n\n\tpage = __alloc_pages_node(nodeid, flags | __GFP_NOTRACK, cachep->gfporder);\n\tif (!page) {\n\t\tslab_out_of_memory(cachep, flags, nodeid);\n\t\treturn NULL;\n\t}\n\n\tif (memcg_charge_slab(page, flags, cachep->gfporder, cachep)) {\n\t\t__free_pages(page, cachep->gfporder);\n\t\treturn NULL;\n\t}\n\n\tnr_pages = (1 << cachep->gfporder);\n\tif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\n\t\tadd_zone_page_state(page_zone(page),\n\t\t\tNR_SLAB_RECLAIMABLE, nr_pages);\n\telse\n\t\tadd_zone_page_state(page_zone(page),\n\t\t\tNR_SLAB_UNRECLAIMABLE, nr_pages);\n\n\t__SetPageSlab(page);\n\t/* Record if ALLOC_NO_WATERMARKS was set when allocating the slab */\n\tif (sk_memalloc_socks() && page_is_pfmemalloc(page))\n\t\tSetPageSlabPfmemalloc(page);\n\n\tif (kmemcheck_enabled && !(cachep->flags & SLAB_NOTRACK)) {\n\t\tkmemcheck_alloc_shadow(page, cachep->gfporder, flags, nodeid);\n\n\t\tif (cachep->ctor)\n\t\t\tkmemcheck_mark_uninitialized_pages(page, nr_pages);\n\t\telse\n\t\t\tkmemcheck_mark_unallocated_pages(page, nr_pages);\n\t}\n\n\treturn page;\n}\n\n/*\n * Interface to system's page release.\n */\nstatic void kmem_freepages(struct kmem_cache *cachep, struct page *page)\n{\n\tint order = cachep->gfporder;\n\tunsigned long nr_freed = (1 << order);\n\n\tkmemcheck_free_shadow(page, order);\n\n\tif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\n\t\tsub_zone_page_state(page_zone(page),\n\t\t\t\tNR_SLAB_RECLAIMABLE, nr_freed);\n\telse\n\t\tsub_zone_page_state(page_zone(page),\n\t\t\t\tNR_SLAB_UNRECLAIMABLE, nr_freed);\n\n\tBUG_ON(!PageSlab(page));\n\t__ClearPageSlabPfmemalloc(page);\n\t__ClearPageSlab(page);\n\tpage_mapcount_reset(page);\n\tpage->mapping = NULL;\n\n\tif (current->reclaim_state)\n\t\tcurrent->reclaim_state->reclaimed_slab += nr_freed;\n\tmemcg_uncharge_slab(page, order, cachep);\n\t__free_pages(page, order);\n}\n\nstatic void kmem_rcu_free(struct rcu_head *head)\n{\n\tstruct kmem_cache *cachep;\n\tstruct page *page;\n\n\tpage = container_of(head, struct page, rcu_head);\n\tcachep = page->slab_cache;\n\n\tkmem_freepages(cachep, page);\n}\n\n#if DEBUG\nstatic bool is_debug_pagealloc_cache(struct kmem_cache *cachep)\n{\n\tif (debug_pagealloc_enabled() && OFF_SLAB(cachep) &&\n\t\t(cachep->size % PAGE_SIZE) == 0)\n\t\treturn true;\n\n\treturn false;\n}\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nstatic void store_stackinfo(struct kmem_cache *cachep, unsigned long *addr,\n\t\t\t    unsigned long caller)\n{\n\tint size = cachep->object_size;\n\n\taddr = (unsigned long *)&((char *)addr)[obj_offset(cachep)];\n\n\tif (size < 5 * sizeof(unsigned long))\n\t\treturn;\n\n\t*addr++ = 0x12345678;\n\t*addr++ = caller;\n\t*addr++ = smp_processor_id();\n\tsize -= 3 * sizeof(unsigned long);\n\t{\n\t\tunsigned long *sptr = &caller;\n\t\tunsigned long svalue;\n\n\t\twhile (!kstack_end(sptr)) {\n\t\t\tsvalue = *sptr++;\n\t\t\tif (kernel_text_address(svalue)) {\n\t\t\t\t*addr++ = svalue;\n\t\t\t\tsize -= sizeof(unsigned long);\n\t\t\t\tif (size <= sizeof(unsigned long))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t}\n\t*addr++ = 0x87654321;\n}\n\nstatic void slab_kernel_map(struct kmem_cache *cachep, void *objp,\n\t\t\t\tint map, unsigned long caller)\n{\n\tif (!is_debug_pagealloc_cache(cachep))\n\t\treturn;\n\n\tif (caller)\n\t\tstore_stackinfo(cachep, objp, caller);\n\n\tkernel_map_pages(virt_to_page(objp), cachep->size / PAGE_SIZE, map);\n}\n\n#else\nstatic inline void slab_kernel_map(struct kmem_cache *cachep, void *objp,\n\t\t\t\tint map, unsigned long caller) {}\n\n#endif\n\nstatic void poison_obj(struct kmem_cache *cachep, void *addr, unsigned char val)\n{\n\tint size = cachep->object_size;\n\taddr = &((char *)addr)[obj_offset(cachep)];\n\n\tmemset(addr, val, size);\n\t*(unsigned char *)(addr + size - 1) = POISON_END;\n}\n\nstatic void dump_line(char *data, int offset, int limit)\n{\n\tint i;\n\tunsigned char error = 0;\n\tint bad_count = 0;\n\n\tpr_err(\"%03x: \", offset);\n\tfor (i = 0; i < limit; i++) {\n\t\tif (data[offset + i] != POISON_FREE) {\n\t\t\terror = data[offset + i];\n\t\t\tbad_count++;\n\t\t}\n\t}\n\tprint_hex_dump(KERN_CONT, \"\", 0, 16, 1,\n\t\t\t&data[offset], limit, 1);\n\n\tif (bad_count == 1) {\n\t\terror ^= POISON_FREE;\n\t\tif (!(error & (error - 1))) {\n\t\t\tpr_err(\"Single bit error detected. Probably bad RAM.\\n\");\n#ifdef CONFIG_X86\n\t\t\tpr_err(\"Run memtest86+ or a similar memory test tool.\\n\");\n#else\n\t\t\tpr_err(\"Run a memory test tool.\\n\");\n#endif\n\t\t}\n\t}\n}\n#endif\n\n#if DEBUG\n\nstatic void print_objinfo(struct kmem_cache *cachep, void *objp, int lines)\n{\n\tint i, size;\n\tchar *realobj;\n\n\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\tpr_err(\"Redzone: 0x%llx/0x%llx\\n\",\n\t\t       *dbg_redzone1(cachep, objp),\n\t\t       *dbg_redzone2(cachep, objp));\n\t}\n\n\tif (cachep->flags & SLAB_STORE_USER) {\n\t\tpr_err(\"Last user: [<%p>](%pSR)\\n\",\n\t\t       *dbg_userword(cachep, objp),\n\t\t       *dbg_userword(cachep, objp));\n\t}\n\trealobj = (char *)objp + obj_offset(cachep);\n\tsize = cachep->object_size;\n\tfor (i = 0; i < size && lines; i += 16, lines--) {\n\t\tint limit;\n\t\tlimit = 16;\n\t\tif (i + limit > size)\n\t\t\tlimit = size - i;\n\t\tdump_line(realobj, i, limit);\n\t}\n}\n\nstatic void check_poison_obj(struct kmem_cache *cachep, void *objp)\n{\n\tchar *realobj;\n\tint size, i;\n\tint lines = 0;\n\n\tif (is_debug_pagealloc_cache(cachep))\n\t\treturn;\n\n\trealobj = (char *)objp + obj_offset(cachep);\n\tsize = cachep->object_size;\n\n\tfor (i = 0; i < size; i++) {\n\t\tchar exp = POISON_FREE;\n\t\tif (i == size - 1)\n\t\t\texp = POISON_END;\n\t\tif (realobj[i] != exp) {\n\t\t\tint limit;\n\t\t\t/* Mismatch ! */\n\t\t\t/* Print header */\n\t\t\tif (lines == 0) {\n\t\t\t\tpr_err(\"Slab corruption (%s): %s start=%p, len=%d\\n\",\n\t\t\t\t       print_tainted(), cachep->name,\n\t\t\t\t       realobj, size);\n\t\t\t\tprint_objinfo(cachep, objp, 0);\n\t\t\t}\n\t\t\t/* Hexdump the affected line */\n\t\t\ti = (i / 16) * 16;\n\t\t\tlimit = 16;\n\t\t\tif (i + limit > size)\n\t\t\t\tlimit = size - i;\n\t\t\tdump_line(realobj, i, limit);\n\t\t\ti += 16;\n\t\t\tlines++;\n\t\t\t/* Limit to 5 lines */\n\t\t\tif (lines > 5)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (lines != 0) {\n\t\t/* Print some data about the neighboring objects, if they\n\t\t * exist:\n\t\t */\n\t\tstruct page *page = virt_to_head_page(objp);\n\t\tunsigned int objnr;\n\n\t\tobjnr = obj_to_index(cachep, page, objp);\n\t\tif (objnr) {\n\t\t\tobjp = index_to_obj(cachep, page, objnr - 1);\n\t\t\trealobj = (char *)objp + obj_offset(cachep);\n\t\t\tpr_err(\"Prev obj: start=%p, len=%d\\n\", realobj, size);\n\t\t\tprint_objinfo(cachep, objp, 2);\n\t\t}\n\t\tif (objnr + 1 < cachep->num) {\n\t\t\tobjp = index_to_obj(cachep, page, objnr + 1);\n\t\t\trealobj = (char *)objp + obj_offset(cachep);\n\t\t\tpr_err(\"Next obj: start=%p, len=%d\\n\", realobj, size);\n\t\t\tprint_objinfo(cachep, objp, 2);\n\t\t}\n\t}\n}\n#endif\n\n#if DEBUG\nstatic void slab_destroy_debugcheck(struct kmem_cache *cachep,\n\t\t\t\t\t\tstruct page *page)\n{\n\tint i;\n\n\tif (OBJFREELIST_SLAB(cachep) && cachep->flags & SLAB_POISON) {\n\t\tpoison_obj(cachep, page->freelist - obj_offset(cachep),\n\t\t\tPOISON_FREE);\n\t}\n\n\tfor (i = 0; i < cachep->num; i++) {\n\t\tvoid *objp = index_to_obj(cachep, page, i);\n\n\t\tif (cachep->flags & SLAB_POISON) {\n\t\t\tcheck_poison_obj(cachep, objp);\n\t\t\tslab_kernel_map(cachep, objp, 1, 0);\n\t\t}\n\t\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\t\tif (*dbg_redzone1(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"start of a freed object was overwritten\");\n\t\t\tif (*dbg_redzone2(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"end of a freed object was overwritten\");\n\t\t}\n\t}\n}\n#else\nstatic void slab_destroy_debugcheck(struct kmem_cache *cachep,\n\t\t\t\t\t\tstruct page *page)\n{\n}\n#endif\n\n/**\n * slab_destroy - destroy and release all objects in a slab\n * @cachep: cache pointer being destroyed\n * @page: page pointer being destroyed\n *\n * Destroy all the objs in a slab page, and release the mem back to the system.\n * Before calling the slab page must have been unlinked from the cache. The\n * kmem_cache_node ->list_lock is not held/needed.\n */\nstatic void slab_destroy(struct kmem_cache *cachep, struct page *page)\n{\n\tvoid *freelist;\n\n\tfreelist = page->freelist;\n\tslab_destroy_debugcheck(cachep, page);\n\tif (unlikely(cachep->flags & SLAB_DESTROY_BY_RCU))\n\t\tcall_rcu(&page->rcu_head, kmem_rcu_free);\n\telse\n\t\tkmem_freepages(cachep, page);\n\n\t/*\n\t * From now on, we don't use freelist\n\t * although actual page can be freed in rcu context\n\t */\n\tif (OFF_SLAB(cachep))\n\t\tkmem_cache_free(cachep->freelist_cache, freelist);\n}\n\nstatic void slabs_destroy(struct kmem_cache *cachep, struct list_head *list)\n{\n\tstruct page *page, *n;\n\n\tlist_for_each_entry_safe(page, n, list, lru) {\n\t\tlist_del(&page->lru);\n\t\tslab_destroy(cachep, page);\n\t}\n}\n\n/**\n * calculate_slab_order - calculate size (page order) of slabs\n * @cachep: pointer to the cache that is being created\n * @size: size of objects to be created in this cache.\n * @flags: slab allocation flags\n *\n * Also calculates the number of objects per slab.\n *\n * This could be made much more intelligent.  For now, try to avoid using\n * high order pages for slabs.  When the gfp() functions are more friendly\n * towards high-order requests, this should be changed.\n */\nstatic size_t calculate_slab_order(struct kmem_cache *cachep,\n\t\t\t\tsize_t size, unsigned long flags)\n{\n\tsize_t left_over = 0;\n\tint gfporder;\n\n\tfor (gfporder = 0; gfporder <= KMALLOC_MAX_ORDER; gfporder++) {\n\t\tunsigned int num;\n\t\tsize_t remainder;\n\n\t\tnum = cache_estimate(gfporder, size, flags, &remainder);\n\t\tif (!num)\n\t\t\tcontinue;\n\n\t\t/* Can't handle number of objects more than SLAB_OBJ_MAX_NUM */\n\t\tif (num > SLAB_OBJ_MAX_NUM)\n\t\t\tbreak;\n\n\t\tif (flags & CFLGS_OFF_SLAB) {\n\t\t\tstruct kmem_cache *freelist_cache;\n\t\t\tsize_t freelist_size;\n\n\t\t\tfreelist_size = num * sizeof(freelist_idx_t);\n\t\t\tfreelist_cache = kmalloc_slab(freelist_size, 0u);\n\t\t\tif (!freelist_cache)\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * Needed to avoid possible looping condition\n\t\t\t * in cache_grow_begin()\n\t\t\t */\n\t\t\tif (OFF_SLAB(freelist_cache))\n\t\t\t\tcontinue;\n\n\t\t\t/* check if off slab has enough benefit */\n\t\t\tif (freelist_cache->size > cachep->size / 2)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t/* Found something acceptable - save it away */\n\t\tcachep->num = num;\n\t\tcachep->gfporder = gfporder;\n\t\tleft_over = remainder;\n\n\t\t/*\n\t\t * A VFS-reclaimable slab tends to have most allocations\n\t\t * as GFP_NOFS and we really don't want to have to be allocating\n\t\t * higher-order pages when we are unable to shrink dcache.\n\t\t */\n\t\tif (flags & SLAB_RECLAIM_ACCOUNT)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Large number of objects is good, but very large slabs are\n\t\t * currently bad for the gfp()s.\n\t\t */\n\t\tif (gfporder >= slab_max_order)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Acceptable internal fragmentation?\n\t\t */\n\t\tif (left_over * 8 <= (PAGE_SIZE << gfporder))\n\t\t\tbreak;\n\t}\n\treturn left_over;\n}\n\nstatic struct array_cache __percpu *alloc_kmem_cache_cpus(\n\t\tstruct kmem_cache *cachep, int entries, int batchcount)\n{\n\tint cpu;\n\tsize_t size;\n\tstruct array_cache __percpu *cpu_cache;\n\n\tsize = sizeof(void *) * entries + sizeof(struct array_cache);\n\tcpu_cache = __alloc_percpu(size, sizeof(void *));\n\n\tif (!cpu_cache)\n\t\treturn NULL;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tinit_arraycache(per_cpu_ptr(cpu_cache, cpu),\n\t\t\t\tentries, batchcount);\n\t}\n\n\treturn cpu_cache;\n}\n\nstatic int __ref setup_cpu_cache(struct kmem_cache *cachep, gfp_t gfp)\n{\n\tif (slab_state >= FULL)\n\t\treturn enable_cpucache(cachep, gfp);\n\n\tcachep->cpu_cache = alloc_kmem_cache_cpus(cachep, 1, 1);\n\tif (!cachep->cpu_cache)\n\t\treturn 1;\n\n\tif (slab_state == DOWN) {\n\t\t/* Creation of first cache (kmem_cache). */\n\t\tset_up_node(kmem_cache, CACHE_CACHE);\n\t} else if (slab_state == PARTIAL) {\n\t\t/* For kmem_cache_node */\n\t\tset_up_node(cachep, SIZE_NODE);\n\t} else {\n\t\tint node;\n\n\t\tfor_each_online_node(node) {\n\t\t\tcachep->node[node] = kmalloc_node(\n\t\t\t\tsizeof(struct kmem_cache_node), gfp, node);\n\t\t\tBUG_ON(!cachep->node[node]);\n\t\t\tkmem_cache_node_init(cachep->node[node]);\n\t\t}\n\t}\n\n\tcachep->node[numa_mem_id()]->next_reap =\n\t\t\tjiffies + REAPTIMEOUT_NODE +\n\t\t\t((unsigned long)cachep) % REAPTIMEOUT_NODE;\n\n\tcpu_cache_get(cachep)->avail = 0;\n\tcpu_cache_get(cachep)->limit = BOOT_CPUCACHE_ENTRIES;\n\tcpu_cache_get(cachep)->batchcount = 1;\n\tcpu_cache_get(cachep)->touched = 0;\n\tcachep->batchcount = 1;\n\tcachep->limit = BOOT_CPUCACHE_ENTRIES;\n\treturn 0;\n}\n\nunsigned long kmem_cache_flags(unsigned long object_size,\n\tunsigned long flags, const char *name,\n\tvoid (*ctor)(void *))\n{\n\treturn flags;\n}\n\nstruct kmem_cache *\n__kmem_cache_alias(const char *name, size_t size, size_t align,\n\t\t   unsigned long flags, void (*ctor)(void *))\n{\n\tstruct kmem_cache *cachep;\n\n\tcachep = find_mergeable(size, align, flags, name, ctor);\n\tif (cachep) {\n\t\tcachep->refcount++;\n\n\t\t/*\n\t\t * Adjust the object sizes so that we clear\n\t\t * the complete object on kzalloc.\n\t\t */\n\t\tcachep->object_size = max_t(int, cachep->object_size, size);\n\t}\n\treturn cachep;\n}\n\nstatic bool set_objfreelist_slab_cache(struct kmem_cache *cachep,\n\t\t\tsize_t size, unsigned long flags)\n{\n\tsize_t left;\n\n\tcachep->num = 0;\n\n\tif (cachep->ctor || flags & SLAB_DESTROY_BY_RCU)\n\t\treturn false;\n\n\tleft = calculate_slab_order(cachep, size,\n\t\t\tflags | CFLGS_OBJFREELIST_SLAB);\n\tif (!cachep->num)\n\t\treturn false;\n\n\tif (cachep->num * sizeof(freelist_idx_t) > cachep->object_size)\n\t\treturn false;\n\n\tcachep->colour = left / cachep->colour_off;\n\n\treturn true;\n}\n\nstatic bool set_off_slab_cache(struct kmem_cache *cachep,\n\t\t\tsize_t size, unsigned long flags)\n{\n\tsize_t left;\n\n\tcachep->num = 0;\n\n\t/*\n\t * Always use on-slab management when SLAB_NOLEAKTRACE\n\t * to avoid recursive calls into kmemleak.\n\t */\n\tif (flags & SLAB_NOLEAKTRACE)\n\t\treturn false;\n\n\t/*\n\t * Size is large, assume best to place the slab management obj\n\t * off-slab (should allow better packing of objs).\n\t */\n\tleft = calculate_slab_order(cachep, size, flags | CFLGS_OFF_SLAB);\n\tif (!cachep->num)\n\t\treturn false;\n\n\t/*\n\t * If the slab has been placed off-slab, and we have enough space then\n\t * move it on-slab. This is at the expense of any extra colouring.\n\t */\n\tif (left >= cachep->num * sizeof(freelist_idx_t))\n\t\treturn false;\n\n\tcachep->colour = left / cachep->colour_off;\n\n\treturn true;\n}\n\nstatic bool set_on_slab_cache(struct kmem_cache *cachep,\n\t\t\tsize_t size, unsigned long flags)\n{\n\tsize_t left;\n\n\tcachep->num = 0;\n\n\tleft = calculate_slab_order(cachep, size, flags);\n\tif (!cachep->num)\n\t\treturn false;\n\n\tcachep->colour = left / cachep->colour_off;\n\n\treturn true;\n}\n\n/**\n * __kmem_cache_create - Create a cache.\n * @cachep: cache management descriptor\n * @flags: SLAB flags\n *\n * Returns a ptr to the cache on success, NULL on failure.\n * Cannot be called within a int, but can be interrupted.\n * The @ctor is run when new pages are allocated by the cache.\n *\n * The flags are\n *\n * %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)\n * to catch references to uninitialised memory.\n *\n * %SLAB_RED_ZONE - Insert `Red' zones around the allocated memory to check\n * for buffer overruns.\n *\n * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware\n * cacheline.  This can be beneficial if you're counting cycles as closely\n * as davem.\n */\nint\n__kmem_cache_create (struct kmem_cache *cachep, unsigned long flags)\n{\n\tsize_t ralign = BYTES_PER_WORD;\n\tgfp_t gfp;\n\tint err;\n\tsize_t size = cachep->size;\n\n#if DEBUG\n#if FORCED_DEBUG\n\t/*\n\t * Enable redzoning and last user accounting, except for caches with\n\t * large objects, if the increased size would increase the object size\n\t * above the next power of two: caches with object sizes just above a\n\t * power of two have a significant amount of internal fragmentation.\n\t */\n\tif (size < 4096 || fls(size - 1) == fls(size-1 + REDZONE_ALIGN +\n\t\t\t\t\t\t2 * sizeof(unsigned long long)))\n\t\tflags |= SLAB_RED_ZONE | SLAB_STORE_USER;\n\tif (!(flags & SLAB_DESTROY_BY_RCU))\n\t\tflags |= SLAB_POISON;\n#endif\n#endif\n\n\t/*\n\t * Check that size is in terms of words.  This is needed to avoid\n\t * unaligned accesses for some archs when redzoning is used, and makes\n\t * sure any on-slab bufctl's are also correctly aligned.\n\t */\n\tif (size & (BYTES_PER_WORD - 1)) {\n\t\tsize += (BYTES_PER_WORD - 1);\n\t\tsize &= ~(BYTES_PER_WORD - 1);\n\t}\n\n\tif (flags & SLAB_RED_ZONE) {\n\t\tralign = REDZONE_ALIGN;\n\t\t/* If redzoning, ensure that the second redzone is suitably\n\t\t * aligned, by adjusting the object size accordingly. */\n\t\tsize += REDZONE_ALIGN - 1;\n\t\tsize &= ~(REDZONE_ALIGN - 1);\n\t}\n\n\t/* 3) caller mandated alignment */\n\tif (ralign < cachep->align) {\n\t\tralign = cachep->align;\n\t}\n\t/* disable debug if necessary */\n\tif (ralign > __alignof__(unsigned long long))\n\t\tflags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);\n\t/*\n\t * 4) Store it.\n\t */\n\tcachep->align = ralign;\n\tcachep->colour_off = cache_line_size();\n\t/* Offset must be a multiple of the alignment. */\n\tif (cachep->colour_off < cachep->align)\n\t\tcachep->colour_off = cachep->align;\n\n\tif (slab_is_available())\n\t\tgfp = GFP_KERNEL;\n\telse\n\t\tgfp = GFP_NOWAIT;\n\n#if DEBUG\n\n\t/*\n\t * Both debugging options require word-alignment which is calculated\n\t * into align above.\n\t */\n\tif (flags & SLAB_RED_ZONE) {\n\t\t/* add space for red zone words */\n\t\tcachep->obj_offset += sizeof(unsigned long long);\n\t\tsize += 2 * sizeof(unsigned long long);\n\t}\n\tif (flags & SLAB_STORE_USER) {\n\t\t/* user store requires one word storage behind the end of\n\t\t * the real object. But if the second red zone needs to be\n\t\t * aligned to 64 bits, we must allow that much space.\n\t\t */\n\t\tif (flags & SLAB_RED_ZONE)\n\t\t\tsize += REDZONE_ALIGN;\n\t\telse\n\t\t\tsize += BYTES_PER_WORD;\n\t}\n#endif\n\n\tkasan_cache_create(cachep, &size, &flags);\n\n\tsize = ALIGN(size, cachep->align);\n\t/*\n\t * We should restrict the number of objects in a slab to implement\n\t * byte sized index. Refer comment on SLAB_OBJ_MIN_SIZE definition.\n\t */\n\tif (FREELIST_BYTE_INDEX && size < SLAB_OBJ_MIN_SIZE)\n\t\tsize = ALIGN(SLAB_OBJ_MIN_SIZE, cachep->align);\n\n#if DEBUG\n\t/*\n\t * To activate debug pagealloc, off-slab management is necessary\n\t * requirement. In early phase of initialization, small sized slab\n\t * doesn't get initialized so it would not be possible. So, we need\n\t * to check size >= 256. It guarantees that all necessary small\n\t * sized slab is initialized in current slab initialization sequence.\n\t */\n\tif (debug_pagealloc_enabled() && (flags & SLAB_POISON) &&\n\t\tsize >= 256 && cachep->object_size > cache_line_size()) {\n\t\tif (size < PAGE_SIZE || size % PAGE_SIZE == 0) {\n\t\t\tsize_t tmp_size = ALIGN(size, PAGE_SIZE);\n\n\t\t\tif (set_off_slab_cache(cachep, tmp_size, flags)) {\n\t\t\t\tflags |= CFLGS_OFF_SLAB;\n\t\t\t\tcachep->obj_offset += tmp_size - size;\n\t\t\t\tsize = tmp_size;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\t}\n#endif\n\n\tif (set_objfreelist_slab_cache(cachep, size, flags)) {\n\t\tflags |= CFLGS_OBJFREELIST_SLAB;\n\t\tgoto done;\n\t}\n\n\tif (set_off_slab_cache(cachep, size, flags)) {\n\t\tflags |= CFLGS_OFF_SLAB;\n\t\tgoto done;\n\t}\n\n\tif (set_on_slab_cache(cachep, size, flags))\n\t\tgoto done;\n\n\treturn -E2BIG;\n\ndone:\n\tcachep->freelist_size = cachep->num * sizeof(freelist_idx_t);\n\tcachep->flags = flags;\n\tcachep->allocflags = __GFP_COMP;\n\tif (flags & SLAB_CACHE_DMA)\n\t\tcachep->allocflags |= GFP_DMA;\n\tcachep->size = size;\n\tcachep->reciprocal_buffer_size = reciprocal_value(size);\n\n#if DEBUG\n\t/*\n\t * If we're going to use the generic kernel_map_pages()\n\t * poisoning, then it's going to smash the contents of\n\t * the redzone and userword anyhow, so switch them off.\n\t */\n\tif (IS_ENABLED(CONFIG_PAGE_POISONING) &&\n\t\t(cachep->flags & SLAB_POISON) &&\n\t\tis_debug_pagealloc_cache(cachep))\n\t\tcachep->flags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);\n#endif\n\n\tif (OFF_SLAB(cachep)) {\n\t\tcachep->freelist_cache =\n\t\t\tkmalloc_slab(cachep->freelist_size, 0u);\n\t}\n\n\terr = setup_cpu_cache(cachep, gfp);\n\tif (err) {\n\t\t__kmem_cache_release(cachep);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n#if DEBUG\nstatic void check_irq_off(void)\n{\n\tBUG_ON(!irqs_disabled());\n}\n\nstatic void check_irq_on(void)\n{\n\tBUG_ON(irqs_disabled());\n}\n\nstatic void check_mutex_acquired(void)\n{\n\tBUG_ON(!mutex_is_locked(&slab_mutex));\n}\n\nstatic void check_spinlock_acquired(struct kmem_cache *cachep)\n{\n#ifdef CONFIG_SMP\n\tcheck_irq_off();\n\tassert_spin_locked(&get_node(cachep, numa_mem_id())->list_lock);\n#endif\n}\n\nstatic void check_spinlock_acquired_node(struct kmem_cache *cachep, int node)\n{\n#ifdef CONFIG_SMP\n\tcheck_irq_off();\n\tassert_spin_locked(&get_node(cachep, node)->list_lock);\n#endif\n}\n\n#else\n#define check_irq_off()\tdo { } while(0)\n#define check_irq_on()\tdo { } while(0)\n#define check_mutex_acquired()\tdo { } while(0)\n#define check_spinlock_acquired(x) do { } while(0)\n#define check_spinlock_acquired_node(x, y) do { } while(0)\n#endif\n\nstatic void drain_array_locked(struct kmem_cache *cachep, struct array_cache *ac,\n\t\t\t\tint node, bool free_all, struct list_head *list)\n{\n\tint tofree;\n\n\tif (!ac || !ac->avail)\n\t\treturn;\n\n\ttofree = free_all ? ac->avail : (ac->limit + 4) / 5;\n\tif (tofree > ac->avail)\n\t\ttofree = (ac->avail + 1) / 2;\n\n\tfree_block(cachep, ac->entry, tofree, node, list);\n\tac->avail -= tofree;\n\tmemmove(ac->entry, &(ac->entry[tofree]), sizeof(void *) * ac->avail);\n}\n\nstatic void do_drain(void *arg)\n{\n\tstruct kmem_cache *cachep = arg;\n\tstruct array_cache *ac;\n\tint node = numa_mem_id();\n\tstruct kmem_cache_node *n;\n\tLIST_HEAD(list);\n\n\tcheck_irq_off();\n\tac = cpu_cache_get(cachep);\n\tn = get_node(cachep, node);\n\tspin_lock(&n->list_lock);\n\tfree_block(cachep, ac->entry, ac->avail, node, &list);\n\tspin_unlock(&n->list_lock);\n\tslabs_destroy(cachep, &list);\n\tac->avail = 0;\n}\n\nstatic void drain_cpu_caches(struct kmem_cache *cachep)\n{\n\tstruct kmem_cache_node *n;\n\tint node;\n\tLIST_HEAD(list);\n\n\ton_each_cpu(do_drain, cachep, 1);\n\tcheck_irq_on();\n\tfor_each_kmem_cache_node(cachep, node, n)\n\t\tif (n->alien)\n\t\t\tdrain_alien_cache(cachep, n->alien);\n\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tspin_lock_irq(&n->list_lock);\n\t\tdrain_array_locked(cachep, n->shared, node, true, &list);\n\t\tspin_unlock_irq(&n->list_lock);\n\n\t\tslabs_destroy(cachep, &list);\n\t}\n}\n\n/*\n * Remove slabs from the list of free slabs.\n * Specify the number of slabs to drain in tofree.\n *\n * Returns the actual number of slabs released.\n */\nstatic int drain_freelist(struct kmem_cache *cache,\n\t\t\tstruct kmem_cache_node *n, int tofree)\n{\n\tstruct list_head *p;\n\tint nr_freed;\n\tstruct page *page;\n\n\tnr_freed = 0;\n\twhile (nr_freed < tofree && !list_empty(&n->slabs_free)) {\n\n\t\tspin_lock_irq(&n->list_lock);\n\t\tp = n->slabs_free.prev;\n\t\tif (p == &n->slabs_free) {\n\t\t\tspin_unlock_irq(&n->list_lock);\n\t\t\tgoto out;\n\t\t}\n\n\t\tpage = list_entry(p, struct page, lru);\n\t\tlist_del(&page->lru);\n\t\tn->free_slabs--;\n\t\tn->total_slabs--;\n\t\t/*\n\t\t * Safe to drop the lock. The slab is no longer linked\n\t\t * to the cache.\n\t\t */\n\t\tn->free_objects -= cache->num;\n\t\tspin_unlock_irq(&n->list_lock);\n\t\tslab_destroy(cache, page);\n\t\tnr_freed++;\n\t}\nout:\n\treturn nr_freed;\n}\n\nint __kmem_cache_shrink(struct kmem_cache *cachep)\n{\n\tint ret = 0;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tdrain_cpu_caches(cachep);\n\n\tcheck_irq_on();\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tdrain_freelist(cachep, n, INT_MAX);\n\n\t\tret += !list_empty(&n->slabs_full) ||\n\t\t\t!list_empty(&n->slabs_partial);\n\t}\n\treturn (ret ? 1 : 0);\n}\n\nint __kmem_cache_shutdown(struct kmem_cache *cachep)\n{\n\treturn __kmem_cache_shrink(cachep);\n}\n\nvoid __kmem_cache_release(struct kmem_cache *cachep)\n{\n\tint i;\n\tstruct kmem_cache_node *n;\n\n\tcache_random_seq_destroy(cachep);\n\n\tfree_percpu(cachep->cpu_cache);\n\n\t/* NUMA: free the node structures */\n\tfor_each_kmem_cache_node(cachep, i, n) {\n\t\tkfree(n->shared);\n\t\tfree_alien_cache(n->alien);\n\t\tkfree(n);\n\t\tcachep->node[i] = NULL;\n\t}\n}\n\n/*\n * Get the memory for a slab management obj.\n *\n * For a slab cache when the slab descriptor is off-slab, the\n * slab descriptor can't come from the same cache which is being created,\n * Because if it is the case, that means we defer the creation of\n * the kmalloc_{dma,}_cache of size sizeof(slab descriptor) to this point.\n * And we eventually call down to __kmem_cache_create(), which\n * in turn looks up in the kmalloc_{dma,}_caches for the disired-size one.\n * This is a \"chicken-and-egg\" problem.\n *\n * So the off-slab slab descriptor shall come from the kmalloc_{dma,}_caches,\n * which are all initialized during kmem_cache_init().\n */\nstatic void *alloc_slabmgmt(struct kmem_cache *cachep,\n\t\t\t\t   struct page *page, int colour_off,\n\t\t\t\t   gfp_t local_flags, int nodeid)\n{\n\tvoid *freelist;\n\tvoid *addr = page_address(page);\n\n\tpage->s_mem = addr + colour_off;\n\tpage->active = 0;\n\n\tif (OBJFREELIST_SLAB(cachep))\n\t\tfreelist = NULL;\n\telse if (OFF_SLAB(cachep)) {\n\t\t/* Slab management obj is off-slab. */\n\t\tfreelist = kmem_cache_alloc_node(cachep->freelist_cache,\n\t\t\t\t\t      local_flags, nodeid);\n\t\tif (!freelist)\n\t\t\treturn NULL;\n\t} else {\n\t\t/* We will use last bytes at the slab for freelist */\n\t\tfreelist = addr + (PAGE_SIZE << cachep->gfporder) -\n\t\t\t\tcachep->freelist_size;\n\t}\n\n\treturn freelist;\n}\n\nstatic inline freelist_idx_t get_free_obj(struct page *page, unsigned int idx)\n{\n\treturn ((freelist_idx_t *)page->freelist)[idx];\n}\n\nstatic inline void set_free_obj(struct page *page,\n\t\t\t\t\tunsigned int idx, freelist_idx_t val)\n{\n\t((freelist_idx_t *)(page->freelist))[idx] = val;\n}\n\nstatic void cache_init_objs_debug(struct kmem_cache *cachep, struct page *page)\n{\n#if DEBUG\n\tint i;\n\n\tfor (i = 0; i < cachep->num; i++) {\n\t\tvoid *objp = index_to_obj(cachep, page, i);\n\n\t\tif (cachep->flags & SLAB_STORE_USER)\n\t\t\t*dbg_userword(cachep, objp) = NULL;\n\n\t\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\t\t*dbg_redzone1(cachep, objp) = RED_INACTIVE;\n\t\t\t*dbg_redzone2(cachep, objp) = RED_INACTIVE;\n\t\t}\n\t\t/*\n\t\t * Constructors are not allowed to allocate memory from the same\n\t\t * cache which they are a constructor for.  Otherwise, deadlock.\n\t\t * They must also be threaded.\n\t\t */\n\t\tif (cachep->ctor && !(cachep->flags & SLAB_POISON)) {\n\t\t\tkasan_unpoison_object_data(cachep,\n\t\t\t\t\t\t   objp + obj_offset(cachep));\n\t\t\tcachep->ctor(objp + obj_offset(cachep));\n\t\t\tkasan_poison_object_data(\n\t\t\t\tcachep, objp + obj_offset(cachep));\n\t\t}\n\n\t\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\t\tif (*dbg_redzone2(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"constructor overwrote the end of an object\");\n\t\t\tif (*dbg_redzone1(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"constructor overwrote the start of an object\");\n\t\t}\n\t\t/* need to poison the objs? */\n\t\tif (cachep->flags & SLAB_POISON) {\n\t\t\tpoison_obj(cachep, objp, POISON_FREE);\n\t\t\tslab_kernel_map(cachep, objp, 0, 0);\n\t\t}\n\t}\n#endif\n}\n\n#ifdef CONFIG_SLAB_FREELIST_RANDOM\n/* Hold information during a freelist initialization */\nunion freelist_init_state {\n\tstruct {\n\t\tunsigned int pos;\n\t\tunsigned int *list;\n\t\tunsigned int count;\n\t\tunsigned int rand;\n\t};\n\tstruct rnd_state rnd_state;\n};\n\n/*\n * Initialize the state based on the randomization methode available.\n * return true if the pre-computed list is available, false otherwize.\n */\nstatic bool freelist_state_initialize(union freelist_init_state *state,\n\t\t\t\tstruct kmem_cache *cachep,\n\t\t\t\tunsigned int count)\n{\n\tbool ret;\n\tunsigned int rand;\n\n\t/* Use best entropy available to define a random shift */\n\trand = get_random_int();\n\n\t/* Use a random state if the pre-computed list is not available */\n\tif (!cachep->random_seq) {\n\t\tprandom_seed_state(&state->rnd_state, rand);\n\t\tret = false;\n\t} else {\n\t\tstate->list = cachep->random_seq;\n\t\tstate->count = count;\n\t\tstate->pos = 0;\n\t\tstate->rand = rand;\n\t\tret = true;\n\t}\n\treturn ret;\n}\n\n/* Get the next entry on the list and randomize it using a random shift */\nstatic freelist_idx_t next_random_slot(union freelist_init_state *state)\n{\n\treturn (state->list[state->pos++] + state->rand) % state->count;\n}\n\n/* Swap two freelist entries */\nstatic void swap_free_obj(struct page *page, unsigned int a, unsigned int b)\n{\n\tswap(((freelist_idx_t *)page->freelist)[a],\n\t\t((freelist_idx_t *)page->freelist)[b]);\n}\n\n/*\n * Shuffle the freelist initialization state based on pre-computed lists.\n * return true if the list was successfully shuffled, false otherwise.\n */\nstatic bool shuffle_freelist(struct kmem_cache *cachep, struct page *page)\n{\n\tunsigned int objfreelist = 0, i, rand, count = cachep->num;\n\tunion freelist_init_state state;\n\tbool precomputed;\n\n\tif (count < 2)\n\t\treturn false;\n\n\tprecomputed = freelist_state_initialize(&state, cachep, count);\n\n\t/* Take a random entry as the objfreelist */\n\tif (OBJFREELIST_SLAB(cachep)) {\n\t\tif (!precomputed)\n\t\t\tobjfreelist = count - 1;\n\t\telse\n\t\t\tobjfreelist = next_random_slot(&state);\n\t\tpage->freelist = index_to_obj(cachep, page, objfreelist) +\n\t\t\t\t\t\tobj_offset(cachep);\n\t\tcount--;\n\t}\n\n\t/*\n\t * On early boot, generate the list dynamically.\n\t * Later use a pre-computed list for speed.\n\t */\n\tif (!precomputed) {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tset_free_obj(page, i, i);\n\n\t\t/* Fisher-Yates shuffle */\n\t\tfor (i = count - 1; i > 0; i--) {\n\t\t\trand = prandom_u32_state(&state.rnd_state);\n\t\t\trand %= (i + 1);\n\t\t\tswap_free_obj(page, i, rand);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tset_free_obj(page, i, next_random_slot(&state));\n\t}\n\n\tif (OBJFREELIST_SLAB(cachep))\n\t\tset_free_obj(page, cachep->num - 1, objfreelist);\n\n\treturn true;\n}\n#else\nstatic inline bool shuffle_freelist(struct kmem_cache *cachep,\n\t\t\t\tstruct page *page)\n{\n\treturn false;\n}\n#endif /* CONFIG_SLAB_FREELIST_RANDOM */\n\nstatic void cache_init_objs(struct kmem_cache *cachep,\n\t\t\t    struct page *page)\n{\n\tint i;\n\tvoid *objp;\n\tbool shuffled;\n\n\tcache_init_objs_debug(cachep, page);\n\n\t/* Try to randomize the freelist if enabled */\n\tshuffled = shuffle_freelist(cachep, page);\n\n\tif (!shuffled && OBJFREELIST_SLAB(cachep)) {\n\t\tpage->freelist = index_to_obj(cachep, page, cachep->num - 1) +\n\t\t\t\t\t\tobj_offset(cachep);\n\t}\n\n\tfor (i = 0; i < cachep->num; i++) {\n\t\tobjp = index_to_obj(cachep, page, i);\n\t\tkasan_init_slab_obj(cachep, objp);\n\n\t\t/* constructor could break poison info */\n\t\tif (DEBUG == 0 && cachep->ctor) {\n\t\t\tkasan_unpoison_object_data(cachep, objp);\n\t\t\tcachep->ctor(objp);\n\t\t\tkasan_poison_object_data(cachep, objp);\n\t\t}\n\n\t\tif (!shuffled)\n\t\t\tset_free_obj(page, i, i);\n\t}\n}\n\nstatic void *slab_get_obj(struct kmem_cache *cachep, struct page *page)\n{\n\tvoid *objp;\n\n\tobjp = index_to_obj(cachep, page, get_free_obj(page, page->active));\n\tpage->active++;\n\n#if DEBUG\n\tif (cachep->flags & SLAB_STORE_USER)\n\t\tset_store_user_dirty(cachep);\n#endif\n\n\treturn objp;\n}\n\nstatic void slab_put_obj(struct kmem_cache *cachep,\n\t\t\tstruct page *page, void *objp)\n{\n\tunsigned int objnr = obj_to_index(cachep, page, objp);\n#if DEBUG\n\tunsigned int i;\n\n\t/* Verify double free bug */\n\tfor (i = page->active; i < cachep->num; i++) {\n\t\tif (get_free_obj(page, i) == objnr) {\n\t\t\tpr_err(\"slab: double free detected in cache '%s', objp %p\\n\",\n\t\t\t       cachep->name, objp);\n\t\t\tBUG();\n\t\t}\n\t}\n#endif\n\tpage->active--;\n\tif (!page->freelist)\n\t\tpage->freelist = objp + obj_offset(cachep);\n\n\tset_free_obj(page, page->active, objnr);\n}\n\n/*\n * Map pages beginning at addr to the given cache and slab. This is required\n * for the slab allocator to be able to lookup the cache and slab of a\n * virtual address for kfree, ksize, and slab debugging.\n */\nstatic void slab_map_pages(struct kmem_cache *cache, struct page *page,\n\t\t\t   void *freelist)\n{\n\tpage->slab_cache = cache;\n\tpage->freelist = freelist;\n}\n\n/*\n * Grow (by 1) the number of slabs within a cache.  This is called by\n * kmem_cache_alloc() when there are no active objs left in a cache.\n */\nstatic struct page *cache_grow_begin(struct kmem_cache *cachep,\n\t\t\t\tgfp_t flags, int nodeid)\n{\n\tvoid *freelist;\n\tsize_t offset;\n\tgfp_t local_flags;\n\tint page_node;\n\tstruct kmem_cache_node *n;\n\tstruct page *page;\n\n\t/*\n\t * Be lazy and only check for valid flags here,  keeping it out of the\n\t * critical path in kmem_cache_alloc().\n\t */\n\tif (unlikely(flags & GFP_SLAB_BUG_MASK)) {\n\t\tgfp_t invalid_mask = flags & GFP_SLAB_BUG_MASK;\n\t\tflags &= ~GFP_SLAB_BUG_MASK;\n\t\tpr_warn(\"Unexpected gfp: %#x (%pGg). Fixing up to gfp: %#x (%pGg). Fix your code!\\n\",\n\t\t\t\tinvalid_mask, &invalid_mask, flags, &flags);\n\t\tdump_stack();\n\t}\n\tlocal_flags = flags & (GFP_CONSTRAINT_MASK|GFP_RECLAIM_MASK);\n\n\tcheck_irq_off();\n\tif (gfpflags_allow_blocking(local_flags))\n\t\tlocal_irq_enable();\n\n\t/*\n\t * Get mem for the objs.  Attempt to allocate a physical page from\n\t * 'nodeid'.\n\t */\n\tpage = kmem_getpages(cachep, local_flags, nodeid);\n\tif (!page)\n\t\tgoto failed;\n\n\tpage_node = page_to_nid(page);\n\tn = get_node(cachep, page_node);\n\n\t/* Get colour for the slab, and cal the next value. */\n\tn->colour_next++;\n\tif (n->colour_next >= cachep->colour)\n\t\tn->colour_next = 0;\n\n\toffset = n->colour_next;\n\tif (offset >= cachep->colour)\n\t\toffset = 0;\n\n\toffset *= cachep->colour_off;\n\n\t/* Get slab management. */\n\tfreelist = alloc_slabmgmt(cachep, page, offset,\n\t\t\tlocal_flags & ~GFP_CONSTRAINT_MASK, page_node);\n\tif (OFF_SLAB(cachep) && !freelist)\n\t\tgoto opps1;\n\n\tslab_map_pages(cachep, page, freelist);\n\n\tkasan_poison_slab(page);\n\tcache_init_objs(cachep, page);\n\n\tif (gfpflags_allow_blocking(local_flags))\n\t\tlocal_irq_disable();\n\n\treturn page;\n\nopps1:\n\tkmem_freepages(cachep, page);\nfailed:\n\tif (gfpflags_allow_blocking(local_flags))\n\t\tlocal_irq_disable();\n\treturn NULL;\n}\n\nstatic void cache_grow_end(struct kmem_cache *cachep, struct page *page)\n{\n\tstruct kmem_cache_node *n;\n\tvoid *list = NULL;\n\n\tcheck_irq_off();\n\n\tif (!page)\n\t\treturn;\n\n\tINIT_LIST_HEAD(&page->lru);\n\tn = get_node(cachep, page_to_nid(page));\n\n\tspin_lock(&n->list_lock);\n\tn->total_slabs++;\n\tif (!page->active) {\n\t\tlist_add_tail(&page->lru, &(n->slabs_free));\n\t\tn->free_slabs++;\n\t} else\n\t\tfixup_slab_list(cachep, n, page, &list);\n\n\tSTATS_INC_GROWN(cachep);\n\tn->free_objects += cachep->num - page->active;\n\tspin_unlock(&n->list_lock);\n\n\tfixup_objfreelist_debug(cachep, &list);\n}\n\n#if DEBUG\n\n/*\n * Perform extra freeing checks:\n * - detect bad pointers.\n * - POISON/RED_ZONE checking\n */\nstatic void kfree_debugcheck(const void *objp)\n{\n\tif (!virt_addr_valid(objp)) {\n\t\tpr_err(\"kfree_debugcheck: out of range ptr %lxh\\n\",\n\t\t       (unsigned long)objp);\n\t\tBUG();\n\t}\n}\n\nstatic inline void verify_redzone_free(struct kmem_cache *cache, void *obj)\n{\n\tunsigned long long redzone1, redzone2;\n\n\tredzone1 = *dbg_redzone1(cache, obj);\n\tredzone2 = *dbg_redzone2(cache, obj);\n\n\t/*\n\t * Redzone is ok.\n\t */\n\tif (redzone1 == RED_ACTIVE && redzone2 == RED_ACTIVE)\n\t\treturn;\n\n\tif (redzone1 == RED_INACTIVE && redzone2 == RED_INACTIVE)\n\t\tslab_error(cache, \"double free detected\");\n\telse\n\t\tslab_error(cache, \"memory outside object was overwritten\");\n\n\tpr_err(\"%p: redzone 1:0x%llx, redzone 2:0x%llx\\n\",\n\t       obj, redzone1, redzone2);\n}\n\nstatic void *cache_free_debugcheck(struct kmem_cache *cachep, void *objp,\n\t\t\t\t   unsigned long caller)\n{\n\tunsigned int objnr;\n\tstruct page *page;\n\n\tBUG_ON(virt_to_cache(objp) != cachep);\n\n\tobjp -= obj_offset(cachep);\n\tkfree_debugcheck(objp);\n\tpage = virt_to_head_page(objp);\n\n\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\tverify_redzone_free(cachep, objp);\n\t\t*dbg_redzone1(cachep, objp) = RED_INACTIVE;\n\t\t*dbg_redzone2(cachep, objp) = RED_INACTIVE;\n\t}\n\tif (cachep->flags & SLAB_STORE_USER) {\n\t\tset_store_user_dirty(cachep);\n\t\t*dbg_userword(cachep, objp) = (void *)caller;\n\t}\n\n\tobjnr = obj_to_index(cachep, page, objp);\n\n\tBUG_ON(objnr >= cachep->num);\n\tBUG_ON(objp != index_to_obj(cachep, page, objnr));\n\n\tif (cachep->flags & SLAB_POISON) {\n\t\tpoison_obj(cachep, objp, POISON_FREE);\n\t\tslab_kernel_map(cachep, objp, 0, caller);\n\t}\n\treturn objp;\n}\n\n#else\n#define kfree_debugcheck(x) do { } while(0)\n#define cache_free_debugcheck(x,objp,z) (objp)\n#endif\n\nstatic inline void fixup_objfreelist_debug(struct kmem_cache *cachep,\n\t\t\t\t\t\tvoid **list)\n{\n#if DEBUG\n\tvoid *next = *list;\n\tvoid *objp;\n\n\twhile (next) {\n\t\tobjp = next - obj_offset(cachep);\n\t\tnext = *(void **)next;\n\t\tpoison_obj(cachep, objp, POISON_FREE);\n\t}\n#endif\n}\n\nstatic inline void fixup_slab_list(struct kmem_cache *cachep,\n\t\t\t\tstruct kmem_cache_node *n, struct page *page,\n\t\t\t\tvoid **list)\n{\n\t/* move slabp to correct slabp list: */\n\tlist_del(&page->lru);\n\tif (page->active == cachep->num) {\n\t\tlist_add(&page->lru, &n->slabs_full);\n\t\tif (OBJFREELIST_SLAB(cachep)) {\n#if DEBUG\n\t\t\t/* Poisoning will be done without holding the lock */\n\t\t\tif (cachep->flags & SLAB_POISON) {\n\t\t\t\tvoid **objp = page->freelist;\n\n\t\t\t\t*objp = *list;\n\t\t\t\t*list = objp;\n\t\t\t}\n#endif\n\t\t\tpage->freelist = NULL;\n\t\t}\n\t} else\n\t\tlist_add(&page->lru, &n->slabs_partial);\n}\n\n/* Try to find non-pfmemalloc slab if needed */\nstatic noinline struct page *get_valid_first_slab(struct kmem_cache_node *n,\n\t\t\t\t\tstruct page *page, bool pfmemalloc)\n{\n\tif (!page)\n\t\treturn NULL;\n\n\tif (pfmemalloc)\n\t\treturn page;\n\n\tif (!PageSlabPfmemalloc(page))\n\t\treturn page;\n\n\t/* No need to keep pfmemalloc slab if we have enough free objects */\n\tif (n->free_objects > n->free_limit) {\n\t\tClearPageSlabPfmemalloc(page);\n\t\treturn page;\n\t}\n\n\t/* Move pfmemalloc slab to the end of list to speed up next search */\n\tlist_del(&page->lru);\n\tif (!page->active) {\n\t\tlist_add_tail(&page->lru, &n->slabs_free);\n\t\tn->free_slabs++;\n\t} else\n\t\tlist_add_tail(&page->lru, &n->slabs_partial);\n\n\tlist_for_each_entry(page, &n->slabs_partial, lru) {\n\t\tif (!PageSlabPfmemalloc(page))\n\t\t\treturn page;\n\t}\n\n\tn->free_touched = 1;\n\tlist_for_each_entry(page, &n->slabs_free, lru) {\n\t\tif (!PageSlabPfmemalloc(page)) {\n\t\t\tn->free_slabs--;\n\t\t\treturn page;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct page *get_first_slab(struct kmem_cache_node *n, bool pfmemalloc)\n{\n\tstruct page *page;\n\n\tassert_spin_locked(&n->list_lock);\n\tpage = list_first_entry_or_null(&n->slabs_partial, struct page, lru);\n\tif (!page) {\n\t\tn->free_touched = 1;\n\t\tpage = list_first_entry_or_null(&n->slabs_free, struct page,\n\t\t\t\t\t\tlru);\n\t\tif (page)\n\t\t\tn->free_slabs--;\n\t}\n\n\tif (sk_memalloc_socks())\n\t\tpage = get_valid_first_slab(n, page, pfmemalloc);\n\n\treturn page;\n}\n\nstatic noinline void *cache_alloc_pfmemalloc(struct kmem_cache *cachep,\n\t\t\t\tstruct kmem_cache_node *n, gfp_t flags)\n{\n\tstruct page *page;\n\tvoid *obj;\n\tvoid *list = NULL;\n\n\tif (!gfp_pfmemalloc_allowed(flags))\n\t\treturn NULL;\n\n\tspin_lock(&n->list_lock);\n\tpage = get_first_slab(n, true);\n\tif (!page) {\n\t\tspin_unlock(&n->list_lock);\n\t\treturn NULL;\n\t}\n\n\tobj = slab_get_obj(cachep, page);\n\tn->free_objects--;\n\n\tfixup_slab_list(cachep, n, page, &list);\n\n\tspin_unlock(&n->list_lock);\n\tfixup_objfreelist_debug(cachep, &list);\n\n\treturn obj;\n}\n\n/*\n * Slab list should be fixed up by fixup_slab_list() for existing slab\n * or cache_grow_end() for new slab\n */\nstatic __always_inline int alloc_block(struct kmem_cache *cachep,\n\t\tstruct array_cache *ac, struct page *page, int batchcount)\n{\n\t/*\n\t * There must be at least one object available for\n\t * allocation.\n\t */\n\tBUG_ON(page->active >= cachep->num);\n\n\twhile (page->active < cachep->num && batchcount--) {\n\t\tSTATS_INC_ALLOCED(cachep);\n\t\tSTATS_INC_ACTIVE(cachep);\n\t\tSTATS_SET_HIGH(cachep);\n\n\t\tac->entry[ac->avail++] = slab_get_obj(cachep, page);\n\t}\n\n\treturn batchcount;\n}\n\nstatic void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags)\n{\n\tint batchcount;\n\tstruct kmem_cache_node *n;\n\tstruct array_cache *ac, *shared;\n\tint node;\n\tvoid *list = NULL;\n\tstruct page *page;\n\n\tcheck_irq_off();\n\tnode = numa_mem_id();\n\n\tac = cpu_cache_get(cachep);\n\tbatchcount = ac->batchcount;\n\tif (!ac->touched && batchcount > BATCHREFILL_LIMIT) {\n\t\t/*\n\t\t * If there was little recent activity on this cache, then\n\t\t * perform only a partial refill.  Otherwise we could generate\n\t\t * refill bouncing.\n\t\t */\n\t\tbatchcount = BATCHREFILL_LIMIT;\n\t}\n\tn = get_node(cachep, node);\n\n\tBUG_ON(ac->avail > 0 || !n);\n\tshared = READ_ONCE(n->shared);\n\tif (!n->free_objects && (!shared || !shared->avail))\n\t\tgoto direct_grow;\n\n\tspin_lock(&n->list_lock);\n\tshared = READ_ONCE(n->shared);\n\n\t/* See if we can refill from the shared array */\n\tif (shared && transfer_objects(ac, shared, batchcount)) {\n\t\tshared->touched = 1;\n\t\tgoto alloc_done;\n\t}\n\n\twhile (batchcount > 0) {\n\t\t/* Get slab alloc is to come from. */\n\t\tpage = get_first_slab(n, false);\n\t\tif (!page)\n\t\t\tgoto must_grow;\n\n\t\tcheck_spinlock_acquired(cachep);\n\n\t\tbatchcount = alloc_block(cachep, ac, page, batchcount);\n\t\tfixup_slab_list(cachep, n, page, &list);\n\t}\n\nmust_grow:\n\tn->free_objects -= ac->avail;\nalloc_done:\n\tspin_unlock(&n->list_lock);\n\tfixup_objfreelist_debug(cachep, &list);\n\ndirect_grow:\n\tif (unlikely(!ac->avail)) {\n\t\t/* Check if we can use obj in pfmemalloc slab */\n\t\tif (sk_memalloc_socks()) {\n\t\t\tvoid *obj = cache_alloc_pfmemalloc(cachep, n, flags);\n\n\t\t\tif (obj)\n\t\t\t\treturn obj;\n\t\t}\n\n\t\tpage = cache_grow_begin(cachep, gfp_exact_node(flags), node);\n\n\t\t/*\n\t\t * cache_grow_begin() can reenable interrupts,\n\t\t * then ac could change.\n\t\t */\n\t\tac = cpu_cache_get(cachep);\n\t\tif (!ac->avail && page)\n\t\t\talloc_block(cachep, ac, page, batchcount);\n\t\tcache_grow_end(cachep, page);\n\n\t\tif (!ac->avail)\n\t\t\treturn NULL;\n\t}\n\tac->touched = 1;\n\n\treturn ac->entry[--ac->avail];\n}\n\nstatic inline void cache_alloc_debugcheck_before(struct kmem_cache *cachep,\n\t\t\t\t\t\tgfp_t flags)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(flags));\n}\n\n#if DEBUG\nstatic void *cache_alloc_debugcheck_after(struct kmem_cache *cachep,\n\t\t\t\tgfp_t flags, void *objp, unsigned long caller)\n{\n\tif (!objp)\n\t\treturn objp;\n\tif (cachep->flags & SLAB_POISON) {\n\t\tcheck_poison_obj(cachep, objp);\n\t\tslab_kernel_map(cachep, objp, 1, 0);\n\t\tpoison_obj(cachep, objp, POISON_INUSE);\n\t}\n\tif (cachep->flags & SLAB_STORE_USER)\n\t\t*dbg_userword(cachep, objp) = (void *)caller;\n\n\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\tif (*dbg_redzone1(cachep, objp) != RED_INACTIVE ||\n\t\t\t\t*dbg_redzone2(cachep, objp) != RED_INACTIVE) {\n\t\t\tslab_error(cachep, \"double free, or memory outside object was overwritten\");\n\t\t\tpr_err(\"%p: redzone 1:0x%llx, redzone 2:0x%llx\\n\",\n\t\t\t       objp, *dbg_redzone1(cachep, objp),\n\t\t\t       *dbg_redzone2(cachep, objp));\n\t\t}\n\t\t*dbg_redzone1(cachep, objp) = RED_ACTIVE;\n\t\t*dbg_redzone2(cachep, objp) = RED_ACTIVE;\n\t}\n\n\tobjp += obj_offset(cachep);\n\tif (cachep->ctor && cachep->flags & SLAB_POISON)\n\t\tcachep->ctor(objp);\n\tif (ARCH_SLAB_MINALIGN &&\n\t    ((unsigned long)objp & (ARCH_SLAB_MINALIGN-1))) {\n\t\tpr_err(\"0x%p: not aligned to ARCH_SLAB_MINALIGN=%d\\n\",\n\t\t       objp, (int)ARCH_SLAB_MINALIGN);\n\t}\n\treturn objp;\n}\n#else\n#define cache_alloc_debugcheck_after(a,b,objp,d) (objp)\n#endif\n\nstatic inline void *____cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\tvoid *objp;\n\tstruct array_cache *ac;\n\n\tcheck_irq_off();\n\n\tac = cpu_cache_get(cachep);\n\tif (likely(ac->avail)) {\n\t\tac->touched = 1;\n\t\tobjp = ac->entry[--ac->avail];\n\n\t\tSTATS_INC_ALLOCHIT(cachep);\n\t\tgoto out;\n\t}\n\n\tSTATS_INC_ALLOCMISS(cachep);\n\tobjp = cache_alloc_refill(cachep, flags);\n\t/*\n\t * the 'ac' may be updated by cache_alloc_refill(),\n\t * and kmemleak_erase() requires its correct value.\n\t */\n\tac = cpu_cache_get(cachep);\n\nout:\n\t/*\n\t * To avoid a false negative, if an object that is in one of the\n\t * per-CPU caches is leaked, we need to make sure kmemleak doesn't\n\t * treat the array pointers as a reference to the object.\n\t */\n\tif (objp)\n\t\tkmemleak_erase(&ac->entry[ac->avail]);\n\treturn objp;\n}\n\n#ifdef CONFIG_NUMA\n/*\n * Try allocating on another node if PFA_SPREAD_SLAB is a mempolicy is set.\n *\n * If we are in_interrupt, then process context, including cpusets and\n * mempolicy, may not apply and should not be used for allocation policy.\n */\nstatic void *alternate_node_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\tint nid_alloc, nid_here;\n\n\tif (in_interrupt() || (flags & __GFP_THISNODE))\n\t\treturn NULL;\n\tnid_alloc = nid_here = numa_mem_id();\n\tif (cpuset_do_slab_mem_spread() && (cachep->flags & SLAB_MEM_SPREAD))\n\t\tnid_alloc = cpuset_slab_spread_node();\n\telse if (current->mempolicy)\n\t\tnid_alloc = mempolicy_slab_node();\n\tif (nid_alloc != nid_here)\n\t\treturn ____cache_alloc_node(cachep, flags, nid_alloc);\n\treturn NULL;\n}\n\n/*\n * Fallback function if there was no memory available and no objects on a\n * certain node and fall back is permitted. First we scan all the\n * available node for available objects. If that fails then we\n * perform an allocation without specifying a node. This allows the page\n * allocator to do its reclaim / fallback magic. We then insert the\n * slab into the proper nodelist and then allocate from it.\n */\nstatic void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)\n{\n\tstruct zonelist *zonelist;\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tenum zone_type high_zoneidx = gfp_zone(flags);\n\tvoid *obj = NULL;\n\tstruct page *page;\n\tint nid;\n\tunsigned int cpuset_mems_cookie;\n\n\tif (flags & __GFP_THISNODE)\n\t\treturn NULL;\n\nretry_cpuset:\n\tcpuset_mems_cookie = read_mems_allowed_begin();\n\tzonelist = node_zonelist(mempolicy_slab_node(), flags);\n\nretry:\n\t/*\n\t * Look through allowed nodes for objects available\n\t * from existing per node queues.\n\t */\n\tfor_each_zone_zonelist(zone, z, zonelist, high_zoneidx) {\n\t\tnid = zone_to_nid(zone);\n\n\t\tif (cpuset_zone_allowed(zone, flags) &&\n\t\t\tget_node(cache, nid) &&\n\t\t\tget_node(cache, nid)->free_objects) {\n\t\t\t\tobj = ____cache_alloc_node(cache,\n\t\t\t\t\tgfp_exact_node(flags), nid);\n\t\t\t\tif (obj)\n\t\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!obj) {\n\t\t/*\n\t\t * This allocation will be performed within the constraints\n\t\t * of the current cpuset / memory policy requirements.\n\t\t * We may trigger various forms of reclaim on the allowed\n\t\t * set and go into memory reserves if necessary.\n\t\t */\n\t\tpage = cache_grow_begin(cache, flags, numa_mem_id());\n\t\tcache_grow_end(cache, page);\n\t\tif (page) {\n\t\t\tnid = page_to_nid(page);\n\t\t\tobj = ____cache_alloc_node(cache,\n\t\t\t\tgfp_exact_node(flags), nid);\n\n\t\t\t/*\n\t\t\t * Another processor may allocate the objects in\n\t\t\t * the slab since we are not holding any locks.\n\t\t\t */\n\t\t\tif (!obj)\n\t\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tif (unlikely(!obj && read_mems_allowed_retry(cpuset_mems_cookie)))\n\t\tgoto retry_cpuset;\n\treturn obj;\n}\n\n/*\n * A interface to enable slab creation on nodeid\n */\nstatic void *____cache_alloc_node(struct kmem_cache *cachep, gfp_t flags,\n\t\t\t\tint nodeid)\n{\n\tstruct page *page;\n\tstruct kmem_cache_node *n;\n\tvoid *obj = NULL;\n\tvoid *list = NULL;\n\n\tVM_BUG_ON(nodeid < 0 || nodeid >= MAX_NUMNODES);\n\tn = get_node(cachep, nodeid);\n\tBUG_ON(!n);\n\n\tcheck_irq_off();\n\tspin_lock(&n->list_lock);\n\tpage = get_first_slab(n, false);\n\tif (!page)\n\t\tgoto must_grow;\n\n\tcheck_spinlock_acquired_node(cachep, nodeid);\n\n\tSTATS_INC_NODEALLOCS(cachep);\n\tSTATS_INC_ACTIVE(cachep);\n\tSTATS_SET_HIGH(cachep);\n\n\tBUG_ON(page->active == cachep->num);\n\n\tobj = slab_get_obj(cachep, page);\n\tn->free_objects--;\n\n\tfixup_slab_list(cachep, n, page, &list);\n\n\tspin_unlock(&n->list_lock);\n\tfixup_objfreelist_debug(cachep, &list);\n\treturn obj;\n\nmust_grow:\n\tspin_unlock(&n->list_lock);\n\tpage = cache_grow_begin(cachep, gfp_exact_node(flags), nodeid);\n\tif (page) {\n\t\t/* This slab isn't counted yet so don't update free_objects */\n\t\tobj = slab_get_obj(cachep, page);\n\t}\n\tcache_grow_end(cachep, page);\n\n\treturn obj ? obj : fallback_alloc(cachep, flags);\n}\n\nstatic __always_inline void *\nslab_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid,\n\t\t   unsigned long caller)\n{\n\tunsigned long save_flags;\n\tvoid *ptr;\n\tint slab_node = numa_mem_id();\n\n\tflags &= gfp_allowed_mask;\n\tcachep = slab_pre_alloc_hook(cachep, flags);\n\tif (unlikely(!cachep))\n\t\treturn NULL;\n\n\tcache_alloc_debugcheck_before(cachep, flags);\n\tlocal_irq_save(save_flags);\n\n\tif (nodeid == NUMA_NO_NODE)\n\t\tnodeid = slab_node;\n\n\tif (unlikely(!get_node(cachep, nodeid))) {\n\t\t/* Node not bootstrapped yet */\n\t\tptr = fallback_alloc(cachep, flags);\n\t\tgoto out;\n\t}\n\n\tif (nodeid == slab_node) {\n\t\t/*\n\t\t * Use the locally cached objects if possible.\n\t\t * However ____cache_alloc does not allow fallback\n\t\t * to other nodes. It may fail while we still have\n\t\t * objects on other nodes available.\n\t\t */\n\t\tptr = ____cache_alloc(cachep, flags);\n\t\tif (ptr)\n\t\t\tgoto out;\n\t}\n\t/* ___cache_alloc_node can fall back to other nodes */\n\tptr = ____cache_alloc_node(cachep, flags, nodeid);\n  out:\n\tlocal_irq_restore(save_flags);\n\tptr = cache_alloc_debugcheck_after(cachep, flags, ptr, caller);\n\n\tif (unlikely(flags & __GFP_ZERO) && ptr)\n\t\tmemset(ptr, 0, cachep->object_size);\n\n\tslab_post_alloc_hook(cachep, flags, 1, &ptr);\n\treturn ptr;\n}\n\nstatic __always_inline void *\n__do_cache_alloc(struct kmem_cache *cache, gfp_t flags)\n{\n\tvoid *objp;\n\n\tif (current->mempolicy || cpuset_do_slab_mem_spread()) {\n\t\tobjp = alternate_node_alloc(cache, flags);\n\t\tif (objp)\n\t\t\tgoto out;\n\t}\n\tobjp = ____cache_alloc(cache, flags);\n\n\t/*\n\t * We may just have run out of memory on the local node.\n\t * ____cache_alloc_node() knows how to locate memory on other nodes\n\t */\n\tif (!objp)\n\t\tobjp = ____cache_alloc_node(cache, flags, numa_mem_id());\n\n  out:\n\treturn objp;\n}\n#else\n\nstatic __always_inline void *\n__do_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\treturn ____cache_alloc(cachep, flags);\n}\n\n#endif /* CONFIG_NUMA */\n\nstatic __always_inline void *\nslab_alloc(struct kmem_cache *cachep, gfp_t flags, unsigned long caller)\n{\n\tunsigned long save_flags;\n\tvoid *objp;\n\n\tflags &= gfp_allowed_mask;\n\tcachep = slab_pre_alloc_hook(cachep, flags);\n\tif (unlikely(!cachep))\n\t\treturn NULL;\n\n\tcache_alloc_debugcheck_before(cachep, flags);\n\tlocal_irq_save(save_flags);\n\tobjp = __do_cache_alloc(cachep, flags);\n\tlocal_irq_restore(save_flags);\n\tobjp = cache_alloc_debugcheck_after(cachep, flags, objp, caller);\n\tprefetchw(objp);\n\n\tif (unlikely(flags & __GFP_ZERO) && objp)\n\t\tmemset(objp, 0, cachep->object_size);\n\n\tslab_post_alloc_hook(cachep, flags, 1, &objp);\n\treturn objp;\n}\n\n/*\n * Caller needs to acquire correct kmem_cache_node's list_lock\n * @list: List of detached free slabs should be freed by caller\n */\nstatic void free_block(struct kmem_cache *cachep, void **objpp,\n\t\t\tint nr_objects, int node, struct list_head *list)\n{\n\tint i;\n\tstruct kmem_cache_node *n = get_node(cachep, node);\n\tstruct page *page;\n\n\tn->free_objects += nr_objects;\n\n\tfor (i = 0; i < nr_objects; i++) {\n\t\tvoid *objp;\n\t\tstruct page *page;\n\n\t\tobjp = objpp[i];\n\n\t\tpage = virt_to_head_page(objp);\n\t\tlist_del(&page->lru);\n\t\tcheck_spinlock_acquired_node(cachep, node);\n\t\tslab_put_obj(cachep, page, objp);\n\t\tSTATS_DEC_ACTIVE(cachep);\n\n\t\t/* fixup slab chains */\n\t\tif (page->active == 0) {\n\t\t\tlist_add(&page->lru, &n->slabs_free);\n\t\t\tn->free_slabs++;\n\t\t} else {\n\t\t\t/* Unconditionally move a slab to the end of the\n\t\t\t * partial list on free - maximum time for the\n\t\t\t * other objects to be freed, too.\n\t\t\t */\n\t\t\tlist_add_tail(&page->lru, &n->slabs_partial);\n\t\t}\n\t}\n\n\twhile (n->free_objects > n->free_limit && !list_empty(&n->slabs_free)) {\n\t\tn->free_objects -= cachep->num;\n\n\t\tpage = list_last_entry(&n->slabs_free, struct page, lru);\n\t\tlist_move(&page->lru, list);\n\t\tn->free_slabs--;\n\t\tn->total_slabs--;\n\t}\n}\n\nstatic void cache_flusharray(struct kmem_cache *cachep, struct array_cache *ac)\n{\n\tint batchcount;\n\tstruct kmem_cache_node *n;\n\tint node = numa_mem_id();\n\tLIST_HEAD(list);\n\n\tbatchcount = ac->batchcount;\n\n\tcheck_irq_off();\n\tn = get_node(cachep, node);\n\tspin_lock(&n->list_lock);\n\tif (n->shared) {\n\t\tstruct array_cache *shared_array = n->shared;\n\t\tint max = shared_array->limit - shared_array->avail;\n\t\tif (max) {\n\t\t\tif (batchcount > max)\n\t\t\t\tbatchcount = max;\n\t\t\tmemcpy(&(shared_array->entry[shared_array->avail]),\n\t\t\t       ac->entry, sizeof(void *) * batchcount);\n\t\t\tshared_array->avail += batchcount;\n\t\t\tgoto free_done;\n\t\t}\n\t}\n\n\tfree_block(cachep, ac->entry, batchcount, node, &list);\nfree_done:\n#if STATS\n\t{\n\t\tint i = 0;\n\t\tstruct page *page;\n\n\t\tlist_for_each_entry(page, &n->slabs_free, lru) {\n\t\t\tBUG_ON(page->active);\n\n\t\t\ti++;\n\t\t}\n\t\tSTATS_SET_FREEABLE(cachep, i);\n\t}\n#endif\n\tspin_unlock(&n->list_lock);\n\tslabs_destroy(cachep, &list);\n\tac->avail -= batchcount;\n\tmemmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);\n}\n\n/*\n * Release an obj back to its cache. If the obj has a constructed state, it must\n * be in this state _before_ it is released.  Called with disabled ints.\n */\nstatic inline void __cache_free(struct kmem_cache *cachep, void *objp,\n\t\t\t\tunsigned long caller)\n{\n\t/* Put the object into the quarantine, don't touch it for now. */\n\tif (kasan_slab_free(cachep, objp))\n\t\treturn;\n\n\t___cache_free(cachep, objp, caller);\n}\n\nvoid ___cache_free(struct kmem_cache *cachep, void *objp,\n\t\tunsigned long caller)\n{\n\tstruct array_cache *ac = cpu_cache_get(cachep);\n\n\tcheck_irq_off();\n\tkmemleak_free_recursive(objp, cachep->flags);\n\tobjp = cache_free_debugcheck(cachep, objp, caller);\n\n\tkmemcheck_slab_free(cachep, objp, cachep->object_size);\n\n\t/*\n\t * Skip calling cache_free_alien() when the platform is not numa.\n\t * This will avoid cache misses that happen while accessing slabp (which\n\t * is per page memory  reference) to get nodeid. Instead use a global\n\t * variable to skip the call, which is mostly likely to be present in\n\t * the cache.\n\t */\n\tif (nr_online_nodes > 1 && cache_free_alien(cachep, objp))\n\t\treturn;\n\n\tif (ac->avail < ac->limit) {\n\t\tSTATS_INC_FREEHIT(cachep);\n\t} else {\n\t\tSTATS_INC_FREEMISS(cachep);\n\t\tcache_flusharray(cachep, ac);\n\t}\n\n\tif (sk_memalloc_socks()) {\n\t\tstruct page *page = virt_to_head_page(objp);\n\n\t\tif (unlikely(PageSlabPfmemalloc(page))) {\n\t\t\tcache_free_pfmemalloc(cachep, page, objp);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tac->entry[ac->avail++] = objp;\n}\n\n/**\n * kmem_cache_alloc - Allocate an object\n * @cachep: The cache to allocate from.\n * @flags: See kmalloc().\n *\n * Allocate an object from this cache.  The flags are only relevant\n * if the cache has no available objects.\n */\nvoid *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\tvoid *ret = slab_alloc(cachep, flags, _RET_IP_);\n\n\tkasan_slab_alloc(cachep, ret, flags);\n\ttrace_kmem_cache_alloc(_RET_IP_, ret,\n\t\t\t       cachep->object_size, cachep->size, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc);\n\nstatic __always_inline void\ncache_alloc_debugcheck_after_bulk(struct kmem_cache *s, gfp_t flags,\n\t\t\t\t  size_t size, void **p, unsigned long caller)\n{\n\tsize_t i;\n\n\tfor (i = 0; i < size; i++)\n\t\tp[i] = cache_alloc_debugcheck_after(s, flags, p[i], caller);\n}\n\nint kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,\n\t\t\t  void **p)\n{\n\tsize_t i;\n\n\ts = slab_pre_alloc_hook(s, flags);\n\tif (!s)\n\t\treturn 0;\n\n\tcache_alloc_debugcheck_before(s, flags);\n\n\tlocal_irq_disable();\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *objp = __do_cache_alloc(s, flags);\n\n\t\tif (unlikely(!objp))\n\t\t\tgoto error;\n\t\tp[i] = objp;\n\t}\n\tlocal_irq_enable();\n\n\tcache_alloc_debugcheck_after_bulk(s, flags, size, p, _RET_IP_);\n\n\t/* Clear memory outside IRQ disabled section */\n\tif (unlikely(flags & __GFP_ZERO))\n\t\tfor (i = 0; i < size; i++)\n\t\t\tmemset(p[i], 0, s->object_size);\n\n\tslab_post_alloc_hook(s, flags, size, p);\n\t/* FIXME: Trace call missing. Christoph would like a bulk variant */\n\treturn size;\nerror:\n\tlocal_irq_enable();\n\tcache_alloc_debugcheck_after_bulk(s, flags, i, p, _RET_IP_);\n\tslab_post_alloc_hook(s, flags, i, p);\n\t__kmem_cache_free_bulk(s, i, p);\n\treturn 0;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_bulk);\n\n#ifdef CONFIG_TRACING\nvoid *\nkmem_cache_alloc_trace(struct kmem_cache *cachep, gfp_t flags, size_t size)\n{\n\tvoid *ret;\n\n\tret = slab_alloc(cachep, flags, _RET_IP_);\n\n\tkasan_kmalloc(cachep, ret, size, flags);\n\ttrace_kmalloc(_RET_IP_, ret,\n\t\t      size, cachep->size, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_trace);\n#endif\n\n#ifdef CONFIG_NUMA\n/**\n * kmem_cache_alloc_node - Allocate an object on the specified node\n * @cachep: The cache to allocate from.\n * @flags: See kmalloc().\n * @nodeid: node number of the target node.\n *\n * Identical to kmem_cache_alloc but it will allocate memory on the given\n * node, which can improve the performance for cpu bound structures.\n *\n * Fallback to other node is possible if __GFP_THISNODE is not set.\n */\nvoid *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)\n{\n\tvoid *ret = slab_alloc_node(cachep, flags, nodeid, _RET_IP_);\n\n\tkasan_slab_alloc(cachep, ret, flags);\n\ttrace_kmem_cache_alloc_node(_RET_IP_, ret,\n\t\t\t\t    cachep->object_size, cachep->size,\n\t\t\t\t    flags, nodeid);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_node);\n\n#ifdef CONFIG_TRACING\nvoid *kmem_cache_alloc_node_trace(struct kmem_cache *cachep,\n\t\t\t\t  gfp_t flags,\n\t\t\t\t  int nodeid,\n\t\t\t\t  size_t size)\n{\n\tvoid *ret;\n\n\tret = slab_alloc_node(cachep, flags, nodeid, _RET_IP_);\n\n\tkasan_kmalloc(cachep, ret, size, flags);\n\ttrace_kmalloc_node(_RET_IP_, ret,\n\t\t\t   size, cachep->size,\n\t\t\t   flags, nodeid);\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_node_trace);\n#endif\n\nstatic __always_inline void *\n__do_kmalloc_node(size_t size, gfp_t flags, int node, unsigned long caller)\n{\n\tstruct kmem_cache *cachep;\n\tvoid *ret;\n\n\tcachep = kmalloc_slab(size, flags);\n\tif (unlikely(ZERO_OR_NULL_PTR(cachep)))\n\t\treturn cachep;\n\tret = kmem_cache_alloc_node_trace(cachep, flags, node, size);\n\tkasan_kmalloc(cachep, ret, size, flags);\n\n\treturn ret;\n}\n\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node)\n{\n\treturn __do_kmalloc_node(size, flags, node, _RET_IP_);\n}\nEXPORT_SYMBOL(__kmalloc_node);\n\nvoid *__kmalloc_node_track_caller(size_t size, gfp_t flags,\n\t\tint node, unsigned long caller)\n{\n\treturn __do_kmalloc_node(size, flags, node, caller);\n}\nEXPORT_SYMBOL(__kmalloc_node_track_caller);\n#endif /* CONFIG_NUMA */\n\n/**\n * __do_kmalloc - allocate memory\n * @size: how many bytes of memory are required.\n * @flags: the type of memory to allocate (see kmalloc).\n * @caller: function caller for debug tracking of the caller\n */\nstatic __always_inline void *__do_kmalloc(size_t size, gfp_t flags,\n\t\t\t\t\t  unsigned long caller)\n{\n\tstruct kmem_cache *cachep;\n\tvoid *ret;\n\n\tcachep = kmalloc_slab(size, flags);\n\tif (unlikely(ZERO_OR_NULL_PTR(cachep)))\n\t\treturn cachep;\n\tret = slab_alloc(cachep, flags, caller);\n\n\tkasan_kmalloc(cachep, ret, size, flags);\n\ttrace_kmalloc(caller, ret,\n\t\t      size, cachep->size, flags);\n\n\treturn ret;\n}\n\nvoid *__kmalloc(size_t size, gfp_t flags)\n{\n\treturn __do_kmalloc(size, flags, _RET_IP_);\n}\nEXPORT_SYMBOL(__kmalloc);\n\nvoid *__kmalloc_track_caller(size_t size, gfp_t flags, unsigned long caller)\n{\n\treturn __do_kmalloc(size, flags, caller);\n}\nEXPORT_SYMBOL(__kmalloc_track_caller);\n\n/**\n * kmem_cache_free - Deallocate an object\n * @cachep: The cache the allocation was from.\n * @objp: The previously allocated object.\n *\n * Free an object which was previously allocated from this\n * cache.\n */\nvoid kmem_cache_free(struct kmem_cache *cachep, void *objp)\n{\n\tunsigned long flags;\n\tcachep = cache_from_obj(cachep, objp);\n\tif (!cachep)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tdebug_check_no_locks_freed(objp, cachep->object_size);\n\tif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(objp, cachep->object_size);\n\t__cache_free(cachep, objp, _RET_IP_);\n\tlocal_irq_restore(flags);\n\n\ttrace_kmem_cache_free(_RET_IP_, objp);\n}\nEXPORT_SYMBOL(kmem_cache_free);\n\nvoid kmem_cache_free_bulk(struct kmem_cache *orig_s, size_t size, void **p)\n{\n\tstruct kmem_cache *s;\n\tsize_t i;\n\n\tlocal_irq_disable();\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *objp = p[i];\n\n\t\tif (!orig_s) /* called via kfree_bulk */\n\t\t\ts = virt_to_cache(objp);\n\t\telse\n\t\t\ts = cache_from_obj(orig_s, objp);\n\n\t\tdebug_check_no_locks_freed(objp, s->object_size);\n\t\tif (!(s->flags & SLAB_DEBUG_OBJECTS))\n\t\t\tdebug_check_no_obj_freed(objp, s->object_size);\n\n\t\t__cache_free(s, objp, _RET_IP_);\n\t}\n\tlocal_irq_enable();\n\n\t/* FIXME: add tracing */\n}\nEXPORT_SYMBOL(kmem_cache_free_bulk);\n\n/**\n * kfree - free previously allocated memory\n * @objp: pointer returned by kmalloc.\n *\n * If @objp is NULL, no operation is performed.\n *\n * Don't free memory not originally allocated by kmalloc()\n * or you will run into trouble.\n */\nvoid kfree(const void *objp)\n{\n\tstruct kmem_cache *c;\n\tunsigned long flags;\n\n\ttrace_kfree(_RET_IP_, objp);\n\n\tif (unlikely(ZERO_OR_NULL_PTR(objp)))\n\t\treturn;\n\tlocal_irq_save(flags);\n\tkfree_debugcheck(objp);\n\tc = virt_to_cache(objp);\n\tdebug_check_no_locks_freed(objp, c->object_size);\n\n\tdebug_check_no_obj_freed(objp, c->object_size);\n\t__cache_free(c, (void *)objp, _RET_IP_);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(kfree);\n\n/*\n * This initializes kmem_cache_node or resizes various caches for all nodes.\n */\nstatic int setup_kmem_cache_nodes(struct kmem_cache *cachep, gfp_t gfp)\n{\n\tint ret;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_online_node(node) {\n\t\tret = setup_kmem_cache_node(cachep, node, gfp, true);\n\t\tif (ret)\n\t\t\tgoto fail;\n\n\t}\n\n\treturn 0;\n\nfail:\n\tif (!cachep->list.next) {\n\t\t/* Cache is not active yet. Roll back what we did */\n\t\tnode--;\n\t\twhile (node >= 0) {\n\t\t\tn = get_node(cachep, node);\n\t\t\tif (n) {\n\t\t\t\tkfree(n->shared);\n\t\t\t\tfree_alien_cache(n->alien);\n\t\t\t\tkfree(n);\n\t\t\t\tcachep->node[node] = NULL;\n\t\t\t}\n\t\t\tnode--;\n\t\t}\n\t}\n\treturn -ENOMEM;\n}\n\n/* Always called with the slab_mutex held */\nstatic int __do_tune_cpucache(struct kmem_cache *cachep, int limit,\n\t\t\t\tint batchcount, int shared, gfp_t gfp)\n{\n\tstruct array_cache __percpu *cpu_cache, *prev;\n\tint cpu;\n\n\tcpu_cache = alloc_kmem_cache_cpus(cachep, limit, batchcount);\n\tif (!cpu_cache)\n\t\treturn -ENOMEM;\n\n\tprev = cachep->cpu_cache;\n\tcachep->cpu_cache = cpu_cache;\n\tkick_all_cpus_sync();\n\n\tcheck_irq_on();\n\tcachep->batchcount = batchcount;\n\tcachep->limit = limit;\n\tcachep->shared = shared;\n\n\tif (!prev)\n\t\tgoto setup_node;\n\n\tfor_each_online_cpu(cpu) {\n\t\tLIST_HEAD(list);\n\t\tint node;\n\t\tstruct kmem_cache_node *n;\n\t\tstruct array_cache *ac = per_cpu_ptr(prev, cpu);\n\n\t\tnode = cpu_to_mem(cpu);\n\t\tn = get_node(cachep, node);\n\t\tspin_lock_irq(&n->list_lock);\n\t\tfree_block(cachep, ac->entry, ac->avail, node, &list);\n\t\tspin_unlock_irq(&n->list_lock);\n\t\tslabs_destroy(cachep, &list);\n\t}\n\tfree_percpu(prev);\n\nsetup_node:\n\treturn setup_kmem_cache_nodes(cachep, gfp);\n}\n\nstatic int do_tune_cpucache(struct kmem_cache *cachep, int limit,\n\t\t\t\tint batchcount, int shared, gfp_t gfp)\n{\n\tint ret;\n\tstruct kmem_cache *c;\n\n\tret = __do_tune_cpucache(cachep, limit, batchcount, shared, gfp);\n\n\tif (slab_state < FULL)\n\t\treturn ret;\n\n\tif ((ret < 0) || !is_root_cache(cachep))\n\t\treturn ret;\n\n\tlockdep_assert_held(&slab_mutex);\n\tfor_each_memcg_cache(c, cachep) {\n\t\t/* return value determined by the root cache only */\n\t\t__do_tune_cpucache(c, limit, batchcount, shared, gfp);\n\t}\n\n\treturn ret;\n}\n\n/* Called with slab_mutex held always */\nstatic int enable_cpucache(struct kmem_cache *cachep, gfp_t gfp)\n{\n\tint err;\n\tint limit = 0;\n\tint shared = 0;\n\tint batchcount = 0;\n\n\terr = cache_random_seq_create(cachep, cachep->num, gfp);\n\tif (err)\n\t\tgoto end;\n\n\tif (!is_root_cache(cachep)) {\n\t\tstruct kmem_cache *root = memcg_root_cache(cachep);\n\t\tlimit = root->limit;\n\t\tshared = root->shared;\n\t\tbatchcount = root->batchcount;\n\t}\n\n\tif (limit && shared && batchcount)\n\t\tgoto skip_setup;\n\t/*\n\t * The head array serves three purposes:\n\t * - create a LIFO ordering, i.e. return objects that are cache-warm\n\t * - reduce the number of spinlock operations.\n\t * - reduce the number of linked list operations on the slab and\n\t *   bufctl chains: array operations are cheaper.\n\t * The numbers are guessed, we should auto-tune as described by\n\t * Bonwick.\n\t */\n\tif (cachep->size > 131072)\n\t\tlimit = 1;\n\telse if (cachep->size > PAGE_SIZE)\n\t\tlimit = 8;\n\telse if (cachep->size > 1024)\n\t\tlimit = 24;\n\telse if (cachep->size > 256)\n\t\tlimit = 54;\n\telse\n\t\tlimit = 120;\n\n\t/*\n\t * CPU bound tasks (e.g. network routing) can exhibit cpu bound\n\t * allocation behaviour: Most allocs on one cpu, most free operations\n\t * on another cpu. For these cases, an efficient object passing between\n\t * cpus is necessary. This is provided by a shared array. The array\n\t * replaces Bonwick's magazine layer.\n\t * On uniprocessor, it's functionally equivalent (but less efficient)\n\t * to a larger limit. Thus disabled by default.\n\t */\n\tshared = 0;\n\tif (cachep->size <= PAGE_SIZE && num_possible_cpus() > 1)\n\t\tshared = 8;\n\n#if DEBUG\n\t/*\n\t * With debugging enabled, large batchcount lead to excessively long\n\t * periods with disabled local interrupts. Limit the batchcount\n\t */\n\tif (limit > 32)\n\t\tlimit = 32;\n#endif\n\tbatchcount = (limit + 1) / 2;\nskip_setup:\n\terr = do_tune_cpucache(cachep, limit, batchcount, shared, gfp);\nend:\n\tif (err)\n\t\tpr_err(\"enable_cpucache failed for %s, error %d\\n\",\n\t\t       cachep->name, -err);\n\treturn err;\n}\n\n/*\n * Drain an array if it contains any elements taking the node lock only if\n * necessary. Note that the node listlock also protects the array_cache\n * if drain_array() is used on the shared array.\n */\nstatic void drain_array(struct kmem_cache *cachep, struct kmem_cache_node *n,\n\t\t\t struct array_cache *ac, int node)\n{\n\tLIST_HEAD(list);\n\n\t/* ac from n->shared can be freed if we don't hold the slab_mutex. */\n\tcheck_mutex_acquired();\n\n\tif (!ac || !ac->avail)\n\t\treturn;\n\n\tif (ac->touched) {\n\t\tac->touched = 0;\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&n->list_lock);\n\tdrain_array_locked(cachep, ac, node, false, &list);\n\tspin_unlock_irq(&n->list_lock);\n\n\tslabs_destroy(cachep, &list);\n}\n\n/**\n * cache_reap - Reclaim memory from caches.\n * @w: work descriptor\n *\n * Called from workqueue/eventd every few seconds.\n * Purpose:\n * - clear the per-cpu caches for this CPU.\n * - return freeable pages to the main free memory pool.\n *\n * If we cannot acquire the cache chain mutex then just give up - we'll try\n * again on the next iteration.\n */\nstatic void cache_reap(struct work_struct *w)\n{\n\tstruct kmem_cache *searchp;\n\tstruct kmem_cache_node *n;\n\tint node = numa_mem_id();\n\tstruct delayed_work *work = to_delayed_work(w);\n\n\tif (!mutex_trylock(&slab_mutex))\n\t\t/* Give up. Setup the next iteration. */\n\t\tgoto out;\n\n\tlist_for_each_entry(searchp, &slab_caches, list) {\n\t\tcheck_irq_on();\n\n\t\t/*\n\t\t * We only take the node lock if absolutely necessary and we\n\t\t * have established with reasonable certainty that\n\t\t * we can do some work if the lock was obtained.\n\t\t */\n\t\tn = get_node(searchp, node);\n\n\t\treap_alien(searchp, n);\n\n\t\tdrain_array(searchp, n, cpu_cache_get(searchp), node);\n\n\t\t/*\n\t\t * These are racy checks but it does not matter\n\t\t * if we skip one check or scan twice.\n\t\t */\n\t\tif (time_after(n->next_reap, jiffies))\n\t\t\tgoto next;\n\n\t\tn->next_reap = jiffies + REAPTIMEOUT_NODE;\n\n\t\tdrain_array(searchp, n, n->shared, node);\n\n\t\tif (n->free_touched)\n\t\t\tn->free_touched = 0;\n\t\telse {\n\t\t\tint freed;\n\n\t\t\tfreed = drain_freelist(searchp, n, (n->free_limit +\n\t\t\t\t5 * searchp->num - 1) / (5 * searchp->num));\n\t\t\tSTATS_ADD_REAPED(searchp, freed);\n\t\t}\nnext:\n\t\tcond_resched();\n\t}\n\tcheck_irq_on();\n\tmutex_unlock(&slab_mutex);\n\tnext_reap_node();\nout:\n\t/* Set up the next iteration */\n\tschedule_delayed_work(work, round_jiffies_relative(REAPTIMEOUT_AC));\n}\n\n#ifdef CONFIG_SLABINFO\nvoid get_slabinfo(struct kmem_cache *cachep, struct slabinfo *sinfo)\n{\n\tunsigned long active_objs, num_objs, active_slabs;\n\tunsigned long total_slabs = 0, free_objs = 0, shared_avail = 0;\n\tunsigned long free_slabs = 0;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tcheck_irq_on();\n\t\tspin_lock_irq(&n->list_lock);\n\n\t\ttotal_slabs += n->total_slabs;\n\t\tfree_slabs += n->free_slabs;\n\t\tfree_objs += n->free_objects;\n\n\t\tif (n->shared)\n\t\t\tshared_avail += n->shared->avail;\n\n\t\tspin_unlock_irq(&n->list_lock);\n\t}\n\tnum_objs = total_slabs * cachep->num;\n\tactive_slabs = total_slabs - free_slabs;\n\tactive_objs = num_objs - free_objs;\n\n\tsinfo->active_objs = active_objs;\n\tsinfo->num_objs = num_objs;\n\tsinfo->active_slabs = active_slabs;\n\tsinfo->num_slabs = total_slabs;\n\tsinfo->shared_avail = shared_avail;\n\tsinfo->limit = cachep->limit;\n\tsinfo->batchcount = cachep->batchcount;\n\tsinfo->shared = cachep->shared;\n\tsinfo->objects_per_slab = cachep->num;\n\tsinfo->cache_order = cachep->gfporder;\n}\n\nvoid slabinfo_show_stats(struct seq_file *m, struct kmem_cache *cachep)\n{\n#if STATS\n\t{\t\t\t/* node stats */\n\t\tunsigned long high = cachep->high_mark;\n\t\tunsigned long allocs = cachep->num_allocations;\n\t\tunsigned long grown = cachep->grown;\n\t\tunsigned long reaped = cachep->reaped;\n\t\tunsigned long errors = cachep->errors;\n\t\tunsigned long max_freeable = cachep->max_freeable;\n\t\tunsigned long node_allocs = cachep->node_allocs;\n\t\tunsigned long node_frees = cachep->node_frees;\n\t\tunsigned long overflows = cachep->node_overflow;\n\n\t\tseq_printf(m, \" : globalstat %7lu %6lu %5lu %4lu %4lu %4lu %4lu %4lu %4lu\",\n\t\t\t   allocs, high, grown,\n\t\t\t   reaped, errors, max_freeable, node_allocs,\n\t\t\t   node_frees, overflows);\n\t}\n\t/* cpu stats */\n\t{\n\t\tunsigned long allochit = atomic_read(&cachep->allochit);\n\t\tunsigned long allocmiss = atomic_read(&cachep->allocmiss);\n\t\tunsigned long freehit = atomic_read(&cachep->freehit);\n\t\tunsigned long freemiss = atomic_read(&cachep->freemiss);\n\n\t\tseq_printf(m, \" : cpustat %6lu %6lu %6lu %6lu\",\n\t\t\t   allochit, allocmiss, freehit, freemiss);\n\t}\n#endif\n}\n\n#define MAX_SLABINFO_WRITE 128\n/**\n * slabinfo_write - Tuning for the slab allocator\n * @file: unused\n * @buffer: user buffer\n * @count: data length\n * @ppos: unused\n */\nssize_t slabinfo_write(struct file *file, const char __user *buffer,\n\t\t       size_t count, loff_t *ppos)\n{\n\tchar kbuf[MAX_SLABINFO_WRITE + 1], *tmp;\n\tint limit, batchcount, shared, res;\n\tstruct kmem_cache *cachep;\n\n\tif (count > MAX_SLABINFO_WRITE)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&kbuf, buffer, count))\n\t\treturn -EFAULT;\n\tkbuf[MAX_SLABINFO_WRITE] = '\\0';\n\n\ttmp = strchr(kbuf, ' ');\n\tif (!tmp)\n\t\treturn -EINVAL;\n\t*tmp = '\\0';\n\ttmp++;\n\tif (sscanf(tmp, \" %d %d %d\", &limit, &batchcount, &shared) != 3)\n\t\treturn -EINVAL;\n\n\t/* Find the cache in the chain of caches. */\n\tmutex_lock(&slab_mutex);\n\tres = -EINVAL;\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tif (!strcmp(cachep->name, kbuf)) {\n\t\t\tif (limit < 1 || batchcount < 1 ||\n\t\t\t\t\tbatchcount > limit || shared < 0) {\n\t\t\t\tres = 0;\n\t\t\t} else {\n\t\t\t\tres = do_tune_cpucache(cachep, limit,\n\t\t\t\t\t\t       batchcount, shared,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&slab_mutex);\n\tif (res >= 0)\n\t\tres = count;\n\treturn res;\n}\n\n#ifdef CONFIG_DEBUG_SLAB_LEAK\n\nstatic inline int add_caller(unsigned long *n, unsigned long v)\n{\n\tunsigned long *p;\n\tint l;\n\tif (!v)\n\t\treturn 1;\n\tl = n[1];\n\tp = n + 2;\n\twhile (l) {\n\t\tint i = l/2;\n\t\tunsigned long *q = p + 2 * i;\n\t\tif (*q == v) {\n\t\t\tq[1]++;\n\t\t\treturn 1;\n\t\t}\n\t\tif (*q > v) {\n\t\t\tl = i;\n\t\t} else {\n\t\t\tp = q + 2;\n\t\t\tl -= i + 1;\n\t\t}\n\t}\n\tif (++n[1] == n[0])\n\t\treturn 0;\n\tmemmove(p + 2, p, n[1] * 2 * sizeof(unsigned long) - ((void *)p - (void *)n));\n\tp[0] = v;\n\tp[1] = 1;\n\treturn 1;\n}\n\nstatic void handle_slab(unsigned long *n, struct kmem_cache *c,\n\t\t\t\t\t\tstruct page *page)\n{\n\tvoid *p;\n\tint i, j;\n\tunsigned long v;\n\n\tif (n[0] == n[1])\n\t\treturn;\n\tfor (i = 0, p = page->s_mem; i < c->num; i++, p += c->size) {\n\t\tbool active = true;\n\n\t\tfor (j = page->active; j < c->num; j++) {\n\t\t\tif (get_free_obj(page, j) == i) {\n\t\t\t\tactive = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!active)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * probe_kernel_read() is used for DEBUG_PAGEALLOC. page table\n\t\t * mapping is established when actual object allocation and\n\t\t * we could mistakenly access the unmapped object in the cpu\n\t\t * cache.\n\t\t */\n\t\tif (probe_kernel_read(&v, dbg_userword(c, p), sizeof(v)))\n\t\t\tcontinue;\n\n\t\tif (!add_caller(n, v))\n\t\t\treturn;\n\t}\n}\n\nstatic void show_symbol(struct seq_file *m, unsigned long address)\n{\n#ifdef CONFIG_KALLSYMS\n\tunsigned long offset, size;\n\tchar modname[MODULE_NAME_LEN], name[KSYM_NAME_LEN];\n\n\tif (lookup_symbol_attrs(address, &size, &offset, modname, name) == 0) {\n\t\tseq_printf(m, \"%s+%#lx/%#lx\", name, offset, size);\n\t\tif (modname[0])\n\t\t\tseq_printf(m, \" [%s]\", modname);\n\t\treturn;\n\t}\n#endif\n\tseq_printf(m, \"%p\", (void *)address);\n}\n\nstatic int leaks_show(struct seq_file *m, void *p)\n{\n\tstruct kmem_cache *cachep = list_entry(p, struct kmem_cache, list);\n\tstruct page *page;\n\tstruct kmem_cache_node *n;\n\tconst char *name;\n\tunsigned long *x = m->private;\n\tint node;\n\tint i;\n\n\tif (!(cachep->flags & SLAB_STORE_USER))\n\t\treturn 0;\n\tif (!(cachep->flags & SLAB_RED_ZONE))\n\t\treturn 0;\n\n\t/*\n\t * Set store_user_clean and start to grab stored user information\n\t * for all objects on this cache. If some alloc/free requests comes\n\t * during the processing, information would be wrong so restart\n\t * whole processing.\n\t */\n\tdo {\n\t\tset_store_user_clean(cachep);\n\t\tdrain_cpu_caches(cachep);\n\n\t\tx[1] = 0;\n\n\t\tfor_each_kmem_cache_node(cachep, node, n) {\n\n\t\t\tcheck_irq_on();\n\t\t\tspin_lock_irq(&n->list_lock);\n\n\t\t\tlist_for_each_entry(page, &n->slabs_full, lru)\n\t\t\t\thandle_slab(x, cachep, page);\n\t\t\tlist_for_each_entry(page, &n->slabs_partial, lru)\n\t\t\t\thandle_slab(x, cachep, page);\n\t\t\tspin_unlock_irq(&n->list_lock);\n\t\t}\n\t} while (!is_store_user_clean(cachep));\n\n\tname = cachep->name;\n\tif (x[0] == x[1]) {\n\t\t/* Increase the buffer size */\n\t\tmutex_unlock(&slab_mutex);\n\t\tm->private = kzalloc(x[0] * 4 * sizeof(unsigned long), GFP_KERNEL);\n\t\tif (!m->private) {\n\t\t\t/* Too bad, we are really out */\n\t\t\tm->private = x;\n\t\t\tmutex_lock(&slab_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\t*(unsigned long *)m->private = x[0] * 2;\n\t\tkfree(x);\n\t\tmutex_lock(&slab_mutex);\n\t\t/* Now make sure this entry will be retried */\n\t\tm->count = m->size;\n\t\treturn 0;\n\t}\n\tfor (i = 0; i < x[1]; i++) {\n\t\tseq_printf(m, \"%s: %lu \", name, x[2*i+3]);\n\t\tshow_symbol(m, x[2*i+2]);\n\t\tseq_putc(m, '\\n');\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations slabstats_op = {\n\t.start = slab_start,\n\t.next = slab_next,\n\t.stop = slab_stop,\n\t.show = leaks_show,\n};\n\nstatic int slabstats_open(struct inode *inode, struct file *file)\n{\n\tunsigned long *n;\n\n\tn = __seq_open_private(file, &slabstats_op, PAGE_SIZE);\n\tif (!n)\n\t\treturn -ENOMEM;\n\n\t*n = PAGE_SIZE / (2 * sizeof(unsigned long));\n\n\treturn 0;\n}\n\nstatic const struct file_operations proc_slabstats_operations = {\n\t.open\t\t= slabstats_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_private,\n};\n#endif\n\nstatic int __init slab_proc_init(void)\n{\n#ifdef CONFIG_DEBUG_SLAB_LEAK\n\tproc_create(\"slab_allocators\", 0, NULL, &proc_slabstats_operations);\n#endif\n\treturn 0;\n}\nmodule_init(slab_proc_init);\n#endif\n\n#ifdef CONFIG_HARDENED_USERCOPY\n/*\n * Rejects objects that are incorrectly sized.\n *\n * Returns NULL if check passes, otherwise const char * to name of cache\n * to indicate an error.\n */\nconst char *__check_heap_object(const void *ptr, unsigned long n,\n\t\t\t\tstruct page *page)\n{\n\tstruct kmem_cache *cachep;\n\tunsigned int objnr;\n\tunsigned long offset;\n\n\t/* Find and validate object. */\n\tcachep = page->slab_cache;\n\tobjnr = obj_to_index(cachep, page, (void *)ptr);\n\tBUG_ON(objnr >= cachep->num);\n\n\t/* Find offset within object. */\n\toffset = ptr - index_to_obj(cachep, page, objnr) - obj_offset(cachep);\n\n\t/* Allow address range falling entirely within object size. */\n\tif (offset <= cachep->object_size && n <= cachep->object_size - offset)\n\t\treturn NULL;\n\n\treturn cachep->name;\n}\n#endif /* CONFIG_HARDENED_USERCOPY */\n\n/**\n * ksize - get the actual amount of memory allocated for a given object\n * @objp: Pointer to the object\n *\n * kmalloc may internally round up allocations and return more memory\n * than requested. ksize() can be used to determine the actual amount of\n * memory allocated. The caller may use this additional memory, even though\n * a smaller amount of memory was initially specified with the kmalloc call.\n * The caller must guarantee that objp points to a valid object previously\n * allocated with either kmalloc() or kmem_cache_alloc(). The object\n * must not be freed during the duration of the call.\n */\nsize_t ksize(const void *objp)\n{\n\tsize_t size;\n\n\tBUG_ON(!objp);\n\tif (unlikely(objp == ZERO_SIZE_PTR))\n\t\treturn 0;\n\n\tsize = virt_to_cache(objp)->object_size;\n\t/* We assume that ksize callers could use the whole allocated area,\n\t * so we need to unpoison this area.\n\t */\n\tkasan_unpoison_shadow(objp, size);\n\n\treturn size;\n}\nEXPORT_SYMBOL(ksize);\n"], "fixing_code": ["/*\n * linux/mm/slab.c\n * Written by Mark Hemment, 1996/97.\n * (markhe@nextd.demon.co.uk)\n *\n * kmem_cache_destroy() + some cleanup - 1999 Andrea Arcangeli\n *\n * Major cleanup, different bufctl logic, per-cpu arrays\n *\t(c) 2000 Manfred Spraul\n *\n * Cleanup, make the head arrays unconditional, preparation for NUMA\n * \t(c) 2002 Manfred Spraul\n *\n * An implementation of the Slab Allocator as described in outline in;\n *\tUNIX Internals: The New Frontiers by Uresh Vahalia\n *\tPub: Prentice Hall\tISBN 0-13-101908-2\n * or with a little more detail in;\n *\tThe Slab Allocator: An Object-Caching Kernel Memory Allocator\n *\tJeff Bonwick (Sun Microsystems).\n *\tPresented at: USENIX Summer 1994 Technical Conference\n *\n * The memory is organized in caches, one cache for each object type.\n * (e.g. inode_cache, dentry_cache, buffer_head, vm_area_struct)\n * Each cache consists out of many slabs (they are small (usually one\n * page long) and always contiguous), and each slab contains multiple\n * initialized objects.\n *\n * This means, that your constructor is used only for newly allocated\n * slabs and you must pass objects with the same initializations to\n * kmem_cache_free.\n *\n * Each cache can only support one memory type (GFP_DMA, GFP_HIGHMEM,\n * normal). If you need a special memory type, then must create a new\n * cache for that memory type.\n *\n * In order to reduce fragmentation, the slabs are sorted in 3 groups:\n *   full slabs with 0 free objects\n *   partial slabs\n *   empty slabs with no allocated objects\n *\n * If partial slabs exist, then new allocations come from these slabs,\n * otherwise from empty slabs or new slabs are allocated.\n *\n * kmem_cache_destroy() CAN CRASH if you try to allocate from the cache\n * during kmem_cache_destroy(). The caller must prevent concurrent allocs.\n *\n * Each cache has a short per-cpu head array, most allocs\n * and frees go into that array, and if that array overflows, then 1/2\n * of the entries in the array are given back into the global cache.\n * The head array is strictly LIFO and should improve the cache hit rates.\n * On SMP, it additionally reduces the spinlock operations.\n *\n * The c_cpuarray may not be read with enabled local interrupts -\n * it's changed with a smp_call_function().\n *\n * SMP synchronization:\n *  constructors and destructors are called without any locking.\n *  Several members in struct kmem_cache and struct slab never change, they\n *\tare accessed without any locking.\n *  The per-cpu arrays are never accessed from the wrong cpu, no locking,\n *  \tand local interrupts are disabled so slab code is preempt-safe.\n *  The non-constant members are protected with a per-cache irq spinlock.\n *\n * Many thanks to Mark Hemment, who wrote another per-cpu slab patch\n * in 2000 - many ideas in the current implementation are derived from\n * his patch.\n *\n * Further notes from the original documentation:\n *\n * 11 April '97.  Started multi-threading - markhe\n *\tThe global cache-chain is protected by the mutex 'slab_mutex'.\n *\tThe sem is only needed when accessing/extending the cache-chain, which\n *\tcan never happen inside an interrupt (kmem_cache_create(),\n *\tkmem_cache_shrink() and kmem_cache_reap()).\n *\n *\tAt present, each engine can be growing a cache.  This should be blocked.\n *\n * 15 March 2005. NUMA slab allocator.\n *\tShai Fultheim <shai@scalex86.org>.\n *\tShobhit Dayal <shobhit@calsoftinc.com>\n *\tAlok N Kataria <alokk@calsoftinc.com>\n *\tChristoph Lameter <christoph@lameter.com>\n *\n *\tModified the slab allocator to be node aware on NUMA systems.\n *\tEach node has its own list of partial, free and full slabs.\n *\tAll object allocations for a node occur from node specific slab lists.\n */\n\n#include\t<linux/slab.h>\n#include\t<linux/mm.h>\n#include\t<linux/poison.h>\n#include\t<linux/swap.h>\n#include\t<linux/cache.h>\n#include\t<linux/interrupt.h>\n#include\t<linux/init.h>\n#include\t<linux/compiler.h>\n#include\t<linux/cpuset.h>\n#include\t<linux/proc_fs.h>\n#include\t<linux/seq_file.h>\n#include\t<linux/notifier.h>\n#include\t<linux/kallsyms.h>\n#include\t<linux/cpu.h>\n#include\t<linux/sysctl.h>\n#include\t<linux/module.h>\n#include\t<linux/rcupdate.h>\n#include\t<linux/string.h>\n#include\t<linux/uaccess.h>\n#include\t<linux/nodemask.h>\n#include\t<linux/kmemleak.h>\n#include\t<linux/mempolicy.h>\n#include\t<linux/mutex.h>\n#include\t<linux/fault-inject.h>\n#include\t<linux/rtmutex.h>\n#include\t<linux/reciprocal_div.h>\n#include\t<linux/debugobjects.h>\n#include\t<linux/kmemcheck.h>\n#include\t<linux/memory.h>\n#include\t<linux/prefetch.h>\n\n#include\t<net/sock.h>\n\n#include\t<asm/cacheflush.h>\n#include\t<asm/tlbflush.h>\n#include\t<asm/page.h>\n\n#include <trace/events/kmem.h>\n\n#include\t\"internal.h\"\n\n#include\t\"slab.h\"\n\n/*\n * DEBUG\t- 1 for kmem_cache_create() to honour; SLAB_RED_ZONE & SLAB_POISON.\n *\t\t  0 for faster, smaller code (especially in the critical paths).\n *\n * STATS\t- 1 to collect stats for /proc/slabinfo.\n *\t\t  0 for faster, smaller code (especially in the critical paths).\n *\n * FORCED_DEBUG\t- 1 enables SLAB_RED_ZONE and SLAB_POISON (if possible)\n */\n\n#ifdef CONFIG_DEBUG_SLAB\n#define\tDEBUG\t\t1\n#define\tSTATS\t\t1\n#define\tFORCED_DEBUG\t1\n#else\n#define\tDEBUG\t\t0\n#define\tSTATS\t\t0\n#define\tFORCED_DEBUG\t0\n#endif\n\n/* Shouldn't this be in a header file somewhere? */\n#define\tBYTES_PER_WORD\t\tsizeof(void *)\n#define\tREDZONE_ALIGN\t\tmax(BYTES_PER_WORD, __alignof__(unsigned long long))\n\n#ifndef ARCH_KMALLOC_FLAGS\n#define ARCH_KMALLOC_FLAGS SLAB_HWCACHE_ALIGN\n#endif\n\n#define FREELIST_BYTE_INDEX (((PAGE_SIZE >> BITS_PER_BYTE) \\\n\t\t\t\t<= SLAB_OBJ_MIN_SIZE) ? 1 : 0)\n\n#if FREELIST_BYTE_INDEX\ntypedef unsigned char freelist_idx_t;\n#else\ntypedef unsigned short freelist_idx_t;\n#endif\n\n#define SLAB_OBJ_MAX_NUM ((1 << sizeof(freelist_idx_t) * BITS_PER_BYTE) - 1)\n\n/*\n * struct array_cache\n *\n * Purpose:\n * - LIFO ordering, to hand out cache-warm objects from _alloc\n * - reduce the number of linked list operations\n * - reduce spinlock operations\n *\n * The limit is stored in the per-cpu structure to reduce the data cache\n * footprint.\n *\n */\nstruct array_cache {\n\tunsigned int avail;\n\tunsigned int limit;\n\tunsigned int batchcount;\n\tunsigned int touched;\n\tvoid *entry[];\t/*\n\t\t\t * Must have this definition in here for the proper\n\t\t\t * alignment of array_cache. Also simplifies accessing\n\t\t\t * the entries.\n\t\t\t */\n};\n\nstruct alien_cache {\n\tspinlock_t lock;\n\tstruct array_cache ac;\n};\n\n/*\n * Need this for bootstrapping a per node allocator.\n */\n#define NUM_INIT_LISTS (2 * MAX_NUMNODES)\nstatic struct kmem_cache_node __initdata init_kmem_cache_node[NUM_INIT_LISTS];\n#define\tCACHE_CACHE 0\n#define\tSIZE_NODE (MAX_NUMNODES)\n\nstatic int drain_freelist(struct kmem_cache *cache,\n\t\t\tstruct kmem_cache_node *n, int tofree);\nstatic void free_block(struct kmem_cache *cachep, void **objpp, int len,\n\t\t\tint node, struct list_head *list);\nstatic void slabs_destroy(struct kmem_cache *cachep, struct list_head *list);\nstatic int enable_cpucache(struct kmem_cache *cachep, gfp_t gfp);\nstatic void cache_reap(struct work_struct *unused);\n\nstatic inline void fixup_objfreelist_debug(struct kmem_cache *cachep,\n\t\t\t\t\t\tvoid **list);\nstatic inline void fixup_slab_list(struct kmem_cache *cachep,\n\t\t\t\tstruct kmem_cache_node *n, struct page *page,\n\t\t\t\tvoid **list);\nstatic int slab_early_init = 1;\n\n#define INDEX_NODE kmalloc_index(sizeof(struct kmem_cache_node))\n\nstatic void kmem_cache_node_init(struct kmem_cache_node *parent)\n{\n\tINIT_LIST_HEAD(&parent->slabs_full);\n\tINIT_LIST_HEAD(&parent->slabs_partial);\n\tINIT_LIST_HEAD(&parent->slabs_free);\n\tparent->total_slabs = 0;\n\tparent->free_slabs = 0;\n\tparent->shared = NULL;\n\tparent->alien = NULL;\n\tparent->colour_next = 0;\n\tspin_lock_init(&parent->list_lock);\n\tparent->free_objects = 0;\n\tparent->free_touched = 0;\n}\n\n#define MAKE_LIST(cachep, listp, slab, nodeid)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tINIT_LIST_HEAD(listp);\t\t\t\t\t\\\n\t\tlist_splice(&get_node(cachep, nodeid)->slab, listp);\t\\\n\t} while (0)\n\n#define\tMAKE_ALL_LISTS(cachep, ptr, nodeid)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\tMAKE_LIST((cachep), (&(ptr)->slabs_full), slabs_full, nodeid);\t\\\n\tMAKE_LIST((cachep), (&(ptr)->slabs_partial), slabs_partial, nodeid); \\\n\tMAKE_LIST((cachep), (&(ptr)->slabs_free), slabs_free, nodeid);\t\\\n\t} while (0)\n\n#define CFLGS_OBJFREELIST_SLAB\t(0x40000000UL)\n#define CFLGS_OFF_SLAB\t\t(0x80000000UL)\n#define\tOBJFREELIST_SLAB(x)\t((x)->flags & CFLGS_OBJFREELIST_SLAB)\n#define\tOFF_SLAB(x)\t((x)->flags & CFLGS_OFF_SLAB)\n\n#define BATCHREFILL_LIMIT\t16\n/*\n * Optimization question: fewer reaps means less probability for unnessary\n * cpucache drain/refill cycles.\n *\n * OTOH the cpuarrays can contain lots of objects,\n * which could lock up otherwise freeable slabs.\n */\n#define REAPTIMEOUT_AC\t\t(2*HZ)\n#define REAPTIMEOUT_NODE\t(4*HZ)\n\n#if STATS\n#define\tSTATS_INC_ACTIVE(x)\t((x)->num_active++)\n#define\tSTATS_DEC_ACTIVE(x)\t((x)->num_active--)\n#define\tSTATS_INC_ALLOCED(x)\t((x)->num_allocations++)\n#define\tSTATS_INC_GROWN(x)\t((x)->grown++)\n#define\tSTATS_ADD_REAPED(x,y)\t((x)->reaped += (y))\n#define\tSTATS_SET_HIGH(x)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif ((x)->num_active > (x)->high_mark)\t\t\t\\\n\t\t\t(x)->high_mark = (x)->num_active;\t\t\\\n\t} while (0)\n#define\tSTATS_INC_ERR(x)\t((x)->errors++)\n#define\tSTATS_INC_NODEALLOCS(x)\t((x)->node_allocs++)\n#define\tSTATS_INC_NODEFREES(x)\t((x)->node_frees++)\n#define STATS_INC_ACOVERFLOW(x)   ((x)->node_overflow++)\n#define\tSTATS_SET_FREEABLE(x, i)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif ((x)->max_freeable < i)\t\t\t\t\\\n\t\t\t(x)->max_freeable = i;\t\t\t\t\\\n\t} while (0)\n#define STATS_INC_ALLOCHIT(x)\tatomic_inc(&(x)->allochit)\n#define STATS_INC_ALLOCMISS(x)\tatomic_inc(&(x)->allocmiss)\n#define STATS_INC_FREEHIT(x)\tatomic_inc(&(x)->freehit)\n#define STATS_INC_FREEMISS(x)\tatomic_inc(&(x)->freemiss)\n#else\n#define\tSTATS_INC_ACTIVE(x)\tdo { } while (0)\n#define\tSTATS_DEC_ACTIVE(x)\tdo { } while (0)\n#define\tSTATS_INC_ALLOCED(x)\tdo { } while (0)\n#define\tSTATS_INC_GROWN(x)\tdo { } while (0)\n#define\tSTATS_ADD_REAPED(x,y)\tdo { (void)(y); } while (0)\n#define\tSTATS_SET_HIGH(x)\tdo { } while (0)\n#define\tSTATS_INC_ERR(x)\tdo { } while (0)\n#define\tSTATS_INC_NODEALLOCS(x)\tdo { } while (0)\n#define\tSTATS_INC_NODEFREES(x)\tdo { } while (0)\n#define STATS_INC_ACOVERFLOW(x)   do { } while (0)\n#define\tSTATS_SET_FREEABLE(x, i) do { } while (0)\n#define STATS_INC_ALLOCHIT(x)\tdo { } while (0)\n#define STATS_INC_ALLOCMISS(x)\tdo { } while (0)\n#define STATS_INC_FREEHIT(x)\tdo { } while (0)\n#define STATS_INC_FREEMISS(x)\tdo { } while (0)\n#endif\n\n#if DEBUG\n\n/*\n * memory layout of objects:\n * 0\t\t: objp\n * 0 .. cachep->obj_offset - BYTES_PER_WORD - 1: padding. This ensures that\n * \t\tthe end of an object is aligned with the end of the real\n * \t\tallocation. Catches writes behind the end of the allocation.\n * cachep->obj_offset - BYTES_PER_WORD .. cachep->obj_offset - 1:\n * \t\tredzone word.\n * cachep->obj_offset: The real object.\n * cachep->size - 2* BYTES_PER_WORD: redzone word [BYTES_PER_WORD long]\n * cachep->size - 1* BYTES_PER_WORD: last caller address\n *\t\t\t\t\t[BYTES_PER_WORD long]\n */\nstatic int obj_offset(struct kmem_cache *cachep)\n{\n\treturn cachep->obj_offset;\n}\n\nstatic unsigned long long *dbg_redzone1(struct kmem_cache *cachep, void *objp)\n{\n\tBUG_ON(!(cachep->flags & SLAB_RED_ZONE));\n\treturn (unsigned long long*) (objp + obj_offset(cachep) -\n\t\t\t\t      sizeof(unsigned long long));\n}\n\nstatic unsigned long long *dbg_redzone2(struct kmem_cache *cachep, void *objp)\n{\n\tBUG_ON(!(cachep->flags & SLAB_RED_ZONE));\n\tif (cachep->flags & SLAB_STORE_USER)\n\t\treturn (unsigned long long *)(objp + cachep->size -\n\t\t\t\t\t      sizeof(unsigned long long) -\n\t\t\t\t\t      REDZONE_ALIGN);\n\treturn (unsigned long long *) (objp + cachep->size -\n\t\t\t\t       sizeof(unsigned long long));\n}\n\nstatic void **dbg_userword(struct kmem_cache *cachep, void *objp)\n{\n\tBUG_ON(!(cachep->flags & SLAB_STORE_USER));\n\treturn (void **)(objp + cachep->size - BYTES_PER_WORD);\n}\n\n#else\n\n#define obj_offset(x)\t\t\t0\n#define dbg_redzone1(cachep, objp)\t({BUG(); (unsigned long long *)NULL;})\n#define dbg_redzone2(cachep, objp)\t({BUG(); (unsigned long long *)NULL;})\n#define dbg_userword(cachep, objp)\t({BUG(); (void **)NULL;})\n\n#endif\n\n#ifdef CONFIG_DEBUG_SLAB_LEAK\n\nstatic inline bool is_store_user_clean(struct kmem_cache *cachep)\n{\n\treturn atomic_read(&cachep->store_user_clean) == 1;\n}\n\nstatic inline void set_store_user_clean(struct kmem_cache *cachep)\n{\n\tatomic_set(&cachep->store_user_clean, 1);\n}\n\nstatic inline void set_store_user_dirty(struct kmem_cache *cachep)\n{\n\tif (is_store_user_clean(cachep))\n\t\tatomic_set(&cachep->store_user_clean, 0);\n}\n\n#else\nstatic inline void set_store_user_dirty(struct kmem_cache *cachep) {}\n\n#endif\n\n/*\n * Do not go above this order unless 0 objects fit into the slab or\n * overridden on the command line.\n */\n#define\tSLAB_MAX_ORDER_HI\t1\n#define\tSLAB_MAX_ORDER_LO\t0\nstatic int slab_max_order = SLAB_MAX_ORDER_LO;\nstatic bool slab_max_order_set __initdata;\n\nstatic inline struct kmem_cache *virt_to_cache(const void *obj)\n{\n\tstruct page *page = virt_to_head_page(obj);\n\treturn page->slab_cache;\n}\n\nstatic inline void *index_to_obj(struct kmem_cache *cache, struct page *page,\n\t\t\t\t unsigned int idx)\n{\n\treturn page->s_mem + cache->size * idx;\n}\n\n/*\n * We want to avoid an expensive divide : (offset / cache->size)\n *   Using the fact that size is a constant for a particular cache,\n *   we can replace (offset / cache->size) by\n *   reciprocal_divide(offset, cache->reciprocal_buffer_size)\n */\nstatic inline unsigned int obj_to_index(const struct kmem_cache *cache,\n\t\t\t\t\tconst struct page *page, void *obj)\n{\n\tu32 offset = (obj - page->s_mem);\n\treturn reciprocal_divide(offset, cache->reciprocal_buffer_size);\n}\n\n#define BOOT_CPUCACHE_ENTRIES\t1\n/* internal cache of cache description objs */\nstatic struct kmem_cache kmem_cache_boot = {\n\t.batchcount = 1,\n\t.limit = BOOT_CPUCACHE_ENTRIES,\n\t.shared = 1,\n\t.size = sizeof(struct kmem_cache),\n\t.name = \"kmem_cache\",\n};\n\nstatic DEFINE_PER_CPU(struct delayed_work, slab_reap_work);\n\nstatic inline struct array_cache *cpu_cache_get(struct kmem_cache *cachep)\n{\n\treturn this_cpu_ptr(cachep->cpu_cache);\n}\n\n/*\n * Calculate the number of objects and left-over bytes for a given buffer size.\n */\nstatic unsigned int cache_estimate(unsigned long gfporder, size_t buffer_size,\n\t\tunsigned long flags, size_t *left_over)\n{\n\tunsigned int num;\n\tsize_t slab_size = PAGE_SIZE << gfporder;\n\n\t/*\n\t * The slab management structure can be either off the slab or\n\t * on it. For the latter case, the memory allocated for a\n\t * slab is used for:\n\t *\n\t * - @buffer_size bytes for each object\n\t * - One freelist_idx_t for each object\n\t *\n\t * We don't need to consider alignment of freelist because\n\t * freelist will be at the end of slab page. The objects will be\n\t * at the correct alignment.\n\t *\n\t * If the slab management structure is off the slab, then the\n\t * alignment will already be calculated into the size. Because\n\t * the slabs are all pages aligned, the objects will be at the\n\t * correct alignment when allocated.\n\t */\n\tif (flags & (CFLGS_OBJFREELIST_SLAB | CFLGS_OFF_SLAB)) {\n\t\tnum = slab_size / buffer_size;\n\t\t*left_over = slab_size % buffer_size;\n\t} else {\n\t\tnum = slab_size / (buffer_size + sizeof(freelist_idx_t));\n\t\t*left_over = slab_size %\n\t\t\t(buffer_size + sizeof(freelist_idx_t));\n\t}\n\n\treturn num;\n}\n\n#if DEBUG\n#define slab_error(cachep, msg) __slab_error(__func__, cachep, msg)\n\nstatic void __slab_error(const char *function, struct kmem_cache *cachep,\n\t\t\tchar *msg)\n{\n\tpr_err(\"slab error in %s(): cache `%s': %s\\n\",\n\t       function, cachep->name, msg);\n\tdump_stack();\n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n}\n#endif\n\n/*\n * By default on NUMA we use alien caches to stage the freeing of\n * objects allocated from other nodes. This causes massive memory\n * inefficiencies when using fake NUMA setup to split memory into a\n * large number of small nodes, so it can be disabled on the command\n * line\n  */\n\nstatic int use_alien_caches __read_mostly = 1;\nstatic int __init noaliencache_setup(char *s)\n{\n\tuse_alien_caches = 0;\n\treturn 1;\n}\n__setup(\"noaliencache\", noaliencache_setup);\n\nstatic int __init slab_max_order_setup(char *str)\n{\n\tget_option(&str, &slab_max_order);\n\tslab_max_order = slab_max_order < 0 ? 0 :\n\t\t\t\tmin(slab_max_order, MAX_ORDER - 1);\n\tslab_max_order_set = true;\n\n\treturn 1;\n}\n__setup(\"slab_max_order=\", slab_max_order_setup);\n\n#ifdef CONFIG_NUMA\n/*\n * Special reaping functions for NUMA systems called from cache_reap().\n * These take care of doing round robin flushing of alien caches (containing\n * objects freed on different nodes from which they were allocated) and the\n * flushing of remote pcps by calling drain_node_pages.\n */\nstatic DEFINE_PER_CPU(unsigned long, slab_reap_node);\n\nstatic void init_reap_node(int cpu)\n{\n\tper_cpu(slab_reap_node, cpu) = next_node_in(cpu_to_mem(cpu),\n\t\t\t\t\t\t    node_online_map);\n}\n\nstatic void next_reap_node(void)\n{\n\tint node = __this_cpu_read(slab_reap_node);\n\n\tnode = next_node_in(node, node_online_map);\n\t__this_cpu_write(slab_reap_node, node);\n}\n\n#else\n#define init_reap_node(cpu) do { } while (0)\n#define next_reap_node(void) do { } while (0)\n#endif\n\n/*\n * Initiate the reap timer running on the target CPU.  We run at around 1 to 2Hz\n * via the workqueue/eventd.\n * Add the CPU number into the expiration time to minimize the possibility of\n * the CPUs getting into lockstep and contending for the global cache chain\n * lock.\n */\nstatic void start_cpu_timer(int cpu)\n{\n\tstruct delayed_work *reap_work = &per_cpu(slab_reap_work, cpu);\n\n\tif (reap_work->work.func == NULL) {\n\t\tinit_reap_node(cpu);\n\t\tINIT_DEFERRABLE_WORK(reap_work, cache_reap);\n\t\tschedule_delayed_work_on(cpu, reap_work,\n\t\t\t\t\t__round_jiffies_relative(HZ, cpu));\n\t}\n}\n\nstatic void init_arraycache(struct array_cache *ac, int limit, int batch)\n{\n\t/*\n\t * The array_cache structures contain pointers to free object.\n\t * However, when such objects are allocated or transferred to another\n\t * cache the pointers are not cleared and they could be counted as\n\t * valid references during a kmemleak scan. Therefore, kmemleak must\n\t * not scan such objects.\n\t */\n\tkmemleak_no_scan(ac);\n\tif (ac) {\n\t\tac->avail = 0;\n\t\tac->limit = limit;\n\t\tac->batchcount = batch;\n\t\tac->touched = 0;\n\t}\n}\n\nstatic struct array_cache *alloc_arraycache(int node, int entries,\n\t\t\t\t\t    int batchcount, gfp_t gfp)\n{\n\tsize_t memsize = sizeof(void *) * entries + sizeof(struct array_cache);\n\tstruct array_cache *ac = NULL;\n\n\tac = kmalloc_node(memsize, gfp, node);\n\tinit_arraycache(ac, entries, batchcount);\n\treturn ac;\n}\n\nstatic noinline void cache_free_pfmemalloc(struct kmem_cache *cachep,\n\t\t\t\t\tstruct page *page, void *objp)\n{\n\tstruct kmem_cache_node *n;\n\tint page_node;\n\tLIST_HEAD(list);\n\n\tpage_node = page_to_nid(page);\n\tn = get_node(cachep, page_node);\n\n\tspin_lock(&n->list_lock);\n\tfree_block(cachep, &objp, 1, page_node, &list);\n\tspin_unlock(&n->list_lock);\n\n\tslabs_destroy(cachep, &list);\n}\n\n/*\n * Transfer objects in one arraycache to another.\n * Locking must be handled by the caller.\n *\n * Return the number of entries transferred.\n */\nstatic int transfer_objects(struct array_cache *to,\n\t\tstruct array_cache *from, unsigned int max)\n{\n\t/* Figure out how many entries to transfer */\n\tint nr = min3(from->avail, max, to->limit - to->avail);\n\n\tif (!nr)\n\t\treturn 0;\n\n\tmemcpy(to->entry + to->avail, from->entry + from->avail -nr,\n\t\t\tsizeof(void *) *nr);\n\n\tfrom->avail -= nr;\n\tto->avail += nr;\n\treturn nr;\n}\n\n#ifndef CONFIG_NUMA\n\n#define drain_alien_cache(cachep, alien) do { } while (0)\n#define reap_alien(cachep, n) do { } while (0)\n\nstatic inline struct alien_cache **alloc_alien_cache(int node,\n\t\t\t\t\t\tint limit, gfp_t gfp)\n{\n\treturn NULL;\n}\n\nstatic inline void free_alien_cache(struct alien_cache **ac_ptr)\n{\n}\n\nstatic inline int cache_free_alien(struct kmem_cache *cachep, void *objp)\n{\n\treturn 0;\n}\n\nstatic inline void *alternate_node_alloc(struct kmem_cache *cachep,\n\t\tgfp_t flags)\n{\n\treturn NULL;\n}\n\nstatic inline void *____cache_alloc_node(struct kmem_cache *cachep,\n\t\t gfp_t flags, int nodeid)\n{\n\treturn NULL;\n}\n\nstatic inline gfp_t gfp_exact_node(gfp_t flags)\n{\n\treturn flags & ~__GFP_NOFAIL;\n}\n\n#else\t/* CONFIG_NUMA */\n\nstatic void *____cache_alloc_node(struct kmem_cache *, gfp_t, int);\nstatic void *alternate_node_alloc(struct kmem_cache *, gfp_t);\n\nstatic struct alien_cache *__alloc_alien_cache(int node, int entries,\n\t\t\t\t\t\tint batch, gfp_t gfp)\n{\n\tsize_t memsize = sizeof(void *) * entries + sizeof(struct alien_cache);\n\tstruct alien_cache *alc = NULL;\n\n\talc = kmalloc_node(memsize, gfp, node);\n\tinit_arraycache(&alc->ac, entries, batch);\n\tspin_lock_init(&alc->lock);\n\treturn alc;\n}\n\nstatic struct alien_cache **alloc_alien_cache(int node, int limit, gfp_t gfp)\n{\n\tstruct alien_cache **alc_ptr;\n\tsize_t memsize = sizeof(void *) * nr_node_ids;\n\tint i;\n\n\tif (limit > 1)\n\t\tlimit = 12;\n\talc_ptr = kzalloc_node(memsize, gfp, node);\n\tif (!alc_ptr)\n\t\treturn NULL;\n\n\tfor_each_node(i) {\n\t\tif (i == node || !node_online(i))\n\t\t\tcontinue;\n\t\talc_ptr[i] = __alloc_alien_cache(node, limit, 0xbaadf00d, gfp);\n\t\tif (!alc_ptr[i]) {\n\t\t\tfor (i--; i >= 0; i--)\n\t\t\t\tkfree(alc_ptr[i]);\n\t\t\tkfree(alc_ptr);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\treturn alc_ptr;\n}\n\nstatic void free_alien_cache(struct alien_cache **alc_ptr)\n{\n\tint i;\n\n\tif (!alc_ptr)\n\t\treturn;\n\tfor_each_node(i)\n\t    kfree(alc_ptr[i]);\n\tkfree(alc_ptr);\n}\n\nstatic void __drain_alien_cache(struct kmem_cache *cachep,\n\t\t\t\tstruct array_cache *ac, int node,\n\t\t\t\tstruct list_head *list)\n{\n\tstruct kmem_cache_node *n = get_node(cachep, node);\n\n\tif (ac->avail) {\n\t\tspin_lock(&n->list_lock);\n\t\t/*\n\t\t * Stuff objects into the remote nodes shared array first.\n\t\t * That way we could avoid the overhead of putting the objects\n\t\t * into the free lists and getting them back later.\n\t\t */\n\t\tif (n->shared)\n\t\t\ttransfer_objects(n->shared, ac, ac->limit);\n\n\t\tfree_block(cachep, ac->entry, ac->avail, node, list);\n\t\tac->avail = 0;\n\t\tspin_unlock(&n->list_lock);\n\t}\n}\n\n/*\n * Called from cache_reap() to regularly drain alien caches round robin.\n */\nstatic void reap_alien(struct kmem_cache *cachep, struct kmem_cache_node *n)\n{\n\tint node = __this_cpu_read(slab_reap_node);\n\n\tif (n->alien) {\n\t\tstruct alien_cache *alc = n->alien[node];\n\t\tstruct array_cache *ac;\n\n\t\tif (alc) {\n\t\t\tac = &alc->ac;\n\t\t\tif (ac->avail && spin_trylock_irq(&alc->lock)) {\n\t\t\t\tLIST_HEAD(list);\n\n\t\t\t\t__drain_alien_cache(cachep, ac, node, &list);\n\t\t\t\tspin_unlock_irq(&alc->lock);\n\t\t\t\tslabs_destroy(cachep, &list);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void drain_alien_cache(struct kmem_cache *cachep,\n\t\t\t\tstruct alien_cache **alien)\n{\n\tint i = 0;\n\tstruct alien_cache *alc;\n\tstruct array_cache *ac;\n\tunsigned long flags;\n\n\tfor_each_online_node(i) {\n\t\talc = alien[i];\n\t\tif (alc) {\n\t\t\tLIST_HEAD(list);\n\n\t\t\tac = &alc->ac;\n\t\t\tspin_lock_irqsave(&alc->lock, flags);\n\t\t\t__drain_alien_cache(cachep, ac, i, &list);\n\t\t\tspin_unlock_irqrestore(&alc->lock, flags);\n\t\t\tslabs_destroy(cachep, &list);\n\t\t}\n\t}\n}\n\nstatic int __cache_free_alien(struct kmem_cache *cachep, void *objp,\n\t\t\t\tint node, int page_node)\n{\n\tstruct kmem_cache_node *n;\n\tstruct alien_cache *alien = NULL;\n\tstruct array_cache *ac;\n\tLIST_HEAD(list);\n\n\tn = get_node(cachep, node);\n\tSTATS_INC_NODEFREES(cachep);\n\tif (n->alien && n->alien[page_node]) {\n\t\talien = n->alien[page_node];\n\t\tac = &alien->ac;\n\t\tspin_lock(&alien->lock);\n\t\tif (unlikely(ac->avail == ac->limit)) {\n\t\t\tSTATS_INC_ACOVERFLOW(cachep);\n\t\t\t__drain_alien_cache(cachep, ac, page_node, &list);\n\t\t}\n\t\tac->entry[ac->avail++] = objp;\n\t\tspin_unlock(&alien->lock);\n\t\tslabs_destroy(cachep, &list);\n\t} else {\n\t\tn = get_node(cachep, page_node);\n\t\tspin_lock(&n->list_lock);\n\t\tfree_block(cachep, &objp, 1, page_node, &list);\n\t\tspin_unlock(&n->list_lock);\n\t\tslabs_destroy(cachep, &list);\n\t}\n\treturn 1;\n}\n\nstatic inline int cache_free_alien(struct kmem_cache *cachep, void *objp)\n{\n\tint page_node = page_to_nid(virt_to_page(objp));\n\tint node = numa_mem_id();\n\t/*\n\t * Make sure we are not freeing a object from another node to the array\n\t * cache on this cpu.\n\t */\n\tif (likely(node == page_node))\n\t\treturn 0;\n\n\treturn __cache_free_alien(cachep, objp, node, page_node);\n}\n\n/*\n * Construct gfp mask to allocate from a specific node but do not reclaim or\n * warn about failures.\n */\nstatic inline gfp_t gfp_exact_node(gfp_t flags)\n{\n\treturn (flags | __GFP_THISNODE | __GFP_NOWARN) & ~(__GFP_RECLAIM|__GFP_NOFAIL);\n}\n#endif\n\nstatic int init_cache_node(struct kmem_cache *cachep, int node, gfp_t gfp)\n{\n\tstruct kmem_cache_node *n;\n\n\t/*\n\t * Set up the kmem_cache_node for cpu before we can\n\t * begin anything. Make sure some other cpu on this\n\t * node has not already allocated this\n\t */\n\tn = get_node(cachep, node);\n\tif (n) {\n\t\tspin_lock_irq(&n->list_lock);\n\t\tn->free_limit = (1 + nr_cpus_node(node)) * cachep->batchcount +\n\t\t\t\tcachep->num;\n\t\tspin_unlock_irq(&n->list_lock);\n\n\t\treturn 0;\n\t}\n\n\tn = kmalloc_node(sizeof(struct kmem_cache_node), gfp, node);\n\tif (!n)\n\t\treturn -ENOMEM;\n\n\tkmem_cache_node_init(n);\n\tn->next_reap = jiffies + REAPTIMEOUT_NODE +\n\t\t    ((unsigned long)cachep) % REAPTIMEOUT_NODE;\n\n\tn->free_limit =\n\t\t(1 + nr_cpus_node(node)) * cachep->batchcount + cachep->num;\n\n\t/*\n\t * The kmem_cache_nodes don't come and go as CPUs\n\t * come and go.  slab_mutex is sufficient\n\t * protection here.\n\t */\n\tcachep->node[node] = n;\n\n\treturn 0;\n}\n\n#if (defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)) || defined(CONFIG_SMP)\n/*\n * Allocates and initializes node for a node on each slab cache, used for\n * either memory or cpu hotplug.  If memory is being hot-added, the kmem_cache_node\n * will be allocated off-node since memory is not yet online for the new node.\n * When hotplugging memory or a cpu, existing node are not replaced if\n * already in use.\n *\n * Must hold slab_mutex.\n */\nstatic int init_cache_node_node(int node)\n{\n\tint ret;\n\tstruct kmem_cache *cachep;\n\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tret = init_cache_node(cachep, node, GFP_KERNEL);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic int setup_kmem_cache_node(struct kmem_cache *cachep,\n\t\t\t\tint node, gfp_t gfp, bool force_change)\n{\n\tint ret = -ENOMEM;\n\tstruct kmem_cache_node *n;\n\tstruct array_cache *old_shared = NULL;\n\tstruct array_cache *new_shared = NULL;\n\tstruct alien_cache **new_alien = NULL;\n\tLIST_HEAD(list);\n\n\tif (use_alien_caches) {\n\t\tnew_alien = alloc_alien_cache(node, cachep->limit, gfp);\n\t\tif (!new_alien)\n\t\t\tgoto fail;\n\t}\n\n\tif (cachep->shared) {\n\t\tnew_shared = alloc_arraycache(node,\n\t\t\tcachep->shared * cachep->batchcount, 0xbaadf00d, gfp);\n\t\tif (!new_shared)\n\t\t\tgoto fail;\n\t}\n\n\tret = init_cache_node(cachep, node, gfp);\n\tif (ret)\n\t\tgoto fail;\n\n\tn = get_node(cachep, node);\n\tspin_lock_irq(&n->list_lock);\n\tif (n->shared && force_change) {\n\t\tfree_block(cachep, n->shared->entry,\n\t\t\t\tn->shared->avail, node, &list);\n\t\tn->shared->avail = 0;\n\t}\n\n\tif (!n->shared || force_change) {\n\t\told_shared = n->shared;\n\t\tn->shared = new_shared;\n\t\tnew_shared = NULL;\n\t}\n\n\tif (!n->alien) {\n\t\tn->alien = new_alien;\n\t\tnew_alien = NULL;\n\t}\n\n\tspin_unlock_irq(&n->list_lock);\n\tslabs_destroy(cachep, &list);\n\n\t/*\n\t * To protect lockless access to n->shared during irq disabled context.\n\t * If n->shared isn't NULL in irq disabled context, accessing to it is\n\t * guaranteed to be valid until irq is re-enabled, because it will be\n\t * freed after synchronize_sched().\n\t */\n\tif (old_shared && force_change)\n\t\tsynchronize_sched();\n\nfail:\n\tkfree(old_shared);\n\tkfree(new_shared);\n\tfree_alien_cache(new_alien);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_SMP\n\nstatic void cpuup_canceled(long cpu)\n{\n\tstruct kmem_cache *cachep;\n\tstruct kmem_cache_node *n = NULL;\n\tint node = cpu_to_mem(cpu);\n\tconst struct cpumask *mask = cpumask_of_node(node);\n\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tstruct array_cache *nc;\n\t\tstruct array_cache *shared;\n\t\tstruct alien_cache **alien;\n\t\tLIST_HEAD(list);\n\n\t\tn = get_node(cachep, node);\n\t\tif (!n)\n\t\t\tcontinue;\n\n\t\tspin_lock_irq(&n->list_lock);\n\n\t\t/* Free limit for this kmem_cache_node */\n\t\tn->free_limit -= cachep->batchcount;\n\n\t\t/* cpu is dead; no one can alloc from it. */\n\t\tnc = per_cpu_ptr(cachep->cpu_cache, cpu);\n\t\tif (nc) {\n\t\t\tfree_block(cachep, nc->entry, nc->avail, node, &list);\n\t\t\tnc->avail = 0;\n\t\t}\n\n\t\tif (!cpumask_empty(mask)) {\n\t\t\tspin_unlock_irq(&n->list_lock);\n\t\t\tgoto free_slab;\n\t\t}\n\n\t\tshared = n->shared;\n\t\tif (shared) {\n\t\t\tfree_block(cachep, shared->entry,\n\t\t\t\t   shared->avail, node, &list);\n\t\t\tn->shared = NULL;\n\t\t}\n\n\t\talien = n->alien;\n\t\tn->alien = NULL;\n\n\t\tspin_unlock_irq(&n->list_lock);\n\n\t\tkfree(shared);\n\t\tif (alien) {\n\t\t\tdrain_alien_cache(cachep, alien);\n\t\t\tfree_alien_cache(alien);\n\t\t}\n\nfree_slab:\n\t\tslabs_destroy(cachep, &list);\n\t}\n\t/*\n\t * In the previous loop, all the objects were freed to\n\t * the respective cache's slabs,  now we can go ahead and\n\t * shrink each nodelist to its limit.\n\t */\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tn = get_node(cachep, node);\n\t\tif (!n)\n\t\t\tcontinue;\n\t\tdrain_freelist(cachep, n, INT_MAX);\n\t}\n}\n\nstatic int cpuup_prepare(long cpu)\n{\n\tstruct kmem_cache *cachep;\n\tint node = cpu_to_mem(cpu);\n\tint err;\n\n\t/*\n\t * We need to do this right in the beginning since\n\t * alloc_arraycache's are going to use this list.\n\t * kmalloc_node allows us to add the slab to the right\n\t * kmem_cache_node and not this cpu's kmem_cache_node\n\t */\n\terr = init_cache_node_node(node);\n\tif (err < 0)\n\t\tgoto bad;\n\n\t/*\n\t * Now we can go ahead with allocating the shared arrays and\n\t * array caches\n\t */\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\terr = setup_kmem_cache_node(cachep, node, GFP_KERNEL, false);\n\t\tif (err)\n\t\t\tgoto bad;\n\t}\n\n\treturn 0;\nbad:\n\tcpuup_canceled(cpu);\n\treturn -ENOMEM;\n}\n\nint slab_prepare_cpu(unsigned int cpu)\n{\n\tint err;\n\n\tmutex_lock(&slab_mutex);\n\terr = cpuup_prepare(cpu);\n\tmutex_unlock(&slab_mutex);\n\treturn err;\n}\n\n/*\n * This is called for a failed online attempt and for a successful\n * offline.\n *\n * Even if all the cpus of a node are down, we don't free the\n * kmem_list3 of any cache. This to avoid a race between cpu_down, and\n * a kmalloc allocation from another cpu for memory from the node of\n * the cpu going down.  The list3 structure is usually allocated from\n * kmem_cache_create() and gets destroyed at kmem_cache_destroy().\n */\nint slab_dead_cpu(unsigned int cpu)\n{\n\tmutex_lock(&slab_mutex);\n\tcpuup_canceled(cpu);\n\tmutex_unlock(&slab_mutex);\n\treturn 0;\n}\n#endif\n\nstatic int slab_online_cpu(unsigned int cpu)\n{\n\tstart_cpu_timer(cpu);\n\treturn 0;\n}\n\nstatic int slab_offline_cpu(unsigned int cpu)\n{\n\t/*\n\t * Shutdown cache reaper. Note that the slab_mutex is held so\n\t * that if cache_reap() is invoked it cannot do anything\n\t * expensive but will only modify reap_work and reschedule the\n\t * timer.\n\t */\n\tcancel_delayed_work_sync(&per_cpu(slab_reap_work, cpu));\n\t/* Now the cache_reaper is guaranteed to be not running. */\n\tper_cpu(slab_reap_work, cpu).work.func = NULL;\n\treturn 0;\n}\n\n#if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\n/*\n * Drains freelist for a node on each slab cache, used for memory hot-remove.\n * Returns -EBUSY if all objects cannot be drained so that the node is not\n * removed.\n *\n * Must hold slab_mutex.\n */\nstatic int __meminit drain_cache_node_node(int node)\n{\n\tstruct kmem_cache *cachep;\n\tint ret = 0;\n\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tstruct kmem_cache_node *n;\n\n\t\tn = get_node(cachep, node);\n\t\tif (!n)\n\t\t\tcontinue;\n\n\t\tdrain_freelist(cachep, n, INT_MAX);\n\n\t\tif (!list_empty(&n->slabs_full) ||\n\t\t    !list_empty(&n->slabs_partial)) {\n\t\t\tret = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int __meminit slab_memory_callback(struct notifier_block *self,\n\t\t\t\t\tunsigned long action, void *arg)\n{\n\tstruct memory_notify *mnb = arg;\n\tint ret = 0;\n\tint nid;\n\n\tnid = mnb->status_change_nid;\n\tif (nid < 0)\n\t\tgoto out;\n\n\tswitch (action) {\n\tcase MEM_GOING_ONLINE:\n\t\tmutex_lock(&slab_mutex);\n\t\tret = init_cache_node_node(nid);\n\t\tmutex_unlock(&slab_mutex);\n\t\tbreak;\n\tcase MEM_GOING_OFFLINE:\n\t\tmutex_lock(&slab_mutex);\n\t\tret = drain_cache_node_node(nid);\n\t\tmutex_unlock(&slab_mutex);\n\t\tbreak;\n\tcase MEM_ONLINE:\n\tcase MEM_OFFLINE:\n\tcase MEM_CANCEL_ONLINE:\n\tcase MEM_CANCEL_OFFLINE:\n\t\tbreak;\n\t}\nout:\n\treturn notifier_from_errno(ret);\n}\n#endif /* CONFIG_NUMA && CONFIG_MEMORY_HOTPLUG */\n\n/*\n * swap the static kmem_cache_node with kmalloced memory\n */\nstatic void __init init_list(struct kmem_cache *cachep, struct kmem_cache_node *list,\n\t\t\t\tint nodeid)\n{\n\tstruct kmem_cache_node *ptr;\n\n\tptr = kmalloc_node(sizeof(struct kmem_cache_node), GFP_NOWAIT, nodeid);\n\tBUG_ON(!ptr);\n\n\tmemcpy(ptr, list, sizeof(struct kmem_cache_node));\n\t/*\n\t * Do not assume that spinlocks can be initialized via memcpy:\n\t */\n\tspin_lock_init(&ptr->list_lock);\n\n\tMAKE_ALL_LISTS(cachep, ptr, nodeid);\n\tcachep->node[nodeid] = ptr;\n}\n\n/*\n * For setting up all the kmem_cache_node for cache whose buffer_size is same as\n * size of kmem_cache_node.\n */\nstatic void __init set_up_node(struct kmem_cache *cachep, int index)\n{\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tcachep->node[node] = &init_kmem_cache_node[index + node];\n\t\tcachep->node[node]->next_reap = jiffies +\n\t\t    REAPTIMEOUT_NODE +\n\t\t    ((unsigned long)cachep) % REAPTIMEOUT_NODE;\n\t}\n}\n\n/*\n * Initialisation.  Called after the page allocator have been initialised and\n * before smp_init().\n */\nvoid __init kmem_cache_init(void)\n{\n\tint i;\n\n\tBUILD_BUG_ON(sizeof(((struct page *)NULL)->lru) <\n\t\t\t\t\tsizeof(struct rcu_head));\n\tkmem_cache = &kmem_cache_boot;\n\n\tif (!IS_ENABLED(CONFIG_NUMA) || num_possible_nodes() == 1)\n\t\tuse_alien_caches = 0;\n\n\tfor (i = 0; i < NUM_INIT_LISTS; i++)\n\t\tkmem_cache_node_init(&init_kmem_cache_node[i]);\n\n\t/*\n\t * Fragmentation resistance on low memory - only use bigger\n\t * page orders on machines with more than 32MB of memory if\n\t * not overridden on the command line.\n\t */\n\tif (!slab_max_order_set && totalram_pages > (32 << 20) >> PAGE_SHIFT)\n\t\tslab_max_order = SLAB_MAX_ORDER_HI;\n\n\t/* Bootstrap is tricky, because several objects are allocated\n\t * from caches that do not exist yet:\n\t * 1) initialize the kmem_cache cache: it contains the struct\n\t *    kmem_cache structures of all caches, except kmem_cache itself:\n\t *    kmem_cache is statically allocated.\n\t *    Initially an __init data area is used for the head array and the\n\t *    kmem_cache_node structures, it's replaced with a kmalloc allocated\n\t *    array at the end of the bootstrap.\n\t * 2) Create the first kmalloc cache.\n\t *    The struct kmem_cache for the new cache is allocated normally.\n\t *    An __init data area is used for the head array.\n\t * 3) Create the remaining kmalloc caches, with minimally sized\n\t *    head arrays.\n\t * 4) Replace the __init data head arrays for kmem_cache and the first\n\t *    kmalloc cache with kmalloc allocated arrays.\n\t * 5) Replace the __init data for kmem_cache_node for kmem_cache and\n\t *    the other cache's with kmalloc allocated memory.\n\t * 6) Resize the head arrays of the kmalloc caches to their final sizes.\n\t */\n\n\t/* 1) create the kmem_cache */\n\n\t/*\n\t * struct kmem_cache size depends on nr_node_ids & nr_cpu_ids\n\t */\n\tcreate_boot_cache(kmem_cache, \"kmem_cache\",\n\t\toffsetof(struct kmem_cache, node) +\n\t\t\t\t  nr_node_ids * sizeof(struct kmem_cache_node *),\n\t\t\t\t  SLAB_HWCACHE_ALIGN);\n\tlist_add(&kmem_cache->list, &slab_caches);\n\tslab_state = PARTIAL;\n\n\t/*\n\t * Initialize the caches that provide memory for the  kmem_cache_node\n\t * structures first.  Without this, further allocations will bug.\n\t */\n\tkmalloc_caches[INDEX_NODE] = create_kmalloc_cache(\"kmalloc-node\",\n\t\t\t\tkmalloc_size(INDEX_NODE), ARCH_KMALLOC_FLAGS);\n\tslab_state = PARTIAL_NODE;\n\tsetup_kmalloc_cache_index_table();\n\n\tslab_early_init = 0;\n\n\t/* 5) Replace the bootstrap kmem_cache_node */\n\t{\n\t\tint nid;\n\n\t\tfor_each_online_node(nid) {\n\t\t\tinit_list(kmem_cache, &init_kmem_cache_node[CACHE_CACHE + nid], nid);\n\n\t\t\tinit_list(kmalloc_caches[INDEX_NODE],\n\t\t\t\t\t  &init_kmem_cache_node[SIZE_NODE + nid], nid);\n\t\t}\n\t}\n\n\tcreate_kmalloc_caches(ARCH_KMALLOC_FLAGS);\n}\n\nvoid __init kmem_cache_init_late(void)\n{\n\tstruct kmem_cache *cachep;\n\n\tslab_state = UP;\n\n\t/* 6) resize the head arrays to their final sizes */\n\tmutex_lock(&slab_mutex);\n\tlist_for_each_entry(cachep, &slab_caches, list)\n\t\tif (enable_cpucache(cachep, GFP_NOWAIT))\n\t\t\tBUG();\n\tmutex_unlock(&slab_mutex);\n\n\t/* Done! */\n\tslab_state = FULL;\n\n#ifdef CONFIG_NUMA\n\t/*\n\t * Register a memory hotplug callback that initializes and frees\n\t * node.\n\t */\n\thotplug_memory_notifier(slab_memory_callback, SLAB_CALLBACK_PRI);\n#endif\n\n\t/*\n\t * The reap timers are started later, with a module init call: That part\n\t * of the kernel is not yet operational.\n\t */\n}\n\nstatic int __init cpucache_init(void)\n{\n\tint ret;\n\n\t/*\n\t * Register the timers that return unneeded pages to the page allocator\n\t */\n\tret = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, \"SLAB online\",\n\t\t\t\tslab_online_cpu, slab_offline_cpu);\n\tWARN_ON(ret < 0);\n\n\t/* Done! */\n\tslab_state = FULL;\n\treturn 0;\n}\n__initcall(cpucache_init);\n\nstatic noinline void\nslab_out_of_memory(struct kmem_cache *cachep, gfp_t gfpflags, int nodeid)\n{\n#if DEBUG\n\tstruct kmem_cache_node *n;\n\tunsigned long flags;\n\tint node;\n\tstatic DEFINE_RATELIMIT_STATE(slab_oom_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\n\tif ((gfpflags & __GFP_NOWARN) || !__ratelimit(&slab_oom_rs))\n\t\treturn;\n\n\tpr_warn(\"SLAB: Unable to allocate memory on node %d, gfp=%#x(%pGg)\\n\",\n\t\tnodeid, gfpflags, &gfpflags);\n\tpr_warn(\"  cache: %s, object size: %d, order: %d\\n\",\n\t\tcachep->name, cachep->size, cachep->gfporder);\n\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tunsigned long total_slabs, free_slabs, free_objs;\n\n\t\tspin_lock_irqsave(&n->list_lock, flags);\n\t\ttotal_slabs = n->total_slabs;\n\t\tfree_slabs = n->free_slabs;\n\t\tfree_objs = n->free_objects;\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\n\t\tpr_warn(\"  node %d: slabs: %ld/%ld, objs: %ld/%ld\\n\",\n\t\t\tnode, total_slabs - free_slabs, total_slabs,\n\t\t\t(total_slabs * cachep->num) - free_objs,\n\t\t\ttotal_slabs * cachep->num);\n\t}\n#endif\n}\n\n/*\n * Interface to system's page allocator. No need to hold the\n * kmem_cache_node ->list_lock.\n *\n * If we requested dmaable memory, we will get it. Even if we\n * did not request dmaable memory, we might get it, but that\n * would be relatively rare and ignorable.\n */\nstatic struct page *kmem_getpages(struct kmem_cache *cachep, gfp_t flags,\n\t\t\t\t\t\t\t\tint nodeid)\n{\n\tstruct page *page;\n\tint nr_pages;\n\n\tflags |= cachep->allocflags;\n\tif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\n\t\tflags |= __GFP_RECLAIMABLE;\n\n\tpage = __alloc_pages_node(nodeid, flags | __GFP_NOTRACK, cachep->gfporder);\n\tif (!page) {\n\t\tslab_out_of_memory(cachep, flags, nodeid);\n\t\treturn NULL;\n\t}\n\n\tif (memcg_charge_slab(page, flags, cachep->gfporder, cachep)) {\n\t\t__free_pages(page, cachep->gfporder);\n\t\treturn NULL;\n\t}\n\n\tnr_pages = (1 << cachep->gfporder);\n\tif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\n\t\tadd_zone_page_state(page_zone(page),\n\t\t\tNR_SLAB_RECLAIMABLE, nr_pages);\n\telse\n\t\tadd_zone_page_state(page_zone(page),\n\t\t\tNR_SLAB_UNRECLAIMABLE, nr_pages);\n\n\t__SetPageSlab(page);\n\t/* Record if ALLOC_NO_WATERMARKS was set when allocating the slab */\n\tif (sk_memalloc_socks() && page_is_pfmemalloc(page))\n\t\tSetPageSlabPfmemalloc(page);\n\n\tif (kmemcheck_enabled && !(cachep->flags & SLAB_NOTRACK)) {\n\t\tkmemcheck_alloc_shadow(page, cachep->gfporder, flags, nodeid);\n\n\t\tif (cachep->ctor)\n\t\t\tkmemcheck_mark_uninitialized_pages(page, nr_pages);\n\t\telse\n\t\t\tkmemcheck_mark_unallocated_pages(page, nr_pages);\n\t}\n\n\treturn page;\n}\n\n/*\n * Interface to system's page release.\n */\nstatic void kmem_freepages(struct kmem_cache *cachep, struct page *page)\n{\n\tint order = cachep->gfporder;\n\tunsigned long nr_freed = (1 << order);\n\n\tkmemcheck_free_shadow(page, order);\n\n\tif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\n\t\tsub_zone_page_state(page_zone(page),\n\t\t\t\tNR_SLAB_RECLAIMABLE, nr_freed);\n\telse\n\t\tsub_zone_page_state(page_zone(page),\n\t\t\t\tNR_SLAB_UNRECLAIMABLE, nr_freed);\n\n\tBUG_ON(!PageSlab(page));\n\t__ClearPageSlabPfmemalloc(page);\n\t__ClearPageSlab(page);\n\tpage_mapcount_reset(page);\n\tpage->mapping = NULL;\n\n\tif (current->reclaim_state)\n\t\tcurrent->reclaim_state->reclaimed_slab += nr_freed;\n\tmemcg_uncharge_slab(page, order, cachep);\n\t__free_pages(page, order);\n}\n\nstatic void kmem_rcu_free(struct rcu_head *head)\n{\n\tstruct kmem_cache *cachep;\n\tstruct page *page;\n\n\tpage = container_of(head, struct page, rcu_head);\n\tcachep = page->slab_cache;\n\n\tkmem_freepages(cachep, page);\n}\n\n#if DEBUG\nstatic bool is_debug_pagealloc_cache(struct kmem_cache *cachep)\n{\n\tif (debug_pagealloc_enabled() && OFF_SLAB(cachep) &&\n\t\t(cachep->size % PAGE_SIZE) == 0)\n\t\treturn true;\n\n\treturn false;\n}\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nstatic void store_stackinfo(struct kmem_cache *cachep, unsigned long *addr,\n\t\t\t    unsigned long caller)\n{\n\tint size = cachep->object_size;\n\n\taddr = (unsigned long *)&((char *)addr)[obj_offset(cachep)];\n\n\tif (size < 5 * sizeof(unsigned long))\n\t\treturn;\n\n\t*addr++ = 0x12345678;\n\t*addr++ = caller;\n\t*addr++ = smp_processor_id();\n\tsize -= 3 * sizeof(unsigned long);\n\t{\n\t\tunsigned long *sptr = &caller;\n\t\tunsigned long svalue;\n\n\t\twhile (!kstack_end(sptr)) {\n\t\t\tsvalue = *sptr++;\n\t\t\tif (kernel_text_address(svalue)) {\n\t\t\t\t*addr++ = svalue;\n\t\t\t\tsize -= sizeof(unsigned long);\n\t\t\t\tif (size <= sizeof(unsigned long))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t}\n\t*addr++ = 0x87654321;\n}\n\nstatic void slab_kernel_map(struct kmem_cache *cachep, void *objp,\n\t\t\t\tint map, unsigned long caller)\n{\n\tif (!is_debug_pagealloc_cache(cachep))\n\t\treturn;\n\n\tif (caller)\n\t\tstore_stackinfo(cachep, objp, caller);\n\n\tkernel_map_pages(virt_to_page(objp), cachep->size / PAGE_SIZE, map);\n}\n\n#else\nstatic inline void slab_kernel_map(struct kmem_cache *cachep, void *objp,\n\t\t\t\tint map, unsigned long caller) {}\n\n#endif\n\nstatic void poison_obj(struct kmem_cache *cachep, void *addr, unsigned char val)\n{\n\tint size = cachep->object_size;\n\taddr = &((char *)addr)[obj_offset(cachep)];\n\n\tmemset(addr, val, size);\n\t*(unsigned char *)(addr + size - 1) = POISON_END;\n}\n\nstatic void dump_line(char *data, int offset, int limit)\n{\n\tint i;\n\tunsigned char error = 0;\n\tint bad_count = 0;\n\n\tpr_err(\"%03x: \", offset);\n\tfor (i = 0; i < limit; i++) {\n\t\tif (data[offset + i] != POISON_FREE) {\n\t\t\terror = data[offset + i];\n\t\t\tbad_count++;\n\t\t}\n\t}\n\tprint_hex_dump(KERN_CONT, \"\", 0, 16, 1,\n\t\t\t&data[offset], limit, 1);\n\n\tif (bad_count == 1) {\n\t\terror ^= POISON_FREE;\n\t\tif (!(error & (error - 1))) {\n\t\t\tpr_err(\"Single bit error detected. Probably bad RAM.\\n\");\n#ifdef CONFIG_X86\n\t\t\tpr_err(\"Run memtest86+ or a similar memory test tool.\\n\");\n#else\n\t\t\tpr_err(\"Run a memory test tool.\\n\");\n#endif\n\t\t}\n\t}\n}\n#endif\n\n#if DEBUG\n\nstatic void print_objinfo(struct kmem_cache *cachep, void *objp, int lines)\n{\n\tint i, size;\n\tchar *realobj;\n\n\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\tpr_err(\"Redzone: 0x%llx/0x%llx\\n\",\n\t\t       *dbg_redzone1(cachep, objp),\n\t\t       *dbg_redzone2(cachep, objp));\n\t}\n\n\tif (cachep->flags & SLAB_STORE_USER) {\n\t\tpr_err(\"Last user: [<%p>](%pSR)\\n\",\n\t\t       *dbg_userword(cachep, objp),\n\t\t       *dbg_userword(cachep, objp));\n\t}\n\trealobj = (char *)objp + obj_offset(cachep);\n\tsize = cachep->object_size;\n\tfor (i = 0; i < size && lines; i += 16, lines--) {\n\t\tint limit;\n\t\tlimit = 16;\n\t\tif (i + limit > size)\n\t\t\tlimit = size - i;\n\t\tdump_line(realobj, i, limit);\n\t}\n}\n\nstatic void check_poison_obj(struct kmem_cache *cachep, void *objp)\n{\n\tchar *realobj;\n\tint size, i;\n\tint lines = 0;\n\n\tif (is_debug_pagealloc_cache(cachep))\n\t\treturn;\n\n\trealobj = (char *)objp + obj_offset(cachep);\n\tsize = cachep->object_size;\n\n\tfor (i = 0; i < size; i++) {\n\t\tchar exp = POISON_FREE;\n\t\tif (i == size - 1)\n\t\t\texp = POISON_END;\n\t\tif (realobj[i] != exp) {\n\t\t\tint limit;\n\t\t\t/* Mismatch ! */\n\t\t\t/* Print header */\n\t\t\tif (lines == 0) {\n\t\t\t\tpr_err(\"Slab corruption (%s): %s start=%p, len=%d\\n\",\n\t\t\t\t       print_tainted(), cachep->name,\n\t\t\t\t       realobj, size);\n\t\t\t\tprint_objinfo(cachep, objp, 0);\n\t\t\t}\n\t\t\t/* Hexdump the affected line */\n\t\t\ti = (i / 16) * 16;\n\t\t\tlimit = 16;\n\t\t\tif (i + limit > size)\n\t\t\t\tlimit = size - i;\n\t\t\tdump_line(realobj, i, limit);\n\t\t\ti += 16;\n\t\t\tlines++;\n\t\t\t/* Limit to 5 lines */\n\t\t\tif (lines > 5)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (lines != 0) {\n\t\t/* Print some data about the neighboring objects, if they\n\t\t * exist:\n\t\t */\n\t\tstruct page *page = virt_to_head_page(objp);\n\t\tunsigned int objnr;\n\n\t\tobjnr = obj_to_index(cachep, page, objp);\n\t\tif (objnr) {\n\t\t\tobjp = index_to_obj(cachep, page, objnr - 1);\n\t\t\trealobj = (char *)objp + obj_offset(cachep);\n\t\t\tpr_err(\"Prev obj: start=%p, len=%d\\n\", realobj, size);\n\t\t\tprint_objinfo(cachep, objp, 2);\n\t\t}\n\t\tif (objnr + 1 < cachep->num) {\n\t\t\tobjp = index_to_obj(cachep, page, objnr + 1);\n\t\t\trealobj = (char *)objp + obj_offset(cachep);\n\t\t\tpr_err(\"Next obj: start=%p, len=%d\\n\", realobj, size);\n\t\t\tprint_objinfo(cachep, objp, 2);\n\t\t}\n\t}\n}\n#endif\n\n#if DEBUG\nstatic void slab_destroy_debugcheck(struct kmem_cache *cachep,\n\t\t\t\t\t\tstruct page *page)\n{\n\tint i;\n\n\tif (OBJFREELIST_SLAB(cachep) && cachep->flags & SLAB_POISON) {\n\t\tpoison_obj(cachep, page->freelist - obj_offset(cachep),\n\t\t\tPOISON_FREE);\n\t}\n\n\tfor (i = 0; i < cachep->num; i++) {\n\t\tvoid *objp = index_to_obj(cachep, page, i);\n\n\t\tif (cachep->flags & SLAB_POISON) {\n\t\t\tcheck_poison_obj(cachep, objp);\n\t\t\tslab_kernel_map(cachep, objp, 1, 0);\n\t\t}\n\t\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\t\tif (*dbg_redzone1(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"start of a freed object was overwritten\");\n\t\t\tif (*dbg_redzone2(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"end of a freed object was overwritten\");\n\t\t}\n\t}\n}\n#else\nstatic void slab_destroy_debugcheck(struct kmem_cache *cachep,\n\t\t\t\t\t\tstruct page *page)\n{\n}\n#endif\n\n/**\n * slab_destroy - destroy and release all objects in a slab\n * @cachep: cache pointer being destroyed\n * @page: page pointer being destroyed\n *\n * Destroy all the objs in a slab page, and release the mem back to the system.\n * Before calling the slab page must have been unlinked from the cache. The\n * kmem_cache_node ->list_lock is not held/needed.\n */\nstatic void slab_destroy(struct kmem_cache *cachep, struct page *page)\n{\n\tvoid *freelist;\n\n\tfreelist = page->freelist;\n\tslab_destroy_debugcheck(cachep, page);\n\tif (unlikely(cachep->flags & SLAB_DESTROY_BY_RCU))\n\t\tcall_rcu(&page->rcu_head, kmem_rcu_free);\n\telse\n\t\tkmem_freepages(cachep, page);\n\n\t/*\n\t * From now on, we don't use freelist\n\t * although actual page can be freed in rcu context\n\t */\n\tif (OFF_SLAB(cachep))\n\t\tkmem_cache_free(cachep->freelist_cache, freelist);\n}\n\nstatic void slabs_destroy(struct kmem_cache *cachep, struct list_head *list)\n{\n\tstruct page *page, *n;\n\n\tlist_for_each_entry_safe(page, n, list, lru) {\n\t\tlist_del(&page->lru);\n\t\tslab_destroy(cachep, page);\n\t}\n}\n\n/**\n * calculate_slab_order - calculate size (page order) of slabs\n * @cachep: pointer to the cache that is being created\n * @size: size of objects to be created in this cache.\n * @flags: slab allocation flags\n *\n * Also calculates the number of objects per slab.\n *\n * This could be made much more intelligent.  For now, try to avoid using\n * high order pages for slabs.  When the gfp() functions are more friendly\n * towards high-order requests, this should be changed.\n */\nstatic size_t calculate_slab_order(struct kmem_cache *cachep,\n\t\t\t\tsize_t size, unsigned long flags)\n{\n\tsize_t left_over = 0;\n\tint gfporder;\n\n\tfor (gfporder = 0; gfporder <= KMALLOC_MAX_ORDER; gfporder++) {\n\t\tunsigned int num;\n\t\tsize_t remainder;\n\n\t\tnum = cache_estimate(gfporder, size, flags, &remainder);\n\t\tif (!num)\n\t\t\tcontinue;\n\n\t\t/* Can't handle number of objects more than SLAB_OBJ_MAX_NUM */\n\t\tif (num > SLAB_OBJ_MAX_NUM)\n\t\t\tbreak;\n\n\t\tif (flags & CFLGS_OFF_SLAB) {\n\t\t\tstruct kmem_cache *freelist_cache;\n\t\t\tsize_t freelist_size;\n\n\t\t\tfreelist_size = num * sizeof(freelist_idx_t);\n\t\t\tfreelist_cache = kmalloc_slab(freelist_size, 0u);\n\t\t\tif (!freelist_cache)\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * Needed to avoid possible looping condition\n\t\t\t * in cache_grow_begin()\n\t\t\t */\n\t\t\tif (OFF_SLAB(freelist_cache))\n\t\t\t\tcontinue;\n\n\t\t\t/* check if off slab has enough benefit */\n\t\t\tif (freelist_cache->size > cachep->size / 2)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t/* Found something acceptable - save it away */\n\t\tcachep->num = num;\n\t\tcachep->gfporder = gfporder;\n\t\tleft_over = remainder;\n\n\t\t/*\n\t\t * A VFS-reclaimable slab tends to have most allocations\n\t\t * as GFP_NOFS and we really don't want to have to be allocating\n\t\t * higher-order pages when we are unable to shrink dcache.\n\t\t */\n\t\tif (flags & SLAB_RECLAIM_ACCOUNT)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Large number of objects is good, but very large slabs are\n\t\t * currently bad for the gfp()s.\n\t\t */\n\t\tif (gfporder >= slab_max_order)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Acceptable internal fragmentation?\n\t\t */\n\t\tif (left_over * 8 <= (PAGE_SIZE << gfporder))\n\t\t\tbreak;\n\t}\n\treturn left_over;\n}\n\nstatic struct array_cache __percpu *alloc_kmem_cache_cpus(\n\t\tstruct kmem_cache *cachep, int entries, int batchcount)\n{\n\tint cpu;\n\tsize_t size;\n\tstruct array_cache __percpu *cpu_cache;\n\n\tsize = sizeof(void *) * entries + sizeof(struct array_cache);\n\tcpu_cache = __alloc_percpu(size, sizeof(void *));\n\n\tif (!cpu_cache)\n\t\treturn NULL;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tinit_arraycache(per_cpu_ptr(cpu_cache, cpu),\n\t\t\t\tentries, batchcount);\n\t}\n\n\treturn cpu_cache;\n}\n\nstatic int __ref setup_cpu_cache(struct kmem_cache *cachep, gfp_t gfp)\n{\n\tif (slab_state >= FULL)\n\t\treturn enable_cpucache(cachep, gfp);\n\n\tcachep->cpu_cache = alloc_kmem_cache_cpus(cachep, 1, 1);\n\tif (!cachep->cpu_cache)\n\t\treturn 1;\n\n\tif (slab_state == DOWN) {\n\t\t/* Creation of first cache (kmem_cache). */\n\t\tset_up_node(kmem_cache, CACHE_CACHE);\n\t} else if (slab_state == PARTIAL) {\n\t\t/* For kmem_cache_node */\n\t\tset_up_node(cachep, SIZE_NODE);\n\t} else {\n\t\tint node;\n\n\t\tfor_each_online_node(node) {\n\t\t\tcachep->node[node] = kmalloc_node(\n\t\t\t\tsizeof(struct kmem_cache_node), gfp, node);\n\t\t\tBUG_ON(!cachep->node[node]);\n\t\t\tkmem_cache_node_init(cachep->node[node]);\n\t\t}\n\t}\n\n\tcachep->node[numa_mem_id()]->next_reap =\n\t\t\tjiffies + REAPTIMEOUT_NODE +\n\t\t\t((unsigned long)cachep) % REAPTIMEOUT_NODE;\n\n\tcpu_cache_get(cachep)->avail = 0;\n\tcpu_cache_get(cachep)->limit = BOOT_CPUCACHE_ENTRIES;\n\tcpu_cache_get(cachep)->batchcount = 1;\n\tcpu_cache_get(cachep)->touched = 0;\n\tcachep->batchcount = 1;\n\tcachep->limit = BOOT_CPUCACHE_ENTRIES;\n\treturn 0;\n}\n\nunsigned long kmem_cache_flags(unsigned long object_size,\n\tunsigned long flags, const char *name,\n\tvoid (*ctor)(void *))\n{\n\treturn flags;\n}\n\nstruct kmem_cache *\n__kmem_cache_alias(const char *name, size_t size, size_t align,\n\t\t   unsigned long flags, void (*ctor)(void *))\n{\n\tstruct kmem_cache *cachep;\n\n\tcachep = find_mergeable(size, align, flags, name, ctor);\n\tif (cachep) {\n\t\tcachep->refcount++;\n\n\t\t/*\n\t\t * Adjust the object sizes so that we clear\n\t\t * the complete object on kzalloc.\n\t\t */\n\t\tcachep->object_size = max_t(int, cachep->object_size, size);\n\t}\n\treturn cachep;\n}\n\nstatic bool set_objfreelist_slab_cache(struct kmem_cache *cachep,\n\t\t\tsize_t size, unsigned long flags)\n{\n\tsize_t left;\n\n\tcachep->num = 0;\n\n\tif (cachep->ctor || flags & SLAB_DESTROY_BY_RCU)\n\t\treturn false;\n\n\tleft = calculate_slab_order(cachep, size,\n\t\t\tflags | CFLGS_OBJFREELIST_SLAB);\n\tif (!cachep->num)\n\t\treturn false;\n\n\tif (cachep->num * sizeof(freelist_idx_t) > cachep->object_size)\n\t\treturn false;\n\n\tcachep->colour = left / cachep->colour_off;\n\n\treturn true;\n}\n\nstatic bool set_off_slab_cache(struct kmem_cache *cachep,\n\t\t\tsize_t size, unsigned long flags)\n{\n\tsize_t left;\n\n\tcachep->num = 0;\n\n\t/*\n\t * Always use on-slab management when SLAB_NOLEAKTRACE\n\t * to avoid recursive calls into kmemleak.\n\t */\n\tif (flags & SLAB_NOLEAKTRACE)\n\t\treturn false;\n\n\t/*\n\t * Size is large, assume best to place the slab management obj\n\t * off-slab (should allow better packing of objs).\n\t */\n\tleft = calculate_slab_order(cachep, size, flags | CFLGS_OFF_SLAB);\n\tif (!cachep->num)\n\t\treturn false;\n\n\t/*\n\t * If the slab has been placed off-slab, and we have enough space then\n\t * move it on-slab. This is at the expense of any extra colouring.\n\t */\n\tif (left >= cachep->num * sizeof(freelist_idx_t))\n\t\treturn false;\n\n\tcachep->colour = left / cachep->colour_off;\n\n\treturn true;\n}\n\nstatic bool set_on_slab_cache(struct kmem_cache *cachep,\n\t\t\tsize_t size, unsigned long flags)\n{\n\tsize_t left;\n\n\tcachep->num = 0;\n\n\tleft = calculate_slab_order(cachep, size, flags);\n\tif (!cachep->num)\n\t\treturn false;\n\n\tcachep->colour = left / cachep->colour_off;\n\n\treturn true;\n}\n\n/**\n * __kmem_cache_create - Create a cache.\n * @cachep: cache management descriptor\n * @flags: SLAB flags\n *\n * Returns a ptr to the cache on success, NULL on failure.\n * Cannot be called within a int, but can be interrupted.\n * The @ctor is run when new pages are allocated by the cache.\n *\n * The flags are\n *\n * %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)\n * to catch references to uninitialised memory.\n *\n * %SLAB_RED_ZONE - Insert `Red' zones around the allocated memory to check\n * for buffer overruns.\n *\n * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware\n * cacheline.  This can be beneficial if you're counting cycles as closely\n * as davem.\n */\nint\n__kmem_cache_create (struct kmem_cache *cachep, unsigned long flags)\n{\n\tsize_t ralign = BYTES_PER_WORD;\n\tgfp_t gfp;\n\tint err;\n\tsize_t size = cachep->size;\n\n#if DEBUG\n#if FORCED_DEBUG\n\t/*\n\t * Enable redzoning and last user accounting, except for caches with\n\t * large objects, if the increased size would increase the object size\n\t * above the next power of two: caches with object sizes just above a\n\t * power of two have a significant amount of internal fragmentation.\n\t */\n\tif (size < 4096 || fls(size - 1) == fls(size-1 + REDZONE_ALIGN +\n\t\t\t\t\t\t2 * sizeof(unsigned long long)))\n\t\tflags |= SLAB_RED_ZONE | SLAB_STORE_USER;\n\tif (!(flags & SLAB_DESTROY_BY_RCU))\n\t\tflags |= SLAB_POISON;\n#endif\n#endif\n\n\t/*\n\t * Check that size is in terms of words.  This is needed to avoid\n\t * unaligned accesses for some archs when redzoning is used, and makes\n\t * sure any on-slab bufctl's are also correctly aligned.\n\t */\n\tif (size & (BYTES_PER_WORD - 1)) {\n\t\tsize += (BYTES_PER_WORD - 1);\n\t\tsize &= ~(BYTES_PER_WORD - 1);\n\t}\n\n\tif (flags & SLAB_RED_ZONE) {\n\t\tralign = REDZONE_ALIGN;\n\t\t/* If redzoning, ensure that the second redzone is suitably\n\t\t * aligned, by adjusting the object size accordingly. */\n\t\tsize += REDZONE_ALIGN - 1;\n\t\tsize &= ~(REDZONE_ALIGN - 1);\n\t}\n\n\t/* 3) caller mandated alignment */\n\tif (ralign < cachep->align) {\n\t\tralign = cachep->align;\n\t}\n\t/* disable debug if necessary */\n\tif (ralign > __alignof__(unsigned long long))\n\t\tflags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);\n\t/*\n\t * 4) Store it.\n\t */\n\tcachep->align = ralign;\n\tcachep->colour_off = cache_line_size();\n\t/* Offset must be a multiple of the alignment. */\n\tif (cachep->colour_off < cachep->align)\n\t\tcachep->colour_off = cachep->align;\n\n\tif (slab_is_available())\n\t\tgfp = GFP_KERNEL;\n\telse\n\t\tgfp = GFP_NOWAIT;\n\n#if DEBUG\n\n\t/*\n\t * Both debugging options require word-alignment which is calculated\n\t * into align above.\n\t */\n\tif (flags & SLAB_RED_ZONE) {\n\t\t/* add space for red zone words */\n\t\tcachep->obj_offset += sizeof(unsigned long long);\n\t\tsize += 2 * sizeof(unsigned long long);\n\t}\n\tif (flags & SLAB_STORE_USER) {\n\t\t/* user store requires one word storage behind the end of\n\t\t * the real object. But if the second red zone needs to be\n\t\t * aligned to 64 bits, we must allow that much space.\n\t\t */\n\t\tif (flags & SLAB_RED_ZONE)\n\t\t\tsize += REDZONE_ALIGN;\n\t\telse\n\t\t\tsize += BYTES_PER_WORD;\n\t}\n#endif\n\n\tkasan_cache_create(cachep, &size, &flags);\n\n\tsize = ALIGN(size, cachep->align);\n\t/*\n\t * We should restrict the number of objects in a slab to implement\n\t * byte sized index. Refer comment on SLAB_OBJ_MIN_SIZE definition.\n\t */\n\tif (FREELIST_BYTE_INDEX && size < SLAB_OBJ_MIN_SIZE)\n\t\tsize = ALIGN(SLAB_OBJ_MIN_SIZE, cachep->align);\n\n#if DEBUG\n\t/*\n\t * To activate debug pagealloc, off-slab management is necessary\n\t * requirement. In early phase of initialization, small sized slab\n\t * doesn't get initialized so it would not be possible. So, we need\n\t * to check size >= 256. It guarantees that all necessary small\n\t * sized slab is initialized in current slab initialization sequence.\n\t */\n\tif (debug_pagealloc_enabled() && (flags & SLAB_POISON) &&\n\t\tsize >= 256 && cachep->object_size > cache_line_size()) {\n\t\tif (size < PAGE_SIZE || size % PAGE_SIZE == 0) {\n\t\t\tsize_t tmp_size = ALIGN(size, PAGE_SIZE);\n\n\t\t\tif (set_off_slab_cache(cachep, tmp_size, flags)) {\n\t\t\t\tflags |= CFLGS_OFF_SLAB;\n\t\t\t\tcachep->obj_offset += tmp_size - size;\n\t\t\t\tsize = tmp_size;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\t}\n#endif\n\n\tif (set_objfreelist_slab_cache(cachep, size, flags)) {\n\t\tflags |= CFLGS_OBJFREELIST_SLAB;\n\t\tgoto done;\n\t}\n\n\tif (set_off_slab_cache(cachep, size, flags)) {\n\t\tflags |= CFLGS_OFF_SLAB;\n\t\tgoto done;\n\t}\n\n\tif (set_on_slab_cache(cachep, size, flags))\n\t\tgoto done;\n\n\treturn -E2BIG;\n\ndone:\n\tcachep->freelist_size = cachep->num * sizeof(freelist_idx_t);\n\tcachep->flags = flags;\n\tcachep->allocflags = __GFP_COMP;\n\tif (flags & SLAB_CACHE_DMA)\n\t\tcachep->allocflags |= GFP_DMA;\n\tcachep->size = size;\n\tcachep->reciprocal_buffer_size = reciprocal_value(size);\n\n#if DEBUG\n\t/*\n\t * If we're going to use the generic kernel_map_pages()\n\t * poisoning, then it's going to smash the contents of\n\t * the redzone and userword anyhow, so switch them off.\n\t */\n\tif (IS_ENABLED(CONFIG_PAGE_POISONING) &&\n\t\t(cachep->flags & SLAB_POISON) &&\n\t\tis_debug_pagealloc_cache(cachep))\n\t\tcachep->flags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);\n#endif\n\n\tif (OFF_SLAB(cachep)) {\n\t\tcachep->freelist_cache =\n\t\t\tkmalloc_slab(cachep->freelist_size, 0u);\n\t}\n\n\terr = setup_cpu_cache(cachep, gfp);\n\tif (err) {\n\t\t__kmem_cache_release(cachep);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n#if DEBUG\nstatic void check_irq_off(void)\n{\n\tBUG_ON(!irqs_disabled());\n}\n\nstatic void check_irq_on(void)\n{\n\tBUG_ON(irqs_disabled());\n}\n\nstatic void check_mutex_acquired(void)\n{\n\tBUG_ON(!mutex_is_locked(&slab_mutex));\n}\n\nstatic void check_spinlock_acquired(struct kmem_cache *cachep)\n{\n#ifdef CONFIG_SMP\n\tcheck_irq_off();\n\tassert_spin_locked(&get_node(cachep, numa_mem_id())->list_lock);\n#endif\n}\n\nstatic void check_spinlock_acquired_node(struct kmem_cache *cachep, int node)\n{\n#ifdef CONFIG_SMP\n\tcheck_irq_off();\n\tassert_spin_locked(&get_node(cachep, node)->list_lock);\n#endif\n}\n\n#else\n#define check_irq_off()\tdo { } while(0)\n#define check_irq_on()\tdo { } while(0)\n#define check_mutex_acquired()\tdo { } while(0)\n#define check_spinlock_acquired(x) do { } while(0)\n#define check_spinlock_acquired_node(x, y) do { } while(0)\n#endif\n\nstatic void drain_array_locked(struct kmem_cache *cachep, struct array_cache *ac,\n\t\t\t\tint node, bool free_all, struct list_head *list)\n{\n\tint tofree;\n\n\tif (!ac || !ac->avail)\n\t\treturn;\n\n\ttofree = free_all ? ac->avail : (ac->limit + 4) / 5;\n\tif (tofree > ac->avail)\n\t\ttofree = (ac->avail + 1) / 2;\n\n\tfree_block(cachep, ac->entry, tofree, node, list);\n\tac->avail -= tofree;\n\tmemmove(ac->entry, &(ac->entry[tofree]), sizeof(void *) * ac->avail);\n}\n\nstatic void do_drain(void *arg)\n{\n\tstruct kmem_cache *cachep = arg;\n\tstruct array_cache *ac;\n\tint node = numa_mem_id();\n\tstruct kmem_cache_node *n;\n\tLIST_HEAD(list);\n\n\tcheck_irq_off();\n\tac = cpu_cache_get(cachep);\n\tn = get_node(cachep, node);\n\tspin_lock(&n->list_lock);\n\tfree_block(cachep, ac->entry, ac->avail, node, &list);\n\tspin_unlock(&n->list_lock);\n\tslabs_destroy(cachep, &list);\n\tac->avail = 0;\n}\n\nstatic void drain_cpu_caches(struct kmem_cache *cachep)\n{\n\tstruct kmem_cache_node *n;\n\tint node;\n\tLIST_HEAD(list);\n\n\ton_each_cpu(do_drain, cachep, 1);\n\tcheck_irq_on();\n\tfor_each_kmem_cache_node(cachep, node, n)\n\t\tif (n->alien)\n\t\t\tdrain_alien_cache(cachep, n->alien);\n\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tspin_lock_irq(&n->list_lock);\n\t\tdrain_array_locked(cachep, n->shared, node, true, &list);\n\t\tspin_unlock_irq(&n->list_lock);\n\n\t\tslabs_destroy(cachep, &list);\n\t}\n}\n\n/*\n * Remove slabs from the list of free slabs.\n * Specify the number of slabs to drain in tofree.\n *\n * Returns the actual number of slabs released.\n */\nstatic int drain_freelist(struct kmem_cache *cache,\n\t\t\tstruct kmem_cache_node *n, int tofree)\n{\n\tstruct list_head *p;\n\tint nr_freed;\n\tstruct page *page;\n\n\tnr_freed = 0;\n\twhile (nr_freed < tofree && !list_empty(&n->slabs_free)) {\n\n\t\tspin_lock_irq(&n->list_lock);\n\t\tp = n->slabs_free.prev;\n\t\tif (p == &n->slabs_free) {\n\t\t\tspin_unlock_irq(&n->list_lock);\n\t\t\tgoto out;\n\t\t}\n\n\t\tpage = list_entry(p, struct page, lru);\n\t\tlist_del(&page->lru);\n\t\tn->free_slabs--;\n\t\tn->total_slabs--;\n\t\t/*\n\t\t * Safe to drop the lock. The slab is no longer linked\n\t\t * to the cache.\n\t\t */\n\t\tn->free_objects -= cache->num;\n\t\tspin_unlock_irq(&n->list_lock);\n\t\tslab_destroy(cache, page);\n\t\tnr_freed++;\n\t}\nout:\n\treturn nr_freed;\n}\n\nint __kmem_cache_shrink(struct kmem_cache *cachep)\n{\n\tint ret = 0;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tdrain_cpu_caches(cachep);\n\n\tcheck_irq_on();\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tdrain_freelist(cachep, n, INT_MAX);\n\n\t\tret += !list_empty(&n->slabs_full) ||\n\t\t\t!list_empty(&n->slabs_partial);\n\t}\n\treturn (ret ? 1 : 0);\n}\n\nint __kmem_cache_shutdown(struct kmem_cache *cachep)\n{\n\treturn __kmem_cache_shrink(cachep);\n}\n\nvoid __kmem_cache_release(struct kmem_cache *cachep)\n{\n\tint i;\n\tstruct kmem_cache_node *n;\n\n\tcache_random_seq_destroy(cachep);\n\n\tfree_percpu(cachep->cpu_cache);\n\n\t/* NUMA: free the node structures */\n\tfor_each_kmem_cache_node(cachep, i, n) {\n\t\tkfree(n->shared);\n\t\tfree_alien_cache(n->alien);\n\t\tkfree(n);\n\t\tcachep->node[i] = NULL;\n\t}\n}\n\n/*\n * Get the memory for a slab management obj.\n *\n * For a slab cache when the slab descriptor is off-slab, the\n * slab descriptor can't come from the same cache which is being created,\n * Because if it is the case, that means we defer the creation of\n * the kmalloc_{dma,}_cache of size sizeof(slab descriptor) to this point.\n * And we eventually call down to __kmem_cache_create(), which\n * in turn looks up in the kmalloc_{dma,}_caches for the disired-size one.\n * This is a \"chicken-and-egg\" problem.\n *\n * So the off-slab slab descriptor shall come from the kmalloc_{dma,}_caches,\n * which are all initialized during kmem_cache_init().\n */\nstatic void *alloc_slabmgmt(struct kmem_cache *cachep,\n\t\t\t\t   struct page *page, int colour_off,\n\t\t\t\t   gfp_t local_flags, int nodeid)\n{\n\tvoid *freelist;\n\tvoid *addr = page_address(page);\n\n\tpage->s_mem = addr + colour_off;\n\tpage->active = 0;\n\n\tif (OBJFREELIST_SLAB(cachep))\n\t\tfreelist = NULL;\n\telse if (OFF_SLAB(cachep)) {\n\t\t/* Slab management obj is off-slab. */\n\t\tfreelist = kmem_cache_alloc_node(cachep->freelist_cache,\n\t\t\t\t\t      local_flags, nodeid);\n\t\tif (!freelist)\n\t\t\treturn NULL;\n\t} else {\n\t\t/* We will use last bytes at the slab for freelist */\n\t\tfreelist = addr + (PAGE_SIZE << cachep->gfporder) -\n\t\t\t\tcachep->freelist_size;\n\t}\n\n\treturn freelist;\n}\n\nstatic inline freelist_idx_t get_free_obj(struct page *page, unsigned int idx)\n{\n\treturn ((freelist_idx_t *)page->freelist)[idx];\n}\n\nstatic inline void set_free_obj(struct page *page,\n\t\t\t\t\tunsigned int idx, freelist_idx_t val)\n{\n\t((freelist_idx_t *)(page->freelist))[idx] = val;\n}\n\nstatic void cache_init_objs_debug(struct kmem_cache *cachep, struct page *page)\n{\n#if DEBUG\n\tint i;\n\n\tfor (i = 0; i < cachep->num; i++) {\n\t\tvoid *objp = index_to_obj(cachep, page, i);\n\n\t\tif (cachep->flags & SLAB_STORE_USER)\n\t\t\t*dbg_userword(cachep, objp) = NULL;\n\n\t\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\t\t*dbg_redzone1(cachep, objp) = RED_INACTIVE;\n\t\t\t*dbg_redzone2(cachep, objp) = RED_INACTIVE;\n\t\t}\n\t\t/*\n\t\t * Constructors are not allowed to allocate memory from the same\n\t\t * cache which they are a constructor for.  Otherwise, deadlock.\n\t\t * They must also be threaded.\n\t\t */\n\t\tif (cachep->ctor && !(cachep->flags & SLAB_POISON)) {\n\t\t\tkasan_unpoison_object_data(cachep,\n\t\t\t\t\t\t   objp + obj_offset(cachep));\n\t\t\tcachep->ctor(objp + obj_offset(cachep));\n\t\t\tkasan_poison_object_data(\n\t\t\t\tcachep, objp + obj_offset(cachep));\n\t\t}\n\n\t\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\t\tif (*dbg_redzone2(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"constructor overwrote the end of an object\");\n\t\t\tif (*dbg_redzone1(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"constructor overwrote the start of an object\");\n\t\t}\n\t\t/* need to poison the objs? */\n\t\tif (cachep->flags & SLAB_POISON) {\n\t\t\tpoison_obj(cachep, objp, POISON_FREE);\n\t\t\tslab_kernel_map(cachep, objp, 0, 0);\n\t\t}\n\t}\n#endif\n}\n\n#ifdef CONFIG_SLAB_FREELIST_RANDOM\n/* Hold information during a freelist initialization */\nunion freelist_init_state {\n\tstruct {\n\t\tunsigned int pos;\n\t\tunsigned int *list;\n\t\tunsigned int count;\n\t};\n\tstruct rnd_state rnd_state;\n};\n\n/*\n * Initialize the state based on the randomization methode available.\n * return true if the pre-computed list is available, false otherwize.\n */\nstatic bool freelist_state_initialize(union freelist_init_state *state,\n\t\t\t\tstruct kmem_cache *cachep,\n\t\t\t\tunsigned int count)\n{\n\tbool ret;\n\tunsigned int rand;\n\n\t/* Use best entropy available to define a random shift */\n\trand = get_random_int();\n\n\t/* Use a random state if the pre-computed list is not available */\n\tif (!cachep->random_seq) {\n\t\tprandom_seed_state(&state->rnd_state, rand);\n\t\tret = false;\n\t} else {\n\t\tstate->list = cachep->random_seq;\n\t\tstate->count = count;\n\t\tstate->pos = rand % count;\n\t\tret = true;\n\t}\n\treturn ret;\n}\n\n/* Get the next entry on the list and randomize it using a random shift */\nstatic freelist_idx_t next_random_slot(union freelist_init_state *state)\n{\n\tif (state->pos >= state->count)\n\t\tstate->pos = 0;\n\treturn state->list[state->pos++];\n}\n\n/* Swap two freelist entries */\nstatic void swap_free_obj(struct page *page, unsigned int a, unsigned int b)\n{\n\tswap(((freelist_idx_t *)page->freelist)[a],\n\t\t((freelist_idx_t *)page->freelist)[b]);\n}\n\n/*\n * Shuffle the freelist initialization state based on pre-computed lists.\n * return true if the list was successfully shuffled, false otherwise.\n */\nstatic bool shuffle_freelist(struct kmem_cache *cachep, struct page *page)\n{\n\tunsigned int objfreelist = 0, i, rand, count = cachep->num;\n\tunion freelist_init_state state;\n\tbool precomputed;\n\n\tif (count < 2)\n\t\treturn false;\n\n\tprecomputed = freelist_state_initialize(&state, cachep, count);\n\n\t/* Take a random entry as the objfreelist */\n\tif (OBJFREELIST_SLAB(cachep)) {\n\t\tif (!precomputed)\n\t\t\tobjfreelist = count - 1;\n\t\telse\n\t\t\tobjfreelist = next_random_slot(&state);\n\t\tpage->freelist = index_to_obj(cachep, page, objfreelist) +\n\t\t\t\t\t\tobj_offset(cachep);\n\t\tcount--;\n\t}\n\n\t/*\n\t * On early boot, generate the list dynamically.\n\t * Later use a pre-computed list for speed.\n\t */\n\tif (!precomputed) {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tset_free_obj(page, i, i);\n\n\t\t/* Fisher-Yates shuffle */\n\t\tfor (i = count - 1; i > 0; i--) {\n\t\t\trand = prandom_u32_state(&state.rnd_state);\n\t\t\trand %= (i + 1);\n\t\t\tswap_free_obj(page, i, rand);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tset_free_obj(page, i, next_random_slot(&state));\n\t}\n\n\tif (OBJFREELIST_SLAB(cachep))\n\t\tset_free_obj(page, cachep->num - 1, objfreelist);\n\n\treturn true;\n}\n#else\nstatic inline bool shuffle_freelist(struct kmem_cache *cachep,\n\t\t\t\tstruct page *page)\n{\n\treturn false;\n}\n#endif /* CONFIG_SLAB_FREELIST_RANDOM */\n\nstatic void cache_init_objs(struct kmem_cache *cachep,\n\t\t\t    struct page *page)\n{\n\tint i;\n\tvoid *objp;\n\tbool shuffled;\n\n\tcache_init_objs_debug(cachep, page);\n\n\t/* Try to randomize the freelist if enabled */\n\tshuffled = shuffle_freelist(cachep, page);\n\n\tif (!shuffled && OBJFREELIST_SLAB(cachep)) {\n\t\tpage->freelist = index_to_obj(cachep, page, cachep->num - 1) +\n\t\t\t\t\t\tobj_offset(cachep);\n\t}\n\n\tfor (i = 0; i < cachep->num; i++) {\n\t\tobjp = index_to_obj(cachep, page, i);\n\t\tkasan_init_slab_obj(cachep, objp);\n\n\t\t/* constructor could break poison info */\n\t\tif (DEBUG == 0 && cachep->ctor) {\n\t\t\tkasan_unpoison_object_data(cachep, objp);\n\t\t\tcachep->ctor(objp);\n\t\t\tkasan_poison_object_data(cachep, objp);\n\t\t}\n\n\t\tif (!shuffled)\n\t\t\tset_free_obj(page, i, i);\n\t}\n}\n\nstatic void *slab_get_obj(struct kmem_cache *cachep, struct page *page)\n{\n\tvoid *objp;\n\n\tobjp = index_to_obj(cachep, page, get_free_obj(page, page->active));\n\tpage->active++;\n\n#if DEBUG\n\tif (cachep->flags & SLAB_STORE_USER)\n\t\tset_store_user_dirty(cachep);\n#endif\n\n\treturn objp;\n}\n\nstatic void slab_put_obj(struct kmem_cache *cachep,\n\t\t\tstruct page *page, void *objp)\n{\n\tunsigned int objnr = obj_to_index(cachep, page, objp);\n#if DEBUG\n\tunsigned int i;\n\n\t/* Verify double free bug */\n\tfor (i = page->active; i < cachep->num; i++) {\n\t\tif (get_free_obj(page, i) == objnr) {\n\t\t\tpr_err(\"slab: double free detected in cache '%s', objp %p\\n\",\n\t\t\t       cachep->name, objp);\n\t\t\tBUG();\n\t\t}\n\t}\n#endif\n\tpage->active--;\n\tif (!page->freelist)\n\t\tpage->freelist = objp + obj_offset(cachep);\n\n\tset_free_obj(page, page->active, objnr);\n}\n\n/*\n * Map pages beginning at addr to the given cache and slab. This is required\n * for the slab allocator to be able to lookup the cache and slab of a\n * virtual address for kfree, ksize, and slab debugging.\n */\nstatic void slab_map_pages(struct kmem_cache *cache, struct page *page,\n\t\t\t   void *freelist)\n{\n\tpage->slab_cache = cache;\n\tpage->freelist = freelist;\n}\n\n/*\n * Grow (by 1) the number of slabs within a cache.  This is called by\n * kmem_cache_alloc() when there are no active objs left in a cache.\n */\nstatic struct page *cache_grow_begin(struct kmem_cache *cachep,\n\t\t\t\tgfp_t flags, int nodeid)\n{\n\tvoid *freelist;\n\tsize_t offset;\n\tgfp_t local_flags;\n\tint page_node;\n\tstruct kmem_cache_node *n;\n\tstruct page *page;\n\n\t/*\n\t * Be lazy and only check for valid flags here,  keeping it out of the\n\t * critical path in kmem_cache_alloc().\n\t */\n\tif (unlikely(flags & GFP_SLAB_BUG_MASK)) {\n\t\tgfp_t invalid_mask = flags & GFP_SLAB_BUG_MASK;\n\t\tflags &= ~GFP_SLAB_BUG_MASK;\n\t\tpr_warn(\"Unexpected gfp: %#x (%pGg). Fixing up to gfp: %#x (%pGg). Fix your code!\\n\",\n\t\t\t\tinvalid_mask, &invalid_mask, flags, &flags);\n\t\tdump_stack();\n\t}\n\tlocal_flags = flags & (GFP_CONSTRAINT_MASK|GFP_RECLAIM_MASK);\n\n\tcheck_irq_off();\n\tif (gfpflags_allow_blocking(local_flags))\n\t\tlocal_irq_enable();\n\n\t/*\n\t * Get mem for the objs.  Attempt to allocate a physical page from\n\t * 'nodeid'.\n\t */\n\tpage = kmem_getpages(cachep, local_flags, nodeid);\n\tif (!page)\n\t\tgoto failed;\n\n\tpage_node = page_to_nid(page);\n\tn = get_node(cachep, page_node);\n\n\t/* Get colour for the slab, and cal the next value. */\n\tn->colour_next++;\n\tif (n->colour_next >= cachep->colour)\n\t\tn->colour_next = 0;\n\n\toffset = n->colour_next;\n\tif (offset >= cachep->colour)\n\t\toffset = 0;\n\n\toffset *= cachep->colour_off;\n\n\t/* Get slab management. */\n\tfreelist = alloc_slabmgmt(cachep, page, offset,\n\t\t\tlocal_flags & ~GFP_CONSTRAINT_MASK, page_node);\n\tif (OFF_SLAB(cachep) && !freelist)\n\t\tgoto opps1;\n\n\tslab_map_pages(cachep, page, freelist);\n\n\tkasan_poison_slab(page);\n\tcache_init_objs(cachep, page);\n\n\tif (gfpflags_allow_blocking(local_flags))\n\t\tlocal_irq_disable();\n\n\treturn page;\n\nopps1:\n\tkmem_freepages(cachep, page);\nfailed:\n\tif (gfpflags_allow_blocking(local_flags))\n\t\tlocal_irq_disable();\n\treturn NULL;\n}\n\nstatic void cache_grow_end(struct kmem_cache *cachep, struct page *page)\n{\n\tstruct kmem_cache_node *n;\n\tvoid *list = NULL;\n\n\tcheck_irq_off();\n\n\tif (!page)\n\t\treturn;\n\n\tINIT_LIST_HEAD(&page->lru);\n\tn = get_node(cachep, page_to_nid(page));\n\n\tspin_lock(&n->list_lock);\n\tn->total_slabs++;\n\tif (!page->active) {\n\t\tlist_add_tail(&page->lru, &(n->slabs_free));\n\t\tn->free_slabs++;\n\t} else\n\t\tfixup_slab_list(cachep, n, page, &list);\n\n\tSTATS_INC_GROWN(cachep);\n\tn->free_objects += cachep->num - page->active;\n\tspin_unlock(&n->list_lock);\n\n\tfixup_objfreelist_debug(cachep, &list);\n}\n\n#if DEBUG\n\n/*\n * Perform extra freeing checks:\n * - detect bad pointers.\n * - POISON/RED_ZONE checking\n */\nstatic void kfree_debugcheck(const void *objp)\n{\n\tif (!virt_addr_valid(objp)) {\n\t\tpr_err(\"kfree_debugcheck: out of range ptr %lxh\\n\",\n\t\t       (unsigned long)objp);\n\t\tBUG();\n\t}\n}\n\nstatic inline void verify_redzone_free(struct kmem_cache *cache, void *obj)\n{\n\tunsigned long long redzone1, redzone2;\n\n\tredzone1 = *dbg_redzone1(cache, obj);\n\tredzone2 = *dbg_redzone2(cache, obj);\n\n\t/*\n\t * Redzone is ok.\n\t */\n\tif (redzone1 == RED_ACTIVE && redzone2 == RED_ACTIVE)\n\t\treturn;\n\n\tif (redzone1 == RED_INACTIVE && redzone2 == RED_INACTIVE)\n\t\tslab_error(cache, \"double free detected\");\n\telse\n\t\tslab_error(cache, \"memory outside object was overwritten\");\n\n\tpr_err(\"%p: redzone 1:0x%llx, redzone 2:0x%llx\\n\",\n\t       obj, redzone1, redzone2);\n}\n\nstatic void *cache_free_debugcheck(struct kmem_cache *cachep, void *objp,\n\t\t\t\t   unsigned long caller)\n{\n\tunsigned int objnr;\n\tstruct page *page;\n\n\tBUG_ON(virt_to_cache(objp) != cachep);\n\n\tobjp -= obj_offset(cachep);\n\tkfree_debugcheck(objp);\n\tpage = virt_to_head_page(objp);\n\n\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\tverify_redzone_free(cachep, objp);\n\t\t*dbg_redzone1(cachep, objp) = RED_INACTIVE;\n\t\t*dbg_redzone2(cachep, objp) = RED_INACTIVE;\n\t}\n\tif (cachep->flags & SLAB_STORE_USER) {\n\t\tset_store_user_dirty(cachep);\n\t\t*dbg_userword(cachep, objp) = (void *)caller;\n\t}\n\n\tobjnr = obj_to_index(cachep, page, objp);\n\n\tBUG_ON(objnr >= cachep->num);\n\tBUG_ON(objp != index_to_obj(cachep, page, objnr));\n\n\tif (cachep->flags & SLAB_POISON) {\n\t\tpoison_obj(cachep, objp, POISON_FREE);\n\t\tslab_kernel_map(cachep, objp, 0, caller);\n\t}\n\treturn objp;\n}\n\n#else\n#define kfree_debugcheck(x) do { } while(0)\n#define cache_free_debugcheck(x,objp,z) (objp)\n#endif\n\nstatic inline void fixup_objfreelist_debug(struct kmem_cache *cachep,\n\t\t\t\t\t\tvoid **list)\n{\n#if DEBUG\n\tvoid *next = *list;\n\tvoid *objp;\n\n\twhile (next) {\n\t\tobjp = next - obj_offset(cachep);\n\t\tnext = *(void **)next;\n\t\tpoison_obj(cachep, objp, POISON_FREE);\n\t}\n#endif\n}\n\nstatic inline void fixup_slab_list(struct kmem_cache *cachep,\n\t\t\t\tstruct kmem_cache_node *n, struct page *page,\n\t\t\t\tvoid **list)\n{\n\t/* move slabp to correct slabp list: */\n\tlist_del(&page->lru);\n\tif (page->active == cachep->num) {\n\t\tlist_add(&page->lru, &n->slabs_full);\n\t\tif (OBJFREELIST_SLAB(cachep)) {\n#if DEBUG\n\t\t\t/* Poisoning will be done without holding the lock */\n\t\t\tif (cachep->flags & SLAB_POISON) {\n\t\t\t\tvoid **objp = page->freelist;\n\n\t\t\t\t*objp = *list;\n\t\t\t\t*list = objp;\n\t\t\t}\n#endif\n\t\t\tpage->freelist = NULL;\n\t\t}\n\t} else\n\t\tlist_add(&page->lru, &n->slabs_partial);\n}\n\n/* Try to find non-pfmemalloc slab if needed */\nstatic noinline struct page *get_valid_first_slab(struct kmem_cache_node *n,\n\t\t\t\t\tstruct page *page, bool pfmemalloc)\n{\n\tif (!page)\n\t\treturn NULL;\n\n\tif (pfmemalloc)\n\t\treturn page;\n\n\tif (!PageSlabPfmemalloc(page))\n\t\treturn page;\n\n\t/* No need to keep pfmemalloc slab if we have enough free objects */\n\tif (n->free_objects > n->free_limit) {\n\t\tClearPageSlabPfmemalloc(page);\n\t\treturn page;\n\t}\n\n\t/* Move pfmemalloc slab to the end of list to speed up next search */\n\tlist_del(&page->lru);\n\tif (!page->active) {\n\t\tlist_add_tail(&page->lru, &n->slabs_free);\n\t\tn->free_slabs++;\n\t} else\n\t\tlist_add_tail(&page->lru, &n->slabs_partial);\n\n\tlist_for_each_entry(page, &n->slabs_partial, lru) {\n\t\tif (!PageSlabPfmemalloc(page))\n\t\t\treturn page;\n\t}\n\n\tn->free_touched = 1;\n\tlist_for_each_entry(page, &n->slabs_free, lru) {\n\t\tif (!PageSlabPfmemalloc(page)) {\n\t\t\tn->free_slabs--;\n\t\t\treturn page;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct page *get_first_slab(struct kmem_cache_node *n, bool pfmemalloc)\n{\n\tstruct page *page;\n\n\tassert_spin_locked(&n->list_lock);\n\tpage = list_first_entry_or_null(&n->slabs_partial, struct page, lru);\n\tif (!page) {\n\t\tn->free_touched = 1;\n\t\tpage = list_first_entry_or_null(&n->slabs_free, struct page,\n\t\t\t\t\t\tlru);\n\t\tif (page)\n\t\t\tn->free_slabs--;\n\t}\n\n\tif (sk_memalloc_socks())\n\t\tpage = get_valid_first_slab(n, page, pfmemalloc);\n\n\treturn page;\n}\n\nstatic noinline void *cache_alloc_pfmemalloc(struct kmem_cache *cachep,\n\t\t\t\tstruct kmem_cache_node *n, gfp_t flags)\n{\n\tstruct page *page;\n\tvoid *obj;\n\tvoid *list = NULL;\n\n\tif (!gfp_pfmemalloc_allowed(flags))\n\t\treturn NULL;\n\n\tspin_lock(&n->list_lock);\n\tpage = get_first_slab(n, true);\n\tif (!page) {\n\t\tspin_unlock(&n->list_lock);\n\t\treturn NULL;\n\t}\n\n\tobj = slab_get_obj(cachep, page);\n\tn->free_objects--;\n\n\tfixup_slab_list(cachep, n, page, &list);\n\n\tspin_unlock(&n->list_lock);\n\tfixup_objfreelist_debug(cachep, &list);\n\n\treturn obj;\n}\n\n/*\n * Slab list should be fixed up by fixup_slab_list() for existing slab\n * or cache_grow_end() for new slab\n */\nstatic __always_inline int alloc_block(struct kmem_cache *cachep,\n\t\tstruct array_cache *ac, struct page *page, int batchcount)\n{\n\t/*\n\t * There must be at least one object available for\n\t * allocation.\n\t */\n\tBUG_ON(page->active >= cachep->num);\n\n\twhile (page->active < cachep->num && batchcount--) {\n\t\tSTATS_INC_ALLOCED(cachep);\n\t\tSTATS_INC_ACTIVE(cachep);\n\t\tSTATS_SET_HIGH(cachep);\n\n\t\tac->entry[ac->avail++] = slab_get_obj(cachep, page);\n\t}\n\n\treturn batchcount;\n}\n\nstatic void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags)\n{\n\tint batchcount;\n\tstruct kmem_cache_node *n;\n\tstruct array_cache *ac, *shared;\n\tint node;\n\tvoid *list = NULL;\n\tstruct page *page;\n\n\tcheck_irq_off();\n\tnode = numa_mem_id();\n\n\tac = cpu_cache_get(cachep);\n\tbatchcount = ac->batchcount;\n\tif (!ac->touched && batchcount > BATCHREFILL_LIMIT) {\n\t\t/*\n\t\t * If there was little recent activity on this cache, then\n\t\t * perform only a partial refill.  Otherwise we could generate\n\t\t * refill bouncing.\n\t\t */\n\t\tbatchcount = BATCHREFILL_LIMIT;\n\t}\n\tn = get_node(cachep, node);\n\n\tBUG_ON(ac->avail > 0 || !n);\n\tshared = READ_ONCE(n->shared);\n\tif (!n->free_objects && (!shared || !shared->avail))\n\t\tgoto direct_grow;\n\n\tspin_lock(&n->list_lock);\n\tshared = READ_ONCE(n->shared);\n\n\t/* See if we can refill from the shared array */\n\tif (shared && transfer_objects(ac, shared, batchcount)) {\n\t\tshared->touched = 1;\n\t\tgoto alloc_done;\n\t}\n\n\twhile (batchcount > 0) {\n\t\t/* Get slab alloc is to come from. */\n\t\tpage = get_first_slab(n, false);\n\t\tif (!page)\n\t\t\tgoto must_grow;\n\n\t\tcheck_spinlock_acquired(cachep);\n\n\t\tbatchcount = alloc_block(cachep, ac, page, batchcount);\n\t\tfixup_slab_list(cachep, n, page, &list);\n\t}\n\nmust_grow:\n\tn->free_objects -= ac->avail;\nalloc_done:\n\tspin_unlock(&n->list_lock);\n\tfixup_objfreelist_debug(cachep, &list);\n\ndirect_grow:\n\tif (unlikely(!ac->avail)) {\n\t\t/* Check if we can use obj in pfmemalloc slab */\n\t\tif (sk_memalloc_socks()) {\n\t\t\tvoid *obj = cache_alloc_pfmemalloc(cachep, n, flags);\n\n\t\t\tif (obj)\n\t\t\t\treturn obj;\n\t\t}\n\n\t\tpage = cache_grow_begin(cachep, gfp_exact_node(flags), node);\n\n\t\t/*\n\t\t * cache_grow_begin() can reenable interrupts,\n\t\t * then ac could change.\n\t\t */\n\t\tac = cpu_cache_get(cachep);\n\t\tif (!ac->avail && page)\n\t\t\talloc_block(cachep, ac, page, batchcount);\n\t\tcache_grow_end(cachep, page);\n\n\t\tif (!ac->avail)\n\t\t\treturn NULL;\n\t}\n\tac->touched = 1;\n\n\treturn ac->entry[--ac->avail];\n}\n\nstatic inline void cache_alloc_debugcheck_before(struct kmem_cache *cachep,\n\t\t\t\t\t\tgfp_t flags)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(flags));\n}\n\n#if DEBUG\nstatic void *cache_alloc_debugcheck_after(struct kmem_cache *cachep,\n\t\t\t\tgfp_t flags, void *objp, unsigned long caller)\n{\n\tif (!objp)\n\t\treturn objp;\n\tif (cachep->flags & SLAB_POISON) {\n\t\tcheck_poison_obj(cachep, objp);\n\t\tslab_kernel_map(cachep, objp, 1, 0);\n\t\tpoison_obj(cachep, objp, POISON_INUSE);\n\t}\n\tif (cachep->flags & SLAB_STORE_USER)\n\t\t*dbg_userword(cachep, objp) = (void *)caller;\n\n\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\tif (*dbg_redzone1(cachep, objp) != RED_INACTIVE ||\n\t\t\t\t*dbg_redzone2(cachep, objp) != RED_INACTIVE) {\n\t\t\tslab_error(cachep, \"double free, or memory outside object was overwritten\");\n\t\t\tpr_err(\"%p: redzone 1:0x%llx, redzone 2:0x%llx\\n\",\n\t\t\t       objp, *dbg_redzone1(cachep, objp),\n\t\t\t       *dbg_redzone2(cachep, objp));\n\t\t}\n\t\t*dbg_redzone1(cachep, objp) = RED_ACTIVE;\n\t\t*dbg_redzone2(cachep, objp) = RED_ACTIVE;\n\t}\n\n\tobjp += obj_offset(cachep);\n\tif (cachep->ctor && cachep->flags & SLAB_POISON)\n\t\tcachep->ctor(objp);\n\tif (ARCH_SLAB_MINALIGN &&\n\t    ((unsigned long)objp & (ARCH_SLAB_MINALIGN-1))) {\n\t\tpr_err(\"0x%p: not aligned to ARCH_SLAB_MINALIGN=%d\\n\",\n\t\t       objp, (int)ARCH_SLAB_MINALIGN);\n\t}\n\treturn objp;\n}\n#else\n#define cache_alloc_debugcheck_after(a,b,objp,d) (objp)\n#endif\n\nstatic inline void *____cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\tvoid *objp;\n\tstruct array_cache *ac;\n\n\tcheck_irq_off();\n\n\tac = cpu_cache_get(cachep);\n\tif (likely(ac->avail)) {\n\t\tac->touched = 1;\n\t\tobjp = ac->entry[--ac->avail];\n\n\t\tSTATS_INC_ALLOCHIT(cachep);\n\t\tgoto out;\n\t}\n\n\tSTATS_INC_ALLOCMISS(cachep);\n\tobjp = cache_alloc_refill(cachep, flags);\n\t/*\n\t * the 'ac' may be updated by cache_alloc_refill(),\n\t * and kmemleak_erase() requires its correct value.\n\t */\n\tac = cpu_cache_get(cachep);\n\nout:\n\t/*\n\t * To avoid a false negative, if an object that is in one of the\n\t * per-CPU caches is leaked, we need to make sure kmemleak doesn't\n\t * treat the array pointers as a reference to the object.\n\t */\n\tif (objp)\n\t\tkmemleak_erase(&ac->entry[ac->avail]);\n\treturn objp;\n}\n\n#ifdef CONFIG_NUMA\n/*\n * Try allocating on another node if PFA_SPREAD_SLAB is a mempolicy is set.\n *\n * If we are in_interrupt, then process context, including cpusets and\n * mempolicy, may not apply and should not be used for allocation policy.\n */\nstatic void *alternate_node_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\tint nid_alloc, nid_here;\n\n\tif (in_interrupt() || (flags & __GFP_THISNODE))\n\t\treturn NULL;\n\tnid_alloc = nid_here = numa_mem_id();\n\tif (cpuset_do_slab_mem_spread() && (cachep->flags & SLAB_MEM_SPREAD))\n\t\tnid_alloc = cpuset_slab_spread_node();\n\telse if (current->mempolicy)\n\t\tnid_alloc = mempolicy_slab_node();\n\tif (nid_alloc != nid_here)\n\t\treturn ____cache_alloc_node(cachep, flags, nid_alloc);\n\treturn NULL;\n}\n\n/*\n * Fallback function if there was no memory available and no objects on a\n * certain node and fall back is permitted. First we scan all the\n * available node for available objects. If that fails then we\n * perform an allocation without specifying a node. This allows the page\n * allocator to do its reclaim / fallback magic. We then insert the\n * slab into the proper nodelist and then allocate from it.\n */\nstatic void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)\n{\n\tstruct zonelist *zonelist;\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tenum zone_type high_zoneidx = gfp_zone(flags);\n\tvoid *obj = NULL;\n\tstruct page *page;\n\tint nid;\n\tunsigned int cpuset_mems_cookie;\n\n\tif (flags & __GFP_THISNODE)\n\t\treturn NULL;\n\nretry_cpuset:\n\tcpuset_mems_cookie = read_mems_allowed_begin();\n\tzonelist = node_zonelist(mempolicy_slab_node(), flags);\n\nretry:\n\t/*\n\t * Look through allowed nodes for objects available\n\t * from existing per node queues.\n\t */\n\tfor_each_zone_zonelist(zone, z, zonelist, high_zoneidx) {\n\t\tnid = zone_to_nid(zone);\n\n\t\tif (cpuset_zone_allowed(zone, flags) &&\n\t\t\tget_node(cache, nid) &&\n\t\t\tget_node(cache, nid)->free_objects) {\n\t\t\t\tobj = ____cache_alloc_node(cache,\n\t\t\t\t\tgfp_exact_node(flags), nid);\n\t\t\t\tif (obj)\n\t\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!obj) {\n\t\t/*\n\t\t * This allocation will be performed within the constraints\n\t\t * of the current cpuset / memory policy requirements.\n\t\t * We may trigger various forms of reclaim on the allowed\n\t\t * set and go into memory reserves if necessary.\n\t\t */\n\t\tpage = cache_grow_begin(cache, flags, numa_mem_id());\n\t\tcache_grow_end(cache, page);\n\t\tif (page) {\n\t\t\tnid = page_to_nid(page);\n\t\t\tobj = ____cache_alloc_node(cache,\n\t\t\t\tgfp_exact_node(flags), nid);\n\n\t\t\t/*\n\t\t\t * Another processor may allocate the objects in\n\t\t\t * the slab since we are not holding any locks.\n\t\t\t */\n\t\t\tif (!obj)\n\t\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tif (unlikely(!obj && read_mems_allowed_retry(cpuset_mems_cookie)))\n\t\tgoto retry_cpuset;\n\treturn obj;\n}\n\n/*\n * A interface to enable slab creation on nodeid\n */\nstatic void *____cache_alloc_node(struct kmem_cache *cachep, gfp_t flags,\n\t\t\t\tint nodeid)\n{\n\tstruct page *page;\n\tstruct kmem_cache_node *n;\n\tvoid *obj = NULL;\n\tvoid *list = NULL;\n\n\tVM_BUG_ON(nodeid < 0 || nodeid >= MAX_NUMNODES);\n\tn = get_node(cachep, nodeid);\n\tBUG_ON(!n);\n\n\tcheck_irq_off();\n\tspin_lock(&n->list_lock);\n\tpage = get_first_slab(n, false);\n\tif (!page)\n\t\tgoto must_grow;\n\n\tcheck_spinlock_acquired_node(cachep, nodeid);\n\n\tSTATS_INC_NODEALLOCS(cachep);\n\tSTATS_INC_ACTIVE(cachep);\n\tSTATS_SET_HIGH(cachep);\n\n\tBUG_ON(page->active == cachep->num);\n\n\tobj = slab_get_obj(cachep, page);\n\tn->free_objects--;\n\n\tfixup_slab_list(cachep, n, page, &list);\n\n\tspin_unlock(&n->list_lock);\n\tfixup_objfreelist_debug(cachep, &list);\n\treturn obj;\n\nmust_grow:\n\tspin_unlock(&n->list_lock);\n\tpage = cache_grow_begin(cachep, gfp_exact_node(flags), nodeid);\n\tif (page) {\n\t\t/* This slab isn't counted yet so don't update free_objects */\n\t\tobj = slab_get_obj(cachep, page);\n\t}\n\tcache_grow_end(cachep, page);\n\n\treturn obj ? obj : fallback_alloc(cachep, flags);\n}\n\nstatic __always_inline void *\nslab_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid,\n\t\t   unsigned long caller)\n{\n\tunsigned long save_flags;\n\tvoid *ptr;\n\tint slab_node = numa_mem_id();\n\n\tflags &= gfp_allowed_mask;\n\tcachep = slab_pre_alloc_hook(cachep, flags);\n\tif (unlikely(!cachep))\n\t\treturn NULL;\n\n\tcache_alloc_debugcheck_before(cachep, flags);\n\tlocal_irq_save(save_flags);\n\n\tif (nodeid == NUMA_NO_NODE)\n\t\tnodeid = slab_node;\n\n\tif (unlikely(!get_node(cachep, nodeid))) {\n\t\t/* Node not bootstrapped yet */\n\t\tptr = fallback_alloc(cachep, flags);\n\t\tgoto out;\n\t}\n\n\tif (nodeid == slab_node) {\n\t\t/*\n\t\t * Use the locally cached objects if possible.\n\t\t * However ____cache_alloc does not allow fallback\n\t\t * to other nodes. It may fail while we still have\n\t\t * objects on other nodes available.\n\t\t */\n\t\tptr = ____cache_alloc(cachep, flags);\n\t\tif (ptr)\n\t\t\tgoto out;\n\t}\n\t/* ___cache_alloc_node can fall back to other nodes */\n\tptr = ____cache_alloc_node(cachep, flags, nodeid);\n  out:\n\tlocal_irq_restore(save_flags);\n\tptr = cache_alloc_debugcheck_after(cachep, flags, ptr, caller);\n\n\tif (unlikely(flags & __GFP_ZERO) && ptr)\n\t\tmemset(ptr, 0, cachep->object_size);\n\n\tslab_post_alloc_hook(cachep, flags, 1, &ptr);\n\treturn ptr;\n}\n\nstatic __always_inline void *\n__do_cache_alloc(struct kmem_cache *cache, gfp_t flags)\n{\n\tvoid *objp;\n\n\tif (current->mempolicy || cpuset_do_slab_mem_spread()) {\n\t\tobjp = alternate_node_alloc(cache, flags);\n\t\tif (objp)\n\t\t\tgoto out;\n\t}\n\tobjp = ____cache_alloc(cache, flags);\n\n\t/*\n\t * We may just have run out of memory on the local node.\n\t * ____cache_alloc_node() knows how to locate memory on other nodes\n\t */\n\tif (!objp)\n\t\tobjp = ____cache_alloc_node(cache, flags, numa_mem_id());\n\n  out:\n\treturn objp;\n}\n#else\n\nstatic __always_inline void *\n__do_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\treturn ____cache_alloc(cachep, flags);\n}\n\n#endif /* CONFIG_NUMA */\n\nstatic __always_inline void *\nslab_alloc(struct kmem_cache *cachep, gfp_t flags, unsigned long caller)\n{\n\tunsigned long save_flags;\n\tvoid *objp;\n\n\tflags &= gfp_allowed_mask;\n\tcachep = slab_pre_alloc_hook(cachep, flags);\n\tif (unlikely(!cachep))\n\t\treturn NULL;\n\n\tcache_alloc_debugcheck_before(cachep, flags);\n\tlocal_irq_save(save_flags);\n\tobjp = __do_cache_alloc(cachep, flags);\n\tlocal_irq_restore(save_flags);\n\tobjp = cache_alloc_debugcheck_after(cachep, flags, objp, caller);\n\tprefetchw(objp);\n\n\tif (unlikely(flags & __GFP_ZERO) && objp)\n\t\tmemset(objp, 0, cachep->object_size);\n\n\tslab_post_alloc_hook(cachep, flags, 1, &objp);\n\treturn objp;\n}\n\n/*\n * Caller needs to acquire correct kmem_cache_node's list_lock\n * @list: List of detached free slabs should be freed by caller\n */\nstatic void free_block(struct kmem_cache *cachep, void **objpp,\n\t\t\tint nr_objects, int node, struct list_head *list)\n{\n\tint i;\n\tstruct kmem_cache_node *n = get_node(cachep, node);\n\tstruct page *page;\n\n\tn->free_objects += nr_objects;\n\n\tfor (i = 0; i < nr_objects; i++) {\n\t\tvoid *objp;\n\t\tstruct page *page;\n\n\t\tobjp = objpp[i];\n\n\t\tpage = virt_to_head_page(objp);\n\t\tlist_del(&page->lru);\n\t\tcheck_spinlock_acquired_node(cachep, node);\n\t\tslab_put_obj(cachep, page, objp);\n\t\tSTATS_DEC_ACTIVE(cachep);\n\n\t\t/* fixup slab chains */\n\t\tif (page->active == 0) {\n\t\t\tlist_add(&page->lru, &n->slabs_free);\n\t\t\tn->free_slabs++;\n\t\t} else {\n\t\t\t/* Unconditionally move a slab to the end of the\n\t\t\t * partial list on free - maximum time for the\n\t\t\t * other objects to be freed, too.\n\t\t\t */\n\t\t\tlist_add_tail(&page->lru, &n->slabs_partial);\n\t\t}\n\t}\n\n\twhile (n->free_objects > n->free_limit && !list_empty(&n->slabs_free)) {\n\t\tn->free_objects -= cachep->num;\n\n\t\tpage = list_last_entry(&n->slabs_free, struct page, lru);\n\t\tlist_move(&page->lru, list);\n\t\tn->free_slabs--;\n\t\tn->total_slabs--;\n\t}\n}\n\nstatic void cache_flusharray(struct kmem_cache *cachep, struct array_cache *ac)\n{\n\tint batchcount;\n\tstruct kmem_cache_node *n;\n\tint node = numa_mem_id();\n\tLIST_HEAD(list);\n\n\tbatchcount = ac->batchcount;\n\n\tcheck_irq_off();\n\tn = get_node(cachep, node);\n\tspin_lock(&n->list_lock);\n\tif (n->shared) {\n\t\tstruct array_cache *shared_array = n->shared;\n\t\tint max = shared_array->limit - shared_array->avail;\n\t\tif (max) {\n\t\t\tif (batchcount > max)\n\t\t\t\tbatchcount = max;\n\t\t\tmemcpy(&(shared_array->entry[shared_array->avail]),\n\t\t\t       ac->entry, sizeof(void *) * batchcount);\n\t\t\tshared_array->avail += batchcount;\n\t\t\tgoto free_done;\n\t\t}\n\t}\n\n\tfree_block(cachep, ac->entry, batchcount, node, &list);\nfree_done:\n#if STATS\n\t{\n\t\tint i = 0;\n\t\tstruct page *page;\n\n\t\tlist_for_each_entry(page, &n->slabs_free, lru) {\n\t\t\tBUG_ON(page->active);\n\n\t\t\ti++;\n\t\t}\n\t\tSTATS_SET_FREEABLE(cachep, i);\n\t}\n#endif\n\tspin_unlock(&n->list_lock);\n\tslabs_destroy(cachep, &list);\n\tac->avail -= batchcount;\n\tmemmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);\n}\n\n/*\n * Release an obj back to its cache. If the obj has a constructed state, it must\n * be in this state _before_ it is released.  Called with disabled ints.\n */\nstatic inline void __cache_free(struct kmem_cache *cachep, void *objp,\n\t\t\t\tunsigned long caller)\n{\n\t/* Put the object into the quarantine, don't touch it for now. */\n\tif (kasan_slab_free(cachep, objp))\n\t\treturn;\n\n\t___cache_free(cachep, objp, caller);\n}\n\nvoid ___cache_free(struct kmem_cache *cachep, void *objp,\n\t\tunsigned long caller)\n{\n\tstruct array_cache *ac = cpu_cache_get(cachep);\n\n\tcheck_irq_off();\n\tkmemleak_free_recursive(objp, cachep->flags);\n\tobjp = cache_free_debugcheck(cachep, objp, caller);\n\n\tkmemcheck_slab_free(cachep, objp, cachep->object_size);\n\n\t/*\n\t * Skip calling cache_free_alien() when the platform is not numa.\n\t * This will avoid cache misses that happen while accessing slabp (which\n\t * is per page memory  reference) to get nodeid. Instead use a global\n\t * variable to skip the call, which is mostly likely to be present in\n\t * the cache.\n\t */\n\tif (nr_online_nodes > 1 && cache_free_alien(cachep, objp))\n\t\treturn;\n\n\tif (ac->avail < ac->limit) {\n\t\tSTATS_INC_FREEHIT(cachep);\n\t} else {\n\t\tSTATS_INC_FREEMISS(cachep);\n\t\tcache_flusharray(cachep, ac);\n\t}\n\n\tif (sk_memalloc_socks()) {\n\t\tstruct page *page = virt_to_head_page(objp);\n\n\t\tif (unlikely(PageSlabPfmemalloc(page))) {\n\t\t\tcache_free_pfmemalloc(cachep, page, objp);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tac->entry[ac->avail++] = objp;\n}\n\n/**\n * kmem_cache_alloc - Allocate an object\n * @cachep: The cache to allocate from.\n * @flags: See kmalloc().\n *\n * Allocate an object from this cache.  The flags are only relevant\n * if the cache has no available objects.\n */\nvoid *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\tvoid *ret = slab_alloc(cachep, flags, _RET_IP_);\n\n\tkasan_slab_alloc(cachep, ret, flags);\n\ttrace_kmem_cache_alloc(_RET_IP_, ret,\n\t\t\t       cachep->object_size, cachep->size, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc);\n\nstatic __always_inline void\ncache_alloc_debugcheck_after_bulk(struct kmem_cache *s, gfp_t flags,\n\t\t\t\t  size_t size, void **p, unsigned long caller)\n{\n\tsize_t i;\n\n\tfor (i = 0; i < size; i++)\n\t\tp[i] = cache_alloc_debugcheck_after(s, flags, p[i], caller);\n}\n\nint kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,\n\t\t\t  void **p)\n{\n\tsize_t i;\n\n\ts = slab_pre_alloc_hook(s, flags);\n\tif (!s)\n\t\treturn 0;\n\n\tcache_alloc_debugcheck_before(s, flags);\n\n\tlocal_irq_disable();\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *objp = __do_cache_alloc(s, flags);\n\n\t\tif (unlikely(!objp))\n\t\t\tgoto error;\n\t\tp[i] = objp;\n\t}\n\tlocal_irq_enable();\n\n\tcache_alloc_debugcheck_after_bulk(s, flags, size, p, _RET_IP_);\n\n\t/* Clear memory outside IRQ disabled section */\n\tif (unlikely(flags & __GFP_ZERO))\n\t\tfor (i = 0; i < size; i++)\n\t\t\tmemset(p[i], 0, s->object_size);\n\n\tslab_post_alloc_hook(s, flags, size, p);\n\t/* FIXME: Trace call missing. Christoph would like a bulk variant */\n\treturn size;\nerror:\n\tlocal_irq_enable();\n\tcache_alloc_debugcheck_after_bulk(s, flags, i, p, _RET_IP_);\n\tslab_post_alloc_hook(s, flags, i, p);\n\t__kmem_cache_free_bulk(s, i, p);\n\treturn 0;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_bulk);\n\n#ifdef CONFIG_TRACING\nvoid *\nkmem_cache_alloc_trace(struct kmem_cache *cachep, gfp_t flags, size_t size)\n{\n\tvoid *ret;\n\n\tret = slab_alloc(cachep, flags, _RET_IP_);\n\n\tkasan_kmalloc(cachep, ret, size, flags);\n\ttrace_kmalloc(_RET_IP_, ret,\n\t\t      size, cachep->size, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_trace);\n#endif\n\n#ifdef CONFIG_NUMA\n/**\n * kmem_cache_alloc_node - Allocate an object on the specified node\n * @cachep: The cache to allocate from.\n * @flags: See kmalloc().\n * @nodeid: node number of the target node.\n *\n * Identical to kmem_cache_alloc but it will allocate memory on the given\n * node, which can improve the performance for cpu bound structures.\n *\n * Fallback to other node is possible if __GFP_THISNODE is not set.\n */\nvoid *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)\n{\n\tvoid *ret = slab_alloc_node(cachep, flags, nodeid, _RET_IP_);\n\n\tkasan_slab_alloc(cachep, ret, flags);\n\ttrace_kmem_cache_alloc_node(_RET_IP_, ret,\n\t\t\t\t    cachep->object_size, cachep->size,\n\t\t\t\t    flags, nodeid);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_node);\n\n#ifdef CONFIG_TRACING\nvoid *kmem_cache_alloc_node_trace(struct kmem_cache *cachep,\n\t\t\t\t  gfp_t flags,\n\t\t\t\t  int nodeid,\n\t\t\t\t  size_t size)\n{\n\tvoid *ret;\n\n\tret = slab_alloc_node(cachep, flags, nodeid, _RET_IP_);\n\n\tkasan_kmalloc(cachep, ret, size, flags);\n\ttrace_kmalloc_node(_RET_IP_, ret,\n\t\t\t   size, cachep->size,\n\t\t\t   flags, nodeid);\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_node_trace);\n#endif\n\nstatic __always_inline void *\n__do_kmalloc_node(size_t size, gfp_t flags, int node, unsigned long caller)\n{\n\tstruct kmem_cache *cachep;\n\tvoid *ret;\n\n\tcachep = kmalloc_slab(size, flags);\n\tif (unlikely(ZERO_OR_NULL_PTR(cachep)))\n\t\treturn cachep;\n\tret = kmem_cache_alloc_node_trace(cachep, flags, node, size);\n\tkasan_kmalloc(cachep, ret, size, flags);\n\n\treturn ret;\n}\n\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node)\n{\n\treturn __do_kmalloc_node(size, flags, node, _RET_IP_);\n}\nEXPORT_SYMBOL(__kmalloc_node);\n\nvoid *__kmalloc_node_track_caller(size_t size, gfp_t flags,\n\t\tint node, unsigned long caller)\n{\n\treturn __do_kmalloc_node(size, flags, node, caller);\n}\nEXPORT_SYMBOL(__kmalloc_node_track_caller);\n#endif /* CONFIG_NUMA */\n\n/**\n * __do_kmalloc - allocate memory\n * @size: how many bytes of memory are required.\n * @flags: the type of memory to allocate (see kmalloc).\n * @caller: function caller for debug tracking of the caller\n */\nstatic __always_inline void *__do_kmalloc(size_t size, gfp_t flags,\n\t\t\t\t\t  unsigned long caller)\n{\n\tstruct kmem_cache *cachep;\n\tvoid *ret;\n\n\tcachep = kmalloc_slab(size, flags);\n\tif (unlikely(ZERO_OR_NULL_PTR(cachep)))\n\t\treturn cachep;\n\tret = slab_alloc(cachep, flags, caller);\n\n\tkasan_kmalloc(cachep, ret, size, flags);\n\ttrace_kmalloc(caller, ret,\n\t\t      size, cachep->size, flags);\n\n\treturn ret;\n}\n\nvoid *__kmalloc(size_t size, gfp_t flags)\n{\n\treturn __do_kmalloc(size, flags, _RET_IP_);\n}\nEXPORT_SYMBOL(__kmalloc);\n\nvoid *__kmalloc_track_caller(size_t size, gfp_t flags, unsigned long caller)\n{\n\treturn __do_kmalloc(size, flags, caller);\n}\nEXPORT_SYMBOL(__kmalloc_track_caller);\n\n/**\n * kmem_cache_free - Deallocate an object\n * @cachep: The cache the allocation was from.\n * @objp: The previously allocated object.\n *\n * Free an object which was previously allocated from this\n * cache.\n */\nvoid kmem_cache_free(struct kmem_cache *cachep, void *objp)\n{\n\tunsigned long flags;\n\tcachep = cache_from_obj(cachep, objp);\n\tif (!cachep)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tdebug_check_no_locks_freed(objp, cachep->object_size);\n\tif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(objp, cachep->object_size);\n\t__cache_free(cachep, objp, _RET_IP_);\n\tlocal_irq_restore(flags);\n\n\ttrace_kmem_cache_free(_RET_IP_, objp);\n}\nEXPORT_SYMBOL(kmem_cache_free);\n\nvoid kmem_cache_free_bulk(struct kmem_cache *orig_s, size_t size, void **p)\n{\n\tstruct kmem_cache *s;\n\tsize_t i;\n\n\tlocal_irq_disable();\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *objp = p[i];\n\n\t\tif (!orig_s) /* called via kfree_bulk */\n\t\t\ts = virt_to_cache(objp);\n\t\telse\n\t\t\ts = cache_from_obj(orig_s, objp);\n\n\t\tdebug_check_no_locks_freed(objp, s->object_size);\n\t\tif (!(s->flags & SLAB_DEBUG_OBJECTS))\n\t\t\tdebug_check_no_obj_freed(objp, s->object_size);\n\n\t\t__cache_free(s, objp, _RET_IP_);\n\t}\n\tlocal_irq_enable();\n\n\t/* FIXME: add tracing */\n}\nEXPORT_SYMBOL(kmem_cache_free_bulk);\n\n/**\n * kfree - free previously allocated memory\n * @objp: pointer returned by kmalloc.\n *\n * If @objp is NULL, no operation is performed.\n *\n * Don't free memory not originally allocated by kmalloc()\n * or you will run into trouble.\n */\nvoid kfree(const void *objp)\n{\n\tstruct kmem_cache *c;\n\tunsigned long flags;\n\n\ttrace_kfree(_RET_IP_, objp);\n\n\tif (unlikely(ZERO_OR_NULL_PTR(objp)))\n\t\treturn;\n\tlocal_irq_save(flags);\n\tkfree_debugcheck(objp);\n\tc = virt_to_cache(objp);\n\tdebug_check_no_locks_freed(objp, c->object_size);\n\n\tdebug_check_no_obj_freed(objp, c->object_size);\n\t__cache_free(c, (void *)objp, _RET_IP_);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(kfree);\n\n/*\n * This initializes kmem_cache_node or resizes various caches for all nodes.\n */\nstatic int setup_kmem_cache_nodes(struct kmem_cache *cachep, gfp_t gfp)\n{\n\tint ret;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_online_node(node) {\n\t\tret = setup_kmem_cache_node(cachep, node, gfp, true);\n\t\tif (ret)\n\t\t\tgoto fail;\n\n\t}\n\n\treturn 0;\n\nfail:\n\tif (!cachep->list.next) {\n\t\t/* Cache is not active yet. Roll back what we did */\n\t\tnode--;\n\t\twhile (node >= 0) {\n\t\t\tn = get_node(cachep, node);\n\t\t\tif (n) {\n\t\t\t\tkfree(n->shared);\n\t\t\t\tfree_alien_cache(n->alien);\n\t\t\t\tkfree(n);\n\t\t\t\tcachep->node[node] = NULL;\n\t\t\t}\n\t\t\tnode--;\n\t\t}\n\t}\n\treturn -ENOMEM;\n}\n\n/* Always called with the slab_mutex held */\nstatic int __do_tune_cpucache(struct kmem_cache *cachep, int limit,\n\t\t\t\tint batchcount, int shared, gfp_t gfp)\n{\n\tstruct array_cache __percpu *cpu_cache, *prev;\n\tint cpu;\n\n\tcpu_cache = alloc_kmem_cache_cpus(cachep, limit, batchcount);\n\tif (!cpu_cache)\n\t\treturn -ENOMEM;\n\n\tprev = cachep->cpu_cache;\n\tcachep->cpu_cache = cpu_cache;\n\tkick_all_cpus_sync();\n\n\tcheck_irq_on();\n\tcachep->batchcount = batchcount;\n\tcachep->limit = limit;\n\tcachep->shared = shared;\n\n\tif (!prev)\n\t\tgoto setup_node;\n\n\tfor_each_online_cpu(cpu) {\n\t\tLIST_HEAD(list);\n\t\tint node;\n\t\tstruct kmem_cache_node *n;\n\t\tstruct array_cache *ac = per_cpu_ptr(prev, cpu);\n\n\t\tnode = cpu_to_mem(cpu);\n\t\tn = get_node(cachep, node);\n\t\tspin_lock_irq(&n->list_lock);\n\t\tfree_block(cachep, ac->entry, ac->avail, node, &list);\n\t\tspin_unlock_irq(&n->list_lock);\n\t\tslabs_destroy(cachep, &list);\n\t}\n\tfree_percpu(prev);\n\nsetup_node:\n\treturn setup_kmem_cache_nodes(cachep, gfp);\n}\n\nstatic int do_tune_cpucache(struct kmem_cache *cachep, int limit,\n\t\t\t\tint batchcount, int shared, gfp_t gfp)\n{\n\tint ret;\n\tstruct kmem_cache *c;\n\n\tret = __do_tune_cpucache(cachep, limit, batchcount, shared, gfp);\n\n\tif (slab_state < FULL)\n\t\treturn ret;\n\n\tif ((ret < 0) || !is_root_cache(cachep))\n\t\treturn ret;\n\n\tlockdep_assert_held(&slab_mutex);\n\tfor_each_memcg_cache(c, cachep) {\n\t\t/* return value determined by the root cache only */\n\t\t__do_tune_cpucache(c, limit, batchcount, shared, gfp);\n\t}\n\n\treturn ret;\n}\n\n/* Called with slab_mutex held always */\nstatic int enable_cpucache(struct kmem_cache *cachep, gfp_t gfp)\n{\n\tint err;\n\tint limit = 0;\n\tint shared = 0;\n\tint batchcount = 0;\n\n\terr = cache_random_seq_create(cachep, cachep->num, gfp);\n\tif (err)\n\t\tgoto end;\n\n\tif (!is_root_cache(cachep)) {\n\t\tstruct kmem_cache *root = memcg_root_cache(cachep);\n\t\tlimit = root->limit;\n\t\tshared = root->shared;\n\t\tbatchcount = root->batchcount;\n\t}\n\n\tif (limit && shared && batchcount)\n\t\tgoto skip_setup;\n\t/*\n\t * The head array serves three purposes:\n\t * - create a LIFO ordering, i.e. return objects that are cache-warm\n\t * - reduce the number of spinlock operations.\n\t * - reduce the number of linked list operations on the slab and\n\t *   bufctl chains: array operations are cheaper.\n\t * The numbers are guessed, we should auto-tune as described by\n\t * Bonwick.\n\t */\n\tif (cachep->size > 131072)\n\t\tlimit = 1;\n\telse if (cachep->size > PAGE_SIZE)\n\t\tlimit = 8;\n\telse if (cachep->size > 1024)\n\t\tlimit = 24;\n\telse if (cachep->size > 256)\n\t\tlimit = 54;\n\telse\n\t\tlimit = 120;\n\n\t/*\n\t * CPU bound tasks (e.g. network routing) can exhibit cpu bound\n\t * allocation behaviour: Most allocs on one cpu, most free operations\n\t * on another cpu. For these cases, an efficient object passing between\n\t * cpus is necessary. This is provided by a shared array. The array\n\t * replaces Bonwick's magazine layer.\n\t * On uniprocessor, it's functionally equivalent (but less efficient)\n\t * to a larger limit. Thus disabled by default.\n\t */\n\tshared = 0;\n\tif (cachep->size <= PAGE_SIZE && num_possible_cpus() > 1)\n\t\tshared = 8;\n\n#if DEBUG\n\t/*\n\t * With debugging enabled, large batchcount lead to excessively long\n\t * periods with disabled local interrupts. Limit the batchcount\n\t */\n\tif (limit > 32)\n\t\tlimit = 32;\n#endif\n\tbatchcount = (limit + 1) / 2;\nskip_setup:\n\terr = do_tune_cpucache(cachep, limit, batchcount, shared, gfp);\nend:\n\tif (err)\n\t\tpr_err(\"enable_cpucache failed for %s, error %d\\n\",\n\t\t       cachep->name, -err);\n\treturn err;\n}\n\n/*\n * Drain an array if it contains any elements taking the node lock only if\n * necessary. Note that the node listlock also protects the array_cache\n * if drain_array() is used on the shared array.\n */\nstatic void drain_array(struct kmem_cache *cachep, struct kmem_cache_node *n,\n\t\t\t struct array_cache *ac, int node)\n{\n\tLIST_HEAD(list);\n\n\t/* ac from n->shared can be freed if we don't hold the slab_mutex. */\n\tcheck_mutex_acquired();\n\n\tif (!ac || !ac->avail)\n\t\treturn;\n\n\tif (ac->touched) {\n\t\tac->touched = 0;\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&n->list_lock);\n\tdrain_array_locked(cachep, ac, node, false, &list);\n\tspin_unlock_irq(&n->list_lock);\n\n\tslabs_destroy(cachep, &list);\n}\n\n/**\n * cache_reap - Reclaim memory from caches.\n * @w: work descriptor\n *\n * Called from workqueue/eventd every few seconds.\n * Purpose:\n * - clear the per-cpu caches for this CPU.\n * - return freeable pages to the main free memory pool.\n *\n * If we cannot acquire the cache chain mutex then just give up - we'll try\n * again on the next iteration.\n */\nstatic void cache_reap(struct work_struct *w)\n{\n\tstruct kmem_cache *searchp;\n\tstruct kmem_cache_node *n;\n\tint node = numa_mem_id();\n\tstruct delayed_work *work = to_delayed_work(w);\n\n\tif (!mutex_trylock(&slab_mutex))\n\t\t/* Give up. Setup the next iteration. */\n\t\tgoto out;\n\n\tlist_for_each_entry(searchp, &slab_caches, list) {\n\t\tcheck_irq_on();\n\n\t\t/*\n\t\t * We only take the node lock if absolutely necessary and we\n\t\t * have established with reasonable certainty that\n\t\t * we can do some work if the lock was obtained.\n\t\t */\n\t\tn = get_node(searchp, node);\n\n\t\treap_alien(searchp, n);\n\n\t\tdrain_array(searchp, n, cpu_cache_get(searchp), node);\n\n\t\t/*\n\t\t * These are racy checks but it does not matter\n\t\t * if we skip one check or scan twice.\n\t\t */\n\t\tif (time_after(n->next_reap, jiffies))\n\t\t\tgoto next;\n\n\t\tn->next_reap = jiffies + REAPTIMEOUT_NODE;\n\n\t\tdrain_array(searchp, n, n->shared, node);\n\n\t\tif (n->free_touched)\n\t\t\tn->free_touched = 0;\n\t\telse {\n\t\t\tint freed;\n\n\t\t\tfreed = drain_freelist(searchp, n, (n->free_limit +\n\t\t\t\t5 * searchp->num - 1) / (5 * searchp->num));\n\t\t\tSTATS_ADD_REAPED(searchp, freed);\n\t\t}\nnext:\n\t\tcond_resched();\n\t}\n\tcheck_irq_on();\n\tmutex_unlock(&slab_mutex);\n\tnext_reap_node();\nout:\n\t/* Set up the next iteration */\n\tschedule_delayed_work(work, round_jiffies_relative(REAPTIMEOUT_AC));\n}\n\n#ifdef CONFIG_SLABINFO\nvoid get_slabinfo(struct kmem_cache *cachep, struct slabinfo *sinfo)\n{\n\tunsigned long active_objs, num_objs, active_slabs;\n\tunsigned long total_slabs = 0, free_objs = 0, shared_avail = 0;\n\tunsigned long free_slabs = 0;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tcheck_irq_on();\n\t\tspin_lock_irq(&n->list_lock);\n\n\t\ttotal_slabs += n->total_slabs;\n\t\tfree_slabs += n->free_slabs;\n\t\tfree_objs += n->free_objects;\n\n\t\tif (n->shared)\n\t\t\tshared_avail += n->shared->avail;\n\n\t\tspin_unlock_irq(&n->list_lock);\n\t}\n\tnum_objs = total_slabs * cachep->num;\n\tactive_slabs = total_slabs - free_slabs;\n\tactive_objs = num_objs - free_objs;\n\n\tsinfo->active_objs = active_objs;\n\tsinfo->num_objs = num_objs;\n\tsinfo->active_slabs = active_slabs;\n\tsinfo->num_slabs = total_slabs;\n\tsinfo->shared_avail = shared_avail;\n\tsinfo->limit = cachep->limit;\n\tsinfo->batchcount = cachep->batchcount;\n\tsinfo->shared = cachep->shared;\n\tsinfo->objects_per_slab = cachep->num;\n\tsinfo->cache_order = cachep->gfporder;\n}\n\nvoid slabinfo_show_stats(struct seq_file *m, struct kmem_cache *cachep)\n{\n#if STATS\n\t{\t\t\t/* node stats */\n\t\tunsigned long high = cachep->high_mark;\n\t\tunsigned long allocs = cachep->num_allocations;\n\t\tunsigned long grown = cachep->grown;\n\t\tunsigned long reaped = cachep->reaped;\n\t\tunsigned long errors = cachep->errors;\n\t\tunsigned long max_freeable = cachep->max_freeable;\n\t\tunsigned long node_allocs = cachep->node_allocs;\n\t\tunsigned long node_frees = cachep->node_frees;\n\t\tunsigned long overflows = cachep->node_overflow;\n\n\t\tseq_printf(m, \" : globalstat %7lu %6lu %5lu %4lu %4lu %4lu %4lu %4lu %4lu\",\n\t\t\t   allocs, high, grown,\n\t\t\t   reaped, errors, max_freeable, node_allocs,\n\t\t\t   node_frees, overflows);\n\t}\n\t/* cpu stats */\n\t{\n\t\tunsigned long allochit = atomic_read(&cachep->allochit);\n\t\tunsigned long allocmiss = atomic_read(&cachep->allocmiss);\n\t\tunsigned long freehit = atomic_read(&cachep->freehit);\n\t\tunsigned long freemiss = atomic_read(&cachep->freemiss);\n\n\t\tseq_printf(m, \" : cpustat %6lu %6lu %6lu %6lu\",\n\t\t\t   allochit, allocmiss, freehit, freemiss);\n\t}\n#endif\n}\n\n#define MAX_SLABINFO_WRITE 128\n/**\n * slabinfo_write - Tuning for the slab allocator\n * @file: unused\n * @buffer: user buffer\n * @count: data length\n * @ppos: unused\n */\nssize_t slabinfo_write(struct file *file, const char __user *buffer,\n\t\t       size_t count, loff_t *ppos)\n{\n\tchar kbuf[MAX_SLABINFO_WRITE + 1], *tmp;\n\tint limit, batchcount, shared, res;\n\tstruct kmem_cache *cachep;\n\n\tif (count > MAX_SLABINFO_WRITE)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&kbuf, buffer, count))\n\t\treturn -EFAULT;\n\tkbuf[MAX_SLABINFO_WRITE] = '\\0';\n\n\ttmp = strchr(kbuf, ' ');\n\tif (!tmp)\n\t\treturn -EINVAL;\n\t*tmp = '\\0';\n\ttmp++;\n\tif (sscanf(tmp, \" %d %d %d\", &limit, &batchcount, &shared) != 3)\n\t\treturn -EINVAL;\n\n\t/* Find the cache in the chain of caches. */\n\tmutex_lock(&slab_mutex);\n\tres = -EINVAL;\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tif (!strcmp(cachep->name, kbuf)) {\n\t\t\tif (limit < 1 || batchcount < 1 ||\n\t\t\t\t\tbatchcount > limit || shared < 0) {\n\t\t\t\tres = 0;\n\t\t\t} else {\n\t\t\t\tres = do_tune_cpucache(cachep, limit,\n\t\t\t\t\t\t       batchcount, shared,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&slab_mutex);\n\tif (res >= 0)\n\t\tres = count;\n\treturn res;\n}\n\n#ifdef CONFIG_DEBUG_SLAB_LEAK\n\nstatic inline int add_caller(unsigned long *n, unsigned long v)\n{\n\tunsigned long *p;\n\tint l;\n\tif (!v)\n\t\treturn 1;\n\tl = n[1];\n\tp = n + 2;\n\twhile (l) {\n\t\tint i = l/2;\n\t\tunsigned long *q = p + 2 * i;\n\t\tif (*q == v) {\n\t\t\tq[1]++;\n\t\t\treturn 1;\n\t\t}\n\t\tif (*q > v) {\n\t\t\tl = i;\n\t\t} else {\n\t\t\tp = q + 2;\n\t\t\tl -= i + 1;\n\t\t}\n\t}\n\tif (++n[1] == n[0])\n\t\treturn 0;\n\tmemmove(p + 2, p, n[1] * 2 * sizeof(unsigned long) - ((void *)p - (void *)n));\n\tp[0] = v;\n\tp[1] = 1;\n\treturn 1;\n}\n\nstatic void handle_slab(unsigned long *n, struct kmem_cache *c,\n\t\t\t\t\t\tstruct page *page)\n{\n\tvoid *p;\n\tint i, j;\n\tunsigned long v;\n\n\tif (n[0] == n[1])\n\t\treturn;\n\tfor (i = 0, p = page->s_mem; i < c->num; i++, p += c->size) {\n\t\tbool active = true;\n\n\t\tfor (j = page->active; j < c->num; j++) {\n\t\t\tif (get_free_obj(page, j) == i) {\n\t\t\t\tactive = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!active)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * probe_kernel_read() is used for DEBUG_PAGEALLOC. page table\n\t\t * mapping is established when actual object allocation and\n\t\t * we could mistakenly access the unmapped object in the cpu\n\t\t * cache.\n\t\t */\n\t\tif (probe_kernel_read(&v, dbg_userword(c, p), sizeof(v)))\n\t\t\tcontinue;\n\n\t\tif (!add_caller(n, v))\n\t\t\treturn;\n\t}\n}\n\nstatic void show_symbol(struct seq_file *m, unsigned long address)\n{\n#ifdef CONFIG_KALLSYMS\n\tunsigned long offset, size;\n\tchar modname[MODULE_NAME_LEN], name[KSYM_NAME_LEN];\n\n\tif (lookup_symbol_attrs(address, &size, &offset, modname, name) == 0) {\n\t\tseq_printf(m, \"%s+%#lx/%#lx\", name, offset, size);\n\t\tif (modname[0])\n\t\t\tseq_printf(m, \" [%s]\", modname);\n\t\treturn;\n\t}\n#endif\n\tseq_printf(m, \"%p\", (void *)address);\n}\n\nstatic int leaks_show(struct seq_file *m, void *p)\n{\n\tstruct kmem_cache *cachep = list_entry(p, struct kmem_cache, list);\n\tstruct page *page;\n\tstruct kmem_cache_node *n;\n\tconst char *name;\n\tunsigned long *x = m->private;\n\tint node;\n\tint i;\n\n\tif (!(cachep->flags & SLAB_STORE_USER))\n\t\treturn 0;\n\tif (!(cachep->flags & SLAB_RED_ZONE))\n\t\treturn 0;\n\n\t/*\n\t * Set store_user_clean and start to grab stored user information\n\t * for all objects on this cache. If some alloc/free requests comes\n\t * during the processing, information would be wrong so restart\n\t * whole processing.\n\t */\n\tdo {\n\t\tset_store_user_clean(cachep);\n\t\tdrain_cpu_caches(cachep);\n\n\t\tx[1] = 0;\n\n\t\tfor_each_kmem_cache_node(cachep, node, n) {\n\n\t\t\tcheck_irq_on();\n\t\t\tspin_lock_irq(&n->list_lock);\n\n\t\t\tlist_for_each_entry(page, &n->slabs_full, lru)\n\t\t\t\thandle_slab(x, cachep, page);\n\t\t\tlist_for_each_entry(page, &n->slabs_partial, lru)\n\t\t\t\thandle_slab(x, cachep, page);\n\t\t\tspin_unlock_irq(&n->list_lock);\n\t\t}\n\t} while (!is_store_user_clean(cachep));\n\n\tname = cachep->name;\n\tif (x[0] == x[1]) {\n\t\t/* Increase the buffer size */\n\t\tmutex_unlock(&slab_mutex);\n\t\tm->private = kzalloc(x[0] * 4 * sizeof(unsigned long), GFP_KERNEL);\n\t\tif (!m->private) {\n\t\t\t/* Too bad, we are really out */\n\t\t\tm->private = x;\n\t\t\tmutex_lock(&slab_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\t*(unsigned long *)m->private = x[0] * 2;\n\t\tkfree(x);\n\t\tmutex_lock(&slab_mutex);\n\t\t/* Now make sure this entry will be retried */\n\t\tm->count = m->size;\n\t\treturn 0;\n\t}\n\tfor (i = 0; i < x[1]; i++) {\n\t\tseq_printf(m, \"%s: %lu \", name, x[2*i+3]);\n\t\tshow_symbol(m, x[2*i+2]);\n\t\tseq_putc(m, '\\n');\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations slabstats_op = {\n\t.start = slab_start,\n\t.next = slab_next,\n\t.stop = slab_stop,\n\t.show = leaks_show,\n};\n\nstatic int slabstats_open(struct inode *inode, struct file *file)\n{\n\tunsigned long *n;\n\n\tn = __seq_open_private(file, &slabstats_op, PAGE_SIZE);\n\tif (!n)\n\t\treturn -ENOMEM;\n\n\t*n = PAGE_SIZE / (2 * sizeof(unsigned long));\n\n\treturn 0;\n}\n\nstatic const struct file_operations proc_slabstats_operations = {\n\t.open\t\t= slabstats_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_private,\n};\n#endif\n\nstatic int __init slab_proc_init(void)\n{\n#ifdef CONFIG_DEBUG_SLAB_LEAK\n\tproc_create(\"slab_allocators\", 0, NULL, &proc_slabstats_operations);\n#endif\n\treturn 0;\n}\nmodule_init(slab_proc_init);\n#endif\n\n#ifdef CONFIG_HARDENED_USERCOPY\n/*\n * Rejects objects that are incorrectly sized.\n *\n * Returns NULL if check passes, otherwise const char * to name of cache\n * to indicate an error.\n */\nconst char *__check_heap_object(const void *ptr, unsigned long n,\n\t\t\t\tstruct page *page)\n{\n\tstruct kmem_cache *cachep;\n\tunsigned int objnr;\n\tunsigned long offset;\n\n\t/* Find and validate object. */\n\tcachep = page->slab_cache;\n\tobjnr = obj_to_index(cachep, page, (void *)ptr);\n\tBUG_ON(objnr >= cachep->num);\n\n\t/* Find offset within object. */\n\toffset = ptr - index_to_obj(cachep, page, objnr) - obj_offset(cachep);\n\n\t/* Allow address range falling entirely within object size. */\n\tif (offset <= cachep->object_size && n <= cachep->object_size - offset)\n\t\treturn NULL;\n\n\treturn cachep->name;\n}\n#endif /* CONFIG_HARDENED_USERCOPY */\n\n/**\n * ksize - get the actual amount of memory allocated for a given object\n * @objp: Pointer to the object\n *\n * kmalloc may internally round up allocations and return more memory\n * than requested. ksize() can be used to determine the actual amount of\n * memory allocated. The caller may use this additional memory, even though\n * a smaller amount of memory was initially specified with the kmalloc call.\n * The caller must guarantee that objp points to a valid object previously\n * allocated with either kmalloc() or kmem_cache_alloc(). The object\n * must not be freed during the duration of the call.\n */\nsize_t ksize(const void *objp)\n{\n\tsize_t size;\n\n\tBUG_ON(!objp);\n\tif (unlikely(objp == ZERO_SIZE_PTR))\n\t\treturn 0;\n\n\tsize = virt_to_cache(objp)->object_size;\n\t/* We assume that ksize callers could use the whole allocated area,\n\t * so we need to unpoison this area.\n\t */\n\tkasan_unpoison_shadow(objp, size);\n\n\treturn size;\n}\nEXPORT_SYMBOL(ksize);\n"], "filenames": ["mm/slab.c"], "buggy_code_start_loc": [2460], "buggy_code_end_loc": [2497], "fixing_code_start_loc": [2459], "fixing_code_end_loc": [2497], "type": "NVD-CWE-noinfo", "message": "The freelist-randomization feature in mm/slab.c in the Linux kernel 4.8.x and 4.9.x before 4.9.5 allows local users to cause a denial of service (duplicate freelist entries and system crash) or possibly have unspecified other impact in opportunistic circumstances by leveraging the selection of a large value for a random number.", "other": {"cve": {"id": "CVE-2017-5546", "sourceIdentifier": "cve@mitre.org", "published": "2017-02-06T06:59:00.480", "lastModified": "2023-02-28T15:45:44.103", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The freelist-randomization feature in mm/slab.c in the Linux kernel 4.8.x and 4.9.x before 4.9.5 allows local users to cause a denial of service (duplicate freelist entries and system crash) or possibly have unspecified other impact in opportunistic circumstances by leveraging the selection of a large value for a random number."}, {"lang": "es", "value": "La caracter\u00edstica de freelist-randomization en mm/slab.c en el kernel 4.8.x de Linux y 4.9.x en versiones anteriores a 4.9.5 permite a usuarios locales provocar una denegaci\u00f3n de servicio (entradas freelist duplicadas y ca\u00edda del sistema) o posiblemente tener otro impacto no especificado en circunstancias oportunistas aprovechando la selecci\u00f3n de un valor grande para un n\u00famero aleatorio."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.7", "versionEndExcluding": "4.9.5", "matchCriteriaId": "E7C80006-A48A-4709-BBED-83D2F1411141"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=c4e490cf148e85ead0d1b1c2caaba833f1d5b29f", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.9.5", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2017/01/21/3", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/95711", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1415733", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/c4e490cf148e85ead0d1b1c2caaba833f1d5b29f", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/c4e490cf148e85ead0d1b1c2caaba833f1d5b29f"}}