{"buggy_code": [";; Instruction formats.\n(type MInst\n      (enum\n       ;; A no-op of zero size.\n       (Nop0)\n\n       ;; A no-op that is one instruction large.\n       (Nop4)\n\n       ;; An ALU operation with two register sources and a register destination.\n       (AluRRR\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg))\n\n       ;; An ALU operation with three register sources and a register destination.\n       (AluRRRR\n        (alu_op ALUOp3)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (ra Reg))\n\n       ;; An ALU operation with a register source and an immediate-12 source, and a register\n       ;; destination.\n       (AluRRImm12\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (imm12 Imm12))\n\n       ;; An ALU operation with a register source and an immediate-logic source, and a register destination.\n       (AluRRImmLogic\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (imml ImmLogic))\n\n       ;; An ALU operation with a register source and an immediate-shiftamt source, and a register destination.\n       (AluRRImmShift\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (immshift ImmShift))\n\n       ;; An ALU operation with two register sources, one of which can be shifted, and a register\n       ;; destination.\n       (AluRRRShift\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (shiftop ShiftOpAndAmt))\n\n       ;; An ALU operation with two register sources, one of which can be {zero,sign}-extended and\n       ;; shifted, and a register destination.\n       (AluRRRExtend\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (extendop ExtendOp))\n\n       ;; A bit op instruction with a single register source.\n       (BitRR\n        (op BitOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; An unsigned (zero-extending) 8-bit load.\n       (ULoad8\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A signed (sign-extending) 8-bit load.\n       (SLoad8\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; An unsigned (zero-extending) 16-bit load.\n       (ULoad16\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A signed (sign-extending) 16-bit load.\n       (SLoad16\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; An unsigned (zero-extending) 32-bit load.\n       (ULoad32\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A signed (sign-extending) 32-bit load.\n       (SLoad32\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A 64-bit load.\n       (ULoad64\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; An 8-bit store.\n       (Store8\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A 16-bit store.\n       (Store16\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A 32-bit store.\n       (Store32\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A 64-bit store.\n       (Store64\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A store of a pair of registers.\n       (StoreP64\n        (rt Reg)\n        (rt2 Reg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       ;; A load of a pair of registers.\n       (LoadP64\n        (rt WritableReg)\n        (rt2 WritableReg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       ;; A MOV instruction. These are encoded as ORR's (AluRRR form).\n       ;; The 32-bit version zeroes the top 32 bits of the\n       ;; destination, which is effectively an alias for an unsigned\n       ;; 32-to-64-bit extension.\n       (Mov\n        (size OperandSize)\n        (rd WritableReg)\n        (rm Reg))\n\n       ;; A MOV[Z,N,K] with a 16-bit immediate.\n       (MovWide\n        (op MoveWideOp)\n        (rd WritableReg)\n        (imm MoveWideConst)\n        (size OperandSize))\n\n       ;; A sign- or zero-extend operation.\n       (Extend\n        (rd WritableReg)\n        (rn Reg)\n        (signed bool)\n        (from_bits u8)\n        (to_bits u8))\n\n       ;; A conditional-select operation.\n       (CSel\n        (rd WritableReg)\n        (cond Cond)\n        (rn Reg)\n        (rm Reg))\n\n       ;; A conditional-select negation operation.\n       (CSNeg\n        (rd WritableReg)\n        (cond Cond)\n        (rn Reg)\n        (rm Reg))\n\n       ;; A conditional-set operation.\n       (CSet\n        (rd WritableReg)\n        (cond Cond))\n\n       ;; A conditional-set-mask operation.\n       (CSetm\n        (rd WritableReg)\n        (cond Cond))\n\n       ;; A conditional comparison with an immediate.\n       (CCmpImm\n        (size OperandSize)\n        (rn Reg)\n        (imm UImm5)\n        (nzcv NZCV)\n        (cond Cond))\n\n       ;; A synthetic insn, which is a load-linked store-conditional loop, that has the overall\n       ;; effect of atomically modifying a memory location in a particular way.  Because we have\n       ;; no way to explain to the regalloc about earlyclobber registers, this instruction has\n       ;; completely fixed operand registers, and we rely on the RA's coalescing to remove copies\n       ;; in the surrounding code to the extent it can. Load- and store-exclusive instructions,\n       ;; with acquire-release semantics, are used to access memory. The operand conventions are:\n       ;;\n       ;; x25   (rd) address\n       ;; x26   (rd) second operand for `op`\n       ;; x27   (wr) old value\n       ;; x24   (wr) scratch reg; value afterwards has no meaning\n       ;; x28   (wr) scratch reg; value afterwards has no meaning\n       (AtomicRMWLoop\n        (ty Type) ;; I8, I16, I32 or I64\n        (op AtomicRMWLoopOp))\n\n       ;; Similar to AtomicRMWLoop, a compare-and-swap operation implemented using a load-linked\n       ;; store-conditional loop, with acquire-release semantics.\n       ;; Note that the operand conventions, although very similar to AtomicRMWLoop, are different:\n       ;;\n       ;; x25   (rd) address\n       ;; x26   (rd) expected value\n       ;; x28   (rd) replacement value\n       ;; x27   (wr) old value\n       ;; x24   (wr) scratch reg; value afterwards has no meaning\n       (AtomicCASLoop\n        (ty Type) ;; I8, I16, I32 or I64\n        )\n\n       ;; An atomic read-modify-write operation. These instructions require the\n       ;; Large System Extension (LSE) ISA support (FEAT_LSE). The instructions have\n       ;; acquire-release semantics.\n       (AtomicRMW\n         (op AtomicRMWOp)\n         (rs Reg)\n         (rt WritableReg)\n         (rn Reg)\n         (ty Type))\n\n       ;; An atomic compare-and-swap operation. These instructions require the\n       ;; Large System Extension (LSE) ISA support (FEAT_LSE). The instructions have\n       ;; acquire-release semantics.\n       (AtomicCAS\n         (rs WritableReg)\n         (rt Reg)\n         (rn Reg)\n         (ty Type))\n\n       ;; Read `access_ty` bits from address `rt`, either 8, 16, 32 or 64-bits, and put\n       ;; it in `rn`, optionally zero-extending to fill a word or double word result.\n       ;; This instruction is sequentially consistent.\n       (LoadAcquire\n        (access_ty Type) ;; I8, I16, I32 or I64\n        (rt WritableReg)\n        (rn Reg))\n\n       ;; Write the lowest `ty` bits of `rt` to address `rn`.\n       ;; This instruction is sequentially consistent.\n       (StoreRelease\n        (access_ty Type) ;; I8, I16, I32 or I64\n        (rt Reg)\n        (rn Reg))\n\n       ;; A memory fence.  This must provide ordering to ensure that, at a minimum, neither loads\n       ;; nor stores may move forwards or backwards across the fence.  Currently emitted as \"dmb\n       ;; ish\".  This instruction is sequentially consistent.\n       (Fence)\n\n       ;; FPU move. Note that this is distinct from a vector-register\n       ;; move; moving just 64 bits seems to be significantly faster.\n       (FpuMove64\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; Vector register move.\n       (FpuMove128\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; Move to scalar from a vector element.\n       (FpuMoveFromVec\n        (rd WritableReg)\n        (rn Reg)\n        (idx u8)\n        (size VectorSize))\n\n       ;; Zero-extend a SIMD & FP scalar to the full width of a vector register.\n       ;; 16-bit scalars require half-precision floating-point support (FEAT_FP16).\n       (FpuExtend\n        (rd WritableReg)\n        (rn Reg)\n        (size ScalarSize))\n\n       ;; 1-op FPU instruction.\n       (FpuRR\n        (fpu_op FPUOp1)\n        (size ScalarSize)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; 2-op FPU instruction.\n       (FpuRRR\n        (fpu_op FPUOp2)\n        (size ScalarSize)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg))\n\n       (FpuRRI\n        (fpu_op FPUOpRI)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; 3-op FPU instruction.\n       (FpuRRRR\n        (fpu_op FPUOp3)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (ra Reg))\n\n       ;; FPU comparison.\n       (FpuCmp\n        (size ScalarSize)\n        (rn Reg)\n        (rm Reg))\n\n       ;; Floating-point load, single-precision (32 bit).\n       (FpuLoad32\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; Floating-point store, single-precision (32 bit).\n       (FpuStore32\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; Floating-point load, double-precision (64 bit).\n       (FpuLoad64\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; Floating-point store, double-precision (64 bit).\n       (FpuStore64\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; Floating-point/vector load, 128 bit.\n       (FpuLoad128\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; Floating-point/vector store, 128 bit.\n       (FpuStore128\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A load of a pair of floating-point registers, double precision (64-bit).\n       (FpuLoadP64\n        (rt WritableReg)\n        (rt2 WritableReg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       ;; A store of a pair of floating-point registers, double precision (64-bit).\n       (FpuStoreP64\n        (rt Reg)\n        (rt2 Reg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       ;; A load of a pair of floating-point registers, 128-bit.\n       (FpuLoadP128\n        (rt WritableReg)\n        (rt2 WritableReg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       ;; A store of a pair of floating-point registers, 128-bit.\n       (FpuStoreP128\n        (rt Reg)\n        (rt2 Reg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       (LoadFpuConst64\n        (rd WritableReg)\n        (const_data u64))\n\n       (LoadFpuConst128\n        (rd WritableReg)\n        (const_data u128))\n\n       ;; Conversion: FP -> integer.\n       (FpuToInt\n        (op FpuToIntOp)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; Conversion: integer -> FP.\n       (IntToFpu\n        (op IntToFpuOp)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; FP conditional select, 32 bit.\n       (FpuCSel32\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (cond Cond))\n\n       ;; FP conditional select, 64 bit.\n       (FpuCSel64\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (cond Cond))\n\n       ;; Round to integer.\n       (FpuRound\n        (op FpuRoundMode)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; Move from a GPR to a vector register.  The scalar value is parked in the lowest lane\n       ;; of the destination, and all other lanes are zeroed out.  Currently only 32- and 64-bit\n       ;; transactions are supported.\n       (MovToFpu\n        (rd WritableReg)\n        (rn Reg)\n        (size ScalarSize))\n\n       ;; Loads a floating-point immediate.\n       (FpuMoveFPImm\n        (rd WritableReg)\n        (imm ASIMDFPModImm)\n        (size ScalarSize))\n\n       ;; Move to a vector element from a GPR.\n       (MovToVec\n        (rd WritableReg)\n        (rn Reg)\n        (idx u8)\n        (size VectorSize))\n\n       ;; Unsigned move from a vector element to a GPR.\n       (MovFromVec\n        (rd WritableReg)\n        (rn Reg)\n        (idx u8)\n        (size VectorSize))\n\n       ;; Signed move from a vector element to a GPR.\n       (MovFromVecSigned\n        (rd WritableReg)\n        (rn Reg)\n        (idx u8)\n        (size VectorSize)\n        (scalar_size OperandSize))\n\n       ;; Duplicate general-purpose register to vector.\n       (VecDup\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize))\n\n       ;; Duplicate scalar to vector.\n       (VecDupFromFpu\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize))\n\n       ;; Duplicate FP immediate to vector.\n       (VecDupFPImm\n        (rd WritableReg)\n        (imm ASIMDFPModImm)\n        (size VectorSize))\n\n       ;; Duplicate immediate to vector.\n       (VecDupImm\n        (rd WritableReg)\n        (imm ASIMDMovModImm)\n        (invert bool)\n        (size VectorSize))\n\n       ;; Vector extend.\n       (VecExtend\n        (t VecExtendOp)\n        (rd WritableReg)\n        (rn Reg)\n        (high_half bool))\n\n       ;; Move vector element to another vector element.\n       (VecMovElement\n        (rd WritableReg)\n        (rn Reg)\n        (dest_idx u8)\n        (src_idx u8)\n        (size VectorSize))\n\n       ;; Vector widening operation.\n       (VecRRLong\n        (op VecRRLongOp)\n        (rd WritableReg)\n        (rn Reg)\n        (high_half bool))\n\n       ;; Vector narrowing operation.\n       (VecRRNarrow\n        (op VecRRNarrowOp)\n        (rd WritableReg)\n        (rn Reg)\n        (high_half bool))\n\n       ;; 1-operand vector instruction that operates on a pair of elements.\n       (VecRRPair\n        (op VecPairOp)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; 2-operand vector instruction that produces a result with twice the\n       ;; lane width and half the number of lanes.\n       (VecRRRLong\n        (alu_op VecRRRLongOp)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (high_half bool))\n\n       ;; 1-operand vector instruction that extends elements of the input\n       ;; register and operates on a pair of elements. The output lane width\n       ;; is double that of the input.\n       (VecRRPairLong\n        (op VecRRPairLongOp)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; A vector ALU op.\n       (VecRRR\n        (alu_op VecALUOp)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (size VectorSize))\n\n       ;; Vector two register miscellaneous instruction.\n       (VecMisc\n        (op VecMisc2)\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize))\n\n       ;; Vector instruction across lanes.\n       (VecLanes\n        (op VecLanesOp)\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize))\n\n       ;; Vector shift by immediate Shift Left (immediate), Unsigned Shift Right (immediate)\n       ;; Signed Shift Right (immediate).  These are somewhat unusual in that, for right shifts,\n       ;; the allowed range of `imm` values is 1 to lane-size-in-bits, inclusive.  A zero\n       ;; right-shift cannot be encoded.  Left shifts are \"normal\", though, having valid `imm`\n       ;; values from 0 to lane-size-in-bits - 1 inclusive.\n       (VecShiftImm\n        (op VecShiftImmOp)\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize)\n        (imm u8))\n\n       ;; Vector extract - create a new vector, being the concatenation of the lowest `imm4` bytes\n       ;; of `rm` followed by the uppermost `16 - imm4` bytes of `rn`.\n       (VecExtract\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (imm4 u8))\n\n       ;; Table vector lookup - single register table. The table consists of 8-bit elements and is\n       ;; stored in `rn`, while `rm` contains 8-bit element indices. `is_extension` specifies whether\n       ;; to emit a TBX or a TBL instruction, i.e. whether to leave the elements in the destination\n       ;; vector that correspond to out-of-range indices (greater than 15) unmodified or to set them\n       ;; to 0.\n       (VecTbl\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (is_extension bool))\n\n       ;; Table vector lookup - two register table. The table consists of 8-bit elements and is\n       ;; stored in `rn` and `rn2`, while `rm` contains 8-bit element indices. `is_extension`\n       ;; specifies whether to emit a TBX or a TBL instruction, i.e. whether to leave the elements in\n       ;; the destination vector that correspond to out-of-range indices (greater than 31) unmodified\n       ;; or to set them to 0. The table registers `rn` and `rn2` must have consecutive numbers\n       ;; modulo 32, that is v31 and v0 (in that order) are consecutive registers.\n       (VecTbl2\n        (rd WritableReg)\n        (rn Reg)\n        (rn2 Reg)\n        (rm Reg)\n        (is_extension bool))\n\n       ;; Load an element and replicate to all lanes of a vector.\n       (VecLoadReplicate\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize))\n\n       ;; Vector conditional select, 128 bit.  A synthetic instruction, which generates a 4-insn\n       ;; control-flow diamond.\n       (VecCSel\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (cond Cond))\n\n       ;; Move to the NZCV flags (actually a `MSR NZCV, Xn` insn).\n       (MovToNZCV\n        (rn Reg))\n\n       ;; Move from the NZCV flags (actually a `MRS Xn, NZCV` insn).\n       (MovFromNZCV\n        (rd WritableReg))\n\n       ;; A machine call instruction. N.B.: this allows only a +/- 128MB offset (it uses a relocation\n       ;; of type `Reloc::Arm64Call`); if the destination distance is not `RelocDistance::Near`, the\n       ;; code should use a `LoadExtName` / `CallInd` sequence instead, allowing an arbitrary 64-bit\n       ;; target.\n       (Call\n        (info BoxCallInfo))\n\n       ;; A machine indirect-call instruction.\n       (CallInd\n        (info BoxCallIndInfo))\n\n       ;; ---- branches (exactly one must appear at end of BB) ----\n\n       ;; A machine return instruction.\n       (Ret\n        (rets VecReg))\n\n       ;; A placeholder instruction, generating no code, meaning that a function epilogue must be\n       ;; inserted there.\n       (EpiloguePlaceholder)\n\n       ;; An unconditional branch.\n       (Jump\n        (dest BranchTarget))\n\n       ;; A conditional branch. Contains two targets; at emission time, both are emitted, but\n       ;; the MachBuffer knows to truncate the trailing branch if fallthrough. We optimize the\n       ;; choice of taken/not_taken (inverting the branch polarity as needed) based on the\n       ;; fallthrough at the time of lowering.\n       (CondBr\n        (taken BranchTarget)\n        (not_taken BranchTarget)\n        (kind CondBrKind))\n\n       ;; A conditional trap: execute a `udf` if the condition is true. This is\n       ;; one VCode instruction because it uses embedded control flow; it is\n       ;; logically a single-in, single-out region, but needs to appear as one\n       ;; unit to the register allocator.\n       ;;\n       ;; The `CondBrKind` gives the conditional-branch condition that will\n       ;; *execute* the embedded `Inst`. (In the emitted code, we use the inverse\n       ;; of this condition in a branch that skips the trap instruction.)\n       (TrapIf\n        (kind CondBrKind)\n        (trap_code TrapCode))\n\n       ;; An indirect branch through a register, augmented with set of all\n       ;; possible successors.\n       (IndirectBr\n        (rn Reg)\n        (targets VecMachLabel))\n\n       ;; A \"break\" instruction, used for e.g. traps and debug breakpoints.\n       (Brk)\n\n       ;; An instruction guaranteed to always be undefined and to trigger an illegal instruction at\n       ;; runtime.\n       (Udf\n        (use_allocated_encoding bool)\n        (trap_code TrapCode))\n\n       ;; Compute the address (using a PC-relative offset) of a memory location, using the `ADR`\n       ;; instruction. Note that we take a simple offset, not a `MemLabel`, here, because `Adr` is\n       ;; only used for now in fixed lowering sequences with hardcoded offsets. In the future we may\n       ;; need full `MemLabel` support.\n       (Adr\n        (rd WritableReg)\n        ;; Offset in range -2^20 .. 2^20.\n        (off i32))\n\n       ;; Raw 32-bit word, used for inline constants and jump-table entries.\n       (Word4\n        (data u32))\n\n       ;; Raw 64-bit word, used for inline constants.\n       (Word8\n        (data u64))\n\n       ;; Jump-table sequence, as one compound instruction (see note in lower_inst.rs for rationale).\n       (JTSequence\n        (info BoxJTSequenceInfo)\n        (ridx Reg)\n        (rtmp1 WritableReg)\n        (rtmp2 WritableReg))\n\n       ;; Load an inline symbol reference.\n       (LoadExtName\n        (rd WritableReg)\n        (name BoxExternalName)\n        (offset i64))\n\n       ;; Load address referenced by `mem` into `rd`.\n       (LoadAddr\n        (rd WritableReg)\n        (mem AMode))\n\n       ;; Marker, no-op in generated code: SP \"virtual offset\" is adjusted. This\n       ;; controls how AMode::NominalSPOffset args are lowered.\n       (VirtualSPOffsetAdj\n        (offset i64))\n\n       ;; Meta-insn, no-op in generated code: emit constant/branch veneer island\n       ;; at this point (with a guard jump around it) if less than the needed\n       ;; space is available before the next branch deadline. See the `MachBuffer`\n       ;; implementation in `machinst/buffer.rs` for the overall algorithm. In\n       ;; brief, we retain a set of \"pending/unresolved label references\" from\n       ;; branches as we scan forward through instructions to emit machine code;\n       ;; if we notice we're about to go out of range on an unresolved reference,\n       ;; we stop, emit a bunch of \"veneers\" (branches in a form that has a longer\n       ;; range, e.g. a 26-bit-offset unconditional jump), and point the original\n       ;; label references to those. This is an \"island\" because it comes in the\n       ;; middle of the code.\n       ;;\n       ;; This meta-instruction is a necessary part of the logic that determines\n       ;; where to place islands. Ordinarily, we want to place them between basic\n       ;; blocks, so we compute the worst-case size of each block, and emit the\n       ;; island before starting a block if we would exceed a deadline before the\n       ;; end of the block. However, some sequences (such as an inline jumptable)\n       ;; are variable-length and not accounted for by this logic; so these\n       ;; lowered sequences include an `EmitIsland` to trigger island generation\n       ;; where necessary.\n       (EmitIsland\n        ;; The needed space before the next deadline.\n        (needed_space CodeOffset))\n\n       ;; A call to the `ElfTlsGetAddr` libcall. Returns address of TLS symbol in x0.\n       (ElfTlsGetAddr\n        (symbol ExternalName))\n\n       ;; An unwind pseudo-instruction.\n       (Unwind\n        (inst UnwindInst))\n\n       ;; A dummy use, useful to keep a value alive.\n       (DummyUse\n        (reg Reg))))\n\n;; An ALU operation. This can be paired with several instruction formats\n;; below (see `Inst`) in any combination.\n(type ALUOp\n  (enum\n    (Add)\n    (Sub)\n    (Orr)\n    (OrrNot)\n    (And)\n    (AndS)\n    (AndNot)\n    ;; XOR (AArch64 calls this \"EOR\")\n    (Eor)\n    ;; XNOR (AArch64 calls this \"EOR-NOT\")\n    (EorNot)\n    ;; Add, setting flags\n    (AddS)\n    ;; Sub, setting flags\n    (SubS)\n    ;; Signed multiply, high-word result\n    (SMulH)\n    ;; Unsigned multiply, high-word result\n    (UMulH)\n    (SDiv)\n    (UDiv)\n    (RotR)\n    (Lsr)\n    (Asr)\n    (Lsl)\n    ;; Add with carry\n    (Adc)\n    ;; Add with carry, settings flags\n    (AdcS)\n    ;; Subtract with carry\n    (Sbc)\n    ;; Subtract with carry, settings flags\n    (SbcS)\n))\n\n;; An ALU operation with three arguments.\n(type ALUOp3\n  (enum\n    ;; Multiply-add\n    (MAdd)\n    ;; Multiply-sub\n    (MSub)\n))\n\n(type MoveWideOp\n  (enum\n    (MovZ)\n    (MovN)\n    (MovK)\n))\n\n(type UImm5 (primitive UImm5))\n(type Imm12 (primitive Imm12))\n(type ImmLogic (primitive ImmLogic))\n(type ImmShift (primitive ImmShift))\n(type ShiftOpAndAmt (primitive ShiftOpAndAmt))\n(type MoveWideConst (primitive MoveWideConst))\n(type NZCV (primitive NZCV))\n(type ASIMDFPModImm (primitive ASIMDFPModImm))\n(type ASIMDMovModImm (primitive ASIMDMovModImm))\n\n(type BoxCallInfo (primitive BoxCallInfo))\n(type BoxCallIndInfo (primitive BoxCallIndInfo))\n(type CondBrKind (primitive CondBrKind))\n(type BranchTarget (primitive BranchTarget))\n(type BoxJTSequenceInfo (primitive BoxJTSequenceInfo))\n(type CodeOffset (primitive CodeOffset))\n\n(type ExtendOp extern\n  (enum\n    (UXTB)\n    (UXTH)\n    (UXTW)\n    (UXTX)\n    (SXTB)\n    (SXTH)\n    (SXTW)\n    (SXTX)\n))\n\n;; An operation on the bits of a register. This can be paired with several instruction formats\n;; below (see `Inst`) in any combination.\n(type BitOp\n  (enum\n    ;; Bit reverse\n    (RBit)\n    (Clz)\n    (Cls)\n))\n\n(type AMode extern (enum))\n(type PairAMode extern (enum))\n(type FPUOpRI extern (enum))\n\n(type OperandSize extern\n      (enum Size32\n            Size64))\n\n;; Helper for calculating the `OperandSize` corresponding to a type\n(decl operand_size (Type) OperandSize)\n(rule (operand_size (fits_in_32 _ty)) (OperandSize.Size32))\n(rule (operand_size (fits_in_64 _ty)) (OperandSize.Size64))\n\n(type ScalarSize extern\n      (enum Size8\n            Size16\n            Size32\n            Size64\n            Size128))\n\n;; Helper for calculating the `ScalarSize` corresponding to a type\n(decl scalar_size (Type) ScalarSize)\n(rule (scalar_size $I8) (ScalarSize.Size8))\n(rule (scalar_size $I16) (ScalarSize.Size16))\n(rule (scalar_size $I32) (ScalarSize.Size32))\n(rule (scalar_size $I64) (ScalarSize.Size64))\n(rule (scalar_size $I128) (ScalarSize.Size128))\n(rule (scalar_size $F32) (ScalarSize.Size32))\n(rule (scalar_size $F64) (ScalarSize.Size64))\n\n(type Cond extern\n  (enum\n    (Eq)\n    (Ne)\n    (Hs)\n    (Lo)\n    (Mi)\n    (Pl)\n    (Vs)\n    (Vc)\n    (Hi)\n    (Ls)\n    (Ge)\n    (Lt)\n    (Gt)\n    (Le)\n    (Al)\n    (Nv)\n))\n\n(type VectorSize extern\n  (enum\n    (Size8x8)\n    (Size8x16)\n    (Size16x4)\n    (Size16x8)\n    (Size32x2)\n    (Size32x4)\n    (Size64x2)\n))\n\n(type DynamicVectorSize extern\n  (enum\n    (Size8x8xN)\n    (Size8x16xN)\n    (Size16x4xN)\n    (Size16x8xN)\n    (Size32x2xN)\n    (Size32x4xN)\n    (Size64x2xN)\n))\n\n;; Helper for calculating the `VectorSize` corresponding to a type\n(decl vector_size (Type) VectorSize)\n(rule (vector_size (multi_lane 8 8)) (VectorSize.Size8x8))\n(rule (vector_size (multi_lane 8 16)) (VectorSize.Size8x16))\n(rule (vector_size (multi_lane 16 4)) (VectorSize.Size16x4))\n(rule (vector_size (multi_lane 16 8)) (VectorSize.Size16x8))\n(rule (vector_size (multi_lane 32 2)) (VectorSize.Size32x2))\n(rule (vector_size (multi_lane 32 4)) (VectorSize.Size32x4))\n(rule (vector_size (multi_lane 64 2)) (VectorSize.Size64x2))\n(rule (vector_size (dynamic_lane 8 8)) (VectorSize.Size8x8))\n(rule (vector_size (dynamic_lane 8 16)) (VectorSize.Size8x16))\n(rule (vector_size (dynamic_lane 16 4)) (VectorSize.Size16x4))\n(rule (vector_size (dynamic_lane 16 8)) (VectorSize.Size16x8))\n(rule (vector_size (dynamic_lane 32 2)) (VectorSize.Size32x2))\n(rule (vector_size (dynamic_lane 32 4)) (VectorSize.Size32x4))\n(rule (vector_size (dynamic_lane 64 2)) (VectorSize.Size64x2))\n\n;; A floating-point unit (FPU) operation with one arg.\n(type FPUOp1\n  (enum\n    (Abs)\n    (Neg)\n    (Sqrt)\n    (Cvt32To64)\n    (Cvt64To32)\n))\n\n;; A floating-point unit (FPU) operation with two args.\n(type FPUOp2\n  (enum\n    (Add)\n    (Sub)\n    (Mul)\n    (Div)\n    (Max)\n    (Min)\n))\n\n;; A floating-point unit (FPU) operation with three args.\n(type FPUOp3\n  (enum\n    (MAdd32)\n    (MAdd64)\n))\n\n;; A conversion from an FP to an integer value.\n(type FpuToIntOp\n  (enum\n    (F32ToU32)\n    (F32ToI32)\n    (F32ToU64)\n    (F32ToI64)\n    (F64ToU32)\n    (F64ToI32)\n    (F64ToU64)\n    (F64ToI64)\n))\n\n;; A conversion from an integer to an FP value.\n(type IntToFpuOp\n  (enum\n    (U32ToF32)\n    (I32ToF32)\n    (U32ToF64)\n    (I32ToF64)\n    (U64ToF32)\n    (I64ToF32)\n    (U64ToF64)\n    (I64ToF64)\n))\n\n;; Modes for FP rounding ops: round down (floor) or up (ceil), or toward zero (trunc), or to\n;; nearest, and for 32- or 64-bit FP values.\n(type FpuRoundMode\n  (enum\n    (Minus32)\n    (Minus64)\n    (Plus32)\n    (Plus64)\n    (Zero32)\n    (Zero64)\n    (Nearest32)\n    (Nearest64)\n))\n\n;; Type of vector element extensions.\n(type VecExtendOp\n  (enum\n    ;; Signed extension of 8-bit elements\n    (Sxtl8)\n    ;; Signed extension of 16-bit elements\n    (Sxtl16)\n    ;; Signed extension of 32-bit elements\n    (Sxtl32)\n    ;; Unsigned extension of 8-bit elements\n    (Uxtl8)\n    ;; Unsigned extension of 16-bit elements\n    (Uxtl16)\n    ;; Unsigned extension of 32-bit elements\n    (Uxtl32)\n))\n\n;; A vector ALU operation.\n(type VecALUOp\n  (enum\n    ;; Signed saturating add\n    (Sqadd)\n    ;; Unsigned saturating add\n    (Uqadd)\n    ;; Signed saturating subtract\n    (Sqsub)\n    ;; Unsigned saturating subtract\n    (Uqsub)\n    ;; Compare bitwise equal\n    (Cmeq)\n    ;; Compare signed greater than or equal\n    (Cmge)\n    ;; Compare signed greater than\n    (Cmgt)\n    ;; Compare unsigned higher\n    (Cmhs)\n    ;; Compare unsigned higher or same\n    (Cmhi)\n    ;; Floating-point compare equal\n    (Fcmeq)\n    ;; Floating-point compare greater than\n    (Fcmgt)\n    ;; Floating-point compare greater than or equal\n    (Fcmge)\n    ;; Bitwise and\n    (And)\n    ;; Bitwise bit clear\n    (Bic)\n    ;; Bitwise inclusive or\n    (Orr)\n    ;; Bitwise exclusive or\n    (Eor)\n    ;; Bitwise select\n    (Bsl)\n    ;; Unsigned maximum pairwise\n    (Umaxp)\n    ;; Add\n    (Add)\n    ;; Subtract\n    (Sub)\n    ;; Multiply\n    (Mul)\n    ;; Signed shift left\n    (Sshl)\n    ;; Unsigned shift left\n    (Ushl)\n    ;; Unsigned minimum\n    (Umin)\n    ;; Signed minimum\n    (Smin)\n    ;; Unsigned maximum\n    (Umax)\n    ;; Signed maximum\n    (Smax)\n    ;; Unsigned rounding halving add\n    (Urhadd)\n    ;; Floating-point add\n    (Fadd)\n    ;; Floating-point subtract\n    (Fsub)\n    ;; Floating-point divide\n    (Fdiv)\n    ;; Floating-point maximum\n    (Fmax)\n    ;; Floating-point minimum\n    (Fmin)\n    ;; Floating-point multiply\n    (Fmul)\n    ;; Add pairwise\n    (Addp)\n    ;; Zip vectors (primary) [meaning, high halves]\n    (Zip1)\n    ;; Signed saturating rounding doubling multiply returning high half\n    (Sqrdmulh)\n))\n\n;; A Vector miscellaneous operation with two registers.\n(type VecMisc2\n  (enum\n    ;; Bitwise NOT\n    (Not)\n    ;; Negate\n    (Neg)\n    ;; Absolute value\n    (Abs)\n    ;; Floating-point absolute value\n    (Fabs)\n    ;; Floating-point negate\n    (Fneg)\n    ;; Floating-point square root\n    (Fsqrt)\n    ;; Reverse elements in 64-bit doublewords\n    (Rev64)\n    ;; Floating-point convert to signed integer, rounding toward zero\n    (Fcvtzs)\n    ;; Floating-point convert to unsigned integer, rounding toward zero\n    (Fcvtzu)\n    ;; Signed integer convert to floating-point\n    (Scvtf)\n    ;; Unsigned integer convert to floating-point\n    (Ucvtf)\n    ;; Floating point round to integral, rounding towards nearest\n    (Frintn)\n    ;; Floating point round to integral, rounding towards zero\n    (Frintz)\n    ;; Floating point round to integral, rounding towards minus infinity\n    (Frintm)\n    ;; Floating point round to integral, rounding towards plus infinity\n    (Frintp)\n    ;; Population count per byte\n    (Cnt)\n    ;; Compare bitwise equal to 0\n    (Cmeq0)\n    ;; Compare signed greater than or equal to 0\n    (Cmge0)\n    ;; Compare signed greater than 0\n    (Cmgt0)\n    ;; Compare signed less than or equal to 0\n    (Cmle0)\n    ;; Compare signed less than 0\n    (Cmlt0)\n    ;; Floating point compare equal to 0\n    (Fcmeq0)\n    ;; Floating point compare greater than or equal to 0\n    (Fcmge0)\n    ;; Floating point compare greater than 0\n    (Fcmgt0)\n    ;; Floating point compare less than or equal to 0\n    (Fcmle0)\n    ;; Floating point compare less than 0\n    (Fcmlt0)\n))\n\n;; A vector widening operation with one argument.\n(type VecRRLongOp\n  (enum\n    ;; Floating-point convert to higher precision long, 16-bit elements\n    (Fcvtl16)\n    ;; Floating-point convert to higher precision long, 32-bit elements\n    (Fcvtl32)\n    ;; Shift left long (by element size), 8-bit elements\n    (Shll8)\n    ;; Shift left long (by element size), 16-bit elements\n    (Shll16)\n    ;; Shift left long (by element size), 32-bit elements\n    (Shll32)\n))\n\n;; A vector narrowing operation with one argument.\n(type VecRRNarrowOp\n  (enum\n    ;; Extract narrow, 16-bit elements\n    (Xtn16)\n    ;; Extract narrow, 32-bit elements\n    (Xtn32)\n    ;; Extract narrow, 64-bit elements\n    (Xtn64)\n    ;; Signed saturating extract narrow, 16-bit elements\n    (Sqxtn16)\n    ;; Signed saturating extract narrow, 32-bit elements\n    (Sqxtn32)\n    ;; Signed saturating extract narrow, 64-bit elements\n    (Sqxtn64)\n    ;; Signed saturating extract unsigned narrow, 16-bit elements\n    (Sqxtun16)\n    ;; Signed saturating extract unsigned narrow, 32-bit elements\n    (Sqxtun32)\n    ;; Signed saturating extract unsigned narrow, 64-bit elements\n    (Sqxtun64)\n    ;; Unsigned saturating extract narrow, 16-bit elements\n    (Uqxtn16)\n    ;; Unsigned saturating extract narrow, 32-bit elements\n    (Uqxtn32)\n    ;; Unsigned saturating extract narrow, 64-bit elements\n    (Uqxtn64)\n    ;; Floating-point convert to lower precision narrow, 32-bit elements\n    (Fcvtn32)\n    ;; Floating-point convert to lower precision narrow, 64-bit elements\n    (Fcvtn64)\n))\n\n(type VecRRRLongOp\n  (enum\n    ;; Signed multiply long.\n    (Smull8)\n    (Smull16)\n    (Smull32)\n    ;; Unsigned multiply long.\n    (Umull8)\n    (Umull16)\n    (Umull32)\n    ;; Unsigned multiply add long\n    (Umlal8)\n    (Umlal16)\n    (Umlal32)\n))\n\n;; A vector operation on a pair of elements with one register.\n(type VecPairOp\n  (enum\n    ;; Add pair of elements\n    (Addp)\n))\n\n;; 1-operand vector instruction that extends elements of the input register\n;; and operates on a pair of elements.\n(type VecRRPairLongOp\n  (enum\n    ;; Sign extend and add pair of elements\n    (Saddlp8)\n    (Saddlp16)\n    ;; Unsigned extend and add pair of elements\n    (Uaddlp8)\n    (Uaddlp16)\n))\n\n;; An operation across the lanes of vectors.\n(type VecLanesOp\n  (enum\n    ;; Integer addition across a vector\n    (Addv)\n    ;; Unsigned minimum across a vector\n    (Uminv)\n))\n\n;; A shift-by-immediate operation on each lane of a vector.\n(type VecShiftImmOp\n  (enum\n    ;; Unsigned shift left\n    (Shl)\n    ;; Unsigned shift right\n    (Ushr)\n    ;; Signed shift right\n    (Sshr)\n))\n\n;; Atomic read-modify-write operations with acquire-release semantics\n(type AtomicRMWOp\n  (enum\n    (Add)\n    (Clr)\n    (Eor)\n    (Set)\n    (Smax)\n    (Smin)\n    (Umax)\n    (Umin)\n    (Swp)\n))\n\n;; Atomic read-modify-write operations, with acquire-release semantics,\n;; implemented with a loop.\n(type AtomicRMWLoopOp\n  (enum\n    (Add)\n    (Sub)\n    (And)\n    (Nand)\n    (Eor)\n    (Orr)\n    (Smax)\n    (Smin)\n    (Umax)\n    (Umin)\n    (Xchg)\n))\n\n;; Extractors for target features ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n(decl use_lse () Inst)\n(extern extractor use_lse use_lse)\n\n;; Extractor helpers for various immmediate constants ;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(decl move_wide_const_from_u64 (MoveWideConst) u64)\n(extern extractor move_wide_const_from_u64 move_wide_const_from_u64)\n\n(decl move_wide_const_from_negated_u64 (MoveWideConst) u64)\n(extern extractor move_wide_const_from_negated_u64 move_wide_const_from_negated_u64)\n\n(decl pure imm_logic_from_u64 (Type u64) ImmLogic)\n(extern constructor imm_logic_from_u64 imm_logic_from_u64)\n\n(decl pure imm_logic_from_imm64 (Type Imm64) ImmLogic)\n(extern constructor imm_logic_from_imm64 imm_logic_from_imm64)\n\n(decl pure imm_shift_from_imm64 (Type Imm64) ImmShift)\n(extern constructor imm_shift_from_imm64 imm_shift_from_imm64)\n\n(decl imm_shift_from_u8 (u8) ImmShift)\n(extern constructor imm_shift_from_u8 imm_shift_from_u8)\n\n(decl imm12_from_u64 (Imm12) u64)\n(extern extractor imm12_from_u64 imm12_from_u64)\n\n(decl u8_into_uimm5 (u8) UImm5)\n(extern constructor u8_into_uimm5 u8_into_uimm5)\n\n(decl u8_into_imm12 (u8) Imm12)\n(extern constructor u8_into_imm12 u8_into_imm12)\n\n(decl u64_into_imm_logic (Type u64) ImmLogic)\n(extern constructor u64_into_imm_logic u64_into_imm_logic)\n\n(decl imm12_from_negated_u64 (Imm12) u64)\n(extern extractor imm12_from_negated_u64 imm12_from_negated_u64)\n\n(decl pure lshl_from_imm64 (Type Imm64) ShiftOpAndAmt)\n(extern constructor lshl_from_imm64 lshl_from_imm64)\n\n(decl integral_ty (Type) Type)\n(extern extractor integral_ty integral_ty)\n\n(decl valid_atomic_transaction (Type) Type)\n(extern extractor valid_atomic_transaction valid_atomic_transaction)\n\n;; Helper to go directly from a `Value`, when it's an `iconst`, to an `Imm12`.\n(decl imm12_from_value (Imm12) Value)\n(extractor\n  (imm12_from_value n)\n  (iconst (u64_from_imm64 (imm12_from_u64 n))))\n\n;; Same as `imm12_from_value`, but tries negating the constant value.\n(decl imm12_from_negated_value (Imm12) Value)\n(extractor\n  (imm12_from_negated_value n)\n  (iconst (u64_from_imm64 (imm12_from_negated_u64 n))))\n\n;; Helper type to represent a value and an extend operation fused together.\n(type ExtendedValue extern (enum))\n(decl extended_value_from_value (ExtendedValue) Value)\n(extern extractor extended_value_from_value extended_value_from_value)\n\n;; Constructors used to poke at the fields of an `ExtendedValue`.\n(decl put_extended_in_reg (ExtendedValue) Reg)\n(extern constructor put_extended_in_reg put_extended_in_reg)\n(decl get_extended_op (ExtendedValue) ExtendOp)\n(extern constructor get_extended_op get_extended_op)\n\n(decl nzcv (bool bool bool bool) NZCV)\n(extern constructor nzcv nzcv)\n\n(decl cond_br_zero (Reg) CondBrKind)\n(extern constructor cond_br_zero cond_br_zero)\n\n(decl cond_br_cond (Cond) CondBrKind)\n(extern constructor cond_br_cond cond_br_cond)\n\n;; Instruction creation helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Helper for creating the zero register.\n(decl zero_reg () Reg)\n(extern constructor zero_reg zero_reg)\n\n(decl writable_zero_reg () WritableReg)\n(extern constructor writable_zero_reg writable_zero_reg)\n\n;; Helpers for getting a particular real register\n(decl xreg (u8) Reg)\n(extern constructor xreg xreg)\n\n(decl writable_xreg (u8) WritableReg)\n(extern constructor writable_xreg writable_xreg)\n\n;; Helper for emitting `MInst.Mov64` instructions.\n(decl mov64_to_real (u8 Reg) Reg)\n(rule (mov64_to_real num src)\n      (let ((dst WritableReg (writable_xreg num))\n            (_ Unit (emit (MInst.Mov (operand_size $I64) dst src))))\n        dst))\n\n(decl mov64_from_real (u8) Reg)\n(rule (mov64_from_real num)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.Mov (operand_size $I64) dst (xreg num)))))\n        dst))\n\n;; Helper for emitting `MInst.MovZ` instructions.\n(decl movz (MoveWideConst OperandSize) Reg)\n(rule (movz imm size)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.MovWide (MoveWideOp.MovZ) dst imm size))))\n        dst))\n\n;; Helper for emitting `MInst.MovN` instructions.\n(decl movn (MoveWideConst OperandSize) Reg)\n(rule (movn imm size)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.MovWide (MoveWideOp.MovN) dst imm size))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRImmLogic` instructions.\n(decl alu_rr_imm_logic (ALUOp Type Reg ImmLogic) Reg)\n(rule (alu_rr_imm_logic op ty src imm)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRImmLogic op (operand_size ty) dst src imm))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRImmShift` instructions.\n(decl alu_rr_imm_shift (ALUOp Type Reg ImmShift) Reg)\n(rule (alu_rr_imm_shift op ty src imm)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRImmShift op (operand_size ty) dst src imm))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRR` instructions.\n(decl alu_rrr (ALUOp Type Reg Reg) Reg)\n(rule (alu_rrr op ty src1 src2)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRR op (operand_size ty) dst src1 src2))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRR` instructions.\n(decl vec_rrr (VecALUOp Reg Reg VectorSize) Reg)\n(rule (vec_rrr op src1 src2 size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecRRR op dst src1 src2 size))))\n        dst))\n\n;; Helper for emitting `MInst.FpuRRR` instructions.\n(decl fpu_rrr (FPUOp2 Reg Reg ScalarSize) Reg)\n(rule (fpu_rrr op src1 src2 size)\n      (let ((dst WritableReg (temp_writable_reg $F64))\n            (_ Unit (emit (MInst.FpuRRR op size dst src1 src2))))\n        dst))\n\n;; Helper for emitting `MInst.FpuCmp` instructions.\n(decl fpu_cmp (ScalarSize Reg Reg) ProducesFlags)\n(rule (fpu_cmp size rn rm)\n      (ProducesFlags.ProducesFlagsSideEffect\n       (MInst.FpuCmp size rn rm)))\n\n;; Helper for emitting `MInst.VecLanes` instructions.\n(decl vec_lanes (VecLanesOp Reg VectorSize) Reg)\n(rule (vec_lanes op src size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecLanes op dst src size))))\n        dst))\n\n;; Helper for emitting `MInst.VecDup` instructions.\n(decl vec_dup (Reg VectorSize) Reg)\n(rule (vec_dup src size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecDup dst src size))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRImm12` instructions.\n(decl alu_rr_imm12 (ALUOp Type Reg Imm12) Reg)\n(rule (alu_rr_imm12 op ty src imm)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRImm12 op (operand_size ty) dst src imm))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRRShift` instructions.\n(decl alu_rrr_shift (ALUOp Type Reg Reg ShiftOpAndAmt) Reg)\n(rule (alu_rrr_shift op ty src1 src2 shift)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRRShift op (operand_size ty) dst src1 src2 shift))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRRExtend` instructions.\n(decl alu_rrr_extend (ALUOp Type Reg Reg ExtendOp) Reg)\n(rule (alu_rrr_extend op ty src1 src2 extend)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRRExtend op (operand_size ty) dst src1 src2 extend))))\n        dst))\n\n;; Same as `alu_rrr_extend`, but takes an `ExtendedValue` packed \"pair\" instead\n;; of a `Reg` and an `ExtendOp`.\n(decl alu_rr_extend_reg (ALUOp Type Reg ExtendedValue) Reg)\n(rule (alu_rr_extend_reg op ty src1 extended_reg)\n      (let ((src2 Reg (put_extended_in_reg extended_reg))\n            (extend ExtendOp (get_extended_op extended_reg)))\n        (alu_rrr_extend op ty src1 src2 extend)))\n\n;; Helper for emitting `MInst.AluRRRR` instructions.\n(decl alu_rrrr (ALUOp3 Type Reg Reg Reg) Reg)\n(rule (alu_rrrr op ty src1 src2 src3)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRRR op (operand_size ty) dst src1 src2 src3))))\n        dst))\n\n;; Helper for emitting `MInst.BitRR` instructions.\n(decl bit_rr (BitOp Type Reg) Reg)\n(rule (bit_rr op ty src)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.BitRR op (operand_size ty) dst src))))\n        dst))\n\n;; Helper for emitting `adds` instructions.\n(decl add_with_flags_paired (Type Reg Reg) ProducesFlags)\n(rule (add_with_flags_paired ty src1 src2)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ProducesFlags.ProducesFlagsReturnsResultWithConsumer\n         (MInst.AluRRR (ALUOp.AddS) (operand_size ty) dst src1 src2)\n         dst)))\n\n;; Helper for emitting `adc` instructions.\n(decl adc_paired (Type Reg Reg) ConsumesFlags)\n(rule (adc_paired ty src1 src2)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ConsumesFlags.ConsumesFlagsReturnsResultWithProducer\n         (MInst.AluRRR (ALUOp.Adc) (operand_size ty) dst src1 src2)\n         dst)))\n\n;; Helper for emitting `subs` instructions.\n(decl sub_with_flags_paired (Type Reg Reg) ProducesFlags)\n(rule (sub_with_flags_paired ty src1 src2)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ProducesFlags.ProducesFlagsReturnsResultWithConsumer\n         (MInst.AluRRR (ALUOp.SubS) (operand_size ty) dst src1 src2)\n         dst)))\n\n(decl cmp_imm (OperandSize Reg Imm12) ProducesFlags)\n(rule (cmp_imm size src1 src2)\n      (ProducesFlags.ProducesFlagsSideEffect\n       (MInst.AluRRImm12 (ALUOp.SubS) size (writable_zero_reg)\n        src1 src2)))\n\n(decl cmp64_imm (Reg Imm12) ProducesFlags)\n(rule (cmp64_imm src1 src2)\n      (cmp_imm (OperandSize.Size64) src1 src2))\n\n;; Helper for emitting `sbc` instructions.\n(decl sbc_paired (Type Reg Reg) ConsumesFlags)\n(rule (sbc_paired ty src1 src2)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ConsumesFlags.ConsumesFlagsReturnsResultWithProducer\n         (MInst.AluRRR (ALUOp.Sbc) (operand_size ty) dst src1 src2)\n         dst)))\n\n;; Helper for emitting `MInst.VecMisc` instructions.\n(decl vec_misc (VecMisc2 Reg VectorSize) Reg)\n(rule (vec_misc op src size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecMisc op dst src size))))\n        dst))\n\n;; Helper for emitting `MInst.VecTbl` instructions.\n(decl vec_tbl (Reg Reg bool) Reg)\n(rule (vec_tbl rn rm is_extension)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecTbl dst rn rm is_extension))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRRLong` instructions.\n(decl vec_rrr_long (VecRRRLongOp Reg Reg bool) Reg)\n(rule (vec_rrr_long op src1 src2 high_half)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecRRRLong op dst src1 src2 high_half))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRPairLong` instructions.\n(decl vec_rr_pair_long (VecRRPairLongOp Reg) Reg)\n(rule (vec_rr_pair_long op src)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecRRPairLong op dst src))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRRLong` instructions, but for variants\n;; where the operation both reads and modifies the destination register.\n;;\n;; Currently this is only used for `VecRRRLongOp.Umlal*`\n(decl vec_rrrr_long (VecRRRLongOp Reg Reg Reg bool) Reg)\n(rule (vec_rrrr_long op src1 src2 src3 high_half)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_1 Unit (emit (MInst.FpuMove128 dst src1)))\n            (_2 Unit (emit (MInst.VecRRRLong op dst src2 src3 high_half))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRNarrow` instructions.\n(decl vec_rr_narrow (VecRRNarrowOp Reg bool) Reg)\n(rule (vec_rr_narrow op src high_half)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecRRNarrow op dst src high_half))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRLong` instructions.\n(decl vec_rr_long (VecRRLongOp Reg bool) Reg)\n(rule (vec_rr_long op src high_half)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecRRLong op dst src high_half))))\n        dst))\n\n;; Helper for emitting `MInst.FpuCSel32` / `MInst.FpuCSel64`\n;; instructions.\n(decl fpu_csel (Type Cond Reg Reg) ConsumesFlags)\n(rule (fpu_csel $F32 cond if_true if_false)\n      (let ((dst WritableReg (temp_writable_reg $F32)))\n        (ConsumesFlags.ConsumesFlagsReturnsReg\n         (MInst.FpuCSel32 dst if_true if_false cond)\n         dst)))\n\n(rule (fpu_csel $F64 cond if_true if_false)\n      (let ((dst WritableReg (temp_writable_reg $F64)))\n        (ConsumesFlags.ConsumesFlagsReturnsReg\n         (MInst.FpuCSel64 dst if_true if_false cond)\n         dst)))\n\n\n;; Helper for emitting `MInst.MovToFpu` instructions.\n(decl mov_to_fpu (Reg ScalarSize) Reg)\n(rule (mov_to_fpu x size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.MovToFpu dst x size))))\n        dst))\n\n;; Helper for emitting `MInst.MovToVec` instructions.\n(decl mov_to_vec (Reg Reg u8 VectorSize) Reg)\n(rule (mov_to_vec src1 src2 lane size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_1 Unit (emit (MInst.FpuMove128 dst src1)))\n            (_2 Unit (emit (MInst.MovToVec dst src2 lane size))))\n        dst))\n\n;; Helper for emitting `MInst.MovFromVec` instructions.\n(decl mov_from_vec (Reg u8 VectorSize) Reg)\n(rule (mov_from_vec rn idx size)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.MovFromVec dst rn idx size))))\n        dst))\n\n;; Helper for emitting `MInst.MovFromVecSigned` instructions.\n(decl mov_from_vec_signed (Reg u8 VectorSize OperandSize) Reg)\n(rule (mov_from_vec_signed rn idx size scalar_size)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.MovFromVecSigned dst rn idx size scalar_size))))\n        dst))\n\n;; Helper for emitting `MInst.Extend` instructions.\n(decl extend (Reg bool u8 u8) Reg)\n(rule (extend rn signed from_bits to_bits)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.Extend dst rn signed from_bits to_bits))))\n        dst))\n\n;; Helper for emitting `MInst.FpuExtend` instructions.\n(decl fpu_extend (Reg ScalarSize) Reg)\n(rule (fpu_extend src size)\n      (let ((dst WritableReg (temp_writable_reg $F32X4))\n            (_ Unit (emit (MInst.FpuExtend dst src size))))\n        dst))\n\n;; Helper for emitting `MInst.LoadAcquire` instructions.\n(decl load_acquire (Type Reg) Reg)\n(rule (load_acquire ty addr)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.LoadAcquire ty dst addr))))\n        dst))\n\n;; Helper for emitting `MInst.StoreRelease` instructions.\n(decl store_release (Type Reg Reg) SideEffectNoResult)\n(rule (store_release ty src addr)\n      (SideEffectNoResult.Inst (MInst.StoreRelease ty src addr)))\n\n;; Helper for generating a `tst` instruction.\n;;\n;; Produces a `ProducesFlags` rather than a register or emitted instruction\n;; which must be paired with `with_flags*` helpers.\n(decl tst_imm (Type Reg ImmLogic) ProducesFlags)\n(rule (tst_imm ty reg imm)\n      (ProducesFlags.ProducesFlagsSideEffect\n       (MInst.AluRRImmLogic (ALUOp.AndS)\n                            (operand_size ty)\n                            (writable_zero_reg)\n                            reg\n                            imm)))\n\n;; Helper for generating a `CSel` instruction.\n;;\n;; Note that this doesn't actually emit anything, instead it produces a\n;; `ConsumesFlags` instruction which must be consumed with `with_flags*`\n;; helpers.\n(decl csel (Cond Reg Reg) ConsumesFlags)\n(rule (csel cond if_true if_false)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ConsumesFlags.ConsumesFlagsReturnsReg\n         (MInst.CSel dst cond if_true if_false)\n         dst)))\n\n;; Helper for generating a `CSNeg` instruction.\n;;\n;; Note that this doesn't actually emit anything, instead it produces a\n;; `ConsumesFlags` instruction which must be consumed with `with_flags*`\n;; helpers.\n(decl csneg (Cond Reg Reg) ConsumesFlags)\n(rule (csneg cond if_true if_false)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ConsumesFlags.ConsumesFlagsReturnsReg\n         (MInst.CSNeg dst cond if_true if_false)\n         dst)))\n\n;; Helpers for generating `add` instructions.\n\n(decl add (Type Reg Reg) Reg)\n(rule (add ty x y) (alu_rrr (ALUOp.Add) ty x y))\n\n(decl add_imm (Type Reg Imm12) Reg)\n(rule (add_imm ty x y) (alu_rr_imm12 (ALUOp.Add) ty x y))\n\n(decl add_extend (Type Reg ExtendedValue) Reg)\n(rule (add_extend ty x y) (alu_rr_extend_reg (ALUOp.Add) ty x y))\n\n(decl add_shift (Type Reg Reg ShiftOpAndAmt) Reg)\n(rule (add_shift ty x y z) (alu_rrr_shift (ALUOp.Add) ty x y z))\n\n(decl add_vec (Reg Reg VectorSize) Reg)\n(rule (add_vec x y size) (vec_rrr (VecALUOp.Add) x y size))\n\n;; Helpers for generating `sub` instructions.\n\n(decl sub (Type Reg Reg) Reg)\n(rule (sub ty x y) (alu_rrr (ALUOp.Sub) ty x y))\n\n(decl sub_imm (Type Reg Imm12) Reg)\n(rule (sub_imm ty x y) (alu_rr_imm12 (ALUOp.Sub) ty x y))\n\n(decl sub_extend (Type Reg ExtendedValue) Reg)\n(rule (sub_extend ty x y) (alu_rr_extend_reg (ALUOp.Sub) ty x y))\n\n(decl sub_shift (Type Reg Reg ShiftOpAndAmt) Reg)\n(rule (sub_shift ty x y z) (alu_rrr_shift (ALUOp.Sub) ty x y z))\n\n(decl sub_vec (Reg Reg VectorSize) Reg)\n(rule (sub_vec x y size) (vec_rrr (VecALUOp.Sub) x y size))\n\n;; Helpers for generating `madd` instructions.\n\n(decl madd (Type Reg Reg Reg) Reg)\n(rule (madd ty x y z) (alu_rrrr (ALUOp3.MAdd) ty x y z))\n\n;; Helpers for generating `msub` instructions.\n\n(decl msub (Type Reg Reg Reg) Reg)\n(rule (msub ty x y z) (alu_rrrr (ALUOp3.MSub) ty x y z))\n\n;; Helper for generating `uqadd` instructions.\n(decl uqadd (Reg Reg VectorSize) Reg)\n(rule (uqadd x y size) (vec_rrr (VecALUOp.Uqadd) x y size))\n\n;; Helper for generating `sqadd` instructions.\n(decl sqadd (Reg Reg VectorSize) Reg)\n(rule (sqadd x y size) (vec_rrr (VecALUOp.Sqadd) x y size))\n\n;; Helper for generating `uqsub` instructions.\n(decl uqsub (Reg Reg VectorSize) Reg)\n(rule (uqsub x y size) (vec_rrr (VecALUOp.Uqsub) x y size))\n\n;; Helper for generating `sqsub` instructions.\n(decl sqsub (Reg Reg VectorSize) Reg)\n(rule (sqsub x y size) (vec_rrr (VecALUOp.Sqsub) x y size))\n\n;; Helper for generating `umulh` instructions.\n(decl umulh (Type Reg Reg) Reg)\n(rule (umulh ty x y) (alu_rrr (ALUOp.UMulH) ty x y))\n\n;; Helper for generating `smulh` instructions.\n(decl smulh (Type Reg Reg) Reg)\n(rule (smulh ty x y) (alu_rrr (ALUOp.SMulH) ty x y))\n\n;; Helper for generating `mul` instructions.\n(decl mul (Reg Reg VectorSize) Reg)\n(rule (mul x y size) (vec_rrr (VecALUOp.Mul) x y size))\n\n;; Helper for generating `neg` instructions.\n(decl neg (Reg VectorSize) Reg)\n(rule (neg x size) (vec_misc (VecMisc2.Neg) x size))\n\n;; Helper for generating `rev64` instructions.\n(decl rev64 (Reg VectorSize) Reg)\n(rule (rev64 x size) (vec_misc (VecMisc2.Rev64) x size))\n\n;; Helper for generating `xtn64` instructions.\n(decl xtn64 (Reg bool) Reg)\n(rule (xtn64 x high_half) (vec_rr_narrow (VecRRNarrowOp.Xtn64) x high_half))\n\n;; Helper for generating `addp` instructions.\n(decl addp (Reg Reg VectorSize) Reg)\n(rule (addp x y size) (vec_rrr (VecALUOp.Addp) x y size))\n\n;; Helper for generating vector `abs` instructions.\n(decl vec_abs (Reg VectorSize) Reg)\n(rule (vec_abs x size) (vec_misc (VecMisc2.Abs) x size))\n\n;; Helper for generating instruction sequences to calculate a scalar absolute\n;; value.\n(decl abs (OperandSize Reg) Reg)\n(rule (abs size x)\n      (value_regs_get (with_flags (cmp_imm size x (u8_into_imm12 0))\n                                  (csneg (Cond.Gt) x x)) 0))\n\n;; Helper for generating `addv` instructions.\n(decl addv (Reg VectorSize) Reg)\n(rule (addv x size) (vec_lanes (VecLanesOp.Addv) x size))\n\n;; Helper for generating `shll32` instructions.\n(decl shll32 (Reg bool) Reg)\n(rule (shll32 x high_half) (vec_rr_long (VecRRLongOp.Shll32) x high_half))\n\n;; Helpers for generating `addlp` instructions.\n\n(decl saddlp8 (Reg) Reg)\n(rule (saddlp8 x) (vec_rr_pair_long (VecRRPairLongOp.Saddlp8) x))\n\n(decl saddlp16 (Reg) Reg)\n(rule (saddlp16 x) (vec_rr_pair_long (VecRRPairLongOp.Saddlp16) x))\n\n(decl uaddlp8 (Reg) Reg)\n(rule (uaddlp8 x) (vec_rr_pair_long (VecRRPairLongOp.Uaddlp8) x))\n\n(decl uaddlp16 (Reg) Reg)\n(rule (uaddlp16 x) (vec_rr_pair_long (VecRRPairLongOp.Uaddlp16) x))\n\n;; Helper for generating `umlal32` instructions.\n(decl umlal32 (Reg Reg Reg bool) Reg)\n(rule (umlal32 x y z high_half) (vec_rrrr_long (VecRRRLongOp.Umlal32) x y z high_half))\n\n;; Helper for generating `smull8` instructions.\n(decl smull8 (Reg Reg bool) Reg)\n(rule (smull8 x y high_half) (vec_rrr_long (VecRRRLongOp.Smull8) x y high_half))\n\n;; Helper for generating `umull8` instructions.\n(decl umull8 (Reg Reg bool) Reg)\n(rule (umull8 x y high_half) (vec_rrr_long (VecRRRLongOp.Umull8) x y high_half))\n\n;; Helper for generating `smull16` instructions.\n(decl smull16 (Reg Reg bool) Reg)\n(rule (smull16 x y high_half) (vec_rrr_long (VecRRRLongOp.Smull16) x y high_half))\n\n;; Helper for generating `umull16` instructions.\n(decl umull16 (Reg Reg bool) Reg)\n(rule (umull16 x y high_half) (vec_rrr_long (VecRRRLongOp.Umull16) x y high_half))\n\n;; Helper for generating `smull32` instructions.\n(decl smull32 (Reg Reg bool) Reg)\n(rule (smull32 x y high_half) (vec_rrr_long (VecRRRLongOp.Smull32) x y high_half))\n\n;; Helper for generating `umull32` instructions.\n(decl umull32 (Reg Reg bool) Reg)\n(rule (umull32 x y high_half) (vec_rrr_long (VecRRRLongOp.Umull32) x y high_half))\n\n;; Helper for generating `asr` instructions.\n(decl asr (Type Reg Reg) Reg)\n(rule (asr ty x y) (alu_rrr (ALUOp.Asr) ty x y))\n\n(decl asr_imm (Type Reg ImmShift) Reg)\n(rule (asr_imm ty x imm) (alu_rr_imm_shift (ALUOp.Asr) ty x imm))\n\n;; Helper for generating `lsr` instructions.\n(decl lsr (Type Reg Reg) Reg)\n(rule (lsr ty x y) (alu_rrr (ALUOp.Lsr) ty x y))\n\n(decl lsr_imm (Type Reg ImmShift) Reg)\n(rule (lsr_imm ty x imm) (alu_rr_imm_shift (ALUOp.Lsr) ty x imm))\n\n;; Helper for generating `lsl` instructions.\n(decl lsl (Type Reg Reg) Reg)\n(rule (lsl ty x y) (alu_rrr (ALUOp.Lsl) ty x y))\n\n(decl lsl_imm (Type Reg ImmShift) Reg)\n(rule (lsl_imm ty x imm) (alu_rr_imm_shift (ALUOp.Lsl) ty x imm))\n\n;; Helper for generating `udiv` instructions.\n(decl a64_udiv (Type Reg Reg) Reg)\n(rule (a64_udiv ty x y) (alu_rrr (ALUOp.UDiv) ty x y))\n\n;; Helper for generating `sdiv` instructions.\n(decl a64_sdiv (Type Reg Reg) Reg)\n(rule (a64_sdiv ty x y) (alu_rrr (ALUOp.SDiv) ty x y))\n\n;; Helper for generating `not` instructions.\n(decl not (Reg VectorSize) Reg)\n(rule (not x size) (vec_misc (VecMisc2.Not) x size))\n\n;; Helpers for generating `orr_not` instructions.\n\n(decl orr_not (Type Reg Reg) Reg)\n(rule (orr_not ty x y) (alu_rrr (ALUOp.OrrNot) ty x y))\n\n(decl orr_not_shift (Type Reg Reg ShiftOpAndAmt) Reg)\n(rule (orr_not_shift ty x y shift) (alu_rrr_shift (ALUOp.OrrNot) ty x y shift))\n\n;; Helpers for generating `orr` instructions.\n\n(decl orr (Type Reg Reg) Reg)\n(rule (orr ty x y) (alu_rrr (ALUOp.Orr) ty x y))\n\n(decl orr_imm (Type Reg ImmLogic) Reg)\n(rule (orr_imm ty x y) (alu_rr_imm_logic (ALUOp.Orr) ty x y))\n\n(decl orr_vec (Reg Reg VectorSize) Reg)\n(rule (orr_vec x y size) (vec_rrr (VecALUOp.Orr) x y size))\n\n;; Helpers for generating `and` instructions.\n\n(decl and_reg (Type Reg Reg) Reg)\n(rule (and_reg ty x y) (alu_rrr (ALUOp.And) ty x y))\n\n(decl and_imm (Type Reg ImmLogic) Reg)\n(rule (and_imm ty x y) (alu_rr_imm_logic (ALUOp.And) ty x y))\n\n(decl and_vec (Reg Reg VectorSize) Reg)\n(rule (and_vec x y size) (vec_rrr (VecALUOp.And) x y size))\n\n;; Helpers for generating `eor` instructions.\n(decl eor_vec (Reg Reg VectorSize) Reg)\n(rule (eor_vec x y size) (vec_rrr (VecALUOp.Eor) x y size))\n\n;; Helpers for generating `bic` instructions.\n\n(decl bic (Type Reg Reg) Reg)\n(rule (bic ty x y) (alu_rrr (ALUOp.AndNot) ty x y))\n\n(decl bic_vec (Reg Reg VectorSize) Reg)\n(rule (bic_vec x y size) (vec_rrr (VecALUOp.Bic) x y size))\n\n;; Helpers for generating `sshl` instructions.\n(decl sshl (Reg Reg VectorSize) Reg)\n(rule (sshl x y size) (vec_rrr (VecALUOp.Sshl) x y size))\n\n;; Helpers for generating `ushl` instructions.\n(decl ushl (Reg Reg VectorSize) Reg)\n(rule (ushl x y size) (vec_rrr (VecALUOp.Ushl) x y size))\n\n;; Helpers for generating `rotr` instructions.\n\n(decl a64_rotr (Type Reg Reg) Reg)\n(rule (a64_rotr ty x y) (alu_rrr (ALUOp.RotR) ty x y))\n\n(decl a64_rotr_imm (Type Reg ImmShift) Reg)\n(rule (a64_rotr_imm ty x y) (alu_rr_imm_shift (ALUOp.RotR) ty x y))\n\n;; Helpers for generating `rbit` instructions.\n\n(decl rbit (Type Reg) Reg)\n(rule (rbit ty x) (bit_rr (BitOp.RBit) ty x))\n\n;; Helpers for generating `clz` instructions.\n\n(decl a64_clz (Type Reg) Reg)\n(rule (a64_clz ty x) (bit_rr (BitOp.Clz) ty x))\n\n;; Helpers for generating `cls` instructions.\n\n(decl a64_cls (Type Reg) Reg)\n(rule (a64_cls ty x) (bit_rr (BitOp.Cls) ty x))\n\n;; Helpers for generating `eon` instructions.\n\n(decl eon (Type Reg Reg) Reg)\n(rule (eon ty x y) (alu_rrr (ALUOp.EorNot) ty x y))\n\n;; Helpers for generating `cnt` instructions.\n\n(decl vec_cnt (Reg VectorSize) Reg)\n(rule (vec_cnt x size) (vec_misc (VecMisc2.Cnt) x size))\n\n;; Helpers for generating a `bsl` instruction.\n\n(decl bsl (Type Reg Reg Reg) Reg)\n(rule (bsl ty c x y)\n      (let ((dst WritableReg (temp_writable_reg ty))\n            (_1 Unit (emit (MInst.FpuMove128 dst c)))\n            (_2 Unit (emit (MInst.VecRRR (VecALUOp.Bsl) dst x y (vector_size ty)))))\n        dst))\n\n;; Helper for generating a `udf` instruction.\n\n(decl udf (bool TrapCode) SideEffectNoResult)\n(rule (udf use_allocated_encoding trap_code)\n      (SideEffectNoResult.Inst (MInst.Udf use_allocated_encoding trap_code)))\n\n;; Immediate value helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(decl imm (Type u64) Reg)\n\n;; 16-bit immediate (shifted by 0, 16, 32 or 48 bits) in MOVZ\n(rule (imm (integral_ty _ty) (move_wide_const_from_u64 n))\n      (movz n (OperandSize.Size64)))\n\n;; 16-bit immediate (shifted by 0, 16, 32 or 48 bits) in MOVN\n(rule (imm (integral_ty _ty) (move_wide_const_from_negated_u64 n))\n      (movn n (OperandSize.Size64)))\n\n;; Weird logical-instruction immediate in ORI using zero register\n(rule (imm (integral_ty _ty) k)\n      (if-let n (imm_logic_from_u64 $I64 k))\n      (orr_imm $I64 (zero_reg) n))\n\n(decl load_constant64_full (u64) Reg)\n(extern constructor load_constant64_full load_constant64_full)\n\n;; Fallback for integral 64-bit constants that uses lots of `movk`\n(rule (imm (integral_ty _ty) n)\n      (load_constant64_full n))\n\n;; Sign extension helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Place a `Value` into a register, sign extending it to 32-bits\n(decl put_in_reg_sext32 (Value) Reg)\n(rule (put_in_reg_sext32 val @ (value_type (fits_in_32 ty)))\n      (extend val $true (ty_bits ty) 32))\n\n;; 32/64-bit passthrough.\n(rule (put_in_reg_sext32 val @ (value_type $I32)) val)\n(rule (put_in_reg_sext32 val @ (value_type $I64)) val)\n\n;; Place a `Value` into a register, zero extending it to 32-bits\n(decl put_in_reg_zext32 (Value) Reg)\n(rule (put_in_reg_zext32 val @ (value_type (fits_in_32 ty)))\n      (extend val $false (ty_bits ty) 32))\n\n;; 32/64-bit passthrough.\n(rule (put_in_reg_zext32 val @ (value_type $I32)) val)\n(rule (put_in_reg_zext32 val @ (value_type $I64)) val)\n\n;; Place a `Value` into a register, sign extending it to 64-bits\n(decl put_in_reg_sext64 (Value) Reg)\n(rule (put_in_reg_sext64 val @ (value_type (fits_in_32 ty)))\n      (extend val $true (ty_bits ty) 64))\n\n;; 64-bit passthrough.\n(rule (put_in_reg_sext64 val @ (value_type $I64)) val)\n\n;; Place a `Value` into a register, zero extending it to 64-bits\n(decl put_in_reg_zext64 (Value) Reg)\n(rule (put_in_reg_zext64 val @ (value_type (fits_in_32 ty)))\n      (extend val $false (ty_bits ty) 64))\n\n;; 64-bit passthrough.\n(rule (put_in_reg_zext64 val @ (value_type $I64)) val)\n\n;; Misc instruction helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(decl trap_if_zero_divisor (Reg) Reg)\n(rule (trap_if_zero_divisor reg)\n      (let ((_ Unit (emit (MInst.TrapIf (cond_br_zero reg) (trap_code_division_by_zero)))))\n        reg))\n\n(decl size_from_ty (Type) OperandSize)\n(rule (size_from_ty (fits_in_32 _ty)) (OperandSize.Size32))\n(rule (size_from_ty $I64) (OperandSize.Size64))\n\n;; Check for signed overflow. The only case is min_value / -1.\n;; The following checks must be done in 32-bit or 64-bit, depending\n;; on the input type.\n(decl trap_if_div_overflow (Type Reg Reg) Reg)\n(rule (trap_if_div_overflow ty x y)\n      (let (\n          ;; Check RHS is -1.\n          (_1 Unit (emit (MInst.AluRRImm12 (ALUOp.AddS) (operand_size ty) (writable_zero_reg) y (u8_into_imm12 1))))\n\n          ;; Check LHS is min_value, by subtracting 1 and branching if\n          ;; there is overflow.\n          (_2 Unit (emit (MInst.CCmpImm (size_from_ty ty)\n                                        x\n                                        (u8_into_uimm5 1)\n                                        (nzcv $false $false $false $false)\n                                        (Cond.Eq))))\n          (_3 Unit (emit (MInst.TrapIf (cond_br_cond (Cond.Vs))\n                                      (trap_code_integer_overflow))))\n        )\n        x))\n\n;; An atomic load that can be sunk into another operation.\n(type SinkableAtomicLoad extern (enum))\n\n;; Extract a `SinkableAtomicLoad` that works with `Reg` from a value\n;; operand.\n(decl sinkable_atomic_load (SinkableAtomicLoad) Value)\n(extern extractor sinkable_atomic_load sinkable_atomic_load)\n\n;; Sink a `SinkableLoad` into a `Reg`.\n;;\n;; This is a side-effectful operation that notifies the context that the\n;; instruction that produced the `SinkableAtomicLoad` has been sunk into another\n;; instruction, and no longer needs to be lowered.\n(decl sink_atomic_load (SinkableAtomicLoad) Reg)\n(extern constructor sink_atomic_load sink_atomic_load)\n\n;; Helper for generating either an `AluRRR`, `AluRRRShift`, or `AluRRImmLogic`\n;; instruction depending on the input. Note that this requires that the `ALUOp`\n;; specified is commutative.\n(decl alu_rs_imm_logic_commutative (ALUOp Type Value Value) Reg)\n\n;; Base case of operating on registers.\n(rule (alu_rs_imm_logic_commutative op ty x y)\n      (alu_rrr op ty x y))\n\n;; Special cases for when one operand is a constant.\n(rule (alu_rs_imm_logic_commutative op ty x (iconst k))\n      (if-let imm (imm_logic_from_imm64 ty k))\n      (alu_rr_imm_logic op ty x imm))\n(rule (alu_rs_imm_logic_commutative op ty (iconst k) x)\n      (if-let imm (imm_logic_from_imm64 ty k))\n      (alu_rr_imm_logic op ty x imm))\n\n;; Special cases for when one operand is shifted left by a constant.\n(rule (alu_rs_imm_logic_commutative op ty x (ishl y (iconst k)))\n      (if-let amt (lshl_from_imm64 ty k))\n      (alu_rrr_shift op ty x y amt))\n(rule (alu_rs_imm_logic_commutative op ty (ishl x (iconst k)) y)\n      (if-let amt (lshl_from_imm64 ty k))\n      (alu_rrr_shift op ty y x amt))\n\n;; Same as `alu_rs_imm_logic_commutative` above, except that it doesn't require\n;; that the operation is commutative.\n(decl alu_rs_imm_logic (ALUOp Type Value Value) Reg)\n(rule (alu_rs_imm_logic op ty x y)\n      (alu_rrr op ty x y))\n(rule (alu_rs_imm_logic op ty x (iconst k))\n      (if-let imm (imm_logic_from_imm64 ty k))\n      (alu_rr_imm_logic op ty x imm))\n(rule (alu_rs_imm_logic op ty x (ishl y (iconst k)))\n      (if-let amt (lshl_from_imm64 ty k))\n      (alu_rrr_shift op ty x y amt))\n\n;; Helper for generating i128 bitops which simply do the same operation to the\n;; hi/lo registers.\n;;\n;; TODO: Support immlogic here\n(decl i128_alu_bitop (ALUOp Type Value Value) ValueRegs)\n(rule (i128_alu_bitop op ty x y)\n      (let (\n          (x_regs ValueRegs (put_in_regs x))\n          (x_lo Reg (value_regs_get x_regs 0))\n          (x_hi Reg (value_regs_get x_regs 1))\n          (y_regs ValueRegs (put_in_regs y))\n          (y_lo Reg (value_regs_get y_regs 0))\n          (y_hi Reg (value_regs_get y_regs 1))\n        )\n        (value_regs\n          (alu_rrr op ty x_lo y_lo)\n          (alu_rrr op ty x_hi y_hi))))\n\n;; Float vector compare helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Match 32 bit float 0 value\n(decl zero_value_f32 (Ieee32) Ieee32)\n(extern extractor zero_value_f32 zero_value_f32)\n\n;; Match 64 bit float 0 value\n(decl zero_value_f64 (Ieee64) Ieee64)\n(extern extractor zero_value_f64 zero_value_f64)\n\n;; Generate comparison to zero operator from input condition code\n(decl float_cc_cmp_zero_to_vec_misc_op (FloatCC) VecMisc2)\n(extern constructor float_cc_cmp_zero_to_vec_misc_op float_cc_cmp_zero_to_vec_misc_op)\n\n(decl float_cc_cmp_zero_to_vec_misc_op_swap (FloatCC) VecMisc2)\n(extern constructor float_cc_cmp_zero_to_vec_misc_op_swap float_cc_cmp_zero_to_vec_misc_op_swap)\n\n;; Match valid generic compare to zero cases\n(decl fcmp_zero_cond (FloatCC) FloatCC)\n(extern extractor fcmp_zero_cond fcmp_zero_cond)\n\n;; Match not equal compare to zero separately as it requires two output instructions\n(decl fcmp_zero_cond_not_eq (FloatCC) FloatCC)\n(extern extractor fcmp_zero_cond_not_eq fcmp_zero_cond_not_eq)\n\n;; Helper for generating float compare to zero instructions where 2nd argument is zero\n(decl float_cmp_zero (FloatCC Reg VectorSize) Reg)\n(rule (float_cmp_zero cond rn size)\n      (vec_misc (float_cc_cmp_zero_to_vec_misc_op cond) rn size))\n\n;; Helper for generating float compare to zero instructions in case where 1st argument is zero\n(decl float_cmp_zero_swap (FloatCC Reg VectorSize) Reg)\n(rule (float_cmp_zero_swap cond rn size)\n      (vec_misc (float_cc_cmp_zero_to_vec_misc_op_swap cond) rn size))\n\n;; Helper for generating float compare equal to zero instruction\n(decl fcmeq0 (Reg VectorSize) Reg)\n(rule (fcmeq0 rn size)\n      (vec_misc (VecMisc2.Fcmeq0) rn size))\n\n;; Int vector compare helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Match integer 0 value\n(decl zero_value (Imm64) Imm64)\n(extern extractor zero_value zero_value)\n\n;; Generate comparison to zero operator from input condition code\n(decl int_cc_cmp_zero_to_vec_misc_op (IntCC) VecMisc2)\n(extern constructor int_cc_cmp_zero_to_vec_misc_op int_cc_cmp_zero_to_vec_misc_op)\n\n(decl int_cc_cmp_zero_to_vec_misc_op_swap (IntCC) VecMisc2)\n(extern constructor int_cc_cmp_zero_to_vec_misc_op_swap int_cc_cmp_zero_to_vec_misc_op_swap)\n\n;; Match valid generic compare to zero cases\n(decl icmp_zero_cond (IntCC) IntCC)\n(extern extractor icmp_zero_cond icmp_zero_cond)\n\n;; Match not equal compare to zero separately as it requires two output instructions\n(decl icmp_zero_cond_not_eq (IntCC) IntCC)\n(extern extractor icmp_zero_cond_not_eq icmp_zero_cond_not_eq)\n\n;; Helper for generating int compare to zero instructions where 2nd argument is zero\n(decl int_cmp_zero (IntCC Reg VectorSize) Reg)\n(rule (int_cmp_zero cond rn size)\n      (vec_misc (int_cc_cmp_zero_to_vec_misc_op cond) rn size))\n\n;; Helper for generating int compare to zero instructions in case where 1st argument is zero\n(decl int_cmp_zero_swap (IntCC Reg VectorSize) Reg)\n(rule (int_cmp_zero_swap cond rn size)\n      (vec_misc (int_cc_cmp_zero_to_vec_misc_op_swap cond) rn size))\n\n;; Helper for generating int compare equal to zero instruction\n(decl cmeq0 (Reg VectorSize) Reg)\n(rule (cmeq0 rn size)\n      (vec_misc (VecMisc2.Cmeq0) rn size))\n\n;; Helper for emitting `MInst.AtomicRMW` instructions.\n(decl lse_atomic_rmw (AtomicRMWOp Value Reg Type) Reg)\n(rule (lse_atomic_rmw op p r_arg2 ty)\n      (let (\n          (r_addr Reg p)\n          (dst WritableReg (temp_writable_reg ty))\n          (_ Unit (emit (MInst.AtomicRMW op r_arg2 dst r_addr ty)))\n        )\n        dst))\n\n;; Helper for emitting `MInst.AtomicCAS` instructions.\n(decl lse_atomic_cas (Reg Reg Reg Type) Reg)\n(rule (lse_atomic_cas addr expect replace ty)\n      (let (\n            (dst WritableReg (temp_writable_reg ty))\n            (_1 Unit (emit (MInst.Mov (operand_size ty) dst expect)))\n            (_2 Unit (emit (MInst.AtomicCAS dst replace addr ty)))\n          )\n          dst))\n\n;; Helper for emitting `MInst.AtomicRMWLoop` instructions.\n;; - Make sure that both args are in virtual regs, since in effect\n;; we have to do a parallel copy to get them safely to the AtomicRMW input\n;; regs, and that's not guaranteed safe if either is in a real reg.\n;; - Move the args to the preordained AtomicRMW input regs\n;; - And finally, copy the preordained AtomicRMW output reg to its destination.\n(decl atomic_rmw_loop (AtomicRMWLoopOp Value Value Type) Reg)\n(rule (atomic_rmw_loop op p arg2 ty)\n      (let (\n          (v_addr Reg (ensure_in_vreg p $I64))\n          (v_arg2 Reg (ensure_in_vreg arg2 $I64))\n          (r_addr Reg (mov64_to_real 25 v_addr))\n          (r_arg2 Reg (mov64_to_real 26 v_arg2))\n          (_ Unit (emit (MInst.AtomicRMWLoop ty op)))\n        )\n        (mov64_from_real 27)))\n\n;; Helper for emitting `MInst.AtomicCASLoop` instructions.\n;; This is very similar to, but not identical to, the AtomicRmw case.  Note\n;; that the AtomicCASLoop sequence does its own masking, so we don't need to worry\n;; about zero-extending narrow (I8/I16/I32) values here.\n;; Make sure that all three args are in virtual regs.  See corresponding comment\n;; for `atomic_rmw_loop` above.\n(decl atomic_cas_loop (Reg Reg Reg Type) Reg)\n(rule (atomic_cas_loop addr expect replace ty)\n      (let (\n          (v_addr Reg (ensure_in_vreg addr $I64))\n          (v_exp Reg (ensure_in_vreg expect $I64))\n          (v_rep Reg (ensure_in_vreg replace $I64))\n          ;; Move the args to the preordained AtomicCASLoop input regs\n          (r_addr Reg (mov64_to_real 25 v_addr))\n          (r_exp Reg (mov64_to_real 26 v_exp))\n          (r_rep Reg (mov64_to_real 28 v_rep))\n          ;; Now the AtomicCASLoop itself, implemented in the normal way, with a\n          ;; load-exclusive, store-exclusive loop\n          (_ Unit (emit (MInst.AtomicCASLoop ty)))\n        )\n        ;; And finally, copy the preordained AtomicCASLoop output reg to its destination.\n        ;; Also, x24 and x28 are trashed.\n        (mov64_from_real 27)))\n", "//! AArch64 ISA definitions: immediate constants.\n\n// Some variants are never constructed, but we still want them as options in the future.\n#[allow(dead_code)]\nuse crate::ir::types::*;\nuse crate::ir::Type;\nuse crate::isa::aarch64::inst::{OperandSize, ScalarSize};\nuse crate::machinst::{AllocationConsumer, PrettyPrint};\n\nuse core::convert::TryFrom;\nuse std::string::String;\n\n/// An immediate that represents the NZCV flags.\n#[derive(Clone, Copy, Debug)]\npub struct NZCV {\n    /// The negative condition flag.\n    n: bool,\n    /// The zero condition flag.\n    z: bool,\n    /// The carry condition flag.\n    c: bool,\n    /// The overflow condition flag.\n    v: bool,\n}\n\nimpl NZCV {\n    pub fn new(n: bool, z: bool, c: bool, v: bool) -> NZCV {\n        NZCV { n, z, c, v }\n    }\n\n    /// Bits for encoding.\n    pub fn bits(&self) -> u32 {\n        (u32::from(self.n) << 3)\n            | (u32::from(self.z) << 2)\n            | (u32::from(self.c) << 1)\n            | u32::from(self.v)\n    }\n}\n\n/// An unsigned 5-bit immediate.\n#[derive(Clone, Copy, Debug)]\npub struct UImm5 {\n    /// The value.\n    value: u8,\n}\n\nimpl UImm5 {\n    pub fn maybe_from_u8(value: u8) -> Option<UImm5> {\n        if value < 32 {\n            Some(UImm5 { value })\n        } else {\n            None\n        }\n    }\n\n    /// Bits for encoding.\n    pub fn bits(&self) -> u32 {\n        u32::from(self.value)\n    }\n}\n\n/// A signed, scaled 7-bit offset.\n#[derive(Clone, Copy, Debug)]\npub struct SImm7Scaled {\n    /// The value.\n    pub value: i16,\n    /// multiplied by the size of this type\n    pub scale_ty: Type,\n}\n\nimpl SImm7Scaled {\n    /// Create a SImm7Scaled from a raw offset and the known scale type, if\n    /// possible.\n    pub fn maybe_from_i64(value: i64, scale_ty: Type) -> Option<SImm7Scaled> {\n        assert!(scale_ty == I64 || scale_ty == I32 || scale_ty == F64 || scale_ty == I8X16);\n        let scale = scale_ty.bytes();\n        assert!(scale.is_power_of_two());\n        let scale = i64::from(scale);\n        let upper_limit = 63 * scale;\n        let lower_limit = -(64 * scale);\n        if value >= lower_limit && value <= upper_limit && (value & (scale - 1)) == 0 {\n            Some(SImm7Scaled {\n                value: i16::try_from(value).unwrap(),\n                scale_ty,\n            })\n        } else {\n            None\n        }\n    }\n\n    /// Bits for encoding.\n    pub fn bits(&self) -> u32 {\n        let ty_bytes: i16 = self.scale_ty.bytes() as i16;\n        let scaled: i16 = self.value / ty_bytes;\n        assert!(scaled <= 63 && scaled >= -64);\n        let scaled: i8 = scaled as i8;\n        let encoded: u32 = scaled as u32;\n        encoded & 0x7f\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct FPULeftShiftImm {\n    pub amount: u8,\n    pub lane_size_in_bits: u8,\n}\n\nimpl FPULeftShiftImm {\n    pub fn maybe_from_u8(amount: u8, lane_size_in_bits: u8) -> Option<Self> {\n        debug_assert!(lane_size_in_bits == 32 || lane_size_in_bits == 64);\n        if amount < lane_size_in_bits {\n            Some(Self {\n                amount,\n                lane_size_in_bits,\n            })\n        } else {\n            None\n        }\n    }\n\n    pub fn enc(&self) -> u32 {\n        debug_assert!(self.lane_size_in_bits.is_power_of_two());\n        debug_assert!(self.lane_size_in_bits > self.amount);\n        // The encoding of the immediate follows the table below,\n        // where xs encode the shift amount.\n        //\n        // | lane_size_in_bits | encoding |\n        // +------------------------------+\n        // | 8                 | 0001xxx  |\n        // | 16                | 001xxxx  |\n        // | 32                | 01xxxxx  |\n        // | 64                | 1xxxxxx  |\n        //\n        // The highest one bit is represented by `lane_size_in_bits`. Since\n        // `lane_size_in_bits` is a power of 2 and `amount` is less\n        // than `lane_size_in_bits`, they can be ORed\n        // together to produced the encoded value.\n        u32::from(self.lane_size_in_bits | self.amount)\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct FPURightShiftImm {\n    pub amount: u8,\n    pub lane_size_in_bits: u8,\n}\n\nimpl FPURightShiftImm {\n    pub fn maybe_from_u8(amount: u8, lane_size_in_bits: u8) -> Option<Self> {\n        debug_assert!(lane_size_in_bits == 32 || lane_size_in_bits == 64);\n        if amount > 0 && amount <= lane_size_in_bits {\n            Some(Self {\n                amount,\n                lane_size_in_bits,\n            })\n        } else {\n            None\n        }\n    }\n\n    pub fn enc(&self) -> u32 {\n        debug_assert_ne!(0, self.amount);\n        // The encoding of the immediate follows the table below,\n        // where xs encodes the negated shift amount.\n        //\n        // | lane_size_in_bits | encoding |\n        // +------------------------------+\n        // | 8                 | 0001xxx  |\n        // | 16                | 001xxxx  |\n        // | 32                | 01xxxxx  |\n        // | 64                | 1xxxxxx  |\n        //\n        // The shift amount is negated such that a shift ammount\n        // of 1 (in 64-bit) is encoded as 0b111111 and a shift\n        // amount of 64 is encoded as 0b000000,\n        // in the bottom 6 bits.\n        u32::from((self.lane_size_in_bits * 2) - self.amount)\n    }\n}\n\n/// a 9-bit signed offset.\n#[derive(Clone, Copy, Debug)]\npub struct SImm9 {\n    /// The value.\n    pub value: i16,\n}\n\nimpl SImm9 {\n    /// Create a signed 9-bit offset from a full-range value, if possible.\n    pub fn maybe_from_i64(value: i64) -> Option<SImm9> {\n        if value >= -256 && value <= 255 {\n            Some(SImm9 {\n                value: value as i16,\n            })\n        } else {\n            None\n        }\n    }\n\n    /// Bits for encoding.\n    pub fn bits(&self) -> u32 {\n        (self.value as u32) & 0x1ff\n    }\n\n    /// Signed value of immediate.\n    pub fn value(&self) -> i32 {\n        self.value as i32\n    }\n}\n\n/// An unsigned, scaled 12-bit offset.\n#[derive(Clone, Copy, Debug)]\npub struct UImm12Scaled {\n    /// The value.\n    pub value: u16,\n    /// multiplied by the size of this type\n    pub scale_ty: Type,\n}\n\nimpl UImm12Scaled {\n    /// Create a UImm12Scaled from a raw offset and the known scale type, if\n    /// possible.\n    pub fn maybe_from_i64(value: i64, scale_ty: Type) -> Option<UImm12Scaled> {\n        // Ensure the type is at least one byte.\n        let scale_ty = if scale_ty == B1 { B8 } else { scale_ty };\n\n        let scale = scale_ty.bytes();\n        assert!(scale.is_power_of_two());\n        let scale = scale as i64;\n        let limit = 4095 * scale;\n        if value >= 0 && value <= limit && (value & (scale - 1)) == 0 {\n            Some(UImm12Scaled {\n                value: value as u16,\n                scale_ty,\n            })\n        } else {\n            None\n        }\n    }\n\n    /// Create a zero immediate of this format.\n    pub fn zero(scale_ty: Type) -> UImm12Scaled {\n        UImm12Scaled { value: 0, scale_ty }\n    }\n\n    /// Encoded bits.\n    pub fn bits(&self) -> u32 {\n        (self.value as u32 / self.scale_ty.bytes()) & 0xfff\n    }\n\n    /// Value after scaling.\n    pub fn value(&self) -> u32 {\n        self.value as u32\n    }\n\n    /// The value type which is the scaling base.\n    pub fn scale_ty(&self) -> Type {\n        self.scale_ty\n    }\n}\n\n/// A shifted immediate value in 'imm12' format: supports 12 bits, shifted\n/// left by 0 or 12 places.\n#[derive(Copy, Clone, Debug)]\npub struct Imm12 {\n    /// The immediate bits.\n    pub bits: u16,\n    /// Whether the immediate bits are shifted left by 12 or not.\n    pub shift12: bool,\n}\n\nimpl Imm12 {\n    /// Compute a Imm12 from raw bits, if possible.\n    pub fn maybe_from_u64(val: u64) -> Option<Imm12> {\n        if val == 0 {\n            Some(Imm12 {\n                bits: 0,\n                shift12: false,\n            })\n        } else if val < 0xfff {\n            Some(Imm12 {\n                bits: val as u16,\n                shift12: false,\n            })\n        } else if val < 0xfff_000 && (val & 0xfff == 0) {\n            Some(Imm12 {\n                bits: (val >> 12) as u16,\n                shift12: true,\n            })\n        } else {\n            None\n        }\n    }\n\n    /// Create a zero immediate of this format.\n    pub fn zero() -> Self {\n        Imm12 {\n            bits: 0,\n            shift12: false,\n        }\n    }\n\n    /// Bits for 2-bit \"shift\" field in e.g. AddI.\n    pub fn shift_bits(&self) -> u32 {\n        if self.shift12 {\n            0b01\n        } else {\n            0b00\n        }\n    }\n\n    /// Bits for 12-bit \"imm\" field in e.g. AddI.\n    pub fn imm_bits(&self) -> u32 {\n        self.bits as u32\n    }\n}\n\n/// An immediate for logical instructions.\n#[derive(Copy, Clone, Debug, PartialEq)]\npub struct ImmLogic {\n    /// The actual value.\n    value: u64,\n    /// `N` flag.\n    pub n: bool,\n    /// `S` field: element size and element bits.\n    pub r: u8,\n    /// `R` field: rotate amount.\n    pub s: u8,\n    /// Was this constructed for a 32-bit or 64-bit instruction?\n    pub size: OperandSize,\n}\n\nimpl ImmLogic {\n    /// Compute an ImmLogic from raw bits, if possible.\n    pub fn maybe_from_u64(value: u64, ty: Type) -> Option<ImmLogic> {\n        // Note: This function is a port of VIXL's Assembler::IsImmLogical.\n\n        if ty != I64 && ty != I32 {\n            return None;\n        }\n        let operand_size = OperandSize::from_ty(ty);\n\n        let original_value = value;\n\n        let value = if ty == I32 {\n            // To handle 32-bit logical immediates, the very easiest thing is to repeat\n            // the input value twice to make a 64-bit word. The correct encoding of that\n            // as a logical immediate will also be the correct encoding of the 32-bit\n            // value.\n\n            // Avoid making the assumption that the most-significant 32 bits are zero by\n            // shifting the value left and duplicating it.\n            let value = value << 32;\n            value | value >> 32\n        } else {\n            value\n        };\n\n        // Logical immediates are encoded using parameters n, imm_s and imm_r using\n        // the following table:\n        //\n        //    N   imms    immr    size        S             R\n        //    1  ssssss  rrrrrr    64    UInt(ssssss)  UInt(rrrrrr)\n        //    0  0sssss  xrrrrr    32    UInt(sssss)   UInt(rrrrr)\n        //    0  10ssss  xxrrrr    16    UInt(ssss)    UInt(rrrr)\n        //    0  110sss  xxxrrr     8    UInt(sss)     UInt(rrr)\n        //    0  1110ss  xxxxrr     4    UInt(ss)      UInt(rr)\n        //    0  11110s  xxxxxr     2    UInt(s)       UInt(r)\n        // (s bits must not be all set)\n        //\n        // A pattern is constructed of size bits, where the least significant S+1 bits\n        // are set. The pattern is rotated right by R, and repeated across a 32 or\n        // 64-bit value, depending on destination register width.\n        //\n        // Put another way: the basic format of a logical immediate is a single\n        // contiguous stretch of 1 bits, repeated across the whole word at intervals\n        // given by a power of 2. To identify them quickly, we first locate the\n        // lowest stretch of 1 bits, then the next 1 bit above that; that combination\n        // is different for every logical immediate, so it gives us all the\n        // information we need to identify the only logical immediate that our input\n        // could be, and then we simply check if that's the value we actually have.\n        //\n        // (The rotation parameter does give the possibility of the stretch of 1 bits\n        // going 'round the end' of the word. To deal with that, we observe that in\n        // any situation where that happens the bitwise NOT of the value is also a\n        // valid logical immediate. So we simply invert the input whenever its low bit\n        // is set, and then we know that the rotated case can't arise.)\n        let (value, inverted) = if value & 1 == 1 {\n            (!value, true)\n        } else {\n            (value, false)\n        };\n\n        if value == 0 {\n            return None;\n        }\n\n        // The basic analysis idea: imagine our input word looks like this.\n        //\n        //    0011111000111110001111100011111000111110001111100011111000111110\n        //                                                          c  b    a\n        //                                                          |<--d-->|\n        //\n        // We find the lowest set bit (as an actual power-of-2 value, not its index)\n        // and call it a. Then we add a to our original number, which wipes out the\n        // bottommost stretch of set bits and replaces it with a 1 carried into the\n        // next zero bit. Then we look for the new lowest set bit, which is in\n        // position b, and subtract it, so now our number is just like the original\n        // but with the lowest stretch of set bits completely gone. Now we find the\n        // lowest set bit again, which is position c in the diagram above. Then we'll\n        // measure the distance d between bit positions a and c (using CLZ), and that\n        // tells us that the only valid logical immediate that could possibly be equal\n        // to this number is the one in which a stretch of bits running from a to just\n        // below b is replicated every d bits.\n        fn lowest_set_bit(value: u64) -> u64 {\n            let bit = value.trailing_zeros();\n            1u64.checked_shl(bit).unwrap_or(0)\n        }\n        let a = lowest_set_bit(value);\n        assert_ne!(0, a);\n        let value_plus_a = value.wrapping_add(a);\n        let b = lowest_set_bit(value_plus_a);\n        let value_plus_a_minus_b = value_plus_a - b;\n        let c = lowest_set_bit(value_plus_a_minus_b);\n\n        let (d, clz_a, out_n, mask) = if c != 0 {\n            // The general case, in which there is more than one stretch of set bits.\n            // Compute the repeat distance d, and set up a bitmask covering the basic\n            // unit of repetition (i.e. a word with the bottom d bits set). Also, in all\n            // of these cases the N bit of the output will be zero.\n            let clz_a = a.leading_zeros();\n            let clz_c = c.leading_zeros();\n            let d = clz_a - clz_c;\n            let mask = (1 << d) - 1;\n            (d, clz_a, 0, mask)\n        } else {\n            (64, a.leading_zeros(), 1, u64::max_value())\n        };\n\n        // If the repeat period d is not a power of two, it can't be encoded.\n        if !d.is_power_of_two() {\n            return None;\n        }\n\n        if ((b.wrapping_sub(a)) & !mask) != 0 {\n            // If the bit stretch (b - a) does not fit within the mask derived from the\n            // repeat period, then fail.\n            return None;\n        }\n\n        // The only possible option is b - a repeated every d bits. Now we're going to\n        // actually construct the valid logical immediate derived from that\n        // specification, and see if it equals our original input.\n        //\n        // To repeat a value every d bits, we multiply it by a number of the form\n        // (1 + 2^d + 2^(2d) + ...), i.e. 0x0001000100010001 or similar. These can\n        // be derived using a table lookup on CLZ(d).\n        const MULTIPLIERS: [u64; 6] = [\n            0x0000000000000001,\n            0x0000000100000001,\n            0x0001000100010001,\n            0x0101010101010101,\n            0x1111111111111111,\n            0x5555555555555555,\n        ];\n        let multiplier = MULTIPLIERS[(u64::from(d).leading_zeros() - 57) as usize];\n        let candidate = b.wrapping_sub(a) * multiplier;\n\n        if value != candidate {\n            // The candidate pattern doesn't match our input value, so fail.\n            return None;\n        }\n\n        // We have a match! This is a valid logical immediate, so now we have to\n        // construct the bits and pieces of the instruction encoding that generates\n        // it.\n\n        // Count the set bits in our basic stretch. The special case of clz(0) == -1\n        // makes the answer come out right for stretches that reach the very top of\n        // the word (e.g. numbers like 0xffffc00000000000).\n        let clz_b = if b == 0 {\n            u32::max_value() // -1\n        } else {\n            b.leading_zeros()\n        };\n        let s = clz_a.wrapping_sub(clz_b);\n\n        // Decide how many bits to rotate right by, to put the low bit of that basic\n        // stretch in position a.\n        let (s, r) = if inverted {\n            // If we inverted the input right at the start of this function, here's\n            // where we compensate: the number of set bits becomes the number of clear\n            // bits, and the rotation count is based on position b rather than position\n            // a (since b is the location of the 'lowest' 1 bit after inversion).\n            // Need wrapping for when clz_b is max_value() (for when b == 0).\n            (d - s, clz_b.wrapping_add(1) & (d - 1))\n        } else {\n            (s, (clz_a + 1) & (d - 1))\n        };\n\n        // Now we're done, except for having to encode the S output in such a way that\n        // it gives both the number of set bits and the length of the repeated\n        // segment. The s field is encoded like this:\n        //\n        //     imms    size        S\n        //    ssssss    64    UInt(ssssss)\n        //    0sssss    32    UInt(sssss)\n        //    10ssss    16    UInt(ssss)\n        //    110sss     8    UInt(sss)\n        //    1110ss     4    UInt(ss)\n        //    11110s     2    UInt(s)\n        //\n        // So we 'or' (2 * -d) with our computed s to form imms.\n        let s = ((d * 2).wrapping_neg() | (s - 1)) & 0x3f;\n        debug_assert!(u8::try_from(r).is_ok());\n        debug_assert!(u8::try_from(s).is_ok());\n        Some(ImmLogic {\n            value: original_value,\n            n: out_n != 0,\n            r: r as u8,\n            s: s as u8,\n            size: operand_size,\n        })\n    }\n\n    /// Returns bits ready for encoding: (N:1, R:6, S:6)\n    pub fn enc_bits(&self) -> u32 {\n        ((self.n as u32) << 12) | ((self.r as u32) << 6) | (self.s as u32)\n    }\n\n    /// Returns the value that this immediate represents.\n    pub fn value(&self) -> u64 {\n        self.value\n    }\n\n    /// Return an immediate for the bitwise-inverted value.\n    pub fn invert(&self) -> ImmLogic {\n        // For every ImmLogical immediate, the inverse can also be encoded.\n        Self::maybe_from_u64(!self.value, self.size.to_ty()).unwrap()\n    }\n}\n\n/// An immediate for shift instructions.\n#[derive(Copy, Clone, Debug)]\npub struct ImmShift {\n    /// 6-bit shift amount.\n    pub imm: u8,\n}\n\nimpl ImmShift {\n    /// Create an ImmShift from raw bits, if possible.\n    pub fn maybe_from_u64(val: u64) -> Option<ImmShift> {\n        if val < 64 {\n            Some(ImmShift { imm: val as u8 })\n        } else {\n            None\n        }\n    }\n\n    /// Get the immediate value.\n    pub fn value(&self) -> u8 {\n        self.imm\n    }\n}\n\n/// A 16-bit immediate for a MOVZ instruction, with a {0,16,32,48}-bit shift.\n#[derive(Clone, Copy, Debug)]\npub struct MoveWideConst {\n    /// The value.\n    pub bits: u16,\n    /// Result is `bits` shifted 16*shift bits to the left.\n    pub shift: u8,\n}\n\nimpl MoveWideConst {\n    /// Construct a MoveWideConst from an arbitrary 64-bit constant if possible.\n    pub fn maybe_from_u64(value: u64) -> Option<MoveWideConst> {\n        let mask0 = 0x0000_0000_0000_ffffu64;\n        let mask1 = 0x0000_0000_ffff_0000u64;\n        let mask2 = 0x0000_ffff_0000_0000u64;\n        let mask3 = 0xffff_0000_0000_0000u64;\n\n        if value == (value & mask0) {\n            return Some(MoveWideConst {\n                bits: (value & mask0) as u16,\n                shift: 0,\n            });\n        }\n        if value == (value & mask1) {\n            return Some(MoveWideConst {\n                bits: ((value >> 16) & mask0) as u16,\n                shift: 1,\n            });\n        }\n        if value == (value & mask2) {\n            return Some(MoveWideConst {\n                bits: ((value >> 32) & mask0) as u16,\n                shift: 2,\n            });\n        }\n        if value == (value & mask3) {\n            return Some(MoveWideConst {\n                bits: ((value >> 48) & mask0) as u16,\n                shift: 3,\n            });\n        }\n        None\n    }\n\n    pub fn maybe_with_shift(imm: u16, shift: u8) -> Option<MoveWideConst> {\n        let shift_enc = shift / 16;\n        if shift_enc > 3 {\n            None\n        } else {\n            Some(MoveWideConst {\n                bits: imm,\n                shift: shift_enc,\n            })\n        }\n    }\n}\n\n/// Advanced SIMD modified immediate as used by MOVI/MVNI.\n#[derive(Clone, Copy, Debug, PartialEq)]\npub struct ASIMDMovModImm {\n    imm: u8,\n    shift: u8,\n    is_64bit: bool,\n    shift_ones: bool,\n}\n\nimpl ASIMDMovModImm {\n    /// Construct an ASIMDMovModImm from an arbitrary 64-bit constant, if possible.\n    /// Note that the bits in `value` outside of the range specified by `size` are\n    /// ignored; for example, in the case of `ScalarSize::Size8` all bits above the\n    /// lowest 8 are ignored.\n    pub fn maybe_from_u64(value: u64, size: ScalarSize) -> Option<ASIMDMovModImm> {\n        match size {\n            ScalarSize::Size8 => Some(ASIMDMovModImm {\n                imm: value as u8,\n                shift: 0,\n                is_64bit: false,\n                shift_ones: false,\n            }),\n            ScalarSize::Size16 => {\n                let value = value as u16;\n\n                if value >> 8 == 0 {\n                    Some(ASIMDMovModImm {\n                        imm: value as u8,\n                        shift: 0,\n                        is_64bit: false,\n                        shift_ones: false,\n                    })\n                } else if value as u8 == 0 {\n                    Some(ASIMDMovModImm {\n                        imm: (value >> 8) as u8,\n                        shift: 8,\n                        is_64bit: false,\n                        shift_ones: false,\n                    })\n                } else {\n                    None\n                }\n            }\n            ScalarSize::Size32 => {\n                let value = value as u32;\n\n                // Value is of the form 0x00MMFFFF.\n                if value & 0xFF00FFFF == 0x0000FFFF {\n                    let imm = (value >> 16) as u8;\n\n                    Some(ASIMDMovModImm {\n                        imm,\n                        shift: 16,\n                        is_64bit: false,\n                        shift_ones: true,\n                    })\n                // Value is of the form 0x0000MMFF.\n                } else if value & 0xFFFF00FF == 0x000000FF {\n                    let imm = (value >> 8) as u8;\n\n                    Some(ASIMDMovModImm {\n                        imm,\n                        shift: 8,\n                        is_64bit: false,\n                        shift_ones: true,\n                    })\n                } else {\n                    // Of the 4 bytes, at most one is non-zero.\n                    for shift in (0..32).step_by(8) {\n                        if value & (0xFF << shift) == value {\n                            return Some(ASIMDMovModImm {\n                                imm: (value >> shift) as u8,\n                                shift,\n                                is_64bit: false,\n                                shift_ones: false,\n                            });\n                        }\n                    }\n\n                    None\n                }\n            }\n            ScalarSize::Size64 => {\n                let mut imm = 0u8;\n\n                // Check if all bytes are either 0 or 0xFF.\n                for i in 0..8 {\n                    let b = (value >> (i * 8)) as u8;\n\n                    if b == 0 || b == 0xFF {\n                        imm |= (b & 1) << i;\n                    } else {\n                        return None;\n                    }\n                }\n\n                Some(ASIMDMovModImm {\n                    imm,\n                    shift: 0,\n                    is_64bit: true,\n                    shift_ones: false,\n                })\n            }\n            _ => None,\n        }\n    }\n\n    /// Create a zero immediate of this format.\n    pub fn zero(size: ScalarSize) -> Self {\n        ASIMDMovModImm {\n            imm: 0,\n            shift: 0,\n            is_64bit: size == ScalarSize::Size64,\n            shift_ones: false,\n        }\n    }\n\n    /// Returns the value that this immediate represents.\n    pub fn value(&self) -> (u8, u32, bool) {\n        (self.imm, self.shift as u32, self.shift_ones)\n    }\n}\n\n/// Advanced SIMD modified immediate as used by the vector variant of FMOV.\n#[derive(Clone, Copy, Debug, PartialEq)]\npub struct ASIMDFPModImm {\n    imm: u8,\n    is_64bit: bool,\n}\n\nimpl ASIMDFPModImm {\n    /// Construct an ASIMDFPModImm from an arbitrary 64-bit constant, if possible.\n    pub fn maybe_from_u64(value: u64, size: ScalarSize) -> Option<ASIMDFPModImm> {\n        // In all cases immediates are encoded as an 8-bit number 0b_abcdefgh;\n        // let `D` be the inverse of the digit `d`.\n        match size {\n            ScalarSize::Size32 => {\n                // In this case the representable immediates are 32-bit numbers of the form\n                // 0b_aBbb_bbbc_defg_h000 shifted to the left by 16.\n                let value = value as u32;\n                let b0_5 = (value >> 19) & 0b111111;\n                let b6 = (value >> 19) & (1 << 6);\n                let b7 = (value >> 24) & (1 << 7);\n                let imm = (b0_5 | b6 | b7) as u8;\n\n                if value == Self::value32(imm) {\n                    Some(ASIMDFPModImm {\n                        imm,\n                        is_64bit: false,\n                    })\n                } else {\n                    None\n                }\n            }\n            ScalarSize::Size64 => {\n                // In this case the representable immediates are 64-bit numbers of the form\n                // 0b_aBbb_bbbb_bbcd_efgh shifted to the left by 48.\n                let b0_5 = (value >> 48) & 0b111111;\n                let b6 = (value >> 48) & (1 << 6);\n                let b7 = (value >> 56) & (1 << 7);\n                let imm = (b0_5 | b6 | b7) as u8;\n\n                if value == Self::value64(imm) {\n                    Some(ASIMDFPModImm {\n                        imm,\n                        is_64bit: true,\n                    })\n                } else {\n                    None\n                }\n            }\n            _ => None,\n        }\n    }\n\n    /// Returns bits ready for encoding.\n    pub fn enc_bits(&self) -> u8 {\n        self.imm\n    }\n\n    /// Returns the 32-bit value that corresponds to an 8-bit encoding.\n    fn value32(imm: u8) -> u32 {\n        let imm = imm as u32;\n        let b0_5 = imm & 0b111111;\n        let b6 = (imm >> 6) & 1;\n        let b6_inv = b6 ^ 1;\n        let b7 = (imm >> 7) & 1;\n\n        b0_5 << 19 | (b6 * 0b11111) << 25 | b6_inv << 30 | b7 << 31\n    }\n\n    /// Returns the 64-bit value that corresponds to an 8-bit encoding.\n    fn value64(imm: u8) -> u64 {\n        let imm = imm as u64;\n        let b0_5 = imm & 0b111111;\n        let b6 = (imm >> 6) & 1;\n        let b6_inv = b6 ^ 1;\n        let b7 = (imm >> 7) & 1;\n\n        b0_5 << 48 | (b6 * 0b11111111) << 54 | b6_inv << 62 | b7 << 63\n    }\n}\n\nimpl PrettyPrint for NZCV {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        let fmt = |c: char, v| if v { c.to_ascii_uppercase() } else { c };\n        format!(\n            \"#{}{}{}{}\",\n            fmt('n', self.n),\n            fmt('z', self.z),\n            fmt('c', self.c),\n            fmt('v', self.v)\n        )\n    }\n}\n\nimpl PrettyPrint for UImm5 {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.value)\n    }\n}\n\nimpl PrettyPrint for Imm12 {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        let shift = if self.shift12 { 12 } else { 0 };\n        let value = u32::from(self.bits) << shift;\n        format!(\"#{}\", value)\n    }\n}\n\nimpl PrettyPrint for SImm7Scaled {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.value)\n    }\n}\n\nimpl PrettyPrint for FPULeftShiftImm {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.amount)\n    }\n}\n\nimpl PrettyPrint for FPURightShiftImm {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.amount)\n    }\n}\n\nimpl PrettyPrint for SImm9 {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.value)\n    }\n}\n\nimpl PrettyPrint for UImm12Scaled {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.value)\n    }\n}\n\nimpl PrettyPrint for ImmLogic {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.value())\n    }\n}\n\nimpl PrettyPrint for ImmShift {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.imm)\n    }\n}\n\nimpl PrettyPrint for MoveWideConst {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        if self.shift == 0 {\n            format!(\"#{}\", self.bits)\n        } else {\n            format!(\"#{}, LSL #{}\", self.bits, self.shift * 16)\n        }\n    }\n}\n\nimpl PrettyPrint for ASIMDMovModImm {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        if self.is_64bit {\n            debug_assert_eq!(self.shift, 0);\n\n            let enc_imm = self.imm as i8;\n            let mut imm = 0u64;\n\n            for i in 0..8 {\n                let b = (enc_imm >> i) & 1;\n\n                imm |= (-b as u8 as u64) << (i * 8);\n            }\n\n            format!(\"#{}\", imm)\n        } else if self.shift == 0 {\n            format!(\"#{}\", self.imm)\n        } else {\n            let shift_type = if self.shift_ones { \"MSL\" } else { \"LSL\" };\n            format!(\"#{}, {} #{}\", self.imm, shift_type, self.shift)\n        }\n    }\n}\n\nimpl PrettyPrint for ASIMDFPModImm {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        if self.is_64bit {\n            format!(\"#{}\", f64::from_bits(Self::value64(self.imm)))\n        } else {\n            format!(\"#{}\", f32::from_bits(Self::value32(self.imm)))\n        }\n    }\n}\n\n#[cfg(test)]\nmod test {\n    use super::*;\n\n    #[test]\n    fn imm_logical_test() {\n        assert_eq!(None, ImmLogic::maybe_from_u64(0, I64));\n        assert_eq!(None, ImmLogic::maybe_from_u64(u64::max_value(), I64));\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 1,\n                n: true,\n                r: 0,\n                s: 0,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(1, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 2,\n                n: true,\n                r: 63,\n                s: 0,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(2, I64)\n        );\n\n        assert_eq!(None, ImmLogic::maybe_from_u64(5, I64));\n\n        assert_eq!(None, ImmLogic::maybe_from_u64(11, I64));\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 248,\n                n: true,\n                r: 61,\n                s: 4,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(248, I64)\n        );\n\n        assert_eq!(None, ImmLogic::maybe_from_u64(249, I64));\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 1920,\n                n: true,\n                r: 57,\n                s: 3,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(1920, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x7ffe,\n                n: true,\n                r: 63,\n                s: 13,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x7ffe, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x30000,\n                n: true,\n                r: 48,\n                s: 1,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x30000, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x100000,\n                n: true,\n                r: 44,\n                s: 0,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x100000, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: u64::max_value() - 1,\n                n: true,\n                r: 63,\n                s: 62,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(u64::max_value() - 1, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0xaaaaaaaaaaaaaaaa,\n                n: false,\n                r: 1,\n                s: 60,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0xaaaaaaaaaaaaaaaa, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x8181818181818181,\n                n: false,\n                r: 1,\n                s: 49,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x8181818181818181, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0xffc3ffc3ffc3ffc3,\n                n: false,\n                r: 10,\n                s: 43,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0xffc3ffc3ffc3ffc3, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x100000001,\n                n: false,\n                r: 0,\n                s: 0,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x100000001, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x1111111111111111,\n                n: false,\n                r: 0,\n                s: 56,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x1111111111111111, I64)\n        );\n\n        for n in 0..2 {\n            let types = if n == 0 { vec![I64, I32] } else { vec![I64] };\n            for s in 0..64 {\n                for r in 0..64 {\n                    let imm = get_logical_imm(n, s, r);\n                    for &ty in &types {\n                        match ImmLogic::maybe_from_u64(imm, ty) {\n                            Some(ImmLogic { value, .. }) => {\n                                assert_eq!(imm, value);\n                                ImmLogic::maybe_from_u64(!value, ty).unwrap();\n                            }\n                            None => assert_eq!(0, imm),\n                        };\n                    }\n                }\n            }\n        }\n    }\n\n    // Repeat a value that has `width` bits, across a 64-bit value.\n    fn repeat(value: u64, width: u64) -> u64 {\n        let mut result = value & ((1 << width) - 1);\n        let mut i = width;\n        while i < 64 {\n            result |= result << i;\n            i *= 2;\n        }\n        result\n    }\n\n    // Get the logical immediate, from the encoding N/R/S bits.\n    fn get_logical_imm(n: u32, s: u32, r: u32) -> u64 {\n        // An integer is constructed from the n, imm_s and imm_r bits according to\n        // the following table:\n        //\n        //  N   imms    immr    size        S             R\n        //  1  ssssss  rrrrrr    64    UInt(ssssss)  UInt(rrrrrr)\n        //  0  0sssss  xrrrrr    32    UInt(sssss)   UInt(rrrrr)\n        //  0  10ssss  xxrrrr    16    UInt(ssss)    UInt(rrrr)\n        //  0  110sss  xxxrrr     8    UInt(sss)     UInt(rrr)\n        //  0  1110ss  xxxxrr     4    UInt(ss)      UInt(rr)\n        //  0  11110s  xxxxxr     2    UInt(s)       UInt(r)\n        // (s bits must not be all set)\n        //\n        // A pattern is constructed of size bits, where the least significant S+1\n        // bits are set. The pattern is rotated right by R, and repeated across a\n        // 64-bit value.\n\n        if n == 1 {\n            if s == 0x3f {\n                return 0;\n            }\n            let bits = (1u64 << (s + 1)) - 1;\n            bits.rotate_right(r)\n        } else {\n            if (s >> 1) == 0x1f {\n                return 0;\n            }\n            let mut width = 0x20;\n            while width >= 0x2 {\n                if (s & width) == 0 {\n                    let mask = width - 1;\n                    if (s & mask) == mask {\n                        return 0;\n                    }\n                    let bits = (1u64 << ((s & mask) + 1)) - 1;\n                    return repeat(bits.rotate_right(r & mask), width.into());\n                }\n                width >>= 1;\n            }\n            unreachable!();\n        }\n    }\n\n    #[test]\n    fn asimd_fp_mod_imm_test() {\n        assert_eq!(None, ASIMDFPModImm::maybe_from_u64(0, ScalarSize::Size32));\n        assert_eq!(\n            None,\n            ASIMDFPModImm::maybe_from_u64(0.013671875_f32.to_bits() as u64, ScalarSize::Size32)\n        );\n        assert_eq!(None, ASIMDFPModImm::maybe_from_u64(0, ScalarSize::Size64));\n        assert_eq!(\n            None,\n            ASIMDFPModImm::maybe_from_u64(10000_f64.to_bits(), ScalarSize::Size64)\n        );\n    }\n\n    #[test]\n    fn asimd_mov_mod_imm_test() {\n        assert_eq!(\n            None,\n            ASIMDMovModImm::maybe_from_u64(513, ScalarSize::Size16)\n        );\n        assert_eq!(\n            None,\n            ASIMDMovModImm::maybe_from_u64(4278190335, ScalarSize::Size32)\n        );\n        assert_eq!(\n            None,\n            ASIMDMovModImm::maybe_from_u64(8388608, ScalarSize::Size64)\n        );\n\n        assert_eq!(\n            Some(ASIMDMovModImm {\n                imm: 66,\n                shift: 16,\n                is_64bit: false,\n                shift_ones: true,\n            }),\n            ASIMDMovModImm::maybe_from_u64(4390911, ScalarSize::Size32)\n        );\n    }\n}\n", ";; aarch64 instruction selection and CLIF-to-MachInst lowering.\n\n;; The main lowering constructor term: takes a clif `Inst` and returns the\n;; register(s) within which the lowered instruction's result values live.\n(decl lower (Inst) InstOutput)\n\n;;;; Rules for `iconst` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty (iconst (u64_from_imm64 n))))\n      (imm ty n))\n\n;;;; Rules for `bconst` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty (bconst $false)))\n      (imm ty 0))\n\n(rule (lower (has_type ty (bconst $true)))\n      (imm ty 1))\n\n;;;; Rules for `null` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty (null)))\n      (imm ty 0))\n\n;;;; Rules for `iadd` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; `i64` and smaller\n\n;; Base case, simply adding things in registers.\n(rule (lower (has_type (fits_in_64 ty) (iadd x y)))\n      (add ty  x y))\n\n;; Special cases for when one operand is an immediate that fits in 12 bits.\n(rule (lower (has_type (fits_in_64 ty) (iadd x (imm12_from_value y))))\n      (add_imm ty x y))\n\n(rule (lower (has_type (fits_in_64 ty) (iadd (imm12_from_value x) y)))\n      (add_imm ty y x))\n\n;; Same as the previous special cases, except we can switch the addition to a\n;; subtraction if the negated immediate fits in 12 bits.\n(rule (lower (has_type (fits_in_64 ty) (iadd x (imm12_from_negated_value y))))\n      (sub_imm ty x y))\n\n(rule (lower (has_type (fits_in_64 ty) (iadd (imm12_from_negated_value x) y)))\n      (sub_imm ty y x))\n\n;; Special cases for when we're adding an extended register where the extending\n;; operation can get folded into the add itself.\n(rule (lower (has_type (fits_in_64 ty) (iadd x (extended_value_from_value y))))\n      (add_extend ty x y))\n\n(rule (lower (has_type (fits_in_64 ty) (iadd (extended_value_from_value x) y)))\n      (add_extend ty y x))\n\n;; Special cases for when we're adding the shift of a different\n;; register by a constant amount and the shift can get folded into the add.\n(rule (lower (has_type (fits_in_64 ty)\n                       (iadd x (ishl y (iconst k)))))\n      (if-let amt (lshl_from_imm64 ty k))\n      (add_shift ty x y amt))\n\n(rule (lower (has_type (fits_in_64 ty)\n                       (iadd (ishl x (iconst k)) y)))\n      (if-let amt (lshl_from_imm64 ty k))\n      (add_shift ty y x amt))\n\n;; Fold an `iadd` and `imul` combination into a `madd` instruction.\n(rule (lower (has_type (fits_in_64 ty) (iadd x (imul y z))))\n      (madd ty y z x))\n\n(rule (lower (has_type (fits_in_64 ty) (iadd (imul x y) z)))\n      (madd ty x y z))\n\n;; Fold an `isub` and `imul` combination into a `msub` instruction.\n(rule (lower (has_type (fits_in_64 ty) (isub x (imul y z))))\n      (msub ty y z x))\n\n;; vectors\n\n(rule (lower (has_type ty @ (multi_lane _ _) (iadd x y)))\n      (add_vec x y (vector_size ty)))\n\n;; `i128`\n(rule (lower (has_type $I128 (iadd x y)))\n      (let\n          ;; Get the high/low registers for `x`.\n          ((x_regs ValueRegs x)\n           (x_lo Reg (value_regs_get x_regs 0))\n           (x_hi Reg (value_regs_get x_regs 1))\n\n           ;; Get the high/low registers for `y`.\n           (y_regs ValueRegs y)\n           (y_lo Reg (value_regs_get y_regs 0))\n           (y_hi Reg (value_regs_get y_regs 1)))\n        ;; the actual addition is `adds` followed by `adc` which comprises the\n        ;; low/high bits of the result\n        (with_flags\n          (add_with_flags_paired $I64 x_lo y_lo)\n          (adc_paired $I64 x_hi y_hi))))\n\n;;;; Rules for `swizzle` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type vec_i128_ty (swizzle rn rm)))\n      (vec_tbl rn rm #f))\n\n;;;; Rules for `isplit` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I64 (isplit x)))\n      (let\n          ((x_regs ValueRegs x)\n           (x_lo ValueRegs (value_regs_get x_regs 0))\n           (x_hi ValueRegs (value_regs_get x_regs 1)))\n        (output_pair x_lo x_hi)))\n\n;;;; Rules for `iconcat` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I128 (iconcat lo hi)))\n      (output (value_regs lo hi)))\n\n;;;; Rules for `scalar_to_vector` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $F32X4 (scalar_to_vector x)))\n      (fpu_extend x (ScalarSize.Size32)))\n\n(rule (lower (has_type $F64X2 (scalar_to_vector x)))\n      (fpu_extend x (ScalarSize.Size64)))\n\n(rule (lower (scalar_to_vector x @ (value_type (ty_int_bool_64 _))))\n      (mov_to_fpu x (ScalarSize.Size64)))\n\n(rule (lower (scalar_to_vector x @ (value_type (int_bool_fits_in_32 _))))\n      (mov_to_fpu (put_in_reg_zext32 x) (ScalarSize.Size32)))\n\n;;;; Rules for `iadd_pairwise` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I16X8 (iadd_pairwise (swiden_low x) (swiden_high y))))\n      (if-let z (same_value x y))\n      (saddlp8 z))\n\n(rule (lower (has_type $I32X4 (iadd_pairwise (swiden_low x) (swiden_high y))))\n      (if-let z (same_value x y))\n      (saddlp16 z))\n\n(rule (lower (has_type $I16X8 (iadd_pairwise (uwiden_low x) (uwiden_high y))))\n      (if-let z (same_value x y))\n      (uaddlp8 z))\n\n(rule (lower (has_type $I32X4 (iadd_pairwise (uwiden_low x) (uwiden_high y))))\n      (if-let z (same_value x y))\n      (uaddlp16 z))\n\n(rule (lower (has_type ty (iadd_pairwise x y)))\n      (addp x y (vector_size ty)))\n\n;;;; Rules for `iabs` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (iabs x)))\n      (vec_abs x (vector_size ty)))\n\n(rule (lower (has_type $I64 (iabs x)))\n      (abs (OperandSize.Size64) x))\n\n(rule (lower (has_type (fits_in_32 ty) (iabs x)))\n      (abs (OperandSize.Size32) (put_in_reg_sext32 x)))\n\n;;;; Rules for `fadd` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fadd rn rm)))\n      (vec_rrr (VecALUOp.Fadd) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fadd rn rm)))\n      (fpu_rrr (FPUOp2.Add) rn rm (scalar_size ty)))\n\n;;;; Rules for `fsub` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fsub rn rm)))\n      (vec_rrr (VecALUOp.Fsub) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fsub rn rm)))\n      (fpu_rrr (FPUOp2.Sub) rn rm (scalar_size ty)))\n\n;;;; Rules for `fmul` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fmul rn rm)))\n      (vec_rrr (VecALUOp.Fmul) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fmul rn rm)))\n      (fpu_rrr (FPUOp2.Mul) rn rm (scalar_size ty)))\n\n;;;; Rules for `fdiv` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fdiv rn rm)))\n      (vec_rrr (VecALUOp.Fdiv) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fdiv rn rm)))\n      (fpu_rrr (FPUOp2.Div) rn rm (scalar_size ty)))\n\n;;;; Rules for `fmin` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fmin rn rm)))\n      (vec_rrr (VecALUOp.Fmin) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fmin rn rm)))\n      (fpu_rrr (FPUOp2.Min) rn rm (scalar_size ty)))\n\n;;;; Rules for `fmax` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fmax rn rm)))\n      (vec_rrr (VecALUOp.Fmax) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fmax rn rm)))\n      (fpu_rrr (FPUOp2.Max) rn rm (scalar_size ty)))\n\n;;;; Rules for `fmin_pseudo` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fmin_pseudo rm rn)))\n      (bsl ty (vec_rrr (VecALUOp.Fcmgt) rm rn (vector_size ty)) rn rm))\n\n(rule (lower (has_type (ty_scalar_float ty) (fmin_pseudo rm rn)))\n      (with_flags (fpu_cmp (scalar_size ty) rm rn)\n                  (fpu_csel ty (Cond.Gt) rn rm)))\n\n;;;; Rules for `fmax_pseudo` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fmax_pseudo rm rn)))\n      (bsl ty (vec_rrr (VecALUOp.Fcmgt) rn rm (vector_size ty)) rn rm))\n\n(rule (lower (has_type (ty_scalar_float ty) (fmax_pseudo rm rn)))\n      (with_flags (fpu_cmp (scalar_size ty) rn rm)\n                  (fpu_csel ty (Cond.Gt) rn rm)))\n\n;;;; Rules for `isub` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; `i64` and smaller\n\n;; Base case, simply subtracting things in registers.\n(rule (lower (has_type (fits_in_64 ty) (isub x y)))\n      (sub ty x y))\n\n;; Special case for when one operand is an immediate that fits in 12 bits.\n(rule (lower (has_type (fits_in_64 ty) (isub x (imm12_from_value y))))\n      (sub_imm ty x y))\n\n;; Same as the previous special case, except we can switch the subtraction to an\n;; addition if the negated immediate fits in 12 bits.\n(rule (lower (has_type (fits_in_64 ty) (isub x (imm12_from_negated_value y))))\n      (add_imm ty x y))\n\n;; Special cases for when we're subtracting an extended register where the\n;; extending operation can get folded into the sub itself.\n(rule (lower (has_type (fits_in_64 ty) (isub x (extended_value_from_value y))))\n      (sub_extend ty x y))\n\n;; Finally a special case for when we're subtracting the shift of a different\n;; register by a constant amount and the shift can get folded into the sub.\n(rule (lower (has_type (fits_in_64 ty)\n                       (isub x (ishl y (iconst k)))))\n      (if-let amt (lshl_from_imm64 ty k))\n      (sub_shift ty x y amt))\n\n;; vectors\n(rule (lower (has_type ty @ (multi_lane _ _) (isub x y)))\n      (sub_vec x y (vector_size ty)))\n\n;; `i128`\n(rule (lower (has_type $I128 (isub x y)))\n      (let\n          ;; Get the high/low registers for `x`.\n          ((x_regs ValueRegs x)\n           (x_lo Reg (value_regs_get x_regs 0))\n           (x_hi Reg (value_regs_get x_regs 1))\n\n           ;; Get the high/low registers for `y`.\n           (y_regs ValueRegs y)\n           (y_lo Reg (value_regs_get y_regs 0))\n           (y_hi Reg (value_regs_get y_regs 1)))\n        ;; the actual subtraction is `subs` followed by `sbc` which comprises\n        ;; the low/high bits of the result\n        (with_flags\n          (sub_with_flags_paired $I64 x_lo y_lo)\n          (sbc_paired $I64 x_hi y_hi))))\n\n;;;; Rules for `uadd_sat` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_vec128 ty) (uadd_sat x y)))\n      (uqadd x y (vector_size ty)))\n\n;;;; Rules for `sadd_sat` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_vec128 ty) (sadd_sat x y)))\n      (sqadd x y (vector_size ty)))\n\n;;;; Rules for `usub_sat` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_vec128 ty) (usub_sat x y)))\n      (uqsub x y (vector_size ty)))\n\n;;;; Rules for `ssub_sat` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_vec128 ty) (ssub_sat x y)))\n      (sqsub x y (vector_size ty)))\n\n;;;; Rules for `ineg` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; `i64` and smaller.\n(rule (lower (has_type (fits_in_64 ty) (ineg x)))\n      (sub ty (zero_reg) x))\n\n;; vectors.\n(rule (lower (has_type (ty_vec128 ty) (ineg x)))\n      (neg x (vector_size ty)))\n\n;;;; Rules for `imul` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; `i64` and smaller.\n(rule (lower (has_type (fits_in_64 ty) (imul x y)))\n      (madd ty x y (zero_reg)))\n\n;; `i128`.\n(rule (lower (has_type $I128 (imul x y)))\n      (let\n          ;; Get the high/low registers for `x`.\n          ((x_regs ValueRegs x)\n           (x_lo Reg (value_regs_get x_regs 0))\n           (x_hi Reg (value_regs_get x_regs 1))\n\n           ;; Get the high/low registers for `y`.\n           (y_regs ValueRegs y)\n           (y_lo Reg (value_regs_get y_regs 0))\n           (y_hi Reg (value_regs_get y_regs 1))\n\n           ;; 128bit mul formula:\n           ;;   dst_lo = x_lo * y_lo\n           ;;   dst_hi = umulhi(x_lo, y_lo) + (x_lo * y_hi) + (x_hi * y_lo)\n           ;;\n           ;; We can convert the above formula into the following\n           ;; umulh   dst_hi, x_lo, y_lo\n           ;; madd    dst_hi, x_lo, y_hi, dst_hi\n           ;; madd    dst_hi, x_hi, y_lo, dst_hi\n           ;; madd    dst_lo, x_lo, y_lo, zero\n           (dst_hi1 Reg (umulh $I64 x_lo y_lo))\n           (dst_hi2 Reg (madd $I64 x_lo y_hi dst_hi1))\n           (dst_hi Reg (madd $I64 x_hi y_lo dst_hi2))\n           (dst_lo Reg (madd $I64 x_lo y_lo (zero_reg))))\n        (value_regs dst_lo dst_hi)))\n\n;; Case for i8x16, i16x8, and i32x4.\n(rule (lower (has_type (ty_vec128 ty @ (not_i64x2)) (imul x y)))\n      (mul x y (vector_size ty)))\n\n;; Special lowering for i64x2.\n;;\n;; This I64X2 multiplication is performed with several 32-bit\n;; operations.\n;;\n;; 64-bit numbers x and y, can be represented as:\n;;   x = a + 2^32(b)\n;;   y = c + 2^32(d)\n;;\n;; A 64-bit multiplication is:\n;;   x * y = ac + 2^32(ad + bc) + 2^64(bd)\n;; note: `2^64(bd)` can be ignored, the value is too large to fit in\n;; 64 bits.\n;;\n;; This sequence implements a I64X2 multiply, where the registers\n;; `rn` and `rm` are split up into 32-bit components:\n;;   rn = |d|c|b|a|\n;;   rm = |h|g|f|e|\n;;\n;;   rn * rm = |cg + 2^32(ch + dg)|ae + 2^32(af + be)|\n;;\n;;  The sequence is:\n;;  rev64 rd.4s, rm.4s\n;;  mul rd.4s, rd.4s, rn.4s\n;;  xtn tmp1.2s, rn.2d\n;;  addp rd.4s, rd.4s, rd.4s\n;;  xtn tmp2.2s, rm.2d\n;;  shll rd.2d, rd.2s, #32\n;;  umlal rd.2d, tmp2.2s, tmp1.2s\n(rule (lower (has_type $I64X2 (imul x y)))\n      (let ((rn Reg x)\n            (rm Reg y)\n            ;; Reverse the 32-bit elements in the 64-bit words.\n            ;;   rd = |g|h|e|f|\n            (rev Reg (rev64 rm (VectorSize.Size32x4)))\n\n            ;; Calculate the high half components.\n            ;;   rd = |dg|ch|be|af|\n            ;;\n            ;; Note that this 32-bit multiply of the high half\n            ;; discards the bits that would overflow, same as\n            ;; if 64-bit operations were used. Also the Shll\n            ;; below would shift out the overflow bits anyway.\n            (mul Reg (mul rev rn (VectorSize.Size32x4)))\n\n            ;; Extract the low half components of rn.\n            ;;   tmp1 = |c|a|\n            (tmp1 Reg (xtn64 rn $false))\n\n            ;; Sum the respective high half components.\n            ;;   rd = |dg+ch|be+af||dg+ch|be+af|\n            (sum Reg (addp mul mul (VectorSize.Size32x4)))\n\n            ;; Extract the low half components of rm.\n            ;;   tmp2 = |g|e|\n            (tmp2 Reg (xtn64 rm $false))\n\n            ;; Shift the high half components, into the high half.\n            ;;   rd = |dg+ch << 32|be+af << 32|\n            (shift Reg (shll32 sum $false))\n\n            ;; Multiply the low components together, and accumulate with the high\n            ;; half.\n            ;;   rd = |rd[1] + cg|rd[0] + ae|\n            (result Reg (umlal32 shift tmp2 tmp1 $false)))\n        result))\n\n;; Special case for `i16x8.extmul_low_i8x16_s`.\n(rule (lower (has_type $I16X8\n                       (imul (swiden_low x @ (value_type $I8X16))\n                             (swiden_low y @ (value_type $I8X16)))))\n      (smull8 x y $false))\n\n;; Special case for `i16x8.extmul_high_i8x16_s`.\n(rule (lower (has_type $I16X8\n                       (imul (swiden_high x @ (value_type $I8X16))\n                             (swiden_high y @ (value_type $I8X16)))))\n      (smull8 x y $true))\n\n;; Special case for `i16x8.extmul_low_i8x16_u`.\n(rule (lower (has_type $I16X8\n                       (imul (uwiden_low x @ (value_type $I8X16))\n                             (uwiden_low y @ (value_type $I8X16)))))\n      (umull8 x y $false))\n\n;; Special case for `i16x8.extmul_high_i8x16_u`.\n(rule (lower (has_type $I16X8\n                       (imul (uwiden_high x @ (value_type $I8X16))\n                             (uwiden_high y @ (value_type $I8X16)))))\n      (umull8 x y $true))\n\n;; Special case for `i32x4.extmul_low_i16x8_s`.\n(rule (lower (has_type $I32X4\n                       (imul (swiden_low x @ (value_type $I16X8))\n                             (swiden_low y @ (value_type $I16X8)))))\n      (smull16 x y $false))\n\n;; Special case for `i32x4.extmul_high_i16x8_s`.\n(rule (lower (has_type $I32X4\n                       (imul (swiden_high x @ (value_type $I16X8))\n                             (swiden_high y @ (value_type $I16X8)))))\n      (smull16 x y $true))\n\n;; Special case for `i32x4.extmul_low_i16x8_u`.\n(rule (lower (has_type $I32X4\n                       (imul (uwiden_low x @ (value_type $I16X8))\n                             (uwiden_low y @ (value_type $I16X8)))))\n      (umull16 x y $false))\n\n;; Special case for `i32x4.extmul_high_i16x8_u`.\n(rule (lower (has_type $I32X4\n                       (imul (uwiden_high x @ (value_type $I16X8))\n                             (uwiden_high y @ (value_type $I16X8)))))\n      (umull16 x y $true))\n\n;; Special case for `i64x2.extmul_low_i32x4_s`.\n(rule (lower (has_type $I64X2\n                       (imul (swiden_low x @ (value_type $I32X4))\n                             (swiden_low y @ (value_type $I32X4)))))\n      (smull32 x y $false))\n\n;; Special case for `i64x2.extmul_high_i32x4_s`.\n(rule (lower (has_type $I64X2\n                       (imul (swiden_high x @ (value_type $I32X4))\n                             (swiden_high y @ (value_type $I32X4)))))\n      (smull32 x y $true))\n\n;; Special case for `i64x2.extmul_low_i32x4_u`.\n(rule (lower (has_type $I64X2\n                       (imul (uwiden_low x @ (value_type $I32X4))\n                             (uwiden_low y @ (value_type $I32X4)))))\n      (umull32 x y $false))\n\n;; Special case for `i64x2.extmul_high_i32x4_u`.\n(rule (lower (has_type $I64X2\n                       (imul (uwiden_high x @ (value_type $I32X4))\n                             (uwiden_high y @ (value_type $I32X4)))))\n      (umull32 x y $true))\n\n;;;; Rules for `smulhi` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I64 (smulhi x y)))\n      (smulh $I64 x y))\n\n(rule (lower (has_type (fits_in_32 ty) (smulhi x y)))\n      (let ((x64 Reg (put_in_reg_sext64 x))\n            (y64 Reg (put_in_reg_sext64 y))\n            (mul Reg (madd $I64 x64 y64 (zero_reg)))\n            (result Reg (asr_imm $I64 mul (imm_shift_from_u8 (ty_bits ty)))))\n        result))\n\n;;;; Rules for `umulhi` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I64 (umulhi x y)))\n      (umulh $I64 x y))\n\n(rule (lower (has_type (fits_in_32 ty) (umulhi x y)))\n      (let (\n          (x64 Reg (put_in_reg_zext64 x))\n          (y64 Reg (put_in_reg_zext64 y))\n          (mul Reg (madd $I64 x64 y64 (zero_reg)))\n          (result Reg (lsr_imm $I64 mul (imm_shift_from_u8 (ty_bits ty))))\n        )\n        (value_reg result)))\n\n;;;; Rules for `udiv` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; TODO: Add UDiv32 to implement 32-bit directly, rather\n;; than extending the input.\n;;\n;; Note that aarch64's `udiv` doesn't trap so to respect the semantics of\n;; CLIF's `udiv` the check for zero needs to be manually performed.\n(rule (lower (has_type (fits_in_64 ty) (udiv x y)))\n      (a64_udiv $I64 (put_in_reg_zext64 x) (put_nonzero_in_reg_zext64 y)))\n\n;; Helper for placing a `Value` into a `Reg` and validating that it's nonzero.\n(decl put_nonzero_in_reg_zext64 (Value) Reg)\n(rule (put_nonzero_in_reg_zext64 val)\n      (trap_if_zero_divisor (put_in_reg_zext64 val)))\n\n;; Special case where if a `Value` is known to be nonzero we can trivially\n;; move it into a register.\n(rule (put_nonzero_in_reg_zext64 (and (value_type ty)\n                                      (iconst (nonzero_u64_from_imm64 n))))\n      (imm ty n))\n\n;;;; Rules for `sdiv` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; TODO: Add SDiv32 to implement 32-bit directly, rather\n;; than extending the input.\n;;\n;; The sequence of checks here should look like:\n;;\n;;   cbnz rm, #8\n;;   udf ; divide by zero\n;;   cmn rm, 1\n;;   ccmp rn, 1, #nzcv, eq\n;;   b.vc #8\n;;   udf ; signed overflow\n;;\n;; Note The div instruction does not trap on divide by zero or overflow, so\n;; checks need to be manually inserted.\n;;\n;; TODO: if `y` is -1 then a check that `x` is not INT_MIN is all that's\n;; necessary, but right now `y` is checked to not be -1 as well.\n(rule (lower (has_type (fits_in_64 ty) (sdiv x y)))\n      (let ((x64 Reg (put_in_reg_sext64 x))\n            (y64 Reg (put_nonzero_in_reg_sext64 y))\n            (valid_x64 Reg (trap_if_div_overflow ty x64 y64))\n            (result Reg (a64_sdiv $I64 valid_x64 y64)))\n        result))\n\n;; Helper for extracting an immediate that's not 0 and not -1 from an imm64.\n(decl safe_divisor_from_imm64 (u64) Imm64)\n(extern extractor safe_divisor_from_imm64 safe_divisor_from_imm64)\n\n;; Special case for `sdiv` where no checks are needed due to division by a\n;; constant meaning the checks are always passed.\n(rule (lower (has_type (fits_in_64 ty) (sdiv x (iconst (safe_divisor_from_imm64 y)))))\n      (a64_sdiv $I64 (put_in_reg_sext64 x) (imm ty y)))\n\n;; Helper for placing a `Value` into a `Reg` and validating that it's nonzero.\n(decl put_nonzero_in_reg_sext64 (Value) Reg)\n(rule (put_nonzero_in_reg_sext64 val)\n      (trap_if_zero_divisor (put_in_reg_sext64 val)))\n\n;; Note that this has a special case where if the `Value` is a constant that's\n;; not zero we can skip the zero check.\n(rule (put_nonzero_in_reg_sext64 (and (value_type ty)\n                                      (iconst (nonzero_u64_from_imm64 n))))\n      (imm ty n))\n\n;;;; Rules for `urem` and `srem` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Remainder (x % y) is implemented as:\n;;\n;;   tmp = x / y\n;;   result = x - (tmp*y)\n;;\n;; use 'result' for tmp and you have:\n;;\n;;   cbnz y, #8         ; branch over trap\n;;   udf                ; divide by zero\n;;   div rd, x, y       ; rd = x / y\n;;   msub rd, rd, y, x  ; rd = x - rd * y\n\n(rule (lower (has_type (fits_in_64 ty) (urem x y)))\n      (let ((x64 Reg (put_in_reg_zext64 x))\n            (y64 Reg (put_nonzero_in_reg_zext64 y))\n            (div Reg (a64_udiv $I64 x64 y64))\n            (result Reg (msub $I64 div y64 x64)))\n        result))\n\n(rule (lower (has_type (fits_in_64 ty) (srem x y)))\n      (let ((x64 Reg (put_in_reg_sext64 x))\n            (y64 Reg (put_nonzero_in_reg_sext64 y))\n            (div Reg (a64_sdiv $I64 x64 y64))\n            (result Reg (msub $I64 div y64 x64)))\n        result))\n\n;;; Rules for integer min/max: umin, imin, umax, imax ;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (not_i64x2) (imin x y)))\n      (vec_rrr (VecALUOp.Smin) x y (vector_size ty)))\n\n(rule (lower (has_type ty @ (not_i64x2) (umin x y)))\n      (vec_rrr (VecALUOp.Umin) x y (vector_size ty)))\n\n(rule (lower (has_type ty @ (not_i64x2) (imax x y)))\n      (vec_rrr (VecALUOp.Smax) x y (vector_size ty)))\n\n(rule (lower (has_type ty @ (not_i64x2) (umax x y)))\n      (vec_rrr (VecALUOp.Umax) x y (vector_size ty)))\n\n;;;; Rules for `uextend` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; General rule for extending input to an output which fits in a single\n;; register.\n(rule (lower (has_type (fits_in_64 out) (uextend x @ (value_type in))))\n      (extend x $false (ty_bits in) (ty_bits out)))\n\n;; Extraction of a vector lane automatically extends as necessary, so we can\n;; skip an explicit extending instruction.\n(rule (lower (has_type (fits_in_64 out)\n                       (uextend (extractlane vec @ (value_type in)\n                                             (u8_from_uimm8 lane)))))\n      (mov_from_vec (put_in_reg vec) lane (vector_size in)))\n\n;; Atomic loads will also automatically zero their upper bits so the `uextend`\n;; instruction can effectively get skipped here.\n(rule (lower (has_type (fits_in_64 out)\n                       (uextend (and (value_type in) (sinkable_atomic_load addr)))))\n      (load_acquire in (sink_atomic_load addr)))\n\n;; Conversion to 128-bit needs a zero-extension of the lower bits and the upper\n;; bits are all zero.\n(rule (lower (has_type $I128 (uextend x)))\n      (value_regs (put_in_reg_zext64 x) (imm $I64 0)))\n\n;; Like above where vector extraction automatically zero-extends extending to\n;; i128 only requires generating a 0 constant for the upper bits.\n(rule (lower (has_type $I128\n                       (uextend (extractlane vec @ (value_type in)\n                                             (u8_from_uimm8 lane)))))\n      (value_regs (mov_from_vec (put_in_reg vec) lane (vector_size in)) (imm $I64 0)))\n\n;;;; Rules for `sextend` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; General rule for extending input to an output which fits in a single\n;; register.\n(rule (lower (has_type (fits_in_64 out) (sextend x @ (value_type in))))\n      (extend x $true (ty_bits in) (ty_bits out)))\n\n;; Extraction of a vector lane automatically extends as necessary, so we can\n;; skip an explicit extending instruction.\n(rule (lower (has_type (fits_in_64 out)\n                       (sextend (extractlane vec @ (value_type in)\n                                             (u8_from_uimm8 lane)))))\n      (mov_from_vec_signed (put_in_reg vec)\n                           lane\n                           (vector_size in)\n                           (size_from_ty out)))\n\n;; 64-bit to 128-bit only needs to sign-extend the input to the upper bits.\n(rule (lower (has_type $I128 (sextend x)))\n      (let ((lo Reg (put_in_reg_sext64 x))\n            (hi Reg (asr_imm $I64 lo (imm_shift_from_u8 63))))\n        (value_regs lo hi)))\n\n;; Like above where vector extraction automatically zero-extends extending to\n;; i128 only requires generating a 0 constant for the upper bits.\n;;\n;; Note that `mov_from_vec_signed` doesn't exist for i64x2, so that's\n;; specifically excluded here.\n(rule (lower (has_type $I128\n                       (sextend (extractlane vec @ (value_type in @ (not_i64x2))\n                                             (u8_from_uimm8 lane)))))\n      (let ((lo Reg (mov_from_vec_signed (put_in_reg vec)\n                                         lane\n                                         (vector_size in)\n                                         (size_from_ty $I64)))\n            (hi Reg (asr_imm $I64 lo (imm_shift_from_u8 63))))\n        (value_regs lo hi)))\n\n;; Extension from an extraction of i64x2 into i128.\n(rule (lower (has_type $I128\n                       (sextend (extractlane vec @ (value_type $I64X2)\n                                             (u8_from_uimm8 lane)))))\n      (let ((lo Reg (mov_from_vec (put_in_reg vec)\n                                  lane\n                                  (VectorSize.Size64x2)))\n            (hi Reg (asr_imm $I64 lo (imm_shift_from_u8 63))))\n        (value_regs lo hi)))\n\n;;;; Rules for `bnot` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Base case using `orn` between two registers.\n;;\n;; Note that bitwise negation is implemented here as\n;;\n;;      NOT rd, rm ==> ORR_NOT rd, zero, rm\n(rule (lower (has_type (fits_in_64 ty) (bnot x)))\n      (orr_not ty (zero_reg) x))\n\n;; Special case to use `orr_not_shift` if it's a `bnot` of a const-left-shifted\n;; value.\n(rule (lower (has_type (fits_in_64 ty)\n                       (bnot (ishl x (iconst k)))))\n      (if-let amt (lshl_from_imm64 ty k))\n      (orr_not_shift ty (zero_reg) x amt))\n\n;; Implementation of `bnot` for `i128`.\n(rule (lower (has_type $I128 (bnot x)))\n      (let ((x_regs ValueRegs x)\n            (x_lo Reg (value_regs_get x_regs 0))\n            (x_hi Reg (value_regs_get x_regs 1))\n            (new_lo Reg (orr_not $I64 (zero_reg) x_lo))\n            (new_hi Reg (orr_not $I64 (zero_reg) x_hi)))\n        (value_regs new_lo new_hi)))\n\n;; Implementation of `bnot` for vector types.\n(rule (lower (has_type (ty_vec128 ty) (bnot x)))\n      (not x (vector_size ty)))\n\n;;;; Rules for `band` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (band x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.And) ty x y))\n\n(rule (lower (has_type $I64 (band x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.And) $I64 x y))\n\n(rule (lower (has_type $I128 (band x y))) (i128_alu_bitop (ALUOp.And) $I64 x y))\n\n(rule (lower (has_type (ty_vec128 ty) (band x y)))\n      (and_vec x y (vector_size ty)))\n\n;;;; Rules for `bor` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (bor x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.Orr) ty x y))\n\n(rule (lower (has_type $I64 (bor x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.Orr) $I64 x y))\n\n(rule (lower (has_type $I128 (bor x y))) (i128_alu_bitop (ALUOp.Orr) $I64 x y))\n\n(rule (lower (has_type (ty_vec128 ty) (bor x y)))\n      (orr_vec x y (vector_size ty)))\n\n;;;; Rules for `bxor` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (bxor x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.Eor) ty x y))\n\n(rule (lower (has_type $I64 (bxor x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.Eor) $I64 x y))\n\n(rule (lower (has_type $I128 (bxor x y))) (i128_alu_bitop (ALUOp.Eor) $I64 x y))\n\n(rule (lower (has_type (ty_vec128 ty) (bxor x y)))\n      (eor_vec x y (vector_size ty)))\n\n;;;; Rules for `band_not` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (band_not x y)))\n      (alu_rs_imm_logic (ALUOp.AndNot) ty x y))\n\n(rule (lower (has_type $I64 (band_not x y)))\n      (alu_rs_imm_logic (ALUOp.AndNot) $I64 x y))\n\n(rule (lower (has_type $I128 (band_not x y))) (i128_alu_bitop (ALUOp.AndNot) $I64 x y))\n\n(rule (lower (has_type (ty_vec128 ty) (band_not x y)))\n      (bic_vec x y (vector_size ty)))\n\n;;;; Rules for `bor_not` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (bor_not x y)))\n      (alu_rs_imm_logic (ALUOp.OrrNot) ty x y))\n\n(rule (lower (has_type $I64 (bor_not x y)))\n      (alu_rs_imm_logic (ALUOp.OrrNot) $I64 x y))\n\n(rule (lower (has_type $I128 (bor_not x y))) (i128_alu_bitop (ALUOp.OrrNot) $I64 x y))\n\n;;;; Rules for `bxor_not` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (bxor_not x y)))\n      (alu_rs_imm_logic (ALUOp.EorNot) $I32 x y))\n\n(rule (lower (has_type $I64 (bxor_not x y)))\n      (alu_rs_imm_logic (ALUOp.EorNot) $I64 x y))\n\n(rule (lower (has_type $I128 (bxor_not x y))) (i128_alu_bitop (ALUOp.EorNot) $I64 x y))\n\n;;;; Rules for `ishl` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Shift for i8/i16/i32.\n(rule (lower (has_type (fits_in_32 ty) (ishl x y)))\n      (do_shift (ALUOp.Lsl) ty x y))\n\n;; Shift for i64.\n(rule (lower (has_type $I64 (ishl x y)))\n      (do_shift (ALUOp.Lsl) $I64 x y))\n\n;; Shift for i128.\n(rule (lower (has_type $I128 (ishl x y)))\n      (lower_shl128 x (value_regs_get y 0)))\n\n;;     lsl     lo_lshift, src_lo, amt\n;;     lsl     hi_lshift, src_hi, amt\n;;     mvn     inv_amt, amt\n;;     lsr     lo_rshift, src_lo, #1\n;;     lsr     lo_rshift, lo_rshift, inv_amt\n;;     orr     maybe_hi, hi_lshift, lo_rshift\n;;     tst     amt, #0x40\n;;     csel    dst_hi, lo_lshift, maybe_hi, ne\n;;     csel    dst_lo, xzr, lo_lshift, ne\n(decl lower_shl128 (ValueRegs Reg) ValueRegs)\n(rule (lower_shl128 src amt)\n      (let ((src_lo Reg (value_regs_get src 0))\n            (src_hi Reg (value_regs_get src 1))\n            (lo_lshift Reg (lsl $I64 src_lo amt))\n            (hi_lshift Reg (lsl $I64 src_hi amt))\n            (inv_amt Reg (orr_not $I32 (zero_reg) amt))\n            (lo_rshift Reg (lsr $I64 (lsr_imm $I64 src_lo (imm_shift_from_u8 1))\n                                inv_amt))\n          (maybe_hi Reg (orr $I64 hi_lshift lo_rshift))\n        )\n        (with_flags\n         (tst_imm $I64 amt (u64_into_imm_logic $I64 64))\n         (consumes_flags_concat\n          (csel (Cond.Ne) (zero_reg) lo_lshift)\n          (csel (Cond.Ne) lo_lshift maybe_hi)))))\n\n;; Shift for vector types.\n(rule (lower (has_type (ty_vec128 ty) (ishl x y)))\n      (let ((size VectorSize (vector_size ty))\n            (shift Reg (vec_dup y size)))\n        (sshl x shift size)))\n\n;; Helper function to emit a shift operation with the opcode specified and\n;; the output type specified. The `Reg` provided is shifted by the `Value`\n;; given.\n;;\n;; Note that this automatically handles the clif semantics of masking the\n;; shift amount where necessary.\n(decl do_shift (ALUOp Type Reg Value) Reg)\n\n;; 8/16-bit shift base case.\n;;\n;; When shifting for amounts larger than the size of the type, the CLIF shift\n;; instructions implement a \"wrapping\" behaviour, such that an i8 << 8 is\n;; equivalent to i8 << 0\n;;\n;; On i32 and i64 types this matches what the aarch64 spec does, but on smaller\n;; types (i16, i8) we need to do this manually, so we wrap the shift amount\n;; with an AND instruction\n(rule (do_shift op (fits_in_16 ty) x y)\n      (let ((shift_amt Reg (value_regs_get y 0))\n            (masked_shift_amt Reg (and_imm $I32 shift_amt (shift_mask ty))))\n        (alu_rrr op $I32 x masked_shift_amt)))\n\n(decl shift_mask (Type) ImmLogic)\n(extern constructor shift_mask shift_mask)\n\n;; 32/64-bit shift base cases.\n(rule (do_shift op $I32 x y) (alu_rrr op $I32 x (value_regs_get y 0)))\n(rule (do_shift op $I64 x y) (alu_rrr op $I64 x (value_regs_get y 0)))\n\n;; Special case for shifting by a constant value where the value can fit into an\n;; `ImmShift`.\n;;\n;; Note that this rule explicitly has a higher priority than the others\n;; to ensure it's attempted first, otherwise the type-based filters on the\n;; previous rules seem to take priority over this rule.\n(rule 1 (do_shift op ty x (iconst k))\n      (if-let shift (imm_shift_from_imm64 ty k))\n      (alu_rr_imm_shift op ty x shift))\n\n;;;; Rules for `ushr` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Shift for i8/i16/i32.\n(rule (lower (has_type (fits_in_32 ty) (ushr x y)))\n      (do_shift (ALUOp.Lsr) ty (put_in_reg_zext32 x) y))\n\n;; Shift for i64.\n(rule (lower (has_type $I64 (ushr x y)))\n      (do_shift (ALUOp.Lsr) $I64 (put_in_reg_zext64 x) y))\n\n;; Shift for i128.\n(rule (lower (has_type $I128 (ushr x y)))\n      (lower_ushr128 x (value_regs_get y 0)))\n\n;; Vector shifts.\n(rule (lower (has_type (ty_vec128 ty) (ushr x y)))\n      (let ((size VectorSize (vector_size ty))\n            (shift Reg (vec_dup (sub $I32 (zero_reg) y) size)))\n        (ushl x shift size)))\n\n;;     lsr       lo_rshift, src_lo, amt\n;;     lsr       hi_rshift, src_hi, amt\n;;     mvn       inv_amt, amt\n;;     lsl       hi_lshift, src_hi, #1\n;;     lsl       hi_lshift, hi_lshift, inv_amt\n;;     tst       amt, #0x40\n;;     orr       maybe_lo, lo_rshift, hi_lshift\n;;     csel      dst_hi, xzr, hi_rshift, ne\n;;     csel      dst_lo, hi_rshift, maybe_lo, ne\n(decl lower_ushr128 (ValueRegs Reg) ValueRegs)\n(rule (lower_ushr128 src amt)\n      (let ((src_lo Reg (value_regs_get src 0))\n            (src_hi Reg (value_regs_get src 1))\n            (lo_rshift Reg (lsr $I64 src_lo amt))\n            (hi_rshift Reg (lsr $I64 src_hi amt))\n\n            (inv_amt Reg (orr_not $I32 (zero_reg) amt))\n            (hi_lshift Reg (lsl $I64 (lsl_imm $I64 src_hi (imm_shift_from_u8 1))\n                                inv_amt))\n          (maybe_lo Reg (orr $I64 lo_rshift hi_lshift))\n        )\n        (with_flags\n         (tst_imm $I64 amt (u64_into_imm_logic $I64 64))\n         (consumes_flags_concat\n          (csel (Cond.Ne) hi_rshift maybe_lo)\n          (csel (Cond.Ne) (zero_reg) hi_rshift)))))\n\n;;;; Rules for `sshr` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Shift for i8/i16/i32.\n(rule (lower (has_type (fits_in_32 ty) (sshr x y)))\n      (do_shift (ALUOp.Asr) ty (put_in_reg_sext32 x) y))\n\n;; Shift for i64.\n(rule (lower (has_type $I64 (sshr x y)))\n      (do_shift (ALUOp.Asr) $I64 (put_in_reg_sext64 x) y))\n\n;; Shift for i128.\n(rule (lower (has_type $I128 (sshr x y)))\n      (lower_sshr128 x (value_regs_get y 0)))\n\n;; Vector shifts.\n;;\n;; Note that right shifts are implemented with a negative left shift.\n(rule (lower (has_type (ty_vec128 ty) (sshr x y)))\n      (let ((size VectorSize (vector_size ty))\n            (shift Reg (vec_dup (sub $I32 (zero_reg) y) size)))\n        (sshl x shift size)))\n\n;;     lsr       lo_rshift, src_lo, amt\n;;     asr       hi_rshift, src_hi, amt\n;;     mvn       inv_amt, amt\n;;     lsl       hi_lshift, src_hi, #1\n;;     lsl       hi_lshift, hi_lshift, inv_amt\n;;     asr       hi_sign, src_hi, #63\n;;     orr       maybe_lo, lo_rshift, hi_lshift\n;;     tst       amt, #0x40\n;;     csel      dst_hi, hi_sign, hi_rshift, ne\n;;     csel      dst_lo, hi_rshift, maybe_lo, ne\n(decl lower_sshr128 (ValueRegs Reg) ValueRegs)\n(rule (lower_sshr128 src amt)\n      (let ((src_lo Reg (value_regs_get src 0))\n            (src_hi Reg (value_regs_get src 1))\n            (lo_rshift Reg (lsr $I64 src_lo amt))\n            (hi_rshift Reg (asr $I64 src_hi amt))\n\n            (inv_amt Reg (orr_not $I32 (zero_reg) amt))\n            (hi_lshift Reg (lsl $I64 (lsl_imm $I64 src_hi (imm_shift_from_u8 1))\n                                inv_amt))\n          (hi_sign Reg (asr_imm $I64 src_hi (imm_shift_from_u8 63)))\n          (maybe_lo Reg (orr $I64 lo_rshift hi_lshift))\n        )\n        (with_flags\n         (tst_imm $I64 amt (u64_into_imm_logic $I64 64))\n         (consumes_flags_concat\n          (csel (Cond.Ne) hi_rshift maybe_lo)\n          (csel (Cond.Ne) hi_sign hi_rshift)))))\n\n;;;; Rules for `rotl` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; General 8/16-bit case.\n(rule (lower (has_type (fits_in_16 ty) (rotl x y)))\n      (let ((neg_shift Reg (sub $I32 (zero_reg) y)))\n        (small_rotr ty (put_in_reg_zext32 x) neg_shift)))\n\n;; Specialization for the 8/16-bit case when the rotation amount is an immediate.\n(rule (lower (has_type (fits_in_16 ty) (rotl x (iconst k))))\n      (if-let n (imm_shift_from_imm64 ty k))\n      (small_rotr_imm ty (put_in_reg_zext32 x) (negate_imm_shift ty n)))\n\n;; aarch64 doesn't have a left-rotate instruction, but a left rotation of K\n;; places is effectively a right rotation of N - K places, if N is the integer's\n;; bit size. We implement left rotations with this trick.\n;;\n;; Note that when negating the shift amount here the upper bits are ignored\n;; by the rotr instruction, meaning that we'll still left-shift by the desired\n;; amount.\n\n;; General 32-bit case.\n(rule (lower (has_type $I32 (rotl x y)))\n      (let ((neg_shift Reg (sub $I32 (zero_reg) y)))\n        (a64_rotr $I32 x neg_shift)))\n\n;; General 64-bit case.\n(rule (lower (has_type $I64 (rotl x y)))\n      (let ((neg_shift Reg (sub $I64 (zero_reg) y)))\n        (a64_rotr $I64 x neg_shift)))\n\n;; Specialization for the 32-bit case when the rotation amount is an immediate.\n(rule (lower (has_type $I32 (rotl x (iconst k))))\n      (if-let n (imm_shift_from_imm64 $I32 k))\n      (a64_rotr_imm $I32 x (negate_imm_shift $I32 n)))\n\n;; Specialization for the 64-bit case when the rotation amount is an immediate.\n(rule (lower (has_type $I64 (rotl x (iconst k))))\n      (if-let n (imm_shift_from_imm64 $I64 k))\n      (a64_rotr_imm $I64 x (negate_imm_shift $I64 n)))\n\n(decl negate_imm_shift (Type ImmShift) ImmShift)\n(extern constructor negate_imm_shift negate_imm_shift)\n\n;; General 128-bit case.\n;;\n;; TODO: much better codegen is possible with a constant amount.\n(rule (lower (has_type $I128 (rotl x y)))\n      (let ((val ValueRegs x)\n            (amt Reg (value_regs_get y 0))\n            (neg_amt Reg (sub $I64 (imm $I64 128) amt))\n            (lshift ValueRegs (lower_shl128 val amt))\n            (rshift ValueRegs (lower_ushr128 val neg_amt)))\n        (value_regs\n          (orr $I64 (value_regs_get lshift 0) (value_regs_get rshift 0))\n          (orr $I64 (value_regs_get lshift 1) (value_regs_get rshift 1)))))\n\n;;;; Rules for `rotr` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; General 8/16-bit case.\n(rule (lower (has_type (fits_in_16 ty) (rotr x y)))\n      (small_rotr ty (put_in_reg_zext32 x) y))\n\n;; General 32-bit case.\n(rule (lower (has_type $I32 (rotr x y)))\n      (a64_rotr $I32 x y))\n\n;; General 64-bit case.\n(rule (lower (has_type $I64 (rotr x y)))\n      (a64_rotr $I64 x y))\n\n;; Specialization for the 8/16-bit case when the rotation amount is an immediate.\n(rule (lower (has_type (fits_in_16 ty) (rotr x (iconst k))))\n      (if-let n (imm_shift_from_imm64 ty k))\n      (small_rotr_imm ty (put_in_reg_zext32 x) n))\n\n;; Specialization for the 32-bit case when the rotation amount is an immediate.\n(rule (lower (has_type $I32 (rotr x (iconst k))))\n      (if-let n (imm_shift_from_imm64 $I32 k))\n      (a64_rotr_imm $I32 x n))\n\n;; Specialization for the 64-bit case when the rotation amount is an immediate.\n(rule (lower (has_type $I64 (rotr x (iconst k))))\n      (if-let n (imm_shift_from_imm64 $I64 k))\n      (a64_rotr_imm $I64 x n))\n\n;; For a < 32-bit rotate-right, we synthesize this as:\n;;\n;;    rotr rd, val, amt\n;;\n;;       =>\n;;\n;;    and masked_amt, amt, <bitwidth - 1>\n;;    sub tmp_sub, masked_amt, <bitwidth>\n;;    sub neg_amt, zero, tmp_sub  ; neg\n;;    lsr val_rshift, val, masked_amt\n;;    lsl val_lshift, val, neg_amt\n;;    orr rd, val_lshift val_rshift\n(decl small_rotr (Type Reg Reg) Reg)\n(rule (small_rotr ty val amt)\n      (let ((masked_amt Reg (and_imm $I32 amt (rotr_mask ty)))\n            (tmp_sub Reg (sub_imm $I32 masked_amt (u8_into_imm12 (ty_bits ty))))\n            (neg_amt Reg (sub $I32 (zero_reg) tmp_sub))\n            (val_rshift Reg (lsr $I32 val masked_amt))\n            (val_lshift Reg (lsl $I32 val neg_amt)))\n        (orr $I32 val_lshift val_rshift)))\n\n(decl rotr_mask (Type) ImmLogic)\n(extern constructor rotr_mask rotr_mask)\n\n;; For a constant amount, we can instead do:\n;;\n;;    rotr rd, val, #amt\n;;\n;;       =>\n;;\n;;    lsr val_rshift, val, #<amt>\n;;    lsl val_lshift, val, <bitwidth - amt>\n;;    orr rd, val_lshift, val_rshift\n(decl small_rotr_imm (Type Reg ImmShift) Reg)\n(rule (small_rotr_imm ty val amt)\n      (let ((val_rshift Reg (lsr_imm $I32 val amt))\n            (val_lshift Reg (lsl_imm $I32 val (rotr_opposite_amount ty amt))))\n        (orr $I32 val_lshift val_rshift)))\n\n(decl rotr_opposite_amount (Type ImmShift) ImmShift)\n(extern constructor rotr_opposite_amount rotr_opposite_amount)\n\n;; General 128-bit case.\n;;\n;; TODO: much better codegen is possible with a constant amount.\n(rule (lower (has_type $I128 (rotr x y)))\n      (let ((val ValueRegs x)\n            (amt Reg (value_regs_get y 0))\n            (neg_amt Reg (sub $I64 (imm $I64 128) amt))\n            (rshift ValueRegs (lower_ushr128 val amt))\n            (lshift ValueRegs (lower_shl128 val neg_amt))\n            (hi Reg (orr $I64 (value_regs_get rshift 1) (value_regs_get lshift 1)))\n            (lo Reg (orr $I64 (value_regs_get rshift 0) (value_regs_get lshift 0))))\n        (value_regs lo hi)))\n\n;;;; Rules for `bitrev` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Reversing an 8-bit value with a 32-bit bitrev instruction will place\n;; the reversed result in the highest 8 bits, so we need to shift them down into\n;; place.\n(rule (lower (has_type $I8 (bitrev x)))\n      (lsr_imm $I32 (rbit $I32 x) (imm_shift_from_u8 24)))\n\n;; Reversing an 16-bit value with a 32-bit bitrev instruction will place\n;; the reversed result in the highest 16 bits, so we need to shift them down into\n;; place.\n(rule (lower (has_type $I16 (bitrev x)))\n      (lsr_imm $I32 (rbit $I32 x) (imm_shift_from_u8 16)))\n\n(rule (lower (has_type $I128 (bitrev x)))\n      (let ((val ValueRegs x)\n            (lo_rev Reg (rbit $I64 (value_regs_get val 0)))\n            (hi_rev Reg (rbit $I64 (value_regs_get val 1))))\n        (value_regs hi_rev lo_rev)))\n\n(rule (lower (has_type ty (bitrev x)))\n      (rbit ty x))\n\n\n;;;; Rules for `clz` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I8 (clz x)))\n      (sub_imm $I32 (a64_clz $I32 (put_in_reg_zext32 x)) (u8_into_imm12 24)))\n\n(rule (lower (has_type $I16 (clz x)))\n      (sub_imm $I32 (a64_clz $I32 (put_in_reg_zext32 x)) (u8_into_imm12 16)))\n\n(rule (lower (has_type $I128 (clz x)))\n      (lower_clz128 x))\n\n(rule (lower (has_type ty (clz x)))\n      (a64_clz ty x))\n\n;; clz hi_clz, hi\n;; clz lo_clz, lo\n;; lsr tmp, hi_clz, #6\n;; madd dst_lo, lo_clz, tmp, hi_clz\n;; mov  dst_hi, 0\n(decl lower_clz128 (ValueRegs) ValueRegs)\n(rule (lower_clz128 val)\n      (let ((hi_clz Reg (a64_clz $I64 (value_regs_get val 1)))\n            (lo_clz Reg (a64_clz $I64 (value_regs_get val 0)))\n            (tmp Reg (lsr_imm $I64 hi_clz (imm_shift_from_u8 6))))\n        (value_regs (madd $I64 lo_clz tmp hi_clz) (imm $I64 0))))\n\n;;;; Rules for `ctz` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Note that all `ctz` instructions are implemented by reversing the bits and\n;; then using a `clz` instruction since the tail zeros are the same as the\n;; leading zeros of the reversed value.\n\n(rule (lower (has_type $I8 (ctz x)))\n      (a64_clz $I32 (orr_imm $I32 (rbit $I32 x) (u64_into_imm_logic $I32 0x800000))))\n\n(rule (lower (has_type $I16 (ctz x)))\n      (a64_clz $I32 (orr_imm $I32 (rbit $I32 x) (u64_into_imm_logic $I32 0x8000))))\n\n(rule (lower (has_type $I128 (ctz x)))\n      (let ((val ValueRegs x)\n            (lo Reg (rbit $I64 (value_regs_get val 0)))\n            (hi Reg (rbit $I64 (value_regs_get val 1))))\n        (lower_clz128 (value_regs hi lo))))\n\n(rule (lower (has_type ty (ctz x)))\n      (a64_clz ty (rbit ty x)))\n\n;;;; Rules for `cls` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I8 (cls x)))\n      (sub_imm $I32 (a64_cls $I32 (put_in_reg_sext32 x)) (u8_into_imm12 24)))\n\n(rule (lower (has_type $I16 (cls x)))\n      (sub_imm $I32 (a64_cls $I32 (put_in_reg_sext32 x)) (u8_into_imm12 16)))\n\n;; cls lo_cls, lo\n;; cls hi_cls, hi\n;; eon sign_eq_eor, hi, lo\n;; lsr sign_eq, sign_eq_eor, #63\n;; madd lo_sign_bits, out_lo, sign_eq, sign_eq\n;; cmp hi_cls, #63\n;; csel maybe_lo, lo_sign_bits, xzr, eq\n;; add  out_lo, maybe_lo, hi_cls\n;; mov  out_hi, 0\n(rule (lower (has_type $I128 (cls x)))\n      (let ((val ValueRegs x)\n            (lo Reg (value_regs_get val 0))\n            (hi Reg (value_regs_get val 1))\n            (lo_cls Reg (a64_cls $I64 lo))\n            (hi_cls Reg (a64_cls $I64 hi))\n            (sign_eq_eon Reg (eon $I64 hi lo))\n            (sign_eq Reg (lsr_imm $I64 sign_eq_eon (imm_shift_from_u8 63)))\n            (lo_sign_bits Reg (madd $I64 lo_cls sign_eq sign_eq))\n            (maybe_lo Reg (with_flags_reg\n                           (cmp64_imm hi_cls (u8_into_imm12 63))\n                           (csel (Cond.Eq) lo_sign_bits (zero_reg)))))\n        (value_regs (add $I64 maybe_lo hi_cls) (imm $I64 0))))\n\n(rule (lower (has_type ty (cls x)))\n      (a64_cls ty x))\n\n;;;; Rules for `bint` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Booleans are stored as all-zeroes (0) or all-ones (-1). We AND\n;; out the LSB to give a 0 / 1-valued integer result.\n\n(rule (lower (has_type $I128 (bint x)))\n      (let ((val ValueRegs x)\n            (in_lo Reg (value_regs_get val 0))\n            (dst_lo Reg (and_imm $I32 in_lo (u64_into_imm_logic $I32 1)))\n            (dst_hi Reg (imm $I64 0)))\n        (value_regs dst_lo dst_hi)))\n\n(rule (lower (bint x))\n      (and_imm $I32 x (u64_into_imm_logic $I32 1)))\n\n;;;; Rules for `bmask`/`bextend` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Bextend and Bmask both simply sign-extend. This works for:\n;; - Bextend, because booleans are stored as 0 / -1, so we\n;;   sign-extend the -1 to a -1 in the wider width.\n;; - Bmask, because the resulting integer mask value must be\n;;   all-ones (-1) if the argument is true.\n\n;; Use a common helper to type cast bools to either bool or integer types.\n(decl cast_bool (Type Type Value) InstOutput)\n(rule (lower (has_type out_ty (bextend x @ (value_type in_ty))))\n      (cast_bool in_ty out_ty x))\n(rule (lower (has_type out_ty (bmask x @ (value_type in_ty))))\n      (cast_bool in_ty out_ty x))\n\n\n;; If the target has the same or a smaller size than the source, it's a no-op.\n(rule (cast_bool $B8 $I8 x) x)\n(rule (cast_bool $B16 (fits_in_16 _out) x) x)\n(rule (cast_bool $B32 (fits_in_32 _out) x) x)\n(rule (cast_bool $B64 (fits_in_64 _out) x) x)\n\n;; Casting between 128 bits is a noop\n(rule (cast_bool (ty_int_bool_128 _in) (ty_int_bool_128 _out) x)\n    x)\n\n;; Converting from 128 bits to anything below we just ignore the top register\n(rule (cast_bool (ty_int_bool_128 _in) (fits_in_64 _out) x)\n    (value_regs_get x 0))\n\n;; Extend to 64 bits first, then this will be all 0s or all 1s and we can\n;; duplicate to both halves of 128 bits\n(rule (cast_bool in (ty_int_bool_128 _out) x)\n      (let ((tmp Reg (extend x $true (ty_bits in) 64)))\n        (value_regs tmp tmp)))\n\n;; Values that fit in a single register are sign extended normally\n(rule (cast_bool (fits_in_64 in) (fits_in_64 out) x)\n      (extend x $true (ty_bits in) (ty_bits out)))\n\n;;;; Rules for `popcnt` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; The implementation of `popcnt` for scalar types is done by moving the value\n;; into a vector register, using the `cnt` instruction, and then collating the\n;; result back into a normal register.\n;;\n;; The general sequence emitted here is\n;;\n;;     fmov tmp, in_lo\n;;     if ty == i128:\n;;         mov tmp.d[1], in_hi\n;;\n;;     cnt tmp.16b, tmp.16b / cnt tmp.8b, tmp.8b\n;;     addv tmp, tmp.16b / addv tmp, tmp.8b / addp tmp.8b, tmp.8b, tmp.8b / (no instruction for 8-bit inputs)\n;;\n;;     umov out_lo, tmp.b[0]\n;;     if ty == i128:\n;;         mov out_hi, 0\n\n(rule (lower (has_type $I8 (popcnt x)))\n      (let ((tmp Reg (mov_to_fpu x (ScalarSize.Size32)))\n            (nbits Reg (vec_cnt tmp (VectorSize.Size8x8))))\n        (mov_from_vec nbits 0 (VectorSize.Size8x16))))\n\n;; Note that this uses `addp` instead of `addv` as it's usually cheaper.\n(rule (lower (has_type $I16 (popcnt x)))\n      (let ((tmp Reg (mov_to_fpu x (ScalarSize.Size32)))\n            (nbits Reg (vec_cnt tmp (VectorSize.Size8x8)))\n            (added Reg (addp nbits nbits (VectorSize.Size8x8))))\n        (mov_from_vec added 0 (VectorSize.Size8x16))))\n\n(rule (lower (has_type $I32 (popcnt x)))\n      (let ((tmp Reg (mov_to_fpu x (ScalarSize.Size32)))\n            (nbits Reg (vec_cnt tmp (VectorSize.Size8x8)))\n            (added Reg (addv nbits (VectorSize.Size8x8))))\n        (mov_from_vec added 0 (VectorSize.Size8x16))))\n\n(rule (lower (has_type $I64 (popcnt x)))\n      (let ((tmp Reg (mov_to_fpu x (ScalarSize.Size64)))\n            (nbits Reg (vec_cnt tmp (VectorSize.Size8x8)))\n            (added Reg (addv nbits (VectorSize.Size8x8))))\n        (mov_from_vec added 0 (VectorSize.Size8x16))))\n\n(rule (lower (has_type $I128 (popcnt x)))\n      (let ((val ValueRegs x)\n            (tmp_half Reg (mov_to_fpu (value_regs_get val 0) (ScalarSize.Size64)))\n            (tmp Reg (mov_to_vec tmp_half (value_regs_get val 1) 1 (VectorSize.Size64x2)))\n            (nbits Reg (vec_cnt tmp (VectorSize.Size8x16)))\n            (added Reg (addv nbits (VectorSize.Size8x16))))\n        (value_regs (mov_from_vec added 0 (VectorSize.Size8x16)) (imm $I64 0))))\n\n(rule (lower (has_type $I8X16 (popcnt x)))\n      (vec_cnt x (VectorSize.Size8x16)))\n\n;;;; Rules for `bitselect` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_int_bool_ref_scalar_64 ty) (bitselect c x y)))\n      (let ((tmp1 Reg (and_reg ty x c))\n            (tmp2 Reg (bic ty y c)))\n        (orr ty tmp1 tmp2)))\n\n(rule (lower (has_type (ty_vec128 ty) (bitselect c x y)))\n        (bsl ty c x y))\n\n;;;; Rules for `vselect` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_vec128 ty) (vselect c x y)))\n        (bsl ty c x y))\n\n;;;; Rules for `ireduce` / `breduce` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; T -> I{64,32,16,8}: We can simply pass through the value: values\n;; are always stored with high bits undefined, so we can just leave\n;; them be.\n(rule (lower (has_type (ty_int_bool_ref_scalar_64 ty) (ireduce src)))\n    (value_regs_get src 0))\n\n;; Likewise for breduce.\n\n(rule (lower (has_type (ty_int_bool_ref_scalar_64 ty) (breduce src)))\n      (value_regs_get src 0))\n\n\n;;;; Rules for `fcmp` 32 bit ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond_not_eq cond) x (splat (f32const (zero_value_f32 y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (fcmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond cond) x (splat (f32const (zero_value_f32 y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (float_cmp_zero cond rn vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond_not_eq cond) (splat (f32const (zero_value_f32 x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (fcmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond cond) (splat (f32const (zero_value_f32 x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (float_cmp_zero_swap cond rn vec_size))))\n\n;;;; Rules for `fcmp` 64 bit ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond_not_eq cond) x (splat (f64const (zero_value_f64 y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (fcmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond cond) x (splat (f64const (zero_value_f64 y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (float_cmp_zero cond rn vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond_not_eq cond) (splat (f64const (zero_value_f64 x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (fcmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond cond) (splat (f64const (zero_value_f64 x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (float_cmp_zero_swap cond rn vec_size))))\n\n;;;; Rules for `icmp` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond_not_eq cond) x (splat (iconst (zero_value y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (cmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond cond) x (splat (iconst (zero_value y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (int_cmp_zero cond rn vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond_not_eq cond) (splat (iconst (zero_value x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (cmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond cond) (splat (iconst (zero_value x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (int_cmp_zero_swap cond rn vec_size))))\n\n;;;; Rules for `trap` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (trap trap_code))\n      (let ((use_allocated_encoding bool (is_not_baldrdash_call_conv)))\n         (side_effect (udf use_allocated_encoding trap_code))))\n\n;;;; Rules for `resumable_trap` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (resumable_trap trap_code))\n      (let ((use_allocated_encoding bool (is_not_baldrdash_call_conv)))\n         (side_effect (udf use_allocated_encoding trap_code))))\n\n\n;;;; Rules for `AtomicLoad` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n(rule (lower (has_type (valid_atomic_transaction ty) (atomic_load flags addr)))\n      (load_acquire ty addr))\n\n\n;;;; Rules for `AtomicStore` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n(rule (lower (atomic_store flags\n                src @ (value_type (valid_atomic_transaction ty))\n                addr))\n      (side_effect (store_release ty src addr)))\n\n\n;;;; Rules for `AtomicRMW` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Add) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Add) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Xor) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Eor) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Or) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Set) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Smax) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Smax) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Smin) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Smin) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Umax) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Umax) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Umin) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Umin) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Sub) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Add) addr (sub ty (zero_reg) src) ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.And) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Clr) addr (eon ty src (zero_reg)) ty))\n\n\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Add) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Add) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Sub) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Sub) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.And) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.And) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Nand) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Nand) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Or) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Orr) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Xor) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Eor) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Smin) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Smin) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Smax) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Smax) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Umin) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Umin) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Umax) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Umax) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Xchg) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Xchg) addr src ty))\n\n;;;; Rules for `AtomicCAS` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                  (atomic_cas flags addr src1 src2))))\n      (lse_atomic_cas addr src1 src2 ty))\n\n(rule (lower (and (has_type (valid_atomic_transaction ty)\n                  (atomic_cas flags addr src1 src2))))\n      (atomic_cas_loop addr src1 src2 ty))\n", "//! ISLE integration glue code for aarch64 lowering.\n\n// Pull in the ISLE generated code.\npub mod generated_code;\n\n// Types that the generated ISLE code uses via `use super::*`.\nuse super::{\n    writable_zero_reg, zero_reg, AMode, ASIMDFPModImm, ASIMDMovModImm, BranchTarget, CallIndInfo,\n    CallInfo, Cond, CondBrKind, ExtendOp, FPUOpRI, FloatCC, Imm12, ImmLogic, ImmShift,\n    Inst as MInst, IntCC, JTSequenceInfo, MachLabel, MoveWideConst, MoveWideOp, NarrowValueMode,\n    Opcode, OperandSize, PairAMode, Reg, ScalarSize, ShiftOpAndAmt, UImm5, VecMisc2, VectorSize,\n    NZCV,\n};\nuse crate::isa::aarch64::settings::Flags as IsaFlags;\nuse crate::machinst::{isle::*, InputSourceInst};\nuse crate::settings::Flags;\nuse crate::{\n    binemit::CodeOffset,\n    ir::{\n        immediates::*, types::*, AtomicRmwOp, ExternalName, Inst, InstructionData, MemFlags,\n        TrapCode, Value, ValueList,\n    },\n    isa::aarch64::inst::args::{ShiftOp, ShiftOpShiftImm},\n    isa::aarch64::lower::{writable_xreg, xreg},\n    isa::unwind::UnwindInst,\n    machinst::{ty_bits, InsnOutput, LowerCtx, VCodeConstant, VCodeConstantData},\n};\nuse std::boxed::Box;\nuse std::convert::TryFrom;\nuse std::vec::Vec;\n\ntype BoxCallInfo = Box<CallInfo>;\ntype BoxCallIndInfo = Box<CallIndInfo>;\ntype VecMachLabel = Vec<MachLabel>;\ntype BoxJTSequenceInfo = Box<JTSequenceInfo>;\ntype BoxExternalName = Box<ExternalName>;\n\n/// The main entry point for lowering with ISLE.\npub(crate) fn lower<C>(\n    lower_ctx: &mut C,\n    flags: &Flags,\n    isa_flags: &IsaFlags,\n    outputs: &[InsnOutput],\n    inst: Inst,\n) -> Result<(), ()>\nwhere\n    C: LowerCtx<I = MInst>,\n{\n    lower_common(lower_ctx, flags, isa_flags, outputs, inst, |cx, insn| {\n        generated_code::constructor_lower(cx, insn)\n    })\n}\n\npub struct ExtendedValue {\n    val: Value,\n    extend: ExtendOp,\n}\n\npub struct SinkableAtomicLoad {\n    atomic_load: Inst,\n    atomic_addr: Value,\n}\n\nimpl<C> generated_code::Context for IsleContext<'_, C, Flags, IsaFlags, 6>\nwhere\n    C: LowerCtx<I = MInst>,\n{\n    isle_prelude_methods!();\n\n    fn use_lse(&mut self, _: Inst) -> Option<()> {\n        if self.isa_flags.use_lse() {\n            Some(())\n        } else {\n            None\n        }\n    }\n\n    fn move_wide_const_from_u64(&mut self, n: u64) -> Option<MoveWideConst> {\n        MoveWideConst::maybe_from_u64(n)\n    }\n\n    fn move_wide_const_from_negated_u64(&mut self, n: u64) -> Option<MoveWideConst> {\n        MoveWideConst::maybe_from_u64(!n)\n    }\n\n    fn imm_logic_from_u64(&mut self, ty: Type, n: u64) -> Option<ImmLogic> {\n        let ty = if ty.bits() < 32 { I32 } else { ty };\n        ImmLogic::maybe_from_u64(n, ty)\n    }\n\n    fn imm_logic_from_imm64(&mut self, ty: Type, n: Imm64) -> Option<ImmLogic> {\n        self.imm_logic_from_u64(ty, n.bits() as u64)\n    }\n\n    fn imm12_from_u64(&mut self, n: u64) -> Option<Imm12> {\n        Imm12::maybe_from_u64(n)\n    }\n\n    fn imm12_from_negated_u64(&mut self, n: u64) -> Option<Imm12> {\n        Imm12::maybe_from_u64((n as i64).wrapping_neg() as u64)\n    }\n\n    fn imm_shift_from_u8(&mut self, n: u8) -> ImmShift {\n        ImmShift::maybe_from_u64(n.into()).unwrap()\n    }\n\n    fn lshl_from_imm64(&mut self, ty: Type, n: Imm64) -> Option<ShiftOpAndAmt> {\n        let shiftimm = ShiftOpShiftImm::maybe_from_shift(n.bits() as u64)?;\n        let shiftee_bits = ty_bits(ty);\n        if shiftee_bits <= std::u8::MAX as usize {\n            let shiftimm = shiftimm.mask(shiftee_bits as u8);\n            Some(ShiftOpAndAmt::new(ShiftOp::LSL, shiftimm))\n        } else {\n            None\n        }\n    }\n\n    fn integral_ty(&mut self, ty: Type) -> Option<Type> {\n        match ty {\n            I8 | I16 | I32 | I64 | R64 => Some(ty),\n            ty if ty.is_bool() => Some(ty),\n            _ => None,\n        }\n    }\n\n    /// This is target-word-size dependent.  And it excludes booleans and reftypes.\n    fn valid_atomic_transaction(&mut self, ty: Type) -> Option<Type> {\n        match ty {\n            I8 | I16 | I32 | I64 => Some(ty),\n            _ => None,\n        }\n    }\n\n    /// This is the fallback case for loading a 64-bit integral constant into a\n    /// register.\n    ///\n    /// The logic here is nontrivial enough that it's not really worth porting\n    /// this over to ISLE.\n    fn load_constant64_full(&mut self, value: u64) -> Reg {\n        // If the top 32 bits are zero, use 32-bit `mov` operations.\n        let (num_half_words, size, negated) = if value >> 32 == 0 {\n            (2, OperandSize::Size32, (!value << 32) >> 32)\n        } else {\n            (4, OperandSize::Size64, !value)\n        };\n        // If the number of 0xffff half words is greater than the number of 0x0000 half words\n        // it is more efficient to use `movn` for the first instruction.\n        let first_is_inverted = count_zero_half_words(negated, num_half_words)\n            > count_zero_half_words(value, num_half_words);\n        // Either 0xffff or 0x0000 half words can be skipped, depending on the first\n        // instruction used.\n        let ignored_halfword = if first_is_inverted { 0xffff } else { 0 };\n        let mut first_mov_emitted = false;\n\n        let rd = self.temp_writable_reg(I64);\n\n        for i in 0..num_half_words {\n            let imm16 = (value >> (16 * i)) & 0xffff;\n            if imm16 != ignored_halfword {\n                if !first_mov_emitted {\n                    first_mov_emitted = true;\n                    if first_is_inverted {\n                        let imm =\n                            MoveWideConst::maybe_with_shift(((!imm16) & 0xffff) as u16, i * 16)\n                                .unwrap();\n                        self.emit(&MInst::MovWide {\n                            op: MoveWideOp::MovN,\n                            rd,\n                            imm,\n                            size,\n                        });\n                    } else {\n                        let imm = MoveWideConst::maybe_with_shift(imm16 as u16, i * 16).unwrap();\n                        self.emit(&MInst::MovWide {\n                            op: MoveWideOp::MovZ,\n                            rd,\n                            imm,\n                            size,\n                        });\n                    }\n                } else {\n                    let imm = MoveWideConst::maybe_with_shift(imm16 as u16, i * 16).unwrap();\n                    self.emit(&MInst::MovWide {\n                        op: MoveWideOp::MovK,\n                        rd,\n                        imm,\n                        size,\n                    });\n                }\n            }\n        }\n\n        assert!(first_mov_emitted);\n\n        return self.writable_reg_to_reg(rd);\n\n        fn count_zero_half_words(mut value: u64, num_half_words: u8) -> usize {\n            let mut count = 0;\n            for _ in 0..num_half_words {\n                if value & 0xffff == 0 {\n                    count += 1;\n                }\n                value >>= 16;\n            }\n\n            count\n        }\n    }\n\n    fn zero_reg(&mut self) -> Reg {\n        zero_reg()\n    }\n\n    fn xreg(&mut self, index: u8) -> Reg {\n        xreg(index)\n    }\n\n    fn writable_xreg(&mut self, index: u8) -> WritableReg {\n        writable_xreg(index)\n    }\n\n    fn extended_value_from_value(&mut self, val: Value) -> Option<ExtendedValue> {\n        let (val, extend) =\n            super::get_as_extended_value(self.lower_ctx, val, NarrowValueMode::None)?;\n        Some(ExtendedValue { val, extend })\n    }\n\n    fn put_extended_in_reg(&mut self, reg: &ExtendedValue) -> Reg {\n        self.put_in_reg(reg.val)\n    }\n\n    fn get_extended_op(&mut self, reg: &ExtendedValue) -> ExtendOp {\n        reg.extend\n    }\n\n    fn emit(&mut self, inst: &MInst) -> Unit {\n        self.lower_ctx.emit(inst.clone());\n    }\n\n    fn cond_br_zero(&mut self, reg: Reg) -> CondBrKind {\n        CondBrKind::Zero(reg)\n    }\n\n    fn cond_br_cond(&mut self, cond: &Cond) -> CondBrKind {\n        CondBrKind::Cond(*cond)\n    }\n\n    fn nzcv(&mut self, n: bool, z: bool, c: bool, v: bool) -> NZCV {\n        NZCV::new(n, z, c, v)\n    }\n\n    fn u8_into_uimm5(&mut self, x: u8) -> UImm5 {\n        UImm5::maybe_from_u8(x).unwrap()\n    }\n\n    fn u8_into_imm12(&mut self, x: u8) -> Imm12 {\n        Imm12::maybe_from_u64(x.into()).unwrap()\n    }\n\n    fn writable_zero_reg(&mut self) -> WritableReg {\n        writable_zero_reg()\n    }\n\n    fn safe_divisor_from_imm64(&mut self, val: Imm64) -> Option<u64> {\n        match val.bits() {\n            0 | -1 => None,\n            n => Some(n as u64),\n        }\n    }\n\n    fn sinkable_atomic_load(&mut self, val: Value) -> Option<SinkableAtomicLoad> {\n        let input = self.lower_ctx.get_value_as_source_or_const(val);\n        if let InputSourceInst::UniqueUse(atomic_load, 0) = input.inst {\n            if self.lower_ctx.data(atomic_load).opcode() == Opcode::AtomicLoad {\n                let atomic_addr = self.lower_ctx.input_as_value(atomic_load, 0);\n                return Some(SinkableAtomicLoad {\n                    atomic_load,\n                    atomic_addr,\n                });\n            }\n        }\n        None\n    }\n\n    fn sink_atomic_load(&mut self, load: &SinkableAtomicLoad) -> Reg {\n        self.lower_ctx.sink_inst(load.atomic_load);\n        self.put_in_reg(load.atomic_addr)\n    }\n\n    fn shift_mask(&mut self, ty: Type) -> ImmLogic {\n        let mask = (ty.bits() - 1) as u64;\n        ImmLogic::maybe_from_u64(mask, I32).unwrap()\n    }\n\n    fn imm_shift_from_imm64(&mut self, ty: Type, val: Imm64) -> Option<ImmShift> {\n        let imm_value = (val.bits() as u64) & ((ty.bits() - 1) as u64);\n        ImmShift::maybe_from_u64(imm_value)\n    }\n\n    fn u64_into_imm_logic(&mut self, ty: Type, val: u64) -> ImmLogic {\n        ImmLogic::maybe_from_u64(val, ty).unwrap()\n    }\n\n    fn negate_imm_shift(&mut self, ty: Type, mut imm: ImmShift) -> ImmShift {\n        let size = u8::try_from(ty.bits()).unwrap();\n        imm.imm = size.wrapping_sub(imm.value());\n        imm.imm &= size - 1;\n        imm\n    }\n\n    fn rotr_mask(&mut self, ty: Type) -> ImmLogic {\n        ImmLogic::maybe_from_u64((ty.bits() - 1) as u64, I32).unwrap()\n    }\n\n    fn rotr_opposite_amount(&mut self, ty: Type, val: ImmShift) -> ImmShift {\n        let amount = val.value() & u8::try_from(ty.bits() - 1).unwrap();\n        ImmShift::maybe_from_u64(u64::from(ty.bits()) - u64::from(amount)).unwrap()\n    }\n\n    fn icmp_zero_cond(&mut self, cond: &IntCC) -> Option<IntCC> {\n        match cond {\n            &IntCC::Equal\n            | &IntCC::SignedGreaterThanOrEqual\n            | &IntCC::SignedGreaterThan\n            | &IntCC::SignedLessThanOrEqual\n            | &IntCC::SignedLessThan => Some(*cond),\n            _ => None,\n        }\n    }\n\n    fn fcmp_zero_cond(&mut self, cond: &FloatCC) -> Option<FloatCC> {\n        match cond {\n            &FloatCC::Equal\n            | &FloatCC::GreaterThanOrEqual\n            | &FloatCC::GreaterThan\n            | &FloatCC::LessThanOrEqual\n            | &FloatCC::LessThan => Some(*cond),\n            _ => None,\n        }\n    }\n\n    fn fcmp_zero_cond_not_eq(&mut self, cond: &FloatCC) -> Option<FloatCC> {\n        match cond {\n            &FloatCC::NotEqual => Some(FloatCC::NotEqual),\n            _ => None,\n        }\n    }\n\n    fn icmp_zero_cond_not_eq(&mut self, cond: &IntCC) -> Option<IntCC> {\n        match cond {\n            &IntCC::NotEqual => Some(IntCC::NotEqual),\n            _ => None,\n        }\n    }\n\n    fn float_cc_cmp_zero_to_vec_misc_op(&mut self, cond: &FloatCC) -> VecMisc2 {\n        match cond {\n            &FloatCC::Equal => VecMisc2::Fcmeq0,\n            &FloatCC::GreaterThanOrEqual => VecMisc2::Fcmge0,\n            &FloatCC::LessThanOrEqual => VecMisc2::Fcmle0,\n            &FloatCC::GreaterThan => VecMisc2::Fcmgt0,\n            &FloatCC::LessThan => VecMisc2::Fcmlt0,\n            _ => panic!(),\n        }\n    }\n\n    fn int_cc_cmp_zero_to_vec_misc_op(&mut self, cond: &IntCC) -> VecMisc2 {\n        match cond {\n            &IntCC::Equal => VecMisc2::Cmeq0,\n            &IntCC::SignedGreaterThanOrEqual => VecMisc2::Cmge0,\n            &IntCC::SignedLessThanOrEqual => VecMisc2::Cmle0,\n            &IntCC::SignedGreaterThan => VecMisc2::Cmgt0,\n            &IntCC::SignedLessThan => VecMisc2::Cmlt0,\n            _ => panic!(),\n        }\n    }\n\n    fn float_cc_cmp_zero_to_vec_misc_op_swap(&mut self, cond: &FloatCC) -> VecMisc2 {\n        match cond {\n            &FloatCC::Equal => VecMisc2::Fcmeq0,\n            &FloatCC::GreaterThanOrEqual => VecMisc2::Fcmle0,\n            &FloatCC::LessThanOrEqual => VecMisc2::Fcmge0,\n            &FloatCC::GreaterThan => VecMisc2::Fcmlt0,\n            &FloatCC::LessThan => VecMisc2::Fcmgt0,\n            _ => panic!(),\n        }\n    }\n\n    fn int_cc_cmp_zero_to_vec_misc_op_swap(&mut self, cond: &IntCC) -> VecMisc2 {\n        match cond {\n            &IntCC::Equal => VecMisc2::Cmeq0,\n            &IntCC::SignedGreaterThanOrEqual => VecMisc2::Cmle0,\n            &IntCC::SignedLessThanOrEqual => VecMisc2::Cmge0,\n            &IntCC::SignedGreaterThan => VecMisc2::Cmlt0,\n            &IntCC::SignedLessThan => VecMisc2::Cmgt0,\n            _ => panic!(),\n        }\n    }\n\n    fn zero_value(&mut self, value: Imm64) -> Option<Imm64> {\n        if value.bits() == 0 {\n            return Some(value);\n        }\n        None\n    }\n\n    fn zero_value_f32(&mut self, value: Ieee32) -> Option<Ieee32> {\n        if value.bits() == 0 {\n            return Some(value);\n        }\n        None\n    }\n\n    fn zero_value_f64(&mut self, value: Ieee64) -> Option<Ieee64> {\n        if value.bits() == 0 {\n            return Some(value);\n        }\n        None\n    }\n}\n", "test compile precise-output\nset unwind_info=false\ntarget aarch64\n\nfunction %f1(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = iadd.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   add x0, x0, x1\n;   ret\n\nfunction %f2(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = isub.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   sub x0, x0, x1\n;   ret\n\nfunction %f3(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = imul.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   madd x0, x0, x1, xzr\n;   ret\n\nfunction %f4(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = umulhi.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   umulh x0, x0, x1\n;   ret\n\nfunction %f5(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = smulhi.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   smulh x0, x0, x1\n;   ret\n\nfunction %f6(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = sdiv.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   cbnz x1, 8 ; udf\n;   adds xzr, x1, #1\n;   ccmp x0, #1, #nzcv, eq\n;   b.vc 8 ; udf\n;   sdiv x0, x0, x1\n;   ret\n\nfunction %f7(i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 2\n  v2 = sdiv.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   orr x3, xzr, #2\n;   sdiv x0, x0, x3\n;   ret\n\nfunction %f8(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = udiv.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   cbnz x1, 8 ; udf\n;   udiv x0, x0, x1\n;   ret\n\nfunction %f9(i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 2\n  v2 = udiv.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   orr x3, xzr, #2\n;   udiv x0, x0, x3\n;   ret\n\nfunction %f10(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = srem.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   cbnz x1, 8 ; udf\n;   sdiv x6, x0, x1\n;   msub x0, x6, x1, x0\n;   ret\n\nfunction %f11(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = urem.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   cbnz x1, 8 ; udf\n;   udiv x6, x0, x1\n;   msub x0, x6, x1, x0\n;   ret\n\nfunction %f12(i32, i32) -> i32 {\nblock0(v0: i32, v1: i32):\n  v2 = sdiv.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   sxtw x5, w0\n;   sxtw x7, w1\n;   cbnz x7, 8 ; udf\n;   adds wzr, w7, #1\n;   ccmp w5, #1, #nzcv, eq\n;   b.vc 8 ; udf\n;   sdiv x0, x5, x7\n;   ret\n\nfunction %f13(i32) -> i32 {\nblock0(v0: i32):\n  v1 = iconst.i32 2\n  v2 = sdiv.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   sxtw x3, w0\n;   orr x5, xzr, #2\n;   sdiv x0, x3, x5\n;   ret\n\nfunction %f14(i32, i32) -> i32 {\nblock0(v0: i32, v1: i32):\n  v2 = udiv.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   mov w5, w0\n;   mov w7, w1\n;   cbnz x7, 8 ; udf\n;   udiv x0, x5, x7\n;   ret\n\nfunction %f15(i32) -> i32 {\nblock0(v0: i32):\n  v1 = iconst.i32 2\n  v2 = udiv.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   mov w3, w0\n;   orr x5, xzr, #2\n;   udiv x0, x3, x5\n;   ret\n\nfunction %f16(i32, i32) -> i32 {\nblock0(v0: i32, v1: i32):\n  v2 = srem.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   sxtw x5, w0\n;   sxtw x7, w1\n;   cbnz x7, 8 ; udf\n;   sdiv x10, x5, x7\n;   msub x0, x10, x7, x5\n;   ret\n\nfunction %f17(i32, i32) -> i32 {\nblock0(v0: i32, v1: i32):\n  v2 = urem.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   mov w5, w0\n;   mov w7, w1\n;   cbnz x7, 8 ; udf\n;   udiv x10, x5, x7\n;   msub x0, x10, x7, x5\n;   ret\n\nfunction %f18(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = band.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   and x0, x0, x1\n;   ret\n\nfunction %f19(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = bor.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   orr x0, x0, x1\n;   ret\n\nfunction %f20(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = bxor.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   eor x0, x0, x1\n;   ret\n\nfunction %f21(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = band_not.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   bic x0, x0, x1\n;   ret\n\nfunction %f22(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = bor_not.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   orn x0, x0, x1\n;   ret\n\nfunction %f23(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = bxor_not.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   eon x0, x0, x1\n;   ret\n\nfunction %f24(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = bnot.i64 v0\n  return v2\n}\n\n; block0:\n;   orn x0, xzr, x0\n;   ret\n\nfunction %f25(i32, i32) -> i32 {\nblock0(v0: i32, v1: i32):\n  v2 = iconst.i32 53\n  v3 = ishl.i32 v0, v2\n  v4 = isub.i32 v1, v3\n  return v4\n}\n\n; block0:\n;   sub w0, w1, w0, LSL 21\n;   ret\n\nfunction %f26(i32) -> i32 {\nblock0(v0: i32):\n  v1 = iconst.i32 -1\n  v2 = iadd.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   sub w0, w0, #1\n;   ret\n\nfunction %f27(i32) -> i32 {\nblock0(v0: i32):\n  v1 = iconst.i32 -1\n  v2 = isub.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   add w0, w0, #1\n;   ret\n\nfunction %f28(i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 -1\n  v2 = isub.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   add x0, x0, #1\n;   ret\n\nfunction %f29(i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 1\n  v2 = ineg v1\n  return v2\n}\n\n; block0:\n;   movz x3, #1\n;   sub x0, xzr, x3\n;   ret\n\nfunction %f30(i8x16) -> i8x16 {\nblock0(v0: i8x16):\n  v1 = iconst.i64 1\n  v2 = ushr.i8x16 v0, v1\n  return v2\n}\n\n; block0:\n;   movz x3, #1\n;   sub w5, wzr, w3\n;   dup v7.16b, w5\n;   ushl v0.16b, v0.16b, v7.16b\n;   ret\n\nfunction %add_i128(i128, i128) -> i128 {\nblock0(v0: i128, v1: i128):\n    v2 = iadd v0, v1\n    return v2\n}\n\n; block0:\n;   adds x0, x0, x2\n;   adc x1, x1, x3\n;   ret\n\nfunction %sub_i128(i128, i128) -> i128 {\nblock0(v0: i128, v1: i128):\n    v2 = isub v0, v1\n    return v2\n}\n\n; block0:\n;   subs x0, x0, x2\n;   sbc x1, x1, x3\n;   ret\n\nfunction %mul_i128(i128, i128) -> i128 {\nblock0(v0: i128, v1: i128):\n    v2 = imul v0, v1\n    return v2\n}\n\n; block0:\n;   umulh x10, x0, x2\n;   madd x12, x0, x3, x10\n;   madd x1, x1, x2, x12\n;   madd x0, x0, x2, xzr\n;   ret\n\nfunction %add_mul_1(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = iadd v0, v3\n    return v4\n}\n\n; block0:\n;   madd w0, w1, w2, w0\n;   ret\n\nfunction %add_mul_2(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = iadd v3, v0\n    return v4\n}\n\n; block0:\n;   madd w0, w1, w2, w0\n;   ret\n\nfunction %msub_i32(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n\n; block0:\n;   msub w0, w1, w2, w0\n;   ret\n\nfunction %msub_i64(i64, i64, i64) -> i64 {\nblock0(v0: i64, v1: i64, v2: i64):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n\n; block0:\n;   msub x0, x1, x2, x0\n;   ret\n\nfunction %imul_sub_i32(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = isub v3, v0\n    return v4\n}\n\n; block0:\n;   madd w8, w1, w2, wzr\n;   sub w0, w8, w0\n;   ret\n\nfunction %imul_sub_i64(i64, i64, i64) -> i64 {\nblock0(v0: i64, v1: i64, v2: i64):\n    v3 = imul v1, v2\n    v4 = isub v3, v0\n    return v4\n}\n\n; block0:\n;   madd x8, x1, x2, xzr\n;   sub x0, x8, x0\n;   ret\n\nfunction %srem_const (i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 2\n  v2 = srem.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   orr x3, xzr, #2\n;   sdiv x5, x0, x3\n;   msub x0, x5, x3, x0\n;   ret\n\nfunction %urem_const (i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 2\n  v2 = urem.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   orr x3, xzr, #2\n;   udiv x5, x0, x3\n;   msub x0, x5, x3, x0\n;   ret\n\nfunction %sdiv_minus_one(i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 -1\n  v2 = sdiv.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   movn x3, #0\n;   adds xzr, x3, #1\n;   ccmp x0, #1, #nzcv, eq\n;   b.vc 8 ; udf\n;   sdiv x0, x0, x3\n;   ret\n\n", "test interpret\ntest run\ntarget aarch64\ntarget s390x\ntarget x86_64\n\nfunction %add_i64(i64, i64) -> i64 {\nblock0(v0: i64,v1: i64):\n    v2 = iadd v0, v1\n    return v2\n}\n; run: %add_i64(0, 0) == 0\n; run: %add_i64(0, 1) == 1\n; run: %add_i64(-1, 0) == -1\n; run: %add_i64(-1, 1) == 0\n; run: %add_i64(0x7FFFFFFF_FFFFFFFF, 0x7FFFFFFF_FFFFFFFF) == -2\n; run: %add_i64(0x7FFFFFFF_FFFFFFFF, 0x80000000_00000000) == -1\n; run: %add_i64(0x01234567_89ABCDEF, 0xFEDCBA98_76543210) == -1\n; run: %add_i64(0xA00A00A0_0A00A00A, 0x0BB0BB0B_B0BB0BB0) == 0xABBABBAB_BABBABBA\n; run: %add_i64(0xC0FFEEEE_C0FFEEEE, 0x1DCB1111_1DCB1111) == 0xDECAFFFF_DECAFFFF\n\nfunction %add_i32(i32, i32) -> i32 {\nblock0(v0: i32,v1: i32):\n    v2 = iadd v0, v1\n    return v2\n}\n; run: %add_i32(0, 0) == 0\n; run: %add_i32(0, 1) == 1\n; run: %add_i32(-1, 0) == -1\n; run: %add_i32(-1, 1) == 0\n; run: %add_i32(0x7FFFFFFF, 0x7FFFFFFF) == -2\n; run: %add_i32(0x7FFFFFFF, 0x80000000) == -1\n; run: %add_i32(0x01234567, 0xFEDCBA98) == -1\n; run: %add_i32(0xA00A00A0, 0x0BB0BB0B) == 0xABBABBAB\n; run: %add_i32(0xC0FFEEEE, 0x1DCB1111) == 0xDECAFFFF\n\nfunction %add_i16(i16, i16) -> i16 {\nblock0(v0: i16,v1: i16):\n    v2 = iadd v0, v1\n    return v2\n}\n; run: %add_i16(0, 0) == 0\n; run: %add_i16(0, 1) == 1\n; run: %add_i16(-1, 0) == -1\n; run: %add_i16(-1, 1) == 0\n; run: %add_i16(0x7FFF, 0x7FFF) == -2\n; run: %add_i16(0x7FFF, 0x8000) == -1\n; run: %add_i16(0x0123, 0xFEDC) == -1\n; run: %add_i16(0xA00A, 0x0BB0) == 0xABBA\n; run: %add_i16(0xC0FF, 0x1DCB) == 0xDECA\n\nfunction %add_i8(i8, i8) -> i8 {\nblock0(v0: i8,v1: i8):\n    v2 = iadd v0, v1\n    return v2\n}\n; run: %add_i8(0, 0) == 0\n; run: %add_i8(0, 1) == 1\n; run: %add_i8(-1, 0) == -1\n; run: %add_i8(-1, 1) == 0\n; run: %add_i8(0x7F, 0x7F) == -2\n; run: %add_i8(0x7F, 0x80) == -1\n; run: %add_i8(0x01, 0xFE) == -1\n; run: %add_i8(0xA0, 0x0B) == 0xAB\n; run: %add_i8(0xC0, 0x1D) == 0xDD\n\n\nfunction %sub_i64(i64, i64) -> i64 {\nblock0(v0: i64,v1: i64):\n    v2 = isub v0, v1\n    return v2\n}\n; run: %sub_i64(0, 0) == 0\n; run: %sub_i64(0, 1) == -1\n; run: %sub_i64(1, 0) == 1\n; run: %sub_i64(-1, 0) == -1\n; run: %sub_i64(-1, 1) == -2\n; run: %sub_i64(0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 1\n; run: %sub_i64(0xFFFFFFFF_FFFFFFFF, 0xFEDCBA98_76543210) == 0x01234567_89ABCDEF\n; run: %sub_i64(0xABBABBAB_BABBABBA, 0x0BB0BB0B_B0BB0BB0) == 0xA00A00A0_0A00A00A\n; run: %sub_i64(0xC0FFEEEE_C0FFEEEE, 0xDECAFFFF_DECAFFFF) == 0xE234EEEE_E234EEEF\n\nfunction %sub_i32(i32, i32) -> i32 {\nblock0(v0: i32,v1: i32):\n    v2 = isub v0, v1\n    return v2\n}\n; run: %sub_i32(0, 0) == 0\n; run: %sub_i32(0, 1) == -1\n; run: %sub_i32(1, 0) == 1\n; run: %sub_i32(-1, 0) == -1\n; run: %sub_i32(-1, 1) == -2\n; run: %sub_i32(0x80000000, 0x7FFFFFFF) == 1\n; run: %sub_i32(0xFFFFFFFF, 0xFEDCBA98) == 0x01234567\n; run: %sub_i32(0xABBABBAB, 0x0BB0BB0B) == 0xA00A00A0\n; run: %sub_i32(0xC0FFEEEE, 0xDECAFFFF) == 0xE234EEEF\n\nfunction %sub_i16(i16, i16) -> i16 {\nblock0(v0: i16,v1: i16):\n    v2 = isub v0, v1\n    return v2\n}\n; run: %sub_i16(0, 0) == 0\n; run: %sub_i16(0, 1) == -1\n; run: %sub_i16(1, 0) == 1\n; run: %sub_i16(-1, 0) == -1\n; run: %sub_i16(-1, 1) == -2\n; run: %sub_i16(0x8000, 0x7FFF) == 1\n; run: %sub_i16(0xFFFF, 0xFEDC) == 0x0123\n; run: %sub_i16(0xABBA, 0x0BB0) == 0xA00A\n; run: %sub_i16(0xC0FF, 0xDECA) == 0xE235\n\nfunction %sub_i8(i8, i8) -> i8 {\nblock0(v0: i8,v1: i8):\n    v2 = isub v0, v1\n    return v2\n}\n; run: %sub_i8(0, 0) == 0\n; run: %sub_i8(0, 1) == -1\n; run: %sub_i8(1, 0) == 1\n; run: %sub_i8(-1, 0) == -1\n; run: %sub_i8(-1, 1) == -2\n; run: %sub_i8(0x80, 0x7F) == 1\n; run: %sub_i8(0xFF, 0xFE) == 0x01\n; run: %sub_i8(0xAB, 0x0B) == 0xA0\n; run: %sub_i8(0xC0, 0xDE) == 0xE2\n\n\nfunction %mul_i64(i64, i64) -> i64 {\nblock0(v0: i64,v1: i64):\n    v2 = imul v0, v1\n    return v2\n}\n; run: %mul_i64(0, 0) == 0\n; run: %mul_i64(0, 1) == 0\n; run: %mul_i64(1, -1) == -1\n; run: %mul_i64(2, 2) == 4\n; run: %mul_i64(0x7FFFFFFF_FFFFFFFF, 0x7FFFFFFF_FFFFFFFF) == 1\n; run: %mul_i64(0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 0x80000000_00000000\n; run: %mul_i64(0x01234567_89ABCDEF, 0xFEDCBA98_76543210) == 0x2236D88F_E5618CF0\n; run: %mul_i64(0xC0FFEEEE_C0FFEEEE, 0xDECAFFFF_DECAFFFF) == 0xDB6B1E48_19BA1112\n\nfunction %mul_i32(i32, i32) -> i32 {\nblock0(v0: i32,v1: i32):\n    v2 = imul v0, v1\n    return v2\n}\n; run: %mul_i32(0, 0) == 0\n; run: %mul_i32(0, 1) == 0\n; run: %mul_i32(1, -1) == -1\n; run: %mul_i32(2, 2) == 4\n; run: %mul_i32(0x7FFFFFFF, 0x7FFFFFFF) == 1\n; run: %mul_i32(0x80000000, 0x7FFFFFFF) == 0x80000000\n; run: %mul_i32(0x01234567, 0xFEDCBA98) == 0x23E20B28\n; run: %mul_i32(0xC0FFEEEE, 0xDECAFFFF) == 0x19BA1112\n\nfunction %mul_i16(i16, i16) -> i16 {\nblock0(v0: i16,v1: i16):\n    v2 = imul v0, v1\n    return v2\n}\n; run: %mul_i16(0, 0) == 0\n; run: %mul_i16(0, 1) == 0\n; run: %mul_i16(1, -1) == -1\n; run: %mul_i16(2, 2) == 4\n; run: %mul_i16(0x7FFF, 0x7FFF) == 1\n; run: %mul_i16(0x8000, 0x7FFF) == 0x8000\n; run: %mul_i16(0x0123, 0xFEDC) == 0xB414\n; run: %mul_i16(0xC0FF, 0xDECA) == 0x6B36\n\nfunction %mul_i8(i8, i8) -> i8 {\nblock0(v0: i8,v1: i8):\n    v2 = imul v0, v1\n    return v2\n}\n; run: %mul_i8(0, 0) == 0\n; run: %mul_i8(0, 1) == 0\n; run: %mul_i8(1, -1) == -1\n; run: %mul_i8(2, 2) == 4\n; run: %mul_i8(0x7F, 0x7F) == 1\n; run: %mul_i8(0x80, 0x7F) == 0x80\n; run: %mul_i8(0x01, 0xFE) == 0xFE\n; run: %mul_i8(0xC0, 0xDE) == 0x80\n\n\nfunction %madd_i8(i8, i8, i8) -> i8 {\nblock0(v0: i8, v1: i8, v2: i8):\n    v3 = imul v1, v2\n    v4 = iadd v0, v3\n    return v4\n}\n; run: %madd_i8(0, 1, 0) == 0\n; run: %madd_i8(1, 0, 0) == 1\n; run: %madd_i8(0, -1, 1) == -1\n; run: %madd_i8(2, 2, 2) == 6\n; run: %madd_i8(0, 0x7F, 0x7F) == 1\n; run: %madd_i8(0x7F, 0x7F, -1) == 0\n; run: %madd_i8(0x80, 0x7F, 0) == 0x80\n; run: %madd_i8(0x80, 0x7F, 0x80) == 0\n; run: %madd_i8(0x01, 0xFE, 0) == 1\n; run: %madd_i8(0x01, 0xFE, 2) == -3\n; run: %madd_i8(0, 0xC0, 0xDE) == 0x80\n; run: %madd_i8(0xC0, 0xC0, 0xDE) == 0x40\n\nfunction %madd_i16(i16, i16, i16) -> i16 {\nblock0(v0: i16, v1: i16, v2: i16):\n    v3 = imul v1, v2\n    v4 = iadd v0, v3\n    return v4\n}\n; run: %madd_i16(0, 1, 0) == 0\n; run: %madd_i16(1, 0, 0) == 1\n; run: %madd_i16(0, -1, 1) == -1\n; run: %madd_i16(2, 2, 2) == 6\n; run: %madd_i16(0, 0x7FFF, 0x7FFF) == 1\n; run: %madd_i16(0x7FFF, 1, 0x7FFF) == -2\n; run: %madd_i16(0x8000, 1, 0x7FFF) == -1\n; run: %madd_i16(0x0123, 0x0456, 0xFEDC) == 0xF0B\n; run: %madd_i16(0xC0FF, 0x0123, 0xDECA) == 0x9D\n\nfunction %madd_i32(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = iadd v0, v3\n    return v4\n}\n; run: %madd_i32(0, 1, 0) == 0\n; run: %madd_i32(1, 0, 0) == 1\n; run: %madd_i32(0, -1, 1) == -1\n; run: %madd_i32(2, 2, 2) == 6\n; run: %madd_i32(0, 0x7FFFFFFF, 0x7FFFFFFF) == 1\n; run: %madd_i32(-1, 0x7FFFFFFF, 0x7FFFFFFF) == 0\n; run: %madd_i32(0x80000000, 1, 0x7FFFFFFF) == -1\n; run: %madd_i32(0x80000000, 0x01234567, 0x7FFFFFFF) == 0xFEDCBA99\n; run: %madd_i32(0x01234567, 0x80000000, 0xFEDCBA98) == 0x1234567\n; run: %madd_i32(0xC0FFEEEE, 0xDECAFFFF, 0x0DEBAC1E) == 0x32DE42D0\n\nfunction %madd_i64(i64, i64, i64) -> i64 {\nblock0(v0: i64, v1: i64, v2: i64):\n    v3 = imul v1, v2\n    v4 = iadd v0, v3\n    return v4\n}\n; run: %madd_i64(0, 1, 0) == 0\n; run: %madd_i64(1, 0, 0) == 1\n; run: %madd_i64(0, -1, 1) == -1\n; run: %madd_i64(2, 2, 2) == 6\n; run: %madd_i64(0, 0x7FFFFFFF_FFFFFFFF, 0x7FFFFFFF_FFFFFFFF) == 1\n; run: %madd_i64(-1, 0x7FFFFFFF_FFFFFFFF, 0x7FFFFFFF_FFFFFFFF) == 0\n; run: %madd_i64(0, 0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 0x80000000_00000000\n; run: %madd_i64(1, 0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 0x80000000_00000001\n; run: %madd_i64(0x01234567_89ABCDEF, 0x01234567_FEDCBA98, 0xFEDCBA98_76543210) == 0x89C0845D_DDC9276F\n; run: %madd_i64(0xC0FFEEEE_C0FFEEEE, 0xBAADF00D_BAADF00D, 0xDECAFFFF_DECAFFFF) == 0x2EB8ECEC_A6A0FEE1\n\nfunction %msub_i8(i8, i8, i8) -> i8 {\nblock0(v0: i8, v1: i8, v2: i8):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n; run: %msub_i8(0, 0, 0) == 0\n; run: %msub_i8(1, 1, 1) == 0\n; run: %msub_i8(1, 1, 0) == 1\n; run: %msub_i8(0, 1, 1) == -1\n; run: %msub_i8(1, 1, -1) == 2\n; run: %msub_i8(-2, 1, -1) == -1\n; run: %msub_i8(2, 2, 2) == -2\n; run: %msub_i8(0, 0x7F, 0x7F) == -1\n; run: %msub_i8(0x7F, 0x80, 0x7F) == -1\n; run: %msub_i8(0x80, 1, 0x7F) == 1\n; run: %msub_i8(0x01, 0x80, 0xFE) == 1\n; run: %msub_i8(0xFF, 1, 0xDE) == 0x21\n\nfunction %msub_i16(i16, i16, i16) -> i16 {\nblock0(v0: i16, v1: i16, v2: i16):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n; run: %msub_i16(0, 0, 0) == 0\n; run: %msub_i16(1, 1, 1) == 0\n; run: %msub_i16(1, 1, 0) == 1\n; run: %msub_i16(0, 1, 1) == -1\n; run: %msub_i16(1, 1, -1) == 2\n; run: %msub_i16(-2, 1, -1) == -1\n; run: %msub_i16(2, 2, 2) == -2\n; run: %msub_i16(0, 0x7FFF, 0x7FFF) == -1\n; run: %msub_i16(0x0FFF, 1, 0x7FFF) == 0x9000\n; run: %msub_i16(0x7000, 1, 0x7FFF) == 0xF001\n; run: %msub_i16(0x0123, 0x0456, 0xFEDC) == 0xF33B\n; run: %msub_i16(0xC0FF, 0x0123, 0xDECA) == 0x8161\n\nfunction %msub_i32(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n; run: %msub_i32(0, 0, 0) == 0\n; run: %msub_i32(1, 1, 1) == 0\n; run: %msub_i32(1, 1, 0) == 1\n; run: %msub_i32(0, 1, 1) == -1\n; run: %msub_i32(1, 1, -1) == 2\n; run: %msub_i32(-2, 1, -1) == -1\n; run: %msub_i32(2, 2, 2) == -2\n; run: %msub_i32(0, 0x7FFFFFFF, 0x7FFFFFFF) == -1\n; run: %msub_i32(0x0FFFFF, 1, 0x7FFFFFFF) == 0x80100000\n; run: %msub_i32(0x7FFFFFFF, 1, 0x80000000) == -1\n; run: %msub_i32(0x80000000, 0x01234567, 0x7FFFFFFF) == 0x01234567\n; run: %msub_i32(0xFEDCBA98, 0x80000000, 0x01234567) == 0x7EDCBA98\n; run: %msub_i32(0xC0FFEEEE, 0xDECAFFFF, 0x0DEBAC1E) == 0x4F219B0C\n\nfunction %msub_i64(i64, i64, i64) -> i64 {\nblock0(v0: i64, v1: i64, v2: i64):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n; run: %msub_i64(0, 0, 0) == 0\n; run: %msub_i64(1, 1, 1) == 0\n; run: %msub_i64(1, 1, 0) == 1\n; run: %msub_i64(0, 1, 1) == -1\n; run: %msub_i64(1, 1, -1) == 2\n; run: %msub_i64(-2, 1, -1) == -1\n; run: %msub_i64(2, 2, 2) == -2\n; run: %msub_i64(0, 0x7FFFFFFF_FFFFFFFF, 0x7FFFFFFF_FFFFFFFF) == -1\n; run: %msub_i64(0x0FFFFF_FFFFFFFF, 1, 0x7FFFFFFF_FFFFFFFF) == 0x80100000_00000000\n; run: %msub_i64(0, 0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 0x80000000_00000000\n; run: %msub_i64(1, 0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 0x80000000_00000001\n; run: %msub_i64(0x01234567_89ABCDEF, 0x01234567_FEDCBA98, 0xFEDCBA98_76543210) == 0x78860671_358E746F\n; run: %msub_i64(0xC0FFEEEE_C0FFEEEE, 0xBAADF00D_BAADF00D, 0xDECAFFFF_DECAFFFF) == 0x5346F0F0_DB5EDEFB\n\n\nfunction %sdiv_i64(i64, i64) -> i64 {\nblock0(v0: i64,v1: i64):\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i64(0, 1) == 0\n; run: %sdiv_i64(2, 2) == 1\n; run: %sdiv_i64(1, -1) == -1\n; run: %sdiv_i64(3, 2) == 1\n; run: %sdiv_i64(19, 7) == 2\n; run: %sdiv_i64(3, -2) == -1\n; run: %sdiv_i64(-19, 7) == -2\n; run: %sdiv_i64(0xC0FFEEEE_DECAFFFF, 8) == 0xF81FFDDD_DBD96000\n; run: %sdiv_i64(0xC0FFEEEE_DECAFFFF, -8) == 0x7E002222_426A000\n; run: %sdiv_i64(0x80000000_00000000, -2) == 0x40000000_00000000\n\nfunction %sdiv_i32(i32, i32) -> i32 {\nblock0(v0: i32,v1: i32):\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i32(0, 1) == 0\n; run: %sdiv_i32(2, 2) == 1\n; run: %sdiv_i32(1, -1) == -1\n; run: %sdiv_i32(3, 2) == 1\n; run: %sdiv_i32(19, 7) == 2\n; run: %sdiv_i32(3, -2) == -1\n; run: %sdiv_i32(-19, 7) == -2\n; run: %sdiv_i32(0xC0FFEEEE, 8) == 0xF81FFDDE\n; run: %sdiv_i32(0xC0FFEEEE, -8) == 0x7E00222\n; run: %sdiv_i32(0x80000000, -2) == 0x40000000\n\nfunction %sdiv_i16(i16, i16) -> i16 {\nblock0(v0: i16,v1: i16):\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i16(0, 1) == 0\n; run: %sdiv_i16(2, 2) == 1\n; run: %sdiv_i16(1, -1) == -1\n; run: %sdiv_i16(3, 2) == 1\n; run: %sdiv_i16(19, 7) == 2\n; run: %sdiv_i16(3, -2) == -1\n; run: %sdiv_i16(-19, 7) == -2\n; run: %sdiv_i16(0xC0FF, 8) == 0xF820\n; run: %sdiv_i16(0xC0FF, -8) == 0x07E0\n; run: %sdiv_i16(0x8000, -2) == 0x4000\n\nfunction %sdiv_i8(i8, i8) -> i8 {\nblock0(v0: i8,v1: i8):\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i8(0, 1) == 0\n; run: %sdiv_i8(2, 2) == 1\n; run: %sdiv_i8(1, -1) == -1\n; run: %sdiv_i8(3, 2) == 1\n; run: %sdiv_i8(19, 7) == 2\n; run: %sdiv_i8(3, -2) == -1\n; run: %sdiv_i8(-19, 7) == -2\n; run: %sdiv_i8(0xC0, 8) == 0xF8\n; run: %sdiv_i8(0xC0, -8) == 0x08\n; run: %sdiv_i8(0x80, -2) == 0x40\n\n\nfunction %udiv_i64(i64, i64) -> i64 {\nblock0(v0: i64,v1: i64):\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i64(0, 1) == 0\n; run: %udiv_i64(2, 2) == 1\n; run: %udiv_i64(1, -1) == 0\n; run: %udiv_i64(3, 2) == 1\n; run: %udiv_i64(19, 7) == 2\n; run: %udiv_i64(3, -2) == 0\n; run: %udiv_i64(-19, 7) == 0x24924924_9249248F\n; run: %udiv_i64(0xC0FFEEEE_DECAFFFF, 8) == 0x181FFDDD_DBD95FFF\n; run: %udiv_i64(0xC0FFEEEE_DECAFFFF, -8) == 0\n; run: %udiv_i64(0x80000000_00000000, -1) == 0\n; run: %udiv_i64(0x80000000_00000000, -2) == 0\n\nfunction %udiv_i32(i32, i32) -> i32 {\nblock0(v0: i32,v1: i32):\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i32(0, 1) == 0\n; run: %udiv_i32(2, 2) == 1\n; run: %udiv_i32(1, -1) == 0\n; run: %udiv_i32(3, 2) == 1\n; run: %udiv_i32(19, 7) == 2\n; run: %udiv_i32(3, -2) == 0\n; run: %udiv_i32(-19, 7) == 0x24924921\n; run: %udiv_i32(0xC0FFEEEE, 8) == 0x181FFDDD\n; run: %udiv_i32(0xC0FFEEEE, -8) == 0\n; run: %udiv_i32(0x80000000, -1) == 0\n; run: %udiv_i32(0x80000000, -2) == 0\n\nfunction %udiv_i16(i16, i16) -> i16 {\nblock0(v0: i16,v1: i16):\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i16(0, 1) == 0\n; run: %udiv_i16(2, 2) == 1\n; run: %udiv_i16(1, -1) == 0\n; run: %udiv_i16(3, 2) == 1\n; run: %udiv_i16(19, 7) == 2\n; run: %udiv_i16(3, -2) == 0\n; run: %udiv_i16(-19, 7) == 0x248F\n; run: %udiv_i16(0xC0FF, 8) == 0x181F\n; run: %udiv_i16(0xC0FF, -8) == 0\n; run: %udiv_i16(0x8000, -1) == 0\n; run: %udiv_i16(0x8000, -2) == 0\n\nfunction %udiv_i8(i8, i8) -> i8 {\nblock0(v0: i8,v1: i8):\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i8(0, 1) == 0\n; run: %udiv_i8(2, 2) == 1\n; run: %udiv_i8(1, -1) == 0\n; run: %udiv_i8(3, 2) == 1\n; run: %udiv_i8(19, 7) == 2\n; run: %udiv_i8(3, -2) == 0\n; run: %udiv_i8(-19, 7) == 0x21\n; run: %udiv_i8(0xC0, 8) == 0x18\n; run: %udiv_i8(0xC0, -8) == 0\n; run: %udiv_i8(0x80, -1) == 0\n; run: %udiv_i8(0x80, -2) == 0\n"], "fixing_code": [";; Instruction formats.\n(type MInst\n      (enum\n       ;; A no-op of zero size.\n       (Nop0)\n\n       ;; A no-op that is one instruction large.\n       (Nop4)\n\n       ;; An ALU operation with two register sources and a register destination.\n       (AluRRR\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg))\n\n       ;; An ALU operation with three register sources and a register destination.\n       (AluRRRR\n        (alu_op ALUOp3)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (ra Reg))\n\n       ;; An ALU operation with a register source and an immediate-12 source, and a register\n       ;; destination.\n       (AluRRImm12\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (imm12 Imm12))\n\n       ;; An ALU operation with a register source and an immediate-logic source, and a register destination.\n       (AluRRImmLogic\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (imml ImmLogic))\n\n       ;; An ALU operation with a register source and an immediate-shiftamt source, and a register destination.\n       (AluRRImmShift\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (immshift ImmShift))\n\n       ;; An ALU operation with two register sources, one of which can be shifted, and a register\n       ;; destination.\n       (AluRRRShift\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (shiftop ShiftOpAndAmt))\n\n       ;; An ALU operation with two register sources, one of which can be {zero,sign}-extended and\n       ;; shifted, and a register destination.\n       (AluRRRExtend\n        (alu_op ALUOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (extendop ExtendOp))\n\n       ;; A bit op instruction with a single register source.\n       (BitRR\n        (op BitOp)\n        (size OperandSize)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; An unsigned (zero-extending) 8-bit load.\n       (ULoad8\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A signed (sign-extending) 8-bit load.\n       (SLoad8\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; An unsigned (zero-extending) 16-bit load.\n       (ULoad16\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A signed (sign-extending) 16-bit load.\n       (SLoad16\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; An unsigned (zero-extending) 32-bit load.\n       (ULoad32\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A signed (sign-extending) 32-bit load.\n       (SLoad32\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A 64-bit load.\n       (ULoad64\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; An 8-bit store.\n       (Store8\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A 16-bit store.\n       (Store16\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A 32-bit store.\n       (Store32\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A 64-bit store.\n       (Store64\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A store of a pair of registers.\n       (StoreP64\n        (rt Reg)\n        (rt2 Reg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       ;; A load of a pair of registers.\n       (LoadP64\n        (rt WritableReg)\n        (rt2 WritableReg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       ;; A MOV instruction. These are encoded as ORR's (AluRRR form).\n       ;; The 32-bit version zeroes the top 32 bits of the\n       ;; destination, which is effectively an alias for an unsigned\n       ;; 32-to-64-bit extension.\n       (Mov\n        (size OperandSize)\n        (rd WritableReg)\n        (rm Reg))\n\n       ;; A MOV[Z,N,K] with a 16-bit immediate.\n       (MovWide\n        (op MoveWideOp)\n        (rd WritableReg)\n        (imm MoveWideConst)\n        (size OperandSize))\n\n       ;; A sign- or zero-extend operation.\n       (Extend\n        (rd WritableReg)\n        (rn Reg)\n        (signed bool)\n        (from_bits u8)\n        (to_bits u8))\n\n       ;; A conditional-select operation.\n       (CSel\n        (rd WritableReg)\n        (cond Cond)\n        (rn Reg)\n        (rm Reg))\n\n       ;; A conditional-select negation operation.\n       (CSNeg\n        (rd WritableReg)\n        (cond Cond)\n        (rn Reg)\n        (rm Reg))\n\n       ;; A conditional-set operation.\n       (CSet\n        (rd WritableReg)\n        (cond Cond))\n\n       ;; A conditional-set-mask operation.\n       (CSetm\n        (rd WritableReg)\n        (cond Cond))\n\n       ;; A conditional comparison with an immediate.\n       (CCmpImm\n        (size OperandSize)\n        (rn Reg)\n        (imm UImm5)\n        (nzcv NZCV)\n        (cond Cond))\n\n       ;; A synthetic insn, which is a load-linked store-conditional loop, that has the overall\n       ;; effect of atomically modifying a memory location in a particular way.  Because we have\n       ;; no way to explain to the regalloc about earlyclobber registers, this instruction has\n       ;; completely fixed operand registers, and we rely on the RA's coalescing to remove copies\n       ;; in the surrounding code to the extent it can. Load- and store-exclusive instructions,\n       ;; with acquire-release semantics, are used to access memory. The operand conventions are:\n       ;;\n       ;; x25   (rd) address\n       ;; x26   (rd) second operand for `op`\n       ;; x27   (wr) old value\n       ;; x24   (wr) scratch reg; value afterwards has no meaning\n       ;; x28   (wr) scratch reg; value afterwards has no meaning\n       (AtomicRMWLoop\n        (ty Type) ;; I8, I16, I32 or I64\n        (op AtomicRMWLoopOp))\n\n       ;; Similar to AtomicRMWLoop, a compare-and-swap operation implemented using a load-linked\n       ;; store-conditional loop, with acquire-release semantics.\n       ;; Note that the operand conventions, although very similar to AtomicRMWLoop, are different:\n       ;;\n       ;; x25   (rd) address\n       ;; x26   (rd) expected value\n       ;; x28   (rd) replacement value\n       ;; x27   (wr) old value\n       ;; x24   (wr) scratch reg; value afterwards has no meaning\n       (AtomicCASLoop\n        (ty Type) ;; I8, I16, I32 or I64\n        )\n\n       ;; An atomic read-modify-write operation. These instructions require the\n       ;; Large System Extension (LSE) ISA support (FEAT_LSE). The instructions have\n       ;; acquire-release semantics.\n       (AtomicRMW\n         (op AtomicRMWOp)\n         (rs Reg)\n         (rt WritableReg)\n         (rn Reg)\n         (ty Type))\n\n       ;; An atomic compare-and-swap operation. These instructions require the\n       ;; Large System Extension (LSE) ISA support (FEAT_LSE). The instructions have\n       ;; acquire-release semantics.\n       (AtomicCAS\n         (rs WritableReg)\n         (rt Reg)\n         (rn Reg)\n         (ty Type))\n\n       ;; Read `access_ty` bits from address `rt`, either 8, 16, 32 or 64-bits, and put\n       ;; it in `rn`, optionally zero-extending to fill a word or double word result.\n       ;; This instruction is sequentially consistent.\n       (LoadAcquire\n        (access_ty Type) ;; I8, I16, I32 or I64\n        (rt WritableReg)\n        (rn Reg))\n\n       ;; Write the lowest `ty` bits of `rt` to address `rn`.\n       ;; This instruction is sequentially consistent.\n       (StoreRelease\n        (access_ty Type) ;; I8, I16, I32 or I64\n        (rt Reg)\n        (rn Reg))\n\n       ;; A memory fence.  This must provide ordering to ensure that, at a minimum, neither loads\n       ;; nor stores may move forwards or backwards across the fence.  Currently emitted as \"dmb\n       ;; ish\".  This instruction is sequentially consistent.\n       (Fence)\n\n       ;; FPU move. Note that this is distinct from a vector-register\n       ;; move; moving just 64 bits seems to be significantly faster.\n       (FpuMove64\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; Vector register move.\n       (FpuMove128\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; Move to scalar from a vector element.\n       (FpuMoveFromVec\n        (rd WritableReg)\n        (rn Reg)\n        (idx u8)\n        (size VectorSize))\n\n       ;; Zero-extend a SIMD & FP scalar to the full width of a vector register.\n       ;; 16-bit scalars require half-precision floating-point support (FEAT_FP16).\n       (FpuExtend\n        (rd WritableReg)\n        (rn Reg)\n        (size ScalarSize))\n\n       ;; 1-op FPU instruction.\n       (FpuRR\n        (fpu_op FPUOp1)\n        (size ScalarSize)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; 2-op FPU instruction.\n       (FpuRRR\n        (fpu_op FPUOp2)\n        (size ScalarSize)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg))\n\n       (FpuRRI\n        (fpu_op FPUOpRI)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; 3-op FPU instruction.\n       (FpuRRRR\n        (fpu_op FPUOp3)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (ra Reg))\n\n       ;; FPU comparison.\n       (FpuCmp\n        (size ScalarSize)\n        (rn Reg)\n        (rm Reg))\n\n       ;; Floating-point load, single-precision (32 bit).\n       (FpuLoad32\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; Floating-point store, single-precision (32 bit).\n       (FpuStore32\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; Floating-point load, double-precision (64 bit).\n       (FpuLoad64\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; Floating-point store, double-precision (64 bit).\n       (FpuStore64\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; Floating-point/vector load, 128 bit.\n       (FpuLoad128\n        (rd WritableReg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; Floating-point/vector store, 128 bit.\n       (FpuStore128\n        (rd Reg)\n        (mem AMode)\n        (flags MemFlags))\n\n       ;; A load of a pair of floating-point registers, double precision (64-bit).\n       (FpuLoadP64\n        (rt WritableReg)\n        (rt2 WritableReg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       ;; A store of a pair of floating-point registers, double precision (64-bit).\n       (FpuStoreP64\n        (rt Reg)\n        (rt2 Reg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       ;; A load of a pair of floating-point registers, 128-bit.\n       (FpuLoadP128\n        (rt WritableReg)\n        (rt2 WritableReg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       ;; A store of a pair of floating-point registers, 128-bit.\n       (FpuStoreP128\n        (rt Reg)\n        (rt2 Reg)\n        (mem PairAMode)\n        (flags MemFlags))\n\n       (LoadFpuConst64\n        (rd WritableReg)\n        (const_data u64))\n\n       (LoadFpuConst128\n        (rd WritableReg)\n        (const_data u128))\n\n       ;; Conversion: FP -> integer.\n       (FpuToInt\n        (op FpuToIntOp)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; Conversion: integer -> FP.\n       (IntToFpu\n        (op IntToFpuOp)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; FP conditional select, 32 bit.\n       (FpuCSel32\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (cond Cond))\n\n       ;; FP conditional select, 64 bit.\n       (FpuCSel64\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (cond Cond))\n\n       ;; Round to integer.\n       (FpuRound\n        (op FpuRoundMode)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; Move from a GPR to a vector register.  The scalar value is parked in the lowest lane\n       ;; of the destination, and all other lanes are zeroed out.  Currently only 32- and 64-bit\n       ;; transactions are supported.\n       (MovToFpu\n        (rd WritableReg)\n        (rn Reg)\n        (size ScalarSize))\n\n       ;; Loads a floating-point immediate.\n       (FpuMoveFPImm\n        (rd WritableReg)\n        (imm ASIMDFPModImm)\n        (size ScalarSize))\n\n       ;; Move to a vector element from a GPR.\n       (MovToVec\n        (rd WritableReg)\n        (rn Reg)\n        (idx u8)\n        (size VectorSize))\n\n       ;; Unsigned move from a vector element to a GPR.\n       (MovFromVec\n        (rd WritableReg)\n        (rn Reg)\n        (idx u8)\n        (size VectorSize))\n\n       ;; Signed move from a vector element to a GPR.\n       (MovFromVecSigned\n        (rd WritableReg)\n        (rn Reg)\n        (idx u8)\n        (size VectorSize)\n        (scalar_size OperandSize))\n\n       ;; Duplicate general-purpose register to vector.\n       (VecDup\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize))\n\n       ;; Duplicate scalar to vector.\n       (VecDupFromFpu\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize))\n\n       ;; Duplicate FP immediate to vector.\n       (VecDupFPImm\n        (rd WritableReg)\n        (imm ASIMDFPModImm)\n        (size VectorSize))\n\n       ;; Duplicate immediate to vector.\n       (VecDupImm\n        (rd WritableReg)\n        (imm ASIMDMovModImm)\n        (invert bool)\n        (size VectorSize))\n\n       ;; Vector extend.\n       (VecExtend\n        (t VecExtendOp)\n        (rd WritableReg)\n        (rn Reg)\n        (high_half bool))\n\n       ;; Move vector element to another vector element.\n       (VecMovElement\n        (rd WritableReg)\n        (rn Reg)\n        (dest_idx u8)\n        (src_idx u8)\n        (size VectorSize))\n\n       ;; Vector widening operation.\n       (VecRRLong\n        (op VecRRLongOp)\n        (rd WritableReg)\n        (rn Reg)\n        (high_half bool))\n\n       ;; Vector narrowing operation.\n       (VecRRNarrow\n        (op VecRRNarrowOp)\n        (rd WritableReg)\n        (rn Reg)\n        (high_half bool))\n\n       ;; 1-operand vector instruction that operates on a pair of elements.\n       (VecRRPair\n        (op VecPairOp)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; 2-operand vector instruction that produces a result with twice the\n       ;; lane width and half the number of lanes.\n       (VecRRRLong\n        (alu_op VecRRRLongOp)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (high_half bool))\n\n       ;; 1-operand vector instruction that extends elements of the input\n       ;; register and operates on a pair of elements. The output lane width\n       ;; is double that of the input.\n       (VecRRPairLong\n        (op VecRRPairLongOp)\n        (rd WritableReg)\n        (rn Reg))\n\n       ;; A vector ALU op.\n       (VecRRR\n        (alu_op VecALUOp)\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (size VectorSize))\n\n       ;; Vector two register miscellaneous instruction.\n       (VecMisc\n        (op VecMisc2)\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize))\n\n       ;; Vector instruction across lanes.\n       (VecLanes\n        (op VecLanesOp)\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize))\n\n       ;; Vector shift by immediate Shift Left (immediate), Unsigned Shift Right (immediate)\n       ;; Signed Shift Right (immediate).  These are somewhat unusual in that, for right shifts,\n       ;; the allowed range of `imm` values is 1 to lane-size-in-bits, inclusive.  A zero\n       ;; right-shift cannot be encoded.  Left shifts are \"normal\", though, having valid `imm`\n       ;; values from 0 to lane-size-in-bits - 1 inclusive.\n       (VecShiftImm\n        (op VecShiftImmOp)\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize)\n        (imm u8))\n\n       ;; Vector extract - create a new vector, being the concatenation of the lowest `imm4` bytes\n       ;; of `rm` followed by the uppermost `16 - imm4` bytes of `rn`.\n       (VecExtract\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (imm4 u8))\n\n       ;; Table vector lookup - single register table. The table consists of 8-bit elements and is\n       ;; stored in `rn`, while `rm` contains 8-bit element indices. `is_extension` specifies whether\n       ;; to emit a TBX or a TBL instruction, i.e. whether to leave the elements in the destination\n       ;; vector that correspond to out-of-range indices (greater than 15) unmodified or to set them\n       ;; to 0.\n       (VecTbl\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (is_extension bool))\n\n       ;; Table vector lookup - two register table. The table consists of 8-bit elements and is\n       ;; stored in `rn` and `rn2`, while `rm` contains 8-bit element indices. `is_extension`\n       ;; specifies whether to emit a TBX or a TBL instruction, i.e. whether to leave the elements in\n       ;; the destination vector that correspond to out-of-range indices (greater than 31) unmodified\n       ;; or to set them to 0. The table registers `rn` and `rn2` must have consecutive numbers\n       ;; modulo 32, that is v31 and v0 (in that order) are consecutive registers.\n       (VecTbl2\n        (rd WritableReg)\n        (rn Reg)\n        (rn2 Reg)\n        (rm Reg)\n        (is_extension bool))\n\n       ;; Load an element and replicate to all lanes of a vector.\n       (VecLoadReplicate\n        (rd WritableReg)\n        (rn Reg)\n        (size VectorSize))\n\n       ;; Vector conditional select, 128 bit.  A synthetic instruction, which generates a 4-insn\n       ;; control-flow diamond.\n       (VecCSel\n        (rd WritableReg)\n        (rn Reg)\n        (rm Reg)\n        (cond Cond))\n\n       ;; Move to the NZCV flags (actually a `MSR NZCV, Xn` insn).\n       (MovToNZCV\n        (rn Reg))\n\n       ;; Move from the NZCV flags (actually a `MRS Xn, NZCV` insn).\n       (MovFromNZCV\n        (rd WritableReg))\n\n       ;; A machine call instruction. N.B.: this allows only a +/- 128MB offset (it uses a relocation\n       ;; of type `Reloc::Arm64Call`); if the destination distance is not `RelocDistance::Near`, the\n       ;; code should use a `LoadExtName` / `CallInd` sequence instead, allowing an arbitrary 64-bit\n       ;; target.\n       (Call\n        (info BoxCallInfo))\n\n       ;; A machine indirect-call instruction.\n       (CallInd\n        (info BoxCallIndInfo))\n\n       ;; ---- branches (exactly one must appear at end of BB) ----\n\n       ;; A machine return instruction.\n       (Ret\n        (rets VecReg))\n\n       ;; A placeholder instruction, generating no code, meaning that a function epilogue must be\n       ;; inserted there.\n       (EpiloguePlaceholder)\n\n       ;; An unconditional branch.\n       (Jump\n        (dest BranchTarget))\n\n       ;; A conditional branch. Contains two targets; at emission time, both are emitted, but\n       ;; the MachBuffer knows to truncate the trailing branch if fallthrough. We optimize the\n       ;; choice of taken/not_taken (inverting the branch polarity as needed) based on the\n       ;; fallthrough at the time of lowering.\n       (CondBr\n        (taken BranchTarget)\n        (not_taken BranchTarget)\n        (kind CondBrKind))\n\n       ;; A conditional trap: execute a `udf` if the condition is true. This is\n       ;; one VCode instruction because it uses embedded control flow; it is\n       ;; logically a single-in, single-out region, but needs to appear as one\n       ;; unit to the register allocator.\n       ;;\n       ;; The `CondBrKind` gives the conditional-branch condition that will\n       ;; *execute* the embedded `Inst`. (In the emitted code, we use the inverse\n       ;; of this condition in a branch that skips the trap instruction.)\n       (TrapIf\n        (kind CondBrKind)\n        (trap_code TrapCode))\n\n       ;; An indirect branch through a register, augmented with set of all\n       ;; possible successors.\n       (IndirectBr\n        (rn Reg)\n        (targets VecMachLabel))\n\n       ;; A \"break\" instruction, used for e.g. traps and debug breakpoints.\n       (Brk)\n\n       ;; An instruction guaranteed to always be undefined and to trigger an illegal instruction at\n       ;; runtime.\n       (Udf\n        (use_allocated_encoding bool)\n        (trap_code TrapCode))\n\n       ;; Compute the address (using a PC-relative offset) of a memory location, using the `ADR`\n       ;; instruction. Note that we take a simple offset, not a `MemLabel`, here, because `Adr` is\n       ;; only used for now in fixed lowering sequences with hardcoded offsets. In the future we may\n       ;; need full `MemLabel` support.\n       (Adr\n        (rd WritableReg)\n        ;; Offset in range -2^20 .. 2^20.\n        (off i32))\n\n       ;; Raw 32-bit word, used for inline constants and jump-table entries.\n       (Word4\n        (data u32))\n\n       ;; Raw 64-bit word, used for inline constants.\n       (Word8\n        (data u64))\n\n       ;; Jump-table sequence, as one compound instruction (see note in lower_inst.rs for rationale).\n       (JTSequence\n        (info BoxJTSequenceInfo)\n        (ridx Reg)\n        (rtmp1 WritableReg)\n        (rtmp2 WritableReg))\n\n       ;; Load an inline symbol reference.\n       (LoadExtName\n        (rd WritableReg)\n        (name BoxExternalName)\n        (offset i64))\n\n       ;; Load address referenced by `mem` into `rd`.\n       (LoadAddr\n        (rd WritableReg)\n        (mem AMode))\n\n       ;; Marker, no-op in generated code: SP \"virtual offset\" is adjusted. This\n       ;; controls how AMode::NominalSPOffset args are lowered.\n       (VirtualSPOffsetAdj\n        (offset i64))\n\n       ;; Meta-insn, no-op in generated code: emit constant/branch veneer island\n       ;; at this point (with a guard jump around it) if less than the needed\n       ;; space is available before the next branch deadline. See the `MachBuffer`\n       ;; implementation in `machinst/buffer.rs` for the overall algorithm. In\n       ;; brief, we retain a set of \"pending/unresolved label references\" from\n       ;; branches as we scan forward through instructions to emit machine code;\n       ;; if we notice we're about to go out of range on an unresolved reference,\n       ;; we stop, emit a bunch of \"veneers\" (branches in a form that has a longer\n       ;; range, e.g. a 26-bit-offset unconditional jump), and point the original\n       ;; label references to those. This is an \"island\" because it comes in the\n       ;; middle of the code.\n       ;;\n       ;; This meta-instruction is a necessary part of the logic that determines\n       ;; where to place islands. Ordinarily, we want to place them between basic\n       ;; blocks, so we compute the worst-case size of each block, and emit the\n       ;; island before starting a block if we would exceed a deadline before the\n       ;; end of the block. However, some sequences (such as an inline jumptable)\n       ;; are variable-length and not accounted for by this logic; so these\n       ;; lowered sequences include an `EmitIsland` to trigger island generation\n       ;; where necessary.\n       (EmitIsland\n        ;; The needed space before the next deadline.\n        (needed_space CodeOffset))\n\n       ;; A call to the `ElfTlsGetAddr` libcall. Returns address of TLS symbol in x0.\n       (ElfTlsGetAddr\n        (symbol ExternalName))\n\n       ;; An unwind pseudo-instruction.\n       (Unwind\n        (inst UnwindInst))\n\n       ;; A dummy use, useful to keep a value alive.\n       (DummyUse\n        (reg Reg))))\n\n;; An ALU operation. This can be paired with several instruction formats\n;; below (see `Inst`) in any combination.\n(type ALUOp\n  (enum\n    (Add)\n    (Sub)\n    (Orr)\n    (OrrNot)\n    (And)\n    (AndS)\n    (AndNot)\n    ;; XOR (AArch64 calls this \"EOR\")\n    (Eor)\n    ;; XNOR (AArch64 calls this \"EOR-NOT\")\n    (EorNot)\n    ;; Add, setting flags\n    (AddS)\n    ;; Sub, setting flags\n    (SubS)\n    ;; Signed multiply, high-word result\n    (SMulH)\n    ;; Unsigned multiply, high-word result\n    (UMulH)\n    (SDiv)\n    (UDiv)\n    (RotR)\n    (Lsr)\n    (Asr)\n    (Lsl)\n    ;; Add with carry\n    (Adc)\n    ;; Add with carry, settings flags\n    (AdcS)\n    ;; Subtract with carry\n    (Sbc)\n    ;; Subtract with carry, settings flags\n    (SbcS)\n))\n\n;; An ALU operation with three arguments.\n(type ALUOp3\n  (enum\n    ;; Multiply-add\n    (MAdd)\n    ;; Multiply-sub\n    (MSub)\n))\n\n(type MoveWideOp\n  (enum\n    (MovZ)\n    (MovN)\n    (MovK)\n))\n\n(type UImm5 (primitive UImm5))\n(type Imm12 (primitive Imm12))\n(type ImmLogic (primitive ImmLogic))\n(type ImmShift (primitive ImmShift))\n(type ShiftOpAndAmt (primitive ShiftOpAndAmt))\n(type MoveWideConst (primitive MoveWideConst))\n(type NZCV (primitive NZCV))\n(type ASIMDFPModImm (primitive ASIMDFPModImm))\n(type ASIMDMovModImm (primitive ASIMDMovModImm))\n\n(type BoxCallInfo (primitive BoxCallInfo))\n(type BoxCallIndInfo (primitive BoxCallIndInfo))\n(type CondBrKind (primitive CondBrKind))\n(type BranchTarget (primitive BranchTarget))\n(type BoxJTSequenceInfo (primitive BoxJTSequenceInfo))\n(type CodeOffset (primitive CodeOffset))\n\n(type ExtendOp extern\n  (enum\n    (UXTB)\n    (UXTH)\n    (UXTW)\n    (UXTX)\n    (SXTB)\n    (SXTH)\n    (SXTW)\n    (SXTX)\n))\n\n;; An operation on the bits of a register. This can be paired with several instruction formats\n;; below (see `Inst`) in any combination.\n(type BitOp\n  (enum\n    ;; Bit reverse\n    (RBit)\n    (Clz)\n    (Cls)\n))\n\n(type AMode extern (enum))\n(type PairAMode extern (enum))\n(type FPUOpRI extern (enum))\n\n(type OperandSize extern\n      (enum Size32\n            Size64))\n\n;; Helper for calculating the `OperandSize` corresponding to a type\n(decl operand_size (Type) OperandSize)\n(rule (operand_size (fits_in_32 _ty)) (OperandSize.Size32))\n(rule (operand_size (fits_in_64 _ty)) (OperandSize.Size64))\n\n(type ScalarSize extern\n      (enum Size8\n            Size16\n            Size32\n            Size64\n            Size128))\n\n;; Helper for calculating the `ScalarSize` corresponding to a type\n(decl scalar_size (Type) ScalarSize)\n(rule (scalar_size $I8) (ScalarSize.Size8))\n(rule (scalar_size $I16) (ScalarSize.Size16))\n(rule (scalar_size $I32) (ScalarSize.Size32))\n(rule (scalar_size $I64) (ScalarSize.Size64))\n(rule (scalar_size $I128) (ScalarSize.Size128))\n(rule (scalar_size $F32) (ScalarSize.Size32))\n(rule (scalar_size $F64) (ScalarSize.Size64))\n\n(type Cond extern\n  (enum\n    (Eq)\n    (Ne)\n    (Hs)\n    (Lo)\n    (Mi)\n    (Pl)\n    (Vs)\n    (Vc)\n    (Hi)\n    (Ls)\n    (Ge)\n    (Lt)\n    (Gt)\n    (Le)\n    (Al)\n    (Nv)\n))\n\n(type VectorSize extern\n  (enum\n    (Size8x8)\n    (Size8x16)\n    (Size16x4)\n    (Size16x8)\n    (Size32x2)\n    (Size32x4)\n    (Size64x2)\n))\n\n(type DynamicVectorSize extern\n  (enum\n    (Size8x8xN)\n    (Size8x16xN)\n    (Size16x4xN)\n    (Size16x8xN)\n    (Size32x2xN)\n    (Size32x4xN)\n    (Size64x2xN)\n))\n\n;; Helper for calculating the `VectorSize` corresponding to a type\n(decl vector_size (Type) VectorSize)\n(rule (vector_size (multi_lane 8 8)) (VectorSize.Size8x8))\n(rule (vector_size (multi_lane 8 16)) (VectorSize.Size8x16))\n(rule (vector_size (multi_lane 16 4)) (VectorSize.Size16x4))\n(rule (vector_size (multi_lane 16 8)) (VectorSize.Size16x8))\n(rule (vector_size (multi_lane 32 2)) (VectorSize.Size32x2))\n(rule (vector_size (multi_lane 32 4)) (VectorSize.Size32x4))\n(rule (vector_size (multi_lane 64 2)) (VectorSize.Size64x2))\n(rule (vector_size (dynamic_lane 8 8)) (VectorSize.Size8x8))\n(rule (vector_size (dynamic_lane 8 16)) (VectorSize.Size8x16))\n(rule (vector_size (dynamic_lane 16 4)) (VectorSize.Size16x4))\n(rule (vector_size (dynamic_lane 16 8)) (VectorSize.Size16x8))\n(rule (vector_size (dynamic_lane 32 2)) (VectorSize.Size32x2))\n(rule (vector_size (dynamic_lane 32 4)) (VectorSize.Size32x4))\n(rule (vector_size (dynamic_lane 64 2)) (VectorSize.Size64x2))\n\n;; A floating-point unit (FPU) operation with one arg.\n(type FPUOp1\n  (enum\n    (Abs)\n    (Neg)\n    (Sqrt)\n    (Cvt32To64)\n    (Cvt64To32)\n))\n\n;; A floating-point unit (FPU) operation with two args.\n(type FPUOp2\n  (enum\n    (Add)\n    (Sub)\n    (Mul)\n    (Div)\n    (Max)\n    (Min)\n))\n\n;; A floating-point unit (FPU) operation with three args.\n(type FPUOp3\n  (enum\n    (MAdd32)\n    (MAdd64)\n))\n\n;; A conversion from an FP to an integer value.\n(type FpuToIntOp\n  (enum\n    (F32ToU32)\n    (F32ToI32)\n    (F32ToU64)\n    (F32ToI64)\n    (F64ToU32)\n    (F64ToI32)\n    (F64ToU64)\n    (F64ToI64)\n))\n\n;; A conversion from an integer to an FP value.\n(type IntToFpuOp\n  (enum\n    (U32ToF32)\n    (I32ToF32)\n    (U32ToF64)\n    (I32ToF64)\n    (U64ToF32)\n    (I64ToF32)\n    (U64ToF64)\n    (I64ToF64)\n))\n\n;; Modes for FP rounding ops: round down (floor) or up (ceil), or toward zero (trunc), or to\n;; nearest, and for 32- or 64-bit FP values.\n(type FpuRoundMode\n  (enum\n    (Minus32)\n    (Minus64)\n    (Plus32)\n    (Plus64)\n    (Zero32)\n    (Zero64)\n    (Nearest32)\n    (Nearest64)\n))\n\n;; Type of vector element extensions.\n(type VecExtendOp\n  (enum\n    ;; Signed extension of 8-bit elements\n    (Sxtl8)\n    ;; Signed extension of 16-bit elements\n    (Sxtl16)\n    ;; Signed extension of 32-bit elements\n    (Sxtl32)\n    ;; Unsigned extension of 8-bit elements\n    (Uxtl8)\n    ;; Unsigned extension of 16-bit elements\n    (Uxtl16)\n    ;; Unsigned extension of 32-bit elements\n    (Uxtl32)\n))\n\n;; A vector ALU operation.\n(type VecALUOp\n  (enum\n    ;; Signed saturating add\n    (Sqadd)\n    ;; Unsigned saturating add\n    (Uqadd)\n    ;; Signed saturating subtract\n    (Sqsub)\n    ;; Unsigned saturating subtract\n    (Uqsub)\n    ;; Compare bitwise equal\n    (Cmeq)\n    ;; Compare signed greater than or equal\n    (Cmge)\n    ;; Compare signed greater than\n    (Cmgt)\n    ;; Compare unsigned higher\n    (Cmhs)\n    ;; Compare unsigned higher or same\n    (Cmhi)\n    ;; Floating-point compare equal\n    (Fcmeq)\n    ;; Floating-point compare greater than\n    (Fcmgt)\n    ;; Floating-point compare greater than or equal\n    (Fcmge)\n    ;; Bitwise and\n    (And)\n    ;; Bitwise bit clear\n    (Bic)\n    ;; Bitwise inclusive or\n    (Orr)\n    ;; Bitwise exclusive or\n    (Eor)\n    ;; Bitwise select\n    (Bsl)\n    ;; Unsigned maximum pairwise\n    (Umaxp)\n    ;; Add\n    (Add)\n    ;; Subtract\n    (Sub)\n    ;; Multiply\n    (Mul)\n    ;; Signed shift left\n    (Sshl)\n    ;; Unsigned shift left\n    (Ushl)\n    ;; Unsigned minimum\n    (Umin)\n    ;; Signed minimum\n    (Smin)\n    ;; Unsigned maximum\n    (Umax)\n    ;; Signed maximum\n    (Smax)\n    ;; Unsigned rounding halving add\n    (Urhadd)\n    ;; Floating-point add\n    (Fadd)\n    ;; Floating-point subtract\n    (Fsub)\n    ;; Floating-point divide\n    (Fdiv)\n    ;; Floating-point maximum\n    (Fmax)\n    ;; Floating-point minimum\n    (Fmin)\n    ;; Floating-point multiply\n    (Fmul)\n    ;; Add pairwise\n    (Addp)\n    ;; Zip vectors (primary) [meaning, high halves]\n    (Zip1)\n    ;; Signed saturating rounding doubling multiply returning high half\n    (Sqrdmulh)\n))\n\n;; A Vector miscellaneous operation with two registers.\n(type VecMisc2\n  (enum\n    ;; Bitwise NOT\n    (Not)\n    ;; Negate\n    (Neg)\n    ;; Absolute value\n    (Abs)\n    ;; Floating-point absolute value\n    (Fabs)\n    ;; Floating-point negate\n    (Fneg)\n    ;; Floating-point square root\n    (Fsqrt)\n    ;; Reverse elements in 64-bit doublewords\n    (Rev64)\n    ;; Floating-point convert to signed integer, rounding toward zero\n    (Fcvtzs)\n    ;; Floating-point convert to unsigned integer, rounding toward zero\n    (Fcvtzu)\n    ;; Signed integer convert to floating-point\n    (Scvtf)\n    ;; Unsigned integer convert to floating-point\n    (Ucvtf)\n    ;; Floating point round to integral, rounding towards nearest\n    (Frintn)\n    ;; Floating point round to integral, rounding towards zero\n    (Frintz)\n    ;; Floating point round to integral, rounding towards minus infinity\n    (Frintm)\n    ;; Floating point round to integral, rounding towards plus infinity\n    (Frintp)\n    ;; Population count per byte\n    (Cnt)\n    ;; Compare bitwise equal to 0\n    (Cmeq0)\n    ;; Compare signed greater than or equal to 0\n    (Cmge0)\n    ;; Compare signed greater than 0\n    (Cmgt0)\n    ;; Compare signed less than or equal to 0\n    (Cmle0)\n    ;; Compare signed less than 0\n    (Cmlt0)\n    ;; Floating point compare equal to 0\n    (Fcmeq0)\n    ;; Floating point compare greater than or equal to 0\n    (Fcmge0)\n    ;; Floating point compare greater than 0\n    (Fcmgt0)\n    ;; Floating point compare less than or equal to 0\n    (Fcmle0)\n    ;; Floating point compare less than 0\n    (Fcmlt0)\n))\n\n;; A vector widening operation with one argument.\n(type VecRRLongOp\n  (enum\n    ;; Floating-point convert to higher precision long, 16-bit elements\n    (Fcvtl16)\n    ;; Floating-point convert to higher precision long, 32-bit elements\n    (Fcvtl32)\n    ;; Shift left long (by element size), 8-bit elements\n    (Shll8)\n    ;; Shift left long (by element size), 16-bit elements\n    (Shll16)\n    ;; Shift left long (by element size), 32-bit elements\n    (Shll32)\n))\n\n;; A vector narrowing operation with one argument.\n(type VecRRNarrowOp\n  (enum\n    ;; Extract narrow, 16-bit elements\n    (Xtn16)\n    ;; Extract narrow, 32-bit elements\n    (Xtn32)\n    ;; Extract narrow, 64-bit elements\n    (Xtn64)\n    ;; Signed saturating extract narrow, 16-bit elements\n    (Sqxtn16)\n    ;; Signed saturating extract narrow, 32-bit elements\n    (Sqxtn32)\n    ;; Signed saturating extract narrow, 64-bit elements\n    (Sqxtn64)\n    ;; Signed saturating extract unsigned narrow, 16-bit elements\n    (Sqxtun16)\n    ;; Signed saturating extract unsigned narrow, 32-bit elements\n    (Sqxtun32)\n    ;; Signed saturating extract unsigned narrow, 64-bit elements\n    (Sqxtun64)\n    ;; Unsigned saturating extract narrow, 16-bit elements\n    (Uqxtn16)\n    ;; Unsigned saturating extract narrow, 32-bit elements\n    (Uqxtn32)\n    ;; Unsigned saturating extract narrow, 64-bit elements\n    (Uqxtn64)\n    ;; Floating-point convert to lower precision narrow, 32-bit elements\n    (Fcvtn32)\n    ;; Floating-point convert to lower precision narrow, 64-bit elements\n    (Fcvtn64)\n))\n\n(type VecRRRLongOp\n  (enum\n    ;; Signed multiply long.\n    (Smull8)\n    (Smull16)\n    (Smull32)\n    ;; Unsigned multiply long.\n    (Umull8)\n    (Umull16)\n    (Umull32)\n    ;; Unsigned multiply add long\n    (Umlal8)\n    (Umlal16)\n    (Umlal32)\n))\n\n;; A vector operation on a pair of elements with one register.\n(type VecPairOp\n  (enum\n    ;; Add pair of elements\n    (Addp)\n))\n\n;; 1-operand vector instruction that extends elements of the input register\n;; and operates on a pair of elements.\n(type VecRRPairLongOp\n  (enum\n    ;; Sign extend and add pair of elements\n    (Saddlp8)\n    (Saddlp16)\n    ;; Unsigned extend and add pair of elements\n    (Uaddlp8)\n    (Uaddlp16)\n))\n\n;; An operation across the lanes of vectors.\n(type VecLanesOp\n  (enum\n    ;; Integer addition across a vector\n    (Addv)\n    ;; Unsigned minimum across a vector\n    (Uminv)\n))\n\n;; A shift-by-immediate operation on each lane of a vector.\n(type VecShiftImmOp\n  (enum\n    ;; Unsigned shift left\n    (Shl)\n    ;; Unsigned shift right\n    (Ushr)\n    ;; Signed shift right\n    (Sshr)\n))\n\n;; Atomic read-modify-write operations with acquire-release semantics\n(type AtomicRMWOp\n  (enum\n    (Add)\n    (Clr)\n    (Eor)\n    (Set)\n    (Smax)\n    (Smin)\n    (Umax)\n    (Umin)\n    (Swp)\n))\n\n;; Atomic read-modify-write operations, with acquire-release semantics,\n;; implemented with a loop.\n(type AtomicRMWLoopOp\n  (enum\n    (Add)\n    (Sub)\n    (And)\n    (Nand)\n    (Eor)\n    (Orr)\n    (Smax)\n    (Smin)\n    (Umax)\n    (Umin)\n    (Xchg)\n))\n\n;; Extractors for target features ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n(decl use_lse () Inst)\n(extern extractor use_lse use_lse)\n\n;; Extractor helpers for various immmediate constants ;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(decl pure imm_logic_from_u64 (Type u64) ImmLogic)\n(extern constructor imm_logic_from_u64 imm_logic_from_u64)\n\n(decl pure imm_logic_from_imm64 (Type Imm64) ImmLogic)\n(extern constructor imm_logic_from_imm64 imm_logic_from_imm64)\n\n(decl pure imm_shift_from_imm64 (Type Imm64) ImmShift)\n(extern constructor imm_shift_from_imm64 imm_shift_from_imm64)\n\n(decl imm_shift_from_u8 (u8) ImmShift)\n(extern constructor imm_shift_from_u8 imm_shift_from_u8)\n\n(decl imm12_from_u64 (Imm12) u64)\n(extern extractor imm12_from_u64 imm12_from_u64)\n\n(decl u8_into_uimm5 (u8) UImm5)\n(extern constructor u8_into_uimm5 u8_into_uimm5)\n\n(decl u8_into_imm12 (u8) Imm12)\n(extern constructor u8_into_imm12 u8_into_imm12)\n\n(decl u64_into_imm_logic (Type u64) ImmLogic)\n(extern constructor u64_into_imm_logic u64_into_imm_logic)\n\n(decl imm12_from_negated_u64 (Imm12) u64)\n(extern extractor imm12_from_negated_u64 imm12_from_negated_u64)\n\n(decl pure lshl_from_imm64 (Type Imm64) ShiftOpAndAmt)\n(extern constructor lshl_from_imm64 lshl_from_imm64)\n\n(decl integral_ty (Type) Type)\n(extern extractor integral_ty integral_ty)\n\n(decl valid_atomic_transaction (Type) Type)\n(extern extractor valid_atomic_transaction valid_atomic_transaction)\n\n;; Helper to go directly from a `Value`, when it's an `iconst`, to an `Imm12`.\n(decl imm12_from_value (Imm12) Value)\n(extractor\n  (imm12_from_value n)\n  (iconst (u64_from_imm64 (imm12_from_u64 n))))\n\n;; Same as `imm12_from_value`, but tries negating the constant value.\n(decl imm12_from_negated_value (Imm12) Value)\n(extractor\n  (imm12_from_negated_value n)\n  (iconst (u64_from_imm64 (imm12_from_negated_u64 n))))\n\n;; Helper type to represent a value and an extend operation fused together.\n(type ExtendedValue extern (enum))\n(decl extended_value_from_value (ExtendedValue) Value)\n(extern extractor extended_value_from_value extended_value_from_value)\n\n;; Constructors used to poke at the fields of an `ExtendedValue`.\n(decl put_extended_in_reg (ExtendedValue) Reg)\n(extern constructor put_extended_in_reg put_extended_in_reg)\n(decl get_extended_op (ExtendedValue) ExtendOp)\n(extern constructor get_extended_op get_extended_op)\n\n(decl nzcv (bool bool bool bool) NZCV)\n(extern constructor nzcv nzcv)\n\n(decl cond_br_zero (Reg) CondBrKind)\n(extern constructor cond_br_zero cond_br_zero)\n\n(decl cond_br_cond (Cond) CondBrKind)\n(extern constructor cond_br_cond cond_br_cond)\n\n;; Instruction creation helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Helper for creating the zero register.\n(decl zero_reg () Reg)\n(extern constructor zero_reg zero_reg)\n\n(decl writable_zero_reg () WritableReg)\n(extern constructor writable_zero_reg writable_zero_reg)\n\n;; Helpers for getting a particular real register\n(decl xreg (u8) Reg)\n(extern constructor xreg xreg)\n\n(decl writable_xreg (u8) WritableReg)\n(extern constructor writable_xreg writable_xreg)\n\n;; Helper for emitting `MInst.Mov64` instructions.\n(decl mov64_to_real (u8 Reg) Reg)\n(rule (mov64_to_real num src)\n      (let ((dst WritableReg (writable_xreg num))\n            (_ Unit (emit (MInst.Mov (operand_size $I64) dst src))))\n        dst))\n\n(decl mov64_from_real (u8) Reg)\n(rule (mov64_from_real num)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.Mov (operand_size $I64) dst (xreg num)))))\n        dst))\n\n;; Helper for emitting `MInst.MovZ` instructions.\n(decl movz (MoveWideConst OperandSize) Reg)\n(rule (movz imm size)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.MovWide (MoveWideOp.MovZ) dst imm size))))\n        dst))\n\n;; Helper for emitting `MInst.MovN` instructions.\n(decl movn (MoveWideConst OperandSize) Reg)\n(rule (movn imm size)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.MovWide (MoveWideOp.MovN) dst imm size))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRImmLogic` instructions.\n(decl alu_rr_imm_logic (ALUOp Type Reg ImmLogic) Reg)\n(rule (alu_rr_imm_logic op ty src imm)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRImmLogic op (operand_size ty) dst src imm))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRImmShift` instructions.\n(decl alu_rr_imm_shift (ALUOp Type Reg ImmShift) Reg)\n(rule (alu_rr_imm_shift op ty src imm)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRImmShift op (operand_size ty) dst src imm))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRR` instructions.\n(decl alu_rrr (ALUOp Type Reg Reg) Reg)\n(rule (alu_rrr op ty src1 src2)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRR op (operand_size ty) dst src1 src2))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRR` instructions.\n(decl vec_rrr (VecALUOp Reg Reg VectorSize) Reg)\n(rule (vec_rrr op src1 src2 size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecRRR op dst src1 src2 size))))\n        dst))\n\n;; Helper for emitting `MInst.FpuRRR` instructions.\n(decl fpu_rrr (FPUOp2 Reg Reg ScalarSize) Reg)\n(rule (fpu_rrr op src1 src2 size)\n      (let ((dst WritableReg (temp_writable_reg $F64))\n            (_ Unit (emit (MInst.FpuRRR op size dst src1 src2))))\n        dst))\n\n;; Helper for emitting `MInst.FpuCmp` instructions.\n(decl fpu_cmp (ScalarSize Reg Reg) ProducesFlags)\n(rule (fpu_cmp size rn rm)\n      (ProducesFlags.ProducesFlagsSideEffect\n       (MInst.FpuCmp size rn rm)))\n\n;; Helper for emitting `MInst.VecLanes` instructions.\n(decl vec_lanes (VecLanesOp Reg VectorSize) Reg)\n(rule (vec_lanes op src size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecLanes op dst src size))))\n        dst))\n\n;; Helper for emitting `MInst.VecDup` instructions.\n(decl vec_dup (Reg VectorSize) Reg)\n(rule (vec_dup src size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecDup dst src size))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRImm12` instructions.\n(decl alu_rr_imm12 (ALUOp Type Reg Imm12) Reg)\n(rule (alu_rr_imm12 op ty src imm)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRImm12 op (operand_size ty) dst src imm))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRRShift` instructions.\n(decl alu_rrr_shift (ALUOp Type Reg Reg ShiftOpAndAmt) Reg)\n(rule (alu_rrr_shift op ty src1 src2 shift)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRRShift op (operand_size ty) dst src1 src2 shift))))\n        dst))\n\n;; Helper for emitting `MInst.AluRRRExtend` instructions.\n(decl alu_rrr_extend (ALUOp Type Reg Reg ExtendOp) Reg)\n(rule (alu_rrr_extend op ty src1 src2 extend)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRRExtend op (operand_size ty) dst src1 src2 extend))))\n        dst))\n\n;; Same as `alu_rrr_extend`, but takes an `ExtendedValue` packed \"pair\" instead\n;; of a `Reg` and an `ExtendOp`.\n(decl alu_rr_extend_reg (ALUOp Type Reg ExtendedValue) Reg)\n(rule (alu_rr_extend_reg op ty src1 extended_reg)\n      (let ((src2 Reg (put_extended_in_reg extended_reg))\n            (extend ExtendOp (get_extended_op extended_reg)))\n        (alu_rrr_extend op ty src1 src2 extend)))\n\n;; Helper for emitting `MInst.AluRRRR` instructions.\n(decl alu_rrrr (ALUOp3 Type Reg Reg Reg) Reg)\n(rule (alu_rrrr op ty src1 src2 src3)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.AluRRRR op (operand_size ty) dst src1 src2 src3))))\n        dst))\n\n;; Helper for emitting `MInst.BitRR` instructions.\n(decl bit_rr (BitOp Type Reg) Reg)\n(rule (bit_rr op ty src)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.BitRR op (operand_size ty) dst src))))\n        dst))\n\n;; Helper for emitting `adds` instructions.\n(decl add_with_flags_paired (Type Reg Reg) ProducesFlags)\n(rule (add_with_flags_paired ty src1 src2)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ProducesFlags.ProducesFlagsReturnsResultWithConsumer\n         (MInst.AluRRR (ALUOp.AddS) (operand_size ty) dst src1 src2)\n         dst)))\n\n;; Helper for emitting `adc` instructions.\n(decl adc_paired (Type Reg Reg) ConsumesFlags)\n(rule (adc_paired ty src1 src2)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ConsumesFlags.ConsumesFlagsReturnsResultWithProducer\n         (MInst.AluRRR (ALUOp.Adc) (operand_size ty) dst src1 src2)\n         dst)))\n\n;; Helper for emitting `subs` instructions.\n(decl sub_with_flags_paired (Type Reg Reg) ProducesFlags)\n(rule (sub_with_flags_paired ty src1 src2)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ProducesFlags.ProducesFlagsReturnsResultWithConsumer\n         (MInst.AluRRR (ALUOp.SubS) (operand_size ty) dst src1 src2)\n         dst)))\n\n(decl cmp_imm (OperandSize Reg Imm12) ProducesFlags)\n(rule (cmp_imm size src1 src2)\n      (ProducesFlags.ProducesFlagsSideEffect\n       (MInst.AluRRImm12 (ALUOp.SubS) size (writable_zero_reg)\n        src1 src2)))\n\n(decl cmp64_imm (Reg Imm12) ProducesFlags)\n(rule (cmp64_imm src1 src2)\n      (cmp_imm (OperandSize.Size64) src1 src2))\n\n;; Helper for emitting `sbc` instructions.\n(decl sbc_paired (Type Reg Reg) ConsumesFlags)\n(rule (sbc_paired ty src1 src2)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ConsumesFlags.ConsumesFlagsReturnsResultWithProducer\n         (MInst.AluRRR (ALUOp.Sbc) (operand_size ty) dst src1 src2)\n         dst)))\n\n;; Helper for emitting `MInst.VecMisc` instructions.\n(decl vec_misc (VecMisc2 Reg VectorSize) Reg)\n(rule (vec_misc op src size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecMisc op dst src size))))\n        dst))\n\n;; Helper for emitting `MInst.VecTbl` instructions.\n(decl vec_tbl (Reg Reg bool) Reg)\n(rule (vec_tbl rn rm is_extension)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecTbl dst rn rm is_extension))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRRLong` instructions.\n(decl vec_rrr_long (VecRRRLongOp Reg Reg bool) Reg)\n(rule (vec_rrr_long op src1 src2 high_half)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecRRRLong op dst src1 src2 high_half))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRPairLong` instructions.\n(decl vec_rr_pair_long (VecRRPairLongOp Reg) Reg)\n(rule (vec_rr_pair_long op src)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecRRPairLong op dst src))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRRLong` instructions, but for variants\n;; where the operation both reads and modifies the destination register.\n;;\n;; Currently this is only used for `VecRRRLongOp.Umlal*`\n(decl vec_rrrr_long (VecRRRLongOp Reg Reg Reg bool) Reg)\n(rule (vec_rrrr_long op src1 src2 src3 high_half)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_1 Unit (emit (MInst.FpuMove128 dst src1)))\n            (_2 Unit (emit (MInst.VecRRRLong op dst src2 src3 high_half))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRNarrow` instructions.\n(decl vec_rr_narrow (VecRRNarrowOp Reg bool) Reg)\n(rule (vec_rr_narrow op src high_half)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecRRNarrow op dst src high_half))))\n        dst))\n\n;; Helper for emitting `MInst.VecRRLong` instructions.\n(decl vec_rr_long (VecRRLongOp Reg bool) Reg)\n(rule (vec_rr_long op src high_half)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.VecRRLong op dst src high_half))))\n        dst))\n\n;; Helper for emitting `MInst.FpuCSel32` / `MInst.FpuCSel64`\n;; instructions.\n(decl fpu_csel (Type Cond Reg Reg) ConsumesFlags)\n(rule (fpu_csel $F32 cond if_true if_false)\n      (let ((dst WritableReg (temp_writable_reg $F32)))\n        (ConsumesFlags.ConsumesFlagsReturnsReg\n         (MInst.FpuCSel32 dst if_true if_false cond)\n         dst)))\n\n(rule (fpu_csel $F64 cond if_true if_false)\n      (let ((dst WritableReg (temp_writable_reg $F64)))\n        (ConsumesFlags.ConsumesFlagsReturnsReg\n         (MInst.FpuCSel64 dst if_true if_false cond)\n         dst)))\n\n\n;; Helper for emitting `MInst.MovToFpu` instructions.\n(decl mov_to_fpu (Reg ScalarSize) Reg)\n(rule (mov_to_fpu x size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_ Unit (emit (MInst.MovToFpu dst x size))))\n        dst))\n\n;; Helper for emitting `MInst.MovToVec` instructions.\n(decl mov_to_vec (Reg Reg u8 VectorSize) Reg)\n(rule (mov_to_vec src1 src2 lane size)\n      (let ((dst WritableReg (temp_writable_reg $I8X16))\n            (_1 Unit (emit (MInst.FpuMove128 dst src1)))\n            (_2 Unit (emit (MInst.MovToVec dst src2 lane size))))\n        dst))\n\n;; Helper for emitting `MInst.MovFromVec` instructions.\n(decl mov_from_vec (Reg u8 VectorSize) Reg)\n(rule (mov_from_vec rn idx size)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.MovFromVec dst rn idx size))))\n        dst))\n\n;; Helper for emitting `MInst.MovFromVecSigned` instructions.\n(decl mov_from_vec_signed (Reg u8 VectorSize OperandSize) Reg)\n(rule (mov_from_vec_signed rn idx size scalar_size)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.MovFromVecSigned dst rn idx size scalar_size))))\n        dst))\n\n;; Helper for emitting `MInst.Extend` instructions.\n(decl extend (Reg bool u8 u8) Reg)\n(rule (extend rn signed from_bits to_bits)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.Extend dst rn signed from_bits to_bits))))\n        dst))\n\n;; Helper for emitting `MInst.FpuExtend` instructions.\n(decl fpu_extend (Reg ScalarSize) Reg)\n(rule (fpu_extend src size)\n      (let ((dst WritableReg (temp_writable_reg $F32X4))\n            (_ Unit (emit (MInst.FpuExtend dst src size))))\n        dst))\n\n;; Helper for emitting `MInst.LoadAcquire` instructions.\n(decl load_acquire (Type Reg) Reg)\n(rule (load_acquire ty addr)\n      (let ((dst WritableReg (temp_writable_reg $I64))\n            (_ Unit (emit (MInst.LoadAcquire ty dst addr))))\n        dst))\n\n;; Helper for emitting `MInst.StoreRelease` instructions.\n(decl store_release (Type Reg Reg) SideEffectNoResult)\n(rule (store_release ty src addr)\n      (SideEffectNoResult.Inst (MInst.StoreRelease ty src addr)))\n\n;; Helper for generating a `tst` instruction.\n;;\n;; Produces a `ProducesFlags` rather than a register or emitted instruction\n;; which must be paired with `with_flags*` helpers.\n(decl tst_imm (Type Reg ImmLogic) ProducesFlags)\n(rule (tst_imm ty reg imm)\n      (ProducesFlags.ProducesFlagsSideEffect\n       (MInst.AluRRImmLogic (ALUOp.AndS)\n                            (operand_size ty)\n                            (writable_zero_reg)\n                            reg\n                            imm)))\n\n;; Helper for generating a `CSel` instruction.\n;;\n;; Note that this doesn't actually emit anything, instead it produces a\n;; `ConsumesFlags` instruction which must be consumed with `with_flags*`\n;; helpers.\n(decl csel (Cond Reg Reg) ConsumesFlags)\n(rule (csel cond if_true if_false)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ConsumesFlags.ConsumesFlagsReturnsReg\n         (MInst.CSel dst cond if_true if_false)\n         dst)))\n\n;; Helper for generating a `CSNeg` instruction.\n;;\n;; Note that this doesn't actually emit anything, instead it produces a\n;; `ConsumesFlags` instruction which must be consumed with `with_flags*`\n;; helpers.\n(decl csneg (Cond Reg Reg) ConsumesFlags)\n(rule (csneg cond if_true if_false)\n      (let ((dst WritableReg (temp_writable_reg $I64)))\n        (ConsumesFlags.ConsumesFlagsReturnsReg\n         (MInst.CSNeg dst cond if_true if_false)\n         dst)))\n\n;; Helpers for generating `add` instructions.\n\n(decl add (Type Reg Reg) Reg)\n(rule (add ty x y) (alu_rrr (ALUOp.Add) ty x y))\n\n(decl add_imm (Type Reg Imm12) Reg)\n(rule (add_imm ty x y) (alu_rr_imm12 (ALUOp.Add) ty x y))\n\n(decl add_extend (Type Reg ExtendedValue) Reg)\n(rule (add_extend ty x y) (alu_rr_extend_reg (ALUOp.Add) ty x y))\n\n(decl add_shift (Type Reg Reg ShiftOpAndAmt) Reg)\n(rule (add_shift ty x y z) (alu_rrr_shift (ALUOp.Add) ty x y z))\n\n(decl add_vec (Reg Reg VectorSize) Reg)\n(rule (add_vec x y size) (vec_rrr (VecALUOp.Add) x y size))\n\n;; Helpers for generating `sub` instructions.\n\n(decl sub (Type Reg Reg) Reg)\n(rule (sub ty x y) (alu_rrr (ALUOp.Sub) ty x y))\n\n(decl sub_imm (Type Reg Imm12) Reg)\n(rule (sub_imm ty x y) (alu_rr_imm12 (ALUOp.Sub) ty x y))\n\n(decl sub_extend (Type Reg ExtendedValue) Reg)\n(rule (sub_extend ty x y) (alu_rr_extend_reg (ALUOp.Sub) ty x y))\n\n(decl sub_shift (Type Reg Reg ShiftOpAndAmt) Reg)\n(rule (sub_shift ty x y z) (alu_rrr_shift (ALUOp.Sub) ty x y z))\n\n(decl sub_vec (Reg Reg VectorSize) Reg)\n(rule (sub_vec x y size) (vec_rrr (VecALUOp.Sub) x y size))\n\n;; Helpers for generating `madd` instructions.\n\n(decl madd (Type Reg Reg Reg) Reg)\n(rule (madd ty x y z) (alu_rrrr (ALUOp3.MAdd) ty x y z))\n\n;; Helpers for generating `msub` instructions.\n\n(decl msub (Type Reg Reg Reg) Reg)\n(rule (msub ty x y z) (alu_rrrr (ALUOp3.MSub) ty x y z))\n\n;; Helper for generating `uqadd` instructions.\n(decl uqadd (Reg Reg VectorSize) Reg)\n(rule (uqadd x y size) (vec_rrr (VecALUOp.Uqadd) x y size))\n\n;; Helper for generating `sqadd` instructions.\n(decl sqadd (Reg Reg VectorSize) Reg)\n(rule (sqadd x y size) (vec_rrr (VecALUOp.Sqadd) x y size))\n\n;; Helper for generating `uqsub` instructions.\n(decl uqsub (Reg Reg VectorSize) Reg)\n(rule (uqsub x y size) (vec_rrr (VecALUOp.Uqsub) x y size))\n\n;; Helper for generating `sqsub` instructions.\n(decl sqsub (Reg Reg VectorSize) Reg)\n(rule (sqsub x y size) (vec_rrr (VecALUOp.Sqsub) x y size))\n\n;; Helper for generating `umulh` instructions.\n(decl umulh (Type Reg Reg) Reg)\n(rule (umulh ty x y) (alu_rrr (ALUOp.UMulH) ty x y))\n\n;; Helper for generating `smulh` instructions.\n(decl smulh (Type Reg Reg) Reg)\n(rule (smulh ty x y) (alu_rrr (ALUOp.SMulH) ty x y))\n\n;; Helper for generating `mul` instructions.\n(decl mul (Reg Reg VectorSize) Reg)\n(rule (mul x y size) (vec_rrr (VecALUOp.Mul) x y size))\n\n;; Helper for generating `neg` instructions.\n(decl neg (Reg VectorSize) Reg)\n(rule (neg x size) (vec_misc (VecMisc2.Neg) x size))\n\n;; Helper for generating `rev64` instructions.\n(decl rev64 (Reg VectorSize) Reg)\n(rule (rev64 x size) (vec_misc (VecMisc2.Rev64) x size))\n\n;; Helper for generating `xtn64` instructions.\n(decl xtn64 (Reg bool) Reg)\n(rule (xtn64 x high_half) (vec_rr_narrow (VecRRNarrowOp.Xtn64) x high_half))\n\n;; Helper for generating `addp` instructions.\n(decl addp (Reg Reg VectorSize) Reg)\n(rule (addp x y size) (vec_rrr (VecALUOp.Addp) x y size))\n\n;; Helper for generating vector `abs` instructions.\n(decl vec_abs (Reg VectorSize) Reg)\n(rule (vec_abs x size) (vec_misc (VecMisc2.Abs) x size))\n\n;; Helper for generating instruction sequences to calculate a scalar absolute\n;; value.\n(decl abs (OperandSize Reg) Reg)\n(rule (abs size x)\n      (value_regs_get (with_flags (cmp_imm size x (u8_into_imm12 0))\n                                  (csneg (Cond.Gt) x x)) 0))\n\n;; Helper for generating `addv` instructions.\n(decl addv (Reg VectorSize) Reg)\n(rule (addv x size) (vec_lanes (VecLanesOp.Addv) x size))\n\n;; Helper for generating `shll32` instructions.\n(decl shll32 (Reg bool) Reg)\n(rule (shll32 x high_half) (vec_rr_long (VecRRLongOp.Shll32) x high_half))\n\n;; Helpers for generating `addlp` instructions.\n\n(decl saddlp8 (Reg) Reg)\n(rule (saddlp8 x) (vec_rr_pair_long (VecRRPairLongOp.Saddlp8) x))\n\n(decl saddlp16 (Reg) Reg)\n(rule (saddlp16 x) (vec_rr_pair_long (VecRRPairLongOp.Saddlp16) x))\n\n(decl uaddlp8 (Reg) Reg)\n(rule (uaddlp8 x) (vec_rr_pair_long (VecRRPairLongOp.Uaddlp8) x))\n\n(decl uaddlp16 (Reg) Reg)\n(rule (uaddlp16 x) (vec_rr_pair_long (VecRRPairLongOp.Uaddlp16) x))\n\n;; Helper for generating `umlal32` instructions.\n(decl umlal32 (Reg Reg Reg bool) Reg)\n(rule (umlal32 x y z high_half) (vec_rrrr_long (VecRRRLongOp.Umlal32) x y z high_half))\n\n;; Helper for generating `smull8` instructions.\n(decl smull8 (Reg Reg bool) Reg)\n(rule (smull8 x y high_half) (vec_rrr_long (VecRRRLongOp.Smull8) x y high_half))\n\n;; Helper for generating `umull8` instructions.\n(decl umull8 (Reg Reg bool) Reg)\n(rule (umull8 x y high_half) (vec_rrr_long (VecRRRLongOp.Umull8) x y high_half))\n\n;; Helper for generating `smull16` instructions.\n(decl smull16 (Reg Reg bool) Reg)\n(rule (smull16 x y high_half) (vec_rrr_long (VecRRRLongOp.Smull16) x y high_half))\n\n;; Helper for generating `umull16` instructions.\n(decl umull16 (Reg Reg bool) Reg)\n(rule (umull16 x y high_half) (vec_rrr_long (VecRRRLongOp.Umull16) x y high_half))\n\n;; Helper for generating `smull32` instructions.\n(decl smull32 (Reg Reg bool) Reg)\n(rule (smull32 x y high_half) (vec_rrr_long (VecRRRLongOp.Smull32) x y high_half))\n\n;; Helper for generating `umull32` instructions.\n(decl umull32 (Reg Reg bool) Reg)\n(rule (umull32 x y high_half) (vec_rrr_long (VecRRRLongOp.Umull32) x y high_half))\n\n;; Helper for generating `asr` instructions.\n(decl asr (Type Reg Reg) Reg)\n(rule (asr ty x y) (alu_rrr (ALUOp.Asr) ty x y))\n\n(decl asr_imm (Type Reg ImmShift) Reg)\n(rule (asr_imm ty x imm) (alu_rr_imm_shift (ALUOp.Asr) ty x imm))\n\n;; Helper for generating `lsr` instructions.\n(decl lsr (Type Reg Reg) Reg)\n(rule (lsr ty x y) (alu_rrr (ALUOp.Lsr) ty x y))\n\n(decl lsr_imm (Type Reg ImmShift) Reg)\n(rule (lsr_imm ty x imm) (alu_rr_imm_shift (ALUOp.Lsr) ty x imm))\n\n;; Helper for generating `lsl` instructions.\n(decl lsl (Type Reg Reg) Reg)\n(rule (lsl ty x y) (alu_rrr (ALUOp.Lsl) ty x y))\n\n(decl lsl_imm (Type Reg ImmShift) Reg)\n(rule (lsl_imm ty x imm) (alu_rr_imm_shift (ALUOp.Lsl) ty x imm))\n\n;; Helper for generating `udiv` instructions.\n(decl a64_udiv (Type Reg Reg) Reg)\n(rule (a64_udiv ty x y) (alu_rrr (ALUOp.UDiv) ty x y))\n\n;; Helper for generating `sdiv` instructions.\n(decl a64_sdiv (Type Reg Reg) Reg)\n(rule (a64_sdiv ty x y) (alu_rrr (ALUOp.SDiv) ty x y))\n\n;; Helper for generating `not` instructions.\n(decl not (Reg VectorSize) Reg)\n(rule (not x size) (vec_misc (VecMisc2.Not) x size))\n\n;; Helpers for generating `orr_not` instructions.\n\n(decl orr_not (Type Reg Reg) Reg)\n(rule (orr_not ty x y) (alu_rrr (ALUOp.OrrNot) ty x y))\n\n(decl orr_not_shift (Type Reg Reg ShiftOpAndAmt) Reg)\n(rule (orr_not_shift ty x y shift) (alu_rrr_shift (ALUOp.OrrNot) ty x y shift))\n\n;; Helpers for generating `orr` instructions.\n\n(decl orr (Type Reg Reg) Reg)\n(rule (orr ty x y) (alu_rrr (ALUOp.Orr) ty x y))\n\n(decl orr_imm (Type Reg ImmLogic) Reg)\n(rule (orr_imm ty x y) (alu_rr_imm_logic (ALUOp.Orr) ty x y))\n\n(decl orr_vec (Reg Reg VectorSize) Reg)\n(rule (orr_vec x y size) (vec_rrr (VecALUOp.Orr) x y size))\n\n;; Helpers for generating `and` instructions.\n\n(decl and_reg (Type Reg Reg) Reg)\n(rule (and_reg ty x y) (alu_rrr (ALUOp.And) ty x y))\n\n(decl and_imm (Type Reg ImmLogic) Reg)\n(rule (and_imm ty x y) (alu_rr_imm_logic (ALUOp.And) ty x y))\n\n(decl and_vec (Reg Reg VectorSize) Reg)\n(rule (and_vec x y size) (vec_rrr (VecALUOp.And) x y size))\n\n;; Helpers for generating `eor` instructions.\n(decl eor_vec (Reg Reg VectorSize) Reg)\n(rule (eor_vec x y size) (vec_rrr (VecALUOp.Eor) x y size))\n\n;; Helpers for generating `bic` instructions.\n\n(decl bic (Type Reg Reg) Reg)\n(rule (bic ty x y) (alu_rrr (ALUOp.AndNot) ty x y))\n\n(decl bic_vec (Reg Reg VectorSize) Reg)\n(rule (bic_vec x y size) (vec_rrr (VecALUOp.Bic) x y size))\n\n;; Helpers for generating `sshl` instructions.\n(decl sshl (Reg Reg VectorSize) Reg)\n(rule (sshl x y size) (vec_rrr (VecALUOp.Sshl) x y size))\n\n;; Helpers for generating `ushl` instructions.\n(decl ushl (Reg Reg VectorSize) Reg)\n(rule (ushl x y size) (vec_rrr (VecALUOp.Ushl) x y size))\n\n;; Helpers for generating `rotr` instructions.\n\n(decl a64_rotr (Type Reg Reg) Reg)\n(rule (a64_rotr ty x y) (alu_rrr (ALUOp.RotR) ty x y))\n\n(decl a64_rotr_imm (Type Reg ImmShift) Reg)\n(rule (a64_rotr_imm ty x y) (alu_rr_imm_shift (ALUOp.RotR) ty x y))\n\n;; Helpers for generating `rbit` instructions.\n\n(decl rbit (Type Reg) Reg)\n(rule (rbit ty x) (bit_rr (BitOp.RBit) ty x))\n\n;; Helpers for generating `clz` instructions.\n\n(decl a64_clz (Type Reg) Reg)\n(rule (a64_clz ty x) (bit_rr (BitOp.Clz) ty x))\n\n;; Helpers for generating `cls` instructions.\n\n(decl a64_cls (Type Reg) Reg)\n(rule (a64_cls ty x) (bit_rr (BitOp.Cls) ty x))\n\n;; Helpers for generating `eon` instructions.\n\n(decl eon (Type Reg Reg) Reg)\n(rule (eon ty x y) (alu_rrr (ALUOp.EorNot) ty x y))\n\n;; Helpers for generating `cnt` instructions.\n\n(decl vec_cnt (Reg VectorSize) Reg)\n(rule (vec_cnt x size) (vec_misc (VecMisc2.Cnt) x size))\n\n;; Helpers for generating a `bsl` instruction.\n\n(decl bsl (Type Reg Reg Reg) Reg)\n(rule (bsl ty c x y)\n      (let ((dst WritableReg (temp_writable_reg ty))\n            (_1 Unit (emit (MInst.FpuMove128 dst c)))\n            (_2 Unit (emit (MInst.VecRRR (VecALUOp.Bsl) dst x y (vector_size ty)))))\n        dst))\n\n;; Helper for generating a `udf` instruction.\n\n(decl udf (bool TrapCode) SideEffectNoResult)\n(rule (udf use_allocated_encoding trap_code)\n      (SideEffectNoResult.Inst (MInst.Udf use_allocated_encoding trap_code)))\n\n;; Immediate value helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Type of extension performed by an immediate helper\n(type ImmExtend\n  (enum\n    (Sign)\n    (Zero)))\n\n;; Arguments:\n;; * Immediate type\n;; * Way to extend the immediate value to the full width of the destination\n;;   register\n;; * Immediate value - only the bits that fit within the type are used and\n;;   extended, while the rest are ignored\n;;\n;; Note that, unlike the convention in the AArch64 backend, this helper leaves\n;; all bits in the destination register in a defined state, i.e. smaller types\n;; such as `I8` are either sign- or zero-extended.\n(decl imm (Type ImmExtend u64) Reg)\n\n;; Weird logical-instruction immediate in ORI using zero register; to simplify,\n;; we only match when we are zero-extending the value.\n(rule (imm (integral_ty ty) (ImmExtend.Zero) k)\n      (if-let n (imm_logic_from_u64 ty k))\n      (orr_imm ty (zero_reg) n))\n\n(decl load_constant64_full (Type ImmExtend u64) Reg)\n(extern constructor load_constant64_full load_constant64_full)\n\n;; Fallback for integral 64-bit constants\n(rule (imm (integral_ty ty) extend n)\n      (load_constant64_full ty extend n))\n\n;; Sign extension helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Place a `Value` into a register, sign extending it to 32-bits\n(decl put_in_reg_sext32 (Value) Reg)\n(rule (put_in_reg_sext32 val @ (value_type (fits_in_32 ty)))\n      (extend val $true (ty_bits ty) 32))\n\n;; 32/64-bit passthrough.\n(rule (put_in_reg_sext32 val @ (value_type $I32)) val)\n(rule (put_in_reg_sext32 val @ (value_type $I64)) val)\n\n;; Place a `Value` into a register, zero extending it to 32-bits\n(decl put_in_reg_zext32 (Value) Reg)\n(rule (put_in_reg_zext32 val @ (value_type (fits_in_32 ty)))\n      (extend val $false (ty_bits ty) 32))\n\n;; 32/64-bit passthrough.\n(rule (put_in_reg_zext32 val @ (value_type $I32)) val)\n(rule (put_in_reg_zext32 val @ (value_type $I64)) val)\n\n;; Place a `Value` into a register, sign extending it to 64-bits\n(decl put_in_reg_sext64 (Value) Reg)\n(rule (put_in_reg_sext64 val @ (value_type (fits_in_32 ty)))\n      (extend val $true (ty_bits ty) 64))\n\n;; 64-bit passthrough.\n(rule (put_in_reg_sext64 val @ (value_type $I64)) val)\n\n;; Place a `Value` into a register, zero extending it to 64-bits\n(decl put_in_reg_zext64 (Value) Reg)\n(rule (put_in_reg_zext64 val @ (value_type (fits_in_32 ty)))\n      (extend val $false (ty_bits ty) 64))\n\n;; 64-bit passthrough.\n(rule (put_in_reg_zext64 val @ (value_type $I64)) val)\n\n;; Misc instruction helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(decl trap_if_zero_divisor (Reg) Reg)\n(rule (trap_if_zero_divisor reg)\n      (let ((_ Unit (emit (MInst.TrapIf (cond_br_zero reg) (trap_code_division_by_zero)))))\n        reg))\n\n(decl size_from_ty (Type) OperandSize)\n(rule (size_from_ty (fits_in_32 _ty)) (OperandSize.Size32))\n(rule (size_from_ty $I64) (OperandSize.Size64))\n\n;; Check for signed overflow. The only case is min_value / -1.\n;; The following checks must be done in 32-bit or 64-bit, depending\n;; on the input type.\n(decl trap_if_div_overflow (Type Reg Reg) Reg)\n(rule (trap_if_div_overflow ty x y)\n      (let (\n          ;; Check RHS is -1.\n          (_1 Unit (emit (MInst.AluRRImm12 (ALUOp.AddS) (operand_size ty) (writable_zero_reg) y (u8_into_imm12 1))))\n\n          ;; Check LHS is min_value, by subtracting 1 and branching if\n          ;; there is overflow.\n          (_2 Unit (emit (MInst.CCmpImm (size_from_ty ty)\n                                        x\n                                        (u8_into_uimm5 1)\n                                        (nzcv $false $false $false $false)\n                                        (Cond.Eq))))\n          (_3 Unit (emit (MInst.TrapIf (cond_br_cond (Cond.Vs))\n                                      (trap_code_integer_overflow))))\n        )\n        x))\n\n;; An atomic load that can be sunk into another operation.\n(type SinkableAtomicLoad extern (enum))\n\n;; Extract a `SinkableAtomicLoad` that works with `Reg` from a value\n;; operand.\n(decl sinkable_atomic_load (SinkableAtomicLoad) Value)\n(extern extractor sinkable_atomic_load sinkable_atomic_load)\n\n;; Sink a `SinkableLoad` into a `Reg`.\n;;\n;; This is a side-effectful operation that notifies the context that the\n;; instruction that produced the `SinkableAtomicLoad` has been sunk into another\n;; instruction, and no longer needs to be lowered.\n(decl sink_atomic_load (SinkableAtomicLoad) Reg)\n(extern constructor sink_atomic_load sink_atomic_load)\n\n;; Helper for generating either an `AluRRR`, `AluRRRShift`, or `AluRRImmLogic`\n;; instruction depending on the input. Note that this requires that the `ALUOp`\n;; specified is commutative.\n(decl alu_rs_imm_logic_commutative (ALUOp Type Value Value) Reg)\n\n;; Base case of operating on registers.\n(rule (alu_rs_imm_logic_commutative op ty x y)\n      (alu_rrr op ty x y))\n\n;; Special cases for when one operand is a constant.\n(rule (alu_rs_imm_logic_commutative op ty x (iconst k))\n      (if-let imm (imm_logic_from_imm64 ty k))\n      (alu_rr_imm_logic op ty x imm))\n(rule (alu_rs_imm_logic_commutative op ty (iconst k) x)\n      (if-let imm (imm_logic_from_imm64 ty k))\n      (alu_rr_imm_logic op ty x imm))\n\n;; Special cases for when one operand is shifted left by a constant.\n(rule (alu_rs_imm_logic_commutative op ty x (ishl y (iconst k)))\n      (if-let amt (lshl_from_imm64 ty k))\n      (alu_rrr_shift op ty x y amt))\n(rule (alu_rs_imm_logic_commutative op ty (ishl x (iconst k)) y)\n      (if-let amt (lshl_from_imm64 ty k))\n      (alu_rrr_shift op ty y x amt))\n\n;; Same as `alu_rs_imm_logic_commutative` above, except that it doesn't require\n;; that the operation is commutative.\n(decl alu_rs_imm_logic (ALUOp Type Value Value) Reg)\n(rule (alu_rs_imm_logic op ty x y)\n      (alu_rrr op ty x y))\n(rule (alu_rs_imm_logic op ty x (iconst k))\n      (if-let imm (imm_logic_from_imm64 ty k))\n      (alu_rr_imm_logic op ty x imm))\n(rule (alu_rs_imm_logic op ty x (ishl y (iconst k)))\n      (if-let amt (lshl_from_imm64 ty k))\n      (alu_rrr_shift op ty x y amt))\n\n;; Helper for generating i128 bitops which simply do the same operation to the\n;; hi/lo registers.\n;;\n;; TODO: Support immlogic here\n(decl i128_alu_bitop (ALUOp Type Value Value) ValueRegs)\n(rule (i128_alu_bitop op ty x y)\n      (let (\n          (x_regs ValueRegs (put_in_regs x))\n          (x_lo Reg (value_regs_get x_regs 0))\n          (x_hi Reg (value_regs_get x_regs 1))\n          (y_regs ValueRegs (put_in_regs y))\n          (y_lo Reg (value_regs_get y_regs 0))\n          (y_hi Reg (value_regs_get y_regs 1))\n        )\n        (value_regs\n          (alu_rrr op ty x_lo y_lo)\n          (alu_rrr op ty x_hi y_hi))))\n\n;; Float vector compare helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Match 32 bit float 0 value\n(decl zero_value_f32 (Ieee32) Ieee32)\n(extern extractor zero_value_f32 zero_value_f32)\n\n;; Match 64 bit float 0 value\n(decl zero_value_f64 (Ieee64) Ieee64)\n(extern extractor zero_value_f64 zero_value_f64)\n\n;; Generate comparison to zero operator from input condition code\n(decl float_cc_cmp_zero_to_vec_misc_op (FloatCC) VecMisc2)\n(extern constructor float_cc_cmp_zero_to_vec_misc_op float_cc_cmp_zero_to_vec_misc_op)\n\n(decl float_cc_cmp_zero_to_vec_misc_op_swap (FloatCC) VecMisc2)\n(extern constructor float_cc_cmp_zero_to_vec_misc_op_swap float_cc_cmp_zero_to_vec_misc_op_swap)\n\n;; Match valid generic compare to zero cases\n(decl fcmp_zero_cond (FloatCC) FloatCC)\n(extern extractor fcmp_zero_cond fcmp_zero_cond)\n\n;; Match not equal compare to zero separately as it requires two output instructions\n(decl fcmp_zero_cond_not_eq (FloatCC) FloatCC)\n(extern extractor fcmp_zero_cond_not_eq fcmp_zero_cond_not_eq)\n\n;; Helper for generating float compare to zero instructions where 2nd argument is zero\n(decl float_cmp_zero (FloatCC Reg VectorSize) Reg)\n(rule (float_cmp_zero cond rn size)\n      (vec_misc (float_cc_cmp_zero_to_vec_misc_op cond) rn size))\n\n;; Helper for generating float compare to zero instructions in case where 1st argument is zero\n(decl float_cmp_zero_swap (FloatCC Reg VectorSize) Reg)\n(rule (float_cmp_zero_swap cond rn size)\n      (vec_misc (float_cc_cmp_zero_to_vec_misc_op_swap cond) rn size))\n\n;; Helper for generating float compare equal to zero instruction\n(decl fcmeq0 (Reg VectorSize) Reg)\n(rule (fcmeq0 rn size)\n      (vec_misc (VecMisc2.Fcmeq0) rn size))\n\n;; Int vector compare helpers ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Match integer 0 value\n(decl zero_value (Imm64) Imm64)\n(extern extractor zero_value zero_value)\n\n;; Generate comparison to zero operator from input condition code\n(decl int_cc_cmp_zero_to_vec_misc_op (IntCC) VecMisc2)\n(extern constructor int_cc_cmp_zero_to_vec_misc_op int_cc_cmp_zero_to_vec_misc_op)\n\n(decl int_cc_cmp_zero_to_vec_misc_op_swap (IntCC) VecMisc2)\n(extern constructor int_cc_cmp_zero_to_vec_misc_op_swap int_cc_cmp_zero_to_vec_misc_op_swap)\n\n;; Match valid generic compare to zero cases\n(decl icmp_zero_cond (IntCC) IntCC)\n(extern extractor icmp_zero_cond icmp_zero_cond)\n\n;; Match not equal compare to zero separately as it requires two output instructions\n(decl icmp_zero_cond_not_eq (IntCC) IntCC)\n(extern extractor icmp_zero_cond_not_eq icmp_zero_cond_not_eq)\n\n;; Helper for generating int compare to zero instructions where 2nd argument is zero\n(decl int_cmp_zero (IntCC Reg VectorSize) Reg)\n(rule (int_cmp_zero cond rn size)\n      (vec_misc (int_cc_cmp_zero_to_vec_misc_op cond) rn size))\n\n;; Helper for generating int compare to zero instructions in case where 1st argument is zero\n(decl int_cmp_zero_swap (IntCC Reg VectorSize) Reg)\n(rule (int_cmp_zero_swap cond rn size)\n      (vec_misc (int_cc_cmp_zero_to_vec_misc_op_swap cond) rn size))\n\n;; Helper for generating int compare equal to zero instruction\n(decl cmeq0 (Reg VectorSize) Reg)\n(rule (cmeq0 rn size)\n      (vec_misc (VecMisc2.Cmeq0) rn size))\n\n;; Helper for emitting `MInst.AtomicRMW` instructions.\n(decl lse_atomic_rmw (AtomicRMWOp Value Reg Type) Reg)\n(rule (lse_atomic_rmw op p r_arg2 ty)\n      (let (\n          (r_addr Reg p)\n          (dst WritableReg (temp_writable_reg ty))\n          (_ Unit (emit (MInst.AtomicRMW op r_arg2 dst r_addr ty)))\n        )\n        dst))\n\n;; Helper for emitting `MInst.AtomicCAS` instructions.\n(decl lse_atomic_cas (Reg Reg Reg Type) Reg)\n(rule (lse_atomic_cas addr expect replace ty)\n      (let (\n            (dst WritableReg (temp_writable_reg ty))\n            (_1 Unit (emit (MInst.Mov (operand_size ty) dst expect)))\n            (_2 Unit (emit (MInst.AtomicCAS dst replace addr ty)))\n          )\n          dst))\n\n;; Helper for emitting `MInst.AtomicRMWLoop` instructions.\n;; - Make sure that both args are in virtual regs, since in effect\n;; we have to do a parallel copy to get them safely to the AtomicRMW input\n;; regs, and that's not guaranteed safe if either is in a real reg.\n;; - Move the args to the preordained AtomicRMW input regs\n;; - And finally, copy the preordained AtomicRMW output reg to its destination.\n(decl atomic_rmw_loop (AtomicRMWLoopOp Value Value Type) Reg)\n(rule (atomic_rmw_loop op p arg2 ty)\n      (let (\n          (v_addr Reg (ensure_in_vreg p $I64))\n          (v_arg2 Reg (ensure_in_vreg arg2 $I64))\n          (r_addr Reg (mov64_to_real 25 v_addr))\n          (r_arg2 Reg (mov64_to_real 26 v_arg2))\n          (_ Unit (emit (MInst.AtomicRMWLoop ty op)))\n        )\n        (mov64_from_real 27)))\n\n;; Helper for emitting `MInst.AtomicCASLoop` instructions.\n;; This is very similar to, but not identical to, the AtomicRmw case.  Note\n;; that the AtomicCASLoop sequence does its own masking, so we don't need to worry\n;; about zero-extending narrow (I8/I16/I32) values here.\n;; Make sure that all three args are in virtual regs.  See corresponding comment\n;; for `atomic_rmw_loop` above.\n(decl atomic_cas_loop (Reg Reg Reg Type) Reg)\n(rule (atomic_cas_loop addr expect replace ty)\n      (let (\n          (v_addr Reg (ensure_in_vreg addr $I64))\n          (v_exp Reg (ensure_in_vreg expect $I64))\n          (v_rep Reg (ensure_in_vreg replace $I64))\n          ;; Move the args to the preordained AtomicCASLoop input regs\n          (r_addr Reg (mov64_to_real 25 v_addr))\n          (r_exp Reg (mov64_to_real 26 v_exp))\n          (r_rep Reg (mov64_to_real 28 v_rep))\n          ;; Now the AtomicCASLoop itself, implemented in the normal way, with a\n          ;; load-exclusive, store-exclusive loop\n          (_ Unit (emit (MInst.AtomicCASLoop ty)))\n        )\n        ;; And finally, copy the preordained AtomicCASLoop output reg to its destination.\n        ;; Also, x24 and x28 are trashed.\n        (mov64_from_real 27)))\n", "//! AArch64 ISA definitions: immediate constants.\n\n// Some variants are never constructed, but we still want them as options in the future.\n#[allow(dead_code)]\nuse crate::ir::types::*;\nuse crate::ir::Type;\nuse crate::isa::aarch64::inst::{OperandSize, ScalarSize};\nuse crate::machinst::{AllocationConsumer, PrettyPrint};\n\nuse core::convert::TryFrom;\nuse std::string::String;\n\n/// An immediate that represents the NZCV flags.\n#[derive(Clone, Copy, Debug)]\npub struct NZCV {\n    /// The negative condition flag.\n    n: bool,\n    /// The zero condition flag.\n    z: bool,\n    /// The carry condition flag.\n    c: bool,\n    /// The overflow condition flag.\n    v: bool,\n}\n\nimpl NZCV {\n    pub fn new(n: bool, z: bool, c: bool, v: bool) -> NZCV {\n        NZCV { n, z, c, v }\n    }\n\n    /// Bits for encoding.\n    pub fn bits(&self) -> u32 {\n        (u32::from(self.n) << 3)\n            | (u32::from(self.z) << 2)\n            | (u32::from(self.c) << 1)\n            | u32::from(self.v)\n    }\n}\n\n/// An unsigned 5-bit immediate.\n#[derive(Clone, Copy, Debug)]\npub struct UImm5 {\n    /// The value.\n    value: u8,\n}\n\nimpl UImm5 {\n    pub fn maybe_from_u8(value: u8) -> Option<UImm5> {\n        if value < 32 {\n            Some(UImm5 { value })\n        } else {\n            None\n        }\n    }\n\n    /// Bits for encoding.\n    pub fn bits(&self) -> u32 {\n        u32::from(self.value)\n    }\n}\n\n/// A signed, scaled 7-bit offset.\n#[derive(Clone, Copy, Debug)]\npub struct SImm7Scaled {\n    /// The value.\n    pub value: i16,\n    /// multiplied by the size of this type\n    pub scale_ty: Type,\n}\n\nimpl SImm7Scaled {\n    /// Create a SImm7Scaled from a raw offset and the known scale type, if\n    /// possible.\n    pub fn maybe_from_i64(value: i64, scale_ty: Type) -> Option<SImm7Scaled> {\n        assert!(scale_ty == I64 || scale_ty == I32 || scale_ty == F64 || scale_ty == I8X16);\n        let scale = scale_ty.bytes();\n        assert!(scale.is_power_of_two());\n        let scale = i64::from(scale);\n        let upper_limit = 63 * scale;\n        let lower_limit = -(64 * scale);\n        if value >= lower_limit && value <= upper_limit && (value & (scale - 1)) == 0 {\n            Some(SImm7Scaled {\n                value: i16::try_from(value).unwrap(),\n                scale_ty,\n            })\n        } else {\n            None\n        }\n    }\n\n    /// Bits for encoding.\n    pub fn bits(&self) -> u32 {\n        let ty_bytes: i16 = self.scale_ty.bytes() as i16;\n        let scaled: i16 = self.value / ty_bytes;\n        assert!(scaled <= 63 && scaled >= -64);\n        let scaled: i8 = scaled as i8;\n        let encoded: u32 = scaled as u32;\n        encoded & 0x7f\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct FPULeftShiftImm {\n    pub amount: u8,\n    pub lane_size_in_bits: u8,\n}\n\nimpl FPULeftShiftImm {\n    pub fn maybe_from_u8(amount: u8, lane_size_in_bits: u8) -> Option<Self> {\n        debug_assert!(lane_size_in_bits == 32 || lane_size_in_bits == 64);\n        if amount < lane_size_in_bits {\n            Some(Self {\n                amount,\n                lane_size_in_bits,\n            })\n        } else {\n            None\n        }\n    }\n\n    pub fn enc(&self) -> u32 {\n        debug_assert!(self.lane_size_in_bits.is_power_of_two());\n        debug_assert!(self.lane_size_in_bits > self.amount);\n        // The encoding of the immediate follows the table below,\n        // where xs encode the shift amount.\n        //\n        // | lane_size_in_bits | encoding |\n        // +------------------------------+\n        // | 8                 | 0001xxx  |\n        // | 16                | 001xxxx  |\n        // | 32                | 01xxxxx  |\n        // | 64                | 1xxxxxx  |\n        //\n        // The highest one bit is represented by `lane_size_in_bits`. Since\n        // `lane_size_in_bits` is a power of 2 and `amount` is less\n        // than `lane_size_in_bits`, they can be ORed\n        // together to produced the encoded value.\n        u32::from(self.lane_size_in_bits | self.amount)\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct FPURightShiftImm {\n    pub amount: u8,\n    pub lane_size_in_bits: u8,\n}\n\nimpl FPURightShiftImm {\n    pub fn maybe_from_u8(amount: u8, lane_size_in_bits: u8) -> Option<Self> {\n        debug_assert!(lane_size_in_bits == 32 || lane_size_in_bits == 64);\n        if amount > 0 && amount <= lane_size_in_bits {\n            Some(Self {\n                amount,\n                lane_size_in_bits,\n            })\n        } else {\n            None\n        }\n    }\n\n    pub fn enc(&self) -> u32 {\n        debug_assert_ne!(0, self.amount);\n        // The encoding of the immediate follows the table below,\n        // where xs encodes the negated shift amount.\n        //\n        // | lane_size_in_bits | encoding |\n        // +------------------------------+\n        // | 8                 | 0001xxx  |\n        // | 16                | 001xxxx  |\n        // | 32                | 01xxxxx  |\n        // | 64                | 1xxxxxx  |\n        //\n        // The shift amount is negated such that a shift ammount\n        // of 1 (in 64-bit) is encoded as 0b111111 and a shift\n        // amount of 64 is encoded as 0b000000,\n        // in the bottom 6 bits.\n        u32::from((self.lane_size_in_bits * 2) - self.amount)\n    }\n}\n\n/// a 9-bit signed offset.\n#[derive(Clone, Copy, Debug)]\npub struct SImm9 {\n    /// The value.\n    pub value: i16,\n}\n\nimpl SImm9 {\n    /// Create a signed 9-bit offset from a full-range value, if possible.\n    pub fn maybe_from_i64(value: i64) -> Option<SImm9> {\n        if value >= -256 && value <= 255 {\n            Some(SImm9 {\n                value: value as i16,\n            })\n        } else {\n            None\n        }\n    }\n\n    /// Bits for encoding.\n    pub fn bits(&self) -> u32 {\n        (self.value as u32) & 0x1ff\n    }\n\n    /// Signed value of immediate.\n    pub fn value(&self) -> i32 {\n        self.value as i32\n    }\n}\n\n/// An unsigned, scaled 12-bit offset.\n#[derive(Clone, Copy, Debug)]\npub struct UImm12Scaled {\n    /// The value.\n    pub value: u16,\n    /// multiplied by the size of this type\n    pub scale_ty: Type,\n}\n\nimpl UImm12Scaled {\n    /// Create a UImm12Scaled from a raw offset and the known scale type, if\n    /// possible.\n    pub fn maybe_from_i64(value: i64, scale_ty: Type) -> Option<UImm12Scaled> {\n        // Ensure the type is at least one byte.\n        let scale_ty = if scale_ty == B1 { B8 } else { scale_ty };\n\n        let scale = scale_ty.bytes();\n        assert!(scale.is_power_of_two());\n        let scale = scale as i64;\n        let limit = 4095 * scale;\n        if value >= 0 && value <= limit && (value & (scale - 1)) == 0 {\n            Some(UImm12Scaled {\n                value: value as u16,\n                scale_ty,\n            })\n        } else {\n            None\n        }\n    }\n\n    /// Create a zero immediate of this format.\n    pub fn zero(scale_ty: Type) -> UImm12Scaled {\n        UImm12Scaled { value: 0, scale_ty }\n    }\n\n    /// Encoded bits.\n    pub fn bits(&self) -> u32 {\n        (self.value as u32 / self.scale_ty.bytes()) & 0xfff\n    }\n\n    /// Value after scaling.\n    pub fn value(&self) -> u32 {\n        self.value as u32\n    }\n\n    /// The value type which is the scaling base.\n    pub fn scale_ty(&self) -> Type {\n        self.scale_ty\n    }\n}\n\n/// A shifted immediate value in 'imm12' format: supports 12 bits, shifted\n/// left by 0 or 12 places.\n#[derive(Copy, Clone, Debug)]\npub struct Imm12 {\n    /// The immediate bits.\n    pub bits: u16,\n    /// Whether the immediate bits are shifted left by 12 or not.\n    pub shift12: bool,\n}\n\nimpl Imm12 {\n    /// Compute a Imm12 from raw bits, if possible.\n    pub fn maybe_from_u64(val: u64) -> Option<Imm12> {\n        if val == 0 {\n            Some(Imm12 {\n                bits: 0,\n                shift12: false,\n            })\n        } else if val < 0xfff {\n            Some(Imm12 {\n                bits: val as u16,\n                shift12: false,\n            })\n        } else if val < 0xfff_000 && (val & 0xfff == 0) {\n            Some(Imm12 {\n                bits: (val >> 12) as u16,\n                shift12: true,\n            })\n        } else {\n            None\n        }\n    }\n\n    /// Create a zero immediate of this format.\n    pub fn zero() -> Self {\n        Imm12 {\n            bits: 0,\n            shift12: false,\n        }\n    }\n\n    /// Bits for 2-bit \"shift\" field in e.g. AddI.\n    pub fn shift_bits(&self) -> u32 {\n        if self.shift12 {\n            0b01\n        } else {\n            0b00\n        }\n    }\n\n    /// Bits for 12-bit \"imm\" field in e.g. AddI.\n    pub fn imm_bits(&self) -> u32 {\n        self.bits as u32\n    }\n}\n\n/// An immediate for logical instructions.\n#[derive(Copy, Clone, Debug, PartialEq)]\npub struct ImmLogic {\n    /// The actual value.\n    value: u64,\n    /// `N` flag.\n    pub n: bool,\n    /// `S` field: element size and element bits.\n    pub r: u8,\n    /// `R` field: rotate amount.\n    pub s: u8,\n    /// Was this constructed for a 32-bit or 64-bit instruction?\n    pub size: OperandSize,\n}\n\nimpl ImmLogic {\n    /// Compute an ImmLogic from raw bits, if possible.\n    pub fn maybe_from_u64(value: u64, ty: Type) -> Option<ImmLogic> {\n        // Note: This function is a port of VIXL's Assembler::IsImmLogical.\n\n        if ty != I64 && ty != I32 {\n            return None;\n        }\n        let operand_size = OperandSize::from_ty(ty);\n\n        let original_value = value;\n\n        let value = if ty == I32 {\n            // To handle 32-bit logical immediates, the very easiest thing is to repeat\n            // the input value twice to make a 64-bit word. The correct encoding of that\n            // as a logical immediate will also be the correct encoding of the 32-bit\n            // value.\n\n            // Avoid making the assumption that the most-significant 32 bits are zero by\n            // shifting the value left and duplicating it.\n            let value = value << 32;\n            value | value >> 32\n        } else {\n            value\n        };\n\n        // Logical immediates are encoded using parameters n, imm_s and imm_r using\n        // the following table:\n        //\n        //    N   imms    immr    size        S             R\n        //    1  ssssss  rrrrrr    64    UInt(ssssss)  UInt(rrrrrr)\n        //    0  0sssss  xrrrrr    32    UInt(sssss)   UInt(rrrrr)\n        //    0  10ssss  xxrrrr    16    UInt(ssss)    UInt(rrrr)\n        //    0  110sss  xxxrrr     8    UInt(sss)     UInt(rrr)\n        //    0  1110ss  xxxxrr     4    UInt(ss)      UInt(rr)\n        //    0  11110s  xxxxxr     2    UInt(s)       UInt(r)\n        // (s bits must not be all set)\n        //\n        // A pattern is constructed of size bits, where the least significant S+1 bits\n        // are set. The pattern is rotated right by R, and repeated across a 32 or\n        // 64-bit value, depending on destination register width.\n        //\n        // Put another way: the basic format of a logical immediate is a single\n        // contiguous stretch of 1 bits, repeated across the whole word at intervals\n        // given by a power of 2. To identify them quickly, we first locate the\n        // lowest stretch of 1 bits, then the next 1 bit above that; that combination\n        // is different for every logical immediate, so it gives us all the\n        // information we need to identify the only logical immediate that our input\n        // could be, and then we simply check if that's the value we actually have.\n        //\n        // (The rotation parameter does give the possibility of the stretch of 1 bits\n        // going 'round the end' of the word. To deal with that, we observe that in\n        // any situation where that happens the bitwise NOT of the value is also a\n        // valid logical immediate. So we simply invert the input whenever its low bit\n        // is set, and then we know that the rotated case can't arise.)\n        let (value, inverted) = if value & 1 == 1 {\n            (!value, true)\n        } else {\n            (value, false)\n        };\n\n        if value == 0 {\n            return None;\n        }\n\n        // The basic analysis idea: imagine our input word looks like this.\n        //\n        //    0011111000111110001111100011111000111110001111100011111000111110\n        //                                                          c  b    a\n        //                                                          |<--d-->|\n        //\n        // We find the lowest set bit (as an actual power-of-2 value, not its index)\n        // and call it a. Then we add a to our original number, which wipes out the\n        // bottommost stretch of set bits and replaces it with a 1 carried into the\n        // next zero bit. Then we look for the new lowest set bit, which is in\n        // position b, and subtract it, so now our number is just like the original\n        // but with the lowest stretch of set bits completely gone. Now we find the\n        // lowest set bit again, which is position c in the diagram above. Then we'll\n        // measure the distance d between bit positions a and c (using CLZ), and that\n        // tells us that the only valid logical immediate that could possibly be equal\n        // to this number is the one in which a stretch of bits running from a to just\n        // below b is replicated every d bits.\n        fn lowest_set_bit(value: u64) -> u64 {\n            let bit = value.trailing_zeros();\n            1u64.checked_shl(bit).unwrap_or(0)\n        }\n        let a = lowest_set_bit(value);\n        assert_ne!(0, a);\n        let value_plus_a = value.wrapping_add(a);\n        let b = lowest_set_bit(value_plus_a);\n        let value_plus_a_minus_b = value_plus_a - b;\n        let c = lowest_set_bit(value_plus_a_minus_b);\n\n        let (d, clz_a, out_n, mask) = if c != 0 {\n            // The general case, in which there is more than one stretch of set bits.\n            // Compute the repeat distance d, and set up a bitmask covering the basic\n            // unit of repetition (i.e. a word with the bottom d bits set). Also, in all\n            // of these cases the N bit of the output will be zero.\n            let clz_a = a.leading_zeros();\n            let clz_c = c.leading_zeros();\n            let d = clz_a - clz_c;\n            let mask = (1 << d) - 1;\n            (d, clz_a, 0, mask)\n        } else {\n            (64, a.leading_zeros(), 1, u64::max_value())\n        };\n\n        // If the repeat period d is not a power of two, it can't be encoded.\n        if !d.is_power_of_two() {\n            return None;\n        }\n\n        if ((b.wrapping_sub(a)) & !mask) != 0 {\n            // If the bit stretch (b - a) does not fit within the mask derived from the\n            // repeat period, then fail.\n            return None;\n        }\n\n        // The only possible option is b - a repeated every d bits. Now we're going to\n        // actually construct the valid logical immediate derived from that\n        // specification, and see if it equals our original input.\n        //\n        // To repeat a value every d bits, we multiply it by a number of the form\n        // (1 + 2^d + 2^(2d) + ...), i.e. 0x0001000100010001 or similar. These can\n        // be derived using a table lookup on CLZ(d).\n        const MULTIPLIERS: [u64; 6] = [\n            0x0000000000000001,\n            0x0000000100000001,\n            0x0001000100010001,\n            0x0101010101010101,\n            0x1111111111111111,\n            0x5555555555555555,\n        ];\n        let multiplier = MULTIPLIERS[(u64::from(d).leading_zeros() - 57) as usize];\n        let candidate = b.wrapping_sub(a) * multiplier;\n\n        if value != candidate {\n            // The candidate pattern doesn't match our input value, so fail.\n            return None;\n        }\n\n        // We have a match! This is a valid logical immediate, so now we have to\n        // construct the bits and pieces of the instruction encoding that generates\n        // it.\n\n        // Count the set bits in our basic stretch. The special case of clz(0) == -1\n        // makes the answer come out right for stretches that reach the very top of\n        // the word (e.g. numbers like 0xffffc00000000000).\n        let clz_b = if b == 0 {\n            u32::max_value() // -1\n        } else {\n            b.leading_zeros()\n        };\n        let s = clz_a.wrapping_sub(clz_b);\n\n        // Decide how many bits to rotate right by, to put the low bit of that basic\n        // stretch in position a.\n        let (s, r) = if inverted {\n            // If we inverted the input right at the start of this function, here's\n            // where we compensate: the number of set bits becomes the number of clear\n            // bits, and the rotation count is based on position b rather than position\n            // a (since b is the location of the 'lowest' 1 bit after inversion).\n            // Need wrapping for when clz_b is max_value() (for when b == 0).\n            (d - s, clz_b.wrapping_add(1) & (d - 1))\n        } else {\n            (s, (clz_a + 1) & (d - 1))\n        };\n\n        // Now we're done, except for having to encode the S output in such a way that\n        // it gives both the number of set bits and the length of the repeated\n        // segment. The s field is encoded like this:\n        //\n        //     imms    size        S\n        //    ssssss    64    UInt(ssssss)\n        //    0sssss    32    UInt(sssss)\n        //    10ssss    16    UInt(ssss)\n        //    110sss     8    UInt(sss)\n        //    1110ss     4    UInt(ss)\n        //    11110s     2    UInt(s)\n        //\n        // So we 'or' (2 * -d) with our computed s to form imms.\n        let s = ((d * 2).wrapping_neg() | (s - 1)) & 0x3f;\n        debug_assert!(u8::try_from(r).is_ok());\n        debug_assert!(u8::try_from(s).is_ok());\n        Some(ImmLogic {\n            value: original_value,\n            n: out_n != 0,\n            r: r as u8,\n            s: s as u8,\n            size: operand_size,\n        })\n    }\n\n    /// Returns bits ready for encoding: (N:1, R:6, S:6)\n    pub fn enc_bits(&self) -> u32 {\n        ((self.n as u32) << 12) | ((self.r as u32) << 6) | (self.s as u32)\n    }\n\n    /// Returns the value that this immediate represents.\n    pub fn value(&self) -> u64 {\n        self.value\n    }\n\n    /// Return an immediate for the bitwise-inverted value.\n    pub fn invert(&self) -> ImmLogic {\n        // For every ImmLogical immediate, the inverse can also be encoded.\n        Self::maybe_from_u64(!self.value, self.size.to_ty()).unwrap()\n    }\n}\n\n/// An immediate for shift instructions.\n#[derive(Copy, Clone, Debug)]\npub struct ImmShift {\n    /// 6-bit shift amount.\n    pub imm: u8,\n}\n\nimpl ImmShift {\n    /// Create an ImmShift from raw bits, if possible.\n    pub fn maybe_from_u64(val: u64) -> Option<ImmShift> {\n        if val < 64 {\n            Some(ImmShift { imm: val as u8 })\n        } else {\n            None\n        }\n    }\n\n    /// Get the immediate value.\n    pub fn value(&self) -> u8 {\n        self.imm\n    }\n}\n\n/// A 16-bit immediate for a MOVZ instruction, with a {0,16,32,48}-bit shift.\n#[derive(Clone, Copy, Debug)]\npub struct MoveWideConst {\n    /// The value.\n    pub bits: u16,\n    /// Result is `bits` shifted 16*shift bits to the left.\n    pub shift: u8,\n}\n\nimpl MoveWideConst {\n    /// Construct a MoveWideConst from an arbitrary 64-bit constant if possible.\n    pub fn maybe_from_u64(value: u64) -> Option<MoveWideConst> {\n        let mask0 = 0x0000_0000_0000_ffffu64;\n        let mask1 = 0x0000_0000_ffff_0000u64;\n        let mask2 = 0x0000_ffff_0000_0000u64;\n        let mask3 = 0xffff_0000_0000_0000u64;\n\n        if value == (value & mask0) {\n            return Some(MoveWideConst {\n                bits: (value & mask0) as u16,\n                shift: 0,\n            });\n        }\n        if value == (value & mask1) {\n            return Some(MoveWideConst {\n                bits: ((value >> 16) & mask0) as u16,\n                shift: 1,\n            });\n        }\n        if value == (value & mask2) {\n            return Some(MoveWideConst {\n                bits: ((value >> 32) & mask0) as u16,\n                shift: 2,\n            });\n        }\n        if value == (value & mask3) {\n            return Some(MoveWideConst {\n                bits: ((value >> 48) & mask0) as u16,\n                shift: 3,\n            });\n        }\n        None\n    }\n\n    pub fn maybe_with_shift(imm: u16, shift: u8) -> Option<MoveWideConst> {\n        let shift_enc = shift / 16;\n        if shift_enc > 3 {\n            None\n        } else {\n            Some(MoveWideConst {\n                bits: imm,\n                shift: shift_enc,\n            })\n        }\n    }\n\n    pub fn zero() -> MoveWideConst {\n        MoveWideConst { bits: 0, shift: 0 }\n    }\n}\n\n/// Advanced SIMD modified immediate as used by MOVI/MVNI.\n#[derive(Clone, Copy, Debug, PartialEq)]\npub struct ASIMDMovModImm {\n    imm: u8,\n    shift: u8,\n    is_64bit: bool,\n    shift_ones: bool,\n}\n\nimpl ASIMDMovModImm {\n    /// Construct an ASIMDMovModImm from an arbitrary 64-bit constant, if possible.\n    /// Note that the bits in `value` outside of the range specified by `size` are\n    /// ignored; for example, in the case of `ScalarSize::Size8` all bits above the\n    /// lowest 8 are ignored.\n    pub fn maybe_from_u64(value: u64, size: ScalarSize) -> Option<ASIMDMovModImm> {\n        match size {\n            ScalarSize::Size8 => Some(ASIMDMovModImm {\n                imm: value as u8,\n                shift: 0,\n                is_64bit: false,\n                shift_ones: false,\n            }),\n            ScalarSize::Size16 => {\n                let value = value as u16;\n\n                if value >> 8 == 0 {\n                    Some(ASIMDMovModImm {\n                        imm: value as u8,\n                        shift: 0,\n                        is_64bit: false,\n                        shift_ones: false,\n                    })\n                } else if value as u8 == 0 {\n                    Some(ASIMDMovModImm {\n                        imm: (value >> 8) as u8,\n                        shift: 8,\n                        is_64bit: false,\n                        shift_ones: false,\n                    })\n                } else {\n                    None\n                }\n            }\n            ScalarSize::Size32 => {\n                let value = value as u32;\n\n                // Value is of the form 0x00MMFFFF.\n                if value & 0xFF00FFFF == 0x0000FFFF {\n                    let imm = (value >> 16) as u8;\n\n                    Some(ASIMDMovModImm {\n                        imm,\n                        shift: 16,\n                        is_64bit: false,\n                        shift_ones: true,\n                    })\n                // Value is of the form 0x0000MMFF.\n                } else if value & 0xFFFF00FF == 0x000000FF {\n                    let imm = (value >> 8) as u8;\n\n                    Some(ASIMDMovModImm {\n                        imm,\n                        shift: 8,\n                        is_64bit: false,\n                        shift_ones: true,\n                    })\n                } else {\n                    // Of the 4 bytes, at most one is non-zero.\n                    for shift in (0..32).step_by(8) {\n                        if value & (0xFF << shift) == value {\n                            return Some(ASIMDMovModImm {\n                                imm: (value >> shift) as u8,\n                                shift,\n                                is_64bit: false,\n                                shift_ones: false,\n                            });\n                        }\n                    }\n\n                    None\n                }\n            }\n            ScalarSize::Size64 => {\n                let mut imm = 0u8;\n\n                // Check if all bytes are either 0 or 0xFF.\n                for i in 0..8 {\n                    let b = (value >> (i * 8)) as u8;\n\n                    if b == 0 || b == 0xFF {\n                        imm |= (b & 1) << i;\n                    } else {\n                        return None;\n                    }\n                }\n\n                Some(ASIMDMovModImm {\n                    imm,\n                    shift: 0,\n                    is_64bit: true,\n                    shift_ones: false,\n                })\n            }\n            _ => None,\n        }\n    }\n\n    /// Create a zero immediate of this format.\n    pub fn zero(size: ScalarSize) -> Self {\n        ASIMDMovModImm {\n            imm: 0,\n            shift: 0,\n            is_64bit: size == ScalarSize::Size64,\n            shift_ones: false,\n        }\n    }\n\n    /// Returns the value that this immediate represents.\n    pub fn value(&self) -> (u8, u32, bool) {\n        (self.imm, self.shift as u32, self.shift_ones)\n    }\n}\n\n/// Advanced SIMD modified immediate as used by the vector variant of FMOV.\n#[derive(Clone, Copy, Debug, PartialEq)]\npub struct ASIMDFPModImm {\n    imm: u8,\n    is_64bit: bool,\n}\n\nimpl ASIMDFPModImm {\n    /// Construct an ASIMDFPModImm from an arbitrary 64-bit constant, if possible.\n    pub fn maybe_from_u64(value: u64, size: ScalarSize) -> Option<ASIMDFPModImm> {\n        // In all cases immediates are encoded as an 8-bit number 0b_abcdefgh;\n        // let `D` be the inverse of the digit `d`.\n        match size {\n            ScalarSize::Size32 => {\n                // In this case the representable immediates are 32-bit numbers of the form\n                // 0b_aBbb_bbbc_defg_h000 shifted to the left by 16.\n                let value = value as u32;\n                let b0_5 = (value >> 19) & 0b111111;\n                let b6 = (value >> 19) & (1 << 6);\n                let b7 = (value >> 24) & (1 << 7);\n                let imm = (b0_5 | b6 | b7) as u8;\n\n                if value == Self::value32(imm) {\n                    Some(ASIMDFPModImm {\n                        imm,\n                        is_64bit: false,\n                    })\n                } else {\n                    None\n                }\n            }\n            ScalarSize::Size64 => {\n                // In this case the representable immediates are 64-bit numbers of the form\n                // 0b_aBbb_bbbb_bbcd_efgh shifted to the left by 48.\n                let b0_5 = (value >> 48) & 0b111111;\n                let b6 = (value >> 48) & (1 << 6);\n                let b7 = (value >> 56) & (1 << 7);\n                let imm = (b0_5 | b6 | b7) as u8;\n\n                if value == Self::value64(imm) {\n                    Some(ASIMDFPModImm {\n                        imm,\n                        is_64bit: true,\n                    })\n                } else {\n                    None\n                }\n            }\n            _ => None,\n        }\n    }\n\n    /// Returns bits ready for encoding.\n    pub fn enc_bits(&self) -> u8 {\n        self.imm\n    }\n\n    /// Returns the 32-bit value that corresponds to an 8-bit encoding.\n    fn value32(imm: u8) -> u32 {\n        let imm = imm as u32;\n        let b0_5 = imm & 0b111111;\n        let b6 = (imm >> 6) & 1;\n        let b6_inv = b6 ^ 1;\n        let b7 = (imm >> 7) & 1;\n\n        b0_5 << 19 | (b6 * 0b11111) << 25 | b6_inv << 30 | b7 << 31\n    }\n\n    /// Returns the 64-bit value that corresponds to an 8-bit encoding.\n    fn value64(imm: u8) -> u64 {\n        let imm = imm as u64;\n        let b0_5 = imm & 0b111111;\n        let b6 = (imm >> 6) & 1;\n        let b6_inv = b6 ^ 1;\n        let b7 = (imm >> 7) & 1;\n\n        b0_5 << 48 | (b6 * 0b11111111) << 54 | b6_inv << 62 | b7 << 63\n    }\n}\n\nimpl PrettyPrint for NZCV {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        let fmt = |c: char, v| if v { c.to_ascii_uppercase() } else { c };\n        format!(\n            \"#{}{}{}{}\",\n            fmt('n', self.n),\n            fmt('z', self.z),\n            fmt('c', self.c),\n            fmt('v', self.v)\n        )\n    }\n}\n\nimpl PrettyPrint for UImm5 {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.value)\n    }\n}\n\nimpl PrettyPrint for Imm12 {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        let shift = if self.shift12 { 12 } else { 0 };\n        let value = u32::from(self.bits) << shift;\n        format!(\"#{}\", value)\n    }\n}\n\nimpl PrettyPrint for SImm7Scaled {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.value)\n    }\n}\n\nimpl PrettyPrint for FPULeftShiftImm {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.amount)\n    }\n}\n\nimpl PrettyPrint for FPURightShiftImm {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.amount)\n    }\n}\n\nimpl PrettyPrint for SImm9 {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.value)\n    }\n}\n\nimpl PrettyPrint for UImm12Scaled {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.value)\n    }\n}\n\nimpl PrettyPrint for ImmLogic {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.value())\n    }\n}\n\nimpl PrettyPrint for ImmShift {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        format!(\"#{}\", self.imm)\n    }\n}\n\nimpl PrettyPrint for MoveWideConst {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        if self.shift == 0 {\n            format!(\"#{}\", self.bits)\n        } else {\n            format!(\"#{}, LSL #{}\", self.bits, self.shift * 16)\n        }\n    }\n}\n\nimpl PrettyPrint for ASIMDMovModImm {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        if self.is_64bit {\n            debug_assert_eq!(self.shift, 0);\n\n            let enc_imm = self.imm as i8;\n            let mut imm = 0u64;\n\n            for i in 0..8 {\n                let b = (enc_imm >> i) & 1;\n\n                imm |= (-b as u8 as u64) << (i * 8);\n            }\n\n            format!(\"#{}\", imm)\n        } else if self.shift == 0 {\n            format!(\"#{}\", self.imm)\n        } else {\n            let shift_type = if self.shift_ones { \"MSL\" } else { \"LSL\" };\n            format!(\"#{}, {} #{}\", self.imm, shift_type, self.shift)\n        }\n    }\n}\n\nimpl PrettyPrint for ASIMDFPModImm {\n    fn pretty_print(&self, _: u8, _: &mut AllocationConsumer<'_>) -> String {\n        if self.is_64bit {\n            format!(\"#{}\", f64::from_bits(Self::value64(self.imm)))\n        } else {\n            format!(\"#{}\", f32::from_bits(Self::value32(self.imm)))\n        }\n    }\n}\n\n#[cfg(test)]\nmod test {\n    use super::*;\n\n    #[test]\n    fn imm_logical_test() {\n        assert_eq!(None, ImmLogic::maybe_from_u64(0, I64));\n        assert_eq!(None, ImmLogic::maybe_from_u64(u64::max_value(), I64));\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 1,\n                n: true,\n                r: 0,\n                s: 0,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(1, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 2,\n                n: true,\n                r: 63,\n                s: 0,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(2, I64)\n        );\n\n        assert_eq!(None, ImmLogic::maybe_from_u64(5, I64));\n\n        assert_eq!(None, ImmLogic::maybe_from_u64(11, I64));\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 248,\n                n: true,\n                r: 61,\n                s: 4,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(248, I64)\n        );\n\n        assert_eq!(None, ImmLogic::maybe_from_u64(249, I64));\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 1920,\n                n: true,\n                r: 57,\n                s: 3,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(1920, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x7ffe,\n                n: true,\n                r: 63,\n                s: 13,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x7ffe, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x30000,\n                n: true,\n                r: 48,\n                s: 1,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x30000, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x100000,\n                n: true,\n                r: 44,\n                s: 0,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x100000, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: u64::max_value() - 1,\n                n: true,\n                r: 63,\n                s: 62,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(u64::max_value() - 1, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0xaaaaaaaaaaaaaaaa,\n                n: false,\n                r: 1,\n                s: 60,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0xaaaaaaaaaaaaaaaa, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x8181818181818181,\n                n: false,\n                r: 1,\n                s: 49,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x8181818181818181, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0xffc3ffc3ffc3ffc3,\n                n: false,\n                r: 10,\n                s: 43,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0xffc3ffc3ffc3ffc3, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x100000001,\n                n: false,\n                r: 0,\n                s: 0,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x100000001, I64)\n        );\n\n        assert_eq!(\n            Some(ImmLogic {\n                value: 0x1111111111111111,\n                n: false,\n                r: 0,\n                s: 56,\n                size: OperandSize::Size64,\n            }),\n            ImmLogic::maybe_from_u64(0x1111111111111111, I64)\n        );\n\n        for n in 0..2 {\n            let types = if n == 0 { vec![I64, I32] } else { vec![I64] };\n            for s in 0..64 {\n                for r in 0..64 {\n                    let imm = get_logical_imm(n, s, r);\n                    for &ty in &types {\n                        match ImmLogic::maybe_from_u64(imm, ty) {\n                            Some(ImmLogic { value, .. }) => {\n                                assert_eq!(imm, value);\n                                ImmLogic::maybe_from_u64(!value, ty).unwrap();\n                            }\n                            None => assert_eq!(0, imm),\n                        };\n                    }\n                }\n            }\n        }\n    }\n\n    // Repeat a value that has `width` bits, across a 64-bit value.\n    fn repeat(value: u64, width: u64) -> u64 {\n        let mut result = value & ((1 << width) - 1);\n        let mut i = width;\n        while i < 64 {\n            result |= result << i;\n            i *= 2;\n        }\n        result\n    }\n\n    // Get the logical immediate, from the encoding N/R/S bits.\n    fn get_logical_imm(n: u32, s: u32, r: u32) -> u64 {\n        // An integer is constructed from the n, imm_s and imm_r bits according to\n        // the following table:\n        //\n        //  N   imms    immr    size        S             R\n        //  1  ssssss  rrrrrr    64    UInt(ssssss)  UInt(rrrrrr)\n        //  0  0sssss  xrrrrr    32    UInt(sssss)   UInt(rrrrr)\n        //  0  10ssss  xxrrrr    16    UInt(ssss)    UInt(rrrr)\n        //  0  110sss  xxxrrr     8    UInt(sss)     UInt(rrr)\n        //  0  1110ss  xxxxrr     4    UInt(ss)      UInt(rr)\n        //  0  11110s  xxxxxr     2    UInt(s)       UInt(r)\n        // (s bits must not be all set)\n        //\n        // A pattern is constructed of size bits, where the least significant S+1\n        // bits are set. The pattern is rotated right by R, and repeated across a\n        // 64-bit value.\n\n        if n == 1 {\n            if s == 0x3f {\n                return 0;\n            }\n            let bits = (1u64 << (s + 1)) - 1;\n            bits.rotate_right(r)\n        } else {\n            if (s >> 1) == 0x1f {\n                return 0;\n            }\n            let mut width = 0x20;\n            while width >= 0x2 {\n                if (s & width) == 0 {\n                    let mask = width - 1;\n                    if (s & mask) == mask {\n                        return 0;\n                    }\n                    let bits = (1u64 << ((s & mask) + 1)) - 1;\n                    return repeat(bits.rotate_right(r & mask), width.into());\n                }\n                width >>= 1;\n            }\n            unreachable!();\n        }\n    }\n\n    #[test]\n    fn asimd_fp_mod_imm_test() {\n        assert_eq!(None, ASIMDFPModImm::maybe_from_u64(0, ScalarSize::Size32));\n        assert_eq!(\n            None,\n            ASIMDFPModImm::maybe_from_u64(0.013671875_f32.to_bits() as u64, ScalarSize::Size32)\n        );\n        assert_eq!(None, ASIMDFPModImm::maybe_from_u64(0, ScalarSize::Size64));\n        assert_eq!(\n            None,\n            ASIMDFPModImm::maybe_from_u64(10000_f64.to_bits(), ScalarSize::Size64)\n        );\n    }\n\n    #[test]\n    fn asimd_mov_mod_imm_test() {\n        assert_eq!(\n            None,\n            ASIMDMovModImm::maybe_from_u64(513, ScalarSize::Size16)\n        );\n        assert_eq!(\n            None,\n            ASIMDMovModImm::maybe_from_u64(4278190335, ScalarSize::Size32)\n        );\n        assert_eq!(\n            None,\n            ASIMDMovModImm::maybe_from_u64(8388608, ScalarSize::Size64)\n        );\n\n        assert_eq!(\n            Some(ASIMDMovModImm {\n                imm: 66,\n                shift: 16,\n                is_64bit: false,\n                shift_ones: true,\n            }),\n            ASIMDMovModImm::maybe_from_u64(4390911, ScalarSize::Size32)\n        );\n    }\n}\n", ";; aarch64 instruction selection and CLIF-to-MachInst lowering.\n\n;; The main lowering constructor term: takes a clif `Inst` and returns the\n;; register(s) within which the lowered instruction's result values live.\n(decl lower (Inst) InstOutput)\n\n;;;; Rules for `iconst` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty (iconst (u64_from_imm64 n))))\n      (imm ty (ImmExtend.Zero) n))\n\n;;;; Rules for `bconst` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty (bconst $false)))\n      (imm ty (ImmExtend.Zero) 0))\n\n(rule (lower (has_type ty (bconst $true)))\n      (imm ty (ImmExtend.Zero) 1))\n\n;;;; Rules for `null` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty (null)))\n      (imm ty (ImmExtend.Zero) 0))\n\n;;;; Rules for `iadd` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; `i64` and smaller\n\n;; Base case, simply adding things in registers.\n(rule (lower (has_type (fits_in_64 ty) (iadd x y)))\n      (add ty  x y))\n\n;; Special cases for when one operand is an immediate that fits in 12 bits.\n(rule (lower (has_type (fits_in_64 ty) (iadd x (imm12_from_value y))))\n      (add_imm ty x y))\n\n(rule (lower (has_type (fits_in_64 ty) (iadd (imm12_from_value x) y)))\n      (add_imm ty y x))\n\n;; Same as the previous special cases, except we can switch the addition to a\n;; subtraction if the negated immediate fits in 12 bits.\n(rule (lower (has_type (fits_in_64 ty) (iadd x (imm12_from_negated_value y))))\n      (sub_imm ty x y))\n\n(rule (lower (has_type (fits_in_64 ty) (iadd (imm12_from_negated_value x) y)))\n      (sub_imm ty y x))\n\n;; Special cases for when we're adding an extended register where the extending\n;; operation can get folded into the add itself.\n(rule (lower (has_type (fits_in_64 ty) (iadd x (extended_value_from_value y))))\n      (add_extend ty x y))\n\n(rule (lower (has_type (fits_in_64 ty) (iadd (extended_value_from_value x) y)))\n      (add_extend ty y x))\n\n;; Special cases for when we're adding the shift of a different\n;; register by a constant amount and the shift can get folded into the add.\n(rule (lower (has_type (fits_in_64 ty)\n                       (iadd x (ishl y (iconst k)))))\n      (if-let amt (lshl_from_imm64 ty k))\n      (add_shift ty x y amt))\n\n(rule (lower (has_type (fits_in_64 ty)\n                       (iadd (ishl x (iconst k)) y)))\n      (if-let amt (lshl_from_imm64 ty k))\n      (add_shift ty y x amt))\n\n;; Fold an `iadd` and `imul` combination into a `madd` instruction.\n(rule (lower (has_type (fits_in_64 ty) (iadd x (imul y z))))\n      (madd ty y z x))\n\n(rule (lower (has_type (fits_in_64 ty) (iadd (imul x y) z)))\n      (madd ty x y z))\n\n;; Fold an `isub` and `imul` combination into a `msub` instruction.\n(rule (lower (has_type (fits_in_64 ty) (isub x (imul y z))))\n      (msub ty y z x))\n\n;; vectors\n\n(rule (lower (has_type ty @ (multi_lane _ _) (iadd x y)))\n      (add_vec x y (vector_size ty)))\n\n;; `i128`\n(rule (lower (has_type $I128 (iadd x y)))\n      (let\n          ;; Get the high/low registers for `x`.\n          ((x_regs ValueRegs x)\n           (x_lo Reg (value_regs_get x_regs 0))\n           (x_hi Reg (value_regs_get x_regs 1))\n\n           ;; Get the high/low registers for `y`.\n           (y_regs ValueRegs y)\n           (y_lo Reg (value_regs_get y_regs 0))\n           (y_hi Reg (value_regs_get y_regs 1)))\n        ;; the actual addition is `adds` followed by `adc` which comprises the\n        ;; low/high bits of the result\n        (with_flags\n          (add_with_flags_paired $I64 x_lo y_lo)\n          (adc_paired $I64 x_hi y_hi))))\n\n;;;; Rules for `swizzle` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type vec_i128_ty (swizzle rn rm)))\n      (vec_tbl rn rm #f))\n\n;;;; Rules for `isplit` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I64 (isplit x)))\n      (let\n          ((x_regs ValueRegs x)\n           (x_lo ValueRegs (value_regs_get x_regs 0))\n           (x_hi ValueRegs (value_regs_get x_regs 1)))\n        (output_pair x_lo x_hi)))\n\n;;;; Rules for `iconcat` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I128 (iconcat lo hi)))\n      (output (value_regs lo hi)))\n\n;;;; Rules for `scalar_to_vector` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $F32X4 (scalar_to_vector x)))\n      (fpu_extend x (ScalarSize.Size32)))\n\n(rule (lower (has_type $F64X2 (scalar_to_vector x)))\n      (fpu_extend x (ScalarSize.Size64)))\n\n(rule (lower (scalar_to_vector x @ (value_type (ty_int_bool_64 _))))\n      (mov_to_fpu x (ScalarSize.Size64)))\n\n(rule (lower (scalar_to_vector x @ (value_type (int_bool_fits_in_32 _))))\n      (mov_to_fpu (put_in_reg_zext32 x) (ScalarSize.Size32)))\n\n;;;; Rules for `iadd_pairwise` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I16X8 (iadd_pairwise (swiden_low x) (swiden_high y))))\n      (if-let z (same_value x y))\n      (saddlp8 z))\n\n(rule (lower (has_type $I32X4 (iadd_pairwise (swiden_low x) (swiden_high y))))\n      (if-let z (same_value x y))\n      (saddlp16 z))\n\n(rule (lower (has_type $I16X8 (iadd_pairwise (uwiden_low x) (uwiden_high y))))\n      (if-let z (same_value x y))\n      (uaddlp8 z))\n\n(rule (lower (has_type $I32X4 (iadd_pairwise (uwiden_low x) (uwiden_high y))))\n      (if-let z (same_value x y))\n      (uaddlp16 z))\n\n(rule (lower (has_type ty (iadd_pairwise x y)))\n      (addp x y (vector_size ty)))\n\n;;;; Rules for `iabs` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (iabs x)))\n      (vec_abs x (vector_size ty)))\n\n(rule (lower (has_type $I64 (iabs x)))\n      (abs (OperandSize.Size64) x))\n\n(rule (lower (has_type (fits_in_32 ty) (iabs x)))\n      (abs (OperandSize.Size32) (put_in_reg_sext32 x)))\n\n;;;; Rules for `fadd` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fadd rn rm)))\n      (vec_rrr (VecALUOp.Fadd) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fadd rn rm)))\n      (fpu_rrr (FPUOp2.Add) rn rm (scalar_size ty)))\n\n;;;; Rules for `fsub` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fsub rn rm)))\n      (vec_rrr (VecALUOp.Fsub) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fsub rn rm)))\n      (fpu_rrr (FPUOp2.Sub) rn rm (scalar_size ty)))\n\n;;;; Rules for `fmul` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fmul rn rm)))\n      (vec_rrr (VecALUOp.Fmul) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fmul rn rm)))\n      (fpu_rrr (FPUOp2.Mul) rn rm (scalar_size ty)))\n\n;;;; Rules for `fdiv` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fdiv rn rm)))\n      (vec_rrr (VecALUOp.Fdiv) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fdiv rn rm)))\n      (fpu_rrr (FPUOp2.Div) rn rm (scalar_size ty)))\n\n;;;; Rules for `fmin` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fmin rn rm)))\n      (vec_rrr (VecALUOp.Fmin) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fmin rn rm)))\n      (fpu_rrr (FPUOp2.Min) rn rm (scalar_size ty)))\n\n;;;; Rules for `fmax` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fmax rn rm)))\n      (vec_rrr (VecALUOp.Fmax) rn rm (vector_size ty)))\n\n(rule (lower (has_type (ty_scalar_float ty) (fmax rn rm)))\n      (fpu_rrr (FPUOp2.Max) rn rm (scalar_size ty)))\n\n;;;; Rules for `fmin_pseudo` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fmin_pseudo rm rn)))\n      (bsl ty (vec_rrr (VecALUOp.Fcmgt) rm rn (vector_size ty)) rn rm))\n\n(rule (lower (has_type (ty_scalar_float ty) (fmin_pseudo rm rn)))\n      (with_flags (fpu_cmp (scalar_size ty) rm rn)\n                  (fpu_csel ty (Cond.Gt) rn rm)))\n\n;;;; Rules for `fmax_pseudo` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fmax_pseudo rm rn)))\n      (bsl ty (vec_rrr (VecALUOp.Fcmgt) rn rm (vector_size ty)) rn rm))\n\n(rule (lower (has_type (ty_scalar_float ty) (fmax_pseudo rm rn)))\n      (with_flags (fpu_cmp (scalar_size ty) rn rm)\n                  (fpu_csel ty (Cond.Gt) rn rm)))\n\n;;;; Rules for `isub` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; `i64` and smaller\n\n;; Base case, simply subtracting things in registers.\n(rule (lower (has_type (fits_in_64 ty) (isub x y)))\n      (sub ty x y))\n\n;; Special case for when one operand is an immediate that fits in 12 bits.\n(rule (lower (has_type (fits_in_64 ty) (isub x (imm12_from_value y))))\n      (sub_imm ty x y))\n\n;; Same as the previous special case, except we can switch the subtraction to an\n;; addition if the negated immediate fits in 12 bits.\n(rule (lower (has_type (fits_in_64 ty) (isub x (imm12_from_negated_value y))))\n      (add_imm ty x y))\n\n;; Special cases for when we're subtracting an extended register where the\n;; extending operation can get folded into the sub itself.\n(rule (lower (has_type (fits_in_64 ty) (isub x (extended_value_from_value y))))\n      (sub_extend ty x y))\n\n;; Finally a special case for when we're subtracting the shift of a different\n;; register by a constant amount and the shift can get folded into the sub.\n(rule (lower (has_type (fits_in_64 ty)\n                       (isub x (ishl y (iconst k)))))\n      (if-let amt (lshl_from_imm64 ty k))\n      (sub_shift ty x y amt))\n\n;; vectors\n(rule (lower (has_type ty @ (multi_lane _ _) (isub x y)))\n      (sub_vec x y (vector_size ty)))\n\n;; `i128`\n(rule (lower (has_type $I128 (isub x y)))\n      (let\n          ;; Get the high/low registers for `x`.\n          ((x_regs ValueRegs x)\n           (x_lo Reg (value_regs_get x_regs 0))\n           (x_hi Reg (value_regs_get x_regs 1))\n\n           ;; Get the high/low registers for `y`.\n           (y_regs ValueRegs y)\n           (y_lo Reg (value_regs_get y_regs 0))\n           (y_hi Reg (value_regs_get y_regs 1)))\n        ;; the actual subtraction is `subs` followed by `sbc` which comprises\n        ;; the low/high bits of the result\n        (with_flags\n          (sub_with_flags_paired $I64 x_lo y_lo)\n          (sbc_paired $I64 x_hi y_hi))))\n\n;;;; Rules for `uadd_sat` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_vec128 ty) (uadd_sat x y)))\n      (uqadd x y (vector_size ty)))\n\n;;;; Rules for `sadd_sat` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_vec128 ty) (sadd_sat x y)))\n      (sqadd x y (vector_size ty)))\n\n;;;; Rules for `usub_sat` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_vec128 ty) (usub_sat x y)))\n      (uqsub x y (vector_size ty)))\n\n;;;; Rules for `ssub_sat` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_vec128 ty) (ssub_sat x y)))\n      (sqsub x y (vector_size ty)))\n\n;;;; Rules for `ineg` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; `i64` and smaller.\n(rule (lower (has_type (fits_in_64 ty) (ineg x)))\n      (sub ty (zero_reg) x))\n\n;; vectors.\n(rule (lower (has_type (ty_vec128 ty) (ineg x)))\n      (neg x (vector_size ty)))\n\n;;;; Rules for `imul` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; `i64` and smaller.\n(rule (lower (has_type (fits_in_64 ty) (imul x y)))\n      (madd ty x y (zero_reg)))\n\n;; `i128`.\n(rule (lower (has_type $I128 (imul x y)))\n      (let\n          ;; Get the high/low registers for `x`.\n          ((x_regs ValueRegs x)\n           (x_lo Reg (value_regs_get x_regs 0))\n           (x_hi Reg (value_regs_get x_regs 1))\n\n           ;; Get the high/low registers for `y`.\n           (y_regs ValueRegs y)\n           (y_lo Reg (value_regs_get y_regs 0))\n           (y_hi Reg (value_regs_get y_regs 1))\n\n           ;; 128bit mul formula:\n           ;;   dst_lo = x_lo * y_lo\n           ;;   dst_hi = umulhi(x_lo, y_lo) + (x_lo * y_hi) + (x_hi * y_lo)\n           ;;\n           ;; We can convert the above formula into the following\n           ;; umulh   dst_hi, x_lo, y_lo\n           ;; madd    dst_hi, x_lo, y_hi, dst_hi\n           ;; madd    dst_hi, x_hi, y_lo, dst_hi\n           ;; madd    dst_lo, x_lo, y_lo, zero\n           (dst_hi1 Reg (umulh $I64 x_lo y_lo))\n           (dst_hi2 Reg (madd $I64 x_lo y_hi dst_hi1))\n           (dst_hi Reg (madd $I64 x_hi y_lo dst_hi2))\n           (dst_lo Reg (madd $I64 x_lo y_lo (zero_reg))))\n        (value_regs dst_lo dst_hi)))\n\n;; Case for i8x16, i16x8, and i32x4.\n(rule (lower (has_type (ty_vec128 ty @ (not_i64x2)) (imul x y)))\n      (mul x y (vector_size ty)))\n\n;; Special lowering for i64x2.\n;;\n;; This I64X2 multiplication is performed with several 32-bit\n;; operations.\n;;\n;; 64-bit numbers x and y, can be represented as:\n;;   x = a + 2^32(b)\n;;   y = c + 2^32(d)\n;;\n;; A 64-bit multiplication is:\n;;   x * y = ac + 2^32(ad + bc) + 2^64(bd)\n;; note: `2^64(bd)` can be ignored, the value is too large to fit in\n;; 64 bits.\n;;\n;; This sequence implements a I64X2 multiply, where the registers\n;; `rn` and `rm` are split up into 32-bit components:\n;;   rn = |d|c|b|a|\n;;   rm = |h|g|f|e|\n;;\n;;   rn * rm = |cg + 2^32(ch + dg)|ae + 2^32(af + be)|\n;;\n;;  The sequence is:\n;;  rev64 rd.4s, rm.4s\n;;  mul rd.4s, rd.4s, rn.4s\n;;  xtn tmp1.2s, rn.2d\n;;  addp rd.4s, rd.4s, rd.4s\n;;  xtn tmp2.2s, rm.2d\n;;  shll rd.2d, rd.2s, #32\n;;  umlal rd.2d, tmp2.2s, tmp1.2s\n(rule (lower (has_type $I64X2 (imul x y)))\n      (let ((rn Reg x)\n            (rm Reg y)\n            ;; Reverse the 32-bit elements in the 64-bit words.\n            ;;   rd = |g|h|e|f|\n            (rev Reg (rev64 rm (VectorSize.Size32x4)))\n\n            ;; Calculate the high half components.\n            ;;   rd = |dg|ch|be|af|\n            ;;\n            ;; Note that this 32-bit multiply of the high half\n            ;; discards the bits that would overflow, same as\n            ;; if 64-bit operations were used. Also the Shll\n            ;; below would shift out the overflow bits anyway.\n            (mul Reg (mul rev rn (VectorSize.Size32x4)))\n\n            ;; Extract the low half components of rn.\n            ;;   tmp1 = |c|a|\n            (tmp1 Reg (xtn64 rn $false))\n\n            ;; Sum the respective high half components.\n            ;;   rd = |dg+ch|be+af||dg+ch|be+af|\n            (sum Reg (addp mul mul (VectorSize.Size32x4)))\n\n            ;; Extract the low half components of rm.\n            ;;   tmp2 = |g|e|\n            (tmp2 Reg (xtn64 rm $false))\n\n            ;; Shift the high half components, into the high half.\n            ;;   rd = |dg+ch << 32|be+af << 32|\n            (shift Reg (shll32 sum $false))\n\n            ;; Multiply the low components together, and accumulate with the high\n            ;; half.\n            ;;   rd = |rd[1] + cg|rd[0] + ae|\n            (result Reg (umlal32 shift tmp2 tmp1 $false)))\n        result))\n\n;; Special case for `i16x8.extmul_low_i8x16_s`.\n(rule (lower (has_type $I16X8\n                       (imul (swiden_low x @ (value_type $I8X16))\n                             (swiden_low y @ (value_type $I8X16)))))\n      (smull8 x y $false))\n\n;; Special case for `i16x8.extmul_high_i8x16_s`.\n(rule (lower (has_type $I16X8\n                       (imul (swiden_high x @ (value_type $I8X16))\n                             (swiden_high y @ (value_type $I8X16)))))\n      (smull8 x y $true))\n\n;; Special case for `i16x8.extmul_low_i8x16_u`.\n(rule (lower (has_type $I16X8\n                       (imul (uwiden_low x @ (value_type $I8X16))\n                             (uwiden_low y @ (value_type $I8X16)))))\n      (umull8 x y $false))\n\n;; Special case for `i16x8.extmul_high_i8x16_u`.\n(rule (lower (has_type $I16X8\n                       (imul (uwiden_high x @ (value_type $I8X16))\n                             (uwiden_high y @ (value_type $I8X16)))))\n      (umull8 x y $true))\n\n;; Special case for `i32x4.extmul_low_i16x8_s`.\n(rule (lower (has_type $I32X4\n                       (imul (swiden_low x @ (value_type $I16X8))\n                             (swiden_low y @ (value_type $I16X8)))))\n      (smull16 x y $false))\n\n;; Special case for `i32x4.extmul_high_i16x8_s`.\n(rule (lower (has_type $I32X4\n                       (imul (swiden_high x @ (value_type $I16X8))\n                             (swiden_high y @ (value_type $I16X8)))))\n      (smull16 x y $true))\n\n;; Special case for `i32x4.extmul_low_i16x8_u`.\n(rule (lower (has_type $I32X4\n                       (imul (uwiden_low x @ (value_type $I16X8))\n                             (uwiden_low y @ (value_type $I16X8)))))\n      (umull16 x y $false))\n\n;; Special case for `i32x4.extmul_high_i16x8_u`.\n(rule (lower (has_type $I32X4\n                       (imul (uwiden_high x @ (value_type $I16X8))\n                             (uwiden_high y @ (value_type $I16X8)))))\n      (umull16 x y $true))\n\n;; Special case for `i64x2.extmul_low_i32x4_s`.\n(rule (lower (has_type $I64X2\n                       (imul (swiden_low x @ (value_type $I32X4))\n                             (swiden_low y @ (value_type $I32X4)))))\n      (smull32 x y $false))\n\n;; Special case for `i64x2.extmul_high_i32x4_s`.\n(rule (lower (has_type $I64X2\n                       (imul (swiden_high x @ (value_type $I32X4))\n                             (swiden_high y @ (value_type $I32X4)))))\n      (smull32 x y $true))\n\n;; Special case for `i64x2.extmul_low_i32x4_u`.\n(rule (lower (has_type $I64X2\n                       (imul (uwiden_low x @ (value_type $I32X4))\n                             (uwiden_low y @ (value_type $I32X4)))))\n      (umull32 x y $false))\n\n;; Special case for `i64x2.extmul_high_i32x4_u`.\n(rule (lower (has_type $I64X2\n                       (imul (uwiden_high x @ (value_type $I32X4))\n                             (uwiden_high y @ (value_type $I32X4)))))\n      (umull32 x y $true))\n\n;;;; Rules for `smulhi` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I64 (smulhi x y)))\n      (smulh $I64 x y))\n\n(rule (lower (has_type (fits_in_32 ty) (smulhi x y)))\n      (let ((x64 Reg (put_in_reg_sext64 x))\n            (y64 Reg (put_in_reg_sext64 y))\n            (mul Reg (madd $I64 x64 y64 (zero_reg)))\n            (result Reg (asr_imm $I64 mul (imm_shift_from_u8 (ty_bits ty)))))\n        result))\n\n;;;; Rules for `umulhi` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I64 (umulhi x y)))\n      (umulh $I64 x y))\n\n(rule (lower (has_type (fits_in_32 ty) (umulhi x y)))\n      (let (\n          (x64 Reg (put_in_reg_zext64 x))\n          (y64 Reg (put_in_reg_zext64 y))\n          (mul Reg (madd $I64 x64 y64 (zero_reg)))\n          (result Reg (lsr_imm $I64 mul (imm_shift_from_u8 (ty_bits ty))))\n        )\n        (value_reg result)))\n\n;;;; Rules for `udiv` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; TODO: Add UDiv32 to implement 32-bit directly, rather\n;; than extending the input.\n;;\n;; Note that aarch64's `udiv` doesn't trap so to respect the semantics of\n;; CLIF's `udiv` the check for zero needs to be manually performed.\n(rule (lower (has_type (fits_in_64 ty) (udiv x y)))\n      (a64_udiv $I64 (put_in_reg_zext64 x) (put_nonzero_in_reg_zext64 y)))\n\n;; Helper for placing a `Value` into a `Reg` and validating that it's nonzero.\n(decl put_nonzero_in_reg_zext64 (Value) Reg)\n(rule (put_nonzero_in_reg_zext64 val)\n      (trap_if_zero_divisor (put_in_reg_zext64 val)))\n\n;; Special case where if a `Value` is known to be nonzero we can trivially\n;; move it into a register.\n(rule (put_nonzero_in_reg_zext64 (and (value_type ty)\n                                      (iconst (nonzero_u64_from_imm64 n))))\n      (imm ty (ImmExtend.Zero) n))\n\n;;;; Rules for `sdiv` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; TODO: Add SDiv32 to implement 32-bit directly, rather\n;; than extending the input.\n;;\n;; The sequence of checks here should look like:\n;;\n;;   cbnz rm, #8\n;;   udf ; divide by zero\n;;   cmn rm, 1\n;;   ccmp rn, 1, #nzcv, eq\n;;   b.vc #8\n;;   udf ; signed overflow\n;;\n;; Note The div instruction does not trap on divide by zero or overflow, so\n;; checks need to be manually inserted.\n;;\n;; TODO: if `y` is -1 then a check that `x` is not INT_MIN is all that's\n;; necessary, but right now `y` is checked to not be -1 as well.\n(rule (lower (has_type (fits_in_64 ty) (sdiv x y)))\n      (let ((x64 Reg (put_in_reg_sext64 x))\n            (y64 Reg (put_nonzero_in_reg_sext64 y))\n            (valid_x64 Reg (trap_if_div_overflow ty x64 y64))\n            (result Reg (a64_sdiv $I64 valid_x64 y64)))\n        result))\n\n;; Helper for extracting an immediate that's not 0 and not -1 from an imm64.\n(decl safe_divisor_from_imm64 (u64) Imm64)\n(extern extractor safe_divisor_from_imm64 safe_divisor_from_imm64)\n\n;; Special case for `sdiv` where no checks are needed due to division by a\n;; constant meaning the checks are always passed.\n(rule (lower (has_type (fits_in_64 ty) (sdiv x (iconst (safe_divisor_from_imm64 y)))))\n      (a64_sdiv $I64 (put_in_reg_sext64 x) (imm ty (ImmExtend.Sign) y)))\n\n;; Helper for placing a `Value` into a `Reg` and validating that it's nonzero.\n(decl put_nonzero_in_reg_sext64 (Value) Reg)\n(rule (put_nonzero_in_reg_sext64 val)\n      (trap_if_zero_divisor (put_in_reg_sext64 val)))\n\n;; Note that this has a special case where if the `Value` is a constant that's\n;; not zero we can skip the zero check.\n(rule (put_nonzero_in_reg_sext64 (and (value_type ty)\n                                      (iconst (nonzero_u64_from_imm64 n))))\n      (imm ty (ImmExtend.Sign) n))\n\n;;;; Rules for `urem` and `srem` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Remainder (x % y) is implemented as:\n;;\n;;   tmp = x / y\n;;   result = x - (tmp*y)\n;;\n;; use 'result' for tmp and you have:\n;;\n;;   cbnz y, #8         ; branch over trap\n;;   udf                ; divide by zero\n;;   div rd, x, y       ; rd = x / y\n;;   msub rd, rd, y, x  ; rd = x - rd * y\n\n(rule (lower (has_type (fits_in_64 ty) (urem x y)))\n      (let ((x64 Reg (put_in_reg_zext64 x))\n            (y64 Reg (put_nonzero_in_reg_zext64 y))\n            (div Reg (a64_udiv $I64 x64 y64))\n            (result Reg (msub $I64 div y64 x64)))\n        result))\n\n(rule (lower (has_type (fits_in_64 ty) (srem x y)))\n      (let ((x64 Reg (put_in_reg_sext64 x))\n            (y64 Reg (put_nonzero_in_reg_sext64 y))\n            (div Reg (a64_sdiv $I64 x64 y64))\n            (result Reg (msub $I64 div y64 x64)))\n        result))\n\n;;; Rules for integer min/max: umin, imin, umax, imax ;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (not_i64x2) (imin x y)))\n      (vec_rrr (VecALUOp.Smin) x y (vector_size ty)))\n\n(rule (lower (has_type ty @ (not_i64x2) (umin x y)))\n      (vec_rrr (VecALUOp.Umin) x y (vector_size ty)))\n\n(rule (lower (has_type ty @ (not_i64x2) (imax x y)))\n      (vec_rrr (VecALUOp.Smax) x y (vector_size ty)))\n\n(rule (lower (has_type ty @ (not_i64x2) (umax x y)))\n      (vec_rrr (VecALUOp.Umax) x y (vector_size ty)))\n\n;;;; Rules for `uextend` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; General rule for extending input to an output which fits in a single\n;; register.\n(rule (lower (has_type (fits_in_64 out) (uextend x @ (value_type in))))\n      (extend x $false (ty_bits in) (ty_bits out)))\n\n;; Extraction of a vector lane automatically extends as necessary, so we can\n;; skip an explicit extending instruction.\n(rule (lower (has_type (fits_in_64 out)\n                       (uextend (extractlane vec @ (value_type in)\n                                             (u8_from_uimm8 lane)))))\n      (mov_from_vec (put_in_reg vec) lane (vector_size in)))\n\n;; Atomic loads will also automatically zero their upper bits so the `uextend`\n;; instruction can effectively get skipped here.\n(rule (lower (has_type (fits_in_64 out)\n                       (uextend (and (value_type in) (sinkable_atomic_load addr)))))\n      (load_acquire in (sink_atomic_load addr)))\n\n;; Conversion to 128-bit needs a zero-extension of the lower bits and the upper\n;; bits are all zero.\n(rule (lower (has_type $I128 (uextend x)))\n      (value_regs (put_in_reg_zext64 x) (imm $I64 (ImmExtend.Zero) 0)))\n\n;; Like above where vector extraction automatically zero-extends extending to\n;; i128 only requires generating a 0 constant for the upper bits.\n(rule (lower (has_type $I128\n                       (uextend (extractlane vec @ (value_type in)\n                                             (u8_from_uimm8 lane)))))\n      (value_regs (mov_from_vec (put_in_reg vec) lane (vector_size in)) (imm $I64 (ImmExtend.Zero) 0)))\n\n;;;; Rules for `sextend` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; General rule for extending input to an output which fits in a single\n;; register.\n(rule (lower (has_type (fits_in_64 out) (sextend x @ (value_type in))))\n      (extend x $true (ty_bits in) (ty_bits out)))\n\n;; Extraction of a vector lane automatically extends as necessary, so we can\n;; skip an explicit extending instruction.\n(rule (lower (has_type (fits_in_64 out)\n                       (sextend (extractlane vec @ (value_type in)\n                                             (u8_from_uimm8 lane)))))\n      (mov_from_vec_signed (put_in_reg vec)\n                           lane\n                           (vector_size in)\n                           (size_from_ty out)))\n\n;; 64-bit to 128-bit only needs to sign-extend the input to the upper bits.\n(rule (lower (has_type $I128 (sextend x)))\n      (let ((lo Reg (put_in_reg_sext64 x))\n            (hi Reg (asr_imm $I64 lo (imm_shift_from_u8 63))))\n        (value_regs lo hi)))\n\n;; Like above where vector extraction automatically zero-extends extending to\n;; i128 only requires generating a 0 constant for the upper bits.\n;;\n;; Note that `mov_from_vec_signed` doesn't exist for i64x2, so that's\n;; specifically excluded here.\n(rule (lower (has_type $I128\n                       (sextend (extractlane vec @ (value_type in @ (not_i64x2))\n                                             (u8_from_uimm8 lane)))))\n      (let ((lo Reg (mov_from_vec_signed (put_in_reg vec)\n                                         lane\n                                         (vector_size in)\n                                         (size_from_ty $I64)))\n            (hi Reg (asr_imm $I64 lo (imm_shift_from_u8 63))))\n        (value_regs lo hi)))\n\n;; Extension from an extraction of i64x2 into i128.\n(rule (lower (has_type $I128\n                       (sextend (extractlane vec @ (value_type $I64X2)\n                                             (u8_from_uimm8 lane)))))\n      (let ((lo Reg (mov_from_vec (put_in_reg vec)\n                                  lane\n                                  (VectorSize.Size64x2)))\n            (hi Reg (asr_imm $I64 lo (imm_shift_from_u8 63))))\n        (value_regs lo hi)))\n\n;;;; Rules for `bnot` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Base case using `orn` between two registers.\n;;\n;; Note that bitwise negation is implemented here as\n;;\n;;      NOT rd, rm ==> ORR_NOT rd, zero, rm\n(rule (lower (has_type (fits_in_64 ty) (bnot x)))\n      (orr_not ty (zero_reg) x))\n\n;; Special case to use `orr_not_shift` if it's a `bnot` of a const-left-shifted\n;; value.\n(rule (lower (has_type (fits_in_64 ty)\n                       (bnot (ishl x (iconst k)))))\n      (if-let amt (lshl_from_imm64 ty k))\n      (orr_not_shift ty (zero_reg) x amt))\n\n;; Implementation of `bnot` for `i128`.\n(rule (lower (has_type $I128 (bnot x)))\n      (let ((x_regs ValueRegs x)\n            (x_lo Reg (value_regs_get x_regs 0))\n            (x_hi Reg (value_regs_get x_regs 1))\n            (new_lo Reg (orr_not $I64 (zero_reg) x_lo))\n            (new_hi Reg (orr_not $I64 (zero_reg) x_hi)))\n        (value_regs new_lo new_hi)))\n\n;; Implementation of `bnot` for vector types.\n(rule (lower (has_type (ty_vec128 ty) (bnot x)))\n      (not x (vector_size ty)))\n\n;;;; Rules for `band` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (band x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.And) ty x y))\n\n(rule (lower (has_type $I64 (band x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.And) $I64 x y))\n\n(rule (lower (has_type $I128 (band x y))) (i128_alu_bitop (ALUOp.And) $I64 x y))\n\n(rule (lower (has_type (ty_vec128 ty) (band x y)))\n      (and_vec x y (vector_size ty)))\n\n;;;; Rules for `bor` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (bor x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.Orr) ty x y))\n\n(rule (lower (has_type $I64 (bor x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.Orr) $I64 x y))\n\n(rule (lower (has_type $I128 (bor x y))) (i128_alu_bitop (ALUOp.Orr) $I64 x y))\n\n(rule (lower (has_type (ty_vec128 ty) (bor x y)))\n      (orr_vec x y (vector_size ty)))\n\n;;;; Rules for `bxor` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (bxor x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.Eor) ty x y))\n\n(rule (lower (has_type $I64 (bxor x y)))\n      (alu_rs_imm_logic_commutative (ALUOp.Eor) $I64 x y))\n\n(rule (lower (has_type $I128 (bxor x y))) (i128_alu_bitop (ALUOp.Eor) $I64 x y))\n\n(rule (lower (has_type (ty_vec128 ty) (bxor x y)))\n      (eor_vec x y (vector_size ty)))\n\n;;;; Rules for `band_not` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (band_not x y)))\n      (alu_rs_imm_logic (ALUOp.AndNot) ty x y))\n\n(rule (lower (has_type $I64 (band_not x y)))\n      (alu_rs_imm_logic (ALUOp.AndNot) $I64 x y))\n\n(rule (lower (has_type $I128 (band_not x y))) (i128_alu_bitop (ALUOp.AndNot) $I64 x y))\n\n(rule (lower (has_type (ty_vec128 ty) (band_not x y)))\n      (bic_vec x y (vector_size ty)))\n\n;;;; Rules for `bor_not` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (bor_not x y)))\n      (alu_rs_imm_logic (ALUOp.OrrNot) ty x y))\n\n(rule (lower (has_type $I64 (bor_not x y)))\n      (alu_rs_imm_logic (ALUOp.OrrNot) $I64 x y))\n\n(rule (lower (has_type $I128 (bor_not x y))) (i128_alu_bitop (ALUOp.OrrNot) $I64 x y))\n\n;;;; Rules for `bxor_not` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (fits_in_32 ty) (bxor_not x y)))\n      (alu_rs_imm_logic (ALUOp.EorNot) $I32 x y))\n\n(rule (lower (has_type $I64 (bxor_not x y)))\n      (alu_rs_imm_logic (ALUOp.EorNot) $I64 x y))\n\n(rule (lower (has_type $I128 (bxor_not x y))) (i128_alu_bitop (ALUOp.EorNot) $I64 x y))\n\n;;;; Rules for `ishl` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Shift for i8/i16/i32.\n(rule (lower (has_type (fits_in_32 ty) (ishl x y)))\n      (do_shift (ALUOp.Lsl) ty x y))\n\n;; Shift for i64.\n(rule (lower (has_type $I64 (ishl x y)))\n      (do_shift (ALUOp.Lsl) $I64 x y))\n\n;; Shift for i128.\n(rule (lower (has_type $I128 (ishl x y)))\n      (lower_shl128 x (value_regs_get y 0)))\n\n;;     lsl     lo_lshift, src_lo, amt\n;;     lsl     hi_lshift, src_hi, amt\n;;     mvn     inv_amt, amt\n;;     lsr     lo_rshift, src_lo, #1\n;;     lsr     lo_rshift, lo_rshift, inv_amt\n;;     orr     maybe_hi, hi_lshift, lo_rshift\n;;     tst     amt, #0x40\n;;     csel    dst_hi, lo_lshift, maybe_hi, ne\n;;     csel    dst_lo, xzr, lo_lshift, ne\n(decl lower_shl128 (ValueRegs Reg) ValueRegs)\n(rule (lower_shl128 src amt)\n      (let ((src_lo Reg (value_regs_get src 0))\n            (src_hi Reg (value_regs_get src 1))\n            (lo_lshift Reg (lsl $I64 src_lo amt))\n            (hi_lshift Reg (lsl $I64 src_hi amt))\n            (inv_amt Reg (orr_not $I32 (zero_reg) amt))\n            (lo_rshift Reg (lsr $I64 (lsr_imm $I64 src_lo (imm_shift_from_u8 1))\n                                inv_amt))\n          (maybe_hi Reg (orr $I64 hi_lshift lo_rshift))\n        )\n        (with_flags\n         (tst_imm $I64 amt (u64_into_imm_logic $I64 64))\n         (consumes_flags_concat\n          (csel (Cond.Ne) (zero_reg) lo_lshift)\n          (csel (Cond.Ne) lo_lshift maybe_hi)))))\n\n;; Shift for vector types.\n(rule (lower (has_type (ty_vec128 ty) (ishl x y)))\n      (let ((size VectorSize (vector_size ty))\n            (shift Reg (vec_dup y size)))\n        (sshl x shift size)))\n\n;; Helper function to emit a shift operation with the opcode specified and\n;; the output type specified. The `Reg` provided is shifted by the `Value`\n;; given.\n;;\n;; Note that this automatically handles the clif semantics of masking the\n;; shift amount where necessary.\n(decl do_shift (ALUOp Type Reg Value) Reg)\n\n;; 8/16-bit shift base case.\n;;\n;; When shifting for amounts larger than the size of the type, the CLIF shift\n;; instructions implement a \"wrapping\" behaviour, such that an i8 << 8 is\n;; equivalent to i8 << 0\n;;\n;; On i32 and i64 types this matches what the aarch64 spec does, but on smaller\n;; types (i16, i8) we need to do this manually, so we wrap the shift amount\n;; with an AND instruction\n(rule (do_shift op (fits_in_16 ty) x y)\n      (let ((shift_amt Reg (value_regs_get y 0))\n            (masked_shift_amt Reg (and_imm $I32 shift_amt (shift_mask ty))))\n        (alu_rrr op $I32 x masked_shift_amt)))\n\n(decl shift_mask (Type) ImmLogic)\n(extern constructor shift_mask shift_mask)\n\n;; 32/64-bit shift base cases.\n(rule (do_shift op $I32 x y) (alu_rrr op $I32 x (value_regs_get y 0)))\n(rule (do_shift op $I64 x y) (alu_rrr op $I64 x (value_regs_get y 0)))\n\n;; Special case for shifting by a constant value where the value can fit into an\n;; `ImmShift`.\n;;\n;; Note that this rule explicitly has a higher priority than the others\n;; to ensure it's attempted first, otherwise the type-based filters on the\n;; previous rules seem to take priority over this rule.\n(rule 1 (do_shift op ty x (iconst k))\n      (if-let shift (imm_shift_from_imm64 ty k))\n      (alu_rr_imm_shift op ty x shift))\n\n;;;; Rules for `ushr` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Shift for i8/i16/i32.\n(rule (lower (has_type (fits_in_32 ty) (ushr x y)))\n      (do_shift (ALUOp.Lsr) ty (put_in_reg_zext32 x) y))\n\n;; Shift for i64.\n(rule (lower (has_type $I64 (ushr x y)))\n      (do_shift (ALUOp.Lsr) $I64 (put_in_reg_zext64 x) y))\n\n;; Shift for i128.\n(rule (lower (has_type $I128 (ushr x y)))\n      (lower_ushr128 x (value_regs_get y 0)))\n\n;; Vector shifts.\n(rule (lower (has_type (ty_vec128 ty) (ushr x y)))\n      (let ((size VectorSize (vector_size ty))\n            (shift Reg (vec_dup (sub $I32 (zero_reg) y) size)))\n        (ushl x shift size)))\n\n;;     lsr       lo_rshift, src_lo, amt\n;;     lsr       hi_rshift, src_hi, amt\n;;     mvn       inv_amt, amt\n;;     lsl       hi_lshift, src_hi, #1\n;;     lsl       hi_lshift, hi_lshift, inv_amt\n;;     tst       amt, #0x40\n;;     orr       maybe_lo, lo_rshift, hi_lshift\n;;     csel      dst_hi, xzr, hi_rshift, ne\n;;     csel      dst_lo, hi_rshift, maybe_lo, ne\n(decl lower_ushr128 (ValueRegs Reg) ValueRegs)\n(rule (lower_ushr128 src amt)\n      (let ((src_lo Reg (value_regs_get src 0))\n            (src_hi Reg (value_regs_get src 1))\n            (lo_rshift Reg (lsr $I64 src_lo amt))\n            (hi_rshift Reg (lsr $I64 src_hi amt))\n\n            (inv_amt Reg (orr_not $I32 (zero_reg) amt))\n            (hi_lshift Reg (lsl $I64 (lsl_imm $I64 src_hi (imm_shift_from_u8 1))\n                                inv_amt))\n          (maybe_lo Reg (orr $I64 lo_rshift hi_lshift))\n        )\n        (with_flags\n         (tst_imm $I64 amt (u64_into_imm_logic $I64 64))\n         (consumes_flags_concat\n          (csel (Cond.Ne) hi_rshift maybe_lo)\n          (csel (Cond.Ne) (zero_reg) hi_rshift)))))\n\n;;;; Rules for `sshr` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Shift for i8/i16/i32.\n(rule (lower (has_type (fits_in_32 ty) (sshr x y)))\n      (do_shift (ALUOp.Asr) ty (put_in_reg_sext32 x) y))\n\n;; Shift for i64.\n(rule (lower (has_type $I64 (sshr x y)))\n      (do_shift (ALUOp.Asr) $I64 (put_in_reg_sext64 x) y))\n\n;; Shift for i128.\n(rule (lower (has_type $I128 (sshr x y)))\n      (lower_sshr128 x (value_regs_get y 0)))\n\n;; Vector shifts.\n;;\n;; Note that right shifts are implemented with a negative left shift.\n(rule (lower (has_type (ty_vec128 ty) (sshr x y)))\n      (let ((size VectorSize (vector_size ty))\n            (shift Reg (vec_dup (sub $I32 (zero_reg) y) size)))\n        (sshl x shift size)))\n\n;;     lsr       lo_rshift, src_lo, amt\n;;     asr       hi_rshift, src_hi, amt\n;;     mvn       inv_amt, amt\n;;     lsl       hi_lshift, src_hi, #1\n;;     lsl       hi_lshift, hi_lshift, inv_amt\n;;     asr       hi_sign, src_hi, #63\n;;     orr       maybe_lo, lo_rshift, hi_lshift\n;;     tst       amt, #0x40\n;;     csel      dst_hi, hi_sign, hi_rshift, ne\n;;     csel      dst_lo, hi_rshift, maybe_lo, ne\n(decl lower_sshr128 (ValueRegs Reg) ValueRegs)\n(rule (lower_sshr128 src amt)\n      (let ((src_lo Reg (value_regs_get src 0))\n            (src_hi Reg (value_regs_get src 1))\n            (lo_rshift Reg (lsr $I64 src_lo amt))\n            (hi_rshift Reg (asr $I64 src_hi amt))\n\n            (inv_amt Reg (orr_not $I32 (zero_reg) amt))\n            (hi_lshift Reg (lsl $I64 (lsl_imm $I64 src_hi (imm_shift_from_u8 1))\n                                inv_amt))\n          (hi_sign Reg (asr_imm $I64 src_hi (imm_shift_from_u8 63)))\n          (maybe_lo Reg (orr $I64 lo_rshift hi_lshift))\n        )\n        (with_flags\n         (tst_imm $I64 amt (u64_into_imm_logic $I64 64))\n         (consumes_flags_concat\n          (csel (Cond.Ne) hi_rshift maybe_lo)\n          (csel (Cond.Ne) hi_sign hi_rshift)))))\n\n;;;; Rules for `rotl` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; General 8/16-bit case.\n(rule (lower (has_type (fits_in_16 ty) (rotl x y)))\n      (let ((neg_shift Reg (sub $I32 (zero_reg) y)))\n        (small_rotr ty (put_in_reg_zext32 x) neg_shift)))\n\n;; Specialization for the 8/16-bit case when the rotation amount is an immediate.\n(rule (lower (has_type (fits_in_16 ty) (rotl x (iconst k))))\n      (if-let n (imm_shift_from_imm64 ty k))\n      (small_rotr_imm ty (put_in_reg_zext32 x) (negate_imm_shift ty n)))\n\n;; aarch64 doesn't have a left-rotate instruction, but a left rotation of K\n;; places is effectively a right rotation of N - K places, if N is the integer's\n;; bit size. We implement left rotations with this trick.\n;;\n;; Note that when negating the shift amount here the upper bits are ignored\n;; by the rotr instruction, meaning that we'll still left-shift by the desired\n;; amount.\n\n;; General 32-bit case.\n(rule (lower (has_type $I32 (rotl x y)))\n      (let ((neg_shift Reg (sub $I32 (zero_reg) y)))\n        (a64_rotr $I32 x neg_shift)))\n\n;; General 64-bit case.\n(rule (lower (has_type $I64 (rotl x y)))\n      (let ((neg_shift Reg (sub $I64 (zero_reg) y)))\n        (a64_rotr $I64 x neg_shift)))\n\n;; Specialization for the 32-bit case when the rotation amount is an immediate.\n(rule (lower (has_type $I32 (rotl x (iconst k))))\n      (if-let n (imm_shift_from_imm64 $I32 k))\n      (a64_rotr_imm $I32 x (negate_imm_shift $I32 n)))\n\n;; Specialization for the 64-bit case when the rotation amount is an immediate.\n(rule (lower (has_type $I64 (rotl x (iconst k))))\n      (if-let n (imm_shift_from_imm64 $I64 k))\n      (a64_rotr_imm $I64 x (negate_imm_shift $I64 n)))\n\n(decl negate_imm_shift (Type ImmShift) ImmShift)\n(extern constructor negate_imm_shift negate_imm_shift)\n\n;; General 128-bit case.\n;;\n;; TODO: much better codegen is possible with a constant amount.\n(rule (lower (has_type $I128 (rotl x y)))\n      (let ((val ValueRegs x)\n            (amt Reg (value_regs_get y 0))\n            (neg_amt Reg (sub $I64 (imm $I64 (ImmExtend.Zero) 128) amt))\n            (lshift ValueRegs (lower_shl128 val amt))\n            (rshift ValueRegs (lower_ushr128 val neg_amt)))\n        (value_regs\n          (orr $I64 (value_regs_get lshift 0) (value_regs_get rshift 0))\n          (orr $I64 (value_regs_get lshift 1) (value_regs_get rshift 1)))))\n\n;;;; Rules for `rotr` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; General 8/16-bit case.\n(rule (lower (has_type (fits_in_16 ty) (rotr x y)))\n      (small_rotr ty (put_in_reg_zext32 x) y))\n\n;; General 32-bit case.\n(rule (lower (has_type $I32 (rotr x y)))\n      (a64_rotr $I32 x y))\n\n;; General 64-bit case.\n(rule (lower (has_type $I64 (rotr x y)))\n      (a64_rotr $I64 x y))\n\n;; Specialization for the 8/16-bit case when the rotation amount is an immediate.\n(rule (lower (has_type (fits_in_16 ty) (rotr x (iconst k))))\n      (if-let n (imm_shift_from_imm64 ty k))\n      (small_rotr_imm ty (put_in_reg_zext32 x) n))\n\n;; Specialization for the 32-bit case when the rotation amount is an immediate.\n(rule (lower (has_type $I32 (rotr x (iconst k))))\n      (if-let n (imm_shift_from_imm64 $I32 k))\n      (a64_rotr_imm $I32 x n))\n\n;; Specialization for the 64-bit case when the rotation amount is an immediate.\n(rule (lower (has_type $I64 (rotr x (iconst k))))\n      (if-let n (imm_shift_from_imm64 $I64 k))\n      (a64_rotr_imm $I64 x n))\n\n;; For a < 32-bit rotate-right, we synthesize this as:\n;;\n;;    rotr rd, val, amt\n;;\n;;       =>\n;;\n;;    and masked_amt, amt, <bitwidth - 1>\n;;    sub tmp_sub, masked_amt, <bitwidth>\n;;    sub neg_amt, zero, tmp_sub  ; neg\n;;    lsr val_rshift, val, masked_amt\n;;    lsl val_lshift, val, neg_amt\n;;    orr rd, val_lshift val_rshift\n(decl small_rotr (Type Reg Reg) Reg)\n(rule (small_rotr ty val amt)\n      (let ((masked_amt Reg (and_imm $I32 amt (rotr_mask ty)))\n            (tmp_sub Reg (sub_imm $I32 masked_amt (u8_into_imm12 (ty_bits ty))))\n            (neg_amt Reg (sub $I32 (zero_reg) tmp_sub))\n            (val_rshift Reg (lsr $I32 val masked_amt))\n            (val_lshift Reg (lsl $I32 val neg_amt)))\n        (orr $I32 val_lshift val_rshift)))\n\n(decl rotr_mask (Type) ImmLogic)\n(extern constructor rotr_mask rotr_mask)\n\n;; For a constant amount, we can instead do:\n;;\n;;    rotr rd, val, #amt\n;;\n;;       =>\n;;\n;;    lsr val_rshift, val, #<amt>\n;;    lsl val_lshift, val, <bitwidth - amt>\n;;    orr rd, val_lshift, val_rshift\n(decl small_rotr_imm (Type Reg ImmShift) Reg)\n(rule (small_rotr_imm ty val amt)\n      (let ((val_rshift Reg (lsr_imm $I32 val amt))\n            (val_lshift Reg (lsl_imm $I32 val (rotr_opposite_amount ty amt))))\n        (orr $I32 val_lshift val_rshift)))\n\n(decl rotr_opposite_amount (Type ImmShift) ImmShift)\n(extern constructor rotr_opposite_amount rotr_opposite_amount)\n\n;; General 128-bit case.\n;;\n;; TODO: much better codegen is possible with a constant amount.\n(rule (lower (has_type $I128 (rotr x y)))\n      (let ((val ValueRegs x)\n            (amt Reg (value_regs_get y 0))\n            (neg_amt Reg (sub $I64 (imm $I64 (ImmExtend.Zero) 128) amt))\n            (rshift ValueRegs (lower_ushr128 val amt))\n            (lshift ValueRegs (lower_shl128 val neg_amt))\n            (hi Reg (orr $I64 (value_regs_get rshift 1) (value_regs_get lshift 1)))\n            (lo Reg (orr $I64 (value_regs_get rshift 0) (value_regs_get lshift 0))))\n        (value_regs lo hi)))\n\n;;;; Rules for `bitrev` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Reversing an 8-bit value with a 32-bit bitrev instruction will place\n;; the reversed result in the highest 8 bits, so we need to shift them down into\n;; place.\n(rule (lower (has_type $I8 (bitrev x)))\n      (lsr_imm $I32 (rbit $I32 x) (imm_shift_from_u8 24)))\n\n;; Reversing an 16-bit value with a 32-bit bitrev instruction will place\n;; the reversed result in the highest 16 bits, so we need to shift them down into\n;; place.\n(rule (lower (has_type $I16 (bitrev x)))\n      (lsr_imm $I32 (rbit $I32 x) (imm_shift_from_u8 16)))\n\n(rule (lower (has_type $I128 (bitrev x)))\n      (let ((val ValueRegs x)\n            (lo_rev Reg (rbit $I64 (value_regs_get val 0)))\n            (hi_rev Reg (rbit $I64 (value_regs_get val 1))))\n        (value_regs hi_rev lo_rev)))\n\n(rule (lower (has_type ty (bitrev x)))\n      (rbit ty x))\n\n\n;;;; Rules for `clz` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I8 (clz x)))\n      (sub_imm $I32 (a64_clz $I32 (put_in_reg_zext32 x)) (u8_into_imm12 24)))\n\n(rule (lower (has_type $I16 (clz x)))\n      (sub_imm $I32 (a64_clz $I32 (put_in_reg_zext32 x)) (u8_into_imm12 16)))\n\n(rule (lower (has_type $I128 (clz x)))\n      (lower_clz128 x))\n\n(rule (lower (has_type ty (clz x)))\n      (a64_clz ty x))\n\n;; clz hi_clz, hi\n;; clz lo_clz, lo\n;; lsr tmp, hi_clz, #6\n;; madd dst_lo, lo_clz, tmp, hi_clz\n;; mov  dst_hi, 0\n(decl lower_clz128 (ValueRegs) ValueRegs)\n(rule (lower_clz128 val)\n      (let ((hi_clz Reg (a64_clz $I64 (value_regs_get val 1)))\n            (lo_clz Reg (a64_clz $I64 (value_regs_get val 0)))\n            (tmp Reg (lsr_imm $I64 hi_clz (imm_shift_from_u8 6))))\n        (value_regs (madd $I64 lo_clz tmp hi_clz) (imm $I64 (ImmExtend.Zero) 0))))\n\n;;;; Rules for `ctz` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Note that all `ctz` instructions are implemented by reversing the bits and\n;; then using a `clz` instruction since the tail zeros are the same as the\n;; leading zeros of the reversed value.\n\n(rule (lower (has_type $I8 (ctz x)))\n      (a64_clz $I32 (orr_imm $I32 (rbit $I32 x) (u64_into_imm_logic $I32 0x800000))))\n\n(rule (lower (has_type $I16 (ctz x)))\n      (a64_clz $I32 (orr_imm $I32 (rbit $I32 x) (u64_into_imm_logic $I32 0x8000))))\n\n(rule (lower (has_type $I128 (ctz x)))\n      (let ((val ValueRegs x)\n            (lo Reg (rbit $I64 (value_regs_get val 0)))\n            (hi Reg (rbit $I64 (value_regs_get val 1))))\n        (lower_clz128 (value_regs hi lo))))\n\n(rule (lower (has_type ty (ctz x)))\n      (a64_clz ty (rbit ty x)))\n\n;;;; Rules for `cls` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type $I8 (cls x)))\n      (sub_imm $I32 (a64_cls $I32 (put_in_reg_sext32 x)) (u8_into_imm12 24)))\n\n(rule (lower (has_type $I16 (cls x)))\n      (sub_imm $I32 (a64_cls $I32 (put_in_reg_sext32 x)) (u8_into_imm12 16)))\n\n;; cls lo_cls, lo\n;; cls hi_cls, hi\n;; eon sign_eq_eor, hi, lo\n;; lsr sign_eq, sign_eq_eor, #63\n;; madd lo_sign_bits, out_lo, sign_eq, sign_eq\n;; cmp hi_cls, #63\n;; csel maybe_lo, lo_sign_bits, xzr, eq\n;; add  out_lo, maybe_lo, hi_cls\n;; mov  out_hi, 0\n(rule (lower (has_type $I128 (cls x)))\n      (let ((val ValueRegs x)\n            (lo Reg (value_regs_get val 0))\n            (hi Reg (value_regs_get val 1))\n            (lo_cls Reg (a64_cls $I64 lo))\n            (hi_cls Reg (a64_cls $I64 hi))\n            (sign_eq_eon Reg (eon $I64 hi lo))\n            (sign_eq Reg (lsr_imm $I64 sign_eq_eon (imm_shift_from_u8 63)))\n            (lo_sign_bits Reg (madd $I64 lo_cls sign_eq sign_eq))\n            (maybe_lo Reg (with_flags_reg\n                           (cmp64_imm hi_cls (u8_into_imm12 63))\n                           (csel (Cond.Eq) lo_sign_bits (zero_reg)))))\n        (value_regs (add $I64 maybe_lo hi_cls) (imm $I64 (ImmExtend.Zero) 0))))\n\n(rule (lower (has_type ty (cls x)))\n      (a64_cls ty x))\n\n;;;; Rules for `bint` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Booleans are stored as all-zeroes (0) or all-ones (-1). We AND\n;; out the LSB to give a 0 / 1-valued integer result.\n\n(rule (lower (has_type $I128 (bint x)))\n      (let ((val ValueRegs x)\n            (in_lo Reg (value_regs_get val 0))\n            (dst_lo Reg (and_imm $I32 in_lo (u64_into_imm_logic $I32 1)))\n            (dst_hi Reg (imm $I64 (ImmExtend.Zero) 0)))\n        (value_regs dst_lo dst_hi)))\n\n(rule (lower (bint x))\n      (and_imm $I32 x (u64_into_imm_logic $I32 1)))\n\n;;;; Rules for `bmask`/`bextend` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; Bextend and Bmask both simply sign-extend. This works for:\n;; - Bextend, because booleans are stored as 0 / -1, so we\n;;   sign-extend the -1 to a -1 in the wider width.\n;; - Bmask, because the resulting integer mask value must be\n;;   all-ones (-1) if the argument is true.\n\n;; Use a common helper to type cast bools to either bool or integer types.\n(decl cast_bool (Type Type Value) InstOutput)\n(rule (lower (has_type out_ty (bextend x @ (value_type in_ty))))\n      (cast_bool in_ty out_ty x))\n(rule (lower (has_type out_ty (bmask x @ (value_type in_ty))))\n      (cast_bool in_ty out_ty x))\n\n\n;; If the target has the same or a smaller size than the source, it's a no-op.\n(rule (cast_bool $B8 $I8 x) x)\n(rule (cast_bool $B16 (fits_in_16 _out) x) x)\n(rule (cast_bool $B32 (fits_in_32 _out) x) x)\n(rule (cast_bool $B64 (fits_in_64 _out) x) x)\n\n;; Casting between 128 bits is a noop\n(rule (cast_bool (ty_int_bool_128 _in) (ty_int_bool_128 _out) x)\n    x)\n\n;; Converting from 128 bits to anything below we just ignore the top register\n(rule (cast_bool (ty_int_bool_128 _in) (fits_in_64 _out) x)\n    (value_regs_get x 0))\n\n;; Extend to 64 bits first, then this will be all 0s or all 1s and we can\n;; duplicate to both halves of 128 bits\n(rule (cast_bool in (ty_int_bool_128 _out) x)\n      (let ((tmp Reg (extend x $true (ty_bits in) 64)))\n        (value_regs tmp tmp)))\n\n;; Values that fit in a single register are sign extended normally\n(rule (cast_bool (fits_in_64 in) (fits_in_64 out) x)\n      (extend x $true (ty_bits in) (ty_bits out)))\n\n;;;; Rules for `popcnt` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; The implementation of `popcnt` for scalar types is done by moving the value\n;; into a vector register, using the `cnt` instruction, and then collating the\n;; result back into a normal register.\n;;\n;; The general sequence emitted here is\n;;\n;;     fmov tmp, in_lo\n;;     if ty == i128:\n;;         mov tmp.d[1], in_hi\n;;\n;;     cnt tmp.16b, tmp.16b / cnt tmp.8b, tmp.8b\n;;     addv tmp, tmp.16b / addv tmp, tmp.8b / addp tmp.8b, tmp.8b, tmp.8b / (no instruction for 8-bit inputs)\n;;\n;;     umov out_lo, tmp.b[0]\n;;     if ty == i128:\n;;         mov out_hi, 0\n\n(rule (lower (has_type $I8 (popcnt x)))\n      (let ((tmp Reg (mov_to_fpu x (ScalarSize.Size32)))\n            (nbits Reg (vec_cnt tmp (VectorSize.Size8x8))))\n        (mov_from_vec nbits 0 (VectorSize.Size8x16))))\n\n;; Note that this uses `addp` instead of `addv` as it's usually cheaper.\n(rule (lower (has_type $I16 (popcnt x)))\n      (let ((tmp Reg (mov_to_fpu x (ScalarSize.Size32)))\n            (nbits Reg (vec_cnt tmp (VectorSize.Size8x8)))\n            (added Reg (addp nbits nbits (VectorSize.Size8x8))))\n        (mov_from_vec added 0 (VectorSize.Size8x16))))\n\n(rule (lower (has_type $I32 (popcnt x)))\n      (let ((tmp Reg (mov_to_fpu x (ScalarSize.Size32)))\n            (nbits Reg (vec_cnt tmp (VectorSize.Size8x8)))\n            (added Reg (addv nbits (VectorSize.Size8x8))))\n        (mov_from_vec added 0 (VectorSize.Size8x16))))\n\n(rule (lower (has_type $I64 (popcnt x)))\n      (let ((tmp Reg (mov_to_fpu x (ScalarSize.Size64)))\n            (nbits Reg (vec_cnt tmp (VectorSize.Size8x8)))\n            (added Reg (addv nbits (VectorSize.Size8x8))))\n        (mov_from_vec added 0 (VectorSize.Size8x16))))\n\n(rule (lower (has_type $I128 (popcnt x)))\n      (let ((val ValueRegs x)\n            (tmp_half Reg (mov_to_fpu (value_regs_get val 0) (ScalarSize.Size64)))\n            (tmp Reg (mov_to_vec tmp_half (value_regs_get val 1) 1 (VectorSize.Size64x2)))\n            (nbits Reg (vec_cnt tmp (VectorSize.Size8x16)))\n            (added Reg (addv nbits (VectorSize.Size8x16))))\n        (value_regs (mov_from_vec added 0 (VectorSize.Size8x16)) (imm $I64 (ImmExtend.Zero) 0))))\n\n(rule (lower (has_type $I8X16 (popcnt x)))\n      (vec_cnt x (VectorSize.Size8x16)))\n\n;;;; Rules for `bitselect` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_int_bool_ref_scalar_64 ty) (bitselect c x y)))\n      (let ((tmp1 Reg (and_reg ty x c))\n            (tmp2 Reg (bic ty y c)))\n        (orr ty tmp1 tmp2)))\n\n(rule (lower (has_type (ty_vec128 ty) (bitselect c x y)))\n        (bsl ty c x y))\n\n;;;; Rules for `vselect` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type (ty_vec128 ty) (vselect c x y)))\n        (bsl ty c x y))\n\n;;;; Rules for `ireduce` / `breduce` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n;; T -> I{64,32,16,8}: We can simply pass through the value: values\n;; are always stored with high bits undefined, so we can just leave\n;; them be.\n(rule (lower (has_type (ty_int_bool_ref_scalar_64 ty) (ireduce src)))\n    (value_regs_get src 0))\n\n;; Likewise for breduce.\n\n(rule (lower (has_type (ty_int_bool_ref_scalar_64 ty) (breduce src)))\n      (value_regs_get src 0))\n\n\n;;;; Rules for `fcmp` 32 bit ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond_not_eq cond) x (splat (f32const (zero_value_f32 y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (fcmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond cond) x (splat (f32const (zero_value_f32 y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (float_cmp_zero cond rn vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond_not_eq cond) (splat (f32const (zero_value_f32 x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (fcmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond cond) (splat (f32const (zero_value_f32 x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (float_cmp_zero_swap cond rn vec_size))))\n\n;;;; Rules for `fcmp` 64 bit ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond_not_eq cond) x (splat (f64const (zero_value_f64 y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (fcmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond cond) x (splat (f64const (zero_value_f64 y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (float_cmp_zero cond rn vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond_not_eq cond) (splat (f64const (zero_value_f64 x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (fcmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond cond) (splat (f64const (zero_value_f64 x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (float_cmp_zero_swap cond rn vec_size))))\n\n;;;; Rules for `icmp` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond_not_eq cond) x (splat (iconst (zero_value y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (cmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond cond) x (splat (iconst (zero_value y))))))\n      (let ((rn Reg x)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (int_cmp_zero cond rn vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond_not_eq cond) (splat (iconst (zero_value x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (not (cmeq0 rn vec_size) vec_size))))\n\n(rule (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond cond) (splat (iconst (zero_value x))) y)))\n      (let ((rn Reg y)\n            (vec_size VectorSize (vector_size ty)))\n          (value_reg (int_cmp_zero_swap cond rn vec_size))))\n\n;;;; Rules for `trap` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (trap trap_code))\n      (let ((use_allocated_encoding bool (is_not_baldrdash_call_conv)))\n         (side_effect (udf use_allocated_encoding trap_code))))\n\n;;;; Rules for `resumable_trap` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule (lower (resumable_trap trap_code))\n      (let ((use_allocated_encoding bool (is_not_baldrdash_call_conv)))\n         (side_effect (udf use_allocated_encoding trap_code))))\n\n\n;;;; Rules for `AtomicLoad` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n(rule (lower (has_type (valid_atomic_transaction ty) (atomic_load flags addr)))\n      (load_acquire ty addr))\n\n\n;;;; Rules for `AtomicStore` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n(rule (lower (atomic_store flags\n                src @ (value_type (valid_atomic_transaction ty))\n                addr))\n      (side_effect (store_release ty src addr)))\n\n\n;;;; Rules for `AtomicRMW` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Add) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Add) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Xor) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Eor) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Or) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Set) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Smax) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Smax) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Smin) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Smin) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Umax) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Umax) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Umin) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Umin) addr src ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.Sub) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Add) addr (sub ty (zero_reg) src) ty))\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                      (atomic_rmw flags (AtomicRmwOp.And) addr src))))\n      (lse_atomic_rmw (AtomicRMWOp.Clr) addr (eon ty src (zero_reg)) ty))\n\n\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Add) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Add) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Sub) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Sub) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.And) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.And) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Nand) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Nand) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Or) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Orr) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Xor) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Eor) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Smin) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Smin) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Smax) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Smax) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Umin) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Umin) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Umax) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Umax) addr src ty))\n(rule (lower (has_type (valid_atomic_transaction ty)\n             (atomic_rmw flags (AtomicRmwOp.Xchg) addr src)))\n      (atomic_rmw_loop (AtomicRMWLoopOp.Xchg) addr src ty))\n\n;;;; Rules for `AtomicCAS` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n(rule 1 (lower (and (use_lse)\n                  (has_type (valid_atomic_transaction ty)\n                  (atomic_cas flags addr src1 src2))))\n      (lse_atomic_cas addr src1 src2 ty))\n\n(rule (lower (and (has_type (valid_atomic_transaction ty)\n                  (atomic_cas flags addr src1 src2))))\n      (atomic_cas_loop addr src1 src2 ty))\n", "//! ISLE integration glue code for aarch64 lowering.\n\n// Pull in the ISLE generated code.\npub mod generated_code;\n\n// Types that the generated ISLE code uses via `use super::*`.\nuse super::{\n    writable_zero_reg, zero_reg, AMode, ASIMDFPModImm, ASIMDMovModImm, BranchTarget, CallIndInfo,\n    CallInfo, Cond, CondBrKind, ExtendOp, FPUOpRI, FloatCC, Imm12, ImmLogic, ImmShift,\n    Inst as MInst, IntCC, JTSequenceInfo, MachLabel, MoveWideConst, MoveWideOp, NarrowValueMode,\n    Opcode, OperandSize, PairAMode, Reg, ScalarSize, ShiftOpAndAmt, UImm5, VecMisc2, VectorSize,\n    NZCV,\n};\nuse crate::isa::aarch64::settings::Flags as IsaFlags;\nuse crate::machinst::{isle::*, InputSourceInst};\nuse crate::settings::Flags;\nuse crate::{\n    binemit::CodeOffset,\n    ir::{\n        immediates::*, types::*, AtomicRmwOp, ExternalName, Inst, InstructionData, MemFlags,\n        TrapCode, Value, ValueList,\n    },\n    isa::aarch64::inst::args::{ShiftOp, ShiftOpShiftImm},\n    isa::aarch64::lower::{writable_xreg, xreg},\n    isa::unwind::UnwindInst,\n    machinst::{ty_bits, InsnOutput, LowerCtx, VCodeConstant, VCodeConstantData},\n};\nuse std::boxed::Box;\nuse std::convert::TryFrom;\nuse std::vec::Vec;\n\ntype BoxCallInfo = Box<CallInfo>;\ntype BoxCallIndInfo = Box<CallIndInfo>;\ntype VecMachLabel = Vec<MachLabel>;\ntype BoxJTSequenceInfo = Box<JTSequenceInfo>;\ntype BoxExternalName = Box<ExternalName>;\n\n/// The main entry point for lowering with ISLE.\npub(crate) fn lower<C>(\n    lower_ctx: &mut C,\n    flags: &Flags,\n    isa_flags: &IsaFlags,\n    outputs: &[InsnOutput],\n    inst: Inst,\n) -> Result<(), ()>\nwhere\n    C: LowerCtx<I = MInst>,\n{\n    lower_common(lower_ctx, flags, isa_flags, outputs, inst, |cx, insn| {\n        generated_code::constructor_lower(cx, insn)\n    })\n}\n\npub struct ExtendedValue {\n    val: Value,\n    extend: ExtendOp,\n}\n\npub struct SinkableAtomicLoad {\n    atomic_load: Inst,\n    atomic_addr: Value,\n}\n\nimpl<C> generated_code::Context for IsleContext<'_, C, Flags, IsaFlags, 6>\nwhere\n    C: LowerCtx<I = MInst>,\n{\n    isle_prelude_methods!();\n\n    fn use_lse(&mut self, _: Inst) -> Option<()> {\n        if self.isa_flags.use_lse() {\n            Some(())\n        } else {\n            None\n        }\n    }\n\n    fn imm_logic_from_u64(&mut self, ty: Type, n: u64) -> Option<ImmLogic> {\n        ImmLogic::maybe_from_u64(n, ty)\n    }\n\n    fn imm_logic_from_imm64(&mut self, ty: Type, n: Imm64) -> Option<ImmLogic> {\n        let ty = if ty.bits() < 32 { I32 } else { ty };\n        self.imm_logic_from_u64(ty, n.bits() as u64)\n    }\n\n    fn imm12_from_u64(&mut self, n: u64) -> Option<Imm12> {\n        Imm12::maybe_from_u64(n)\n    }\n\n    fn imm12_from_negated_u64(&mut self, n: u64) -> Option<Imm12> {\n        Imm12::maybe_from_u64((n as i64).wrapping_neg() as u64)\n    }\n\n    fn imm_shift_from_u8(&mut self, n: u8) -> ImmShift {\n        ImmShift::maybe_from_u64(n.into()).unwrap()\n    }\n\n    fn lshl_from_imm64(&mut self, ty: Type, n: Imm64) -> Option<ShiftOpAndAmt> {\n        let shiftimm = ShiftOpShiftImm::maybe_from_shift(n.bits() as u64)?;\n        let shiftee_bits = ty_bits(ty);\n        if shiftee_bits <= std::u8::MAX as usize {\n            let shiftimm = shiftimm.mask(shiftee_bits as u8);\n            Some(ShiftOpAndAmt::new(ShiftOp::LSL, shiftimm))\n        } else {\n            None\n        }\n    }\n\n    fn integral_ty(&mut self, ty: Type) -> Option<Type> {\n        match ty {\n            I8 | I16 | I32 | I64 | R64 => Some(ty),\n            ty if ty.is_bool() => Some(ty),\n            _ => None,\n        }\n    }\n\n    /// This is target-word-size dependent.  And it excludes booleans and reftypes.\n    fn valid_atomic_transaction(&mut self, ty: Type) -> Option<Type> {\n        match ty {\n            I8 | I16 | I32 | I64 => Some(ty),\n            _ => None,\n        }\n    }\n\n    /// This is the fallback case for loading a 64-bit integral constant into a\n    /// register.\n    ///\n    /// The logic here is nontrivial enough that it's not really worth porting\n    /// this over to ISLE.\n    fn load_constant64_full(\n        &mut self,\n        ty: Type,\n        extend: &generated_code::ImmExtend,\n        value: u64,\n    ) -> Reg {\n        let bits = ty.bits();\n        let value = if bits < 64 {\n            if *extend == generated_code::ImmExtend::Sign {\n                let shift = 64 - bits;\n                let value = value as i64;\n\n                ((value << shift) >> shift) as u64\n            } else {\n                value & !(u64::MAX << bits)\n            }\n        } else {\n            value\n        };\n        let rd = self.temp_writable_reg(I64);\n\n        if value == 0 {\n            self.emit(&MInst::MovWide {\n                op: MoveWideOp::MovZ,\n                rd,\n                imm: MoveWideConst::zero(),\n                size: OperandSize::Size64,\n            });\n            return rd.to_reg();\n        } else if value == u64::MAX {\n            self.emit(&MInst::MovWide {\n                op: MoveWideOp::MovN,\n                rd,\n                imm: MoveWideConst::zero(),\n                size: OperandSize::Size64,\n            });\n            return rd.to_reg();\n        };\n\n        // If the top 32 bits are zero, use 32-bit `mov` operations.\n        let (num_half_words, size, negated) = if value >> 32 == 0 {\n            (2, OperandSize::Size32, (!value << 32) >> 32)\n        } else {\n            (4, OperandSize::Size64, !value)\n        };\n        // If the number of 0xffff half words is greater than the number of 0x0000 half words\n        // it is more efficient to use `movn` for the first instruction.\n        let first_is_inverted = count_zero_half_words(negated, num_half_words)\n            > count_zero_half_words(value, num_half_words);\n        // Either 0xffff or 0x0000 half words can be skipped, depending on the first\n        // instruction used.\n        let ignored_halfword = if first_is_inverted { 0xffff } else { 0 };\n        let mut first_mov_emitted = false;\n\n        for i in 0..num_half_words {\n            let imm16 = (value >> (16 * i)) & 0xffff;\n            if imm16 != ignored_halfword {\n                if !first_mov_emitted {\n                    first_mov_emitted = true;\n                    if first_is_inverted {\n                        let imm =\n                            MoveWideConst::maybe_with_shift(((!imm16) & 0xffff) as u16, i * 16)\n                                .unwrap();\n                        self.emit(&MInst::MovWide {\n                            op: MoveWideOp::MovN,\n                            rd,\n                            imm,\n                            size,\n                        });\n                    } else {\n                        let imm = MoveWideConst::maybe_with_shift(imm16 as u16, i * 16).unwrap();\n                        self.emit(&MInst::MovWide {\n                            op: MoveWideOp::MovZ,\n                            rd,\n                            imm,\n                            size,\n                        });\n                    }\n                } else {\n                    let imm = MoveWideConst::maybe_with_shift(imm16 as u16, i * 16).unwrap();\n                    self.emit(&MInst::MovWide {\n                        op: MoveWideOp::MovK,\n                        rd,\n                        imm,\n                        size,\n                    });\n                }\n            }\n        }\n\n        assert!(first_mov_emitted);\n\n        return self.writable_reg_to_reg(rd);\n\n        fn count_zero_half_words(mut value: u64, num_half_words: u8) -> usize {\n            let mut count = 0;\n            for _ in 0..num_half_words {\n                if value & 0xffff == 0 {\n                    count += 1;\n                }\n                value >>= 16;\n            }\n\n            count\n        }\n    }\n\n    fn zero_reg(&mut self) -> Reg {\n        zero_reg()\n    }\n\n    fn xreg(&mut self, index: u8) -> Reg {\n        xreg(index)\n    }\n\n    fn writable_xreg(&mut self, index: u8) -> WritableReg {\n        writable_xreg(index)\n    }\n\n    fn extended_value_from_value(&mut self, val: Value) -> Option<ExtendedValue> {\n        let (val, extend) =\n            super::get_as_extended_value(self.lower_ctx, val, NarrowValueMode::None)?;\n        Some(ExtendedValue { val, extend })\n    }\n\n    fn put_extended_in_reg(&mut self, reg: &ExtendedValue) -> Reg {\n        self.put_in_reg(reg.val)\n    }\n\n    fn get_extended_op(&mut self, reg: &ExtendedValue) -> ExtendOp {\n        reg.extend\n    }\n\n    fn emit(&mut self, inst: &MInst) -> Unit {\n        self.lower_ctx.emit(inst.clone());\n    }\n\n    fn cond_br_zero(&mut self, reg: Reg) -> CondBrKind {\n        CondBrKind::Zero(reg)\n    }\n\n    fn cond_br_cond(&mut self, cond: &Cond) -> CondBrKind {\n        CondBrKind::Cond(*cond)\n    }\n\n    fn nzcv(&mut self, n: bool, z: bool, c: bool, v: bool) -> NZCV {\n        NZCV::new(n, z, c, v)\n    }\n\n    fn u8_into_uimm5(&mut self, x: u8) -> UImm5 {\n        UImm5::maybe_from_u8(x).unwrap()\n    }\n\n    fn u8_into_imm12(&mut self, x: u8) -> Imm12 {\n        Imm12::maybe_from_u64(x.into()).unwrap()\n    }\n\n    fn writable_zero_reg(&mut self) -> WritableReg {\n        writable_zero_reg()\n    }\n\n    fn safe_divisor_from_imm64(&mut self, val: Imm64) -> Option<u64> {\n        match val.bits() {\n            0 | -1 => None,\n            n => Some(n as u64),\n        }\n    }\n\n    fn sinkable_atomic_load(&mut self, val: Value) -> Option<SinkableAtomicLoad> {\n        let input = self.lower_ctx.get_value_as_source_or_const(val);\n        if let InputSourceInst::UniqueUse(atomic_load, 0) = input.inst {\n            if self.lower_ctx.data(atomic_load).opcode() == Opcode::AtomicLoad {\n                let atomic_addr = self.lower_ctx.input_as_value(atomic_load, 0);\n                return Some(SinkableAtomicLoad {\n                    atomic_load,\n                    atomic_addr,\n                });\n            }\n        }\n        None\n    }\n\n    fn sink_atomic_load(&mut self, load: &SinkableAtomicLoad) -> Reg {\n        self.lower_ctx.sink_inst(load.atomic_load);\n        self.put_in_reg(load.atomic_addr)\n    }\n\n    fn shift_mask(&mut self, ty: Type) -> ImmLogic {\n        let mask = (ty.bits() - 1) as u64;\n        ImmLogic::maybe_from_u64(mask, I32).unwrap()\n    }\n\n    fn imm_shift_from_imm64(&mut self, ty: Type, val: Imm64) -> Option<ImmShift> {\n        let imm_value = (val.bits() as u64) & ((ty.bits() - 1) as u64);\n        ImmShift::maybe_from_u64(imm_value)\n    }\n\n    fn u64_into_imm_logic(&mut self, ty: Type, val: u64) -> ImmLogic {\n        ImmLogic::maybe_from_u64(val, ty).unwrap()\n    }\n\n    fn negate_imm_shift(&mut self, ty: Type, mut imm: ImmShift) -> ImmShift {\n        let size = u8::try_from(ty.bits()).unwrap();\n        imm.imm = size.wrapping_sub(imm.value());\n        imm.imm &= size - 1;\n        imm\n    }\n\n    fn rotr_mask(&mut self, ty: Type) -> ImmLogic {\n        ImmLogic::maybe_from_u64((ty.bits() - 1) as u64, I32).unwrap()\n    }\n\n    fn rotr_opposite_amount(&mut self, ty: Type, val: ImmShift) -> ImmShift {\n        let amount = val.value() & u8::try_from(ty.bits() - 1).unwrap();\n        ImmShift::maybe_from_u64(u64::from(ty.bits()) - u64::from(amount)).unwrap()\n    }\n\n    fn icmp_zero_cond(&mut self, cond: &IntCC) -> Option<IntCC> {\n        match cond {\n            &IntCC::Equal\n            | &IntCC::SignedGreaterThanOrEqual\n            | &IntCC::SignedGreaterThan\n            | &IntCC::SignedLessThanOrEqual\n            | &IntCC::SignedLessThan => Some(*cond),\n            _ => None,\n        }\n    }\n\n    fn fcmp_zero_cond(&mut self, cond: &FloatCC) -> Option<FloatCC> {\n        match cond {\n            &FloatCC::Equal\n            | &FloatCC::GreaterThanOrEqual\n            | &FloatCC::GreaterThan\n            | &FloatCC::LessThanOrEqual\n            | &FloatCC::LessThan => Some(*cond),\n            _ => None,\n        }\n    }\n\n    fn fcmp_zero_cond_not_eq(&mut self, cond: &FloatCC) -> Option<FloatCC> {\n        match cond {\n            &FloatCC::NotEqual => Some(FloatCC::NotEqual),\n            _ => None,\n        }\n    }\n\n    fn icmp_zero_cond_not_eq(&mut self, cond: &IntCC) -> Option<IntCC> {\n        match cond {\n            &IntCC::NotEqual => Some(IntCC::NotEqual),\n            _ => None,\n        }\n    }\n\n    fn float_cc_cmp_zero_to_vec_misc_op(&mut self, cond: &FloatCC) -> VecMisc2 {\n        match cond {\n            &FloatCC::Equal => VecMisc2::Fcmeq0,\n            &FloatCC::GreaterThanOrEqual => VecMisc2::Fcmge0,\n            &FloatCC::LessThanOrEqual => VecMisc2::Fcmle0,\n            &FloatCC::GreaterThan => VecMisc2::Fcmgt0,\n            &FloatCC::LessThan => VecMisc2::Fcmlt0,\n            _ => panic!(),\n        }\n    }\n\n    fn int_cc_cmp_zero_to_vec_misc_op(&mut self, cond: &IntCC) -> VecMisc2 {\n        match cond {\n            &IntCC::Equal => VecMisc2::Cmeq0,\n            &IntCC::SignedGreaterThanOrEqual => VecMisc2::Cmge0,\n            &IntCC::SignedLessThanOrEqual => VecMisc2::Cmle0,\n            &IntCC::SignedGreaterThan => VecMisc2::Cmgt0,\n            &IntCC::SignedLessThan => VecMisc2::Cmlt0,\n            _ => panic!(),\n        }\n    }\n\n    fn float_cc_cmp_zero_to_vec_misc_op_swap(&mut self, cond: &FloatCC) -> VecMisc2 {\n        match cond {\n            &FloatCC::Equal => VecMisc2::Fcmeq0,\n            &FloatCC::GreaterThanOrEqual => VecMisc2::Fcmle0,\n            &FloatCC::LessThanOrEqual => VecMisc2::Fcmge0,\n            &FloatCC::GreaterThan => VecMisc2::Fcmlt0,\n            &FloatCC::LessThan => VecMisc2::Fcmgt0,\n            _ => panic!(),\n        }\n    }\n\n    fn int_cc_cmp_zero_to_vec_misc_op_swap(&mut self, cond: &IntCC) -> VecMisc2 {\n        match cond {\n            &IntCC::Equal => VecMisc2::Cmeq0,\n            &IntCC::SignedGreaterThanOrEqual => VecMisc2::Cmle0,\n            &IntCC::SignedLessThanOrEqual => VecMisc2::Cmge0,\n            &IntCC::SignedGreaterThan => VecMisc2::Cmlt0,\n            &IntCC::SignedLessThan => VecMisc2::Cmgt0,\n            _ => panic!(),\n        }\n    }\n\n    fn zero_value(&mut self, value: Imm64) -> Option<Imm64> {\n        if value.bits() == 0 {\n            return Some(value);\n        }\n        None\n    }\n\n    fn zero_value_f32(&mut self, value: Ieee32) -> Option<Ieee32> {\n        if value.bits() == 0 {\n            return Some(value);\n        }\n        None\n    }\n\n    fn zero_value_f64(&mut self, value: Ieee64) -> Option<Ieee64> {\n        if value.bits() == 0 {\n            return Some(value);\n        }\n        None\n    }\n}\n", "test compile precise-output\nset unwind_info=false\ntarget aarch64\n\nfunction %f1(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = iadd.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   add x0, x0, x1\n;   ret\n\nfunction %f2(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = isub.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   sub x0, x0, x1\n;   ret\n\nfunction %f3(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = imul.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   madd x0, x0, x1, xzr\n;   ret\n\nfunction %f4(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = umulhi.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   umulh x0, x0, x1\n;   ret\n\nfunction %f5(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = smulhi.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   smulh x0, x0, x1\n;   ret\n\nfunction %f6(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = sdiv.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   cbnz x1, 8 ; udf\n;   adds xzr, x1, #1\n;   ccmp x0, #1, #nzcv, eq\n;   b.vc 8 ; udf\n;   sdiv x0, x0, x1\n;   ret\n\nfunction %f7(i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 2\n  v2 = sdiv.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   movz w3, #2\n;   sdiv x0, x0, x3\n;   ret\n\nfunction %f8(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = udiv.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   cbnz x1, 8 ; udf\n;   udiv x0, x0, x1\n;   ret\n\nfunction %f9(i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 2\n  v2 = udiv.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   orr x3, xzr, #2\n;   udiv x0, x0, x3\n;   ret\n\nfunction %f10(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = srem.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   cbnz x1, 8 ; udf\n;   sdiv x6, x0, x1\n;   msub x0, x6, x1, x0\n;   ret\n\nfunction %f11(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = urem.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   cbnz x1, 8 ; udf\n;   udiv x6, x0, x1\n;   msub x0, x6, x1, x0\n;   ret\n\nfunction %f12(i32, i32) -> i32 {\nblock0(v0: i32, v1: i32):\n  v2 = sdiv.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   sxtw x5, w0\n;   sxtw x7, w1\n;   cbnz x7, 8 ; udf\n;   adds wzr, w7, #1\n;   ccmp w5, #1, #nzcv, eq\n;   b.vc 8 ; udf\n;   sdiv x0, x5, x7\n;   ret\n\nfunction %f13(i32) -> i32 {\nblock0(v0: i32):\n  v1 = iconst.i32 2\n  v2 = sdiv.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   sxtw x3, w0\n;   movz w5, #2\n;   sdiv x0, x3, x5\n;   ret\n\nfunction %f14(i32, i32) -> i32 {\nblock0(v0: i32, v1: i32):\n  v2 = udiv.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   mov w5, w0\n;   mov w7, w1\n;   cbnz x7, 8 ; udf\n;   udiv x0, x5, x7\n;   ret\n\nfunction %f15(i32) -> i32 {\nblock0(v0: i32):\n  v1 = iconst.i32 2\n  v2 = udiv.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   mov w3, w0\n;   orr w5, wzr, #2\n;   udiv x0, x3, x5\n;   ret\n\nfunction %f16(i32, i32) -> i32 {\nblock0(v0: i32, v1: i32):\n  v2 = srem.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   sxtw x5, w0\n;   sxtw x7, w1\n;   cbnz x7, 8 ; udf\n;   sdiv x10, x5, x7\n;   msub x0, x10, x7, x5\n;   ret\n\nfunction %f17(i32, i32) -> i32 {\nblock0(v0: i32, v1: i32):\n  v2 = urem.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   mov w5, w0\n;   mov w7, w1\n;   cbnz x7, 8 ; udf\n;   udiv x10, x5, x7\n;   msub x0, x10, x7, x5\n;   ret\n\nfunction %f18(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = band.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   and x0, x0, x1\n;   ret\n\nfunction %f19(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = bor.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   orr x0, x0, x1\n;   ret\n\nfunction %f20(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = bxor.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   eor x0, x0, x1\n;   ret\n\nfunction %f21(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = band_not.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   bic x0, x0, x1\n;   ret\n\nfunction %f22(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = bor_not.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   orn x0, x0, x1\n;   ret\n\nfunction %f23(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = bxor_not.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   eon x0, x0, x1\n;   ret\n\nfunction %f24(i64, i64) -> i64 {\nblock0(v0: i64, v1: i64):\n  v2 = bnot.i64 v0\n  return v2\n}\n\n; block0:\n;   orn x0, xzr, x0\n;   ret\n\nfunction %f25(i32, i32) -> i32 {\nblock0(v0: i32, v1: i32):\n  v2 = iconst.i32 53\n  v3 = ishl.i32 v0, v2\n  v4 = isub.i32 v1, v3\n  return v4\n}\n\n; block0:\n;   sub w0, w1, w0, LSL 21\n;   ret\n\nfunction %f26(i32) -> i32 {\nblock0(v0: i32):\n  v1 = iconst.i32 -1\n  v2 = iadd.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   sub w0, w0, #1\n;   ret\n\nfunction %f27(i32) -> i32 {\nblock0(v0: i32):\n  v1 = iconst.i32 -1\n  v2 = isub.i32 v0, v1\n  return v2\n}\n\n; block0:\n;   add w0, w0, #1\n;   ret\n\nfunction %f28(i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 -1\n  v2 = isub.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   add x0, x0, #1\n;   ret\n\nfunction %f29(i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 1\n  v2 = ineg v1\n  return v2\n}\n\n; block0:\n;   movz x3, #1\n;   sub x0, xzr, x3\n;   ret\n\nfunction %f30(i8x16) -> i8x16 {\nblock0(v0: i8x16):\n  v1 = iconst.i64 1\n  v2 = ushr.i8x16 v0, v1\n  return v2\n}\n\n; block0:\n;   movz x3, #1\n;   sub w5, wzr, w3\n;   dup v7.16b, w5\n;   ushl v0.16b, v0.16b, v7.16b\n;   ret\n\nfunction %add_i128(i128, i128) -> i128 {\nblock0(v0: i128, v1: i128):\n    v2 = iadd v0, v1\n    return v2\n}\n\n; block0:\n;   adds x0, x0, x2\n;   adc x1, x1, x3\n;   ret\n\nfunction %sub_i128(i128, i128) -> i128 {\nblock0(v0: i128, v1: i128):\n    v2 = isub v0, v1\n    return v2\n}\n\n; block0:\n;   subs x0, x0, x2\n;   sbc x1, x1, x3\n;   ret\n\nfunction %mul_i128(i128, i128) -> i128 {\nblock0(v0: i128, v1: i128):\n    v2 = imul v0, v1\n    return v2\n}\n\n; block0:\n;   umulh x10, x0, x2\n;   madd x12, x0, x3, x10\n;   madd x1, x1, x2, x12\n;   madd x0, x0, x2, xzr\n;   ret\n\nfunction %add_mul_1(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = iadd v0, v3\n    return v4\n}\n\n; block0:\n;   madd w0, w1, w2, w0\n;   ret\n\nfunction %add_mul_2(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = iadd v3, v0\n    return v4\n}\n\n; block0:\n;   madd w0, w1, w2, w0\n;   ret\n\nfunction %msub_i32(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n\n; block0:\n;   msub w0, w1, w2, w0\n;   ret\n\nfunction %msub_i64(i64, i64, i64) -> i64 {\nblock0(v0: i64, v1: i64, v2: i64):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n\n; block0:\n;   msub x0, x1, x2, x0\n;   ret\n\nfunction %imul_sub_i32(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = isub v3, v0\n    return v4\n}\n\n; block0:\n;   madd w8, w1, w2, wzr\n;   sub w0, w8, w0\n;   ret\n\nfunction %imul_sub_i64(i64, i64, i64) -> i64 {\nblock0(v0: i64, v1: i64, v2: i64):\n    v3 = imul v1, v2\n    v4 = isub v3, v0\n    return v4\n}\n\n; block0:\n;   madd x8, x1, x2, xzr\n;   sub x0, x8, x0\n;   ret\n\nfunction %srem_const (i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 2\n  v2 = srem.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   movz w3, #2\n;   sdiv x5, x0, x3\n;   msub x0, x5, x3, x0\n;   ret\n\nfunction %urem_const (i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 2\n  v2 = urem.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   orr x3, xzr, #2\n;   udiv x5, x0, x3\n;   msub x0, x5, x3, x0\n;   ret\n\nfunction %sdiv_minus_one(i64) -> i64 {\nblock0(v0: i64):\n  v1 = iconst.i64 -1\n  v2 = sdiv.i64 v0, v1\n  return v2\n}\n\n; block0:\n;   movn x3, #0\n;   adds xzr, x3, #1\n;   ccmp x0, #1, #nzcv, eq\n;   b.vc 8 ; udf\n;   sdiv x0, x0, x3\n;   ret\n\n", "test interpret\ntest run\ntarget aarch64\ntarget s390x\ntarget x86_64\n\nfunction %add_i64(i64, i64) -> i64 {\nblock0(v0: i64,v1: i64):\n    v2 = iadd v0, v1\n    return v2\n}\n; run: %add_i64(0, 0) == 0\n; run: %add_i64(0, 1) == 1\n; run: %add_i64(-1, 0) == -1\n; run: %add_i64(-1, 1) == 0\n; run: %add_i64(0x7FFFFFFF_FFFFFFFF, 0x7FFFFFFF_FFFFFFFF) == -2\n; run: %add_i64(0x7FFFFFFF_FFFFFFFF, 0x80000000_00000000) == -1\n; run: %add_i64(0x01234567_89ABCDEF, 0xFEDCBA98_76543210) == -1\n; run: %add_i64(0xA00A00A0_0A00A00A, 0x0BB0BB0B_B0BB0BB0) == 0xABBABBAB_BABBABBA\n; run: %add_i64(0xC0FFEEEE_C0FFEEEE, 0x1DCB1111_1DCB1111) == 0xDECAFFFF_DECAFFFF\n\nfunction %add_i32(i32, i32) -> i32 {\nblock0(v0: i32,v1: i32):\n    v2 = iadd v0, v1\n    return v2\n}\n; run: %add_i32(0, 0) == 0\n; run: %add_i32(0, 1) == 1\n; run: %add_i32(-1, 0) == -1\n; run: %add_i32(-1, 1) == 0\n; run: %add_i32(0x7FFFFFFF, 0x7FFFFFFF) == -2\n; run: %add_i32(0x7FFFFFFF, 0x80000000) == -1\n; run: %add_i32(0x01234567, 0xFEDCBA98) == -1\n; run: %add_i32(0xA00A00A0, 0x0BB0BB0B) == 0xABBABBAB\n; run: %add_i32(0xC0FFEEEE, 0x1DCB1111) == 0xDECAFFFF\n\nfunction %add_i16(i16, i16) -> i16 {\nblock0(v0: i16,v1: i16):\n    v2 = iadd v0, v1\n    return v2\n}\n; run: %add_i16(0, 0) == 0\n; run: %add_i16(0, 1) == 1\n; run: %add_i16(-1, 0) == -1\n; run: %add_i16(-1, 1) == 0\n; run: %add_i16(0x7FFF, 0x7FFF) == -2\n; run: %add_i16(0x7FFF, 0x8000) == -1\n; run: %add_i16(0x0123, 0xFEDC) == -1\n; run: %add_i16(0xA00A, 0x0BB0) == 0xABBA\n; run: %add_i16(0xC0FF, 0x1DCB) == 0xDECA\n\nfunction %add_i8(i8, i8) -> i8 {\nblock0(v0: i8,v1: i8):\n    v2 = iadd v0, v1\n    return v2\n}\n; run: %add_i8(0, 0) == 0\n; run: %add_i8(0, 1) == 1\n; run: %add_i8(-1, 0) == -1\n; run: %add_i8(-1, 1) == 0\n; run: %add_i8(0x7F, 0x7F) == -2\n; run: %add_i8(0x7F, 0x80) == -1\n; run: %add_i8(0x01, 0xFE) == -1\n; run: %add_i8(0xA0, 0x0B) == 0xAB\n; run: %add_i8(0xC0, 0x1D) == 0xDD\n\n\nfunction %sub_i64(i64, i64) -> i64 {\nblock0(v0: i64,v1: i64):\n    v2 = isub v0, v1\n    return v2\n}\n; run: %sub_i64(0, 0) == 0\n; run: %sub_i64(0, 1) == -1\n; run: %sub_i64(1, 0) == 1\n; run: %sub_i64(-1, 0) == -1\n; run: %sub_i64(-1, 1) == -2\n; run: %sub_i64(0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 1\n; run: %sub_i64(0xFFFFFFFF_FFFFFFFF, 0xFEDCBA98_76543210) == 0x01234567_89ABCDEF\n; run: %sub_i64(0xABBABBAB_BABBABBA, 0x0BB0BB0B_B0BB0BB0) == 0xA00A00A0_0A00A00A\n; run: %sub_i64(0xC0FFEEEE_C0FFEEEE, 0xDECAFFFF_DECAFFFF) == 0xE234EEEE_E234EEEF\n\nfunction %sub_i32(i32, i32) -> i32 {\nblock0(v0: i32,v1: i32):\n    v2 = isub v0, v1\n    return v2\n}\n; run: %sub_i32(0, 0) == 0\n; run: %sub_i32(0, 1) == -1\n; run: %sub_i32(1, 0) == 1\n; run: %sub_i32(-1, 0) == -1\n; run: %sub_i32(-1, 1) == -2\n; run: %sub_i32(0x80000000, 0x7FFFFFFF) == 1\n; run: %sub_i32(0xFFFFFFFF, 0xFEDCBA98) == 0x01234567\n; run: %sub_i32(0xABBABBAB, 0x0BB0BB0B) == 0xA00A00A0\n; run: %sub_i32(0xC0FFEEEE, 0xDECAFFFF) == 0xE234EEEF\n\nfunction %sub_i16(i16, i16) -> i16 {\nblock0(v0: i16,v1: i16):\n    v2 = isub v0, v1\n    return v2\n}\n; run: %sub_i16(0, 0) == 0\n; run: %sub_i16(0, 1) == -1\n; run: %sub_i16(1, 0) == 1\n; run: %sub_i16(-1, 0) == -1\n; run: %sub_i16(-1, 1) == -2\n; run: %sub_i16(0x8000, 0x7FFF) == 1\n; run: %sub_i16(0xFFFF, 0xFEDC) == 0x0123\n; run: %sub_i16(0xABBA, 0x0BB0) == 0xA00A\n; run: %sub_i16(0xC0FF, 0xDECA) == 0xE235\n\nfunction %sub_i8(i8, i8) -> i8 {\nblock0(v0: i8,v1: i8):\n    v2 = isub v0, v1\n    return v2\n}\n; run: %sub_i8(0, 0) == 0\n; run: %sub_i8(0, 1) == -1\n; run: %sub_i8(1, 0) == 1\n; run: %sub_i8(-1, 0) == -1\n; run: %sub_i8(-1, 1) == -2\n; run: %sub_i8(0x80, 0x7F) == 1\n; run: %sub_i8(0xFF, 0xFE) == 0x01\n; run: %sub_i8(0xAB, 0x0B) == 0xA0\n; run: %sub_i8(0xC0, 0xDE) == 0xE2\n\n\nfunction %mul_i64(i64, i64) -> i64 {\nblock0(v0: i64,v1: i64):\n    v2 = imul v0, v1\n    return v2\n}\n; run: %mul_i64(0, 0) == 0\n; run: %mul_i64(0, 1) == 0\n; run: %mul_i64(1, -1) == -1\n; run: %mul_i64(2, 2) == 4\n; run: %mul_i64(0x7FFFFFFF_FFFFFFFF, 0x7FFFFFFF_FFFFFFFF) == 1\n; run: %mul_i64(0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 0x80000000_00000000\n; run: %mul_i64(0x01234567_89ABCDEF, 0xFEDCBA98_76543210) == 0x2236D88F_E5618CF0\n; run: %mul_i64(0xC0FFEEEE_C0FFEEEE, 0xDECAFFFF_DECAFFFF) == 0xDB6B1E48_19BA1112\n\nfunction %mul_i32(i32, i32) -> i32 {\nblock0(v0: i32,v1: i32):\n    v2 = imul v0, v1\n    return v2\n}\n; run: %mul_i32(0, 0) == 0\n; run: %mul_i32(0, 1) == 0\n; run: %mul_i32(1, -1) == -1\n; run: %mul_i32(2, 2) == 4\n; run: %mul_i32(0x7FFFFFFF, 0x7FFFFFFF) == 1\n; run: %mul_i32(0x80000000, 0x7FFFFFFF) == 0x80000000\n; run: %mul_i32(0x01234567, 0xFEDCBA98) == 0x23E20B28\n; run: %mul_i32(0xC0FFEEEE, 0xDECAFFFF) == 0x19BA1112\n\nfunction %mul_i16(i16, i16) -> i16 {\nblock0(v0: i16,v1: i16):\n    v2 = imul v0, v1\n    return v2\n}\n; run: %mul_i16(0, 0) == 0\n; run: %mul_i16(0, 1) == 0\n; run: %mul_i16(1, -1) == -1\n; run: %mul_i16(2, 2) == 4\n; run: %mul_i16(0x7FFF, 0x7FFF) == 1\n; run: %mul_i16(0x8000, 0x7FFF) == 0x8000\n; run: %mul_i16(0x0123, 0xFEDC) == 0xB414\n; run: %mul_i16(0xC0FF, 0xDECA) == 0x6B36\n\nfunction %mul_i8(i8, i8) -> i8 {\nblock0(v0: i8,v1: i8):\n    v2 = imul v0, v1\n    return v2\n}\n; run: %mul_i8(0, 0) == 0\n; run: %mul_i8(0, 1) == 0\n; run: %mul_i8(1, -1) == -1\n; run: %mul_i8(2, 2) == 4\n; run: %mul_i8(0x7F, 0x7F) == 1\n; run: %mul_i8(0x80, 0x7F) == 0x80\n; run: %mul_i8(0x01, 0xFE) == 0xFE\n; run: %mul_i8(0xC0, 0xDE) == 0x80\n\n\nfunction %madd_i8(i8, i8, i8) -> i8 {\nblock0(v0: i8, v1: i8, v2: i8):\n    v3 = imul v1, v2\n    v4 = iadd v0, v3\n    return v4\n}\n; run: %madd_i8(0, 1, 0) == 0\n; run: %madd_i8(1, 0, 0) == 1\n; run: %madd_i8(0, -1, 1) == -1\n; run: %madd_i8(2, 2, 2) == 6\n; run: %madd_i8(0, 0x7F, 0x7F) == 1\n; run: %madd_i8(0x7F, 0x7F, -1) == 0\n; run: %madd_i8(0x80, 0x7F, 0) == 0x80\n; run: %madd_i8(0x80, 0x7F, 0x80) == 0\n; run: %madd_i8(0x01, 0xFE, 0) == 1\n; run: %madd_i8(0x01, 0xFE, 2) == -3\n; run: %madd_i8(0, 0xC0, 0xDE) == 0x80\n; run: %madd_i8(0xC0, 0xC0, 0xDE) == 0x40\n\nfunction %madd_i16(i16, i16, i16) -> i16 {\nblock0(v0: i16, v1: i16, v2: i16):\n    v3 = imul v1, v2\n    v4 = iadd v0, v3\n    return v4\n}\n; run: %madd_i16(0, 1, 0) == 0\n; run: %madd_i16(1, 0, 0) == 1\n; run: %madd_i16(0, -1, 1) == -1\n; run: %madd_i16(2, 2, 2) == 6\n; run: %madd_i16(0, 0x7FFF, 0x7FFF) == 1\n; run: %madd_i16(0x7FFF, 1, 0x7FFF) == -2\n; run: %madd_i16(0x8000, 1, 0x7FFF) == -1\n; run: %madd_i16(0x0123, 0x0456, 0xFEDC) == 0xF0B\n; run: %madd_i16(0xC0FF, 0x0123, 0xDECA) == 0x9D\n\nfunction %madd_i32(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = iadd v0, v3\n    return v4\n}\n; run: %madd_i32(0, 1, 0) == 0\n; run: %madd_i32(1, 0, 0) == 1\n; run: %madd_i32(0, -1, 1) == -1\n; run: %madd_i32(2, 2, 2) == 6\n; run: %madd_i32(0, 0x7FFFFFFF, 0x7FFFFFFF) == 1\n; run: %madd_i32(-1, 0x7FFFFFFF, 0x7FFFFFFF) == 0\n; run: %madd_i32(0x80000000, 1, 0x7FFFFFFF) == -1\n; run: %madd_i32(0x80000000, 0x01234567, 0x7FFFFFFF) == 0xFEDCBA99\n; run: %madd_i32(0x01234567, 0x80000000, 0xFEDCBA98) == 0x1234567\n; run: %madd_i32(0xC0FFEEEE, 0xDECAFFFF, 0x0DEBAC1E) == 0x32DE42D0\n\nfunction %madd_i64(i64, i64, i64) -> i64 {\nblock0(v0: i64, v1: i64, v2: i64):\n    v3 = imul v1, v2\n    v4 = iadd v0, v3\n    return v4\n}\n; run: %madd_i64(0, 1, 0) == 0\n; run: %madd_i64(1, 0, 0) == 1\n; run: %madd_i64(0, -1, 1) == -1\n; run: %madd_i64(2, 2, 2) == 6\n; run: %madd_i64(0, 0x7FFFFFFF_FFFFFFFF, 0x7FFFFFFF_FFFFFFFF) == 1\n; run: %madd_i64(-1, 0x7FFFFFFF_FFFFFFFF, 0x7FFFFFFF_FFFFFFFF) == 0\n; run: %madd_i64(0, 0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 0x80000000_00000000\n; run: %madd_i64(1, 0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 0x80000000_00000001\n; run: %madd_i64(0x01234567_89ABCDEF, 0x01234567_FEDCBA98, 0xFEDCBA98_76543210) == 0x89C0845D_DDC9276F\n; run: %madd_i64(0xC0FFEEEE_C0FFEEEE, 0xBAADF00D_BAADF00D, 0xDECAFFFF_DECAFFFF) == 0x2EB8ECEC_A6A0FEE1\n\nfunction %msub_i8(i8, i8, i8) -> i8 {\nblock0(v0: i8, v1: i8, v2: i8):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n; run: %msub_i8(0, 0, 0) == 0\n; run: %msub_i8(1, 1, 1) == 0\n; run: %msub_i8(1, 1, 0) == 1\n; run: %msub_i8(0, 1, 1) == -1\n; run: %msub_i8(1, 1, -1) == 2\n; run: %msub_i8(-2, 1, -1) == -1\n; run: %msub_i8(2, 2, 2) == -2\n; run: %msub_i8(0, 0x7F, 0x7F) == -1\n; run: %msub_i8(0x7F, 0x80, 0x7F) == -1\n; run: %msub_i8(0x80, 1, 0x7F) == 1\n; run: %msub_i8(0x01, 0x80, 0xFE) == 1\n; run: %msub_i8(0xFF, 1, 0xDE) == 0x21\n\nfunction %msub_i16(i16, i16, i16) -> i16 {\nblock0(v0: i16, v1: i16, v2: i16):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n; run: %msub_i16(0, 0, 0) == 0\n; run: %msub_i16(1, 1, 1) == 0\n; run: %msub_i16(1, 1, 0) == 1\n; run: %msub_i16(0, 1, 1) == -1\n; run: %msub_i16(1, 1, -1) == 2\n; run: %msub_i16(-2, 1, -1) == -1\n; run: %msub_i16(2, 2, 2) == -2\n; run: %msub_i16(0, 0x7FFF, 0x7FFF) == -1\n; run: %msub_i16(0x0FFF, 1, 0x7FFF) == 0x9000\n; run: %msub_i16(0x7000, 1, 0x7FFF) == 0xF001\n; run: %msub_i16(0x0123, 0x0456, 0xFEDC) == 0xF33B\n; run: %msub_i16(0xC0FF, 0x0123, 0xDECA) == 0x8161\n\nfunction %msub_i32(i32, i32, i32) -> i32 {\nblock0(v0: i32, v1: i32, v2: i32):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n; run: %msub_i32(0, 0, 0) == 0\n; run: %msub_i32(1, 1, 1) == 0\n; run: %msub_i32(1, 1, 0) == 1\n; run: %msub_i32(0, 1, 1) == -1\n; run: %msub_i32(1, 1, -1) == 2\n; run: %msub_i32(-2, 1, -1) == -1\n; run: %msub_i32(2, 2, 2) == -2\n; run: %msub_i32(0, 0x7FFFFFFF, 0x7FFFFFFF) == -1\n; run: %msub_i32(0x0FFFFF, 1, 0x7FFFFFFF) == 0x80100000\n; run: %msub_i32(0x7FFFFFFF, 1, 0x80000000) == -1\n; run: %msub_i32(0x80000000, 0x01234567, 0x7FFFFFFF) == 0x01234567\n; run: %msub_i32(0xFEDCBA98, 0x80000000, 0x01234567) == 0x7EDCBA98\n; run: %msub_i32(0xC0FFEEEE, 0xDECAFFFF, 0x0DEBAC1E) == 0x4F219B0C\n\nfunction %msub_i64(i64, i64, i64) -> i64 {\nblock0(v0: i64, v1: i64, v2: i64):\n    v3 = imul v1, v2\n    v4 = isub v0, v3\n    return v4\n}\n; run: %msub_i64(0, 0, 0) == 0\n; run: %msub_i64(1, 1, 1) == 0\n; run: %msub_i64(1, 1, 0) == 1\n; run: %msub_i64(0, 1, 1) == -1\n; run: %msub_i64(1, 1, -1) == 2\n; run: %msub_i64(-2, 1, -1) == -1\n; run: %msub_i64(2, 2, 2) == -2\n; run: %msub_i64(0, 0x7FFFFFFF_FFFFFFFF, 0x7FFFFFFF_FFFFFFFF) == -1\n; run: %msub_i64(0x0FFFFF_FFFFFFFF, 1, 0x7FFFFFFF_FFFFFFFF) == 0x80100000_00000000\n; run: %msub_i64(0, 0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 0x80000000_00000000\n; run: %msub_i64(1, 0x80000000_00000000, 0x7FFFFFFF_FFFFFFFF) == 0x80000000_00000001\n; run: %msub_i64(0x01234567_89ABCDEF, 0x01234567_FEDCBA98, 0xFEDCBA98_76543210) == 0x78860671_358E746F\n; run: %msub_i64(0xC0FFEEEE_C0FFEEEE, 0xBAADF00D_BAADF00D, 0xDECAFFFF_DECAFFFF) == 0x5346F0F0_DB5EDEFB\n\n\nfunction %sdiv_i64(i64, i64) -> i64 {\nblock0(v0: i64,v1: i64):\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i64(0, 1) == 0\n; run: %sdiv_i64(2, 2) == 1\n; run: %sdiv_i64(1, -1) == -1\n; run: %sdiv_i64(3, 2) == 1\n; run: %sdiv_i64(19, 7) == 2\n; run: %sdiv_i64(3, -2) == -1\n; run: %sdiv_i64(-19, 7) == -2\n; run: %sdiv_i64(0xC0FFEEEE_DECAFFFF, 8) == 0xF81FFDDD_DBD96000\n; run: %sdiv_i64(0xC0FFEEEE_DECAFFFF, -8) == 0x7E002222_426A000\n; run: %sdiv_i64(0x80000000_00000000, -2) == 0x40000000_00000000\n\nfunction %sdiv_i64_const(i64) -> i64 {\nblock0(v0: i64):\n    v1 = iconst.i64 -2\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i64_const(0) == 0\n; run: %sdiv_i64_const(-1) == 0\n; run: %sdiv_i64_const(0xFFFFFFFF_FFFFFFFE) == 1\n\nfunction %sdiv_i32(i32, i32) -> i32 {\nblock0(v0: i32,v1: i32):\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i32(0, 1) == 0\n; run: %sdiv_i32(2, 2) == 1\n; run: %sdiv_i32(1, -1) == -1\n; run: %sdiv_i32(3, 2) == 1\n; run: %sdiv_i32(19, 7) == 2\n; run: %sdiv_i32(3, -2) == -1\n; run: %sdiv_i32(-19, 7) == -2\n; run: %sdiv_i32(0xC0FFEEEE, 8) == 0xF81FFDDE\n; run: %sdiv_i32(0xC0FFEEEE, -8) == 0x7E00222\n; run: %sdiv_i32(0x80000000, -2) == 0x40000000\n\nfunction %sdiv_i32_const(i32) -> i32 {\nblock0(v0: i32):\n    v1 = iconst.i32 -2\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i32_const(0) == 0\n; run: %sdiv_i32_const(-1) == 0\n; run: %sdiv_i32_const(0xFFFFFFFE) == 1\n\nfunction %sdiv_i16(i16, i16) -> i16 {\nblock0(v0: i16,v1: i16):\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i16(0, 1) == 0\n; run: %sdiv_i16(2, 2) == 1\n; run: %sdiv_i16(1, -1) == -1\n; run: %sdiv_i16(3, 2) == 1\n; run: %sdiv_i16(19, 7) == 2\n; run: %sdiv_i16(3, -2) == -1\n; run: %sdiv_i16(-19, 7) == -2\n; run: %sdiv_i16(0xC0FF, 8) == 0xF820\n; run: %sdiv_i16(0xC0FF, -8) == 0x07E0\n; run: %sdiv_i16(0x8000, -2) == 0x4000\n\nfunction %sdiv_i16_const(i16) -> i16 {\nblock0(v0: i16):\n    v1 = iconst.i16 -2\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i16_const(0) == 0\n; run: %sdiv_i16_const(-1) == 0\n; run: %sdiv_i16_const(0xFFFE) == 1\n\nfunction %sdiv_i8(i8, i8) -> i8 {\nblock0(v0: i8,v1: i8):\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i8(0, 1) == 0\n; run: %sdiv_i8(2, 2) == 1\n; run: %sdiv_i8(1, -1) == -1\n; run: %sdiv_i8(3, 2) == 1\n; run: %sdiv_i8(19, 7) == 2\n; run: %sdiv_i8(3, -2) == -1\n; run: %sdiv_i8(-19, 7) == -2\n; run: %sdiv_i8(0xC0, 8) == 0xF8\n; run: %sdiv_i8(0xC0, -8) == 0x08\n; run: %sdiv_i8(0x80, -2) == 0x40\n\nfunction %sdiv_i8_const(i8) -> i8 {\nblock0(v0: i8):\n    v1 = iconst.i8 -2\n    v2 = sdiv v0, v1\n    return v2\n}\n; run: %sdiv_i8_const(0) == 0\n; run: %sdiv_i8_const(-1) == 0\n; run: %sdiv_i8_const(0xFE) == 1\n\nfunction %udiv_i64(i64, i64) -> i64 {\nblock0(v0: i64,v1: i64):\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i64(0, 1) == 0\n; run: %udiv_i64(2, 2) == 1\n; run: %udiv_i64(1, -1) == 0\n; run: %udiv_i64(3, 2) == 1\n; run: %udiv_i64(19, 7) == 2\n; run: %udiv_i64(3, -2) == 0\n; run: %udiv_i64(-19, 7) == 0x24924924_9249248F\n; run: %udiv_i64(0xC0FFEEEE_DECAFFFF, 8) == 0x181FFDDD_DBD95FFF\n; run: %udiv_i64(0xC0FFEEEE_DECAFFFF, -8) == 0\n; run: %udiv_i64(0x80000000_00000000, -1) == 0\n; run: %udiv_i64(0x80000000_00000000, -2) == 0\n\nfunction %udiv_i64_const(i64) -> i64 {\nblock0(v0: i64):\n    v1 = iconst.i64 -2\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i64_const(0) == 0\n; run: %udiv_i64_const(-1) == 1\n; run: %udiv_i64_const(0xFFFFFFFF_FFFFFFFE) == 1\n\nfunction %udiv_i32(i32, i32) -> i32 {\nblock0(v0: i32,v1: i32):\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i32(0, 1) == 0\n; run: %udiv_i32(2, 2) == 1\n; run: %udiv_i32(1, -1) == 0\n; run: %udiv_i32(3, 2) == 1\n; run: %udiv_i32(19, 7) == 2\n; run: %udiv_i32(3, -2) == 0\n; run: %udiv_i32(-19, 7) == 0x24924921\n; run: %udiv_i32(0xC0FFEEEE, 8) == 0x181FFDDD\n; run: %udiv_i32(0xC0FFEEEE, -8) == 0\n; run: %udiv_i32(0x80000000, -1) == 0\n; run: %udiv_i32(0x80000000, -2) == 0\n\nfunction %udiv_i32_const(i32) -> i32 {\nblock0(v0: i32):\n    v1 = iconst.i32 -2\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i32_const(0) == 0\n; run: %udiv_i32_const(-1) == 1\n; run: %udiv_i32_const(0xFFFFFFFE) == 1\n\nfunction %udiv_i16(i16, i16) -> i16 {\nblock0(v0: i16,v1: i16):\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i16(0, 1) == 0\n; run: %udiv_i16(2, 2) == 1\n; run: %udiv_i16(1, -1) == 0\n; run: %udiv_i16(3, 2) == 1\n; run: %udiv_i16(19, 7) == 2\n; run: %udiv_i16(3, -2) == 0\n; run: %udiv_i16(-19, 7) == 0x248F\n; run: %udiv_i16(0xC0FF, 8) == 0x181F\n; run: %udiv_i16(0xC0FF, -8) == 0\n; run: %udiv_i16(0x8000, -1) == 0\n; run: %udiv_i16(0x8000, -2) == 0\n\nfunction %udiv_i16_const(i16) -> i16 {\nblock0(v0: i16):\n    v1 = iconst.i16 -2\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i16_const(0) == 0\n; run: %udiv_i16_const(-1) == 1\n; run: %udiv_i16_const(0xFFFE) == 1\n\nfunction %udiv_i8(i8, i8) -> i8 {\nblock0(v0: i8,v1: i8):\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i8(0, 1) == 0\n; run: %udiv_i8(2, 2) == 1\n; run: %udiv_i8(1, -1) == 0\n; run: %udiv_i8(3, 2) == 1\n; run: %udiv_i8(19, 7) == 2\n; run: %udiv_i8(3, -2) == 0\n; run: %udiv_i8(-19, 7) == 0x21\n; run: %udiv_i8(0xC0, 8) == 0x18\n; run: %udiv_i8(0xC0, -8) == 0\n; run: %udiv_i8(0x80, -1) == 0\n; run: %udiv_i8(0x80, -2) == 0\n\nfunction %udiv_i8_const(i8) -> i8 {\nblock0(v0: i8):\n    v1 = iconst.i8 -2\n    v2 = udiv v0, v1\n    return v2\n}\n; run: %udiv_i8_const(0) == 0\n; run: %udiv_i8_const(-1) == 1\n; run: %udiv_i8_const(0xFE) == 1\n"], "filenames": ["cranelift/codegen/src/isa/aarch64/inst.isle", "cranelift/codegen/src/isa/aarch64/inst/imms.rs", "cranelift/codegen/src/isa/aarch64/lower.isle", "cranelift/codegen/src/isa/aarch64/lower/isle.rs", "cranelift/filetests/filetests/isa/aarch64/arithmetic.clif", "cranelift/filetests/filetests/runtests/arithmetic.clif"], "buggy_code_start_loc": [1328, 619, 10, 78, 77, 349], "buggy_code_end_loc": [2049, 619, 1341, 156, 464, 465], "fixing_code_start_loc": [1327, 620, 10, 77, 77, 350], "fixing_code_end_loc": [2052, 624, 1341, 183, 464, 545], "type": "CWE-682", "message": "Wasmtime is a standalone runtime for WebAssembly. There is a bug in Wasmtime's code generator, Cranelift, for AArch64 targets where constant divisors can result in incorrect division results at runtime. This affects Wasmtime prior to version 0.38.2 and Cranelift prior to 0.85.2. This issue only affects the AArch64 platform. Other platforms are not affected. The translation rules for constants did not take into account whether sign or zero-extension should happen which resulted in an incorrect value being placed into a register when a division was encountered. The impact of this bug is that programs executing within the WebAssembly sandbox would not behave according to the WebAssembly specification. This means that it is hypothetically possible for execution within the sandbox to go awry and WebAssembly programs could produce unexpected results. This should not impact hosts executing WebAssembly but does affect the correctness of guest programs. This bug has been patched in Wasmtime version 0.38.2 and cranelift-codegen 0.85.2. There are no known workarounds.", "other": {"cve": {"id": "CVE-2022-31169", "sourceIdentifier": "security-advisories@github.com", "published": "2022-07-22T04:15:14.463", "lastModified": "2022-08-03T18:33:49.627", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Wasmtime is a standalone runtime for WebAssembly. There is a bug in Wasmtime's code generator, Cranelift, for AArch64 targets where constant divisors can result in incorrect division results at runtime. This affects Wasmtime prior to version 0.38.2 and Cranelift prior to 0.85.2. This issue only affects the AArch64 platform. Other platforms are not affected. The translation rules for constants did not take into account whether sign or zero-extension should happen which resulted in an incorrect value being placed into a register when a division was encountered. The impact of this bug is that programs executing within the WebAssembly sandbox would not behave according to the WebAssembly specification. This means that it is hypothetically possible for execution within the sandbox to go awry and WebAssembly programs could produce unexpected results. This should not impact hosts executing WebAssembly but does affect the correctness of guest programs. This bug has been patched in Wasmtime version 0.38.2 and cranelift-codegen 0.85.2. There are no known workarounds."}, {"lang": "es", "value": "Wasmtime es un tiempo de ejecuci\u00f3n independiente para WebAssembly. Se presenta un error en el generador de c\u00f3digo de Wasmtime, Cranelift, para los objetivos AArch64 donde los divisores constantes pueden resultar en resultados de divisi\u00f3n incorrectos en tiempo de ejecuci\u00f3n. Esto afecta a Wasmtime versiones anteriores a 0.38.2 y a Cranelift versiones anteriores a 0.85.2. Este problema s\u00f3lo afecta a la plataforma AArch64. Las dem\u00e1s plataformas no est\u00e1n afectadas. Las reglas de traducci\u00f3n de las constantes no ten\u00edan en cuenta si el signo o la extensi\u00f3n cero deb\u00edan producirse, lo que resultaba en la colocaci\u00f3n de un valor incorrecto en un registro cuando era encontrada una divisi\u00f3n. El impacto de este bug es que los programas que son ejecutados dentro del sandbox de WebAssembly no son comportados de acuerdo con la especificaci\u00f3n de WebAssembly. Esto significa que es hipot\u00e9ticamente posible que la ejecuci\u00f3n dentro del sandbox sea desviado y los programas WebAssembly puedan producir resultados no esperados. Esto no deber\u00eda afectar a los hosts que ejecutan WebAssembly, pero s\u00ed a la correcci\u00f3n de los programas invitados. Este error ha sido parcheado en Wasmtime versi\u00f3n 0.38.2 y en cranelift-codegen versi\u00f3n 0.85.2. No se presentan mitigaciones conocidas"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-682"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:cranelift-codegen:*:*:*:*:*:rust:*:*", "versionEndExcluding": "0.85.1", "matchCriteriaId": "1E1303DC-2166-4DCB-8ABE-208849CDD044"}, {"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:wasmtime:*:*:*:*:*:rust:*:*", "versionEndExcluding": "0.38.1", "matchCriteriaId": "B9C6651A-233F-40DD-AB1B-1A435F97728B"}]}]}], "references": [{"url": "https://github.com/bytecodealliance/wasmtime/commit/2ba4bce5cc719e5a74e571a534424614e62ecc41", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-7f6x-jwh5-m9r4", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/bytecodealliance/wasmtime/commit/2ba4bce5cc719e5a74e571a534424614e62ecc41"}}