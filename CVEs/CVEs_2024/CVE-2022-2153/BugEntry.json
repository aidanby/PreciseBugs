{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n\n/*\n * Local APIC virtualization\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright (C) 2007 Novell\n * Copyright (C) 2007 Intel\n * Copyright 2009 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Dor Laor <dor.laor@qumranet.com>\n *   Gregory Haskins <ghaskins@novell.com>\n *   Yaozu (Eddie) Dong <eddie.dong@intel.com>\n *\n * Based on Xen 3.1 code, Copyright (c) 2004, Intel Corporation.\n */\n\n#include <linux/kvm_host.h>\n#include <linux/kvm.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/smp.h>\n#include <linux/hrtimer.h>\n#include <linux/io.h>\n#include <linux/export.h>\n#include <linux/math64.h>\n#include <linux/slab.h>\n#include <asm/processor.h>\n#include <asm/msr.h>\n#include <asm/page.h>\n#include <asm/current.h>\n#include <asm/apicdef.h>\n#include <asm/delay.h>\n#include <linux/atomic.h>\n#include <linux/jump_label.h>\n#include \"kvm_cache_regs.h\"\n#include \"irq.h\"\n#include \"ioapic.h\"\n#include \"trace.h\"\n#include \"x86.h\"\n#include \"cpuid.h\"\n#include \"hyperv.h\"\n\n#ifndef CONFIG_X86_64\n#define mod_64(x, y) ((x) - (y) * div64_u64(x, y))\n#else\n#define mod_64(x, y) ((x) % (y))\n#endif\n\n#define PRId64 \"d\"\n#define PRIx64 \"llx\"\n#define PRIu64 \"u\"\n#define PRIo64 \"o\"\n\n/* 14 is the version for Xeon and Pentium 8.4.8*/\n#define APIC_VERSION\t\t\t(0x14UL | ((KVM_APIC_LVT_NUM - 1) << 16))\n#define LAPIC_MMIO_LENGTH\t\t(1 << 12)\n/* followed define is not in apicdef.h */\n#define MAX_APIC_VECTOR\t\t\t256\n#define APIC_VECTORS_PER_REG\t\t32\n\nstatic bool lapic_timer_advance_dynamic __read_mostly;\n#define LAPIC_TIMER_ADVANCE_ADJUST_MIN\t100\t/* clock cycles */\n#define LAPIC_TIMER_ADVANCE_ADJUST_MAX\t10000\t/* clock cycles */\n#define LAPIC_TIMER_ADVANCE_NS_INIT\t1000\n#define LAPIC_TIMER_ADVANCE_NS_MAX     5000\n/* step-by-step approximation to mitigate fluctuation */\n#define LAPIC_TIMER_ADVANCE_ADJUST_STEP 8\n\nstatic inline void __kvm_lapic_set_reg(char *regs, int reg_off, u32 val)\n{\n\t*((u32 *) (regs + reg_off)) = val;\n}\n\nstatic inline void kvm_lapic_set_reg(struct kvm_lapic *apic, int reg_off, u32 val)\n{\n\t__kvm_lapic_set_reg(apic->regs, reg_off, val);\n}\n\nstatic __always_inline u64 __kvm_lapic_get_reg64(char *regs, int reg)\n{\n\tBUILD_BUG_ON(reg != APIC_ICR);\n\treturn *((u64 *) (regs + reg));\n}\n\nstatic __always_inline u64 kvm_lapic_get_reg64(struct kvm_lapic *apic, int reg)\n{\n\treturn __kvm_lapic_get_reg64(apic->regs, reg);\n}\n\nstatic __always_inline void __kvm_lapic_set_reg64(char *regs, int reg, u64 val)\n{\n\tBUILD_BUG_ON(reg != APIC_ICR);\n\t*((u64 *) (regs + reg)) = val;\n}\n\nstatic __always_inline void kvm_lapic_set_reg64(struct kvm_lapic *apic,\n\t\t\t\t\t\tint reg, u64 val)\n{\n\t__kvm_lapic_set_reg64(apic->regs, reg, val);\n}\n\nstatic inline int apic_test_vector(int vec, void *bitmap)\n{\n\treturn test_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));\n}\n\nbool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\treturn apic_test_vector(vector, apic->regs + APIC_ISR) ||\n\t\tapic_test_vector(vector, apic->regs + APIC_IRR);\n}\n\nstatic inline int __apic_test_and_set_vector(int vec, void *bitmap)\n{\n\treturn __test_and_set_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));\n}\n\nstatic inline int __apic_test_and_clear_vector(int vec, void *bitmap)\n{\n\treturn __test_and_clear_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));\n}\n\n__read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_hw_disabled, HZ);\n__read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_sw_disabled, HZ);\n\nstatic inline int apic_enabled(struct kvm_lapic *apic)\n{\n\treturn kvm_apic_sw_enabled(apic) &&\tkvm_apic_hw_enabled(apic);\n}\n\n#define LVT_MASK\t\\\n\t(APIC_LVT_MASKED | APIC_SEND_PENDING | APIC_VECTOR_MASK)\n\n#define LINT_MASK\t\\\n\t(LVT_MASK | APIC_MODE_MASK | APIC_INPUT_POLARITY | \\\n\t APIC_LVT_REMOTE_IRR | APIC_LVT_LEVEL_TRIGGER)\n\nstatic inline u32 kvm_x2apic_id(struct kvm_lapic *apic)\n{\n\treturn apic->vcpu->vcpu_id;\n}\n\nstatic bool kvm_can_post_timer_interrupt(struct kvm_vcpu *vcpu)\n{\n\treturn pi_inject_timer && kvm_vcpu_apicv_active(vcpu) &&\n\t\t(kvm_mwait_in_guest(vcpu->kvm) || kvm_hlt_in_guest(vcpu->kvm));\n}\n\nbool kvm_can_use_hv_timer(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_x86_ops.set_hv_timer\n\t       && !(kvm_mwait_in_guest(vcpu->kvm) ||\n\t\t    kvm_can_post_timer_interrupt(vcpu));\n}\nEXPORT_SYMBOL_GPL(kvm_can_use_hv_timer);\n\nstatic bool kvm_use_posted_timer_interrupt(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;\n}\n\nstatic inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,\n\t\tu32 dest_id, struct kvm_lapic ***cluster, u16 *mask) {\n\tswitch (map->mode) {\n\tcase KVM_APIC_MODE_X2APIC: {\n\t\tu32 offset = (dest_id >> 16) * 16;\n\t\tu32 max_apic_id = map->max_apic_id;\n\n\t\tif (offset <= max_apic_id) {\n\t\t\tu8 cluster_size = min(max_apic_id - offset + 1, 16U);\n\n\t\t\toffset = array_index_nospec(offset, map->max_apic_id + 1);\n\t\t\t*cluster = &map->phys_map[offset];\n\t\t\t*mask = dest_id & (0xffff >> (16 - cluster_size));\n\t\t} else {\n\t\t\t*mask = 0;\n\t\t}\n\n\t\treturn true;\n\t\t}\n\tcase KVM_APIC_MODE_XAPIC_FLAT:\n\t\t*cluster = map->xapic_flat_map;\n\t\t*mask = dest_id & 0xff;\n\t\treturn true;\n\tcase KVM_APIC_MODE_XAPIC_CLUSTER:\n\t\t*cluster = map->xapic_cluster_map[(dest_id >> 4) & 0xf];\n\t\t*mask = dest_id & 0xf;\n\t\treturn true;\n\tdefault:\n\t\t/* Not optimized. */\n\t\treturn false;\n\t}\n}\n\nstatic void kvm_apic_map_free(struct rcu_head *rcu)\n{\n\tstruct kvm_apic_map *map = container_of(rcu, struct kvm_apic_map, rcu);\n\n\tkvfree(map);\n}\n\n/*\n * CLEAN -> DIRTY and UPDATE_IN_PROGRESS -> DIRTY changes happen without a lock.\n *\n * DIRTY -> UPDATE_IN_PROGRESS and UPDATE_IN_PROGRESS -> CLEAN happen with\n * apic_map_lock_held.\n */\nenum {\n\tCLEAN,\n\tUPDATE_IN_PROGRESS,\n\tDIRTY\n};\n\nvoid kvm_recalculate_apic_map(struct kvm *kvm)\n{\n\tstruct kvm_apic_map *new, *old = NULL;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\tu32 max_id = 255; /* enough space for any xAPIC ID */\n\n\t/* Read kvm->arch.apic_map_dirty before kvm->arch.apic_map.  */\n\tif (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)\n\t\treturn;\n\n\tWARN_ONCE(!irqchip_in_kernel(kvm),\n\t\t  \"Dirty APIC map without an in-kernel local APIC\");\n\n\tmutex_lock(&kvm->arch.apic_map_lock);\n\t/*\n\t * Read kvm->arch.apic_map_dirty before kvm->arch.apic_map\n\t * (if clean) or the APIC registers (if dirty).\n\t */\n\tif (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,\n\t\t\t\t   DIRTY, UPDATE_IN_PROGRESS) == CLEAN) {\n\t\t/* Someone else has updated the map. */\n\t\tmutex_unlock(&kvm->arch.apic_map_lock);\n\t\treturn;\n\t}\n\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tif (kvm_apic_present(vcpu))\n\t\t\tmax_id = max(max_id, kvm_x2apic_id(vcpu->arch.apic));\n\n\tnew = kvzalloc(sizeof(struct kvm_apic_map) +\n\t                   sizeof(struct kvm_lapic *) * ((u64)max_id + 1),\n\t\t\t   GFP_KERNEL_ACCOUNT);\n\n\tif (!new)\n\t\tgoto out;\n\n\tnew->max_apic_id = max_id;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\t\tstruct kvm_lapic **cluster;\n\t\tu16 mask;\n\t\tu32 ldr;\n\t\tu8 xapic_id;\n\t\tu32 x2apic_id;\n\n\t\tif (!kvm_apic_present(vcpu))\n\t\t\tcontinue;\n\n\t\txapic_id = kvm_xapic_id(apic);\n\t\tx2apic_id = kvm_x2apic_id(apic);\n\n\t\t/* Hotplug hack: see kvm_apic_match_physical_addr(), ... */\n\t\tif ((apic_x2apic_mode(apic) || x2apic_id > 0xff) &&\n\t\t\t\tx2apic_id <= new->max_apic_id)\n\t\t\tnew->phys_map[x2apic_id] = apic;\n\t\t/*\n\t\t * ... xAPIC ID of VCPUs with APIC ID > 0xff will wrap-around,\n\t\t * prevent them from masking VCPUs with APIC ID <= 0xff.\n\t\t */\n\t\tif (!apic_x2apic_mode(apic) && !new->phys_map[xapic_id])\n\t\t\tnew->phys_map[xapic_id] = apic;\n\n\t\tif (!kvm_apic_sw_enabled(apic))\n\t\t\tcontinue;\n\n\t\tldr = kvm_lapic_get_reg(apic, APIC_LDR);\n\n\t\tif (apic_x2apic_mode(apic)) {\n\t\t\tnew->mode |= KVM_APIC_MODE_X2APIC;\n\t\t} else if (ldr) {\n\t\t\tldr = GET_APIC_LOGICAL_ID(ldr);\n\t\t\tif (kvm_lapic_get_reg(apic, APIC_DFR) == APIC_DFR_FLAT)\n\t\t\t\tnew->mode |= KVM_APIC_MODE_XAPIC_FLAT;\n\t\t\telse\n\t\t\t\tnew->mode |= KVM_APIC_MODE_XAPIC_CLUSTER;\n\t\t}\n\n\t\tif (!kvm_apic_map_get_logical_dest(new, ldr, &cluster, &mask))\n\t\t\tcontinue;\n\n\t\tif (mask)\n\t\t\tcluster[ffs(mask) - 1] = apic;\n\t}\nout:\n\told = rcu_dereference_protected(kvm->arch.apic_map,\n\t\t\tlockdep_is_held(&kvm->arch.apic_map_lock));\n\trcu_assign_pointer(kvm->arch.apic_map, new);\n\t/*\n\t * Write kvm->arch.apic_map before clearing apic->apic_map_dirty.\n\t * If another update has come in, leave it DIRTY.\n\t */\n\tatomic_cmpxchg_release(&kvm->arch.apic_map_dirty,\n\t\t\t       UPDATE_IN_PROGRESS, CLEAN);\n\tmutex_unlock(&kvm->arch.apic_map_lock);\n\n\tif (old)\n\t\tcall_rcu(&old->rcu, kvm_apic_map_free);\n\n\tkvm_make_scan_ioapic_request(kvm);\n}\n\nstatic inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)\n{\n\tbool enabled = val & APIC_SPIV_APIC_ENABLED;\n\n\tkvm_lapic_set_reg(apic, APIC_SPIV, val);\n\n\tif (enabled != apic->sw_enabled) {\n\t\tapic->sw_enabled = enabled;\n\t\tif (enabled)\n\t\t\tstatic_branch_slow_dec_deferred(&apic_sw_disabled);\n\t\telse\n\t\t\tstatic_branch_inc(&apic_sw_disabled.key);\n\n\t\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n\t}\n\n\t/* Check if there are APF page ready requests pending */\n\tif (enabled)\n\t\tkvm_make_request(KVM_REQ_APF_READY, apic->vcpu);\n}\n\nstatic inline void kvm_apic_set_xapic_id(struct kvm_lapic *apic, u8 id)\n{\n\tkvm_lapic_set_reg(apic, APIC_ID, id << 24);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline void kvm_apic_set_ldr(struct kvm_lapic *apic, u32 id)\n{\n\tkvm_lapic_set_reg(apic, APIC_LDR, id);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline void kvm_apic_set_dfr(struct kvm_lapic *apic, u32 val)\n{\n\tkvm_lapic_set_reg(apic, APIC_DFR, val);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline u32 kvm_apic_calc_x2apic_ldr(u32 id)\n{\n\treturn ((id >> 4) << 16) | (1 << (id & 0xf));\n}\n\nstatic inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)\n{\n\tu32 ldr = kvm_apic_calc_x2apic_ldr(id);\n\n\tWARN_ON_ONCE(id != apic->vcpu->vcpu_id);\n\n\tkvm_lapic_set_reg(apic, APIC_ID, id);\n\tkvm_lapic_set_reg(apic, APIC_LDR, ldr);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline int apic_lvt_enabled(struct kvm_lapic *apic, int lvt_type)\n{\n\treturn !(kvm_lapic_get_reg(apic, lvt_type) & APIC_LVT_MASKED);\n}\n\nstatic inline int apic_lvtt_oneshot(struct kvm_lapic *apic)\n{\n\treturn apic->lapic_timer.timer_mode == APIC_LVT_TIMER_ONESHOT;\n}\n\nstatic inline int apic_lvtt_period(struct kvm_lapic *apic)\n{\n\treturn apic->lapic_timer.timer_mode == APIC_LVT_TIMER_PERIODIC;\n}\n\nstatic inline int apic_lvtt_tscdeadline(struct kvm_lapic *apic)\n{\n\treturn apic->lapic_timer.timer_mode == APIC_LVT_TIMER_TSCDEADLINE;\n}\n\nstatic inline int apic_lvt_nmi_mode(u32 lvt_val)\n{\n\treturn (lvt_val & (APIC_MODE_MASK | APIC_LVT_MASKED)) == APIC_DM_NMI;\n}\n\nvoid kvm_apic_set_version(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 v = APIC_VERSION;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\t/*\n\t * KVM emulates 82093AA datasheet (with in-kernel IOAPIC implementation)\n\t * which doesn't have EOI register; Some buggy OSes (e.g. Windows with\n\t * Hyper-V role) disable EOI broadcast in lapic not checking for IOAPIC\n\t * version first and level-triggered interrupts never get EOIed in\n\t * IOAPIC.\n\t */\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_X2APIC) &&\n\t    !ioapic_in_kernel(vcpu->kvm))\n\t\tv |= APIC_LVR_DIRECTED_EOI;\n\tkvm_lapic_set_reg(apic, APIC_LVR, v);\n}\n\nstatic const unsigned int apic_lvt_mask[KVM_APIC_LVT_NUM] = {\n\tLVT_MASK ,      /* part LVTT mask, timer mode mask added at runtime */\n\tLVT_MASK | APIC_MODE_MASK,\t/* LVTTHMR */\n\tLVT_MASK | APIC_MODE_MASK,\t/* LVTPC */\n\tLINT_MASK, LINT_MASK,\t/* LVT0-1 */\n\tLVT_MASK\t\t/* LVTERR */\n};\n\nstatic int find_highest_vector(void *bitmap)\n{\n\tint vec;\n\tu32 *reg;\n\n\tfor (vec = MAX_APIC_VECTOR - APIC_VECTORS_PER_REG;\n\t     vec >= 0; vec -= APIC_VECTORS_PER_REG) {\n\t\treg = bitmap + REG_POS(vec);\n\t\tif (*reg)\n\t\t\treturn __fls(*reg) + vec;\n\t}\n\n\treturn -1;\n}\n\nstatic u8 count_vectors(void *bitmap)\n{\n\tint vec;\n\tu32 *reg;\n\tu8 count = 0;\n\n\tfor (vec = 0; vec < MAX_APIC_VECTOR; vec += APIC_VECTORS_PER_REG) {\n\t\treg = bitmap + REG_POS(vec);\n\t\tcount += hweight32(*reg);\n\t}\n\n\treturn count;\n}\n\nbool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)\n{\n\tu32 i, vec;\n\tu32 pir_val, irr_val, prev_irr_val;\n\tint max_updated_irr;\n\n\tmax_updated_irr = -1;\n\t*max_irr = -1;\n\n\tfor (i = vec = 0; i <= 7; i++, vec += 32) {\n\t\tpir_val = READ_ONCE(pir[i]);\n\t\tirr_val = *((u32 *)(regs + APIC_IRR + i * 0x10));\n\t\tif (pir_val) {\n\t\t\tprev_irr_val = irr_val;\n\t\t\tirr_val |= xchg(&pir[i], 0);\n\t\t\t*((u32 *)(regs + APIC_IRR + i * 0x10)) = irr_val;\n\t\t\tif (prev_irr_val != irr_val) {\n\t\t\t\tmax_updated_irr =\n\t\t\t\t\t__fls(irr_val ^ prev_irr_val) + vec;\n\t\t\t}\n\t\t}\n\t\tif (irr_val)\n\t\t\t*max_irr = __fls(irr_val) + vec;\n\t}\n\n\treturn ((max_updated_irr != -1) &&\n\t\t(max_updated_irr == *max_irr));\n}\nEXPORT_SYMBOL_GPL(__kvm_apic_update_irr);\n\nbool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\treturn __kvm_apic_update_irr(pir, apic->regs, max_irr);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_update_irr);\n\nstatic inline int apic_search_irr(struct kvm_lapic *apic)\n{\n\treturn find_highest_vector(apic->regs + APIC_IRR);\n}\n\nstatic inline int apic_find_highest_irr(struct kvm_lapic *apic)\n{\n\tint result;\n\n\t/*\n\t * Note that irr_pending is just a hint. It will be always\n\t * true with virtual interrupt delivery enabled.\n\t */\n\tif (!apic->irr_pending)\n\t\treturn -1;\n\n\tresult = apic_search_irr(apic);\n\tASSERT(result == -1 || result >= 16);\n\n\treturn result;\n}\n\nstatic inline void apic_clear_irr(int vec, struct kvm_lapic *apic)\n{\n\tstruct kvm_vcpu *vcpu;\n\n\tvcpu = apic->vcpu;\n\n\tif (unlikely(vcpu->arch.apicv_active)) {\n\t\t/* need to update RVI */\n\t\tkvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);\n\t\tstatic_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));\n\t} else {\n\t\tapic->irr_pending = false;\n\t\tkvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);\n\t\tif (apic_search_irr(apic) != -1)\n\t\t\tapic->irr_pending = true;\n\t}\n}\n\nvoid kvm_apic_clear_irr(struct kvm_vcpu *vcpu, int vec)\n{\n\tapic_clear_irr(vec, vcpu->arch.apic);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_clear_irr);\n\nstatic inline void apic_set_isr(int vec, struct kvm_lapic *apic)\n{\n\tstruct kvm_vcpu *vcpu;\n\n\tif (__apic_test_and_set_vector(vec, apic->regs + APIC_ISR))\n\t\treturn;\n\n\tvcpu = apic->vcpu;\n\n\t/*\n\t * With APIC virtualization enabled, all caching is disabled\n\t * because the processor can modify ISR under the hood.  Instead\n\t * just set SVI.\n\t */\n\tif (unlikely(vcpu->arch.apicv_active))\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(vcpu, vec);\n\telse {\n\t\t++apic->isr_count;\n\t\tBUG_ON(apic->isr_count > MAX_APIC_VECTOR);\n\t\t/*\n\t\t * ISR (in service register) bit is set when injecting an interrupt.\n\t\t * The highest vector is injected. Thus the latest bit set matches\n\t\t * the highest bit in ISR.\n\t\t */\n\t\tapic->highest_isr_cache = vec;\n\t}\n}\n\nstatic inline int apic_find_highest_isr(struct kvm_lapic *apic)\n{\n\tint result;\n\n\t/*\n\t * Note that isr_count is always 1, and highest_isr_cache\n\t * is always -1, with APIC virtualization enabled.\n\t */\n\tif (!apic->isr_count)\n\t\treturn -1;\n\tif (likely(apic->highest_isr_cache != -1))\n\t\treturn apic->highest_isr_cache;\n\n\tresult = find_highest_vector(apic->regs + APIC_ISR);\n\tASSERT(result == -1 || result >= 16);\n\n\treturn result;\n}\n\nstatic inline void apic_clear_isr(int vec, struct kvm_lapic *apic)\n{\n\tstruct kvm_vcpu *vcpu;\n\tif (!__apic_test_and_clear_vector(vec, apic->regs + APIC_ISR))\n\t\treturn;\n\n\tvcpu = apic->vcpu;\n\n\t/*\n\t * We do get here for APIC virtualization enabled if the guest\n\t * uses the Hyper-V APIC enlightenment.  In this case we may need\n\t * to trigger a new interrupt delivery by writing the SVI field;\n\t * on the other hand isr_count and highest_isr_cache are unused\n\t * and must be left alone.\n\t */\n\tif (unlikely(vcpu->arch.apicv_active))\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));\n\telse {\n\t\t--apic->isr_count;\n\t\tBUG_ON(apic->isr_count < 0);\n\t\tapic->highest_isr_cache = -1;\n\t}\n}\n\nint kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)\n{\n\t/* This may race with setting of irr in __apic_accept_irq() and\n\t * value returned may be wrong, but kvm_vcpu_kick() in __apic_accept_irq\n\t * will cause vmexit immediately and the value will be recalculated\n\t * on the next vmentry.\n\t */\n\treturn apic_find_highest_irr(vcpu->arch.apic);\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_find_highest_irr);\n\nstatic int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,\n\t\t\t     int vector, int level, int trig_mode,\n\t\t\t     struct dest_map *dest_map);\n\nint kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,\n\t\t     struct dest_map *dest_map)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\treturn __apic_accept_irq(apic, irq->delivery_mode, irq->vector,\n\t\t\tirq->level, irq->trig_mode, dest_map);\n}\n\nstatic int __pv_send_ipi(unsigned long *ipi_bitmap, struct kvm_apic_map *map,\n\t\t\t struct kvm_lapic_irq *irq, u32 min)\n{\n\tint i, count = 0;\n\tstruct kvm_vcpu *vcpu;\n\n\tif (min > map->max_apic_id)\n\t\treturn 0;\n\n\tfor_each_set_bit(i, ipi_bitmap,\n\t\tmin((u32)BITS_PER_LONG, (map->max_apic_id - min + 1))) {\n\t\tif (map->phys_map[min + i]) {\n\t\t\tvcpu = map->phys_map[min + i]->vcpu;\n\t\t\tcount += kvm_apic_set_irq(vcpu, irq, NULL);\n\t\t}\n\t}\n\n\treturn count;\n}\n\nint kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,\n\t\t    unsigned long ipi_bitmap_high, u32 min,\n\t\t    unsigned long icr, int op_64_bit)\n{\n\tstruct kvm_apic_map *map;\n\tstruct kvm_lapic_irq irq = {0};\n\tint cluster_size = op_64_bit ? 64 : 32;\n\tint count;\n\n\tif (icr & (APIC_DEST_MASK | APIC_SHORT_MASK))\n\t\treturn -KVM_EINVAL;\n\n\tirq.vector = icr & APIC_VECTOR_MASK;\n\tirq.delivery_mode = icr & APIC_MODE_MASK;\n\tirq.level = (icr & APIC_INT_ASSERT) != 0;\n\tirq.trig_mode = icr & APIC_INT_LEVELTRIG;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tcount = -EOPNOTSUPP;\n\tif (likely(map)) {\n\t\tcount = __pv_send_ipi(&ipi_bitmap_low, map, &irq, min);\n\t\tmin += cluster_size;\n\t\tcount += __pv_send_ipi(&ipi_bitmap_high, map, &irq, min);\n\t}\n\n\trcu_read_unlock();\n\treturn count;\n}\n\nstatic int pv_eoi_put_user(struct kvm_vcpu *vcpu, u8 val)\n{\n\n\treturn kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.pv_eoi.data, &val,\n\t\t\t\t      sizeof(val));\n}\n\nstatic int pv_eoi_get_user(struct kvm_vcpu *vcpu, u8 *val)\n{\n\n\treturn kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.pv_eoi.data, val,\n\t\t\t\t      sizeof(*val));\n}\n\nstatic inline bool pv_eoi_enabled(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.pv_eoi.msr_val & KVM_MSR_ENABLED;\n}\n\nstatic void pv_eoi_set_pending(struct kvm_vcpu *vcpu)\n{\n\tif (pv_eoi_put_user(vcpu, KVM_PV_EOI_ENABLED) < 0)\n\t\treturn;\n\n\t__set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);\n}\n\nstatic bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)\n{\n\tu8 val;\n\n\tif (pv_eoi_get_user(vcpu, &val) < 0)\n\t\treturn false;\n\n\tval &= KVM_PV_EOI_ENABLED;\n\n\tif (val && pv_eoi_put_user(vcpu, KVM_PV_EOI_DISABLED) < 0)\n\t\treturn false;\n\n\t/*\n\t * Clear pending bit in any case: it will be set again on vmentry.\n\t * While this might not be ideal from performance point of view,\n\t * this makes sure pv eoi is only enabled when we know it's safe.\n\t */\n\t__clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);\n\n\treturn val;\n}\n\nstatic int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)\n{\n\tint highest_irr;\n\tif (kvm_x86_ops.sync_pir_to_irr)\n\t\thighest_irr = static_call(kvm_x86_sync_pir_to_irr)(apic->vcpu);\n\telse\n\t\thighest_irr = apic_find_highest_irr(apic);\n\tif (highest_irr == -1 || (highest_irr & 0xF0) <= ppr)\n\t\treturn -1;\n\treturn highest_irr;\n}\n\nstatic bool __apic_update_ppr(struct kvm_lapic *apic, u32 *new_ppr)\n{\n\tu32 tpr, isrv, ppr, old_ppr;\n\tint isr;\n\n\told_ppr = kvm_lapic_get_reg(apic, APIC_PROCPRI);\n\ttpr = kvm_lapic_get_reg(apic, APIC_TASKPRI);\n\tisr = apic_find_highest_isr(apic);\n\tisrv = (isr != -1) ? isr : 0;\n\n\tif ((tpr & 0xf0) >= (isrv & 0xf0))\n\t\tppr = tpr & 0xff;\n\telse\n\t\tppr = isrv & 0xf0;\n\n\t*new_ppr = ppr;\n\tif (old_ppr != ppr)\n\t\tkvm_lapic_set_reg(apic, APIC_PROCPRI, ppr);\n\n\treturn ppr < old_ppr;\n}\n\nstatic void apic_update_ppr(struct kvm_lapic *apic)\n{\n\tu32 ppr;\n\n\tif (__apic_update_ppr(apic, &ppr) &&\n\t    apic_has_interrupt_for_ppr(apic, ppr) != -1)\n\t\tkvm_make_request(KVM_REQ_EVENT, apic->vcpu);\n}\n\nvoid kvm_apic_update_ppr(struct kvm_vcpu *vcpu)\n{\n\tapic_update_ppr(vcpu->arch.apic);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_update_ppr);\n\nstatic void apic_set_tpr(struct kvm_lapic *apic, u32 tpr)\n{\n\tkvm_lapic_set_reg(apic, APIC_TASKPRI, tpr);\n\tapic_update_ppr(apic);\n}\n\nstatic bool kvm_apic_broadcast(struct kvm_lapic *apic, u32 mda)\n{\n\treturn mda == (apic_x2apic_mode(apic) ?\n\t\t\tX2APIC_BROADCAST : APIC_BROADCAST);\n}\n\nstatic bool kvm_apic_match_physical_addr(struct kvm_lapic *apic, u32 mda)\n{\n\tif (kvm_apic_broadcast(apic, mda))\n\t\treturn true;\n\n\tif (apic_x2apic_mode(apic))\n\t\treturn mda == kvm_x2apic_id(apic);\n\n\t/*\n\t * Hotplug hack: Make LAPIC in xAPIC mode also accept interrupts as if\n\t * it were in x2APIC mode.  Hotplugged VCPUs start in xAPIC mode and\n\t * this allows unique addressing of VCPUs with APIC ID over 0xff.\n\t * The 0xff condition is needed because writeable xAPIC ID.\n\t */\n\tif (kvm_x2apic_id(apic) > 0xff && mda == kvm_x2apic_id(apic))\n\t\treturn true;\n\n\treturn mda == kvm_xapic_id(apic);\n}\n\nstatic bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)\n{\n\tu32 logical_id;\n\n\tif (kvm_apic_broadcast(apic, mda))\n\t\treturn true;\n\n\tlogical_id = kvm_lapic_get_reg(apic, APIC_LDR);\n\n\tif (apic_x2apic_mode(apic))\n\t\treturn ((logical_id >> 16) == (mda >> 16))\n\t\t       && (logical_id & mda & 0xffff) != 0;\n\n\tlogical_id = GET_APIC_LOGICAL_ID(logical_id);\n\n\tswitch (kvm_lapic_get_reg(apic, APIC_DFR)) {\n\tcase APIC_DFR_FLAT:\n\t\treturn (logical_id & mda) != 0;\n\tcase APIC_DFR_CLUSTER:\n\t\treturn ((logical_id >> 4) == (mda >> 4))\n\t\t       && (logical_id & mda & 0xf) != 0;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* The KVM local APIC implementation has two quirks:\n *\n *  - Real hardware delivers interrupts destined to x2APIC ID > 0xff to LAPICs\n *    in xAPIC mode if the \"destination & 0xff\" matches its xAPIC ID.\n *    KVM doesn't do that aliasing.\n *\n *  - in-kernel IOAPIC messages have to be delivered directly to\n *    x2APIC, because the kernel does not support interrupt remapping.\n *    In order to support broadcast without interrupt remapping, x2APIC\n *    rewrites the destination of non-IPI messages from APIC_BROADCAST\n *    to X2APIC_BROADCAST.\n *\n * The broadcast quirk can be disabled with KVM_CAP_X2APIC_API.  This is\n * important when userspace wants to use x2APIC-format MSIs, because\n * APIC_BROADCAST (0xff) is a legal route for \"cluster 0, CPUs 0-7\".\n */\nstatic u32 kvm_apic_mda(struct kvm_vcpu *vcpu, unsigned int dest_id,\n\t\tstruct kvm_lapic *source, struct kvm_lapic *target)\n{\n\tbool ipi = source != NULL;\n\n\tif (!vcpu->kvm->arch.x2apic_broadcast_quirk_disabled &&\n\t    !ipi && dest_id == APIC_BROADCAST && apic_x2apic_mode(target))\n\t\treturn X2APIC_BROADCAST;\n\n\treturn dest_id;\n}\n\nbool kvm_apic_match_dest(struct kvm_vcpu *vcpu, struct kvm_lapic *source,\n\t\t\t   int shorthand, unsigned int dest, int dest_mode)\n{\n\tstruct kvm_lapic *target = vcpu->arch.apic;\n\tu32 mda = kvm_apic_mda(vcpu, dest, source, target);\n\n\tASSERT(target);\n\tswitch (shorthand) {\n\tcase APIC_DEST_NOSHORT:\n\t\tif (dest_mode == APIC_DEST_PHYSICAL)\n\t\t\treturn kvm_apic_match_physical_addr(target, mda);\n\t\telse\n\t\t\treturn kvm_apic_match_logical_addr(target, mda);\n\tcase APIC_DEST_SELF:\n\t\treturn target == source;\n\tcase APIC_DEST_ALLINC:\n\t\treturn true;\n\tcase APIC_DEST_ALLBUT:\n\t\treturn target != source;\n\tdefault:\n\t\treturn false;\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_apic_match_dest);\n\nint kvm_vector_to_index(u32 vector, u32 dest_vcpus,\n\t\t       const unsigned long *bitmap, u32 bitmap_size)\n{\n\tu32 mod;\n\tint i, idx = -1;\n\n\tmod = vector % dest_vcpus;\n\n\tfor (i = 0; i <= mod; i++) {\n\t\tidx = find_next_bit(bitmap, bitmap_size, idx + 1);\n\t\tBUG_ON(idx == bitmap_size);\n\t}\n\n\treturn idx;\n}\n\nstatic void kvm_apic_disabled_lapic_found(struct kvm *kvm)\n{\n\tif (!kvm->arch.disabled_lapic_found) {\n\t\tkvm->arch.disabled_lapic_found = true;\n\t\tprintk(KERN_INFO\n\t\t       \"Disabled LAPIC found during irq injection\\n\");\n\t}\n}\n\nstatic bool kvm_apic_is_broadcast_dest(struct kvm *kvm, struct kvm_lapic **src,\n\t\tstruct kvm_lapic_irq *irq, struct kvm_apic_map *map)\n{\n\tif (kvm->arch.x2apic_broadcast_quirk_disabled) {\n\t\tif ((irq->dest_id == APIC_BROADCAST &&\n\t\t\t\tmap->mode != KVM_APIC_MODE_X2APIC))\n\t\t\treturn true;\n\t\tif (irq->dest_id == X2APIC_BROADCAST)\n\t\t\treturn true;\n\t} else {\n\t\tbool x2apic_ipi = src && *src && apic_x2apic_mode(*src);\n\t\tif (irq->dest_id == (x2apic_ipi ?\n\t\t                     X2APIC_BROADCAST : APIC_BROADCAST))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/* Return true if the interrupt can be handled by using *bitmap as index mask\n * for valid destinations in *dst array.\n * Return false if kvm_apic_map_get_dest_lapic did nothing useful.\n * Note: we may have zero kvm_lapic destinations when we return true, which\n * means that the interrupt should be dropped.  In this case, *bitmap would be\n * zero and *dst undefined.\n */\nstatic inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,\n\t\tstruct kvm_lapic **src, struct kvm_lapic_irq *irq,\n\t\tstruct kvm_apic_map *map, struct kvm_lapic ***dst,\n\t\tunsigned long *bitmap)\n{\n\tint i, lowest;\n\n\tif (irq->shorthand == APIC_DEST_SELF && src) {\n\t\t*dst = src;\n\t\t*bitmap = 1;\n\t\treturn true;\n\t} else if (irq->shorthand)\n\t\treturn false;\n\n\tif (!map || kvm_apic_is_broadcast_dest(kvm, src, irq, map))\n\t\treturn false;\n\n\tif (irq->dest_mode == APIC_DEST_PHYSICAL) {\n\t\tif (irq->dest_id > map->max_apic_id) {\n\t\t\t*bitmap = 0;\n\t\t} else {\n\t\t\tu32 dest_id = array_index_nospec(irq->dest_id, map->max_apic_id + 1);\n\t\t\t*dst = &map->phys_map[dest_id];\n\t\t\t*bitmap = 1;\n\t\t}\n\t\treturn true;\n\t}\n\n\t*bitmap = 0;\n\tif (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,\n\t\t\t\t(u16 *)bitmap))\n\t\treturn false;\n\n\tif (!kvm_lowest_prio_delivery(irq))\n\t\treturn true;\n\n\tif (!kvm_vector_hashing_enabled()) {\n\t\tlowest = -1;\n\t\tfor_each_set_bit(i, bitmap, 16) {\n\t\t\tif (!(*dst)[i])\n\t\t\t\tcontinue;\n\t\t\tif (lowest < 0)\n\t\t\t\tlowest = i;\n\t\t\telse if (kvm_apic_compare_prio((*dst)[i]->vcpu,\n\t\t\t\t\t\t(*dst)[lowest]->vcpu) < 0)\n\t\t\t\tlowest = i;\n\t\t}\n\t} else {\n\t\tif (!*bitmap)\n\t\t\treturn true;\n\n\t\tlowest = kvm_vector_to_index(irq->vector, hweight16(*bitmap),\n\t\t\t\tbitmap, 16);\n\n\t\tif (!(*dst)[lowest]) {\n\t\t\tkvm_apic_disabled_lapic_found(kvm);\n\t\t\t*bitmap = 0;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t*bitmap = (lowest >= 0) ? 1 << lowest : 0;\n\n\treturn true;\n}\n\nbool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,\n\t\tstruct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tint i;\n\tbool ret;\n\n\t*r = -1;\n\n\tif (irq->shorthand == APIC_DEST_SELF) {\n\t\t*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);\n\t\treturn true;\n\t}\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);\n\tif (ret) {\n\t\t*r = 0;\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dst[i])\n\t\t\t\tcontinue;\n\t\t\t*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/*\n * This routine tries to handle interrupts in posted mode, here is how\n * it deals with different cases:\n * - For single-destination interrupts, handle it in posted mode\n * - Else if vector hashing is enabled and it is a lowest-priority\n *   interrupt, handle it in posted mode and use the following mechanism\n *   to find the destination vCPU.\n *\t1. For lowest-priority interrupts, store all the possible\n *\t   destination vCPUs in an array.\n *\t2. Use \"guest vector % max number of destination vCPUs\" to find\n *\t   the right destination vCPU in the array for the lowest-priority\n *\t   interrupt.\n * - Otherwise, use remapped mode to inject the interrupt.\n */\nbool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,\n\t\t\tstruct kvm_vcpu **dest_vcpu)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tbool ret = false;\n\n\tif (irq->shorthand)\n\t\treturn false;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tif (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&\n\t\t\thweight16(bitmap) == 1) {\n\t\tunsigned long i = find_first_bit(&bitmap, 16);\n\n\t\tif (dst[i]) {\n\t\t\t*dest_vcpu = dst[i]->vcpu;\n\t\t\tret = true;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/*\n * Add a pending IRQ into lapic.\n * Return 1 if successfully added and 0 if discarded.\n */\nstatic int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,\n\t\t\t     int vector, int level, int trig_mode,\n\t\t\t     struct dest_map *dest_map)\n{\n\tint result = 0;\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\n\ttrace_kvm_apic_accept_irq(vcpu->vcpu_id, delivery_mode,\n\t\t\t\t  trig_mode, vector);\n\tswitch (delivery_mode) {\n\tcase APIC_DM_LOWEST:\n\t\tvcpu->arch.apic_arb_prio++;\n\t\tfallthrough;\n\tcase APIC_DM_FIXED:\n\t\tif (unlikely(trig_mode && !level))\n\t\t\tbreak;\n\n\t\t/* FIXME add logic for vcpu on reset */\n\t\tif (unlikely(!apic_enabled(apic)))\n\t\t\tbreak;\n\n\t\tresult = 1;\n\n\t\tif (dest_map) {\n\t\t\t__set_bit(vcpu->vcpu_id, dest_map->map);\n\t\t\tdest_map->vectors[vcpu->vcpu_id] = vector;\n\t\t}\n\n\t\tif (apic_test_vector(vector, apic->regs + APIC_TMR) != !!trig_mode) {\n\t\t\tif (trig_mode)\n\t\t\t\tkvm_lapic_set_vector(vector,\n\t\t\t\t\t\t     apic->regs + APIC_TMR);\n\t\t\telse\n\t\t\t\tkvm_lapic_clear_vector(vector,\n\t\t\t\t\t\t       apic->regs + APIC_TMR);\n\t\t}\n\n\t\tstatic_call(kvm_x86_deliver_interrupt)(apic, delivery_mode,\n\t\t\t\t\t\t       trig_mode, vector);\n\t\tbreak;\n\n\tcase APIC_DM_REMRD:\n\t\tresult = 1;\n\t\tvcpu->arch.pv.pv_unhalted = 1;\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_SMI:\n\t\tresult = 1;\n\t\tkvm_make_request(KVM_REQ_SMI, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_NMI:\n\t\tresult = 1;\n\t\tkvm_inject_nmi(vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_INIT:\n\t\tif (!trig_mode || level) {\n\t\t\tresult = 1;\n\t\t\t/* assumes that there are only KVM_APIC_INIT/SIPI */\n\t\t\tapic->pending_events = (1UL << KVM_APIC_INIT);\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\t\tkvm_vcpu_kick(vcpu);\n\t\t}\n\t\tbreak;\n\n\tcase APIC_DM_STARTUP:\n\t\tresult = 1;\n\t\tapic->sipi_vector = vector;\n\t\t/* make sure sipi_vector is visible for the receiver */\n\t\tsmp_wmb();\n\t\tset_bit(KVM_APIC_SIPI, &apic->pending_events);\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_EXTINT:\n\t\t/*\n\t\t * Should only be called by kvm_apic_local_deliver() with LVT0,\n\t\t * before NMI watchdog was enabled. Already handled by\n\t\t * kvm_apic_accept_pic_intr().\n\t\t */\n\t\tbreak;\n\n\tdefault:\n\t\tprintk(KERN_ERR \"TODO: unsupported delivery mode %x\\n\",\n\t\t       delivery_mode);\n\t\tbreak;\n\t}\n\treturn result;\n}\n\n/*\n * This routine identifies the destination vcpus mask meant to receive the\n * IOAPIC interrupts. It either uses kvm_apic_map_get_dest_lapic() to find\n * out the destination vcpus array and set the bitmap or it traverses to\n * each available vcpu to identify the same.\n */\nvoid kvm_bitmap_or_dest_vcpus(struct kvm *kvm, struct kvm_lapic_irq *irq,\n\t\t\t      unsigned long *vcpu_bitmap)\n{\n\tstruct kvm_lapic **dest_vcpu = NULL;\n\tstruct kvm_lapic *src = NULL;\n\tstruct kvm_apic_map *map;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long bitmap, i;\n\tint vcpu_idx;\n\tbool ret;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,\n\t\t\t\t\t  &bitmap);\n\tif (ret) {\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dest_vcpu[i])\n\t\t\t\tcontinue;\n\t\t\tvcpu_idx = dest_vcpu[i]->vcpu->vcpu_idx;\n\t\t\t__set_bit(vcpu_idx, vcpu_bitmap);\n\t\t}\n\t} else {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (!kvm_apic_present(vcpu))\n\t\t\t\tcontinue;\n\t\t\tif (!kvm_apic_match_dest(vcpu, NULL,\n\t\t\t\t\t\t irq->shorthand,\n\t\t\t\t\t\t irq->dest_id,\n\t\t\t\t\t\t irq->dest_mode))\n\t\t\t\tcontinue;\n\t\t\t__set_bit(i, vcpu_bitmap);\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\nint kvm_apic_compare_prio(struct kvm_vcpu *vcpu1, struct kvm_vcpu *vcpu2)\n{\n\treturn vcpu1->arch.apic_arb_prio - vcpu2->arch.apic_arb_prio;\n}\n\nstatic bool kvm_ioapic_handles_vector(struct kvm_lapic *apic, int vector)\n{\n\treturn test_bit(vector, apic->vcpu->arch.ioapic_handled_vectors);\n}\n\nstatic void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)\n{\n\tint trigger_mode;\n\n\t/* Eoi the ioapic only if the ioapic doesn't own the vector. */\n\tif (!kvm_ioapic_handles_vector(apic, vector))\n\t\treturn;\n\n\t/* Request a KVM exit to inform the userspace IOAPIC. */\n\tif (irqchip_split(apic->vcpu->kvm)) {\n\t\tapic->vcpu->arch.pending_ioapic_eoi = vector;\n\t\tkvm_make_request(KVM_REQ_IOAPIC_EOI_EXIT, apic->vcpu);\n\t\treturn;\n\t}\n\n\tif (apic_test_vector(vector, apic->regs + APIC_TMR))\n\t\ttrigger_mode = IOAPIC_LEVEL_TRIG;\n\telse\n\t\ttrigger_mode = IOAPIC_EDGE_TRIG;\n\n\tkvm_ioapic_update_eoi(apic->vcpu, vector, trigger_mode);\n}\n\nstatic int apic_set_eoi(struct kvm_lapic *apic)\n{\n\tint vector = apic_find_highest_isr(apic);\n\n\ttrace_kvm_eoi(apic, vector);\n\n\t/*\n\t * Not every write EOI will has corresponding ISR,\n\t * one example is when Kernel check timer on setup_IO_APIC\n\t */\n\tif (vector == -1)\n\t\treturn vector;\n\n\tapic_clear_isr(vector, apic);\n\tapic_update_ppr(apic);\n\n\tif (to_hv_vcpu(apic->vcpu) &&\n\t    test_bit(vector, to_hv_synic(apic->vcpu)->vec_bitmap))\n\t\tkvm_hv_synic_send_eoi(apic->vcpu, vector);\n\n\tkvm_ioapic_send_eoi(apic, vector);\n\tkvm_make_request(KVM_REQ_EVENT, apic->vcpu);\n\treturn vector;\n}\n\n/*\n * this interface assumes a trap-like exit, which has already finished\n * desired side effect including vISR and vPPR update.\n */\nvoid kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\ttrace_kvm_eoi(apic, vector);\n\n\tkvm_ioapic_send_eoi(apic, vector);\n\tkvm_make_request(KVM_REQ_EVENT, apic->vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_set_eoi_accelerated);\n\nvoid kvm_apic_send_ipi(struct kvm_lapic *apic, u32 icr_low, u32 icr_high)\n{\n\tstruct kvm_lapic_irq irq;\n\n\t/* KVM has no delay and should always clear the BUSY/PENDING flag. */\n\tWARN_ON_ONCE(icr_low & APIC_ICR_BUSY);\n\n\tirq.vector = icr_low & APIC_VECTOR_MASK;\n\tirq.delivery_mode = icr_low & APIC_MODE_MASK;\n\tirq.dest_mode = icr_low & APIC_DEST_MASK;\n\tirq.level = (icr_low & APIC_INT_ASSERT) != 0;\n\tirq.trig_mode = icr_low & APIC_INT_LEVELTRIG;\n\tirq.shorthand = icr_low & APIC_SHORT_MASK;\n\tirq.msi_redir_hint = false;\n\tif (apic_x2apic_mode(apic))\n\t\tirq.dest_id = icr_high;\n\telse\n\t\tirq.dest_id = GET_APIC_DEST_FIELD(icr_high);\n\n\ttrace_kvm_apic_ipi(icr_low, irq.dest_id);\n\n\tkvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_send_ipi);\n\nstatic u32 apic_get_tmcct(struct kvm_lapic *apic)\n{\n\tktime_t remaining, now;\n\ts64 ns;\n\tu32 tmcct;\n\n\tASSERT(apic != NULL);\n\n\t/* if initial count is 0, current count should also be 0 */\n\tif (kvm_lapic_get_reg(apic, APIC_TMICT) == 0 ||\n\t\tapic->lapic_timer.period == 0)\n\t\treturn 0;\n\n\tnow = ktime_get();\n\tremaining = ktime_sub(apic->lapic_timer.target_expiration, now);\n\tif (ktime_to_ns(remaining) < 0)\n\t\tremaining = 0;\n\n\tns = mod_64(ktime_to_ns(remaining), apic->lapic_timer.period);\n\ttmcct = div64_u64(ns,\n\t\t\t (APIC_BUS_CYCLE_NS * apic->divide_count));\n\n\treturn tmcct;\n}\n\nstatic void __report_tpr_access(struct kvm_lapic *apic, bool write)\n{\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tstruct kvm_run *run = vcpu->run;\n\n\tkvm_make_request(KVM_REQ_REPORT_TPR_ACCESS, vcpu);\n\trun->tpr_access.rip = kvm_rip_read(vcpu);\n\trun->tpr_access.is_write = write;\n}\n\nstatic inline void report_tpr_access(struct kvm_lapic *apic, bool write)\n{\n\tif (apic->vcpu->arch.tpr_access_reporting)\n\t\t__report_tpr_access(apic, write);\n}\n\nstatic u32 __apic_read(struct kvm_lapic *apic, unsigned int offset)\n{\n\tu32 val = 0;\n\n\tif (offset >= LAPIC_MMIO_LENGTH)\n\t\treturn 0;\n\n\tswitch (offset) {\n\tcase APIC_ARBPRI:\n\t\tbreak;\n\n\tcase APIC_TMCCT:\t/* Timer CCR */\n\t\tif (apic_lvtt_tscdeadline(apic))\n\t\t\treturn 0;\n\n\t\tval = apic_get_tmcct(apic);\n\t\tbreak;\n\tcase APIC_PROCPRI:\n\t\tapic_update_ppr(apic);\n\t\tval = kvm_lapic_get_reg(apic, offset);\n\t\tbreak;\n\tcase APIC_TASKPRI:\n\t\treport_tpr_access(apic, false);\n\t\tfallthrough;\n\tdefault:\n\t\tval = kvm_lapic_get_reg(apic, offset);\n\t\tbreak;\n\t}\n\n\treturn val;\n}\n\nstatic inline struct kvm_lapic *to_lapic(struct kvm_io_device *dev)\n{\n\treturn container_of(dev, struct kvm_lapic, dev);\n}\n\n#define APIC_REG_MASK(reg)\t(1ull << ((reg) >> 4))\n#define APIC_REGS_MASK(first, count) \\\n\t(APIC_REG_MASK(first) * ((1ull << (count)) - 1))\n\nstatic int kvm_lapic_reg_read(struct kvm_lapic *apic, u32 offset, int len,\n\t\t\t      void *data)\n{\n\tunsigned char alignment = offset & 0xf;\n\tu32 result;\n\t/* this bitmask has a bit cleared for each reserved register */\n\tu64 valid_reg_mask =\n\t\tAPIC_REG_MASK(APIC_ID) |\n\t\tAPIC_REG_MASK(APIC_LVR) |\n\t\tAPIC_REG_MASK(APIC_TASKPRI) |\n\t\tAPIC_REG_MASK(APIC_PROCPRI) |\n\t\tAPIC_REG_MASK(APIC_LDR) |\n\t\tAPIC_REG_MASK(APIC_DFR) |\n\t\tAPIC_REG_MASK(APIC_SPIV) |\n\t\tAPIC_REGS_MASK(APIC_ISR, APIC_ISR_NR) |\n\t\tAPIC_REGS_MASK(APIC_TMR, APIC_ISR_NR) |\n\t\tAPIC_REGS_MASK(APIC_IRR, APIC_ISR_NR) |\n\t\tAPIC_REG_MASK(APIC_ESR) |\n\t\tAPIC_REG_MASK(APIC_ICR) |\n\t\tAPIC_REG_MASK(APIC_LVTT) |\n\t\tAPIC_REG_MASK(APIC_LVTTHMR) |\n\t\tAPIC_REG_MASK(APIC_LVTPC) |\n\t\tAPIC_REG_MASK(APIC_LVT0) |\n\t\tAPIC_REG_MASK(APIC_LVT1) |\n\t\tAPIC_REG_MASK(APIC_LVTERR) |\n\t\tAPIC_REG_MASK(APIC_TMICT) |\n\t\tAPIC_REG_MASK(APIC_TMCCT) |\n\t\tAPIC_REG_MASK(APIC_TDCR);\n\n\t/*\n\t * ARBPRI and ICR2 are not valid in x2APIC mode.  WARN if KVM reads ICR\n\t * in x2APIC mode as it's an 8-byte register in x2APIC and needs to be\n\t * manually handled by the caller.\n\t */\n\tif (!apic_x2apic_mode(apic))\n\t\tvalid_reg_mask |= APIC_REG_MASK(APIC_ARBPRI) |\n\t\t\t\t  APIC_REG_MASK(APIC_ICR2);\n\telse\n\t\tWARN_ON_ONCE(offset == APIC_ICR);\n\n\tif (alignment + len > 4)\n\t\treturn 1;\n\n\tif (offset > 0x3f0 || !(valid_reg_mask & APIC_REG_MASK(offset)))\n\t\treturn 1;\n\n\tresult = __apic_read(apic, offset & ~0xf);\n\n\ttrace_kvm_apic_read(offset, result);\n\n\tswitch (len) {\n\tcase 1:\n\tcase 2:\n\tcase 4:\n\t\tmemcpy(data, (char *)&result + alignment, len);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"Local APIC read with len = %x, \"\n\t\t       \"should be 1,2, or 4 instead\\n\", len);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int apic_mmio_in_range(struct kvm_lapic *apic, gpa_t addr)\n{\n\treturn addr >= apic->base_address &&\n\t\taddr < apic->base_address + LAPIC_MMIO_LENGTH;\n}\n\nstatic int apic_mmio_read(struct kvm_vcpu *vcpu, struct kvm_io_device *this,\n\t\t\t   gpa_t address, int len, void *data)\n{\n\tstruct kvm_lapic *apic = to_lapic(this);\n\tu32 offset = address - apic->base_address;\n\n\tif (!apic_mmio_in_range(apic, address))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {\n\t\tif (!kvm_check_has_quirk(vcpu->kvm,\n\t\t\t\t\t KVM_X86_QUIRK_LAPIC_MMIO_HOLE))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tmemset(data, 0xff, len);\n\t\treturn 0;\n\t}\n\n\tkvm_lapic_reg_read(apic, offset, len, data);\n\n\treturn 0;\n}\n\nstatic void update_divide_count(struct kvm_lapic *apic)\n{\n\tu32 tmp1, tmp2, tdcr;\n\n\ttdcr = kvm_lapic_get_reg(apic, APIC_TDCR);\n\ttmp1 = tdcr & 0xf;\n\ttmp2 = ((tmp1 & 0x3) | ((tmp1 & 0x8) >> 1)) + 1;\n\tapic->divide_count = 0x1 << (tmp2 & 0x7);\n}\n\nstatic void limit_periodic_timer_frequency(struct kvm_lapic *apic)\n{\n\t/*\n\t * Do not allow the guest to program periodic timers with small\n\t * interval, since the hrtimers are not throttled by the host\n\t * scheduler.\n\t */\n\tif (apic_lvtt_period(apic) && apic->lapic_timer.period) {\n\t\ts64 min_period = min_timer_period_us * 1000LL;\n\n\t\tif (apic->lapic_timer.period < min_period) {\n\t\t\tpr_info_ratelimited(\n\t\t\t    \"kvm: vcpu %i: requested %lld ns \"\n\t\t\t    \"lapic timer period limited to %lld ns\\n\",\n\t\t\t    apic->vcpu->vcpu_id,\n\t\t\t    apic->lapic_timer.period, min_period);\n\t\t\tapic->lapic_timer.period = min_period;\n\t\t}\n\t}\n}\n\nstatic void cancel_hv_timer(struct kvm_lapic *apic);\n\nstatic void cancel_apic_timer(struct kvm_lapic *apic)\n{\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\tpreempt_disable();\n\tif (apic->lapic_timer.hv_timer_in_use)\n\t\tcancel_hv_timer(apic);\n\tpreempt_enable();\n}\n\nstatic void apic_update_lvtt(struct kvm_lapic *apic)\n{\n\tu32 timer_mode = kvm_lapic_get_reg(apic, APIC_LVTT) &\n\t\t\tapic->lapic_timer.timer_mode_mask;\n\n\tif (apic->lapic_timer.timer_mode != timer_mode) {\n\t\tif (apic_lvtt_tscdeadline(apic) != (timer_mode ==\n\t\t\t\tAPIC_LVT_TIMER_TSCDEADLINE)) {\n\t\t\tcancel_apic_timer(apic);\n\t\t\tkvm_lapic_set_reg(apic, APIC_TMICT, 0);\n\t\t\tapic->lapic_timer.period = 0;\n\t\t\tapic->lapic_timer.tscdeadline = 0;\n\t\t}\n\t\tapic->lapic_timer.timer_mode = timer_mode;\n\t\tlimit_periodic_timer_frequency(apic);\n\t}\n}\n\n/*\n * On APICv, this test will cause a busy wait\n * during a higher-priority task.\n */\n\nstatic bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 reg = kvm_lapic_get_reg(apic, APIC_LVTT);\n\n\tif (kvm_apic_hw_enabled(apic)) {\n\t\tint vec = reg & APIC_VECTOR_MASK;\n\t\tvoid *bitmap = apic->regs + APIC_ISR;\n\n\t\tif (vcpu->arch.apicv_active)\n\t\t\tbitmap = apic->regs + APIC_IRR;\n\n\t\tif (apic_test_vector(vec, bitmap))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)\n{\n\tu64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;\n\n\t/*\n\t * If the guest TSC is running at a different ratio than the host, then\n\t * convert the delay to nanoseconds to achieve an accurate delay.  Note\n\t * that __delay() uses delay_tsc whenever the hardware has TSC, thus\n\t * always for VMX enabled hardware.\n\t */\n\tif (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {\n\t\t__delay(min(guest_cycles,\n\t\t\tnsec_to_cycles(vcpu, timer_advance_ns)));\n\t} else {\n\t\tu64 delay_ns = guest_cycles * 1000000ULL;\n\t\tdo_div(delay_ns, vcpu->arch.virtual_tsc_khz);\n\t\tndelay(min_t(u32, delay_ns, timer_advance_ns));\n\t}\n}\n\nstatic inline void adjust_lapic_timer_advance(struct kvm_vcpu *vcpu,\n\t\t\t\t\t      s64 advance_expire_delta)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;\n\tu64 ns;\n\n\t/* Do not adjust for tiny fluctuations or large random spikes. */\n\tif (abs(advance_expire_delta) > LAPIC_TIMER_ADVANCE_ADJUST_MAX ||\n\t    abs(advance_expire_delta) < LAPIC_TIMER_ADVANCE_ADJUST_MIN)\n\t\treturn;\n\n\t/* too early */\n\tif (advance_expire_delta < 0) {\n\t\tns = -advance_expire_delta * 1000000ULL;\n\t\tdo_div(ns, vcpu->arch.virtual_tsc_khz);\n\t\ttimer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;\n\t} else {\n\t/* too late */\n\t\tns = advance_expire_delta * 1000000ULL;\n\t\tdo_div(ns, vcpu->arch.virtual_tsc_khz);\n\t\ttimer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;\n\t}\n\n\tif (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))\n\t\ttimer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;\n\tapic->lapic_timer.timer_advance_ns = timer_advance_ns;\n}\n\nstatic void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu64 guest_tsc, tsc_deadline;\n\n\ttsc_deadline = apic->lapic_timer.expired_tscdeadline;\n\tapic->lapic_timer.expired_tscdeadline = 0;\n\tguest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\tapic->lapic_timer.advance_expire_delta = guest_tsc - tsc_deadline;\n\n\tif (lapic_timer_advance_dynamic) {\n\t\tadjust_lapic_timer_advance(vcpu, apic->lapic_timer.advance_expire_delta);\n\t\t/*\n\t\t * If the timer fired early, reread the TSC to account for the\n\t\t * overhead of the above adjustment to avoid waiting longer\n\t\t * than is necessary.\n\t\t */\n\t\tif (guest_tsc < tsc_deadline)\n\t\t\tguest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\t}\n\n\tif (guest_tsc < tsc_deadline)\n\t\t__wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);\n}\n\nvoid kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)\n{\n\tif (lapic_in_kernel(vcpu) &&\n\t    vcpu->arch.apic->lapic_timer.expired_tscdeadline &&\n\t    vcpu->arch.apic->lapic_timer.timer_advance_ns &&\n\t    lapic_timer_int_injected(vcpu))\n\t\t__kvm_wait_lapic_expire(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_wait_lapic_expire);\n\nstatic void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\n\tkvm_apic_local_deliver(apic, APIC_LVTT);\n\tif (apic_lvtt_tscdeadline(apic)) {\n\t\tktimer->tscdeadline = 0;\n\t} else if (apic_lvtt_oneshot(apic)) {\n\t\tktimer->tscdeadline = 0;\n\t\tktimer->target_expiration = 0;\n\t}\n}\n\nstatic void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)\n{\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\n\tif (atomic_read(&apic->lapic_timer.pending))\n\t\treturn;\n\n\tif (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)\n\t\tktimer->expired_tscdeadline = ktimer->tscdeadline;\n\n\tif (!from_timer_fn && vcpu->arch.apicv_active) {\n\t\tWARN_ON(kvm_get_running_vcpu() != vcpu);\n\t\tkvm_apic_inject_pending_timer_irqs(apic);\n\t\treturn;\n\t}\n\n\tif (kvm_use_posted_timer_interrupt(apic->vcpu)) {\n\t\t/*\n\t\t * Ensure the guest's timer has truly expired before posting an\n\t\t * interrupt.  Open code the relevant checks to avoid querying\n\t\t * lapic_timer_int_injected(), which will be false since the\n\t\t * interrupt isn't yet injected.  Waiting until after injecting\n\t\t * is not an option since that won't help a posted interrupt.\n\t\t */\n\t\tif (vcpu->arch.apic->lapic_timer.expired_tscdeadline &&\n\t\t    vcpu->arch.apic->lapic_timer.timer_advance_ns)\n\t\t\t__kvm_wait_lapic_expire(vcpu);\n\t\tkvm_apic_inject_pending_timer_irqs(apic);\n\t\treturn;\n\t}\n\n\tatomic_inc(&apic->lapic_timer.pending);\n\tkvm_make_request(KVM_REQ_UNBLOCK, vcpu);\n\tif (from_timer_fn)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nstatic void start_sw_tscdeadline(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\tu64 guest_tsc, tscdeadline = ktimer->tscdeadline;\n\tu64 ns = 0;\n\tktime_t expire;\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tunsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;\n\tunsigned long flags;\n\tktime_t now;\n\n\tif (unlikely(!tscdeadline || !this_tsc_khz))\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tnow = ktime_get();\n\tguest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\n\tns = (tscdeadline - guest_tsc) * 1000000ULL;\n\tdo_div(ns, this_tsc_khz);\n\n\tif (likely(tscdeadline > guest_tsc) &&\n\t    likely(ns > apic->lapic_timer.timer_advance_ns)) {\n\t\texpire = ktime_add_ns(now, ns);\n\t\texpire = ktime_sub_ns(expire, ktimer->timer_advance_ns);\n\t\thrtimer_start(&ktimer->timer, expire, HRTIMER_MODE_ABS_HARD);\n\t} else\n\t\tapic_timer_expired(apic, false);\n\n\tlocal_irq_restore(flags);\n}\n\nstatic inline u64 tmict_to_ns(struct kvm_lapic *apic, u32 tmict)\n{\n\treturn (u64)tmict * APIC_BUS_CYCLE_NS * (u64)apic->divide_count;\n}\n\nstatic void update_target_expiration(struct kvm_lapic *apic, uint32_t old_divisor)\n{\n\tktime_t now, remaining;\n\tu64 ns_remaining_old, ns_remaining_new;\n\n\tapic->lapic_timer.period =\n\t\t\ttmict_to_ns(apic, kvm_lapic_get_reg(apic, APIC_TMICT));\n\tlimit_periodic_timer_frequency(apic);\n\n\tnow = ktime_get();\n\tremaining = ktime_sub(apic->lapic_timer.target_expiration, now);\n\tif (ktime_to_ns(remaining) < 0)\n\t\tremaining = 0;\n\n\tns_remaining_old = ktime_to_ns(remaining);\n\tns_remaining_new = mul_u64_u32_div(ns_remaining_old,\n\t                                   apic->divide_count, old_divisor);\n\n\tapic->lapic_timer.tscdeadline +=\n\t\tnsec_to_cycles(apic->vcpu, ns_remaining_new) -\n\t\tnsec_to_cycles(apic->vcpu, ns_remaining_old);\n\tapic->lapic_timer.target_expiration = ktime_add_ns(now, ns_remaining_new);\n}\n\nstatic bool set_target_expiration(struct kvm_lapic *apic, u32 count_reg)\n{\n\tktime_t now;\n\tu64 tscl = rdtsc();\n\ts64 deadline;\n\n\tnow = ktime_get();\n\tapic->lapic_timer.period =\n\t\t\ttmict_to_ns(apic, kvm_lapic_get_reg(apic, APIC_TMICT));\n\n\tif (!apic->lapic_timer.period) {\n\t\tapic->lapic_timer.tscdeadline = 0;\n\t\treturn false;\n\t}\n\n\tlimit_periodic_timer_frequency(apic);\n\tdeadline = apic->lapic_timer.period;\n\n\tif (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic)) {\n\t\tif (unlikely(count_reg != APIC_TMICT)) {\n\t\t\tdeadline = tmict_to_ns(apic,\n\t\t\t\t     kvm_lapic_get_reg(apic, count_reg));\n\t\t\tif (unlikely(deadline <= 0))\n\t\t\t\tdeadline = apic->lapic_timer.period;\n\t\t\telse if (unlikely(deadline > apic->lapic_timer.period)) {\n\t\t\t\tpr_info_ratelimited(\n\t\t\t\t    \"kvm: vcpu %i: requested lapic timer restore with \"\n\t\t\t\t    \"starting count register %#x=%u (%lld ns) > initial count (%lld ns). \"\n\t\t\t\t    \"Using initial count to start timer.\\n\",\n\t\t\t\t    apic->vcpu->vcpu_id,\n\t\t\t\t    count_reg,\n\t\t\t\t    kvm_lapic_get_reg(apic, count_reg),\n\t\t\t\t    deadline, apic->lapic_timer.period);\n\t\t\t\tkvm_lapic_set_reg(apic, count_reg, 0);\n\t\t\t\tdeadline = apic->lapic_timer.period;\n\t\t\t}\n\t\t}\n\t}\n\n\tapic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +\n\t\tnsec_to_cycles(apic->vcpu, deadline);\n\tapic->lapic_timer.target_expiration = ktime_add_ns(now, deadline);\n\n\treturn true;\n}\n\nstatic void advance_periodic_target_expiration(struct kvm_lapic *apic)\n{\n\tktime_t now = ktime_get();\n\tu64 tscl = rdtsc();\n\tktime_t delta;\n\n\t/*\n\t * Synchronize both deadlines to the same time source or\n\t * differences in the periods (caused by differences in the\n\t * underlying clocks or numerical approximation errors) will\n\t * cause the two to drift apart over time as the errors\n\t * accumulate.\n\t */\n\tapic->lapic_timer.target_expiration =\n\t\tktime_add_ns(apic->lapic_timer.target_expiration,\n\t\t\t\tapic->lapic_timer.period);\n\tdelta = ktime_sub(apic->lapic_timer.target_expiration, now);\n\tapic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +\n\t\tnsec_to_cycles(apic->vcpu, delta);\n}\n\nstatic void start_sw_period(struct kvm_lapic *apic)\n{\n\tif (!apic->lapic_timer.period)\n\t\treturn;\n\n\tif (ktime_after(ktime_get(),\n\t\t\tapic->lapic_timer.target_expiration)) {\n\t\tapic_timer_expired(apic, false);\n\n\t\tif (apic_lvtt_oneshot(apic))\n\t\t\treturn;\n\n\t\tadvance_periodic_target_expiration(apic);\n\t}\n\n\thrtimer_start(&apic->lapic_timer.timer,\n\t\tapic->lapic_timer.target_expiration,\n\t\tHRTIMER_MODE_ABS_HARD);\n}\n\nbool kvm_lapic_hv_timer_in_use(struct kvm_vcpu *vcpu)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn false;\n\n\treturn vcpu->arch.apic->lapic_timer.hv_timer_in_use;\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_hv_timer_in_use);\n\nstatic void cancel_hv_timer(struct kvm_lapic *apic)\n{\n\tWARN_ON(preemptible());\n\tWARN_ON(!apic->lapic_timer.hv_timer_in_use);\n\tstatic_call(kvm_x86_cancel_hv_timer)(apic->vcpu);\n\tapic->lapic_timer.hv_timer_in_use = false;\n}\n\nstatic bool start_hv_timer(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tbool expired;\n\n\tWARN_ON(preemptible());\n\tif (!kvm_can_use_hv_timer(vcpu))\n\t\treturn false;\n\n\tif (!ktimer->tscdeadline)\n\t\treturn false;\n\n\tif (static_call(kvm_x86_set_hv_timer)(vcpu, ktimer->tscdeadline, &expired))\n\t\treturn false;\n\n\tktimer->hv_timer_in_use = true;\n\thrtimer_cancel(&ktimer->timer);\n\n\t/*\n\t * To simplify handling the periodic timer, leave the hv timer running\n\t * even if the deadline timer has expired, i.e. rely on the resulting\n\t * VM-Exit to recompute the periodic timer's target expiration.\n\t */\n\tif (!apic_lvtt_period(apic)) {\n\t\t/*\n\t\t * Cancel the hv timer if the sw timer fired while the hv timer\n\t\t * was being programmed, or if the hv timer itself expired.\n\t\t */\n\t\tif (atomic_read(&ktimer->pending)) {\n\t\t\tcancel_hv_timer(apic);\n\t\t} else if (expired) {\n\t\t\tapic_timer_expired(apic, false);\n\t\t\tcancel_hv_timer(apic);\n\t\t}\n\t}\n\n\ttrace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);\n\n\treturn true;\n}\n\nstatic void start_sw_timer(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\n\tWARN_ON(preemptible());\n\tif (apic->lapic_timer.hv_timer_in_use)\n\t\tcancel_hv_timer(apic);\n\tif (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))\n\t\treturn;\n\n\tif (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))\n\t\tstart_sw_period(apic);\n\telse if (apic_lvtt_tscdeadline(apic))\n\t\tstart_sw_tscdeadline(apic);\n\ttrace_kvm_hv_timer_state(apic->vcpu->vcpu_id, false);\n}\n\nstatic void restart_apic_timer(struct kvm_lapic *apic)\n{\n\tpreempt_disable();\n\n\tif (!apic_lvtt_period(apic) && atomic_read(&apic->lapic_timer.pending))\n\t\tgoto out;\n\n\tif (!start_hv_timer(apic))\n\t\tstart_sw_timer(apic);\nout:\n\tpreempt_enable();\n}\n\nvoid kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tpreempt_disable();\n\t/* If the preempt notifier has already run, it also called apic_timer_expired */\n\tif (!apic->lapic_timer.hv_timer_in_use)\n\t\tgoto out;\n\tWARN_ON(kvm_vcpu_is_blocking(vcpu));\n\tapic_timer_expired(apic, false);\n\tcancel_hv_timer(apic);\n\n\tif (apic_lvtt_period(apic) && apic->lapic_timer.period) {\n\t\tadvance_periodic_target_expiration(apic);\n\t\trestart_apic_timer(apic);\n\t}\nout:\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_expired_hv_timer);\n\nvoid kvm_lapic_switch_to_hv_timer(struct kvm_vcpu *vcpu)\n{\n\trestart_apic_timer(vcpu->arch.apic);\n}\n\nvoid kvm_lapic_switch_to_sw_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tpreempt_disable();\n\t/* Possibly the TSC deadline timer is not enabled yet */\n\tif (apic->lapic_timer.hv_timer_in_use)\n\t\tstart_sw_timer(apic);\n\tpreempt_enable();\n}\n\nvoid kvm_lapic_restart_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tWARN_ON(!apic->lapic_timer.hv_timer_in_use);\n\trestart_apic_timer(apic);\n}\n\nstatic void __start_apic_timer(struct kvm_lapic *apic, u32 count_reg)\n{\n\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\tif ((apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))\n\t    && !set_target_expiration(apic, count_reg))\n\t\treturn;\n\n\trestart_apic_timer(apic);\n}\n\nstatic void start_apic_timer(struct kvm_lapic *apic)\n{\n\t__start_apic_timer(apic, APIC_TMICT);\n}\n\nstatic void apic_manage_nmi_watchdog(struct kvm_lapic *apic, u32 lvt0_val)\n{\n\tbool lvt0_in_nmi_mode = apic_lvt_nmi_mode(lvt0_val);\n\n\tif (apic->lvt0_in_nmi_mode != lvt0_in_nmi_mode) {\n\t\tapic->lvt0_in_nmi_mode = lvt0_in_nmi_mode;\n\t\tif (lvt0_in_nmi_mode) {\n\t\t\tatomic_inc(&apic->vcpu->kvm->arch.vapics_in_nmi_mode);\n\t\t} else\n\t\t\tatomic_dec(&apic->vcpu->kvm->arch.vapics_in_nmi_mode);\n\t}\n}\n\nstatic int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)\n{\n\tint ret = 0;\n\n\ttrace_kvm_apic_write(reg, val);\n\n\tswitch (reg) {\n\tcase APIC_ID:\t\t/* Local APIC ID */\n\t\tif (!apic_x2apic_mode(apic))\n\t\t\tkvm_apic_set_xapic_id(apic, val >> 24);\n\t\telse\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_TASKPRI:\n\t\treport_tpr_access(apic, true);\n\t\tapic_set_tpr(apic, val & 0xff);\n\t\tbreak;\n\n\tcase APIC_EOI:\n\t\tapic_set_eoi(apic);\n\t\tbreak;\n\n\tcase APIC_LDR:\n\t\tif (!apic_x2apic_mode(apic))\n\t\t\tkvm_apic_set_ldr(apic, val & APIC_LDR_MASK);\n\t\telse\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_DFR:\n\t\tif (!apic_x2apic_mode(apic))\n\t\t\tkvm_apic_set_dfr(apic, val | 0x0FFFFFFF);\n\t\telse\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_SPIV: {\n\t\tu32 mask = 0x3ff;\n\t\tif (kvm_lapic_get_reg(apic, APIC_LVR) & APIC_LVR_DIRECTED_EOI)\n\t\t\tmask |= APIC_SPIV_DIRECTED_EOI;\n\t\tapic_set_spiv(apic, val & mask);\n\t\tif (!(val & APIC_SPIV_APIC_ENABLED)) {\n\t\t\tint i;\n\t\t\tu32 lvt_val;\n\n\t\t\tfor (i = 0; i < KVM_APIC_LVT_NUM; i++) {\n\t\t\t\tlvt_val = kvm_lapic_get_reg(apic,\n\t\t\t\t\t\t       APIC_LVTT + 0x10 * i);\n\t\t\t\tkvm_lapic_set_reg(apic, APIC_LVTT + 0x10 * i,\n\t\t\t\t\t     lvt_val | APIC_LVT_MASKED);\n\t\t\t}\n\t\t\tapic_update_lvtt(apic);\n\t\t\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\t\t}\n\t\tbreak;\n\t}\n\tcase APIC_ICR:\n\t\tWARN_ON_ONCE(apic_x2apic_mode(apic));\n\n\t\t/* No delay here, so we always clear the pending bit */\n\t\tval &= ~APIC_ICR_BUSY;\n\t\tkvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));\n\t\tkvm_lapic_set_reg(apic, APIC_ICR, val);\n\t\tbreak;\n\tcase APIC_ICR2:\n\t\tif (apic_x2apic_mode(apic))\n\t\t\tret = 1;\n\t\telse\n\t\t\tkvm_lapic_set_reg(apic, APIC_ICR2, val & 0xff000000);\n\t\tbreak;\n\n\tcase APIC_LVT0:\n\t\tapic_manage_nmi_watchdog(apic, val);\n\t\tfallthrough;\n\tcase APIC_LVTTHMR:\n\tcase APIC_LVTPC:\n\tcase APIC_LVT1:\n\tcase APIC_LVTERR: {\n\t\t/* TODO: Check vector */\n\t\tsize_t size;\n\t\tu32 index;\n\n\t\tif (!kvm_apic_sw_enabled(apic))\n\t\t\tval |= APIC_LVT_MASKED;\n\t\tsize = ARRAY_SIZE(apic_lvt_mask);\n\t\tindex = array_index_nospec(\n\t\t\t\t(reg - APIC_LVTT) >> 4, size);\n\t\tval &= apic_lvt_mask[index];\n\t\tkvm_lapic_set_reg(apic, reg, val);\n\t\tbreak;\n\t}\n\n\tcase APIC_LVTT:\n\t\tif (!kvm_apic_sw_enabled(apic))\n\t\t\tval |= APIC_LVT_MASKED;\n\t\tval &= (apic_lvt_mask[0] | apic->lapic_timer.timer_mode_mask);\n\t\tkvm_lapic_set_reg(apic, APIC_LVTT, val);\n\t\tapic_update_lvtt(apic);\n\t\tbreak;\n\n\tcase APIC_TMICT:\n\t\tif (apic_lvtt_tscdeadline(apic))\n\t\t\tbreak;\n\n\t\tcancel_apic_timer(apic);\n\t\tkvm_lapic_set_reg(apic, APIC_TMICT, val);\n\t\tstart_apic_timer(apic);\n\t\tbreak;\n\n\tcase APIC_TDCR: {\n\t\tuint32_t old_divisor = apic->divide_count;\n\n\t\tkvm_lapic_set_reg(apic, APIC_TDCR, val & 0xb);\n\t\tupdate_divide_count(apic);\n\t\tif (apic->divide_count != old_divisor &&\n\t\t\t\tapic->lapic_timer.period) {\n\t\t\thrtimer_cancel(&apic->lapic_timer.timer);\n\t\t\tupdate_target_expiration(apic, old_divisor);\n\t\t\trestart_apic_timer(apic);\n\t\t}\n\t\tbreak;\n\t}\n\tcase APIC_ESR:\n\t\tif (apic_x2apic_mode(apic) && val != 0)\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_SELF_IPI:\n\t\tif (apic_x2apic_mode(apic))\n\t\t\tkvm_apic_send_ipi(apic, APIC_DEST_SELF | (val & APIC_VECTOR_MASK), 0);\n\t\telse\n\t\t\tret = 1;\n\t\tbreak;\n\tdefault:\n\t\tret = 1;\n\t\tbreak;\n\t}\n\n\t/*\n\t * Recalculate APIC maps if necessary, e.g. if the software enable bit\n\t * was toggled, the APIC ID changed, etc...   The maps are marked dirty\n\t * on relevant changes, i.e. this is a nop for most writes.\n\t */\n\tkvm_recalculate_apic_map(apic->vcpu->kvm);\n\n\treturn ret;\n}\n\nstatic int apic_mmio_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this,\n\t\t\t    gpa_t address, int len, const void *data)\n{\n\tstruct kvm_lapic *apic = to_lapic(this);\n\tunsigned int offset = address - apic->base_address;\n\tu32 val;\n\n\tif (!apic_mmio_in_range(apic, address))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {\n\t\tif (!kvm_check_has_quirk(vcpu->kvm,\n\t\t\t\t\t KVM_X86_QUIRK_LAPIC_MMIO_HOLE))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\treturn 0;\n\t}\n\n\t/*\n\t * APIC register must be aligned on 128-bits boundary.\n\t * 32/64/128 bits registers must be accessed thru 32 bits.\n\t * Refer SDM 8.4.1\n\t */\n\tif (len != 4 || (offset & 0xf))\n\t\treturn 0;\n\n\tval = *(u32*)data;\n\n\tkvm_lapic_reg_write(apic, offset & 0xff0, val);\n\n\treturn 0;\n}\n\nvoid kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)\n{\n\tkvm_lapic_reg_write(vcpu->arch.apic, APIC_EOI, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_set_eoi);\n\n/* emulate APIC access in a trap manner */\nvoid kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)\n{\n\tu32 val = kvm_lapic_get_reg(vcpu->arch.apic, offset);\n\n\t/* TODO: optimize to just emulate side effect w/o one more write */\n\tkvm_lapic_reg_write(vcpu->arch.apic, offset, val);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_write_nodecode);\n\nvoid kvm_free_lapic(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (!vcpu->arch.apic)\n\t\treturn;\n\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\n\tif (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))\n\t\tstatic_branch_slow_dec_deferred(&apic_hw_disabled);\n\n\tif (!apic->sw_enabled)\n\t\tstatic_branch_slow_dec_deferred(&apic_sw_disabled);\n\n\tif (apic->regs)\n\t\tfree_page((unsigned long)apic->regs);\n\n\tkfree(apic);\n}\n\n/*\n *----------------------------------------------------------------------\n * LAPIC interface\n *----------------------------------------------------------------------\n */\nu64 kvm_get_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (!kvm_apic_present(vcpu) || !apic_lvtt_tscdeadline(apic))\n\t\treturn 0;\n\n\treturn apic->lapic_timer.tscdeadline;\n}\n\nvoid kvm_set_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu, u64 data)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (!kvm_apic_present(vcpu) || !apic_lvtt_tscdeadline(apic))\n\t\treturn;\n\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\tapic->lapic_timer.tscdeadline = data;\n\tstart_apic_timer(apic);\n}\n\nvoid kvm_lapic_set_tpr(struct kvm_vcpu *vcpu, unsigned long cr8)\n{\n\tapic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);\n}\n\nu64 kvm_lapic_get_cr8(struct kvm_vcpu *vcpu)\n{\n\tu64 tpr;\n\n\ttpr = (u64) kvm_lapic_get_reg(vcpu->arch.apic, APIC_TASKPRI);\n\n\treturn (tpr & 0xf0) >> 4;\n}\n\nvoid kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)\n{\n\tu64 old_value = vcpu->arch.apic_base;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tvcpu->arch.apic_base = value;\n\n\tif ((old_value ^ value) & MSR_IA32_APICBASE_ENABLE)\n\t\tkvm_update_cpuid_runtime(vcpu);\n\n\tif (!apic)\n\t\treturn;\n\n\t/* update jump label if enable bit changes */\n\tif ((old_value ^ value) & MSR_IA32_APICBASE_ENABLE) {\n\t\tif (value & MSR_IA32_APICBASE_ENABLE) {\n\t\t\tkvm_apic_set_xapic_id(apic, vcpu->vcpu_id);\n\t\t\tstatic_branch_slow_dec_deferred(&apic_hw_disabled);\n\t\t\t/* Check if there are APF page ready requests pending */\n\t\t\tkvm_make_request(KVM_REQ_APF_READY, vcpu);\n\t\t} else {\n\t\t\tstatic_branch_inc(&apic_hw_disabled.key);\n\t\t\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n\t\t}\n\t}\n\n\tif (((old_value ^ value) & X2APIC_ENABLE) && (value & X2APIC_ENABLE))\n\t\tkvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);\n\n\tif ((old_value ^ value) & (MSR_IA32_APICBASE_ENABLE | X2APIC_ENABLE))\n\t\tstatic_call_cond(kvm_x86_set_virtual_apic_mode)(vcpu);\n\n\tapic->base_address = apic->vcpu->arch.apic_base &\n\t\t\t     MSR_IA32_APICBASE_BASE;\n\n\tif ((value & MSR_IA32_APICBASE_ENABLE) &&\n\t     apic->base_address != APIC_DEFAULT_PHYS_BASE)\n\t\tpr_warn_once(\"APIC base relocation is unsupported by KVM\");\n}\n\nvoid kvm_apic_update_apicv(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (vcpu->arch.apicv_active) {\n\t\t/* irr_pending is always true when apicv is activated. */\n\t\tapic->irr_pending = true;\n\t\tapic->isr_count = 1;\n\t} else {\n\t\t/*\n\t\t * Don't clear irr_pending, searching the IRR can race with\n\t\t * updates from the CPU as APICv is still active from hardware's\n\t\t * perspective.  The flag will be cleared as appropriate when\n\t\t * KVM injects the interrupt.\n\t\t */\n\t\tapic->isr_count = count_vectors(apic->regs + APIC_ISR);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_apic_update_apicv);\n\nvoid kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu64 msr_val;\n\tint i;\n\n\tif (!init_event) {\n\t\tmsr_val = APIC_DEFAULT_PHYS_BASE | MSR_IA32_APICBASE_ENABLE;\n\t\tif (kvm_vcpu_is_reset_bsp(vcpu))\n\t\t\tmsr_val |= MSR_IA32_APICBASE_BSP;\n\t\tkvm_lapic_set_base(vcpu, msr_val);\n\t}\n\n\tif (!apic)\n\t\treturn;\n\n\t/* Stop the timer in case it's a reset to an active apic */\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\n\t/* The xAPIC ID is set at RESET even if the APIC was already enabled. */\n\tif (!init_event)\n\t\tkvm_apic_set_xapic_id(apic, vcpu->vcpu_id);\n\tkvm_apic_set_version(apic->vcpu);\n\n\tfor (i = 0; i < KVM_APIC_LVT_NUM; i++)\n\t\tkvm_lapic_set_reg(apic, APIC_LVTT + 0x10 * i, APIC_LVT_MASKED);\n\tapic_update_lvtt(apic);\n\tif (kvm_vcpu_is_reset_bsp(vcpu) &&\n\t    kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_LINT0_REENABLED))\n\t\tkvm_lapic_set_reg(apic, APIC_LVT0,\n\t\t\t     SET_APIC_DELIVERY_MODE(0, APIC_MODE_EXTINT));\n\tapic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));\n\n\tkvm_apic_set_dfr(apic, 0xffffffffU);\n\tapic_set_spiv(apic, 0xff);\n\tkvm_lapic_set_reg(apic, APIC_TASKPRI, 0);\n\tif (!apic_x2apic_mode(apic))\n\t\tkvm_apic_set_ldr(apic, 0);\n\tkvm_lapic_set_reg(apic, APIC_ESR, 0);\n\tif (!apic_x2apic_mode(apic)) {\n\t\tkvm_lapic_set_reg(apic, APIC_ICR, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_ICR2, 0);\n\t} else {\n\t\tkvm_lapic_set_reg64(apic, APIC_ICR, 0);\n\t}\n\tkvm_lapic_set_reg(apic, APIC_TDCR, 0);\n\tkvm_lapic_set_reg(apic, APIC_TMICT, 0);\n\tfor (i = 0; i < 8; i++) {\n\t\tkvm_lapic_set_reg(apic, APIC_IRR + 0x10 * i, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_ISR + 0x10 * i, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_TMR + 0x10 * i, 0);\n\t}\n\tkvm_apic_update_apicv(vcpu);\n\tapic->highest_isr_cache = -1;\n\tupdate_divide_count(apic);\n\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\tvcpu->arch.pv_eoi.msr_val = 0;\n\tapic_update_ppr(apic);\n\tif (vcpu->arch.apicv_active) {\n\t\tstatic_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);\n\t\tstatic_call_cond(kvm_x86_hwapic_irr_update)(vcpu, -1);\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(vcpu, -1);\n\t}\n\n\tvcpu->arch.apic_arb_prio = 0;\n\tvcpu->arch.apic_attention = 0;\n\n\tkvm_recalculate_apic_map(vcpu->kvm);\n}\n\n/*\n *----------------------------------------------------------------------\n * timer interface\n *----------------------------------------------------------------------\n */\n\nstatic bool lapic_is_periodic(struct kvm_lapic *apic)\n{\n\treturn apic_lvtt_period(apic);\n}\n\nint apic_has_pending_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (apic_enabled(apic) && apic_lvt_enabled(apic, APIC_LVTT))\n\t\treturn atomic_read(&apic->lapic_timer.pending);\n\n\treturn 0;\n}\n\nint kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)\n{\n\tu32 reg = kvm_lapic_get_reg(apic, lvt_type);\n\tint vector, mode, trig_mode;\n\n\tif (kvm_apic_hw_enabled(apic) && !(reg & APIC_LVT_MASKED)) {\n\t\tvector = reg & APIC_VECTOR_MASK;\n\t\tmode = reg & APIC_MODE_MASK;\n\t\ttrig_mode = reg & APIC_LVT_LEVEL_TRIGGER;\n\t\treturn __apic_accept_irq(apic, mode, vector, 1, trig_mode,\n\t\t\t\t\tNULL);\n\t}\n\treturn 0;\n}\n\nvoid kvm_apic_nmi_wd_deliver(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (apic)\n\t\tkvm_apic_local_deliver(apic, APIC_LVT0);\n}\n\nstatic const struct kvm_io_device_ops apic_mmio_ops = {\n\t.read     = apic_mmio_read,\n\t.write    = apic_mmio_write,\n};\n\nstatic enum hrtimer_restart apic_timer_fn(struct hrtimer *data)\n{\n\tstruct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);\n\tstruct kvm_lapic *apic = container_of(ktimer, struct kvm_lapic, lapic_timer);\n\n\tapic_timer_expired(apic, true);\n\n\tif (lapic_is_periodic(apic)) {\n\t\tadvance_periodic_target_expiration(apic);\n\t\thrtimer_add_expires_ns(&ktimer->timer, ktimer->period);\n\t\treturn HRTIMER_RESTART;\n\t} else\n\t\treturn HRTIMER_NORESTART;\n}\n\nint kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)\n{\n\tstruct kvm_lapic *apic;\n\n\tASSERT(vcpu != NULL);\n\n\tapic = kzalloc(sizeof(*apic), GFP_KERNEL_ACCOUNT);\n\tif (!apic)\n\t\tgoto nomem;\n\n\tvcpu->arch.apic = apic;\n\n\tapic->regs = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);\n\tif (!apic->regs) {\n\t\tprintk(KERN_ERR \"malloc apic regs error for vcpu %x\\n\",\n\t\t       vcpu->vcpu_id);\n\t\tgoto nomem_free_apic;\n\t}\n\tapic->vcpu = vcpu;\n\n\thrtimer_init(&apic->lapic_timer.timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_ABS_HARD);\n\tapic->lapic_timer.timer.function = apic_timer_fn;\n\tif (timer_advance_ns == -1) {\n\t\tapic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;\n\t\tlapic_timer_advance_dynamic = true;\n\t} else {\n\t\tapic->lapic_timer.timer_advance_ns = timer_advance_ns;\n\t\tlapic_timer_advance_dynamic = false;\n\t}\n\n\t/*\n\t * Stuff the APIC ENABLE bit in lieu of temporarily incrementing\n\t * apic_hw_disabled; the full RESET value is set by kvm_lapic_reset().\n\t */\n\tvcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;\n\tstatic_branch_inc(&apic_sw_disabled.key); /* sw disabled at reset */\n\tkvm_iodevice_init(&apic->dev, &apic_mmio_ops);\n\n\treturn 0;\nnomem_free_apic:\n\tkfree(apic);\n\tvcpu->arch.apic = NULL;\nnomem:\n\treturn -ENOMEM;\n}\n\nint kvm_apic_has_interrupt(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 ppr;\n\n\tif (!kvm_apic_present(vcpu))\n\t\treturn -1;\n\n\t__apic_update_ppr(apic, &ppr);\n\treturn apic_has_interrupt_for_ppr(apic, ppr);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_has_interrupt);\n\nint kvm_apic_accept_pic_intr(struct kvm_vcpu *vcpu)\n{\n\tu32 lvt0 = kvm_lapic_get_reg(vcpu->arch.apic, APIC_LVT0);\n\n\tif (!kvm_apic_hw_enabled(vcpu->arch.apic))\n\t\treturn 1;\n\tif ((lvt0 & APIC_LVT_MASKED) == 0 &&\n\t    GET_APIC_DELIVERY_MODE(lvt0) == APIC_MODE_EXTINT)\n\t\treturn 1;\n\treturn 0;\n}\n\nvoid kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (atomic_read(&apic->lapic_timer.pending) > 0) {\n\t\tkvm_apic_inject_pending_timer_irqs(apic);\n\t\tatomic_set(&apic->lapic_timer.pending, 0);\n\t}\n}\n\nint kvm_get_apic_interrupt(struct kvm_vcpu *vcpu)\n{\n\tint vector = kvm_apic_has_interrupt(vcpu);\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 ppr;\n\n\tif (vector == -1)\n\t\treturn -1;\n\n\t/*\n\t * We get here even with APIC virtualization enabled, if doing\n\t * nested virtualization and L1 runs with the \"acknowledge interrupt\n\t * on exit\" mode.  Then we cannot inject the interrupt via RVI,\n\t * because the process would deliver it through the IDT.\n\t */\n\n\tapic_clear_irr(vector, apic);\n\tif (to_hv_vcpu(vcpu) && test_bit(vector, to_hv_synic(vcpu)->auto_eoi_bitmap)) {\n\t\t/*\n\t\t * For auto-EOI interrupts, there might be another pending\n\t\t * interrupt above PPR, so check whether to raise another\n\t\t * KVM_REQ_EVENT.\n\t\t */\n\t\tapic_update_ppr(apic);\n\t} else {\n\t\t/*\n\t\t * For normal interrupts, PPR has been raised and there cannot\n\t\t * be a higher-priority pending interrupt---except if there was\n\t\t * a concurrent interrupt injection, but that would have\n\t\t * triggered KVM_REQ_EVENT already.\n\t\t */\n\t\tapic_set_isr(vector, apic);\n\t\t__apic_update_ppr(apic, &ppr);\n\t}\n\n\treturn vector;\n}\n\nstatic int kvm_apic_state_fixup(struct kvm_vcpu *vcpu,\n\t\tstruct kvm_lapic_state *s, bool set)\n{\n\tif (apic_x2apic_mode(vcpu->arch.apic)) {\n\t\tu32 *id = (u32 *)(s->regs + APIC_ID);\n\t\tu32 *ldr = (u32 *)(s->regs + APIC_LDR);\n\t\tu64 icr;\n\n\t\tif (vcpu->kvm->arch.x2apic_format) {\n\t\t\tif (*id != vcpu->vcpu_id)\n\t\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\tif (set)\n\t\t\t\t*id >>= 24;\n\t\t\telse\n\t\t\t\t*id <<= 24;\n\t\t}\n\n\t\t/*\n\t\t * In x2APIC mode, the LDR is fixed and based on the id.  And\n\t\t * ICR is internally a single 64-bit register, but needs to be\n\t\t * split to ICR+ICR2 in userspace for backwards compatibility.\n\t\t */\n\t\tif (set) {\n\t\t\t*ldr = kvm_apic_calc_x2apic_ldr(*id);\n\n\t\t\ticr = __kvm_lapic_get_reg(s->regs, APIC_ICR) |\n\t\t\t      (u64)__kvm_lapic_get_reg(s->regs, APIC_ICR2) << 32;\n\t\t\t__kvm_lapic_set_reg64(s->regs, APIC_ICR, icr);\n\t\t} else {\n\t\t\ticr = __kvm_lapic_get_reg64(s->regs, APIC_ICR);\n\t\t\t__kvm_lapic_set_reg(s->regs, APIC_ICR2, icr >> 32);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint kvm_apic_get_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)\n{\n\tmemcpy(s->regs, vcpu->arch.apic->regs, sizeof(*s));\n\n\t/*\n\t * Get calculated timer current count for remaining timer period (if\n\t * any) and store it in the returned register set.\n\t */\n\t__kvm_lapic_set_reg(s->regs, APIC_TMCCT,\n\t\t\t    __apic_read(vcpu->arch.apic, APIC_TMCCT));\n\n\treturn kvm_apic_state_fixup(vcpu, s, false);\n}\n\nint kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tint r;\n\n\tkvm_lapic_set_base(vcpu, vcpu->arch.apic_base);\n\t/* set SPIV separately to get count of SW disabled APICs right */\n\tapic_set_spiv(apic, *((u32 *)(s->regs + APIC_SPIV)));\n\n\tr = kvm_apic_state_fixup(vcpu, s, true);\n\tif (r) {\n\t\tkvm_recalculate_apic_map(vcpu->kvm);\n\t\treturn r;\n\t}\n\tmemcpy(vcpu->arch.apic->regs, s->regs, sizeof(*s));\n\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n\tkvm_recalculate_apic_map(vcpu->kvm);\n\tkvm_apic_set_version(vcpu);\n\n\tapic_update_ppr(apic);\n\tcancel_apic_timer(apic);\n\tapic->lapic_timer.expired_tscdeadline = 0;\n\tapic_update_lvtt(apic);\n\tapic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));\n\tupdate_divide_count(apic);\n\t__start_apic_timer(apic, APIC_TMCCT);\n\tkvm_lapic_set_reg(apic, APIC_TMCCT, 0);\n\tkvm_apic_update_apicv(vcpu);\n\tapic->highest_isr_cache = -1;\n\tif (vcpu->arch.apicv_active) {\n\t\tstatic_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);\n\t\tstatic_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));\n\t}\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\tif (ioapic_in_kernel(vcpu->kvm))\n\t\tkvm_rtc_eoi_tracking_restore_one(vcpu);\n\n\tvcpu->arch.apic_arb_prio = 0;\n\n\treturn 0;\n}\n\nvoid __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct hrtimer *timer;\n\n\tif (!lapic_in_kernel(vcpu) ||\n\t\tkvm_can_post_timer_interrupt(vcpu))\n\t\treturn;\n\n\ttimer = &vcpu->arch.apic->lapic_timer.timer;\n\tif (hrtimer_cancel(timer))\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS_HARD);\n}\n\n/*\n * apic_sync_pv_eoi_from_guest - called on vmexit or cancel interrupt\n *\n * Detect whether guest triggered PV EOI since the\n * last entry. If yes, set EOI on guests's behalf.\n * Clear PV EOI in guest memory in any case.\n */\nstatic void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_lapic *apic)\n{\n\tint vector;\n\t/*\n\t * PV EOI state is derived from KVM_APIC_PV_EOI_PENDING in host\n\t * and KVM_PV_EOI_ENABLED in guest memory as follows:\n\t *\n\t * KVM_APIC_PV_EOI_PENDING is unset:\n\t * \t-> host disabled PV EOI.\n\t * KVM_APIC_PV_EOI_PENDING is set, KVM_PV_EOI_ENABLED is set:\n\t * \t-> host enabled PV EOI, guest did not execute EOI yet.\n\t * KVM_APIC_PV_EOI_PENDING is set, KVM_PV_EOI_ENABLED is unset:\n\t * \t-> host enabled PV EOI, guest executed EOI.\n\t */\n\tBUG_ON(!pv_eoi_enabled(vcpu));\n\n\tif (pv_eoi_test_and_clr_pending(vcpu))\n\t\treturn;\n\tvector = apic_set_eoi(apic);\n\ttrace_kvm_pv_eoi(apic, vector);\n}\n\nvoid kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data;\n\n\tif (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))\n\t\tapic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\tif (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n\t\t\t\t  sizeof(u32)))\n\t\treturn;\n\n\tapic_set_tpr(vcpu->arch.apic, data & 0xff);\n}\n\n/*\n * apic_sync_pv_eoi_to_guest - called before vmentry\n *\n * Detect whether it's safe to enable PV EOI and\n * if yes do so.\n */\nstatic void apic_sync_pv_eoi_to_guest(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_lapic *apic)\n{\n\tif (!pv_eoi_enabled(vcpu) ||\n\t    /* IRR set or many bits in ISR: could be nested. */\n\t    apic->irr_pending ||\n\t    /* Cache not set: could be safe but we don't bother. */\n\t    apic->highest_isr_cache == -1 ||\n\t    /* Need EOI to update ioapic. */\n\t    kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {\n\t\t/*\n\t\t * PV EOI was disabled by apic_sync_pv_eoi_from_guest\n\t\t * so we need not do anything here.\n\t\t */\n\t\treturn;\n\t}\n\n\tpv_eoi_set_pending(apic->vcpu);\n}\n\nvoid kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data, tpr;\n\tint max_irr, max_isr;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tapic_sync_pv_eoi_to_guest(vcpu, apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\ttpr = kvm_lapic_get_reg(apic, APIC_TASKPRI) & 0xff;\n\tmax_irr = apic_find_highest_irr(apic);\n\tif (max_irr < 0)\n\t\tmax_irr = 0;\n\tmax_isr = apic_find_highest_isr(apic);\n\tif (max_isr < 0)\n\t\tmax_isr = 0;\n\tdata = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);\n\n\tkvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n\t\t\t\tsizeof(u32));\n}\n\nint kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)\n{\n\tif (vapic_addr) {\n\t\tif (kvm_gfn_to_hva_cache_init(vcpu->kvm,\n\t\t\t\t\t&vcpu->arch.apic->vapic_cache,\n\t\t\t\t\tvapic_addr, sizeof(u32)))\n\t\t\treturn -EINVAL;\n\t\t__set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);\n\t} else {\n\t\t__clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);\n\t}\n\n\tvcpu->arch.apic->vapic_addr = vapic_addr;\n\treturn 0;\n}\n\nint kvm_x2apic_icr_write(struct kvm_lapic *apic, u64 data)\n{\n\tdata &= ~APIC_ICR_BUSY;\n\n\tkvm_apic_send_ipi(apic, (u32)data, (u32)(data >> 32));\n\tkvm_lapic_set_reg64(apic, APIC_ICR, data);\n\ttrace_kvm_apic_write(APIC_ICR, data);\n\treturn 0;\n}\n\nstatic int kvm_lapic_msr_read(struct kvm_lapic *apic, u32 reg, u64 *data)\n{\n\tu32 low;\n\n\tif (reg == APIC_ICR) {\n\t\t*data = kvm_lapic_get_reg64(apic, APIC_ICR);\n\t\treturn 0;\n\t}\n\n\tif (kvm_lapic_reg_read(apic, reg, 4, &low))\n\t\treturn 1;\n\n\t*data = low;\n\n\treturn 0;\n}\n\nstatic int kvm_lapic_msr_write(struct kvm_lapic *apic, u32 reg, u64 data)\n{\n\t/*\n\t * ICR is a 64-bit register in x2APIC mode (and Hyper'v PV vAPIC) and\n\t * can be written as such, all other registers remain accessible only\n\t * through 32-bit reads/writes.\n\t */\n\tif (reg == APIC_ICR)\n\t\treturn kvm_x2apic_icr_write(apic, data);\n\n\treturn kvm_lapic_reg_write(apic, reg, (u32)data);\n}\n\nint kvm_x2apic_msr_write(struct kvm_vcpu *vcpu, u32 msr, u64 data)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 reg = (msr - APIC_BASE_MSR) << 4;\n\n\tif (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(apic))\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_write(apic, reg, data);\n}\n\nint kvm_x2apic_msr_read(struct kvm_vcpu *vcpu, u32 msr, u64 *data)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 reg = (msr - APIC_BASE_MSR) << 4;\n\n\tif (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(apic))\n\t\treturn 1;\n\n\tif (reg == APIC_DFR)\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_read(apic, reg, data);\n}\n\nint kvm_hv_vapic_msr_write(struct kvm_vcpu *vcpu, u32 reg, u64 data)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_write(vcpu->arch.apic, reg, data);\n}\n\nint kvm_hv_vapic_msr_read(struct kvm_vcpu *vcpu, u32 reg, u64 *data)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_read(vcpu->arch.apic, reg, data);\n}\n\nint kvm_lapic_set_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)\n{\n\tu64 addr = data & ~KVM_MSR_ENABLED;\n\tstruct gfn_to_hva_cache *ghc = &vcpu->arch.pv_eoi.data;\n\tunsigned long new_len;\n\tint ret;\n\n\tif (!IS_ALIGNED(addr, 4))\n\t\treturn 1;\n\n\tif (data & KVM_MSR_ENABLED) {\n\t\tif (addr == ghc->gpa && len <= ghc->len)\n\t\t\tnew_len = ghc->len;\n\t\telse\n\t\t\tnew_len = len;\n\n\t\tret = kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, addr, new_len);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tvcpu->arch.pv_eoi.msr_val = data;\n\n\treturn 0;\n}\n\nint kvm_apic_accept_events(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu8 sipi_vector;\n\tint r;\n\tunsigned long pe;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 0;\n\n\t/*\n\t * Read pending events before calling the check_events\n\t * callback.\n\t */\n\tpe = smp_load_acquire(&apic->pending_events);\n\tif (!pe)\n\t\treturn 0;\n\n\tif (is_guest_mode(vcpu)) {\n\t\tr = kvm_check_nested_events(vcpu);\n\t\tif (r < 0)\n\t\t\treturn r == -EBUSY ? 0 : r;\n\t\t/*\n\t\t * If an event has happened and caused a vmexit,\n\t\t * we know INITs are latched and therefore\n\t\t * we will not incorrectly deliver an APIC\n\t\t * event instead of a vmexit.\n\t\t */\n\t}\n\n\t/*\n\t * INITs are latched while CPU is in specific states\n\t * (SMM, VMX root mode, SVM with GIF=0).\n\t * Because a CPU cannot be in these states immediately\n\t * after it has processed an INIT signal (and thus in\n\t * KVM_MP_STATE_INIT_RECEIVED state), just eat SIPIs\n\t * and leave the INIT pending.\n\t */\n\tif (kvm_vcpu_latch_init(vcpu)) {\n\t\tWARN_ON_ONCE(vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED);\n\t\tif (test_bit(KVM_APIC_SIPI, &pe))\n\t\t\tclear_bit(KVM_APIC_SIPI, &apic->pending_events);\n\t\treturn 0;\n\t}\n\n\tif (test_bit(KVM_APIC_INIT, &pe)) {\n\t\tclear_bit(KVM_APIC_INIT, &apic->pending_events);\n\t\tkvm_vcpu_reset(vcpu, true);\n\t\tif (kvm_vcpu_is_bsp(apic->vcpu))\n\t\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\t\telse\n\t\t\tvcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;\n\t}\n\tif (test_bit(KVM_APIC_SIPI, &pe)) {\n\t\tclear_bit(KVM_APIC_SIPI, &apic->pending_events);\n\t\tif (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {\n\t\t\t/* evaluate pending_events before reading the vector */\n\t\t\tsmp_rmb();\n\t\t\tsipi_vector = apic->sipi_vector;\n\t\t\tstatic_call(kvm_x86_vcpu_deliver_sipi_vector)(vcpu, sipi_vector);\n\t\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\t\t}\n\t}\n\treturn 0;\n}\n\nvoid kvm_lapic_exit(void)\n{\n\tstatic_key_deferred_flush(&apic_hw_disabled);\n\tWARN_ON(static_branch_unlikely(&apic_hw_disabled.key));\n\tstatic_key_deferred_flush(&apic_sw_disabled);\n\tWARN_ON(static_branch_unlikely(&apic_sw_disabled.key));\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n\n/*\n * Local APIC virtualization\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright (C) 2007 Novell\n * Copyright (C) 2007 Intel\n * Copyright 2009 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Dor Laor <dor.laor@qumranet.com>\n *   Gregory Haskins <ghaskins@novell.com>\n *   Yaozu (Eddie) Dong <eddie.dong@intel.com>\n *\n * Based on Xen 3.1 code, Copyright (c) 2004, Intel Corporation.\n */\n\n#include <linux/kvm_host.h>\n#include <linux/kvm.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/smp.h>\n#include <linux/hrtimer.h>\n#include <linux/io.h>\n#include <linux/export.h>\n#include <linux/math64.h>\n#include <linux/slab.h>\n#include <asm/processor.h>\n#include <asm/msr.h>\n#include <asm/page.h>\n#include <asm/current.h>\n#include <asm/apicdef.h>\n#include <asm/delay.h>\n#include <linux/atomic.h>\n#include <linux/jump_label.h>\n#include \"kvm_cache_regs.h\"\n#include \"irq.h\"\n#include \"ioapic.h\"\n#include \"trace.h\"\n#include \"x86.h\"\n#include \"cpuid.h\"\n#include \"hyperv.h\"\n\n#ifndef CONFIG_X86_64\n#define mod_64(x, y) ((x) - (y) * div64_u64(x, y))\n#else\n#define mod_64(x, y) ((x) % (y))\n#endif\n\n#define PRId64 \"d\"\n#define PRIx64 \"llx\"\n#define PRIu64 \"u\"\n#define PRIo64 \"o\"\n\n/* 14 is the version for Xeon and Pentium 8.4.8*/\n#define APIC_VERSION\t\t\t(0x14UL | ((KVM_APIC_LVT_NUM - 1) << 16))\n#define LAPIC_MMIO_LENGTH\t\t(1 << 12)\n/* followed define is not in apicdef.h */\n#define MAX_APIC_VECTOR\t\t\t256\n#define APIC_VECTORS_PER_REG\t\t32\n\nstatic bool lapic_timer_advance_dynamic __read_mostly;\n#define LAPIC_TIMER_ADVANCE_ADJUST_MIN\t100\t/* clock cycles */\n#define LAPIC_TIMER_ADVANCE_ADJUST_MAX\t10000\t/* clock cycles */\n#define LAPIC_TIMER_ADVANCE_NS_INIT\t1000\n#define LAPIC_TIMER_ADVANCE_NS_MAX     5000\n/* step-by-step approximation to mitigate fluctuation */\n#define LAPIC_TIMER_ADVANCE_ADJUST_STEP 8\n\nstatic inline void __kvm_lapic_set_reg(char *regs, int reg_off, u32 val)\n{\n\t*((u32 *) (regs + reg_off)) = val;\n}\n\nstatic inline void kvm_lapic_set_reg(struct kvm_lapic *apic, int reg_off, u32 val)\n{\n\t__kvm_lapic_set_reg(apic->regs, reg_off, val);\n}\n\nstatic __always_inline u64 __kvm_lapic_get_reg64(char *regs, int reg)\n{\n\tBUILD_BUG_ON(reg != APIC_ICR);\n\treturn *((u64 *) (regs + reg));\n}\n\nstatic __always_inline u64 kvm_lapic_get_reg64(struct kvm_lapic *apic, int reg)\n{\n\treturn __kvm_lapic_get_reg64(apic->regs, reg);\n}\n\nstatic __always_inline void __kvm_lapic_set_reg64(char *regs, int reg, u64 val)\n{\n\tBUILD_BUG_ON(reg != APIC_ICR);\n\t*((u64 *) (regs + reg)) = val;\n}\n\nstatic __always_inline void kvm_lapic_set_reg64(struct kvm_lapic *apic,\n\t\t\t\t\t\tint reg, u64 val)\n{\n\t__kvm_lapic_set_reg64(apic->regs, reg, val);\n}\n\nstatic inline int apic_test_vector(int vec, void *bitmap)\n{\n\treturn test_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));\n}\n\nbool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\treturn apic_test_vector(vector, apic->regs + APIC_ISR) ||\n\t\tapic_test_vector(vector, apic->regs + APIC_IRR);\n}\n\nstatic inline int __apic_test_and_set_vector(int vec, void *bitmap)\n{\n\treturn __test_and_set_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));\n}\n\nstatic inline int __apic_test_and_clear_vector(int vec, void *bitmap)\n{\n\treturn __test_and_clear_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));\n}\n\n__read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_hw_disabled, HZ);\n__read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_sw_disabled, HZ);\n\nstatic inline int apic_enabled(struct kvm_lapic *apic)\n{\n\treturn kvm_apic_sw_enabled(apic) &&\tkvm_apic_hw_enabled(apic);\n}\n\n#define LVT_MASK\t\\\n\t(APIC_LVT_MASKED | APIC_SEND_PENDING | APIC_VECTOR_MASK)\n\n#define LINT_MASK\t\\\n\t(LVT_MASK | APIC_MODE_MASK | APIC_INPUT_POLARITY | \\\n\t APIC_LVT_REMOTE_IRR | APIC_LVT_LEVEL_TRIGGER)\n\nstatic inline u32 kvm_x2apic_id(struct kvm_lapic *apic)\n{\n\treturn apic->vcpu->vcpu_id;\n}\n\nstatic bool kvm_can_post_timer_interrupt(struct kvm_vcpu *vcpu)\n{\n\treturn pi_inject_timer && kvm_vcpu_apicv_active(vcpu) &&\n\t\t(kvm_mwait_in_guest(vcpu->kvm) || kvm_hlt_in_guest(vcpu->kvm));\n}\n\nbool kvm_can_use_hv_timer(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_x86_ops.set_hv_timer\n\t       && !(kvm_mwait_in_guest(vcpu->kvm) ||\n\t\t    kvm_can_post_timer_interrupt(vcpu));\n}\nEXPORT_SYMBOL_GPL(kvm_can_use_hv_timer);\n\nstatic bool kvm_use_posted_timer_interrupt(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;\n}\n\nstatic inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,\n\t\tu32 dest_id, struct kvm_lapic ***cluster, u16 *mask) {\n\tswitch (map->mode) {\n\tcase KVM_APIC_MODE_X2APIC: {\n\t\tu32 offset = (dest_id >> 16) * 16;\n\t\tu32 max_apic_id = map->max_apic_id;\n\n\t\tif (offset <= max_apic_id) {\n\t\t\tu8 cluster_size = min(max_apic_id - offset + 1, 16U);\n\n\t\t\toffset = array_index_nospec(offset, map->max_apic_id + 1);\n\t\t\t*cluster = &map->phys_map[offset];\n\t\t\t*mask = dest_id & (0xffff >> (16 - cluster_size));\n\t\t} else {\n\t\t\t*mask = 0;\n\t\t}\n\n\t\treturn true;\n\t\t}\n\tcase KVM_APIC_MODE_XAPIC_FLAT:\n\t\t*cluster = map->xapic_flat_map;\n\t\t*mask = dest_id & 0xff;\n\t\treturn true;\n\tcase KVM_APIC_MODE_XAPIC_CLUSTER:\n\t\t*cluster = map->xapic_cluster_map[(dest_id >> 4) & 0xf];\n\t\t*mask = dest_id & 0xf;\n\t\treturn true;\n\tdefault:\n\t\t/* Not optimized. */\n\t\treturn false;\n\t}\n}\n\nstatic void kvm_apic_map_free(struct rcu_head *rcu)\n{\n\tstruct kvm_apic_map *map = container_of(rcu, struct kvm_apic_map, rcu);\n\n\tkvfree(map);\n}\n\n/*\n * CLEAN -> DIRTY and UPDATE_IN_PROGRESS -> DIRTY changes happen without a lock.\n *\n * DIRTY -> UPDATE_IN_PROGRESS and UPDATE_IN_PROGRESS -> CLEAN happen with\n * apic_map_lock_held.\n */\nenum {\n\tCLEAN,\n\tUPDATE_IN_PROGRESS,\n\tDIRTY\n};\n\nvoid kvm_recalculate_apic_map(struct kvm *kvm)\n{\n\tstruct kvm_apic_map *new, *old = NULL;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\tu32 max_id = 255; /* enough space for any xAPIC ID */\n\n\t/* Read kvm->arch.apic_map_dirty before kvm->arch.apic_map.  */\n\tif (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)\n\t\treturn;\n\n\tWARN_ONCE(!irqchip_in_kernel(kvm),\n\t\t  \"Dirty APIC map without an in-kernel local APIC\");\n\n\tmutex_lock(&kvm->arch.apic_map_lock);\n\t/*\n\t * Read kvm->arch.apic_map_dirty before kvm->arch.apic_map\n\t * (if clean) or the APIC registers (if dirty).\n\t */\n\tif (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,\n\t\t\t\t   DIRTY, UPDATE_IN_PROGRESS) == CLEAN) {\n\t\t/* Someone else has updated the map. */\n\t\tmutex_unlock(&kvm->arch.apic_map_lock);\n\t\treturn;\n\t}\n\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tif (kvm_apic_present(vcpu))\n\t\t\tmax_id = max(max_id, kvm_x2apic_id(vcpu->arch.apic));\n\n\tnew = kvzalloc(sizeof(struct kvm_apic_map) +\n\t                   sizeof(struct kvm_lapic *) * ((u64)max_id + 1),\n\t\t\t   GFP_KERNEL_ACCOUNT);\n\n\tif (!new)\n\t\tgoto out;\n\n\tnew->max_apic_id = max_id;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\t\tstruct kvm_lapic **cluster;\n\t\tu16 mask;\n\t\tu32 ldr;\n\t\tu8 xapic_id;\n\t\tu32 x2apic_id;\n\n\t\tif (!kvm_apic_present(vcpu))\n\t\t\tcontinue;\n\n\t\txapic_id = kvm_xapic_id(apic);\n\t\tx2apic_id = kvm_x2apic_id(apic);\n\n\t\t/* Hotplug hack: see kvm_apic_match_physical_addr(), ... */\n\t\tif ((apic_x2apic_mode(apic) || x2apic_id > 0xff) &&\n\t\t\t\tx2apic_id <= new->max_apic_id)\n\t\t\tnew->phys_map[x2apic_id] = apic;\n\t\t/*\n\t\t * ... xAPIC ID of VCPUs with APIC ID > 0xff will wrap-around,\n\t\t * prevent them from masking VCPUs with APIC ID <= 0xff.\n\t\t */\n\t\tif (!apic_x2apic_mode(apic) && !new->phys_map[xapic_id])\n\t\t\tnew->phys_map[xapic_id] = apic;\n\n\t\tif (!kvm_apic_sw_enabled(apic))\n\t\t\tcontinue;\n\n\t\tldr = kvm_lapic_get_reg(apic, APIC_LDR);\n\n\t\tif (apic_x2apic_mode(apic)) {\n\t\t\tnew->mode |= KVM_APIC_MODE_X2APIC;\n\t\t} else if (ldr) {\n\t\t\tldr = GET_APIC_LOGICAL_ID(ldr);\n\t\t\tif (kvm_lapic_get_reg(apic, APIC_DFR) == APIC_DFR_FLAT)\n\t\t\t\tnew->mode |= KVM_APIC_MODE_XAPIC_FLAT;\n\t\t\telse\n\t\t\t\tnew->mode |= KVM_APIC_MODE_XAPIC_CLUSTER;\n\t\t}\n\n\t\tif (!kvm_apic_map_get_logical_dest(new, ldr, &cluster, &mask))\n\t\t\tcontinue;\n\n\t\tif (mask)\n\t\t\tcluster[ffs(mask) - 1] = apic;\n\t}\nout:\n\told = rcu_dereference_protected(kvm->arch.apic_map,\n\t\t\tlockdep_is_held(&kvm->arch.apic_map_lock));\n\trcu_assign_pointer(kvm->arch.apic_map, new);\n\t/*\n\t * Write kvm->arch.apic_map before clearing apic->apic_map_dirty.\n\t * If another update has come in, leave it DIRTY.\n\t */\n\tatomic_cmpxchg_release(&kvm->arch.apic_map_dirty,\n\t\t\t       UPDATE_IN_PROGRESS, CLEAN);\n\tmutex_unlock(&kvm->arch.apic_map_lock);\n\n\tif (old)\n\t\tcall_rcu(&old->rcu, kvm_apic_map_free);\n\n\tkvm_make_scan_ioapic_request(kvm);\n}\n\nstatic inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)\n{\n\tbool enabled = val & APIC_SPIV_APIC_ENABLED;\n\n\tkvm_lapic_set_reg(apic, APIC_SPIV, val);\n\n\tif (enabled != apic->sw_enabled) {\n\t\tapic->sw_enabled = enabled;\n\t\tif (enabled)\n\t\t\tstatic_branch_slow_dec_deferred(&apic_sw_disabled);\n\t\telse\n\t\t\tstatic_branch_inc(&apic_sw_disabled.key);\n\n\t\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n\t}\n\n\t/* Check if there are APF page ready requests pending */\n\tif (enabled)\n\t\tkvm_make_request(KVM_REQ_APF_READY, apic->vcpu);\n}\n\nstatic inline void kvm_apic_set_xapic_id(struct kvm_lapic *apic, u8 id)\n{\n\tkvm_lapic_set_reg(apic, APIC_ID, id << 24);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline void kvm_apic_set_ldr(struct kvm_lapic *apic, u32 id)\n{\n\tkvm_lapic_set_reg(apic, APIC_LDR, id);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline void kvm_apic_set_dfr(struct kvm_lapic *apic, u32 val)\n{\n\tkvm_lapic_set_reg(apic, APIC_DFR, val);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline u32 kvm_apic_calc_x2apic_ldr(u32 id)\n{\n\treturn ((id >> 4) << 16) | (1 << (id & 0xf));\n}\n\nstatic inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)\n{\n\tu32 ldr = kvm_apic_calc_x2apic_ldr(id);\n\n\tWARN_ON_ONCE(id != apic->vcpu->vcpu_id);\n\n\tkvm_lapic_set_reg(apic, APIC_ID, id);\n\tkvm_lapic_set_reg(apic, APIC_LDR, ldr);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline int apic_lvt_enabled(struct kvm_lapic *apic, int lvt_type)\n{\n\treturn !(kvm_lapic_get_reg(apic, lvt_type) & APIC_LVT_MASKED);\n}\n\nstatic inline int apic_lvtt_oneshot(struct kvm_lapic *apic)\n{\n\treturn apic->lapic_timer.timer_mode == APIC_LVT_TIMER_ONESHOT;\n}\n\nstatic inline int apic_lvtt_period(struct kvm_lapic *apic)\n{\n\treturn apic->lapic_timer.timer_mode == APIC_LVT_TIMER_PERIODIC;\n}\n\nstatic inline int apic_lvtt_tscdeadline(struct kvm_lapic *apic)\n{\n\treturn apic->lapic_timer.timer_mode == APIC_LVT_TIMER_TSCDEADLINE;\n}\n\nstatic inline int apic_lvt_nmi_mode(u32 lvt_val)\n{\n\treturn (lvt_val & (APIC_MODE_MASK | APIC_LVT_MASKED)) == APIC_DM_NMI;\n}\n\nvoid kvm_apic_set_version(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 v = APIC_VERSION;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\t/*\n\t * KVM emulates 82093AA datasheet (with in-kernel IOAPIC implementation)\n\t * which doesn't have EOI register; Some buggy OSes (e.g. Windows with\n\t * Hyper-V role) disable EOI broadcast in lapic not checking for IOAPIC\n\t * version first and level-triggered interrupts never get EOIed in\n\t * IOAPIC.\n\t */\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_X2APIC) &&\n\t    !ioapic_in_kernel(vcpu->kvm))\n\t\tv |= APIC_LVR_DIRECTED_EOI;\n\tkvm_lapic_set_reg(apic, APIC_LVR, v);\n}\n\nstatic const unsigned int apic_lvt_mask[KVM_APIC_LVT_NUM] = {\n\tLVT_MASK ,      /* part LVTT mask, timer mode mask added at runtime */\n\tLVT_MASK | APIC_MODE_MASK,\t/* LVTTHMR */\n\tLVT_MASK | APIC_MODE_MASK,\t/* LVTPC */\n\tLINT_MASK, LINT_MASK,\t/* LVT0-1 */\n\tLVT_MASK\t\t/* LVTERR */\n};\n\nstatic int find_highest_vector(void *bitmap)\n{\n\tint vec;\n\tu32 *reg;\n\n\tfor (vec = MAX_APIC_VECTOR - APIC_VECTORS_PER_REG;\n\t     vec >= 0; vec -= APIC_VECTORS_PER_REG) {\n\t\treg = bitmap + REG_POS(vec);\n\t\tif (*reg)\n\t\t\treturn __fls(*reg) + vec;\n\t}\n\n\treturn -1;\n}\n\nstatic u8 count_vectors(void *bitmap)\n{\n\tint vec;\n\tu32 *reg;\n\tu8 count = 0;\n\n\tfor (vec = 0; vec < MAX_APIC_VECTOR; vec += APIC_VECTORS_PER_REG) {\n\t\treg = bitmap + REG_POS(vec);\n\t\tcount += hweight32(*reg);\n\t}\n\n\treturn count;\n}\n\nbool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)\n{\n\tu32 i, vec;\n\tu32 pir_val, irr_val, prev_irr_val;\n\tint max_updated_irr;\n\n\tmax_updated_irr = -1;\n\t*max_irr = -1;\n\n\tfor (i = vec = 0; i <= 7; i++, vec += 32) {\n\t\tpir_val = READ_ONCE(pir[i]);\n\t\tirr_val = *((u32 *)(regs + APIC_IRR + i * 0x10));\n\t\tif (pir_val) {\n\t\t\tprev_irr_val = irr_val;\n\t\t\tirr_val |= xchg(&pir[i], 0);\n\t\t\t*((u32 *)(regs + APIC_IRR + i * 0x10)) = irr_val;\n\t\t\tif (prev_irr_val != irr_val) {\n\t\t\t\tmax_updated_irr =\n\t\t\t\t\t__fls(irr_val ^ prev_irr_val) + vec;\n\t\t\t}\n\t\t}\n\t\tif (irr_val)\n\t\t\t*max_irr = __fls(irr_val) + vec;\n\t}\n\n\treturn ((max_updated_irr != -1) &&\n\t\t(max_updated_irr == *max_irr));\n}\nEXPORT_SYMBOL_GPL(__kvm_apic_update_irr);\n\nbool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\treturn __kvm_apic_update_irr(pir, apic->regs, max_irr);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_update_irr);\n\nstatic inline int apic_search_irr(struct kvm_lapic *apic)\n{\n\treturn find_highest_vector(apic->regs + APIC_IRR);\n}\n\nstatic inline int apic_find_highest_irr(struct kvm_lapic *apic)\n{\n\tint result;\n\n\t/*\n\t * Note that irr_pending is just a hint. It will be always\n\t * true with virtual interrupt delivery enabled.\n\t */\n\tif (!apic->irr_pending)\n\t\treturn -1;\n\n\tresult = apic_search_irr(apic);\n\tASSERT(result == -1 || result >= 16);\n\n\treturn result;\n}\n\nstatic inline void apic_clear_irr(int vec, struct kvm_lapic *apic)\n{\n\tstruct kvm_vcpu *vcpu;\n\n\tvcpu = apic->vcpu;\n\n\tif (unlikely(vcpu->arch.apicv_active)) {\n\t\t/* need to update RVI */\n\t\tkvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);\n\t\tstatic_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));\n\t} else {\n\t\tapic->irr_pending = false;\n\t\tkvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);\n\t\tif (apic_search_irr(apic) != -1)\n\t\t\tapic->irr_pending = true;\n\t}\n}\n\nvoid kvm_apic_clear_irr(struct kvm_vcpu *vcpu, int vec)\n{\n\tapic_clear_irr(vec, vcpu->arch.apic);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_clear_irr);\n\nstatic inline void apic_set_isr(int vec, struct kvm_lapic *apic)\n{\n\tstruct kvm_vcpu *vcpu;\n\n\tif (__apic_test_and_set_vector(vec, apic->regs + APIC_ISR))\n\t\treturn;\n\n\tvcpu = apic->vcpu;\n\n\t/*\n\t * With APIC virtualization enabled, all caching is disabled\n\t * because the processor can modify ISR under the hood.  Instead\n\t * just set SVI.\n\t */\n\tif (unlikely(vcpu->arch.apicv_active))\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(vcpu, vec);\n\telse {\n\t\t++apic->isr_count;\n\t\tBUG_ON(apic->isr_count > MAX_APIC_VECTOR);\n\t\t/*\n\t\t * ISR (in service register) bit is set when injecting an interrupt.\n\t\t * The highest vector is injected. Thus the latest bit set matches\n\t\t * the highest bit in ISR.\n\t\t */\n\t\tapic->highest_isr_cache = vec;\n\t}\n}\n\nstatic inline int apic_find_highest_isr(struct kvm_lapic *apic)\n{\n\tint result;\n\n\t/*\n\t * Note that isr_count is always 1, and highest_isr_cache\n\t * is always -1, with APIC virtualization enabled.\n\t */\n\tif (!apic->isr_count)\n\t\treturn -1;\n\tif (likely(apic->highest_isr_cache != -1))\n\t\treturn apic->highest_isr_cache;\n\n\tresult = find_highest_vector(apic->regs + APIC_ISR);\n\tASSERT(result == -1 || result >= 16);\n\n\treturn result;\n}\n\nstatic inline void apic_clear_isr(int vec, struct kvm_lapic *apic)\n{\n\tstruct kvm_vcpu *vcpu;\n\tif (!__apic_test_and_clear_vector(vec, apic->regs + APIC_ISR))\n\t\treturn;\n\n\tvcpu = apic->vcpu;\n\n\t/*\n\t * We do get here for APIC virtualization enabled if the guest\n\t * uses the Hyper-V APIC enlightenment.  In this case we may need\n\t * to trigger a new interrupt delivery by writing the SVI field;\n\t * on the other hand isr_count and highest_isr_cache are unused\n\t * and must be left alone.\n\t */\n\tif (unlikely(vcpu->arch.apicv_active))\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));\n\telse {\n\t\t--apic->isr_count;\n\t\tBUG_ON(apic->isr_count < 0);\n\t\tapic->highest_isr_cache = -1;\n\t}\n}\n\nint kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)\n{\n\t/* This may race with setting of irr in __apic_accept_irq() and\n\t * value returned may be wrong, but kvm_vcpu_kick() in __apic_accept_irq\n\t * will cause vmexit immediately and the value will be recalculated\n\t * on the next vmentry.\n\t */\n\treturn apic_find_highest_irr(vcpu->arch.apic);\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_find_highest_irr);\n\nstatic int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,\n\t\t\t     int vector, int level, int trig_mode,\n\t\t\t     struct dest_map *dest_map);\n\nint kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,\n\t\t     struct dest_map *dest_map)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\treturn __apic_accept_irq(apic, irq->delivery_mode, irq->vector,\n\t\t\tirq->level, irq->trig_mode, dest_map);\n}\n\nstatic int __pv_send_ipi(unsigned long *ipi_bitmap, struct kvm_apic_map *map,\n\t\t\t struct kvm_lapic_irq *irq, u32 min)\n{\n\tint i, count = 0;\n\tstruct kvm_vcpu *vcpu;\n\n\tif (min > map->max_apic_id)\n\t\treturn 0;\n\n\tfor_each_set_bit(i, ipi_bitmap,\n\t\tmin((u32)BITS_PER_LONG, (map->max_apic_id - min + 1))) {\n\t\tif (map->phys_map[min + i]) {\n\t\t\tvcpu = map->phys_map[min + i]->vcpu;\n\t\t\tcount += kvm_apic_set_irq(vcpu, irq, NULL);\n\t\t}\n\t}\n\n\treturn count;\n}\n\nint kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,\n\t\t    unsigned long ipi_bitmap_high, u32 min,\n\t\t    unsigned long icr, int op_64_bit)\n{\n\tstruct kvm_apic_map *map;\n\tstruct kvm_lapic_irq irq = {0};\n\tint cluster_size = op_64_bit ? 64 : 32;\n\tint count;\n\n\tif (icr & (APIC_DEST_MASK | APIC_SHORT_MASK))\n\t\treturn -KVM_EINVAL;\n\n\tirq.vector = icr & APIC_VECTOR_MASK;\n\tirq.delivery_mode = icr & APIC_MODE_MASK;\n\tirq.level = (icr & APIC_INT_ASSERT) != 0;\n\tirq.trig_mode = icr & APIC_INT_LEVELTRIG;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tcount = -EOPNOTSUPP;\n\tif (likely(map)) {\n\t\tcount = __pv_send_ipi(&ipi_bitmap_low, map, &irq, min);\n\t\tmin += cluster_size;\n\t\tcount += __pv_send_ipi(&ipi_bitmap_high, map, &irq, min);\n\t}\n\n\trcu_read_unlock();\n\treturn count;\n}\n\nstatic int pv_eoi_put_user(struct kvm_vcpu *vcpu, u8 val)\n{\n\n\treturn kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.pv_eoi.data, &val,\n\t\t\t\t      sizeof(val));\n}\n\nstatic int pv_eoi_get_user(struct kvm_vcpu *vcpu, u8 *val)\n{\n\n\treturn kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.pv_eoi.data, val,\n\t\t\t\t      sizeof(*val));\n}\n\nstatic inline bool pv_eoi_enabled(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.pv_eoi.msr_val & KVM_MSR_ENABLED;\n}\n\nstatic void pv_eoi_set_pending(struct kvm_vcpu *vcpu)\n{\n\tif (pv_eoi_put_user(vcpu, KVM_PV_EOI_ENABLED) < 0)\n\t\treturn;\n\n\t__set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);\n}\n\nstatic bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)\n{\n\tu8 val;\n\n\tif (pv_eoi_get_user(vcpu, &val) < 0)\n\t\treturn false;\n\n\tval &= KVM_PV_EOI_ENABLED;\n\n\tif (val && pv_eoi_put_user(vcpu, KVM_PV_EOI_DISABLED) < 0)\n\t\treturn false;\n\n\t/*\n\t * Clear pending bit in any case: it will be set again on vmentry.\n\t * While this might not be ideal from performance point of view,\n\t * this makes sure pv eoi is only enabled when we know it's safe.\n\t */\n\t__clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);\n\n\treturn val;\n}\n\nstatic int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)\n{\n\tint highest_irr;\n\tif (kvm_x86_ops.sync_pir_to_irr)\n\t\thighest_irr = static_call(kvm_x86_sync_pir_to_irr)(apic->vcpu);\n\telse\n\t\thighest_irr = apic_find_highest_irr(apic);\n\tif (highest_irr == -1 || (highest_irr & 0xF0) <= ppr)\n\t\treturn -1;\n\treturn highest_irr;\n}\n\nstatic bool __apic_update_ppr(struct kvm_lapic *apic, u32 *new_ppr)\n{\n\tu32 tpr, isrv, ppr, old_ppr;\n\tint isr;\n\n\told_ppr = kvm_lapic_get_reg(apic, APIC_PROCPRI);\n\ttpr = kvm_lapic_get_reg(apic, APIC_TASKPRI);\n\tisr = apic_find_highest_isr(apic);\n\tisrv = (isr != -1) ? isr : 0;\n\n\tif ((tpr & 0xf0) >= (isrv & 0xf0))\n\t\tppr = tpr & 0xff;\n\telse\n\t\tppr = isrv & 0xf0;\n\n\t*new_ppr = ppr;\n\tif (old_ppr != ppr)\n\t\tkvm_lapic_set_reg(apic, APIC_PROCPRI, ppr);\n\n\treturn ppr < old_ppr;\n}\n\nstatic void apic_update_ppr(struct kvm_lapic *apic)\n{\n\tu32 ppr;\n\n\tif (__apic_update_ppr(apic, &ppr) &&\n\t    apic_has_interrupt_for_ppr(apic, ppr) != -1)\n\t\tkvm_make_request(KVM_REQ_EVENT, apic->vcpu);\n}\n\nvoid kvm_apic_update_ppr(struct kvm_vcpu *vcpu)\n{\n\tapic_update_ppr(vcpu->arch.apic);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_update_ppr);\n\nstatic void apic_set_tpr(struct kvm_lapic *apic, u32 tpr)\n{\n\tkvm_lapic_set_reg(apic, APIC_TASKPRI, tpr);\n\tapic_update_ppr(apic);\n}\n\nstatic bool kvm_apic_broadcast(struct kvm_lapic *apic, u32 mda)\n{\n\treturn mda == (apic_x2apic_mode(apic) ?\n\t\t\tX2APIC_BROADCAST : APIC_BROADCAST);\n}\n\nstatic bool kvm_apic_match_physical_addr(struct kvm_lapic *apic, u32 mda)\n{\n\tif (kvm_apic_broadcast(apic, mda))\n\t\treturn true;\n\n\tif (apic_x2apic_mode(apic))\n\t\treturn mda == kvm_x2apic_id(apic);\n\n\t/*\n\t * Hotplug hack: Make LAPIC in xAPIC mode also accept interrupts as if\n\t * it were in x2APIC mode.  Hotplugged VCPUs start in xAPIC mode and\n\t * this allows unique addressing of VCPUs with APIC ID over 0xff.\n\t * The 0xff condition is needed because writeable xAPIC ID.\n\t */\n\tif (kvm_x2apic_id(apic) > 0xff && mda == kvm_x2apic_id(apic))\n\t\treturn true;\n\n\treturn mda == kvm_xapic_id(apic);\n}\n\nstatic bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)\n{\n\tu32 logical_id;\n\n\tif (kvm_apic_broadcast(apic, mda))\n\t\treturn true;\n\n\tlogical_id = kvm_lapic_get_reg(apic, APIC_LDR);\n\n\tif (apic_x2apic_mode(apic))\n\t\treturn ((logical_id >> 16) == (mda >> 16))\n\t\t       && (logical_id & mda & 0xffff) != 0;\n\n\tlogical_id = GET_APIC_LOGICAL_ID(logical_id);\n\n\tswitch (kvm_lapic_get_reg(apic, APIC_DFR)) {\n\tcase APIC_DFR_FLAT:\n\t\treturn (logical_id & mda) != 0;\n\tcase APIC_DFR_CLUSTER:\n\t\treturn ((logical_id >> 4) == (mda >> 4))\n\t\t       && (logical_id & mda & 0xf) != 0;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* The KVM local APIC implementation has two quirks:\n *\n *  - Real hardware delivers interrupts destined to x2APIC ID > 0xff to LAPICs\n *    in xAPIC mode if the \"destination & 0xff\" matches its xAPIC ID.\n *    KVM doesn't do that aliasing.\n *\n *  - in-kernel IOAPIC messages have to be delivered directly to\n *    x2APIC, because the kernel does not support interrupt remapping.\n *    In order to support broadcast without interrupt remapping, x2APIC\n *    rewrites the destination of non-IPI messages from APIC_BROADCAST\n *    to X2APIC_BROADCAST.\n *\n * The broadcast quirk can be disabled with KVM_CAP_X2APIC_API.  This is\n * important when userspace wants to use x2APIC-format MSIs, because\n * APIC_BROADCAST (0xff) is a legal route for \"cluster 0, CPUs 0-7\".\n */\nstatic u32 kvm_apic_mda(struct kvm_vcpu *vcpu, unsigned int dest_id,\n\t\tstruct kvm_lapic *source, struct kvm_lapic *target)\n{\n\tbool ipi = source != NULL;\n\n\tif (!vcpu->kvm->arch.x2apic_broadcast_quirk_disabled &&\n\t    !ipi && dest_id == APIC_BROADCAST && apic_x2apic_mode(target))\n\t\treturn X2APIC_BROADCAST;\n\n\treturn dest_id;\n}\n\nbool kvm_apic_match_dest(struct kvm_vcpu *vcpu, struct kvm_lapic *source,\n\t\t\t   int shorthand, unsigned int dest, int dest_mode)\n{\n\tstruct kvm_lapic *target = vcpu->arch.apic;\n\tu32 mda = kvm_apic_mda(vcpu, dest, source, target);\n\n\tASSERT(target);\n\tswitch (shorthand) {\n\tcase APIC_DEST_NOSHORT:\n\t\tif (dest_mode == APIC_DEST_PHYSICAL)\n\t\t\treturn kvm_apic_match_physical_addr(target, mda);\n\t\telse\n\t\t\treturn kvm_apic_match_logical_addr(target, mda);\n\tcase APIC_DEST_SELF:\n\t\treturn target == source;\n\tcase APIC_DEST_ALLINC:\n\t\treturn true;\n\tcase APIC_DEST_ALLBUT:\n\t\treturn target != source;\n\tdefault:\n\t\treturn false;\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_apic_match_dest);\n\nint kvm_vector_to_index(u32 vector, u32 dest_vcpus,\n\t\t       const unsigned long *bitmap, u32 bitmap_size)\n{\n\tu32 mod;\n\tint i, idx = -1;\n\n\tmod = vector % dest_vcpus;\n\n\tfor (i = 0; i <= mod; i++) {\n\t\tidx = find_next_bit(bitmap, bitmap_size, idx + 1);\n\t\tBUG_ON(idx == bitmap_size);\n\t}\n\n\treturn idx;\n}\n\nstatic void kvm_apic_disabled_lapic_found(struct kvm *kvm)\n{\n\tif (!kvm->arch.disabled_lapic_found) {\n\t\tkvm->arch.disabled_lapic_found = true;\n\t\tprintk(KERN_INFO\n\t\t       \"Disabled LAPIC found during irq injection\\n\");\n\t}\n}\n\nstatic bool kvm_apic_is_broadcast_dest(struct kvm *kvm, struct kvm_lapic **src,\n\t\tstruct kvm_lapic_irq *irq, struct kvm_apic_map *map)\n{\n\tif (kvm->arch.x2apic_broadcast_quirk_disabled) {\n\t\tif ((irq->dest_id == APIC_BROADCAST &&\n\t\t\t\tmap->mode != KVM_APIC_MODE_X2APIC))\n\t\t\treturn true;\n\t\tif (irq->dest_id == X2APIC_BROADCAST)\n\t\t\treturn true;\n\t} else {\n\t\tbool x2apic_ipi = src && *src && apic_x2apic_mode(*src);\n\t\tif (irq->dest_id == (x2apic_ipi ?\n\t\t                     X2APIC_BROADCAST : APIC_BROADCAST))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/* Return true if the interrupt can be handled by using *bitmap as index mask\n * for valid destinations in *dst array.\n * Return false if kvm_apic_map_get_dest_lapic did nothing useful.\n * Note: we may have zero kvm_lapic destinations when we return true, which\n * means that the interrupt should be dropped.  In this case, *bitmap would be\n * zero and *dst undefined.\n */\nstatic inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,\n\t\tstruct kvm_lapic **src, struct kvm_lapic_irq *irq,\n\t\tstruct kvm_apic_map *map, struct kvm_lapic ***dst,\n\t\tunsigned long *bitmap)\n{\n\tint i, lowest;\n\n\tif (irq->shorthand == APIC_DEST_SELF && src) {\n\t\t*dst = src;\n\t\t*bitmap = 1;\n\t\treturn true;\n\t} else if (irq->shorthand)\n\t\treturn false;\n\n\tif (!map || kvm_apic_is_broadcast_dest(kvm, src, irq, map))\n\t\treturn false;\n\n\tif (irq->dest_mode == APIC_DEST_PHYSICAL) {\n\t\tif (irq->dest_id > map->max_apic_id) {\n\t\t\t*bitmap = 0;\n\t\t} else {\n\t\t\tu32 dest_id = array_index_nospec(irq->dest_id, map->max_apic_id + 1);\n\t\t\t*dst = &map->phys_map[dest_id];\n\t\t\t*bitmap = 1;\n\t\t}\n\t\treturn true;\n\t}\n\n\t*bitmap = 0;\n\tif (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,\n\t\t\t\t(u16 *)bitmap))\n\t\treturn false;\n\n\tif (!kvm_lowest_prio_delivery(irq))\n\t\treturn true;\n\n\tif (!kvm_vector_hashing_enabled()) {\n\t\tlowest = -1;\n\t\tfor_each_set_bit(i, bitmap, 16) {\n\t\t\tif (!(*dst)[i])\n\t\t\t\tcontinue;\n\t\t\tif (lowest < 0)\n\t\t\t\tlowest = i;\n\t\t\telse if (kvm_apic_compare_prio((*dst)[i]->vcpu,\n\t\t\t\t\t\t(*dst)[lowest]->vcpu) < 0)\n\t\t\t\tlowest = i;\n\t\t}\n\t} else {\n\t\tif (!*bitmap)\n\t\t\treturn true;\n\n\t\tlowest = kvm_vector_to_index(irq->vector, hweight16(*bitmap),\n\t\t\t\tbitmap, 16);\n\n\t\tif (!(*dst)[lowest]) {\n\t\t\tkvm_apic_disabled_lapic_found(kvm);\n\t\t\t*bitmap = 0;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t*bitmap = (lowest >= 0) ? 1 << lowest : 0;\n\n\treturn true;\n}\n\nbool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,\n\t\tstruct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tint i;\n\tbool ret;\n\n\t*r = -1;\n\n\tif (irq->shorthand == APIC_DEST_SELF) {\n\t\tif (KVM_BUG_ON(!src, kvm)) {\n\t\t\t*r = 0;\n\t\t\treturn true;\n\t\t}\n\t\t*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);\n\t\treturn true;\n\t}\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);\n\tif (ret) {\n\t\t*r = 0;\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dst[i])\n\t\t\t\tcontinue;\n\t\t\t*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/*\n * This routine tries to handle interrupts in posted mode, here is how\n * it deals with different cases:\n * - For single-destination interrupts, handle it in posted mode\n * - Else if vector hashing is enabled and it is a lowest-priority\n *   interrupt, handle it in posted mode and use the following mechanism\n *   to find the destination vCPU.\n *\t1. For lowest-priority interrupts, store all the possible\n *\t   destination vCPUs in an array.\n *\t2. Use \"guest vector % max number of destination vCPUs\" to find\n *\t   the right destination vCPU in the array for the lowest-priority\n *\t   interrupt.\n * - Otherwise, use remapped mode to inject the interrupt.\n */\nbool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,\n\t\t\tstruct kvm_vcpu **dest_vcpu)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tbool ret = false;\n\n\tif (irq->shorthand)\n\t\treturn false;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tif (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&\n\t\t\thweight16(bitmap) == 1) {\n\t\tunsigned long i = find_first_bit(&bitmap, 16);\n\n\t\tif (dst[i]) {\n\t\t\t*dest_vcpu = dst[i]->vcpu;\n\t\t\tret = true;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/*\n * Add a pending IRQ into lapic.\n * Return 1 if successfully added and 0 if discarded.\n */\nstatic int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,\n\t\t\t     int vector, int level, int trig_mode,\n\t\t\t     struct dest_map *dest_map)\n{\n\tint result = 0;\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\n\ttrace_kvm_apic_accept_irq(vcpu->vcpu_id, delivery_mode,\n\t\t\t\t  trig_mode, vector);\n\tswitch (delivery_mode) {\n\tcase APIC_DM_LOWEST:\n\t\tvcpu->arch.apic_arb_prio++;\n\t\tfallthrough;\n\tcase APIC_DM_FIXED:\n\t\tif (unlikely(trig_mode && !level))\n\t\t\tbreak;\n\n\t\t/* FIXME add logic for vcpu on reset */\n\t\tif (unlikely(!apic_enabled(apic)))\n\t\t\tbreak;\n\n\t\tresult = 1;\n\n\t\tif (dest_map) {\n\t\t\t__set_bit(vcpu->vcpu_id, dest_map->map);\n\t\t\tdest_map->vectors[vcpu->vcpu_id] = vector;\n\t\t}\n\n\t\tif (apic_test_vector(vector, apic->regs + APIC_TMR) != !!trig_mode) {\n\t\t\tif (trig_mode)\n\t\t\t\tkvm_lapic_set_vector(vector,\n\t\t\t\t\t\t     apic->regs + APIC_TMR);\n\t\t\telse\n\t\t\t\tkvm_lapic_clear_vector(vector,\n\t\t\t\t\t\t       apic->regs + APIC_TMR);\n\t\t}\n\n\t\tstatic_call(kvm_x86_deliver_interrupt)(apic, delivery_mode,\n\t\t\t\t\t\t       trig_mode, vector);\n\t\tbreak;\n\n\tcase APIC_DM_REMRD:\n\t\tresult = 1;\n\t\tvcpu->arch.pv.pv_unhalted = 1;\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_SMI:\n\t\tresult = 1;\n\t\tkvm_make_request(KVM_REQ_SMI, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_NMI:\n\t\tresult = 1;\n\t\tkvm_inject_nmi(vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_INIT:\n\t\tif (!trig_mode || level) {\n\t\t\tresult = 1;\n\t\t\t/* assumes that there are only KVM_APIC_INIT/SIPI */\n\t\t\tapic->pending_events = (1UL << KVM_APIC_INIT);\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\t\tkvm_vcpu_kick(vcpu);\n\t\t}\n\t\tbreak;\n\n\tcase APIC_DM_STARTUP:\n\t\tresult = 1;\n\t\tapic->sipi_vector = vector;\n\t\t/* make sure sipi_vector is visible for the receiver */\n\t\tsmp_wmb();\n\t\tset_bit(KVM_APIC_SIPI, &apic->pending_events);\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_EXTINT:\n\t\t/*\n\t\t * Should only be called by kvm_apic_local_deliver() with LVT0,\n\t\t * before NMI watchdog was enabled. Already handled by\n\t\t * kvm_apic_accept_pic_intr().\n\t\t */\n\t\tbreak;\n\n\tdefault:\n\t\tprintk(KERN_ERR \"TODO: unsupported delivery mode %x\\n\",\n\t\t       delivery_mode);\n\t\tbreak;\n\t}\n\treturn result;\n}\n\n/*\n * This routine identifies the destination vcpus mask meant to receive the\n * IOAPIC interrupts. It either uses kvm_apic_map_get_dest_lapic() to find\n * out the destination vcpus array and set the bitmap or it traverses to\n * each available vcpu to identify the same.\n */\nvoid kvm_bitmap_or_dest_vcpus(struct kvm *kvm, struct kvm_lapic_irq *irq,\n\t\t\t      unsigned long *vcpu_bitmap)\n{\n\tstruct kvm_lapic **dest_vcpu = NULL;\n\tstruct kvm_lapic *src = NULL;\n\tstruct kvm_apic_map *map;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long bitmap, i;\n\tint vcpu_idx;\n\tbool ret;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,\n\t\t\t\t\t  &bitmap);\n\tif (ret) {\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dest_vcpu[i])\n\t\t\t\tcontinue;\n\t\t\tvcpu_idx = dest_vcpu[i]->vcpu->vcpu_idx;\n\t\t\t__set_bit(vcpu_idx, vcpu_bitmap);\n\t\t}\n\t} else {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (!kvm_apic_present(vcpu))\n\t\t\t\tcontinue;\n\t\t\tif (!kvm_apic_match_dest(vcpu, NULL,\n\t\t\t\t\t\t irq->shorthand,\n\t\t\t\t\t\t irq->dest_id,\n\t\t\t\t\t\t irq->dest_mode))\n\t\t\t\tcontinue;\n\t\t\t__set_bit(i, vcpu_bitmap);\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\nint kvm_apic_compare_prio(struct kvm_vcpu *vcpu1, struct kvm_vcpu *vcpu2)\n{\n\treturn vcpu1->arch.apic_arb_prio - vcpu2->arch.apic_arb_prio;\n}\n\nstatic bool kvm_ioapic_handles_vector(struct kvm_lapic *apic, int vector)\n{\n\treturn test_bit(vector, apic->vcpu->arch.ioapic_handled_vectors);\n}\n\nstatic void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)\n{\n\tint trigger_mode;\n\n\t/* Eoi the ioapic only if the ioapic doesn't own the vector. */\n\tif (!kvm_ioapic_handles_vector(apic, vector))\n\t\treturn;\n\n\t/* Request a KVM exit to inform the userspace IOAPIC. */\n\tif (irqchip_split(apic->vcpu->kvm)) {\n\t\tapic->vcpu->arch.pending_ioapic_eoi = vector;\n\t\tkvm_make_request(KVM_REQ_IOAPIC_EOI_EXIT, apic->vcpu);\n\t\treturn;\n\t}\n\n\tif (apic_test_vector(vector, apic->regs + APIC_TMR))\n\t\ttrigger_mode = IOAPIC_LEVEL_TRIG;\n\telse\n\t\ttrigger_mode = IOAPIC_EDGE_TRIG;\n\n\tkvm_ioapic_update_eoi(apic->vcpu, vector, trigger_mode);\n}\n\nstatic int apic_set_eoi(struct kvm_lapic *apic)\n{\n\tint vector = apic_find_highest_isr(apic);\n\n\ttrace_kvm_eoi(apic, vector);\n\n\t/*\n\t * Not every write EOI will has corresponding ISR,\n\t * one example is when Kernel check timer on setup_IO_APIC\n\t */\n\tif (vector == -1)\n\t\treturn vector;\n\n\tapic_clear_isr(vector, apic);\n\tapic_update_ppr(apic);\n\n\tif (to_hv_vcpu(apic->vcpu) &&\n\t    test_bit(vector, to_hv_synic(apic->vcpu)->vec_bitmap))\n\t\tkvm_hv_synic_send_eoi(apic->vcpu, vector);\n\n\tkvm_ioapic_send_eoi(apic, vector);\n\tkvm_make_request(KVM_REQ_EVENT, apic->vcpu);\n\treturn vector;\n}\n\n/*\n * this interface assumes a trap-like exit, which has already finished\n * desired side effect including vISR and vPPR update.\n */\nvoid kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\ttrace_kvm_eoi(apic, vector);\n\n\tkvm_ioapic_send_eoi(apic, vector);\n\tkvm_make_request(KVM_REQ_EVENT, apic->vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_set_eoi_accelerated);\n\nvoid kvm_apic_send_ipi(struct kvm_lapic *apic, u32 icr_low, u32 icr_high)\n{\n\tstruct kvm_lapic_irq irq;\n\n\t/* KVM has no delay and should always clear the BUSY/PENDING flag. */\n\tWARN_ON_ONCE(icr_low & APIC_ICR_BUSY);\n\n\tirq.vector = icr_low & APIC_VECTOR_MASK;\n\tirq.delivery_mode = icr_low & APIC_MODE_MASK;\n\tirq.dest_mode = icr_low & APIC_DEST_MASK;\n\tirq.level = (icr_low & APIC_INT_ASSERT) != 0;\n\tirq.trig_mode = icr_low & APIC_INT_LEVELTRIG;\n\tirq.shorthand = icr_low & APIC_SHORT_MASK;\n\tirq.msi_redir_hint = false;\n\tif (apic_x2apic_mode(apic))\n\t\tirq.dest_id = icr_high;\n\telse\n\t\tirq.dest_id = GET_APIC_DEST_FIELD(icr_high);\n\n\ttrace_kvm_apic_ipi(icr_low, irq.dest_id);\n\n\tkvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_send_ipi);\n\nstatic u32 apic_get_tmcct(struct kvm_lapic *apic)\n{\n\tktime_t remaining, now;\n\ts64 ns;\n\tu32 tmcct;\n\n\tASSERT(apic != NULL);\n\n\t/* if initial count is 0, current count should also be 0 */\n\tif (kvm_lapic_get_reg(apic, APIC_TMICT) == 0 ||\n\t\tapic->lapic_timer.period == 0)\n\t\treturn 0;\n\n\tnow = ktime_get();\n\tremaining = ktime_sub(apic->lapic_timer.target_expiration, now);\n\tif (ktime_to_ns(remaining) < 0)\n\t\tremaining = 0;\n\n\tns = mod_64(ktime_to_ns(remaining), apic->lapic_timer.period);\n\ttmcct = div64_u64(ns,\n\t\t\t (APIC_BUS_CYCLE_NS * apic->divide_count));\n\n\treturn tmcct;\n}\n\nstatic void __report_tpr_access(struct kvm_lapic *apic, bool write)\n{\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tstruct kvm_run *run = vcpu->run;\n\n\tkvm_make_request(KVM_REQ_REPORT_TPR_ACCESS, vcpu);\n\trun->tpr_access.rip = kvm_rip_read(vcpu);\n\trun->tpr_access.is_write = write;\n}\n\nstatic inline void report_tpr_access(struct kvm_lapic *apic, bool write)\n{\n\tif (apic->vcpu->arch.tpr_access_reporting)\n\t\t__report_tpr_access(apic, write);\n}\n\nstatic u32 __apic_read(struct kvm_lapic *apic, unsigned int offset)\n{\n\tu32 val = 0;\n\n\tif (offset >= LAPIC_MMIO_LENGTH)\n\t\treturn 0;\n\n\tswitch (offset) {\n\tcase APIC_ARBPRI:\n\t\tbreak;\n\n\tcase APIC_TMCCT:\t/* Timer CCR */\n\t\tif (apic_lvtt_tscdeadline(apic))\n\t\t\treturn 0;\n\n\t\tval = apic_get_tmcct(apic);\n\t\tbreak;\n\tcase APIC_PROCPRI:\n\t\tapic_update_ppr(apic);\n\t\tval = kvm_lapic_get_reg(apic, offset);\n\t\tbreak;\n\tcase APIC_TASKPRI:\n\t\treport_tpr_access(apic, false);\n\t\tfallthrough;\n\tdefault:\n\t\tval = kvm_lapic_get_reg(apic, offset);\n\t\tbreak;\n\t}\n\n\treturn val;\n}\n\nstatic inline struct kvm_lapic *to_lapic(struct kvm_io_device *dev)\n{\n\treturn container_of(dev, struct kvm_lapic, dev);\n}\n\n#define APIC_REG_MASK(reg)\t(1ull << ((reg) >> 4))\n#define APIC_REGS_MASK(first, count) \\\n\t(APIC_REG_MASK(first) * ((1ull << (count)) - 1))\n\nstatic int kvm_lapic_reg_read(struct kvm_lapic *apic, u32 offset, int len,\n\t\t\t      void *data)\n{\n\tunsigned char alignment = offset & 0xf;\n\tu32 result;\n\t/* this bitmask has a bit cleared for each reserved register */\n\tu64 valid_reg_mask =\n\t\tAPIC_REG_MASK(APIC_ID) |\n\t\tAPIC_REG_MASK(APIC_LVR) |\n\t\tAPIC_REG_MASK(APIC_TASKPRI) |\n\t\tAPIC_REG_MASK(APIC_PROCPRI) |\n\t\tAPIC_REG_MASK(APIC_LDR) |\n\t\tAPIC_REG_MASK(APIC_DFR) |\n\t\tAPIC_REG_MASK(APIC_SPIV) |\n\t\tAPIC_REGS_MASK(APIC_ISR, APIC_ISR_NR) |\n\t\tAPIC_REGS_MASK(APIC_TMR, APIC_ISR_NR) |\n\t\tAPIC_REGS_MASK(APIC_IRR, APIC_ISR_NR) |\n\t\tAPIC_REG_MASK(APIC_ESR) |\n\t\tAPIC_REG_MASK(APIC_ICR) |\n\t\tAPIC_REG_MASK(APIC_LVTT) |\n\t\tAPIC_REG_MASK(APIC_LVTTHMR) |\n\t\tAPIC_REG_MASK(APIC_LVTPC) |\n\t\tAPIC_REG_MASK(APIC_LVT0) |\n\t\tAPIC_REG_MASK(APIC_LVT1) |\n\t\tAPIC_REG_MASK(APIC_LVTERR) |\n\t\tAPIC_REG_MASK(APIC_TMICT) |\n\t\tAPIC_REG_MASK(APIC_TMCCT) |\n\t\tAPIC_REG_MASK(APIC_TDCR);\n\n\t/*\n\t * ARBPRI and ICR2 are not valid in x2APIC mode.  WARN if KVM reads ICR\n\t * in x2APIC mode as it's an 8-byte register in x2APIC and needs to be\n\t * manually handled by the caller.\n\t */\n\tif (!apic_x2apic_mode(apic))\n\t\tvalid_reg_mask |= APIC_REG_MASK(APIC_ARBPRI) |\n\t\t\t\t  APIC_REG_MASK(APIC_ICR2);\n\telse\n\t\tWARN_ON_ONCE(offset == APIC_ICR);\n\n\tif (alignment + len > 4)\n\t\treturn 1;\n\n\tif (offset > 0x3f0 || !(valid_reg_mask & APIC_REG_MASK(offset)))\n\t\treturn 1;\n\n\tresult = __apic_read(apic, offset & ~0xf);\n\n\ttrace_kvm_apic_read(offset, result);\n\n\tswitch (len) {\n\tcase 1:\n\tcase 2:\n\tcase 4:\n\t\tmemcpy(data, (char *)&result + alignment, len);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"Local APIC read with len = %x, \"\n\t\t       \"should be 1,2, or 4 instead\\n\", len);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int apic_mmio_in_range(struct kvm_lapic *apic, gpa_t addr)\n{\n\treturn addr >= apic->base_address &&\n\t\taddr < apic->base_address + LAPIC_MMIO_LENGTH;\n}\n\nstatic int apic_mmio_read(struct kvm_vcpu *vcpu, struct kvm_io_device *this,\n\t\t\t   gpa_t address, int len, void *data)\n{\n\tstruct kvm_lapic *apic = to_lapic(this);\n\tu32 offset = address - apic->base_address;\n\n\tif (!apic_mmio_in_range(apic, address))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {\n\t\tif (!kvm_check_has_quirk(vcpu->kvm,\n\t\t\t\t\t KVM_X86_QUIRK_LAPIC_MMIO_HOLE))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tmemset(data, 0xff, len);\n\t\treturn 0;\n\t}\n\n\tkvm_lapic_reg_read(apic, offset, len, data);\n\n\treturn 0;\n}\n\nstatic void update_divide_count(struct kvm_lapic *apic)\n{\n\tu32 tmp1, tmp2, tdcr;\n\n\ttdcr = kvm_lapic_get_reg(apic, APIC_TDCR);\n\ttmp1 = tdcr & 0xf;\n\ttmp2 = ((tmp1 & 0x3) | ((tmp1 & 0x8) >> 1)) + 1;\n\tapic->divide_count = 0x1 << (tmp2 & 0x7);\n}\n\nstatic void limit_periodic_timer_frequency(struct kvm_lapic *apic)\n{\n\t/*\n\t * Do not allow the guest to program periodic timers with small\n\t * interval, since the hrtimers are not throttled by the host\n\t * scheduler.\n\t */\n\tif (apic_lvtt_period(apic) && apic->lapic_timer.period) {\n\t\ts64 min_period = min_timer_period_us * 1000LL;\n\n\t\tif (apic->lapic_timer.period < min_period) {\n\t\t\tpr_info_ratelimited(\n\t\t\t    \"kvm: vcpu %i: requested %lld ns \"\n\t\t\t    \"lapic timer period limited to %lld ns\\n\",\n\t\t\t    apic->vcpu->vcpu_id,\n\t\t\t    apic->lapic_timer.period, min_period);\n\t\t\tapic->lapic_timer.period = min_period;\n\t\t}\n\t}\n}\n\nstatic void cancel_hv_timer(struct kvm_lapic *apic);\n\nstatic void cancel_apic_timer(struct kvm_lapic *apic)\n{\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\tpreempt_disable();\n\tif (apic->lapic_timer.hv_timer_in_use)\n\t\tcancel_hv_timer(apic);\n\tpreempt_enable();\n}\n\nstatic void apic_update_lvtt(struct kvm_lapic *apic)\n{\n\tu32 timer_mode = kvm_lapic_get_reg(apic, APIC_LVTT) &\n\t\t\tapic->lapic_timer.timer_mode_mask;\n\n\tif (apic->lapic_timer.timer_mode != timer_mode) {\n\t\tif (apic_lvtt_tscdeadline(apic) != (timer_mode ==\n\t\t\t\tAPIC_LVT_TIMER_TSCDEADLINE)) {\n\t\t\tcancel_apic_timer(apic);\n\t\t\tkvm_lapic_set_reg(apic, APIC_TMICT, 0);\n\t\t\tapic->lapic_timer.period = 0;\n\t\t\tapic->lapic_timer.tscdeadline = 0;\n\t\t}\n\t\tapic->lapic_timer.timer_mode = timer_mode;\n\t\tlimit_periodic_timer_frequency(apic);\n\t}\n}\n\n/*\n * On APICv, this test will cause a busy wait\n * during a higher-priority task.\n */\n\nstatic bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 reg = kvm_lapic_get_reg(apic, APIC_LVTT);\n\n\tif (kvm_apic_hw_enabled(apic)) {\n\t\tint vec = reg & APIC_VECTOR_MASK;\n\t\tvoid *bitmap = apic->regs + APIC_ISR;\n\n\t\tif (vcpu->arch.apicv_active)\n\t\t\tbitmap = apic->regs + APIC_IRR;\n\n\t\tif (apic_test_vector(vec, bitmap))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)\n{\n\tu64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;\n\n\t/*\n\t * If the guest TSC is running at a different ratio than the host, then\n\t * convert the delay to nanoseconds to achieve an accurate delay.  Note\n\t * that __delay() uses delay_tsc whenever the hardware has TSC, thus\n\t * always for VMX enabled hardware.\n\t */\n\tif (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {\n\t\t__delay(min(guest_cycles,\n\t\t\tnsec_to_cycles(vcpu, timer_advance_ns)));\n\t} else {\n\t\tu64 delay_ns = guest_cycles * 1000000ULL;\n\t\tdo_div(delay_ns, vcpu->arch.virtual_tsc_khz);\n\t\tndelay(min_t(u32, delay_ns, timer_advance_ns));\n\t}\n}\n\nstatic inline void adjust_lapic_timer_advance(struct kvm_vcpu *vcpu,\n\t\t\t\t\t      s64 advance_expire_delta)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;\n\tu64 ns;\n\n\t/* Do not adjust for tiny fluctuations or large random spikes. */\n\tif (abs(advance_expire_delta) > LAPIC_TIMER_ADVANCE_ADJUST_MAX ||\n\t    abs(advance_expire_delta) < LAPIC_TIMER_ADVANCE_ADJUST_MIN)\n\t\treturn;\n\n\t/* too early */\n\tif (advance_expire_delta < 0) {\n\t\tns = -advance_expire_delta * 1000000ULL;\n\t\tdo_div(ns, vcpu->arch.virtual_tsc_khz);\n\t\ttimer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;\n\t} else {\n\t/* too late */\n\t\tns = advance_expire_delta * 1000000ULL;\n\t\tdo_div(ns, vcpu->arch.virtual_tsc_khz);\n\t\ttimer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;\n\t}\n\n\tif (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))\n\t\ttimer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;\n\tapic->lapic_timer.timer_advance_ns = timer_advance_ns;\n}\n\nstatic void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu64 guest_tsc, tsc_deadline;\n\n\ttsc_deadline = apic->lapic_timer.expired_tscdeadline;\n\tapic->lapic_timer.expired_tscdeadline = 0;\n\tguest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\tapic->lapic_timer.advance_expire_delta = guest_tsc - tsc_deadline;\n\n\tif (lapic_timer_advance_dynamic) {\n\t\tadjust_lapic_timer_advance(vcpu, apic->lapic_timer.advance_expire_delta);\n\t\t/*\n\t\t * If the timer fired early, reread the TSC to account for the\n\t\t * overhead of the above adjustment to avoid waiting longer\n\t\t * than is necessary.\n\t\t */\n\t\tif (guest_tsc < tsc_deadline)\n\t\t\tguest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\t}\n\n\tif (guest_tsc < tsc_deadline)\n\t\t__wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);\n}\n\nvoid kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)\n{\n\tif (lapic_in_kernel(vcpu) &&\n\t    vcpu->arch.apic->lapic_timer.expired_tscdeadline &&\n\t    vcpu->arch.apic->lapic_timer.timer_advance_ns &&\n\t    lapic_timer_int_injected(vcpu))\n\t\t__kvm_wait_lapic_expire(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_wait_lapic_expire);\n\nstatic void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\n\tkvm_apic_local_deliver(apic, APIC_LVTT);\n\tif (apic_lvtt_tscdeadline(apic)) {\n\t\tktimer->tscdeadline = 0;\n\t} else if (apic_lvtt_oneshot(apic)) {\n\t\tktimer->tscdeadline = 0;\n\t\tktimer->target_expiration = 0;\n\t}\n}\n\nstatic void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)\n{\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\n\tif (atomic_read(&apic->lapic_timer.pending))\n\t\treturn;\n\n\tif (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)\n\t\tktimer->expired_tscdeadline = ktimer->tscdeadline;\n\n\tif (!from_timer_fn && vcpu->arch.apicv_active) {\n\t\tWARN_ON(kvm_get_running_vcpu() != vcpu);\n\t\tkvm_apic_inject_pending_timer_irqs(apic);\n\t\treturn;\n\t}\n\n\tif (kvm_use_posted_timer_interrupt(apic->vcpu)) {\n\t\t/*\n\t\t * Ensure the guest's timer has truly expired before posting an\n\t\t * interrupt.  Open code the relevant checks to avoid querying\n\t\t * lapic_timer_int_injected(), which will be false since the\n\t\t * interrupt isn't yet injected.  Waiting until after injecting\n\t\t * is not an option since that won't help a posted interrupt.\n\t\t */\n\t\tif (vcpu->arch.apic->lapic_timer.expired_tscdeadline &&\n\t\t    vcpu->arch.apic->lapic_timer.timer_advance_ns)\n\t\t\t__kvm_wait_lapic_expire(vcpu);\n\t\tkvm_apic_inject_pending_timer_irqs(apic);\n\t\treturn;\n\t}\n\n\tatomic_inc(&apic->lapic_timer.pending);\n\tkvm_make_request(KVM_REQ_UNBLOCK, vcpu);\n\tif (from_timer_fn)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nstatic void start_sw_tscdeadline(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\tu64 guest_tsc, tscdeadline = ktimer->tscdeadline;\n\tu64 ns = 0;\n\tktime_t expire;\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tunsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;\n\tunsigned long flags;\n\tktime_t now;\n\n\tif (unlikely(!tscdeadline || !this_tsc_khz))\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tnow = ktime_get();\n\tguest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\n\tns = (tscdeadline - guest_tsc) * 1000000ULL;\n\tdo_div(ns, this_tsc_khz);\n\n\tif (likely(tscdeadline > guest_tsc) &&\n\t    likely(ns > apic->lapic_timer.timer_advance_ns)) {\n\t\texpire = ktime_add_ns(now, ns);\n\t\texpire = ktime_sub_ns(expire, ktimer->timer_advance_ns);\n\t\thrtimer_start(&ktimer->timer, expire, HRTIMER_MODE_ABS_HARD);\n\t} else\n\t\tapic_timer_expired(apic, false);\n\n\tlocal_irq_restore(flags);\n}\n\nstatic inline u64 tmict_to_ns(struct kvm_lapic *apic, u32 tmict)\n{\n\treturn (u64)tmict * APIC_BUS_CYCLE_NS * (u64)apic->divide_count;\n}\n\nstatic void update_target_expiration(struct kvm_lapic *apic, uint32_t old_divisor)\n{\n\tktime_t now, remaining;\n\tu64 ns_remaining_old, ns_remaining_new;\n\n\tapic->lapic_timer.period =\n\t\t\ttmict_to_ns(apic, kvm_lapic_get_reg(apic, APIC_TMICT));\n\tlimit_periodic_timer_frequency(apic);\n\n\tnow = ktime_get();\n\tremaining = ktime_sub(apic->lapic_timer.target_expiration, now);\n\tif (ktime_to_ns(remaining) < 0)\n\t\tremaining = 0;\n\n\tns_remaining_old = ktime_to_ns(remaining);\n\tns_remaining_new = mul_u64_u32_div(ns_remaining_old,\n\t                                   apic->divide_count, old_divisor);\n\n\tapic->lapic_timer.tscdeadline +=\n\t\tnsec_to_cycles(apic->vcpu, ns_remaining_new) -\n\t\tnsec_to_cycles(apic->vcpu, ns_remaining_old);\n\tapic->lapic_timer.target_expiration = ktime_add_ns(now, ns_remaining_new);\n}\n\nstatic bool set_target_expiration(struct kvm_lapic *apic, u32 count_reg)\n{\n\tktime_t now;\n\tu64 tscl = rdtsc();\n\ts64 deadline;\n\n\tnow = ktime_get();\n\tapic->lapic_timer.period =\n\t\t\ttmict_to_ns(apic, kvm_lapic_get_reg(apic, APIC_TMICT));\n\n\tif (!apic->lapic_timer.period) {\n\t\tapic->lapic_timer.tscdeadline = 0;\n\t\treturn false;\n\t}\n\n\tlimit_periodic_timer_frequency(apic);\n\tdeadline = apic->lapic_timer.period;\n\n\tif (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic)) {\n\t\tif (unlikely(count_reg != APIC_TMICT)) {\n\t\t\tdeadline = tmict_to_ns(apic,\n\t\t\t\t     kvm_lapic_get_reg(apic, count_reg));\n\t\t\tif (unlikely(deadline <= 0))\n\t\t\t\tdeadline = apic->lapic_timer.period;\n\t\t\telse if (unlikely(deadline > apic->lapic_timer.period)) {\n\t\t\t\tpr_info_ratelimited(\n\t\t\t\t    \"kvm: vcpu %i: requested lapic timer restore with \"\n\t\t\t\t    \"starting count register %#x=%u (%lld ns) > initial count (%lld ns). \"\n\t\t\t\t    \"Using initial count to start timer.\\n\",\n\t\t\t\t    apic->vcpu->vcpu_id,\n\t\t\t\t    count_reg,\n\t\t\t\t    kvm_lapic_get_reg(apic, count_reg),\n\t\t\t\t    deadline, apic->lapic_timer.period);\n\t\t\t\tkvm_lapic_set_reg(apic, count_reg, 0);\n\t\t\t\tdeadline = apic->lapic_timer.period;\n\t\t\t}\n\t\t}\n\t}\n\n\tapic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +\n\t\tnsec_to_cycles(apic->vcpu, deadline);\n\tapic->lapic_timer.target_expiration = ktime_add_ns(now, deadline);\n\n\treturn true;\n}\n\nstatic void advance_periodic_target_expiration(struct kvm_lapic *apic)\n{\n\tktime_t now = ktime_get();\n\tu64 tscl = rdtsc();\n\tktime_t delta;\n\n\t/*\n\t * Synchronize both deadlines to the same time source or\n\t * differences in the periods (caused by differences in the\n\t * underlying clocks or numerical approximation errors) will\n\t * cause the two to drift apart over time as the errors\n\t * accumulate.\n\t */\n\tapic->lapic_timer.target_expiration =\n\t\tktime_add_ns(apic->lapic_timer.target_expiration,\n\t\t\t\tapic->lapic_timer.period);\n\tdelta = ktime_sub(apic->lapic_timer.target_expiration, now);\n\tapic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +\n\t\tnsec_to_cycles(apic->vcpu, delta);\n}\n\nstatic void start_sw_period(struct kvm_lapic *apic)\n{\n\tif (!apic->lapic_timer.period)\n\t\treturn;\n\n\tif (ktime_after(ktime_get(),\n\t\t\tapic->lapic_timer.target_expiration)) {\n\t\tapic_timer_expired(apic, false);\n\n\t\tif (apic_lvtt_oneshot(apic))\n\t\t\treturn;\n\n\t\tadvance_periodic_target_expiration(apic);\n\t}\n\n\thrtimer_start(&apic->lapic_timer.timer,\n\t\tapic->lapic_timer.target_expiration,\n\t\tHRTIMER_MODE_ABS_HARD);\n}\n\nbool kvm_lapic_hv_timer_in_use(struct kvm_vcpu *vcpu)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn false;\n\n\treturn vcpu->arch.apic->lapic_timer.hv_timer_in_use;\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_hv_timer_in_use);\n\nstatic void cancel_hv_timer(struct kvm_lapic *apic)\n{\n\tWARN_ON(preemptible());\n\tWARN_ON(!apic->lapic_timer.hv_timer_in_use);\n\tstatic_call(kvm_x86_cancel_hv_timer)(apic->vcpu);\n\tapic->lapic_timer.hv_timer_in_use = false;\n}\n\nstatic bool start_hv_timer(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tbool expired;\n\n\tWARN_ON(preemptible());\n\tif (!kvm_can_use_hv_timer(vcpu))\n\t\treturn false;\n\n\tif (!ktimer->tscdeadline)\n\t\treturn false;\n\n\tif (static_call(kvm_x86_set_hv_timer)(vcpu, ktimer->tscdeadline, &expired))\n\t\treturn false;\n\n\tktimer->hv_timer_in_use = true;\n\thrtimer_cancel(&ktimer->timer);\n\n\t/*\n\t * To simplify handling the periodic timer, leave the hv timer running\n\t * even if the deadline timer has expired, i.e. rely on the resulting\n\t * VM-Exit to recompute the periodic timer's target expiration.\n\t */\n\tif (!apic_lvtt_period(apic)) {\n\t\t/*\n\t\t * Cancel the hv timer if the sw timer fired while the hv timer\n\t\t * was being programmed, or if the hv timer itself expired.\n\t\t */\n\t\tif (atomic_read(&ktimer->pending)) {\n\t\t\tcancel_hv_timer(apic);\n\t\t} else if (expired) {\n\t\t\tapic_timer_expired(apic, false);\n\t\t\tcancel_hv_timer(apic);\n\t\t}\n\t}\n\n\ttrace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);\n\n\treturn true;\n}\n\nstatic void start_sw_timer(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\n\tWARN_ON(preemptible());\n\tif (apic->lapic_timer.hv_timer_in_use)\n\t\tcancel_hv_timer(apic);\n\tif (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))\n\t\treturn;\n\n\tif (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))\n\t\tstart_sw_period(apic);\n\telse if (apic_lvtt_tscdeadline(apic))\n\t\tstart_sw_tscdeadline(apic);\n\ttrace_kvm_hv_timer_state(apic->vcpu->vcpu_id, false);\n}\n\nstatic void restart_apic_timer(struct kvm_lapic *apic)\n{\n\tpreempt_disable();\n\n\tif (!apic_lvtt_period(apic) && atomic_read(&apic->lapic_timer.pending))\n\t\tgoto out;\n\n\tif (!start_hv_timer(apic))\n\t\tstart_sw_timer(apic);\nout:\n\tpreempt_enable();\n}\n\nvoid kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tpreempt_disable();\n\t/* If the preempt notifier has already run, it also called apic_timer_expired */\n\tif (!apic->lapic_timer.hv_timer_in_use)\n\t\tgoto out;\n\tWARN_ON(kvm_vcpu_is_blocking(vcpu));\n\tapic_timer_expired(apic, false);\n\tcancel_hv_timer(apic);\n\n\tif (apic_lvtt_period(apic) && apic->lapic_timer.period) {\n\t\tadvance_periodic_target_expiration(apic);\n\t\trestart_apic_timer(apic);\n\t}\nout:\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_expired_hv_timer);\n\nvoid kvm_lapic_switch_to_hv_timer(struct kvm_vcpu *vcpu)\n{\n\trestart_apic_timer(vcpu->arch.apic);\n}\n\nvoid kvm_lapic_switch_to_sw_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tpreempt_disable();\n\t/* Possibly the TSC deadline timer is not enabled yet */\n\tif (apic->lapic_timer.hv_timer_in_use)\n\t\tstart_sw_timer(apic);\n\tpreempt_enable();\n}\n\nvoid kvm_lapic_restart_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tWARN_ON(!apic->lapic_timer.hv_timer_in_use);\n\trestart_apic_timer(apic);\n}\n\nstatic void __start_apic_timer(struct kvm_lapic *apic, u32 count_reg)\n{\n\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\tif ((apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))\n\t    && !set_target_expiration(apic, count_reg))\n\t\treturn;\n\n\trestart_apic_timer(apic);\n}\n\nstatic void start_apic_timer(struct kvm_lapic *apic)\n{\n\t__start_apic_timer(apic, APIC_TMICT);\n}\n\nstatic void apic_manage_nmi_watchdog(struct kvm_lapic *apic, u32 lvt0_val)\n{\n\tbool lvt0_in_nmi_mode = apic_lvt_nmi_mode(lvt0_val);\n\n\tif (apic->lvt0_in_nmi_mode != lvt0_in_nmi_mode) {\n\t\tapic->lvt0_in_nmi_mode = lvt0_in_nmi_mode;\n\t\tif (lvt0_in_nmi_mode) {\n\t\t\tatomic_inc(&apic->vcpu->kvm->arch.vapics_in_nmi_mode);\n\t\t} else\n\t\t\tatomic_dec(&apic->vcpu->kvm->arch.vapics_in_nmi_mode);\n\t}\n}\n\nstatic int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)\n{\n\tint ret = 0;\n\n\ttrace_kvm_apic_write(reg, val);\n\n\tswitch (reg) {\n\tcase APIC_ID:\t\t/* Local APIC ID */\n\t\tif (!apic_x2apic_mode(apic))\n\t\t\tkvm_apic_set_xapic_id(apic, val >> 24);\n\t\telse\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_TASKPRI:\n\t\treport_tpr_access(apic, true);\n\t\tapic_set_tpr(apic, val & 0xff);\n\t\tbreak;\n\n\tcase APIC_EOI:\n\t\tapic_set_eoi(apic);\n\t\tbreak;\n\n\tcase APIC_LDR:\n\t\tif (!apic_x2apic_mode(apic))\n\t\t\tkvm_apic_set_ldr(apic, val & APIC_LDR_MASK);\n\t\telse\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_DFR:\n\t\tif (!apic_x2apic_mode(apic))\n\t\t\tkvm_apic_set_dfr(apic, val | 0x0FFFFFFF);\n\t\telse\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_SPIV: {\n\t\tu32 mask = 0x3ff;\n\t\tif (kvm_lapic_get_reg(apic, APIC_LVR) & APIC_LVR_DIRECTED_EOI)\n\t\t\tmask |= APIC_SPIV_DIRECTED_EOI;\n\t\tapic_set_spiv(apic, val & mask);\n\t\tif (!(val & APIC_SPIV_APIC_ENABLED)) {\n\t\t\tint i;\n\t\t\tu32 lvt_val;\n\n\t\t\tfor (i = 0; i < KVM_APIC_LVT_NUM; i++) {\n\t\t\t\tlvt_val = kvm_lapic_get_reg(apic,\n\t\t\t\t\t\t       APIC_LVTT + 0x10 * i);\n\t\t\t\tkvm_lapic_set_reg(apic, APIC_LVTT + 0x10 * i,\n\t\t\t\t\t     lvt_val | APIC_LVT_MASKED);\n\t\t\t}\n\t\t\tapic_update_lvtt(apic);\n\t\t\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\t\t}\n\t\tbreak;\n\t}\n\tcase APIC_ICR:\n\t\tWARN_ON_ONCE(apic_x2apic_mode(apic));\n\n\t\t/* No delay here, so we always clear the pending bit */\n\t\tval &= ~APIC_ICR_BUSY;\n\t\tkvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));\n\t\tkvm_lapic_set_reg(apic, APIC_ICR, val);\n\t\tbreak;\n\tcase APIC_ICR2:\n\t\tif (apic_x2apic_mode(apic))\n\t\t\tret = 1;\n\t\telse\n\t\t\tkvm_lapic_set_reg(apic, APIC_ICR2, val & 0xff000000);\n\t\tbreak;\n\n\tcase APIC_LVT0:\n\t\tapic_manage_nmi_watchdog(apic, val);\n\t\tfallthrough;\n\tcase APIC_LVTTHMR:\n\tcase APIC_LVTPC:\n\tcase APIC_LVT1:\n\tcase APIC_LVTERR: {\n\t\t/* TODO: Check vector */\n\t\tsize_t size;\n\t\tu32 index;\n\n\t\tif (!kvm_apic_sw_enabled(apic))\n\t\t\tval |= APIC_LVT_MASKED;\n\t\tsize = ARRAY_SIZE(apic_lvt_mask);\n\t\tindex = array_index_nospec(\n\t\t\t\t(reg - APIC_LVTT) >> 4, size);\n\t\tval &= apic_lvt_mask[index];\n\t\tkvm_lapic_set_reg(apic, reg, val);\n\t\tbreak;\n\t}\n\n\tcase APIC_LVTT:\n\t\tif (!kvm_apic_sw_enabled(apic))\n\t\t\tval |= APIC_LVT_MASKED;\n\t\tval &= (apic_lvt_mask[0] | apic->lapic_timer.timer_mode_mask);\n\t\tkvm_lapic_set_reg(apic, APIC_LVTT, val);\n\t\tapic_update_lvtt(apic);\n\t\tbreak;\n\n\tcase APIC_TMICT:\n\t\tif (apic_lvtt_tscdeadline(apic))\n\t\t\tbreak;\n\n\t\tcancel_apic_timer(apic);\n\t\tkvm_lapic_set_reg(apic, APIC_TMICT, val);\n\t\tstart_apic_timer(apic);\n\t\tbreak;\n\n\tcase APIC_TDCR: {\n\t\tuint32_t old_divisor = apic->divide_count;\n\n\t\tkvm_lapic_set_reg(apic, APIC_TDCR, val & 0xb);\n\t\tupdate_divide_count(apic);\n\t\tif (apic->divide_count != old_divisor &&\n\t\t\t\tapic->lapic_timer.period) {\n\t\t\thrtimer_cancel(&apic->lapic_timer.timer);\n\t\t\tupdate_target_expiration(apic, old_divisor);\n\t\t\trestart_apic_timer(apic);\n\t\t}\n\t\tbreak;\n\t}\n\tcase APIC_ESR:\n\t\tif (apic_x2apic_mode(apic) && val != 0)\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_SELF_IPI:\n\t\tif (apic_x2apic_mode(apic))\n\t\t\tkvm_apic_send_ipi(apic, APIC_DEST_SELF | (val & APIC_VECTOR_MASK), 0);\n\t\telse\n\t\t\tret = 1;\n\t\tbreak;\n\tdefault:\n\t\tret = 1;\n\t\tbreak;\n\t}\n\n\t/*\n\t * Recalculate APIC maps if necessary, e.g. if the software enable bit\n\t * was toggled, the APIC ID changed, etc...   The maps are marked dirty\n\t * on relevant changes, i.e. this is a nop for most writes.\n\t */\n\tkvm_recalculate_apic_map(apic->vcpu->kvm);\n\n\treturn ret;\n}\n\nstatic int apic_mmio_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this,\n\t\t\t    gpa_t address, int len, const void *data)\n{\n\tstruct kvm_lapic *apic = to_lapic(this);\n\tunsigned int offset = address - apic->base_address;\n\tu32 val;\n\n\tif (!apic_mmio_in_range(apic, address))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {\n\t\tif (!kvm_check_has_quirk(vcpu->kvm,\n\t\t\t\t\t KVM_X86_QUIRK_LAPIC_MMIO_HOLE))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\treturn 0;\n\t}\n\n\t/*\n\t * APIC register must be aligned on 128-bits boundary.\n\t * 32/64/128 bits registers must be accessed thru 32 bits.\n\t * Refer SDM 8.4.1\n\t */\n\tif (len != 4 || (offset & 0xf))\n\t\treturn 0;\n\n\tval = *(u32*)data;\n\n\tkvm_lapic_reg_write(apic, offset & 0xff0, val);\n\n\treturn 0;\n}\n\nvoid kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)\n{\n\tkvm_lapic_reg_write(vcpu->arch.apic, APIC_EOI, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_set_eoi);\n\n/* emulate APIC access in a trap manner */\nvoid kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)\n{\n\tu32 val = kvm_lapic_get_reg(vcpu->arch.apic, offset);\n\n\t/* TODO: optimize to just emulate side effect w/o one more write */\n\tkvm_lapic_reg_write(vcpu->arch.apic, offset, val);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_write_nodecode);\n\nvoid kvm_free_lapic(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (!vcpu->arch.apic)\n\t\treturn;\n\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\n\tif (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))\n\t\tstatic_branch_slow_dec_deferred(&apic_hw_disabled);\n\n\tif (!apic->sw_enabled)\n\t\tstatic_branch_slow_dec_deferred(&apic_sw_disabled);\n\n\tif (apic->regs)\n\t\tfree_page((unsigned long)apic->regs);\n\n\tkfree(apic);\n}\n\n/*\n *----------------------------------------------------------------------\n * LAPIC interface\n *----------------------------------------------------------------------\n */\nu64 kvm_get_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (!kvm_apic_present(vcpu) || !apic_lvtt_tscdeadline(apic))\n\t\treturn 0;\n\n\treturn apic->lapic_timer.tscdeadline;\n}\n\nvoid kvm_set_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu, u64 data)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (!kvm_apic_present(vcpu) || !apic_lvtt_tscdeadline(apic))\n\t\treturn;\n\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\tapic->lapic_timer.tscdeadline = data;\n\tstart_apic_timer(apic);\n}\n\nvoid kvm_lapic_set_tpr(struct kvm_vcpu *vcpu, unsigned long cr8)\n{\n\tapic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);\n}\n\nu64 kvm_lapic_get_cr8(struct kvm_vcpu *vcpu)\n{\n\tu64 tpr;\n\n\ttpr = (u64) kvm_lapic_get_reg(vcpu->arch.apic, APIC_TASKPRI);\n\n\treturn (tpr & 0xf0) >> 4;\n}\n\nvoid kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)\n{\n\tu64 old_value = vcpu->arch.apic_base;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tvcpu->arch.apic_base = value;\n\n\tif ((old_value ^ value) & MSR_IA32_APICBASE_ENABLE)\n\t\tkvm_update_cpuid_runtime(vcpu);\n\n\tif (!apic)\n\t\treturn;\n\n\t/* update jump label if enable bit changes */\n\tif ((old_value ^ value) & MSR_IA32_APICBASE_ENABLE) {\n\t\tif (value & MSR_IA32_APICBASE_ENABLE) {\n\t\t\tkvm_apic_set_xapic_id(apic, vcpu->vcpu_id);\n\t\t\tstatic_branch_slow_dec_deferred(&apic_hw_disabled);\n\t\t\t/* Check if there are APF page ready requests pending */\n\t\t\tkvm_make_request(KVM_REQ_APF_READY, vcpu);\n\t\t} else {\n\t\t\tstatic_branch_inc(&apic_hw_disabled.key);\n\t\t\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n\t\t}\n\t}\n\n\tif (((old_value ^ value) & X2APIC_ENABLE) && (value & X2APIC_ENABLE))\n\t\tkvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);\n\n\tif ((old_value ^ value) & (MSR_IA32_APICBASE_ENABLE | X2APIC_ENABLE))\n\t\tstatic_call_cond(kvm_x86_set_virtual_apic_mode)(vcpu);\n\n\tapic->base_address = apic->vcpu->arch.apic_base &\n\t\t\t     MSR_IA32_APICBASE_BASE;\n\n\tif ((value & MSR_IA32_APICBASE_ENABLE) &&\n\t     apic->base_address != APIC_DEFAULT_PHYS_BASE)\n\t\tpr_warn_once(\"APIC base relocation is unsupported by KVM\");\n}\n\nvoid kvm_apic_update_apicv(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (vcpu->arch.apicv_active) {\n\t\t/* irr_pending is always true when apicv is activated. */\n\t\tapic->irr_pending = true;\n\t\tapic->isr_count = 1;\n\t} else {\n\t\t/*\n\t\t * Don't clear irr_pending, searching the IRR can race with\n\t\t * updates from the CPU as APICv is still active from hardware's\n\t\t * perspective.  The flag will be cleared as appropriate when\n\t\t * KVM injects the interrupt.\n\t\t */\n\t\tapic->isr_count = count_vectors(apic->regs + APIC_ISR);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_apic_update_apicv);\n\nvoid kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu64 msr_val;\n\tint i;\n\n\tif (!init_event) {\n\t\tmsr_val = APIC_DEFAULT_PHYS_BASE | MSR_IA32_APICBASE_ENABLE;\n\t\tif (kvm_vcpu_is_reset_bsp(vcpu))\n\t\t\tmsr_val |= MSR_IA32_APICBASE_BSP;\n\t\tkvm_lapic_set_base(vcpu, msr_val);\n\t}\n\n\tif (!apic)\n\t\treturn;\n\n\t/* Stop the timer in case it's a reset to an active apic */\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\n\t/* The xAPIC ID is set at RESET even if the APIC was already enabled. */\n\tif (!init_event)\n\t\tkvm_apic_set_xapic_id(apic, vcpu->vcpu_id);\n\tkvm_apic_set_version(apic->vcpu);\n\n\tfor (i = 0; i < KVM_APIC_LVT_NUM; i++)\n\t\tkvm_lapic_set_reg(apic, APIC_LVTT + 0x10 * i, APIC_LVT_MASKED);\n\tapic_update_lvtt(apic);\n\tif (kvm_vcpu_is_reset_bsp(vcpu) &&\n\t    kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_LINT0_REENABLED))\n\t\tkvm_lapic_set_reg(apic, APIC_LVT0,\n\t\t\t     SET_APIC_DELIVERY_MODE(0, APIC_MODE_EXTINT));\n\tapic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));\n\n\tkvm_apic_set_dfr(apic, 0xffffffffU);\n\tapic_set_spiv(apic, 0xff);\n\tkvm_lapic_set_reg(apic, APIC_TASKPRI, 0);\n\tif (!apic_x2apic_mode(apic))\n\t\tkvm_apic_set_ldr(apic, 0);\n\tkvm_lapic_set_reg(apic, APIC_ESR, 0);\n\tif (!apic_x2apic_mode(apic)) {\n\t\tkvm_lapic_set_reg(apic, APIC_ICR, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_ICR2, 0);\n\t} else {\n\t\tkvm_lapic_set_reg64(apic, APIC_ICR, 0);\n\t}\n\tkvm_lapic_set_reg(apic, APIC_TDCR, 0);\n\tkvm_lapic_set_reg(apic, APIC_TMICT, 0);\n\tfor (i = 0; i < 8; i++) {\n\t\tkvm_lapic_set_reg(apic, APIC_IRR + 0x10 * i, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_ISR + 0x10 * i, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_TMR + 0x10 * i, 0);\n\t}\n\tkvm_apic_update_apicv(vcpu);\n\tapic->highest_isr_cache = -1;\n\tupdate_divide_count(apic);\n\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\tvcpu->arch.pv_eoi.msr_val = 0;\n\tapic_update_ppr(apic);\n\tif (vcpu->arch.apicv_active) {\n\t\tstatic_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);\n\t\tstatic_call_cond(kvm_x86_hwapic_irr_update)(vcpu, -1);\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(vcpu, -1);\n\t}\n\n\tvcpu->arch.apic_arb_prio = 0;\n\tvcpu->arch.apic_attention = 0;\n\n\tkvm_recalculate_apic_map(vcpu->kvm);\n}\n\n/*\n *----------------------------------------------------------------------\n * timer interface\n *----------------------------------------------------------------------\n */\n\nstatic bool lapic_is_periodic(struct kvm_lapic *apic)\n{\n\treturn apic_lvtt_period(apic);\n}\n\nint apic_has_pending_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (apic_enabled(apic) && apic_lvt_enabled(apic, APIC_LVTT))\n\t\treturn atomic_read(&apic->lapic_timer.pending);\n\n\treturn 0;\n}\n\nint kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)\n{\n\tu32 reg = kvm_lapic_get_reg(apic, lvt_type);\n\tint vector, mode, trig_mode;\n\n\tif (kvm_apic_hw_enabled(apic) && !(reg & APIC_LVT_MASKED)) {\n\t\tvector = reg & APIC_VECTOR_MASK;\n\t\tmode = reg & APIC_MODE_MASK;\n\t\ttrig_mode = reg & APIC_LVT_LEVEL_TRIGGER;\n\t\treturn __apic_accept_irq(apic, mode, vector, 1, trig_mode,\n\t\t\t\t\tNULL);\n\t}\n\treturn 0;\n}\n\nvoid kvm_apic_nmi_wd_deliver(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (apic)\n\t\tkvm_apic_local_deliver(apic, APIC_LVT0);\n}\n\nstatic const struct kvm_io_device_ops apic_mmio_ops = {\n\t.read     = apic_mmio_read,\n\t.write    = apic_mmio_write,\n};\n\nstatic enum hrtimer_restart apic_timer_fn(struct hrtimer *data)\n{\n\tstruct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);\n\tstruct kvm_lapic *apic = container_of(ktimer, struct kvm_lapic, lapic_timer);\n\n\tapic_timer_expired(apic, true);\n\n\tif (lapic_is_periodic(apic)) {\n\t\tadvance_periodic_target_expiration(apic);\n\t\thrtimer_add_expires_ns(&ktimer->timer, ktimer->period);\n\t\treturn HRTIMER_RESTART;\n\t} else\n\t\treturn HRTIMER_NORESTART;\n}\n\nint kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)\n{\n\tstruct kvm_lapic *apic;\n\n\tASSERT(vcpu != NULL);\n\n\tapic = kzalloc(sizeof(*apic), GFP_KERNEL_ACCOUNT);\n\tif (!apic)\n\t\tgoto nomem;\n\n\tvcpu->arch.apic = apic;\n\n\tapic->regs = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);\n\tif (!apic->regs) {\n\t\tprintk(KERN_ERR \"malloc apic regs error for vcpu %x\\n\",\n\t\t       vcpu->vcpu_id);\n\t\tgoto nomem_free_apic;\n\t}\n\tapic->vcpu = vcpu;\n\n\thrtimer_init(&apic->lapic_timer.timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_ABS_HARD);\n\tapic->lapic_timer.timer.function = apic_timer_fn;\n\tif (timer_advance_ns == -1) {\n\t\tapic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;\n\t\tlapic_timer_advance_dynamic = true;\n\t} else {\n\t\tapic->lapic_timer.timer_advance_ns = timer_advance_ns;\n\t\tlapic_timer_advance_dynamic = false;\n\t}\n\n\t/*\n\t * Stuff the APIC ENABLE bit in lieu of temporarily incrementing\n\t * apic_hw_disabled; the full RESET value is set by kvm_lapic_reset().\n\t */\n\tvcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;\n\tstatic_branch_inc(&apic_sw_disabled.key); /* sw disabled at reset */\n\tkvm_iodevice_init(&apic->dev, &apic_mmio_ops);\n\n\treturn 0;\nnomem_free_apic:\n\tkfree(apic);\n\tvcpu->arch.apic = NULL;\nnomem:\n\treturn -ENOMEM;\n}\n\nint kvm_apic_has_interrupt(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 ppr;\n\n\tif (!kvm_apic_present(vcpu))\n\t\treturn -1;\n\n\t__apic_update_ppr(apic, &ppr);\n\treturn apic_has_interrupt_for_ppr(apic, ppr);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_has_interrupt);\n\nint kvm_apic_accept_pic_intr(struct kvm_vcpu *vcpu)\n{\n\tu32 lvt0 = kvm_lapic_get_reg(vcpu->arch.apic, APIC_LVT0);\n\n\tif (!kvm_apic_hw_enabled(vcpu->arch.apic))\n\t\treturn 1;\n\tif ((lvt0 & APIC_LVT_MASKED) == 0 &&\n\t    GET_APIC_DELIVERY_MODE(lvt0) == APIC_MODE_EXTINT)\n\t\treturn 1;\n\treturn 0;\n}\n\nvoid kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (atomic_read(&apic->lapic_timer.pending) > 0) {\n\t\tkvm_apic_inject_pending_timer_irqs(apic);\n\t\tatomic_set(&apic->lapic_timer.pending, 0);\n\t}\n}\n\nint kvm_get_apic_interrupt(struct kvm_vcpu *vcpu)\n{\n\tint vector = kvm_apic_has_interrupt(vcpu);\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 ppr;\n\n\tif (vector == -1)\n\t\treturn -1;\n\n\t/*\n\t * We get here even with APIC virtualization enabled, if doing\n\t * nested virtualization and L1 runs with the \"acknowledge interrupt\n\t * on exit\" mode.  Then we cannot inject the interrupt via RVI,\n\t * because the process would deliver it through the IDT.\n\t */\n\n\tapic_clear_irr(vector, apic);\n\tif (to_hv_vcpu(vcpu) && test_bit(vector, to_hv_synic(vcpu)->auto_eoi_bitmap)) {\n\t\t/*\n\t\t * For auto-EOI interrupts, there might be another pending\n\t\t * interrupt above PPR, so check whether to raise another\n\t\t * KVM_REQ_EVENT.\n\t\t */\n\t\tapic_update_ppr(apic);\n\t} else {\n\t\t/*\n\t\t * For normal interrupts, PPR has been raised and there cannot\n\t\t * be a higher-priority pending interrupt---except if there was\n\t\t * a concurrent interrupt injection, but that would have\n\t\t * triggered KVM_REQ_EVENT already.\n\t\t */\n\t\tapic_set_isr(vector, apic);\n\t\t__apic_update_ppr(apic, &ppr);\n\t}\n\n\treturn vector;\n}\n\nstatic int kvm_apic_state_fixup(struct kvm_vcpu *vcpu,\n\t\tstruct kvm_lapic_state *s, bool set)\n{\n\tif (apic_x2apic_mode(vcpu->arch.apic)) {\n\t\tu32 *id = (u32 *)(s->regs + APIC_ID);\n\t\tu32 *ldr = (u32 *)(s->regs + APIC_LDR);\n\t\tu64 icr;\n\n\t\tif (vcpu->kvm->arch.x2apic_format) {\n\t\t\tif (*id != vcpu->vcpu_id)\n\t\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\tif (set)\n\t\t\t\t*id >>= 24;\n\t\t\telse\n\t\t\t\t*id <<= 24;\n\t\t}\n\n\t\t/*\n\t\t * In x2APIC mode, the LDR is fixed and based on the id.  And\n\t\t * ICR is internally a single 64-bit register, but needs to be\n\t\t * split to ICR+ICR2 in userspace for backwards compatibility.\n\t\t */\n\t\tif (set) {\n\t\t\t*ldr = kvm_apic_calc_x2apic_ldr(*id);\n\n\t\t\ticr = __kvm_lapic_get_reg(s->regs, APIC_ICR) |\n\t\t\t      (u64)__kvm_lapic_get_reg(s->regs, APIC_ICR2) << 32;\n\t\t\t__kvm_lapic_set_reg64(s->regs, APIC_ICR, icr);\n\t\t} else {\n\t\t\ticr = __kvm_lapic_get_reg64(s->regs, APIC_ICR);\n\t\t\t__kvm_lapic_set_reg(s->regs, APIC_ICR2, icr >> 32);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint kvm_apic_get_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)\n{\n\tmemcpy(s->regs, vcpu->arch.apic->regs, sizeof(*s));\n\n\t/*\n\t * Get calculated timer current count for remaining timer period (if\n\t * any) and store it in the returned register set.\n\t */\n\t__kvm_lapic_set_reg(s->regs, APIC_TMCCT,\n\t\t\t    __apic_read(vcpu->arch.apic, APIC_TMCCT));\n\n\treturn kvm_apic_state_fixup(vcpu, s, false);\n}\n\nint kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tint r;\n\n\tkvm_lapic_set_base(vcpu, vcpu->arch.apic_base);\n\t/* set SPIV separately to get count of SW disabled APICs right */\n\tapic_set_spiv(apic, *((u32 *)(s->regs + APIC_SPIV)));\n\n\tr = kvm_apic_state_fixup(vcpu, s, true);\n\tif (r) {\n\t\tkvm_recalculate_apic_map(vcpu->kvm);\n\t\treturn r;\n\t}\n\tmemcpy(vcpu->arch.apic->regs, s->regs, sizeof(*s));\n\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n\tkvm_recalculate_apic_map(vcpu->kvm);\n\tkvm_apic_set_version(vcpu);\n\n\tapic_update_ppr(apic);\n\tcancel_apic_timer(apic);\n\tapic->lapic_timer.expired_tscdeadline = 0;\n\tapic_update_lvtt(apic);\n\tapic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));\n\tupdate_divide_count(apic);\n\t__start_apic_timer(apic, APIC_TMCCT);\n\tkvm_lapic_set_reg(apic, APIC_TMCCT, 0);\n\tkvm_apic_update_apicv(vcpu);\n\tapic->highest_isr_cache = -1;\n\tif (vcpu->arch.apicv_active) {\n\t\tstatic_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);\n\t\tstatic_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));\n\t}\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\tif (ioapic_in_kernel(vcpu->kvm))\n\t\tkvm_rtc_eoi_tracking_restore_one(vcpu);\n\n\tvcpu->arch.apic_arb_prio = 0;\n\n\treturn 0;\n}\n\nvoid __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct hrtimer *timer;\n\n\tif (!lapic_in_kernel(vcpu) ||\n\t\tkvm_can_post_timer_interrupt(vcpu))\n\t\treturn;\n\n\ttimer = &vcpu->arch.apic->lapic_timer.timer;\n\tif (hrtimer_cancel(timer))\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS_HARD);\n}\n\n/*\n * apic_sync_pv_eoi_from_guest - called on vmexit or cancel interrupt\n *\n * Detect whether guest triggered PV EOI since the\n * last entry. If yes, set EOI on guests's behalf.\n * Clear PV EOI in guest memory in any case.\n */\nstatic void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_lapic *apic)\n{\n\tint vector;\n\t/*\n\t * PV EOI state is derived from KVM_APIC_PV_EOI_PENDING in host\n\t * and KVM_PV_EOI_ENABLED in guest memory as follows:\n\t *\n\t * KVM_APIC_PV_EOI_PENDING is unset:\n\t * \t-> host disabled PV EOI.\n\t * KVM_APIC_PV_EOI_PENDING is set, KVM_PV_EOI_ENABLED is set:\n\t * \t-> host enabled PV EOI, guest did not execute EOI yet.\n\t * KVM_APIC_PV_EOI_PENDING is set, KVM_PV_EOI_ENABLED is unset:\n\t * \t-> host enabled PV EOI, guest executed EOI.\n\t */\n\tBUG_ON(!pv_eoi_enabled(vcpu));\n\n\tif (pv_eoi_test_and_clr_pending(vcpu))\n\t\treturn;\n\tvector = apic_set_eoi(apic);\n\ttrace_kvm_pv_eoi(apic, vector);\n}\n\nvoid kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data;\n\n\tif (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))\n\t\tapic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\tif (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n\t\t\t\t  sizeof(u32)))\n\t\treturn;\n\n\tapic_set_tpr(vcpu->arch.apic, data & 0xff);\n}\n\n/*\n * apic_sync_pv_eoi_to_guest - called before vmentry\n *\n * Detect whether it's safe to enable PV EOI and\n * if yes do so.\n */\nstatic void apic_sync_pv_eoi_to_guest(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_lapic *apic)\n{\n\tif (!pv_eoi_enabled(vcpu) ||\n\t    /* IRR set or many bits in ISR: could be nested. */\n\t    apic->irr_pending ||\n\t    /* Cache not set: could be safe but we don't bother. */\n\t    apic->highest_isr_cache == -1 ||\n\t    /* Need EOI to update ioapic. */\n\t    kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {\n\t\t/*\n\t\t * PV EOI was disabled by apic_sync_pv_eoi_from_guest\n\t\t * so we need not do anything here.\n\t\t */\n\t\treturn;\n\t}\n\n\tpv_eoi_set_pending(apic->vcpu);\n}\n\nvoid kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data, tpr;\n\tint max_irr, max_isr;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tapic_sync_pv_eoi_to_guest(vcpu, apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\ttpr = kvm_lapic_get_reg(apic, APIC_TASKPRI) & 0xff;\n\tmax_irr = apic_find_highest_irr(apic);\n\tif (max_irr < 0)\n\t\tmax_irr = 0;\n\tmax_isr = apic_find_highest_isr(apic);\n\tif (max_isr < 0)\n\t\tmax_isr = 0;\n\tdata = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);\n\n\tkvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n\t\t\t\tsizeof(u32));\n}\n\nint kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)\n{\n\tif (vapic_addr) {\n\t\tif (kvm_gfn_to_hva_cache_init(vcpu->kvm,\n\t\t\t\t\t&vcpu->arch.apic->vapic_cache,\n\t\t\t\t\tvapic_addr, sizeof(u32)))\n\t\t\treturn -EINVAL;\n\t\t__set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);\n\t} else {\n\t\t__clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);\n\t}\n\n\tvcpu->arch.apic->vapic_addr = vapic_addr;\n\treturn 0;\n}\n\nint kvm_x2apic_icr_write(struct kvm_lapic *apic, u64 data)\n{\n\tdata &= ~APIC_ICR_BUSY;\n\n\tkvm_apic_send_ipi(apic, (u32)data, (u32)(data >> 32));\n\tkvm_lapic_set_reg64(apic, APIC_ICR, data);\n\ttrace_kvm_apic_write(APIC_ICR, data);\n\treturn 0;\n}\n\nstatic int kvm_lapic_msr_read(struct kvm_lapic *apic, u32 reg, u64 *data)\n{\n\tu32 low;\n\n\tif (reg == APIC_ICR) {\n\t\t*data = kvm_lapic_get_reg64(apic, APIC_ICR);\n\t\treturn 0;\n\t}\n\n\tif (kvm_lapic_reg_read(apic, reg, 4, &low))\n\t\treturn 1;\n\n\t*data = low;\n\n\treturn 0;\n}\n\nstatic int kvm_lapic_msr_write(struct kvm_lapic *apic, u32 reg, u64 data)\n{\n\t/*\n\t * ICR is a 64-bit register in x2APIC mode (and Hyper'v PV vAPIC) and\n\t * can be written as such, all other registers remain accessible only\n\t * through 32-bit reads/writes.\n\t */\n\tif (reg == APIC_ICR)\n\t\treturn kvm_x2apic_icr_write(apic, data);\n\n\treturn kvm_lapic_reg_write(apic, reg, (u32)data);\n}\n\nint kvm_x2apic_msr_write(struct kvm_vcpu *vcpu, u32 msr, u64 data)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 reg = (msr - APIC_BASE_MSR) << 4;\n\n\tif (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(apic))\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_write(apic, reg, data);\n}\n\nint kvm_x2apic_msr_read(struct kvm_vcpu *vcpu, u32 msr, u64 *data)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 reg = (msr - APIC_BASE_MSR) << 4;\n\n\tif (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(apic))\n\t\treturn 1;\n\n\tif (reg == APIC_DFR)\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_read(apic, reg, data);\n}\n\nint kvm_hv_vapic_msr_write(struct kvm_vcpu *vcpu, u32 reg, u64 data)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_write(vcpu->arch.apic, reg, data);\n}\n\nint kvm_hv_vapic_msr_read(struct kvm_vcpu *vcpu, u32 reg, u64 *data)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_read(vcpu->arch.apic, reg, data);\n}\n\nint kvm_lapic_set_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)\n{\n\tu64 addr = data & ~KVM_MSR_ENABLED;\n\tstruct gfn_to_hva_cache *ghc = &vcpu->arch.pv_eoi.data;\n\tunsigned long new_len;\n\tint ret;\n\n\tif (!IS_ALIGNED(addr, 4))\n\t\treturn 1;\n\n\tif (data & KVM_MSR_ENABLED) {\n\t\tif (addr == ghc->gpa && len <= ghc->len)\n\t\t\tnew_len = ghc->len;\n\t\telse\n\t\t\tnew_len = len;\n\n\t\tret = kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, addr, new_len);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tvcpu->arch.pv_eoi.msr_val = data;\n\n\treturn 0;\n}\n\nint kvm_apic_accept_events(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu8 sipi_vector;\n\tint r;\n\tunsigned long pe;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 0;\n\n\t/*\n\t * Read pending events before calling the check_events\n\t * callback.\n\t */\n\tpe = smp_load_acquire(&apic->pending_events);\n\tif (!pe)\n\t\treturn 0;\n\n\tif (is_guest_mode(vcpu)) {\n\t\tr = kvm_check_nested_events(vcpu);\n\t\tif (r < 0)\n\t\t\treturn r == -EBUSY ? 0 : r;\n\t\t/*\n\t\t * If an event has happened and caused a vmexit,\n\t\t * we know INITs are latched and therefore\n\t\t * we will not incorrectly deliver an APIC\n\t\t * event instead of a vmexit.\n\t\t */\n\t}\n\n\t/*\n\t * INITs are latched while CPU is in specific states\n\t * (SMM, VMX root mode, SVM with GIF=0).\n\t * Because a CPU cannot be in these states immediately\n\t * after it has processed an INIT signal (and thus in\n\t * KVM_MP_STATE_INIT_RECEIVED state), just eat SIPIs\n\t * and leave the INIT pending.\n\t */\n\tif (kvm_vcpu_latch_init(vcpu)) {\n\t\tWARN_ON_ONCE(vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED);\n\t\tif (test_bit(KVM_APIC_SIPI, &pe))\n\t\t\tclear_bit(KVM_APIC_SIPI, &apic->pending_events);\n\t\treturn 0;\n\t}\n\n\tif (test_bit(KVM_APIC_INIT, &pe)) {\n\t\tclear_bit(KVM_APIC_INIT, &apic->pending_events);\n\t\tkvm_vcpu_reset(vcpu, true);\n\t\tif (kvm_vcpu_is_bsp(apic->vcpu))\n\t\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\t\telse\n\t\t\tvcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;\n\t}\n\tif (test_bit(KVM_APIC_SIPI, &pe)) {\n\t\tclear_bit(KVM_APIC_SIPI, &apic->pending_events);\n\t\tif (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {\n\t\t\t/* evaluate pending_events before reading the vector */\n\t\t\tsmp_rmb();\n\t\t\tsipi_vector = apic->sipi_vector;\n\t\t\tstatic_call(kvm_x86_vcpu_deliver_sipi_vector)(vcpu, sipi_vector);\n\t\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\t\t}\n\t}\n\treturn 0;\n}\n\nvoid kvm_lapic_exit(void)\n{\n\tstatic_key_deferred_flush(&apic_hw_disabled);\n\tWARN_ON(static_branch_unlikely(&apic_hw_disabled.key));\n\tstatic_key_deferred_flush(&apic_sw_disabled);\n\tWARN_ON(static_branch_unlikely(&apic_sw_disabled.key));\n}\n"], "filenames": ["arch/x86/kvm/lapic.c"], "buggy_code_start_loc": [1026], "buggy_code_end_loc": [1026], "fixing_code_start_loc": [1027], "fixing_code_end_loc": [1031], "type": "CWE-476", "message": "A flaw was found in the Linux kernel\u2019s KVM when attempting to set a SynIC IRQ. This issue makes it possible for a misbehaving VMM to write to SYNIC/STIMER MSRs, causing a NULL pointer dereference. This flaw allows an unprivileged local attacker on the host to issue specific ioctl calls, causing a kernel oops condition that results in a denial of service.", "other": {"cve": {"id": "CVE-2022-2153", "sourceIdentifier": "secalert@redhat.com", "published": "2022-08-31T16:15:10.837", "lastModified": "2022-11-21T19:45:58.110", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A flaw was found in the Linux kernel\u2019s KVM when attempting to set a SynIC IRQ. This issue makes it possible for a misbehaving VMM to write to SYNIC/STIMER MSRs, causing a NULL pointer dereference. This flaw allows an unprivileged local attacker on the host to issue specific ioctl calls, causing a kernel oops condition that results in a denial of service."}, {"lang": "es", "value": "Se ha encontrado un fallo en el KVM del kernel de Linux cuando es intentado establecer una IRQ SynIC. Este problema hace posible a un VMM que sea comportado inapropiadamente escribir en las MSR de SYNIC/STIMER, causando una desreferencia de puntero NULL. Este fallo permite a un atacante local no privilegiado en el host emitir llamadas ioctl espec\u00edficas, causando una condici\u00f3n de oops en el kernel que resulta en una denegaci\u00f3n de servicio"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}]}, "weaknesses": [{"source": "secalert@redhat.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}, {"source": "nvd@nist.gov", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.18", "matchCriteriaId": "FE93544F-B946-47CF-9697-FBF3484FCB92"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:36:*:*:*:*:*:*:*", "matchCriteriaId": "5C675112-476C-4D7C-BCB9-A2FB2D0BC9FD"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:6.0:*:*:*:*:*:*:*", "matchCriteriaId": "2F6AB192-9D7D-4A9A-8995-E53A9DE9EAFC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "142AD0DD-4CF3-4D74-9442-459CE3347E3A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "7F6FB57C-2BC7-487C-96DD-132683AEB35D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}], "references": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2069736", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/00b5f37189d24ac3ed46cb7f11742094778c46ce", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/7ec37d1cbe17d8189d9562178d8b29167fe1c31a", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/b1e34d325397a33d97d845e312d7cf2a8b646b44", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/10/msg00000.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/11/msg00001.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://www.openwall.com/lists/oss-security/2022/06/22/1", "source": "secalert@redhat.com", "tags": ["Exploit", "Mailing List", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/00b5f37189d24ac3ed46cb7f11742094778c46ce"}}