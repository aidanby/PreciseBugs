{"buggy_code": ["/*\n * Copyright (c) 2006 - 2009 Mellanox Technology Inc.  All rights reserved.\n * Copyright (C) 2008 - 2011 Bart Van Assche <bvanassche@acm.org>.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/kthread.h>\n#include <linux/string.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <scsi/scsi_proto.h>\n#include <scsi/scsi_tcq.h>\n#include <target/target_core_base.h>\n#include <target/target_core_fabric.h>\n#include \"ib_srpt.h\"\n\n/* Name of this kernel module. */\n#define DRV_NAME\t\t\"ib_srpt\"\n#define DRV_VERSION\t\t\"2.0.0\"\n#define DRV_RELDATE\t\t\"2011-02-14\"\n\n#define SRPT_ID_STRING\t\"Linux SRP target\"\n\n#undef pr_fmt\n#define pr_fmt(fmt) DRV_NAME \" \" fmt\n\nMODULE_AUTHOR(\"Vu Pham and Bart Van Assche\");\nMODULE_DESCRIPTION(\"InfiniBand SCSI RDMA Protocol target \"\n\t\t   \"v\" DRV_VERSION \" (\" DRV_RELDATE \")\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\n/*\n * Global Variables\n */\n\nstatic u64 srpt_service_guid;\nstatic DEFINE_SPINLOCK(srpt_dev_lock);\t/* Protects srpt_dev_list. */\nstatic LIST_HEAD(srpt_dev_list);\t/* List of srpt_device structures. */\n\nstatic unsigned srp_max_req_size = DEFAULT_MAX_REQ_SIZE;\nmodule_param(srp_max_req_size, int, 0444);\nMODULE_PARM_DESC(srp_max_req_size,\n\t\t \"Maximum size of SRP request messages in bytes.\");\n\nstatic int srpt_srq_size = DEFAULT_SRPT_SRQ_SIZE;\nmodule_param(srpt_srq_size, int, 0444);\nMODULE_PARM_DESC(srpt_srq_size,\n\t\t \"Shared receive queue (SRQ) size.\");\n\nstatic int srpt_get_u64_x(char *buffer, struct kernel_param *kp)\n{\n\treturn sprintf(buffer, \"0x%016llx\", *(u64 *)kp->arg);\n}\nmodule_param_call(srpt_service_guid, NULL, srpt_get_u64_x, &srpt_service_guid,\n\t\t  0444);\nMODULE_PARM_DESC(srpt_service_guid,\n\t\t \"Using this value for ioc_guid, id_ext, and cm_listen_id\"\n\t\t \" instead of using the node_guid of the first HCA.\");\n\nstatic struct ib_client srpt_client;\nstatic void srpt_release_channel(struct srpt_rdma_ch *ch);\nstatic int srpt_queue_status(struct se_cmd *cmd);\nstatic void srpt_recv_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void srpt_send_done(struct ib_cq *cq, struct ib_wc *wc);\n\n/**\n * opposite_dma_dir() - Swap DMA_TO_DEVICE and DMA_FROM_DEVICE.\n */\nstatic inline\nenum dma_data_direction opposite_dma_dir(enum dma_data_direction dir)\n{\n\tswitch (dir) {\n\tcase DMA_TO_DEVICE:\treturn DMA_FROM_DEVICE;\n\tcase DMA_FROM_DEVICE:\treturn DMA_TO_DEVICE;\n\tdefault:\t\treturn dir;\n\t}\n}\n\n/**\n * srpt_sdev_name() - Return the name associated with the HCA.\n *\n * Examples are ib0, ib1, ...\n */\nstatic inline const char *srpt_sdev_name(struct srpt_device *sdev)\n{\n\treturn sdev->device->name;\n}\n\nstatic enum rdma_ch_state srpt_get_ch_state(struct srpt_rdma_ch *ch)\n{\n\tunsigned long flags;\n\tenum rdma_ch_state state;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tstate = ch->state;\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\treturn state;\n}\n\nstatic enum rdma_ch_state\nsrpt_set_ch_state(struct srpt_rdma_ch *ch, enum rdma_ch_state new_state)\n{\n\tunsigned long flags;\n\tenum rdma_ch_state prev;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tprev = ch->state;\n\tch->state = new_state;\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\treturn prev;\n}\n\n/**\n * srpt_test_and_set_ch_state() - Test and set the channel state.\n *\n * Returns true if and only if the channel state has been set to the new state.\n */\nstatic bool\nsrpt_test_and_set_ch_state(struct srpt_rdma_ch *ch, enum rdma_ch_state old,\n\t\t\t   enum rdma_ch_state new)\n{\n\tunsigned long flags;\n\tenum rdma_ch_state prev;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tprev = ch->state;\n\tif (prev == old)\n\t\tch->state = new;\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\treturn prev == old;\n}\n\n/**\n * srpt_event_handler() - Asynchronous IB event callback function.\n *\n * Callback function called by the InfiniBand core when an asynchronous IB\n * event occurs. This callback may occur in interrupt context. See also\n * section 11.5.2, Set Asynchronous Event Handler in the InfiniBand\n * Architecture Specification.\n */\nstatic void srpt_event_handler(struct ib_event_handler *handler,\n\t\t\t       struct ib_event *event)\n{\n\tstruct srpt_device *sdev;\n\tstruct srpt_port *sport;\n\n\tsdev = ib_get_client_data(event->device, &srpt_client);\n\tif (!sdev || sdev->device != event->device)\n\t\treturn;\n\n\tpr_debug(\"ASYNC event= %d on device= %s\\n\", event->event,\n\t\t srpt_sdev_name(sdev));\n\n\tswitch (event->event) {\n\tcase IB_EVENT_PORT_ERR:\n\t\tif (event->element.port_num <= sdev->device->phys_port_cnt) {\n\t\t\tsport = &sdev->port[event->element.port_num - 1];\n\t\t\tsport->lid = 0;\n\t\t\tsport->sm_lid = 0;\n\t\t}\n\t\tbreak;\n\tcase IB_EVENT_PORT_ACTIVE:\n\tcase IB_EVENT_LID_CHANGE:\n\tcase IB_EVENT_PKEY_CHANGE:\n\tcase IB_EVENT_SM_CHANGE:\n\tcase IB_EVENT_CLIENT_REREGISTER:\n\tcase IB_EVENT_GID_CHANGE:\n\t\t/* Refresh port data asynchronously. */\n\t\tif (event->element.port_num <= sdev->device->phys_port_cnt) {\n\t\t\tsport = &sdev->port[event->element.port_num - 1];\n\t\t\tif (!sport->lid && !sport->sm_lid)\n\t\t\t\tschedule_work(&sport->work);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized IB event %d\\n\",\n\t\t       event->event);\n\t\tbreak;\n\t}\n}\n\n/**\n * srpt_srq_event() - SRQ event callback function.\n */\nstatic void srpt_srq_event(struct ib_event *event, void *ctx)\n{\n\tpr_info(\"SRQ event %d\\n\", event->event);\n}\n\n/**\n * srpt_qp_event() - QP event callback function.\n */\nstatic void srpt_qp_event(struct ib_event *event, struct srpt_rdma_ch *ch)\n{\n\tpr_debug(\"QP event %d on cm_id=%p sess_name=%s state=%d\\n\",\n\t\t event->event, ch->cm_id, ch->sess_name, srpt_get_ch_state(ch));\n\n\tswitch (event->event) {\n\tcase IB_EVENT_COMM_EST:\n\t\tib_cm_notify(ch->cm_id, event->event);\n\t\tbreak;\n\tcase IB_EVENT_QP_LAST_WQE_REACHED:\n\t\tif (srpt_test_and_set_ch_state(ch, CH_DRAINING,\n\t\t\t\t\t       CH_RELEASING))\n\t\t\tsrpt_release_channel(ch);\n\t\telse\n\t\t\tpr_debug(\"%s: state %d - ignored LAST_WQE.\\n\",\n\t\t\t\t ch->sess_name, srpt_get_ch_state(ch));\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized IB QP event %d\\n\", event->event);\n\t\tbreak;\n\t}\n}\n\n/**\n * srpt_set_ioc() - Helper function for initializing an IOUnitInfo structure.\n *\n * @slot: one-based slot number.\n * @value: four-bit value.\n *\n * Copies the lowest four bits of value in element slot of the array of four\n * bit elements called c_list (controller list). The index slot is one-based.\n */\nstatic void srpt_set_ioc(u8 *c_list, u32 slot, u8 value)\n{\n\tu16 id;\n\tu8 tmp;\n\n\tid = (slot - 1) / 2;\n\tif (slot & 0x1) {\n\t\ttmp = c_list[id] & 0xf;\n\t\tc_list[id] = (value << 4) | tmp;\n\t} else {\n\t\ttmp = c_list[id] & 0xf0;\n\t\tc_list[id] = (value & 0xf) | tmp;\n\t}\n}\n\n/**\n * srpt_get_class_port_info() - Copy ClassPortInfo to a management datagram.\n *\n * See also section 16.3.3.1 ClassPortInfo in the InfiniBand Architecture\n * Specification.\n */\nstatic void srpt_get_class_port_info(struct ib_dm_mad *mad)\n{\n\tstruct ib_class_port_info *cif;\n\n\tcif = (struct ib_class_port_info *)mad->data;\n\tmemset(cif, 0, sizeof *cif);\n\tcif->base_version = 1;\n\tcif->class_version = 1;\n\tcif->resp_time_value = 20;\n\n\tmad->mad_hdr.status = 0;\n}\n\n/**\n * srpt_get_iou() - Write IOUnitInfo to a management datagram.\n *\n * See also section 16.3.3.3 IOUnitInfo in the InfiniBand Architecture\n * Specification. See also section B.7, table B.6 in the SRP r16a document.\n */\nstatic void srpt_get_iou(struct ib_dm_mad *mad)\n{\n\tstruct ib_dm_iou_info *ioui;\n\tu8 slot;\n\tint i;\n\n\tioui = (struct ib_dm_iou_info *)mad->data;\n\tioui->change_id = cpu_to_be16(1);\n\tioui->max_controllers = 16;\n\n\t/* set present for slot 1 and empty for the rest */\n\tsrpt_set_ioc(ioui->controller_list, 1, 1);\n\tfor (i = 1, slot = 2; i < 16; i++, slot++)\n\t\tsrpt_set_ioc(ioui->controller_list, slot, 0);\n\n\tmad->mad_hdr.status = 0;\n}\n\n/**\n * srpt_get_ioc() - Write IOControllerprofile to a management datagram.\n *\n * See also section 16.3.3.4 IOControllerProfile in the InfiniBand\n * Architecture Specification. See also section B.7, table B.7 in the SRP\n * r16a document.\n */\nstatic void srpt_get_ioc(struct srpt_port *sport, u32 slot,\n\t\t\t struct ib_dm_mad *mad)\n{\n\tstruct srpt_device *sdev = sport->sdev;\n\tstruct ib_dm_ioc_profile *iocp;\n\n\tiocp = (struct ib_dm_ioc_profile *)mad->data;\n\n\tif (!slot || slot > 16) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_INVALID_FIELD);\n\t\treturn;\n\t}\n\n\tif (slot > 2) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_NO_IOC);\n\t\treturn;\n\t}\n\n\tmemset(iocp, 0, sizeof *iocp);\n\tstrcpy(iocp->id_string, SRPT_ID_STRING);\n\tiocp->guid = cpu_to_be64(srpt_service_guid);\n\tiocp->vendor_id = cpu_to_be32(sdev->device->attrs.vendor_id);\n\tiocp->device_id = cpu_to_be32(sdev->device->attrs.vendor_part_id);\n\tiocp->device_version = cpu_to_be16(sdev->device->attrs.hw_ver);\n\tiocp->subsys_vendor_id = cpu_to_be32(sdev->device->attrs.vendor_id);\n\tiocp->subsys_device_id = 0x0;\n\tiocp->io_class = cpu_to_be16(SRP_REV16A_IB_IO_CLASS);\n\tiocp->io_subclass = cpu_to_be16(SRP_IO_SUBCLASS);\n\tiocp->protocol = cpu_to_be16(SRP_PROTOCOL);\n\tiocp->protocol_version = cpu_to_be16(SRP_PROTOCOL_VERSION);\n\tiocp->send_queue_depth = cpu_to_be16(sdev->srq_size);\n\tiocp->rdma_read_depth = 4;\n\tiocp->send_size = cpu_to_be32(srp_max_req_size);\n\tiocp->rdma_size = cpu_to_be32(min(sport->port_attrib.srp_max_rdma_size,\n\t\t\t\t\t  1U << 24));\n\tiocp->num_svc_entries = 1;\n\tiocp->op_cap_mask = SRP_SEND_TO_IOC | SRP_SEND_FROM_IOC |\n\t\tSRP_RDMA_READ_FROM_IOC | SRP_RDMA_WRITE_FROM_IOC;\n\n\tmad->mad_hdr.status = 0;\n}\n\n/**\n * srpt_get_svc_entries() - Write ServiceEntries to a management datagram.\n *\n * See also section 16.3.3.5 ServiceEntries in the InfiniBand Architecture\n * Specification. See also section B.7, table B.8 in the SRP r16a document.\n */\nstatic void srpt_get_svc_entries(u64 ioc_guid,\n\t\t\t\t u16 slot, u8 hi, u8 lo, struct ib_dm_mad *mad)\n{\n\tstruct ib_dm_svc_entries *svc_entries;\n\n\tWARN_ON(!ioc_guid);\n\n\tif (!slot || slot > 16) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_INVALID_FIELD);\n\t\treturn;\n\t}\n\n\tif (slot > 2 || lo > hi || hi > 1) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_NO_IOC);\n\t\treturn;\n\t}\n\n\tsvc_entries = (struct ib_dm_svc_entries *)mad->data;\n\tmemset(svc_entries, 0, sizeof *svc_entries);\n\tsvc_entries->service_entries[0].id = cpu_to_be64(ioc_guid);\n\tsnprintf(svc_entries->service_entries[0].name,\n\t\t sizeof(svc_entries->service_entries[0].name),\n\t\t \"%s%016llx\",\n\t\t SRP_SERVICE_NAME_PREFIX,\n\t\t ioc_guid);\n\n\tmad->mad_hdr.status = 0;\n}\n\n/**\n * srpt_mgmt_method_get() - Process a received management datagram.\n * @sp:      source port through which the MAD has been received.\n * @rq_mad:  received MAD.\n * @rsp_mad: response MAD.\n */\nstatic void srpt_mgmt_method_get(struct srpt_port *sp, struct ib_mad *rq_mad,\n\t\t\t\t struct ib_dm_mad *rsp_mad)\n{\n\tu16 attr_id;\n\tu32 slot;\n\tu8 hi, lo;\n\n\tattr_id = be16_to_cpu(rq_mad->mad_hdr.attr_id);\n\tswitch (attr_id) {\n\tcase DM_ATTR_CLASS_PORT_INFO:\n\t\tsrpt_get_class_port_info(rsp_mad);\n\t\tbreak;\n\tcase DM_ATTR_IOU_INFO:\n\t\tsrpt_get_iou(rsp_mad);\n\t\tbreak;\n\tcase DM_ATTR_IOC_PROFILE:\n\t\tslot = be32_to_cpu(rq_mad->mad_hdr.attr_mod);\n\t\tsrpt_get_ioc(sp, slot, rsp_mad);\n\t\tbreak;\n\tcase DM_ATTR_SVC_ENTRIES:\n\t\tslot = be32_to_cpu(rq_mad->mad_hdr.attr_mod);\n\t\thi = (u8) ((slot >> 8) & 0xff);\n\t\tlo = (u8) (slot & 0xff);\n\t\tslot = (u16) ((slot >> 16) & 0xffff);\n\t\tsrpt_get_svc_entries(srpt_service_guid,\n\t\t\t\t     slot, hi, lo, rsp_mad);\n\t\tbreak;\n\tdefault:\n\t\trsp_mad->mad_hdr.status =\n\t\t    cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD_ATTR);\n\t\tbreak;\n\t}\n}\n\n/**\n * srpt_mad_send_handler() - Post MAD-send callback function.\n */\nstatic void srpt_mad_send_handler(struct ib_mad_agent *mad_agent,\n\t\t\t\t  struct ib_mad_send_wc *mad_wc)\n{\n\tib_destroy_ah(mad_wc->send_buf->ah);\n\tib_free_send_mad(mad_wc->send_buf);\n}\n\n/**\n * srpt_mad_recv_handler() - MAD reception callback function.\n */\nstatic void srpt_mad_recv_handler(struct ib_mad_agent *mad_agent,\n\t\t\t\t  struct ib_mad_send_buf *send_buf,\n\t\t\t\t  struct ib_mad_recv_wc *mad_wc)\n{\n\tstruct srpt_port *sport = (struct srpt_port *)mad_agent->context;\n\tstruct ib_ah *ah;\n\tstruct ib_mad_send_buf *rsp;\n\tstruct ib_dm_mad *dm_mad;\n\n\tif (!mad_wc || !mad_wc->recv_buf.mad)\n\t\treturn;\n\n\tah = ib_create_ah_from_wc(mad_agent->qp->pd, mad_wc->wc,\n\t\t\t\t  mad_wc->recv_buf.grh, mad_agent->port_num);\n\tif (IS_ERR(ah))\n\t\tgoto err;\n\n\tBUILD_BUG_ON(offsetof(struct ib_dm_mad, data) != IB_MGMT_DEVICE_HDR);\n\n\trsp = ib_create_send_mad(mad_agent, mad_wc->wc->src_qp,\n\t\t\t\t mad_wc->wc->pkey_index, 0,\n\t\t\t\t IB_MGMT_DEVICE_HDR, IB_MGMT_DEVICE_DATA,\n\t\t\t\t GFP_KERNEL,\n\t\t\t\t IB_MGMT_BASE_VERSION);\n\tif (IS_ERR(rsp))\n\t\tgoto err_rsp;\n\n\trsp->ah = ah;\n\n\tdm_mad = rsp->mad;\n\tmemcpy(dm_mad, mad_wc->recv_buf.mad, sizeof *dm_mad);\n\tdm_mad->mad_hdr.method = IB_MGMT_METHOD_GET_RESP;\n\tdm_mad->mad_hdr.status = 0;\n\n\tswitch (mad_wc->recv_buf.mad->mad_hdr.method) {\n\tcase IB_MGMT_METHOD_GET:\n\t\tsrpt_mgmt_method_get(sport, mad_wc->recv_buf.mad, dm_mad);\n\t\tbreak;\n\tcase IB_MGMT_METHOD_SET:\n\t\tdm_mad->mad_hdr.status =\n\t\t    cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD_ATTR);\n\t\tbreak;\n\tdefault:\n\t\tdm_mad->mad_hdr.status =\n\t\t    cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD);\n\t\tbreak;\n\t}\n\n\tif (!ib_post_send_mad(rsp, NULL)) {\n\t\tib_free_recv_mad(mad_wc);\n\t\t/* will destroy_ah & free_send_mad in send completion */\n\t\treturn;\n\t}\n\n\tib_free_send_mad(rsp);\n\nerr_rsp:\n\tib_destroy_ah(ah);\nerr:\n\tib_free_recv_mad(mad_wc);\n}\n\n/**\n * srpt_refresh_port() - Configure a HCA port.\n *\n * Enable InfiniBand management datagram processing, update the cached sm_lid,\n * lid and gid values, and register a callback function for processing MADs\n * on the specified port.\n *\n * Note: It is safe to call this function more than once for the same port.\n */\nstatic int srpt_refresh_port(struct srpt_port *sport)\n{\n\tstruct ib_mad_reg_req reg_req;\n\tstruct ib_port_modify port_modify;\n\tstruct ib_port_attr port_attr;\n\tint ret;\n\n\tmemset(&port_modify, 0, sizeof port_modify);\n\tport_modify.set_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP;\n\tport_modify.clr_port_cap_mask = 0;\n\n\tret = ib_modify_port(sport->sdev->device, sport->port, 0, &port_modify);\n\tif (ret)\n\t\tgoto err_mod_port;\n\n\tret = ib_query_port(sport->sdev->device, sport->port, &port_attr);\n\tif (ret)\n\t\tgoto err_query_port;\n\n\tsport->sm_lid = port_attr.sm_lid;\n\tsport->lid = port_attr.lid;\n\n\tret = ib_query_gid(sport->sdev->device, sport->port, 0, &sport->gid,\n\t\t\t   NULL);\n\tif (ret)\n\t\tgoto err_query_port;\n\n\tif (!sport->mad_agent) {\n\t\tmemset(&reg_req, 0, sizeof reg_req);\n\t\treg_req.mgmt_class = IB_MGMT_CLASS_DEVICE_MGMT;\n\t\treg_req.mgmt_class_version = IB_MGMT_BASE_VERSION;\n\t\tset_bit(IB_MGMT_METHOD_GET, reg_req.method_mask);\n\t\tset_bit(IB_MGMT_METHOD_SET, reg_req.method_mask);\n\n\t\tsport->mad_agent = ib_register_mad_agent(sport->sdev->device,\n\t\t\t\t\t\t\t sport->port,\n\t\t\t\t\t\t\t IB_QPT_GSI,\n\t\t\t\t\t\t\t &reg_req, 0,\n\t\t\t\t\t\t\t srpt_mad_send_handler,\n\t\t\t\t\t\t\t srpt_mad_recv_handler,\n\t\t\t\t\t\t\t sport, 0);\n\t\tif (IS_ERR(sport->mad_agent)) {\n\t\t\tret = PTR_ERR(sport->mad_agent);\n\t\t\tsport->mad_agent = NULL;\n\t\t\tgoto err_query_port;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_query_port:\n\n\tport_modify.set_port_cap_mask = 0;\n\tport_modify.clr_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP;\n\tib_modify_port(sport->sdev->device, sport->port, 0, &port_modify);\n\nerr_mod_port:\n\n\treturn ret;\n}\n\n/**\n * srpt_unregister_mad_agent() - Unregister MAD callback functions.\n *\n * Note: It is safe to call this function more than once for the same device.\n */\nstatic void srpt_unregister_mad_agent(struct srpt_device *sdev)\n{\n\tstruct ib_port_modify port_modify = {\n\t\t.clr_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP,\n\t};\n\tstruct srpt_port *sport;\n\tint i;\n\n\tfor (i = 1; i <= sdev->device->phys_port_cnt; i++) {\n\t\tsport = &sdev->port[i - 1];\n\t\tWARN_ON(sport->port != i);\n\t\tif (ib_modify_port(sdev->device, i, 0, &port_modify) < 0)\n\t\t\tpr_err(\"disabling MAD processing failed.\\n\");\n\t\tif (sport->mad_agent) {\n\t\t\tib_unregister_mad_agent(sport->mad_agent);\n\t\t\tsport->mad_agent = NULL;\n\t\t}\n\t}\n}\n\n/**\n * srpt_alloc_ioctx() - Allocate an SRPT I/O context structure.\n */\nstatic struct srpt_ioctx *srpt_alloc_ioctx(struct srpt_device *sdev,\n\t\t\t\t\t   int ioctx_size, int dma_size,\n\t\t\t\t\t   enum dma_data_direction dir)\n{\n\tstruct srpt_ioctx *ioctx;\n\n\tioctx = kmalloc(ioctx_size, GFP_KERNEL);\n\tif (!ioctx)\n\t\tgoto err;\n\n\tioctx->buf = kmalloc(dma_size, GFP_KERNEL);\n\tif (!ioctx->buf)\n\t\tgoto err_free_ioctx;\n\n\tioctx->dma = ib_dma_map_single(sdev->device, ioctx->buf, dma_size, dir);\n\tif (ib_dma_mapping_error(sdev->device, ioctx->dma))\n\t\tgoto err_free_buf;\n\n\treturn ioctx;\n\nerr_free_buf:\n\tkfree(ioctx->buf);\nerr_free_ioctx:\n\tkfree(ioctx);\nerr:\n\treturn NULL;\n}\n\n/**\n * srpt_free_ioctx() - Free an SRPT I/O context structure.\n */\nstatic void srpt_free_ioctx(struct srpt_device *sdev, struct srpt_ioctx *ioctx,\n\t\t\t    int dma_size, enum dma_data_direction dir)\n{\n\tif (!ioctx)\n\t\treturn;\n\n\tib_dma_unmap_single(sdev->device, ioctx->dma, dma_size, dir);\n\tkfree(ioctx->buf);\n\tkfree(ioctx);\n}\n\n/**\n * srpt_alloc_ioctx_ring() - Allocate a ring of SRPT I/O context structures.\n * @sdev:       Device to allocate the I/O context ring for.\n * @ring_size:  Number of elements in the I/O context ring.\n * @ioctx_size: I/O context size.\n * @dma_size:   DMA buffer size.\n * @dir:        DMA data direction.\n */\nstatic struct srpt_ioctx **srpt_alloc_ioctx_ring(struct srpt_device *sdev,\n\t\t\t\tint ring_size, int ioctx_size,\n\t\t\t\tint dma_size, enum dma_data_direction dir)\n{\n\tstruct srpt_ioctx **ring;\n\tint i;\n\n\tWARN_ON(ioctx_size != sizeof(struct srpt_recv_ioctx)\n\t\t&& ioctx_size != sizeof(struct srpt_send_ioctx));\n\n\tring = kmalloc(ring_size * sizeof(ring[0]), GFP_KERNEL);\n\tif (!ring)\n\t\tgoto out;\n\tfor (i = 0; i < ring_size; ++i) {\n\t\tring[i] = srpt_alloc_ioctx(sdev, ioctx_size, dma_size, dir);\n\t\tif (!ring[i])\n\t\t\tgoto err;\n\t\tring[i]->index = i;\n\t}\n\tgoto out;\n\nerr:\n\twhile (--i >= 0)\n\t\tsrpt_free_ioctx(sdev, ring[i], dma_size, dir);\n\tkfree(ring);\n\tring = NULL;\nout:\n\treturn ring;\n}\n\n/**\n * srpt_free_ioctx_ring() - Free the ring of SRPT I/O context structures.\n */\nstatic void srpt_free_ioctx_ring(struct srpt_ioctx **ioctx_ring,\n\t\t\t\t struct srpt_device *sdev, int ring_size,\n\t\t\t\t int dma_size, enum dma_data_direction dir)\n{\n\tint i;\n\n\tfor (i = 0; i < ring_size; ++i)\n\t\tsrpt_free_ioctx(sdev, ioctx_ring[i], dma_size, dir);\n\tkfree(ioctx_ring);\n}\n\n/**\n * srpt_get_cmd_state() - Get the state of a SCSI command.\n */\nstatic enum srpt_command_state srpt_get_cmd_state(struct srpt_send_ioctx *ioctx)\n{\n\tenum srpt_command_state state;\n\tunsigned long flags;\n\n\tBUG_ON(!ioctx);\n\n\tspin_lock_irqsave(&ioctx->spinlock, flags);\n\tstate = ioctx->state;\n\tspin_unlock_irqrestore(&ioctx->spinlock, flags);\n\treturn state;\n}\n\n/**\n * srpt_set_cmd_state() - Set the state of a SCSI command.\n *\n * Does not modify the state of aborted commands. Returns the previous command\n * state.\n */\nstatic enum srpt_command_state srpt_set_cmd_state(struct srpt_send_ioctx *ioctx,\n\t\t\t\t\t\t  enum srpt_command_state new)\n{\n\tenum srpt_command_state previous;\n\tunsigned long flags;\n\n\tBUG_ON(!ioctx);\n\n\tspin_lock_irqsave(&ioctx->spinlock, flags);\n\tprevious = ioctx->state;\n\tif (previous != SRPT_STATE_DONE)\n\t\tioctx->state = new;\n\tspin_unlock_irqrestore(&ioctx->spinlock, flags);\n\n\treturn previous;\n}\n\n/**\n * srpt_test_and_set_cmd_state() - Test and set the state of a command.\n *\n * Returns true if and only if the previous command state was equal to 'old'.\n */\nstatic bool srpt_test_and_set_cmd_state(struct srpt_send_ioctx *ioctx,\n\t\t\t\t\tenum srpt_command_state old,\n\t\t\t\t\tenum srpt_command_state new)\n{\n\tenum srpt_command_state previous;\n\tunsigned long flags;\n\n\tWARN_ON(!ioctx);\n\tWARN_ON(old == SRPT_STATE_DONE);\n\tWARN_ON(new == SRPT_STATE_NEW);\n\n\tspin_lock_irqsave(&ioctx->spinlock, flags);\n\tprevious = ioctx->state;\n\tif (previous == old)\n\t\tioctx->state = new;\n\tspin_unlock_irqrestore(&ioctx->spinlock, flags);\n\treturn previous == old;\n}\n\n/**\n * srpt_post_recv() - Post an IB receive request.\n */\nstatic int srpt_post_recv(struct srpt_device *sdev,\n\t\t\t  struct srpt_recv_ioctx *ioctx)\n{\n\tstruct ib_sge list;\n\tstruct ib_recv_wr wr, *bad_wr;\n\n\tBUG_ON(!sdev);\n\tlist.addr = ioctx->ioctx.dma;\n\tlist.length = srp_max_req_size;\n\tlist.lkey = sdev->pd->local_dma_lkey;\n\n\tioctx->ioctx.cqe.done = srpt_recv_done;\n\twr.wr_cqe = &ioctx->ioctx.cqe;\n\twr.next = NULL;\n\twr.sg_list = &list;\n\twr.num_sge = 1;\n\n\treturn ib_post_srq_recv(sdev->srq, &wr, &bad_wr);\n}\n\n/**\n * srpt_post_send() - Post an IB send request.\n *\n * Returns zero upon success and a non-zero value upon failure.\n */\nstatic int srpt_post_send(struct srpt_rdma_ch *ch,\n\t\t\t  struct srpt_send_ioctx *ioctx, int len)\n{\n\tstruct ib_sge list;\n\tstruct ib_send_wr wr, *bad_wr;\n\tstruct srpt_device *sdev = ch->sport->sdev;\n\tint ret;\n\n\tatomic_inc(&ch->req_lim);\n\n\tret = -ENOMEM;\n\tif (unlikely(atomic_dec_return(&ch->sq_wr_avail) < 0)) {\n\t\tpr_warn(\"IB send queue full (needed 1)\\n\");\n\t\tgoto out;\n\t}\n\n\tib_dma_sync_single_for_device(sdev->device, ioctx->ioctx.dma, len,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\tlist.addr = ioctx->ioctx.dma;\n\tlist.length = len;\n\tlist.lkey = sdev->pd->local_dma_lkey;\n\n\tioctx->ioctx.cqe.done = srpt_send_done;\n\twr.next = NULL;\n\twr.wr_cqe = &ioctx->ioctx.cqe;\n\twr.sg_list = &list;\n\twr.num_sge = 1;\n\twr.opcode = IB_WR_SEND;\n\twr.send_flags = IB_SEND_SIGNALED;\n\n\tret = ib_post_send(ch->qp, &wr, &bad_wr);\n\nout:\n\tif (ret < 0) {\n\t\tatomic_inc(&ch->sq_wr_avail);\n\t\tatomic_dec(&ch->req_lim);\n\t}\n\treturn ret;\n}\n\n/**\n * srpt_get_desc_tbl() - Parse the data descriptors of an SRP_CMD request.\n * @ioctx: Pointer to the I/O context associated with the request.\n * @srp_cmd: Pointer to the SRP_CMD request data.\n * @dir: Pointer to the variable to which the transfer direction will be\n *   written.\n * @data_len: Pointer to the variable to which the total data length of all\n *   descriptors in the SRP_CMD request will be written.\n *\n * This function initializes ioctx->nrbuf and ioctx->r_bufs.\n *\n * Returns -EINVAL when the SRP_CMD request contains inconsistent descriptors;\n * -ENOMEM when memory allocation fails and zero upon success.\n */\nstatic int srpt_get_desc_tbl(struct srpt_send_ioctx *ioctx,\n\t\t\t     struct srp_cmd *srp_cmd,\n\t\t\t     enum dma_data_direction *dir, u64 *data_len)\n{\n\tstruct srp_indirect_buf *idb;\n\tstruct srp_direct_buf *db;\n\tunsigned add_cdb_offset;\n\tint ret;\n\n\t/*\n\t * The pointer computations below will only be compiled correctly\n\t * if srp_cmd::add_data is declared as s8*, u8*, s8[] or u8[], so check\n\t * whether srp_cmd::add_data has been declared as a byte pointer.\n\t */\n\tBUILD_BUG_ON(!__same_type(srp_cmd->add_data[0], (s8)0)\n\t\t     && !__same_type(srp_cmd->add_data[0], (u8)0));\n\n\tBUG_ON(!dir);\n\tBUG_ON(!data_len);\n\n\tret = 0;\n\t*data_len = 0;\n\n\t/*\n\t * The lower four bits of the buffer format field contain the DATA-IN\n\t * buffer descriptor format, and the highest four bits contain the\n\t * DATA-OUT buffer descriptor format.\n\t */\n\t*dir = DMA_NONE;\n\tif (srp_cmd->buf_fmt & 0xf)\n\t\t/* DATA-IN: transfer data from target to initiator (read). */\n\t\t*dir = DMA_FROM_DEVICE;\n\telse if (srp_cmd->buf_fmt >> 4)\n\t\t/* DATA-OUT: transfer data from initiator to target (write). */\n\t\t*dir = DMA_TO_DEVICE;\n\n\t/*\n\t * According to the SRP spec, the lower two bits of the 'ADDITIONAL\n\t * CDB LENGTH' field are reserved and the size in bytes of this field\n\t * is four times the value specified in bits 3..7. Hence the \"& ~3\".\n\t */\n\tadd_cdb_offset = srp_cmd->add_cdb_len & ~3;\n\tif (((srp_cmd->buf_fmt & 0xf) == SRP_DATA_DESC_DIRECT) ||\n\t    ((srp_cmd->buf_fmt >> 4) == SRP_DATA_DESC_DIRECT)) {\n\t\tioctx->n_rbuf = 1;\n\t\tioctx->rbufs = &ioctx->single_rbuf;\n\n\t\tdb = (struct srp_direct_buf *)(srp_cmd->add_data\n\t\t\t\t\t       + add_cdb_offset);\n\t\tmemcpy(ioctx->rbufs, db, sizeof *db);\n\t\t*data_len = be32_to_cpu(db->len);\n\t} else if (((srp_cmd->buf_fmt & 0xf) == SRP_DATA_DESC_INDIRECT) ||\n\t\t   ((srp_cmd->buf_fmt >> 4) == SRP_DATA_DESC_INDIRECT)) {\n\t\tidb = (struct srp_indirect_buf *)(srp_cmd->add_data\n\t\t\t\t\t\t  + add_cdb_offset);\n\n\t\tioctx->n_rbuf = be32_to_cpu(idb->table_desc.len) / sizeof *db;\n\n\t\tif (ioctx->n_rbuf >\n\t\t    (srp_cmd->data_out_desc_cnt + srp_cmd->data_in_desc_cnt)) {\n\t\t\tpr_err(\"received unsupported SRP_CMD request\"\n\t\t\t       \" type (%u out + %u in != %u / %zu)\\n\",\n\t\t\t       srp_cmd->data_out_desc_cnt,\n\t\t\t       srp_cmd->data_in_desc_cnt,\n\t\t\t       be32_to_cpu(idb->table_desc.len),\n\t\t\t       sizeof(*db));\n\t\t\tioctx->n_rbuf = 0;\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (ioctx->n_rbuf == 1)\n\t\t\tioctx->rbufs = &ioctx->single_rbuf;\n\t\telse {\n\t\t\tioctx->rbufs =\n\t\t\t\tkmalloc(ioctx->n_rbuf * sizeof *db, GFP_ATOMIC);\n\t\t\tif (!ioctx->rbufs) {\n\t\t\t\tioctx->n_rbuf = 0;\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdb = idb->desc_list;\n\t\tmemcpy(ioctx->rbufs, db, ioctx->n_rbuf * sizeof *db);\n\t\t*data_len = be32_to_cpu(idb->len);\n\t}\nout:\n\treturn ret;\n}\n\n/**\n * srpt_init_ch_qp() - Initialize queue pair attributes.\n *\n * Initialized the attributes of queue pair 'qp' by allowing local write,\n * remote read and remote write. Also transitions 'qp' to state IB_QPS_INIT.\n */\nstatic int srpt_init_ch_qp(struct srpt_rdma_ch *ch, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr *attr;\n\tint ret;\n\n\tattr = kzalloc(sizeof *attr, GFP_KERNEL);\n\tif (!attr)\n\t\treturn -ENOMEM;\n\n\tattr->qp_state = IB_QPS_INIT;\n\tattr->qp_access_flags = IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_READ |\n\t    IB_ACCESS_REMOTE_WRITE;\n\tattr->port_num = ch->sport->port;\n\tattr->pkey_index = 0;\n\n\tret = ib_modify_qp(qp, attr,\n\t\t\t   IB_QP_STATE | IB_QP_ACCESS_FLAGS | IB_QP_PORT |\n\t\t\t   IB_QP_PKEY_INDEX);\n\n\tkfree(attr);\n\treturn ret;\n}\n\n/**\n * srpt_ch_qp_rtr() - Change the state of a channel to 'ready to receive' (RTR).\n * @ch: channel of the queue pair.\n * @qp: queue pair to change the state of.\n *\n * Returns zero upon success and a negative value upon failure.\n *\n * Note: currently a struct ib_qp_attr takes 136 bytes on a 64-bit system.\n * If this structure ever becomes larger, it might be necessary to allocate\n * it dynamically instead of on the stack.\n */\nstatic int srpt_ch_qp_rtr(struct srpt_rdma_ch *ch, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint attr_mask;\n\tint ret;\n\n\tqp_attr.qp_state = IB_QPS_RTR;\n\tret = ib_cm_init_qp_attr(ch->cm_id, &qp_attr, &attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tqp_attr.max_dest_rd_atomic = 4;\n\n\tret = ib_modify_qp(qp, &qp_attr, attr_mask);\n\nout:\n\treturn ret;\n}\n\n/**\n * srpt_ch_qp_rts() - Change the state of a channel to 'ready to send' (RTS).\n * @ch: channel of the queue pair.\n * @qp: queue pair to change the state of.\n *\n * Returns zero upon success and a negative value upon failure.\n *\n * Note: currently a struct ib_qp_attr takes 136 bytes on a 64-bit system.\n * If this structure ever becomes larger, it might be necessary to allocate\n * it dynamically instead of on the stack.\n */\nstatic int srpt_ch_qp_rts(struct srpt_rdma_ch *ch, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint attr_mask;\n\tint ret;\n\n\tqp_attr.qp_state = IB_QPS_RTS;\n\tret = ib_cm_init_qp_attr(ch->cm_id, &qp_attr, &attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tqp_attr.max_rd_atomic = 4;\n\n\tret = ib_modify_qp(qp, &qp_attr, attr_mask);\n\nout:\n\treturn ret;\n}\n\n/**\n * srpt_ch_qp_err() - Set the channel queue pair state to 'error'.\n */\nstatic int srpt_ch_qp_err(struct srpt_rdma_ch *ch)\n{\n\tstruct ib_qp_attr qp_attr;\n\n\tqp_attr.qp_state = IB_QPS_ERR;\n\treturn ib_modify_qp(ch->qp, &qp_attr, IB_QP_STATE);\n}\n\n/**\n * srpt_unmap_sg_to_ib_sge() - Unmap an IB SGE list.\n */\nstatic void srpt_unmap_sg_to_ib_sge(struct srpt_rdma_ch *ch,\n\t\t\t\t    struct srpt_send_ioctx *ioctx)\n{\n\tstruct scatterlist *sg;\n\tenum dma_data_direction dir;\n\n\tBUG_ON(!ch);\n\tBUG_ON(!ioctx);\n\tBUG_ON(ioctx->n_rdma && !ioctx->rdma_wrs);\n\n\twhile (ioctx->n_rdma)\n\t\tkfree(ioctx->rdma_wrs[--ioctx->n_rdma].wr.sg_list);\n\n\tkfree(ioctx->rdma_wrs);\n\tioctx->rdma_wrs = NULL;\n\n\tif (ioctx->mapped_sg_count) {\n\t\tsg = ioctx->sg;\n\t\tWARN_ON(!sg);\n\t\tdir = ioctx->cmd.data_direction;\n\t\tBUG_ON(dir == DMA_NONE);\n\t\tib_dma_unmap_sg(ch->sport->sdev->device, sg, ioctx->sg_cnt,\n\t\t\t\topposite_dma_dir(dir));\n\t\tioctx->mapped_sg_count = 0;\n\t}\n}\n\n/**\n * srpt_map_sg_to_ib_sge() - Map an SG list to an IB SGE list.\n */\nstatic int srpt_map_sg_to_ib_sge(struct srpt_rdma_ch *ch,\n\t\t\t\t struct srpt_send_ioctx *ioctx)\n{\n\tstruct ib_device *dev = ch->sport->sdev->device;\n\tstruct se_cmd *cmd;\n\tstruct scatterlist *sg, *sg_orig;\n\tint sg_cnt;\n\tenum dma_data_direction dir;\n\tstruct ib_rdma_wr *riu;\n\tstruct srp_direct_buf *db;\n\tdma_addr_t dma_addr;\n\tstruct ib_sge *sge;\n\tu64 raddr;\n\tu32 rsize;\n\tu32 tsize;\n\tu32 dma_len;\n\tint count, nrdma;\n\tint i, j, k;\n\n\tBUG_ON(!ch);\n\tBUG_ON(!ioctx);\n\tcmd = &ioctx->cmd;\n\tdir = cmd->data_direction;\n\tBUG_ON(dir == DMA_NONE);\n\n\tioctx->sg = sg = sg_orig = cmd->t_data_sg;\n\tioctx->sg_cnt = sg_cnt = cmd->t_data_nents;\n\n\tcount = ib_dma_map_sg(ch->sport->sdev->device, sg, sg_cnt,\n\t\t\t      opposite_dma_dir(dir));\n\tif (unlikely(!count))\n\t\treturn -EAGAIN;\n\n\tioctx->mapped_sg_count = count;\n\n\tif (ioctx->rdma_wrs && ioctx->n_rdma_wrs)\n\t\tnrdma = ioctx->n_rdma_wrs;\n\telse {\n\t\tnrdma = (count + SRPT_DEF_SG_PER_WQE - 1) / SRPT_DEF_SG_PER_WQE\n\t\t\t+ ioctx->n_rbuf;\n\n\t\tioctx->rdma_wrs = kcalloc(nrdma, sizeof(*ioctx->rdma_wrs),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!ioctx->rdma_wrs)\n\t\t\tgoto free_mem;\n\n\t\tioctx->n_rdma_wrs = nrdma;\n\t}\n\n\tdb = ioctx->rbufs;\n\ttsize = cmd->data_length;\n\tdma_len = ib_sg_dma_len(dev, &sg[0]);\n\triu = ioctx->rdma_wrs;\n\n\t/*\n\t * For each remote desc - calculate the #ib_sge.\n\t * If #ib_sge < SRPT_DEF_SG_PER_WQE per rdma operation then\n\t *      each remote desc rdma_iu is required a rdma wr;\n\t * else\n\t *      we need to allocate extra rdma_iu to carry extra #ib_sge in\n\t *      another rdma wr\n\t */\n\tfor (i = 0, j = 0;\n\t     j < count && i < ioctx->n_rbuf && tsize > 0; ++i, ++riu, ++db) {\n\t\trsize = be32_to_cpu(db->len);\n\t\traddr = be64_to_cpu(db->va);\n\t\triu->remote_addr = raddr;\n\t\triu->rkey = be32_to_cpu(db->key);\n\t\triu->wr.num_sge = 0;\n\n\t\t/* calculate how many sge required for this remote_buf */\n\t\twhile (rsize > 0 && tsize > 0) {\n\n\t\t\tif (rsize >= dma_len) {\n\t\t\t\ttsize -= dma_len;\n\t\t\t\trsize -= dma_len;\n\t\t\t\traddr += dma_len;\n\n\t\t\t\tif (tsize > 0) {\n\t\t\t\t\t++j;\n\t\t\t\t\tif (j < count) {\n\t\t\t\t\t\tsg = sg_next(sg);\n\t\t\t\t\t\tdma_len = ib_sg_dma_len(\n\t\t\t\t\t\t\t\tdev, sg);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ttsize -= rsize;\n\t\t\t\tdma_len -= rsize;\n\t\t\t\trsize = 0;\n\t\t\t}\n\n\t\t\t++riu->wr.num_sge;\n\n\t\t\tif (rsize > 0 &&\n\t\t\t    riu->wr.num_sge == SRPT_DEF_SG_PER_WQE) {\n\t\t\t\t++ioctx->n_rdma;\n\t\t\t\triu->wr.sg_list = kmalloc_array(riu->wr.num_sge,\n\t\t\t\t\t\tsizeof(*riu->wr.sg_list),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\t\t\tif (!riu->wr.sg_list)\n\t\t\t\t\tgoto free_mem;\n\n\t\t\t\t++riu;\n\t\t\t\triu->wr.num_sge = 0;\n\t\t\t\triu->remote_addr = raddr;\n\t\t\t\triu->rkey = be32_to_cpu(db->key);\n\t\t\t}\n\t\t}\n\n\t\t++ioctx->n_rdma;\n\t\triu->wr.sg_list = kmalloc_array(riu->wr.num_sge,\n\t\t\t\t\tsizeof(*riu->wr.sg_list),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!riu->wr.sg_list)\n\t\t\tgoto free_mem;\n\t}\n\n\tdb = ioctx->rbufs;\n\ttsize = cmd->data_length;\n\triu = ioctx->rdma_wrs;\n\tsg = sg_orig;\n\tdma_len = ib_sg_dma_len(dev, &sg[0]);\n\tdma_addr = ib_sg_dma_address(dev, &sg[0]);\n\n\t/* this second loop is really mapped sg_addres to rdma_iu->ib_sge */\n\tfor (i = 0, j = 0;\n\t     j < count && i < ioctx->n_rbuf && tsize > 0; ++i, ++riu, ++db) {\n\t\trsize = be32_to_cpu(db->len);\n\t\tsge = riu->wr.sg_list;\n\t\tk = 0;\n\n\t\twhile (rsize > 0 && tsize > 0) {\n\t\t\tsge->addr = dma_addr;\n\t\t\tsge->lkey = ch->sport->sdev->pd->local_dma_lkey;\n\n\t\t\tif (rsize >= dma_len) {\n\t\t\t\tsge->length =\n\t\t\t\t\t(tsize < dma_len) ? tsize : dma_len;\n\t\t\t\ttsize -= dma_len;\n\t\t\t\trsize -= dma_len;\n\n\t\t\t\tif (tsize > 0) {\n\t\t\t\t\t++j;\n\t\t\t\t\tif (j < count) {\n\t\t\t\t\t\tsg = sg_next(sg);\n\t\t\t\t\t\tdma_len = ib_sg_dma_len(\n\t\t\t\t\t\t\t\tdev, sg);\n\t\t\t\t\t\tdma_addr = ib_sg_dma_address(\n\t\t\t\t\t\t\t\tdev, sg);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tsge->length = (tsize < rsize) ? tsize : rsize;\n\t\t\t\ttsize -= rsize;\n\t\t\t\tdma_len -= rsize;\n\t\t\t\tdma_addr += rsize;\n\t\t\t\trsize = 0;\n\t\t\t}\n\n\t\t\t++k;\n\t\t\tif (k == riu->wr.num_sge && rsize > 0 && tsize > 0) {\n\t\t\t\t++riu;\n\t\t\t\tsge = riu->wr.sg_list;\n\t\t\t\tk = 0;\n\t\t\t} else if (rsize > 0 && tsize > 0)\n\t\t\t\t++sge;\n\t\t}\n\t}\n\n\treturn 0;\n\nfree_mem:\n\tsrpt_unmap_sg_to_ib_sge(ch, ioctx);\n\n\treturn -ENOMEM;\n}\n\n/**\n * srpt_get_send_ioctx() - Obtain an I/O context for sending to the initiator.\n */\nstatic struct srpt_send_ioctx *srpt_get_send_ioctx(struct srpt_rdma_ch *ch)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\tunsigned long flags;\n\n\tBUG_ON(!ch);\n\n\tioctx = NULL;\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tif (!list_empty(&ch->free_list)) {\n\t\tioctx = list_first_entry(&ch->free_list,\n\t\t\t\t\t struct srpt_send_ioctx, free_list);\n\t\tlist_del(&ioctx->free_list);\n\t}\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\n\tif (!ioctx)\n\t\treturn ioctx;\n\n\tBUG_ON(ioctx->ch != ch);\n\tspin_lock_init(&ioctx->spinlock);\n\tioctx->state = SRPT_STATE_NEW;\n\tioctx->n_rbuf = 0;\n\tioctx->rbufs = NULL;\n\tioctx->n_rdma = 0;\n\tioctx->n_rdma_wrs = 0;\n\tioctx->rdma_wrs = NULL;\n\tioctx->mapped_sg_count = 0;\n\tinit_completion(&ioctx->tx_done);\n\tioctx->queue_status_only = false;\n\t/*\n\t * transport_init_se_cmd() does not initialize all fields, so do it\n\t * here.\n\t */\n\tmemset(&ioctx->cmd, 0, sizeof(ioctx->cmd));\n\tmemset(&ioctx->sense_data, 0, sizeof(ioctx->sense_data));\n\n\treturn ioctx;\n}\n\n/**\n * srpt_abort_cmd() - Abort a SCSI command.\n * @ioctx:   I/O context associated with the SCSI command.\n * @context: Preferred execution context.\n */\nstatic int srpt_abort_cmd(struct srpt_send_ioctx *ioctx)\n{\n\tenum srpt_command_state state;\n\tunsigned long flags;\n\n\tBUG_ON(!ioctx);\n\n\t/*\n\t * If the command is in a state where the target core is waiting for\n\t * the ib_srpt driver, change the state to the next state. Changing\n\t * the state of the command from SRPT_STATE_NEED_DATA to\n\t * SRPT_STATE_DATA_IN ensures that srpt_xmit_response() will call this\n\t * function a second time.\n\t */\n\n\tspin_lock_irqsave(&ioctx->spinlock, flags);\n\tstate = ioctx->state;\n\tswitch (state) {\n\tcase SRPT_STATE_NEED_DATA:\n\t\tioctx->state = SRPT_STATE_DATA_IN;\n\t\tbreak;\n\tcase SRPT_STATE_DATA_IN:\n\tcase SRPT_STATE_CMD_RSP_SENT:\n\tcase SRPT_STATE_MGMT_RSP_SENT:\n\t\tioctx->state = SRPT_STATE_DONE;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ioctx->spinlock, flags);\n\n\tif (state == SRPT_STATE_DONE) {\n\t\tstruct srpt_rdma_ch *ch = ioctx->ch;\n\n\t\tBUG_ON(ch->sess == NULL);\n\n\t\ttarget_put_sess_cmd(&ioctx->cmd);\n\t\tgoto out;\n\t}\n\n\tpr_debug(\"Aborting cmd with state %d and tag %lld\\n\", state,\n\t\t ioctx->cmd.tag);\n\n\tswitch (state) {\n\tcase SRPT_STATE_NEW:\n\tcase SRPT_STATE_DATA_IN:\n\tcase SRPT_STATE_MGMT:\n\t\t/*\n\t\t * Do nothing - defer abort processing until\n\t\t * srpt_queue_response() is invoked.\n\t\t */\n\t\tWARN_ON(!transport_check_aborted_status(&ioctx->cmd, false));\n\t\tbreak;\n\tcase SRPT_STATE_NEED_DATA:\n\t\t/* DMA_TO_DEVICE (write) - RDMA read error. */\n\n\t\t/* XXX(hch): this is a horrible layering violation.. */\n\t\tspin_lock_irqsave(&ioctx->cmd.t_state_lock, flags);\n\t\tioctx->cmd.transport_state &= ~CMD_T_ACTIVE;\n\t\tspin_unlock_irqrestore(&ioctx->cmd.t_state_lock, flags);\n\t\tbreak;\n\tcase SRPT_STATE_CMD_RSP_SENT:\n\t\t/*\n\t\t * SRP_RSP sending failed or the SRP_RSP send completion has\n\t\t * not been received in time.\n\t\t */\n\t\tsrpt_unmap_sg_to_ib_sge(ioctx->ch, ioctx);\n\t\ttarget_put_sess_cmd(&ioctx->cmd);\n\t\tbreak;\n\tcase SRPT_STATE_MGMT_RSP_SENT:\n\t\tsrpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\n\t\ttarget_put_sess_cmd(&ioctx->cmd);\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"Unexpected command state (%d)\", state);\n\t\tbreak;\n\t}\n\nout:\n\treturn state;\n}\n\n/**\n * XXX: what is now target_execute_cmd used to be asynchronous, and unmapping\n * the data that has been transferred via IB RDMA had to be postponed until the\n * check_stop_free() callback.  None of this is necessary anymore and needs to\n * be cleaned up.\n */\nstatic void srpt_rdma_read_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_rdma_ch *ch = cq->cq_context;\n\tstruct srpt_send_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_send_ioctx, rdma_cqe);\n\n\tWARN_ON(ioctx->n_rdma <= 0);\n\tatomic_add(ioctx->n_rdma, &ch->sq_wr_avail);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tpr_info(\"RDMA_READ for ioctx 0x%p failed with status %d\\n\",\n\t\t\tioctx, wc->status);\n\t\tsrpt_abort_cmd(ioctx);\n\t\treturn;\n\t}\n\n\tif (srpt_test_and_set_cmd_state(ioctx, SRPT_STATE_NEED_DATA,\n\t\t\t\t\tSRPT_STATE_DATA_IN))\n\t\ttarget_execute_cmd(&ioctx->cmd);\n\telse\n\t\tpr_err(\"%s[%d]: wrong state = %d\\n\", __func__,\n\t\t       __LINE__, srpt_get_cmd_state(ioctx));\n}\n\nstatic void srpt_rdma_write_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_send_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_send_ioctx, rdma_cqe);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tpr_info(\"RDMA_WRITE for ioctx 0x%p failed with status %d\\n\",\n\t\t\tioctx, wc->status);\n\t\tsrpt_abort_cmd(ioctx);\n\t}\n}\n\n/**\n * srpt_build_cmd_rsp() - Build an SRP_RSP response.\n * @ch: RDMA channel through which the request has been received.\n * @ioctx: I/O context associated with the SRP_CMD request. The response will\n *   be built in the buffer ioctx->buf points at and hence this function will\n *   overwrite the request data.\n * @tag: tag of the request for which this response is being generated.\n * @status: value for the STATUS field of the SRP_RSP information unit.\n *\n * Returns the size in bytes of the SRP_RSP response.\n *\n * An SRP_RSP response contains a SCSI status or service response. See also\n * section 6.9 in the SRP r16a document for the format of an SRP_RSP\n * response. See also SPC-2 for more information about sense data.\n */\nstatic int srpt_build_cmd_rsp(struct srpt_rdma_ch *ch,\n\t\t\t      struct srpt_send_ioctx *ioctx, u64 tag,\n\t\t\t      int status)\n{\n\tstruct srp_rsp *srp_rsp;\n\tconst u8 *sense_data;\n\tint sense_data_len, max_sense_len;\n\n\t/*\n\t * The lowest bit of all SAM-3 status codes is zero (see also\n\t * paragraph 5.3 in SAM-3).\n\t */\n\tWARN_ON(status & 1);\n\n\tsrp_rsp = ioctx->ioctx.buf;\n\tBUG_ON(!srp_rsp);\n\n\tsense_data = ioctx->sense_data;\n\tsense_data_len = ioctx->cmd.scsi_sense_length;\n\tWARN_ON(sense_data_len > sizeof(ioctx->sense_data));\n\n\tmemset(srp_rsp, 0, sizeof *srp_rsp);\n\tsrp_rsp->opcode = SRP_RSP;\n\tsrp_rsp->req_lim_delta =\n\t\tcpu_to_be32(1 + atomic_xchg(&ch->req_lim_delta, 0));\n\tsrp_rsp->tag = tag;\n\tsrp_rsp->status = status;\n\n\tif (sense_data_len) {\n\t\tBUILD_BUG_ON(MIN_MAX_RSP_SIZE <= sizeof(*srp_rsp));\n\t\tmax_sense_len = ch->max_ti_iu_len - sizeof(*srp_rsp);\n\t\tif (sense_data_len > max_sense_len) {\n\t\t\tpr_warn(\"truncated sense data from %d to %d\"\n\t\t\t\t\" bytes\\n\", sense_data_len, max_sense_len);\n\t\t\tsense_data_len = max_sense_len;\n\t\t}\n\n\t\tsrp_rsp->flags |= SRP_RSP_FLAG_SNSVALID;\n\t\tsrp_rsp->sense_data_len = cpu_to_be32(sense_data_len);\n\t\tmemcpy(srp_rsp + 1, sense_data, sense_data_len);\n\t}\n\n\treturn sizeof(*srp_rsp) + sense_data_len;\n}\n\n/**\n * srpt_build_tskmgmt_rsp() - Build a task management response.\n * @ch:       RDMA channel through which the request has been received.\n * @ioctx:    I/O context in which the SRP_RSP response will be built.\n * @rsp_code: RSP_CODE that will be stored in the response.\n * @tag:      Tag of the request for which this response is being generated.\n *\n * Returns the size in bytes of the SRP_RSP response.\n *\n * An SRP_RSP response contains a SCSI status or service response. See also\n * section 6.9 in the SRP r16a document for the format of an SRP_RSP\n * response.\n */\nstatic int srpt_build_tskmgmt_rsp(struct srpt_rdma_ch *ch,\n\t\t\t\t  struct srpt_send_ioctx *ioctx,\n\t\t\t\t  u8 rsp_code, u64 tag)\n{\n\tstruct srp_rsp *srp_rsp;\n\tint resp_data_len;\n\tint resp_len;\n\n\tresp_data_len = 4;\n\tresp_len = sizeof(*srp_rsp) + resp_data_len;\n\n\tsrp_rsp = ioctx->ioctx.buf;\n\tBUG_ON(!srp_rsp);\n\tmemset(srp_rsp, 0, sizeof *srp_rsp);\n\n\tsrp_rsp->opcode = SRP_RSP;\n\tsrp_rsp->req_lim_delta =\n\t\tcpu_to_be32(1 + atomic_xchg(&ch->req_lim_delta, 0));\n\tsrp_rsp->tag = tag;\n\n\tsrp_rsp->flags |= SRP_RSP_FLAG_RSPVALID;\n\tsrp_rsp->resp_data_len = cpu_to_be32(resp_data_len);\n\tsrp_rsp->data[3] = rsp_code;\n\n\treturn resp_len;\n}\n\n#define NO_SUCH_LUN ((uint64_t)-1LL)\n\n/*\n * SCSI LUN addressing method. See also SAM-2 and the section about\n * eight byte LUNs.\n */\nenum scsi_lun_addr_method {\n\tSCSI_LUN_ADDR_METHOD_PERIPHERAL   = 0,\n\tSCSI_LUN_ADDR_METHOD_FLAT         = 1,\n\tSCSI_LUN_ADDR_METHOD_LUN          = 2,\n\tSCSI_LUN_ADDR_METHOD_EXTENDED_LUN = 3,\n};\n\n/*\n * srpt_unpack_lun() - Convert from network LUN to linear LUN.\n *\n * Convert an 2-byte, 4-byte, 6-byte or 8-byte LUN structure in network byte\n * order (big endian) to a linear LUN. Supports three LUN addressing methods:\n * peripheral, flat and logical unit. See also SAM-2, section 4.9.4 (page 40).\n */\nstatic uint64_t srpt_unpack_lun(const uint8_t *lun, int len)\n{\n\tuint64_t res = NO_SUCH_LUN;\n\tint addressing_method;\n\n\tif (unlikely(len < 2)) {\n\t\tpr_err(\"Illegal LUN length %d, expected 2 bytes or more\\n\",\n\t\t       len);\n\t\tgoto out;\n\t}\n\n\tswitch (len) {\n\tcase 8:\n\t\tif ((*((__be64 *)lun) &\n\t\t     cpu_to_be64(0x0000FFFFFFFFFFFFLL)) != 0)\n\t\t\tgoto out_err;\n\t\tbreak;\n\tcase 4:\n\t\tif (*((__be16 *)&lun[2]) != 0)\n\t\t\tgoto out_err;\n\t\tbreak;\n\tcase 6:\n\t\tif (*((__be32 *)&lun[2]) != 0)\n\t\t\tgoto out_err;\n\t\tbreak;\n\tcase 2:\n\t\tbreak;\n\tdefault:\n\t\tgoto out_err;\n\t}\n\n\taddressing_method = (*lun) >> 6; /* highest two bits of byte 0 */\n\tswitch (addressing_method) {\n\tcase SCSI_LUN_ADDR_METHOD_PERIPHERAL:\n\tcase SCSI_LUN_ADDR_METHOD_FLAT:\n\tcase SCSI_LUN_ADDR_METHOD_LUN:\n\t\tres = *(lun + 1) | (((*lun) & 0x3f) << 8);\n\t\tbreak;\n\n\tcase SCSI_LUN_ADDR_METHOD_EXTENDED_LUN:\n\tdefault:\n\t\tpr_err(\"Unimplemented LUN addressing method %u\\n\",\n\t\t       addressing_method);\n\t\tbreak;\n\t}\n\nout:\n\treturn res;\n\nout_err:\n\tpr_err(\"Support for multi-level LUNs has not yet been implemented\\n\");\n\tgoto out;\n}\n\nstatic int srpt_check_stop_free(struct se_cmd *cmd)\n{\n\tstruct srpt_send_ioctx *ioctx = container_of(cmd,\n\t\t\t\tstruct srpt_send_ioctx, cmd);\n\n\treturn target_put_sess_cmd(&ioctx->cmd);\n}\n\n/**\n * srpt_handle_cmd() - Process SRP_CMD.\n */\nstatic int srpt_handle_cmd(struct srpt_rdma_ch *ch,\n\t\t\t   struct srpt_recv_ioctx *recv_ioctx,\n\t\t\t   struct srpt_send_ioctx *send_ioctx)\n{\n\tstruct se_cmd *cmd;\n\tstruct srp_cmd *srp_cmd;\n\tuint64_t unpacked_lun;\n\tu64 data_len;\n\tenum dma_data_direction dir;\n\tsense_reason_t ret;\n\tint rc;\n\n\tBUG_ON(!send_ioctx);\n\n\tsrp_cmd = recv_ioctx->ioctx.buf;\n\tcmd = &send_ioctx->cmd;\n\tcmd->tag = srp_cmd->tag;\n\n\tswitch (srp_cmd->task_attr) {\n\tcase SRP_CMD_SIMPLE_Q:\n\t\tcmd->sam_task_attr = TCM_SIMPLE_TAG;\n\t\tbreak;\n\tcase SRP_CMD_ORDERED_Q:\n\tdefault:\n\t\tcmd->sam_task_attr = TCM_ORDERED_TAG;\n\t\tbreak;\n\tcase SRP_CMD_HEAD_OF_Q:\n\t\tcmd->sam_task_attr = TCM_HEAD_TAG;\n\t\tbreak;\n\tcase SRP_CMD_ACA:\n\t\tcmd->sam_task_attr = TCM_ACA_TAG;\n\t\tbreak;\n\t}\n\n\tif (srpt_get_desc_tbl(send_ioctx, srp_cmd, &dir, &data_len)) {\n\t\tpr_err(\"0x%llx: parsing SRP descriptor table failed.\\n\",\n\t\t       srp_cmd->tag);\n\t\tret = TCM_INVALID_CDB_FIELD;\n\t\tgoto send_sense;\n\t}\n\n\tunpacked_lun = srpt_unpack_lun((uint8_t *)&srp_cmd->lun,\n\t\t\t\t       sizeof(srp_cmd->lun));\n\trc = target_submit_cmd(cmd, ch->sess, srp_cmd->cdb,\n\t\t\t&send_ioctx->sense_data[0], unpacked_lun, data_len,\n\t\t\tTCM_SIMPLE_TAG, dir, TARGET_SCF_ACK_KREF);\n\tif (rc != 0) {\n\t\tret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\t\tgoto send_sense;\n\t}\n\treturn 0;\n\nsend_sense:\n\ttransport_send_check_condition_and_sense(cmd, ret, 0);\n\treturn -1;\n}\n\n/**\n * srpt_rx_mgmt_fn_tag() - Process a task management function by tag.\n * @ch: RDMA channel of the task management request.\n * @fn: Task management function to perform.\n * @req_tag: Tag of the SRP task management request.\n * @mgmt_ioctx: I/O context of the task management request.\n *\n * Returns zero if the target core will process the task management\n * request asynchronously.\n *\n * Note: It is assumed that the initiator serializes tag-based task management\n * requests.\n */\nstatic int srpt_rx_mgmt_fn_tag(struct srpt_send_ioctx *ioctx, u64 tag)\n{\n\tstruct srpt_device *sdev;\n\tstruct srpt_rdma_ch *ch;\n\tstruct srpt_send_ioctx *target;\n\tint ret, i;\n\n\tret = -EINVAL;\n\tch = ioctx->ch;\n\tBUG_ON(!ch);\n\tBUG_ON(!ch->sport);\n\tsdev = ch->sport->sdev;\n\tBUG_ON(!sdev);\n\tspin_lock_irq(&sdev->spinlock);\n\tfor (i = 0; i < ch->rq_size; ++i) {\n\t\ttarget = ch->ioctx_ring[i];\n\t\tif (target->cmd.se_lun == ioctx->cmd.se_lun &&\n\t\t    target->cmd.tag == tag &&\n\t\t    srpt_get_cmd_state(target) != SRPT_STATE_DONE) {\n\t\t\tret = 0;\n\t\t\t/* now let the target core abort &target->cmd; */\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&sdev->spinlock);\n\treturn ret;\n}\n\nstatic int srp_tmr_to_tcm(int fn)\n{\n\tswitch (fn) {\n\tcase SRP_TSK_ABORT_TASK:\n\t\treturn TMR_ABORT_TASK;\n\tcase SRP_TSK_ABORT_TASK_SET:\n\t\treturn TMR_ABORT_TASK_SET;\n\tcase SRP_TSK_CLEAR_TASK_SET:\n\t\treturn TMR_CLEAR_TASK_SET;\n\tcase SRP_TSK_LUN_RESET:\n\t\treturn TMR_LUN_RESET;\n\tcase SRP_TSK_CLEAR_ACA:\n\t\treturn TMR_CLEAR_ACA;\n\tdefault:\n\t\treturn -1;\n\t}\n}\n\n/**\n * srpt_handle_tsk_mgmt() - Process an SRP_TSK_MGMT information unit.\n *\n * Returns 0 if and only if the request will be processed by the target core.\n *\n * For more information about SRP_TSK_MGMT information units, see also section\n * 6.7 in the SRP r16a document.\n */\nstatic void srpt_handle_tsk_mgmt(struct srpt_rdma_ch *ch,\n\t\t\t\t struct srpt_recv_ioctx *recv_ioctx,\n\t\t\t\t struct srpt_send_ioctx *send_ioctx)\n{\n\tstruct srp_tsk_mgmt *srp_tsk;\n\tstruct se_cmd *cmd;\n\tstruct se_session *sess = ch->sess;\n\tuint64_t unpacked_lun;\n\tuint32_t tag = 0;\n\tint tcm_tmr;\n\tint rc;\n\n\tBUG_ON(!send_ioctx);\n\n\tsrp_tsk = recv_ioctx->ioctx.buf;\n\tcmd = &send_ioctx->cmd;\n\n\tpr_debug(\"recv tsk_mgmt fn %d for task_tag %lld and cmd tag %lld\"\n\t\t \" cm_id %p sess %p\\n\", srp_tsk->tsk_mgmt_func,\n\t\t srp_tsk->task_tag, srp_tsk->tag, ch->cm_id, ch->sess);\n\n\tsrpt_set_cmd_state(send_ioctx, SRPT_STATE_MGMT);\n\tsend_ioctx->cmd.tag = srp_tsk->tag;\n\ttcm_tmr = srp_tmr_to_tcm(srp_tsk->tsk_mgmt_func);\n\tif (tcm_tmr < 0) {\n\t\tsend_ioctx->cmd.se_tmr_req->response =\n\t\t\tTMR_TASK_MGMT_FUNCTION_NOT_SUPPORTED;\n\t\tgoto fail;\n\t}\n\tunpacked_lun = srpt_unpack_lun((uint8_t *)&srp_tsk->lun,\n\t\t\t\t       sizeof(srp_tsk->lun));\n\n\tif (srp_tsk->tsk_mgmt_func == SRP_TSK_ABORT_TASK) {\n\t\trc = srpt_rx_mgmt_fn_tag(send_ioctx, srp_tsk->task_tag);\n\t\tif (rc < 0) {\n\t\t\tsend_ioctx->cmd.se_tmr_req->response =\n\t\t\t\t\tTMR_TASK_DOES_NOT_EXIST;\n\t\t\tgoto fail;\n\t\t}\n\t\ttag = srp_tsk->task_tag;\n\t}\n\trc = target_submit_tmr(&send_ioctx->cmd, sess, NULL, unpacked_lun,\n\t\t\t\tsrp_tsk, tcm_tmr, GFP_KERNEL, tag,\n\t\t\t\tTARGET_SCF_ACK_KREF);\n\tif (rc != 0) {\n\t\tsend_ioctx->cmd.se_tmr_req->response = TMR_FUNCTION_REJECTED;\n\t\tgoto fail;\n\t}\n\treturn;\nfail:\n\ttransport_send_check_condition_and_sense(cmd, 0, 0); // XXX:\n}\n\n/**\n * srpt_handle_new_iu() - Process a newly received information unit.\n * @ch:    RDMA channel through which the information unit has been received.\n * @ioctx: SRPT I/O context associated with the information unit.\n */\nstatic void srpt_handle_new_iu(struct srpt_rdma_ch *ch,\n\t\t\t       struct srpt_recv_ioctx *recv_ioctx,\n\t\t\t       struct srpt_send_ioctx *send_ioctx)\n{\n\tstruct srp_cmd *srp_cmd;\n\tenum rdma_ch_state ch_state;\n\n\tBUG_ON(!ch);\n\tBUG_ON(!recv_ioctx);\n\n\tib_dma_sync_single_for_cpu(ch->sport->sdev->device,\n\t\t\t\t   recv_ioctx->ioctx.dma, srp_max_req_size,\n\t\t\t\t   DMA_FROM_DEVICE);\n\n\tch_state = srpt_get_ch_state(ch);\n\tif (unlikely(ch_state == CH_CONNECTING)) {\n\t\tlist_add_tail(&recv_ioctx->wait_list, &ch->cmd_wait_list);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(ch_state != CH_LIVE))\n\t\tgoto out;\n\n\tsrp_cmd = recv_ioctx->ioctx.buf;\n\tif (srp_cmd->opcode == SRP_CMD || srp_cmd->opcode == SRP_TSK_MGMT) {\n\t\tif (!send_ioctx)\n\t\t\tsend_ioctx = srpt_get_send_ioctx(ch);\n\t\tif (unlikely(!send_ioctx)) {\n\t\t\tlist_add_tail(&recv_ioctx->wait_list,\n\t\t\t\t      &ch->cmd_wait_list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tswitch (srp_cmd->opcode) {\n\tcase SRP_CMD:\n\t\tsrpt_handle_cmd(ch, recv_ioctx, send_ioctx);\n\t\tbreak;\n\tcase SRP_TSK_MGMT:\n\t\tsrpt_handle_tsk_mgmt(ch, recv_ioctx, send_ioctx);\n\t\tbreak;\n\tcase SRP_I_LOGOUT:\n\t\tpr_err(\"Not yet implemented: SRP_I_LOGOUT\\n\");\n\t\tbreak;\n\tcase SRP_CRED_RSP:\n\t\tpr_debug(\"received SRP_CRED_RSP\\n\");\n\t\tbreak;\n\tcase SRP_AER_RSP:\n\t\tpr_debug(\"received SRP_AER_RSP\\n\");\n\t\tbreak;\n\tcase SRP_RSP:\n\t\tpr_err(\"Received SRP_RSP\\n\");\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received IU with unknown opcode 0x%x\\n\",\n\t\t       srp_cmd->opcode);\n\t\tbreak;\n\t}\n\n\tsrpt_post_recv(ch->sport->sdev, recv_ioctx);\nout:\n\treturn;\n}\n\nstatic void srpt_recv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_rdma_ch *ch = cq->cq_context;\n\tstruct srpt_recv_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_recv_ioctx, ioctx.cqe);\n\n\tif (wc->status == IB_WC_SUCCESS) {\n\t\tint req_lim;\n\n\t\treq_lim = atomic_dec_return(&ch->req_lim);\n\t\tif (unlikely(req_lim < 0))\n\t\t\tpr_err(\"req_lim = %d < 0\\n\", req_lim);\n\t\tsrpt_handle_new_iu(ch, ioctx, NULL);\n\t} else {\n\t\tpr_info(\"receiving failed for ioctx %p with status %d\\n\",\n\t\t\tioctx, wc->status);\n\t}\n}\n\n/**\n * Note: Although this has not yet been observed during tests, at least in\n * theory it is possible that the srpt_get_send_ioctx() call invoked by\n * srpt_handle_new_iu() fails. This is possible because the req_lim_delta\n * value in each response is set to one, and it is possible that this response\n * makes the initiator send a new request before the send completion for that\n * response has been processed. This could e.g. happen if the call to\n * srpt_put_send_iotcx() is delayed because of a higher priority interrupt or\n * if IB retransmission causes generation of the send completion to be\n * delayed. Incoming information units for which srpt_get_send_ioctx() fails\n * are queued on cmd_wait_list. The code below processes these delayed\n * requests one at a time.\n */\nstatic void srpt_send_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_rdma_ch *ch = cq->cq_context;\n\tstruct srpt_send_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_send_ioctx, ioctx.cqe);\n\tenum srpt_command_state state;\n\n\tstate = srpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\n\n\tWARN_ON(state != SRPT_STATE_CMD_RSP_SENT &&\n\t\tstate != SRPT_STATE_MGMT_RSP_SENT);\n\n\tatomic_inc(&ch->sq_wr_avail);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\tpr_info(\"sending response for ioctx 0x%p failed\"\n\t\t\t\" with status %d\\n\", ioctx, wc->status);\n\n\t\tatomic_dec(&ch->req_lim);\n\t\tsrpt_abort_cmd(ioctx);\n\t\tgoto out;\n\t}\n\n\tif (state != SRPT_STATE_DONE) {\n\t\tsrpt_unmap_sg_to_ib_sge(ch, ioctx);\n\t\ttransport_generic_free_cmd(&ioctx->cmd, 0);\n\t} else {\n\t\tpr_err(\"IB completion has been received too late for\"\n\t\t       \" wr_id = %u.\\n\", ioctx->ioctx.index);\n\t}\n\nout:\n\twhile (!list_empty(&ch->cmd_wait_list) &&\n\t       srpt_get_ch_state(ch) == CH_LIVE &&\n\t       (ioctx = srpt_get_send_ioctx(ch)) != NULL) {\n\t\tstruct srpt_recv_ioctx *recv_ioctx;\n\n\t\trecv_ioctx = list_first_entry(&ch->cmd_wait_list,\n\t\t\t\t\t      struct srpt_recv_ioctx,\n\t\t\t\t\t      wait_list);\n\t\tlist_del(&recv_ioctx->wait_list);\n\t\tsrpt_handle_new_iu(ch, recv_ioctx, ioctx);\n\t}\n}\n\n/**\n * srpt_create_ch_ib() - Create receive and send completion queues.\n */\nstatic int srpt_create_ch_ib(struct srpt_rdma_ch *ch)\n{\n\tstruct ib_qp_init_attr *qp_init;\n\tstruct srpt_port *sport = ch->sport;\n\tstruct srpt_device *sdev = sport->sdev;\n\tu32 srp_sq_size = sport->port_attrib.srp_sq_size;\n\tint ret;\n\n\tWARN_ON(ch->rq_size < 1);\n\n\tret = -ENOMEM;\n\tqp_init = kzalloc(sizeof *qp_init, GFP_KERNEL);\n\tif (!qp_init)\n\t\tgoto out;\n\nretry:\n\tch->cq = ib_alloc_cq(sdev->device, ch, ch->rq_size + srp_sq_size,\n\t\t\t0 /* XXX: spread CQs */, IB_POLL_WORKQUEUE);\n\tif (IS_ERR(ch->cq)) {\n\t\tret = PTR_ERR(ch->cq);\n\t\tpr_err(\"failed to create CQ cqe= %d ret= %d\\n\",\n\t\t       ch->rq_size + srp_sq_size, ret);\n\t\tgoto out;\n\t}\n\n\tqp_init->qp_context = (void *)ch;\n\tqp_init->event_handler\n\t\t= (void(*)(struct ib_event *, void*))srpt_qp_event;\n\tqp_init->send_cq = ch->cq;\n\tqp_init->recv_cq = ch->cq;\n\tqp_init->srq = sdev->srq;\n\tqp_init->sq_sig_type = IB_SIGNAL_REQ_WR;\n\tqp_init->qp_type = IB_QPT_RC;\n\tqp_init->cap.max_send_wr = srp_sq_size;\n\tqp_init->cap.max_send_sge = SRPT_DEF_SG_PER_WQE;\n\n\tch->qp = ib_create_qp(sdev->pd, qp_init);\n\tif (IS_ERR(ch->qp)) {\n\t\tret = PTR_ERR(ch->qp);\n\t\tif (ret == -ENOMEM) {\n\t\t\tsrp_sq_size /= 2;\n\t\t\tif (srp_sq_size >= MIN_SRPT_SQ_SIZE) {\n\t\t\t\tib_destroy_cq(ch->cq);\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t}\n\t\tpr_err(\"failed to create_qp ret= %d\\n\", ret);\n\t\tgoto err_destroy_cq;\n\t}\n\n\tatomic_set(&ch->sq_wr_avail, qp_init->cap.max_send_wr);\n\n\tpr_debug(\"%s: max_cqe= %d max_sge= %d sq_size = %d cm_id= %p\\n\",\n\t\t __func__, ch->cq->cqe, qp_init->cap.max_send_sge,\n\t\t qp_init->cap.max_send_wr, ch->cm_id);\n\n\tret = srpt_init_ch_qp(ch, ch->qp);\n\tif (ret)\n\t\tgoto err_destroy_qp;\n\nout:\n\tkfree(qp_init);\n\treturn ret;\n\nerr_destroy_qp:\n\tib_destroy_qp(ch->qp);\nerr_destroy_cq:\n\tib_free_cq(ch->cq);\n\tgoto out;\n}\n\nstatic void srpt_destroy_ch_ib(struct srpt_rdma_ch *ch)\n{\n\tib_destroy_qp(ch->qp);\n\tib_free_cq(ch->cq);\n}\n\n/**\n * __srpt_close_ch() - Close an RDMA channel by setting the QP error state.\n *\n * Reset the QP and make sure all resources associated with the channel will\n * be deallocated at an appropriate time.\n *\n * Note: The caller must hold ch->sport->sdev->spinlock.\n */\nstatic void __srpt_close_ch(struct srpt_rdma_ch *ch)\n{\n\tenum rdma_ch_state prev_state;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tprev_state = ch->state;\n\tswitch (prev_state) {\n\tcase CH_CONNECTING:\n\tcase CH_LIVE:\n\t\tch->state = CH_DISCONNECTING;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\n\tswitch (prev_state) {\n\tcase CH_CONNECTING:\n\t\tib_send_cm_rej(ch->cm_id, IB_CM_REJ_NO_RESOURCES, NULL, 0,\n\t\t\t       NULL, 0);\n\t\t/* fall through */\n\tcase CH_LIVE:\n\t\tif (ib_send_cm_dreq(ch->cm_id, NULL, 0) < 0)\n\t\t\tpr_err(\"sending CM DREQ failed.\\n\");\n\t\tbreak;\n\tcase CH_DISCONNECTING:\n\t\tbreak;\n\tcase CH_DRAINING:\n\tcase CH_RELEASING:\n\t\tbreak;\n\t}\n}\n\n/**\n * srpt_close_ch() - Close an RDMA channel.\n */\nstatic void srpt_close_ch(struct srpt_rdma_ch *ch)\n{\n\tstruct srpt_device *sdev;\n\n\tsdev = ch->sport->sdev;\n\tspin_lock_irq(&sdev->spinlock);\n\t__srpt_close_ch(ch);\n\tspin_unlock_irq(&sdev->spinlock);\n}\n\n/**\n * srpt_shutdown_session() - Whether or not a session may be shut down.\n */\nstatic int srpt_shutdown_session(struct se_session *se_sess)\n{\n\tstruct srpt_rdma_ch *ch = se_sess->fabric_sess_ptr;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tif (ch->in_shutdown) {\n\t\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\t\treturn true;\n\t}\n\n\tch->in_shutdown = true;\n\ttarget_sess_cmd_list_set_waiting(se_sess);\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\n\treturn true;\n}\n\n/**\n * srpt_drain_channel() - Drain a channel by resetting the IB queue pair.\n * @cm_id: Pointer to the CM ID of the channel to be drained.\n *\n * Note: Must be called from inside srpt_cm_handler to avoid a race between\n * accessing sdev->spinlock and the call to kfree(sdev) in srpt_remove_one()\n * (the caller of srpt_cm_handler holds the cm_id spinlock; srpt_remove_one()\n * waits until all target sessions for the associated IB device have been\n * unregistered and target session registration involves a call to\n * ib_destroy_cm_id(), which locks the cm_id spinlock and hence waits until\n * this function has finished).\n */\nstatic void srpt_drain_channel(struct ib_cm_id *cm_id)\n{\n\tstruct srpt_device *sdev;\n\tstruct srpt_rdma_ch *ch;\n\tint ret;\n\tbool do_reset = false;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\n\tsdev = cm_id->context;\n\tBUG_ON(!sdev);\n\tspin_lock_irq(&sdev->spinlock);\n\tlist_for_each_entry(ch, &sdev->rch_list, list) {\n\t\tif (ch->cm_id == cm_id) {\n\t\t\tdo_reset = srpt_test_and_set_ch_state(ch,\n\t\t\t\t\tCH_CONNECTING, CH_DRAINING) ||\n\t\t\t\t   srpt_test_and_set_ch_state(ch,\n\t\t\t\t\tCH_LIVE, CH_DRAINING) ||\n\t\t\t\t   srpt_test_and_set_ch_state(ch,\n\t\t\t\t\tCH_DISCONNECTING, CH_DRAINING);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&sdev->spinlock);\n\n\tif (do_reset) {\n\t\tif (ch->sess)\n\t\t\tsrpt_shutdown_session(ch->sess);\n\n\t\tret = srpt_ch_qp_err(ch);\n\t\tif (ret < 0)\n\t\t\tpr_err(\"Setting queue pair in error state\"\n\t\t\t       \" failed: %d\\n\", ret);\n\t}\n}\n\n/**\n * srpt_find_channel() - Look up an RDMA channel.\n * @cm_id: Pointer to the CM ID of the channel to be looked up.\n *\n * Return NULL if no matching RDMA channel has been found.\n */\nstatic struct srpt_rdma_ch *srpt_find_channel(struct srpt_device *sdev,\n\t\t\t\t\t      struct ib_cm_id *cm_id)\n{\n\tstruct srpt_rdma_ch *ch;\n\tbool found;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\tBUG_ON(!sdev);\n\n\tfound = false;\n\tspin_lock_irq(&sdev->spinlock);\n\tlist_for_each_entry(ch, &sdev->rch_list, list) {\n\t\tif (ch->cm_id == cm_id) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&sdev->spinlock);\n\n\treturn found ? ch : NULL;\n}\n\n/**\n * srpt_release_channel() - Release channel resources.\n *\n * Schedules the actual release because:\n * - Calling the ib_destroy_cm_id() call from inside an IB CM callback would\n *   trigger a deadlock.\n * - It is not safe to call TCM transport_* functions from interrupt context.\n */\nstatic void srpt_release_channel(struct srpt_rdma_ch *ch)\n{\n\tschedule_work(&ch->release_work);\n}\n\nstatic void srpt_release_channel_work(struct work_struct *w)\n{\n\tstruct srpt_rdma_ch *ch;\n\tstruct srpt_device *sdev;\n\tstruct se_session *se_sess;\n\n\tch = container_of(w, struct srpt_rdma_ch, release_work);\n\tpr_debug(\"ch = %p; ch->sess = %p; release_done = %p\\n\", ch, ch->sess,\n\t\t ch->release_done);\n\n\tsdev = ch->sport->sdev;\n\tBUG_ON(!sdev);\n\n\tse_sess = ch->sess;\n\tBUG_ON(!se_sess);\n\n\ttarget_wait_for_sess_cmds(se_sess);\n\n\ttransport_deregister_session_configfs(se_sess);\n\ttransport_deregister_session(se_sess);\n\tch->sess = NULL;\n\n\tib_destroy_cm_id(ch->cm_id);\n\n\tsrpt_destroy_ch_ib(ch);\n\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)ch->ioctx_ring,\n\t\t\t     ch->sport->sdev, ch->rq_size,\n\t\t\t     ch->rsp_size, DMA_TO_DEVICE);\n\n\tspin_lock_irq(&sdev->spinlock);\n\tlist_del(&ch->list);\n\tspin_unlock_irq(&sdev->spinlock);\n\n\tif (ch->release_done)\n\t\tcomplete(ch->release_done);\n\n\twake_up(&sdev->ch_releaseQ);\n\n\tkfree(ch);\n}\n\n/**\n * srpt_cm_req_recv() - Process the event IB_CM_REQ_RECEIVED.\n *\n * Ownership of the cm_id is transferred to the target session if this\n * functions returns zero. Otherwise the caller remains the owner of cm_id.\n */\nstatic int srpt_cm_req_recv(struct ib_cm_id *cm_id,\n\t\t\t    struct ib_cm_req_event_param *param,\n\t\t\t    void *private_data)\n{\n\tstruct srpt_device *sdev = cm_id->context;\n\tstruct srpt_port *sport = &sdev->port[param->port - 1];\n\tstruct srp_login_req *req;\n\tstruct srp_login_rsp *rsp;\n\tstruct srp_login_rej *rej;\n\tstruct ib_cm_rep_param *rep_param;\n\tstruct srpt_rdma_ch *ch, *tmp_ch;\n\tstruct se_node_acl *se_acl;\n\tu32 it_iu_len;\n\tint i, ret = 0;\n\tunsigned char *p;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\n\tif (WARN_ON(!sdev || !private_data))\n\t\treturn -EINVAL;\n\n\treq = (struct srp_login_req *)private_data;\n\n\tit_iu_len = be32_to_cpu(req->req_it_iu_len);\n\n\tpr_info(\"Received SRP_LOGIN_REQ with i_port_id 0x%llx:0x%llx,\"\n\t\t\" t_port_id 0x%llx:0x%llx and it_iu_len %d on port %d\"\n\t\t\" (guid=0x%llx:0x%llx)\\n\",\n\t\tbe64_to_cpu(*(__be64 *)&req->initiator_port_id[0]),\n\t\tbe64_to_cpu(*(__be64 *)&req->initiator_port_id[8]),\n\t\tbe64_to_cpu(*(__be64 *)&req->target_port_id[0]),\n\t\tbe64_to_cpu(*(__be64 *)&req->target_port_id[8]),\n\t\tit_iu_len,\n\t\tparam->port,\n\t\tbe64_to_cpu(*(__be64 *)&sdev->port[param->port - 1].gid.raw[0]),\n\t\tbe64_to_cpu(*(__be64 *)&sdev->port[param->port - 1].gid.raw[8]));\n\n\trsp = kzalloc(sizeof *rsp, GFP_KERNEL);\n\trej = kzalloc(sizeof *rej, GFP_KERNEL);\n\trep_param = kzalloc(sizeof *rep_param, GFP_KERNEL);\n\n\tif (!rsp || !rej || !rep_param) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (it_iu_len > srp_max_req_size || it_iu_len < 64) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t      SRP_LOGIN_REJ_REQ_IT_IU_LENGTH_TOO_LARGE);\n\t\tret = -EINVAL;\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because its\"\n\t\t       \" length (%d bytes) is out of range (%d .. %d)\\n\",\n\t\t       it_iu_len, 64, srp_max_req_size);\n\t\tgoto reject;\n\t}\n\n\tif (!sport->enabled) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t      SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tret = -EINVAL;\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because the target port\"\n\t\t       \" has not yet been enabled\\n\");\n\t\tgoto reject;\n\t}\n\n\tif ((req->req_flags & SRP_MTCH_ACTION) == SRP_MULTICHAN_SINGLE) {\n\t\trsp->rsp_flags = SRP_LOGIN_RSP_MULTICHAN_NO_CHAN;\n\n\t\tspin_lock_irq(&sdev->spinlock);\n\n\t\tlist_for_each_entry_safe(ch, tmp_ch, &sdev->rch_list, list) {\n\t\t\tif (!memcmp(ch->i_port_id, req->initiator_port_id, 16)\n\t\t\t    && !memcmp(ch->t_port_id, req->target_port_id, 16)\n\t\t\t    && param->port == ch->sport->port\n\t\t\t    && param->listen_id == ch->sport->sdev->cm_id\n\t\t\t    && ch->cm_id) {\n\t\t\t\tenum rdma_ch_state ch_state;\n\n\t\t\t\tch_state = srpt_get_ch_state(ch);\n\t\t\t\tif (ch_state != CH_CONNECTING\n\t\t\t\t    && ch_state != CH_LIVE)\n\t\t\t\t\tcontinue;\n\n\t\t\t\t/* found an existing channel */\n\t\t\t\tpr_debug(\"Found existing channel %s\"\n\t\t\t\t\t \" cm_id= %p state= %d\\n\",\n\t\t\t\t\t ch->sess_name, ch->cm_id, ch_state);\n\n\t\t\t\t__srpt_close_ch(ch);\n\n\t\t\t\trsp->rsp_flags =\n\t\t\t\t\tSRP_LOGIN_RSP_MULTICHAN_TERMINATED;\n\t\t\t}\n\t\t}\n\n\t\tspin_unlock_irq(&sdev->spinlock);\n\n\t} else\n\t\trsp->rsp_flags = SRP_LOGIN_RSP_MULTICHAN_MAINTAINED;\n\n\tif (*(__be64 *)req->target_port_id != cpu_to_be64(srpt_service_guid)\n\t    || *(__be64 *)(req->target_port_id + 8) !=\n\t       cpu_to_be64(srpt_service_guid)) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t      SRP_LOGIN_REJ_UNABLE_ASSOCIATE_CHANNEL);\n\t\tret = -ENOMEM;\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because it\"\n\t\t       \" has an invalid target port identifier.\\n\");\n\t\tgoto reject;\n\t}\n\n\tch = kzalloc(sizeof *ch, GFP_KERNEL);\n\tif (!ch) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t      SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because no memory.\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto reject;\n\t}\n\n\tINIT_WORK(&ch->release_work, srpt_release_channel_work);\n\tmemcpy(ch->i_port_id, req->initiator_port_id, 16);\n\tmemcpy(ch->t_port_id, req->target_port_id, 16);\n\tch->sport = &sdev->port[param->port - 1];\n\tch->cm_id = cm_id;\n\t/*\n\t * Avoid QUEUE_FULL conditions by limiting the number of buffers used\n\t * for the SRP protocol to the command queue size.\n\t */\n\tch->rq_size = SRPT_RQ_SIZE;\n\tspin_lock_init(&ch->spinlock);\n\tch->state = CH_CONNECTING;\n\tINIT_LIST_HEAD(&ch->cmd_wait_list);\n\tch->rsp_size = ch->sport->port_attrib.srp_max_rsp_size;\n\n\tch->ioctx_ring = (struct srpt_send_ioctx **)\n\t\tsrpt_alloc_ioctx_ring(ch->sport->sdev, ch->rq_size,\n\t\t\t\t      sizeof(*ch->ioctx_ring[0]),\n\t\t\t\t      ch->rsp_size, DMA_TO_DEVICE);\n\tif (!ch->ioctx_ring)\n\t\tgoto free_ch;\n\n\tINIT_LIST_HEAD(&ch->free_list);\n\tfor (i = 0; i < ch->rq_size; i++) {\n\t\tch->ioctx_ring[i]->ch = ch;\n\t\tlist_add_tail(&ch->ioctx_ring[i]->free_list, &ch->free_list);\n\t}\n\n\tret = srpt_create_ch_ib(ch);\n\tif (ret) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t      SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because creating\"\n\t\t       \" a new RDMA channel failed.\\n\");\n\t\tgoto free_ring;\n\t}\n\n\tret = srpt_ch_qp_rtr(ch, ch->qp);\n\tif (ret) {\n\t\trej->reason = cpu_to_be32(SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because enabling\"\n\t\t       \" RTR failed (error code = %d)\\n\", ret);\n\t\tgoto destroy_ib;\n\t}\n\n\t/*\n\t * Use the initator port identifier as the session name, when\n\t * checking against se_node_acl->initiatorname[] this can be\n\t * with or without preceeding '0x'.\n\t */\n\tsnprintf(ch->sess_name, sizeof(ch->sess_name), \"0x%016llx%016llx\",\n\t\t\tbe64_to_cpu(*(__be64 *)ch->i_port_id),\n\t\t\tbe64_to_cpu(*(__be64 *)(ch->i_port_id + 8)));\n\n\tpr_debug(\"registering session %s\\n\", ch->sess_name);\n\tp = &ch->sess_name[0];\n\n\tch->sess = transport_init_session(TARGET_PROT_NORMAL);\n\tif (IS_ERR(ch->sess)) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t\tSRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_debug(\"Failed to create session\\n\");\n\t\tgoto destroy_ib;\n\t}\n\ntry_again:\n\tse_acl = core_tpg_get_initiator_node_acl(&sport->port_tpg_1, p);\n\tif (!se_acl) {\n\t\tpr_info(\"Rejected login because no ACL has been\"\n\t\t\t\" configured yet for initiator %s.\\n\", ch->sess_name);\n\t\t/*\n\t\t * XXX: Hack to retry of ch->i_port_id without leading '0x'\n\t\t */\n\t\tif (p == &ch->sess_name[0]) {\n\t\t\tp += 2;\n\t\t\tgoto try_again;\n\t\t}\n\t\trej->reason = cpu_to_be32(\n\t\t\t\tSRP_LOGIN_REJ_CHANNEL_LIMIT_REACHED);\n\t\ttransport_free_session(ch->sess);\n\t\tgoto destroy_ib;\n\t}\n\tch->sess->se_node_acl = se_acl;\n\n\ttransport_register_session(&sport->port_tpg_1, se_acl, ch->sess, ch);\n\n\tpr_debug(\"Establish connection sess=%p name=%s cm_id=%p\\n\", ch->sess,\n\t\t ch->sess_name, ch->cm_id);\n\n\t/* create srp_login_response */\n\trsp->opcode = SRP_LOGIN_RSP;\n\trsp->tag = req->tag;\n\trsp->max_it_iu_len = req->req_it_iu_len;\n\trsp->max_ti_iu_len = req->req_it_iu_len;\n\tch->max_ti_iu_len = it_iu_len;\n\trsp->buf_fmt = cpu_to_be16(SRP_BUF_FORMAT_DIRECT\n\t\t\t\t   | SRP_BUF_FORMAT_INDIRECT);\n\trsp->req_lim_delta = cpu_to_be32(ch->rq_size);\n\tatomic_set(&ch->req_lim, ch->rq_size);\n\tatomic_set(&ch->req_lim_delta, 0);\n\n\t/* create cm reply */\n\trep_param->qp_num = ch->qp->qp_num;\n\trep_param->private_data = (void *)rsp;\n\trep_param->private_data_len = sizeof *rsp;\n\trep_param->rnr_retry_count = 7;\n\trep_param->flow_control = 1;\n\trep_param->failover_accepted = 0;\n\trep_param->srq = 1;\n\trep_param->responder_resources = 4;\n\trep_param->initiator_depth = 4;\n\n\tret = ib_send_cm_rep(cm_id, rep_param);\n\tif (ret) {\n\t\tpr_err(\"sending SRP_LOGIN_REQ response failed\"\n\t\t       \" (error code = %d)\\n\", ret);\n\t\tgoto release_channel;\n\t}\n\n\tspin_lock_irq(&sdev->spinlock);\n\tlist_add_tail(&ch->list, &sdev->rch_list);\n\tspin_unlock_irq(&sdev->spinlock);\n\n\tgoto out;\n\nrelease_channel:\n\tsrpt_set_ch_state(ch, CH_RELEASING);\n\ttransport_deregister_session_configfs(ch->sess);\n\ttransport_deregister_session(ch->sess);\n\tch->sess = NULL;\n\ndestroy_ib:\n\tsrpt_destroy_ch_ib(ch);\n\nfree_ring:\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)ch->ioctx_ring,\n\t\t\t     ch->sport->sdev, ch->rq_size,\n\t\t\t     ch->rsp_size, DMA_TO_DEVICE);\nfree_ch:\n\tkfree(ch);\n\nreject:\n\trej->opcode = SRP_LOGIN_REJ;\n\trej->tag = req->tag;\n\trej->buf_fmt = cpu_to_be16(SRP_BUF_FORMAT_DIRECT\n\t\t\t\t   | SRP_BUF_FORMAT_INDIRECT);\n\n\tib_send_cm_rej(cm_id, IB_CM_REJ_CONSUMER_DEFINED, NULL, 0,\n\t\t\t     (void *)rej, sizeof *rej);\n\nout:\n\tkfree(rep_param);\n\tkfree(rsp);\n\tkfree(rej);\n\n\treturn ret;\n}\n\nstatic void srpt_cm_rej_recv(struct ib_cm_id *cm_id)\n{\n\tpr_info(\"Received IB REJ for cm_id %p.\\n\", cm_id);\n\tsrpt_drain_channel(cm_id);\n}\n\n/**\n * srpt_cm_rtu_recv() - Process an IB_CM_RTU_RECEIVED or USER_ESTABLISHED event.\n *\n * An IB_CM_RTU_RECEIVED message indicates that the connection is established\n * and that the recipient may begin transmitting (RTU = ready to use).\n */\nstatic void srpt_cm_rtu_recv(struct ib_cm_id *cm_id)\n{\n\tstruct srpt_rdma_ch *ch;\n\tint ret;\n\n\tch = srpt_find_channel(cm_id->context, cm_id);\n\tBUG_ON(!ch);\n\n\tif (srpt_test_and_set_ch_state(ch, CH_CONNECTING, CH_LIVE)) {\n\t\tstruct srpt_recv_ioctx *ioctx, *ioctx_tmp;\n\n\t\tret = srpt_ch_qp_rts(ch, ch->qp);\n\n\t\tlist_for_each_entry_safe(ioctx, ioctx_tmp, &ch->cmd_wait_list,\n\t\t\t\t\t wait_list) {\n\t\t\tlist_del(&ioctx->wait_list);\n\t\t\tsrpt_handle_new_iu(ch, ioctx, NULL);\n\t\t}\n\t\tif (ret)\n\t\t\tsrpt_close_ch(ch);\n\t}\n}\n\nstatic void srpt_cm_timewait_exit(struct ib_cm_id *cm_id)\n{\n\tpr_info(\"Received IB TimeWait exit for cm_id %p.\\n\", cm_id);\n\tsrpt_drain_channel(cm_id);\n}\n\nstatic void srpt_cm_rep_error(struct ib_cm_id *cm_id)\n{\n\tpr_info(\"Received IB REP error for cm_id %p.\\n\", cm_id);\n\tsrpt_drain_channel(cm_id);\n}\n\n/**\n * srpt_cm_dreq_recv() - Process reception of a DREQ message.\n */\nstatic void srpt_cm_dreq_recv(struct ib_cm_id *cm_id)\n{\n\tstruct srpt_rdma_ch *ch;\n\tunsigned long flags;\n\tbool send_drep = false;\n\n\tch = srpt_find_channel(cm_id->context, cm_id);\n\tBUG_ON(!ch);\n\n\tpr_debug(\"cm_id= %p ch->state= %d\\n\", cm_id, srpt_get_ch_state(ch));\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tswitch (ch->state) {\n\tcase CH_CONNECTING:\n\tcase CH_LIVE:\n\t\tsend_drep = true;\n\t\tch->state = CH_DISCONNECTING;\n\t\tbreak;\n\tcase CH_DISCONNECTING:\n\tcase CH_DRAINING:\n\tcase CH_RELEASING:\n\t\tWARN(true, \"unexpected channel state %d\\n\", ch->state);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\n\tif (send_drep) {\n\t\tif (ib_send_cm_drep(ch->cm_id, NULL, 0) < 0)\n\t\t\tpr_err(\"Sending IB DREP failed.\\n\");\n\t\tpr_info(\"Received DREQ and sent DREP for session %s.\\n\",\n\t\t\tch->sess_name);\n\t}\n}\n\n/**\n * srpt_cm_drep_recv() - Process reception of a DREP message.\n */\nstatic void srpt_cm_drep_recv(struct ib_cm_id *cm_id)\n{\n\tpr_info(\"Received InfiniBand DREP message for cm_id %p.\\n\", cm_id);\n\tsrpt_drain_channel(cm_id);\n}\n\n/**\n * srpt_cm_handler() - IB connection manager callback function.\n *\n * A non-zero return value will cause the caller destroy the CM ID.\n *\n * Note: srpt_cm_handler() must only return a non-zero value when transferring\n * ownership of the cm_id to a channel by srpt_cm_req_recv() failed. Returning\n * a non-zero value in any other case will trigger a race with the\n * ib_destroy_cm_id() call in srpt_release_channel().\n */\nstatic int srpt_cm_handler(struct ib_cm_id *cm_id, struct ib_cm_event *event)\n{\n\tint ret;\n\n\tret = 0;\n\tswitch (event->event) {\n\tcase IB_CM_REQ_RECEIVED:\n\t\tret = srpt_cm_req_recv(cm_id, &event->param.req_rcvd,\n\t\t\t\t       event->private_data);\n\t\tbreak;\n\tcase IB_CM_REJ_RECEIVED:\n\t\tsrpt_cm_rej_recv(cm_id);\n\t\tbreak;\n\tcase IB_CM_RTU_RECEIVED:\n\tcase IB_CM_USER_ESTABLISHED:\n\t\tsrpt_cm_rtu_recv(cm_id);\n\t\tbreak;\n\tcase IB_CM_DREQ_RECEIVED:\n\t\tsrpt_cm_dreq_recv(cm_id);\n\t\tbreak;\n\tcase IB_CM_DREP_RECEIVED:\n\t\tsrpt_cm_drep_recv(cm_id);\n\t\tbreak;\n\tcase IB_CM_TIMEWAIT_EXIT:\n\t\tsrpt_cm_timewait_exit(cm_id);\n\t\tbreak;\n\tcase IB_CM_REP_ERROR:\n\t\tsrpt_cm_rep_error(cm_id);\n\t\tbreak;\n\tcase IB_CM_DREQ_ERROR:\n\t\tpr_info(\"Received IB DREQ ERROR event.\\n\");\n\t\tbreak;\n\tcase IB_CM_MRA_RECEIVED:\n\t\tpr_info(\"Received IB MRA event\\n\");\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized IB CM event %d\\n\", event->event);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n/**\n * srpt_perform_rdmas() - Perform IB RDMA.\n *\n * Returns zero upon success or a negative number upon failure.\n */\nstatic int srpt_perform_rdmas(struct srpt_rdma_ch *ch,\n\t\t\t      struct srpt_send_ioctx *ioctx)\n{\n\tstruct ib_send_wr *bad_wr;\n\tint sq_wr_avail, ret, i;\n\tenum dma_data_direction dir;\n\tconst int n_rdma = ioctx->n_rdma;\n\n\tdir = ioctx->cmd.data_direction;\n\tif (dir == DMA_TO_DEVICE) {\n\t\t/* write */\n\t\tret = -ENOMEM;\n\t\tsq_wr_avail = atomic_sub_return(n_rdma, &ch->sq_wr_avail);\n\t\tif (sq_wr_avail < 0) {\n\t\t\tpr_warn(\"IB send queue full (needed %d)\\n\",\n\t\t\t\tn_rdma);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor (i = 0; i < n_rdma; i++) {\n\t\tstruct ib_send_wr *wr = &ioctx->rdma_wrs[i].wr;\n\n\t\twr->opcode = (dir == DMA_FROM_DEVICE) ?\n\t\t\t\tIB_WR_RDMA_WRITE : IB_WR_RDMA_READ;\n\n\t\tif (i == n_rdma - 1) {\n\t\t\t/* only get completion event for the last rdma read */\n\t\t\tif (dir == DMA_TO_DEVICE) {\n\t\t\t\twr->send_flags = IB_SEND_SIGNALED;\n\t\t\t\tioctx->rdma_cqe.done = srpt_rdma_read_done;\n\t\t\t} else {\n\t\t\t\tioctx->rdma_cqe.done = srpt_rdma_write_done;\n\t\t\t}\n\t\t\twr->wr_cqe = &ioctx->rdma_cqe;\n\t\t\twr->next = NULL;\n\t\t} else {\n\t\t\twr->wr_cqe = NULL;\n\t\t\twr->next = &ioctx->rdma_wrs[i + 1].wr;\n\t\t}\n\t}\n\n\tret = ib_post_send(ch->qp, &ioctx->rdma_wrs->wr, &bad_wr);\n\tif (ret)\n\t\tpr_err(\"%s[%d]: ib_post_send() returned %d for %d/%d\\n\",\n\t\t\t\t __func__, __LINE__, ret, i, n_rdma);\nout:\n\tif (unlikely(dir == DMA_TO_DEVICE && ret < 0))\n\t\tatomic_add(n_rdma, &ch->sq_wr_avail);\n\treturn ret;\n}\n\n/**\n * srpt_xfer_data() - Start data transfer from initiator to target.\n */\nstatic int srpt_xfer_data(struct srpt_rdma_ch *ch,\n\t\t\t  struct srpt_send_ioctx *ioctx)\n{\n\tint ret;\n\n\tret = srpt_map_sg_to_ib_sge(ch, ioctx);\n\tif (ret) {\n\t\tpr_err(\"%s[%d] ret=%d\\n\", __func__, __LINE__, ret);\n\t\tgoto out;\n\t}\n\n\tret = srpt_perform_rdmas(ch, ioctx);\n\tif (ret) {\n\t\tif (ret == -EAGAIN || ret == -ENOMEM)\n\t\t\tpr_info(\"%s[%d] queue full -- ret=%d\\n\",\n\t\t\t\t__func__, __LINE__, ret);\n\t\telse\n\t\t\tpr_err(\"%s[%d] fatal error -- ret=%d\\n\",\n\t\t\t       __func__, __LINE__, ret);\n\t\tgoto out_unmap;\n\t}\n\nout:\n\treturn ret;\nout_unmap:\n\tsrpt_unmap_sg_to_ib_sge(ch, ioctx);\n\tgoto out;\n}\n\nstatic int srpt_write_pending_status(struct se_cmd *se_cmd)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\n\tioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\n\treturn srpt_get_cmd_state(ioctx) == SRPT_STATE_NEED_DATA;\n}\n\n/*\n * srpt_write_pending() - Start data transfer from initiator to target (write).\n */\nstatic int srpt_write_pending(struct se_cmd *se_cmd)\n{\n\tstruct srpt_rdma_ch *ch;\n\tstruct srpt_send_ioctx *ioctx;\n\tenum srpt_command_state new_state;\n\tenum rdma_ch_state ch_state;\n\tint ret;\n\n\tioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\n\n\tnew_state = srpt_set_cmd_state(ioctx, SRPT_STATE_NEED_DATA);\n\tWARN_ON(new_state == SRPT_STATE_DONE);\n\n\tch = ioctx->ch;\n\tBUG_ON(!ch);\n\n\tch_state = srpt_get_ch_state(ch);\n\tswitch (ch_state) {\n\tcase CH_CONNECTING:\n\t\tWARN(true, \"unexpected channel state %d\\n\", ch_state);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\tcase CH_LIVE:\n\t\tbreak;\n\tcase CH_DISCONNECTING:\n\tcase CH_DRAINING:\n\tcase CH_RELEASING:\n\t\tpr_debug(\"cmd with tag %lld: channel disconnecting\\n\",\n\t\t\t ioctx->cmd.tag);\n\t\tsrpt_set_cmd_state(ioctx, SRPT_STATE_DATA_IN);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tret = srpt_xfer_data(ch, ioctx);\n\nout:\n\treturn ret;\n}\n\nstatic u8 tcm_to_srp_tsk_mgmt_status(const int tcm_mgmt_status)\n{\n\tswitch (tcm_mgmt_status) {\n\tcase TMR_FUNCTION_COMPLETE:\n\t\treturn SRP_TSK_MGMT_SUCCESS;\n\tcase TMR_FUNCTION_REJECTED:\n\t\treturn SRP_TSK_MGMT_FUNC_NOT_SUPP;\n\t}\n\treturn SRP_TSK_MGMT_FAILED;\n}\n\n/**\n * srpt_queue_response() - Transmits the response to a SCSI command.\n *\n * Callback function called by the TCM core. Must not block since it can be\n * invoked on the context of the IB completion handler.\n */\nstatic void srpt_queue_response(struct se_cmd *cmd)\n{\n\tstruct srpt_rdma_ch *ch;\n\tstruct srpt_send_ioctx *ioctx;\n\tenum srpt_command_state state;\n\tunsigned long flags;\n\tint ret;\n\tenum dma_data_direction dir;\n\tint resp_len;\n\tu8 srp_tm_status;\n\n\tioctx = container_of(cmd, struct srpt_send_ioctx, cmd);\n\tch = ioctx->ch;\n\tBUG_ON(!ch);\n\n\tspin_lock_irqsave(&ioctx->spinlock, flags);\n\tstate = ioctx->state;\n\tswitch (state) {\n\tcase SRPT_STATE_NEW:\n\tcase SRPT_STATE_DATA_IN:\n\t\tioctx->state = SRPT_STATE_CMD_RSP_SENT;\n\t\tbreak;\n\tcase SRPT_STATE_MGMT:\n\t\tioctx->state = SRPT_STATE_MGMT_RSP_SENT;\n\t\tbreak;\n\tdefault:\n\t\tWARN(true, \"ch %p; cmd %d: unexpected command state %d\\n\",\n\t\t\tch, ioctx->ioctx.index, ioctx->state);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ioctx->spinlock, flags);\n\n\tif (unlikely(transport_check_aborted_status(&ioctx->cmd, false)\n\t\t     || WARN_ON_ONCE(state == SRPT_STATE_CMD_RSP_SENT))) {\n\t\tatomic_inc(&ch->req_lim_delta);\n\t\tsrpt_abort_cmd(ioctx);\n\t\treturn;\n\t}\n\n\tdir = ioctx->cmd.data_direction;\n\n\t/* For read commands, transfer the data to the initiator. */\n\tif (dir == DMA_FROM_DEVICE && ioctx->cmd.data_length &&\n\t    !ioctx->queue_status_only) {\n\t\tret = srpt_xfer_data(ch, ioctx);\n\t\tif (ret) {\n\t\t\tpr_err(\"xfer_data failed for tag %llu\\n\",\n\t\t\t       ioctx->cmd.tag);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (state != SRPT_STATE_MGMT)\n\t\tresp_len = srpt_build_cmd_rsp(ch, ioctx, ioctx->cmd.tag,\n\t\t\t\t\t      cmd->scsi_status);\n\telse {\n\t\tsrp_tm_status\n\t\t\t= tcm_to_srp_tsk_mgmt_status(cmd->se_tmr_req->response);\n\t\tresp_len = srpt_build_tskmgmt_rsp(ch, ioctx, srp_tm_status,\n\t\t\t\t\t\t ioctx->cmd.tag);\n\t}\n\tret = srpt_post_send(ch, ioctx, resp_len);\n\tif (ret) {\n\t\tpr_err(\"sending cmd response failed for tag %llu\\n\",\n\t\t       ioctx->cmd.tag);\n\t\tsrpt_unmap_sg_to_ib_sge(ch, ioctx);\n\t\tsrpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\n\t\ttarget_put_sess_cmd(&ioctx->cmd);\n\t}\n}\n\nstatic int srpt_queue_data_in(struct se_cmd *cmd)\n{\n\tsrpt_queue_response(cmd);\n\treturn 0;\n}\n\nstatic void srpt_queue_tm_rsp(struct se_cmd *cmd)\n{\n\tsrpt_queue_response(cmd);\n}\n\nstatic void srpt_aborted_task(struct se_cmd *cmd)\n{\n\tstruct srpt_send_ioctx *ioctx = container_of(cmd,\n\t\t\t\tstruct srpt_send_ioctx, cmd);\n\n\tsrpt_unmap_sg_to_ib_sge(ioctx->ch, ioctx);\n}\n\nstatic int srpt_queue_status(struct se_cmd *cmd)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\n\tioctx = container_of(cmd, struct srpt_send_ioctx, cmd);\n\tBUG_ON(ioctx->sense_data != cmd->sense_buffer);\n\tif (cmd->se_cmd_flags &\n\t    (SCF_TRANSPORT_TASK_SENSE | SCF_EMULATED_TASK_SENSE))\n\t\tWARN_ON(cmd->scsi_status != SAM_STAT_CHECK_CONDITION);\n\tioctx->queue_status_only = true;\n\tsrpt_queue_response(cmd);\n\treturn 0;\n}\n\nstatic void srpt_refresh_port_work(struct work_struct *work)\n{\n\tstruct srpt_port *sport = container_of(work, struct srpt_port, work);\n\n\tsrpt_refresh_port(sport);\n}\n\nstatic int srpt_ch_list_empty(struct srpt_device *sdev)\n{\n\tint res;\n\n\tspin_lock_irq(&sdev->spinlock);\n\tres = list_empty(&sdev->rch_list);\n\tspin_unlock_irq(&sdev->spinlock);\n\n\treturn res;\n}\n\n/**\n * srpt_release_sdev() - Free the channel resources associated with a target.\n */\nstatic int srpt_release_sdev(struct srpt_device *sdev)\n{\n\tstruct srpt_rdma_ch *ch, *tmp_ch;\n\tint res;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\n\tBUG_ON(!sdev);\n\n\tspin_lock_irq(&sdev->spinlock);\n\tlist_for_each_entry_safe(ch, tmp_ch, &sdev->rch_list, list)\n\t\t__srpt_close_ch(ch);\n\tspin_unlock_irq(&sdev->spinlock);\n\n\tres = wait_event_interruptible(sdev->ch_releaseQ,\n\t\t\t\t       srpt_ch_list_empty(sdev));\n\tif (res)\n\t\tpr_err(\"%s: interrupted.\\n\", __func__);\n\n\treturn 0;\n}\n\nstatic struct srpt_port *__srpt_lookup_port(const char *name)\n{\n\tstruct ib_device *dev;\n\tstruct srpt_device *sdev;\n\tstruct srpt_port *sport;\n\tint i;\n\n\tlist_for_each_entry(sdev, &srpt_dev_list, list) {\n\t\tdev = sdev->device;\n\t\tif (!dev)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < dev->phys_port_cnt; i++) {\n\t\t\tsport = &sdev->port[i];\n\n\t\t\tif (!strcmp(sport->port_guid, name))\n\t\t\t\treturn sport;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct srpt_port *srpt_lookup_port(const char *name)\n{\n\tstruct srpt_port *sport;\n\n\tspin_lock(&srpt_dev_lock);\n\tsport = __srpt_lookup_port(name);\n\tspin_unlock(&srpt_dev_lock);\n\n\treturn sport;\n}\n\n/**\n * srpt_add_one() - Infiniband device addition callback function.\n */\nstatic void srpt_add_one(struct ib_device *device)\n{\n\tstruct srpt_device *sdev;\n\tstruct srpt_port *sport;\n\tstruct ib_srq_init_attr srq_attr;\n\tint i;\n\n\tpr_debug(\"device = %p, device->dma_ops = %p\\n\", device,\n\t\t device->dma_ops);\n\n\tsdev = kzalloc(sizeof *sdev, GFP_KERNEL);\n\tif (!sdev)\n\t\tgoto err;\n\n\tsdev->device = device;\n\tINIT_LIST_HEAD(&sdev->rch_list);\n\tinit_waitqueue_head(&sdev->ch_releaseQ);\n\tspin_lock_init(&sdev->spinlock);\n\n\tsdev->pd = ib_alloc_pd(device);\n\tif (IS_ERR(sdev->pd))\n\t\tgoto free_dev;\n\n\tsdev->srq_size = min(srpt_srq_size, sdev->device->attrs.max_srq_wr);\n\n\tsrq_attr.event_handler = srpt_srq_event;\n\tsrq_attr.srq_context = (void *)sdev;\n\tsrq_attr.attr.max_wr = sdev->srq_size;\n\tsrq_attr.attr.max_sge = 1;\n\tsrq_attr.attr.srq_limit = 0;\n\tsrq_attr.srq_type = IB_SRQT_BASIC;\n\n\tsdev->srq = ib_create_srq(sdev->pd, &srq_attr);\n\tif (IS_ERR(sdev->srq))\n\t\tgoto err_pd;\n\n\tpr_debug(\"%s: create SRQ #wr= %d max_allow=%d dev= %s\\n\",\n\t\t __func__, sdev->srq_size, sdev->device->attrs.max_srq_wr,\n\t\t device->name);\n\n\tif (!srpt_service_guid)\n\t\tsrpt_service_guid = be64_to_cpu(device->node_guid);\n\n\tsdev->cm_id = ib_create_cm_id(device, srpt_cm_handler, sdev);\n\tif (IS_ERR(sdev->cm_id))\n\t\tgoto err_srq;\n\n\t/* print out target login information */\n\tpr_debug(\"Target login info: id_ext=%016llx,ioc_guid=%016llx,\"\n\t\t \"pkey=ffff,service_id=%016llx\\n\", srpt_service_guid,\n\t\t srpt_service_guid, srpt_service_guid);\n\n\t/*\n\t * We do not have a consistent service_id (ie. also id_ext of target_id)\n\t * to identify this target. We currently use the guid of the first HCA\n\t * in the system as service_id; therefore, the target_id will change\n\t * if this HCA is gone bad and replaced by different HCA\n\t */\n\tif (ib_cm_listen(sdev->cm_id, cpu_to_be64(srpt_service_guid), 0))\n\t\tgoto err_cm;\n\n\tINIT_IB_EVENT_HANDLER(&sdev->event_handler, sdev->device,\n\t\t\t      srpt_event_handler);\n\tif (ib_register_event_handler(&sdev->event_handler))\n\t\tgoto err_cm;\n\n\tsdev->ioctx_ring = (struct srpt_recv_ioctx **)\n\t\tsrpt_alloc_ioctx_ring(sdev, sdev->srq_size,\n\t\t\t\t      sizeof(*sdev->ioctx_ring[0]),\n\t\t\t\t      srp_max_req_size, DMA_FROM_DEVICE);\n\tif (!sdev->ioctx_ring)\n\t\tgoto err_event;\n\n\tfor (i = 0; i < sdev->srq_size; ++i)\n\t\tsrpt_post_recv(sdev, sdev->ioctx_ring[i]);\n\n\tWARN_ON(sdev->device->phys_port_cnt > ARRAY_SIZE(sdev->port));\n\n\tfor (i = 1; i <= sdev->device->phys_port_cnt; i++) {\n\t\tsport = &sdev->port[i - 1];\n\t\tsport->sdev = sdev;\n\t\tsport->port = i;\n\t\tsport->port_attrib.srp_max_rdma_size = DEFAULT_MAX_RDMA_SIZE;\n\t\tsport->port_attrib.srp_max_rsp_size = DEFAULT_MAX_RSP_SIZE;\n\t\tsport->port_attrib.srp_sq_size = DEF_SRPT_SQ_SIZE;\n\t\tINIT_WORK(&sport->work, srpt_refresh_port_work);\n\n\t\tif (srpt_refresh_port(sport)) {\n\t\t\tpr_err(\"MAD registration failed for %s-%d.\\n\",\n\t\t\t       srpt_sdev_name(sdev), i);\n\t\t\tgoto err_ring;\n\t\t}\n\t\tsnprintf(sport->port_guid, sizeof(sport->port_guid),\n\t\t\t\"0x%016llx%016llx\",\n\t\t\tbe64_to_cpu(sport->gid.global.subnet_prefix),\n\t\t\tbe64_to_cpu(sport->gid.global.interface_id));\n\t}\n\n\tspin_lock(&srpt_dev_lock);\n\tlist_add_tail(&sdev->list, &srpt_dev_list);\n\tspin_unlock(&srpt_dev_lock);\n\nout:\n\tib_set_client_data(device, &srpt_client, sdev);\n\tpr_debug(\"added %s.\\n\", device->name);\n\treturn;\n\nerr_ring:\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)sdev->ioctx_ring, sdev,\n\t\t\t     sdev->srq_size, srp_max_req_size,\n\t\t\t     DMA_FROM_DEVICE);\nerr_event:\n\tib_unregister_event_handler(&sdev->event_handler);\nerr_cm:\n\tib_destroy_cm_id(sdev->cm_id);\nerr_srq:\n\tib_destroy_srq(sdev->srq);\nerr_pd:\n\tib_dealloc_pd(sdev->pd);\nfree_dev:\n\tkfree(sdev);\nerr:\n\tsdev = NULL;\n\tpr_info(\"%s(%s) failed.\\n\", __func__, device->name);\n\tgoto out;\n}\n\n/**\n * srpt_remove_one() - InfiniBand device removal callback function.\n */\nstatic void srpt_remove_one(struct ib_device *device, void *client_data)\n{\n\tstruct srpt_device *sdev = client_data;\n\tint i;\n\n\tif (!sdev) {\n\t\tpr_info(\"%s(%s): nothing to do.\\n\", __func__, device->name);\n\t\treturn;\n\t}\n\n\tsrpt_unregister_mad_agent(sdev);\n\n\tib_unregister_event_handler(&sdev->event_handler);\n\n\t/* Cancel any work queued by the just unregistered IB event handler. */\n\tfor (i = 0; i < sdev->device->phys_port_cnt; i++)\n\t\tcancel_work_sync(&sdev->port[i].work);\n\n\tib_destroy_cm_id(sdev->cm_id);\n\n\t/*\n\t * Unregistering a target must happen after destroying sdev->cm_id\n\t * such that no new SRP_LOGIN_REQ information units can arrive while\n\t * destroying the target.\n\t */\n\tspin_lock(&srpt_dev_lock);\n\tlist_del(&sdev->list);\n\tspin_unlock(&srpt_dev_lock);\n\tsrpt_release_sdev(sdev);\n\n\tib_destroy_srq(sdev->srq);\n\tib_dealloc_pd(sdev->pd);\n\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)sdev->ioctx_ring, sdev,\n\t\t\t     sdev->srq_size, srp_max_req_size, DMA_FROM_DEVICE);\n\tsdev->ioctx_ring = NULL;\n\tkfree(sdev);\n}\n\nstatic struct ib_client srpt_client = {\n\t.name = DRV_NAME,\n\t.add = srpt_add_one,\n\t.remove = srpt_remove_one\n};\n\nstatic int srpt_check_true(struct se_portal_group *se_tpg)\n{\n\treturn 1;\n}\n\nstatic int srpt_check_false(struct se_portal_group *se_tpg)\n{\n\treturn 0;\n}\n\nstatic char *srpt_get_fabric_name(void)\n{\n\treturn \"srpt\";\n}\n\nstatic char *srpt_get_fabric_wwn(struct se_portal_group *tpg)\n{\n\tstruct srpt_port *sport = container_of(tpg, struct srpt_port, port_tpg_1);\n\n\treturn sport->port_guid;\n}\n\nstatic u16 srpt_get_tag(struct se_portal_group *tpg)\n{\n\treturn 1;\n}\n\nstatic u32 srpt_tpg_get_inst_index(struct se_portal_group *se_tpg)\n{\n\treturn 1;\n}\n\nstatic void srpt_release_cmd(struct se_cmd *se_cmd)\n{\n\tstruct srpt_send_ioctx *ioctx = container_of(se_cmd,\n\t\t\t\tstruct srpt_send_ioctx, cmd);\n\tstruct srpt_rdma_ch *ch = ioctx->ch;\n\tunsigned long flags;\n\n\tWARN_ON(ioctx->state != SRPT_STATE_DONE);\n\tWARN_ON(ioctx->mapped_sg_count != 0);\n\n\tif (ioctx->n_rbuf > 1) {\n\t\tkfree(ioctx->rbufs);\n\t\tioctx->rbufs = NULL;\n\t\tioctx->n_rbuf = 0;\n\t}\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tlist_add(&ioctx->free_list, &ch->free_list);\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n}\n\n/**\n * srpt_close_session() - Forcibly close a session.\n *\n * Callback function invoked by the TCM core to clean up sessions associated\n * with a node ACL when the user invokes\n * rmdir /sys/kernel/config/target/$driver/$port/$tpg/acls/$i_port_id\n */\nstatic void srpt_close_session(struct se_session *se_sess)\n{\n\tDECLARE_COMPLETION_ONSTACK(release_done);\n\tstruct srpt_rdma_ch *ch;\n\tstruct srpt_device *sdev;\n\tunsigned long res;\n\n\tch = se_sess->fabric_sess_ptr;\n\tWARN_ON(ch->sess != se_sess);\n\n\tpr_debug(\"ch %p state %d\\n\", ch, srpt_get_ch_state(ch));\n\n\tsdev = ch->sport->sdev;\n\tspin_lock_irq(&sdev->spinlock);\n\tBUG_ON(ch->release_done);\n\tch->release_done = &release_done;\n\t__srpt_close_ch(ch);\n\tspin_unlock_irq(&sdev->spinlock);\n\n\tres = wait_for_completion_timeout(&release_done, 60 * HZ);\n\tWARN_ON(res == 0);\n}\n\n/**\n * srpt_sess_get_index() - Return the value of scsiAttIntrPortIndex (SCSI-MIB).\n *\n * A quote from RFC 4455 (SCSI-MIB) about this MIB object:\n * This object represents an arbitrary integer used to uniquely identify a\n * particular attached remote initiator port to a particular SCSI target port\n * within a particular SCSI target device within a particular SCSI instance.\n */\nstatic u32 srpt_sess_get_index(struct se_session *se_sess)\n{\n\treturn 0;\n}\n\nstatic void srpt_set_default_node_attrs(struct se_node_acl *nacl)\n{\n}\n\n/* Note: only used from inside debug printk's by the TCM core. */\nstatic int srpt_get_tcm_cmd_state(struct se_cmd *se_cmd)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\n\tioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\n\treturn srpt_get_cmd_state(ioctx);\n}\n\n/**\n * srpt_parse_i_port_id() - Parse an initiator port ID.\n * @name: ASCII representation of a 128-bit initiator port ID.\n * @i_port_id: Binary 128-bit port ID.\n */\nstatic int srpt_parse_i_port_id(u8 i_port_id[16], const char *name)\n{\n\tconst char *p;\n\tunsigned len, count, leading_zero_bytes;\n\tint ret, rc;\n\n\tp = name;\n\tif (strncasecmp(p, \"0x\", 2) == 0)\n\t\tp += 2;\n\tret = -EINVAL;\n\tlen = strlen(p);\n\tif (len % 2)\n\t\tgoto out;\n\tcount = min(len / 2, 16U);\n\tleading_zero_bytes = 16 - count;\n\tmemset(i_port_id, 0, leading_zero_bytes);\n\trc = hex2bin(i_port_id + leading_zero_bytes, p, count);\n\tif (rc < 0)\n\t\tpr_debug(\"hex2bin failed for srpt_parse_i_port_id: %d\\n\", rc);\n\tret = 0;\nout:\n\treturn ret;\n}\n\n/*\n * configfs callback function invoked for\n * mkdir /sys/kernel/config/target/$driver/$port/$tpg/acls/$i_port_id\n */\nstatic int srpt_init_nodeacl(struct se_node_acl *se_nacl, const char *name)\n{\n\tu8 i_port_id[16];\n\n\tif (srpt_parse_i_port_id(i_port_id, name) < 0) {\n\t\tpr_err(\"invalid initiator port ID %s\\n\", name);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rdma_size_show(struct config_item *item,\n\t\tchar *page)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\n\treturn sprintf(page, \"%u\\n\", sport->port_attrib.srp_max_rdma_size);\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rdma_size_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul(page, 0, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"kstrtoul() failed with ret: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\tif (val > MAX_SRPT_RDMA_SIZE) {\n\t\tpr_err(\"val: %lu exceeds MAX_SRPT_RDMA_SIZE: %d\\n\", val,\n\t\t\tMAX_SRPT_RDMA_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tif (val < DEFAULT_MAX_RDMA_SIZE) {\n\t\tpr_err(\"val: %lu smaller than DEFAULT_MAX_RDMA_SIZE: %d\\n\",\n\t\t\tval, DEFAULT_MAX_RDMA_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tsport->port_attrib.srp_max_rdma_size = val;\n\n\treturn count;\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rsp_size_show(struct config_item *item,\n\t\tchar *page)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\n\treturn sprintf(page, \"%u\\n\", sport->port_attrib.srp_max_rsp_size);\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rsp_size_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul(page, 0, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"kstrtoul() failed with ret: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\tif (val > MAX_SRPT_RSP_SIZE) {\n\t\tpr_err(\"val: %lu exceeds MAX_SRPT_RSP_SIZE: %d\\n\", val,\n\t\t\tMAX_SRPT_RSP_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tif (val < MIN_MAX_RSP_SIZE) {\n\t\tpr_err(\"val: %lu smaller than MIN_MAX_RSP_SIZE: %d\\n\", val,\n\t\t\tMIN_MAX_RSP_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tsport->port_attrib.srp_max_rsp_size = val;\n\n\treturn count;\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_sq_size_show(struct config_item *item,\n\t\tchar *page)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\n\treturn sprintf(page, \"%u\\n\", sport->port_attrib.srp_sq_size);\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_sq_size_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul(page, 0, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"kstrtoul() failed with ret: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\tif (val > MAX_SRPT_SRQ_SIZE) {\n\t\tpr_err(\"val: %lu exceeds MAX_SRPT_SRQ_SIZE: %d\\n\", val,\n\t\t\tMAX_SRPT_SRQ_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tif (val < MIN_SRPT_SRQ_SIZE) {\n\t\tpr_err(\"val: %lu smaller than MIN_SRPT_SRQ_SIZE: %d\\n\", val,\n\t\t\tMIN_SRPT_SRQ_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tsport->port_attrib.srp_sq_size = val;\n\n\treturn count;\n}\n\nCONFIGFS_ATTR(srpt_tpg_attrib_,  srp_max_rdma_size);\nCONFIGFS_ATTR(srpt_tpg_attrib_,  srp_max_rsp_size);\nCONFIGFS_ATTR(srpt_tpg_attrib_,  srp_sq_size);\n\nstatic struct configfs_attribute *srpt_tpg_attrib_attrs[] = {\n\t&srpt_tpg_attrib_attr_srp_max_rdma_size,\n\t&srpt_tpg_attrib_attr_srp_max_rsp_size,\n\t&srpt_tpg_attrib_attr_srp_sq_size,\n\tNULL,\n};\n\nstatic ssize_t srpt_tpg_enable_show(struct config_item *item, char *page)\n{\n\tstruct se_portal_group *se_tpg = to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\n\treturn snprintf(page, PAGE_SIZE, \"%d\\n\", (sport->enabled) ? 1: 0);\n}\n\nstatic ssize_t srpt_tpg_enable_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\tunsigned long tmp;\n        int ret;\n\n\tret = kstrtoul(page, 0, &tmp);\n\tif (ret < 0) {\n\t\tpr_err(\"Unable to extract srpt_tpg_store_enable\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((tmp != 0) && (tmp != 1)) {\n\t\tpr_err(\"Illegal value for srpt_tpg_store_enable: %lu\\n\", tmp);\n\t\treturn -EINVAL;\n\t}\n\tif (tmp == 1)\n\t\tsport->enabled = true;\n\telse\n\t\tsport->enabled = false;\n\n\treturn count;\n}\n\nCONFIGFS_ATTR(srpt_tpg_, enable);\n\nstatic struct configfs_attribute *srpt_tpg_attrs[] = {\n\t&srpt_tpg_attr_enable,\n\tNULL,\n};\n\n/**\n * configfs callback invoked for\n * mkdir /sys/kernel/config/target/$driver/$port/$tpg\n */\nstatic struct se_portal_group *srpt_make_tpg(struct se_wwn *wwn,\n\t\t\t\t\t     struct config_group *group,\n\t\t\t\t\t     const char *name)\n{\n\tstruct srpt_port *sport = container_of(wwn, struct srpt_port, port_wwn);\n\tint res;\n\n\t/* Initialize sport->port_wwn and sport->port_tpg_1 */\n\tres = core_tpg_register(&sport->port_wwn, &sport->port_tpg_1, SCSI_PROTOCOL_SRP);\n\tif (res)\n\t\treturn ERR_PTR(res);\n\n\treturn &sport->port_tpg_1;\n}\n\n/**\n * configfs callback invoked for\n * rmdir /sys/kernel/config/target/$driver/$port/$tpg\n */\nstatic void srpt_drop_tpg(struct se_portal_group *tpg)\n{\n\tstruct srpt_port *sport = container_of(tpg,\n\t\t\t\tstruct srpt_port, port_tpg_1);\n\n\tsport->enabled = false;\n\tcore_tpg_deregister(&sport->port_tpg_1);\n}\n\n/**\n * configfs callback invoked for\n * mkdir /sys/kernel/config/target/$driver/$port\n */\nstatic struct se_wwn *srpt_make_tport(struct target_fabric_configfs *tf,\n\t\t\t\t      struct config_group *group,\n\t\t\t\t      const char *name)\n{\n\tstruct srpt_port *sport;\n\tint ret;\n\n\tsport = srpt_lookup_port(name);\n\tpr_debug(\"make_tport(%s)\\n\", name);\n\tret = -EINVAL;\n\tif (!sport)\n\t\tgoto err;\n\n\treturn &sport->port_wwn;\n\nerr:\n\treturn ERR_PTR(ret);\n}\n\n/**\n * configfs callback invoked for\n * rmdir /sys/kernel/config/target/$driver/$port\n */\nstatic void srpt_drop_tport(struct se_wwn *wwn)\n{\n\tstruct srpt_port *sport = container_of(wwn, struct srpt_port, port_wwn);\n\n\tpr_debug(\"drop_tport(%s\\n\", config_item_name(&sport->port_wwn.wwn_group.cg_item));\n}\n\nstatic ssize_t srpt_wwn_version_show(struct config_item *item, char *buf)\n{\n\treturn scnprintf(buf, PAGE_SIZE, \"%s\\n\", DRV_VERSION);\n}\n\nCONFIGFS_ATTR_RO(srpt_wwn_, version);\n\nstatic struct configfs_attribute *srpt_wwn_attrs[] = {\n\t&srpt_wwn_attr_version,\n\tNULL,\n};\n\nstatic const struct target_core_fabric_ops srpt_template = {\n\t.module\t\t\t\t= THIS_MODULE,\n\t.name\t\t\t\t= \"srpt\",\n\t.node_acl_size\t\t\t= sizeof(struct srpt_node_acl),\n\t.get_fabric_name\t\t= srpt_get_fabric_name,\n\t.tpg_get_wwn\t\t\t= srpt_get_fabric_wwn,\n\t.tpg_get_tag\t\t\t= srpt_get_tag,\n\t.tpg_check_demo_mode\t\t= srpt_check_false,\n\t.tpg_check_demo_mode_cache\t= srpt_check_true,\n\t.tpg_check_demo_mode_write_protect = srpt_check_true,\n\t.tpg_check_prod_mode_write_protect = srpt_check_false,\n\t.tpg_get_inst_index\t\t= srpt_tpg_get_inst_index,\n\t.release_cmd\t\t\t= srpt_release_cmd,\n\t.check_stop_free\t\t= srpt_check_stop_free,\n\t.shutdown_session\t\t= srpt_shutdown_session,\n\t.close_session\t\t\t= srpt_close_session,\n\t.sess_get_index\t\t\t= srpt_sess_get_index,\n\t.sess_get_initiator_sid\t\t= NULL,\n\t.write_pending\t\t\t= srpt_write_pending,\n\t.write_pending_status\t\t= srpt_write_pending_status,\n\t.set_default_node_attributes\t= srpt_set_default_node_attrs,\n\t.get_cmd_state\t\t\t= srpt_get_tcm_cmd_state,\n\t.queue_data_in\t\t\t= srpt_queue_data_in,\n\t.queue_status\t\t\t= srpt_queue_status,\n\t.queue_tm_rsp\t\t\t= srpt_queue_tm_rsp,\n\t.aborted_task\t\t\t= srpt_aborted_task,\n\t/*\n\t * Setup function pointers for generic logic in\n\t * target_core_fabric_configfs.c\n\t */\n\t.fabric_make_wwn\t\t= srpt_make_tport,\n\t.fabric_drop_wwn\t\t= srpt_drop_tport,\n\t.fabric_make_tpg\t\t= srpt_make_tpg,\n\t.fabric_drop_tpg\t\t= srpt_drop_tpg,\n\t.fabric_init_nodeacl\t\t= srpt_init_nodeacl,\n\n\t.tfc_wwn_attrs\t\t\t= srpt_wwn_attrs,\n\t.tfc_tpg_base_attrs\t\t= srpt_tpg_attrs,\n\t.tfc_tpg_attrib_attrs\t\t= srpt_tpg_attrib_attrs,\n};\n\n/**\n * srpt_init_module() - Kernel module initialization.\n *\n * Note: Since ib_register_client() registers callback functions, and since at\n * least one of these callback functions (srpt_add_one()) calls target core\n * functions, this driver must be registered with the target core before\n * ib_register_client() is called.\n */\nstatic int __init srpt_init_module(void)\n{\n\tint ret;\n\n\tret = -EINVAL;\n\tif (srp_max_req_size < MIN_MAX_REQ_SIZE) {\n\t\tpr_err(\"invalid value %d for kernel module parameter\"\n\t\t       \" srp_max_req_size -- must be at least %d.\\n\",\n\t\t       srp_max_req_size, MIN_MAX_REQ_SIZE);\n\t\tgoto out;\n\t}\n\n\tif (srpt_srq_size < MIN_SRPT_SRQ_SIZE\n\t    || srpt_srq_size > MAX_SRPT_SRQ_SIZE) {\n\t\tpr_err(\"invalid value %d for kernel module parameter\"\n\t\t       \" srpt_srq_size -- must be in the range [%d..%d].\\n\",\n\t\t       srpt_srq_size, MIN_SRPT_SRQ_SIZE, MAX_SRPT_SRQ_SIZE);\n\t\tgoto out;\n\t}\n\n\tret = target_register_template(&srpt_template);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ib_register_client(&srpt_client);\n\tif (ret) {\n\t\tpr_err(\"couldn't register IB client\\n\");\n\t\tgoto out_unregister_target;\n\t}\n\n\treturn 0;\n\nout_unregister_target:\n\ttarget_unregister_template(&srpt_template);\nout:\n\treturn ret;\n}\n\nstatic void __exit srpt_cleanup_module(void)\n{\n\tib_unregister_client(&srpt_client);\n\ttarget_unregister_template(&srpt_template);\n}\n\nmodule_init(srpt_init_module);\nmodule_exit(srpt_cleanup_module);\n"], "fixing_code": ["/*\n * Copyright (c) 2006 - 2009 Mellanox Technology Inc.  All rights reserved.\n * Copyright (C) 2008 - 2011 Bart Van Assche <bvanassche@acm.org>.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/kthread.h>\n#include <linux/string.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <scsi/scsi_proto.h>\n#include <scsi/scsi_tcq.h>\n#include <target/target_core_base.h>\n#include <target/target_core_fabric.h>\n#include \"ib_srpt.h\"\n\n/* Name of this kernel module. */\n#define DRV_NAME\t\t\"ib_srpt\"\n#define DRV_VERSION\t\t\"2.0.0\"\n#define DRV_RELDATE\t\t\"2011-02-14\"\n\n#define SRPT_ID_STRING\t\"Linux SRP target\"\n\n#undef pr_fmt\n#define pr_fmt(fmt) DRV_NAME \" \" fmt\n\nMODULE_AUTHOR(\"Vu Pham and Bart Van Assche\");\nMODULE_DESCRIPTION(\"InfiniBand SCSI RDMA Protocol target \"\n\t\t   \"v\" DRV_VERSION \" (\" DRV_RELDATE \")\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\n/*\n * Global Variables\n */\n\nstatic u64 srpt_service_guid;\nstatic DEFINE_SPINLOCK(srpt_dev_lock);\t/* Protects srpt_dev_list. */\nstatic LIST_HEAD(srpt_dev_list);\t/* List of srpt_device structures. */\n\nstatic unsigned srp_max_req_size = DEFAULT_MAX_REQ_SIZE;\nmodule_param(srp_max_req_size, int, 0444);\nMODULE_PARM_DESC(srp_max_req_size,\n\t\t \"Maximum size of SRP request messages in bytes.\");\n\nstatic int srpt_srq_size = DEFAULT_SRPT_SRQ_SIZE;\nmodule_param(srpt_srq_size, int, 0444);\nMODULE_PARM_DESC(srpt_srq_size,\n\t\t \"Shared receive queue (SRQ) size.\");\n\nstatic int srpt_get_u64_x(char *buffer, struct kernel_param *kp)\n{\n\treturn sprintf(buffer, \"0x%016llx\", *(u64 *)kp->arg);\n}\nmodule_param_call(srpt_service_guid, NULL, srpt_get_u64_x, &srpt_service_guid,\n\t\t  0444);\nMODULE_PARM_DESC(srpt_service_guid,\n\t\t \"Using this value for ioc_guid, id_ext, and cm_listen_id\"\n\t\t \" instead of using the node_guid of the first HCA.\");\n\nstatic struct ib_client srpt_client;\nstatic void srpt_release_channel(struct srpt_rdma_ch *ch);\nstatic int srpt_queue_status(struct se_cmd *cmd);\nstatic void srpt_recv_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void srpt_send_done(struct ib_cq *cq, struct ib_wc *wc);\n\n/**\n * opposite_dma_dir() - Swap DMA_TO_DEVICE and DMA_FROM_DEVICE.\n */\nstatic inline\nenum dma_data_direction opposite_dma_dir(enum dma_data_direction dir)\n{\n\tswitch (dir) {\n\tcase DMA_TO_DEVICE:\treturn DMA_FROM_DEVICE;\n\tcase DMA_FROM_DEVICE:\treturn DMA_TO_DEVICE;\n\tdefault:\t\treturn dir;\n\t}\n}\n\n/**\n * srpt_sdev_name() - Return the name associated with the HCA.\n *\n * Examples are ib0, ib1, ...\n */\nstatic inline const char *srpt_sdev_name(struct srpt_device *sdev)\n{\n\treturn sdev->device->name;\n}\n\nstatic enum rdma_ch_state srpt_get_ch_state(struct srpt_rdma_ch *ch)\n{\n\tunsigned long flags;\n\tenum rdma_ch_state state;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tstate = ch->state;\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\treturn state;\n}\n\nstatic enum rdma_ch_state\nsrpt_set_ch_state(struct srpt_rdma_ch *ch, enum rdma_ch_state new_state)\n{\n\tunsigned long flags;\n\tenum rdma_ch_state prev;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tprev = ch->state;\n\tch->state = new_state;\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\treturn prev;\n}\n\n/**\n * srpt_test_and_set_ch_state() - Test and set the channel state.\n *\n * Returns true if and only if the channel state has been set to the new state.\n */\nstatic bool\nsrpt_test_and_set_ch_state(struct srpt_rdma_ch *ch, enum rdma_ch_state old,\n\t\t\t   enum rdma_ch_state new)\n{\n\tunsigned long flags;\n\tenum rdma_ch_state prev;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tprev = ch->state;\n\tif (prev == old)\n\t\tch->state = new;\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\treturn prev == old;\n}\n\n/**\n * srpt_event_handler() - Asynchronous IB event callback function.\n *\n * Callback function called by the InfiniBand core when an asynchronous IB\n * event occurs. This callback may occur in interrupt context. See also\n * section 11.5.2, Set Asynchronous Event Handler in the InfiniBand\n * Architecture Specification.\n */\nstatic void srpt_event_handler(struct ib_event_handler *handler,\n\t\t\t       struct ib_event *event)\n{\n\tstruct srpt_device *sdev;\n\tstruct srpt_port *sport;\n\n\tsdev = ib_get_client_data(event->device, &srpt_client);\n\tif (!sdev || sdev->device != event->device)\n\t\treturn;\n\n\tpr_debug(\"ASYNC event= %d on device= %s\\n\", event->event,\n\t\t srpt_sdev_name(sdev));\n\n\tswitch (event->event) {\n\tcase IB_EVENT_PORT_ERR:\n\t\tif (event->element.port_num <= sdev->device->phys_port_cnt) {\n\t\t\tsport = &sdev->port[event->element.port_num - 1];\n\t\t\tsport->lid = 0;\n\t\t\tsport->sm_lid = 0;\n\t\t}\n\t\tbreak;\n\tcase IB_EVENT_PORT_ACTIVE:\n\tcase IB_EVENT_LID_CHANGE:\n\tcase IB_EVENT_PKEY_CHANGE:\n\tcase IB_EVENT_SM_CHANGE:\n\tcase IB_EVENT_CLIENT_REREGISTER:\n\tcase IB_EVENT_GID_CHANGE:\n\t\t/* Refresh port data asynchronously. */\n\t\tif (event->element.port_num <= sdev->device->phys_port_cnt) {\n\t\t\tsport = &sdev->port[event->element.port_num - 1];\n\t\t\tif (!sport->lid && !sport->sm_lid)\n\t\t\t\tschedule_work(&sport->work);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized IB event %d\\n\",\n\t\t       event->event);\n\t\tbreak;\n\t}\n}\n\n/**\n * srpt_srq_event() - SRQ event callback function.\n */\nstatic void srpt_srq_event(struct ib_event *event, void *ctx)\n{\n\tpr_info(\"SRQ event %d\\n\", event->event);\n}\n\n/**\n * srpt_qp_event() - QP event callback function.\n */\nstatic void srpt_qp_event(struct ib_event *event, struct srpt_rdma_ch *ch)\n{\n\tpr_debug(\"QP event %d on cm_id=%p sess_name=%s state=%d\\n\",\n\t\t event->event, ch->cm_id, ch->sess_name, srpt_get_ch_state(ch));\n\n\tswitch (event->event) {\n\tcase IB_EVENT_COMM_EST:\n\t\tib_cm_notify(ch->cm_id, event->event);\n\t\tbreak;\n\tcase IB_EVENT_QP_LAST_WQE_REACHED:\n\t\tif (srpt_test_and_set_ch_state(ch, CH_DRAINING,\n\t\t\t\t\t       CH_RELEASING))\n\t\t\tsrpt_release_channel(ch);\n\t\telse\n\t\t\tpr_debug(\"%s: state %d - ignored LAST_WQE.\\n\",\n\t\t\t\t ch->sess_name, srpt_get_ch_state(ch));\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized IB QP event %d\\n\", event->event);\n\t\tbreak;\n\t}\n}\n\n/**\n * srpt_set_ioc() - Helper function for initializing an IOUnitInfo structure.\n *\n * @slot: one-based slot number.\n * @value: four-bit value.\n *\n * Copies the lowest four bits of value in element slot of the array of four\n * bit elements called c_list (controller list). The index slot is one-based.\n */\nstatic void srpt_set_ioc(u8 *c_list, u32 slot, u8 value)\n{\n\tu16 id;\n\tu8 tmp;\n\n\tid = (slot - 1) / 2;\n\tif (slot & 0x1) {\n\t\ttmp = c_list[id] & 0xf;\n\t\tc_list[id] = (value << 4) | tmp;\n\t} else {\n\t\ttmp = c_list[id] & 0xf0;\n\t\tc_list[id] = (value & 0xf) | tmp;\n\t}\n}\n\n/**\n * srpt_get_class_port_info() - Copy ClassPortInfo to a management datagram.\n *\n * See also section 16.3.3.1 ClassPortInfo in the InfiniBand Architecture\n * Specification.\n */\nstatic void srpt_get_class_port_info(struct ib_dm_mad *mad)\n{\n\tstruct ib_class_port_info *cif;\n\n\tcif = (struct ib_class_port_info *)mad->data;\n\tmemset(cif, 0, sizeof *cif);\n\tcif->base_version = 1;\n\tcif->class_version = 1;\n\tcif->resp_time_value = 20;\n\n\tmad->mad_hdr.status = 0;\n}\n\n/**\n * srpt_get_iou() - Write IOUnitInfo to a management datagram.\n *\n * See also section 16.3.3.3 IOUnitInfo in the InfiniBand Architecture\n * Specification. See also section B.7, table B.6 in the SRP r16a document.\n */\nstatic void srpt_get_iou(struct ib_dm_mad *mad)\n{\n\tstruct ib_dm_iou_info *ioui;\n\tu8 slot;\n\tint i;\n\n\tioui = (struct ib_dm_iou_info *)mad->data;\n\tioui->change_id = cpu_to_be16(1);\n\tioui->max_controllers = 16;\n\n\t/* set present for slot 1 and empty for the rest */\n\tsrpt_set_ioc(ioui->controller_list, 1, 1);\n\tfor (i = 1, slot = 2; i < 16; i++, slot++)\n\t\tsrpt_set_ioc(ioui->controller_list, slot, 0);\n\n\tmad->mad_hdr.status = 0;\n}\n\n/**\n * srpt_get_ioc() - Write IOControllerprofile to a management datagram.\n *\n * See also section 16.3.3.4 IOControllerProfile in the InfiniBand\n * Architecture Specification. See also section B.7, table B.7 in the SRP\n * r16a document.\n */\nstatic void srpt_get_ioc(struct srpt_port *sport, u32 slot,\n\t\t\t struct ib_dm_mad *mad)\n{\n\tstruct srpt_device *sdev = sport->sdev;\n\tstruct ib_dm_ioc_profile *iocp;\n\n\tiocp = (struct ib_dm_ioc_profile *)mad->data;\n\n\tif (!slot || slot > 16) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_INVALID_FIELD);\n\t\treturn;\n\t}\n\n\tif (slot > 2) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_NO_IOC);\n\t\treturn;\n\t}\n\n\tmemset(iocp, 0, sizeof *iocp);\n\tstrcpy(iocp->id_string, SRPT_ID_STRING);\n\tiocp->guid = cpu_to_be64(srpt_service_guid);\n\tiocp->vendor_id = cpu_to_be32(sdev->device->attrs.vendor_id);\n\tiocp->device_id = cpu_to_be32(sdev->device->attrs.vendor_part_id);\n\tiocp->device_version = cpu_to_be16(sdev->device->attrs.hw_ver);\n\tiocp->subsys_vendor_id = cpu_to_be32(sdev->device->attrs.vendor_id);\n\tiocp->subsys_device_id = 0x0;\n\tiocp->io_class = cpu_to_be16(SRP_REV16A_IB_IO_CLASS);\n\tiocp->io_subclass = cpu_to_be16(SRP_IO_SUBCLASS);\n\tiocp->protocol = cpu_to_be16(SRP_PROTOCOL);\n\tiocp->protocol_version = cpu_to_be16(SRP_PROTOCOL_VERSION);\n\tiocp->send_queue_depth = cpu_to_be16(sdev->srq_size);\n\tiocp->rdma_read_depth = 4;\n\tiocp->send_size = cpu_to_be32(srp_max_req_size);\n\tiocp->rdma_size = cpu_to_be32(min(sport->port_attrib.srp_max_rdma_size,\n\t\t\t\t\t  1U << 24));\n\tiocp->num_svc_entries = 1;\n\tiocp->op_cap_mask = SRP_SEND_TO_IOC | SRP_SEND_FROM_IOC |\n\t\tSRP_RDMA_READ_FROM_IOC | SRP_RDMA_WRITE_FROM_IOC;\n\n\tmad->mad_hdr.status = 0;\n}\n\n/**\n * srpt_get_svc_entries() - Write ServiceEntries to a management datagram.\n *\n * See also section 16.3.3.5 ServiceEntries in the InfiniBand Architecture\n * Specification. See also section B.7, table B.8 in the SRP r16a document.\n */\nstatic void srpt_get_svc_entries(u64 ioc_guid,\n\t\t\t\t u16 slot, u8 hi, u8 lo, struct ib_dm_mad *mad)\n{\n\tstruct ib_dm_svc_entries *svc_entries;\n\n\tWARN_ON(!ioc_guid);\n\n\tif (!slot || slot > 16) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_INVALID_FIELD);\n\t\treturn;\n\t}\n\n\tif (slot > 2 || lo > hi || hi > 1) {\n\t\tmad->mad_hdr.status\n\t\t\t= cpu_to_be16(DM_MAD_STATUS_NO_IOC);\n\t\treturn;\n\t}\n\n\tsvc_entries = (struct ib_dm_svc_entries *)mad->data;\n\tmemset(svc_entries, 0, sizeof *svc_entries);\n\tsvc_entries->service_entries[0].id = cpu_to_be64(ioc_guid);\n\tsnprintf(svc_entries->service_entries[0].name,\n\t\t sizeof(svc_entries->service_entries[0].name),\n\t\t \"%s%016llx\",\n\t\t SRP_SERVICE_NAME_PREFIX,\n\t\t ioc_guid);\n\n\tmad->mad_hdr.status = 0;\n}\n\n/**\n * srpt_mgmt_method_get() - Process a received management datagram.\n * @sp:      source port through which the MAD has been received.\n * @rq_mad:  received MAD.\n * @rsp_mad: response MAD.\n */\nstatic void srpt_mgmt_method_get(struct srpt_port *sp, struct ib_mad *rq_mad,\n\t\t\t\t struct ib_dm_mad *rsp_mad)\n{\n\tu16 attr_id;\n\tu32 slot;\n\tu8 hi, lo;\n\n\tattr_id = be16_to_cpu(rq_mad->mad_hdr.attr_id);\n\tswitch (attr_id) {\n\tcase DM_ATTR_CLASS_PORT_INFO:\n\t\tsrpt_get_class_port_info(rsp_mad);\n\t\tbreak;\n\tcase DM_ATTR_IOU_INFO:\n\t\tsrpt_get_iou(rsp_mad);\n\t\tbreak;\n\tcase DM_ATTR_IOC_PROFILE:\n\t\tslot = be32_to_cpu(rq_mad->mad_hdr.attr_mod);\n\t\tsrpt_get_ioc(sp, slot, rsp_mad);\n\t\tbreak;\n\tcase DM_ATTR_SVC_ENTRIES:\n\t\tslot = be32_to_cpu(rq_mad->mad_hdr.attr_mod);\n\t\thi = (u8) ((slot >> 8) & 0xff);\n\t\tlo = (u8) (slot & 0xff);\n\t\tslot = (u16) ((slot >> 16) & 0xffff);\n\t\tsrpt_get_svc_entries(srpt_service_guid,\n\t\t\t\t     slot, hi, lo, rsp_mad);\n\t\tbreak;\n\tdefault:\n\t\trsp_mad->mad_hdr.status =\n\t\t    cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD_ATTR);\n\t\tbreak;\n\t}\n}\n\n/**\n * srpt_mad_send_handler() - Post MAD-send callback function.\n */\nstatic void srpt_mad_send_handler(struct ib_mad_agent *mad_agent,\n\t\t\t\t  struct ib_mad_send_wc *mad_wc)\n{\n\tib_destroy_ah(mad_wc->send_buf->ah);\n\tib_free_send_mad(mad_wc->send_buf);\n}\n\n/**\n * srpt_mad_recv_handler() - MAD reception callback function.\n */\nstatic void srpt_mad_recv_handler(struct ib_mad_agent *mad_agent,\n\t\t\t\t  struct ib_mad_send_buf *send_buf,\n\t\t\t\t  struct ib_mad_recv_wc *mad_wc)\n{\n\tstruct srpt_port *sport = (struct srpt_port *)mad_agent->context;\n\tstruct ib_ah *ah;\n\tstruct ib_mad_send_buf *rsp;\n\tstruct ib_dm_mad *dm_mad;\n\n\tif (!mad_wc || !mad_wc->recv_buf.mad)\n\t\treturn;\n\n\tah = ib_create_ah_from_wc(mad_agent->qp->pd, mad_wc->wc,\n\t\t\t\t  mad_wc->recv_buf.grh, mad_agent->port_num);\n\tif (IS_ERR(ah))\n\t\tgoto err;\n\n\tBUILD_BUG_ON(offsetof(struct ib_dm_mad, data) != IB_MGMT_DEVICE_HDR);\n\n\trsp = ib_create_send_mad(mad_agent, mad_wc->wc->src_qp,\n\t\t\t\t mad_wc->wc->pkey_index, 0,\n\t\t\t\t IB_MGMT_DEVICE_HDR, IB_MGMT_DEVICE_DATA,\n\t\t\t\t GFP_KERNEL,\n\t\t\t\t IB_MGMT_BASE_VERSION);\n\tif (IS_ERR(rsp))\n\t\tgoto err_rsp;\n\n\trsp->ah = ah;\n\n\tdm_mad = rsp->mad;\n\tmemcpy(dm_mad, mad_wc->recv_buf.mad, sizeof *dm_mad);\n\tdm_mad->mad_hdr.method = IB_MGMT_METHOD_GET_RESP;\n\tdm_mad->mad_hdr.status = 0;\n\n\tswitch (mad_wc->recv_buf.mad->mad_hdr.method) {\n\tcase IB_MGMT_METHOD_GET:\n\t\tsrpt_mgmt_method_get(sport, mad_wc->recv_buf.mad, dm_mad);\n\t\tbreak;\n\tcase IB_MGMT_METHOD_SET:\n\t\tdm_mad->mad_hdr.status =\n\t\t    cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD_ATTR);\n\t\tbreak;\n\tdefault:\n\t\tdm_mad->mad_hdr.status =\n\t\t    cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD);\n\t\tbreak;\n\t}\n\n\tif (!ib_post_send_mad(rsp, NULL)) {\n\t\tib_free_recv_mad(mad_wc);\n\t\t/* will destroy_ah & free_send_mad in send completion */\n\t\treturn;\n\t}\n\n\tib_free_send_mad(rsp);\n\nerr_rsp:\n\tib_destroy_ah(ah);\nerr:\n\tib_free_recv_mad(mad_wc);\n}\n\n/**\n * srpt_refresh_port() - Configure a HCA port.\n *\n * Enable InfiniBand management datagram processing, update the cached sm_lid,\n * lid and gid values, and register a callback function for processing MADs\n * on the specified port.\n *\n * Note: It is safe to call this function more than once for the same port.\n */\nstatic int srpt_refresh_port(struct srpt_port *sport)\n{\n\tstruct ib_mad_reg_req reg_req;\n\tstruct ib_port_modify port_modify;\n\tstruct ib_port_attr port_attr;\n\tint ret;\n\n\tmemset(&port_modify, 0, sizeof port_modify);\n\tport_modify.set_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP;\n\tport_modify.clr_port_cap_mask = 0;\n\n\tret = ib_modify_port(sport->sdev->device, sport->port, 0, &port_modify);\n\tif (ret)\n\t\tgoto err_mod_port;\n\n\tret = ib_query_port(sport->sdev->device, sport->port, &port_attr);\n\tif (ret)\n\t\tgoto err_query_port;\n\n\tsport->sm_lid = port_attr.sm_lid;\n\tsport->lid = port_attr.lid;\n\n\tret = ib_query_gid(sport->sdev->device, sport->port, 0, &sport->gid,\n\t\t\t   NULL);\n\tif (ret)\n\t\tgoto err_query_port;\n\n\tif (!sport->mad_agent) {\n\t\tmemset(&reg_req, 0, sizeof reg_req);\n\t\treg_req.mgmt_class = IB_MGMT_CLASS_DEVICE_MGMT;\n\t\treg_req.mgmt_class_version = IB_MGMT_BASE_VERSION;\n\t\tset_bit(IB_MGMT_METHOD_GET, reg_req.method_mask);\n\t\tset_bit(IB_MGMT_METHOD_SET, reg_req.method_mask);\n\n\t\tsport->mad_agent = ib_register_mad_agent(sport->sdev->device,\n\t\t\t\t\t\t\t sport->port,\n\t\t\t\t\t\t\t IB_QPT_GSI,\n\t\t\t\t\t\t\t &reg_req, 0,\n\t\t\t\t\t\t\t srpt_mad_send_handler,\n\t\t\t\t\t\t\t srpt_mad_recv_handler,\n\t\t\t\t\t\t\t sport, 0);\n\t\tif (IS_ERR(sport->mad_agent)) {\n\t\t\tret = PTR_ERR(sport->mad_agent);\n\t\t\tsport->mad_agent = NULL;\n\t\t\tgoto err_query_port;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_query_port:\n\n\tport_modify.set_port_cap_mask = 0;\n\tport_modify.clr_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP;\n\tib_modify_port(sport->sdev->device, sport->port, 0, &port_modify);\n\nerr_mod_port:\n\n\treturn ret;\n}\n\n/**\n * srpt_unregister_mad_agent() - Unregister MAD callback functions.\n *\n * Note: It is safe to call this function more than once for the same device.\n */\nstatic void srpt_unregister_mad_agent(struct srpt_device *sdev)\n{\n\tstruct ib_port_modify port_modify = {\n\t\t.clr_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP,\n\t};\n\tstruct srpt_port *sport;\n\tint i;\n\n\tfor (i = 1; i <= sdev->device->phys_port_cnt; i++) {\n\t\tsport = &sdev->port[i - 1];\n\t\tWARN_ON(sport->port != i);\n\t\tif (ib_modify_port(sdev->device, i, 0, &port_modify) < 0)\n\t\t\tpr_err(\"disabling MAD processing failed.\\n\");\n\t\tif (sport->mad_agent) {\n\t\t\tib_unregister_mad_agent(sport->mad_agent);\n\t\t\tsport->mad_agent = NULL;\n\t\t}\n\t}\n}\n\n/**\n * srpt_alloc_ioctx() - Allocate an SRPT I/O context structure.\n */\nstatic struct srpt_ioctx *srpt_alloc_ioctx(struct srpt_device *sdev,\n\t\t\t\t\t   int ioctx_size, int dma_size,\n\t\t\t\t\t   enum dma_data_direction dir)\n{\n\tstruct srpt_ioctx *ioctx;\n\n\tioctx = kmalloc(ioctx_size, GFP_KERNEL);\n\tif (!ioctx)\n\t\tgoto err;\n\n\tioctx->buf = kmalloc(dma_size, GFP_KERNEL);\n\tif (!ioctx->buf)\n\t\tgoto err_free_ioctx;\n\n\tioctx->dma = ib_dma_map_single(sdev->device, ioctx->buf, dma_size, dir);\n\tif (ib_dma_mapping_error(sdev->device, ioctx->dma))\n\t\tgoto err_free_buf;\n\n\treturn ioctx;\n\nerr_free_buf:\n\tkfree(ioctx->buf);\nerr_free_ioctx:\n\tkfree(ioctx);\nerr:\n\treturn NULL;\n}\n\n/**\n * srpt_free_ioctx() - Free an SRPT I/O context structure.\n */\nstatic void srpt_free_ioctx(struct srpt_device *sdev, struct srpt_ioctx *ioctx,\n\t\t\t    int dma_size, enum dma_data_direction dir)\n{\n\tif (!ioctx)\n\t\treturn;\n\n\tib_dma_unmap_single(sdev->device, ioctx->dma, dma_size, dir);\n\tkfree(ioctx->buf);\n\tkfree(ioctx);\n}\n\n/**\n * srpt_alloc_ioctx_ring() - Allocate a ring of SRPT I/O context structures.\n * @sdev:       Device to allocate the I/O context ring for.\n * @ring_size:  Number of elements in the I/O context ring.\n * @ioctx_size: I/O context size.\n * @dma_size:   DMA buffer size.\n * @dir:        DMA data direction.\n */\nstatic struct srpt_ioctx **srpt_alloc_ioctx_ring(struct srpt_device *sdev,\n\t\t\t\tint ring_size, int ioctx_size,\n\t\t\t\tint dma_size, enum dma_data_direction dir)\n{\n\tstruct srpt_ioctx **ring;\n\tint i;\n\n\tWARN_ON(ioctx_size != sizeof(struct srpt_recv_ioctx)\n\t\t&& ioctx_size != sizeof(struct srpt_send_ioctx));\n\n\tring = kmalloc(ring_size * sizeof(ring[0]), GFP_KERNEL);\n\tif (!ring)\n\t\tgoto out;\n\tfor (i = 0; i < ring_size; ++i) {\n\t\tring[i] = srpt_alloc_ioctx(sdev, ioctx_size, dma_size, dir);\n\t\tif (!ring[i])\n\t\t\tgoto err;\n\t\tring[i]->index = i;\n\t}\n\tgoto out;\n\nerr:\n\twhile (--i >= 0)\n\t\tsrpt_free_ioctx(sdev, ring[i], dma_size, dir);\n\tkfree(ring);\n\tring = NULL;\nout:\n\treturn ring;\n}\n\n/**\n * srpt_free_ioctx_ring() - Free the ring of SRPT I/O context structures.\n */\nstatic void srpt_free_ioctx_ring(struct srpt_ioctx **ioctx_ring,\n\t\t\t\t struct srpt_device *sdev, int ring_size,\n\t\t\t\t int dma_size, enum dma_data_direction dir)\n{\n\tint i;\n\n\tfor (i = 0; i < ring_size; ++i)\n\t\tsrpt_free_ioctx(sdev, ioctx_ring[i], dma_size, dir);\n\tkfree(ioctx_ring);\n}\n\n/**\n * srpt_get_cmd_state() - Get the state of a SCSI command.\n */\nstatic enum srpt_command_state srpt_get_cmd_state(struct srpt_send_ioctx *ioctx)\n{\n\tenum srpt_command_state state;\n\tunsigned long flags;\n\n\tBUG_ON(!ioctx);\n\n\tspin_lock_irqsave(&ioctx->spinlock, flags);\n\tstate = ioctx->state;\n\tspin_unlock_irqrestore(&ioctx->spinlock, flags);\n\treturn state;\n}\n\n/**\n * srpt_set_cmd_state() - Set the state of a SCSI command.\n *\n * Does not modify the state of aborted commands. Returns the previous command\n * state.\n */\nstatic enum srpt_command_state srpt_set_cmd_state(struct srpt_send_ioctx *ioctx,\n\t\t\t\t\t\t  enum srpt_command_state new)\n{\n\tenum srpt_command_state previous;\n\tunsigned long flags;\n\n\tBUG_ON(!ioctx);\n\n\tspin_lock_irqsave(&ioctx->spinlock, flags);\n\tprevious = ioctx->state;\n\tif (previous != SRPT_STATE_DONE)\n\t\tioctx->state = new;\n\tspin_unlock_irqrestore(&ioctx->spinlock, flags);\n\n\treturn previous;\n}\n\n/**\n * srpt_test_and_set_cmd_state() - Test and set the state of a command.\n *\n * Returns true if and only if the previous command state was equal to 'old'.\n */\nstatic bool srpt_test_and_set_cmd_state(struct srpt_send_ioctx *ioctx,\n\t\t\t\t\tenum srpt_command_state old,\n\t\t\t\t\tenum srpt_command_state new)\n{\n\tenum srpt_command_state previous;\n\tunsigned long flags;\n\n\tWARN_ON(!ioctx);\n\tWARN_ON(old == SRPT_STATE_DONE);\n\tWARN_ON(new == SRPT_STATE_NEW);\n\n\tspin_lock_irqsave(&ioctx->spinlock, flags);\n\tprevious = ioctx->state;\n\tif (previous == old)\n\t\tioctx->state = new;\n\tspin_unlock_irqrestore(&ioctx->spinlock, flags);\n\treturn previous == old;\n}\n\n/**\n * srpt_post_recv() - Post an IB receive request.\n */\nstatic int srpt_post_recv(struct srpt_device *sdev,\n\t\t\t  struct srpt_recv_ioctx *ioctx)\n{\n\tstruct ib_sge list;\n\tstruct ib_recv_wr wr, *bad_wr;\n\n\tBUG_ON(!sdev);\n\tlist.addr = ioctx->ioctx.dma;\n\tlist.length = srp_max_req_size;\n\tlist.lkey = sdev->pd->local_dma_lkey;\n\n\tioctx->ioctx.cqe.done = srpt_recv_done;\n\twr.wr_cqe = &ioctx->ioctx.cqe;\n\twr.next = NULL;\n\twr.sg_list = &list;\n\twr.num_sge = 1;\n\n\treturn ib_post_srq_recv(sdev->srq, &wr, &bad_wr);\n}\n\n/**\n * srpt_post_send() - Post an IB send request.\n *\n * Returns zero upon success and a non-zero value upon failure.\n */\nstatic int srpt_post_send(struct srpt_rdma_ch *ch,\n\t\t\t  struct srpt_send_ioctx *ioctx, int len)\n{\n\tstruct ib_sge list;\n\tstruct ib_send_wr wr, *bad_wr;\n\tstruct srpt_device *sdev = ch->sport->sdev;\n\tint ret;\n\n\tatomic_inc(&ch->req_lim);\n\n\tret = -ENOMEM;\n\tif (unlikely(atomic_dec_return(&ch->sq_wr_avail) < 0)) {\n\t\tpr_warn(\"IB send queue full (needed 1)\\n\");\n\t\tgoto out;\n\t}\n\n\tib_dma_sync_single_for_device(sdev->device, ioctx->ioctx.dma, len,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\tlist.addr = ioctx->ioctx.dma;\n\tlist.length = len;\n\tlist.lkey = sdev->pd->local_dma_lkey;\n\n\tioctx->ioctx.cqe.done = srpt_send_done;\n\twr.next = NULL;\n\twr.wr_cqe = &ioctx->ioctx.cqe;\n\twr.sg_list = &list;\n\twr.num_sge = 1;\n\twr.opcode = IB_WR_SEND;\n\twr.send_flags = IB_SEND_SIGNALED;\n\n\tret = ib_post_send(ch->qp, &wr, &bad_wr);\n\nout:\n\tif (ret < 0) {\n\t\tatomic_inc(&ch->sq_wr_avail);\n\t\tatomic_dec(&ch->req_lim);\n\t}\n\treturn ret;\n}\n\n/**\n * srpt_get_desc_tbl() - Parse the data descriptors of an SRP_CMD request.\n * @ioctx: Pointer to the I/O context associated with the request.\n * @srp_cmd: Pointer to the SRP_CMD request data.\n * @dir: Pointer to the variable to which the transfer direction will be\n *   written.\n * @data_len: Pointer to the variable to which the total data length of all\n *   descriptors in the SRP_CMD request will be written.\n *\n * This function initializes ioctx->nrbuf and ioctx->r_bufs.\n *\n * Returns -EINVAL when the SRP_CMD request contains inconsistent descriptors;\n * -ENOMEM when memory allocation fails and zero upon success.\n */\nstatic int srpt_get_desc_tbl(struct srpt_send_ioctx *ioctx,\n\t\t\t     struct srp_cmd *srp_cmd,\n\t\t\t     enum dma_data_direction *dir, u64 *data_len)\n{\n\tstruct srp_indirect_buf *idb;\n\tstruct srp_direct_buf *db;\n\tunsigned add_cdb_offset;\n\tint ret;\n\n\t/*\n\t * The pointer computations below will only be compiled correctly\n\t * if srp_cmd::add_data is declared as s8*, u8*, s8[] or u8[], so check\n\t * whether srp_cmd::add_data has been declared as a byte pointer.\n\t */\n\tBUILD_BUG_ON(!__same_type(srp_cmd->add_data[0], (s8)0)\n\t\t     && !__same_type(srp_cmd->add_data[0], (u8)0));\n\n\tBUG_ON(!dir);\n\tBUG_ON(!data_len);\n\n\tret = 0;\n\t*data_len = 0;\n\n\t/*\n\t * The lower four bits of the buffer format field contain the DATA-IN\n\t * buffer descriptor format, and the highest four bits contain the\n\t * DATA-OUT buffer descriptor format.\n\t */\n\t*dir = DMA_NONE;\n\tif (srp_cmd->buf_fmt & 0xf)\n\t\t/* DATA-IN: transfer data from target to initiator (read). */\n\t\t*dir = DMA_FROM_DEVICE;\n\telse if (srp_cmd->buf_fmt >> 4)\n\t\t/* DATA-OUT: transfer data from initiator to target (write). */\n\t\t*dir = DMA_TO_DEVICE;\n\n\t/*\n\t * According to the SRP spec, the lower two bits of the 'ADDITIONAL\n\t * CDB LENGTH' field are reserved and the size in bytes of this field\n\t * is four times the value specified in bits 3..7. Hence the \"& ~3\".\n\t */\n\tadd_cdb_offset = srp_cmd->add_cdb_len & ~3;\n\tif (((srp_cmd->buf_fmt & 0xf) == SRP_DATA_DESC_DIRECT) ||\n\t    ((srp_cmd->buf_fmt >> 4) == SRP_DATA_DESC_DIRECT)) {\n\t\tioctx->n_rbuf = 1;\n\t\tioctx->rbufs = &ioctx->single_rbuf;\n\n\t\tdb = (struct srp_direct_buf *)(srp_cmd->add_data\n\t\t\t\t\t       + add_cdb_offset);\n\t\tmemcpy(ioctx->rbufs, db, sizeof *db);\n\t\t*data_len = be32_to_cpu(db->len);\n\t} else if (((srp_cmd->buf_fmt & 0xf) == SRP_DATA_DESC_INDIRECT) ||\n\t\t   ((srp_cmd->buf_fmt >> 4) == SRP_DATA_DESC_INDIRECT)) {\n\t\tidb = (struct srp_indirect_buf *)(srp_cmd->add_data\n\t\t\t\t\t\t  + add_cdb_offset);\n\n\t\tioctx->n_rbuf = be32_to_cpu(idb->table_desc.len) / sizeof *db;\n\n\t\tif (ioctx->n_rbuf >\n\t\t    (srp_cmd->data_out_desc_cnt + srp_cmd->data_in_desc_cnt)) {\n\t\t\tpr_err(\"received unsupported SRP_CMD request\"\n\t\t\t       \" type (%u out + %u in != %u / %zu)\\n\",\n\t\t\t       srp_cmd->data_out_desc_cnt,\n\t\t\t       srp_cmd->data_in_desc_cnt,\n\t\t\t       be32_to_cpu(idb->table_desc.len),\n\t\t\t       sizeof(*db));\n\t\t\tioctx->n_rbuf = 0;\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (ioctx->n_rbuf == 1)\n\t\t\tioctx->rbufs = &ioctx->single_rbuf;\n\t\telse {\n\t\t\tioctx->rbufs =\n\t\t\t\tkmalloc(ioctx->n_rbuf * sizeof *db, GFP_ATOMIC);\n\t\t\tif (!ioctx->rbufs) {\n\t\t\t\tioctx->n_rbuf = 0;\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdb = idb->desc_list;\n\t\tmemcpy(ioctx->rbufs, db, ioctx->n_rbuf * sizeof *db);\n\t\t*data_len = be32_to_cpu(idb->len);\n\t}\nout:\n\treturn ret;\n}\n\n/**\n * srpt_init_ch_qp() - Initialize queue pair attributes.\n *\n * Initialized the attributes of queue pair 'qp' by allowing local write,\n * remote read and remote write. Also transitions 'qp' to state IB_QPS_INIT.\n */\nstatic int srpt_init_ch_qp(struct srpt_rdma_ch *ch, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr *attr;\n\tint ret;\n\n\tattr = kzalloc(sizeof *attr, GFP_KERNEL);\n\tif (!attr)\n\t\treturn -ENOMEM;\n\n\tattr->qp_state = IB_QPS_INIT;\n\tattr->qp_access_flags = IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_READ |\n\t    IB_ACCESS_REMOTE_WRITE;\n\tattr->port_num = ch->sport->port;\n\tattr->pkey_index = 0;\n\n\tret = ib_modify_qp(qp, attr,\n\t\t\t   IB_QP_STATE | IB_QP_ACCESS_FLAGS | IB_QP_PORT |\n\t\t\t   IB_QP_PKEY_INDEX);\n\n\tkfree(attr);\n\treturn ret;\n}\n\n/**\n * srpt_ch_qp_rtr() - Change the state of a channel to 'ready to receive' (RTR).\n * @ch: channel of the queue pair.\n * @qp: queue pair to change the state of.\n *\n * Returns zero upon success and a negative value upon failure.\n *\n * Note: currently a struct ib_qp_attr takes 136 bytes on a 64-bit system.\n * If this structure ever becomes larger, it might be necessary to allocate\n * it dynamically instead of on the stack.\n */\nstatic int srpt_ch_qp_rtr(struct srpt_rdma_ch *ch, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint attr_mask;\n\tint ret;\n\n\tqp_attr.qp_state = IB_QPS_RTR;\n\tret = ib_cm_init_qp_attr(ch->cm_id, &qp_attr, &attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tqp_attr.max_dest_rd_atomic = 4;\n\n\tret = ib_modify_qp(qp, &qp_attr, attr_mask);\n\nout:\n\treturn ret;\n}\n\n/**\n * srpt_ch_qp_rts() - Change the state of a channel to 'ready to send' (RTS).\n * @ch: channel of the queue pair.\n * @qp: queue pair to change the state of.\n *\n * Returns zero upon success and a negative value upon failure.\n *\n * Note: currently a struct ib_qp_attr takes 136 bytes on a 64-bit system.\n * If this structure ever becomes larger, it might be necessary to allocate\n * it dynamically instead of on the stack.\n */\nstatic int srpt_ch_qp_rts(struct srpt_rdma_ch *ch, struct ib_qp *qp)\n{\n\tstruct ib_qp_attr qp_attr;\n\tint attr_mask;\n\tint ret;\n\n\tqp_attr.qp_state = IB_QPS_RTS;\n\tret = ib_cm_init_qp_attr(ch->cm_id, &qp_attr, &attr_mask);\n\tif (ret)\n\t\tgoto out;\n\n\tqp_attr.max_rd_atomic = 4;\n\n\tret = ib_modify_qp(qp, &qp_attr, attr_mask);\n\nout:\n\treturn ret;\n}\n\n/**\n * srpt_ch_qp_err() - Set the channel queue pair state to 'error'.\n */\nstatic int srpt_ch_qp_err(struct srpt_rdma_ch *ch)\n{\n\tstruct ib_qp_attr qp_attr;\n\n\tqp_attr.qp_state = IB_QPS_ERR;\n\treturn ib_modify_qp(ch->qp, &qp_attr, IB_QP_STATE);\n}\n\n/**\n * srpt_unmap_sg_to_ib_sge() - Unmap an IB SGE list.\n */\nstatic void srpt_unmap_sg_to_ib_sge(struct srpt_rdma_ch *ch,\n\t\t\t\t    struct srpt_send_ioctx *ioctx)\n{\n\tstruct scatterlist *sg;\n\tenum dma_data_direction dir;\n\n\tBUG_ON(!ch);\n\tBUG_ON(!ioctx);\n\tBUG_ON(ioctx->n_rdma && !ioctx->rdma_wrs);\n\n\twhile (ioctx->n_rdma)\n\t\tkfree(ioctx->rdma_wrs[--ioctx->n_rdma].wr.sg_list);\n\n\tkfree(ioctx->rdma_wrs);\n\tioctx->rdma_wrs = NULL;\n\n\tif (ioctx->mapped_sg_count) {\n\t\tsg = ioctx->sg;\n\t\tWARN_ON(!sg);\n\t\tdir = ioctx->cmd.data_direction;\n\t\tBUG_ON(dir == DMA_NONE);\n\t\tib_dma_unmap_sg(ch->sport->sdev->device, sg, ioctx->sg_cnt,\n\t\t\t\topposite_dma_dir(dir));\n\t\tioctx->mapped_sg_count = 0;\n\t}\n}\n\n/**\n * srpt_map_sg_to_ib_sge() - Map an SG list to an IB SGE list.\n */\nstatic int srpt_map_sg_to_ib_sge(struct srpt_rdma_ch *ch,\n\t\t\t\t struct srpt_send_ioctx *ioctx)\n{\n\tstruct ib_device *dev = ch->sport->sdev->device;\n\tstruct se_cmd *cmd;\n\tstruct scatterlist *sg, *sg_orig;\n\tint sg_cnt;\n\tenum dma_data_direction dir;\n\tstruct ib_rdma_wr *riu;\n\tstruct srp_direct_buf *db;\n\tdma_addr_t dma_addr;\n\tstruct ib_sge *sge;\n\tu64 raddr;\n\tu32 rsize;\n\tu32 tsize;\n\tu32 dma_len;\n\tint count, nrdma;\n\tint i, j, k;\n\n\tBUG_ON(!ch);\n\tBUG_ON(!ioctx);\n\tcmd = &ioctx->cmd;\n\tdir = cmd->data_direction;\n\tBUG_ON(dir == DMA_NONE);\n\n\tioctx->sg = sg = sg_orig = cmd->t_data_sg;\n\tioctx->sg_cnt = sg_cnt = cmd->t_data_nents;\n\n\tcount = ib_dma_map_sg(ch->sport->sdev->device, sg, sg_cnt,\n\t\t\t      opposite_dma_dir(dir));\n\tif (unlikely(!count))\n\t\treturn -EAGAIN;\n\n\tioctx->mapped_sg_count = count;\n\n\tif (ioctx->rdma_wrs && ioctx->n_rdma_wrs)\n\t\tnrdma = ioctx->n_rdma_wrs;\n\telse {\n\t\tnrdma = (count + SRPT_DEF_SG_PER_WQE - 1) / SRPT_DEF_SG_PER_WQE\n\t\t\t+ ioctx->n_rbuf;\n\n\t\tioctx->rdma_wrs = kcalloc(nrdma, sizeof(*ioctx->rdma_wrs),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!ioctx->rdma_wrs)\n\t\t\tgoto free_mem;\n\n\t\tioctx->n_rdma_wrs = nrdma;\n\t}\n\n\tdb = ioctx->rbufs;\n\ttsize = cmd->data_length;\n\tdma_len = ib_sg_dma_len(dev, &sg[0]);\n\triu = ioctx->rdma_wrs;\n\n\t/*\n\t * For each remote desc - calculate the #ib_sge.\n\t * If #ib_sge < SRPT_DEF_SG_PER_WQE per rdma operation then\n\t *      each remote desc rdma_iu is required a rdma wr;\n\t * else\n\t *      we need to allocate extra rdma_iu to carry extra #ib_sge in\n\t *      another rdma wr\n\t */\n\tfor (i = 0, j = 0;\n\t     j < count && i < ioctx->n_rbuf && tsize > 0; ++i, ++riu, ++db) {\n\t\trsize = be32_to_cpu(db->len);\n\t\traddr = be64_to_cpu(db->va);\n\t\triu->remote_addr = raddr;\n\t\triu->rkey = be32_to_cpu(db->key);\n\t\triu->wr.num_sge = 0;\n\n\t\t/* calculate how many sge required for this remote_buf */\n\t\twhile (rsize > 0 && tsize > 0) {\n\n\t\t\tif (rsize >= dma_len) {\n\t\t\t\ttsize -= dma_len;\n\t\t\t\trsize -= dma_len;\n\t\t\t\traddr += dma_len;\n\n\t\t\t\tif (tsize > 0) {\n\t\t\t\t\t++j;\n\t\t\t\t\tif (j < count) {\n\t\t\t\t\t\tsg = sg_next(sg);\n\t\t\t\t\t\tdma_len = ib_sg_dma_len(\n\t\t\t\t\t\t\t\tdev, sg);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ttsize -= rsize;\n\t\t\t\tdma_len -= rsize;\n\t\t\t\trsize = 0;\n\t\t\t}\n\n\t\t\t++riu->wr.num_sge;\n\n\t\t\tif (rsize > 0 &&\n\t\t\t    riu->wr.num_sge == SRPT_DEF_SG_PER_WQE) {\n\t\t\t\t++ioctx->n_rdma;\n\t\t\t\triu->wr.sg_list = kmalloc_array(riu->wr.num_sge,\n\t\t\t\t\t\tsizeof(*riu->wr.sg_list),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\t\t\tif (!riu->wr.sg_list)\n\t\t\t\t\tgoto free_mem;\n\n\t\t\t\t++riu;\n\t\t\t\triu->wr.num_sge = 0;\n\t\t\t\triu->remote_addr = raddr;\n\t\t\t\triu->rkey = be32_to_cpu(db->key);\n\t\t\t}\n\t\t}\n\n\t\t++ioctx->n_rdma;\n\t\triu->wr.sg_list = kmalloc_array(riu->wr.num_sge,\n\t\t\t\t\tsizeof(*riu->wr.sg_list),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!riu->wr.sg_list)\n\t\t\tgoto free_mem;\n\t}\n\n\tdb = ioctx->rbufs;\n\ttsize = cmd->data_length;\n\triu = ioctx->rdma_wrs;\n\tsg = sg_orig;\n\tdma_len = ib_sg_dma_len(dev, &sg[0]);\n\tdma_addr = ib_sg_dma_address(dev, &sg[0]);\n\n\t/* this second loop is really mapped sg_addres to rdma_iu->ib_sge */\n\tfor (i = 0, j = 0;\n\t     j < count && i < ioctx->n_rbuf && tsize > 0; ++i, ++riu, ++db) {\n\t\trsize = be32_to_cpu(db->len);\n\t\tsge = riu->wr.sg_list;\n\t\tk = 0;\n\n\t\twhile (rsize > 0 && tsize > 0) {\n\t\t\tsge->addr = dma_addr;\n\t\t\tsge->lkey = ch->sport->sdev->pd->local_dma_lkey;\n\n\t\t\tif (rsize >= dma_len) {\n\t\t\t\tsge->length =\n\t\t\t\t\t(tsize < dma_len) ? tsize : dma_len;\n\t\t\t\ttsize -= dma_len;\n\t\t\t\trsize -= dma_len;\n\n\t\t\t\tif (tsize > 0) {\n\t\t\t\t\t++j;\n\t\t\t\t\tif (j < count) {\n\t\t\t\t\t\tsg = sg_next(sg);\n\t\t\t\t\t\tdma_len = ib_sg_dma_len(\n\t\t\t\t\t\t\t\tdev, sg);\n\t\t\t\t\t\tdma_addr = ib_sg_dma_address(\n\t\t\t\t\t\t\t\tdev, sg);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tsge->length = (tsize < rsize) ? tsize : rsize;\n\t\t\t\ttsize -= rsize;\n\t\t\t\tdma_len -= rsize;\n\t\t\t\tdma_addr += rsize;\n\t\t\t\trsize = 0;\n\t\t\t}\n\n\t\t\t++k;\n\t\t\tif (k == riu->wr.num_sge && rsize > 0 && tsize > 0) {\n\t\t\t\t++riu;\n\t\t\t\tsge = riu->wr.sg_list;\n\t\t\t\tk = 0;\n\t\t\t} else if (rsize > 0 && tsize > 0)\n\t\t\t\t++sge;\n\t\t}\n\t}\n\n\treturn 0;\n\nfree_mem:\n\tsrpt_unmap_sg_to_ib_sge(ch, ioctx);\n\n\treturn -ENOMEM;\n}\n\n/**\n * srpt_get_send_ioctx() - Obtain an I/O context for sending to the initiator.\n */\nstatic struct srpt_send_ioctx *srpt_get_send_ioctx(struct srpt_rdma_ch *ch)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\tunsigned long flags;\n\n\tBUG_ON(!ch);\n\n\tioctx = NULL;\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tif (!list_empty(&ch->free_list)) {\n\t\tioctx = list_first_entry(&ch->free_list,\n\t\t\t\t\t struct srpt_send_ioctx, free_list);\n\t\tlist_del(&ioctx->free_list);\n\t}\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\n\tif (!ioctx)\n\t\treturn ioctx;\n\n\tBUG_ON(ioctx->ch != ch);\n\tspin_lock_init(&ioctx->spinlock);\n\tioctx->state = SRPT_STATE_NEW;\n\tioctx->n_rbuf = 0;\n\tioctx->rbufs = NULL;\n\tioctx->n_rdma = 0;\n\tioctx->n_rdma_wrs = 0;\n\tioctx->rdma_wrs = NULL;\n\tioctx->mapped_sg_count = 0;\n\tinit_completion(&ioctx->tx_done);\n\tioctx->queue_status_only = false;\n\t/*\n\t * transport_init_se_cmd() does not initialize all fields, so do it\n\t * here.\n\t */\n\tmemset(&ioctx->cmd, 0, sizeof(ioctx->cmd));\n\tmemset(&ioctx->sense_data, 0, sizeof(ioctx->sense_data));\n\n\treturn ioctx;\n}\n\n/**\n * srpt_abort_cmd() - Abort a SCSI command.\n * @ioctx:   I/O context associated with the SCSI command.\n * @context: Preferred execution context.\n */\nstatic int srpt_abort_cmd(struct srpt_send_ioctx *ioctx)\n{\n\tenum srpt_command_state state;\n\tunsigned long flags;\n\n\tBUG_ON(!ioctx);\n\n\t/*\n\t * If the command is in a state where the target core is waiting for\n\t * the ib_srpt driver, change the state to the next state. Changing\n\t * the state of the command from SRPT_STATE_NEED_DATA to\n\t * SRPT_STATE_DATA_IN ensures that srpt_xmit_response() will call this\n\t * function a second time.\n\t */\n\n\tspin_lock_irqsave(&ioctx->spinlock, flags);\n\tstate = ioctx->state;\n\tswitch (state) {\n\tcase SRPT_STATE_NEED_DATA:\n\t\tioctx->state = SRPT_STATE_DATA_IN;\n\t\tbreak;\n\tcase SRPT_STATE_DATA_IN:\n\tcase SRPT_STATE_CMD_RSP_SENT:\n\tcase SRPT_STATE_MGMT_RSP_SENT:\n\t\tioctx->state = SRPT_STATE_DONE;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ioctx->spinlock, flags);\n\n\tif (state == SRPT_STATE_DONE) {\n\t\tstruct srpt_rdma_ch *ch = ioctx->ch;\n\n\t\tBUG_ON(ch->sess == NULL);\n\n\t\ttarget_put_sess_cmd(&ioctx->cmd);\n\t\tgoto out;\n\t}\n\n\tpr_debug(\"Aborting cmd with state %d and tag %lld\\n\", state,\n\t\t ioctx->cmd.tag);\n\n\tswitch (state) {\n\tcase SRPT_STATE_NEW:\n\tcase SRPT_STATE_DATA_IN:\n\tcase SRPT_STATE_MGMT:\n\t\t/*\n\t\t * Do nothing - defer abort processing until\n\t\t * srpt_queue_response() is invoked.\n\t\t */\n\t\tWARN_ON(!transport_check_aborted_status(&ioctx->cmd, false));\n\t\tbreak;\n\tcase SRPT_STATE_NEED_DATA:\n\t\t/* DMA_TO_DEVICE (write) - RDMA read error. */\n\n\t\t/* XXX(hch): this is a horrible layering violation.. */\n\t\tspin_lock_irqsave(&ioctx->cmd.t_state_lock, flags);\n\t\tioctx->cmd.transport_state &= ~CMD_T_ACTIVE;\n\t\tspin_unlock_irqrestore(&ioctx->cmd.t_state_lock, flags);\n\t\tbreak;\n\tcase SRPT_STATE_CMD_RSP_SENT:\n\t\t/*\n\t\t * SRP_RSP sending failed or the SRP_RSP send completion has\n\t\t * not been received in time.\n\t\t */\n\t\tsrpt_unmap_sg_to_ib_sge(ioctx->ch, ioctx);\n\t\ttarget_put_sess_cmd(&ioctx->cmd);\n\t\tbreak;\n\tcase SRPT_STATE_MGMT_RSP_SENT:\n\t\tsrpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\n\t\ttarget_put_sess_cmd(&ioctx->cmd);\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"Unexpected command state (%d)\", state);\n\t\tbreak;\n\t}\n\nout:\n\treturn state;\n}\n\n/**\n * XXX: what is now target_execute_cmd used to be asynchronous, and unmapping\n * the data that has been transferred via IB RDMA had to be postponed until the\n * check_stop_free() callback.  None of this is necessary anymore and needs to\n * be cleaned up.\n */\nstatic void srpt_rdma_read_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_rdma_ch *ch = cq->cq_context;\n\tstruct srpt_send_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_send_ioctx, rdma_cqe);\n\n\tWARN_ON(ioctx->n_rdma <= 0);\n\tatomic_add(ioctx->n_rdma, &ch->sq_wr_avail);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tpr_info(\"RDMA_READ for ioctx 0x%p failed with status %d\\n\",\n\t\t\tioctx, wc->status);\n\t\tsrpt_abort_cmd(ioctx);\n\t\treturn;\n\t}\n\n\tif (srpt_test_and_set_cmd_state(ioctx, SRPT_STATE_NEED_DATA,\n\t\t\t\t\tSRPT_STATE_DATA_IN))\n\t\ttarget_execute_cmd(&ioctx->cmd);\n\telse\n\t\tpr_err(\"%s[%d]: wrong state = %d\\n\", __func__,\n\t\t       __LINE__, srpt_get_cmd_state(ioctx));\n}\n\nstatic void srpt_rdma_write_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_send_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_send_ioctx, rdma_cqe);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tpr_info(\"RDMA_WRITE for ioctx 0x%p failed with status %d\\n\",\n\t\t\tioctx, wc->status);\n\t\tsrpt_abort_cmd(ioctx);\n\t}\n}\n\n/**\n * srpt_build_cmd_rsp() - Build an SRP_RSP response.\n * @ch: RDMA channel through which the request has been received.\n * @ioctx: I/O context associated with the SRP_CMD request. The response will\n *   be built in the buffer ioctx->buf points at and hence this function will\n *   overwrite the request data.\n * @tag: tag of the request for which this response is being generated.\n * @status: value for the STATUS field of the SRP_RSP information unit.\n *\n * Returns the size in bytes of the SRP_RSP response.\n *\n * An SRP_RSP response contains a SCSI status or service response. See also\n * section 6.9 in the SRP r16a document for the format of an SRP_RSP\n * response. See also SPC-2 for more information about sense data.\n */\nstatic int srpt_build_cmd_rsp(struct srpt_rdma_ch *ch,\n\t\t\t      struct srpt_send_ioctx *ioctx, u64 tag,\n\t\t\t      int status)\n{\n\tstruct srp_rsp *srp_rsp;\n\tconst u8 *sense_data;\n\tint sense_data_len, max_sense_len;\n\n\t/*\n\t * The lowest bit of all SAM-3 status codes is zero (see also\n\t * paragraph 5.3 in SAM-3).\n\t */\n\tWARN_ON(status & 1);\n\n\tsrp_rsp = ioctx->ioctx.buf;\n\tBUG_ON(!srp_rsp);\n\n\tsense_data = ioctx->sense_data;\n\tsense_data_len = ioctx->cmd.scsi_sense_length;\n\tWARN_ON(sense_data_len > sizeof(ioctx->sense_data));\n\n\tmemset(srp_rsp, 0, sizeof *srp_rsp);\n\tsrp_rsp->opcode = SRP_RSP;\n\tsrp_rsp->req_lim_delta =\n\t\tcpu_to_be32(1 + atomic_xchg(&ch->req_lim_delta, 0));\n\tsrp_rsp->tag = tag;\n\tsrp_rsp->status = status;\n\n\tif (sense_data_len) {\n\t\tBUILD_BUG_ON(MIN_MAX_RSP_SIZE <= sizeof(*srp_rsp));\n\t\tmax_sense_len = ch->max_ti_iu_len - sizeof(*srp_rsp);\n\t\tif (sense_data_len > max_sense_len) {\n\t\t\tpr_warn(\"truncated sense data from %d to %d\"\n\t\t\t\t\" bytes\\n\", sense_data_len, max_sense_len);\n\t\t\tsense_data_len = max_sense_len;\n\t\t}\n\n\t\tsrp_rsp->flags |= SRP_RSP_FLAG_SNSVALID;\n\t\tsrp_rsp->sense_data_len = cpu_to_be32(sense_data_len);\n\t\tmemcpy(srp_rsp + 1, sense_data, sense_data_len);\n\t}\n\n\treturn sizeof(*srp_rsp) + sense_data_len;\n}\n\n/**\n * srpt_build_tskmgmt_rsp() - Build a task management response.\n * @ch:       RDMA channel through which the request has been received.\n * @ioctx:    I/O context in which the SRP_RSP response will be built.\n * @rsp_code: RSP_CODE that will be stored in the response.\n * @tag:      Tag of the request for which this response is being generated.\n *\n * Returns the size in bytes of the SRP_RSP response.\n *\n * An SRP_RSP response contains a SCSI status or service response. See also\n * section 6.9 in the SRP r16a document for the format of an SRP_RSP\n * response.\n */\nstatic int srpt_build_tskmgmt_rsp(struct srpt_rdma_ch *ch,\n\t\t\t\t  struct srpt_send_ioctx *ioctx,\n\t\t\t\t  u8 rsp_code, u64 tag)\n{\n\tstruct srp_rsp *srp_rsp;\n\tint resp_data_len;\n\tint resp_len;\n\n\tresp_data_len = 4;\n\tresp_len = sizeof(*srp_rsp) + resp_data_len;\n\n\tsrp_rsp = ioctx->ioctx.buf;\n\tBUG_ON(!srp_rsp);\n\tmemset(srp_rsp, 0, sizeof *srp_rsp);\n\n\tsrp_rsp->opcode = SRP_RSP;\n\tsrp_rsp->req_lim_delta =\n\t\tcpu_to_be32(1 + atomic_xchg(&ch->req_lim_delta, 0));\n\tsrp_rsp->tag = tag;\n\n\tsrp_rsp->flags |= SRP_RSP_FLAG_RSPVALID;\n\tsrp_rsp->resp_data_len = cpu_to_be32(resp_data_len);\n\tsrp_rsp->data[3] = rsp_code;\n\n\treturn resp_len;\n}\n\n#define NO_SUCH_LUN ((uint64_t)-1LL)\n\n/*\n * SCSI LUN addressing method. See also SAM-2 and the section about\n * eight byte LUNs.\n */\nenum scsi_lun_addr_method {\n\tSCSI_LUN_ADDR_METHOD_PERIPHERAL   = 0,\n\tSCSI_LUN_ADDR_METHOD_FLAT         = 1,\n\tSCSI_LUN_ADDR_METHOD_LUN          = 2,\n\tSCSI_LUN_ADDR_METHOD_EXTENDED_LUN = 3,\n};\n\n/*\n * srpt_unpack_lun() - Convert from network LUN to linear LUN.\n *\n * Convert an 2-byte, 4-byte, 6-byte or 8-byte LUN structure in network byte\n * order (big endian) to a linear LUN. Supports three LUN addressing methods:\n * peripheral, flat and logical unit. See also SAM-2, section 4.9.4 (page 40).\n */\nstatic uint64_t srpt_unpack_lun(const uint8_t *lun, int len)\n{\n\tuint64_t res = NO_SUCH_LUN;\n\tint addressing_method;\n\n\tif (unlikely(len < 2)) {\n\t\tpr_err(\"Illegal LUN length %d, expected 2 bytes or more\\n\",\n\t\t       len);\n\t\tgoto out;\n\t}\n\n\tswitch (len) {\n\tcase 8:\n\t\tif ((*((__be64 *)lun) &\n\t\t     cpu_to_be64(0x0000FFFFFFFFFFFFLL)) != 0)\n\t\t\tgoto out_err;\n\t\tbreak;\n\tcase 4:\n\t\tif (*((__be16 *)&lun[2]) != 0)\n\t\t\tgoto out_err;\n\t\tbreak;\n\tcase 6:\n\t\tif (*((__be32 *)&lun[2]) != 0)\n\t\t\tgoto out_err;\n\t\tbreak;\n\tcase 2:\n\t\tbreak;\n\tdefault:\n\t\tgoto out_err;\n\t}\n\n\taddressing_method = (*lun) >> 6; /* highest two bits of byte 0 */\n\tswitch (addressing_method) {\n\tcase SCSI_LUN_ADDR_METHOD_PERIPHERAL:\n\tcase SCSI_LUN_ADDR_METHOD_FLAT:\n\tcase SCSI_LUN_ADDR_METHOD_LUN:\n\t\tres = *(lun + 1) | (((*lun) & 0x3f) << 8);\n\t\tbreak;\n\n\tcase SCSI_LUN_ADDR_METHOD_EXTENDED_LUN:\n\tdefault:\n\t\tpr_err(\"Unimplemented LUN addressing method %u\\n\",\n\t\t       addressing_method);\n\t\tbreak;\n\t}\n\nout:\n\treturn res;\n\nout_err:\n\tpr_err(\"Support for multi-level LUNs has not yet been implemented\\n\");\n\tgoto out;\n}\n\nstatic int srpt_check_stop_free(struct se_cmd *cmd)\n{\n\tstruct srpt_send_ioctx *ioctx = container_of(cmd,\n\t\t\t\tstruct srpt_send_ioctx, cmd);\n\n\treturn target_put_sess_cmd(&ioctx->cmd);\n}\n\n/**\n * srpt_handle_cmd() - Process SRP_CMD.\n */\nstatic int srpt_handle_cmd(struct srpt_rdma_ch *ch,\n\t\t\t   struct srpt_recv_ioctx *recv_ioctx,\n\t\t\t   struct srpt_send_ioctx *send_ioctx)\n{\n\tstruct se_cmd *cmd;\n\tstruct srp_cmd *srp_cmd;\n\tuint64_t unpacked_lun;\n\tu64 data_len;\n\tenum dma_data_direction dir;\n\tsense_reason_t ret;\n\tint rc;\n\n\tBUG_ON(!send_ioctx);\n\n\tsrp_cmd = recv_ioctx->ioctx.buf;\n\tcmd = &send_ioctx->cmd;\n\tcmd->tag = srp_cmd->tag;\n\n\tswitch (srp_cmd->task_attr) {\n\tcase SRP_CMD_SIMPLE_Q:\n\t\tcmd->sam_task_attr = TCM_SIMPLE_TAG;\n\t\tbreak;\n\tcase SRP_CMD_ORDERED_Q:\n\tdefault:\n\t\tcmd->sam_task_attr = TCM_ORDERED_TAG;\n\t\tbreak;\n\tcase SRP_CMD_HEAD_OF_Q:\n\t\tcmd->sam_task_attr = TCM_HEAD_TAG;\n\t\tbreak;\n\tcase SRP_CMD_ACA:\n\t\tcmd->sam_task_attr = TCM_ACA_TAG;\n\t\tbreak;\n\t}\n\n\tif (srpt_get_desc_tbl(send_ioctx, srp_cmd, &dir, &data_len)) {\n\t\tpr_err(\"0x%llx: parsing SRP descriptor table failed.\\n\",\n\t\t       srp_cmd->tag);\n\t\tret = TCM_INVALID_CDB_FIELD;\n\t\tgoto send_sense;\n\t}\n\n\tunpacked_lun = srpt_unpack_lun((uint8_t *)&srp_cmd->lun,\n\t\t\t\t       sizeof(srp_cmd->lun));\n\trc = target_submit_cmd(cmd, ch->sess, srp_cmd->cdb,\n\t\t\t&send_ioctx->sense_data[0], unpacked_lun, data_len,\n\t\t\tTCM_SIMPLE_TAG, dir, TARGET_SCF_ACK_KREF);\n\tif (rc != 0) {\n\t\tret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\t\tgoto send_sense;\n\t}\n\treturn 0;\n\nsend_sense:\n\ttransport_send_check_condition_and_sense(cmd, ret, 0);\n\treturn -1;\n}\n\nstatic int srp_tmr_to_tcm(int fn)\n{\n\tswitch (fn) {\n\tcase SRP_TSK_ABORT_TASK:\n\t\treturn TMR_ABORT_TASK;\n\tcase SRP_TSK_ABORT_TASK_SET:\n\t\treturn TMR_ABORT_TASK_SET;\n\tcase SRP_TSK_CLEAR_TASK_SET:\n\t\treturn TMR_CLEAR_TASK_SET;\n\tcase SRP_TSK_LUN_RESET:\n\t\treturn TMR_LUN_RESET;\n\tcase SRP_TSK_CLEAR_ACA:\n\t\treturn TMR_CLEAR_ACA;\n\tdefault:\n\t\treturn -1;\n\t}\n}\n\n/**\n * srpt_handle_tsk_mgmt() - Process an SRP_TSK_MGMT information unit.\n *\n * Returns 0 if and only if the request will be processed by the target core.\n *\n * For more information about SRP_TSK_MGMT information units, see also section\n * 6.7 in the SRP r16a document.\n */\nstatic void srpt_handle_tsk_mgmt(struct srpt_rdma_ch *ch,\n\t\t\t\t struct srpt_recv_ioctx *recv_ioctx,\n\t\t\t\t struct srpt_send_ioctx *send_ioctx)\n{\n\tstruct srp_tsk_mgmt *srp_tsk;\n\tstruct se_cmd *cmd;\n\tstruct se_session *sess = ch->sess;\n\tuint64_t unpacked_lun;\n\tint tcm_tmr;\n\tint rc;\n\n\tBUG_ON(!send_ioctx);\n\n\tsrp_tsk = recv_ioctx->ioctx.buf;\n\tcmd = &send_ioctx->cmd;\n\n\tpr_debug(\"recv tsk_mgmt fn %d for task_tag %lld and cmd tag %lld\"\n\t\t \" cm_id %p sess %p\\n\", srp_tsk->tsk_mgmt_func,\n\t\t srp_tsk->task_tag, srp_tsk->tag, ch->cm_id, ch->sess);\n\n\tsrpt_set_cmd_state(send_ioctx, SRPT_STATE_MGMT);\n\tsend_ioctx->cmd.tag = srp_tsk->tag;\n\ttcm_tmr = srp_tmr_to_tcm(srp_tsk->tsk_mgmt_func);\n\tunpacked_lun = srpt_unpack_lun((uint8_t *)&srp_tsk->lun,\n\t\t\t\t       sizeof(srp_tsk->lun));\n\trc = target_submit_tmr(&send_ioctx->cmd, sess, NULL, unpacked_lun,\n\t\t\t\tsrp_tsk, tcm_tmr, GFP_KERNEL, srp_tsk->task_tag,\n\t\t\t\tTARGET_SCF_ACK_KREF);\n\tif (rc != 0) {\n\t\tsend_ioctx->cmd.se_tmr_req->response = TMR_FUNCTION_REJECTED;\n\t\tgoto fail;\n\t}\n\treturn;\nfail:\n\ttransport_send_check_condition_and_sense(cmd, 0, 0); // XXX:\n}\n\n/**\n * srpt_handle_new_iu() - Process a newly received information unit.\n * @ch:    RDMA channel through which the information unit has been received.\n * @ioctx: SRPT I/O context associated with the information unit.\n */\nstatic void srpt_handle_new_iu(struct srpt_rdma_ch *ch,\n\t\t\t       struct srpt_recv_ioctx *recv_ioctx,\n\t\t\t       struct srpt_send_ioctx *send_ioctx)\n{\n\tstruct srp_cmd *srp_cmd;\n\tenum rdma_ch_state ch_state;\n\n\tBUG_ON(!ch);\n\tBUG_ON(!recv_ioctx);\n\n\tib_dma_sync_single_for_cpu(ch->sport->sdev->device,\n\t\t\t\t   recv_ioctx->ioctx.dma, srp_max_req_size,\n\t\t\t\t   DMA_FROM_DEVICE);\n\n\tch_state = srpt_get_ch_state(ch);\n\tif (unlikely(ch_state == CH_CONNECTING)) {\n\t\tlist_add_tail(&recv_ioctx->wait_list, &ch->cmd_wait_list);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(ch_state != CH_LIVE))\n\t\tgoto out;\n\n\tsrp_cmd = recv_ioctx->ioctx.buf;\n\tif (srp_cmd->opcode == SRP_CMD || srp_cmd->opcode == SRP_TSK_MGMT) {\n\t\tif (!send_ioctx)\n\t\t\tsend_ioctx = srpt_get_send_ioctx(ch);\n\t\tif (unlikely(!send_ioctx)) {\n\t\t\tlist_add_tail(&recv_ioctx->wait_list,\n\t\t\t\t      &ch->cmd_wait_list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tswitch (srp_cmd->opcode) {\n\tcase SRP_CMD:\n\t\tsrpt_handle_cmd(ch, recv_ioctx, send_ioctx);\n\t\tbreak;\n\tcase SRP_TSK_MGMT:\n\t\tsrpt_handle_tsk_mgmt(ch, recv_ioctx, send_ioctx);\n\t\tbreak;\n\tcase SRP_I_LOGOUT:\n\t\tpr_err(\"Not yet implemented: SRP_I_LOGOUT\\n\");\n\t\tbreak;\n\tcase SRP_CRED_RSP:\n\t\tpr_debug(\"received SRP_CRED_RSP\\n\");\n\t\tbreak;\n\tcase SRP_AER_RSP:\n\t\tpr_debug(\"received SRP_AER_RSP\\n\");\n\t\tbreak;\n\tcase SRP_RSP:\n\t\tpr_err(\"Received SRP_RSP\\n\");\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received IU with unknown opcode 0x%x\\n\",\n\t\t       srp_cmd->opcode);\n\t\tbreak;\n\t}\n\n\tsrpt_post_recv(ch->sport->sdev, recv_ioctx);\nout:\n\treturn;\n}\n\nstatic void srpt_recv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_rdma_ch *ch = cq->cq_context;\n\tstruct srpt_recv_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_recv_ioctx, ioctx.cqe);\n\n\tif (wc->status == IB_WC_SUCCESS) {\n\t\tint req_lim;\n\n\t\treq_lim = atomic_dec_return(&ch->req_lim);\n\t\tif (unlikely(req_lim < 0))\n\t\t\tpr_err(\"req_lim = %d < 0\\n\", req_lim);\n\t\tsrpt_handle_new_iu(ch, ioctx, NULL);\n\t} else {\n\t\tpr_info(\"receiving failed for ioctx %p with status %d\\n\",\n\t\t\tioctx, wc->status);\n\t}\n}\n\n/**\n * Note: Although this has not yet been observed during tests, at least in\n * theory it is possible that the srpt_get_send_ioctx() call invoked by\n * srpt_handle_new_iu() fails. This is possible because the req_lim_delta\n * value in each response is set to one, and it is possible that this response\n * makes the initiator send a new request before the send completion for that\n * response has been processed. This could e.g. happen if the call to\n * srpt_put_send_iotcx() is delayed because of a higher priority interrupt or\n * if IB retransmission causes generation of the send completion to be\n * delayed. Incoming information units for which srpt_get_send_ioctx() fails\n * are queued on cmd_wait_list. The code below processes these delayed\n * requests one at a time.\n */\nstatic void srpt_send_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct srpt_rdma_ch *ch = cq->cq_context;\n\tstruct srpt_send_ioctx *ioctx =\n\t\tcontainer_of(wc->wr_cqe, struct srpt_send_ioctx, ioctx.cqe);\n\tenum srpt_command_state state;\n\n\tstate = srpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\n\n\tWARN_ON(state != SRPT_STATE_CMD_RSP_SENT &&\n\t\tstate != SRPT_STATE_MGMT_RSP_SENT);\n\n\tatomic_inc(&ch->sq_wr_avail);\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\tpr_info(\"sending response for ioctx 0x%p failed\"\n\t\t\t\" with status %d\\n\", ioctx, wc->status);\n\n\t\tatomic_dec(&ch->req_lim);\n\t\tsrpt_abort_cmd(ioctx);\n\t\tgoto out;\n\t}\n\n\tif (state != SRPT_STATE_DONE) {\n\t\tsrpt_unmap_sg_to_ib_sge(ch, ioctx);\n\t\ttransport_generic_free_cmd(&ioctx->cmd, 0);\n\t} else {\n\t\tpr_err(\"IB completion has been received too late for\"\n\t\t       \" wr_id = %u.\\n\", ioctx->ioctx.index);\n\t}\n\nout:\n\twhile (!list_empty(&ch->cmd_wait_list) &&\n\t       srpt_get_ch_state(ch) == CH_LIVE &&\n\t       (ioctx = srpt_get_send_ioctx(ch)) != NULL) {\n\t\tstruct srpt_recv_ioctx *recv_ioctx;\n\n\t\trecv_ioctx = list_first_entry(&ch->cmd_wait_list,\n\t\t\t\t\t      struct srpt_recv_ioctx,\n\t\t\t\t\t      wait_list);\n\t\tlist_del(&recv_ioctx->wait_list);\n\t\tsrpt_handle_new_iu(ch, recv_ioctx, ioctx);\n\t}\n}\n\n/**\n * srpt_create_ch_ib() - Create receive and send completion queues.\n */\nstatic int srpt_create_ch_ib(struct srpt_rdma_ch *ch)\n{\n\tstruct ib_qp_init_attr *qp_init;\n\tstruct srpt_port *sport = ch->sport;\n\tstruct srpt_device *sdev = sport->sdev;\n\tu32 srp_sq_size = sport->port_attrib.srp_sq_size;\n\tint ret;\n\n\tWARN_ON(ch->rq_size < 1);\n\n\tret = -ENOMEM;\n\tqp_init = kzalloc(sizeof *qp_init, GFP_KERNEL);\n\tif (!qp_init)\n\t\tgoto out;\n\nretry:\n\tch->cq = ib_alloc_cq(sdev->device, ch, ch->rq_size + srp_sq_size,\n\t\t\t0 /* XXX: spread CQs */, IB_POLL_WORKQUEUE);\n\tif (IS_ERR(ch->cq)) {\n\t\tret = PTR_ERR(ch->cq);\n\t\tpr_err(\"failed to create CQ cqe= %d ret= %d\\n\",\n\t\t       ch->rq_size + srp_sq_size, ret);\n\t\tgoto out;\n\t}\n\n\tqp_init->qp_context = (void *)ch;\n\tqp_init->event_handler\n\t\t= (void(*)(struct ib_event *, void*))srpt_qp_event;\n\tqp_init->send_cq = ch->cq;\n\tqp_init->recv_cq = ch->cq;\n\tqp_init->srq = sdev->srq;\n\tqp_init->sq_sig_type = IB_SIGNAL_REQ_WR;\n\tqp_init->qp_type = IB_QPT_RC;\n\tqp_init->cap.max_send_wr = srp_sq_size;\n\tqp_init->cap.max_send_sge = SRPT_DEF_SG_PER_WQE;\n\n\tch->qp = ib_create_qp(sdev->pd, qp_init);\n\tif (IS_ERR(ch->qp)) {\n\t\tret = PTR_ERR(ch->qp);\n\t\tif (ret == -ENOMEM) {\n\t\t\tsrp_sq_size /= 2;\n\t\t\tif (srp_sq_size >= MIN_SRPT_SQ_SIZE) {\n\t\t\t\tib_destroy_cq(ch->cq);\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t}\n\t\tpr_err(\"failed to create_qp ret= %d\\n\", ret);\n\t\tgoto err_destroy_cq;\n\t}\n\n\tatomic_set(&ch->sq_wr_avail, qp_init->cap.max_send_wr);\n\n\tpr_debug(\"%s: max_cqe= %d max_sge= %d sq_size = %d cm_id= %p\\n\",\n\t\t __func__, ch->cq->cqe, qp_init->cap.max_send_sge,\n\t\t qp_init->cap.max_send_wr, ch->cm_id);\n\n\tret = srpt_init_ch_qp(ch, ch->qp);\n\tif (ret)\n\t\tgoto err_destroy_qp;\n\nout:\n\tkfree(qp_init);\n\treturn ret;\n\nerr_destroy_qp:\n\tib_destroy_qp(ch->qp);\nerr_destroy_cq:\n\tib_free_cq(ch->cq);\n\tgoto out;\n}\n\nstatic void srpt_destroy_ch_ib(struct srpt_rdma_ch *ch)\n{\n\tib_destroy_qp(ch->qp);\n\tib_free_cq(ch->cq);\n}\n\n/**\n * __srpt_close_ch() - Close an RDMA channel by setting the QP error state.\n *\n * Reset the QP and make sure all resources associated with the channel will\n * be deallocated at an appropriate time.\n *\n * Note: The caller must hold ch->sport->sdev->spinlock.\n */\nstatic void __srpt_close_ch(struct srpt_rdma_ch *ch)\n{\n\tenum rdma_ch_state prev_state;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tprev_state = ch->state;\n\tswitch (prev_state) {\n\tcase CH_CONNECTING:\n\tcase CH_LIVE:\n\t\tch->state = CH_DISCONNECTING;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\n\tswitch (prev_state) {\n\tcase CH_CONNECTING:\n\t\tib_send_cm_rej(ch->cm_id, IB_CM_REJ_NO_RESOURCES, NULL, 0,\n\t\t\t       NULL, 0);\n\t\t/* fall through */\n\tcase CH_LIVE:\n\t\tif (ib_send_cm_dreq(ch->cm_id, NULL, 0) < 0)\n\t\t\tpr_err(\"sending CM DREQ failed.\\n\");\n\t\tbreak;\n\tcase CH_DISCONNECTING:\n\t\tbreak;\n\tcase CH_DRAINING:\n\tcase CH_RELEASING:\n\t\tbreak;\n\t}\n}\n\n/**\n * srpt_close_ch() - Close an RDMA channel.\n */\nstatic void srpt_close_ch(struct srpt_rdma_ch *ch)\n{\n\tstruct srpt_device *sdev;\n\n\tsdev = ch->sport->sdev;\n\tspin_lock_irq(&sdev->spinlock);\n\t__srpt_close_ch(ch);\n\tspin_unlock_irq(&sdev->spinlock);\n}\n\n/**\n * srpt_shutdown_session() - Whether or not a session may be shut down.\n */\nstatic int srpt_shutdown_session(struct se_session *se_sess)\n{\n\tstruct srpt_rdma_ch *ch = se_sess->fabric_sess_ptr;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tif (ch->in_shutdown) {\n\t\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\t\treturn true;\n\t}\n\n\tch->in_shutdown = true;\n\ttarget_sess_cmd_list_set_waiting(se_sess);\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\n\treturn true;\n}\n\n/**\n * srpt_drain_channel() - Drain a channel by resetting the IB queue pair.\n * @cm_id: Pointer to the CM ID of the channel to be drained.\n *\n * Note: Must be called from inside srpt_cm_handler to avoid a race between\n * accessing sdev->spinlock and the call to kfree(sdev) in srpt_remove_one()\n * (the caller of srpt_cm_handler holds the cm_id spinlock; srpt_remove_one()\n * waits until all target sessions for the associated IB device have been\n * unregistered and target session registration involves a call to\n * ib_destroy_cm_id(), which locks the cm_id spinlock and hence waits until\n * this function has finished).\n */\nstatic void srpt_drain_channel(struct ib_cm_id *cm_id)\n{\n\tstruct srpt_device *sdev;\n\tstruct srpt_rdma_ch *ch;\n\tint ret;\n\tbool do_reset = false;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\n\tsdev = cm_id->context;\n\tBUG_ON(!sdev);\n\tspin_lock_irq(&sdev->spinlock);\n\tlist_for_each_entry(ch, &sdev->rch_list, list) {\n\t\tif (ch->cm_id == cm_id) {\n\t\t\tdo_reset = srpt_test_and_set_ch_state(ch,\n\t\t\t\t\tCH_CONNECTING, CH_DRAINING) ||\n\t\t\t\t   srpt_test_and_set_ch_state(ch,\n\t\t\t\t\tCH_LIVE, CH_DRAINING) ||\n\t\t\t\t   srpt_test_and_set_ch_state(ch,\n\t\t\t\t\tCH_DISCONNECTING, CH_DRAINING);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&sdev->spinlock);\n\n\tif (do_reset) {\n\t\tif (ch->sess)\n\t\t\tsrpt_shutdown_session(ch->sess);\n\n\t\tret = srpt_ch_qp_err(ch);\n\t\tif (ret < 0)\n\t\t\tpr_err(\"Setting queue pair in error state\"\n\t\t\t       \" failed: %d\\n\", ret);\n\t}\n}\n\n/**\n * srpt_find_channel() - Look up an RDMA channel.\n * @cm_id: Pointer to the CM ID of the channel to be looked up.\n *\n * Return NULL if no matching RDMA channel has been found.\n */\nstatic struct srpt_rdma_ch *srpt_find_channel(struct srpt_device *sdev,\n\t\t\t\t\t      struct ib_cm_id *cm_id)\n{\n\tstruct srpt_rdma_ch *ch;\n\tbool found;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\tBUG_ON(!sdev);\n\n\tfound = false;\n\tspin_lock_irq(&sdev->spinlock);\n\tlist_for_each_entry(ch, &sdev->rch_list, list) {\n\t\tif (ch->cm_id == cm_id) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&sdev->spinlock);\n\n\treturn found ? ch : NULL;\n}\n\n/**\n * srpt_release_channel() - Release channel resources.\n *\n * Schedules the actual release because:\n * - Calling the ib_destroy_cm_id() call from inside an IB CM callback would\n *   trigger a deadlock.\n * - It is not safe to call TCM transport_* functions from interrupt context.\n */\nstatic void srpt_release_channel(struct srpt_rdma_ch *ch)\n{\n\tschedule_work(&ch->release_work);\n}\n\nstatic void srpt_release_channel_work(struct work_struct *w)\n{\n\tstruct srpt_rdma_ch *ch;\n\tstruct srpt_device *sdev;\n\tstruct se_session *se_sess;\n\n\tch = container_of(w, struct srpt_rdma_ch, release_work);\n\tpr_debug(\"ch = %p; ch->sess = %p; release_done = %p\\n\", ch, ch->sess,\n\t\t ch->release_done);\n\n\tsdev = ch->sport->sdev;\n\tBUG_ON(!sdev);\n\n\tse_sess = ch->sess;\n\tBUG_ON(!se_sess);\n\n\ttarget_wait_for_sess_cmds(se_sess);\n\n\ttransport_deregister_session_configfs(se_sess);\n\ttransport_deregister_session(se_sess);\n\tch->sess = NULL;\n\n\tib_destroy_cm_id(ch->cm_id);\n\n\tsrpt_destroy_ch_ib(ch);\n\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)ch->ioctx_ring,\n\t\t\t     ch->sport->sdev, ch->rq_size,\n\t\t\t     ch->rsp_size, DMA_TO_DEVICE);\n\n\tspin_lock_irq(&sdev->spinlock);\n\tlist_del(&ch->list);\n\tspin_unlock_irq(&sdev->spinlock);\n\n\tif (ch->release_done)\n\t\tcomplete(ch->release_done);\n\n\twake_up(&sdev->ch_releaseQ);\n\n\tkfree(ch);\n}\n\n/**\n * srpt_cm_req_recv() - Process the event IB_CM_REQ_RECEIVED.\n *\n * Ownership of the cm_id is transferred to the target session if this\n * functions returns zero. Otherwise the caller remains the owner of cm_id.\n */\nstatic int srpt_cm_req_recv(struct ib_cm_id *cm_id,\n\t\t\t    struct ib_cm_req_event_param *param,\n\t\t\t    void *private_data)\n{\n\tstruct srpt_device *sdev = cm_id->context;\n\tstruct srpt_port *sport = &sdev->port[param->port - 1];\n\tstruct srp_login_req *req;\n\tstruct srp_login_rsp *rsp;\n\tstruct srp_login_rej *rej;\n\tstruct ib_cm_rep_param *rep_param;\n\tstruct srpt_rdma_ch *ch, *tmp_ch;\n\tstruct se_node_acl *se_acl;\n\tu32 it_iu_len;\n\tint i, ret = 0;\n\tunsigned char *p;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\n\tif (WARN_ON(!sdev || !private_data))\n\t\treturn -EINVAL;\n\n\treq = (struct srp_login_req *)private_data;\n\n\tit_iu_len = be32_to_cpu(req->req_it_iu_len);\n\n\tpr_info(\"Received SRP_LOGIN_REQ with i_port_id 0x%llx:0x%llx,\"\n\t\t\" t_port_id 0x%llx:0x%llx and it_iu_len %d on port %d\"\n\t\t\" (guid=0x%llx:0x%llx)\\n\",\n\t\tbe64_to_cpu(*(__be64 *)&req->initiator_port_id[0]),\n\t\tbe64_to_cpu(*(__be64 *)&req->initiator_port_id[8]),\n\t\tbe64_to_cpu(*(__be64 *)&req->target_port_id[0]),\n\t\tbe64_to_cpu(*(__be64 *)&req->target_port_id[8]),\n\t\tit_iu_len,\n\t\tparam->port,\n\t\tbe64_to_cpu(*(__be64 *)&sdev->port[param->port - 1].gid.raw[0]),\n\t\tbe64_to_cpu(*(__be64 *)&sdev->port[param->port - 1].gid.raw[8]));\n\n\trsp = kzalloc(sizeof *rsp, GFP_KERNEL);\n\trej = kzalloc(sizeof *rej, GFP_KERNEL);\n\trep_param = kzalloc(sizeof *rep_param, GFP_KERNEL);\n\n\tif (!rsp || !rej || !rep_param) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (it_iu_len > srp_max_req_size || it_iu_len < 64) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t      SRP_LOGIN_REJ_REQ_IT_IU_LENGTH_TOO_LARGE);\n\t\tret = -EINVAL;\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because its\"\n\t\t       \" length (%d bytes) is out of range (%d .. %d)\\n\",\n\t\t       it_iu_len, 64, srp_max_req_size);\n\t\tgoto reject;\n\t}\n\n\tif (!sport->enabled) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t      SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tret = -EINVAL;\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because the target port\"\n\t\t       \" has not yet been enabled\\n\");\n\t\tgoto reject;\n\t}\n\n\tif ((req->req_flags & SRP_MTCH_ACTION) == SRP_MULTICHAN_SINGLE) {\n\t\trsp->rsp_flags = SRP_LOGIN_RSP_MULTICHAN_NO_CHAN;\n\n\t\tspin_lock_irq(&sdev->spinlock);\n\n\t\tlist_for_each_entry_safe(ch, tmp_ch, &sdev->rch_list, list) {\n\t\t\tif (!memcmp(ch->i_port_id, req->initiator_port_id, 16)\n\t\t\t    && !memcmp(ch->t_port_id, req->target_port_id, 16)\n\t\t\t    && param->port == ch->sport->port\n\t\t\t    && param->listen_id == ch->sport->sdev->cm_id\n\t\t\t    && ch->cm_id) {\n\t\t\t\tenum rdma_ch_state ch_state;\n\n\t\t\t\tch_state = srpt_get_ch_state(ch);\n\t\t\t\tif (ch_state != CH_CONNECTING\n\t\t\t\t    && ch_state != CH_LIVE)\n\t\t\t\t\tcontinue;\n\n\t\t\t\t/* found an existing channel */\n\t\t\t\tpr_debug(\"Found existing channel %s\"\n\t\t\t\t\t \" cm_id= %p state= %d\\n\",\n\t\t\t\t\t ch->sess_name, ch->cm_id, ch_state);\n\n\t\t\t\t__srpt_close_ch(ch);\n\n\t\t\t\trsp->rsp_flags =\n\t\t\t\t\tSRP_LOGIN_RSP_MULTICHAN_TERMINATED;\n\t\t\t}\n\t\t}\n\n\t\tspin_unlock_irq(&sdev->spinlock);\n\n\t} else\n\t\trsp->rsp_flags = SRP_LOGIN_RSP_MULTICHAN_MAINTAINED;\n\n\tif (*(__be64 *)req->target_port_id != cpu_to_be64(srpt_service_guid)\n\t    || *(__be64 *)(req->target_port_id + 8) !=\n\t       cpu_to_be64(srpt_service_guid)) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t      SRP_LOGIN_REJ_UNABLE_ASSOCIATE_CHANNEL);\n\t\tret = -ENOMEM;\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because it\"\n\t\t       \" has an invalid target port identifier.\\n\");\n\t\tgoto reject;\n\t}\n\n\tch = kzalloc(sizeof *ch, GFP_KERNEL);\n\tif (!ch) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t      SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because no memory.\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto reject;\n\t}\n\n\tINIT_WORK(&ch->release_work, srpt_release_channel_work);\n\tmemcpy(ch->i_port_id, req->initiator_port_id, 16);\n\tmemcpy(ch->t_port_id, req->target_port_id, 16);\n\tch->sport = &sdev->port[param->port - 1];\n\tch->cm_id = cm_id;\n\t/*\n\t * Avoid QUEUE_FULL conditions by limiting the number of buffers used\n\t * for the SRP protocol to the command queue size.\n\t */\n\tch->rq_size = SRPT_RQ_SIZE;\n\tspin_lock_init(&ch->spinlock);\n\tch->state = CH_CONNECTING;\n\tINIT_LIST_HEAD(&ch->cmd_wait_list);\n\tch->rsp_size = ch->sport->port_attrib.srp_max_rsp_size;\n\n\tch->ioctx_ring = (struct srpt_send_ioctx **)\n\t\tsrpt_alloc_ioctx_ring(ch->sport->sdev, ch->rq_size,\n\t\t\t\t      sizeof(*ch->ioctx_ring[0]),\n\t\t\t\t      ch->rsp_size, DMA_TO_DEVICE);\n\tif (!ch->ioctx_ring)\n\t\tgoto free_ch;\n\n\tINIT_LIST_HEAD(&ch->free_list);\n\tfor (i = 0; i < ch->rq_size; i++) {\n\t\tch->ioctx_ring[i]->ch = ch;\n\t\tlist_add_tail(&ch->ioctx_ring[i]->free_list, &ch->free_list);\n\t}\n\n\tret = srpt_create_ch_ib(ch);\n\tif (ret) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t      SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because creating\"\n\t\t       \" a new RDMA channel failed.\\n\");\n\t\tgoto free_ring;\n\t}\n\n\tret = srpt_ch_qp_rtr(ch, ch->qp);\n\tif (ret) {\n\t\trej->reason = cpu_to_be32(SRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_err(\"rejected SRP_LOGIN_REQ because enabling\"\n\t\t       \" RTR failed (error code = %d)\\n\", ret);\n\t\tgoto destroy_ib;\n\t}\n\n\t/*\n\t * Use the initator port identifier as the session name, when\n\t * checking against se_node_acl->initiatorname[] this can be\n\t * with or without preceeding '0x'.\n\t */\n\tsnprintf(ch->sess_name, sizeof(ch->sess_name), \"0x%016llx%016llx\",\n\t\t\tbe64_to_cpu(*(__be64 *)ch->i_port_id),\n\t\t\tbe64_to_cpu(*(__be64 *)(ch->i_port_id + 8)));\n\n\tpr_debug(\"registering session %s\\n\", ch->sess_name);\n\tp = &ch->sess_name[0];\n\n\tch->sess = transport_init_session(TARGET_PROT_NORMAL);\n\tif (IS_ERR(ch->sess)) {\n\t\trej->reason = cpu_to_be32(\n\t\t\t\tSRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\n\t\tpr_debug(\"Failed to create session\\n\");\n\t\tgoto destroy_ib;\n\t}\n\ntry_again:\n\tse_acl = core_tpg_get_initiator_node_acl(&sport->port_tpg_1, p);\n\tif (!se_acl) {\n\t\tpr_info(\"Rejected login because no ACL has been\"\n\t\t\t\" configured yet for initiator %s.\\n\", ch->sess_name);\n\t\t/*\n\t\t * XXX: Hack to retry of ch->i_port_id without leading '0x'\n\t\t */\n\t\tif (p == &ch->sess_name[0]) {\n\t\t\tp += 2;\n\t\t\tgoto try_again;\n\t\t}\n\t\trej->reason = cpu_to_be32(\n\t\t\t\tSRP_LOGIN_REJ_CHANNEL_LIMIT_REACHED);\n\t\ttransport_free_session(ch->sess);\n\t\tgoto destroy_ib;\n\t}\n\tch->sess->se_node_acl = se_acl;\n\n\ttransport_register_session(&sport->port_tpg_1, se_acl, ch->sess, ch);\n\n\tpr_debug(\"Establish connection sess=%p name=%s cm_id=%p\\n\", ch->sess,\n\t\t ch->sess_name, ch->cm_id);\n\n\t/* create srp_login_response */\n\trsp->opcode = SRP_LOGIN_RSP;\n\trsp->tag = req->tag;\n\trsp->max_it_iu_len = req->req_it_iu_len;\n\trsp->max_ti_iu_len = req->req_it_iu_len;\n\tch->max_ti_iu_len = it_iu_len;\n\trsp->buf_fmt = cpu_to_be16(SRP_BUF_FORMAT_DIRECT\n\t\t\t\t   | SRP_BUF_FORMAT_INDIRECT);\n\trsp->req_lim_delta = cpu_to_be32(ch->rq_size);\n\tatomic_set(&ch->req_lim, ch->rq_size);\n\tatomic_set(&ch->req_lim_delta, 0);\n\n\t/* create cm reply */\n\trep_param->qp_num = ch->qp->qp_num;\n\trep_param->private_data = (void *)rsp;\n\trep_param->private_data_len = sizeof *rsp;\n\trep_param->rnr_retry_count = 7;\n\trep_param->flow_control = 1;\n\trep_param->failover_accepted = 0;\n\trep_param->srq = 1;\n\trep_param->responder_resources = 4;\n\trep_param->initiator_depth = 4;\n\n\tret = ib_send_cm_rep(cm_id, rep_param);\n\tif (ret) {\n\t\tpr_err(\"sending SRP_LOGIN_REQ response failed\"\n\t\t       \" (error code = %d)\\n\", ret);\n\t\tgoto release_channel;\n\t}\n\n\tspin_lock_irq(&sdev->spinlock);\n\tlist_add_tail(&ch->list, &sdev->rch_list);\n\tspin_unlock_irq(&sdev->spinlock);\n\n\tgoto out;\n\nrelease_channel:\n\tsrpt_set_ch_state(ch, CH_RELEASING);\n\ttransport_deregister_session_configfs(ch->sess);\n\ttransport_deregister_session(ch->sess);\n\tch->sess = NULL;\n\ndestroy_ib:\n\tsrpt_destroy_ch_ib(ch);\n\nfree_ring:\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)ch->ioctx_ring,\n\t\t\t     ch->sport->sdev, ch->rq_size,\n\t\t\t     ch->rsp_size, DMA_TO_DEVICE);\nfree_ch:\n\tkfree(ch);\n\nreject:\n\trej->opcode = SRP_LOGIN_REJ;\n\trej->tag = req->tag;\n\trej->buf_fmt = cpu_to_be16(SRP_BUF_FORMAT_DIRECT\n\t\t\t\t   | SRP_BUF_FORMAT_INDIRECT);\n\n\tib_send_cm_rej(cm_id, IB_CM_REJ_CONSUMER_DEFINED, NULL, 0,\n\t\t\t     (void *)rej, sizeof *rej);\n\nout:\n\tkfree(rep_param);\n\tkfree(rsp);\n\tkfree(rej);\n\n\treturn ret;\n}\n\nstatic void srpt_cm_rej_recv(struct ib_cm_id *cm_id)\n{\n\tpr_info(\"Received IB REJ for cm_id %p.\\n\", cm_id);\n\tsrpt_drain_channel(cm_id);\n}\n\n/**\n * srpt_cm_rtu_recv() - Process an IB_CM_RTU_RECEIVED or USER_ESTABLISHED event.\n *\n * An IB_CM_RTU_RECEIVED message indicates that the connection is established\n * and that the recipient may begin transmitting (RTU = ready to use).\n */\nstatic void srpt_cm_rtu_recv(struct ib_cm_id *cm_id)\n{\n\tstruct srpt_rdma_ch *ch;\n\tint ret;\n\n\tch = srpt_find_channel(cm_id->context, cm_id);\n\tBUG_ON(!ch);\n\n\tif (srpt_test_and_set_ch_state(ch, CH_CONNECTING, CH_LIVE)) {\n\t\tstruct srpt_recv_ioctx *ioctx, *ioctx_tmp;\n\n\t\tret = srpt_ch_qp_rts(ch, ch->qp);\n\n\t\tlist_for_each_entry_safe(ioctx, ioctx_tmp, &ch->cmd_wait_list,\n\t\t\t\t\t wait_list) {\n\t\t\tlist_del(&ioctx->wait_list);\n\t\t\tsrpt_handle_new_iu(ch, ioctx, NULL);\n\t\t}\n\t\tif (ret)\n\t\t\tsrpt_close_ch(ch);\n\t}\n}\n\nstatic void srpt_cm_timewait_exit(struct ib_cm_id *cm_id)\n{\n\tpr_info(\"Received IB TimeWait exit for cm_id %p.\\n\", cm_id);\n\tsrpt_drain_channel(cm_id);\n}\n\nstatic void srpt_cm_rep_error(struct ib_cm_id *cm_id)\n{\n\tpr_info(\"Received IB REP error for cm_id %p.\\n\", cm_id);\n\tsrpt_drain_channel(cm_id);\n}\n\n/**\n * srpt_cm_dreq_recv() - Process reception of a DREQ message.\n */\nstatic void srpt_cm_dreq_recv(struct ib_cm_id *cm_id)\n{\n\tstruct srpt_rdma_ch *ch;\n\tunsigned long flags;\n\tbool send_drep = false;\n\n\tch = srpt_find_channel(cm_id->context, cm_id);\n\tBUG_ON(!ch);\n\n\tpr_debug(\"cm_id= %p ch->state= %d\\n\", cm_id, srpt_get_ch_state(ch));\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tswitch (ch->state) {\n\tcase CH_CONNECTING:\n\tcase CH_LIVE:\n\t\tsend_drep = true;\n\t\tch->state = CH_DISCONNECTING;\n\t\tbreak;\n\tcase CH_DISCONNECTING:\n\tcase CH_DRAINING:\n\tcase CH_RELEASING:\n\t\tWARN(true, \"unexpected channel state %d\\n\", ch->state);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n\n\tif (send_drep) {\n\t\tif (ib_send_cm_drep(ch->cm_id, NULL, 0) < 0)\n\t\t\tpr_err(\"Sending IB DREP failed.\\n\");\n\t\tpr_info(\"Received DREQ and sent DREP for session %s.\\n\",\n\t\t\tch->sess_name);\n\t}\n}\n\n/**\n * srpt_cm_drep_recv() - Process reception of a DREP message.\n */\nstatic void srpt_cm_drep_recv(struct ib_cm_id *cm_id)\n{\n\tpr_info(\"Received InfiniBand DREP message for cm_id %p.\\n\", cm_id);\n\tsrpt_drain_channel(cm_id);\n}\n\n/**\n * srpt_cm_handler() - IB connection manager callback function.\n *\n * A non-zero return value will cause the caller destroy the CM ID.\n *\n * Note: srpt_cm_handler() must only return a non-zero value when transferring\n * ownership of the cm_id to a channel by srpt_cm_req_recv() failed. Returning\n * a non-zero value in any other case will trigger a race with the\n * ib_destroy_cm_id() call in srpt_release_channel().\n */\nstatic int srpt_cm_handler(struct ib_cm_id *cm_id, struct ib_cm_event *event)\n{\n\tint ret;\n\n\tret = 0;\n\tswitch (event->event) {\n\tcase IB_CM_REQ_RECEIVED:\n\t\tret = srpt_cm_req_recv(cm_id, &event->param.req_rcvd,\n\t\t\t\t       event->private_data);\n\t\tbreak;\n\tcase IB_CM_REJ_RECEIVED:\n\t\tsrpt_cm_rej_recv(cm_id);\n\t\tbreak;\n\tcase IB_CM_RTU_RECEIVED:\n\tcase IB_CM_USER_ESTABLISHED:\n\t\tsrpt_cm_rtu_recv(cm_id);\n\t\tbreak;\n\tcase IB_CM_DREQ_RECEIVED:\n\t\tsrpt_cm_dreq_recv(cm_id);\n\t\tbreak;\n\tcase IB_CM_DREP_RECEIVED:\n\t\tsrpt_cm_drep_recv(cm_id);\n\t\tbreak;\n\tcase IB_CM_TIMEWAIT_EXIT:\n\t\tsrpt_cm_timewait_exit(cm_id);\n\t\tbreak;\n\tcase IB_CM_REP_ERROR:\n\t\tsrpt_cm_rep_error(cm_id);\n\t\tbreak;\n\tcase IB_CM_DREQ_ERROR:\n\t\tpr_info(\"Received IB DREQ ERROR event.\\n\");\n\t\tbreak;\n\tcase IB_CM_MRA_RECEIVED:\n\t\tpr_info(\"Received IB MRA event\\n\");\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized IB CM event %d\\n\", event->event);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n/**\n * srpt_perform_rdmas() - Perform IB RDMA.\n *\n * Returns zero upon success or a negative number upon failure.\n */\nstatic int srpt_perform_rdmas(struct srpt_rdma_ch *ch,\n\t\t\t      struct srpt_send_ioctx *ioctx)\n{\n\tstruct ib_send_wr *bad_wr;\n\tint sq_wr_avail, ret, i;\n\tenum dma_data_direction dir;\n\tconst int n_rdma = ioctx->n_rdma;\n\n\tdir = ioctx->cmd.data_direction;\n\tif (dir == DMA_TO_DEVICE) {\n\t\t/* write */\n\t\tret = -ENOMEM;\n\t\tsq_wr_avail = atomic_sub_return(n_rdma, &ch->sq_wr_avail);\n\t\tif (sq_wr_avail < 0) {\n\t\t\tpr_warn(\"IB send queue full (needed %d)\\n\",\n\t\t\t\tn_rdma);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor (i = 0; i < n_rdma; i++) {\n\t\tstruct ib_send_wr *wr = &ioctx->rdma_wrs[i].wr;\n\n\t\twr->opcode = (dir == DMA_FROM_DEVICE) ?\n\t\t\t\tIB_WR_RDMA_WRITE : IB_WR_RDMA_READ;\n\n\t\tif (i == n_rdma - 1) {\n\t\t\t/* only get completion event for the last rdma read */\n\t\t\tif (dir == DMA_TO_DEVICE) {\n\t\t\t\twr->send_flags = IB_SEND_SIGNALED;\n\t\t\t\tioctx->rdma_cqe.done = srpt_rdma_read_done;\n\t\t\t} else {\n\t\t\t\tioctx->rdma_cqe.done = srpt_rdma_write_done;\n\t\t\t}\n\t\t\twr->wr_cqe = &ioctx->rdma_cqe;\n\t\t\twr->next = NULL;\n\t\t} else {\n\t\t\twr->wr_cqe = NULL;\n\t\t\twr->next = &ioctx->rdma_wrs[i + 1].wr;\n\t\t}\n\t}\n\n\tret = ib_post_send(ch->qp, &ioctx->rdma_wrs->wr, &bad_wr);\n\tif (ret)\n\t\tpr_err(\"%s[%d]: ib_post_send() returned %d for %d/%d\\n\",\n\t\t\t\t __func__, __LINE__, ret, i, n_rdma);\nout:\n\tif (unlikely(dir == DMA_TO_DEVICE && ret < 0))\n\t\tatomic_add(n_rdma, &ch->sq_wr_avail);\n\treturn ret;\n}\n\n/**\n * srpt_xfer_data() - Start data transfer from initiator to target.\n */\nstatic int srpt_xfer_data(struct srpt_rdma_ch *ch,\n\t\t\t  struct srpt_send_ioctx *ioctx)\n{\n\tint ret;\n\n\tret = srpt_map_sg_to_ib_sge(ch, ioctx);\n\tif (ret) {\n\t\tpr_err(\"%s[%d] ret=%d\\n\", __func__, __LINE__, ret);\n\t\tgoto out;\n\t}\n\n\tret = srpt_perform_rdmas(ch, ioctx);\n\tif (ret) {\n\t\tif (ret == -EAGAIN || ret == -ENOMEM)\n\t\t\tpr_info(\"%s[%d] queue full -- ret=%d\\n\",\n\t\t\t\t__func__, __LINE__, ret);\n\t\telse\n\t\t\tpr_err(\"%s[%d] fatal error -- ret=%d\\n\",\n\t\t\t       __func__, __LINE__, ret);\n\t\tgoto out_unmap;\n\t}\n\nout:\n\treturn ret;\nout_unmap:\n\tsrpt_unmap_sg_to_ib_sge(ch, ioctx);\n\tgoto out;\n}\n\nstatic int srpt_write_pending_status(struct se_cmd *se_cmd)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\n\tioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\n\treturn srpt_get_cmd_state(ioctx) == SRPT_STATE_NEED_DATA;\n}\n\n/*\n * srpt_write_pending() - Start data transfer from initiator to target (write).\n */\nstatic int srpt_write_pending(struct se_cmd *se_cmd)\n{\n\tstruct srpt_rdma_ch *ch;\n\tstruct srpt_send_ioctx *ioctx;\n\tenum srpt_command_state new_state;\n\tenum rdma_ch_state ch_state;\n\tint ret;\n\n\tioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\n\n\tnew_state = srpt_set_cmd_state(ioctx, SRPT_STATE_NEED_DATA);\n\tWARN_ON(new_state == SRPT_STATE_DONE);\n\n\tch = ioctx->ch;\n\tBUG_ON(!ch);\n\n\tch_state = srpt_get_ch_state(ch);\n\tswitch (ch_state) {\n\tcase CH_CONNECTING:\n\t\tWARN(true, \"unexpected channel state %d\\n\", ch_state);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\tcase CH_LIVE:\n\t\tbreak;\n\tcase CH_DISCONNECTING:\n\tcase CH_DRAINING:\n\tcase CH_RELEASING:\n\t\tpr_debug(\"cmd with tag %lld: channel disconnecting\\n\",\n\t\t\t ioctx->cmd.tag);\n\t\tsrpt_set_cmd_state(ioctx, SRPT_STATE_DATA_IN);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tret = srpt_xfer_data(ch, ioctx);\n\nout:\n\treturn ret;\n}\n\nstatic u8 tcm_to_srp_tsk_mgmt_status(const int tcm_mgmt_status)\n{\n\tswitch (tcm_mgmt_status) {\n\tcase TMR_FUNCTION_COMPLETE:\n\t\treturn SRP_TSK_MGMT_SUCCESS;\n\tcase TMR_FUNCTION_REJECTED:\n\t\treturn SRP_TSK_MGMT_FUNC_NOT_SUPP;\n\t}\n\treturn SRP_TSK_MGMT_FAILED;\n}\n\n/**\n * srpt_queue_response() - Transmits the response to a SCSI command.\n *\n * Callback function called by the TCM core. Must not block since it can be\n * invoked on the context of the IB completion handler.\n */\nstatic void srpt_queue_response(struct se_cmd *cmd)\n{\n\tstruct srpt_rdma_ch *ch;\n\tstruct srpt_send_ioctx *ioctx;\n\tenum srpt_command_state state;\n\tunsigned long flags;\n\tint ret;\n\tenum dma_data_direction dir;\n\tint resp_len;\n\tu8 srp_tm_status;\n\n\tioctx = container_of(cmd, struct srpt_send_ioctx, cmd);\n\tch = ioctx->ch;\n\tBUG_ON(!ch);\n\n\tspin_lock_irqsave(&ioctx->spinlock, flags);\n\tstate = ioctx->state;\n\tswitch (state) {\n\tcase SRPT_STATE_NEW:\n\tcase SRPT_STATE_DATA_IN:\n\t\tioctx->state = SRPT_STATE_CMD_RSP_SENT;\n\t\tbreak;\n\tcase SRPT_STATE_MGMT:\n\t\tioctx->state = SRPT_STATE_MGMT_RSP_SENT;\n\t\tbreak;\n\tdefault:\n\t\tWARN(true, \"ch %p; cmd %d: unexpected command state %d\\n\",\n\t\t\tch, ioctx->ioctx.index, ioctx->state);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&ioctx->spinlock, flags);\n\n\tif (unlikely(transport_check_aborted_status(&ioctx->cmd, false)\n\t\t     || WARN_ON_ONCE(state == SRPT_STATE_CMD_RSP_SENT))) {\n\t\tatomic_inc(&ch->req_lim_delta);\n\t\tsrpt_abort_cmd(ioctx);\n\t\treturn;\n\t}\n\n\tdir = ioctx->cmd.data_direction;\n\n\t/* For read commands, transfer the data to the initiator. */\n\tif (dir == DMA_FROM_DEVICE && ioctx->cmd.data_length &&\n\t    !ioctx->queue_status_only) {\n\t\tret = srpt_xfer_data(ch, ioctx);\n\t\tif (ret) {\n\t\t\tpr_err(\"xfer_data failed for tag %llu\\n\",\n\t\t\t       ioctx->cmd.tag);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (state != SRPT_STATE_MGMT)\n\t\tresp_len = srpt_build_cmd_rsp(ch, ioctx, ioctx->cmd.tag,\n\t\t\t\t\t      cmd->scsi_status);\n\telse {\n\t\tsrp_tm_status\n\t\t\t= tcm_to_srp_tsk_mgmt_status(cmd->se_tmr_req->response);\n\t\tresp_len = srpt_build_tskmgmt_rsp(ch, ioctx, srp_tm_status,\n\t\t\t\t\t\t ioctx->cmd.tag);\n\t}\n\tret = srpt_post_send(ch, ioctx, resp_len);\n\tif (ret) {\n\t\tpr_err(\"sending cmd response failed for tag %llu\\n\",\n\t\t       ioctx->cmd.tag);\n\t\tsrpt_unmap_sg_to_ib_sge(ch, ioctx);\n\t\tsrpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\n\t\ttarget_put_sess_cmd(&ioctx->cmd);\n\t}\n}\n\nstatic int srpt_queue_data_in(struct se_cmd *cmd)\n{\n\tsrpt_queue_response(cmd);\n\treturn 0;\n}\n\nstatic void srpt_queue_tm_rsp(struct se_cmd *cmd)\n{\n\tsrpt_queue_response(cmd);\n}\n\nstatic void srpt_aborted_task(struct se_cmd *cmd)\n{\n\tstruct srpt_send_ioctx *ioctx = container_of(cmd,\n\t\t\t\tstruct srpt_send_ioctx, cmd);\n\n\tsrpt_unmap_sg_to_ib_sge(ioctx->ch, ioctx);\n}\n\nstatic int srpt_queue_status(struct se_cmd *cmd)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\n\tioctx = container_of(cmd, struct srpt_send_ioctx, cmd);\n\tBUG_ON(ioctx->sense_data != cmd->sense_buffer);\n\tif (cmd->se_cmd_flags &\n\t    (SCF_TRANSPORT_TASK_SENSE | SCF_EMULATED_TASK_SENSE))\n\t\tWARN_ON(cmd->scsi_status != SAM_STAT_CHECK_CONDITION);\n\tioctx->queue_status_only = true;\n\tsrpt_queue_response(cmd);\n\treturn 0;\n}\n\nstatic void srpt_refresh_port_work(struct work_struct *work)\n{\n\tstruct srpt_port *sport = container_of(work, struct srpt_port, work);\n\n\tsrpt_refresh_port(sport);\n}\n\nstatic int srpt_ch_list_empty(struct srpt_device *sdev)\n{\n\tint res;\n\n\tspin_lock_irq(&sdev->spinlock);\n\tres = list_empty(&sdev->rch_list);\n\tspin_unlock_irq(&sdev->spinlock);\n\n\treturn res;\n}\n\n/**\n * srpt_release_sdev() - Free the channel resources associated with a target.\n */\nstatic int srpt_release_sdev(struct srpt_device *sdev)\n{\n\tstruct srpt_rdma_ch *ch, *tmp_ch;\n\tint res;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\n\tBUG_ON(!sdev);\n\n\tspin_lock_irq(&sdev->spinlock);\n\tlist_for_each_entry_safe(ch, tmp_ch, &sdev->rch_list, list)\n\t\t__srpt_close_ch(ch);\n\tspin_unlock_irq(&sdev->spinlock);\n\n\tres = wait_event_interruptible(sdev->ch_releaseQ,\n\t\t\t\t       srpt_ch_list_empty(sdev));\n\tif (res)\n\t\tpr_err(\"%s: interrupted.\\n\", __func__);\n\n\treturn 0;\n}\n\nstatic struct srpt_port *__srpt_lookup_port(const char *name)\n{\n\tstruct ib_device *dev;\n\tstruct srpt_device *sdev;\n\tstruct srpt_port *sport;\n\tint i;\n\n\tlist_for_each_entry(sdev, &srpt_dev_list, list) {\n\t\tdev = sdev->device;\n\t\tif (!dev)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < dev->phys_port_cnt; i++) {\n\t\t\tsport = &sdev->port[i];\n\n\t\t\tif (!strcmp(sport->port_guid, name))\n\t\t\t\treturn sport;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct srpt_port *srpt_lookup_port(const char *name)\n{\n\tstruct srpt_port *sport;\n\n\tspin_lock(&srpt_dev_lock);\n\tsport = __srpt_lookup_port(name);\n\tspin_unlock(&srpt_dev_lock);\n\n\treturn sport;\n}\n\n/**\n * srpt_add_one() - Infiniband device addition callback function.\n */\nstatic void srpt_add_one(struct ib_device *device)\n{\n\tstruct srpt_device *sdev;\n\tstruct srpt_port *sport;\n\tstruct ib_srq_init_attr srq_attr;\n\tint i;\n\n\tpr_debug(\"device = %p, device->dma_ops = %p\\n\", device,\n\t\t device->dma_ops);\n\n\tsdev = kzalloc(sizeof *sdev, GFP_KERNEL);\n\tif (!sdev)\n\t\tgoto err;\n\n\tsdev->device = device;\n\tINIT_LIST_HEAD(&sdev->rch_list);\n\tinit_waitqueue_head(&sdev->ch_releaseQ);\n\tspin_lock_init(&sdev->spinlock);\n\n\tsdev->pd = ib_alloc_pd(device);\n\tif (IS_ERR(sdev->pd))\n\t\tgoto free_dev;\n\n\tsdev->srq_size = min(srpt_srq_size, sdev->device->attrs.max_srq_wr);\n\n\tsrq_attr.event_handler = srpt_srq_event;\n\tsrq_attr.srq_context = (void *)sdev;\n\tsrq_attr.attr.max_wr = sdev->srq_size;\n\tsrq_attr.attr.max_sge = 1;\n\tsrq_attr.attr.srq_limit = 0;\n\tsrq_attr.srq_type = IB_SRQT_BASIC;\n\n\tsdev->srq = ib_create_srq(sdev->pd, &srq_attr);\n\tif (IS_ERR(sdev->srq))\n\t\tgoto err_pd;\n\n\tpr_debug(\"%s: create SRQ #wr= %d max_allow=%d dev= %s\\n\",\n\t\t __func__, sdev->srq_size, sdev->device->attrs.max_srq_wr,\n\t\t device->name);\n\n\tif (!srpt_service_guid)\n\t\tsrpt_service_guid = be64_to_cpu(device->node_guid);\n\n\tsdev->cm_id = ib_create_cm_id(device, srpt_cm_handler, sdev);\n\tif (IS_ERR(sdev->cm_id))\n\t\tgoto err_srq;\n\n\t/* print out target login information */\n\tpr_debug(\"Target login info: id_ext=%016llx,ioc_guid=%016llx,\"\n\t\t \"pkey=ffff,service_id=%016llx\\n\", srpt_service_guid,\n\t\t srpt_service_guid, srpt_service_guid);\n\n\t/*\n\t * We do not have a consistent service_id (ie. also id_ext of target_id)\n\t * to identify this target. We currently use the guid of the first HCA\n\t * in the system as service_id; therefore, the target_id will change\n\t * if this HCA is gone bad and replaced by different HCA\n\t */\n\tif (ib_cm_listen(sdev->cm_id, cpu_to_be64(srpt_service_guid), 0))\n\t\tgoto err_cm;\n\n\tINIT_IB_EVENT_HANDLER(&sdev->event_handler, sdev->device,\n\t\t\t      srpt_event_handler);\n\tif (ib_register_event_handler(&sdev->event_handler))\n\t\tgoto err_cm;\n\n\tsdev->ioctx_ring = (struct srpt_recv_ioctx **)\n\t\tsrpt_alloc_ioctx_ring(sdev, sdev->srq_size,\n\t\t\t\t      sizeof(*sdev->ioctx_ring[0]),\n\t\t\t\t      srp_max_req_size, DMA_FROM_DEVICE);\n\tif (!sdev->ioctx_ring)\n\t\tgoto err_event;\n\n\tfor (i = 0; i < sdev->srq_size; ++i)\n\t\tsrpt_post_recv(sdev, sdev->ioctx_ring[i]);\n\n\tWARN_ON(sdev->device->phys_port_cnt > ARRAY_SIZE(sdev->port));\n\n\tfor (i = 1; i <= sdev->device->phys_port_cnt; i++) {\n\t\tsport = &sdev->port[i - 1];\n\t\tsport->sdev = sdev;\n\t\tsport->port = i;\n\t\tsport->port_attrib.srp_max_rdma_size = DEFAULT_MAX_RDMA_SIZE;\n\t\tsport->port_attrib.srp_max_rsp_size = DEFAULT_MAX_RSP_SIZE;\n\t\tsport->port_attrib.srp_sq_size = DEF_SRPT_SQ_SIZE;\n\t\tINIT_WORK(&sport->work, srpt_refresh_port_work);\n\n\t\tif (srpt_refresh_port(sport)) {\n\t\t\tpr_err(\"MAD registration failed for %s-%d.\\n\",\n\t\t\t       srpt_sdev_name(sdev), i);\n\t\t\tgoto err_ring;\n\t\t}\n\t\tsnprintf(sport->port_guid, sizeof(sport->port_guid),\n\t\t\t\"0x%016llx%016llx\",\n\t\t\tbe64_to_cpu(sport->gid.global.subnet_prefix),\n\t\t\tbe64_to_cpu(sport->gid.global.interface_id));\n\t}\n\n\tspin_lock(&srpt_dev_lock);\n\tlist_add_tail(&sdev->list, &srpt_dev_list);\n\tspin_unlock(&srpt_dev_lock);\n\nout:\n\tib_set_client_data(device, &srpt_client, sdev);\n\tpr_debug(\"added %s.\\n\", device->name);\n\treturn;\n\nerr_ring:\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)sdev->ioctx_ring, sdev,\n\t\t\t     sdev->srq_size, srp_max_req_size,\n\t\t\t     DMA_FROM_DEVICE);\nerr_event:\n\tib_unregister_event_handler(&sdev->event_handler);\nerr_cm:\n\tib_destroy_cm_id(sdev->cm_id);\nerr_srq:\n\tib_destroy_srq(sdev->srq);\nerr_pd:\n\tib_dealloc_pd(sdev->pd);\nfree_dev:\n\tkfree(sdev);\nerr:\n\tsdev = NULL;\n\tpr_info(\"%s(%s) failed.\\n\", __func__, device->name);\n\tgoto out;\n}\n\n/**\n * srpt_remove_one() - InfiniBand device removal callback function.\n */\nstatic void srpt_remove_one(struct ib_device *device, void *client_data)\n{\n\tstruct srpt_device *sdev = client_data;\n\tint i;\n\n\tif (!sdev) {\n\t\tpr_info(\"%s(%s): nothing to do.\\n\", __func__, device->name);\n\t\treturn;\n\t}\n\n\tsrpt_unregister_mad_agent(sdev);\n\n\tib_unregister_event_handler(&sdev->event_handler);\n\n\t/* Cancel any work queued by the just unregistered IB event handler. */\n\tfor (i = 0; i < sdev->device->phys_port_cnt; i++)\n\t\tcancel_work_sync(&sdev->port[i].work);\n\n\tib_destroy_cm_id(sdev->cm_id);\n\n\t/*\n\t * Unregistering a target must happen after destroying sdev->cm_id\n\t * such that no new SRP_LOGIN_REQ information units can arrive while\n\t * destroying the target.\n\t */\n\tspin_lock(&srpt_dev_lock);\n\tlist_del(&sdev->list);\n\tspin_unlock(&srpt_dev_lock);\n\tsrpt_release_sdev(sdev);\n\n\tib_destroy_srq(sdev->srq);\n\tib_dealloc_pd(sdev->pd);\n\n\tsrpt_free_ioctx_ring((struct srpt_ioctx **)sdev->ioctx_ring, sdev,\n\t\t\t     sdev->srq_size, srp_max_req_size, DMA_FROM_DEVICE);\n\tsdev->ioctx_ring = NULL;\n\tkfree(sdev);\n}\n\nstatic struct ib_client srpt_client = {\n\t.name = DRV_NAME,\n\t.add = srpt_add_one,\n\t.remove = srpt_remove_one\n};\n\nstatic int srpt_check_true(struct se_portal_group *se_tpg)\n{\n\treturn 1;\n}\n\nstatic int srpt_check_false(struct se_portal_group *se_tpg)\n{\n\treturn 0;\n}\n\nstatic char *srpt_get_fabric_name(void)\n{\n\treturn \"srpt\";\n}\n\nstatic char *srpt_get_fabric_wwn(struct se_portal_group *tpg)\n{\n\tstruct srpt_port *sport = container_of(tpg, struct srpt_port, port_tpg_1);\n\n\treturn sport->port_guid;\n}\n\nstatic u16 srpt_get_tag(struct se_portal_group *tpg)\n{\n\treturn 1;\n}\n\nstatic u32 srpt_tpg_get_inst_index(struct se_portal_group *se_tpg)\n{\n\treturn 1;\n}\n\nstatic void srpt_release_cmd(struct se_cmd *se_cmd)\n{\n\tstruct srpt_send_ioctx *ioctx = container_of(se_cmd,\n\t\t\t\tstruct srpt_send_ioctx, cmd);\n\tstruct srpt_rdma_ch *ch = ioctx->ch;\n\tunsigned long flags;\n\n\tWARN_ON(ioctx->state != SRPT_STATE_DONE);\n\tWARN_ON(ioctx->mapped_sg_count != 0);\n\n\tif (ioctx->n_rbuf > 1) {\n\t\tkfree(ioctx->rbufs);\n\t\tioctx->rbufs = NULL;\n\t\tioctx->n_rbuf = 0;\n\t}\n\n\tspin_lock_irqsave(&ch->spinlock, flags);\n\tlist_add(&ioctx->free_list, &ch->free_list);\n\tspin_unlock_irqrestore(&ch->spinlock, flags);\n}\n\n/**\n * srpt_close_session() - Forcibly close a session.\n *\n * Callback function invoked by the TCM core to clean up sessions associated\n * with a node ACL when the user invokes\n * rmdir /sys/kernel/config/target/$driver/$port/$tpg/acls/$i_port_id\n */\nstatic void srpt_close_session(struct se_session *se_sess)\n{\n\tDECLARE_COMPLETION_ONSTACK(release_done);\n\tstruct srpt_rdma_ch *ch;\n\tstruct srpt_device *sdev;\n\tunsigned long res;\n\n\tch = se_sess->fabric_sess_ptr;\n\tWARN_ON(ch->sess != se_sess);\n\n\tpr_debug(\"ch %p state %d\\n\", ch, srpt_get_ch_state(ch));\n\n\tsdev = ch->sport->sdev;\n\tspin_lock_irq(&sdev->spinlock);\n\tBUG_ON(ch->release_done);\n\tch->release_done = &release_done;\n\t__srpt_close_ch(ch);\n\tspin_unlock_irq(&sdev->spinlock);\n\n\tres = wait_for_completion_timeout(&release_done, 60 * HZ);\n\tWARN_ON(res == 0);\n}\n\n/**\n * srpt_sess_get_index() - Return the value of scsiAttIntrPortIndex (SCSI-MIB).\n *\n * A quote from RFC 4455 (SCSI-MIB) about this MIB object:\n * This object represents an arbitrary integer used to uniquely identify a\n * particular attached remote initiator port to a particular SCSI target port\n * within a particular SCSI target device within a particular SCSI instance.\n */\nstatic u32 srpt_sess_get_index(struct se_session *se_sess)\n{\n\treturn 0;\n}\n\nstatic void srpt_set_default_node_attrs(struct se_node_acl *nacl)\n{\n}\n\n/* Note: only used from inside debug printk's by the TCM core. */\nstatic int srpt_get_tcm_cmd_state(struct se_cmd *se_cmd)\n{\n\tstruct srpt_send_ioctx *ioctx;\n\n\tioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\n\treturn srpt_get_cmd_state(ioctx);\n}\n\n/**\n * srpt_parse_i_port_id() - Parse an initiator port ID.\n * @name: ASCII representation of a 128-bit initiator port ID.\n * @i_port_id: Binary 128-bit port ID.\n */\nstatic int srpt_parse_i_port_id(u8 i_port_id[16], const char *name)\n{\n\tconst char *p;\n\tunsigned len, count, leading_zero_bytes;\n\tint ret, rc;\n\n\tp = name;\n\tif (strncasecmp(p, \"0x\", 2) == 0)\n\t\tp += 2;\n\tret = -EINVAL;\n\tlen = strlen(p);\n\tif (len % 2)\n\t\tgoto out;\n\tcount = min(len / 2, 16U);\n\tleading_zero_bytes = 16 - count;\n\tmemset(i_port_id, 0, leading_zero_bytes);\n\trc = hex2bin(i_port_id + leading_zero_bytes, p, count);\n\tif (rc < 0)\n\t\tpr_debug(\"hex2bin failed for srpt_parse_i_port_id: %d\\n\", rc);\n\tret = 0;\nout:\n\treturn ret;\n}\n\n/*\n * configfs callback function invoked for\n * mkdir /sys/kernel/config/target/$driver/$port/$tpg/acls/$i_port_id\n */\nstatic int srpt_init_nodeacl(struct se_node_acl *se_nacl, const char *name)\n{\n\tu8 i_port_id[16];\n\n\tif (srpt_parse_i_port_id(i_port_id, name) < 0) {\n\t\tpr_err(\"invalid initiator port ID %s\\n\", name);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rdma_size_show(struct config_item *item,\n\t\tchar *page)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\n\treturn sprintf(page, \"%u\\n\", sport->port_attrib.srp_max_rdma_size);\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rdma_size_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul(page, 0, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"kstrtoul() failed with ret: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\tif (val > MAX_SRPT_RDMA_SIZE) {\n\t\tpr_err(\"val: %lu exceeds MAX_SRPT_RDMA_SIZE: %d\\n\", val,\n\t\t\tMAX_SRPT_RDMA_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tif (val < DEFAULT_MAX_RDMA_SIZE) {\n\t\tpr_err(\"val: %lu smaller than DEFAULT_MAX_RDMA_SIZE: %d\\n\",\n\t\t\tval, DEFAULT_MAX_RDMA_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tsport->port_attrib.srp_max_rdma_size = val;\n\n\treturn count;\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rsp_size_show(struct config_item *item,\n\t\tchar *page)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\n\treturn sprintf(page, \"%u\\n\", sport->port_attrib.srp_max_rsp_size);\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_max_rsp_size_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul(page, 0, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"kstrtoul() failed with ret: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\tif (val > MAX_SRPT_RSP_SIZE) {\n\t\tpr_err(\"val: %lu exceeds MAX_SRPT_RSP_SIZE: %d\\n\", val,\n\t\t\tMAX_SRPT_RSP_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tif (val < MIN_MAX_RSP_SIZE) {\n\t\tpr_err(\"val: %lu smaller than MIN_MAX_RSP_SIZE: %d\\n\", val,\n\t\t\tMIN_MAX_RSP_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tsport->port_attrib.srp_max_rsp_size = val;\n\n\treturn count;\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_sq_size_show(struct config_item *item,\n\t\tchar *page)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\n\treturn sprintf(page, \"%u\\n\", sport->port_attrib.srp_sq_size);\n}\n\nstatic ssize_t srpt_tpg_attrib_srp_sq_size_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = attrib_to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul(page, 0, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"kstrtoul() failed with ret: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\tif (val > MAX_SRPT_SRQ_SIZE) {\n\t\tpr_err(\"val: %lu exceeds MAX_SRPT_SRQ_SIZE: %d\\n\", val,\n\t\t\tMAX_SRPT_SRQ_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tif (val < MIN_SRPT_SRQ_SIZE) {\n\t\tpr_err(\"val: %lu smaller than MIN_SRPT_SRQ_SIZE: %d\\n\", val,\n\t\t\tMIN_SRPT_SRQ_SIZE);\n\t\treturn -EINVAL;\n\t}\n\tsport->port_attrib.srp_sq_size = val;\n\n\treturn count;\n}\n\nCONFIGFS_ATTR(srpt_tpg_attrib_,  srp_max_rdma_size);\nCONFIGFS_ATTR(srpt_tpg_attrib_,  srp_max_rsp_size);\nCONFIGFS_ATTR(srpt_tpg_attrib_,  srp_sq_size);\n\nstatic struct configfs_attribute *srpt_tpg_attrib_attrs[] = {\n\t&srpt_tpg_attrib_attr_srp_max_rdma_size,\n\t&srpt_tpg_attrib_attr_srp_max_rsp_size,\n\t&srpt_tpg_attrib_attr_srp_sq_size,\n\tNULL,\n};\n\nstatic ssize_t srpt_tpg_enable_show(struct config_item *item, char *page)\n{\n\tstruct se_portal_group *se_tpg = to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\n\treturn snprintf(page, PAGE_SIZE, \"%d\\n\", (sport->enabled) ? 1: 0);\n}\n\nstatic ssize_t srpt_tpg_enable_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_portal_group *se_tpg = to_tpg(item);\n\tstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\n\tunsigned long tmp;\n        int ret;\n\n\tret = kstrtoul(page, 0, &tmp);\n\tif (ret < 0) {\n\t\tpr_err(\"Unable to extract srpt_tpg_store_enable\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((tmp != 0) && (tmp != 1)) {\n\t\tpr_err(\"Illegal value for srpt_tpg_store_enable: %lu\\n\", tmp);\n\t\treturn -EINVAL;\n\t}\n\tif (tmp == 1)\n\t\tsport->enabled = true;\n\telse\n\t\tsport->enabled = false;\n\n\treturn count;\n}\n\nCONFIGFS_ATTR(srpt_tpg_, enable);\n\nstatic struct configfs_attribute *srpt_tpg_attrs[] = {\n\t&srpt_tpg_attr_enable,\n\tNULL,\n};\n\n/**\n * configfs callback invoked for\n * mkdir /sys/kernel/config/target/$driver/$port/$tpg\n */\nstatic struct se_portal_group *srpt_make_tpg(struct se_wwn *wwn,\n\t\t\t\t\t     struct config_group *group,\n\t\t\t\t\t     const char *name)\n{\n\tstruct srpt_port *sport = container_of(wwn, struct srpt_port, port_wwn);\n\tint res;\n\n\t/* Initialize sport->port_wwn and sport->port_tpg_1 */\n\tres = core_tpg_register(&sport->port_wwn, &sport->port_tpg_1, SCSI_PROTOCOL_SRP);\n\tif (res)\n\t\treturn ERR_PTR(res);\n\n\treturn &sport->port_tpg_1;\n}\n\n/**\n * configfs callback invoked for\n * rmdir /sys/kernel/config/target/$driver/$port/$tpg\n */\nstatic void srpt_drop_tpg(struct se_portal_group *tpg)\n{\n\tstruct srpt_port *sport = container_of(tpg,\n\t\t\t\tstruct srpt_port, port_tpg_1);\n\n\tsport->enabled = false;\n\tcore_tpg_deregister(&sport->port_tpg_1);\n}\n\n/**\n * configfs callback invoked for\n * mkdir /sys/kernel/config/target/$driver/$port\n */\nstatic struct se_wwn *srpt_make_tport(struct target_fabric_configfs *tf,\n\t\t\t\t      struct config_group *group,\n\t\t\t\t      const char *name)\n{\n\tstruct srpt_port *sport;\n\tint ret;\n\n\tsport = srpt_lookup_port(name);\n\tpr_debug(\"make_tport(%s)\\n\", name);\n\tret = -EINVAL;\n\tif (!sport)\n\t\tgoto err;\n\n\treturn &sport->port_wwn;\n\nerr:\n\treturn ERR_PTR(ret);\n}\n\n/**\n * configfs callback invoked for\n * rmdir /sys/kernel/config/target/$driver/$port\n */\nstatic void srpt_drop_tport(struct se_wwn *wwn)\n{\n\tstruct srpt_port *sport = container_of(wwn, struct srpt_port, port_wwn);\n\n\tpr_debug(\"drop_tport(%s\\n\", config_item_name(&sport->port_wwn.wwn_group.cg_item));\n}\n\nstatic ssize_t srpt_wwn_version_show(struct config_item *item, char *buf)\n{\n\treturn scnprintf(buf, PAGE_SIZE, \"%s\\n\", DRV_VERSION);\n}\n\nCONFIGFS_ATTR_RO(srpt_wwn_, version);\n\nstatic struct configfs_attribute *srpt_wwn_attrs[] = {\n\t&srpt_wwn_attr_version,\n\tNULL,\n};\n\nstatic const struct target_core_fabric_ops srpt_template = {\n\t.module\t\t\t\t= THIS_MODULE,\n\t.name\t\t\t\t= \"srpt\",\n\t.node_acl_size\t\t\t= sizeof(struct srpt_node_acl),\n\t.get_fabric_name\t\t= srpt_get_fabric_name,\n\t.tpg_get_wwn\t\t\t= srpt_get_fabric_wwn,\n\t.tpg_get_tag\t\t\t= srpt_get_tag,\n\t.tpg_check_demo_mode\t\t= srpt_check_false,\n\t.tpg_check_demo_mode_cache\t= srpt_check_true,\n\t.tpg_check_demo_mode_write_protect = srpt_check_true,\n\t.tpg_check_prod_mode_write_protect = srpt_check_false,\n\t.tpg_get_inst_index\t\t= srpt_tpg_get_inst_index,\n\t.release_cmd\t\t\t= srpt_release_cmd,\n\t.check_stop_free\t\t= srpt_check_stop_free,\n\t.shutdown_session\t\t= srpt_shutdown_session,\n\t.close_session\t\t\t= srpt_close_session,\n\t.sess_get_index\t\t\t= srpt_sess_get_index,\n\t.sess_get_initiator_sid\t\t= NULL,\n\t.write_pending\t\t\t= srpt_write_pending,\n\t.write_pending_status\t\t= srpt_write_pending_status,\n\t.set_default_node_attributes\t= srpt_set_default_node_attrs,\n\t.get_cmd_state\t\t\t= srpt_get_tcm_cmd_state,\n\t.queue_data_in\t\t\t= srpt_queue_data_in,\n\t.queue_status\t\t\t= srpt_queue_status,\n\t.queue_tm_rsp\t\t\t= srpt_queue_tm_rsp,\n\t.aborted_task\t\t\t= srpt_aborted_task,\n\t/*\n\t * Setup function pointers for generic logic in\n\t * target_core_fabric_configfs.c\n\t */\n\t.fabric_make_wwn\t\t= srpt_make_tport,\n\t.fabric_drop_wwn\t\t= srpt_drop_tport,\n\t.fabric_make_tpg\t\t= srpt_make_tpg,\n\t.fabric_drop_tpg\t\t= srpt_drop_tpg,\n\t.fabric_init_nodeacl\t\t= srpt_init_nodeacl,\n\n\t.tfc_wwn_attrs\t\t\t= srpt_wwn_attrs,\n\t.tfc_tpg_base_attrs\t\t= srpt_tpg_attrs,\n\t.tfc_tpg_attrib_attrs\t\t= srpt_tpg_attrib_attrs,\n};\n\n/**\n * srpt_init_module() - Kernel module initialization.\n *\n * Note: Since ib_register_client() registers callback functions, and since at\n * least one of these callback functions (srpt_add_one()) calls target core\n * functions, this driver must be registered with the target core before\n * ib_register_client() is called.\n */\nstatic int __init srpt_init_module(void)\n{\n\tint ret;\n\n\tret = -EINVAL;\n\tif (srp_max_req_size < MIN_MAX_REQ_SIZE) {\n\t\tpr_err(\"invalid value %d for kernel module parameter\"\n\t\t       \" srp_max_req_size -- must be at least %d.\\n\",\n\t\t       srp_max_req_size, MIN_MAX_REQ_SIZE);\n\t\tgoto out;\n\t}\n\n\tif (srpt_srq_size < MIN_SRPT_SRQ_SIZE\n\t    || srpt_srq_size > MAX_SRPT_SRQ_SIZE) {\n\t\tpr_err(\"invalid value %d for kernel module parameter\"\n\t\t       \" srpt_srq_size -- must be in the range [%d..%d].\\n\",\n\t\t       srpt_srq_size, MIN_SRPT_SRQ_SIZE, MAX_SRPT_SRQ_SIZE);\n\t\tgoto out;\n\t}\n\n\tret = target_register_template(&srpt_template);\n\tif (ret)\n\t\tgoto out;\n\n\tret = ib_register_client(&srpt_client);\n\tif (ret) {\n\t\tpr_err(\"couldn't register IB client\\n\");\n\t\tgoto out_unregister_target;\n\t}\n\n\treturn 0;\n\nout_unregister_target:\n\ttarget_unregister_template(&srpt_template);\nout:\n\treturn ret;\n}\n\nstatic void __exit srpt_cleanup_module(void)\n{\n\tib_unregister_client(&srpt_client);\n\ttarget_unregister_template(&srpt_template);\n}\n\nmodule_init(srpt_init_module);\nmodule_exit(srpt_cleanup_module);\n"], "filenames": ["drivers/infiniband/ulp/srpt/ib_srpt.c"], "buggy_code_start_loc": [1673], "buggy_code_end_loc": [1783], "fixing_code_start_loc": [1672], "fixing_code_end_loc": [1726], "type": "CWE-476", "message": "drivers/infiniband/ulp/srpt/ib_srpt.c in the Linux kernel before 4.5.1 allows local users to cause a denial of service (NULL pointer dereference and system crash) by using an ABORT_TASK command to abort a device write operation.", "other": {"cve": {"id": "CVE-2016-6327", "sourceIdentifier": "secalert@redhat.com", "published": "2016-10-16T21:59:06.660", "lastModified": "2023-02-12T23:24:59.433", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "drivers/infiniband/ulp/srpt/ib_srpt.c in the Linux kernel before 4.5.1 allows local users to cause a denial of service (NULL pointer dereference and system crash) by using an ABORT_TASK command to abort a device write operation."}, {"lang": "es", "value": "drivers/infiniband/ulp/srpt/ib_srpt.c en el kernel de Linux en versiones anteriores a 4.5.1 permite a usuarios locales provocar una denegaci\u00f3n de servicio (referencia a puntero NULL y ca\u00edda de sistema) usando un comando ABORT_TASK para abortar una operaci\u00f3n de escritura de dispositivo."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.5.0", "matchCriteriaId": "F61E0DB9-4FAB-4B47-91DA-A0FAF09E3747"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=51093254bf879bc9ce96590400a87897c7498463", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2574.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2584.html", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.5.1", "source": "secalert@redhat.com", "tags": ["Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/08/19/5", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/92549", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1354525", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/51093254bf879bc9ce96590400a87897c7498463", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/51093254bf879bc9ce96590400a87897c7498463"}}