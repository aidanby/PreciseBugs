{"buggy_code": ["/*\n * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <net/ipv6.h>\n#include <net/inet6_hashtables.h>\n#include <net/addrconf.h>\n\n#include \"rds.h\"\n#include \"loop.h\"\n\n#define RDS_CONNECTION_HASH_BITS 12\n#define RDS_CONNECTION_HASH_ENTRIES (1 << RDS_CONNECTION_HASH_BITS)\n#define RDS_CONNECTION_HASH_MASK (RDS_CONNECTION_HASH_ENTRIES - 1)\n\n/* converting this to RCU is a chore for another day.. */\nstatic DEFINE_SPINLOCK(rds_conn_lock);\nstatic unsigned long rds_conn_count;\nstatic struct hlist_head rds_conn_hash[RDS_CONNECTION_HASH_ENTRIES];\nstatic struct kmem_cache *rds_conn_slab;\n\nstatic struct hlist_head *rds_conn_bucket(const struct in6_addr *laddr,\n\t\t\t\t\t  const struct in6_addr *faddr)\n{\n\tstatic u32 rds6_hash_secret __read_mostly;\n\tstatic u32 rds_hash_secret __read_mostly;\n\n\tu32 lhash, fhash, hash;\n\n\tnet_get_random_once(&rds_hash_secret, sizeof(rds_hash_secret));\n\tnet_get_random_once(&rds6_hash_secret, sizeof(rds6_hash_secret));\n\n\tlhash = (__force u32)laddr->s6_addr32[3];\n#if IS_ENABLED(CONFIG_IPV6)\n\tfhash = __ipv6_addr_jhash(faddr, rds6_hash_secret);\n#else\n\tfhash = (__force u32)faddr->s6_addr32[3];\n#endif\n\thash = __inet_ehashfn(lhash, 0, fhash, 0, rds_hash_secret);\n\n\treturn &rds_conn_hash[hash & RDS_CONNECTION_HASH_MASK];\n}\n\n#define rds_conn_info_set(var, test, suffix) do {\t\t\\\n\tif (test)\t\t\t\t\t\t\\\n\t\tvar |= RDS_INFO_CONNECTION_FLAG_##suffix;\t\\\n} while (0)\n\n/* rcu read lock must be held or the connection spinlock */\nstatic struct rds_connection *rds_conn_lookup(struct net *net,\n\t\t\t\t\t      struct hlist_head *head,\n\t\t\t\t\t      const struct in6_addr *laddr,\n\t\t\t\t\t      const struct in6_addr *faddr,\n\t\t\t\t\t      struct rds_transport *trans,\n\t\t\t\t\t      u8 tos, int dev_if)\n{\n\tstruct rds_connection *conn, *ret = NULL;\n\n\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\t\tif (ipv6_addr_equal(&conn->c_faddr, faddr) &&\n\t\t    ipv6_addr_equal(&conn->c_laddr, laddr) &&\n\t\t    conn->c_trans == trans &&\n\t\t    conn->c_tos == tos &&\n\t\t    net == rds_conn_net(conn) &&\n\t\t    conn->c_dev_if == dev_if) {\n\t\t\tret = conn;\n\t\t\tbreak;\n\t\t}\n\t}\n\trdsdebug(\"returning conn %p for %pI6c -> %pI6c\\n\", ret,\n\t\t laddr, faddr);\n\treturn ret;\n}\n\n/*\n * This is called by transports as they're bringing down a connection.\n * It clears partial message state so that the transport can start sending\n * and receiving over this connection again in the future.  It is up to\n * the transport to have serialized this call with its send and recv.\n */\nstatic void rds_conn_path_reset(struct rds_conn_path *cp)\n{\n\tstruct rds_connection *conn = cp->cp_conn;\n\n\trdsdebug(\"connection %pI6c to %pI6c reset\\n\",\n\t\t &conn->c_laddr, &conn->c_faddr);\n\n\trds_stats_inc(s_conn_reset);\n\trds_send_path_reset(cp);\n\tcp->cp_flags = 0;\n\n\t/* Do not clear next_rx_seq here, else we cannot distinguish\n\t * retransmitted packets from new packets, and will hand all\n\t * of them to the application. That is not consistent with the\n\t * reliability guarantees of RDS. */\n}\n\nstatic void __rds_conn_path_init(struct rds_connection *conn,\n\t\t\t\t struct rds_conn_path *cp, bool is_outgoing)\n{\n\tspin_lock_init(&cp->cp_lock);\n\tcp->cp_next_tx_seq = 1;\n\tinit_waitqueue_head(&cp->cp_waitq);\n\tINIT_LIST_HEAD(&cp->cp_send_queue);\n\tINIT_LIST_HEAD(&cp->cp_retrans);\n\n\tcp->cp_conn = conn;\n\tatomic_set(&cp->cp_state, RDS_CONN_DOWN);\n\tcp->cp_send_gen = 0;\n\tcp->cp_reconnect_jiffies = 0;\n\tcp->cp_conn->c_proposed_version = RDS_PROTOCOL_VERSION;\n\tINIT_DELAYED_WORK(&cp->cp_send_w, rds_send_worker);\n\tINIT_DELAYED_WORK(&cp->cp_recv_w, rds_recv_worker);\n\tINIT_DELAYED_WORK(&cp->cp_conn_w, rds_connect_worker);\n\tINIT_WORK(&cp->cp_down_w, rds_shutdown_worker);\n\tmutex_init(&cp->cp_cm_lock);\n\tcp->cp_flags = 0;\n}\n\n/*\n * There is only every one 'conn' for a given pair of addresses in the\n * system at a time.  They contain messages to be retransmitted and so\n * span the lifetime of the actual underlying transport connections.\n *\n * For now they are not garbage collected once they're created.  They\n * are torn down as the module is removed, if ever.\n */\nstatic struct rds_connection *__rds_conn_create(struct net *net,\n\t\t\t\t\t\tconst struct in6_addr *laddr,\n\t\t\t\t\t\tconst struct in6_addr *faddr,\n\t\t\t\t\t\tstruct rds_transport *trans,\n\t\t\t\t\t\tgfp_t gfp, u8 tos,\n\t\t\t\t\t\tint is_outgoing,\n\t\t\t\t\t\tint dev_if)\n{\n\tstruct rds_connection *conn, *parent = NULL;\n\tstruct hlist_head *head = rds_conn_bucket(laddr, faddr);\n\tstruct rds_transport *loop_trans;\n\tunsigned long flags;\n\tint ret, i;\n\tint npaths = (trans->t_mp_capable ? RDS_MPATH_WORKERS : 1);\n\n\trcu_read_lock();\n\tconn = rds_conn_lookup(net, head, laddr, faddr, trans, tos, dev_if);\n\tif (conn &&\n\t    conn->c_loopback &&\n\t    conn->c_trans != &rds_loop_transport &&\n\t    ipv6_addr_equal(laddr, faddr) &&\n\t    !is_outgoing) {\n\t\t/* This is a looped back IB connection, and we're\n\t\t * called by the code handling the incoming connect.\n\t\t * We need a second connection object into which we\n\t\t * can stick the other QP. */\n\t\tparent = conn;\n\t\tconn = parent->c_passive;\n\t}\n\trcu_read_unlock();\n\tif (conn)\n\t\tgoto out;\n\n\tconn = kmem_cache_zalloc(rds_conn_slab, gfp);\n\tif (!conn) {\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\tconn->c_path = kcalloc(npaths, sizeof(struct rds_conn_path), gfp);\n\tif (!conn->c_path) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tINIT_HLIST_NODE(&conn->c_hash_node);\n\tconn->c_laddr = *laddr;\n\tconn->c_isv6 = !ipv6_addr_v4mapped(laddr);\n\tconn->c_faddr = *faddr;\n\tconn->c_dev_if = dev_if;\n\tconn->c_tos = tos;\n\n#if IS_ENABLED(CONFIG_IPV6)\n\t/* If the local address is link local, set c_bound_if to be the\n\t * index used for this connection.  Otherwise, set it to 0 as\n\t * the socket is not bound to an interface.  c_bound_if is used\n\t * to look up a socket when a packet is received\n\t */\n\tif (ipv6_addr_type(laddr) & IPV6_ADDR_LINKLOCAL)\n\t\tconn->c_bound_if = dev_if;\n\telse\n#endif\n\t\tconn->c_bound_if = 0;\n\n\trds_conn_net_set(conn, net);\n\n\tret = rds_cong_get_maps(conn);\n\tif (ret) {\n\t\tkfree(conn->c_path);\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This is where a connection becomes loopback.  If *any* RDS sockets\n\t * can bind to the destination address then we'd rather the messages\n\t * flow through loopback rather than either transport.\n\t */\n\tloop_trans = rds_trans_get_preferred(net, faddr, conn->c_dev_if);\n\tif (loop_trans) {\n\t\trds_trans_put(loop_trans);\n\t\tconn->c_loopback = 1;\n\t\tif (trans->t_prefer_loopback) {\n\t\t\tif (likely(is_outgoing)) {\n\t\t\t\t/* \"outgoing\" connection to local address.\n\t\t\t\t * Protocol says it wants the connection\n\t\t\t\t * handled by the loopback transport.\n\t\t\t\t * This is what TCP does.\n\t\t\t\t */\n\t\t\t\ttrans = &rds_loop_transport;\n\t\t\t} else {\n\t\t\t\t/* No transport currently in use\n\t\t\t\t * should end up here, but if it\n\t\t\t\t * does, reset/destroy the connection.\n\t\t\t\t */\n\t\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\t\tconn = ERR_PTR(-EOPNOTSUPP);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tconn->c_trans = trans;\n\n\tinit_waitqueue_head(&conn->c_hs_waitq);\n\tfor (i = 0; i < npaths; i++) {\n\t\t__rds_conn_path_init(conn, &conn->c_path[i],\n\t\t\t\t     is_outgoing);\n\t\tconn->c_path[i].cp_index = i;\n\t}\n\trcu_read_lock();\n\tif (rds_destroy_pending(conn))\n\t\tret = -ENETDOWN;\n\telse\n\t\tret = trans->conn_alloc(conn, GFP_ATOMIC);\n\tif (ret) {\n\t\trcu_read_unlock();\n\t\tkfree(conn->c_path);\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"allocated conn %p for %pI6c -> %pI6c over %s %s\\n\",\n\t\t conn, laddr, faddr,\n\t\t strnlen(trans->t_name, sizeof(trans->t_name)) ?\n\t\t trans->t_name : \"[unknown]\", is_outgoing ? \"(outgoing)\" : \"\");\n\n\t/*\n\t * Since we ran without holding the conn lock, someone could\n\t * have created the same conn (either normal or passive) in the\n\t * interim. We check while holding the lock. If we won, we complete\n\t * init and return our conn. If we lost, we rollback and return the\n\t * other one.\n\t */\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\tif (parent) {\n\t\t/* Creating passive conn */\n\t\tif (parent->c_passive) {\n\t\t\ttrans->conn_free(conn->c_path[0].cp_transport_data);\n\t\t\tkfree(conn->c_path);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = parent->c_passive;\n\t\t} else {\n\t\t\tparent->c_passive = conn;\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t} else {\n\t\t/* Creating normal conn */\n\t\tstruct rds_connection *found;\n\n\t\tfound = rds_conn_lookup(net, head, laddr, faddr, trans,\n\t\t\t\t\ttos, dev_if);\n\t\tif (found) {\n\t\t\tstruct rds_conn_path *cp;\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < npaths; i++) {\n\t\t\t\tcp = &conn->c_path[i];\n\t\t\t\t/* The ->conn_alloc invocation may have\n\t\t\t\t * allocated resource for all paths, so all\n\t\t\t\t * of them may have to be freed here.\n\t\t\t\t */\n\t\t\t\tif (cp->cp_transport_data)\n\t\t\t\t\ttrans->conn_free(cp->cp_transport_data);\n\t\t\t}\n\t\t\tkfree(conn->c_path);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = found;\n\t\t} else {\n\t\t\tconn->c_my_gen_num = rds_gen_num;\n\t\t\tconn->c_peer_gen_num = 0;\n\t\t\thlist_add_head_rcu(&conn->c_hash_node, head);\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n\trcu_read_unlock();\n\nout:\n\treturn conn;\n}\n\nstruct rds_connection *rds_conn_create(struct net *net,\n\t\t\t\t       const struct in6_addr *laddr,\n\t\t\t\t       const struct in6_addr *faddr,\n\t\t\t\t       struct rds_transport *trans, u8 tos,\n\t\t\t\t       gfp_t gfp, int dev_if)\n{\n\treturn __rds_conn_create(net, laddr, faddr, trans, gfp, tos, 0, dev_if);\n}\nEXPORT_SYMBOL_GPL(rds_conn_create);\n\nstruct rds_connection *rds_conn_create_outgoing(struct net *net,\n\t\t\t\t\t\tconst struct in6_addr *laddr,\n\t\t\t\t\t\tconst struct in6_addr *faddr,\n\t\t\t\t\t\tstruct rds_transport *trans,\n\t\t\t\t\t\tu8 tos, gfp_t gfp, int dev_if)\n{\n\treturn __rds_conn_create(net, laddr, faddr, trans, gfp, tos, 1, dev_if);\n}\nEXPORT_SYMBOL_GPL(rds_conn_create_outgoing);\n\nvoid rds_conn_shutdown(struct rds_conn_path *cp)\n{\n\tstruct rds_connection *conn = cp->cp_conn;\n\n\t/* shut it down unless it's down already */\n\tif (!rds_conn_path_transition(cp, RDS_CONN_DOWN, RDS_CONN_DOWN)) {\n\t\t/*\n\t\t * Quiesce the connection mgmt handlers before we start tearing\n\t\t * things down. We don't hold the mutex for the entire\n\t\t * duration of the shutdown operation, else we may be\n\t\t * deadlocking with the CM handler. Instead, the CM event\n\t\t * handler is supposed to check for state DISCONNECTING\n\t\t */\n\t\tmutex_lock(&cp->cp_cm_lock);\n\t\tif (!rds_conn_path_transition(cp, RDS_CONN_UP,\n\t\t\t\t\t      RDS_CONN_DISCONNECTING) &&\n\t\t    !rds_conn_path_transition(cp, RDS_CONN_ERROR,\n\t\t\t\t\t      RDS_CONN_DISCONNECTING)) {\n\t\t\trds_conn_path_error(cp,\n\t\t\t\t\t    \"shutdown called in state %d\\n\",\n\t\t\t\t\t    atomic_read(&cp->cp_state));\n\t\t\tmutex_unlock(&cp->cp_cm_lock);\n\t\t\treturn;\n\t\t}\n\t\tmutex_unlock(&cp->cp_cm_lock);\n\n\t\twait_event(cp->cp_waitq,\n\t\t\t   !test_bit(RDS_IN_XMIT, &cp->cp_flags));\n\t\twait_event(cp->cp_waitq,\n\t\t\t   !test_bit(RDS_RECV_REFILL, &cp->cp_flags));\n\n\t\tconn->c_trans->conn_path_shutdown(cp);\n\t\trds_conn_path_reset(cp);\n\n\t\tif (!rds_conn_path_transition(cp, RDS_CONN_DISCONNECTING,\n\t\t\t\t\t      RDS_CONN_DOWN) &&\n\t\t    !rds_conn_path_transition(cp, RDS_CONN_ERROR,\n\t\t\t\t\t      RDS_CONN_DOWN)) {\n\t\t\t/* This can happen - eg when we're in the middle of tearing\n\t\t\t * down the connection, and someone unloads the rds module.\n\t\t\t * Quite reproducible with loopback connections.\n\t\t\t * Mostly harmless.\n\t\t\t *\n\t\t\t * Note that this also happens with rds-tcp because\n\t\t\t * we could have triggered rds_conn_path_drop in irq\n\t\t\t * mode from rds_tcp_state change on the receipt of\n\t\t\t * a FIN, thus we need to recheck for RDS_CONN_ERROR\n\t\t\t * here.\n\t\t\t */\n\t\t\trds_conn_path_error(cp, \"%s: failed to transition \"\n\t\t\t\t\t    \"to state DOWN, current state \"\n\t\t\t\t\t    \"is %d\\n\", __func__,\n\t\t\t\t\t    atomic_read(&cp->cp_state));\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Then reconnect if it's still live.\n\t * The passive side of an IB loopback connection is never added\n\t * to the conn hash, so we never trigger a reconnect on this\n\t * conn - the reconnect is always triggered by the active peer. */\n\tcancel_delayed_work_sync(&cp->cp_conn_w);\n\trcu_read_lock();\n\tif (!hlist_unhashed(&conn->c_hash_node)) {\n\t\trcu_read_unlock();\n\t\trds_queue_reconnect(cp);\n\t} else {\n\t\trcu_read_unlock();\n\t}\n}\n\n/* destroy a single rds_conn_path. rds_conn_destroy() iterates over\n * all paths using rds_conn_path_destroy()\n */\nstatic void rds_conn_path_destroy(struct rds_conn_path *cp)\n{\n\tstruct rds_message *rm, *rtmp;\n\n\tif (!cp->cp_transport_data)\n\t\treturn;\n\n\t/* make sure lingering queued work won't try to ref the conn */\n\tcancel_delayed_work_sync(&cp->cp_send_w);\n\tcancel_delayed_work_sync(&cp->cp_recv_w);\n\n\trds_conn_path_drop(cp, true);\n\tflush_work(&cp->cp_down_w);\n\n\t/* tear down queued messages */\n\tlist_for_each_entry_safe(rm, rtmp,\n\t\t\t\t &cp->cp_send_queue,\n\t\t\t\t m_conn_item) {\n\t\tlist_del_init(&rm->m_conn_item);\n\t\tBUG_ON(!list_empty(&rm->m_sock_item));\n\t\trds_message_put(rm);\n\t}\n\tif (cp->cp_xmit_rm)\n\t\trds_message_put(cp->cp_xmit_rm);\n\n\tWARN_ON(delayed_work_pending(&cp->cp_send_w));\n\tWARN_ON(delayed_work_pending(&cp->cp_recv_w));\n\tWARN_ON(delayed_work_pending(&cp->cp_conn_w));\n\tWARN_ON(work_pending(&cp->cp_down_w));\n\n\tcp->cp_conn->c_trans->conn_free(cp->cp_transport_data);\n}\n\n/*\n * Stop and free a connection.\n *\n * This can only be used in very limited circumstances.  It assumes that once\n * the conn has been shutdown that no one else is referencing the connection.\n * We can only ensure this in the rmmod path in the current code.\n */\nvoid rds_conn_destroy(struct rds_connection *conn)\n{\n\tunsigned long flags;\n\tint i;\n\tstruct rds_conn_path *cp;\n\tint npaths = (conn->c_trans->t_mp_capable ? RDS_MPATH_WORKERS : 1);\n\n\trdsdebug(\"freeing conn %p for %pI4 -> \"\n\t\t \"%pI4\\n\", conn, &conn->c_laddr,\n\t\t &conn->c_faddr);\n\n\t/* Ensure conn will not be scheduled for reconnect */\n\tspin_lock_irq(&rds_conn_lock);\n\thlist_del_init_rcu(&conn->c_hash_node);\n\tspin_unlock_irq(&rds_conn_lock);\n\tsynchronize_rcu();\n\n\t/* shut the connection down */\n\tfor (i = 0; i < npaths; i++) {\n\t\tcp = &conn->c_path[i];\n\t\trds_conn_path_destroy(cp);\n\t\tBUG_ON(!list_empty(&cp->cp_retrans));\n\t}\n\n\t/*\n\t * The congestion maps aren't freed up here.  They're\n\t * freed by rds_cong_exit() after all the connections\n\t * have been freed.\n\t */\n\trds_cong_remove_conn(conn);\n\n\tkfree(conn->c_path);\n\tkmem_cache_free(rds_conn_slab, conn);\n\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\trds_conn_count--;\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n}\nEXPORT_SYMBOL_GPL(rds_conn_destroy);\n\nstatic void __rds_inc_msg_cp(struct rds_incoming *inc,\n\t\t\t     struct rds_info_iterator *iter,\n\t\t\t     void *saddr, void *daddr, int flip, bool isv6)\n{\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (isv6)\n\t\trds6_inc_info_copy(inc, iter, saddr, daddr, flip);\n\telse\n#endif\n\t\trds_inc_info_copy(inc, iter, *(__be32 *)saddr,\n\t\t\t\t  *(__be32 *)daddr, flip);\n}\n\nstatic void rds_conn_message_info_cmn(struct socket *sock, unsigned int len,\n\t\t\t\t      struct rds_info_iterator *iter,\n\t\t\t\t      struct rds_info_lengths *lens,\n\t\t\t\t      int want_send, bool isv6)\n{\n\tstruct hlist_head *head;\n\tstruct list_head *list;\n\tstruct rds_connection *conn;\n\tstruct rds_message *rm;\n\tunsigned int total = 0;\n\tunsigned long flags;\n\tsize_t i;\n\tint j;\n\n\tif (isv6)\n\t\tlen /= sizeof(struct rds6_info_message);\n\telse\n\t\tlen /= sizeof(struct rds_info_message);\n\n\trcu_read_lock();\n\n\tfor (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);\n\t     i++, head++) {\n\t\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\t\t\tstruct rds_conn_path *cp;\n\t\t\tint npaths;\n\n\t\t\tif (!isv6 && conn->c_isv6)\n\t\t\t\tcontinue;\n\n\t\t\tnpaths = (conn->c_trans->t_mp_capable ?\n\t\t\t\t RDS_MPATH_WORKERS : 1);\n\n\t\t\tfor (j = 0; j < npaths; j++) {\n\t\t\t\tcp = &conn->c_path[j];\n\t\t\t\tif (want_send)\n\t\t\t\t\tlist = &cp->cp_send_queue;\n\t\t\t\telse\n\t\t\t\t\tlist = &cp->cp_retrans;\n\n\t\t\t\tspin_lock_irqsave(&cp->cp_lock, flags);\n\n\t\t\t\t/* XXX too lazy to maintain counts.. */\n\t\t\t\tlist_for_each_entry(rm, list, m_conn_item) {\n\t\t\t\t\ttotal++;\n\t\t\t\t\tif (total <= len)\n\t\t\t\t\t\t__rds_inc_msg_cp(&rm->m_inc,\n\t\t\t\t\t\t\t\t iter,\n\t\t\t\t\t\t\t\t &conn->c_laddr,\n\t\t\t\t\t\t\t\t &conn->c_faddr,\n\t\t\t\t\t\t\t\t 0, isv6);\n\t\t\t\t}\n\n\t\t\t\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tlens->nr = total;\n\tif (isv6)\n\t\tlens->each = sizeof(struct rds6_info_message);\n\telse\n\t\tlens->each = sizeof(struct rds_info_message);\n}\n\nstatic void rds_conn_message_info(struct socket *sock, unsigned int len,\n\t\t\t\t  struct rds_info_iterator *iter,\n\t\t\t\t  struct rds_info_lengths *lens,\n\t\t\t\t  int want_send)\n{\n\trds_conn_message_info_cmn(sock, len, iter, lens, want_send, false);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic void rds6_conn_message_info(struct socket *sock, unsigned int len,\n\t\t\t\t   struct rds_info_iterator *iter,\n\t\t\t\t   struct rds_info_lengths *lens,\n\t\t\t\t   int want_send)\n{\n\trds_conn_message_info_cmn(sock, len, iter, lens, want_send, true);\n}\n#endif\n\nstatic void rds_conn_message_info_send(struct socket *sock, unsigned int len,\n\t\t\t\t       struct rds_info_iterator *iter,\n\t\t\t\t       struct rds_info_lengths *lens)\n{\n\trds_conn_message_info(sock, len, iter, lens, 1);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic void rds6_conn_message_info_send(struct socket *sock, unsigned int len,\n\t\t\t\t\tstruct rds_info_iterator *iter,\n\t\t\t\t\tstruct rds_info_lengths *lens)\n{\n\trds6_conn_message_info(sock, len, iter, lens, 1);\n}\n#endif\n\nstatic void rds_conn_message_info_retrans(struct socket *sock,\n\t\t\t\t\t  unsigned int len,\n\t\t\t\t\t  struct rds_info_iterator *iter,\n\t\t\t\t\t  struct rds_info_lengths *lens)\n{\n\trds_conn_message_info(sock, len, iter, lens, 0);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic void rds6_conn_message_info_retrans(struct socket *sock,\n\t\t\t\t\t   unsigned int len,\n\t\t\t\t\t   struct rds_info_iterator *iter,\n\t\t\t\t\t   struct rds_info_lengths *lens)\n{\n\trds6_conn_message_info(sock, len, iter, lens, 0);\n}\n#endif\n\nvoid rds_for_each_conn_info(struct socket *sock, unsigned int len,\n\t\t\t  struct rds_info_iterator *iter,\n\t\t\t  struct rds_info_lengths *lens,\n\t\t\t  int (*visitor)(struct rds_connection *, void *),\n\t\t\t  u64 *buffer,\n\t\t\t  size_t item_len)\n{\n\tstruct hlist_head *head;\n\tstruct rds_connection *conn;\n\tsize_t i;\n\n\trcu_read_lock();\n\n\tlens->nr = 0;\n\tlens->each = item_len;\n\n\tfor (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);\n\t     i++, head++) {\n\t\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\n\t\t\t/* XXX no c_lock usage.. */\n\t\t\tif (!visitor(conn, buffer))\n\t\t\t\tcontinue;\n\n\t\t\t/* We copy as much as we can fit in the buffer,\n\t\t\t * but we count all items so that the caller\n\t\t\t * can resize the buffer. */\n\t\t\tif (len >= item_len) {\n\t\t\t\trds_info_copy(iter, buffer, item_len);\n\t\t\t\tlen -= item_len;\n\t\t\t}\n\t\t\tlens->nr++;\n\t\t}\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(rds_for_each_conn_info);\n\nstatic void rds_walk_conn_path_info(struct socket *sock, unsigned int len,\n\t\t\t\t    struct rds_info_iterator *iter,\n\t\t\t\t    struct rds_info_lengths *lens,\n\t\t\t\t    int (*visitor)(struct rds_conn_path *, void *),\n\t\t\t\t    u64 *buffer,\n\t\t\t\t    size_t item_len)\n{\n\tstruct hlist_head *head;\n\tstruct rds_connection *conn;\n\tsize_t i;\n\n\trcu_read_lock();\n\n\tlens->nr = 0;\n\tlens->each = item_len;\n\n\tfor (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);\n\t     i++, head++) {\n\t\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\t\t\tstruct rds_conn_path *cp;\n\n\t\t\t/* XXX We only copy the information from the first\n\t\t\t * path for now.  The problem is that if there are\n\t\t\t * more than one underlying paths, we cannot report\n\t\t\t * information of all of them using the existing\n\t\t\t * API.  For example, there is only one next_tx_seq,\n\t\t\t * which path's next_tx_seq should we report?  It is\n\t\t\t * a bug in the design of MPRDS.\n\t\t\t */\n\t\t\tcp = conn->c_path;\n\n\t\t\t/* XXX no cp_lock usage.. */\n\t\t\tif (!visitor(cp, buffer))\n\t\t\t\tcontinue;\n\n\t\t\t/* We copy as much as we can fit in the buffer,\n\t\t\t * but we count all items so that the caller\n\t\t\t * can resize the buffer.\n\t\t\t */\n\t\t\tif (len >= item_len) {\n\t\t\t\trds_info_copy(iter, buffer, item_len);\n\t\t\t\tlen -= item_len;\n\t\t\t}\n\t\t\tlens->nr++;\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\nstatic int rds_conn_info_visitor(struct rds_conn_path *cp, void *buffer)\n{\n\tstruct rds_info_connection *cinfo = buffer;\n\tstruct rds_connection *conn = cp->cp_conn;\n\n\tif (conn->c_isv6)\n\t\treturn 0;\n\n\tcinfo->next_tx_seq = cp->cp_next_tx_seq;\n\tcinfo->next_rx_seq = cp->cp_next_rx_seq;\n\tcinfo->laddr = conn->c_laddr.s6_addr32[3];\n\tcinfo->faddr = conn->c_faddr.s6_addr32[3];\n\tcinfo->tos = conn->c_tos;\n\tstrncpy(cinfo->transport, conn->c_trans->t_name,\n\t\tsizeof(cinfo->transport));\n\tcinfo->flags = 0;\n\n\trds_conn_info_set(cinfo->flags, test_bit(RDS_IN_XMIT, &cp->cp_flags),\n\t\t\t  SENDING);\n\t/* XXX Future: return the state rather than these funky bits */\n\trds_conn_info_set(cinfo->flags,\n\t\t\t  atomic_read(&cp->cp_state) == RDS_CONN_CONNECTING,\n\t\t\t  CONNECTING);\n\trds_conn_info_set(cinfo->flags,\n\t\t\t  atomic_read(&cp->cp_state) == RDS_CONN_UP,\n\t\t\t  CONNECTED);\n\treturn 1;\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic int rds6_conn_info_visitor(struct rds_conn_path *cp, void *buffer)\n{\n\tstruct rds6_info_connection *cinfo6 = buffer;\n\tstruct rds_connection *conn = cp->cp_conn;\n\n\tcinfo6->next_tx_seq = cp->cp_next_tx_seq;\n\tcinfo6->next_rx_seq = cp->cp_next_rx_seq;\n\tcinfo6->laddr = conn->c_laddr;\n\tcinfo6->faddr = conn->c_faddr;\n\tstrncpy(cinfo6->transport, conn->c_trans->t_name,\n\t\tsizeof(cinfo6->transport));\n\tcinfo6->flags = 0;\n\n\trds_conn_info_set(cinfo6->flags, test_bit(RDS_IN_XMIT, &cp->cp_flags),\n\t\t\t  SENDING);\n\t/* XXX Future: return the state rather than these funky bits */\n\trds_conn_info_set(cinfo6->flags,\n\t\t\t  atomic_read(&cp->cp_state) == RDS_CONN_CONNECTING,\n\t\t\t  CONNECTING);\n\trds_conn_info_set(cinfo6->flags,\n\t\t\t  atomic_read(&cp->cp_state) == RDS_CONN_UP,\n\t\t\t  CONNECTED);\n\t/* Just return 1 as there is no error case. This is a helper function\n\t * for rds_walk_conn_path_info() and it wants a return value.\n\t */\n\treturn 1;\n}\n#endif\n\nstatic void rds_conn_info(struct socket *sock, unsigned int len,\n\t\t\t  struct rds_info_iterator *iter,\n\t\t\t  struct rds_info_lengths *lens)\n{\n\tu64 buffer[(sizeof(struct rds_info_connection) + 7) / 8];\n\n\trds_walk_conn_path_info(sock, len, iter, lens,\n\t\t\t\trds_conn_info_visitor,\n\t\t\t\tbuffer,\n\t\t\t\tsizeof(struct rds_info_connection));\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic void rds6_conn_info(struct socket *sock, unsigned int len,\n\t\t\t   struct rds_info_iterator *iter,\n\t\t\t   struct rds_info_lengths *lens)\n{\n\tu64 buffer[(sizeof(struct rds6_info_connection) + 7) / 8];\n\n\trds_walk_conn_path_info(sock, len, iter, lens,\n\t\t\t\trds6_conn_info_visitor,\n\t\t\t\tbuffer,\n\t\t\t\tsizeof(struct rds6_info_connection));\n}\n#endif\n\nint rds_conn_init(void)\n{\n\tint ret;\n\n\tret = rds_loop_net_init(); /* register pernet callback */\n\tif (ret)\n\t\treturn ret;\n\n\trds_conn_slab = kmem_cache_create(\"rds_connection\",\n\t\t\t\t\t  sizeof(struct rds_connection),\n\t\t\t\t\t  0, 0, NULL);\n\tif (!rds_conn_slab) {\n\t\trds_loop_net_exit();\n\t\treturn -ENOMEM;\n\t}\n\n\trds_info_register_func(RDS_INFO_CONNECTIONS, rds_conn_info);\n\trds_info_register_func(RDS_INFO_SEND_MESSAGES,\n\t\t\t       rds_conn_message_info_send);\n\trds_info_register_func(RDS_INFO_RETRANS_MESSAGES,\n\t\t\t       rds_conn_message_info_retrans);\n#if IS_ENABLED(CONFIG_IPV6)\n\trds_info_register_func(RDS6_INFO_CONNECTIONS, rds6_conn_info);\n\trds_info_register_func(RDS6_INFO_SEND_MESSAGES,\n\t\t\t       rds6_conn_message_info_send);\n\trds_info_register_func(RDS6_INFO_RETRANS_MESSAGES,\n\t\t\t       rds6_conn_message_info_retrans);\n#endif\n\treturn 0;\n}\n\nvoid rds_conn_exit(void)\n{\n\trds_loop_net_exit(); /* unregister pernet callback */\n\trds_loop_exit();\n\n\tWARN_ON(!hlist_empty(rds_conn_hash));\n\n\tkmem_cache_destroy(rds_conn_slab);\n\n\trds_info_deregister_func(RDS_INFO_CONNECTIONS, rds_conn_info);\n\trds_info_deregister_func(RDS_INFO_SEND_MESSAGES,\n\t\t\t\t rds_conn_message_info_send);\n\trds_info_deregister_func(RDS_INFO_RETRANS_MESSAGES,\n\t\t\t\t rds_conn_message_info_retrans);\n#if IS_ENABLED(CONFIG_IPV6)\n\trds_info_deregister_func(RDS6_INFO_CONNECTIONS, rds6_conn_info);\n\trds_info_deregister_func(RDS6_INFO_SEND_MESSAGES,\n\t\t\t\t rds6_conn_message_info_send);\n\trds_info_deregister_func(RDS6_INFO_RETRANS_MESSAGES,\n\t\t\t\t rds6_conn_message_info_retrans);\n#endif\n}\n\n/*\n * Force a disconnect\n */\nvoid rds_conn_path_drop(struct rds_conn_path *cp, bool destroy)\n{\n\tatomic_set(&cp->cp_state, RDS_CONN_ERROR);\n\n\trcu_read_lock();\n\tif (!destroy && rds_destroy_pending(cp->cp_conn)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tqueue_work(rds_wq, &cp->cp_down_w);\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(rds_conn_path_drop);\n\nvoid rds_conn_drop(struct rds_connection *conn)\n{\n\tWARN_ON(conn->c_trans->t_mp_capable);\n\trds_conn_path_drop(&conn->c_path[0], false);\n}\nEXPORT_SYMBOL_GPL(rds_conn_drop);\n\n/*\n * If the connection is down, trigger a connect. We may have scheduled a\n * delayed reconnect however - in this case we should not interfere.\n */\nvoid rds_conn_path_connect_if_down(struct rds_conn_path *cp)\n{\n\trcu_read_lock();\n\tif (rds_destroy_pending(cp->cp_conn)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tif (rds_conn_path_state(cp) == RDS_CONN_DOWN &&\n\t    !test_and_set_bit(RDS_RECONNECT_PENDING, &cp->cp_flags))\n\t\tqueue_delayed_work(rds_wq, &cp->cp_conn_w, 0);\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(rds_conn_path_connect_if_down);\n\n/* Check connectivity of all paths\n */\nvoid rds_check_all_paths(struct rds_connection *conn)\n{\n\tint i = 0;\n\n\tdo {\n\t\trds_conn_path_connect_if_down(&conn->c_path[i]);\n\t} while (++i < conn->c_npaths);\n}\n\nvoid rds_conn_connect_if_down(struct rds_connection *conn)\n{\n\tWARN_ON(conn->c_trans->t_mp_capable);\n\trds_conn_path_connect_if_down(&conn->c_path[0]);\n}\nEXPORT_SYMBOL_GPL(rds_conn_connect_if_down);\n\nvoid\n__rds_conn_path_error(struct rds_conn_path *cp, const char *fmt, ...)\n{\n\tva_list ap;\n\n\tva_start(ap, fmt);\n\tvprintk(fmt, ap);\n\tva_end(ap);\n\n\trds_conn_path_drop(cp, false);\n}\n"], "fixing_code": ["/*\n * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <net/ipv6.h>\n#include <net/inet6_hashtables.h>\n#include <net/addrconf.h>\n\n#include \"rds.h\"\n#include \"loop.h\"\n\n#define RDS_CONNECTION_HASH_BITS 12\n#define RDS_CONNECTION_HASH_ENTRIES (1 << RDS_CONNECTION_HASH_BITS)\n#define RDS_CONNECTION_HASH_MASK (RDS_CONNECTION_HASH_ENTRIES - 1)\n\n/* converting this to RCU is a chore for another day.. */\nstatic DEFINE_SPINLOCK(rds_conn_lock);\nstatic unsigned long rds_conn_count;\nstatic struct hlist_head rds_conn_hash[RDS_CONNECTION_HASH_ENTRIES];\nstatic struct kmem_cache *rds_conn_slab;\n\nstatic struct hlist_head *rds_conn_bucket(const struct in6_addr *laddr,\n\t\t\t\t\t  const struct in6_addr *faddr)\n{\n\tstatic u32 rds6_hash_secret __read_mostly;\n\tstatic u32 rds_hash_secret __read_mostly;\n\n\tu32 lhash, fhash, hash;\n\n\tnet_get_random_once(&rds_hash_secret, sizeof(rds_hash_secret));\n\tnet_get_random_once(&rds6_hash_secret, sizeof(rds6_hash_secret));\n\n\tlhash = (__force u32)laddr->s6_addr32[3];\n#if IS_ENABLED(CONFIG_IPV6)\n\tfhash = __ipv6_addr_jhash(faddr, rds6_hash_secret);\n#else\n\tfhash = (__force u32)faddr->s6_addr32[3];\n#endif\n\thash = __inet_ehashfn(lhash, 0, fhash, 0, rds_hash_secret);\n\n\treturn &rds_conn_hash[hash & RDS_CONNECTION_HASH_MASK];\n}\n\n#define rds_conn_info_set(var, test, suffix) do {\t\t\\\n\tif (test)\t\t\t\t\t\t\\\n\t\tvar |= RDS_INFO_CONNECTION_FLAG_##suffix;\t\\\n} while (0)\n\n/* rcu read lock must be held or the connection spinlock */\nstatic struct rds_connection *rds_conn_lookup(struct net *net,\n\t\t\t\t\t      struct hlist_head *head,\n\t\t\t\t\t      const struct in6_addr *laddr,\n\t\t\t\t\t      const struct in6_addr *faddr,\n\t\t\t\t\t      struct rds_transport *trans,\n\t\t\t\t\t      u8 tos, int dev_if)\n{\n\tstruct rds_connection *conn, *ret = NULL;\n\n\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\t\tif (ipv6_addr_equal(&conn->c_faddr, faddr) &&\n\t\t    ipv6_addr_equal(&conn->c_laddr, laddr) &&\n\t\t    conn->c_trans == trans &&\n\t\t    conn->c_tos == tos &&\n\t\t    net == rds_conn_net(conn) &&\n\t\t    conn->c_dev_if == dev_if) {\n\t\t\tret = conn;\n\t\t\tbreak;\n\t\t}\n\t}\n\trdsdebug(\"returning conn %p for %pI6c -> %pI6c\\n\", ret,\n\t\t laddr, faddr);\n\treturn ret;\n}\n\n/*\n * This is called by transports as they're bringing down a connection.\n * It clears partial message state so that the transport can start sending\n * and receiving over this connection again in the future.  It is up to\n * the transport to have serialized this call with its send and recv.\n */\nstatic void rds_conn_path_reset(struct rds_conn_path *cp)\n{\n\tstruct rds_connection *conn = cp->cp_conn;\n\n\trdsdebug(\"connection %pI6c to %pI6c reset\\n\",\n\t\t &conn->c_laddr, &conn->c_faddr);\n\n\trds_stats_inc(s_conn_reset);\n\trds_send_path_reset(cp);\n\tcp->cp_flags = 0;\n\n\t/* Do not clear next_rx_seq here, else we cannot distinguish\n\t * retransmitted packets from new packets, and will hand all\n\t * of them to the application. That is not consistent with the\n\t * reliability guarantees of RDS. */\n}\n\nstatic void __rds_conn_path_init(struct rds_connection *conn,\n\t\t\t\t struct rds_conn_path *cp, bool is_outgoing)\n{\n\tspin_lock_init(&cp->cp_lock);\n\tcp->cp_next_tx_seq = 1;\n\tinit_waitqueue_head(&cp->cp_waitq);\n\tINIT_LIST_HEAD(&cp->cp_send_queue);\n\tINIT_LIST_HEAD(&cp->cp_retrans);\n\n\tcp->cp_conn = conn;\n\tatomic_set(&cp->cp_state, RDS_CONN_DOWN);\n\tcp->cp_send_gen = 0;\n\tcp->cp_reconnect_jiffies = 0;\n\tcp->cp_conn->c_proposed_version = RDS_PROTOCOL_VERSION;\n\tINIT_DELAYED_WORK(&cp->cp_send_w, rds_send_worker);\n\tINIT_DELAYED_WORK(&cp->cp_recv_w, rds_recv_worker);\n\tINIT_DELAYED_WORK(&cp->cp_conn_w, rds_connect_worker);\n\tINIT_WORK(&cp->cp_down_w, rds_shutdown_worker);\n\tmutex_init(&cp->cp_cm_lock);\n\tcp->cp_flags = 0;\n}\n\n/*\n * There is only every one 'conn' for a given pair of addresses in the\n * system at a time.  They contain messages to be retransmitted and so\n * span the lifetime of the actual underlying transport connections.\n *\n * For now they are not garbage collected once they're created.  They\n * are torn down as the module is removed, if ever.\n */\nstatic struct rds_connection *__rds_conn_create(struct net *net,\n\t\t\t\t\t\tconst struct in6_addr *laddr,\n\t\t\t\t\t\tconst struct in6_addr *faddr,\n\t\t\t\t\t\tstruct rds_transport *trans,\n\t\t\t\t\t\tgfp_t gfp, u8 tos,\n\t\t\t\t\t\tint is_outgoing,\n\t\t\t\t\t\tint dev_if)\n{\n\tstruct rds_connection *conn, *parent = NULL;\n\tstruct hlist_head *head = rds_conn_bucket(laddr, faddr);\n\tstruct rds_transport *loop_trans;\n\tunsigned long flags;\n\tint ret, i;\n\tint npaths = (trans->t_mp_capable ? RDS_MPATH_WORKERS : 1);\n\n\trcu_read_lock();\n\tconn = rds_conn_lookup(net, head, laddr, faddr, trans, tos, dev_if);\n\tif (conn &&\n\t    conn->c_loopback &&\n\t    conn->c_trans != &rds_loop_transport &&\n\t    ipv6_addr_equal(laddr, faddr) &&\n\t    !is_outgoing) {\n\t\t/* This is a looped back IB connection, and we're\n\t\t * called by the code handling the incoming connect.\n\t\t * We need a second connection object into which we\n\t\t * can stick the other QP. */\n\t\tparent = conn;\n\t\tconn = parent->c_passive;\n\t}\n\trcu_read_unlock();\n\tif (conn)\n\t\tgoto out;\n\n\tconn = kmem_cache_zalloc(rds_conn_slab, gfp);\n\tif (!conn) {\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\tconn->c_path = kcalloc(npaths, sizeof(struct rds_conn_path), gfp);\n\tif (!conn->c_path) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tINIT_HLIST_NODE(&conn->c_hash_node);\n\tconn->c_laddr = *laddr;\n\tconn->c_isv6 = !ipv6_addr_v4mapped(laddr);\n\tconn->c_faddr = *faddr;\n\tconn->c_dev_if = dev_if;\n\tconn->c_tos = tos;\n\n#if IS_ENABLED(CONFIG_IPV6)\n\t/* If the local address is link local, set c_bound_if to be the\n\t * index used for this connection.  Otherwise, set it to 0 as\n\t * the socket is not bound to an interface.  c_bound_if is used\n\t * to look up a socket when a packet is received\n\t */\n\tif (ipv6_addr_type(laddr) & IPV6_ADDR_LINKLOCAL)\n\t\tconn->c_bound_if = dev_if;\n\telse\n#endif\n\t\tconn->c_bound_if = 0;\n\n\trds_conn_net_set(conn, net);\n\n\tret = rds_cong_get_maps(conn);\n\tif (ret) {\n\t\tkfree(conn->c_path);\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This is where a connection becomes loopback.  If *any* RDS sockets\n\t * can bind to the destination address then we'd rather the messages\n\t * flow through loopback rather than either transport.\n\t */\n\tloop_trans = rds_trans_get_preferred(net, faddr, conn->c_dev_if);\n\tif (loop_trans) {\n\t\trds_trans_put(loop_trans);\n\t\tconn->c_loopback = 1;\n\t\tif (trans->t_prefer_loopback) {\n\t\t\tif (likely(is_outgoing)) {\n\t\t\t\t/* \"outgoing\" connection to local address.\n\t\t\t\t * Protocol says it wants the connection\n\t\t\t\t * handled by the loopback transport.\n\t\t\t\t * This is what TCP does.\n\t\t\t\t */\n\t\t\t\ttrans = &rds_loop_transport;\n\t\t\t} else {\n\t\t\t\t/* No transport currently in use\n\t\t\t\t * should end up here, but if it\n\t\t\t\t * does, reset/destroy the connection.\n\t\t\t\t */\n\t\t\t\tkfree(conn->c_path);\n\t\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\t\tconn = ERR_PTR(-EOPNOTSUPP);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tconn->c_trans = trans;\n\n\tinit_waitqueue_head(&conn->c_hs_waitq);\n\tfor (i = 0; i < npaths; i++) {\n\t\t__rds_conn_path_init(conn, &conn->c_path[i],\n\t\t\t\t     is_outgoing);\n\t\tconn->c_path[i].cp_index = i;\n\t}\n\trcu_read_lock();\n\tif (rds_destroy_pending(conn))\n\t\tret = -ENETDOWN;\n\telse\n\t\tret = trans->conn_alloc(conn, GFP_ATOMIC);\n\tif (ret) {\n\t\trcu_read_unlock();\n\t\tkfree(conn->c_path);\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"allocated conn %p for %pI6c -> %pI6c over %s %s\\n\",\n\t\t conn, laddr, faddr,\n\t\t strnlen(trans->t_name, sizeof(trans->t_name)) ?\n\t\t trans->t_name : \"[unknown]\", is_outgoing ? \"(outgoing)\" : \"\");\n\n\t/*\n\t * Since we ran without holding the conn lock, someone could\n\t * have created the same conn (either normal or passive) in the\n\t * interim. We check while holding the lock. If we won, we complete\n\t * init and return our conn. If we lost, we rollback and return the\n\t * other one.\n\t */\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\tif (parent) {\n\t\t/* Creating passive conn */\n\t\tif (parent->c_passive) {\n\t\t\ttrans->conn_free(conn->c_path[0].cp_transport_data);\n\t\t\tkfree(conn->c_path);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = parent->c_passive;\n\t\t} else {\n\t\t\tparent->c_passive = conn;\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t} else {\n\t\t/* Creating normal conn */\n\t\tstruct rds_connection *found;\n\n\t\tfound = rds_conn_lookup(net, head, laddr, faddr, trans,\n\t\t\t\t\ttos, dev_if);\n\t\tif (found) {\n\t\t\tstruct rds_conn_path *cp;\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < npaths; i++) {\n\t\t\t\tcp = &conn->c_path[i];\n\t\t\t\t/* The ->conn_alloc invocation may have\n\t\t\t\t * allocated resource for all paths, so all\n\t\t\t\t * of them may have to be freed here.\n\t\t\t\t */\n\t\t\t\tif (cp->cp_transport_data)\n\t\t\t\t\ttrans->conn_free(cp->cp_transport_data);\n\t\t\t}\n\t\t\tkfree(conn->c_path);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = found;\n\t\t} else {\n\t\t\tconn->c_my_gen_num = rds_gen_num;\n\t\t\tconn->c_peer_gen_num = 0;\n\t\t\thlist_add_head_rcu(&conn->c_hash_node, head);\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n\trcu_read_unlock();\n\nout:\n\treturn conn;\n}\n\nstruct rds_connection *rds_conn_create(struct net *net,\n\t\t\t\t       const struct in6_addr *laddr,\n\t\t\t\t       const struct in6_addr *faddr,\n\t\t\t\t       struct rds_transport *trans, u8 tos,\n\t\t\t\t       gfp_t gfp, int dev_if)\n{\n\treturn __rds_conn_create(net, laddr, faddr, trans, gfp, tos, 0, dev_if);\n}\nEXPORT_SYMBOL_GPL(rds_conn_create);\n\nstruct rds_connection *rds_conn_create_outgoing(struct net *net,\n\t\t\t\t\t\tconst struct in6_addr *laddr,\n\t\t\t\t\t\tconst struct in6_addr *faddr,\n\t\t\t\t\t\tstruct rds_transport *trans,\n\t\t\t\t\t\tu8 tos, gfp_t gfp, int dev_if)\n{\n\treturn __rds_conn_create(net, laddr, faddr, trans, gfp, tos, 1, dev_if);\n}\nEXPORT_SYMBOL_GPL(rds_conn_create_outgoing);\n\nvoid rds_conn_shutdown(struct rds_conn_path *cp)\n{\n\tstruct rds_connection *conn = cp->cp_conn;\n\n\t/* shut it down unless it's down already */\n\tif (!rds_conn_path_transition(cp, RDS_CONN_DOWN, RDS_CONN_DOWN)) {\n\t\t/*\n\t\t * Quiesce the connection mgmt handlers before we start tearing\n\t\t * things down. We don't hold the mutex for the entire\n\t\t * duration of the shutdown operation, else we may be\n\t\t * deadlocking with the CM handler. Instead, the CM event\n\t\t * handler is supposed to check for state DISCONNECTING\n\t\t */\n\t\tmutex_lock(&cp->cp_cm_lock);\n\t\tif (!rds_conn_path_transition(cp, RDS_CONN_UP,\n\t\t\t\t\t      RDS_CONN_DISCONNECTING) &&\n\t\t    !rds_conn_path_transition(cp, RDS_CONN_ERROR,\n\t\t\t\t\t      RDS_CONN_DISCONNECTING)) {\n\t\t\trds_conn_path_error(cp,\n\t\t\t\t\t    \"shutdown called in state %d\\n\",\n\t\t\t\t\t    atomic_read(&cp->cp_state));\n\t\t\tmutex_unlock(&cp->cp_cm_lock);\n\t\t\treturn;\n\t\t}\n\t\tmutex_unlock(&cp->cp_cm_lock);\n\n\t\twait_event(cp->cp_waitq,\n\t\t\t   !test_bit(RDS_IN_XMIT, &cp->cp_flags));\n\t\twait_event(cp->cp_waitq,\n\t\t\t   !test_bit(RDS_RECV_REFILL, &cp->cp_flags));\n\n\t\tconn->c_trans->conn_path_shutdown(cp);\n\t\trds_conn_path_reset(cp);\n\n\t\tif (!rds_conn_path_transition(cp, RDS_CONN_DISCONNECTING,\n\t\t\t\t\t      RDS_CONN_DOWN) &&\n\t\t    !rds_conn_path_transition(cp, RDS_CONN_ERROR,\n\t\t\t\t\t      RDS_CONN_DOWN)) {\n\t\t\t/* This can happen - eg when we're in the middle of tearing\n\t\t\t * down the connection, and someone unloads the rds module.\n\t\t\t * Quite reproducible with loopback connections.\n\t\t\t * Mostly harmless.\n\t\t\t *\n\t\t\t * Note that this also happens with rds-tcp because\n\t\t\t * we could have triggered rds_conn_path_drop in irq\n\t\t\t * mode from rds_tcp_state change on the receipt of\n\t\t\t * a FIN, thus we need to recheck for RDS_CONN_ERROR\n\t\t\t * here.\n\t\t\t */\n\t\t\trds_conn_path_error(cp, \"%s: failed to transition \"\n\t\t\t\t\t    \"to state DOWN, current state \"\n\t\t\t\t\t    \"is %d\\n\", __func__,\n\t\t\t\t\t    atomic_read(&cp->cp_state));\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Then reconnect if it's still live.\n\t * The passive side of an IB loopback connection is never added\n\t * to the conn hash, so we never trigger a reconnect on this\n\t * conn - the reconnect is always triggered by the active peer. */\n\tcancel_delayed_work_sync(&cp->cp_conn_w);\n\trcu_read_lock();\n\tif (!hlist_unhashed(&conn->c_hash_node)) {\n\t\trcu_read_unlock();\n\t\trds_queue_reconnect(cp);\n\t} else {\n\t\trcu_read_unlock();\n\t}\n}\n\n/* destroy a single rds_conn_path. rds_conn_destroy() iterates over\n * all paths using rds_conn_path_destroy()\n */\nstatic void rds_conn_path_destroy(struct rds_conn_path *cp)\n{\n\tstruct rds_message *rm, *rtmp;\n\n\tif (!cp->cp_transport_data)\n\t\treturn;\n\n\t/* make sure lingering queued work won't try to ref the conn */\n\tcancel_delayed_work_sync(&cp->cp_send_w);\n\tcancel_delayed_work_sync(&cp->cp_recv_w);\n\n\trds_conn_path_drop(cp, true);\n\tflush_work(&cp->cp_down_w);\n\n\t/* tear down queued messages */\n\tlist_for_each_entry_safe(rm, rtmp,\n\t\t\t\t &cp->cp_send_queue,\n\t\t\t\t m_conn_item) {\n\t\tlist_del_init(&rm->m_conn_item);\n\t\tBUG_ON(!list_empty(&rm->m_sock_item));\n\t\trds_message_put(rm);\n\t}\n\tif (cp->cp_xmit_rm)\n\t\trds_message_put(cp->cp_xmit_rm);\n\n\tWARN_ON(delayed_work_pending(&cp->cp_send_w));\n\tWARN_ON(delayed_work_pending(&cp->cp_recv_w));\n\tWARN_ON(delayed_work_pending(&cp->cp_conn_w));\n\tWARN_ON(work_pending(&cp->cp_down_w));\n\n\tcp->cp_conn->c_trans->conn_free(cp->cp_transport_data);\n}\n\n/*\n * Stop and free a connection.\n *\n * This can only be used in very limited circumstances.  It assumes that once\n * the conn has been shutdown that no one else is referencing the connection.\n * We can only ensure this in the rmmod path in the current code.\n */\nvoid rds_conn_destroy(struct rds_connection *conn)\n{\n\tunsigned long flags;\n\tint i;\n\tstruct rds_conn_path *cp;\n\tint npaths = (conn->c_trans->t_mp_capable ? RDS_MPATH_WORKERS : 1);\n\n\trdsdebug(\"freeing conn %p for %pI4 -> \"\n\t\t \"%pI4\\n\", conn, &conn->c_laddr,\n\t\t &conn->c_faddr);\n\n\t/* Ensure conn will not be scheduled for reconnect */\n\tspin_lock_irq(&rds_conn_lock);\n\thlist_del_init_rcu(&conn->c_hash_node);\n\tspin_unlock_irq(&rds_conn_lock);\n\tsynchronize_rcu();\n\n\t/* shut the connection down */\n\tfor (i = 0; i < npaths; i++) {\n\t\tcp = &conn->c_path[i];\n\t\trds_conn_path_destroy(cp);\n\t\tBUG_ON(!list_empty(&cp->cp_retrans));\n\t}\n\n\t/*\n\t * The congestion maps aren't freed up here.  They're\n\t * freed by rds_cong_exit() after all the connections\n\t * have been freed.\n\t */\n\trds_cong_remove_conn(conn);\n\n\tkfree(conn->c_path);\n\tkmem_cache_free(rds_conn_slab, conn);\n\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\trds_conn_count--;\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n}\nEXPORT_SYMBOL_GPL(rds_conn_destroy);\n\nstatic void __rds_inc_msg_cp(struct rds_incoming *inc,\n\t\t\t     struct rds_info_iterator *iter,\n\t\t\t     void *saddr, void *daddr, int flip, bool isv6)\n{\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (isv6)\n\t\trds6_inc_info_copy(inc, iter, saddr, daddr, flip);\n\telse\n#endif\n\t\trds_inc_info_copy(inc, iter, *(__be32 *)saddr,\n\t\t\t\t  *(__be32 *)daddr, flip);\n}\n\nstatic void rds_conn_message_info_cmn(struct socket *sock, unsigned int len,\n\t\t\t\t      struct rds_info_iterator *iter,\n\t\t\t\t      struct rds_info_lengths *lens,\n\t\t\t\t      int want_send, bool isv6)\n{\n\tstruct hlist_head *head;\n\tstruct list_head *list;\n\tstruct rds_connection *conn;\n\tstruct rds_message *rm;\n\tunsigned int total = 0;\n\tunsigned long flags;\n\tsize_t i;\n\tint j;\n\n\tif (isv6)\n\t\tlen /= sizeof(struct rds6_info_message);\n\telse\n\t\tlen /= sizeof(struct rds_info_message);\n\n\trcu_read_lock();\n\n\tfor (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);\n\t     i++, head++) {\n\t\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\t\t\tstruct rds_conn_path *cp;\n\t\t\tint npaths;\n\n\t\t\tif (!isv6 && conn->c_isv6)\n\t\t\t\tcontinue;\n\n\t\t\tnpaths = (conn->c_trans->t_mp_capable ?\n\t\t\t\t RDS_MPATH_WORKERS : 1);\n\n\t\t\tfor (j = 0; j < npaths; j++) {\n\t\t\t\tcp = &conn->c_path[j];\n\t\t\t\tif (want_send)\n\t\t\t\t\tlist = &cp->cp_send_queue;\n\t\t\t\telse\n\t\t\t\t\tlist = &cp->cp_retrans;\n\n\t\t\t\tspin_lock_irqsave(&cp->cp_lock, flags);\n\n\t\t\t\t/* XXX too lazy to maintain counts.. */\n\t\t\t\tlist_for_each_entry(rm, list, m_conn_item) {\n\t\t\t\t\ttotal++;\n\t\t\t\t\tif (total <= len)\n\t\t\t\t\t\t__rds_inc_msg_cp(&rm->m_inc,\n\t\t\t\t\t\t\t\t iter,\n\t\t\t\t\t\t\t\t &conn->c_laddr,\n\t\t\t\t\t\t\t\t &conn->c_faddr,\n\t\t\t\t\t\t\t\t 0, isv6);\n\t\t\t\t}\n\n\t\t\t\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tlens->nr = total;\n\tif (isv6)\n\t\tlens->each = sizeof(struct rds6_info_message);\n\telse\n\t\tlens->each = sizeof(struct rds_info_message);\n}\n\nstatic void rds_conn_message_info(struct socket *sock, unsigned int len,\n\t\t\t\t  struct rds_info_iterator *iter,\n\t\t\t\t  struct rds_info_lengths *lens,\n\t\t\t\t  int want_send)\n{\n\trds_conn_message_info_cmn(sock, len, iter, lens, want_send, false);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic void rds6_conn_message_info(struct socket *sock, unsigned int len,\n\t\t\t\t   struct rds_info_iterator *iter,\n\t\t\t\t   struct rds_info_lengths *lens,\n\t\t\t\t   int want_send)\n{\n\trds_conn_message_info_cmn(sock, len, iter, lens, want_send, true);\n}\n#endif\n\nstatic void rds_conn_message_info_send(struct socket *sock, unsigned int len,\n\t\t\t\t       struct rds_info_iterator *iter,\n\t\t\t\t       struct rds_info_lengths *lens)\n{\n\trds_conn_message_info(sock, len, iter, lens, 1);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic void rds6_conn_message_info_send(struct socket *sock, unsigned int len,\n\t\t\t\t\tstruct rds_info_iterator *iter,\n\t\t\t\t\tstruct rds_info_lengths *lens)\n{\n\trds6_conn_message_info(sock, len, iter, lens, 1);\n}\n#endif\n\nstatic void rds_conn_message_info_retrans(struct socket *sock,\n\t\t\t\t\t  unsigned int len,\n\t\t\t\t\t  struct rds_info_iterator *iter,\n\t\t\t\t\t  struct rds_info_lengths *lens)\n{\n\trds_conn_message_info(sock, len, iter, lens, 0);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic void rds6_conn_message_info_retrans(struct socket *sock,\n\t\t\t\t\t   unsigned int len,\n\t\t\t\t\t   struct rds_info_iterator *iter,\n\t\t\t\t\t   struct rds_info_lengths *lens)\n{\n\trds6_conn_message_info(sock, len, iter, lens, 0);\n}\n#endif\n\nvoid rds_for_each_conn_info(struct socket *sock, unsigned int len,\n\t\t\t  struct rds_info_iterator *iter,\n\t\t\t  struct rds_info_lengths *lens,\n\t\t\t  int (*visitor)(struct rds_connection *, void *),\n\t\t\t  u64 *buffer,\n\t\t\t  size_t item_len)\n{\n\tstruct hlist_head *head;\n\tstruct rds_connection *conn;\n\tsize_t i;\n\n\trcu_read_lock();\n\n\tlens->nr = 0;\n\tlens->each = item_len;\n\n\tfor (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);\n\t     i++, head++) {\n\t\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\n\t\t\t/* XXX no c_lock usage.. */\n\t\t\tif (!visitor(conn, buffer))\n\t\t\t\tcontinue;\n\n\t\t\t/* We copy as much as we can fit in the buffer,\n\t\t\t * but we count all items so that the caller\n\t\t\t * can resize the buffer. */\n\t\t\tif (len >= item_len) {\n\t\t\t\trds_info_copy(iter, buffer, item_len);\n\t\t\t\tlen -= item_len;\n\t\t\t}\n\t\t\tlens->nr++;\n\t\t}\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(rds_for_each_conn_info);\n\nstatic void rds_walk_conn_path_info(struct socket *sock, unsigned int len,\n\t\t\t\t    struct rds_info_iterator *iter,\n\t\t\t\t    struct rds_info_lengths *lens,\n\t\t\t\t    int (*visitor)(struct rds_conn_path *, void *),\n\t\t\t\t    u64 *buffer,\n\t\t\t\t    size_t item_len)\n{\n\tstruct hlist_head *head;\n\tstruct rds_connection *conn;\n\tsize_t i;\n\n\trcu_read_lock();\n\n\tlens->nr = 0;\n\tlens->each = item_len;\n\n\tfor (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);\n\t     i++, head++) {\n\t\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\t\t\tstruct rds_conn_path *cp;\n\n\t\t\t/* XXX We only copy the information from the first\n\t\t\t * path for now.  The problem is that if there are\n\t\t\t * more than one underlying paths, we cannot report\n\t\t\t * information of all of them using the existing\n\t\t\t * API.  For example, there is only one next_tx_seq,\n\t\t\t * which path's next_tx_seq should we report?  It is\n\t\t\t * a bug in the design of MPRDS.\n\t\t\t */\n\t\t\tcp = conn->c_path;\n\n\t\t\t/* XXX no cp_lock usage.. */\n\t\t\tif (!visitor(cp, buffer))\n\t\t\t\tcontinue;\n\n\t\t\t/* We copy as much as we can fit in the buffer,\n\t\t\t * but we count all items so that the caller\n\t\t\t * can resize the buffer.\n\t\t\t */\n\t\t\tif (len >= item_len) {\n\t\t\t\trds_info_copy(iter, buffer, item_len);\n\t\t\t\tlen -= item_len;\n\t\t\t}\n\t\t\tlens->nr++;\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\nstatic int rds_conn_info_visitor(struct rds_conn_path *cp, void *buffer)\n{\n\tstruct rds_info_connection *cinfo = buffer;\n\tstruct rds_connection *conn = cp->cp_conn;\n\n\tif (conn->c_isv6)\n\t\treturn 0;\n\n\tcinfo->next_tx_seq = cp->cp_next_tx_seq;\n\tcinfo->next_rx_seq = cp->cp_next_rx_seq;\n\tcinfo->laddr = conn->c_laddr.s6_addr32[3];\n\tcinfo->faddr = conn->c_faddr.s6_addr32[3];\n\tcinfo->tos = conn->c_tos;\n\tstrncpy(cinfo->transport, conn->c_trans->t_name,\n\t\tsizeof(cinfo->transport));\n\tcinfo->flags = 0;\n\n\trds_conn_info_set(cinfo->flags, test_bit(RDS_IN_XMIT, &cp->cp_flags),\n\t\t\t  SENDING);\n\t/* XXX Future: return the state rather than these funky bits */\n\trds_conn_info_set(cinfo->flags,\n\t\t\t  atomic_read(&cp->cp_state) == RDS_CONN_CONNECTING,\n\t\t\t  CONNECTING);\n\trds_conn_info_set(cinfo->flags,\n\t\t\t  atomic_read(&cp->cp_state) == RDS_CONN_UP,\n\t\t\t  CONNECTED);\n\treturn 1;\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic int rds6_conn_info_visitor(struct rds_conn_path *cp, void *buffer)\n{\n\tstruct rds6_info_connection *cinfo6 = buffer;\n\tstruct rds_connection *conn = cp->cp_conn;\n\n\tcinfo6->next_tx_seq = cp->cp_next_tx_seq;\n\tcinfo6->next_rx_seq = cp->cp_next_rx_seq;\n\tcinfo6->laddr = conn->c_laddr;\n\tcinfo6->faddr = conn->c_faddr;\n\tstrncpy(cinfo6->transport, conn->c_trans->t_name,\n\t\tsizeof(cinfo6->transport));\n\tcinfo6->flags = 0;\n\n\trds_conn_info_set(cinfo6->flags, test_bit(RDS_IN_XMIT, &cp->cp_flags),\n\t\t\t  SENDING);\n\t/* XXX Future: return the state rather than these funky bits */\n\trds_conn_info_set(cinfo6->flags,\n\t\t\t  atomic_read(&cp->cp_state) == RDS_CONN_CONNECTING,\n\t\t\t  CONNECTING);\n\trds_conn_info_set(cinfo6->flags,\n\t\t\t  atomic_read(&cp->cp_state) == RDS_CONN_UP,\n\t\t\t  CONNECTED);\n\t/* Just return 1 as there is no error case. This is a helper function\n\t * for rds_walk_conn_path_info() and it wants a return value.\n\t */\n\treturn 1;\n}\n#endif\n\nstatic void rds_conn_info(struct socket *sock, unsigned int len,\n\t\t\t  struct rds_info_iterator *iter,\n\t\t\t  struct rds_info_lengths *lens)\n{\n\tu64 buffer[(sizeof(struct rds_info_connection) + 7) / 8];\n\n\trds_walk_conn_path_info(sock, len, iter, lens,\n\t\t\t\trds_conn_info_visitor,\n\t\t\t\tbuffer,\n\t\t\t\tsizeof(struct rds_info_connection));\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic void rds6_conn_info(struct socket *sock, unsigned int len,\n\t\t\t   struct rds_info_iterator *iter,\n\t\t\t   struct rds_info_lengths *lens)\n{\n\tu64 buffer[(sizeof(struct rds6_info_connection) + 7) / 8];\n\n\trds_walk_conn_path_info(sock, len, iter, lens,\n\t\t\t\trds6_conn_info_visitor,\n\t\t\t\tbuffer,\n\t\t\t\tsizeof(struct rds6_info_connection));\n}\n#endif\n\nint rds_conn_init(void)\n{\n\tint ret;\n\n\tret = rds_loop_net_init(); /* register pernet callback */\n\tif (ret)\n\t\treturn ret;\n\n\trds_conn_slab = kmem_cache_create(\"rds_connection\",\n\t\t\t\t\t  sizeof(struct rds_connection),\n\t\t\t\t\t  0, 0, NULL);\n\tif (!rds_conn_slab) {\n\t\trds_loop_net_exit();\n\t\treturn -ENOMEM;\n\t}\n\n\trds_info_register_func(RDS_INFO_CONNECTIONS, rds_conn_info);\n\trds_info_register_func(RDS_INFO_SEND_MESSAGES,\n\t\t\t       rds_conn_message_info_send);\n\trds_info_register_func(RDS_INFO_RETRANS_MESSAGES,\n\t\t\t       rds_conn_message_info_retrans);\n#if IS_ENABLED(CONFIG_IPV6)\n\trds_info_register_func(RDS6_INFO_CONNECTIONS, rds6_conn_info);\n\trds_info_register_func(RDS6_INFO_SEND_MESSAGES,\n\t\t\t       rds6_conn_message_info_send);\n\trds_info_register_func(RDS6_INFO_RETRANS_MESSAGES,\n\t\t\t       rds6_conn_message_info_retrans);\n#endif\n\treturn 0;\n}\n\nvoid rds_conn_exit(void)\n{\n\trds_loop_net_exit(); /* unregister pernet callback */\n\trds_loop_exit();\n\n\tWARN_ON(!hlist_empty(rds_conn_hash));\n\n\tkmem_cache_destroy(rds_conn_slab);\n\n\trds_info_deregister_func(RDS_INFO_CONNECTIONS, rds_conn_info);\n\trds_info_deregister_func(RDS_INFO_SEND_MESSAGES,\n\t\t\t\t rds_conn_message_info_send);\n\trds_info_deregister_func(RDS_INFO_RETRANS_MESSAGES,\n\t\t\t\t rds_conn_message_info_retrans);\n#if IS_ENABLED(CONFIG_IPV6)\n\trds_info_deregister_func(RDS6_INFO_CONNECTIONS, rds6_conn_info);\n\trds_info_deregister_func(RDS6_INFO_SEND_MESSAGES,\n\t\t\t\t rds6_conn_message_info_send);\n\trds_info_deregister_func(RDS6_INFO_RETRANS_MESSAGES,\n\t\t\t\t rds6_conn_message_info_retrans);\n#endif\n}\n\n/*\n * Force a disconnect\n */\nvoid rds_conn_path_drop(struct rds_conn_path *cp, bool destroy)\n{\n\tatomic_set(&cp->cp_state, RDS_CONN_ERROR);\n\n\trcu_read_lock();\n\tif (!destroy && rds_destroy_pending(cp->cp_conn)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tqueue_work(rds_wq, &cp->cp_down_w);\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(rds_conn_path_drop);\n\nvoid rds_conn_drop(struct rds_connection *conn)\n{\n\tWARN_ON(conn->c_trans->t_mp_capable);\n\trds_conn_path_drop(&conn->c_path[0], false);\n}\nEXPORT_SYMBOL_GPL(rds_conn_drop);\n\n/*\n * If the connection is down, trigger a connect. We may have scheduled a\n * delayed reconnect however - in this case we should not interfere.\n */\nvoid rds_conn_path_connect_if_down(struct rds_conn_path *cp)\n{\n\trcu_read_lock();\n\tif (rds_destroy_pending(cp->cp_conn)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tif (rds_conn_path_state(cp) == RDS_CONN_DOWN &&\n\t    !test_and_set_bit(RDS_RECONNECT_PENDING, &cp->cp_flags))\n\t\tqueue_delayed_work(rds_wq, &cp->cp_conn_w, 0);\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(rds_conn_path_connect_if_down);\n\n/* Check connectivity of all paths\n */\nvoid rds_check_all_paths(struct rds_connection *conn)\n{\n\tint i = 0;\n\n\tdo {\n\t\trds_conn_path_connect_if_down(&conn->c_path[i]);\n\t} while (++i < conn->c_npaths);\n}\n\nvoid rds_conn_connect_if_down(struct rds_connection *conn)\n{\n\tWARN_ON(conn->c_trans->t_mp_capable);\n\trds_conn_path_connect_if_down(&conn->c_path[0]);\n}\nEXPORT_SYMBOL_GPL(rds_conn_connect_if_down);\n\nvoid\n__rds_conn_path_error(struct rds_conn_path *cp, const char *fmt, ...)\n{\n\tva_list ap;\n\n\tva_start(ap, fmt);\n\tvprintk(fmt, ap);\n\tva_end(ap);\n\n\trds_conn_path_drop(cp, false);\n}\n"], "filenames": ["net/rds/connection.c"], "buggy_code_start_loc": [255], "buggy_code_end_loc": [255], "fixing_code_start_loc": [256], "fixing_code_end_loc": [257], "type": "CWE-401", "message": "An issue was discovered in the Linux kernel before 5.15.11. There is a memory leak in the __rds_conn_create() function in net/rds/connection.c in a certain combination of circumstances.", "other": {"cve": {"id": "CVE-2021-45480", "sourceIdentifier": "cve@mitre.org", "published": "2021-12-24T23:15:07.237", "lastModified": "2022-04-06T14:11:00.587", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in the Linux kernel before 5.15.11. There is a memory leak in the __rds_conn_create() function in net/rds/connection.c in a certain combination of circumstances."}, {"lang": "es", "value": "Se ha detectado un problema en el kernel de Linux versiones anteriores a 5.15.11. Se presenta una p\u00e9rdida de memoria en la funci\u00f3n __rds_conn_create() en el archivo net/rds/connection.c en una determinada combinaci\u00f3n de circunstancias"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.7}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-401"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.15.11", "matchCriteriaId": "B271E1AF-BC45-4C3E-9614-055846520592"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:11.0:*:*:*:*:*:*:*", "matchCriteriaId": "FA6FEEC2-9F11-4643-8827-749718254FED"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.15.11", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/5f9562ebe710c307adc5f666bf1a2162ee7977c0", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/03/msg00012.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://www.debian.org/security/2022/dsa-5050", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2022/dsa-5096", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/5f9562ebe710c307adc5f666bf1a2162ee7977c0"}}