{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/pipe.c\n *\n *  Copyright (C) 1991, 1992, 1999  Linus Torvalds\n */\n\n#include <linux/mm.h>\n#include <linux/file.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/log2.h>\n#include <linux/mount.h>\n#include <linux/pseudo_fs.h>\n#include <linux/magic.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/uio.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/fcntl.h>\n#include <linux/memcontrol.h>\n#include <linux/watch_queue.h>\n#include <linux/sysctl.h>\n\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n\n#include \"internal.h\"\n\n/*\n * New pipe buffers will be restricted to this size while the user is exceeding\n * their pipe buffer quota. The general pipe use case needs at least two\n * buffers: one for data yet to be read, and one for new data. If this is less\n * than two, then a write to a non-empty pipe may block even if the pipe is not\n * full. This can occur with GNU make jobserver or similar uses of pipes as\n * semaphores: multiple processes may be waiting to write tokens back to the\n * pipe before reading tokens: https://lore.kernel.org/lkml/1628086770.5rn8p04n6j.none@localhost/.\n *\n * Users can reduce their pipe buffers with F_SETPIPE_SZ below this at their\n * own risk, namely: pipe writes to non-full pipes may block until the pipe is\n * emptied.\n */\n#define PIPE_MIN_DEF_BUFFERS 2\n\n/*\n * The max size that a non-root user is allowed to grow the pipe. Can\n * be set by root in /proc/sys/fs/pipe-max-size\n */\nstatic unsigned int pipe_max_size = 1048576;\n\n/* Maximum allocatable pages per user. Hard limit is unset by default, soft\n * matches default values.\n */\nstatic unsigned long pipe_user_pages_hard;\nstatic unsigned long pipe_user_pages_soft = PIPE_DEF_BUFFERS * INR_OPEN_CUR;\n\n/*\n * We use head and tail indices that aren't masked off, except at the point of\n * dereference, but rather they're allowed to wrap naturally.  This means there\n * isn't a dead spot in the buffer, but the ring has to be a power of two and\n * <= 2^31.\n * -- David Howells 2019-09-23.\n *\n * Reads with count = 0 should always return 0.\n * -- Julian Bradfield 1999-06-07.\n *\n * FIFOs and Pipes now generate SIGIO for both readers and writers.\n * -- Jeremy Elson <jelson@circlemud.org> 2001-08-16\n *\n * pipe_read & write cleanup\n * -- Manfred Spraul <manfred@colorfullife.com> 2002-05-09\n */\n\nstatic void pipe_lock_nested(struct pipe_inode_info *pipe, int subclass)\n{\n\tif (pipe->files)\n\t\tmutex_lock_nested(&pipe->mutex, subclass);\n}\n\nvoid pipe_lock(struct pipe_inode_info *pipe)\n{\n\t/*\n\t * pipe_lock() nests non-pipe inode locks (for writing to a file)\n\t */\n\tpipe_lock_nested(pipe, I_MUTEX_PARENT);\n}\nEXPORT_SYMBOL(pipe_lock);\n\nvoid pipe_unlock(struct pipe_inode_info *pipe)\n{\n\tif (pipe->files)\n\t\tmutex_unlock(&pipe->mutex);\n}\nEXPORT_SYMBOL(pipe_unlock);\n\nstatic inline void __pipe_lock(struct pipe_inode_info *pipe)\n{\n\tmutex_lock_nested(&pipe->mutex, I_MUTEX_PARENT);\n}\n\nstatic inline void __pipe_unlock(struct pipe_inode_info *pipe)\n{\n\tmutex_unlock(&pipe->mutex);\n}\n\nvoid pipe_double_lock(struct pipe_inode_info *pipe1,\n\t\t      struct pipe_inode_info *pipe2)\n{\n\tBUG_ON(pipe1 == pipe2);\n\n\tif (pipe1 < pipe2) {\n\t\tpipe_lock_nested(pipe1, I_MUTEX_PARENT);\n\t\tpipe_lock_nested(pipe2, I_MUTEX_CHILD);\n\t} else {\n\t\tpipe_lock_nested(pipe2, I_MUTEX_PARENT);\n\t\tpipe_lock_nested(pipe1, I_MUTEX_CHILD);\n\t}\n}\n\nstatic void anon_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t  struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\t/*\n\t * If nobody else uses this page, and we don't already have a\n\t * temporary page, let's keep track of it as a one-deep\n\t * allocation cache. (Otherwise just release our reference to it)\n\t */\n\tif (page_count(page) == 1 && !pipe->tmp_page)\n\t\tpipe->tmp_page = page;\n\telse\n\t\tput_page(page);\n}\n\nstatic bool anon_pipe_buf_try_steal(struct pipe_inode_info *pipe,\n\t\tstruct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\tif (page_count(page) != 1)\n\t\treturn false;\n\tmemcg_kmem_uncharge_page(page, 0);\n\t__SetPageLocked(page);\n\treturn true;\n}\n\n/**\n * generic_pipe_buf_try_steal - attempt to take ownership of a &pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to attempt to steal\n *\n * Description:\n *\tThis function attempts to steal the &struct page attached to\n *\t@buf. If successful, this function returns 0 and returns with\n *\tthe page locked. The caller may then reuse the page for whatever\n *\the wishes; the typical use is insertion into a different file\n *\tpage cache.\n */\nbool generic_pipe_buf_try_steal(struct pipe_inode_info *pipe,\n\t\tstruct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\t/*\n\t * A reference of one is golden, that means that the owner of this\n\t * page is the only one holding a reference to it. lock the page\n\t * and return OK.\n\t */\n\tif (page_count(page) == 1) {\n\t\tlock_page(page);\n\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL(generic_pipe_buf_try_steal);\n\n/**\n * generic_pipe_buf_get - get a reference to a &struct pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to get a reference to\n *\n * Description:\n *\tThis function grabs an extra reference to @buf. It's used in\n *\tthe tee() system call, when we duplicate the buffers in one\n *\tpipe into another.\n */\nbool generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n{\n\treturn try_get_page(buf->page);\n}\nEXPORT_SYMBOL(generic_pipe_buf_get);\n\n/**\n * generic_pipe_buf_release - put a reference to a &struct pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to put a reference to\n *\n * Description:\n *\tThis function releases a reference to @buf.\n */\nvoid generic_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t      struct pipe_buffer *buf)\n{\n\tput_page(buf->page);\n}\nEXPORT_SYMBOL(generic_pipe_buf_release);\n\nstatic const struct pipe_buf_operations anon_pipe_buf_ops = {\n\t.release\t= anon_pipe_buf_release,\n\t.try_steal\t= anon_pipe_buf_try_steal,\n\t.get\t\t= generic_pipe_buf_get,\n};\n\n/* Done while waiting without holding the pipe lock - thus the READ_ONCE() */\nstatic inline bool pipe_readable(const struct pipe_inode_info *pipe)\n{\n\tunsigned int head = READ_ONCE(pipe->head);\n\tunsigned int tail = READ_ONCE(pipe->tail);\n\tunsigned int writers = READ_ONCE(pipe->writers);\n\n\treturn !pipe_empty(head, tail) || !writers;\n}\n\nstatic ssize_t\npipe_read(struct kiocb *iocb, struct iov_iter *to)\n{\n\tsize_t total_len = iov_iter_count(to);\n\tstruct file *filp = iocb->ki_filp;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tbool was_full, wake_next_reader = false;\n\tssize_t ret;\n\n\t/* Null read succeeds. */\n\tif (unlikely(total_len == 0))\n\t\treturn 0;\n\n\tret = 0;\n\t__pipe_lock(pipe);\n\n\t/*\n\t * We only wake up writers if the pipe was full when we started\n\t * reading in order to avoid unnecessary wakeups.\n\t *\n\t * But when we do wake up writers, we do so using a sync wakeup\n\t * (WF_SYNC), because we want them to get going and generate more\n\t * data for us.\n\t */\n\twas_full = pipe_full(pipe->head, pipe->tail, pipe->max_usage);\n\tfor (;;) {\n\t\t/* Read ->head with a barrier vs post_one_notification() */\n\t\tunsigned int head = smp_load_acquire(&pipe->head);\n\t\tunsigned int tail = pipe->tail;\n\t\tunsigned int mask = pipe->ring_size - 1;\n\n#ifdef CONFIG_WATCH_QUEUE\n\t\tif (pipe->note_loss) {\n\t\t\tstruct watch_notification n;\n\n\t\t\tif (total_len < 8) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -ENOBUFS;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tn.type = WATCH_TYPE_META;\n\t\t\tn.subtype = WATCH_META_LOSS_NOTIFICATION;\n\t\t\tn.info = watch_sizeof(n);\n\t\t\tif (copy_to_iter(&n, sizeof(n), to) != sizeof(n)) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret += sizeof(n);\n\t\t\ttotal_len -= sizeof(n);\n\t\t\tpipe->note_loss = false;\n\t\t}\n#endif\n\n\t\tif (!pipe_empty(head, tail)) {\n\t\t\tstruct pipe_buffer *buf = &pipe->bufs[tail & mask];\n\t\t\tsize_t chars = buf->len;\n\t\t\tsize_t written;\n\t\t\tint error;\n\n\t\t\tif (chars > total_len) {\n\t\t\t\tif (buf->flags & PIPE_BUF_FLAG_WHOLE) {\n\t\t\t\t\tif (ret == 0)\n\t\t\t\t\t\tret = -ENOBUFS;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tchars = total_len;\n\t\t\t}\n\n\t\t\terror = pipe_buf_confirm(pipe, buf);\n\t\t\tif (error) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = error;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\twritten = copy_page_to_iter(buf->page, buf->offset, chars, to);\n\t\t\tif (unlikely(written < chars)) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret += chars;\n\t\t\tbuf->offset += chars;\n\t\t\tbuf->len -= chars;\n\n\t\t\t/* Was it a packet buffer? Clean up and exit */\n\t\t\tif (buf->flags & PIPE_BUF_FLAG_PACKET) {\n\t\t\t\ttotal_len = chars;\n\t\t\t\tbuf->len = 0;\n\t\t\t}\n\n\t\t\tif (!buf->len) {\n\t\t\t\tpipe_buf_release(pipe, buf);\n\t\t\t\tspin_lock_irq(&pipe->rd_wait.lock);\n#ifdef CONFIG_WATCH_QUEUE\n\t\t\t\tif (buf->flags & PIPE_BUF_FLAG_LOSS)\n\t\t\t\t\tpipe->note_loss = true;\n#endif\n\t\t\t\ttail++;\n\t\t\t\tpipe->tail = tail;\n\t\t\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\t\t}\n\t\t\ttotal_len -= chars;\n\t\t\tif (!total_len)\n\t\t\t\tbreak;\t/* common path: read succeeded */\n\t\t\tif (!pipe_empty(head, tail))\t/* More to do? */\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (!pipe->writers)\n\t\t\tbreak;\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\t__pipe_unlock(pipe);\n\n\t\t/*\n\t\t * We only get here if we didn't actually read anything.\n\t\t *\n\t\t * However, we could have seen (and removed) a zero-sized\n\t\t * pipe buffer, and might have made space in the buffers\n\t\t * that way.\n\t\t *\n\t\t * You can't make zero-sized pipe buffers by doing an empty\n\t\t * write (not even in packet mode), but they can happen if\n\t\t * the writer gets an EFAULT when trying to fill a buffer\n\t\t * that already got allocated and inserted in the buffer\n\t\t * array.\n\t\t *\n\t\t * So we still need to wake up any pending writers in the\n\t\t * _very_ unlikely case that the pipe was full, but we got\n\t\t * no data.\n\t\t */\n\t\tif (unlikely(was_full))\n\t\t\twake_up_interruptible_sync_poll(&pipe->wr_wait, EPOLLOUT | EPOLLWRNORM);\n\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\n\t\t/*\n\t\t * But because we didn't read anything, at this point we can\n\t\t * just return directly with -ERESTARTSYS if we're interrupted,\n\t\t * since we've done any required wakeups and there's no need\n\t\t * to mark anything accessed. And we've dropped the lock.\n\t\t */\n\t\tif (wait_event_interruptible_exclusive(pipe->rd_wait, pipe_readable(pipe)) < 0)\n\t\t\treturn -ERESTARTSYS;\n\n\t\t__pipe_lock(pipe);\n\t\twas_full = pipe_full(pipe->head, pipe->tail, pipe->max_usage);\n\t\twake_next_reader = true;\n\t}\n\tif (pipe_empty(pipe->head, pipe->tail))\n\t\twake_next_reader = false;\n\t__pipe_unlock(pipe);\n\n\tif (was_full)\n\t\twake_up_interruptible_sync_poll(&pipe->wr_wait, EPOLLOUT | EPOLLWRNORM);\n\tif (wake_next_reader)\n\t\twake_up_interruptible_sync_poll(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\tif (ret > 0)\n\t\tfile_accessed(filp);\n\treturn ret;\n}\n\nstatic inline int is_packetized(struct file *file)\n{\n\treturn (file->f_flags & O_DIRECT) != 0;\n}\n\n/* Done while waiting without holding the pipe lock - thus the READ_ONCE() */\nstatic inline bool pipe_writable(const struct pipe_inode_info *pipe)\n{\n\tunsigned int head = READ_ONCE(pipe->head);\n\tunsigned int tail = READ_ONCE(pipe->tail);\n\tunsigned int max_usage = READ_ONCE(pipe->max_usage);\n\n\treturn !pipe_full(head, tail, max_usage) ||\n\t\t!READ_ONCE(pipe->readers);\n}\n\nstatic ssize_t\npipe_write(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tunsigned int head;\n\tssize_t ret = 0;\n\tsize_t total_len = iov_iter_count(from);\n\tssize_t chars;\n\tbool was_empty = false;\n\tbool wake_next_writer = false;\n\n\t/* Null write succeeds. */\n\tif (unlikely(total_len == 0))\n\t\treturn 0;\n\n\t__pipe_lock(pipe);\n\n\tif (!pipe->readers) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tret = -EPIPE;\n\t\tgoto out;\n\t}\n\n#ifdef CONFIG_WATCH_QUEUE\n\tif (pipe->watch_queue) {\n\t\tret = -EXDEV;\n\t\tgoto out;\n\t}\n#endif\n\n\t/*\n\t * If it wasn't empty we try to merge new data into\n\t * the last buffer.\n\t *\n\t * That naturally merges small writes, but it also\n\t * page-aligns the rest of the writes for large writes\n\t * spanning multiple pages.\n\t */\n\thead = pipe->head;\n\twas_empty = pipe_empty(head, pipe->tail);\n\tchars = total_len & (PAGE_SIZE-1);\n\tif (chars && !was_empty) {\n\t\tunsigned int mask = pipe->ring_size - 1;\n\t\tstruct pipe_buffer *buf = &pipe->bufs[(head - 1) & mask];\n\t\tint offset = buf->offset + buf->len;\n\n\t\tif ((buf->flags & PIPE_BUF_FLAG_CAN_MERGE) &&\n\t\t    offset + chars <= PAGE_SIZE) {\n\t\t\tret = pipe_buf_confirm(pipe, buf);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tret = copy_page_from_iter(buf->page, offset, chars, from);\n\t\t\tif (unlikely(ret < chars)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tbuf->len += ret;\n\t\t\tif (!iov_iter_count(from))\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor (;;) {\n\t\tif (!pipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\thead = pipe->head;\n\t\tif (!pipe_full(head, pipe->tail, pipe->max_usage)) {\n\t\t\tunsigned int mask = pipe->ring_size - 1;\n\t\t\tstruct pipe_buffer *buf = &pipe->bufs[head & mask];\n\t\t\tstruct page *page = pipe->tmp_page;\n\t\t\tint copied;\n\n\t\t\tif (!page) {\n\t\t\t\tpage = alloc_page(GFP_HIGHUSER | __GFP_ACCOUNT);\n\t\t\t\tif (unlikely(!page)) {\n\t\t\t\t\tret = ret ? : -ENOMEM;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tpipe->tmp_page = page;\n\t\t\t}\n\n\t\t\t/* Allocate a slot in the ring in advance and attach an\n\t\t\t * empty buffer.  If we fault or otherwise fail to use\n\t\t\t * it, either the reader will consume it or it'll still\n\t\t\t * be there for the next write.\n\t\t\t */\n\t\t\tspin_lock_irq(&pipe->rd_wait.lock);\n\n\t\t\thead = pipe->head;\n\t\t\tif (pipe_full(head, pipe->tail, pipe->max_usage)) {\n\t\t\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tpipe->head = head + 1;\n\t\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\n\t\t\t/* Insert it into the buffer array */\n\t\t\tbuf = &pipe->bufs[head & mask];\n\t\t\tbuf->page = page;\n\t\t\tbuf->ops = &anon_pipe_buf_ops;\n\t\t\tbuf->offset = 0;\n\t\t\tbuf->len = 0;\n\t\t\tif (is_packetized(filp))\n\t\t\t\tbuf->flags = PIPE_BUF_FLAG_PACKET;\n\t\t\telse\n\t\t\t\tbuf->flags = PIPE_BUF_FLAG_CAN_MERGE;\n\t\t\tpipe->tmp_page = NULL;\n\n\t\t\tcopied = copy_page_from_iter(page, 0, PAGE_SIZE, from);\n\t\t\tif (unlikely(copied < PAGE_SIZE && iov_iter_count(from))) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret += copied;\n\t\t\tbuf->offset = 0;\n\t\t\tbuf->len = copied;\n\n\t\t\tif (!iov_iter_count(from))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!pipe_full(head, pipe->tail, pipe->max_usage))\n\t\t\tcontinue;\n\n\t\t/* Wait for buffer space to become available. */\n\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\tif (!ret)\n\t\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tif (!ret)\n\t\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We're going to release the pipe lock and wait for more\n\t\t * space. We wake up any readers if necessary, and then\n\t\t * after waiting we need to re-check whether the pipe\n\t\t * become empty while we dropped the lock.\n\t\t */\n\t\t__pipe_unlock(pipe);\n\t\tif (was_empty)\n\t\t\twake_up_interruptible_sync_poll(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t\twait_event_interruptible_exclusive(pipe->wr_wait, pipe_writable(pipe));\n\t\t__pipe_lock(pipe);\n\t\twas_empty = pipe_empty(pipe->head, pipe->tail);\n\t\twake_next_writer = true;\n\t}\nout:\n\tif (pipe_full(pipe->head, pipe->tail, pipe->max_usage))\n\t\twake_next_writer = false;\n\t__pipe_unlock(pipe);\n\n\t/*\n\t * If we do do a wakeup event, we do a 'sync' wakeup, because we\n\t * want the reader to start processing things asap, rather than\n\t * leave the data pending.\n\t *\n\t * This is particularly important for small writes, because of\n\t * how (for example) the GNU make jobserver uses small writes to\n\t * wake up pending jobs\n\t *\n\t * Epoll nonsensically wants a wakeup whether the pipe\n\t * was already empty or not.\n\t */\n\tif (was_empty || pipe->poll_usage)\n\t\twake_up_interruptible_sync_poll(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\tif (wake_next_writer)\n\t\twake_up_interruptible_sync_poll(&pipe->wr_wait, EPOLLOUT | EPOLLWRNORM);\n\tif (ret > 0 && sb_start_write_trylock(file_inode(filp)->i_sb)) {\n\t\tint err = file_update_time(filp);\n\t\tif (err)\n\t\t\tret = err;\n\t\tsb_end_write(file_inode(filp)->i_sb);\n\t}\n\treturn ret;\n}\n\nstatic long pipe_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tunsigned int count, head, tail, mask;\n\n\tswitch (cmd) {\n\tcase FIONREAD:\n\t\t__pipe_lock(pipe);\n\t\tcount = 0;\n\t\thead = pipe->head;\n\t\ttail = pipe->tail;\n\t\tmask = pipe->ring_size - 1;\n\n\t\twhile (tail != head) {\n\t\t\tcount += pipe->bufs[tail & mask].len;\n\t\t\ttail++;\n\t\t}\n\t\t__pipe_unlock(pipe);\n\n\t\treturn put_user(count, (int __user *)arg);\n\n#ifdef CONFIG_WATCH_QUEUE\n\tcase IOC_WATCH_QUEUE_SET_SIZE: {\n\t\tint ret;\n\t\t__pipe_lock(pipe);\n\t\tret = watch_queue_set_size(pipe, arg);\n\t\t__pipe_unlock(pipe);\n\t\treturn ret;\n\t}\n\n\tcase IOC_WATCH_QUEUE_SET_FILTER:\n\t\treturn watch_queue_set_filter(\n\t\t\tpipe, (struct watch_notification_filter __user *)arg);\n#endif\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n}\n\n/* No kernel lock held - fine */\nstatic __poll_t\npipe_poll(struct file *filp, poll_table *wait)\n{\n\t__poll_t mask;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tunsigned int head, tail;\n\n\t/* Epoll has some historical nasty semantics, this enables them */\n\tpipe->poll_usage = 1;\n\n\t/*\n\t * Reading pipe state only -- no need for acquiring the semaphore.\n\t *\n\t * But because this is racy, the code has to add the\n\t * entry to the poll table _first_ ..\n\t */\n\tif (filp->f_mode & FMODE_READ)\n\t\tpoll_wait(filp, &pipe->rd_wait, wait);\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tpoll_wait(filp, &pipe->wr_wait, wait);\n\n\t/*\n\t * .. and only then can you do the racy tests. That way,\n\t * if something changes and you got it wrong, the poll\n\t * table entry will wake you up and fix it.\n\t */\n\thead = READ_ONCE(pipe->head);\n\ttail = READ_ONCE(pipe->tail);\n\n\tmask = 0;\n\tif (filp->f_mode & FMODE_READ) {\n\t\tif (!pipe_empty(head, tail))\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\t\tif (!pipe->writers && filp->f_version != pipe->w_counter)\n\t\t\tmask |= EPOLLHUP;\n\t}\n\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tif (!pipe_full(head, tail, pipe->max_usage))\n\t\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t\t/*\n\t\t * Most Unices do not set EPOLLERR for FIFOs but on Linux they\n\t\t * behave exactly like pipes for poll().\n\t\t */\n\t\tif (!pipe->readers)\n\t\t\tmask |= EPOLLERR;\n\t}\n\n\treturn mask;\n}\n\nstatic void put_pipe_info(struct inode *inode, struct pipe_inode_info *pipe)\n{\n\tint kill = 0;\n\n\tspin_lock(&inode->i_lock);\n\tif (!--pipe->files) {\n\t\tinode->i_pipe = NULL;\n\t\tkill = 1;\n\t}\n\tspin_unlock(&inode->i_lock);\n\n\tif (kill)\n\t\tfree_pipe_info(pipe);\n}\n\nstatic int\npipe_release(struct inode *inode, struct file *file)\n{\n\tstruct pipe_inode_info *pipe = file->private_data;\n\n\t__pipe_lock(pipe);\n\tif (file->f_mode & FMODE_READ)\n\t\tpipe->readers--;\n\tif (file->f_mode & FMODE_WRITE)\n\t\tpipe->writers--;\n\n\t/* Was that the last reader or writer, but not the other side? */\n\tif (!pipe->readers != !pipe->writers) {\n\t\twake_up_interruptible_all(&pipe->rd_wait);\n\t\twake_up_interruptible_all(&pipe->wr_wait);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\t}\n\t__pipe_unlock(pipe);\n\n\tput_pipe_info(inode, pipe);\n\treturn 0;\n}\n\nstatic int\npipe_fasync(int fd, struct file *filp, int on)\n{\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tint retval = 0;\n\n\t__pipe_lock(pipe);\n\tif (filp->f_mode & FMODE_READ)\n\t\tretval = fasync_helper(fd, filp, on, &pipe->fasync_readers);\n\tif ((filp->f_mode & FMODE_WRITE) && retval >= 0) {\n\t\tretval = fasync_helper(fd, filp, on, &pipe->fasync_writers);\n\t\tif (retval < 0 && (filp->f_mode & FMODE_READ))\n\t\t\t/* this can happen only if on == T */\n\t\t\tfasync_helper(-1, filp, 0, &pipe->fasync_readers);\n\t}\n\t__pipe_unlock(pipe);\n\treturn retval;\n}\n\nunsigned long account_pipe_buffers(struct user_struct *user,\n\t\t\t\t   unsigned long old, unsigned long new)\n{\n\treturn atomic_long_add_return(new - old, &user->pipe_bufs);\n}\n\nbool too_many_pipe_buffers_soft(unsigned long user_bufs)\n{\n\tunsigned long soft_limit = READ_ONCE(pipe_user_pages_soft);\n\n\treturn soft_limit && user_bufs > soft_limit;\n}\n\nbool too_many_pipe_buffers_hard(unsigned long user_bufs)\n{\n\tunsigned long hard_limit = READ_ONCE(pipe_user_pages_hard);\n\n\treturn hard_limit && user_bufs > hard_limit;\n}\n\nbool pipe_is_unprivileged_user(void)\n{\n\treturn !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN);\n}\n\nstruct pipe_inode_info *alloc_pipe_info(void)\n{\n\tstruct pipe_inode_info *pipe;\n\tunsigned long pipe_bufs = PIPE_DEF_BUFFERS;\n\tstruct user_struct *user = get_current_user();\n\tunsigned long user_bufs;\n\tunsigned int max_size = READ_ONCE(pipe_max_size);\n\n\tpipe = kzalloc(sizeof(struct pipe_inode_info), GFP_KERNEL_ACCOUNT);\n\tif (pipe == NULL)\n\t\tgoto out_free_uid;\n\n\tif (pipe_bufs * PAGE_SIZE > max_size && !capable(CAP_SYS_RESOURCE))\n\t\tpipe_bufs = max_size >> PAGE_SHIFT;\n\n\tuser_bufs = account_pipe_buffers(user, 0, pipe_bufs);\n\n\tif (too_many_pipe_buffers_soft(user_bufs) && pipe_is_unprivileged_user()) {\n\t\tuser_bufs = account_pipe_buffers(user, pipe_bufs, PIPE_MIN_DEF_BUFFERS);\n\t\tpipe_bufs = PIPE_MIN_DEF_BUFFERS;\n\t}\n\n\tif (too_many_pipe_buffers_hard(user_bufs) && pipe_is_unprivileged_user())\n\t\tgoto out_revert_acct;\n\n\tpipe->bufs = kcalloc(pipe_bufs, sizeof(struct pipe_buffer),\n\t\t\t     GFP_KERNEL_ACCOUNT);\n\n\tif (pipe->bufs) {\n\t\tinit_waitqueue_head(&pipe->rd_wait);\n\t\tinit_waitqueue_head(&pipe->wr_wait);\n\t\tpipe->r_counter = pipe->w_counter = 1;\n\t\tpipe->max_usage = pipe_bufs;\n\t\tpipe->ring_size = pipe_bufs;\n\t\tpipe->nr_accounted = pipe_bufs;\n\t\tpipe->user = user;\n\t\tmutex_init(&pipe->mutex);\n\t\treturn pipe;\n\t}\n\nout_revert_acct:\n\t(void) account_pipe_buffers(user, pipe_bufs, 0);\n\tkfree(pipe);\nout_free_uid:\n\tfree_uid(user);\n\treturn NULL;\n}\n\nvoid free_pipe_info(struct pipe_inode_info *pipe)\n{\n\tunsigned int i;\n\n#ifdef CONFIG_WATCH_QUEUE\n\tif (pipe->watch_queue)\n\t\twatch_queue_clear(pipe->watch_queue);\n#endif\n\n\t(void) account_pipe_buffers(pipe->user, pipe->nr_accounted, 0);\n\tfree_uid(pipe->user);\n\tfor (i = 0; i < pipe->ring_size; i++) {\n\t\tstruct pipe_buffer *buf = pipe->bufs + i;\n\t\tif (buf->ops)\n\t\t\tpipe_buf_release(pipe, buf);\n\t}\n#ifdef CONFIG_WATCH_QUEUE\n\tif (pipe->watch_queue)\n\t\tput_watch_queue(pipe->watch_queue);\n#endif\n\tif (pipe->tmp_page)\n\t\t__free_page(pipe->tmp_page);\n\tkfree(pipe->bufs);\n\tkfree(pipe);\n}\n\nstatic struct vfsmount *pipe_mnt __read_mostly;\n\n/*\n * pipefs_dname() is called from d_path().\n */\nstatic char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n{\n\treturn dynamic_dname(dentry, buffer, buflen, \"pipe:[%lu]\",\n\t\t\t\td_inode(dentry)->i_ino);\n}\n\nstatic const struct dentry_operations pipefs_dentry_operations = {\n\t.d_dname\t= pipefs_dname,\n};\n\nstatic struct inode * get_pipe_inode(void)\n{\n\tstruct inode *inode = new_inode_pseudo(pipe_mnt->mnt_sb);\n\tstruct pipe_inode_info *pipe;\n\n\tif (!inode)\n\t\tgoto fail_inode;\n\n\tinode->i_ino = get_next_ino();\n\n\tpipe = alloc_pipe_info();\n\tif (!pipe)\n\t\tgoto fail_iput;\n\n\tinode->i_pipe = pipe;\n\tpipe->files = 2;\n\tpipe->readers = pipe->writers = 1;\n\tinode->i_fop = &pipefifo_fops;\n\n\t/*\n\t * Mark the inode dirty from the very beginning,\n\t * that way it will never be moved to the dirty\n\t * list because \"mark_inode_dirty()\" will think\n\t * that it already _is_ on the dirty list.\n\t */\n\tinode->i_state = I_DIRTY;\n\tinode->i_mode = S_IFIFO | S_IRUSR | S_IWUSR;\n\tinode->i_uid = current_fsuid();\n\tinode->i_gid = current_fsgid();\n\tinode->i_atime = inode->i_mtime = inode->i_ctime = current_time(inode);\n\n\treturn inode;\n\nfail_iput:\n\tiput(inode);\n\nfail_inode:\n\treturn NULL;\n}\n\nint create_pipe_files(struct file **res, int flags)\n{\n\tstruct inode *inode = get_pipe_inode();\n\tstruct file *f;\n\tint error;\n\n\tif (!inode)\n\t\treturn -ENFILE;\n\n\tif (flags & O_NOTIFICATION_PIPE) {\n\t\terror = watch_queue_init(inode->i_pipe);\n\t\tif (error) {\n\t\t\tfree_pipe_info(inode->i_pipe);\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n\t}\n\n\tf = alloc_file_pseudo(inode, pipe_mnt, \"\",\n\t\t\t\tO_WRONLY | (flags & (O_NONBLOCK | O_DIRECT)),\n\t\t\t\t&pipefifo_fops);\n\tif (IS_ERR(f)) {\n\t\tfree_pipe_info(inode->i_pipe);\n\t\tiput(inode);\n\t\treturn PTR_ERR(f);\n\t}\n\n\tf->private_data = inode->i_pipe;\n\n\tres[0] = alloc_file_clone(f, O_RDONLY | (flags & O_NONBLOCK),\n\t\t\t\t  &pipefifo_fops);\n\tif (IS_ERR(res[0])) {\n\t\tput_pipe_info(inode, inode->i_pipe);\n\t\tfput(f);\n\t\treturn PTR_ERR(res[0]);\n\t}\n\tres[0]->private_data = inode->i_pipe;\n\tres[1] = f;\n\tstream_open(inode, res[0]);\n\tstream_open(inode, res[1]);\n\treturn 0;\n}\n\nstatic int __do_pipe_flags(int *fd, struct file **files, int flags)\n{\n\tint error;\n\tint fdw, fdr;\n\n\tif (flags & ~(O_CLOEXEC | O_NONBLOCK | O_DIRECT | O_NOTIFICATION_PIPE))\n\t\treturn -EINVAL;\n\n\terror = create_pipe_files(files, flags);\n\tif (error)\n\t\treturn error;\n\n\terror = get_unused_fd_flags(flags);\n\tif (error < 0)\n\t\tgoto err_read_pipe;\n\tfdr = error;\n\n\terror = get_unused_fd_flags(flags);\n\tif (error < 0)\n\t\tgoto err_fdr;\n\tfdw = error;\n\n\taudit_fd_pair(fdr, fdw);\n\tfd[0] = fdr;\n\tfd[1] = fdw;\n\treturn 0;\n\n err_fdr:\n\tput_unused_fd(fdr);\n err_read_pipe:\n\tfput(files[0]);\n\tfput(files[1]);\n\treturn error;\n}\n\nint do_pipe_flags(int *fd, int flags)\n{\n\tstruct file *files[2];\n\tint error = __do_pipe_flags(fd, files, flags);\n\tif (!error) {\n\t\tfd_install(fd[0], files[0]);\n\t\tfd_install(fd[1], files[1]);\n\t}\n\treturn error;\n}\n\n/*\n * sys_pipe() is the normal C calling standard for creating\n * a pipe. It's not the way Unix traditionally does this, though.\n */\nstatic int do_pipe2(int __user *fildes, int flags)\n{\n\tstruct file *files[2];\n\tint fd[2];\n\tint error;\n\n\terror = __do_pipe_flags(fd, files, flags);\n\tif (!error) {\n\t\tif (unlikely(copy_to_user(fildes, fd, sizeof(fd)))) {\n\t\t\tfput(files[0]);\n\t\t\tfput(files[1]);\n\t\t\tput_unused_fd(fd[0]);\n\t\t\tput_unused_fd(fd[1]);\n\t\t\terror = -EFAULT;\n\t\t} else {\n\t\t\tfd_install(fd[0], files[0]);\n\t\t\tfd_install(fd[1], files[1]);\n\t\t}\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE2(pipe2, int __user *, fildes, int, flags)\n{\n\treturn do_pipe2(fildes, flags);\n}\n\nSYSCALL_DEFINE1(pipe, int __user *, fildes)\n{\n\treturn do_pipe2(fildes, 0);\n}\n\n/*\n * This is the stupid \"wait for pipe to be readable or writable\"\n * model.\n *\n * See pipe_read/write() for the proper kind of exclusive wait,\n * but that requires that we wake up any other readers/writers\n * if we then do not end up reading everything (ie the whole\n * \"wake_next_reader/writer\" logic in pipe_read/write()).\n */\nvoid pipe_wait_readable(struct pipe_inode_info *pipe)\n{\n\tpipe_unlock(pipe);\n\twait_event_interruptible(pipe->rd_wait, pipe_readable(pipe));\n\tpipe_lock(pipe);\n}\n\nvoid pipe_wait_writable(struct pipe_inode_info *pipe)\n{\n\tpipe_unlock(pipe);\n\twait_event_interruptible(pipe->wr_wait, pipe_writable(pipe));\n\tpipe_lock(pipe);\n}\n\n/*\n * This depends on both the wait (here) and the wakeup (wake_up_partner)\n * holding the pipe lock, so \"*cnt\" is stable and we know a wakeup cannot\n * race with the count check and waitqueue prep.\n *\n * Normally in order to avoid races, you'd do the prepare_to_wait() first,\n * then check the condition you're waiting for, and only then sleep. But\n * because of the pipe lock, we can check the condition before being on\n * the wait queue.\n *\n * We use the 'rd_wait' waitqueue for pipe partner waiting.\n */\nstatic int wait_for_partner(struct pipe_inode_info *pipe, unsigned int *cnt)\n{\n\tDEFINE_WAIT(rdwait);\n\tint cur = *cnt;\n\n\twhile (cur == *cnt) {\n\t\tprepare_to_wait(&pipe->rd_wait, &rdwait, TASK_INTERRUPTIBLE);\n\t\tpipe_unlock(pipe);\n\t\tschedule();\n\t\tfinish_wait(&pipe->rd_wait, &rdwait);\n\t\tpipe_lock(pipe);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t}\n\treturn cur == *cnt ? -ERESTARTSYS : 0;\n}\n\nstatic void wake_up_partner(struct pipe_inode_info *pipe)\n{\n\twake_up_interruptible_all(&pipe->rd_wait);\n}\n\nstatic int fifo_open(struct inode *inode, struct file *filp)\n{\n\tstruct pipe_inode_info *pipe;\n\tbool is_pipe = inode->i_sb->s_magic == PIPEFS_MAGIC;\n\tint ret;\n\n\tfilp->f_version = 0;\n\n\tspin_lock(&inode->i_lock);\n\tif (inode->i_pipe) {\n\t\tpipe = inode->i_pipe;\n\t\tpipe->files++;\n\t\tspin_unlock(&inode->i_lock);\n\t} else {\n\t\tspin_unlock(&inode->i_lock);\n\t\tpipe = alloc_pipe_info();\n\t\tif (!pipe)\n\t\t\treturn -ENOMEM;\n\t\tpipe->files = 1;\n\t\tspin_lock(&inode->i_lock);\n\t\tif (unlikely(inode->i_pipe)) {\n\t\t\tinode->i_pipe->files++;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tfree_pipe_info(pipe);\n\t\t\tpipe = inode->i_pipe;\n\t\t} else {\n\t\t\tinode->i_pipe = pipe;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t}\n\t}\n\tfilp->private_data = pipe;\n\t/* OK, we have a pipe and it's pinned down */\n\n\t__pipe_lock(pipe);\n\n\t/* We can only do regular read/write on fifos */\n\tstream_open(inode, filp);\n\n\tswitch (filp->f_mode & (FMODE_READ | FMODE_WRITE)) {\n\tcase FMODE_READ:\n\t/*\n\t *  O_RDONLY\n\t *  POSIX.1 says that O_NONBLOCK means return with the FIFO\n\t *  opened, even when there is no process writing the FIFO.\n\t */\n\t\tpipe->r_counter++;\n\t\tif (pipe->readers++ == 0)\n\t\t\twake_up_partner(pipe);\n\n\t\tif (!is_pipe && !pipe->writers) {\n\t\t\tif ((filp->f_flags & O_NONBLOCK)) {\n\t\t\t\t/* suppress EPOLLHUP until we have\n\t\t\t\t * seen a writer */\n\t\t\t\tfilp->f_version = pipe->w_counter;\n\t\t\t} else {\n\t\t\t\tif (wait_for_partner(pipe, &pipe->w_counter))\n\t\t\t\t\tgoto err_rd;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase FMODE_WRITE:\n\t/*\n\t *  O_WRONLY\n\t *  POSIX.1 says that O_NONBLOCK means return -1 with\n\t *  errno=ENXIO when there is no process reading the FIFO.\n\t */\n\t\tret = -ENXIO;\n\t\tif (!is_pipe && (filp->f_flags & O_NONBLOCK) && !pipe->readers)\n\t\t\tgoto err;\n\n\t\tpipe->w_counter++;\n\t\tif (!pipe->writers++)\n\t\t\twake_up_partner(pipe);\n\n\t\tif (!is_pipe && !pipe->readers) {\n\t\t\tif (wait_for_partner(pipe, &pipe->r_counter))\n\t\t\t\tgoto err_wr;\n\t\t}\n\t\tbreak;\n\n\tcase FMODE_READ | FMODE_WRITE:\n\t/*\n\t *  O_RDWR\n\t *  POSIX.1 leaves this case \"undefined\" when O_NONBLOCK is set.\n\t *  This implementation will NEVER block on a O_RDWR open, since\n\t *  the process can at least talk to itself.\n\t */\n\n\t\tpipe->readers++;\n\t\tpipe->writers++;\n\t\tpipe->r_counter++;\n\t\tpipe->w_counter++;\n\t\tif (pipe->readers == 1 || pipe->writers == 1)\n\t\t\twake_up_partner(pipe);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t/* Ok! */\n\t__pipe_unlock(pipe);\n\treturn 0;\n\nerr_rd:\n\tif (!--pipe->readers)\n\t\twake_up_interruptible(&pipe->wr_wait);\n\tret = -ERESTARTSYS;\n\tgoto err;\n\nerr_wr:\n\tif (!--pipe->writers)\n\t\twake_up_interruptible_all(&pipe->rd_wait);\n\tret = -ERESTARTSYS;\n\tgoto err;\n\nerr:\n\t__pipe_unlock(pipe);\n\n\tput_pipe_info(inode, pipe);\n\treturn ret;\n}\n\nconst struct file_operations pipefifo_fops = {\n\t.open\t\t= fifo_open,\n\t.llseek\t\t= no_llseek,\n\t.read_iter\t= pipe_read,\n\t.write_iter\t= pipe_write,\n\t.poll\t\t= pipe_poll,\n\t.unlocked_ioctl\t= pipe_ioctl,\n\t.release\t= pipe_release,\n\t.fasync\t\t= pipe_fasync,\n\t.splice_write\t= iter_file_splice_write,\n};\n\n/*\n * Currently we rely on the pipe array holding a power-of-2 number\n * of pages. Returns 0 on error.\n */\nunsigned int round_pipe_size(unsigned long size)\n{\n\tif (size > (1U << 31))\n\t\treturn 0;\n\n\t/* Minimum pipe size, as required by POSIX */\n\tif (size < PAGE_SIZE)\n\t\treturn PAGE_SIZE;\n\n\treturn roundup_pow_of_two(size);\n}\n\n/*\n * Resize the pipe ring to a number of slots.\n */\nint pipe_resize_ring(struct pipe_inode_info *pipe, unsigned int nr_slots)\n{\n\tstruct pipe_buffer *bufs;\n\tunsigned int head, tail, mask, n;\n\n\t/*\n\t * We can shrink the pipe, if arg is greater than the ring occupancy.\n\t * Since we don't expect a lot of shrink+grow operations, just free and\n\t * allocate again like we would do for growing.  If the pipe currently\n\t * contains more buffers than arg, then return busy.\n\t */\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\tn = pipe_occupancy(pipe->head, pipe->tail);\n\tif (nr_slots < n)\n\t\treturn -EBUSY;\n\n\tbufs = kcalloc(nr_slots, sizeof(*bufs),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n\tif (unlikely(!bufs))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * The pipe array wraps around, so just start the new one at zero\n\t * and adjust the indices.\n\t */\n\tif (n > 0) {\n\t\tunsigned int h = head & mask;\n\t\tunsigned int t = tail & mask;\n\t\tif (h > t) {\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       n * sizeof(struct pipe_buffer));\n\t\t} else {\n\t\t\tunsigned int tsize = pipe->ring_size - t;\n\t\t\tif (h > 0)\n\t\t\t\tmemcpy(bufs + tsize, pipe->bufs,\n\t\t\t\t       h * sizeof(struct pipe_buffer));\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       tsize * sizeof(struct pipe_buffer));\n\t\t}\n\t}\n\n\thead = n;\n\ttail = 0;\n\n\tkfree(pipe->bufs);\n\tpipe->bufs = bufs;\n\tpipe->ring_size = nr_slots;\n\tif (pipe->max_usage > nr_slots)\n\t\tpipe->max_usage = nr_slots;\n\tpipe->tail = tail;\n\tpipe->head = head;\n\n\t/* This might have made more room for writers */\n\twake_up_interruptible(&pipe->wr_wait);\n\treturn 0;\n}\n\n/*\n * Allocate a new array of pipe buffers and copy the info over. Returns the\n * pipe size if successful, or return -ERROR on error.\n */\nstatic long pipe_set_size(struct pipe_inode_info *pipe, unsigned long arg)\n{\n\tunsigned long user_bufs;\n\tunsigned int nr_slots, size;\n\tlong ret = 0;\n\n#ifdef CONFIG_WATCH_QUEUE\n\tif (pipe->watch_queue)\n\t\treturn -EBUSY;\n#endif\n\n\tsize = round_pipe_size(arg);\n\tnr_slots = size >> PAGE_SHIFT;\n\n\tif (!nr_slots)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If trying to increase the pipe capacity, check that an\n\t * unprivileged user is not trying to exceed various limits\n\t * (soft limit check here, hard limit check just below).\n\t * Decreasing the pipe capacity is always permitted, even\n\t * if the user is currently over a limit.\n\t */\n\tif (nr_slots > pipe->max_usage &&\n\t\t\tsize > pipe_max_size && !capable(CAP_SYS_RESOURCE))\n\t\treturn -EPERM;\n\n\tuser_bufs = account_pipe_buffers(pipe->user, pipe->nr_accounted, nr_slots);\n\n\tif (nr_slots > pipe->max_usage &&\n\t\t\t(too_many_pipe_buffers_hard(user_bufs) ||\n\t\t\t too_many_pipe_buffers_soft(user_bufs)) &&\n\t\t\tpipe_is_unprivileged_user()) {\n\t\tret = -EPERM;\n\t\tgoto out_revert_acct;\n\t}\n\n\tret = pipe_resize_ring(pipe, nr_slots);\n\tif (ret < 0)\n\t\tgoto out_revert_acct;\n\n\tpipe->max_usage = nr_slots;\n\tpipe->nr_accounted = nr_slots;\n\treturn pipe->max_usage * PAGE_SIZE;\n\nout_revert_acct:\n\t(void) account_pipe_buffers(pipe->user, nr_slots, pipe->nr_accounted);\n\treturn ret;\n}\n\n/*\n * Note that i_pipe and i_cdev share the same location, so checking ->i_pipe is\n * not enough to verify that this is a pipe.\n */\nstruct pipe_inode_info *get_pipe_info(struct file *file, bool for_splice)\n{\n\tstruct pipe_inode_info *pipe = file->private_data;\n\n\tif (file->f_op != &pipefifo_fops || !pipe)\n\t\treturn NULL;\n#ifdef CONFIG_WATCH_QUEUE\n\tif (for_splice && pipe->watch_queue)\n\t\treturn NULL;\n#endif\n\treturn pipe;\n}\n\nlong pipe_fcntl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct pipe_inode_info *pipe;\n\tlong ret;\n\n\tpipe = get_pipe_info(file, false);\n\tif (!pipe)\n\t\treturn -EBADF;\n\n\t__pipe_lock(pipe);\n\n\tswitch (cmd) {\n\tcase F_SETPIPE_SZ:\n\t\tret = pipe_set_size(pipe, arg);\n\t\tbreak;\n\tcase F_GETPIPE_SZ:\n\t\tret = pipe->max_usage * PAGE_SIZE;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\t__pipe_unlock(pipe);\n\treturn ret;\n}\n\nstatic const struct super_operations pipefs_ops = {\n\t.destroy_inode = free_inode_nonrcu,\n\t.statfs = simple_statfs,\n};\n\n/*\n * pipefs should _never_ be mounted by userland - too much of security hassle,\n * no real gain from having the whole whorehouse mounted. So we don't need\n * any operations on the root directory. However, we need a non-trivial\n * d_name - pipe: will go nicely and kill the special-casing in procfs.\n */\n\nstatic int pipefs_init_fs_context(struct fs_context *fc)\n{\n\tstruct pseudo_fs_context *ctx = init_pseudo(fc, PIPEFS_MAGIC);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx->ops = &pipefs_ops;\n\tctx->dops = &pipefs_dentry_operations;\n\treturn 0;\n}\n\nstatic struct file_system_type pipe_fs_type = {\n\t.name\t\t= \"pipefs\",\n\t.init_fs_context = pipefs_init_fs_context,\n\t.kill_sb\t= kill_anon_super,\n};\n\n#ifdef CONFIG_SYSCTL\nstatic int do_proc_dopipe_max_size_conv(unsigned long *lvalp,\n\t\t\t\t\tunsigned int *valp,\n\t\t\t\t\tint write, void *data)\n{\n\tif (write) {\n\t\tunsigned int val;\n\n\t\tval = round_pipe_size(*lvalp);\n\t\tif (val == 0)\n\t\t\treturn -EINVAL;\n\n\t\t*valp = val;\n\t} else {\n\t\tunsigned int val = *valp;\n\t\t*lvalp = (unsigned long) val;\n\t}\n\n\treturn 0;\n}\n\nstatic int proc_dopipe_max_size(struct ctl_table *table, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn do_proc_douintvec(table, write, buffer, lenp, ppos,\n\t\t\t\t do_proc_dopipe_max_size_conv, NULL);\n}\n\nstatic struct ctl_table fs_pipe_sysctls[] = {\n\t{\n\t\t.procname\t= \"pipe-max-size\",\n\t\t.data\t\t= &pipe_max_size,\n\t\t.maxlen\t\t= sizeof(pipe_max_size),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dopipe_max_size,\n\t},\n\t{\n\t\t.procname\t= \"pipe-user-pages-hard\",\n\t\t.data\t\t= &pipe_user_pages_hard,\n\t\t.maxlen\t\t= sizeof(pipe_user_pages_hard),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"pipe-user-pages-soft\",\n\t\t.data\t\t= &pipe_user_pages_soft,\n\t\t.maxlen\t\t= sizeof(pipe_user_pages_soft),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{ }\n};\n#endif\n\nstatic int __init init_pipe_fs(void)\n{\n\tint err = register_filesystem(&pipe_fs_type);\n\n\tif (!err) {\n\t\tpipe_mnt = kern_mount(&pipe_fs_type);\n\t\tif (IS_ERR(pipe_mnt)) {\n\t\t\terr = PTR_ERR(pipe_mnt);\n\t\t\tunregister_filesystem(&pipe_fs_type);\n\t\t}\n\t}\n#ifdef CONFIG_SYSCTL\n\tregister_sysctl_init(\"fs\", fs_pipe_sysctls);\n#endif\n\treturn err;\n}\n\nfs_initcall(init_pipe_fs);\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/pipe.c\n *\n *  Copyright (C) 1991, 1992, 1999  Linus Torvalds\n */\n\n#include <linux/mm.h>\n#include <linux/file.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/log2.h>\n#include <linux/mount.h>\n#include <linux/pseudo_fs.h>\n#include <linux/magic.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/uio.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/fcntl.h>\n#include <linux/memcontrol.h>\n#include <linux/watch_queue.h>\n#include <linux/sysctl.h>\n\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n\n#include \"internal.h\"\n\n/*\n * New pipe buffers will be restricted to this size while the user is exceeding\n * their pipe buffer quota. The general pipe use case needs at least two\n * buffers: one for data yet to be read, and one for new data. If this is less\n * than two, then a write to a non-empty pipe may block even if the pipe is not\n * full. This can occur with GNU make jobserver or similar uses of pipes as\n * semaphores: multiple processes may be waiting to write tokens back to the\n * pipe before reading tokens: https://lore.kernel.org/lkml/1628086770.5rn8p04n6j.none@localhost/.\n *\n * Users can reduce their pipe buffers with F_SETPIPE_SZ below this at their\n * own risk, namely: pipe writes to non-full pipes may block until the pipe is\n * emptied.\n */\n#define PIPE_MIN_DEF_BUFFERS 2\n\n/*\n * The max size that a non-root user is allowed to grow the pipe. Can\n * be set by root in /proc/sys/fs/pipe-max-size\n */\nstatic unsigned int pipe_max_size = 1048576;\n\n/* Maximum allocatable pages per user. Hard limit is unset by default, soft\n * matches default values.\n */\nstatic unsigned long pipe_user_pages_hard;\nstatic unsigned long pipe_user_pages_soft = PIPE_DEF_BUFFERS * INR_OPEN_CUR;\n\n/*\n * We use head and tail indices that aren't masked off, except at the point of\n * dereference, but rather they're allowed to wrap naturally.  This means there\n * isn't a dead spot in the buffer, but the ring has to be a power of two and\n * <= 2^31.\n * -- David Howells 2019-09-23.\n *\n * Reads with count = 0 should always return 0.\n * -- Julian Bradfield 1999-06-07.\n *\n * FIFOs and Pipes now generate SIGIO for both readers and writers.\n * -- Jeremy Elson <jelson@circlemud.org> 2001-08-16\n *\n * pipe_read & write cleanup\n * -- Manfred Spraul <manfred@colorfullife.com> 2002-05-09\n */\n\nstatic void pipe_lock_nested(struct pipe_inode_info *pipe, int subclass)\n{\n\tif (pipe->files)\n\t\tmutex_lock_nested(&pipe->mutex, subclass);\n}\n\nvoid pipe_lock(struct pipe_inode_info *pipe)\n{\n\t/*\n\t * pipe_lock() nests non-pipe inode locks (for writing to a file)\n\t */\n\tpipe_lock_nested(pipe, I_MUTEX_PARENT);\n}\nEXPORT_SYMBOL(pipe_lock);\n\nvoid pipe_unlock(struct pipe_inode_info *pipe)\n{\n\tif (pipe->files)\n\t\tmutex_unlock(&pipe->mutex);\n}\nEXPORT_SYMBOL(pipe_unlock);\n\nstatic inline void __pipe_lock(struct pipe_inode_info *pipe)\n{\n\tmutex_lock_nested(&pipe->mutex, I_MUTEX_PARENT);\n}\n\nstatic inline void __pipe_unlock(struct pipe_inode_info *pipe)\n{\n\tmutex_unlock(&pipe->mutex);\n}\n\nvoid pipe_double_lock(struct pipe_inode_info *pipe1,\n\t\t      struct pipe_inode_info *pipe2)\n{\n\tBUG_ON(pipe1 == pipe2);\n\n\tif (pipe1 < pipe2) {\n\t\tpipe_lock_nested(pipe1, I_MUTEX_PARENT);\n\t\tpipe_lock_nested(pipe2, I_MUTEX_CHILD);\n\t} else {\n\t\tpipe_lock_nested(pipe2, I_MUTEX_PARENT);\n\t\tpipe_lock_nested(pipe1, I_MUTEX_CHILD);\n\t}\n}\n\nstatic void anon_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t  struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\t/*\n\t * If nobody else uses this page, and we don't already have a\n\t * temporary page, let's keep track of it as a one-deep\n\t * allocation cache. (Otherwise just release our reference to it)\n\t */\n\tif (page_count(page) == 1 && !pipe->tmp_page)\n\t\tpipe->tmp_page = page;\n\telse\n\t\tput_page(page);\n}\n\nstatic bool anon_pipe_buf_try_steal(struct pipe_inode_info *pipe,\n\t\tstruct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\tif (page_count(page) != 1)\n\t\treturn false;\n\tmemcg_kmem_uncharge_page(page, 0);\n\t__SetPageLocked(page);\n\treturn true;\n}\n\n/**\n * generic_pipe_buf_try_steal - attempt to take ownership of a &pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to attempt to steal\n *\n * Description:\n *\tThis function attempts to steal the &struct page attached to\n *\t@buf. If successful, this function returns 0 and returns with\n *\tthe page locked. The caller may then reuse the page for whatever\n *\the wishes; the typical use is insertion into a different file\n *\tpage cache.\n */\nbool generic_pipe_buf_try_steal(struct pipe_inode_info *pipe,\n\t\tstruct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\t/*\n\t * A reference of one is golden, that means that the owner of this\n\t * page is the only one holding a reference to it. lock the page\n\t * and return OK.\n\t */\n\tif (page_count(page) == 1) {\n\t\tlock_page(page);\n\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL(generic_pipe_buf_try_steal);\n\n/**\n * generic_pipe_buf_get - get a reference to a &struct pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to get a reference to\n *\n * Description:\n *\tThis function grabs an extra reference to @buf. It's used in\n *\tthe tee() system call, when we duplicate the buffers in one\n *\tpipe into another.\n */\nbool generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n{\n\treturn try_get_page(buf->page);\n}\nEXPORT_SYMBOL(generic_pipe_buf_get);\n\n/**\n * generic_pipe_buf_release - put a reference to a &struct pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to put a reference to\n *\n * Description:\n *\tThis function releases a reference to @buf.\n */\nvoid generic_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t      struct pipe_buffer *buf)\n{\n\tput_page(buf->page);\n}\nEXPORT_SYMBOL(generic_pipe_buf_release);\n\nstatic const struct pipe_buf_operations anon_pipe_buf_ops = {\n\t.release\t= anon_pipe_buf_release,\n\t.try_steal\t= anon_pipe_buf_try_steal,\n\t.get\t\t= generic_pipe_buf_get,\n};\n\n/* Done while waiting without holding the pipe lock - thus the READ_ONCE() */\nstatic inline bool pipe_readable(const struct pipe_inode_info *pipe)\n{\n\tunsigned int head = READ_ONCE(pipe->head);\n\tunsigned int tail = READ_ONCE(pipe->tail);\n\tunsigned int writers = READ_ONCE(pipe->writers);\n\n\treturn !pipe_empty(head, tail) || !writers;\n}\n\nstatic ssize_t\npipe_read(struct kiocb *iocb, struct iov_iter *to)\n{\n\tsize_t total_len = iov_iter_count(to);\n\tstruct file *filp = iocb->ki_filp;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tbool was_full, wake_next_reader = false;\n\tssize_t ret;\n\n\t/* Null read succeeds. */\n\tif (unlikely(total_len == 0))\n\t\treturn 0;\n\n\tret = 0;\n\t__pipe_lock(pipe);\n\n\t/*\n\t * We only wake up writers if the pipe was full when we started\n\t * reading in order to avoid unnecessary wakeups.\n\t *\n\t * But when we do wake up writers, we do so using a sync wakeup\n\t * (WF_SYNC), because we want them to get going and generate more\n\t * data for us.\n\t */\n\twas_full = pipe_full(pipe->head, pipe->tail, pipe->max_usage);\n\tfor (;;) {\n\t\t/* Read ->head with a barrier vs post_one_notification() */\n\t\tunsigned int head = smp_load_acquire(&pipe->head);\n\t\tunsigned int tail = pipe->tail;\n\t\tunsigned int mask = pipe->ring_size - 1;\n\n#ifdef CONFIG_WATCH_QUEUE\n\t\tif (pipe->note_loss) {\n\t\t\tstruct watch_notification n;\n\n\t\t\tif (total_len < 8) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -ENOBUFS;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tn.type = WATCH_TYPE_META;\n\t\t\tn.subtype = WATCH_META_LOSS_NOTIFICATION;\n\t\t\tn.info = watch_sizeof(n);\n\t\t\tif (copy_to_iter(&n, sizeof(n), to) != sizeof(n)) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret += sizeof(n);\n\t\t\ttotal_len -= sizeof(n);\n\t\t\tpipe->note_loss = false;\n\t\t}\n#endif\n\n\t\tif (!pipe_empty(head, tail)) {\n\t\t\tstruct pipe_buffer *buf = &pipe->bufs[tail & mask];\n\t\t\tsize_t chars = buf->len;\n\t\t\tsize_t written;\n\t\t\tint error;\n\n\t\t\tif (chars > total_len) {\n\t\t\t\tif (buf->flags & PIPE_BUF_FLAG_WHOLE) {\n\t\t\t\t\tif (ret == 0)\n\t\t\t\t\t\tret = -ENOBUFS;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tchars = total_len;\n\t\t\t}\n\n\t\t\terror = pipe_buf_confirm(pipe, buf);\n\t\t\tif (error) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = error;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\twritten = copy_page_to_iter(buf->page, buf->offset, chars, to);\n\t\t\tif (unlikely(written < chars)) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret += chars;\n\t\t\tbuf->offset += chars;\n\t\t\tbuf->len -= chars;\n\n\t\t\t/* Was it a packet buffer? Clean up and exit */\n\t\t\tif (buf->flags & PIPE_BUF_FLAG_PACKET) {\n\t\t\t\ttotal_len = chars;\n\t\t\t\tbuf->len = 0;\n\t\t\t}\n\n\t\t\tif (!buf->len) {\n\t\t\t\tpipe_buf_release(pipe, buf);\n\t\t\t\tspin_lock_irq(&pipe->rd_wait.lock);\n#ifdef CONFIG_WATCH_QUEUE\n\t\t\t\tif (buf->flags & PIPE_BUF_FLAG_LOSS)\n\t\t\t\t\tpipe->note_loss = true;\n#endif\n\t\t\t\ttail++;\n\t\t\t\tpipe->tail = tail;\n\t\t\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\t\t}\n\t\t\ttotal_len -= chars;\n\t\t\tif (!total_len)\n\t\t\t\tbreak;\t/* common path: read succeeded */\n\t\t\tif (!pipe_empty(head, tail))\t/* More to do? */\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (!pipe->writers)\n\t\t\tbreak;\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\t__pipe_unlock(pipe);\n\n\t\t/*\n\t\t * We only get here if we didn't actually read anything.\n\t\t *\n\t\t * However, we could have seen (and removed) a zero-sized\n\t\t * pipe buffer, and might have made space in the buffers\n\t\t * that way.\n\t\t *\n\t\t * You can't make zero-sized pipe buffers by doing an empty\n\t\t * write (not even in packet mode), but they can happen if\n\t\t * the writer gets an EFAULT when trying to fill a buffer\n\t\t * that already got allocated and inserted in the buffer\n\t\t * array.\n\t\t *\n\t\t * So we still need to wake up any pending writers in the\n\t\t * _very_ unlikely case that the pipe was full, but we got\n\t\t * no data.\n\t\t */\n\t\tif (unlikely(was_full))\n\t\t\twake_up_interruptible_sync_poll(&pipe->wr_wait, EPOLLOUT | EPOLLWRNORM);\n\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\n\t\t/*\n\t\t * But because we didn't read anything, at this point we can\n\t\t * just return directly with -ERESTARTSYS if we're interrupted,\n\t\t * since we've done any required wakeups and there's no need\n\t\t * to mark anything accessed. And we've dropped the lock.\n\t\t */\n\t\tif (wait_event_interruptible_exclusive(pipe->rd_wait, pipe_readable(pipe)) < 0)\n\t\t\treturn -ERESTARTSYS;\n\n\t\t__pipe_lock(pipe);\n\t\twas_full = pipe_full(pipe->head, pipe->tail, pipe->max_usage);\n\t\twake_next_reader = true;\n\t}\n\tif (pipe_empty(pipe->head, pipe->tail))\n\t\twake_next_reader = false;\n\t__pipe_unlock(pipe);\n\n\tif (was_full)\n\t\twake_up_interruptible_sync_poll(&pipe->wr_wait, EPOLLOUT | EPOLLWRNORM);\n\tif (wake_next_reader)\n\t\twake_up_interruptible_sync_poll(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\tif (ret > 0)\n\t\tfile_accessed(filp);\n\treturn ret;\n}\n\nstatic inline int is_packetized(struct file *file)\n{\n\treturn (file->f_flags & O_DIRECT) != 0;\n}\n\n/* Done while waiting without holding the pipe lock - thus the READ_ONCE() */\nstatic inline bool pipe_writable(const struct pipe_inode_info *pipe)\n{\n\tunsigned int head = READ_ONCE(pipe->head);\n\tunsigned int tail = READ_ONCE(pipe->tail);\n\tunsigned int max_usage = READ_ONCE(pipe->max_usage);\n\n\treturn !pipe_full(head, tail, max_usage) ||\n\t\t!READ_ONCE(pipe->readers);\n}\n\nstatic ssize_t\npipe_write(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tunsigned int head;\n\tssize_t ret = 0;\n\tsize_t total_len = iov_iter_count(from);\n\tssize_t chars;\n\tbool was_empty = false;\n\tbool wake_next_writer = false;\n\n\t/* Null write succeeds. */\n\tif (unlikely(total_len == 0))\n\t\treturn 0;\n\n\t__pipe_lock(pipe);\n\n\tif (!pipe->readers) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tret = -EPIPE;\n\t\tgoto out;\n\t}\n\n#ifdef CONFIG_WATCH_QUEUE\n\tif (pipe->watch_queue) {\n\t\tret = -EXDEV;\n\t\tgoto out;\n\t}\n#endif\n\n\t/*\n\t * If it wasn't empty we try to merge new data into\n\t * the last buffer.\n\t *\n\t * That naturally merges small writes, but it also\n\t * page-aligns the rest of the writes for large writes\n\t * spanning multiple pages.\n\t */\n\thead = pipe->head;\n\twas_empty = pipe_empty(head, pipe->tail);\n\tchars = total_len & (PAGE_SIZE-1);\n\tif (chars && !was_empty) {\n\t\tunsigned int mask = pipe->ring_size - 1;\n\t\tstruct pipe_buffer *buf = &pipe->bufs[(head - 1) & mask];\n\t\tint offset = buf->offset + buf->len;\n\n\t\tif ((buf->flags & PIPE_BUF_FLAG_CAN_MERGE) &&\n\t\t    offset + chars <= PAGE_SIZE) {\n\t\t\tret = pipe_buf_confirm(pipe, buf);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tret = copy_page_from_iter(buf->page, offset, chars, from);\n\t\t\tif (unlikely(ret < chars)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tbuf->len += ret;\n\t\t\tif (!iov_iter_count(from))\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor (;;) {\n\t\tif (!pipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\thead = pipe->head;\n\t\tif (!pipe_full(head, pipe->tail, pipe->max_usage)) {\n\t\t\tunsigned int mask = pipe->ring_size - 1;\n\t\t\tstruct pipe_buffer *buf = &pipe->bufs[head & mask];\n\t\t\tstruct page *page = pipe->tmp_page;\n\t\t\tint copied;\n\n\t\t\tif (!page) {\n\t\t\t\tpage = alloc_page(GFP_HIGHUSER | __GFP_ACCOUNT);\n\t\t\t\tif (unlikely(!page)) {\n\t\t\t\t\tret = ret ? : -ENOMEM;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tpipe->tmp_page = page;\n\t\t\t}\n\n\t\t\t/* Allocate a slot in the ring in advance and attach an\n\t\t\t * empty buffer.  If we fault or otherwise fail to use\n\t\t\t * it, either the reader will consume it or it'll still\n\t\t\t * be there for the next write.\n\t\t\t */\n\t\t\tspin_lock_irq(&pipe->rd_wait.lock);\n\n\t\t\thead = pipe->head;\n\t\t\tif (pipe_full(head, pipe->tail, pipe->max_usage)) {\n\t\t\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tpipe->head = head + 1;\n\t\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\n\t\t\t/* Insert it into the buffer array */\n\t\t\tbuf = &pipe->bufs[head & mask];\n\t\t\tbuf->page = page;\n\t\t\tbuf->ops = &anon_pipe_buf_ops;\n\t\t\tbuf->offset = 0;\n\t\t\tbuf->len = 0;\n\t\t\tif (is_packetized(filp))\n\t\t\t\tbuf->flags = PIPE_BUF_FLAG_PACKET;\n\t\t\telse\n\t\t\t\tbuf->flags = PIPE_BUF_FLAG_CAN_MERGE;\n\t\t\tpipe->tmp_page = NULL;\n\n\t\t\tcopied = copy_page_from_iter(page, 0, PAGE_SIZE, from);\n\t\t\tif (unlikely(copied < PAGE_SIZE && iov_iter_count(from))) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret += copied;\n\t\t\tbuf->offset = 0;\n\t\t\tbuf->len = copied;\n\n\t\t\tif (!iov_iter_count(from))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!pipe_full(head, pipe->tail, pipe->max_usage))\n\t\t\tcontinue;\n\n\t\t/* Wait for buffer space to become available. */\n\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\tif (!ret)\n\t\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tif (!ret)\n\t\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We're going to release the pipe lock and wait for more\n\t\t * space. We wake up any readers if necessary, and then\n\t\t * after waiting we need to re-check whether the pipe\n\t\t * become empty while we dropped the lock.\n\t\t */\n\t\t__pipe_unlock(pipe);\n\t\tif (was_empty)\n\t\t\twake_up_interruptible_sync_poll(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t\twait_event_interruptible_exclusive(pipe->wr_wait, pipe_writable(pipe));\n\t\t__pipe_lock(pipe);\n\t\twas_empty = pipe_empty(pipe->head, pipe->tail);\n\t\twake_next_writer = true;\n\t}\nout:\n\tif (pipe_full(pipe->head, pipe->tail, pipe->max_usage))\n\t\twake_next_writer = false;\n\t__pipe_unlock(pipe);\n\n\t/*\n\t * If we do do a wakeup event, we do a 'sync' wakeup, because we\n\t * want the reader to start processing things asap, rather than\n\t * leave the data pending.\n\t *\n\t * This is particularly important for small writes, because of\n\t * how (for example) the GNU make jobserver uses small writes to\n\t * wake up pending jobs\n\t *\n\t * Epoll nonsensically wants a wakeup whether the pipe\n\t * was already empty or not.\n\t */\n\tif (was_empty || pipe->poll_usage)\n\t\twake_up_interruptible_sync_poll(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\tif (wake_next_writer)\n\t\twake_up_interruptible_sync_poll(&pipe->wr_wait, EPOLLOUT | EPOLLWRNORM);\n\tif (ret > 0 && sb_start_write_trylock(file_inode(filp)->i_sb)) {\n\t\tint err = file_update_time(filp);\n\t\tif (err)\n\t\t\tret = err;\n\t\tsb_end_write(file_inode(filp)->i_sb);\n\t}\n\treturn ret;\n}\n\nstatic long pipe_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tunsigned int count, head, tail, mask;\n\n\tswitch (cmd) {\n\tcase FIONREAD:\n\t\t__pipe_lock(pipe);\n\t\tcount = 0;\n\t\thead = pipe->head;\n\t\ttail = pipe->tail;\n\t\tmask = pipe->ring_size - 1;\n\n\t\twhile (tail != head) {\n\t\t\tcount += pipe->bufs[tail & mask].len;\n\t\t\ttail++;\n\t\t}\n\t\t__pipe_unlock(pipe);\n\n\t\treturn put_user(count, (int __user *)arg);\n\n#ifdef CONFIG_WATCH_QUEUE\n\tcase IOC_WATCH_QUEUE_SET_SIZE: {\n\t\tint ret;\n\t\t__pipe_lock(pipe);\n\t\tret = watch_queue_set_size(pipe, arg);\n\t\t__pipe_unlock(pipe);\n\t\treturn ret;\n\t}\n\n\tcase IOC_WATCH_QUEUE_SET_FILTER:\n\t\treturn watch_queue_set_filter(\n\t\t\tpipe, (struct watch_notification_filter __user *)arg);\n#endif\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n}\n\n/* No kernel lock held - fine */\nstatic __poll_t\npipe_poll(struct file *filp, poll_table *wait)\n{\n\t__poll_t mask;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tunsigned int head, tail;\n\n\t/* Epoll has some historical nasty semantics, this enables them */\n\tpipe->poll_usage = 1;\n\n\t/*\n\t * Reading pipe state only -- no need for acquiring the semaphore.\n\t *\n\t * But because this is racy, the code has to add the\n\t * entry to the poll table _first_ ..\n\t */\n\tif (filp->f_mode & FMODE_READ)\n\t\tpoll_wait(filp, &pipe->rd_wait, wait);\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tpoll_wait(filp, &pipe->wr_wait, wait);\n\n\t/*\n\t * .. and only then can you do the racy tests. That way,\n\t * if something changes and you got it wrong, the poll\n\t * table entry will wake you up and fix it.\n\t */\n\thead = READ_ONCE(pipe->head);\n\ttail = READ_ONCE(pipe->tail);\n\n\tmask = 0;\n\tif (filp->f_mode & FMODE_READ) {\n\t\tif (!pipe_empty(head, tail))\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\t\tif (!pipe->writers && filp->f_version != pipe->w_counter)\n\t\t\tmask |= EPOLLHUP;\n\t}\n\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tif (!pipe_full(head, tail, pipe->max_usage))\n\t\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t\t/*\n\t\t * Most Unices do not set EPOLLERR for FIFOs but on Linux they\n\t\t * behave exactly like pipes for poll().\n\t\t */\n\t\tif (!pipe->readers)\n\t\t\tmask |= EPOLLERR;\n\t}\n\n\treturn mask;\n}\n\nstatic void put_pipe_info(struct inode *inode, struct pipe_inode_info *pipe)\n{\n\tint kill = 0;\n\n\tspin_lock(&inode->i_lock);\n\tif (!--pipe->files) {\n\t\tinode->i_pipe = NULL;\n\t\tkill = 1;\n\t}\n\tspin_unlock(&inode->i_lock);\n\n\tif (kill)\n\t\tfree_pipe_info(pipe);\n}\n\nstatic int\npipe_release(struct inode *inode, struct file *file)\n{\n\tstruct pipe_inode_info *pipe = file->private_data;\n\n\t__pipe_lock(pipe);\n\tif (file->f_mode & FMODE_READ)\n\t\tpipe->readers--;\n\tif (file->f_mode & FMODE_WRITE)\n\t\tpipe->writers--;\n\n\t/* Was that the last reader or writer, but not the other side? */\n\tif (!pipe->readers != !pipe->writers) {\n\t\twake_up_interruptible_all(&pipe->rd_wait);\n\t\twake_up_interruptible_all(&pipe->wr_wait);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\t}\n\t__pipe_unlock(pipe);\n\n\tput_pipe_info(inode, pipe);\n\treturn 0;\n}\n\nstatic int\npipe_fasync(int fd, struct file *filp, int on)\n{\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tint retval = 0;\n\n\t__pipe_lock(pipe);\n\tif (filp->f_mode & FMODE_READ)\n\t\tretval = fasync_helper(fd, filp, on, &pipe->fasync_readers);\n\tif ((filp->f_mode & FMODE_WRITE) && retval >= 0) {\n\t\tretval = fasync_helper(fd, filp, on, &pipe->fasync_writers);\n\t\tif (retval < 0 && (filp->f_mode & FMODE_READ))\n\t\t\t/* this can happen only if on == T */\n\t\t\tfasync_helper(-1, filp, 0, &pipe->fasync_readers);\n\t}\n\t__pipe_unlock(pipe);\n\treturn retval;\n}\n\nunsigned long account_pipe_buffers(struct user_struct *user,\n\t\t\t\t   unsigned long old, unsigned long new)\n{\n\treturn atomic_long_add_return(new - old, &user->pipe_bufs);\n}\n\nbool too_many_pipe_buffers_soft(unsigned long user_bufs)\n{\n\tunsigned long soft_limit = READ_ONCE(pipe_user_pages_soft);\n\n\treturn soft_limit && user_bufs > soft_limit;\n}\n\nbool too_many_pipe_buffers_hard(unsigned long user_bufs)\n{\n\tunsigned long hard_limit = READ_ONCE(pipe_user_pages_hard);\n\n\treturn hard_limit && user_bufs > hard_limit;\n}\n\nbool pipe_is_unprivileged_user(void)\n{\n\treturn !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN);\n}\n\nstruct pipe_inode_info *alloc_pipe_info(void)\n{\n\tstruct pipe_inode_info *pipe;\n\tunsigned long pipe_bufs = PIPE_DEF_BUFFERS;\n\tstruct user_struct *user = get_current_user();\n\tunsigned long user_bufs;\n\tunsigned int max_size = READ_ONCE(pipe_max_size);\n\n\tpipe = kzalloc(sizeof(struct pipe_inode_info), GFP_KERNEL_ACCOUNT);\n\tif (pipe == NULL)\n\t\tgoto out_free_uid;\n\n\tif (pipe_bufs * PAGE_SIZE > max_size && !capable(CAP_SYS_RESOURCE))\n\t\tpipe_bufs = max_size >> PAGE_SHIFT;\n\n\tuser_bufs = account_pipe_buffers(user, 0, pipe_bufs);\n\n\tif (too_many_pipe_buffers_soft(user_bufs) && pipe_is_unprivileged_user()) {\n\t\tuser_bufs = account_pipe_buffers(user, pipe_bufs, PIPE_MIN_DEF_BUFFERS);\n\t\tpipe_bufs = PIPE_MIN_DEF_BUFFERS;\n\t}\n\n\tif (too_many_pipe_buffers_hard(user_bufs) && pipe_is_unprivileged_user())\n\t\tgoto out_revert_acct;\n\n\tpipe->bufs = kcalloc(pipe_bufs, sizeof(struct pipe_buffer),\n\t\t\t     GFP_KERNEL_ACCOUNT);\n\n\tif (pipe->bufs) {\n\t\tinit_waitqueue_head(&pipe->rd_wait);\n\t\tinit_waitqueue_head(&pipe->wr_wait);\n\t\tpipe->r_counter = pipe->w_counter = 1;\n\t\tpipe->max_usage = pipe_bufs;\n\t\tpipe->ring_size = pipe_bufs;\n\t\tpipe->nr_accounted = pipe_bufs;\n\t\tpipe->user = user;\n\t\tmutex_init(&pipe->mutex);\n\t\treturn pipe;\n\t}\n\nout_revert_acct:\n\t(void) account_pipe_buffers(user, pipe_bufs, 0);\n\tkfree(pipe);\nout_free_uid:\n\tfree_uid(user);\n\treturn NULL;\n}\n\nvoid free_pipe_info(struct pipe_inode_info *pipe)\n{\n\tunsigned int i;\n\n#ifdef CONFIG_WATCH_QUEUE\n\tif (pipe->watch_queue)\n\t\twatch_queue_clear(pipe->watch_queue);\n#endif\n\n\t(void) account_pipe_buffers(pipe->user, pipe->nr_accounted, 0);\n\tfree_uid(pipe->user);\n\tfor (i = 0; i < pipe->ring_size; i++) {\n\t\tstruct pipe_buffer *buf = pipe->bufs + i;\n\t\tif (buf->ops)\n\t\t\tpipe_buf_release(pipe, buf);\n\t}\n#ifdef CONFIG_WATCH_QUEUE\n\tif (pipe->watch_queue)\n\t\tput_watch_queue(pipe->watch_queue);\n#endif\n\tif (pipe->tmp_page)\n\t\t__free_page(pipe->tmp_page);\n\tkfree(pipe->bufs);\n\tkfree(pipe);\n}\n\nstatic struct vfsmount *pipe_mnt __read_mostly;\n\n/*\n * pipefs_dname() is called from d_path().\n */\nstatic char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n{\n\treturn dynamic_dname(dentry, buffer, buflen, \"pipe:[%lu]\",\n\t\t\t\td_inode(dentry)->i_ino);\n}\n\nstatic const struct dentry_operations pipefs_dentry_operations = {\n\t.d_dname\t= pipefs_dname,\n};\n\nstatic struct inode * get_pipe_inode(void)\n{\n\tstruct inode *inode = new_inode_pseudo(pipe_mnt->mnt_sb);\n\tstruct pipe_inode_info *pipe;\n\n\tif (!inode)\n\t\tgoto fail_inode;\n\n\tinode->i_ino = get_next_ino();\n\n\tpipe = alloc_pipe_info();\n\tif (!pipe)\n\t\tgoto fail_iput;\n\n\tinode->i_pipe = pipe;\n\tpipe->files = 2;\n\tpipe->readers = pipe->writers = 1;\n\tinode->i_fop = &pipefifo_fops;\n\n\t/*\n\t * Mark the inode dirty from the very beginning,\n\t * that way it will never be moved to the dirty\n\t * list because \"mark_inode_dirty()\" will think\n\t * that it already _is_ on the dirty list.\n\t */\n\tinode->i_state = I_DIRTY;\n\tinode->i_mode = S_IFIFO | S_IRUSR | S_IWUSR;\n\tinode->i_uid = current_fsuid();\n\tinode->i_gid = current_fsgid();\n\tinode->i_atime = inode->i_mtime = inode->i_ctime = current_time(inode);\n\n\treturn inode;\n\nfail_iput:\n\tiput(inode);\n\nfail_inode:\n\treturn NULL;\n}\n\nint create_pipe_files(struct file **res, int flags)\n{\n\tstruct inode *inode = get_pipe_inode();\n\tstruct file *f;\n\tint error;\n\n\tif (!inode)\n\t\treturn -ENFILE;\n\n\tif (flags & O_NOTIFICATION_PIPE) {\n\t\terror = watch_queue_init(inode->i_pipe);\n\t\tif (error) {\n\t\t\tfree_pipe_info(inode->i_pipe);\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n\t}\n\n\tf = alloc_file_pseudo(inode, pipe_mnt, \"\",\n\t\t\t\tO_WRONLY | (flags & (O_NONBLOCK | O_DIRECT)),\n\t\t\t\t&pipefifo_fops);\n\tif (IS_ERR(f)) {\n\t\tfree_pipe_info(inode->i_pipe);\n\t\tiput(inode);\n\t\treturn PTR_ERR(f);\n\t}\n\n\tf->private_data = inode->i_pipe;\n\n\tres[0] = alloc_file_clone(f, O_RDONLY | (flags & O_NONBLOCK),\n\t\t\t\t  &pipefifo_fops);\n\tif (IS_ERR(res[0])) {\n\t\tput_pipe_info(inode, inode->i_pipe);\n\t\tfput(f);\n\t\treturn PTR_ERR(res[0]);\n\t}\n\tres[0]->private_data = inode->i_pipe;\n\tres[1] = f;\n\tstream_open(inode, res[0]);\n\tstream_open(inode, res[1]);\n\treturn 0;\n}\n\nstatic int __do_pipe_flags(int *fd, struct file **files, int flags)\n{\n\tint error;\n\tint fdw, fdr;\n\n\tif (flags & ~(O_CLOEXEC | O_NONBLOCK | O_DIRECT | O_NOTIFICATION_PIPE))\n\t\treturn -EINVAL;\n\n\terror = create_pipe_files(files, flags);\n\tif (error)\n\t\treturn error;\n\n\terror = get_unused_fd_flags(flags);\n\tif (error < 0)\n\t\tgoto err_read_pipe;\n\tfdr = error;\n\n\terror = get_unused_fd_flags(flags);\n\tif (error < 0)\n\t\tgoto err_fdr;\n\tfdw = error;\n\n\taudit_fd_pair(fdr, fdw);\n\tfd[0] = fdr;\n\tfd[1] = fdw;\n\treturn 0;\n\n err_fdr:\n\tput_unused_fd(fdr);\n err_read_pipe:\n\tfput(files[0]);\n\tfput(files[1]);\n\treturn error;\n}\n\nint do_pipe_flags(int *fd, int flags)\n{\n\tstruct file *files[2];\n\tint error = __do_pipe_flags(fd, files, flags);\n\tif (!error) {\n\t\tfd_install(fd[0], files[0]);\n\t\tfd_install(fd[1], files[1]);\n\t}\n\treturn error;\n}\n\n/*\n * sys_pipe() is the normal C calling standard for creating\n * a pipe. It's not the way Unix traditionally does this, though.\n */\nstatic int do_pipe2(int __user *fildes, int flags)\n{\n\tstruct file *files[2];\n\tint fd[2];\n\tint error;\n\n\terror = __do_pipe_flags(fd, files, flags);\n\tif (!error) {\n\t\tif (unlikely(copy_to_user(fildes, fd, sizeof(fd)))) {\n\t\t\tfput(files[0]);\n\t\t\tfput(files[1]);\n\t\t\tput_unused_fd(fd[0]);\n\t\t\tput_unused_fd(fd[1]);\n\t\t\terror = -EFAULT;\n\t\t} else {\n\t\t\tfd_install(fd[0], files[0]);\n\t\t\tfd_install(fd[1], files[1]);\n\t\t}\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE2(pipe2, int __user *, fildes, int, flags)\n{\n\treturn do_pipe2(fildes, flags);\n}\n\nSYSCALL_DEFINE1(pipe, int __user *, fildes)\n{\n\treturn do_pipe2(fildes, 0);\n}\n\n/*\n * This is the stupid \"wait for pipe to be readable or writable\"\n * model.\n *\n * See pipe_read/write() for the proper kind of exclusive wait,\n * but that requires that we wake up any other readers/writers\n * if we then do not end up reading everything (ie the whole\n * \"wake_next_reader/writer\" logic in pipe_read/write()).\n */\nvoid pipe_wait_readable(struct pipe_inode_info *pipe)\n{\n\tpipe_unlock(pipe);\n\twait_event_interruptible(pipe->rd_wait, pipe_readable(pipe));\n\tpipe_lock(pipe);\n}\n\nvoid pipe_wait_writable(struct pipe_inode_info *pipe)\n{\n\tpipe_unlock(pipe);\n\twait_event_interruptible(pipe->wr_wait, pipe_writable(pipe));\n\tpipe_lock(pipe);\n}\n\n/*\n * This depends on both the wait (here) and the wakeup (wake_up_partner)\n * holding the pipe lock, so \"*cnt\" is stable and we know a wakeup cannot\n * race with the count check and waitqueue prep.\n *\n * Normally in order to avoid races, you'd do the prepare_to_wait() first,\n * then check the condition you're waiting for, and only then sleep. But\n * because of the pipe lock, we can check the condition before being on\n * the wait queue.\n *\n * We use the 'rd_wait' waitqueue for pipe partner waiting.\n */\nstatic int wait_for_partner(struct pipe_inode_info *pipe, unsigned int *cnt)\n{\n\tDEFINE_WAIT(rdwait);\n\tint cur = *cnt;\n\n\twhile (cur == *cnt) {\n\t\tprepare_to_wait(&pipe->rd_wait, &rdwait, TASK_INTERRUPTIBLE);\n\t\tpipe_unlock(pipe);\n\t\tschedule();\n\t\tfinish_wait(&pipe->rd_wait, &rdwait);\n\t\tpipe_lock(pipe);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t}\n\treturn cur == *cnt ? -ERESTARTSYS : 0;\n}\n\nstatic void wake_up_partner(struct pipe_inode_info *pipe)\n{\n\twake_up_interruptible_all(&pipe->rd_wait);\n}\n\nstatic int fifo_open(struct inode *inode, struct file *filp)\n{\n\tstruct pipe_inode_info *pipe;\n\tbool is_pipe = inode->i_sb->s_magic == PIPEFS_MAGIC;\n\tint ret;\n\n\tfilp->f_version = 0;\n\n\tspin_lock(&inode->i_lock);\n\tif (inode->i_pipe) {\n\t\tpipe = inode->i_pipe;\n\t\tpipe->files++;\n\t\tspin_unlock(&inode->i_lock);\n\t} else {\n\t\tspin_unlock(&inode->i_lock);\n\t\tpipe = alloc_pipe_info();\n\t\tif (!pipe)\n\t\t\treturn -ENOMEM;\n\t\tpipe->files = 1;\n\t\tspin_lock(&inode->i_lock);\n\t\tif (unlikely(inode->i_pipe)) {\n\t\t\tinode->i_pipe->files++;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tfree_pipe_info(pipe);\n\t\t\tpipe = inode->i_pipe;\n\t\t} else {\n\t\t\tinode->i_pipe = pipe;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t}\n\t}\n\tfilp->private_data = pipe;\n\t/* OK, we have a pipe and it's pinned down */\n\n\t__pipe_lock(pipe);\n\n\t/* We can only do regular read/write on fifos */\n\tstream_open(inode, filp);\n\n\tswitch (filp->f_mode & (FMODE_READ | FMODE_WRITE)) {\n\tcase FMODE_READ:\n\t/*\n\t *  O_RDONLY\n\t *  POSIX.1 says that O_NONBLOCK means return with the FIFO\n\t *  opened, even when there is no process writing the FIFO.\n\t */\n\t\tpipe->r_counter++;\n\t\tif (pipe->readers++ == 0)\n\t\t\twake_up_partner(pipe);\n\n\t\tif (!is_pipe && !pipe->writers) {\n\t\t\tif ((filp->f_flags & O_NONBLOCK)) {\n\t\t\t\t/* suppress EPOLLHUP until we have\n\t\t\t\t * seen a writer */\n\t\t\t\tfilp->f_version = pipe->w_counter;\n\t\t\t} else {\n\t\t\t\tif (wait_for_partner(pipe, &pipe->w_counter))\n\t\t\t\t\tgoto err_rd;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase FMODE_WRITE:\n\t/*\n\t *  O_WRONLY\n\t *  POSIX.1 says that O_NONBLOCK means return -1 with\n\t *  errno=ENXIO when there is no process reading the FIFO.\n\t */\n\t\tret = -ENXIO;\n\t\tif (!is_pipe && (filp->f_flags & O_NONBLOCK) && !pipe->readers)\n\t\t\tgoto err;\n\n\t\tpipe->w_counter++;\n\t\tif (!pipe->writers++)\n\t\t\twake_up_partner(pipe);\n\n\t\tif (!is_pipe && !pipe->readers) {\n\t\t\tif (wait_for_partner(pipe, &pipe->r_counter))\n\t\t\t\tgoto err_wr;\n\t\t}\n\t\tbreak;\n\n\tcase FMODE_READ | FMODE_WRITE:\n\t/*\n\t *  O_RDWR\n\t *  POSIX.1 leaves this case \"undefined\" when O_NONBLOCK is set.\n\t *  This implementation will NEVER block on a O_RDWR open, since\n\t *  the process can at least talk to itself.\n\t */\n\n\t\tpipe->readers++;\n\t\tpipe->writers++;\n\t\tpipe->r_counter++;\n\t\tpipe->w_counter++;\n\t\tif (pipe->readers == 1 || pipe->writers == 1)\n\t\t\twake_up_partner(pipe);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t/* Ok! */\n\t__pipe_unlock(pipe);\n\treturn 0;\n\nerr_rd:\n\tif (!--pipe->readers)\n\t\twake_up_interruptible(&pipe->wr_wait);\n\tret = -ERESTARTSYS;\n\tgoto err;\n\nerr_wr:\n\tif (!--pipe->writers)\n\t\twake_up_interruptible_all(&pipe->rd_wait);\n\tret = -ERESTARTSYS;\n\tgoto err;\n\nerr:\n\t__pipe_unlock(pipe);\n\n\tput_pipe_info(inode, pipe);\n\treturn ret;\n}\n\nconst struct file_operations pipefifo_fops = {\n\t.open\t\t= fifo_open,\n\t.llseek\t\t= no_llseek,\n\t.read_iter\t= pipe_read,\n\t.write_iter\t= pipe_write,\n\t.poll\t\t= pipe_poll,\n\t.unlocked_ioctl\t= pipe_ioctl,\n\t.release\t= pipe_release,\n\t.fasync\t\t= pipe_fasync,\n\t.splice_write\t= iter_file_splice_write,\n};\n\n/*\n * Currently we rely on the pipe array holding a power-of-2 number\n * of pages. Returns 0 on error.\n */\nunsigned int round_pipe_size(unsigned long size)\n{\n\tif (size > (1U << 31))\n\t\treturn 0;\n\n\t/* Minimum pipe size, as required by POSIX */\n\tif (size < PAGE_SIZE)\n\t\treturn PAGE_SIZE;\n\n\treturn roundup_pow_of_two(size);\n}\n\n/*\n * Resize the pipe ring to a number of slots.\n *\n * Note the pipe can be reduced in capacity, but only if the current\n * occupancy doesn't exceed nr_slots; if it does, EBUSY will be\n * returned instead.\n */\nint pipe_resize_ring(struct pipe_inode_info *pipe, unsigned int nr_slots)\n{\n\tstruct pipe_buffer *bufs;\n\tunsigned int head, tail, mask, n;\n\n\tbufs = kcalloc(nr_slots, sizeof(*bufs),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n\tif (unlikely(!bufs))\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(&pipe->rd_wait.lock);\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\n\tn = pipe_occupancy(head, tail);\n\tif (nr_slots < n) {\n\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\tkfree(bufs);\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The pipe array wraps around, so just start the new one at zero\n\t * and adjust the indices.\n\t */\n\tif (n > 0) {\n\t\tunsigned int h = head & mask;\n\t\tunsigned int t = tail & mask;\n\t\tif (h > t) {\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       n * sizeof(struct pipe_buffer));\n\t\t} else {\n\t\t\tunsigned int tsize = pipe->ring_size - t;\n\t\t\tif (h > 0)\n\t\t\t\tmemcpy(bufs + tsize, pipe->bufs,\n\t\t\t\t       h * sizeof(struct pipe_buffer));\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       tsize * sizeof(struct pipe_buffer));\n\t\t}\n\t}\n\n\thead = n;\n\ttail = 0;\n\n\tkfree(pipe->bufs);\n\tpipe->bufs = bufs;\n\tpipe->ring_size = nr_slots;\n\tif (pipe->max_usage > nr_slots)\n\t\tpipe->max_usage = nr_slots;\n\tpipe->tail = tail;\n\tpipe->head = head;\n\n\tspin_unlock_irq(&pipe->rd_wait.lock);\n\n\t/* This might have made more room for writers */\n\twake_up_interruptible(&pipe->wr_wait);\n\treturn 0;\n}\n\n/*\n * Allocate a new array of pipe buffers and copy the info over. Returns the\n * pipe size if successful, or return -ERROR on error.\n */\nstatic long pipe_set_size(struct pipe_inode_info *pipe, unsigned long arg)\n{\n\tunsigned long user_bufs;\n\tunsigned int nr_slots, size;\n\tlong ret = 0;\n\n#ifdef CONFIG_WATCH_QUEUE\n\tif (pipe->watch_queue)\n\t\treturn -EBUSY;\n#endif\n\n\tsize = round_pipe_size(arg);\n\tnr_slots = size >> PAGE_SHIFT;\n\n\tif (!nr_slots)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If trying to increase the pipe capacity, check that an\n\t * unprivileged user is not trying to exceed various limits\n\t * (soft limit check here, hard limit check just below).\n\t * Decreasing the pipe capacity is always permitted, even\n\t * if the user is currently over a limit.\n\t */\n\tif (nr_slots > pipe->max_usage &&\n\t\t\tsize > pipe_max_size && !capable(CAP_SYS_RESOURCE))\n\t\treturn -EPERM;\n\n\tuser_bufs = account_pipe_buffers(pipe->user, pipe->nr_accounted, nr_slots);\n\n\tif (nr_slots > pipe->max_usage &&\n\t\t\t(too_many_pipe_buffers_hard(user_bufs) ||\n\t\t\t too_many_pipe_buffers_soft(user_bufs)) &&\n\t\t\tpipe_is_unprivileged_user()) {\n\t\tret = -EPERM;\n\t\tgoto out_revert_acct;\n\t}\n\n\tret = pipe_resize_ring(pipe, nr_slots);\n\tif (ret < 0)\n\t\tgoto out_revert_acct;\n\n\tpipe->max_usage = nr_slots;\n\tpipe->nr_accounted = nr_slots;\n\treturn pipe->max_usage * PAGE_SIZE;\n\nout_revert_acct:\n\t(void) account_pipe_buffers(pipe->user, nr_slots, pipe->nr_accounted);\n\treturn ret;\n}\n\n/*\n * Note that i_pipe and i_cdev share the same location, so checking ->i_pipe is\n * not enough to verify that this is a pipe.\n */\nstruct pipe_inode_info *get_pipe_info(struct file *file, bool for_splice)\n{\n\tstruct pipe_inode_info *pipe = file->private_data;\n\n\tif (file->f_op != &pipefifo_fops || !pipe)\n\t\treturn NULL;\n#ifdef CONFIG_WATCH_QUEUE\n\tif (for_splice && pipe->watch_queue)\n\t\treturn NULL;\n#endif\n\treturn pipe;\n}\n\nlong pipe_fcntl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct pipe_inode_info *pipe;\n\tlong ret;\n\n\tpipe = get_pipe_info(file, false);\n\tif (!pipe)\n\t\treturn -EBADF;\n\n\t__pipe_lock(pipe);\n\n\tswitch (cmd) {\n\tcase F_SETPIPE_SZ:\n\t\tret = pipe_set_size(pipe, arg);\n\t\tbreak;\n\tcase F_GETPIPE_SZ:\n\t\tret = pipe->max_usage * PAGE_SIZE;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\t__pipe_unlock(pipe);\n\treturn ret;\n}\n\nstatic const struct super_operations pipefs_ops = {\n\t.destroy_inode = free_inode_nonrcu,\n\t.statfs = simple_statfs,\n};\n\n/*\n * pipefs should _never_ be mounted by userland - too much of security hassle,\n * no real gain from having the whole whorehouse mounted. So we don't need\n * any operations on the root directory. However, we need a non-trivial\n * d_name - pipe: will go nicely and kill the special-casing in procfs.\n */\n\nstatic int pipefs_init_fs_context(struct fs_context *fc)\n{\n\tstruct pseudo_fs_context *ctx = init_pseudo(fc, PIPEFS_MAGIC);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx->ops = &pipefs_ops;\n\tctx->dops = &pipefs_dentry_operations;\n\treturn 0;\n}\n\nstatic struct file_system_type pipe_fs_type = {\n\t.name\t\t= \"pipefs\",\n\t.init_fs_context = pipefs_init_fs_context,\n\t.kill_sb\t= kill_anon_super,\n};\n\n#ifdef CONFIG_SYSCTL\nstatic int do_proc_dopipe_max_size_conv(unsigned long *lvalp,\n\t\t\t\t\tunsigned int *valp,\n\t\t\t\t\tint write, void *data)\n{\n\tif (write) {\n\t\tunsigned int val;\n\n\t\tval = round_pipe_size(*lvalp);\n\t\tif (val == 0)\n\t\t\treturn -EINVAL;\n\n\t\t*valp = val;\n\t} else {\n\t\tunsigned int val = *valp;\n\t\t*lvalp = (unsigned long) val;\n\t}\n\n\treturn 0;\n}\n\nstatic int proc_dopipe_max_size(struct ctl_table *table, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn do_proc_douintvec(table, write, buffer, lenp, ppos,\n\t\t\t\t do_proc_dopipe_max_size_conv, NULL);\n}\n\nstatic struct ctl_table fs_pipe_sysctls[] = {\n\t{\n\t\t.procname\t= \"pipe-max-size\",\n\t\t.data\t\t= &pipe_max_size,\n\t\t.maxlen\t\t= sizeof(pipe_max_size),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dopipe_max_size,\n\t},\n\t{\n\t\t.procname\t= \"pipe-user-pages-hard\",\n\t\t.data\t\t= &pipe_user_pages_hard,\n\t\t.maxlen\t\t= sizeof(pipe_user_pages_hard),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"pipe-user-pages-soft\",\n\t\t.data\t\t= &pipe_user_pages_soft,\n\t\t.maxlen\t\t= sizeof(pipe_user_pages_soft),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{ }\n};\n#endif\n\nstatic int __init init_pipe_fs(void)\n{\n\tint err = register_filesystem(&pipe_fs_type);\n\n\tif (!err) {\n\t\tpipe_mnt = kern_mount(&pipe_fs_type);\n\t\tif (IS_ERR(pipe_mnt)) {\n\t\t\terr = PTR_ERR(pipe_mnt);\n\t\t\tunregister_filesystem(&pipe_fs_type);\n\t\t}\n\t}\n#ifdef CONFIG_SYSCTL\n\tregister_sysctl_init(\"fs\", fs_pipe_sysctls);\n#endif\n\treturn err;\n}\n\nfs_initcall(init_pipe_fs);\n"], "filenames": ["fs/pipe.c"], "buggy_code_start_loc": [1247], "buggy_code_end_loc": [1301], "fixing_code_start_loc": [1248], "fixing_code_end_loc": [1307], "type": "CWE-362", "message": "A race condition was found in the Linux kernel's watch queue due to a missing lock in pipe_resize_ring(). The specific flaw exists within the handling of pipe buffers. The issue results from the lack of proper locking when performing operations on an object. This flaw allows a local user to crash the system or escalate their privileges on the system.", "other": {"cve": {"id": "CVE-2022-2959", "sourceIdentifier": "secalert@redhat.com", "published": "2022-08-25T18:15:10.303", "lastModified": "2023-05-26T19:42:24.853", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A race condition was found in the Linux kernel's watch queue due to a missing lock in pipe_resize_ring(). The specific flaw exists within the handling of pipe buffers. The issue results from the lack of proper locking when performing operations on an object. This flaw allows a local user to crash the system or escalate their privileges on the system."}, {"lang": "es", "value": "Se ha encontrado una condici\u00f3n de carrera en la cola de vigilancia del kernel de Linux debido a una falta de bloqueo en la funci\u00f3n pipe_resize_ring(). El fallo espec\u00edfico se presenta en la administraci\u00f3n de los b\u00faferes de las tuber\u00edas. El problema es debido a una falta de bloqueo apropiado cuando son realizadas operaciones sobre un objeto. Este fallo permite a un usuario local bloquear el sistema o escalar sus privilegios en el sistema."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.0, "impactScore": 5.9}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}, {"lang": "en", "value": "CWE-667"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-667"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.8", "versionEndExcluding": "5.10.120", "matchCriteriaId": "2E3E8401-3132-4A03-AB2A-C9BF29856AFA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.11", "versionEndExcluding": "5.15.45", "matchCriteriaId": "08D699AD-F4CE-4BDD-A97E-4997299C7712"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.16", "versionEndExcluding": "5.17.13", "matchCriteriaId": "192FC54B-5367-49D6-B410-0285F14665B1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.18", "versionEndExcluding": "5.18.2", "matchCriteriaId": "9FF255A1-64F4-4E31-AF44-C92FB8773BA2"}]}]}], "references": [{"url": "https://github.com/torvalds/linux/commit/189b0ddc245139af81198d1a3637cac74f96e13a", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20230214-0005/", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://www.zerodayinitiative.com/advisories/ZDI-22-1165/", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/189b0ddc245139af81198d1a3637cac74f96e13a"}}