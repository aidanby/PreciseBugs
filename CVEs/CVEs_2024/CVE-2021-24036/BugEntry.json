{"buggy_code": ["/*\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#ifndef __STDC_LIMIT_MACROS\n#define __STDC_LIMIT_MACROS\n#endif\n\n#include <folly/io/IOBuf.h>\n\n#include <cassert>\n#include <cstdint>\n#include <cstdlib>\n#include <stdexcept>\n\n#include <folly/Conv.h>\n#include <folly/Likely.h>\n#include <folly/Memory.h>\n#include <folly/ScopeGuard.h>\n#include <folly/hash/SpookyHashV2.h>\n#include <folly/io/Cursor.h>\n#include <folly/lang/Align.h>\n#include <folly/lang/Exception.h>\n#include <folly/memory/Malloc.h>\n#include <folly/memory/SanitizeAddress.h>\n\n/*\n * Callbacks that will be invoked when IOBuf allocates or frees memory.\n * Note that io_buf_alloc_cb() will also be invoked when IOBuf takes ownership\n * of a malloc-allocated buffer, even if it was allocated earlier by another\n * part of the code.\n *\n * By default these are unimplemented, but programs can define these functions\n * to perform their own custom logic on memory allocation.  This is intended\n * primarily to help programs track memory usage and possibly take action\n * when thresholds are hit.  Callers should generally avoid performing any\n * expensive work in these callbacks, since they may be called from arbitrary\n * locations in the code that use IOBuf, possibly while holding locks.\n */\n\n#if FOLLY_HAVE_WEAK_SYMBOLS\nFOLLY_ATTR_WEAK void io_buf_alloc_cb(void* /*ptr*/, size_t /*size*/) noexcept;\nFOLLY_ATTR_WEAK void io_buf_free_cb(void* /*ptr*/, size_t /*size*/) noexcept;\n#else\nstatic void (*io_buf_alloc_cb)(void* /*ptr*/, size_t /*size*/) noexcept =\n    nullptr;\nstatic void (*io_buf_free_cb)(void* /*ptr*/, size_t /*size*/) noexcept =\n    nullptr;\n#endif\n\nusing std::unique_ptr;\n\nnamespace {\n\nenum : uint16_t {\n  kHeapMagic = 0xa5a5,\n  // This memory segment contains an IOBuf that is still in use\n  kIOBufInUse = 0x01,\n  // This memory segment contains buffer data that is still in use\n  kDataInUse = 0x02,\n  // This memory segment contains a SharedInfo that is still in use\n  kSharedInfoInUse = 0x04,\n};\n\nenum : std::size_t {\n  // When create() is called for buffers less than kDefaultCombinedBufSize,\n  // we allocate a single combined memory segment for the IOBuf and the data\n  // together.  See the comments for createCombined()/createSeparate() for more\n  // details.\n  //\n  // (The size of 1k is largely just a guess here.  We could could probably do\n  // benchmarks of real applications to see if adjusting this number makes a\n  // difference.  Callers that know their exact use case can also explicitly\n  // call createCombined() or createSeparate().)\n  kDefaultCombinedBufSize = 1024\n};\n\n// Helper function for IOBuf::takeOwnership()\n// The user's free function is not allowed to throw.\n// (We are already in the middle of throwing an exception, so\n// we cannot let this exception go unhandled.)\nvoid takeOwnershipError(\n    bool freeOnError,\n    void* buf,\n    folly::IOBuf::FreeFunction freeFn,\n    void* userData) noexcept {\n  if (!freeOnError) {\n    return;\n  }\n  if (!freeFn) {\n    free(buf);\n    return;\n  }\n  freeFn(buf, userData);\n}\n\n} // namespace\n\nnamespace folly {\n\n// use free for size >= 4GB\n// since we can store only 32 bits in the size var\nstruct IOBuf::HeapPrefix {\n  HeapPrefix(uint16_t flg, size_t sz)\n      : magic(kHeapMagic),\n        flags(flg),\n        size((sz == ((size_t)(uint32_t)sz)) ? static_cast<uint32_t>(sz) : 0) {}\n  ~HeapPrefix() {\n    // Reset magic to 0 on destruction.  This is solely for debugging purposes\n    // to help catch bugs where someone tries to use HeapStorage after it has\n    // been deleted.\n    magic = 0;\n  }\n\n  uint16_t magic;\n  std::atomic<uint16_t> flags;\n  uint32_t size;\n};\n\nstruct IOBuf::HeapStorage {\n  HeapPrefix prefix;\n  // The IOBuf is last in the HeapStorage object.\n  // This way operator new will work even if allocating a subclass of IOBuf\n  // that requires more space.\n  folly::IOBuf buf;\n};\n\nstruct IOBuf::HeapFullStorage {\n  // Make sure jemalloc allocates from the 64-byte class.  Putting this here\n  // because HeapStorage is private so it can't be at namespace level.\n  static_assert(sizeof(HeapStorage) <= 64, \"IOBuf may not grow over 56 bytes!\");\n\n  HeapStorage hs;\n  SharedInfo shared;\n  folly::max_align_t align;\n};\n\nIOBuf::SharedInfo::SharedInfo()\n    : freeFn(nullptr), userData(nullptr), useHeapFullStorage(false) {\n  // Use relaxed memory ordering here.  Since we are creating a new SharedInfo,\n  // no other threads should be referring to it yet.\n  refcount.store(1, std::memory_order_relaxed);\n}\n\nIOBuf::SharedInfo::SharedInfo(FreeFunction fn, void* arg, bool hfs)\n    : freeFn(fn), userData(arg), useHeapFullStorage(hfs) {\n  // Use relaxed memory ordering here.  Since we are creating a new SharedInfo,\n  // no other threads should be referring to it yet.\n  refcount.store(1, std::memory_order_relaxed);\n}\n\nvoid IOBuf::SharedInfo::invokeAndDeleteEachObserver(\n    SharedInfoObserverEntryBase* observerListHead, ObserverCb cb) noexcept {\n  if (observerListHead && cb) {\n    // break the chain\n    observerListHead->prev->next = nullptr;\n    auto entry = observerListHead;\n    while (entry) {\n      auto tmp = entry->next;\n      cb(*entry);\n      delete entry;\n      entry = tmp;\n    }\n  }\n}\n\nvoid IOBuf::SharedInfo::releaseStorage(SharedInfo* info) noexcept {\n  if (info->useHeapFullStorage) {\n    auto storageAddr =\n        reinterpret_cast<uint8_t*>(info) - offsetof(HeapFullStorage, shared);\n    auto storage = reinterpret_cast<HeapFullStorage*>(storageAddr);\n    info->~SharedInfo();\n    IOBuf::releaseStorage(&storage->hs, kSharedInfoInUse);\n  }\n}\n\nvoid* IOBuf::operator new(size_t size) {\n  size_t fullSize = offsetof(HeapStorage, buf) + size;\n  auto storage = static_cast<HeapStorage*>(checkedMalloc(fullSize));\n\n  new (&storage->prefix) HeapPrefix(kIOBufInUse, fullSize);\n\n  if (io_buf_alloc_cb) {\n    io_buf_alloc_cb(storage, fullSize);\n  }\n\n  return &(storage->buf);\n}\n\nvoid* IOBuf::operator new(size_t /* size */, void* ptr) {\n  return ptr;\n}\n\nvoid IOBuf::operator delete(void* ptr) {\n  auto storageAddr = static_cast<uint8_t*>(ptr) - offsetof(HeapStorage, buf);\n  auto storage = reinterpret_cast<HeapStorage*>(storageAddr);\n  releaseStorage(storage, kIOBufInUse);\n}\n\nvoid IOBuf::operator delete(void* /* ptr */, void* /* placement */) {\n  // Provide matching operator for `IOBuf::new` to avoid MSVC compilation\n  // warning (C4291) about memory leak when exception is thrown in the\n  // constructor.\n}\n\nvoid IOBuf::releaseStorage(HeapStorage* storage, uint16_t freeFlags) noexcept {\n  CHECK_EQ(storage->prefix.magic, static_cast<uint16_t>(kHeapMagic));\n\n  // Use relaxed memory order here.  If we are unlucky and happen to get\n  // out-of-date data the compare_exchange_weak() call below will catch\n  // it and load new data with memory_order_acq_rel.\n  auto flags = storage->prefix.flags.load(std::memory_order_acquire);\n  DCHECK_EQ((flags & freeFlags), freeFlags);\n\n  while (true) {\n    auto newFlags = uint16_t(flags & ~freeFlags);\n    if (newFlags == 0) {\n      // save the size\n      size_t size = storage->prefix.size;\n      // The storage space is now unused.  Free it.\n      storage->prefix.HeapPrefix::~HeapPrefix();\n      if (FOLLY_LIKELY(size)) {\n        if (io_buf_free_cb) {\n          io_buf_free_cb(storage, size);\n        }\n        sizedFree(storage, size);\n      } else {\n        free(storage);\n      }\n      return;\n    }\n\n    // This storage segment still contains portions that are in use.\n    // Just clear the flags specified in freeFlags for now.\n    auto ret = storage->prefix.flags.compare_exchange_weak(\n        flags, newFlags, std::memory_order_acq_rel);\n    if (ret) {\n      // We successfully updated the flags.\n      return;\n    }\n\n    // We failed to update the flags.  Some other thread probably updated them\n    // and cleared some of the other bits.  Continue around the loop to see if\n    // we are the last user now, or if we need to try updating the flags again.\n  }\n}\n\nvoid IOBuf::freeInternalBuf(void* /* buf */, void* userData) noexcept {\n  auto storage = static_cast<HeapStorage*>(userData);\n  releaseStorage(storage, kDataInUse);\n}\n\nIOBuf::IOBuf(CreateOp, std::size_t capacity)\n    : next_(this),\n      prev_(this),\n      data_(nullptr),\n      length_(0),\n      flagsAndSharedInfo_(0) {\n  SharedInfo* info;\n  allocExtBuffer(capacity, &buf_, &info, &capacity_);\n  setSharedInfo(info);\n  data_ = buf_;\n}\n\nIOBuf::IOBuf(\n    CopyBufferOp /* op */,\n    const void* buf,\n    std::size_t size,\n    std::size_t headroom,\n    std::size_t minTailroom)\n    : IOBuf(CREATE, headroom + size + minTailroom) {\n  advance(headroom);\n  if (size > 0) {\n    assert(buf != nullptr);\n    memcpy(writableData(), buf, size);\n    append(size);\n  }\n}\n\nIOBuf::IOBuf(\n    CopyBufferOp op,\n    ByteRange br,\n    std::size_t headroom,\n    std::size_t minTailroom)\n    : IOBuf(op, br.data(), br.size(), headroom, minTailroom) {}\n\nunique_ptr<IOBuf> IOBuf::create(std::size_t capacity) {\n  // For smaller-sized buffers, allocate the IOBuf, SharedInfo, and the buffer\n  // all with a single allocation.\n  //\n  // We don't do this for larger buffers since it can be wasteful if the user\n  // needs to reallocate the buffer but keeps using the same IOBuf object.\n  // In this case we can't free the data space until the IOBuf is also\n  // destroyed.  Callers can explicitly call createCombined() or\n  // createSeparate() if they know their use case better, and know if they are\n  // likely to reallocate the buffer later.\n  if (capacity <= kDefaultCombinedBufSize) {\n    return createCombined(capacity);\n  }\n\n  // if we have nallocx, we want to allocate the capacity and the overhead in\n  // a single allocation only if we do not cross into the next allocation class\n  // for some buffer sizes, this can use about 25% extra memory\n  if (canNallocx()) {\n    auto mallocSize = goodMallocSize(capacity);\n    // round capacity to a multiple of 8\n    size_t minSize = ((capacity + 7) & ~7) + sizeof(SharedInfo);\n    // if we do not have space for the overhead, allocate the mem separateley\n    if (mallocSize < minSize) {\n      auto* buf = checkedMalloc(mallocSize);\n      return takeOwnership(SIZED_FREE, buf, mallocSize, 0, 0);\n    }\n  }\n\n  return createSeparate(capacity);\n}\n\nunique_ptr<IOBuf> IOBuf::createCombined(std::size_t capacity) {\n  // To save a memory allocation, allocate space for the IOBuf object, the\n  // SharedInfo struct, and the data itself all with a single call to malloc().\n  size_t requiredStorage = offsetof(HeapFullStorage, align) + capacity;\n  size_t mallocSize = goodMallocSize(requiredStorage);\n  auto storage = static_cast<HeapFullStorage*>(checkedMalloc(mallocSize));\n\n  new (&storage->hs.prefix) HeapPrefix(kIOBufInUse | kDataInUse, mallocSize);\n  new (&storage->shared) SharedInfo(freeInternalBuf, storage);\n\n  if (io_buf_alloc_cb) {\n    io_buf_alloc_cb(storage, mallocSize);\n  }\n\n  auto bufAddr = reinterpret_cast<uint8_t*>(&storage->align);\n  uint8_t* storageEnd = reinterpret_cast<uint8_t*>(storage) + mallocSize;\n  auto actualCapacity = size_t(storageEnd - bufAddr);\n  unique_ptr<IOBuf> ret(new (&storage->hs.buf) IOBuf(\n      InternalConstructor(),\n      packFlagsAndSharedInfo(0, &storage->shared),\n      bufAddr,\n      actualCapacity,\n      bufAddr,\n      0));\n  return ret;\n}\n\nunique_ptr<IOBuf> IOBuf::createSeparate(std::size_t capacity) {\n  return std::make_unique<IOBuf>(CREATE, capacity);\n}\n\nunique_ptr<IOBuf> IOBuf::createChain(\n    size_t totalCapacity, std::size_t maxBufCapacity) {\n  unique_ptr<IOBuf> out =\n      create(std::min(totalCapacity, size_t(maxBufCapacity)));\n  size_t allocatedCapacity = out->capacity();\n\n  while (allocatedCapacity < totalCapacity) {\n    unique_ptr<IOBuf> newBuf = create(\n        std::min(totalCapacity - allocatedCapacity, size_t(maxBufCapacity)));\n    allocatedCapacity += newBuf->capacity();\n    out->prependChain(std::move(newBuf));\n  }\n\n  return out;\n}\n\nsize_t IOBuf::goodSize(size_t minCapacity, CombinedOption combined) {\n  if (combined == CombinedOption::DEFAULT) {\n    combined = minCapacity <= kDefaultCombinedBufSize\n        ? CombinedOption::COMBINED\n        : CombinedOption::SEPARATE;\n  }\n  size_t overhead;\n  if (combined == CombinedOption::COMBINED) {\n    overhead = offsetof(HeapFullStorage, align);\n  } else {\n    // Pad minCapacity to a multiple of 8\n    minCapacity = (minCapacity + 7) & ~7;\n    overhead = sizeof(SharedInfo);\n  }\n  size_t goodSize = folly::goodMallocSize(minCapacity + overhead);\n  return goodSize - overhead;\n}\n\nIOBuf::IOBuf(\n    TakeOwnershipOp,\n    void* buf,\n    std::size_t capacity,\n    std::size_t offset,\n    std::size_t length,\n    FreeFunction freeFn,\n    void* userData,\n    bool freeOnError)\n    : next_(this),\n      prev_(this),\n      data_(static_cast<uint8_t*>(buf) + offset),\n      buf_(static_cast<uint8_t*>(buf)),\n      length_(length),\n      capacity_(capacity),\n      flagsAndSharedInfo_(\n          packFlagsAndSharedInfo(kFlagFreeSharedInfo, nullptr)) {\n  // do not allow only user data without a freeFn\n  // since we use that for folly::sizedFree\n  DCHECK(!userData || (userData && freeFn));\n\n  auto rollback = makeGuard([&] { //\n    takeOwnershipError(freeOnError, buf, freeFn, userData);\n  });\n  setSharedInfo(new SharedInfo(freeFn, userData));\n  rollback.dismiss();\n}\n\nIOBuf::IOBuf(\n    TakeOwnershipOp,\n    SizedFree,\n    void* buf,\n    std::size_t capacity,\n    std::size_t offset,\n    std::size_t length,\n    bool freeOnError)\n    : next_(this),\n      prev_(this),\n      data_(static_cast<uint8_t*>(buf) + offset),\n      buf_(static_cast<uint8_t*>(buf)),\n      length_(length),\n      capacity_(capacity),\n      flagsAndSharedInfo_(\n          packFlagsAndSharedInfo(kFlagFreeSharedInfo, nullptr)) {\n  auto rollback = makeGuard([&] { //\n    takeOwnershipError(freeOnError, buf, nullptr, nullptr);\n  });\n  setSharedInfo(new SharedInfo(nullptr, reinterpret_cast<void*>(capacity)));\n  rollback.dismiss();\n\n  if (io_buf_alloc_cb && capacity) {\n    io_buf_alloc_cb(buf, capacity);\n  }\n}\n\nunique_ptr<IOBuf> IOBuf::takeOwnership(\n    void* buf,\n    std::size_t capacity,\n    std::size_t offset,\n    std::size_t length,\n    FreeFunction freeFn,\n    void* userData,\n    bool freeOnError,\n    TakeOwnershipOption option) {\n  // do not allow only user data without a freeFn\n  // since we use that for folly::sizedFree\n\n  DCHECK(\n      !userData || (userData && freeFn) ||\n      (userData && !freeFn && (option == TakeOwnershipOption::STORE_SIZE)));\n\n  HeapFullStorage* storage = nullptr;\n  auto rollback = makeGuard([&] {\n    if (storage) {\n      free(storage);\n    }\n    takeOwnershipError(freeOnError, buf, freeFn, userData);\n  });\n\n  size_t requiredStorage = sizeof(HeapFullStorage);\n  size_t mallocSize = goodMallocSize(requiredStorage);\n  storage = static_cast<HeapFullStorage*>(checkedMalloc(mallocSize));\n\n  new (&storage->hs.prefix)\n      HeapPrefix(kIOBufInUse | kSharedInfoInUse, mallocSize);\n  new (&storage->shared)\n      SharedInfo(freeFn, userData, true /*useHeapFullStorage*/);\n\n  auto result = unique_ptr<IOBuf>(new (&storage->hs.buf) IOBuf(\n      InternalConstructor(),\n      packFlagsAndSharedInfo(0, &storage->shared),\n      static_cast<uint8_t*>(buf),\n      capacity,\n      static_cast<uint8_t*>(buf) + offset,\n      length));\n\n  rollback.dismiss();\n\n  if (io_buf_alloc_cb) {\n    io_buf_alloc_cb(storage, mallocSize);\n    if (userData && !freeFn && (option == TakeOwnershipOption::STORE_SIZE)) {\n      // Even though we did not allocate the buffer, call io_buf_alloc_cb()\n      // since we will call io_buf_free_cb() on destruction, and we want these\n      // calls to be 1:1.\n      io_buf_alloc_cb(buf, capacity);\n    }\n  }\n\n  return result;\n}\n\nIOBuf::IOBuf(WrapBufferOp, const void* buf, std::size_t capacity) noexcept\n    : IOBuf(\n          InternalConstructor(),\n          0,\n          // We cast away the const-ness of the buffer here.\n          // This is okay since IOBuf users must use unshare() to create a copy\n          // of this buffer before writing to the buffer.\n          static_cast<uint8_t*>(const_cast<void*>(buf)),\n          capacity,\n          static_cast<uint8_t*>(const_cast<void*>(buf)),\n          capacity) {}\n\nIOBuf::IOBuf(WrapBufferOp op, ByteRange br) noexcept\n    : IOBuf(op, br.data(), br.size()) {}\n\nunique_ptr<IOBuf> IOBuf::wrapBuffer(const void* buf, std::size_t capacity) {\n  return std::make_unique<IOBuf>(WRAP_BUFFER, buf, capacity);\n}\n\nIOBuf IOBuf::wrapBufferAsValue(const void* buf, std::size_t capacity) noexcept {\n  return IOBuf(WrapBufferOp::WRAP_BUFFER, buf, capacity);\n}\n\nIOBuf::IOBuf() noexcept = default;\n\nIOBuf::IOBuf(IOBuf&& other) noexcept\n    : data_(other.data_),\n      buf_(other.buf_),\n      length_(other.length_),\n      capacity_(other.capacity_),\n      flagsAndSharedInfo_(other.flagsAndSharedInfo_) {\n  // Reset other so it is a clean state to be destroyed.\n  other.data_ = nullptr;\n  other.buf_ = nullptr;\n  other.length_ = 0;\n  other.capacity_ = 0;\n  other.flagsAndSharedInfo_ = 0;\n\n  // If other was part of the chain, assume ownership of the rest of its chain.\n  // (It's only valid to perform move assignment on the head of a chain.)\n  if (other.next_ != &other) {\n    next_ = other.next_;\n    next_->prev_ = this;\n    other.next_ = &other;\n\n    prev_ = other.prev_;\n    prev_->next_ = this;\n    other.prev_ = &other;\n  }\n\n  // Sanity check to make sure that other is in a valid state to be destroyed.\n  DCHECK_EQ(other.prev_, &other);\n  DCHECK_EQ(other.next_, &other);\n}\n\nIOBuf::IOBuf(const IOBuf& other) {\n  *this = other.cloneAsValue();\n}\n\nIOBuf::IOBuf(\n    InternalConstructor,\n    uintptr_t flagsAndSharedInfo,\n    uint8_t* buf,\n    std::size_t capacity,\n    uint8_t* data,\n    std::size_t length) noexcept\n    : next_(this),\n      prev_(this),\n      data_(data),\n      buf_(buf),\n      length_(length),\n      capacity_(capacity),\n      flagsAndSharedInfo_(flagsAndSharedInfo) {\n  assert(data >= buf);\n  assert(data + length <= buf + capacity);\n\n  CHECK(!folly::asan_region_is_poisoned(buf, capacity));\n}\n\nIOBuf::~IOBuf() {\n  // Destroying an IOBuf destroys the entire chain.\n  // Users of IOBuf should only explicitly delete the head of any chain.\n  // The other elements in the chain will be automatically destroyed.\n  while (next_ != this) {\n    // Since unlink() returns unique_ptr() and we don't store it,\n    // it will automatically delete the unlinked element.\n    (void)next_->unlink();\n  }\n\n  decrementRefcount();\n}\n\nIOBuf& IOBuf::operator=(IOBuf&& other) noexcept {\n  if (this == &other) {\n    return *this;\n  }\n\n  // If we are part of a chain, delete the rest of the chain.\n  while (next_ != this) {\n    // Since unlink() returns unique_ptr() and we don't store it,\n    // it will automatically delete the unlinked element.\n    (void)next_->unlink();\n  }\n\n  // Decrement our refcount on the current buffer\n  decrementRefcount();\n\n  // Take ownership of the other buffer's data\n  data_ = other.data_;\n  buf_ = other.buf_;\n  length_ = other.length_;\n  capacity_ = other.capacity_;\n  flagsAndSharedInfo_ = other.flagsAndSharedInfo_;\n  // Reset other so it is a clean state to be destroyed.\n  other.data_ = nullptr;\n  other.buf_ = nullptr;\n  other.length_ = 0;\n  other.capacity_ = 0;\n  other.flagsAndSharedInfo_ = 0;\n\n  // If other was part of the chain, assume ownership of the rest of its chain.\n  // (It's only valid to perform move assignment on the head of a chain.)\n  if (other.next_ != &other) {\n    next_ = other.next_;\n    next_->prev_ = this;\n    other.next_ = &other;\n\n    prev_ = other.prev_;\n    prev_->next_ = this;\n    other.prev_ = &other;\n  }\n\n  // Sanity check to make sure that other is in a valid state to be destroyed.\n  DCHECK_EQ(other.prev_, &other);\n  DCHECK_EQ(other.next_, &other);\n\n  return *this;\n}\n\nIOBuf& IOBuf::operator=(const IOBuf& other) {\n  if (this != &other) {\n    *this = IOBuf(other);\n  }\n  return *this;\n}\n\nbool IOBuf::empty() const {\n  const IOBuf* current = this;\n  do {\n    if (current->length() != 0) {\n      return false;\n    }\n    current = current->next_;\n  } while (current != this);\n  return true;\n}\n\nsize_t IOBuf::countChainElements() const {\n  size_t numElements = 1;\n  for (IOBuf* current = next_; current != this; current = current->next_) {\n    ++numElements;\n  }\n  return numElements;\n}\n\nstd::size_t IOBuf::computeChainDataLength() const {\n  std::size_t fullLength = length_;\n  for (IOBuf* current = next_; current != this; current = current->next_) {\n    fullLength += current->length_;\n  }\n  return fullLength;\n}\n\nstd::size_t IOBuf::computeChainCapacity() const {\n  std::size_t fullCapacity = capacity_;\n  for (IOBuf* current = next_; current != this; current = current->next_) {\n    fullCapacity += current->capacity_;\n  }\n  return fullCapacity;\n}\n\nvoid IOBuf::prependChain(unique_ptr<IOBuf>&& iobuf) {\n  // Take ownership of the specified IOBuf\n  IOBuf* other = iobuf.release();\n\n  // Remember the pointer to the tail of the other chain\n  IOBuf* otherTail = other->prev_;\n\n  // Hook up prev_->next_ to point at the start of the other chain,\n  // and other->prev_ to point at prev_\n  prev_->next_ = other;\n  other->prev_ = prev_;\n\n  // Hook up otherTail->next_ to point at us,\n  // and prev_ to point back at otherTail,\n  otherTail->next_ = this;\n  prev_ = otherTail;\n}\n\nunique_ptr<IOBuf> IOBuf::clone() const {\n  auto tmp = cloneOne();\n\n  for (IOBuf* current = next_; current != this; current = current->next_) {\n    tmp->prependChain(current->cloneOne());\n  }\n\n  return tmp;\n}\n\nunique_ptr<IOBuf> IOBuf::cloneOne() const {\n  if (SharedInfo* info = sharedInfo()) {\n    info->refcount.fetch_add(1, std::memory_order_acq_rel);\n  }\n  return std::unique_ptr<IOBuf>(new IOBuf(\n      InternalConstructor(),\n      flagsAndSharedInfo_,\n      buf_,\n      capacity_,\n      data_,\n      length_));\n}\n\nunique_ptr<IOBuf> IOBuf::cloneCoalesced() const {\n  return std::make_unique<IOBuf>(cloneCoalescedAsValue());\n}\n\nunique_ptr<IOBuf> IOBuf::cloneCoalescedWithHeadroomTailroom(\n    std::size_t newHeadroom, std::size_t newTailroom) const {\n  return std::make_unique<IOBuf>(\n      cloneCoalescedAsValueWithHeadroomTailroom(newHeadroom, newTailroom));\n}\n\nIOBuf IOBuf::cloneAsValue() const {\n  auto tmp = cloneOneAsValue();\n\n  for (IOBuf* current = next_; current != this; current = current->next_) {\n    tmp.prependChain(current->cloneOne());\n  }\n\n  return tmp;\n}\n\nIOBuf IOBuf::cloneOneAsValue() const {\n  if (SharedInfo* info = sharedInfo()) {\n    info->refcount.fetch_add(1, std::memory_order_acq_rel);\n  }\n  return IOBuf(\n      InternalConstructor(),\n      flagsAndSharedInfo_,\n      buf_,\n      capacity_,\n      data_,\n      length_);\n}\n\nIOBuf IOBuf::cloneCoalescedAsValue() const {\n  const std::size_t newHeadroom = headroom();\n  const std::size_t newTailroom = prev()->tailroom();\n  return cloneCoalescedAsValueWithHeadroomTailroom(newHeadroom, newTailroom);\n}\n\nIOBuf IOBuf::cloneCoalescedAsValueWithHeadroomTailroom(\n    std::size_t newHeadroom, std::size_t newTailroom) const {\n  if (!isChained() && newHeadroom <= headroom() && newTailroom <= tailroom()) {\n    return cloneOneAsValue();\n  }\n  // Coalesce into newBuf\n  const std::size_t newLength = computeChainDataLength();\n  const std::size_t newCapacity = newLength + newHeadroom + newTailroom;\n  IOBuf newBuf{CREATE, newCapacity};\n  newBuf.advance(newHeadroom);\n\n  auto current = this;\n  do {\n    if (current->length() > 0) {\n      DCHECK_NOTNULL(current->data());\n      DCHECK_LE(current->length(), newBuf.tailroom());\n      memcpy(newBuf.writableTail(), current->data(), current->length());\n      newBuf.append(current->length());\n    }\n    current = current->next();\n  } while (current != this);\n\n  DCHECK_EQ(newLength, newBuf.length());\n  DCHECK_EQ(newHeadroom, newBuf.headroom());\n  DCHECK_LE(newTailroom, newBuf.tailroom());\n\n  return newBuf;\n}\n\nvoid IOBuf::unshareOneSlow() {\n  // Allocate a new buffer for the data\n  uint8_t* buf;\n  SharedInfo* sharedInfo;\n  std::size_t actualCapacity;\n  allocExtBuffer(capacity_, &buf, &sharedInfo, &actualCapacity);\n\n  // Copy the data\n  // Maintain the same amount of headroom.  Since we maintained the same\n  // minimum capacity we also maintain at least the same amount of tailroom.\n  std::size_t headlen = headroom();\n  if (length_ > 0) {\n    assert(data_ != nullptr);\n    memcpy(buf + headlen, data_, length_);\n  }\n\n  // Release our reference on the old buffer\n  decrementRefcount();\n  // Make sure flags are all cleared.\n  setFlagsAndSharedInfo(0, sharedInfo);\n\n  // Update the buffer pointers to point to the new buffer\n  data_ = buf + headlen;\n  buf_ = buf;\n}\n\nvoid IOBuf::unshareChained() {\n  // unshareChained() should only be called if we are part of a chain of\n  // multiple IOBufs.  The caller should have already verified this.\n  assert(isChained());\n\n  IOBuf* current = this;\n  while (true) {\n    if (current->isSharedOne()) {\n      // we have to unshare\n      break;\n    }\n\n    current = current->next_;\n    if (current == this) {\n      // None of the IOBufs in the chain are shared,\n      // so return without doing anything\n      return;\n    }\n  }\n\n  // We have to unshare.  Let coalesceSlow() do the work.\n  coalesceSlow();\n}\n\nvoid IOBuf::markExternallyShared() {\n  IOBuf* current = this;\n  do {\n    current->markExternallySharedOne();\n    current = current->next_;\n  } while (current != this);\n}\n\nvoid IOBuf::makeManagedChained() {\n  assert(isChained());\n\n  IOBuf* current = this;\n  while (true) {\n    current->makeManagedOne();\n    current = current->next_;\n    if (current == this) {\n      break;\n    }\n  }\n}\n\nvoid IOBuf::coalesceSlow() {\n  // coalesceSlow() should only be called if we are part of a chain of multiple\n  // IOBufs.  The caller should have already verified this.\n  DCHECK(isChained());\n\n  // Compute the length of the entire chain\n  std::size_t newLength = 0;\n  IOBuf* end = this;\n  do {\n    newLength += end->length_;\n    end = end->next_;\n  } while (end != this);\n\n  coalesceAndReallocate(newLength, end);\n  // We should be only element left in the chain now\n  DCHECK(!isChained());\n}\n\nvoid IOBuf::coalesceSlow(size_t maxLength) {\n  // coalesceSlow() should only be called if we are part of a chain of multiple\n  // IOBufs.  The caller should have already verified this.\n  DCHECK(isChained());\n  DCHECK_LT(length_, maxLength);\n\n  // Compute the length of the entire chain\n  std::size_t newLength = 0;\n  IOBuf* end = this;\n  while (true) {\n    newLength += end->length_;\n    end = end->next_;\n    if (newLength >= maxLength) {\n      break;\n    }\n    if (end == this) {\n      throw_exception<std::overflow_error>(\n          \"attempted to coalesce more data than \"\n          \"available\");\n    }\n  }\n\n  coalesceAndReallocate(newLength, end);\n  // We should have the requested length now\n  DCHECK_GE(length_, maxLength);\n}\n\nvoid IOBuf::coalesceAndReallocate(\n    size_t newHeadroom, size_t newLength, IOBuf* end, size_t newTailroom) {\n  std::size_t newCapacity = newLength + newHeadroom + newTailroom;\n\n  // Allocate space for the coalesced buffer.\n  // We always convert to an external buffer, even if we happened to be an\n  // internal buffer before.\n  uint8_t* newBuf;\n  SharedInfo* newInfo;\n  std::size_t actualCapacity;\n  allocExtBuffer(newCapacity, &newBuf, &newInfo, &actualCapacity);\n\n  // Copy the data into the new buffer\n  uint8_t* newData = newBuf + newHeadroom;\n  uint8_t* p = newData;\n  IOBuf* current = this;\n  size_t remaining = newLength;\n  do {\n    if (current->length_ > 0) {\n      assert(current->length_ <= remaining);\n      assert(current->data_ != nullptr);\n      remaining -= current->length_;\n      memcpy(p, current->data_, current->length_);\n      p += current->length_;\n    }\n    current = current->next_;\n  } while (current != end);\n  assert(remaining == 0);\n\n  // Point at the new buffer\n  decrementRefcount();\n\n  // Make sure flags are all cleared.\n  setFlagsAndSharedInfo(0, newInfo);\n\n  capacity_ = actualCapacity;\n  buf_ = newBuf;\n  data_ = newData;\n  length_ = newLength;\n\n  // Separate from the rest of our chain.\n  // Since we don't store the unique_ptr returned by separateChain(),\n  // this will immediately delete the returned subchain.\n  if (isChained()) {\n    (void)separateChain(next_, current->prev_);\n  }\n}\n\nvoid IOBuf::decrementRefcount() noexcept {\n  // Externally owned buffers don't have a SharedInfo object and aren't managed\n  // by the reference count\n  SharedInfo* info = sharedInfo();\n  if (!info) {\n    return;\n  }\n\n  // Avoid doing atomic decrement if the refcount is 1.\n  // This is safe, because it means that we're the last reference and destroying\n  // the object. Anything trying to copy it is already undefined behavior.\n  if (info->refcount.load(std::memory_order_acquire) > 1) {\n    // Decrement the refcount\n    uint32_t newcnt = info->refcount.fetch_sub(1, std::memory_order_acq_rel);\n    // Note that fetch_sub() returns the value before we decremented.\n    // If it is 1, we were the only remaining user; if it is greater there are\n    // still other users.\n    if (newcnt > 1) {\n      return;\n    }\n  }\n\n  // save the useHeapFullStorage flag here since\n  // freeExtBuffer can delete the sharedInfo()\n  bool useHeapFullStorage = info->useHeapFullStorage;\n\n  // We were the last user.  Free the buffer\n  freeExtBuffer();\n\n  // Free the SharedInfo if it was allocated separately.\n  //\n  // This is only used by takeOwnership().\n  //\n  // To avoid this special case handling in decrementRefcount(), we could have\n  // takeOwnership() set a custom freeFn() that calls the user's free function\n  // then frees the SharedInfo object.  (This would require that\n  // takeOwnership() store the user's free function with its allocated\n  // SharedInfo object.)  However, handling this specially with a flag seems\n  // like it shouldn't be problematic.\n  if (flags() & kFlagFreeSharedInfo) {\n    delete info;\n  } else {\n    if (useHeapFullStorage) {\n      SharedInfo::releaseStorage(info);\n    }\n  }\n}\n\nvoid IOBuf::reserveSlow(std::size_t minHeadroom, std::size_t minTailroom) {\n  size_t newCapacity = (size_t)length_ + minHeadroom + minTailroom;\n  DCHECK_LT(newCapacity, UINT32_MAX);\n\n  // reserveSlow() is dangerous if anyone else is sharing the buffer, as we may\n  // reallocate and free the original buffer.  It should only ever be called if\n  // we are the only user of the buffer.\n  DCHECK(!isSharedOne());\n\n  // We'll need to reallocate the buffer.\n  // There are a few options.\n  // - If we have enough total room, move the data around in the buffer\n  //   and adjust the data_ pointer.\n  // - If we're using an internal buffer, we'll switch to an external\n  //   buffer with enough headroom and tailroom.\n  // - If we have enough headroom (headroom() >= minHeadroom) but not too much\n  //   (so we don't waste memory), we can try one of two things, depending on\n  //   whether we use jemalloc or not:\n  //   - If using jemalloc, we can try to expand in place, avoiding a memcpy()\n  //   - If not using jemalloc and we don't have too much to copy,\n  //     we'll use realloc() (note that realloc might have to copy\n  //     headroom + data + tailroom, see smartRealloc in folly/memory/Malloc.h)\n  // - Otherwise, bite the bullet and reallocate.\n  if (headroom() + tailroom() >= minHeadroom + minTailroom) {\n    uint8_t* newData = writableBuffer() + minHeadroom;\n    memmove(newData, data_, length_);\n    data_ = newData;\n    return;\n  }\n\n  size_t newAllocatedCapacity = 0;\n  uint8_t* newBuffer = nullptr;\n  std::size_t newHeadroom = 0;\n  std::size_t oldHeadroom = headroom();\n\n  // If we have a buffer allocated with malloc and we just need more tailroom,\n  // try to use realloc()/xallocx() to grow the buffer in place.\n  SharedInfo* info = sharedInfo();\n  bool useHeapFullStorage = info && info->useHeapFullStorage;\n  if (info && (info->freeFn == nullptr) && length_ != 0 &&\n      oldHeadroom >= minHeadroom) {\n    size_t headSlack = oldHeadroom - minHeadroom;\n    newAllocatedCapacity = goodExtBufferSize(newCapacity + headSlack);\n    if (usingJEMalloc()) {\n      // We assume that tailroom is more useful and more important than\n      // headroom (not least because realloc / xallocx allow us to grow the\n      // buffer at the tail, but not at the head)  So, if we have more headroom\n      // than we need, we consider that \"wasted\".  We arbitrarily define \"too\n      // much\" headroom to be 25% of the capacity.\n      if (headSlack * 4 <= newCapacity) {\n        size_t allocatedCapacity = capacity() + sizeof(SharedInfo);\n        void* p = buf_;\n        if (allocatedCapacity >= jemallocMinInPlaceExpandable) {\n          if (xallocx(p, newAllocatedCapacity, 0, 0) == newAllocatedCapacity) {\n            if (io_buf_free_cb) {\n              io_buf_free_cb(p, reinterpret_cast<size_t>(info->userData));\n            }\n            newBuffer = static_cast<uint8_t*>(p);\n            newHeadroom = oldHeadroom;\n            // update the userData\n            info->userData = reinterpret_cast<void*>(newAllocatedCapacity);\n            if (io_buf_alloc_cb) {\n              io_buf_alloc_cb(newBuffer, newAllocatedCapacity);\n            }\n          }\n          // if xallocx failed, do nothing, fall back to malloc/memcpy/free\n        }\n      }\n    } else { // Not using jemalloc\n      size_t copySlack = capacity() - length_;\n      if (copySlack * 2 <= length_) {\n        void* p = realloc(buf_, newAllocatedCapacity);\n        if (UNLIKELY(p == nullptr)) {\n          throw_exception<std::bad_alloc>();\n        }\n        newBuffer = static_cast<uint8_t*>(p);\n        newHeadroom = oldHeadroom;\n      }\n    }\n  }\n\n  // None of the previous reallocation strategies worked (or we're using\n  // an internal buffer).  malloc/copy/free.\n  if (newBuffer == nullptr) {\n    newAllocatedCapacity = goodExtBufferSize(newCapacity);\n    newBuffer = static_cast<uint8_t*>(checkedMalloc(newAllocatedCapacity));\n    if (length_ > 0) {\n      assert(data_ != nullptr);\n      memcpy(newBuffer + minHeadroom, data_, length_);\n    }\n    if (sharedInfo()) {\n      freeExtBuffer();\n    }\n    newHeadroom = minHeadroom;\n  }\n\n  std::size_t cap;\n  initExtBuffer(newBuffer, newAllocatedCapacity, &info, &cap);\n\n  if (flags() & kFlagFreeSharedInfo) {\n    delete sharedInfo();\n  } else {\n    if (useHeapFullStorage) {\n      SharedInfo::releaseStorage(sharedInfo());\n    }\n  }\n\n  setFlagsAndSharedInfo(0, info);\n  capacity_ = cap;\n  buf_ = newBuffer;\n  data_ = newBuffer + newHeadroom;\n  // length_ is unchanged\n}\n\n// The user's free function should never throw. Otherwise we might throw from\n// the IOBuf destructor. Other code paths like coalesce() also assume that\n// decrementRefcount() cannot throw.\nvoid IOBuf::freeExtBuffer() noexcept {\n  SharedInfo* info = sharedInfo();\n  DCHECK(info);\n\n  // save the observerListHead\n  // since the SharedInfo can be freed\n  auto observerListHead = info->observerListHead;\n  info->observerListHead = nullptr;\n\n  if (info->freeFn) {\n    info->freeFn(buf_, info->userData);\n  } else {\n    // this will invoke free if info->userData is 0\n    size_t size = reinterpret_cast<size_t>(info->userData);\n    if (size) {\n      if (io_buf_free_cb) {\n        io_buf_free_cb(buf_, size);\n      }\n      folly::sizedFree(buf_, size);\n    } else {\n      free(buf_);\n    }\n  }\n  SharedInfo::invokeAndDeleteEachObserver(\n      observerListHead, [](auto& entry) { entry.afterFreeExtBuffer(); });\n\n  if (kIsMobile) {\n    buf_ = nullptr;\n  }\n}\n\nvoid IOBuf::allocExtBuffer(\n    std::size_t minCapacity,\n    uint8_t** bufReturn,\n    SharedInfo** infoReturn,\n    std::size_t* capacityReturn) {\n  size_t mallocSize = goodExtBufferSize(minCapacity);\n  auto buf = static_cast<uint8_t*>(checkedMalloc(mallocSize));\n  initExtBuffer(buf, mallocSize, infoReturn, capacityReturn);\n\n  // the userData and the freeFn are nullptr here\n  // just store the mallocSize in userData\n  (*infoReturn)->userData = reinterpret_cast<void*>(mallocSize);\n  if (io_buf_alloc_cb) {\n    io_buf_alloc_cb(buf, mallocSize);\n  }\n\n  *bufReturn = buf;\n}\n\nsize_t IOBuf::goodExtBufferSize(std::size_t minCapacity) {\n  // Determine how much space we should allocate.  We'll store the SharedInfo\n  // for the external buffer just after the buffer itself.  (We store it just\n  // after the buffer rather than just before so that the code can still just\n  // use free(buf_) to free the buffer.)\n  size_t minSize = static_cast<size_t>(minCapacity) + sizeof(SharedInfo);\n  // Add room for padding so that the SharedInfo will be aligned on an 8-byte\n  // boundary.\n  minSize = (minSize + 7) & ~7;\n\n  // Use goodMallocSize() to bump up the capacity to a decent size to request\n  // from malloc, so we can use all of the space that malloc will probably give\n  // us anyway.\n  return goodMallocSize(minSize);\n}\n\nvoid IOBuf::initExtBuffer(\n    uint8_t* buf,\n    size_t mallocSize,\n    SharedInfo** infoReturn,\n    std::size_t* capacityReturn) {\n  // Find the SharedInfo storage at the end of the buffer\n  // and construct the SharedInfo.\n  uint8_t* infoStart = (buf + mallocSize) - sizeof(SharedInfo);\n  auto sharedInfo = new (infoStart) SharedInfo;\n\n  *capacityReturn = std::size_t(infoStart - buf);\n  *infoReturn = sharedInfo;\n}\n\nfbstring IOBuf::moveToFbString() {\n  // we need to save useHeapFullStorage and the observerListHead since\n  // sharedInfo() may not be valid after fbstring str\n  bool useHeapFullStorage = false;\n  SharedInfoObserverEntryBase* observerListHead = nullptr;\n  // malloc-allocated buffers are just fine, everything else needs\n  // to be turned into one.\n  if (!sharedInfo() || // user owned, not ours to give up\n      sharedInfo()->freeFn || // not malloc()-ed\n      headroom() != 0 || // malloc()-ed block doesn't start at beginning\n      tailroom() == 0 || // no room for NUL terminator\n      isShared() || // shared\n      isChained()) { // chained\n    // We might as well get rid of all head and tailroom if we're going\n    // to reallocate; we need 1 byte for NUL terminator.\n    coalesceAndReallocate(0, computeChainDataLength(), this, 1);\n  } else {\n    auto info = sharedInfo();\n    if (info) {\n      // if we do not call coalesceAndReallocate\n      // we might need to call SharedInfo::releaseStorage()\n      // and/or SharedInfo::invokeAndDeleteEachObserver()\n      useHeapFullStorage = info->useHeapFullStorage;\n      // save the observerListHead\n      // the coalesceAndReallocate path will call\n      // decrementRefcount and freeExtBuffer if needed\n      // so the observer lis notification is needed here\n      observerListHead = info->observerListHead;\n      info->observerListHead = nullptr;\n    }\n  }\n\n  // Ensure NUL terminated\n  *writableTail() = 0;\n  fbstring str(\n      reinterpret_cast<char*>(writableData()),\n      length(),\n      capacity(),\n      AcquireMallocatedString());\n\n  if (io_buf_free_cb && sharedInfo() && sharedInfo()->userData) {\n    io_buf_free_cb(\n        writableData(), reinterpret_cast<size_t>(sharedInfo()->userData));\n  }\n\n  SharedInfo::invokeAndDeleteEachObserver(\n      observerListHead, [](auto& entry) { entry.afterReleaseExtBuffer(); });\n\n  if (flags() & kFlagFreeSharedInfo) {\n    delete sharedInfo();\n  } else {\n    if (useHeapFullStorage) {\n      SharedInfo::releaseStorage(sharedInfo());\n    }\n  }\n\n  // Reset to a state where we can be deleted cleanly\n  flagsAndSharedInfo_ = 0;\n  buf_ = nullptr;\n  clear();\n  return str;\n}\n\nIOBuf::Iterator IOBuf::cbegin() const {\n  return Iterator(this, this);\n}\n\nIOBuf::Iterator IOBuf::cend() const {\n  return Iterator(nullptr, nullptr);\n}\n\nfolly::fbvector<struct iovec> IOBuf::getIov() const {\n  folly::fbvector<struct iovec> iov;\n  iov.reserve(countChainElements());\n  appendToIov(&iov);\n  return iov;\n}\n\nvoid IOBuf::appendToIov(folly::fbvector<struct iovec>* iov) const {\n  IOBuf const* p = this;\n  do {\n    // some code can get confused by empty iovs, so skip them\n    if (p->length() > 0) {\n      iov->push_back({(void*)p->data(), folly::to<size_t>(p->length())});\n    }\n    p = p->next();\n  } while (p != this);\n}\n\nunique_ptr<IOBuf> IOBuf::wrapIov(const iovec* vec, size_t count) {\n  unique_ptr<IOBuf> result = nullptr;\n  for (size_t i = 0; i < count; ++i) {\n    size_t len = vec[i].iov_len;\n    void* data = vec[i].iov_base;\n    if (len > 0) {\n      auto buf = wrapBuffer(data, len);\n      if (!result) {\n        result = std::move(buf);\n      } else {\n        result->prependChain(std::move(buf));\n      }\n    }\n  }\n  if (UNLIKELY(result == nullptr)) {\n    return create(0);\n  }\n  return result;\n}\n\nstd::unique_ptr<IOBuf> IOBuf::takeOwnershipIov(\n    const iovec* vec,\n    size_t count,\n    FreeFunction freeFn,\n    void* userData,\n    bool freeOnError) {\n  unique_ptr<IOBuf> result = nullptr;\n  for (size_t i = 0; i < count; ++i) {\n    size_t len = vec[i].iov_len;\n    void* data = vec[i].iov_base;\n    if (len > 0) {\n      auto buf = takeOwnership(data, len, freeFn, userData, freeOnError);\n      if (!result) {\n        result = std::move(buf);\n      } else {\n        result->prependChain(std::move(buf));\n      }\n    }\n  }\n  if (UNLIKELY(result == nullptr)) {\n    return create(0);\n  }\n  return result;\n}\n\nIOBuf::FillIovResult IOBuf::fillIov(struct iovec* iov, size_t len) const {\n  IOBuf const* p = this;\n  size_t i = 0;\n  size_t totalBytes = 0;\n  while (i < len) {\n    // some code can get confused by empty iovs, so skip them\n    if (p->length() > 0) {\n      iov[i].iov_base = const_cast<uint8_t*>(p->data());\n      iov[i].iov_len = p->length();\n      totalBytes += p->length();\n      i++;\n    }\n    p = p->next();\n    if (p == this) {\n      return {i, totalBytes};\n    }\n  }\n  return {0, 0};\n}\n\nuint32_t IOBuf::approximateShareCountOne() const {\n  if (UNLIKELY(!sharedInfo())) {\n    return 1U;\n  }\n  return sharedInfo()->refcount.load(std::memory_order_acquire);\n}\n\nsize_t IOBufHash::operator()(const IOBuf& buf) const noexcept {\n  folly::hash::SpookyHashV2 hasher;\n  hasher.Init(0, 0);\n  io::Cursor cursor(&buf);\n  for (;;) {\n    auto b = cursor.peekBytes();\n    if (b.empty()) {\n      break;\n    }\n    hasher.Update(b.data(), b.size());\n    cursor.skip(b.size());\n  }\n  uint64_t h1;\n  uint64_t h2;\n  hasher.Final(&h1, &h2);\n  return static_cast<std::size_t>(h1);\n}\n\nordering IOBufCompare::impl(const IOBuf& a, const IOBuf& b) const noexcept {\n  io::Cursor ca(&a);\n  io::Cursor cb(&b);\n  for (;;) {\n    auto ba = ca.peekBytes();\n    auto bb = cb.peekBytes();\n    if (ba.empty() || bb.empty()) {\n      return to_ordering(int(bb.empty()) - int(ba.empty()));\n    }\n    const size_t n = std::min(ba.size(), bb.size());\n    DCHECK_GT(n, 0u);\n    const ordering r = to_ordering(std::memcmp(ba.data(), bb.data(), n));\n    if (r != ordering::eq) {\n      return r;\n    }\n    // Cursor::skip() may throw if n is too large, but n is not too large here\n    ca.skip(n);\n    cb.skip(n);\n  }\n}\n\n} // namespace folly\n"], "fixing_code": ["/*\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#ifndef __STDC_LIMIT_MACROS\n#define __STDC_LIMIT_MACROS\n#endif\n\n#include <folly/io/IOBuf.h>\n\n#include <cassert>\n#include <cstdint>\n#include <cstdlib>\n#include <limits>\n#include <stdexcept>\n\n#include <folly/Conv.h>\n#include <folly/Likely.h>\n#include <folly/Memory.h>\n#include <folly/ScopeGuard.h>\n#include <folly/hash/SpookyHashV2.h>\n#include <folly/io/Cursor.h>\n#include <folly/lang/Align.h>\n#include <folly/lang/CheckedMath.h>\n#include <folly/lang/Exception.h>\n#include <folly/memory/Malloc.h>\n#include <folly/memory/SanitizeAddress.h>\n\n/*\n * Callbacks that will be invoked when IOBuf allocates or frees memory.\n * Note that io_buf_alloc_cb() will also be invoked when IOBuf takes ownership\n * of a malloc-allocated buffer, even if it was allocated earlier by another\n * part of the code.\n *\n * By default these are unimplemented, but programs can define these functions\n * to perform their own custom logic on memory allocation.  This is intended\n * primarily to help programs track memory usage and possibly take action\n * when thresholds are hit.  Callers should generally avoid performing any\n * expensive work in these callbacks, since they may be called from arbitrary\n * locations in the code that use IOBuf, possibly while holding locks.\n */\n\n#if FOLLY_HAVE_WEAK_SYMBOLS\nFOLLY_ATTR_WEAK void io_buf_alloc_cb(void* /*ptr*/, size_t /*size*/) noexcept;\nFOLLY_ATTR_WEAK void io_buf_free_cb(void* /*ptr*/, size_t /*size*/) noexcept;\n#else\nstatic void (*io_buf_alloc_cb)(void* /*ptr*/, size_t /*size*/) noexcept =\n    nullptr;\nstatic void (*io_buf_free_cb)(void* /*ptr*/, size_t /*size*/) noexcept =\n    nullptr;\n#endif\n\nusing std::unique_ptr;\n\nnamespace {\n\nenum : uint16_t {\n  kHeapMagic = 0xa5a5,\n  // This memory segment contains an IOBuf that is still in use\n  kIOBufInUse = 0x01,\n  // This memory segment contains buffer data that is still in use\n  kDataInUse = 0x02,\n  // This memory segment contains a SharedInfo that is still in use\n  kSharedInfoInUse = 0x04,\n};\n\nenum : std::size_t {\n  // When create() is called for buffers less than kDefaultCombinedBufSize,\n  // we allocate a single combined memory segment for the IOBuf and the data\n  // together.  See the comments for createCombined()/createSeparate() for more\n  // details.\n  //\n  // (The size of 1k is largely just a guess here.  We could could probably do\n  // benchmarks of real applications to see if adjusting this number makes a\n  // difference.  Callers that know their exact use case can also explicitly\n  // call createCombined() or createSeparate().)\n  kDefaultCombinedBufSize = 1024,\n  kMaxIOBufSize = std::numeric_limits<size_t>::max() >> 1,\n};\n\n// Helper function for IOBuf::takeOwnership()\n// The user's free function is not allowed to throw.\n// (We are already in the middle of throwing an exception, so\n// we cannot let this exception go unhandled.)\nvoid takeOwnershipError(\n    bool freeOnError,\n    void* buf,\n    folly::IOBuf::FreeFunction freeFn,\n    void* userData) noexcept {\n  if (!freeOnError) {\n    return;\n  }\n  if (!freeFn) {\n    free(buf);\n    return;\n  }\n  freeFn(buf, userData);\n}\n\n} // namespace\n\nnamespace folly {\n\n// use free for size >= 4GB\n// since we can store only 32 bits in the size var\nstruct IOBuf::HeapPrefix {\n  HeapPrefix(uint16_t flg, size_t sz)\n      : magic(kHeapMagic),\n        flags(flg),\n        size((sz == ((size_t)(uint32_t)sz)) ? static_cast<uint32_t>(sz) : 0) {}\n  ~HeapPrefix() {\n    // Reset magic to 0 on destruction.  This is solely for debugging purposes\n    // to help catch bugs where someone tries to use HeapStorage after it has\n    // been deleted.\n    magic = 0;\n  }\n\n  uint16_t magic;\n  std::atomic<uint16_t> flags;\n  uint32_t size;\n};\n\nstruct IOBuf::HeapStorage {\n  HeapPrefix prefix;\n  // The IOBuf is last in the HeapStorage object.\n  // This way operator new will work even if allocating a subclass of IOBuf\n  // that requires more space.\n  folly::IOBuf buf;\n};\n\nstruct IOBuf::HeapFullStorage {\n  // Make sure jemalloc allocates from the 64-byte class.  Putting this here\n  // because HeapStorage is private so it can't be at namespace level.\n  static_assert(sizeof(HeapStorage) <= 64, \"IOBuf may not grow over 56 bytes!\");\n\n  HeapStorage hs;\n  SharedInfo shared;\n  folly::max_align_t align;\n};\n\nIOBuf::SharedInfo::SharedInfo()\n    : freeFn(nullptr), userData(nullptr), useHeapFullStorage(false) {\n  // Use relaxed memory ordering here.  Since we are creating a new SharedInfo,\n  // no other threads should be referring to it yet.\n  refcount.store(1, std::memory_order_relaxed);\n}\n\nIOBuf::SharedInfo::SharedInfo(FreeFunction fn, void* arg, bool hfs)\n    : freeFn(fn), userData(arg), useHeapFullStorage(hfs) {\n  // Use relaxed memory ordering here.  Since we are creating a new SharedInfo,\n  // no other threads should be referring to it yet.\n  refcount.store(1, std::memory_order_relaxed);\n}\n\nvoid IOBuf::SharedInfo::invokeAndDeleteEachObserver(\n    SharedInfoObserverEntryBase* observerListHead, ObserverCb cb) noexcept {\n  if (observerListHead && cb) {\n    // break the chain\n    observerListHead->prev->next = nullptr;\n    auto entry = observerListHead;\n    while (entry) {\n      auto tmp = entry->next;\n      cb(*entry);\n      delete entry;\n      entry = tmp;\n    }\n  }\n}\n\nvoid IOBuf::SharedInfo::releaseStorage(SharedInfo* info) noexcept {\n  if (info->useHeapFullStorage) {\n    auto storageAddr =\n        reinterpret_cast<uint8_t*>(info) - offsetof(HeapFullStorage, shared);\n    auto storage = reinterpret_cast<HeapFullStorage*>(storageAddr);\n    info->~SharedInfo();\n    IOBuf::releaseStorage(&storage->hs, kSharedInfoInUse);\n  }\n}\n\nvoid* IOBuf::operator new(size_t size) {\n  if (size > kMaxIOBufSize) {\n    throw_exception<std::bad_alloc>();\n  }\n  size_t fullSize = offsetof(HeapStorage, buf) + size;\n  auto storage = static_cast<HeapStorage*>(checkedMalloc(fullSize));\n\n  new (&storage->prefix) HeapPrefix(kIOBufInUse, fullSize);\n\n  if (io_buf_alloc_cb) {\n    io_buf_alloc_cb(storage, fullSize);\n  }\n\n  return &(storage->buf);\n}\n\nvoid* IOBuf::operator new(size_t /* size */, void* ptr) {\n  return ptr;\n}\n\nvoid IOBuf::operator delete(void* ptr) {\n  auto storageAddr = static_cast<uint8_t*>(ptr) - offsetof(HeapStorage, buf);\n  auto storage = reinterpret_cast<HeapStorage*>(storageAddr);\n  releaseStorage(storage, kIOBufInUse);\n}\n\nvoid IOBuf::operator delete(void* /* ptr */, void* /* placement */) {\n  // Provide matching operator for `IOBuf::new` to avoid MSVC compilation\n  // warning (C4291) about memory leak when exception is thrown in the\n  // constructor.\n}\n\nvoid IOBuf::releaseStorage(HeapStorage* storage, uint16_t freeFlags) noexcept {\n  CHECK_EQ(storage->prefix.magic, static_cast<uint16_t>(kHeapMagic));\n\n  // Use relaxed memory order here.  If we are unlucky and happen to get\n  // out-of-date data the compare_exchange_weak() call below will catch\n  // it and load new data with memory_order_acq_rel.\n  auto flags = storage->prefix.flags.load(std::memory_order_acquire);\n  DCHECK_EQ((flags & freeFlags), freeFlags);\n\n  while (true) {\n    auto newFlags = uint16_t(flags & ~freeFlags);\n    if (newFlags == 0) {\n      // save the size\n      size_t size = storage->prefix.size;\n      // The storage space is now unused.  Free it.\n      storage->prefix.HeapPrefix::~HeapPrefix();\n      if (FOLLY_LIKELY(size)) {\n        if (io_buf_free_cb) {\n          io_buf_free_cb(storage, size);\n        }\n        sizedFree(storage, size);\n      } else {\n        free(storage);\n      }\n      return;\n    }\n\n    // This storage segment still contains portions that are in use.\n    // Just clear the flags specified in freeFlags for now.\n    auto ret = storage->prefix.flags.compare_exchange_weak(\n        flags, newFlags, std::memory_order_acq_rel);\n    if (ret) {\n      // We successfully updated the flags.\n      return;\n    }\n\n    // We failed to update the flags.  Some other thread probably updated them\n    // and cleared some of the other bits.  Continue around the loop to see if\n    // we are the last user now, or if we need to try updating the flags again.\n  }\n}\n\nvoid IOBuf::freeInternalBuf(void* /* buf */, void* userData) noexcept {\n  auto storage = static_cast<HeapStorage*>(userData);\n  releaseStorage(storage, kDataInUse);\n}\n\nIOBuf::IOBuf(CreateOp, std::size_t capacity)\n    : next_(this),\n      prev_(this),\n      data_(nullptr),\n      length_(0),\n      flagsAndSharedInfo_(0) {\n  SharedInfo* info;\n  allocExtBuffer(capacity, &buf_, &info, &capacity_);\n  setSharedInfo(info);\n  data_ = buf_;\n}\n\nIOBuf::IOBuf(\n    CopyBufferOp /* op */,\n    const void* buf,\n    std::size_t size,\n    std::size_t headroom,\n    std::size_t minTailroom)\n    : IOBuf(CREATE, headroom + size + minTailroom) {\n  advance(headroom);\n  if (size > 0) {\n    assert(buf != nullptr);\n    memcpy(writableData(), buf, size);\n    append(size);\n  }\n}\n\nIOBuf::IOBuf(\n    CopyBufferOp op,\n    ByteRange br,\n    std::size_t headroom,\n    std::size_t minTailroom)\n    : IOBuf(op, br.data(), br.size(), headroom, minTailroom) {}\n\nunique_ptr<IOBuf> IOBuf::create(std::size_t capacity) {\n  if (capacity > kMaxIOBufSize) {\n    throw_exception<std::bad_alloc>();\n  }\n\n  // For smaller-sized buffers, allocate the IOBuf, SharedInfo, and the buffer\n  // all with a single allocation.\n  //\n  // We don't do this for larger buffers since it can be wasteful if the user\n  // needs to reallocate the buffer but keeps using the same IOBuf object.\n  // In this case we can't free the data space until the IOBuf is also\n  // destroyed.  Callers can explicitly call createCombined() or\n  // createSeparate() if they know their use case better, and know if they are\n  // likely to reallocate the buffer later.\n  if (capacity <= kDefaultCombinedBufSize) {\n    return createCombined(capacity);\n  }\n\n  // if we have nallocx, we want to allocate the capacity and the overhead in\n  // a single allocation only if we do not cross into the next allocation class\n  // for some buffer sizes, this can use about 25% extra memory\n  if (canNallocx()) {\n    auto mallocSize = goodMallocSize(capacity);\n    // round capacity to a multiple of 8\n    size_t minSize = ((capacity + 7) & ~7) + sizeof(SharedInfo);\n    // if we do not have space for the overhead, allocate the mem separateley\n    if (mallocSize < minSize) {\n      auto* buf = checkedMalloc(mallocSize);\n      return takeOwnership(SIZED_FREE, buf, mallocSize, 0, 0);\n    }\n  }\n\n  return createSeparate(capacity);\n}\n\nunique_ptr<IOBuf> IOBuf::createCombined(std::size_t capacity) {\n  if (capacity > kMaxIOBufSize) {\n    throw_exception<std::bad_alloc>();\n  }\n\n  // To save a memory allocation, allocate space for the IOBuf object, the\n  // SharedInfo struct, and the data itself all with a single call to malloc().\n  size_t requiredStorage = offsetof(HeapFullStorage, align) + capacity;\n  size_t mallocSize = goodMallocSize(requiredStorage);\n  auto storage = static_cast<HeapFullStorage*>(checkedMalloc(mallocSize));\n\n  new (&storage->hs.prefix) HeapPrefix(kIOBufInUse | kDataInUse, mallocSize);\n  new (&storage->shared) SharedInfo(freeInternalBuf, storage);\n\n  if (io_buf_alloc_cb) {\n    io_buf_alloc_cb(storage, mallocSize);\n  }\n\n  auto bufAddr = reinterpret_cast<uint8_t*>(&storage->align);\n  uint8_t* storageEnd = reinterpret_cast<uint8_t*>(storage) + mallocSize;\n  auto actualCapacity = size_t(storageEnd - bufAddr);\n  unique_ptr<IOBuf> ret(new (&storage->hs.buf) IOBuf(\n      InternalConstructor(),\n      packFlagsAndSharedInfo(0, &storage->shared),\n      bufAddr,\n      actualCapacity,\n      bufAddr,\n      0));\n  return ret;\n}\n\nunique_ptr<IOBuf> IOBuf::createSeparate(std::size_t capacity) {\n  return std::make_unique<IOBuf>(CREATE, capacity);\n}\n\nunique_ptr<IOBuf> IOBuf::createChain(\n    size_t totalCapacity, std::size_t maxBufCapacity) {\n  unique_ptr<IOBuf> out =\n      create(std::min(totalCapacity, size_t(maxBufCapacity)));\n  size_t allocatedCapacity = out->capacity();\n\n  while (allocatedCapacity < totalCapacity) {\n    unique_ptr<IOBuf> newBuf = create(\n        std::min(totalCapacity - allocatedCapacity, size_t(maxBufCapacity)));\n    allocatedCapacity += newBuf->capacity();\n    out->prependChain(std::move(newBuf));\n  }\n\n  return out;\n}\n\nsize_t IOBuf::goodSize(size_t minCapacity, CombinedOption combined) {\n  if (combined == CombinedOption::DEFAULT) {\n    combined = minCapacity <= kDefaultCombinedBufSize\n        ? CombinedOption::COMBINED\n        : CombinedOption::SEPARATE;\n  }\n  size_t overhead;\n  if (combined == CombinedOption::COMBINED) {\n    overhead = offsetof(HeapFullStorage, align);\n  } else {\n    // Pad minCapacity to a multiple of 8\n    minCapacity = (minCapacity + 7) & ~7;\n    overhead = sizeof(SharedInfo);\n  }\n  size_t goodSize = folly::goodMallocSize(minCapacity + overhead);\n  return goodSize - overhead;\n}\n\nIOBuf::IOBuf(\n    TakeOwnershipOp,\n    void* buf,\n    std::size_t capacity,\n    std::size_t offset,\n    std::size_t length,\n    FreeFunction freeFn,\n    void* userData,\n    bool freeOnError)\n    : next_(this),\n      prev_(this),\n      data_(static_cast<uint8_t*>(buf) + offset),\n      buf_(static_cast<uint8_t*>(buf)),\n      length_(length),\n      capacity_(capacity),\n      flagsAndSharedInfo_(\n          packFlagsAndSharedInfo(kFlagFreeSharedInfo, nullptr)) {\n  // do not allow only user data without a freeFn\n  // since we use that for folly::sizedFree\n  DCHECK(!userData || (userData && freeFn));\n\n  auto rollback = makeGuard([&] { //\n    takeOwnershipError(freeOnError, buf, freeFn, userData);\n  });\n  setSharedInfo(new SharedInfo(freeFn, userData));\n  rollback.dismiss();\n}\n\nIOBuf::IOBuf(\n    TakeOwnershipOp,\n    SizedFree,\n    void* buf,\n    std::size_t capacity,\n    std::size_t offset,\n    std::size_t length,\n    bool freeOnError)\n    : next_(this),\n      prev_(this),\n      data_(static_cast<uint8_t*>(buf) + offset),\n      buf_(static_cast<uint8_t*>(buf)),\n      length_(length),\n      capacity_(capacity),\n      flagsAndSharedInfo_(\n          packFlagsAndSharedInfo(kFlagFreeSharedInfo, nullptr)) {\n  auto rollback = makeGuard([&] { //\n    takeOwnershipError(freeOnError, buf, nullptr, nullptr);\n  });\n  setSharedInfo(new SharedInfo(nullptr, reinterpret_cast<void*>(capacity)));\n  rollback.dismiss();\n\n  if (io_buf_alloc_cb && capacity) {\n    io_buf_alloc_cb(buf, capacity);\n  }\n}\n\nunique_ptr<IOBuf> IOBuf::takeOwnership(\n    void* buf,\n    std::size_t capacity,\n    std::size_t offset,\n    std::size_t length,\n    FreeFunction freeFn,\n    void* userData,\n    bool freeOnError,\n    TakeOwnershipOption option) {\n  if (capacity > kMaxIOBufSize) {\n    throw_exception<std::bad_alloc>();\n  }\n\n  // do not allow only user data without a freeFn\n  // since we use that for folly::sizedFree\n\n  DCHECK(\n      !userData || (userData && freeFn) ||\n      (userData && !freeFn && (option == TakeOwnershipOption::STORE_SIZE)));\n\n  HeapFullStorage* storage = nullptr;\n  auto rollback = makeGuard([&] {\n    if (storage) {\n      free(storage);\n    }\n    takeOwnershipError(freeOnError, buf, freeFn, userData);\n  });\n\n  size_t requiredStorage = sizeof(HeapFullStorage);\n  size_t mallocSize = goodMallocSize(requiredStorage);\n  storage = static_cast<HeapFullStorage*>(checkedMalloc(mallocSize));\n\n  new (&storage->hs.prefix)\n      HeapPrefix(kIOBufInUse | kSharedInfoInUse, mallocSize);\n  new (&storage->shared)\n      SharedInfo(freeFn, userData, true /*useHeapFullStorage*/);\n\n  auto result = unique_ptr<IOBuf>(new (&storage->hs.buf) IOBuf(\n      InternalConstructor(),\n      packFlagsAndSharedInfo(0, &storage->shared),\n      static_cast<uint8_t*>(buf),\n      capacity,\n      static_cast<uint8_t*>(buf) + offset,\n      length));\n\n  rollback.dismiss();\n\n  if (io_buf_alloc_cb) {\n    io_buf_alloc_cb(storage, mallocSize);\n    if (userData && !freeFn && (option == TakeOwnershipOption::STORE_SIZE)) {\n      // Even though we did not allocate the buffer, call io_buf_alloc_cb()\n      // since we will call io_buf_free_cb() on destruction, and we want these\n      // calls to be 1:1.\n      io_buf_alloc_cb(buf, capacity);\n    }\n  }\n\n  return result;\n}\n\nIOBuf::IOBuf(WrapBufferOp, const void* buf, std::size_t capacity) noexcept\n    : IOBuf(\n          InternalConstructor(),\n          0,\n          // We cast away the const-ness of the buffer here.\n          // This is okay since IOBuf users must use unshare() to create a copy\n          // of this buffer before writing to the buffer.\n          static_cast<uint8_t*>(const_cast<void*>(buf)),\n          capacity,\n          static_cast<uint8_t*>(const_cast<void*>(buf)),\n          capacity) {}\n\nIOBuf::IOBuf(WrapBufferOp op, ByteRange br) noexcept\n    : IOBuf(op, br.data(), br.size()) {}\n\nunique_ptr<IOBuf> IOBuf::wrapBuffer(const void* buf, std::size_t capacity) {\n  return std::make_unique<IOBuf>(WRAP_BUFFER, buf, capacity);\n}\n\nIOBuf IOBuf::wrapBufferAsValue(const void* buf, std::size_t capacity) noexcept {\n  return IOBuf(WrapBufferOp::WRAP_BUFFER, buf, capacity);\n}\n\nIOBuf::IOBuf() noexcept = default;\n\nIOBuf::IOBuf(IOBuf&& other) noexcept\n    : data_(other.data_),\n      buf_(other.buf_),\n      length_(other.length_),\n      capacity_(other.capacity_),\n      flagsAndSharedInfo_(other.flagsAndSharedInfo_) {\n  // Reset other so it is a clean state to be destroyed.\n  other.data_ = nullptr;\n  other.buf_ = nullptr;\n  other.length_ = 0;\n  other.capacity_ = 0;\n  other.flagsAndSharedInfo_ = 0;\n\n  // If other was part of the chain, assume ownership of the rest of its chain.\n  // (It's only valid to perform move assignment on the head of a chain.)\n  if (other.next_ != &other) {\n    next_ = other.next_;\n    next_->prev_ = this;\n    other.next_ = &other;\n\n    prev_ = other.prev_;\n    prev_->next_ = this;\n    other.prev_ = &other;\n  }\n\n  // Sanity check to make sure that other is in a valid state to be destroyed.\n  DCHECK_EQ(other.prev_, &other);\n  DCHECK_EQ(other.next_, &other);\n}\n\nIOBuf::IOBuf(const IOBuf& other) {\n  *this = other.cloneAsValue();\n}\n\nIOBuf::IOBuf(\n    InternalConstructor,\n    uintptr_t flagsAndSharedInfo,\n    uint8_t* buf,\n    std::size_t capacity,\n    uint8_t* data,\n    std::size_t length) noexcept\n    : next_(this),\n      prev_(this),\n      data_(data),\n      buf_(buf),\n      length_(length),\n      capacity_(capacity),\n      flagsAndSharedInfo_(flagsAndSharedInfo) {\n  assert(data >= buf);\n  assert(data + length <= buf + capacity);\n\n  CHECK(!folly::asan_region_is_poisoned(buf, capacity));\n}\n\nIOBuf::~IOBuf() {\n  // Destroying an IOBuf destroys the entire chain.\n  // Users of IOBuf should only explicitly delete the head of any chain.\n  // The other elements in the chain will be automatically destroyed.\n  while (next_ != this) {\n    // Since unlink() returns unique_ptr() and we don't store it,\n    // it will automatically delete the unlinked element.\n    (void)next_->unlink();\n  }\n\n  decrementRefcount();\n}\n\nIOBuf& IOBuf::operator=(IOBuf&& other) noexcept {\n  if (this == &other) {\n    return *this;\n  }\n\n  // If we are part of a chain, delete the rest of the chain.\n  while (next_ != this) {\n    // Since unlink() returns unique_ptr() and we don't store it,\n    // it will automatically delete the unlinked element.\n    (void)next_->unlink();\n  }\n\n  // Decrement our refcount on the current buffer\n  decrementRefcount();\n\n  // Take ownership of the other buffer's data\n  data_ = other.data_;\n  buf_ = other.buf_;\n  length_ = other.length_;\n  capacity_ = other.capacity_;\n  flagsAndSharedInfo_ = other.flagsAndSharedInfo_;\n  // Reset other so it is a clean state to be destroyed.\n  other.data_ = nullptr;\n  other.buf_ = nullptr;\n  other.length_ = 0;\n  other.capacity_ = 0;\n  other.flagsAndSharedInfo_ = 0;\n\n  // If other was part of the chain, assume ownership of the rest of its chain.\n  // (It's only valid to perform move assignment on the head of a chain.)\n  if (other.next_ != &other) {\n    next_ = other.next_;\n    next_->prev_ = this;\n    other.next_ = &other;\n\n    prev_ = other.prev_;\n    prev_->next_ = this;\n    other.prev_ = &other;\n  }\n\n  // Sanity check to make sure that other is in a valid state to be destroyed.\n  DCHECK_EQ(other.prev_, &other);\n  DCHECK_EQ(other.next_, &other);\n\n  return *this;\n}\n\nIOBuf& IOBuf::operator=(const IOBuf& other) {\n  if (this != &other) {\n    *this = IOBuf(other);\n  }\n  return *this;\n}\n\nbool IOBuf::empty() const {\n  const IOBuf* current = this;\n  do {\n    if (current->length() != 0) {\n      return false;\n    }\n    current = current->next_;\n  } while (current != this);\n  return true;\n}\n\nsize_t IOBuf::countChainElements() const {\n  size_t numElements = 1;\n  for (IOBuf* current = next_; current != this; current = current->next_) {\n    ++numElements;\n  }\n  return numElements;\n}\n\nstd::size_t IOBuf::computeChainDataLength() const {\n  std::size_t fullLength = length_;\n  for (IOBuf* current = next_; current != this; current = current->next_) {\n    fullLength += current->length_;\n  }\n  return fullLength;\n}\n\nstd::size_t IOBuf::computeChainCapacity() const {\n  std::size_t fullCapacity = capacity_;\n  for (IOBuf* current = next_; current != this; current = current->next_) {\n    fullCapacity += current->capacity_;\n  }\n  return fullCapacity;\n}\n\nvoid IOBuf::prependChain(unique_ptr<IOBuf>&& iobuf) {\n  // Take ownership of the specified IOBuf\n  IOBuf* other = iobuf.release();\n\n  // Remember the pointer to the tail of the other chain\n  IOBuf* otherTail = other->prev_;\n\n  // Hook up prev_->next_ to point at the start of the other chain,\n  // and other->prev_ to point at prev_\n  prev_->next_ = other;\n  other->prev_ = prev_;\n\n  // Hook up otherTail->next_ to point at us,\n  // and prev_ to point back at otherTail,\n  otherTail->next_ = this;\n  prev_ = otherTail;\n}\n\nunique_ptr<IOBuf> IOBuf::clone() const {\n  auto tmp = cloneOne();\n\n  for (IOBuf* current = next_; current != this; current = current->next_) {\n    tmp->prependChain(current->cloneOne());\n  }\n\n  return tmp;\n}\n\nunique_ptr<IOBuf> IOBuf::cloneOne() const {\n  if (SharedInfo* info = sharedInfo()) {\n    info->refcount.fetch_add(1, std::memory_order_acq_rel);\n  }\n  return std::unique_ptr<IOBuf>(new IOBuf(\n      InternalConstructor(),\n      flagsAndSharedInfo_,\n      buf_,\n      capacity_,\n      data_,\n      length_));\n}\n\nunique_ptr<IOBuf> IOBuf::cloneCoalesced() const {\n  return std::make_unique<IOBuf>(cloneCoalescedAsValue());\n}\n\nunique_ptr<IOBuf> IOBuf::cloneCoalescedWithHeadroomTailroom(\n    std::size_t newHeadroom, std::size_t newTailroom) const {\n  return std::make_unique<IOBuf>(\n      cloneCoalescedAsValueWithHeadroomTailroom(newHeadroom, newTailroom));\n}\n\nIOBuf IOBuf::cloneAsValue() const {\n  auto tmp = cloneOneAsValue();\n\n  for (IOBuf* current = next_; current != this; current = current->next_) {\n    tmp.prependChain(current->cloneOne());\n  }\n\n  return tmp;\n}\n\nIOBuf IOBuf::cloneOneAsValue() const {\n  if (SharedInfo* info = sharedInfo()) {\n    info->refcount.fetch_add(1, std::memory_order_acq_rel);\n  }\n  return IOBuf(\n      InternalConstructor(),\n      flagsAndSharedInfo_,\n      buf_,\n      capacity_,\n      data_,\n      length_);\n}\n\nIOBuf IOBuf::cloneCoalescedAsValue() const {\n  const std::size_t newHeadroom = headroom();\n  const std::size_t newTailroom = prev()->tailroom();\n  return cloneCoalescedAsValueWithHeadroomTailroom(newHeadroom, newTailroom);\n}\n\nIOBuf IOBuf::cloneCoalescedAsValueWithHeadroomTailroom(\n    std::size_t newHeadroom, std::size_t newTailroom) const {\n  if (!isChained() && newHeadroom <= headroom() && newTailroom <= tailroom()) {\n    return cloneOneAsValue();\n  }\n  // Coalesce into newBuf\n  const std::size_t newLength = computeChainDataLength();\n  const std::size_t newCapacity = newLength + newHeadroom + newTailroom;\n  IOBuf newBuf{CREATE, newCapacity};\n  newBuf.advance(newHeadroom);\n\n  auto current = this;\n  do {\n    if (current->length() > 0) {\n      DCHECK_NOTNULL(current->data());\n      DCHECK_LE(current->length(), newBuf.tailroom());\n      memcpy(newBuf.writableTail(), current->data(), current->length());\n      newBuf.append(current->length());\n    }\n    current = current->next();\n  } while (current != this);\n\n  DCHECK_EQ(newLength, newBuf.length());\n  DCHECK_EQ(newHeadroom, newBuf.headroom());\n  DCHECK_LE(newTailroom, newBuf.tailroom());\n\n  return newBuf;\n}\n\nvoid IOBuf::unshareOneSlow() {\n  // Allocate a new buffer for the data\n  uint8_t* buf;\n  SharedInfo* sharedInfo;\n  std::size_t actualCapacity;\n  allocExtBuffer(capacity_, &buf, &sharedInfo, &actualCapacity);\n\n  // Copy the data\n  // Maintain the same amount of headroom.  Since we maintained the same\n  // minimum capacity we also maintain at least the same amount of tailroom.\n  std::size_t headlen = headroom();\n  if (length_ > 0) {\n    assert(data_ != nullptr);\n    memcpy(buf + headlen, data_, length_);\n  }\n\n  // Release our reference on the old buffer\n  decrementRefcount();\n  // Make sure flags are all cleared.\n  setFlagsAndSharedInfo(0, sharedInfo);\n\n  // Update the buffer pointers to point to the new buffer\n  data_ = buf + headlen;\n  buf_ = buf;\n}\n\nvoid IOBuf::unshareChained() {\n  // unshareChained() should only be called if we are part of a chain of\n  // multiple IOBufs.  The caller should have already verified this.\n  assert(isChained());\n\n  IOBuf* current = this;\n  while (true) {\n    if (current->isSharedOne()) {\n      // we have to unshare\n      break;\n    }\n\n    current = current->next_;\n    if (current == this) {\n      // None of the IOBufs in the chain are shared,\n      // so return without doing anything\n      return;\n    }\n  }\n\n  // We have to unshare.  Let coalesceSlow() do the work.\n  coalesceSlow();\n}\n\nvoid IOBuf::markExternallyShared() {\n  IOBuf* current = this;\n  do {\n    current->markExternallySharedOne();\n    current = current->next_;\n  } while (current != this);\n}\n\nvoid IOBuf::makeManagedChained() {\n  assert(isChained());\n\n  IOBuf* current = this;\n  while (true) {\n    current->makeManagedOne();\n    current = current->next_;\n    if (current == this) {\n      break;\n    }\n  }\n}\n\nvoid IOBuf::coalesceSlow() {\n  // coalesceSlow() should only be called if we are part of a chain of multiple\n  // IOBufs.  The caller should have already verified this.\n  DCHECK(isChained());\n\n  // Compute the length of the entire chain\n  std::size_t newLength = 0;\n  IOBuf* end = this;\n  do {\n    newLength += end->length_;\n    end = end->next_;\n  } while (end != this);\n\n  coalesceAndReallocate(newLength, end);\n  // We should be only element left in the chain now\n  DCHECK(!isChained());\n}\n\nvoid IOBuf::coalesceSlow(size_t maxLength) {\n  // coalesceSlow() should only be called if we are part of a chain of multiple\n  // IOBufs.  The caller should have already verified this.\n  DCHECK(isChained());\n  DCHECK_LT(length_, maxLength);\n\n  // Compute the length of the entire chain\n  std::size_t newLength = 0;\n  IOBuf* end = this;\n  while (true) {\n    newLength += end->length_;\n    end = end->next_;\n    if (newLength >= maxLength) {\n      break;\n    }\n    if (end == this) {\n      throw_exception<std::overflow_error>(\n          \"attempted to coalesce more data than \"\n          \"available\");\n    }\n  }\n\n  coalesceAndReallocate(newLength, end);\n  // We should have the requested length now\n  DCHECK_GE(length_, maxLength);\n}\n\nvoid IOBuf::coalesceAndReallocate(\n    size_t newHeadroom, size_t newLength, IOBuf* end, size_t newTailroom) {\n  std::size_t newCapacity = newLength + newHeadroom + newTailroom;\n\n  // Allocate space for the coalesced buffer.\n  // We always convert to an external buffer, even if we happened to be an\n  // internal buffer before.\n  uint8_t* newBuf;\n  SharedInfo* newInfo;\n  std::size_t actualCapacity;\n  allocExtBuffer(newCapacity, &newBuf, &newInfo, &actualCapacity);\n\n  // Copy the data into the new buffer\n  uint8_t* newData = newBuf + newHeadroom;\n  uint8_t* p = newData;\n  IOBuf* current = this;\n  size_t remaining = newLength;\n  do {\n    if (current->length_ > 0) {\n      assert(current->length_ <= remaining);\n      assert(current->data_ != nullptr);\n      remaining -= current->length_;\n      memcpy(p, current->data_, current->length_);\n      p += current->length_;\n    }\n    current = current->next_;\n  } while (current != end);\n  assert(remaining == 0);\n\n  // Point at the new buffer\n  decrementRefcount();\n\n  // Make sure flags are all cleared.\n  setFlagsAndSharedInfo(0, newInfo);\n\n  capacity_ = actualCapacity;\n  buf_ = newBuf;\n  data_ = newData;\n  length_ = newLength;\n\n  // Separate from the rest of our chain.\n  // Since we don't store the unique_ptr returned by separateChain(),\n  // this will immediately delete the returned subchain.\n  if (isChained()) {\n    (void)separateChain(next_, current->prev_);\n  }\n}\n\nvoid IOBuf::decrementRefcount() noexcept {\n  // Externally owned buffers don't have a SharedInfo object and aren't managed\n  // by the reference count\n  SharedInfo* info = sharedInfo();\n  if (!info) {\n    return;\n  }\n\n  // Avoid doing atomic decrement if the refcount is 1.\n  // This is safe, because it means that we're the last reference and destroying\n  // the object. Anything trying to copy it is already undefined behavior.\n  if (info->refcount.load(std::memory_order_acquire) > 1) {\n    // Decrement the refcount\n    uint32_t newcnt = info->refcount.fetch_sub(1, std::memory_order_acq_rel);\n    // Note that fetch_sub() returns the value before we decremented.\n    // If it is 1, we were the only remaining user; if it is greater there are\n    // still other users.\n    if (newcnt > 1) {\n      return;\n    }\n  }\n\n  // save the useHeapFullStorage flag here since\n  // freeExtBuffer can delete the sharedInfo()\n  bool useHeapFullStorage = info->useHeapFullStorage;\n\n  // We were the last user.  Free the buffer\n  freeExtBuffer();\n\n  // Free the SharedInfo if it was allocated separately.\n  //\n  // This is only used by takeOwnership().\n  //\n  // To avoid this special case handling in decrementRefcount(), we could have\n  // takeOwnership() set a custom freeFn() that calls the user's free function\n  // then frees the SharedInfo object.  (This would require that\n  // takeOwnership() store the user's free function with its allocated\n  // SharedInfo object.)  However, handling this specially with a flag seems\n  // like it shouldn't be problematic.\n  if (flags() & kFlagFreeSharedInfo) {\n    delete info;\n  } else {\n    if (useHeapFullStorage) {\n      SharedInfo::releaseStorage(info);\n    }\n  }\n}\n\nvoid IOBuf::reserveSlow(std::size_t minHeadroom, std::size_t minTailroom) {\n  size_t newCapacity = length_;\n  if (!checked_add(&newCapacity, newCapacity, minHeadroom) ||\n      !checked_add(&newCapacity, newCapacity, minTailroom) ||\n      newCapacity > kMaxIOBufSize) {\n    // overflow\n    throw_exception<std::bad_alloc>();\n  }\n\n  // reserveSlow() is dangerous if anyone else is sharing the buffer, as we may\n  // reallocate and free the original buffer.  It should only ever be called if\n  // we are the only user of the buffer.\n  DCHECK(!isSharedOne());\n\n  // We'll need to reallocate the buffer.\n  // There are a few options.\n  // - If we have enough total room, move the data around in the buffer\n  //   and adjust the data_ pointer.\n  // - If we're using an internal buffer, we'll switch to an external\n  //   buffer with enough headroom and tailroom.\n  // - If we have enough headroom (headroom() >= minHeadroom) but not too much\n  //   (so we don't waste memory), we can try one of two things, depending on\n  //   whether we use jemalloc or not:\n  //   - If using jemalloc, we can try to expand in place, avoiding a memcpy()\n  //   - If not using jemalloc and we don't have too much to copy,\n  //     we'll use realloc() (note that realloc might have to copy\n  //     headroom + data + tailroom, see smartRealloc in folly/memory/Malloc.h)\n  // - Otherwise, bite the bullet and reallocate.\n  if (headroom() + tailroom() >= minHeadroom + minTailroom) {\n    uint8_t* newData = writableBuffer() + minHeadroom;\n    memmove(newData, data_, length_);\n    data_ = newData;\n    return;\n  }\n\n  size_t newAllocatedCapacity = 0;\n  uint8_t* newBuffer = nullptr;\n  std::size_t newHeadroom = 0;\n  std::size_t oldHeadroom = headroom();\n\n  // If we have a buffer allocated with malloc and we just need more tailroom,\n  // try to use realloc()/xallocx() to grow the buffer in place.\n  SharedInfo* info = sharedInfo();\n  bool useHeapFullStorage = info && info->useHeapFullStorage;\n  if (info && (info->freeFn == nullptr) && length_ != 0 &&\n      oldHeadroom >= minHeadroom) {\n    size_t headSlack = oldHeadroom - minHeadroom;\n    newAllocatedCapacity = goodExtBufferSize(newCapacity + headSlack);\n    if (usingJEMalloc()) {\n      // We assume that tailroom is more useful and more important than\n      // headroom (not least because realloc / xallocx allow us to grow the\n      // buffer at the tail, but not at the head)  So, if we have more headroom\n      // than we need, we consider that \"wasted\".  We arbitrarily define \"too\n      // much\" headroom to be 25% of the capacity.\n      if (headSlack * 4 <= newCapacity) {\n        size_t allocatedCapacity = capacity() + sizeof(SharedInfo);\n        void* p = buf_;\n        if (allocatedCapacity >= jemallocMinInPlaceExpandable) {\n          if (xallocx(p, newAllocatedCapacity, 0, 0) == newAllocatedCapacity) {\n            if (io_buf_free_cb) {\n              io_buf_free_cb(p, reinterpret_cast<size_t>(info->userData));\n            }\n            newBuffer = static_cast<uint8_t*>(p);\n            newHeadroom = oldHeadroom;\n            // update the userData\n            info->userData = reinterpret_cast<void*>(newAllocatedCapacity);\n            if (io_buf_alloc_cb) {\n              io_buf_alloc_cb(newBuffer, newAllocatedCapacity);\n            }\n          }\n          // if xallocx failed, do nothing, fall back to malloc/memcpy/free\n        }\n      }\n    } else { // Not using jemalloc\n      size_t copySlack = capacity() - length_;\n      if (copySlack * 2 <= length_) {\n        void* p = realloc(buf_, newAllocatedCapacity);\n        if (UNLIKELY(p == nullptr)) {\n          throw_exception<std::bad_alloc>();\n        }\n        newBuffer = static_cast<uint8_t*>(p);\n        newHeadroom = oldHeadroom;\n      }\n    }\n  }\n\n  // None of the previous reallocation strategies worked (or we're using\n  // an internal buffer).  malloc/copy/free.\n  if (newBuffer == nullptr) {\n    newAllocatedCapacity = goodExtBufferSize(newCapacity);\n    newBuffer = static_cast<uint8_t*>(checkedMalloc(newAllocatedCapacity));\n    if (length_ > 0) {\n      assert(data_ != nullptr);\n      memcpy(newBuffer + minHeadroom, data_, length_);\n    }\n    if (sharedInfo()) {\n      freeExtBuffer();\n    }\n    newHeadroom = minHeadroom;\n  }\n\n  std::size_t cap;\n  initExtBuffer(newBuffer, newAllocatedCapacity, &info, &cap);\n\n  if (flags() & kFlagFreeSharedInfo) {\n    delete sharedInfo();\n  } else {\n    if (useHeapFullStorage) {\n      SharedInfo::releaseStorage(sharedInfo());\n    }\n  }\n\n  setFlagsAndSharedInfo(0, info);\n  capacity_ = cap;\n  buf_ = newBuffer;\n  data_ = newBuffer + newHeadroom;\n  // length_ is unchanged\n}\n\n// The user's free function should never throw. Otherwise we might throw from\n// the IOBuf destructor. Other code paths like coalesce() also assume that\n// decrementRefcount() cannot throw.\nvoid IOBuf::freeExtBuffer() noexcept {\n  SharedInfo* info = sharedInfo();\n  DCHECK(info);\n\n  // save the observerListHead\n  // since the SharedInfo can be freed\n  auto observerListHead = info->observerListHead;\n  info->observerListHead = nullptr;\n\n  if (info->freeFn) {\n    info->freeFn(buf_, info->userData);\n  } else {\n    // this will invoke free if info->userData is 0\n    size_t size = reinterpret_cast<size_t>(info->userData);\n    if (size) {\n      if (io_buf_free_cb) {\n        io_buf_free_cb(buf_, size);\n      }\n      folly::sizedFree(buf_, size);\n    } else {\n      free(buf_);\n    }\n  }\n  SharedInfo::invokeAndDeleteEachObserver(\n      observerListHead, [](auto& entry) { entry.afterFreeExtBuffer(); });\n\n  if (kIsMobile) {\n    buf_ = nullptr;\n  }\n}\n\nvoid IOBuf::allocExtBuffer(\n    std::size_t minCapacity,\n    uint8_t** bufReturn,\n    SharedInfo** infoReturn,\n    std::size_t* capacityReturn) {\n  if (minCapacity > kMaxIOBufSize) {\n    throw_exception<std::bad_alloc>();\n  }\n\n  size_t mallocSize = goodExtBufferSize(minCapacity);\n  auto buf = static_cast<uint8_t*>(checkedMalloc(mallocSize));\n  initExtBuffer(buf, mallocSize, infoReturn, capacityReturn);\n\n  // the userData and the freeFn are nullptr here\n  // just store the mallocSize in userData\n  (*infoReturn)->userData = reinterpret_cast<void*>(mallocSize);\n  if (io_buf_alloc_cb) {\n    io_buf_alloc_cb(buf, mallocSize);\n  }\n\n  *bufReturn = buf;\n}\n\nsize_t IOBuf::goodExtBufferSize(std::size_t minCapacity) {\n  if (minCapacity > kMaxIOBufSize) {\n    throw_exception<std::bad_alloc>();\n  }\n\n  // Determine how much space we should allocate.  We'll store the SharedInfo\n  // for the external buffer just after the buffer itself.  (We store it just\n  // after the buffer rather than just before so that the code can still just\n  // use free(buf_) to free the buffer.)\n  size_t minSize = static_cast<size_t>(minCapacity) + sizeof(SharedInfo);\n  // Add room for padding so that the SharedInfo will be aligned on an 8-byte\n  // boundary.\n  minSize = (minSize + 7) & ~7;\n\n  // Use goodMallocSize() to bump up the capacity to a decent size to request\n  // from malloc, so we can use all of the space that malloc will probably give\n  // us anyway.\n  return goodMallocSize(minSize);\n}\n\nvoid IOBuf::initExtBuffer(\n    uint8_t* buf,\n    size_t mallocSize,\n    SharedInfo** infoReturn,\n    std::size_t* capacityReturn) {\n  // Find the SharedInfo storage at the end of the buffer\n  // and construct the SharedInfo.\n  uint8_t* infoStart = (buf + mallocSize) - sizeof(SharedInfo);\n  auto sharedInfo = new (infoStart) SharedInfo;\n\n  *capacityReturn = std::size_t(infoStart - buf);\n  *infoReturn = sharedInfo;\n}\n\nfbstring IOBuf::moveToFbString() {\n  // we need to save useHeapFullStorage and the observerListHead since\n  // sharedInfo() may not be valid after fbstring str\n  bool useHeapFullStorage = false;\n  SharedInfoObserverEntryBase* observerListHead = nullptr;\n  // malloc-allocated buffers are just fine, everything else needs\n  // to be turned into one.\n  if (!sharedInfo() || // user owned, not ours to give up\n      sharedInfo()->freeFn || // not malloc()-ed\n      headroom() != 0 || // malloc()-ed block doesn't start at beginning\n      tailroom() == 0 || // no room for NUL terminator\n      isShared() || // shared\n      isChained()) { // chained\n    // We might as well get rid of all head and tailroom if we're going\n    // to reallocate; we need 1 byte for NUL terminator.\n    coalesceAndReallocate(0, computeChainDataLength(), this, 1);\n  } else {\n    auto info = sharedInfo();\n    if (info) {\n      // if we do not call coalesceAndReallocate\n      // we might need to call SharedInfo::releaseStorage()\n      // and/or SharedInfo::invokeAndDeleteEachObserver()\n      useHeapFullStorage = info->useHeapFullStorage;\n      // save the observerListHead\n      // the coalesceAndReallocate path will call\n      // decrementRefcount and freeExtBuffer if needed\n      // so the observer lis notification is needed here\n      observerListHead = info->observerListHead;\n      info->observerListHead = nullptr;\n    }\n  }\n\n  // Ensure NUL terminated\n  *writableTail() = 0;\n  fbstring str(\n      reinterpret_cast<char*>(writableData()),\n      length(),\n      capacity(),\n      AcquireMallocatedString());\n\n  if (io_buf_free_cb && sharedInfo() && sharedInfo()->userData) {\n    io_buf_free_cb(\n        writableData(), reinterpret_cast<size_t>(sharedInfo()->userData));\n  }\n\n  SharedInfo::invokeAndDeleteEachObserver(\n      observerListHead, [](auto& entry) { entry.afterReleaseExtBuffer(); });\n\n  if (flags() & kFlagFreeSharedInfo) {\n    delete sharedInfo();\n  } else {\n    if (useHeapFullStorage) {\n      SharedInfo::releaseStorage(sharedInfo());\n    }\n  }\n\n  // Reset to a state where we can be deleted cleanly\n  flagsAndSharedInfo_ = 0;\n  buf_ = nullptr;\n  clear();\n  return str;\n}\n\nIOBuf::Iterator IOBuf::cbegin() const {\n  return Iterator(this, this);\n}\n\nIOBuf::Iterator IOBuf::cend() const {\n  return Iterator(nullptr, nullptr);\n}\n\nfolly::fbvector<struct iovec> IOBuf::getIov() const {\n  folly::fbvector<struct iovec> iov;\n  iov.reserve(countChainElements());\n  appendToIov(&iov);\n  return iov;\n}\n\nvoid IOBuf::appendToIov(folly::fbvector<struct iovec>* iov) const {\n  IOBuf const* p = this;\n  do {\n    // some code can get confused by empty iovs, so skip them\n    if (p->length() > 0) {\n      iov->push_back({(void*)p->data(), folly::to<size_t>(p->length())});\n    }\n    p = p->next();\n  } while (p != this);\n}\n\nunique_ptr<IOBuf> IOBuf::wrapIov(const iovec* vec, size_t count) {\n  unique_ptr<IOBuf> result = nullptr;\n  for (size_t i = 0; i < count; ++i) {\n    size_t len = vec[i].iov_len;\n    void* data = vec[i].iov_base;\n    if (len > 0) {\n      auto buf = wrapBuffer(data, len);\n      if (!result) {\n        result = std::move(buf);\n      } else {\n        result->prependChain(std::move(buf));\n      }\n    }\n  }\n  if (UNLIKELY(result == nullptr)) {\n    return create(0);\n  }\n  return result;\n}\n\nstd::unique_ptr<IOBuf> IOBuf::takeOwnershipIov(\n    const iovec* vec,\n    size_t count,\n    FreeFunction freeFn,\n    void* userData,\n    bool freeOnError) {\n  unique_ptr<IOBuf> result = nullptr;\n  for (size_t i = 0; i < count; ++i) {\n    size_t len = vec[i].iov_len;\n    void* data = vec[i].iov_base;\n    if (len > 0) {\n      auto buf = takeOwnership(data, len, freeFn, userData, freeOnError);\n      if (!result) {\n        result = std::move(buf);\n      } else {\n        result->prependChain(std::move(buf));\n      }\n    }\n  }\n  if (UNLIKELY(result == nullptr)) {\n    return create(0);\n  }\n  return result;\n}\n\nIOBuf::FillIovResult IOBuf::fillIov(struct iovec* iov, size_t len) const {\n  IOBuf const* p = this;\n  size_t i = 0;\n  size_t totalBytes = 0;\n  while (i < len) {\n    // some code can get confused by empty iovs, so skip them\n    if (p->length() > 0) {\n      iov[i].iov_base = const_cast<uint8_t*>(p->data());\n      iov[i].iov_len = p->length();\n      totalBytes += p->length();\n      i++;\n    }\n    p = p->next();\n    if (p == this) {\n      return {i, totalBytes};\n    }\n  }\n  return {0, 0};\n}\n\nuint32_t IOBuf::approximateShareCountOne() const {\n  if (UNLIKELY(!sharedInfo())) {\n    return 1U;\n  }\n  return sharedInfo()->refcount.load(std::memory_order_acquire);\n}\n\nsize_t IOBufHash::operator()(const IOBuf& buf) const noexcept {\n  folly::hash::SpookyHashV2 hasher;\n  hasher.Init(0, 0);\n  io::Cursor cursor(&buf);\n  for (;;) {\n    auto b = cursor.peekBytes();\n    if (b.empty()) {\n      break;\n    }\n    hasher.Update(b.data(), b.size());\n    cursor.skip(b.size());\n  }\n  uint64_t h1;\n  uint64_t h2;\n  hasher.Final(&h1, &h2);\n  return static_cast<std::size_t>(h1);\n}\n\nordering IOBufCompare::impl(const IOBuf& a, const IOBuf& b) const noexcept {\n  io::Cursor ca(&a);\n  io::Cursor cb(&b);\n  for (;;) {\n    auto ba = ca.peekBytes();\n    auto bb = cb.peekBytes();\n    if (ba.empty() || bb.empty()) {\n      return to_ordering(int(bb.empty()) - int(ba.empty()));\n    }\n    const size_t n = std::min(ba.size(), bb.size());\n    DCHECK_GT(n, 0u);\n    const ordering r = to_ordering(std::memcmp(ba.data(), bb.data(), n));\n    if (r != ordering::eq) {\n      return r;\n    }\n    // Cursor::skip() may throw if n is too large, but n is not too large here\n    ca.skip(n);\n    cb.skip(n);\n  }\n}\n\n} // namespace folly\n"], "filenames": ["folly/io/IOBuf.cpp"], "buggy_code_start_loc": [25], "buggy_code_end_loc": [1175], "fixing_code_start_loc": [26], "fixing_code_end_loc": [1207], "type": "CWE-190", "message": "Passing an attacker controlled size when creating an IOBuf could cause integer overflow, leading to an out of bounds write on the heap with the possibility of remote code execution. This issue affects versions of folly prior to v2021.07.22.00. This issue affects HHVM versions prior to 4.80.5, all versions between 4.81.0 and 4.102.1, all versions between 4.103.0 and 4.113.0, and versions 4.114.0, 4.115.0, 4.116.0, 4.117.0, 4.118.0 and 4.118.1.", "other": {"cve": {"id": "CVE-2021-24036", "sourceIdentifier": "cve-assign@fb.com", "published": "2021-07-23T01:15:07.073", "lastModified": "2022-10-26T00:30:43.453", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Passing an attacker controlled size when creating an IOBuf could cause integer overflow, leading to an out of bounds write on the heap with the possibility of remote code execution. This issue affects versions of folly prior to v2021.07.22.00. This issue affects HHVM versions prior to 4.80.5, all versions between 4.81.0 and 4.102.1, all versions between 4.103.0 and 4.113.0, and versions 4.114.0, 4.115.0, 4.116.0, 4.117.0, 4.118.0 and 4.118.1."}, {"lang": "es", "value": "Pasar un tama\u00f1o controlado por un atacante al crear un IOBuf podr\u00eda causar un desbordamiento de enteros, lo que llevar\u00eda a una escritura fuera de l\u00edmites en la pila con la posibilidad de ejecuci\u00f3n de c\u00f3digo remoto. Este problema afecta a las versiones de folly anteriores a la v2021.07.22.00. Este problema afecta a las versiones de HHVM anteriores a la 4.80.5, a todas las versiones entre la 4.81.0 y la 4.102.1, a todas las versiones entre la 4.103.0 y la 4.113.0, y a las versiones 4.114.0, 4.115.0, 4.116.0, 4.117.0, 4.118.0 y 4.118.1"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 7.5}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}, {"source": "cve-assign@fb.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-122"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:facebook:folly:*:*:*:*:*:*:*:*", "versionEndExcluding": "2021.07.22.00", "matchCriteriaId": "ED848EC8-F695-4D35-B067-D9E726DB8279"}, {"vulnerable": true, "criteria": "cpe:2.3:a:facebook:hhvm:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.80.5", "matchCriteriaId": "DE717551-482F-4C7B-BE36-294F96327735"}, {"vulnerable": true, "criteria": "cpe:2.3:a:facebook:hhvm:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.81.0", "versionEndIncluding": "4.102.1", "matchCriteriaId": "A47916E5-F1B3-4C34-8E91-D205F4FAEF69"}, {"vulnerable": true, "criteria": "cpe:2.3:a:facebook:hhvm:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.103.0", "versionEndIncluding": "4.113.0", "matchCriteriaId": "DCB9EA9C-DD7A-4CA3-B957-710AA082A3B9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:facebook:hhvm:4.114.0:*:*:*:*:*:*:*", "matchCriteriaId": "90F2D55E-3F60-45D8-98E4-3E61E9E5AD17"}, {"vulnerable": true, "criteria": "cpe:2.3:a:facebook:hhvm:4.115.0:*:*:*:*:*:*:*", "matchCriteriaId": "9BDA3CE0-633D-43A7-8B88-E0A1F046BC47"}, {"vulnerable": true, "criteria": "cpe:2.3:a:facebook:hhvm:4.116.0:*:*:*:*:*:*:*", "matchCriteriaId": "B613CAC3-7B20-4315-978D-D7F5FC92F873"}, {"vulnerable": true, "criteria": "cpe:2.3:a:facebook:hhvm:4.117.0:*:*:*:*:*:*:*", "matchCriteriaId": "A93EB623-9A2B-4554-AC52-BB23E85A0CB2"}, {"vulnerable": true, "criteria": "cpe:2.3:a:facebook:hhvm:4.118.0:*:*:*:*:*:*:*", "matchCriteriaId": "26CBE42E-25C6-4150-9C0B-1B7CE5BEF03A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:facebook:hhvm:4.118.1:*:*:*:*:*:*:*", "matchCriteriaId": "73A1E792-0EB5-40F8-A4D6-E6ECAABC8AE5"}]}]}], "references": [{"url": "https://github.com/facebook/folly/commit/4f304af1411e68851bdd00ef6140e9de4616f7d3", "source": "cve-assign@fb.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://hhvm.com/blog/2021/07/20/security-update.html", "source": "cve-assign@fb.com", "tags": ["Product", "Vendor Advisory"]}, {"url": "https://www.facebook.com/security/advisories/cve-2021-24036", "source": "cve-assign@fb.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/facebook/folly/commit/4f304af1411e68851bdd00ef6140e9de4616f7d3"}}