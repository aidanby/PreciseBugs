{"buggy_code": ["/*\n * Copyright (C) 2012 - Virtual Open Systems and Columbia University\n * Author: Christoffer Dall <c.dall@virtualopensystems.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License, version 2, as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n */\n\n#include <linux/cpu.h>\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/kvm_host.h>\n#include <linux/module.h>\n#include <linux/vmalloc.h>\n#include <linux/fs.h>\n#include <linux/mman.h>\n#include <linux/sched.h>\n#include <linux/kvm.h>\n#include <trace/events/kvm.h>\n\n#define CREATE_TRACE_POINTS\n#include \"trace.h\"\n\n#include <asm/uaccess.h>\n#include <asm/ptrace.h>\n#include <asm/mman.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n#include <asm/virt.h>\n#include <asm/kvm_arm.h>\n#include <asm/kvm_asm.h>\n#include <asm/kvm_mmu.h>\n#include <asm/kvm_emulate.h>\n#include <asm/kvm_coproc.h>\n#include <asm/kvm_psci.h>\n\n#ifdef REQUIRES_VIRT\n__asm__(\".arch_extension\tvirt\");\n#endif\n\nstatic DEFINE_PER_CPU(unsigned long, kvm_arm_hyp_stack_page);\nstatic kvm_cpu_context_t __percpu *kvm_host_cpu_state;\nstatic unsigned long hyp_default_vectors;\n\n/* Per-CPU variable containing the currently running vcpu. */\nstatic DEFINE_PER_CPU(struct kvm_vcpu *, kvm_arm_running_vcpu);\n\n/* The VMID used in the VTTBR */\nstatic atomic64_t kvm_vmid_gen = ATOMIC64_INIT(1);\nstatic u8 kvm_next_vmid;\nstatic DEFINE_SPINLOCK(kvm_vmid_lock);\n\nstatic bool vgic_present;\n\nstatic void kvm_arm_set_running_vcpu(struct kvm_vcpu *vcpu)\n{\n\tBUG_ON(preemptible());\n\t__get_cpu_var(kvm_arm_running_vcpu) = vcpu;\n}\n\n/**\n * kvm_arm_get_running_vcpu - get the vcpu running on the current CPU.\n * Must be called from non-preemptible context\n */\nstruct kvm_vcpu *kvm_arm_get_running_vcpu(void)\n{\n\tBUG_ON(preemptible());\n\treturn __get_cpu_var(kvm_arm_running_vcpu);\n}\n\n/**\n * kvm_arm_get_running_vcpus - get the per-CPU array of currently running vcpus.\n */\nstruct kvm_vcpu __percpu **kvm_get_running_vcpus(void)\n{\n\treturn &kvm_arm_running_vcpu;\n}\n\nint kvm_arch_hardware_enable(void *garbage)\n{\n\treturn 0;\n}\n\nint kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;\n}\n\nvoid kvm_arch_hardware_disable(void *garbage)\n{\n}\n\nint kvm_arch_hardware_setup(void)\n{\n\treturn 0;\n}\n\nvoid kvm_arch_hardware_unsetup(void)\n{\n}\n\nvoid kvm_arch_check_processor_compat(void *rtn)\n{\n\t*(int *)rtn = 0;\n}\n\nvoid kvm_arch_sync_events(struct kvm *kvm)\n{\n}\n\n/**\n * kvm_arch_init_vm - initializes a VM data structure\n * @kvm:\tpointer to the KVM struct\n */\nint kvm_arch_init_vm(struct kvm *kvm, unsigned long type)\n{\n\tint ret = 0;\n\n\tif (type)\n\t\treturn -EINVAL;\n\n\tret = kvm_alloc_stage2_pgd(kvm);\n\tif (ret)\n\t\tgoto out_fail_alloc;\n\n\tret = create_hyp_mappings(kvm, kvm + 1);\n\tif (ret)\n\t\tgoto out_free_stage2_pgd;\n\n\t/* Mark the initial VMID generation invalid */\n\tkvm->arch.vmid_gen = 0;\n\n\treturn ret;\nout_free_stage2_pgd:\n\tkvm_free_stage2_pgd(kvm);\nout_fail_alloc:\n\treturn ret;\n}\n\nint kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)\n{\n\treturn VM_FAULT_SIGBUS;\n}\n\nvoid kvm_arch_free_memslot(struct kvm_memory_slot *free,\n\t\t\t   struct kvm_memory_slot *dont)\n{\n}\n\nint kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)\n{\n\treturn 0;\n}\n\n/**\n * kvm_arch_destroy_vm - destroy the VM data structure\n * @kvm:\tpointer to the KVM struct\n */\nvoid kvm_arch_destroy_vm(struct kvm *kvm)\n{\n\tint i;\n\n\tkvm_free_stage2_pgd(kvm);\n\n\tfor (i = 0; i < KVM_MAX_VCPUS; ++i) {\n\t\tif (kvm->vcpus[i]) {\n\t\t\tkvm_arch_vcpu_free(kvm->vcpus[i]);\n\t\t\tkvm->vcpus[i] = NULL;\n\t\t}\n\t}\n}\n\nint kvm_dev_ioctl_check_extension(long ext)\n{\n\tint r;\n\tswitch (ext) {\n\tcase KVM_CAP_IRQCHIP:\n\t\tr = vgic_present;\n\t\tbreak;\n\tcase KVM_CAP_USER_MEMORY:\n\tcase KVM_CAP_SYNC_MMU:\n\tcase KVM_CAP_DESTROY_MEMORY_REGION_WORKS:\n\tcase KVM_CAP_ONE_REG:\n\tcase KVM_CAP_ARM_PSCI:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_COALESCED_MMIO:\n\t\tr = KVM_COALESCED_MMIO_PAGE_OFFSET;\n\t\tbreak;\n\tcase KVM_CAP_ARM_SET_DEVICE_ADDR:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_NR_VCPUS:\n\t\tr = num_online_cpus();\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPUS:\n\t\tr = KVM_MAX_VCPUS;\n\t\tbreak;\n\tdefault:\n\t\tr = kvm_arch_dev_ioctl_check_extension(ext);\n\t\tbreak;\n\t}\n\treturn r;\n}\n\nlong kvm_arch_dev_ioctl(struct file *filp,\n\t\t\tunsigned int ioctl, unsigned long arg)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *memslot,\n\t\t\t\t   struct kvm_userspace_memory_region *mem,\n\t\t\t\t   enum kvm_mr_change change)\n{\n\treturn 0;\n}\n\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\n\t\t\t\t   struct kvm_userspace_memory_region *mem,\n\t\t\t\t   const struct kvm_memory_slot *old,\n\t\t\t\t   enum kvm_mr_change change)\n{\n}\n\nvoid kvm_arch_flush_shadow_all(struct kvm *kvm)\n{\n}\n\nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot)\n{\n}\n\nstruct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)\n{\n\tint err;\n\tstruct kvm_vcpu *vcpu;\n\n\tvcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);\n\tif (!vcpu) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\terr = kvm_vcpu_init(vcpu, kvm, id);\n\tif (err)\n\t\tgoto free_vcpu;\n\n\terr = create_hyp_mappings(vcpu, vcpu + 1);\n\tif (err)\n\t\tgoto vcpu_uninit;\n\n\treturn vcpu;\nvcpu_uninit:\n\tkvm_vcpu_uninit(vcpu);\nfree_vcpu:\n\tkmem_cache_free(kvm_vcpu_cache, vcpu);\nout:\n\treturn ERR_PTR(err);\n}\n\nint kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n\nvoid kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)\n{\n\tkvm_mmu_free_memory_caches(vcpu);\n\tkvm_timer_vcpu_terminate(vcpu);\n\tkmem_cache_free(kvm_vcpu_cache, vcpu);\n}\n\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)\n{\n\tkvm_arch_vcpu_free(vcpu);\n}\n\nint kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n\nint kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\n\t/* Force users to call KVM_ARM_VCPU_INIT */\n\tvcpu->arch.target = -1;\n\n\t/* Set up VGIC */\n\tret = kvm_vgic_vcpu_init(vcpu);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Set up the timer */\n\tkvm_timer_vcpu_init(vcpu);\n\n\treturn 0;\n}\n\nvoid kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)\n{\n}\n\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\tvcpu->cpu = cpu;\n\tvcpu->arch.host_cpu_context = this_cpu_ptr(kvm_host_cpu_state);\n\n\t/*\n\t * Check whether this vcpu requires the cache to be flushed on\n\t * this physical CPU. This is a consequence of doing dcache\n\t * operations by set/way on this vcpu. We do it here to be in\n\t * a non-preemptible section.\n\t */\n\tif (cpumask_test_and_clear_cpu(cpu, &vcpu->arch.require_dcache_flush))\n\t\tflush_cache_all(); /* We'd really want v7_flush_dcache_all() */\n\n\tkvm_arm_set_running_vcpu(vcpu);\n}\n\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tkvm_arm_set_running_vcpu(NULL);\n}\n\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_guest_debug *dbg)\n{\n\treturn -EINVAL;\n}\n\n\nint kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state)\n{\n\treturn -EINVAL;\n}\n\n/**\n * kvm_arch_vcpu_runnable - determine if the vcpu can be scheduled\n * @v:\t\tThe VCPU pointer\n *\n * If the guest CPU is not waiting for interrupts or an interrupt line is\n * asserted, the CPU is by definition runnable.\n */\nint kvm_arch_vcpu_runnable(struct kvm_vcpu *v)\n{\n\treturn !!v->arch.irq_lines || kvm_vgic_vcpu_pending_irq(v);\n}\n\n/* Just ensure a guest exit from a particular CPU */\nstatic void exit_vm_noop(void *info)\n{\n}\n\nvoid force_vm_exit(const cpumask_t *mask)\n{\n\tsmp_call_function_many(mask, exit_vm_noop, NULL, true);\n}\n\n/**\n * need_new_vmid_gen - check that the VMID is still valid\n * @kvm: The VM's VMID to checkt\n *\n * return true if there is a new generation of VMIDs being used\n *\n * The hardware supports only 256 values with the value zero reserved for the\n * host, so we check if an assigned value belongs to a previous generation,\n * which which requires us to assign a new value. If we're the first to use a\n * VMID for the new generation, we must flush necessary caches and TLBs on all\n * CPUs.\n */\nstatic bool need_new_vmid_gen(struct kvm *kvm)\n{\n\treturn unlikely(kvm->arch.vmid_gen != atomic64_read(&kvm_vmid_gen));\n}\n\n/**\n * update_vttbr - Update the VTTBR with a valid VMID before the guest runs\n * @kvm\tThe guest that we are about to run\n *\n * Called from kvm_arch_vcpu_ioctl_run before entering the guest to ensure the\n * VM has a valid VMID, otherwise assigns a new one and flushes corresponding\n * caches and TLBs.\n */\nstatic void update_vttbr(struct kvm *kvm)\n{\n\tphys_addr_t pgd_phys;\n\tu64 vmid;\n\n\tif (!need_new_vmid_gen(kvm))\n\t\treturn;\n\n\tspin_lock(&kvm_vmid_lock);\n\n\t/*\n\t * We need to re-check the vmid_gen here to ensure that if another vcpu\n\t * already allocated a valid vmid for this vm, then this vcpu should\n\t * use the same vmid.\n\t */\n\tif (!need_new_vmid_gen(kvm)) {\n\t\tspin_unlock(&kvm_vmid_lock);\n\t\treturn;\n\t}\n\n\t/* First user of a new VMID generation? */\n\tif (unlikely(kvm_next_vmid == 0)) {\n\t\tatomic64_inc(&kvm_vmid_gen);\n\t\tkvm_next_vmid = 1;\n\n\t\t/*\n\t\t * On SMP we know no other CPUs can use this CPU's or each\n\t\t * other's VMID after force_vm_exit returns since the\n\t\t * kvm_vmid_lock blocks them from reentry to the guest.\n\t\t */\n\t\tforce_vm_exit(cpu_all_mask);\n\t\t/*\n\t\t * Now broadcast TLB + ICACHE invalidation over the inner\n\t\t * shareable domain to make sure all data structures are\n\t\t * clean.\n\t\t */\n\t\tkvm_call_hyp(__kvm_flush_vm_context);\n\t}\n\n\tkvm->arch.vmid_gen = atomic64_read(&kvm_vmid_gen);\n\tkvm->arch.vmid = kvm_next_vmid;\n\tkvm_next_vmid++;\n\n\t/* update vttbr to be used with the new vmid */\n\tpgd_phys = virt_to_phys(kvm->arch.pgd);\n\tvmid = ((u64)(kvm->arch.vmid) << VTTBR_VMID_SHIFT) & VTTBR_VMID_MASK;\n\tkvm->arch.vttbr = pgd_phys & VTTBR_BADDR_MASK;\n\tkvm->arch.vttbr |= vmid;\n\n\tspin_unlock(&kvm_vmid_lock);\n}\n\nstatic int kvm_vcpu_first_run_init(struct kvm_vcpu *vcpu)\n{\n\tif (likely(vcpu->arch.has_run_once))\n\t\treturn 0;\n\n\tvcpu->arch.has_run_once = true;\n\n\t/*\n\t * Initialize the VGIC before running a vcpu the first time on\n\t * this VM.\n\t */\n\tif (irqchip_in_kernel(vcpu->kvm) &&\n\t    unlikely(!vgic_initialized(vcpu->kvm))) {\n\t\tint ret = kvm_vgic_init(vcpu->kvm);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Handle the \"start in power-off\" case by calling into the\n\t * PSCI code.\n\t */\n\tif (test_and_clear_bit(KVM_ARM_VCPU_POWER_OFF, vcpu->arch.features)) {\n\t\t*vcpu_reg(vcpu, 0) = KVM_PSCI_FN_CPU_OFF;\n\t\tkvm_psci_call(vcpu);\n\t}\n\n\treturn 0;\n}\n\nstatic void vcpu_pause(struct kvm_vcpu *vcpu)\n{\n\twait_queue_head_t *wq = kvm_arch_vcpu_wq(vcpu);\n\n\twait_event_interruptible(*wq, !vcpu->arch.pause);\n}\n\n/**\n * kvm_arch_vcpu_ioctl_run - the main VCPU run function to execute guest code\n * @vcpu:\tThe VCPU pointer\n * @run:\tThe kvm_run structure pointer used for userspace state exchange\n *\n * This function is called through the VCPU_RUN ioctl called from user space. It\n * will execute VM code in a loop until the time slice for the process is used\n * or some emulation is needed from user space in which case the function will\n * return with return value 0 and with the kvm_run structure filled in with the\n * required data for the requested emulation.\n */\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tint ret;\n\tsigset_t sigsaved;\n\n\t/* Make sure they initialize the vcpu with KVM_ARM_VCPU_INIT */\n\tif (unlikely(vcpu->arch.target < 0))\n\t\treturn -ENOEXEC;\n\n\tret = kvm_vcpu_first_run_init(vcpu);\n\tif (ret)\n\t\treturn ret;\n\n\tif (run->exit_reason == KVM_EXIT_MMIO) {\n\t\tret = kvm_handle_mmio_return(vcpu, vcpu->run);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (vcpu->sigset_active)\n\t\tsigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);\n\n\tret = 1;\n\trun->exit_reason = KVM_EXIT_UNKNOWN;\n\twhile (ret > 0) {\n\t\t/*\n\t\t * Check conditions before entering the guest\n\t\t */\n\t\tcond_resched();\n\n\t\tupdate_vttbr(vcpu->kvm);\n\n\t\tif (vcpu->arch.pause)\n\t\t\tvcpu_pause(vcpu);\n\n\t\tkvm_vgic_flush_hwstate(vcpu);\n\t\tkvm_timer_flush_hwstate(vcpu);\n\n\t\tlocal_irq_disable();\n\n\t\t/*\n\t\t * Re-check atomic conditions\n\t\t */\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\trun->exit_reason = KVM_EXIT_INTR;\n\t\t}\n\n\t\tif (ret <= 0 || need_new_vmid_gen(vcpu->kvm)) {\n\t\t\tlocal_irq_enable();\n\t\t\tkvm_timer_sync_hwstate(vcpu);\n\t\t\tkvm_vgic_sync_hwstate(vcpu);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/**************************************************************\n\t\t * Enter the guest\n\t\t */\n\t\ttrace_kvm_entry(*vcpu_pc(vcpu));\n\t\tkvm_guest_enter();\n\t\tvcpu->mode = IN_GUEST_MODE;\n\n\t\tret = kvm_call_hyp(__kvm_vcpu_run, vcpu);\n\n\t\tvcpu->mode = OUTSIDE_GUEST_MODE;\n\t\tvcpu->arch.last_pcpu = smp_processor_id();\n\t\tkvm_guest_exit();\n\t\ttrace_kvm_exit(*vcpu_pc(vcpu));\n\t\t/*\n\t\t * We may have taken a host interrupt in HYP mode (ie\n\t\t * while executing the guest). This interrupt is still\n\t\t * pending, as we haven't serviced it yet!\n\t\t *\n\t\t * We're now back in SVC mode, with interrupts\n\t\t * disabled.  Enabling the interrupts now will have\n\t\t * the effect of taking the interrupt again, in SVC\n\t\t * mode this time.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Back from guest\n\t\t *************************************************************/\n\n\t\tkvm_timer_sync_hwstate(vcpu);\n\t\tkvm_vgic_sync_hwstate(vcpu);\n\n\t\tret = handle_exit(vcpu, run, ret);\n\t}\n\n\tif (vcpu->sigset_active)\n\t\tsigprocmask(SIG_SETMASK, &sigsaved, NULL);\n\treturn ret;\n}\n\nstatic int vcpu_interrupt_line(struct kvm_vcpu *vcpu, int number, bool level)\n{\n\tint bit_index;\n\tbool set;\n\tunsigned long *ptr;\n\n\tif (number == KVM_ARM_IRQ_CPU_IRQ)\n\t\tbit_index = __ffs(HCR_VI);\n\telse /* KVM_ARM_IRQ_CPU_FIQ */\n\t\tbit_index = __ffs(HCR_VF);\n\n\tptr = (unsigned long *)&vcpu->arch.irq_lines;\n\tif (level)\n\t\tset = test_and_set_bit(bit_index, ptr);\n\telse\n\t\tset = test_and_clear_bit(bit_index, ptr);\n\n\t/*\n\t * If we didn't change anything, no need to wake up or kick other CPUs\n\t */\n\tif (set == level)\n\t\treturn 0;\n\n\t/*\n\t * The vcpu irq_lines field was updated, wake up sleeping VCPUs and\n\t * trigger a world-switch round on the running physical CPU to set the\n\t * virtual IRQ/FIQ fields in the HCR appropriately.\n\t */\n\tkvm_vcpu_kick(vcpu);\n\n\treturn 0;\n}\n\nint kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level,\n\t\t\t  bool line_status)\n{\n\tu32 irq = irq_level->irq;\n\tunsigned int irq_type, vcpu_idx, irq_num;\n\tint nrcpus = atomic_read(&kvm->online_vcpus);\n\tstruct kvm_vcpu *vcpu = NULL;\n\tbool level = irq_level->level;\n\n\tirq_type = (irq >> KVM_ARM_IRQ_TYPE_SHIFT) & KVM_ARM_IRQ_TYPE_MASK;\n\tvcpu_idx = (irq >> KVM_ARM_IRQ_VCPU_SHIFT) & KVM_ARM_IRQ_VCPU_MASK;\n\tirq_num = (irq >> KVM_ARM_IRQ_NUM_SHIFT) & KVM_ARM_IRQ_NUM_MASK;\n\n\ttrace_kvm_irq_line(irq_type, vcpu_idx, irq_num, irq_level->level);\n\n\tswitch (irq_type) {\n\tcase KVM_ARM_IRQ_TYPE_CPU:\n\t\tif (irqchip_in_kernel(kvm))\n\t\t\treturn -ENXIO;\n\n\t\tif (vcpu_idx >= nrcpus)\n\t\t\treturn -EINVAL;\n\n\t\tvcpu = kvm_get_vcpu(kvm, vcpu_idx);\n\t\tif (!vcpu)\n\t\t\treturn -EINVAL;\n\n\t\tif (irq_num > KVM_ARM_IRQ_CPU_FIQ)\n\t\t\treturn -EINVAL;\n\n\t\treturn vcpu_interrupt_line(vcpu, irq_num, level);\n\tcase KVM_ARM_IRQ_TYPE_PPI:\n\t\tif (!irqchip_in_kernel(kvm))\n\t\t\treturn -ENXIO;\n\n\t\tif (vcpu_idx >= nrcpus)\n\t\t\treturn -EINVAL;\n\n\t\tvcpu = kvm_get_vcpu(kvm, vcpu_idx);\n\t\tif (!vcpu)\n\t\t\treturn -EINVAL;\n\n\t\tif (irq_num < VGIC_NR_SGIS || irq_num >= VGIC_NR_PRIVATE_IRQS)\n\t\t\treturn -EINVAL;\n\n\t\treturn kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level);\n\tcase KVM_ARM_IRQ_TYPE_SPI:\n\t\tif (!irqchip_in_kernel(kvm))\n\t\t\treturn -ENXIO;\n\n\t\tif (irq_num < VGIC_NR_PRIVATE_IRQS ||\n\t\t    irq_num > KVM_ARM_IRQ_GIC_MAX)\n\t\t\treturn -EINVAL;\n\n\t\treturn kvm_vgic_inject_irq(kvm, 0, irq_num, level);\n\t}\n\n\treturn -EINVAL;\n}\n\nlong kvm_arch_vcpu_ioctl(struct file *filp,\n\t\t\t unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (ioctl) {\n\tcase KVM_ARM_VCPU_INIT: {\n\t\tstruct kvm_vcpu_init init;\n\n\t\tif (copy_from_user(&init, argp, sizeof(init)))\n\t\t\treturn -EFAULT;\n\n\t\treturn kvm_vcpu_set_target(vcpu, &init);\n\n\t}\n\tcase KVM_SET_ONE_REG:\n\tcase KVM_GET_ONE_REG: {\n\t\tstruct kvm_one_reg reg;\n\t\tif (copy_from_user(&reg, argp, sizeof(reg)))\n\t\t\treturn -EFAULT;\n\t\tif (ioctl == KVM_SET_ONE_REG)\n\t\t\treturn kvm_arm_set_reg(vcpu, &reg);\n\t\telse\n\t\t\treturn kvm_arm_get_reg(vcpu, &reg);\n\t}\n\tcase KVM_GET_REG_LIST: {\n\t\tstruct kvm_reg_list __user *user_list = argp;\n\t\tstruct kvm_reg_list reg_list;\n\t\tunsigned n;\n\n\t\tif (copy_from_user(&reg_list, user_list, sizeof(reg_list)))\n\t\t\treturn -EFAULT;\n\t\tn = reg_list.n;\n\t\treg_list.n = kvm_arm_num_regs(vcpu);\n\t\tif (copy_to_user(user_list, &reg_list, sizeof(reg_list)))\n\t\t\treturn -EFAULT;\n\t\tif (n < reg_list.n)\n\t\t\treturn -E2BIG;\n\t\treturn kvm_arm_copy_reg_indices(vcpu, user_list->reg);\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nint kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)\n{\n\treturn -EINVAL;\n}\n\nstatic int kvm_vm_ioctl_set_device_addr(struct kvm *kvm,\n\t\t\t\t\tstruct kvm_arm_device_addr *dev_addr)\n{\n\tunsigned long dev_id, type;\n\n\tdev_id = (dev_addr->id & KVM_ARM_DEVICE_ID_MASK) >>\n\t\tKVM_ARM_DEVICE_ID_SHIFT;\n\ttype = (dev_addr->id & KVM_ARM_DEVICE_TYPE_MASK) >>\n\t\tKVM_ARM_DEVICE_TYPE_SHIFT;\n\n\tswitch (dev_id) {\n\tcase KVM_ARM_DEVICE_VGIC_V2:\n\t\tif (!vgic_present)\n\t\t\treturn -ENXIO;\n\t\treturn kvm_vgic_set_addr(kvm, type, dev_addr->addr);\n\tdefault:\n\t\treturn -ENODEV;\n\t}\n}\n\nlong kvm_arch_vm_ioctl(struct file *filp,\n\t\t       unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (ioctl) {\n\tcase KVM_CREATE_IRQCHIP: {\n\t\tif (vgic_present)\n\t\t\treturn kvm_vgic_create(kvm);\n\t\telse\n\t\t\treturn -ENXIO;\n\t}\n\tcase KVM_ARM_SET_DEVICE_ADDR: {\n\t\tstruct kvm_arm_device_addr dev_addr;\n\n\t\tif (copy_from_user(&dev_addr, argp, sizeof(dev_addr)))\n\t\t\treturn -EFAULT;\n\t\treturn kvm_vm_ioctl_set_device_addr(kvm, &dev_addr);\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic void cpu_init_hyp_mode(void *dummy)\n{\n\tunsigned long long boot_pgd_ptr;\n\tunsigned long long pgd_ptr;\n\tunsigned long hyp_stack_ptr;\n\tunsigned long stack_page;\n\tunsigned long vector_ptr;\n\n\t/* Switch from the HYP stub to our own HYP init vector */\n\t__hyp_set_vectors(kvm_get_idmap_vector());\n\n\tboot_pgd_ptr = (unsigned long long)kvm_mmu_get_boot_httbr();\n\tpgd_ptr = (unsigned long long)kvm_mmu_get_httbr();\n\tstack_page = __get_cpu_var(kvm_arm_hyp_stack_page);\n\thyp_stack_ptr = stack_page + PAGE_SIZE;\n\tvector_ptr = (unsigned long)__kvm_hyp_vector;\n\n\t__cpu_init_hyp_mode(boot_pgd_ptr, pgd_ptr, hyp_stack_ptr, vector_ptr);\n}\n\nstatic int hyp_init_cpu_notify(struct notifier_block *self,\n\t\t\t       unsigned long action, void *cpu)\n{\n\tswitch (action) {\n\tcase CPU_STARTING:\n\tcase CPU_STARTING_FROZEN:\n\t\tcpu_init_hyp_mode(NULL);\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block hyp_init_cpu_nb = {\n\t.notifier_call = hyp_init_cpu_notify,\n};\n\n/**\n * Inits Hyp-mode on all online CPUs\n */\nstatic int init_hyp_mode(void)\n{\n\tint cpu;\n\tint err = 0;\n\n\t/*\n\t * Allocate Hyp PGD and setup Hyp identity mapping\n\t */\n\terr = kvm_mmu_init();\n\tif (err)\n\t\tgoto out_err;\n\n\t/*\n\t * It is probably enough to obtain the default on one\n\t * CPU. It's unlikely to be different on the others.\n\t */\n\thyp_default_vectors = __hyp_get_vectors();\n\n\t/*\n\t * Allocate stack pages for Hypervisor-mode\n\t */\n\tfor_each_possible_cpu(cpu) {\n\t\tunsigned long stack_page;\n\n\t\tstack_page = __get_free_page(GFP_KERNEL);\n\t\tif (!stack_page) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_free_stack_pages;\n\t\t}\n\n\t\tper_cpu(kvm_arm_hyp_stack_page, cpu) = stack_page;\n\t}\n\n\t/*\n\t * Map the Hyp-code called directly from the host\n\t */\n\terr = create_hyp_mappings(__kvm_hyp_code_start, __kvm_hyp_code_end);\n\tif (err) {\n\t\tkvm_err(\"Cannot map world-switch code\\n\");\n\t\tgoto out_free_mappings;\n\t}\n\n\t/*\n\t * Map the Hyp stack pages\n\t */\n\tfor_each_possible_cpu(cpu) {\n\t\tchar *stack_page = (char *)per_cpu(kvm_arm_hyp_stack_page, cpu);\n\t\terr = create_hyp_mappings(stack_page, stack_page + PAGE_SIZE);\n\n\t\tif (err) {\n\t\t\tkvm_err(\"Cannot map hyp stack\\n\");\n\t\t\tgoto out_free_mappings;\n\t\t}\n\t}\n\n\t/*\n\t * Map the host CPU structures\n\t */\n\tkvm_host_cpu_state = alloc_percpu(kvm_cpu_context_t);\n\tif (!kvm_host_cpu_state) {\n\t\terr = -ENOMEM;\n\t\tkvm_err(\"Cannot allocate host CPU state\\n\");\n\t\tgoto out_free_mappings;\n\t}\n\n\tfor_each_possible_cpu(cpu) {\n\t\tkvm_cpu_context_t *cpu_ctxt;\n\n\t\tcpu_ctxt = per_cpu_ptr(kvm_host_cpu_state, cpu);\n\t\terr = create_hyp_mappings(cpu_ctxt, cpu_ctxt + 1);\n\n\t\tif (err) {\n\t\t\tkvm_err(\"Cannot map host CPU state: %d\\n\", err);\n\t\t\tgoto out_free_context;\n\t\t}\n\t}\n\n\t/*\n\t * Execute the init code on each CPU.\n\t */\n\ton_each_cpu(cpu_init_hyp_mode, NULL, 1);\n\n\t/*\n\t * Init HYP view of VGIC\n\t */\n\terr = kvm_vgic_hyp_init();\n\tif (err)\n\t\tgoto out_free_context;\n\n#ifdef CONFIG_KVM_ARM_VGIC\n\t\tvgic_present = true;\n#endif\n\n\t/*\n\t * Init HYP architected timer support\n\t */\n\terr = kvm_timer_hyp_init();\n\tif (err)\n\t\tgoto out_free_mappings;\n\n#ifndef CONFIG_HOTPLUG_CPU\n\tfree_boot_hyp_pgd();\n#endif\n\n\tkvm_perf_init();\n\n\tkvm_info(\"Hyp mode initialized successfully\\n\");\n\n\treturn 0;\nout_free_context:\n\tfree_percpu(kvm_host_cpu_state);\nout_free_mappings:\n\tfree_hyp_pgds();\nout_free_stack_pages:\n\tfor_each_possible_cpu(cpu)\n\t\tfree_page(per_cpu(kvm_arm_hyp_stack_page, cpu));\nout_err:\n\tkvm_err(\"error initializing Hyp mode: %d\\n\", err);\n\treturn err;\n}\n\nstatic void check_kvm_target_cpu(void *ret)\n{\n\t*(int *)ret = kvm_target_cpu();\n}\n\n/**\n * Initialize Hyp-mode and memory mappings on all CPUs.\n */\nint kvm_arch_init(void *opaque)\n{\n\tint err;\n\tint ret, cpu;\n\n\tif (!is_hyp_mode_available()) {\n\t\tkvm_err(\"HYP mode not available\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tfor_each_online_cpu(cpu) {\n\t\tsmp_call_function_single(cpu, check_kvm_target_cpu, &ret, 1);\n\t\tif (ret < 0) {\n\t\t\tkvm_err(\"Error, CPU %d not supported!\\n\", cpu);\n\t\t\treturn -ENODEV;\n\t\t}\n\t}\n\n\terr = init_hyp_mode();\n\tif (err)\n\t\tgoto out_err;\n\n\terr = register_cpu_notifier(&hyp_init_cpu_nb);\n\tif (err) {\n\t\tkvm_err(\"Cannot register HYP init CPU notifier (%d)\\n\", err);\n\t\tgoto out_err;\n\t}\n\n\tkvm_coproc_table_init();\n\treturn 0;\nout_err:\n\treturn err;\n}\n\n/* NOP: Compiling as a module not supported */\nvoid kvm_arch_exit(void)\n{\n\tkvm_perf_teardown();\n}\n\nstatic int arm_init(void)\n{\n\tint rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);\n\treturn rc;\n}\n\nmodule_init(arm_init);\n"], "fixing_code": ["/*\n * Copyright (C) 2012 - Virtual Open Systems and Columbia University\n * Author: Christoffer Dall <c.dall@virtualopensystems.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License, version 2, as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n */\n\n#include <linux/cpu.h>\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/kvm_host.h>\n#include <linux/module.h>\n#include <linux/vmalloc.h>\n#include <linux/fs.h>\n#include <linux/mman.h>\n#include <linux/sched.h>\n#include <linux/kvm.h>\n#include <trace/events/kvm.h>\n\n#define CREATE_TRACE_POINTS\n#include \"trace.h\"\n\n#include <asm/uaccess.h>\n#include <asm/ptrace.h>\n#include <asm/mman.h>\n#include <asm/tlbflush.h>\n#include <asm/cacheflush.h>\n#include <asm/virt.h>\n#include <asm/kvm_arm.h>\n#include <asm/kvm_asm.h>\n#include <asm/kvm_mmu.h>\n#include <asm/kvm_emulate.h>\n#include <asm/kvm_coproc.h>\n#include <asm/kvm_psci.h>\n\n#ifdef REQUIRES_VIRT\n__asm__(\".arch_extension\tvirt\");\n#endif\n\nstatic DEFINE_PER_CPU(unsigned long, kvm_arm_hyp_stack_page);\nstatic kvm_cpu_context_t __percpu *kvm_host_cpu_state;\nstatic unsigned long hyp_default_vectors;\n\n/* Per-CPU variable containing the currently running vcpu. */\nstatic DEFINE_PER_CPU(struct kvm_vcpu *, kvm_arm_running_vcpu);\n\n/* The VMID used in the VTTBR */\nstatic atomic64_t kvm_vmid_gen = ATOMIC64_INIT(1);\nstatic u8 kvm_next_vmid;\nstatic DEFINE_SPINLOCK(kvm_vmid_lock);\n\nstatic bool vgic_present;\n\nstatic void kvm_arm_set_running_vcpu(struct kvm_vcpu *vcpu)\n{\n\tBUG_ON(preemptible());\n\t__get_cpu_var(kvm_arm_running_vcpu) = vcpu;\n}\n\n/**\n * kvm_arm_get_running_vcpu - get the vcpu running on the current CPU.\n * Must be called from non-preemptible context\n */\nstruct kvm_vcpu *kvm_arm_get_running_vcpu(void)\n{\n\tBUG_ON(preemptible());\n\treturn __get_cpu_var(kvm_arm_running_vcpu);\n}\n\n/**\n * kvm_arm_get_running_vcpus - get the per-CPU array of currently running vcpus.\n */\nstruct kvm_vcpu __percpu **kvm_get_running_vcpus(void)\n{\n\treturn &kvm_arm_running_vcpu;\n}\n\nint kvm_arch_hardware_enable(void *garbage)\n{\n\treturn 0;\n}\n\nint kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;\n}\n\nvoid kvm_arch_hardware_disable(void *garbage)\n{\n}\n\nint kvm_arch_hardware_setup(void)\n{\n\treturn 0;\n}\n\nvoid kvm_arch_hardware_unsetup(void)\n{\n}\n\nvoid kvm_arch_check_processor_compat(void *rtn)\n{\n\t*(int *)rtn = 0;\n}\n\nvoid kvm_arch_sync_events(struct kvm *kvm)\n{\n}\n\n/**\n * kvm_arch_init_vm - initializes a VM data structure\n * @kvm:\tpointer to the KVM struct\n */\nint kvm_arch_init_vm(struct kvm *kvm, unsigned long type)\n{\n\tint ret = 0;\n\n\tif (type)\n\t\treturn -EINVAL;\n\n\tret = kvm_alloc_stage2_pgd(kvm);\n\tif (ret)\n\t\tgoto out_fail_alloc;\n\n\tret = create_hyp_mappings(kvm, kvm + 1);\n\tif (ret)\n\t\tgoto out_free_stage2_pgd;\n\n\t/* Mark the initial VMID generation invalid */\n\tkvm->arch.vmid_gen = 0;\n\n\treturn ret;\nout_free_stage2_pgd:\n\tkvm_free_stage2_pgd(kvm);\nout_fail_alloc:\n\treturn ret;\n}\n\nint kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)\n{\n\treturn VM_FAULT_SIGBUS;\n}\n\nvoid kvm_arch_free_memslot(struct kvm_memory_slot *free,\n\t\t\t   struct kvm_memory_slot *dont)\n{\n}\n\nint kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)\n{\n\treturn 0;\n}\n\n/**\n * kvm_arch_destroy_vm - destroy the VM data structure\n * @kvm:\tpointer to the KVM struct\n */\nvoid kvm_arch_destroy_vm(struct kvm *kvm)\n{\n\tint i;\n\n\tkvm_free_stage2_pgd(kvm);\n\n\tfor (i = 0; i < KVM_MAX_VCPUS; ++i) {\n\t\tif (kvm->vcpus[i]) {\n\t\t\tkvm_arch_vcpu_free(kvm->vcpus[i]);\n\t\t\tkvm->vcpus[i] = NULL;\n\t\t}\n\t}\n}\n\nint kvm_dev_ioctl_check_extension(long ext)\n{\n\tint r;\n\tswitch (ext) {\n\tcase KVM_CAP_IRQCHIP:\n\t\tr = vgic_present;\n\t\tbreak;\n\tcase KVM_CAP_USER_MEMORY:\n\tcase KVM_CAP_SYNC_MMU:\n\tcase KVM_CAP_DESTROY_MEMORY_REGION_WORKS:\n\tcase KVM_CAP_ONE_REG:\n\tcase KVM_CAP_ARM_PSCI:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_COALESCED_MMIO:\n\t\tr = KVM_COALESCED_MMIO_PAGE_OFFSET;\n\t\tbreak;\n\tcase KVM_CAP_ARM_SET_DEVICE_ADDR:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_NR_VCPUS:\n\t\tr = num_online_cpus();\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPUS:\n\t\tr = KVM_MAX_VCPUS;\n\t\tbreak;\n\tdefault:\n\t\tr = kvm_arch_dev_ioctl_check_extension(ext);\n\t\tbreak;\n\t}\n\treturn r;\n}\n\nlong kvm_arch_dev_ioctl(struct file *filp,\n\t\t\tunsigned int ioctl, unsigned long arg)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *memslot,\n\t\t\t\t   struct kvm_userspace_memory_region *mem,\n\t\t\t\t   enum kvm_mr_change change)\n{\n\treturn 0;\n}\n\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\n\t\t\t\t   struct kvm_userspace_memory_region *mem,\n\t\t\t\t   const struct kvm_memory_slot *old,\n\t\t\t\t   enum kvm_mr_change change)\n{\n}\n\nvoid kvm_arch_flush_shadow_all(struct kvm *kvm)\n{\n}\n\nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot)\n{\n}\n\nstruct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)\n{\n\tint err;\n\tstruct kvm_vcpu *vcpu;\n\n\tvcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);\n\tif (!vcpu) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\terr = kvm_vcpu_init(vcpu, kvm, id);\n\tif (err)\n\t\tgoto free_vcpu;\n\n\terr = create_hyp_mappings(vcpu, vcpu + 1);\n\tif (err)\n\t\tgoto vcpu_uninit;\n\n\treturn vcpu;\nvcpu_uninit:\n\tkvm_vcpu_uninit(vcpu);\nfree_vcpu:\n\tkmem_cache_free(kvm_vcpu_cache, vcpu);\nout:\n\treturn ERR_PTR(err);\n}\n\nint kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n\nvoid kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)\n{\n\tkvm_mmu_free_memory_caches(vcpu);\n\tkvm_timer_vcpu_terminate(vcpu);\n\tkmem_cache_free(kvm_vcpu_cache, vcpu);\n}\n\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)\n{\n\tkvm_arch_vcpu_free(vcpu);\n}\n\nint kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n\nint kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\n\t/* Force users to call KVM_ARM_VCPU_INIT */\n\tvcpu->arch.target = -1;\n\n\t/* Set up VGIC */\n\tret = kvm_vgic_vcpu_init(vcpu);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Set up the timer */\n\tkvm_timer_vcpu_init(vcpu);\n\n\treturn 0;\n}\n\nvoid kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)\n{\n}\n\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\tvcpu->cpu = cpu;\n\tvcpu->arch.host_cpu_context = this_cpu_ptr(kvm_host_cpu_state);\n\n\t/*\n\t * Check whether this vcpu requires the cache to be flushed on\n\t * this physical CPU. This is a consequence of doing dcache\n\t * operations by set/way on this vcpu. We do it here to be in\n\t * a non-preemptible section.\n\t */\n\tif (cpumask_test_and_clear_cpu(cpu, &vcpu->arch.require_dcache_flush))\n\t\tflush_cache_all(); /* We'd really want v7_flush_dcache_all() */\n\n\tkvm_arm_set_running_vcpu(vcpu);\n}\n\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tkvm_arm_set_running_vcpu(NULL);\n}\n\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_guest_debug *dbg)\n{\n\treturn -EINVAL;\n}\n\n\nint kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state)\n{\n\treturn -EINVAL;\n}\n\n/**\n * kvm_arch_vcpu_runnable - determine if the vcpu can be scheduled\n * @v:\t\tThe VCPU pointer\n *\n * If the guest CPU is not waiting for interrupts or an interrupt line is\n * asserted, the CPU is by definition runnable.\n */\nint kvm_arch_vcpu_runnable(struct kvm_vcpu *v)\n{\n\treturn !!v->arch.irq_lines || kvm_vgic_vcpu_pending_irq(v);\n}\n\n/* Just ensure a guest exit from a particular CPU */\nstatic void exit_vm_noop(void *info)\n{\n}\n\nvoid force_vm_exit(const cpumask_t *mask)\n{\n\tsmp_call_function_many(mask, exit_vm_noop, NULL, true);\n}\n\n/**\n * need_new_vmid_gen - check that the VMID is still valid\n * @kvm: The VM's VMID to checkt\n *\n * return true if there is a new generation of VMIDs being used\n *\n * The hardware supports only 256 values with the value zero reserved for the\n * host, so we check if an assigned value belongs to a previous generation,\n * which which requires us to assign a new value. If we're the first to use a\n * VMID for the new generation, we must flush necessary caches and TLBs on all\n * CPUs.\n */\nstatic bool need_new_vmid_gen(struct kvm *kvm)\n{\n\treturn unlikely(kvm->arch.vmid_gen != atomic64_read(&kvm_vmid_gen));\n}\n\n/**\n * update_vttbr - Update the VTTBR with a valid VMID before the guest runs\n * @kvm\tThe guest that we are about to run\n *\n * Called from kvm_arch_vcpu_ioctl_run before entering the guest to ensure the\n * VM has a valid VMID, otherwise assigns a new one and flushes corresponding\n * caches and TLBs.\n */\nstatic void update_vttbr(struct kvm *kvm)\n{\n\tphys_addr_t pgd_phys;\n\tu64 vmid;\n\n\tif (!need_new_vmid_gen(kvm))\n\t\treturn;\n\n\tspin_lock(&kvm_vmid_lock);\n\n\t/*\n\t * We need to re-check the vmid_gen here to ensure that if another vcpu\n\t * already allocated a valid vmid for this vm, then this vcpu should\n\t * use the same vmid.\n\t */\n\tif (!need_new_vmid_gen(kvm)) {\n\t\tspin_unlock(&kvm_vmid_lock);\n\t\treturn;\n\t}\n\n\t/* First user of a new VMID generation? */\n\tif (unlikely(kvm_next_vmid == 0)) {\n\t\tatomic64_inc(&kvm_vmid_gen);\n\t\tkvm_next_vmid = 1;\n\n\t\t/*\n\t\t * On SMP we know no other CPUs can use this CPU's or each\n\t\t * other's VMID after force_vm_exit returns since the\n\t\t * kvm_vmid_lock blocks them from reentry to the guest.\n\t\t */\n\t\tforce_vm_exit(cpu_all_mask);\n\t\t/*\n\t\t * Now broadcast TLB + ICACHE invalidation over the inner\n\t\t * shareable domain to make sure all data structures are\n\t\t * clean.\n\t\t */\n\t\tkvm_call_hyp(__kvm_flush_vm_context);\n\t}\n\n\tkvm->arch.vmid_gen = atomic64_read(&kvm_vmid_gen);\n\tkvm->arch.vmid = kvm_next_vmid;\n\tkvm_next_vmid++;\n\n\t/* update vttbr to be used with the new vmid */\n\tpgd_phys = virt_to_phys(kvm->arch.pgd);\n\tvmid = ((u64)(kvm->arch.vmid) << VTTBR_VMID_SHIFT) & VTTBR_VMID_MASK;\n\tkvm->arch.vttbr = pgd_phys & VTTBR_BADDR_MASK;\n\tkvm->arch.vttbr |= vmid;\n\n\tspin_unlock(&kvm_vmid_lock);\n}\n\nstatic int kvm_vcpu_first_run_init(struct kvm_vcpu *vcpu)\n{\n\tif (likely(vcpu->arch.has_run_once))\n\t\treturn 0;\n\n\tvcpu->arch.has_run_once = true;\n\n\t/*\n\t * Initialize the VGIC before running a vcpu the first time on\n\t * this VM.\n\t */\n\tif (irqchip_in_kernel(vcpu->kvm) &&\n\t    unlikely(!vgic_initialized(vcpu->kvm))) {\n\t\tint ret = kvm_vgic_init(vcpu->kvm);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Handle the \"start in power-off\" case by calling into the\n\t * PSCI code.\n\t */\n\tif (test_and_clear_bit(KVM_ARM_VCPU_POWER_OFF, vcpu->arch.features)) {\n\t\t*vcpu_reg(vcpu, 0) = KVM_PSCI_FN_CPU_OFF;\n\t\tkvm_psci_call(vcpu);\n\t}\n\n\treturn 0;\n}\n\nstatic void vcpu_pause(struct kvm_vcpu *vcpu)\n{\n\twait_queue_head_t *wq = kvm_arch_vcpu_wq(vcpu);\n\n\twait_event_interruptible(*wq, !vcpu->arch.pause);\n}\n\nstatic int kvm_vcpu_initialized(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.target >= 0;\n}\n\n/**\n * kvm_arch_vcpu_ioctl_run - the main VCPU run function to execute guest code\n * @vcpu:\tThe VCPU pointer\n * @run:\tThe kvm_run structure pointer used for userspace state exchange\n *\n * This function is called through the VCPU_RUN ioctl called from user space. It\n * will execute VM code in a loop until the time slice for the process is used\n * or some emulation is needed from user space in which case the function will\n * return with return value 0 and with the kvm_run structure filled in with the\n * required data for the requested emulation.\n */\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tint ret;\n\tsigset_t sigsaved;\n\n\tif (unlikely(!kvm_vcpu_initialized(vcpu)))\n\t\treturn -ENOEXEC;\n\n\tret = kvm_vcpu_first_run_init(vcpu);\n\tif (ret)\n\t\treturn ret;\n\n\tif (run->exit_reason == KVM_EXIT_MMIO) {\n\t\tret = kvm_handle_mmio_return(vcpu, vcpu->run);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (vcpu->sigset_active)\n\t\tsigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);\n\n\tret = 1;\n\trun->exit_reason = KVM_EXIT_UNKNOWN;\n\twhile (ret > 0) {\n\t\t/*\n\t\t * Check conditions before entering the guest\n\t\t */\n\t\tcond_resched();\n\n\t\tupdate_vttbr(vcpu->kvm);\n\n\t\tif (vcpu->arch.pause)\n\t\t\tvcpu_pause(vcpu);\n\n\t\tkvm_vgic_flush_hwstate(vcpu);\n\t\tkvm_timer_flush_hwstate(vcpu);\n\n\t\tlocal_irq_disable();\n\n\t\t/*\n\t\t * Re-check atomic conditions\n\t\t */\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\trun->exit_reason = KVM_EXIT_INTR;\n\t\t}\n\n\t\tif (ret <= 0 || need_new_vmid_gen(vcpu->kvm)) {\n\t\t\tlocal_irq_enable();\n\t\t\tkvm_timer_sync_hwstate(vcpu);\n\t\t\tkvm_vgic_sync_hwstate(vcpu);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/**************************************************************\n\t\t * Enter the guest\n\t\t */\n\t\ttrace_kvm_entry(*vcpu_pc(vcpu));\n\t\tkvm_guest_enter();\n\t\tvcpu->mode = IN_GUEST_MODE;\n\n\t\tret = kvm_call_hyp(__kvm_vcpu_run, vcpu);\n\n\t\tvcpu->mode = OUTSIDE_GUEST_MODE;\n\t\tvcpu->arch.last_pcpu = smp_processor_id();\n\t\tkvm_guest_exit();\n\t\ttrace_kvm_exit(*vcpu_pc(vcpu));\n\t\t/*\n\t\t * We may have taken a host interrupt in HYP mode (ie\n\t\t * while executing the guest). This interrupt is still\n\t\t * pending, as we haven't serviced it yet!\n\t\t *\n\t\t * We're now back in SVC mode, with interrupts\n\t\t * disabled.  Enabling the interrupts now will have\n\t\t * the effect of taking the interrupt again, in SVC\n\t\t * mode this time.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Back from guest\n\t\t *************************************************************/\n\n\t\tkvm_timer_sync_hwstate(vcpu);\n\t\tkvm_vgic_sync_hwstate(vcpu);\n\n\t\tret = handle_exit(vcpu, run, ret);\n\t}\n\n\tif (vcpu->sigset_active)\n\t\tsigprocmask(SIG_SETMASK, &sigsaved, NULL);\n\treturn ret;\n}\n\nstatic int vcpu_interrupt_line(struct kvm_vcpu *vcpu, int number, bool level)\n{\n\tint bit_index;\n\tbool set;\n\tunsigned long *ptr;\n\n\tif (number == KVM_ARM_IRQ_CPU_IRQ)\n\t\tbit_index = __ffs(HCR_VI);\n\telse /* KVM_ARM_IRQ_CPU_FIQ */\n\t\tbit_index = __ffs(HCR_VF);\n\n\tptr = (unsigned long *)&vcpu->arch.irq_lines;\n\tif (level)\n\t\tset = test_and_set_bit(bit_index, ptr);\n\telse\n\t\tset = test_and_clear_bit(bit_index, ptr);\n\n\t/*\n\t * If we didn't change anything, no need to wake up or kick other CPUs\n\t */\n\tif (set == level)\n\t\treturn 0;\n\n\t/*\n\t * The vcpu irq_lines field was updated, wake up sleeping VCPUs and\n\t * trigger a world-switch round on the running physical CPU to set the\n\t * virtual IRQ/FIQ fields in the HCR appropriately.\n\t */\n\tkvm_vcpu_kick(vcpu);\n\n\treturn 0;\n}\n\nint kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level,\n\t\t\t  bool line_status)\n{\n\tu32 irq = irq_level->irq;\n\tunsigned int irq_type, vcpu_idx, irq_num;\n\tint nrcpus = atomic_read(&kvm->online_vcpus);\n\tstruct kvm_vcpu *vcpu = NULL;\n\tbool level = irq_level->level;\n\n\tirq_type = (irq >> KVM_ARM_IRQ_TYPE_SHIFT) & KVM_ARM_IRQ_TYPE_MASK;\n\tvcpu_idx = (irq >> KVM_ARM_IRQ_VCPU_SHIFT) & KVM_ARM_IRQ_VCPU_MASK;\n\tirq_num = (irq >> KVM_ARM_IRQ_NUM_SHIFT) & KVM_ARM_IRQ_NUM_MASK;\n\n\ttrace_kvm_irq_line(irq_type, vcpu_idx, irq_num, irq_level->level);\n\n\tswitch (irq_type) {\n\tcase KVM_ARM_IRQ_TYPE_CPU:\n\t\tif (irqchip_in_kernel(kvm))\n\t\t\treturn -ENXIO;\n\n\t\tif (vcpu_idx >= nrcpus)\n\t\t\treturn -EINVAL;\n\n\t\tvcpu = kvm_get_vcpu(kvm, vcpu_idx);\n\t\tif (!vcpu)\n\t\t\treturn -EINVAL;\n\n\t\tif (irq_num > KVM_ARM_IRQ_CPU_FIQ)\n\t\t\treturn -EINVAL;\n\n\t\treturn vcpu_interrupt_line(vcpu, irq_num, level);\n\tcase KVM_ARM_IRQ_TYPE_PPI:\n\t\tif (!irqchip_in_kernel(kvm))\n\t\t\treturn -ENXIO;\n\n\t\tif (vcpu_idx >= nrcpus)\n\t\t\treturn -EINVAL;\n\n\t\tvcpu = kvm_get_vcpu(kvm, vcpu_idx);\n\t\tif (!vcpu)\n\t\t\treturn -EINVAL;\n\n\t\tif (irq_num < VGIC_NR_SGIS || irq_num >= VGIC_NR_PRIVATE_IRQS)\n\t\t\treturn -EINVAL;\n\n\t\treturn kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level);\n\tcase KVM_ARM_IRQ_TYPE_SPI:\n\t\tif (!irqchip_in_kernel(kvm))\n\t\t\treturn -ENXIO;\n\n\t\tif (irq_num < VGIC_NR_PRIVATE_IRQS ||\n\t\t    irq_num > KVM_ARM_IRQ_GIC_MAX)\n\t\t\treturn -EINVAL;\n\n\t\treturn kvm_vgic_inject_irq(kvm, 0, irq_num, level);\n\t}\n\n\treturn -EINVAL;\n}\n\nlong kvm_arch_vcpu_ioctl(struct file *filp,\n\t\t\t unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (ioctl) {\n\tcase KVM_ARM_VCPU_INIT: {\n\t\tstruct kvm_vcpu_init init;\n\n\t\tif (copy_from_user(&init, argp, sizeof(init)))\n\t\t\treturn -EFAULT;\n\n\t\treturn kvm_vcpu_set_target(vcpu, &init);\n\n\t}\n\tcase KVM_SET_ONE_REG:\n\tcase KVM_GET_ONE_REG: {\n\t\tstruct kvm_one_reg reg;\n\n\t\tif (unlikely(!kvm_vcpu_initialized(vcpu)))\n\t\t\treturn -ENOEXEC;\n\n\t\tif (copy_from_user(&reg, argp, sizeof(reg)))\n\t\t\treturn -EFAULT;\n\t\tif (ioctl == KVM_SET_ONE_REG)\n\t\t\treturn kvm_arm_set_reg(vcpu, &reg);\n\t\telse\n\t\t\treturn kvm_arm_get_reg(vcpu, &reg);\n\t}\n\tcase KVM_GET_REG_LIST: {\n\t\tstruct kvm_reg_list __user *user_list = argp;\n\t\tstruct kvm_reg_list reg_list;\n\t\tunsigned n;\n\n\t\tif (unlikely(!kvm_vcpu_initialized(vcpu)))\n\t\t\treturn -ENOEXEC;\n\n\t\tif (copy_from_user(&reg_list, user_list, sizeof(reg_list)))\n\t\t\treturn -EFAULT;\n\t\tn = reg_list.n;\n\t\treg_list.n = kvm_arm_num_regs(vcpu);\n\t\tif (copy_to_user(user_list, &reg_list, sizeof(reg_list)))\n\t\t\treturn -EFAULT;\n\t\tif (n < reg_list.n)\n\t\t\treturn -E2BIG;\n\t\treturn kvm_arm_copy_reg_indices(vcpu, user_list->reg);\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nint kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)\n{\n\treturn -EINVAL;\n}\n\nstatic int kvm_vm_ioctl_set_device_addr(struct kvm *kvm,\n\t\t\t\t\tstruct kvm_arm_device_addr *dev_addr)\n{\n\tunsigned long dev_id, type;\n\n\tdev_id = (dev_addr->id & KVM_ARM_DEVICE_ID_MASK) >>\n\t\tKVM_ARM_DEVICE_ID_SHIFT;\n\ttype = (dev_addr->id & KVM_ARM_DEVICE_TYPE_MASK) >>\n\t\tKVM_ARM_DEVICE_TYPE_SHIFT;\n\n\tswitch (dev_id) {\n\tcase KVM_ARM_DEVICE_VGIC_V2:\n\t\tif (!vgic_present)\n\t\t\treturn -ENXIO;\n\t\treturn kvm_vgic_set_addr(kvm, type, dev_addr->addr);\n\tdefault:\n\t\treturn -ENODEV;\n\t}\n}\n\nlong kvm_arch_vm_ioctl(struct file *filp,\n\t\t       unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\n\tswitch (ioctl) {\n\tcase KVM_CREATE_IRQCHIP: {\n\t\tif (vgic_present)\n\t\t\treturn kvm_vgic_create(kvm);\n\t\telse\n\t\t\treturn -ENXIO;\n\t}\n\tcase KVM_ARM_SET_DEVICE_ADDR: {\n\t\tstruct kvm_arm_device_addr dev_addr;\n\n\t\tif (copy_from_user(&dev_addr, argp, sizeof(dev_addr)))\n\t\t\treturn -EFAULT;\n\t\treturn kvm_vm_ioctl_set_device_addr(kvm, &dev_addr);\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic void cpu_init_hyp_mode(void *dummy)\n{\n\tunsigned long long boot_pgd_ptr;\n\tunsigned long long pgd_ptr;\n\tunsigned long hyp_stack_ptr;\n\tunsigned long stack_page;\n\tunsigned long vector_ptr;\n\n\t/* Switch from the HYP stub to our own HYP init vector */\n\t__hyp_set_vectors(kvm_get_idmap_vector());\n\n\tboot_pgd_ptr = (unsigned long long)kvm_mmu_get_boot_httbr();\n\tpgd_ptr = (unsigned long long)kvm_mmu_get_httbr();\n\tstack_page = __get_cpu_var(kvm_arm_hyp_stack_page);\n\thyp_stack_ptr = stack_page + PAGE_SIZE;\n\tvector_ptr = (unsigned long)__kvm_hyp_vector;\n\n\t__cpu_init_hyp_mode(boot_pgd_ptr, pgd_ptr, hyp_stack_ptr, vector_ptr);\n}\n\nstatic int hyp_init_cpu_notify(struct notifier_block *self,\n\t\t\t       unsigned long action, void *cpu)\n{\n\tswitch (action) {\n\tcase CPU_STARTING:\n\tcase CPU_STARTING_FROZEN:\n\t\tcpu_init_hyp_mode(NULL);\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block hyp_init_cpu_nb = {\n\t.notifier_call = hyp_init_cpu_notify,\n};\n\n/**\n * Inits Hyp-mode on all online CPUs\n */\nstatic int init_hyp_mode(void)\n{\n\tint cpu;\n\tint err = 0;\n\n\t/*\n\t * Allocate Hyp PGD and setup Hyp identity mapping\n\t */\n\terr = kvm_mmu_init();\n\tif (err)\n\t\tgoto out_err;\n\n\t/*\n\t * It is probably enough to obtain the default on one\n\t * CPU. It's unlikely to be different on the others.\n\t */\n\thyp_default_vectors = __hyp_get_vectors();\n\n\t/*\n\t * Allocate stack pages for Hypervisor-mode\n\t */\n\tfor_each_possible_cpu(cpu) {\n\t\tunsigned long stack_page;\n\n\t\tstack_page = __get_free_page(GFP_KERNEL);\n\t\tif (!stack_page) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_free_stack_pages;\n\t\t}\n\n\t\tper_cpu(kvm_arm_hyp_stack_page, cpu) = stack_page;\n\t}\n\n\t/*\n\t * Map the Hyp-code called directly from the host\n\t */\n\terr = create_hyp_mappings(__kvm_hyp_code_start, __kvm_hyp_code_end);\n\tif (err) {\n\t\tkvm_err(\"Cannot map world-switch code\\n\");\n\t\tgoto out_free_mappings;\n\t}\n\n\t/*\n\t * Map the Hyp stack pages\n\t */\n\tfor_each_possible_cpu(cpu) {\n\t\tchar *stack_page = (char *)per_cpu(kvm_arm_hyp_stack_page, cpu);\n\t\terr = create_hyp_mappings(stack_page, stack_page + PAGE_SIZE);\n\n\t\tif (err) {\n\t\t\tkvm_err(\"Cannot map hyp stack\\n\");\n\t\t\tgoto out_free_mappings;\n\t\t}\n\t}\n\n\t/*\n\t * Map the host CPU structures\n\t */\n\tkvm_host_cpu_state = alloc_percpu(kvm_cpu_context_t);\n\tif (!kvm_host_cpu_state) {\n\t\terr = -ENOMEM;\n\t\tkvm_err(\"Cannot allocate host CPU state\\n\");\n\t\tgoto out_free_mappings;\n\t}\n\n\tfor_each_possible_cpu(cpu) {\n\t\tkvm_cpu_context_t *cpu_ctxt;\n\n\t\tcpu_ctxt = per_cpu_ptr(kvm_host_cpu_state, cpu);\n\t\terr = create_hyp_mappings(cpu_ctxt, cpu_ctxt + 1);\n\n\t\tif (err) {\n\t\t\tkvm_err(\"Cannot map host CPU state: %d\\n\", err);\n\t\t\tgoto out_free_context;\n\t\t}\n\t}\n\n\t/*\n\t * Execute the init code on each CPU.\n\t */\n\ton_each_cpu(cpu_init_hyp_mode, NULL, 1);\n\n\t/*\n\t * Init HYP view of VGIC\n\t */\n\terr = kvm_vgic_hyp_init();\n\tif (err)\n\t\tgoto out_free_context;\n\n#ifdef CONFIG_KVM_ARM_VGIC\n\t\tvgic_present = true;\n#endif\n\n\t/*\n\t * Init HYP architected timer support\n\t */\n\terr = kvm_timer_hyp_init();\n\tif (err)\n\t\tgoto out_free_mappings;\n\n#ifndef CONFIG_HOTPLUG_CPU\n\tfree_boot_hyp_pgd();\n#endif\n\n\tkvm_perf_init();\n\n\tkvm_info(\"Hyp mode initialized successfully\\n\");\n\n\treturn 0;\nout_free_context:\n\tfree_percpu(kvm_host_cpu_state);\nout_free_mappings:\n\tfree_hyp_pgds();\nout_free_stack_pages:\n\tfor_each_possible_cpu(cpu)\n\t\tfree_page(per_cpu(kvm_arm_hyp_stack_page, cpu));\nout_err:\n\tkvm_err(\"error initializing Hyp mode: %d\\n\", err);\n\treturn err;\n}\n\nstatic void check_kvm_target_cpu(void *ret)\n{\n\t*(int *)ret = kvm_target_cpu();\n}\n\n/**\n * Initialize Hyp-mode and memory mappings on all CPUs.\n */\nint kvm_arch_init(void *opaque)\n{\n\tint err;\n\tint ret, cpu;\n\n\tif (!is_hyp_mode_available()) {\n\t\tkvm_err(\"HYP mode not available\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tfor_each_online_cpu(cpu) {\n\t\tsmp_call_function_single(cpu, check_kvm_target_cpu, &ret, 1);\n\t\tif (ret < 0) {\n\t\t\tkvm_err(\"Error, CPU %d not supported!\\n\", cpu);\n\t\t\treturn -ENODEV;\n\t\t}\n\t}\n\n\terr = init_hyp_mode();\n\tif (err)\n\t\tgoto out_err;\n\n\terr = register_cpu_notifier(&hyp_init_cpu_nb);\n\tif (err) {\n\t\tkvm_err(\"Cannot register HYP init CPU notifier (%d)\\n\", err);\n\t\tgoto out_err;\n\t}\n\n\tkvm_coproc_table_init();\n\treturn 0;\nout_err:\n\treturn err;\n}\n\n/* NOP: Compiling as a module not supported */\nvoid kvm_arch_exit(void)\n{\n\tkvm_perf_teardown();\n}\n\nstatic int arm_init(void)\n{\n\tint rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);\n\treturn rc;\n}\n\nmodule_init(arm_init);\n"], "filenames": ["arch/arm/kvm/arm.c"], "buggy_code_start_loc": [494], "buggy_code_end_loc": [723], "fixing_code_start_loc": [495], "fixing_code_end_loc": [735], "type": "CWE-399", "message": "arch/arm/kvm/arm.c in the Linux kernel before 3.10 on the ARM platform, when KVM is used, allows host OS users to cause a denial of service (NULL pointer dereference, OOPS, and host OS crash) or possibly have unspecified other impact by omitting vCPU initialization before a KVM_GET_REG_LIST ioctl call.", "other": {"cve": {"id": "CVE-2013-5634", "sourceIdentifier": "secalert@redhat.com", "published": "2013-09-25T10:31:29.330", "lastModified": "2023-02-13T04:49:03.973", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "arch/arm/kvm/arm.c in the Linux kernel before 3.10 on the ARM platform, when KVM is used, allows host OS users to cause a denial of service (NULL pointer dereference, OOPS, and host OS crash) or possibly have unspecified other impact by omitting vCPU initialization before a KVM_GET_REG_LIST ioctl call."}, {"lang": "es", "value": "arch/arm/kvm/arm.c en el kernel de Linux anterior a v3.10  en la plataforma ARM, cuando KVM es utilizado, permite a los usuarios del sistema operativo anfitri\u00f3n provocar una denegaci\u00f3n de servicio (referencia a puntero nulo, OOPS y ca\u00edda del sistema operativo invitado) o posiblemente tener otro impacto no especificado mediante la omisi\u00f3n de la inicializaci\u00f3n vCPU anterior a la llamada KVM_GET_REG_LIST ioctl."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:A/AC:H/Au:S/C:N/I:N/A:C", "accessVector": "ADJACENT_NETWORK", "accessComplexity": "HIGH", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 2.5, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-399"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:arm64:*", "versionEndIncluding": "3.9.11", "matchCriteriaId": "7EB94BEB-9A72-41A5-8B5B-1D2211413B28"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.0:*:*:*:*:*:arm64:*", "matchCriteriaId": "59E39242-8051-4018-82BC-F7F2C52749E0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.1:*:*:*:*:*:arm64:*", "matchCriteriaId": "EDAADF21-6FBC-4AE4-88E6-912E2EF367E0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.2:*:*:*:*:*:arm64:*", "matchCriteriaId": "D4A5AB12-E57A-4D49-93EF-1BE2AC1A0FF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.3:*:*:*:*:*:arm64:*", "matchCriteriaId": "BD62DFE5-690C-4972-AA2E-B01BB7A9E16D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.4:*:*:*:*:*:arm64:*", "matchCriteriaId": "EF771D87-8608-4243-872D-2C735625B097"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.5:*:*:*:*:*:arm64:*", "matchCriteriaId": "F4442A5E-66A5-406F-AC9A-A37175D5A01B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.6:*:*:*:*:*:arm64:*", "matchCriteriaId": "667ED70A-AC2A-4060-86EF-71A71A85E514"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.7:*:*:*:*:*:arm64:*", "matchCriteriaId": "A002A628-663C-4923-8E03-F6EAF4878449"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.8:*:*:*:*:*:arm64:*", "matchCriteriaId": "880C2DC8-C9A7-4935-BCF1-0F0D62785067"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.9:*:*:*:*:*:arm64:*", "matchCriteriaId": "F98FBE36-A39B-4C1F-BC51-A0BCC8690708"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.9.10:*:*:*:*:*:arm64:*", "matchCriteriaId": "218E5271-9206-4D69-8C39-1CE4AB067FDD"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=e8180dcaa8470ceca21109f143876fdcd9fe050a", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2013/08/26/4", "source": "secalert@redhat.com"}, {"url": "http://www.securityfocus.com/bid/61995", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/e8180dcaa8470ceca21109f143876fdcd9fe050a", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v3.x/patch-3.10.bz2", "source": "secalert@redhat.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/e8180dcaa8470ceca21109f143876fdcd9fe050a"}}