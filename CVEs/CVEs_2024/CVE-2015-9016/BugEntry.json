{"buggy_code": ["/*\n * Functions to sequence FLUSH and FUA writes.\n *\n * Copyright (C) 2011\t\tMax Planck Institute for Gravitational Physics\n * Copyright (C) 2011\t\tTejun Heo <tj@kernel.org>\n *\n * This file is released under the GPLv2.\n *\n * REQ_{FLUSH|FUA} requests are decomposed to sequences consisted of three\n * optional steps - PREFLUSH, DATA and POSTFLUSH - according to the request\n * properties and hardware capability.\n *\n * If a request doesn't have data, only REQ_FLUSH makes sense, which\n * indicates a simple flush request.  If there is data, REQ_FLUSH indicates\n * that the device cache should be flushed before the data is executed, and\n * REQ_FUA means that the data must be on non-volatile media on request\n * completion.\n *\n * If the device doesn't have writeback cache, FLUSH and FUA don't make any\n * difference.  The requests are either completed immediately if there's no\n * data or executed as normal requests otherwise.\n *\n * If the device has writeback cache and supports FUA, REQ_FLUSH is\n * translated to PREFLUSH but REQ_FUA is passed down directly with DATA.\n *\n * If the device has writeback cache and doesn't support FUA, REQ_FLUSH is\n * translated to PREFLUSH and REQ_FUA to POSTFLUSH.\n *\n * The actual execution of flush is double buffered.  Whenever a request\n * needs to execute PRE or POSTFLUSH, it queues at\n * fq->flush_queue[fq->flush_pending_idx].  Once certain criteria are met, a\n * flush is issued and the pending_idx is toggled.  When the flush\n * completes, all the requests which were pending are proceeded to the next\n * step.  This allows arbitrary merging of different types of FLUSH/FUA\n * requests.\n *\n * Currently, the following conditions are used to determine when to issue\n * flush.\n *\n * C1. At any given time, only one flush shall be in progress.  This makes\n *     double buffering sufficient.\n *\n * C2. Flush is deferred if any request is executing DATA of its sequence.\n *     This avoids issuing separate POSTFLUSHes for requests which shared\n *     PREFLUSH.\n *\n * C3. The second condition is ignored if there is a request which has\n *     waited longer than FLUSH_PENDING_TIMEOUT.  This is to avoid\n *     starvation in the unlikely case where there are continuous stream of\n *     FUA (without FLUSH) requests.\n *\n * For devices which support FUA, it isn't clear whether C2 (and thus C3)\n * is beneficial.\n *\n * Note that a sequenced FLUSH/FUA request with DATA is completed twice.\n * Once while executing DATA and again after the whole sequence is\n * complete.  The first completion updates the contained bio but doesn't\n * finish it so that the bio submitter is notified only after the whole\n * sequence is complete.  This is implemented by testing REQ_FLUSH_SEQ in\n * req_bio_endio().\n *\n * The above peculiarity requires that each FLUSH/FUA request has only one\n * bio attached to it, which is guaranteed as they aren't allowed to be\n * merged in the usual way.\n */\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/gfp.h>\n#include <linux/blk-mq.h>\n\n#include \"blk.h\"\n#include \"blk-mq.h\"\n\n/* FLUSH/FUA sequences */\nenum {\n\tREQ_FSEQ_PREFLUSH\t= (1 << 0), /* pre-flushing in progress */\n\tREQ_FSEQ_DATA\t\t= (1 << 1), /* data write in progress */\n\tREQ_FSEQ_POSTFLUSH\t= (1 << 2), /* post-flushing in progress */\n\tREQ_FSEQ_DONE\t\t= (1 << 3),\n\n\tREQ_FSEQ_ACTIONS\t= REQ_FSEQ_PREFLUSH | REQ_FSEQ_DATA |\n\t\t\t\t  REQ_FSEQ_POSTFLUSH,\n\n\t/*\n\t * If flush has been pending longer than the following timeout,\n\t * it's issued even if flush_data requests are still in flight.\n\t */\n\tFLUSH_PENDING_TIMEOUT\t= 5 * HZ,\n};\n\nstatic bool blk_kick_flush(struct request_queue *q,\n\t\t\t   struct blk_flush_queue *fq);\n\nstatic unsigned int blk_flush_policy(unsigned int fflags, struct request *rq)\n{\n\tunsigned int policy = 0;\n\n\tif (blk_rq_sectors(rq))\n\t\tpolicy |= REQ_FSEQ_DATA;\n\n\tif (fflags & REQ_FLUSH) {\n\t\tif (rq->cmd_flags & REQ_FLUSH)\n\t\t\tpolicy |= REQ_FSEQ_PREFLUSH;\n\t\tif (!(fflags & REQ_FUA) && (rq->cmd_flags & REQ_FUA))\n\t\t\tpolicy |= REQ_FSEQ_POSTFLUSH;\n\t}\n\treturn policy;\n}\n\nstatic unsigned int blk_flush_cur_seq(struct request *rq)\n{\n\treturn 1 << ffz(rq->flush.seq);\n}\n\nstatic void blk_flush_restore_request(struct request *rq)\n{\n\t/*\n\t * After flush data completion, @rq->bio is %NULL but we need to\n\t * complete the bio again.  @rq->biotail is guaranteed to equal the\n\t * original @rq->bio.  Restore it.\n\t */\n\trq->bio = rq->biotail;\n\n\t/* make @rq a normal request */\n\trq->cmd_flags &= ~REQ_FLUSH_SEQ;\n\trq->end_io = rq->flush.saved_end_io;\n}\n\nstatic bool blk_flush_queue_rq(struct request *rq, bool add_front)\n{\n\tif (rq->q->mq_ops) {\n\t\tstruct request_queue *q = rq->q;\n\n\t\tblk_mq_add_to_requeue_list(rq, add_front);\n\t\tblk_mq_kick_requeue_list(q);\n\t\treturn false;\n\t} else {\n\t\tif (add_front)\n\t\t\tlist_add(&rq->queuelist, &rq->q->queue_head);\n\t\telse\n\t\t\tlist_add_tail(&rq->queuelist, &rq->q->queue_head);\n\t\treturn true;\n\t}\n}\n\n/**\n * blk_flush_complete_seq - complete flush sequence\n * @rq: FLUSH/FUA request being sequenced\n * @fq: flush queue\n * @seq: sequences to complete (mask of %REQ_FSEQ_*, can be zero)\n * @error: whether an error occurred\n *\n * @rq just completed @seq part of its flush sequence, record the\n * completion and trigger the next step.\n *\n * CONTEXT:\n * spin_lock_irq(q->queue_lock or fq->mq_flush_lock)\n *\n * RETURNS:\n * %true if requests were added to the dispatch queue, %false otherwise.\n */\nstatic bool blk_flush_complete_seq(struct request *rq,\n\t\t\t\t   struct blk_flush_queue *fq,\n\t\t\t\t   unsigned int seq, int error)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tbool queued = false, kicked;\n\n\tBUG_ON(rq->flush.seq & seq);\n\trq->flush.seq |= seq;\n\n\tif (likely(!error))\n\t\tseq = blk_flush_cur_seq(rq);\n\telse\n\t\tseq = REQ_FSEQ_DONE;\n\n\tswitch (seq) {\n\tcase REQ_FSEQ_PREFLUSH:\n\tcase REQ_FSEQ_POSTFLUSH:\n\t\t/* queue for flush */\n\t\tif (list_empty(pending))\n\t\t\tfq->flush_pending_since = jiffies;\n\t\tlist_move_tail(&rq->flush.list, pending);\n\t\tbreak;\n\n\tcase REQ_FSEQ_DATA:\n\t\tlist_move_tail(&rq->flush.list, &fq->flush_data_in_flight);\n\t\tqueued = blk_flush_queue_rq(rq, true);\n\t\tbreak;\n\n\tcase REQ_FSEQ_DONE:\n\t\t/*\n\t\t * @rq was previously adjusted by blk_flush_issue() for\n\t\t * flush sequencing and may already have gone through the\n\t\t * flush data request completion path.  Restore @rq for\n\t\t * normal completion and end it.\n\t\t */\n\t\tBUG_ON(!list_empty(&rq->queuelist));\n\t\tlist_del_init(&rq->flush.list);\n\t\tblk_flush_restore_request(rq);\n\t\tif (q->mq_ops)\n\t\t\tblk_mq_end_request(rq, error);\n\t\telse\n\t\t\t__blk_end_request_all(rq, error);\n\t\tbreak;\n\n\tdefault:\n\t\tBUG();\n\t}\n\n\tkicked = blk_kick_flush(q, fq);\n\treturn kicked | queued;\n}\n\nstatic void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}\n\n/**\n * blk_kick_flush - consider issuing flush request\n * @q: request_queue being kicked\n * @fq: flush queue\n *\n * Flush related states of @q have changed, consider issuing flush request.\n * Please read the comment at the top of this file for more info.\n *\n * CONTEXT:\n * spin_lock_irq(q->queue_lock or fq->mq_flush_lock)\n *\n * RETURNS:\n * %true if flush was issued, %false otherwise.\n */\nstatic bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time.\n\t */\n\tif (q->mq_ops) {\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}\n\nstatic void flush_data_end_io(struct request *rq, int error)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, NULL);\n\n\t/*\n\t * After populating an empty queue, kick it to avoid stall.  Read\n\t * the comment in flush_end_io().\n\t */\n\tif (blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error))\n\t\tblk_run_queue_async(q);\n}\n\nstatic void mq_flush_data_end_io(struct request *rq, int error)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\tunsigned long flags;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, ctx);\n\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\n\t/*\n\t * After populating an empty queue, kick it to avoid stall.  Read\n\t * the comment in flush_end_io().\n\t */\n\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\tif (blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error))\n\t\tblk_mq_run_hw_queue(hctx, true);\n\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}\n\n/**\n * blk_insert_flush - insert a new FLUSH/FUA request\n * @rq: request to insert\n *\n * To be called from __elv_add_request() for %ELEVATOR_INSERT_FLUSH insertions.\n * or __blk_mq_run_hw_queue() to dispatch request.\n * @rq is being submitted.  Analyze what needs to be done and put it on the\n * right queue.\n *\n * CONTEXT:\n * spin_lock_irq(q->queue_lock) in !mq case\n */\nvoid blk_insert_flush(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned int fflags = q->flush_flags;\t/* may change, cache */\n\tunsigned int policy = blk_flush_policy(fflags, rq);\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);\n\n\t/*\n\t * @policy now records what operations need to be done.  Adjust\n\t * REQ_FLUSH and FUA for the driver.\n\t */\n\trq->cmd_flags &= ~REQ_FLUSH;\n\tif (!(fflags & REQ_FUA))\n\t\trq->cmd_flags &= ~REQ_FUA;\n\n\t/*\n\t * An empty flush handed down from a stacking driver may\n\t * translate into nothing if the underlying device does not\n\t * advertise a write-back cache.  In this case, simply\n\t * complete the request.\n\t */\n\tif (!policy) {\n\t\tif (q->mq_ops)\n\t\t\tblk_mq_end_request(rq, 0);\n\t\telse\n\t\t\t__blk_end_bidi_request(rq, 0, 0, 0);\n\t\treturn;\n\t}\n\n\tBUG_ON(rq->bio != rq->biotail); /*assumes zero or single bio rq */\n\n\t/*\n\t * If there's data but flush is not necessary, the request can be\n\t * processed directly without going through flush machinery.  Queue\n\t * for normal execution.\n\t */\n\tif ((policy & REQ_FSEQ_DATA) &&\n\t    !(policy & (REQ_FSEQ_PREFLUSH | REQ_FSEQ_POSTFLUSH))) {\n\t\tif (q->mq_ops) {\n\t\t\tblk_mq_insert_request(rq, false, false, true);\n\t\t} else\n\t\t\tlist_add_tail(&rq->queuelist, &q->queue_head);\n\t\treturn;\n\t}\n\n\t/*\n\t * @rq should go through flush machinery.  Mark it part of flush\n\t * sequence and submit for further processing.\n\t */\n\tmemset(&rq->flush, 0, sizeof(rq->flush));\n\tINIT_LIST_HEAD(&rq->flush.list);\n\trq->cmd_flags |= REQ_FLUSH_SEQ;\n\trq->flush.saved_end_io = rq->end_io; /* Usually NULL */\n\tif (q->mq_ops) {\n\t\trq->end_io = mq_flush_data_end_io;\n\n\t\tspin_lock_irq(&fq->mq_flush_lock);\n\t\tblk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);\n\t\tspin_unlock_irq(&fq->mq_flush_lock);\n\t\treturn;\n\t}\n\trq->end_io = flush_data_end_io;\n\n\tblk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);\n}\n\n/**\n * blkdev_issue_flush - queue a flush\n * @bdev:\tblockdev to issue flush for\n * @gfp_mask:\tmemory allocation flags (for bio_alloc)\n * @error_sector:\terror sector\n *\n * Description:\n *    Issue a flush for the block device in question. Caller can supply\n *    room for storing the error offset in case of a flush error, if they\n *    wish to. If WAIT flag is not passed then caller may check only what\n *    request was pushed in some internal queue for later handling.\n */\nint blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,\n\t\tsector_t *error_sector)\n{\n\tstruct request_queue *q;\n\tstruct bio *bio;\n\tint ret = 0;\n\n\tif (bdev->bd_disk == NULL)\n\t\treturn -ENXIO;\n\n\tq = bdev_get_queue(bdev);\n\tif (!q)\n\t\treturn -ENXIO;\n\n\t/*\n\t * some block devices may not have their queue correctly set up here\n\t * (e.g. loop device without a backing file) and so issuing a flush\n\t * here will panic. Ensure there is a request function before issuing\n\t * the flush.\n\t */\n\tif (!q->make_request_fn)\n\t\treturn -ENXIO;\n\n\tbio = bio_alloc(gfp_mask, 0);\n\tbio->bi_bdev = bdev;\n\n\tret = submit_bio_wait(WRITE_FLUSH, bio);\n\n\t/*\n\t * The driver must store the error location in ->bi_sector, if\n\t * it supports it. For non-stacked drivers, this should be\n\t * copied from blk_rq_pos(rq).\n\t */\n\tif (error_sector)\n\t\t*error_sector = bio->bi_iter.bi_sector;\n\n\tbio_put(bio);\n\treturn ret;\n}\nEXPORT_SYMBOL(blkdev_issue_flush);\n\nstruct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,\n\t\tint node, int cmd_size)\n{\n\tstruct blk_flush_queue *fq;\n\tint rq_sz = sizeof(struct request);\n\n\tfq = kzalloc_node(sizeof(*fq), GFP_KERNEL, node);\n\tif (!fq)\n\t\tgoto fail;\n\n\tif (q->mq_ops) {\n\t\tspin_lock_init(&fq->mq_flush_lock);\n\t\trq_sz = round_up(rq_sz + cmd_size, cache_line_size());\n\t}\n\n\tfq->flush_rq = kzalloc_node(rq_sz, GFP_KERNEL, node);\n\tif (!fq->flush_rq)\n\t\tgoto fail_rq;\n\n\tINIT_LIST_HEAD(&fq->flush_queue[0]);\n\tINIT_LIST_HEAD(&fq->flush_queue[1]);\n\tINIT_LIST_HEAD(&fq->flush_data_in_flight);\n\n\treturn fq;\n\n fail_rq:\n\tkfree(fq);\n fail:\n\treturn NULL;\n}\n\nvoid blk_free_flush_queue(struct blk_flush_queue *fq)\n{\n\t/* bio based request queue hasn't flush queue */\n\tif (!fq)\n\t\treturn;\n\n\tkfree(fq->flush_rq);\n\tkfree(fq);\n}\n", "/*\n * Fast and scalable bitmap tagging variant. Uses sparser bitmaps spread\n * over multiple cachelines to avoid ping-pong between multiple submitters\n * or submitter and completer. Uses rolling wakeups to avoid falling of\n * the scaling cliff when we run out of tags and have to start putting\n * submitters to sleep.\n *\n * Uses active queue tracking to support fairer distribution of tags\n * between multiple submitters when a shared tag map is used.\n *\n * Copyright (C) 2013-2014 Jens Axboe\n */\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/random.h>\n\n#include <linux/blk-mq.h>\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-tag.h\"\n\nstatic bool bt_has_free_tags(struct blk_mq_bitmap_tags *bt)\n{\n\tint i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\t\tint ret;\n\n\t\tret = find_first_zero_bit(&bm->word, bm->depth);\n\t\tif (ret < bm->depth)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nbool blk_mq_has_free_tags(struct blk_mq_tags *tags)\n{\n\tif (!tags)\n\t\treturn true;\n\n\treturn bt_has_free_tags(&tags->bitmap_tags);\n}\n\nstatic inline int bt_index_inc(int index)\n{\n\treturn (index + 1) & (BT_WAIT_QUEUES - 1);\n}\n\nstatic inline void bt_index_atomic_inc(atomic_t *index)\n{\n\tint old = atomic_read(index);\n\tint new = bt_index_inc(old);\n\tatomic_cmpxchg(index, old, new);\n}\n\n/*\n * If a previously inactive queue goes active, bump the active user count.\n */\nbool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)\n{\n\tif (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&\n\t    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\n\t\tatomic_inc(&hctx->tags->active_queues);\n\n\treturn true;\n}\n\n/*\n * Wakeup all potentially sleeping on tags\n */\nvoid blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)\n{\n\tstruct blk_mq_bitmap_tags *bt;\n\tint i, wake_index;\n\n\tbt = &tags->bitmap_tags;\n\twake_index = atomic_read(&bt->wake_index);\n\tfor (i = 0; i < BT_WAIT_QUEUES; i++) {\n\t\tstruct bt_wait_state *bs = &bt->bs[wake_index];\n\n\t\tif (waitqueue_active(&bs->wait))\n\t\t\twake_up(&bs->wait);\n\n\t\twake_index = bt_index_inc(wake_index);\n\t}\n\n\tif (include_reserve) {\n\t\tbt = &tags->breserved_tags;\n\t\tif (waitqueue_active(&bt->bs[0].wait))\n\t\t\twake_up(&bt->bs[0].wait);\n\t}\n}\n\n/*\n * If a previously busy queue goes inactive, potential waiters could now\n * be allowed to queue. Wake them up and check.\n */\nvoid __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct blk_mq_tags *tags = hctx->tags;\n\n\tif (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\n\t\treturn;\n\n\tatomic_dec(&tags->active_queues);\n\n\tblk_mq_tag_wakeup_all(tags, false);\n}\n\n/*\n * For shared tag users, we track the number of currently active users\n * and attempt to provide a fair share of the tag depth for each of them.\n */\nstatic inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t  struct blk_mq_bitmap_tags *bt)\n{\n\tunsigned int depth, users;\n\n\tif (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))\n\t\treturn true;\n\tif (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\n\t\treturn true;\n\n\t/*\n\t * Don't try dividing an ant\n\t */\n\tif (bt->depth == 1)\n\t\treturn true;\n\n\tusers = atomic_read(&hctx->tags->active_queues);\n\tif (!users)\n\t\treturn true;\n\n\t/*\n\t * Allow at least some tags\n\t */\n\tdepth = max((bt->depth + users - 1) / users, 4U);\n\treturn atomic_read(&hctx->nr_active) < depth;\n}\n\nstatic int __bt_get_word(struct blk_align_bitmap *bm, unsigned int last_tag,\n\t\t\t bool nowrap)\n{\n\tint tag, org_last_tag = last_tag;\n\n\twhile (1) {\n\t\ttag = find_next_zero_bit(&bm->word, bm->depth, last_tag);\n\t\tif (unlikely(tag >= bm->depth)) {\n\t\t\t/*\n\t\t\t * We started with an offset, and we didn't reset the\n\t\t\t * offset to 0 in a failure case, so start from 0 to\n\t\t\t * exhaust the map.\n\t\t\t */\n\t\t\tif (org_last_tag && last_tag && !nowrap) {\n\t\t\t\tlast_tag = org_last_tag = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (!test_and_set_bit(tag, &bm->word))\n\t\t\tbreak;\n\n\t\tlast_tag = tag + 1;\n\t\tif (last_tag >= bm->depth - 1)\n\t\t\tlast_tag = 0;\n\t}\n\n\treturn tag;\n}\n\n#define BT_ALLOC_RR(tags) (tags->alloc_policy == BLK_TAG_ALLOC_RR)\n\n/*\n * Straight forward bitmap tag implementation, where each bit is a tag\n * (cleared == free, and set == busy). The small twist is using per-cpu\n * last_tag caches, which blk-mq stores in the blk_mq_ctx software queue\n * contexts. This enables us to drastically limit the space searched,\n * without dirtying an extra shared cacheline like we would if we stored\n * the cache value inside the shared blk_mq_bitmap_tags structure. On top\n * of that, each word of tags is in a separate cacheline. This means that\n * multiple users will tend to stick to different cachelines, at least\n * until the map is exhausted.\n */\nstatic int __bt_get(struct blk_mq_hw_ctx *hctx, struct blk_mq_bitmap_tags *bt,\n\t\t    unsigned int *tag_cache, struct blk_mq_tags *tags)\n{\n\tunsigned int last_tag, org_last_tag;\n\tint index, i, tag;\n\n\tif (!hctx_may_queue(hctx, bt))\n\t\treturn -1;\n\n\tlast_tag = org_last_tag = *tag_cache;\n\tindex = TAG_TO_INDEX(bt, last_tag);\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\ttag = __bt_get_word(&bt->map[index], TAG_TO_BIT(bt, last_tag),\n\t\t\t\t    BT_ALLOC_RR(tags));\n\t\tif (tag != -1) {\n\t\t\ttag += (index << bt->bits_per_word);\n\t\t\tgoto done;\n\t\t}\n\n\t\t/*\n\t\t * Jump to next index, and reset the last tag to be the\n\t\t * first tag of that index\n\t\t */\n\t\tindex++;\n\t\tlast_tag = (index << bt->bits_per_word);\n\n\t\tif (index >= bt->map_nr) {\n\t\t\tindex = 0;\n\t\t\tlast_tag = 0;\n\t\t}\n\t}\n\n\t*tag_cache = 0;\n\treturn -1;\n\n\t/*\n\t * Only update the cache from the allocation path, if we ended\n\t * up using the specific cached tag.\n\t */\ndone:\n\tif (tag == org_last_tag || unlikely(BT_ALLOC_RR(tags))) {\n\t\tlast_tag = tag + 1;\n\t\tif (last_tag >= bt->depth - 1)\n\t\t\tlast_tag = 0;\n\n\t\t*tag_cache = last_tag;\n\t}\n\n\treturn tag;\n}\n\nstatic struct bt_wait_state *bt_wait_ptr(struct blk_mq_bitmap_tags *bt,\n\t\t\t\t\t struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bt_wait_state *bs;\n\tint wait_index;\n\n\tif (!hctx)\n\t\treturn &bt->bs[0];\n\n\twait_index = atomic_read(&hctx->wait_index);\n\tbs = &bt->bs[wait_index];\n\tbt_index_atomic_inc(&hctx->wait_index);\n\treturn bs;\n}\n\nstatic int bt_get(struct blk_mq_alloc_data *data,\n\t\tstruct blk_mq_bitmap_tags *bt,\n\t\tstruct blk_mq_hw_ctx *hctx,\n\t\tunsigned int *last_tag, struct blk_mq_tags *tags)\n{\n\tstruct bt_wait_state *bs;\n\tDEFINE_WAIT(wait);\n\tint tag;\n\n\ttag = __bt_get(hctx, bt, last_tag, tags);\n\tif (tag != -1)\n\t\treturn tag;\n\n\tif (!(data->gfp & __GFP_WAIT))\n\t\treturn -1;\n\n\tbs = bt_wait_ptr(bt, hctx);\n\tdo {\n\t\tprepare_to_wait(&bs->wait, &wait, TASK_UNINTERRUPTIBLE);\n\n\t\ttag = __bt_get(hctx, bt, last_tag, tags);\n\t\tif (tag != -1)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * We're out of tags on this hardware queue, kick any\n\t\t * pending IO submits before going to sleep waiting for\n\t\t * some to complete. Note that hctx can be NULL here for\n\t\t * reserved tag allocation.\n\t\t */\n\t\tif (hctx)\n\t\t\tblk_mq_run_hw_queue(hctx, false);\n\n\t\t/*\n\t\t * Retry tag allocation after running the hardware queue,\n\t\t * as running the queue may also have found completions.\n\t\t */\n\t\ttag = __bt_get(hctx, bt, last_tag, tags);\n\t\tif (tag != -1)\n\t\t\tbreak;\n\n\t\tblk_mq_put_ctx(data->ctx);\n\n\t\tio_schedule();\n\n\t\tdata->ctx = blk_mq_get_ctx(data->q);\n\t\tdata->hctx = data->q->mq_ops->map_queue(data->q,\n\t\t\t\tdata->ctx->cpu);\n\t\tif (data->reserved) {\n\t\t\tbt = &data->hctx->tags->breserved_tags;\n\t\t} else {\n\t\t\tlast_tag = &data->ctx->last_tag;\n\t\t\thctx = data->hctx;\n\t\t\tbt = &hctx->tags->bitmap_tags;\n\t\t}\n\t\tfinish_wait(&bs->wait, &wait);\n\t\tbs = bt_wait_ptr(bt, hctx);\n\t} while (1);\n\n\tfinish_wait(&bs->wait, &wait);\n\treturn tag;\n}\n\nstatic unsigned int __blk_mq_get_tag(struct blk_mq_alloc_data *data)\n{\n\tint tag;\n\n\ttag = bt_get(data, &data->hctx->tags->bitmap_tags, data->hctx,\n\t\t\t&data->ctx->last_tag, data->hctx->tags);\n\tif (tag >= 0)\n\t\treturn tag + data->hctx->tags->nr_reserved_tags;\n\n\treturn BLK_MQ_TAG_FAIL;\n}\n\nstatic unsigned int __blk_mq_get_reserved_tag(struct blk_mq_alloc_data *data)\n{\n\tint tag, zero = 0;\n\n\tif (unlikely(!data->hctx->tags->nr_reserved_tags)) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn BLK_MQ_TAG_FAIL;\n\t}\n\n\ttag = bt_get(data, &data->hctx->tags->breserved_tags, NULL, &zero,\n\t\tdata->hctx->tags);\n\tif (tag < 0)\n\t\treturn BLK_MQ_TAG_FAIL;\n\n\treturn tag;\n}\n\nunsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)\n{\n\tif (!data->reserved)\n\t\treturn __blk_mq_get_tag(data);\n\n\treturn __blk_mq_get_reserved_tag(data);\n}\n\nstatic struct bt_wait_state *bt_wake_ptr(struct blk_mq_bitmap_tags *bt)\n{\n\tint i, wake_index;\n\n\twake_index = atomic_read(&bt->wake_index);\n\tfor (i = 0; i < BT_WAIT_QUEUES; i++) {\n\t\tstruct bt_wait_state *bs = &bt->bs[wake_index];\n\n\t\tif (waitqueue_active(&bs->wait)) {\n\t\t\tint o = atomic_read(&bt->wake_index);\n\t\t\tif (wake_index != o)\n\t\t\t\tatomic_cmpxchg(&bt->wake_index, o, wake_index);\n\n\t\t\treturn bs;\n\t\t}\n\n\t\twake_index = bt_index_inc(wake_index);\n\t}\n\n\treturn NULL;\n}\n\nstatic void bt_clear_tag(struct blk_mq_bitmap_tags *bt, unsigned int tag)\n{\n\tconst int index = TAG_TO_INDEX(bt, tag);\n\tstruct bt_wait_state *bs;\n\tint wait_cnt;\n\n\tclear_bit(TAG_TO_BIT(bt, tag), &bt->map[index].word);\n\n\t/* Ensure that the wait list checks occur after clear_bit(). */\n\tsmp_mb();\n\n\tbs = bt_wake_ptr(bt);\n\tif (!bs)\n\t\treturn;\n\n\twait_cnt = atomic_dec_return(&bs->wait_cnt);\n\tif (unlikely(wait_cnt < 0))\n\t\twait_cnt = atomic_inc_return(&bs->wait_cnt);\n\tif (wait_cnt == 0) {\n\t\tatomic_add(bt->wake_cnt, &bs->wait_cnt);\n\t\tbt_index_atomic_inc(&bt->wake_index);\n\t\twake_up(&bs->wait);\n\t}\n}\n\nvoid blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag,\n\t\t    unsigned int *last_tag)\n{\n\tstruct blk_mq_tags *tags = hctx->tags;\n\n\tif (tag >= tags->nr_reserved_tags) {\n\t\tconst int real_tag = tag - tags->nr_reserved_tags;\n\n\t\tBUG_ON(real_tag >= tags->nr_tags);\n\t\tbt_clear_tag(&tags->bitmap_tags, real_tag);\n\t\tif (likely(tags->alloc_policy == BLK_TAG_ALLOC_FIFO))\n\t\t\t*last_tag = real_tag;\n\t} else {\n\t\tBUG_ON(tag >= tags->nr_reserved_tags);\n\t\tbt_clear_tag(&tags->breserved_tags, tag);\n\t}\n}\n\nstatic void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n\nstatic void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n\nvoid blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,\n\t\tvoid *priv)\n{\n\tif (tags->nr_reserved_tags)\n\t\tbt_tags_for_each(tags, &tags->breserved_tags, 0, fn, priv, true);\n\tbt_tags_for_each(tags, &tags->bitmap_tags, tags->nr_reserved_tags, fn, priv,\n\t\t\tfalse);\n}\nEXPORT_SYMBOL(blk_mq_all_tag_busy_iter);\n\nvoid blk_mq_tag_busy_iter(struct blk_mq_hw_ctx *hctx, busy_iter_fn *fn,\n\t\tvoid *priv)\n{\n\tstruct blk_mq_tags *tags = hctx->tags;\n\n\tif (tags->nr_reserved_tags)\n\t\tbt_for_each(hctx, &tags->breserved_tags, 0, fn, priv, true);\n\tbt_for_each(hctx, &tags->bitmap_tags, tags->nr_reserved_tags, fn, priv,\n\t\t\tfalse);\n}\nEXPORT_SYMBOL(blk_mq_tag_busy_iter);\n\nstatic unsigned int bt_unused_tags(struct blk_mq_bitmap_tags *bt)\n{\n\tunsigned int i, used;\n\n\tfor (i = 0, used = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tused += bitmap_weight(&bm->word, bm->depth);\n\t}\n\n\treturn bt->depth - used;\n}\n\nstatic void bt_update_count(struct blk_mq_bitmap_tags *bt,\n\t\t\t    unsigned int depth)\n{\n\tunsigned int tags_per_word = 1U << bt->bits_per_word;\n\tunsigned int map_depth = depth;\n\n\tif (depth) {\n\t\tint i;\n\n\t\tfor (i = 0; i < bt->map_nr; i++) {\n\t\t\tbt->map[i].depth = min(map_depth, tags_per_word);\n\t\t\tmap_depth -= bt->map[i].depth;\n\t\t}\n\t}\n\n\tbt->wake_cnt = BT_WAIT_BATCH;\n\tif (bt->wake_cnt > depth / BT_WAIT_QUEUES)\n\t\tbt->wake_cnt = max(1U, depth / BT_WAIT_QUEUES);\n\n\tbt->depth = depth;\n}\n\nstatic int bt_alloc(struct blk_mq_bitmap_tags *bt, unsigned int depth,\n\t\t\tint node, bool reserved)\n{\n\tint i;\n\n\tbt->bits_per_word = ilog2(BITS_PER_LONG);\n\n\t/*\n\t * Depth can be zero for reserved tags, that's not a failure\n\t * condition.\n\t */\n\tif (depth) {\n\t\tunsigned int nr, tags_per_word;\n\n\t\ttags_per_word = (1 << bt->bits_per_word);\n\n\t\t/*\n\t\t * If the tag space is small, shrink the number of tags\n\t\t * per word so we spread over a few cachelines, at least.\n\t\t * If less than 4 tags, just forget about it, it's not\n\t\t * going to work optimally anyway.\n\t\t */\n\t\tif (depth >= 4) {\n\t\t\twhile (tags_per_word * 4 > depth) {\n\t\t\t\tbt->bits_per_word--;\n\t\t\t\ttags_per_word = (1 << bt->bits_per_word);\n\t\t\t}\n\t\t}\n\n\t\tnr = ALIGN(depth, tags_per_word) / tags_per_word;\n\t\tbt->map = kzalloc_node(nr * sizeof(struct blk_align_bitmap),\n\t\t\t\t\t\tGFP_KERNEL, node);\n\t\tif (!bt->map)\n\t\t\treturn -ENOMEM;\n\n\t\tbt->map_nr = nr;\n\t}\n\n\tbt->bs = kzalloc(BT_WAIT_QUEUES * sizeof(*bt->bs), GFP_KERNEL);\n\tif (!bt->bs) {\n\t\tkfree(bt->map);\n\t\tbt->map = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tbt_update_count(bt, depth);\n\n\tfor (i = 0; i < BT_WAIT_QUEUES; i++) {\n\t\tinit_waitqueue_head(&bt->bs[i].wait);\n\t\tatomic_set(&bt->bs[i].wait_cnt, bt->wake_cnt);\n\t}\n\n\treturn 0;\n}\n\nstatic void bt_free(struct blk_mq_bitmap_tags *bt)\n{\n\tkfree(bt->map);\n\tkfree(bt->bs);\n}\n\nstatic struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,\n\t\t\t\t\t\t   int node, int alloc_policy)\n{\n\tunsigned int depth = tags->nr_tags - tags->nr_reserved_tags;\n\n\ttags->alloc_policy = alloc_policy;\n\n\tif (bt_alloc(&tags->bitmap_tags, depth, node, false))\n\t\tgoto enomem;\n\tif (bt_alloc(&tags->breserved_tags, tags->nr_reserved_tags, node, true))\n\t\tgoto enomem;\n\n\treturn tags;\nenomem:\n\tbt_free(&tags->bitmap_tags);\n\tkfree(tags);\n\treturn NULL;\n}\n\nstruct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,\n\t\t\t\t     unsigned int reserved_tags,\n\t\t\t\t     int node, int alloc_policy)\n{\n\tstruct blk_mq_tags *tags;\n\n\tif (total_tags > BLK_MQ_TAG_MAX) {\n\t\tpr_err(\"blk-mq: tag depth too large\\n\");\n\t\treturn NULL;\n\t}\n\n\ttags = kzalloc_node(sizeof(*tags), GFP_KERNEL, node);\n\tif (!tags)\n\t\treturn NULL;\n\n\tif (!zalloc_cpumask_var(&tags->cpumask, GFP_KERNEL)) {\n\t\tkfree(tags);\n\t\treturn NULL;\n\t}\n\n\ttags->nr_tags = total_tags;\n\ttags->nr_reserved_tags = reserved_tags;\n\n\treturn blk_mq_init_bitmap_tags(tags, node, alloc_policy);\n}\n\nvoid blk_mq_free_tags(struct blk_mq_tags *tags)\n{\n\tbt_free(&tags->bitmap_tags);\n\tbt_free(&tags->breserved_tags);\n\tkfree(tags);\n}\n\nvoid blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *tag)\n{\n\tunsigned int depth = tags->nr_tags - tags->nr_reserved_tags;\n\n\t*tag = prandom_u32() % depth;\n}\n\nint blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int tdepth)\n{\n\ttdepth -= tags->nr_reserved_tags;\n\tif (tdepth > tags->nr_tags)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Don't need (or can't) update reserved tags here, they remain\n\t * static and should never need resizing.\n\t */\n\tbt_update_count(&tags->bitmap_tags, tdepth);\n\tblk_mq_tag_wakeup_all(tags, false);\n\treturn 0;\n}\n\n/**\n * blk_mq_unique_tag() - return a tag that is unique queue-wide\n * @rq: request for which to compute a unique tag\n *\n * The tag field in struct request is unique per hardware queue but not over\n * all hardware queues. Hence this function that returns a tag with the\n * hardware context index in the upper bits and the per hardware queue tag in\n * the lower bits.\n *\n * Note: When called for a request that is queued on a non-multiqueue request\n * queue, the hardware context index is set to zero.\n */\nu32 blk_mq_unique_tag(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_hw_ctx *hctx;\n\tint hwq = 0;\n\n\tif (q->mq_ops) {\n\t\thctx = q->mq_ops->map_queue(q, rq->mq_ctx->cpu);\n\t\thwq = hctx->queue_num;\n\t}\n\n\treturn (hwq << BLK_MQ_UNIQUE_TAG_BITS) |\n\t\t(rq->tag & BLK_MQ_UNIQUE_TAG_MASK);\n}\nEXPORT_SYMBOL(blk_mq_unique_tag);\n\nssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page)\n{\n\tchar *orig_page = page;\n\tunsigned int free, res;\n\n\tif (!tags)\n\t\treturn 0;\n\n\tpage += sprintf(page, \"nr_tags=%u, reserved_tags=%u, \"\n\t\t\t\"bits_per_word=%u\\n\",\n\t\t\ttags->nr_tags, tags->nr_reserved_tags,\n\t\t\ttags->bitmap_tags.bits_per_word);\n\n\tfree = bt_unused_tags(&tags->bitmap_tags);\n\tres = bt_unused_tags(&tags->breserved_tags);\n\n\tpage += sprintf(page, \"nr_free=%u, nr_reserved=%u\\n\", free, res);\n\tpage += sprintf(page, \"active_queues=%u\\n\", atomic_read(&tags->active_queues));\n\n\treturn page - orig_page;\n}\n", "#ifndef INT_BLK_MQ_TAG_H\n#define INT_BLK_MQ_TAG_H\n\n#include \"blk-mq.h\"\n\nenum {\n\tBT_WAIT_QUEUES\t= 8,\n\tBT_WAIT_BATCH\t= 8,\n};\n\nstruct bt_wait_state {\n\tatomic_t wait_cnt;\n\twait_queue_head_t wait;\n} ____cacheline_aligned_in_smp;\n\n#define TAG_TO_INDEX(bt, tag)\t((tag) >> (bt)->bits_per_word)\n#define TAG_TO_BIT(bt, tag)\t((tag) & ((1 << (bt)->bits_per_word) - 1))\n\nstruct blk_mq_bitmap_tags {\n\tunsigned int depth;\n\tunsigned int wake_cnt;\n\tunsigned int bits_per_word;\n\n\tunsigned int map_nr;\n\tstruct blk_align_bitmap *map;\n\n\tatomic_t wake_index;\n\tstruct bt_wait_state *bs;\n};\n\n/*\n * Tag address space map.\n */\nstruct blk_mq_tags {\n\tunsigned int nr_tags;\n\tunsigned int nr_reserved_tags;\n\n\tatomic_t active_queues;\n\n\tstruct blk_mq_bitmap_tags bitmap_tags;\n\tstruct blk_mq_bitmap_tags breserved_tags;\n\n\tstruct request **rqs;\n\tstruct list_head page_list;\n\n\tint alloc_policy;\n\tcpumask_var_t cpumask;\n};\n\n\nextern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int reserved_tags, int node, int alloc_policy);\nextern void blk_mq_free_tags(struct blk_mq_tags *tags);\n\nextern unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data);\nextern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag, unsigned int *last_tag);\nextern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);\nextern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);\nextern void blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *last_tag);\nextern int blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int depth);\nextern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);\n\nenum {\n\tBLK_MQ_TAG_CACHE_MIN\t= 1,\n\tBLK_MQ_TAG_CACHE_MAX\t= 64,\n};\n\nenum {\n\tBLK_MQ_TAG_FAIL\t\t= -1U,\n\tBLK_MQ_TAG_MIN\t\t= BLK_MQ_TAG_CACHE_MIN,\n\tBLK_MQ_TAG_MAX\t\t= BLK_MQ_TAG_FAIL - 1,\n};\n\nextern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);\nextern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);\n\nstatic inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)\n{\n\tif (!(hctx->flags & BLK_MQ_F_TAG_SHARED))\n\t\treturn false;\n\n\treturn __blk_mq_tag_busy(hctx);\n}\n\nstatic inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)\n{\n\tif (!(hctx->flags & BLK_MQ_F_TAG_SHARED))\n\t\treturn;\n\n\t__blk_mq_tag_idle(hctx);\n}\n\n#endif\n", "/*\n * Block multiqueue core code\n *\n * Copyright (C) 2013-2014 Jens Axboe\n * Copyright (C) 2013-2014 Christoph Hellwig\n */\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/backing-dev.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/mm.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/smp.h>\n#include <linux/llist.h>\n#include <linux/list_sort.h>\n#include <linux/cpu.h>\n#include <linux/cache.h>\n#include <linux/sched/sysctl.h>\n#include <linux/delay.h>\n#include <linux/crash_dump.h>\n\n#include <trace/events/block.h>\n\n#include <linux/blk-mq.h>\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-tag.h\"\n\nstatic DEFINE_MUTEX(all_q_mutex);\nstatic LIST_HEAD(all_q_list);\n\nstatic void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx);\n\n/*\n * Check if any of the ctx's have pending work in this hardware queue\n */\nstatic bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < hctx->ctx_map.size; i++)\n\t\tif (hctx->ctx_map.map[i].word)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline struct blk_align_bitmap *get_bm(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t      struct blk_mq_ctx *ctx)\n{\n\treturn &hctx->ctx_map.map[ctx->index_hw / hctx->ctx_map.bits_per_word];\n}\n\n#define CTX_TO_BIT(hctx, ctx)\t\\\n\t((ctx)->index_hw & ((hctx)->ctx_map.bits_per_word - 1))\n\n/*\n * Mark this ctx as having pending work in this hardware queue\n */\nstatic void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t     struct blk_mq_ctx *ctx)\n{\n\tstruct blk_align_bitmap *bm = get_bm(hctx, ctx);\n\n\tif (!test_bit(CTX_TO_BIT(hctx, ctx), &bm->word))\n\t\tset_bit(CTX_TO_BIT(hctx, ctx), &bm->word);\n}\n\nstatic void blk_mq_hctx_clear_pending(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t      struct blk_mq_ctx *ctx)\n{\n\tstruct blk_align_bitmap *bm = get_bm(hctx, ctx);\n\n\tclear_bit(CTX_TO_BIT(hctx, ctx), &bm->word);\n}\n\nstatic int blk_mq_queue_enter(struct request_queue *q, gfp_t gfp)\n{\n\twhile (true) {\n\t\tint ret;\n\n\t\tif (percpu_ref_tryget_live(&q->mq_usage_counter))\n\t\t\treturn 0;\n\n\t\tif (!(gfp & __GFP_WAIT))\n\t\t\treturn -EBUSY;\n\n\t\tret = wait_event_interruptible(q->mq_freeze_wq,\n\t\t\t\t!atomic_read(&q->mq_freeze_depth) ||\n\t\t\t\tblk_queue_dying(q));\n\t\tif (blk_queue_dying(q))\n\t\t\treturn -ENODEV;\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n}\n\nstatic void blk_mq_queue_exit(struct request_queue *q)\n{\n\tpercpu_ref_put(&q->mq_usage_counter);\n}\n\nstatic void blk_mq_usage_counter_release(struct percpu_ref *ref)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(ref, struct request_queue, mq_usage_counter);\n\n\twake_up_all(&q->mq_freeze_wq);\n}\n\nvoid blk_mq_freeze_queue_start(struct request_queue *q)\n{\n\tint freeze_depth;\n\n\tfreeze_depth = atomic_inc_return(&q->mq_freeze_depth);\n\tif (freeze_depth == 1) {\n\t\tpercpu_ref_kill(&q->mq_usage_counter);\n\t\tblk_mq_run_hw_queues(q, false);\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue_start);\n\nstatic void blk_mq_freeze_queue_wait(struct request_queue *q)\n{\n\twait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->mq_usage_counter));\n}\n\n/*\n * Guarantee no request is in use, so we can change any data structure of\n * the queue afterward.\n */\nvoid blk_mq_freeze_queue(struct request_queue *q)\n{\n\tblk_mq_freeze_queue_start(q);\n\tblk_mq_freeze_queue_wait(q);\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue);\n\nvoid blk_mq_unfreeze_queue(struct request_queue *q)\n{\n\tint freeze_depth;\n\n\tfreeze_depth = atomic_dec_return(&q->mq_freeze_depth);\n\tWARN_ON_ONCE(freeze_depth < 0);\n\tif (!freeze_depth) {\n\t\tpercpu_ref_reinit(&q->mq_usage_counter);\n\t\twake_up_all(&q->mq_freeze_wq);\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);\n\nvoid blk_mq_wake_waiters(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tif (blk_mq_hw_queue_mapped(hctx))\n\t\t\tblk_mq_tag_wakeup_all(hctx->tags, true);\n\n\t/*\n\t * If we are called because the queue has now been marked as\n\t * dying, we need to ensure that processes currently waiting on\n\t * the queue are notified as well.\n\t */\n\twake_up_all(&q->mq_freeze_wq);\n}\n\nbool blk_mq_can_queue(struct blk_mq_hw_ctx *hctx)\n{\n\treturn blk_mq_has_free_tags(hctx->tags);\n}\nEXPORT_SYMBOL(blk_mq_can_queue);\n\nstatic void blk_mq_rq_ctx_init(struct request_queue *q, struct blk_mq_ctx *ctx,\n\t\t\t       struct request *rq, unsigned int rw_flags)\n{\n\tif (blk_queue_io_stat(q))\n\t\trw_flags |= REQ_IO_STAT;\n\n\tINIT_LIST_HEAD(&rq->queuelist);\n\t/* csd/requeue_work/fifo_time is initialized before use */\n\trq->q = q;\n\trq->mq_ctx = ctx;\n\trq->cmd_flags |= rw_flags;\n\t/* do not touch atomic flags, it needs atomic ops against the timer */\n\trq->cpu = -1;\n\tINIT_HLIST_NODE(&rq->hash);\n\tRB_CLEAR_NODE(&rq->rb_node);\n\trq->rq_disk = NULL;\n\trq->part = NULL;\n\trq->start_time = jiffies;\n#ifdef CONFIG_BLK_CGROUP\n\trq->rl = NULL;\n\tset_start_time_ns(rq);\n\trq->io_start_time_ns = 0;\n#endif\n\trq->nr_phys_segments = 0;\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\trq->nr_integrity_segments = 0;\n#endif\n\trq->special = NULL;\n\t/* tag was already set */\n\trq->errors = 0;\n\n\trq->cmd = rq->__cmd;\n\n\trq->extra_len = 0;\n\trq->sense_len = 0;\n\trq->resid_len = 0;\n\trq->sense = NULL;\n\n\tINIT_LIST_HEAD(&rq->timeout_list);\n\trq->timeout = 0;\n\n\trq->end_io = NULL;\n\trq->end_io_data = NULL;\n\trq->next_rq = NULL;\n\n\tctx->rq_dispatched[rw_is_sync(rw_flags)]++;\n}\n\nstatic struct request *\n__blk_mq_alloc_request(struct blk_mq_alloc_data *data, int rw)\n{\n\tstruct request *rq;\n\tunsigned int tag;\n\n\ttag = blk_mq_get_tag(data);\n\tif (tag != BLK_MQ_TAG_FAIL) {\n\t\trq = data->hctx->tags->rqs[tag];\n\n\t\tif (blk_mq_tag_busy(data->hctx)) {\n\t\t\trq->cmd_flags = REQ_MQ_INFLIGHT;\n\t\t\tatomic_inc(&data->hctx->nr_active);\n\t\t}\n\n\t\trq->tag = tag;\n\t\tblk_mq_rq_ctx_init(data->q, data->ctx, rq, rw);\n\t\treturn rq;\n\t}\n\n\treturn NULL;\n}\n\nstruct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,\n\t\tbool reserved)\n{\n\tstruct blk_mq_ctx *ctx;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct request *rq;\n\tstruct blk_mq_alloc_data alloc_data;\n\tint ret;\n\n\tret = blk_mq_queue_enter(q, gfp);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tctx = blk_mq_get_ctx(q);\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\tblk_mq_set_alloc_data(&alloc_data, q, gfp & ~__GFP_WAIT,\n\t\t\treserved, ctx, hctx);\n\n\trq = __blk_mq_alloc_request(&alloc_data, rw);\n\tif (!rq && (gfp & __GFP_WAIT)) {\n\t\t__blk_mq_run_hw_queue(hctx);\n\t\tblk_mq_put_ctx(ctx);\n\n\t\tctx = blk_mq_get_ctx(q);\n\t\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\t\tblk_mq_set_alloc_data(&alloc_data, q, gfp, reserved, ctx,\n\t\t\t\thctx);\n\t\trq =  __blk_mq_alloc_request(&alloc_data, rw);\n\t\tctx = alloc_data.ctx;\n\t}\n\tblk_mq_put_ctx(ctx);\n\tif (!rq) {\n\t\tblk_mq_queue_exit(q);\n\t\treturn ERR_PTR(-EWOULDBLOCK);\n\t}\n\treturn rq;\n}\nEXPORT_SYMBOL(blk_mq_alloc_request);\n\nstatic void __blk_mq_free_request(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t  struct blk_mq_ctx *ctx, struct request *rq)\n{\n\tconst int tag = rq->tag;\n\tstruct request_queue *q = rq->q;\n\n\tif (rq->cmd_flags & REQ_MQ_INFLIGHT)\n\t\tatomic_dec(&hctx->nr_active);\n\trq->cmd_flags = 0;\n\n\tclear_bit(REQ_ATOM_STARTED, &rq->atomic_flags);\n\tblk_mq_put_tag(hctx, tag, &ctx->last_tag);\n\tblk_mq_queue_exit(q);\n}\n\nvoid blk_mq_free_hctx_request(struct blk_mq_hw_ctx *hctx, struct request *rq)\n{\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\n\tctx->rq_completed[rq_is_sync(rq)]++;\n\t__blk_mq_free_request(hctx, ctx, rq);\n\n}\nEXPORT_SYMBOL_GPL(blk_mq_free_hctx_request);\n\nvoid blk_mq_free_request(struct request *rq)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct request_queue *q = rq->q;\n\n\thctx = q->mq_ops->map_queue(q, rq->mq_ctx->cpu);\n\tblk_mq_free_hctx_request(hctx, rq);\n}\nEXPORT_SYMBOL_GPL(blk_mq_free_request);\n\ninline void __blk_mq_end_request(struct request *rq, int error)\n{\n\tblk_account_io_done(rq);\n\n\tif (rq->end_io) {\n\t\trq->end_io(rq, error);\n\t} else {\n\t\tif (unlikely(blk_bidi_rq(rq)))\n\t\t\tblk_mq_free_request(rq->next_rq);\n\t\tblk_mq_free_request(rq);\n\t}\n}\nEXPORT_SYMBOL(__blk_mq_end_request);\n\nvoid blk_mq_end_request(struct request *rq, int error)\n{\n\tif (blk_update_request(rq, error, blk_rq_bytes(rq)))\n\t\tBUG();\n\t__blk_mq_end_request(rq, error);\n}\nEXPORT_SYMBOL(blk_mq_end_request);\n\nstatic void __blk_mq_complete_request_remote(void *data)\n{\n\tstruct request *rq = data;\n\n\trq->q->softirq_done_fn(rq);\n}\n\nstatic void blk_mq_ipi_complete_request(struct request *rq)\n{\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\tbool shared = false;\n\tint cpu;\n\n\tif (!test_bit(QUEUE_FLAG_SAME_COMP, &rq->q->queue_flags)) {\n\t\trq->q->softirq_done_fn(rq);\n\t\treturn;\n\t}\n\n\tcpu = get_cpu();\n\tif (!test_bit(QUEUE_FLAG_SAME_FORCE, &rq->q->queue_flags))\n\t\tshared = cpus_share_cache(cpu, ctx->cpu);\n\n\tif (cpu != ctx->cpu && !shared && cpu_online(ctx->cpu)) {\n\t\trq->csd.func = __blk_mq_complete_request_remote;\n\t\trq->csd.info = rq;\n\t\trq->csd.flags = 0;\n\t\tsmp_call_function_single_async(ctx->cpu, &rq->csd);\n\t} else {\n\t\trq->q->softirq_done_fn(rq);\n\t}\n\tput_cpu();\n}\n\nvoid __blk_mq_complete_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\tif (!q->softirq_done_fn)\n\t\tblk_mq_end_request(rq, rq->errors);\n\telse\n\t\tblk_mq_ipi_complete_request(rq);\n}\n\n/**\n * blk_mq_complete_request - end I/O on a request\n * @rq:\t\tthe request being processed\n *\n * Description:\n *\tEnds all I/O on a request. It does not handle partial completions.\n *\tThe actual completion happens out-of-order, through a IPI handler.\n **/\nvoid blk_mq_complete_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\tif (unlikely(blk_should_fake_timeout(q)))\n\t\treturn;\n\tif (!blk_mark_rq_complete(rq))\n\t\t__blk_mq_complete_request(rq);\n}\nEXPORT_SYMBOL(blk_mq_complete_request);\n\nint blk_mq_request_started(struct request *rq)\n{\n\treturn test_bit(REQ_ATOM_STARTED, &rq->atomic_flags);\n}\nEXPORT_SYMBOL_GPL(blk_mq_request_started);\n\nvoid blk_mq_start_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\ttrace_block_rq_issue(q, rq);\n\n\trq->resid_len = blk_rq_bytes(rq);\n\tif (unlikely(blk_bidi_rq(rq)))\n\t\trq->next_rq->resid_len = blk_rq_bytes(rq->next_rq);\n\n\tblk_add_timer(rq);\n\n\t/*\n\t * Ensure that ->deadline is visible before set the started\n\t * flag and clear the completed flag.\n\t */\n\tsmp_mb__before_atomic();\n\n\t/*\n\t * Mark us as started and clear complete. Complete might have been\n\t * set if requeue raced with timeout, which then marked it as\n\t * complete. So be sure to clear complete again when we start\n\t * the request, otherwise we'll ignore the completion event.\n\t */\n\tif (!test_bit(REQ_ATOM_STARTED, &rq->atomic_flags))\n\t\tset_bit(REQ_ATOM_STARTED, &rq->atomic_flags);\n\tif (test_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags))\n\t\tclear_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags);\n\n\tif (q->dma_drain_size && blk_rq_bytes(rq)) {\n\t\t/*\n\t\t * Make sure space for the drain appears.  We know we can do\n\t\t * this because max_hw_segments has been adjusted to be one\n\t\t * fewer than the device can handle.\n\t\t */\n\t\trq->nr_phys_segments++;\n\t}\n}\nEXPORT_SYMBOL(blk_mq_start_request);\n\nstatic void __blk_mq_requeue_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\ttrace_block_rq_requeue(q, rq);\n\n\tif (test_and_clear_bit(REQ_ATOM_STARTED, &rq->atomic_flags)) {\n\t\tif (q->dma_drain_size && blk_rq_bytes(rq))\n\t\t\trq->nr_phys_segments--;\n\t}\n}\n\nvoid blk_mq_requeue_request(struct request *rq)\n{\n\t__blk_mq_requeue_request(rq);\n\n\tBUG_ON(blk_queued_rq(rq));\n\tblk_mq_add_to_requeue_list(rq, true);\n}\nEXPORT_SYMBOL(blk_mq_requeue_request);\n\nstatic void blk_mq_requeue_work(struct work_struct *work)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(work, struct request_queue, requeue_work);\n\tLIST_HEAD(rq_list);\n\tstruct request *rq, *next;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->requeue_lock, flags);\n\tlist_splice_init(&q->requeue_list, &rq_list);\n\tspin_unlock_irqrestore(&q->requeue_lock, flags);\n\n\tlist_for_each_entry_safe(rq, next, &rq_list, queuelist) {\n\t\tif (!(rq->cmd_flags & REQ_SOFTBARRIER))\n\t\t\tcontinue;\n\n\t\trq->cmd_flags &= ~REQ_SOFTBARRIER;\n\t\tlist_del_init(&rq->queuelist);\n\t\tblk_mq_insert_request(rq, true, false, false);\n\t}\n\n\twhile (!list_empty(&rq_list)) {\n\t\trq = list_entry(rq_list.next, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\tblk_mq_insert_request(rq, false, false, false);\n\t}\n\n\t/*\n\t * Use the start variant of queue running here, so that running\n\t * the requeue work will kick stopped queues.\n\t */\n\tblk_mq_start_hw_queues(q);\n}\n\nvoid blk_mq_add_to_requeue_list(struct request *rq, bool at_head)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned long flags;\n\n\t/*\n\t * We abuse this flag that is otherwise used by the I/O scheduler to\n\t * request head insertation from the workqueue.\n\t */\n\tBUG_ON(rq->cmd_flags & REQ_SOFTBARRIER);\n\n\tspin_lock_irqsave(&q->requeue_lock, flags);\n\tif (at_head) {\n\t\trq->cmd_flags |= REQ_SOFTBARRIER;\n\t\tlist_add(&rq->queuelist, &q->requeue_list);\n\t} else {\n\t\tlist_add_tail(&rq->queuelist, &q->requeue_list);\n\t}\n\tspin_unlock_irqrestore(&q->requeue_lock, flags);\n}\nEXPORT_SYMBOL(blk_mq_add_to_requeue_list);\n\nvoid blk_mq_cancel_requeue_work(struct request_queue *q)\n{\n\tcancel_work_sync(&q->requeue_work);\n}\nEXPORT_SYMBOL_GPL(blk_mq_cancel_requeue_work);\n\nvoid blk_mq_kick_requeue_list(struct request_queue *q)\n{\n\tkblockd_schedule_work(&q->requeue_work);\n}\nEXPORT_SYMBOL(blk_mq_kick_requeue_list);\n\nvoid blk_mq_abort_requeue_list(struct request_queue *q)\n{\n\tunsigned long flags;\n\tLIST_HEAD(rq_list);\n\n\tspin_lock_irqsave(&q->requeue_lock, flags);\n\tlist_splice_init(&q->requeue_list, &rq_list);\n\tspin_unlock_irqrestore(&q->requeue_lock, flags);\n\n\twhile (!list_empty(&rq_list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(&rq_list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\trq->errors = -EIO;\n\t\tblk_mq_end_request(rq, rq->errors);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_abort_requeue_list);\n\nstatic inline bool is_flush_request(struct request *rq,\n\t\tstruct blk_flush_queue *fq, unsigned int tag)\n{\n\treturn ((rq->cmd_flags & REQ_FLUSH_SEQ) &&\n\t\t\tfq->flush_rq->tag == tag);\n}\n\nstruct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct request *rq = tags->rqs[tag];\n\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n\n\tif (!is_flush_request(rq, fq, tag))\n\t\treturn rq;\n\n\treturn fq->flush_rq;\n}\nEXPORT_SYMBOL(blk_mq_tag_to_rq);\n\nstruct blk_mq_timeout_data {\n\tunsigned long next;\n\tunsigned int next_set;\n};\n\nvoid blk_mq_rq_timed_out(struct request *req, bool reserved)\n{\n\tstruct blk_mq_ops *ops = req->q->mq_ops;\n\tenum blk_eh_timer_return ret = BLK_EH_RESET_TIMER;\n\n\t/*\n\t * We know that complete is set at this point. If STARTED isn't set\n\t * anymore, then the request isn't active and the \"timeout\" should\n\t * just be ignored. This can happen due to the bitflag ordering.\n\t * Timeout first checks if STARTED is set, and if it is, assumes\n\t * the request is active. But if we race with completion, then\n\t * we both flags will get cleared. So check here again, and ignore\n\t * a timeout event with a request that isn't active.\n\t */\n\tif (!test_bit(REQ_ATOM_STARTED, &req->atomic_flags))\n\t\treturn;\n\n\tif (ops->timeout)\n\t\tret = ops->timeout(req, reserved);\n\n\tswitch (ret) {\n\tcase BLK_EH_HANDLED:\n\t\t__blk_mq_complete_request(req);\n\t\tbreak;\n\tcase BLK_EH_RESET_TIMER:\n\t\tblk_add_timer(req);\n\t\tblk_clear_rq_complete(req);\n\t\tbreak;\n\tcase BLK_EH_NOT_HANDLED:\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"block: bad eh return: %d\\n\", ret);\n\t\tbreak;\n\t}\n}\n\nstatic void blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,\n\t\tstruct request *rq, void *priv, bool reserved)\n{\n\tstruct blk_mq_timeout_data *data = priv;\n\n\tif (!test_bit(REQ_ATOM_STARTED, &rq->atomic_flags)) {\n\t\t/*\n\t\t * If a request wasn't started before the queue was\n\t\t * marked dying, kill it here or it'll go unnoticed.\n\t\t */\n\t\tif (unlikely(blk_queue_dying(rq->q))) {\n\t\t\trq->errors = -EIO;\n\t\t\tblk_mq_complete_request(rq);\n\t\t}\n\t\treturn;\n\t}\n\tif (rq->cmd_flags & REQ_NO_TIMEOUT)\n\t\treturn;\n\n\tif (time_after_eq(jiffies, rq->deadline)) {\n\t\tif (!blk_mark_rq_complete(rq))\n\t\t\tblk_mq_rq_timed_out(rq, reserved);\n\t} else if (!data->next_set || time_after(data->next, rq->deadline)) {\n\t\tdata->next = rq->deadline;\n\t\tdata->next_set = 1;\n\t}\n}\n\nstatic void blk_mq_rq_timer(unsigned long priv)\n{\n\tstruct request_queue *q = (struct request_queue *)priv;\n\tstruct blk_mq_timeout_data data = {\n\t\t.next\t\t= 0,\n\t\t.next_set\t= 0,\n\t};\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t/*\n\t\t * If not software queues are currently mapped to this\n\t\t * hardware queue, there's nothing to check\n\t\t */\n\t\tif (!blk_mq_hw_queue_mapped(hctx))\n\t\t\tcontinue;\n\n\t\tblk_mq_tag_busy_iter(hctx, blk_mq_check_expired, &data);\n\t}\n\n\tif (data.next_set) {\n\t\tdata.next = blk_rq_timeout(round_jiffies_up(data.next));\n\t\tmod_timer(&q->timeout, data.next);\n\t} else {\n\t\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t\t/* the hctx may be unmapped, so check it here */\n\t\t\tif (blk_mq_hw_queue_mapped(hctx))\n\t\t\t\tblk_mq_tag_idle(hctx);\n\t\t}\n\t}\n}\n\n/*\n * Reverse check our software queue for entries that we could potentially\n * merge with. Currently includes a hand-wavy stop count of 8, to not spend\n * too much time checking for merges.\n */\nstatic bool blk_mq_attempt_merge(struct request_queue *q,\n\t\t\t\t struct blk_mq_ctx *ctx, struct bio *bio)\n{\n\tstruct request *rq;\n\tint checked = 8;\n\n\tlist_for_each_entry_reverse(rq, &ctx->rq_list, queuelist) {\n\t\tint el_ret;\n\n\t\tif (!checked--)\n\t\t\tbreak;\n\n\t\tif (!blk_rq_merge_ok(rq, bio))\n\t\t\tcontinue;\n\n\t\tel_ret = blk_try_merge(rq, bio);\n\t\tif (el_ret == ELEVATOR_BACK_MERGE) {\n\t\t\tif (bio_attempt_back_merge(q, rq, bio)) {\n\t\t\t\tctx->rq_merged++;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tbreak;\n\t\t} else if (el_ret == ELEVATOR_FRONT_MERGE) {\n\t\t\tif (bio_attempt_front_merge(q, rq, bio)) {\n\t\t\t\tctx->rq_merged++;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn false;\n}\n\n/*\n * Process software queues that have been marked busy, splicing them\n * to the for-dispatch\n */\nstatic void flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)\n{\n\tstruct blk_mq_ctx *ctx;\n\tint i;\n\n\tfor (i = 0; i < hctx->ctx_map.size; i++) {\n\t\tstruct blk_align_bitmap *bm = &hctx->ctx_map.map[i];\n\t\tunsigned int off, bit;\n\n\t\tif (!bm->word)\n\t\t\tcontinue;\n\n\t\tbit = 0;\n\t\toff = i * hctx->ctx_map.bits_per_word;\n\t\tdo {\n\t\t\tbit = find_next_bit(&bm->word, bm->depth, bit);\n\t\t\tif (bit >= bm->depth)\n\t\t\t\tbreak;\n\n\t\t\tctx = hctx->ctxs[bit + off];\n\t\t\tclear_bit(bit, &bm->word);\n\t\t\tspin_lock(&ctx->lock);\n\t\t\tlist_splice_tail_init(&ctx->rq_list, list);\n\t\t\tspin_unlock(&ctx->lock);\n\n\t\t\tbit++;\n\t\t} while (1);\n\t}\n}\n\n/*\n * Run this hardware queue, pulling any software queues mapped to it in.\n * Note that this function currently has various problems around ordering\n * of IO. In particular, we'd like FIFO behaviour on handling existing\n * items on the hctx->dispatch list. Ignore that for now.\n */\nstatic void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct request *rq;\n\tLIST_HEAD(rq_list);\n\tLIST_HEAD(driver_list);\n\tstruct list_head *dptr;\n\tint queued;\n\n\tWARN_ON(!cpumask_test_cpu(raw_smp_processor_id(), hctx->cpumask));\n\n\tif (unlikely(test_bit(BLK_MQ_S_STOPPED, &hctx->state)))\n\t\treturn;\n\n\thctx->run++;\n\n\t/*\n\t * Touch any software queue that has pending entries.\n\t */\n\tflush_busy_ctxs(hctx, &rq_list);\n\n\t/*\n\t * If we have previous entries on our dispatch list, grab them\n\t * and stuff them at the front for more fair dispatch.\n\t */\n\tif (!list_empty_careful(&hctx->dispatch)) {\n\t\tspin_lock(&hctx->lock);\n\t\tif (!list_empty(&hctx->dispatch))\n\t\t\tlist_splice_init(&hctx->dispatch, &rq_list);\n\t\tspin_unlock(&hctx->lock);\n\t}\n\n\t/*\n\t * Start off with dptr being NULL, so we start the first request\n\t * immediately, even if we have more pending.\n\t */\n\tdptr = NULL;\n\n\t/*\n\t * Now process all the entries, sending them to the driver.\n\t */\n\tqueued = 0;\n\twhile (!list_empty(&rq_list)) {\n\t\tstruct blk_mq_queue_data bd;\n\t\tint ret;\n\n\t\trq = list_first_entry(&rq_list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\n\t\tbd.rq = rq;\n\t\tbd.list = dptr;\n\t\tbd.last = list_empty(&rq_list);\n\n\t\tret = q->mq_ops->queue_rq(hctx, &bd);\n\t\tswitch (ret) {\n\t\tcase BLK_MQ_RQ_QUEUE_OK:\n\t\t\tqueued++;\n\t\t\tcontinue;\n\t\tcase BLK_MQ_RQ_QUEUE_BUSY:\n\t\t\tlist_add(&rq->queuelist, &rq_list);\n\t\t\t__blk_mq_requeue_request(rq);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"blk-mq: bad return on queue: %d\\n\", ret);\n\t\tcase BLK_MQ_RQ_QUEUE_ERROR:\n\t\t\trq->errors = -EIO;\n\t\t\tblk_mq_end_request(rq, rq->errors);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret == BLK_MQ_RQ_QUEUE_BUSY)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * We've done the first request. If we have more than 1\n\t\t * left in the list, set dptr to defer issue.\n\t\t */\n\t\tif (!dptr && rq_list.next != rq_list.prev)\n\t\t\tdptr = &driver_list;\n\t}\n\n\tif (!queued)\n\t\thctx->dispatched[0]++;\n\telse if (queued < (1 << (BLK_MQ_MAX_DISPATCH_ORDER - 1)))\n\t\thctx->dispatched[ilog2(queued) + 1]++;\n\n\t/*\n\t * Any items that need requeuing? Stuff them into hctx->dispatch,\n\t * that is where we will continue on next queue run.\n\t */\n\tif (!list_empty(&rq_list)) {\n\t\tspin_lock(&hctx->lock);\n\t\tlist_splice(&rq_list, &hctx->dispatch);\n\t\tspin_unlock(&hctx->lock);\n\t\t/*\n\t\t * the queue is expected stopped with BLK_MQ_RQ_QUEUE_BUSY, but\n\t\t * it's possible the queue is stopped and restarted again\n\t\t * before this. Queue restart will dispatch requests. And since\n\t\t * requests in rq_list aren't added into hctx->dispatch yet,\n\t\t * the requests in rq_list might get lost.\n\t\t *\n\t\t * blk_mq_run_hw_queue() already checks the STOPPED bit\n\t\t **/\n\t\tblk_mq_run_hw_queue(hctx, true);\n\t}\n}\n\n/*\n * It'd be great if the workqueue API had a way to pass\n * in a mask and had some smarts for more clever placement.\n * For now we just round-robin here, switching for every\n * BLK_MQ_CPU_WORK_BATCH queued items.\n */\nstatic int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)\n{\n\tif (hctx->queue->nr_hw_queues == 1)\n\t\treturn WORK_CPU_UNBOUND;\n\n\tif (--hctx->next_cpu_batch <= 0) {\n\t\tint cpu = hctx->next_cpu, next_cpu;\n\n\t\tnext_cpu = cpumask_next(hctx->next_cpu, hctx->cpumask);\n\t\tif (next_cpu >= nr_cpu_ids)\n\t\t\tnext_cpu = cpumask_first(hctx->cpumask);\n\n\t\thctx->next_cpu = next_cpu;\n\t\thctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;\n\n\t\treturn cpu;\n\t}\n\n\treturn hctx->next_cpu;\n}\n\nvoid blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)\n{\n\tif (unlikely(test_bit(BLK_MQ_S_STOPPED, &hctx->state) ||\n\t    !blk_mq_hw_queue_mapped(hctx)))\n\t\treturn;\n\n\tif (!async) {\n\t\tint cpu = get_cpu();\n\t\tif (cpumask_test_cpu(cpu, hctx->cpumask)) {\n\t\t\t__blk_mq_run_hw_queue(hctx);\n\t\t\tput_cpu();\n\t\t\treturn;\n\t\t}\n\n\t\tput_cpu();\n\t}\n\n\tkblockd_schedule_delayed_work_on(blk_mq_hctx_next_cpu(hctx),\n\t\t\t&hctx->run_work, 0);\n}\n\nvoid blk_mq_run_hw_queues(struct request_queue *q, bool async)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif ((!blk_mq_hctx_has_pending(hctx) &&\n\t\t    list_empty_careful(&hctx->dispatch)) ||\n\t\t    test_bit(BLK_MQ_S_STOPPED, &hctx->state))\n\t\t\tcontinue;\n\n\t\tblk_mq_run_hw_queue(hctx, async);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_run_hw_queues);\n\nvoid blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tcancel_delayed_work(&hctx->run_work);\n\tcancel_delayed_work(&hctx->delay_work);\n\tset_bit(BLK_MQ_S_STOPPED, &hctx->state);\n}\nEXPORT_SYMBOL(blk_mq_stop_hw_queue);\n\nvoid blk_mq_stop_hw_queues(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_stop_hw_queue(hctx);\n}\nEXPORT_SYMBOL(blk_mq_stop_hw_queues);\n\nvoid blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tclear_bit(BLK_MQ_S_STOPPED, &hctx->state);\n\n\tblk_mq_run_hw_queue(hctx, false);\n}\nEXPORT_SYMBOL(blk_mq_start_hw_queue);\n\nvoid blk_mq_start_hw_queues(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_start_hw_queue(hctx);\n}\nEXPORT_SYMBOL(blk_mq_start_hw_queues);\n\nvoid blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (!test_bit(BLK_MQ_S_STOPPED, &hctx->state))\n\t\t\tcontinue;\n\n\t\tclear_bit(BLK_MQ_S_STOPPED, &hctx->state);\n\t\tblk_mq_run_hw_queue(hctx, async);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);\n\nstatic void blk_mq_run_work_fn(struct work_struct *work)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\n\thctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);\n\n\t__blk_mq_run_hw_queue(hctx);\n}\n\nstatic void blk_mq_delay_work_fn(struct work_struct *work)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\n\thctx = container_of(work, struct blk_mq_hw_ctx, delay_work.work);\n\n\tif (test_and_clear_bit(BLK_MQ_S_STOPPED, &hctx->state))\n\t\t__blk_mq_run_hw_queue(hctx);\n}\n\nvoid blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)\n{\n\tif (unlikely(!blk_mq_hw_queue_mapped(hctx)))\n\t\treturn;\n\n\tkblockd_schedule_delayed_work_on(blk_mq_hctx_next_cpu(hctx),\n\t\t\t&hctx->delay_work, msecs_to_jiffies(msecs));\n}\nEXPORT_SYMBOL(blk_mq_delay_queue);\n\nstatic void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t    struct request *rq, bool at_head)\n{\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\n\ttrace_block_rq_insert(hctx->queue, rq);\n\n\tif (at_head)\n\t\tlist_add(&rq->queuelist, &ctx->rq_list);\n\telse\n\t\tlist_add_tail(&rq->queuelist, &ctx->rq_list);\n\n\tblk_mq_hctx_mark_pending(hctx, ctx);\n}\n\nvoid blk_mq_insert_request(struct request *rq, bool at_head, bool run_queue,\n\t\tbool async)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx, *current_ctx;\n\n\tcurrent_ctx = blk_mq_get_ctx(q);\n\tif (!cpu_online(ctx->cpu))\n\t\trq->mq_ctx = ctx = current_ctx;\n\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\n\tspin_lock(&ctx->lock);\n\t__blk_mq_insert_request(hctx, rq, at_head);\n\tspin_unlock(&ctx->lock);\n\n\tif (run_queue)\n\t\tblk_mq_run_hw_queue(hctx, async);\n\n\tblk_mq_put_ctx(current_ctx);\n}\n\nstatic void blk_mq_insert_requests(struct request_queue *q,\n\t\t\t\t     struct blk_mq_ctx *ctx,\n\t\t\t\t     struct list_head *list,\n\t\t\t\t     int depth,\n\t\t\t\t     bool from_schedule)\n\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *current_ctx;\n\n\ttrace_block_unplug(q, depth, !from_schedule);\n\n\tcurrent_ctx = blk_mq_get_ctx(q);\n\n\tif (!cpu_online(ctx->cpu))\n\t\tctx = current_ctx;\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\n\t/*\n\t * preemption doesn't flush plug list, so it's possible ctx->cpu is\n\t * offline now\n\t */\n\tspin_lock(&ctx->lock);\n\twhile (!list_empty(list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\trq->mq_ctx = ctx;\n\t\t__blk_mq_insert_request(hctx, rq, false);\n\t}\n\tspin_unlock(&ctx->lock);\n\n\tblk_mq_run_hw_queue(hctx, from_schedule);\n\tblk_mq_put_ctx(current_ctx);\n}\n\nstatic int plug_ctx_cmp(void *priv, struct list_head *a, struct list_head *b)\n{\n\tstruct request *rqa = container_of(a, struct request, queuelist);\n\tstruct request *rqb = container_of(b, struct request, queuelist);\n\n\treturn !(rqa->mq_ctx < rqb->mq_ctx ||\n\t\t (rqa->mq_ctx == rqb->mq_ctx &&\n\t\t  blk_rq_pos(rqa) < blk_rq_pos(rqb)));\n}\n\nvoid blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)\n{\n\tstruct blk_mq_ctx *this_ctx;\n\tstruct request_queue *this_q;\n\tstruct request *rq;\n\tLIST_HEAD(list);\n\tLIST_HEAD(ctx_list);\n\tunsigned int depth;\n\n\tlist_splice_init(&plug->mq_list, &list);\n\n\tlist_sort(NULL, &list, plug_ctx_cmp);\n\n\tthis_q = NULL;\n\tthis_ctx = NULL;\n\tdepth = 0;\n\n\twhile (!list_empty(&list)) {\n\t\trq = list_entry_rq(list.next);\n\t\tlist_del_init(&rq->queuelist);\n\t\tBUG_ON(!rq->q);\n\t\tif (rq->mq_ctx != this_ctx) {\n\t\t\tif (this_ctx) {\n\t\t\t\tblk_mq_insert_requests(this_q, this_ctx,\n\t\t\t\t\t\t\t&ctx_list, depth,\n\t\t\t\t\t\t\tfrom_schedule);\n\t\t\t}\n\n\t\t\tthis_ctx = rq->mq_ctx;\n\t\t\tthis_q = rq->q;\n\t\t\tdepth = 0;\n\t\t}\n\n\t\tdepth++;\n\t\tlist_add_tail(&rq->queuelist, &ctx_list);\n\t}\n\n\t/*\n\t * If 'this_ctx' is set, we know we have entries to complete\n\t * on 'ctx_list'. Do those.\n\t */\n\tif (this_ctx) {\n\t\tblk_mq_insert_requests(this_q, this_ctx, &ctx_list, depth,\n\t\t\t\t       from_schedule);\n\t}\n}\n\nstatic void blk_mq_bio_to_request(struct request *rq, struct bio *bio)\n{\n\tinit_request_from_bio(rq, bio);\n\n\tif (blk_do_io_stat(rq))\n\t\tblk_account_io_start(rq, 1);\n}\n\nstatic inline bool hctx_allow_merges(struct blk_mq_hw_ctx *hctx)\n{\n\treturn (hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&\n\t\t!blk_queue_nomerges(hctx->queue);\n}\n\nstatic inline bool blk_mq_merge_queue_io(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t struct blk_mq_ctx *ctx,\n\t\t\t\t\t struct request *rq, struct bio *bio)\n{\n\tif (!hctx_allow_merges(hctx)) {\n\t\tblk_mq_bio_to_request(rq, bio);\n\t\tspin_lock(&ctx->lock);\ninsert_rq:\n\t\t__blk_mq_insert_request(hctx, rq, false);\n\t\tspin_unlock(&ctx->lock);\n\t\treturn false;\n\t} else {\n\t\tstruct request_queue *q = hctx->queue;\n\n\t\tspin_lock(&ctx->lock);\n\t\tif (!blk_mq_attempt_merge(q, ctx, bio)) {\n\t\t\tblk_mq_bio_to_request(rq, bio);\n\t\t\tgoto insert_rq;\n\t\t}\n\n\t\tspin_unlock(&ctx->lock);\n\t\t__blk_mq_free_request(hctx, ctx, rq);\n\t\treturn true;\n\t}\n}\n\nstruct blk_map_ctx {\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx;\n};\n\nstatic struct request *blk_mq_map_request(struct request_queue *q,\n\t\t\t\t\t  struct bio *bio,\n\t\t\t\t\t  struct blk_map_ctx *data)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx;\n\tstruct request *rq;\n\tint rw = bio_data_dir(bio);\n\tstruct blk_mq_alloc_data alloc_data;\n\n\tif (unlikely(blk_mq_queue_enter(q, GFP_KERNEL))) {\n\t\tbio_io_error(bio);\n\t\treturn NULL;\n\t}\n\n\tctx = blk_mq_get_ctx(q);\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\n\tif (rw_is_sync(bio->bi_rw))\n\t\trw |= REQ_SYNC;\n\n\ttrace_block_getrq(q, bio, rw);\n\tblk_mq_set_alloc_data(&alloc_data, q, GFP_ATOMIC, false, ctx,\n\t\t\thctx);\n\trq = __blk_mq_alloc_request(&alloc_data, rw);\n\tif (unlikely(!rq)) {\n\t\t__blk_mq_run_hw_queue(hctx);\n\t\tblk_mq_put_ctx(ctx);\n\t\ttrace_block_sleeprq(q, bio, rw);\n\n\t\tctx = blk_mq_get_ctx(q);\n\t\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\t\tblk_mq_set_alloc_data(&alloc_data, q,\n\t\t\t\t__GFP_WAIT|GFP_ATOMIC, false, ctx, hctx);\n\t\trq = __blk_mq_alloc_request(&alloc_data, rw);\n\t\tctx = alloc_data.ctx;\n\t\thctx = alloc_data.hctx;\n\t}\n\n\thctx->queued++;\n\tdata->hctx = hctx;\n\tdata->ctx = ctx;\n\treturn rq;\n}\n\nstatic int blk_mq_direct_issue_request(struct request *rq)\n{\n\tint ret;\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_hw_ctx *hctx = q->mq_ops->map_queue(q,\n\t\t\trq->mq_ctx->cpu);\n\tstruct blk_mq_queue_data bd = {\n\t\t.rq = rq,\n\t\t.list = NULL,\n\t\t.last = 1\n\t};\n\n\t/*\n\t * For OK queue, we are done. For error, kill it. Any other\n\t * error (busy), just add it to our list as we previously\n\t * would have done\n\t */\n\tret = q->mq_ops->queue_rq(hctx, &bd);\n\tif (ret == BLK_MQ_RQ_QUEUE_OK)\n\t\treturn 0;\n\telse {\n\t\t__blk_mq_requeue_request(rq);\n\n\t\tif (ret == BLK_MQ_RQ_QUEUE_ERROR) {\n\t\t\trq->errors = -EIO;\n\t\t\tblk_mq_end_request(rq, rq->errors);\n\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n}\n\n/*\n * Multiple hardware queue variant. This will not use per-process plugs,\n * but will attempt to bypass the hctx queueing if we can go straight to\n * hardware for SYNC IO.\n */\nstatic void blk_mq_make_request(struct request_queue *q, struct bio *bio)\n{\n\tconst int is_sync = rw_is_sync(bio->bi_rw);\n\tconst int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);\n\tstruct blk_map_ctx data;\n\tstruct request *rq;\n\tunsigned int request_count = 0;\n\tstruct blk_plug *plug;\n\tstruct request *same_queue_rq = NULL;\n\n\tblk_queue_bounce(q, &bio);\n\n\tif (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {\n\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\n\tblk_queue_split(q, &bio, q->bio_split);\n\n\tif (!is_flush_fua && !blk_queue_nomerges(q) &&\n\t    blk_attempt_plug_merge(q, bio, &request_count, &same_queue_rq))\n\t\treturn;\n\n\trq = blk_mq_map_request(q, bio, &data);\n\tif (unlikely(!rq))\n\t\treturn;\n\n\tif (unlikely(is_flush_fua)) {\n\t\tblk_mq_bio_to_request(rq, bio);\n\t\tblk_insert_flush(rq);\n\t\tgoto run_queue;\n\t}\n\n\tplug = current->plug;\n\t/*\n\t * If the driver supports defer issued based on 'last', then\n\t * queue it up like normal since we can potentially save some\n\t * CPU this way.\n\t */\n\tif (((plug && !blk_queue_nomerges(q)) || is_sync) &&\n\t    !(data.hctx->flags & BLK_MQ_F_DEFER_ISSUE)) {\n\t\tstruct request *old_rq = NULL;\n\n\t\tblk_mq_bio_to_request(rq, bio);\n\n\t\t/*\n\t\t * we do limited pluging. If bio can be merged, do merge.\n\t\t * Otherwise the existing request in the plug list will be\n\t\t * issued. So the plug list will have one request at most\n\t\t */\n\t\tif (plug) {\n\t\t\t/*\n\t\t\t * The plug list might get flushed before this. If that\n\t\t\t * happens, same_queue_rq is invalid and plug list is empty\n\t\t\t **/\n\t\t\tif (same_queue_rq && !list_empty(&plug->mq_list)) {\n\t\t\t\told_rq = same_queue_rq;\n\t\t\t\tlist_del_init(&old_rq->queuelist);\n\t\t\t}\n\t\t\tlist_add_tail(&rq->queuelist, &plug->mq_list);\n\t\t} else /* is_sync */\n\t\t\told_rq = rq;\n\t\tblk_mq_put_ctx(data.ctx);\n\t\tif (!old_rq)\n\t\t\treturn;\n\t\tif (!blk_mq_direct_issue_request(old_rq))\n\t\t\treturn;\n\t\tblk_mq_insert_request(old_rq, false, true, true);\n\t\treturn;\n\t}\n\n\tif (!blk_mq_merge_queue_io(data.hctx, data.ctx, rq, bio)) {\n\t\t/*\n\t\t * For a SYNC request, send it to the hardware immediately. For\n\t\t * an ASYNC request, just ensure that we run it later on. The\n\t\t * latter allows for merging opportunities and more efficient\n\t\t * dispatching.\n\t\t */\nrun_queue:\n\t\tblk_mq_run_hw_queue(data.hctx, !is_sync || is_flush_fua);\n\t}\n\tblk_mq_put_ctx(data.ctx);\n}\n\n/*\n * Single hardware queue variant. This will attempt to use any per-process\n * plug for merging and IO deferral.\n */\nstatic void blk_sq_make_request(struct request_queue *q, struct bio *bio)\n{\n\tconst int is_sync = rw_is_sync(bio->bi_rw);\n\tconst int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);\n\tstruct blk_plug *plug;\n\tunsigned int request_count = 0;\n\tstruct blk_map_ctx data;\n\tstruct request *rq;\n\n\tblk_queue_bounce(q, &bio);\n\n\tif (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {\n\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\n\tblk_queue_split(q, &bio, q->bio_split);\n\n\tif (!is_flush_fua && !blk_queue_nomerges(q) &&\n\t    blk_attempt_plug_merge(q, bio, &request_count, NULL))\n\t\treturn;\n\n\trq = blk_mq_map_request(q, bio, &data);\n\tif (unlikely(!rq))\n\t\treturn;\n\n\tif (unlikely(is_flush_fua)) {\n\t\tblk_mq_bio_to_request(rq, bio);\n\t\tblk_insert_flush(rq);\n\t\tgoto run_queue;\n\t}\n\n\t/*\n\t * A task plug currently exists. Since this is completely lockless,\n\t * utilize that to temporarily store requests until the task is\n\t * either done or scheduled away.\n\t */\n\tplug = current->plug;\n\tif (plug) {\n\t\tblk_mq_bio_to_request(rq, bio);\n\t\tif (list_empty(&plug->mq_list))\n\t\t\ttrace_block_plug(q);\n\t\telse if (request_count >= BLK_MAX_REQUEST_COUNT) {\n\t\t\tblk_flush_plug_list(plug, false);\n\t\t\ttrace_block_plug(q);\n\t\t}\n\t\tlist_add_tail(&rq->queuelist, &plug->mq_list);\n\t\tblk_mq_put_ctx(data.ctx);\n\t\treturn;\n\t}\n\n\tif (!blk_mq_merge_queue_io(data.hctx, data.ctx, rq, bio)) {\n\t\t/*\n\t\t * For a SYNC request, send it to the hardware immediately. For\n\t\t * an ASYNC request, just ensure that we run it later on. The\n\t\t * latter allows for merging opportunities and more efficient\n\t\t * dispatching.\n\t\t */\nrun_queue:\n\t\tblk_mq_run_hw_queue(data.hctx, !is_sync || is_flush_fua);\n\t}\n\n\tblk_mq_put_ctx(data.ctx);\n}\n\n/*\n * Default mapping to a software queue, since we use one per CPU.\n */\nstruct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q, const int cpu)\n{\n\treturn q->queue_hw_ctx[q->mq_map[cpu]];\n}\nEXPORT_SYMBOL(blk_mq_map_queue);\n\nstatic void blk_mq_free_rq_map(struct blk_mq_tag_set *set,\n\t\tstruct blk_mq_tags *tags, unsigned int hctx_idx)\n{\n\tstruct page *page;\n\n\tif (tags->rqs && set->ops->exit_request) {\n\t\tint i;\n\n\t\tfor (i = 0; i < tags->nr_tags; i++) {\n\t\t\tif (!tags->rqs[i])\n\t\t\t\tcontinue;\n\t\t\tset->ops->exit_request(set->driver_data, tags->rqs[i],\n\t\t\t\t\t\thctx_idx, i);\n\t\t\ttags->rqs[i] = NULL;\n\t\t}\n\t}\n\n\twhile (!list_empty(&tags->page_list)) {\n\t\tpage = list_first_entry(&tags->page_list, struct page, lru);\n\t\tlist_del_init(&page->lru);\n\t\t__free_pages(page, page->private);\n\t}\n\n\tkfree(tags->rqs);\n\n\tblk_mq_free_tags(tags);\n}\n\nstatic size_t order_to_size(unsigned int order)\n{\n\treturn (size_t)PAGE_SIZE << order;\n}\n\nstatic struct blk_mq_tags *blk_mq_init_rq_map(struct blk_mq_tag_set *set,\n\t\tunsigned int hctx_idx)\n{\n\tstruct blk_mq_tags *tags;\n\tunsigned int i, j, entries_per_page, max_order = 4;\n\tsize_t rq_size, left;\n\n\ttags = blk_mq_init_tags(set->queue_depth, set->reserved_tags,\n\t\t\t\tset->numa_node,\n\t\t\t\tBLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));\n\tif (!tags)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&tags->page_list);\n\n\ttags->rqs = kzalloc_node(set->queue_depth * sizeof(struct request *),\n\t\t\t\t GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY,\n\t\t\t\t set->numa_node);\n\tif (!tags->rqs) {\n\t\tblk_mq_free_tags(tags);\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * rq_size is the size of the request plus driver payload, rounded\n\t * to the cacheline size\n\t */\n\trq_size = round_up(sizeof(struct request) + set->cmd_size,\n\t\t\t\tcache_line_size());\n\tleft = rq_size * set->queue_depth;\n\n\tfor (i = 0; i < set->queue_depth; ) {\n\t\tint this_order = max_order;\n\t\tstruct page *page;\n\t\tint to_do;\n\t\tvoid *p;\n\n\t\twhile (left < order_to_size(this_order - 1) && this_order)\n\t\t\tthis_order--;\n\n\t\tdo {\n\t\t\tpage = alloc_pages_node(set->numa_node,\n\t\t\t\tGFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY | __GFP_ZERO,\n\t\t\t\tthis_order);\n\t\t\tif (page)\n\t\t\t\tbreak;\n\t\t\tif (!this_order--)\n\t\t\t\tbreak;\n\t\t\tif (order_to_size(this_order) < rq_size)\n\t\t\t\tbreak;\n\t\t} while (1);\n\n\t\tif (!page)\n\t\t\tgoto fail;\n\n\t\tpage->private = this_order;\n\t\tlist_add_tail(&page->lru, &tags->page_list);\n\n\t\tp = page_address(page);\n\t\tentries_per_page = order_to_size(this_order) / rq_size;\n\t\tto_do = min(entries_per_page, set->queue_depth - i);\n\t\tleft -= to_do * rq_size;\n\t\tfor (j = 0; j < to_do; j++) {\n\t\t\ttags->rqs[i] = p;\n\t\t\tif (set->ops->init_request) {\n\t\t\t\tif (set->ops->init_request(set->driver_data,\n\t\t\t\t\t\ttags->rqs[i], hctx_idx, i,\n\t\t\t\t\t\tset->numa_node)) {\n\t\t\t\t\ttags->rqs[i] = NULL;\n\t\t\t\t\tgoto fail;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tp += rq_size;\n\t\t\ti++;\n\t\t}\n\t}\n\treturn tags;\n\nfail:\n\tblk_mq_free_rq_map(set, tags, hctx_idx);\n\treturn NULL;\n}\n\nstatic void blk_mq_free_bitmap(struct blk_mq_ctxmap *bitmap)\n{\n\tkfree(bitmap->map);\n}\n\nstatic int blk_mq_alloc_bitmap(struct blk_mq_ctxmap *bitmap, int node)\n{\n\tunsigned int bpw = 8, total, num_maps, i;\n\n\tbitmap->bits_per_word = bpw;\n\n\tnum_maps = ALIGN(nr_cpu_ids, bpw) / bpw;\n\tbitmap->map = kzalloc_node(num_maps * sizeof(struct blk_align_bitmap),\n\t\t\t\t\tGFP_KERNEL, node);\n\tif (!bitmap->map)\n\t\treturn -ENOMEM;\n\n\ttotal = nr_cpu_ids;\n\tfor (i = 0; i < num_maps; i++) {\n\t\tbitmap->map[i].depth = min(total, bitmap->bits_per_word);\n\t\ttotal -= bitmap->map[i].depth;\n\t}\n\n\treturn 0;\n}\n\nstatic int blk_mq_hctx_cpu_offline(struct blk_mq_hw_ctx *hctx, int cpu)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct blk_mq_ctx *ctx;\n\tLIST_HEAD(tmp);\n\n\t/*\n\t * Move ctx entries to new CPU, if this one is going away.\n\t */\n\tctx = __blk_mq_get_ctx(q, cpu);\n\n\tspin_lock(&ctx->lock);\n\tif (!list_empty(&ctx->rq_list)) {\n\t\tlist_splice_init(&ctx->rq_list, &tmp);\n\t\tblk_mq_hctx_clear_pending(hctx, ctx);\n\t}\n\tspin_unlock(&ctx->lock);\n\n\tif (list_empty(&tmp))\n\t\treturn NOTIFY_OK;\n\n\tctx = blk_mq_get_ctx(q);\n\tspin_lock(&ctx->lock);\n\n\twhile (!list_empty(&tmp)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(&tmp, struct request, queuelist);\n\t\trq->mq_ctx = ctx;\n\t\tlist_move_tail(&rq->queuelist, &ctx->rq_list);\n\t}\n\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\tblk_mq_hctx_mark_pending(hctx, ctx);\n\n\tspin_unlock(&ctx->lock);\n\n\tblk_mq_run_hw_queue(hctx, true);\n\tblk_mq_put_ctx(ctx);\n\treturn NOTIFY_OK;\n}\n\nstatic int blk_mq_hctx_notify(void *data, unsigned long action,\n\t\t\t      unsigned int cpu)\n{\n\tstruct blk_mq_hw_ctx *hctx = data;\n\n\tif (action == CPU_DEAD || action == CPU_DEAD_FROZEN)\n\t\treturn blk_mq_hctx_cpu_offline(hctx, cpu);\n\n\t/*\n\t * In case of CPU online, tags may be reallocated\n\t * in blk_mq_map_swqueue() after mapping is updated.\n\t */\n\n\treturn NOTIFY_OK;\n}\n\n/* hctx->ctxs will be freed in queue's release handler */\nstatic void blk_mq_exit_hctx(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set,\n\t\tstruct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)\n{\n\tunsigned flush_start_tag = set->queue_depth;\n\n\tblk_mq_tag_idle(hctx);\n\n\tif (set->ops->exit_request)\n\t\tset->ops->exit_request(set->driver_data,\n\t\t\t\t       hctx->fq->flush_rq, hctx_idx,\n\t\t\t\t       flush_start_tag + hctx_idx);\n\n\tif (set->ops->exit_hctx)\n\t\tset->ops->exit_hctx(hctx, hctx_idx);\n\n\tblk_mq_unregister_cpu_notifier(&hctx->cpu_notifier);\n\tblk_free_flush_queue(hctx->fq);\n\tblk_mq_free_bitmap(&hctx->ctx_map);\n}\n\nstatic void blk_mq_exit_hw_queues(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set, int nr_queue)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (i == nr_queue)\n\t\t\tbreak;\n\t\tblk_mq_exit_hctx(q, set, hctx, i);\n\t}\n}\n\nstatic void blk_mq_free_hw_queues(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tfree_cpumask_var(hctx->cpumask);\n}\n\nstatic int blk_mq_init_hctx(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set,\n\t\tstruct blk_mq_hw_ctx *hctx, unsigned hctx_idx)\n{\n\tint node;\n\tunsigned flush_start_tag = set->queue_depth;\n\n\tnode = hctx->numa_node;\n\tif (node == NUMA_NO_NODE)\n\t\tnode = hctx->numa_node = set->numa_node;\n\n\tINIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);\n\tINIT_DELAYED_WORK(&hctx->delay_work, blk_mq_delay_work_fn);\n\tspin_lock_init(&hctx->lock);\n\tINIT_LIST_HEAD(&hctx->dispatch);\n\thctx->queue = q;\n\thctx->queue_num = hctx_idx;\n\thctx->flags = set->flags;\n\n\tblk_mq_init_cpu_notifier(&hctx->cpu_notifier,\n\t\t\t\t\tblk_mq_hctx_notify, hctx);\n\tblk_mq_register_cpu_notifier(&hctx->cpu_notifier);\n\n\thctx->tags = set->tags[hctx_idx];\n\n\t/*\n\t * Allocate space for all possible cpus to avoid allocation at\n\t * runtime\n\t */\n\thctx->ctxs = kmalloc_node(nr_cpu_ids * sizeof(void *),\n\t\t\t\t\tGFP_KERNEL, node);\n\tif (!hctx->ctxs)\n\t\tgoto unregister_cpu_notifier;\n\n\tif (blk_mq_alloc_bitmap(&hctx->ctx_map, node))\n\t\tgoto free_ctxs;\n\n\thctx->nr_ctx = 0;\n\n\tif (set->ops->init_hctx &&\n\t    set->ops->init_hctx(hctx, set->driver_data, hctx_idx))\n\t\tgoto free_bitmap;\n\n\thctx->fq = blk_alloc_flush_queue(q, hctx->numa_node, set->cmd_size);\n\tif (!hctx->fq)\n\t\tgoto exit_hctx;\n\n\tif (set->ops->init_request &&\n\t    set->ops->init_request(set->driver_data,\n\t\t\t\t   hctx->fq->flush_rq, hctx_idx,\n\t\t\t\t   flush_start_tag + hctx_idx, node))\n\t\tgoto free_fq;\n\n\treturn 0;\n\n free_fq:\n\tkfree(hctx->fq);\n exit_hctx:\n\tif (set->ops->exit_hctx)\n\t\tset->ops->exit_hctx(hctx, hctx_idx);\n free_bitmap:\n\tblk_mq_free_bitmap(&hctx->ctx_map);\n free_ctxs:\n\tkfree(hctx->ctxs);\n unregister_cpu_notifier:\n\tblk_mq_unregister_cpu_notifier(&hctx->cpu_notifier);\n\n\treturn -1;\n}\n\nstatic int blk_mq_init_hw_queues(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\t/*\n\t * Initialize hardware queues\n\t */\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (blk_mq_init_hctx(q, set, hctx, i))\n\t\t\tbreak;\n\t}\n\n\tif (i == q->nr_hw_queues)\n\t\treturn 0;\n\n\t/*\n\t * Init failed\n\t */\n\tblk_mq_exit_hw_queues(q, set, i);\n\n\treturn 1;\n}\n\nstatic void blk_mq_init_cpu_queues(struct request_queue *q,\n\t\t\t\t   unsigned int nr_hw_queues)\n{\n\tunsigned int i;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct blk_mq_ctx *__ctx = per_cpu_ptr(q->queue_ctx, i);\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\tmemset(__ctx, 0, sizeof(*__ctx));\n\t\t__ctx->cpu = i;\n\t\tspin_lock_init(&__ctx->lock);\n\t\tINIT_LIST_HEAD(&__ctx->rq_list);\n\t\t__ctx->queue = q;\n\n\t\t/* If the cpu isn't online, the cpu is mapped to first hctx */\n\t\tif (!cpu_online(i))\n\t\t\tcontinue;\n\n\t\thctx = q->mq_ops->map_queue(q, i);\n\n\t\t/*\n\t\t * Set local node, IFF we have more than one hw queue. If\n\t\t * not, we remain on the home node of the device\n\t\t */\n\t\tif (nr_hw_queues > 1 && hctx->numa_node == NUMA_NO_NODE)\n\t\t\thctx->numa_node = cpu_to_node(i);\n\t}\n}\n\nstatic void blk_mq_map_swqueue(struct request_queue *q)\n{\n\tunsigned int i;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx;\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tcpumask_clear(hctx->cpumask);\n\t\thctx->nr_ctx = 0;\n\t}\n\n\t/*\n\t * Map software to hardware queues\n\t */\n\tqueue_for_each_ctx(q, ctx, i) {\n\t\t/* If the cpu isn't online, the cpu is mapped to first hctx */\n\t\tif (!cpu_online(i))\n\t\t\tcontinue;\n\n\t\thctx = q->mq_ops->map_queue(q, i);\n\t\tcpumask_set_cpu(i, hctx->cpumask);\n\t\tcpumask_set_cpu(i, hctx->tags->cpumask);\n\t\tctx->index_hw = hctx->nr_ctx;\n\t\thctx->ctxs[hctx->nr_ctx++] = ctx;\n\t}\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tstruct blk_mq_ctxmap *map = &hctx->ctx_map;\n\n\t\t/*\n\t\t * If no software queues are mapped to this hardware queue,\n\t\t * disable it and free the request entries.\n\t\t */\n\t\tif (!hctx->nr_ctx) {\n\t\t\tif (set->tags[i]) {\n\t\t\t\tblk_mq_free_rq_map(set, set->tags[i], i);\n\t\t\t\tset->tags[i] = NULL;\n\t\t\t}\n\t\t\thctx->tags = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* unmapped hw queue can be remapped after CPU topo changed */\n\t\tif (!set->tags[i])\n\t\t\tset->tags[i] = blk_mq_init_rq_map(set, i);\n\t\thctx->tags = set->tags[i];\n\t\tWARN_ON(!hctx->tags);\n\n\t\t/*\n\t\t * Set the map size to the number of mapped software queues.\n\t\t * This is more accurate and more efficient than looping\n\t\t * over all possibly mapped software queues.\n\t\t */\n\t\tmap->size = DIV_ROUND_UP(hctx->nr_ctx, map->bits_per_word);\n\n\t\t/*\n\t\t * Initialize batch roundrobin counts\n\t\t */\n\t\thctx->next_cpu = cpumask_first(hctx->cpumask);\n\t\thctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;\n\t}\n}\n\nstatic void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct request_queue *q;\n\tbool shared;\n\tint i;\n\n\tif (set->tag_list.next == set->tag_list.prev)\n\t\tshared = false;\n\telse\n\t\tshared = true;\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_freeze_queue(q);\n\n\t\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t\tif (shared)\n\t\t\t\thctx->flags |= BLK_MQ_F_TAG_SHARED;\n\t\t\telse\n\t\t\t\thctx->flags &= ~BLK_MQ_F_TAG_SHARED;\n\t\t}\n\t\tblk_mq_unfreeze_queue(q);\n\t}\n}\n\nstatic void blk_mq_del_queue_tag_set(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\n\tmutex_lock(&set->tag_list_lock);\n\tlist_del_init(&q->tag_set_list);\n\tblk_mq_update_tag_set_depth(set);\n\tmutex_unlock(&set->tag_list_lock);\n}\n\nstatic void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,\n\t\t\t\t     struct request_queue *q)\n{\n\tq->tag_set = set;\n\n\tmutex_lock(&set->tag_list_lock);\n\tlist_add_tail(&q->tag_set_list, &set->tag_list);\n\tblk_mq_update_tag_set_depth(set);\n\tmutex_unlock(&set->tag_list_lock);\n}\n\n/*\n * It is the actual release handler for mq, but we do it from\n * request queue's release handler for avoiding use-after-free\n * and headache because q->mq_kobj shouldn't have been introduced,\n * but we can't group ctx/kctx kobj without it.\n */\nvoid blk_mq_release(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\t/* hctx kobj stays in hctx */\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (!hctx)\n\t\t\tcontinue;\n\t\tkfree(hctx->ctxs);\n\t\tkfree(hctx);\n\t}\n\n\tkfree(q->queue_hw_ctx);\n\n\t/* ctx kobj stays in queue_ctx */\n\tfree_percpu(q->queue_ctx);\n}\n\nstruct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)\n{\n\tstruct request_queue *uninit_q, *q;\n\n\tuninit_q = blk_alloc_queue_node(GFP_KERNEL, set->numa_node);\n\tif (!uninit_q)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tq = blk_mq_init_allocated_queue(set, uninit_q);\n\tif (IS_ERR(q))\n\t\tblk_cleanup_queue(uninit_q);\n\n\treturn q;\n}\nEXPORT_SYMBOL(blk_mq_init_queue);\n\nstruct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,\n\t\t\t\t\t\t  struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx **hctxs;\n\tstruct blk_mq_ctx __percpu *ctx;\n\tunsigned int *map;\n\tint i;\n\n\tctx = alloc_percpu(struct blk_mq_ctx);\n\tif (!ctx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\thctxs = kmalloc_node(set->nr_hw_queues * sizeof(*hctxs), GFP_KERNEL,\n\t\t\tset->numa_node);\n\n\tif (!hctxs)\n\t\tgoto err_percpu;\n\n\tmap = blk_mq_make_queue_map(set);\n\tif (!map)\n\t\tgoto err_map;\n\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tint node = blk_mq_hw_queue_to_node(map, i);\n\n\t\thctxs[i] = kzalloc_node(sizeof(struct blk_mq_hw_ctx),\n\t\t\t\t\tGFP_KERNEL, node);\n\t\tif (!hctxs[i])\n\t\t\tgoto err_hctxs;\n\n\t\tif (!zalloc_cpumask_var_node(&hctxs[i]->cpumask, GFP_KERNEL,\n\t\t\t\t\t\tnode))\n\t\t\tgoto err_hctxs;\n\n\t\tatomic_set(&hctxs[i]->nr_active, 0);\n\t\thctxs[i]->numa_node = node;\n\t\thctxs[i]->queue_num = i;\n\t}\n\n\t/*\n\t * Init percpu_ref in atomic mode so that it's faster to shutdown.\n\t * See blk_register_queue() for details.\n\t */\n\tif (percpu_ref_init(&q->mq_usage_counter, blk_mq_usage_counter_release,\n\t\t\t    PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))\n\t\tgoto err_hctxs;\n\n\tsetup_timer(&q->timeout, blk_mq_rq_timer, (unsigned long) q);\n\tblk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ);\n\n\tq->nr_queues = nr_cpu_ids;\n\tq->nr_hw_queues = set->nr_hw_queues;\n\tq->mq_map = map;\n\n\tq->queue_ctx = ctx;\n\tq->queue_hw_ctx = hctxs;\n\n\tq->mq_ops = set->ops;\n\tq->queue_flags |= QUEUE_FLAG_MQ_DEFAULT;\n\n\tif (!(set->flags & BLK_MQ_F_SG_MERGE))\n\t\tq->queue_flags |= 1 << QUEUE_FLAG_NO_SG_MERGE;\n\n\tq->sg_reserved_size = INT_MAX;\n\n\tINIT_WORK(&q->requeue_work, blk_mq_requeue_work);\n\tINIT_LIST_HEAD(&q->requeue_list);\n\tspin_lock_init(&q->requeue_lock);\n\n\tif (q->nr_hw_queues > 1)\n\t\tblk_queue_make_request(q, blk_mq_make_request);\n\telse\n\t\tblk_queue_make_request(q, blk_sq_make_request);\n\n\t/*\n\t * Do this after blk_queue_make_request() overrides it...\n\t */\n\tq->nr_requests = set->queue_depth;\n\n\tif (set->ops->complete)\n\t\tblk_queue_softirq_done(q, set->ops->complete);\n\n\tblk_mq_init_cpu_queues(q, set->nr_hw_queues);\n\n\tif (blk_mq_init_hw_queues(q, set))\n\t\tgoto err_hctxs;\n\n\tmutex_lock(&all_q_mutex);\n\tlist_add_tail(&q->all_q_node, &all_q_list);\n\tmutex_unlock(&all_q_mutex);\n\n\tblk_mq_add_queue_tag_set(set, q);\n\n\tblk_mq_map_swqueue(q);\n\n\treturn q;\n\nerr_hctxs:\n\tkfree(map);\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tif (!hctxs[i])\n\t\t\tbreak;\n\t\tfree_cpumask_var(hctxs[i]->cpumask);\n\t\tkfree(hctxs[i]);\n\t}\nerr_map:\n\tkfree(hctxs);\nerr_percpu:\n\tfree_percpu(ctx);\n\treturn ERR_PTR(-ENOMEM);\n}\nEXPORT_SYMBOL(blk_mq_init_allocated_queue);\n\nvoid blk_mq_free_queue(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set\t*set = q->tag_set;\n\n\tblk_mq_del_queue_tag_set(q);\n\n\tblk_mq_exit_hw_queues(q, set, set->nr_hw_queues);\n\tblk_mq_free_hw_queues(q, set);\n\n\tpercpu_ref_exit(&q->mq_usage_counter);\n\n\tkfree(q->mq_map);\n\n\tq->mq_map = NULL;\n\n\tmutex_lock(&all_q_mutex);\n\tlist_del_init(&q->all_q_node);\n\tmutex_unlock(&all_q_mutex);\n}\n\n/* Basically redo blk_mq_init_queue with queue frozen */\nstatic void blk_mq_queue_reinit(struct request_queue *q)\n{\n\tWARN_ON_ONCE(!atomic_read(&q->mq_freeze_depth));\n\n\tblk_mq_sysfs_unregister(q);\n\n\tblk_mq_update_queue_map(q->mq_map, q->nr_hw_queues);\n\n\t/*\n\t * redo blk_mq_init_cpu_queues and blk_mq_init_hw_queues. FIXME: maybe\n\t * we should change hctx numa_node according to new topology (this\n\t * involves free and re-allocate memory, worthy doing?)\n\t */\n\n\tblk_mq_map_swqueue(q);\n\n\tblk_mq_sysfs_register(q);\n}\n\nstatic int blk_mq_queue_reinit_notify(struct notifier_block *nb,\n\t\t\t\t      unsigned long action, void *hcpu)\n{\n\tstruct request_queue *q;\n\n\t/*\n\t * Before new mappings are established, hotadded cpu might already\n\t * start handling requests. This doesn't break anything as we map\n\t * offline CPUs to first hardware queue. We will re-init the queue\n\t * below to get optimal settings.\n\t */\n\tif (action != CPU_DEAD && action != CPU_DEAD_FROZEN &&\n\t    action != CPU_ONLINE && action != CPU_ONLINE_FROZEN)\n\t\treturn NOTIFY_OK;\n\n\tmutex_lock(&all_q_mutex);\n\n\t/*\n\t * We need to freeze and reinit all existing queues.  Freezing\n\t * involves synchronous wait for an RCU grace period and doing it\n\t * one by one may take a long time.  Start freezing all queues in\n\t * one swoop and then wait for the completions so that freezing can\n\t * take place in parallel.\n\t */\n\tlist_for_each_entry(q, &all_q_list, all_q_node)\n\t\tblk_mq_freeze_queue_start(q);\n\tlist_for_each_entry(q, &all_q_list, all_q_node) {\n\t\tblk_mq_freeze_queue_wait(q);\n\n\t\t/*\n\t\t * timeout handler can't touch hw queue during the\n\t\t * reinitialization\n\t\t */\n\t\tdel_timer_sync(&q->timeout);\n\t}\n\n\tlist_for_each_entry(q, &all_q_list, all_q_node)\n\t\tblk_mq_queue_reinit(q);\n\n\tlist_for_each_entry(q, &all_q_list, all_q_node)\n\t\tblk_mq_unfreeze_queue(q);\n\n\tmutex_unlock(&all_q_mutex);\n\treturn NOTIFY_OK;\n}\n\nstatic int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)\n{\n\tint i;\n\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tset->tags[i] = blk_mq_init_rq_map(set, i);\n\t\tif (!set->tags[i])\n\t\t\tgoto out_unwind;\n\t}\n\n\treturn 0;\n\nout_unwind:\n\twhile (--i >= 0)\n\t\tblk_mq_free_rq_map(set, set->tags[i], i);\n\n\treturn -ENOMEM;\n}\n\n/*\n * Allocate the request maps associated with this tag_set. Note that this\n * may reduce the depth asked for, if memory is tight. set->queue_depth\n * will be updated to reflect the allocated depth.\n */\nstatic int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)\n{\n\tunsigned int depth;\n\tint err;\n\n\tdepth = set->queue_depth;\n\tdo {\n\t\terr = __blk_mq_alloc_rq_maps(set);\n\t\tif (!err)\n\t\t\tbreak;\n\n\t\tset->queue_depth >>= 1;\n\t\tif (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t} while (set->queue_depth);\n\n\tif (!set->queue_depth || err) {\n\t\tpr_err(\"blk-mq: failed to allocate request map\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (depth != set->queue_depth)\n\t\tpr_info(\"blk-mq: reduced tag depth (%u -> %u)\\n\",\n\t\t\t\t\t\tdepth, set->queue_depth);\n\n\treturn 0;\n}\n\nstruct cpumask *blk_mq_tags_cpumask(struct blk_mq_tags *tags)\n{\n\treturn tags->cpumask;\n}\nEXPORT_SYMBOL_GPL(blk_mq_tags_cpumask);\n\n/*\n * Alloc a tag set to be associated with one or more request queues.\n * May fail with EINVAL for various error conditions. May adjust the\n * requested depth down, if if it too large. In that case, the set\n * value will be stored in set->queue_depth.\n */\nint blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)\n{\n\tBUILD_BUG_ON(BLK_MQ_MAX_DEPTH > 1 << BLK_MQ_UNIQUE_TAG_BITS);\n\n\tif (!set->nr_hw_queues)\n\t\treturn -EINVAL;\n\tif (!set->queue_depth)\n\t\treturn -EINVAL;\n\tif (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN)\n\t\treturn -EINVAL;\n\n\tif (!set->ops->queue_rq || !set->ops->map_queue)\n\t\treturn -EINVAL;\n\n\tif (set->queue_depth > BLK_MQ_MAX_DEPTH) {\n\t\tpr_info(\"blk-mq: reduced tag depth to %u\\n\",\n\t\t\tBLK_MQ_MAX_DEPTH);\n\t\tset->queue_depth = BLK_MQ_MAX_DEPTH;\n\t}\n\n\t/*\n\t * If a crashdump is active, then we are potentially in a very\n\t * memory constrained environment. Limit us to 1 queue and\n\t * 64 tags to prevent using too much memory.\n\t */\n\tif (is_kdump_kernel()) {\n\t\tset->nr_hw_queues = 1;\n\t\tset->queue_depth = min(64U, set->queue_depth);\n\t}\n\n\tset->tags = kmalloc_node(set->nr_hw_queues *\n\t\t\t\t sizeof(struct blk_mq_tags *),\n\t\t\t\t GFP_KERNEL, set->numa_node);\n\tif (!set->tags)\n\t\treturn -ENOMEM;\n\n\tif (blk_mq_alloc_rq_maps(set))\n\t\tgoto enomem;\n\n\tmutex_init(&set->tag_list_lock);\n\tINIT_LIST_HEAD(&set->tag_list);\n\n\treturn 0;\nenomem:\n\tkfree(set->tags);\n\tset->tags = NULL;\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(blk_mq_alloc_tag_set);\n\nvoid blk_mq_free_tag_set(struct blk_mq_tag_set *set)\n{\n\tint i;\n\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tif (set->tags[i]) {\n\t\t\tblk_mq_free_rq_map(set, set->tags[i], i);\n\t\t\tfree_cpumask_var(set->tags[i]->cpumask);\n\t\t}\n\t}\n\n\tkfree(set->tags);\n\tset->tags = NULL;\n}\nEXPORT_SYMBOL(blk_mq_free_tag_set);\n\nint blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i, ret;\n\n\tif (!set || nr > set->queue_depth)\n\t\treturn -EINVAL;\n\n\tret = 0;\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tret = blk_mq_tag_update_depth(hctx->tags, nr);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (!ret)\n\t\tq->nr_requests = nr;\n\n\treturn ret;\n}\n\nvoid blk_mq_disable_hotplug(void)\n{\n\tmutex_lock(&all_q_mutex);\n}\n\nvoid blk_mq_enable_hotplug(void)\n{\n\tmutex_unlock(&all_q_mutex);\n}\n\nstatic int __init blk_mq_init(void)\n{\n\tblk_mq_cpu_init();\n\n\thotcpu_notifier(blk_mq_queue_reinit_notify, 0);\n\n\treturn 0;\n}\nsubsys_initcall(blk_mq_init);\n", "#ifndef BLK_INTERNAL_H\n#define BLK_INTERNAL_H\n\n#include <linux/idr.h>\n#include <linux/blk-mq.h>\n#include \"blk-mq.h\"\n\n/* Amount of time in which a process may batch requests */\n#define BLK_BATCH_TIME\t(HZ/50UL)\n\n/* Number of requests a \"batching\" process may submit */\n#define BLK_BATCH_REQ\t32\n\n/* Max future timer expiry for timeouts */\n#define BLK_MAX_TIMEOUT\t\t(5 * HZ)\n\nstruct blk_flush_queue {\n\tunsigned int\t\tflush_queue_delayed:1;\n\tunsigned int\t\tflush_pending_idx:1;\n\tunsigned int\t\tflush_running_idx:1;\n\tunsigned long\t\tflush_pending_since;\n\tstruct list_head\tflush_queue[2];\n\tstruct list_head\tflush_data_in_flight;\n\tstruct request\t\t*flush_rq;\n\tspinlock_t\t\tmq_flush_lock;\n};\n\nextern struct kmem_cache *blk_requestq_cachep;\nextern struct kmem_cache *request_cachep;\nextern struct kobj_type blk_queue_ktype;\nextern struct ida blk_queue_ida;\n\nstatic inline struct blk_flush_queue *blk_get_flush_queue(\n\t\tstruct request_queue *q, struct blk_mq_ctx *ctx)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\n\tif (!q->mq_ops)\n\t\treturn q->fq;\n\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\n\treturn hctx->fq;\n}\n\nstatic inline void __blk_get_queue(struct request_queue *q)\n{\n\tkobject_get(&q->kobj);\n}\n\nstruct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,\n\t\tint node, int cmd_size);\nvoid blk_free_flush_queue(struct blk_flush_queue *q);\n\nint blk_init_rl(struct request_list *rl, struct request_queue *q,\n\t\tgfp_t gfp_mask);\nvoid blk_exit_rl(struct request_list *rl);\nvoid init_request_from_bio(struct request *req, struct bio *bio);\nvoid blk_rq_bio_prep(struct request_queue *q, struct request *rq,\n\t\t\tstruct bio *bio);\nint blk_rq_append_bio(struct request_queue *q, struct request *rq,\n\t\t      struct bio *bio);\nvoid blk_queue_bypass_start(struct request_queue *q);\nvoid blk_queue_bypass_end(struct request_queue *q);\nvoid blk_dequeue_request(struct request *rq);\nvoid __blk_queue_free_tags(struct request_queue *q);\nbool __blk_end_bidi_request(struct request *rq, int error,\n\t\t\t    unsigned int nr_bytes, unsigned int bidi_bytes);\n\nvoid blk_rq_timed_out_timer(unsigned long data);\nunsigned long blk_rq_timeout(unsigned long timeout);\nvoid blk_add_timer(struct request *req);\nvoid blk_delete_timer(struct request *);\n\n\nbool bio_attempt_front_merge(struct request_queue *q, struct request *req,\n\t\t\t     struct bio *bio);\nbool bio_attempt_back_merge(struct request_queue *q, struct request *req,\n\t\t\t    struct bio *bio);\nbool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,\n\t\t\t    unsigned int *request_count,\n\t\t\t    struct request **same_queue_rq);\n\nvoid blk_account_io_start(struct request *req, bool new_io);\nvoid blk_account_io_completion(struct request *req, unsigned int bytes);\nvoid blk_account_io_done(struct request *req);\n\n/*\n * Internal atomic flags for request handling\n */\nenum rq_atomic_flags {\n\tREQ_ATOM_COMPLETE = 0,\n\tREQ_ATOM_STARTED,\n};\n\n/*\n * EH timer and IO completion will both attempt to 'grab' the request, make\n * sure that only one of them succeeds\n */\nstatic inline int blk_mark_rq_complete(struct request *rq)\n{\n\treturn test_and_set_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags);\n}\n\nstatic inline void blk_clear_rq_complete(struct request *rq)\n{\n\tclear_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags);\n}\n\n/*\n * Internal elevator interface\n */\n#define ELV_ON_HASH(rq) ((rq)->cmd_flags & REQ_HASHED)\n\nvoid blk_insert_flush(struct request *rq);\n\nstatic inline struct request *__elv_next_request(struct request_queue *q)\n{\n\tstruct request *rq;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, NULL);\n\n\twhile (1) {\n\t\tif (!list_empty(&q->queue_head)) {\n\t\t\trq = list_entry_rq(q->queue_head.next);\n\t\t\treturn rq;\n\t\t}\n\n\t\t/*\n\t\t * Flush request is running and flush request isn't queueable\n\t\t * in the drive, we can hold the queue till flush request is\n\t\t * finished. Even we don't do this, driver can't dispatch next\n\t\t * requests and will requeue them. And this can improve\n\t\t * throughput too. For example, we have request flush1, write1,\n\t\t * flush 2. flush1 is dispatched, then queue is hold, write1\n\t\t * isn't inserted to queue. After flush1 is finished, flush2\n\t\t * will be dispatched. Since disk cache is already clean,\n\t\t * flush2 will be finished very soon, so looks like flush2 is\n\t\t * folded to flush1.\n\t\t * Since the queue is hold, a flag is set to indicate the queue\n\t\t * should be restarted later. Please see flush_end_io() for\n\t\t * details.\n\t\t */\n\t\tif (fq->flush_pending_idx != fq->flush_running_idx &&\n\t\t\t\t!queue_flush_queueable(q)) {\n\t\t\tfq->flush_queue_delayed = 1;\n\t\t\treturn NULL;\n\t\t}\n\t\tif (unlikely(blk_queue_bypass(q)) ||\n\t\t    !q->elevator->type->ops.elevator_dispatch_fn(q, 0))\n\t\t\treturn NULL;\n\t}\n}\n\nstatic inline void elv_activate_rq(struct request_queue *q, struct request *rq)\n{\n\tstruct elevator_queue *e = q->elevator;\n\n\tif (e->type->ops.elevator_activate_req_fn)\n\t\te->type->ops.elevator_activate_req_fn(q, rq);\n}\n\nstatic inline void elv_deactivate_rq(struct request_queue *q, struct request *rq)\n{\n\tstruct elevator_queue *e = q->elevator;\n\n\tif (e->type->ops.elevator_deactivate_req_fn)\n\t\te->type->ops.elevator_deactivate_req_fn(q, rq);\n}\n\n#ifdef CONFIG_FAIL_IO_TIMEOUT\nint blk_should_fake_timeout(struct request_queue *);\nssize_t part_timeout_show(struct device *, struct device_attribute *, char *);\nssize_t part_timeout_store(struct device *, struct device_attribute *,\n\t\t\t\tconst char *, size_t);\n#else\nstatic inline int blk_should_fake_timeout(struct request_queue *q)\n{\n\treturn 0;\n}\n#endif\n\nint ll_back_merge_fn(struct request_queue *q, struct request *req,\n\t\t     struct bio *bio);\nint ll_front_merge_fn(struct request_queue *q, struct request *req, \n\t\t      struct bio *bio);\nint attempt_back_merge(struct request_queue *q, struct request *rq);\nint attempt_front_merge(struct request_queue *q, struct request *rq);\nint blk_attempt_req_merge(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct request *next);\nvoid blk_recalc_rq_segments(struct request *rq);\nvoid blk_rq_set_mixed_merge(struct request *rq);\nbool blk_rq_merge_ok(struct request *rq, struct bio *bio);\nint blk_try_merge(struct request *rq, struct bio *bio);\n\nvoid blk_queue_congestion_threshold(struct request_queue *q);\n\nint blk_dev_init(void);\n\n\n/*\n * Return the threshold (number of used requests) at which the queue is\n * considered to be congested.  It include a little hysteresis to keep the\n * context switch rate down.\n */\nstatic inline int queue_congestion_on_threshold(struct request_queue *q)\n{\n\treturn q->nr_congestion_on;\n}\n\n/*\n * The threshold at which a queue is considered to be uncongested\n */\nstatic inline int queue_congestion_off_threshold(struct request_queue *q)\n{\n\treturn q->nr_congestion_off;\n}\n\nextern int blk_update_nr_requests(struct request_queue *, unsigned int);\n\n/*\n * Contribute to IO statistics IFF:\n *\n *\ta) it's attached to a gendisk, and\n *\tb) the queue had IO stats enabled when this request was started, and\n *\tc) it's a file system request\n */\nstatic inline int blk_do_io_stat(struct request *rq)\n{\n\treturn rq->rq_disk &&\n\t       (rq->cmd_flags & REQ_IO_STAT) &&\n\t\t(rq->cmd_type == REQ_TYPE_FS);\n}\n\n/*\n * Internal io_context interface\n */\nvoid get_io_context(struct io_context *ioc);\nstruct io_cq *ioc_lookup_icq(struct io_context *ioc, struct request_queue *q);\nstruct io_cq *ioc_create_icq(struct io_context *ioc, struct request_queue *q,\n\t\t\t     gfp_t gfp_mask);\nvoid ioc_clear_queue(struct request_queue *q);\n\nint create_task_io_context(struct task_struct *task, gfp_t gfp_mask, int node);\n\n/**\n * create_io_context - try to create task->io_context\n * @gfp_mask: allocation mask\n * @node: allocation node\n *\n * If %current->io_context is %NULL, allocate a new io_context and install\n * it.  Returns the current %current->io_context which may be %NULL if\n * allocation failed.\n *\n * Note that this function can't be called with IRQ disabled because\n * task_lock which protects %current->io_context is IRQ-unsafe.\n */\nstatic inline struct io_context *create_io_context(gfp_t gfp_mask, int node)\n{\n\tWARN_ON_ONCE(irqs_disabled());\n\tif (unlikely(!current->io_context))\n\t\tcreate_task_io_context(current, gfp_mask, node);\n\treturn current->io_context;\n}\n\n/*\n * Internal throttling interface\n */\n#ifdef CONFIG_BLK_DEV_THROTTLING\nextern bool blk_throtl_bio(struct request_queue *q, struct bio *bio);\nextern void blk_throtl_drain(struct request_queue *q);\nextern int blk_throtl_init(struct request_queue *q);\nextern void blk_throtl_exit(struct request_queue *q);\n#else /* CONFIG_BLK_DEV_THROTTLING */\nstatic inline bool blk_throtl_bio(struct request_queue *q, struct bio *bio)\n{\n\treturn false;\n}\nstatic inline void blk_throtl_drain(struct request_queue *q) { }\nstatic inline int blk_throtl_init(struct request_queue *q) { return 0; }\nstatic inline void blk_throtl_exit(struct request_queue *q) { }\n#endif /* CONFIG_BLK_DEV_THROTTLING */\n\n#endif /* BLK_INTERNAL_H */\n"], "fixing_code": ["/*\n * Functions to sequence FLUSH and FUA writes.\n *\n * Copyright (C) 2011\t\tMax Planck Institute for Gravitational Physics\n * Copyright (C) 2011\t\tTejun Heo <tj@kernel.org>\n *\n * This file is released under the GPLv2.\n *\n * REQ_{FLUSH|FUA} requests are decomposed to sequences consisted of three\n * optional steps - PREFLUSH, DATA and POSTFLUSH - according to the request\n * properties and hardware capability.\n *\n * If a request doesn't have data, only REQ_FLUSH makes sense, which\n * indicates a simple flush request.  If there is data, REQ_FLUSH indicates\n * that the device cache should be flushed before the data is executed, and\n * REQ_FUA means that the data must be on non-volatile media on request\n * completion.\n *\n * If the device doesn't have writeback cache, FLUSH and FUA don't make any\n * difference.  The requests are either completed immediately if there's no\n * data or executed as normal requests otherwise.\n *\n * If the device has writeback cache and supports FUA, REQ_FLUSH is\n * translated to PREFLUSH but REQ_FUA is passed down directly with DATA.\n *\n * If the device has writeback cache and doesn't support FUA, REQ_FLUSH is\n * translated to PREFLUSH and REQ_FUA to POSTFLUSH.\n *\n * The actual execution of flush is double buffered.  Whenever a request\n * needs to execute PRE or POSTFLUSH, it queues at\n * fq->flush_queue[fq->flush_pending_idx].  Once certain criteria are met, a\n * flush is issued and the pending_idx is toggled.  When the flush\n * completes, all the requests which were pending are proceeded to the next\n * step.  This allows arbitrary merging of different types of FLUSH/FUA\n * requests.\n *\n * Currently, the following conditions are used to determine when to issue\n * flush.\n *\n * C1. At any given time, only one flush shall be in progress.  This makes\n *     double buffering sufficient.\n *\n * C2. Flush is deferred if any request is executing DATA of its sequence.\n *     This avoids issuing separate POSTFLUSHes for requests which shared\n *     PREFLUSH.\n *\n * C3. The second condition is ignored if there is a request which has\n *     waited longer than FLUSH_PENDING_TIMEOUT.  This is to avoid\n *     starvation in the unlikely case where there are continuous stream of\n *     FUA (without FLUSH) requests.\n *\n * For devices which support FUA, it isn't clear whether C2 (and thus C3)\n * is beneficial.\n *\n * Note that a sequenced FLUSH/FUA request with DATA is completed twice.\n * Once while executing DATA and again after the whole sequence is\n * complete.  The first completion updates the contained bio but doesn't\n * finish it so that the bio submitter is notified only after the whole\n * sequence is complete.  This is implemented by testing REQ_FLUSH_SEQ in\n * req_bio_endio().\n *\n * The above peculiarity requires that each FLUSH/FUA request has only one\n * bio attached to it, which is guaranteed as they aren't allowed to be\n * merged in the usual way.\n */\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/gfp.h>\n#include <linux/blk-mq.h>\n\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-tag.h\"\n\n/* FLUSH/FUA sequences */\nenum {\n\tREQ_FSEQ_PREFLUSH\t= (1 << 0), /* pre-flushing in progress */\n\tREQ_FSEQ_DATA\t\t= (1 << 1), /* data write in progress */\n\tREQ_FSEQ_POSTFLUSH\t= (1 << 2), /* post-flushing in progress */\n\tREQ_FSEQ_DONE\t\t= (1 << 3),\n\n\tREQ_FSEQ_ACTIONS\t= REQ_FSEQ_PREFLUSH | REQ_FSEQ_DATA |\n\t\t\t\t  REQ_FSEQ_POSTFLUSH,\n\n\t/*\n\t * If flush has been pending longer than the following timeout,\n\t * it's issued even if flush_data requests are still in flight.\n\t */\n\tFLUSH_PENDING_TIMEOUT\t= 5 * HZ,\n};\n\nstatic bool blk_kick_flush(struct request_queue *q,\n\t\t\t   struct blk_flush_queue *fq);\n\nstatic unsigned int blk_flush_policy(unsigned int fflags, struct request *rq)\n{\n\tunsigned int policy = 0;\n\n\tif (blk_rq_sectors(rq))\n\t\tpolicy |= REQ_FSEQ_DATA;\n\n\tif (fflags & REQ_FLUSH) {\n\t\tif (rq->cmd_flags & REQ_FLUSH)\n\t\t\tpolicy |= REQ_FSEQ_PREFLUSH;\n\t\tif (!(fflags & REQ_FUA) && (rq->cmd_flags & REQ_FUA))\n\t\t\tpolicy |= REQ_FSEQ_POSTFLUSH;\n\t}\n\treturn policy;\n}\n\nstatic unsigned int blk_flush_cur_seq(struct request *rq)\n{\n\treturn 1 << ffz(rq->flush.seq);\n}\n\nstatic void blk_flush_restore_request(struct request *rq)\n{\n\t/*\n\t * After flush data completion, @rq->bio is %NULL but we need to\n\t * complete the bio again.  @rq->biotail is guaranteed to equal the\n\t * original @rq->bio.  Restore it.\n\t */\n\trq->bio = rq->biotail;\n\n\t/* make @rq a normal request */\n\trq->cmd_flags &= ~REQ_FLUSH_SEQ;\n\trq->end_io = rq->flush.saved_end_io;\n}\n\nstatic bool blk_flush_queue_rq(struct request *rq, bool add_front)\n{\n\tif (rq->q->mq_ops) {\n\t\tstruct request_queue *q = rq->q;\n\n\t\tblk_mq_add_to_requeue_list(rq, add_front);\n\t\tblk_mq_kick_requeue_list(q);\n\t\treturn false;\n\t} else {\n\t\tif (add_front)\n\t\t\tlist_add(&rq->queuelist, &rq->q->queue_head);\n\t\telse\n\t\t\tlist_add_tail(&rq->queuelist, &rq->q->queue_head);\n\t\treturn true;\n\t}\n}\n\n/**\n * blk_flush_complete_seq - complete flush sequence\n * @rq: FLUSH/FUA request being sequenced\n * @fq: flush queue\n * @seq: sequences to complete (mask of %REQ_FSEQ_*, can be zero)\n * @error: whether an error occurred\n *\n * @rq just completed @seq part of its flush sequence, record the\n * completion and trigger the next step.\n *\n * CONTEXT:\n * spin_lock_irq(q->queue_lock or fq->mq_flush_lock)\n *\n * RETURNS:\n * %true if requests were added to the dispatch queue, %false otherwise.\n */\nstatic bool blk_flush_complete_seq(struct request *rq,\n\t\t\t\t   struct blk_flush_queue *fq,\n\t\t\t\t   unsigned int seq, int error)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tbool queued = false, kicked;\n\n\tBUG_ON(rq->flush.seq & seq);\n\trq->flush.seq |= seq;\n\n\tif (likely(!error))\n\t\tseq = blk_flush_cur_seq(rq);\n\telse\n\t\tseq = REQ_FSEQ_DONE;\n\n\tswitch (seq) {\n\tcase REQ_FSEQ_PREFLUSH:\n\tcase REQ_FSEQ_POSTFLUSH:\n\t\t/* queue for flush */\n\t\tif (list_empty(pending))\n\t\t\tfq->flush_pending_since = jiffies;\n\t\tlist_move_tail(&rq->flush.list, pending);\n\t\tbreak;\n\n\tcase REQ_FSEQ_DATA:\n\t\tlist_move_tail(&rq->flush.list, &fq->flush_data_in_flight);\n\t\tqueued = blk_flush_queue_rq(rq, true);\n\t\tbreak;\n\n\tcase REQ_FSEQ_DONE:\n\t\t/*\n\t\t * @rq was previously adjusted by blk_flush_issue() for\n\t\t * flush sequencing and may already have gone through the\n\t\t * flush data request completion path.  Restore @rq for\n\t\t * normal completion and end it.\n\t\t */\n\t\tBUG_ON(!list_empty(&rq->queuelist));\n\t\tlist_del_init(&rq->flush.list);\n\t\tblk_flush_restore_request(rq);\n\t\tif (q->mq_ops)\n\t\t\tblk_mq_end_request(rq, error);\n\t\telse\n\t\t\t__blk_end_request_all(rq, error);\n\t\tbreak;\n\n\tdefault:\n\t\tBUG();\n\t}\n\n\tkicked = blk_kick_flush(q, fq);\n\treturn kicked | queued;\n}\n\nstatic void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\t/* release the tag's ownership to the req cloned from */\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}\n\n/**\n * blk_kick_flush - consider issuing flush request\n * @q: request_queue being kicked\n * @fq: flush queue\n *\n * Flush related states of @q have changed, consider issuing flush request.\n * Please read the comment at the top of this file for more info.\n *\n * CONTEXT:\n * spin_lock_irq(q->queue_lock or fq->mq_flush_lock)\n *\n * RETURNS:\n * %true if flush was issued, %false otherwise.\n */\nstatic bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time. And acquire the tag's\n\t * ownership for flush req.\n\t */\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t\tfq->orig_rq = first_rq;\n\n\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}\n\nstatic void flush_data_end_io(struct request *rq, int error)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, NULL);\n\n\t/*\n\t * After populating an empty queue, kick it to avoid stall.  Read\n\t * the comment in flush_end_io().\n\t */\n\tif (blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error))\n\t\tblk_run_queue_async(q);\n}\n\nstatic void mq_flush_data_end_io(struct request *rq, int error)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\tunsigned long flags;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, ctx);\n\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\n\t/*\n\t * After populating an empty queue, kick it to avoid stall.  Read\n\t * the comment in flush_end_io().\n\t */\n\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\tif (blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error))\n\t\tblk_mq_run_hw_queue(hctx, true);\n\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}\n\n/**\n * blk_insert_flush - insert a new FLUSH/FUA request\n * @rq: request to insert\n *\n * To be called from __elv_add_request() for %ELEVATOR_INSERT_FLUSH insertions.\n * or __blk_mq_run_hw_queue() to dispatch request.\n * @rq is being submitted.  Analyze what needs to be done and put it on the\n * right queue.\n *\n * CONTEXT:\n * spin_lock_irq(q->queue_lock) in !mq case\n */\nvoid blk_insert_flush(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned int fflags = q->flush_flags;\t/* may change, cache */\n\tunsigned int policy = blk_flush_policy(fflags, rq);\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);\n\n\t/*\n\t * @policy now records what operations need to be done.  Adjust\n\t * REQ_FLUSH and FUA for the driver.\n\t */\n\trq->cmd_flags &= ~REQ_FLUSH;\n\tif (!(fflags & REQ_FUA))\n\t\trq->cmd_flags &= ~REQ_FUA;\n\n\t/*\n\t * An empty flush handed down from a stacking driver may\n\t * translate into nothing if the underlying device does not\n\t * advertise a write-back cache.  In this case, simply\n\t * complete the request.\n\t */\n\tif (!policy) {\n\t\tif (q->mq_ops)\n\t\t\tblk_mq_end_request(rq, 0);\n\t\telse\n\t\t\t__blk_end_bidi_request(rq, 0, 0, 0);\n\t\treturn;\n\t}\n\n\tBUG_ON(rq->bio != rq->biotail); /*assumes zero or single bio rq */\n\n\t/*\n\t * If there's data but flush is not necessary, the request can be\n\t * processed directly without going through flush machinery.  Queue\n\t * for normal execution.\n\t */\n\tif ((policy & REQ_FSEQ_DATA) &&\n\t    !(policy & (REQ_FSEQ_PREFLUSH | REQ_FSEQ_POSTFLUSH))) {\n\t\tif (q->mq_ops) {\n\t\t\tblk_mq_insert_request(rq, false, false, true);\n\t\t} else\n\t\t\tlist_add_tail(&rq->queuelist, &q->queue_head);\n\t\treturn;\n\t}\n\n\t/*\n\t * @rq should go through flush machinery.  Mark it part of flush\n\t * sequence and submit for further processing.\n\t */\n\tmemset(&rq->flush, 0, sizeof(rq->flush));\n\tINIT_LIST_HEAD(&rq->flush.list);\n\trq->cmd_flags |= REQ_FLUSH_SEQ;\n\trq->flush.saved_end_io = rq->end_io; /* Usually NULL */\n\tif (q->mq_ops) {\n\t\trq->end_io = mq_flush_data_end_io;\n\n\t\tspin_lock_irq(&fq->mq_flush_lock);\n\t\tblk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);\n\t\tspin_unlock_irq(&fq->mq_flush_lock);\n\t\treturn;\n\t}\n\trq->end_io = flush_data_end_io;\n\n\tblk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);\n}\n\n/**\n * blkdev_issue_flush - queue a flush\n * @bdev:\tblockdev to issue flush for\n * @gfp_mask:\tmemory allocation flags (for bio_alloc)\n * @error_sector:\terror sector\n *\n * Description:\n *    Issue a flush for the block device in question. Caller can supply\n *    room for storing the error offset in case of a flush error, if they\n *    wish to. If WAIT flag is not passed then caller may check only what\n *    request was pushed in some internal queue for later handling.\n */\nint blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,\n\t\tsector_t *error_sector)\n{\n\tstruct request_queue *q;\n\tstruct bio *bio;\n\tint ret = 0;\n\n\tif (bdev->bd_disk == NULL)\n\t\treturn -ENXIO;\n\n\tq = bdev_get_queue(bdev);\n\tif (!q)\n\t\treturn -ENXIO;\n\n\t/*\n\t * some block devices may not have their queue correctly set up here\n\t * (e.g. loop device without a backing file) and so issuing a flush\n\t * here will panic. Ensure there is a request function before issuing\n\t * the flush.\n\t */\n\tif (!q->make_request_fn)\n\t\treturn -ENXIO;\n\n\tbio = bio_alloc(gfp_mask, 0);\n\tbio->bi_bdev = bdev;\n\n\tret = submit_bio_wait(WRITE_FLUSH, bio);\n\n\t/*\n\t * The driver must store the error location in ->bi_sector, if\n\t * it supports it. For non-stacked drivers, this should be\n\t * copied from blk_rq_pos(rq).\n\t */\n\tif (error_sector)\n\t\t*error_sector = bio->bi_iter.bi_sector;\n\n\tbio_put(bio);\n\treturn ret;\n}\nEXPORT_SYMBOL(blkdev_issue_flush);\n\nstruct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,\n\t\tint node, int cmd_size)\n{\n\tstruct blk_flush_queue *fq;\n\tint rq_sz = sizeof(struct request);\n\n\tfq = kzalloc_node(sizeof(*fq), GFP_KERNEL, node);\n\tif (!fq)\n\t\tgoto fail;\n\n\tif (q->mq_ops) {\n\t\tspin_lock_init(&fq->mq_flush_lock);\n\t\trq_sz = round_up(rq_sz + cmd_size, cache_line_size());\n\t}\n\n\tfq->flush_rq = kzalloc_node(rq_sz, GFP_KERNEL, node);\n\tif (!fq->flush_rq)\n\t\tgoto fail_rq;\n\n\tINIT_LIST_HEAD(&fq->flush_queue[0]);\n\tINIT_LIST_HEAD(&fq->flush_queue[1]);\n\tINIT_LIST_HEAD(&fq->flush_data_in_flight);\n\n\treturn fq;\n\n fail_rq:\n\tkfree(fq);\n fail:\n\treturn NULL;\n}\n\nvoid blk_free_flush_queue(struct blk_flush_queue *fq)\n{\n\t/* bio based request queue hasn't flush queue */\n\tif (!fq)\n\t\treturn;\n\n\tkfree(fq->flush_rq);\n\tkfree(fq);\n}\n", "/*\n * Fast and scalable bitmap tagging variant. Uses sparser bitmaps spread\n * over multiple cachelines to avoid ping-pong between multiple submitters\n * or submitter and completer. Uses rolling wakeups to avoid falling of\n * the scaling cliff when we run out of tags and have to start putting\n * submitters to sleep.\n *\n * Uses active queue tracking to support fairer distribution of tags\n * between multiple submitters when a shared tag map is used.\n *\n * Copyright (C) 2013-2014 Jens Axboe\n */\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/random.h>\n\n#include <linux/blk-mq.h>\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-tag.h\"\n\nstatic bool bt_has_free_tags(struct blk_mq_bitmap_tags *bt)\n{\n\tint i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\t\tint ret;\n\n\t\tret = find_first_zero_bit(&bm->word, bm->depth);\n\t\tif (ret < bm->depth)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nbool blk_mq_has_free_tags(struct blk_mq_tags *tags)\n{\n\tif (!tags)\n\t\treturn true;\n\n\treturn bt_has_free_tags(&tags->bitmap_tags);\n}\n\nstatic inline int bt_index_inc(int index)\n{\n\treturn (index + 1) & (BT_WAIT_QUEUES - 1);\n}\n\nstatic inline void bt_index_atomic_inc(atomic_t *index)\n{\n\tint old = atomic_read(index);\n\tint new = bt_index_inc(old);\n\tatomic_cmpxchg(index, old, new);\n}\n\n/*\n * If a previously inactive queue goes active, bump the active user count.\n */\nbool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)\n{\n\tif (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&\n\t    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\n\t\tatomic_inc(&hctx->tags->active_queues);\n\n\treturn true;\n}\n\n/*\n * Wakeup all potentially sleeping on tags\n */\nvoid blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)\n{\n\tstruct blk_mq_bitmap_tags *bt;\n\tint i, wake_index;\n\n\tbt = &tags->bitmap_tags;\n\twake_index = atomic_read(&bt->wake_index);\n\tfor (i = 0; i < BT_WAIT_QUEUES; i++) {\n\t\tstruct bt_wait_state *bs = &bt->bs[wake_index];\n\n\t\tif (waitqueue_active(&bs->wait))\n\t\t\twake_up(&bs->wait);\n\n\t\twake_index = bt_index_inc(wake_index);\n\t}\n\n\tif (include_reserve) {\n\t\tbt = &tags->breserved_tags;\n\t\tif (waitqueue_active(&bt->bs[0].wait))\n\t\t\twake_up(&bt->bs[0].wait);\n\t}\n}\n\n/*\n * If a previously busy queue goes inactive, potential waiters could now\n * be allowed to queue. Wake them up and check.\n */\nvoid __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct blk_mq_tags *tags = hctx->tags;\n\n\tif (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\n\t\treturn;\n\n\tatomic_dec(&tags->active_queues);\n\n\tblk_mq_tag_wakeup_all(tags, false);\n}\n\n/*\n * For shared tag users, we track the number of currently active users\n * and attempt to provide a fair share of the tag depth for each of them.\n */\nstatic inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t  struct blk_mq_bitmap_tags *bt)\n{\n\tunsigned int depth, users;\n\n\tif (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))\n\t\treturn true;\n\tif (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\n\t\treturn true;\n\n\t/*\n\t * Don't try dividing an ant\n\t */\n\tif (bt->depth == 1)\n\t\treturn true;\n\n\tusers = atomic_read(&hctx->tags->active_queues);\n\tif (!users)\n\t\treturn true;\n\n\t/*\n\t * Allow at least some tags\n\t */\n\tdepth = max((bt->depth + users - 1) / users, 4U);\n\treturn atomic_read(&hctx->nr_active) < depth;\n}\n\nstatic int __bt_get_word(struct blk_align_bitmap *bm, unsigned int last_tag,\n\t\t\t bool nowrap)\n{\n\tint tag, org_last_tag = last_tag;\n\n\twhile (1) {\n\t\ttag = find_next_zero_bit(&bm->word, bm->depth, last_tag);\n\t\tif (unlikely(tag >= bm->depth)) {\n\t\t\t/*\n\t\t\t * We started with an offset, and we didn't reset the\n\t\t\t * offset to 0 in a failure case, so start from 0 to\n\t\t\t * exhaust the map.\n\t\t\t */\n\t\t\tif (org_last_tag && last_tag && !nowrap) {\n\t\t\t\tlast_tag = org_last_tag = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (!test_and_set_bit(tag, &bm->word))\n\t\t\tbreak;\n\n\t\tlast_tag = tag + 1;\n\t\tif (last_tag >= bm->depth - 1)\n\t\t\tlast_tag = 0;\n\t}\n\n\treturn tag;\n}\n\n#define BT_ALLOC_RR(tags) (tags->alloc_policy == BLK_TAG_ALLOC_RR)\n\n/*\n * Straight forward bitmap tag implementation, where each bit is a tag\n * (cleared == free, and set == busy). The small twist is using per-cpu\n * last_tag caches, which blk-mq stores in the blk_mq_ctx software queue\n * contexts. This enables us to drastically limit the space searched,\n * without dirtying an extra shared cacheline like we would if we stored\n * the cache value inside the shared blk_mq_bitmap_tags structure. On top\n * of that, each word of tags is in a separate cacheline. This means that\n * multiple users will tend to stick to different cachelines, at least\n * until the map is exhausted.\n */\nstatic int __bt_get(struct blk_mq_hw_ctx *hctx, struct blk_mq_bitmap_tags *bt,\n\t\t    unsigned int *tag_cache, struct blk_mq_tags *tags)\n{\n\tunsigned int last_tag, org_last_tag;\n\tint index, i, tag;\n\n\tif (!hctx_may_queue(hctx, bt))\n\t\treturn -1;\n\n\tlast_tag = org_last_tag = *tag_cache;\n\tindex = TAG_TO_INDEX(bt, last_tag);\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\ttag = __bt_get_word(&bt->map[index], TAG_TO_BIT(bt, last_tag),\n\t\t\t\t    BT_ALLOC_RR(tags));\n\t\tif (tag != -1) {\n\t\t\ttag += (index << bt->bits_per_word);\n\t\t\tgoto done;\n\t\t}\n\n\t\t/*\n\t\t * Jump to next index, and reset the last tag to be the\n\t\t * first tag of that index\n\t\t */\n\t\tindex++;\n\t\tlast_tag = (index << bt->bits_per_word);\n\n\t\tif (index >= bt->map_nr) {\n\t\t\tindex = 0;\n\t\t\tlast_tag = 0;\n\t\t}\n\t}\n\n\t*tag_cache = 0;\n\treturn -1;\n\n\t/*\n\t * Only update the cache from the allocation path, if we ended\n\t * up using the specific cached tag.\n\t */\ndone:\n\tif (tag == org_last_tag || unlikely(BT_ALLOC_RR(tags))) {\n\t\tlast_tag = tag + 1;\n\t\tif (last_tag >= bt->depth - 1)\n\t\t\tlast_tag = 0;\n\n\t\t*tag_cache = last_tag;\n\t}\n\n\treturn tag;\n}\n\nstatic struct bt_wait_state *bt_wait_ptr(struct blk_mq_bitmap_tags *bt,\n\t\t\t\t\t struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bt_wait_state *bs;\n\tint wait_index;\n\n\tif (!hctx)\n\t\treturn &bt->bs[0];\n\n\twait_index = atomic_read(&hctx->wait_index);\n\tbs = &bt->bs[wait_index];\n\tbt_index_atomic_inc(&hctx->wait_index);\n\treturn bs;\n}\n\nstatic int bt_get(struct blk_mq_alloc_data *data,\n\t\tstruct blk_mq_bitmap_tags *bt,\n\t\tstruct blk_mq_hw_ctx *hctx,\n\t\tunsigned int *last_tag, struct blk_mq_tags *tags)\n{\n\tstruct bt_wait_state *bs;\n\tDEFINE_WAIT(wait);\n\tint tag;\n\n\ttag = __bt_get(hctx, bt, last_tag, tags);\n\tif (tag != -1)\n\t\treturn tag;\n\n\tif (!(data->gfp & __GFP_WAIT))\n\t\treturn -1;\n\n\tbs = bt_wait_ptr(bt, hctx);\n\tdo {\n\t\tprepare_to_wait(&bs->wait, &wait, TASK_UNINTERRUPTIBLE);\n\n\t\ttag = __bt_get(hctx, bt, last_tag, tags);\n\t\tif (tag != -1)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * We're out of tags on this hardware queue, kick any\n\t\t * pending IO submits before going to sleep waiting for\n\t\t * some to complete. Note that hctx can be NULL here for\n\t\t * reserved tag allocation.\n\t\t */\n\t\tif (hctx)\n\t\t\tblk_mq_run_hw_queue(hctx, false);\n\n\t\t/*\n\t\t * Retry tag allocation after running the hardware queue,\n\t\t * as running the queue may also have found completions.\n\t\t */\n\t\ttag = __bt_get(hctx, bt, last_tag, tags);\n\t\tif (tag != -1)\n\t\t\tbreak;\n\n\t\tblk_mq_put_ctx(data->ctx);\n\n\t\tio_schedule();\n\n\t\tdata->ctx = blk_mq_get_ctx(data->q);\n\t\tdata->hctx = data->q->mq_ops->map_queue(data->q,\n\t\t\t\tdata->ctx->cpu);\n\t\tif (data->reserved) {\n\t\t\tbt = &data->hctx->tags->breserved_tags;\n\t\t} else {\n\t\t\tlast_tag = &data->ctx->last_tag;\n\t\t\thctx = data->hctx;\n\t\t\tbt = &hctx->tags->bitmap_tags;\n\t\t}\n\t\tfinish_wait(&bs->wait, &wait);\n\t\tbs = bt_wait_ptr(bt, hctx);\n\t} while (1);\n\n\tfinish_wait(&bs->wait, &wait);\n\treturn tag;\n}\n\nstatic unsigned int __blk_mq_get_tag(struct blk_mq_alloc_data *data)\n{\n\tint tag;\n\n\ttag = bt_get(data, &data->hctx->tags->bitmap_tags, data->hctx,\n\t\t\t&data->ctx->last_tag, data->hctx->tags);\n\tif (tag >= 0)\n\t\treturn tag + data->hctx->tags->nr_reserved_tags;\n\n\treturn BLK_MQ_TAG_FAIL;\n}\n\nstatic unsigned int __blk_mq_get_reserved_tag(struct blk_mq_alloc_data *data)\n{\n\tint tag, zero = 0;\n\n\tif (unlikely(!data->hctx->tags->nr_reserved_tags)) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn BLK_MQ_TAG_FAIL;\n\t}\n\n\ttag = bt_get(data, &data->hctx->tags->breserved_tags, NULL, &zero,\n\t\tdata->hctx->tags);\n\tif (tag < 0)\n\t\treturn BLK_MQ_TAG_FAIL;\n\n\treturn tag;\n}\n\nunsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)\n{\n\tif (!data->reserved)\n\t\treturn __blk_mq_get_tag(data);\n\n\treturn __blk_mq_get_reserved_tag(data);\n}\n\nstatic struct bt_wait_state *bt_wake_ptr(struct blk_mq_bitmap_tags *bt)\n{\n\tint i, wake_index;\n\n\twake_index = atomic_read(&bt->wake_index);\n\tfor (i = 0; i < BT_WAIT_QUEUES; i++) {\n\t\tstruct bt_wait_state *bs = &bt->bs[wake_index];\n\n\t\tif (waitqueue_active(&bs->wait)) {\n\t\t\tint o = atomic_read(&bt->wake_index);\n\t\t\tif (wake_index != o)\n\t\t\t\tatomic_cmpxchg(&bt->wake_index, o, wake_index);\n\n\t\t\treturn bs;\n\t\t}\n\n\t\twake_index = bt_index_inc(wake_index);\n\t}\n\n\treturn NULL;\n}\n\nstatic void bt_clear_tag(struct blk_mq_bitmap_tags *bt, unsigned int tag)\n{\n\tconst int index = TAG_TO_INDEX(bt, tag);\n\tstruct bt_wait_state *bs;\n\tint wait_cnt;\n\n\tclear_bit(TAG_TO_BIT(bt, tag), &bt->map[index].word);\n\n\t/* Ensure that the wait list checks occur after clear_bit(). */\n\tsmp_mb();\n\n\tbs = bt_wake_ptr(bt);\n\tif (!bs)\n\t\treturn;\n\n\twait_cnt = atomic_dec_return(&bs->wait_cnt);\n\tif (unlikely(wait_cnt < 0))\n\t\twait_cnt = atomic_inc_return(&bs->wait_cnt);\n\tif (wait_cnt == 0) {\n\t\tatomic_add(bt->wake_cnt, &bs->wait_cnt);\n\t\tbt_index_atomic_inc(&bt->wake_index);\n\t\twake_up(&bs->wait);\n\t}\n}\n\nvoid blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag,\n\t\t    unsigned int *last_tag)\n{\n\tstruct blk_mq_tags *tags = hctx->tags;\n\n\tif (tag >= tags->nr_reserved_tags) {\n\t\tconst int real_tag = tag - tags->nr_reserved_tags;\n\n\t\tBUG_ON(real_tag >= tags->nr_tags);\n\t\tbt_clear_tag(&tags->bitmap_tags, real_tag);\n\t\tif (likely(tags->alloc_policy == BLK_TAG_ALLOC_FIFO))\n\t\t\t*last_tag = real_tag;\n\t} else {\n\t\tBUG_ON(tag >= tags->nr_reserved_tags);\n\t\tbt_clear_tag(&tags->breserved_tags, tag);\n\t}\n}\n\nstatic void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = hctx->tags->rqs[off + bit];\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n\nstatic void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = tags->rqs[off + bit];\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n\nvoid blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,\n\t\tvoid *priv)\n{\n\tif (tags->nr_reserved_tags)\n\t\tbt_tags_for_each(tags, &tags->breserved_tags, 0, fn, priv, true);\n\tbt_tags_for_each(tags, &tags->bitmap_tags, tags->nr_reserved_tags, fn, priv,\n\t\t\tfalse);\n}\nEXPORT_SYMBOL(blk_mq_all_tag_busy_iter);\n\nvoid blk_mq_tag_busy_iter(struct blk_mq_hw_ctx *hctx, busy_iter_fn *fn,\n\t\tvoid *priv)\n{\n\tstruct blk_mq_tags *tags = hctx->tags;\n\n\tif (tags->nr_reserved_tags)\n\t\tbt_for_each(hctx, &tags->breserved_tags, 0, fn, priv, true);\n\tbt_for_each(hctx, &tags->bitmap_tags, tags->nr_reserved_tags, fn, priv,\n\t\t\tfalse);\n}\nEXPORT_SYMBOL(blk_mq_tag_busy_iter);\n\nstatic unsigned int bt_unused_tags(struct blk_mq_bitmap_tags *bt)\n{\n\tunsigned int i, used;\n\n\tfor (i = 0, used = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tused += bitmap_weight(&bm->word, bm->depth);\n\t}\n\n\treturn bt->depth - used;\n}\n\nstatic void bt_update_count(struct blk_mq_bitmap_tags *bt,\n\t\t\t    unsigned int depth)\n{\n\tunsigned int tags_per_word = 1U << bt->bits_per_word;\n\tunsigned int map_depth = depth;\n\n\tif (depth) {\n\t\tint i;\n\n\t\tfor (i = 0; i < bt->map_nr; i++) {\n\t\t\tbt->map[i].depth = min(map_depth, tags_per_word);\n\t\t\tmap_depth -= bt->map[i].depth;\n\t\t}\n\t}\n\n\tbt->wake_cnt = BT_WAIT_BATCH;\n\tif (bt->wake_cnt > depth / BT_WAIT_QUEUES)\n\t\tbt->wake_cnt = max(1U, depth / BT_WAIT_QUEUES);\n\n\tbt->depth = depth;\n}\n\nstatic int bt_alloc(struct blk_mq_bitmap_tags *bt, unsigned int depth,\n\t\t\tint node, bool reserved)\n{\n\tint i;\n\n\tbt->bits_per_word = ilog2(BITS_PER_LONG);\n\n\t/*\n\t * Depth can be zero for reserved tags, that's not a failure\n\t * condition.\n\t */\n\tif (depth) {\n\t\tunsigned int nr, tags_per_word;\n\n\t\ttags_per_word = (1 << bt->bits_per_word);\n\n\t\t/*\n\t\t * If the tag space is small, shrink the number of tags\n\t\t * per word so we spread over a few cachelines, at least.\n\t\t * If less than 4 tags, just forget about it, it's not\n\t\t * going to work optimally anyway.\n\t\t */\n\t\tif (depth >= 4) {\n\t\t\twhile (tags_per_word * 4 > depth) {\n\t\t\t\tbt->bits_per_word--;\n\t\t\t\ttags_per_word = (1 << bt->bits_per_word);\n\t\t\t}\n\t\t}\n\n\t\tnr = ALIGN(depth, tags_per_word) / tags_per_word;\n\t\tbt->map = kzalloc_node(nr * sizeof(struct blk_align_bitmap),\n\t\t\t\t\t\tGFP_KERNEL, node);\n\t\tif (!bt->map)\n\t\t\treturn -ENOMEM;\n\n\t\tbt->map_nr = nr;\n\t}\n\n\tbt->bs = kzalloc(BT_WAIT_QUEUES * sizeof(*bt->bs), GFP_KERNEL);\n\tif (!bt->bs) {\n\t\tkfree(bt->map);\n\t\tbt->map = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tbt_update_count(bt, depth);\n\n\tfor (i = 0; i < BT_WAIT_QUEUES; i++) {\n\t\tinit_waitqueue_head(&bt->bs[i].wait);\n\t\tatomic_set(&bt->bs[i].wait_cnt, bt->wake_cnt);\n\t}\n\n\treturn 0;\n}\n\nstatic void bt_free(struct blk_mq_bitmap_tags *bt)\n{\n\tkfree(bt->map);\n\tkfree(bt->bs);\n}\n\nstatic struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,\n\t\t\t\t\t\t   int node, int alloc_policy)\n{\n\tunsigned int depth = tags->nr_tags - tags->nr_reserved_tags;\n\n\ttags->alloc_policy = alloc_policy;\n\n\tif (bt_alloc(&tags->bitmap_tags, depth, node, false))\n\t\tgoto enomem;\n\tif (bt_alloc(&tags->breserved_tags, tags->nr_reserved_tags, node, true))\n\t\tgoto enomem;\n\n\treturn tags;\nenomem:\n\tbt_free(&tags->bitmap_tags);\n\tkfree(tags);\n\treturn NULL;\n}\n\nstruct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,\n\t\t\t\t     unsigned int reserved_tags,\n\t\t\t\t     int node, int alloc_policy)\n{\n\tstruct blk_mq_tags *tags;\n\n\tif (total_tags > BLK_MQ_TAG_MAX) {\n\t\tpr_err(\"blk-mq: tag depth too large\\n\");\n\t\treturn NULL;\n\t}\n\n\ttags = kzalloc_node(sizeof(*tags), GFP_KERNEL, node);\n\tif (!tags)\n\t\treturn NULL;\n\n\tif (!zalloc_cpumask_var(&tags->cpumask, GFP_KERNEL)) {\n\t\tkfree(tags);\n\t\treturn NULL;\n\t}\n\n\ttags->nr_tags = total_tags;\n\ttags->nr_reserved_tags = reserved_tags;\n\n\treturn blk_mq_init_bitmap_tags(tags, node, alloc_policy);\n}\n\nvoid blk_mq_free_tags(struct blk_mq_tags *tags)\n{\n\tbt_free(&tags->bitmap_tags);\n\tbt_free(&tags->breserved_tags);\n\tkfree(tags);\n}\n\nvoid blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *tag)\n{\n\tunsigned int depth = tags->nr_tags - tags->nr_reserved_tags;\n\n\t*tag = prandom_u32() % depth;\n}\n\nint blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int tdepth)\n{\n\ttdepth -= tags->nr_reserved_tags;\n\tif (tdepth > tags->nr_tags)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Don't need (or can't) update reserved tags here, they remain\n\t * static and should never need resizing.\n\t */\n\tbt_update_count(&tags->bitmap_tags, tdepth);\n\tblk_mq_tag_wakeup_all(tags, false);\n\treturn 0;\n}\n\n/**\n * blk_mq_unique_tag() - return a tag that is unique queue-wide\n * @rq: request for which to compute a unique tag\n *\n * The tag field in struct request is unique per hardware queue but not over\n * all hardware queues. Hence this function that returns a tag with the\n * hardware context index in the upper bits and the per hardware queue tag in\n * the lower bits.\n *\n * Note: When called for a request that is queued on a non-multiqueue request\n * queue, the hardware context index is set to zero.\n */\nu32 blk_mq_unique_tag(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_hw_ctx *hctx;\n\tint hwq = 0;\n\n\tif (q->mq_ops) {\n\t\thctx = q->mq_ops->map_queue(q, rq->mq_ctx->cpu);\n\t\thwq = hctx->queue_num;\n\t}\n\n\treturn (hwq << BLK_MQ_UNIQUE_TAG_BITS) |\n\t\t(rq->tag & BLK_MQ_UNIQUE_TAG_MASK);\n}\nEXPORT_SYMBOL(blk_mq_unique_tag);\n\nssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page)\n{\n\tchar *orig_page = page;\n\tunsigned int free, res;\n\n\tif (!tags)\n\t\treturn 0;\n\n\tpage += sprintf(page, \"nr_tags=%u, reserved_tags=%u, \"\n\t\t\t\"bits_per_word=%u\\n\",\n\t\t\ttags->nr_tags, tags->nr_reserved_tags,\n\t\t\ttags->bitmap_tags.bits_per_word);\n\n\tfree = bt_unused_tags(&tags->bitmap_tags);\n\tres = bt_unused_tags(&tags->breserved_tags);\n\n\tpage += sprintf(page, \"nr_free=%u, nr_reserved=%u\\n\", free, res);\n\tpage += sprintf(page, \"active_queues=%u\\n\", atomic_read(&tags->active_queues));\n\n\treturn page - orig_page;\n}\n", "#ifndef INT_BLK_MQ_TAG_H\n#define INT_BLK_MQ_TAG_H\n\n#include \"blk-mq.h\"\n\nenum {\n\tBT_WAIT_QUEUES\t= 8,\n\tBT_WAIT_BATCH\t= 8,\n};\n\nstruct bt_wait_state {\n\tatomic_t wait_cnt;\n\twait_queue_head_t wait;\n} ____cacheline_aligned_in_smp;\n\n#define TAG_TO_INDEX(bt, tag)\t((tag) >> (bt)->bits_per_word)\n#define TAG_TO_BIT(bt, tag)\t((tag) & ((1 << (bt)->bits_per_word) - 1))\n\nstruct blk_mq_bitmap_tags {\n\tunsigned int depth;\n\tunsigned int wake_cnt;\n\tunsigned int bits_per_word;\n\n\tunsigned int map_nr;\n\tstruct blk_align_bitmap *map;\n\n\tatomic_t wake_index;\n\tstruct bt_wait_state *bs;\n};\n\n/*\n * Tag address space map.\n */\nstruct blk_mq_tags {\n\tunsigned int nr_tags;\n\tunsigned int nr_reserved_tags;\n\n\tatomic_t active_queues;\n\n\tstruct blk_mq_bitmap_tags bitmap_tags;\n\tstruct blk_mq_bitmap_tags breserved_tags;\n\n\tstruct request **rqs;\n\tstruct list_head page_list;\n\n\tint alloc_policy;\n\tcpumask_var_t cpumask;\n};\n\n\nextern struct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags, unsigned int reserved_tags, int node, int alloc_policy);\nextern void blk_mq_free_tags(struct blk_mq_tags *tags);\n\nextern unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data);\nextern void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag, unsigned int *last_tag);\nextern bool blk_mq_has_free_tags(struct blk_mq_tags *tags);\nextern ssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page);\nextern void blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *last_tag);\nextern int blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int depth);\nextern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);\n\nenum {\n\tBLK_MQ_TAG_CACHE_MIN\t= 1,\n\tBLK_MQ_TAG_CACHE_MAX\t= 64,\n};\n\nenum {\n\tBLK_MQ_TAG_FAIL\t\t= -1U,\n\tBLK_MQ_TAG_MIN\t\t= BLK_MQ_TAG_CACHE_MIN,\n\tBLK_MQ_TAG_MAX\t\t= BLK_MQ_TAG_FAIL - 1,\n};\n\nextern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);\nextern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);\n\nstatic inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)\n{\n\tif (!(hctx->flags & BLK_MQ_F_TAG_SHARED))\n\t\treturn false;\n\n\treturn __blk_mq_tag_busy(hctx);\n}\n\nstatic inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)\n{\n\tif (!(hctx->flags & BLK_MQ_F_TAG_SHARED))\n\t\treturn;\n\n\t__blk_mq_tag_idle(hctx);\n}\n\n/*\n * This helper should only be used for flush request to share tag\n * with the request cloned from, and both the two requests can't be\n * in flight at the same time. The caller has to make sure the tag\n * can't be freed.\n */\nstatic inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,\n\t\tunsigned int tag, struct request *rq)\n{\n\thctx->tags->rqs[tag] = rq;\n}\n\n#endif\n", "/*\n * Block multiqueue core code\n *\n * Copyright (C) 2013-2014 Jens Axboe\n * Copyright (C) 2013-2014 Christoph Hellwig\n */\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/backing-dev.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/mm.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/smp.h>\n#include <linux/llist.h>\n#include <linux/list_sort.h>\n#include <linux/cpu.h>\n#include <linux/cache.h>\n#include <linux/sched/sysctl.h>\n#include <linux/delay.h>\n#include <linux/crash_dump.h>\n\n#include <trace/events/block.h>\n\n#include <linux/blk-mq.h>\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-tag.h\"\n\nstatic DEFINE_MUTEX(all_q_mutex);\nstatic LIST_HEAD(all_q_list);\n\nstatic void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx);\n\n/*\n * Check if any of the ctx's have pending work in this hardware queue\n */\nstatic bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < hctx->ctx_map.size; i++)\n\t\tif (hctx->ctx_map.map[i].word)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline struct blk_align_bitmap *get_bm(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t      struct blk_mq_ctx *ctx)\n{\n\treturn &hctx->ctx_map.map[ctx->index_hw / hctx->ctx_map.bits_per_word];\n}\n\n#define CTX_TO_BIT(hctx, ctx)\t\\\n\t((ctx)->index_hw & ((hctx)->ctx_map.bits_per_word - 1))\n\n/*\n * Mark this ctx as having pending work in this hardware queue\n */\nstatic void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t     struct blk_mq_ctx *ctx)\n{\n\tstruct blk_align_bitmap *bm = get_bm(hctx, ctx);\n\n\tif (!test_bit(CTX_TO_BIT(hctx, ctx), &bm->word))\n\t\tset_bit(CTX_TO_BIT(hctx, ctx), &bm->word);\n}\n\nstatic void blk_mq_hctx_clear_pending(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t      struct blk_mq_ctx *ctx)\n{\n\tstruct blk_align_bitmap *bm = get_bm(hctx, ctx);\n\n\tclear_bit(CTX_TO_BIT(hctx, ctx), &bm->word);\n}\n\nstatic int blk_mq_queue_enter(struct request_queue *q, gfp_t gfp)\n{\n\twhile (true) {\n\t\tint ret;\n\n\t\tif (percpu_ref_tryget_live(&q->mq_usage_counter))\n\t\t\treturn 0;\n\n\t\tif (!(gfp & __GFP_WAIT))\n\t\t\treturn -EBUSY;\n\n\t\tret = wait_event_interruptible(q->mq_freeze_wq,\n\t\t\t\t!atomic_read(&q->mq_freeze_depth) ||\n\t\t\t\tblk_queue_dying(q));\n\t\tif (blk_queue_dying(q))\n\t\t\treturn -ENODEV;\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n}\n\nstatic void blk_mq_queue_exit(struct request_queue *q)\n{\n\tpercpu_ref_put(&q->mq_usage_counter);\n}\n\nstatic void blk_mq_usage_counter_release(struct percpu_ref *ref)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(ref, struct request_queue, mq_usage_counter);\n\n\twake_up_all(&q->mq_freeze_wq);\n}\n\nvoid blk_mq_freeze_queue_start(struct request_queue *q)\n{\n\tint freeze_depth;\n\n\tfreeze_depth = atomic_inc_return(&q->mq_freeze_depth);\n\tif (freeze_depth == 1) {\n\t\tpercpu_ref_kill(&q->mq_usage_counter);\n\t\tblk_mq_run_hw_queues(q, false);\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue_start);\n\nstatic void blk_mq_freeze_queue_wait(struct request_queue *q)\n{\n\twait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->mq_usage_counter));\n}\n\n/*\n * Guarantee no request is in use, so we can change any data structure of\n * the queue afterward.\n */\nvoid blk_mq_freeze_queue(struct request_queue *q)\n{\n\tblk_mq_freeze_queue_start(q);\n\tblk_mq_freeze_queue_wait(q);\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue);\n\nvoid blk_mq_unfreeze_queue(struct request_queue *q)\n{\n\tint freeze_depth;\n\n\tfreeze_depth = atomic_dec_return(&q->mq_freeze_depth);\n\tWARN_ON_ONCE(freeze_depth < 0);\n\tif (!freeze_depth) {\n\t\tpercpu_ref_reinit(&q->mq_usage_counter);\n\t\twake_up_all(&q->mq_freeze_wq);\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);\n\nvoid blk_mq_wake_waiters(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tif (blk_mq_hw_queue_mapped(hctx))\n\t\t\tblk_mq_tag_wakeup_all(hctx->tags, true);\n\n\t/*\n\t * If we are called because the queue has now been marked as\n\t * dying, we need to ensure that processes currently waiting on\n\t * the queue are notified as well.\n\t */\n\twake_up_all(&q->mq_freeze_wq);\n}\n\nbool blk_mq_can_queue(struct blk_mq_hw_ctx *hctx)\n{\n\treturn blk_mq_has_free_tags(hctx->tags);\n}\nEXPORT_SYMBOL(blk_mq_can_queue);\n\nstatic void blk_mq_rq_ctx_init(struct request_queue *q, struct blk_mq_ctx *ctx,\n\t\t\t       struct request *rq, unsigned int rw_flags)\n{\n\tif (blk_queue_io_stat(q))\n\t\trw_flags |= REQ_IO_STAT;\n\n\tINIT_LIST_HEAD(&rq->queuelist);\n\t/* csd/requeue_work/fifo_time is initialized before use */\n\trq->q = q;\n\trq->mq_ctx = ctx;\n\trq->cmd_flags |= rw_flags;\n\t/* do not touch atomic flags, it needs atomic ops against the timer */\n\trq->cpu = -1;\n\tINIT_HLIST_NODE(&rq->hash);\n\tRB_CLEAR_NODE(&rq->rb_node);\n\trq->rq_disk = NULL;\n\trq->part = NULL;\n\trq->start_time = jiffies;\n#ifdef CONFIG_BLK_CGROUP\n\trq->rl = NULL;\n\tset_start_time_ns(rq);\n\trq->io_start_time_ns = 0;\n#endif\n\trq->nr_phys_segments = 0;\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\trq->nr_integrity_segments = 0;\n#endif\n\trq->special = NULL;\n\t/* tag was already set */\n\trq->errors = 0;\n\n\trq->cmd = rq->__cmd;\n\n\trq->extra_len = 0;\n\trq->sense_len = 0;\n\trq->resid_len = 0;\n\trq->sense = NULL;\n\n\tINIT_LIST_HEAD(&rq->timeout_list);\n\trq->timeout = 0;\n\n\trq->end_io = NULL;\n\trq->end_io_data = NULL;\n\trq->next_rq = NULL;\n\n\tctx->rq_dispatched[rw_is_sync(rw_flags)]++;\n}\n\nstatic struct request *\n__blk_mq_alloc_request(struct blk_mq_alloc_data *data, int rw)\n{\n\tstruct request *rq;\n\tunsigned int tag;\n\n\ttag = blk_mq_get_tag(data);\n\tif (tag != BLK_MQ_TAG_FAIL) {\n\t\trq = data->hctx->tags->rqs[tag];\n\n\t\tif (blk_mq_tag_busy(data->hctx)) {\n\t\t\trq->cmd_flags = REQ_MQ_INFLIGHT;\n\t\t\tatomic_inc(&data->hctx->nr_active);\n\t\t}\n\n\t\trq->tag = tag;\n\t\tblk_mq_rq_ctx_init(data->q, data->ctx, rq, rw);\n\t\treturn rq;\n\t}\n\n\treturn NULL;\n}\n\nstruct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,\n\t\tbool reserved)\n{\n\tstruct blk_mq_ctx *ctx;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct request *rq;\n\tstruct blk_mq_alloc_data alloc_data;\n\tint ret;\n\n\tret = blk_mq_queue_enter(q, gfp);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tctx = blk_mq_get_ctx(q);\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\tblk_mq_set_alloc_data(&alloc_data, q, gfp & ~__GFP_WAIT,\n\t\t\treserved, ctx, hctx);\n\n\trq = __blk_mq_alloc_request(&alloc_data, rw);\n\tif (!rq && (gfp & __GFP_WAIT)) {\n\t\t__blk_mq_run_hw_queue(hctx);\n\t\tblk_mq_put_ctx(ctx);\n\n\t\tctx = blk_mq_get_ctx(q);\n\t\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\t\tblk_mq_set_alloc_data(&alloc_data, q, gfp, reserved, ctx,\n\t\t\t\thctx);\n\t\trq =  __blk_mq_alloc_request(&alloc_data, rw);\n\t\tctx = alloc_data.ctx;\n\t}\n\tblk_mq_put_ctx(ctx);\n\tif (!rq) {\n\t\tblk_mq_queue_exit(q);\n\t\treturn ERR_PTR(-EWOULDBLOCK);\n\t}\n\treturn rq;\n}\nEXPORT_SYMBOL(blk_mq_alloc_request);\n\nstatic void __blk_mq_free_request(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t  struct blk_mq_ctx *ctx, struct request *rq)\n{\n\tconst int tag = rq->tag;\n\tstruct request_queue *q = rq->q;\n\n\tif (rq->cmd_flags & REQ_MQ_INFLIGHT)\n\t\tatomic_dec(&hctx->nr_active);\n\trq->cmd_flags = 0;\n\n\tclear_bit(REQ_ATOM_STARTED, &rq->atomic_flags);\n\tblk_mq_put_tag(hctx, tag, &ctx->last_tag);\n\tblk_mq_queue_exit(q);\n}\n\nvoid blk_mq_free_hctx_request(struct blk_mq_hw_ctx *hctx, struct request *rq)\n{\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\n\tctx->rq_completed[rq_is_sync(rq)]++;\n\t__blk_mq_free_request(hctx, ctx, rq);\n\n}\nEXPORT_SYMBOL_GPL(blk_mq_free_hctx_request);\n\nvoid blk_mq_free_request(struct request *rq)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct request_queue *q = rq->q;\n\n\thctx = q->mq_ops->map_queue(q, rq->mq_ctx->cpu);\n\tblk_mq_free_hctx_request(hctx, rq);\n}\nEXPORT_SYMBOL_GPL(blk_mq_free_request);\n\ninline void __blk_mq_end_request(struct request *rq, int error)\n{\n\tblk_account_io_done(rq);\n\n\tif (rq->end_io) {\n\t\trq->end_io(rq, error);\n\t} else {\n\t\tif (unlikely(blk_bidi_rq(rq)))\n\t\t\tblk_mq_free_request(rq->next_rq);\n\t\tblk_mq_free_request(rq);\n\t}\n}\nEXPORT_SYMBOL(__blk_mq_end_request);\n\nvoid blk_mq_end_request(struct request *rq, int error)\n{\n\tif (blk_update_request(rq, error, blk_rq_bytes(rq)))\n\t\tBUG();\n\t__blk_mq_end_request(rq, error);\n}\nEXPORT_SYMBOL(blk_mq_end_request);\n\nstatic void __blk_mq_complete_request_remote(void *data)\n{\n\tstruct request *rq = data;\n\n\trq->q->softirq_done_fn(rq);\n}\n\nstatic void blk_mq_ipi_complete_request(struct request *rq)\n{\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\tbool shared = false;\n\tint cpu;\n\n\tif (!test_bit(QUEUE_FLAG_SAME_COMP, &rq->q->queue_flags)) {\n\t\trq->q->softirq_done_fn(rq);\n\t\treturn;\n\t}\n\n\tcpu = get_cpu();\n\tif (!test_bit(QUEUE_FLAG_SAME_FORCE, &rq->q->queue_flags))\n\t\tshared = cpus_share_cache(cpu, ctx->cpu);\n\n\tif (cpu != ctx->cpu && !shared && cpu_online(ctx->cpu)) {\n\t\trq->csd.func = __blk_mq_complete_request_remote;\n\t\trq->csd.info = rq;\n\t\trq->csd.flags = 0;\n\t\tsmp_call_function_single_async(ctx->cpu, &rq->csd);\n\t} else {\n\t\trq->q->softirq_done_fn(rq);\n\t}\n\tput_cpu();\n}\n\nvoid __blk_mq_complete_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\tif (!q->softirq_done_fn)\n\t\tblk_mq_end_request(rq, rq->errors);\n\telse\n\t\tblk_mq_ipi_complete_request(rq);\n}\n\n/**\n * blk_mq_complete_request - end I/O on a request\n * @rq:\t\tthe request being processed\n *\n * Description:\n *\tEnds all I/O on a request. It does not handle partial completions.\n *\tThe actual completion happens out-of-order, through a IPI handler.\n **/\nvoid blk_mq_complete_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\tif (unlikely(blk_should_fake_timeout(q)))\n\t\treturn;\n\tif (!blk_mark_rq_complete(rq))\n\t\t__blk_mq_complete_request(rq);\n}\nEXPORT_SYMBOL(blk_mq_complete_request);\n\nint blk_mq_request_started(struct request *rq)\n{\n\treturn test_bit(REQ_ATOM_STARTED, &rq->atomic_flags);\n}\nEXPORT_SYMBOL_GPL(blk_mq_request_started);\n\nvoid blk_mq_start_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\ttrace_block_rq_issue(q, rq);\n\n\trq->resid_len = blk_rq_bytes(rq);\n\tif (unlikely(blk_bidi_rq(rq)))\n\t\trq->next_rq->resid_len = blk_rq_bytes(rq->next_rq);\n\n\tblk_add_timer(rq);\n\n\t/*\n\t * Ensure that ->deadline is visible before set the started\n\t * flag and clear the completed flag.\n\t */\n\tsmp_mb__before_atomic();\n\n\t/*\n\t * Mark us as started and clear complete. Complete might have been\n\t * set if requeue raced with timeout, which then marked it as\n\t * complete. So be sure to clear complete again when we start\n\t * the request, otherwise we'll ignore the completion event.\n\t */\n\tif (!test_bit(REQ_ATOM_STARTED, &rq->atomic_flags))\n\t\tset_bit(REQ_ATOM_STARTED, &rq->atomic_flags);\n\tif (test_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags))\n\t\tclear_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags);\n\n\tif (q->dma_drain_size && blk_rq_bytes(rq)) {\n\t\t/*\n\t\t * Make sure space for the drain appears.  We know we can do\n\t\t * this because max_hw_segments has been adjusted to be one\n\t\t * fewer than the device can handle.\n\t\t */\n\t\trq->nr_phys_segments++;\n\t}\n}\nEXPORT_SYMBOL(blk_mq_start_request);\n\nstatic void __blk_mq_requeue_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\ttrace_block_rq_requeue(q, rq);\n\n\tif (test_and_clear_bit(REQ_ATOM_STARTED, &rq->atomic_flags)) {\n\t\tif (q->dma_drain_size && blk_rq_bytes(rq))\n\t\t\trq->nr_phys_segments--;\n\t}\n}\n\nvoid blk_mq_requeue_request(struct request *rq)\n{\n\t__blk_mq_requeue_request(rq);\n\n\tBUG_ON(blk_queued_rq(rq));\n\tblk_mq_add_to_requeue_list(rq, true);\n}\nEXPORT_SYMBOL(blk_mq_requeue_request);\n\nstatic void blk_mq_requeue_work(struct work_struct *work)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(work, struct request_queue, requeue_work);\n\tLIST_HEAD(rq_list);\n\tstruct request *rq, *next;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->requeue_lock, flags);\n\tlist_splice_init(&q->requeue_list, &rq_list);\n\tspin_unlock_irqrestore(&q->requeue_lock, flags);\n\n\tlist_for_each_entry_safe(rq, next, &rq_list, queuelist) {\n\t\tif (!(rq->cmd_flags & REQ_SOFTBARRIER))\n\t\t\tcontinue;\n\n\t\trq->cmd_flags &= ~REQ_SOFTBARRIER;\n\t\tlist_del_init(&rq->queuelist);\n\t\tblk_mq_insert_request(rq, true, false, false);\n\t}\n\n\twhile (!list_empty(&rq_list)) {\n\t\trq = list_entry(rq_list.next, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\tblk_mq_insert_request(rq, false, false, false);\n\t}\n\n\t/*\n\t * Use the start variant of queue running here, so that running\n\t * the requeue work will kick stopped queues.\n\t */\n\tblk_mq_start_hw_queues(q);\n}\n\nvoid blk_mq_add_to_requeue_list(struct request *rq, bool at_head)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned long flags;\n\n\t/*\n\t * We abuse this flag that is otherwise used by the I/O scheduler to\n\t * request head insertation from the workqueue.\n\t */\n\tBUG_ON(rq->cmd_flags & REQ_SOFTBARRIER);\n\n\tspin_lock_irqsave(&q->requeue_lock, flags);\n\tif (at_head) {\n\t\trq->cmd_flags |= REQ_SOFTBARRIER;\n\t\tlist_add(&rq->queuelist, &q->requeue_list);\n\t} else {\n\t\tlist_add_tail(&rq->queuelist, &q->requeue_list);\n\t}\n\tspin_unlock_irqrestore(&q->requeue_lock, flags);\n}\nEXPORT_SYMBOL(blk_mq_add_to_requeue_list);\n\nvoid blk_mq_cancel_requeue_work(struct request_queue *q)\n{\n\tcancel_work_sync(&q->requeue_work);\n}\nEXPORT_SYMBOL_GPL(blk_mq_cancel_requeue_work);\n\nvoid blk_mq_kick_requeue_list(struct request_queue *q)\n{\n\tkblockd_schedule_work(&q->requeue_work);\n}\nEXPORT_SYMBOL(blk_mq_kick_requeue_list);\n\nvoid blk_mq_abort_requeue_list(struct request_queue *q)\n{\n\tunsigned long flags;\n\tLIST_HEAD(rq_list);\n\n\tspin_lock_irqsave(&q->requeue_lock, flags);\n\tlist_splice_init(&q->requeue_list, &rq_list);\n\tspin_unlock_irqrestore(&q->requeue_lock, flags);\n\n\twhile (!list_empty(&rq_list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(&rq_list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\trq->errors = -EIO;\n\t\tblk_mq_end_request(rq, rq->errors);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_abort_requeue_list);\n\nstruct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\treturn tags->rqs[tag];\n}\nEXPORT_SYMBOL(blk_mq_tag_to_rq);\n\nstruct blk_mq_timeout_data {\n\tunsigned long next;\n\tunsigned int next_set;\n};\n\nvoid blk_mq_rq_timed_out(struct request *req, bool reserved)\n{\n\tstruct blk_mq_ops *ops = req->q->mq_ops;\n\tenum blk_eh_timer_return ret = BLK_EH_RESET_TIMER;\n\n\t/*\n\t * We know that complete is set at this point. If STARTED isn't set\n\t * anymore, then the request isn't active and the \"timeout\" should\n\t * just be ignored. This can happen due to the bitflag ordering.\n\t * Timeout first checks if STARTED is set, and if it is, assumes\n\t * the request is active. But if we race with completion, then\n\t * we both flags will get cleared. So check here again, and ignore\n\t * a timeout event with a request that isn't active.\n\t */\n\tif (!test_bit(REQ_ATOM_STARTED, &req->atomic_flags))\n\t\treturn;\n\n\tif (ops->timeout)\n\t\tret = ops->timeout(req, reserved);\n\n\tswitch (ret) {\n\tcase BLK_EH_HANDLED:\n\t\t__blk_mq_complete_request(req);\n\t\tbreak;\n\tcase BLK_EH_RESET_TIMER:\n\t\tblk_add_timer(req);\n\t\tblk_clear_rq_complete(req);\n\t\tbreak;\n\tcase BLK_EH_NOT_HANDLED:\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"block: bad eh return: %d\\n\", ret);\n\t\tbreak;\n\t}\n}\n\nstatic void blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,\n\t\tstruct request *rq, void *priv, bool reserved)\n{\n\tstruct blk_mq_timeout_data *data = priv;\n\n\tif (!test_bit(REQ_ATOM_STARTED, &rq->atomic_flags)) {\n\t\t/*\n\t\t * If a request wasn't started before the queue was\n\t\t * marked dying, kill it here or it'll go unnoticed.\n\t\t */\n\t\tif (unlikely(blk_queue_dying(rq->q))) {\n\t\t\trq->errors = -EIO;\n\t\t\tblk_mq_complete_request(rq);\n\t\t}\n\t\treturn;\n\t}\n\tif (rq->cmd_flags & REQ_NO_TIMEOUT)\n\t\treturn;\n\n\tif (time_after_eq(jiffies, rq->deadline)) {\n\t\tif (!blk_mark_rq_complete(rq))\n\t\t\tblk_mq_rq_timed_out(rq, reserved);\n\t} else if (!data->next_set || time_after(data->next, rq->deadline)) {\n\t\tdata->next = rq->deadline;\n\t\tdata->next_set = 1;\n\t}\n}\n\nstatic void blk_mq_rq_timer(unsigned long priv)\n{\n\tstruct request_queue *q = (struct request_queue *)priv;\n\tstruct blk_mq_timeout_data data = {\n\t\t.next\t\t= 0,\n\t\t.next_set\t= 0,\n\t};\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t/*\n\t\t * If not software queues are currently mapped to this\n\t\t * hardware queue, there's nothing to check\n\t\t */\n\t\tif (!blk_mq_hw_queue_mapped(hctx))\n\t\t\tcontinue;\n\n\t\tblk_mq_tag_busy_iter(hctx, blk_mq_check_expired, &data);\n\t}\n\n\tif (data.next_set) {\n\t\tdata.next = blk_rq_timeout(round_jiffies_up(data.next));\n\t\tmod_timer(&q->timeout, data.next);\n\t} else {\n\t\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t\t/* the hctx may be unmapped, so check it here */\n\t\t\tif (blk_mq_hw_queue_mapped(hctx))\n\t\t\t\tblk_mq_tag_idle(hctx);\n\t\t}\n\t}\n}\n\n/*\n * Reverse check our software queue for entries that we could potentially\n * merge with. Currently includes a hand-wavy stop count of 8, to not spend\n * too much time checking for merges.\n */\nstatic bool blk_mq_attempt_merge(struct request_queue *q,\n\t\t\t\t struct blk_mq_ctx *ctx, struct bio *bio)\n{\n\tstruct request *rq;\n\tint checked = 8;\n\n\tlist_for_each_entry_reverse(rq, &ctx->rq_list, queuelist) {\n\t\tint el_ret;\n\n\t\tif (!checked--)\n\t\t\tbreak;\n\n\t\tif (!blk_rq_merge_ok(rq, bio))\n\t\t\tcontinue;\n\n\t\tel_ret = blk_try_merge(rq, bio);\n\t\tif (el_ret == ELEVATOR_BACK_MERGE) {\n\t\t\tif (bio_attempt_back_merge(q, rq, bio)) {\n\t\t\t\tctx->rq_merged++;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tbreak;\n\t\t} else if (el_ret == ELEVATOR_FRONT_MERGE) {\n\t\t\tif (bio_attempt_front_merge(q, rq, bio)) {\n\t\t\t\tctx->rq_merged++;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn false;\n}\n\n/*\n * Process software queues that have been marked busy, splicing them\n * to the for-dispatch\n */\nstatic void flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)\n{\n\tstruct blk_mq_ctx *ctx;\n\tint i;\n\n\tfor (i = 0; i < hctx->ctx_map.size; i++) {\n\t\tstruct blk_align_bitmap *bm = &hctx->ctx_map.map[i];\n\t\tunsigned int off, bit;\n\n\t\tif (!bm->word)\n\t\t\tcontinue;\n\n\t\tbit = 0;\n\t\toff = i * hctx->ctx_map.bits_per_word;\n\t\tdo {\n\t\t\tbit = find_next_bit(&bm->word, bm->depth, bit);\n\t\t\tif (bit >= bm->depth)\n\t\t\t\tbreak;\n\n\t\t\tctx = hctx->ctxs[bit + off];\n\t\t\tclear_bit(bit, &bm->word);\n\t\t\tspin_lock(&ctx->lock);\n\t\t\tlist_splice_tail_init(&ctx->rq_list, list);\n\t\t\tspin_unlock(&ctx->lock);\n\n\t\t\tbit++;\n\t\t} while (1);\n\t}\n}\n\n/*\n * Run this hardware queue, pulling any software queues mapped to it in.\n * Note that this function currently has various problems around ordering\n * of IO. In particular, we'd like FIFO behaviour on handling existing\n * items on the hctx->dispatch list. Ignore that for now.\n */\nstatic void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct request *rq;\n\tLIST_HEAD(rq_list);\n\tLIST_HEAD(driver_list);\n\tstruct list_head *dptr;\n\tint queued;\n\n\tWARN_ON(!cpumask_test_cpu(raw_smp_processor_id(), hctx->cpumask));\n\n\tif (unlikely(test_bit(BLK_MQ_S_STOPPED, &hctx->state)))\n\t\treturn;\n\n\thctx->run++;\n\n\t/*\n\t * Touch any software queue that has pending entries.\n\t */\n\tflush_busy_ctxs(hctx, &rq_list);\n\n\t/*\n\t * If we have previous entries on our dispatch list, grab them\n\t * and stuff them at the front for more fair dispatch.\n\t */\n\tif (!list_empty_careful(&hctx->dispatch)) {\n\t\tspin_lock(&hctx->lock);\n\t\tif (!list_empty(&hctx->dispatch))\n\t\t\tlist_splice_init(&hctx->dispatch, &rq_list);\n\t\tspin_unlock(&hctx->lock);\n\t}\n\n\t/*\n\t * Start off with dptr being NULL, so we start the first request\n\t * immediately, even if we have more pending.\n\t */\n\tdptr = NULL;\n\n\t/*\n\t * Now process all the entries, sending them to the driver.\n\t */\n\tqueued = 0;\n\twhile (!list_empty(&rq_list)) {\n\t\tstruct blk_mq_queue_data bd;\n\t\tint ret;\n\n\t\trq = list_first_entry(&rq_list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\n\t\tbd.rq = rq;\n\t\tbd.list = dptr;\n\t\tbd.last = list_empty(&rq_list);\n\n\t\tret = q->mq_ops->queue_rq(hctx, &bd);\n\t\tswitch (ret) {\n\t\tcase BLK_MQ_RQ_QUEUE_OK:\n\t\t\tqueued++;\n\t\t\tcontinue;\n\t\tcase BLK_MQ_RQ_QUEUE_BUSY:\n\t\t\tlist_add(&rq->queuelist, &rq_list);\n\t\t\t__blk_mq_requeue_request(rq);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"blk-mq: bad return on queue: %d\\n\", ret);\n\t\tcase BLK_MQ_RQ_QUEUE_ERROR:\n\t\t\trq->errors = -EIO;\n\t\t\tblk_mq_end_request(rq, rq->errors);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret == BLK_MQ_RQ_QUEUE_BUSY)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * We've done the first request. If we have more than 1\n\t\t * left in the list, set dptr to defer issue.\n\t\t */\n\t\tif (!dptr && rq_list.next != rq_list.prev)\n\t\t\tdptr = &driver_list;\n\t}\n\n\tif (!queued)\n\t\thctx->dispatched[0]++;\n\telse if (queued < (1 << (BLK_MQ_MAX_DISPATCH_ORDER - 1)))\n\t\thctx->dispatched[ilog2(queued) + 1]++;\n\n\t/*\n\t * Any items that need requeuing? Stuff them into hctx->dispatch,\n\t * that is where we will continue on next queue run.\n\t */\n\tif (!list_empty(&rq_list)) {\n\t\tspin_lock(&hctx->lock);\n\t\tlist_splice(&rq_list, &hctx->dispatch);\n\t\tspin_unlock(&hctx->lock);\n\t\t/*\n\t\t * the queue is expected stopped with BLK_MQ_RQ_QUEUE_BUSY, but\n\t\t * it's possible the queue is stopped and restarted again\n\t\t * before this. Queue restart will dispatch requests. And since\n\t\t * requests in rq_list aren't added into hctx->dispatch yet,\n\t\t * the requests in rq_list might get lost.\n\t\t *\n\t\t * blk_mq_run_hw_queue() already checks the STOPPED bit\n\t\t **/\n\t\tblk_mq_run_hw_queue(hctx, true);\n\t}\n}\n\n/*\n * It'd be great if the workqueue API had a way to pass\n * in a mask and had some smarts for more clever placement.\n * For now we just round-robin here, switching for every\n * BLK_MQ_CPU_WORK_BATCH queued items.\n */\nstatic int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)\n{\n\tif (hctx->queue->nr_hw_queues == 1)\n\t\treturn WORK_CPU_UNBOUND;\n\n\tif (--hctx->next_cpu_batch <= 0) {\n\t\tint cpu = hctx->next_cpu, next_cpu;\n\n\t\tnext_cpu = cpumask_next(hctx->next_cpu, hctx->cpumask);\n\t\tif (next_cpu >= nr_cpu_ids)\n\t\t\tnext_cpu = cpumask_first(hctx->cpumask);\n\n\t\thctx->next_cpu = next_cpu;\n\t\thctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;\n\n\t\treturn cpu;\n\t}\n\n\treturn hctx->next_cpu;\n}\n\nvoid blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)\n{\n\tif (unlikely(test_bit(BLK_MQ_S_STOPPED, &hctx->state) ||\n\t    !blk_mq_hw_queue_mapped(hctx)))\n\t\treturn;\n\n\tif (!async) {\n\t\tint cpu = get_cpu();\n\t\tif (cpumask_test_cpu(cpu, hctx->cpumask)) {\n\t\t\t__blk_mq_run_hw_queue(hctx);\n\t\t\tput_cpu();\n\t\t\treturn;\n\t\t}\n\n\t\tput_cpu();\n\t}\n\n\tkblockd_schedule_delayed_work_on(blk_mq_hctx_next_cpu(hctx),\n\t\t\t&hctx->run_work, 0);\n}\n\nvoid blk_mq_run_hw_queues(struct request_queue *q, bool async)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif ((!blk_mq_hctx_has_pending(hctx) &&\n\t\t    list_empty_careful(&hctx->dispatch)) ||\n\t\t    test_bit(BLK_MQ_S_STOPPED, &hctx->state))\n\t\t\tcontinue;\n\n\t\tblk_mq_run_hw_queue(hctx, async);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_run_hw_queues);\n\nvoid blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tcancel_delayed_work(&hctx->run_work);\n\tcancel_delayed_work(&hctx->delay_work);\n\tset_bit(BLK_MQ_S_STOPPED, &hctx->state);\n}\nEXPORT_SYMBOL(blk_mq_stop_hw_queue);\n\nvoid blk_mq_stop_hw_queues(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_stop_hw_queue(hctx);\n}\nEXPORT_SYMBOL(blk_mq_stop_hw_queues);\n\nvoid blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tclear_bit(BLK_MQ_S_STOPPED, &hctx->state);\n\n\tblk_mq_run_hw_queue(hctx, false);\n}\nEXPORT_SYMBOL(blk_mq_start_hw_queue);\n\nvoid blk_mq_start_hw_queues(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_start_hw_queue(hctx);\n}\nEXPORT_SYMBOL(blk_mq_start_hw_queues);\n\nvoid blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (!test_bit(BLK_MQ_S_STOPPED, &hctx->state))\n\t\t\tcontinue;\n\n\t\tclear_bit(BLK_MQ_S_STOPPED, &hctx->state);\n\t\tblk_mq_run_hw_queue(hctx, async);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);\n\nstatic void blk_mq_run_work_fn(struct work_struct *work)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\n\thctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);\n\n\t__blk_mq_run_hw_queue(hctx);\n}\n\nstatic void blk_mq_delay_work_fn(struct work_struct *work)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\n\thctx = container_of(work, struct blk_mq_hw_ctx, delay_work.work);\n\n\tif (test_and_clear_bit(BLK_MQ_S_STOPPED, &hctx->state))\n\t\t__blk_mq_run_hw_queue(hctx);\n}\n\nvoid blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)\n{\n\tif (unlikely(!blk_mq_hw_queue_mapped(hctx)))\n\t\treturn;\n\n\tkblockd_schedule_delayed_work_on(blk_mq_hctx_next_cpu(hctx),\n\t\t\t&hctx->delay_work, msecs_to_jiffies(msecs));\n}\nEXPORT_SYMBOL(blk_mq_delay_queue);\n\nstatic void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t    struct request *rq, bool at_head)\n{\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\n\ttrace_block_rq_insert(hctx->queue, rq);\n\n\tif (at_head)\n\t\tlist_add(&rq->queuelist, &ctx->rq_list);\n\telse\n\t\tlist_add_tail(&rq->queuelist, &ctx->rq_list);\n\n\tblk_mq_hctx_mark_pending(hctx, ctx);\n}\n\nvoid blk_mq_insert_request(struct request *rq, bool at_head, bool run_queue,\n\t\tbool async)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx, *current_ctx;\n\n\tcurrent_ctx = blk_mq_get_ctx(q);\n\tif (!cpu_online(ctx->cpu))\n\t\trq->mq_ctx = ctx = current_ctx;\n\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\n\tspin_lock(&ctx->lock);\n\t__blk_mq_insert_request(hctx, rq, at_head);\n\tspin_unlock(&ctx->lock);\n\n\tif (run_queue)\n\t\tblk_mq_run_hw_queue(hctx, async);\n\n\tblk_mq_put_ctx(current_ctx);\n}\n\nstatic void blk_mq_insert_requests(struct request_queue *q,\n\t\t\t\t     struct blk_mq_ctx *ctx,\n\t\t\t\t     struct list_head *list,\n\t\t\t\t     int depth,\n\t\t\t\t     bool from_schedule)\n\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *current_ctx;\n\n\ttrace_block_unplug(q, depth, !from_schedule);\n\n\tcurrent_ctx = blk_mq_get_ctx(q);\n\n\tif (!cpu_online(ctx->cpu))\n\t\tctx = current_ctx;\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\n\t/*\n\t * preemption doesn't flush plug list, so it's possible ctx->cpu is\n\t * offline now\n\t */\n\tspin_lock(&ctx->lock);\n\twhile (!list_empty(list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\trq->mq_ctx = ctx;\n\t\t__blk_mq_insert_request(hctx, rq, false);\n\t}\n\tspin_unlock(&ctx->lock);\n\n\tblk_mq_run_hw_queue(hctx, from_schedule);\n\tblk_mq_put_ctx(current_ctx);\n}\n\nstatic int plug_ctx_cmp(void *priv, struct list_head *a, struct list_head *b)\n{\n\tstruct request *rqa = container_of(a, struct request, queuelist);\n\tstruct request *rqb = container_of(b, struct request, queuelist);\n\n\treturn !(rqa->mq_ctx < rqb->mq_ctx ||\n\t\t (rqa->mq_ctx == rqb->mq_ctx &&\n\t\t  blk_rq_pos(rqa) < blk_rq_pos(rqb)));\n}\n\nvoid blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)\n{\n\tstruct blk_mq_ctx *this_ctx;\n\tstruct request_queue *this_q;\n\tstruct request *rq;\n\tLIST_HEAD(list);\n\tLIST_HEAD(ctx_list);\n\tunsigned int depth;\n\n\tlist_splice_init(&plug->mq_list, &list);\n\n\tlist_sort(NULL, &list, plug_ctx_cmp);\n\n\tthis_q = NULL;\n\tthis_ctx = NULL;\n\tdepth = 0;\n\n\twhile (!list_empty(&list)) {\n\t\trq = list_entry_rq(list.next);\n\t\tlist_del_init(&rq->queuelist);\n\t\tBUG_ON(!rq->q);\n\t\tif (rq->mq_ctx != this_ctx) {\n\t\t\tif (this_ctx) {\n\t\t\t\tblk_mq_insert_requests(this_q, this_ctx,\n\t\t\t\t\t\t\t&ctx_list, depth,\n\t\t\t\t\t\t\tfrom_schedule);\n\t\t\t}\n\n\t\t\tthis_ctx = rq->mq_ctx;\n\t\t\tthis_q = rq->q;\n\t\t\tdepth = 0;\n\t\t}\n\n\t\tdepth++;\n\t\tlist_add_tail(&rq->queuelist, &ctx_list);\n\t}\n\n\t/*\n\t * If 'this_ctx' is set, we know we have entries to complete\n\t * on 'ctx_list'. Do those.\n\t */\n\tif (this_ctx) {\n\t\tblk_mq_insert_requests(this_q, this_ctx, &ctx_list, depth,\n\t\t\t\t       from_schedule);\n\t}\n}\n\nstatic void blk_mq_bio_to_request(struct request *rq, struct bio *bio)\n{\n\tinit_request_from_bio(rq, bio);\n\n\tif (blk_do_io_stat(rq))\n\t\tblk_account_io_start(rq, 1);\n}\n\nstatic inline bool hctx_allow_merges(struct blk_mq_hw_ctx *hctx)\n{\n\treturn (hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&\n\t\t!blk_queue_nomerges(hctx->queue);\n}\n\nstatic inline bool blk_mq_merge_queue_io(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t struct blk_mq_ctx *ctx,\n\t\t\t\t\t struct request *rq, struct bio *bio)\n{\n\tif (!hctx_allow_merges(hctx)) {\n\t\tblk_mq_bio_to_request(rq, bio);\n\t\tspin_lock(&ctx->lock);\ninsert_rq:\n\t\t__blk_mq_insert_request(hctx, rq, false);\n\t\tspin_unlock(&ctx->lock);\n\t\treturn false;\n\t} else {\n\t\tstruct request_queue *q = hctx->queue;\n\n\t\tspin_lock(&ctx->lock);\n\t\tif (!blk_mq_attempt_merge(q, ctx, bio)) {\n\t\t\tblk_mq_bio_to_request(rq, bio);\n\t\t\tgoto insert_rq;\n\t\t}\n\n\t\tspin_unlock(&ctx->lock);\n\t\t__blk_mq_free_request(hctx, ctx, rq);\n\t\treturn true;\n\t}\n}\n\nstruct blk_map_ctx {\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx;\n};\n\nstatic struct request *blk_mq_map_request(struct request_queue *q,\n\t\t\t\t\t  struct bio *bio,\n\t\t\t\t\t  struct blk_map_ctx *data)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx;\n\tstruct request *rq;\n\tint rw = bio_data_dir(bio);\n\tstruct blk_mq_alloc_data alloc_data;\n\n\tif (unlikely(blk_mq_queue_enter(q, GFP_KERNEL))) {\n\t\tbio_io_error(bio);\n\t\treturn NULL;\n\t}\n\n\tctx = blk_mq_get_ctx(q);\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\n\tif (rw_is_sync(bio->bi_rw))\n\t\trw |= REQ_SYNC;\n\n\ttrace_block_getrq(q, bio, rw);\n\tblk_mq_set_alloc_data(&alloc_data, q, GFP_ATOMIC, false, ctx,\n\t\t\thctx);\n\trq = __blk_mq_alloc_request(&alloc_data, rw);\n\tif (unlikely(!rq)) {\n\t\t__blk_mq_run_hw_queue(hctx);\n\t\tblk_mq_put_ctx(ctx);\n\t\ttrace_block_sleeprq(q, bio, rw);\n\n\t\tctx = blk_mq_get_ctx(q);\n\t\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\t\tblk_mq_set_alloc_data(&alloc_data, q,\n\t\t\t\t__GFP_WAIT|GFP_ATOMIC, false, ctx, hctx);\n\t\trq = __blk_mq_alloc_request(&alloc_data, rw);\n\t\tctx = alloc_data.ctx;\n\t\thctx = alloc_data.hctx;\n\t}\n\n\thctx->queued++;\n\tdata->hctx = hctx;\n\tdata->ctx = ctx;\n\treturn rq;\n}\n\nstatic int blk_mq_direct_issue_request(struct request *rq)\n{\n\tint ret;\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_hw_ctx *hctx = q->mq_ops->map_queue(q,\n\t\t\trq->mq_ctx->cpu);\n\tstruct blk_mq_queue_data bd = {\n\t\t.rq = rq,\n\t\t.list = NULL,\n\t\t.last = 1\n\t};\n\n\t/*\n\t * For OK queue, we are done. For error, kill it. Any other\n\t * error (busy), just add it to our list as we previously\n\t * would have done\n\t */\n\tret = q->mq_ops->queue_rq(hctx, &bd);\n\tif (ret == BLK_MQ_RQ_QUEUE_OK)\n\t\treturn 0;\n\telse {\n\t\t__blk_mq_requeue_request(rq);\n\n\t\tif (ret == BLK_MQ_RQ_QUEUE_ERROR) {\n\t\t\trq->errors = -EIO;\n\t\t\tblk_mq_end_request(rq, rq->errors);\n\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n}\n\n/*\n * Multiple hardware queue variant. This will not use per-process plugs,\n * but will attempt to bypass the hctx queueing if we can go straight to\n * hardware for SYNC IO.\n */\nstatic void blk_mq_make_request(struct request_queue *q, struct bio *bio)\n{\n\tconst int is_sync = rw_is_sync(bio->bi_rw);\n\tconst int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);\n\tstruct blk_map_ctx data;\n\tstruct request *rq;\n\tunsigned int request_count = 0;\n\tstruct blk_plug *plug;\n\tstruct request *same_queue_rq = NULL;\n\n\tblk_queue_bounce(q, &bio);\n\n\tif (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {\n\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\n\tblk_queue_split(q, &bio, q->bio_split);\n\n\tif (!is_flush_fua && !blk_queue_nomerges(q) &&\n\t    blk_attempt_plug_merge(q, bio, &request_count, &same_queue_rq))\n\t\treturn;\n\n\trq = blk_mq_map_request(q, bio, &data);\n\tif (unlikely(!rq))\n\t\treturn;\n\n\tif (unlikely(is_flush_fua)) {\n\t\tblk_mq_bio_to_request(rq, bio);\n\t\tblk_insert_flush(rq);\n\t\tgoto run_queue;\n\t}\n\n\tplug = current->plug;\n\t/*\n\t * If the driver supports defer issued based on 'last', then\n\t * queue it up like normal since we can potentially save some\n\t * CPU this way.\n\t */\n\tif (((plug && !blk_queue_nomerges(q)) || is_sync) &&\n\t    !(data.hctx->flags & BLK_MQ_F_DEFER_ISSUE)) {\n\t\tstruct request *old_rq = NULL;\n\n\t\tblk_mq_bio_to_request(rq, bio);\n\n\t\t/*\n\t\t * we do limited pluging. If bio can be merged, do merge.\n\t\t * Otherwise the existing request in the plug list will be\n\t\t * issued. So the plug list will have one request at most\n\t\t */\n\t\tif (plug) {\n\t\t\t/*\n\t\t\t * The plug list might get flushed before this. If that\n\t\t\t * happens, same_queue_rq is invalid and plug list is empty\n\t\t\t **/\n\t\t\tif (same_queue_rq && !list_empty(&plug->mq_list)) {\n\t\t\t\told_rq = same_queue_rq;\n\t\t\t\tlist_del_init(&old_rq->queuelist);\n\t\t\t}\n\t\t\tlist_add_tail(&rq->queuelist, &plug->mq_list);\n\t\t} else /* is_sync */\n\t\t\told_rq = rq;\n\t\tblk_mq_put_ctx(data.ctx);\n\t\tif (!old_rq)\n\t\t\treturn;\n\t\tif (!blk_mq_direct_issue_request(old_rq))\n\t\t\treturn;\n\t\tblk_mq_insert_request(old_rq, false, true, true);\n\t\treturn;\n\t}\n\n\tif (!blk_mq_merge_queue_io(data.hctx, data.ctx, rq, bio)) {\n\t\t/*\n\t\t * For a SYNC request, send it to the hardware immediately. For\n\t\t * an ASYNC request, just ensure that we run it later on. The\n\t\t * latter allows for merging opportunities and more efficient\n\t\t * dispatching.\n\t\t */\nrun_queue:\n\t\tblk_mq_run_hw_queue(data.hctx, !is_sync || is_flush_fua);\n\t}\n\tblk_mq_put_ctx(data.ctx);\n}\n\n/*\n * Single hardware queue variant. This will attempt to use any per-process\n * plug for merging and IO deferral.\n */\nstatic void blk_sq_make_request(struct request_queue *q, struct bio *bio)\n{\n\tconst int is_sync = rw_is_sync(bio->bi_rw);\n\tconst int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);\n\tstruct blk_plug *plug;\n\tunsigned int request_count = 0;\n\tstruct blk_map_ctx data;\n\tstruct request *rq;\n\n\tblk_queue_bounce(q, &bio);\n\n\tif (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {\n\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\n\tblk_queue_split(q, &bio, q->bio_split);\n\n\tif (!is_flush_fua && !blk_queue_nomerges(q) &&\n\t    blk_attempt_plug_merge(q, bio, &request_count, NULL))\n\t\treturn;\n\n\trq = blk_mq_map_request(q, bio, &data);\n\tif (unlikely(!rq))\n\t\treturn;\n\n\tif (unlikely(is_flush_fua)) {\n\t\tblk_mq_bio_to_request(rq, bio);\n\t\tblk_insert_flush(rq);\n\t\tgoto run_queue;\n\t}\n\n\t/*\n\t * A task plug currently exists. Since this is completely lockless,\n\t * utilize that to temporarily store requests until the task is\n\t * either done or scheduled away.\n\t */\n\tplug = current->plug;\n\tif (plug) {\n\t\tblk_mq_bio_to_request(rq, bio);\n\t\tif (list_empty(&plug->mq_list))\n\t\t\ttrace_block_plug(q);\n\t\telse if (request_count >= BLK_MAX_REQUEST_COUNT) {\n\t\t\tblk_flush_plug_list(plug, false);\n\t\t\ttrace_block_plug(q);\n\t\t}\n\t\tlist_add_tail(&rq->queuelist, &plug->mq_list);\n\t\tblk_mq_put_ctx(data.ctx);\n\t\treturn;\n\t}\n\n\tif (!blk_mq_merge_queue_io(data.hctx, data.ctx, rq, bio)) {\n\t\t/*\n\t\t * For a SYNC request, send it to the hardware immediately. For\n\t\t * an ASYNC request, just ensure that we run it later on. The\n\t\t * latter allows for merging opportunities and more efficient\n\t\t * dispatching.\n\t\t */\nrun_queue:\n\t\tblk_mq_run_hw_queue(data.hctx, !is_sync || is_flush_fua);\n\t}\n\n\tblk_mq_put_ctx(data.ctx);\n}\n\n/*\n * Default mapping to a software queue, since we use one per CPU.\n */\nstruct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q, const int cpu)\n{\n\treturn q->queue_hw_ctx[q->mq_map[cpu]];\n}\nEXPORT_SYMBOL(blk_mq_map_queue);\n\nstatic void blk_mq_free_rq_map(struct blk_mq_tag_set *set,\n\t\tstruct blk_mq_tags *tags, unsigned int hctx_idx)\n{\n\tstruct page *page;\n\n\tif (tags->rqs && set->ops->exit_request) {\n\t\tint i;\n\n\t\tfor (i = 0; i < tags->nr_tags; i++) {\n\t\t\tif (!tags->rqs[i])\n\t\t\t\tcontinue;\n\t\t\tset->ops->exit_request(set->driver_data, tags->rqs[i],\n\t\t\t\t\t\thctx_idx, i);\n\t\t\ttags->rqs[i] = NULL;\n\t\t}\n\t}\n\n\twhile (!list_empty(&tags->page_list)) {\n\t\tpage = list_first_entry(&tags->page_list, struct page, lru);\n\t\tlist_del_init(&page->lru);\n\t\t__free_pages(page, page->private);\n\t}\n\n\tkfree(tags->rqs);\n\n\tblk_mq_free_tags(tags);\n}\n\nstatic size_t order_to_size(unsigned int order)\n{\n\treturn (size_t)PAGE_SIZE << order;\n}\n\nstatic struct blk_mq_tags *blk_mq_init_rq_map(struct blk_mq_tag_set *set,\n\t\tunsigned int hctx_idx)\n{\n\tstruct blk_mq_tags *tags;\n\tunsigned int i, j, entries_per_page, max_order = 4;\n\tsize_t rq_size, left;\n\n\ttags = blk_mq_init_tags(set->queue_depth, set->reserved_tags,\n\t\t\t\tset->numa_node,\n\t\t\t\tBLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));\n\tif (!tags)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&tags->page_list);\n\n\ttags->rqs = kzalloc_node(set->queue_depth * sizeof(struct request *),\n\t\t\t\t GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY,\n\t\t\t\t set->numa_node);\n\tif (!tags->rqs) {\n\t\tblk_mq_free_tags(tags);\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * rq_size is the size of the request plus driver payload, rounded\n\t * to the cacheline size\n\t */\n\trq_size = round_up(sizeof(struct request) + set->cmd_size,\n\t\t\t\tcache_line_size());\n\tleft = rq_size * set->queue_depth;\n\n\tfor (i = 0; i < set->queue_depth; ) {\n\t\tint this_order = max_order;\n\t\tstruct page *page;\n\t\tint to_do;\n\t\tvoid *p;\n\n\t\twhile (left < order_to_size(this_order - 1) && this_order)\n\t\t\tthis_order--;\n\n\t\tdo {\n\t\t\tpage = alloc_pages_node(set->numa_node,\n\t\t\t\tGFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY | __GFP_ZERO,\n\t\t\t\tthis_order);\n\t\t\tif (page)\n\t\t\t\tbreak;\n\t\t\tif (!this_order--)\n\t\t\t\tbreak;\n\t\t\tif (order_to_size(this_order) < rq_size)\n\t\t\t\tbreak;\n\t\t} while (1);\n\n\t\tif (!page)\n\t\t\tgoto fail;\n\n\t\tpage->private = this_order;\n\t\tlist_add_tail(&page->lru, &tags->page_list);\n\n\t\tp = page_address(page);\n\t\tentries_per_page = order_to_size(this_order) / rq_size;\n\t\tto_do = min(entries_per_page, set->queue_depth - i);\n\t\tleft -= to_do * rq_size;\n\t\tfor (j = 0; j < to_do; j++) {\n\t\t\ttags->rqs[i] = p;\n\t\t\tif (set->ops->init_request) {\n\t\t\t\tif (set->ops->init_request(set->driver_data,\n\t\t\t\t\t\ttags->rqs[i], hctx_idx, i,\n\t\t\t\t\t\tset->numa_node)) {\n\t\t\t\t\ttags->rqs[i] = NULL;\n\t\t\t\t\tgoto fail;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tp += rq_size;\n\t\t\ti++;\n\t\t}\n\t}\n\treturn tags;\n\nfail:\n\tblk_mq_free_rq_map(set, tags, hctx_idx);\n\treturn NULL;\n}\n\nstatic void blk_mq_free_bitmap(struct blk_mq_ctxmap *bitmap)\n{\n\tkfree(bitmap->map);\n}\n\nstatic int blk_mq_alloc_bitmap(struct blk_mq_ctxmap *bitmap, int node)\n{\n\tunsigned int bpw = 8, total, num_maps, i;\n\n\tbitmap->bits_per_word = bpw;\n\n\tnum_maps = ALIGN(nr_cpu_ids, bpw) / bpw;\n\tbitmap->map = kzalloc_node(num_maps * sizeof(struct blk_align_bitmap),\n\t\t\t\t\tGFP_KERNEL, node);\n\tif (!bitmap->map)\n\t\treturn -ENOMEM;\n\n\ttotal = nr_cpu_ids;\n\tfor (i = 0; i < num_maps; i++) {\n\t\tbitmap->map[i].depth = min(total, bitmap->bits_per_word);\n\t\ttotal -= bitmap->map[i].depth;\n\t}\n\n\treturn 0;\n}\n\nstatic int blk_mq_hctx_cpu_offline(struct blk_mq_hw_ctx *hctx, int cpu)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct blk_mq_ctx *ctx;\n\tLIST_HEAD(tmp);\n\n\t/*\n\t * Move ctx entries to new CPU, if this one is going away.\n\t */\n\tctx = __blk_mq_get_ctx(q, cpu);\n\n\tspin_lock(&ctx->lock);\n\tif (!list_empty(&ctx->rq_list)) {\n\t\tlist_splice_init(&ctx->rq_list, &tmp);\n\t\tblk_mq_hctx_clear_pending(hctx, ctx);\n\t}\n\tspin_unlock(&ctx->lock);\n\n\tif (list_empty(&tmp))\n\t\treturn NOTIFY_OK;\n\n\tctx = blk_mq_get_ctx(q);\n\tspin_lock(&ctx->lock);\n\n\twhile (!list_empty(&tmp)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(&tmp, struct request, queuelist);\n\t\trq->mq_ctx = ctx;\n\t\tlist_move_tail(&rq->queuelist, &ctx->rq_list);\n\t}\n\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\tblk_mq_hctx_mark_pending(hctx, ctx);\n\n\tspin_unlock(&ctx->lock);\n\n\tblk_mq_run_hw_queue(hctx, true);\n\tblk_mq_put_ctx(ctx);\n\treturn NOTIFY_OK;\n}\n\nstatic int blk_mq_hctx_notify(void *data, unsigned long action,\n\t\t\t      unsigned int cpu)\n{\n\tstruct blk_mq_hw_ctx *hctx = data;\n\n\tif (action == CPU_DEAD || action == CPU_DEAD_FROZEN)\n\t\treturn blk_mq_hctx_cpu_offline(hctx, cpu);\n\n\t/*\n\t * In case of CPU online, tags may be reallocated\n\t * in blk_mq_map_swqueue() after mapping is updated.\n\t */\n\n\treturn NOTIFY_OK;\n}\n\n/* hctx->ctxs will be freed in queue's release handler */\nstatic void blk_mq_exit_hctx(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set,\n\t\tstruct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)\n{\n\tunsigned flush_start_tag = set->queue_depth;\n\n\tblk_mq_tag_idle(hctx);\n\n\tif (set->ops->exit_request)\n\t\tset->ops->exit_request(set->driver_data,\n\t\t\t\t       hctx->fq->flush_rq, hctx_idx,\n\t\t\t\t       flush_start_tag + hctx_idx);\n\n\tif (set->ops->exit_hctx)\n\t\tset->ops->exit_hctx(hctx, hctx_idx);\n\n\tblk_mq_unregister_cpu_notifier(&hctx->cpu_notifier);\n\tblk_free_flush_queue(hctx->fq);\n\tblk_mq_free_bitmap(&hctx->ctx_map);\n}\n\nstatic void blk_mq_exit_hw_queues(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set, int nr_queue)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (i == nr_queue)\n\t\t\tbreak;\n\t\tblk_mq_exit_hctx(q, set, hctx, i);\n\t}\n}\n\nstatic void blk_mq_free_hw_queues(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tfree_cpumask_var(hctx->cpumask);\n}\n\nstatic int blk_mq_init_hctx(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set,\n\t\tstruct blk_mq_hw_ctx *hctx, unsigned hctx_idx)\n{\n\tint node;\n\tunsigned flush_start_tag = set->queue_depth;\n\n\tnode = hctx->numa_node;\n\tif (node == NUMA_NO_NODE)\n\t\tnode = hctx->numa_node = set->numa_node;\n\n\tINIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);\n\tINIT_DELAYED_WORK(&hctx->delay_work, blk_mq_delay_work_fn);\n\tspin_lock_init(&hctx->lock);\n\tINIT_LIST_HEAD(&hctx->dispatch);\n\thctx->queue = q;\n\thctx->queue_num = hctx_idx;\n\thctx->flags = set->flags;\n\n\tblk_mq_init_cpu_notifier(&hctx->cpu_notifier,\n\t\t\t\t\tblk_mq_hctx_notify, hctx);\n\tblk_mq_register_cpu_notifier(&hctx->cpu_notifier);\n\n\thctx->tags = set->tags[hctx_idx];\n\n\t/*\n\t * Allocate space for all possible cpus to avoid allocation at\n\t * runtime\n\t */\n\thctx->ctxs = kmalloc_node(nr_cpu_ids * sizeof(void *),\n\t\t\t\t\tGFP_KERNEL, node);\n\tif (!hctx->ctxs)\n\t\tgoto unregister_cpu_notifier;\n\n\tif (blk_mq_alloc_bitmap(&hctx->ctx_map, node))\n\t\tgoto free_ctxs;\n\n\thctx->nr_ctx = 0;\n\n\tif (set->ops->init_hctx &&\n\t    set->ops->init_hctx(hctx, set->driver_data, hctx_idx))\n\t\tgoto free_bitmap;\n\n\thctx->fq = blk_alloc_flush_queue(q, hctx->numa_node, set->cmd_size);\n\tif (!hctx->fq)\n\t\tgoto exit_hctx;\n\n\tif (set->ops->init_request &&\n\t    set->ops->init_request(set->driver_data,\n\t\t\t\t   hctx->fq->flush_rq, hctx_idx,\n\t\t\t\t   flush_start_tag + hctx_idx, node))\n\t\tgoto free_fq;\n\n\treturn 0;\n\n free_fq:\n\tkfree(hctx->fq);\n exit_hctx:\n\tif (set->ops->exit_hctx)\n\t\tset->ops->exit_hctx(hctx, hctx_idx);\n free_bitmap:\n\tblk_mq_free_bitmap(&hctx->ctx_map);\n free_ctxs:\n\tkfree(hctx->ctxs);\n unregister_cpu_notifier:\n\tblk_mq_unregister_cpu_notifier(&hctx->cpu_notifier);\n\n\treturn -1;\n}\n\nstatic int blk_mq_init_hw_queues(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\t/*\n\t * Initialize hardware queues\n\t */\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (blk_mq_init_hctx(q, set, hctx, i))\n\t\t\tbreak;\n\t}\n\n\tif (i == q->nr_hw_queues)\n\t\treturn 0;\n\n\t/*\n\t * Init failed\n\t */\n\tblk_mq_exit_hw_queues(q, set, i);\n\n\treturn 1;\n}\n\nstatic void blk_mq_init_cpu_queues(struct request_queue *q,\n\t\t\t\t   unsigned int nr_hw_queues)\n{\n\tunsigned int i;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct blk_mq_ctx *__ctx = per_cpu_ptr(q->queue_ctx, i);\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\tmemset(__ctx, 0, sizeof(*__ctx));\n\t\t__ctx->cpu = i;\n\t\tspin_lock_init(&__ctx->lock);\n\t\tINIT_LIST_HEAD(&__ctx->rq_list);\n\t\t__ctx->queue = q;\n\n\t\t/* If the cpu isn't online, the cpu is mapped to first hctx */\n\t\tif (!cpu_online(i))\n\t\t\tcontinue;\n\n\t\thctx = q->mq_ops->map_queue(q, i);\n\n\t\t/*\n\t\t * Set local node, IFF we have more than one hw queue. If\n\t\t * not, we remain on the home node of the device\n\t\t */\n\t\tif (nr_hw_queues > 1 && hctx->numa_node == NUMA_NO_NODE)\n\t\t\thctx->numa_node = cpu_to_node(i);\n\t}\n}\n\nstatic void blk_mq_map_swqueue(struct request_queue *q)\n{\n\tunsigned int i;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx;\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tcpumask_clear(hctx->cpumask);\n\t\thctx->nr_ctx = 0;\n\t}\n\n\t/*\n\t * Map software to hardware queues\n\t */\n\tqueue_for_each_ctx(q, ctx, i) {\n\t\t/* If the cpu isn't online, the cpu is mapped to first hctx */\n\t\tif (!cpu_online(i))\n\t\t\tcontinue;\n\n\t\thctx = q->mq_ops->map_queue(q, i);\n\t\tcpumask_set_cpu(i, hctx->cpumask);\n\t\tcpumask_set_cpu(i, hctx->tags->cpumask);\n\t\tctx->index_hw = hctx->nr_ctx;\n\t\thctx->ctxs[hctx->nr_ctx++] = ctx;\n\t}\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tstruct blk_mq_ctxmap *map = &hctx->ctx_map;\n\n\t\t/*\n\t\t * If no software queues are mapped to this hardware queue,\n\t\t * disable it and free the request entries.\n\t\t */\n\t\tif (!hctx->nr_ctx) {\n\t\t\tif (set->tags[i]) {\n\t\t\t\tblk_mq_free_rq_map(set, set->tags[i], i);\n\t\t\t\tset->tags[i] = NULL;\n\t\t\t}\n\t\t\thctx->tags = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* unmapped hw queue can be remapped after CPU topo changed */\n\t\tif (!set->tags[i])\n\t\t\tset->tags[i] = blk_mq_init_rq_map(set, i);\n\t\thctx->tags = set->tags[i];\n\t\tWARN_ON(!hctx->tags);\n\n\t\t/*\n\t\t * Set the map size to the number of mapped software queues.\n\t\t * This is more accurate and more efficient than looping\n\t\t * over all possibly mapped software queues.\n\t\t */\n\t\tmap->size = DIV_ROUND_UP(hctx->nr_ctx, map->bits_per_word);\n\n\t\t/*\n\t\t * Initialize batch roundrobin counts\n\t\t */\n\t\thctx->next_cpu = cpumask_first(hctx->cpumask);\n\t\thctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;\n\t}\n}\n\nstatic void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct request_queue *q;\n\tbool shared;\n\tint i;\n\n\tif (set->tag_list.next == set->tag_list.prev)\n\t\tshared = false;\n\telse\n\t\tshared = true;\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_freeze_queue(q);\n\n\t\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t\tif (shared)\n\t\t\t\thctx->flags |= BLK_MQ_F_TAG_SHARED;\n\t\t\telse\n\t\t\t\thctx->flags &= ~BLK_MQ_F_TAG_SHARED;\n\t\t}\n\t\tblk_mq_unfreeze_queue(q);\n\t}\n}\n\nstatic void blk_mq_del_queue_tag_set(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\n\tmutex_lock(&set->tag_list_lock);\n\tlist_del_init(&q->tag_set_list);\n\tblk_mq_update_tag_set_depth(set);\n\tmutex_unlock(&set->tag_list_lock);\n}\n\nstatic void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,\n\t\t\t\t     struct request_queue *q)\n{\n\tq->tag_set = set;\n\n\tmutex_lock(&set->tag_list_lock);\n\tlist_add_tail(&q->tag_set_list, &set->tag_list);\n\tblk_mq_update_tag_set_depth(set);\n\tmutex_unlock(&set->tag_list_lock);\n}\n\n/*\n * It is the actual release handler for mq, but we do it from\n * request queue's release handler for avoiding use-after-free\n * and headache because q->mq_kobj shouldn't have been introduced,\n * but we can't group ctx/kctx kobj without it.\n */\nvoid blk_mq_release(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\t/* hctx kobj stays in hctx */\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (!hctx)\n\t\t\tcontinue;\n\t\tkfree(hctx->ctxs);\n\t\tkfree(hctx);\n\t}\n\n\tkfree(q->queue_hw_ctx);\n\n\t/* ctx kobj stays in queue_ctx */\n\tfree_percpu(q->queue_ctx);\n}\n\nstruct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)\n{\n\tstruct request_queue *uninit_q, *q;\n\n\tuninit_q = blk_alloc_queue_node(GFP_KERNEL, set->numa_node);\n\tif (!uninit_q)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tq = blk_mq_init_allocated_queue(set, uninit_q);\n\tif (IS_ERR(q))\n\t\tblk_cleanup_queue(uninit_q);\n\n\treturn q;\n}\nEXPORT_SYMBOL(blk_mq_init_queue);\n\nstruct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,\n\t\t\t\t\t\t  struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx **hctxs;\n\tstruct blk_mq_ctx __percpu *ctx;\n\tunsigned int *map;\n\tint i;\n\n\tctx = alloc_percpu(struct blk_mq_ctx);\n\tif (!ctx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\thctxs = kmalloc_node(set->nr_hw_queues * sizeof(*hctxs), GFP_KERNEL,\n\t\t\tset->numa_node);\n\n\tif (!hctxs)\n\t\tgoto err_percpu;\n\n\tmap = blk_mq_make_queue_map(set);\n\tif (!map)\n\t\tgoto err_map;\n\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tint node = blk_mq_hw_queue_to_node(map, i);\n\n\t\thctxs[i] = kzalloc_node(sizeof(struct blk_mq_hw_ctx),\n\t\t\t\t\tGFP_KERNEL, node);\n\t\tif (!hctxs[i])\n\t\t\tgoto err_hctxs;\n\n\t\tif (!zalloc_cpumask_var_node(&hctxs[i]->cpumask, GFP_KERNEL,\n\t\t\t\t\t\tnode))\n\t\t\tgoto err_hctxs;\n\n\t\tatomic_set(&hctxs[i]->nr_active, 0);\n\t\thctxs[i]->numa_node = node;\n\t\thctxs[i]->queue_num = i;\n\t}\n\n\t/*\n\t * Init percpu_ref in atomic mode so that it's faster to shutdown.\n\t * See blk_register_queue() for details.\n\t */\n\tif (percpu_ref_init(&q->mq_usage_counter, blk_mq_usage_counter_release,\n\t\t\t    PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))\n\t\tgoto err_hctxs;\n\n\tsetup_timer(&q->timeout, blk_mq_rq_timer, (unsigned long) q);\n\tblk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ);\n\n\tq->nr_queues = nr_cpu_ids;\n\tq->nr_hw_queues = set->nr_hw_queues;\n\tq->mq_map = map;\n\n\tq->queue_ctx = ctx;\n\tq->queue_hw_ctx = hctxs;\n\n\tq->mq_ops = set->ops;\n\tq->queue_flags |= QUEUE_FLAG_MQ_DEFAULT;\n\n\tif (!(set->flags & BLK_MQ_F_SG_MERGE))\n\t\tq->queue_flags |= 1 << QUEUE_FLAG_NO_SG_MERGE;\n\n\tq->sg_reserved_size = INT_MAX;\n\n\tINIT_WORK(&q->requeue_work, blk_mq_requeue_work);\n\tINIT_LIST_HEAD(&q->requeue_list);\n\tspin_lock_init(&q->requeue_lock);\n\n\tif (q->nr_hw_queues > 1)\n\t\tblk_queue_make_request(q, blk_mq_make_request);\n\telse\n\t\tblk_queue_make_request(q, blk_sq_make_request);\n\n\t/*\n\t * Do this after blk_queue_make_request() overrides it...\n\t */\n\tq->nr_requests = set->queue_depth;\n\n\tif (set->ops->complete)\n\t\tblk_queue_softirq_done(q, set->ops->complete);\n\n\tblk_mq_init_cpu_queues(q, set->nr_hw_queues);\n\n\tif (blk_mq_init_hw_queues(q, set))\n\t\tgoto err_hctxs;\n\n\tmutex_lock(&all_q_mutex);\n\tlist_add_tail(&q->all_q_node, &all_q_list);\n\tmutex_unlock(&all_q_mutex);\n\n\tblk_mq_add_queue_tag_set(set, q);\n\n\tblk_mq_map_swqueue(q);\n\n\treturn q;\n\nerr_hctxs:\n\tkfree(map);\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tif (!hctxs[i])\n\t\t\tbreak;\n\t\tfree_cpumask_var(hctxs[i]->cpumask);\n\t\tkfree(hctxs[i]);\n\t}\nerr_map:\n\tkfree(hctxs);\nerr_percpu:\n\tfree_percpu(ctx);\n\treturn ERR_PTR(-ENOMEM);\n}\nEXPORT_SYMBOL(blk_mq_init_allocated_queue);\n\nvoid blk_mq_free_queue(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set\t*set = q->tag_set;\n\n\tblk_mq_del_queue_tag_set(q);\n\n\tblk_mq_exit_hw_queues(q, set, set->nr_hw_queues);\n\tblk_mq_free_hw_queues(q, set);\n\n\tpercpu_ref_exit(&q->mq_usage_counter);\n\n\tkfree(q->mq_map);\n\n\tq->mq_map = NULL;\n\n\tmutex_lock(&all_q_mutex);\n\tlist_del_init(&q->all_q_node);\n\tmutex_unlock(&all_q_mutex);\n}\n\n/* Basically redo blk_mq_init_queue with queue frozen */\nstatic void blk_mq_queue_reinit(struct request_queue *q)\n{\n\tWARN_ON_ONCE(!atomic_read(&q->mq_freeze_depth));\n\n\tblk_mq_sysfs_unregister(q);\n\n\tblk_mq_update_queue_map(q->mq_map, q->nr_hw_queues);\n\n\t/*\n\t * redo blk_mq_init_cpu_queues and blk_mq_init_hw_queues. FIXME: maybe\n\t * we should change hctx numa_node according to new topology (this\n\t * involves free and re-allocate memory, worthy doing?)\n\t */\n\n\tblk_mq_map_swqueue(q);\n\n\tblk_mq_sysfs_register(q);\n}\n\nstatic int blk_mq_queue_reinit_notify(struct notifier_block *nb,\n\t\t\t\t      unsigned long action, void *hcpu)\n{\n\tstruct request_queue *q;\n\n\t/*\n\t * Before new mappings are established, hotadded cpu might already\n\t * start handling requests. This doesn't break anything as we map\n\t * offline CPUs to first hardware queue. We will re-init the queue\n\t * below to get optimal settings.\n\t */\n\tif (action != CPU_DEAD && action != CPU_DEAD_FROZEN &&\n\t    action != CPU_ONLINE && action != CPU_ONLINE_FROZEN)\n\t\treturn NOTIFY_OK;\n\n\tmutex_lock(&all_q_mutex);\n\n\t/*\n\t * We need to freeze and reinit all existing queues.  Freezing\n\t * involves synchronous wait for an RCU grace period and doing it\n\t * one by one may take a long time.  Start freezing all queues in\n\t * one swoop and then wait for the completions so that freezing can\n\t * take place in parallel.\n\t */\n\tlist_for_each_entry(q, &all_q_list, all_q_node)\n\t\tblk_mq_freeze_queue_start(q);\n\tlist_for_each_entry(q, &all_q_list, all_q_node) {\n\t\tblk_mq_freeze_queue_wait(q);\n\n\t\t/*\n\t\t * timeout handler can't touch hw queue during the\n\t\t * reinitialization\n\t\t */\n\t\tdel_timer_sync(&q->timeout);\n\t}\n\n\tlist_for_each_entry(q, &all_q_list, all_q_node)\n\t\tblk_mq_queue_reinit(q);\n\n\tlist_for_each_entry(q, &all_q_list, all_q_node)\n\t\tblk_mq_unfreeze_queue(q);\n\n\tmutex_unlock(&all_q_mutex);\n\treturn NOTIFY_OK;\n}\n\nstatic int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)\n{\n\tint i;\n\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tset->tags[i] = blk_mq_init_rq_map(set, i);\n\t\tif (!set->tags[i])\n\t\t\tgoto out_unwind;\n\t}\n\n\treturn 0;\n\nout_unwind:\n\twhile (--i >= 0)\n\t\tblk_mq_free_rq_map(set, set->tags[i], i);\n\n\treturn -ENOMEM;\n}\n\n/*\n * Allocate the request maps associated with this tag_set. Note that this\n * may reduce the depth asked for, if memory is tight. set->queue_depth\n * will be updated to reflect the allocated depth.\n */\nstatic int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)\n{\n\tunsigned int depth;\n\tint err;\n\n\tdepth = set->queue_depth;\n\tdo {\n\t\terr = __blk_mq_alloc_rq_maps(set);\n\t\tif (!err)\n\t\t\tbreak;\n\n\t\tset->queue_depth >>= 1;\n\t\tif (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t} while (set->queue_depth);\n\n\tif (!set->queue_depth || err) {\n\t\tpr_err(\"blk-mq: failed to allocate request map\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (depth != set->queue_depth)\n\t\tpr_info(\"blk-mq: reduced tag depth (%u -> %u)\\n\",\n\t\t\t\t\t\tdepth, set->queue_depth);\n\n\treturn 0;\n}\n\nstruct cpumask *blk_mq_tags_cpumask(struct blk_mq_tags *tags)\n{\n\treturn tags->cpumask;\n}\nEXPORT_SYMBOL_GPL(blk_mq_tags_cpumask);\n\n/*\n * Alloc a tag set to be associated with one or more request queues.\n * May fail with EINVAL for various error conditions. May adjust the\n * requested depth down, if if it too large. In that case, the set\n * value will be stored in set->queue_depth.\n */\nint blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)\n{\n\tBUILD_BUG_ON(BLK_MQ_MAX_DEPTH > 1 << BLK_MQ_UNIQUE_TAG_BITS);\n\n\tif (!set->nr_hw_queues)\n\t\treturn -EINVAL;\n\tif (!set->queue_depth)\n\t\treturn -EINVAL;\n\tif (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN)\n\t\treturn -EINVAL;\n\n\tif (!set->ops->queue_rq || !set->ops->map_queue)\n\t\treturn -EINVAL;\n\n\tif (set->queue_depth > BLK_MQ_MAX_DEPTH) {\n\t\tpr_info(\"blk-mq: reduced tag depth to %u\\n\",\n\t\t\tBLK_MQ_MAX_DEPTH);\n\t\tset->queue_depth = BLK_MQ_MAX_DEPTH;\n\t}\n\n\t/*\n\t * If a crashdump is active, then we are potentially in a very\n\t * memory constrained environment. Limit us to 1 queue and\n\t * 64 tags to prevent using too much memory.\n\t */\n\tif (is_kdump_kernel()) {\n\t\tset->nr_hw_queues = 1;\n\t\tset->queue_depth = min(64U, set->queue_depth);\n\t}\n\n\tset->tags = kmalloc_node(set->nr_hw_queues *\n\t\t\t\t sizeof(struct blk_mq_tags *),\n\t\t\t\t GFP_KERNEL, set->numa_node);\n\tif (!set->tags)\n\t\treturn -ENOMEM;\n\n\tif (blk_mq_alloc_rq_maps(set))\n\t\tgoto enomem;\n\n\tmutex_init(&set->tag_list_lock);\n\tINIT_LIST_HEAD(&set->tag_list);\n\n\treturn 0;\nenomem:\n\tkfree(set->tags);\n\tset->tags = NULL;\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(blk_mq_alloc_tag_set);\n\nvoid blk_mq_free_tag_set(struct blk_mq_tag_set *set)\n{\n\tint i;\n\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tif (set->tags[i]) {\n\t\t\tblk_mq_free_rq_map(set, set->tags[i], i);\n\t\t\tfree_cpumask_var(set->tags[i]->cpumask);\n\t\t}\n\t}\n\n\tkfree(set->tags);\n\tset->tags = NULL;\n}\nEXPORT_SYMBOL(blk_mq_free_tag_set);\n\nint blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i, ret;\n\n\tif (!set || nr > set->queue_depth)\n\t\treturn -EINVAL;\n\n\tret = 0;\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tret = blk_mq_tag_update_depth(hctx->tags, nr);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (!ret)\n\t\tq->nr_requests = nr;\n\n\treturn ret;\n}\n\nvoid blk_mq_disable_hotplug(void)\n{\n\tmutex_lock(&all_q_mutex);\n}\n\nvoid blk_mq_enable_hotplug(void)\n{\n\tmutex_unlock(&all_q_mutex);\n}\n\nstatic int __init blk_mq_init(void)\n{\n\tblk_mq_cpu_init();\n\n\thotcpu_notifier(blk_mq_queue_reinit_notify, 0);\n\n\treturn 0;\n}\nsubsys_initcall(blk_mq_init);\n", "#ifndef BLK_INTERNAL_H\n#define BLK_INTERNAL_H\n\n#include <linux/idr.h>\n#include <linux/blk-mq.h>\n#include \"blk-mq.h\"\n\n/* Amount of time in which a process may batch requests */\n#define BLK_BATCH_TIME\t(HZ/50UL)\n\n/* Number of requests a \"batching\" process may submit */\n#define BLK_BATCH_REQ\t32\n\n/* Max future timer expiry for timeouts */\n#define BLK_MAX_TIMEOUT\t\t(5 * HZ)\n\nstruct blk_flush_queue {\n\tunsigned int\t\tflush_queue_delayed:1;\n\tunsigned int\t\tflush_pending_idx:1;\n\tunsigned int\t\tflush_running_idx:1;\n\tunsigned long\t\tflush_pending_since;\n\tstruct list_head\tflush_queue[2];\n\tstruct list_head\tflush_data_in_flight;\n\tstruct request\t\t*flush_rq;\n\n\t/*\n\t * flush_rq shares tag with this rq, both can't be active\n\t * at the same time\n\t */\n\tstruct request\t\t*orig_rq;\n\tspinlock_t\t\tmq_flush_lock;\n};\n\nextern struct kmem_cache *blk_requestq_cachep;\nextern struct kmem_cache *request_cachep;\nextern struct kobj_type blk_queue_ktype;\nextern struct ida blk_queue_ida;\n\nstatic inline struct blk_flush_queue *blk_get_flush_queue(\n\t\tstruct request_queue *q, struct blk_mq_ctx *ctx)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\n\tif (!q->mq_ops)\n\t\treturn q->fq;\n\n\thctx = q->mq_ops->map_queue(q, ctx->cpu);\n\n\treturn hctx->fq;\n}\n\nstatic inline void __blk_get_queue(struct request_queue *q)\n{\n\tkobject_get(&q->kobj);\n}\n\nstruct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,\n\t\tint node, int cmd_size);\nvoid blk_free_flush_queue(struct blk_flush_queue *q);\n\nint blk_init_rl(struct request_list *rl, struct request_queue *q,\n\t\tgfp_t gfp_mask);\nvoid blk_exit_rl(struct request_list *rl);\nvoid init_request_from_bio(struct request *req, struct bio *bio);\nvoid blk_rq_bio_prep(struct request_queue *q, struct request *rq,\n\t\t\tstruct bio *bio);\nint blk_rq_append_bio(struct request_queue *q, struct request *rq,\n\t\t      struct bio *bio);\nvoid blk_queue_bypass_start(struct request_queue *q);\nvoid blk_queue_bypass_end(struct request_queue *q);\nvoid blk_dequeue_request(struct request *rq);\nvoid __blk_queue_free_tags(struct request_queue *q);\nbool __blk_end_bidi_request(struct request *rq, int error,\n\t\t\t    unsigned int nr_bytes, unsigned int bidi_bytes);\n\nvoid blk_rq_timed_out_timer(unsigned long data);\nunsigned long blk_rq_timeout(unsigned long timeout);\nvoid blk_add_timer(struct request *req);\nvoid blk_delete_timer(struct request *);\n\n\nbool bio_attempt_front_merge(struct request_queue *q, struct request *req,\n\t\t\t     struct bio *bio);\nbool bio_attempt_back_merge(struct request_queue *q, struct request *req,\n\t\t\t    struct bio *bio);\nbool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,\n\t\t\t    unsigned int *request_count,\n\t\t\t    struct request **same_queue_rq);\n\nvoid blk_account_io_start(struct request *req, bool new_io);\nvoid blk_account_io_completion(struct request *req, unsigned int bytes);\nvoid blk_account_io_done(struct request *req);\n\n/*\n * Internal atomic flags for request handling\n */\nenum rq_atomic_flags {\n\tREQ_ATOM_COMPLETE = 0,\n\tREQ_ATOM_STARTED,\n};\n\n/*\n * EH timer and IO completion will both attempt to 'grab' the request, make\n * sure that only one of them succeeds\n */\nstatic inline int blk_mark_rq_complete(struct request *rq)\n{\n\treturn test_and_set_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags);\n}\n\nstatic inline void blk_clear_rq_complete(struct request *rq)\n{\n\tclear_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags);\n}\n\n/*\n * Internal elevator interface\n */\n#define ELV_ON_HASH(rq) ((rq)->cmd_flags & REQ_HASHED)\n\nvoid blk_insert_flush(struct request *rq);\n\nstatic inline struct request *__elv_next_request(struct request_queue *q)\n{\n\tstruct request *rq;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, NULL);\n\n\twhile (1) {\n\t\tif (!list_empty(&q->queue_head)) {\n\t\t\trq = list_entry_rq(q->queue_head.next);\n\t\t\treturn rq;\n\t\t}\n\n\t\t/*\n\t\t * Flush request is running and flush request isn't queueable\n\t\t * in the drive, we can hold the queue till flush request is\n\t\t * finished. Even we don't do this, driver can't dispatch next\n\t\t * requests and will requeue them. And this can improve\n\t\t * throughput too. For example, we have request flush1, write1,\n\t\t * flush 2. flush1 is dispatched, then queue is hold, write1\n\t\t * isn't inserted to queue. After flush1 is finished, flush2\n\t\t * will be dispatched. Since disk cache is already clean,\n\t\t * flush2 will be finished very soon, so looks like flush2 is\n\t\t * folded to flush1.\n\t\t * Since the queue is hold, a flag is set to indicate the queue\n\t\t * should be restarted later. Please see flush_end_io() for\n\t\t * details.\n\t\t */\n\t\tif (fq->flush_pending_idx != fq->flush_running_idx &&\n\t\t\t\t!queue_flush_queueable(q)) {\n\t\t\tfq->flush_queue_delayed = 1;\n\t\t\treturn NULL;\n\t\t}\n\t\tif (unlikely(blk_queue_bypass(q)) ||\n\t\t    !q->elevator->type->ops.elevator_dispatch_fn(q, 0))\n\t\t\treturn NULL;\n\t}\n}\n\nstatic inline void elv_activate_rq(struct request_queue *q, struct request *rq)\n{\n\tstruct elevator_queue *e = q->elevator;\n\n\tif (e->type->ops.elevator_activate_req_fn)\n\t\te->type->ops.elevator_activate_req_fn(q, rq);\n}\n\nstatic inline void elv_deactivate_rq(struct request_queue *q, struct request *rq)\n{\n\tstruct elevator_queue *e = q->elevator;\n\n\tif (e->type->ops.elevator_deactivate_req_fn)\n\t\te->type->ops.elevator_deactivate_req_fn(q, rq);\n}\n\n#ifdef CONFIG_FAIL_IO_TIMEOUT\nint blk_should_fake_timeout(struct request_queue *);\nssize_t part_timeout_show(struct device *, struct device_attribute *, char *);\nssize_t part_timeout_store(struct device *, struct device_attribute *,\n\t\t\t\tconst char *, size_t);\n#else\nstatic inline int blk_should_fake_timeout(struct request_queue *q)\n{\n\treturn 0;\n}\n#endif\n\nint ll_back_merge_fn(struct request_queue *q, struct request *req,\n\t\t     struct bio *bio);\nint ll_front_merge_fn(struct request_queue *q, struct request *req, \n\t\t      struct bio *bio);\nint attempt_back_merge(struct request_queue *q, struct request *rq);\nint attempt_front_merge(struct request_queue *q, struct request *rq);\nint blk_attempt_req_merge(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct request *next);\nvoid blk_recalc_rq_segments(struct request *rq);\nvoid blk_rq_set_mixed_merge(struct request *rq);\nbool blk_rq_merge_ok(struct request *rq, struct bio *bio);\nint blk_try_merge(struct request *rq, struct bio *bio);\n\nvoid blk_queue_congestion_threshold(struct request_queue *q);\n\nint blk_dev_init(void);\n\n\n/*\n * Return the threshold (number of used requests) at which the queue is\n * considered to be congested.  It include a little hysteresis to keep the\n * context switch rate down.\n */\nstatic inline int queue_congestion_on_threshold(struct request_queue *q)\n{\n\treturn q->nr_congestion_on;\n}\n\n/*\n * The threshold at which a queue is considered to be uncongested\n */\nstatic inline int queue_congestion_off_threshold(struct request_queue *q)\n{\n\treturn q->nr_congestion_off;\n}\n\nextern int blk_update_nr_requests(struct request_queue *, unsigned int);\n\n/*\n * Contribute to IO statistics IFF:\n *\n *\ta) it's attached to a gendisk, and\n *\tb) the queue had IO stats enabled when this request was started, and\n *\tc) it's a file system request\n */\nstatic inline int blk_do_io_stat(struct request *rq)\n{\n\treturn rq->rq_disk &&\n\t       (rq->cmd_flags & REQ_IO_STAT) &&\n\t\t(rq->cmd_type == REQ_TYPE_FS);\n}\n\n/*\n * Internal io_context interface\n */\nvoid get_io_context(struct io_context *ioc);\nstruct io_cq *ioc_lookup_icq(struct io_context *ioc, struct request_queue *q);\nstruct io_cq *ioc_create_icq(struct io_context *ioc, struct request_queue *q,\n\t\t\t     gfp_t gfp_mask);\nvoid ioc_clear_queue(struct request_queue *q);\n\nint create_task_io_context(struct task_struct *task, gfp_t gfp_mask, int node);\n\n/**\n * create_io_context - try to create task->io_context\n * @gfp_mask: allocation mask\n * @node: allocation node\n *\n * If %current->io_context is %NULL, allocate a new io_context and install\n * it.  Returns the current %current->io_context which may be %NULL if\n * allocation failed.\n *\n * Note that this function can't be called with IRQ disabled because\n * task_lock which protects %current->io_context is IRQ-unsafe.\n */\nstatic inline struct io_context *create_io_context(gfp_t gfp_mask, int node)\n{\n\tWARN_ON_ONCE(irqs_disabled());\n\tif (unlikely(!current->io_context))\n\t\tcreate_task_io_context(current, gfp_mask, node);\n\treturn current->io_context;\n}\n\n/*\n * Internal throttling interface\n */\n#ifdef CONFIG_BLK_DEV_THROTTLING\nextern bool blk_throtl_bio(struct request_queue *q, struct bio *bio);\nextern void blk_throtl_drain(struct request_queue *q);\nextern int blk_throtl_init(struct request_queue *q);\nextern void blk_throtl_exit(struct request_queue *q);\n#else /* CONFIG_BLK_DEV_THROTTLING */\nstatic inline bool blk_throtl_bio(struct request_queue *q, struct bio *bio)\n{\n\treturn false;\n}\nstatic inline void blk_throtl_drain(struct request_queue *q) { }\nstatic inline int blk_throtl_init(struct request_queue *q) { return 0; }\nstatic inline void blk_throtl_exit(struct request_queue *q) { }\n#endif /* CONFIG_BLK_DEV_THROTTLING */\n\n#endif /* BLK_INTERNAL_H */\n"], "filenames": ["block/blk-flush.c", "block/blk-mq-tag.c", "block/blk-mq-tag.h", "block/blk-mq.c", "block/blk.h"], "buggy_code_start_loc": [75, 432, 91, 562, 24], "buggy_code_end_loc": [315, 457, 91, 579, 24], "fixing_code_start_loc": [76, 432, 92, 561, 25], "fixing_code_end_loc": [329, 457, 104, 565, 31], "type": "CWE-264", "message": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.", "other": {"cve": {"id": "CVE-2015-9016", "sourceIdentifier": "security@android.com", "published": "2018-04-05T18:29:00.223", "lastModified": "2018-05-03T01:29:35.273", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046."}, {"lang": "es", "value": "En blk_mq_tag_to_rq en blk-mq.c en el kernel upstream, hay un posible uso de memoria previamente liberada debido a una condici\u00f3n de carrera cuando una petici\u00f3n ha sido previamente liberada por blk_mq_complete_request. Esto podr\u00eda conducir a un escalado de privilegios. Producto: Android. Versiones: Android kernel. Android ID: A-63083046."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.0, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-264"}, {"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:google:android:-:*:*:*:*:*:*:*", "matchCriteriaId": "F8B9FEC8-73B6-43B8-B24E-1F7C20D91D26"}]}]}], "references": [{"url": "https://github.com/torvalds/linux/commit/0048b4837affd153897ed1222283492070027aa9", "source": "security@android.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://source.android.com/security/bulletin/2018-02-01", "source": "security@android.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://www.debian.org/security/2018/dsa-4187", "source": "security@android.com"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0048b4837affd153897ed1222283492070027aa9"}}