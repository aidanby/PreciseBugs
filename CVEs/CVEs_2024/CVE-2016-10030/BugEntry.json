{"buggy_code": ["This file describes changes in recent versions of Slurm. It primarily\ndocuments those changes that are of interest to users and administrators.\n\n* Changes in Slurm 16.05.8\n==========================\n -- Remove StoragePass from being printed out in the slurmdbd log at debug2\n    level.\n -- Defer PATH search for task program until launch in slurmstepd.\n -- Modify regression test1.89 to avoid leaving vestigial job. Also reduce\n    logging to reduce likelyhood of Expect buffer overflow.\n -- Do not PATH search for mult-prog launches if LaunchParamters=test_exec is\n    enabled.\n -- Fix for possible infinite loop in select/cons_res plugin when trying to\n    satisfy a job's ntasks_per_core or socket specification.\n -- If job is held for bad constraints make it so once updated the job doesn't\n    go into JobAdminHeld.\n -- sched/backfill - Fix logic to reserve resources for jobs that require a\n    node reboot (i.e. to change KNL mode) in order to start.\n -- When unpacking a node or front_end record from state and the protocol\n    version is lower than the min version, set it to the min.\n -- Remove redundant lookup for part_ptr when updating a reservation's nodes.\n -- Fix memory and file descriptor leaks in slurmd daemon's sbcast logic.\n -- Do not allocate specialized cores to jobs using the --exclusive option.\n -- Cancel interactive job if Prolog failure with \"PrologFlags=contain\" or\n    \"PrologFlags=alloc\" configured. Send new error prolog failure message to\n    the salloc or srun command as needed.\n -- Prevent possible out-of-bounds read in slurmstepd on an invalid #! line.\n -- Fix check for PluginDir within slurmctld to work with multiple directories.\n -- Cancel interactive jobs automatically on communication error to launching\n    srun/salloc process.\n\n* Changes in Slurm 16.05.7\n==========================\n -- Fix issue in the priority/multifactor plugin where on a slurmctld restart,\n    where more time is accounted for than should be allowed.\n -- cray/busrt_buffer - If total_space in a pool decreases, reset used_space\n    rather than trying to account for buffer allocations in progress.\n -- cray/busrt_buffer - Fix for double counting of used_space at slurmctld\n    startup.\n -- Fix regression in 16.05.6 where if you request multiple cpus per task (-c2)\n    and request --ntasks-per-core=1 and only 1 task on the node\n    the slurmd would abort on an infinite loop fatal.\n -- cray/busrt_buffer - Internally track both allocated and unusable space.\n    The reported UsedSpace in a pool is now the allocated space (previously was\n    unusable space). Base available space on whichever value leaves least free\n    space.\n -- cray/burst_buffer - Preserve job ID and don't translate to job array ID.\n -- cray/burst_buffer - Update \"instance\" parsing to match updated dw_wlm_cli\n    output.\n -- sched/backfill - Insure we don't try to start a job that was already started\n    and requeued by the main scheduling logic.\n -- job_submit/lua - add access to the job features field in job_record.\n -- select/linear plugin modified to better support heterogeneous clusters when\n    topology/none is also configured.\n -- Permit cancellation of jobs in configuring state.\n -- acct_gather_energy/rapl - prevent segfault in slurmd from race to gather\n    data at slurmd startup.\n -- Integrate node_feature/knl_generic with \"hbm\" GRES information.\n -- Fix output routines to prevent rounding the TRES values for memory or BB.\n -- switch/cray plugin - fix use after free error.\n -- docs - elaborate on how way to clear TRES limits in sacctmgr.\n -- knl_cray plugin - Avoid abort from backup slurmctld at start time.\n -- cgroup plugins - fix two minor memory leaks.\n -- If a node is booting for some job, don't allocate additional jobs to the\n    node until the boot completes.\n -- testsuite - fix job id output in test17.39.\n -- Modify backfill algorithm to improve performance with large numbers of\n    running jobs. Group running jobs that end in a \"similar\" time frame using a\n    time window that grows exponentially rather than linearly. After one second\n    of wall time, simulate the termination of all remaining running jobs in\n    order to respond in a reasonable time frame.\n -- Fix slurm_job_cpus_allocated_str_on_node_id() API call.\n -- sched/backfill plugin: Make malloc match data type (defined as uint32_t and\n    allocated as int).\n -- srun - prevent segfault when terminating job step before step has launched.\n -- sacctmgr - prevent segfault when trying to reset usage for an invalid\n    account name.\n -- Make the openssl crypto plugin compile with openssl >= 1.1.\n -- Fix SuspendExcNodes and SuspendExcParts on slurmctld reconfiguration.\n -- sbcast - prevent segfault in slurmd due to race condition between file\n    transfers from separate jobs using zlib compression\n -- cray/burst_buffer - Increase time to synchronize operations between threads\n    from 5 to 60 seconds (\"setup\" operation time observed over 17 seconds).\n -- node_features/knl_cray - Fix possible race condition when changing node\n    state that could result in old KNL mode as an active features.\n -- Make sure if a job can't run because of resources we also check accounting\n    limits after the node selection to make sure it doesn't violate those limits\n    and if it does change the reason for waiting so we don't reserve resources\n    on jobs violating accounting limits.\n -- NRT - Make it so a system running against IBM's PE will work with PE\n    version 1.3.\n -- NRT - Make it so protocols pgas and test are allowed to be used.\n -- NRT - Make it so you can have more than 1 protocol listed in MP_MSG_API.\n -- cray/burst_buffer - If slurmctld daemon restarts with pending job and burst\n    buffer having unknown file stage-in status, teardown the buffer, defer the\n    job, and start stage-in over again.\n -- On state restore in the slurmctld don't overwrite the mem_spec_limit given\n    from the slurm.conf when using FastSchedule=0.\n -- Recognize a KNL's proper NUMA count (rather than setting it to the value\n    in slurm.conf) when using FastSchedule=0.\n -- Fix parsing in regression test1.92 for some prompts.\n -- sbcast - use slurmd's gid cache rather than a separate lookup.\n -- slurmd - return error if setgroups() call fails in _drop_privileges().\n -- Remove error messages about gres counts changing when a job is resized on\n    a slurmctld restart or reconfig, as they aren't really error messages.\n -- Fix possible memory corruption if a job is using GRES and changing size.\n -- jobcomp/elasticsearch - fix printf format for a value on 32-bit builds.\n -- task/cgroup - Change error message if CPU binding can not take place to\n    better identify the root cause of the problem.\n -- Fix issue where task/cgroup would not always honor --cpu_bind=threads.\n -- Fix race condition in with getgrouplist() in slurmd that can lead to\n    user accounts being granted access to incorrect group memberships during\n    job launch.\n\n* Changes in Slurm 16.05.6\n==========================\n -- Docs - the correct default value for GroupUpdateForce is 0.\n -- mpi/pmix - improve point to point communication performance.\n -- SlurmDB - include pending jobs in search during 'sacctmgr show runawayjobs'.\n -- Add client side out-of-range checks to --nice flag.\n -- Fix support for sbatch \"-W\" option, previously eeded to use \"--wait\".\n -- node_features/knl_cray plugin and capmc_suspend/resume programs modified to\n    sleep and retry capmc operations if the Cray State Manager is down. Added\n    CapmcRetries configuration parameter to knl_cray.conf.\n -- node_features/knl_cray plugin: Remove any KNL MCDRAM or NUMA features from\n    node's configuration if capmc does NOT report the node as being KNL.\n -- node_features/knl_cray plugin: drain any node not reported by\n    \"capmc node_status\" on startup or reconfig.\n -- node_features/knl_cray plugin: Substantially streamline and speed up logic\n    to load current node state on reconfigure failure or unexpected node boot.\n -- node_features/knl_cray plugin: Add separate thread to interact with capmc\n    in response to unexpected node reboots.\n -- node_features plugin - Add \"mode\" argument to node_features_p_node_xlate()\n    function to fix some bugs updating a node's features using the node update\n    RPC.\n -- node_features/knl_cray plugin: If the reconfiguration of nodes for an\n    interactive job fails, kill the job (it can't be requeued like a batch job).\n -- Testsuite - Added srun/salloc/sbatch tests with --use-min-nodes option.\n -- Fix typo when an error occurs when discovering pmix version on\n    configure.\n -- Fix configuring pmix support when you have your lib dir symlinked to lib64.\n -- Fix waiting reason if a job is waiting for a specific limit instead of\n    always just AccountingPolicy.\n -- Correct SchedulerParameters=bf_busy_nodes logic with respect to the job's\n    minimum node count. Previous logic would not decremement counter in some\n    locations and reject valid job request for not reaching minimum node count.\n -- Fix FreeBSD-11 build by using llabs() function in place of abs().\n -- Cray: The slurmd can manipulate the socket/core/thread values reported based\n    upon the configuration. The logic failed to consider select/cray with\n    SelectTypeParameters=other_cons_res as equivalent to select/cons_res.\n -- If a node's socket or core count are changed at registration time (e.g. a\n    KNL node's NUMA mode is changed), change it's board count to match.\n -- Prevent possible divide by zero in select/cons_res if a node's board count\n    is higher than it's socket count.\n -- Allow an advanced reservation to contain a license count of zero.\n -- Preserve non-KNL node features when updating the KNL node features for a\n    multi-node job in which the non-KNL node features vary by node.\n -- task/affinity plugin: Honor a job's --ntasks-per-socket and\n    --ntasks-per-core options in task binding.\n -- slurmd - do not print ClusterName when using 'slurmd -C'.\n -- Correct a bitmap test function (used only by the select/bluegene plugin).\n -- Do not propagate SLURM_UMASK environment variable to batch script.\n -- Added node_features/knl_generic plugin for KNL support on non-Cray systems.\n -- Cray: Prevent abort in backfill scheduling logic for requeued job that has\n    been cancelled while NHC is running.\n -- Improve reported estimates of start and end times for pending jobs.\n -- pbsnodes: Show OS value as \"unknown\" for down nodes.\n -- BlueGene - correctly scale node counts when enforcing MaxNodes limit take 2.\n -- Fix \"sbatch --hold\" to set JobHeldUser correctly instead of JobHeldAdmin.\n -- Cray - print warning that task/cgroup is required, and must be after\n    task/cray in the TaskPlugin settings.\n -- Document that node Weight takes precedence over load with LLN scheduling.\n -- Fix issue where gang scheduling could happen even with OverSubscribe=NO.\n -- Expose JOB_SHARED_* values to job_submit/lua plugin.\n -- Fix issue where number of nodes is not properly allocated when srun is\n    requested with -n tasks < hosts from -w hostlist.\n -- Update srun documentation for -N, -w and -m arbitrary.\n -- Fix bug that was clearing MAINT mode on nodes scheduled for reboot (bug\n    introduced in version 16.05.5 to address bug in overlapping reservations).\n -- Add logging of node reboot requests.\n -- Docs - remove recommendation for ReleaseAgent setting in cgroup.conf.\n -- Make sure a job cleans up completely if it has a node fail.  Mostly an\n    issue with gang scheduling.\n\n* Changes in Slurm 16.05.5\n==========================\n -- Fix accounting for jobs requeued after the previous job was finished.\n -- slurmstepd modified to pre-load all relevant plugins at startup to avoid\n    the possibility of modified plugins later resulting in inconsistent API\n    or data structures and a failure of slurmstepd.\n -- Export functions from parse_time.c in libslurm.so.\n -- Export unit convert functions from slurm_protocol_api.c in libslurm.so.\n -- Fix scancel to allow multiple steps from a job to be cancelled at once.\n -- Update and expand upgrade guide (in Quick Start Administrator web page).\n -- burst_buffer/cray: Requeue, but do not hold a job which fails the pre_run\n    operation.\n -- Insure reported expected job start time is not in the past for pending jobs.\n -- Add support for PMIx v2.\n -- mpi/pmix: support for passing TMPDIR path through info key\n -- Cray: update slurmconfgen_smw.py script to correctly identify service nodes\n    versus compute nodes.\n -- FreeBSD - fix build issue in knl_cray plugin.\n -- Corrections to gres.conf parsing logic.\n -- Make partition State independent of EnforcePartLimits value.\n -- Fix multipart srun submission with EnforcePartLimits=NO and job violating\n    the partition limits.\n -- Fix problem updating job state_reason.\n -- pmix - Provide HWLOC topology in the job-data if Slurm was configured\n    with hwloc.\n -- Cray - Fix issue restoring jobs when blade count increases due to hardware\n    reconfiguration.\n -- burst_buffer/cray - Hold job after 3 failed pre-run operations.\n -- sched/backfill - Check that a user's QOS is allowed to use a partition\n    before trying to schedule resources on that partition for the job.\n -- sacctmgr - Fix displaying nodenames when printing out events or\n    reservations.\n -- Fix mpiexec wrapper to accept task count with more than one digit.\n -- Add mpiexec man page to the script.\n -- Add salloc_wait_nodes option to the SchedulerParameters parameter in the\n    slurm.conf file controlling when the salloc command returns in relation to\n    when nodes are ready for use (i.e. booted).\n -- Handle case when slurmctld daemon restart while compute node reboot in\n    progress. Return node to service rather than setting DOWN.\n -- Preserve node \"RESERVATION\" state when one of multiple overlapping\n    reservations ends.\n -- Restructure srun command locking for task_exit processing logic for improved\n    parallelism.\n -- Modify srun task completion handling to only build the task/node string for\n    logging purposes if it is needed. Modified for performance purposes.\n -- Docs - update salloc/sbatch/srun man pages to mention corresponding\n    environment variables for --mem/--mem-per-cpu and allowed suffixes.\n -- Silence srun warning when overriding the job ntasks-per-node count\n    with a lower task count for the step.\n -- Docs - assorted spelling fixes.\n -- node_features/knl_cray: Fix bug where MCDRAM state could be taken from\n    capmc rather than cnselect.\n -- node_features/knl_cray: If a node is rebooted outside of Slurm's direction,\n    update it's active features with current MCDRAM and NUMA mode information.\n -- Restore ability to manually power down nodes, broken in 15.08.12.\n -- Don't log error for job end_time being zero if node health check is still\n    running.\n -- When powering up a node to change it's state (e.g. KNL NUMA or MCDRAM mode)\n    then pass to the ResumeProgram the job ID assigned to the nodes in the\n    SLURM_JOB_ID environment variable.\n -- Allow a node's PowerUp state flag to be cleared using update_node RPC.\n -- capmc_suspend/resume - If a request modify NUMA or MCDRAM state on a set of\n    nodes or reboot a set of nodes fails then just requeue the job and abort the\n    entire operation rather than trying to operate on individual nodes.\n -- node_features/knl_cray plugin: Increase default CapmcTimeout parameter from\n    10 to 60 seconds.\n -- Fix squeue filter by job license when a job has requested more than 1\n    license of a certain type.\n -- Fix bug in PMIX_Ring in the pmi2 plugin so that it supports singleton mode.\n    It also updates the testpmixring.c test program so it can be used to check\n    singleton runs.\n -- Automically cleanup task/cgroup cpuset and devices cgroups after steps are\n    done.\n -- Testsuite - Fix test1.83 to handle gaps in node names properly.\n -- BlueGene - correctly scale node counts when enforcing MaxNodes limit.\n -- Make sure no attempt is made to schedule a requeued job until all steps are\n    cleaned (Node Health Check completes for all steps on a Cray).\n -- KNL: Correct task affinity logic for some NUMA modes.\n -- Add salloc/sbatch/srun --priority option of \"TOP\" to set job priority to\n    the highest possible value. This option is only available to Slurm operators\n    and administrators.\n -- Add salloc/sbatch/srun option --use-min-nodes to prefer smaller node counts\n    when a range of node counts is specified (e.g. \"-N 2-4\").\n -- Validate salloc/sbatch --wait-all-nodes argument.\n -- Add \"sbatch_wait_nodes\" to SchedulerParameters to control default sbatch\n    behaviour with respect to waiting for all allocated nodes to be ready for\n    use. Job can override the configuration option using the --wait-all-nodes=#\n    option.\n -- Prevent partition group access updates from resetting last_part_update when\n    no changes have been made. Prevents backfill scheduler from restarting\n    mid-cycle unnecessarily.\n -- Cray - add NHC_ABSOLUTELY_NO to never run NHC, even on certain edge cases\n    that it would otherwise be run on with NHC_NO.\n -- Ignore GRES/QOS updates that maintain the same value as before.\n -- mpi/pmix - prepare temp directory for application.\n -- Fix display for the nice and priority values in sprio/scontrol/squeue.\n\n* Changes in Slurm 16.05.4\n==========================\n -- Fix potential deadlock if running with message aggregation.\n -- Streamline when schedule() is called when running with message aggregation\n    on batch script completes.\n -- Fix incorrect casting when [un]packing derived_ec on slurmdb_job_rec_t.\n -- Document that persistent burst buffers can not be created or destroyed using\n    the salloc or srun --bb options.\n -- Add support for setting the SLURM_JOB_ACCOUNT, SLURM_JOB_QOS and\n    SLURM_JOB_RESERVAION environment variables are set for the salloc command.\n    Document the same environment variables for the salloc, sbatch and srun\n    commands in their man pages.\n -- Fix issue where sacctmgr load cluster.cfg wouldn't load associations\n    that had a partition in them.\n -- Don't return the extern step from sstat by default.\n -- In sstat print 'extern' instead of 4294967295 for the extern step.\n -- Make advanced reservations work properly with core specialization.\n -- Fix race condition in the account_gather plugin that could result in job\n    stuck in COMPLETING state.\n -- Regression test fixes if SelectTypePlugin not managing memory and no node\n    memory size set (defaults to 1 MB per node).\n -- Add missing partition write locks to _slurm_rpc_dump_nodes/node_single to\n    prevent a race condition leading to inconsistent sinfo results.\n -- Fix task:CPU binding logic for some processors. This bug was introduced\n    in version 16.05.1 to address KNL bunding problem.\n -- Fix two minor memory leaks in slurmctld.\n -- Improve partition-specific limit logging from slurmctld daemon.\n -- Fix incorrect access check when using MaxNodes setting on the partition.\n -- Fix issue with sacctmgr when specifying a list of clusters to query.\n -- Fix issue when calculating future StartTime for a job.\n -- Make EnforcePartLimit support logic work with any ordering of partitions\n    in job submit request.\n -- Prevent restoration of wrong CPU governor and frequency when using\n    multiple task plugins.\n -- Prevent slurmd abort if hwloc library fails to populate the \"children\"\n    arrays (observed with hwloc version \"dev-333-g85ea6e4\").\n -- burst_buffer/cray: Add \"--groupid\" to DataWarp \"setup\" command.\n -- Fix lustre profiling putting it in the Filesystem dataset instead of the\n    Network dataset.\n -- Fix profiling documentation and code to match be consistent with\n    Filesystem instead of Lustre.\n -- Correct the way watts is calculated in the rapl plugin when using a poll\n    frequency other than AcctGatherNodeFreq.\n -- Don't about step launch if job reaches expected end time while node is\n    configuring/booting (NOTE: The job end time will be adjusted after node\n    becomes ready for use).\n -- Fix several print routines to respect a custom output delimiter when\n    printing NO_VAL or INFINITE.\n -- Correct documented configurations where --ntasks-per-core and\n    --ntasks-per-socket are supported.\n -- task/affinity plugin buffer allocated too small, can corrupt memory.\n\n* Changes in Slurm 16.05.3\n==========================\n -- Make it so the extern step uses a reverse tree when cleaning up.\n -- If extern step doesn't get added into the proctrack plugin make sure the\n    sleep is killed.\n -- Fix areas the slurmctld can segfault if an extern step is in the system\n    cleaning up on a restart.\n -- Prevent possible incorrect counting of GRES of a given type if a node has\n    the multiple \"types\" of a given GRES \"name\", which could over-subscribe\n    GRES of a given type.\n -- Add web links to Slurm Diamond Collectors (from Harvard University) and\n    collectd (from EDF).\n -- Add job_submit plugin for the \"reboot\" field.\n -- Make some more Slurm constants (INFINITE, NO_VAL64, etc.) available to\n    job_submit/lua plugins.\n -- Send in a -1 for a taskid into spank_task_post_fork for the extern_step.\n -- MYSQL - Sightly better logic if a job completion comes in with an end time\n    of 0.\n -- task/cgroup plugin is configured with ConstrainRAMSpace=yes, then set soft\n    memory limit to allocated memory limit (previously no soft limit was set).\n -- Document limitations in burst buffer use by the salloc command (possible\n    access problems from a login node).\n -- Fix proctrack plugin to only add the pid of a process once\n    (regression in 16.05.2).\n -- Fix for sstat to print correct info when requesting jobid.batch as part of\n    a comma-separated list.\n -- CRAY - Fix issue if pid has already been added to another job container.\n -- CRAY - Fix add of extern step to AELD.\n -- burstbufer/cray: avoid batch submit error condition if waiting for stagein.\n -- CRAY - Fix for reporting steps lingering after they are already finished.\n -- Testsuite - fix test1.29 / 17.15 for limits with values above 32-bits.\n -- CRAY - Simplify when a NHC is called on a step that has unkillable\n    processes.\n -- CRAY - If trying to kill a step and you have NHC_NO_STEPS set run NHC\n    anyway to attempt to log the backtraces of the potential\n    unkillable processes.\n -- Fix gang scheduling and license release logic if single node job killed on\n    bad node.\n -- Make scontrol show steps show the extern step correctly.\n -- Do not scheduled powered down nodes in FAILED state.\n -- Do not start slurmctld power_save thread until partition information is read\n    in order to prevent race condition that can result invalid pointer when\n    trying to resolve configured SuspendExcParts.\n -- Add SLURM_PENDING_STEP id so it won't be confused with SLURM_EXTERN_CONT.\n -- Fix for core selection with job --gres-flags=enforce-binding option.\n    Previous logic would in some cases allocate a job zero cores, resulting in\n    slurmctld abort.\n -- Minimize preempted jobs for configurations with multiple jobs per node.\n -- Improve partition AllowGroups caching. Update the table of UIDs permitted to\n    use a partition based upon it's AllowGroups configuration parameter as new\n    valid UIDs are found rather than looking up that user's group information\n    for every job they submit. If the user is now allowed to use the partition,\n    then do not check that user's group access again for 5 seconds.\n -- Add routing queue information to Slurm FAQ web page.\n -- Do not select_g_step_finish() a SLURM_PENDING_STEP step, as nothing has\n    been allocated for the step yet.\n -- Fixed race condition in PMIx Fence logic.\n -- Prevent slurmctld abort if job is killed or requeued while waiting for\n    reboot of its allocated compute nodes.\n -- Treat invalid user ID in AllowUserBoot option of knl.conf file as error\n    rather than fatal (log and do not exit).\n -- qsub - When doing the default output files for an array in qsub style\n    make them using the master job ID instead of the normal job ID.\n -- Create the extern step while creating the job instead of waiting until the\n    end of the job to do it.\n -- Always report a 0 exit code for the extern step instead of being canceled\n    or failed based on the signal that would always be killing it.\n -- Fix to allow users to update QOS of pending jobs.\n -- CRAY - Fix minor memory leak in switch plugin.\n -- CRAY - Change slurmconfgen_smw.py to skip over disabled nodes.\n -- Fix eligible_time for elasticsearch as well as add queue_wait\n    (difference between start of job and when it was eligible).\n\n* Changes in Slurm 16.05.2\n==========================\n -- CRAY - Fix issue where the proctrack plugin could hang if the container\n    id wasn't able to be made.\n -- Move test for job wait reason value of BurstBufferResources and\n    BurstBufferStageIn later in the scheduling logic.\n -- Document which srun options apply to only job, only step, or job and step\n    allocations.\n -- Use more compatible function to get thread name (>= 2.6.11).\n -- Fix order of job then step id when noting cleaning flag being set.\n -- Make it so the extern step sends a message with accounting information\n    back to the slurmctld.\n -- Make it so the extern step calls the select_g_step_start|finish functions.\n -- Don't print error when extern step is canceled because job is ending.\n -- Handle a few error codes when dealing with the extern step to make sure\n    we have the pids added to the system correctly.\n -- Add support for job dependencies with job array expressions. Previous logic\n    required listing each task of job array individually.\n -- Make sure tres_cnt is set before creating a slurmdb_assoc_usage_t.\n -- Prevent backfill scheduler from starting a second \"singleton\" job if another\n    one started during a backfill sleep.\n -- Fix for invalid array pointer when creating advanced reservation when job\n    allocations span heterogeneous nodes (differing core or socket counts).\n -- Fix hostlist_ranged_string_xmalloc_dims to correctly not put brackets on\n    hostlists when brackets == 0.\n -- Make sure we don't get brackets when making a range of reserved ports\n    for a step.\n -- Change fatal to an error if port ranges aren't correct when reading state\n    for steps.\n\n* Changes in Slurm 16.05.1\n==========================\n -- Fix __cplusplus macro in spank.h to allow compilation with C++.\n -- Fix compile issue with older glibc < 2.12\n -- Fix for starting batch step with mpi/pmix plugin.\n -- Fix for \"scontrol -dd show job\" with respect to displaying the specific\n    CPUs allocated to a job on each node. Prior logic would only display\n    the CPU information for the first node in the job allocation.\n -- Print correct return code on failure to update active node features\n    through sview.\n -- Allow QOS timelimit to override partition timelimit when EnforcePartLimits\n    is set to all/any.\n -- Make it so qsub will do a \"basename\" on a wrapped command for the output\n    and error files.\n -- Fix issue where slurmd could core when running the ipmi energy plugin.\n -- Documentation - clean up typos.\n -- Add logic so that slurmstepd can be launched under valgrind.\n -- Increase buffer size to read /proc/*/stat files.\n -- Fix for tracking job resource allocation when slurmctld is reconfigured\n    while Cray Node Health Check (NHC) is running. Previous logic would fail to\n    record the job's allocation then perform release operation upon NHC\n    completion, resulting in underflow error messages.\n -- Make \"scontrol show daemons\" work with long node names.\n -- CRAY - Collect energy using a uint64_t instead of uint32_t.\n -- Fix incorrect if statements when determining if the user has a default\n    account or wckey.\n -- Prevent job stuck in configuring state if slurmctld daemon restarted while\n    PrologSlurmctld is running. Also re-issue burst_buffer/pre-load operation\n    as needed.\n -- Correct task affinity support for FreeBSD.\n -- Fix for task affinity on KNL in SNC2/Flat mode.\n -- Recalculate a job's memory allocation after node reboot if job requests all\n    of a node's memory and FastSchedule=0 is configured. Intel KNL memory size\n    can change on reboot with various MCDRAM modes.\n -- Fix small memory leak when printing HealthCheckNodeState.\n -- Eliminate memory leaks when AuthInfo is configured.\n -- Improve sdiag output description in man page.\n -- Cray/capmc_resume script modify a node's features (as needed) when the\n    reinit (reboot) command is issued rather than wait for the nodes to change\n    to the \"on\" state.\n -- Correctly print ranges when using step values in job arrays.\n -- Allow from file names / paths over 256 characters when launching steps,\n    as well as spaces in the executable name.\n -- job_submit.license.lua example modified to send message back to user.\n -- Document job --mem=0 option means all memory on a node.\n -- Set SLURM_JOB_QOS environment variable to QOS name instead of description.\n -- knl_cray.conf file option of CnselectPath added.\n -- node_features/knl_cray plugin modified to get current node NUMA and MCDRAM\n    modes using cnselect command rather than capmc command.\n -- liblua - add SLES12 paths to runtime search list.\n -- Fix qsub default output and error files for task arrays.\n -- Fix qsub to set job_name correctly when wrapping a script (-b y)\n -- Cray - set EnforcePartLimits=any in slurm.conf template.\n\n* Changes in Slurm 16.05.0\n==========================\n -- Update seff to fix warnings with ncpus, and list slurm-perlapi dependency\n    in spec file.\n -- Fix testsuite to consistent use /usr/bin/env {bash,expect} construct.\n -- Cray - Ensure that step completion messages get to the database.\n -- Fix step cpus_per_task calculation for heterogeneous job allocation.\n -- Fix --with-json= configure option to use specified path.\n -- Add back thread_id to \"thread_id\" LogTimeFormat to distinguish between\n    mutliple threads with the same name. Now displays thread name and id.\n -- Change how Slurm determines the NUMA count of a node. Ignore KNL NUMA\n    that only include memory.\n -- Cray - Fix node list parsing in capmc_suspend/resume programs.\n -- Fix sbatch #BSUB parsing for -W and -M options.\n -- Fix GRES task layout bug that could cause slurmctld to abort.\n -- Fix to --gres-flags=enforce-binding logic when multiple sockets needed.\n\n* Changes in Slurm 16.05.0rc2\n=============================\n -- Cray node shutdown/reboot scripts, perform operations on all nodes in one\n    capmc command. Only if that fails, issue the operations in parallel on\n    individual nodes. Required for scalability.\n -- Cleanup two minor Coverity warnings.\n -- Make it so the tres units in a job's formatted string are converted like\n    they are in a step.\n -- Correct partition's MaxCPUsPerNode enforcement when nodes are shared by\n    multiple partitions.\n -- node_feature/knl_cray - Prevent slurmctld GRES errors for \"hbm\" references.\n -- Display thread name instead of thread id and remove process name in stderr\n    logging for \"thread_id\" LogTimeFormat.\n -- Log IP address of bad incomming message to slurmctld.\n -- If a user requests tasks, nodes and ntasks-per-node and\n    tasks-per-node/nodes != tasks print warning and ignore ntasks-per-node.\n -- Release CPU \"owner\" file locks.\n -- Fix for job step memory allocation: Reject invalid step at submit time\n    rather than leaving it queued.\n -- Whenever possible, avoid allocating nodes that require a reboot.\n\n* Changes in Slurm 16.05.0rc1\n==============================\n -- Remove the SchedulerParameters option of \"assoc_limit_continue\", making it\n    the default value. Add option of \"assoc_limit_stop\". If \"assoc_limit_stop\"\n    is set and a job cannot start due to association limits, then do not attempt\n    to initiate any lower priority jobs in that partition. Setting this can\n    decrease system throughput and utlization, but avoid potentially starving\n    larger jobs by preventing them from launching indefinitely.\n -- Update a node's socket and cores per socket counts as needed after a node\n    boot to reflect configuration changes which can occur on KNL processors.\n    Note that the node's total core count must not change, only the distribution\n    of cores across varying socket counts (KNL NUMA nodes treated as sockets by\n    Slurm).\n -- Rename partition configuration from \"Shared\" to \"OverSubscribe\". Rename\n    salloc, sbatch, srun option from \"--shared\" to \"--oversubscribe\". The old\n    options will continue to function. Output field names also changed in\n    scontrol, sinfo, squeue and sview.\n -- Add SLURM_UMASK environment variable to user job.\n -- knl_conf: Added new configuration parameter of CapmcPollFreq.\n -- squeue: remove errant spaces in column formats for \"squeue -o %all\".\n -- Add ARRAY_TASKS mail option to send emails to each task in a job array.\n -- Change default compression library for sbcast to lz4.\n -- select/cray - Initiate step node health check at start of step termination\n    rather than after application completely ends so that NHC can capture\n    information about hung (non-killable) processes.\n -- Add --units=[KMGTP] option to sacct to display values in specific unit type.\n -- Modify sacct and sacctmgr to display TRES values in converted units.\n -- Modify sacctmgr to accept TRES values with [KMGTP] suffixes.\n -- Replace hash function with more modern SipHash functions.\n -- Add \"--with-cray_dir\" build/configure option.\n -- BB- Only send stage_out email when stage_out is set in script.\n -- Add r/w locking to file_bcast receive functions in slurmd.\n -- Add TopologyParam option of \"TopoOptional\" to optimize network topology\n    only for jobs requesting it.\n -- Fix build on FreeBSD.\n -- Configuration parameter \"CpuFreqDef\" used to set default governor for job\n    step not specifying --cpu-freq (previously the parameter was unused).\n -- Fix sshare -o<format> to correctly display new lengths.\n -- Update documentation to rename Shared option to OverSubscribe.\n -- Update documentation to rename partition Priority option to PriorityTier.\n -- Prevent changing of QOS on running jobs.\n -- Update accounting when changing QOS on pending jobs.\n -- Add support to ntasks_per_socket in task/affinity.\n -- Generate init.d and systemd service scripts in etc/ through Make rather\n    than at configure time to ensure all variable substitutions happen.\n -- Use TaskPluginParam for default task binding if no user specified CPU\n    binding. User --cpu_bind option takes precident over default. No longer\n    any error if user --cpu_bind option does not match TaskPluginParam.\n -- Make sacct and sattach work with older slurmd versions.\n -- Fix protocol handling between 15.08 and 16.05 for 'scontrol show config'.\n -- Enable prefixes (e.g. info, debug, etc.) in slurmstepd debugging.\n\n* Changes in Slurm 16.05.0pre2\n==============================\n -- Split partition's \"Priority\" field into \"PriorityTier\" (used to order\n    partitions for scheduling and preemption) plus \"PriorityJobFactor\" (used by\n    priority/multifactor plugin in calculating job priority, which is used to\n    order jobs within a partition for scheduling).\n -- Revert call to getaddrinfo, restoring gethostbyaddr (introduced in Slurm\n    16.05.0pre1) which was failing on some systems.\n -- knl_cray.conf - Added AllowMCDRAM, AllowNUMA and ALlowUserBoot\n    configuration options.\n -- Add node_features_p_user_update() function to node_features plugin.\n -- Don't print Weight=1 lines in 'scontrol write config' (its the default).\n -- Remove PARAMS macro from slurm.h.\n -- Remove BEGIN_C_DECLS and END_C_DECLS macros.\n -- Check that PowerSave mode configured for node_features/knl_cray plugin.\n    It is required to reconfigure and reboot nodes.\n -- Update documentation to reflect new cgroup default location change from\n    /cgroup to /sys/fs/cgroup.\n -- If NodeHealthCheckProgram configured HealthCheckInterval is non-zero, then\n    modify slurmd to run it before registering with slurmctld.\n -- Fix for tasks being packed onto cores when the requested --cpus-per-task is\n    greater than the number of threads on a core and --ntasks-per-core is 1.\n -- Make it so jobs/steps track ':' named gres/tres, before hand gres/gpu:tesla\n    would only track gres/gpu, now it will track both gres/gpu and\n    gres/gpu:tesla as separate gres if configured like\n    AccountingStorageTRES=gres/gpu,gres/gpu:tesla\n -- Added new job dependency type of \"aftercorr\" which will start a task of a\n    job array after the corresponding task of another job array completes.\n -- Increase default MaxTasksPerNode configuration parameter from 128 to 512.\n -- Enable sbcast data compression logic (compress option previously ignored).\n -- Add --compress option to srun command for use with --bcast option.\n -- Add TCPTimeout option to slurm[dbd].conf. Decouples MessageTimeout from TCP\n    connections.\n -- Don't call primary controller for every RPC when backup is in control.\n -- Add --gres-flags=enforce-binding option to salloc, sbatch and srun commands.\n    If set, the only CPUs available to the job will be those bound to the\n    selected GRES (i.e. the CPUs identifed in the gres.conf file will be\n    strictly enforced rather than advisory).\n -- Change how a node's allocated CPU count is calculated to avoid double\n    counting CPUs allocated to multiple jobs at the same time.\n -- Added SchedulingParameters option of \"bf_min_prio_reserve\". Jobs below\n    the specified threshold will not have resources reserved for them.\n -- Added \"sacctmgr show lostjobs\" to report any orphaned jobs in the database.\n -- When a stepd is about to shutdown and send it's response to srun\n    make the wait to return data only hit after 500 nodes and configurable\n    based on the TcpTimeout value.\n -- Add functionality to reset the lft and rgt values of the association table\n    with the slurmdbd.\n -- Add SchedulerParameter no_env_cache, if set no env cache will be use when\n    launching a job, instead the job will fail and drain the node if the env\n    isn't loaded normally.\n\n* Changes in Slurm 16.05.0pre1\n==============================\n -- Add sbatch \"--wait\" option that waits for job completion before exiting.\n    Exit code will match that of spawned job.\n -- Modify advanced reservation save/restore logic for core reservations to\n    support configuration changes (changes in configured nodes or cores counts).\n -- Allow ControlMachine, BackupController, DbdHost and DbdBackupHost to be\n    either short or long hostname.\n -- Job output and error files can now contain \"%\" character by specifying\n    a file name with two consecutive \"%\" characters. For example,\n    \"sbatch -o \"slurm.%%.%j\" for job ID 123 will generate an output file named\n    \"slurm.%.123\".\n -- Pass user name in Prolog RPC from controller to slurmd when using\n    PrologFlags=Alloc. Allows SLURM_JOB_USER env variable to be set when using\n    Native Slurm on a Cray.\n -- Add \"NumTasks\" to job information visible to Slurm commands.\n -- Add mail wrapper script \"smail\" that will include job statistics in email\n    notification messages.\n -- Remove vestigial \"SICP\" job option (inter-cluster job option). Completely\n    different logic will be forthcoming.\n -- Fix case where the primary and backup dbds would both be performing rollup.\n -- Add an ack reply from slurmd to slurmstepd when job setup is done and the\n    job is ready to be executed.\n -- Removed support for authd. authd has not been developed and supported since\n    several years. \n -- Introduce a new parameter requeue_setup_env_fail in SchedulerParameters.\n    A job that fails to setup the environment will be requeued and the node\n    drained.\n -- Add ValidateTimeout and OtherTimeout to \"scontrol show burst\" output.\n -- Increase default sbcast buffer size from 512KB to 8MB.\n -- Enable the hdf5 profiling of the batch step.\n -- Eliminate redundant environment and script files for job arrays.\n -- Stop searching sbatch scripts for #PBS directives after 100 lines of\n    non-comments. Stop parsing #PBS or #SLURM directives after 1024 characters\n    into a line. Required for decent perforamnce with huge scripts.\n -- Add debug flag for timing Cray portions of the code.\n -- Remove all *.la files from RPMs.\n -- Add Multi-Category Security (MCS) infrastructure to permit nodes to be bound\n    to specific users or groups.\n -- Install the pmi2 unix sockets in slurmd spool directory instead of /tmp.\n -- Implement the getaddrinfo and getnameinfo instead of gethostbyaddr and\n    gethostbyname.\n -- Finished PMIx implementation.\n -- Implemented the --without=package option for configure.\n -- Fix sshare to show each individual cluster with -M,--clusters option.\n -- Added --deadline option to salloc, sbatch and srun. Jobs which can not be\n    completed by the user specified deadline will be terminated with a state of\n    \"Deadline\" or \"DL\".\n -- Implemented and documented PMIX protocol which is used to bootstrap an\n    MPI job. PMIX is an alternative to PMI and PMI2.\n -- Change default CgroupMountpoint (in cgroup.conf) from \"/cgroup\" to\n    \"/sys/fs/cgroup\" to match current standard.\n -- Add #BSUB options to sbatch to read in from the batch script.\n -- HDF: Change group name of node from nodename to nodeid.\n -- The partition-specific SelectTypeParameters parameter can now be used to\n    change the memory allocation tracking specification in the global\n    SelectTypeParameters configuration parameter. Supported partition-specific\n    values are CR_Core, CR_Core_Memory, CR_Socket and CR_Socket_Memory. If the\n    global SelectTypeParameters value includes memory allocation management and\n    the partition-specific value does not, then memory allocation management for\n    that partition will NOT be supported (i.e. memory can be over-allocated).\n    Likewise the global SelectTypeParameters might not include memory management\n    while the partition-specific value does.\n -- Burst buffer/cray - Add support for multiple buffer pools including support\n    for different resource granularity by pool.\n -- Burst buffer advanced reservation units treated as bytes (per documentation)\n    rather than GB.\n -- Add an \"scontrol top <jobid>\" command to re-order the priorities of a user's\n    pending jobs. May be disabled with the \"disable_user_top\" option in the\n    SchedulerParameters configuration parameter.\n -- Modify sview to display negative job nice values.\n -- Increase job's nice value field from 16 to 32 bits.\n -- Remove deprecated job_submit/cnode plugin.\n -- Enhance slurm.conf option EnforcePartLimit to include options like \"ANY\" and\n    \"ALL\".  \"Any\" is equivalent to \"Yes\" and \"All\" will check all partitions\n    a job is submitted to and if any partition limit is violated the job will\n    be rejected even if it could possibly run on another partition.\n -- Add \"features_act\" field (currently active features) to the node\n    information. Output of scontrol, sinfo, and sview changed accordingly.\n    The field previously displayed as \"Features\" is now \"AvailableFeatures\"\n    while the new field is displayed as \"ActiveFeatures\".\n -- Remove Sun Constellation, IBM Federation Switches (replaced by NRT switch\n    plugin) and long-defunct Quadrics Elan support.\n -- Add -M<clusters> option to sreport.\n -- Rework group caching to work better in environments with\n    enumeration disabled. Removed CacheGroups config directive, group\n    membership lists are now always cached, controlled by\n    GroupUpdateTime parameter. GroupUpdateForce parameter default\n    value changed to 1.\n -- Add reservation flag of \"purge_comp\" which will purge an advanced\n    reservation once it has no more active (pending, suspended or running) jobs.\n -- Add new configuration parameter \"KNLPlugins\" and plugin infrastructure.\n -- Add optional job \"features\" to node reboot RPC.\n -- Add slurmd \"-b\" option to report node rebooted at daemon start time. Used\n    for testing purposes.\n -- contribs/cray: Add framework for powering nodes up and down.\n -- For job constraint, convert comma separator to \"&\".\n -- Add Max*PerAccount options for QOS.\n -- Protect slurm_mutex_* calls with abort() on failure.\n\n* Changes in Slurm 15.08.14\n===========================\n\n* Changes in Slurm 15.08.13\n===========================\n -- Fix issue where slurmd could core when running the ipmi energy plugin.\n -- Print correct return code on failure to update node features through sview.\n -- Documentation - cleanup typos.\n -- Add logic so that slurmstepd can be launched under valgrind.\n -- Increase buffer size to read /proc/*/stat files.\n -- MYSQL - Handle ER_HOST_IS_BLOCKED better by failing when it occurs instead\n    of continuously printing the message over and over as the problem will\n    most likely not resolve itself.\n -- Add --disable-bluegene to configure.  This will make it so Slurm\n    can work on a BGAS node.\n -- Prevent job stuck in configuring state if slurmctld daemon restarted while\n    PrologSlurmctld is running.\n -- Handle association correctly if using FAIR_TREE as well as shares=Parent\n -- Fix race condition when setting priority of a job and the association\n    doesn't have a parent.\n -- MYSQL - Fix issue with adding a reservation if the name has single quotes in\n    it.\n -- Correctly print ranges when using step values in job arrays.\n -- Fix for invalid array pointer when creating advanced reservation when job\n    allocations span heterogeneous nodes (differing core or socket counts).\n -- Fix for sstat to print correct info when requesting jobid.batch as part of\n    a comma-separated list.\n -- Cray - Fix issue restoring jobs when blade count increases due to hardware\n    reconfiguration.\n -- Ignore warnings about depricated functions. This is primarily there for\n    new glibc 2.24+ that depricates readdir_r.\n -- Fix security issue caused by insecure file path handling triggered by the\n    failure of a Prolog script. To exploit this a user needs to anticipate or\n    cause the Prolog to fail for their job. CVE-2016-10030.\n\n* Changes in Slurm 15.08.12\n===========================\n -- Do not attempt to power down a node which has never responded if the\n    slurmctld daemon restarts without state.\n -- Fix for possible slurmstepd segfault on invalid user ID.\n -- MySQL - Fix for possible race condition when archiving multiple clusters\n    at the same time.\n -- Fix compile for when you don't have hwloc.\n -- Fix issue where daemons would only listen on specific address given in\n    slurm.conf instead of all.  If looking for specific addresses use\n    TopologyParam options No*InAddrAny.\n -- Cray - Better robustness when dealing with the aeld interface.\n -- job_submit.lua - add array_inx value for job arrays.\n -- Perlapi - Remove unneeded/undefined mutex.\n -- Fix issue when TopologyParam=NoInAddrAny is set the responses wouldn't\n    make it to the slurmctld when using message aggregation.\n -- MySQL - Fix potential memory leak when rolling up data.\n -- Fix issue with clustername file when running on NFS with root_squash.\n -- Fix race condition with respects to cleaning up the profiling threads\n    when in use.\n -- Fix issues when building on NetBSD.\n -- Fix jobcomp/elasticsearch build when libcurl is installed in a\n    non-standard location.\n -- Fix MemSpecLimit to explicitly require TaskPlugin=task/cgroup and\n    ConstrainRAMSpace set in cgroup.conf.\n -- MYSQL - Fix order of operations issue where if the database is locked up\n    and the slurmctld doesn't wait long enough for the response it would give\n    up leaving the connection open and create a situation where the next message\n    sent could receive the response of the first one.\n -- Fix CFULL_BLOCK distribution type.\n -- Prevent sbatch from trying to enable debug messages when using job arrays.\n -- Prevent sbcast from enabling \"--preserve\" when specifying a jobid.\n -- Prevent wrong error message from spank plugin stack on GLOB_NOSPACE error.\n -- Fix proctrack/lua plugin to prevent possible deadlock.\n -- Prevent infinite loop in slurmstepd if execve fails.\n -- Prevent multiple responses to REQUEST_UPDATE_JOB_STEP message.\n -- Prevent possible deadlock in acct_gather_filesystem/lustre on error.\n -- Make it so --mail-type=NONE didn't throw an invalid error.\n -- If no default account is given for a user when creating (only a list of\n    accounts) no default account is printed, previously NULL was printed.\n -- Fix for tracking a node's allocated CPUs with gang scheduling.\n -- Fix Hidden error during _rpc_forward_data call.\n -- Fix bug resulting from wrong order-of-operations in _connect_srun_cr(),\n    and two others that cause incorrect debug messages.\n -- Fix backwards compatibility with sreport going to <= 14.11 coming from\n    >= 15.08 for some reports.\n\n* Changes in Slurm 15.08.11\n===========================\n -- Fix for job \"--contiguous\" option that could cause job allocation/launch\n    failure or slurmctld crash.\n -- Fix to setup logs for single-character program names correctly.\n -- Backfill scheduling performance enhancement with large number of running\n    jobs.\n -- Reset job's prolog_running counter on slurmctld restart or reconfigure.\n -- burst_buffer/cray - Update job's prolog_running counter if pre_run fails.\n -- MYSQL - Make the error message more specific when removing a reservation\n    and it doesn't meet basic requirements.\n -- burst_buffer/cray - Fix for script creating or deleting persistent buffer\n    would fail \"paths\" operation and hold the job.\n -- power/cray - Prevent possible divide by zero.\n -- power/cray - Fix bug introduced in 15.08.10 preventin operation in many\n    cases.\n -- Prevent deadlock for flow of data to the slurmdbd when sending reservation\n    that wasn't set up correctly.\n -- burst_buffer/cray - Don't call Datawarp \"paths\" function if script includes\n    only create or destroy of persistent burst buffer. Some versions of Datawarp\n    software return an error for such scripts, causing the job to be held.\n -- Fix potential issue when adding and removing TRES which could result\n    in the slurmdbd segfaulting.\n -- Add cast to memory limit calculation to prevent integer overflow for\n    very large memory values.\n -- Bluegene - Fix issue with reservations resizing under the covers on a\n    restart of the slurmctld.\n -- Avoid error message of \"Requested cpu_bind option requires entire node to\n    be allocated; disabling affinity\" being generated in some cases where\n    task/affinity and task/cgroup plugins used together.\n -- Fix version issue when packing GRES information between 2 different versions\n    of Slurm.\n -- Fix for srun hanging with OpenMPI and PMIx\n -- Better initialization of node_ptr when dealing with protocol_version.\n -- Fix incorrect type when initializing header of a message.\n -- MYSQL - Fix incorrect usage of limit and union.\n -- MYSQL - Remove 'ignore' from alter ignore when updating a table.\n -- Documentation - update prolog_epilog page to reflect current behavior\n    if the Prolog fails.\n -- Documentation - clarify behavior of 'srun --export=NONE' in man page.\n -- Fix potential gres underflow on restart of slurmctld.\n -- Fix sacctmgr to remove a user who has no associations.\n\n* Changes in Slurm 15.08.10\n===========================\n -- Fix issue where if a slurmdbd rollup lasted longer than 1 hour the\n    rollup would effectively never run again.\n -- Make error message in the pmi2 code to debug as the issue can be expected\n    and retries are done making the error message a little misleading.\n -- Power/cray: Don't specify NID list to Cray APIs. If any of those nodes are\n    not in a ready state, the API returned an error for ALL nodes rather than\n    valid data for nodes in ready state.\n -- Fix potential divide by zero when tree_width=1.\n -- checkpoint/blcr plugin: Fix memory leak.\n -- If using PrologFlags=contain: Don't launch the extern step if a job is\n    cancelled while launching.\n -- Remove duplicates from AccountingStorageTRES\n -- Fix backfill scheduler race condition that could cause invalid pointer in\n    select/cons_res plugin. Bug introduced in 15.08.9.\n -- Avoid double calculation on partition QOS if the job is using the same QOS.\n -- Do not change a job's time limit when updating unrelated field in a job.\n -- Fix situation on a heterogeneous memory cluster where the order of\n    constraints mattered in a job.\n\n* Changes in Slurm 15.08.9\n==========================\n -- BurstBuffer/cray - Defer job cancellation or time limit while \"pre-run\"\n    operation in progress to avoid inconsistent state due to multiple calls\n    to job termination functions.\n -- Fix issue with resizing jobs and limits not be kept track of correctly.\n -- BGQ - Remove redeclaration of job_read_lock.\n -- BGQ - Tighter locks around structures when nodes/cables change state.\n -- Make it possible to change CPUsPerTask with scontrol.\n -- Make it so scontrol update part qos= will take away a partition QOS from\n    a partition.\n -- Fix issue where SocketsPerBoard didn't translate to Sockets when CPUS=\n    was also given.\n -- Add note to slurm.conf man page about setting \"--cpu_bind=no\" as part\n    of SallocDefaultCommand if a TaskPlugin is in use.\n -- Set correct reason when a QOS' MaxTresMins is violated.\n -- Insure that a job is completely launched before trying to suspend it.\n -- Remove historical presentations and design notes. Only distribute\n    maintained doc/html and doc/man directories.\n -- Remove duplicate xmalloc() in task/cgroup plugin.\n -- Backfill scheduler to validate correct job partition for job submitted to\n    multiple partitions.\n -- Force close on exec on first 256 file descriptors when launching a\n    slurmstepd to close potential open ones.\n -- Step GRES value changed from type \"int\" to \"int64_t\" to support larger\n    values.\n -- Fix getting reservations to database when database is down.\n -- Fix issue with sbcast not doing a correct fanout.\n -- Fix issue where steps weren't always getting the gres/tres involved.\n -- Fixed double read lock on getting job's gres/tres.\n -- Fix display for RoutePlugin parameter to display the correct value.\n -- Fix route/topology plugin to prevent segfault in sbcast when in use.\n -- Fix Cray slurmconfgen_smw.py script to use nid as nid, not nic.\n -- Fix Cray NHC spawning on job requeue. Previous logic would leave nodes\n    allocated to a requeued job as non-usable on job termination.\n -- burst_buffer/cray plugin: Prevent a requeued job from being restarted while\n    file stage-out is still in progress. Previous logic could restart the job\n    and not perform a new stage-in.\n -- Fix job array formatting to allow return [0-100:2] display for arrays with\n    step functions rather than [0,2,4,6,8,...] .\n -- FreeBSD - replace Linux-specific set_oom_adj to avoid errors in slurmd log.\n -- Add option for TopologyParam=NoInAddrAnyCtld to make the slurmctld listen\n    on only one port like TopologyParam=NoInAddrAny does for everything else.\n -- Fix burst buffer plugin to prevent corruption of the CPU TRES data when bb\n    is not set as an AccountingStorageTRES type.\n -- Surpress error messages in acct_gather_energy/ipmi plugin after repeated\n    failures.\n -- Change burst buffer use completion email message from\n    \"SLURM Job_id=1360353 Name=tmp Staged Out, StageOut time 00:01:47\" to\n    \"SLURM Job_id=1360353 Name=tmp StageOut/Teardown time 00:01:47\"\n -- Generate burst buffer use completion email immediately afer teardown\n    completes rather than at job purge time (likely minutes later).\n -- Fix issue when adding a new TRES to AccountingStorageTRES for the first\n    time.\n -- Update gang scheduling tables when job manually suspended or resumed. Prior\n    logic could mess up job suspend/resume sequencing.\n -- Update gang scheduling data structures when job changes in size.\n -- Associations - prevent hash table corruption if uid initially unset for\n    a user, which can cause slurmctld to crash if that user is deleted.\n -- Avoid possibly aborting srun on SIGSTOP while creating the job step due to\n    threading bug.\n -- Fix deadlock issue with burst_buffer/cray when a newly created burst\n    buffer is found.\n -- burst_buffer/cray: Set environment variables just before starting job rather\n    than at job submission time to reflect persistent buffers created or\n    modified while the job is pending.\n -- Fix check of per-user qos limits on the initial run by a user.\n -- Fix gang scheduling resource selection bug which could prevent multiple jobs\n    from being allocated the same resources. Bug was introduced in 15.08.6.\n -- Don't print the Rgt value of an association from the cache as it isn't\n    kept up to date.\n -- burst_buffer/cray - If the pre-run operation fails then don't issue\n    duplicate job cancel/requeue unless the job is still in run state. Prevents\n    jobs hung in COMPLETING state.\n -- task/cgroup - Fix bug in task binding to CPUs.\n\n* Changes in Slurm 15.08.8\n==========================\n -- Backfill scheduling properly synchronized with Cray Node Health Check.\n    Prior logic could result in highest priority job getting improperly\n    postponed.\n -- Make it so daemons also support TopologyParam=NoInAddrAny.\n -- If scancel is operating on large number of jobs and RPC responses from\n    slurmctld daemon are slow then introduce a delay in sending the cancel job\n    requests from scancel in order to reduce load on slurmctld.\n -- Remove redundant logic when updating a job's task count.\n -- MySQL - Fix querying jobs with reservations when the id's have rolled.\n -- Perl - Fix use of uninitialized variable in slurm_job_step_get_pids.\n -- Launch batch job requsting --reboot after the boot completes.\n -- Move debug messages like \"not the right user\" from association manager\n    to debug3 when trying to find the correct association.\n -- Fix incorrect logic when querying assoc_mgr information.\n -- Move debug messages to debug3 notifying a gres_bit_alloc was NULL for\n    gres types without a file.\n -- Sanity Check Patch to setup variables for RAPL if in a race for it.\n -- GRES - Fix minor typecast issues.\n -- burst_buffer/cray - Increase size of intermediate variable used to store\n    buffer byte size read from DW instance from 32 to 64-bits to avoid overflow\n    and reporting invalid buffer sizes.\n -- Allow an existing reservation with running jobs to be modified without\n    Flags=IGNORE_JOBS.\n -- srun - don't attempt to execve() a directory with a name matching the\n    requested command\n -- Do not automatically relocate an advanced reservation for individual cores\n    that spans multiple nodes when nodes in that reservation go down (e.g.\n    a 1 core reservation on node \"tux1\" will be moved if node \"tux1\" goes\n    down, but a reservation containing 2 cores on node \"tux1\" and 3 cores on\n    \"tux2\" will not be moved node \"tux1\" goes down). Advanced reservations for\n    whole nodes will be moved by default for down nodes.\n -- Avoid possible double free of memory (and likely abort) for slurmctld in\n    background mode.\n -- contribs/cray/csm/slurmconfgen_smw.py - avoid including repurposed compute\n    nodes in configs.\n -- Support AuthInfo in slurmdbd.conf that is different from the value in\n    slurm.conf.\n -- Fix build on FreeBSD 10.\n -- Fix hdf5 build on ppc64 by using correct fprintf formatting for types.\n -- Fix cosmetic printing of NO_VALs in scontrol show assoc_mgr.\n -- Fix perl api for newer perl versions.\n -- Fix for jobs requesting cpus-per-task (eg. -c3) that exceed the number of\n    cpus on a core.\n -- Remove unneeded perl files from the .spec file.\n -- Flesh out filters for scontrol show assoc_mgr.\n -- Add function to remove assoc_mgr_info_request_t members without freeing\n    structure.\n -- Fix build on some non-glibc systems by updating includes.\n -- Add new PowerParameters options of get_timeout and set_timeout. The default\n    set_timeout was increased from 5 seconds to 30 seconds. Also re-read current\n    power caps periodically or after any failed \"set\" operation.\n -- Fix slurmdbd segfault when listing users with blank user condition.\n -- Save the ClusterName to a file in SaveStateLocation, and use that to\n    verify the state directory belongs to the given cluster at startup to avoid\n    corruption from multiple clusters attempting to share a state directory.\n -- MYSQL - Fix issue when rerolling monthly data to work off correct time\n    period.  This would only hit you if you rerolled a 15.08 prior to this\n    commit.\n -- If FastSchedule=0 is used make sure TRES are set up correctly in accounting.\n -- Fix sreport's truncation of columns with large TRES and not using\n    a parsing option.\n -- Make sure count of boards are restored when slurmctld has option -R.\n -- When determine if a job can fit into a TRES time limit after resources\n    have been selected set the time limit appropriately if the job didn't\n    request one.\n -- Fix inadequate locks when updating a partition's TRES.\n -- Add new assoc_limit_continue flag to SchedulerParameters.\n -- Avoid race in acct_gather_energy_cray if energy requested before available.\n -- MYSQL - Avoid having multiple default accounts when a user is added to\n    a new account and making it a default all at once.\n\n* Changes in Slurm 15.08.7\n==========================\n -- sched/backfill: If a job can not be started within the configured\n    backfill_window, set it's start time to 0 (unknown) rather than the end\n    of the backfill_window.\n -- Remove the 1024-character limit on lines in batch scripts.\n -- burst_buffer/cray: Round up swap size by configured granularity.\n -- select/cray: Log repeated aeld reconnects.\n -- task/affinity: Disable core-level task binding if more CPUs required than\n    available cores.\n -- Preemption/gang scheduling: If a job is suspended at slurmctld restart or\n    reconfiguration time, then leave it suspended rather than resume+suspend.\n -- Don't use lower weight nodes for job allocation when topology/tree used.\n -- BGQ - If a cable goes into error state remove the under lying block on\n    a dynamic system and mark the block in error on a static/overlap system.\n -- BGQ - Fix regression in 9cc4ae8add7f where blocks would be deleted on\n    static/overlap systems when some hardware issue happens when restarting\n    the slurmctld.\n -- Log if CLOUD node configured without a resume/suspend program or suspend\n    time.\n -- MYSQL - Better locking around g_qos_count which was previously unprotected.\n -- Correct size of buffer used for jobid2str to avoid truncation.\n -- Fix allocation/distribution of tasks across multiple nodes when\n    --hint=nomultithread is requested.\n -- If a reservation's nodes value is \"all\" then track the current nodes in the\n    system, even if those nodes change.\n -- Fix formatting if using \"tree\" option with sreport.\n -- Make it so sreport prints out a line for non-existent TRES instead of\n    error message.\n -- Set job's reason to \"Priority\" when higher priority job in that partition\n    (or reservation) can not start rather than leaving the reason set to\n    \"Resources\".\n -- Fix memory corruption when a new non-generic TRES is added to the\n    DBD for the first time.  The corruption is only noticed at shutdown.\n -- burst_buffer/cray - Improve tracking of allocated resources to handle race\n    condition when reading state while buffer allocation is in progress.\n -- If a job is submitted only with -c option and numcpus is updated before\n    the job starts update the cpus_per_task appropriately.\n -- Update salloc/sbatch/srun documentation to mention time granularity.\n -- Fixed memory leak when freeing assoc_mgr_info_msg_t.\n -- Prevent possible use of empty reservation core bitmap, causing abort.\n -- Remove unneeded pack32's from qos_rec when qos_rec is NULL.\n -- Make sacctmgr print MaxJobsPerUser when adding/altering a QOS.\n -- Correct dependency formatting to print array task ids if set.\n -- Update sacctmgr help with current QOS options.\n -- Update slurmstepd to initialize authentication before task launch.\n -- burst_cray/cray: Eliminate need for dedicated nodes.\n -- If no MsgAggregationParams is set don't set the internal string to\n    anything.  The slurmd will process things correctly after the fact.\n -- Fix output from api when printing job step not found.\n -- Don't allow user specified reservation names to disrupt the normal\n    reservation sequeuece numbering scheme.\n -- Fix scontrol to be able to accept TRES as an option when creating\n    a reservation.\n -- contrib/torque/qstat.pl - return exit code of zero even with no records\n    printed for 'qstat -u'.\n -- When a reservation is created or updated, compress user provided node names\n    using hostlist functions (e.g. translate user input of \"Nodes=tux1,tux2\"\n    into \"Nodes=tux[1-2]\").\n -- Change output routines for scontrol show partition/reservation to handle\n    unexpectedly large strings.\n -- Add more partition fields to \"scontrol write config\" output file.\n -- Backfill scheduling fix: If a job can't be started due to a \"group\" resource\n    limit, rather than reserve resources for it when the next job ends, don't\n    reserve any resources for it.\n -- Avoid slurmstepd abort if malloc fails during accounting gather operation.\n -- Fix nodes from being overallocated when allocation straddles multiple nodes.\n -- Fix memory leak in slurmctld job array logic.\n -- Prevent decrementing of TRESRunMins when AccountingStorageEnforce=limits is\n    not set.\n -- Fix backfill scheduling bug which could postpone the scheduling of jobs due\n    to avoidance of nodes in COMPLETING state.\n -- Properly account for memory, CPUs and GRES when slurmctld is reconfigured\n    while there is a suspended job. Previous logic would add the CPUs, but not\n    memory or GPUs. This would result in underflow/overflow errors in select\n    cons_res plugin.\n -- Strip flags from a job state in qstat wrapper before evaluating.\n -- Add missing job states from the qstat wrapper.\n -- Cleanup output routines to reduce number of fixed-sized buffer function\n    calls and allow for unexpectedly large strings.\n\n* Changes in Slurm 15.08.6\n==========================\n -- In slurmctld log file, log duplicate job ID found by slurmd. Previously was\n    being logged as prolog/epilog failure.\n -- If a job is requeued while in the process of being launch, remove it's\n    job ID from slurmd's record of active jobs in order to avoid generating a\n    duplicate job ID error when launched for the second time (which would\n    drain the node).\n -- Cleanup messages when handling job script and environment variables in\n    older directory structure formats.\n -- Prevent triggering gang scheduling within a partition if configured with\n    PreemptType=partition_prio and PreemptMode=suspend,gang.\n -- Decrease parallelism in job cancel request to prevent denial of service\n    when cancelling huge numbers of jobs.\n -- If all ephemeral ports are in use, try using other port numbers.\n -- Revert way lib lua is handled when doing a dlopen, fixing a regression in\n    15.08.5.\n -- Set the debug level of the rmdir message in xcgroup_delete() to debug2.\n -- Fix the qstat wrapper when user is removed from the system but still\n    has running jobs.\n -- Log the request to terminate a job at info level if DebugFlags includes\n    the Steps keyword.\n -- Fix potential memory corruption in _slurm_rpc_epilog_complete as well as\n    _slurm_rpc_complete_job_allocation.\n -- Fix cosmetic display of AccountingStorageEnforce option \"nosteps\" when\n    in use.\n -- If a job can never be started due to unsatisfied job dependencies, report\n    the full original job dependency specification rather than the dependencies\n    remaining to be satisfied (typically NULL).\n -- Refactor logic to synchronize active batch jobs and their script/environment\n    files, reducing overhead dramatically for large numbers of active jobs.\n -- Avoid hard-link/copy of script/environment files for job arrays. Use the\n    master job record file for all tasks of the job array.\n    NOTE: Job arrays submitted to Slurm version 15.08.6 or later will fail if\n    the slurmctld daemon is downgraded to an earlier version of Slurm.\n -- Move slurmctld mail handler to separate thread for improved performance.\n -- Fix containment of adopted processes from pam_slurm_adopt.\n -- If a pending job array has multiple reasons for being in a pending state,\n    then print all reasons in a comma separated list.\n\n* Changes in Slurm 15.08.5\n==========================\n -- Prevent \"scontrol update job\" from updating jobs that have already finished.\n -- Show requested TRES in \"squeue -O tres\" when job is pending.\n -- Backfill scheduler: Test association and QOS node limits before reserving\n    resources for pending job.\n -- burst_buffer/cray: If teardown operations fails, sleep and retry.\n -- Clean up the external pids when using the PrologFlags=Contain feature\n    and the job finishes.\n -- burst_buffer/cray: Support file staging when job lacks job-specific buffer\n    (i.e. only persistent burst buffers).\n -- Added srun option of --bcast to copy executable file to compute nodes.\n -- Fix for advanced reservation of burst buffer space.\n -- BurstBuffer/cray: Add logic to terminate dw_wlm_cli child processes at\n    shutdown.\n -- If job can't be launch or requeued, then terminate it.\n -- BurstBuffer/cray: Enable clearing of burst buffer string on completed job\n    as a means of recovering from a failure mode.\n -- Fix wrong memory free when parsing SrunPortRange=0-0 configuration.\n -- BurstBuffer/cray: Fix job record purging if cancelled from pending state.\n -- BGQ - Handle database throw correctly when syncing users on blocks.\n -- MySQL - Make sure we don't have a NULL string returned when not\n    requesting any specific association.\n -- sched/backfill: If max_rpc_cnt is configured and the backlog of RPCs has\n    not cleared after yielding locks, then continue to sleep.\n -- Preserve the job dependency description displayed in 'scontrol show job'\n    even if the dependee jobs was terminated and cleaned causing the\n    dependent to never run because of DependencyNeverSatisfied.\n -- Correct job task count calculation if only node count and ntasks-per-node\n    options supplied.\n -- Make sure the association manager converts any string to be lower case\n    as all the associations from the database will be lower case.\n -- Sanity check for xcgroup_delete() to verify incoming parameter is valid.\n -- Fix formatting for sacct with variables that switched from uint32_t to\n    uint64_t.\n -- Fix a typo in sacct man page.\n -- Set up extern step to track any children of an ssh if it leaves anything\n    else behind.\n -- Prevent slurmdbd divide by zero if no associations defined at rollup time.\n -- Multifactor - Add sanity check to make sure pending jobs are handled\n    correctly when PriorityFlags=CALCULATE_RUNNING is set.\n -- Add slurmdb_find_tres_count_in_string() to slurm db perl api.\n -- Make lua dlopen() conditional on version found at build.\n -- sched/backfill - Delay backfill scheduler for completing jobs only if\n    CompleteWait configuration parameter is set (make code match documentation).\n -- Release a job's allocated licenses only after epilog runs on all nodes\n    rather than at start of termination process.\n -- Cray job NHC delayed until after burst buffer released and epilog completes\n    on all allocated nodes.\n -- Fix abort of srun if using PrologFlags=NoHold\n -- Let devices step_extern cgroup inherit attributes of job cgroup.\n -- Add new hook to Task plugin to be able to put adopted processes in the\n    step_extern cgroups.\n -- Fix AllowUsers documentation in burst_buffer.conf man page. Usernames are\n    comma separated, not colon delimited.\n -- Fix issue with time limit not being set correctly from a QOS when a job\n    requests no time limit.\n -- Various CLANG fixes.\n -- In both sched/basic and backfill: If a job can not be started due to some\n    account/qos limit, then don't start other jobs which could delay jobs. The\n    old logic would skip the job and start other jobs, which could delay the\n    higher priority job.\n -- select/cray: Prevent NHC from running more than once per job or step.\n -- Fix fields not properly printed when adding an account through sacctmgr.\n -- Update LBNL Node Health Check (NHC) link on FAQ.\n -- Fix multifactor plugin to prevent slurmctld from getting segmentation fault\n    should the tres_alloc_cnt be NULL.\n -- sbatch/salloc - Move nodelist logic before the time min_nodes is used\n    so we can set it correctly before tasks are set.\n\n* Changes in Slurm 15.08.4\n==========================\n -- Fix typo for the \"devices\" cgroup subsystem in pam_slurm_adopt.c\n -- Fix TRES_MAX flag to work correctly.\n -- Improve the systemd startup files.\n -- Added burst_buffer.conf flag parameter of \"TeardownFailure\" which will\n    teardown and remove a burst buffer after failed stage-in or stage-out.\n    By default, the buffer will be preserved for analysis and manual teardown.\n -- Prevent a core dump in srun if the signal handler runs during the job\n    allocation causing the step context to be NULL.\n -- Don't fail job if multiple prolog operations in progress at slurmctld\n    restart time.\n -- Burst_buffer/cray: Fix to purge terminated jobs with burst buffer errors.\n -- Burst_buffer/cray: Don't stall scheduling of other jobs while a stage-in\n    is in progress.\n -- Make it possible to query 'extern' step with sstat.\n -- Make 'extern' step show up in the database.\n -- MYSQL - Quote assoc table name in mysql query.\n -- Make SLURM_ARRAY_TASK_MIN, SLURM_ARRAY_TASK_MAX, and SLURM_ARRAY_TASK_STEP\n    environment variables available to PrologSlurmctld and EpilogSlurmctld.\n -- Fix slurmctld bug in which a pending job array could be canceled\n    by a user different from the owner or the administrator.\n -- Support taking node out of FUTURE state with \"scontrol reconfig\" command.\n -- Sched/backfill: Fix to properly enforce SchedulerParameters of\n    bf_max_job_array_resv.\n -- Enable operator to reset sdiag data.\n -- jobcomp/elasticsearch plugin: Add array_job_id and array_task_id fields.\n -- Remove duplicate #define IS_NODE_POWER_UP.\n -- Added SchedulerParameters option of max_script_size.\n -- Add REQUEST_ADD_EXTERN_PID option to add pid to the slurmstepd's extern\n    step.\n -- Add unique identifiers to anchor tags in HTML generated from the man pages.\n -- Add with_freeipmi option to spec file.\n -- Minor elasticsearch code improvements\n\n* Changes in Slurm 15.08.3\n==========================\n -- Correct Slurm's RPM build if Munge is not installed.\n -- Job array termination status email ExitCode based upon highest exit code\n    from any task in the job array rather than the last task. Also change the\n    state from \"Ended\" or \"Failed\" to \"Mixed\" where appropriate.\n -- Squeue recombines pending job array records only if their name and partition\n    are identical.\n -- Fix some minor leaks in the job info and step info API.\n -- Export missing QOS id when filling in association with the association\n    manager.\n -- Fix invalid reference if a lua job_submit plugin references a default qos\n    when a user doesn't exist in the database.\n -- Use association enforcement in the lua plugin.\n -- Fix a few spots missing defines of accounting_enforce or acct_db_conn\n    in the plugins.\n -- Show requested TRES in scontrol show jobs when job is pending.\n -- Improve sched/backfill support for job features, especially XOR construct.\n -- Correct scheduling logic for job features option with XOR construct that\n    could delay a job's initiation.\n -- Remove unneeded frees when creating a tres string.\n -- Send a tres_alloc_str for the batch step\n -- Fix incorrect check for slurmdb_find_tres_count_in_string in various places,\n    it needed to check for INFINITE64 instead of zero.\n -- Don't allow scontrol to create partitions with the name \"DEFAULT\".\n -- burst_buffer/cray: Change error from \"invalid request\" to \"permssion denied\"\n    if a non-authorized user tries to create/destroy a persistent buffer.\n -- PrologFlags work: Setting a flag of \"Contain\" implicitly sets the \"Alloc\"\n    flag. Fix code path which could prevent execution of the Prolog when the\n    \"Alloc\" or \"Contain\" flag were set.\n -- Fix for acct_gather_energy/cray|ibmaem to work with missed enum.\n -- MYSQL - When inserting a job and begin_time is 0 do not set it to\n    submit_time.  0 means the job isn't eligible yet so we need to treat it so.\n -- MYSQL - Don't display ineligible jobs when querying for a window of time.\n -- Fix creation of advanced reservation of cores on nodes which are DOWN.\n -- Return permission denied if regular user tries to release job held by an\n    administrator.\n -- MYSQL - Fix rollups for multiple jobs running by the same association\n    in an hour counting multiple times.\n -- Burstbuffer/Cray plugin - Fix for persistent burst buffer use.\n    Don't call paths if no #DW options.\n -- Modifications to pam_slurm_adopt to work correctly for the \"extern\" step.\n -- Alphabetize debugflags when printing them out.\n -- Fix systemd's slurmd service from killing slurmstepds on shutdown.\n -- Fixed counter of not indexed jobs, error_cnt post-increment changed to\n    pre-increment.\n\n* Changes in Slurm 15.08.2\n==========================\n -- Fix for tracking node state when jobs that have been allocated exclusive\n    access to nodes (i.e. entire nodes) and later relinquish some nodes. Nodes\n    would previously appear partly allocated and prevent use by other jobs.\n -- Correct some cgroup paths (\"step_batch\" vs. \"step_4294967294\", \"step_exter\"\n    vs. \"step_extern\", and \"step_extern\" vs. \"step_4294967295\").\n -- Fix advanced reservation core selection logic with network topology.\n -- MYSQL - Remove restriction to have to be at least an operator to query TRES\n    values.\n -- For pending jobs have sacct print 0 for nnodes instead of the bogus 2.\n -- Fix for tracking node state when jobs that have been allocated exclusive\n    access to nodes (i.e. entire nodes) and later relinquish some nodes. Nodes\n    would previously appear partly allocated and prevent use by other jobs.\n -- Fix updating job in db after extending job's timelimit past partition's\n    timelimit.\n -- Fix srun -I<timeout> from flooding the controller with step create requests.\n -- Requeue/hold batch job launch request if job already running (possible if\n    node went to DOWN state, but jobs remained active).\n -- If a job's CPUs/task ratio is increased due to configured MaxMemPerCPU,\n    then increase it's allocated CPU count in order to enforce CPU limits.\n -- Don't mark powered down node as not responding. This could be triggered by\n    race condition of the node suspend and ping logic, preventing use of the\n    node.\n -- Don't requeue RPC going out from slurmctld to DOWN nodes (can generate\n    repeating communication errors).\n -- Propagate sbatch \"--dist=plane=#\" option to srun.\n -- Add acct_gather_energy/ibmaem plugin for systems with IBM Systems Director\n    Active Energy Manager.\n -- Fix spec file to look for mariadb or mysql devel packages for build\n    requirements.\n -- MySQL - Improve the code with asking for jobs in a suspended state.\n -- Fix slurcmtld allowing root to see job steps using squeues -s.\n -- Do not send burst buffer stage out email unless the job uses burst buffers.\n -- Fix sacct to not return all jobs if the -j option is given with a trailing\n    ','.\n -- Permit job_submit plugin to set a job's priority.\n -- Fix occasional srun segfault.\n -- Fix issue with sacct, printing 0_0 for array's that had finished in the\n    database but the start record hadn't made it yet.\n -- sacctmgr - Don't allow default account associations to be removed\n    from a user.\n -- Fix sacct -j, (nothing but a comma) to not return all jobs.\n -- Fixed slurmctld not sending cold-start messages correctly to the database\n    when a cold-start (-c) happens to the slurmctld.\n -- Fix case where if the backup slurmdbd has existing connections when it gives\n    up control that the it would be killed.\n -- Fix task/cgroup affinity to work correctly with multi-socket\n    single-threaded cores.  A regression caused only 1 socket to be used on\n    this kind of node instead of all that were available.\n -- MYSQL - Fix minor issue after an index was added to the database it would\n    previously take 2 restarts of the slurmdbd to make it stick correctly.\n -- Add hv_to_qos_cond() and qos_rec_to_hv() functions to the Perl interface.\n -- Add new burst_buffer.conf parameters: ValidateTimeout and OtherTimeout.\n    See man page for details.\n -- Fix burst_buffer/cray support for interactive allocations >4GB.\n -- Correct backfill scheduling logic for job with INFINITE time limit.\n -- Fix issue on a scontrol reconfig all available GRES/TRES would be zeroed\n    out.\n -- Set SLURM_HINT environment variable when --hint is used with sbatch or\n    salloc.\n -- Add scancel -f/--full option to signal all steps including batch script and\n    all of its child processes.\n -- Fix salloc -I to accept an argument.\n -- Avoid reporting more allocated CPUs than exist on a node. This can be\n    triggered by resuming a previosly suspended job, resulting in\n    oversubscription of CPUs.\n -- Fix the pty window manager in slurmstepd not to retry IO operation with\n    srun if it read EOF from the connection with it.\n -- sbatch --ntasks option to take precedence over --ntasks-per-node plus node\n    count, as documented. Set SLURM_NTASKS/SLURM_NPROCS environment variables\n    accordingly.\n -- MYSQL - Make sure suspended time is only subtracted from the CPU TRES\n    as it is the only TRES that can be given to another job while suspended.\n -- Clarify how TRESBillingWeights operates on memory and burst buffers.\n\n* Changes in Slurm 15.08.1\n==========================\n -- Fix test21.30 and 21.34 to check grpwall better.\n -- Add time to the partition QOS the job is running on instead of just the\n    job QOS.\n -- Print usage for GrpJobs, GrpSubmitJobs and GrpWall even if there is no\n    limit.\n -- If AccountingEnforce=safe is set make sure a job can finish before going\n    over the limit with grpwall on a QOS or association.\n -- burst_buffer/cray - Major updates based upon recent Cray changes.\n -- Improve clean up logic of pmi2 plugin.\n -- Improve job state reason string when required nodes not available.\n -- Fix missing else when packing an update partition message\n -- Fix srun from inheriting the SLURM_CPU_BIND and SLURM_MEM_BIND environment\n    variables when running in an existing srun (e.g. an srun within an salloc).\n -- Fix missing else when packing an update partition message.\n -- Use more flexible mechnanism to find json installation.\n -- Make sure safe_limits was initialized before processing limits in the\n    slurmctld.\n -- Fix for burst_buffer/cray to parse type option correctly.\n -- Fix memory error and version number in the nonstop plugin and reservation\n    code.\n -- When requesting GRES in a step check for correct variable for the count.\n -- Fix issue with GRES in steps so that if you have multiple exclusive steps\n    and you use all the GRES up instead of reporting the configuration isn't\n    available you hold the requesting step until the GRES is available.\n -- MYSQL - Change debug to print out with DebugFlags=DB_Step instead of debug4\n -- Simplify code when user is selecting a job/step/array id and removed\n    anomaly when only asking for 1 (task_id was never set to INFINITE).\n -- MYSQL - If user is requesting various task_ids only return requested steps.\n -- Fix issue when tres cnt for energy is 0 for total reported.\n -- Resolved scalability issues of power adaptive scheduling with layouts.\n -- Burst_buffer/cray bug - Fix teardown race condition that can result in\n    infinite loop.\n -- Add support for --mail-type=NONE option.\n -- Job \"--reboot\" option automatically, set's exclusive node mode.\n -- Fix memory leak when using PrologFlags=Alloc.\n -- Fix truncation of job reason in squeue.\n -- If a node is in DOWN or DRAIN state, leave it unavailable for allocation\n    when powered down.\n -- Update the slurm.conf man page documenting better nohold_on_prolog_fail\n    variable.\n -- Don't trucate task ID information in \"squeue --array/-r\" or \"sview\".\n -- Fix a bug which caused scontrol to core dump when releasing or\n    holding a job by name.\n -- Fix unit conversion bug in slurmd which caused wrong memory calculation\n    for cgroups.\n -- Fix issue with GRES in steps so that if you have multiple exclusive steps\n    and you use all the GRES up instead of reporting the configuration isn't\n    available you hold the requesting step until the GRES is available.\n -- Fix slurmdbd backup to use DbdAddr when contacting the primary.\n -- Fix error in MPI documentation.\n -- Fix to handle arrays with respect to number of jobs submitted.  Previously\n    only 1 job was accounted (against MaxSubmitJob) for when an array was\n    submitted.\n -- Correct counting for job array limits, job count limit underflow possible\n    when master cancellation of master job record.\n -- Combine 2 _valid_uid_gid functions into a single function to avoid\n    diversion.\n -- Pending job array records will be combined into single line by default,\n    even if started and requeued or modified.\n -- Fix sacct --format=nnodes to print out correct information for pending\n    jobs.\n -- Make is so 'scontrol update job 1234 qos='' will set the qos back to\n    the default qos for the association.\n -- Add [Alloc|Req]Nodes to sacct to be more like cpus.\n -- Fix sacct documentation about [Alloc|Req]TRES\n -- Put node count in TRES string for steps.\n -- Fix issue with wrong protocol version when using the srun --no-allocate\n    option.\n -- Fix TRES counts on GRES on a clean start of the slurmctld.\n -- Add ability to change a job array's maximum running task count:\n    \"scontrol update jobid=# arraytaskthrottle=#\"\n\n* Changes in Slurm 15.08.0\n==========================\n -- Fix issue with frontend systems (outside ALPs or BlueGene) where srun\n    wouldn't get the correct protocol version to launch a step.\n -- Fix for message aggregation return rpcs where none of the messages are\n    intended for the head of the tree.\n -- Fix segfault in sreport when there was no response from the dbd.\n -- ALPS - Fix compile to not link against -ljob and -lexpat with every lib\n    or binary.\n -- Fix testing for CR_Memory when CR_Memory and CR_ONE_TASK_PER_CORE are used\n    with select/linear.\n -- When restarting or reconfiging the slurmctld, if job is completing handle\n    accounting correctly to avoid meaningless errors about overflow.\n -- Add AccountingStorageTRES to scontrol show config\n -- MySQL - Fix minor memory leak if a connection ever goes away whist using it.\n -- ALPS - Make it so srun --hint=nomultithread works correctly.\n -- Make MaxTRESPerUser work in sacctmgr.\n -- Fix handling of requeued jobs with steps that are still finishing.\n -- Cleaner copy for PriorityWeightTRES, it also fixes a core dump when trying\n    to free it otherwise.\n -- Add environment variables SLURM_ARRAY_TASK_MAX, SLURM_ARRAY_TASK_MIN,\n    SLURM_ARRAY_TASK_STEP for job arrays.\n -- Fix srun to use the NoInAddrAny TopologyParam option.\n -- Change QOS flag name from PartitionQOS to OverPartQOS to be a better\n    description.\n -- Fix rpmbuild issue on Centos7.\n\n* Changes in Slurm 15.08.0rc1\n==============================\n -- Added power_cpufreq layout.\n -- Make complete_batch_script RPC work with message aggregation.\n -- Do not count slurmctld threads waiting in a \"throttle\" lock against the\n    daemon's thread limit as they are not contending for resources.\n -- Modify slurmctld outgoing RPC logic to support more parallel tasks (up to\n    85 RPCs and 256 pthreads; the old logic supported up to 21 RPCs and 256\n    threads). This change can dramatically improve performance for RPCs\n    operating on small node counts.\n -- Increase total backfill scheduler run time in stats_info_response_msg data\n    structure from 32 to 64 bits in order to prevent overflow.\n -- Add NoInAddrAny option to TopologyParam in the slurm.conf which allows to\n    bind to the interface of return of gethostname instead of any address on\n    the node which avoid RSIP issues in Cray systems.  This is most likely\n    useful in other systems as well.\n -- Fix memory leak in Slurm::load_jobs perl api call.\n -- Added --noconvert option to sacct, sstat, squeue and sinfo which allows\n    values to be displayed in their original unit types (e.g. 2048M won't be\n    converted to 2G).\n -- Fix spelling of node_rescrs to node_resrcs in Perl API.\n -- Fix node state race condition, UNKNOWN->IDLE without configuration info.\n -- Cray: Disable LDAP references from slurmstepd on job launch due for\n    improved scalability.\n -- Remove srun \"read header error\" due to application termination race\n    condition.\n -- Optimize sacct queries with additional db indexes.\n -- Add SLURM_TOPO_LEN env variable for scontrol show topology.\n -- Add free_mem to node information.\n -- Fix abort of batch launch if prolog is running, wait for prolog instead.\n -- Fix case where job would get the wrong cpu count when using\n    --ntasks-per-core and --cpus-per-task together.\n -- Add TRESBillingWeights to partitions in slurm.conf which allows taking into\n    consideration any TRES Type when calculating the usage of a job.\n -- Add PriorityWeightTRES slurm.conf option to be able to configure priority\n    factors for TRES types.\n\n* Changes in Slurm 15.08.0pre6\n==============================\n -- Add scontrol options to view and modify layouts tables.\n -- Add MsgAggregationParams which controls a reverse tree to the slurmctld\n    which can be used to aggregate messages to the slurmctld into a single\n    message to reduce communication to the slurmctld.  Currently only epilog\n    complete messages and node registration messages use this logic.\n -- Add sacct and squeue options to print trackable resources.\n -- Add sacctmgr option to display trackable resources.\n -- If an salloc or srun command is executed on a \"front-end\" configuration,\n    that job will be assigned a slurmd shepherd daemon on the same host as used\n    to execute the command when possible rather than an slurmd daemon on an\n    arbitrary front-end node.\n -- Add srun --accel-bind option to control how tasks are bound to GPUs and NIC\n    Generic RESources (GRES).\n -- gres/nic plugin modified to set OMPI_MCA_btl_openib_if_include environment\n    variable based upon allocated devices (usable with OpenMPI and Melanox).\n -- Make it so info options for srun/salloc/sbatch print with just 1 -v instead\n    of 4.\n -- Add \"no_backup_scheduling\" SchedulerParameter to prevent jobs from being\n    scheduled when the backup takes over. Jobs can be submitted, modified and\n    cancelled while the backup is in control.\n -- Enable native Slurm backup controller to reside on an external Cray node\n    when the \"no_backup_scheduling\" SchedulerParameter is used.\n -- Removed TICKET_BASED fairshare. Consider using the FAIR_TREE algorithm.\n -- Disable advanced reservation \"REPLACE\" option on IBM Bluegene systems.\n -- Add support for control distribution of tasks across cores (in addition\n    to existing support for nodes and sockets, (e.g. \"block\", \"cyclic\" or\n    \"fcyclic\" task distribution at 3 levels in the hardware rather than 2).\n -- Create db index on <cluster>_assoc_table.acct. Deleting accounts that didn't\n    have jobs in the job table could take a long time.\n -- The performance of Profiling with HDF5 is improved. In addition, internal\n    structures are changed to make it easier to add new profile types,\n    particularly energy sensors. sh5util will continue to work with either\n    format.\n -- Add partition information to sshare output if the --partition option\n    is specified on the sshare command line.\n -- Add sreport -T/--tres option to identify Trackable RESources (TRES) to\n    report.\n -- Display job in sacct when single step's cpus are different from the job\n    allocation.\n -- Add association usage information to \"scontrol show cache\" command output.\n -- MPI/MVAPICH plugin now requires Munge for authentication.\n -- job_submit/lua: Add default_qos fields. Add job record qos.  Add partition\n    record allow_qos and qos_char fields.\n\n* Changes in Slurm 15.08.0pre5\n==============================\n -- Add jobcomp/elasticsearch plugin. Libcurl is required for build. Configure\n    the server as follows: \"JobCompLoc=http://YOUR_ELASTICSEARCH_SERVER:9200\".\n -- Scancel logic large re-written to better support job arrays.\n -- Added a slurm.conf parameter PrologEpilogTimeout to control how long\n    prolog/epilog can run.\n -- Added TRES (Trackable resources) to track Mem, GRES, license, etc\n    utilization.\n -- Add re-entrant versions of glibc time functions (e.g. localtime) to Slurm\n    in order to eliminate rare deadlock of slurmstepd fork and exec calls.\n -- Constrain kernel memory (if available) in cgroups.\n -- Add PrologFlags option of \"Contain\" to create a proctrack container at\n    job resource allocation time.\n -- Disable the OOM Killer in slurmd and slurmstepd's memory cgroup when using\n    MemSpecLimit.\n\n* Changes in Slurm 15.08.0pre4\n==============================\n -- Burst_buffer/cray - Convert logic to use new commands/API names (e.g.\n    \"dws_setup\" rather than \"bbs_setup\").\n -- Remove the MinJobAge size limitation. It can now exceed 65533 as it\n    is represented using an unsigned integer.\n -- Verify that all plugin version numbers are identical to the component\n    attempting to load them. Without this verification, the plugin can reference\n    Slurm functions in the caller which differ (e.g. the underlying function's\n    arguments could have changed between Slurm versions).\n    NOTE: All plugins (except SPANK) must be built against the identical\n    version of Slurm in order to be used by any Slurm command or daemon. This\n    should eliminate some very difficult to diagnose problems due to use of old\n    plugins.\n -- Increase the MAX_PACK_MEM_LEN define to avoid PMI2 failure when fencing\n    with large amount of ranks (to 1GB).\n -- Requests by normal user to reset a job priority (even to lower it) will\n    result in an error saying to change the job's nice value instead.\n -- SPANK naming changes: For environment variables set using the\n    spank_job_control_setenv() function, the values were available in the\n    slurm_spank_job_prolog() and slurm_spank_job_epilog() functions using\n    getenv where the name was given a prefix of \"SPANK_\". That prefix has\n    been removed for consistency with the environment variables available in\n    the Prolog and Epilog scripts.\n -- Major additions to the layouts framework code.\n -- Add \"TopologyParam\" configuration parameter. Optional value of \"dragonfly\"\n    is supported.\n -- Optimize resource allocation for systems with dragonfly networks.\n -- Add \"--thread-spec\" option to salloc, sbatch and srun commands. This is\n    the count of threads reserved for system use per node.\n -- job_submit/lua: Enable reading and writing job environment variables.\n    For example: if (job_desc.environment.LANGUAGE == \"en_US\") then ...\n -- Added two new APIs slurm_job_cpus_allocated_str_on_node_id()\n    and slurm_job_cpus_allocated_str_on_node() to print the CPUs id\n    allocated to a job.\n -- Specialized memory (a node's MemSpecLimit configuration parameter) is not\n    available for allocation to jobs.\n -- Modify scontrol update job to allow jobid specification without\n    the = sign. 'scontrol update job=123 ...' and 'scontrol update job 123 ...'\n    are both valid syntax.\n -- Archive a month at a time when there are lots of records to archive.\n -- Introduce new sbatch option '--kill-on-invalid-dep=yes|no' which allows\n    users to specify which behavior they want if a job dependency is not\n    satisfied.\n -- Add Slurmdb::qos_get() interface to perl api.\n -- If a job fails to start set the requeue reason to be:\n    job requeued in held state.\n -- Implemented a new MPI key,value PMIX_RING() exchange algorithm as\n    an alternative to PMI2.\n -- Remove possible deadlocks in the slurmctld when the slurmdbd is busy\n    archiving/purging.\n -- Add DB_ARCHIVE debug flag for filtering out debug messages in the slurmdbd\n    when the slurmdbd is archiving/purging.\n -- Fix some power_save mode issues: Parsing of SuspendTime in slurm.conf was\n    bad, powered down nodes would get set non-responding if there was an\n    in-flight message, and permit nodes to be powered down from any state.\n -- Initialize variables in consumable resource plugin to prevent core dump.\n\n* Changes in Slurm 15.08.0pre3\n==============================\n -- CRAY - addition of acct_gather_energy/cray plugin.\n -- Add job credential to \"Run Prolog\" RPC used with a configuration of\n    PrologFlags=alloc. This allows the Prolog to be passed identification of\n    GPUs allocated to the job.\n -- Add SLURM_JOB_CONSTAINTS to environment variables available to the Prolog.\n -- Added \"--mail=stage_out\" option to job submission commands to notify user\n    when burst buffer state out is complete.\n -- Require a \"Reason\" when using scontrol to set a node state to DOWN.\n -- Mail notifications on job BEGIN, END and FAIL now apply to a job array as a\n    whole rather than generating individual email messages for each task in the\n    job array.\n -- task/affinity - Fix memory binding to NUMA with cpusets.\n -- Display job's estimated NodeCount based off of partition's configured\n    resources rather than the whole system's.\n -- Add AuthInfo option of \"cred_expire=#\" to specify the lifetime of a job\n    step credential. The default value was changed from 1200 to 120 seconds.\n -- Set the delay time for job requeue to the job credential lifetime (120\n    seconds by default). This insures that prolog runs on every node when a\n    job is requeued. (This change will slow down launch of re-queued jobs).\n -- Add AuthInfo option of \"cred_expire=#\" to specify the lifetime of a job\n    step credential.\n -- Remove srun --max-launch-time option. The option has not been functional\n    since Slurm version 2.0.\n -- Add sockets and cores to TaskPluginParams' autobind option.\n -- Added LaunchParameters configuration parameter. Have srun command test\n    locally for the executable file if LaunchParameters=test_exec or the\n    environment variable SLURM_TEST_EXEC is set. Without this an invalid\n    command will generate one error message per task launched.\n -- Fix the slurm /etc/init.d script to return 0 upon stopping the\n    daemons and return 1 in case of failure.\n -- Add the ability for a compute node to be allocated to multiple jobs, but\n    restricted to a single user. Added \"--exclusive=user\" option to salloc,\n    sbatch and srun commands. Added \"owner\" field to node record, visible using\n    the scontrol and sview commands. Added new partition configuration parameter\n    \"ExclusiveUser=yes|no\".\n\n* Changes in Slurm 15.08.0pre2\n==============================\n -- Add the environment variables SLURM_JOB_ACCOUNT, SLURM_JOB_QOS\n    and SLURM_JOB_RESERVATION in the batch/srun jobs.\n -- Add sview burst buffer display.\n -- Properly enforce partition Shared=YES option. Previously oversubscribing\n    resources required gang scheduling to be configured.\n -- Enable per-partition gang scheduling resource resolution (e.g. the partition\n    can have SelectTypeParameters=CR_CORE, while the global value is CR_SOCKET).\n -- Make it so a newer version of a slurmstepd can talk to an older srun.\n    allocation. Nodes could have been added while waiting for an allocation.\n -- Expanded --cpu-freq parameters to include min-max:governor specifications.\n    --cpu-freq now supported on salloc and sbatch.\n -- Add support for optimized job allocations with respect to SGI Hypercube\n    topology.\n    NOTE: Only supported with select/linear plugin.\n    NOTE: The program contribs/sgi/netloc_to_topology can be used to build\n    Slurm's topology.conf file.\n -- Remove 64k validation of incoming RPC nodelist size. Validated at 64MB\n    when unpacking.\n -- In slurmstepd() add the user primary group if it is not part of the\n    groups sent from the client.\n -- Added BurstBuffer field to advanced reservations.\n -- For advanced reservation, replace flag \"License_only\" with flag \"Any_Nodes\".\n    It can be used to indicate the an advanced reservation resources (licenses\n    and/or burst buffers) can be used with any compute nodes.\n -- Allow users to specify the srun --resv-ports as 0 in which case no ports\n    will be reserved. The default behaviour is to allocate one port per task.\n -- Interpret a partition configuration of \"Nodes=ALL\" in slurm.conf as\n    including all nodes defined in the cluster.\n -- Added new configuration parameters PowerParameters and PowerPlugin.\n -- Added power management plugin infrastructure.\n -- If job already exceeded one of its QOS/Accounting limits do not\n    return error if user modifies QOS unrelated job settings.\n -- Added DebugFlags value of \"Power\".\n -- When caching user ids of AllowGroups use both getgrnam_r() and getgrent_r()\n    then remove eventual duplicate entries.\n -- Remove rpm dependency between slurm-pam and slurm-devel.\n -- Remove support for the XCPU (cluster management) package.\n -- Add Slurmdb::jobs_get() interface to perl api.\n -- Performance improvement when sending data from srun to stepds when\n    processing fencing.\n -- Add the feature to specify arbitrary field separator when running\n    sacct -p or sacct -P. The command line option is --separator.\n -- Introduce slurm.conf parameter to use Proportional Set Size (PSS) instead\n    of RSS to determinate the memory footprint of a job.\n    Add an slurm.conf option not to kill jobs that is over memory limit.\n -- Add job submission command options: --sicp (available for inter-cluster\n    dependencies) and --power (specify power management options) to salloc,\n    sbatch, and srun commands.\n -- Add DebugFlags option of SICP (inter-cluster option logging).\n -- In order to support inter-cluster job dependencies, the MaxJobID\n    configuration parameter default value has been reduced from 4,294,901,760\n    to 2,147,418,112 and it's maximum value is now 2,147,463,647.\n    ANY JOBS WITH A JOB ID ABOVE 2,147,463,647 WILL BE PURGED WHEN SLURM IS\n    UPGRADED FROM AN OLDER VERSION!\n -- Add QOS name to the output of a partition in squeue/scontrol/sview/smap.\n\n* Changes in Slurm 15.08.0pre1\n==============================\n -- Add sbcast support for file transfer to resources allocated to a job step\n    rather than a job allocation.\n -- Change structures with association in them to assoc to save space.\n -- Add support for job dependencies jointed with OR operator (e.g.\n    \"--depend=afterok:123?afternotok:124\").\n -- Add \"--bb\" (burst buffer specification) option to salloc, sbatch, and srun.\n -- Added configuration parameters BurstBufferParameters and BurstBufferType.\n -- Added burst_buffer plugin infrastructure (needs many more functions).\n -- Make it so when the fanout logic comes across a node that is down we abandon\n    the tree to avoid worst case scenarios when the entire branch is down and\n    we have to try each serially.\n -- Add better error reporting of invalid partitions at submission time.\n -- Move will-run test for multiple clusters from the sbatch code into the API\n    so that it can be used with DRMAA.\n -- If a non-exclusive allocation requests --hint=nomultithread on a\n    CR_CORE/SOCKET system lay out tasks correctly.\n -- Avoid including unused CPUs in a job's allocation when cores or sockets are\n    allocated.\n -- Added new job state of STOPPED indicating processes have been stopped with a\n    SIGSTOP (using scancel or sview), but retain its allocated CPUs. Job state\n    returns to RUNNING when SIGCONT is sent (also using scancel or sview).\n -- Added EioTimeout parameter to slurm.conf. It is the number of seconds srun\n    waits for slurmstepd to close the TCP/IP connection used to relay data\n    between the user application and srun when the user application terminates.\n -- Remove slurmctld/dynalloc plugin as the work was never completed, so it is\n    not worth the effort of continued support at this time.\n -- Remove DynAllocPort configuration parameter.\n -- Add advance reservation flag of \"replace\" that causes allocated resources\n    to be replaced with idle resources. This maintains a pool of available\n    resources that maintains a constant size (to the extent possible).\n -- Added SchedulerParameters option of \"bf_busy_nodes\". When selecting\n    resources for pending jobs to reserve for future execution (i.e. the job\n    can not be started immediately), then preferentially select nodes that are\n    in use. This will tend to leave currently idle resources available for\n    backfilling longer running jobs, but may result in allocations having less\n    than optimal network topology. This option is currently only supported by\n    the select/cons_res plugin.\n -- Permit \"SuspendTime=NONE\" as slurm.conf value rather than only a numeric\n    value to match \"scontrol show config\" output.\n -- Add the 'scontrol show cache' command which displays the associations\n    in slurmctld.\n -- Test more frequently for node boot completion before starting a job.\n    Provides better responsiveness.\n -- Fix PMI2 singleton initialization.\n -- Permit PreemptType=qos and PreemptMode=suspend,gang to be used together.\n    A high-priority QOS job will now oversubscribe resources and gang schedule,\n    but only if there are insufficient resources for the job to be started\n    without preemption. NOTE: That with PreemptType=qos, the partition's\n    Shared=FORCE:# configuration option will permit one job more per resource\n    to be run than than specified, but only if started by preemption.\n -- Remove the CR_ALLOCATE_FULL_SOCKET configuration option.  It is now the\n    default.\n -- Fix a race condition in PMI2 when fencing counters can be out of sync.\n -- Increase the MAX_PACK_MEM_LEN define to avoid PMI2 failure when fencing\n    with large amount of ranks.\n -- Add QOS option to a partition.  This will allow a partition to have\n    all the limits a QOS has.  If a limit is set in both QOS the partition\n    QOS will override the job's QOS unless the job's QOS has the\n    OverPartQOS flag set.\n -- The task_dist_states variable has been split into \"flags\" and \"base\"\n    components. Added SLURM_DIST_PACK_NODES and SLURM_DIST_NO_PACK_NODES values\n    to give user greater control over task distribution. The srun --dist options\n    has been modified to accept a \"Pack\" and \"NoPack\" option. These options can\n    be used to override the CR_PACK_NODE configuration option.\n\n* Changes in Slurm 14.11.12\n===========================\n -- Correct dependency formatting to print array task ids if set.\n -- Fix for configuration of \"AuthType=munge\" and \"AuthInfo=socket=...\" with\n    alternate munge socket path.\n -- BGQ - Remove redeclaration of job_read_lock.\n -- BGQ - Tighter locks around structures when nodes/cables change state.\n -- Fix job array formatting to allow return [0-100:2] display for arrays with\n    step functions rather than [0,2,4,6,8,...] .\n -- Associations - prevent hash table corruption if uid initially unset for\n    a user, which can cause slurmctld to crash if that user is deleted.\n -- Add cast to memory limit calculation to prevent integer overflow for\n    very large memory values.\n -- Fix test cases to have proper int return signature.\n\n* Changes in Slurm 14.11.11\n===========================\n -- Fix systemd's slurmd service from killing slurmstepds on shutdown.\n -- Fix the qstat wrapper when user is removed from the system but still\n    has running jobs.\n -- Log the request to terminate a job at info level if DebugFlags includes\n    the Steps keyword.\n -- Fix potential memory corruption in _slurm_rpc_epilog_complete as well as\n    _slurm_rpc_complete_job_allocation.\n -- Fix incorrectly sized buffer used by jobid2str which will cause buffer\n    overflow in slurmctld. (Bug 2295.)\n\n* Changes in Slurm 14.11.10\n===========================\n -- Fix truncation of job reason in squeue.\n -- If a node is in DOWN or DRAIN state, leave it unavailable for allocation\n    when powered down.\n -- Update the slurm.conf man page documenting better nohold_on_prolog_fail\n    variable.\n -- Don't trucate task ID information in \"squeue --array/-r\" or \"sview\".\n -- Fix a bug which caused scontrol to core dump when releasing or\n    holding a job by name.\n -- Fix unit conversion bug in slurmd which caused wrong memory calculation\n    for cgroups.\n -- Fix issue with GRES in steps so that if you have multiple exclusive steps\n    and you use all the GRES up instead of reporting the configuration isn't\n    available you hold the requesting step until the GRES is available.\n -- Fix slurmdbd backup to use DbdAddr when contacting the primary.\n -- Fix error in MPI documentation.\n -- Fix to handle arrays with respect to number of jobs submitted.  Previously\n    only 1 job was accounted (against MaxSubmitJob) for when an array was\n    submitted.\n -- Correct counting for job array limits, job count limit underflow possible\n    when master cancellation of master job record.\n -- For pending jobs have sacct print 0 for nnodes instead of the bogus 2.\n -- Fix for tracking node state when jobs that have been allocated exclusive\n    access to nodes (i.e. entire nodes) and later relinquish some nodes. Nodes\n    would previously appear partly allocated and prevent use by other jobs.\n -- Fix updating job in db after extending job's timelimit past partition's\n    timelimit.\n -- Fix srun -I<timeout> from flooding the controller with step create requests.\n -- Requeue/hold batch job launch request if job already running (possible if\n    node went to DOWN state, but jobs remained active).\n -- If a job's CPUs/task ratio is increased due to configured MaxMemPerCPU,\n    then increase it's allocated CPU count in order to enforce CPU limits.\n -- Don't mark powered down node as not responding. This could be triggered by\n    race condition of the node suspend and ping logic.\n -- Don't requeue RPC going out from slurmctld to DOWN nodes (can generate\n    repeating communication errors).\n -- Propagate sbatch \"--dist=plane=#\" option to srun.\n -- Fix sacct to not return all jobs if the -j option is given with a trailing\n    ','.\n -- Permit job_submit plugin to set a job's priority.\n -- Fix occasional srun segfault.\n -- Fix issue with sacct, printing 0_0 for array's that had finished in the\n    database but the start record hadn't made it yet.\n -- Fix sacct -j, (nothing but a comma) to not return all jobs.\n -- Prevent slurmstepd from core dumping if /proc/<pid>/stat has\n    unexpected format.\n\n* Changes in Slurm 14.11.9\n==========================\n -- Correct \"sdiag\" backfill cycle time calculation if it yields locks. A\n    microsecond value was being treated as a second value resulting in an\n    overflow in the calcuation.\n -- Fix segfault when updating timelimit on jobarray task.\n -- Fix to job array update logic that can result in a task ID of 4294967294.\n -- Fix of job array update, previous logic could fail to update some tasks\n    of a job array for some fields.\n -- CRAY - Fix seg fault if a blade is replaced and slurmctld is restarted.\n -- Fix plane distribution to allocate in blocks rather than cyclically.\n -- squeue - Remove newline from job array ID value printed.\n -- squeue - Enable filtering for job state SPECIAL_EXIT.\n -- Prevent job array task ID being inappropriately set to NO_VAL.\n -- MYSQL - Make it so you don't have to restart the slurmctld\n    to gain the correct limit when a parent account is root and you\n    remove a subaccount's limit which exists on the parent account.\n -- MYSQL - Close chance of setting the wrong limit on an association\n    when removing a limit from an association on multiple clusters\n    at the same time.\n -- MYSQL - Fix minor memory leak when modifying an association but no\n    change was made.\n -- srun command line of either --mem or --mem-per-cpu will override both the\n    SLURM_MEM_PER_CPU and SLURM_MEM_PER_NODE environment variables.\n -- Prevent slurmctld abort on update of advanced reservation that contains no\n    nodes.\n -- ALPS - Revert commit 2c95e2d22 which also removes commit 2e2de6a4 allowing\n    cray with the SubAllocate option to work as it did with 2.5.\n -- Properly parse CPU frequency data on POWER systems.\n -- Correct sacct.a man pages describing -i option.\n -- Capture salloc/srun information in sdiag statistics.\n -- Fix bug in node selection with topology optimization.\n -- Don't set distribution when srun requests 0 memory.\n -- Read in correct number of nodes from SLURM_HOSTFILE when specifying nodes\n    and --distribution=arbitrary.\n -- Fix segfault in Bluegene setups where RebootQOSList is defined in\n    bluegene.conf and accounting is not setup.\n -- MYSQL - Update mod_time when updating a start job record or adding one.\n -- MYSQL - Fix issue where if an association id ever changes on at least a\n    portion of a job array is pending after it's initial start in the\n    database it could create another row for the remain array instead\n    of using the already existing row.\n -- Fix scheduling anomaly with job arrays submitted to multiple partitions,\n    jobs could be started out of priority order.\n -- If a host has suspened jobs do not reboot it. Reboot only hosts\n    with no jobs in any state.\n -- ALPS - Fix issue when using --exclusive flag on srun to do the correct\n    thing (-F exclusive) instead of -F share.\n -- Fix various memory leaks in the Perl API.\n -- Fix a bug in the controller which display jobs in CF state as RUNNING.\n -- Preserve advanced _core_ reservation when nodes added/removed/resized on\n    slurmctld restart. Rebuild core_bitmap as needed.\n -- Fix for non-standard Munge port location for srun/pmi use.\n -- Fix gang scheduling/preemption issue that could cancel job at startup.\n -- Fix a bug in squeue which prevented squeue -tPD to print array jobs.\n -- Sort job arrays in job queue according to array_task_id when priorities are\n    equal.\n -- Fix segfault in sreport when there was no response from the dbd.\n -- ALPS - Fix compile to not link against -ljob and -lexpat with every lib\n    or binary.\n -- Fix testing for CR_Memory when CR_Memory and CR_ONE_TASK_PER_CORE are used\n    with select/linear.\n -- MySQL - Fix minor memory leak if a connection ever goes away whist using it.\n -- ALPS - Make it so srun --hint=nomultithread works correctly.\n -- Prevent job array task ID from being reported as NO_VAL if last task in the\n    array gets requeued.\n -- Fix some potential deadlock issues when state files don't exist in the\n    association manager.\n -- Correct RebootProgram logic when executed outside of a maintenance\n    reservation.\n -- Requeue job if possible when slurmstepd aborts.\n\n* Changes in Slurm 14.11.8\n==========================\n -- Eliminate need for user to set user_id on job_update calls.\n -- Correct list of unavailable nodes reported in a job's \"reason\" field when\n    that job can not start.\n -- Map job --mem-per-cpu=0 to --mem=0.\n -- Fix squeue -o %m and %d unit conversion to Megabytes.\n -- Fix issue with incorrect time calculation in the priority plugin when\n    a job runs past it's time limit.\n -- Prevent users from setting job's partition to an invalid partition.\n -- Fix sreport core dump when requesting\n    'job SizesByAccount grouping=individual'.\n -- select/linear: Correct count of CPUs allocated to job on system with\n    hyperthreads.\n -- Fix race condition where last array task might not get updated in the db.\n -- CRAY - Remove libpmi from rpm install\n -- Fix squeue -o %X output to correctly handle NO_VAL and suffix.\n -- When deleting a job from the system set the job_id to 0 to avoid memory\n    corruption if thread uses the pointer basing validity off the id.\n -- Fix issue where sbatch would set ntasks-per-node to 0 making any srun\n    afterward cause a divide by zero error.\n -- switch/cray: Refine logic to set PMI_CRAY_NO_SMP_ENV environment variable.\n -- When sacctmgr loads archives with version less than 14.11 set the array\n    task id to NO_VAL, so sacct can display the job ids correctly.\n -- When using memory cgroup if a task uses more memory than requested\n    the failures are logged into memory.failcnt count file by cgroup\n    and the user is notified by slurmstepd about it.\n -- Fix scheduling inconsistency with GRES bound to specific CPUs.\n -- If user belongs to a group which has split entries in /etc/group\n    search for its username in all groups.\n -- Do not consider nodes explicitly powered up as DOWN with reason of \"Node\n    unexpected rebooted\".\n -- Use correct slurmd spooldir when creating cpu-frequency locks.\n -- Note that TICKET_BASED fairshare will be deprecated in the future. Consider\n    using the FAIR_TREE algorithm instead.\n -- Set job's reason to BadConstaints when job can't run on any node.\n -- Prevent abort on update of reservation with no nodes (licenses only).\n -- Prevent slurmctld from dumping core if job_resrcs is missing in the\n    job data structure.\n -- Fix squeue to print array task ids according to man page when\n    SLURM_BITSTR_LEN is defined in the environment.\n -- In squeue, sort jobs based on array job ID if available.\n -- Fix the calculation of job energy by not including the NO_VAL values.\n -- Advanced reservation fixes: enable update of bluegene reservation, avoid\n    abort on multi-core reservations.\n -- Set the totalview_stepid to the value of the job step instead of NO_VAL.\n -- Fix slurmdbd core dump if the daemon does not have connection with\n    the database.\n -- Display error message when attempting to modify priority of a held job.\n -- Backfill scheduler: The configured backfill_interval value (default 30\n    seconds) is now interpretted as a maximum run time for the backfill\n    scheduler. Once reached, the scheduler will build a new job queue and\n    start over, even if not all jobs have been tested.\n -- Backfill scheduler now considers OverTimeLimit and KillWait configuration\n    parameters to estimate when running jobs will exit.\n -- Correct task layout with CR_Pack_Node option and more than 1 CPU per task.\n -- Fix the scontrol man page describing the release argument.\n -- When job QOS is modified, do so before attempting to change partition in\n    order to validate the partition's Allow/DenyQOS parameter.\n\n* Changes in Slurm 14.11.7\n==========================\n -- Initialize some variables used with the srun --no-alloc option that may\n    cause random failures.\n -- Add SchedulerParameters option of sched_min_interval that controls the\n    minimum time interval between any job scheduling action. The default value\n    is zero (disabled).\n -- Change default SchedulerParameters=max_sched_time from 4 seconds to 2.\n -- Refactor scancel so that all pending jobs are cancelled before starting\n    cancellation of running jobs. Otherwise they happen in parallel and the\n    pending jobs can be scheduled on resources as the running jobs are being\n    cancelled.\n -- ALPS - Add new cray.conf variable NoAPIDSignalOnKill.  When set to yes this\n    will make it so the slurmctld will not signal the apid's in a batch job.\n    Instead it relies on the rpc coming from the slurmctld to kill the job to\n    end things correctly.\n -- ALPS - Have the slurmstepd running a batch job wait for an ALPS release\n    before ending the job.\n -- Initialize variables in consumable resource plugin to prevent core dump.\n -- Fix scancel bug which could return an error on attempt to signal a job step.\n -- In slurmctld communication agent, make the thread timeout be the configured\n    value of MessageTimeout rather than 30 seconds.\n -- sshare -U/--Users only flag was used uninitialized.\n -- Cray systems, add \"plugstack.conf.template\" sample SPANK configuration file.\n -- BLUEGENE - Set DB2NOEXITLIST when starting the slurmctld daemon to avoid\n    random crashing in db2 when the slurmctld is exiting.\n -- Make full node reservations display correctly the core count instead of\n    cpu count.\n -- Preserve original errno on execve() failure in task plugin.\n -- Add SLURM_JOB_NAME env variable to an salloc's environment.\n -- Overwrite SLURM_JOB_NAME in an srun when it gets an allocation.\n -- Make sure each job has a wckey if that is something that is tracked.\n -- Make sure old step data is cleared when job is requeued.\n -- Load libtinfo as needed when building ncurses tools.\n -- Fix small memory leak in backup controller.\n -- Fix segfault when backup controller takes control for second time.\n -- Cray - Fix backup controller running native Slurm.\n -- Provide prototypes for init_setproctitle()/fini_setproctitle on NetBSD.\n -- Add configuration test to find out the full path to su command.\n -- preempt/job_prio plugin: Fix for possible infinite loop when identifying\n    preemptable jobs.\n -- preempt/job_prio plugin: Implement the concept of Warm-up Time here. Use\n    the QoS GraceTime as the amount of time to wait before preempting.\n    Basically, skip preemption if your time is not up.\n -- Make srun wait KillWait time when a task is cancelled.\n -- switch/cray: Revert logic added to 14.11.6 that set \"PMI_CRAY_NO_SMP_ENV=1\"\n    if CR_PACK_NODES is configured.\n\n* Changes in Slurm 14.11.6\n==========================\n -- If SchedulerParameters value of bf_min_age_reserve is configured, then\n    a newly submitted job can start immediately even if there is a higher\n    priority non-runnable job which has been waiting for less time than\n    bf_min_age_reserve.\n -- qsub wrapper modified to export \"all\" with -V option\n -- RequeueExit and RequeueExitHold configuration parameters modified to accept\n    numeric ranges. For example \"RequeueExit=1,2,3,4\" and \"RequeueExit=1-4\" are\n    equivalent.\n -- Correct the job array specification parser to accept brackets in job array\n    expression (e.g. \"123_[4,7-9]\").\n -- Fix for misleading job submit failure errors sent to users. Previous error\n    could indicate why specific nodes could not be used (e.g. too small memory)\n    when other nodes could be used, but were not for another reason.\n -- Fix squeue --array to display correctly the array elements when the\n    % separator is specified at the array submission time.\n -- Fix priority from not being calculated correctly due to memory issues.\n -- Fix a transient pending reason 'JobId=job_id has invalid QOS'.\n -- A non-administrator change to job priority will not be persistent except\n    for holding the job. User's wanting to change a job priority on a persistent\n    basis should reset it's \"nice\" value.\n -- Print buffer sizes as unsigned values when failed to pack messages.\n -- Fix race condition where sprio would print factors without weights applied.\n -- Document the sacct option JobIDRaw which for arrays prints the jobid instead\n    of the arrayTaskId.\n -- Allow users to modify MinCPUsNode, MinMemoryNode and MinTmpDiskNode of\n    their own jobs.\n -- Increase the jobid print field in SQUEUE_FORMAT in\n    opt_modulefiles_slurm.in.\n -- Enable compiling without optimizations and with debugging symbols by\n    default. Disable this by configuring with --disable-debug.\n -- job_submit/lua plugin: Add mail_type and mail_user fields.\n -- Correct output message from sshare.\n -- Use standard statvfs(2) syscall if available, in preference to\n    non-standard statfs.\n -- Add a new option -U/--Users to sshare to display only users\n    information, parent and ancestors are not printed.\n -- Purge 50000 records at a time so that locks can released periodically.\n -- Fix potentially uninitialized variables\n -- ALPS - Fix issue where a frontend node could become unresponsive and never\n    added back into the system.\n -- Gate epilog complete messages as done with other messages\n -- If we have more than a certain number of agents (50) wait longer when gating\n    rpcs.\n -- FrontEnd - ping non-responding or down nodes.\n -- switch/cray: If CR_PACK_NODES is configured, then set the environment\n    variable \"PMI_CRAY_NO_SMP_ENV=1\"\n -- Fix invalid memory reference in SlurmDBD when putting a node up.\n -- Allow opening of plugstack.conf even when a symlink.\n -- Fix scontrol reboot so that rebooted nodes will not be set down with reason\n    'Node xyz unexpectedly rebooted' but will be correctly put back to service.\n -- CRAY - Throttle the post NHC operations as to not hog the job write lock\n    if many steps/jobs finish at once.\n -- Disable changes to GRES count while jobs are running on the node.\n -- CRAY - Fix issue with scontrol reconfig.\n -- slurmd: Remove wrong reporting of \"Error reading step  ... memory limit\".\n    The logic was treating success as an error.\n -- Eliminate \"Node ping apparently hung\" error messages.\n -- Fix average CPU frequency calculation.\n -- When allocating resources with resolution of sockets, charge the job for all\n    CPUs on allocated sockets rather than just the CPUs on used cores.\n -- Prevent slurmdbd error if cluster added or removed while rollup in progress.\n    Removing a cluster can cause slurmdbd to abort. Adding a cluster can cause\n    the slurmdbd rollup to hang.\n -- sview - When right clicking on a tab make sure we don't display the page\n    list, but only the column list.\n -- FRONTEND - If doing a clean start make sure the nodes are brought up in the\n    database.\n -- MySQL - Fix issue when using the TrackSlurmctldDown and nodes are down at\n    the same time, don't double bill the down time.\n -- MySQL - Various memory leak fixes.\n -- sreport - Fix Energy displays\n -- Fix node manager logic to keep unexpectedly rebooted node in state\n    NODE_STATE_DOWN even if already down when rebooted.\n -- Fix for array jobs submitted to multiple partitions not starting.\n -- CRAY - Enable ALPs mpp compatibility code in sbatch for native Slurm.\n -- ALPS - Move basil_inventory to less confusing function.\n -- Add SchedulerParameters option of \"sched_max_job_start=\"  to limit the\n    number of jobs that can be started in any single execution of the main\n    scheduling logic.\n -- Fixed compiler warnings generated by gcc version >= 4.6.\n -- sbatch to stop parsing script for \"#SBATCH\" directives after first command,\n    which matches the documentation.\n -- Overwrite the SLURM_JOB_NAME in sbatch if already exist in the environment\n    and use the one specified on the command line --job-name.\n -- Remove xmalloc_nz from unpack functions.  If the unpack ever failed the\n    free afterwards would not have zeroed out memory on the variables that\n    didn't get unpacked.\n -- Improve database interaction from controller.\n -- Fix for data shift when loading job archives.\n -- ALPS - Added new SchedulerParameters=inventory_interval to specify how\n    often an inventory request is handled.\n -- ALPS - Don't run a release on a reservation on the slurmctld for a batch\n    job.  This is already handled on the stepd when the script finishes.\n\n* Changes in Slurm 14.11.5\n==========================\n -- Correct the squeue command taking into account that a node can\n    have NULL name if it is not in DNS but still in slurm.conf.\n -- Fix slurmdbd regression which would cause a segfault when a node is set\n    down with no reason.\n -- BGQ - Fix issue with job arrays not being handled correctly\n    in the runjob_mux plugin.\n -- Print FAIR_TREE, if configured, in \"scontrol show config\" output for\n    PriorityFlags.\n -- Add SLURM_JOB_GPUS environment variable to those available in the Prolog.\n -- Load lua-5.2 library if using lua5.2 for lua job submit plugin.\n -- GRES logic: Prevent bad node_offset due to not preserving no_consume flag.\n -- Fix wrong variables used in the wrapper functions needed for systems that\n    don't support strong_alias\n -- Fix code for apple computers SOL_TCP is not defined\n -- Cray/BASIL - Check for mysql credentials in /root/.my.cnf.\n -- Fix sprio showing wrong priority for job arrays until priority is\n    recalculated.\n -- Account to batch step all CPUs that are allocated to a job not\n    just one since the batch step has access to all CPUs like other steps.\n -- Fix job getting EligibleTime set before meeting dependency requirements.\n -- Correct the initialization of QOS MinCPUs per job limit.\n -- Set the debug level of information messages in cgroup plugin to debug2.\n -- For job running under a debugger, if the exec of the task fails, then\n    cancel its I/O and abort immediately rather than waiting 60 seconds for\n    I/O timeout.\n -- Fix associations not getting default qos set until after a restart.\n -- Set the value of total_cpus not to be zero before invoking\n    acct_policy_job_runnable_post_select.\n -- MySQL - When requesting cluster resources, only return resources for the\n    cluster(s) requested.\n -- Add TaskPluginParam=autobind=threads option to set a default binding in the\n    case that \"auto binding\" doesn't find a match.\n -- Introduce a new SchedulerParameters variable nohold_on_prolog_fail.\n    If configured don't requeue jobs on hold is a Prolog fails.\n -- Make it so sched_params isn't read over and over when an epilog complete\n    message comes in\n -- Fix squeue -L <licenses> not filtering out jobs with licenses.\n -- Changed the implementation of xcpuinfo_abs_to_mac() be identical\n    _abs_to_mac() to fix CPUs allocation using cpuset cgroup.\n -- Improve the explanation of the unbuffered feature in the\n    srun man page.\n -- Make taskplugin=cgroup work for core spec.  needed to have task/cgroup\n    before.\n -- Fix reports not using the month usage table.\n -- BGQ - Sanity check given for translating small blocks into slurm bg_records.\n -- Fix bug preventing the requeue/hold or requeue/special_exit of job from the\n    completing state.\n -- Cray - Fix for launching batch step within an existing job allocation.\n -- Cray - Add ALPS_APP_ID_ENV environment variable.\n -- Increase maximum MaxArraySize configuration parameter value from 1,000,001\n    to 4,000,001.\n -- Added new SchedulerParameters value of bf_min_age_reserve. The backfill\n    scheduler will not reserve resources for pending jobs until they have\n    been pending for at least the specified number of seconds. This can be\n    valuable if jobs lack time limits or all time limits have the same value.\n -- Fix support for --mem=0 (all memory of a node) with select/cons_res plugin.\n -- Fix bug that can permit someone to kill job array belonging to another user.\n -- Don't set the default partition on a license only reservation.\n -- Show a NodeCnt=0, instead of NO_VAL, in \"scontrol show res\" for a license\n    only reservation.\n -- BGQ - When using static small blocks make sure when clearing the job the\n    block is set up to it's original state.\n -- Start job allocation using lowest numbered sockets for block task\n    distribution for consistency with cyclic distribution.\n\n* Changes in Slurm 14.11.4\n==========================\n -- Make sure assoc_mgr locks are initialized correctly.\n -- Correct check of enforcement when filling in an association.\n -- Make sacctmgr print out classification correctly for clusters.\n -- Add array_task_str to the perlapi job info.\n -- Fix for slurmctld abort with GRES types configured and no CPU binding.\n -- Fix for GRES scheduling where count > 1 per topology type (or GRES types).\n -- Make CR_ONE_TASK_PER_CORE work correctly with task/affinity.\n -- job_submit/pbs - Fix possible deadlock.\n -- job_submit/lua - Add \"alloc_node\" to job information available.\n -- Fix memory leak in mysql accounting when usage rollup happens.\n -- If users specify ALL together with other variables using the\n    --export sbatch/srun command line option, propagate the users'\n    environ to the execution side.\n -- Fix job array scheduling anomaly that can stop scheduling of valid tasks.\n -- Fix perl api tests for libslurmdb to work correctly.\n -- Remove some misleading logs related to non-consumable GRES.\n -- Allow --ignore-pbs to take effect when read as an #SBATCH argument.\n -- Fix Slurmdb::clusters_get() in perl api from not returning information.\n -- Fix TaskPluginParam=Cpusets from logging error message about not being able\n    to remove cpuset dir which was already removed by the release_agent.\n -- Fix sorting by time left in squeue.\n -- Fix the file name substitution for job stderr when %A, %a %j and %u\n    are specified.\n -- Remove minor warning when compiling slurmstepd.\n -- Fix database resources so they can add new clusters to them after they have\n    initially been added.\n -- Use the slurm_getpwuid_r wrapper of getpwuid_r to handle possible\n    interrupts.\n -- Correct the scontrol man page and command listing which node states can\n    be set by the command.\n -- Stop sacct from printing non-existent stat information for\n    Front End systems.\n -- Correct srun and acct_gather.conf man pages, mention Filesystem instead\n    of Lustre.\n -- When a job using multiple partition starts send to slurmdbd only\n    the partition in which the job runs.\n -- ALPS - Fix depth for MemoryAllocation in BASIL with CLE 5.2.3.\n -- Fix assoc_mgr hash to deal with users that don't have a uid yet when making\n    reservations.\n -- When a job uses multiple partition set the environment variable\n    SLURM_JOB_PARTITION to be the one in which the job started.\n -- Print spurious message about the absence of cgroup.conf at log level debug2\n    instead of info.\n -- Enable CUDA v7.0+ use with a Slurm configuration of TaskPlugin=task/cgroup\n    ConstrainDevices=yes (in cgroup.conf). With that configuration\n    CUDA_VISIBLE_DEVICES will start at 0 rather than the device number.\n -- Fix job array logic that can cause slurmctld to abort.\n -- Report job \"shared\" field properly in scontrol, squeue, and sview.\n -- If a job is requeued because of RequeueExit or RequeueExitHold sent event\n    REQUEUED to slurmdbd.\n -- Fix build if hwloc is in non-standard location.\n -- Fix slurmctld job recovery logic which could cause the last task in a job\n    array to be lost.\n -- Fix slurmctld initialization problem which could cause requeue of the last\n    task in a job array to fail if executed prior to the slurmctld loading\n    the maximum size of a job array into a variable in the job_mgr.c module.\n -- Fix fatal in controller when deleting a user association of a user which\n    had been previously removed from the system.\n -- MySQL - If a node state and reason are the same on a node state change\n    don't insert a new row in the event table.\n -- Fix issue with \"sreport cluster AccountUtilizationByUser\" when using\n    PrivateData=users.\n -- Fix perlapi tests for libslurm perl module.\n -- MySQL - Fix potential issue when PrivateData=Usage and a normal user\n    runs certain sreport reports.\n\n* Changes in Slurm 14.11.3\n==========================\n -- Prevent vestigial job record when canceling a pending job array record.\n -- Fixed squeue core dump.\n -- Fix job array hash table bug, could result in slurmctld infinite loop or\n    invalid memory reference.\n -- In srun honor ntasks_per_node before looking at cpu count when the user\n    doesn't request a number of tasks.\n -- Fix ghost job when submitting job after all jobids are exhausted.\n -- MySQL - Enhanced coordinator security checks.\n -- Fix for task/affinity if an admin configures a node for having threads\n    but then sets CPUs to only represent the number of cores on the node.\n -- Make it so previous versions of salloc/srun work with newer versions\n    of Slurm daemons.\n -- Avoid delay on commit for PMI rank 0 to improve performance with some\n    MPI implementations.\n -- auth/munge - Correct logic to read old format AccountingStoragePass.\n -- Reset node \"RESERVED\" state as appropriate when deleting a maintenance\n    reservation.\n -- Prevent a job manually suspended from being resumed by gang scheduler once\n    free resources are available.\n -- Prevent invalid job array task ID value if a task is started using gang\n    scheduling.\n -- Fixes for clean build on FreeBSD.\n -- Fix documentation bugs in slurm.conf.5. DenyAccount should be DenyAccounts.\n -- For backward compatibility with older versions of OMPI not compiled\n    with --with-pmi restore the SLURM_STEP_RESV_PORTS in the job environment.\n -- Update the html documentation describing the integration with openmpi.\n -- Fix sacct when searching by nodelist.\n -- Fix cosmetic info statements when dealing with a job array task instead of\n    a normal job.\n -- Fix segfault with job arrays.\n -- Correct the sbatch pbs parser to process -j.\n -- BGQ - Put print statement under a DebugFlag.  This was just an oversight.\n -- BLUEGENE - Remove check that would erroneously remove the CONFIGURING\n    flag from a job while the job is waiting for a block to boot.\n -- Fix segfault in slurmstepd when job exceeded memory limit.\n -- Fix race condition that could start a job that is dependent upon a job array\n    before all tasks of that job array complete.\n -- PMI2 race condition fix.\n\n* Changes in Slurm 14.11.2\n==========================\n -- Fix Centos5 compile errors.\n -- Fix issue with association hash not getting the correct index which\n    could result in seg fault.\n -- Fix salloc/sbatch -B segfault.\n -- Avoid huge malloc if GRES configured with \"Type\" and huge \"Count\".\n -- Fix jobs from starting in overlapping reservations that won't finish before\n    a \"maint\" reservation begins.\n -- When node gets drained while in state mixed display its status as draining\n    in sinfo output.\n -- Allow priority/multifactor to work with sched/wiki(2) if all priorities\n    have no weight.  This allows for association and QOS decay limits to work.\n -- Fix \"squeue --start\" to override SQUEUE_FORMAT env variable.\n -- Fix scancel to be able to cancel multiple jobs that are space delimited.\n -- Log Cray MPI job calling exit() without mpi_fini(), but do not treat it as\n    a fatal error. This partially reverts logic added in version 14.03.9.\n -- sview - Fix displaying of suspended steps elapsed times.\n -- Increase number of messages that get cached before throwing them away\n    when the DBD is down.\n -- Fix jobs from starting in overlapping reservations that won't finish before\n    a \"maint\" reservation begins.\n -- Restore GRES functionality with select/linear plugin. It was broken in\n    version  14.03.10.\n -- Fix bug with GRES having multiple types that can cause slurmctld abort.\n -- Fix squeue issue with not recognizing \"localhost\" in --nodelist option.\n -- Make sure the bitstrings for a partitions Allow/DenyQOS are up to date\n    when running from cache.\n -- Add smap support for job arrays and larger job ID values.\n -- Fix possible race condition when attempting to use QOS on a system running\n    accounting_storage/filetxt.\n -- Fix issue with accounting_storage/filetxt and job arrays not being printed\n    correctly.\n -- In proctrack/linuxproc and proctrack/pgid, check the result of strtol()\n    for error condition rather than errno, which might have a vestigial error\n    code.\n -- Improve information recording for jobs deferred due to advanced\n    reservation.\n -- Exports eio_new_initial_obj to the plugins and initialize kvs_seq on\n    mpi/pmi2 setup to support launching.\n\n* Changes in Slurm 14.11.1\n==========================\n -- Get libs correct when doing the xtree/xhash make check.\n -- Update xhash/tree make check to work correctly with current code.\n -- Remove the reference 'experimental' for the jobacct_gather/cgroup\n    plugin.\n -- Add QOS manipulation examples to the qos.html documentation page.\n -- If 'squeue -w node_name' specifies an unknown host name print\n    an error message and return 1.\n -- Fix race condition in job_submit plugin logic that could cause slurmctld to\n    deadlock.\n -- Job wait reason of \"ReqNodeNotAvail\" expanded to identify unavailable nodes\n    (e.g. \"ReqNodeNotAvail(Unavailable:tux[3-6])\").\n\n* Changes in Slurm 14.11.0\n==========================\n -- ALPS - Fix issue with core_spec warning.\n -- Allow multiple partitions to be specified in sinfo -p.\n -- Install the service files in /usr/lib/systemd/system.\n -- MYSQL - Add id_array_job and id_resv keys to $CLUSTER_job_table.  THIS\n    COULD TAKE A WHILE TO CREATE THE KEYS SO BE PATIENT.\n -- CRAY - Resize bitmaps on a restart and find we have more blades\n    than before.\n -- Add new eio API function for removing unused connections.\n -- ALPS - Fix issue where batch allocations weren't correctly confirmed or\n    released.\n -- Define DEFAULT_MAX_TASKS_PER_NODE based on MAX_TASKS_PER_NODE from\n    slurm.h as per documentation.\n -- Update the FAQ about relocating slurmctld.\n -- In the memory cgroup enable memory.use_hierarchy in the cgroup root.\n -- Export eio.c functions for use by MPI/PMI2.\n -- Add SLURM_CLUSTER_NAME to job environment.\n\n* Changes in Slurm 14.11.0rc3\n=============================\n -- Allow envs to override autotools binaries in autogen.sh\n -- Added system services files.\n -- If the jobs pends with DependencyNeverSatisfied keep it pending even after\n    the job which it was depending upon was cleaned.\n -- Let operators (in addition to user root and SlurmUser) see job script for\n    other user's jobs.\n -- Perl API modified to return node state of MIXED rather than ALLOCATED if\n    only some CPUs allocated.\n -- Double Munge connect retry timeout from 1 to 2 seconds.\n -- sview - Remove unneeded code that was resolved globally in commit\n    98e24b0dedc.\n -- Collect and report the accounting of the batch step and its children.\n -- Add configure checks for faccessat and eaccess, and make use of one of\n    them if available.\n -- Make configure --enable-developer also set --enable-debug\n -- Introduce a SchedulerParameters variable kill_invalid_depend, if set\n    then jobs pending with invalid dependency are going to be terminated.\n -- Move spank_user_task() call in slurmstepd after the task_g_pre_launch()\n    so that the task affinity information is available to spank.\n -- Make /etc/init.d/slurm script return value 3 when the daemon is\n    not running. This is required by Linux Standard Base Core\n    Specification 3.1\n\n* Changes in Slurm 14.11.0rc2\n=============================\n -- Logs for jobs which are explicitly requeued will say so rather than saying\n    that a node in their allocation failed.\n -- Updated the documentation about the remote licenses served by\n    the Slurm database.\n -- Insure that slurm_spank_exit() is only called once from srun.\n -- Change the signature of net_set_low_water() to use 4 bytes instead of 8.\n -- Export working_cluster_rec in libslurmdb.so as well as move some function\n    definitions needed for drmaa.\n -- If using cons_res or serial cause a fatal in the plugin instead of causing\n    the SelectTypeParameters to magically set to CR_CPU.\n -- Enhance task/affinity auto binding to consider tasks * cpus-per-task.\n -- Fix regression the priority/multifactor which would cause memory corruption.\n    Issue is only in rc1.\n -- Add PrivateData value of \"cloud\". If set, powered down nodes in the cloud\n    will be visible.\n -- Sched/backfill - Eliminate clearing start_time of running jobs.\n -- Fix various backwards compatibility issues.\n -- If failed to launch a batch job, requeue it in hold.\n\n* Changes in Slurm 14.11.0rc1\n=============================\n -- When using cgroup name the batch step as step_batch instead of\n    batch_4294967294\n -- Changed LEVEL_BASED priority to be \"Fair_Tree\"\n -- Port to NetBSD.\n -- BGQ - Add cnode based reservations.\n -- Alongside totalview_jobid implement totalview_stepid available\n    to sattach.\n -- Add ability to include other files in slurm.conf based upon the ClusterName.\n -- Update strlcpy to latest upstream version.\n -- Add reservation information in the sacct and sreport output.\n -- Add job priority calculation check for overflow and fix memory leak.\n -- Add SchedulerParameters option of pack_serial_at_end to put serial jobs at\n    the end of the available nodes rather than using a best fit algorithm.\n -- Allow regular users to view default sinfo output when\n    privatedata=reservations is set.\n -- PrivateData=reservation modified to permit users to view the reservations\n    which they have access to (rather then preventing them from seeing ANY\n    reservation).\n -- job_submit/lua: Fix job_desc set field logic\n\n* Changes in Slurm 14.11.0pre5\n==============================\n -- Fix sbatch --export=ALL, it was treated by srun as a request to explicitly\n    export only the environment variable named \"ALL\".\n -- Improve scheduling of jobs in reservations that overlap other reservations.\n -- Modify sgather to make global file systems easier to configure.\n -- Added sacctmgr reconfig to reread the slurmdbd.conf in the slurmdbd.\n -- Modify scontrol job operations to accept comma delimited list of job IDs.\n    Applies to job update, hold, release, suspend, resume, requeue, and\n    requeuehold operations.\n -- Refactor job_submit/lua interface. LUA FUNCTIONS NEED TO CHANGE! The\n    lua script no longer needs to explicitly load meta-tables, but information\n    is available directly using names slurm.reservations, slurm.jobs,\n    slurm.log_info, etc. Also, the job_submit.lua script is reloaded when\n    updated without restarting the slurmctld daemon.\n -- Allow users to specify --resv_ports to have value 0.\n -- Cray MPMD (Multiple-Program Multiple-Data) support completed.\n -- Added ability for \"scontrol update\" to references jobs by JobName (and\n    filtered optionally by UserID).\n -- Add support for an advanced reservation start time that remains constant\n    relative to the current time. This can be used to prevent the starting of\n    longer running jobs on select nodes for maintenance purpose. See the\n    reservation flag \"TIME_FLOAT\" for more information.\n -- Enlarge the jobid field to 18 characters in squeue output.\n -- Added \"scontrol write config\" option to save a copy of the current\n    configuration in a file containing a time stamp.\n -- Eliminate native Cray specific port management. Native Cray systems must\n    now use the MpiParams configuration parameter to specify ports to be used\n    for commmunications. When upgrading Native Cray systems from version 14.03,\n    all running jobs should be killed and the switch_cray_state file (in\n    SaveStateLocation of the nodes where the slurmctld daemon runs) must be\n    explicitly deleted.\n\n* Changes in Slurm 14.11.0pre4\n==============================\n -- Added job array data structure and removed 64k array size restriction.\n -- Added SchedulerParameters options of bf_max_job_array_resv to control how\n    many tasks of a job array should have resources reserved for them.\n -- Added more validity checking of incoming job submit requests.\n -- Added srun --export option to set/export specific environment variables.\n -- Scontrol modified to print separate error messages for job arrays with\n    different exit codes on the different tasks of the job array. Applies to\n    job suspend and resume operations.\n -- Fix race condition in CPU frequency set with job preemption.\n -- Always call select plugin on step termination, even if the job is also\n    complete.\n -- Srun executable names beginning with \".\" will be resolved based upon the\n    working directory and path on the compute node rather than the submit node.\n -- Add node state string suffix of \"$\" to identify nodes in maintenance\n    reservation or scheduled for reboot. This applies to scontrol, sinfo,\n    and sview commands.\n -- Enable scontrol to clear a nodes's scheduled reboot by setting its state\n    to \"RESUME\".\n -- As per sbatch and srun documentation when the --signal option is used\n    signal only the steps and unless, in the case, of a batch job B is\n    specified in which case signal only the batch script.\n -- Modify AuthInfo configuration parameter to accept credential lifetime\n    option.\n -- Modify crypto/munge plugin to use socket and timeout specified in AuthInfo.\n -- If we have a state for a step on completion put that in the database\n    instead of guessing off the exit_code.\n -- Added squeue -P/--priority option that can be used to display pending jobs\n    in the same order as used by the Slurm scheduler even if jobs are submitted\n    to multiple partitions (job is reported once per usable partition).\n -- Improve the pending reason description for various QOS limits. For each\n    QOS limit that causes a job to be pending print its specific reason.\n    For example if job pends because of GrpCpus the squeue command will\n    print QOSGrpCpuLimit as pending reason.\n -- sched/backfill - Set expected start time of job submitted to multiple\n    partitions to the earliest start time on any of the partitions.\n -- Introduce a MAX_BATCH_REQUEUE define that indicates how many times a job\n    can be requeued upon prolog failure. When the number is reached the job\n    is put on hold with reason JobHoldMaxRequeue.\n -- Add sbatch job array option to limit the number of simultaneously running\n    tasks from a job array (e.g. \"--array=0-15%4\").\n -- Implemented a new QOS limit MinCPUs. Users running under a QOS must\n    request a minimum number of CPUs which is at least MinCPUs otherwise\n    their job will pend.\n -- Introduced a new pending reason WAIT_QOS_MIN_CPUS to reflect the new QOS\n    limit.\n -- Job array dependency based upon state is now dependent upon the state of\n    the array as a whole (e.g. afterok requires ALL tasks to complete\n    sucessfully, afternotok is true if ANY tasks does not complete successfully,\n    and after requires all tasks to at least be started).\n -- The srun -u/--unbuffered options set the stdout of the task launched\n    by srun to be line buffered.\n -- The srun options -/--label and -u/--unbuffered can be specified together.\n    This limitation has been removed.\n -- Provide sacct display of gres accounting information per job.\n -- Change the node status size from uin16_t to uint32_t.\n\n* Changes in Slurm 14.11.0pre3\n==============================\n -- Move xcpuinfo.[c|h] to the slurmd since it isn't needed anywhere else\n    and will avoid the need for all the daemons to link to libhwloc.\n -- Add memory test to job_submit/partition plugin.\n -- Added new internal Slurm functions xmalloc_nz() and xrealloc_nz(), which do\n    not initialize the allocated memory to zero for improved performance.\n -- Modify hostlist function to dynamically allocate buffer space for improved\n    performance.\n -- In the job_submit plugin: Remove all slurmctld locks prior to job_submit()\n    being called for improved performance. If any slurmctld data structures are\n    read or modified, add locks directly in the plugin.\n -- Added PriorityFlag LEVEL_BASED described in doc/html/level_based.shtml\n -- If Fairshare=parent is set on an account, that account's children will be\n    effectively reparented for fairshare calculations to the first parent of\n    their parent that is not Fairshare=parent.  Limits remain the same,\n    only it's fairshare value is affected.\n\n* Changes in Slurm 14.11.0pre2\n==============================\n -- Added AllowSpecResourcesUsage configuration parameter in slurm.conf. This\n    allows jobs to use specialized resources on nodes allocated to them if the\n    job designates --core-spec=0.\n -- Add new SchedulerParameters option of build_queue_timeout to throttle how\n    much time can be consumed building the job queue for scheduling.\n -- Added HealthCheckNodeState option of \"cycle\" to cycle through the compute\n    nodes over the course of HealthCheckInterval rather than running all at\n    the same time.\n -- Add job \"reboot\" option for Linux clusters. This invokes the configured\n    RebootProgram to reboot nodes allocated to a job before it begins execution.\n -- Added squeue -O/--Format option that makes all job and step fields available\n    for printing.\n -- Improve database slurmctld entry speed dramatically.\n -- Add \"CPUs\" count to output of \"scontrol show step\".\n -- Add support for lua5.2\n -- scancel -b signals only the batch step neither any other step nor any\n    children of the shell script.\n -- MySQL - enforce NO_ENGINE_SUBSTITUTION\n -- Added CpuFreqDef configuration parameter in slurm.conf to specify the\n    default CPU frequency and governor to be set at job end.\n -- Added support for job email triggers: TIME_LIMIT, TIME_LIMIT_90 (reached\n    90% of time limit), TIME_LIMIT_80 (reached 80% of time limit), and\n    TIME_LIMIT_50 (reached 50% of time limit). Applies to salloc, sbatch and\n    srun commands.\n -- In slurm.conf add the parameter SrunPortRange=min-max. If this is configured\n    then srun will use its dynamic ports only from the configured range.\n -- Make debug_flags 64 bit to handle more flags.\n\n* Changes in Slurm 14.11.0pre1\n==============================\n -- Modify etc/cgroup.release_common.example to set specify full path to the\n    scontrol command. Also find cgroup mount point by reading cgroup.conf file.\n -- Improve qsub wrapper support for passing environment variables.\n -- Modify sdiag to report Slurm RPC traffic by user, type, count and time\n    consumed.\n -- In select plugins, stop triggering extra logging based upon the debug flag\n    CPU_Bind and use SelectType instead.\n -- Added SchedulerParameters options of bf_yield_interval and bf_yield_sleep\n    to control how frequently and for how long the backfill scheduler will\n    relinquish its locks.\n -- To support larger numbers of jobs when the StateSaveDirectory is on a\n    file system that supports a limited number of files in a directory, add a\n    subdirectory called \"hash.#\" based upon the last digit of the job ID.\n -- More gracefully handle missing batch script file. Just kill the job and do\n    not drain the compute node.\n -- Add support for allocation of GRES by model type for heterogenous systems\n    (e.g. request a Kepler GPU, a Tesla GPU, or a GPU of any type).\n -- Record and enable display of nodes anticipated to be used for pending jobs.\n -- Modify squeue --start option to print the nodes expected to be used for\n    pending job (in addition to expected start time, etc.).\n -- Add association hash to the assoc_mgr.\n -- Better logic to handle resized jobs when the DBD is down.\n -- Introduce MemLimitEnforce yes|no in slurm.conf. If set no Slurm will\n    not terminate jobs if they exceed requested memory.\n -- Add support for non-consumable generic resources for resources that are\n    limited, but can be shared between jobs.\n -- Introduce 5 new Slurm errors in slurm_errno.h related to job to better\n    report error conditions.\n -- Modify scontrol to print error message for each array task when updating\n    the entire array.\n -- Added gres_drain and gres_used fields to node_info_t.\n -- Added PriorityParameters configuration parameter in slurm.conf.\n -- Introduce automatic job requeue policy based on exit value. See RequeueExit\n    and RequeueExitHold descriptions in slurm.conf man page.\n -- Modify slurmd to cache launched job IDs for more responsive job suspend and\n    gang scheduling.\n -- Permit jobs steps full control over cpu_bind options if specialized cores\n    are included in the job allocation.\n -- Added ChosLoc configuration parameter to specifiy the pathname of the\n    Chroot OS tool.\n -- Sent SIGCONT/SIGTERM when a job is selected for preemption with GraceTime\n    configured rather than waiting for GraceTime to be reached before notifying\n    the job.\n -- Do not resume a job with specialized cores on a node running another job\n    with specialized cores (only one can run at a time).\n -- Add specialized core count to job suspend/resume calls.\n -- task/affinity and task/cgroup - Correct specialized core task binding with\n    user supplied invalid CPU mask or map.\n -- Add srun --cpu-freq options to set the CPU governor (OnDemand, Performance,\n    PowerSave or UserSpace).\n -- Add support for a job step's CPU governor and/or frequency to be reset on\n    suspend/resume (or gang scheduling). The default for an idle CPU will now\n    be \"ondemand\" rather than \"userspace\" with the lowest frequency (to recover\n    from hard slurmd failures and support gang scheduling).\n -- Added PriorityFlags option of Calulate_Running to continue recalculating\n    the priority of running jobs.\n -- Replace round-robin front-end node selection with least-loaded algorithm.\n -- CRAY - Improve support of XC30 systems when running natively.\n -- Add new node configuration parameters CoreSpecCount, CPUSpecList and\n    MemSpecLimit which support the reservation of resources for system use\n    with Linux cgroup.\n -- Add child_forked() function to the slurm_acct_gather_profile plugin to\n    close open files, leaving application with no extra open file descriptors.\n -- Cray/ALPS system - Enable backup controller to run outside of the Cray to\n    accept new job submissions and most other operations on the pending jobs.\n -- Have sacct print job and task array id's for job arrays.\n -- Smooth out fanout logic\n -- If <sys/prctl.h> is present name major threads in slurmctld, for\n    example backfill\n    thread: slurmctld_bckfl, the rpc manager: slurmctld_rpcmg etc.\n    The name can be seen for example using top -H.\n -- sview - Better job_array support.\n -- Provide more precise error message when job allocation can not be satisfied\n    (e.g. memory, disk, cpu count, etc. rather than just \"node configuration\n    not available\").\n -- Create a new DebugFlags named TraceJobs in slurm.conf to print detailed\n    information about jobs in slurmctld. The information include job ids, state\n    and node count.\n -- When a job dependency can never be satisfied do not cancel the job but keep\n    pending with reason WAIT_DEP_INVALID (DependencyNeverSatisfied).\n\n* Changes in Slurm 14.03.12\n===========================\n -- Make it so previous versions of salloc/srun work with newer versions\n    of Slurm daemons.\n -- PMI2 race condition fix.\n -- Avoid delay on commit for PMI rank 0 to improve performance with some\n    MPI implementations.\n -- Correct the sbatch pbs parser to process -j.\n -- Squeue modified to not merge tasks of a job array if their wait reasons\n    differ.\n -- Use the slurm_getpwuid_r wrapper of getpwuid_r to handle possible\n    interrupts.\n -- Allow --ignore-pbs to take effect when read as an #SBATCH argument.\n -- Do not launch step if job killed while the prolog was running.\n\n* Changes in Slurm 14.03.11\n===========================\n -- ALPS - Fix depth for Memory items in BASIL with CLE 5.2\n    (changed starting in 5.2.3).\n -- ALPS - Fix issue when tracking memory on a PerNode basis instead of\n    PerCPU.\n -- Modify assoc_mgr_fill_in_qos() to allow for a flag to know if the QOS read\n    lock was locked outside of the function or not.\n -- Give even better estimates on pending node count if no node count\n    is requested.\n -- Fix jobcomp/mysql plugin for MariaDB 10+/Mysql 5.6+ to work with reserved\n    work \"partition\".\n -- If requested (scontrol reboot node_name) reboot a node even if it has\n    an maintenance reservation that is not active yet.\n -- Fix issue where exclusive allocations wouldn't lay tasks out correctly\n    with CR_PACK_NODES.\n -- Do not requeue a batch job from slurmd daemon if it is killed while in\n    the process of being launched (a race condition introduced in v14.03.9).\n -- Do not let srun overwrite SLURM_JOB_NUM_NODES if already in an allocation.\n -- Prevent a job's end_time from being too small after a basil reservation\n    error.\n -- Fix sbatch --ntasks-per-core option from setting invalid\n    SLURM_NTASKS_PER_CORE environment value.\n -- Prevent scancel abort when no job satisfies filter options.\n -- ALPS - Fix --ntasks-per-core option on multiple nodes.\n -- Double max string that Slurm can pack from 16MB to 32MB to support\n    larger MPI2 configurations.\n -- Fix Centos5 compile issues.\n -- Log Cray MPI job calling exit() without mpi_fini(), but do not treat it as\n    a fatal error. This partially reverts logic added in version 14.03.9.\n -- sview - Fix displaying of suspended steps elapsed times.\n -- Increase number of messages that get cached before throwing them away\n    when the DBD is down.\n -- Fix jobs from starting in overlapping reservations that won't finish before\n    a \"maint\" reservation begins.\n -- Fix \"squeue --start\" to override SQUEUE_FORMAT env variable.\n -- Restore GRES functionality with select/linear plugin. It was broken in\n    version  14.03.10.\n -- Fix possible race condition when attempting to use QOS on a system running\n    accounting_storage/filetxt.\n -- Sanity check for Correct QOS on startup.\n\n* Changes in Slurm 14.03.10\n===========================\n -- Fix a few sacctmgr error messages.\n -- Treat non-zero SlurmSchedLogLevel without SlurmSchedLogFile as a fatal\n    error.\n -- Correct sched_config.html documentation SchedulingParameters\n    should be SchedulerParameters.\n -- When using gres and cgroup ConstrainDevices set correct access\n    permission for the batch step.\n -- Fix minor memory leak in jobcomp/mysql on slurmctld reconfig.\n -- Fix bug that prevented preservation of a job's GRES bitmap on slurmctld\n    restart or reconfigure (bug was introduced in 14.03.5 \"Clear record of a\n    job's gres when requeued\" and only applies when GRES mapped to specific\n    files).\n -- BGQ: Fix race condition when job fails due to hardware failure and is\n    requeued. Previous code could result in slurmctld abort with NULL pointer.\n -- Prevent negative job array index, which could cause slurmctld to crash.\n -- Fix issue with squeue/scontrol showing correct node_cnt when only tasks\n    are specified.\n -- Check the status of the database connection before using it.\n -- ALPS - If an allocation requests -n set the BASIL -N option to the\n    amount of tasks / number of node.\n -- ALPS - Don't set the env var APRUN_DEFAULT_MEMORY, it is not needed anymore.\n -- Fix potential buffer overflow.\n -- Give better estimates on pending node count if no node count is requested.\n -- BLUEGENE - Fix issue where requeuing jobs could cause an assert.\n\n* Changes in Slurm 14.03.9\n==========================\n -- If slurmd fails to stat(2) the configuration print the string describing\n    the error code.\n -- Fix for mixing core base reservations with whole node based reservations\n    to avoid overlapping erroneously.\n -- BLUEGENE - Remove references to Base Partition.\n -- sview - If compiled on a non-bluegene system then used to view a BGQ fix\n    to allow sview to display blocks correctly.\n -- Fix bug in update reservation. When modifying the reservation the end time\n    was set incorrectly.\n -- The start time of a reservation that is in ACTIVE state cannot be modified.\n -- Update the cgroup documentation about release agent for devices.\n -- MYSQL - fix for setting up preempt list on a QOS for multiple QOS.\n -- Correct a minor error in the scancel.1 man page related to the\n    --signal option.\n -- Enhance the scancel.1 man page to document the sequence of signals sent\n -- Fix slurmstepd core dump if the cgroup hierarchy is not completed\n    when terminating the job.\n -- Fix hostlist_shift to be able to give correct node names on names with a\n    different number of dimensions than the cluster.\n -- BLUEGENE - Fix invalid pointer in corner case in the plugin.\n -- Make sure on a reconfigure the select information for a node is preserved.\n -- Correct logic to support job GRES specification over 31 bits (problem\n    in logic converting int to uint32_t).\n -- Remove logic that was creating GRES bitmap for node when not needed (only\n    needed when GRES mapped to specific files).\n -- BLUEGENE - Fix sinfo -tr before it would only print idle nodes correctly.\n -- BLUEGENE - Fix for licenses_only reservation on bluegene systems.\n -- sview - Verify pointer before using strchr.\n -- -M option on tools talking to a Cray from a non-Cray fixed.\n -- CRAY - Fix rpmbuild issue for missing file slurm.conf.template.\n -- Fix race condition when dealing with removing many associations at\n    different times when reservations are using the associations that are\n    being deleted.\n -- When a node's state is set to power_down/power_up, then execute\n    SuspendProgram/ResumeProgram even if previously executed for that node.\n -- Fix logic determining when job configuration (i.e. running node power up\n    logic) is complete.\n -- Setting the state of a node in powered down state node to \"resume\" will\n    no longer cause it to reboot, but only clear the \"drain\" state flag.\n -- Fix srun documentation to remove SLURM_NODELIST being equivalent as the -w\n    option (since it isn't).\n -- Fix issue with --hint=nomultithread and allocations with steps running\n    arbitrary layouts (test1.59).\n -- PrivateData=reservation modified to permit users to view the reservations\n    which they have access to (rather then preventing them from seeing ANY\n    reservation).  Backport from 14.11 commit 77c2bd25c.\n -- Fix PrivateData=reservation when using associations to give privileges to\n    a reservation.\n -- Better checking to see if select plugin is linear or not.\n -- Add support for time specification of \"fika\" (3 PM).\n -- Standardize qstat wrapper more.\n -- Provide better estimate of minimum node count for pending jobs using more\n    job parameters.\n -- ALPS - Add SubAllocate to cray.conf file for those who like the way <=2.5\n    did the ALPS reservation.\n -- Safer check to avoid invalid reads when shutting down the slurmctld with\n    lots of jobs.\n -- Fix minor memory leak in the backfill scheduler when shutting down.\n -- Add ArchiveResvs to the output of sacctmgr show config and init the variable\n    on slurmdbd startup.\n -- SLURMDBD - Only set the archive flag if purging the object\n    (i.e ArchiveJobs PurgeJobs).  This is only a cosmetic change.\n -- Fix for job step memory allocation logic if step requests GRES and memory\n    is not allocations are not managed.\n -- Fix sinfo to display mixed nodes as allocated in '%F' output.\n -- Sview - Fix cpu and node counts for partitions.\n -- Ignore NO_VAL in SLURMDB_PURGE_* macros.\n -- ALPS - Don't drain nodes if epilog fails.  It leaves them in drain state\n    with no way to get them out.\n -- Fix issue with task/affinity oversubscribing cpus erroneously when\n    using --ntasks-per-node.\n -- MYSQL - Fix load of archive files.\n -- Treat Cray MPI job calling exit() without mpi_fini() as fatal error for\n    that specific task and let srun handle all timeout logic.\n -- Fix small memory leak in jobcomp/mysql.\n -- Correct tracking of licenses for suspended jobs on slurmctld reconfigure or\n    restart.\n -- If failed to launch a batch job requeue it in hold.\n\n* Changes in Slurm 14.03.8\n==========================\n -- Fix minor memory leak when Job doesn't have nodes on it (Meaning the job\n    has finished)\n -- Fix sinfo/sview to be able to query against nodes in reserved and other\n    states.\n -- Make sbatch/salloc read in (SLURM|(SBATCH|SALLOC))_HINT in order to\n    handle sruns in the script that will use it.\n -- srun properly interprets a leading \".\" in the executable name based upon\n    the working directory of the compute node rather than the submit host.\n -- Fix Lustre misspellings in hdf5 guide\n -- Fix wrong reference in slurm.conf man page to what --profile option should\n    be used for AcctGatherFilesystemType.\n -- Update HDF5 document to point out the SlurmdUser is who creates the\n    ProfileHDF5Dir directory as well as all it's sub-directories and files.\n -- CRAY NATIVE - Remove error message for srun's ran inside an salloc that\n    had --network= specified.\n -- Defer job step initiation of required GRES are in use by other steps rather\n    than immediately returning an error.\n -- Deprecate --cpu_bind from sbatch and salloc.  These never worked correctly\n    and only caused confusion since the cpu_bind options mostly refer to a\n    step we opted to only allow srun to set them in future versions.\n -- Modify sgather to work if Nodename and NodeHostname differ.\n -- Changed use of JobContainerPlugin where it should be JobContainerType.\n -- Fix for possible error if job has GRES, but the step explicitly requests a\n    GRES count of zero.\n -- Make \"srun --gres=none ...\" work when executed without a job allocation.\n -- Change the global eio_shutdown_time to a field in eio handle.\n -- Advanced reservation fixes for heterogeneous systems, especially when\n    reserving cores.\n -- If --hint=nomultithread is used in a job allocation make sure any srun's\n    ran inside the allocation can read the environment correctly.\n -- If batchdir can't be made set errno correctly so the slurmctld is notified\n    correctly.\n -- Remove repeated batch complete if batch directory isn't able to be made\n    since the slurmd will send the same message.\n -- sacctmgr fix default format for list transactions.\n -- BLUEGENE - Fix backfill issue with backfilling jobs on blocks already\n    reserved for higher priority jobs.\n -- When creating job arrays the job specification files for each elements\n    are hard links to the first element specification files. If the controller\n    fails to make the links the files are copied instead.\n -- Fix error handling for job array create failure due to inability to copy\n    job files (script and environment).\n -- Added patch in the contribs directory for integrating make version 4.0 with\n    Slurm and renamed the previous patch \"make-3.81.slurm.patch\".\n -- Don't wait for an update message from the DBD to finish before sending rc\n    message back.  In slow systems with many associations this could speed\n    responsiveness in sacctmgr after adding associations.\n -- Eliminate race condition in enforcement of MaxJobCount limit for job arrays.\n -- Fix anomaly allocating cores for GRES with specific device/CPU mapping.\n -- cons_res - When requesting exclusive access make sure we set the number\n    of cpus in the job_resources_t structure so as nodes finish the correct\n    cpu count is displayed in the user tools.\n -- If the job_submit plugin calls take longer than 1 second to run, print a\n    warning.\n -- Make sure transfer_s_p_options transfers all the portions of the\n    s_p_options_t struct.\n -- Correct the srun man page, the SLURM_CPU_BIND_VERBOSE, SLURM_CPU_BIND_TYPE\n    SLURM_CPU_BIND_LIST environment variable are set only when task/affinity\n    plugin is configured.\n -- sacct - Initialize variables correctly to avoid incorrect structure\n    reference.\n -- Performance adjustment to avoid calling a function multiple times when it\n    only needs to be called once.\n -- Give more correct waiting reason if job is waiting on association/QOS\n    MaxNode limit.\n -- DB - When sending lft updates to the slurmctld only send non-deleted lfts.\n -- BLUEGENE - Fix documentation on how to build a reservation less than\n    a midplane.\n -- If Slurmctld fails to read the job environment consider it an error\n    and abort the job.\n -- Add the name of the node a job is running on to the message printed by\n    slurmstepd when terminating a job.\n -- Remove unsupported options from sacctmgr help and the dump function.\n -- Update sacctmgr man page removing reference to obsolete parameter\n    MaxProcSecondsPerJob.\n -- Added more validity checking of incoming job submit requests.\n\n* Changes in Slurm 14.03.7\n==========================\n -- Correct typos in man pages.\n -- Add note to MaxNodesPerUser and multiple jobs running on the same node\n    counting as multiple nodes.\n -- PerlAPI - fix renamed call from slurm_api_set_conf_file to\n    slurm_conf_reinit.\n -- Fix gres race condition that could result in job deallocation error message.\n -- Correct NumCPUs count for jobs with --exclusive option.\n -- When creating reservation with CoreCnt, check that Slurm uses\n    SelectType=select/cons_res, otherwise don't send the request to slurmctld\n    and return an error.\n -- Save the state of scheduled node reboots so they will not be lost should the\n    slurmctld restart.\n -- In select/cons_res plugin - Insure the node count does not exceed the task\n    count.\n -- switch/nrt - Do not explicitly unload windows for a job on termination,\n    only unload its table (which automatically unloads its windows).\n -- When HealthCheckNodeState is configured as IDLE don't run the\n    HealthCheckProgram for nodes in any other states than IDLE.\n -- Remove all slurmctld locks prior to job_submit() being called in plugins.\n    If any slurmctld data structures are read or modified, add locks directly\n    in the plugin.\n -- Minor sanity check to verify the string sent in isn't NULL when using\n    bit_unfmt.\n -- CRAY NATIVE - Fix issue on heavy systems to only run the NHC once per\n    job/step completion.\n -- Remove unneeded step cleanup for pending steps.\n -- Fix issue where if a batch job was manually requeued the batch step\n    information wasn't stored in accounting.\n -- When job is release from a requeue hold state clean up its previous\n    exit code.\n -- Correct the srun man page about how the output from the user application\n    is sent to srun.\n -- Increase the timeout of the main thread while waiting for the i/o thread.\n    Allow up to 180 seconds for the i/o thread to complete.\n -- When using sacct -c to read the job completion data compute the correct\n    job elapsed time.\n -- Perl package: Define some missing node states.\n -- When using AccountingStorageType=accounting_storage/mysql zero out the\n    database index for the array elements avoiding duplicate database values.\n -- Reword the explanation of cputime and cputimeraw in the sacct man page.\n -- JobCompType allows \"jobcomp/mysql\" as valid name but the code used\n    \"job_comp/mysql\" setting an incorrect default database.\n -- Try to load libslurm.so only when necessary.\n -- When nodes scheduled for reboot, set state to DOWN rather than FUTURE so\n    they are still visible to sinfo. State set to IDLE after reboot completes.\n -- Apply BatchStartTimeout configuration to task launch and avoid aborting\n    srun commands due to long running Prolog scripts.\n -- Fix minor memory leaks when freeing node_info_t structure.\n -- Fix various memory leaks in sview\n -- If a batch script is requeued and running steps get correct exit code/signal\n    previous it was always -2.\n -- If step exitcode hasn't been set display with sacct the -2 instead\n    of acting like it is a signal and exitcode.\n -- Send calculated step_rc for batch step instead of raw status as\n    done for normal steps.\n -- If a job times out, set the exit code in accounting to 1 instead of the\n    signal 1.\n -- Update the acct_gather.conf.5 man page removing the reference to\n    InfinibandOFEDFrequency.\n -- Fix gang scheduling for jobs submitted to multiple partitions.\n -- Enable srun to submit job to multiple partitions.\n -- Update slurm.conf man page. When Epilog or Prolog fail the node state\n    is set ro DRAIN.\n -- Start a job in the highest priority partition possible, even if it requires\n    preempting other jobs and delaying initiation, rather than using a lower\n    priority partition. Previous logic would preempt lower priority jobs, but\n    then might start the job in a lower priority partition and not use the\n    resources released by the preempted jobs.\n -- Fix SelectTypeParameters=CR_PACK_NODES for srun making both job and step\n    resource allocation.\n -- BGQ - Make it possible to pack multiple tasks on a core when not using\n    the entire cnode.\n -- MYSQL - if unable to connect to mysqld close connection that was inited.\n -- DBD - when connecting make sure we wait MessageTimeout + 5 since the\n    timeout when talking to the Database is the same timeout so a race\n    condition could occur in the requesting client when receiving the response\n    if the database is unresponsive.\n\n* Changes in Slurm 14.03.6\n==========================\n -- Added examples to demonstrate the use of the sacct -T option to the man\n    page.\n -- Fix for regression in 14.03.5 with sacctmgr load when Parent has \"'\"\n    around it.\n -- Update comments in sacctmgr dump header.\n -- Fix for possible abort on change in GRES configuration.\n -- CRAY - fix modules file, (backport from 14.11 commit 78fe86192b.\n -- Fix race condition which could result in requeue if batch job exit and node\n    registration occur at the same time.\n -- switch/nrt - Unload job tables (in addition to windows) in user space mode.\n -- Differentiate between two identical debug messages about purging vestigial\n    job scripts.\n -- If the socket used by slurmstepd to communicate with slurmd exist when\n    slurmstepd attempts to create it, for example left over from a previous\n    requeue or crash, delete it and recreate it.\n\n* Changes in Slurm 14.03.5\n==========================\n -- If a srun runs in an exclusive allocation and doesn't use the entire\n    allocation and CR_PACK_NODES is set layout tasks appropriately.\n -- Correct Shared field in job state information seen by scontrol, sview, etc.\n -- Print Slurm error string in scontrol update job and reset the Slurm errno\n    before each call to the API.\n -- Fix task/cgroup to handle -mblock:fcyclic correctly\n -- Fix for core-based advanced reservations where the distribution of cores\n    across nodes is not even.\n -- Fix issue where association maxnodes wouldn't be evaluated correctly if a\n    QOS had a GrpNodes set.\n -- GRES fix with multiple files defined per line in gres.conf.\n -- When a job is requeued make sure accounting marks it as such.\n -- Print the state of requeued job as REQUEUED.\n -- Fix if a job's partition was taken away from it don't allow a requeue.\n -- Make sure we lock on the conf when sending slurmd's conf to the slurmstepd.\n -- Fix issue with sacctmgr 'load' not able to gracefully handle bad formatted\n    file.\n -- sched/backfill: Correct job start time estimate with advanced reservations.\n -- Error message added when in proctrack/cgroup the step freezer path isn't\n    able to be destroyed for debug.\n -- Added extra index's into the database for better performance when\n    deleting users.\n -- Fix issue with wckeys when tracking wckeys, but not enforcing them,\n    you could get multiple '*' wckeys.\n -- Fix bug which could report to squeue the wrong partition for a running job\n    that is submitted to multiple partitions.\n -- Report correct CPU count allocated to job when allocated whole node even if\n    not using all CPUs.\n -- If job's constraints cannot be satisfied put it in pending state with reason\n    BadConstraints and don't remove it.\n -- sched/backfill - If job started with infinite time limit, set its end_time\n    one year in the future.\n -- Clear record of a job's gres when requeued.\n -- Clear QOS GrpUsedCPUs when resetting raw usage if QOS is not using any cpus.\n -- Remove log message left over from debugging.\n -- When using CR_PACK_NODES fix make --ntasks-per-node work correctly.\n -- Report correct partition associated with a step if the job is submitted to\n    multiple partitions.\n -- Fix to allow removing of preemption from a QOS\n -- If the proctrack plugins fail to destroy the job container print an error\n    message and avoid to loop forever, give up after 120 seconds.\n -- Make srun obey POSIX convention and increase the exit code by 128 when the\n    process terminated by a signal.\n -- Sanity check for acct_gather_energy/rapl\n -- If the proctrack plugins fail to destroy the job container print an error\n    message and avoid to loop forever, give up after 120 seconds.\n -- If the sbatch command specifies the option --signal=B:signum sent the signal\n    to the batch script only.\n -- If we cancel a task and we have no other exit code send the signal and\n    exit code.\n -- Added note about InnoDB storage engine being used with MySQL.\n -- Set the job exit code when the job is signaled and set the log level to\n    debug2() when processing an already completed job.\n -- Reset diagnostics time stamp when \"sdiag --reset\" is called.\n -- squeue and scontrol to report a job's \"shared\" value based upon partition\n    options rather than reporting \"unknown\" if job submission does not use\n    --exclusive or --shared option.\n -- task/cgroup - Fix cpuset binding for batch script.\n -- sched/backfill - Fix anomaly that could result in jobs being scheduled out\n    of order.\n -- Expand pseudo-terminal size data structure field sizes from 8 to 16 bits.\n -- Set the job exit code when the job is signaled and set the log level to\n    debug2() when processing an already completed job.\n -- Distinguish between two identical error messages.\n -- If using accounting_storage/mysql directly without a DBD fix issue with\n    start of requeued jobs.\n -- If a job fails because of batch node failure and the job is requeued and an\n    epilog complete message comes from that node do not process the batch step\n    information since the job has already been requeued because the epilog\n    script running isn't guaranteed in this situation.\n -- Change message to note a NO_VAL for return code could of come from node\n    failure as well as interactive user.\n -- Modify test4.5 to only look at one partition instead of all of them.\n -- Fix sh5util -u to accept username different from the user that runs the\n    command.\n -- Corrections to man pages:salloc.1 sbatch.1 srun.1 nonstop.conf.5\n    slurm.conf.5.\n -- Restore srun --pty resize ability.\n -- Have sacctmgr dump cluster handle situations where users or such have\n    special characters in their names like ':'\n -- Add more debugging for information should the job ran on wrong node\n    and should there be problems accessing the state files.\n\n* Changes in Slurm 14.03.4\n==========================\n -- Fix issue where not enforcing QOS but a partition either allows or denies\n    them.\n -- CRAY - Make switch/cray default when running on a Cray natively.\n -- CRAY - Make job_container/cncu default when running on a Cray natively.\n -- Disable job time limit change if it's preemption is in progress.\n -- Correct logic to properly enforce job preemption GraceTime.\n -- Fix sinfo -R to print each down/drained node once, rather than once per\n    partition.\n -- If a job has non-responding node, retry job step create rather than\n    returning with DOWN node error.\n -- Support SLURM_CONF path which does not have \"slurm.conf\" as the file name.\n -- CRAY - make job_container/cncu default when running on a Cray natively\n -- Fix issue where batch cpuset wasn't looked at correctly in\n    jobacct_gather/cgroup.\n -- Correct squeue's job node and CPU counts for requeued jobs.\n -- Correct SelectTypeParameters=CR_LLN with job selecition of specific nodes.\n -- Only if ALL of their partitions are hidden will a job be hidden by default.\n -- Run EpilogSlurmctld for a job is killed during slurmctld reconfiguration.\n -- Close window with srun if waiting for an allocation and while printing\n    something you also get a signal which would produce deadlock.\n -- Add SelectTypeParameters option of CR_PACK_NODES to pack a job's tasks\n    tightly on its allocated nodes rather than distributing them evenly across\n    the allocated nodes.\n -- cpus-per-task support: Try to pack all CPUs of each tasks onto one socket.\n    Previous logic could spread the tasks CPUs across multiple sockets.\n -- Add new distribution method fcyclic so when a task is using multiple cpus\n    it can bind cyclically across sockets.\n -- task/affinity - When using --hint=nomultithread only bind to the first\n    thread in a core.\n -- Make cgroup task layout (block | cyclic) method mirror that of\n    task/affinity.\n -- If TaskProlog sets SLURM_PROLOG_CPU_MASK reset affinity for that task\n    based on the mask given.\n -- Keep supporting 'srun -N x --pty bash' for historical reasons.\n -- If EnforcePartLimits=Yes and QOS job is using can override limits, allow\n    it.\n -- Fix issues if partition allows or denies account's or QOS' and either are\n    not set.\n -- If a job requests a partition and it doesn't allow a QOS or account the\n    job is requesting pend unless EnforcePartLimits=Yes.  Before it would\n    always kill the job at submit.\n -- Fix format output of scontrol command when printing node state.\n -- Improve the clean up of cgroup hierarchy when using the\n    jobacct_gather/cgroup plugin.\n -- Added SchedulerParameters value of Ignore_NUMA.\n -- Fix issues with code when using automake 1.14.1\n -- select/cons_res plugin: Fix memory leak related to job preemption.\n -- After reconfig rebuild the job node counters only for jobs that have\n    not finished yet, otherwise if requeued the job may enter an invalid\n    COMPLETING state.\n -- Do not purge the script and environment files for completed jobs on\n    slurmctld reconfiguration or restart (they might be later requeued).\n -- scontrol now accepts the option job=xxx or jobid=xxx for the requeue,\n    requeuehold and release operations.\n -- task/cgroup - fix to bind batch job in the proper CPUs.\n -- Added strigger option of -N, --noheader to not print the header when\n    displaying a list of triggers.\n -- Modify strigger to accept arguments to the program to execute when an\n    event trigger occurs.\n -- Attempt to create duplicate event trigger now generates ESLURM_TRIGGER_DUP\n    (\"Duplicate event trigger\").\n -- Treat special characters like %A, %s etc. literally in the file names\n    when specified escaped e.g. sbatch -o /home/zebra\\\\%s will not expand\n    %s as the stepid of the running job.\n -- CRAYALPS - Add better support for CLE 5.2 when running Slurm over ALPS.\n -- Test time when job_state file was written to detect multiple primary\n    slurmctld daemons (e.g. both backup and primary are functioning as\n    primary and there is a split brain problem).\n -- Fix scontrol to accept update jobid=# numtasks=#\n -- If the backup slurmctld assumes primary status, then do NOT purge any\n    job state files (batch script and environment files) and do not re-use them.\n    This may indicate that multiple primary slurmctld daemons are active (e.g.\n    both backup and primary are functioning as primary and there is a split\n    brain problem).\n -- Set correct error code when requeuing a completing/pending job\n -- When checking for if dependency of type afterany, afterok and afternotok\n    don't clear the dependency if the job is completing.\n -- Cleanup the JOB_COMPLETING flag and eventually requeue the job when the\n    last epilog completes, either slurmd epilog or slurmctld epilog, whichever\n    comes last.\n -- When attempting to requeue a job distinguish the case in which the job is\n    JOB_COMPLETING or already pending.\n -- When reconfiguring the controller don't restart the slurmctld epilog if it\n    is already running.\n -- Email messages for job array events print now use the job ID using the\n    format \"#_# (#)\" rather than just the internal job ID.\n -- Set the number of free licenses to be 0 if the global license count\n    decreases and total is less than in use.\n -- Add DebugFlag of BackfillMap. Previously a DebugFlag value of Backfill\n    logged information about what it was doing plus a map of expected resouce\n    use in the future. Now that very verbose resource use map is only logged\n    with a DebugFlag value of BackfillMap\n -- Fix slurmstepd core dump.\n -- Modify the description of -E and -S option of sacct command as point in time\n    'before' or 'after' the database records are returned.\n -- Correct support for partition with Shared=YES configuration.\n -- If job requests --exclusive then do not use nodes which have any cores in an\n    advanced reservation. Also prevents case where nodes can be shared by other\n    jobs.\n -- For \"scontrol --details show job\" report the correct CPU_IDs when thre are\n    multiple threads per core (we are translating a core bitmap to CPU IDs).\n -- If DebugFlags=Protocol is configured in slurm.conf print details of the\n    connection, ip address and port accepted by the controller.\n -- Fix minor memory leak when reading in incomplete node data checkpoint file.\n -- Enlarge the width specifier when printing partition SHARE to display larger\n    sharing values.\n -- sinfo locks added to prevent possibly duplicate record printing for\n    resources in multiple partitions.\n\n* Changes in Slurm 14.03.3-2\n============================\n -- BGQ - Fix issue with uninitialized variable.\n\n* Changes in Slurm 14.03.3\n==========================\n -- Correction to default batch output file name. In version 14.03.2 was using\n    \"slurm_<jobid>_4294967294.out\" due to error in job array logic.\n -- In slurm.spec file, replace \"Requires cray-MySQL-devel-enterprise\" with\n    \"Requires mysql-devel\".\n\n* Changes in Slurm 14.03.2\n==========================\n -- Fix race condition if PrologFlags=Alloc,NoHold is used.\n -- Cray - Make NPC only limit running other NPC jobs on shared blades instead\n    of limited non NPC jobs.\n -- Fix for sbatch #PBS -m (mail) option parsing.\n -- Fix job dependency bug. Jobs dependent upon multiple other jobs may start\n    prematurely.\n -- Set \"Reason\" field for all elements of a job array on short-circuited\n    scheduling for job arrays.\n -- Allow -D option of salloc/srun/sbatch to specify relative path.\n -- Added SchedulerParameter of batch_sched_delay to permit many batch jobs\n    to be submitted between each scheduling attempt to reduce overhead of\n    scheduling logic.\n -- Added job reason of \"SchedTimeout\" if the scheduler was not able to reach\n    the job to attempt scheduling it.\n -- Add job's exit state and exit code to email message.\n -- scontrol hold/release accepts job name option (in addition to job ID).\n -- Handle when trying to cancel a step that hasn't started yet better.\n -- Handle Max/GrpCPU limits better\n -- Add --priority option to salloc, sbatch and srun commands.\n -- Honor partition priorities over job priorities.\n -- Fix sacct -c when using jobcomp/filetxt to read newer variables\n -- Fix segfault of sacct -c if spaces are in the variables.\n -- Release held job only with \"scontrol release <jobid>\" and not by resetting\n    the job's priority. This is needed to support job arrays better.\n -- Correct squeue command not to merge jobs with state pending and completing\n    together.\n -- Fix issue where user is requesting --acctg-freq=0 and no memory limits.\n -- Fix issue with GrpCPURunMins if a job's timelimit is altered while the job\n    is running.\n -- Temporary fix for handling our typemap for the perl api with newer perl.\n -- Fix allowgroup on bad group seg fault with the controller.\n -- Handle node ranges better when dealing with accounting max node limits.\n\n* Changes in Slurm 14.03.1-2\n==========================\n -- Update configure to set correct version without having to run autogen.sh\n\n* Changes in Slurm 14.03.1\n==========================\n -- Add support for job std_in, std_out and std_err fields in Perl API.\n -- Add \"Scheduling Configuration Guide\" web page.\n -- BGQ - fix check for jobinfo when it is NULL\n -- Do not check cleaning on \"pending\" steps.\n -- task/cgroup plugin - Fix for building on older hwloc (v1.0.2).\n -- In the PMI implementation by default don't check for duplicate keys.\n    Set the SLURM_PMI_KVS_DUP_KEYS if you want the code to check for\n    duplicate keys.\n -- Add job submission time to squeue.\n -- Permit user root to propagate resource limits higher than the hard limit\n    slurmd has on that compute node has (i.e. raise both current and maximum\n    limits).\n -- Fix issue with license used count when doing an scontrol reconfig.\n -- Fix the PMI iterator to not report duplicated keys.\n -- Fix issue with sinfo when -o is used without the %P option.\n -- Rather than immediately invoking an execution of the scheduling logic on\n    every event type that can enable the execution of a new job, queue its\n    execution. This permits faster execution of some operations, such as\n    modifying large counts of jobs, by executing the scheduling logic less\n    frequently, but still in a timely fashion.\n -- If the environment variable is greater than MAX_ENV_STRLEN don't\n    set it in the job env otherwise the exec() fails.\n -- Optimize scontrol hold/release logic for job arrays.\n -- Modify srun to report an exit code of zero rather than nine if some tasks\n    exit with a return code of zero and others are killed with SIGKILL. Only an\n    exit code of zero did this.\n -- Fix a typo in scontrol man page.\n -- Avoid slurmctld crash getting job info if detail_ptr is NULL.\n -- Fix sacctmgr add user where both defaultaccount and accounts are specified.\n -- Added SchedulerParameters option of max_sched_time to limit how long the\n    main scheduling loop can execute for.\n -- Added SchedulerParameters option of sched_interval to control how frequently\n    the main scheduling loop will execute.\n -- Move start time of main scheduling loop timeout after locks are aquired.\n -- Add squeue job format option of \"%y\" to print a job's nice value.\n -- Update scontrol update jobID logic to operate on entire job arrays.\n -- Fix PrologFlags=Alloc to run the prolog on each of the nodes in the\n    allocation instead of just the first.\n -- Fix race condition if a step is starting while the slurmd is being\n    restarted.\n -- Make sure a job's prolog has ran before starting a step.\n -- BGQ - Fix invalid memory read when using DefaultConnType in the\n    bluegene.conf\n -- Make sure we send node state to the DBD on clean start of controller.\n -- Fix some sinfo and squeue sorting anomalies due to differences in data\n    types.\n -- Only send message back to slurmctld when PrologFlags=Alloc is used on a\n    Cray/ALPS system, otherwise use the slurmd to wait on the prolog to gate\n    the start of the step.\n -- Remove need to check PrologFlags=Alloc in slurmd since we can tell if prolog\n    has ran yet or not.\n -- Fix squeue to use a correct macro to check job state.\n -- BGQ - Fix incorrect logic issues if MaxBlockInError=0 in the bluegene.conf.\n -- priority/basic - Insure job priorities continue to decrease when jobs are\n    submitted with the --nice option.\n -- Make the PrologFlag=Alloc work on batch scripts\n -- Make PrologFlag=NoHold (automatically sets PrologFlag=Alloc) not hold in\n    salloc/srun, instead wait in the slurmd when a step hits a node and the\n    prolog is still running.\n -- Added --cpu-freq=highm1 (high minus one) option.\n -- Expand StdIn/Out/Err string length output by \"scontrol show job\" from 128\n    to 1024 bytes.\n -- squeue %F format will now print the job ID for non-array jobs.\n -- Use quicksort for all priority based job sorting, which improves performance\n    significantly with large job counts.\n -- If a job has already been released from a held state ignore successive\n    release requests.\n -- Fix srun/salloc/sbatch man pages for the --no-kill option.\n -- Add squeue -L/--licenses option to filter jobs by license names.\n -- Handle abort job on node on front end systems without core dumping.\n -- Fix dependency support for job arrays.\n -- When updating jobs verify the update request is not identical to\n    the current settings.\n -- When sorting jobs and priorities are equal sort by job_id.\n -- Do not overwrite existing reason for node being down or drained.\n -- Requeue batch job if Munge is down and credential can not be created.\n -- Make _slurm_init_msg_engine() tolerate bug in bind() returning a busy\n    ephemeral port.\n -- Don't block scheduling of entire job array if it could run in multiple\n    partitions.\n -- Introduce a new debug flag Protocol to print protocol requests received\n    together with the remote IP address and port.\n -- CRAY - Set up the network even when only using 1 node.\n -- CRAY - Greatly reduce the number of error messages produced from the task\n    plugin and provide more information in the message.\n\n* Changes in Slurm 14.03.0\n==========================\n -- job_submit/lua: Fix invalid memory reference if script returns error message\n    for user.\n -- Add logic to sleep and retry if slurm.conf can't be read.\n -- Reset a node's CpuLoad value at least once each SlurmdTimeout seconds.\n -- Scheduler enhancements for reservations: When a job needs to run in\n    reservation, but can not due to busy resources, then do not block all jobs\n    in that partition from being scheduled, but only the jobs in that\n    reservation.\n -- Export \"SLURM*\" environment variables from sbatch even if --export=NONE.\n -- When recovering node state if the Slurm version is 2.6 or 2.5 set the\n    protocol version to be SLURM_2_5_PROTOCOL_VERSION which is the minimum\n    supported version.\n -- Update the scancel man page documenting the -s option.\n -- Update sacctmgr man page documenting how to modify account's QOS.\n -- Fix for sjstat which currently does not print >1TB memory values correctly.\n -- Change xmalloc()/xfree() to malloc()/free() in hostlist.c for better\n    performance.\n -- Update squeue.1 man page describing the SPECIAL_EXIT state.\n -- Added scontrol option of errnumstr to return error message given a slurm\n    error number.\n -- If srun invoked with the --multi-prog option, but no task count, then use\n    the task count provided in the MPMD configuration file.\n -- Prevent sview abort on some systems when adding or removing columns to the\n    display for nodes, jobs, partitions, etc.\n -- Add job array hash table for improved performance.\n -- Make AccountingStorageEnforce=all not include nojobs or nosteps.\n -- Added sacctmgr mod qos set RawUsage=0.\n -- Modify hostlist functions to accept more than two numeric ranges (e.g.\n    \"row[1-3]rack[0-8]slot[0-63]\")\n\n* Changes in Slurm 14.03.0rc1\n==============================\n -- Fixed typos in srun_cr man page.\n -- Run job scheduling logic immediately when nodes enter service.\n -- Added sbatch '--parsable' option to output only the job id number and the\n    cluster name separated by a semicolon. Errors will still be displayed.\n -- Added failure management \"slurmctld/nonstop\" plugin.\n -- Prevent jobs being killed when a checkpoint plugin is enabled or disabled.\n -- Update the documentation about SLURM_PMI_KVS_NO_DUP_KEYS environment\n    variable.\n -- select/cons_res bug fix for range of node counts with --cpus-per-task\n    option (e.g. \"srun -N2-3 -c2 hostname\" would allocate 2 CPUs on the first\n    node and 0 CPUs on the second node).\n -- Change reservation flags field from 16 to 32-bits.\n -- Add reservation flag value of \"FIRST_CORES\".\n -- Added the idea of Resources to the database.  Framework for handling\n    license servers outside of Slurm.\n -- When starting the slurmctld only send past job/node state information to\n    accounting if running for the first time (should speed up startup\n    dramatically on systems with lots of nodes or lots of jobs).\n -- Compile and run on FreeBSD 8.4.\n -- Make job array expressions more flexible to accept multiple step counts in\n    the expression (e.g. \"--array=1-10:2,50-60:5,123\").\n -- switch/cray - add state save/restore logic tracking allocated ports.\n -- SchedulerParameters - Replace max_job_bf with bf_max_job_start (both will\n    work for now).\n -- Add SchedulerParameters options of preempt_reorder_count and\n    preempt_strict_order.\n -- Make memory types in acct_gather uint64_t to handle systems with more than\n    4TB of memory on them.\n -- BGQ - --export=NONE option for srun to make it so only the SLURM_JOB_ID\n    and SLURM_STEP_ID env vars are set.\n -- Munge plugins - Add sleep between retries if can't connect to socket.\n -- Added DebugFlags value of \"License\".\n -- Added --enable-developer which will give you -Werror when compiling.\n -- Fix for job request with GRES count of zero.\n -- Fix a potential memory leak in hostlist.\n -- Job array dependency logic: Cache results for major performance improvement.\n -- Modify squeue to support filter on job states Special_Exit and Resizing.\n -- Defer purging job record until after EpilogSlurmctld completes.\n -- Add -j option for jobid to sbcast.\n -- Fix handling RPCs from a 14.03 slurmctld to a 2.6 slurmd\n\n* Changes in Slurm 14.03.0pre6\n==============================\n -- Modify slurmstepd to log messages according to the LogTimeFormat\n    parameter in slurm.conf.\n -- Insure that overlapping reservations do not oversubscribe available\n    licenses.\n -- Added core specialization logic to select/cons_res plugin.\n -- Added whole_node field to job_resources structure and enable gang scheduling\n    for jobs with core specialization.\n -- When using FastSchedule = 1 the nodes with less than configured resources\n    are not longer set DOWN, they are set to DRAIN instead.\n -- Modified 'sacctmgr show associations' command to show GrpCPURunMins\n    by default.\n -- Replace the hostlist_push() function with a more efficient\n    hostlist_push_host().\n -- Modify the reading of lustre file system statistics to print more\n    information when debug and when io error occur.\n -- Add specialized core count field to job credential data.\n    NOTE: This changes the communications protocol from other pre-releases of\n    version 14.03. All programs must be cancelled and daemons upgraded from\n    previous pre-releases of version 14.03. Upgrades from version 2.6 or earlier\n    can take place without loss of jobs\n -- Add version number to node and front-end configuration information visible\n    using the scontrol tool.\n -- Add idea of a RESERVED flag for node state so idle resources are marked\n    not \"idle\" when in a reservation.\n -- Added core specialization plugin infrastructure.\n -- Added new job_submit/trottle plugin to control the rate at which a user\n    can submit jobs.\n -- CRAY - added network performance counters option.\n -- Allow scontrol suspend/resume to accept jobid in the format jobid_taskid\n    to suspend/resume array elements.\n -- In the slurmctld job record, split \"shared\" variable into \"share_res\" (share\n    resource) and \"whole_node\" fields.\n -- Fix the format of SLURM_STEP_RESV_PORTS. It was generated incorrectly\n    when using the hostlist_push_host function and input surrounded by [].\n -- Modify the srun --slurmd-debug option to accept debug string tags\n    (quiet, fatal, error, info verbose) beside the numerical values.\n -- Fix the bug where --cpu_bind=map_cpu is interpreted as mask_cpu.\n -- Update the documentation egarding the state of cpu frequencies after\n    a step using --cpu-freq completes.\n -- CRAY - Fix issue when a job is requeued and nhc is still running as it is\n    being scheduled to run again.  This would erase the previous job info\n    that was still needed to clean up the nodes from the previous job run.\n    (Bug 526).\n -- Set SLURM_JOB_PARTITION environment variable set for all job allocations.\n -- Set SLURM_JOB_PARTITION environment variable for Prolog program.\n -- Added SchedulerParameters option of partition_job_depth to limit scheduling\n    logic depth by partition.\n -- Handle the case in which errno is not reset to 0 after calling\n    getgrent_r(), which causes the controller to core dump.\n\n* Changes in Slurm 14.03.0pre5\n==============================\n -- Added squeue format option of \"%X\" (core specialization count).\n -- Added core specialization web page (just a start for now).\n -- Added the SLURM_ARRAY_JOB_ID and SLURM_ARRAY_TASK_ID\n    in epilog slurmctld environment.\n -- Fix bug in job step allocation failing due to memory limit.\n -- Modify the pbsnodes script to reflect its output on a TORQUE system.\n -- Add ability to clear a node's DRAIN flag using scontrol or sview by setting\n    it's state to \"UNDRAIN\". The node's base state (e.g. \"DOWN\" or \"IDLE\") will\n    not be changed.\n -- Modify the output of 'scontrol show partition' by displaying\n    DefMemPerCPU=UNLIMITED and MaxMemPerCPU=UNLIMITED when these limits are\n    configured as 0.\n -- mpirun-mic - Major re-write of the command wrapper for Xeon Phi use.\n -- Add new configuration parameter of AuthInfo to specify port used by\n    authentication plugin.\n -- Fixed conditional RPM compiling.\n -- Corrected slurmstepd ident name when logging to syslog.\n -- Fixed sh5util loop when there are no node-step files.\n -- Add SLURM_CLUSTER_NAME to environment variables passed to PrologSlurmctld,\n    Prolog, EpilogSlurmctld, and Epilog\n -- Add the idea of running a prolog right when an allocation happens\n    instead of when running on the node for the first time.\n -- If user runs 'scontrol reconfig' but hostnames or the host count changes\n    the slurmctld throws a fatal error.\n -- gres.conf - Add \"NodeName\" specification so that a single gres.conf file\n    can be used for a heterogeneous cluster.\n -- Add flag to accounting RPC to indicate if job data is packed or not.\n -- After all srun tasks have terminated on a node close the stdout/stderr\n    channel with the slurmstepd on that node.\n -- In case of i/o error with slurmstepd log an error message and abort the\n    job.\n -- Add --test-only option to sbatch command to validate the script and options.\n    The response includes expected start time and resources to be allocated.\n\n* Changes in Slurm 14.03.0pre4\n==============================\n -- Remove the ThreadID documentation from slurm.conf. This functionality has\n    been obsoleted by the LogTimeFormat.\n -- Sched plugins - rename global and plugin functions names for consistency\n    with other plugin types.\n -- BGQ - Added RebootQOSList option to bluegene.conf to allow an implicate\n    reboot of a block if only jobs in the list are running on it when cnodes\n    go into a failure state.\n -- Correct task count of pending job steps.\n -- Improve limit enforcement for jobs, set RLIMIT_RSS, RLIMIT_AS and/or\n    RLIMIT_DATA to enforce memory limit.\n -- Pending job steps will have step_id of INFINITE rather than NO_VAL and\n    will be reported as \"TBD\" by scontrol and squeue commands.\n -- Add logic so PMI_Abort or PMI2_Abort can propagate an exit code.\n -- Added SlurmdPlugstack configuration parameter.\n -- Added PriorityFlag DEPTH_OBLIVIOUS to have the depth of an association\n    not effect it's priorty.\n -- Multi-thread the sinfo command (one thread per partition).\n -- Added sgather tool to gather files from a job's compute nodes into a\n    central location.\n -- Added configuration parameter FairShareDampeningFactor to offer a greater\n    priority range based upon utilization.\n -- Change MaxArraySize and job's array_task_id from 16-bit to 32-bit field.\n    Additional Slurm enhancements are be required to support larger job arrays.\n -- Added -S/--core-spec option to salloc, sbatch and srun commands to reserve\n    specialized cores for system use. Modify scontrol and sview to get/set\n    the new field. No enforcement exists yet for these new options.\n    struct job_info / slurm_job_info_t: Added core_spec\n    struct job_descriptorjob_desc_msg_t: Added core_spec\n\n* Changes in Slurm 14.03.0pre3\n==============================\n -- Do not set SLURM_NODEID environment variable on front-end systems.\n -- Convert bitmap functions to use int32_t instead of int in data structures\n    and function arguments. This is to reliably enable use of bitmaps containing\n    up to 4 billion elements. Several data structures containing index values\n    were also changed from data type int to int32_t:\n    - Struct job_info / slurm_job_info_t: Changed exc_node_inx, node_inx, and\n      req_node_inx from type int to type int32_t\n    - job_step_info_t: Changed node_inx from type int to type int32_t\n    - Struct partition_info / partition_info_t: Changed node_inx from type int\n      to type int32_t\n    - block_job_info_t: Changed cnode_inx from type int to type int32_t\n    - block_info_t: Changed ionode_inx and mp_inx from type int to type int32_t\n    - Struct reserve_info / reserve_info_t: Changed node_inx from type int to\n      type int32_t\n -- Modify qsub wrapper output to match torque command output, just print the\n    job ID rather than \"Submitted batch job #\"\n -- Change Slurm error string for ESLURM_MISSING_TIME_LIMIT from\n    \"Missing time limit\" to\n    \"Time limit specification required, but not provided\"\n -- Change salloc job_allocate error message header from\n    \"Failed to allocate resources\" to\n    \"Job submit/allocate failed\"\n -- Modify slurmctld message retry logic to support Cray cold-standby SDB.\n\n* Changes in Slurm 14.03.0pre2\n==============================\n -- Added \"JobAcctGatherParams\" configuration parameter. Value of \"NoShare\"\n    disables accounting for shared memory.\n -- Added fields to \"scontrol show job\" output: boards_per_node,\n    sockets_per_board, ntasks_per_node, ntasks_per_board, ntasks_per_socket,\n    ntasks_per_core, and nice.\n -- Add squeue output format options for job command and working directory\n    (%o and %Z respectively).\n -- Add stdin/out/err to sview job output.\n -- Add new job_state of JOB_BOOT_FAIL for job terminations due to failure to\n    boot it's allocated nodes or BlueGene block.\n -- CRAY - Add SelectTypeParameters NHC_NO_STEPS and NHC_NO which will disable\n    the node health check script for steps and allocations respectfully.\n -- Reservation with CoreCnt: Avoid possible invalid memory reference.\n -- Add new error code for attempt to create a reservation with duplicate name.\n -- Validate that a hostlist file contains text (i.e. not a binary).\n -- switch/generic - propagate switch information from srun down to slurmd and\n    slurmstepd.\n -- CRAY - Do not package Slurm's libpmi or libpmi2 libraries. The Cray version\n    of those libraries must be used.\n -- Added a new option to the scontrol command to view licenses that are\n    configured in use and avalable. 'scontrol show licenses'.\n -- MySQL - Made Slurm compatible with 5.6\n\n* Changes in Slurm 14.03.0pre1\n==============================\n -- sview - improve scalability\n -- Add task pointer to the task_post_term() function in task plugins. The\n    terminating task's PID is available in task->pid.\n -- Move select/cray to select/alps\n -- Defer sending SIGKILL signal to processes while core dump in progress.\n -- Added JobContainerPlugin configuration parameter and plugin infrastructure.\n -- Added partition configuration parameters AllowAccounts, AllowQOS,\n    DenyAccounts and DenyQOS.\n -- The rpmbuild option for a cray system with ALPS has changed from\n    %_with_cray to %_with_cray_alps.\n -- The log file timestamp format can now be selected at runtime via the\n    LogTimeFormat configuration option. See the slurm.conf and slurmdbd.conf\n    man pages for details.\n -- Added switch/generic plugin to a job's convey network topology.\n -- BLUEGENE - If block is in 'D' state or has more cnodes in error than\n    MaxBlockInError set the job wait reason appropriately.\n -- API use: Generate an error return rather than fatal error and exit if the\n    configuraiton file is absent or invalid. This will permit Slurm APIs to be\n    more reliably used by other programs.\n -- Add support for load-based scheduling, allocate jobs to nodes with the\n    largest number of available CPUs. Added SchedulingParameters paramter of\n    \"CR_LLN\" and partition parameter of \"LLN=yes|no\".\n -- Added job_info() and step_info() functions to the gres plugins to extract\n    plugin specific fields from the job's or step's GRES data structure.\n -- Added sbatch --signal option of \"B:\" to signal the batch shell rather than\n    only the spawned job steps.\n -- Added sinfo and squeue format option of \"%all\" to print all fields available\n    for the data type with a vertical bar separating each field.\n -- Add mechanism for job_submit plugin to generate error message for srun,\n    salloc or sbatch to stderr. New argument added to job_submit function in\n    the plugin.\n -- Add StdIn, StdOut, and StdErr paths to job information dumped with\n    \"scontrol show job\".\n -- Permit Slurm administrator to submit a batch job as any user.\n -- Set a job's RLIMIT_AS limit based upon it's memory limit and VsizeFactor\n    configuration value.\n -- Remove Postgres plugins\n -- Make jobacct_gather/cgroup work correctly and also make all jobacct_gather\n    plugins more maintainable.\n -- Proctrack/pgid - Add support for proctrack_p_plugin_get_pids() function.\n -- Sched/backfill - Change default max_job_bf parameter from 50 to 100.\n -- Added -I|--item-extract option to sh5util to extract data item from series.\n\n* Changes in Slurm 2.6.10\n=========================\n -- Switch/nrt - On switch resource allocation failure, free partial allocation.\n -- Switch/nrt - Properly track usage of CAU and RDMA resources with multiple\n    tasks per compute node.\n -- Fix issue where user is requesting --acctg-freq=0 and no memory limits.\n -- BGQ - Temp fix issue where job could be left on job_list after it finished.\n -- BGQ - Fix issue where limits were checked on midplane counts instead of\n    cnode counts.\n -- BGQ - Move code to only start job on a block after limits are checked.\n -- Handle node ranges better when dealing with accounting max node limits.\n -- Fix perlapi to compile correctly with perl 5.18\n -- BGQ - Fix issue with uninitialized variable.\n -- Correct sinfo --sort fields to match documentation: E => Reason,\n    H -> Reason Time (new), R -> Partition Name, u/U -> Reason user (new)\n -- If an invalid assoc_ptr comes in don't use the id to verify it.\n -- Sched/backfill modified to avoid using nodes in completing state.\n -- Correct support for job --profile=none option and related documentation.\n -- Properly enforce job --requeue and --norequeue options.\n -- If a job --mem-per-cpu limit exceeds the partition or system limit, then\n    scale the job's memory limit and CPUs per task to satisfy the limit.\n -- Correct logic to support Power7 processor with 1 or 2 threads per core\n    (CPU IDs are not consecutive).\n\n* Changes in Slurm 2.6.9\n========================\n -- Fix sinfo to work correctly with draining/mixed nodes as well as filtering\n    on Mixed state.\n -- Fix sacctmgr update user with no \"where\" condition.\n -- Fix logic bugs for SchedulerParameters option of max_rpc_cnt.\n\n* Changes in Slurm 2.6.8\n========================\n -- Add support for Torque/PBS job array options and environment variables.\n -- CRAY/ALPS - Add support for CLE52\n -- Fix issue where jobs still pending after a reservation would remain\n    in waiting reason ReqNodeNotAvail.\n -- Update last_job_update when a job's state_reason was modified.\n -- Free job_ptr->state_desc where ever state_reason is set.\n -- Fixed sacct.1 and srun.1 manual pages which contains a hyphen where\n    a minus sign for options was intended.\n -- sinfo - Make sure if partition name is long and the default the last char\n    doesn't get chopped off.\n -- task/affinity - Protect against zero divide when simulating more hardware\n    than you really have.\n -- NRT - Fix issue with 1 node jobs.  It turns out the network does need to\n    be setup for 1 node jobs.\n -- Fix recovery of job dependency on task of job array when slurmctld restarts.\n -- mysql - Fix invalid memory reference.\n -- Lock the /cgroup/freezer subsystem when creating files for tracking processes.\n -- Fix preempt/partition_prio to avoid preempting jobs in partitions with\n    PreemptMode=OFF\n -- launch/poe - Implicitly set --network in job step create request as needed.\n -- Permit multiple batch job submissions to be made for each run of the\n    scheduler logic if the job submissions occur at the nearly same time.\n -- Fix issue where associations weren't correct if backup takes control and\n    new associations were added since it was started.\n -- Fix race condition is corner case with backup slurmctld.\n -- With the backup slurmctld make sure we reinit beginning values in the\n    slurmdbd plugin.\n -- Fix sinfo to work correctly with draining/mixed nodes.\n -- MySQL - Fix it so a lock isn't held unnecessarily.\n -- Added new SchedulerParameters option of max_rpc_cnt when too many RPCs\n    are active.\n -- BGQ - Fix deny_pass to work correctly.\n -- BGQ - Fix sub block steps using a block when the block has passthrough's\n    in it.\n\n* Changes in Slurm 2.6.7\n========================\n -- Properly enforce a job's cpus-per-task option when a job's allocation is\n    constrained on some nodes by the mem-per-cpu option.\n -- Correct the slurm.conf man pages and checkpoint_blcr.html page\n    describing that jobs must be drained from cluster before deploying\n    any checkpoint plugin. Corrected in version 14.03.\n -- Fix issue where if using munge and munge wasn't running and a slurmd\n    needed to forward a message, the slurmd would core dump.\n -- Update srun.1 man page documenting the PMI2 support.\n -- Fix slurmctld core dump when a jobs gets its QOS updated but there\n    is not a corresponding association.\n -- If a job requires specific nodes and can not run due to those nodes being\n    busy, the main scheduling loop will block those specific nodes rather than\n    the entire queue/partition.\n -- Fix minor memory leak when updating a job's name.\n -- Fix minor memory leak when updating a reservation on a partition using \"ALL\"\n    nodes.\n -- Fix minor memory leak when adding a reservation with a nodelist and core\n    count.\n -- Update sacct man page description of job states.\n -- BGQ - Fix minor memory leak when selecting blocks that can't immediately be\n    placed.\n -- Fixed minor memory leak in backfill scheduler.\n -- MYSQL - Fixed memory leak when querying clusters.\n -- MYSQL - Fix when updating QOS on an association.\n -- NRT - Fix to supply correct error messages to poe/pmd when a launch fails.\n -- Add SLURM_STEP_ID to Prolog environment.\n -- Add support for SchedulerParameters value of bf_max_job_start that limits\n    the total number of jobs that can be started in a single iteration of the\n    backfill scheduler.\n -- Don't print negative number when dealing with large memory sizes with\n    sacct.\n -- Fix sinfo output so that host in state allocated and mixed will not be\n    merged together.\n -- GRES: Avoid crash if GRES configurations is inconstent.\n -- Make S_SLURM_RESTART_COUNT item available to SPANK.\n -- Munge plugins - Add sleep between retries if can't connect to socket.\n -- Fix the database query to return all pending jobs in a given time interval.\n -- switch/nrt - Correct logic to get dynamic window count.\n -- Remove need to use job->ctx_params in the launch plugin, just to simplify\n    code.\n -- NRT - Fix possible memory leak if using multiple adapters.\n -- NRT - Fix issue where there are more than NRT_MAXADAPTERS on a system.\n -- NRT - Increase Max number of adapters from 8 -> 9\n -- NRT - Initialize missing variables when the PMD is starting a job.\n -- NRT - Fix issue where we are launching hosts out of numerical order,\n    this would cause pmd's to hang.\n -- NRT - Change xmalloc's to malloc just to be safe.\n -- NRT - Sanity check to make sure a jobinfo is there before packing.\n -- Add missing options to the print of TaskPluginParam.\n -- Fix a couple of issues with scontrol reconfig and adding nodes to\n    slurm.conf.  Rebooting daemons after adding nodes to the slurm.conf\n    is highly recommended.\n\n* Changes in Slurm 2.6.6\n========================\n -- sched/backfill - Fix bug that could result in failing to reserve resources\n    for high priority jobs.\n -- Correct job RunTime if requeued from suspended state.\n -- Reset job priority from zero (held) on manual resume from suspend state.\n -- If FastSchedule=0 then do not DOWN a node with low memory or disk size.\n -- Remove vestigial note.\n -- Update sshare.1 man page making it consistent with sacctmgr.1.\n -- Do not reset a job's priority when the slurmctld restarts if previously\n    set to some specific value.\n -- sview - Fix regression where the Node tab wasn't able to add/remove columns.\n -- Fix slurmstepd lock when job terminates inside the infiniband\n    network traffic accounting plugin.\n -- Correct the documentation to read filesystem instead of Lustre. Update\n    the srun help.\n -- Fix the acct_gather_filesystem_lustre.c to compute the Lustre accounting\n    data correctly accumulating differences between sampling intervals.\n    Fix the data structure mismatch between acct_gather_filesystem_lustre.c\n    and slurm_jobacct_gather.h which caused the hdf5 plugin to log incorrect\n    data.\n -- Don't allow PMI_TIME to be zero which will cause floating exception.\n -- Fix purging of old reservation errors in database.\n -- MYSQL - If starting the plugin and the database isn't up attempt to\n    connect in a loop instead of producing a fatal.\n -- BLUEGENE - If IONodesPerMP changes in bluegene.conf recalculate bitmaps\n    based on ionode count correctly on slurmctld restart.\n -- Fix step allocation when some CPUs are not available due to memory limits.\n    This happens when one step is active and using memory that blocks the\n    scheduling of another step on a portion of the CPUs needed. The new step\n    is now delayed rather than aborting with \"Requested node configuration is\n    not available\".\n -- Make sure node limits get assessed if no node count was given in request.\n -- Removed obsolete slurm_terminate_job() API.\n -- Update documentation about QOS limits\n -- Retry task exit message from slurmstepd to srun on message timeout.\n -- Correction to logic reserving all nodes in a specified partition.\n -- Added support for selecting AMD GPU by setting GPU_DEVICE_ORDINAL env var.\n -- Properly enforce GrpSubmit limit for job arrays.\n -- CRAY - fix issue with using CR_ONE_TASK_PER_CORE\n -- CRAY - fix memory leak when using accelerators\n\n* Changes in Slurm 2.6.5\n========================\n -- Correction to hostlist parsing bug introduced in v2.6.4 for hostlists with\n    more than one numeric range in brackets (e.g. rack[0-3]_blade[0-63]\").\n -- Add notification if using proctrack/cgroup and task/cgroup when oom hits.\n -- Corrections to advanced reservation logic with overlapping jobs.\n -- job_submit/lua - add cpus_per_task field to those available.\n -- Add cpu_load to the node information available using the Perl API.\n -- Correct a job's GRES allocation data in accounting records for non-Cray\n    systems.\n -- Substantial performance improvement for systems with Shared=YES or FORCE\n    and large numbers of running jobs (replace bubble sort with quick sort).\n -- proctrack/cgroup - Add locking to prevent race condition where one job step\n    is ending for a user or job at the same time another job stepsis starting\n    and the user or job container is deleted from under the starting job step.\n -- Fixed sh5util loop when there are no node-step files.\n -- Fix race condition on batch job termination that could result in a job exit\n    code of 0xfffffffe if the slurmd on node zero registers its active jobs at\n    the same time that slurmstepd is recording the job's exit code.\n -- Correct logic returning remaining job dependencies in job information\n    reported by scontrol and squeue. Eliminates vestigial descriptors with\n    no job ID values (e.g. \"afterany\").\n -- Improve performance of REQUEST_JOB_INFO_SINGLE RPC by removing unnecessary\n    locks and use hash function to find the desired job.\n -- jobcomp/filetxt - Reopen the file when slurmctld daemon is reconfigured\n    or gets SIGHUP.\n -- Remove notice of CVE with very old/deprecated versions of Slurm in\n    news.html.\n -- Fix if hwloc_get_nbobjs_by_type() returns zero core count (set to 1).\n -- Added ApbasilTimeout parameter to the cray.conf configuration file.\n -- Handle in the API if parts of the node structure are NULL.\n -- Fix srun hang when IO fails to start at launch.\n -- Fix for GRES bitmap not matching the GRES count resulting in abort\n    (requires manual resetting of GRES count, changes to gres.conf file,\n    and slurmd restarts).\n -- Modify sview to better support job arrays.\n -- Modify squeue to support longer job ID values (for many job array tasks).\n -- Fix race condition in authentication credential creation that could corrupt\n    memory. (NOTE: This race condition has existed since 2003 and would be\n    exceedingly rare.)\n -- HDF5 - Fix minor memory leak.\n -- Slurmstepd variable initialization - Without this patch, free() is called\n    on a random memory location (i.e. whatever is on the stack), which can\n    result in slurmstepd dying and a completed job not being purged in a\n    timely fashion.\n -- Fix slurmstepd race condition when separate threads are reading and\n    modifying the job's environment, which can result in the slurmstepd failing\n    with an invalid memory reference.\n -- Fix erroneous error messages when running gang scheduling.\n -- Fix minor memory leak.\n -- scontrol modified to suspend, resume, hold, uhold, or release multiple\n    jobs in a space separated list.\n -- Minor debug error when a connection goes away at the end of a job.\n -- Validate return code from calls to slurm_get_peer_addr\n -- BGQ - Fix issues with making sure all cnodes are accounted for when mulitple\n    steps cause multiple cnodes in one allocation to go into error at the\n    same time.\n -- scontrol show job - Correct NumNodes value calculated based upon job\n    specifications.\n -- BGQ - Fix issue if user runs multiple sub-block jobs inside a multiple\n    midplane block that starts on a higher coordinate than it ends (i.e if a\n    block has midplanes [0010,0013] 0013 is the start even though it is\n    listed second in the hostlist).\n -- BGQ - Add midplane to the total_cnodes used in the runjob_mux plugin\n    for better debug.\n -- Update AllocNodes paragraph in slurm.conf.5.\n\n* Changes in Slurm 2.6.4\n========================\n -- Fixed sh5util to print its usage.\n -- Corrected commit f9a3c7e4e8ec.\n -- Honor ntasks-per-node option with exclusive node allocations.\n -- sched/backfill - Prevent invalid memory reference if bf_continue option is\n    configured and slurm is reconfigured during one of the sleep cycles or if\n    there are any changes to the partition configuration or if the normal\n    scheduler runs and starts a job that the backfill scheduler is actively\n    working on.\n -- Update man pages information about acct-freq and JobAcctGatherFrequency\n    to reflect only the latest supported format.\n -- Minor document update to include note about PrivateData=Usage for the\n    slurm.conf when using the DBD.\n -- Expand information reported with DebugFlags=backfill.\n -- Initiate jobs pending to run in a reservation as soon as the reservation\n    becomes active.\n -- Purged expired reservation even if it has pending jobs.\n -- Corrections to calculation of a pending job's expected start time.\n -- Remove some vestigial logic treating job priority of 1 as a special case.\n -- Memory freeing up to avoid minor memory leaks at close of daemons\n -- Updated documentation to give correct units being displayed.\n -- Report AccountingStorageBackupHost with \"scontrol show config\".\n -- init scripts ignore quotes around Pid file name specifications.\n -- Fixed typo about command case in quickstart.html.\n -- task/cgroup - handle new cpuset files, similar to commit c4223940.\n -- Replace the tempname() function call with mkstemp().\n -- Fix for --cpu_bind=map_cpu/mask_cpu/map_ldom/mask_ldom plus\n    --mem_bind=map_mem/mask_mem options, broken in 2.6.2.\n -- Restore default behavior of allocating cores to jobs on a cyclic basis\n    across the sockets unless SelectTypeParameters=CR_CORE_DEFAULT_DIST_BLOCK\n    or user specifies other distribution options.\n -- Enforce JobRequeue configuration parameter on node failure. Previously\n    always requeued the job.\n -- acct_gather_energy/ipmi - Add delay before retry on read error.\n -- select/cons_res with GRES and multiple threads per core, fix possible\n    infinite loop.\n -- proctrack/cgroup - Add cgroup create retry logic in case one step is\n    starting at the same time as another step is ending and the logic to create\n    and delete cgroups overlaps.\n -- Improve setting of job wait \"Reason\" field.\n -- Correct sbatch documentation and job_submit/pbs plugin \"%j\" is job ID,\n    not \"%J\" (which is job_id.step_id).\n -- Improvements to sinfo performance, especially for large numbers of\n    partitions.\n -- SlurmdDebug - Permit changes to slurmd debug level with \"scontrol reconfig\"\n -- smap - Avoid invalid memory reference with hidden nodes.\n -- Fix sacctmgr modify qos set preempt+/-=.\n -- BLUEGENE - fix issue where node count wasn't set up correctly when srun\n    preforms the allocation, regression in 2.6.3.\n -- Add support for dependencies of job array elements (e.g.\n    \"sbatch --depend=afterok:123_4 ...\") or all elements of a job array (e.g.\n    \"sbatch --depend=afterok:123 ...\").\n -- Add support for new options in sbatch qsub wrapper:\n    -W block=true\t(wait for job completion)\n    Clear PBS_NODEFILE environment variable\n -- Fixed the MaxSubmitJobsPerUser limit in QOS which limited submissions\n    a job too early.\n -- sched/wiki, sched/wiki2 - Fix to work with change logic introduced in\n    version 2.6.3 preventing Maui/Moab from starting jobs.\n -- Updated the QOS limits documentation and man page.\n\n* Changes in Slurm 2.6.3\n========================\n -- Add support for some new #PBS options in sbatch scripts and qsub wrapper:\n    -l accelerator=true|false\t(GPU use)\n    -l mpiprocs=#\t(processors per node)\n    -l naccelerators=#\t(GPU count)\n    -l select=#\t\t(node count)\n    -l ncpus=#\t\t(task count)\n    -v key=value\t(environment variable)\n    -W depend=opts\t(job dependencies, including \"on\" and \"before\" options)\n    -W umask=#\t\t(set job's umask)\n -- Added qalter and qrerun commands to torque package.\n -- Corrections to qstat logic: job CPU count and partition time format.\n -- Add job_submit/pbs plugin to translate PBS job dependency options to the\n    extend possible (no support for PBS \"before\" options) and set some PBS\n    environment variables.\n -- Add spank/pbs plugin to set a bunch of PBS environment variables.\n -- Backported sh5util from master to 2.6 as there are some important\n    bugfixes and the new item extraction feature.\n -- select/cons_res - Correct MacCPUsPerNode partition constraint for CR_Socket.\n -- scontrol - for setdebugflags command, avoid parsing \"-flagname\" as an\n    scontrol command line option.\n -- Fix issue with step accounting if a job is requeued.\n -- Close file descriptors on exec of prolog, epilog, etc.\n -- Fix issue when a user has held a job and then sets the begin time\n    into the future.\n -- Scontrol - Enable changing a job's stdout file.\n -- Fix issues where memory or node count of a srun job is altered while the\n    srun is pending.  The step creation would use the old values and possibly\n    hang srun since the step wouldn't be able to be created in the modified\n    allocation.\n -- Add support for new SchedulerParameters value of \"bf_max_job_part\", the\n    maximum depth the backfill scheduler should go in any single partition.\n -- acct_gather/infiniband plugin - Correct packets_in/out values.\n -- BLUEGENE - Don't ignore a conn-type request from the user.\n -- BGQ - Force a request on a Q for a MESH to be a TORUS in a dimension that\n    can only be a TORUS (1).\n -- Change max message length from 100MB to 1GB before generating \"Insane\n    message length\" error.\n -- sched/backfill - Prevent possible memory corruption due to use of\n    bf_continue option and long running scheduling cycle (pending jobs could\n    have been cancelled and purged).\n -- CRAY - fix AcceleratorAllocation depth correctly for basil 1.3\n -- Created the environment variable SLURM_JOB_NUM_NODES for srun jobs and\n    updated the srun man page.\n -- BLUEGENE/CRAY - Don't set env variables that pertain to a node when Slurm\n    isn't doing the launching.\n -- gres/gpu and gres/mic - Do not treat the existence of an empty gres.conf\n    file as a fatal error.\n -- Fixed for if hours are specified as 0 the time days-0:min specification\n    is not parsed correctly.\n -- switch/nrt - Fix for memory leak.\n -- Subtract the PMII_COMMANDLEN_SIZE in contribs/pmi2/pmi2_api.c to prevent\n    certain implementation of snprintf() to segfault.\n\n* Changes in Slurm 2.6.2\n========================\n -- Fix issue with reconfig and GrpCPURunMins\n -- Fix of wrong node/job state problem after reconfig\n -- Allow users who are coordinators update their own limits in the accounts\n    they are coordinators over.\n -- BackupController - Make sure we have a connection to the DBD first thing\n    to avoid it thinking we don't have a cluster name.\n -- Correct value of min_nodes returned by loading job information to consider\n    the job's task count and maximum CPUs per node.\n -- If running jobacct_gather/none fix issue on unpacking step completion.\n -- Reservation with CoreCnt: Avoid possible invalid memory reference.\n -- sjstat - Add man page when generating rpms.\n -- Make sure GrpCPURunMins is added when creating a user, account or QOS with\n    sacctmgr.\n -- Fix for invalid memory reference due to multiple free calls caused by\n    job arrays submitted to multiple partitions.\n -- Enforce --ntasks-per-socket=1 job option when allocating by socket.\n -- Validate permissions of key directories at slurmctld startup. Report\n    anything that is world writable.\n -- Improve GRES support for CPU topology. Previous logic would pick CPUs then\n    reject jobs that can not match GRES to the allocated CPUs. New logic first\n    filters out CPUs that can not use the GRES, next picks CPUs for the job,\n    and finally picks the GRES that best match those CPUs.\n -- Switch/nrt - Prevent invalid memory reference when allocating single adapter\n    per node of specific adapter type\n -- CRAY - Make Slurm work with CLE 5.1.1\n -- Fix segfault if submitting to multiple partitions and holding the job.\n -- Use MAXPATHLEN instead of the hardcoded value 1024 for maximum file path\n    lengths.\n -- If OverTimeLimit is defined do not declare failed those jobs that ended\n    in the OverTimeLimit interval.\n\n* Changes in Slurm 2.6.1\n========================\n -- slurmdbd - Allow job derived ec and comments to be modified by non-root\n    users.\n -- Fix issue with job name being truncated to 24 chars when sending a mail\n    message.\n -- Fix minor issues with spec file, missing files and including files\n    erroneously on a bluegene system.\n -- sacct - fix --name and --partition options when using\n    accounting_storage/filetxt.\n -- squeue - Remove extra whitespace of default printout.\n -- BGQ - added head ppcfloor as an include dir when building.\n -- BGQ - Better debug messages in runjob_mux plugin.\n -- PMI2 Updated the Makefile.am to build a versioned library.\n -- CRAY - Fix srun --mem_bind=local option with launch/aprun.\n -- PMI2 Corrected buffer size computation in the pmi2_api.c module.\n -- GRES accounting data wrong in database: gres_alloc, gres_req, and gres_used\n    fields were empty if the job was not started immediately.\n -- Fix sbatch and srun task count logic when --ntasks-per-node specified,\n    but no explicit task count.\n -- Corrected the hdf5 profile user guide and the acct_gather.conf\n    documentation.\n -- IPMI - Fix Math bug getting new wattage.\n -- Corrected the AcctGatherProfileType documentation in slurm.conf\n -- Corrected the sh5util program to print the header in the csv file\n    only once, set the debug messages at debug() level, make the argument\n    check case insensitive and avoid printing duplicate \\n.\n -- If cannot collect energy values send message to the controller\n    to drain the node and log error slurmd log file.\n -- Handle complete removal of CPURunMins time at the end of the job instead\n    of at multifactor poll.\n -- sview - Add missing debug_flag options.\n -- PGSQL - Notes about Postgres functionality being removed in the next\n    version of Slurm.\n -- MYSQL - fix issue when rolling up usage and events happened when a cluster\n    was down (slurmctld not running) during that time period.\n -- sched/wiki2 - Insure that Moab gets current CPU load information.\n -- Prevent infinite loop in parsing configuration if including file containing\n    one blank line.\n -- Fix pack and unpack between 2.6 and 2.5.\n -- Fix job state recovery logic in which a job's accounting frequency was\n    not set. This would result in a value of 65534 seconds being used (the\n    equivalent of NO_VAL in uint16_t), which could result in the job being\n    requeued or aborted.\n -- Validate a job's accounting frequency at submission time rather than\n    waiting for it's initiation to possibly fail.\n -- Fix CPURunMins if a job is requeued from a failed launch.\n -- Fix in accounting_storage/filetxt to correct start times which sometimes\n    could end up before the job started.\n -- Fix issue with potentially referencing past an array in parse_time()\n -- CRAY - fix issue with accelerators on a cray when parsing BASIL 1.3 XML.\n -- Fix issue with a 2.5 slurmstepd locking up when talking to a 2.6 slurmd.\n -- Add argument to priority plugin's priority_p_reconfig function to note\n    when the association and QOS used_cpu_run_secs field has been reset.\n\n* Changes in Slurm 2.6.0\n========================\n -- Fix it so bluegene and serial systems don't get warnings over new NODEDATA\n    enum.\n -- When a job is aborted send a message for any tasks that have completed.\n -- Correction to memory per CPU calculation on system with threads and\n    allocating cores or sockets.\n -- Requeue batch job if it's node reboots (used to abort the job).\n -- Enlarge maximum size of srun's hostlist file.\n -- IPMI - Fix first poll to get correct consumed_energy for a step.\n -- Correction to job state recovery logic that could result in assert failure.\n -- Record partial step accounting record if allocated nodes fail abnormally.\n -- Accounting - fix issue where PrivateData=jobs or users could potentially\n    show information to users that had no associations on the system.\n -- Make PrivateData in slurmdbd.conf case insensitive.\n -- sacct/sstat - Add format option ConsumedEnergyRaw to print full energy\n    values.\n\n* Changes in Slurm 2.6.0rc2\n===========================\n -- HDF5 - Fix issue with Ubuntu where HDF5 development headers are\n    overwritten by the parallel versions thus making it so we need handle\n    both cases.\n -- ACCT_GATHER - handle suspending correctly for polling threads.\n -- Make SLURM_DISTRIBUTION env var hold both types of distribution if\n    specified.\n -- Remove hardcoded /usr/local from slurm.spec.\n -- Modify slurmctld locking to improve performance under heavy load with\n    very large numbers of batch job submissions or job cancellations.\n -- sstat - Fix issue where if -j wasn't given allow last argument to be checked\n    for as the job/step id.\n -- IPMI - fix adjustment on poll when using EnergyIPMICalcAdjustment.\n\n* Changes in Slurm 2.6.0rc1\n===========================\n -- Added helper script for launching symmetric and MIC-only MPI tasks within\n    SLURM (in contribs/mic/mpirun-mic).\n -- Change maximum delay for state save from 2 secs to 5 secs. Make timeout\n    configurable at build time by defining SAVE_MAX_WAIT.\n -- Modify slurmctld data structure locking to interleave read and write\n    locks rather than always favor write locks over read locks.\n -- Added sacct format option of \"ALL\" to print all fields.\n -- Deprecate the SchedulerParameters value of \"interval\" use \"bf_interval\"\n    instead as documented.\n -- Add acct_gather_profile/hdf5 to profile jobs with hdf5\n -- Added MaxCPUsPerNode partition configuration parameter. This can be\n    especially useful to schedule systems with GPUs.\n -- Permit \"scontrol reboot_node\" for nodes in MAINT reservation.\n -- Added \"PriorityFlags\" value of \"SMALL_RELATIVE_TO_TIME\". If set, the job's\n    size component will be based upon not the job size alone, but the job's\n    size divided by it's time limit.\n -- Added sbatch option \"--ignore-pbs\" to ignore \"#PBS\" options in the batch\n    script.\n -- Rename slurm_step_ctx_params_t field from \"mem_per_cpu\" to \"pn_min_memory\".\n    Job step now accepts memory specification in either per-cpu or per-node\n    basis.\n -- Add ability to specify host repitition count in the srun hostfile (e.g.\n    \"host1*2\" is equivalent to \"host1,host1\").\n\n* Changes in Slurm 2.6.0pre3\n============================\n -- Add milliseconds to default log message header (both RFC 5424 and ISO 8601\n    time formats). Disable milliseconds logging using the configure\n    parameter \"--disable-log-time-msec\". Default time format changes to\n    ISO 8601 (without time zone information). Specify \"--enable-rfc5424time\"\n    to restore the time zone information.\n -- Add username (%u) to the filename pattern in the batch script.\n -- Added options for front end nodes of AllowGroups, AllowUsers, DenyGroups,\n    and DenyUsers.\n -- Fix sched/backfill logic to initiate jobs with maximum time limit over the\n    partition limit, but the minimum time limit permits it to start.\n -- gres/gpu - Fix for gres.conf file with multiple files on a single line\n    using a slurm expression (e.g. \"File=/dev/nvidia[0-1]\").\n -- Replaced ipmi.conf with generic acct_gather.conf file for all acct_gather\n    plugins.  For those doing development to use this follow the model set\n    forth in the acct_gather_energy_ipmi plugin.\n -- Added more options to update a step's information\n -- Add DebugFlags=ThreadID which will print the thread id of the calling\n    thread.\n -- CRAY - Allocate whole node (CPUs) in reservation despite what the\n    user requests.  We have found any srun/aprun afterwards will work on a\n    subset of resources.\n\n* Changes in Slurm 2.6.0pre2\n============================\n -- Do not purge inactive interactive jobs that lack a port to ping (added\n    for MR+ operation).\n -- Advanced reservations with hostname and core counts now supports asymetric\n    reservations (e.g. specific different core count for each node).\n -- Added slurmctld/dynalloc plugin for MapReduce+ support.\n -- Added \"DynAllocPort\" configuration parameter.\n -- Added partition paramter of SelectTypeParameters to override system-wide\n    value.\n -- Added cr_type to partition_info data structure.\n -- Added allocated memory to node information available (within the existing\n    select_nodeinfo field of the node_info_t data structure). Added Allocated\n    Memory to node information displayed by sview and scontrol commands.\n -- Make sched/backfill the default scheduling plugin rather than sched/builtin\n    (FIFO).\n -- Added support for a job having different priorities in different partitions.\n -- Added new SchedulerParameters configuration parameter of \"bf_continue\"\n    which permits the backfill scheduler to continue considering jobs for\n    backfill scheduling after yielding locks even if new jobs have been\n    submitted. This can result in lower priority jobs from being backfill\n    scheduled instead of newly arrived higher priority jobs, but will permit\n    more queued jobs to be considered for backfill scheduling.\n -- Added support to purge reservation records from accounting.\n -- Cray - Add support for Basil 1.3\n\n* Changes in SLURM 2.6.0pre1\n============================\n -- Add \"state\" field to job step information reported by scontrol.\n -- Notify srun to retry step creation upon completion of other job steps\n    rather than polling. This results in much faster throughput for job step\n    execution with --exclusive option.\n -- Added \"ResvEpilog\" and \"ResvProlog\" configuration parameters to execute a\n    program at the beginning and end of each reservation.\n -- Added \"slurm_load_job_user\" function. This is a variation of\n    \"slurm_load_jobs\", but accepts a user ID argument, potentially resulting\n    in substantial performance improvement for \"squeue --user=ID\"\n -- Added \"slurm_load_node_single\" function. This is a variation of\n    \"slurm_load_nodes\", but accepts a node name argument, potentially resulting\n    in substantial performance improvement for \"sinfo --nodes=NAME\".\n -- Added \"HealthCheckNodeState\" configuration parameter identify node states\n    on which HealthCheckProgram should be executed.\n -- Remove sacct --dump --formatted-dump options which were deprecated in\n    2.5.\n -- Added support for job arrays (phase 1 of effort). See \"man sbatch\" option\n    -a/--array for details.\n -- Add new AccountStorageEnforce options of 'nojobs' and 'nosteps' which will\n    allow the use of accounting features like associations, qos and limits but\n    not keep track of jobs or steps in accounting.\n -- Cray - Add new cray.conf parameter of \"AlpsEngine\" to specify the\n    communication protocol to be used for ALPS/BASIL.\n -- select/cons_res plugin: Correction to CPU allocation count logic in for\n    cores without hyperthreading.\n -- Added new SelectTypeParameter value of \"CR_ALLOCATE_FULL_SOCKET\".\n -- Added PriorityFlags value of \"TICKET_BASED\" and merged priority/multifactor2\n    plugin into priority/multifactor plugin.\n -- Add \"KeepAliveTime\" configuration parameter controlling how long sockets\n    used for srun/slurmstepd communications are kept alive after disconnect.\n -- Added SLURM_SUBMIT_HOST to salloc, sbatch and srun job environment.\n -- Added SLURM_ARRAY_TASK_ID to environment of job array.\n -- Added squeue --array/-r option to optimize output for job arrays.\n -- Added \"SlurmctldPlugstack\" configuration parameter for generic stack of\n    slurmctld daemon plugins.\n -- Removed contribs/arrayrun tool. Use native support for job arrays.\n -- Modify default installation locations for RPMs to match \"make install\":\n    _prefix /usr/local\n    _slurm_sysconfdir %{_prefix}/etc/slurm\n    _mandir %{_prefix}/share/man\n    _infodir %{_prefix}/share/info\n -- Add acct_gather_energy/ipmi which works off freeipmi for energy gathering\n\n* Changes in Slurm 2.5.8\n========================\n -- Fix for slurmctld segfault on NULL front-end reason field.\n -- Avoid gres step allocation errors when a job shrinks in size due to either\n    down nodes or explicit resizing. Generated slurmctld errors of this type:\n    \"step_test ... gres_bit_alloc is NULL\"\n -- Fix bug that would leak memory and over-write the AllowGroups field if on\n    \"scontrol reconfig\" when AllowNodes is manually changed using scontrol.\n -- Get html/man files to install in correct places with rpms.\n -- Remove --program-prefix from spec file since it appears to be added by\n    default and appeared to break other things.\n -- Updated the automake min version in autogen.sh to be correct.\n -- Select/cons_res - Correct total CPU count allocated to a job with\n    --exclusive and --cpus-per-task options\n -- switch/nrt - Don't allocate network resources unless job step has 2+ nodes.\n -- select/cons_res - Avoid extraneous \"oversubscribe\" error messages.\n -- Reorder get config logic to avoid deadlock.\n -- Enforce QOS MaxCPUsMin limit when job submission contains no user-specified\n    time limit.\n -- EpilogSlurmctld pthread is passed required arguments rather than a pointer\n    to the job record, which under some conditions could be purged and result\n    in an invalid memory reference.\n\n* Changes in Slurm 2.5.7\n========================\n -- Fix for linking to the select/cray plugin to not give warning about\n    undefined variable.\n -- Add missing symbols to the xlator.h\n -- Avoid placing pending jobs in AdminHold state due to backfill scheduler\n    interactions with advanced reservation.\n -- Accounting - make average by task not cpu.\n -- CRAY - Change logging of transient ALPS errors from error() to debug().\n -- POE - Correct logic to support poe option \"-euidevice sn_all\" and\n    \"-euidevice sn_single\".\n -- Accounting - Fix minor initialization error.\n -- POE - Correct logic to support srun network instances count with POE.\n -- POE - With the srun --launch-cmd option, report proper task count when\n    the --cpus-per-task option is used without the --ntasks option.\n -- POE - Fix logic binding tasks to CPUs.\n -- sview - Fix race condition where new information could of slipped past\n    the node tab and we didn't notice.\n -- Accounting - Fix an invalid memory read when slurmctld sends data about\n    start job to slurmdbd.\n -- If a prolog or epilog failure occurs, drain the node rather than setting it\n    down and killing all of its jobs.\n -- Priority/multifactor - Avoid underflow in half-life calculation.\n -- POE - pack missing variable to allow fanout (more than 32 nodes)\n -- Prevent clearing reason field for pending jobs. This bug was introduced in\n    v2.5.5 (see \"Reject job at submit time ...\").\n -- BGQ - Fix issue with preemption on sub-block jobs where a job would kill\n    all preemptable jobs on the midplane instead of just the ones it needed to.\n -- switch/nrt - Validate dynamic window allocation size.\n -- BGQ - When --geo is requested do not impose the default conn_types.\n -- CRAY - Support CLE 4.2.0\n -- RebootNode logic - Defers (rather than forgets) reboot request with job\n    running on the node within a reservation.\n -- switch/nrt - Correct network_id use logic. Correct support for user sn_all\n    and sn_single options.\n -- sched/backfill - Modify logic to reduce overhead under heavy load.\n -- Fix job step allocation with --exclusive and --hostlist option.\n -- Select/cons_res - Fix bug resulting in error of \"cons_res: sync loop not\n    progressing, holding job #\"\n -- checkpoint/blcr - Reset max_nodes from zero to NO_VAL on job restart.\n -- launch/poe - Fix for hostlist file support with repeated host names.\n -- priority/multifactor2 - Prevent possible divide by zero.\n -- srun - Don't check for executable if --test-only flag is used.\n -- energy - On a single node only use the last task for gathering energy.\n    Since we don't currently track energy usage per task (only per step).\n    Otherwise we get double the energy.\n\n* Changes in Slurm 2.5.6\n========================\n -- Gres fix for requeued jobs.\n -- Gres accounting - Fix regression in 2.5.5 for keeping track of gres\n    requested and allocated.\n\n* Changes in Slurm 2.5.5\n========================\n -- Fix for sacctmgr add qos to handle the 'flags' option.\n -- Export SLURM_ environment variables from sbatch, even if \"--export\"\n    option does not explicitly list them.\n -- If node is in more than one partition, correct counting of allocated CPUs.\n -- If step requests more CPUs than possible in specified node count of job\n    allocation then return ESLURM_TOO_MANY_REQUESTED_CPUS rather than\n    ESLURM_NODES_BUSY and retrying.\n -- CRAY - Fix SLURM_TASKS_PER_NODE to be set correctly.\n -- Accounting - more checks for strings with a possible `'` in it.\n -- sreport - Fix by adding planned down time to utilization reports.\n -- Do not report an error when sstat identifies job steps terminated during\n    its execution, but log using debug type message.\n -- Select/cons_res - Permit node removed from job by going down to be returned\n    to service and re-used by another job.\n -- Select/cons_res - Tighter packing of job allocations on sockets.\n -- SlurmDBD - fix to allow user root along with the slurm user to register a\n    cluster.\n -- Select/cons_res - Fix for support of consecutive node option.\n -- Select/cray - Modify build to enable direct use of libslurm library.\n -- Bug fixes related to job step allocation logic.\n -- Cray - Disable enforcement of MaxTasksPerNode, which is not applicable\n    with launch/aprun.\n -- Accounting - When rolling up data from past usage ignore \"idle\" time from\n    a reservation when it has the \"Ignore_Jobs\" flag set.  Since jobs could run\n    outside of the reservation in it's nodes without this you could have\n    double time.\n -- Accounting - Minor fix to avoid reuse of variable erroneously.\n -- Reject job at submit time if the node count is invalid. Previously such a\n    job submitted to a DOWN partition would be queued.\n -- Purge vestigial job scripts when the slurmd cold starts or slurmstepd\n    terminates abnormally.\n -- Add support for FreeBSD.\n -- Add sanity check for NULL cluster names trying to register.\n -- BGQ - Push action 'D' info to scontrol for admins.\n -- Reset a job's reason from PartitionDown when the partition is set up.\n -- BGQ - Handle issue where blocks would have a pending job on them and\n    while it was free cnodes would go into software error and kill the job.\n -- BGQ - Fix issue where if for some reason we are freeing a block with\n    a pending job on it we don't kill the job.\n -- BGQ - Fix race condition were a job could of been removed from a block\n    without it still existing there.  This is extremely rare.\n -- BGQ - Fix for when a step completes in Slurm before the runjob_mux notifies\n    the slurmctld there were software errors on some nodes.\n -- BGQ - Fix issue on state recover if block states are not around\n    and when reading in state from DB2 we find a block that can't be created.\n    You can now do a clean start to rid the bad block.\n -- Modify slurmdbd to retransmit to slurmctld daemon if it is not responding.\n -- BLUEGENE - Fix issue where when doing backfill preemptable jobs were\n    never looked at to determine eligibility of backfillable job.\n -- Cray/BlueGene - Disable srun --pty option unless LaunchType=launch/slurm.\n -- CRAY - Fix sanity check for systems with more than 32 cores per node.\n -- CRAY - Remove other objects from MySQL query that are available from\n    the XML.\n -- BLUEGENE - Set the geometry of a job when a block is picked and the job\n    isn't a sub-block job.\n -- Cray - avoid check of macro versions of CLE for version 5.0.\n -- CRAY - Fix memory issue with reading in the cray.conf file.\n -- CRAY - If hostlist is given with srun make sure the node count is the same\n    as the hosts given.\n -- CRAY - If task count specified, but no tasks-per-node, then set the tasks\n    per node in the BASIL reservation request.\n -- CRAY - fix issue with --mem option not giving correct amount of memory\n    per cpu.\n -- CRAY - Fix if srun --mem is given outside an allocation to set the\n    APRUN_DEFAULT_MEMORY env var for aprun.  This scenario will not display\n    the option when used with --launch-cmd.\n -- Change sview to use GMutex instead of GStaticMutex\n -- CRAY - set APRUN_DEFAULT_MEMROY instead of CRAY_AUTO_APRUN_OPTIONS\n -- sview - fix issue where if a partition was completely in one state the\n    cpu count would be reflected correctly.\n -- BGQ - fix for handling half rack system in STATIC of OVERLAP mode to\n    implicitly create full system block.\n -- CRAY - Dynamically create BASIL XML buffer to resize as needed.\n -- Fix checking if QOS limit MaxCPUMinsPJ is set along with DenyOnLimit to\n    deny the job instead of holding it.\n -- Make sure on systems that use a different launcher than launch/slurm not\n    to attempt to signal tasks on the frontend node.\n -- Cray - when a step is requested count other steps running on nodes in the\n    allocation as taking up the entire node instead of just part of the node\n    allocated.  And always enforce exclusive on a step request.\n -- Cray - display correct nodelist, node/cpu count on steps.\n\n* Changes in Slurm 2.5.4\n========================\n -- Fix bug in PrologSlurmctld use that would block job steps until node\n    responds.\n -- CRAY - If a partition has MinNodes=0 and a batch job doesn't request nodes\n    put the allocation to 1 instead of 0 which prevents the allocation to\n    happen.\n -- Better debug when the database is down and using the --cluster option in\n    the user commands.\n -- When asking for job states with sacct, default to 'now' instead of midnight\n    of the current day.\n -- Fix for handling a test-only job or immediate job that fails while being\n    built.\n -- Comment out all of the logic in the job_submit/defaults plugin. The logic\n    is only an example and not meant for actual use.\n -- Eliminate configuration file 4096 character line limitation.\n -- More robust logic for tree message forward\n -- BGQ - When cnodes fail in a timeout fashion correctly look up parent\n    midplane.\n -- Correct sinfo \"%c\" (node's CPU count) output value for Bluegene systems.\n -- Backfill - Responsive improvements for systems with large numbers of jobs\n    (>5000) and using the SchedulerParameters option bf_max_job_user.\n -- slurmstepd: ensure that IO redirection openings from/to files correctly\n    handle interruption\n -- BGQ - Able to handle when midplanes go into Hardware::SoftwareFailure\n -- GRES - Correct tracking of specific resources used after slurmctld restart.\n    Counts would previously go negative as jobs terminate and decrement from\n    a base value of zero.\n -- Fix for priority/multifactor2 plugin to not assert when configured with\n    --enable-debug.\n -- Select/cons_res - If the job request specified --ntasks-per-socket and the\n    allocation using is cores, then pack the tasks onto the sockets up to the\n    specified value.\n -- BGQ - If a cnode goes into an 'error' state and the block containing the\n    cnode does not have a job running on it do not resume the block.\n -- BGQ - Handle blocks that don't free themselves in a reasonable time better.\n -- BGQ - Fix for signaling steps when allocation ends before step.\n -- Fix for backfill scheduling logic with job preemption; starts more jobs.\n -- xcgroup - remove bugs with EINTR management in write calls\n -- jobacct_gather - fix total values to not always == the max values.\n -- Fix for handling node registration messages from older versions without\n    energy data.\n -- BGQ - Allow user to request full dimensional mesh.\n -- sdiag command - Correction to jobs started value reported.\n -- Prevent slurmctld assert when invalid change to reservation with running\n    jobs is made.\n -- BGQ - If signal is NODE_FAIL allow forward even if job is completing\n    and timeout in the runjob_mux trying to send in this situation.\n -- BGQ - More robust checking for correct node, task, and ntasks-per-node\n    options in srun, and push that logic to salloc and sbatch.\n -- GRES topology bug in core selection logic fixed.\n -- Fix to handle init.d script for querying status and not return 1 on\n    success.\n\n* Changes in SLURM 2.5.3\n========================\n -- Gres/gpu plugin - If no GPUs requested, set CUDA_VISIBLE_DEVICES=NoDevFiles.\n    This bug was introduced in 2.5.2 for the case where a GPU count was\n    configured, but without device files.\n -- task/affinity plugin - Fix bug in CPU masks for some processors.\n -- Modify sacct command to get format from SACCT_FORMAT environment variable.\n -- BGQ - Changed order of library inclusions and fixed incorrect declaration\n    to compile correctly on newer compilers\n -- Fix for not building sview if glib exists on a system but not the gtk libs.\n -- BGQ - Fix for handling a job cleanup on a small block if the job has long\n    since left the system.\n -- Fix race condition in job dependency logic which can result in invalid\n    memory reference.\n\n\n* Changes in SLURM 2.5.2\n========================\n -- Fix advanced reservation recovery logic when upgrading from version 2.4.\n -- BLUEGENE - fix for QOS/Association node limits.\n -- Add missing \"safe\" flag from print of AccountStorageEnforce option.\n -- Fix logic to optimize GRES topology with respect to allocated CPUs.\n -- Add job_submit/all_partitions plugin to set a job's default partition\n    to ALL available partitions in the cluster.\n -- Modify switch/nrt logic to permit build without libnrt.so library.\n -- Handle srun task launch failure without duplicate error messages or abort.\n -- Fix bug in QoS limits enforcement when slurmctld restarts and user not yet\n    added to the QOS list.\n -- Fix issue where sjstat and sjobexitmod was installed in 2 different RPMs.\n -- Fix for job request of multiple partitions in which some partitions lack\n    nodes with required features.\n -- Permit a job to use a QOS they do not have access to if an administrator\n    manually set the job's QOS (previously the job would be rejected).\n -- Make more variables available to job_submit/lua plugin: slurm.MEM_PER_CPU,\n    slurm.NO_VAL, etc.\n -- Fix topology/tree logic when nodes defined in slurm.conf get re-ordered.\n -- In select/cons_res, correct logic to allocate whole sockets to jobs. Work\n    by Magnus Jonsson, Umea University.\n -- In select/cons_res, correct logic when job removed from only some nodes.\n -- Avoid apparent kernel bug in 2.6.32 which apparently is solved in\n    at least 3.5.0.  This avoids a stack overflow when running jobs on\n    more than 120k nodes.\n -- BLUEGENE - If we made a block that isn't runnable because of a overlapping\n    block, destroy it correctly.\n -- Switch/nrt - Dynamically load libnrt.so from within the plugin as needed.\n    This eliminates the need for libnrt.so on the head node.\n -- BLUEGENE - Fix in reservation logic that could cause abort.\n\n* Changes in SLURM 2.5.1\n========================\n -- Correction to hostlist sorting for hostnames that contain two numeric\n    components and the first numeric component has various sizes (e.g.\n    \"rack9blade1\" should come before \"rack10blade1\")\n -- BGQ - Only poll on initialized blocks instead of calling getBlocks on\n    each block independently.\n -- Fix of task/affinity plugin logic for Power7 processors having hyper-\n    threading disabled (cpu mask has gaps).\n -- Fix of job priority ordering with sched/builtin and priority/multifactor.\n    Patch from Chris Read.\n -- CRAY - Fix for setting up the aprun for a large job (+2000 nodes).\n -- Fix for race condition related to compute node boot resulting in node being\n    set down with reason of \"Node <name> unexpectedly rebooted\"\n -- RAPL - Fix for handling errors when opening msr files.\n -- BGQ - Fix for salloc/sbatch to do the correct allocation when asking for\n    -N1 -n#.\n -- BGQ - in emulation make it so we can pretend to run large jobs (>64k nodes)\n -- BLUEGENE - Correct method to update conn_type of a job.\n -- BLUEGENE - Fix issue with preemption when needing to preempt multiple jobs\n    to make one job run.\n -- Fixed issue where if an srun dies inside of an allocation abnormally it\n    would of also killed the allocation.\n -- FRONTEND - fixed issue where if a systems nodes weren't defined in the\n    slurm.conf with NodeAddr's signals going to a step could be handled\n    incorrectly.\n -- If sched/backfill starts a job with a QOS having NO_RESERVE and not job\n    time limit, start it with the partition time limit (or one year if the\n    partition has no time limit) rather than NO_VAL (140 year time limit);\n -- Alter hostlist logic to allocate large grid dynamically instead of on\n    stack.\n -- Change RPC version checks to support version 2.5 slurmctld with version 2.4\n    slurmd daemons.\n -- Correct core reservation logic for use with select/serial plugin.\n -- Exit scontrol command on stdin EOF.\n -- Disable job --exclusive option with select/serial plugin.\n\n* Changes in SLURM 2.5.0\n========================\n -- Add DenyOnLimit flag for QOS to deny jobs at submission time if they\n    request resources that reach a 'Max' limit.\n -- Permit SlurmUser or operator to change QOS of non-pending jobs (e.g.\n    running jobs).\n -- BGQ - move initial poll to beginning of realtime interaction, which will\n    also cause it to run if the realtime server ever goes away.\n\n* Changes in SLURM 2.5.0-rc2\n============================\n -- Modify sbcast logic to survive slurmd daemon restart while file a\n    transmission is in progress.\n -- Add retry logic to munge encode/decode calls. This is needed if the munge\n    deamon is under very heavy load (e.g. with 1000 slurmd daemons per compute\n    node).\n -- Add launch and acct_gather_energy plugins to RPMs.\n -- Restore support for srun \"--mpi=list\" option.\n -- CRAY - Introduce step accounting for a Cray.\n -- Modify srun to abandon I/O 60 seconds after the last task ends. Otherwise\n    an aborted slurmstepd can cause the srun process to hang indefinitely.\n -- ENERGY - RAPL - alter code to close open files (and only open them once\n    where needed)\n -- If the PrologSlurmctld fails, then requeue the job an indefinite number\n    of times instead of only one time.\n\n* Changes in SLURM 2.5.0-rc1\n============================\n -- Added Prolog and Epilog Guide (web page). Based upon work by Jason Sollom,\n    Cray Inc. and used by permission.\n -- Restore gang scheduling functionality. Preemptor was not being scheduled.\n    Fix for bugzilla #3.\n -- Add \"cpu_load\" to node information. Populate CPULOAD in node information\n    reported to Moab cluster manager.\n -- Preempt jobs only when insufficient idle resources exist to start job,\n    regardless of the node weight.\n -- Added priority/multifactor2 plugin based upon ticket distribution system.\n    Work by Janne Blomqvist, Aalto University.\n -- Add SLURM_NODELIST to environment variables available to Prolog and Epilog.\n -- Permit reservations to allow or deny access by account and/or user.\n -- Add ReconfigFlags value of KeepPartState. See \"man slurm.conf\" for details.\n -- Modify the task/cgroup plugin adding a task_pre_launch_priv function and\n    move slurmstepd outside of the step's cgroup. Work by Matthieu Hautreux.\n -- Intel MIC processor support added using gres/mic plugin. BIG thanks to\n    Olli-Pekka Lehto, CSC-IT Center for Science Ltd.\n -- Accounting - Change empty jobacctinfo structs to not actually be used\n    instead of putting 0's into the database we put NO_VALS and have sacct\n    figure out jobacct_gather wasn't used.\n -- Cray - Prevent calling basil_confirm more than once per job using a flag.\n -- Fix bug with topology/tree and job with min-max node count. Now try to\n    get max node count rather than minimizing leaf switches used.\n -- Add AccountingStorageEnforce=safe option to provide method to avoid jobs\n    launching that wouldn't be able to run to completion because of a\n    GrpCPUMins limit.\n -- Add support for RFC 5424 timestamps in logfiles. Disable with configuration\n    option of \"--disable-rfc5424time\". By Janne Blomqvist, Aalto University.\n -- CRAY - Replace srun.pl with launch/aprun plugin to use srun to wrap the\n    aprun process instead of a perl script.\n -- srun - Rename --runjob-opts to --launcher-opts to be used on systems other\n    than BGQ.\n -- Added new DebugFlags - Energy for AcctGatherEnergy plugins.\n -- start deprecation of sacct --dump --fdump\n -- BGQ - added --verbose=OFF when srun --quiet is used\n -- Added acct_gather_energy/rapl plugin to record power consumption by job.\n    Work by Yiannis Georgiou, Martin Perry, et. al., Bull\n\n* Changes in SLURM 2.5.0.pre3\n=============================\n -- Add Google search to all web pages.\n -- Add sinfo -T option to print reservation information. Work by Bill Brophy,\n    Bull.\n -- Force slurmd exit after 2 minute wait, even if threads are hung.\n -- Change node_req field in struct job_resources from 8 to 32 bits so we can\n    run more than 256 jobs per node.\n -- sched/backfill: Improve accuracy of expected job start with respect to\n    reservations.\n -- sinfo partition field size will be set the the length of the longest\n    partition name by default.\n -- Make it so the parse_time will return a valid 0 if given epoch time and\n    set errno == ESLURM_INVALID_TIME_VALUE on error instead.\n -- Correct srun --no-alloc logic when node count exceeds node list or task\n    task count is not a multiple of the node count. Work by Hongjia Cao, NUDT.\n -- Completed integration with IBM Parallel Environment including POE and IBM's\n    NRT switch library.\n\n* Changes in SLURM 2.5.0.pre2\n=============================\n -- When running with multiple slurmd daemons per node, enable specifying a\n    range of ports on a single line of the node configuration in slurm.conf.\n -- Add reservation flag of Part_Nodes to allocate all nodes in a partition to\n    a reservation and automatically change the reservation when nodes are\n    added to or removed from the reservation. Based upon work by\n    Bill Brophy, Bull.\n -- Add support for advanced reservation for specific cores rather than whole\n    nodes. Current limiations: homogeneous cluster, nodes idle when reservation\n    created, and no more than one reservation per node. Code is still under\n    development. Work by Alejandro Lucero Palau, et. al, BSC.\n -- Add DebugFlag of Switch to log switch plugin details.\n -- Correct job node_cnt value in job completion plugin when job fails due to\n    down node. Previously was too low by one.\n -- Add new srun option --cpu-freq to enable user control over the job's CPU\n    frequency and thus it's power consumption. NOTE: cpu frequency is not\n    currently preserved for jobs being suspended and later resumed. Work by\n    Don Albert, Bull.\n -- Add node configuration information about \"boards\" and optimize task\n    placement on minimum number of boards. Work by Rod Schultz, Bull.\n\n* Changes in SLURM 2.5.0.pre1\n=============================\n -- Add new output to \"scontrol show configuration\" of LicensesUsed. Output is\n    \"name:used/total\"\n -- Changed jobacct_gather plugin infrastructure to be cleaner and easier to\n    maintain.\n -- Change license option count separator from \"*\" to \":\" for consistency with\n    the gres option (e.g. \"--licenses=foo:2 --gres=gpu:2\"). The \"*\" will still\n    be accepted, but is no longer documented.\n -- Permit more than 100 jobs to be scheduled per node (new limit is 250\n    jobs).\n -- Restructure of srun code to allow outside programs to utilize existing\n    logic.\n\n* Changes in SLURM 2.4.6\n========================\n -- Correct WillRun authentication logic when issued for non-job owner.\n -- BGQ - fix memory leak\n -- BGQ - Fix to check block for action 'D' if it also has nodes in error.\n\n* Changes in SLURM 2.4.5\n========================\n -- Cray - On job kill requeust, send SIGCONT, SIGTERM, wait KillWait and send\n    SIGKILL. Previously just sent SIGKILL to tasks.\n -- BGQ - Fix issue when running srun outside of an allocation and only\n    specifying the number of tasks and not the number of nodes.\n -- BGQ - validate correct ntasks_per_node\n -- BGQ - when srun -Q is given make runjob be quiet\n -- Modify use of OOM (out of memory protection) for Linux 2.6.36 kernel\n    or later. NOTE: If you were setting the environment variable\n    SLURMSTEPD_OOM_ADJ=-17, it should be set to -1000 for Linux 2.6.36 kernel\n    or later.\n -- BGQ - Fix job step timeout actually happen when done from within an\n    allocation.\n -- Reset node MAINT state flag when a reservation's nodes or flags change.\n -- Accounting - Fix issue where QOS usage was being zeroed out on a\n    slurmctld restart.\n -- BGQ - Add 64 tasks per node as a valid option for srun when used with\n    overcommit.\n -- BLUEGENE - With Dynamic layout mode - Fix issue where if a larger block\n    was already in error and isn't deallocating and underlying hardware goes\n    bad one could get overlapping blocks in error making the code assert when\n    a new job request comes in.\n -- BGQ - handle pending actions on a block better when trying to deallocate it.\n -- Accounting - Fixed issue where if nodenames have changed on a system and\n    you query against that with -N and -E you will get all jobs during that\n    time instead of only the ones running on -N.\n -- BGP - Fix for HTC mode\n -- Accounting - If a job start message fails to the SlurmDBD reset the db_inx\n    so it gets sent again.  This isn't a major problem since the start will\n    happen when the job ends, but this does make things cleaner.\n -- If an salloc is waiting for an allocation to happen and is canceled by the\n    user mark the state canceled instead of completed.\n -- Fix issue in accounting if a user puts a '\\' in their job name.\n -- Accounting - Fix for if asking for users or accounts that were deleted\n    with associations get the deleted associations as well.\n -- BGQ - Handle shared blocks that need to be removed and have jobs running\n    on them.  This should only happen in extreme conditions.\n -- Fix inconsistency for hostlists that have more than 1 range.\n -- BGQ - Add mutex around recovery for the Real Time server to avoid hitting\n    DB2 so hard.\n -- BGQ - If an allocation exists on a block that has a 'D' action on it fail\n    job on future step creation attempts.\n\n* Changes in SLURM 2.4.4\n========================\n -- BGQ - minor fix to make build work in emulated mode.\n -- BGQ - Fix if large block goes into error and the next highest priority jobs\n    are planning on using the block.  Previously it would fail those jobs\n    erroneously.\n -- BGQ - Fix issue when a cnode going to an error (not SoftwareError) state\n    with a job running or trying to run on it.\n -- Execute slurm_spank_job_epilog when there is no system Epilog configured.\n -- Fix for srun --test-only to work correctly with timelimits\n -- BGQ - If a job goes away while still trying to free it up in the\n    database, and the job is running on a small block make sure we free up\n    the correct node count.\n -- BGQ - Logic added to make sure a job has finished on a block before it is\n    purged from the system if its front-end node goes down.\n -- Modify strigger so that a filter option of \"--user=0\" is supported.\n -- Correct --mem-per-cpu logic for core or socket allocations with multiple\n    threads per core.\n -- Fix for older < glibc 2.4 systems to use euidaccess() instead of eaccess().\n -- BLUEGENE - Do not alter a pending job's node count when changing it's\n    partition.\n -- BGQ - Add functionality to make it so we track the actions on a block.\n    This is needed for when a free request is added to a block but there are\n    jobs finishing up so we don't start new jobs on the block since they will\n    fail on start.\n -- BGQ - Fixed InactiveLimit to work correctly to avoid scenarios where a\n    user's pending allocation was started with srun and then for some reason\n    the slurmctld was brought down and while it was down the srun was removed.\n -- Fixed InactiveLimit math to work correctly\n -- BGQ - Add logic to make it so blocks can't use a midplane with a nodeboard\n    in error for passthrough.\n -- BGQ - Make it so if a nodeboard goes in error any block using that midplane\n    for passthrough gets removed on a dynamic system.\n -- BGQ - Fix for printing realtime server debug correctly.\n -- BGQ - Cleaner handling of cnode failures when reported through the runjob\n    interface instead of through the normal method.\n -- smap - spread node information across multiple lines for larger systems.\n -- Cray - Defer salloc until after PrologSlurmctld completes.\n -- Correction to slurmdbd communications failure handling logic, incorrect\n    error codes returned in some cases.\n\n* Changes in SLURM 2.4.3\n========================\n -- Accounting - Fix so complete 32 bit numbers can be put in for a priority.\n -- cgroups - fix if initial directory is non-existent SLURM creates it\n    correctly.  Before the errno wasn't being checked correctly\n -- BGQ - fixed srun when only requesting a task count and not a node count\n    to operate the same way salloc or sbatch did and assign a task per cpu\n    by default instead of task per node.\n -- Fix salloc --gid to work correctly.  Reported by Brian Gilmer\n -- BGQ - fix smap to set the correct default MloaderImage\n -- BLUEGENE - updated documentation.\n -- Close the batch job's environment file when it contains no data to avoid\n    leaking file descriptors.\n -- Fix sbcast's credential to last till the end of a job instead of the\n    previous 20 minute time limit.  The previous behavior would fail for\n    large files 20 minutes into the transfer.\n -- Return ESLURM_NODES_BUSY rather than ESLURM_NODE_NOT_AVAIL error on job\n    submit when required nodes are up, but completing a job or in exclusive\n    job allocation.\n -- Add HWLOC_FLAGS so linking to libslurm works correctly\n -- BGQ - If using backfill and a shared block is running at least one job\n    and a job comes through backfill and can fit on the block without ending\n    jobs don't set an end_time for the running jobs since they don't need to\n    end to start the job.\n -- Initialize bind_verbose when using task/cgroup.\n -- BGQ - Fix for handling backfill much better when sharing blocks.\n -- BGQ - Fix for making small blocks on first pass if not sharing blocks.\n -- BLUEGENE - Remove force of default conn_type instead of leaving NAV\n    when none are requested.  The Block allocator sets it up temporarily so\n    this isn't needed.\n -- BLUEGENE - Fix deadlock issue when dealing with bad hardware if using\n    static blocks.\n -- Fix to mysql plugin during rollup to only query suspended table when jobs\n    reported some suspended time.\n -- Fix compile with glibc 2.16 (Kacper Kowalik)\n -- BGQ - fix for deadlock where a block has error on it and all jobs\n    running on it are preemptable by scheduling job.\n -- proctrack/cgroup: Exclude internal threads from \"scontrol list pids\".\n    Patch from Matthieu Hautreux, CEA.\n -- Memory leak fixed for select/linear when preempting jobs.\n -- Fix if updating begin time of a job to update the eligible time in\n    accounting as well.\n -- BGQ - make it so you can signal steps when signaling the job allocation.\n -- BGQ - Remove extra overhead if a large block has many cnode failures.\n -- Priority/Multifactor - Fix issue with age factor when a job is estimated to\n    start in the future but is able to run now.\n -- CRAY - update to work with ALPS 5.1\n -- BGQ - Handle issue of speed and mutexes when polling instead of using the\n    realtime server.\n -- BGQ - Fix minor sorting issue with sview when sorting by midplanes.\n -- Accounting - Fix for handling per user max node/cpus limits on a QOS\n    correctly for current job.\n -- Update documentation for -/+= when updating a reservation's\n    users/accounts/flags\n -- Update pam module to work if using aliases on nodes instead of actual\n    host names.\n -- Correction to task layout logic in select/cons_res for job with minimum\n    and maximum node count.\n -- BGQ - Put final poll after realtime comes back into service to avoid\n    having the realtime server go down over and over again while waiting\n    for the poll to finish.\n -- task/cgroup/memory - ensure that ConstrainSwapSpace=no is correctly\n    handled. Work by Matthieu Hautreux, CEA.\n -- CRAY - Fix for sacct -N option to work correctly\n -- CRAY - Update documentation to describe installation from rpm instead\n    or previous piecemeal method.\n -- Fix sacct to work with QOS' that have previously been deleted.\n -- Added all available limits to the output of sacctmgr list qos\n\n* Changes in SLURM 2.4.2\n========================\n -- BLUEGENE - Correct potential deadlock issue when hardware goes bad and\n    there are jobs running on that hardware.\n -- If job is submitted to more than one partition, it's partition pointer can\n    be set to an invalid value. This can result in the count of CPUs allocated\n    on a node being bad, resulting in over- or under-allocation of its CPUs.\n    Patch by Carles Fenoy, BSC.\n -- Fix bug in task layout with select/cons_res plugin and --ntasks-per-node\n    option. Patch by Martin Perry, Bull.\n -- BLUEGENE - remove race condition where if a block is removed while waiting\n    for a job to finish on it the number of unused cpus wasn't updated\n    correctly.\n -- BGQ - make sure we have a valid block when creating or finishing a step\n    allocation.\n -- BLUEGENE - If a large block (> 1 midplane) is in error and underlying\n    hardware is marked bad remove the larger block and create a block over\n    just the bad hardware making the other hardware available to run on.\n -- BLUEGENE - Handle job completion correctly if an admin removes a block\n    where other blocks on an overlapping midplane are running jobs.\n -- BLUEGENE - correctly remove running jobs when freeing a block.\n -- BGQ - correct logic to place multiple (< 1 midplane) steps inside a\n    multi midplane block allocation.\n -- BGQ - Make it possible for a multi midplane allocation to run on more\n    than 1 midplane but not the entire allocation.\n -- BGL - Fix for syncing users on block from Tim Wickberg\n -- Fix initialization of protocol_version for some messages to make sure it\n    is always set when sending or receiving a message.\n -- Reset backfilled job counter only when explicitly cleared using scontrol.\n    Patch from Alejandro Lucero Palau, BSC.\n -- BLUEGENE - Fix for handling blocks when a larger block will not free and\n    while it is attempting to free underlying hardware is marked in error\n    making small blocks overlapping with the freeing block.  This only\n    applies to dynamic layout mode.\n -- Cray and BlueGene - Do not treat lack of usable front-end nodes when\n    slurmctld deamon starts as a fatal error. Also preserve correct front-end\n    node for jobs when there is more than one front-end node and the slurmctld\n    daemon restarts.\n -- Correct parsing of srun/sbatch input/output/error file names so that only\n    the name \"none\" is mapped to /dev/null and not any file name starting\n    with \"none\" (e.g. \"none.o\").\n -- BGQ - added version string to the load of the runjob_mux plugin to verify\n    the current plugin has been loaded when using runjob_mux_refresh_config\n -- CGROUPS - Use system mount/umount function calls instead of doing fork\n    exec of mount/umount from Janne Blomqvist.\n -- BLUEGENE - correct start time setup when no jobs are blocking the way\n    from Mark Nelson\n -- Fixed sacct --state=S query to return information about suspended jobs\n    current or in the past.\n -- FRONTEND - Made error warning more apparent if a frontend node isn't\n    configured correctly.\n -- BGQ - update documentation about runjob_mux_refresh_config which works\n    correctly as of IBM driver V1R1M1 efix 008.\n\n* Changes in SLURM 2.4.1\n========================\n -- Fix bug for job state change from 2.3 -> 2.4 job state can now be preserved\n    correctly when transitioning.  This also applies for 2.4.0 -> 2.4.1, no\n    state will be lost. (Thanks to Carles Fenoy)\n\n* Changes in SLURM 2.4.0\n========================\n -- Cray - Improve support for zero compute note resource allocations.\n    Partition used can now be configured with no nodes nodes.\n -- BGQ - make it so srun -i<taskid> works correctly.\n -- Fix parse_uint32/16 to complain if a non-digit is given.\n -- Add SUBMITHOST to job state passed to Moab vial sched/wiki2. Patch by Jon\n    Bringhurst (LANL).\n -- BGQ - Fix issue when running with AllowSubBlockAllocations=Yes without\n    compiling with --enable-debug\n -- Modify scontrol to require \"-dd\" option to report batch job's script. Patch\n    from Don Albert, Bull.\n -- Modify SchedulerParamters option to match documentation: \"bf_res=\"\n    changed to \"bf_resolution=\". Patch from Rod Schultz, Bull.\n -- Fix bug that clears job pending reason field. Patch fron Don Lipari, LLNL.\n -- In etc/init.d/slurm move check for scontrol after sourcing\n    /etc/sysconfig/slurm. Patch from Andy Wettstein, University of Chicago.\n -- Fix in scheduling logic that can delay jobs with min/max node counts.\n -- BGQ - fix issue where if a step uses the entire allocation and then\n    the next step in the allocation only uses part of the allocation it gets\n    the correct cnodes.\n -- BGQ - Fix checking for IO on a block with new IBM driver V1R1M1 previous\n    function didn't always work correctly.\n -- BGQ - Fix issue when a nodeboard goes down and you want to combine blocks\n    to make a larger small block and are running with sub-blocks.\n -- BLUEGENE - Better logic for making small blocks around bad nodeboard/card.\n -- BGQ - When using an old IBM driver cnodes that go into error because of\n    a job kill timeout aren't always reported to the system.  This is now\n    handled by the runjob_mux plugin.\n -- BGQ - Added information on how to setup the runjob_mux to run as SlurmUser.\n -- Improve memory consumption on step layouts with high task count.\n -- BGQ - quiter debug when the real time server comes back but there are\n    still messages we find when we poll but haven't given it back to the real\n    time yet.\n -- BGQ - fix for if a request comes in smaller than the smallest block and\n    we must use a small block instead of a shared midplane block.\n -- Fix issues on large jobs (>64k tasks) to have the correct counter type when\n    packing the step layout structure.\n -- BGQ - fix issue where if a user was asking for tasks and ntasks-per-node\n    but not node count the node count is correctly figured out.\n -- Move logic to always use the 1st alphanumeric node as the batch host for\n    batch jobs.\n -- BLUEGENE - fix race condition where if a nodeboard/card goes down at the\n    same time a block is destroyed and that block just happens to be the\n    smallest overlapping block over the bad hardware.\n -- Fix bug when querying accounting looking for a job node size.\n -- BLUEGENE - fix possible race condition if cleaning up a block and the\n    removal of the job on the block failed.\n -- BLUEGENE - fix issue if a cable was in an error state make it so we can\n    check if a block is still makable if the cable wasn't in error.\n -- Put nodes names in alphabetic order in node table.\n -- If preempted job should have a grace time and preempt mode is not cancel\n    but job is going to be canceled because it is interactive or other reason\n    it now receives the grace time.\n -- BGQ - Modified documents to explain new plugin_flags needed in bg.properties\n    in order for the runjob_mux to run correctly.\n -- BGQ - change linking from libslurm.o to libslurmhelper.la to avoid warning.\n\n* Changes in SLURM 2.4.0.rc1\n=============================\n -- Improve task binding logic by making fuller use of HWLOC library,\n    especially with respect to Opteron 6000 series processors. Work contributed\n    by Komoto Masahiro.\n -- Add new configuration parameter PriorityFlags, based upon work by\n    Carles Fenoy (Barcelona Supercomputer Center).\n -- Modify the step completion RPC between slurmd and slurmstepd in order to\n    eliminate a possible deadlock. Based on work by Matthieu Hautreux, CEA.\n -- Change the owner of slurmctld and slurmdbd log files to the appropriate\n    user. Without this change the files will be created by and owned by the\n    user starting the daemons (likely user root).\n -- Reorganize the slurmstepd logic in order to better support NFS and\n    Kerberos credentials via the AUKS plugin. Work by Matthieu Hautreux, CEA.\n -- Fix bug in allocating GRES that are associated with specific CPUs. In some\n    cases the code allocated first available GRES to job instead of allocating\n    GRES accessible to the specific CPUs allocated to the job.\n -- spank: Add callbacks in slurmd: slurm_spank_slurmd_{init,exit}\n    and job epilog/prolog: slurm_spank_job_{prolog,epilog}\n -- spank: Add spank_option_getopt() function to api\n -- Change resolution of switch wait time from minutes to seconds.\n -- Added CrpCPUMins to the output of sshare -l for those using hard limit\n    accounting.  Work contributed by Mark Nelson.\n -- Added mpi/pmi2 plugin for complete support of pmi2 including acquiring\n    additional resources for newly launched tasks. Contributed by Hongjia Cao,\n    NUDT.\n -- BGQ - fixed issue where if a user asked for a specific node count and more\n    tasks than possible without overcommit the request would be allowed on more\n    nodes than requested.\n -- Add support for new SchedulerParameters of bf_max_job_user, maximum number\n    of jobs to attempt backfilling per user. Work by Bj\u00f8rn-Helge Mevik,\n    University of Oslo.\n -- BLUEGENE - fixed issue where MaxNodes limit on a partition only limited\n    larger than midplane jobs.\n -- Added cpu_run_min to the output of sshare --long.  Work contributed by\n    Mark Nelson.\n -- BGQ - allow regular users to resolve Rack-Midplane to AXYZ coords.\n -- Add sinfo output format option of \"%R\" for partition name without \"*\"\n    appended for default partition.\n -- Cray - Add support for zero compute note resource allocation to run batch\n    script on front-end node with no ALPS reservation. Useful for pre- or post-\n    processing.\n -- Support for cyclic distribution of cpus in task/cgroup plugin from Martin\n    Perry, Bull.\n -- GrpMEM limit for QOSes and associations added Patch from Bj\u00f8rn-Helge Mevik,\n    University of Oslo.\n -- Various performance improvements for up to 500% higher throughput depending\n    upon configuration. Work supported by the Oak Ridge National Laboratory\n    Extreme Scale Systems Center.\n -- Added jobacct_gather/cgroup plugin.  It is not advised to use this in\n    production as it isn't currently complete and doesn't provide an equivalent\n    substitution for jobacct_gather/linux yet. Work by Martin Perry, Bull.\n\n* Changes in SLURM 2.4.0.pre4\n=============================\n -- Add logic to cache GPU file information (bitmap index mapping to device\n    file number) in the slurmd daemon and transfer that information to the\n    slurmstepd whenever a job step is initiated. This is needed to set the\n    appropriate CUDA_VISIBLE_DEVICES environment variable value when the\n    devices are not in strict numeric order (e.g. some GPUs are skipped).\n    Based upon work by Nicolas Bigaouette.\n -- BGQ - Remove ability to make a sub-block with a geometry with one or more\n    of it's dimensions of length 3.  There is a limitation in the IBM I/O\n    subsystem that is problematic with multiple sub-blocks with a dimension\n    of length 3, so we will disallow them to be able to be created.  This\n    mean you if you ask the system for an allocation of 12 c-nodes you will\n    be given 16.  If this is ever fix in BGQ you can remove this patch.\n -- BLUEGENE - Better handling blocks that go into error state or deallocate\n    while jobs are running on them.\n -- BGQ - fix for handling mix of steps running at same time some of which\n    are full allocation jobs, and others that are smaller.\n -- BGQ - fix for core dump after running multiple sub-block jobs on static\n    blocks.\n -- BGQ - fixed sync issue where if a job finishes in SLURM but not in mmcs\n    for a long time after the SLURM job has been flushed from the system\n    we don't have to worry about rebooting the block to sync the system.\n -- BGQ - In scontrol/sview node counts are now displayed with\n    CnodeCount/CnodeErrCount so to point out there are cnodes in an error state\n    on the block.  Draining the block and having it reboot when all jobs are\n    gone will clear up the cnodes in Software Failure.\n -- Change default SchedulerParameters max_switch_wait field value from 60 to\n    300 seconds.\n -- BGQ - catch errors from the kill option of the runjob client.\n -- BLUEGENE - make it so the epilog runs until slurmctld tells it the job is\n    gone.  Previously it had a timelimit which has proven to not be the right\n    thing.\n -- FRONTEND - fix issue where if a compute node was in a down state and\n    an admin updates the node to idle/resume the compute nodes will go\n    instantly to idle instead of idle* which means no response.\n -- Fix regression in 2.4.0.pre3 where number of submitted jobs limit wasn't\n    being honored for QOS.\n -- Cray - Enable logging of BASIL communications with environment variables.\n    Set XML_LOG to enable logging. Set XML_LOG_LOC to specify path to log file\n    or \"SLURM\" to write to SlurmctldLogFile or unset for \"slurm_basil_xml.log\".\n    Patch from Steve Tronfinoff, CSCS.\n -- FRONTEND - if a front end unexpectedly reboots kill all jobs but don't\n    mark front end node down.\n -- FRONTEND - don't down a front end node if you have an epilog error\n -- BLUEGENE - if a job has an epilog error don't down the midplane it was\n    running on.\n -- BGQ - added new DebugFlag (NoRealTime) for only printing debug from\n    state change while the realtime server is running.\n -- Fix multi-cluster mode with sview starting on a non-bluegene cluster going\n    to a bluegene cluster.\n -- BLUEGENE - ability to show Rack Midplane name of midplanes in sview and\n    scontrol.\n\n* Changes in SLURM 2.4.0.pre3\n=============================\n -- Let a job be submitted even if it exceeds a QOS limit. Job will be left\n    in a pending state until the QOS limit or job parameters change. Patch by\n    Phil Eckert, LLNL.\n -- Add sacct support for the option \"--name\". Work by Yuri D'Elia, Center for\n    Biomedicine, EURAC Research, Italy.\n -- BGQ - handle preemption.\n -- Add an srun shepard process to cancel a job and/or step of the srun process\n    is killed abnormally (e.g. SIGKILL).\n -- BGQ - handle deadlock issue when a nodeboard goes into an error state.\n -- BGQ - more thorough handling of blocks with multiple jobs running on them.\n -- Fix man2html process to compile in the build directory instead of the\n    source dir.\n -- Behavior of srun --multi-prog modified so that any program arguments\n    specified on the command line will be appended to the program arguments\n    specified in the program configuration file.\n -- Add new command, sdiag, which reports a variety of job scheduling\n    statistics. Based upon work by Alejandro Lucero Palau, BSC.\n -- BLUEGENE - Added DefaultConnType to the bluegene.conf file.  This makes it\n    so you can specify any connection type you would like (TORUS or MESH) as\n    the default in dynamic mode.  Previously it always defaulted to TORUS.\n -- Made squeue -n and -w options more consistent with salloc, sbatch, srun,\n    and scancel. Patch by Don Lipari, LLNL.\n -- Have sacctmgr remove user records when no associations exist for that user.\n -- Several header file changes for clean build with NetBSD. Patches from\n    Aleksej Saushev.\n -- Fix for possible deadlock in accounting logic: Avoid calling\n    jobacct_gather_g_getinfo() until there is data to read from the socket.\n -- Fix race condition that could generate \"job_cnt_comp underflow\" errors on\n    front-end architectures.\n -- BGQ - Fix issue where a system with missing cables could cause core dump.\n\n* Changes in SLURM 2.4.0.pre2\n=============================\n -- CRAY - Add support for GPU memory allocation using SLURM GRES (Generic\n    RESource) support. Work by Steve Trofinoff, CSCS.\n -- Add support for job allocations with multiple job constraint counts. For\n    example: salloc -C \"[rack1*2&rack2*4]\" ... will allocate the job 2 nodes\n    from rack1 and 4 nodes from rack2. Support for only a single constraint\n    name been added to job step support.\n -- BGQ - Remove old method for marking cnodes down.\n -- BGQ - Remove BGP images from view in sview.\n -- BGQ - print out failed cnodes in scontrol show nodes.\n -- BGQ - Add srun option of \"--runjob-opts\" to pass options to the runjob\n    command.\n -- FRONTEND - handle step launch failure better.\n -- BGQ - Added a mutex to protect the now changing ba_system pointers.\n -- BGQ - added new functionality for sub-block allocations - no preemption\n    for this yet though.\n -- Add --name option to squeue to filter output by job name. Patch from Yuri\n    D'Elia.\n -- BGQ - Added linking to runjob client libary which gives support to totalview\n    to use srun instead of runjob.\n -- Add numeric range checks to scontrol update options. Patch from Phil\n    Eckert, LLNL.\n -- Add ReconfigFlags configuration option to control actions of \"scontrol\n    reconfig\". Patch from Don Albert, Bull.\n -- BGQ - handle reboots with multiple jobs running on a block.\n -- BGQ - Add message handler thread to forward signals to runjob process.\n\n* Changes in SLURM 2.4.0.pre1\n=============================\n -- BGQ - use the ba_geo_tables to figure out the blocks instead of the old\n    algorithm.  The improves timing in the worst cases and simplifies the code\n    greatly.\n -- BLUEGENE - Change to output tools labels from BP to Midplane\n    (i.e. BP List -> MidplaneList).\n -- BLUEGENE - read MPs and BPs from the bluegene.conf\n -- Modify srun's SIGINT handling logic timer (two SIGINTs within one second) to\n    be based microsecond rather than second timer.\n -- Modify advance reservation to accept multiple specific block sizes rather\n    than a single node count.\n -- Permit administrator to change a job's QOS to any value without validating\n    the job's owner has permission to use that QOS. Based upon patch by Phil\n    Eckert (LLNL).\n -- Add trigger flag for a permanent trigger. The trigger will NOT be purged\n    after an event occurs, but only when explicitly deleted.\n -- Interpret a reservation with Nodes=ALL and a Partition specification as\n    reserving all nodes within the specified partition rather than all nodes\n    on the system. Based upon patch by Phil Eckert (LLNL).\n -- Add the ability to reboot all compute nodes after they become idle. The\n    RebootProgram configuration parameter must be set and an authorized user\n    must execute the command \"scontrol reboot_nodes\". Patch from Andriy\n    Grytsenko (Massive Solutions Limited).\n -- Modify slurmdbd.conf parsing to accept DebugLevel strings (quiet, fatal,\n    info, etc.) in addition to numeric values. The parsing of slurm.conf was\n    modified in the same fashion for SlurmctldDebug and SlurmdDebug values.\n    The output of sview and \"scontrol show config\" was also modified to report\n    those values as strings rather than numeric values.\n -- Changed default value of StateSaveLocation configuration parameter from\n    /tmp to /var/spool.\n -- Prevent associations from being deleted if it has any jobs in running,\n    pending or suspended state. Previous code prevented this only for running\n    jobs.\n -- If a job can not run due to QOS or association limits, then do not cancel\n    the job, but leave it pending in a system held state (priority = 1). The\n    job will run when its limits or the QOS/association limits change. Based\n    upon a patch by Phil Ekcert (LLNL).\n -- BGQ - Added logic to keep track of cnodes in an error state inside of a\n    booted block.\n -- Added the ability to update a node's NodeAddr and NodeHostName with\n    scontrol. Also enable setting a node's state to \"future\" using scontrol.\n -- Add a node state flag of CLOUD and save/restore NodeAddr and NodeHostName\n    information for nodes with a flag of CLOUD.\n -- Cray: Add support for job reservations with node IDs that are not in\n    numeric order. Fix for Bugzilla #5.\n -- BGQ - Fix issue with smap -R\n -- Fix association limit support for jobs queued for multiple partitions.\n -- BLUEGENE - fix issue for sub-midplane systems to create a full system\n    block correctly.\n -- BLUEGENE - Added option to the bluegene.conf to tell you are running on\n    a sub midplane system.\n -- Added the UserID of the user issuing the RPC to the job_submit/lua\n    functions.\n -- Fixed issue where if a job ended with ESLURMD_UID_NOT_FOUND and\n    ESLURMD_GID_NOT_FOUND where slurm would be a little over zealous\n    in treating missing a GID or UID as a fatal error.\n -- If job time limit exceeds partition maximum, but job's minimum time limit\n    does not, set job's time limit to partition maximum at allocation time.\n\n* Changes in SLURM 2.3.6\n========================\n -- Fix DefMemPerCPU for partition definitions.\n -- Fix to create a reservation with licenses and no nodes.\n -- Fix issue with assoc_mgr if a bad state file is given and the database\n    isn't up at the time the slurmctld starts, not running the\n    priority/multifactor plugin, and then the database is started up later.\n -- Gres: If a gres has a count of one and an associated file then when doing\n    a reconfiguration, the node's bitmap was not cleared resulting in an\n    underflow upon job termination or removal from scheduling matrix by the\n    backfill scheduler.\n -- Fix race condition in job dependency logic which can result in invalid\n    memory reference.\n\n* Changes in SLURM 2.3.5\n========================\n -- Improve support for overlapping advanced reservations. Patch from\n    Bill Brophy, Bull.\n -- Modify Makefiles for support of Debian hardening flags. Patch from\n    Simon Ruderich.\n -- CRAY: Fix support for configuration with SlurmdTimeout=0 (never mark\n    node that is DOWN in ALPS as DOWN in SLURM).\n -- Fixed the setting of SLURM_SUBMIT_DIR for jobs submitted by Moab (BZ#1467).\n    Patch by Don Lipari, LLNL.\n -- Correction to init.d/slurmdbd exit code for status option. Patch by Bill\n    Brophy, Bull.\n -- When the optional max_time is not specified for --switches=count, the site\n    max (SchedulerParameters=max_switch_wait=seconds) is used for the job.\n    Based on patch from Rod Schultz.\n -- Fix bug in select/cons_res plugin when used with topology/tree and a node\n    range count in job allocation request.\n -- Fixed moab_2_slurmdb.pl script to correctly work for end records.\n -- Add support for new SchedulerParameters of max_depend_depth defining the\n    maximum number of jobs to test for circular dependencies (i.e. job A waits\n    for job B to start and job B waits for job A to start). Default value is\n    10 jobs.\n -- Fix potential race condition if MinJobAge is very low (i.e. 1) and using\n    slurmdbd accounting and running large amounts of jobs (>50 sec).  Job\n    information could be corrupted before it had a chance to reach the DBD.\n -- Fix state restore of job limit set from admin value for min_cpus.\n -- Fix clearing of limit values if an admin removes the limit for max cpus\n    and time limit where it was previously set by an admin.\n -- Fix issue where log message is more than 256 chars and then has a format.\n -- Fix sched/wiki2 to support job account name, gres, partition name, wckey,\n    or working directory that contains \"#\" (a job record separator). Also fix\n    for wckey or working directory that contains a double quote '\\\"'.\n -- CRAY - fix for handling memory requests from user for an allocation.\n -- Add support for switches parameter to the job_submit/lua plugin. Work by\n    Par Andersson, NSC.\n -- Fix to job preemption logic to preempt multiple jobs at the same time.\n -- Fix minor issue where uid and gid were switched in sview for submitting\n    batch jobs.\n -- Fix possible illegal memory reference in slurmctld for job step with\n    relative option. Work by Matthieu Hautreux (CEA).\n -- Reset priority of system held jobs when dependency is satisfied. Work by\n    Don Lipari, LLNL.\n\n* Changes in SLURM 2.3.4\n========================\n -- Set DEFAULT flag in partition structure when slurmctld reads the\n    configuration file. Patch from R\u00e9mi Palancher.\n -- Fix for possible deadlock in accounting logic: Avoid calling\n    jobacct_gather_g_getinfo() until there is data to read from the socket.\n -- Fix typo in accounting when using reservations. Patch from Alejandro\n    Lucero Palau.\n -- Fix to the multifactor priority plugin to calculate effective usage earlier\n    to give a correct priority on the first decay cycle after a restart of the\n    slurmctld. Patch from Martin Perry, Bull.\n -- Permit user root to run a job step for any job as any user. Patch from\n    Didier Gazen, Laboratoire d'Aerologie.\n -- BLUEGENE - fix for not allowing jobs if all midplanes are drained and all\n    blocks are in an error state.\n -- Avoid slurmctld abort due to bad pointer when setting an advanced\n    reservation MAINT flag if it contains no nodes (only licenses).\n -- Fix bug when requeued batch job is scheduled to run on a different node\n    zero, but attemts job launch on old node zero.\n -- Fix bug in step task distribution when nodes are not configured in numeric\n    order. Patch from Hongjia Cao, NUDT.\n -- Fix for srun allocating running within existing allocation with --exclude\n    option and --nnodes count small enough to remove more nodes. Patch from\n    Phil Eckert, LLNL.\n -- Work around to handle certain combinations of glibc/kernel\n    (i.e. glibc-2.14/Linux-3.1) to correctly open the pty of the slurmstepd\n    as the job user. Patch from Mark Grondona, LLNL.\n -- Modify linking to include \"-ldl\" only when needed. Patch from Aleksej\n    Saushev.\n -- Fix smap regression to display nodes that are drained or down correctly.\n -- Several bug fixes and performance improvements with related to batch\n    scripts containing very large numbers of arguments. Patches from Par\n    Andersson, NSC.\n -- Fixed extremely hard to reproduce threading issue in assoc_mgr.\n -- Correct \"scontrol show daemons\" output if there is more than one\n    ControlMachine configured.\n -- Add node read lock where needed in slurmctld/agent code.\n -- Added test for LUA library named \"liblua5.1.so.0\" in addition to\n    \"liblua5.1.so\" as needed by Debian. Patch by Remi Palancher.\n -- Added partition default_time field to job_submit LUA plugin. Patch by\n    Remi Palancher.\n -- Fix bug in cray/srun wrapper stdin/out/err file handling.\n -- In cray/srun wrapper, only include aprun \"-q\" option when srun \"--quiet\"\n    option is used.\n -- BLUEGENE - fix issue where if a small block was in error it could hold up\n    the queue when trying to place a larger than midplane job.\n -- CRAY - ignore all interactive nodes and jobs on interactive nodes.\n -- Add new job state reason of \"FrontEndDown\" which applies only to Cray and\n    IBM BlueGene systems.\n -- Cray - Enable configure option of \"--enable-salloc-background\" to permit\n    the srun and salloc commands to be executed in the background. This does\n    NOT remove the ALPS limitation that only one job reservation can be created\n    for each Linux session ID.\n -- Cray - For srun wrapper when creating a job allocation, set the default job\n    name to the executable file's name.\n -- Add support for Cray ALPS 5.0.0\n -- FRONTEND - if a front end unexpectedly reboots kill all jobs but don't\n    mark front end node down.\n -- FRONTEND - don't down a front end node if you have an epilog error.\n -- Cray - fix for if a frontend slurmd was started after the slurmctld had\n    already pinged it on startup the unresponding flag would be removed from\n    the frontend node.\n -- Cray - Fix issue on smap not displaying grid correctly.\n -- Fixed minor memory leak in sview.\n\n* Changes in SLURM 2.3.3\n========================\n -- Fix task/cgroup plugin error when used with GRES. Patch by Alexander\n    Bersenev (Institute of Mathematics and Mechanics, Russia).\n -- Permit pending job exceeding a partition limit to run if its QOS flag is\n    modified to permit the partition limit to be exceeded. Patch from Bill\n    Brophy, Bull.\n -- BLUEGENE - Fixed preemption issue.\n -- sacct search for jobs using filtering was ignoring wckey filter.\n -- Fixed issue with QOS preemption when adding new QOS.\n -- Fixed issue with comment field being used in a job finishing before it\n    starts in accounting.\n -- Add slashes in front of derived exit code when modifying a job.\n -- Handle numeric suffix of \"T\" for terabyte units. Patch from John Thiltges,\n    University of Nebraska-Lincoln.\n -- Prevent resetting a held job's priority when updating other job parameters.\n    Patch from Alejandro Lucero Palau, BSC.\n -- Improve logic to import a user's environment. Needed with --get-user-env\n    option used with Moab. Patch from Mark Grondona, LLNL.\n -- Fix bug in sview layout if node count less than configured grid_x_width.\n -- Modify PAM module to prefer to use SLURM library with same major release\n    number that it was built with.\n -- Permit gres count configuration of zero.\n -- Fix race condition where sbcast command can result in deadlock of slurmd\n    daemon. Patch by Don Albert, Bull.\n -- Fix bug in srun --multi-prog configuration file to avoid printing duplicate\n    record error when \"*\" is used at the end of the file for the task ID.\n -- Let operators see reservation data even if \"PrivateData=reservations\" flag\n    is set in slurm.conf. Patch from Don Albert, Bull.\n -- Added new sbatch option \"--export-file\" as needed for latest version of\n    Moab. Patch from Phil Eckert, LLNL.\n -- Fix for sacct printing CPUTime(RAW) where the the is greater than a 32 bit\n    number.\n -- Fix bug in --switch option with topology resulting in bad switch count use.\n    Patch from Alejandro Lucero Palau (Barcelona Supercomputer Center).\n -- Fix PrivateFlags bug when using Priority Multifactor plugin.  If using sprio\n    all jobs would be returned even if the flag was set.\n    Patch from Bill Brophy, Bull.\n -- Fix for possible invalid memory reference in slurmctld in job dependency\n    logic. Patch from Carles Fenoy (Barcelona Supercomputer Center).\n\n* Changes in SLURM 2.3.2\n========================\n -- Add configure option of \"--without-rpath\" which builds SLURM tools without\n    the rpath option, which will work if Munge and BlueGene libraries are in\n    the default library search path and make system updates easier.\n -- Fixed issue where if a job ended with ESLURMD_UID_NOT_FOUND and\n    ESLURMD_GID_NOT_FOUND where slurm would be a little over zealous\n    in treating missing a GID or UID as a fatal error.\n -- Backfill scheduling - Add SchedulerParameters configuration parameter of\n    \"bf_res\" to control the resolution in the backfill scheduler's data about\n    when jobs begin and end. Default value is 60 seconds (used to be 1 second).\n -- Cray - Remove the \"family\" specification from the GPU reservation request.\n -- Updated set_oomadj.c, replacing deprecated oom_adj reference with\n    oom_score_adj\n -- Fix resource allocation bug, generic resources allocation was ignoring the\n    job's ntasks_per_node and cpus_per_task parameters. Patch from Carles\n    Fenoy, BSC.\n -- Avoid orphan job step if slurmctld is down when a job step completes.\n -- Fix Lua link order, patch from P\u00e4r Andersson, NSC.\n -- Set SLURM_CPUS_PER_TASK=1 when user specifies --cpus-per-task=1.\n -- Fix for fatal error managing GRES. Patch by Carles Fenoy, BSC.\n -- Fixed race condition when using the DBD in accounting where if a job\n    wasn't started at the time the eligible message was sent but started\n    before the db_index was returned information like start time would be lost.\n -- Fix issue in accounting where normalized shares could be updated\n    incorrectly when getting fairshare from the parent.\n -- Fixed if not enforcing associations  but want QOS support for a default\n    qos on the cluster to fill that in correctly.\n -- Fix in select/cons_res for \"fatal: cons_res: sync loop not progressing\"\n    with some configurations and job option combinations.\n -- BLUEGNE - Fixed issue with handling HTC modes and rebooting.\n\n* Changes in SLURM 2.3.1\n========================\n -- Do not remove the backup slurmctld's pid file when it assumes control, only\n    when it actually shuts down. Patch from Andriy Grytsenko (Massive Solutions\n    Limited).\n -- Avoid clearing a job's reason from JobHeldAdmin or JobHeldUser when it is\n    otherwise updated using scontrol or sview commands. Patch based upon work\n    by Phil Eckert (LLNL).\n -- BLUEGENE - Fix for if changing the defined blocks in the bluegene.conf and\n    jobs happen to be running on blocks not in the new config.\n -- Many cosmetic modifications to eliminate warning message from GCC version\n    4.6 compiler.\n -- Fix for sview reservation tab when finding correct reservation.\n -- Fix for handling QOS limits per user on a reconfig of the slurmctld.\n -- Do not treat the absence of a gres.conf file as a fatal error on systems\n    configured with GRES, but set GRES counts to zero.\n -- BLUEGENE - Update correctly the state in the reason of a block if an\n    admin sets the state to error.\n -- BLUEGENE - handle reason of blocks in error more correctly between\n    restarts of the slurmctld.\n -- BLUEGENE - Fix minor potential memory leak when setting block error reason.\n -- BLUEGENE - Fix if running in Static/Overlap mode and full system block\n    is in an error state, won't deny jobs.\n -- Fix for accounting where your cluster isn't numbered in counting order\n    (i.e. 1-9,0 instead of 0-9).  The bug would cause 'sacct -N nodename' to\n    not give correct results on these systems.\n -- Fix to GRES allocation logic when resources are associated with specific\n    CPUs on a node. Patch from Steve Trofinoff, CSCS.\n -- Fix bugs in sched/backfill with respect to QOS reservation support and job\n    time limits. Patch from Alejandro Lucero Palau (Barcelona Supercomputer\n    Center).\n -- BGQ - fix to set up corner correctly for sub block jobs.\n -- Major re-write of the CPU Management User and Administrator Guide (web\n    page) by Martin Perry, Bull.\n -- BLUEGENE - If removing blocks from system that once existed cleanup of old\n    block happens correctly now.\n -- Prevent slurmctld crashing with configuration of MaxMemPerCPU=0.\n -- Prevent job hold by operator or account coordinator of his own job from\n    being an Administrator Hold rather than User Hold by default.\n -- Cray - Fix for srun.pl parsing to avoid adding spaces between option and\n    argument (e.g. \"-N2\" parsed properly without changing to \"-N 2\").\n -- Major updates to cgroup support by Mark Grondona (LLNL) and Matthieu\n    Hautreux (CEA) and Sam Lang. Fixes timing problems with respect to the\n    task_epilog. Allows cgroup mount point to be configurable. Added new\n    configuration parameters MaxRAMPercent and MaxSwapPercent. Allow cgroup\n    configuration parameters that are precentages to be floating point.\n -- Fixed issue where sview wasn't displaying correct nice value for jobs.\n -- Fixed issue where sview wasn't displaying correct min memory per node/cpu\n    value for jobs.\n -- Disable some SelectTypeParameters for select/linear that aren't compatible.\n -- Move slurm_select_init to proper place to avoid loading multiple select\n    plugins in the slurmd.\n -- BGQ - Include runjob_plugin.so in the bluegene rpm.\n -- Report correct job \"Reason\" if needed nodes are DOWN, DRAINED, or\n    NOT_RESPONDING, \"Resources\" rather than \"PartitionNodeLimit\".\n -- BLUEGENE - Fixed issues with running on a sub-midplane system.\n -- Added some missing calls to allow older versions of SLURM to talk to newer.\n -- BGQ - allow steps to be ran.\n -- Do not attempt to run HeathCheckProgram on powered down nodes. Patch from\n    Ramiro Alba, Centre Tecnol\u00f2gic de Tranfer\u00e8ncia de Calor, Spain.\n\n* Changes in SLURM 2.3.0-2\n==========================\n -- Fix for memory issue inside sview.\n -- Fix issue where if a job was pending and the slurmctld was restarted a\n    variable wasn't initialized in the job structure making it so that job\n    wouldn't run.\n\n* Changes in SLURM 2.3.0\n========================\n -- BLUEGENE - make sure we only set the jobinfo_select start_loc on a job\n    when we are on a small block, not a regular one.\n -- BGQ - fix issue where not copying the correct amount of memory.\n -- BLUEGENE - fix clean start if jobs were running when the slurmctld was\n    shutdown and then the system size changed.  This would probably only happen\n    if you were emulating a system.\n -- Fix sview for calling a cray system from a non-cray system to get the\n    correct geometry of the system.\n -- BLUEGENE - fix to correctly import pervious version of block state file.\n -- BLUEGENE - handle loading better when doing a clean start with static\n    blocks.\n -- Add sinfo format and sort option \"%n\" for NodeHostName and \"%o\" for\n    NodeAddr.\n -- If a job is deferred due to partition limits, then re-test those limits\n    after a partition is modified. Patch from Don Lipari.\n -- Fix bug which would crash slurmcld if job's owner (not root) tries to clear\n    a job's licenses by setting value to \"\".\n -- Cosmetic fix for printing out debug info in the priority plugin.\n -- In sview when switching from a bluegene machine to a regular linux cluster\n    and vice versa the node->base partition lists will be displayed if setup\n    in your .slurm/sviewrc file.\n -- BLUEGENE - Fix for creating full system static block on a BGQ system.\n -- BLUEGENE - Fix deadlock issue if toggling between Dynamic and Static block\n    allocation with jobs running on blocks that don't exist in the static\n    setup.\n -- BLUEGENE - Modify code to only give HTC states to BGP systems and not\n    allow them on Q systems.\n -- BLUEGENE - Make it possible for an admin to define multiple dimension\n    conn_types in a block definition.\n -- BGQ - Alter tools to output multiple dimensional conn_type.\n\n* Changes in SLURM 2.3.0.rc2\n============================\n -- With sched/wiki or sched/wiki2 (Maui or Moab scheduler), insure that a\n    requeued job's priority is reset to zero.\n -- BLUEGENE - fix to run steps correctly in a BGL/P emulated system.\n -- Fixed issue where if there was a network issue between the slurmctld and\n    the DBD where both remained up but were disconnected the slurmctld would\n    get registered again with the DBD.\n -- Fixed issue where if the DBD connection from the ctld goes away because of\n    a POLLERR the dbd_fail callback is called.\n -- BLUEGENE - Fix to smap command-line mode display.\n -- Change in GRES behavior for job steps: A job step's default generic\n    resource allocation will be set to that of the job. If a job step's --gres\n    value is set to \"none\" then none of the generic resources which have been\n    allocated to the job will be allocated to the job step.\n -- Add srun environment value of SLURM_STEP_GRES to set default --gres value\n    for a job step.\n -- Require SchedulerTimeSlice configuration parameter to be at least 5 seconds\n    to avoid thrashing slurmd daemon.\n -- Cray - Fix to make nodes state in accounting consistent with state set by\n    ALPS.\n -- Cray - A node DOWN to ALPS will be marked DOWN to SLURM only after reaching\n    SlurmdTimeout. In the interim, the node state will be NO_RESPOND. This\n    change makes behavior makes SLURM handling of the node DOWN state more\n    consistent with ALPS. This change effects only Cray systems.\n -- Cray - Fix to work with 4.0.* instead of just 4.0.0\n -- Cray - Modify srun/aprun wrapper to map --exclusive to -F exclusive and\n    --share to -F share. Note this does not consider the partition's Shared\n    configuration, so it is an imperfect mapping of options.\n -- BLUEGENE - Added notice in the print config to tell if you are emulated\n    or not.\n -- BLUEGENE - Fix job step scalability issue with large task count.\n -- BGQ - Improved c-node selection when asked for a sub-block job that\n    cannot fit into the available shape.\n -- BLUEGENE - Modify \"scontrol show step\" to show  I/O nodes (BGL and BGP) or\n    c-nodes (BGQ) allocated to each step. Change field name from \"Nodes=\" to\n    \"BP_List=\".\n -- Code cleanup on step request to get the correct select_jobinfo.\n -- Memory leak fixed for rolling up accounting with down clusters.\n -- BGQ - fix issue where if first job step is the entire block and then the\n    next parallel step is ran on a sub block, SLURM won't over subscribe cnodes.\n -- Treat duplicate switch name in topology.conf as fatal error. Patch from Rod\n    Schultz, Bull\n -- Minor update to documentation describing the AllowGroups option for a\n    partition in the slurm.conf.\n -- Fix problem with _job_create() when not using qos's.  It makes\n    _job_create() consistent with similar logic in select_nodes().\n -- GrpCPURunMins in a QOS flushed out.\n -- Fix for squeue -t \"CONFIGURING\" to actually work.\n -- CRAY - Add cray.conf parameter of SyncTimeout, maximum time to defer job\n    scheduling if SLURM node or job state are out of synchronization with ALPS.\n -- If salloc was run as interactive, with job control, reset the foreground\n    process group of the terminal to the process group of the parent pid before\n    exiting. Patch from Don Albert, Bull.\n -- BGQ - set up the corner of a sub block correctly based on a relative\n    position in the block instead of absolute.\n -- BGQ - make sure the recently added select_jobinfo of a step launch request\n    isn't sent to the slurmd where environment variables would be overwritten\n    incorrectly.\n\n* Changes in SLURM 2.3.0.rc1\n============================\n -- NOTE THERE HAVE BEEN NEW FIELDS ADDED TO THE JOB AND PARTITION STATE SAVE\n    FILES AND RPCS. PENDING AND RUNNING JOBS WILL BE LOST WHEN UPGRADING FROM\n    EARLIER VERSION 2.3 PRE-RELEASES AND RPCS WILL NOT WORK WITH EARLIER\n    VERSIONS.\n -- select/cray: Add support for Accelerator information including model and\n    memory options.\n -- Cray systems: Add support to suspend/resume salloc command to insure that\n    aprun does not get initiated when the job is suspended. Processes suspended\n    and resumed are determined by using process group ID and parent process ID,\n    so some processes may be missed. Since salloc runs as a normal user, it's\n    ability to identify processes associated with a job is limited.\n -- Cray systems: Modify smap and sview to display all nodes even if multiple\n    nodes exist at each coordinate.\n -- Improve efficiency of select/linear plugin with topology/tree plugin\n    configured, Patch by Andriy Grytsenko (Massive Solutions Limited).\n -- For front-end architectures on which job steps are run (emulated Cray and\n    BlueGene systems only), fix bug that would free memory still in use.\n -- Add squeue support to display a job's license information. Patch by Andy\n    Roosen (University of Deleware).\n -- Add flag to the select APIs for job suspend/resume indicating if the action\n    is for gang scheduling or an explicit job suspend/resume by the user. Only\n    an explicit job suspend/resume will reset the job's priority and make\n    resources exclusively held by the job available to other jobs.\n -- Fix possible invalid memory reference in sched/backfill. Patch by Andriy\n    Grytsenko (Massive Solutions Limited).\n -- Add select_jobinfo to the task launch RPC. Based upon patch by Andriy\n    Grytsenko (Massive Solutions Limited).\n -- Add DefMemPerCPU/Node and MaxMemPerCPU/Node to partition configuration.\n    This improves flexibility when gang scheduling only specific partitions.\n -- Added new enums to print out when a job is held by a QOS instead of an\n    association limit.\n -- Enhancements to sched/backfill performance with select/cons_res plugin.\n    Patch from Bj\u00f8rn-Helge Mevik, University of Oslo.\n -- Correct job run time reported by smap for suspended jobs.\n -- Improve job preemption logic to avoid preempting more jobs than needed.\n -- Add contribs/arrayrun tool providing support for job arrays. Contributed by\n    Bj\u00f8rn-Helge Mevik, University of Oslo. NOTE: Not currently packaged as RPM\n    and manual file editing is required.\n -- When suspending a job, wait 2 seconds instead of 1 second between sending\n    SIGTSTP and SIGSTOP. Some MPI implementation were not stopping within the\n    1 second delay.\n -- Add support for managing devices based upon Linux cgroup container. Based\n    upon patch by Yiannis Georgiou, Bull.\n -- Fix memory buffering bug if a AllowGroups parameter of a partition has 100\n    or more users. Patch by Andriy Grytsenko (Massive Solutions Limited).\n -- Fix bug in generic resource tracking of gres associated with specific CPUs.\n    Resources were being over-allocated.\n -- On systems with front-end nodes (IBM BlueGene and Cray) limit batch jobs to\n    only one CPU of these shared resources.\n -- Set SLURM_MEM_PER_CPU or SLURM_MEM_PER_NODE environment variables for both\n    interactive (salloc) and batch jobs if the job has a memory limit. For Cray\n    systems also set CRAY_AUTO_APRUN_OPTIONS environment variable with the\n    memory limit.\n -- Fix bug in select/cons_res task distribution logic when tasks-per-node=0.\n    Patch from Rod Schultz, Bull.\n -- Restore node configuration information (CPUs, memory, etc.) for powered\n    down when slurmctld daemon restarts rather than waiting for the node to be\n    restored to service and getting the information from the node (NOTE: Only\n    relevent if FastSchedule=0).\n -- For Cray systems with the srun2aprun wrapper, rebuild the srun man page\n    identifying the srun optioins which are valid on that system.\n -- BlueGene: Permit users to specify a separate connection type for each\n    dimension (e.g. \"--conn-type=torus,mesh,torus\").\n -- Add the ability for a user to limit the number of leaf switches in a job's\n    allocation using the --switch option of salloc, sbatch and srun. There is\n    also a new SchedulerParameters value of max_switch_wait, which a SLURM\n    administrator can used to set a maximum job delay and prevent a user job\n    from blocking lower priority jobs for too long. Based on work by Rod\n    Schultz, Bull.\n\n* Changes in SLURM 2.3.0.pre6\n=============================\n -- NOTE: THERE HAS BEEN A NEW FIELD ADDED TO THE CONFIGURATION RESPONSE RPC\n    AS SHOWN BY \"SCONTROL SHOW CONFIG\". THIS FUNCTION WILL ONLY WORK WHEN THE\n    SERVER AND CLIENT ARE BOTH RUNNING SLURM VERSION 2.3.0.pre6\n -- Modify job expansion logic to support licenses, generic resources, and\n    currently running job steps.\n -- Added an rpath if using the --with-munge option of configure.\n -- Add support for multiple sets of DEFAULT node, partition, and frontend\n    specifications in slurm.conf so that default values can be changed mulitple\n    times as the configuration file is read.\n -- BLUEGENE - Improved logic to place small blocks in free space before freeing\n    larger blocks.\n -- Add optional argument to srun's --kill-on-bad-exit so that user can set\n    its value to zero and override a SLURM configuration parameter of\n    KillOnBadExit.\n -- Fix bug in GraceTime support for preempted jobs that prevented proper\n    operation when more than one job was being preempted. Based on patch from\n    Bill Brophy, Bull.\n -- Fix for running sview from a non-bluegene cluster to a bluegene cluster.\n    Regression from pre5.\n -- If job's TMPDIR environment is not set or is not usable, reset to \"/tmp\".\n    Patch from Andriy Grytsenko (Massive Solutions Limited).\n -- Remove logic for defunct RPC: DBD_GET_JOBS.\n -- Propagate DebugFlag changes by scontrol to the plugins.\n -- Improve accuracy of REQUEST_JOB_WILL_RUN start time with respect to higher\n    priority pending jobs.\n -- Add -R/--reservation option to squeue command as a job filter.\n -- Add scancel support for --clusters option.\n -- Note that scontrol and sprio can only support a single cluster at one time.\n -- Add support to salloc for a new environment variable SALLOC_KILL_CMD.\n -- Add scontrol ability to increment or decrement a job or step time limit.\n -- Add support for SLURM_TIME_FORMAT environment variable to control time\n    stamp output format. Work by Gerrit Renker, CSCS.\n -- Fix error handling in mvapich plugin that could cause srun to enter an\n    infinite loop under rare circumstances.\n -- Add support for multiple task plugins. Patch from Andriy Grytsenko (Massive\n    Solutions Limited).\n -- Addition of per-user node/cpu limits for QOS's. Patch from Aaron Knister,\n    UMBC.\n -- Fix logic for multiple job resize operations.\n -- BLUEGENE - many fixes to make things work correctly on a L/P system.\n -- Fix bug in layout of job step with --nodelist option plus node count. Old\n    code could allocate too few nodes.\n\n* Changes in SLURM 2.3.0.pre5\n=============================\n -- NOTE: THERE HAS BEEN A NEW FIELD ADDED TO THE JOB STATE FILE. UPGRADES FROM\n    VERSION 2.3.0-PRE4 WILL RESULT IN LOST JOBS UNLESS THE \"orig_dependency\"\n    FIELD IS REMOVED FROM JOB STATE SAVE/RESTORE LOGIC. ON CRAY SYSTEMS A NEW\n    \"confirm_cookie\" FIELD WAS ADDED AND HAS THE SAME EFFECT OF DISABLING JOB\n    STATE RESTORE.\n -- BLUEGENE - Improve speed of start up when removing blocks at the beginning.\n -- Correct init.d/slurm status to have non-zero exit code if ANY Slurm\n    damon that should be running on the node is not running. Patch from Rod\n    Schulz, Bull.\n -- Improve accuracy of response to \"srun --test-only jobid=#\".\n -- Fix bug in front-end configurations which reports job_cnt_comp underflow\n    errors after slurmctld restarts.\n -- Eliminate \"error from _trigger_slurmctld_event in backup.c\" due to lack of\n    event triggers.\n -- Fix logic in BackupController to properly recover front-end node state and\n    avoid purging active jobs.\n -- Added man pages to html pages and the new cpu_management.html page.\n    Submitted by Martin Perry / Rod Schultz, Bull.\n -- Job dependency information will only show the currently active dependencies\n    rather than the original dependencies. From Dan Rusak, Bull.\n -- Add RPCs to get the SPANK environment variables from the slurmctld daemon.\n    Patch from Andrej N. Gritsenko.\n -- Updated plugins/task/cgroup/task_cgroup_cpuset.c to support newer\n    HWLOC_API_VERSION.\n -- Do not build select/bluegene plugin if C++ compiler is not installed.\n -- Add new configure option --with-srun2aprun to build an srun command\n    which is a wrapper over Cray's aprun command and supports many srun\n    options. Without this option, the srun command will advise the user\n    to use the aprun command.\n -- Change container ID supported by proctrack plugin from 32-bit to 64-bit.\n -- Added contribs/cray/libalps_test_programs.tar.gz with tools to validate\n    SLURM's logic used to support Cray systems.\n -- Create RPM for srun command that is a wrapper for the Cray/ALPS aprun\n    command. Dependent upon .rpmmacros parameter of \"%_with_srun2aprun\".\n -- Add configuration parameter MaxStepCount to limit effect of bad batch\n    scripts.\n -- Moving to github\n -- Fix for handling a 2.3 system talking to a 2.2 slurmctld.\n -- Add contribs/lua/job_submit.license.lua script. Update job_submit and Lua\n    related documentation.\n -- Test if _make_batch_script() is called with a NULL script.\n -- Increase hostlist support from 24k to 64k nodes.\n -- Renamed the Accounting Storage database's \"DerivedExitString\" job field to\n    \"Comment\".  Provided backward compatible support for \"DerivedExitString\" in\n    the sacctmgr tool.\n -- Added the ability to save the job's comment field to the Accounting\n    Storage db (to the formerly named, \"DerivedExitString\" job field).  This\n    behavior is enabled by a new slurm.conf parameter:\n    AccountingStoreJobComment.\n -- Test if _make_batch_script() is called with a NULL script.\n -- Increase hostlist support from 24k to 64k nodes.\n -- Fix srun to handle signals correctly when waiting for a step creation.\n -- Preserve the last job ID across slurmctld daemon restarts even if the job\n    state file can not be fully recovered.\n -- Made the hostlist functions be able to arbitrarily handle any size\n    dimension no matter what the size of the cluster is in dimensions.\n\n* Changes in SLURM 2.3.0.pre4\n=============================\n -- Add GraceTime to Partition and QOS data structures. Preempted jobs will be\n    given this time interval before termination. Work by Bill Brophy, Bull.\n -- Add the ability for scontrol and sview to modify slurmctld DebugFlags\n    values.\n -- Various Cray-specific patches:\n    - Fix a bug in distinguishing XT from XE.\n    - Avoids problems with empty nodenames on Cray.\n    - Check whether ALPS is hanging on to nodes, which happens if ALPS has not\n      yet cleaned up the node partition.\n    - Stops select/cray from clobbering node_ptr->reason.\n    - Perform 'safe' release of ALPS reservations using inventory and apkill.\n    - Compile-time sanity check for the apbasil and apkill files.\n    - Changes error handling in do_basil_release() (called by\n      select_g_job_fini()).\n    - Warn that salloc --no-shell option is not supported on Cray systems.\n -- Add a reservation flag of \"License_Only\". If set, then jobs using the\n    reservation may use the licenses associated with it plus any compute nodes.\n    Otherwise the job is limited to the compute nodes associated with the\n    reservation.\n -- Change slurm.conf node configuration parameter from \"Procs\" to \"CPUs\".\n    Both parameters will be supported for now.\n -- BLUEGENE - fix for when user requests only midplane names with no count at\n    job submission time to process the node count correctly.\n -- Fix job step resource allocation problem when both node and tasks counts\n    are specified. New logic selects nodes with larger CPU counts as needed.\n -- BGQ - make it so srun wraps runjob (still under construction, but works\n    for most cases)\n -- Permit a job's QOS and Comment field to both change in a single RPC. This\n    was previously disabled since Moab stored the QOS within the Comment field.\n -- Add support for jobs to expand in size. Submit additional batch job with\n    the option \"--dependency=expand:<jobid>\". See web page \"faq.html#job_size\"\n    for details. Restrictions to be removed in the future.\n -- Added --with-alps-emulation to configure, and also an optional cray.conf\n    to setup alps location and database information.\n -- Modify PMI data types from 16-bits to 32-bits in order to support MPICH2\n    jobs with more than 65,536 tasks. Patch from Hongjia Cao, NUDT.\n -- Set slurmd's soft process CPU limit equal to it's hard limit and notify the\n    user if the limit is not infinite.\n -- Added proctrack/cgroup and task/cgroup plugins from Matthieu Hautreux, CEA.\n -- Fix slurmctld restart logic that could leave nodes in UNKNOWN state for a\n    longer time than necessary after restart.\n\n* Changes in SLURM 2.3.0.pre3\n=============================\n -- BGQ - Appears to work correctly in emulation mode, no sub blocks just yet.\n -- Minor typos fixed\n -- Various bug fixes for Cray systems.\n -- Fix bug that when setting a compute node to idle state, it was failing to\n    set the systems up_node_bitmap.\n -- BLUEGENE - code reorder\n -- BLUEGENE - Now only one select plugin for all Bluegene systems.\n -- Modify srun to set the SLURM_JOB_NAME environment variable when srun is\n    used to create a new job allocation. Not set when srun is used to create a\n    job step within an existing job allocation.\n -- Modify init.d/slurm script to start multiple slurmd daemons per compute\n    node if so configured. Patch from Matthieu Hautreux, CEA.\n -- Change license data structure counters from uint16_t to uint32_t to support\n    larger license counts.\n\n* Changes in SLURM 2.3.0.pre2\n=============================\n -- Log a job's requeue or cancellation due to preemption to that job's stderr:\n    \"*** JOB 65547 CANCELLED AT 2011-01-21T12:59:33 DUE TO PREEMPTION ***\".\n -- Added new job termination state of JOB_PREEMPTED, \"PR\" or \"PREEMPTED\" to\n    indicate job termination was due to preemption.\n -- Optimize advanced reservations resource selection for computer topology.\n    The logic has been added to select/linear and select/cons_res, but will\n    not be enabled until the other select plugins are modified.\n -- Remove checkpoint/xlch plugin.\n -- Disable deletion of partitions that have unfinished jobs (pending,\n    running or suspended states). Patch from Martin Perry, BULL.\n -- In sview, disable the sorting of node records by name at startup for\n    clusters over 1000 nodes. Users can enable this by selecting the \"Name\"\n    tab. This change dramatically improves scalability of sview.\n -- Report error when trying to change a node's state from scontrol for Cray\n    systems.\n -- Do not attempt to read the batch script for non-batch jobs. This patch\n    eliminates some inappropriate error messages.\n -- Preserve NodeHostName when reordering nodes due to system topology.\n -- On Cray/ALPS systems  do node inventory before scheduling jobs.\n -- Disable some salloc options on Cray systems.\n -- Disable scontrol's wait_job command on Cray systems.\n -- Disable srun command on native Cray/ALPS systems.\n -- Updated configure option \"--enable-cray-emulation\" (still under\n    development) to emulate a cray XT/XE system, and auto-detect a real\n    Cray XT/XE systems (removed no longer needed --enable-cray configure\n    option).  Building on native Cray systems requires the\n    cray-MySQL-devel-enterprise rpm and expat XML parser library/headers.\n\n* Changes in SLURM 2.3.0.pre1\n=============================\n -- Added that when a slurmctld closes the connection to the database it's\n    registered host and port are removed.\n -- Added flag to slurmdbd.conf TrackSlurmctldDown where if set will mark idle\n    resources as down on a cluster when a slurmctld disconnects or is no\n    longer reachable.\n -- Added support for more than one front-end node to run slurmd on\n    architectures where the slurmd does not execute on the compute nodes\n    (e.g. BlueGene). New configuration parameters FrontendNode and FrontendAddr\n    added. See \"man slurm.conf\" for more information.\n -- With the scontrol show job command when using the --details option, show\n    a batch job's script.\n -- Add ability to create reservations or partitions and submit batch jobs\n    using sview. Also add the ability to delete reservations and partitions.\n -- Added new configuration parameter MaxJobId. Once reached, restart job ID\n    values at FirstJobId.\n -- When restarting slurmctld with priority/basic, increment all job priorities\n    so the highest job priority becomes TOP_PRIORITY.\n\n* Changes in SLURM 2.2.8\n========================\n -- Prevent background salloc disconnecting terminal at termination. Patch by\n    Don Albert, Bull.\n -- Fixed issue where preempt mode is skipped when creating a QOS. Patch by\n    Bill Brophy, Bull.\n -- Fixed documention (html) for PriorityUsageResetPeriod to match that in the\n    man pages. Patch by Nancy Kritkausky, Bull.\n\n* Changes in SLURM 2.2.7\n========================\n -- Eliminate zombie process created if salloc exits with stopped child\n    process. Patch from Gerrit Renker, CSCS.\n -- With default configuration on non-Cray systems, enable salloc to be\n    spawned as a background process. Based upon work by Don Albert (Bull) and\n    Gerrit Renker (CSCS).\n -- Fixed Regression from 2.2.4 in accounting where an inherited limit\n    would not be set correctly in the added child association.\n -- Fixed issue with accounting when asking for jobs with a hostlist.\n -- Avoid clearing a node's Arch, OS, BootTime and SlurmdStartTime when\n    \"scontrol reconfig\" is run. Patch from Martin Perry, Bull.\n\n* Changes in SLURM 2.2.6\n========================\n -- Fix displaying of account coordinators with sacctmgr.  Possiblity to show\n    deleted accounts.  Only a cosmetic issue, since the accounts are already\n    deleted, and have no associations.\n -- Prevent opaque ncurses WINDOW struct on OS X 10.6.\n -- Fix issue with accounting when using PrivateData=jobs... users would not be\n    able to view there own jobs unless they were admin or coordinators which is\n    obviously wrong.\n -- Fix bug in node stat if slurmctld is restarted while nodes are in the\n    process of being powered up. Patch from Andriy Grytsenko.\n -- Change maximum batch script size from 128k to 4M.\n -- Get slurmd -f option working. Patch from Andriy Grytsenko.\n -- Fix for linking problem on OSX. Patches from Jon Bringhurst (LANL) and\n    Tyler Strickland.\n -- Reset a job's priority to zero (suspended) when Moab requeues the job.\n    Patch from Par Andersson, NSC.\n -- When enforcing accounting, fix polling for unknown uids for users after\n    the slurmctld started.  Previously one would have to issue a reconfigure\n    to the slurmctld to have it look for new uids.\n -- BLUEGENE - if a block goes into an error state.  Fix issue where accounting\n    wasn't updated correctly when the block was resumed.\n -- Synchronize power-save module better with scheduler. Patch from\n    Andriy Grytsenko (Massive Solutions Limited).\n -- Avoid SEGV in association logic with user=NULL. Patch from\n    Andriy Grytsenko (Massive Solutions Limited).\n -- Fixed issue in accounting where it was possible for a new\n    association/wckey to be set incorrectly as a default the new object\n    was added after an original default object already existed.  Before\n    the slurmctld would need to be restarted to fix the issue.\n -- Updated the Normalized Usage section in priority_multifactor.shtml.\n -- Disable use of SQUEUE_FORMAT env var if squeue -l, -o, or -s option is\n    used. Patch from Aaron Knister (UMBC).\n\n* Changes in SLURM 2.2.5\n========================\n -- Correct init.d/slurm status to have non-zero exit code if ANY Slurm\n    damon that should be running on the node is not running. Patch from Rod\n    Schulz, Bull.\n -- Improve accuracy of response to \"srun --test-only jobid=#\".\n -- Correct logic to properly support --ntasks-per-node option in the\n    select/cons_res plugin. Patch from Rod Schulz, Bull.\n -- Fix bug in select/cons_res with respect to generic resource (gres)\n    scheduling which prevented some jobs from starting as soon as possible.\n -- Fix memory leak in select/cons_res when backfill scheduling generic\n    resources (gres).\n -- Fix for when configuring a node with more resources than in real life\n    and using task/affinity.\n -- Fix so slurmctld will pack correctly 2.1 step information. (Only needed if\n    a 2.1 client is talking to a 2.2 slurmctld.)\n -- Set powered down node's state to IDLE+POWER after slurmctld restart instead\n    of leaving in UNKNOWN+POWER. Patch from Andrej Gritsenko.\n -- Fix bug where is srun's executable is not on it's current search path, but\n    can be found in the user's default search path. Modify slurmstepd to find\n    the executable. Patch from Andrej Gritsenko.\n -- Make sview display correct cpu count for steps.\n -- BLUEGENE - when running in overlap mode make sure to check the connection\n    type so you can create overlapping blocks on the exact same nodes with\n    different connection types (i.e. one torus, one mesh).\n -- Fix memory leak if MPI ports are reserved (for OpenMPI) and srun's\n    --resv-ports option is used.\n -- Fix some anomalies in select/cons_res task layout when using the\n    --cpus-per-task option. Patch from Martin Perry, Bull.\n -- Improve backfill scheduling logic when job specifies --ntasks-per-node and\n    --mem-per-cpu options on a heterogeneous cluster. Patch from Bjorn-Helge\n    Mevik, University of Oslo.\n -- Print warning message if srun specifies --cpus-per-task larger than used\n    to create job allocation.\n -- Fix issue when changing a users name in accounting, if using wckeys would\n    execute correctly, but bad memcopy would core the DBD.  No information\n    would be lost or corrupted, but you would need to restart the DBD.\n\n* Changes in SLURM 2.2.4\n========================\n -- For batch jobs for which the Prolog fails, substitute the job ID for any\n    \"%j\" in the job's output or error file specification.\n -- Add licenses field to the sview reservation information.\n -- BLUEGENE - Fix for handling extremely overloaded system on Dynamic system\n    dealing with starting jobs on overlapping blocks.  Previous fallout\n    was job would be requeued.  (happens very rarely)\n -- In accounting_storage/filetxt plugin, substitute spaces within job names,\n    step names, and account names with an underscore to insure proper parsing.\n -- When building contribs/perlapi ignore both INSTALL_BASE and PERL_MM_OPT.\n    Use PREFIX instead to avoid build errors from multiple installation\n    specifications.\n -- Add job_submit/cnode plugin to support resource reservations of less than\n    a full midplane on BlueGene computers. Treat cnodes as liceses which can\n    be reserved and are consumed by jobs. This reservation mechanism for less\n    than an entire midplane is still under development.\n -- Clear a job's \"reason\" field when a held job is released.\n -- When releasing a held job, calculate a new priority for it rather than\n    just setting the priority to 1.\n -- Fix for sview started on a non-bluegene system to pick colors correctly\n    when talking to a real bluegene system.\n -- Improve sched/backfill's expected start time calculation.\n -- Prevent abort of sacctmgr for dump command with invalid (or no) filename.\n -- Improve handling of job updates when using limits in accounting, and\n    updating jobs as a non-admin user.\n -- Fix for \"squeue --states=all\" option. Bug would show no jobs.\n -- Schedule jobs with reservations before those without reservations.\n -- Fix squeue/scancel to query correctly against accounts of different case.\n -- Abort an srun command when it's associated job gets aborted due to a\n    dependency that can not be satisfied.\n -- In jobcomp plugins, report start time of zeroif pending job is cancelled.\n    Previously may report expected start time.\n -- Fixed sacctmgr man to state correct variables.\n -- Select nodes based upon their Weight when job allocation requests include\n    a constraint field with a count (e.g. \"srun --constraint=gpu*2 -N4 a.out\").\n -- Add support for user names that are entirely numeric and do not treat them\n    as UID values. Patch from Dennis Leepow.\n -- Patch to un/pack double values properly if negative value.  Patch from\n    Dennis Leepow\n -- Do not reset a job's priority when requeued or suspended.\n -- Fix problemm that could let new jobs start on a node in DRAINED state.\n -- Fix cosmetic sacctmgr issue where if the user you are trying to add\n    doesn't exist in the /etc/passwd file and the account you are trying\n    to add them to doesn't exist it would print (null) instead of the bad\n    account name.\n -- Fix associations/qos for when adding back a previously deleted object\n    the object will be cleared of all old limits.\n -- BLUEGENE - Added back a lock when creating dynamic blocks to be more thread\n    safe on larger systems with heavy load.\n\n* Changes in SLURM 2.2.3\n========================\n -- Update srun, salloc, and sbatch man page description of --distribution\n    option. Patches from Rod Schulz, Bull.\n -- Applied patch from Martin Perry to fix \"Incorrect results for task/affinity\n    block second distribution and cpus-per-task > 1\" bug.\n -- Avoid setting a job's eligible time while held (priority == 0).\n -- Substantial performance improvement to backfill scheduling. Patch from\n    Bjorn-Helge Mevik, University of Oslo.\n -- Make timeout for communications to the slurmctld be based upon the\n    MessageTimeout configuration parameter rather than always 3 seconds.\n    Patch from Matthieu Hautreux, CEA.\n -- Add new scontrol option of \"show aliases\" to report every NodeName that is\n    associated with a given NodeHostName when running multiple slurmd daemons\n    per compute node (typically used for testing purposes). Patch from\n    Matthieu Hautreux, CEA.\n -- Fix for handling job names with a \"'\" in the name within MySQL accounting.\n    Patch from Gerrit Renker, CSCS.\n -- Modify condition under which salloc execution delayed until moved to the\n    foreground. Patch from Gerrit Renker, CSCS.\n\tJob control for interactive salloc sessions: only if ...\n\ta) input is from a terminal (stdin has valid termios attributes),\n\tb) controlling terminal exists (non-negative tpgid),\n\tc) salloc is not run in allocation-only (--no-shell) mode,\n\td) salloc runs in its own process group (true in interactive\n\t   shells that support job control),\n\te) salloc has been configured at compile-time to support background\n\t   execution and is not currently in the background process group.\n -- Abort salloc if no controlling terminal and --no-shell option is not used\n    (\"setsid salloc ...\" is disabled). Patch from Gerrit Renker, CSCS.\n -- Fix to gang scheduling logic which could cause jobs to not be suspended\n    or resumed when appropriate.\n -- Applied patch from Martin Perry to fix \"Slurmd abort when using task\n    affinity with plane distribution\" bug.\n -- Applied patch from Yiannis Georgiou to fix \"Problem with cpu binding to\n    sockets option\" behaviour. This change causes \"--cpu_bind=sockets\" to bind\n    tasks only to the CPUs on each socket allocated to the job rather than all\n    CPUs on each socket.\n -- Advance daily or weekly reservations immediately after termination to avoid\n    having a job start that runs into the reservation when later advanced.\n -- Fix for enabling users to change there own default account, wckey, or QOS.\n -- BLUEGENE - If using OVERLAP mode fixed issue with multiple overlapping\n    blocks in error mode.\n -- Fix for sacctmgr to display correctly default accounts.\n -- scancel -s SIGKILL will always sent the RPC to the slurmctld rather than\n    the slurmd daemon(s). This insures that tasks in the process of getting\n    spawned are killed.\n -- BLUEGENE - If using OVERLAP mode fixed issue with jobs getting denied\n    at submit if the only option for their job was overlapping a block in\n    error state.\n\n* Changes in SLURM 2.2.2\n========================\n -- Correct logic to set correct job hold state (admin or user) when setting\n    the job's priority using scontrol's \"update jobid=...\" rather than its\n    \"hold\" or \"holdu\" commands.\n -- Modify squeue to report unset --mincores, --minthreads or --extra-node-info\n    values as \"*\" rather than 65534. Patch from Rod Schulz, BULL.\n -- Report the StartTime of a job as \"Unknown\" rather than the year 2106 if its\n    expected start time was too far in the future for the backfill scheduler\n    to compute.\n -- Prevent a pending job reason field from inappropriately being set to\n    \"Priority\".\n -- In sched/backfill with jobs having QOS_FLAG_NO_RESERVE set, then don't\n    consider the job's time limit when attempting to backfill schedule. The job\n    will just be preempted as needed at any time.\n -- Eliminated a bug in sbatch when no valid target clusters are specified.\n -- When explicitly sending a signal to a job with the scancel command and that\n    job is in a pending state, then send the request directly to the slurmctld\n    daemon and do not attempt to send the request to slurmd daemons, which are\n    not running the job anyway.\n -- In slurmctld, properly set the up_node_bitmap when setting it's state to\n    IDLE (in case the previous node state was DOWN).\n -- Fix smap to process block midplane names correctly when on a bluegene\n    system.\n -- Fix smap to once again print out the Letter 'ID' for each line of a block/\n    partition view.\n -- Corrected the NOTES section of the scancel man page\n -- Fix for accounting_storage/mysql plugin to correctly query cluster based\n    transactions.\n -- Fix issue when updating database for clusters that were previously deleted\n    before upgrade to 2.2 database.\n -- BLUEGENE - Handle mesh torus check better in dynamic mode.\n -- BLUEGENE - Fixed race condition when freeing block, most likely only would\n    happen in emulation.\n -- Fix for calculating used QOS limits correctly on a slurmctld reconfig.\n -- BLUEGENE - Fix for bad conn-type set when running small blocks in HTC mode.\n -- If salloc's --no-shell option is used, then do not attempt to preserve the\n    terminal's state.\n -- Add new SLURM configure time parameter of --disable-salloc-background. If\n    set, then salloc can only execute in the foreground. If started in the\n    background, then a message will be printed and the job allocation halted\n    until brought into the foreground.\n    NOTE: THIS IS A CHANGE IN DEFAULT SALLOC BEHAVIOR FROM V2.2.1, BUT IS\n    CONSISTENT WITH V2.1 AND EARLIER.\n -- Added the Multi-Cluster Operation web page.\n -- Removed remnant code for enforcing max sockets/cores/threads in the\n    cons_res plugin (see last item in 2.1.0-pre5).  This was responsible\n    for a bug reported by Rod Schultz.\n -- BLUEGENE - Set correct env vars for HTC mode on a P system to get correct\n    block.\n -- Correct RunTime reported by \"scontrol show job\" for pending jobs.\n\n* Changes in SLURM 2.2.1\n========================\n -- Fix setting derived exit code correctly for jobs that happen to have the\n    same jobid.\n -- Better checking for time overflow when rolling up in accounting.\n -- Add scancel --reservation option to cancel all jobs associated with a\n    specific reservation.\n -- Treat reservation with no nodes like one that starts later (let jobs of any\n    size get queued and do not block any pending jobs).\n -- Fix bug in gang scheduling logic that would temporarily resume to many jobs\n    after a job completed.\n -- Change srun message about job step being deferred due to SlurmctldProlog\n    running to be more clear and only print when --verbose option is used.\n -- Made it so you could remove the hold on jobs with sview by setting the\n    priority to infinite.\n -- BLUEGENE - better checking small blocks in dynamic mode whether a full\n    midplane job could run or not.\n -- Decrease the maximum sleep time between srun job step creation retry\n    attempts from 60 seconds to 29 seconds. This should eliminate a possible\n    synchronization problem with gang scheduling that could result in job\n    step creation requests only occuring when a job is suspended.\n -- Fix to prevent changing a held job's state from HELD to DEPENDENCY\n    until the job is released. Patch from Rod Schultz, Bull.\n -- Fixed sprio -M to reflect PriorityWeight values from remote cluster.\n -- Fix bug in sview when trying to update arbitrary field on more than one\n    job. Formerly would display information about one job, but update next\n    selected job.\n -- Made it so QOS with UsageFactor set to 0 would make it so jobs running\n    under that QOS wouldn't add time to fairshare or association/qos\n    limits.\n -- Fixed issue where QOS priority wasn't re-normalized until a slurmctld\n    restart when a QOS priority was changed.\n -- Fix sprio to use calculated numbers from slurmctld instead of\n    calulating it own numbers.\n -- BLUEGENE - fixed race condition with preemption where if the wind blows the\n    right way the slurmctld could lock up when preempting jobs to run others.\n -- BLUEGENE - fixed epilog to wait until MMCS job is totally complete before\n    finishing.\n -- BLUEGENE - more robust checking for states when freeing blocks.\n -- Added correct files to the slurm.spec file for correct perl api rpm\n    creation.\n -- Added flag \"NoReserve\" to a QOS to make it so all jobs are created equal\n    within a QOS.  So if larger, higher priority jobs are unable to run they\n    don't prevent smaller jobs from running even if running the smaller\n    jobs delay the start of the larger, higher priority jobs.\n -- BLUEGENE - Check preemptees one by one to preempt lower priority jobs first\n    instead of first fit.\n -- In select/cons_res, correct handling of the option\n    SelectTypeParameters=CR_ONE_TASK_PER_CORE.\n -- Fix for checking QOS to override partition limits, previously if not using\n    QOS some limits would be overlooked.\n -- Fix bug which would terminate a job step if any of the nodes allocated to\n    it were removed from the job's allocation. Now only the tasks on those\n    nodes are terminated.\n -- Fixed issue when using a storage_accounting plugin directly without the\n    slurmDBD updates weren't always sent correctly to the slurmctld, appears to\n    OS dependent, reported by Fredrik Tegenfeldt.\n\n* Changes in SLURM 2.2.0\n========================\n -- Change format of Duration field in \"scontrol show reservation\" output from\n    an integer number of minutes to \"[days-]hours:minutes:seconds\".\n -- Add support for changing the reservation of pending or running jobs.\n -- On Cray systems only, salloc sends SIGKILL to spawned process group when\n    job allocation is revoked. Patch from Gerrit Renker, CSCS.\n -- Fix for sacctmgr to work correctly when modifying user associations where\n    all the associations contain a partition.\n -- Minor mods to salloc signal handling logic: forwards more signals and\n    releases allocation on real-time signals. Patch from Gerrit Renker, CSCS.\n -- Add salloc logic to preserve tty attributes after abnormal exit. Patch\n    from Mark Grondona, LLNL.\n -- BLUEGENE - Fix for issue in dynamic mode when trying to create a block\n    overlapping a block with no job running on it but in configuring state.\n -- BLUEGENE - Speedup by skipping blocks that are deallocating for other jobs\n    when starting overlapping jobs in dynamic mode.\n -- Fix for sacct --state to work correctly when not specifying a start time.\n -- Fix upgrade process in accounting from 2.1 for clusters named \"cluster\".\n -- Export more jobacct_common symbols needed for the slurm api on some systems.\n\n* Changes in SLURM 2.2.0.rc4\n============================\n -- Correction in logic to spread out over time highly parallel messages to\n    minimize lost messages. Effects slurmd epilog complete messages and PMI\n    key-pair transmissions. Patch from Gerrit Renker, CSCS.\n -- Fixed issue where if a system has unset messages to the dbd in 2.1 and\n    upgrades to 2.2.  Messages are now processed correctly now.\n -- Fixed issue where assoc_mgr cache wasn't always loaded correctly if the\n    slurmdbd wasn't running when the slurmctld was started.\n -- Make sure on a pthread create in step launch that the error code is looked\n    at. Improves fault-tolerance of slurmd.\n -- Fix setting up default acct/wckey when upgrading from 2.1 to 2.2.\n -- Fix issue with associations attached to a specific partition with no other\n    association, and requesting a different partition.\n -- Added perlapi to the slurmdb to the slurm.spec.\n -- In sched/backfill, correct handling of CompleteWait parameter to avoid\n    backfill scheduling while a job is completing. Patch from Gerrit Renker,\n    CSCS.\n -- Send message back to user when trying to launch job on computing lacking\n    that user ID. Patch from Hongjia Cao, NUDT.\n -- BLUEGENE - Fix it so 1 midplane clusters will run small block jobs.\n -- Add Command and WorkDir to the output of \"scontrol show job\" for job\n    allocations created using srun (not just sbatch).\n -- Fixed sacctmgr to not add blank defaultqos' when doing a cluster dump.\n -- Correct processing of memory and disk space specifications in the salloc,\n    sbatch, and srun commands to work properly with a suffix of \"MB\", \"GB\",\n    etc. and not only with a single letter (e.g. \"M\", \"G\", etc.).\n -- Prevent nodes with suspended jobs from being powered down by SLURM.\n -- Normalized the way pidfile are created by the slurm daemons.\n -- Fixed modifying the root association to no read in it's last value\n    when clearing a limit being set.\n -- Revert some resent signal handling logic from salloc so that SIGHUP sent\n    after the job allocation will properly release the allocation and cause\n    salloc to exit.\n -- BLUEGENE - Fix for recreating a block in a ready state.\n -- Fix debug flags for incorrect logic when dealing with DEBUG_FLAG_WIKI.\n -- Report reservation's Nodes as a hostlist expression of all nodes rather\n    than using \"ALL\".\n -- Fix reporting of nodes in BlueGene reservation (was reporting CPU count\n    rather than cnode count in scontrol output for NodeCnt field).\n\n* Changes in SLURM 2.2.0.rc3\n============================\n -- Modify sacctmgr command to accept plural versions of options (e.g. \"Users\"\n    in addition to \"User\"). Patch from Don Albert, BULL.\n -- BLUEGENE - make it so reset of boot counter happens only on state change\n    and not when a new job comes along.\n -- Modify srun and salloc signal handling so they can be interrupted while\n    waiting for an allocation. This was broken in version 2.2.0.rc2.\n -- Fix NULL pointer reference in sview. Patch from Gerrit Renker, CSCS.\n -- Fix file descriptor leak in slurmstepd on spank_task_post_fork() failure.\n    Patch from Gerrit Renker, CSCS.\n -- Fix bug in preserving job state information when upgrading from SLURM\n    version 2.1. Bug introduced in version 2.2.0-pre10. Patch from Par\n    Andersson, NSC.\n -- Fix bug where if using the slurmdbd if a job wasn't able to start right\n    away some accounting information may be lost.\n -- BLUEGENE - when a prolog failure happens the offending block is put in\n    an error state.\n -- Changed the last column heading of the sshare output from \"FS Usage\" to\n    \"FairShare\" and added more detail to the sshare man page.\n -- Fix bug in enforcement of reservation by account name. Used wrong index\n    into an array. Patch from Gerrit Renker, CSCS.\n -- Modify job_submit/lua plugin to treat any non-zero return code from the\n    job_submit and job_modify functions as an error and the user request should\n    be aborted.\n -- Fix bug which would permit pending job to be started on completing node\n    when job preemption is configured.\n\n* Changes in SLURM 2.2.0.rc2\n============================\n -- Fix memory leak in job step allocation logic. Patch from Hongjia Cao, NUDT.\n -- If a preempted job was submitted with the --no-requeue option then cancel\n    rather than requeue it.\n -- Fix for problems when adding a user for the first time to a new cluster\n    with a 2.1 sacctmgr without specifying a default account.\n -- Resend TERMINATE_JOB message only to nodes that the job still has not\n    terminated on. Patch from Hongjia Cao, NUDT.\n -- Treat time limit specification of \"0:300\" as a request for 300 seconds\n    (5 minutes) instead of one minute.\n -- Modify sched/backfill plugin logic to continue working its way down the\n    queue of jobs rather than restarting at the top if there are no changes in\n    job, node, or partition state between runs. Patch from Hongjia Cao, NUDT.\n -- Improve scalability of select/cons_res logic. Patch from Matthieu Hautreux,\n    CEA.\n -- Fix for possible deadlock in the slurmstepd when cancelling a job that is\n    also writing a large amount of data to stderr.\n -- Fix in select/cons_res to eliminate \"mem underflow\" error when the\n    slurmctld is reconfigured while a job is in completing state.\n -- Send a message to the a user's job when it's real or virual memory limit\n    is exceeded. :\n -- Apply rlimits right before execing the users task so to lower the risk of\n    the task exiting because the slurmstepd ran over a limit (log file size,\n    etc.)\n -- Add scontrol command of \"uhold <job_id>\" so that an administrator can hold\n    a job and let the job's owner release it. The scontrol command of\n    \"hold <job_id>\" when executed by a SLURM administrator can only be released\n    by a SLURM administrator and not the job owner.\n -- Change atoi to slurm_atoul in mysql plugin, needed for running on 32-bit\n    systems in some cases.\n -- If a batch job is found to be missing from a node, make its termination\n    state be NODE_FAIL rather than CANCELLED.\n -- Fatal error put back if running a bluegene or cray plugin from a controller\n    not of that type.\n -- Make sure jobacct_gather plugin is not shutdown before messing with the\n    proccess list.\n -- Modify signal handling in srun and salloc commands to avoid deadlock if the\n    malloc function is interupted and called again. The malloc function is\n    thread safe, but not reentrant, which is a problem when signal handling if\n    the malloc function itself has a lock. Problem fixed by moving signal\n    handling in those commands to a new pthread.\n -- In srun set job abort flag on completion to handle the case when a user\n    cancels a job while the node is not responding but slurmctld has not yet\n    the node down. Patch from Hongjia Cao, NUDT.\n -- Streamline the PMI logic if no duplicate keys are included in the key-pairs\n    managed. Substantially improves performance for large numbers of tasks.\n    Adds support for SLURM_PMI_KVS_NO_DUP_KEYS environment variable. Patch\n    from Hongjia Cao, NUDT.\n -- Fix issues with sview dealing with older versions of sview and saving\n    defaults.\n -- Remove references to --mincores, --minsockets, and --minthreads from the\n    salloc, sbatch and srun man pages. These options are defunct, Patch from\n    Rod Schultz, Bull.\n -- Made openssl not be required to build RPMs, it is not required anymore\n    since munge is the default crypto plugin.\n -- sacctmgr now has smarts to figure out if a qos is a default qos when\n    modifing a user/acct or removing a qos.\n -- For reservations on BlueGene systems, set and report c-node counts rather\n    than midplane counts.\n\n* Changes in SLURM 2.2.0.rc1\n============================\n -- Add show_flags parameter to the slurm_load_block_info() function.\n -- perlapi has been brought up to speed courtesy of Hongjia Coa. (make sure to\n    run 'make clean' if building in a different dir than source)\n -- Fixed regression in pre12 in crypto/munge when running with\n    --enable-multiple-slurmd which would cause the slurmd's to core.\n -- Fixed regression where cpu count wasn't figured out correctly for steps.\n -- Fixed issue when using old mysql that can't handle a '.' in the table\n    name.\n -- Mysql plugin works correctly without the SlurmDBD\n -- Added ability to query batch step with sstat.  Currently no accounting data\n    is stored for the batch step, but the internals are inplace if we decide to\n    do that in the future.\n -- Fixed some backwards compatibility issues with 2.2 talking to 2.1.\n -- Fixed regression where modifying associations didn't get sent to the\n    slurmctld.\n -- Made sshare sort things the same way saccmgr list assoc does\n    (alphabetically)\n -- Fixed issue with default accounts being set up correctly.\n -- Changed sortting in the slurmctld so sshare output is similar to that of\n    sacctmgr list assoc.\n -- Modify reservation logic so that daily and weekly reservations maintain\n    the same time when daylight savings time starts or ends in the interim.\n -- Edit to make reservations handle updates to associations.\n -- Added the derived exit code to the slurmctld job record and the derived\n    exit code and string to the job record in the SLURM db.\n -- Added slurm-sjobexit RPM for SLURM job exit code management tools.\n -- Added ability to use sstat/sacct against the batch step.\n -- Added OnlyDefaults option to sacctmgr list associations.\n -- Modified the fairshare priority formula to F = 2**(-Ue/S)\n -- Modify the PMI functions key-pair exchange function to support a 32-bit\n    counter for larger job sizes. Patch from Hongjia Cao, NUDT.\n -- In sched/builtin - Make the estimated job start time logic faster (borrowed\n    new logic from sched/backfill and added pthread) and more accurate.\n -- In select/cons_res fix bug that could result in a job being allocated zero\n    CPUs on some nodes. Patch from Hongjia Cao, NUDT.\n -- Fix bug in sched/backfill that could set expected start time of a job too\n    far in the future.\n -- Added ability to enforce new limits given to associations/qos on\n    pending jobs.\n -- Increase max message size for the slurmdbd from 1000000 to 16*1024*1024\n -- Increase number of active threads in the slurmdbd from 50 to 100\n -- Fixed small bug in src/common/slurmdb_defs.c reported by Bjorn-Helge Mevik\n -- Fixed sacctmgr's ability to query associations against qos again.\n -- Fixed sview show config on non-bluegene systems.\n -- Fixed bug in selecting jobs based on sacct -N option\n -- Fix bug that prevented job Epilog from running more than once on a node if\n    a job was requeued and started no job steps.\n -- Fixed issue where node index wasn't stored correcting when using DBD.\n -- Enable srun's use of the --nodes option with --exclusive (previously the\n    --nodes option was ignored).\n -- Added UsageThreshold and Flags to the QOS object.\n -- Patch to improve threadsafeness in the mysql plugins.\n -- Add support for fair-share scheduling to be based upon resource use at\n    the level of bank accounts and ignore use of individual users. Patch by\n    Par Andersson, National Supercomputer Centre, Sweden.\n\n* Changes in SLURM 2.2.0.pre12\n==============================\n -- Log if Prolog or Epilog run for longer than MessageTimeout / 2.\n -- Log the RPC number associated with messages from slurmctld that timeout.\n -- Fix bug in select/cons_res logic when job allocation includes --overcommit\n    and --ntasks-per-node options and the node has fewer CPUs than the count\n    specified by --ntasks-per-node.\n -- Fix bug in gang scheduling and job preemption logic so that preempted jobs\n    get resumed properly after a slurmctld hot-start.\n -- Fix bug in select/linear handling of gang scheduled jobs that could result\n    in run_job_cnt underflow error message.\n -- Fix bug in gang scheduling logic to properly support partitions added\n    using the scontrol command.\n -- Fix a segmentation fault in sview where the 'excluded_partitions' field\n    was set to NULL, caused by the absence of ~/.slurm/sviewrc.\n -- Rewrote some calls to is_user_any_coord() in src/plugins/accounting_storage\n    modules to make use of is_user_any_coord()'s return value.\n -- Add configure option of --with=dimensions=#.\n -- Modify srun ping logic so that srun would only be considered not responsive\n    if three ping messages were not responded to. Patch from Hongjia Cao (NUDT).\n -- Preserve a node's ReasonTime field after scontrol reconfig command. Patch\n    from Hongjia Cao (NUDT).\n -- Added the authority for users with AdminLevel's defined in the SLURM db\n    (Operators and Admins) and account coordinators to invoke commands that\n    affect jobs, reservations, nodes, etc.\n -- Fix for slurmd restart on completing node with no tasks to get the correct\n    state, completing. Patch from Hongjia Cao (NUDT).\n -- Prevent scontrol setting a node's Reason=\"\". Patch from Hongjia Cao (NUDT).\n -- Add new functions hostlist_ranged_string_malloc,\n    hostlist_ranged_string_xmalloc, hostlist_deranged_string_malloc, and\n    hostlist_deranged_string_xmalloc which will allocate memory as needed.\n -- Make the slurm commands support both the --cluster and --clusters option.\n    Previously, some commands support one of those options, but not the other.\n -- Fix bug when resizing a job that has steps running on some of those nodes.\n    Avoid killing the job step on remaining nodes. Patch from Rod Schultz\n    (BULL). Also fix bug related to tracking the CPUs allocated to job steps\n    on each node after releasing some nodes from the job's allocation.\n -- Applied patch from Rod Schultz / Matthieu Hautreux to keep the Node-to-Host\n    cache from becoming corrupted when a hostname cannot be resolved.\n -- Export more symbols in libslurm for job and node state information\n    translation (numbers to strings). Patch from Hongia Cao, NUDT.\n -- Add logic to retry sending RESPONSE_LAUNCH_TASKS messages from slurmd to\n    srun. Patch from Hongia Cao, NUDT.\n -- Modify bit_unfmt_hexmask() and bit_unfmt_binmask() functions to clear the\n    bitmap input before setting the bits indicated in the input string.\n -- Add SchedulerParameters option of bf_window to control how far into the\n    future that the backfill scheduler will look when considering jobs to start.\n    The default value is one day. See \"man slurm.conf\" for details.\n -- Fix bug that can result in duplicate job termination records in accounting\n    for job termination when slurmctld restarts or reconfigures.\n -- Modify plugin and library logic as needed to support use of the function\n    slurm_job_step_stat() from user commands.\n -- Fix race condition in which PrologSlurmctld failure could cause slurmctld\n    to abort.\n -- Fix bug preventing users in secondary user groups from being granted access\n    to partitions configured with AllowGroups.\n -- Added support for a default account and wckey per cluster within accounting.\n -- Modified select/cons_res plugin so that if MaxMemPerCPU is configured and a\n    job specifies it's memory requirement, then more CPUs than requested will\n    automatically be allocated to a job to honor the MaxMemPerCPU parameter.\n -- Added the derived_ec (exit_code) member to job_info_t.  exit_code captures\n    the exit code of the job script (or salloc) while derived_ec contains the\n    highest exit code of all the job steps.\n -- Added SLURM_JOB_EXIT_CODE and SLURM_JOB_DERIVED_EC variables to the\n    EpilogSlurmctld environment\n -- More work done on the accounting_storage/pgsql plugin, still beta.\n    Patch from Hongjia Cao (NUDT).\n -- Major updates to sview from Dan Rusak (Bull), including:\n    - Persistent option selections for each tab page\n    - Clean up topology in grids\n    - Leverage AllowGroups and Hidden options\n    - Cascade full-info popups for ease of selection\n -- Add locks around the MySQL calls for proper operation if the non-thread\n    safe version of the MySQL library is used.\n -- Remove libslurm.a, libpmi.a and libslurmdb.a from SLURM RPM. These static\n    libraries are not generally usable.\n -- Fixed bug in sacctmgr when zeroing raw usage reported by Gerrit Renker.\n\n* Changes in SLURM 2.2.0.pre11\n==============================\n -- Permit a regular user to change the partition of a pending job.\n -- Major re-write of the job_submit/lua plugin to pass pointers to available\n    partitions and use lua metatables to reference the job and partition fields.\n -- Add support for serveral new trigger types: SlurmDBD failure/restart,\n    Database failure/restart, Slurmctld failure/restart.\n -- Add support for SLURM_CLUSTERS environment variable in the sbatch, sinfo,\n    squeue commands.\n -- Modify the sinfo and squeue commands to report state of multiple clusters\n    if the --clusters option is used.\n -- Added printf __attribute__ qualifiers to info, debug, ... to help prevent\n    bad/incorrect parameters being sent to them.  Original patch from\n    Eygene Ryabinkin (Russian Research Centre).\n -- Fix bug in slurmctld job completion logic when nodes allocated to a\n    completing job are re-booted. Patch from Hongjia Cao (NUDT).\n -- In slurmctld's node record data structure, rename \"hilbert_integer\" to\n    \"node_rank\".\n -- Add topology/node_rank plugin to sort nodes based upon rank loaded from\n    BASIL on Cray computers.\n -- Fix memory leak in the auth/munge and crypto/munge plugins in the case of\n    some failure modes.\n\n* Changes in SLURM 2.2.0.pre10\n==============================\n -- Fix issue when EnforcePartLimits=yes in slurm.conf all jobs where no nodecnt\n    was specified the job would be seen to have maxnodes=0 which would not\n    allow jobs to run.\n -- Fix issue where if not suspending a job the gang scheduler does the correct\n    kill procedure.\n -- Fixed some issues when dealing with jobs from a 2.1 system so they live\n    after an upgrade.\n -- In srun, log if --cpu_bind options are specified, but not supported by the\n    current system configuration.\n -- Various Patchs from Hongjia Cao dealing with bugs found in sacctmgr and\n    the slurmdbd.\n -- Fix bug in changing the nodes allocated to a running job and some node\n    names specified are invalid, avoid invalid memory reference.\n -- Fixed filename substitution of %h and %n based on patch from Ralph Bean\n -- Added better job sorting logic when preempting jobs with qos.\n -- Log the IP address and port number for some communication errors.\n -- Fix bug in select/cons_res when --cpus_per_task option is used, could\n    oversubscribe resources.\n -- In srun, do not implicitly set the job's maximum node count based upon a\n    required hostlist.\n -- Avoid running the HealthCheckProgram on non-responding nodes rather than\n    DOWN nodes.\n -- Fix bug in handling of poll() functions on OS X (SLURM was ignoring POLLIN\n    if POLLHUP flag was set at the same time).\n -- Pulled Cray logic out of common/node_select.c into it's own\n    select/cray plugin cons_res is the default.  To use linear add 'Linear' to\n    SelectTypeParameters.\n -- Fixed bug where resizing jobs didn't correctly set used limits correctly.\n -- Change sched/backfill default time interval to 30 seconds and defer attempt\n    to backfill schedule if slurmctld has more than 5 active RPCs. General\n    improvements in logic scalability.\n -- Add SchedulerParameters option of default_sched_depth=# to control how\n    many jobs on queue should be tested for attempted scheduling when a job\n    completes or other routine events. Default value is 100 jobs. The full job\n    queue is tested on a less frequent basis. This option can dramatically\n    improve performance on systems with thousands of queued jobs.\n -- Gres/gpu now sets the CUDA_VISIBLE_DEVICES environment to control which\n    GPU devices should be used for each job or job step and CUDA version 3.1+\n    is used. NOTE: SLURM's generic resource support is still under development.\n -- Modify select/cons_res to pack jobs onto allocated nodes differently and\n    minimize system fragmentation. For example on nodes with 8 CPUs each, a\n    job needing 10 CPUs will now ideally be allocated 8 CPUs on one node and\n    2 CPUs on another node. Previously the job would have ideally been\n    allocated 5 CPUs on each node, fragmenting the unused resources more.\n -- Modified the behavior of update_job() in job_mgr.c to return when the first\n    error is encountered instead of continuing with more job updates.\n -- Removed all references to the following slurm.conf parameters, all of which\n    have been removed or replaced since version 2.0 or earlier: HashBase,\n    HeartbeatInterval, JobAcctFrequency, JobAcctLogFile (instead use\n    AccountingStorageLoc), JobAcctType, KillTree, MaxMemPerTask, and\n    MpichGmDirectSupport.\n -- Fix bug in slurmctld restart logic that improperly reported jobs had\n    invalid features: \"Job 65537 has invalid feature list: fat\".\n -- BLUEGENE - Removed thread pool for destroying blocks.  It turns out the\n    memory leak we were concerned about for creating and destroying threads\n    in a plugin doesn't exist anymore.  This increases throughput dramatically,\n    allowing multiple jobs to start at the same time.\n -- BLUEGENE - Removed thread pool for starting and stopping jobs.  For similar\n    reasons as noted above.\n -- BLUEGENE - Handle blocks that never deallocate.\n\n* Changes in SLURM 2.2.0.pre9\n=============================\n -- sbatch can now submit jobs to multiple clusters and run on the earliest\n    available.\n -- Fix bug introduced in pre8 that prevented job dependencies and job\n    triggers from working without the --enable-debug configure option.\n -- Replaced slurm_addr with slurm_addr_t\n -- Replaced slurm_fd with slurm_fd_t\n -- Skeleton code added for BlueGeneQ.\n -- Jobs can now be submitted to multiple partitions (job queues) and use the\n    one permitting earliest start time.\n -- Change slurmdb_coord_table back to acct_coord_table to keep consistant\n    with < 2.1.\n -- Introduced locking system similar to that in the slurmctld for the\n    assoc_mgr.\n -- Added ability to change a users name in accounting.\n -- Restore squeue support for \"%G\" format (group id) accidentally removed in\n    2.2.0.pre7.\n -- Added preempt_mode option to QOS.\n -- Added a grouping=individual for sreport size reports.\n -- Added remove_qos logic to jobs running under a QOS that was removed.\n -- scancel now exits with a 1 if any job is non-existant when canceling.\n -- Better handling of select plugins that don't exist on various systems for\n    cross cluster communication.  Slurmctld, slurmd, and slurmstepd now only\n    load the default select plugin as well.\n -- Better error handling when loading plugins.\n -- Prevent scontrol from aborting if getlogin() returns NULL.\n -- Prevent scontrol segfault when there are hidden nodes.\n -- Prevent srun segfault after task launch failure.\n -- Added job_submit/lua plugin.\n -- Fixed sinfo on a bluegene system to print correctly the output for:\n    sinfo -e -o \"%9P %6m %.4c %.22F %f\"\n -- Add scontrol commands \"hold\" and \"release\" to simplify setting a job's\n    priority to 0 or 1. Also tests that the job is in pending state.\n -- Increase maximum node list size (for incoming RPC) from 1024 bytes to 64k.\n -- In the backup slurmctld, purge triggers before recovering trigger state to\n    avoid duplicate entries.\n -- Fix bug in sacct processing of --fields= option.\n -- Fix bug in checkpoint/blcr for jobs spanning multiple nodes introduced when\n    changing some variable names in version 2.2.0.pre5.\n -- Removed the vestigal set_max_cluster_usage() function from the Priority\n    Plugin API.\n -- Modify the output of \"scontrol show job\" for the field ReqS:C:T=. Fields\n    not specified by the user will be reported as \"*\" instead of 65534.\n -- Added DefaultQOS option for an association.\n -- BLUEGENE - Added -B option to the slurmctld to clear created blocks from\n    the system on start.\n -- BLUEGENE - Added option to scontrol & sview to recreate existing blocks.\n -- Fixed flags for returning messages to use the correct munge key when going\n    cross-cluster.\n -- BLUEGENE - Added option to scontrol & sview to resume blocks in an error\n    state instead of just freeing them.\n -- sview patched to allow multiple row selection of jobs, patch from Dan Rusak\n -- Lower default slurmctld server thread count from 1024 to 256. Some systems\n    process threads on a last-in first-out basis and the high thread count was\n    causing unexpectedly high delays for some RPCs.\n -- Added to sacctmgr the ability for admins to reset the raw usage of a user\n    or account\n -- Improved the efficiency of a few lines in sacctmgr\n\n* Changes in SLURM 2.2.0.pre8\n=============================\n -- Add DebugFlags parameter of \"Backfill\" for sched/backfill detailed logging.\n -- Add DebugFlags parameter of \"Gang\" for detailed logging of gang scheduling\n    activities.\n -- Add DebugFlags parameter of \"Priority\" for detailed logging of priority\n    multifactor activities.\n -- Add DebugFlags parameter of \"Reservation\" for detailed logging of advanced\n    reservations.\n -- Add run time to mail message upon job termination and queue time for mail\n    message upon job begin.\n -- Add email notification option for job requeue.\n -- Generate a fatal error if the srun --relative option is used when not\n    within an existing job allocation.\n -- Modify the meaning of InactiveLimit slightly. It will now cancel the job\n    allocation created using the salloc or srun command if those commands\n    cease responding for the InactiveLimit regardless of any running job steps.\n    This parameter will no longer effect jobs spawned using sbatch.\n -- Remove AccountingStoragePass and JobCompPass from configuration RPC and\n    scontrol show config command output. The use of SlurmDBD is still strongly\n    recommended as SLURM will have limited database functionality or protection\n    otherwise.\n -- Add sbatch options of --export and SBATCH_EXPORT to control which\n    environment variables (if any) get propagated to the spawned job. This is\n    particularly important for jobs that are submitted on one cluster and run\n    on a different cluster.\n -- Fix bug in select/linear when used with gang scheduling and there are\n    preempted jobs at the time slurmctld restarts that can result in over-\n    subscribing resources.\n -- Added keeping track of the qos a job is running with in accounting.\n -- Fix for handling correctly jobs that resize, and also reporting correct\n    stats on a job after it finishes.\n -- Modify gang scheduler so with SelectTypeParameter=CR_CPUS and task\n    affinity is enabled, keep track of the individual CPUs allocated to jobs\n    rather than just the count of CPUs allocated (which could overcommit\n    specific CPUs for running jobs).\n -- Modify select/linear plugin data structures to eliminate underflow errors\n    for the exclusive_cnt and tot_job_cnt variables (previously happened when\n    slurmctld reconfigured while the job was in completing state).\n -- Change slurmd's working directory (and location of core files) to match\n    that of the slurmctld daemon: the same directory used for log files,\n    SlurmdLogFile (if specified with an absolute pathname) otherwise the\n    directory used to save state, SlurmdSpoolDir.\n -- Add sattach support for the --pty option.\n -- Modify slurmctld communications logic to accept incoming messages on more\n    than one port for improved scalability.\n -- Add SchedulerParameters option of \"defer\" to avoid trying to schedule a\n    job at submission time, but to attempt scheduling many jobs at once for\n    improved performance under heavy load.\n -- Correct logic controlling slurmctld thread limit eliminating check of\n    RLIMIT_STACK.\n -- Make slurmctld's trigger logic more robust in the event that job records\n    get purged before their trigger can be processed (e.g. MinJobAge=1).\n -- Add support for users to hold/release their own jobs (submit the job with\n    srun/sbatch --hold/-H option or use \"scontrol update jobid=# priority=0\"\n    to hold and \"scontrol update jobid=# priority=1\" to release).\n -- Added ability for sacct to query jobs by qos and a range of timelimits.\n -- Added ability for sstat to query pids of steps running.\n -- Support time specification in UTS format with a prefix of \"uts\" (e.g.\n    \"sbatch --begin=uts458389988 my.script\").\n\n* Changes in SLURM 2.2.0.pre7\n=============================\n -- Fixed issue with sacctmgr if querying against non-existent cluster it\n    works the same way as 2.1.\n -- Added infrastructure to support allocation of generic node resources (gres).\n    -Modified select/linear and select/cons_res plugins to allocate resources\n     at the level of a job without oversubcription.\n    -Get sched/backfill operating with gres allocations.\n    -Get gres configuration changes (reconfiguration) working.\n    -Have job steps allocate resources.\n    -Modified job step credential to include the job's and step's gres\n     allocation details.\n    -Integrate with HWLOC library to identify GPUs and NICs configured on each\n     node.\n -- SLURM commands (squeue, sinfo, etc...) can now go cross-cluster on like\n    linux systems.  Cross-cluster for bluegene to linux and such should\n    work fine, even sview.\n -- Added the ability to configure PreemptMode on a per-partition basis.\n -- Change slurmctld's default thread limit count to 1024, but adjust that down\n    as needed based upon the process's resource limits.\n -- Removed the non-functional \"SystemCPU\" and \"TotalCPU\" reporting fields from\n    sstat and updated man page\n -- Correct location of apbasil command on Cray XT systems.\n -- Fixed bug in MinCPU and AveCPU calculations in sstat command\n -- Send message to srun when the Prolog takes too long (MessageTimeout) to\n    complete.\n -- Change timeout for socket connect() to be half of configured MessageTimeout.\n -- Added high-throughput computing web page with configuration guidance.\n -- Use more srun sockets to process incoming PMI (MPICH2) connections for\n    better scalability.\n -- Added DebugFlags for the select/bluegene plugin: DEBUG_FLAG_BG_PICK,\n    DEBUG_FLAG_BG_WIRES, DEBUG_FLAG_BG_ALGO, and DEBUG_FLAG_BG_ALGO_DEEP.\n -- Remove vestigial job record field \"kill_on_step_done\" (internal to the\n    slurmctld daemon only).\n -- For MPICH2 jobs: Clear PMI state between job steps.\n\n* Changes in SLURM 2.2.0.pre6\n=============================\n -- sview - added ability to see database configuration.\n -- sview - added ability to add/remove visible tabs.\n -- sview - change way grid highlighting takes place on selected objects.\n -- Added infrastructure to support allocation of generic node resources.\n    -Added node configuration parameter of Gres=.\n    -Added ability to view/modify a node's gres using scontrol, sinfo and sview.\n    -Added salloc, sbatch and srun --gres option.\n    -Added ability to view a job or job step's gres using scontrol, squeue and\n     sview.\n    -Added new configuration parameter GresPlugins to define plugins used to\n     manage generic resources.\n    -Added framework for gres plugins.\n    -Added DebugFlags option of \"gres\" for detailed debugging of gres actions.\n -- Slurmd modified to log slow slurmstepd startup and note possible file system\n    problem.\n -- sview - There is now a .slurm/sviewrc created when running sview.\n    Defaults are put in there as to how sview looks when first launched.\n    You can set these by Ctrl-S or Options->Set Default Settings.\n -- Add scontrol \"wait_job <job_id>\" option to wait for nodes to boot as needed.\n    Useful for batch jobs (in Prolog, PrologSlurmctld or the script) if powering\n    down idle nodes.\n -- Added salloc and sbatch option --wait-all-nodes. If set non-zero, job\n    initiation will be delayed until all allocated nodes have booted. Salloc\n    will log the delay with the messages \"Waiting for nodes to boot\" and \"Nodes\n    are ready for job\".\n -- The Priority/mulitfactor plugin now takes into consideration size of job\n    in cpus as well as size in nodes when looking at the job size factor.\n    Previously only nodes were considered.\n -- When using the SlurmDBD messages waiting to be sent will be combined\n    and sent in one message.\n -- Remove srun's --core option. Move the logic to an optional SPANK plugin\n    (currently in the contribs directory, but plan to distribute through\n    http://code.google.com/p/slurm-spank-plugins/).\n -- Patch for adding CR_CORE_DEFAULT_DIST_BLOCK as a select option to layout\n    jobs using block layout across cores within each node instead of cyclic\n    which was previously the default.\n -- Accounting - When removing associations if jobs are running, those jobs\n    must be killed before proceeding.  Before the jobs were killed\n    automatically thus causing user confusion on what is most likely an\n    admin's mistake.\n -- sview - color column keeps reference color when highlighting.\n -- Configuration parameter MaxJobCount changed from 16-bit to 32-bit field.\n    The default MaxJobCount was changed from 5,000 to 10,000.\n -- SLURM commands (squeue, sinfo, etc...) can now go cross-cluster on like\n    linux systems.  Cross-cluster for bluegene to linux and such does not\n    currently work.  You can submit jobs with sbatch.  Salloc and srun are not\n    cross-cluster compatible, and given their nature to talk to actual compute\n    nodes these will likely never be.\n -- salloc modified to forward SIGTERM to the spawned program.\n -- In sched/wiki2 (for Moab support) - Add GRES and WCKEY fields to MODIFYJOBS\n    and GETJOBS commands. Add GRES field to GETNODES command.\n -- In struct job_descriptor and struct job_info: rename min_sockets to\n    sockets_per_node, min_cores to cores_per_socket, and min_threads to\n    threads_per_core (the values are not minimum, but represent the target\n    values).\n -- Fixed bug in clearing a partition's DisableRootJobs value reported by\n    Hongjia Cao.\n -- Purge (or ignore) terminated jobs in a more timely fashion based upon the\n    MinJobAge configuration parameter. Small values for MinJobAge should improve\n    responsiveness for high job throughput.\n\n* Changes in SLURM 2.2.0.pre5\n=============================\n -- Modify commands to accept time format with one or two digit hour value\n    (e.g. 8:00 or 08:00 or 8:00:00 or 08:00:00).\n -- Modify time parsing logic to accept \"minute\", \"hour\", \"day\", and \"week\" in\n    addition to the currently accepted \"minutes\", \"hours\", etc.\n -- Add slurmd option of \"-C\" to print actual hardware configuration and exit.\n -- Pass EnforcePartLimits configuration parameter from slurmctld for user\n    commands to see the correct value instead of always \"NO\".\n -- Modify partition data structures to replace the default_part,\n    disable_root_jobs, hidden and root_only fields with a single field called\n    \"flags\" populated with the flags PART_FLAG_DEFAULT, PART_FLAG_NO_ROOT\n    PART_FLAG_HIDDEN and/or PART_FLAG_ROOT_ONLY. This is a more flexible\n    solution besides making for smaller data structures.\n -- Add node state flag of JOB_RESIZING. This will only exist when a job's\n    accounting record is being written immediately before or after it changes\n    size. This permits job accounting records to be written for a job at each\n    size.\n -- Make calls to jobcomp and accounting_storage plugins before and after a job\n    changes size (with the job state being JOB_RESIZING). All plugins write a\n    record for the job at each size with intermediate job states being\n    JOB_RESIZING.\n -- When changing a job size using scontrol, generate a script that can be\n    executed by the user to reset SLURM environment variables.\n -- Modify select/linear and select/cons_res to use resources released by job\n    resizing.\n -- Added to contribs foundation for Perl extension for slurmdb library.\n -- Add new configuration parameter JobSubmitPlugins which provides a mechanism\n    to set default job parameters or perform other site-configurable actions at\n    job submit time.\n -- Better postgres support for accounting, still beta.\n -- Speed up job start when using the slurmdbd.\n -- Forward step failure reason back to slurmd before in some cases it would\n    just be SLURM_FAILURE returned.\n -- Changed squeue to fail when passed invalid -o <output_format> or\n    -S <sort_list> specifications.\n\n* Changes in SLURM 2.2.0.pre4\n=============================\n -- Add support for a PropagatePrioProcess configuration parameter value of 2\n    to restrict spawned task nice values to that of the slurmd daemon plus 1.\n    This insures that the slurmd daemon always have a higher scheduling\n    priority than spawned tasks.\n -- Add support in slurmctld, slurmd and slurmdbd for option of \"-n <value>\" to\n    reset the daemon's nice value.\n -- Fixed slurm_load_slurmd_status and slurm_pid2jobid to work correctly when\n    multiple slurmds are in use.\n -- Altered srun to set max_nodes to min_nodes if not set when doing an\n    allocation to mimic that which salloc and sbatch do.  If running a step if\n    the max isn't set it remains unset.\n -- Applied patch from David Egolf (David.Egolf@Bull.com). Added the ability\n    to purge/archive accounting data on a day or hour basis, previously\n    it was only available on a monthly basis.\n -- Add support for maximum node count in job step request.\n -- Fix bug in CPU count logic for job step allocation (used count of CPUS per\n    node rather than CPUs allocated to the job).\n -- Add new configuration parameters GroupUpdateForce and GroupUpdateTime.\n    See \"man slurm.conf\" for details about how these control when slurmctld\n    updates its information of which users are in the groups allowed to use\n    partitions.\n -- Added sacctmgr list events which will list events that have happened on\n    clusters in accounting.\n -- Permit a running job to shrink in size using a command of\n    \"scontrol update JobId=# NumNodes=#\" or\n    \"scontrol update JobId=# NodeList=<names>\". Subsequent job steps must\n    explicitly specify an appropriate node count to work properly.\n -- Added resize_time field to job record noting the time of the latest job\n    size change (to be used for accounting purposes).\n -- sview/smap now hides hidden partitions and their jobs by default, with an\n    option to display them.\n\n* Changes in SLURM 2.2.0.pre3\n=============================\n -- Refine support for TotalView partial attach. Add parameter to configure\n    program of \"--enable-partial-attach\".\n -- In select/cons_res, the count of CPUs on required nodes was formerly\n    ignored in enforcing the maximum CPU limit. Also enforce maximum CPU\n    limit when the topology/tree plugin is configured (previously ignored).\n -- In select/cons_res, allocate cores for a job using a best-fit approach.\n -- In select/cons_res, for jobs that can run on a single node, use a best-fit\n    packing approach.\n -- Add support for new partition states of DRAIN and INACTIVE and new partition\n    option of \"Alternate\" (alternate partition to use for jobs submitted to\n    partitions that are currently in a state of DRAIN or INACTIVE).\n -- Add group membership cache. This can substantially speed up slurmctld\n    startup or reconfiguration if many partitions have AllowGroups configured.\n -- Added slurmdb api for accessing slurm DB information.\n -- In select/linear: Modify data structures for better performance and to\n    avoid underflow error messages when slurmctld restarts while jobs are\n    in completing state.\n -- Added hash for slurm.conf so when nodes check in to the controller it can\n    verify the slurm.conf is the same as the one it is running.  If not an\n    error message is displayed.  To silence this message add NO_CONF_HASH\n    to DebugFlags in your slurm.conf.\n -- Added error code ESLURM_CIRCULAR_DEPENDENCY and prevent circular job\n    dependencies (e.g. job 12 dependent upon job 11 AND job 11 is dependent\n    upon job 12).\n -- Add BootTime and SlurmdStartTime to available node information.\n -- Fixed moab_2_slurmdb to work correctly under new database schema.\n -- Slurmd will drain a compute node when the SlurmdSpoolDir is full.\n\n* Changes in SLURM 2.2.0.pre2\n=============================\n -- Add support for spank_get_item() to get S_STEP_ALLOC_CORES and\n    S_STEP_ALLOC_MEM. Support will remain for S_JOB_ALLOC_CORES and\n    S_JOB_ALLOC_MEM.\n -- Kill individual job steps that exceed their memory limit rather than\n    killing an entire job if one step exceeds its memory limit.\n -- Added configuration parameter VSizeFactor to enforce virtual memory limits\n    for jobs and job steps as a percentage of their real memory allocation.\n -- Add scontrol ability to update job step's time limits.\n -- Add scontrol ability to update job's NumCPUs count.\n -- Add --time-min options to salloc, sbatch and srun. The scontrol command\n    has been modified to display and modify the new field. sched/backfill\n    plugin has been changed to alter time limits of jobs with the\n    --time-min option if doing so permits earlier job initiation.\n -- Add support for TotalView symbol MPIR_partial_attach_ok with srun support\n    to release processes which TotalView does not attach to.\n -- Add new option for SelectTypeParameters of CR_ONE_TASK_PER_CORE. This\n    option will allocate one task per core by default. Without this option,\n    by default one task will be allocated per thread on nodes with more than\n    one ThreadsPerCore configured.\n -- Avoid accounting separately for a current pid corresponds to a Light Weight\n    Process (Thread POSIX) appearing in the /proc directory. Only account for\n    the original process (pid==tgid) to avoid accounting for memory use more\n    than once.\n -- Add proctrack/cgroup plugin which uses Linux control groups (aka cgroup)\n    to track processes on Linux systems having this feature enabled (kernel\n    >= 2.6.24).\n -- Add logging of license transations including job_id.\n -- Add configuration parameters SlurmSchedLogFile and SlurmSchedLogLevel to\n    support writing scheduling events to a separate log file.\n -- Added contribs/web_apps/chart_stats.cgi, a web app that invokes sreport to\n    retrieve from the accounting storage db a user's request for job usage or\n    machine utilization statistics and charts the results to a browser.\n -- Massive change to the schema in the storage_accounting/mysql plugin.  When\n    starting the slurmdbd the process of conversion may take a few minutes.\n    You might also see some errors such as 'error: mysql_query failed: 1206\n    The total number of locks exceeds the lock table size'.  If you get this,\n    do not worry, it is because your setting of innodb_buffer_pool_size in\n    your my.cnf file is not set or set too low.  A decent value there should\n    be 64M or higher depending on the system you are running on.  See\n    RELEASE_NOTES for more information.  But setting this and then\n    restarting the mysqld and slurmdbd will put things right.  After this\n    change we have noticed 50-75% increase in performance with sreport and\n    sacct.\n -- Fix for MaxCPUs to honor partitions of 1 node that have more than the\n    maxcpus for a job.\n -- Add support for \"scontrol notify <message>\" to work for batch jobs.\n\n* Changes in SLURM 2.2.0.pre1\n=============================\n -- Added RunTime field to scontrol show job report\n -- Added SLURM_VERSION_NUMBER and removed SLURM_API_VERSION from\n    slurm/slurm.h.\n -- Added support to handle communication with SLURM 2.1 clusters.  Job's\n    should not be lost in the future when upgrading to higher versions of\n    SLURM.\n -- Added withdeleted options for listing clusters, users, and accounts\n -- Remove PLPA task affinity functions due to that package being deprecated.\n -- Preserve current partition state information and node Feature and Weight\n    information rather than use contents of slurm.conf file after slurmctld\n    restart with -R option or SIGHUP. Replace information with contents of\n    slurm.conf after slurmctld restart without -R or \"scontrol reconfigure\".\n    See RELEASE_NOTES file fore more details.\n -- Modify SLURM's PMI library (for MPICH2) to properly execute an executable\n    program stand-alone (single MPI task launched without srun).\n -- Made GrpCPUs and MaxCPUs limits work for select/cons_res.\n -- Moved all SQL dependant plugins into a seperate rpm slurm-sql.  This\n    should be needed only where a connection to a database is needed (i.e.\n    where the slurmdbd is running)\n -- Add command line option \"no_sys_info\" to PAM module to supress system\n    logging of \"access granted for user ...\", access denied and other errors\n    will still be logged.\n -- sinfo -R now has the user and timestamp in separate fields from the reason.\n -- Much functionality has been added to account_storage/pgsql.  The plugin\n    is still in a very beta state.  It is still highly advised to use the\n    mysql plugin, but if you feel like living on the edge or just really\n    like postgres over mysql for some reason here you go. (Work done\n    primarily by Hongjia Cao, NUDT.)\n\n* Changes in SLURM 2.1.17\n=========================\n -- Correct format of --begin reported in salloc, sbatch and srun --help\n    message.\n -- Correct logic for regular users to increase nice value of their own jobs.\n\n* Changes in SLURM 2.1.16\n=========================\n -- Fixed minor warnings from gcc-4.5\n -- Fixed initialization of accounting_stroage_enforce in the slurmctld.\n -- Fixed bug where if GrpNodes was lowered while pending jobs existed and where\n    above the limit the slurmctld would seg fault.\n -- Fixed minor memory leak when unpack error happens on an\n    association_shares_object_t.\n -- Set Lft and Rgt correctly when adding association.  Fix for regression\n    caused in 2.1.15, cosmetic fix only.\n -- Replaced optarg which was undefined in some spots to make sure ENV vars are\n    set up correctly.\n -- When removing an account from a cluster with sacctmgr you no longer get\n    a list of previously deleted associations.\n -- Fix to make jobcomp/(pg/my)sql correctly work when the database name is\n    different than the default.\n\n* Changes in SLURM 2.1.15\n=========================\n -- Fix bug in which backup slurmctld can purge job scripts (and kill batch\n    jobs) when it assumes primary control, particularly when this happens\n    multiple times in a short time interval.\n -- In sched/wiki and sched/wiki2 add IWD (Initial Working Directory) to the\n    information reported about jobs.\n -- Fix bug in calculating a daily or weekly reservation start time when the\n    reservation is updated. Patch from Per Lundqvist (National Supercomputer\n    Centre, Link\u00f6ping University, Sweden).\n -- Fix bug in how job step memory limits are calculated when the --relative\n    option is used.\n -- Restore operation of srun -X option to forward SIGINT to spawned tasks\n    without killing them.\n -- Fixed a bug in calculating the root account's raw usage reported by Par\n    Andersson\n -- Fixed a bug in sshare displaying account hierarchy reported by Per\n    Lundqvist.\n -- In select/linear plugin, when a node allocated to a running job is removed\n    from a partition, only log the event once. Fixes problem reported by Per\n    Lundqvist.\n\n* Changes in SLURM 2.1.14\n=========================\n -- Fixed coding mistakes in _slurm_rpc_resv_show() and job_alloc_info() found\n    while reviewing the code.\n -- Fix select/cons_res logic to prevent allocating resources while jobs\n    previously allocated resources on the node are still completing.\n -- Fixed typo in job_mgr.c dealing with qos instead of associations.\n -- Make sure associations and qos' are initiated when added.\n -- Fixed wrong initialization for wckeys in the association manager.\n -- Added wiki.conf configuration parameter of HidePartitionNodes. See\n    \"man wiki.conf\" for more information.\n -- Add \"JobAggregationTime=#\" field SchedulerParameter configuration parameter\n    output.\n -- Modify init.d/slurm and slurmdbd scripts to prevent the possible\n    inadvertent inclusion of \".\" in LD_LIBRARY_PATH environment variable.\n    To fail, the script would need to be executed by user root or SlurmUser\n    without the LD_LIBRARY_PATH environment variable set and there would\n    have to be a maliciously altered library in the working directory.\n    Thanks to Raphael Geissert for identifying the problem. This addresses\n    security vulnerability CVE-2010-3380.\n\n* Changes in SLURM 2.1.13\n=========================\n -- Fix race condition which can set a node state to IDLE on slurmctld startup\n    even if it has running jobs.\n\n* Changes in SLURM 2.1.12\n=========================\n -- Fixes for building on OS X 10.5.\n -- Fixed a few '-' without a '\\' in front of them in the man pages.\n -- Fixed issues in client tools where a requeued job did get displayed\n    correctly.\n -- Update typos in doc/html/accounting.shtml doc/html/resource_limits.shtml\n    doc/man/man5/slurmdbd.conf.5 and doc/man/man5/slurm.conf.5\n -- Fixed a bug in exitcode:signal display in sacct\n -- Fix bug when request comes in for consumable resources and the -c option\n    is used in conjunction with -O\n -- Fixed squeue -o \"%h\" output formatting\n -- Change select/linear message \"error: job xxx: best_fit topology failure\"\n    to debug type.\n -- BLUEGENE - Fix for sinfo -R to group all midplanes together in a single\n    line for midplanes in an error state instead of 1 line for each midplane.\n -- Fix srun to work correctly with --uid when getting an allocation\n    and creating a step, also fix salloc to assume identity at the correct\n    time as well.\n -- BLUEGENE - Fixed issue with jobs being refused when running dynamic mode\n    and every job on the system happens to be the same size.\n -- Removed bad #define _SLURMD_H from slurmd/get_mach_stat.h.  Didn't appear\n    to cause any problems being there, just incorrect syntax.\n -- Validate the job ID when salloc or srun receive an SRUN_JOB_COMPLETE RPC to\n    avoid killing the wrong job if the original command exits and the port gets\n    re-used by another command right away.\n -- Fix to node in correct state in accounting when updating it to drain from\n    scontrol/sview.\n -- BLUEGENE - Removed incorrect unlocking on error cases when starting jobs.\n -- Improve logging of invalid sinfo and squeue print options.\n -- BLUEGENE - Added check to libsched_if to allow root to run even outside of\n    SLURM.  This is needed when running certain blocks outside of SLURM in HTC\n    mode.\n\n* Changes in SLURM 2.1.11-2\n===========================\n -- BLUEGENE - make it so libsched_if.so is named correctly on 'L' it is\n    libsched_if64.so and on 'P' it is libsched_if.so\n\n* Changes in SLURM 2.1.11\n=========================\n -- BLUEGENE - fix sinfo to not get duplicate entries when running command\n    sinfo -e -o \"%9P %6m %.4c %.22F %f\"\n -- Fix bug that caused segv when deleting a partition with pending jobs.\n -- Better error message for when trying to modify an account's name with\n    sacctmgr.\n -- Added back removal of #include \"src/common/slurm_xlator.h\" from\n    select/cons_res.\n -- Fix incorrect logic in global_accounting in regression tests for\n    setting QOS.\n -- BLUEGENE - Fixed issue where removing a small block in dynamic mode,\n    and other blocks also in that midplane needed to be removed and were in\n    and error state.  They all weren't removed correctly in accounting.\n -- Prevent scontrol segv with \"scontrol show node <name>\" command with nodes\n    in a hidden partition.\n -- Fixed sizing of popup grids in sview.\n -- Fixed sacct when querying against a jobid the start time is not set.\n -- Fix configure to get correct version of pkg-config if both 32bit and 64bit\n    libs are installed.\n -- Fix issue with sshare not sorting correctly the tree of associations.\n -- Update documentation for sreport.\n -- BLUEGENE - fix regression in 2.1.10 on assigning multiple jobs to one block.\n -- Minor memory leak fixed when killing job error happens.\n -- Fix sacctmgr list assoc when talking to a 2.2 slurmdbd.\n\n* Changes in SLURM 2.1.10\n=========================\n -- Fix memory leak in sched/builtin plugin.\n -- Fixed sbatch to work correctly when no nodes are specified, but\n    --ntasks-per-node is.\n -- Make sure account and wckey for a job are lower case before inserting into\n    accounting.\n -- Added note to squeue documentation about --jobs option displaying jobs\n    even if they are on hidden partitions.\n -- Fix srun to work correctly with --uid when getting an allocation\n    and creating a step.\n -- Fix for when removing a limit from a users association inside the\n    fairshare tree the parents limit is now inherited automatically in\n    the slurmctld.  Previously the slurmctld would have to be restarted.\n    This problem only exists when setting a users association limit to -1.\n -- Patch from Matthieu Hautreux (CEA) dealing with possible overflows that\n    could come up with the select/cons_res plugin with uint32_t's being treated\n    as uint16_t's.\n -- Correct logic for creating a reservation with a Duration=Infinite (used to\n    set reservation end time in the past).\n -- Correct logic for creating a reservation that properly handles the OVERLAP\n    and IGNORE_JOBS flags (flags were ignored under some conditions).\n -- Fixed a fair-share calculation bug in the priority/multifactor plugin.\n -- Make sure a user entry in the database that was previously deleted is\n    restored clean when added back, i.e. remove admin privileges previously\n    given.\n -- BLUEGENE - Future start time is set correctly when eligible time for a job\n    is in the future, but the job can physically run earlier.\n -- Updated Documentation for sacctmgr for Wall and CPUMin options stating when\n    the limit is reached running jobs will be killed.\n -- Fix deadlock issue in the slurmctld when lowering limits in accounting to\n    lower than that of pending jobs.\n -- Fix bug in salloc, sbatch and srun that could under some conditions process\n    the --threads-per-core, --cores-per-socket and --sockets-per-node options\n    improperly.\n -- Fix bug in select/cons_res with memory management plus job preemption with\n    job removal (e.g. requeue) which under some conditions failed to preempt\n    jobs.\n -- Fix deadlock potential when using qos and associations in the slurmctld.\n -- Update documentation to state --ntasks-per-* is for a maximum value\n    instead of an absolute.\n -- Get ReturnToService=2 working for front-end configurations (e.g. Cray or\n    BlueGene).\n -- Do not make a non-responding node available for use after running\n    \"scontrol update nodename=<name> state=resume\". Wait for node to respond\n    before use.\n -- Added slurm_xlator.h to jobacct_gather plugins so they resolve symbols\n    correctly when linking to the slurm api.\n -- You can now update a jobs QOS from scontrol.  Previously you could only do\n    this from sview.\n -- BLUEGENE - Fixed bug where if running in non-dynamic mode sometimes the\n    start time returned for a job when using test-only would not be correct.\n\n* Changes in SLURM 2.1.9\n========================\n -- In select/linear - Fix logic to prevent over-subscribing memory with shared\n    nodes (Shared=YES or Shared=FORCE).\n -- Fix for handling -N and --ntasks-per-node without specifying -n with\n    salloc and sbatch.\n -- Fix jobacct_gather/linux if not polling on tasks to give tasks time to\n    start before doing initial gather.\n -- When changing priority with the multifactor plugin we make sure we update\n    the last_job_update variable.\n -- Fixed sview for gtk < 2.10 to display correct debug level at first.\n -- Fixed sview to not select too fast when using a mouse right click.\n -- Fixed sacct to display correct timelimits for jobs from accounting.\n -- Fixed sacct when running as root by default query all users as documented.\n -- In proctrack/linuxproc, skip over files in /proc that are not really user\n    processes (e.g. \"/proc/bus\").\n -- Fix documentation bug for slurmdbd.conf\n -- Fix slurmctld to update qos preempt list without restart.\n -- Fix bug in select/cons_res that in some cases would prevent a preempting job\n    from using of resources already allocated to a preemptable running job.\n -- Fix for sreport in interactive mode to honor parsable/2 options.\n -- Fixed minor bugs in sacct and sstat commands\n -- BLUEGENE - Fixed issue if the slurmd becomes unresponsive and you have\n    blocks in an error state accounting is correct when the slurmd comes\n    back up.\n -- Corrected documentation for -n option in srun/salloc/sbatch\n -- BLUEGENE - when running a willrun test along with preemption the bluegene\n    plugin now does the correct thing.\n -- Fix possible memory corruption issue which can cause slurmctld to abort.\n -- BLUEGENE - fixed small memory leak when setting up env.\n -- Fixed deadlock if using accounting and cluster changes size in the\n    database.  This can happen if you mistakenly have multiple primary\n    slurmctld's running for a single cluster, which should rarely if ever\n    happen.\n -- Fixed sacct -c option.\n -- Critical bug fix in sched/backfill plugin that caused memory corruption.\n\n* Changes in SLURM 2.1.8\n========================\n -- Update BUILD_NOTES for AIX and bgp systems on how to get sview to\n    build correctly.\n -- Update man page for scontrol when nodes are in the \"MIXED\" state.\n -- Better error messages for sacctmgr.\n -- Fix bug in allocation of CPUs with select/cons_res and --cpus-per-task\n    option.\n -- Fix bug in dependency support for afterok and afternotok options to insure\n    that the job's exit status gets checked for dependent jobs prior to puring\n    completed job records.\n -- Fix bug in sched/backfill that could set an incorrect expected start time\n    for a job.\n -- BLUEGENE - Fix for systems that have midplanes defined in the database\n    that don't exist.\n -- Accounting, fixed bug where if removing an object a rollback wasn't\n    possible.\n -- Fix possible scontrol stack corruption when listing jobs with very a long\n    job or working directory name (over 511 characters).\n -- Insure that SPANK environment variables set by salloc or sbatch get\n    propagated to the Prolog on all nodes by setting SLURM_SPANK_* environment\n    variables for srun's use.\n -- In sched/wiki2 - Add support for the MODIFYJOB command to alter a job's\n    comment field\n -- When a cluster first registers with the SlurmDBD only send nodes in an\n    non-usable state.  Before all nodes were sent.\n -- Alter sacct to be able to query jobs by association id.\n -- Edit documentation for scontrol stating ExitCode as something not alterable.\n -- Update documentation about ReturnToService and silently rebooting nodes.\n -- When combining --ntasks-per-node and --exclusive in an allocation request\n    the correct thing, giving the allocation the entire node but only\n    ntasks-per-node, happens.\n -- Fix accounting transaction logs when deleting associations to put the\n    ids instead of the lfts which could change over time.\n -- Fix support for salloc, sbatch and srun's --hint option to avoid allocating\n    a job more sockets per node or more cores per socket than desired. Also\n    when --hint=compute_bound or --hint=memory_bound then avoid allocating more\n    than one task per hyperthread (a change in behavior, but almost certainly\n    a preferable mode of operation).\n\n* Changes in SLURM 2.1.7\n========================\n -- Modify srun, salloc and sbatch parsing for the --signal option to accept\n    either a signal name in addition to the previously supported signal\n    numbers (e.g. \"--signal=USR2@200\").\n -- BLUEGENE - Fixed sinfo --long --Node output for cpus on a single cnode.\n -- In sched/wiki2 - Fix another logic bug in support of Moab being able to\n    identify preemptable jobs.\n -- In sched/wiki2 - For BlueGene systems only: Fix bug preventing Moab from\n    being able to correctly change the node count of pending jobs.\n -- In select/cons_res - Fix bug preventing job preemption with a configuration\n    of Shared=FORCE:1 and PreemptMode=GANG,SUSPEND.\n -- In the TaskProlog, add support for an \"unset\" option to clear environment\n    variables for the user application. Also add support for embedded white-\n    space in the environment variables exported to the user application\n    (everything after the equal sign to the end of the line is included without\n    alteration).\n -- Do not install /etc/init.d/slurm or /etc/init.d/slurmdbd on AIX systems.\n -- BLUEGENE - fixed check for small blocks if a node card of a midplane is\n    in an error state other jobs can still run on the midplane on other\n    nodecards.\n -- BLUEGENE - Check to make sure job killing is in the active job table in\n    DB2 when killing the job.\n -- Correct logic to support ResvOverRun configuration parameter.\n -- Get --acctg-freq option working for srun and salloc commands.\n -- Fix sinfo display of drained nodes correctly with the summarize flag.\n -- Fix minor memory leaks in slurmd and slurmstepd.\n -- Better error messages for failed step launch.\n -- Modify srun to insure compatability of the --relative option with the node\n    count requested.\n\n* Changes in SLURM 2.1.6-2\n==========================\n -- In sched/wiki2 - Fix logic in support of Moab being able to identify\n    preemptable jobs.\n -- Applied fixes to a debug4 message in priority_multifactor.c sent in by\n    Per Lundqvist\n -- BLUEGENE - Fixed issue where incorrect nodecards could be picked when\n    looking at combining small blocks to make a larger small block.\n\n* Changes in SLURM 2.1.6\n========================\n -- For newly submitted jobs, report expected start time in squeue --start as\n    \"N/A\" rather than current time.\n -- Correct sched/backfill logic so that it runs in a more timely fashion.\n -- Fixed issue if running on accounting cache and priority/multifactor to\n    initialize the root association when the database comes back up.\n -- Emulated BLUEGENE - fixed issue where blocks weren't always created\n    correctly when loading from state.  This does not apply to a real\n    bluegene system, only emulated.\n -- Fixed bug when job is completing and its cpu_cnt would be calculated\n    incorrectly, possibly resulting in an underflow being logged.\n -- Fixed bug where if there are pending jobs in a partition which was\n    updated to have no nodes in it the slurmctld would dump core.\n -- Fixed smap and sview to display partitions with no nodes in them.\n -- Improve configure script's logic to detect LUA libraries.\n -- Fix bug that could cause slurmctld to abort if select/cons_res is used AND a\n    job is submitted using the --no-kill option AND one of the job's nodes goes\n    DOWN AND slurmctld restarts while that job is still running.\n -- In jobcomp plugins, job time limit was sometimes recorded improperly if not\n    set by user (recorded huge number rather than partition's time limit).\n\n* Changes in SLURM 2.1.5\n========================\n -- BLUEGENE - Fixed display of draining nodes for sinfo -R.\n -- Fixes to scontrol and sview when setting a job to an impossible start time.\n -- Added -h to srun for help.\n -- Fix for sacctmgr man page to remove erroneous 'with' statements.\n -- Fix for unpacking jobs with accounting statistics, previously it appears\n    only steps were unpacked correctly, for the most case sacct would only\n    display this information making this fix a very minor one.\n -- Changed scontrol and sview output for jobs with unknown end times from\n    'NONE' to 'Unknown'.\n -- Fixed mysql plugin to reset classification when adding a\n    previously deleted cluster.\n -- Permit a batch script to reset umask and have that propagate to tasks\n    spawed by subsequent srun. Previously the umask in effect when sbatch was\n    executed was propagated to tasks spawed by srun.\n -- Modify slurm_job_cpus_allocated_on_node_id() and\n    slurm_job_cpus_allocated_on_node() functions to not write explanation of\n    failures to stderr. Only return -1 and set errno.\n -- Correction in configurator.html script. Prolog and Epilog were reversed.\n -- BLUEGENE - Fixed race condition where if a nodecard has an error on an\n    un-booted block when a job comes to use it before the state checking\n    thread notices it which could cause the slurmctld to lock up on a\n    non-dynamic system.\n -- In select/cons_res with FastSchedule=0 and Procs=# defined for the node,\n    but no specific socket/core/thread count configured, avoid fatal error if\n    the number of cores on a node is less than the number of Procs configured.\n -- Added ability for the perlapi to utilize opaque data types returned from\n    the C api.\n -- BLUEGENE - made the perlapi get correct values for cpus per node,\n    Previously it would give the number of cpus per cnode instead of midplane.\n -- BLEUGENE - Fixed issue where if a block being selected for a job to use\n    and during the process a hardware failure happens, previously the block\n    would still be allowed to be used which would fail or requeue the job\n    depending on the configuration.\n -- For SPANK job environment, avoid duplicate \"SPANK_\" prefix for environment\n    set by sbatch jobs.\n -- Make squeue select jobs on hidden partitions when requesting more than one.\n -- Avoid automatically cancelling job steps when all of the tasks on some node\n    have gracefully terminated.\n\n* Changes in SLURM 2.1.4\n========================\n -- Fix for purge script in accounting to use correct options.\n -- If SelectType=select/linear and SelectTypeParameters=CR_Memory fix bug that\n    would fail to release memory reserved for a job if \"scontrol reconfigure\"\n    is executed while the job is in completing state.\n -- Fix bug in handling event trigger for job time limit while job is still\n    in pending state.\n -- Fixed display of Ave/MaxCPU in sacct for jobs. Steps were printed\n    correctly.\n -- When node current features differs from slurm.conf, log the node names\n    using a hostlist expression rather than listing individual node names.\n -- Improve ability of srun to abort job step for some task launch failures.\n -- Fix mvapich plugin logic to release the created job allocation on\n    initialization failure (previously the failures would cancel job step,\n    but retain job allocation).\n -- Fix bug in srun for task count so large that it overflows int data type.\n -- Fix important bug in select/cons_res handling of ntasks-per-core parameter\n    that was uncovered by a bug fixed in v2.1.3. Bug produced fatal error for\n    slurmctld: \"cons_res: cpus computation error\".\n -- Fix bug in select/cons_res handling of partitions configured with\n    Shared=YES. Prior logic failed to support running multiple jobs per node.\n\n* Changes in SLURM 2.1.3-2\n==========================\n -- Modified spec file to obsolete pam_slurm when installing\n    the slurm-pam_slurm rpm.\n\n* Changes in SLURM 2.1.3-1\n==========================\n -- BLUEGENE - Fix issues on static/overlap systems where if a midplane\n    was drained you would not be able to create new blocks on it.\n -- In sched/wiki2 (for Moab): Add excluded host list to job information\n    using new keyword \"EXCLUDE_HOSTLIST\".\n -- Correct slurmd reporting of incorrect socket/core/thread counts.\n -- For sched/wiki2 (Moab): Do not extend a job's end time for suspend/resume\n    or startup delay due to node boot time. A job's end time will always be\n    its start time plus time limit.\n -- Added build-time option (to configure program) of --with-pam_dir to\n    specify the directory into which PAM modules get installed, although it\n    should pick the proper directory by default. \"make install\" and \"rpmbuild\"\n    should now put the pam_slurm.so file in the proper directory.\n -- Modify PAM module to link against SLURM API shared library and use exported\n    slurm_hostlist functions.\n -- Do not block new jobs with --immediate option while another job is in the\n    process of being requeued (which can take a long time for some node failure\n    modes).\n -- For topology/tree, log invalid hostnames in a single hostlist expression\n    rather than one per line.\n -- A job step's default time limit will be UNLIMITED rather than partition's\n    default time limit. The step will automatically be cancelled as part of the\n    job termination logic when the job's time limit is reached.\n -- sacct - fixed bug when checking jobs against a reservation\n -- In select/cons_res, fix support for job allocation with --ntasks_per_node\n    option. Previously could allocate too few CPUs on some nodes.\n -- Adjustment made to init message to the slurmdbd to allow backwards\n    compatibility with future 2.2 release. YOU NEED TO UPGRADE SLURMDBD\n    BEFORE ANYTHING ELSE.\n -- Fix accounting when comment of down/drained node has double quotes in it.\n\n* Changes in SLURM 2.1.2\n========================\n -- Added nodelist to sview for jobs on non-bluegene systems\n -- Correction in value of batch job environment variable SLURM_TASKS_PER_NODE\n    under some conditions.\n -- When a node silently fails which is already drained/down the reason\n    for draining for the node is not changed.\n -- Srun will ignore SLURM_NNODES environment variable and use the count of\n    currently allocated nodes if that count changes during the job's lifetime\n    (e.g. job allocation uses the --no-kill option and a node goes DOWN, job\n    step would previously always fail).\n -- Made it so sacctmgr can't add blank user or account.  The MySQL plugin\n    will also reject such requests.\n -- Revert libpmi.so version for compatibility with SLURM version 2.0 and\n    earlier to avoid forcing applications using a specific libpmi.so version to\n    rebuild unnecessarily (revert from libpmi.so.21.0.0 to libpmi.so.0.0.0).\n -- Restore support for a pending job's constraints (required node features)\n    when slurmctld is restarted (internal structure needed to be rebuilt).\n -- Removed checkpoint_blcr.so from the plugin rpm in the slurm.spec since\n    it is also in the blcr rpm.\n -- Fixed issue in sview where you were unable to edit the count\n    of jobs to share resources.\n -- BLUEGENE - Fixed issue where tasks on steps weren't being displayed\n    correctly with scontrol and sview.\n -- BLUEGENE - fixed wiki2 plugin to report correct task count for pending\n    jobs.\n -- BLUEGENE - Added /etc/ld.so.conf.d/slurm.conf to point to the\n    directory holding libsched_if64.so when building rpms.\n -- Adjust get_wckeys call in slurmdbd to allow operators to list wckeys.\n\n* Changes in SLURM 2.1.1\n========================\n -- Fix for case sensitive databases when a slurmctld has a mixed case\n    clustername to lower case the string to easy compares.\n -- Fix squeue if job is completing and failed to print remaining\n    nodes instead of failed message.\n -- Fix sview core when searching for partitions by state.\n -- Fixed setting the start time when querying in sacct to the\n    beginning of the day if not set previously.\n -- Defined slurm_free_reservation_info_msg and slurm_free_topo_info_msg\n    in common/slurm_protocol_defs.h\n -- Avoid generating error when a job step includes a memory specification and\n    memory is not configured as a consumable resource.\n -- Patch for small memory leak in src/common/plugstack.c\n -- Fix sview search on node state.\n -- Fix bug in which improperly formed job dependency specification can cause\n    slurmctld to abort.\n -- Fixed issue where slurmctld wouldn't always get a message to send cluster\n    information when registering for the first time with the slurmdbd.\n -- Add slurm_*_trigger.3 man pages for event trigger APIs.\n -- Fix bug in job preemption logic that would free allocated memory twice.\n -- Fix spelling issues (from Gennaro Oliva)\n -- Fix issue when changing parents of an account in accounting all children\n    weren't always sent to their respected slurmctlds until a restart.\n -- Restore support for srun/salloc/sbatch option --hint=nomultithread to\n    bind tasks to cores rather than threads (broken in slurm v2.1.0-pre5).\n -- Fix issue where a 2.0 sacct could not talk correctly to a 2.1 slurmdbd.\n -- BLUEGENE - Fix issue where no partitions have any nodes assigned them to\n    alert user no blocks can be created.\n -- BLUEGENE - Fix smap to put BGP images when using -Dc on a Blue Gene/P\n    system.\n -- Set SLURM_SUBMIT_DIR environment variable for srun and salloc commands to\n    match behavior of sbatch command.\n -- Report WorkDir from \"scontrol show job\" command for jobs launched using\n    salloc and srun.\n -- Update correctly the wckey when changing it on a pending job.\n -- Set wckeyid correctly in accounting when cancelling a pending job.\n -- BLUEGENE - critical fix where jobs would be killed incorrectly.\n -- BLUEGENE - fix for sview putting multiple ionodes on to nodelists when\n    viewing the jobs tab.\n\n* Changes in SLURM 2.1.0\n========================\n -- Improve sview layout of blocks in use.\n -- A user can now change the dimensions of the grid in sview.\n -- BLUEGENE - improved startup speed further for large numbers of defined\n    blocks\n -- Fix to _get_job_min_nodes() in wiki2/get_jobs.c suggested by Michal Novotny\n -- BLUEGENE - fixed issues when updating a pending job when a node\n    count was incorrect for the asked for connection type.\n -- BLUEGENE - fixed issue when combining blocks that are in ready states to\n    make a larger block from those or make multiple smaller blocks by\n    splitting the larger block.  Previously this would only work with block\n    in a free state.\n -- Fix bug in wiki(2) plugins where if HostFormat=2 and the task list is\n    greater than 64 we don't truncate.  Previously this would mess up Moab\n    by sending a truncated task list when doing a get jobs.\n -- Added update slurmctld debug level to sview when in admin mode.\n -- Added logic to make sure if enforcing a memory limit when using the\n    jobacct_gather plugin a user can no longer turn off the logic to enforce\n    the limit.\n -- Replaced many calls to getpwuid() with reentrant uid_to_string()\n -- The slurmstepd will now refresh it's log file handle on a reconfig,\n    previously if a log was rolled any output from the stepd was lost.\n", "/*****************************************************************************\\\n *  src/slurmd/slurmd/req.c - slurmd request handling\n *****************************************************************************\n *  Copyright (C) 2002-2007 The Regents of the University of California.\n *  Copyright (C) 2008-2010 Lawrence Livermore National Security.\n *  Portions Copyright (C) 2010-2016 SchedMD LLC.\n *  Portions copyright (C) 2015 Mellanox Technologies Inc.\n *  Produced at Lawrence Livermore National Laboratory (cf, DISCLAIMER).\n *  Written by Mark Grondona <mgrondona@llnl.gov>.\n *  CODE-OCEC-09-009. All rights reserved.\n *\n *  This file is part of SLURM, a resource management program.\n *  For details, see <http://slurm.schedmd.com/>.\n *  Please also read the included file: DISCLAIMER.\n *\n *  SLURM is free software; you can redistribute it and/or modify it under\n *  the terms of the GNU General Public License as published by the Free\n *  Software Foundation; either version 2 of the License, or (at your option)\n *  any later version.\n *\n *  In addition, as a special exception, the copyright holders give permission\n *  to link the code of portions of this program with the OpenSSL library under\n *  certain conditions as described in each individual source file, and\n *  distribute linked combinations including the two. You must obey the GNU\n *  General Public License in all respects for all of the code used other than\n *  OpenSSL. If you modify file(s) with this exception, you may extend this\n *  exception to your version of the file(s), but you are not obligated to do\n *  so. If you do not wish to do so, delete this exception statement from your\n *  version.  If you delete this exception statement from all source files in\n *  the program, then also delete it here.\n *\n *  SLURM is distributed in the hope that it will be useful, but WITHOUT ANY\n *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n *  details.\n *\n *  You should have received a copy of the GNU General Public License along\n *  with SLURM; if not, write to the Free Software Foundation, Inc.,\n *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.\n\\*****************************************************************************/\n#if HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include <fcntl.h>\n#include <grp.h>\n#include <pthread.h>\n#include <sched.h>\n#include <signal.h>\n#include <stdlib.h>\n#include <string.h>\n#include <time.h>\n#include <sys/param.h>\n#include <poll.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <sys/un.h>\n#include <sys/wait.h>\n#include <unistd.h>\n#include <utime.h>\n\n#include \"src/common/callerid.h\"\n#include \"src/common/cpu_frequency.h\"\n#include \"src/common/env.h\"\n#include \"src/common/fd.h\"\n#include \"src/common/forward.h\"\n#include \"src/common/gres.h\"\n#include \"src/common/hostlist.h\"\n#include \"src/common/list.h\"\n#include \"src/common/log.h\"\n#include \"src/common/macros.h\"\n#include \"src/common/msg_aggr.h\"\n#include \"src/common/node_features.h\"\n#include \"src/common/node_select.h\"\n#include \"src/common/plugstack.h\"\n#include \"src/common/read_config.h\"\n#include \"src/common/siphash.h\"\n#include \"src/common/slurm_auth.h\"\n#include \"src/common/slurm_cred.h\"\n#include \"src/common/slurm_acct_gather_energy.h\"\n#include \"src/common/slurm_jobacct_gather.h\"\n#include \"src/common/slurm_protocol_defs.h\"\n#include \"src/common/slurm_protocol_api.h\"\n#include \"src/common/slurm_protocol_interface.h\"\n#include \"src/common/slurm_strcasestr.h\"\n#include \"src/common/stepd_api.h\"\n#include \"src/common/uid.h\"\n#include \"src/common/util-net.h\"\n#include \"src/common/xstring.h\"\n#include \"src/common/xmalloc.h\"\n\n#include \"src/bcast/file_bcast.h\"\n\n#include \"src/slurmd/slurmd/get_mach_stat.h\"\n#include \"src/slurmd/slurmd/slurmd.h\"\n\n#include \"src/slurmd/common/job_container_plugin.h\"\n#include \"src/slurmd/common/proctrack.h\"\n#include \"src/slurmd/common/run_script.h\"\n#include \"src/slurmd/common/reverse_tree_math.h\"\n#include \"src/slurmd/common/slurmstepd_init.h\"\n#include \"src/slurmd/common/task_plugin.h\"\n\n#define _LIMIT_INFO 0\n\n#define RETRY_DELAY 15\t\t/* retry every 15 seconds */\n#define MAX_RETRY   240\t\t/* retry 240 times (one hour max) */\n\n#define EPIL_RETRY_MAX 2\t/* max retries of epilog complete message */\n\n#ifndef MAXHOSTNAMELEN\n#define MAXHOSTNAMELEN\t64\n#endif\n\ntypedef struct {\n\tint ngids;\n\tgid_t *gids;\n} gids_t;\n\ntypedef struct {\n\tuint32_t job_id;\n\tuint32_t step_id;\n\tuint32_t job_mem;\n\tuint32_t step_mem;\n} job_mem_limits_t;\n\ntypedef struct {\n\tuint32_t job_id;\n\tuint32_t step_id;\n} starting_step_t;\n\ntypedef struct {\n\tuint32_t job_id;\n\tuint16_t msg_timeout;\n\tbool *prolog_fini;\n\tpthread_cond_t *timer_cond;\n\tpthread_mutex_t *timer_mutex;\n} timer_struct_t;\n\ntypedef struct {\n\tuint32_t jobid;\n\tuint32_t step_id;\n\tchar *node_list;\n\tchar *partition;\n\tchar *resv_id;\n\tchar **spank_job_env;\n\tuint32_t spank_job_env_size;\n\tuid_t uid;\n\tchar *user_name;\n} job_env_t;\n\nstatic int  _abort_step(uint32_t job_id, uint32_t step_id);\nstatic char **_build_env(job_env_t *job_env);\nstatic void _delay_rpc(int host_inx, int host_cnt, int usec_per_rpc);\nstatic void _destroy_env(char **env);\nstatic bool _is_batch_job_finished(uint32_t job_id);\nstatic void _job_limits_free(void *x);\nstatic int  _job_limits_match(void *x, void *key);\nstatic bool _job_still_running(uint32_t job_id);\nstatic int  _kill_all_active_steps(uint32_t jobid, int sig, bool batch);\nstatic void _launch_complete_add(uint32_t job_id);\nstatic void _launch_complete_log(char *type, uint32_t job_id);\nstatic void _launch_complete_rm(uint32_t job_id);\nstatic void _launch_complete_wait(uint32_t job_id);\nstatic int  _launch_job_fail(uint32_t job_id, uint32_t slurm_rc);\nstatic bool _launch_job_test(uint32_t job_id);\nstatic void _note_batch_job_finished(uint32_t job_id);\nstatic int  _prolog_is_running (uint32_t jobid);\nstatic int  _step_limits_match(void *x, void *key);\nstatic int  _terminate_all_steps(uint32_t jobid, bool batch);\nstatic void _rpc_launch_tasks(slurm_msg_t *);\nstatic void _rpc_abort_job(slurm_msg_t *);\nstatic void _rpc_batch_job(slurm_msg_t *msg, bool new_msg);\nstatic void _rpc_prolog(slurm_msg_t *msg);\nstatic void _rpc_job_notify(slurm_msg_t *);\nstatic void _rpc_signal_tasks(slurm_msg_t *);\nstatic void _rpc_checkpoint_tasks(slurm_msg_t *);\nstatic void _rpc_complete_batch(slurm_msg_t *);\nstatic void _rpc_terminate_tasks(slurm_msg_t *);\nstatic void _rpc_timelimit(slurm_msg_t *);\nstatic void _rpc_reattach_tasks(slurm_msg_t *);\nstatic void _rpc_signal_job(slurm_msg_t *);\nstatic void _rpc_suspend_job(slurm_msg_t *msg);\nstatic void _rpc_terminate_job(slurm_msg_t *);\nstatic void _rpc_update_time(slurm_msg_t *);\nstatic void _rpc_shutdown(slurm_msg_t *msg);\nstatic void _rpc_reconfig(slurm_msg_t *msg);\nstatic void _rpc_reboot(slurm_msg_t *msg);\nstatic void _rpc_pid2jid(slurm_msg_t *msg);\nstatic int  _rpc_file_bcast(slurm_msg_t *msg);\nstatic void _file_bcast_cleanup(void);\nstatic int  _file_bcast_register_file(slurm_msg_t *msg,\n\t\t\t\t      file_bcast_info_t *key);\nstatic int  _rpc_ping(slurm_msg_t *);\nstatic int  _rpc_health_check(slurm_msg_t *);\nstatic int  _rpc_acct_gather_update(slurm_msg_t *);\nstatic int  _rpc_acct_gather_energy(slurm_msg_t *);\nstatic int  _rpc_step_complete(slurm_msg_t *msg);\nstatic int  _rpc_step_complete_aggr(slurm_msg_t *msg);\nstatic int  _rpc_stat_jobacct(slurm_msg_t *msg);\nstatic int  _rpc_list_pids(slurm_msg_t *msg);\nstatic int  _rpc_daemon_status(slurm_msg_t *msg);\nstatic int  _run_epilog(job_env_t *job_env);\nstatic int  _run_prolog(job_env_t *job_env, slurm_cred_t *cred);\nstatic void _rpc_forward_data(slurm_msg_t *msg);\nstatic int  _rpc_network_callerid(slurm_msg_t *msg);\nstatic void _dealloc_gids(gids_t *p);\n\n\nstatic bool _pause_for_job_completion(uint32_t jobid, char *nodes,\n\t\tint maxtime);\nstatic bool _slurm_authorized_user(uid_t uid);\nstatic void _sync_messages_kill(kill_job_msg_t *req);\nstatic int  _waiter_init (uint32_t jobid);\nstatic int  _waiter_complete (uint32_t jobid);\n\nstatic bool _steps_completed_now(uint32_t jobid);\nstatic int  _valid_sbcast_cred(file_bcast_msg_t *req, uid_t req_uid,\n\t\t\t       uint16_t block_no, uint32_t *job_id);\nstatic void _wait_state_completed(uint32_t jobid, int max_delay);\nstatic uid_t _get_job_uid(uint32_t jobid);\n\nstatic gids_t *_gids_cache_lookup(char *user, gid_t gid);\n\nstatic int  _add_starting_step(uint16_t type, void *req);\nstatic int  _remove_starting_step(uint16_t type, void *req);\nstatic int  _compare_starting_steps(void *s0, void *s1);\nstatic int  _wait_for_starting_step(uint32_t job_id, uint32_t step_id);\nstatic bool _step_is_starting(uint32_t job_id, uint32_t step_id);\n\nstatic void _add_job_running_prolog(uint32_t job_id);\nstatic void _remove_job_running_prolog(uint32_t job_id);\nstatic int  _match_jobid(void *s0, void *s1);\nstatic void _wait_for_job_running_prolog(uint32_t job_id);\nstatic bool _requeue_setup_env_fail(void);\n\n/*\n *  List of threads waiting for jobs to complete\n */\nstatic List waiters;\n\nstatic pthread_mutex_t launch_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic time_t startup = 0;\t\t/* daemon startup time */\nstatic time_t last_slurmctld_msg = 0;\n\nstatic pthread_mutex_t job_limits_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic List job_limits_list = NULL;\nstatic bool job_limits_loaded = false;\n\n#define FINI_JOB_CNT 32\nstatic pthread_mutex_t fini_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic uint32_t fini_job_id[FINI_JOB_CNT];\nstatic int next_fini_job_inx = 0;\n\n/* NUM_PARALLEL_SUSP_JOBS controls the number of jobs that can be suspended or\n * resumed at one time. */\n#define NUM_PARALLEL_SUSP_JOBS 64\n/* NUM_PARALLEL_SUSP_STEPS controls the number of steps per job that can be\n * suspended at one time. */\n#define NUM_PARALLEL_SUSP_STEPS 8\nstatic pthread_mutex_t suspend_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic uint32_t job_suspend_array[NUM_PARALLEL_SUSP_JOBS];\nstatic int job_suspend_size = 0;\n\n#define JOB_STATE_CNT 64\nstatic pthread_mutex_t job_state_mutex   = PTHREAD_MUTEX_INITIALIZER;\nstatic pthread_cond_t  job_state_cond    = PTHREAD_COND_INITIALIZER;\nstatic uint32_t active_job_id[JOB_STATE_CNT];\n\nstatic pthread_mutex_t prolog_mutex = PTHREAD_MUTEX_INITIALIZER;\n\n#define FILE_BCAST_TIMEOUT 300\nstatic pthread_mutex_t file_bcast_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic pthread_cond_t  file_bcast_cond  = PTHREAD_COND_INITIALIZER;\nstatic int fb_read_lock = 0, fb_write_wait_lock = 0, fb_write_lock = 0;\nstatic List file_bcast_list = NULL;\n\nvoid\nslurmd_req(slurm_msg_t *msg)\n{\n\tint rc;\n\n\tif (msg == NULL) {\n\t\tif (startup == 0)\n\t\t\tstartup = time(NULL);\n\t\tFREE_NULL_LIST(waiters);\n\t\tslurm_mutex_lock(&job_limits_mutex);\n\t\tif (job_limits_list) {\n\t\t\tFREE_NULL_LIST(job_limits_list);\n\t\t\tjob_limits_loaded = false;\n\t\t}\n\t\tslurm_mutex_unlock(&job_limits_mutex);\n\t\treturn;\n\t}\n\n\tswitch (msg->msg_type) {\n\tcase REQUEST_LAUNCH_PROLOG:\n\t\tdebug2(\"Processing RPC: REQUEST_LAUNCH_PROLOG\");\n\t\t_rpc_prolog(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_BATCH_JOB_LAUNCH:\n\t\tdebug2(\"Processing RPC: REQUEST_BATCH_JOB_LAUNCH\");\n\t\t/* Mutex locking moved into _rpc_batch_job() due to\n\t\t * very slow prolog on Blue Gene system. Only batch\n\t\t * jobs are supported on Blue Gene (no job steps). */\n\t\t_rpc_batch_job(msg, true);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_LAUNCH_TASKS:\n\t\tdebug2(\"Processing RPC: REQUEST_LAUNCH_TASKS\");\n\t\tslurm_mutex_lock(&launch_mutex);\n\t\t_rpc_launch_tasks(msg);\n\t\tslurm_mutex_unlock(&launch_mutex);\n\t\tbreak;\n\tcase REQUEST_SIGNAL_TASKS:\n\t\tdebug2(\"Processing RPC: REQUEST_SIGNAL_TASKS\");\n\t\t_rpc_signal_tasks(msg);\n\t\tbreak;\n\tcase REQUEST_CHECKPOINT_TASKS:\n\t\tdebug2(\"Processing RPC: REQUEST_CHECKPOINT_TASKS\");\n\t\t_rpc_checkpoint_tasks(msg);\n\t\tbreak;\n\tcase REQUEST_TERMINATE_TASKS:\n\t\tdebug2(\"Processing RPC: REQUEST_TERMINATE_TASKS\");\n\t\t_rpc_terminate_tasks(msg);\n\t\tbreak;\n\tcase REQUEST_KILL_PREEMPTED:\n\t\tdebug2(\"Processing RPC: REQUEST_KILL_PREEMPTED\");\n\t\tlast_slurmctld_msg = time(NULL);\n\t\t_rpc_timelimit(msg);\n\t\tbreak;\n\tcase REQUEST_KILL_TIMELIMIT:\n\t\tdebug2(\"Processing RPC: REQUEST_KILL_TIMELIMIT\");\n\t\tlast_slurmctld_msg = time(NULL);\n\t\t_rpc_timelimit(msg);\n\t\tbreak;\n\tcase REQUEST_REATTACH_TASKS:\n\t\tdebug2(\"Processing RPC: REQUEST_REATTACH_TASKS\");\n\t\t_rpc_reattach_tasks(msg);\n\t\tbreak;\n\tcase REQUEST_SIGNAL_JOB:\n\t\tdebug2(\"Processing RPC: REQUEST_SIGNAL_JOB\");\n\t\t_rpc_signal_job(msg);\n\t\tbreak;\n\tcase REQUEST_SUSPEND_INT:\n\t\tdebug2(\"Processing RPC: REQUEST_SUSPEND_INT\");\n\t\t_rpc_suspend_job(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_ABORT_JOB:\n\t\tdebug2(\"Processing RPC: REQUEST_ABORT_JOB\");\n\t\tlast_slurmctld_msg = time(NULL);\n\t\t_rpc_abort_job(msg);\n\t\tbreak;\n\tcase REQUEST_TERMINATE_JOB:\n\t\tdebug2(\"Processing RPC: REQUEST_TERMINATE_JOB\");\n\t\tlast_slurmctld_msg = time(NULL);\n\t\t_rpc_terminate_job(msg);\n\t\tbreak;\n\tcase REQUEST_COMPLETE_BATCH_SCRIPT:\n\t\tdebug2(\"Processing RPC: REQUEST_COMPLETE_BATCH_SCRIPT\");\n\t\t_rpc_complete_batch(msg);\n\t\tbreak;\n\tcase REQUEST_UPDATE_JOB_TIME:\n\t\tdebug2(\"Processing RPC: REQUEST_UPDATE_JOB_TIME\");\n\t\t_rpc_update_time(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_SHUTDOWN:\n\t\tdebug2(\"Processing RPC: REQUEST_SHUTDOWN\");\n\t\t_rpc_shutdown(msg);\n\t\tbreak;\n\tcase REQUEST_RECONFIGURE:\n\t\tdebug2(\"Processing RPC: REQUEST_RECONFIGURE\");\n\t\t_rpc_reconfig(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_REBOOT_NODES:\n\t\tdebug2(\"Processing RPC: REQUEST_REBOOT_NODES\");\n\t\t_rpc_reboot(msg);\n\t\tbreak;\n\tcase REQUEST_NODE_REGISTRATION_STATUS:\n\t\tdebug2(\"Processing RPC: REQUEST_NODE_REGISTRATION_STATUS\");\n\t\t/* Treat as ping (for slurmctld agent, just return SUCCESS) */\n\t\trc = _rpc_ping(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\t/* Then initiate a separate node registration */\n\t\tif (rc == SLURM_SUCCESS)\n\t\t\tsend_registration_msg(SLURM_SUCCESS, true);\n\t\tbreak;\n\tcase REQUEST_PING:\n\t\t_rpc_ping(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_HEALTH_CHECK:\n\t\tdebug2(\"Processing RPC: REQUEST_HEALTH_CHECK\");\n\t\t_rpc_health_check(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_ACCT_GATHER_UPDATE:\n\t\tdebug2(\"Processing RPC: REQUEST_ACCT_GATHER_UPDATE\");\n\t\t_rpc_acct_gather_update(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_ACCT_GATHER_ENERGY:\n\t\tdebug2(\"Processing RPC: REQUEST_ACCT_GATHER_ENERGY\");\n\t\t_rpc_acct_gather_energy(msg);\n\t\tbreak;\n\tcase REQUEST_JOB_ID:\n\t\t_rpc_pid2jid(msg);\n\t\tbreak;\n\tcase REQUEST_FILE_BCAST:\n\t\trc = _rpc_file_bcast(msg);\n\t\tslurm_send_rc_msg(msg, rc);\n\t\tbreak;\n\tcase REQUEST_STEP_COMPLETE:\n\t\t(void) _rpc_step_complete(msg);\n\t\tbreak;\n\tcase REQUEST_STEP_COMPLETE_AGGR:\n\t\t(void) _rpc_step_complete_aggr(msg);\n\t\tbreak;\n\tcase REQUEST_JOB_STEP_STAT:\n\t\t(void) _rpc_stat_jobacct(msg);\n\t\tbreak;\n\tcase REQUEST_JOB_STEP_PIDS:\n\t\t(void) _rpc_list_pids(msg);\n\t\tbreak;\n\tcase REQUEST_DAEMON_STATUS:\n\t\t_rpc_daemon_status(msg);\n\t\tbreak;\n\tcase REQUEST_JOB_NOTIFY:\n\t\t_rpc_job_notify(msg);\n\t\tbreak;\n\tcase REQUEST_FORWARD_DATA:\n\t\t_rpc_forward_data(msg);\n\t\tbreak;\n\tcase REQUEST_NETWORK_CALLERID:\n\t\tdebug2(\"Processing RPC: REQUEST_NETWORK_CALLERID\");\n\t\t_rpc_network_callerid(msg);\n\t\tbreak;\n\tcase MESSAGE_COMPOSITE:\n\t\terror(\"Processing RPC: MESSAGE_COMPOSITE: \"\n\t\t      \"This should never happen\");\n\t\tmsg_aggr_add_msg(msg, 0, NULL);\n\t\tbreak;\n\tcase RESPONSE_MESSAGE_COMPOSITE:\n\t\tdebug2(\"Processing RPC: RESPONSE_MESSAGE_COMPOSITE\");\n\t\tmsg_aggr_resp(msg);\n\t\tbreak;\n\tdefault:\n\t\terror(\"slurmd_req: invalid request msg type %d\",\n\t\t      msg->msg_type);\n\t\tslurm_send_rc_msg(msg, EINVAL);\n\t\tbreak;\n\t}\n\treturn;\n}\nstatic int _send_slurmd_conf_lite (int fd, slurmd_conf_t *cf)\n{\n\tint len;\n\tBuf buffer = init_buf(0);\n\tslurm_mutex_lock(&cf->config_mutex);\n\tpack_slurmd_conf_lite(cf, buffer);\n\tslurm_mutex_unlock(&cf->config_mutex);\n\tlen = get_buf_offset(buffer);\n\tsafe_write(fd, &len, sizeof(int));\n\tsafe_write(fd, get_buf_data(buffer), len);\n\tfree_buf(buffer);\n\treturn (0);\n rwfail:\n\treturn (-1);\n}\n\nstatic int\n_send_slurmstepd_init(int fd, int type, void *req,\n\t\t      slurm_addr_t *cli, slurm_addr_t *self,\n\t\t      hostset_t step_hset, uint16_t protocol_version)\n{\n\tint len = 0;\n\tBuf buffer = NULL;\n\tslurm_msg_t msg;\n\tuid_t uid = (uid_t)-1;\n\tgid_t gid = (uid_t)-1;\n\tgids_t *gids = NULL;\n\n\tint rank, proto;\n\tint parent_rank, children, depth, max_depth;\n\tchar *parent_alias = NULL;\n\tchar *user_name = NULL;\n\tslurm_addr_t parent_addr = {0};\n\tchar pwd_buffer[PW_BUF_SIZE];\n\tstruct passwd pwd, *pwd_result;\n\n\tslurm_msg_t_init(&msg);\n\t/* send type over to slurmstepd */\n\tsafe_write(fd, &type, sizeof(int));\n\n\t/* step_hset can be NULL for batch scripts OR if the job was submitted\n\t * by SlurmUser or root using the --no-allocate/-Z option and the job\n\t * job credential validation by _check_job_credential() failed. If the\n\t * job credential did not validate, then it did not come from slurmctld\n\t * and there is no reason to send step completion messages to slurmctld.\n\t */\n\tif (step_hset == NULL) {\n\t\tbool send_error = false;\n\t\tif (type == LAUNCH_TASKS) {\n\t\t\tlaunch_tasks_request_msg_t *launch_req;\n\t\t\tlaunch_req = (launch_tasks_request_msg_t *) req;\n\t\t\tif (launch_req->job_step_id != SLURM_EXTERN_CONT)\n\t\t\t\tsend_error = true;\n\t\t}\n\t\tif (send_error) {\n\t\t\tinfo(\"task rank unavailable due to invalid job \"\n\t\t\t     \"credential, step completion RPC impossible\");\n\t\t}\n\t\trank = -1;\n\t\tparent_rank = -1;\n\t\tchildren = 0;\n\t\tdepth = 0;\n\t\tmax_depth = 0;\n\t} else if ((type == LAUNCH_TASKS) &&\n\t\t   (((launch_tasks_request_msg_t *)req)->alias_list)) {\n\t\t/* In the cloud, each task talks directly to the slurmctld\n\t\t * since node addressing is abnormal */\n\t\trank = 0;\n\t\tparent_rank = -1;\n\t\tchildren = 0;\n\t\tdepth = 0;\n\t\tmax_depth = 0;\n\t} else {\n#ifndef HAVE_FRONT_END\n\t\tint count;\n\t\tcount = hostset_count(step_hset);\n\t\trank = hostset_find(step_hset, conf->node_name);\n\t\treverse_tree_info(rank, count, REVERSE_TREE_WIDTH,\n\t\t\t\t  &parent_rank, &children,\n\t\t\t\t  &depth, &max_depth);\n\t\tif (rank > 0) { /* rank 0 talks directly to the slurmctld */\n\t\t\tint rc;\n\t\t\t/* Find the slurm_addr_t of this node's parent slurmd\n\t\t\t * in the step host list */\n\t\t\tparent_alias = hostset_nth(step_hset, parent_rank);\n\t\t\trc = slurm_conf_get_addr(parent_alias, &parent_addr);\n\t\t\tif (rc != SLURM_SUCCESS) {\n\t\t\t\terror(\"Failed looking up address for \"\n\t\t\t\t      \"NodeName %s\", parent_alias);\n\t\t\t\t/* parent_rank = -1; */\n\t\t\t}\n\t\t}\n#else\n\t\t/* In FRONT_END mode, one slurmd pretends to be all\n\t\t * NodeNames, so we can't compare conf->node_name\n\t\t * to the NodeNames in step_hset.  Just send step complete\n\t\t * RPC directly to the controller.\n\t\t */\n\t\trank = 0;\n\t\tparent_rank = -1;\n\t\tchildren = 0;\n\t\tdepth = 0;\n\t\tmax_depth = 0;\n#endif\n\t}\n\tdebug3(\"slurmstepd rank %d (%s), parent rank %d (%s), \"\n\t       \"children %d, depth %d, max_depth %d\",\n\t       rank, conf->node_name,\n\t       parent_rank, parent_alias ? parent_alias : \"NONE\",\n\t       children, depth, max_depth);\n\tif (parent_alias)\n\t\tfree(parent_alias);\n\n\t/* send reverse-tree info to the slurmstepd */\n\tsafe_write(fd, &rank, sizeof(int));\n\tsafe_write(fd, &parent_rank, sizeof(int));\n\tsafe_write(fd, &children, sizeof(int));\n\tsafe_write(fd, &depth, sizeof(int));\n\tsafe_write(fd, &max_depth, sizeof(int));\n\tsafe_write(fd, &parent_addr, sizeof(slurm_addr_t));\n\n\t/* send conf over to slurmstepd */\n\tif (_send_slurmd_conf_lite(fd, conf) < 0)\n\t\tgoto rwfail;\n\n\t/* send cli address over to slurmstepd */\n\tbuffer = init_buf(0);\n\tslurm_pack_slurm_addr(cli, buffer);\n\tlen = get_buf_offset(buffer);\n\tsafe_write(fd, &len, sizeof(int));\n\tsafe_write(fd, get_buf_data(buffer), len);\n\tfree_buf(buffer);\n\tbuffer = NULL;\n\n\t/* send self address over to slurmstepd */\n\tif (self) {\n\t\tbuffer = init_buf(0);\n\t\tslurm_pack_slurm_addr(self, buffer);\n\t\tlen = get_buf_offset(buffer);\n\t\tsafe_write(fd, &len, sizeof(int));\n\t\tsafe_write(fd, get_buf_data(buffer), len);\n\t\tfree_buf(buffer);\n\t\tbuffer = NULL;\n\n\t} else {\n\t\tlen = 0;\n\t\tsafe_write(fd, &len, sizeof(int));\n\t}\n\n\t/* Send GRES information to slurmstepd */\n\tgres_plugin_send_stepd(fd);\n\n\t/* send cpu_frequency info to slurmstepd */\n\tcpu_freq_send_info(fd);\n\n\t/* send req over to slurmstepd */\n\tswitch(type) {\n\tcase LAUNCH_BATCH_JOB:\n\t\tgid = (uid_t)((batch_job_launch_msg_t *)req)->gid;\n\t\tuid = (uid_t)((batch_job_launch_msg_t *)req)->uid;\n\t\tuser_name = ((batch_job_launch_msg_t *)req)->user_name;\n\t\tmsg.msg_type = REQUEST_BATCH_JOB_LAUNCH;\n\t\tbreak;\n\tcase LAUNCH_TASKS:\n\t\t/*\n\t\t * The validity of req->uid was verified against the\n\t\t * auth credential in _rpc_launch_tasks().  req->gid\n\t\t * has NOT yet been checked!\n\t\t */\n\t\tgid = (uid_t)((launch_tasks_request_msg_t *)req)->gid;\n\t\tuid = (uid_t)((launch_tasks_request_msg_t *)req)->uid;\n\t\tuser_name = ((launch_tasks_request_msg_t *)req)->user_name;\n\t\tmsg.msg_type = REQUEST_LAUNCH_TASKS;\n\t\tbreak;\n\tdefault:\n\t\terror(\"Was sent a task I didn't understand\");\n\t\tbreak;\n\t}\n\tbuffer = init_buf(0);\n\tmsg.data = req;\n\n\tif (protocol_version == (uint16_t)NO_VAL)\n\t\tproto = SLURM_PROTOCOL_VERSION;\n\telse\n\t\tproto = protocol_version;\n\n\tmsg.protocol_version = (uint16_t)proto;\n\tpack_msg(&msg, buffer);\n\tlen = get_buf_offset(buffer);\n\n\tsafe_write(fd, &proto, sizeof(int));\n\n\tsafe_write(fd, &len, sizeof(int));\n\tsafe_write(fd, get_buf_data(buffer), len);\n\tfree_buf(buffer);\n\tbuffer = NULL;\n\n#ifdef HAVE_NATIVE_CRAY\n\t/* Try to avoid calling this on a system which is a native\n\t * cray.  getpwuid_r is slow on the compute nodes and this has\n\t * in theory been verified earlier.\n\t */\n\tif (!user_name) {\n#endif\n\t\t/* send cached group ids array for the relevant uid */\n\t\tdebug3(\"_send_slurmstepd_init: call to getpwuid_r\");\n\t\tif (slurm_getpwuid_r(uid, &pwd, pwd_buffer, PW_BUF_SIZE,\n\t\t\t\t     &pwd_result) || (pwd_result == NULL)) {\n\t\t\terror(\"%s: getpwuid_r: %m\", __func__);\n\t\t\tlen = 0;\n\t\t\tsafe_write(fd, &len, sizeof(int));\n\t\t\terrno = ESLURMD_UID_NOT_FOUND;\n\t\t\treturn errno;\n\t\t}\n\t\tdebug3(\"%s: return from getpwuid_r\", __func__);\n\t\tif (gid != pwd_result->pw_gid) {\n\t\t\tdebug(\"%s: Changing gid from %d to %d\",\n\t\t\t      __func__, gid, pwd_result->pw_gid);\n\t\t}\n\t\tgid = pwd_result->pw_gid;\n\t\tif (!user_name)\n\t\t\tuser_name = pwd_result->pw_name;\n#ifdef HAVE_NATIVE_CRAY\n\t}\n#endif\n\tif (!user_name) {\n\t\t/* Sanity check since gids_cache_lookup will fail\n\t\t * with a NULL. */\n\t\terror(\"%s: No user name for %d: %m\", __func__, uid);\n\t\tlen = 0;\n\t\tsafe_write(fd, &len, sizeof(int));\n\t\terrno = ESLURMD_UID_NOT_FOUND;\n\t\treturn errno;\n\t}\n\n\tif ((gids = _gids_cache_lookup(user_name, gid))) {\n\t\tint i;\n\t\tuint32_t tmp32;\n\t\tsafe_write(fd, &gids->ngids, sizeof(int));\n\t\tfor (i = 0; i < gids->ngids; i++) {\n\t\t\ttmp32 = (uint32_t)gids->gids[i];\n\t\t\tsafe_write(fd, &tmp32, sizeof(uint32_t));\n\t\t}\n\t\t_dealloc_gids(gids);\n\t} else {\n\t\tlen = 0;\n\t\tsafe_write(fd, &len, sizeof(int));\n\t}\n\treturn 0;\n\nrwfail:\n\tif (buffer)\n\t\tfree_buf(buffer);\n\terror(\"_send_slurmstepd_init failed\");\n\treturn errno;\n}\n\n\n/*\n * Fork and exec the slurmstepd, then send the slurmstepd its\n * initialization data.  Then wait for slurmstepd to send an \"ok\"\n * message before returning.  When the \"ok\" message is received,\n * the slurmstepd has created and begun listening on its unix\n * domain socket.\n *\n * Note that this code forks twice and it is the grandchild that\n * becomes the slurmstepd process, so the slurmstepd's parent process\n * will be init, not slurmd.\n */\nstatic int\n_forkexec_slurmstepd(uint16_t type, void *req,\n\t\t     slurm_addr_t *cli, slurm_addr_t *self,\n\t\t     const hostset_t step_hset, uint16_t protocol_version)\n{\n\tpid_t pid;\n\tint to_stepd[2] = {-1, -1};\n\tint to_slurmd[2] = {-1, -1};\n\n\tif (pipe(to_stepd) < 0 || pipe(to_slurmd) < 0) {\n\t\terror(\"_forkexec_slurmstepd pipe failed: %m\");\n\t\treturn SLURM_FAILURE;\n\t}\n\n\tif (_add_starting_step(type, req)) {\n\t\terror(\"_forkexec_slurmstepd failed in _add_starting_step: %m\");\n\t\treturn SLURM_FAILURE;\n\t}\n\n\tif ((pid = fork()) < 0) {\n\t\terror(\"_forkexec_slurmstepd: fork: %m\");\n\t\tclose(to_stepd[0]);\n\t\tclose(to_stepd[1]);\n\t\tclose(to_slurmd[0]);\n\t\tclose(to_slurmd[1]);\n\t\t_remove_starting_step(type, req);\n\t\treturn SLURM_FAILURE;\n\t} else if (pid > 0) {\n\t\tint rc = SLURM_SUCCESS;\n#if (SLURMSTEPD_MEMCHECK == 0)\n\t\tint i;\n\t\ttime_t start_time = time(NULL);\n#endif\n\t\t/*\n\t\t * Parent sends initialization data to the slurmstepd\n\t\t * over the to_stepd pipe, and waits for the return code\n\t\t * reply on the to_slurmd pipe.\n\t\t */\n\t\tif (close(to_stepd[0]) < 0)\n\t\t\terror(\"Unable to close read to_stepd in parent: %m\");\n\t\tif (close(to_slurmd[1]) < 0)\n\t\t\terror(\"Unable to close write to_slurmd in parent: %m\");\n\n\t\tif ((rc = _send_slurmstepd_init(to_stepd[1], type,\n\t\t\t\t\t\treq, cli, self,\n\t\t\t\t\t\tstep_hset,\n\t\t\t\t\t\tprotocol_version)) != 0) {\n\t\t\terror(\"Unable to init slurmstepd\");\n\t\t\tgoto done;\n\t\t}\n\n\t\t/* If running under valgrind/memcheck, this pipe doesn't work\n\t\t * correctly so just skip it. */\n#if (SLURMSTEPD_MEMCHECK == 0)\n\t\ti = read(to_slurmd[0], &rc, sizeof(int));\n\t\tif (i < 0) {\n\t\t\terror(\"%s: Can not read return code from slurmstepd \"\n\t\t\t      \"got %d: %m\", __func__, i);\n\t\t\trc = SLURM_FAILURE;\n\t\t} else if (i != sizeof(int)) {\n\t\t\terror(\"%s: slurmstepd failed to send return code \"\n\t\t\t      \"got %d: %m\", __func__, i);\n\t\t\trc = SLURM_FAILURE;\n\t\t} else {\n\t\t\tint delta_time = time(NULL) - start_time;\n\t\t\tint cc;\n\t\t\tif (delta_time > 5) {\n\t\t\t\tinfo(\"Warning: slurmstepd startup took %d sec, \"\n\t\t\t\t     \"possible file system problem or full \"\n\t\t\t\t     \"memory\", delta_time);\n\t\t\t}\n\t\t\tif (rc != SLURM_SUCCESS)\n\t\t\t\terror(\"slurmstepd return code %d\", rc);\n\n\t\t\tcc = SLURM_SUCCESS;\n\t\t\tcc = write(to_stepd[1], &cc, sizeof(int));\n\t\t\tif (cc != sizeof(int)) {\n\t\t\t\terror(\"%s: failed to send ack to stepd %d: %m\",\n\t\t\t\t      __func__, cc);\n\t\t\t}\n\t\t}\n#endif\n\tdone:\n\t\tif (_remove_starting_step(type, req))\n\t\t\terror(\"Error cleaning up starting_step list\");\n\n\t\t/* Reap child */\n\t\tif (waitpid(pid, NULL, 0) < 0)\n\t\t\terror(\"Unable to reap slurmd child process\");\n\t\tif (close(to_stepd[1]) < 0)\n\t\t\terror(\"close write to_stepd in parent: %m\");\n\t\tif (close(to_slurmd[0]) < 0)\n\t\t\terror(\"close read to_slurmd in parent: %m\");\n\t\treturn rc;\n\t} else {\n#if (SLURMSTEPD_MEMCHECK == 1)\n\t\t/* memcheck test of slurmstepd, option #1 */\n\t\tchar *const argv[3] = {\"memcheck\",\n\t\t\t\t       (char *)conf->stepd_loc, NULL};\n#elif (SLURMSTEPD_MEMCHECK == 2)\n\t\t/* valgrind test of slurmstepd, option #2 */\n\t\tuint32_t job_id = 0, step_id = 0;\n\t\tchar log_file[256];\n\t\tchar *const argv[13] = {\"valgrind\", \"--tool=memcheck\",\n\t\t\t\t\t\"--error-limit=no\",\n\t\t\t\t\t\"--leak-check=summary\",\n\t\t\t\t\t\"--show-reachable=yes\",\n\t\t\t\t\t\"--max-stackframe=16777216\",\n\t\t\t\t\t\"--num-callers=20\",\n\t\t\t\t\t\"--child-silent-after-fork=yes\",\n\t\t\t\t\t\"--track-origins=yes\",\n\t\t\t\t\tlog_file, (char *)conf->stepd_loc,\n\t\t\t\t\tNULL};\n\t\tif (type == LAUNCH_BATCH_JOB) {\n\t\t\tjob_id = ((batch_job_launch_msg_t *)req)->job_id;\n\t\t\tstep_id = ((batch_job_launch_msg_t *)req)->step_id;\n\t\t} else if (type == LAUNCH_TASKS) {\n\t\t\tjob_id = ((launch_tasks_request_msg_t *)req)->job_id;\n\t\t\tstep_id = ((launch_tasks_request_msg_t *)req)->job_step_id;\n\t\t}\n\t\tsnprintf(log_file, sizeof(log_file),\n\t\t\t \"--log-file=/tmp/slurmstepd_valgrind_%u.%u\",\n\t\t\t job_id, step_id);\n#elif (SLURMSTEPD_MEMCHECK == 3)\n\t\t/* valgrind/drd test of slurmstepd, option #3 */\n\t\tuint32_t job_id = 0, step_id = 0;\n\t\tchar log_file[256];\n\t\tchar *const argv[10] = {\"valgrind\", \"--tool=drd\",\n\t\t\t\t\t\"--error-limit=no\",\n\t\t\t\t\t\"--max-stackframe=16777216\",\n\t\t\t\t\t\"--num-callers=20\",\n\t\t\t\t\t\"--child-silent-after-fork=yes\",\n\t\t\t\t\tlog_file, (char *)conf->stepd_loc,\n\t\t\t\t\tNULL};\n\t\tif (type == LAUNCH_BATCH_JOB) {\n\t\t\tjob_id = ((batch_job_launch_msg_t *)req)->job_id;\n\t\t\tstep_id = ((batch_job_launch_msg_t *)req)->step_id;\n\t\t} else if (type == LAUNCH_TASKS) {\n\t\t\tjob_id = ((launch_tasks_request_msg_t *)req)->job_id;\n\t\t\tstep_id = ((launch_tasks_request_msg_t *)req)->job_step_id;\n\t\t}\n\t\tsnprintf(log_file, sizeof(log_file),\n\t\t\t \"--log-file=/tmp/slurmstepd_valgrind_%u.%u\",\n\t\t\t job_id, step_id);\n#elif (SLURMSTEPD_MEMCHECK == 4)\n\t\t/* valgrind/helgrind test of slurmstepd, option #4 */\n\t\tuint32_t job_id = 0, step_id = 0;\n\t\tchar log_file[256];\n\t\tchar *const argv[10] = {\"valgrind\", \"--tool=helgrind\",\n\t\t\t\t\t\"--error-limit=no\",\n\t\t\t\t\t\"--max-stackframe=16777216\",\n\t\t\t\t\t\"--num-callers=20\",\n\t\t\t\t\t\"--child-silent-after-fork=yes\",\n\t\t\t\t\tlog_file, (char *)conf->stepd_loc,\n\t\t\t\t\tNULL};\n\t\tif (type == LAUNCH_BATCH_JOB) {\n\t\t\tjob_id = ((batch_job_launch_msg_t *)req)->job_id;\n\t\t\tstep_id = ((batch_job_launch_msg_t *)req)->step_id;\n\t\t} else if (type == LAUNCH_TASKS) {\n\t\t\tjob_id = ((launch_tasks_request_msg_t *)req)->job_id;\n\t\t\tstep_id = ((launch_tasks_request_msg_t *)req)->job_step_id;\n\t\t}\n\t\tsnprintf(log_file, sizeof(log_file),\n\t\t\t \"--log-file=/tmp/slurmstepd_valgrind_%u.%u\",\n\t\t\t job_id, step_id);\n#else\n\t\t/* no memory checking, default */\n\t\tchar *const argv[2] = { (char *)conf->stepd_loc, NULL};\n#endif\n\t\tint i;\n\t\tint failed = 0;\n\t\t/* inform slurmstepd about our config */\n\t\tsetenv(\"SLURM_CONF\", conf->conffile, 1);\n\n\t\t/*\n\t\t * Child forks and exits\n\t\t */\n\t\tif (setsid() < 0) {\n\t\t\terror(\"_forkexec_slurmstepd: setsid: %m\");\n\t\t\tfailed = 1;\n\t\t}\n\t\tif ((pid = fork()) < 0) {\n\t\t\terror(\"_forkexec_slurmstepd: \"\n\t\t\t      \"Unable to fork grandchild: %m\");\n\t\t\tfailed = 2;\n\t\t} else if (pid > 0) { /* child */\n\t\t\texit(0);\n\t\t}\n\n\t\t/*\n\t\t * Just incase we (or someone we are linking to)\n\t\t * opened a file and didn't do a close on exec.  This\n\t\t * is needed mostly to protect us against libs we link\n\t\t * to that don't set the flag as we should already be\n\t\t * setting it for those that we open.  The number 256\n\t\t * is an arbitrary number based off test7.9.\n\t\t */\n\t\tfor (i=3; i<256; i++) {\n\t\t\t(void) fcntl(i, F_SETFD, FD_CLOEXEC);\n\t\t}\n\n\t\t/*\n\t\t * Grandchild exec's the slurmstepd\n\t\t *\n\t\t * If the slurmd is being shutdown/restarted before\n\t\t * the pipe happens the old conf->lfd could be reused\n\t\t * and if we close it the dup2 below will fail.\n\t\t */\n\t\tif ((to_stepd[0] != conf->lfd)\n\t\t    && (to_slurmd[1] != conf->lfd))\n\t\t\tslurm_shutdown_msg_engine(conf->lfd);\n\n\t\tif (close(to_stepd[1]) < 0)\n\t\t\terror(\"close write to_stepd in grandchild: %m\");\n\t\tif (close(to_slurmd[0]) < 0)\n\t\t\terror(\"close read to_slurmd in parent: %m\");\n\n\t\t(void) close(STDIN_FILENO); /* ignore return */\n\t\tif (dup2(to_stepd[0], STDIN_FILENO) == -1) {\n\t\t\terror(\"dup2 over STDIN_FILENO: %m\");\n\t\t\texit(1);\n\t\t}\n\t\tfd_set_close_on_exec(to_stepd[0]);\n\t\t(void) close(STDOUT_FILENO); /* ignore return */\n\t\tif (dup2(to_slurmd[1], STDOUT_FILENO) == -1) {\n\t\t\terror(\"dup2 over STDOUT_FILENO: %m\");\n\t\t\texit(1);\n\t\t}\n\t\tfd_set_close_on_exec(to_slurmd[1]);\n\t\t(void) close(STDERR_FILENO); /* ignore return */\n\t\tif (dup2(devnull, STDERR_FILENO) == -1) {\n\t\t\terror(\"dup2 /dev/null to STDERR_FILENO: %m\");\n\t\t\texit(1);\n\t\t}\n\t\tfd_set_noclose_on_exec(STDERR_FILENO);\n\t\tlog_fini();\n\t\tif (!failed) {\n\t\t\tif (conf->chos_loc && !access(conf->chos_loc, X_OK))\n\t\t\t\texecvp(conf->chos_loc, argv);\n\t\t\telse\n\t\t\t\texecvp(argv[0], argv);\n\t\t\terror(\"exec of slurmstepd failed: %m\");\n\t\t}\n\t\texit(2);\n\t}\n}\n\n\n/*\n * The job(step) credential is the only place to get a definitive\n * list of the nodes allocated to a job step.  We need to return\n * a hostset_t of the nodes. Validate the incoming RPC, updating\n * job_mem needed.\n */\nstatic int\n_check_job_credential(launch_tasks_request_msg_t *req, uid_t uid,\n\t\t      int node_id, hostset_t *step_hset,\n\t\t      uint16_t protocol_version)\n{\n\tslurm_cred_arg_t arg;\n\thostset_t\ts_hset = NULL;\n\tbool\t\tuser_ok = _slurm_authorized_user(uid);\n\tbool\t\tverified = true;\n\tint\t\thost_index = -1;\n\tint\t\trc;\n\tslurm_cred_t    *cred = req->cred;\n\tuint32_t\tjobid = req->job_id;\n\tuint32_t\tstepid = req->job_step_id;\n\tint\t\ttasks_to_launch = req->tasks_to_launch[node_id];\n\tuint32_t\tjob_cpus = 0, step_cpus = 0;\n\n\t/*\n\t * First call slurm_cred_verify() so that all valid\n\t * credentials are checked\n\t */\n\trc = slurm_cred_verify(conf->vctx, cred, &arg, protocol_version);\n\tif (rc < 0) {\n\t\tverified = false;\n\t\tif ((!user_ok) || (errno != ESLURMD_INVALID_JOB_CREDENTIAL))\n\t\t\treturn SLURM_ERROR;\n\t\telse {\n\t\t\tdebug(\"_check_job_credential slurm_cred_verify failed:\"\n\t\t\t      \" %m, but continuing anyway.\");\n\t\t}\n\t}\n\n\t/* If uid is the SlurmUser or root and the credential is bad,\n\t * then do not attempt validating the credential */\n\tif (!verified) {\n\t\t*step_hset = NULL;\n\t\tif (rc >= 0) {\n\t\t\tif ((s_hset = hostset_create(arg.step_hostlist)))\n\t\t\t\t*step_hset = s_hset;\n\t\t\tslurm_cred_free_args(&arg);\n\t\t}\n\t\treturn SLURM_SUCCESS;\n\t}\n\n\tif ((arg.jobid != jobid) || (arg.stepid != stepid)) {\n\t\terror(\"job credential for %u.%u, expected %u.%u\",\n\t\t      arg.jobid, arg.stepid, jobid, stepid);\n\t\tgoto fail;\n\t}\n\n\tif (arg.uid != uid) {\n\t\terror(\"job credential created for uid %ld, expected %ld\",\n\t\t      (long) arg.uid, (long) uid);\n\t\tgoto fail;\n\t}\n\n\t/*\n\t * Check that credential is valid for this host\n\t */\n\tif (!(s_hset = hostset_create(arg.step_hostlist))) {\n\t\terror(\"Unable to parse credential hostlist: `%s'\",\n\t\t      arg.step_hostlist);\n\t\tgoto fail;\n\t}\n\n\tif (!hostset_within(s_hset, conf->node_name)) {\n\t\terror(\"Invalid job %u.%u credential for user %u: \"\n\t\t      \"host %s not in hostset %s\",\n\t\t      arg.jobid, arg.stepid, arg.uid,\n\t\t      conf->node_name, arg.step_hostlist);\n\t\tgoto fail;\n\t}\n\n\tif ((arg.job_nhosts > 0) && (tasks_to_launch > 0)) {\n\t\tuint32_t hi, i, i_first_bit=0, i_last_bit=0, j;\n\t\tbool cpu_log = slurm_get_debug_flags() & DEBUG_FLAG_CPU_BIND;\n\n#ifdef HAVE_FRONT_END\n\t\thost_index = 0;\t/* It is always 0 for front end systems */\n#else\n\t\thostset_t j_hset;\n\t\t/* Determine the CPU count based upon this node's index into\n\t\t * the _job's_ allocation (job's hostlist and core_bitmap) */\n\t\tif (!(j_hset = hostset_create(arg.job_hostlist))) {\n\t\t\terror(\"Unable to parse credential hostlist: `%s'\",\n\t\t\t      arg.job_hostlist);\n\t\t\tgoto fail;\n\t\t}\n\t\thost_index = hostset_find(j_hset, conf->node_name);\n\t\thostset_destroy(j_hset);\n\n\t\tif ((host_index < 0) || (host_index >= arg.job_nhosts)) {\n\t\t\terror(\"job cr credential invalid host_index %d for \"\n\t\t\t      \"job %u\", host_index, arg.jobid);\n\t\t\tgoto fail;\n\t\t}\n#endif\n\n\t\tif (cpu_log) {\n\t\t\tchar *per_job = \"\", *per_step = \"\";\n\t\t\tuint32_t job_mem  = arg.job_mem_limit;\n\t\t\tuint32_t step_mem = arg.step_mem_limit;\n\t\t\tif (job_mem & MEM_PER_CPU) {\n\t\t\t\tjob_mem &= (~MEM_PER_CPU);\n\t\t\t\tper_job = \"_per_CPU\";\n\t\t\t}\n\t\t\tif (step_mem & MEM_PER_CPU) {\n\t\t\t\tstep_mem &= (~MEM_PER_CPU);\n\t\t\t\tper_step = \"_per_CPU\";\n\t\t\t}\n\t\t\tinfo(\"====================\");\n\t\t\tinfo(\"step_id:%u.%u job_mem:%uMB%s step_mem:%uMB%s\",\n\t\t\t     arg.jobid, arg.stepid, job_mem, per_job,\n\t\t\t     step_mem, per_step);\n\t\t}\n\n\t\thi = host_index + 1;\t/* change from 0-origin to 1-origin */\n\t\tfor (i=0; hi; i++) {\n\t\t\tif (hi > arg.sock_core_rep_count[i]) {\n\t\t\t\ti_first_bit += arg.sockets_per_node[i] *\n\t\t\t\t\t       arg.cores_per_socket[i] *\n\t\t\t\t\t       arg.sock_core_rep_count[i];\n\t\t\t\thi -= arg.sock_core_rep_count[i];\n\t\t\t} else {\n\t\t\t\ti_first_bit += arg.sockets_per_node[i] *\n\t\t\t\t\t       arg.cores_per_socket[i] *\n\t\t\t\t\t       (hi - 1);\n\t\t\t\ti_last_bit = i_first_bit +\n\t\t\t\t\t     arg.sockets_per_node[i] *\n\t\t\t\t\t     arg.cores_per_socket[i];\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t/* Now count the allocated processors */\n\t\tfor (i=i_first_bit, j=0; i<i_last_bit; i++, j++) {\n\t\t\tchar *who_has = NULL;\n\t\t\tif (bit_test(arg.job_core_bitmap, i)) {\n\t\t\t\tjob_cpus++;\n\t\t\t\twho_has = \"Job\";\n\t\t\t}\n\t\t\tif (bit_test(arg.step_core_bitmap, i)) {\n\t\t\t\tstep_cpus++;\n\t\t\t\twho_has = \"Step\";\n\t\t\t}\n\t\t\tif (cpu_log && who_has) {\n\t\t\t\tinfo(\"JobNode[%u] CPU[%u] %s alloc\",\n\t\t\t\t     host_index, j, who_has);\n\t\t\t}\n\t\t}\n\t\tif (cpu_log)\n\t\t\tinfo(\"====================\");\n\t\tif (step_cpus == 0) {\n\t\t\terror(\"cons_res: zero processors allocated to step\");\n\t\t\tstep_cpus = 1;\n\t\t}\n\t\t/* NOTE: step_cpus is the count of allocated resources\n\t\t * (typically cores). Convert to CPU count as needed */\n\t\tif (i_last_bit <= i_first_bit)\n\t\t\terror(\"step credential has no CPUs selected\");\n\t\telse {\n\t\t\ti = conf->cpus / (i_last_bit - i_first_bit);\n\t\t\tif (i > 1) {\n\t\t\t\tif (cpu_log)\n\t\t\t\t\tinfo(\"Scaling CPU count by factor of \"\n\t\t\t\t\t     \"%d (%u/(%u-%u))\",\n\t\t\t\t\t     i, conf->cpus,\n\t\t\t\t\t     i_last_bit, i_first_bit);\n\t\t\t\tstep_cpus *= i;\n\t\t\t\tjob_cpus *= i;\n\t\t\t}\n\t\t}\n\t\tif (tasks_to_launch > step_cpus) {\n\t\t\t/* This is expected with the --overcommit option\n\t\t\t * or hyperthreads */\n\t\t\tdebug(\"cons_res: More than one tasks per logical \"\n\t\t\t      \"processor (%d > %u) on host [%u.%u %ld %s] \",\n\t\t\t      tasks_to_launch, step_cpus, arg.jobid,\n\t\t\t      arg.stepid, (long) arg.uid, arg.step_hostlist);\n\t\t}\n\t} else {\n\t\tstep_cpus = 1;\n\t\tjob_cpus  = 1;\n\t}\n\n\t/* Overwrite any memory limits in the RPC with contents of the\n\t * memory limit within the credential.\n\t * Reset the CPU count on this node to correct value. */\n\tif (arg.step_mem_limit) {\n\t\tif (arg.step_mem_limit & MEM_PER_CPU) {\n\t\t\treq->step_mem_lim  = arg.step_mem_limit &\n\t\t\t\t\t     (~MEM_PER_CPU);\n\t\t\treq->step_mem_lim *= step_cpus;\n\t\t} else\n\t\t\treq->step_mem_lim  = arg.step_mem_limit;\n\t} else {\n\t\tif (arg.job_mem_limit & MEM_PER_CPU) {\n\t\t\treq->step_mem_lim  = arg.job_mem_limit &\n\t\t\t\t\t     (~MEM_PER_CPU);\n\t\t\treq->step_mem_lim *= job_cpus;\n\t\t} else\n\t\t\treq->step_mem_lim  = arg.job_mem_limit;\n\t}\n\tif (arg.job_mem_limit & MEM_PER_CPU) {\n\t\treq->job_mem_lim  = arg.job_mem_limit & (~MEM_PER_CPU);\n\t\treq->job_mem_lim *= job_cpus;\n\t} else\n\t\treq->job_mem_lim  = arg.job_mem_limit;\n\treq->job_core_spec = arg.job_core_spec;\n\treq->node_cpus = step_cpus;\n#if 0\n\tinfo(\"%u.%u node_id:%d mem orig:%u cpus:%u limit:%u\",\n\t     jobid, stepid, node_id, arg.job_mem_limit,\n\t     step_cpus, req->job_mem_lim);\n#endif\n\n\t*step_hset = s_hset;\n\tslurm_cred_free_args(&arg);\n\treturn SLURM_SUCCESS;\n\n    fail:\n\tif (s_hset)\n\t\thostset_destroy(s_hset);\n\t*step_hset = NULL;\n\tslurm_cred_free_args(&arg);\n\tslurm_seterrno_ret(ESLURMD_INVALID_JOB_CREDENTIAL);\n}\n\n\nstatic void\n_rpc_launch_tasks(slurm_msg_t *msg)\n{\n\tint      errnum = SLURM_SUCCESS;\n\tuint16_t port;\n\tchar     host[MAXHOSTNAMELEN];\n\tuid_t    req_uid;\n\tlaunch_tasks_request_msg_t *req = msg->data;\n\tbool     super_user = false;\n#ifndef HAVE_FRONT_END\n\tbool     first_job_run;\n#endif\n\tslurm_addr_t self;\n\tslurm_addr_t *cli = &msg->orig_addr;\n\thostset_t step_hset = NULL;\n\tjob_mem_limits_t *job_limits_ptr;\n\tint nodeid = 0;\n#ifndef HAVE_FRONT_END\n\t/* It is always 0 for front end systems */\n\tnodeid = nodelist_find(req->complete_nodelist, conf->node_name);\n#endif\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tmemcpy(&req->orig_addr, &msg->orig_addr, sizeof(slurm_addr_t));\n\n\tsuper_user = _slurm_authorized_user(req_uid);\n\n\tif ((super_user == false) && (req_uid != req->uid)) {\n\t\terror(\"launch task request from uid %u\",\n\t\t      (unsigned int) req_uid);\n\t\terrnum = ESLURM_USER_ID_MISSING;\t/* or invalid user */\n\t\tgoto done;\n\t}\n\n\tslurm_get_ip_str(cli, &port, host, sizeof(host));\n\tinfo(\"launch task %u.%u request from %u.%u@%s (port %hu)\", req->job_id,\n\t     req->job_step_id, req->uid, req->gid, host, port);\n\n\t/* this could be set previously and needs to be overwritten by\n\t * this call for messages to work correctly for the new call */\n\tenv_array_overwrite(&req->env, \"SLURM_SRUN_COMM_HOST\", host);\n\treq->envc = envcount(req->env);\n\n#ifndef HAVE_FRONT_END\n\tslurm_mutex_lock(&prolog_mutex);\n\tfirst_job_run = !slurm_cred_jobid_cached(conf->vctx, req->job_id);\n#endif\n\tif (_check_job_credential(req, req_uid, nodeid, &step_hset,\n\t\t\t\t  msg->protocol_version) < 0) {\n\t\terrnum = errno;\n\t\terror(\"Invalid job credential from %ld@%s: %m\",\n\t\t      (long) req_uid, host);\n#ifndef HAVE_FRONT_END\n\t\tslurm_mutex_unlock(&prolog_mutex);\n#endif\n\t\tgoto done;\n\t}\n\n\t/* Must follow _check_job_credential(), which sets some req fields */\n\ttask_g_slurmd_launch_request(req->job_id, req, nodeid);\n\n#ifndef HAVE_FRONT_END\n\tif (first_job_run) {\n\t\tint rc;\n\t\tjob_env_t job_env;\n\n\t\tslurm_cred_insert_jobid(conf->vctx, req->job_id);\n\t\t_add_job_running_prolog(req->job_id);\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\n\t\tif (container_g_create(req->job_id))\n\t\t\terror(\"container_g_create(%u): %m\", req->job_id);\n\n\t\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\t\tjob_env.jobid = req->job_id;\n\t\tjob_env.step_id = req->job_step_id;\n\t\tjob_env.node_list = req->complete_nodelist;\n\t\tjob_env.partition = req->partition;\n\t\tjob_env.spank_job_env = req->spank_job_env;\n\t\tjob_env.spank_job_env_size = req->spank_job_env_size;\n\t\tjob_env.uid = req->uid;\n\t\tjob_env.user_name = req->user_name;\n\t\trc =  _run_prolog(&job_env, req->cred);\n\t\tif (rc) {\n\t\t\tint term_sig, exit_status;\n\t\t\tif (WIFSIGNALED(rc)) {\n\t\t\t\texit_status = 0;\n\t\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t\t} else {\n\t\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\t\tterm_sig    = 0;\n\t\t\t}\n\t\t\terror(\"[job %u] prolog failed status=%d:%d\",\n\t\t\t      req->job_id, exit_status, term_sig);\n\t\t\terrnum = ESLURMD_PROLOG_FAILED;\n\t\t\tgoto done;\n\t\t}\n\t\t/* Since the job could have been killed while the prolog was\n\t\t * running, test if the credential has since been revoked\n\t\t * and exit as needed. */\n\t\tif (slurm_cred_revoked(conf->vctx, req->cred)) {\n\t\t\tinfo(\"Job %u already killed, do not launch step %u.%u\",\n\t\t\t     req->job_id, req->job_id, req->job_step_id);\n\t\t\terrnum = ESLURMD_CREDENTIAL_REVOKED;\n\t\t\tgoto done;\n\t\t}\n\t} else {\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\t\t_wait_for_job_running_prolog(req->job_id);\n\t}\n#endif\n\n\tif (req->job_mem_lim || req->step_mem_lim) {\n\t\tstep_loc_t step_info;\n\t\tslurm_mutex_lock(&job_limits_mutex);\n\t\tif (!job_limits_list)\n\t\t\tjob_limits_list = list_create(_job_limits_free);\n\t\tstep_info.jobid  = req->job_id;\n\t\tstep_info.stepid = req->job_step_id;\n\t\tjob_limits_ptr = list_find_first (job_limits_list,\n\t\t\t\t\t\t  _step_limits_match,\n\t\t\t\t\t\t  &step_info);\n\t\tif (!job_limits_ptr) {\n\t\t\tjob_limits_ptr = xmalloc(sizeof(job_mem_limits_t));\n\t\t\tjob_limits_ptr->job_id   = req->job_id;\n\t\t\tjob_limits_ptr->job_mem  = req->job_mem_lim;\n\t\t\tjob_limits_ptr->step_id  = req->job_step_id;\n\t\t\tjob_limits_ptr->step_mem = req->step_mem_lim;\n#if _LIMIT_INFO\n\t\t\tinfo(\"AddLim step:%u.%u job_mem:%u step_mem:%u\",\n\t\t\t      job_limits_ptr->job_id, job_limits_ptr->step_id,\n\t\t\t      job_limits_ptr->job_mem,\n\t\t\t      job_limits_ptr->step_mem);\n#endif\n\t\t\tlist_append(job_limits_list, job_limits_ptr);\n\t\t}\n\t\tslurm_mutex_unlock(&job_limits_mutex);\n\t}\n\n\tslurm_get_stream_addr(msg->conn_fd, &self);\n\n\tdebug3(\"_rpc_launch_tasks: call to _forkexec_slurmstepd\");\n\terrnum = _forkexec_slurmstepd(LAUNCH_TASKS, (void *)req, cli, &self,\n\t\t\t\t      step_hset, msg->protocol_version);\n\tdebug3(\"_rpc_launch_tasks: return from _forkexec_slurmstepd\");\n\t_launch_complete_add(req->job_id);\n\n    done:\n\tif (step_hset)\n\t\thostset_destroy(step_hset);\n\n\tif (slurm_send_rc_msg(msg, errnum) < 0) {\n\t\tchar addr_str[32];\n\t\tslurm_print_slurm_addr(&msg->address, addr_str,\n\t\t\t\t       sizeof(addr_str));\n\t\terror(\"_rpc_launch_tasks: unable to send return code to \"\n\t\t      \"address:port=%s msg_type=%u: %m\",\n\t\t      addr_str, msg->msg_type);\n\n\t\t/*\n\t\t * Rewind credential so that srun may perform retry\n\t\t */\n\t\tslurm_cred_rewind(conf->vctx, req->cred); /* ignore errors */\n\n\t} else if (errnum == SLURM_SUCCESS) {\n\t\tsave_cred_state(conf->vctx);\n\t\ttask_g_slurmd_reserve_resources(req->job_id, req, nodeid);\n\t}\n\n\t/*\n\t *  If job prolog failed, indicate failure to slurmctld\n\t */\n\tif (errnum == ESLURMD_PROLOG_FAILED)\n\t\tsend_registration_msg(errnum, false);\n}\n\nstatic void\n_prolog_error(batch_job_launch_msg_t *req, int rc)\n{\n\tchar *err_name_ptr, err_name[256], path_name[MAXPATHLEN];\n\tchar *fmt_char;\n\tint fd;\n\n\tif (req->std_err || req->std_out) {\n\t\tif (req->std_err)\n\t\t\tstrncpy(err_name, req->std_err, sizeof(err_name));\n\t\telse\n\t\t\tstrncpy(err_name, req->std_out, sizeof(err_name));\n\t\tif ((fmt_char = strchr(err_name, (int) '%')) &&\n\t\t    (fmt_char[1] == 'j') && !strchr(fmt_char+1, (int) '%')) {\n\t\t\tchar tmp_name[256];\n\t\t\tfmt_char[1] = 'u';\n\t\t\tsnprintf(tmp_name, sizeof(tmp_name), err_name,\n\t\t\t\t req->job_id);\n\t\t\tstrncpy(err_name, tmp_name, sizeof(err_name));\n\t\t}\n\t} else {\n\t\tsnprintf(err_name, sizeof(err_name), \"slurm-%u.out\",\n\t\t\t req->job_id);\n\t}\n\terr_name_ptr = err_name;\n\tif (err_name_ptr[0] == '/')\n\t\tsnprintf(path_name, MAXPATHLEN, \"%s\", err_name_ptr);\n\telse if (req->work_dir)\n\t\tsnprintf(path_name, MAXPATHLEN, \"%s/%s\",\n\t\t\treq->work_dir, err_name_ptr);\n\telse\n\t\tsnprintf(path_name, MAXPATHLEN, \"/%s\", err_name_ptr);\n\n\tif ((fd = open(path_name, (O_CREAT|O_APPEND|O_WRONLY), 0644)) == -1) {\n\t\terror(\"Unable to open %s: %s\", path_name,\n\t\t      slurm_strerror(errno));\n\t\treturn;\n\t}\n\tsnprintf(err_name, sizeof(err_name),\n\t\t \"Error running slurm prolog: %d\\n\", WEXITSTATUS(rc));\n\tsafe_write(fd, err_name, strlen(err_name));\n\tif (fchown(fd, (uid_t) req->uid, (gid_t) req->gid) == -1) {\n\t\tsnprintf(err_name, sizeof(err_name),\n\t\t\t \"Couldn't change fd owner to %u:%u: %m\\n\",\n\t\t\t req->uid, req->gid);\n\t}\nrwfail:\n\tclose(fd);\n}\n\n/* load the user's environment on this machine if requested\n * SLURM_GET_USER_ENV environment variable is set */\nstatic int\n_get_user_env(batch_job_launch_msg_t *req)\n{\n\tstruct passwd pwd, *pwd_ptr = NULL;\n\tchar pwd_buf[PW_BUF_SIZE];\n\tchar **new_env;\n\tint i;\n\tstatic time_t config_update = 0;\n\tstatic bool no_env_cache = false;\n\n\tif (config_update != conf->last_update) {\n\t\tchar *sched_params = slurm_get_sched_params();\n\t\tno_env_cache = (sched_params &&\n\t\t\t\tstrstr(sched_params, \"no_env_cache\"));\n\t\txfree(sched_params);\n\t\tconfig_update = conf->last_update;\n\t}\n\n\tfor (i=0; i<req->envc; i++) {\n\t\tif (xstrcmp(req->environment[i], \"SLURM_GET_USER_ENV=1\") == 0)\n\t\t\tbreak;\n\t}\n\tif (i >= req->envc)\n\t\treturn 0;\t\t/* don't need to load env */\n\n\tif (slurm_getpwuid_r(req->uid, &pwd, pwd_buf, PW_BUF_SIZE, &pwd_ptr)\n\t    || (pwd_ptr == NULL)) {\n\t\terror(\"%s: getpwuid_r(%u):%m\", __func__, req->uid);\n\t\treturn -1;\n\t}\n\tverbose(\"%s: get env for user %s here\", __func__, pwd.pw_name);\n\n\t/* Permit up to 120 second delay before using cache file */\n\tnew_env = env_array_user_default(pwd.pw_name, 120, 0, no_env_cache);\n\tif (! new_env) {\n\t\terror(\"%s: Unable to get user's local environment%s\",\n\t\t      __func__, no_env_cache ?\n\t\t      \"\" : \", running only with passed environment\");\n\t\treturn -1;\n\t}\n\n\tenv_array_merge(&new_env,\n\t\t\t(const char **) req->environment);\n\tenv_array_free(req->environment);\n\treq->environment = new_env;\n\treq->envc = envcount(new_env);\n\n\treturn 0;\n}\n\n/* The RPC currently contains a memory size limit, but we load the\n * value from the job credential to be certain it has not been\n * altered by the user */\nstatic void\n_set_batch_job_limits(slurm_msg_t *msg)\n{\n\tint i;\n\tuint32_t alloc_lps = 0, last_bit = 0;\n\tbool cpu_log = slurm_get_debug_flags() & DEBUG_FLAG_CPU_BIND;\n\tslurm_cred_arg_t arg;\n\tbatch_job_launch_msg_t *req = (batch_job_launch_msg_t *)msg->data;\n\n\tif (slurm_cred_get_args(req->cred, &arg) != SLURM_SUCCESS)\n\t\treturn;\n\treq->job_core_spec = arg.job_core_spec;\t/* Prevent user reset */\n\n\tif (cpu_log) {\n\t\tchar *per_job = \"\";\n\t\tuint32_t job_mem  = arg.job_mem_limit;\n\t\tif (job_mem & MEM_PER_CPU) {\n\t\t\tjob_mem &= (~MEM_PER_CPU);\n\t\t\tper_job = \"_per_CPU\";\n\t\t}\n\t\tinfo(\"====================\");\n\t\tinfo(\"batch_job:%u job_mem:%uMB%s\", req->job_id,\n\t\t     job_mem, per_job);\n\t}\n\tif (cpu_log || (arg.job_mem_limit & MEM_PER_CPU)) {\n\t\tif (arg.job_nhosts > 0) {\n\t\t\tlast_bit = arg.sockets_per_node[0] *\n\t\t\t\t   arg.cores_per_socket[0];\n\t\t\tfor (i=0; i<last_bit; i++) {\n\t\t\t\tif (!bit_test(arg.job_core_bitmap, i))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (cpu_log)\n\t\t\t\t\tinfo(\"JobNode[0] CPU[%u] Job alloc\",i);\n\t\t\t\talloc_lps++;\n\t\t\t}\n\t\t}\n\t\tif (cpu_log)\n\t\t\tinfo(\"====================\");\n\t\tif (alloc_lps == 0) {\n\t\t\terror(\"_set_batch_job_limit: alloc_lps is zero\");\n\t\t\talloc_lps = 1;\n\t\t}\n\n\t\t/* NOTE: alloc_lps is the count of allocated resources\n\t\t * (typically cores). Convert to CPU count as needed */\n\t\tif (last_bit < 1)\n\t\t\terror(\"Batch job credential allocates no CPUs\");\n\t\telse {\n\t\t\ti = conf->cpus / last_bit;\n\t\t\tif (i > 1)\n\t\t\t\talloc_lps *= i;\n\t\t}\n\t}\n\n\tif (arg.job_mem_limit & MEM_PER_CPU) {\n\t\treq->job_mem = arg.job_mem_limit & (~MEM_PER_CPU);\n\t\treq->job_mem *= alloc_lps;\n\t} else\n\t\treq->job_mem = arg.job_mem_limit;\n\n\tslurm_cred_free_args(&arg);\n}\n\n/* These functions prevent a possible race condition if the batch script's\n * complete RPC is processed before it's launch_successful response. This\n *  */\nstatic bool _is_batch_job_finished(uint32_t job_id)\n{\n\tbool found_job = false;\n\tint i;\n\n\tslurm_mutex_lock(&fini_mutex);\n\tfor (i = 0; i < FINI_JOB_CNT; i++) {\n\t\tif (fini_job_id[i] == job_id) {\n\t\t\tfound_job = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tslurm_mutex_unlock(&fini_mutex);\n\n\treturn found_job;\n}\nstatic void _note_batch_job_finished(uint32_t job_id)\n{\n\tslurm_mutex_lock(&fini_mutex);\n\tfini_job_id[next_fini_job_inx] = job_id;\n\tif (++next_fini_job_inx >= FINI_JOB_CNT)\n\t\tnext_fini_job_inx = 0;\n\tslurm_mutex_unlock(&fini_mutex);\n}\n\n/* Send notification to slurmctld we are finished running the prolog.\n * This is needed on system that don't use srun to launch their tasks.\n */\nstatic void _notify_slurmctld_prolog_fini(\n\tuint32_t job_id, uint32_t prolog_return_code)\n{\n\tint rc;\n\tslurm_msg_t req_msg;\n\tcomplete_prolog_msg_t req;\n\n\tslurm_msg_t_init(&req_msg);\n\treq.job_id\t= job_id;\n\treq.prolog_rc\t= prolog_return_code;\n\n\treq_msg.msg_type= REQUEST_COMPLETE_PROLOG;\n\treq_msg.data\t= &req;\n\n\tif ((slurm_send_recv_controller_rc_msg(&req_msg, &rc) < 0) ||\n\t    (rc != SLURM_SUCCESS))\n\t\terror(\"Error sending prolog completion notification: %m\");\n}\n\n/* Convert memory limits from per-CPU to per-node */\nstatic void _convert_job_mem(slurm_msg_t *msg)\n{\n\tprolog_launch_msg_t *req = (prolog_launch_msg_t *)msg->data;\n\tslurm_cred_arg_t arg;\n\thostset_t j_hset = NULL;\n\tint rc, hi, host_index, job_cpus;\n\tint i, i_first_bit = 0, i_last_bit = 0;\n\n\trc = slurm_cred_verify(conf->vctx, req->cred, &arg,\n\t\t\t       msg->protocol_version);\n\tif (rc < 0) {\n\t\terror(\"%s: slurm_cred_verify failed: %m\", __func__);\n\t\treq->nnodes = 1;\t/* best guess */\n\t\treturn;\n\t}\n\n\treq->nnodes = arg.job_nhosts;\n\n\tif (arg.job_mem_limit == 0)\n\t\tgoto fini;\n\tif ((arg.job_mem_limit & MEM_PER_CPU) == 0) {\n\t\treq->job_mem_limit = arg.job_mem_limit;\n\t\tgoto fini;\n\t}\n\n\t/* Assume 1 CPU on error */\n\treq->job_mem_limit = arg.job_mem_limit & (~MEM_PER_CPU);\n\n\tif (!(j_hset = hostset_create(arg.job_hostlist))) {\n\t\terror(\"%s: Unable to parse credential hostlist: `%s'\",\n\t\t      __func__, arg.step_hostlist);\n\t\tgoto fini;\n\t}\n\thost_index = hostset_find(j_hset, conf->node_name);\n\thostset_destroy(j_hset);\n\n\thi = host_index + 1;\t/* change from 0-origin to 1-origin */\n\tfor (i = 0; hi; i++) {\n\t\tif (hi > arg.sock_core_rep_count[i]) {\n\t\t\ti_first_bit += arg.sockets_per_node[i] *\n\t\t\t\t       arg.cores_per_socket[i] *\n\t\t\t\t       arg.sock_core_rep_count[i];\n\t\t\ti_last_bit = i_first_bit +\n\t\t\t\t     arg.sockets_per_node[i] *\n\t\t\t\t     arg.cores_per_socket[i] *\n\t\t\t\t     arg.sock_core_rep_count[i];\n\t\t\thi -= arg.sock_core_rep_count[i];\n\t\t} else {\n\t\t\ti_first_bit += arg.sockets_per_node[i] *\n\t\t\t\t       arg.cores_per_socket[i] * (hi - 1);\n\t\t\ti_last_bit = i_first_bit +\n\t\t\t\t     arg.sockets_per_node[i] *\n\t\t\t\t     arg.cores_per_socket[i];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Now count the allocated processors on this node */\n\tjob_cpus = 0;\n\tfor (i = i_first_bit; i < i_last_bit; i++) {\n\t\tif (bit_test(arg.job_core_bitmap, i))\n\t\t\tjob_cpus++;\n\t}\n\n\t/* NOTE: alloc_lps is the count of allocated resources\n\t * (typically cores). Convert to CPU count as needed */\n\tif (i_last_bit > i_first_bit) {\n\t\ti = conf->cpus / (i_last_bit - i_first_bit);\n\t\tif (i > 1)\n\t\t\tjob_cpus *= i;\n\t}\n\n\treq->job_mem_limit *= job_cpus;\n\nfini:\tslurm_cred_free_args(&arg);\n}\n\nstatic void _make_prolog_mem_container(slurm_msg_t *msg)\n{\n\tprolog_launch_msg_t *req = (prolog_launch_msg_t *)msg->data;\n\tjob_mem_limits_t *job_limits_ptr;\n\tstep_loc_t step_info;\n\n\t_convert_job_mem(msg);\t/* Convert per-CPU mem limit */\n\tif (req->job_mem_limit) {\n\t\tslurm_mutex_lock(&job_limits_mutex);\n\t\tif (!job_limits_list)\n\t\t\tjob_limits_list = list_create(_job_limits_free);\n\t\tstep_info.jobid  = req->job_id;\n\t\tstep_info.stepid = SLURM_EXTERN_CONT;\n\t\tjob_limits_ptr = list_find_first (job_limits_list,\n\t\t\t\t\t\t  _step_limits_match,\n\t\t\t\t\t\t  &step_info);\n\t\tif (!job_limits_ptr) {\n\t\t\tjob_limits_ptr = xmalloc(sizeof(job_mem_limits_t));\n\t\t\tjob_limits_ptr->job_id   = req->job_id;\n\t\t\tjob_limits_ptr->job_mem  = req->job_mem_limit;\n\t\t\tjob_limits_ptr->step_id  = SLURM_EXTERN_CONT;\n\t\t\tjob_limits_ptr->step_mem = req->job_mem_limit;\n#if _LIMIT_INFO\n\t\t\tinfo(\"AddLim step:%u.%u job_mem:%u step_mem:%u\",\n\t\t\t      job_limits_ptr->job_id, job_limits_ptr->step_id,\n\t\t\t      job_limits_ptr->job_mem,\n\t\t\t      job_limits_ptr->step_mem);\n#endif\n\t\t\tlist_append(job_limits_list, job_limits_ptr);\n\t\t}\n\t\tslurm_mutex_unlock(&job_limits_mutex);\n\t}\n}\n\nstatic void _spawn_prolog_stepd(slurm_msg_t *msg)\n{\n\tprolog_launch_msg_t *req = (prolog_launch_msg_t *)msg->data;\n\tlaunch_tasks_request_msg_t *launch_req;\n\tslurm_addr_t self;\n\tslurm_addr_t *cli = &msg->orig_addr;\n\tint i;\n\n\tlaunch_req = xmalloc(sizeof(launch_tasks_request_msg_t));\n\tlaunch_req->alias_list\t\t= req->alias_list;\n\tlaunch_req->complete_nodelist\t= req->nodes;\n\tlaunch_req->cpus_per_task\t= 1;\n\tlaunch_req->cred\t\t= req->cred;\n\tlaunch_req->cwd\t\t\t= req->work_dir;\n\tlaunch_req->efname\t\t= \"/dev/null\";\n\tlaunch_req->gid\t\t\t= req->gid;\n\tlaunch_req->global_task_ids\t= xmalloc(sizeof(uint32_t *)\n\t\t\t\t\t\t  * req->nnodes);\n\tlaunch_req->ifname\t\t= \"/dev/null\";\n\tlaunch_req->job_id\t\t= req->job_id;\n\tlaunch_req->job_mem_lim\t\t= req->job_mem_limit;\n\tlaunch_req->job_step_id\t\t= SLURM_EXTERN_CONT;\n\tlaunch_req->nnodes\t\t= req->nnodes;\n\tlaunch_req->ntasks\t\t= req->nnodes;\n\tlaunch_req->ofname\t\t= \"/dev/null\";\n\tlaunch_req->partition\t\t= req->partition;\n\tlaunch_req->spank_job_env_size\t= req->spank_job_env_size;\n\tlaunch_req->spank_job_env\t= req->spank_job_env;\n\tlaunch_req->step_mem_lim\t= req->job_mem_limit;\n\tlaunch_req->tasks_to_launch\t= xmalloc(sizeof(uint16_t)\n\t\t\t\t\t\t  * req->nnodes);\n\tlaunch_req->uid\t\t\t= req->uid;\n\n\tfor (i = 0; i < req->nnodes; i++) {\n\t\tuint32_t *tmp32 = xmalloc(sizeof(uint32_t));\n\t\t*tmp32 = i;\n\t\tlaunch_req->global_task_ids[i] = tmp32;\n\t\tlaunch_req->tasks_to_launch[i] = 1;\n\t}\n\n\tslurm_get_stream_addr(msg->conn_fd, &self);\n\t/* Since job could have been killed while the prolog was\n\t * running (especially on BlueGene, which can take minutes\n\t * for partition booting). Test if the credential has since\n\t * been revoked and exit as needed. */\n\tif (slurm_cred_revoked(conf->vctx, req->cred)) {\n\t\tinfo(\"Job %u already killed, do not launch extern step\",\n\t\t     req->job_id);\n\t} else {\n\t\thostset_t step_hset = hostset_create(req->nodes);\n\n\t\tdebug3(\"%s: call to _forkexec_slurmstepd\", __func__);\n\t\t(void) _forkexec_slurmstepd(\n\t\t\tLAUNCH_TASKS, (void *)launch_req, cli,\n\t\t\t&self, step_hset, msg->protocol_version);\n\t\tdebug3(\"%s: return from _forkexec_slurmstepd\", __func__);\n\t\tif (step_hset)\n\t\t\thostset_destroy(step_hset);\n\t}\n\n\tfor (i = 0; i < req->nnodes; i++)\n\t\txfree(launch_req->global_task_ids[i]);\n\txfree(launch_req->global_task_ids);\n\txfree(launch_req->tasks_to_launch);\n\txfree(launch_req);\n}\n\nstatic void _rpc_prolog(slurm_msg_t *msg)\n{\n\tint rc = SLURM_SUCCESS;\n\tprolog_launch_msg_t *req = (prolog_launch_msg_t *)msg->data;\n\tjob_env_t job_env;\n\tbool     first_job_run;\n\tuid_t    req_uid;\n\n\tif (req == NULL)\n\t\treturn;\n\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"REQUEST_LAUNCH_PROLOG request from uid %u\",\n\t\t      (unsigned int) req_uid);\n\t\treturn;\n\t}\n\n\tif (slurm_send_rc_msg(msg, rc) < 0) {\n\t\terror(\"Error starting prolog: %m\");\n\t}\n\tif (rc) {\n\t\tint term_sig, exit_status;\n\t\tif (WIFSIGNALED(rc)) {\n\t\t\texit_status = 0;\n\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t} else {\n\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\tterm_sig    = 0;\n\t\t}\n\t\terror(\"[job %u] prolog start failed status=%d:%d\",\n\t\t      req->job_id, exit_status, term_sig);\n\t\trc = ESLURMD_PROLOG_FAILED;\n\t}\n\n\tslurm_mutex_lock(&prolog_mutex);\n\tfirst_job_run = !slurm_cred_jobid_cached(conf->vctx, req->job_id);\n\tif (first_job_run) {\n\t\tif (slurmctld_conf.prolog_flags & PROLOG_FLAG_CONTAIN)\n\t\t\t_make_prolog_mem_container(msg);\n\n\t\tif (container_g_create(req->job_id))\n\t\t\terror(\"container_g_create(%u): %m\", req->job_id);\n\n\t\tslurm_cred_insert_jobid(conf->vctx, req->job_id);\n\t\t_add_job_running_prolog(req->job_id);\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\n\t\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\t\tjob_env.jobid = req->job_id;\n\t\tjob_env.step_id = 0;\t/* not available */\n\t\tjob_env.node_list = req->nodes;\n\t\tjob_env.partition = req->partition;\n\t\tjob_env.spank_job_env = req->spank_job_env;\n\t\tjob_env.spank_job_env_size = req->spank_job_env_size;\n\t\tjob_env.uid = req->uid;\n\t\tjob_env.user_name = req->user_name;\n#if defined(HAVE_BG)\n\t\tselect_g_select_jobinfo_get(req->select_jobinfo,\n\t\t\t\t\t    SELECT_JOBDATA_BLOCK_ID,\n\t\t\t\t\t    &job_env.resv_id);\n#elif defined(HAVE_ALPS_CRAY)\n\t\tjob_env.resv_id = select_g_select_jobinfo_xstrdup(\n\t\t\treq->select_jobinfo, SELECT_PRINT_RESV_ID);\n#endif\n\t\trc = _run_prolog(&job_env, req->cred);\n\n\t\tif (rc) {\n\t\t\tint term_sig, exit_status;\n\t\t\tif (WIFSIGNALED(rc)) {\n\t\t\t\texit_status = 0;\n\t\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t\t} else {\n\t\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\t\tterm_sig    = 0;\n\t\t\t}\n\t\t\terror(\"[job %u] prolog failed status=%d:%d\",\n\t\t\t      req->job_id, exit_status, term_sig);\n\t\t\trc = ESLURMD_PROLOG_FAILED;\n\t\t}\n\t} else\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\n\tif (!(slurmctld_conf.prolog_flags & PROLOG_FLAG_NOHOLD))\n\t\t_notify_slurmctld_prolog_fini(req->job_id, rc);\n\n\tif (rc == SLURM_SUCCESS) {\n\t\tif (slurmctld_conf.prolog_flags & PROLOG_FLAG_CONTAIN)\n\t\t\t_spawn_prolog_stepd(msg);\n\t} else {\n\t\t_launch_job_fail(req->job_id, rc);\n\t\t/*\n\t\t *  If job prolog failed or we could not reply,\n\t\t *  initiate message to slurmctld with current state\n\t\t */\n\t\tif ((rc == ESLURMD_PROLOG_FAILED) ||\n\t\t    (rc == SLURM_COMMUNICATIONS_SEND_ERROR) ||\n\t\t    (rc == ESLURMD_SETUP_ENVIRONMENT_ERROR))\n\t\t\tsend_registration_msg(rc, false);\n\t}\n}\n\nstatic void\n_rpc_batch_job(slurm_msg_t *msg, bool new_msg)\n{\n\tbatch_job_launch_msg_t *req = (batch_job_launch_msg_t *)msg->data;\n\tbool     first_job_run;\n\tint      rc = SLURM_SUCCESS;\n\tbool\t replied = false, revoked;\n\tslurm_addr_t *cli = &msg->orig_addr;\n\n\tif (new_msg) {\n\t\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t     conf->auth_info);\n\t\tif (!_slurm_authorized_user(req_uid)) {\n\t\t\terror(\"Security violation, batch launch RPC from uid %d\",\n\t\t\t      req_uid);\n\t\t\trc = ESLURM_USER_ID_MISSING;  /* or bad in this case */\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (_launch_job_test(req->job_id)) {\n\t\terror(\"Job %u already running, do not launch second copy\",\n\t\t      req->job_id);\n\t\trc = ESLURM_DUPLICATE_JOB_ID;\t/* job already running */\n\t\t_launch_job_fail(req->job_id, rc);\n\t\tgoto done;\n\t}\n\n\tslurm_cred_handle_reissue(conf->vctx, req->cred);\n\tif (slurm_cred_revoked(conf->vctx, req->cred)) {\n\t\terror(\"Job %u already killed, do not launch batch job\",\n\t\t      req->job_id);\n\t\trc = ESLURMD_CREDENTIAL_REVOKED;\t/* job already ran */\n\t\tgoto done;\n\t}\n\n\ttask_g_slurmd_batch_request(req->job_id, req);\t/* determine task affinity */\n\n\tslurm_mutex_lock(&prolog_mutex);\n\tfirst_job_run = !slurm_cred_jobid_cached(conf->vctx, req->job_id);\n\n\t/* BlueGene prolog waits for partition boot and is very slow.\n\t * On any system we might need to load environment variables\n\t * for Moab (see --get-user-env), which could also be slow.\n\t * Just reply now and send a separate kill job request if the\n\t * prolog or launch fail. */\n\treplied = true;\n\tif (new_msg && (slurm_send_rc_msg(msg, rc) < 1)) {\n\t\t/* The slurmctld is no longer waiting for a reply.\n\t\t * This typically indicates that the slurmd was\n\t\t * blocked from memory and/or CPUs and the slurmctld\n\t\t * has requeued the batch job request. */\n\t\terror(\"Could not confirm batch launch for job %u, \"\n\t\t      \"aborting request\", req->job_id);\n\t\trc = SLURM_COMMUNICATIONS_SEND_ERROR;\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\t\tgoto done;\n\t}\n\n\t/*\n\t * Insert jobid into credential context to denote that\n\t * we've now \"seen\" an instance of the job\n\t */\n\tif (first_job_run) {\n\t\tjob_env_t job_env;\n\t\tslurm_cred_insert_jobid(conf->vctx, req->job_id);\n\t\t_add_job_running_prolog(req->job_id);\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\n\t\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\t\tjob_env.jobid = req->job_id;\n\t\tjob_env.step_id = req->step_id;\n\t\tjob_env.node_list = req->nodes;\n\t\tjob_env.partition = req->partition;\n\t\tjob_env.spank_job_env = req->spank_job_env;\n\t\tjob_env.spank_job_env_size = req->spank_job_env_size;\n\t\tjob_env.uid = req->uid;\n\t\tjob_env.user_name = req->user_name;\n\t\t/*\n\t \t * Run job prolog on this node\n\t \t */\n#if defined(HAVE_BG)\n\t\tselect_g_select_jobinfo_get(req->select_jobinfo,\n\t\t\t\t\t    SELECT_JOBDATA_BLOCK_ID,\n\t\t\t\t\t    &job_env.resv_id);\n#elif defined(HAVE_ALPS_CRAY)\n\t\tjob_env.resv_id = select_g_select_jobinfo_xstrdup(\n\t\t\treq->select_jobinfo, SELECT_PRINT_RESV_ID);\n#endif\n\t\tif (container_g_create(req->job_id))\n\t\t\terror(\"container_g_create(%u): %m\", req->job_id);\n\t\trc = _run_prolog(&job_env, req->cred);\n\t\txfree(job_env.resv_id);\n\t\tif (rc) {\n\t\t\tint term_sig, exit_status;\n\t\t\tif (WIFSIGNALED(rc)) {\n\t\t\t\texit_status = 0;\n\t\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t\t} else {\n\t\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\t\tterm_sig    = 0;\n\t\t\t}\n\t\t\terror(\"[job %u] prolog failed status=%d:%d\",\n\t\t\t      req->job_id, exit_status, term_sig);\n\t\t\t_prolog_error(req, rc);\n\t\t\trc = ESLURMD_PROLOG_FAILED;\n\t\t\tgoto done;\n\t\t}\n\t} else {\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\t\t_wait_for_job_running_prolog(req->job_id);\n\t}\n\n\tif (_get_user_env(req) < 0) {\n\t\tbool requeue = _requeue_setup_env_fail();\n\t\tif (requeue) {\n\t\t\trc = ESLURMD_SETUP_ENVIRONMENT_ERROR;\n\t\t\tgoto done;\n\t\t}\n\t}\n\t_set_batch_job_limits(msg);\n\n\t/* Since job could have been killed while the prolog was\n\t * running (especially on BlueGene, which can take minutes\n\t * for partition booting). Test if the credential has since\n\t * been revoked and exit as needed. */\n\tif (slurm_cred_revoked(conf->vctx, req->cred)) {\n\t\tinfo(\"Job %u already killed, do not launch batch job\",\n\t\t     req->job_id);\n\t\trc = ESLURMD_CREDENTIAL_REVOKED;     /* job already ran */\n\t\tgoto done;\n\t}\n\n\tslurm_mutex_lock(&launch_mutex);\n\tif (req->step_id == SLURM_BATCH_SCRIPT)\n\t\tinfo(\"Launching batch job %u for UID %d\",\n\t\t     req->job_id, req->uid);\n\telse\n\t\tinfo(\"Launching batch job %u.%u for UID %d\",\n\t\t     req->job_id, req->step_id, req->uid);\n\n\tdebug3(\"_rpc_batch_job: call to _forkexec_slurmstepd\");\n\trc = _forkexec_slurmstepd(LAUNCH_BATCH_JOB, (void *)req, cli, NULL,\n\t\t\t\t  (hostset_t)NULL, SLURM_PROTOCOL_VERSION);\n\tdebug3(\"_rpc_batch_job: return from _forkexec_slurmstepd: %d\", rc);\n\n\tslurm_mutex_unlock(&launch_mutex);\n\t_launch_complete_add(req->job_id);\n\n\t/* On a busy system, slurmstepd may take a while to respond,\n\t * if the job was cancelled in the interim, run through the\n\t * abort logic below. */\n\trevoked = slurm_cred_revoked(conf->vctx, req->cred);\n\tif (revoked)\n\t\t_launch_complete_rm(req->job_id);\n\tif (revoked && _is_batch_job_finished(req->job_id)) {\n\t\t/* If configured with select/serial and the batch job already\n\t\t * completed, consider the job sucessfully launched and do\n\t\t * not repeat termination logic below, which in the worst case\n\t\t * just slows things down with another message. */\n\t\trevoked = false;\n\t}\n\tif (revoked) {\n\t\tinfo(\"Job %u killed while launch was in progress\",\n\t\t     req->job_id);\n\t\tsleep(1);\t/* give slurmstepd time to create\n\t\t\t\t * the communication socket */\n\t\t_terminate_all_steps(req->job_id, true);\n\t\trc = ESLURMD_CREDENTIAL_REVOKED;\n\t\tgoto done;\n\t}\n\ndone:\n\tif (!replied) {\n\t\tif (new_msg && (slurm_send_rc_msg(msg, rc) < 1)) {\n\t\t\t/* The slurmctld is no longer waiting for a reply.\n\t\t\t * This typically indicates that the slurmd was\n\t\t\t * blocked from memory and/or CPUs and the slurmctld\n\t\t\t * has requeued the batch job request. */\n\t\t\terror(\"Could not confirm batch launch for job %u, \"\n\t\t\t      \"aborting request\", req->job_id);\n\t\t\trc = SLURM_COMMUNICATIONS_SEND_ERROR;\n\t\t} else {\n\t\t\t/* No need to initiate separate reply below */\n\t\t\trc = SLURM_SUCCESS;\n\t\t}\n\t}\n\tif (rc != SLURM_SUCCESS) {\n\t\t/* prolog or job launch failure,\n\t\t * tell slurmctld that the job failed */\n\t\tif (req->step_id == SLURM_BATCH_SCRIPT)\n\t\t\t_launch_job_fail(req->job_id, rc);\n\t\telse\n\t\t\t_abort_step(req->job_id, req->step_id);\n\t}\n\n\t/*\n\t *  If job prolog failed or we could not reply,\n\t *  initiate message to slurmctld with current state\n\t */\n\tif ((rc == ESLURMD_PROLOG_FAILED)\n\t    || (rc == SLURM_COMMUNICATIONS_SEND_ERROR)\n\t    || (rc == ESLURMD_SETUP_ENVIRONMENT_ERROR)) {\n\t\tsend_registration_msg(rc, false);\n\t}\n}\n\n/*\n * Send notification message to batch job\n */\nstatic void\n_rpc_job_notify(slurm_msg_t *msg)\n{\n\tjob_notify_msg_t *req = msg->data;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tuid_t job_uid;\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd = NULL;\n\tint step_cnt  = 0;\n\tint fd;\n\n\tdebug(\"_rpc_job_notify, uid = %d, jobid = %u\", req_uid, req->job_id);\n\tjob_uid = _get_job_uid(req->job_id);\n\tif ((int)job_uid < 0)\n\t\tgoto no_job;\n\n\t/*\n\t * check that requesting user ID is the SLURM UID or root\n\t */\n\tif ((req_uid != job_uid) && (!_slurm_authorized_user(req_uid))) {\n\t\terror(\"Security violation: job_notify(%u) from uid %d\",\n\t\t      req->job_id, req_uid);\n\t\treturn;\n\t}\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif ((stepd->jobid  != req->job_id) ||\n\t\t    (stepd->stepid != SLURM_BATCH_SCRIPT)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tstep_cnt++;\n\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1) {\n\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tinfo(\"send notification to job %u.%u\",\n\t\t     stepd->jobid, stepd->stepid);\n\t\tif (stepd_notify_job(fd, stepd->protocol_version,\n\t\t\t\t     req->message) < 0)\n\t\t\tdebug(\"notify jobid=%u failed: %m\", stepd->jobid);\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\nno_job:\n\tif (step_cnt == 0) {\n\t\tdebug2(\"Can't find jobid %u to send notification message\",\n\t\t       req->job_id);\n\t}\n}\n\nstatic int\n_launch_job_fail(uint32_t job_id, uint32_t slurm_rc)\n{\n\tcomplete_batch_script_msg_t comp_msg;\n\tstruct requeue_msg req_msg;\n\tslurm_msg_t resp_msg;\n\tint rc = 0, rpc_rc;\n\tstatic time_t config_update = 0;\n\tstatic bool requeue_no_hold = false;\n\n\tif (config_update != conf->last_update) {\n\t\tchar *sched_params = slurm_get_sched_params();\n\t\trequeue_no_hold = (sched_params && strstr(\n\t\t\t\t\t   sched_params,\n\t\t\t\t\t   \"nohold_on_prolog_fail\"));\n\t\txfree(sched_params);\n\t\tconfig_update = conf->last_update;\n\t}\n\n\tslurm_msg_t_init(&resp_msg);\n\n\tif (slurm_rc == ESLURMD_CREDENTIAL_REVOKED) {\n\t\tcomp_msg.job_id = job_id;\n\t\tcomp_msg.job_rc = INFINITE;\n\t\tcomp_msg.slurm_rc = slurm_rc;\n\t\tcomp_msg.node_name = conf->node_name;\n\t\tcomp_msg.jobacct = NULL; /* unused */\n\t\tresp_msg.msg_type = REQUEST_COMPLETE_BATCH_SCRIPT;\n\t\tresp_msg.data = &comp_msg;\n\t} else {\n\t\treq_msg.job_id = job_id;\n\t\treq_msg.job_id_str = NULL;\n\t\tif (requeue_no_hold) {\n\t\t\treq_msg.state = JOB_PENDING;\n\t\t} else {\n\t\t\treq_msg.state = (JOB_REQUEUE_HOLD|JOB_LAUNCH_FAILED);\n\t\t}\n\t\tresp_msg.msg_type = REQUEST_JOB_REQUEUE;\n\t\tresp_msg.data = &req_msg;\n\t}\n\n\trpc_rc = slurm_send_recv_controller_rc_msg(&resp_msg, &rc);\n\tif ((resp_msg.msg_type == REQUEST_JOB_REQUEUE) &&\n\t    ((rc == ESLURM_DISABLED) || (rc == ESLURM_BATCH_ONLY))) {\n\t\tinfo(\"Could not launch job %u and not able to requeue it, \"\n\t\t     \"cancelling job\", job_id);\n\n\t\tif ((slurm_rc == ESLURMD_PROLOG_FAILED) &&\n\t\t    (rc == ESLURM_BATCH_ONLY)) {\n\t\t\tchar *buf = NULL;\n\t\t\txstrfmtcat(buf, \"Prolog failure on node %s\",\n\t\t\t\t   conf->node_name);\n\t\t\tslurm_notify_job(job_id, buf);\n\t\t\txfree(buf);\n\t\t}\n\n\t\tcomp_msg.job_id = job_id;\n\t\tcomp_msg.job_rc = INFINITE;\n\t\tcomp_msg.slurm_rc = slurm_rc;\n\t\tcomp_msg.node_name = conf->node_name;\n\t\tcomp_msg.jobacct = NULL; /* unused */\n\t\tresp_msg.msg_type = REQUEST_COMPLETE_BATCH_SCRIPT;\n\t\tresp_msg.data = &comp_msg;\n\t\trpc_rc = slurm_send_recv_controller_rc_msg(&resp_msg, &rc);\n\t}\n\n\treturn rpc_rc;\n}\n\nstatic int\n_abort_step(uint32_t job_id, uint32_t step_id)\n{\n\tstep_complete_msg_t resp;\n\tslurm_msg_t resp_msg;\n\tslurm_msg_t_init(&resp_msg);\n\tint rc, rc2;\n\n\tresp.job_id       = job_id;\n\tresp.job_step_id  = step_id;\n\tresp.range_first  = 0;\n\tresp.range_last   = 0;\n\tresp.step_rc      = 1;\n\tresp.jobacct      = jobacctinfo_create(NULL);\n\tresp_msg.msg_type = REQUEST_STEP_COMPLETE;\n\tresp_msg.data     = &resp;\n\trc2 = slurm_send_recv_controller_rc_msg(&resp_msg, &rc);\n\t/* Note: we are ignoring the RPC return code */\n\tjobacctinfo_destroy(resp.jobacct);\n\treturn rc2;\n}\n\nstatic void\n_rpc_reconfig(slurm_msg_t *msg)\n{\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\n\tif (!_slurm_authorized_user(req_uid))\n\t\terror(\"Security violation, reconfig RPC from uid %d\",\n\t\t      req_uid);\n\telse\n\t\tkill(conf->pid, SIGHUP);\n\tforward_wait(msg);\n\t/* Never return a message, slurmctld does not expect one */\n}\n\nstatic void\n_rpc_shutdown(slurm_msg_t *msg)\n{\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\n\tforward_wait(msg);\n\tif (!_slurm_authorized_user(req_uid))\n\t\terror(\"Security violation, shutdown RPC from uid %d\",\n\t\t      req_uid);\n\telse {\n\t\tif (kill(conf->pid, SIGTERM) != 0)\n\t\t\terror(\"kill(%u,SIGTERM): %m\", conf->pid);\n\t}\n\n\t/* Never return a message, slurmctld does not expect one */\n}\n\nstatic void\n_rpc_reboot(slurm_msg_t *msg)\n{\n\tchar *reboot_program, *cmd = NULL, *sp;\n\treboot_msg_t *reboot_msg;\n\tslurm_ctl_conf_t *cfg;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tint exit_code;\n\n\tif (!_slurm_authorized_user(req_uid))\n\t\terror(\"Security violation, reboot RPC from uid %d\",\n\t\t      req_uid);\n\telse {\n\t\tcfg = slurm_conf_lock();\n\t\treboot_program = cfg->reboot_program;\n\t\tif (reboot_program) {\n\t\t\tsp = strchr(reboot_program, ' ');\n\t\t\tif (sp)\n\t\t\t\tsp = xstrndup(reboot_program,\n\t\t\t\t\t      (sp - reboot_program));\n\t\t\telse\n\t\t\t\tsp = xstrdup(reboot_program);\n\t\t\treboot_msg = (reboot_msg_t *) msg->data;\n\t\t\tif (reboot_msg && reboot_msg->features) {\n\t\t\t\tinfo(\"Node reboot request with features %s being processed\",\n\t\t\t\t     reboot_msg->features);\n\t\t\t\t(void) node_features_g_node_set(\n\t\t\t\t\t\treboot_msg->features);\n\t\t\t\tif (reboot_msg->features[0]) {\n\t\t\t\t\txstrfmtcat(cmd, \"%s %s\",\n\t\t\t\t\t\t   sp, reboot_msg->features);\n\t\t\t\t} else {\n\t\t\t\t\tcmd = xstrdup(sp);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tcmd = xstrdup(sp);\n\t\t\t\tinfo(\"Node reboot request being processed\");\n\t\t\t}\n\t\t\tif (access(sp, R_OK | X_OK) < 0)\n\t\t\t\terror(\"Cannot run RebootProgram [%s]: %m\", sp);\n\t\t\telse if ((exit_code = system(cmd)))\n\t\t\t\terror(\"system(%s) returned %d\", reboot_program,\n\t\t\t\t      exit_code);\n\t\t\txfree(sp);\n\t\t\txfree(cmd);\n\t\t} else\n\t\t\terror(\"RebootProgram isn't defined in config\");\n\t\tslurm_conf_unlock();\n\t}\n\n\t/* Never return a message, slurmctld does not expect one */\n\t/* slurm_send_rc_msg(msg, rc); */\n}\n\nstatic void _job_limits_free(void *x)\n{\n\txfree(x);\n}\n\nstatic int _job_limits_match(void *x, void *key)\n{\n\tjob_mem_limits_t *job_limits_ptr = (job_mem_limits_t *) x;\n\tuint32_t *job_id = (uint32_t *) key;\n\tif (job_limits_ptr->job_id == *job_id)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int _step_limits_match(void *x, void *key)\n{\n\tjob_mem_limits_t *job_limits_ptr = (job_mem_limits_t *) x;\n\tstep_loc_t *step_ptr = (step_loc_t *) key;\n\n\tif ((job_limits_ptr->job_id  == step_ptr->jobid) &&\n\t    (job_limits_ptr->step_id == step_ptr->stepid))\n\t\treturn 1;\n\treturn 0;\n}\n\n/* Call only with job_limits_mutex locked */\nstatic void\n_load_job_limits(void)\n{\n\tList steps;\n\tListIterator step_iter;\n\tstep_loc_t *stepd;\n\tint fd;\n\tjob_mem_limits_t *job_limits_ptr;\n\tslurmstepd_mem_info_t stepd_mem_info;\n\n\tif (!job_limits_list)\n\t\tjob_limits_list = list_create(_job_limits_free);\n\tjob_limits_loaded = true;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\tstep_iter = list_iterator_create(steps);\n\twhile ((stepd = list_next(step_iter))) {\n\t\tjob_limits_ptr = list_find_first(job_limits_list,\n\t\t\t\t\t\t _step_limits_match, stepd);\n\t\tif (job_limits_ptr)\t/* already processed */\n\t\t\tcontinue;\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1)\n\t\t\tcontinue;\t/* step completed */\n\n\t\tif (stepd_get_mem_limits(fd, stepd->protocol_version,\n\t\t\t\t\t  &stepd_mem_info) != SLURM_SUCCESS) {\n\t\t\terror(\"Error reading step %u.%u memory limits from \"\n\t\t\t      \"slurmstepd\",\n\t\t\t      stepd->jobid, stepd->stepid);\n\t\t\tclose(fd);\n\t\t\tcontinue;\n\t\t}\n\n\n\t\tif ((stepd_mem_info.job_mem_limit\n\t\t     || stepd_mem_info.step_mem_limit)) {\n\t\t\t/* create entry for this job */\n\t\t\tjob_limits_ptr = xmalloc(sizeof(job_mem_limits_t));\n\t\t\tjob_limits_ptr->job_id   = stepd->jobid;\n\t\t\tjob_limits_ptr->step_id  = stepd->stepid;\n\t\t\tjob_limits_ptr->job_mem  =\n\t\t\t\tstepd_mem_info.job_mem_limit;\n\t\t\tjob_limits_ptr->step_mem =\n\t\t\t\tstepd_mem_info.step_mem_limit;\n#if _LIMIT_INFO\n\t\t\tinfo(\"RecLim step:%u.%u job_mem:%u step_mem:%u\",\n\t\t\t     job_limits_ptr->job_id, job_limits_ptr->step_id,\n\t\t\t     job_limits_ptr->job_mem,\n\t\t\t     job_limits_ptr->step_mem);\n#endif\n\t\t\tlist_append(job_limits_list, job_limits_ptr);\n\t\t}\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(step_iter);\n\tFREE_NULL_LIST(steps);\n}\n\nstatic void\n_cancel_step_mem_limit(uint32_t job_id, uint32_t step_id)\n{\n\tslurm_msg_t msg;\n\tjob_notify_msg_t notify_req;\n\tjob_step_kill_msg_t kill_req;\n\n\t/* NOTE: Batch jobs may have no srun to get this message */\n\tslurm_msg_t_init(&msg);\n\tnotify_req.job_id      = job_id;\n\tnotify_req.job_step_id = step_id;\n\tnotify_req.message     = \"Exceeded job memory limit\";\n\tmsg.msg_type    = REQUEST_JOB_NOTIFY;\n\tmsg.data        = &notify_req;\n\tslurm_send_only_controller_msg(&msg);\n\n\tmemset(&kill_req, 0, sizeof(job_step_kill_msg_t));\n\tkill_req.job_id      = job_id;\n\tkill_req.job_step_id = step_id;\n\tkill_req.signal      = SIGKILL;\n\tkill_req.flags       = (uint16_t) 0;\n\tmsg.msg_type    = REQUEST_CANCEL_JOB_STEP;\n\tmsg.data        = &kill_req;\n\tslurm_send_only_controller_msg(&msg);\n}\n\n/* Enforce job memory limits here in slurmd. Step memory limits are\n * enforced within slurmstepd (using jobacct_gather plugin). */\nstatic void\n_enforce_job_mem_limit(void)\n{\n\tList steps;\n\tListIterator step_iter, job_limits_iter;\n\tjob_mem_limits_t *job_limits_ptr;\n\tstep_loc_t *stepd;\n\tint fd, i, job_inx, job_cnt;\n\tuint16_t vsize_factor;\n\tuint64_t step_rss, step_vsize;\n\tjob_step_id_msg_t acct_req;\n\tjob_step_stat_t *resp = NULL;\n\tstruct job_mem_info {\n\t\tuint32_t job_id;\n\t\tuint32_t mem_limit;\t/* MB */\n\t\tuint32_t mem_used;\t/* MB */\n\t\tuint32_t vsize_limit;\t/* MB */\n\t\tuint32_t vsize_used;\t/* MB */\n\t};\n\tstruct job_mem_info *job_mem_info_ptr = NULL;\n\n\t/* If users have configured MemLimitEnforce=no\n\t * in their slurm.conf keep going.\n\t */\n\tif (conf->mem_limit_enforce == false)\n\t\treturn;\n\n\tslurm_mutex_lock(&job_limits_mutex);\n\tif (!job_limits_loaded)\n\t\t_load_job_limits();\n\tif (list_count(job_limits_list) == 0) {\n\t\tslurm_mutex_unlock(&job_limits_mutex);\n\t\treturn;\n\t}\n\n\t/* Build table of job limits, use highest mem limit recorded */\n\tjob_mem_info_ptr = xmalloc((list_count(job_limits_list) + 1) *\n\t\t\t\t   sizeof(struct job_mem_info));\n\tjob_cnt = 0;\n\tjob_limits_iter = list_iterator_create(job_limits_list);\n\twhile ((job_limits_ptr = list_next(job_limits_iter))) {\n\t\tif (job_limits_ptr->job_mem == 0) \t/* no job limit */\n\t\t\tcontinue;\n\t\tfor (i=0; i<job_cnt; i++) {\n\t\t\tif (job_mem_info_ptr[i].job_id !=\n\t\t\t    job_limits_ptr->job_id)\n\t\t\t\tcontinue;\n\t\t\tjob_mem_info_ptr[i].mem_limit = MAX(\n\t\t\t\tjob_mem_info_ptr[i].mem_limit,\n\t\t\t\tjob_limits_ptr->job_mem);\n\t\t\tbreak;\n\t\t}\n\t\tif (i < job_cnt)\t/* job already found & recorded */\n\t\t\tcontinue;\n\t\tjob_mem_info_ptr[job_cnt].job_id    = job_limits_ptr->job_id;\n\t\tjob_mem_info_ptr[job_cnt].mem_limit = job_limits_ptr->job_mem;\n\t\tjob_cnt++;\n\t}\n\tlist_iterator_destroy(job_limits_iter);\n\tslurm_mutex_unlock(&job_limits_mutex);\n\n\tvsize_factor = slurm_get_vsize_factor();\n\tfor (i=0; i<job_cnt; i++) {\n\t\tjob_mem_info_ptr[i].vsize_limit = job_mem_info_ptr[i].\n\t\t\tmem_limit;\n\t\tjob_mem_info_ptr[i].vsize_limit *= (vsize_factor / 100.0);\n\t}\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\tstep_iter = list_iterator_create(steps);\n\twhile ((stepd = list_next(step_iter))) {\n\t\tfor (job_inx=0; job_inx<job_cnt; job_inx++) {\n\t\t\tif (job_mem_info_ptr[job_inx].job_id == stepd->jobid)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (job_inx >= job_cnt)\n\t\t\tcontinue;\t/* job/step not being tracked */\n\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1)\n\t\t\tcontinue;\t/* step completed */\n\t\tacct_req.job_id  = stepd->jobid;\n\t\tacct_req.step_id = stepd->stepid;\n\t\tresp = xmalloc(sizeof(job_step_stat_t));\n\n\t\tif ((!stepd_stat_jobacct(\n\t\t\t     fd, stepd->protocol_version,\n\t\t\t     &acct_req, resp)) &&\n\t\t    (resp->jobacct)) {\n\t\t\t/* resp->jobacct is NULL if account is disabled */\n\t\t\tjobacctinfo_getinfo((struct jobacctinfo *)\n\t\t\t\t\t    resp->jobacct,\n\t\t\t\t\t    JOBACCT_DATA_TOT_RSS,\n\t\t\t\t\t    &step_rss,\n\t\t\t\t\t    stepd->protocol_version);\n\t\t\tjobacctinfo_getinfo((struct jobacctinfo *)\n\t\t\t\t\t    resp->jobacct,\n\t\t\t\t\t    JOBACCT_DATA_TOT_VSIZE,\n\t\t\t\t\t    &step_vsize,\n\t\t\t\t\t    stepd->protocol_version);\n#if _LIMIT_INFO\n\t\t\tinfo(\"Step:%u.%u RSS:%\"PRIu64\" KB VSIZE:%\"PRIu64\" KB\",\n\t\t\t     stepd->jobid, stepd->stepid,\n\t\t\t     step_rss, step_vsize);\n#endif\n\t\t\tstep_rss /= 1024;\t/* KB to MB */\n\t\t\tstep_rss = MAX(step_rss, 1);\n\t\t\tjob_mem_info_ptr[job_inx].mem_used += step_rss;\n\t\t\tstep_vsize /= 1024;\t/* KB to MB */\n\t\t\tstep_vsize = MAX(step_vsize, 1);\n\t\t\tjob_mem_info_ptr[job_inx].vsize_used += step_vsize;\n\t\t}\n\t\tslurm_free_job_step_stat(resp);\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(step_iter);\n\tFREE_NULL_LIST(steps);\n\n\tfor (i=0; i<job_cnt; i++) {\n\t\tif (job_mem_info_ptr[i].mem_used == 0) {\n\t\t\t/* no steps found,\n\t\t\t * purge records for all steps of this job */\n\t\t\tslurm_mutex_lock(&job_limits_mutex);\n\t\t\tlist_delete_all(job_limits_list, _job_limits_match,\n\t\t\t\t\t&job_mem_info_ptr[i].job_id);\n\t\t\tslurm_mutex_unlock(&job_limits_mutex);\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((job_mem_info_ptr[i].mem_limit != 0) &&\n\t\t    (job_mem_info_ptr[i].mem_used >\n\t\t     job_mem_info_ptr[i].mem_limit)) {\n\t\t\tinfo(\"Job %u exceeded memory limit (%u>%u), \"\n\t\t\t     \"cancelling it\", job_mem_info_ptr[i].job_id,\n\t\t\t     job_mem_info_ptr[i].mem_used,\n\t\t\t     job_mem_info_ptr[i].mem_limit);\n\t\t\t_cancel_step_mem_limit(job_mem_info_ptr[i].job_id,\n\t\t\t\t\t       NO_VAL);\n\t\t} else if ((job_mem_info_ptr[i].vsize_limit != 0) &&\n\t\t\t   (job_mem_info_ptr[i].vsize_used >\n\t\t\t    job_mem_info_ptr[i].vsize_limit)) {\n\t\t\tinfo(\"Job %u exceeded virtual memory limit (%u>%u), \"\n\t\t\t     \"cancelling it\", job_mem_info_ptr[i].job_id,\n\t\t\t     job_mem_info_ptr[i].vsize_used,\n\t\t\t     job_mem_info_ptr[i].vsize_limit);\n\t\t\t_cancel_step_mem_limit(job_mem_info_ptr[i].job_id,\n\t\t\t\t\t       NO_VAL);\n\t\t}\n\t}\n\txfree(job_mem_info_ptr);\n}\n\nstatic int\n_rpc_ping(slurm_msg_t *msg)\n{\n\tint        rc = SLURM_SUCCESS;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tstatic bool first_msg = true;\n\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"Security violation, ping RPC from uid %d\",\n\t\t      req_uid);\n\t\tif (first_msg) {\n\t\t\terror(\"Do you have SlurmUser configured as uid %d?\",\n\t\t\t      req_uid);\n\t\t}\n\t\trc = ESLURM_USER_ID_MISSING;\t/* or bad in this case */\n\t}\n\tfirst_msg = false;\n\n\tif (rc != SLURM_SUCCESS) {\n\t\t/* Return result. If the reply can't be sent this indicates\n\t\t * 1. The network is broken OR\n\t\t * 2. slurmctld has died    OR\n\t\t * 3. slurmd was paged out due to full memory\n\t\t * If the reply request fails, we send an registration message\n\t\t * to slurmctld in hopes of avoiding having the node set DOWN\n\t\t * due to slurmd paging and not being able to respond in a\n\t\t * timely fashion. */\n\t\tif (slurm_send_rc_msg(msg, rc) < 0) {\n\t\t\terror(\"Error responding to ping: %m\");\n\t\t\tsend_registration_msg(SLURM_SUCCESS, false);\n\t\t}\n\t} else {\n\t\tslurm_msg_t resp_msg;\n\t\tping_slurmd_resp_msg_t ping_resp;\n\t\tget_cpu_load(&ping_resp.cpu_load);\n\t\tget_free_mem(&ping_resp.free_mem);\n\t\tslurm_msg_t_copy(&resp_msg, msg);\n\t\tresp_msg.msg_type = RESPONSE_PING_SLURMD;\n\t\tresp_msg.data     = &ping_resp;\n\n\t\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\t}\n\n\t/* Take this opportunity to enforce any job memory limits */\n\t_enforce_job_mem_limit();\n\t/* Clear up any stalled file transfers as well */\n\t_file_bcast_cleanup();\n\treturn rc;\n}\n\nstatic int\n_rpc_health_check(slurm_msg_t *msg)\n{\n\tint        rc = SLURM_SUCCESS;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"Security violation, health check RPC from uid %d\",\n\t\t      req_uid);\n\t\trc = ESLURM_USER_ID_MISSING;\t/* or bad in this case */\n\t}\n\n\t/* Return result. If the reply can't be sent this indicates that\n\t * 1. The network is broken OR\n\t * 2. slurmctld has died    OR\n\t * 3. slurmd was paged out due to full memory\n\t * If the reply request fails, we send an registration message to\n\t * slurmctld in hopes of avoiding having the node set DOWN due to\n\t * slurmd paging and not being able to respond in a timely fashion. */\n\tif (slurm_send_rc_msg(msg, rc) < 0) {\n\t\terror(\"Error responding to health check: %m\");\n\t\tsend_registration_msg(SLURM_SUCCESS, false);\n\t}\n\n\tif (rc == SLURM_SUCCESS)\n\t\trc = run_script_health_check();\n\n\t/* Take this opportunity to enforce any job memory limits */\n\t_enforce_job_mem_limit();\n\t/* Clear up any stalled file transfers as well */\n\t_file_bcast_cleanup();\n\treturn rc;\n}\n\n\nstatic int\n_rpc_acct_gather_update(slurm_msg_t *msg)\n{\n\tint        rc = SLURM_SUCCESS;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tstatic bool first_msg = true;\n\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"Security violation, acct_gather_update RPC from uid %d\",\n\t\t      req_uid);\n\t\tif (first_msg) {\n\t\t\terror(\"Do you have SlurmUser configured as uid %d?\",\n\t\t\t      req_uid);\n\t\t}\n\t\trc = ESLURM_USER_ID_MISSING;\t/* or bad in this case */\n\t}\n\tfirst_msg = false;\n\n\tif (rc != SLURM_SUCCESS) {\n\t\t/* Return result. If the reply can't be sent this indicates\n\t\t * 1. The network is broken OR\n\t\t * 2. slurmctld has died    OR\n\t\t * 3. slurmd was paged out due to full memory\n\t\t * If the reply request fails, we send an registration message\n\t\t * to slurmctld in hopes of avoiding having the node set DOWN\n\t\t * due to slurmd paging and not being able to respond in a\n\t\t * timely fashion. */\n\t\tif (slurm_send_rc_msg(msg, rc) < 0) {\n\t\t\terror(\"Error responding to account gather: %m\");\n\t\t\tsend_registration_msg(SLURM_SUCCESS, false);\n\t\t}\n\t} else {\n\t\tslurm_msg_t resp_msg;\n\t\tacct_gather_node_resp_msg_t acct_msg;\n\n\t\t/* Update node energy usage data */\n\t\tacct_gather_energy_g_update_node_energy();\n\n\t\tmemset(&acct_msg, 0, sizeof(acct_gather_node_resp_msg_t));\n\t\tacct_msg.node_name = conf->node_name;\n\t\tacct_msg.sensor_cnt = 1;\n\t\tacct_msg.energy = acct_gather_energy_alloc(acct_msg.sensor_cnt);\n\t\tacct_gather_energy_g_get_data(\n\t\t\tENERGY_DATA_NODE_ENERGY, acct_msg.energy);\n\n\t\tslurm_msg_t_copy(&resp_msg, msg);\n\t\tresp_msg.msg_type = RESPONSE_ACCT_GATHER_UPDATE;\n\t\tresp_msg.data     = &acct_msg;\n\n\t\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\n\t\tacct_gather_energy_destroy(acct_msg.energy);\n\t}\n\treturn rc;\n}\n\nstatic int\n_rpc_acct_gather_energy(slurm_msg_t *msg)\n{\n\tint        rc = SLURM_SUCCESS;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tstatic bool first_msg = true;\n\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"Security violation, acct_gather_update RPC from uid %d\",\n\t\t      req_uid);\n\t\tif (first_msg) {\n\t\t\terror(\"Do you have SlurmUser configured as uid %d?\",\n\t\t\t      req_uid);\n\t\t}\n\t\trc = ESLURM_USER_ID_MISSING;\t/* or bad in this case */\n\t}\n\tfirst_msg = false;\n\n\tif (rc != SLURM_SUCCESS) {\n\t\tif (slurm_send_rc_msg(msg, rc) < 0)\n\t\t\terror(\"Error responding to energy request: %m\");\n\t} else {\n\t\tslurm_msg_t resp_msg;\n\t\tacct_gather_node_resp_msg_t acct_msg;\n\t\ttime_t now = time(NULL), last_poll = 0;\n\t\tint data_type = ENERGY_DATA_STRUCT;\n\t\tuint16_t sensor_cnt;\n\t\tacct_gather_energy_req_msg_t *req = msg->data;\n\n\t\tacct_gather_energy_g_get_data(ENERGY_DATA_LAST_POLL,\n\t\t\t\t\t      &last_poll);\n\t\tacct_gather_energy_g_get_data(ENERGY_DATA_SENSOR_CNT,\n\t\t\t\t\t      &sensor_cnt);\n\n\t\t/* If we polled later than delta seconds then force a\n\t\t   new poll.\n\t\t*/\n\t\tif ((now - last_poll) > req->delta)\n\t\t\tdata_type = ENERGY_DATA_JOULES_TASK;\n\n\t\tmemset(&acct_msg, 0, sizeof(acct_gather_node_resp_msg_t));\n\t\tacct_msg.sensor_cnt = sensor_cnt;\n\t\tacct_msg.energy = acct_gather_energy_alloc(acct_msg.sensor_cnt);\n\n\t\tacct_gather_energy_g_get_data(data_type, acct_msg.energy);\n\n\t\tslurm_msg_t_copy(&resp_msg, msg);\n\t\tresp_msg.msg_type = RESPONSE_ACCT_GATHER_ENERGY;\n\t\tresp_msg.data     = &acct_msg;\n\n\t\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\n\t\tacct_gather_energy_destroy(acct_msg.energy);\n\t}\n\treturn rc;\n}\n\nstatic int\n_signal_jobstep(uint32_t jobid, uint32_t stepid, uid_t req_uid,\n\t\tuint32_t signal)\n{\n\tint               fd, rc = SLURM_SUCCESS;\n\tuid_t uid;\n\tuint16_t protocol_version;\n\n\t/*  There will be no stepd if the prolog is still running\n\t *   Return failure so caller can retry.\n\t */\n\tif (_prolog_is_running (jobid)) {\n\t\tinfo (\"signal %d req for %u.%u while prolog is running.\"\n\t\t      \" Returning failure.\", signal, jobid, stepid);\n\t\treturn SLURM_FAILURE;\n\t}\n\n\tfd = stepd_connect(conf->spooldir, conf->node_name, jobid, stepid,\n\t\t\t   &protocol_version);\n\tif (fd == -1) {\n\t\tdebug(\"signal for nonexistent %u.%u stepd_connect failed: %m\",\n\t\t      jobid, stepid);\n\t\treturn ESLURM_INVALID_JOB_ID;\n\t}\n\n\tif ((int)(uid = stepd_get_uid(fd, protocol_version)) < 0) {\n\t\tdebug(\"_signal_jobstep: couldn't read from the step %u.%u: %m\",\n\t\t      jobid, stepid);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done2;\n\t}\n\n\tif ((req_uid != uid) && (!_slurm_authorized_user(req_uid))) {\n\t\tdebug(\"kill req from uid %ld for job %u.%u owned by uid %ld\",\n\t\t      (long) req_uid, jobid, stepid, (long) uid);\n\t\trc = ESLURM_USER_ID_MISSING;     /* or bad in this case */\n\t\tgoto done2;\n\t}\n\n#ifdef HAVE_AIX\n#  ifdef SIGMIGRATE\n#    ifdef SIGSOUND\n\t/* SIGMIGRATE and SIGSOUND are used to initiate job checkpoint on AIX.\n\t * These signals are not sent to the entire process group, but just a\n\t * single process, namely the PMD. */\n\tif (signal == SIGMIGRATE || signal == SIGSOUND) {\n\t\trc = stepd_signal_task_local(fd, protocol_version,\n\t\t\t\t\t     signal, 0);\n\t\tgoto done2;\n\t}\n#    endif\n#  endif\n#endif\n\n\trc = stepd_signal_container(fd, protocol_version, signal);\n\tif (rc == -1)\n\t\trc = ESLURMD_JOB_NOTRUNNING;\n\ndone2:\n\tclose(fd);\n\treturn rc;\n}\n\nstatic void\n_rpc_signal_tasks(slurm_msg_t *msg)\n{\n\tint               rc = SLURM_SUCCESS;\n\tuid_t             req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t\t conf->auth_info);\n\tkill_tasks_msg_t *req = (kill_tasks_msg_t *) msg->data;\n\tuint32_t flag;\n\tuint32_t sig;\n\n\tflag = req->signal >> 24;\n\tsig  = req->signal & 0xfff;\n\n\tif (flag & KILL_FULL_JOB) {\n\t\tdebug(\"%s: sending signal %u to entire job %u flag %u\",\n\t\t      __func__, sig, req->job_id, flag);\n\t\t_kill_all_active_steps(req->job_id, sig, true);\n\t} else if (flag & KILL_STEPS_ONLY) {\n\t\tdebug(\"%s: sending signal %u to all steps job %u flag %u\",\n\t\t      __func__, sig, req->job_id, flag);\n\t\t_kill_all_active_steps(req->job_id, sig, false);\n\t} else {\n\t\tdebug(\"%s: sending signal %u to step %u.%u flag %u\", __func__,\n\t\t      sig, req->job_id, req->job_step_id, flag);\n\t\trc = _signal_jobstep(req->job_id, req->job_step_id, req_uid,\n\t\t\t\t     req->signal);\n\t}\n\tslurm_send_rc_msg(msg, rc);\n}\n\nstatic void\n_rpc_checkpoint_tasks(slurm_msg_t *msg)\n{\n\tint               fd;\n\tint               rc = SLURM_SUCCESS;\n\tuid_t             req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t\t conf->auth_info);\n\tcheckpoint_tasks_msg_t *req = (checkpoint_tasks_msg_t *) msg->data;\n\tuint16_t protocol_version;\n\tuid_t uid;\n\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->job_step_id, &protocol_version);\n\tif (fd == -1) {\n\t\tdebug(\"checkpoint for nonexistent %u.%u stepd_connect \"\n\t\t      \"failed: %m\", req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done;\n\t}\n\n\tif ((int)(uid = stepd_get_uid(fd, protocol_version)) < 0) {\n\t\tdebug(\"_rpc_checkpoint_tasks: couldn't read from the \"\n\t\t      \"step %u.%u: %m\",\n\t\t      req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done2;\n\t}\n\n\tif ((req_uid != uid) && (!_slurm_authorized_user(req_uid))) {\n\t\tdebug(\"checkpoint req from uid %ld for job %u.%u owned by \"\n\t\t      \"uid %ld\", (long) req_uid, req->job_id, req->job_step_id,\n\t\t      (long) uid);\n\t\trc = ESLURM_USER_ID_MISSING;     /* or bad in this case */\n\t\tgoto done2;\n\t}\n\n\trc = stepd_checkpoint(fd, protocol_version,\n\t\t\t      req->timestamp, req->image_dir);\n\tif (rc == -1)\n\t\trc = ESLURMD_JOB_NOTRUNNING;\n\ndone2:\n\tclose(fd);\ndone:\n\tslurm_send_rc_msg(msg, rc);\n}\n\nstatic void\n_rpc_terminate_tasks(slurm_msg_t *msg)\n{\n\tkill_tasks_msg_t *req = (kill_tasks_msg_t *) msg->data;\n\tint               rc = SLURM_SUCCESS;\n\tint               fd;\n\tuid_t             req_uid, uid;\n\tuint16_t protocol_version;\n\n\tdebug3(\"Entering _rpc_terminate_tasks\");\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->job_step_id, &protocol_version);\n\tif (fd == -1) {\n\t\tdebug(\"kill for nonexistent job %u.%u stepd_connect \"\n\t\t      \"failed: %m\", req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done;\n\t}\n\n\tif ((int)(uid = stepd_get_uid(fd, protocol_version)) < 0) {\n\t\tdebug(\"terminate_tasks couldn't read from the step %u.%u: %m\",\n\t\t      req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done2;\n\t}\n\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tif ((req_uid != uid)\n\t    && (!_slurm_authorized_user(req_uid))) {\n\t\tdebug(\"kill req from uid %ld for job %u.%u owned by uid %ld\",\n\t\t      (long) req_uid, req->job_id, req->job_step_id,\n\t\t      (long) uid);\n\t\trc = ESLURM_USER_ID_MISSING;     /* or bad in this case */\n\t\tgoto done2;\n\t}\n\n\trc = stepd_terminate(fd, protocol_version);\n\tif (rc == -1)\n\t\trc = ESLURMD_JOB_NOTRUNNING;\n\ndone2:\n\tclose(fd);\ndone:\n\tslurm_send_rc_msg(msg, rc);\n}\n\nstatic int\n_rpc_step_complete(slurm_msg_t *msg)\n{\n\tstep_complete_msg_t *req = (step_complete_msg_t *)msg->data;\n\tint               rc = SLURM_SUCCESS;\n\tint               fd;\n\tuid_t             req_uid;\n\tuint16_t protocol_version;\n\n\tdebug3(\"Entering _rpc_step_complete\");\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->job_step_id, &protocol_version);\n\tif (fd == -1) {\n\t\terror(\"stepd_connect to %u.%u failed: %m\",\n\t\t      req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done;\n\t}\n\n\t/* step completion messages are only allowed from other slurmstepd,\n\t   so only root or SlurmUser is allowed here */\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\tdebug(\"step completion from uid %ld for job %u.%u\",\n\t\t      (long) req_uid, req->job_id, req->job_step_id);\n\t\trc = ESLURM_USER_ID_MISSING;     /* or bad in this case */\n\t\tgoto done2;\n\t}\n\n\trc = stepd_completion(fd, protocol_version, req);\n\tif (rc == -1)\n\t\trc = ESLURMD_JOB_NOTRUNNING;\n\ndone2:\n\tclose(fd);\ndone:\n\tslurm_send_rc_msg(msg, rc);\n\n\treturn rc;\n}\n\nstatic void _setup_step_complete_msg(slurm_msg_t *msg, void *data)\n{\n\tslurm_msg_t_init(msg);\n\tmsg->msg_type = REQUEST_STEP_COMPLETE;\n\tmsg->data = data;\n}\n\n/* This step_complete RPC came from slurmstepd because we are using\n * message aggregation configured and we are at the head of the tree.\n * This just adds the message to the list and goes on it's merry way. */\nstatic int\n_rpc_step_complete_aggr(slurm_msg_t *msg)\n{\n\tint rc;\n\tuid_t uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\n\tif (!_slurm_authorized_user(uid)) {\n\t\terror(\"Security violation: step_complete_aggr from uid %d\",\n\t\t      uid);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\treturn SLURM_ERROR;\n\t}\n\n\tif (conf->msg_aggr_window_msgs > 1) {\n\t\tslurm_msg_t *req = xmalloc_nz(sizeof(slurm_msg_t));\n\t\t_setup_step_complete_msg(req, msg->data);\n\t\tmsg->data = NULL;\n\n\t\tmsg_aggr_add_msg(req, 1, NULL);\n\t} else {\n\t\tslurm_msg_t req;\n\t\t_setup_step_complete_msg(&req, msg->data);\n\n\t\twhile (slurm_send_recv_controller_rc_msg(&req, &rc) < 0) {\n\t\t\terror(\"Unable to send step complete, \"\n\t\t\t      \"trying again in a minute: %m\");\n\t\t}\n\t}\n\n\t/* Finish communication with the stepd, we have to wait for\n\t * the message back from the slurmctld or we will cause a race\n\t * condition with srun.\n\t */\n\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\n\treturn SLURM_SUCCESS;\n}\n\n/* Get list of active jobs and steps, xfree returned value */\nstatic char *\n_get_step_list(void)\n{\n\tchar tmp[64];\n\tchar *step_list = NULL;\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tint fd;\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1)\n\t\t\tcontinue;\n\n\t\tif (stepd_state(fd, stepd->protocol_version)\n\t\t    == SLURMSTEPD_NOT_RUNNING) {\n\t\t\tdebug(\"stale domain socket for stepd %u.%u \",\n\t\t\t      stepd->jobid, stepd->stepid);\n\t\t\tclose(fd);\n\t\t\tcontinue;\n\t\t}\n\t\tclose(fd);\n\n\t\tif (step_list)\n\t\t\txstrcat(step_list, \", \");\n\t\tif (stepd->stepid == NO_VAL) {\n\t\t\tsnprintf(tmp, sizeof(tmp), \"%u\",\n\t\t\t\t stepd->jobid);\n\t\t\txstrcat(step_list, tmp);\n\t\t} else {\n\t\t\tsnprintf(tmp, sizeof(tmp), \"%u.%u\",\n\t\t\t\t stepd->jobid, stepd->stepid);\n\t\t\txstrcat(step_list, tmp);\n\t\t}\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\tif (step_list == NULL)\n\t\txstrcat(step_list, \"NONE\");\n\treturn step_list;\n}\n\nstatic int\n_rpc_daemon_status(slurm_msg_t *msg)\n{\n\tslurm_msg_t      resp_msg;\n\tslurmd_status_t *resp = NULL;\n\n\tresp = xmalloc(sizeof(slurmd_status_t));\n\tresp->actual_cpus        = conf->actual_cpus;\n\tresp->actual_boards      = conf->actual_boards;\n\tresp->actual_sockets     = conf->actual_sockets;\n\tresp->actual_cores       = conf->actual_cores;\n\tresp->actual_threads     = conf->actual_threads;\n\tresp->actual_real_mem    = conf->real_memory_size;\n\tresp->actual_tmp_disk    = conf->tmp_disk_space;\n\tresp->booted             = startup;\n\tresp->hostname           = xstrdup(conf->node_name);\n\tresp->step_list          = _get_step_list();\n\tresp->last_slurmctld_msg = last_slurmctld_msg;\n\tresp->pid                = conf->pid;\n\tresp->slurmd_debug       = conf->debug_level;\n\tresp->slurmd_logfile     = xstrdup(conf->logfile);\n\tresp->version            = xstrdup(SLURM_VERSION_STRING);\n\n\tslurm_msg_t_copy(&resp_msg, msg);\n\tresp_msg.msg_type = RESPONSE_SLURMD_STATUS;\n\tresp_msg.data     = resp;\n\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\tslurm_free_slurmd_status(resp);\n\treturn SLURM_SUCCESS;\n}\n\nstatic int\n_rpc_stat_jobacct(slurm_msg_t *msg)\n{\n\tjob_step_id_msg_t *req = (job_step_id_msg_t *)msg->data;\n\tslurm_msg_t        resp_msg;\n\tjob_step_stat_t *resp = NULL;\n\tint fd;\n\tuid_t req_uid, uid;\n\tuint16_t protocol_version;\n\n\tdebug3(\"Entering _rpc_stat_jobacct\");\n\t/* step completion messages are only allowed from other slurmstepd,\n\t   so only root or SlurmUser is allowed here */\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->step_id, &protocol_version);\n\tif (fd == -1) {\n\t\terror(\"stepd_connect to %u.%u failed: %m\",\n\t\t      req->job_id, req->step_id);\n\t\tslurm_send_rc_msg(msg, ESLURM_INVALID_JOB_ID);\n\t\treturn\tESLURM_INVALID_JOB_ID;\n\t}\n\n\tif ((int)(uid = stepd_get_uid(fd, protocol_version)) < 0) {\n\t\tdebug(\"stat_jobacct couldn't read from the step %u.%u: %m\",\n\t\t      req->job_id, req->step_id);\n\t\tclose(fd);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_INVALID_JOB_ID);\n\t\treturn\tESLURM_INVALID_JOB_ID;\n\t}\n\n\t/*\n\t * check that requesting user ID is the SLURM UID or root\n\t */\n\tif ((req_uid != uid) && (!_slurm_authorized_user(req_uid))) {\n\t\terror(\"stat_jobacct from uid %ld for job %u \"\n\t\t      \"owned by uid %ld\",\n\t\t      (long) req_uid, req->job_id, (long) uid);\n\n\t\tif (msg->conn_fd >= 0) {\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\t\tclose(fd);\n\t\t\treturn ESLURM_USER_ID_MISSING;/* or bad in this case */\n\t\t}\n\t}\n\n\tresp = xmalloc(sizeof(job_step_stat_t));\n\tresp->step_pids = xmalloc(sizeof(job_step_pids_t));\n\tresp->step_pids->node_name = xstrdup(conf->node_name);\n\tslurm_msg_t_copy(&resp_msg, msg);\n\tresp->return_code = SLURM_SUCCESS;\n\n\tif (stepd_stat_jobacct(fd, protocol_version, req, resp)\n\t    == SLURM_ERROR) {\n\t\tdebug(\"accounting for nonexistent job %u.%u requested\",\n\t\t      req->job_id, req->step_id);\n\t}\n\n\t/* FIX ME: This should probably happen in the\n\t   stepd_stat_jobacct to get more information about the pids.\n\t*/\n\tif (stepd_list_pids(fd, protocol_version, &resp->step_pids->pid,\n\t\t\t    &resp->step_pids->pid_cnt) == SLURM_ERROR) {\n\t\tdebug(\"No pids for nonexistent job %u.%u requested\",\n\t\t      req->job_id, req->step_id);\n\t}\n\n\tclose(fd);\n\n\tresp_msg.msg_type     = RESPONSE_JOB_STEP_STAT;\n\tresp_msg.data         = resp;\n\n\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\tslurm_free_job_step_stat(resp);\n\treturn SLURM_SUCCESS;\n}\n\nstatic int\n_callerid_find_job(callerid_conn_t conn, uint32_t *job_id)\n{\n\tino_t inode;\n\tpid_t pid;\n\tint rc;\n\n\trc = callerid_find_inode_by_conn(conn, &inode);\n\tif (rc != SLURM_SUCCESS) {\n\t\tdebug3(\"network_callerid inode not found\");\n\t\treturn ESLURM_INVALID_JOB_ID;\n\t}\n\tdebug3(\"network_callerid found inode %lu\", (long unsigned int)inode);\n\n\trc = find_pid_by_inode(&pid, inode);\n\tif (rc != SLURM_SUCCESS) {\n\t\tdebug3(\"network_callerid process not found\");\n\t\treturn ESLURM_INVALID_JOB_ID;\n\t}\n\tdebug3(\"network_callerid found process %d\", (pid_t)pid);\n\n\trc = slurm_pid2jobid(pid, job_id);\n\tif (rc != SLURM_SUCCESS) {\n\t\tdebug3(\"network_callerid job not found\");\n\t\treturn ESLURM_INVALID_JOB_ID;\n\t}\n\tdebug3(\"network_callerid found job %u\", *job_id);\n\treturn SLURM_SUCCESS;\n}\n\nstatic int\n_rpc_network_callerid(slurm_msg_t *msg)\n{\n\tnetwork_callerid_msg_t *req = (network_callerid_msg_t *)msg->data;\n\tslurm_msg_t resp_msg;\n\tnetwork_callerid_resp_t *resp = NULL;\n\n\tuid_t req_uid = -1;\n\tuid_t job_uid = -1;\n\tuint32_t job_id = (uint32_t)NO_VAL;\n\tcallerid_conn_t conn;\n\tint rc = ESLURM_INVALID_JOB_ID;\n\tchar ip_src_str[INET6_ADDRSTRLEN];\n\tchar ip_dst_str[INET6_ADDRSTRLEN];\n\n\tdebug3(\"Entering _rpc_network_callerid\");\n\n\tresp = xmalloc(sizeof(network_callerid_resp_t));\n\tslurm_msg_t_copy(&resp_msg, msg);\n\n\t/* Ideally this would be in an if block only when debug3 is enabled */\n\tinet_ntop(req->af, req->ip_src, ip_src_str, INET6_ADDRSTRLEN);\n\tinet_ntop(req->af, req->ip_dst, ip_dst_str, INET6_ADDRSTRLEN);\n\tdebug3(\"network_callerid checking %s:%u => %s:%u\",\n\t\tip_src_str, req->port_src, ip_dst_str, req->port_dst);\n\n\t/* My remote is the other's source */\n\tmemcpy((void*)&conn.ip_dst, (void*)&req->ip_src, 16);\n\tmemcpy((void*)&conn.ip_src, (void*)&req->ip_dst, 16);\n\tconn.port_src = req->port_dst;\n\tconn.port_dst = req->port_src;\n\tconn.af = req->af;\n\n\t/* Find the job id */\n\trc = _callerid_find_job(conn, &job_id);\n\tif (rc == SLURM_SUCCESS) {\n\t\t/* We found the job */\n\t\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\t\tif (!_slurm_authorized_user(req_uid)) {\n\t\t\t/* Requestor is not root or SlurmUser */\n\t\t\tjob_uid = _get_job_uid(job_id);\n\t\t\tif (job_uid != req_uid) {\n\t\t\t\t/* RPC call sent by non-root user who does not\n\t\t\t\t * own this job. Do not send them the job ID. */\n\t\t\t\terror(\"Security violation, REQUEST_NETWORK_CALLERID from uid=%d\",\n\t\t\t\t      req_uid);\n\t\t\t\tjob_id = NO_VAL;\n\t\t\t\trc = ESLURM_INVALID_JOB_ID;\n\t\t\t}\n\t\t}\n\t}\n\n\tresp->job_id = job_id;\n\tresp->node_name = xstrdup(conf->node_name);\n\n\tresp_msg.msg_type = RESPONSE_NETWORK_CALLERID;\n\tresp_msg.data     = resp;\n\n\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\tslurm_free_network_callerid_resp(resp);\n\treturn rc;\n}\n\nstatic int\n_rpc_list_pids(slurm_msg_t *msg)\n{\n\tjob_step_id_msg_t *req = (job_step_id_msg_t *)msg->data;\n\tslurm_msg_t        resp_msg;\n\tjob_step_pids_t *resp = NULL;\n\tint fd;\n\tuid_t req_uid;\n\tuid_t job_uid;\n\tuint16_t protocol_version = 0;\n\n\tdebug3(\"Entering _rpc_list_pids\");\n\t/* step completion messages are only allowed from other slurmstepd,\n\t * so only root or SlurmUser is allowed here */\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\n\tjob_uid = _get_job_uid(req->job_id);\n\n\tif ((int)job_uid < 0) {\n\t\terror(\"stat_pid for invalid job_id: %u\",\n\t\t      req->job_id);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_INVALID_JOB_ID);\n\t\treturn  ESLURM_INVALID_JOB_ID;\n\t}\n\n\t/*\n\t * check that requesting user ID is the SLURM UID or root\n\t */\n\tif ((req_uid != job_uid)\n\t    && (!_slurm_authorized_user(req_uid))) {\n\t\terror(\"stat_pid from uid %ld for job %u \"\n\t\t      \"owned by uid %ld\",\n\t\t      (long) req_uid, req->job_id, (long) job_uid);\n\n\t\tif (msg->conn_fd >= 0) {\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\t\treturn ESLURM_USER_ID_MISSING;/* or bad in this case */\n\t\t}\n\t}\n\n\tresp = xmalloc(sizeof(job_step_pids_t));\n\tslurm_msg_t_copy(&resp_msg, msg);\n\tresp->node_name = xstrdup(conf->node_name);\n\tresp->pid_cnt = 0;\n\tresp->pid = NULL;\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->step_id, &protocol_version);\n\tif (fd == -1) {\n\t\terror(\"stepd_connect to %u.%u failed: %m\",\n\t\t      req->job_id, req->step_id);\n\t\tslurm_send_rc_msg(msg, ESLURM_INVALID_JOB_ID);\n\t\tslurm_free_job_step_pids(resp);\n\t\treturn  ESLURM_INVALID_JOB_ID;\n\n\t}\n\n\tif (stepd_list_pids(fd, protocol_version,\n\t\t\t    &resp->pid, &resp->pid_cnt) == SLURM_ERROR) {\n\t\tdebug(\"No pids for nonexistent job %u.%u requested\",\n\t\t      req->job_id, req->step_id);\n\t}\n\n\tclose(fd);\n\n\tresp_msg.msg_type = RESPONSE_JOB_STEP_PIDS;\n\tresp_msg.data     = resp;\n\n\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\tslurm_free_job_step_pids(resp);\n\treturn SLURM_SUCCESS;\n}\n\n/*\n *  For the specified job_id: reply to slurmctld,\n *   sleep(configured kill_wait), then send SIGKILL\n */\nstatic void\n_rpc_timelimit(slurm_msg_t *msg)\n{\n\tuid_t           uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t   conf->auth_info);\n\tkill_job_msg_t *req = msg->data;\n\tint             nsteps, rc;\n\n\tif (!_slurm_authorized_user(uid)) {\n\t\terror (\"Security violation: rpc_timelimit req from uid %d\",\n\t\t       uid);\n\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\treturn;\n\t}\n\n\t/*\n\t *  Indicate to slurmctld that we've received the message\n\t */\n\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\tslurm_close(msg->conn_fd);\n\tmsg->conn_fd = -1;\n\n\tif (req->step_id != NO_VAL) {\n\t\tslurm_ctl_conf_t *cf;\n\t\tint delay;\n\t\t/* A jobstep has timed out:\n\t\t * - send the container a SIG_TIME_LIMIT or SIG_PREEMPTED\n\t\t *   to log the event\n\t\t * - send a SIGCONT to resume any suspended tasks\n\t\t * - send a SIGTERM to begin termination\n\t\t * - sleep KILL_WAIT\n\t\t * - send a SIGKILL to clean up\n\t\t */\n\t\tif (msg->msg_type == REQUEST_KILL_TIMELIMIT) {\n\t\t\trc = _signal_jobstep(req->job_id, req->step_id, uid,\n\t\t\t\t\t     SIG_TIME_LIMIT);\n\t\t} else {\n\t\t\trc = _signal_jobstep(req->job_id, req->step_id, uid,\n\t\t\t\t\t     SIG_PREEMPTED);\n\t\t}\n\t\tif (rc != SLURM_SUCCESS)\n\t\t\treturn;\n\t\trc = _signal_jobstep(req->job_id, req->step_id, uid, SIGCONT);\n\t\tif (rc != SLURM_SUCCESS)\n\t\t\treturn;\n\t\trc = _signal_jobstep(req->job_id, req->step_id, uid, SIGTERM);\n\t\tif (rc != SLURM_SUCCESS)\n\t\t\treturn;\n\t\tcf = slurm_conf_lock();\n\t\tdelay = MAX(cf->kill_wait, 5);\n\t\tslurm_conf_unlock();\n\t\tsleep(delay);\n\t\t_signal_jobstep(req->job_id, req->step_id, uid, SIGKILL);\n\t\treturn;\n\t}\n\n\tif (msg->msg_type == REQUEST_KILL_TIMELIMIT)\n\t\t_kill_all_active_steps(req->job_id, SIG_TIME_LIMIT, true);\n\telse /* (msg->type == REQUEST_KILL_PREEMPTED) */\n\t\t_kill_all_active_steps(req->job_id, SIG_PREEMPTED, true);\n\tnsteps = _kill_all_active_steps(req->job_id, SIGTERM, false);\n\tverbose( \"Job %u: timeout: sent SIGTERM to %d active steps\",\n\t\t req->job_id, nsteps );\n\n\t/* Revoke credential, send SIGKILL, run epilog, etc. */\n\t_rpc_terminate_job(msg);\n}\n\nstatic void  _rpc_pid2jid(slurm_msg_t *msg)\n{\n\tjob_id_request_msg_t *req = (job_id_request_msg_t *) msg->data;\n\tslurm_msg_t           resp_msg;\n\tjob_id_response_msg_t resp;\n\tbool         found = false;\n\tList         steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tint fd;\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1)\n\t\t\tcontinue;\n\n\t\tif (stepd_pid_in_container(\n\t\t\t    fd, stepd->protocol_version,\n\t\t\t    req->job_pid)\n\t\t    || req->job_pid == stepd_daemon_pid(\n\t\t\t    fd, stepd->protocol_version)) {\n\t\t\tslurm_msg_t_copy(&resp_msg, msg);\n\t\t\tresp.job_id = stepd->jobid;\n\t\t\tresp.return_code = SLURM_SUCCESS;\n\t\t\tfound = true;\n\t\t\tclose(fd);\n\t\t\tbreak;\n\t\t}\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\tif (found) {\n\t\tdebug3(\"_rpc_pid2jid: pid(%u) found in %u\",\n\t\t       req->job_pid, resp.job_id);\n\t\tresp_msg.address      = msg->address;\n\t\tresp_msg.msg_type     = RESPONSE_JOB_ID;\n\t\tresp_msg.data         = &resp;\n\n\t\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\t} else {\n\t\tdebug3(\"_rpc_pid2jid: pid(%u) not found\", req->job_pid);\n\t\tslurm_send_rc_msg(msg, ESLURM_INVALID_JOB_ID);\n\t}\n}\n\n/* Validate sbcast credential.\n * NOTE: We can only perform the full credential validation once with\n * Munge without generating a credential replay error\n * RET SLURM_SUCCESS or an error code */\nstatic int\n_valid_sbcast_cred(file_bcast_msg_t *req, uid_t req_uid, uint16_t block_no,\n\t\t   uint32_t *job_id)\n{\n\tint rc = SLURM_SUCCESS;\n\tchar *nodes = NULL;\n\thostset_t hset = NULL;\n\n\t*job_id = NO_VAL;\n\trc = extract_sbcast_cred(conf->vctx, req->cred, block_no,\n\t\t\t\t job_id, &nodes);\n\tif (rc != 0) {\n\t\terror(\"Security violation: Invalid sbcast_cred from uid %d\",\n\t\t      req_uid);\n\t\treturn ESLURMD_INVALID_JOB_CREDENTIAL;\n\t}\n\n\tif (!(hset = hostset_create(nodes))) {\n\t\terror(\"Unable to parse sbcast_cred hostlist %s\", nodes);\n\t\trc = ESLURMD_INVALID_JOB_CREDENTIAL;\n\t} else if (!hostset_within(hset, conf->node_name)) {\n\t\terror(\"Security violation: sbcast_cred from %d has \"\n\t\t      \"bad hostset %s\", req_uid, nodes);\n\t\trc = ESLURMD_INVALID_JOB_CREDENTIAL;\n\t}\n\tif (hset)\n\t\thostset_destroy(hset);\n\txfree(nodes);\n\n\t/* print_sbcast_cred(req->cred); */\n\n\treturn rc;\n}\n\nstatic void _fb_rdlock(void)\n{\n\tslurm_mutex_lock(&file_bcast_mutex);\n\twhile (1) {\n\t\tif ((fb_write_wait_lock == 0) && (fb_write_lock == 0)) {\n\t\t\tfb_read_lock++;\n\t\t\tbreak;\n\t\t} else {\t/* wait for state change and retry */\n\t\t\tpthread_cond_wait(&file_bcast_cond, &file_bcast_mutex);\n\t\t}\n\t}\n\tslurm_mutex_unlock(&file_bcast_mutex);\n}\n\nstatic void _fb_rdunlock(void)\n{\n\tslurm_mutex_lock(&file_bcast_mutex);\n\tfb_read_lock--;\n\tpthread_cond_broadcast(&file_bcast_cond);\n\tslurm_mutex_unlock(&file_bcast_mutex);\n}\n\nstatic void _fb_wrlock(void)\n{\n\tslurm_mutex_lock(&file_bcast_mutex);\n\tfb_write_wait_lock++;\n\twhile (1) {\n\t\tif ((fb_read_lock == 0) && (fb_write_lock == 0)) {\n\t\t\tfb_write_lock++;\n\t\t\tfb_write_wait_lock--;\n\t\t\tbreak;\n\t\t} else {\t/* wait for state change and retry */\n\t\t\tpthread_cond_wait(&file_bcast_cond, &file_bcast_mutex);\n\t\t}\n\t}\n\tslurm_mutex_unlock(&file_bcast_mutex);\n}\n\nstatic void _fb_wrunlock(void)\n{\n\tslurm_mutex_lock(&file_bcast_mutex);\n\tfb_write_lock--;\n\tpthread_cond_broadcast(&file_bcast_cond);\n\tslurm_mutex_unlock(&file_bcast_mutex);\n}\n\nstatic int _bcast_find_in_list(void *x, void *y)\n{\n\tfile_bcast_info_t *info = (file_bcast_info_t *)x;\n\tfile_bcast_info_t *key = (file_bcast_info_t *)y;\n\t/* uid, job_id, and fname must match */\n\treturn ((info->uid == key->uid)\n\t\t&& (info->job_id == key->job_id)\n\t\t&& (!xstrcmp(info->fname, key->fname)));\n}\n\n/* must have read lock */\nstatic file_bcast_info_t *_bcast_lookup_file(file_bcast_info_t *key)\n{\n\treturn list_find_first(file_bcast_list, _bcast_find_in_list, key);\n}\n\n/* must not have read lock, will get write lock */\nstatic void _file_bcast_close_file(file_bcast_info_t *key)\n{\n\t_fb_wrlock();\n\tlist_delete_all(file_bcast_list, _bcast_find_in_list, key);\n\t_fb_wrunlock();\n}\n\nstatic void _free_file_bcast_info_t(file_bcast_info_t *f)\n{\n\txfree(f->fname);\n\tif (f->fd)\n\t\tclose(f->fd);\n\txfree(f);\n}\n\nstatic int _bcast_find_in_list_to_remove(void *x, void *y)\n{\n\tfile_bcast_info_t *f = (file_bcast_info_t *)x;\n\ttime_t *now = (time_t *) y;\n\n\tif (f->last_update + FILE_BCAST_TIMEOUT < *now) {\n\t\terror(\"Removing stalled file_bcast transfer from uid \"\n\t\t      \"%u to file `%s`\", f->uid, f->fname);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/* remove transfers that have stalled */\nstatic void _file_bcast_cleanup(void)\n{\n\ttime_t now = time(NULL);\n\n\t_fb_wrlock();\n\tlist_delete_all(file_bcast_list, _bcast_find_in_list_to_remove, &now);\n\t_fb_wrunlock();\n}\n\nvoid file_bcast_init(void)\n{\n\t/* skip locks during slurmd init */\n\tfile_bcast_list = list_create((ListDelF) _free_file_bcast_info_t);\n}\n\nvoid file_bcast_purge(void)\n{\n\t_fb_wrlock();\n\tlist_destroy(file_bcast_list);\n\t/* destroying list before exit, no need to unlock */\n}\n\nstatic int _rpc_file_bcast(slurm_msg_t *msg)\n{\n\tint rc, offset, inx;\n\tfile_bcast_info_t *file_info;\n\tfile_bcast_msg_t *req = msg->data;\n\tfile_bcast_info_t key;\n\n\tkey.uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tkey.gid = g_slurm_auth_get_gid(msg->auth_cred, conf->auth_info);\n\tkey.fname = req->fname;\n\n\trc = _valid_sbcast_cred(req, key.uid, req->block_no, &key.job_id);\n\tif ((rc != SLURM_SUCCESS) && !_slurm_authorized_user(key.uid))\n\t\treturn rc;\n\n#if 0\n\tinfo(\"last_block=%u force=%u modes=%o\",\n\t     req->last_block, req->force, req->modes);\n\tinfo(\"uid=%u gid=%u atime=%lu mtime=%lu block_len[0]=%u\",\n\t     req->uid, req->gid, req->atime, req->mtime, req->block_len);\n#if 0\n\t/* when the file being transferred is binary, the following line\n\t * can break the terminal output for slurmd */\n\tinfo(\"req->block[0]=%s, @ %lu\", \\\n\t     req->block[0], (unsigned long) &req->block);\n#endif\n#endif\n\n\tif (req->block_no == 1) {\n\t\tinfo(\"sbcast req_uid=%u job_id=%u fname=%s block_no=%u\",\n\t\t     key.uid, key.job_id, key.fname, req->block_no);\n\t} else {\n\t\tdebug(\"sbcast req_uid=%u job_id=%u fname=%s block_no=%u\",\n\t\t      key.uid, key.job_id, key.fname, req->block_no);\n\t}\n\n\t/* first block must register the file and open fd/mmap */\n\tif (req->block_no == 1) {\n\t\tif ((rc = _file_bcast_register_file(msg, &key)))\n\t\t\treturn rc;\n\t}\n\n\t_fb_rdlock();\n\tif (!(file_info = _bcast_lookup_file(&key))) {\n\t\terror(\"No registered file transfer for uid %u file `%s`.\",\n\t\t      key.uid, key.fname);\n\t\t_fb_rdunlock();\n\t\treturn SLURM_ERROR;\n\t}\n\n\t/* now decompress file */\n\tif (bcast_decompress_data(req) < 0) {\n\t\terror(\"sbcast: data decompression error for UID %u, file %s\",\n\t\t      key.uid, key.fname);\n\t\t_fb_rdunlock();\n\t\treturn SLURM_FAILURE;\n\t}\n\n\toffset = 0;\n\twhile (req->block_len - offset) {\n\t\tinx = write(file_info->fd, &req->block[offset],\n\t\t\t    (req->block_len - offset));\n\t\tif (inx == -1) {\n\t\t\tif ((errno == EINTR) || (errno == EAGAIN))\n\t\t\t\tcontinue;\n\t\t\terror(\"sbcast: uid:%u can't write `%s`: %m\",\n\t\t\t      key.uid, key.fname);\n\t\t\t_fb_rdunlock();\n\t\t\treturn SLURM_FAILURE;\n\t\t}\n\t\toffset += inx;\n\t}\n\n\tfile_info->last_update = time(NULL);\n\n\tif (req->last_block && fchmod(file_info->fd, (req->modes & 0777))) {\n\t\terror(\"sbcast: uid:%u can't chmod `%s`: %m\",\n\t\t      key.uid, key.fname);\n\t}\n\tif (req->last_block && fchown(file_info->fd, key.uid, key.gid)) {\n\t\terror(\"sbcast: uid:%u gid:%u can't chown `%s`: %m\",\n\t\t      key.uid, key.gid, key.fname);\n\t}\n\tif (req->last_block && req->atime) {\n\t\tstruct utimbuf time_buf;\n\t\ttime_buf.actime  = req->atime;\n\t\ttime_buf.modtime = req->mtime;\n\t\tif (utime(key.fname, &time_buf)) {\n\t\t\terror(\"sbcast: uid:%u can't utime `%s`: %m\",\n\t\t\t      key.uid, key.fname);\n\t\t}\n\t}\n\n\t_fb_rdunlock();\n\n\tif (req->last_block) {\n\t\t_file_bcast_close_file(&key);\n\t}\n\treturn SLURM_SUCCESS;\n}\n\n/* pass an open file descriptor back to the parent process */\nstatic void _send_back_fd(int socket, int fd)\n{\n\tstruct msghdr msg = { 0 };\n\tstruct cmsghdr *cmsg;\n\tchar buf[CMSG_SPACE(sizeof(fd))];\n\tmemset(buf, '\\0', sizeof(buf));\n\n\tmsg.msg_iov = NULL;\n\tmsg.msg_iovlen = 0;\n\tmsg.msg_control = buf;\n\tmsg.msg_controllen = sizeof(buf);\n\n\tcmsg = CMSG_FIRSTHDR(&msg);\n\tcmsg->cmsg_level = SOL_SOCKET;\n\tcmsg->cmsg_type = SCM_RIGHTS;\n\tcmsg->cmsg_len = CMSG_LEN(sizeof(fd));\n\n\tmemmove(CMSG_DATA(cmsg), &fd, sizeof(fd));\n\tmsg.msg_controllen = cmsg->cmsg_len;\n\n\tif (sendmsg(socket, &msg, 0) < 0)\n\t\terror(\"%s: failed to send fd: %m\", __func__);\n}\n\n/* receive an open file descriptor from fork()'d child over unix socket */\nstatic int _receive_fd(int socket)\n{\n\tstruct msghdr msg = {0};\n\tstruct cmsghdr *cmsg;\n\tint fd;\n\tmsg.msg_iov = NULL;\n\tmsg.msg_iovlen = 0;\n\tchar c_buffer[256];\n\tmsg.msg_control = c_buffer;\n\tmsg.msg_controllen = sizeof(c_buffer);\n\n\tif (recvmsg(socket, &msg, 0) < 0) {\n\t\terror(\"%s: failed to receive fd: %m\", __func__);\n\t\treturn -1;\n\t}\n\n\tcmsg = CMSG_FIRSTHDR(&msg);\n\tmemmove(&fd, CMSG_DATA(cmsg), sizeof(fd));\n\treturn fd;\n}\n\nstatic int _file_bcast_register_file(slurm_msg_t *msg,\n\t\t\t\t     file_bcast_info_t *key)\n{\n\tfile_bcast_msg_t *req = msg->data;\n\tint fd, flags, rc;\n\tint pipe[2];\n\tgids_t *gids;\n\tpid_t child;\n\tfile_bcast_info_t *file_info;\n\n\tif (!(gids = _gids_cache_lookup(req->user_name, key->gid))) {\n\t\terror(\"sbcast: gids_cache_lookup for %s failed\", req->user_name);\n\t\treturn SLURM_ERROR;\n\t}\n\n\tif ((rc = container_g_create(key->job_id))) {\n\t\terror(\"sbcast: container_g_create(%u): %m\", key->job_id);\n\t\t_dealloc_gids(gids);\n\t\treturn rc;\n\t}\n\n\t/* child process will setuid to the user, register the process\n\t * with the container, and open the file for us. */\n\n\tif (socketpair(AF_UNIX, SOCK_DGRAM, 0, pipe) != 0) {\n\t\terror(\"%s: Failed to open pipe: %m\", __func__);\n\t\t_dealloc_gids(gids);\n\t\treturn SLURM_ERROR;\n\t}\n\n\tchild = fork();\n\tif (child == -1) {\n\t\terror(\"sbcast: fork failure\");\n\t\t_dealloc_gids(gids);\n\t\tclose(pipe[0]);\n\t\tclose(pipe[1]);\n\t\treturn errno;\n\t} else if (child > 0) {\n\t\t/* get fd back from pipe */\n\t\tclose(pipe[0]);\n\t\twaitpid(child, &rc, 0);\n\t\t_dealloc_gids(gids);\n\t\tif (rc) {\n\t\t\tclose(pipe[1]);\n\t\t\treturn WEXITSTATUS(rc);\n\t\t}\n\n\t\tfd = _receive_fd(pipe[1]);\n\t\tclose(pipe[1]);\n\n\t\tfile_info = xmalloc(sizeof(file_bcast_info_t));\n\t\tfile_info->fd = fd;\n\t\tfile_info->fname = xstrdup(req->fname);\n\t\tfile_info->uid = key->uid;\n\t\tfile_info->gid = key->gid;\n\t\tfile_info->job_id = key->job_id;\n\t\tfile_info->start_time = time(NULL);\n\n\t\t//TODO: mmap the file here\n\t\t_fb_wrlock();\n\t\tlist_append(file_bcast_list, file_info);\n\t\t_fb_wrunlock();\n\n\t\treturn SLURM_SUCCESS;\n\t}\n\n\t/* child process below here */\n\n\tclose(pipe[1]);\n\n\t/* container_g_add_pid needs to be called in the\n\t   forked process part of the fork to avoid a race\n\t   condition where if this process makes a file or\n\t   detacts itself from a child before we add the pid\n\t   to the container in the parent of the fork.\n\t*/\n\tif (container_g_add_pid(key->job_id, getpid(), key->uid)) {\n\t\terror(\"container_g_add_pid(%u): %m\", key->job_id);\n\t\texit(SLURM_ERROR);\n\t}\n\n\t/* The child actually performs the I/O and exits with\n\t * a return code, do not return! */\n\n\t/*********************************************************************\\\n\t * NOTE: It would be best to do an exec() immediately after the fork()\n\t * in order to help prevent a possible deadlock in the child process\n\t * due to locks being set at the time of the fork and being freed by\n\t * the parent process, but not freed by the child process. Performing\n\t * the work inline is done for simplicity. Note that the logging\n\t * performed by error() should be safe due to the use of\n\t * atfork_install_handlers() as defined in src/common/log.c.\n\t * Change the code below with caution.\n\t\\*********************************************************************/\n\n\tif (setgroups(gids->ngids, gids->gids) < 0) {\n\t\terror(\"sbcast: uid: %u setgroups failed: %m\", key->uid);\n\t\texit(errno);\n\t}\n\t_dealloc_gids(gids);\n\n\tif (setgid(key->gid) < 0) {\n\t\terror(\"sbcast: uid:%u setgid(%u): %m\", key->uid, key->gid);\n\t\texit(errno);\n\t}\n\tif (setuid(key->uid) < 0) {\n\t\terror(\"sbcast: getuid(%u): %m\", key->uid);\n\t\texit(errno);\n\t}\n\n\tflags = O_WRONLY | O_CREAT;\n\tif (req->force)\n\t\tflags |= O_TRUNC;\n\telse\n\t\tflags |= O_EXCL;\n\n\tfd = open(key->fname, flags, 0700);\n\tif (fd == -1) {\n\t\terror(\"sbcast: uid:%u can't open `%s`: %m\",\n\t\t      key->uid, key->fname);\n\t\texit(errno);\n\t}\n\t_send_back_fd(pipe[0], fd);\n\tclose(fd);\n\texit(SLURM_SUCCESS);\n}\n\nstatic void\n_rpc_reattach_tasks(slurm_msg_t *msg)\n{\n\treattach_tasks_request_msg_t  *req = msg->data;\n\treattach_tasks_response_msg_t *resp =\n\t\txmalloc(sizeof(reattach_tasks_response_msg_t));\n\tslurm_msg_t                    resp_msg;\n\tint          rc   = SLURM_SUCCESS;\n\tuint16_t     port = 0;\n\tchar         host[MAXHOSTNAMELEN];\n\tslurm_addr_t   ioaddr;\n\tvoid        *job_cred_sig;\n\tuint32_t     len;\n\tint               fd;\n\tuid_t             req_uid;\n\tslurm_addr_t *cli = &msg->orig_addr;\n\tuint32_t nodeid = (uint32_t)NO_VAL;\n\tuid_t uid = -1;\n\tuint16_t protocol_version;\n\n\tslurm_msg_t_copy(&resp_msg, msg);\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->job_step_id, &protocol_version);\n\tif (fd == -1) {\n\t\tdebug(\"reattach for nonexistent job %u.%u stepd_connect\"\n\t\t      \" failed: %m\", req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done;\n\t}\n\n\tif ((int)(uid = stepd_get_uid(fd, protocol_version)) < 0) {\n\t\tdebug(\"_rpc_reattach_tasks couldn't read from the \"\n\t\t      \"step %u.%u: %m\",\n\t\t      req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done2;\n\t}\n\n\tnodeid = stepd_get_nodeid(fd, protocol_version);\n\n\tdebug2(\"_rpc_reattach_tasks: nodeid %d in the job step\", nodeid);\n\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tif ((req_uid != uid) && (!_slurm_authorized_user(req_uid))) {\n\t\terror(\"uid %ld attempt to attach to job %u.%u owned by %ld\",\n\t\t      (long) req_uid, req->job_id, req->job_step_id,\n\t\t      (long) uid);\n\t\trc = EPERM;\n\t\tgoto done2;\n\t}\n\n\tmemset(resp, 0, sizeof(reattach_tasks_response_msg_t));\n\tslurm_get_ip_str(cli, &port, host, sizeof(host));\n\n\t/*\n\t * Set response address by resp_port and client address\n\t */\n\tmemcpy(&resp_msg.address, cli, sizeof(slurm_addr_t));\n\tif (req->num_resp_port > 0) {\n\t\tport = req->resp_port[nodeid % req->num_resp_port];\n\t\tslurm_set_addr(&resp_msg.address, port, NULL);\n\t}\n\n\t/*\n\t * Set IO address by io_port and client address\n\t */\n\tmemcpy(&ioaddr, cli, sizeof(slurm_addr_t));\n\n\tif (req->num_io_port > 0) {\n\t\tport = req->io_port[nodeid % req->num_io_port];\n\t\tslurm_set_addr(&ioaddr, port, NULL);\n\t}\n\n\t/*\n\t * Get the signature of the job credential.  slurmstepd will need\n\t * this to prove its identity when it connects back to srun.\n\t */\n\tslurm_cred_get_signature(req->cred, (char **)(&job_cred_sig), &len);\n\tif (len != SLURM_IO_KEY_SIZE) {\n\t\terror(\"Incorrect slurm cred signature length\");\n\t\tgoto done2;\n\t}\n\n\tresp->gtids = NULL;\n\tresp->local_pids = NULL;\n\n\t /* NOTE: We need to use the protocol_version from\n\t  * sattach here since responses will be sent back to it. */\n\tif (msg->protocol_version < protocol_version)\n\t\tprotocol_version = msg->protocol_version;\n\n\t/* Following call fills in gtids and local_pids when successful. */\n\trc = stepd_attach(fd, protocol_version, &ioaddr,\n\t\t\t  &resp_msg.address, job_cred_sig, resp);\n\tif (rc != SLURM_SUCCESS) {\n\t\tdebug2(\"stepd_attach call failed\");\n\t\tgoto done2;\n\t}\n\ndone2:\n\tclose(fd);\ndone:\n\tdebug2(\"update step addrs rc = %d\", rc);\n\tresp_msg.data         = resp;\n\tresp_msg.msg_type     = RESPONSE_REATTACH_TASKS;\n\tresp->node_name       = xstrdup(conf->node_name);\n\tresp->return_code     = rc;\n\tdebug2(\"node %s sending rc = %d\", conf->node_name, rc);\n\n\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\tslurm_free_reattach_tasks_response_msg(resp);\n}\n\nstatic uid_t _get_job_uid(uint32_t jobid)\n{\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\tuid_t uid = -1;\n\tint fd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif (stepd->jobid != jobid) {\n\t\t\t/* multiple jobs expected on shared nodes */\n\t\t\tcontinue;\n\t\t}\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1) {\n\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\t\tuid = stepd_get_uid(fd, stepd->protocol_version);\n\n\t\tclose(fd);\n\t\tif ((int)uid < 0) {\n\t\t\tdebug(\"stepd_get_uid failed %u.%u: %m\",\n\t\t\t      stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\t\tbreak;\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\treturn uid;\n}\n\n/*\n * _kill_all_active_steps - signals the container of all steps of a job\n * jobid IN - id of job to signal\n * sig   IN - signal to send\n * batch IN - if true signal batch script, otherwise skip it\n * RET count of signaled job steps (plus batch script, if applicable)\n */\nstatic int\n_kill_all_active_steps(uint32_t jobid, int sig, bool batch)\n{\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\tint step_cnt  = 0;\n\tint fd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif (stepd->jobid != jobid) {\n\t\t\t/* multiple jobs expected on shared nodes */\n\t\t\tdebug3(\"Step from other job: jobid=%u (this jobid=%u)\",\n\t\t\t       stepd->jobid, jobid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif ((stepd->stepid == SLURM_BATCH_SCRIPT) && (!batch))\n\t\t\tcontinue;\n\n\t\tstep_cnt++;\n\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1) {\n\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdebug2(\"container signal %d to job %u.%u\",\n\t\t       sig, jobid, stepd->stepid);\n\t\tif (stepd_signal_container(\n\t\t\t    fd, stepd->protocol_version, sig) < 0)\n\t\t\tdebug(\"kill jobid=%u failed: %m\", jobid);\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\tif (step_cnt == 0)\n\t\tdebug2(\"No steps in jobid %u to send signal %d\", jobid, sig);\n\treturn step_cnt;\n}\n\n/*\n * _terminate_all_steps - signals the container of all steps of a job\n * jobid IN - id of job to signal\n * batch IN - if true signal batch script, otherwise skip it\n * RET count of signaled job steps (plus batch script, if applicable)\n */\nstatic int\n_terminate_all_steps(uint32_t jobid, bool batch)\n{\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\tint step_cnt  = 0;\n\tint fd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif (stepd->jobid != jobid) {\n\t\t\t/* multiple jobs expected on shared nodes */\n\t\t\tdebug3(\"Step from other job: jobid=%u (this jobid=%u)\",\n\t\t\t       stepd->jobid, jobid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif ((stepd->stepid == SLURM_BATCH_SCRIPT) && (!batch))\n\t\t\tcontinue;\n\n\t\tstep_cnt++;\n\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1) {\n\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdebug2(\"terminate job step %u.%u\", jobid, stepd->stepid);\n\t\tif (stepd_terminate(fd, stepd->protocol_version) < 0)\n\t\t\tdebug(\"kill jobid=%u.%u failed: %m\", jobid,\n\t\t\t      stepd->stepid);\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\tif (step_cnt == 0)\n\t\tdebug2(\"No steps in job %u to terminate\", jobid);\n\treturn step_cnt;\n}\n\nstatic bool\n_job_still_running(uint32_t job_id)\n{\n\tbool         retval = false;\n\tList         steps;\n\tListIterator i;\n\tstep_loc_t  *s     = NULL;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((s = list_next(i))) {\n\t\tif (s->jobid == job_id) {\n\t\t\tint fd;\n\t\t\tfd = stepd_connect(s->directory, s->nodename,\n\t\t\t\t\t   s->jobid, s->stepid,\n\t\t\t\t\t   &s->protocol_version);\n\t\t\tif (fd == -1)\n\t\t\t\tcontinue;\n\n\t\t\tif (stepd_state(fd, s->protocol_version)\n\t\t\t    != SLURMSTEPD_NOT_RUNNING) {\n\t\t\t\tretval = true;\n\t\t\t\tclose(fd);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tclose(fd);\n\t\t}\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\treturn retval;\n}\n\n/*\n * Wait until all job steps are in SLURMSTEPD_NOT_RUNNING state.\n * This indicates that switch_g_job_postfini has completed and\n * freed the switch windows (as needed only for Federation switch).\n */\nstatic void\n_wait_state_completed(uint32_t jobid, int max_delay)\n{\n\tint i;\n\n\tfor (i=0; i<max_delay; i++) {\n\t\tif (_steps_completed_now(jobid))\n\t\t\tbreak;\n\t\tsleep(1);\n\t}\n\tif (i >= max_delay)\n\t\terror(\"timed out waiting for job %u to complete\", jobid);\n}\n\nstatic bool\n_steps_completed_now(uint32_t jobid)\n{\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\tbool rc = true;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif (stepd->jobid == jobid) {\n\t\t\tint fd;\n\t\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t\t   &stepd->protocol_version);\n\t\t\tif (fd == -1)\n\t\t\t\tcontinue;\n\n\t\t\tif (stepd_state(fd, stepd->protocol_version)\n\t\t\t    != SLURMSTEPD_NOT_RUNNING) {\n\t\t\t\trc = false;\n\t\t\t\tclose(fd);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tclose(fd);\n\t\t}\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\treturn rc;\n}\n\nstatic void _epilog_complete_msg_setup(\n\tslurm_msg_t *msg, epilog_complete_msg_t *req, uint32_t jobid, int rc)\n{\n\tslurm_msg_t_init(msg);\n\tmemset(req, 0, sizeof(epilog_complete_msg_t));\n\n\treq->job_id      = jobid;\n\treq->return_code = rc;\n\treq->node_name   = conf->node_name;\n\n\tmsg->msg_type    = MESSAGE_EPILOG_COMPLETE;\n\tmsg->data        = req;\n}\n\n/*\n *  Send epilog complete message to currently active controller.\n *  If enabled, use message aggregation.\n *   Returns SLURM_SUCCESS if message sent successfully,\n *           SLURM_FAILURE if epilog complete message fails to be sent.\n */\nstatic int\n_epilog_complete(uint32_t jobid, int rc)\n{\n\tint ret = SLURM_SUCCESS;\n\n\tif (conf->msg_aggr_window_msgs > 1) {\n\t\t/* message aggregation is enabled */\n\t\tslurm_msg_t *msg = xmalloc(sizeof(slurm_msg_t));\n\t\tepilog_complete_msg_t *req =\n\t\t\txmalloc(sizeof(epilog_complete_msg_t));\n\n\t\t_epilog_complete_msg_setup(msg, req, jobid, rc);\n\n\t\t/* we need to copy this symbol */\n\t\treq->node_name   = xstrdup(conf->node_name);\n\n\t\tmsg_aggr_add_msg(msg, 0, NULL);\n\t} else {\n\t\tslurm_msg_t msg;\n\t\tepilog_complete_msg_t req;\n\n\t\t_epilog_complete_msg_setup(&msg, &req, jobid, rc);\n\n\t\t/* Note: No return code to message, slurmctld will resend\n\t\t * TERMINATE_JOB request if message send fails */\n\t\tif (slurm_send_only_controller_msg(&msg) < 0) {\n\t\t\terror(\"Unable to send epilog complete message: %m\");\n\t\t\tret = SLURM_ERROR;\n\t\t} else {\n\t\t\tdebug(\"Job %u: sent epilog complete msg: rc = %d\",\n\t\t\t      jobid, rc);\n\t\t}\n\t}\n\treturn ret;\n}\n\n\n/*\n * Send a signal through the appropriate slurmstepds for each job step\n * belonging to a given job allocation.\n */\nstatic void\n_rpc_signal_job(slurm_msg_t *msg)\n{\n\tsignal_job_msg_t *req = msg->data;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tuid_t job_uid;\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd = NULL;\n\tint step_cnt  = 0;\n\tint fd;\n\n\tdebug(\"_rpc_signal_job, uid = %d, signal = %d\", req_uid, req->signal);\n\tjob_uid = _get_job_uid(req->job_id);\n\tif ((int)job_uid < 0)\n\t\tgoto no_job;\n\n\t/*\n\t * check that requesting user ID is the SLURM UID or root\n\t */\n\tif ((req_uid != job_uid) && (!_slurm_authorized_user(req_uid))) {\n\t\terror(\"Security violation: kill_job(%u) from uid %d\",\n\t\t      req->job_id, req_uid);\n\t\tif (msg->conn_fd >= 0) {\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\t\terror (\"_rpc_signal_job: close(%d): %m\",\n\t\t\t\t       msg->conn_fd);\n\t\t\tmsg->conn_fd = -1;\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Loop through all job steps for this job and signal the\n\t * step's process group through the slurmstepd.\n\t */\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif (stepd->jobid != req->job_id) {\n\t\t\t/* multiple jobs expected on shared nodes */\n\t\t\tdebug3(\"Step from other job: jobid=%u (this jobid=%u)\",\n\t\t\t       stepd->jobid, req->job_id);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (stepd->stepid == SLURM_BATCH_SCRIPT) {\n\t\t\tdebug2(\"batch script itself not signalled\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tstep_cnt++;\n\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1) {\n\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdebug2(\"  signal %d to job %u.%u\",\n\t\t       req->signal, stepd->jobid, stepd->stepid);\n\t\tif (stepd_signal_container(\n\t\t\t    fd, stepd->protocol_version, req->signal) < 0)\n\t\t\tdebug(\"signal jobid=%u failed: %m\", stepd->jobid);\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\nno_job:\n\tif (step_cnt == 0) {\n\t\tdebug2(\"No steps in jobid %u to send signal %d\",\n\t\t       req->job_id, req->signal);\n\t}\n\n\t/*\n\t *  At this point, if connection still open, we send controller\n\t *   a \"success\" reply to indicate that we've recvd the msg.\n\t */\n\tif (msg->conn_fd >= 0) {\n\t\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\terror (\"_rpc_signal_job: close(%d): %m\", msg->conn_fd);\n\t\tmsg->conn_fd = -1;\n\t}\n}\n\n/* if a lock is granted to the job then return 1; else return 0 if\n * the lock for the job is already taken or there's no more locks */\nstatic int\n_get_suspend_job_lock(uint32_t job_id)\n{\n\tstatic bool logged = false;\n\tint i, empty_loc = -1, rc = 0;\n\n\tslurm_mutex_lock(&suspend_mutex);\n\tfor (i = 0; i < job_suspend_size; i++) {\n\t\tif (job_suspend_array[i] == 0) {\n\t\t\tempty_loc = i;\n\t\t\tcontinue;\n\t\t}\n\t\tif (job_suspend_array[i] == job_id) {\n\t\t\t/* another thread already a lock for this job ID */\n\t\t\tslurm_mutex_unlock(&suspend_mutex);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\tif (empty_loc != -1) {\n\t\t/* nobody has the lock and here's an available used lock */\n\t\tjob_suspend_array[empty_loc] = job_id;\n\t\trc = 1;\n\t} else if (job_suspend_size < NUM_PARALLEL_SUSP_JOBS) {\n\t\t/* a new lock is available */\n\t\tjob_suspend_array[job_suspend_size++] = job_id;\n\t\trc = 1;\n\t} else if (!logged) {\n\t\terror(\"Simultaneous job suspend/resume limit reached (%d). \"\n\t\t      \"Configure SchedulerTimeSlice higher.\",\n\t\t      NUM_PARALLEL_SUSP_JOBS);\n\t\tlogged = true;\n\t}\n\tslurm_mutex_unlock(&suspend_mutex);\n\treturn rc;\n}\n\nstatic void\n_unlock_suspend_job(uint32_t job_id)\n{\n\tint i;\n\tslurm_mutex_lock(&suspend_mutex);\n\tfor (i = 0; i < job_suspend_size; i++) {\n\t\tif (job_suspend_array[i] == job_id)\n\t\t\tjob_suspend_array[i] = 0;\n\t}\n\tslurm_mutex_unlock(&suspend_mutex);\n}\n\n/* Add record for every launched job so we know they are ready for suspend */\nextern void record_launched_jobs(void)\n{\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\t_launch_complete_add(stepd->jobid);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n}\n\n/*\n * Send a job suspend/resume request through the appropriate slurmstepds for\n * each job step belonging to a given job allocation.\n */\nstatic void\n_rpc_suspend_job(slurm_msg_t *msg)\n{\n\tint time_slice = -1;\n\tsuspend_int_msg_t *req = msg->data;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\tint step_cnt  = 0;\n\tint rc = SLURM_SUCCESS;\n\tDEF_TIMERS;\n\n\tif (time_slice == -1)\n\t\ttime_slice = slurm_get_time_slice();\n\tif ((req->op != SUSPEND_JOB) && (req->op != RESUME_JOB)) {\n\t\terror(\"REQUEST_SUSPEND_INT: bad op code %u\", req->op);\n\t\trc = ESLURM_NOT_SUPPORTED;\n\t}\n\n\t/*\n\t * check that requesting user ID is the SLURM UID or root\n\t */\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"Security violation: suspend_job(%u) from uid %d\",\n\t\t      req->job_id, req_uid);\n\t\trc =  ESLURM_USER_ID_MISSING;\n\t}\n\n\t/* send a response now, which will include any errors\n\t * detected with the request */\n\tif (msg->conn_fd >= 0) {\n\t\tslurm_send_rc_msg(msg, rc);\n\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\terror(\"_rpc_suspend_job: close(%d): %m\",\n\t\t\t      msg->conn_fd);\n\t\tmsg->conn_fd = -1;\n\t}\n\tif (rc != SLURM_SUCCESS)\n\t\treturn;\n\n\t/* now we can focus on performing the requested action,\n\t * which could take a few seconds to complete */\n\tdebug(\"_rpc_suspend_job jobid=%u uid=%d action=%s\", req->job_id,\n\t      req_uid, req->op == SUSPEND_JOB ? \"suspend\" : \"resume\");\n\n\t/* Try to get a thread lock for this job. If the lock\n\t * is not available then sleep and try again */\n\twhile (!_get_suspend_job_lock(req->job_id)) {\n\t\tdebug3(\"suspend lock sleep for %u\", req->job_id);\n\t\tusleep(10000);\n\t}\n\tSTART_TIMER;\n\n\t/* Defer suspend until job prolog and launch complete */\n\tif (req->op == SUSPEND_JOB)\n\t\t_launch_complete_wait(req->job_id);\n\n\tif ((req->op == SUSPEND_JOB) && (req->indf_susp))\n\t\tswitch_g_job_suspend(req->switch_info, 5);\n\n\t/* Release or reclaim resources bound to these tasks (task affinity) */\n\tif (req->op == SUSPEND_JOB) {\n\t\t(void) task_g_slurmd_suspend_job(req->job_id);\n\t} else {\n\t\t(void) task_g_slurmd_resume_job(req->job_id);\n\t}\n\n\t/*\n\t * Loop through all job steps and call stepd_suspend or stepd_resume\n\t * as appropriate. Since the \"suspend\" action may contains a sleep\n\t * (if the launch is in progress) suspend multiple jobsteps in parallel.\n\t */\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\n\twhile (1) {\n\t\tint x, fdi, fd[NUM_PARALLEL_SUSP_STEPS];\n\t\tuint16_t protocol_version[NUM_PARALLEL_SUSP_STEPS];\n\n\t\tfdi = 0;\n\t\twhile ((stepd = list_next(i))) {\n\t\t\tif (stepd->jobid != req->job_id) {\n\t\t\t\t/* multiple jobs expected on shared nodes */\n\t\t\t\tdebug3(\"Step from other job: jobid=%u \"\n\t\t\t\t       \"(this jobid=%u)\",\n\t\t\t\t       stepd->jobid, req->job_id);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tstep_cnt++;\n\n\t\t\tfd[fdi] = stepd_connect(stepd->directory,\n\t\t\t\t\t\tstepd->nodename, stepd->jobid,\n\t\t\t\t\t\tstepd->stepid,\n\t\t\t\t\t\t&protocol_version[fdi]);\n\t\t\tif (fd[fdi] == -1) {\n\t\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tfdi++;\n\t\t\tif (fdi >= NUM_PARALLEL_SUSP_STEPS)\n\t\t\t\tbreak;\n\t\t}\n\t\t/* check for open connections */\n\t\tif (fdi == 0)\n\t\t\tbreak;\n\n\t\tif (req->op == SUSPEND_JOB) {\n\t\t\tint susp_fail_count = 0;\n\t\t\t/* The suspend RPCs are processed in parallel for\n\t\t\t * every step in the job */\n\t\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t\t(void) stepd_suspend(fd[x],\n\t\t\t\t\t\t     protocol_version[x],\n\t\t\t\t\t\t     req, 0);\n\t\t\t}\n\t\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t\tif (stepd_suspend(fd[x],\n\t\t\t\t\t\t  protocol_version[x],\n\t\t\t\t\t\t  req, 1) < 0) {\n\t\t\t\t\tsusp_fail_count++;\n\t\t\t\t} else {\n\t\t\t\t\tclose(fd[x]);\n\t\t\t\t\tfd[x] = -1;\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Suspend RPCs can fail at step startup, so retry */\n\t\t\tif (susp_fail_count) {\n\t\t\t\tsleep(1);\n\t\t\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t\t\tif (fd[x] == -1)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t(void) stepd_suspend(\n\t\t\t\t\t\tfd[x],\n\t\t\t\t\t\tprotocol_version[x],\n\t\t\t\t\t\treq, 0);\n\t\t\t\t\tif (stepd_suspend(\n\t\t\t\t\t\t    fd[x],\n\t\t\t\t\t\t    protocol_version[x],\n\t\t\t\t\t\t    req, 1) >= 0)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tdebug(\"Suspend of job %u failed: %m\",\n\t\t\t\t\t      req->job_id);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t/* The resume RPCs are processed in parallel for\n\t\t\t * every step in the job */\n\t\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t\t(void) stepd_resume(fd[x],\n\t\t\t\t\t\t    protocol_version[x],\n\t\t\t\t\t\t    req, 0);\n\t\t\t}\n\t\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t\tif (stepd_resume(fd[x],\n\t\t\t\t\t\t protocol_version[x],\n\t\t\t\t\t\t req, 1) < 0) {\n\t\t\t\t\tdebug(\"Resume of job %u failed: %m\",\n\t\t\t\t\t      req->job_id);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t/* fd may have been closed by stepd_suspend */\n\t\t\tif (fd[x] != -1)\n\t\t\t\tclose(fd[x]);\n\t\t}\n\n\t\t/* check for no more jobs */\n\t\tif (fdi < NUM_PARALLEL_SUSP_STEPS)\n\t\t\tbreak;\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\tif ((req->op == RESUME_JOB) && (req->indf_susp))\n\t\tswitch_g_job_resume(req->switch_info, 5);\n\n\t_unlock_suspend_job(req->job_id);\n\n\tEND_TIMER;\n\tif (DELTA_TIMER >= (time_slice * 1000000)) {\n\t\tif (req->op == SUSPEND_JOB) {\n\t\t\tinfo(\"Suspend time for job_id %u was %s. \"\n\t\t\t     \"Configure SchedulerTimeSlice higher.\",\n\t\t\t     req->job_id, TIME_STR);\n\t\t} else {\n\t\t\tinfo(\"Resume time for job_id %u was %s. \"\n\t\t\t     \"Configure SchedulerTimeSlice higher.\",\n\t\t\t     req->job_id, TIME_STR);\n\t\t}\n\t}\n\n\tif (step_cnt == 0) {\n\t\tdebug2(\"No steps in jobid %u to suspend/resume\", req->job_id);\n\t}\n}\n\n/* Job shouldn't even be running here, abort it immediately */\nstatic void\n_rpc_abort_job(slurm_msg_t *msg)\n{\n\tkill_job_msg_t *req    = msg->data;\n\tuid_t           uid    = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t      conf->auth_info);\n\tjob_env_t       job_env;\n\n\tdebug(\"_rpc_abort_job, uid = %d\", uid);\n\t/*\n\t * check that requesting user ID is the SLURM UID\n\t */\n\tif (!_slurm_authorized_user(uid)) {\n\t\terror(\"Security violation: abort_job(%u) from uid %d\",\n\t\t      req->job_id, uid);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\treturn;\n\t}\n\n\ttask_g_slurmd_release_resources(req->job_id);\n\n\t/*\n\t * \"revoke\" all future credentials for this jobid\n\t */\n\tif (slurm_cred_revoke(conf->vctx, req->job_id, req->time,\n\t\t\t      req->start_time) < 0) {\n\t\tdebug(\"revoking cred for job %u: %m\", req->job_id);\n\t} else {\n\t\tsave_cred_state(conf->vctx);\n\t\tdebug(\"credential for job %u revoked\", req->job_id);\n\t}\n\n\t/*\n\t *  At this point, if connection still open, we send controller\n\t *   a \"success\" reply to indicate that we've recvd the msg.\n\t */\n\tif (msg->conn_fd >= 0) {\n\t\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\terror (\"rpc_abort_job: close(%d): %m\", msg->conn_fd);\n\t\tmsg->conn_fd = -1;\n\t}\n\n\tif (_kill_all_active_steps(req->job_id, SIG_ABORT, true)) {\n\t\t/*\n\t\t *  Block until all user processes are complete.\n\t\t */\n\t\t_pause_for_job_completion (req->job_id, req->nodes, 0);\n\t}\n\n\t/*\n\t *  Begin expiration period for cached information about job.\n\t *   If expiration period has already begun, then do not run\n\t *   the epilog again, as that script has already been executed\n\t *   for this job.\n\t */\n\tif (slurm_cred_begin_expiration(conf->vctx, req->job_id) < 0) {\n\t\tdebug(\"Not running epilog for jobid %d: %m\", req->job_id);\n\t\treturn;\n\t}\n\n\tsave_cred_state(conf->vctx);\n\n\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\tjob_env.jobid = req->job_id;\n\tjob_env.node_list = req->nodes;\n\tjob_env.spank_job_env = req->spank_job_env;\n\tjob_env.spank_job_env_size = req->spank_job_env_size;\n\tjob_env.uid = req->job_uid;\n\n#if defined(HAVE_BG)\n\tselect_g_select_jobinfo_get(req->select_jobinfo,\n\t\t\t\t    SELECT_JOBDATA_BLOCK_ID,\n\t\t\t\t    &job_env.resv_id);\n#elif defined(HAVE_ALPS_CRAY)\n\tjob_env.resv_id = select_g_select_jobinfo_xstrdup(req->select_jobinfo,\n\t\t\t\t\t\t\t  SELECT_PRINT_RESV_ID);\n#endif\n\n\t_run_epilog(&job_env);\n\n\tif (container_g_delete(req->job_id))\n\t\terror(\"container_g_delete(%u): %m\", req->job_id);\n\t_launch_complete_rm(req->job_id);\n\n\txfree(job_env.resv_id);\n}\n\n/* This is a variant of _rpc_terminate_job for use with select/serial */\nstatic void\n_rpc_terminate_batch_job(uint32_t job_id, uint32_t user_id, char *node_name)\n{\n\tint             rc     = SLURM_SUCCESS;\n\tint             nsteps = 0;\n\tint\t\tdelay;\n\ttime_t\t\tnow = time(NULL);\n\tslurm_ctl_conf_t *cf;\n\tjob_env_t job_env;\n\n\ttask_g_slurmd_release_resources(job_id);\n\n\tif (_waiter_init(job_id) == SLURM_ERROR)\n\t\treturn;\n\n\t/*\n\t * \"revoke\" all future credentials for this jobid\n\t */\n\t_note_batch_job_finished(job_id);\n\tif (slurm_cred_revoke(conf->vctx, job_id, now, now) < 0) {\n\t\tdebug(\"revoking cred for job %u: %m\", job_id);\n\t} else {\n\t\tsave_cred_state(conf->vctx);\n\t\tdebug(\"credential for job %u revoked\", job_id);\n\t}\n\n\t/*\n\t * Tasks might be stopped (possibly by a debugger)\n\t * so send SIGCONT first.\n\t */\n\t_kill_all_active_steps(job_id, SIGCONT, true);\n\tif (errno == ESLURMD_STEP_SUSPENDED) {\n\t\t/*\n\t\t * If the job step is currently suspended, we don't\n\t\t * bother with a \"nice\" termination.\n\t\t */\n\t\tdebug2(\"Job is currently suspended, terminating\");\n\t\tnsteps = _terminate_all_steps(job_id, true);\n\t} else {\n\t\tnsteps = _kill_all_active_steps(job_id, SIGTERM, true);\n\t}\n\n#ifndef HAVE_AIX\n\tif ((nsteps == 0) && !conf->epilog) {\n\t\tslurm_cred_begin_expiration(conf->vctx, job_id);\n\t\tsave_cred_state(conf->vctx);\n\t\t_waiter_complete(job_id);\n\t\tif (container_g_delete(job_id))\n\t\t\terror(\"container_g_delete(%u): %m\", job_id);\n\t\t_launch_complete_rm(job_id);\n\t\treturn;\n\t}\n#endif\n\n\t/*\n\t *  Check for corpses\n\t */\n\tcf = slurm_conf_lock();\n\tdelay = MAX(cf->kill_wait, 5);\n\tslurm_conf_unlock();\n\tif (!_pause_for_job_completion(job_id, NULL, delay) &&\n\t     _terminate_all_steps(job_id, true) ) {\n\t\t/*\n\t\t *  Block until all user processes are complete.\n\t\t */\n\t\t_pause_for_job_completion(job_id, NULL, 0);\n\t}\n\n\t/*\n\t *  Begin expiration period for cached information about job.\n\t *   If expiration period has already begun, then do not run\n\t *   the epilog again, as that script has already been executed\n\t *   for this job.\n\t */\n\tif (slurm_cred_begin_expiration(conf->vctx, job_id) < 0) {\n\t\tdebug(\"Not running epilog for jobid %d: %m\", job_id);\n\t\tgoto done;\n\t}\n\n\tsave_cred_state(conf->vctx);\n\n\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\tjob_env.jobid = job_id;\n\tjob_env.node_list = node_name;\n\tjob_env.uid = (uid_t)user_id;\n\t/* NOTE: We lack the job's SPANK environment variables */\n\trc = _run_epilog(&job_env);\n\tif (rc) {\n\t\tint term_sig, exit_status;\n\t\tif (WIFSIGNALED(rc)) {\n\t\t\texit_status = 0;\n\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t} else {\n\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\tterm_sig    = 0;\n\t\t}\n\t\terror(\"[job %u] epilog failed status=%d:%d\",\n\t\t      job_id, exit_status, term_sig);\n\t} else\n\t\tdebug(\"completed epilog for jobid %u\", job_id);\n\tif (container_g_delete(job_id))\n\t\terror(\"container_g_delete(%u): %m\", job_id);\n\t_launch_complete_rm(job_id);\n\n    done:\n\t_wait_state_completed(job_id, 5);\n\t_waiter_complete(job_id);\n}\n\nstatic void _handle_old_batch_job_launch(slurm_msg_t *msg)\n{\n\tif (msg->msg_type != REQUEST_BATCH_JOB_LAUNCH) {\n\t\terror(\"_handle_batch_job_launch: \"\n\t\t      \"Invalid response msg_type (%u)\", msg->msg_type);\n\t\treturn;\n\t}\n\n\t/* (resp_msg.msg_type == REQUEST_BATCH_JOB_LAUNCH) */\n\tdebug2(\"Processing RPC: REQUEST_BATCH_JOB_LAUNCH\");\n\tlast_slurmctld_msg = time(NULL);\n\t_rpc_batch_job(msg, false);\n\tslurm_free_job_launch_msg(msg->data);\n\tmsg->data = NULL;\n\n}\n\n/* This complete batch RPC came from slurmstepd because we have select/serial\n * configured. Terminate the job here. Forward the batch completion RPC to\n * slurmctld and possible get a new batch launch RPC in response. */\nstatic void\n_rpc_complete_batch(slurm_msg_t *msg)\n{\n\tint\t\ti, rc, msg_rc;\n\tslurm_msg_t\tresp_msg;\n\tuid_t           uid    = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t      conf->auth_info);\n\tcomplete_batch_script_msg_t *req = msg->data;\n\tstatic int\trunning_serial = -1;\n\tuint16_t msg_type;\n\n\tif (running_serial == -1) {\n\t\tchar *select_type = slurm_get_select_type();\n\t\tif (!xstrcmp(select_type, \"select/serial\"))\n\t\t\trunning_serial = 1;\n\t\telse\n\t\t\trunning_serial = 0;\n\t\txfree(select_type);\n\t}\n\n\tif (!_slurm_authorized_user(uid)) {\n\t\terror(\"Security violation: complete_batch(%u) from uid %d\",\n\t\t      req->job_id, uid);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\treturn;\n\t}\n\n\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\n\tif (running_serial) {\n\t\t_rpc_terminate_batch_job(\n\t\t\treq->job_id, req->user_id, req->node_name);\n\t\tmsg_type = REQUEST_COMPLETE_BATCH_JOB;\n\t} else\n\t\tmsg_type = msg->msg_type;\n\n\tfor (i = 0; i <= MAX_RETRY; i++) {\n\t\tif (conf->msg_aggr_window_msgs > 1) {\n\t\t\tslurm_msg_t *req_msg =\n\t\t\t\txmalloc_nz(sizeof(slurm_msg_t));\n\t\t\tslurm_msg_t_init(req_msg);\n\t\t\treq_msg->msg_type = msg_type;\n\t\t\treq_msg->data = msg->data;\n\t\t\tmsg->data = NULL;\n\n\t\t\tmsg_aggr_add_msg(req_msg, 1,\n\t\t\t\t\t _handle_old_batch_job_launch);\n\t\t\treturn;\n\t\t} else {\n\t\t\tslurm_msg_t req_msg;\n\t\t\tslurm_msg_t_init(&req_msg);\n\t\t\treq_msg.msg_type = msg_type;\n\t\t\treq_msg.data\t = msg->data;\n\t\t\tmsg_rc = slurm_send_recv_controller_msg(\n\t\t\t\t&req_msg, &resp_msg);\n\n\t\t\tif (msg_rc == SLURM_SUCCESS)\n\t\t\t\tbreak;\n\t\t}\n\t\tinfo(\"Retrying job complete RPC for job %u\", req->job_id);\n\t\tsleep(RETRY_DELAY);\n\t}\n\tif (i > MAX_RETRY) {\n\t\terror(\"Unable to send job complete message: %m\");\n\t\treturn;\n\t}\n\n\tif (resp_msg.msg_type == RESPONSE_SLURM_RC) {\n\t\tlast_slurmctld_msg = time(NULL);\n\t\trc = ((return_code_msg_t *) resp_msg.data)->return_code;\n\t\tslurm_free_return_code_msg(resp_msg.data);\n\t\tif (rc) {\n\t\t\terror(\"complete_batch for job %u: %s\", req->job_id,\n\t\t\t      slurm_strerror(rc));\n\t\t}\n\t\treturn;\n\t}\n\n\t_handle_old_batch_job_launch(&resp_msg);\n}\n\nstatic void\n_rpc_terminate_job(slurm_msg_t *msg)\n{\n#ifndef HAVE_AIX\n\tbool\t\thave_spank = false;\n#endif\n\tint             rc     = SLURM_SUCCESS;\n\tkill_job_msg_t *req    = msg->data;\n\tuid_t           uid    = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t      conf->auth_info);\n\tint             nsteps = 0;\n\tint\t\tdelay;\n//\tslurm_ctl_conf_t *cf;\n//\tstruct stat\tstat_buf;\n\tjob_env_t       job_env;\n\n\tdebug(\"_rpc_terminate_job, uid = %d\", uid);\n\t/*\n\t * check that requesting user ID is the SLURM UID\n\t */\n\tif (!_slurm_authorized_user(uid)) {\n\t\terror(\"Security violation: kill_job(%u) from uid %d\",\n\t\t      req->job_id, uid);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\treturn;\n\t}\n\n\ttask_g_slurmd_release_resources(req->job_id);\n\n\t/*\n\t *  Initialize a \"waiter\" thread for this jobid. If another\n\t *   thread is already waiting on termination of this job,\n\t *   _waiter_init() will return SLURM_ERROR. In this case, just\n\t *   notify slurmctld that we recvd the message successfully,\n\t *   then exit this thread.\n\t */\n\tif (_waiter_init(req->job_id) == SLURM_ERROR) {\n\t\tif (msg->conn_fd >= 0) {\n\t\t\t/* No matter if the step hasn't started yet or\n\t\t\t * not just send a success to let the\n\t\t\t * controller know we got this request.\n\t\t\t */\n\t\t\tslurm_send_rc_msg (msg, SLURM_SUCCESS);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * \"revoke\" all future credentials for this jobid\n\t */\n\tif (slurm_cred_revoke(conf->vctx, req->job_id, req->time,\n\t\t\t      req->start_time) < 0) {\n\t\tdebug(\"revoking cred for job %u: %m\", req->job_id);\n\t} else {\n\t\tsave_cred_state(conf->vctx);\n\t\tdebug(\"credential for job %u revoked\", req->job_id);\n\t}\n\n\t/*\n\t * Before signalling steps, if the job has any steps that are still\n\t * in the process of fork/exec/check in with slurmd, wait on a condition\n\t * var for the start.  Otherwise a slow-starting step can miss the\n\t * job termination message and run indefinitely.\n\t */\n\tif (_step_is_starting(req->job_id, NO_VAL)) {\n\t\tif (msg->conn_fd >= 0) {\n\t\t\t/* If the step hasn't started yet just send a\n\t\t\t * success to let the controller know we got\n\t\t\t * this request.\n\t\t\t */\n\t\t\tdebug(\"sent SUCCESS, waiting for step to start\");\n\t\t\tslurm_send_rc_msg (msg, SLURM_SUCCESS);\n\t\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\t\terror ( \"rpc_kill_job: close(%d): %m\",\n\t\t\t\t\tmsg->conn_fd);\n\t\t\tmsg->conn_fd = -1;\n\t\t}\n\t\tif (_wait_for_starting_step(req->job_id, NO_VAL)) {\n\t\t\t/*\n\t\t\t * There's currently no case in which we enter this\n\t\t\t * error condition.  If there was, it's hard to say\n\t\t\t * whether to to proceed with the job termination.\n\t\t\t */\n\t\t\terror(\"Error in _wait_for_starting_step\");\n\t\t}\n\t}\n\tif (IS_JOB_NODE_FAILED(req))\n\t\t_kill_all_active_steps(req->job_id, SIG_NODE_FAIL, true);\n\tif (IS_JOB_PENDING(req))\n\t\t_kill_all_active_steps(req->job_id, SIG_REQUEUED, true);\n\telse if (IS_JOB_FAILED(req))\n\t\t_kill_all_active_steps(req->job_id, SIG_FAILURE, true);\n\n\t/*\n\t * Tasks might be stopped (possibly by a debugger)\n\t * so send SIGCONT first.\n\t */\n\t_kill_all_active_steps(req->job_id, SIGCONT, true);\n\tif (errno == ESLURMD_STEP_SUSPENDED) {\n\t\t/*\n\t\t * If the job step is currently suspended, we don't\n\t\t * bother with a \"nice\" termination.\n\t\t */\n\t\tdebug2(\"Job is currently suspended, terminating\");\n\t\tnsteps = _terminate_all_steps(req->job_id, true);\n\t} else {\n\t\tnsteps = _kill_all_active_steps(req->job_id, SIGTERM, true);\n\t}\n\n#ifndef HAVE_AIX\n\tif ((nsteps == 0) && !conf->epilog) {\n\t\tstruct stat stat_buf;\n\t\tif (conf->plugstack && (stat(conf->plugstack, &stat_buf) == 0))\n\t\t\thave_spank = true;\n\t}\n\t/*\n\t *  If there are currently no active job steps and no\n\t *    configured epilog to run, bypass asynchronous reply and\n\t *    notify slurmctld that we have already completed this\n\t *    request. We need to send current switch state on AIX\n\t *    systems, so this bypass can not be used.\n\t */\n\tif ((nsteps == 0) && !conf->epilog && !have_spank) {\n\t\tdebug4(\"sent ALREADY_COMPLETE\");\n\t\tif (msg->conn_fd >= 0) {\n\t\t\tslurm_send_rc_msg(msg,\n\t\t\t\t\t  ESLURMD_KILL_JOB_ALREADY_COMPLETE);\n\t\t}\n\t\tslurm_cred_begin_expiration(conf->vctx, req->job_id);\n\t\tsave_cred_state(conf->vctx);\n\t\t_waiter_complete(req->job_id);\n\n\t\t/*\n\t\t * The controller needs to get MESSAGE_EPILOG_COMPLETE to bring\n\t\t * the job out of \"completing\" state.  Otherwise, the job\n\t\t * could remain \"completing\" unnecessarily, until the request\n\t\t * to terminate is resent.\n\t\t */\n\t\t_sync_messages_kill(req);\n\t\tif (msg->conn_fd < 0) {\n\t\t\t/* The epilog complete message processing on\n\t\t\t * slurmctld is equivalent to that of a\n\t\t\t * ESLURMD_KILL_JOB_ALREADY_COMPLETE reply above */\n\t\t\t_epilog_complete(req->job_id, rc);\n\t\t}\n\t\tif (container_g_delete(req->job_id))\n\t\t\terror(\"container_g_delete(%u): %m\", req->job_id);\n\t\t_launch_complete_rm(req->job_id);\n\t\treturn;\n\t}\n#endif\n\n\t/*\n\t *  At this point, if connection still open, we send controller\n\t *   a \"success\" reply to indicate that we've recvd the msg.\n\t */\n\tif (msg->conn_fd >= 0) {\n\t\tdebug4(\"sent SUCCESS\");\n\t\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\terror (\"rpc_kill_job: close(%d): %m\", msg->conn_fd);\n\t\tmsg->conn_fd = -1;\n\t}\n\n\t/*\n\t *  Check for corpses\n\t */\n\tdelay = MAX(conf->kill_wait, 5);\n\tif ( !_pause_for_job_completion (req->job_id, req->nodes, delay) &&\n\t     _terminate_all_steps(req->job_id, true) ) {\n\t\t/*\n\t\t *  Block until all user processes are complete.\n\t\t */\n\t\t_pause_for_job_completion (req->job_id, req->nodes, 0);\n\t}\n\n\t/*\n\t *  Begin expiration period for cached information about job.\n\t *   If expiration period has already begun, then do not run\n\t *   the epilog again, as that script has already been executed\n\t *   for this job.\n\t */\n\tif (slurm_cred_begin_expiration(conf->vctx, req->job_id) < 0) {\n\t\tdebug(\"Not running epilog for jobid %d: %m\", req->job_id);\n\t\tgoto done;\n\t}\n\n\tsave_cred_state(conf->vctx);\n\n\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\tjob_env.jobid = req->job_id;\n\tjob_env.node_list = req->nodes;\n\tjob_env.spank_job_env = req->spank_job_env;\n\tjob_env.spank_job_env_size = req->spank_job_env_size;\n\tjob_env.uid = req->job_uid;\n\n#if defined(HAVE_BG)\n\tselect_g_select_jobinfo_get(req->select_jobinfo,\n\t\t\t\t    SELECT_JOBDATA_BLOCK_ID,\n\t\t\t\t    &job_env.resv_id);\n#elif defined(HAVE_ALPS_CRAY)\n\tjob_env.resv_id = select_g_select_jobinfo_xstrdup(\n\t\treq->select_jobinfo, SELECT_PRINT_RESV_ID);\n#endif\n\trc = _run_epilog(&job_env);\n\txfree(job_env.resv_id);\n\n\tif (rc) {\n\t\tint term_sig, exit_status;\n\t\tif (WIFSIGNALED(rc)) {\n\t\t\texit_status = 0;\n\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t} else {\n\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\tterm_sig    = 0;\n\t\t}\n\t\terror(\"[job %u] epilog failed status=%d:%d\",\n\t\t      req->job_id, exit_status, term_sig);\n\t\trc = ESLURMD_EPILOG_FAILED;\n\t} else\n\t\tdebug(\"completed epilog for jobid %u\", req->job_id);\n\tif (container_g_delete(req->job_id))\n\t\terror(\"container_g_delete(%u): %m\", req->job_id);\n\t_launch_complete_rm(req->job_id);\n\n    done:\n\t_wait_state_completed(req->job_id, 5);\n\t_waiter_complete(req->job_id);\n\t_sync_messages_kill(req);\n\n\t_epilog_complete(req->job_id, rc);\n}\n\n/* On a parallel job, every slurmd may send the EPILOG_COMPLETE\n * message to the slurmctld at the same time, resulting in lost\n * messages. We add a delay here to spead out the message traffic\n * assuming synchronized clocks across the cluster.\n * Allow 10 msec processing time in slurmctld for each RPC. */\nstatic void _sync_messages_kill(kill_job_msg_t *req)\n{\n\tint host_cnt, host_inx;\n\tchar *host;\n\thostset_t hosts;\n\tint epilog_msg_time;\n\n\thosts = hostset_create(req->nodes);\n\thost_cnt = hostset_count(hosts);\n\tif (host_cnt <= 64)\n\t\tgoto fini;\n\tif (conf->hostname == NULL)\n\t\tgoto fini;\t/* should never happen */\n\n\tfor (host_inx=0; host_inx<host_cnt; host_inx++) {\n\t\thost = hostset_shift(hosts);\n\t\tif (host == NULL)\n\t\t\tbreak;\n\t\tif (xstrcmp(host, conf->node_name) == 0) {\n\t\t\tfree(host);\n\t\t\tbreak;\n\t\t}\n\t\tfree(host);\n\t}\n\tepilog_msg_time = slurm_get_epilog_msg_time();\n\t_delay_rpc(host_inx, host_cnt, epilog_msg_time);\n\n fini:\thostset_destroy(hosts);\n}\n\n/* Delay a message based upon the host index, total host count and RPC_TIME.\n * This logic depends upon synchronized clocks across the cluster. */\nstatic void _delay_rpc(int host_inx, int host_cnt, int usec_per_rpc)\n{\n\tstruct timeval tv1;\n\tuint32_t cur_time;\t/* current time in usec (just 9 digits) */\n\tuint32_t tot_time;\t/* total time expected for all RPCs */\n\tuint32_t offset_time;\t/* relative time within tot_time */\n\tuint32_t target_time;\t/* desired time to issue the RPC */\n\tuint32_t delta_time;\n\nagain:\tif (gettimeofday(&tv1, NULL)) {\n\t\tusleep(host_inx * usec_per_rpc);\n\t\treturn;\n\t}\n\n\tcur_time = ((tv1.tv_sec % 1000) * 1000000) + tv1.tv_usec;\n\ttot_time = host_cnt * usec_per_rpc;\n\toffset_time = cur_time % tot_time;\n\ttarget_time = host_inx * usec_per_rpc;\n\tif (target_time < offset_time)\n\t\tdelta_time = target_time - offset_time + tot_time;\n\telse\n\t\tdelta_time = target_time - offset_time;\n\tif (usleep(delta_time)) {\n\t\tif (errno == EINVAL) /* usleep for more than 1 sec */\n\t\t\tusleep(900000);\n\t\t/* errno == EINTR */\n\t\tgoto again;\n\t}\n}\n\n/*\n *  Returns true if \"uid\" is a \"slurm authorized user\" - i.e. uid == 0\n *   or uid == slurm user id at this time.\n */\nstatic bool\n_slurm_authorized_user(uid_t uid)\n{\n\treturn ((uid == (uid_t) 0) || (uid == conf->slurm_user_id));\n}\n\n\nstruct waiter {\n\tuint32_t jobid;\n\tpthread_t thd;\n};\n\n\nstatic struct waiter *\n_waiter_create(uint32_t jobid)\n{\n\tstruct waiter *wp = xmalloc(sizeof(struct waiter));\n\n\twp->jobid = jobid;\n\twp->thd   = pthread_self();\n\n\treturn wp;\n}\n\nstatic int _find_waiter(struct waiter *w, uint32_t *jp)\n{\n\treturn (w->jobid == *jp);\n}\n\nstatic void _waiter_destroy(struct waiter *wp)\n{\n\txfree(wp);\n}\n\nstatic int _waiter_init (uint32_t jobid)\n{\n\tif (!waiters)\n\t\twaiters = list_create((ListDelF) _waiter_destroy);\n\n\t/*\n\t *  Exit this thread if another thread is waiting on job\n\t */\n\tif (list_find_first (waiters, (ListFindF) _find_waiter, &jobid))\n\t\treturn SLURM_ERROR;\n\telse\n\t\tlist_append(waiters, _waiter_create(jobid));\n\n\treturn (SLURM_SUCCESS);\n}\n\nstatic int _waiter_complete (uint32_t jobid)\n{\n\treturn (list_delete_all (waiters, (ListFindF) _find_waiter, &jobid));\n}\n\n/*\n *  Like _wait_for_procs(), but only wait for up to max_time seconds\n *  if max_time == 0, send SIGKILL to tasks repeatedly\n *\n *  Returns true if all job processes are gone\n */\nstatic bool\n_pause_for_job_completion (uint32_t job_id, char *nodes, int max_time)\n{\n\tint sec = 0;\n\tint pause = 1;\n\tbool rc = false;\n\n\twhile ((sec < max_time) || (max_time == 0)) {\n\t\trc = _job_still_running (job_id);\n\t\tif (!rc)\n\t\t\tbreak;\n\t\tif ((max_time == 0) && (sec > 1)) {\n\t\t\t_terminate_all_steps(job_id, true);\n\t\t}\n\t\tif (sec > 10) {\n\t\t\t/* Reduce logging frequency about unkillable tasks */\n\t\t\tif (max_time)\n\t\t\t\tpause = MIN((max_time - sec), 10);\n\t\t\telse\n\t\t\t\tpause = 10;\n\t\t}\n\t\tsleep(pause);\n\t\tsec += pause;\n\t}\n\n\t/*\n\t * Return true if job is NOT running\n\t */\n\treturn (!rc);\n}\n\n/*\n * Does nothing and returns SLURM_SUCCESS (if uid authenticates).\n *\n * Timelimit is not currently used in the slurmd or slurmstepd.\n */\nstatic void\n_rpc_update_time(slurm_msg_t *msg)\n{\n\tint   rc      = SLURM_SUCCESS;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\n\tif ((req_uid != conf->slurm_user_id) && (req_uid != 0)) {\n\t\trc = ESLURM_USER_ID_MISSING;\n\t\terror(\"Security violation, uid %d can't update time limit\",\n\t\t      req_uid);\n\t\tgoto done;\n\t}\n\n/* \tif (shm_update_job_timelimit(req->job_id, req->expiration_time) < 0) { */\n/* \t\terror(\"updating lifetime for job %u: %m\", req->job_id); */\n/* \t\trc = ESLURM_INVALID_JOB_ID; */\n/* \t} else */\n/* \t\tdebug(\"reset job %u lifetime\", req->job_id); */\n\n    done:\n\tslurm_send_rc_msg(msg, rc);\n}\n\n/* NOTE: call _destroy_env() to free returned value */\nstatic char **\n_build_env(job_env_t *job_env)\n{\n\tchar **env = xmalloc(sizeof(char *));\n\tbool user_name_set = 0;\n\n\tenv[0]  = NULL;\n\tif (!valid_spank_job_env(job_env->spank_job_env,\n\t\t\t\t job_env->spank_job_env_size,\n\t\t\t\t job_env->uid)) {\n\t\t/* If SPANK job environment is bad, log it and do not use */\n\t\tjob_env->spank_job_env_size = 0;\n\t\tjob_env->spank_job_env = (char **) NULL;\n\t}\n\tif (job_env->spank_job_env_size) {\n\t\tenv_array_merge_spank(&env,\n\t\t\t\t      (const char **) job_env->spank_job_env);\n\t}\n\n\tslurm_mutex_lock(&conf->config_mutex);\n\tsetenvf(&env, \"SLURMD_NODENAME\", \"%s\", conf->node_name);\n\tsetenvf(&env, \"SLURM_CONF\", conf->conffile);\n\tslurm_mutex_unlock(&conf->config_mutex);\n\n\tsetenvf(&env, \"SLURM_CLUSTER_NAME\", \"%s\", conf->cluster_name);\n\tsetenvf(&env, \"SLURM_JOB_ID\", \"%u\", job_env->jobid);\n\tsetenvf(&env, \"SLURM_JOB_UID\",   \"%u\", job_env->uid);\n\n#ifndef HAVE_NATIVE_CRAY\n\t/* uid_to_string on a cray is a heavy call, so try to avoid it */\n\tif (!job_env->user_name) {\n\t\tjob_env->user_name = uid_to_string(job_env->uid);\n\t\tuser_name_set = 1;\n\t}\n#endif\n\n\tsetenvf(&env, \"SLURM_JOB_USER\", \"%s\", job_env->user_name);\n\tif (user_name_set)\n\t\txfree(job_env->user_name);\n\n\tsetenvf(&env, \"SLURM_JOBID\", \"%u\", job_env->jobid);\n\tsetenvf(&env, \"SLURM_UID\",   \"%u\", job_env->uid);\n\tif (job_env->node_list)\n\t\tsetenvf(&env, \"SLURM_NODELIST\", \"%s\", job_env->node_list);\n\n\tif (job_env->partition)\n\t\tsetenvf(&env, \"SLURM_JOB_PARTITION\", \"%s\", job_env->partition);\n\n\tif (job_env->resv_id) {\n#if defined(HAVE_BG)\n\t\tsetenvf(&env, \"MPIRUN_PARTITION\", \"%s\", job_env->resv_id);\n# ifdef HAVE_BGP\n\t\t/* Needed for HTC jobs */\n\t\tsetenvf(&env, \"SUBMIT_POOL\", \"%s\", job_env->resv_id);\n# endif\n#elif defined(HAVE_ALPS_CRAY)\n\t\tsetenvf(&env, \"BASIL_RESERVATION_ID\", \"%s\", job_env->resv_id);\n#endif\n\t}\n\treturn env;\n}\n\nstatic void\n_destroy_env(char **env)\n{\n\tint i=0;\n\n\tif (env) {\n\t\tfor(i=0; env[i]; i++) {\n\t\t\txfree(env[i]);\n\t\t}\n\t\txfree(env);\n\t}\n\n\treturn;\n}\n\n/* Trigger srun of spank prolog or epilog in slurmstepd */\nstatic int\n_run_spank_job_script (const char *mode, char **env, uint32_t job_id, uid_t uid)\n{\n\tpid_t cpid;\n\tint status = 0, timeout;\n\tint pfds[2];\n\n\tif (pipe (pfds) < 0) {\n\t\terror (\"_run_spank_job_script: pipe: %m\");\n\t\treturn (-1);\n\t}\n\n\tfd_set_close_on_exec (pfds[1]);\n\n\tdebug (\"Calling %s spank %s\", conf->stepd_loc, mode);\n\tif ((cpid = fork ()) < 0) {\n\t\terror (\"executing spank %s: %m\", mode);\n\t\treturn (-1);\n\t}\n\tif (cpid == 0) {\n\t\t/* Run slurmstepd spank [prolog|epilog] */\n\t\tchar *argv[4] = {\n\t\t\t(char *) conf->stepd_loc,\n\t\t\t\"spank\",\n\t\t\t(char *) mode,\n\t\t\tNULL };\n\n\t\t/* container_g_add_pid needs to be called in the\n\t\t   forked process part of the fork to avoid a race\n\t\t   condition where if this process makes a file or\n\t\t   detacts itself from a child before we add the pid\n\t\t   to the container in the parent of the fork.\n\t\t*/\n\t\tif (container_g_add_pid(job_id, getpid(), getuid())\n\t\t    != SLURM_SUCCESS)\n\t\t\terror(\"container_g_add_pid(%u): %m\", job_id);\n\n\t\tif (dup2 (pfds[0], STDIN_FILENO) < 0)\n\t\t\tfatal (\"dup2: %m\");\n#ifdef SETPGRP_TWO_ARGS\n\t\tsetpgrp(0, 0);\n#else\n\t\tsetpgrp();\n#endif\n\t\tif (conf->chos_loc && !access(conf->chos_loc, X_OK))\n\t\t\texecve(conf->chos_loc, argv, env);\n\t\telse\n\t\t\texecve(argv[0], argv, env);\n\t\terror (\"execve(%s): %m\", argv[0]);\n\t\texit (127);\n\t}\n\n\tclose (pfds[0]);\n\n\tif (_send_slurmd_conf_lite (pfds[1], conf) < 0)\n\t\terror (\"Failed to send slurmd conf to slurmstepd\\n\");\n\tclose (pfds[1]);\n\n\ttimeout = MAX(slurm_get_prolog_timeout(), 120); /* 120 secs in v15.08 */\n\tif (waitpid_timeout (mode, cpid, &status, timeout) < 0) {\n\t\terror (\"spank/%s timed out after %u secs\", mode, timeout);\n\t\treturn (-1);\n\t}\n\n\tif (status)\n\t\terror (\"spank/%s returned status 0x%04x\", mode, status);\n\n\t/*\n\t *  No longer need SPANK option env vars in environment\n\t */\n\tspank_clear_remote_options_env (env);\n\n\treturn (status);\n}\n\nstatic int _run_job_script(const char *name, const char *path,\n\t\t\t   uint32_t jobid, int timeout, char **env, uid_t uid)\n{\n\tstruct stat stat_buf;\n\tint status = 0, rc;\n\n\t/*\n\t *  Always run both spank prolog/epilog and real prolog/epilog script,\n\t *   even if spank plugins fail. (May want to alter this in the future)\n\t *   If both \"script\" mechanisms fail, prefer to return the \"real\"\n\t *   prolog/epilog status.\n\t */\n\tif (conf->plugstack && (stat(conf->plugstack, &stat_buf) == 0))\n\t\tstatus = _run_spank_job_script(name, env, jobid, uid);\n\tif ((rc = run_script(name, path, jobid, timeout, env, uid)))\n\t\tstatus = rc;\n\treturn (status);\n}\n\n#ifdef HAVE_BG\n/* a slow prolog is expected on bluegene systems */\nstatic int\n_run_prolog(job_env_t *job_env, slurm_cred_t *cred)\n{\n\tint rc;\n\tchar *my_prolog;\n\tchar **my_env;\n\n\tmy_env = _build_env(job_env);\n\tsetenvf(&my_env, \"SLURM_STEP_ID\", \"%u\", job_env->step_id);\n\n\tslurm_mutex_lock(&conf->config_mutex);\n\tmy_prolog = xstrdup(conf->prolog);\n\tslurm_mutex_unlock(&conf->config_mutex);\n\n\trc = _run_job_script(\"prolog\", my_prolog, job_env->jobid,\n\t\t\t     -1, my_env, job_env->uid);\n\t_remove_job_running_prolog(job_env->jobid);\n\txfree(my_prolog);\n\t_destroy_env(my_env);\n\n\treturn rc;\n}\n#else\nstatic void *_prolog_timer(void *x)\n{\n\tint delay_time, rc = SLURM_SUCCESS;\n\tstruct timespec ts;\n\tstruct timeval now;\n\tslurm_msg_t msg;\n\tjob_notify_msg_t notify_req;\n\tchar srun_msg[128];\n\ttimer_struct_t *timer_struct = (timer_struct_t *) x;\n\n\tdelay_time = MAX(2, (timer_struct->msg_timeout - 2));\n\tgettimeofday(&now, NULL);\n\tts.tv_sec = now.tv_sec + delay_time;\n\tts.tv_nsec = now.tv_usec * 1000;\n\tslurm_mutex_lock(timer_struct->timer_mutex);\n\tif (!timer_struct->prolog_fini) {\n\t\trc = pthread_cond_timedwait(timer_struct->timer_cond,\n\t\t\t\t\t    timer_struct->timer_mutex,\n\t\t\t\t\t    &ts);\n\t}\n\tslurm_mutex_unlock(timer_struct->timer_mutex);\n\n\tif (rc != ETIMEDOUT)\n\t\treturn NULL;\n\n\tslurm_msg_t_init(&msg);\n\tsnprintf(srun_msg, sizeof(srun_msg), \"Prolog hung on node %s\",\n\t\t conf->node_name);\n\tnotify_req.job_id\t= timer_struct->job_id;\n\tnotify_req.job_step_id\t= NO_VAL;\n\tnotify_req.message\t= srun_msg;\n\tmsg.msg_type\t= REQUEST_JOB_NOTIFY;\n\tmsg.data\t= &notify_req;\n\tslurm_send_only_controller_msg(&msg);\n\treturn NULL;\n}\n\nstatic int\n_run_prolog(job_env_t *job_env, slurm_cred_t *cred)\n{\n\tDEF_TIMERS;\n\tint rc, diff_time;\n\tchar *my_prolog;\n\ttime_t start_time = time(NULL);\n\tstatic uint16_t msg_timeout = 0;\n\tstatic uint16_t timeout;\n\tpthread_t       timer_id;\n\tpthread_attr_t  timer_attr;\n\tpthread_cond_t  timer_cond  = PTHREAD_COND_INITIALIZER;\n\tpthread_mutex_t timer_mutex = PTHREAD_MUTEX_INITIALIZER;\n\ttimer_struct_t  timer_struct;\n\tbool prolog_fini = false;\n\tchar **my_env;\n\n\tmy_env = _build_env(job_env);\n\tsetenvf(&my_env, \"SLURM_STEP_ID\", \"%u\", job_env->step_id);\n\tif (cred) {\n\t\tslurm_cred_arg_t cred_arg;\n\t\tslurm_cred_get_args(cred, &cred_arg);\n\t\tsetenvf(&my_env, \"SLURM_JOB_CONSTRAINTS\", \"%s\",\n\t\t\tcred_arg.job_constraints);\n\t\tgres_plugin_job_set_env(&my_env, cred_arg.job_gres_list);\n\t\tslurm_cred_free_args(&cred_arg);\n\t}\n\n\tif (msg_timeout == 0)\n\t\tmsg_timeout = slurm_get_msg_timeout();\n\n\tif (timeout == 0)\n\t\ttimeout = slurm_get_prolog_timeout();\n\n\tslurm_mutex_lock(&conf->config_mutex);\n\tmy_prolog = xstrdup(conf->prolog);\n\tslurm_mutex_unlock(&conf->config_mutex);\n\n\tslurm_attr_init(&timer_attr);\n\ttimer_struct.job_id      = job_env->jobid;\n\ttimer_struct.msg_timeout = msg_timeout;\n\ttimer_struct.prolog_fini = &prolog_fini;\n\ttimer_struct.timer_cond  = &timer_cond;\n\ttimer_struct.timer_mutex = &timer_mutex;\n\tpthread_create(&timer_id, &timer_attr, &_prolog_timer, &timer_struct);\n\tSTART_TIMER;\n\n\tif (timeout == (uint16_t)NO_VAL)\n\t\trc = _run_job_script(\"prolog\", my_prolog, job_env->jobid,\n\t\t\t\t     -1, my_env, job_env->uid);\n\telse\n\t\trc = _run_job_script(\"prolog\", my_prolog, job_env->jobid,\n\t\t\t\t     timeout, my_env, job_env->uid);\n\n\tEND_TIMER;\n\tinfo(\"%s: run job script took %s\", __func__, TIME_STR);\n\tslurm_mutex_lock(&timer_mutex);\n\tprolog_fini = true;\n\tpthread_cond_broadcast(&timer_cond);\n\tslurm_mutex_unlock(&timer_mutex);\n\n\tdiff_time = difftime(time(NULL), start_time);\n\tinfo(\"%s: prolog with lock for job %u ran for %d seconds\",\n\t     __func__, job_env->jobid, diff_time);\n\tif (diff_time >= (msg_timeout / 2)) {\n\t\tinfo(\"prolog for job %u ran for %d seconds\",\n\t\t     job_env->jobid, diff_time);\n\t}\n\n\t_remove_job_running_prolog(job_env->jobid);\n\txfree(my_prolog);\n\t_destroy_env(my_env);\n\n\tpthread_join(timer_id, NULL);\n\treturn rc;\n}\n#endif\n\nstatic int\n_run_epilog(job_env_t *job_env)\n{\n\ttime_t start_time = time(NULL);\n\tstatic uint16_t msg_timeout = 0;\n\tstatic uint16_t timeout;\n\tint error_code, diff_time;\n\tchar *my_epilog;\n\tchar **my_env = _build_env(job_env);\n\n\tif (msg_timeout == 0)\n\t\tmsg_timeout = slurm_get_msg_timeout();\n\n\tif (timeout == 0)\n\t\ttimeout = slurm_get_prolog_timeout();\n\n\tslurm_mutex_lock(&conf->config_mutex);\n\tmy_epilog = xstrdup(conf->epilog);\n\tslurm_mutex_unlock(&conf->config_mutex);\n\n\t_wait_for_job_running_prolog(job_env->jobid);\n\n\tif (timeout == (uint16_t)NO_VAL)\n\t\terror_code = _run_job_script(\"epilog\", my_epilog, job_env->jobid,\n\t\t\t\t\t     -1, my_env, job_env->uid);\n\telse\n\t\terror_code = _run_job_script(\"epilog\", my_epilog, job_env->jobid,\n\t\t\t\t\t     timeout, my_env, job_env->uid);\n\n\txfree(my_epilog);\n\t_destroy_env(my_env);\n\n\tdiff_time = difftime(time(NULL), start_time);\n\tif (diff_time >= (msg_timeout / 2)) {\n\t\tinfo(\"epilog for job %u ran for %d seconds\",\n\t\t     job_env->jobid, diff_time);\n\t}\n\n\treturn error_code;\n}\n\n\n/**********************************************************************/\n/* Because calling initgroups(2)/getgrouplist(3) can be expensive and */\n/* is not cached by sssd or nscd, we cache the group access list.     */\n/**********************************************************************/\n\ntypedef struct gid_cache_s {\n\tchar *user;\n\ttime_t timestamp;\n\tgid_t gid;\n\tgids_t *gids;\n\tstruct gid_cache_s *next;\n} gids_cache_t;\n\n#define GIDS_HASH_LEN 64\nstatic gids_cache_t *gids_hashtbl[GIDS_HASH_LEN] = {NULL};\nstatic pthread_mutex_t gids_mutex = PTHREAD_MUTEX_INITIALIZER;\n\nstatic gids_t *\n_alloc_gids(int n, gid_t *gids)\n{\n\tgids_t *new;\n\n\tnew = (gids_t *)xmalloc(sizeof(gids_t));\n\tnew->ngids = n;\n\tnew->gids = gids;\n\treturn new;\n}\n\nstatic void\n_dealloc_gids(gids_t *p)\n{\n\txfree(p->gids);\n\txfree(p);\n}\n\n/* Duplicate a gids_t struct.  */\nstatic gids_t *\n_gids_dup(gids_t *g)\n{\n\tint buf_size;\n\tgids_t *n = xmalloc(sizeof(gids_t));\n\tn->ngids = g->ngids;\n\tbuf_size = g->ngids * sizeof(gid_t);\n\tn->gids = xmalloc(buf_size);\n\tmemcpy(n->gids, g->gids, buf_size);\n\treturn n;\n}\n\nstatic gids_cache_t *\n_alloc_gids_cache(char *user, gid_t gid, gids_t *gids, gids_cache_t *next)\n{\n\tgids_cache_t *p;\n\n\tp = (gids_cache_t *)xmalloc(sizeof(gids_cache_t));\n\tp->user = xstrdup(user);\n\tp->timestamp = time(NULL);\n\tp->gid = gid;\n\tp->gids = gids;\n\tp->next = next;\n\treturn p;\n}\n\nstatic void\n_dealloc_gids_cache(gids_cache_t *p)\n{\n\txfree(p->user);\n\t_dealloc_gids(p->gids);\n\txfree(p);\n}\n\nstatic size_t\n_gids_hashtbl_idx(const char *user)\n{\n\tuint64_t x = siphash_str(user);\n\treturn x % GIDS_HASH_LEN;\n}\n\nvoid\ngids_cache_purge(void)\n{\n\tint i;\n\tgids_cache_t *p, *q;\n\n\tslurm_mutex_lock(&gids_mutex);\n\tfor (i=0; i<GIDS_HASH_LEN; i++) {\n\t\tp = gids_hashtbl[i];\n\t\twhile (p) {\n\t\t\tq = p->next;\n\t\t\t_dealloc_gids_cache(p);\n\t\t\tp = q;\n\t\t}\n\t\tgids_hashtbl[i] = NULL;\n\t}\n\tslurm_mutex_unlock(&gids_mutex);\n}\n\nstatic void\n_gids_cache_register(char *user, gid_t gid, gids_t *gids)\n{\n\tsize_t idx;\n\tgids_cache_t *p, *q;\n\n\tidx = _gids_hashtbl_idx(user);\n\tq = gids_hashtbl[idx];\n\tp = _alloc_gids_cache(user, gid, gids, q);\n\tgids_hashtbl[idx] = p;\n\tdebug2(\"Cached group access list for %s/%d\", user, gid);\n}\n\n/* how many groups to use by default to avoid repeated calls to getgrouplist */\n#define NGROUPS_START 64\n\nstatic gids_t *_gids_cache_lookup(char *user, gid_t gid)\n{\n\tsize_t idx;\n\tgids_cache_t *p;\n\tbool found_but_old = false;\n\ttime_t now = 0;\n\tint ngroups = NGROUPS_START;\n\tgid_t *groups;\n\tgids_t *ret_gids = NULL;\n\n\tidx = _gids_hashtbl_idx(user);\n\tslurm_mutex_lock(&gids_mutex);\n\tp = gids_hashtbl[idx];\n\twhile (p) {\n\t\tif (xstrcmp(p->user, user) == 0 && p->gid == gid) {\n\t\t\tslurm_ctl_conf_t *cf = slurm_conf_lock();\n\t\t\tint group_ttl = cf->group_info & GROUP_TIME_MASK;\n\t\t\tslurm_conf_unlock();\n\t\t\tif (!group_ttl) {\n\t\t\t\tret_gids = _gids_dup(p->gids);\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t\tnow = time(NULL);\n\t\t\tif (difftime(now, p->timestamp) < group_ttl) {\n\t\t\t\tret_gids = _gids_dup(p->gids);\n\t\t\t\tgoto done;\n\t\t\t} else {\n\t\t\t\tfound_but_old = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tp = p->next;\n\t}\n\t/* Cache lookup failed or cached value was too old, fetch new\n\t * value and insert it into cache.  */\n\tgroups = xmalloc(ngroups * sizeof(gid_t));\n\twhile (getgrouplist(user, gid, groups, &ngroups) == -1) {\n\t\t/* group list larger than array, resize array to fit */\n\t\tgroups = xrealloc(groups, ngroups * sizeof(gid_t));\n\t}\n\tif (found_but_old) {\n\t\txfree(p->gids->gids);\n\t\tp->gids->gids = groups;\n\t\tp->gids->ngids = ngroups;\n\t\tp->timestamp = now;\n\t\tret_gids = _gids_dup(p->gids);\n\t} else {\n\t\tgids_t *gids = _alloc_gids(ngroups, groups);\n\t\t_gids_cache_register(user, gid, gids);\n\t\tret_gids = _gids_dup(gids);\n\t}\ndone:\n\tslurm_mutex_unlock(&gids_mutex);\n\treturn ret_gids;\n}\n\n\nextern void\ndestroy_starting_step(void *x)\n{\n\txfree(x);\n}\n\n\nstatic int\n_add_starting_step(uint16_t type, void *req)\n{\n\tstarting_step_t *starting_step;\n\tint rc = SLURM_SUCCESS;\n\n\t/* Add the step info to a list of starting processes that\n\t   cannot reliably be contacted. */\n\tslurm_mutex_lock(&conf->starting_steps_lock);\n\tstarting_step = xmalloc(sizeof(starting_step_t));\n\tif (!starting_step) {\n\t\terror(\"%s failed to allocate memory\", __func__);\n\t\trc = SLURM_FAILURE;\n\t\tgoto fail;\n\t}\n\tswitch (type) {\n\tcase LAUNCH_BATCH_JOB:\n\t\tstarting_step->job_id =\n\t\t\t((batch_job_launch_msg_t *)req)->job_id;\n\t\tstarting_step->step_id =\n\t\t\t((batch_job_launch_msg_t *)req)->step_id;\n\t\tbreak;\n\tcase LAUNCH_TASKS:\n\t\tstarting_step->job_id =\n\t\t\t((launch_tasks_request_msg_t *)req)->job_id;\n\t\tstarting_step->step_id =\n\t\t\t((launch_tasks_request_msg_t *)req)->job_step_id;\n\t\tbreak;\n\tcase REQUEST_LAUNCH_PROLOG:\n\t\tstarting_step->job_id  = ((prolog_launch_msg_t *)req)->job_id;\n\t\tstarting_step->step_id = SLURM_EXTERN_CONT;\n\t\tbreak;\n\tdefault:\n\t\terror(\"%s called with an invalid type: %u\", __func__, type);\n\t\trc = SLURM_FAILURE;\n\t\txfree(starting_step);\n\t\tgoto fail;\n\t}\n\n\tif (!list_append(conf->starting_steps, starting_step)) {\n\t\terror(\"%s failed to allocate memory for list\", __func__);\n\t\trc = SLURM_FAILURE;\n\t\txfree(starting_step);\n\t\tgoto fail;\n\t}\n\nfail:\n\tslurm_mutex_unlock(&conf->starting_steps_lock);\n\treturn rc;\n}\n\n\nstatic int\n_remove_starting_step(uint16_t type, void *req)\n{\n\tuint32_t job_id, step_id;\n\tListIterator iter;\n\tstarting_step_t *starting_step;\n\tint rc = SLURM_SUCCESS;\n\tbool found = false;\n\n\tslurm_mutex_lock(&conf->starting_steps_lock);\n\n\tswitch(type) {\n\tcase LAUNCH_BATCH_JOB:\n\t\tjob_id =  ((batch_job_launch_msg_t *)req)->job_id;\n\t\tstep_id = ((batch_job_launch_msg_t *)req)->step_id;\n\t\tbreak;\n\tcase LAUNCH_TASKS:\n\t\tjob_id =  ((launch_tasks_request_msg_t *)req)->job_id;\n\t\tstep_id = ((launch_tasks_request_msg_t *)req)->job_step_id;\n\t\tbreak;\n\tdefault:\n\t\terror(\"%s called with an invalid type: %u\", __func__, type);\n\t\trc = SLURM_FAILURE;\n\t\tgoto fail;\n\t}\n\n\titer = list_iterator_create(conf->starting_steps);\n\twhile ((starting_step = list_next(iter))) {\n\t\tif (starting_step->job_id  == job_id &&\n\t\t    starting_step->step_id == step_id) {\n\t\t\tstarting_step = list_remove(iter);\n\t\t\txfree(starting_step);\n\n\t\t\tfound = true;\n\t\t\tpthread_cond_broadcast(&conf->starting_steps_cond);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\terror(\"%s: step %u.%u not found\", __func__, job_id, step_id);\n\t\trc = SLURM_FAILURE;\n\t}\nfail:\n\tslurm_mutex_unlock(&conf->starting_steps_lock);\n\treturn rc;\n}\n\n\n\nstatic int _compare_starting_steps(void *listentry, void *key)\n{\n\tstarting_step_t *step0 = (starting_step_t *)listentry;\n\tstarting_step_t *step1 = (starting_step_t *)key;\n\n\tif (step1->step_id != NO_VAL)\n\t\treturn (step0->job_id  == step1->job_id &&\n\t\t\tstep0->step_id == step1->step_id);\n\telse\n\t\treturn (step0->job_id  == step1->job_id);\n}\n\n\n/* Wait for a step to get far enough in the launch process to have\n   a socket open, ready to handle RPC calls.  Pass step_id = NO_VAL\n   to wait on any step for the given job. */\n\nstatic int _wait_for_starting_step(uint32_t job_id, uint32_t step_id)\n{\n\tstarting_step_t  starting_step;\n\tstarting_step.job_id  = job_id;\n\tstarting_step.step_id = step_id;\n\tint num_passes = 0;\n\n\tslurm_mutex_lock(&conf->starting_steps_lock);\n\n\twhile (list_find_first( conf->starting_steps,\n\t\t\t\t&_compare_starting_steps,\n\t\t\t\t&starting_step )) {\n\t\tif (num_passes == 0) {\n\t\t\tif (step_id != NO_VAL)\n\t\t\t\tdebug( \"Blocked waiting for step %d.%d\",\n\t\t\t\t\tjob_id, step_id);\n\t\t\telse\n\t\t\t\tdebug( \"Blocked waiting for job %d, all steps\",\n\t\t\t\t\tjob_id);\n\t\t}\n\t\tnum_passes++;\n\n\t\tpthread_cond_wait(&conf->starting_steps_cond,\n\t\t\t\t  &conf->starting_steps_lock);\n\t}\n\tif (num_passes > 0) {\n\t\tif (step_id != NO_VAL)\n\t\t\tdebug( \"Finished wait for step %d.%d\",\n\t\t\t\tjob_id, step_id);\n\t\telse\n\t\t\tdebug( \"Finished wait for job %d, all steps\",\n\t\t\t\tjob_id);\n\t}\n\tslurm_mutex_unlock(&conf->starting_steps_lock);\n\n\treturn SLURM_SUCCESS;\n}\n\n\n/* Return true if the step has not yet confirmed that its socket to\n   handle RPC calls has been created.  Pass step_id = NO_VAL\n   to return true if any of the job's steps are still starting. */\nstatic bool _step_is_starting(uint32_t job_id, uint32_t step_id)\n{\n\tstarting_step_t  starting_step;\n\tstarting_step.job_id  = job_id;\n\tstarting_step.step_id = step_id;\n\tbool ret = false;\n\n\tslurm_mutex_lock(&conf->starting_steps_lock);\n\n\tif (list_find_first( conf->starting_steps,\n\t\t\t     &_compare_starting_steps,\n\t\t\t     &starting_step )) {\n\t\tret = true;\n\t}\n\n\tslurm_mutex_unlock(&conf->starting_steps_lock);\n\treturn ret;\n}\n\n/* Add this job to the list of jobs currently running their prolog */\nstatic void _add_job_running_prolog(uint32_t job_id)\n{\n\tuint32_t *job_running_prolog;\n\n\t/* Add the job to a list of jobs whose prologs are running */\n\tslurm_mutex_lock(&conf->prolog_running_lock);\n\tjob_running_prolog = xmalloc(sizeof(uint32_t));\n\tif (!job_running_prolog) {\n\t\terror(\"_add_job_running_prolog failed to allocate memory\");\n\t\tgoto fail;\n\t}\n\n\t*job_running_prolog = job_id;\n\tif (!list_append(conf->prolog_running_jobs, job_running_prolog)) {\n\t\terror(\"_add_job_running_prolog failed to append job to list\");\n\t\txfree(job_running_prolog);\n\t}\n\nfail:\n\tslurm_mutex_unlock(&conf->prolog_running_lock);\n}\n\n/* Remove this job from the list of jobs currently running their prolog */\nstatic void _remove_job_running_prolog(uint32_t job_id)\n{\n\tListIterator iter;\n\tuint32_t *job_running_prolog;\n\tbool found = false;\n\n\tslurm_mutex_lock(&conf->prolog_running_lock);\n\n\titer = list_iterator_create(conf->prolog_running_jobs);\n\twhile ((job_running_prolog = list_next(iter))) {\n\t\tif (*job_running_prolog  == job_id) {\n\t\t\tjob_running_prolog = list_remove(iter);\n\t\t\txfree(job_running_prolog);\n\n\t\t\tfound = true;\n\t\t\tpthread_cond_broadcast(&conf->prolog_running_cond);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\terror(\"_remove_job_running_prolog: job not found\");\n\n\tslurm_mutex_unlock(&conf->prolog_running_lock);\n}\n\nstatic int _match_jobid(void *listentry, void *key)\n{\n\tuint32_t *job0 = (uint32_t *)listentry;\n\tuint32_t *job1 = (uint32_t *)key;\n\n\treturn (*job0 == *job1);\n}\n\nstatic int _prolog_is_running (uint32_t jobid)\n{\n\tint rc = 0;\n\tif (list_find_first (conf->prolog_running_jobs,\n\t                     (ListFindF) _match_jobid, &jobid))\n\t\trc = 1;\n\treturn (rc);\n}\n\n/* Wait for the job's prolog to complete */\nstatic void _wait_for_job_running_prolog(uint32_t job_id)\n{\n\tdebug( \"Waiting for job %d's prolog to complete\", job_id);\n\tslurm_mutex_lock(&conf->prolog_running_lock);\n\n\twhile (_prolog_is_running (job_id)) {\n\t\tpthread_cond_wait(&conf->prolog_running_cond,\n\t\t\t\t  &conf->prolog_running_lock);\n\t}\n\n\tslurm_mutex_unlock(&conf->prolog_running_lock);\n\tdebug( \"Finished wait for job %d's prolog to complete\", job_id);\n}\n\n\nstatic void\n_rpc_forward_data(slurm_msg_t *msg)\n{\n\tforward_data_msg_t *req = (forward_data_msg_t *)msg->data;\n\tuint32_t req_uid;\n\tstruct sockaddr_un sa;\n\tint fd = -1, rc = 0;\n\n\tdebug3(\"Entering _rpc_forward_data, address: %s, len: %u\",\n\t       req->address, req->len);\n\n\t/* sanity check */\n\tif (strlen(req->address) > sizeof(sa.sun_path) - 1) {\n\t\tslurm_seterrno(EINVAL);\n\t\trc = errno;\n\t\tgoto done;\n\t}\n\n\t/* connect to specified address */\n\tfd = socket(AF_UNIX, SOCK_STREAM, 0);\n\tif (fd < 0) {\n\t\trc = errno;\n\t\terror(\"failed creating UNIX domain socket: %m\");\n\t\tgoto done;\n\t}\n\tmemset(&sa, 0, sizeof(sa));\n\tsa.sun_family = AF_UNIX;\n\tstrcpy(sa.sun_path, req->address);\n\twhile (((rc = connect(fd, (struct sockaddr *)&sa, SUN_LEN(&sa))) < 0) &&\n\t       (errno == EINTR));\n\tif (rc < 0) {\n\t\trc = errno;\n\t\tdebug2(\"failed connecting to specified socket '%s': %m\",\n\t\t       req->address);\n\t\tgoto done;\n\t}\n\n\treq_uid = (uint32_t)g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t conf->auth_info);\n\t/*\n\t * although always in localhost, we still convert it to network\n\t * byte order, to make it consistent with pack/unpack.\n\t */\n\treq_uid = htonl(req_uid);\n\tsafe_write(fd, &req_uid, sizeof(uint32_t));\n\treq_uid = htonl(req->len);\n\tsafe_write(fd, &req_uid, sizeof(uint32_t));\n\tsafe_write(fd, req->data, req->len);\n\nrwfail:\ndone:\n\tif (fd >= 0){\n\t\tclose(fd);\n\t}\n\tslurm_send_rc_msg(msg, rc);\n}\n\nstatic void _launch_complete_add(uint32_t job_id)\n{\n\tint j, empty;\n\n\tslurm_mutex_lock(&job_state_mutex);\n\tempty = -1;\n\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\tif (job_id == active_job_id[j])\n\t\t\tbreak;\n\t\tif ((active_job_id[j] == 0) && (empty == -1))\n\t\t\tempty = j;\n\t}\n\tif (j >= JOB_STATE_CNT || job_id != active_job_id[j]) {\n\t\tif (empty == -1)\t/* Discard oldest job */\n\t\t\tempty = 0;\n\t\tfor (j = empty + 1; j < JOB_STATE_CNT; j++) {\n\t\t\tactive_job_id[j - 1] = active_job_id[j];\n\t\t}\n\t\tactive_job_id[JOB_STATE_CNT - 1] = 0;\n\t\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\t\tif (active_job_id[j] == 0) {\n\t\t\t\tactive_job_id[j] = job_id;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tpthread_cond_signal(&job_state_cond);\n\tslurm_mutex_unlock(&job_state_mutex);\n\t_launch_complete_log(\"job add\", job_id);\n}\n\nstatic void _launch_complete_log(char *type, uint32_t job_id)\n{\n#if 0\n\tint j;\n\n\tinfo(\"active %s %u\", type, job_id);\n\tslurm_mutex_lock(&job_state_mutex);\n\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\tif (active_job_id[j] != 0) {\n\t\t\tinfo(\"active_job_id[%d]=%u\", j, active_job_id[j]);\n\t\t}\n\t}\n\tslurm_mutex_unlock(&job_state_mutex);\n#endif\n}\n\n/* Test if we have a specific job ID still running */\nstatic bool _launch_job_test(uint32_t job_id)\n{\n\tbool found = false;\n\tint j;\n\n\tslurm_mutex_lock(&job_state_mutex);\n\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\tif (job_id == active_job_id[j]) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tslurm_mutex_unlock(&job_state_mutex);\n\treturn found;\n}\n\n\nstatic void _launch_complete_rm(uint32_t job_id)\n{\n\tint j;\n\n\tslurm_mutex_lock(&job_state_mutex);\n\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\tif (job_id == active_job_id[j])\n\t\t\tbreak;\n\t}\n\tif (j < JOB_STATE_CNT && job_id == active_job_id[j]) {\n\t\tfor (j = j + 1; j < JOB_STATE_CNT; j++) {\n\t\t\tactive_job_id[j - 1] = active_job_id[j];\n\t\t}\n\t\tactive_job_id[JOB_STATE_CNT - 1] = 0;\n\t}\n\tslurm_mutex_unlock(&job_state_mutex);\n\t_launch_complete_log(\"job remove\", job_id);\n}\n\nstatic void _launch_complete_wait(uint32_t job_id)\n{\n\tint i, j, empty;\n\ttime_t start = time(NULL);\n\tstruct timeval now;\n\tstruct timespec timeout;\n\n\tslurm_mutex_lock(&job_state_mutex);\n\tfor (i = 0; ; i++) {\n\t\tempty = -1;\n\t\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\t\tif (job_id == active_job_id[j])\n\t\t\t\tbreak;\n\t\t\tif ((active_job_id[j] == 0) && (empty == -1))\n\t\t\t\tempty = j;\n\t\t}\n\t\tif (j < JOB_STATE_CNT)\t/* Found job, ready to return */\n\t\t\tbreak;\n\t\tif (difftime(time(NULL), start) <= 9) {  /* Retry for 9 secs */\n\t\t\tdebug2(\"wait for launch of job %u before suspending it\",\n\t\t\t       job_id);\n\t\t\tgettimeofday(&now, NULL);\n\t\t\ttimeout.tv_sec  = now.tv_sec + 1;\n\t\t\ttimeout.tv_nsec = now.tv_usec * 1000;\n\t\t\tpthread_cond_timedwait(&job_state_cond,&job_state_mutex,\n\t\t\t\t\t       &timeout);\n\t\t\tcontinue;\n\t\t}\n\t\tif (empty == -1)\t/* Discard oldest job */\n\t\t\tempty = 0;\n\t\tfor (j = empty + 1; j < JOB_STATE_CNT; j++) {\n\t\t\tactive_job_id[j - 1] = active_job_id[j];\n\t\t}\n\t\tactive_job_id[JOB_STATE_CNT - 1] = 0;\n\t\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\t\tif (active_job_id[j] == 0) {\n\t\t\t\tactive_job_id[j] = job_id;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\tslurm_mutex_unlock(&job_state_mutex);\n\t_launch_complete_log(\"job wait\", job_id);\n}\n\nstatic bool\n_requeue_setup_env_fail(void)\n{\n\tstatic time_t config_update = 0;\n\tstatic bool requeue = false;\n\n\tif (config_update != conf->last_update) {\n\t\tchar *sched_params = slurm_get_sched_params();\n\t\trequeue = (sched_params &&\n\t\t\t   (strstr(sched_params, \"no_env_cache\") ||\n\t\t\t    strstr(sched_params, \"requeue_setup_env_fail\")));\n\t\txfree(sched_params);\n\t\tconfig_update = conf->last_update;\n\t}\n\n\treturn requeue;\n}\n"], "fixing_code": ["This file describes changes in recent versions of Slurm. It primarily\ndocuments those changes that are of interest to users and administrators.\n\n* Changes in Slurm 16.05.8\n==========================\n -- Remove StoragePass from being printed out in the slurmdbd log at debug2\n    level.\n -- Defer PATH search for task program until launch in slurmstepd.\n -- Modify regression test1.89 to avoid leaving vestigial job. Also reduce\n    logging to reduce likelyhood of Expect buffer overflow.\n -- Do not PATH search for mult-prog launches if LaunchParamters=test_exec is\n    enabled.\n -- Fix for possible infinite loop in select/cons_res plugin when trying to\n    satisfy a job's ntasks_per_core or socket specification.\n -- If job is held for bad constraints make it so once updated the job doesn't\n    go into JobAdminHeld.\n -- sched/backfill - Fix logic to reserve resources for jobs that require a\n    node reboot (i.e. to change KNL mode) in order to start.\n -- When unpacking a node or front_end record from state and the protocol\n    version is lower than the min version, set it to the min.\n -- Remove redundant lookup for part_ptr when updating a reservation's nodes.\n -- Fix memory and file descriptor leaks in slurmd daemon's sbcast logic.\n -- Do not allocate specialized cores to jobs using the --exclusive option.\n -- Cancel interactive job if Prolog failure with \"PrologFlags=contain\" or\n    \"PrologFlags=alloc\" configured. Send new error prolog failure message to\n    the salloc or srun command as needed.\n -- Prevent possible out-of-bounds read in slurmstepd on an invalid #! line.\n -- Fix check for PluginDir within slurmctld to work with multiple directories.\n -- Cancel interactive jobs automatically on communication error to launching\n    srun/salloc process.\n -- Fix security issue caused by insecure file path handling triggered by the\n    failure of a Prolog script. To exploit this a user needs to anticipate or\n    cause the Prolog to fail for their job. CVE-2016-10030.\n\n* Changes in Slurm 16.05.7\n==========================\n -- Fix issue in the priority/multifactor plugin where on a slurmctld restart,\n    where more time is accounted for than should be allowed.\n -- cray/busrt_buffer - If total_space in a pool decreases, reset used_space\n    rather than trying to account for buffer allocations in progress.\n -- cray/busrt_buffer - Fix for double counting of used_space at slurmctld\n    startup.\n -- Fix regression in 16.05.6 where if you request multiple cpus per task (-c2)\n    and request --ntasks-per-core=1 and only 1 task on the node\n    the slurmd would abort on an infinite loop fatal.\n -- cray/busrt_buffer - Internally track both allocated and unusable space.\n    The reported UsedSpace in a pool is now the allocated space (previously was\n    unusable space). Base available space on whichever value leaves least free\n    space.\n -- cray/burst_buffer - Preserve job ID and don't translate to job array ID.\n -- cray/burst_buffer - Update \"instance\" parsing to match updated dw_wlm_cli\n    output.\n -- sched/backfill - Insure we don't try to start a job that was already started\n    and requeued by the main scheduling logic.\n -- job_submit/lua - add access to the job features field in job_record.\n -- select/linear plugin modified to better support heterogeneous clusters when\n    topology/none is also configured.\n -- Permit cancellation of jobs in configuring state.\n -- acct_gather_energy/rapl - prevent segfault in slurmd from race to gather\n    data at slurmd startup.\n -- Integrate node_feature/knl_generic with \"hbm\" GRES information.\n -- Fix output routines to prevent rounding the TRES values for memory or BB.\n -- switch/cray plugin - fix use after free error.\n -- docs - elaborate on how way to clear TRES limits in sacctmgr.\n -- knl_cray plugin - Avoid abort from backup slurmctld at start time.\n -- cgroup plugins - fix two minor memory leaks.\n -- If a node is booting for some job, don't allocate additional jobs to the\n    node until the boot completes.\n -- testsuite - fix job id output in test17.39.\n -- Modify backfill algorithm to improve performance with large numbers of\n    running jobs. Group running jobs that end in a \"similar\" time frame using a\n    time window that grows exponentially rather than linearly. After one second\n    of wall time, simulate the termination of all remaining running jobs in\n    order to respond in a reasonable time frame.\n -- Fix slurm_job_cpus_allocated_str_on_node_id() API call.\n -- sched/backfill plugin: Make malloc match data type (defined as uint32_t and\n    allocated as int).\n -- srun - prevent segfault when terminating job step before step has launched.\n -- sacctmgr - prevent segfault when trying to reset usage for an invalid\n    account name.\n -- Make the openssl crypto plugin compile with openssl >= 1.1.\n -- Fix SuspendExcNodes and SuspendExcParts on slurmctld reconfiguration.\n -- sbcast - prevent segfault in slurmd due to race condition between file\n    transfers from separate jobs using zlib compression\n -- cray/burst_buffer - Increase time to synchronize operations between threads\n    from 5 to 60 seconds (\"setup\" operation time observed over 17 seconds).\n -- node_features/knl_cray - Fix possible race condition when changing node\n    state that could result in old KNL mode as an active features.\n -- Make sure if a job can't run because of resources we also check accounting\n    limits after the node selection to make sure it doesn't violate those limits\n    and if it does change the reason for waiting so we don't reserve resources\n    on jobs violating accounting limits.\n -- NRT - Make it so a system running against IBM's PE will work with PE\n    version 1.3.\n -- NRT - Make it so protocols pgas and test are allowed to be used.\n -- NRT - Make it so you can have more than 1 protocol listed in MP_MSG_API.\n -- cray/burst_buffer - If slurmctld daemon restarts with pending job and burst\n    buffer having unknown file stage-in status, teardown the buffer, defer the\n    job, and start stage-in over again.\n -- On state restore in the slurmctld don't overwrite the mem_spec_limit given\n    from the slurm.conf when using FastSchedule=0.\n -- Recognize a KNL's proper NUMA count (rather than setting it to the value\n    in slurm.conf) when using FastSchedule=0.\n -- Fix parsing in regression test1.92 for some prompts.\n -- sbcast - use slurmd's gid cache rather than a separate lookup.\n -- slurmd - return error if setgroups() call fails in _drop_privileges().\n -- Remove error messages about gres counts changing when a job is resized on\n    a slurmctld restart or reconfig, as they aren't really error messages.\n -- Fix possible memory corruption if a job is using GRES and changing size.\n -- jobcomp/elasticsearch - fix printf format for a value on 32-bit builds.\n -- task/cgroup - Change error message if CPU binding can not take place to\n    better identify the root cause of the problem.\n -- Fix issue where task/cgroup would not always honor --cpu_bind=threads.\n -- Fix race condition in with getgrouplist() in slurmd that can lead to\n    user accounts being granted access to incorrect group memberships during\n    job launch.\n\n* Changes in Slurm 16.05.6\n==========================\n -- Docs - the correct default value for GroupUpdateForce is 0.\n -- mpi/pmix - improve point to point communication performance.\n -- SlurmDB - include pending jobs in search during 'sacctmgr show runawayjobs'.\n -- Add client side out-of-range checks to --nice flag.\n -- Fix support for sbatch \"-W\" option, previously eeded to use \"--wait\".\n -- node_features/knl_cray plugin and capmc_suspend/resume programs modified to\n    sleep and retry capmc operations if the Cray State Manager is down. Added\n    CapmcRetries configuration parameter to knl_cray.conf.\n -- node_features/knl_cray plugin: Remove any KNL MCDRAM or NUMA features from\n    node's configuration if capmc does NOT report the node as being KNL.\n -- node_features/knl_cray plugin: drain any node not reported by\n    \"capmc node_status\" on startup or reconfig.\n -- node_features/knl_cray plugin: Substantially streamline and speed up logic\n    to load current node state on reconfigure failure or unexpected node boot.\n -- node_features/knl_cray plugin: Add separate thread to interact with capmc\n    in response to unexpected node reboots.\n -- node_features plugin - Add \"mode\" argument to node_features_p_node_xlate()\n    function to fix some bugs updating a node's features using the node update\n    RPC.\n -- node_features/knl_cray plugin: If the reconfiguration of nodes for an\n    interactive job fails, kill the job (it can't be requeued like a batch job).\n -- Testsuite - Added srun/salloc/sbatch tests with --use-min-nodes option.\n -- Fix typo when an error occurs when discovering pmix version on\n    configure.\n -- Fix configuring pmix support when you have your lib dir symlinked to lib64.\n -- Fix waiting reason if a job is waiting for a specific limit instead of\n    always just AccountingPolicy.\n -- Correct SchedulerParameters=bf_busy_nodes logic with respect to the job's\n    minimum node count. Previous logic would not decremement counter in some\n    locations and reject valid job request for not reaching minimum node count.\n -- Fix FreeBSD-11 build by using llabs() function in place of abs().\n -- Cray: The slurmd can manipulate the socket/core/thread values reported based\n    upon the configuration. The logic failed to consider select/cray with\n    SelectTypeParameters=other_cons_res as equivalent to select/cons_res.\n -- If a node's socket or core count are changed at registration time (e.g. a\n    KNL node's NUMA mode is changed), change it's board count to match.\n -- Prevent possible divide by zero in select/cons_res if a node's board count\n    is higher than it's socket count.\n -- Allow an advanced reservation to contain a license count of zero.\n -- Preserve non-KNL node features when updating the KNL node features for a\n    multi-node job in which the non-KNL node features vary by node.\n -- task/affinity plugin: Honor a job's --ntasks-per-socket and\n    --ntasks-per-core options in task binding.\n -- slurmd - do not print ClusterName when using 'slurmd -C'.\n -- Correct a bitmap test function (used only by the select/bluegene plugin).\n -- Do not propagate SLURM_UMASK environment variable to batch script.\n -- Added node_features/knl_generic plugin for KNL support on non-Cray systems.\n -- Cray: Prevent abort in backfill scheduling logic for requeued job that has\n    been cancelled while NHC is running.\n -- Improve reported estimates of start and end times for pending jobs.\n -- pbsnodes: Show OS value as \"unknown\" for down nodes.\n -- BlueGene - correctly scale node counts when enforcing MaxNodes limit take 2.\n -- Fix \"sbatch --hold\" to set JobHeldUser correctly instead of JobHeldAdmin.\n -- Cray - print warning that task/cgroup is required, and must be after\n    task/cray in the TaskPlugin settings.\n -- Document that node Weight takes precedence over load with LLN scheduling.\n -- Fix issue where gang scheduling could happen even with OverSubscribe=NO.\n -- Expose JOB_SHARED_* values to job_submit/lua plugin.\n -- Fix issue where number of nodes is not properly allocated when srun is\n    requested with -n tasks < hosts from -w hostlist.\n -- Update srun documentation for -N, -w and -m arbitrary.\n -- Fix bug that was clearing MAINT mode on nodes scheduled for reboot (bug\n    introduced in version 16.05.5 to address bug in overlapping reservations).\n -- Add logging of node reboot requests.\n -- Docs - remove recommendation for ReleaseAgent setting in cgroup.conf.\n -- Make sure a job cleans up completely if it has a node fail.  Mostly an\n    issue with gang scheduling.\n\n* Changes in Slurm 16.05.5\n==========================\n -- Fix accounting for jobs requeued after the previous job was finished.\n -- slurmstepd modified to pre-load all relevant plugins at startup to avoid\n    the possibility of modified plugins later resulting in inconsistent API\n    or data structures and a failure of slurmstepd.\n -- Export functions from parse_time.c in libslurm.so.\n -- Export unit convert functions from slurm_protocol_api.c in libslurm.so.\n -- Fix scancel to allow multiple steps from a job to be cancelled at once.\n -- Update and expand upgrade guide (in Quick Start Administrator web page).\n -- burst_buffer/cray: Requeue, but do not hold a job which fails the pre_run\n    operation.\n -- Insure reported expected job start time is not in the past for pending jobs.\n -- Add support for PMIx v2.\n -- mpi/pmix: support for passing TMPDIR path through info key\n -- Cray: update slurmconfgen_smw.py script to correctly identify service nodes\n    versus compute nodes.\n -- FreeBSD - fix build issue in knl_cray plugin.\n -- Corrections to gres.conf parsing logic.\n -- Make partition State independent of EnforcePartLimits value.\n -- Fix multipart srun submission with EnforcePartLimits=NO and job violating\n    the partition limits.\n -- Fix problem updating job state_reason.\n -- pmix - Provide HWLOC topology in the job-data if Slurm was configured\n    with hwloc.\n -- Cray - Fix issue restoring jobs when blade count increases due to hardware\n    reconfiguration.\n -- burst_buffer/cray - Hold job after 3 failed pre-run operations.\n -- sched/backfill - Check that a user's QOS is allowed to use a partition\n    before trying to schedule resources on that partition for the job.\n -- sacctmgr - Fix displaying nodenames when printing out events or\n    reservations.\n -- Fix mpiexec wrapper to accept task count with more than one digit.\n -- Add mpiexec man page to the script.\n -- Add salloc_wait_nodes option to the SchedulerParameters parameter in the\n    slurm.conf file controlling when the salloc command returns in relation to\n    when nodes are ready for use (i.e. booted).\n -- Handle case when slurmctld daemon restart while compute node reboot in\n    progress. Return node to service rather than setting DOWN.\n -- Preserve node \"RESERVATION\" state when one of multiple overlapping\n    reservations ends.\n -- Restructure srun command locking for task_exit processing logic for improved\n    parallelism.\n -- Modify srun task completion handling to only build the task/node string for\n    logging purposes if it is needed. Modified for performance purposes.\n -- Docs - update salloc/sbatch/srun man pages to mention corresponding\n    environment variables for --mem/--mem-per-cpu and allowed suffixes.\n -- Silence srun warning when overriding the job ntasks-per-node count\n    with a lower task count for the step.\n -- Docs - assorted spelling fixes.\n -- node_features/knl_cray: Fix bug where MCDRAM state could be taken from\n    capmc rather than cnselect.\n -- node_features/knl_cray: If a node is rebooted outside of Slurm's direction,\n    update it's active features with current MCDRAM and NUMA mode information.\n -- Restore ability to manually power down nodes, broken in 15.08.12.\n -- Don't log error for job end_time being zero if node health check is still\n    running.\n -- When powering up a node to change it's state (e.g. KNL NUMA or MCDRAM mode)\n    then pass to the ResumeProgram the job ID assigned to the nodes in the\n    SLURM_JOB_ID environment variable.\n -- Allow a node's PowerUp state flag to be cleared using update_node RPC.\n -- capmc_suspend/resume - If a request modify NUMA or MCDRAM state on a set of\n    nodes or reboot a set of nodes fails then just requeue the job and abort the\n    entire operation rather than trying to operate on individual nodes.\n -- node_features/knl_cray plugin: Increase default CapmcTimeout parameter from\n    10 to 60 seconds.\n -- Fix squeue filter by job license when a job has requested more than 1\n    license of a certain type.\n -- Fix bug in PMIX_Ring in the pmi2 plugin so that it supports singleton mode.\n    It also updates the testpmixring.c test program so it can be used to check\n    singleton runs.\n -- Automically cleanup task/cgroup cpuset and devices cgroups after steps are\n    done.\n -- Testsuite - Fix test1.83 to handle gaps in node names properly.\n -- BlueGene - correctly scale node counts when enforcing MaxNodes limit.\n -- Make sure no attempt is made to schedule a requeued job until all steps are\n    cleaned (Node Health Check completes for all steps on a Cray).\n -- KNL: Correct task affinity logic for some NUMA modes.\n -- Add salloc/sbatch/srun --priority option of \"TOP\" to set job priority to\n    the highest possible value. This option is only available to Slurm operators\n    and administrators.\n -- Add salloc/sbatch/srun option --use-min-nodes to prefer smaller node counts\n    when a range of node counts is specified (e.g. \"-N 2-4\").\n -- Validate salloc/sbatch --wait-all-nodes argument.\n -- Add \"sbatch_wait_nodes\" to SchedulerParameters to control default sbatch\n    behaviour with respect to waiting for all allocated nodes to be ready for\n    use. Job can override the configuration option using the --wait-all-nodes=#\n    option.\n -- Prevent partition group access updates from resetting last_part_update when\n    no changes have been made. Prevents backfill scheduler from restarting\n    mid-cycle unnecessarily.\n -- Cray - add NHC_ABSOLUTELY_NO to never run NHC, even on certain edge cases\n    that it would otherwise be run on with NHC_NO.\n -- Ignore GRES/QOS updates that maintain the same value as before.\n -- mpi/pmix - prepare temp directory for application.\n -- Fix display for the nice and priority values in sprio/scontrol/squeue.\n\n* Changes in Slurm 16.05.4\n==========================\n -- Fix potential deadlock if running with message aggregation.\n -- Streamline when schedule() is called when running with message aggregation\n    on batch script completes.\n -- Fix incorrect casting when [un]packing derived_ec on slurmdb_job_rec_t.\n -- Document that persistent burst buffers can not be created or destroyed using\n    the salloc or srun --bb options.\n -- Add support for setting the SLURM_JOB_ACCOUNT, SLURM_JOB_QOS and\n    SLURM_JOB_RESERVAION environment variables are set for the salloc command.\n    Document the same environment variables for the salloc, sbatch and srun\n    commands in their man pages.\n -- Fix issue where sacctmgr load cluster.cfg wouldn't load associations\n    that had a partition in them.\n -- Don't return the extern step from sstat by default.\n -- In sstat print 'extern' instead of 4294967295 for the extern step.\n -- Make advanced reservations work properly with core specialization.\n -- Fix race condition in the account_gather plugin that could result in job\n    stuck in COMPLETING state.\n -- Regression test fixes if SelectTypePlugin not managing memory and no node\n    memory size set (defaults to 1 MB per node).\n -- Add missing partition write locks to _slurm_rpc_dump_nodes/node_single to\n    prevent a race condition leading to inconsistent sinfo results.\n -- Fix task:CPU binding logic for some processors. This bug was introduced\n    in version 16.05.1 to address KNL bunding problem.\n -- Fix two minor memory leaks in slurmctld.\n -- Improve partition-specific limit logging from slurmctld daemon.\n -- Fix incorrect access check when using MaxNodes setting on the partition.\n -- Fix issue with sacctmgr when specifying a list of clusters to query.\n -- Fix issue when calculating future StartTime for a job.\n -- Make EnforcePartLimit support logic work with any ordering of partitions\n    in job submit request.\n -- Prevent restoration of wrong CPU governor and frequency when using\n    multiple task plugins.\n -- Prevent slurmd abort if hwloc library fails to populate the \"children\"\n    arrays (observed with hwloc version \"dev-333-g85ea6e4\").\n -- burst_buffer/cray: Add \"--groupid\" to DataWarp \"setup\" command.\n -- Fix lustre profiling putting it in the Filesystem dataset instead of the\n    Network dataset.\n -- Fix profiling documentation and code to match be consistent with\n    Filesystem instead of Lustre.\n -- Correct the way watts is calculated in the rapl plugin when using a poll\n    frequency other than AcctGatherNodeFreq.\n -- Don't about step launch if job reaches expected end time while node is\n    configuring/booting (NOTE: The job end time will be adjusted after node\n    becomes ready for use).\n -- Fix several print routines to respect a custom output delimiter when\n    printing NO_VAL or INFINITE.\n -- Correct documented configurations where --ntasks-per-core and\n    --ntasks-per-socket are supported.\n -- task/affinity plugin buffer allocated too small, can corrupt memory.\n\n* Changes in Slurm 16.05.3\n==========================\n -- Make it so the extern step uses a reverse tree when cleaning up.\n -- If extern step doesn't get added into the proctrack plugin make sure the\n    sleep is killed.\n -- Fix areas the slurmctld can segfault if an extern step is in the system\n    cleaning up on a restart.\n -- Prevent possible incorrect counting of GRES of a given type if a node has\n    the multiple \"types\" of a given GRES \"name\", which could over-subscribe\n    GRES of a given type.\n -- Add web links to Slurm Diamond Collectors (from Harvard University) and\n    collectd (from EDF).\n -- Add job_submit plugin for the \"reboot\" field.\n -- Make some more Slurm constants (INFINITE, NO_VAL64, etc.) available to\n    job_submit/lua plugins.\n -- Send in a -1 for a taskid into spank_task_post_fork for the extern_step.\n -- MYSQL - Sightly better logic if a job completion comes in with an end time\n    of 0.\n -- task/cgroup plugin is configured with ConstrainRAMSpace=yes, then set soft\n    memory limit to allocated memory limit (previously no soft limit was set).\n -- Document limitations in burst buffer use by the salloc command (possible\n    access problems from a login node).\n -- Fix proctrack plugin to only add the pid of a process once\n    (regression in 16.05.2).\n -- Fix for sstat to print correct info when requesting jobid.batch as part of\n    a comma-separated list.\n -- CRAY - Fix issue if pid has already been added to another job container.\n -- CRAY - Fix add of extern step to AELD.\n -- burstbufer/cray: avoid batch submit error condition if waiting for stagein.\n -- CRAY - Fix for reporting steps lingering after they are already finished.\n -- Testsuite - fix test1.29 / 17.15 for limits with values above 32-bits.\n -- CRAY - Simplify when a NHC is called on a step that has unkillable\n    processes.\n -- CRAY - If trying to kill a step and you have NHC_NO_STEPS set run NHC\n    anyway to attempt to log the backtraces of the potential\n    unkillable processes.\n -- Fix gang scheduling and license release logic if single node job killed on\n    bad node.\n -- Make scontrol show steps show the extern step correctly.\n -- Do not scheduled powered down nodes in FAILED state.\n -- Do not start slurmctld power_save thread until partition information is read\n    in order to prevent race condition that can result invalid pointer when\n    trying to resolve configured SuspendExcParts.\n -- Add SLURM_PENDING_STEP id so it won't be confused with SLURM_EXTERN_CONT.\n -- Fix for core selection with job --gres-flags=enforce-binding option.\n    Previous logic would in some cases allocate a job zero cores, resulting in\n    slurmctld abort.\n -- Minimize preempted jobs for configurations with multiple jobs per node.\n -- Improve partition AllowGroups caching. Update the table of UIDs permitted to\n    use a partition based upon it's AllowGroups configuration parameter as new\n    valid UIDs are found rather than looking up that user's group information\n    for every job they submit. If the user is now allowed to use the partition,\n    then do not check that user's group access again for 5 seconds.\n -- Add routing queue information to Slurm FAQ web page.\n -- Do not select_g_step_finish() a SLURM_PENDING_STEP step, as nothing has\n    been allocated for the step yet.\n -- Fixed race condition in PMIx Fence logic.\n -- Prevent slurmctld abort if job is killed or requeued while waiting for\n    reboot of its allocated compute nodes.\n -- Treat invalid user ID in AllowUserBoot option of knl.conf file as error\n    rather than fatal (log and do not exit).\n -- qsub - When doing the default output files for an array in qsub style\n    make them using the master job ID instead of the normal job ID.\n -- Create the extern step while creating the job instead of waiting until the\n    end of the job to do it.\n -- Always report a 0 exit code for the extern step instead of being canceled\n    or failed based on the signal that would always be killing it.\n -- Fix to allow users to update QOS of pending jobs.\n -- CRAY - Fix minor memory leak in switch plugin.\n -- CRAY - Change slurmconfgen_smw.py to skip over disabled nodes.\n -- Fix eligible_time for elasticsearch as well as add queue_wait\n    (difference between start of job and when it was eligible).\n\n* Changes in Slurm 16.05.2\n==========================\n -- CRAY - Fix issue where the proctrack plugin could hang if the container\n    id wasn't able to be made.\n -- Move test for job wait reason value of BurstBufferResources and\n    BurstBufferStageIn later in the scheduling logic.\n -- Document which srun options apply to only job, only step, or job and step\n    allocations.\n -- Use more compatible function to get thread name (>= 2.6.11).\n -- Fix order of job then step id when noting cleaning flag being set.\n -- Make it so the extern step sends a message with accounting information\n    back to the slurmctld.\n -- Make it so the extern step calls the select_g_step_start|finish functions.\n -- Don't print error when extern step is canceled because job is ending.\n -- Handle a few error codes when dealing with the extern step to make sure\n    we have the pids added to the system correctly.\n -- Add support for job dependencies with job array expressions. Previous logic\n    required listing each task of job array individually.\n -- Make sure tres_cnt is set before creating a slurmdb_assoc_usage_t.\n -- Prevent backfill scheduler from starting a second \"singleton\" job if another\n    one started during a backfill sleep.\n -- Fix for invalid array pointer when creating advanced reservation when job\n    allocations span heterogeneous nodes (differing core or socket counts).\n -- Fix hostlist_ranged_string_xmalloc_dims to correctly not put brackets on\n    hostlists when brackets == 0.\n -- Make sure we don't get brackets when making a range of reserved ports\n    for a step.\n -- Change fatal to an error if port ranges aren't correct when reading state\n    for steps.\n\n* Changes in Slurm 16.05.1\n==========================\n -- Fix __cplusplus macro in spank.h to allow compilation with C++.\n -- Fix compile issue with older glibc < 2.12\n -- Fix for starting batch step with mpi/pmix plugin.\n -- Fix for \"scontrol -dd show job\" with respect to displaying the specific\n    CPUs allocated to a job on each node. Prior logic would only display\n    the CPU information for the first node in the job allocation.\n -- Print correct return code on failure to update active node features\n    through sview.\n -- Allow QOS timelimit to override partition timelimit when EnforcePartLimits\n    is set to all/any.\n -- Make it so qsub will do a \"basename\" on a wrapped command for the output\n    and error files.\n -- Fix issue where slurmd could core when running the ipmi energy plugin.\n -- Documentation - clean up typos.\n -- Add logic so that slurmstepd can be launched under valgrind.\n -- Increase buffer size to read /proc/*/stat files.\n -- Fix for tracking job resource allocation when slurmctld is reconfigured\n    while Cray Node Health Check (NHC) is running. Previous logic would fail to\n    record the job's allocation then perform release operation upon NHC\n    completion, resulting in underflow error messages.\n -- Make \"scontrol show daemons\" work with long node names.\n -- CRAY - Collect energy using a uint64_t instead of uint32_t.\n -- Fix incorrect if statements when determining if the user has a default\n    account or wckey.\n -- Prevent job stuck in configuring state if slurmctld daemon restarted while\n    PrologSlurmctld is running. Also re-issue burst_buffer/pre-load operation\n    as needed.\n -- Correct task affinity support for FreeBSD.\n -- Fix for task affinity on KNL in SNC2/Flat mode.\n -- Recalculate a job's memory allocation after node reboot if job requests all\n    of a node's memory and FastSchedule=0 is configured. Intel KNL memory size\n    can change on reboot with various MCDRAM modes.\n -- Fix small memory leak when printing HealthCheckNodeState.\n -- Eliminate memory leaks when AuthInfo is configured.\n -- Improve sdiag output description in man page.\n -- Cray/capmc_resume script modify a node's features (as needed) when the\n    reinit (reboot) command is issued rather than wait for the nodes to change\n    to the \"on\" state.\n -- Correctly print ranges when using step values in job arrays.\n -- Allow from file names / paths over 256 characters when launching steps,\n    as well as spaces in the executable name.\n -- job_submit.license.lua example modified to send message back to user.\n -- Document job --mem=0 option means all memory on a node.\n -- Set SLURM_JOB_QOS environment variable to QOS name instead of description.\n -- knl_cray.conf file option of CnselectPath added.\n -- node_features/knl_cray plugin modified to get current node NUMA and MCDRAM\n    modes using cnselect command rather than capmc command.\n -- liblua - add SLES12 paths to runtime search list.\n -- Fix qsub default output and error files for task arrays.\n -- Fix qsub to set job_name correctly when wrapping a script (-b y)\n -- Cray - set EnforcePartLimits=any in slurm.conf template.\n\n* Changes in Slurm 16.05.0\n==========================\n -- Update seff to fix warnings with ncpus, and list slurm-perlapi dependency\n    in spec file.\n -- Fix testsuite to consistent use /usr/bin/env {bash,expect} construct.\n -- Cray - Ensure that step completion messages get to the database.\n -- Fix step cpus_per_task calculation for heterogeneous job allocation.\n -- Fix --with-json= configure option to use specified path.\n -- Add back thread_id to \"thread_id\" LogTimeFormat to distinguish between\n    mutliple threads with the same name. Now displays thread name and id.\n -- Change how Slurm determines the NUMA count of a node. Ignore KNL NUMA\n    that only include memory.\n -- Cray - Fix node list parsing in capmc_suspend/resume programs.\n -- Fix sbatch #BSUB parsing for -W and -M options.\n -- Fix GRES task layout bug that could cause slurmctld to abort.\n -- Fix to --gres-flags=enforce-binding logic when multiple sockets needed.\n\n* Changes in Slurm 16.05.0rc2\n=============================\n -- Cray node shutdown/reboot scripts, perform operations on all nodes in one\n    capmc command. Only if that fails, issue the operations in parallel on\n    individual nodes. Required for scalability.\n -- Cleanup two minor Coverity warnings.\n -- Make it so the tres units in a job's formatted string are converted like\n    they are in a step.\n -- Correct partition's MaxCPUsPerNode enforcement when nodes are shared by\n    multiple partitions.\n -- node_feature/knl_cray - Prevent slurmctld GRES errors for \"hbm\" references.\n -- Display thread name instead of thread id and remove process name in stderr\n    logging for \"thread_id\" LogTimeFormat.\n -- Log IP address of bad incomming message to slurmctld.\n -- If a user requests tasks, nodes and ntasks-per-node and\n    tasks-per-node/nodes != tasks print warning and ignore ntasks-per-node.\n -- Release CPU \"owner\" file locks.\n -- Fix for job step memory allocation: Reject invalid step at submit time\n    rather than leaving it queued.\n -- Whenever possible, avoid allocating nodes that require a reboot.\n\n* Changes in Slurm 16.05.0rc1\n==============================\n -- Remove the SchedulerParameters option of \"assoc_limit_continue\", making it\n    the default value. Add option of \"assoc_limit_stop\". If \"assoc_limit_stop\"\n    is set and a job cannot start due to association limits, then do not attempt\n    to initiate any lower priority jobs in that partition. Setting this can\n    decrease system throughput and utlization, but avoid potentially starving\n    larger jobs by preventing them from launching indefinitely.\n -- Update a node's socket and cores per socket counts as needed after a node\n    boot to reflect configuration changes which can occur on KNL processors.\n    Note that the node's total core count must not change, only the distribution\n    of cores across varying socket counts (KNL NUMA nodes treated as sockets by\n    Slurm).\n -- Rename partition configuration from \"Shared\" to \"OverSubscribe\". Rename\n    salloc, sbatch, srun option from \"--shared\" to \"--oversubscribe\". The old\n    options will continue to function. Output field names also changed in\n    scontrol, sinfo, squeue and sview.\n -- Add SLURM_UMASK environment variable to user job.\n -- knl_conf: Added new configuration parameter of CapmcPollFreq.\n -- squeue: remove errant spaces in column formats for \"squeue -o %all\".\n -- Add ARRAY_TASKS mail option to send emails to each task in a job array.\n -- Change default compression library for sbcast to lz4.\n -- select/cray - Initiate step node health check at start of step termination\n    rather than after application completely ends so that NHC can capture\n    information about hung (non-killable) processes.\n -- Add --units=[KMGTP] option to sacct to display values in specific unit type.\n -- Modify sacct and sacctmgr to display TRES values in converted units.\n -- Modify sacctmgr to accept TRES values with [KMGTP] suffixes.\n -- Replace hash function with more modern SipHash functions.\n -- Add \"--with-cray_dir\" build/configure option.\n -- BB- Only send stage_out email when stage_out is set in script.\n -- Add r/w locking to file_bcast receive functions in slurmd.\n -- Add TopologyParam option of \"TopoOptional\" to optimize network topology\n    only for jobs requesting it.\n -- Fix build on FreeBSD.\n -- Configuration parameter \"CpuFreqDef\" used to set default governor for job\n    step not specifying --cpu-freq (previously the parameter was unused).\n -- Fix sshare -o<format> to correctly display new lengths.\n -- Update documentation to rename Shared option to OverSubscribe.\n -- Update documentation to rename partition Priority option to PriorityTier.\n -- Prevent changing of QOS on running jobs.\n -- Update accounting when changing QOS on pending jobs.\n -- Add support to ntasks_per_socket in task/affinity.\n -- Generate init.d and systemd service scripts in etc/ through Make rather\n    than at configure time to ensure all variable substitutions happen.\n -- Use TaskPluginParam for default task binding if no user specified CPU\n    binding. User --cpu_bind option takes precident over default. No longer\n    any error if user --cpu_bind option does not match TaskPluginParam.\n -- Make sacct and sattach work with older slurmd versions.\n -- Fix protocol handling between 15.08 and 16.05 for 'scontrol show config'.\n -- Enable prefixes (e.g. info, debug, etc.) in slurmstepd debugging.\n\n* Changes in Slurm 16.05.0pre2\n==============================\n -- Split partition's \"Priority\" field into \"PriorityTier\" (used to order\n    partitions for scheduling and preemption) plus \"PriorityJobFactor\" (used by\n    priority/multifactor plugin in calculating job priority, which is used to\n    order jobs within a partition for scheduling).\n -- Revert call to getaddrinfo, restoring gethostbyaddr (introduced in Slurm\n    16.05.0pre1) which was failing on some systems.\n -- knl_cray.conf - Added AllowMCDRAM, AllowNUMA and ALlowUserBoot\n    configuration options.\n -- Add node_features_p_user_update() function to node_features plugin.\n -- Don't print Weight=1 lines in 'scontrol write config' (its the default).\n -- Remove PARAMS macro from slurm.h.\n -- Remove BEGIN_C_DECLS and END_C_DECLS macros.\n -- Check that PowerSave mode configured for node_features/knl_cray plugin.\n    It is required to reconfigure and reboot nodes.\n -- Update documentation to reflect new cgroup default location change from\n    /cgroup to /sys/fs/cgroup.\n -- If NodeHealthCheckProgram configured HealthCheckInterval is non-zero, then\n    modify slurmd to run it before registering with slurmctld.\n -- Fix for tasks being packed onto cores when the requested --cpus-per-task is\n    greater than the number of threads on a core and --ntasks-per-core is 1.\n -- Make it so jobs/steps track ':' named gres/tres, before hand gres/gpu:tesla\n    would only track gres/gpu, now it will track both gres/gpu and\n    gres/gpu:tesla as separate gres if configured like\n    AccountingStorageTRES=gres/gpu,gres/gpu:tesla\n -- Added new job dependency type of \"aftercorr\" which will start a task of a\n    job array after the corresponding task of another job array completes.\n -- Increase default MaxTasksPerNode configuration parameter from 128 to 512.\n -- Enable sbcast data compression logic (compress option previously ignored).\n -- Add --compress option to srun command for use with --bcast option.\n -- Add TCPTimeout option to slurm[dbd].conf. Decouples MessageTimeout from TCP\n    connections.\n -- Don't call primary controller for every RPC when backup is in control.\n -- Add --gres-flags=enforce-binding option to salloc, sbatch and srun commands.\n    If set, the only CPUs available to the job will be those bound to the\n    selected GRES (i.e. the CPUs identifed in the gres.conf file will be\n    strictly enforced rather than advisory).\n -- Change how a node's allocated CPU count is calculated to avoid double\n    counting CPUs allocated to multiple jobs at the same time.\n -- Added SchedulingParameters option of \"bf_min_prio_reserve\". Jobs below\n    the specified threshold will not have resources reserved for them.\n -- Added \"sacctmgr show lostjobs\" to report any orphaned jobs in the database.\n -- When a stepd is about to shutdown and send it's response to srun\n    make the wait to return data only hit after 500 nodes and configurable\n    based on the TcpTimeout value.\n -- Add functionality to reset the lft and rgt values of the association table\n    with the slurmdbd.\n -- Add SchedulerParameter no_env_cache, if set no env cache will be use when\n    launching a job, instead the job will fail and drain the node if the env\n    isn't loaded normally.\n\n* Changes in Slurm 16.05.0pre1\n==============================\n -- Add sbatch \"--wait\" option that waits for job completion before exiting.\n    Exit code will match that of spawned job.\n -- Modify advanced reservation save/restore logic for core reservations to\n    support configuration changes (changes in configured nodes or cores counts).\n -- Allow ControlMachine, BackupController, DbdHost and DbdBackupHost to be\n    either short or long hostname.\n -- Job output and error files can now contain \"%\" character by specifying\n    a file name with two consecutive \"%\" characters. For example,\n    \"sbatch -o \"slurm.%%.%j\" for job ID 123 will generate an output file named\n    \"slurm.%.123\".\n -- Pass user name in Prolog RPC from controller to slurmd when using\n    PrologFlags=Alloc. Allows SLURM_JOB_USER env variable to be set when using\n    Native Slurm on a Cray.\n -- Add \"NumTasks\" to job information visible to Slurm commands.\n -- Add mail wrapper script \"smail\" that will include job statistics in email\n    notification messages.\n -- Remove vestigial \"SICP\" job option (inter-cluster job option). Completely\n    different logic will be forthcoming.\n -- Fix case where the primary and backup dbds would both be performing rollup.\n -- Add an ack reply from slurmd to slurmstepd when job setup is done and the\n    job is ready to be executed.\n -- Removed support for authd. authd has not been developed and supported since\n    several years. \n -- Introduce a new parameter requeue_setup_env_fail in SchedulerParameters.\n    A job that fails to setup the environment will be requeued and the node\n    drained.\n -- Add ValidateTimeout and OtherTimeout to \"scontrol show burst\" output.\n -- Increase default sbcast buffer size from 512KB to 8MB.\n -- Enable the hdf5 profiling of the batch step.\n -- Eliminate redundant environment and script files for job arrays.\n -- Stop searching sbatch scripts for #PBS directives after 100 lines of\n    non-comments. Stop parsing #PBS or #SLURM directives after 1024 characters\n    into a line. Required for decent perforamnce with huge scripts.\n -- Add debug flag for timing Cray portions of the code.\n -- Remove all *.la files from RPMs.\n -- Add Multi-Category Security (MCS) infrastructure to permit nodes to be bound\n    to specific users or groups.\n -- Install the pmi2 unix sockets in slurmd spool directory instead of /tmp.\n -- Implement the getaddrinfo and getnameinfo instead of gethostbyaddr and\n    gethostbyname.\n -- Finished PMIx implementation.\n -- Implemented the --without=package option for configure.\n -- Fix sshare to show each individual cluster with -M,--clusters option.\n -- Added --deadline option to salloc, sbatch and srun. Jobs which can not be\n    completed by the user specified deadline will be terminated with a state of\n    \"Deadline\" or \"DL\".\n -- Implemented and documented PMIX protocol which is used to bootstrap an\n    MPI job. PMIX is an alternative to PMI and PMI2.\n -- Change default CgroupMountpoint (in cgroup.conf) from \"/cgroup\" to\n    \"/sys/fs/cgroup\" to match current standard.\n -- Add #BSUB options to sbatch to read in from the batch script.\n -- HDF: Change group name of node from nodename to nodeid.\n -- The partition-specific SelectTypeParameters parameter can now be used to\n    change the memory allocation tracking specification in the global\n    SelectTypeParameters configuration parameter. Supported partition-specific\n    values are CR_Core, CR_Core_Memory, CR_Socket and CR_Socket_Memory. If the\n    global SelectTypeParameters value includes memory allocation management and\n    the partition-specific value does not, then memory allocation management for\n    that partition will NOT be supported (i.e. memory can be over-allocated).\n    Likewise the global SelectTypeParameters might not include memory management\n    while the partition-specific value does.\n -- Burst buffer/cray - Add support for multiple buffer pools including support\n    for different resource granularity by pool.\n -- Burst buffer advanced reservation units treated as bytes (per documentation)\n    rather than GB.\n -- Add an \"scontrol top <jobid>\" command to re-order the priorities of a user's\n    pending jobs. May be disabled with the \"disable_user_top\" option in the\n    SchedulerParameters configuration parameter.\n -- Modify sview to display negative job nice values.\n -- Increase job's nice value field from 16 to 32 bits.\n -- Remove deprecated job_submit/cnode plugin.\n -- Enhance slurm.conf option EnforcePartLimit to include options like \"ANY\" and\n    \"ALL\".  \"Any\" is equivalent to \"Yes\" and \"All\" will check all partitions\n    a job is submitted to and if any partition limit is violated the job will\n    be rejected even if it could possibly run on another partition.\n -- Add \"features_act\" field (currently active features) to the node\n    information. Output of scontrol, sinfo, and sview changed accordingly.\n    The field previously displayed as \"Features\" is now \"AvailableFeatures\"\n    while the new field is displayed as \"ActiveFeatures\".\n -- Remove Sun Constellation, IBM Federation Switches (replaced by NRT switch\n    plugin) and long-defunct Quadrics Elan support.\n -- Add -M<clusters> option to sreport.\n -- Rework group caching to work better in environments with\n    enumeration disabled. Removed CacheGroups config directive, group\n    membership lists are now always cached, controlled by\n    GroupUpdateTime parameter. GroupUpdateForce parameter default\n    value changed to 1.\n -- Add reservation flag of \"purge_comp\" which will purge an advanced\n    reservation once it has no more active (pending, suspended or running) jobs.\n -- Add new configuration parameter \"KNLPlugins\" and plugin infrastructure.\n -- Add optional job \"features\" to node reboot RPC.\n -- Add slurmd \"-b\" option to report node rebooted at daemon start time. Used\n    for testing purposes.\n -- contribs/cray: Add framework for powering nodes up and down.\n -- For job constraint, convert comma separator to \"&\".\n -- Add Max*PerAccount options for QOS.\n -- Protect slurm_mutex_* calls with abort() on failure.\n\n* Changes in Slurm 15.08.14\n===========================\n\n* Changes in Slurm 15.08.13\n===========================\n -- Fix issue where slurmd could core when running the ipmi energy plugin.\n -- Print correct return code on failure to update node features through sview.\n -- Documentation - cleanup typos.\n -- Add logic so that slurmstepd can be launched under valgrind.\n -- Increase buffer size to read /proc/*/stat files.\n -- MYSQL - Handle ER_HOST_IS_BLOCKED better by failing when it occurs instead\n    of continuously printing the message over and over as the problem will\n    most likely not resolve itself.\n -- Add --disable-bluegene to configure.  This will make it so Slurm\n    can work on a BGAS node.\n -- Prevent job stuck in configuring state if slurmctld daemon restarted while\n    PrologSlurmctld is running.\n -- Handle association correctly if using FAIR_TREE as well as shares=Parent\n -- Fix race condition when setting priority of a job and the association\n    doesn't have a parent.\n -- MYSQL - Fix issue with adding a reservation if the name has single quotes in\n    it.\n -- Correctly print ranges when using step values in job arrays.\n -- Fix for invalid array pointer when creating advanced reservation when job\n    allocations span heterogeneous nodes (differing core or socket counts).\n -- Fix for sstat to print correct info when requesting jobid.batch as part of\n    a comma-separated list.\n -- Cray - Fix issue restoring jobs when blade count increases due to hardware\n    reconfiguration.\n -- Ignore warnings about depricated functions. This is primarily there for\n    new glibc 2.24+ that depricates readdir_r.\n -- Fix security issue caused by insecure file path handling triggered by the\n    failure of a Prolog script. To exploit this a user needs to anticipate or\n    cause the Prolog to fail for their job. CVE-2016-10030.\n\n* Changes in Slurm 15.08.12\n===========================\n -- Do not attempt to power down a node which has never responded if the\n    slurmctld daemon restarts without state.\n -- Fix for possible slurmstepd segfault on invalid user ID.\n -- MySQL - Fix for possible race condition when archiving multiple clusters\n    at the same time.\n -- Fix compile for when you don't have hwloc.\n -- Fix issue where daemons would only listen on specific address given in\n    slurm.conf instead of all.  If looking for specific addresses use\n    TopologyParam options No*InAddrAny.\n -- Cray - Better robustness when dealing with the aeld interface.\n -- job_submit.lua - add array_inx value for job arrays.\n -- Perlapi - Remove unneeded/undefined mutex.\n -- Fix issue when TopologyParam=NoInAddrAny is set the responses wouldn't\n    make it to the slurmctld when using message aggregation.\n -- MySQL - Fix potential memory leak when rolling up data.\n -- Fix issue with clustername file when running on NFS with root_squash.\n -- Fix race condition with respects to cleaning up the profiling threads\n    when in use.\n -- Fix issues when building on NetBSD.\n -- Fix jobcomp/elasticsearch build when libcurl is installed in a\n    non-standard location.\n -- Fix MemSpecLimit to explicitly require TaskPlugin=task/cgroup and\n    ConstrainRAMSpace set in cgroup.conf.\n -- MYSQL - Fix order of operations issue where if the database is locked up\n    and the slurmctld doesn't wait long enough for the response it would give\n    up leaving the connection open and create a situation where the next message\n    sent could receive the response of the first one.\n -- Fix CFULL_BLOCK distribution type.\n -- Prevent sbatch from trying to enable debug messages when using job arrays.\n -- Prevent sbcast from enabling \"--preserve\" when specifying a jobid.\n -- Prevent wrong error message from spank plugin stack on GLOB_NOSPACE error.\n -- Fix proctrack/lua plugin to prevent possible deadlock.\n -- Prevent infinite loop in slurmstepd if execve fails.\n -- Prevent multiple responses to REQUEST_UPDATE_JOB_STEP message.\n -- Prevent possible deadlock in acct_gather_filesystem/lustre on error.\n -- Make it so --mail-type=NONE didn't throw an invalid error.\n -- If no default account is given for a user when creating (only a list of\n    accounts) no default account is printed, previously NULL was printed.\n -- Fix for tracking a node's allocated CPUs with gang scheduling.\n -- Fix Hidden error during _rpc_forward_data call.\n -- Fix bug resulting from wrong order-of-operations in _connect_srun_cr(),\n    and two others that cause incorrect debug messages.\n -- Fix backwards compatibility with sreport going to <= 14.11 coming from\n    >= 15.08 for some reports.\n\n* Changes in Slurm 15.08.11\n===========================\n -- Fix for job \"--contiguous\" option that could cause job allocation/launch\n    failure or slurmctld crash.\n -- Fix to setup logs for single-character program names correctly.\n -- Backfill scheduling performance enhancement with large number of running\n    jobs.\n -- Reset job's prolog_running counter on slurmctld restart or reconfigure.\n -- burst_buffer/cray - Update job's prolog_running counter if pre_run fails.\n -- MYSQL - Make the error message more specific when removing a reservation\n    and it doesn't meet basic requirements.\n -- burst_buffer/cray - Fix for script creating or deleting persistent buffer\n    would fail \"paths\" operation and hold the job.\n -- power/cray - Prevent possible divide by zero.\n -- power/cray - Fix bug introduced in 15.08.10 preventin operation in many\n    cases.\n -- Prevent deadlock for flow of data to the slurmdbd when sending reservation\n    that wasn't set up correctly.\n -- burst_buffer/cray - Don't call Datawarp \"paths\" function if script includes\n    only create or destroy of persistent burst buffer. Some versions of Datawarp\n    software return an error for such scripts, causing the job to be held.\n -- Fix potential issue when adding and removing TRES which could result\n    in the slurmdbd segfaulting.\n -- Add cast to memory limit calculation to prevent integer overflow for\n    very large memory values.\n -- Bluegene - Fix issue with reservations resizing under the covers on a\n    restart of the slurmctld.\n -- Avoid error message of \"Requested cpu_bind option requires entire node to\n    be allocated; disabling affinity\" being generated in some cases where\n    task/affinity and task/cgroup plugins used together.\n -- Fix version issue when packing GRES information between 2 different versions\n    of Slurm.\n -- Fix for srun hanging with OpenMPI and PMIx\n -- Better initialization of node_ptr when dealing with protocol_version.\n -- Fix incorrect type when initializing header of a message.\n -- MYSQL - Fix incorrect usage of limit and union.\n -- MYSQL - Remove 'ignore' from alter ignore when updating a table.\n -- Documentation - update prolog_epilog page to reflect current behavior\n    if the Prolog fails.\n -- Documentation - clarify behavior of 'srun --export=NONE' in man page.\n -- Fix potential gres underflow on restart of slurmctld.\n -- Fix sacctmgr to remove a user who has no associations.\n\n* Changes in Slurm 15.08.10\n===========================\n -- Fix issue where if a slurmdbd rollup lasted longer than 1 hour the\n    rollup would effectively never run again.\n -- Make error message in the pmi2 code to debug as the issue can be expected\n    and retries are done making the error message a little misleading.\n -- Power/cray: Don't specify NID list to Cray APIs. If any of those nodes are\n    not in a ready state, the API returned an error for ALL nodes rather than\n    valid data for nodes in ready state.\n -- Fix potential divide by zero when tree_width=1.\n -- checkpoint/blcr plugin: Fix memory leak.\n -- If using PrologFlags=contain: Don't launch the extern step if a job is\n    cancelled while launching.\n -- Remove duplicates from AccountingStorageTRES\n -- Fix backfill scheduler race condition that could cause invalid pointer in\n    select/cons_res plugin. Bug introduced in 15.08.9.\n -- Avoid double calculation on partition QOS if the job is using the same QOS.\n -- Do not change a job's time limit when updating unrelated field in a job.\n -- Fix situation on a heterogeneous memory cluster where the order of\n    constraints mattered in a job.\n\n* Changes in Slurm 15.08.9\n==========================\n -- BurstBuffer/cray - Defer job cancellation or time limit while \"pre-run\"\n    operation in progress to avoid inconsistent state due to multiple calls\n    to job termination functions.\n -- Fix issue with resizing jobs and limits not be kept track of correctly.\n -- BGQ - Remove redeclaration of job_read_lock.\n -- BGQ - Tighter locks around structures when nodes/cables change state.\n -- Make it possible to change CPUsPerTask with scontrol.\n -- Make it so scontrol update part qos= will take away a partition QOS from\n    a partition.\n -- Fix issue where SocketsPerBoard didn't translate to Sockets when CPUS=\n    was also given.\n -- Add note to slurm.conf man page about setting \"--cpu_bind=no\" as part\n    of SallocDefaultCommand if a TaskPlugin is in use.\n -- Set correct reason when a QOS' MaxTresMins is violated.\n -- Insure that a job is completely launched before trying to suspend it.\n -- Remove historical presentations and design notes. Only distribute\n    maintained doc/html and doc/man directories.\n -- Remove duplicate xmalloc() in task/cgroup plugin.\n -- Backfill scheduler to validate correct job partition for job submitted to\n    multiple partitions.\n -- Force close on exec on first 256 file descriptors when launching a\n    slurmstepd to close potential open ones.\n -- Step GRES value changed from type \"int\" to \"int64_t\" to support larger\n    values.\n -- Fix getting reservations to database when database is down.\n -- Fix issue with sbcast not doing a correct fanout.\n -- Fix issue where steps weren't always getting the gres/tres involved.\n -- Fixed double read lock on getting job's gres/tres.\n -- Fix display for RoutePlugin parameter to display the correct value.\n -- Fix route/topology plugin to prevent segfault in sbcast when in use.\n -- Fix Cray slurmconfgen_smw.py script to use nid as nid, not nic.\n -- Fix Cray NHC spawning on job requeue. Previous logic would leave nodes\n    allocated to a requeued job as non-usable on job termination.\n -- burst_buffer/cray plugin: Prevent a requeued job from being restarted while\n    file stage-out is still in progress. Previous logic could restart the job\n    and not perform a new stage-in.\n -- Fix job array formatting to allow return [0-100:2] display for arrays with\n    step functions rather than [0,2,4,6,8,...] .\n -- FreeBSD - replace Linux-specific set_oom_adj to avoid errors in slurmd log.\n -- Add option for TopologyParam=NoInAddrAnyCtld to make the slurmctld listen\n    on only one port like TopologyParam=NoInAddrAny does for everything else.\n -- Fix burst buffer plugin to prevent corruption of the CPU TRES data when bb\n    is not set as an AccountingStorageTRES type.\n -- Surpress error messages in acct_gather_energy/ipmi plugin after repeated\n    failures.\n -- Change burst buffer use completion email message from\n    \"SLURM Job_id=1360353 Name=tmp Staged Out, StageOut time 00:01:47\" to\n    \"SLURM Job_id=1360353 Name=tmp StageOut/Teardown time 00:01:47\"\n -- Generate burst buffer use completion email immediately afer teardown\n    completes rather than at job purge time (likely minutes later).\n -- Fix issue when adding a new TRES to AccountingStorageTRES for the first\n    time.\n -- Update gang scheduling tables when job manually suspended or resumed. Prior\n    logic could mess up job suspend/resume sequencing.\n -- Update gang scheduling data structures when job changes in size.\n -- Associations - prevent hash table corruption if uid initially unset for\n    a user, which can cause slurmctld to crash if that user is deleted.\n -- Avoid possibly aborting srun on SIGSTOP while creating the job step due to\n    threading bug.\n -- Fix deadlock issue with burst_buffer/cray when a newly created burst\n    buffer is found.\n -- burst_buffer/cray: Set environment variables just before starting job rather\n    than at job submission time to reflect persistent buffers created or\n    modified while the job is pending.\n -- Fix check of per-user qos limits on the initial run by a user.\n -- Fix gang scheduling resource selection bug which could prevent multiple jobs\n    from being allocated the same resources. Bug was introduced in 15.08.6.\n -- Don't print the Rgt value of an association from the cache as it isn't\n    kept up to date.\n -- burst_buffer/cray - If the pre-run operation fails then don't issue\n    duplicate job cancel/requeue unless the job is still in run state. Prevents\n    jobs hung in COMPLETING state.\n -- task/cgroup - Fix bug in task binding to CPUs.\n\n* Changes in Slurm 15.08.8\n==========================\n -- Backfill scheduling properly synchronized with Cray Node Health Check.\n    Prior logic could result in highest priority job getting improperly\n    postponed.\n -- Make it so daemons also support TopologyParam=NoInAddrAny.\n -- If scancel is operating on large number of jobs and RPC responses from\n    slurmctld daemon are slow then introduce a delay in sending the cancel job\n    requests from scancel in order to reduce load on slurmctld.\n -- Remove redundant logic when updating a job's task count.\n -- MySQL - Fix querying jobs with reservations when the id's have rolled.\n -- Perl - Fix use of uninitialized variable in slurm_job_step_get_pids.\n -- Launch batch job requsting --reboot after the boot completes.\n -- Move debug messages like \"not the right user\" from association manager\n    to debug3 when trying to find the correct association.\n -- Fix incorrect logic when querying assoc_mgr information.\n -- Move debug messages to debug3 notifying a gres_bit_alloc was NULL for\n    gres types without a file.\n -- Sanity Check Patch to setup variables for RAPL if in a race for it.\n -- GRES - Fix minor typecast issues.\n -- burst_buffer/cray - Increase size of intermediate variable used to store\n    buffer byte size read from DW instance from 32 to 64-bits to avoid overflow\n    and reporting invalid buffer sizes.\n -- Allow an existing reservation with running jobs to be modified without\n    Flags=IGNORE_JOBS.\n -- srun - don't attempt to execve() a directory with a name matching the\n    requested command\n -- Do not automatically relocate an advanced reservation for individual cores\n    that spans multiple nodes when nodes in that reservation go down (e.g.\n    a 1 core reservation on node \"tux1\" will be moved if node \"tux1\" goes\n    down, but a reservation containing 2 cores on node \"tux1\" and 3 cores on\n    \"tux2\" will not be moved node \"tux1\" goes down). Advanced reservations for\n    whole nodes will be moved by default for down nodes.\n -- Avoid possible double free of memory (and likely abort) for slurmctld in\n    background mode.\n -- contribs/cray/csm/slurmconfgen_smw.py - avoid including repurposed compute\n    nodes in configs.\n -- Support AuthInfo in slurmdbd.conf that is different from the value in\n    slurm.conf.\n -- Fix build on FreeBSD 10.\n -- Fix hdf5 build on ppc64 by using correct fprintf formatting for types.\n -- Fix cosmetic printing of NO_VALs in scontrol show assoc_mgr.\n -- Fix perl api for newer perl versions.\n -- Fix for jobs requesting cpus-per-task (eg. -c3) that exceed the number of\n    cpus on a core.\n -- Remove unneeded perl files from the .spec file.\n -- Flesh out filters for scontrol show assoc_mgr.\n -- Add function to remove assoc_mgr_info_request_t members without freeing\n    structure.\n -- Fix build on some non-glibc systems by updating includes.\n -- Add new PowerParameters options of get_timeout and set_timeout. The default\n    set_timeout was increased from 5 seconds to 30 seconds. Also re-read current\n    power caps periodically or after any failed \"set\" operation.\n -- Fix slurmdbd segfault when listing users with blank user condition.\n -- Save the ClusterName to a file in SaveStateLocation, and use that to\n    verify the state directory belongs to the given cluster at startup to avoid\n    corruption from multiple clusters attempting to share a state directory.\n -- MYSQL - Fix issue when rerolling monthly data to work off correct time\n    period.  This would only hit you if you rerolled a 15.08 prior to this\n    commit.\n -- If FastSchedule=0 is used make sure TRES are set up correctly in accounting.\n -- Fix sreport's truncation of columns with large TRES and not using\n    a parsing option.\n -- Make sure count of boards are restored when slurmctld has option -R.\n -- When determine if a job can fit into a TRES time limit after resources\n    have been selected set the time limit appropriately if the job didn't\n    request one.\n -- Fix inadequate locks when updating a partition's TRES.\n -- Add new assoc_limit_continue flag to SchedulerParameters.\n -- Avoid race in acct_gather_energy_cray if energy requested before available.\n -- MYSQL - Avoid having multiple default accounts when a user is added to\n    a new account and making it a default all at once.\n\n* Changes in Slurm 15.08.7\n==========================\n -- sched/backfill: If a job can not be started within the configured\n    backfill_window, set it's start time to 0 (unknown) rather than the end\n    of the backfill_window.\n -- Remove the 1024-character limit on lines in batch scripts.\n -- burst_buffer/cray: Round up swap size by configured granularity.\n -- select/cray: Log repeated aeld reconnects.\n -- task/affinity: Disable core-level task binding if more CPUs required than\n    available cores.\n -- Preemption/gang scheduling: If a job is suspended at slurmctld restart or\n    reconfiguration time, then leave it suspended rather than resume+suspend.\n -- Don't use lower weight nodes for job allocation when topology/tree used.\n -- BGQ - If a cable goes into error state remove the under lying block on\n    a dynamic system and mark the block in error on a static/overlap system.\n -- BGQ - Fix regression in 9cc4ae8add7f where blocks would be deleted on\n    static/overlap systems when some hardware issue happens when restarting\n    the slurmctld.\n -- Log if CLOUD node configured without a resume/suspend program or suspend\n    time.\n -- MYSQL - Better locking around g_qos_count which was previously unprotected.\n -- Correct size of buffer used for jobid2str to avoid truncation.\n -- Fix allocation/distribution of tasks across multiple nodes when\n    --hint=nomultithread is requested.\n -- If a reservation's nodes value is \"all\" then track the current nodes in the\n    system, even if those nodes change.\n -- Fix formatting if using \"tree\" option with sreport.\n -- Make it so sreport prints out a line for non-existent TRES instead of\n    error message.\n -- Set job's reason to \"Priority\" when higher priority job in that partition\n    (or reservation) can not start rather than leaving the reason set to\n    \"Resources\".\n -- Fix memory corruption when a new non-generic TRES is added to the\n    DBD for the first time.  The corruption is only noticed at shutdown.\n -- burst_buffer/cray - Improve tracking of allocated resources to handle race\n    condition when reading state while buffer allocation is in progress.\n -- If a job is submitted only with -c option and numcpus is updated before\n    the job starts update the cpus_per_task appropriately.\n -- Update salloc/sbatch/srun documentation to mention time granularity.\n -- Fixed memory leak when freeing assoc_mgr_info_msg_t.\n -- Prevent possible use of empty reservation core bitmap, causing abort.\n -- Remove unneeded pack32's from qos_rec when qos_rec is NULL.\n -- Make sacctmgr print MaxJobsPerUser when adding/altering a QOS.\n -- Correct dependency formatting to print array task ids if set.\n -- Update sacctmgr help with current QOS options.\n -- Update slurmstepd to initialize authentication before task launch.\n -- burst_cray/cray: Eliminate need for dedicated nodes.\n -- If no MsgAggregationParams is set don't set the internal string to\n    anything.  The slurmd will process things correctly after the fact.\n -- Fix output from api when printing job step not found.\n -- Don't allow user specified reservation names to disrupt the normal\n    reservation sequeuece numbering scheme.\n -- Fix scontrol to be able to accept TRES as an option when creating\n    a reservation.\n -- contrib/torque/qstat.pl - return exit code of zero even with no records\n    printed for 'qstat -u'.\n -- When a reservation is created or updated, compress user provided node names\n    using hostlist functions (e.g. translate user input of \"Nodes=tux1,tux2\"\n    into \"Nodes=tux[1-2]\").\n -- Change output routines for scontrol show partition/reservation to handle\n    unexpectedly large strings.\n -- Add more partition fields to \"scontrol write config\" output file.\n -- Backfill scheduling fix: If a job can't be started due to a \"group\" resource\n    limit, rather than reserve resources for it when the next job ends, don't\n    reserve any resources for it.\n -- Avoid slurmstepd abort if malloc fails during accounting gather operation.\n -- Fix nodes from being overallocated when allocation straddles multiple nodes.\n -- Fix memory leak in slurmctld job array logic.\n -- Prevent decrementing of TRESRunMins when AccountingStorageEnforce=limits is\n    not set.\n -- Fix backfill scheduling bug which could postpone the scheduling of jobs due\n    to avoidance of nodes in COMPLETING state.\n -- Properly account for memory, CPUs and GRES when slurmctld is reconfigured\n    while there is a suspended job. Previous logic would add the CPUs, but not\n    memory or GPUs. This would result in underflow/overflow errors in select\n    cons_res plugin.\n -- Strip flags from a job state in qstat wrapper before evaluating.\n -- Add missing job states from the qstat wrapper.\n -- Cleanup output routines to reduce number of fixed-sized buffer function\n    calls and allow for unexpectedly large strings.\n\n* Changes in Slurm 15.08.6\n==========================\n -- In slurmctld log file, log duplicate job ID found by slurmd. Previously was\n    being logged as prolog/epilog failure.\n -- If a job is requeued while in the process of being launch, remove it's\n    job ID from slurmd's record of active jobs in order to avoid generating a\n    duplicate job ID error when launched for the second time (which would\n    drain the node).\n -- Cleanup messages when handling job script and environment variables in\n    older directory structure formats.\n -- Prevent triggering gang scheduling within a partition if configured with\n    PreemptType=partition_prio and PreemptMode=suspend,gang.\n -- Decrease parallelism in job cancel request to prevent denial of service\n    when cancelling huge numbers of jobs.\n -- If all ephemeral ports are in use, try using other port numbers.\n -- Revert way lib lua is handled when doing a dlopen, fixing a regression in\n    15.08.5.\n -- Set the debug level of the rmdir message in xcgroup_delete() to debug2.\n -- Fix the qstat wrapper when user is removed from the system but still\n    has running jobs.\n -- Log the request to terminate a job at info level if DebugFlags includes\n    the Steps keyword.\n -- Fix potential memory corruption in _slurm_rpc_epilog_complete as well as\n    _slurm_rpc_complete_job_allocation.\n -- Fix cosmetic display of AccountingStorageEnforce option \"nosteps\" when\n    in use.\n -- If a job can never be started due to unsatisfied job dependencies, report\n    the full original job dependency specification rather than the dependencies\n    remaining to be satisfied (typically NULL).\n -- Refactor logic to synchronize active batch jobs and their script/environment\n    files, reducing overhead dramatically for large numbers of active jobs.\n -- Avoid hard-link/copy of script/environment files for job arrays. Use the\n    master job record file for all tasks of the job array.\n    NOTE: Job arrays submitted to Slurm version 15.08.6 or later will fail if\n    the slurmctld daemon is downgraded to an earlier version of Slurm.\n -- Move slurmctld mail handler to separate thread for improved performance.\n -- Fix containment of adopted processes from pam_slurm_adopt.\n -- If a pending job array has multiple reasons for being in a pending state,\n    then print all reasons in a comma separated list.\n\n* Changes in Slurm 15.08.5\n==========================\n -- Prevent \"scontrol update job\" from updating jobs that have already finished.\n -- Show requested TRES in \"squeue -O tres\" when job is pending.\n -- Backfill scheduler: Test association and QOS node limits before reserving\n    resources for pending job.\n -- burst_buffer/cray: If teardown operations fails, sleep and retry.\n -- Clean up the external pids when using the PrologFlags=Contain feature\n    and the job finishes.\n -- burst_buffer/cray: Support file staging when job lacks job-specific buffer\n    (i.e. only persistent burst buffers).\n -- Added srun option of --bcast to copy executable file to compute nodes.\n -- Fix for advanced reservation of burst buffer space.\n -- BurstBuffer/cray: Add logic to terminate dw_wlm_cli child processes at\n    shutdown.\n -- If job can't be launch or requeued, then terminate it.\n -- BurstBuffer/cray: Enable clearing of burst buffer string on completed job\n    as a means of recovering from a failure mode.\n -- Fix wrong memory free when parsing SrunPortRange=0-0 configuration.\n -- BurstBuffer/cray: Fix job record purging if cancelled from pending state.\n -- BGQ - Handle database throw correctly when syncing users on blocks.\n -- MySQL - Make sure we don't have a NULL string returned when not\n    requesting any specific association.\n -- sched/backfill: If max_rpc_cnt is configured and the backlog of RPCs has\n    not cleared after yielding locks, then continue to sleep.\n -- Preserve the job dependency description displayed in 'scontrol show job'\n    even if the dependee jobs was terminated and cleaned causing the\n    dependent to never run because of DependencyNeverSatisfied.\n -- Correct job task count calculation if only node count and ntasks-per-node\n    options supplied.\n -- Make sure the association manager converts any string to be lower case\n    as all the associations from the database will be lower case.\n -- Sanity check for xcgroup_delete() to verify incoming parameter is valid.\n -- Fix formatting for sacct with variables that switched from uint32_t to\n    uint64_t.\n -- Fix a typo in sacct man page.\n -- Set up extern step to track any children of an ssh if it leaves anything\n    else behind.\n -- Prevent slurmdbd divide by zero if no associations defined at rollup time.\n -- Multifactor - Add sanity check to make sure pending jobs are handled\n    correctly when PriorityFlags=CALCULATE_RUNNING is set.\n -- Add slurmdb_find_tres_count_in_string() to slurm db perl api.\n -- Make lua dlopen() conditional on version found at build.\n -- sched/backfill - Delay backfill scheduler for completing jobs only if\n    CompleteWait configuration parameter is set (make code match documentation).\n -- Release a job's allocated licenses only after epilog runs on all nodes\n    rather than at start of termination process.\n -- Cray job NHC delayed until after burst buffer released and epilog completes\n    on all allocated nodes.\n -- Fix abort of srun if using PrologFlags=NoHold\n -- Let devices step_extern cgroup inherit attributes of job cgroup.\n -- Add new hook to Task plugin to be able to put adopted processes in the\n    step_extern cgroups.\n -- Fix AllowUsers documentation in burst_buffer.conf man page. Usernames are\n    comma separated, not colon delimited.\n -- Fix issue with time limit not being set correctly from a QOS when a job\n    requests no time limit.\n -- Various CLANG fixes.\n -- In both sched/basic and backfill: If a job can not be started due to some\n    account/qos limit, then don't start other jobs which could delay jobs. The\n    old logic would skip the job and start other jobs, which could delay the\n    higher priority job.\n -- select/cray: Prevent NHC from running more than once per job or step.\n -- Fix fields not properly printed when adding an account through sacctmgr.\n -- Update LBNL Node Health Check (NHC) link on FAQ.\n -- Fix multifactor plugin to prevent slurmctld from getting segmentation fault\n    should the tres_alloc_cnt be NULL.\n -- sbatch/salloc - Move nodelist logic before the time min_nodes is used\n    so we can set it correctly before tasks are set.\n\n* Changes in Slurm 15.08.4\n==========================\n -- Fix typo for the \"devices\" cgroup subsystem in pam_slurm_adopt.c\n -- Fix TRES_MAX flag to work correctly.\n -- Improve the systemd startup files.\n -- Added burst_buffer.conf flag parameter of \"TeardownFailure\" which will\n    teardown and remove a burst buffer after failed stage-in or stage-out.\n    By default, the buffer will be preserved for analysis and manual teardown.\n -- Prevent a core dump in srun if the signal handler runs during the job\n    allocation causing the step context to be NULL.\n -- Don't fail job if multiple prolog operations in progress at slurmctld\n    restart time.\n -- Burst_buffer/cray: Fix to purge terminated jobs with burst buffer errors.\n -- Burst_buffer/cray: Don't stall scheduling of other jobs while a stage-in\n    is in progress.\n -- Make it possible to query 'extern' step with sstat.\n -- Make 'extern' step show up in the database.\n -- MYSQL - Quote assoc table name in mysql query.\n -- Make SLURM_ARRAY_TASK_MIN, SLURM_ARRAY_TASK_MAX, and SLURM_ARRAY_TASK_STEP\n    environment variables available to PrologSlurmctld and EpilogSlurmctld.\n -- Fix slurmctld bug in which a pending job array could be canceled\n    by a user different from the owner or the administrator.\n -- Support taking node out of FUTURE state with \"scontrol reconfig\" command.\n -- Sched/backfill: Fix to properly enforce SchedulerParameters of\n    bf_max_job_array_resv.\n -- Enable operator to reset sdiag data.\n -- jobcomp/elasticsearch plugin: Add array_job_id and array_task_id fields.\n -- Remove duplicate #define IS_NODE_POWER_UP.\n -- Added SchedulerParameters option of max_script_size.\n -- Add REQUEST_ADD_EXTERN_PID option to add pid to the slurmstepd's extern\n    step.\n -- Add unique identifiers to anchor tags in HTML generated from the man pages.\n -- Add with_freeipmi option to spec file.\n -- Minor elasticsearch code improvements\n\n* Changes in Slurm 15.08.3\n==========================\n -- Correct Slurm's RPM build if Munge is not installed.\n -- Job array termination status email ExitCode based upon highest exit code\n    from any task in the job array rather than the last task. Also change the\n    state from \"Ended\" or \"Failed\" to \"Mixed\" where appropriate.\n -- Squeue recombines pending job array records only if their name and partition\n    are identical.\n -- Fix some minor leaks in the job info and step info API.\n -- Export missing QOS id when filling in association with the association\n    manager.\n -- Fix invalid reference if a lua job_submit plugin references a default qos\n    when a user doesn't exist in the database.\n -- Use association enforcement in the lua plugin.\n -- Fix a few spots missing defines of accounting_enforce or acct_db_conn\n    in the plugins.\n -- Show requested TRES in scontrol show jobs when job is pending.\n -- Improve sched/backfill support for job features, especially XOR construct.\n -- Correct scheduling logic for job features option with XOR construct that\n    could delay a job's initiation.\n -- Remove unneeded frees when creating a tres string.\n -- Send a tres_alloc_str for the batch step\n -- Fix incorrect check for slurmdb_find_tres_count_in_string in various places,\n    it needed to check for INFINITE64 instead of zero.\n -- Don't allow scontrol to create partitions with the name \"DEFAULT\".\n -- burst_buffer/cray: Change error from \"invalid request\" to \"permssion denied\"\n    if a non-authorized user tries to create/destroy a persistent buffer.\n -- PrologFlags work: Setting a flag of \"Contain\" implicitly sets the \"Alloc\"\n    flag. Fix code path which could prevent execution of the Prolog when the\n    \"Alloc\" or \"Contain\" flag were set.\n -- Fix for acct_gather_energy/cray|ibmaem to work with missed enum.\n -- MYSQL - When inserting a job and begin_time is 0 do not set it to\n    submit_time.  0 means the job isn't eligible yet so we need to treat it so.\n -- MYSQL - Don't display ineligible jobs when querying for a window of time.\n -- Fix creation of advanced reservation of cores on nodes which are DOWN.\n -- Return permission denied if regular user tries to release job held by an\n    administrator.\n -- MYSQL - Fix rollups for multiple jobs running by the same association\n    in an hour counting multiple times.\n -- Burstbuffer/Cray plugin - Fix for persistent burst buffer use.\n    Don't call paths if no #DW options.\n -- Modifications to pam_slurm_adopt to work correctly for the \"extern\" step.\n -- Alphabetize debugflags when printing them out.\n -- Fix systemd's slurmd service from killing slurmstepds on shutdown.\n -- Fixed counter of not indexed jobs, error_cnt post-increment changed to\n    pre-increment.\n\n* Changes in Slurm 15.08.2\n==========================\n -- Fix for tracking node state when jobs that have been allocated exclusive\n    access to nodes (i.e. entire nodes) and later relinquish some nodes. Nodes\n    would previously appear partly allocated and prevent use by other jobs.\n -- Correct some cgroup paths (\"step_batch\" vs. \"step_4294967294\", \"step_exter\"\n    vs. \"step_extern\", and \"step_extern\" vs. \"step_4294967295\").\n -- Fix advanced reservation core selection logic with network topology.\n -- MYSQL - Remove restriction to have to be at least an operator to query TRES\n    values.\n -- For pending jobs have sacct print 0 for nnodes instead of the bogus 2.\n -- Fix for tracking node state when jobs that have been allocated exclusive\n    access to nodes (i.e. entire nodes) and later relinquish some nodes. Nodes\n    would previously appear partly allocated and prevent use by other jobs.\n -- Fix updating job in db after extending job's timelimit past partition's\n    timelimit.\n -- Fix srun -I<timeout> from flooding the controller with step create requests.\n -- Requeue/hold batch job launch request if job already running (possible if\n    node went to DOWN state, but jobs remained active).\n -- If a job's CPUs/task ratio is increased due to configured MaxMemPerCPU,\n    then increase it's allocated CPU count in order to enforce CPU limits.\n -- Don't mark powered down node as not responding. This could be triggered by\n    race condition of the node suspend and ping logic, preventing use of the\n    node.\n -- Don't requeue RPC going out from slurmctld to DOWN nodes (can generate\n    repeating communication errors).\n -- Propagate sbatch \"--dist=plane=#\" option to srun.\n -- Add acct_gather_energy/ibmaem plugin for systems with IBM Systems Director\n    Active Energy Manager.\n -- Fix spec file to look for mariadb or mysql devel packages for build\n    requirements.\n -- MySQL - Improve the code with asking for jobs in a suspended state.\n -- Fix slurcmtld allowing root to see job steps using squeues -s.\n -- Do not send burst buffer stage out email unless the job uses burst buffers.\n -- Fix sacct to not return all jobs if the -j option is given with a trailing\n    ','.\n -- Permit job_submit plugin to set a job's priority.\n -- Fix occasional srun segfault.\n -- Fix issue with sacct, printing 0_0 for array's that had finished in the\n    database but the start record hadn't made it yet.\n -- sacctmgr - Don't allow default account associations to be removed\n    from a user.\n -- Fix sacct -j, (nothing but a comma) to not return all jobs.\n -- Fixed slurmctld not sending cold-start messages correctly to the database\n    when a cold-start (-c) happens to the slurmctld.\n -- Fix case where if the backup slurmdbd has existing connections when it gives\n    up control that the it would be killed.\n -- Fix task/cgroup affinity to work correctly with multi-socket\n    single-threaded cores.  A regression caused only 1 socket to be used on\n    this kind of node instead of all that were available.\n -- MYSQL - Fix minor issue after an index was added to the database it would\n    previously take 2 restarts of the slurmdbd to make it stick correctly.\n -- Add hv_to_qos_cond() and qos_rec_to_hv() functions to the Perl interface.\n -- Add new burst_buffer.conf parameters: ValidateTimeout and OtherTimeout.\n    See man page for details.\n -- Fix burst_buffer/cray support for interactive allocations >4GB.\n -- Correct backfill scheduling logic for job with INFINITE time limit.\n -- Fix issue on a scontrol reconfig all available GRES/TRES would be zeroed\n    out.\n -- Set SLURM_HINT environment variable when --hint is used with sbatch or\n    salloc.\n -- Add scancel -f/--full option to signal all steps including batch script and\n    all of its child processes.\n -- Fix salloc -I to accept an argument.\n -- Avoid reporting more allocated CPUs than exist on a node. This can be\n    triggered by resuming a previosly suspended job, resulting in\n    oversubscription of CPUs.\n -- Fix the pty window manager in slurmstepd not to retry IO operation with\n    srun if it read EOF from the connection with it.\n -- sbatch --ntasks option to take precedence over --ntasks-per-node plus node\n    count, as documented. Set SLURM_NTASKS/SLURM_NPROCS environment variables\n    accordingly.\n -- MYSQL - Make sure suspended time is only subtracted from the CPU TRES\n    as it is the only TRES that can be given to another job while suspended.\n -- Clarify how TRESBillingWeights operates on memory and burst buffers.\n\n* Changes in Slurm 15.08.1\n==========================\n -- Fix test21.30 and 21.34 to check grpwall better.\n -- Add time to the partition QOS the job is running on instead of just the\n    job QOS.\n -- Print usage for GrpJobs, GrpSubmitJobs and GrpWall even if there is no\n    limit.\n -- If AccountingEnforce=safe is set make sure a job can finish before going\n    over the limit with grpwall on a QOS or association.\n -- burst_buffer/cray - Major updates based upon recent Cray changes.\n -- Improve clean up logic of pmi2 plugin.\n -- Improve job state reason string when required nodes not available.\n -- Fix missing else when packing an update partition message\n -- Fix srun from inheriting the SLURM_CPU_BIND and SLURM_MEM_BIND environment\n    variables when running in an existing srun (e.g. an srun within an salloc).\n -- Fix missing else when packing an update partition message.\n -- Use more flexible mechnanism to find json installation.\n -- Make sure safe_limits was initialized before processing limits in the\n    slurmctld.\n -- Fix for burst_buffer/cray to parse type option correctly.\n -- Fix memory error and version number in the nonstop plugin and reservation\n    code.\n -- When requesting GRES in a step check for correct variable for the count.\n -- Fix issue with GRES in steps so that if you have multiple exclusive steps\n    and you use all the GRES up instead of reporting the configuration isn't\n    available you hold the requesting step until the GRES is available.\n -- MYSQL - Change debug to print out with DebugFlags=DB_Step instead of debug4\n -- Simplify code when user is selecting a job/step/array id and removed\n    anomaly when only asking for 1 (task_id was never set to INFINITE).\n -- MYSQL - If user is requesting various task_ids only return requested steps.\n -- Fix issue when tres cnt for energy is 0 for total reported.\n -- Resolved scalability issues of power adaptive scheduling with layouts.\n -- Burst_buffer/cray bug - Fix teardown race condition that can result in\n    infinite loop.\n -- Add support for --mail-type=NONE option.\n -- Job \"--reboot\" option automatically, set's exclusive node mode.\n -- Fix memory leak when using PrologFlags=Alloc.\n -- Fix truncation of job reason in squeue.\n -- If a node is in DOWN or DRAIN state, leave it unavailable for allocation\n    when powered down.\n -- Update the slurm.conf man page documenting better nohold_on_prolog_fail\n    variable.\n -- Don't trucate task ID information in \"squeue --array/-r\" or \"sview\".\n -- Fix a bug which caused scontrol to core dump when releasing or\n    holding a job by name.\n -- Fix unit conversion bug in slurmd which caused wrong memory calculation\n    for cgroups.\n -- Fix issue with GRES in steps so that if you have multiple exclusive steps\n    and you use all the GRES up instead of reporting the configuration isn't\n    available you hold the requesting step until the GRES is available.\n -- Fix slurmdbd backup to use DbdAddr when contacting the primary.\n -- Fix error in MPI documentation.\n -- Fix to handle arrays with respect to number of jobs submitted.  Previously\n    only 1 job was accounted (against MaxSubmitJob) for when an array was\n    submitted.\n -- Correct counting for job array limits, job count limit underflow possible\n    when master cancellation of master job record.\n -- Combine 2 _valid_uid_gid functions into a single function to avoid\n    diversion.\n -- Pending job array records will be combined into single line by default,\n    even if started and requeued or modified.\n -- Fix sacct --format=nnodes to print out correct information for pending\n    jobs.\n -- Make is so 'scontrol update job 1234 qos='' will set the qos back to\n    the default qos for the association.\n -- Add [Alloc|Req]Nodes to sacct to be more like cpus.\n -- Fix sacct documentation about [Alloc|Req]TRES\n -- Put node count in TRES string for steps.\n -- Fix issue with wrong protocol version when using the srun --no-allocate\n    option.\n -- Fix TRES counts on GRES on a clean start of the slurmctld.\n -- Add ability to change a job array's maximum running task count:\n    \"scontrol update jobid=# arraytaskthrottle=#\"\n\n* Changes in Slurm 15.08.0\n==========================\n -- Fix issue with frontend systems (outside ALPs or BlueGene) where srun\n    wouldn't get the correct protocol version to launch a step.\n -- Fix for message aggregation return rpcs where none of the messages are\n    intended for the head of the tree.\n -- Fix segfault in sreport when there was no response from the dbd.\n -- ALPS - Fix compile to not link against -ljob and -lexpat with every lib\n    or binary.\n -- Fix testing for CR_Memory when CR_Memory and CR_ONE_TASK_PER_CORE are used\n    with select/linear.\n -- When restarting or reconfiging the slurmctld, if job is completing handle\n    accounting correctly to avoid meaningless errors about overflow.\n -- Add AccountingStorageTRES to scontrol show config\n -- MySQL - Fix minor memory leak if a connection ever goes away whist using it.\n -- ALPS - Make it so srun --hint=nomultithread works correctly.\n -- Make MaxTRESPerUser work in sacctmgr.\n -- Fix handling of requeued jobs with steps that are still finishing.\n -- Cleaner copy for PriorityWeightTRES, it also fixes a core dump when trying\n    to free it otherwise.\n -- Add environment variables SLURM_ARRAY_TASK_MAX, SLURM_ARRAY_TASK_MIN,\n    SLURM_ARRAY_TASK_STEP for job arrays.\n -- Fix srun to use the NoInAddrAny TopologyParam option.\n -- Change QOS flag name from PartitionQOS to OverPartQOS to be a better\n    description.\n -- Fix rpmbuild issue on Centos7.\n\n* Changes in Slurm 15.08.0rc1\n==============================\n -- Added power_cpufreq layout.\n -- Make complete_batch_script RPC work with message aggregation.\n -- Do not count slurmctld threads waiting in a \"throttle\" lock against the\n    daemon's thread limit as they are not contending for resources.\n -- Modify slurmctld outgoing RPC logic to support more parallel tasks (up to\n    85 RPCs and 256 pthreads; the old logic supported up to 21 RPCs and 256\n    threads). This change can dramatically improve performance for RPCs\n    operating on small node counts.\n -- Increase total backfill scheduler run time in stats_info_response_msg data\n    structure from 32 to 64 bits in order to prevent overflow.\n -- Add NoInAddrAny option to TopologyParam in the slurm.conf which allows to\n    bind to the interface of return of gethostname instead of any address on\n    the node which avoid RSIP issues in Cray systems.  This is most likely\n    useful in other systems as well.\n -- Fix memory leak in Slurm::load_jobs perl api call.\n -- Added --noconvert option to sacct, sstat, squeue and sinfo which allows\n    values to be displayed in their original unit types (e.g. 2048M won't be\n    converted to 2G).\n -- Fix spelling of node_rescrs to node_resrcs in Perl API.\n -- Fix node state race condition, UNKNOWN->IDLE without configuration info.\n -- Cray: Disable LDAP references from slurmstepd on job launch due for\n    improved scalability.\n -- Remove srun \"read header error\" due to application termination race\n    condition.\n -- Optimize sacct queries with additional db indexes.\n -- Add SLURM_TOPO_LEN env variable for scontrol show topology.\n -- Add free_mem to node information.\n -- Fix abort of batch launch if prolog is running, wait for prolog instead.\n -- Fix case where job would get the wrong cpu count when using\n    --ntasks-per-core and --cpus-per-task together.\n -- Add TRESBillingWeights to partitions in slurm.conf which allows taking into\n    consideration any TRES Type when calculating the usage of a job.\n -- Add PriorityWeightTRES slurm.conf option to be able to configure priority\n    factors for TRES types.\n\n* Changes in Slurm 15.08.0pre6\n==============================\n -- Add scontrol options to view and modify layouts tables.\n -- Add MsgAggregationParams which controls a reverse tree to the slurmctld\n    which can be used to aggregate messages to the slurmctld into a single\n    message to reduce communication to the slurmctld.  Currently only epilog\n    complete messages and node registration messages use this logic.\n -- Add sacct and squeue options to print trackable resources.\n -- Add sacctmgr option to display trackable resources.\n -- If an salloc or srun command is executed on a \"front-end\" configuration,\n    that job will be assigned a slurmd shepherd daemon on the same host as used\n    to execute the command when possible rather than an slurmd daemon on an\n    arbitrary front-end node.\n -- Add srun --accel-bind option to control how tasks are bound to GPUs and NIC\n    Generic RESources (GRES).\n -- gres/nic plugin modified to set OMPI_MCA_btl_openib_if_include environment\n    variable based upon allocated devices (usable with OpenMPI and Melanox).\n -- Make it so info options for srun/salloc/sbatch print with just 1 -v instead\n    of 4.\n -- Add \"no_backup_scheduling\" SchedulerParameter to prevent jobs from being\n    scheduled when the backup takes over. Jobs can be submitted, modified and\n    cancelled while the backup is in control.\n -- Enable native Slurm backup controller to reside on an external Cray node\n    when the \"no_backup_scheduling\" SchedulerParameter is used.\n -- Removed TICKET_BASED fairshare. Consider using the FAIR_TREE algorithm.\n -- Disable advanced reservation \"REPLACE\" option on IBM Bluegene systems.\n -- Add support for control distribution of tasks across cores (in addition\n    to existing support for nodes and sockets, (e.g. \"block\", \"cyclic\" or\n    \"fcyclic\" task distribution at 3 levels in the hardware rather than 2).\n -- Create db index on <cluster>_assoc_table.acct. Deleting accounts that didn't\n    have jobs in the job table could take a long time.\n -- The performance of Profiling with HDF5 is improved. In addition, internal\n    structures are changed to make it easier to add new profile types,\n    particularly energy sensors. sh5util will continue to work with either\n    format.\n -- Add partition information to sshare output if the --partition option\n    is specified on the sshare command line.\n -- Add sreport -T/--tres option to identify Trackable RESources (TRES) to\n    report.\n -- Display job in sacct when single step's cpus are different from the job\n    allocation.\n -- Add association usage information to \"scontrol show cache\" command output.\n -- MPI/MVAPICH plugin now requires Munge for authentication.\n -- job_submit/lua: Add default_qos fields. Add job record qos.  Add partition\n    record allow_qos and qos_char fields.\n\n* Changes in Slurm 15.08.0pre5\n==============================\n -- Add jobcomp/elasticsearch plugin. Libcurl is required for build. Configure\n    the server as follows: \"JobCompLoc=http://YOUR_ELASTICSEARCH_SERVER:9200\".\n -- Scancel logic large re-written to better support job arrays.\n -- Added a slurm.conf parameter PrologEpilogTimeout to control how long\n    prolog/epilog can run.\n -- Added TRES (Trackable resources) to track Mem, GRES, license, etc\n    utilization.\n -- Add re-entrant versions of glibc time functions (e.g. localtime) to Slurm\n    in order to eliminate rare deadlock of slurmstepd fork and exec calls.\n -- Constrain kernel memory (if available) in cgroups.\n -- Add PrologFlags option of \"Contain\" to create a proctrack container at\n    job resource allocation time.\n -- Disable the OOM Killer in slurmd and slurmstepd's memory cgroup when using\n    MemSpecLimit.\n\n* Changes in Slurm 15.08.0pre4\n==============================\n -- Burst_buffer/cray - Convert logic to use new commands/API names (e.g.\n    \"dws_setup\" rather than \"bbs_setup\").\n -- Remove the MinJobAge size limitation. It can now exceed 65533 as it\n    is represented using an unsigned integer.\n -- Verify that all plugin version numbers are identical to the component\n    attempting to load them. Without this verification, the plugin can reference\n    Slurm functions in the caller which differ (e.g. the underlying function's\n    arguments could have changed between Slurm versions).\n    NOTE: All plugins (except SPANK) must be built against the identical\n    version of Slurm in order to be used by any Slurm command or daemon. This\n    should eliminate some very difficult to diagnose problems due to use of old\n    plugins.\n -- Increase the MAX_PACK_MEM_LEN define to avoid PMI2 failure when fencing\n    with large amount of ranks (to 1GB).\n -- Requests by normal user to reset a job priority (even to lower it) will\n    result in an error saying to change the job's nice value instead.\n -- SPANK naming changes: For environment variables set using the\n    spank_job_control_setenv() function, the values were available in the\n    slurm_spank_job_prolog() and slurm_spank_job_epilog() functions using\n    getenv where the name was given a prefix of \"SPANK_\". That prefix has\n    been removed for consistency with the environment variables available in\n    the Prolog and Epilog scripts.\n -- Major additions to the layouts framework code.\n -- Add \"TopologyParam\" configuration parameter. Optional value of \"dragonfly\"\n    is supported.\n -- Optimize resource allocation for systems with dragonfly networks.\n -- Add \"--thread-spec\" option to salloc, sbatch and srun commands. This is\n    the count of threads reserved for system use per node.\n -- job_submit/lua: Enable reading and writing job environment variables.\n    For example: if (job_desc.environment.LANGUAGE == \"en_US\") then ...\n -- Added two new APIs slurm_job_cpus_allocated_str_on_node_id()\n    and slurm_job_cpus_allocated_str_on_node() to print the CPUs id\n    allocated to a job.\n -- Specialized memory (a node's MemSpecLimit configuration parameter) is not\n    available for allocation to jobs.\n -- Modify scontrol update job to allow jobid specification without\n    the = sign. 'scontrol update job=123 ...' and 'scontrol update job 123 ...'\n    are both valid syntax.\n -- Archive a month at a time when there are lots of records to archive.\n -- Introduce new sbatch option '--kill-on-invalid-dep=yes|no' which allows\n    users to specify which behavior they want if a job dependency is not\n    satisfied.\n -- Add Slurmdb::qos_get() interface to perl api.\n -- If a job fails to start set the requeue reason to be:\n    job requeued in held state.\n -- Implemented a new MPI key,value PMIX_RING() exchange algorithm as\n    an alternative to PMI2.\n -- Remove possible deadlocks in the slurmctld when the slurmdbd is busy\n    archiving/purging.\n -- Add DB_ARCHIVE debug flag for filtering out debug messages in the slurmdbd\n    when the slurmdbd is archiving/purging.\n -- Fix some power_save mode issues: Parsing of SuspendTime in slurm.conf was\n    bad, powered down nodes would get set non-responding if there was an\n    in-flight message, and permit nodes to be powered down from any state.\n -- Initialize variables in consumable resource plugin to prevent core dump.\n\n* Changes in Slurm 15.08.0pre3\n==============================\n -- CRAY - addition of acct_gather_energy/cray plugin.\n -- Add job credential to \"Run Prolog\" RPC used with a configuration of\n    PrologFlags=alloc. This allows the Prolog to be passed identification of\n    GPUs allocated to the job.\n -- Add SLURM_JOB_CONSTAINTS to environment variables available to the Prolog.\n -- Added \"--mail=stage_out\" option to job submission commands to notify user\n    when burst buffer state out is complete.\n -- Require a \"Reason\" when using scontrol to set a node state to DOWN.\n -- Mail notifications on job BEGIN, END and FAIL now apply to a job array as a\n    whole rather than generating individual email messages for each task in the\n    job array.\n -- task/affinity - Fix memory binding to NUMA with cpusets.\n -- Display job's estimated NodeCount based off of partition's configured\n    resources rather than the whole system's.\n -- Add AuthInfo option of \"cred_expire=#\" to specify the lifetime of a job\n    step credential. The default value was changed from 1200 to 120 seconds.\n -- Set the delay time for job requeue to the job credential lifetime (120\n    seconds by default). This insures that prolog runs on every node when a\n    job is requeued. (This change will slow down launch of re-queued jobs).\n -- Add AuthInfo option of \"cred_expire=#\" to specify the lifetime of a job\n    step credential.\n -- Remove srun --max-launch-time option. The option has not been functional\n    since Slurm version 2.0.\n -- Add sockets and cores to TaskPluginParams' autobind option.\n -- Added LaunchParameters configuration parameter. Have srun command test\n    locally for the executable file if LaunchParameters=test_exec or the\n    environment variable SLURM_TEST_EXEC is set. Without this an invalid\n    command will generate one error message per task launched.\n -- Fix the slurm /etc/init.d script to return 0 upon stopping the\n    daemons and return 1 in case of failure.\n -- Add the ability for a compute node to be allocated to multiple jobs, but\n    restricted to a single user. Added \"--exclusive=user\" option to salloc,\n    sbatch and srun commands. Added \"owner\" field to node record, visible using\n    the scontrol and sview commands. Added new partition configuration parameter\n    \"ExclusiveUser=yes|no\".\n\n* Changes in Slurm 15.08.0pre2\n==============================\n -- Add the environment variables SLURM_JOB_ACCOUNT, SLURM_JOB_QOS\n    and SLURM_JOB_RESERVATION in the batch/srun jobs.\n -- Add sview burst buffer display.\n -- Properly enforce partition Shared=YES option. Previously oversubscribing\n    resources required gang scheduling to be configured.\n -- Enable per-partition gang scheduling resource resolution (e.g. the partition\n    can have SelectTypeParameters=CR_CORE, while the global value is CR_SOCKET).\n -- Make it so a newer version of a slurmstepd can talk to an older srun.\n    allocation. Nodes could have been added while waiting for an allocation.\n -- Expanded --cpu-freq parameters to include min-max:governor specifications.\n    --cpu-freq now supported on salloc and sbatch.\n -- Add support for optimized job allocations with respect to SGI Hypercube\n    topology.\n    NOTE: Only supported with select/linear plugin.\n    NOTE: The program contribs/sgi/netloc_to_topology can be used to build\n    Slurm's topology.conf file.\n -- Remove 64k validation of incoming RPC nodelist size. Validated at 64MB\n    when unpacking.\n -- In slurmstepd() add the user primary group if it is not part of the\n    groups sent from the client.\n -- Added BurstBuffer field to advanced reservations.\n -- For advanced reservation, replace flag \"License_only\" with flag \"Any_Nodes\".\n    It can be used to indicate the an advanced reservation resources (licenses\n    and/or burst buffers) can be used with any compute nodes.\n -- Allow users to specify the srun --resv-ports as 0 in which case no ports\n    will be reserved. The default behaviour is to allocate one port per task.\n -- Interpret a partition configuration of \"Nodes=ALL\" in slurm.conf as\n    including all nodes defined in the cluster.\n -- Added new configuration parameters PowerParameters and PowerPlugin.\n -- Added power management plugin infrastructure.\n -- If job already exceeded one of its QOS/Accounting limits do not\n    return error if user modifies QOS unrelated job settings.\n -- Added DebugFlags value of \"Power\".\n -- When caching user ids of AllowGroups use both getgrnam_r() and getgrent_r()\n    then remove eventual duplicate entries.\n -- Remove rpm dependency between slurm-pam and slurm-devel.\n -- Remove support for the XCPU (cluster management) package.\n -- Add Slurmdb::jobs_get() interface to perl api.\n -- Performance improvement when sending data from srun to stepds when\n    processing fencing.\n -- Add the feature to specify arbitrary field separator when running\n    sacct -p or sacct -P. The command line option is --separator.\n -- Introduce slurm.conf parameter to use Proportional Set Size (PSS) instead\n    of RSS to determinate the memory footprint of a job.\n    Add an slurm.conf option not to kill jobs that is over memory limit.\n -- Add job submission command options: --sicp (available for inter-cluster\n    dependencies) and --power (specify power management options) to salloc,\n    sbatch, and srun commands.\n -- Add DebugFlags option of SICP (inter-cluster option logging).\n -- In order to support inter-cluster job dependencies, the MaxJobID\n    configuration parameter default value has been reduced from 4,294,901,760\n    to 2,147,418,112 and it's maximum value is now 2,147,463,647.\n    ANY JOBS WITH A JOB ID ABOVE 2,147,463,647 WILL BE PURGED WHEN SLURM IS\n    UPGRADED FROM AN OLDER VERSION!\n -- Add QOS name to the output of a partition in squeue/scontrol/sview/smap.\n\n* Changes in Slurm 15.08.0pre1\n==============================\n -- Add sbcast support for file transfer to resources allocated to a job step\n    rather than a job allocation.\n -- Change structures with association in them to assoc to save space.\n -- Add support for job dependencies jointed with OR operator (e.g.\n    \"--depend=afterok:123?afternotok:124\").\n -- Add \"--bb\" (burst buffer specification) option to salloc, sbatch, and srun.\n -- Added configuration parameters BurstBufferParameters and BurstBufferType.\n -- Added burst_buffer plugin infrastructure (needs many more functions).\n -- Make it so when the fanout logic comes across a node that is down we abandon\n    the tree to avoid worst case scenarios when the entire branch is down and\n    we have to try each serially.\n -- Add better error reporting of invalid partitions at submission time.\n -- Move will-run test for multiple clusters from the sbatch code into the API\n    so that it can be used with DRMAA.\n -- If a non-exclusive allocation requests --hint=nomultithread on a\n    CR_CORE/SOCKET system lay out tasks correctly.\n -- Avoid including unused CPUs in a job's allocation when cores or sockets are\n    allocated.\n -- Added new job state of STOPPED indicating processes have been stopped with a\n    SIGSTOP (using scancel or sview), but retain its allocated CPUs. Job state\n    returns to RUNNING when SIGCONT is sent (also using scancel or sview).\n -- Added EioTimeout parameter to slurm.conf. It is the number of seconds srun\n    waits for slurmstepd to close the TCP/IP connection used to relay data\n    between the user application and srun when the user application terminates.\n -- Remove slurmctld/dynalloc plugin as the work was never completed, so it is\n    not worth the effort of continued support at this time.\n -- Remove DynAllocPort configuration parameter.\n -- Add advance reservation flag of \"replace\" that causes allocated resources\n    to be replaced with idle resources. This maintains a pool of available\n    resources that maintains a constant size (to the extent possible).\n -- Added SchedulerParameters option of \"bf_busy_nodes\". When selecting\n    resources for pending jobs to reserve for future execution (i.e. the job\n    can not be started immediately), then preferentially select nodes that are\n    in use. This will tend to leave currently idle resources available for\n    backfilling longer running jobs, but may result in allocations having less\n    than optimal network topology. This option is currently only supported by\n    the select/cons_res plugin.\n -- Permit \"SuspendTime=NONE\" as slurm.conf value rather than only a numeric\n    value to match \"scontrol show config\" output.\n -- Add the 'scontrol show cache' command which displays the associations\n    in slurmctld.\n -- Test more frequently for node boot completion before starting a job.\n    Provides better responsiveness.\n -- Fix PMI2 singleton initialization.\n -- Permit PreemptType=qos and PreemptMode=suspend,gang to be used together.\n    A high-priority QOS job will now oversubscribe resources and gang schedule,\n    but only if there are insufficient resources for the job to be started\n    without preemption. NOTE: That with PreemptType=qos, the partition's\n    Shared=FORCE:# configuration option will permit one job more per resource\n    to be run than than specified, but only if started by preemption.\n -- Remove the CR_ALLOCATE_FULL_SOCKET configuration option.  It is now the\n    default.\n -- Fix a race condition in PMI2 when fencing counters can be out of sync.\n -- Increase the MAX_PACK_MEM_LEN define to avoid PMI2 failure when fencing\n    with large amount of ranks.\n -- Add QOS option to a partition.  This will allow a partition to have\n    all the limits a QOS has.  If a limit is set in both QOS the partition\n    QOS will override the job's QOS unless the job's QOS has the\n    OverPartQOS flag set.\n -- The task_dist_states variable has been split into \"flags\" and \"base\"\n    components. Added SLURM_DIST_PACK_NODES and SLURM_DIST_NO_PACK_NODES values\n    to give user greater control over task distribution. The srun --dist options\n    has been modified to accept a \"Pack\" and \"NoPack\" option. These options can\n    be used to override the CR_PACK_NODE configuration option.\n\n* Changes in Slurm 14.11.12\n===========================\n -- Correct dependency formatting to print array task ids if set.\n -- Fix for configuration of \"AuthType=munge\" and \"AuthInfo=socket=...\" with\n    alternate munge socket path.\n -- BGQ - Remove redeclaration of job_read_lock.\n -- BGQ - Tighter locks around structures when nodes/cables change state.\n -- Fix job array formatting to allow return [0-100:2] display for arrays with\n    step functions rather than [0,2,4,6,8,...] .\n -- Associations - prevent hash table corruption if uid initially unset for\n    a user, which can cause slurmctld to crash if that user is deleted.\n -- Add cast to memory limit calculation to prevent integer overflow for\n    very large memory values.\n -- Fix test cases to have proper int return signature.\n\n* Changes in Slurm 14.11.11\n===========================\n -- Fix systemd's slurmd service from killing slurmstepds on shutdown.\n -- Fix the qstat wrapper when user is removed from the system but still\n    has running jobs.\n -- Log the request to terminate a job at info level if DebugFlags includes\n    the Steps keyword.\n -- Fix potential memory corruption in _slurm_rpc_epilog_complete as well as\n    _slurm_rpc_complete_job_allocation.\n -- Fix incorrectly sized buffer used by jobid2str which will cause buffer\n    overflow in slurmctld. (Bug 2295.)\n\n* Changes in Slurm 14.11.10\n===========================\n -- Fix truncation of job reason in squeue.\n -- If a node is in DOWN or DRAIN state, leave it unavailable for allocation\n    when powered down.\n -- Update the slurm.conf man page documenting better nohold_on_prolog_fail\n    variable.\n -- Don't trucate task ID information in \"squeue --array/-r\" or \"sview\".\n -- Fix a bug which caused scontrol to core dump when releasing or\n    holding a job by name.\n -- Fix unit conversion bug in slurmd which caused wrong memory calculation\n    for cgroups.\n -- Fix issue with GRES in steps so that if you have multiple exclusive steps\n    and you use all the GRES up instead of reporting the configuration isn't\n    available you hold the requesting step until the GRES is available.\n -- Fix slurmdbd backup to use DbdAddr when contacting the primary.\n -- Fix error in MPI documentation.\n -- Fix to handle arrays with respect to number of jobs submitted.  Previously\n    only 1 job was accounted (against MaxSubmitJob) for when an array was\n    submitted.\n -- Correct counting for job array limits, job count limit underflow possible\n    when master cancellation of master job record.\n -- For pending jobs have sacct print 0 for nnodes instead of the bogus 2.\n -- Fix for tracking node state when jobs that have been allocated exclusive\n    access to nodes (i.e. entire nodes) and later relinquish some nodes. Nodes\n    would previously appear partly allocated and prevent use by other jobs.\n -- Fix updating job in db after extending job's timelimit past partition's\n    timelimit.\n -- Fix srun -I<timeout> from flooding the controller with step create requests.\n -- Requeue/hold batch job launch request if job already running (possible if\n    node went to DOWN state, but jobs remained active).\n -- If a job's CPUs/task ratio is increased due to configured MaxMemPerCPU,\n    then increase it's allocated CPU count in order to enforce CPU limits.\n -- Don't mark powered down node as not responding. This could be triggered by\n    race condition of the node suspend and ping logic.\n -- Don't requeue RPC going out from slurmctld to DOWN nodes (can generate\n    repeating communication errors).\n -- Propagate sbatch \"--dist=plane=#\" option to srun.\n -- Fix sacct to not return all jobs if the -j option is given with a trailing\n    ','.\n -- Permit job_submit plugin to set a job's priority.\n -- Fix occasional srun segfault.\n -- Fix issue with sacct, printing 0_0 for array's that had finished in the\n    database but the start record hadn't made it yet.\n -- Fix sacct -j, (nothing but a comma) to not return all jobs.\n -- Prevent slurmstepd from core dumping if /proc/<pid>/stat has\n    unexpected format.\n\n* Changes in Slurm 14.11.9\n==========================\n -- Correct \"sdiag\" backfill cycle time calculation if it yields locks. A\n    microsecond value was being treated as a second value resulting in an\n    overflow in the calcuation.\n -- Fix segfault when updating timelimit on jobarray task.\n -- Fix to job array update logic that can result in a task ID of 4294967294.\n -- Fix of job array update, previous logic could fail to update some tasks\n    of a job array for some fields.\n -- CRAY - Fix seg fault if a blade is replaced and slurmctld is restarted.\n -- Fix plane distribution to allocate in blocks rather than cyclically.\n -- squeue - Remove newline from job array ID value printed.\n -- squeue - Enable filtering for job state SPECIAL_EXIT.\n -- Prevent job array task ID being inappropriately set to NO_VAL.\n -- MYSQL - Make it so you don't have to restart the slurmctld\n    to gain the correct limit when a parent account is root and you\n    remove a subaccount's limit which exists on the parent account.\n -- MYSQL - Close chance of setting the wrong limit on an association\n    when removing a limit from an association on multiple clusters\n    at the same time.\n -- MYSQL - Fix minor memory leak when modifying an association but no\n    change was made.\n -- srun command line of either --mem or --mem-per-cpu will override both the\n    SLURM_MEM_PER_CPU and SLURM_MEM_PER_NODE environment variables.\n -- Prevent slurmctld abort on update of advanced reservation that contains no\n    nodes.\n -- ALPS - Revert commit 2c95e2d22 which also removes commit 2e2de6a4 allowing\n    cray with the SubAllocate option to work as it did with 2.5.\n -- Properly parse CPU frequency data on POWER systems.\n -- Correct sacct.a man pages describing -i option.\n -- Capture salloc/srun information in sdiag statistics.\n -- Fix bug in node selection with topology optimization.\n -- Don't set distribution when srun requests 0 memory.\n -- Read in correct number of nodes from SLURM_HOSTFILE when specifying nodes\n    and --distribution=arbitrary.\n -- Fix segfault in Bluegene setups where RebootQOSList is defined in\n    bluegene.conf and accounting is not setup.\n -- MYSQL - Update mod_time when updating a start job record or adding one.\n -- MYSQL - Fix issue where if an association id ever changes on at least a\n    portion of a job array is pending after it's initial start in the\n    database it could create another row for the remain array instead\n    of using the already existing row.\n -- Fix scheduling anomaly with job arrays submitted to multiple partitions,\n    jobs could be started out of priority order.\n -- If a host has suspened jobs do not reboot it. Reboot only hosts\n    with no jobs in any state.\n -- ALPS - Fix issue when using --exclusive flag on srun to do the correct\n    thing (-F exclusive) instead of -F share.\n -- Fix various memory leaks in the Perl API.\n -- Fix a bug in the controller which display jobs in CF state as RUNNING.\n -- Preserve advanced _core_ reservation when nodes added/removed/resized on\n    slurmctld restart. Rebuild core_bitmap as needed.\n -- Fix for non-standard Munge port location for srun/pmi use.\n -- Fix gang scheduling/preemption issue that could cancel job at startup.\n -- Fix a bug in squeue which prevented squeue -tPD to print array jobs.\n -- Sort job arrays in job queue according to array_task_id when priorities are\n    equal.\n -- Fix segfault in sreport when there was no response from the dbd.\n -- ALPS - Fix compile to not link against -ljob and -lexpat with every lib\n    or binary.\n -- Fix testing for CR_Memory when CR_Memory and CR_ONE_TASK_PER_CORE are used\n    with select/linear.\n -- MySQL - Fix minor memory leak if a connection ever goes away whist using it.\n -- ALPS - Make it so srun --hint=nomultithread works correctly.\n -- Prevent job array task ID from being reported as NO_VAL if last task in the\n    array gets requeued.\n -- Fix some potential deadlock issues when state files don't exist in the\n    association manager.\n -- Correct RebootProgram logic when executed outside of a maintenance\n    reservation.\n -- Requeue job if possible when slurmstepd aborts.\n\n* Changes in Slurm 14.11.8\n==========================\n -- Eliminate need for user to set user_id on job_update calls.\n -- Correct list of unavailable nodes reported in a job's \"reason\" field when\n    that job can not start.\n -- Map job --mem-per-cpu=0 to --mem=0.\n -- Fix squeue -o %m and %d unit conversion to Megabytes.\n -- Fix issue with incorrect time calculation in the priority plugin when\n    a job runs past it's time limit.\n -- Prevent users from setting job's partition to an invalid partition.\n -- Fix sreport core dump when requesting\n    'job SizesByAccount grouping=individual'.\n -- select/linear: Correct count of CPUs allocated to job on system with\n    hyperthreads.\n -- Fix race condition where last array task might not get updated in the db.\n -- CRAY - Remove libpmi from rpm install\n -- Fix squeue -o %X output to correctly handle NO_VAL and suffix.\n -- When deleting a job from the system set the job_id to 0 to avoid memory\n    corruption if thread uses the pointer basing validity off the id.\n -- Fix issue where sbatch would set ntasks-per-node to 0 making any srun\n    afterward cause a divide by zero error.\n -- switch/cray: Refine logic to set PMI_CRAY_NO_SMP_ENV environment variable.\n -- When sacctmgr loads archives with version less than 14.11 set the array\n    task id to NO_VAL, so sacct can display the job ids correctly.\n -- When using memory cgroup if a task uses more memory than requested\n    the failures are logged into memory.failcnt count file by cgroup\n    and the user is notified by slurmstepd about it.\n -- Fix scheduling inconsistency with GRES bound to specific CPUs.\n -- If user belongs to a group which has split entries in /etc/group\n    search for its username in all groups.\n -- Do not consider nodes explicitly powered up as DOWN with reason of \"Node\n    unexpected rebooted\".\n -- Use correct slurmd spooldir when creating cpu-frequency locks.\n -- Note that TICKET_BASED fairshare will be deprecated in the future. Consider\n    using the FAIR_TREE algorithm instead.\n -- Set job's reason to BadConstaints when job can't run on any node.\n -- Prevent abort on update of reservation with no nodes (licenses only).\n -- Prevent slurmctld from dumping core if job_resrcs is missing in the\n    job data structure.\n -- Fix squeue to print array task ids according to man page when\n    SLURM_BITSTR_LEN is defined in the environment.\n -- In squeue, sort jobs based on array job ID if available.\n -- Fix the calculation of job energy by not including the NO_VAL values.\n -- Advanced reservation fixes: enable update of bluegene reservation, avoid\n    abort on multi-core reservations.\n -- Set the totalview_stepid to the value of the job step instead of NO_VAL.\n -- Fix slurmdbd core dump if the daemon does not have connection with\n    the database.\n -- Display error message when attempting to modify priority of a held job.\n -- Backfill scheduler: The configured backfill_interval value (default 30\n    seconds) is now interpretted as a maximum run time for the backfill\n    scheduler. Once reached, the scheduler will build a new job queue and\n    start over, even if not all jobs have been tested.\n -- Backfill scheduler now considers OverTimeLimit and KillWait configuration\n    parameters to estimate when running jobs will exit.\n -- Correct task layout with CR_Pack_Node option and more than 1 CPU per task.\n -- Fix the scontrol man page describing the release argument.\n -- When job QOS is modified, do so before attempting to change partition in\n    order to validate the partition's Allow/DenyQOS parameter.\n\n* Changes in Slurm 14.11.7\n==========================\n -- Initialize some variables used with the srun --no-alloc option that may\n    cause random failures.\n -- Add SchedulerParameters option of sched_min_interval that controls the\n    minimum time interval between any job scheduling action. The default value\n    is zero (disabled).\n -- Change default SchedulerParameters=max_sched_time from 4 seconds to 2.\n -- Refactor scancel so that all pending jobs are cancelled before starting\n    cancellation of running jobs. Otherwise they happen in parallel and the\n    pending jobs can be scheduled on resources as the running jobs are being\n    cancelled.\n -- ALPS - Add new cray.conf variable NoAPIDSignalOnKill.  When set to yes this\n    will make it so the slurmctld will not signal the apid's in a batch job.\n    Instead it relies on the rpc coming from the slurmctld to kill the job to\n    end things correctly.\n -- ALPS - Have the slurmstepd running a batch job wait for an ALPS release\n    before ending the job.\n -- Initialize variables in consumable resource plugin to prevent core dump.\n -- Fix scancel bug which could return an error on attempt to signal a job step.\n -- In slurmctld communication agent, make the thread timeout be the configured\n    value of MessageTimeout rather than 30 seconds.\n -- sshare -U/--Users only flag was used uninitialized.\n -- Cray systems, add \"plugstack.conf.template\" sample SPANK configuration file.\n -- BLUEGENE - Set DB2NOEXITLIST when starting the slurmctld daemon to avoid\n    random crashing in db2 when the slurmctld is exiting.\n -- Make full node reservations display correctly the core count instead of\n    cpu count.\n -- Preserve original errno on execve() failure in task plugin.\n -- Add SLURM_JOB_NAME env variable to an salloc's environment.\n -- Overwrite SLURM_JOB_NAME in an srun when it gets an allocation.\n -- Make sure each job has a wckey if that is something that is tracked.\n -- Make sure old step data is cleared when job is requeued.\n -- Load libtinfo as needed when building ncurses tools.\n -- Fix small memory leak in backup controller.\n -- Fix segfault when backup controller takes control for second time.\n -- Cray - Fix backup controller running native Slurm.\n -- Provide prototypes for init_setproctitle()/fini_setproctitle on NetBSD.\n -- Add configuration test to find out the full path to su command.\n -- preempt/job_prio plugin: Fix for possible infinite loop when identifying\n    preemptable jobs.\n -- preempt/job_prio plugin: Implement the concept of Warm-up Time here. Use\n    the QoS GraceTime as the amount of time to wait before preempting.\n    Basically, skip preemption if your time is not up.\n -- Make srun wait KillWait time when a task is cancelled.\n -- switch/cray: Revert logic added to 14.11.6 that set \"PMI_CRAY_NO_SMP_ENV=1\"\n    if CR_PACK_NODES is configured.\n\n* Changes in Slurm 14.11.6\n==========================\n -- If SchedulerParameters value of bf_min_age_reserve is configured, then\n    a newly submitted job can start immediately even if there is a higher\n    priority non-runnable job which has been waiting for less time than\n    bf_min_age_reserve.\n -- qsub wrapper modified to export \"all\" with -V option\n -- RequeueExit and RequeueExitHold configuration parameters modified to accept\n    numeric ranges. For example \"RequeueExit=1,2,3,4\" and \"RequeueExit=1-4\" are\n    equivalent.\n -- Correct the job array specification parser to accept brackets in job array\n    expression (e.g. \"123_[4,7-9]\").\n -- Fix for misleading job submit failure errors sent to users. Previous error\n    could indicate why specific nodes could not be used (e.g. too small memory)\n    when other nodes could be used, but were not for another reason.\n -- Fix squeue --array to display correctly the array elements when the\n    % separator is specified at the array submission time.\n -- Fix priority from not being calculated correctly due to memory issues.\n -- Fix a transient pending reason 'JobId=job_id has invalid QOS'.\n -- A non-administrator change to job priority will not be persistent except\n    for holding the job. User's wanting to change a job priority on a persistent\n    basis should reset it's \"nice\" value.\n -- Print buffer sizes as unsigned values when failed to pack messages.\n -- Fix race condition where sprio would print factors without weights applied.\n -- Document the sacct option JobIDRaw which for arrays prints the jobid instead\n    of the arrayTaskId.\n -- Allow users to modify MinCPUsNode, MinMemoryNode and MinTmpDiskNode of\n    their own jobs.\n -- Increase the jobid print field in SQUEUE_FORMAT in\n    opt_modulefiles_slurm.in.\n -- Enable compiling without optimizations and with debugging symbols by\n    default. Disable this by configuring with --disable-debug.\n -- job_submit/lua plugin: Add mail_type and mail_user fields.\n -- Correct output message from sshare.\n -- Use standard statvfs(2) syscall if available, in preference to\n    non-standard statfs.\n -- Add a new option -U/--Users to sshare to display only users\n    information, parent and ancestors are not printed.\n -- Purge 50000 records at a time so that locks can released periodically.\n -- Fix potentially uninitialized variables\n -- ALPS - Fix issue where a frontend node could become unresponsive and never\n    added back into the system.\n -- Gate epilog complete messages as done with other messages\n -- If we have more than a certain number of agents (50) wait longer when gating\n    rpcs.\n -- FrontEnd - ping non-responding or down nodes.\n -- switch/cray: If CR_PACK_NODES is configured, then set the environment\n    variable \"PMI_CRAY_NO_SMP_ENV=1\"\n -- Fix invalid memory reference in SlurmDBD when putting a node up.\n -- Allow opening of plugstack.conf even when a symlink.\n -- Fix scontrol reboot so that rebooted nodes will not be set down with reason\n    'Node xyz unexpectedly rebooted' but will be correctly put back to service.\n -- CRAY - Throttle the post NHC operations as to not hog the job write lock\n    if many steps/jobs finish at once.\n -- Disable changes to GRES count while jobs are running on the node.\n -- CRAY - Fix issue with scontrol reconfig.\n -- slurmd: Remove wrong reporting of \"Error reading step  ... memory limit\".\n    The logic was treating success as an error.\n -- Eliminate \"Node ping apparently hung\" error messages.\n -- Fix average CPU frequency calculation.\n -- When allocating resources with resolution of sockets, charge the job for all\n    CPUs on allocated sockets rather than just the CPUs on used cores.\n -- Prevent slurmdbd error if cluster added or removed while rollup in progress.\n    Removing a cluster can cause slurmdbd to abort. Adding a cluster can cause\n    the slurmdbd rollup to hang.\n -- sview - When right clicking on a tab make sure we don't display the page\n    list, but only the column list.\n -- FRONTEND - If doing a clean start make sure the nodes are brought up in the\n    database.\n -- MySQL - Fix issue when using the TrackSlurmctldDown and nodes are down at\n    the same time, don't double bill the down time.\n -- MySQL - Various memory leak fixes.\n -- sreport - Fix Energy displays\n -- Fix node manager logic to keep unexpectedly rebooted node in state\n    NODE_STATE_DOWN even if already down when rebooted.\n -- Fix for array jobs submitted to multiple partitions not starting.\n -- CRAY - Enable ALPs mpp compatibility code in sbatch for native Slurm.\n -- ALPS - Move basil_inventory to less confusing function.\n -- Add SchedulerParameters option of \"sched_max_job_start=\"  to limit the\n    number of jobs that can be started in any single execution of the main\n    scheduling logic.\n -- Fixed compiler warnings generated by gcc version >= 4.6.\n -- sbatch to stop parsing script for \"#SBATCH\" directives after first command,\n    which matches the documentation.\n -- Overwrite the SLURM_JOB_NAME in sbatch if already exist in the environment\n    and use the one specified on the command line --job-name.\n -- Remove xmalloc_nz from unpack functions.  If the unpack ever failed the\n    free afterwards would not have zeroed out memory on the variables that\n    didn't get unpacked.\n -- Improve database interaction from controller.\n -- Fix for data shift when loading job archives.\n -- ALPS - Added new SchedulerParameters=inventory_interval to specify how\n    often an inventory request is handled.\n -- ALPS - Don't run a release on a reservation on the slurmctld for a batch\n    job.  This is already handled on the stepd when the script finishes.\n\n* Changes in Slurm 14.11.5\n==========================\n -- Correct the squeue command taking into account that a node can\n    have NULL name if it is not in DNS but still in slurm.conf.\n -- Fix slurmdbd regression which would cause a segfault when a node is set\n    down with no reason.\n -- BGQ - Fix issue with job arrays not being handled correctly\n    in the runjob_mux plugin.\n -- Print FAIR_TREE, if configured, in \"scontrol show config\" output for\n    PriorityFlags.\n -- Add SLURM_JOB_GPUS environment variable to those available in the Prolog.\n -- Load lua-5.2 library if using lua5.2 for lua job submit plugin.\n -- GRES logic: Prevent bad node_offset due to not preserving no_consume flag.\n -- Fix wrong variables used in the wrapper functions needed for systems that\n    don't support strong_alias\n -- Fix code for apple computers SOL_TCP is not defined\n -- Cray/BASIL - Check for mysql credentials in /root/.my.cnf.\n -- Fix sprio showing wrong priority for job arrays until priority is\n    recalculated.\n -- Account to batch step all CPUs that are allocated to a job not\n    just one since the batch step has access to all CPUs like other steps.\n -- Fix job getting EligibleTime set before meeting dependency requirements.\n -- Correct the initialization of QOS MinCPUs per job limit.\n -- Set the debug level of information messages in cgroup plugin to debug2.\n -- For job running under a debugger, if the exec of the task fails, then\n    cancel its I/O and abort immediately rather than waiting 60 seconds for\n    I/O timeout.\n -- Fix associations not getting default qos set until after a restart.\n -- Set the value of total_cpus not to be zero before invoking\n    acct_policy_job_runnable_post_select.\n -- MySQL - When requesting cluster resources, only return resources for the\n    cluster(s) requested.\n -- Add TaskPluginParam=autobind=threads option to set a default binding in the\n    case that \"auto binding\" doesn't find a match.\n -- Introduce a new SchedulerParameters variable nohold_on_prolog_fail.\n    If configured don't requeue jobs on hold is a Prolog fails.\n -- Make it so sched_params isn't read over and over when an epilog complete\n    message comes in\n -- Fix squeue -L <licenses> not filtering out jobs with licenses.\n -- Changed the implementation of xcpuinfo_abs_to_mac() be identical\n    _abs_to_mac() to fix CPUs allocation using cpuset cgroup.\n -- Improve the explanation of the unbuffered feature in the\n    srun man page.\n -- Make taskplugin=cgroup work for core spec.  needed to have task/cgroup\n    before.\n -- Fix reports not using the month usage table.\n -- BGQ - Sanity check given for translating small blocks into slurm bg_records.\n -- Fix bug preventing the requeue/hold or requeue/special_exit of job from the\n    completing state.\n -- Cray - Fix for launching batch step within an existing job allocation.\n -- Cray - Add ALPS_APP_ID_ENV environment variable.\n -- Increase maximum MaxArraySize configuration parameter value from 1,000,001\n    to 4,000,001.\n -- Added new SchedulerParameters value of bf_min_age_reserve. The backfill\n    scheduler will not reserve resources for pending jobs until they have\n    been pending for at least the specified number of seconds. This can be\n    valuable if jobs lack time limits or all time limits have the same value.\n -- Fix support for --mem=0 (all memory of a node) with select/cons_res plugin.\n -- Fix bug that can permit someone to kill job array belonging to another user.\n -- Don't set the default partition on a license only reservation.\n -- Show a NodeCnt=0, instead of NO_VAL, in \"scontrol show res\" for a license\n    only reservation.\n -- BGQ - When using static small blocks make sure when clearing the job the\n    block is set up to it's original state.\n -- Start job allocation using lowest numbered sockets for block task\n    distribution for consistency with cyclic distribution.\n\n* Changes in Slurm 14.11.4\n==========================\n -- Make sure assoc_mgr locks are initialized correctly.\n -- Correct check of enforcement when filling in an association.\n -- Make sacctmgr print out classification correctly for clusters.\n -- Add array_task_str to the perlapi job info.\n -- Fix for slurmctld abort with GRES types configured and no CPU binding.\n -- Fix for GRES scheduling where count > 1 per topology type (or GRES types).\n -- Make CR_ONE_TASK_PER_CORE work correctly with task/affinity.\n -- job_submit/pbs - Fix possible deadlock.\n -- job_submit/lua - Add \"alloc_node\" to job information available.\n -- Fix memory leak in mysql accounting when usage rollup happens.\n -- If users specify ALL together with other variables using the\n    --export sbatch/srun command line option, propagate the users'\n    environ to the execution side.\n -- Fix job array scheduling anomaly that can stop scheduling of valid tasks.\n -- Fix perl api tests for libslurmdb to work correctly.\n -- Remove some misleading logs related to non-consumable GRES.\n -- Allow --ignore-pbs to take effect when read as an #SBATCH argument.\n -- Fix Slurmdb::clusters_get() in perl api from not returning information.\n -- Fix TaskPluginParam=Cpusets from logging error message about not being able\n    to remove cpuset dir which was already removed by the release_agent.\n -- Fix sorting by time left in squeue.\n -- Fix the file name substitution for job stderr when %A, %a %j and %u\n    are specified.\n -- Remove minor warning when compiling slurmstepd.\n -- Fix database resources so they can add new clusters to them after they have\n    initially been added.\n -- Use the slurm_getpwuid_r wrapper of getpwuid_r to handle possible\n    interrupts.\n -- Correct the scontrol man page and command listing which node states can\n    be set by the command.\n -- Stop sacct from printing non-existent stat information for\n    Front End systems.\n -- Correct srun and acct_gather.conf man pages, mention Filesystem instead\n    of Lustre.\n -- When a job using multiple partition starts send to slurmdbd only\n    the partition in which the job runs.\n -- ALPS - Fix depth for MemoryAllocation in BASIL with CLE 5.2.3.\n -- Fix assoc_mgr hash to deal with users that don't have a uid yet when making\n    reservations.\n -- When a job uses multiple partition set the environment variable\n    SLURM_JOB_PARTITION to be the one in which the job started.\n -- Print spurious message about the absence of cgroup.conf at log level debug2\n    instead of info.\n -- Enable CUDA v7.0+ use with a Slurm configuration of TaskPlugin=task/cgroup\n    ConstrainDevices=yes (in cgroup.conf). With that configuration\n    CUDA_VISIBLE_DEVICES will start at 0 rather than the device number.\n -- Fix job array logic that can cause slurmctld to abort.\n -- Report job \"shared\" field properly in scontrol, squeue, and sview.\n -- If a job is requeued because of RequeueExit or RequeueExitHold sent event\n    REQUEUED to slurmdbd.\n -- Fix build if hwloc is in non-standard location.\n -- Fix slurmctld job recovery logic which could cause the last task in a job\n    array to be lost.\n -- Fix slurmctld initialization problem which could cause requeue of the last\n    task in a job array to fail if executed prior to the slurmctld loading\n    the maximum size of a job array into a variable in the job_mgr.c module.\n -- Fix fatal in controller when deleting a user association of a user which\n    had been previously removed from the system.\n -- MySQL - If a node state and reason are the same on a node state change\n    don't insert a new row in the event table.\n -- Fix issue with \"sreport cluster AccountUtilizationByUser\" when using\n    PrivateData=users.\n -- Fix perlapi tests for libslurm perl module.\n -- MySQL - Fix potential issue when PrivateData=Usage and a normal user\n    runs certain sreport reports.\n\n* Changes in Slurm 14.11.3\n==========================\n -- Prevent vestigial job record when canceling a pending job array record.\n -- Fixed squeue core dump.\n -- Fix job array hash table bug, could result in slurmctld infinite loop or\n    invalid memory reference.\n -- In srun honor ntasks_per_node before looking at cpu count when the user\n    doesn't request a number of tasks.\n -- Fix ghost job when submitting job after all jobids are exhausted.\n -- MySQL - Enhanced coordinator security checks.\n -- Fix for task/affinity if an admin configures a node for having threads\n    but then sets CPUs to only represent the number of cores on the node.\n -- Make it so previous versions of salloc/srun work with newer versions\n    of Slurm daemons.\n -- Avoid delay on commit for PMI rank 0 to improve performance with some\n    MPI implementations.\n -- auth/munge - Correct logic to read old format AccountingStoragePass.\n -- Reset node \"RESERVED\" state as appropriate when deleting a maintenance\n    reservation.\n -- Prevent a job manually suspended from being resumed by gang scheduler once\n    free resources are available.\n -- Prevent invalid job array task ID value if a task is started using gang\n    scheduling.\n -- Fixes for clean build on FreeBSD.\n -- Fix documentation bugs in slurm.conf.5. DenyAccount should be DenyAccounts.\n -- For backward compatibility with older versions of OMPI not compiled\n    with --with-pmi restore the SLURM_STEP_RESV_PORTS in the job environment.\n -- Update the html documentation describing the integration with openmpi.\n -- Fix sacct when searching by nodelist.\n -- Fix cosmetic info statements when dealing with a job array task instead of\n    a normal job.\n -- Fix segfault with job arrays.\n -- Correct the sbatch pbs parser to process -j.\n -- BGQ - Put print statement under a DebugFlag.  This was just an oversight.\n -- BLUEGENE - Remove check that would erroneously remove the CONFIGURING\n    flag from a job while the job is waiting for a block to boot.\n -- Fix segfault in slurmstepd when job exceeded memory limit.\n -- Fix race condition that could start a job that is dependent upon a job array\n    before all tasks of that job array complete.\n -- PMI2 race condition fix.\n\n* Changes in Slurm 14.11.2\n==========================\n -- Fix Centos5 compile errors.\n -- Fix issue with association hash not getting the correct index which\n    could result in seg fault.\n -- Fix salloc/sbatch -B segfault.\n -- Avoid huge malloc if GRES configured with \"Type\" and huge \"Count\".\n -- Fix jobs from starting in overlapping reservations that won't finish before\n    a \"maint\" reservation begins.\n -- When node gets drained while in state mixed display its status as draining\n    in sinfo output.\n -- Allow priority/multifactor to work with sched/wiki(2) if all priorities\n    have no weight.  This allows for association and QOS decay limits to work.\n -- Fix \"squeue --start\" to override SQUEUE_FORMAT env variable.\n -- Fix scancel to be able to cancel multiple jobs that are space delimited.\n -- Log Cray MPI job calling exit() without mpi_fini(), but do not treat it as\n    a fatal error. This partially reverts logic added in version 14.03.9.\n -- sview - Fix displaying of suspended steps elapsed times.\n -- Increase number of messages that get cached before throwing them away\n    when the DBD is down.\n -- Fix jobs from starting in overlapping reservations that won't finish before\n    a \"maint\" reservation begins.\n -- Restore GRES functionality with select/linear plugin. It was broken in\n    version  14.03.10.\n -- Fix bug with GRES having multiple types that can cause slurmctld abort.\n -- Fix squeue issue with not recognizing \"localhost\" in --nodelist option.\n -- Make sure the bitstrings for a partitions Allow/DenyQOS are up to date\n    when running from cache.\n -- Add smap support for job arrays and larger job ID values.\n -- Fix possible race condition when attempting to use QOS on a system running\n    accounting_storage/filetxt.\n -- Fix issue with accounting_storage/filetxt and job arrays not being printed\n    correctly.\n -- In proctrack/linuxproc and proctrack/pgid, check the result of strtol()\n    for error condition rather than errno, which might have a vestigial error\n    code.\n -- Improve information recording for jobs deferred due to advanced\n    reservation.\n -- Exports eio_new_initial_obj to the plugins and initialize kvs_seq on\n    mpi/pmi2 setup to support launching.\n\n* Changes in Slurm 14.11.1\n==========================\n -- Get libs correct when doing the xtree/xhash make check.\n -- Update xhash/tree make check to work correctly with current code.\n -- Remove the reference 'experimental' for the jobacct_gather/cgroup\n    plugin.\n -- Add QOS manipulation examples to the qos.html documentation page.\n -- If 'squeue -w node_name' specifies an unknown host name print\n    an error message and return 1.\n -- Fix race condition in job_submit plugin logic that could cause slurmctld to\n    deadlock.\n -- Job wait reason of \"ReqNodeNotAvail\" expanded to identify unavailable nodes\n    (e.g. \"ReqNodeNotAvail(Unavailable:tux[3-6])\").\n\n* Changes in Slurm 14.11.0\n==========================\n -- ALPS - Fix issue with core_spec warning.\n -- Allow multiple partitions to be specified in sinfo -p.\n -- Install the service files in /usr/lib/systemd/system.\n -- MYSQL - Add id_array_job and id_resv keys to $CLUSTER_job_table.  THIS\n    COULD TAKE A WHILE TO CREATE THE KEYS SO BE PATIENT.\n -- CRAY - Resize bitmaps on a restart and find we have more blades\n    than before.\n -- Add new eio API function for removing unused connections.\n -- ALPS - Fix issue where batch allocations weren't correctly confirmed or\n    released.\n -- Define DEFAULT_MAX_TASKS_PER_NODE based on MAX_TASKS_PER_NODE from\n    slurm.h as per documentation.\n -- Update the FAQ about relocating slurmctld.\n -- In the memory cgroup enable memory.use_hierarchy in the cgroup root.\n -- Export eio.c functions for use by MPI/PMI2.\n -- Add SLURM_CLUSTER_NAME to job environment.\n\n* Changes in Slurm 14.11.0rc3\n=============================\n -- Allow envs to override autotools binaries in autogen.sh\n -- Added system services files.\n -- If the jobs pends with DependencyNeverSatisfied keep it pending even after\n    the job which it was depending upon was cleaned.\n -- Let operators (in addition to user root and SlurmUser) see job script for\n    other user's jobs.\n -- Perl API modified to return node state of MIXED rather than ALLOCATED if\n    only some CPUs allocated.\n -- Double Munge connect retry timeout from 1 to 2 seconds.\n -- sview - Remove unneeded code that was resolved globally in commit\n    98e24b0dedc.\n -- Collect and report the accounting of the batch step and its children.\n -- Add configure checks for faccessat and eaccess, and make use of one of\n    them if available.\n -- Make configure --enable-developer also set --enable-debug\n -- Introduce a SchedulerParameters variable kill_invalid_depend, if set\n    then jobs pending with invalid dependency are going to be terminated.\n -- Move spank_user_task() call in slurmstepd after the task_g_pre_launch()\n    so that the task affinity information is available to spank.\n -- Make /etc/init.d/slurm script return value 3 when the daemon is\n    not running. This is required by Linux Standard Base Core\n    Specification 3.1\n\n* Changes in Slurm 14.11.0rc2\n=============================\n -- Logs for jobs which are explicitly requeued will say so rather than saying\n    that a node in their allocation failed.\n -- Updated the documentation about the remote licenses served by\n    the Slurm database.\n -- Insure that slurm_spank_exit() is only called once from srun.\n -- Change the signature of net_set_low_water() to use 4 bytes instead of 8.\n -- Export working_cluster_rec in libslurmdb.so as well as move some function\n    definitions needed for drmaa.\n -- If using cons_res or serial cause a fatal in the plugin instead of causing\n    the SelectTypeParameters to magically set to CR_CPU.\n -- Enhance task/affinity auto binding to consider tasks * cpus-per-task.\n -- Fix regression the priority/multifactor which would cause memory corruption.\n    Issue is only in rc1.\n -- Add PrivateData value of \"cloud\". If set, powered down nodes in the cloud\n    will be visible.\n -- Sched/backfill - Eliminate clearing start_time of running jobs.\n -- Fix various backwards compatibility issues.\n -- If failed to launch a batch job, requeue it in hold.\n\n* Changes in Slurm 14.11.0rc1\n=============================\n -- When using cgroup name the batch step as step_batch instead of\n    batch_4294967294\n -- Changed LEVEL_BASED priority to be \"Fair_Tree\"\n -- Port to NetBSD.\n -- BGQ - Add cnode based reservations.\n -- Alongside totalview_jobid implement totalview_stepid available\n    to sattach.\n -- Add ability to include other files in slurm.conf based upon the ClusterName.\n -- Update strlcpy to latest upstream version.\n -- Add reservation information in the sacct and sreport output.\n -- Add job priority calculation check for overflow and fix memory leak.\n -- Add SchedulerParameters option of pack_serial_at_end to put serial jobs at\n    the end of the available nodes rather than using a best fit algorithm.\n -- Allow regular users to view default sinfo output when\n    privatedata=reservations is set.\n -- PrivateData=reservation modified to permit users to view the reservations\n    which they have access to (rather then preventing them from seeing ANY\n    reservation).\n -- job_submit/lua: Fix job_desc set field logic\n\n* Changes in Slurm 14.11.0pre5\n==============================\n -- Fix sbatch --export=ALL, it was treated by srun as a request to explicitly\n    export only the environment variable named \"ALL\".\n -- Improve scheduling of jobs in reservations that overlap other reservations.\n -- Modify sgather to make global file systems easier to configure.\n -- Added sacctmgr reconfig to reread the slurmdbd.conf in the slurmdbd.\n -- Modify scontrol job operations to accept comma delimited list of job IDs.\n    Applies to job update, hold, release, suspend, resume, requeue, and\n    requeuehold operations.\n -- Refactor job_submit/lua interface. LUA FUNCTIONS NEED TO CHANGE! The\n    lua script no longer needs to explicitly load meta-tables, but information\n    is available directly using names slurm.reservations, slurm.jobs,\n    slurm.log_info, etc. Also, the job_submit.lua script is reloaded when\n    updated without restarting the slurmctld daemon.\n -- Allow users to specify --resv_ports to have value 0.\n -- Cray MPMD (Multiple-Program Multiple-Data) support completed.\n -- Added ability for \"scontrol update\" to references jobs by JobName (and\n    filtered optionally by UserID).\n -- Add support for an advanced reservation start time that remains constant\n    relative to the current time. This can be used to prevent the starting of\n    longer running jobs on select nodes for maintenance purpose. See the\n    reservation flag \"TIME_FLOAT\" for more information.\n -- Enlarge the jobid field to 18 characters in squeue output.\n -- Added \"scontrol write config\" option to save a copy of the current\n    configuration in a file containing a time stamp.\n -- Eliminate native Cray specific port management. Native Cray systems must\n    now use the MpiParams configuration parameter to specify ports to be used\n    for commmunications. When upgrading Native Cray systems from version 14.03,\n    all running jobs should be killed and the switch_cray_state file (in\n    SaveStateLocation of the nodes where the slurmctld daemon runs) must be\n    explicitly deleted.\n\n* Changes in Slurm 14.11.0pre4\n==============================\n -- Added job array data structure and removed 64k array size restriction.\n -- Added SchedulerParameters options of bf_max_job_array_resv to control how\n    many tasks of a job array should have resources reserved for them.\n -- Added more validity checking of incoming job submit requests.\n -- Added srun --export option to set/export specific environment variables.\n -- Scontrol modified to print separate error messages for job arrays with\n    different exit codes on the different tasks of the job array. Applies to\n    job suspend and resume operations.\n -- Fix race condition in CPU frequency set with job preemption.\n -- Always call select plugin on step termination, even if the job is also\n    complete.\n -- Srun executable names beginning with \".\" will be resolved based upon the\n    working directory and path on the compute node rather than the submit node.\n -- Add node state string suffix of \"$\" to identify nodes in maintenance\n    reservation or scheduled for reboot. This applies to scontrol, sinfo,\n    and sview commands.\n -- Enable scontrol to clear a nodes's scheduled reboot by setting its state\n    to \"RESUME\".\n -- As per sbatch and srun documentation when the --signal option is used\n    signal only the steps and unless, in the case, of a batch job B is\n    specified in which case signal only the batch script.\n -- Modify AuthInfo configuration parameter to accept credential lifetime\n    option.\n -- Modify crypto/munge plugin to use socket and timeout specified in AuthInfo.\n -- If we have a state for a step on completion put that in the database\n    instead of guessing off the exit_code.\n -- Added squeue -P/--priority option that can be used to display pending jobs\n    in the same order as used by the Slurm scheduler even if jobs are submitted\n    to multiple partitions (job is reported once per usable partition).\n -- Improve the pending reason description for various QOS limits. For each\n    QOS limit that causes a job to be pending print its specific reason.\n    For example if job pends because of GrpCpus the squeue command will\n    print QOSGrpCpuLimit as pending reason.\n -- sched/backfill - Set expected start time of job submitted to multiple\n    partitions to the earliest start time on any of the partitions.\n -- Introduce a MAX_BATCH_REQUEUE define that indicates how many times a job\n    can be requeued upon prolog failure. When the number is reached the job\n    is put on hold with reason JobHoldMaxRequeue.\n -- Add sbatch job array option to limit the number of simultaneously running\n    tasks from a job array (e.g. \"--array=0-15%4\").\n -- Implemented a new QOS limit MinCPUs. Users running under a QOS must\n    request a minimum number of CPUs which is at least MinCPUs otherwise\n    their job will pend.\n -- Introduced a new pending reason WAIT_QOS_MIN_CPUS to reflect the new QOS\n    limit.\n -- Job array dependency based upon state is now dependent upon the state of\n    the array as a whole (e.g. afterok requires ALL tasks to complete\n    sucessfully, afternotok is true if ANY tasks does not complete successfully,\n    and after requires all tasks to at least be started).\n -- The srun -u/--unbuffered options set the stdout of the task launched\n    by srun to be line buffered.\n -- The srun options -/--label and -u/--unbuffered can be specified together.\n    This limitation has been removed.\n -- Provide sacct display of gres accounting information per job.\n -- Change the node status size from uin16_t to uint32_t.\n\n* Changes in Slurm 14.11.0pre3\n==============================\n -- Move xcpuinfo.[c|h] to the slurmd since it isn't needed anywhere else\n    and will avoid the need for all the daemons to link to libhwloc.\n -- Add memory test to job_submit/partition plugin.\n -- Added new internal Slurm functions xmalloc_nz() and xrealloc_nz(), which do\n    not initialize the allocated memory to zero for improved performance.\n -- Modify hostlist function to dynamically allocate buffer space for improved\n    performance.\n -- In the job_submit plugin: Remove all slurmctld locks prior to job_submit()\n    being called for improved performance. If any slurmctld data structures are\n    read or modified, add locks directly in the plugin.\n -- Added PriorityFlag LEVEL_BASED described in doc/html/level_based.shtml\n -- If Fairshare=parent is set on an account, that account's children will be\n    effectively reparented for fairshare calculations to the first parent of\n    their parent that is not Fairshare=parent.  Limits remain the same,\n    only it's fairshare value is affected.\n\n* Changes in Slurm 14.11.0pre2\n==============================\n -- Added AllowSpecResourcesUsage configuration parameter in slurm.conf. This\n    allows jobs to use specialized resources on nodes allocated to them if the\n    job designates --core-spec=0.\n -- Add new SchedulerParameters option of build_queue_timeout to throttle how\n    much time can be consumed building the job queue for scheduling.\n -- Added HealthCheckNodeState option of \"cycle\" to cycle through the compute\n    nodes over the course of HealthCheckInterval rather than running all at\n    the same time.\n -- Add job \"reboot\" option for Linux clusters. This invokes the configured\n    RebootProgram to reboot nodes allocated to a job before it begins execution.\n -- Added squeue -O/--Format option that makes all job and step fields available\n    for printing.\n -- Improve database slurmctld entry speed dramatically.\n -- Add \"CPUs\" count to output of \"scontrol show step\".\n -- Add support for lua5.2\n -- scancel -b signals only the batch step neither any other step nor any\n    children of the shell script.\n -- MySQL - enforce NO_ENGINE_SUBSTITUTION\n -- Added CpuFreqDef configuration parameter in slurm.conf to specify the\n    default CPU frequency and governor to be set at job end.\n -- Added support for job email triggers: TIME_LIMIT, TIME_LIMIT_90 (reached\n    90% of time limit), TIME_LIMIT_80 (reached 80% of time limit), and\n    TIME_LIMIT_50 (reached 50% of time limit). Applies to salloc, sbatch and\n    srun commands.\n -- In slurm.conf add the parameter SrunPortRange=min-max. If this is configured\n    then srun will use its dynamic ports only from the configured range.\n -- Make debug_flags 64 bit to handle more flags.\n\n* Changes in Slurm 14.11.0pre1\n==============================\n -- Modify etc/cgroup.release_common.example to set specify full path to the\n    scontrol command. Also find cgroup mount point by reading cgroup.conf file.\n -- Improve qsub wrapper support for passing environment variables.\n -- Modify sdiag to report Slurm RPC traffic by user, type, count and time\n    consumed.\n -- In select plugins, stop triggering extra logging based upon the debug flag\n    CPU_Bind and use SelectType instead.\n -- Added SchedulerParameters options of bf_yield_interval and bf_yield_sleep\n    to control how frequently and for how long the backfill scheduler will\n    relinquish its locks.\n -- To support larger numbers of jobs when the StateSaveDirectory is on a\n    file system that supports a limited number of files in a directory, add a\n    subdirectory called \"hash.#\" based upon the last digit of the job ID.\n -- More gracefully handle missing batch script file. Just kill the job and do\n    not drain the compute node.\n -- Add support for allocation of GRES by model type for heterogenous systems\n    (e.g. request a Kepler GPU, a Tesla GPU, or a GPU of any type).\n -- Record and enable display of nodes anticipated to be used for pending jobs.\n -- Modify squeue --start option to print the nodes expected to be used for\n    pending job (in addition to expected start time, etc.).\n -- Add association hash to the assoc_mgr.\n -- Better logic to handle resized jobs when the DBD is down.\n -- Introduce MemLimitEnforce yes|no in slurm.conf. If set no Slurm will\n    not terminate jobs if they exceed requested memory.\n -- Add support for non-consumable generic resources for resources that are\n    limited, but can be shared between jobs.\n -- Introduce 5 new Slurm errors in slurm_errno.h related to job to better\n    report error conditions.\n -- Modify scontrol to print error message for each array task when updating\n    the entire array.\n -- Added gres_drain and gres_used fields to node_info_t.\n -- Added PriorityParameters configuration parameter in slurm.conf.\n -- Introduce automatic job requeue policy based on exit value. See RequeueExit\n    and RequeueExitHold descriptions in slurm.conf man page.\n -- Modify slurmd to cache launched job IDs for more responsive job suspend and\n    gang scheduling.\n -- Permit jobs steps full control over cpu_bind options if specialized cores\n    are included in the job allocation.\n -- Added ChosLoc configuration parameter to specifiy the pathname of the\n    Chroot OS tool.\n -- Sent SIGCONT/SIGTERM when a job is selected for preemption with GraceTime\n    configured rather than waiting for GraceTime to be reached before notifying\n    the job.\n -- Do not resume a job with specialized cores on a node running another job\n    with specialized cores (only one can run at a time).\n -- Add specialized core count to job suspend/resume calls.\n -- task/affinity and task/cgroup - Correct specialized core task binding with\n    user supplied invalid CPU mask or map.\n -- Add srun --cpu-freq options to set the CPU governor (OnDemand, Performance,\n    PowerSave or UserSpace).\n -- Add support for a job step's CPU governor and/or frequency to be reset on\n    suspend/resume (or gang scheduling). The default for an idle CPU will now\n    be \"ondemand\" rather than \"userspace\" with the lowest frequency (to recover\n    from hard slurmd failures and support gang scheduling).\n -- Added PriorityFlags option of Calulate_Running to continue recalculating\n    the priority of running jobs.\n -- Replace round-robin front-end node selection with least-loaded algorithm.\n -- CRAY - Improve support of XC30 systems when running natively.\n -- Add new node configuration parameters CoreSpecCount, CPUSpecList and\n    MemSpecLimit which support the reservation of resources for system use\n    with Linux cgroup.\n -- Add child_forked() function to the slurm_acct_gather_profile plugin to\n    close open files, leaving application with no extra open file descriptors.\n -- Cray/ALPS system - Enable backup controller to run outside of the Cray to\n    accept new job submissions and most other operations on the pending jobs.\n -- Have sacct print job and task array id's for job arrays.\n -- Smooth out fanout logic\n -- If <sys/prctl.h> is present name major threads in slurmctld, for\n    example backfill\n    thread: slurmctld_bckfl, the rpc manager: slurmctld_rpcmg etc.\n    The name can be seen for example using top -H.\n -- sview - Better job_array support.\n -- Provide more precise error message when job allocation can not be satisfied\n    (e.g. memory, disk, cpu count, etc. rather than just \"node configuration\n    not available\").\n -- Create a new DebugFlags named TraceJobs in slurm.conf to print detailed\n    information about jobs in slurmctld. The information include job ids, state\n    and node count.\n -- When a job dependency can never be satisfied do not cancel the job but keep\n    pending with reason WAIT_DEP_INVALID (DependencyNeverSatisfied).\n\n* Changes in Slurm 14.03.12\n===========================\n -- Make it so previous versions of salloc/srun work with newer versions\n    of Slurm daemons.\n -- PMI2 race condition fix.\n -- Avoid delay on commit for PMI rank 0 to improve performance with some\n    MPI implementations.\n -- Correct the sbatch pbs parser to process -j.\n -- Squeue modified to not merge tasks of a job array if their wait reasons\n    differ.\n -- Use the slurm_getpwuid_r wrapper of getpwuid_r to handle possible\n    interrupts.\n -- Allow --ignore-pbs to take effect when read as an #SBATCH argument.\n -- Do not launch step if job killed while the prolog was running.\n\n* Changes in Slurm 14.03.11\n===========================\n -- ALPS - Fix depth for Memory items in BASIL with CLE 5.2\n    (changed starting in 5.2.3).\n -- ALPS - Fix issue when tracking memory on a PerNode basis instead of\n    PerCPU.\n -- Modify assoc_mgr_fill_in_qos() to allow for a flag to know if the QOS read\n    lock was locked outside of the function or not.\n -- Give even better estimates on pending node count if no node count\n    is requested.\n -- Fix jobcomp/mysql plugin for MariaDB 10+/Mysql 5.6+ to work with reserved\n    work \"partition\".\n -- If requested (scontrol reboot node_name) reboot a node even if it has\n    an maintenance reservation that is not active yet.\n -- Fix issue where exclusive allocations wouldn't lay tasks out correctly\n    with CR_PACK_NODES.\n -- Do not requeue a batch job from slurmd daemon if it is killed while in\n    the process of being launched (a race condition introduced in v14.03.9).\n -- Do not let srun overwrite SLURM_JOB_NUM_NODES if already in an allocation.\n -- Prevent a job's end_time from being too small after a basil reservation\n    error.\n -- Fix sbatch --ntasks-per-core option from setting invalid\n    SLURM_NTASKS_PER_CORE environment value.\n -- Prevent scancel abort when no job satisfies filter options.\n -- ALPS - Fix --ntasks-per-core option on multiple nodes.\n -- Double max string that Slurm can pack from 16MB to 32MB to support\n    larger MPI2 configurations.\n -- Fix Centos5 compile issues.\n -- Log Cray MPI job calling exit() without mpi_fini(), but do not treat it as\n    a fatal error. This partially reverts logic added in version 14.03.9.\n -- sview - Fix displaying of suspended steps elapsed times.\n -- Increase number of messages that get cached before throwing them away\n    when the DBD is down.\n -- Fix jobs from starting in overlapping reservations that won't finish before\n    a \"maint\" reservation begins.\n -- Fix \"squeue --start\" to override SQUEUE_FORMAT env variable.\n -- Restore GRES functionality with select/linear plugin. It was broken in\n    version  14.03.10.\n -- Fix possible race condition when attempting to use QOS on a system running\n    accounting_storage/filetxt.\n -- Sanity check for Correct QOS on startup.\n\n* Changes in Slurm 14.03.10\n===========================\n -- Fix a few sacctmgr error messages.\n -- Treat non-zero SlurmSchedLogLevel without SlurmSchedLogFile as a fatal\n    error.\n -- Correct sched_config.html documentation SchedulingParameters\n    should be SchedulerParameters.\n -- When using gres and cgroup ConstrainDevices set correct access\n    permission for the batch step.\n -- Fix minor memory leak in jobcomp/mysql on slurmctld reconfig.\n -- Fix bug that prevented preservation of a job's GRES bitmap on slurmctld\n    restart or reconfigure (bug was introduced in 14.03.5 \"Clear record of a\n    job's gres when requeued\" and only applies when GRES mapped to specific\n    files).\n -- BGQ: Fix race condition when job fails due to hardware failure and is\n    requeued. Previous code could result in slurmctld abort with NULL pointer.\n -- Prevent negative job array index, which could cause slurmctld to crash.\n -- Fix issue with squeue/scontrol showing correct node_cnt when only tasks\n    are specified.\n -- Check the status of the database connection before using it.\n -- ALPS - If an allocation requests -n set the BASIL -N option to the\n    amount of tasks / number of node.\n -- ALPS - Don't set the env var APRUN_DEFAULT_MEMORY, it is not needed anymore.\n -- Fix potential buffer overflow.\n -- Give better estimates on pending node count if no node count is requested.\n -- BLUEGENE - Fix issue where requeuing jobs could cause an assert.\n\n* Changes in Slurm 14.03.9\n==========================\n -- If slurmd fails to stat(2) the configuration print the string describing\n    the error code.\n -- Fix for mixing core base reservations with whole node based reservations\n    to avoid overlapping erroneously.\n -- BLUEGENE - Remove references to Base Partition.\n -- sview - If compiled on a non-bluegene system then used to view a BGQ fix\n    to allow sview to display blocks correctly.\n -- Fix bug in update reservation. When modifying the reservation the end time\n    was set incorrectly.\n -- The start time of a reservation that is in ACTIVE state cannot be modified.\n -- Update the cgroup documentation about release agent for devices.\n -- MYSQL - fix for setting up preempt list on a QOS for multiple QOS.\n -- Correct a minor error in the scancel.1 man page related to the\n    --signal option.\n -- Enhance the scancel.1 man page to document the sequence of signals sent\n -- Fix slurmstepd core dump if the cgroup hierarchy is not completed\n    when terminating the job.\n -- Fix hostlist_shift to be able to give correct node names on names with a\n    different number of dimensions than the cluster.\n -- BLUEGENE - Fix invalid pointer in corner case in the plugin.\n -- Make sure on a reconfigure the select information for a node is preserved.\n -- Correct logic to support job GRES specification over 31 bits (problem\n    in logic converting int to uint32_t).\n -- Remove logic that was creating GRES bitmap for node when not needed (only\n    needed when GRES mapped to specific files).\n -- BLUEGENE - Fix sinfo -tr before it would only print idle nodes correctly.\n -- BLUEGENE - Fix for licenses_only reservation on bluegene systems.\n -- sview - Verify pointer before using strchr.\n -- -M option on tools talking to a Cray from a non-Cray fixed.\n -- CRAY - Fix rpmbuild issue for missing file slurm.conf.template.\n -- Fix race condition when dealing with removing many associations at\n    different times when reservations are using the associations that are\n    being deleted.\n -- When a node's state is set to power_down/power_up, then execute\n    SuspendProgram/ResumeProgram even if previously executed for that node.\n -- Fix logic determining when job configuration (i.e. running node power up\n    logic) is complete.\n -- Setting the state of a node in powered down state node to \"resume\" will\n    no longer cause it to reboot, but only clear the \"drain\" state flag.\n -- Fix srun documentation to remove SLURM_NODELIST being equivalent as the -w\n    option (since it isn't).\n -- Fix issue with --hint=nomultithread and allocations with steps running\n    arbitrary layouts (test1.59).\n -- PrivateData=reservation modified to permit users to view the reservations\n    which they have access to (rather then preventing them from seeing ANY\n    reservation).  Backport from 14.11 commit 77c2bd25c.\n -- Fix PrivateData=reservation when using associations to give privileges to\n    a reservation.\n -- Better checking to see if select plugin is linear or not.\n -- Add support for time specification of \"fika\" (3 PM).\n -- Standardize qstat wrapper more.\n -- Provide better estimate of minimum node count for pending jobs using more\n    job parameters.\n -- ALPS - Add SubAllocate to cray.conf file for those who like the way <=2.5\n    did the ALPS reservation.\n -- Safer check to avoid invalid reads when shutting down the slurmctld with\n    lots of jobs.\n -- Fix minor memory leak in the backfill scheduler when shutting down.\n -- Add ArchiveResvs to the output of sacctmgr show config and init the variable\n    on slurmdbd startup.\n -- SLURMDBD - Only set the archive flag if purging the object\n    (i.e ArchiveJobs PurgeJobs).  This is only a cosmetic change.\n -- Fix for job step memory allocation logic if step requests GRES and memory\n    is not allocations are not managed.\n -- Fix sinfo to display mixed nodes as allocated in '%F' output.\n -- Sview - Fix cpu and node counts for partitions.\n -- Ignore NO_VAL in SLURMDB_PURGE_* macros.\n -- ALPS - Don't drain nodes if epilog fails.  It leaves them in drain state\n    with no way to get them out.\n -- Fix issue with task/affinity oversubscribing cpus erroneously when\n    using --ntasks-per-node.\n -- MYSQL - Fix load of archive files.\n -- Treat Cray MPI job calling exit() without mpi_fini() as fatal error for\n    that specific task and let srun handle all timeout logic.\n -- Fix small memory leak in jobcomp/mysql.\n -- Correct tracking of licenses for suspended jobs on slurmctld reconfigure or\n    restart.\n -- If failed to launch a batch job requeue it in hold.\n\n* Changes in Slurm 14.03.8\n==========================\n -- Fix minor memory leak when Job doesn't have nodes on it (Meaning the job\n    has finished)\n -- Fix sinfo/sview to be able to query against nodes in reserved and other\n    states.\n -- Make sbatch/salloc read in (SLURM|(SBATCH|SALLOC))_HINT in order to\n    handle sruns in the script that will use it.\n -- srun properly interprets a leading \".\" in the executable name based upon\n    the working directory of the compute node rather than the submit host.\n -- Fix Lustre misspellings in hdf5 guide\n -- Fix wrong reference in slurm.conf man page to what --profile option should\n    be used for AcctGatherFilesystemType.\n -- Update HDF5 document to point out the SlurmdUser is who creates the\n    ProfileHDF5Dir directory as well as all it's sub-directories and files.\n -- CRAY NATIVE - Remove error message for srun's ran inside an salloc that\n    had --network= specified.\n -- Defer job step initiation of required GRES are in use by other steps rather\n    than immediately returning an error.\n -- Deprecate --cpu_bind from sbatch and salloc.  These never worked correctly\n    and only caused confusion since the cpu_bind options mostly refer to a\n    step we opted to only allow srun to set them in future versions.\n -- Modify sgather to work if Nodename and NodeHostname differ.\n -- Changed use of JobContainerPlugin where it should be JobContainerType.\n -- Fix for possible error if job has GRES, but the step explicitly requests a\n    GRES count of zero.\n -- Make \"srun --gres=none ...\" work when executed without a job allocation.\n -- Change the global eio_shutdown_time to a field in eio handle.\n -- Advanced reservation fixes for heterogeneous systems, especially when\n    reserving cores.\n -- If --hint=nomultithread is used in a job allocation make sure any srun's\n    ran inside the allocation can read the environment correctly.\n -- If batchdir can't be made set errno correctly so the slurmctld is notified\n    correctly.\n -- Remove repeated batch complete if batch directory isn't able to be made\n    since the slurmd will send the same message.\n -- sacctmgr fix default format for list transactions.\n -- BLUEGENE - Fix backfill issue with backfilling jobs on blocks already\n    reserved for higher priority jobs.\n -- When creating job arrays the job specification files for each elements\n    are hard links to the first element specification files. If the controller\n    fails to make the links the files are copied instead.\n -- Fix error handling for job array create failure due to inability to copy\n    job files (script and environment).\n -- Added patch in the contribs directory for integrating make version 4.0 with\n    Slurm and renamed the previous patch \"make-3.81.slurm.patch\".\n -- Don't wait for an update message from the DBD to finish before sending rc\n    message back.  In slow systems with many associations this could speed\n    responsiveness in sacctmgr after adding associations.\n -- Eliminate race condition in enforcement of MaxJobCount limit for job arrays.\n -- Fix anomaly allocating cores for GRES with specific device/CPU mapping.\n -- cons_res - When requesting exclusive access make sure we set the number\n    of cpus in the job_resources_t structure so as nodes finish the correct\n    cpu count is displayed in the user tools.\n -- If the job_submit plugin calls take longer than 1 second to run, print a\n    warning.\n -- Make sure transfer_s_p_options transfers all the portions of the\n    s_p_options_t struct.\n -- Correct the srun man page, the SLURM_CPU_BIND_VERBOSE, SLURM_CPU_BIND_TYPE\n    SLURM_CPU_BIND_LIST environment variable are set only when task/affinity\n    plugin is configured.\n -- sacct - Initialize variables correctly to avoid incorrect structure\n    reference.\n -- Performance adjustment to avoid calling a function multiple times when it\n    only needs to be called once.\n -- Give more correct waiting reason if job is waiting on association/QOS\n    MaxNode limit.\n -- DB - When sending lft updates to the slurmctld only send non-deleted lfts.\n -- BLUEGENE - Fix documentation on how to build a reservation less than\n    a midplane.\n -- If Slurmctld fails to read the job environment consider it an error\n    and abort the job.\n -- Add the name of the node a job is running on to the message printed by\n    slurmstepd when terminating a job.\n -- Remove unsupported options from sacctmgr help and the dump function.\n -- Update sacctmgr man page removing reference to obsolete parameter\n    MaxProcSecondsPerJob.\n -- Added more validity checking of incoming job submit requests.\n\n* Changes in Slurm 14.03.7\n==========================\n -- Correct typos in man pages.\n -- Add note to MaxNodesPerUser and multiple jobs running on the same node\n    counting as multiple nodes.\n -- PerlAPI - fix renamed call from slurm_api_set_conf_file to\n    slurm_conf_reinit.\n -- Fix gres race condition that could result in job deallocation error message.\n -- Correct NumCPUs count for jobs with --exclusive option.\n -- When creating reservation with CoreCnt, check that Slurm uses\n    SelectType=select/cons_res, otherwise don't send the request to slurmctld\n    and return an error.\n -- Save the state of scheduled node reboots so they will not be lost should the\n    slurmctld restart.\n -- In select/cons_res plugin - Insure the node count does not exceed the task\n    count.\n -- switch/nrt - Do not explicitly unload windows for a job on termination,\n    only unload its table (which automatically unloads its windows).\n -- When HealthCheckNodeState is configured as IDLE don't run the\n    HealthCheckProgram for nodes in any other states than IDLE.\n -- Remove all slurmctld locks prior to job_submit() being called in plugins.\n    If any slurmctld data structures are read or modified, add locks directly\n    in the plugin.\n -- Minor sanity check to verify the string sent in isn't NULL when using\n    bit_unfmt.\n -- CRAY NATIVE - Fix issue on heavy systems to only run the NHC once per\n    job/step completion.\n -- Remove unneeded step cleanup for pending steps.\n -- Fix issue where if a batch job was manually requeued the batch step\n    information wasn't stored in accounting.\n -- When job is release from a requeue hold state clean up its previous\n    exit code.\n -- Correct the srun man page about how the output from the user application\n    is sent to srun.\n -- Increase the timeout of the main thread while waiting for the i/o thread.\n    Allow up to 180 seconds for the i/o thread to complete.\n -- When using sacct -c to read the job completion data compute the correct\n    job elapsed time.\n -- Perl package: Define some missing node states.\n -- When using AccountingStorageType=accounting_storage/mysql zero out the\n    database index for the array elements avoiding duplicate database values.\n -- Reword the explanation of cputime and cputimeraw in the sacct man page.\n -- JobCompType allows \"jobcomp/mysql\" as valid name but the code used\n    \"job_comp/mysql\" setting an incorrect default database.\n -- Try to load libslurm.so only when necessary.\n -- When nodes scheduled for reboot, set state to DOWN rather than FUTURE so\n    they are still visible to sinfo. State set to IDLE after reboot completes.\n -- Apply BatchStartTimeout configuration to task launch and avoid aborting\n    srun commands due to long running Prolog scripts.\n -- Fix minor memory leaks when freeing node_info_t structure.\n -- Fix various memory leaks in sview\n -- If a batch script is requeued and running steps get correct exit code/signal\n    previous it was always -2.\n -- If step exitcode hasn't been set display with sacct the -2 instead\n    of acting like it is a signal and exitcode.\n -- Send calculated step_rc for batch step instead of raw status as\n    done for normal steps.\n -- If a job times out, set the exit code in accounting to 1 instead of the\n    signal 1.\n -- Update the acct_gather.conf.5 man page removing the reference to\n    InfinibandOFEDFrequency.\n -- Fix gang scheduling for jobs submitted to multiple partitions.\n -- Enable srun to submit job to multiple partitions.\n -- Update slurm.conf man page. When Epilog or Prolog fail the node state\n    is set ro DRAIN.\n -- Start a job in the highest priority partition possible, even if it requires\n    preempting other jobs and delaying initiation, rather than using a lower\n    priority partition. Previous logic would preempt lower priority jobs, but\n    then might start the job in a lower priority partition and not use the\n    resources released by the preempted jobs.\n -- Fix SelectTypeParameters=CR_PACK_NODES for srun making both job and step\n    resource allocation.\n -- BGQ - Make it possible to pack multiple tasks on a core when not using\n    the entire cnode.\n -- MYSQL - if unable to connect to mysqld close connection that was inited.\n -- DBD - when connecting make sure we wait MessageTimeout + 5 since the\n    timeout when talking to the Database is the same timeout so a race\n    condition could occur in the requesting client when receiving the response\n    if the database is unresponsive.\n\n* Changes in Slurm 14.03.6\n==========================\n -- Added examples to demonstrate the use of the sacct -T option to the man\n    page.\n -- Fix for regression in 14.03.5 with sacctmgr load when Parent has \"'\"\n    around it.\n -- Update comments in sacctmgr dump header.\n -- Fix for possible abort on change in GRES configuration.\n -- CRAY - fix modules file, (backport from 14.11 commit 78fe86192b.\n -- Fix race condition which could result in requeue if batch job exit and node\n    registration occur at the same time.\n -- switch/nrt - Unload job tables (in addition to windows) in user space mode.\n -- Differentiate between two identical debug messages about purging vestigial\n    job scripts.\n -- If the socket used by slurmstepd to communicate with slurmd exist when\n    slurmstepd attempts to create it, for example left over from a previous\n    requeue or crash, delete it and recreate it.\n\n* Changes in Slurm 14.03.5\n==========================\n -- If a srun runs in an exclusive allocation and doesn't use the entire\n    allocation and CR_PACK_NODES is set layout tasks appropriately.\n -- Correct Shared field in job state information seen by scontrol, sview, etc.\n -- Print Slurm error string in scontrol update job and reset the Slurm errno\n    before each call to the API.\n -- Fix task/cgroup to handle -mblock:fcyclic correctly\n -- Fix for core-based advanced reservations where the distribution of cores\n    across nodes is not even.\n -- Fix issue where association maxnodes wouldn't be evaluated correctly if a\n    QOS had a GrpNodes set.\n -- GRES fix with multiple files defined per line in gres.conf.\n -- When a job is requeued make sure accounting marks it as such.\n -- Print the state of requeued job as REQUEUED.\n -- Fix if a job's partition was taken away from it don't allow a requeue.\n -- Make sure we lock on the conf when sending slurmd's conf to the slurmstepd.\n -- Fix issue with sacctmgr 'load' not able to gracefully handle bad formatted\n    file.\n -- sched/backfill: Correct job start time estimate with advanced reservations.\n -- Error message added when in proctrack/cgroup the step freezer path isn't\n    able to be destroyed for debug.\n -- Added extra index's into the database for better performance when\n    deleting users.\n -- Fix issue with wckeys when tracking wckeys, but not enforcing them,\n    you could get multiple '*' wckeys.\n -- Fix bug which could report to squeue the wrong partition for a running job\n    that is submitted to multiple partitions.\n -- Report correct CPU count allocated to job when allocated whole node even if\n    not using all CPUs.\n -- If job's constraints cannot be satisfied put it in pending state with reason\n    BadConstraints and don't remove it.\n -- sched/backfill - If job started with infinite time limit, set its end_time\n    one year in the future.\n -- Clear record of a job's gres when requeued.\n -- Clear QOS GrpUsedCPUs when resetting raw usage if QOS is not using any cpus.\n -- Remove log message left over from debugging.\n -- When using CR_PACK_NODES fix make --ntasks-per-node work correctly.\n -- Report correct partition associated with a step if the job is submitted to\n    multiple partitions.\n -- Fix to allow removing of preemption from a QOS\n -- If the proctrack plugins fail to destroy the job container print an error\n    message and avoid to loop forever, give up after 120 seconds.\n -- Make srun obey POSIX convention and increase the exit code by 128 when the\n    process terminated by a signal.\n -- Sanity check for acct_gather_energy/rapl\n -- If the proctrack plugins fail to destroy the job container print an error\n    message and avoid to loop forever, give up after 120 seconds.\n -- If the sbatch command specifies the option --signal=B:signum sent the signal\n    to the batch script only.\n -- If we cancel a task and we have no other exit code send the signal and\n    exit code.\n -- Added note about InnoDB storage engine being used with MySQL.\n -- Set the job exit code when the job is signaled and set the log level to\n    debug2() when processing an already completed job.\n -- Reset diagnostics time stamp when \"sdiag --reset\" is called.\n -- squeue and scontrol to report a job's \"shared\" value based upon partition\n    options rather than reporting \"unknown\" if job submission does not use\n    --exclusive or --shared option.\n -- task/cgroup - Fix cpuset binding for batch script.\n -- sched/backfill - Fix anomaly that could result in jobs being scheduled out\n    of order.\n -- Expand pseudo-terminal size data structure field sizes from 8 to 16 bits.\n -- Set the job exit code when the job is signaled and set the log level to\n    debug2() when processing an already completed job.\n -- Distinguish between two identical error messages.\n -- If using accounting_storage/mysql directly without a DBD fix issue with\n    start of requeued jobs.\n -- If a job fails because of batch node failure and the job is requeued and an\n    epilog complete message comes from that node do not process the batch step\n    information since the job has already been requeued because the epilog\n    script running isn't guaranteed in this situation.\n -- Change message to note a NO_VAL for return code could of come from node\n    failure as well as interactive user.\n -- Modify test4.5 to only look at one partition instead of all of them.\n -- Fix sh5util -u to accept username different from the user that runs the\n    command.\n -- Corrections to man pages:salloc.1 sbatch.1 srun.1 nonstop.conf.5\n    slurm.conf.5.\n -- Restore srun --pty resize ability.\n -- Have sacctmgr dump cluster handle situations where users or such have\n    special characters in their names like ':'\n -- Add more debugging for information should the job ran on wrong node\n    and should there be problems accessing the state files.\n\n* Changes in Slurm 14.03.4\n==========================\n -- Fix issue where not enforcing QOS but a partition either allows or denies\n    them.\n -- CRAY - Make switch/cray default when running on a Cray natively.\n -- CRAY - Make job_container/cncu default when running on a Cray natively.\n -- Disable job time limit change if it's preemption is in progress.\n -- Correct logic to properly enforce job preemption GraceTime.\n -- Fix sinfo -R to print each down/drained node once, rather than once per\n    partition.\n -- If a job has non-responding node, retry job step create rather than\n    returning with DOWN node error.\n -- Support SLURM_CONF path which does not have \"slurm.conf\" as the file name.\n -- CRAY - make job_container/cncu default when running on a Cray natively\n -- Fix issue where batch cpuset wasn't looked at correctly in\n    jobacct_gather/cgroup.\n -- Correct squeue's job node and CPU counts for requeued jobs.\n -- Correct SelectTypeParameters=CR_LLN with job selecition of specific nodes.\n -- Only if ALL of their partitions are hidden will a job be hidden by default.\n -- Run EpilogSlurmctld for a job is killed during slurmctld reconfiguration.\n -- Close window with srun if waiting for an allocation and while printing\n    something you also get a signal which would produce deadlock.\n -- Add SelectTypeParameters option of CR_PACK_NODES to pack a job's tasks\n    tightly on its allocated nodes rather than distributing them evenly across\n    the allocated nodes.\n -- cpus-per-task support: Try to pack all CPUs of each tasks onto one socket.\n    Previous logic could spread the tasks CPUs across multiple sockets.\n -- Add new distribution method fcyclic so when a task is using multiple cpus\n    it can bind cyclically across sockets.\n -- task/affinity - When using --hint=nomultithread only bind to the first\n    thread in a core.\n -- Make cgroup task layout (block | cyclic) method mirror that of\n    task/affinity.\n -- If TaskProlog sets SLURM_PROLOG_CPU_MASK reset affinity for that task\n    based on the mask given.\n -- Keep supporting 'srun -N x --pty bash' for historical reasons.\n -- If EnforcePartLimits=Yes and QOS job is using can override limits, allow\n    it.\n -- Fix issues if partition allows or denies account's or QOS' and either are\n    not set.\n -- If a job requests a partition and it doesn't allow a QOS or account the\n    job is requesting pend unless EnforcePartLimits=Yes.  Before it would\n    always kill the job at submit.\n -- Fix format output of scontrol command when printing node state.\n -- Improve the clean up of cgroup hierarchy when using the\n    jobacct_gather/cgroup plugin.\n -- Added SchedulerParameters value of Ignore_NUMA.\n -- Fix issues with code when using automake 1.14.1\n -- select/cons_res plugin: Fix memory leak related to job preemption.\n -- After reconfig rebuild the job node counters only for jobs that have\n    not finished yet, otherwise if requeued the job may enter an invalid\n    COMPLETING state.\n -- Do not purge the script and environment files for completed jobs on\n    slurmctld reconfiguration or restart (they might be later requeued).\n -- scontrol now accepts the option job=xxx or jobid=xxx for the requeue,\n    requeuehold and release operations.\n -- task/cgroup - fix to bind batch job in the proper CPUs.\n -- Added strigger option of -N, --noheader to not print the header when\n    displaying a list of triggers.\n -- Modify strigger to accept arguments to the program to execute when an\n    event trigger occurs.\n -- Attempt to create duplicate event trigger now generates ESLURM_TRIGGER_DUP\n    (\"Duplicate event trigger\").\n -- Treat special characters like %A, %s etc. literally in the file names\n    when specified escaped e.g. sbatch -o /home/zebra\\\\%s will not expand\n    %s as the stepid of the running job.\n -- CRAYALPS - Add better support for CLE 5.2 when running Slurm over ALPS.\n -- Test time when job_state file was written to detect multiple primary\n    slurmctld daemons (e.g. both backup and primary are functioning as\n    primary and there is a split brain problem).\n -- Fix scontrol to accept update jobid=# numtasks=#\n -- If the backup slurmctld assumes primary status, then do NOT purge any\n    job state files (batch script and environment files) and do not re-use them.\n    This may indicate that multiple primary slurmctld daemons are active (e.g.\n    both backup and primary are functioning as primary and there is a split\n    brain problem).\n -- Set correct error code when requeuing a completing/pending job\n -- When checking for if dependency of type afterany, afterok and afternotok\n    don't clear the dependency if the job is completing.\n -- Cleanup the JOB_COMPLETING flag and eventually requeue the job when the\n    last epilog completes, either slurmd epilog or slurmctld epilog, whichever\n    comes last.\n -- When attempting to requeue a job distinguish the case in which the job is\n    JOB_COMPLETING or already pending.\n -- When reconfiguring the controller don't restart the slurmctld epilog if it\n    is already running.\n -- Email messages for job array events print now use the job ID using the\n    format \"#_# (#)\" rather than just the internal job ID.\n -- Set the number of free licenses to be 0 if the global license count\n    decreases and total is less than in use.\n -- Add DebugFlag of BackfillMap. Previously a DebugFlag value of Backfill\n    logged information about what it was doing plus a map of expected resouce\n    use in the future. Now that very verbose resource use map is only logged\n    with a DebugFlag value of BackfillMap\n -- Fix slurmstepd core dump.\n -- Modify the description of -E and -S option of sacct command as point in time\n    'before' or 'after' the database records are returned.\n -- Correct support for partition with Shared=YES configuration.\n -- If job requests --exclusive then do not use nodes which have any cores in an\n    advanced reservation. Also prevents case where nodes can be shared by other\n    jobs.\n -- For \"scontrol --details show job\" report the correct CPU_IDs when thre are\n    multiple threads per core (we are translating a core bitmap to CPU IDs).\n -- If DebugFlags=Protocol is configured in slurm.conf print details of the\n    connection, ip address and port accepted by the controller.\n -- Fix minor memory leak when reading in incomplete node data checkpoint file.\n -- Enlarge the width specifier when printing partition SHARE to display larger\n    sharing values.\n -- sinfo locks added to prevent possibly duplicate record printing for\n    resources in multiple partitions.\n\n* Changes in Slurm 14.03.3-2\n============================\n -- BGQ - Fix issue with uninitialized variable.\n\n* Changes in Slurm 14.03.3\n==========================\n -- Correction to default batch output file name. In version 14.03.2 was using\n    \"slurm_<jobid>_4294967294.out\" due to error in job array logic.\n -- In slurm.spec file, replace \"Requires cray-MySQL-devel-enterprise\" with\n    \"Requires mysql-devel\".\n\n* Changes in Slurm 14.03.2\n==========================\n -- Fix race condition if PrologFlags=Alloc,NoHold is used.\n -- Cray - Make NPC only limit running other NPC jobs on shared blades instead\n    of limited non NPC jobs.\n -- Fix for sbatch #PBS -m (mail) option parsing.\n -- Fix job dependency bug. Jobs dependent upon multiple other jobs may start\n    prematurely.\n -- Set \"Reason\" field for all elements of a job array on short-circuited\n    scheduling for job arrays.\n -- Allow -D option of salloc/srun/sbatch to specify relative path.\n -- Added SchedulerParameter of batch_sched_delay to permit many batch jobs\n    to be submitted between each scheduling attempt to reduce overhead of\n    scheduling logic.\n -- Added job reason of \"SchedTimeout\" if the scheduler was not able to reach\n    the job to attempt scheduling it.\n -- Add job's exit state and exit code to email message.\n -- scontrol hold/release accepts job name option (in addition to job ID).\n -- Handle when trying to cancel a step that hasn't started yet better.\n -- Handle Max/GrpCPU limits better\n -- Add --priority option to salloc, sbatch and srun commands.\n -- Honor partition priorities over job priorities.\n -- Fix sacct -c when using jobcomp/filetxt to read newer variables\n -- Fix segfault of sacct -c if spaces are in the variables.\n -- Release held job only with \"scontrol release <jobid>\" and not by resetting\n    the job's priority. This is needed to support job arrays better.\n -- Correct squeue command not to merge jobs with state pending and completing\n    together.\n -- Fix issue where user is requesting --acctg-freq=0 and no memory limits.\n -- Fix issue with GrpCPURunMins if a job's timelimit is altered while the job\n    is running.\n -- Temporary fix for handling our typemap for the perl api with newer perl.\n -- Fix allowgroup on bad group seg fault with the controller.\n -- Handle node ranges better when dealing with accounting max node limits.\n\n* Changes in Slurm 14.03.1-2\n==========================\n -- Update configure to set correct version without having to run autogen.sh\n\n* Changes in Slurm 14.03.1\n==========================\n -- Add support for job std_in, std_out and std_err fields in Perl API.\n -- Add \"Scheduling Configuration Guide\" web page.\n -- BGQ - fix check for jobinfo when it is NULL\n -- Do not check cleaning on \"pending\" steps.\n -- task/cgroup plugin - Fix for building on older hwloc (v1.0.2).\n -- In the PMI implementation by default don't check for duplicate keys.\n    Set the SLURM_PMI_KVS_DUP_KEYS if you want the code to check for\n    duplicate keys.\n -- Add job submission time to squeue.\n -- Permit user root to propagate resource limits higher than the hard limit\n    slurmd has on that compute node has (i.e. raise both current and maximum\n    limits).\n -- Fix issue with license used count when doing an scontrol reconfig.\n -- Fix the PMI iterator to not report duplicated keys.\n -- Fix issue with sinfo when -o is used without the %P option.\n -- Rather than immediately invoking an execution of the scheduling logic on\n    every event type that can enable the execution of a new job, queue its\n    execution. This permits faster execution of some operations, such as\n    modifying large counts of jobs, by executing the scheduling logic less\n    frequently, but still in a timely fashion.\n -- If the environment variable is greater than MAX_ENV_STRLEN don't\n    set it in the job env otherwise the exec() fails.\n -- Optimize scontrol hold/release logic for job arrays.\n -- Modify srun to report an exit code of zero rather than nine if some tasks\n    exit with a return code of zero and others are killed with SIGKILL. Only an\n    exit code of zero did this.\n -- Fix a typo in scontrol man page.\n -- Avoid slurmctld crash getting job info if detail_ptr is NULL.\n -- Fix sacctmgr add user where both defaultaccount and accounts are specified.\n -- Added SchedulerParameters option of max_sched_time to limit how long the\n    main scheduling loop can execute for.\n -- Added SchedulerParameters option of sched_interval to control how frequently\n    the main scheduling loop will execute.\n -- Move start time of main scheduling loop timeout after locks are aquired.\n -- Add squeue job format option of \"%y\" to print a job's nice value.\n -- Update scontrol update jobID logic to operate on entire job arrays.\n -- Fix PrologFlags=Alloc to run the prolog on each of the nodes in the\n    allocation instead of just the first.\n -- Fix race condition if a step is starting while the slurmd is being\n    restarted.\n -- Make sure a job's prolog has ran before starting a step.\n -- BGQ - Fix invalid memory read when using DefaultConnType in the\n    bluegene.conf\n -- Make sure we send node state to the DBD on clean start of controller.\n -- Fix some sinfo and squeue sorting anomalies due to differences in data\n    types.\n -- Only send message back to slurmctld when PrologFlags=Alloc is used on a\n    Cray/ALPS system, otherwise use the slurmd to wait on the prolog to gate\n    the start of the step.\n -- Remove need to check PrologFlags=Alloc in slurmd since we can tell if prolog\n    has ran yet or not.\n -- Fix squeue to use a correct macro to check job state.\n -- BGQ - Fix incorrect logic issues if MaxBlockInError=0 in the bluegene.conf.\n -- priority/basic - Insure job priorities continue to decrease when jobs are\n    submitted with the --nice option.\n -- Make the PrologFlag=Alloc work on batch scripts\n -- Make PrologFlag=NoHold (automatically sets PrologFlag=Alloc) not hold in\n    salloc/srun, instead wait in the slurmd when a step hits a node and the\n    prolog is still running.\n -- Added --cpu-freq=highm1 (high minus one) option.\n -- Expand StdIn/Out/Err string length output by \"scontrol show job\" from 128\n    to 1024 bytes.\n -- squeue %F format will now print the job ID for non-array jobs.\n -- Use quicksort for all priority based job sorting, which improves performance\n    significantly with large job counts.\n -- If a job has already been released from a held state ignore successive\n    release requests.\n -- Fix srun/salloc/sbatch man pages for the --no-kill option.\n -- Add squeue -L/--licenses option to filter jobs by license names.\n -- Handle abort job on node on front end systems without core dumping.\n -- Fix dependency support for job arrays.\n -- When updating jobs verify the update request is not identical to\n    the current settings.\n -- When sorting jobs and priorities are equal sort by job_id.\n -- Do not overwrite existing reason for node being down or drained.\n -- Requeue batch job if Munge is down and credential can not be created.\n -- Make _slurm_init_msg_engine() tolerate bug in bind() returning a busy\n    ephemeral port.\n -- Don't block scheduling of entire job array if it could run in multiple\n    partitions.\n -- Introduce a new debug flag Protocol to print protocol requests received\n    together with the remote IP address and port.\n -- CRAY - Set up the network even when only using 1 node.\n -- CRAY - Greatly reduce the number of error messages produced from the task\n    plugin and provide more information in the message.\n\n* Changes in Slurm 14.03.0\n==========================\n -- job_submit/lua: Fix invalid memory reference if script returns error message\n    for user.\n -- Add logic to sleep and retry if slurm.conf can't be read.\n -- Reset a node's CpuLoad value at least once each SlurmdTimeout seconds.\n -- Scheduler enhancements for reservations: When a job needs to run in\n    reservation, but can not due to busy resources, then do not block all jobs\n    in that partition from being scheduled, but only the jobs in that\n    reservation.\n -- Export \"SLURM*\" environment variables from sbatch even if --export=NONE.\n -- When recovering node state if the Slurm version is 2.6 or 2.5 set the\n    protocol version to be SLURM_2_5_PROTOCOL_VERSION which is the minimum\n    supported version.\n -- Update the scancel man page documenting the -s option.\n -- Update sacctmgr man page documenting how to modify account's QOS.\n -- Fix for sjstat which currently does not print >1TB memory values correctly.\n -- Change xmalloc()/xfree() to malloc()/free() in hostlist.c for better\n    performance.\n -- Update squeue.1 man page describing the SPECIAL_EXIT state.\n -- Added scontrol option of errnumstr to return error message given a slurm\n    error number.\n -- If srun invoked with the --multi-prog option, but no task count, then use\n    the task count provided in the MPMD configuration file.\n -- Prevent sview abort on some systems when adding or removing columns to the\n    display for nodes, jobs, partitions, etc.\n -- Add job array hash table for improved performance.\n -- Make AccountingStorageEnforce=all not include nojobs or nosteps.\n -- Added sacctmgr mod qos set RawUsage=0.\n -- Modify hostlist functions to accept more than two numeric ranges (e.g.\n    \"row[1-3]rack[0-8]slot[0-63]\")\n\n* Changes in Slurm 14.03.0rc1\n==============================\n -- Fixed typos in srun_cr man page.\n -- Run job scheduling logic immediately when nodes enter service.\n -- Added sbatch '--parsable' option to output only the job id number and the\n    cluster name separated by a semicolon. Errors will still be displayed.\n -- Added failure management \"slurmctld/nonstop\" plugin.\n -- Prevent jobs being killed when a checkpoint plugin is enabled or disabled.\n -- Update the documentation about SLURM_PMI_KVS_NO_DUP_KEYS environment\n    variable.\n -- select/cons_res bug fix for range of node counts with --cpus-per-task\n    option (e.g. \"srun -N2-3 -c2 hostname\" would allocate 2 CPUs on the first\n    node and 0 CPUs on the second node).\n -- Change reservation flags field from 16 to 32-bits.\n -- Add reservation flag value of \"FIRST_CORES\".\n -- Added the idea of Resources to the database.  Framework for handling\n    license servers outside of Slurm.\n -- When starting the slurmctld only send past job/node state information to\n    accounting if running for the first time (should speed up startup\n    dramatically on systems with lots of nodes or lots of jobs).\n -- Compile and run on FreeBSD 8.4.\n -- Make job array expressions more flexible to accept multiple step counts in\n    the expression (e.g. \"--array=1-10:2,50-60:5,123\").\n -- switch/cray - add state save/restore logic tracking allocated ports.\n -- SchedulerParameters - Replace max_job_bf with bf_max_job_start (both will\n    work for now).\n -- Add SchedulerParameters options of preempt_reorder_count and\n    preempt_strict_order.\n -- Make memory types in acct_gather uint64_t to handle systems with more than\n    4TB of memory on them.\n -- BGQ - --export=NONE option for srun to make it so only the SLURM_JOB_ID\n    and SLURM_STEP_ID env vars are set.\n -- Munge plugins - Add sleep between retries if can't connect to socket.\n -- Added DebugFlags value of \"License\".\n -- Added --enable-developer which will give you -Werror when compiling.\n -- Fix for job request with GRES count of zero.\n -- Fix a potential memory leak in hostlist.\n -- Job array dependency logic: Cache results for major performance improvement.\n -- Modify squeue to support filter on job states Special_Exit and Resizing.\n -- Defer purging job record until after EpilogSlurmctld completes.\n -- Add -j option for jobid to sbcast.\n -- Fix handling RPCs from a 14.03 slurmctld to a 2.6 slurmd\n\n* Changes in Slurm 14.03.0pre6\n==============================\n -- Modify slurmstepd to log messages according to the LogTimeFormat\n    parameter in slurm.conf.\n -- Insure that overlapping reservations do not oversubscribe available\n    licenses.\n -- Added core specialization logic to select/cons_res plugin.\n -- Added whole_node field to job_resources structure and enable gang scheduling\n    for jobs with core specialization.\n -- When using FastSchedule = 1 the nodes with less than configured resources\n    are not longer set DOWN, they are set to DRAIN instead.\n -- Modified 'sacctmgr show associations' command to show GrpCPURunMins\n    by default.\n -- Replace the hostlist_push() function with a more efficient\n    hostlist_push_host().\n -- Modify the reading of lustre file system statistics to print more\n    information when debug and when io error occur.\n -- Add specialized core count field to job credential data.\n    NOTE: This changes the communications protocol from other pre-releases of\n    version 14.03. All programs must be cancelled and daemons upgraded from\n    previous pre-releases of version 14.03. Upgrades from version 2.6 or earlier\n    can take place without loss of jobs\n -- Add version number to node and front-end configuration information visible\n    using the scontrol tool.\n -- Add idea of a RESERVED flag for node state so idle resources are marked\n    not \"idle\" when in a reservation.\n -- Added core specialization plugin infrastructure.\n -- Added new job_submit/trottle plugin to control the rate at which a user\n    can submit jobs.\n -- CRAY - added network performance counters option.\n -- Allow scontrol suspend/resume to accept jobid in the format jobid_taskid\n    to suspend/resume array elements.\n -- In the slurmctld job record, split \"shared\" variable into \"share_res\" (share\n    resource) and \"whole_node\" fields.\n -- Fix the format of SLURM_STEP_RESV_PORTS. It was generated incorrectly\n    when using the hostlist_push_host function and input surrounded by [].\n -- Modify the srun --slurmd-debug option to accept debug string tags\n    (quiet, fatal, error, info verbose) beside the numerical values.\n -- Fix the bug where --cpu_bind=map_cpu is interpreted as mask_cpu.\n -- Update the documentation egarding the state of cpu frequencies after\n    a step using --cpu-freq completes.\n -- CRAY - Fix issue when a job is requeued and nhc is still running as it is\n    being scheduled to run again.  This would erase the previous job info\n    that was still needed to clean up the nodes from the previous job run.\n    (Bug 526).\n -- Set SLURM_JOB_PARTITION environment variable set for all job allocations.\n -- Set SLURM_JOB_PARTITION environment variable for Prolog program.\n -- Added SchedulerParameters option of partition_job_depth to limit scheduling\n    logic depth by partition.\n -- Handle the case in which errno is not reset to 0 after calling\n    getgrent_r(), which causes the controller to core dump.\n\n* Changes in Slurm 14.03.0pre5\n==============================\n -- Added squeue format option of \"%X\" (core specialization count).\n -- Added core specialization web page (just a start for now).\n -- Added the SLURM_ARRAY_JOB_ID and SLURM_ARRAY_TASK_ID\n    in epilog slurmctld environment.\n -- Fix bug in job step allocation failing due to memory limit.\n -- Modify the pbsnodes script to reflect its output on a TORQUE system.\n -- Add ability to clear a node's DRAIN flag using scontrol or sview by setting\n    it's state to \"UNDRAIN\". The node's base state (e.g. \"DOWN\" or \"IDLE\") will\n    not be changed.\n -- Modify the output of 'scontrol show partition' by displaying\n    DefMemPerCPU=UNLIMITED and MaxMemPerCPU=UNLIMITED when these limits are\n    configured as 0.\n -- mpirun-mic - Major re-write of the command wrapper for Xeon Phi use.\n -- Add new configuration parameter of AuthInfo to specify port used by\n    authentication plugin.\n -- Fixed conditional RPM compiling.\n -- Corrected slurmstepd ident name when logging to syslog.\n -- Fixed sh5util loop when there are no node-step files.\n -- Add SLURM_CLUSTER_NAME to environment variables passed to PrologSlurmctld,\n    Prolog, EpilogSlurmctld, and Epilog\n -- Add the idea of running a prolog right when an allocation happens\n    instead of when running on the node for the first time.\n -- If user runs 'scontrol reconfig' but hostnames or the host count changes\n    the slurmctld throws a fatal error.\n -- gres.conf - Add \"NodeName\" specification so that a single gres.conf file\n    can be used for a heterogeneous cluster.\n -- Add flag to accounting RPC to indicate if job data is packed or not.\n -- After all srun tasks have terminated on a node close the stdout/stderr\n    channel with the slurmstepd on that node.\n -- In case of i/o error with slurmstepd log an error message and abort the\n    job.\n -- Add --test-only option to sbatch command to validate the script and options.\n    The response includes expected start time and resources to be allocated.\n\n* Changes in Slurm 14.03.0pre4\n==============================\n -- Remove the ThreadID documentation from slurm.conf. This functionality has\n    been obsoleted by the LogTimeFormat.\n -- Sched plugins - rename global and plugin functions names for consistency\n    with other plugin types.\n -- BGQ - Added RebootQOSList option to bluegene.conf to allow an implicate\n    reboot of a block if only jobs in the list are running on it when cnodes\n    go into a failure state.\n -- Correct task count of pending job steps.\n -- Improve limit enforcement for jobs, set RLIMIT_RSS, RLIMIT_AS and/or\n    RLIMIT_DATA to enforce memory limit.\n -- Pending job steps will have step_id of INFINITE rather than NO_VAL and\n    will be reported as \"TBD\" by scontrol and squeue commands.\n -- Add logic so PMI_Abort or PMI2_Abort can propagate an exit code.\n -- Added SlurmdPlugstack configuration parameter.\n -- Added PriorityFlag DEPTH_OBLIVIOUS to have the depth of an association\n    not effect it's priorty.\n -- Multi-thread the sinfo command (one thread per partition).\n -- Added sgather tool to gather files from a job's compute nodes into a\n    central location.\n -- Added configuration parameter FairShareDampeningFactor to offer a greater\n    priority range based upon utilization.\n -- Change MaxArraySize and job's array_task_id from 16-bit to 32-bit field.\n    Additional Slurm enhancements are be required to support larger job arrays.\n -- Added -S/--core-spec option to salloc, sbatch and srun commands to reserve\n    specialized cores for system use. Modify scontrol and sview to get/set\n    the new field. No enforcement exists yet for these new options.\n    struct job_info / slurm_job_info_t: Added core_spec\n    struct job_descriptorjob_desc_msg_t: Added core_spec\n\n* Changes in Slurm 14.03.0pre3\n==============================\n -- Do not set SLURM_NODEID environment variable on front-end systems.\n -- Convert bitmap functions to use int32_t instead of int in data structures\n    and function arguments. This is to reliably enable use of bitmaps containing\n    up to 4 billion elements. Several data structures containing index values\n    were also changed from data type int to int32_t:\n    - Struct job_info / slurm_job_info_t: Changed exc_node_inx, node_inx, and\n      req_node_inx from type int to type int32_t\n    - job_step_info_t: Changed node_inx from type int to type int32_t\n    - Struct partition_info / partition_info_t: Changed node_inx from type int\n      to type int32_t\n    - block_job_info_t: Changed cnode_inx from type int to type int32_t\n    - block_info_t: Changed ionode_inx and mp_inx from type int to type int32_t\n    - Struct reserve_info / reserve_info_t: Changed node_inx from type int to\n      type int32_t\n -- Modify qsub wrapper output to match torque command output, just print the\n    job ID rather than \"Submitted batch job #\"\n -- Change Slurm error string for ESLURM_MISSING_TIME_LIMIT from\n    \"Missing time limit\" to\n    \"Time limit specification required, but not provided\"\n -- Change salloc job_allocate error message header from\n    \"Failed to allocate resources\" to\n    \"Job submit/allocate failed\"\n -- Modify slurmctld message retry logic to support Cray cold-standby SDB.\n\n* Changes in Slurm 14.03.0pre2\n==============================\n -- Added \"JobAcctGatherParams\" configuration parameter. Value of \"NoShare\"\n    disables accounting for shared memory.\n -- Added fields to \"scontrol show job\" output: boards_per_node,\n    sockets_per_board, ntasks_per_node, ntasks_per_board, ntasks_per_socket,\n    ntasks_per_core, and nice.\n -- Add squeue output format options for job command and working directory\n    (%o and %Z respectively).\n -- Add stdin/out/err to sview job output.\n -- Add new job_state of JOB_BOOT_FAIL for job terminations due to failure to\n    boot it's allocated nodes or BlueGene block.\n -- CRAY - Add SelectTypeParameters NHC_NO_STEPS and NHC_NO which will disable\n    the node health check script for steps and allocations respectfully.\n -- Reservation with CoreCnt: Avoid possible invalid memory reference.\n -- Add new error code for attempt to create a reservation with duplicate name.\n -- Validate that a hostlist file contains text (i.e. not a binary).\n -- switch/generic - propagate switch information from srun down to slurmd and\n    slurmstepd.\n -- CRAY - Do not package Slurm's libpmi or libpmi2 libraries. The Cray version\n    of those libraries must be used.\n -- Added a new option to the scontrol command to view licenses that are\n    configured in use and avalable. 'scontrol show licenses'.\n -- MySQL - Made Slurm compatible with 5.6\n\n* Changes in Slurm 14.03.0pre1\n==============================\n -- sview - improve scalability\n -- Add task pointer to the task_post_term() function in task plugins. The\n    terminating task's PID is available in task->pid.\n -- Move select/cray to select/alps\n -- Defer sending SIGKILL signal to processes while core dump in progress.\n -- Added JobContainerPlugin configuration parameter and plugin infrastructure.\n -- Added partition configuration parameters AllowAccounts, AllowQOS,\n    DenyAccounts and DenyQOS.\n -- The rpmbuild option for a cray system with ALPS has changed from\n    %_with_cray to %_with_cray_alps.\n -- The log file timestamp format can now be selected at runtime via the\n    LogTimeFormat configuration option. See the slurm.conf and slurmdbd.conf\n    man pages for details.\n -- Added switch/generic plugin to a job's convey network topology.\n -- BLUEGENE - If block is in 'D' state or has more cnodes in error than\n    MaxBlockInError set the job wait reason appropriately.\n -- API use: Generate an error return rather than fatal error and exit if the\n    configuraiton file is absent or invalid. This will permit Slurm APIs to be\n    more reliably used by other programs.\n -- Add support for load-based scheduling, allocate jobs to nodes with the\n    largest number of available CPUs. Added SchedulingParameters paramter of\n    \"CR_LLN\" and partition parameter of \"LLN=yes|no\".\n -- Added job_info() and step_info() functions to the gres plugins to extract\n    plugin specific fields from the job's or step's GRES data structure.\n -- Added sbatch --signal option of \"B:\" to signal the batch shell rather than\n    only the spawned job steps.\n -- Added sinfo and squeue format option of \"%all\" to print all fields available\n    for the data type with a vertical bar separating each field.\n -- Add mechanism for job_submit plugin to generate error message for srun,\n    salloc or sbatch to stderr. New argument added to job_submit function in\n    the plugin.\n -- Add StdIn, StdOut, and StdErr paths to job information dumped with\n    \"scontrol show job\".\n -- Permit Slurm administrator to submit a batch job as any user.\n -- Set a job's RLIMIT_AS limit based upon it's memory limit and VsizeFactor\n    configuration value.\n -- Remove Postgres plugins\n -- Make jobacct_gather/cgroup work correctly and also make all jobacct_gather\n    plugins more maintainable.\n -- Proctrack/pgid - Add support for proctrack_p_plugin_get_pids() function.\n -- Sched/backfill - Change default max_job_bf parameter from 50 to 100.\n -- Added -I|--item-extract option to sh5util to extract data item from series.\n\n* Changes in Slurm 2.6.10\n=========================\n -- Switch/nrt - On switch resource allocation failure, free partial allocation.\n -- Switch/nrt - Properly track usage of CAU and RDMA resources with multiple\n    tasks per compute node.\n -- Fix issue where user is requesting --acctg-freq=0 and no memory limits.\n -- BGQ - Temp fix issue where job could be left on job_list after it finished.\n -- BGQ - Fix issue where limits were checked on midplane counts instead of\n    cnode counts.\n -- BGQ - Move code to only start job on a block after limits are checked.\n -- Handle node ranges better when dealing with accounting max node limits.\n -- Fix perlapi to compile correctly with perl 5.18\n -- BGQ - Fix issue with uninitialized variable.\n -- Correct sinfo --sort fields to match documentation: E => Reason,\n    H -> Reason Time (new), R -> Partition Name, u/U -> Reason user (new)\n -- If an invalid assoc_ptr comes in don't use the id to verify it.\n -- Sched/backfill modified to avoid using nodes in completing state.\n -- Correct support for job --profile=none option and related documentation.\n -- Properly enforce job --requeue and --norequeue options.\n -- If a job --mem-per-cpu limit exceeds the partition or system limit, then\n    scale the job's memory limit and CPUs per task to satisfy the limit.\n -- Correct logic to support Power7 processor with 1 or 2 threads per core\n    (CPU IDs are not consecutive).\n\n* Changes in Slurm 2.6.9\n========================\n -- Fix sinfo to work correctly with draining/mixed nodes as well as filtering\n    on Mixed state.\n -- Fix sacctmgr update user with no \"where\" condition.\n -- Fix logic bugs for SchedulerParameters option of max_rpc_cnt.\n\n* Changes in Slurm 2.6.8\n========================\n -- Add support for Torque/PBS job array options and environment variables.\n -- CRAY/ALPS - Add support for CLE52\n -- Fix issue where jobs still pending after a reservation would remain\n    in waiting reason ReqNodeNotAvail.\n -- Update last_job_update when a job's state_reason was modified.\n -- Free job_ptr->state_desc where ever state_reason is set.\n -- Fixed sacct.1 and srun.1 manual pages which contains a hyphen where\n    a minus sign for options was intended.\n -- sinfo - Make sure if partition name is long and the default the last char\n    doesn't get chopped off.\n -- task/affinity - Protect against zero divide when simulating more hardware\n    than you really have.\n -- NRT - Fix issue with 1 node jobs.  It turns out the network does need to\n    be setup for 1 node jobs.\n -- Fix recovery of job dependency on task of job array when slurmctld restarts.\n -- mysql - Fix invalid memory reference.\n -- Lock the /cgroup/freezer subsystem when creating files for tracking processes.\n -- Fix preempt/partition_prio to avoid preempting jobs in partitions with\n    PreemptMode=OFF\n -- launch/poe - Implicitly set --network in job step create request as needed.\n -- Permit multiple batch job submissions to be made for each run of the\n    scheduler logic if the job submissions occur at the nearly same time.\n -- Fix issue where associations weren't correct if backup takes control and\n    new associations were added since it was started.\n -- Fix race condition is corner case with backup slurmctld.\n -- With the backup slurmctld make sure we reinit beginning values in the\n    slurmdbd plugin.\n -- Fix sinfo to work correctly with draining/mixed nodes.\n -- MySQL - Fix it so a lock isn't held unnecessarily.\n -- Added new SchedulerParameters option of max_rpc_cnt when too many RPCs\n    are active.\n -- BGQ - Fix deny_pass to work correctly.\n -- BGQ - Fix sub block steps using a block when the block has passthrough's\n    in it.\n\n* Changes in Slurm 2.6.7\n========================\n -- Properly enforce a job's cpus-per-task option when a job's allocation is\n    constrained on some nodes by the mem-per-cpu option.\n -- Correct the slurm.conf man pages and checkpoint_blcr.html page\n    describing that jobs must be drained from cluster before deploying\n    any checkpoint plugin. Corrected in version 14.03.\n -- Fix issue where if using munge and munge wasn't running and a slurmd\n    needed to forward a message, the slurmd would core dump.\n -- Update srun.1 man page documenting the PMI2 support.\n -- Fix slurmctld core dump when a jobs gets its QOS updated but there\n    is not a corresponding association.\n -- If a job requires specific nodes and can not run due to those nodes being\n    busy, the main scheduling loop will block those specific nodes rather than\n    the entire queue/partition.\n -- Fix minor memory leak when updating a job's name.\n -- Fix minor memory leak when updating a reservation on a partition using \"ALL\"\n    nodes.\n -- Fix minor memory leak when adding a reservation with a nodelist and core\n    count.\n -- Update sacct man page description of job states.\n -- BGQ - Fix minor memory leak when selecting blocks that can't immediately be\n    placed.\n -- Fixed minor memory leak in backfill scheduler.\n -- MYSQL - Fixed memory leak when querying clusters.\n -- MYSQL - Fix when updating QOS on an association.\n -- NRT - Fix to supply correct error messages to poe/pmd when a launch fails.\n -- Add SLURM_STEP_ID to Prolog environment.\n -- Add support for SchedulerParameters value of bf_max_job_start that limits\n    the total number of jobs that can be started in a single iteration of the\n    backfill scheduler.\n -- Don't print negative number when dealing with large memory sizes with\n    sacct.\n -- Fix sinfo output so that host in state allocated and mixed will not be\n    merged together.\n -- GRES: Avoid crash if GRES configurations is inconstent.\n -- Make S_SLURM_RESTART_COUNT item available to SPANK.\n -- Munge plugins - Add sleep between retries if can't connect to socket.\n -- Fix the database query to return all pending jobs in a given time interval.\n -- switch/nrt - Correct logic to get dynamic window count.\n -- Remove need to use job->ctx_params in the launch plugin, just to simplify\n    code.\n -- NRT - Fix possible memory leak if using multiple adapters.\n -- NRT - Fix issue where there are more than NRT_MAXADAPTERS on a system.\n -- NRT - Increase Max number of adapters from 8 -> 9\n -- NRT - Initialize missing variables when the PMD is starting a job.\n -- NRT - Fix issue where we are launching hosts out of numerical order,\n    this would cause pmd's to hang.\n -- NRT - Change xmalloc's to malloc just to be safe.\n -- NRT - Sanity check to make sure a jobinfo is there before packing.\n -- Add missing options to the print of TaskPluginParam.\n -- Fix a couple of issues with scontrol reconfig and adding nodes to\n    slurm.conf.  Rebooting daemons after adding nodes to the slurm.conf\n    is highly recommended.\n\n* Changes in Slurm 2.6.6\n========================\n -- sched/backfill - Fix bug that could result in failing to reserve resources\n    for high priority jobs.\n -- Correct job RunTime if requeued from suspended state.\n -- Reset job priority from zero (held) on manual resume from suspend state.\n -- If FastSchedule=0 then do not DOWN a node with low memory or disk size.\n -- Remove vestigial note.\n -- Update sshare.1 man page making it consistent with sacctmgr.1.\n -- Do not reset a job's priority when the slurmctld restarts if previously\n    set to some specific value.\n -- sview - Fix regression where the Node tab wasn't able to add/remove columns.\n -- Fix slurmstepd lock when job terminates inside the infiniband\n    network traffic accounting plugin.\n -- Correct the documentation to read filesystem instead of Lustre. Update\n    the srun help.\n -- Fix the acct_gather_filesystem_lustre.c to compute the Lustre accounting\n    data correctly accumulating differences between sampling intervals.\n    Fix the data structure mismatch between acct_gather_filesystem_lustre.c\n    and slurm_jobacct_gather.h which caused the hdf5 plugin to log incorrect\n    data.\n -- Don't allow PMI_TIME to be zero which will cause floating exception.\n -- Fix purging of old reservation errors in database.\n -- MYSQL - If starting the plugin and the database isn't up attempt to\n    connect in a loop instead of producing a fatal.\n -- BLUEGENE - If IONodesPerMP changes in bluegene.conf recalculate bitmaps\n    based on ionode count correctly on slurmctld restart.\n -- Fix step allocation when some CPUs are not available due to memory limits.\n    This happens when one step is active and using memory that blocks the\n    scheduling of another step on a portion of the CPUs needed. The new step\n    is now delayed rather than aborting with \"Requested node configuration is\n    not available\".\n -- Make sure node limits get assessed if no node count was given in request.\n -- Removed obsolete slurm_terminate_job() API.\n -- Update documentation about QOS limits\n -- Retry task exit message from slurmstepd to srun on message timeout.\n -- Correction to logic reserving all nodes in a specified partition.\n -- Added support for selecting AMD GPU by setting GPU_DEVICE_ORDINAL env var.\n -- Properly enforce GrpSubmit limit for job arrays.\n -- CRAY - fix issue with using CR_ONE_TASK_PER_CORE\n -- CRAY - fix memory leak when using accelerators\n\n* Changes in Slurm 2.6.5\n========================\n -- Correction to hostlist parsing bug introduced in v2.6.4 for hostlists with\n    more than one numeric range in brackets (e.g. rack[0-3]_blade[0-63]\").\n -- Add notification if using proctrack/cgroup and task/cgroup when oom hits.\n -- Corrections to advanced reservation logic with overlapping jobs.\n -- job_submit/lua - add cpus_per_task field to those available.\n -- Add cpu_load to the node information available using the Perl API.\n -- Correct a job's GRES allocation data in accounting records for non-Cray\n    systems.\n -- Substantial performance improvement for systems with Shared=YES or FORCE\n    and large numbers of running jobs (replace bubble sort with quick sort).\n -- proctrack/cgroup - Add locking to prevent race condition where one job step\n    is ending for a user or job at the same time another job stepsis starting\n    and the user or job container is deleted from under the starting job step.\n -- Fixed sh5util loop when there are no node-step files.\n -- Fix race condition on batch job termination that could result in a job exit\n    code of 0xfffffffe if the slurmd on node zero registers its active jobs at\n    the same time that slurmstepd is recording the job's exit code.\n -- Correct logic returning remaining job dependencies in job information\n    reported by scontrol and squeue. Eliminates vestigial descriptors with\n    no job ID values (e.g. \"afterany\").\n -- Improve performance of REQUEST_JOB_INFO_SINGLE RPC by removing unnecessary\n    locks and use hash function to find the desired job.\n -- jobcomp/filetxt - Reopen the file when slurmctld daemon is reconfigured\n    or gets SIGHUP.\n -- Remove notice of CVE with very old/deprecated versions of Slurm in\n    news.html.\n -- Fix if hwloc_get_nbobjs_by_type() returns zero core count (set to 1).\n -- Added ApbasilTimeout parameter to the cray.conf configuration file.\n -- Handle in the API if parts of the node structure are NULL.\n -- Fix srun hang when IO fails to start at launch.\n -- Fix for GRES bitmap not matching the GRES count resulting in abort\n    (requires manual resetting of GRES count, changes to gres.conf file,\n    and slurmd restarts).\n -- Modify sview to better support job arrays.\n -- Modify squeue to support longer job ID values (for many job array tasks).\n -- Fix race condition in authentication credential creation that could corrupt\n    memory. (NOTE: This race condition has existed since 2003 and would be\n    exceedingly rare.)\n -- HDF5 - Fix minor memory leak.\n -- Slurmstepd variable initialization - Without this patch, free() is called\n    on a random memory location (i.e. whatever is on the stack), which can\n    result in slurmstepd dying and a completed job not being purged in a\n    timely fashion.\n -- Fix slurmstepd race condition when separate threads are reading and\n    modifying the job's environment, which can result in the slurmstepd failing\n    with an invalid memory reference.\n -- Fix erroneous error messages when running gang scheduling.\n -- Fix minor memory leak.\n -- scontrol modified to suspend, resume, hold, uhold, or release multiple\n    jobs in a space separated list.\n -- Minor debug error when a connection goes away at the end of a job.\n -- Validate return code from calls to slurm_get_peer_addr\n -- BGQ - Fix issues with making sure all cnodes are accounted for when mulitple\n    steps cause multiple cnodes in one allocation to go into error at the\n    same time.\n -- scontrol show job - Correct NumNodes value calculated based upon job\n    specifications.\n -- BGQ - Fix issue if user runs multiple sub-block jobs inside a multiple\n    midplane block that starts on a higher coordinate than it ends (i.e if a\n    block has midplanes [0010,0013] 0013 is the start even though it is\n    listed second in the hostlist).\n -- BGQ - Add midplane to the total_cnodes used in the runjob_mux plugin\n    for better debug.\n -- Update AllocNodes paragraph in slurm.conf.5.\n\n* Changes in Slurm 2.6.4\n========================\n -- Fixed sh5util to print its usage.\n -- Corrected commit f9a3c7e4e8ec.\n -- Honor ntasks-per-node option with exclusive node allocations.\n -- sched/backfill - Prevent invalid memory reference if bf_continue option is\n    configured and slurm is reconfigured during one of the sleep cycles or if\n    there are any changes to the partition configuration or if the normal\n    scheduler runs and starts a job that the backfill scheduler is actively\n    working on.\n -- Update man pages information about acct-freq and JobAcctGatherFrequency\n    to reflect only the latest supported format.\n -- Minor document update to include note about PrivateData=Usage for the\n    slurm.conf when using the DBD.\n -- Expand information reported with DebugFlags=backfill.\n -- Initiate jobs pending to run in a reservation as soon as the reservation\n    becomes active.\n -- Purged expired reservation even if it has pending jobs.\n -- Corrections to calculation of a pending job's expected start time.\n -- Remove some vestigial logic treating job priority of 1 as a special case.\n -- Memory freeing up to avoid minor memory leaks at close of daemons\n -- Updated documentation to give correct units being displayed.\n -- Report AccountingStorageBackupHost with \"scontrol show config\".\n -- init scripts ignore quotes around Pid file name specifications.\n -- Fixed typo about command case in quickstart.html.\n -- task/cgroup - handle new cpuset files, similar to commit c4223940.\n -- Replace the tempname() function call with mkstemp().\n -- Fix for --cpu_bind=map_cpu/mask_cpu/map_ldom/mask_ldom plus\n    --mem_bind=map_mem/mask_mem options, broken in 2.6.2.\n -- Restore default behavior of allocating cores to jobs on a cyclic basis\n    across the sockets unless SelectTypeParameters=CR_CORE_DEFAULT_DIST_BLOCK\n    or user specifies other distribution options.\n -- Enforce JobRequeue configuration parameter on node failure. Previously\n    always requeued the job.\n -- acct_gather_energy/ipmi - Add delay before retry on read error.\n -- select/cons_res with GRES and multiple threads per core, fix possible\n    infinite loop.\n -- proctrack/cgroup - Add cgroup create retry logic in case one step is\n    starting at the same time as another step is ending and the logic to create\n    and delete cgroups overlaps.\n -- Improve setting of job wait \"Reason\" field.\n -- Correct sbatch documentation and job_submit/pbs plugin \"%j\" is job ID,\n    not \"%J\" (which is job_id.step_id).\n -- Improvements to sinfo performance, especially for large numbers of\n    partitions.\n -- SlurmdDebug - Permit changes to slurmd debug level with \"scontrol reconfig\"\n -- smap - Avoid invalid memory reference with hidden nodes.\n -- Fix sacctmgr modify qos set preempt+/-=.\n -- BLUEGENE - fix issue where node count wasn't set up correctly when srun\n    preforms the allocation, regression in 2.6.3.\n -- Add support for dependencies of job array elements (e.g.\n    \"sbatch --depend=afterok:123_4 ...\") or all elements of a job array (e.g.\n    \"sbatch --depend=afterok:123 ...\").\n -- Add support for new options in sbatch qsub wrapper:\n    -W block=true\t(wait for job completion)\n    Clear PBS_NODEFILE environment variable\n -- Fixed the MaxSubmitJobsPerUser limit in QOS which limited submissions\n    a job too early.\n -- sched/wiki, sched/wiki2 - Fix to work with change logic introduced in\n    version 2.6.3 preventing Maui/Moab from starting jobs.\n -- Updated the QOS limits documentation and man page.\n\n* Changes in Slurm 2.6.3\n========================\n -- Add support for some new #PBS options in sbatch scripts and qsub wrapper:\n    -l accelerator=true|false\t(GPU use)\n    -l mpiprocs=#\t(processors per node)\n    -l naccelerators=#\t(GPU count)\n    -l select=#\t\t(node count)\n    -l ncpus=#\t\t(task count)\n    -v key=value\t(environment variable)\n    -W depend=opts\t(job dependencies, including \"on\" and \"before\" options)\n    -W umask=#\t\t(set job's umask)\n -- Added qalter and qrerun commands to torque package.\n -- Corrections to qstat logic: job CPU count and partition time format.\n -- Add job_submit/pbs plugin to translate PBS job dependency options to the\n    extend possible (no support for PBS \"before\" options) and set some PBS\n    environment variables.\n -- Add spank/pbs plugin to set a bunch of PBS environment variables.\n -- Backported sh5util from master to 2.6 as there are some important\n    bugfixes and the new item extraction feature.\n -- select/cons_res - Correct MacCPUsPerNode partition constraint for CR_Socket.\n -- scontrol - for setdebugflags command, avoid parsing \"-flagname\" as an\n    scontrol command line option.\n -- Fix issue with step accounting if a job is requeued.\n -- Close file descriptors on exec of prolog, epilog, etc.\n -- Fix issue when a user has held a job and then sets the begin time\n    into the future.\n -- Scontrol - Enable changing a job's stdout file.\n -- Fix issues where memory or node count of a srun job is altered while the\n    srun is pending.  The step creation would use the old values and possibly\n    hang srun since the step wouldn't be able to be created in the modified\n    allocation.\n -- Add support for new SchedulerParameters value of \"bf_max_job_part\", the\n    maximum depth the backfill scheduler should go in any single partition.\n -- acct_gather/infiniband plugin - Correct packets_in/out values.\n -- BLUEGENE - Don't ignore a conn-type request from the user.\n -- BGQ - Force a request on a Q for a MESH to be a TORUS in a dimension that\n    can only be a TORUS (1).\n -- Change max message length from 100MB to 1GB before generating \"Insane\n    message length\" error.\n -- sched/backfill - Prevent possible memory corruption due to use of\n    bf_continue option and long running scheduling cycle (pending jobs could\n    have been cancelled and purged).\n -- CRAY - fix AcceleratorAllocation depth correctly for basil 1.3\n -- Created the environment variable SLURM_JOB_NUM_NODES for srun jobs and\n    updated the srun man page.\n -- BLUEGENE/CRAY - Don't set env variables that pertain to a node when Slurm\n    isn't doing the launching.\n -- gres/gpu and gres/mic - Do not treat the existence of an empty gres.conf\n    file as a fatal error.\n -- Fixed for if hours are specified as 0 the time days-0:min specification\n    is not parsed correctly.\n -- switch/nrt - Fix for memory leak.\n -- Subtract the PMII_COMMANDLEN_SIZE in contribs/pmi2/pmi2_api.c to prevent\n    certain implementation of snprintf() to segfault.\n\n* Changes in Slurm 2.6.2\n========================\n -- Fix issue with reconfig and GrpCPURunMins\n -- Fix of wrong node/job state problem after reconfig\n -- Allow users who are coordinators update their own limits in the accounts\n    they are coordinators over.\n -- BackupController - Make sure we have a connection to the DBD first thing\n    to avoid it thinking we don't have a cluster name.\n -- Correct value of min_nodes returned by loading job information to consider\n    the job's task count and maximum CPUs per node.\n -- If running jobacct_gather/none fix issue on unpacking step completion.\n -- Reservation with CoreCnt: Avoid possible invalid memory reference.\n -- sjstat - Add man page when generating rpms.\n -- Make sure GrpCPURunMins is added when creating a user, account or QOS with\n    sacctmgr.\n -- Fix for invalid memory reference due to multiple free calls caused by\n    job arrays submitted to multiple partitions.\n -- Enforce --ntasks-per-socket=1 job option when allocating by socket.\n -- Validate permissions of key directories at slurmctld startup. Report\n    anything that is world writable.\n -- Improve GRES support for CPU topology. Previous logic would pick CPUs then\n    reject jobs that can not match GRES to the allocated CPUs. New logic first\n    filters out CPUs that can not use the GRES, next picks CPUs for the job,\n    and finally picks the GRES that best match those CPUs.\n -- Switch/nrt - Prevent invalid memory reference when allocating single adapter\n    per node of specific adapter type\n -- CRAY - Make Slurm work with CLE 5.1.1\n -- Fix segfault if submitting to multiple partitions and holding the job.\n -- Use MAXPATHLEN instead of the hardcoded value 1024 for maximum file path\n    lengths.\n -- If OverTimeLimit is defined do not declare failed those jobs that ended\n    in the OverTimeLimit interval.\n\n* Changes in Slurm 2.6.1\n========================\n -- slurmdbd - Allow job derived ec and comments to be modified by non-root\n    users.\n -- Fix issue with job name being truncated to 24 chars when sending a mail\n    message.\n -- Fix minor issues with spec file, missing files and including files\n    erroneously on a bluegene system.\n -- sacct - fix --name and --partition options when using\n    accounting_storage/filetxt.\n -- squeue - Remove extra whitespace of default printout.\n -- BGQ - added head ppcfloor as an include dir when building.\n -- BGQ - Better debug messages in runjob_mux plugin.\n -- PMI2 Updated the Makefile.am to build a versioned library.\n -- CRAY - Fix srun --mem_bind=local option with launch/aprun.\n -- PMI2 Corrected buffer size computation in the pmi2_api.c module.\n -- GRES accounting data wrong in database: gres_alloc, gres_req, and gres_used\n    fields were empty if the job was not started immediately.\n -- Fix sbatch and srun task count logic when --ntasks-per-node specified,\n    but no explicit task count.\n -- Corrected the hdf5 profile user guide and the acct_gather.conf\n    documentation.\n -- IPMI - Fix Math bug getting new wattage.\n -- Corrected the AcctGatherProfileType documentation in slurm.conf\n -- Corrected the sh5util program to print the header in the csv file\n    only once, set the debug messages at debug() level, make the argument\n    check case insensitive and avoid printing duplicate \\n.\n -- If cannot collect energy values send message to the controller\n    to drain the node and log error slurmd log file.\n -- Handle complete removal of CPURunMins time at the end of the job instead\n    of at multifactor poll.\n -- sview - Add missing debug_flag options.\n -- PGSQL - Notes about Postgres functionality being removed in the next\n    version of Slurm.\n -- MYSQL - fix issue when rolling up usage and events happened when a cluster\n    was down (slurmctld not running) during that time period.\n -- sched/wiki2 - Insure that Moab gets current CPU load information.\n -- Prevent infinite loop in parsing configuration if including file containing\n    one blank line.\n -- Fix pack and unpack between 2.6 and 2.5.\n -- Fix job state recovery logic in which a job's accounting frequency was\n    not set. This would result in a value of 65534 seconds being used (the\n    equivalent of NO_VAL in uint16_t), which could result in the job being\n    requeued or aborted.\n -- Validate a job's accounting frequency at submission time rather than\n    waiting for it's initiation to possibly fail.\n -- Fix CPURunMins if a job is requeued from a failed launch.\n -- Fix in accounting_storage/filetxt to correct start times which sometimes\n    could end up before the job started.\n -- Fix issue with potentially referencing past an array in parse_time()\n -- CRAY - fix issue with accelerators on a cray when parsing BASIL 1.3 XML.\n -- Fix issue with a 2.5 slurmstepd locking up when talking to a 2.6 slurmd.\n -- Add argument to priority plugin's priority_p_reconfig function to note\n    when the association and QOS used_cpu_run_secs field has been reset.\n\n* Changes in Slurm 2.6.0\n========================\n -- Fix it so bluegene and serial systems don't get warnings over new NODEDATA\n    enum.\n -- When a job is aborted send a message for any tasks that have completed.\n -- Correction to memory per CPU calculation on system with threads and\n    allocating cores or sockets.\n -- Requeue batch job if it's node reboots (used to abort the job).\n -- Enlarge maximum size of srun's hostlist file.\n -- IPMI - Fix first poll to get correct consumed_energy for a step.\n -- Correction to job state recovery logic that could result in assert failure.\n -- Record partial step accounting record if allocated nodes fail abnormally.\n -- Accounting - fix issue where PrivateData=jobs or users could potentially\n    show information to users that had no associations on the system.\n -- Make PrivateData in slurmdbd.conf case insensitive.\n -- sacct/sstat - Add format option ConsumedEnergyRaw to print full energy\n    values.\n\n* Changes in Slurm 2.6.0rc2\n===========================\n -- HDF5 - Fix issue with Ubuntu where HDF5 development headers are\n    overwritten by the parallel versions thus making it so we need handle\n    both cases.\n -- ACCT_GATHER - handle suspending correctly for polling threads.\n -- Make SLURM_DISTRIBUTION env var hold both types of distribution if\n    specified.\n -- Remove hardcoded /usr/local from slurm.spec.\n -- Modify slurmctld locking to improve performance under heavy load with\n    very large numbers of batch job submissions or job cancellations.\n -- sstat - Fix issue where if -j wasn't given allow last argument to be checked\n    for as the job/step id.\n -- IPMI - fix adjustment on poll when using EnergyIPMICalcAdjustment.\n\n* Changes in Slurm 2.6.0rc1\n===========================\n -- Added helper script for launching symmetric and MIC-only MPI tasks within\n    SLURM (in contribs/mic/mpirun-mic).\n -- Change maximum delay for state save from 2 secs to 5 secs. Make timeout\n    configurable at build time by defining SAVE_MAX_WAIT.\n -- Modify slurmctld data structure locking to interleave read and write\n    locks rather than always favor write locks over read locks.\n -- Added sacct format option of \"ALL\" to print all fields.\n -- Deprecate the SchedulerParameters value of \"interval\" use \"bf_interval\"\n    instead as documented.\n -- Add acct_gather_profile/hdf5 to profile jobs with hdf5\n -- Added MaxCPUsPerNode partition configuration parameter. This can be\n    especially useful to schedule systems with GPUs.\n -- Permit \"scontrol reboot_node\" for nodes in MAINT reservation.\n -- Added \"PriorityFlags\" value of \"SMALL_RELATIVE_TO_TIME\". If set, the job's\n    size component will be based upon not the job size alone, but the job's\n    size divided by it's time limit.\n -- Added sbatch option \"--ignore-pbs\" to ignore \"#PBS\" options in the batch\n    script.\n -- Rename slurm_step_ctx_params_t field from \"mem_per_cpu\" to \"pn_min_memory\".\n    Job step now accepts memory specification in either per-cpu or per-node\n    basis.\n -- Add ability to specify host repitition count in the srun hostfile (e.g.\n    \"host1*2\" is equivalent to \"host1,host1\").\n\n* Changes in Slurm 2.6.0pre3\n============================\n -- Add milliseconds to default log message header (both RFC 5424 and ISO 8601\n    time formats). Disable milliseconds logging using the configure\n    parameter \"--disable-log-time-msec\". Default time format changes to\n    ISO 8601 (without time zone information). Specify \"--enable-rfc5424time\"\n    to restore the time zone information.\n -- Add username (%u) to the filename pattern in the batch script.\n -- Added options for front end nodes of AllowGroups, AllowUsers, DenyGroups,\n    and DenyUsers.\n -- Fix sched/backfill logic to initiate jobs with maximum time limit over the\n    partition limit, but the minimum time limit permits it to start.\n -- gres/gpu - Fix for gres.conf file with multiple files on a single line\n    using a slurm expression (e.g. \"File=/dev/nvidia[0-1]\").\n -- Replaced ipmi.conf with generic acct_gather.conf file for all acct_gather\n    plugins.  For those doing development to use this follow the model set\n    forth in the acct_gather_energy_ipmi plugin.\n -- Added more options to update a step's information\n -- Add DebugFlags=ThreadID which will print the thread id of the calling\n    thread.\n -- CRAY - Allocate whole node (CPUs) in reservation despite what the\n    user requests.  We have found any srun/aprun afterwards will work on a\n    subset of resources.\n\n* Changes in Slurm 2.6.0pre2\n============================\n -- Do not purge inactive interactive jobs that lack a port to ping (added\n    for MR+ operation).\n -- Advanced reservations with hostname and core counts now supports asymetric\n    reservations (e.g. specific different core count for each node).\n -- Added slurmctld/dynalloc plugin for MapReduce+ support.\n -- Added \"DynAllocPort\" configuration parameter.\n -- Added partition paramter of SelectTypeParameters to override system-wide\n    value.\n -- Added cr_type to partition_info data structure.\n -- Added allocated memory to node information available (within the existing\n    select_nodeinfo field of the node_info_t data structure). Added Allocated\n    Memory to node information displayed by sview and scontrol commands.\n -- Make sched/backfill the default scheduling plugin rather than sched/builtin\n    (FIFO).\n -- Added support for a job having different priorities in different partitions.\n -- Added new SchedulerParameters configuration parameter of \"bf_continue\"\n    which permits the backfill scheduler to continue considering jobs for\n    backfill scheduling after yielding locks even if new jobs have been\n    submitted. This can result in lower priority jobs from being backfill\n    scheduled instead of newly arrived higher priority jobs, but will permit\n    more queued jobs to be considered for backfill scheduling.\n -- Added support to purge reservation records from accounting.\n -- Cray - Add support for Basil 1.3\n\n* Changes in SLURM 2.6.0pre1\n============================\n -- Add \"state\" field to job step information reported by scontrol.\n -- Notify srun to retry step creation upon completion of other job steps\n    rather than polling. This results in much faster throughput for job step\n    execution with --exclusive option.\n -- Added \"ResvEpilog\" and \"ResvProlog\" configuration parameters to execute a\n    program at the beginning and end of each reservation.\n -- Added \"slurm_load_job_user\" function. This is a variation of\n    \"slurm_load_jobs\", but accepts a user ID argument, potentially resulting\n    in substantial performance improvement for \"squeue --user=ID\"\n -- Added \"slurm_load_node_single\" function. This is a variation of\n    \"slurm_load_nodes\", but accepts a node name argument, potentially resulting\n    in substantial performance improvement for \"sinfo --nodes=NAME\".\n -- Added \"HealthCheckNodeState\" configuration parameter identify node states\n    on which HealthCheckProgram should be executed.\n -- Remove sacct --dump --formatted-dump options which were deprecated in\n    2.5.\n -- Added support for job arrays (phase 1 of effort). See \"man sbatch\" option\n    -a/--array for details.\n -- Add new AccountStorageEnforce options of 'nojobs' and 'nosteps' which will\n    allow the use of accounting features like associations, qos and limits but\n    not keep track of jobs or steps in accounting.\n -- Cray - Add new cray.conf parameter of \"AlpsEngine\" to specify the\n    communication protocol to be used for ALPS/BASIL.\n -- select/cons_res plugin: Correction to CPU allocation count logic in for\n    cores without hyperthreading.\n -- Added new SelectTypeParameter value of \"CR_ALLOCATE_FULL_SOCKET\".\n -- Added PriorityFlags value of \"TICKET_BASED\" and merged priority/multifactor2\n    plugin into priority/multifactor plugin.\n -- Add \"KeepAliveTime\" configuration parameter controlling how long sockets\n    used for srun/slurmstepd communications are kept alive after disconnect.\n -- Added SLURM_SUBMIT_HOST to salloc, sbatch and srun job environment.\n -- Added SLURM_ARRAY_TASK_ID to environment of job array.\n -- Added squeue --array/-r option to optimize output for job arrays.\n -- Added \"SlurmctldPlugstack\" configuration parameter for generic stack of\n    slurmctld daemon plugins.\n -- Removed contribs/arrayrun tool. Use native support for job arrays.\n -- Modify default installation locations for RPMs to match \"make install\":\n    _prefix /usr/local\n    _slurm_sysconfdir %{_prefix}/etc/slurm\n    _mandir %{_prefix}/share/man\n    _infodir %{_prefix}/share/info\n -- Add acct_gather_energy/ipmi which works off freeipmi for energy gathering\n\n* Changes in Slurm 2.5.8\n========================\n -- Fix for slurmctld segfault on NULL front-end reason field.\n -- Avoid gres step allocation errors when a job shrinks in size due to either\n    down nodes or explicit resizing. Generated slurmctld errors of this type:\n    \"step_test ... gres_bit_alloc is NULL\"\n -- Fix bug that would leak memory and over-write the AllowGroups field if on\n    \"scontrol reconfig\" when AllowNodes is manually changed using scontrol.\n -- Get html/man files to install in correct places with rpms.\n -- Remove --program-prefix from spec file since it appears to be added by\n    default and appeared to break other things.\n -- Updated the automake min version in autogen.sh to be correct.\n -- Select/cons_res - Correct total CPU count allocated to a job with\n    --exclusive and --cpus-per-task options\n -- switch/nrt - Don't allocate network resources unless job step has 2+ nodes.\n -- select/cons_res - Avoid extraneous \"oversubscribe\" error messages.\n -- Reorder get config logic to avoid deadlock.\n -- Enforce QOS MaxCPUsMin limit when job submission contains no user-specified\n    time limit.\n -- EpilogSlurmctld pthread is passed required arguments rather than a pointer\n    to the job record, which under some conditions could be purged and result\n    in an invalid memory reference.\n\n* Changes in Slurm 2.5.7\n========================\n -- Fix for linking to the select/cray plugin to not give warning about\n    undefined variable.\n -- Add missing symbols to the xlator.h\n -- Avoid placing pending jobs in AdminHold state due to backfill scheduler\n    interactions with advanced reservation.\n -- Accounting - make average by task not cpu.\n -- CRAY - Change logging of transient ALPS errors from error() to debug().\n -- POE - Correct logic to support poe option \"-euidevice sn_all\" and\n    \"-euidevice sn_single\".\n -- Accounting - Fix minor initialization error.\n -- POE - Correct logic to support srun network instances count with POE.\n -- POE - With the srun --launch-cmd option, report proper task count when\n    the --cpus-per-task option is used without the --ntasks option.\n -- POE - Fix logic binding tasks to CPUs.\n -- sview - Fix race condition where new information could of slipped past\n    the node tab and we didn't notice.\n -- Accounting - Fix an invalid memory read when slurmctld sends data about\n    start job to slurmdbd.\n -- If a prolog or epilog failure occurs, drain the node rather than setting it\n    down and killing all of its jobs.\n -- Priority/multifactor - Avoid underflow in half-life calculation.\n -- POE - pack missing variable to allow fanout (more than 32 nodes)\n -- Prevent clearing reason field for pending jobs. This bug was introduced in\n    v2.5.5 (see \"Reject job at submit time ...\").\n -- BGQ - Fix issue with preemption on sub-block jobs where a job would kill\n    all preemptable jobs on the midplane instead of just the ones it needed to.\n -- switch/nrt - Validate dynamic window allocation size.\n -- BGQ - When --geo is requested do not impose the default conn_types.\n -- CRAY - Support CLE 4.2.0\n -- RebootNode logic - Defers (rather than forgets) reboot request with job\n    running on the node within a reservation.\n -- switch/nrt - Correct network_id use logic. Correct support for user sn_all\n    and sn_single options.\n -- sched/backfill - Modify logic to reduce overhead under heavy load.\n -- Fix job step allocation with --exclusive and --hostlist option.\n -- Select/cons_res - Fix bug resulting in error of \"cons_res: sync loop not\n    progressing, holding job #\"\n -- checkpoint/blcr - Reset max_nodes from zero to NO_VAL on job restart.\n -- launch/poe - Fix for hostlist file support with repeated host names.\n -- priority/multifactor2 - Prevent possible divide by zero.\n -- srun - Don't check for executable if --test-only flag is used.\n -- energy - On a single node only use the last task for gathering energy.\n    Since we don't currently track energy usage per task (only per step).\n    Otherwise we get double the energy.\n\n* Changes in Slurm 2.5.6\n========================\n -- Gres fix for requeued jobs.\n -- Gres accounting - Fix regression in 2.5.5 for keeping track of gres\n    requested and allocated.\n\n* Changes in Slurm 2.5.5\n========================\n -- Fix for sacctmgr add qos to handle the 'flags' option.\n -- Export SLURM_ environment variables from sbatch, even if \"--export\"\n    option does not explicitly list them.\n -- If node is in more than one partition, correct counting of allocated CPUs.\n -- If step requests more CPUs than possible in specified node count of job\n    allocation then return ESLURM_TOO_MANY_REQUESTED_CPUS rather than\n    ESLURM_NODES_BUSY and retrying.\n -- CRAY - Fix SLURM_TASKS_PER_NODE to be set correctly.\n -- Accounting - more checks for strings with a possible `'` in it.\n -- sreport - Fix by adding planned down time to utilization reports.\n -- Do not report an error when sstat identifies job steps terminated during\n    its execution, but log using debug type message.\n -- Select/cons_res - Permit node removed from job by going down to be returned\n    to service and re-used by another job.\n -- Select/cons_res - Tighter packing of job allocations on sockets.\n -- SlurmDBD - fix to allow user root along with the slurm user to register a\n    cluster.\n -- Select/cons_res - Fix for support of consecutive node option.\n -- Select/cray - Modify build to enable direct use of libslurm library.\n -- Bug fixes related to job step allocation logic.\n -- Cray - Disable enforcement of MaxTasksPerNode, which is not applicable\n    with launch/aprun.\n -- Accounting - When rolling up data from past usage ignore \"idle\" time from\n    a reservation when it has the \"Ignore_Jobs\" flag set.  Since jobs could run\n    outside of the reservation in it's nodes without this you could have\n    double time.\n -- Accounting - Minor fix to avoid reuse of variable erroneously.\n -- Reject job at submit time if the node count is invalid. Previously such a\n    job submitted to a DOWN partition would be queued.\n -- Purge vestigial job scripts when the slurmd cold starts or slurmstepd\n    terminates abnormally.\n -- Add support for FreeBSD.\n -- Add sanity check for NULL cluster names trying to register.\n -- BGQ - Push action 'D' info to scontrol for admins.\n -- Reset a job's reason from PartitionDown when the partition is set up.\n -- BGQ - Handle issue where blocks would have a pending job on them and\n    while it was free cnodes would go into software error and kill the job.\n -- BGQ - Fix issue where if for some reason we are freeing a block with\n    a pending job on it we don't kill the job.\n -- BGQ - Fix race condition were a job could of been removed from a block\n    without it still existing there.  This is extremely rare.\n -- BGQ - Fix for when a step completes in Slurm before the runjob_mux notifies\n    the slurmctld there were software errors on some nodes.\n -- BGQ - Fix issue on state recover if block states are not around\n    and when reading in state from DB2 we find a block that can't be created.\n    You can now do a clean start to rid the bad block.\n -- Modify slurmdbd to retransmit to slurmctld daemon if it is not responding.\n -- BLUEGENE - Fix issue where when doing backfill preemptable jobs were\n    never looked at to determine eligibility of backfillable job.\n -- Cray/BlueGene - Disable srun --pty option unless LaunchType=launch/slurm.\n -- CRAY - Fix sanity check for systems with more than 32 cores per node.\n -- CRAY - Remove other objects from MySQL query that are available from\n    the XML.\n -- BLUEGENE - Set the geometry of a job when a block is picked and the job\n    isn't a sub-block job.\n -- Cray - avoid check of macro versions of CLE for version 5.0.\n -- CRAY - Fix memory issue with reading in the cray.conf file.\n -- CRAY - If hostlist is given with srun make sure the node count is the same\n    as the hosts given.\n -- CRAY - If task count specified, but no tasks-per-node, then set the tasks\n    per node in the BASIL reservation request.\n -- CRAY - fix issue with --mem option not giving correct amount of memory\n    per cpu.\n -- CRAY - Fix if srun --mem is given outside an allocation to set the\n    APRUN_DEFAULT_MEMORY env var for aprun.  This scenario will not display\n    the option when used with --launch-cmd.\n -- Change sview to use GMutex instead of GStaticMutex\n -- CRAY - set APRUN_DEFAULT_MEMROY instead of CRAY_AUTO_APRUN_OPTIONS\n -- sview - fix issue where if a partition was completely in one state the\n    cpu count would be reflected correctly.\n -- BGQ - fix for handling half rack system in STATIC of OVERLAP mode to\n    implicitly create full system block.\n -- CRAY - Dynamically create BASIL XML buffer to resize as needed.\n -- Fix checking if QOS limit MaxCPUMinsPJ is set along with DenyOnLimit to\n    deny the job instead of holding it.\n -- Make sure on systems that use a different launcher than launch/slurm not\n    to attempt to signal tasks on the frontend node.\n -- Cray - when a step is requested count other steps running on nodes in the\n    allocation as taking up the entire node instead of just part of the node\n    allocated.  And always enforce exclusive on a step request.\n -- Cray - display correct nodelist, node/cpu count on steps.\n\n* Changes in Slurm 2.5.4\n========================\n -- Fix bug in PrologSlurmctld use that would block job steps until node\n    responds.\n -- CRAY - If a partition has MinNodes=0 and a batch job doesn't request nodes\n    put the allocation to 1 instead of 0 which prevents the allocation to\n    happen.\n -- Better debug when the database is down and using the --cluster option in\n    the user commands.\n -- When asking for job states with sacct, default to 'now' instead of midnight\n    of the current day.\n -- Fix for handling a test-only job or immediate job that fails while being\n    built.\n -- Comment out all of the logic in the job_submit/defaults plugin. The logic\n    is only an example and not meant for actual use.\n -- Eliminate configuration file 4096 character line limitation.\n -- More robust logic for tree message forward\n -- BGQ - When cnodes fail in a timeout fashion correctly look up parent\n    midplane.\n -- Correct sinfo \"%c\" (node's CPU count) output value for Bluegene systems.\n -- Backfill - Responsive improvements for systems with large numbers of jobs\n    (>5000) and using the SchedulerParameters option bf_max_job_user.\n -- slurmstepd: ensure that IO redirection openings from/to files correctly\n    handle interruption\n -- BGQ - Able to handle when midplanes go into Hardware::SoftwareFailure\n -- GRES - Correct tracking of specific resources used after slurmctld restart.\n    Counts would previously go negative as jobs terminate and decrement from\n    a base value of zero.\n -- Fix for priority/multifactor2 plugin to not assert when configured with\n    --enable-debug.\n -- Select/cons_res - If the job request specified --ntasks-per-socket and the\n    allocation using is cores, then pack the tasks onto the sockets up to the\n    specified value.\n -- BGQ - If a cnode goes into an 'error' state and the block containing the\n    cnode does not have a job running on it do not resume the block.\n -- BGQ - Handle blocks that don't free themselves in a reasonable time better.\n -- BGQ - Fix for signaling steps when allocation ends before step.\n -- Fix for backfill scheduling logic with job preemption; starts more jobs.\n -- xcgroup - remove bugs with EINTR management in write calls\n -- jobacct_gather - fix total values to not always == the max values.\n -- Fix for handling node registration messages from older versions without\n    energy data.\n -- BGQ - Allow user to request full dimensional mesh.\n -- sdiag command - Correction to jobs started value reported.\n -- Prevent slurmctld assert when invalid change to reservation with running\n    jobs is made.\n -- BGQ - If signal is NODE_FAIL allow forward even if job is completing\n    and timeout in the runjob_mux trying to send in this situation.\n -- BGQ - More robust checking for correct node, task, and ntasks-per-node\n    options in srun, and push that logic to salloc and sbatch.\n -- GRES topology bug in core selection logic fixed.\n -- Fix to handle init.d script for querying status and not return 1 on\n    success.\n\n* Changes in SLURM 2.5.3\n========================\n -- Gres/gpu plugin - If no GPUs requested, set CUDA_VISIBLE_DEVICES=NoDevFiles.\n    This bug was introduced in 2.5.2 for the case where a GPU count was\n    configured, but without device files.\n -- task/affinity plugin - Fix bug in CPU masks for some processors.\n -- Modify sacct command to get format from SACCT_FORMAT environment variable.\n -- BGQ - Changed order of library inclusions and fixed incorrect declaration\n    to compile correctly on newer compilers\n -- Fix for not building sview if glib exists on a system but not the gtk libs.\n -- BGQ - Fix for handling a job cleanup on a small block if the job has long\n    since left the system.\n -- Fix race condition in job dependency logic which can result in invalid\n    memory reference.\n\n\n* Changes in SLURM 2.5.2\n========================\n -- Fix advanced reservation recovery logic when upgrading from version 2.4.\n -- BLUEGENE - fix for QOS/Association node limits.\n -- Add missing \"safe\" flag from print of AccountStorageEnforce option.\n -- Fix logic to optimize GRES topology with respect to allocated CPUs.\n -- Add job_submit/all_partitions plugin to set a job's default partition\n    to ALL available partitions in the cluster.\n -- Modify switch/nrt logic to permit build without libnrt.so library.\n -- Handle srun task launch failure without duplicate error messages or abort.\n -- Fix bug in QoS limits enforcement when slurmctld restarts and user not yet\n    added to the QOS list.\n -- Fix issue where sjstat and sjobexitmod was installed in 2 different RPMs.\n -- Fix for job request of multiple partitions in which some partitions lack\n    nodes with required features.\n -- Permit a job to use a QOS they do not have access to if an administrator\n    manually set the job's QOS (previously the job would be rejected).\n -- Make more variables available to job_submit/lua plugin: slurm.MEM_PER_CPU,\n    slurm.NO_VAL, etc.\n -- Fix topology/tree logic when nodes defined in slurm.conf get re-ordered.\n -- In select/cons_res, correct logic to allocate whole sockets to jobs. Work\n    by Magnus Jonsson, Umea University.\n -- In select/cons_res, correct logic when job removed from only some nodes.\n -- Avoid apparent kernel bug in 2.6.32 which apparently is solved in\n    at least 3.5.0.  This avoids a stack overflow when running jobs on\n    more than 120k nodes.\n -- BLUEGENE - If we made a block that isn't runnable because of a overlapping\n    block, destroy it correctly.\n -- Switch/nrt - Dynamically load libnrt.so from within the plugin as needed.\n    This eliminates the need for libnrt.so on the head node.\n -- BLUEGENE - Fix in reservation logic that could cause abort.\n\n* Changes in SLURM 2.5.1\n========================\n -- Correction to hostlist sorting for hostnames that contain two numeric\n    components and the first numeric component has various sizes (e.g.\n    \"rack9blade1\" should come before \"rack10blade1\")\n -- BGQ - Only poll on initialized blocks instead of calling getBlocks on\n    each block independently.\n -- Fix of task/affinity plugin logic for Power7 processors having hyper-\n    threading disabled (cpu mask has gaps).\n -- Fix of job priority ordering with sched/builtin and priority/multifactor.\n    Patch from Chris Read.\n -- CRAY - Fix for setting up the aprun for a large job (+2000 nodes).\n -- Fix for race condition related to compute node boot resulting in node being\n    set down with reason of \"Node <name> unexpectedly rebooted\"\n -- RAPL - Fix for handling errors when opening msr files.\n -- BGQ - Fix for salloc/sbatch to do the correct allocation when asking for\n    -N1 -n#.\n -- BGQ - in emulation make it so we can pretend to run large jobs (>64k nodes)\n -- BLUEGENE - Correct method to update conn_type of a job.\n -- BLUEGENE - Fix issue with preemption when needing to preempt multiple jobs\n    to make one job run.\n -- Fixed issue where if an srun dies inside of an allocation abnormally it\n    would of also killed the allocation.\n -- FRONTEND - fixed issue where if a systems nodes weren't defined in the\n    slurm.conf with NodeAddr's signals going to a step could be handled\n    incorrectly.\n -- If sched/backfill starts a job with a QOS having NO_RESERVE and not job\n    time limit, start it with the partition time limit (or one year if the\n    partition has no time limit) rather than NO_VAL (140 year time limit);\n -- Alter hostlist logic to allocate large grid dynamically instead of on\n    stack.\n -- Change RPC version checks to support version 2.5 slurmctld with version 2.4\n    slurmd daemons.\n -- Correct core reservation logic for use with select/serial plugin.\n -- Exit scontrol command on stdin EOF.\n -- Disable job --exclusive option with select/serial plugin.\n\n* Changes in SLURM 2.5.0\n========================\n -- Add DenyOnLimit flag for QOS to deny jobs at submission time if they\n    request resources that reach a 'Max' limit.\n -- Permit SlurmUser or operator to change QOS of non-pending jobs (e.g.\n    running jobs).\n -- BGQ - move initial poll to beginning of realtime interaction, which will\n    also cause it to run if the realtime server ever goes away.\n\n* Changes in SLURM 2.5.0-rc2\n============================\n -- Modify sbcast logic to survive slurmd daemon restart while file a\n    transmission is in progress.\n -- Add retry logic to munge encode/decode calls. This is needed if the munge\n    deamon is under very heavy load (e.g. with 1000 slurmd daemons per compute\n    node).\n -- Add launch and acct_gather_energy plugins to RPMs.\n -- Restore support for srun \"--mpi=list\" option.\n -- CRAY - Introduce step accounting for a Cray.\n -- Modify srun to abandon I/O 60 seconds after the last task ends. Otherwise\n    an aborted slurmstepd can cause the srun process to hang indefinitely.\n -- ENERGY - RAPL - alter code to close open files (and only open them once\n    where needed)\n -- If the PrologSlurmctld fails, then requeue the job an indefinite number\n    of times instead of only one time.\n\n* Changes in SLURM 2.5.0-rc1\n============================\n -- Added Prolog and Epilog Guide (web page). Based upon work by Jason Sollom,\n    Cray Inc. and used by permission.\n -- Restore gang scheduling functionality. Preemptor was not being scheduled.\n    Fix for bugzilla #3.\n -- Add \"cpu_load\" to node information. Populate CPULOAD in node information\n    reported to Moab cluster manager.\n -- Preempt jobs only when insufficient idle resources exist to start job,\n    regardless of the node weight.\n -- Added priority/multifactor2 plugin based upon ticket distribution system.\n    Work by Janne Blomqvist, Aalto University.\n -- Add SLURM_NODELIST to environment variables available to Prolog and Epilog.\n -- Permit reservations to allow or deny access by account and/or user.\n -- Add ReconfigFlags value of KeepPartState. See \"man slurm.conf\" for details.\n -- Modify the task/cgroup plugin adding a task_pre_launch_priv function and\n    move slurmstepd outside of the step's cgroup. Work by Matthieu Hautreux.\n -- Intel MIC processor support added using gres/mic plugin. BIG thanks to\n    Olli-Pekka Lehto, CSC-IT Center for Science Ltd.\n -- Accounting - Change empty jobacctinfo structs to not actually be used\n    instead of putting 0's into the database we put NO_VALS and have sacct\n    figure out jobacct_gather wasn't used.\n -- Cray - Prevent calling basil_confirm more than once per job using a flag.\n -- Fix bug with topology/tree and job with min-max node count. Now try to\n    get max node count rather than minimizing leaf switches used.\n -- Add AccountingStorageEnforce=safe option to provide method to avoid jobs\n    launching that wouldn't be able to run to completion because of a\n    GrpCPUMins limit.\n -- Add support for RFC 5424 timestamps in logfiles. Disable with configuration\n    option of \"--disable-rfc5424time\". By Janne Blomqvist, Aalto University.\n -- CRAY - Replace srun.pl with launch/aprun plugin to use srun to wrap the\n    aprun process instead of a perl script.\n -- srun - Rename --runjob-opts to --launcher-opts to be used on systems other\n    than BGQ.\n -- Added new DebugFlags - Energy for AcctGatherEnergy plugins.\n -- start deprecation of sacct --dump --fdump\n -- BGQ - added --verbose=OFF when srun --quiet is used\n -- Added acct_gather_energy/rapl plugin to record power consumption by job.\n    Work by Yiannis Georgiou, Martin Perry, et. al., Bull\n\n* Changes in SLURM 2.5.0.pre3\n=============================\n -- Add Google search to all web pages.\n -- Add sinfo -T option to print reservation information. Work by Bill Brophy,\n    Bull.\n -- Force slurmd exit after 2 minute wait, even if threads are hung.\n -- Change node_req field in struct job_resources from 8 to 32 bits so we can\n    run more than 256 jobs per node.\n -- sched/backfill: Improve accuracy of expected job start with respect to\n    reservations.\n -- sinfo partition field size will be set the the length of the longest\n    partition name by default.\n -- Make it so the parse_time will return a valid 0 if given epoch time and\n    set errno == ESLURM_INVALID_TIME_VALUE on error instead.\n -- Correct srun --no-alloc logic when node count exceeds node list or task\n    task count is not a multiple of the node count. Work by Hongjia Cao, NUDT.\n -- Completed integration with IBM Parallel Environment including POE and IBM's\n    NRT switch library.\n\n* Changes in SLURM 2.5.0.pre2\n=============================\n -- When running with multiple slurmd daemons per node, enable specifying a\n    range of ports on a single line of the node configuration in slurm.conf.\n -- Add reservation flag of Part_Nodes to allocate all nodes in a partition to\n    a reservation and automatically change the reservation when nodes are\n    added to or removed from the reservation. Based upon work by\n    Bill Brophy, Bull.\n -- Add support for advanced reservation for specific cores rather than whole\n    nodes. Current limiations: homogeneous cluster, nodes idle when reservation\n    created, and no more than one reservation per node. Code is still under\n    development. Work by Alejandro Lucero Palau, et. al, BSC.\n -- Add DebugFlag of Switch to log switch plugin details.\n -- Correct job node_cnt value in job completion plugin when job fails due to\n    down node. Previously was too low by one.\n -- Add new srun option --cpu-freq to enable user control over the job's CPU\n    frequency and thus it's power consumption. NOTE: cpu frequency is not\n    currently preserved for jobs being suspended and later resumed. Work by\n    Don Albert, Bull.\n -- Add node configuration information about \"boards\" and optimize task\n    placement on minimum number of boards. Work by Rod Schultz, Bull.\n\n* Changes in SLURM 2.5.0.pre1\n=============================\n -- Add new output to \"scontrol show configuration\" of LicensesUsed. Output is\n    \"name:used/total\"\n -- Changed jobacct_gather plugin infrastructure to be cleaner and easier to\n    maintain.\n -- Change license option count separator from \"*\" to \":\" for consistency with\n    the gres option (e.g. \"--licenses=foo:2 --gres=gpu:2\"). The \"*\" will still\n    be accepted, but is no longer documented.\n -- Permit more than 100 jobs to be scheduled per node (new limit is 250\n    jobs).\n -- Restructure of srun code to allow outside programs to utilize existing\n    logic.\n\n* Changes in SLURM 2.4.6\n========================\n -- Correct WillRun authentication logic when issued for non-job owner.\n -- BGQ - fix memory leak\n -- BGQ - Fix to check block for action 'D' if it also has nodes in error.\n\n* Changes in SLURM 2.4.5\n========================\n -- Cray - On job kill requeust, send SIGCONT, SIGTERM, wait KillWait and send\n    SIGKILL. Previously just sent SIGKILL to tasks.\n -- BGQ - Fix issue when running srun outside of an allocation and only\n    specifying the number of tasks and not the number of nodes.\n -- BGQ - validate correct ntasks_per_node\n -- BGQ - when srun -Q is given make runjob be quiet\n -- Modify use of OOM (out of memory protection) for Linux 2.6.36 kernel\n    or later. NOTE: If you were setting the environment variable\n    SLURMSTEPD_OOM_ADJ=-17, it should be set to -1000 for Linux 2.6.36 kernel\n    or later.\n -- BGQ - Fix job step timeout actually happen when done from within an\n    allocation.\n -- Reset node MAINT state flag when a reservation's nodes or flags change.\n -- Accounting - Fix issue where QOS usage was being zeroed out on a\n    slurmctld restart.\n -- BGQ - Add 64 tasks per node as a valid option for srun when used with\n    overcommit.\n -- BLUEGENE - With Dynamic layout mode - Fix issue where if a larger block\n    was already in error and isn't deallocating and underlying hardware goes\n    bad one could get overlapping blocks in error making the code assert when\n    a new job request comes in.\n -- BGQ - handle pending actions on a block better when trying to deallocate it.\n -- Accounting - Fixed issue where if nodenames have changed on a system and\n    you query against that with -N and -E you will get all jobs during that\n    time instead of only the ones running on -N.\n -- BGP - Fix for HTC mode\n -- Accounting - If a job start message fails to the SlurmDBD reset the db_inx\n    so it gets sent again.  This isn't a major problem since the start will\n    happen when the job ends, but this does make things cleaner.\n -- If an salloc is waiting for an allocation to happen and is canceled by the\n    user mark the state canceled instead of completed.\n -- Fix issue in accounting if a user puts a '\\' in their job name.\n -- Accounting - Fix for if asking for users or accounts that were deleted\n    with associations get the deleted associations as well.\n -- BGQ - Handle shared blocks that need to be removed and have jobs running\n    on them.  This should only happen in extreme conditions.\n -- Fix inconsistency for hostlists that have more than 1 range.\n -- BGQ - Add mutex around recovery for the Real Time server to avoid hitting\n    DB2 so hard.\n -- BGQ - If an allocation exists on a block that has a 'D' action on it fail\n    job on future step creation attempts.\n\n* Changes in SLURM 2.4.4\n========================\n -- BGQ - minor fix to make build work in emulated mode.\n -- BGQ - Fix if large block goes into error and the next highest priority jobs\n    are planning on using the block.  Previously it would fail those jobs\n    erroneously.\n -- BGQ - Fix issue when a cnode going to an error (not SoftwareError) state\n    with a job running or trying to run on it.\n -- Execute slurm_spank_job_epilog when there is no system Epilog configured.\n -- Fix for srun --test-only to work correctly with timelimits\n -- BGQ - If a job goes away while still trying to free it up in the\n    database, and the job is running on a small block make sure we free up\n    the correct node count.\n -- BGQ - Logic added to make sure a job has finished on a block before it is\n    purged from the system if its front-end node goes down.\n -- Modify strigger so that a filter option of \"--user=0\" is supported.\n -- Correct --mem-per-cpu logic for core or socket allocations with multiple\n    threads per core.\n -- Fix for older < glibc 2.4 systems to use euidaccess() instead of eaccess().\n -- BLUEGENE - Do not alter a pending job's node count when changing it's\n    partition.\n -- BGQ - Add functionality to make it so we track the actions on a block.\n    This is needed for when a free request is added to a block but there are\n    jobs finishing up so we don't start new jobs on the block since they will\n    fail on start.\n -- BGQ - Fixed InactiveLimit to work correctly to avoid scenarios where a\n    user's pending allocation was started with srun and then for some reason\n    the slurmctld was brought down and while it was down the srun was removed.\n -- Fixed InactiveLimit math to work correctly\n -- BGQ - Add logic to make it so blocks can't use a midplane with a nodeboard\n    in error for passthrough.\n -- BGQ - Make it so if a nodeboard goes in error any block using that midplane\n    for passthrough gets removed on a dynamic system.\n -- BGQ - Fix for printing realtime server debug correctly.\n -- BGQ - Cleaner handling of cnode failures when reported through the runjob\n    interface instead of through the normal method.\n -- smap - spread node information across multiple lines for larger systems.\n -- Cray - Defer salloc until after PrologSlurmctld completes.\n -- Correction to slurmdbd communications failure handling logic, incorrect\n    error codes returned in some cases.\n\n* Changes in SLURM 2.4.3\n========================\n -- Accounting - Fix so complete 32 bit numbers can be put in for a priority.\n -- cgroups - fix if initial directory is non-existent SLURM creates it\n    correctly.  Before the errno wasn't being checked correctly\n -- BGQ - fixed srun when only requesting a task count and not a node count\n    to operate the same way salloc or sbatch did and assign a task per cpu\n    by default instead of task per node.\n -- Fix salloc --gid to work correctly.  Reported by Brian Gilmer\n -- BGQ - fix smap to set the correct default MloaderImage\n -- BLUEGENE - updated documentation.\n -- Close the batch job's environment file when it contains no data to avoid\n    leaking file descriptors.\n -- Fix sbcast's credential to last till the end of a job instead of the\n    previous 20 minute time limit.  The previous behavior would fail for\n    large files 20 minutes into the transfer.\n -- Return ESLURM_NODES_BUSY rather than ESLURM_NODE_NOT_AVAIL error on job\n    submit when required nodes are up, but completing a job or in exclusive\n    job allocation.\n -- Add HWLOC_FLAGS so linking to libslurm works correctly\n -- BGQ - If using backfill and a shared block is running at least one job\n    and a job comes through backfill and can fit on the block without ending\n    jobs don't set an end_time for the running jobs since they don't need to\n    end to start the job.\n -- Initialize bind_verbose when using task/cgroup.\n -- BGQ - Fix for handling backfill much better when sharing blocks.\n -- BGQ - Fix for making small blocks on first pass if not sharing blocks.\n -- BLUEGENE - Remove force of default conn_type instead of leaving NAV\n    when none are requested.  The Block allocator sets it up temporarily so\n    this isn't needed.\n -- BLUEGENE - Fix deadlock issue when dealing with bad hardware if using\n    static blocks.\n -- Fix to mysql plugin during rollup to only query suspended table when jobs\n    reported some suspended time.\n -- Fix compile with glibc 2.16 (Kacper Kowalik)\n -- BGQ - fix for deadlock where a block has error on it and all jobs\n    running on it are preemptable by scheduling job.\n -- proctrack/cgroup: Exclude internal threads from \"scontrol list pids\".\n    Patch from Matthieu Hautreux, CEA.\n -- Memory leak fixed for select/linear when preempting jobs.\n -- Fix if updating begin time of a job to update the eligible time in\n    accounting as well.\n -- BGQ - make it so you can signal steps when signaling the job allocation.\n -- BGQ - Remove extra overhead if a large block has many cnode failures.\n -- Priority/Multifactor - Fix issue with age factor when a job is estimated to\n    start in the future but is able to run now.\n -- CRAY - update to work with ALPS 5.1\n -- BGQ - Handle issue of speed and mutexes when polling instead of using the\n    realtime server.\n -- BGQ - Fix minor sorting issue with sview when sorting by midplanes.\n -- Accounting - Fix for handling per user max node/cpus limits on a QOS\n    correctly for current job.\n -- Update documentation for -/+= when updating a reservation's\n    users/accounts/flags\n -- Update pam module to work if using aliases on nodes instead of actual\n    host names.\n -- Correction to task layout logic in select/cons_res for job with minimum\n    and maximum node count.\n -- BGQ - Put final poll after realtime comes back into service to avoid\n    having the realtime server go down over and over again while waiting\n    for the poll to finish.\n -- task/cgroup/memory - ensure that ConstrainSwapSpace=no is correctly\n    handled. Work by Matthieu Hautreux, CEA.\n -- CRAY - Fix for sacct -N option to work correctly\n -- CRAY - Update documentation to describe installation from rpm instead\n    or previous piecemeal method.\n -- Fix sacct to work with QOS' that have previously been deleted.\n -- Added all available limits to the output of sacctmgr list qos\n\n* Changes in SLURM 2.4.2\n========================\n -- BLUEGENE - Correct potential deadlock issue when hardware goes bad and\n    there are jobs running on that hardware.\n -- If job is submitted to more than one partition, it's partition pointer can\n    be set to an invalid value. This can result in the count of CPUs allocated\n    on a node being bad, resulting in over- or under-allocation of its CPUs.\n    Patch by Carles Fenoy, BSC.\n -- Fix bug in task layout with select/cons_res plugin and --ntasks-per-node\n    option. Patch by Martin Perry, Bull.\n -- BLUEGENE - remove race condition where if a block is removed while waiting\n    for a job to finish on it the number of unused cpus wasn't updated\n    correctly.\n -- BGQ - make sure we have a valid block when creating or finishing a step\n    allocation.\n -- BLUEGENE - If a large block (> 1 midplane) is in error and underlying\n    hardware is marked bad remove the larger block and create a block over\n    just the bad hardware making the other hardware available to run on.\n -- BLUEGENE - Handle job completion correctly if an admin removes a block\n    where other blocks on an overlapping midplane are running jobs.\n -- BLUEGENE - correctly remove running jobs when freeing a block.\n -- BGQ - correct logic to place multiple (< 1 midplane) steps inside a\n    multi midplane block allocation.\n -- BGQ - Make it possible for a multi midplane allocation to run on more\n    than 1 midplane but not the entire allocation.\n -- BGL - Fix for syncing users on block from Tim Wickberg\n -- Fix initialization of protocol_version for some messages to make sure it\n    is always set when sending or receiving a message.\n -- Reset backfilled job counter only when explicitly cleared using scontrol.\n    Patch from Alejandro Lucero Palau, BSC.\n -- BLUEGENE - Fix for handling blocks when a larger block will not free and\n    while it is attempting to free underlying hardware is marked in error\n    making small blocks overlapping with the freeing block.  This only\n    applies to dynamic layout mode.\n -- Cray and BlueGene - Do not treat lack of usable front-end nodes when\n    slurmctld deamon starts as a fatal error. Also preserve correct front-end\n    node for jobs when there is more than one front-end node and the slurmctld\n    daemon restarts.\n -- Correct parsing of srun/sbatch input/output/error file names so that only\n    the name \"none\" is mapped to /dev/null and not any file name starting\n    with \"none\" (e.g. \"none.o\").\n -- BGQ - added version string to the load of the runjob_mux plugin to verify\n    the current plugin has been loaded when using runjob_mux_refresh_config\n -- CGROUPS - Use system mount/umount function calls instead of doing fork\n    exec of mount/umount from Janne Blomqvist.\n -- BLUEGENE - correct start time setup when no jobs are blocking the way\n    from Mark Nelson\n -- Fixed sacct --state=S query to return information about suspended jobs\n    current or in the past.\n -- FRONTEND - Made error warning more apparent if a frontend node isn't\n    configured correctly.\n -- BGQ - update documentation about runjob_mux_refresh_config which works\n    correctly as of IBM driver V1R1M1 efix 008.\n\n* Changes in SLURM 2.4.1\n========================\n -- Fix bug for job state change from 2.3 -> 2.4 job state can now be preserved\n    correctly when transitioning.  This also applies for 2.4.0 -> 2.4.1, no\n    state will be lost. (Thanks to Carles Fenoy)\n\n* Changes in SLURM 2.4.0\n========================\n -- Cray - Improve support for zero compute note resource allocations.\n    Partition used can now be configured with no nodes nodes.\n -- BGQ - make it so srun -i<taskid> works correctly.\n -- Fix parse_uint32/16 to complain if a non-digit is given.\n -- Add SUBMITHOST to job state passed to Moab vial sched/wiki2. Patch by Jon\n    Bringhurst (LANL).\n -- BGQ - Fix issue when running with AllowSubBlockAllocations=Yes without\n    compiling with --enable-debug\n -- Modify scontrol to require \"-dd\" option to report batch job's script. Patch\n    from Don Albert, Bull.\n -- Modify SchedulerParamters option to match documentation: \"bf_res=\"\n    changed to \"bf_resolution=\". Patch from Rod Schultz, Bull.\n -- Fix bug that clears job pending reason field. Patch fron Don Lipari, LLNL.\n -- In etc/init.d/slurm move check for scontrol after sourcing\n    /etc/sysconfig/slurm. Patch from Andy Wettstein, University of Chicago.\n -- Fix in scheduling logic that can delay jobs with min/max node counts.\n -- BGQ - fix issue where if a step uses the entire allocation and then\n    the next step in the allocation only uses part of the allocation it gets\n    the correct cnodes.\n -- BGQ - Fix checking for IO on a block with new IBM driver V1R1M1 previous\n    function didn't always work correctly.\n -- BGQ - Fix issue when a nodeboard goes down and you want to combine blocks\n    to make a larger small block and are running with sub-blocks.\n -- BLUEGENE - Better logic for making small blocks around bad nodeboard/card.\n -- BGQ - When using an old IBM driver cnodes that go into error because of\n    a job kill timeout aren't always reported to the system.  This is now\n    handled by the runjob_mux plugin.\n -- BGQ - Added information on how to setup the runjob_mux to run as SlurmUser.\n -- Improve memory consumption on step layouts with high task count.\n -- BGQ - quiter debug when the real time server comes back but there are\n    still messages we find when we poll but haven't given it back to the real\n    time yet.\n -- BGQ - fix for if a request comes in smaller than the smallest block and\n    we must use a small block instead of a shared midplane block.\n -- Fix issues on large jobs (>64k tasks) to have the correct counter type when\n    packing the step layout structure.\n -- BGQ - fix issue where if a user was asking for tasks and ntasks-per-node\n    but not node count the node count is correctly figured out.\n -- Move logic to always use the 1st alphanumeric node as the batch host for\n    batch jobs.\n -- BLUEGENE - fix race condition where if a nodeboard/card goes down at the\n    same time a block is destroyed and that block just happens to be the\n    smallest overlapping block over the bad hardware.\n -- Fix bug when querying accounting looking for a job node size.\n -- BLUEGENE - fix possible race condition if cleaning up a block and the\n    removal of the job on the block failed.\n -- BLUEGENE - fix issue if a cable was in an error state make it so we can\n    check if a block is still makable if the cable wasn't in error.\n -- Put nodes names in alphabetic order in node table.\n -- If preempted job should have a grace time and preempt mode is not cancel\n    but job is going to be canceled because it is interactive or other reason\n    it now receives the grace time.\n -- BGQ - Modified documents to explain new plugin_flags needed in bg.properties\n    in order for the runjob_mux to run correctly.\n -- BGQ - change linking from libslurm.o to libslurmhelper.la to avoid warning.\n\n* Changes in SLURM 2.4.0.rc1\n=============================\n -- Improve task binding logic by making fuller use of HWLOC library,\n    especially with respect to Opteron 6000 series processors. Work contributed\n    by Komoto Masahiro.\n -- Add new configuration parameter PriorityFlags, based upon work by\n    Carles Fenoy (Barcelona Supercomputer Center).\n -- Modify the step completion RPC between slurmd and slurmstepd in order to\n    eliminate a possible deadlock. Based on work by Matthieu Hautreux, CEA.\n -- Change the owner of slurmctld and slurmdbd log files to the appropriate\n    user. Without this change the files will be created by and owned by the\n    user starting the daemons (likely user root).\n -- Reorganize the slurmstepd logic in order to better support NFS and\n    Kerberos credentials via the AUKS plugin. Work by Matthieu Hautreux, CEA.\n -- Fix bug in allocating GRES that are associated with specific CPUs. In some\n    cases the code allocated first available GRES to job instead of allocating\n    GRES accessible to the specific CPUs allocated to the job.\n -- spank: Add callbacks in slurmd: slurm_spank_slurmd_{init,exit}\n    and job epilog/prolog: slurm_spank_job_{prolog,epilog}\n -- spank: Add spank_option_getopt() function to api\n -- Change resolution of switch wait time from minutes to seconds.\n -- Added CrpCPUMins to the output of sshare -l for those using hard limit\n    accounting.  Work contributed by Mark Nelson.\n -- Added mpi/pmi2 plugin for complete support of pmi2 including acquiring\n    additional resources for newly launched tasks. Contributed by Hongjia Cao,\n    NUDT.\n -- BGQ - fixed issue where if a user asked for a specific node count and more\n    tasks than possible without overcommit the request would be allowed on more\n    nodes than requested.\n -- Add support for new SchedulerParameters of bf_max_job_user, maximum number\n    of jobs to attempt backfilling per user. Work by Bj\u00f8rn-Helge Mevik,\n    University of Oslo.\n -- BLUEGENE - fixed issue where MaxNodes limit on a partition only limited\n    larger than midplane jobs.\n -- Added cpu_run_min to the output of sshare --long.  Work contributed by\n    Mark Nelson.\n -- BGQ - allow regular users to resolve Rack-Midplane to AXYZ coords.\n -- Add sinfo output format option of \"%R\" for partition name without \"*\"\n    appended for default partition.\n -- Cray - Add support for zero compute note resource allocation to run batch\n    script on front-end node with no ALPS reservation. Useful for pre- or post-\n    processing.\n -- Support for cyclic distribution of cpus in task/cgroup plugin from Martin\n    Perry, Bull.\n -- GrpMEM limit for QOSes and associations added Patch from Bj\u00f8rn-Helge Mevik,\n    University of Oslo.\n -- Various performance improvements for up to 500% higher throughput depending\n    upon configuration. Work supported by the Oak Ridge National Laboratory\n    Extreme Scale Systems Center.\n -- Added jobacct_gather/cgroup plugin.  It is not advised to use this in\n    production as it isn't currently complete and doesn't provide an equivalent\n    substitution for jobacct_gather/linux yet. Work by Martin Perry, Bull.\n\n* Changes in SLURM 2.4.0.pre4\n=============================\n -- Add logic to cache GPU file information (bitmap index mapping to device\n    file number) in the slurmd daemon and transfer that information to the\n    slurmstepd whenever a job step is initiated. This is needed to set the\n    appropriate CUDA_VISIBLE_DEVICES environment variable value when the\n    devices are not in strict numeric order (e.g. some GPUs are skipped).\n    Based upon work by Nicolas Bigaouette.\n -- BGQ - Remove ability to make a sub-block with a geometry with one or more\n    of it's dimensions of length 3.  There is a limitation in the IBM I/O\n    subsystem that is problematic with multiple sub-blocks with a dimension\n    of length 3, so we will disallow them to be able to be created.  This\n    mean you if you ask the system for an allocation of 12 c-nodes you will\n    be given 16.  If this is ever fix in BGQ you can remove this patch.\n -- BLUEGENE - Better handling blocks that go into error state or deallocate\n    while jobs are running on them.\n -- BGQ - fix for handling mix of steps running at same time some of which\n    are full allocation jobs, and others that are smaller.\n -- BGQ - fix for core dump after running multiple sub-block jobs on static\n    blocks.\n -- BGQ - fixed sync issue where if a job finishes in SLURM but not in mmcs\n    for a long time after the SLURM job has been flushed from the system\n    we don't have to worry about rebooting the block to sync the system.\n -- BGQ - In scontrol/sview node counts are now displayed with\n    CnodeCount/CnodeErrCount so to point out there are cnodes in an error state\n    on the block.  Draining the block and having it reboot when all jobs are\n    gone will clear up the cnodes in Software Failure.\n -- Change default SchedulerParameters max_switch_wait field value from 60 to\n    300 seconds.\n -- BGQ - catch errors from the kill option of the runjob client.\n -- BLUEGENE - make it so the epilog runs until slurmctld tells it the job is\n    gone.  Previously it had a timelimit which has proven to not be the right\n    thing.\n -- FRONTEND - fix issue where if a compute node was in a down state and\n    an admin updates the node to idle/resume the compute nodes will go\n    instantly to idle instead of idle* which means no response.\n -- Fix regression in 2.4.0.pre3 where number of submitted jobs limit wasn't\n    being honored for QOS.\n -- Cray - Enable logging of BASIL communications with environment variables.\n    Set XML_LOG to enable logging. Set XML_LOG_LOC to specify path to log file\n    or \"SLURM\" to write to SlurmctldLogFile or unset for \"slurm_basil_xml.log\".\n    Patch from Steve Tronfinoff, CSCS.\n -- FRONTEND - if a front end unexpectedly reboots kill all jobs but don't\n    mark front end node down.\n -- FRONTEND - don't down a front end node if you have an epilog error\n -- BLUEGENE - if a job has an epilog error don't down the midplane it was\n    running on.\n -- BGQ - added new DebugFlag (NoRealTime) for only printing debug from\n    state change while the realtime server is running.\n -- Fix multi-cluster mode with sview starting on a non-bluegene cluster going\n    to a bluegene cluster.\n -- BLUEGENE - ability to show Rack Midplane name of midplanes in sview and\n    scontrol.\n\n* Changes in SLURM 2.4.0.pre3\n=============================\n -- Let a job be submitted even if it exceeds a QOS limit. Job will be left\n    in a pending state until the QOS limit or job parameters change. Patch by\n    Phil Eckert, LLNL.\n -- Add sacct support for the option \"--name\". Work by Yuri D'Elia, Center for\n    Biomedicine, EURAC Research, Italy.\n -- BGQ - handle preemption.\n -- Add an srun shepard process to cancel a job and/or step of the srun process\n    is killed abnormally (e.g. SIGKILL).\n -- BGQ - handle deadlock issue when a nodeboard goes into an error state.\n -- BGQ - more thorough handling of blocks with multiple jobs running on them.\n -- Fix man2html process to compile in the build directory instead of the\n    source dir.\n -- Behavior of srun --multi-prog modified so that any program arguments\n    specified on the command line will be appended to the program arguments\n    specified in the program configuration file.\n -- Add new command, sdiag, which reports a variety of job scheduling\n    statistics. Based upon work by Alejandro Lucero Palau, BSC.\n -- BLUEGENE - Added DefaultConnType to the bluegene.conf file.  This makes it\n    so you can specify any connection type you would like (TORUS or MESH) as\n    the default in dynamic mode.  Previously it always defaulted to TORUS.\n -- Made squeue -n and -w options more consistent with salloc, sbatch, srun,\n    and scancel. Patch by Don Lipari, LLNL.\n -- Have sacctmgr remove user records when no associations exist for that user.\n -- Several header file changes for clean build with NetBSD. Patches from\n    Aleksej Saushev.\n -- Fix for possible deadlock in accounting logic: Avoid calling\n    jobacct_gather_g_getinfo() until there is data to read from the socket.\n -- Fix race condition that could generate \"job_cnt_comp underflow\" errors on\n    front-end architectures.\n -- BGQ - Fix issue where a system with missing cables could cause core dump.\n\n* Changes in SLURM 2.4.0.pre2\n=============================\n -- CRAY - Add support for GPU memory allocation using SLURM GRES (Generic\n    RESource) support. Work by Steve Trofinoff, CSCS.\n -- Add support for job allocations with multiple job constraint counts. For\n    example: salloc -C \"[rack1*2&rack2*4]\" ... will allocate the job 2 nodes\n    from rack1 and 4 nodes from rack2. Support for only a single constraint\n    name been added to job step support.\n -- BGQ - Remove old method for marking cnodes down.\n -- BGQ - Remove BGP images from view in sview.\n -- BGQ - print out failed cnodes in scontrol show nodes.\n -- BGQ - Add srun option of \"--runjob-opts\" to pass options to the runjob\n    command.\n -- FRONTEND - handle step launch failure better.\n -- BGQ - Added a mutex to protect the now changing ba_system pointers.\n -- BGQ - added new functionality for sub-block allocations - no preemption\n    for this yet though.\n -- Add --name option to squeue to filter output by job name. Patch from Yuri\n    D'Elia.\n -- BGQ - Added linking to runjob client libary which gives support to totalview\n    to use srun instead of runjob.\n -- Add numeric range checks to scontrol update options. Patch from Phil\n    Eckert, LLNL.\n -- Add ReconfigFlags configuration option to control actions of \"scontrol\n    reconfig\". Patch from Don Albert, Bull.\n -- BGQ - handle reboots with multiple jobs running on a block.\n -- BGQ - Add message handler thread to forward signals to runjob process.\n\n* Changes in SLURM 2.4.0.pre1\n=============================\n -- BGQ - use the ba_geo_tables to figure out the blocks instead of the old\n    algorithm.  The improves timing in the worst cases and simplifies the code\n    greatly.\n -- BLUEGENE - Change to output tools labels from BP to Midplane\n    (i.e. BP List -> MidplaneList).\n -- BLUEGENE - read MPs and BPs from the bluegene.conf\n -- Modify srun's SIGINT handling logic timer (two SIGINTs within one second) to\n    be based microsecond rather than second timer.\n -- Modify advance reservation to accept multiple specific block sizes rather\n    than a single node count.\n -- Permit administrator to change a job's QOS to any value without validating\n    the job's owner has permission to use that QOS. Based upon patch by Phil\n    Eckert (LLNL).\n -- Add trigger flag for a permanent trigger. The trigger will NOT be purged\n    after an event occurs, but only when explicitly deleted.\n -- Interpret a reservation with Nodes=ALL and a Partition specification as\n    reserving all nodes within the specified partition rather than all nodes\n    on the system. Based upon patch by Phil Eckert (LLNL).\n -- Add the ability to reboot all compute nodes after they become idle. The\n    RebootProgram configuration parameter must be set and an authorized user\n    must execute the command \"scontrol reboot_nodes\". Patch from Andriy\n    Grytsenko (Massive Solutions Limited).\n -- Modify slurmdbd.conf parsing to accept DebugLevel strings (quiet, fatal,\n    info, etc.) in addition to numeric values. The parsing of slurm.conf was\n    modified in the same fashion for SlurmctldDebug and SlurmdDebug values.\n    The output of sview and \"scontrol show config\" was also modified to report\n    those values as strings rather than numeric values.\n -- Changed default value of StateSaveLocation configuration parameter from\n    /tmp to /var/spool.\n -- Prevent associations from being deleted if it has any jobs in running,\n    pending or suspended state. Previous code prevented this only for running\n    jobs.\n -- If a job can not run due to QOS or association limits, then do not cancel\n    the job, but leave it pending in a system held state (priority = 1). The\n    job will run when its limits or the QOS/association limits change. Based\n    upon a patch by Phil Ekcert (LLNL).\n -- BGQ - Added logic to keep track of cnodes in an error state inside of a\n    booted block.\n -- Added the ability to update a node's NodeAddr and NodeHostName with\n    scontrol. Also enable setting a node's state to \"future\" using scontrol.\n -- Add a node state flag of CLOUD and save/restore NodeAddr and NodeHostName\n    information for nodes with a flag of CLOUD.\n -- Cray: Add support for job reservations with node IDs that are not in\n    numeric order. Fix for Bugzilla #5.\n -- BGQ - Fix issue with smap -R\n -- Fix association limit support for jobs queued for multiple partitions.\n -- BLUEGENE - fix issue for sub-midplane systems to create a full system\n    block correctly.\n -- BLUEGENE - Added option to the bluegene.conf to tell you are running on\n    a sub midplane system.\n -- Added the UserID of the user issuing the RPC to the job_submit/lua\n    functions.\n -- Fixed issue where if a job ended with ESLURMD_UID_NOT_FOUND and\n    ESLURMD_GID_NOT_FOUND where slurm would be a little over zealous\n    in treating missing a GID or UID as a fatal error.\n -- If job time limit exceeds partition maximum, but job's minimum time limit\n    does not, set job's time limit to partition maximum at allocation time.\n\n* Changes in SLURM 2.3.6\n========================\n -- Fix DefMemPerCPU for partition definitions.\n -- Fix to create a reservation with licenses and no nodes.\n -- Fix issue with assoc_mgr if a bad state file is given and the database\n    isn't up at the time the slurmctld starts, not running the\n    priority/multifactor plugin, and then the database is started up later.\n -- Gres: If a gres has a count of one and an associated file then when doing\n    a reconfiguration, the node's bitmap was not cleared resulting in an\n    underflow upon job termination or removal from scheduling matrix by the\n    backfill scheduler.\n -- Fix race condition in job dependency logic which can result in invalid\n    memory reference.\n\n* Changes in SLURM 2.3.5\n========================\n -- Improve support for overlapping advanced reservations. Patch from\n    Bill Brophy, Bull.\n -- Modify Makefiles for support of Debian hardening flags. Patch from\n    Simon Ruderich.\n -- CRAY: Fix support for configuration with SlurmdTimeout=0 (never mark\n    node that is DOWN in ALPS as DOWN in SLURM).\n -- Fixed the setting of SLURM_SUBMIT_DIR for jobs submitted by Moab (BZ#1467).\n    Patch by Don Lipari, LLNL.\n -- Correction to init.d/slurmdbd exit code for status option. Patch by Bill\n    Brophy, Bull.\n -- When the optional max_time is not specified for --switches=count, the site\n    max (SchedulerParameters=max_switch_wait=seconds) is used for the job.\n    Based on patch from Rod Schultz.\n -- Fix bug in select/cons_res plugin when used with topology/tree and a node\n    range count in job allocation request.\n -- Fixed moab_2_slurmdb.pl script to correctly work for end records.\n -- Add support for new SchedulerParameters of max_depend_depth defining the\n    maximum number of jobs to test for circular dependencies (i.e. job A waits\n    for job B to start and job B waits for job A to start). Default value is\n    10 jobs.\n -- Fix potential race condition if MinJobAge is very low (i.e. 1) and using\n    slurmdbd accounting and running large amounts of jobs (>50 sec).  Job\n    information could be corrupted before it had a chance to reach the DBD.\n -- Fix state restore of job limit set from admin value for min_cpus.\n -- Fix clearing of limit values if an admin removes the limit for max cpus\n    and time limit where it was previously set by an admin.\n -- Fix issue where log message is more than 256 chars and then has a format.\n -- Fix sched/wiki2 to support job account name, gres, partition name, wckey,\n    or working directory that contains \"#\" (a job record separator). Also fix\n    for wckey or working directory that contains a double quote '\\\"'.\n -- CRAY - fix for handling memory requests from user for an allocation.\n -- Add support for switches parameter to the job_submit/lua plugin. Work by\n    Par Andersson, NSC.\n -- Fix to job preemption logic to preempt multiple jobs at the same time.\n -- Fix minor issue where uid and gid were switched in sview for submitting\n    batch jobs.\n -- Fix possible illegal memory reference in slurmctld for job step with\n    relative option. Work by Matthieu Hautreux (CEA).\n -- Reset priority of system held jobs when dependency is satisfied. Work by\n    Don Lipari, LLNL.\n\n* Changes in SLURM 2.3.4\n========================\n -- Set DEFAULT flag in partition structure when slurmctld reads the\n    configuration file. Patch from R\u00e9mi Palancher.\n -- Fix for possible deadlock in accounting logic: Avoid calling\n    jobacct_gather_g_getinfo() until there is data to read from the socket.\n -- Fix typo in accounting when using reservations. Patch from Alejandro\n    Lucero Palau.\n -- Fix to the multifactor priority plugin to calculate effective usage earlier\n    to give a correct priority on the first decay cycle after a restart of the\n    slurmctld. Patch from Martin Perry, Bull.\n -- Permit user root to run a job step for any job as any user. Patch from\n    Didier Gazen, Laboratoire d'Aerologie.\n -- BLUEGENE - fix for not allowing jobs if all midplanes are drained and all\n    blocks are in an error state.\n -- Avoid slurmctld abort due to bad pointer when setting an advanced\n    reservation MAINT flag if it contains no nodes (only licenses).\n -- Fix bug when requeued batch job is scheduled to run on a different node\n    zero, but attemts job launch on old node zero.\n -- Fix bug in step task distribution when nodes are not configured in numeric\n    order. Patch from Hongjia Cao, NUDT.\n -- Fix for srun allocating running within existing allocation with --exclude\n    option and --nnodes count small enough to remove more nodes. Patch from\n    Phil Eckert, LLNL.\n -- Work around to handle certain combinations of glibc/kernel\n    (i.e. glibc-2.14/Linux-3.1) to correctly open the pty of the slurmstepd\n    as the job user. Patch from Mark Grondona, LLNL.\n -- Modify linking to include \"-ldl\" only when needed. Patch from Aleksej\n    Saushev.\n -- Fix smap regression to display nodes that are drained or down correctly.\n -- Several bug fixes and performance improvements with related to batch\n    scripts containing very large numbers of arguments. Patches from Par\n    Andersson, NSC.\n -- Fixed extremely hard to reproduce threading issue in assoc_mgr.\n -- Correct \"scontrol show daemons\" output if there is more than one\n    ControlMachine configured.\n -- Add node read lock where needed in slurmctld/agent code.\n -- Added test for LUA library named \"liblua5.1.so.0\" in addition to\n    \"liblua5.1.so\" as needed by Debian. Patch by Remi Palancher.\n -- Added partition default_time field to job_submit LUA plugin. Patch by\n    Remi Palancher.\n -- Fix bug in cray/srun wrapper stdin/out/err file handling.\n -- In cray/srun wrapper, only include aprun \"-q\" option when srun \"--quiet\"\n    option is used.\n -- BLUEGENE - fix issue where if a small block was in error it could hold up\n    the queue when trying to place a larger than midplane job.\n -- CRAY - ignore all interactive nodes and jobs on interactive nodes.\n -- Add new job state reason of \"FrontEndDown\" which applies only to Cray and\n    IBM BlueGene systems.\n -- Cray - Enable configure option of \"--enable-salloc-background\" to permit\n    the srun and salloc commands to be executed in the background. This does\n    NOT remove the ALPS limitation that only one job reservation can be created\n    for each Linux session ID.\n -- Cray - For srun wrapper when creating a job allocation, set the default job\n    name to the executable file's name.\n -- Add support for Cray ALPS 5.0.0\n -- FRONTEND - if a front end unexpectedly reboots kill all jobs but don't\n    mark front end node down.\n -- FRONTEND - don't down a front end node if you have an epilog error.\n -- Cray - fix for if a frontend slurmd was started after the slurmctld had\n    already pinged it on startup the unresponding flag would be removed from\n    the frontend node.\n -- Cray - Fix issue on smap not displaying grid correctly.\n -- Fixed minor memory leak in sview.\n\n* Changes in SLURM 2.3.3\n========================\n -- Fix task/cgroup plugin error when used with GRES. Patch by Alexander\n    Bersenev (Institute of Mathematics and Mechanics, Russia).\n -- Permit pending job exceeding a partition limit to run if its QOS flag is\n    modified to permit the partition limit to be exceeded. Patch from Bill\n    Brophy, Bull.\n -- BLUEGENE - Fixed preemption issue.\n -- sacct search for jobs using filtering was ignoring wckey filter.\n -- Fixed issue with QOS preemption when adding new QOS.\n -- Fixed issue with comment field being used in a job finishing before it\n    starts in accounting.\n -- Add slashes in front of derived exit code when modifying a job.\n -- Handle numeric suffix of \"T\" for terabyte units. Patch from John Thiltges,\n    University of Nebraska-Lincoln.\n -- Prevent resetting a held job's priority when updating other job parameters.\n    Patch from Alejandro Lucero Palau, BSC.\n -- Improve logic to import a user's environment. Needed with --get-user-env\n    option used with Moab. Patch from Mark Grondona, LLNL.\n -- Fix bug in sview layout if node count less than configured grid_x_width.\n -- Modify PAM module to prefer to use SLURM library with same major release\n    number that it was built with.\n -- Permit gres count configuration of zero.\n -- Fix race condition where sbcast command can result in deadlock of slurmd\n    daemon. Patch by Don Albert, Bull.\n -- Fix bug in srun --multi-prog configuration file to avoid printing duplicate\n    record error when \"*\" is used at the end of the file for the task ID.\n -- Let operators see reservation data even if \"PrivateData=reservations\" flag\n    is set in slurm.conf. Patch from Don Albert, Bull.\n -- Added new sbatch option \"--export-file\" as needed for latest version of\n    Moab. Patch from Phil Eckert, LLNL.\n -- Fix for sacct printing CPUTime(RAW) where the the is greater than a 32 bit\n    number.\n -- Fix bug in --switch option with topology resulting in bad switch count use.\n    Patch from Alejandro Lucero Palau (Barcelona Supercomputer Center).\n -- Fix PrivateFlags bug when using Priority Multifactor plugin.  If using sprio\n    all jobs would be returned even if the flag was set.\n    Patch from Bill Brophy, Bull.\n -- Fix for possible invalid memory reference in slurmctld in job dependency\n    logic. Patch from Carles Fenoy (Barcelona Supercomputer Center).\n\n* Changes in SLURM 2.3.2\n========================\n -- Add configure option of \"--without-rpath\" which builds SLURM tools without\n    the rpath option, which will work if Munge and BlueGene libraries are in\n    the default library search path and make system updates easier.\n -- Fixed issue where if a job ended with ESLURMD_UID_NOT_FOUND and\n    ESLURMD_GID_NOT_FOUND where slurm would be a little over zealous\n    in treating missing a GID or UID as a fatal error.\n -- Backfill scheduling - Add SchedulerParameters configuration parameter of\n    \"bf_res\" to control the resolution in the backfill scheduler's data about\n    when jobs begin and end. Default value is 60 seconds (used to be 1 second).\n -- Cray - Remove the \"family\" specification from the GPU reservation request.\n -- Updated set_oomadj.c, replacing deprecated oom_adj reference with\n    oom_score_adj\n -- Fix resource allocation bug, generic resources allocation was ignoring the\n    job's ntasks_per_node and cpus_per_task parameters. Patch from Carles\n    Fenoy, BSC.\n -- Avoid orphan job step if slurmctld is down when a job step completes.\n -- Fix Lua link order, patch from P\u00e4r Andersson, NSC.\n -- Set SLURM_CPUS_PER_TASK=1 when user specifies --cpus-per-task=1.\n -- Fix for fatal error managing GRES. Patch by Carles Fenoy, BSC.\n -- Fixed race condition when using the DBD in accounting where if a job\n    wasn't started at the time the eligible message was sent but started\n    before the db_index was returned information like start time would be lost.\n -- Fix issue in accounting where normalized shares could be updated\n    incorrectly when getting fairshare from the parent.\n -- Fixed if not enforcing associations  but want QOS support for a default\n    qos on the cluster to fill that in correctly.\n -- Fix in select/cons_res for \"fatal: cons_res: sync loop not progressing\"\n    with some configurations and job option combinations.\n -- BLUEGNE - Fixed issue with handling HTC modes and rebooting.\n\n* Changes in SLURM 2.3.1\n========================\n -- Do not remove the backup slurmctld's pid file when it assumes control, only\n    when it actually shuts down. Patch from Andriy Grytsenko (Massive Solutions\n    Limited).\n -- Avoid clearing a job's reason from JobHeldAdmin or JobHeldUser when it is\n    otherwise updated using scontrol or sview commands. Patch based upon work\n    by Phil Eckert (LLNL).\n -- BLUEGENE - Fix for if changing the defined blocks in the bluegene.conf and\n    jobs happen to be running on blocks not in the new config.\n -- Many cosmetic modifications to eliminate warning message from GCC version\n    4.6 compiler.\n -- Fix for sview reservation tab when finding correct reservation.\n -- Fix for handling QOS limits per user on a reconfig of the slurmctld.\n -- Do not treat the absence of a gres.conf file as a fatal error on systems\n    configured with GRES, but set GRES counts to zero.\n -- BLUEGENE - Update correctly the state in the reason of a block if an\n    admin sets the state to error.\n -- BLUEGENE - handle reason of blocks in error more correctly between\n    restarts of the slurmctld.\n -- BLUEGENE - Fix minor potential memory leak when setting block error reason.\n -- BLUEGENE - Fix if running in Static/Overlap mode and full system block\n    is in an error state, won't deny jobs.\n -- Fix for accounting where your cluster isn't numbered in counting order\n    (i.e. 1-9,0 instead of 0-9).  The bug would cause 'sacct -N nodename' to\n    not give correct results on these systems.\n -- Fix to GRES allocation logic when resources are associated with specific\n    CPUs on a node. Patch from Steve Trofinoff, CSCS.\n -- Fix bugs in sched/backfill with respect to QOS reservation support and job\n    time limits. Patch from Alejandro Lucero Palau (Barcelona Supercomputer\n    Center).\n -- BGQ - fix to set up corner correctly for sub block jobs.\n -- Major re-write of the CPU Management User and Administrator Guide (web\n    page) by Martin Perry, Bull.\n -- BLUEGENE - If removing blocks from system that once existed cleanup of old\n    block happens correctly now.\n -- Prevent slurmctld crashing with configuration of MaxMemPerCPU=0.\n -- Prevent job hold by operator or account coordinator of his own job from\n    being an Administrator Hold rather than User Hold by default.\n -- Cray - Fix for srun.pl parsing to avoid adding spaces between option and\n    argument (e.g. \"-N2\" parsed properly without changing to \"-N 2\").\n -- Major updates to cgroup support by Mark Grondona (LLNL) and Matthieu\n    Hautreux (CEA) and Sam Lang. Fixes timing problems with respect to the\n    task_epilog. Allows cgroup mount point to be configurable. Added new\n    configuration parameters MaxRAMPercent and MaxSwapPercent. Allow cgroup\n    configuration parameters that are precentages to be floating point.\n -- Fixed issue where sview wasn't displaying correct nice value for jobs.\n -- Fixed issue where sview wasn't displaying correct min memory per node/cpu\n    value for jobs.\n -- Disable some SelectTypeParameters for select/linear that aren't compatible.\n -- Move slurm_select_init to proper place to avoid loading multiple select\n    plugins in the slurmd.\n -- BGQ - Include runjob_plugin.so in the bluegene rpm.\n -- Report correct job \"Reason\" if needed nodes are DOWN, DRAINED, or\n    NOT_RESPONDING, \"Resources\" rather than \"PartitionNodeLimit\".\n -- BLUEGENE - Fixed issues with running on a sub-midplane system.\n -- Added some missing calls to allow older versions of SLURM to talk to newer.\n -- BGQ - allow steps to be ran.\n -- Do not attempt to run HeathCheckProgram on powered down nodes. Patch from\n    Ramiro Alba, Centre Tecnol\u00f2gic de Tranfer\u00e8ncia de Calor, Spain.\n\n* Changes in SLURM 2.3.0-2\n==========================\n -- Fix for memory issue inside sview.\n -- Fix issue where if a job was pending and the slurmctld was restarted a\n    variable wasn't initialized in the job structure making it so that job\n    wouldn't run.\n\n* Changes in SLURM 2.3.0\n========================\n -- BLUEGENE - make sure we only set the jobinfo_select start_loc on a job\n    when we are on a small block, not a regular one.\n -- BGQ - fix issue where not copying the correct amount of memory.\n -- BLUEGENE - fix clean start if jobs were running when the slurmctld was\n    shutdown and then the system size changed.  This would probably only happen\n    if you were emulating a system.\n -- Fix sview for calling a cray system from a non-cray system to get the\n    correct geometry of the system.\n -- BLUEGENE - fix to correctly import pervious version of block state file.\n -- BLUEGENE - handle loading better when doing a clean start with static\n    blocks.\n -- Add sinfo format and sort option \"%n\" for NodeHostName and \"%o\" for\n    NodeAddr.\n -- If a job is deferred due to partition limits, then re-test those limits\n    after a partition is modified. Patch from Don Lipari.\n -- Fix bug which would crash slurmcld if job's owner (not root) tries to clear\n    a job's licenses by setting value to \"\".\n -- Cosmetic fix for printing out debug info in the priority plugin.\n -- In sview when switching from a bluegene machine to a regular linux cluster\n    and vice versa the node->base partition lists will be displayed if setup\n    in your .slurm/sviewrc file.\n -- BLUEGENE - Fix for creating full system static block on a BGQ system.\n -- BLUEGENE - Fix deadlock issue if toggling between Dynamic and Static block\n    allocation with jobs running on blocks that don't exist in the static\n    setup.\n -- BLUEGENE - Modify code to only give HTC states to BGP systems and not\n    allow them on Q systems.\n -- BLUEGENE - Make it possible for an admin to define multiple dimension\n    conn_types in a block definition.\n -- BGQ - Alter tools to output multiple dimensional conn_type.\n\n* Changes in SLURM 2.3.0.rc2\n============================\n -- With sched/wiki or sched/wiki2 (Maui or Moab scheduler), insure that a\n    requeued job's priority is reset to zero.\n -- BLUEGENE - fix to run steps correctly in a BGL/P emulated system.\n -- Fixed issue where if there was a network issue between the slurmctld and\n    the DBD where both remained up but were disconnected the slurmctld would\n    get registered again with the DBD.\n -- Fixed issue where if the DBD connection from the ctld goes away because of\n    a POLLERR the dbd_fail callback is called.\n -- BLUEGENE - Fix to smap command-line mode display.\n -- Change in GRES behavior for job steps: A job step's default generic\n    resource allocation will be set to that of the job. If a job step's --gres\n    value is set to \"none\" then none of the generic resources which have been\n    allocated to the job will be allocated to the job step.\n -- Add srun environment value of SLURM_STEP_GRES to set default --gres value\n    for a job step.\n -- Require SchedulerTimeSlice configuration parameter to be at least 5 seconds\n    to avoid thrashing slurmd daemon.\n -- Cray - Fix to make nodes state in accounting consistent with state set by\n    ALPS.\n -- Cray - A node DOWN to ALPS will be marked DOWN to SLURM only after reaching\n    SlurmdTimeout. In the interim, the node state will be NO_RESPOND. This\n    change makes behavior makes SLURM handling of the node DOWN state more\n    consistent with ALPS. This change effects only Cray systems.\n -- Cray - Fix to work with 4.0.* instead of just 4.0.0\n -- Cray - Modify srun/aprun wrapper to map --exclusive to -F exclusive and\n    --share to -F share. Note this does not consider the partition's Shared\n    configuration, so it is an imperfect mapping of options.\n -- BLUEGENE - Added notice in the print config to tell if you are emulated\n    or not.\n -- BLUEGENE - Fix job step scalability issue with large task count.\n -- BGQ - Improved c-node selection when asked for a sub-block job that\n    cannot fit into the available shape.\n -- BLUEGENE - Modify \"scontrol show step\" to show  I/O nodes (BGL and BGP) or\n    c-nodes (BGQ) allocated to each step. Change field name from \"Nodes=\" to\n    \"BP_List=\".\n -- Code cleanup on step request to get the correct select_jobinfo.\n -- Memory leak fixed for rolling up accounting with down clusters.\n -- BGQ - fix issue where if first job step is the entire block and then the\n    next parallel step is ran on a sub block, SLURM won't over subscribe cnodes.\n -- Treat duplicate switch name in topology.conf as fatal error. Patch from Rod\n    Schultz, Bull\n -- Minor update to documentation describing the AllowGroups option for a\n    partition in the slurm.conf.\n -- Fix problem with _job_create() when not using qos's.  It makes\n    _job_create() consistent with similar logic in select_nodes().\n -- GrpCPURunMins in a QOS flushed out.\n -- Fix for squeue -t \"CONFIGURING\" to actually work.\n -- CRAY - Add cray.conf parameter of SyncTimeout, maximum time to defer job\n    scheduling if SLURM node or job state are out of synchronization with ALPS.\n -- If salloc was run as interactive, with job control, reset the foreground\n    process group of the terminal to the process group of the parent pid before\n    exiting. Patch from Don Albert, Bull.\n -- BGQ - set up the corner of a sub block correctly based on a relative\n    position in the block instead of absolute.\n -- BGQ - make sure the recently added select_jobinfo of a step launch request\n    isn't sent to the slurmd where environment variables would be overwritten\n    incorrectly.\n\n* Changes in SLURM 2.3.0.rc1\n============================\n -- NOTE THERE HAVE BEEN NEW FIELDS ADDED TO THE JOB AND PARTITION STATE SAVE\n    FILES AND RPCS. PENDING AND RUNNING JOBS WILL BE LOST WHEN UPGRADING FROM\n    EARLIER VERSION 2.3 PRE-RELEASES AND RPCS WILL NOT WORK WITH EARLIER\n    VERSIONS.\n -- select/cray: Add support for Accelerator information including model and\n    memory options.\n -- Cray systems: Add support to suspend/resume salloc command to insure that\n    aprun does not get initiated when the job is suspended. Processes suspended\n    and resumed are determined by using process group ID and parent process ID,\n    so some processes may be missed. Since salloc runs as a normal user, it's\n    ability to identify processes associated with a job is limited.\n -- Cray systems: Modify smap and sview to display all nodes even if multiple\n    nodes exist at each coordinate.\n -- Improve efficiency of select/linear plugin with topology/tree plugin\n    configured, Patch by Andriy Grytsenko (Massive Solutions Limited).\n -- For front-end architectures on which job steps are run (emulated Cray and\n    BlueGene systems only), fix bug that would free memory still in use.\n -- Add squeue support to display a job's license information. Patch by Andy\n    Roosen (University of Deleware).\n -- Add flag to the select APIs for job suspend/resume indicating if the action\n    is for gang scheduling or an explicit job suspend/resume by the user. Only\n    an explicit job suspend/resume will reset the job's priority and make\n    resources exclusively held by the job available to other jobs.\n -- Fix possible invalid memory reference in sched/backfill. Patch by Andriy\n    Grytsenko (Massive Solutions Limited).\n -- Add select_jobinfo to the task launch RPC. Based upon patch by Andriy\n    Grytsenko (Massive Solutions Limited).\n -- Add DefMemPerCPU/Node and MaxMemPerCPU/Node to partition configuration.\n    This improves flexibility when gang scheduling only specific partitions.\n -- Added new enums to print out when a job is held by a QOS instead of an\n    association limit.\n -- Enhancements to sched/backfill performance with select/cons_res plugin.\n    Patch from Bj\u00f8rn-Helge Mevik, University of Oslo.\n -- Correct job run time reported by smap for suspended jobs.\n -- Improve job preemption logic to avoid preempting more jobs than needed.\n -- Add contribs/arrayrun tool providing support for job arrays. Contributed by\n    Bj\u00f8rn-Helge Mevik, University of Oslo. NOTE: Not currently packaged as RPM\n    and manual file editing is required.\n -- When suspending a job, wait 2 seconds instead of 1 second between sending\n    SIGTSTP and SIGSTOP. Some MPI implementation were not stopping within the\n    1 second delay.\n -- Add support for managing devices based upon Linux cgroup container. Based\n    upon patch by Yiannis Georgiou, Bull.\n -- Fix memory buffering bug if a AllowGroups parameter of a partition has 100\n    or more users. Patch by Andriy Grytsenko (Massive Solutions Limited).\n -- Fix bug in generic resource tracking of gres associated with specific CPUs.\n    Resources were being over-allocated.\n -- On systems with front-end nodes (IBM BlueGene and Cray) limit batch jobs to\n    only one CPU of these shared resources.\n -- Set SLURM_MEM_PER_CPU or SLURM_MEM_PER_NODE environment variables for both\n    interactive (salloc) and batch jobs if the job has a memory limit. For Cray\n    systems also set CRAY_AUTO_APRUN_OPTIONS environment variable with the\n    memory limit.\n -- Fix bug in select/cons_res task distribution logic when tasks-per-node=0.\n    Patch from Rod Schultz, Bull.\n -- Restore node configuration information (CPUs, memory, etc.) for powered\n    down when slurmctld daemon restarts rather than waiting for the node to be\n    restored to service and getting the information from the node (NOTE: Only\n    relevent if FastSchedule=0).\n -- For Cray systems with the srun2aprun wrapper, rebuild the srun man page\n    identifying the srun optioins which are valid on that system.\n -- BlueGene: Permit users to specify a separate connection type for each\n    dimension (e.g. \"--conn-type=torus,mesh,torus\").\n -- Add the ability for a user to limit the number of leaf switches in a job's\n    allocation using the --switch option of salloc, sbatch and srun. There is\n    also a new SchedulerParameters value of max_switch_wait, which a SLURM\n    administrator can used to set a maximum job delay and prevent a user job\n    from blocking lower priority jobs for too long. Based on work by Rod\n    Schultz, Bull.\n\n* Changes in SLURM 2.3.0.pre6\n=============================\n -- NOTE: THERE HAS BEEN A NEW FIELD ADDED TO THE CONFIGURATION RESPONSE RPC\n    AS SHOWN BY \"SCONTROL SHOW CONFIG\". THIS FUNCTION WILL ONLY WORK WHEN THE\n    SERVER AND CLIENT ARE BOTH RUNNING SLURM VERSION 2.3.0.pre6\n -- Modify job expansion logic to support licenses, generic resources, and\n    currently running job steps.\n -- Added an rpath if using the --with-munge option of configure.\n -- Add support for multiple sets of DEFAULT node, partition, and frontend\n    specifications in slurm.conf so that default values can be changed mulitple\n    times as the configuration file is read.\n -- BLUEGENE - Improved logic to place small blocks in free space before freeing\n    larger blocks.\n -- Add optional argument to srun's --kill-on-bad-exit so that user can set\n    its value to zero and override a SLURM configuration parameter of\n    KillOnBadExit.\n -- Fix bug in GraceTime support for preempted jobs that prevented proper\n    operation when more than one job was being preempted. Based on patch from\n    Bill Brophy, Bull.\n -- Fix for running sview from a non-bluegene cluster to a bluegene cluster.\n    Regression from pre5.\n -- If job's TMPDIR environment is not set or is not usable, reset to \"/tmp\".\n    Patch from Andriy Grytsenko (Massive Solutions Limited).\n -- Remove logic for defunct RPC: DBD_GET_JOBS.\n -- Propagate DebugFlag changes by scontrol to the plugins.\n -- Improve accuracy of REQUEST_JOB_WILL_RUN start time with respect to higher\n    priority pending jobs.\n -- Add -R/--reservation option to squeue command as a job filter.\n -- Add scancel support for --clusters option.\n -- Note that scontrol and sprio can only support a single cluster at one time.\n -- Add support to salloc for a new environment variable SALLOC_KILL_CMD.\n -- Add scontrol ability to increment or decrement a job or step time limit.\n -- Add support for SLURM_TIME_FORMAT environment variable to control time\n    stamp output format. Work by Gerrit Renker, CSCS.\n -- Fix error handling in mvapich plugin that could cause srun to enter an\n    infinite loop under rare circumstances.\n -- Add support for multiple task plugins. Patch from Andriy Grytsenko (Massive\n    Solutions Limited).\n -- Addition of per-user node/cpu limits for QOS's. Patch from Aaron Knister,\n    UMBC.\n -- Fix logic for multiple job resize operations.\n -- BLUEGENE - many fixes to make things work correctly on a L/P system.\n -- Fix bug in layout of job step with --nodelist option plus node count. Old\n    code could allocate too few nodes.\n\n* Changes in SLURM 2.3.0.pre5\n=============================\n -- NOTE: THERE HAS BEEN A NEW FIELD ADDED TO THE JOB STATE FILE. UPGRADES FROM\n    VERSION 2.3.0-PRE4 WILL RESULT IN LOST JOBS UNLESS THE \"orig_dependency\"\n    FIELD IS REMOVED FROM JOB STATE SAVE/RESTORE LOGIC. ON CRAY SYSTEMS A NEW\n    \"confirm_cookie\" FIELD WAS ADDED AND HAS THE SAME EFFECT OF DISABLING JOB\n    STATE RESTORE.\n -- BLUEGENE - Improve speed of start up when removing blocks at the beginning.\n -- Correct init.d/slurm status to have non-zero exit code if ANY Slurm\n    damon that should be running on the node is not running. Patch from Rod\n    Schulz, Bull.\n -- Improve accuracy of response to \"srun --test-only jobid=#\".\n -- Fix bug in front-end configurations which reports job_cnt_comp underflow\n    errors after slurmctld restarts.\n -- Eliminate \"error from _trigger_slurmctld_event in backup.c\" due to lack of\n    event triggers.\n -- Fix logic in BackupController to properly recover front-end node state and\n    avoid purging active jobs.\n -- Added man pages to html pages and the new cpu_management.html page.\n    Submitted by Martin Perry / Rod Schultz, Bull.\n -- Job dependency information will only show the currently active dependencies\n    rather than the original dependencies. From Dan Rusak, Bull.\n -- Add RPCs to get the SPANK environment variables from the slurmctld daemon.\n    Patch from Andrej N. Gritsenko.\n -- Updated plugins/task/cgroup/task_cgroup_cpuset.c to support newer\n    HWLOC_API_VERSION.\n -- Do not build select/bluegene plugin if C++ compiler is not installed.\n -- Add new configure option --with-srun2aprun to build an srun command\n    which is a wrapper over Cray's aprun command and supports many srun\n    options. Without this option, the srun command will advise the user\n    to use the aprun command.\n -- Change container ID supported by proctrack plugin from 32-bit to 64-bit.\n -- Added contribs/cray/libalps_test_programs.tar.gz with tools to validate\n    SLURM's logic used to support Cray systems.\n -- Create RPM for srun command that is a wrapper for the Cray/ALPS aprun\n    command. Dependent upon .rpmmacros parameter of \"%_with_srun2aprun\".\n -- Add configuration parameter MaxStepCount to limit effect of bad batch\n    scripts.\n -- Moving to github\n -- Fix for handling a 2.3 system talking to a 2.2 slurmctld.\n -- Add contribs/lua/job_submit.license.lua script. Update job_submit and Lua\n    related documentation.\n -- Test if _make_batch_script() is called with a NULL script.\n -- Increase hostlist support from 24k to 64k nodes.\n -- Renamed the Accounting Storage database's \"DerivedExitString\" job field to\n    \"Comment\".  Provided backward compatible support for \"DerivedExitString\" in\n    the sacctmgr tool.\n -- Added the ability to save the job's comment field to the Accounting\n    Storage db (to the formerly named, \"DerivedExitString\" job field).  This\n    behavior is enabled by a new slurm.conf parameter:\n    AccountingStoreJobComment.\n -- Test if _make_batch_script() is called with a NULL script.\n -- Increase hostlist support from 24k to 64k nodes.\n -- Fix srun to handle signals correctly when waiting for a step creation.\n -- Preserve the last job ID across slurmctld daemon restarts even if the job\n    state file can not be fully recovered.\n -- Made the hostlist functions be able to arbitrarily handle any size\n    dimension no matter what the size of the cluster is in dimensions.\n\n* Changes in SLURM 2.3.0.pre4\n=============================\n -- Add GraceTime to Partition and QOS data structures. Preempted jobs will be\n    given this time interval before termination. Work by Bill Brophy, Bull.\n -- Add the ability for scontrol and sview to modify slurmctld DebugFlags\n    values.\n -- Various Cray-specific patches:\n    - Fix a bug in distinguishing XT from XE.\n    - Avoids problems with empty nodenames on Cray.\n    - Check whether ALPS is hanging on to nodes, which happens if ALPS has not\n      yet cleaned up the node partition.\n    - Stops select/cray from clobbering node_ptr->reason.\n    - Perform 'safe' release of ALPS reservations using inventory and apkill.\n    - Compile-time sanity check for the apbasil and apkill files.\n    - Changes error handling in do_basil_release() (called by\n      select_g_job_fini()).\n    - Warn that salloc --no-shell option is not supported on Cray systems.\n -- Add a reservation flag of \"License_Only\". If set, then jobs using the\n    reservation may use the licenses associated with it plus any compute nodes.\n    Otherwise the job is limited to the compute nodes associated with the\n    reservation.\n -- Change slurm.conf node configuration parameter from \"Procs\" to \"CPUs\".\n    Both parameters will be supported for now.\n -- BLUEGENE - fix for when user requests only midplane names with no count at\n    job submission time to process the node count correctly.\n -- Fix job step resource allocation problem when both node and tasks counts\n    are specified. New logic selects nodes with larger CPU counts as needed.\n -- BGQ - make it so srun wraps runjob (still under construction, but works\n    for most cases)\n -- Permit a job's QOS and Comment field to both change in a single RPC. This\n    was previously disabled since Moab stored the QOS within the Comment field.\n -- Add support for jobs to expand in size. Submit additional batch job with\n    the option \"--dependency=expand:<jobid>\". See web page \"faq.html#job_size\"\n    for details. Restrictions to be removed in the future.\n -- Added --with-alps-emulation to configure, and also an optional cray.conf\n    to setup alps location and database information.\n -- Modify PMI data types from 16-bits to 32-bits in order to support MPICH2\n    jobs with more than 65,536 tasks. Patch from Hongjia Cao, NUDT.\n -- Set slurmd's soft process CPU limit equal to it's hard limit and notify the\n    user if the limit is not infinite.\n -- Added proctrack/cgroup and task/cgroup plugins from Matthieu Hautreux, CEA.\n -- Fix slurmctld restart logic that could leave nodes in UNKNOWN state for a\n    longer time than necessary after restart.\n\n* Changes in SLURM 2.3.0.pre3\n=============================\n -- BGQ - Appears to work correctly in emulation mode, no sub blocks just yet.\n -- Minor typos fixed\n -- Various bug fixes for Cray systems.\n -- Fix bug that when setting a compute node to idle state, it was failing to\n    set the systems up_node_bitmap.\n -- BLUEGENE - code reorder\n -- BLUEGENE - Now only one select plugin for all Bluegene systems.\n -- Modify srun to set the SLURM_JOB_NAME environment variable when srun is\n    used to create a new job allocation. Not set when srun is used to create a\n    job step within an existing job allocation.\n -- Modify init.d/slurm script to start multiple slurmd daemons per compute\n    node if so configured. Patch from Matthieu Hautreux, CEA.\n -- Change license data structure counters from uint16_t to uint32_t to support\n    larger license counts.\n\n* Changes in SLURM 2.3.0.pre2\n=============================\n -- Log a job's requeue or cancellation due to preemption to that job's stderr:\n    \"*** JOB 65547 CANCELLED AT 2011-01-21T12:59:33 DUE TO PREEMPTION ***\".\n -- Added new job termination state of JOB_PREEMPTED, \"PR\" or \"PREEMPTED\" to\n    indicate job termination was due to preemption.\n -- Optimize advanced reservations resource selection for computer topology.\n    The logic has been added to select/linear and select/cons_res, but will\n    not be enabled until the other select plugins are modified.\n -- Remove checkpoint/xlch plugin.\n -- Disable deletion of partitions that have unfinished jobs (pending,\n    running or suspended states). Patch from Martin Perry, BULL.\n -- In sview, disable the sorting of node records by name at startup for\n    clusters over 1000 nodes. Users can enable this by selecting the \"Name\"\n    tab. This change dramatically improves scalability of sview.\n -- Report error when trying to change a node's state from scontrol for Cray\n    systems.\n -- Do not attempt to read the batch script for non-batch jobs. This patch\n    eliminates some inappropriate error messages.\n -- Preserve NodeHostName when reordering nodes due to system topology.\n -- On Cray/ALPS systems  do node inventory before scheduling jobs.\n -- Disable some salloc options on Cray systems.\n -- Disable scontrol's wait_job command on Cray systems.\n -- Disable srun command on native Cray/ALPS systems.\n -- Updated configure option \"--enable-cray-emulation\" (still under\n    development) to emulate a cray XT/XE system, and auto-detect a real\n    Cray XT/XE systems (removed no longer needed --enable-cray configure\n    option).  Building on native Cray systems requires the\n    cray-MySQL-devel-enterprise rpm and expat XML parser library/headers.\n\n* Changes in SLURM 2.3.0.pre1\n=============================\n -- Added that when a slurmctld closes the connection to the database it's\n    registered host and port are removed.\n -- Added flag to slurmdbd.conf TrackSlurmctldDown where if set will mark idle\n    resources as down on a cluster when a slurmctld disconnects or is no\n    longer reachable.\n -- Added support for more than one front-end node to run slurmd on\n    architectures where the slurmd does not execute on the compute nodes\n    (e.g. BlueGene). New configuration parameters FrontendNode and FrontendAddr\n    added. See \"man slurm.conf\" for more information.\n -- With the scontrol show job command when using the --details option, show\n    a batch job's script.\n -- Add ability to create reservations or partitions and submit batch jobs\n    using sview. Also add the ability to delete reservations and partitions.\n -- Added new configuration parameter MaxJobId. Once reached, restart job ID\n    values at FirstJobId.\n -- When restarting slurmctld with priority/basic, increment all job priorities\n    so the highest job priority becomes TOP_PRIORITY.\n\n* Changes in SLURM 2.2.8\n========================\n -- Prevent background salloc disconnecting terminal at termination. Patch by\n    Don Albert, Bull.\n -- Fixed issue where preempt mode is skipped when creating a QOS. Patch by\n    Bill Brophy, Bull.\n -- Fixed documention (html) for PriorityUsageResetPeriod to match that in the\n    man pages. Patch by Nancy Kritkausky, Bull.\n\n* Changes in SLURM 2.2.7\n========================\n -- Eliminate zombie process created if salloc exits with stopped child\n    process. Patch from Gerrit Renker, CSCS.\n -- With default configuration on non-Cray systems, enable salloc to be\n    spawned as a background process. Based upon work by Don Albert (Bull) and\n    Gerrit Renker (CSCS).\n -- Fixed Regression from 2.2.4 in accounting where an inherited limit\n    would not be set correctly in the added child association.\n -- Fixed issue with accounting when asking for jobs with a hostlist.\n -- Avoid clearing a node's Arch, OS, BootTime and SlurmdStartTime when\n    \"scontrol reconfig\" is run. Patch from Martin Perry, Bull.\n\n* Changes in SLURM 2.2.6\n========================\n -- Fix displaying of account coordinators with sacctmgr.  Possiblity to show\n    deleted accounts.  Only a cosmetic issue, since the accounts are already\n    deleted, and have no associations.\n -- Prevent opaque ncurses WINDOW struct on OS X 10.6.\n -- Fix issue with accounting when using PrivateData=jobs... users would not be\n    able to view there own jobs unless they were admin or coordinators which is\n    obviously wrong.\n -- Fix bug in node stat if slurmctld is restarted while nodes are in the\n    process of being powered up. Patch from Andriy Grytsenko.\n -- Change maximum batch script size from 128k to 4M.\n -- Get slurmd -f option working. Patch from Andriy Grytsenko.\n -- Fix for linking problem on OSX. Patches from Jon Bringhurst (LANL) and\n    Tyler Strickland.\n -- Reset a job's priority to zero (suspended) when Moab requeues the job.\n    Patch from Par Andersson, NSC.\n -- When enforcing accounting, fix polling for unknown uids for users after\n    the slurmctld started.  Previously one would have to issue a reconfigure\n    to the slurmctld to have it look for new uids.\n -- BLUEGENE - if a block goes into an error state.  Fix issue where accounting\n    wasn't updated correctly when the block was resumed.\n -- Synchronize power-save module better with scheduler. Patch from\n    Andriy Grytsenko (Massive Solutions Limited).\n -- Avoid SEGV in association logic with user=NULL. Patch from\n    Andriy Grytsenko (Massive Solutions Limited).\n -- Fixed issue in accounting where it was possible for a new\n    association/wckey to be set incorrectly as a default the new object\n    was added after an original default object already existed.  Before\n    the slurmctld would need to be restarted to fix the issue.\n -- Updated the Normalized Usage section in priority_multifactor.shtml.\n -- Disable use of SQUEUE_FORMAT env var if squeue -l, -o, or -s option is\n    used. Patch from Aaron Knister (UMBC).\n\n* Changes in SLURM 2.2.5\n========================\n -- Correct init.d/slurm status to have non-zero exit code if ANY Slurm\n    damon that should be running on the node is not running. Patch from Rod\n    Schulz, Bull.\n -- Improve accuracy of response to \"srun --test-only jobid=#\".\n -- Correct logic to properly support --ntasks-per-node option in the\n    select/cons_res plugin. Patch from Rod Schulz, Bull.\n -- Fix bug in select/cons_res with respect to generic resource (gres)\n    scheduling which prevented some jobs from starting as soon as possible.\n -- Fix memory leak in select/cons_res when backfill scheduling generic\n    resources (gres).\n -- Fix for when configuring a node with more resources than in real life\n    and using task/affinity.\n -- Fix so slurmctld will pack correctly 2.1 step information. (Only needed if\n    a 2.1 client is talking to a 2.2 slurmctld.)\n -- Set powered down node's state to IDLE+POWER after slurmctld restart instead\n    of leaving in UNKNOWN+POWER. Patch from Andrej Gritsenko.\n -- Fix bug where is srun's executable is not on it's current search path, but\n    can be found in the user's default search path. Modify slurmstepd to find\n    the executable. Patch from Andrej Gritsenko.\n -- Make sview display correct cpu count for steps.\n -- BLUEGENE - when running in overlap mode make sure to check the connection\n    type so you can create overlapping blocks on the exact same nodes with\n    different connection types (i.e. one torus, one mesh).\n -- Fix memory leak if MPI ports are reserved (for OpenMPI) and srun's\n    --resv-ports option is used.\n -- Fix some anomalies in select/cons_res task layout when using the\n    --cpus-per-task option. Patch from Martin Perry, Bull.\n -- Improve backfill scheduling logic when job specifies --ntasks-per-node and\n    --mem-per-cpu options on a heterogeneous cluster. Patch from Bjorn-Helge\n    Mevik, University of Oslo.\n -- Print warning message if srun specifies --cpus-per-task larger than used\n    to create job allocation.\n -- Fix issue when changing a users name in accounting, if using wckeys would\n    execute correctly, but bad memcopy would core the DBD.  No information\n    would be lost or corrupted, but you would need to restart the DBD.\n\n* Changes in SLURM 2.2.4\n========================\n -- For batch jobs for which the Prolog fails, substitute the job ID for any\n    \"%j\" in the job's output or error file specification.\n -- Add licenses field to the sview reservation information.\n -- BLUEGENE - Fix for handling extremely overloaded system on Dynamic system\n    dealing with starting jobs on overlapping blocks.  Previous fallout\n    was job would be requeued.  (happens very rarely)\n -- In accounting_storage/filetxt plugin, substitute spaces within job names,\n    step names, and account names with an underscore to insure proper parsing.\n -- When building contribs/perlapi ignore both INSTALL_BASE and PERL_MM_OPT.\n    Use PREFIX instead to avoid build errors from multiple installation\n    specifications.\n -- Add job_submit/cnode plugin to support resource reservations of less than\n    a full midplane on BlueGene computers. Treat cnodes as liceses which can\n    be reserved and are consumed by jobs. This reservation mechanism for less\n    than an entire midplane is still under development.\n -- Clear a job's \"reason\" field when a held job is released.\n -- When releasing a held job, calculate a new priority for it rather than\n    just setting the priority to 1.\n -- Fix for sview started on a non-bluegene system to pick colors correctly\n    when talking to a real bluegene system.\n -- Improve sched/backfill's expected start time calculation.\n -- Prevent abort of sacctmgr for dump command with invalid (or no) filename.\n -- Improve handling of job updates when using limits in accounting, and\n    updating jobs as a non-admin user.\n -- Fix for \"squeue --states=all\" option. Bug would show no jobs.\n -- Schedule jobs with reservations before those without reservations.\n -- Fix squeue/scancel to query correctly against accounts of different case.\n -- Abort an srun command when it's associated job gets aborted due to a\n    dependency that can not be satisfied.\n -- In jobcomp plugins, report start time of zeroif pending job is cancelled.\n    Previously may report expected start time.\n -- Fixed sacctmgr man to state correct variables.\n -- Select nodes based upon their Weight when job allocation requests include\n    a constraint field with a count (e.g. \"srun --constraint=gpu*2 -N4 a.out\").\n -- Add support for user names that are entirely numeric and do not treat them\n    as UID values. Patch from Dennis Leepow.\n -- Patch to un/pack double values properly if negative value.  Patch from\n    Dennis Leepow\n -- Do not reset a job's priority when requeued or suspended.\n -- Fix problemm that could let new jobs start on a node in DRAINED state.\n -- Fix cosmetic sacctmgr issue where if the user you are trying to add\n    doesn't exist in the /etc/passwd file and the account you are trying\n    to add them to doesn't exist it would print (null) instead of the bad\n    account name.\n -- Fix associations/qos for when adding back a previously deleted object\n    the object will be cleared of all old limits.\n -- BLUEGENE - Added back a lock when creating dynamic blocks to be more thread\n    safe on larger systems with heavy load.\n\n* Changes in SLURM 2.2.3\n========================\n -- Update srun, salloc, and sbatch man page description of --distribution\n    option. Patches from Rod Schulz, Bull.\n -- Applied patch from Martin Perry to fix \"Incorrect results for task/affinity\n    block second distribution and cpus-per-task > 1\" bug.\n -- Avoid setting a job's eligible time while held (priority == 0).\n -- Substantial performance improvement to backfill scheduling. Patch from\n    Bjorn-Helge Mevik, University of Oslo.\n -- Make timeout for communications to the slurmctld be based upon the\n    MessageTimeout configuration parameter rather than always 3 seconds.\n    Patch from Matthieu Hautreux, CEA.\n -- Add new scontrol option of \"show aliases\" to report every NodeName that is\n    associated with a given NodeHostName when running multiple slurmd daemons\n    per compute node (typically used for testing purposes). Patch from\n    Matthieu Hautreux, CEA.\n -- Fix for handling job names with a \"'\" in the name within MySQL accounting.\n    Patch from Gerrit Renker, CSCS.\n -- Modify condition under which salloc execution delayed until moved to the\n    foreground. Patch from Gerrit Renker, CSCS.\n\tJob control for interactive salloc sessions: only if ...\n\ta) input is from a terminal (stdin has valid termios attributes),\n\tb) controlling terminal exists (non-negative tpgid),\n\tc) salloc is not run in allocation-only (--no-shell) mode,\n\td) salloc runs in its own process group (true in interactive\n\t   shells that support job control),\n\te) salloc has been configured at compile-time to support background\n\t   execution and is not currently in the background process group.\n -- Abort salloc if no controlling terminal and --no-shell option is not used\n    (\"setsid salloc ...\" is disabled). Patch from Gerrit Renker, CSCS.\n -- Fix to gang scheduling logic which could cause jobs to not be suspended\n    or resumed when appropriate.\n -- Applied patch from Martin Perry to fix \"Slurmd abort when using task\n    affinity with plane distribution\" bug.\n -- Applied patch from Yiannis Georgiou to fix \"Problem with cpu binding to\n    sockets option\" behaviour. This change causes \"--cpu_bind=sockets\" to bind\n    tasks only to the CPUs on each socket allocated to the job rather than all\n    CPUs on each socket.\n -- Advance daily or weekly reservations immediately after termination to avoid\n    having a job start that runs into the reservation when later advanced.\n -- Fix for enabling users to change there own default account, wckey, or QOS.\n -- BLUEGENE - If using OVERLAP mode fixed issue with multiple overlapping\n    blocks in error mode.\n -- Fix for sacctmgr to display correctly default accounts.\n -- scancel -s SIGKILL will always sent the RPC to the slurmctld rather than\n    the slurmd daemon(s). This insures that tasks in the process of getting\n    spawned are killed.\n -- BLUEGENE - If using OVERLAP mode fixed issue with jobs getting denied\n    at submit if the only option for their job was overlapping a block in\n    error state.\n\n* Changes in SLURM 2.2.2\n========================\n -- Correct logic to set correct job hold state (admin or user) when setting\n    the job's priority using scontrol's \"update jobid=...\" rather than its\n    \"hold\" or \"holdu\" commands.\n -- Modify squeue to report unset --mincores, --minthreads or --extra-node-info\n    values as \"*\" rather than 65534. Patch from Rod Schulz, BULL.\n -- Report the StartTime of a job as \"Unknown\" rather than the year 2106 if its\n    expected start time was too far in the future for the backfill scheduler\n    to compute.\n -- Prevent a pending job reason field from inappropriately being set to\n    \"Priority\".\n -- In sched/backfill with jobs having QOS_FLAG_NO_RESERVE set, then don't\n    consider the job's time limit when attempting to backfill schedule. The job\n    will just be preempted as needed at any time.\n -- Eliminated a bug in sbatch when no valid target clusters are specified.\n -- When explicitly sending a signal to a job with the scancel command and that\n    job is in a pending state, then send the request directly to the slurmctld\n    daemon and do not attempt to send the request to slurmd daemons, which are\n    not running the job anyway.\n -- In slurmctld, properly set the up_node_bitmap when setting it's state to\n    IDLE (in case the previous node state was DOWN).\n -- Fix smap to process block midplane names correctly when on a bluegene\n    system.\n -- Fix smap to once again print out the Letter 'ID' for each line of a block/\n    partition view.\n -- Corrected the NOTES section of the scancel man page\n -- Fix for accounting_storage/mysql plugin to correctly query cluster based\n    transactions.\n -- Fix issue when updating database for clusters that were previously deleted\n    before upgrade to 2.2 database.\n -- BLUEGENE - Handle mesh torus check better in dynamic mode.\n -- BLUEGENE - Fixed race condition when freeing block, most likely only would\n    happen in emulation.\n -- Fix for calculating used QOS limits correctly on a slurmctld reconfig.\n -- BLUEGENE - Fix for bad conn-type set when running small blocks in HTC mode.\n -- If salloc's --no-shell option is used, then do not attempt to preserve the\n    terminal's state.\n -- Add new SLURM configure time parameter of --disable-salloc-background. If\n    set, then salloc can only execute in the foreground. If started in the\n    background, then a message will be printed and the job allocation halted\n    until brought into the foreground.\n    NOTE: THIS IS A CHANGE IN DEFAULT SALLOC BEHAVIOR FROM V2.2.1, BUT IS\n    CONSISTENT WITH V2.1 AND EARLIER.\n -- Added the Multi-Cluster Operation web page.\n -- Removed remnant code for enforcing max sockets/cores/threads in the\n    cons_res plugin (see last item in 2.1.0-pre5).  This was responsible\n    for a bug reported by Rod Schultz.\n -- BLUEGENE - Set correct env vars for HTC mode on a P system to get correct\n    block.\n -- Correct RunTime reported by \"scontrol show job\" for pending jobs.\n\n* Changes in SLURM 2.2.1\n========================\n -- Fix setting derived exit code correctly for jobs that happen to have the\n    same jobid.\n -- Better checking for time overflow when rolling up in accounting.\n -- Add scancel --reservation option to cancel all jobs associated with a\n    specific reservation.\n -- Treat reservation with no nodes like one that starts later (let jobs of any\n    size get queued and do not block any pending jobs).\n -- Fix bug in gang scheduling logic that would temporarily resume to many jobs\n    after a job completed.\n -- Change srun message about job step being deferred due to SlurmctldProlog\n    running to be more clear and only print when --verbose option is used.\n -- Made it so you could remove the hold on jobs with sview by setting the\n    priority to infinite.\n -- BLUEGENE - better checking small blocks in dynamic mode whether a full\n    midplane job could run or not.\n -- Decrease the maximum sleep time between srun job step creation retry\n    attempts from 60 seconds to 29 seconds. This should eliminate a possible\n    synchronization problem with gang scheduling that could result in job\n    step creation requests only occuring when a job is suspended.\n -- Fix to prevent changing a held job's state from HELD to DEPENDENCY\n    until the job is released. Patch from Rod Schultz, Bull.\n -- Fixed sprio -M to reflect PriorityWeight values from remote cluster.\n -- Fix bug in sview when trying to update arbitrary field on more than one\n    job. Formerly would display information about one job, but update next\n    selected job.\n -- Made it so QOS with UsageFactor set to 0 would make it so jobs running\n    under that QOS wouldn't add time to fairshare or association/qos\n    limits.\n -- Fixed issue where QOS priority wasn't re-normalized until a slurmctld\n    restart when a QOS priority was changed.\n -- Fix sprio to use calculated numbers from slurmctld instead of\n    calulating it own numbers.\n -- BLUEGENE - fixed race condition with preemption where if the wind blows the\n    right way the slurmctld could lock up when preempting jobs to run others.\n -- BLUEGENE - fixed epilog to wait until MMCS job is totally complete before\n    finishing.\n -- BLUEGENE - more robust checking for states when freeing blocks.\n -- Added correct files to the slurm.spec file for correct perl api rpm\n    creation.\n -- Added flag \"NoReserve\" to a QOS to make it so all jobs are created equal\n    within a QOS.  So if larger, higher priority jobs are unable to run they\n    don't prevent smaller jobs from running even if running the smaller\n    jobs delay the start of the larger, higher priority jobs.\n -- BLUEGENE - Check preemptees one by one to preempt lower priority jobs first\n    instead of first fit.\n -- In select/cons_res, correct handling of the option\n    SelectTypeParameters=CR_ONE_TASK_PER_CORE.\n -- Fix for checking QOS to override partition limits, previously if not using\n    QOS some limits would be overlooked.\n -- Fix bug which would terminate a job step if any of the nodes allocated to\n    it were removed from the job's allocation. Now only the tasks on those\n    nodes are terminated.\n -- Fixed issue when using a storage_accounting plugin directly without the\n    slurmDBD updates weren't always sent correctly to the slurmctld, appears to\n    OS dependent, reported by Fredrik Tegenfeldt.\n\n* Changes in SLURM 2.2.0\n========================\n -- Change format of Duration field in \"scontrol show reservation\" output from\n    an integer number of minutes to \"[days-]hours:minutes:seconds\".\n -- Add support for changing the reservation of pending or running jobs.\n -- On Cray systems only, salloc sends SIGKILL to spawned process group when\n    job allocation is revoked. Patch from Gerrit Renker, CSCS.\n -- Fix for sacctmgr to work correctly when modifying user associations where\n    all the associations contain a partition.\n -- Minor mods to salloc signal handling logic: forwards more signals and\n    releases allocation on real-time signals. Patch from Gerrit Renker, CSCS.\n -- Add salloc logic to preserve tty attributes after abnormal exit. Patch\n    from Mark Grondona, LLNL.\n -- BLUEGENE - Fix for issue in dynamic mode when trying to create a block\n    overlapping a block with no job running on it but in configuring state.\n -- BLUEGENE - Speedup by skipping blocks that are deallocating for other jobs\n    when starting overlapping jobs in dynamic mode.\n -- Fix for sacct --state to work correctly when not specifying a start time.\n -- Fix upgrade process in accounting from 2.1 for clusters named \"cluster\".\n -- Export more jobacct_common symbols needed for the slurm api on some systems.\n\n* Changes in SLURM 2.2.0.rc4\n============================\n -- Correction in logic to spread out over time highly parallel messages to\n    minimize lost messages. Effects slurmd epilog complete messages and PMI\n    key-pair transmissions. Patch from Gerrit Renker, CSCS.\n -- Fixed issue where if a system has unset messages to the dbd in 2.1 and\n    upgrades to 2.2.  Messages are now processed correctly now.\n -- Fixed issue where assoc_mgr cache wasn't always loaded correctly if the\n    slurmdbd wasn't running when the slurmctld was started.\n -- Make sure on a pthread create in step launch that the error code is looked\n    at. Improves fault-tolerance of slurmd.\n -- Fix setting up default acct/wckey when upgrading from 2.1 to 2.2.\n -- Fix issue with associations attached to a specific partition with no other\n    association, and requesting a different partition.\n -- Added perlapi to the slurmdb to the slurm.spec.\n -- In sched/backfill, correct handling of CompleteWait parameter to avoid\n    backfill scheduling while a job is completing. Patch from Gerrit Renker,\n    CSCS.\n -- Send message back to user when trying to launch job on computing lacking\n    that user ID. Patch from Hongjia Cao, NUDT.\n -- BLUEGENE - Fix it so 1 midplane clusters will run small block jobs.\n -- Add Command and WorkDir to the output of \"scontrol show job\" for job\n    allocations created using srun (not just sbatch).\n -- Fixed sacctmgr to not add blank defaultqos' when doing a cluster dump.\n -- Correct processing of memory and disk space specifications in the salloc,\n    sbatch, and srun commands to work properly with a suffix of \"MB\", \"GB\",\n    etc. and not only with a single letter (e.g. \"M\", \"G\", etc.).\n -- Prevent nodes with suspended jobs from being powered down by SLURM.\n -- Normalized the way pidfile are created by the slurm daemons.\n -- Fixed modifying the root association to no read in it's last value\n    when clearing a limit being set.\n -- Revert some resent signal handling logic from salloc so that SIGHUP sent\n    after the job allocation will properly release the allocation and cause\n    salloc to exit.\n -- BLUEGENE - Fix for recreating a block in a ready state.\n -- Fix debug flags for incorrect logic when dealing with DEBUG_FLAG_WIKI.\n -- Report reservation's Nodes as a hostlist expression of all nodes rather\n    than using \"ALL\".\n -- Fix reporting of nodes in BlueGene reservation (was reporting CPU count\n    rather than cnode count in scontrol output for NodeCnt field).\n\n* Changes in SLURM 2.2.0.rc3\n============================\n -- Modify sacctmgr command to accept plural versions of options (e.g. \"Users\"\n    in addition to \"User\"). Patch from Don Albert, BULL.\n -- BLUEGENE - make it so reset of boot counter happens only on state change\n    and not when a new job comes along.\n -- Modify srun and salloc signal handling so they can be interrupted while\n    waiting for an allocation. This was broken in version 2.2.0.rc2.\n -- Fix NULL pointer reference in sview. Patch from Gerrit Renker, CSCS.\n -- Fix file descriptor leak in slurmstepd on spank_task_post_fork() failure.\n    Patch from Gerrit Renker, CSCS.\n -- Fix bug in preserving job state information when upgrading from SLURM\n    version 2.1. Bug introduced in version 2.2.0-pre10. Patch from Par\n    Andersson, NSC.\n -- Fix bug where if using the slurmdbd if a job wasn't able to start right\n    away some accounting information may be lost.\n -- BLUEGENE - when a prolog failure happens the offending block is put in\n    an error state.\n -- Changed the last column heading of the sshare output from \"FS Usage\" to\n    \"FairShare\" and added more detail to the sshare man page.\n -- Fix bug in enforcement of reservation by account name. Used wrong index\n    into an array. Patch from Gerrit Renker, CSCS.\n -- Modify job_submit/lua plugin to treat any non-zero return code from the\n    job_submit and job_modify functions as an error and the user request should\n    be aborted.\n -- Fix bug which would permit pending job to be started on completing node\n    when job preemption is configured.\n\n* Changes in SLURM 2.2.0.rc2\n============================\n -- Fix memory leak in job step allocation logic. Patch from Hongjia Cao, NUDT.\n -- If a preempted job was submitted with the --no-requeue option then cancel\n    rather than requeue it.\n -- Fix for problems when adding a user for the first time to a new cluster\n    with a 2.1 sacctmgr without specifying a default account.\n -- Resend TERMINATE_JOB message only to nodes that the job still has not\n    terminated on. Patch from Hongjia Cao, NUDT.\n -- Treat time limit specification of \"0:300\" as a request for 300 seconds\n    (5 minutes) instead of one minute.\n -- Modify sched/backfill plugin logic to continue working its way down the\n    queue of jobs rather than restarting at the top if there are no changes in\n    job, node, or partition state between runs. Patch from Hongjia Cao, NUDT.\n -- Improve scalability of select/cons_res logic. Patch from Matthieu Hautreux,\n    CEA.\n -- Fix for possible deadlock in the slurmstepd when cancelling a job that is\n    also writing a large amount of data to stderr.\n -- Fix in select/cons_res to eliminate \"mem underflow\" error when the\n    slurmctld is reconfigured while a job is in completing state.\n -- Send a message to the a user's job when it's real or virual memory limit\n    is exceeded. :\n -- Apply rlimits right before execing the users task so to lower the risk of\n    the task exiting because the slurmstepd ran over a limit (log file size,\n    etc.)\n -- Add scontrol command of \"uhold <job_id>\" so that an administrator can hold\n    a job and let the job's owner release it. The scontrol command of\n    \"hold <job_id>\" when executed by a SLURM administrator can only be released\n    by a SLURM administrator and not the job owner.\n -- Change atoi to slurm_atoul in mysql plugin, needed for running on 32-bit\n    systems in some cases.\n -- If a batch job is found to be missing from a node, make its termination\n    state be NODE_FAIL rather than CANCELLED.\n -- Fatal error put back if running a bluegene or cray plugin from a controller\n    not of that type.\n -- Make sure jobacct_gather plugin is not shutdown before messing with the\n    proccess list.\n -- Modify signal handling in srun and salloc commands to avoid deadlock if the\n    malloc function is interupted and called again. The malloc function is\n    thread safe, but not reentrant, which is a problem when signal handling if\n    the malloc function itself has a lock. Problem fixed by moving signal\n    handling in those commands to a new pthread.\n -- In srun set job abort flag on completion to handle the case when a user\n    cancels a job while the node is not responding but slurmctld has not yet\n    the node down. Patch from Hongjia Cao, NUDT.\n -- Streamline the PMI logic if no duplicate keys are included in the key-pairs\n    managed. Substantially improves performance for large numbers of tasks.\n    Adds support for SLURM_PMI_KVS_NO_DUP_KEYS environment variable. Patch\n    from Hongjia Cao, NUDT.\n -- Fix issues with sview dealing with older versions of sview and saving\n    defaults.\n -- Remove references to --mincores, --minsockets, and --minthreads from the\n    salloc, sbatch and srun man pages. These options are defunct, Patch from\n    Rod Schultz, Bull.\n -- Made openssl not be required to build RPMs, it is not required anymore\n    since munge is the default crypto plugin.\n -- sacctmgr now has smarts to figure out if a qos is a default qos when\n    modifing a user/acct or removing a qos.\n -- For reservations on BlueGene systems, set and report c-node counts rather\n    than midplane counts.\n\n* Changes in SLURM 2.2.0.rc1\n============================\n -- Add show_flags parameter to the slurm_load_block_info() function.\n -- perlapi has been brought up to speed courtesy of Hongjia Coa. (make sure to\n    run 'make clean' if building in a different dir than source)\n -- Fixed regression in pre12 in crypto/munge when running with\n    --enable-multiple-slurmd which would cause the slurmd's to core.\n -- Fixed regression where cpu count wasn't figured out correctly for steps.\n -- Fixed issue when using old mysql that can't handle a '.' in the table\n    name.\n -- Mysql plugin works correctly without the SlurmDBD\n -- Added ability to query batch step with sstat.  Currently no accounting data\n    is stored for the batch step, but the internals are inplace if we decide to\n    do that in the future.\n -- Fixed some backwards compatibility issues with 2.2 talking to 2.1.\n -- Fixed regression where modifying associations didn't get sent to the\n    slurmctld.\n -- Made sshare sort things the same way saccmgr list assoc does\n    (alphabetically)\n -- Fixed issue with default accounts being set up correctly.\n -- Changed sortting in the slurmctld so sshare output is similar to that of\n    sacctmgr list assoc.\n -- Modify reservation logic so that daily and weekly reservations maintain\n    the same time when daylight savings time starts or ends in the interim.\n -- Edit to make reservations handle updates to associations.\n -- Added the derived exit code to the slurmctld job record and the derived\n    exit code and string to the job record in the SLURM db.\n -- Added slurm-sjobexit RPM for SLURM job exit code management tools.\n -- Added ability to use sstat/sacct against the batch step.\n -- Added OnlyDefaults option to sacctmgr list associations.\n -- Modified the fairshare priority formula to F = 2**(-Ue/S)\n -- Modify the PMI functions key-pair exchange function to support a 32-bit\n    counter for larger job sizes. Patch from Hongjia Cao, NUDT.\n -- In sched/builtin - Make the estimated job start time logic faster (borrowed\n    new logic from sched/backfill and added pthread) and more accurate.\n -- In select/cons_res fix bug that could result in a job being allocated zero\n    CPUs on some nodes. Patch from Hongjia Cao, NUDT.\n -- Fix bug in sched/backfill that could set expected start time of a job too\n    far in the future.\n -- Added ability to enforce new limits given to associations/qos on\n    pending jobs.\n -- Increase max message size for the slurmdbd from 1000000 to 16*1024*1024\n -- Increase number of active threads in the slurmdbd from 50 to 100\n -- Fixed small bug in src/common/slurmdb_defs.c reported by Bjorn-Helge Mevik\n -- Fixed sacctmgr's ability to query associations against qos again.\n -- Fixed sview show config on non-bluegene systems.\n -- Fixed bug in selecting jobs based on sacct -N option\n -- Fix bug that prevented job Epilog from running more than once on a node if\n    a job was requeued and started no job steps.\n -- Fixed issue where node index wasn't stored correcting when using DBD.\n -- Enable srun's use of the --nodes option with --exclusive (previously the\n    --nodes option was ignored).\n -- Added UsageThreshold and Flags to the QOS object.\n -- Patch to improve threadsafeness in the mysql plugins.\n -- Add support for fair-share scheduling to be based upon resource use at\n    the level of bank accounts and ignore use of individual users. Patch by\n    Par Andersson, National Supercomputer Centre, Sweden.\n\n* Changes in SLURM 2.2.0.pre12\n==============================\n -- Log if Prolog or Epilog run for longer than MessageTimeout / 2.\n -- Log the RPC number associated with messages from slurmctld that timeout.\n -- Fix bug in select/cons_res logic when job allocation includes --overcommit\n    and --ntasks-per-node options and the node has fewer CPUs than the count\n    specified by --ntasks-per-node.\n -- Fix bug in gang scheduling and job preemption logic so that preempted jobs\n    get resumed properly after a slurmctld hot-start.\n -- Fix bug in select/linear handling of gang scheduled jobs that could result\n    in run_job_cnt underflow error message.\n -- Fix bug in gang scheduling logic to properly support partitions added\n    using the scontrol command.\n -- Fix a segmentation fault in sview where the 'excluded_partitions' field\n    was set to NULL, caused by the absence of ~/.slurm/sviewrc.\n -- Rewrote some calls to is_user_any_coord() in src/plugins/accounting_storage\n    modules to make use of is_user_any_coord()'s return value.\n -- Add configure option of --with=dimensions=#.\n -- Modify srun ping logic so that srun would only be considered not responsive\n    if three ping messages were not responded to. Patch from Hongjia Cao (NUDT).\n -- Preserve a node's ReasonTime field after scontrol reconfig command. Patch\n    from Hongjia Cao (NUDT).\n -- Added the authority for users with AdminLevel's defined in the SLURM db\n    (Operators and Admins) and account coordinators to invoke commands that\n    affect jobs, reservations, nodes, etc.\n -- Fix for slurmd restart on completing node with no tasks to get the correct\n    state, completing. Patch from Hongjia Cao (NUDT).\n -- Prevent scontrol setting a node's Reason=\"\". Patch from Hongjia Cao (NUDT).\n -- Add new functions hostlist_ranged_string_malloc,\n    hostlist_ranged_string_xmalloc, hostlist_deranged_string_malloc, and\n    hostlist_deranged_string_xmalloc which will allocate memory as needed.\n -- Make the slurm commands support both the --cluster and --clusters option.\n    Previously, some commands support one of those options, but not the other.\n -- Fix bug when resizing a job that has steps running on some of those nodes.\n    Avoid killing the job step on remaining nodes. Patch from Rod Schultz\n    (BULL). Also fix bug related to tracking the CPUs allocated to job steps\n    on each node after releasing some nodes from the job's allocation.\n -- Applied patch from Rod Schultz / Matthieu Hautreux to keep the Node-to-Host\n    cache from becoming corrupted when a hostname cannot be resolved.\n -- Export more symbols in libslurm for job and node state information\n    translation (numbers to strings). Patch from Hongia Cao, NUDT.\n -- Add logic to retry sending RESPONSE_LAUNCH_TASKS messages from slurmd to\n    srun. Patch from Hongia Cao, NUDT.\n -- Modify bit_unfmt_hexmask() and bit_unfmt_binmask() functions to clear the\n    bitmap input before setting the bits indicated in the input string.\n -- Add SchedulerParameters option of bf_window to control how far into the\n    future that the backfill scheduler will look when considering jobs to start.\n    The default value is one day. See \"man slurm.conf\" for details.\n -- Fix bug that can result in duplicate job termination records in accounting\n    for job termination when slurmctld restarts or reconfigures.\n -- Modify plugin and library logic as needed to support use of the function\n    slurm_job_step_stat() from user commands.\n -- Fix race condition in which PrologSlurmctld failure could cause slurmctld\n    to abort.\n -- Fix bug preventing users in secondary user groups from being granted access\n    to partitions configured with AllowGroups.\n -- Added support for a default account and wckey per cluster within accounting.\n -- Modified select/cons_res plugin so that if MaxMemPerCPU is configured and a\n    job specifies it's memory requirement, then more CPUs than requested will\n    automatically be allocated to a job to honor the MaxMemPerCPU parameter.\n -- Added the derived_ec (exit_code) member to job_info_t.  exit_code captures\n    the exit code of the job script (or salloc) while derived_ec contains the\n    highest exit code of all the job steps.\n -- Added SLURM_JOB_EXIT_CODE and SLURM_JOB_DERIVED_EC variables to the\n    EpilogSlurmctld environment\n -- More work done on the accounting_storage/pgsql plugin, still beta.\n    Patch from Hongjia Cao (NUDT).\n -- Major updates to sview from Dan Rusak (Bull), including:\n    - Persistent option selections for each tab page\n    - Clean up topology in grids\n    - Leverage AllowGroups and Hidden options\n    - Cascade full-info popups for ease of selection\n -- Add locks around the MySQL calls for proper operation if the non-thread\n    safe version of the MySQL library is used.\n -- Remove libslurm.a, libpmi.a and libslurmdb.a from SLURM RPM. These static\n    libraries are not generally usable.\n -- Fixed bug in sacctmgr when zeroing raw usage reported by Gerrit Renker.\n\n* Changes in SLURM 2.2.0.pre11\n==============================\n -- Permit a regular user to change the partition of a pending job.\n -- Major re-write of the job_submit/lua plugin to pass pointers to available\n    partitions and use lua metatables to reference the job and partition fields.\n -- Add support for serveral new trigger types: SlurmDBD failure/restart,\n    Database failure/restart, Slurmctld failure/restart.\n -- Add support for SLURM_CLUSTERS environment variable in the sbatch, sinfo,\n    squeue commands.\n -- Modify the sinfo and squeue commands to report state of multiple clusters\n    if the --clusters option is used.\n -- Added printf __attribute__ qualifiers to info, debug, ... to help prevent\n    bad/incorrect parameters being sent to them.  Original patch from\n    Eygene Ryabinkin (Russian Research Centre).\n -- Fix bug in slurmctld job completion logic when nodes allocated to a\n    completing job are re-booted. Patch from Hongjia Cao (NUDT).\n -- In slurmctld's node record data structure, rename \"hilbert_integer\" to\n    \"node_rank\".\n -- Add topology/node_rank plugin to sort nodes based upon rank loaded from\n    BASIL on Cray computers.\n -- Fix memory leak in the auth/munge and crypto/munge plugins in the case of\n    some failure modes.\n\n* Changes in SLURM 2.2.0.pre10\n==============================\n -- Fix issue when EnforcePartLimits=yes in slurm.conf all jobs where no nodecnt\n    was specified the job would be seen to have maxnodes=0 which would not\n    allow jobs to run.\n -- Fix issue where if not suspending a job the gang scheduler does the correct\n    kill procedure.\n -- Fixed some issues when dealing with jobs from a 2.1 system so they live\n    after an upgrade.\n -- In srun, log if --cpu_bind options are specified, but not supported by the\n    current system configuration.\n -- Various Patchs from Hongjia Cao dealing with bugs found in sacctmgr and\n    the slurmdbd.\n -- Fix bug in changing the nodes allocated to a running job and some node\n    names specified are invalid, avoid invalid memory reference.\n -- Fixed filename substitution of %h and %n based on patch from Ralph Bean\n -- Added better job sorting logic when preempting jobs with qos.\n -- Log the IP address and port number for some communication errors.\n -- Fix bug in select/cons_res when --cpus_per_task option is used, could\n    oversubscribe resources.\n -- In srun, do not implicitly set the job's maximum node count based upon a\n    required hostlist.\n -- Avoid running the HealthCheckProgram on non-responding nodes rather than\n    DOWN nodes.\n -- Fix bug in handling of poll() functions on OS X (SLURM was ignoring POLLIN\n    if POLLHUP flag was set at the same time).\n -- Pulled Cray logic out of common/node_select.c into it's own\n    select/cray plugin cons_res is the default.  To use linear add 'Linear' to\n    SelectTypeParameters.\n -- Fixed bug where resizing jobs didn't correctly set used limits correctly.\n -- Change sched/backfill default time interval to 30 seconds and defer attempt\n    to backfill schedule if slurmctld has more than 5 active RPCs. General\n    improvements in logic scalability.\n -- Add SchedulerParameters option of default_sched_depth=# to control how\n    many jobs on queue should be tested for attempted scheduling when a job\n    completes or other routine events. Default value is 100 jobs. The full job\n    queue is tested on a less frequent basis. This option can dramatically\n    improve performance on systems with thousands of queued jobs.\n -- Gres/gpu now sets the CUDA_VISIBLE_DEVICES environment to control which\n    GPU devices should be used for each job or job step and CUDA version 3.1+\n    is used. NOTE: SLURM's generic resource support is still under development.\n -- Modify select/cons_res to pack jobs onto allocated nodes differently and\n    minimize system fragmentation. For example on nodes with 8 CPUs each, a\n    job needing 10 CPUs will now ideally be allocated 8 CPUs on one node and\n    2 CPUs on another node. Previously the job would have ideally been\n    allocated 5 CPUs on each node, fragmenting the unused resources more.\n -- Modified the behavior of update_job() in job_mgr.c to return when the first\n    error is encountered instead of continuing with more job updates.\n -- Removed all references to the following slurm.conf parameters, all of which\n    have been removed or replaced since version 2.0 or earlier: HashBase,\n    HeartbeatInterval, JobAcctFrequency, JobAcctLogFile (instead use\n    AccountingStorageLoc), JobAcctType, KillTree, MaxMemPerTask, and\n    MpichGmDirectSupport.\n -- Fix bug in slurmctld restart logic that improperly reported jobs had\n    invalid features: \"Job 65537 has invalid feature list: fat\".\n -- BLUEGENE - Removed thread pool for destroying blocks.  It turns out the\n    memory leak we were concerned about for creating and destroying threads\n    in a plugin doesn't exist anymore.  This increases throughput dramatically,\n    allowing multiple jobs to start at the same time.\n -- BLUEGENE - Removed thread pool for starting and stopping jobs.  For similar\n    reasons as noted above.\n -- BLUEGENE - Handle blocks that never deallocate.\n\n* Changes in SLURM 2.2.0.pre9\n=============================\n -- sbatch can now submit jobs to multiple clusters and run on the earliest\n    available.\n -- Fix bug introduced in pre8 that prevented job dependencies and job\n    triggers from working without the --enable-debug configure option.\n -- Replaced slurm_addr with slurm_addr_t\n -- Replaced slurm_fd with slurm_fd_t\n -- Skeleton code added for BlueGeneQ.\n -- Jobs can now be submitted to multiple partitions (job queues) and use the\n    one permitting earliest start time.\n -- Change slurmdb_coord_table back to acct_coord_table to keep consistant\n    with < 2.1.\n -- Introduced locking system similar to that in the slurmctld for the\n    assoc_mgr.\n -- Added ability to change a users name in accounting.\n -- Restore squeue support for \"%G\" format (group id) accidentally removed in\n    2.2.0.pre7.\n -- Added preempt_mode option to QOS.\n -- Added a grouping=individual for sreport size reports.\n -- Added remove_qos logic to jobs running under a QOS that was removed.\n -- scancel now exits with a 1 if any job is non-existant when canceling.\n -- Better handling of select plugins that don't exist on various systems for\n    cross cluster communication.  Slurmctld, slurmd, and slurmstepd now only\n    load the default select plugin as well.\n -- Better error handling when loading plugins.\n -- Prevent scontrol from aborting if getlogin() returns NULL.\n -- Prevent scontrol segfault when there are hidden nodes.\n -- Prevent srun segfault after task launch failure.\n -- Added job_submit/lua plugin.\n -- Fixed sinfo on a bluegene system to print correctly the output for:\n    sinfo -e -o \"%9P %6m %.4c %.22F %f\"\n -- Add scontrol commands \"hold\" and \"release\" to simplify setting a job's\n    priority to 0 or 1. Also tests that the job is in pending state.\n -- Increase maximum node list size (for incoming RPC) from 1024 bytes to 64k.\n -- In the backup slurmctld, purge triggers before recovering trigger state to\n    avoid duplicate entries.\n -- Fix bug in sacct processing of --fields= option.\n -- Fix bug in checkpoint/blcr for jobs spanning multiple nodes introduced when\n    changing some variable names in version 2.2.0.pre5.\n -- Removed the vestigal set_max_cluster_usage() function from the Priority\n    Plugin API.\n -- Modify the output of \"scontrol show job\" for the field ReqS:C:T=. Fields\n    not specified by the user will be reported as \"*\" instead of 65534.\n -- Added DefaultQOS option for an association.\n -- BLUEGENE - Added -B option to the slurmctld to clear created blocks from\n    the system on start.\n -- BLUEGENE - Added option to scontrol & sview to recreate existing blocks.\n -- Fixed flags for returning messages to use the correct munge key when going\n    cross-cluster.\n -- BLUEGENE - Added option to scontrol & sview to resume blocks in an error\n    state instead of just freeing them.\n -- sview patched to allow multiple row selection of jobs, patch from Dan Rusak\n -- Lower default slurmctld server thread count from 1024 to 256. Some systems\n    process threads on a last-in first-out basis and the high thread count was\n    causing unexpectedly high delays for some RPCs.\n -- Added to sacctmgr the ability for admins to reset the raw usage of a user\n    or account\n -- Improved the efficiency of a few lines in sacctmgr\n\n* Changes in SLURM 2.2.0.pre8\n=============================\n -- Add DebugFlags parameter of \"Backfill\" for sched/backfill detailed logging.\n -- Add DebugFlags parameter of \"Gang\" for detailed logging of gang scheduling\n    activities.\n -- Add DebugFlags parameter of \"Priority\" for detailed logging of priority\n    multifactor activities.\n -- Add DebugFlags parameter of \"Reservation\" for detailed logging of advanced\n    reservations.\n -- Add run time to mail message upon job termination and queue time for mail\n    message upon job begin.\n -- Add email notification option for job requeue.\n -- Generate a fatal error if the srun --relative option is used when not\n    within an existing job allocation.\n -- Modify the meaning of InactiveLimit slightly. It will now cancel the job\n    allocation created using the salloc or srun command if those commands\n    cease responding for the InactiveLimit regardless of any running job steps.\n    This parameter will no longer effect jobs spawned using sbatch.\n -- Remove AccountingStoragePass and JobCompPass from configuration RPC and\n    scontrol show config command output. The use of SlurmDBD is still strongly\n    recommended as SLURM will have limited database functionality or protection\n    otherwise.\n -- Add sbatch options of --export and SBATCH_EXPORT to control which\n    environment variables (if any) get propagated to the spawned job. This is\n    particularly important for jobs that are submitted on one cluster and run\n    on a different cluster.\n -- Fix bug in select/linear when used with gang scheduling and there are\n    preempted jobs at the time slurmctld restarts that can result in over-\n    subscribing resources.\n -- Added keeping track of the qos a job is running with in accounting.\n -- Fix for handling correctly jobs that resize, and also reporting correct\n    stats on a job after it finishes.\n -- Modify gang scheduler so with SelectTypeParameter=CR_CPUS and task\n    affinity is enabled, keep track of the individual CPUs allocated to jobs\n    rather than just the count of CPUs allocated (which could overcommit\n    specific CPUs for running jobs).\n -- Modify select/linear plugin data structures to eliminate underflow errors\n    for the exclusive_cnt and tot_job_cnt variables (previously happened when\n    slurmctld reconfigured while the job was in completing state).\n -- Change slurmd's working directory (and location of core files) to match\n    that of the slurmctld daemon: the same directory used for log files,\n    SlurmdLogFile (if specified with an absolute pathname) otherwise the\n    directory used to save state, SlurmdSpoolDir.\n -- Add sattach support for the --pty option.\n -- Modify slurmctld communications logic to accept incoming messages on more\n    than one port for improved scalability.\n -- Add SchedulerParameters option of \"defer\" to avoid trying to schedule a\n    job at submission time, but to attempt scheduling many jobs at once for\n    improved performance under heavy load.\n -- Correct logic controlling slurmctld thread limit eliminating check of\n    RLIMIT_STACK.\n -- Make slurmctld's trigger logic more robust in the event that job records\n    get purged before their trigger can be processed (e.g. MinJobAge=1).\n -- Add support for users to hold/release their own jobs (submit the job with\n    srun/sbatch --hold/-H option or use \"scontrol update jobid=# priority=0\"\n    to hold and \"scontrol update jobid=# priority=1\" to release).\n -- Added ability for sacct to query jobs by qos and a range of timelimits.\n -- Added ability for sstat to query pids of steps running.\n -- Support time specification in UTS format with a prefix of \"uts\" (e.g.\n    \"sbatch --begin=uts458389988 my.script\").\n\n* Changes in SLURM 2.2.0.pre7\n=============================\n -- Fixed issue with sacctmgr if querying against non-existent cluster it\n    works the same way as 2.1.\n -- Added infrastructure to support allocation of generic node resources (gres).\n    -Modified select/linear and select/cons_res plugins to allocate resources\n     at the level of a job without oversubcription.\n    -Get sched/backfill operating with gres allocations.\n    -Get gres configuration changes (reconfiguration) working.\n    -Have job steps allocate resources.\n    -Modified job step credential to include the job's and step's gres\n     allocation details.\n    -Integrate with HWLOC library to identify GPUs and NICs configured on each\n     node.\n -- SLURM commands (squeue, sinfo, etc...) can now go cross-cluster on like\n    linux systems.  Cross-cluster for bluegene to linux and such should\n    work fine, even sview.\n -- Added the ability to configure PreemptMode on a per-partition basis.\n -- Change slurmctld's default thread limit count to 1024, but adjust that down\n    as needed based upon the process's resource limits.\n -- Removed the non-functional \"SystemCPU\" and \"TotalCPU\" reporting fields from\n    sstat and updated man page\n -- Correct location of apbasil command on Cray XT systems.\n -- Fixed bug in MinCPU and AveCPU calculations in sstat command\n -- Send message to srun when the Prolog takes too long (MessageTimeout) to\n    complete.\n -- Change timeout for socket connect() to be half of configured MessageTimeout.\n -- Added high-throughput computing web page with configuration guidance.\n -- Use more srun sockets to process incoming PMI (MPICH2) connections for\n    better scalability.\n -- Added DebugFlags for the select/bluegene plugin: DEBUG_FLAG_BG_PICK,\n    DEBUG_FLAG_BG_WIRES, DEBUG_FLAG_BG_ALGO, and DEBUG_FLAG_BG_ALGO_DEEP.\n -- Remove vestigial job record field \"kill_on_step_done\" (internal to the\n    slurmctld daemon only).\n -- For MPICH2 jobs: Clear PMI state between job steps.\n\n* Changes in SLURM 2.2.0.pre6\n=============================\n -- sview - added ability to see database configuration.\n -- sview - added ability to add/remove visible tabs.\n -- sview - change way grid highlighting takes place on selected objects.\n -- Added infrastructure to support allocation of generic node resources.\n    -Added node configuration parameter of Gres=.\n    -Added ability to view/modify a node's gres using scontrol, sinfo and sview.\n    -Added salloc, sbatch and srun --gres option.\n    -Added ability to view a job or job step's gres using scontrol, squeue and\n     sview.\n    -Added new configuration parameter GresPlugins to define plugins used to\n     manage generic resources.\n    -Added framework for gres plugins.\n    -Added DebugFlags option of \"gres\" for detailed debugging of gres actions.\n -- Slurmd modified to log slow slurmstepd startup and note possible file system\n    problem.\n -- sview - There is now a .slurm/sviewrc created when running sview.\n    Defaults are put in there as to how sview looks when first launched.\n    You can set these by Ctrl-S or Options->Set Default Settings.\n -- Add scontrol \"wait_job <job_id>\" option to wait for nodes to boot as needed.\n    Useful for batch jobs (in Prolog, PrologSlurmctld or the script) if powering\n    down idle nodes.\n -- Added salloc and sbatch option --wait-all-nodes. If set non-zero, job\n    initiation will be delayed until all allocated nodes have booted. Salloc\n    will log the delay with the messages \"Waiting for nodes to boot\" and \"Nodes\n    are ready for job\".\n -- The Priority/mulitfactor plugin now takes into consideration size of job\n    in cpus as well as size in nodes when looking at the job size factor.\n    Previously only nodes were considered.\n -- When using the SlurmDBD messages waiting to be sent will be combined\n    and sent in one message.\n -- Remove srun's --core option. Move the logic to an optional SPANK plugin\n    (currently in the contribs directory, but plan to distribute through\n    http://code.google.com/p/slurm-spank-plugins/).\n -- Patch for adding CR_CORE_DEFAULT_DIST_BLOCK as a select option to layout\n    jobs using block layout across cores within each node instead of cyclic\n    which was previously the default.\n -- Accounting - When removing associations if jobs are running, those jobs\n    must be killed before proceeding.  Before the jobs were killed\n    automatically thus causing user confusion on what is most likely an\n    admin's mistake.\n -- sview - color column keeps reference color when highlighting.\n -- Configuration parameter MaxJobCount changed from 16-bit to 32-bit field.\n    The default MaxJobCount was changed from 5,000 to 10,000.\n -- SLURM commands (squeue, sinfo, etc...) can now go cross-cluster on like\n    linux systems.  Cross-cluster for bluegene to linux and such does not\n    currently work.  You can submit jobs with sbatch.  Salloc and srun are not\n    cross-cluster compatible, and given their nature to talk to actual compute\n    nodes these will likely never be.\n -- salloc modified to forward SIGTERM to the spawned program.\n -- In sched/wiki2 (for Moab support) - Add GRES and WCKEY fields to MODIFYJOBS\n    and GETJOBS commands. Add GRES field to GETNODES command.\n -- In struct job_descriptor and struct job_info: rename min_sockets to\n    sockets_per_node, min_cores to cores_per_socket, and min_threads to\n    threads_per_core (the values are not minimum, but represent the target\n    values).\n -- Fixed bug in clearing a partition's DisableRootJobs value reported by\n    Hongjia Cao.\n -- Purge (or ignore) terminated jobs in a more timely fashion based upon the\n    MinJobAge configuration parameter. Small values for MinJobAge should improve\n    responsiveness for high job throughput.\n\n* Changes in SLURM 2.2.0.pre5\n=============================\n -- Modify commands to accept time format with one or two digit hour value\n    (e.g. 8:00 or 08:00 or 8:00:00 or 08:00:00).\n -- Modify time parsing logic to accept \"minute\", \"hour\", \"day\", and \"week\" in\n    addition to the currently accepted \"minutes\", \"hours\", etc.\n -- Add slurmd option of \"-C\" to print actual hardware configuration and exit.\n -- Pass EnforcePartLimits configuration parameter from slurmctld for user\n    commands to see the correct value instead of always \"NO\".\n -- Modify partition data structures to replace the default_part,\n    disable_root_jobs, hidden and root_only fields with a single field called\n    \"flags\" populated with the flags PART_FLAG_DEFAULT, PART_FLAG_NO_ROOT\n    PART_FLAG_HIDDEN and/or PART_FLAG_ROOT_ONLY. This is a more flexible\n    solution besides making for smaller data structures.\n -- Add node state flag of JOB_RESIZING. This will only exist when a job's\n    accounting record is being written immediately before or after it changes\n    size. This permits job accounting records to be written for a job at each\n    size.\n -- Make calls to jobcomp and accounting_storage plugins before and after a job\n    changes size (with the job state being JOB_RESIZING). All plugins write a\n    record for the job at each size with intermediate job states being\n    JOB_RESIZING.\n -- When changing a job size using scontrol, generate a script that can be\n    executed by the user to reset SLURM environment variables.\n -- Modify select/linear and select/cons_res to use resources released by job\n    resizing.\n -- Added to contribs foundation for Perl extension for slurmdb library.\n -- Add new configuration parameter JobSubmitPlugins which provides a mechanism\n    to set default job parameters or perform other site-configurable actions at\n    job submit time.\n -- Better postgres support for accounting, still beta.\n -- Speed up job start when using the slurmdbd.\n -- Forward step failure reason back to slurmd before in some cases it would\n    just be SLURM_FAILURE returned.\n -- Changed squeue to fail when passed invalid -o <output_format> or\n    -S <sort_list> specifications.\n\n* Changes in SLURM 2.2.0.pre4\n=============================\n -- Add support for a PropagatePrioProcess configuration parameter value of 2\n    to restrict spawned task nice values to that of the slurmd daemon plus 1.\n    This insures that the slurmd daemon always have a higher scheduling\n    priority than spawned tasks.\n -- Add support in slurmctld, slurmd and slurmdbd for option of \"-n <value>\" to\n    reset the daemon's nice value.\n -- Fixed slurm_load_slurmd_status and slurm_pid2jobid to work correctly when\n    multiple slurmds are in use.\n -- Altered srun to set max_nodes to min_nodes if not set when doing an\n    allocation to mimic that which salloc and sbatch do.  If running a step if\n    the max isn't set it remains unset.\n -- Applied patch from David Egolf (David.Egolf@Bull.com). Added the ability\n    to purge/archive accounting data on a day or hour basis, previously\n    it was only available on a monthly basis.\n -- Add support for maximum node count in job step request.\n -- Fix bug in CPU count logic for job step allocation (used count of CPUS per\n    node rather than CPUs allocated to the job).\n -- Add new configuration parameters GroupUpdateForce and GroupUpdateTime.\n    See \"man slurm.conf\" for details about how these control when slurmctld\n    updates its information of which users are in the groups allowed to use\n    partitions.\n -- Added sacctmgr list events which will list events that have happened on\n    clusters in accounting.\n -- Permit a running job to shrink in size using a command of\n    \"scontrol update JobId=# NumNodes=#\" or\n    \"scontrol update JobId=# NodeList=<names>\". Subsequent job steps must\n    explicitly specify an appropriate node count to work properly.\n -- Added resize_time field to job record noting the time of the latest job\n    size change (to be used for accounting purposes).\n -- sview/smap now hides hidden partitions and their jobs by default, with an\n    option to display them.\n\n* Changes in SLURM 2.2.0.pre3\n=============================\n -- Refine support for TotalView partial attach. Add parameter to configure\n    program of \"--enable-partial-attach\".\n -- In select/cons_res, the count of CPUs on required nodes was formerly\n    ignored in enforcing the maximum CPU limit. Also enforce maximum CPU\n    limit when the topology/tree plugin is configured (previously ignored).\n -- In select/cons_res, allocate cores for a job using a best-fit approach.\n -- In select/cons_res, for jobs that can run on a single node, use a best-fit\n    packing approach.\n -- Add support for new partition states of DRAIN and INACTIVE and new partition\n    option of \"Alternate\" (alternate partition to use for jobs submitted to\n    partitions that are currently in a state of DRAIN or INACTIVE).\n -- Add group membership cache. This can substantially speed up slurmctld\n    startup or reconfiguration if many partitions have AllowGroups configured.\n -- Added slurmdb api for accessing slurm DB information.\n -- In select/linear: Modify data structures for better performance and to\n    avoid underflow error messages when slurmctld restarts while jobs are\n    in completing state.\n -- Added hash for slurm.conf so when nodes check in to the controller it can\n    verify the slurm.conf is the same as the one it is running.  If not an\n    error message is displayed.  To silence this message add NO_CONF_HASH\n    to DebugFlags in your slurm.conf.\n -- Added error code ESLURM_CIRCULAR_DEPENDENCY and prevent circular job\n    dependencies (e.g. job 12 dependent upon job 11 AND job 11 is dependent\n    upon job 12).\n -- Add BootTime and SlurmdStartTime to available node information.\n -- Fixed moab_2_slurmdb to work correctly under new database schema.\n -- Slurmd will drain a compute node when the SlurmdSpoolDir is full.\n\n* Changes in SLURM 2.2.0.pre2\n=============================\n -- Add support for spank_get_item() to get S_STEP_ALLOC_CORES and\n    S_STEP_ALLOC_MEM. Support will remain for S_JOB_ALLOC_CORES and\n    S_JOB_ALLOC_MEM.\n -- Kill individual job steps that exceed their memory limit rather than\n    killing an entire job if one step exceeds its memory limit.\n -- Added configuration parameter VSizeFactor to enforce virtual memory limits\n    for jobs and job steps as a percentage of their real memory allocation.\n -- Add scontrol ability to update job step's time limits.\n -- Add scontrol ability to update job's NumCPUs count.\n -- Add --time-min options to salloc, sbatch and srun. The scontrol command\n    has been modified to display and modify the new field. sched/backfill\n    plugin has been changed to alter time limits of jobs with the\n    --time-min option if doing so permits earlier job initiation.\n -- Add support for TotalView symbol MPIR_partial_attach_ok with srun support\n    to release processes which TotalView does not attach to.\n -- Add new option for SelectTypeParameters of CR_ONE_TASK_PER_CORE. This\n    option will allocate one task per core by default. Without this option,\n    by default one task will be allocated per thread on nodes with more than\n    one ThreadsPerCore configured.\n -- Avoid accounting separately for a current pid corresponds to a Light Weight\n    Process (Thread POSIX) appearing in the /proc directory. Only account for\n    the original process (pid==tgid) to avoid accounting for memory use more\n    than once.\n -- Add proctrack/cgroup plugin which uses Linux control groups (aka cgroup)\n    to track processes on Linux systems having this feature enabled (kernel\n    >= 2.6.24).\n -- Add logging of license transations including job_id.\n -- Add configuration parameters SlurmSchedLogFile and SlurmSchedLogLevel to\n    support writing scheduling events to a separate log file.\n -- Added contribs/web_apps/chart_stats.cgi, a web app that invokes sreport to\n    retrieve from the accounting storage db a user's request for job usage or\n    machine utilization statistics and charts the results to a browser.\n -- Massive change to the schema in the storage_accounting/mysql plugin.  When\n    starting the slurmdbd the process of conversion may take a few minutes.\n    You might also see some errors such as 'error: mysql_query failed: 1206\n    The total number of locks exceeds the lock table size'.  If you get this,\n    do not worry, it is because your setting of innodb_buffer_pool_size in\n    your my.cnf file is not set or set too low.  A decent value there should\n    be 64M or higher depending on the system you are running on.  See\n    RELEASE_NOTES for more information.  But setting this and then\n    restarting the mysqld and slurmdbd will put things right.  After this\n    change we have noticed 50-75% increase in performance with sreport and\n    sacct.\n -- Fix for MaxCPUs to honor partitions of 1 node that have more than the\n    maxcpus for a job.\n -- Add support for \"scontrol notify <message>\" to work for batch jobs.\n\n* Changes in SLURM 2.2.0.pre1\n=============================\n -- Added RunTime field to scontrol show job report\n -- Added SLURM_VERSION_NUMBER and removed SLURM_API_VERSION from\n    slurm/slurm.h.\n -- Added support to handle communication with SLURM 2.1 clusters.  Job's\n    should not be lost in the future when upgrading to higher versions of\n    SLURM.\n -- Added withdeleted options for listing clusters, users, and accounts\n -- Remove PLPA task affinity functions due to that package being deprecated.\n -- Preserve current partition state information and node Feature and Weight\n    information rather than use contents of slurm.conf file after slurmctld\n    restart with -R option or SIGHUP. Replace information with contents of\n    slurm.conf after slurmctld restart without -R or \"scontrol reconfigure\".\n    See RELEASE_NOTES file fore more details.\n -- Modify SLURM's PMI library (for MPICH2) to properly execute an executable\n    program stand-alone (single MPI task launched without srun).\n -- Made GrpCPUs and MaxCPUs limits work for select/cons_res.\n -- Moved all SQL dependant plugins into a seperate rpm slurm-sql.  This\n    should be needed only where a connection to a database is needed (i.e.\n    where the slurmdbd is running)\n -- Add command line option \"no_sys_info\" to PAM module to supress system\n    logging of \"access granted for user ...\", access denied and other errors\n    will still be logged.\n -- sinfo -R now has the user and timestamp in separate fields from the reason.\n -- Much functionality has been added to account_storage/pgsql.  The plugin\n    is still in a very beta state.  It is still highly advised to use the\n    mysql plugin, but if you feel like living on the edge or just really\n    like postgres over mysql for some reason here you go. (Work done\n    primarily by Hongjia Cao, NUDT.)\n\n* Changes in SLURM 2.1.17\n=========================\n -- Correct format of --begin reported in salloc, sbatch and srun --help\n    message.\n -- Correct logic for regular users to increase nice value of their own jobs.\n\n* Changes in SLURM 2.1.16\n=========================\n -- Fixed minor warnings from gcc-4.5\n -- Fixed initialization of accounting_stroage_enforce in the slurmctld.\n -- Fixed bug where if GrpNodes was lowered while pending jobs existed and where\n    above the limit the slurmctld would seg fault.\n -- Fixed minor memory leak when unpack error happens on an\n    association_shares_object_t.\n -- Set Lft and Rgt correctly when adding association.  Fix for regression\n    caused in 2.1.15, cosmetic fix only.\n -- Replaced optarg which was undefined in some spots to make sure ENV vars are\n    set up correctly.\n -- When removing an account from a cluster with sacctmgr you no longer get\n    a list of previously deleted associations.\n -- Fix to make jobcomp/(pg/my)sql correctly work when the database name is\n    different than the default.\n\n* Changes in SLURM 2.1.15\n=========================\n -- Fix bug in which backup slurmctld can purge job scripts (and kill batch\n    jobs) when it assumes primary control, particularly when this happens\n    multiple times in a short time interval.\n -- In sched/wiki and sched/wiki2 add IWD (Initial Working Directory) to the\n    information reported about jobs.\n -- Fix bug in calculating a daily or weekly reservation start time when the\n    reservation is updated. Patch from Per Lundqvist (National Supercomputer\n    Centre, Link\u00f6ping University, Sweden).\n -- Fix bug in how job step memory limits are calculated when the --relative\n    option is used.\n -- Restore operation of srun -X option to forward SIGINT to spawned tasks\n    without killing them.\n -- Fixed a bug in calculating the root account's raw usage reported by Par\n    Andersson\n -- Fixed a bug in sshare displaying account hierarchy reported by Per\n    Lundqvist.\n -- In select/linear plugin, when a node allocated to a running job is removed\n    from a partition, only log the event once. Fixes problem reported by Per\n    Lundqvist.\n\n* Changes in SLURM 2.1.14\n=========================\n -- Fixed coding mistakes in _slurm_rpc_resv_show() and job_alloc_info() found\n    while reviewing the code.\n -- Fix select/cons_res logic to prevent allocating resources while jobs\n    previously allocated resources on the node are still completing.\n -- Fixed typo in job_mgr.c dealing with qos instead of associations.\n -- Make sure associations and qos' are initiated when added.\n -- Fixed wrong initialization for wckeys in the association manager.\n -- Added wiki.conf configuration parameter of HidePartitionNodes. See\n    \"man wiki.conf\" for more information.\n -- Add \"JobAggregationTime=#\" field SchedulerParameter configuration parameter\n    output.\n -- Modify init.d/slurm and slurmdbd scripts to prevent the possible\n    inadvertent inclusion of \".\" in LD_LIBRARY_PATH environment variable.\n    To fail, the script would need to be executed by user root or SlurmUser\n    without the LD_LIBRARY_PATH environment variable set and there would\n    have to be a maliciously altered library in the working directory.\n    Thanks to Raphael Geissert for identifying the problem. This addresses\n    security vulnerability CVE-2010-3380.\n\n* Changes in SLURM 2.1.13\n=========================\n -- Fix race condition which can set a node state to IDLE on slurmctld startup\n    even if it has running jobs.\n\n* Changes in SLURM 2.1.12\n=========================\n -- Fixes for building on OS X 10.5.\n -- Fixed a few '-' without a '\\' in front of them in the man pages.\n -- Fixed issues in client tools where a requeued job did get displayed\n    correctly.\n -- Update typos in doc/html/accounting.shtml doc/html/resource_limits.shtml\n    doc/man/man5/slurmdbd.conf.5 and doc/man/man5/slurm.conf.5\n -- Fixed a bug in exitcode:signal display in sacct\n -- Fix bug when request comes in for consumable resources and the -c option\n    is used in conjunction with -O\n -- Fixed squeue -o \"%h\" output formatting\n -- Change select/linear message \"error: job xxx: best_fit topology failure\"\n    to debug type.\n -- BLUEGENE - Fix for sinfo -R to group all midplanes together in a single\n    line for midplanes in an error state instead of 1 line for each midplane.\n -- Fix srun to work correctly with --uid when getting an allocation\n    and creating a step, also fix salloc to assume identity at the correct\n    time as well.\n -- BLUEGENE - Fixed issue with jobs being refused when running dynamic mode\n    and every job on the system happens to be the same size.\n -- Removed bad #define _SLURMD_H from slurmd/get_mach_stat.h.  Didn't appear\n    to cause any problems being there, just incorrect syntax.\n -- Validate the job ID when salloc or srun receive an SRUN_JOB_COMPLETE RPC to\n    avoid killing the wrong job if the original command exits and the port gets\n    re-used by another command right away.\n -- Fix to node in correct state in accounting when updating it to drain from\n    scontrol/sview.\n -- BLUEGENE - Removed incorrect unlocking on error cases when starting jobs.\n -- Improve logging of invalid sinfo and squeue print options.\n -- BLUEGENE - Added check to libsched_if to allow root to run even outside of\n    SLURM.  This is needed when running certain blocks outside of SLURM in HTC\n    mode.\n\n* Changes in SLURM 2.1.11-2\n===========================\n -- BLUEGENE - make it so libsched_if.so is named correctly on 'L' it is\n    libsched_if64.so and on 'P' it is libsched_if.so\n\n* Changes in SLURM 2.1.11\n=========================\n -- BLUEGENE - fix sinfo to not get duplicate entries when running command\n    sinfo -e -o \"%9P %6m %.4c %.22F %f\"\n -- Fix bug that caused segv when deleting a partition with pending jobs.\n -- Better error message for when trying to modify an account's name with\n    sacctmgr.\n -- Added back removal of #include \"src/common/slurm_xlator.h\" from\n    select/cons_res.\n -- Fix incorrect logic in global_accounting in regression tests for\n    setting QOS.\n -- BLUEGENE - Fixed issue where removing a small block in dynamic mode,\n    and other blocks also in that midplane needed to be removed and were in\n    and error state.  They all weren't removed correctly in accounting.\n -- Prevent scontrol segv with \"scontrol show node <name>\" command with nodes\n    in a hidden partition.\n -- Fixed sizing of popup grids in sview.\n -- Fixed sacct when querying against a jobid the start time is not set.\n -- Fix configure to get correct version of pkg-config if both 32bit and 64bit\n    libs are installed.\n -- Fix issue with sshare not sorting correctly the tree of associations.\n -- Update documentation for sreport.\n -- BLUEGENE - fix regression in 2.1.10 on assigning multiple jobs to one block.\n -- Minor memory leak fixed when killing job error happens.\n -- Fix sacctmgr list assoc when talking to a 2.2 slurmdbd.\n\n* Changes in SLURM 2.1.10\n=========================\n -- Fix memory leak in sched/builtin plugin.\n -- Fixed sbatch to work correctly when no nodes are specified, but\n    --ntasks-per-node is.\n -- Make sure account and wckey for a job are lower case before inserting into\n    accounting.\n -- Added note to squeue documentation about --jobs option displaying jobs\n    even if they are on hidden partitions.\n -- Fix srun to work correctly with --uid when getting an allocation\n    and creating a step.\n -- Fix for when removing a limit from a users association inside the\n    fairshare tree the parents limit is now inherited automatically in\n    the slurmctld.  Previously the slurmctld would have to be restarted.\n    This problem only exists when setting a users association limit to -1.\n -- Patch from Matthieu Hautreux (CEA) dealing with possible overflows that\n    could come up with the select/cons_res plugin with uint32_t's being treated\n    as uint16_t's.\n -- Correct logic for creating a reservation with a Duration=Infinite (used to\n    set reservation end time in the past).\n -- Correct logic for creating a reservation that properly handles the OVERLAP\n    and IGNORE_JOBS flags (flags were ignored under some conditions).\n -- Fixed a fair-share calculation bug in the priority/multifactor plugin.\n -- Make sure a user entry in the database that was previously deleted is\n    restored clean when added back, i.e. remove admin privileges previously\n    given.\n -- BLUEGENE - Future start time is set correctly when eligible time for a job\n    is in the future, but the job can physically run earlier.\n -- Updated Documentation for sacctmgr for Wall and CPUMin options stating when\n    the limit is reached running jobs will be killed.\n -- Fix deadlock issue in the slurmctld when lowering limits in accounting to\n    lower than that of pending jobs.\n -- Fix bug in salloc, sbatch and srun that could under some conditions process\n    the --threads-per-core, --cores-per-socket and --sockets-per-node options\n    improperly.\n -- Fix bug in select/cons_res with memory management plus job preemption with\n    job removal (e.g. requeue) which under some conditions failed to preempt\n    jobs.\n -- Fix deadlock potential when using qos and associations in the slurmctld.\n -- Update documentation to state --ntasks-per-* is for a maximum value\n    instead of an absolute.\n -- Get ReturnToService=2 working for front-end configurations (e.g. Cray or\n    BlueGene).\n -- Do not make a non-responding node available for use after running\n    \"scontrol update nodename=<name> state=resume\". Wait for node to respond\n    before use.\n -- Added slurm_xlator.h to jobacct_gather plugins so they resolve symbols\n    correctly when linking to the slurm api.\n -- You can now update a jobs QOS from scontrol.  Previously you could only do\n    this from sview.\n -- BLUEGENE - Fixed bug where if running in non-dynamic mode sometimes the\n    start time returned for a job when using test-only would not be correct.\n\n* Changes in SLURM 2.1.9\n========================\n -- In select/linear - Fix logic to prevent over-subscribing memory with shared\n    nodes (Shared=YES or Shared=FORCE).\n -- Fix for handling -N and --ntasks-per-node without specifying -n with\n    salloc and sbatch.\n -- Fix jobacct_gather/linux if not polling on tasks to give tasks time to\n    start before doing initial gather.\n -- When changing priority with the multifactor plugin we make sure we update\n    the last_job_update variable.\n -- Fixed sview for gtk < 2.10 to display correct debug level at first.\n -- Fixed sview to not select too fast when using a mouse right click.\n -- Fixed sacct to display correct timelimits for jobs from accounting.\n -- Fixed sacct when running as root by default query all users as documented.\n -- In proctrack/linuxproc, skip over files in /proc that are not really user\n    processes (e.g. \"/proc/bus\").\n -- Fix documentation bug for slurmdbd.conf\n -- Fix slurmctld to update qos preempt list without restart.\n -- Fix bug in select/cons_res that in some cases would prevent a preempting job\n    from using of resources already allocated to a preemptable running job.\n -- Fix for sreport in interactive mode to honor parsable/2 options.\n -- Fixed minor bugs in sacct and sstat commands\n -- BLUEGENE - Fixed issue if the slurmd becomes unresponsive and you have\n    blocks in an error state accounting is correct when the slurmd comes\n    back up.\n -- Corrected documentation for -n option in srun/salloc/sbatch\n -- BLUEGENE - when running a willrun test along with preemption the bluegene\n    plugin now does the correct thing.\n -- Fix possible memory corruption issue which can cause slurmctld to abort.\n -- BLUEGENE - fixed small memory leak when setting up env.\n -- Fixed deadlock if using accounting and cluster changes size in the\n    database.  This can happen if you mistakenly have multiple primary\n    slurmctld's running for a single cluster, which should rarely if ever\n    happen.\n -- Fixed sacct -c option.\n -- Critical bug fix in sched/backfill plugin that caused memory corruption.\n\n* Changes in SLURM 2.1.8\n========================\n -- Update BUILD_NOTES for AIX and bgp systems on how to get sview to\n    build correctly.\n -- Update man page for scontrol when nodes are in the \"MIXED\" state.\n -- Better error messages for sacctmgr.\n -- Fix bug in allocation of CPUs with select/cons_res and --cpus-per-task\n    option.\n -- Fix bug in dependency support for afterok and afternotok options to insure\n    that the job's exit status gets checked for dependent jobs prior to puring\n    completed job records.\n -- Fix bug in sched/backfill that could set an incorrect expected start time\n    for a job.\n -- BLUEGENE - Fix for systems that have midplanes defined in the database\n    that don't exist.\n -- Accounting, fixed bug where if removing an object a rollback wasn't\n    possible.\n -- Fix possible scontrol stack corruption when listing jobs with very a long\n    job or working directory name (over 511 characters).\n -- Insure that SPANK environment variables set by salloc or sbatch get\n    propagated to the Prolog on all nodes by setting SLURM_SPANK_* environment\n    variables for srun's use.\n -- In sched/wiki2 - Add support for the MODIFYJOB command to alter a job's\n    comment field\n -- When a cluster first registers with the SlurmDBD only send nodes in an\n    non-usable state.  Before all nodes were sent.\n -- Alter sacct to be able to query jobs by association id.\n -- Edit documentation for scontrol stating ExitCode as something not alterable.\n -- Update documentation about ReturnToService and silently rebooting nodes.\n -- When combining --ntasks-per-node and --exclusive in an allocation request\n    the correct thing, giving the allocation the entire node but only\n    ntasks-per-node, happens.\n -- Fix accounting transaction logs when deleting associations to put the\n    ids instead of the lfts which could change over time.\n -- Fix support for salloc, sbatch and srun's --hint option to avoid allocating\n    a job more sockets per node or more cores per socket than desired. Also\n    when --hint=compute_bound or --hint=memory_bound then avoid allocating more\n    than one task per hyperthread (a change in behavior, but almost certainly\n    a preferable mode of operation).\n\n* Changes in SLURM 2.1.7\n========================\n -- Modify srun, salloc and sbatch parsing for the --signal option to accept\n    either a signal name in addition to the previously supported signal\n    numbers (e.g. \"--signal=USR2@200\").\n -- BLUEGENE - Fixed sinfo --long --Node output for cpus on a single cnode.\n -- In sched/wiki2 - Fix another logic bug in support of Moab being able to\n    identify preemptable jobs.\n -- In sched/wiki2 - For BlueGene systems only: Fix bug preventing Moab from\n    being able to correctly change the node count of pending jobs.\n -- In select/cons_res - Fix bug preventing job preemption with a configuration\n    of Shared=FORCE:1 and PreemptMode=GANG,SUSPEND.\n -- In the TaskProlog, add support for an \"unset\" option to clear environment\n    variables for the user application. Also add support for embedded white-\n    space in the environment variables exported to the user application\n    (everything after the equal sign to the end of the line is included without\n    alteration).\n -- Do not install /etc/init.d/slurm or /etc/init.d/slurmdbd on AIX systems.\n -- BLUEGENE - fixed check for small blocks if a node card of a midplane is\n    in an error state other jobs can still run on the midplane on other\n    nodecards.\n -- BLUEGENE - Check to make sure job killing is in the active job table in\n    DB2 when killing the job.\n -- Correct logic to support ResvOverRun configuration parameter.\n -- Get --acctg-freq option working for srun and salloc commands.\n -- Fix sinfo display of drained nodes correctly with the summarize flag.\n -- Fix minor memory leaks in slurmd and slurmstepd.\n -- Better error messages for failed step launch.\n -- Modify srun to insure compatability of the --relative option with the node\n    count requested.\n\n* Changes in SLURM 2.1.6-2\n==========================\n -- In sched/wiki2 - Fix logic in support of Moab being able to identify\n    preemptable jobs.\n -- Applied fixes to a debug4 message in priority_multifactor.c sent in by\n    Per Lundqvist\n -- BLUEGENE - Fixed issue where incorrect nodecards could be picked when\n    looking at combining small blocks to make a larger small block.\n\n* Changes in SLURM 2.1.6\n========================\n -- For newly submitted jobs, report expected start time in squeue --start as\n    \"N/A\" rather than current time.\n -- Correct sched/backfill logic so that it runs in a more timely fashion.\n -- Fixed issue if running on accounting cache and priority/multifactor to\n    initialize the root association when the database comes back up.\n -- Emulated BLUEGENE - fixed issue where blocks weren't always created\n    correctly when loading from state.  This does not apply to a real\n    bluegene system, only emulated.\n -- Fixed bug when job is completing and its cpu_cnt would be calculated\n    incorrectly, possibly resulting in an underflow being logged.\n -- Fixed bug where if there are pending jobs in a partition which was\n    updated to have no nodes in it the slurmctld would dump core.\n -- Fixed smap and sview to display partitions with no nodes in them.\n -- Improve configure script's logic to detect LUA libraries.\n -- Fix bug that could cause slurmctld to abort if select/cons_res is used AND a\n    job is submitted using the --no-kill option AND one of the job's nodes goes\n    DOWN AND slurmctld restarts while that job is still running.\n -- In jobcomp plugins, job time limit was sometimes recorded improperly if not\n    set by user (recorded huge number rather than partition's time limit).\n\n* Changes in SLURM 2.1.5\n========================\n -- BLUEGENE - Fixed display of draining nodes for sinfo -R.\n -- Fixes to scontrol and sview when setting a job to an impossible start time.\n -- Added -h to srun for help.\n -- Fix for sacctmgr man page to remove erroneous 'with' statements.\n -- Fix for unpacking jobs with accounting statistics, previously it appears\n    only steps were unpacked correctly, for the most case sacct would only\n    display this information making this fix a very minor one.\n -- Changed scontrol and sview output for jobs with unknown end times from\n    'NONE' to 'Unknown'.\n -- Fixed mysql plugin to reset classification when adding a\n    previously deleted cluster.\n -- Permit a batch script to reset umask and have that propagate to tasks\n    spawed by subsequent srun. Previously the umask in effect when sbatch was\n    executed was propagated to tasks spawed by srun.\n -- Modify slurm_job_cpus_allocated_on_node_id() and\n    slurm_job_cpus_allocated_on_node() functions to not write explanation of\n    failures to stderr. Only return -1 and set errno.\n -- Correction in configurator.html script. Prolog and Epilog were reversed.\n -- BLUEGENE - Fixed race condition where if a nodecard has an error on an\n    un-booted block when a job comes to use it before the state checking\n    thread notices it which could cause the slurmctld to lock up on a\n    non-dynamic system.\n -- In select/cons_res with FastSchedule=0 and Procs=# defined for the node,\n    but no specific socket/core/thread count configured, avoid fatal error if\n    the number of cores on a node is less than the number of Procs configured.\n -- Added ability for the perlapi to utilize opaque data types returned from\n    the C api.\n -- BLUEGENE - made the perlapi get correct values for cpus per node,\n    Previously it would give the number of cpus per cnode instead of midplane.\n -- BLEUGENE - Fixed issue where if a block being selected for a job to use\n    and during the process a hardware failure happens, previously the block\n    would still be allowed to be used which would fail or requeue the job\n    depending on the configuration.\n -- For SPANK job environment, avoid duplicate \"SPANK_\" prefix for environment\n    set by sbatch jobs.\n -- Make squeue select jobs on hidden partitions when requesting more than one.\n -- Avoid automatically cancelling job steps when all of the tasks on some node\n    have gracefully terminated.\n\n* Changes in SLURM 2.1.4\n========================\n -- Fix for purge script in accounting to use correct options.\n -- If SelectType=select/linear and SelectTypeParameters=CR_Memory fix bug that\n    would fail to release memory reserved for a job if \"scontrol reconfigure\"\n    is executed while the job is in completing state.\n -- Fix bug in handling event trigger for job time limit while job is still\n    in pending state.\n -- Fixed display of Ave/MaxCPU in sacct for jobs. Steps were printed\n    correctly.\n -- When node current features differs from slurm.conf, log the node names\n    using a hostlist expression rather than listing individual node names.\n -- Improve ability of srun to abort job step for some task launch failures.\n -- Fix mvapich plugin logic to release the created job allocation on\n    initialization failure (previously the failures would cancel job step,\n    but retain job allocation).\n -- Fix bug in srun for task count so large that it overflows int data type.\n -- Fix important bug in select/cons_res handling of ntasks-per-core parameter\n    that was uncovered by a bug fixed in v2.1.3. Bug produced fatal error for\n    slurmctld: \"cons_res: cpus computation error\".\n -- Fix bug in select/cons_res handling of partitions configured with\n    Shared=YES. Prior logic failed to support running multiple jobs per node.\n\n* Changes in SLURM 2.1.3-2\n==========================\n -- Modified spec file to obsolete pam_slurm when installing\n    the slurm-pam_slurm rpm.\n\n* Changes in SLURM 2.1.3-1\n==========================\n -- BLUEGENE - Fix issues on static/overlap systems where if a midplane\n    was drained you would not be able to create new blocks on it.\n -- In sched/wiki2 (for Moab): Add excluded host list to job information\n    using new keyword \"EXCLUDE_HOSTLIST\".\n -- Correct slurmd reporting of incorrect socket/core/thread counts.\n -- For sched/wiki2 (Moab): Do not extend a job's end time for suspend/resume\n    or startup delay due to node boot time. A job's end time will always be\n    its start time plus time limit.\n -- Added build-time option (to configure program) of --with-pam_dir to\n    specify the directory into which PAM modules get installed, although it\n    should pick the proper directory by default. \"make install\" and \"rpmbuild\"\n    should now put the pam_slurm.so file in the proper directory.\n -- Modify PAM module to link against SLURM API shared library and use exported\n    slurm_hostlist functions.\n -- Do not block new jobs with --immediate option while another job is in the\n    process of being requeued (which can take a long time for some node failure\n    modes).\n -- For topology/tree, log invalid hostnames in a single hostlist expression\n    rather than one per line.\n -- A job step's default time limit will be UNLIMITED rather than partition's\n    default time limit. The step will automatically be cancelled as part of the\n    job termination logic when the job's time limit is reached.\n -- sacct - fixed bug when checking jobs against a reservation\n -- In select/cons_res, fix support for job allocation with --ntasks_per_node\n    option. Previously could allocate too few CPUs on some nodes.\n -- Adjustment made to init message to the slurmdbd to allow backwards\n    compatibility with future 2.2 release. YOU NEED TO UPGRADE SLURMDBD\n    BEFORE ANYTHING ELSE.\n -- Fix accounting when comment of down/drained node has double quotes in it.\n\n* Changes in SLURM 2.1.2\n========================\n -- Added nodelist to sview for jobs on non-bluegene systems\n -- Correction in value of batch job environment variable SLURM_TASKS_PER_NODE\n    under some conditions.\n -- When a node silently fails which is already drained/down the reason\n    for draining for the node is not changed.\n -- Srun will ignore SLURM_NNODES environment variable and use the count of\n    currently allocated nodes if that count changes during the job's lifetime\n    (e.g. job allocation uses the --no-kill option and a node goes DOWN, job\n    step would previously always fail).\n -- Made it so sacctmgr can't add blank user or account.  The MySQL plugin\n    will also reject such requests.\n -- Revert libpmi.so version for compatibility with SLURM version 2.0 and\n    earlier to avoid forcing applications using a specific libpmi.so version to\n    rebuild unnecessarily (revert from libpmi.so.21.0.0 to libpmi.so.0.0.0).\n -- Restore support for a pending job's constraints (required node features)\n    when slurmctld is restarted (internal structure needed to be rebuilt).\n -- Removed checkpoint_blcr.so from the plugin rpm in the slurm.spec since\n    it is also in the blcr rpm.\n -- Fixed issue in sview where you were unable to edit the count\n    of jobs to share resources.\n -- BLUEGENE - Fixed issue where tasks on steps weren't being displayed\n    correctly with scontrol and sview.\n -- BLUEGENE - fixed wiki2 plugin to report correct task count for pending\n    jobs.\n -- BLUEGENE - Added /etc/ld.so.conf.d/slurm.conf to point to the\n    directory holding libsched_if64.so when building rpms.\n -- Adjust get_wckeys call in slurmdbd to allow operators to list wckeys.\n\n* Changes in SLURM 2.1.1\n========================\n -- Fix for case sensitive databases when a slurmctld has a mixed case\n    clustername to lower case the string to easy compares.\n -- Fix squeue if job is completing and failed to print remaining\n    nodes instead of failed message.\n -- Fix sview core when searching for partitions by state.\n -- Fixed setting the start time when querying in sacct to the\n    beginning of the day if not set previously.\n -- Defined slurm_free_reservation_info_msg and slurm_free_topo_info_msg\n    in common/slurm_protocol_defs.h\n -- Avoid generating error when a job step includes a memory specification and\n    memory is not configured as a consumable resource.\n -- Patch for small memory leak in src/common/plugstack.c\n -- Fix sview search on node state.\n -- Fix bug in which improperly formed job dependency specification can cause\n    slurmctld to abort.\n -- Fixed issue where slurmctld wouldn't always get a message to send cluster\n    information when registering for the first time with the slurmdbd.\n -- Add slurm_*_trigger.3 man pages for event trigger APIs.\n -- Fix bug in job preemption logic that would free allocated memory twice.\n -- Fix spelling issues (from Gennaro Oliva)\n -- Fix issue when changing parents of an account in accounting all children\n    weren't always sent to their respected slurmctlds until a restart.\n -- Restore support for srun/salloc/sbatch option --hint=nomultithread to\n    bind tasks to cores rather than threads (broken in slurm v2.1.0-pre5).\n -- Fix issue where a 2.0 sacct could not talk correctly to a 2.1 slurmdbd.\n -- BLUEGENE - Fix issue where no partitions have any nodes assigned them to\n    alert user no blocks can be created.\n -- BLUEGENE - Fix smap to put BGP images when using -Dc on a Blue Gene/P\n    system.\n -- Set SLURM_SUBMIT_DIR environment variable for srun and salloc commands to\n    match behavior of sbatch command.\n -- Report WorkDir from \"scontrol show job\" command for jobs launched using\n    salloc and srun.\n -- Update correctly the wckey when changing it on a pending job.\n -- Set wckeyid correctly in accounting when cancelling a pending job.\n -- BLUEGENE - critical fix where jobs would be killed incorrectly.\n -- BLUEGENE - fix for sview putting multiple ionodes on to nodelists when\n    viewing the jobs tab.\n\n* Changes in SLURM 2.1.0\n========================\n -- Improve sview layout of blocks in use.\n -- A user can now change the dimensions of the grid in sview.\n -- BLUEGENE - improved startup speed further for large numbers of defined\n    blocks\n -- Fix to _get_job_min_nodes() in wiki2/get_jobs.c suggested by Michal Novotny\n -- BLUEGENE - fixed issues when updating a pending job when a node\n    count was incorrect for the asked for connection type.\n -- BLUEGENE - fixed issue when combining blocks that are in ready states to\n    make a larger block from those or make multiple smaller blocks by\n    splitting the larger block.  Previously this would only work with block\n    in a free state.\n -- Fix bug in wiki(2) plugins where if HostFormat=2 and the task list is\n    greater than 64 we don't truncate.  Previously this would mess up Moab\n    by sending a truncated task list when doing a get jobs.\n -- Added update slurmctld debug level to sview when in admin mode.\n -- Added logic to make sure if enforcing a memory limit when using the\n    jobacct_gather plugin a user can no longer turn off the logic to enforce\n    the limit.\n -- Replaced many calls to getpwuid() with reentrant uid_to_string()\n -- The slurmstepd will now refresh it's log file handle on a reconfig,\n    previously if a log was rolled any output from the stepd was lost.\n", "/*****************************************************************************\\\n *  src/slurmd/slurmd/req.c - slurmd request handling\n *****************************************************************************\n *  Copyright (C) 2002-2007 The Regents of the University of California.\n *  Copyright (C) 2008-2010 Lawrence Livermore National Security.\n *  Portions Copyright (C) 2010-2016 SchedMD LLC.\n *  Portions copyright (C) 2015 Mellanox Technologies Inc.\n *  Produced at Lawrence Livermore National Laboratory (cf, DISCLAIMER).\n *  Written by Mark Grondona <mgrondona@llnl.gov>.\n *  CODE-OCEC-09-009. All rights reserved.\n *\n *  This file is part of SLURM, a resource management program.\n *  For details, see <http://slurm.schedmd.com/>.\n *  Please also read the included file: DISCLAIMER.\n *\n *  SLURM is free software; you can redistribute it and/or modify it under\n *  the terms of the GNU General Public License as published by the Free\n *  Software Foundation; either version 2 of the License, or (at your option)\n *  any later version.\n *\n *  In addition, as a special exception, the copyright holders give permission\n *  to link the code of portions of this program with the OpenSSL library under\n *  certain conditions as described in each individual source file, and\n *  distribute linked combinations including the two. You must obey the GNU\n *  General Public License in all respects for all of the code used other than\n *  OpenSSL. If you modify file(s) with this exception, you may extend this\n *  exception to your version of the file(s), but you are not obligated to do\n *  so. If you do not wish to do so, delete this exception statement from your\n *  version.  If you delete this exception statement from all source files in\n *  the program, then also delete it here.\n *\n *  SLURM is distributed in the hope that it will be useful, but WITHOUT ANY\n *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n *  details.\n *\n *  You should have received a copy of the GNU General Public License along\n *  with SLURM; if not, write to the Free Software Foundation, Inc.,\n *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.\n\\*****************************************************************************/\n#if HAVE_CONFIG_H\n#  include \"config.h\"\n#endif\n\n#include <fcntl.h>\n#include <grp.h>\n#include <pthread.h>\n#include <sched.h>\n#include <signal.h>\n#include <stdlib.h>\n#include <string.h>\n#include <time.h>\n#include <sys/param.h>\n#include <poll.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <sys/un.h>\n#include <sys/wait.h>\n#include <unistd.h>\n#include <utime.h>\n\n#include \"src/common/callerid.h\"\n#include \"src/common/cpu_frequency.h\"\n#include \"src/common/env.h\"\n#include \"src/common/fd.h\"\n#include \"src/common/forward.h\"\n#include \"src/common/gres.h\"\n#include \"src/common/hostlist.h\"\n#include \"src/common/list.h\"\n#include \"src/common/log.h\"\n#include \"src/common/macros.h\"\n#include \"src/common/msg_aggr.h\"\n#include \"src/common/node_features.h\"\n#include \"src/common/node_select.h\"\n#include \"src/common/plugstack.h\"\n#include \"src/common/read_config.h\"\n#include \"src/common/siphash.h\"\n#include \"src/common/slurm_auth.h\"\n#include \"src/common/slurm_cred.h\"\n#include \"src/common/slurm_acct_gather_energy.h\"\n#include \"src/common/slurm_jobacct_gather.h\"\n#include \"src/common/slurm_protocol_defs.h\"\n#include \"src/common/slurm_protocol_api.h\"\n#include \"src/common/slurm_protocol_interface.h\"\n#include \"src/common/slurm_strcasestr.h\"\n#include \"src/common/stepd_api.h\"\n#include \"src/common/uid.h\"\n#include \"src/common/util-net.h\"\n#include \"src/common/xstring.h\"\n#include \"src/common/xmalloc.h\"\n\n#include \"src/bcast/file_bcast.h\"\n\n#include \"src/slurmd/slurmd/get_mach_stat.h\"\n#include \"src/slurmd/slurmd/slurmd.h\"\n\n#include \"src/slurmd/common/job_container_plugin.h\"\n#include \"src/slurmd/common/proctrack.h\"\n#include \"src/slurmd/common/run_script.h\"\n#include \"src/slurmd/common/reverse_tree_math.h\"\n#include \"src/slurmd/common/slurmstepd_init.h\"\n#include \"src/slurmd/common/task_plugin.h\"\n\n#define _LIMIT_INFO 0\n\n#define RETRY_DELAY 15\t\t/* retry every 15 seconds */\n#define MAX_RETRY   240\t\t/* retry 240 times (one hour max) */\n\n#define EPIL_RETRY_MAX 2\t/* max retries of epilog complete message */\n\n#ifndef MAXHOSTNAMELEN\n#define MAXHOSTNAMELEN\t64\n#endif\n\ntypedef struct {\n\tint ngids;\n\tgid_t *gids;\n} gids_t;\n\ntypedef struct {\n\tuint32_t job_id;\n\tuint32_t step_id;\n\tuint32_t job_mem;\n\tuint32_t step_mem;\n} job_mem_limits_t;\n\ntypedef struct {\n\tuint32_t job_id;\n\tuint32_t step_id;\n} starting_step_t;\n\ntypedef struct {\n\tuint32_t job_id;\n\tuint16_t msg_timeout;\n\tbool *prolog_fini;\n\tpthread_cond_t *timer_cond;\n\tpthread_mutex_t *timer_mutex;\n} timer_struct_t;\n\ntypedef struct {\n\tuint32_t jobid;\n\tuint32_t step_id;\n\tchar *node_list;\n\tchar *partition;\n\tchar *resv_id;\n\tchar **spank_job_env;\n\tuint32_t spank_job_env_size;\n\tuid_t uid;\n\tchar *user_name;\n} job_env_t;\n\nstatic int  _abort_step(uint32_t job_id, uint32_t step_id);\nstatic char **_build_env(job_env_t *job_env);\nstatic void _delay_rpc(int host_inx, int host_cnt, int usec_per_rpc);\nstatic void _destroy_env(char **env);\nstatic bool _is_batch_job_finished(uint32_t job_id);\nstatic void _job_limits_free(void *x);\nstatic int  _job_limits_match(void *x, void *key);\nstatic bool _job_still_running(uint32_t job_id);\nstatic int  _kill_all_active_steps(uint32_t jobid, int sig, bool batch);\nstatic void _launch_complete_add(uint32_t job_id);\nstatic void _launch_complete_log(char *type, uint32_t job_id);\nstatic void _launch_complete_rm(uint32_t job_id);\nstatic void _launch_complete_wait(uint32_t job_id);\nstatic int  _launch_job_fail(uint32_t job_id, uint32_t slurm_rc);\nstatic bool _launch_job_test(uint32_t job_id);\nstatic void _note_batch_job_finished(uint32_t job_id);\nstatic int  _prolog_is_running (uint32_t jobid);\nstatic int  _step_limits_match(void *x, void *key);\nstatic int  _terminate_all_steps(uint32_t jobid, bool batch);\nstatic int  _receive_fd(int socket);\nstatic void _rpc_launch_tasks(slurm_msg_t *);\nstatic void _rpc_abort_job(slurm_msg_t *);\nstatic void _rpc_batch_job(slurm_msg_t *msg, bool new_msg);\nstatic void _rpc_prolog(slurm_msg_t *msg);\nstatic void _rpc_job_notify(slurm_msg_t *);\nstatic void _rpc_signal_tasks(slurm_msg_t *);\nstatic void _rpc_checkpoint_tasks(slurm_msg_t *);\nstatic void _rpc_complete_batch(slurm_msg_t *);\nstatic void _rpc_terminate_tasks(slurm_msg_t *);\nstatic void _rpc_timelimit(slurm_msg_t *);\nstatic void _rpc_reattach_tasks(slurm_msg_t *);\nstatic void _rpc_signal_job(slurm_msg_t *);\nstatic void _rpc_suspend_job(slurm_msg_t *msg);\nstatic void _rpc_terminate_job(slurm_msg_t *);\nstatic void _rpc_update_time(slurm_msg_t *);\nstatic void _rpc_shutdown(slurm_msg_t *msg);\nstatic void _rpc_reconfig(slurm_msg_t *msg);\nstatic void _rpc_reboot(slurm_msg_t *msg);\nstatic void _rpc_pid2jid(slurm_msg_t *msg);\nstatic int  _rpc_file_bcast(slurm_msg_t *msg);\nstatic void _file_bcast_cleanup(void);\nstatic int  _file_bcast_register_file(slurm_msg_t *msg,\n\t\t\t\t      file_bcast_info_t *key);\nstatic int  _rpc_ping(slurm_msg_t *);\nstatic int  _rpc_health_check(slurm_msg_t *);\nstatic int  _rpc_acct_gather_update(slurm_msg_t *);\nstatic int  _rpc_acct_gather_energy(slurm_msg_t *);\nstatic int  _rpc_step_complete(slurm_msg_t *msg);\nstatic int  _rpc_step_complete_aggr(slurm_msg_t *msg);\nstatic int  _rpc_stat_jobacct(slurm_msg_t *msg);\nstatic int  _rpc_list_pids(slurm_msg_t *msg);\nstatic int  _rpc_daemon_status(slurm_msg_t *msg);\nstatic int  _run_epilog(job_env_t *job_env);\nstatic int  _run_prolog(job_env_t *job_env, slurm_cred_t *cred);\nstatic void _rpc_forward_data(slurm_msg_t *msg);\nstatic int  _rpc_network_callerid(slurm_msg_t *msg);\nstatic void _dealloc_gids(gids_t *p);\n\n\nstatic bool _pause_for_job_completion(uint32_t jobid, char *nodes,\n\t\tint maxtime);\nstatic bool _slurm_authorized_user(uid_t uid);\nstatic void _sync_messages_kill(kill_job_msg_t *req);\nstatic int  _waiter_init (uint32_t jobid);\nstatic int  _waiter_complete (uint32_t jobid);\n\nstatic void _send_back_fd(int socket, int fd);\nstatic bool _steps_completed_now(uint32_t jobid);\nstatic int  _valid_sbcast_cred(file_bcast_msg_t *req, uid_t req_uid,\n\t\t\t       uint16_t block_no, uint32_t *job_id);\nstatic void _wait_state_completed(uint32_t jobid, int max_delay);\nstatic uid_t _get_job_uid(uint32_t jobid);\n\nstatic gids_t *_gids_cache_lookup(char *user, gid_t gid);\n\nstatic int  _add_starting_step(uint16_t type, void *req);\nstatic int  _remove_starting_step(uint16_t type, void *req);\nstatic int  _compare_starting_steps(void *s0, void *s1);\nstatic int  _wait_for_starting_step(uint32_t job_id, uint32_t step_id);\nstatic bool _step_is_starting(uint32_t job_id, uint32_t step_id);\n\nstatic void _add_job_running_prolog(uint32_t job_id);\nstatic void _remove_job_running_prolog(uint32_t job_id);\nstatic int  _match_jobid(void *s0, void *s1);\nstatic void _wait_for_job_running_prolog(uint32_t job_id);\nstatic bool _requeue_setup_env_fail(void);\n\n/*\n *  List of threads waiting for jobs to complete\n */\nstatic List waiters;\n\nstatic pthread_mutex_t launch_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic time_t startup = 0;\t\t/* daemon startup time */\nstatic time_t last_slurmctld_msg = 0;\n\nstatic pthread_mutex_t job_limits_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic List job_limits_list = NULL;\nstatic bool job_limits_loaded = false;\n\n#define FINI_JOB_CNT 32\nstatic pthread_mutex_t fini_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic uint32_t fini_job_id[FINI_JOB_CNT];\nstatic int next_fini_job_inx = 0;\n\n/* NUM_PARALLEL_SUSP_JOBS controls the number of jobs that can be suspended or\n * resumed at one time. */\n#define NUM_PARALLEL_SUSP_JOBS 64\n/* NUM_PARALLEL_SUSP_STEPS controls the number of steps per job that can be\n * suspended at one time. */\n#define NUM_PARALLEL_SUSP_STEPS 8\nstatic pthread_mutex_t suspend_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic uint32_t job_suspend_array[NUM_PARALLEL_SUSP_JOBS];\nstatic int job_suspend_size = 0;\n\n#define JOB_STATE_CNT 64\nstatic pthread_mutex_t job_state_mutex   = PTHREAD_MUTEX_INITIALIZER;\nstatic pthread_cond_t  job_state_cond    = PTHREAD_COND_INITIALIZER;\nstatic uint32_t active_job_id[JOB_STATE_CNT];\n\nstatic pthread_mutex_t prolog_mutex = PTHREAD_MUTEX_INITIALIZER;\n\n#define FILE_BCAST_TIMEOUT 300\nstatic pthread_mutex_t file_bcast_mutex = PTHREAD_MUTEX_INITIALIZER;\nstatic pthread_cond_t  file_bcast_cond  = PTHREAD_COND_INITIALIZER;\nstatic int fb_read_lock = 0, fb_write_wait_lock = 0, fb_write_lock = 0;\nstatic List file_bcast_list = NULL;\n\nvoid\nslurmd_req(slurm_msg_t *msg)\n{\n\tint rc;\n\n\tif (msg == NULL) {\n\t\tif (startup == 0)\n\t\t\tstartup = time(NULL);\n\t\tFREE_NULL_LIST(waiters);\n\t\tslurm_mutex_lock(&job_limits_mutex);\n\t\tif (job_limits_list) {\n\t\t\tFREE_NULL_LIST(job_limits_list);\n\t\t\tjob_limits_loaded = false;\n\t\t}\n\t\tslurm_mutex_unlock(&job_limits_mutex);\n\t\treturn;\n\t}\n\n\tswitch (msg->msg_type) {\n\tcase REQUEST_LAUNCH_PROLOG:\n\t\tdebug2(\"Processing RPC: REQUEST_LAUNCH_PROLOG\");\n\t\t_rpc_prolog(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_BATCH_JOB_LAUNCH:\n\t\tdebug2(\"Processing RPC: REQUEST_BATCH_JOB_LAUNCH\");\n\t\t/* Mutex locking moved into _rpc_batch_job() due to\n\t\t * very slow prolog on Blue Gene system. Only batch\n\t\t * jobs are supported on Blue Gene (no job steps). */\n\t\t_rpc_batch_job(msg, true);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_LAUNCH_TASKS:\n\t\tdebug2(\"Processing RPC: REQUEST_LAUNCH_TASKS\");\n\t\tslurm_mutex_lock(&launch_mutex);\n\t\t_rpc_launch_tasks(msg);\n\t\tslurm_mutex_unlock(&launch_mutex);\n\t\tbreak;\n\tcase REQUEST_SIGNAL_TASKS:\n\t\tdebug2(\"Processing RPC: REQUEST_SIGNAL_TASKS\");\n\t\t_rpc_signal_tasks(msg);\n\t\tbreak;\n\tcase REQUEST_CHECKPOINT_TASKS:\n\t\tdebug2(\"Processing RPC: REQUEST_CHECKPOINT_TASKS\");\n\t\t_rpc_checkpoint_tasks(msg);\n\t\tbreak;\n\tcase REQUEST_TERMINATE_TASKS:\n\t\tdebug2(\"Processing RPC: REQUEST_TERMINATE_TASKS\");\n\t\t_rpc_terminate_tasks(msg);\n\t\tbreak;\n\tcase REQUEST_KILL_PREEMPTED:\n\t\tdebug2(\"Processing RPC: REQUEST_KILL_PREEMPTED\");\n\t\tlast_slurmctld_msg = time(NULL);\n\t\t_rpc_timelimit(msg);\n\t\tbreak;\n\tcase REQUEST_KILL_TIMELIMIT:\n\t\tdebug2(\"Processing RPC: REQUEST_KILL_TIMELIMIT\");\n\t\tlast_slurmctld_msg = time(NULL);\n\t\t_rpc_timelimit(msg);\n\t\tbreak;\n\tcase REQUEST_REATTACH_TASKS:\n\t\tdebug2(\"Processing RPC: REQUEST_REATTACH_TASKS\");\n\t\t_rpc_reattach_tasks(msg);\n\t\tbreak;\n\tcase REQUEST_SIGNAL_JOB:\n\t\tdebug2(\"Processing RPC: REQUEST_SIGNAL_JOB\");\n\t\t_rpc_signal_job(msg);\n\t\tbreak;\n\tcase REQUEST_SUSPEND_INT:\n\t\tdebug2(\"Processing RPC: REQUEST_SUSPEND_INT\");\n\t\t_rpc_suspend_job(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_ABORT_JOB:\n\t\tdebug2(\"Processing RPC: REQUEST_ABORT_JOB\");\n\t\tlast_slurmctld_msg = time(NULL);\n\t\t_rpc_abort_job(msg);\n\t\tbreak;\n\tcase REQUEST_TERMINATE_JOB:\n\t\tdebug2(\"Processing RPC: REQUEST_TERMINATE_JOB\");\n\t\tlast_slurmctld_msg = time(NULL);\n\t\t_rpc_terminate_job(msg);\n\t\tbreak;\n\tcase REQUEST_COMPLETE_BATCH_SCRIPT:\n\t\tdebug2(\"Processing RPC: REQUEST_COMPLETE_BATCH_SCRIPT\");\n\t\t_rpc_complete_batch(msg);\n\t\tbreak;\n\tcase REQUEST_UPDATE_JOB_TIME:\n\t\tdebug2(\"Processing RPC: REQUEST_UPDATE_JOB_TIME\");\n\t\t_rpc_update_time(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_SHUTDOWN:\n\t\tdebug2(\"Processing RPC: REQUEST_SHUTDOWN\");\n\t\t_rpc_shutdown(msg);\n\t\tbreak;\n\tcase REQUEST_RECONFIGURE:\n\t\tdebug2(\"Processing RPC: REQUEST_RECONFIGURE\");\n\t\t_rpc_reconfig(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_REBOOT_NODES:\n\t\tdebug2(\"Processing RPC: REQUEST_REBOOT_NODES\");\n\t\t_rpc_reboot(msg);\n\t\tbreak;\n\tcase REQUEST_NODE_REGISTRATION_STATUS:\n\t\tdebug2(\"Processing RPC: REQUEST_NODE_REGISTRATION_STATUS\");\n\t\t/* Treat as ping (for slurmctld agent, just return SUCCESS) */\n\t\trc = _rpc_ping(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\t/* Then initiate a separate node registration */\n\t\tif (rc == SLURM_SUCCESS)\n\t\t\tsend_registration_msg(SLURM_SUCCESS, true);\n\t\tbreak;\n\tcase REQUEST_PING:\n\t\t_rpc_ping(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_HEALTH_CHECK:\n\t\tdebug2(\"Processing RPC: REQUEST_HEALTH_CHECK\");\n\t\t_rpc_health_check(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_ACCT_GATHER_UPDATE:\n\t\tdebug2(\"Processing RPC: REQUEST_ACCT_GATHER_UPDATE\");\n\t\t_rpc_acct_gather_update(msg);\n\t\tlast_slurmctld_msg = time(NULL);\n\t\tbreak;\n\tcase REQUEST_ACCT_GATHER_ENERGY:\n\t\tdebug2(\"Processing RPC: REQUEST_ACCT_GATHER_ENERGY\");\n\t\t_rpc_acct_gather_energy(msg);\n\t\tbreak;\n\tcase REQUEST_JOB_ID:\n\t\t_rpc_pid2jid(msg);\n\t\tbreak;\n\tcase REQUEST_FILE_BCAST:\n\t\trc = _rpc_file_bcast(msg);\n\t\tslurm_send_rc_msg(msg, rc);\n\t\tbreak;\n\tcase REQUEST_STEP_COMPLETE:\n\t\t(void) _rpc_step_complete(msg);\n\t\tbreak;\n\tcase REQUEST_STEP_COMPLETE_AGGR:\n\t\t(void) _rpc_step_complete_aggr(msg);\n\t\tbreak;\n\tcase REQUEST_JOB_STEP_STAT:\n\t\t(void) _rpc_stat_jobacct(msg);\n\t\tbreak;\n\tcase REQUEST_JOB_STEP_PIDS:\n\t\t(void) _rpc_list_pids(msg);\n\t\tbreak;\n\tcase REQUEST_DAEMON_STATUS:\n\t\t_rpc_daemon_status(msg);\n\t\tbreak;\n\tcase REQUEST_JOB_NOTIFY:\n\t\t_rpc_job_notify(msg);\n\t\tbreak;\n\tcase REQUEST_FORWARD_DATA:\n\t\t_rpc_forward_data(msg);\n\t\tbreak;\n\tcase REQUEST_NETWORK_CALLERID:\n\t\tdebug2(\"Processing RPC: REQUEST_NETWORK_CALLERID\");\n\t\t_rpc_network_callerid(msg);\n\t\tbreak;\n\tcase MESSAGE_COMPOSITE:\n\t\terror(\"Processing RPC: MESSAGE_COMPOSITE: \"\n\t\t      \"This should never happen\");\n\t\tmsg_aggr_add_msg(msg, 0, NULL);\n\t\tbreak;\n\tcase RESPONSE_MESSAGE_COMPOSITE:\n\t\tdebug2(\"Processing RPC: RESPONSE_MESSAGE_COMPOSITE\");\n\t\tmsg_aggr_resp(msg);\n\t\tbreak;\n\tdefault:\n\t\terror(\"slurmd_req: invalid request msg type %d\",\n\t\t      msg->msg_type);\n\t\tslurm_send_rc_msg(msg, EINVAL);\n\t\tbreak;\n\t}\n\treturn;\n}\nstatic int _send_slurmd_conf_lite (int fd, slurmd_conf_t *cf)\n{\n\tint len;\n\tBuf buffer = init_buf(0);\n\tslurm_mutex_lock(&cf->config_mutex);\n\tpack_slurmd_conf_lite(cf, buffer);\n\tslurm_mutex_unlock(&cf->config_mutex);\n\tlen = get_buf_offset(buffer);\n\tsafe_write(fd, &len, sizeof(int));\n\tsafe_write(fd, get_buf_data(buffer), len);\n\tfree_buf(buffer);\n\treturn (0);\n rwfail:\n\treturn (-1);\n}\n\nstatic int\n_send_slurmstepd_init(int fd, int type, void *req,\n\t\t      slurm_addr_t *cli, slurm_addr_t *self,\n\t\t      hostset_t step_hset, uint16_t protocol_version)\n{\n\tint len = 0;\n\tBuf buffer = NULL;\n\tslurm_msg_t msg;\n\tuid_t uid = (uid_t)-1;\n\tgid_t gid = (uid_t)-1;\n\tgids_t *gids = NULL;\n\n\tint rank, proto;\n\tint parent_rank, children, depth, max_depth;\n\tchar *parent_alias = NULL;\n\tchar *user_name = NULL;\n\tslurm_addr_t parent_addr = {0};\n\tchar pwd_buffer[PW_BUF_SIZE];\n\tstruct passwd pwd, *pwd_result;\n\n\tslurm_msg_t_init(&msg);\n\t/* send type over to slurmstepd */\n\tsafe_write(fd, &type, sizeof(int));\n\n\t/* step_hset can be NULL for batch scripts OR if the job was submitted\n\t * by SlurmUser or root using the --no-allocate/-Z option and the job\n\t * job credential validation by _check_job_credential() failed. If the\n\t * job credential did not validate, then it did not come from slurmctld\n\t * and there is no reason to send step completion messages to slurmctld.\n\t */\n\tif (step_hset == NULL) {\n\t\tbool send_error = false;\n\t\tif (type == LAUNCH_TASKS) {\n\t\t\tlaunch_tasks_request_msg_t *launch_req;\n\t\t\tlaunch_req = (launch_tasks_request_msg_t *) req;\n\t\t\tif (launch_req->job_step_id != SLURM_EXTERN_CONT)\n\t\t\t\tsend_error = true;\n\t\t}\n\t\tif (send_error) {\n\t\t\tinfo(\"task rank unavailable due to invalid job \"\n\t\t\t     \"credential, step completion RPC impossible\");\n\t\t}\n\t\trank = -1;\n\t\tparent_rank = -1;\n\t\tchildren = 0;\n\t\tdepth = 0;\n\t\tmax_depth = 0;\n\t} else if ((type == LAUNCH_TASKS) &&\n\t\t   (((launch_tasks_request_msg_t *)req)->alias_list)) {\n\t\t/* In the cloud, each task talks directly to the slurmctld\n\t\t * since node addressing is abnormal */\n\t\trank = 0;\n\t\tparent_rank = -1;\n\t\tchildren = 0;\n\t\tdepth = 0;\n\t\tmax_depth = 0;\n\t} else {\n#ifndef HAVE_FRONT_END\n\t\tint count;\n\t\tcount = hostset_count(step_hset);\n\t\trank = hostset_find(step_hset, conf->node_name);\n\t\treverse_tree_info(rank, count, REVERSE_TREE_WIDTH,\n\t\t\t\t  &parent_rank, &children,\n\t\t\t\t  &depth, &max_depth);\n\t\tif (rank > 0) { /* rank 0 talks directly to the slurmctld */\n\t\t\tint rc;\n\t\t\t/* Find the slurm_addr_t of this node's parent slurmd\n\t\t\t * in the step host list */\n\t\t\tparent_alias = hostset_nth(step_hset, parent_rank);\n\t\t\trc = slurm_conf_get_addr(parent_alias, &parent_addr);\n\t\t\tif (rc != SLURM_SUCCESS) {\n\t\t\t\terror(\"Failed looking up address for \"\n\t\t\t\t      \"NodeName %s\", parent_alias);\n\t\t\t\t/* parent_rank = -1; */\n\t\t\t}\n\t\t}\n#else\n\t\t/* In FRONT_END mode, one slurmd pretends to be all\n\t\t * NodeNames, so we can't compare conf->node_name\n\t\t * to the NodeNames in step_hset.  Just send step complete\n\t\t * RPC directly to the controller.\n\t\t */\n\t\trank = 0;\n\t\tparent_rank = -1;\n\t\tchildren = 0;\n\t\tdepth = 0;\n\t\tmax_depth = 0;\n#endif\n\t}\n\tdebug3(\"slurmstepd rank %d (%s), parent rank %d (%s), \"\n\t       \"children %d, depth %d, max_depth %d\",\n\t       rank, conf->node_name,\n\t       parent_rank, parent_alias ? parent_alias : \"NONE\",\n\t       children, depth, max_depth);\n\tif (parent_alias)\n\t\tfree(parent_alias);\n\n\t/* send reverse-tree info to the slurmstepd */\n\tsafe_write(fd, &rank, sizeof(int));\n\tsafe_write(fd, &parent_rank, sizeof(int));\n\tsafe_write(fd, &children, sizeof(int));\n\tsafe_write(fd, &depth, sizeof(int));\n\tsafe_write(fd, &max_depth, sizeof(int));\n\tsafe_write(fd, &parent_addr, sizeof(slurm_addr_t));\n\n\t/* send conf over to slurmstepd */\n\tif (_send_slurmd_conf_lite(fd, conf) < 0)\n\t\tgoto rwfail;\n\n\t/* send cli address over to slurmstepd */\n\tbuffer = init_buf(0);\n\tslurm_pack_slurm_addr(cli, buffer);\n\tlen = get_buf_offset(buffer);\n\tsafe_write(fd, &len, sizeof(int));\n\tsafe_write(fd, get_buf_data(buffer), len);\n\tfree_buf(buffer);\n\tbuffer = NULL;\n\n\t/* send self address over to slurmstepd */\n\tif (self) {\n\t\tbuffer = init_buf(0);\n\t\tslurm_pack_slurm_addr(self, buffer);\n\t\tlen = get_buf_offset(buffer);\n\t\tsafe_write(fd, &len, sizeof(int));\n\t\tsafe_write(fd, get_buf_data(buffer), len);\n\t\tfree_buf(buffer);\n\t\tbuffer = NULL;\n\n\t} else {\n\t\tlen = 0;\n\t\tsafe_write(fd, &len, sizeof(int));\n\t}\n\n\t/* Send GRES information to slurmstepd */\n\tgres_plugin_send_stepd(fd);\n\n\t/* send cpu_frequency info to slurmstepd */\n\tcpu_freq_send_info(fd);\n\n\t/* send req over to slurmstepd */\n\tswitch(type) {\n\tcase LAUNCH_BATCH_JOB:\n\t\tgid = (uid_t)((batch_job_launch_msg_t *)req)->gid;\n\t\tuid = (uid_t)((batch_job_launch_msg_t *)req)->uid;\n\t\tuser_name = ((batch_job_launch_msg_t *)req)->user_name;\n\t\tmsg.msg_type = REQUEST_BATCH_JOB_LAUNCH;\n\t\tbreak;\n\tcase LAUNCH_TASKS:\n\t\t/*\n\t\t * The validity of req->uid was verified against the\n\t\t * auth credential in _rpc_launch_tasks().  req->gid\n\t\t * has NOT yet been checked!\n\t\t */\n\t\tgid = (uid_t)((launch_tasks_request_msg_t *)req)->gid;\n\t\tuid = (uid_t)((launch_tasks_request_msg_t *)req)->uid;\n\t\tuser_name = ((launch_tasks_request_msg_t *)req)->user_name;\n\t\tmsg.msg_type = REQUEST_LAUNCH_TASKS;\n\t\tbreak;\n\tdefault:\n\t\terror(\"Was sent a task I didn't understand\");\n\t\tbreak;\n\t}\n\tbuffer = init_buf(0);\n\tmsg.data = req;\n\n\tif (protocol_version == (uint16_t)NO_VAL)\n\t\tproto = SLURM_PROTOCOL_VERSION;\n\telse\n\t\tproto = protocol_version;\n\n\tmsg.protocol_version = (uint16_t)proto;\n\tpack_msg(&msg, buffer);\n\tlen = get_buf_offset(buffer);\n\n\tsafe_write(fd, &proto, sizeof(int));\n\n\tsafe_write(fd, &len, sizeof(int));\n\tsafe_write(fd, get_buf_data(buffer), len);\n\tfree_buf(buffer);\n\tbuffer = NULL;\n\n#ifdef HAVE_NATIVE_CRAY\n\t/* Try to avoid calling this on a system which is a native\n\t * cray.  getpwuid_r is slow on the compute nodes and this has\n\t * in theory been verified earlier.\n\t */\n\tif (!user_name) {\n#endif\n\t\t/* send cached group ids array for the relevant uid */\n\t\tdebug3(\"_send_slurmstepd_init: call to getpwuid_r\");\n\t\tif (slurm_getpwuid_r(uid, &pwd, pwd_buffer, PW_BUF_SIZE,\n\t\t\t\t     &pwd_result) || (pwd_result == NULL)) {\n\t\t\terror(\"%s: getpwuid_r: %m\", __func__);\n\t\t\tlen = 0;\n\t\t\tsafe_write(fd, &len, sizeof(int));\n\t\t\terrno = ESLURMD_UID_NOT_FOUND;\n\t\t\treturn errno;\n\t\t}\n\t\tdebug3(\"%s: return from getpwuid_r\", __func__);\n\t\tif (gid != pwd_result->pw_gid) {\n\t\t\tdebug(\"%s: Changing gid from %d to %d\",\n\t\t\t      __func__, gid, pwd_result->pw_gid);\n\t\t}\n\t\tgid = pwd_result->pw_gid;\n\t\tif (!user_name)\n\t\t\tuser_name = pwd_result->pw_name;\n#ifdef HAVE_NATIVE_CRAY\n\t}\n#endif\n\tif (!user_name) {\n\t\t/* Sanity check since gids_cache_lookup will fail\n\t\t * with a NULL. */\n\t\terror(\"%s: No user name for %d: %m\", __func__, uid);\n\t\tlen = 0;\n\t\tsafe_write(fd, &len, sizeof(int));\n\t\terrno = ESLURMD_UID_NOT_FOUND;\n\t\treturn errno;\n\t}\n\n\tif ((gids = _gids_cache_lookup(user_name, gid))) {\n\t\tint i;\n\t\tuint32_t tmp32;\n\t\tsafe_write(fd, &gids->ngids, sizeof(int));\n\t\tfor (i = 0; i < gids->ngids; i++) {\n\t\t\ttmp32 = (uint32_t)gids->gids[i];\n\t\t\tsafe_write(fd, &tmp32, sizeof(uint32_t));\n\t\t}\n\t\t_dealloc_gids(gids);\n\t} else {\n\t\tlen = 0;\n\t\tsafe_write(fd, &len, sizeof(int));\n\t}\n\treturn 0;\n\nrwfail:\n\tif (buffer)\n\t\tfree_buf(buffer);\n\terror(\"_send_slurmstepd_init failed\");\n\treturn errno;\n}\n\n\n/*\n * Fork and exec the slurmstepd, then send the slurmstepd its\n * initialization data.  Then wait for slurmstepd to send an \"ok\"\n * message before returning.  When the \"ok\" message is received,\n * the slurmstepd has created and begun listening on its unix\n * domain socket.\n *\n * Note that this code forks twice and it is the grandchild that\n * becomes the slurmstepd process, so the slurmstepd's parent process\n * will be init, not slurmd.\n */\nstatic int\n_forkexec_slurmstepd(uint16_t type, void *req,\n\t\t     slurm_addr_t *cli, slurm_addr_t *self,\n\t\t     const hostset_t step_hset, uint16_t protocol_version)\n{\n\tpid_t pid;\n\tint to_stepd[2] = {-1, -1};\n\tint to_slurmd[2] = {-1, -1};\n\n\tif (pipe(to_stepd) < 0 || pipe(to_slurmd) < 0) {\n\t\terror(\"_forkexec_slurmstepd pipe failed: %m\");\n\t\treturn SLURM_FAILURE;\n\t}\n\n\tif (_add_starting_step(type, req)) {\n\t\terror(\"_forkexec_slurmstepd failed in _add_starting_step: %m\");\n\t\treturn SLURM_FAILURE;\n\t}\n\n\tif ((pid = fork()) < 0) {\n\t\terror(\"_forkexec_slurmstepd: fork: %m\");\n\t\tclose(to_stepd[0]);\n\t\tclose(to_stepd[1]);\n\t\tclose(to_slurmd[0]);\n\t\tclose(to_slurmd[1]);\n\t\t_remove_starting_step(type, req);\n\t\treturn SLURM_FAILURE;\n\t} else if (pid > 0) {\n\t\tint rc = SLURM_SUCCESS;\n#if (SLURMSTEPD_MEMCHECK == 0)\n\t\tint i;\n\t\ttime_t start_time = time(NULL);\n#endif\n\t\t/*\n\t\t * Parent sends initialization data to the slurmstepd\n\t\t * over the to_stepd pipe, and waits for the return code\n\t\t * reply on the to_slurmd pipe.\n\t\t */\n\t\tif (close(to_stepd[0]) < 0)\n\t\t\terror(\"Unable to close read to_stepd in parent: %m\");\n\t\tif (close(to_slurmd[1]) < 0)\n\t\t\terror(\"Unable to close write to_slurmd in parent: %m\");\n\n\t\tif ((rc = _send_slurmstepd_init(to_stepd[1], type,\n\t\t\t\t\t\treq, cli, self,\n\t\t\t\t\t\tstep_hset,\n\t\t\t\t\t\tprotocol_version)) != 0) {\n\t\t\terror(\"Unable to init slurmstepd\");\n\t\t\tgoto done;\n\t\t}\n\n\t\t/* If running under valgrind/memcheck, this pipe doesn't work\n\t\t * correctly so just skip it. */\n#if (SLURMSTEPD_MEMCHECK == 0)\n\t\ti = read(to_slurmd[0], &rc, sizeof(int));\n\t\tif (i < 0) {\n\t\t\terror(\"%s: Can not read return code from slurmstepd \"\n\t\t\t      \"got %d: %m\", __func__, i);\n\t\t\trc = SLURM_FAILURE;\n\t\t} else if (i != sizeof(int)) {\n\t\t\terror(\"%s: slurmstepd failed to send return code \"\n\t\t\t      \"got %d: %m\", __func__, i);\n\t\t\trc = SLURM_FAILURE;\n\t\t} else {\n\t\t\tint delta_time = time(NULL) - start_time;\n\t\t\tint cc;\n\t\t\tif (delta_time > 5) {\n\t\t\t\tinfo(\"Warning: slurmstepd startup took %d sec, \"\n\t\t\t\t     \"possible file system problem or full \"\n\t\t\t\t     \"memory\", delta_time);\n\t\t\t}\n\t\t\tif (rc != SLURM_SUCCESS)\n\t\t\t\terror(\"slurmstepd return code %d\", rc);\n\n\t\t\tcc = SLURM_SUCCESS;\n\t\t\tcc = write(to_stepd[1], &cc, sizeof(int));\n\t\t\tif (cc != sizeof(int)) {\n\t\t\t\terror(\"%s: failed to send ack to stepd %d: %m\",\n\t\t\t\t      __func__, cc);\n\t\t\t}\n\t\t}\n#endif\n\tdone:\n\t\tif (_remove_starting_step(type, req))\n\t\t\terror(\"Error cleaning up starting_step list\");\n\n\t\t/* Reap child */\n\t\tif (waitpid(pid, NULL, 0) < 0)\n\t\t\terror(\"Unable to reap slurmd child process\");\n\t\tif (close(to_stepd[1]) < 0)\n\t\t\terror(\"close write to_stepd in parent: %m\");\n\t\tif (close(to_slurmd[0]) < 0)\n\t\t\terror(\"close read to_slurmd in parent: %m\");\n\t\treturn rc;\n\t} else {\n#if (SLURMSTEPD_MEMCHECK == 1)\n\t\t/* memcheck test of slurmstepd, option #1 */\n\t\tchar *const argv[3] = {\"memcheck\",\n\t\t\t\t       (char *)conf->stepd_loc, NULL};\n#elif (SLURMSTEPD_MEMCHECK == 2)\n\t\t/* valgrind test of slurmstepd, option #2 */\n\t\tuint32_t job_id = 0, step_id = 0;\n\t\tchar log_file[256];\n\t\tchar *const argv[13] = {\"valgrind\", \"--tool=memcheck\",\n\t\t\t\t\t\"--error-limit=no\",\n\t\t\t\t\t\"--leak-check=summary\",\n\t\t\t\t\t\"--show-reachable=yes\",\n\t\t\t\t\t\"--max-stackframe=16777216\",\n\t\t\t\t\t\"--num-callers=20\",\n\t\t\t\t\t\"--child-silent-after-fork=yes\",\n\t\t\t\t\t\"--track-origins=yes\",\n\t\t\t\t\tlog_file, (char *)conf->stepd_loc,\n\t\t\t\t\tNULL};\n\t\tif (type == LAUNCH_BATCH_JOB) {\n\t\t\tjob_id = ((batch_job_launch_msg_t *)req)->job_id;\n\t\t\tstep_id = ((batch_job_launch_msg_t *)req)->step_id;\n\t\t} else if (type == LAUNCH_TASKS) {\n\t\t\tjob_id = ((launch_tasks_request_msg_t *)req)->job_id;\n\t\t\tstep_id = ((launch_tasks_request_msg_t *)req)->job_step_id;\n\t\t}\n\t\tsnprintf(log_file, sizeof(log_file),\n\t\t\t \"--log-file=/tmp/slurmstepd_valgrind_%u.%u\",\n\t\t\t job_id, step_id);\n#elif (SLURMSTEPD_MEMCHECK == 3)\n\t\t/* valgrind/drd test of slurmstepd, option #3 */\n\t\tuint32_t job_id = 0, step_id = 0;\n\t\tchar log_file[256];\n\t\tchar *const argv[10] = {\"valgrind\", \"--tool=drd\",\n\t\t\t\t\t\"--error-limit=no\",\n\t\t\t\t\t\"--max-stackframe=16777216\",\n\t\t\t\t\t\"--num-callers=20\",\n\t\t\t\t\t\"--child-silent-after-fork=yes\",\n\t\t\t\t\tlog_file, (char *)conf->stepd_loc,\n\t\t\t\t\tNULL};\n\t\tif (type == LAUNCH_BATCH_JOB) {\n\t\t\tjob_id = ((batch_job_launch_msg_t *)req)->job_id;\n\t\t\tstep_id = ((batch_job_launch_msg_t *)req)->step_id;\n\t\t} else if (type == LAUNCH_TASKS) {\n\t\t\tjob_id = ((launch_tasks_request_msg_t *)req)->job_id;\n\t\t\tstep_id = ((launch_tasks_request_msg_t *)req)->job_step_id;\n\t\t}\n\t\tsnprintf(log_file, sizeof(log_file),\n\t\t\t \"--log-file=/tmp/slurmstepd_valgrind_%u.%u\",\n\t\t\t job_id, step_id);\n#elif (SLURMSTEPD_MEMCHECK == 4)\n\t\t/* valgrind/helgrind test of slurmstepd, option #4 */\n\t\tuint32_t job_id = 0, step_id = 0;\n\t\tchar log_file[256];\n\t\tchar *const argv[10] = {\"valgrind\", \"--tool=helgrind\",\n\t\t\t\t\t\"--error-limit=no\",\n\t\t\t\t\t\"--max-stackframe=16777216\",\n\t\t\t\t\t\"--num-callers=20\",\n\t\t\t\t\t\"--child-silent-after-fork=yes\",\n\t\t\t\t\tlog_file, (char *)conf->stepd_loc,\n\t\t\t\t\tNULL};\n\t\tif (type == LAUNCH_BATCH_JOB) {\n\t\t\tjob_id = ((batch_job_launch_msg_t *)req)->job_id;\n\t\t\tstep_id = ((batch_job_launch_msg_t *)req)->step_id;\n\t\t} else if (type == LAUNCH_TASKS) {\n\t\t\tjob_id = ((launch_tasks_request_msg_t *)req)->job_id;\n\t\t\tstep_id = ((launch_tasks_request_msg_t *)req)->job_step_id;\n\t\t}\n\t\tsnprintf(log_file, sizeof(log_file),\n\t\t\t \"--log-file=/tmp/slurmstepd_valgrind_%u.%u\",\n\t\t\t job_id, step_id);\n#else\n\t\t/* no memory checking, default */\n\t\tchar *const argv[2] = { (char *)conf->stepd_loc, NULL};\n#endif\n\t\tint i;\n\t\tint failed = 0;\n\t\t/* inform slurmstepd about our config */\n\t\tsetenv(\"SLURM_CONF\", conf->conffile, 1);\n\n\t\t/*\n\t\t * Child forks and exits\n\t\t */\n\t\tif (setsid() < 0) {\n\t\t\terror(\"_forkexec_slurmstepd: setsid: %m\");\n\t\t\tfailed = 1;\n\t\t}\n\t\tif ((pid = fork()) < 0) {\n\t\t\terror(\"_forkexec_slurmstepd: \"\n\t\t\t      \"Unable to fork grandchild: %m\");\n\t\t\tfailed = 2;\n\t\t} else if (pid > 0) { /* child */\n\t\t\texit(0);\n\t\t}\n\n\t\t/*\n\t\t * Just incase we (or someone we are linking to)\n\t\t * opened a file and didn't do a close on exec.  This\n\t\t * is needed mostly to protect us against libs we link\n\t\t * to that don't set the flag as we should already be\n\t\t * setting it for those that we open.  The number 256\n\t\t * is an arbitrary number based off test7.9.\n\t\t */\n\t\tfor (i=3; i<256; i++) {\n\t\t\t(void) fcntl(i, F_SETFD, FD_CLOEXEC);\n\t\t}\n\n\t\t/*\n\t\t * Grandchild exec's the slurmstepd\n\t\t *\n\t\t * If the slurmd is being shutdown/restarted before\n\t\t * the pipe happens the old conf->lfd could be reused\n\t\t * and if we close it the dup2 below will fail.\n\t\t */\n\t\tif ((to_stepd[0] != conf->lfd)\n\t\t    && (to_slurmd[1] != conf->lfd))\n\t\t\tslurm_shutdown_msg_engine(conf->lfd);\n\n\t\tif (close(to_stepd[1]) < 0)\n\t\t\terror(\"close write to_stepd in grandchild: %m\");\n\t\tif (close(to_slurmd[0]) < 0)\n\t\t\terror(\"close read to_slurmd in parent: %m\");\n\n\t\t(void) close(STDIN_FILENO); /* ignore return */\n\t\tif (dup2(to_stepd[0], STDIN_FILENO) == -1) {\n\t\t\terror(\"dup2 over STDIN_FILENO: %m\");\n\t\t\texit(1);\n\t\t}\n\t\tfd_set_close_on_exec(to_stepd[0]);\n\t\t(void) close(STDOUT_FILENO); /* ignore return */\n\t\tif (dup2(to_slurmd[1], STDOUT_FILENO) == -1) {\n\t\t\terror(\"dup2 over STDOUT_FILENO: %m\");\n\t\t\texit(1);\n\t\t}\n\t\tfd_set_close_on_exec(to_slurmd[1]);\n\t\t(void) close(STDERR_FILENO); /* ignore return */\n\t\tif (dup2(devnull, STDERR_FILENO) == -1) {\n\t\t\terror(\"dup2 /dev/null to STDERR_FILENO: %m\");\n\t\t\texit(1);\n\t\t}\n\t\tfd_set_noclose_on_exec(STDERR_FILENO);\n\t\tlog_fini();\n\t\tif (!failed) {\n\t\t\tif (conf->chos_loc && !access(conf->chos_loc, X_OK))\n\t\t\t\texecvp(conf->chos_loc, argv);\n\t\t\telse\n\t\t\t\texecvp(argv[0], argv);\n\t\t\terror(\"exec of slurmstepd failed: %m\");\n\t\t}\n\t\texit(2);\n\t}\n}\n\n\n/*\n * The job(step) credential is the only place to get a definitive\n * list of the nodes allocated to a job step.  We need to return\n * a hostset_t of the nodes. Validate the incoming RPC, updating\n * job_mem needed.\n */\nstatic int\n_check_job_credential(launch_tasks_request_msg_t *req, uid_t uid,\n\t\t      int node_id, hostset_t *step_hset,\n\t\t      uint16_t protocol_version)\n{\n\tslurm_cred_arg_t arg;\n\thostset_t\ts_hset = NULL;\n\tbool\t\tuser_ok = _slurm_authorized_user(uid);\n\tbool\t\tverified = true;\n\tint\t\thost_index = -1;\n\tint\t\trc;\n\tslurm_cred_t    *cred = req->cred;\n\tuint32_t\tjobid = req->job_id;\n\tuint32_t\tstepid = req->job_step_id;\n\tint\t\ttasks_to_launch = req->tasks_to_launch[node_id];\n\tuint32_t\tjob_cpus = 0, step_cpus = 0;\n\n\t/*\n\t * First call slurm_cred_verify() so that all valid\n\t * credentials are checked\n\t */\n\trc = slurm_cred_verify(conf->vctx, cred, &arg, protocol_version);\n\tif (rc < 0) {\n\t\tverified = false;\n\t\tif ((!user_ok) || (errno != ESLURMD_INVALID_JOB_CREDENTIAL))\n\t\t\treturn SLURM_ERROR;\n\t\telse {\n\t\t\tdebug(\"_check_job_credential slurm_cred_verify failed:\"\n\t\t\t      \" %m, but continuing anyway.\");\n\t\t}\n\t}\n\n\t/* If uid is the SlurmUser or root and the credential is bad,\n\t * then do not attempt validating the credential */\n\tif (!verified) {\n\t\t*step_hset = NULL;\n\t\tif (rc >= 0) {\n\t\t\tif ((s_hset = hostset_create(arg.step_hostlist)))\n\t\t\t\t*step_hset = s_hset;\n\t\t\tslurm_cred_free_args(&arg);\n\t\t}\n\t\treturn SLURM_SUCCESS;\n\t}\n\n\tif ((arg.jobid != jobid) || (arg.stepid != stepid)) {\n\t\terror(\"job credential for %u.%u, expected %u.%u\",\n\t\t      arg.jobid, arg.stepid, jobid, stepid);\n\t\tgoto fail;\n\t}\n\n\tif (arg.uid != uid) {\n\t\terror(\"job credential created for uid %ld, expected %ld\",\n\t\t      (long) arg.uid, (long) uid);\n\t\tgoto fail;\n\t}\n\n\t/*\n\t * Check that credential is valid for this host\n\t */\n\tif (!(s_hset = hostset_create(arg.step_hostlist))) {\n\t\terror(\"Unable to parse credential hostlist: `%s'\",\n\t\t      arg.step_hostlist);\n\t\tgoto fail;\n\t}\n\n\tif (!hostset_within(s_hset, conf->node_name)) {\n\t\terror(\"Invalid job %u.%u credential for user %u: \"\n\t\t      \"host %s not in hostset %s\",\n\t\t      arg.jobid, arg.stepid, arg.uid,\n\t\t      conf->node_name, arg.step_hostlist);\n\t\tgoto fail;\n\t}\n\n\tif ((arg.job_nhosts > 0) && (tasks_to_launch > 0)) {\n\t\tuint32_t hi, i, i_first_bit=0, i_last_bit=0, j;\n\t\tbool cpu_log = slurm_get_debug_flags() & DEBUG_FLAG_CPU_BIND;\n\n#ifdef HAVE_FRONT_END\n\t\thost_index = 0;\t/* It is always 0 for front end systems */\n#else\n\t\thostset_t j_hset;\n\t\t/* Determine the CPU count based upon this node's index into\n\t\t * the _job's_ allocation (job's hostlist and core_bitmap) */\n\t\tif (!(j_hset = hostset_create(arg.job_hostlist))) {\n\t\t\terror(\"Unable to parse credential hostlist: `%s'\",\n\t\t\t      arg.job_hostlist);\n\t\t\tgoto fail;\n\t\t}\n\t\thost_index = hostset_find(j_hset, conf->node_name);\n\t\thostset_destroy(j_hset);\n\n\t\tif ((host_index < 0) || (host_index >= arg.job_nhosts)) {\n\t\t\terror(\"job cr credential invalid host_index %d for \"\n\t\t\t      \"job %u\", host_index, arg.jobid);\n\t\t\tgoto fail;\n\t\t}\n#endif\n\n\t\tif (cpu_log) {\n\t\t\tchar *per_job = \"\", *per_step = \"\";\n\t\t\tuint32_t job_mem  = arg.job_mem_limit;\n\t\t\tuint32_t step_mem = arg.step_mem_limit;\n\t\t\tif (job_mem & MEM_PER_CPU) {\n\t\t\t\tjob_mem &= (~MEM_PER_CPU);\n\t\t\t\tper_job = \"_per_CPU\";\n\t\t\t}\n\t\t\tif (step_mem & MEM_PER_CPU) {\n\t\t\t\tstep_mem &= (~MEM_PER_CPU);\n\t\t\t\tper_step = \"_per_CPU\";\n\t\t\t}\n\t\t\tinfo(\"====================\");\n\t\t\tinfo(\"step_id:%u.%u job_mem:%uMB%s step_mem:%uMB%s\",\n\t\t\t     arg.jobid, arg.stepid, job_mem, per_job,\n\t\t\t     step_mem, per_step);\n\t\t}\n\n\t\thi = host_index + 1;\t/* change from 0-origin to 1-origin */\n\t\tfor (i=0; hi; i++) {\n\t\t\tif (hi > arg.sock_core_rep_count[i]) {\n\t\t\t\ti_first_bit += arg.sockets_per_node[i] *\n\t\t\t\t\t       arg.cores_per_socket[i] *\n\t\t\t\t\t       arg.sock_core_rep_count[i];\n\t\t\t\thi -= arg.sock_core_rep_count[i];\n\t\t\t} else {\n\t\t\t\ti_first_bit += arg.sockets_per_node[i] *\n\t\t\t\t\t       arg.cores_per_socket[i] *\n\t\t\t\t\t       (hi - 1);\n\t\t\t\ti_last_bit = i_first_bit +\n\t\t\t\t\t     arg.sockets_per_node[i] *\n\t\t\t\t\t     arg.cores_per_socket[i];\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t/* Now count the allocated processors */\n\t\tfor (i=i_first_bit, j=0; i<i_last_bit; i++, j++) {\n\t\t\tchar *who_has = NULL;\n\t\t\tif (bit_test(arg.job_core_bitmap, i)) {\n\t\t\t\tjob_cpus++;\n\t\t\t\twho_has = \"Job\";\n\t\t\t}\n\t\t\tif (bit_test(arg.step_core_bitmap, i)) {\n\t\t\t\tstep_cpus++;\n\t\t\t\twho_has = \"Step\";\n\t\t\t}\n\t\t\tif (cpu_log && who_has) {\n\t\t\t\tinfo(\"JobNode[%u] CPU[%u] %s alloc\",\n\t\t\t\t     host_index, j, who_has);\n\t\t\t}\n\t\t}\n\t\tif (cpu_log)\n\t\t\tinfo(\"====================\");\n\t\tif (step_cpus == 0) {\n\t\t\terror(\"cons_res: zero processors allocated to step\");\n\t\t\tstep_cpus = 1;\n\t\t}\n\t\t/* NOTE: step_cpus is the count of allocated resources\n\t\t * (typically cores). Convert to CPU count as needed */\n\t\tif (i_last_bit <= i_first_bit)\n\t\t\terror(\"step credential has no CPUs selected\");\n\t\telse {\n\t\t\ti = conf->cpus / (i_last_bit - i_first_bit);\n\t\t\tif (i > 1) {\n\t\t\t\tif (cpu_log)\n\t\t\t\t\tinfo(\"Scaling CPU count by factor of \"\n\t\t\t\t\t     \"%d (%u/(%u-%u))\",\n\t\t\t\t\t     i, conf->cpus,\n\t\t\t\t\t     i_last_bit, i_first_bit);\n\t\t\t\tstep_cpus *= i;\n\t\t\t\tjob_cpus *= i;\n\t\t\t}\n\t\t}\n\t\tif (tasks_to_launch > step_cpus) {\n\t\t\t/* This is expected with the --overcommit option\n\t\t\t * or hyperthreads */\n\t\t\tdebug(\"cons_res: More than one tasks per logical \"\n\t\t\t      \"processor (%d > %u) on host [%u.%u %ld %s] \",\n\t\t\t      tasks_to_launch, step_cpus, arg.jobid,\n\t\t\t      arg.stepid, (long) arg.uid, arg.step_hostlist);\n\t\t}\n\t} else {\n\t\tstep_cpus = 1;\n\t\tjob_cpus  = 1;\n\t}\n\n\t/* Overwrite any memory limits in the RPC with contents of the\n\t * memory limit within the credential.\n\t * Reset the CPU count on this node to correct value. */\n\tif (arg.step_mem_limit) {\n\t\tif (arg.step_mem_limit & MEM_PER_CPU) {\n\t\t\treq->step_mem_lim  = arg.step_mem_limit &\n\t\t\t\t\t     (~MEM_PER_CPU);\n\t\t\treq->step_mem_lim *= step_cpus;\n\t\t} else\n\t\t\treq->step_mem_lim  = arg.step_mem_limit;\n\t} else {\n\t\tif (arg.job_mem_limit & MEM_PER_CPU) {\n\t\t\treq->step_mem_lim  = arg.job_mem_limit &\n\t\t\t\t\t     (~MEM_PER_CPU);\n\t\t\treq->step_mem_lim *= job_cpus;\n\t\t} else\n\t\t\treq->step_mem_lim  = arg.job_mem_limit;\n\t}\n\tif (arg.job_mem_limit & MEM_PER_CPU) {\n\t\treq->job_mem_lim  = arg.job_mem_limit & (~MEM_PER_CPU);\n\t\treq->job_mem_lim *= job_cpus;\n\t} else\n\t\treq->job_mem_lim  = arg.job_mem_limit;\n\treq->job_core_spec = arg.job_core_spec;\n\treq->node_cpus = step_cpus;\n#if 0\n\tinfo(\"%u.%u node_id:%d mem orig:%u cpus:%u limit:%u\",\n\t     jobid, stepid, node_id, arg.job_mem_limit,\n\t     step_cpus, req->job_mem_lim);\n#endif\n\n\t*step_hset = s_hset;\n\tslurm_cred_free_args(&arg);\n\treturn SLURM_SUCCESS;\n\n    fail:\n\tif (s_hset)\n\t\thostset_destroy(s_hset);\n\t*step_hset = NULL;\n\tslurm_cred_free_args(&arg);\n\tslurm_seterrno_ret(ESLURMD_INVALID_JOB_CREDENTIAL);\n}\n\n\nstatic void\n_rpc_launch_tasks(slurm_msg_t *msg)\n{\n\tint      errnum = SLURM_SUCCESS;\n\tuint16_t port;\n\tchar     host[MAXHOSTNAMELEN];\n\tuid_t    req_uid;\n\tlaunch_tasks_request_msg_t *req = msg->data;\n\tbool     super_user = false;\n#ifndef HAVE_FRONT_END\n\tbool     first_job_run;\n#endif\n\tslurm_addr_t self;\n\tslurm_addr_t *cli = &msg->orig_addr;\n\thostset_t step_hset = NULL;\n\tjob_mem_limits_t *job_limits_ptr;\n\tint nodeid = 0;\n#ifndef HAVE_FRONT_END\n\t/* It is always 0 for front end systems */\n\tnodeid = nodelist_find(req->complete_nodelist, conf->node_name);\n#endif\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tmemcpy(&req->orig_addr, &msg->orig_addr, sizeof(slurm_addr_t));\n\n\tsuper_user = _slurm_authorized_user(req_uid);\n\n\tif ((super_user == false) && (req_uid != req->uid)) {\n\t\terror(\"launch task request from uid %u\",\n\t\t      (unsigned int) req_uid);\n\t\terrnum = ESLURM_USER_ID_MISSING;\t/* or invalid user */\n\t\tgoto done;\n\t}\n\n\tslurm_get_ip_str(cli, &port, host, sizeof(host));\n\tinfo(\"launch task %u.%u request from %u.%u@%s (port %hu)\", req->job_id,\n\t     req->job_step_id, req->uid, req->gid, host, port);\n\n\t/* this could be set previously and needs to be overwritten by\n\t * this call for messages to work correctly for the new call */\n\tenv_array_overwrite(&req->env, \"SLURM_SRUN_COMM_HOST\", host);\n\treq->envc = envcount(req->env);\n\n#ifndef HAVE_FRONT_END\n\tslurm_mutex_lock(&prolog_mutex);\n\tfirst_job_run = !slurm_cred_jobid_cached(conf->vctx, req->job_id);\n#endif\n\tif (_check_job_credential(req, req_uid, nodeid, &step_hset,\n\t\t\t\t  msg->protocol_version) < 0) {\n\t\terrnum = errno;\n\t\terror(\"Invalid job credential from %ld@%s: %m\",\n\t\t      (long) req_uid, host);\n#ifndef HAVE_FRONT_END\n\t\tslurm_mutex_unlock(&prolog_mutex);\n#endif\n\t\tgoto done;\n\t}\n\n\t/* Must follow _check_job_credential(), which sets some req fields */\n\ttask_g_slurmd_launch_request(req->job_id, req, nodeid);\n\n#ifndef HAVE_FRONT_END\n\tif (first_job_run) {\n\t\tint rc;\n\t\tjob_env_t job_env;\n\n\t\tslurm_cred_insert_jobid(conf->vctx, req->job_id);\n\t\t_add_job_running_prolog(req->job_id);\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\n\t\tif (container_g_create(req->job_id))\n\t\t\terror(\"container_g_create(%u): %m\", req->job_id);\n\n\t\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\t\tjob_env.jobid = req->job_id;\n\t\tjob_env.step_id = req->job_step_id;\n\t\tjob_env.node_list = req->complete_nodelist;\n\t\tjob_env.partition = req->partition;\n\t\tjob_env.spank_job_env = req->spank_job_env;\n\t\tjob_env.spank_job_env_size = req->spank_job_env_size;\n\t\tjob_env.uid = req->uid;\n\t\tjob_env.user_name = req->user_name;\n\t\trc =  _run_prolog(&job_env, req->cred);\n\t\tif (rc) {\n\t\t\tint term_sig, exit_status;\n\t\t\tif (WIFSIGNALED(rc)) {\n\t\t\t\texit_status = 0;\n\t\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t\t} else {\n\t\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\t\tterm_sig    = 0;\n\t\t\t}\n\t\t\terror(\"[job %u] prolog failed status=%d:%d\",\n\t\t\t      req->job_id, exit_status, term_sig);\n\t\t\terrnum = ESLURMD_PROLOG_FAILED;\n\t\t\tgoto done;\n\t\t}\n\t\t/* Since the job could have been killed while the prolog was\n\t\t * running, test if the credential has since been revoked\n\t\t * and exit as needed. */\n\t\tif (slurm_cred_revoked(conf->vctx, req->cred)) {\n\t\t\tinfo(\"Job %u already killed, do not launch step %u.%u\",\n\t\t\t     req->job_id, req->job_id, req->job_step_id);\n\t\t\terrnum = ESLURMD_CREDENTIAL_REVOKED;\n\t\t\tgoto done;\n\t\t}\n\t} else {\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\t\t_wait_for_job_running_prolog(req->job_id);\n\t}\n#endif\n\n\tif (req->job_mem_lim || req->step_mem_lim) {\n\t\tstep_loc_t step_info;\n\t\tslurm_mutex_lock(&job_limits_mutex);\n\t\tif (!job_limits_list)\n\t\t\tjob_limits_list = list_create(_job_limits_free);\n\t\tstep_info.jobid  = req->job_id;\n\t\tstep_info.stepid = req->job_step_id;\n\t\tjob_limits_ptr = list_find_first (job_limits_list,\n\t\t\t\t\t\t  _step_limits_match,\n\t\t\t\t\t\t  &step_info);\n\t\tif (!job_limits_ptr) {\n\t\t\tjob_limits_ptr = xmalloc(sizeof(job_mem_limits_t));\n\t\t\tjob_limits_ptr->job_id   = req->job_id;\n\t\t\tjob_limits_ptr->job_mem  = req->job_mem_lim;\n\t\t\tjob_limits_ptr->step_id  = req->job_step_id;\n\t\t\tjob_limits_ptr->step_mem = req->step_mem_lim;\n#if _LIMIT_INFO\n\t\t\tinfo(\"AddLim step:%u.%u job_mem:%u step_mem:%u\",\n\t\t\t      job_limits_ptr->job_id, job_limits_ptr->step_id,\n\t\t\t      job_limits_ptr->job_mem,\n\t\t\t      job_limits_ptr->step_mem);\n#endif\n\t\t\tlist_append(job_limits_list, job_limits_ptr);\n\t\t}\n\t\tslurm_mutex_unlock(&job_limits_mutex);\n\t}\n\n\tslurm_get_stream_addr(msg->conn_fd, &self);\n\n\tdebug3(\"_rpc_launch_tasks: call to _forkexec_slurmstepd\");\n\terrnum = _forkexec_slurmstepd(LAUNCH_TASKS, (void *)req, cli, &self,\n\t\t\t\t      step_hset, msg->protocol_version);\n\tdebug3(\"_rpc_launch_tasks: return from _forkexec_slurmstepd\");\n\t_launch_complete_add(req->job_id);\n\n    done:\n\tif (step_hset)\n\t\thostset_destroy(step_hset);\n\n\tif (slurm_send_rc_msg(msg, errnum) < 0) {\n\t\tchar addr_str[32];\n\t\tslurm_print_slurm_addr(&msg->address, addr_str,\n\t\t\t\t       sizeof(addr_str));\n\t\terror(\"_rpc_launch_tasks: unable to send return code to \"\n\t\t      \"address:port=%s msg_type=%u: %m\",\n\t\t      addr_str, msg->msg_type);\n\n\t\t/*\n\t\t * Rewind credential so that srun may perform retry\n\t\t */\n\t\tslurm_cred_rewind(conf->vctx, req->cred); /* ignore errors */\n\n\t} else if (errnum == SLURM_SUCCESS) {\n\t\tsave_cred_state(conf->vctx);\n\t\ttask_g_slurmd_reserve_resources(req->job_id, req, nodeid);\n\t}\n\n\t/*\n\t *  If job prolog failed, indicate failure to slurmctld\n\t */\n\tif (errnum == ESLURMD_PROLOG_FAILED)\n\t\tsend_registration_msg(errnum, false);\n}\n\n/*\n * Open file based upon permissions of a different user\n * IN path_name - name of file to open\n * IN uid - User ID to use for file access check\n * IN gid - Group ID to use for file access check\n * RET -1 on error, file descriptor otherwise\n */\nstatic int _open_as_other(char *path_name, batch_job_launch_msg_t *req)\n{\n\tpid_t child;\n\tgids_t *gids;\n\tint pipe[2];\n\tint fd = -1, rc = 0;\n\n\tif (!(gids = _gids_cache_lookup(req->user_name, req->gid))) {\n\t\terror(\"%s: gids_cache_lookup for %s failed\",\n\t\t      __func__, req->user_name);\n\t\treturn -1;\n\t}\n\n\tif ((rc = container_g_create(req->job_id))) {\n\t\terror(\"%s: container_g_create(%u): %m\", __func__, req->job_id);\n\t\t_dealloc_gids(gids);\n\t\treturn -1;\n\t}\n\n\t/* child process will setuid to the user, register the process\n\t * with the container, and open the file for us. */\n\tif (socketpair(AF_UNIX, SOCK_DGRAM, 0, pipe) != 0) {\n\t\terror(\"%s: Failed to open pipe: %m\", __func__);\n\t\t_dealloc_gids(gids);\n\t\treturn -1;\n\t}\n\n\tchild = fork();\n\tif (child == -1) {\n\t\terror(\"%s: fork failure\", __func__);\n\t\t_dealloc_gids(gids);\n\t\tclose(pipe[0]);\n\t\tclose(pipe[1]);\n\t\treturn -1;\n\t} else if (child > 0) {\n\t\tclose(pipe[0]);\n\t\t(void) waitpid(child, &rc, 0);\n\t\t_dealloc_gids(gids);\n\t\tif (WIFEXITED(rc) && (WEXITSTATUS(rc) == 0))\n\t\t\tfd = _receive_fd(pipe[1]);\n\t\tclose(pipe[1]);\n\t\treturn fd;\n\t}\n\n\t/* child process below here */\n\n\tclose(pipe[1]);\n\n\t/* container_g_add_pid needs to be called in the\n\t * forked process part of the fork to avoid a race\n\t * condition where if this process makes a file or\n\t * detacts itself from a child before we add the pid\n\t * to the container in the parent of the fork. */\n\tif (container_g_add_pid(req->job_id, getpid(), req->uid)) {\n\t\terror(\"%s container_g_add_pid(%u): %m\", __func__, req->job_id);\n\t\texit(SLURM_ERROR);\n\t}\n\n\t/* The child actually performs the I/O and exits with\n\t * a return code, do not return! */\n\n\t/*********************************************************************\\\n\t * NOTE: It would be best to do an exec() immediately after the fork()\n\t * in order to help prevent a possible deadlock in the child process\n\t * due to locks being set at the time of the fork and being freed by\n\t * the parent process, but not freed by the child process. Performing\n\t * the work inline is done for simplicity. Note that the logging\n\t * performed by error() should be safe due to the use of\n\t * atfork_install_handlers() as defined in src/common/log.c.\n\t * Change the code below with caution.\n\t\\*********************************************************************/\n\n\tif (setgroups(gids->ngids, gids->gids) < 0) {\n\t\terror(\"%s: uid: %u setgroups failed: %m\", __func__, req->uid);\n\t\texit(errno);\n\t}\n\t_dealloc_gids(gids);\n\n\tif (setgid(req->gid) < 0) {\n\t\terror(\"%s: uid:%u setgid(%u): %m\", __func__, req->uid,req->gid);\n\t\texit(errno);\n\t}\n\tif (setuid(req->uid) < 0) {\n\t\terror(\"%s: getuid(%u): %m\", __func__, req->uid);\n\t\texit(errno);\n\t}\n\n\tfd = open(path_name, (O_CREAT|O_APPEND|O_WRONLY), 0644);\n\tif (fd == -1) {\n\t\terror(\"%s: uid:%u can't open `%s`: %m\",\n\t\t      __func__, req->uid, path_name);\n\t\texit(errno);\n\t}\n\t_send_back_fd(pipe[0], fd);\n\tclose(fd);\n\texit(SLURM_SUCCESS);\n}\n\nstatic void\n_prolog_error(batch_job_launch_msg_t *req, int rc)\n{\n\tchar *err_name_ptr, err_name[256], path_name[MAXPATHLEN];\n\tchar *fmt_char;\n\tint fd;\n\n\tif (req->std_err || req->std_out) {\n\t\tif (req->std_err)\n\t\t\tstrncpy(err_name, req->std_err, sizeof(err_name));\n\t\telse\n\t\t\tstrncpy(err_name, req->std_out, sizeof(err_name));\n\t\tif ((fmt_char = strchr(err_name, (int) '%')) &&\n\t\t    (fmt_char[1] == 'j') && !strchr(fmt_char+1, (int) '%')) {\n\t\t\tchar tmp_name[256];\n\t\t\tfmt_char[1] = 'u';\n\t\t\tsnprintf(tmp_name, sizeof(tmp_name), err_name,\n\t\t\t\t req->job_id);\n\t\t\tstrncpy(err_name, tmp_name, sizeof(err_name));\n\t\t}\n\t} else {\n\t\tsnprintf(err_name, sizeof(err_name), \"slurm-%u.out\",\n\t\t\t req->job_id);\n\t}\n\terr_name_ptr = err_name;\n\tif (err_name_ptr[0] == '/')\n\t\tsnprintf(path_name, MAXPATHLEN, \"%s\", err_name_ptr);\n\telse if (req->work_dir)\n\t\tsnprintf(path_name, MAXPATHLEN, \"%s/%s\",\n\t\t\treq->work_dir, err_name_ptr);\n\telse\n\t\tsnprintf(path_name, MAXPATHLEN, \"/%s\", err_name_ptr);\n\tif ((fd = _open_as_other(path_name, req)) == -1) {\n\t\terror(\"Unable to open %s: Permission denied\", path_name);\n\t\treturn;\n\t}\n\tsnprintf(err_name, sizeof(err_name),\n\t\t \"Error running slurm prolog: %d\\n\", WEXITSTATUS(rc));\n\tsafe_write(fd, err_name, strlen(err_name));\n\tif (fchown(fd, (uid_t) req->uid, (gid_t) req->gid) == -1) {\n\t\tsnprintf(err_name, sizeof(err_name),\n\t\t\t \"Couldn't change fd owner to %u:%u: %m\\n\",\n\t\t\t req->uid, req->gid);\n\t}\nrwfail:\n\tclose(fd);\n}\n\n/* load the user's environment on this machine if requested\n * SLURM_GET_USER_ENV environment variable is set */\nstatic int\n_get_user_env(batch_job_launch_msg_t *req)\n{\n\tstruct passwd pwd, *pwd_ptr = NULL;\n\tchar pwd_buf[PW_BUF_SIZE];\n\tchar **new_env;\n\tint i;\n\tstatic time_t config_update = 0;\n\tstatic bool no_env_cache = false;\n\n\tif (config_update != conf->last_update) {\n\t\tchar *sched_params = slurm_get_sched_params();\n\t\tno_env_cache = (sched_params &&\n\t\t\t\tstrstr(sched_params, \"no_env_cache\"));\n\t\txfree(sched_params);\n\t\tconfig_update = conf->last_update;\n\t}\n\n\tfor (i=0; i<req->envc; i++) {\n\t\tif (xstrcmp(req->environment[i], \"SLURM_GET_USER_ENV=1\") == 0)\n\t\t\tbreak;\n\t}\n\tif (i >= req->envc)\n\t\treturn 0;\t\t/* don't need to load env */\n\n\tif (slurm_getpwuid_r(req->uid, &pwd, pwd_buf, PW_BUF_SIZE, &pwd_ptr)\n\t    || (pwd_ptr == NULL)) {\n\t\terror(\"%s: getpwuid_r(%u):%m\", __func__, req->uid);\n\t\treturn -1;\n\t}\n\tverbose(\"%s: get env for user %s here\", __func__, pwd.pw_name);\n\n\t/* Permit up to 120 second delay before using cache file */\n\tnew_env = env_array_user_default(pwd.pw_name, 120, 0, no_env_cache);\n\tif (! new_env) {\n\t\terror(\"%s: Unable to get user's local environment%s\",\n\t\t      __func__, no_env_cache ?\n\t\t      \"\" : \", running only with passed environment\");\n\t\treturn -1;\n\t}\n\n\tenv_array_merge(&new_env,\n\t\t\t(const char **) req->environment);\n\tenv_array_free(req->environment);\n\treq->environment = new_env;\n\treq->envc = envcount(new_env);\n\n\treturn 0;\n}\n\n/* The RPC currently contains a memory size limit, but we load the\n * value from the job credential to be certain it has not been\n * altered by the user */\nstatic void\n_set_batch_job_limits(slurm_msg_t *msg)\n{\n\tint i;\n\tuint32_t alloc_lps = 0, last_bit = 0;\n\tbool cpu_log = slurm_get_debug_flags() & DEBUG_FLAG_CPU_BIND;\n\tslurm_cred_arg_t arg;\n\tbatch_job_launch_msg_t *req = (batch_job_launch_msg_t *)msg->data;\n\n\tif (slurm_cred_get_args(req->cred, &arg) != SLURM_SUCCESS)\n\t\treturn;\n\treq->job_core_spec = arg.job_core_spec;\t/* Prevent user reset */\n\n\tif (cpu_log) {\n\t\tchar *per_job = \"\";\n\t\tuint32_t job_mem  = arg.job_mem_limit;\n\t\tif (job_mem & MEM_PER_CPU) {\n\t\t\tjob_mem &= (~MEM_PER_CPU);\n\t\t\tper_job = \"_per_CPU\";\n\t\t}\n\t\tinfo(\"====================\");\n\t\tinfo(\"batch_job:%u job_mem:%uMB%s\", req->job_id,\n\t\t     job_mem, per_job);\n\t}\n\tif (cpu_log || (arg.job_mem_limit & MEM_PER_CPU)) {\n\t\tif (arg.job_nhosts > 0) {\n\t\t\tlast_bit = arg.sockets_per_node[0] *\n\t\t\t\t   arg.cores_per_socket[0];\n\t\t\tfor (i=0; i<last_bit; i++) {\n\t\t\t\tif (!bit_test(arg.job_core_bitmap, i))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (cpu_log)\n\t\t\t\t\tinfo(\"JobNode[0] CPU[%u] Job alloc\",i);\n\t\t\t\talloc_lps++;\n\t\t\t}\n\t\t}\n\t\tif (cpu_log)\n\t\t\tinfo(\"====================\");\n\t\tif (alloc_lps == 0) {\n\t\t\terror(\"_set_batch_job_limit: alloc_lps is zero\");\n\t\t\talloc_lps = 1;\n\t\t}\n\n\t\t/* NOTE: alloc_lps is the count of allocated resources\n\t\t * (typically cores). Convert to CPU count as needed */\n\t\tif (last_bit < 1)\n\t\t\terror(\"Batch job credential allocates no CPUs\");\n\t\telse {\n\t\t\ti = conf->cpus / last_bit;\n\t\t\tif (i > 1)\n\t\t\t\talloc_lps *= i;\n\t\t}\n\t}\n\n\tif (arg.job_mem_limit & MEM_PER_CPU) {\n\t\treq->job_mem = arg.job_mem_limit & (~MEM_PER_CPU);\n\t\treq->job_mem *= alloc_lps;\n\t} else\n\t\treq->job_mem = arg.job_mem_limit;\n\n\tslurm_cred_free_args(&arg);\n}\n\n/* These functions prevent a possible race condition if the batch script's\n * complete RPC is processed before it's launch_successful response. This\n *  */\nstatic bool _is_batch_job_finished(uint32_t job_id)\n{\n\tbool found_job = false;\n\tint i;\n\n\tslurm_mutex_lock(&fini_mutex);\n\tfor (i = 0; i < FINI_JOB_CNT; i++) {\n\t\tif (fini_job_id[i] == job_id) {\n\t\t\tfound_job = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tslurm_mutex_unlock(&fini_mutex);\n\n\treturn found_job;\n}\nstatic void _note_batch_job_finished(uint32_t job_id)\n{\n\tslurm_mutex_lock(&fini_mutex);\n\tfini_job_id[next_fini_job_inx] = job_id;\n\tif (++next_fini_job_inx >= FINI_JOB_CNT)\n\t\tnext_fini_job_inx = 0;\n\tslurm_mutex_unlock(&fini_mutex);\n}\n\n/* Send notification to slurmctld we are finished running the prolog.\n * This is needed on system that don't use srun to launch their tasks.\n */\nstatic void _notify_slurmctld_prolog_fini(\n\tuint32_t job_id, uint32_t prolog_return_code)\n{\n\tint rc;\n\tslurm_msg_t req_msg;\n\tcomplete_prolog_msg_t req;\n\n\tslurm_msg_t_init(&req_msg);\n\treq.job_id\t= job_id;\n\treq.prolog_rc\t= prolog_return_code;\n\n\treq_msg.msg_type= REQUEST_COMPLETE_PROLOG;\n\treq_msg.data\t= &req;\n\n\tif ((slurm_send_recv_controller_rc_msg(&req_msg, &rc) < 0) ||\n\t    (rc != SLURM_SUCCESS))\n\t\terror(\"Error sending prolog completion notification: %m\");\n}\n\n/* Convert memory limits from per-CPU to per-node */\nstatic void _convert_job_mem(slurm_msg_t *msg)\n{\n\tprolog_launch_msg_t *req = (prolog_launch_msg_t *)msg->data;\n\tslurm_cred_arg_t arg;\n\thostset_t j_hset = NULL;\n\tint rc, hi, host_index, job_cpus;\n\tint i, i_first_bit = 0, i_last_bit = 0;\n\n\trc = slurm_cred_verify(conf->vctx, req->cred, &arg,\n\t\t\t       msg->protocol_version);\n\tif (rc < 0) {\n\t\terror(\"%s: slurm_cred_verify failed: %m\", __func__);\n\t\treq->nnodes = 1;\t/* best guess */\n\t\treturn;\n\t}\n\n\treq->nnodes = arg.job_nhosts;\n\n\tif (arg.job_mem_limit == 0)\n\t\tgoto fini;\n\tif ((arg.job_mem_limit & MEM_PER_CPU) == 0) {\n\t\treq->job_mem_limit = arg.job_mem_limit;\n\t\tgoto fini;\n\t}\n\n\t/* Assume 1 CPU on error */\n\treq->job_mem_limit = arg.job_mem_limit & (~MEM_PER_CPU);\n\n\tif (!(j_hset = hostset_create(arg.job_hostlist))) {\n\t\terror(\"%s: Unable to parse credential hostlist: `%s'\",\n\t\t      __func__, arg.step_hostlist);\n\t\tgoto fini;\n\t}\n\thost_index = hostset_find(j_hset, conf->node_name);\n\thostset_destroy(j_hset);\n\n\thi = host_index + 1;\t/* change from 0-origin to 1-origin */\n\tfor (i = 0; hi; i++) {\n\t\tif (hi > arg.sock_core_rep_count[i]) {\n\t\t\ti_first_bit += arg.sockets_per_node[i] *\n\t\t\t\t       arg.cores_per_socket[i] *\n\t\t\t\t       arg.sock_core_rep_count[i];\n\t\t\ti_last_bit = i_first_bit +\n\t\t\t\t     arg.sockets_per_node[i] *\n\t\t\t\t     arg.cores_per_socket[i] *\n\t\t\t\t     arg.sock_core_rep_count[i];\n\t\t\thi -= arg.sock_core_rep_count[i];\n\t\t} else {\n\t\t\ti_first_bit += arg.sockets_per_node[i] *\n\t\t\t\t       arg.cores_per_socket[i] * (hi - 1);\n\t\t\ti_last_bit = i_first_bit +\n\t\t\t\t     arg.sockets_per_node[i] *\n\t\t\t\t     arg.cores_per_socket[i];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Now count the allocated processors on this node */\n\tjob_cpus = 0;\n\tfor (i = i_first_bit; i < i_last_bit; i++) {\n\t\tif (bit_test(arg.job_core_bitmap, i))\n\t\t\tjob_cpus++;\n\t}\n\n\t/* NOTE: alloc_lps is the count of allocated resources\n\t * (typically cores). Convert to CPU count as needed */\n\tif (i_last_bit > i_first_bit) {\n\t\ti = conf->cpus / (i_last_bit - i_first_bit);\n\t\tif (i > 1)\n\t\t\tjob_cpus *= i;\n\t}\n\n\treq->job_mem_limit *= job_cpus;\n\nfini:\tslurm_cred_free_args(&arg);\n}\n\nstatic void _make_prolog_mem_container(slurm_msg_t *msg)\n{\n\tprolog_launch_msg_t *req = (prolog_launch_msg_t *)msg->data;\n\tjob_mem_limits_t *job_limits_ptr;\n\tstep_loc_t step_info;\n\n\t_convert_job_mem(msg);\t/* Convert per-CPU mem limit */\n\tif (req->job_mem_limit) {\n\t\tslurm_mutex_lock(&job_limits_mutex);\n\t\tif (!job_limits_list)\n\t\t\tjob_limits_list = list_create(_job_limits_free);\n\t\tstep_info.jobid  = req->job_id;\n\t\tstep_info.stepid = SLURM_EXTERN_CONT;\n\t\tjob_limits_ptr = list_find_first (job_limits_list,\n\t\t\t\t\t\t  _step_limits_match,\n\t\t\t\t\t\t  &step_info);\n\t\tif (!job_limits_ptr) {\n\t\t\tjob_limits_ptr = xmalloc(sizeof(job_mem_limits_t));\n\t\t\tjob_limits_ptr->job_id   = req->job_id;\n\t\t\tjob_limits_ptr->job_mem  = req->job_mem_limit;\n\t\t\tjob_limits_ptr->step_id  = SLURM_EXTERN_CONT;\n\t\t\tjob_limits_ptr->step_mem = req->job_mem_limit;\n#if _LIMIT_INFO\n\t\t\tinfo(\"AddLim step:%u.%u job_mem:%u step_mem:%u\",\n\t\t\t      job_limits_ptr->job_id, job_limits_ptr->step_id,\n\t\t\t      job_limits_ptr->job_mem,\n\t\t\t      job_limits_ptr->step_mem);\n#endif\n\t\t\tlist_append(job_limits_list, job_limits_ptr);\n\t\t}\n\t\tslurm_mutex_unlock(&job_limits_mutex);\n\t}\n}\n\nstatic void _spawn_prolog_stepd(slurm_msg_t *msg)\n{\n\tprolog_launch_msg_t *req = (prolog_launch_msg_t *)msg->data;\n\tlaunch_tasks_request_msg_t *launch_req;\n\tslurm_addr_t self;\n\tslurm_addr_t *cli = &msg->orig_addr;\n\tint i;\n\n\tlaunch_req = xmalloc(sizeof(launch_tasks_request_msg_t));\n\tlaunch_req->alias_list\t\t= req->alias_list;\n\tlaunch_req->complete_nodelist\t= req->nodes;\n\tlaunch_req->cpus_per_task\t= 1;\n\tlaunch_req->cred\t\t= req->cred;\n\tlaunch_req->cwd\t\t\t= req->work_dir;\n\tlaunch_req->efname\t\t= \"/dev/null\";\n\tlaunch_req->gid\t\t\t= req->gid;\n\tlaunch_req->global_task_ids\t= xmalloc(sizeof(uint32_t *)\n\t\t\t\t\t\t  * req->nnodes);\n\tlaunch_req->ifname\t\t= \"/dev/null\";\n\tlaunch_req->job_id\t\t= req->job_id;\n\tlaunch_req->job_mem_lim\t\t= req->job_mem_limit;\n\tlaunch_req->job_step_id\t\t= SLURM_EXTERN_CONT;\n\tlaunch_req->nnodes\t\t= req->nnodes;\n\tlaunch_req->ntasks\t\t= req->nnodes;\n\tlaunch_req->ofname\t\t= \"/dev/null\";\n\tlaunch_req->partition\t\t= req->partition;\n\tlaunch_req->spank_job_env_size\t= req->spank_job_env_size;\n\tlaunch_req->spank_job_env\t= req->spank_job_env;\n\tlaunch_req->step_mem_lim\t= req->job_mem_limit;\n\tlaunch_req->tasks_to_launch\t= xmalloc(sizeof(uint16_t)\n\t\t\t\t\t\t  * req->nnodes);\n\tlaunch_req->uid\t\t\t= req->uid;\n\n\tfor (i = 0; i < req->nnodes; i++) {\n\t\tuint32_t *tmp32 = xmalloc(sizeof(uint32_t));\n\t\t*tmp32 = i;\n\t\tlaunch_req->global_task_ids[i] = tmp32;\n\t\tlaunch_req->tasks_to_launch[i] = 1;\n\t}\n\n\tslurm_get_stream_addr(msg->conn_fd, &self);\n\t/* Since job could have been killed while the prolog was\n\t * running (especially on BlueGene, which can take minutes\n\t * for partition booting). Test if the credential has since\n\t * been revoked and exit as needed. */\n\tif (slurm_cred_revoked(conf->vctx, req->cred)) {\n\t\tinfo(\"Job %u already killed, do not launch extern step\",\n\t\t     req->job_id);\n\t} else {\n\t\thostset_t step_hset = hostset_create(req->nodes);\n\n\t\tdebug3(\"%s: call to _forkexec_slurmstepd\", __func__);\n\t\t(void) _forkexec_slurmstepd(\n\t\t\tLAUNCH_TASKS, (void *)launch_req, cli,\n\t\t\t&self, step_hset, msg->protocol_version);\n\t\tdebug3(\"%s: return from _forkexec_slurmstepd\", __func__);\n\t\tif (step_hset)\n\t\t\thostset_destroy(step_hset);\n\t}\n\n\tfor (i = 0; i < req->nnodes; i++)\n\t\txfree(launch_req->global_task_ids[i]);\n\txfree(launch_req->global_task_ids);\n\txfree(launch_req->tasks_to_launch);\n\txfree(launch_req);\n}\n\nstatic void _rpc_prolog(slurm_msg_t *msg)\n{\n\tint rc = SLURM_SUCCESS;\n\tprolog_launch_msg_t *req = (prolog_launch_msg_t *)msg->data;\n\tjob_env_t job_env;\n\tbool     first_job_run;\n\tuid_t    req_uid;\n\n\tif (req == NULL)\n\t\treturn;\n\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"REQUEST_LAUNCH_PROLOG request from uid %u\",\n\t\t      (unsigned int) req_uid);\n\t\treturn;\n\t}\n\n\tif (slurm_send_rc_msg(msg, rc) < 0) {\n\t\terror(\"Error starting prolog: %m\");\n\t}\n\tif (rc) {\n\t\tint term_sig, exit_status;\n\t\tif (WIFSIGNALED(rc)) {\n\t\t\texit_status = 0;\n\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t} else {\n\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\tterm_sig    = 0;\n\t\t}\n\t\terror(\"[job %u] prolog start failed status=%d:%d\",\n\t\t      req->job_id, exit_status, term_sig);\n\t\trc = ESLURMD_PROLOG_FAILED;\n\t}\n\n\tslurm_mutex_lock(&prolog_mutex);\n\tfirst_job_run = !slurm_cred_jobid_cached(conf->vctx, req->job_id);\n\tif (first_job_run) {\n\t\tif (slurmctld_conf.prolog_flags & PROLOG_FLAG_CONTAIN)\n\t\t\t_make_prolog_mem_container(msg);\n\n\t\tif (container_g_create(req->job_id))\n\t\t\terror(\"container_g_create(%u): %m\", req->job_id);\n\n\t\tslurm_cred_insert_jobid(conf->vctx, req->job_id);\n\t\t_add_job_running_prolog(req->job_id);\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\n\t\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\t\tjob_env.jobid = req->job_id;\n\t\tjob_env.step_id = 0;\t/* not available */\n\t\tjob_env.node_list = req->nodes;\n\t\tjob_env.partition = req->partition;\n\t\tjob_env.spank_job_env = req->spank_job_env;\n\t\tjob_env.spank_job_env_size = req->spank_job_env_size;\n\t\tjob_env.uid = req->uid;\n\t\tjob_env.user_name = req->user_name;\n#if defined(HAVE_BG)\n\t\tselect_g_select_jobinfo_get(req->select_jobinfo,\n\t\t\t\t\t    SELECT_JOBDATA_BLOCK_ID,\n\t\t\t\t\t    &job_env.resv_id);\n#elif defined(HAVE_ALPS_CRAY)\n\t\tjob_env.resv_id = select_g_select_jobinfo_xstrdup(\n\t\t\treq->select_jobinfo, SELECT_PRINT_RESV_ID);\n#endif\n\t\trc = _run_prolog(&job_env, req->cred);\n\n\t\tif (rc) {\n\t\t\tint term_sig, exit_status;\n\t\t\tif (WIFSIGNALED(rc)) {\n\t\t\t\texit_status = 0;\n\t\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t\t} else {\n\t\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\t\tterm_sig    = 0;\n\t\t\t}\n\t\t\terror(\"[job %u] prolog failed status=%d:%d\",\n\t\t\t      req->job_id, exit_status, term_sig);\n\t\t\trc = ESLURMD_PROLOG_FAILED;\n\t\t}\n\t} else\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\n\tif (!(slurmctld_conf.prolog_flags & PROLOG_FLAG_NOHOLD))\n\t\t_notify_slurmctld_prolog_fini(req->job_id, rc);\n\n\tif (rc == SLURM_SUCCESS) {\n\t\tif (slurmctld_conf.prolog_flags & PROLOG_FLAG_CONTAIN)\n\t\t\t_spawn_prolog_stepd(msg);\n\t} else {\n\t\t_launch_job_fail(req->job_id, rc);\n\t\t/*\n\t\t *  If job prolog failed or we could not reply,\n\t\t *  initiate message to slurmctld with current state\n\t\t */\n\t\tif ((rc == ESLURMD_PROLOG_FAILED) ||\n\t\t    (rc == SLURM_COMMUNICATIONS_SEND_ERROR) ||\n\t\t    (rc == ESLURMD_SETUP_ENVIRONMENT_ERROR))\n\t\t\tsend_registration_msg(rc, false);\n\t}\n}\n\nstatic void\n_rpc_batch_job(slurm_msg_t *msg, bool new_msg)\n{\n\tbatch_job_launch_msg_t *req = (batch_job_launch_msg_t *)msg->data;\n\tbool     first_job_run;\n\tint      rc = SLURM_SUCCESS;\n\tbool\t replied = false, revoked;\n\tslurm_addr_t *cli = &msg->orig_addr;\n\n\tif (new_msg) {\n\t\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t     conf->auth_info);\n\t\tif (!_slurm_authorized_user(req_uid)) {\n\t\t\terror(\"Security violation, batch launch RPC from uid %d\",\n\t\t\t      req_uid);\n\t\t\trc = ESLURM_USER_ID_MISSING;  /* or bad in this case */\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (_launch_job_test(req->job_id)) {\n\t\terror(\"Job %u already running, do not launch second copy\",\n\t\t      req->job_id);\n\t\trc = ESLURM_DUPLICATE_JOB_ID;\t/* job already running */\n\t\t_launch_job_fail(req->job_id, rc);\n\t\tgoto done;\n\t}\n\n\tslurm_cred_handle_reissue(conf->vctx, req->cred);\n\tif (slurm_cred_revoked(conf->vctx, req->cred)) {\n\t\terror(\"Job %u already killed, do not launch batch job\",\n\t\t      req->job_id);\n\t\trc = ESLURMD_CREDENTIAL_REVOKED;\t/* job already ran */\n\t\tgoto done;\n\t}\n\n\ttask_g_slurmd_batch_request(req->job_id, req);\t/* determine task affinity */\n\n\tslurm_mutex_lock(&prolog_mutex);\n\tfirst_job_run = !slurm_cred_jobid_cached(conf->vctx, req->job_id);\n\n\t/* BlueGene prolog waits for partition boot and is very slow.\n\t * On any system we might need to load environment variables\n\t * for Moab (see --get-user-env), which could also be slow.\n\t * Just reply now and send a separate kill job request if the\n\t * prolog or launch fail. */\n\treplied = true;\n\tif (new_msg && (slurm_send_rc_msg(msg, rc) < 1)) {\n\t\t/* The slurmctld is no longer waiting for a reply.\n\t\t * This typically indicates that the slurmd was\n\t\t * blocked from memory and/or CPUs and the slurmctld\n\t\t * has requeued the batch job request. */\n\t\terror(\"Could not confirm batch launch for job %u, \"\n\t\t      \"aborting request\", req->job_id);\n\t\trc = SLURM_COMMUNICATIONS_SEND_ERROR;\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\t\tgoto done;\n\t}\n\n\t/*\n\t * Insert jobid into credential context to denote that\n\t * we've now \"seen\" an instance of the job\n\t */\n\tif (first_job_run) {\n\t\tjob_env_t job_env;\n\t\tslurm_cred_insert_jobid(conf->vctx, req->job_id);\n\t\t_add_job_running_prolog(req->job_id);\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\n\t\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\t\tjob_env.jobid = req->job_id;\n\t\tjob_env.step_id = req->step_id;\n\t\tjob_env.node_list = req->nodes;\n\t\tjob_env.partition = req->partition;\n\t\tjob_env.spank_job_env = req->spank_job_env;\n\t\tjob_env.spank_job_env_size = req->spank_job_env_size;\n\t\tjob_env.uid = req->uid;\n\t\tjob_env.user_name = req->user_name;\n\t\t/*\n\t \t * Run job prolog on this node\n\t \t */\n#if defined(HAVE_BG)\n\t\tselect_g_select_jobinfo_get(req->select_jobinfo,\n\t\t\t\t\t    SELECT_JOBDATA_BLOCK_ID,\n\t\t\t\t\t    &job_env.resv_id);\n#elif defined(HAVE_ALPS_CRAY)\n\t\tjob_env.resv_id = select_g_select_jobinfo_xstrdup(\n\t\t\treq->select_jobinfo, SELECT_PRINT_RESV_ID);\n#endif\n\t\tif (container_g_create(req->job_id))\n\t\t\terror(\"container_g_create(%u): %m\", req->job_id);\n\t\trc = _run_prolog(&job_env, req->cred);\n\t\txfree(job_env.resv_id);\n\t\tif (rc) {\n\t\t\tint term_sig, exit_status;\n\t\t\tif (WIFSIGNALED(rc)) {\n\t\t\t\texit_status = 0;\n\t\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t\t} else {\n\t\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\t\tterm_sig    = 0;\n\t\t\t}\n\t\t\terror(\"[job %u] prolog failed status=%d:%d\",\n\t\t\t      req->job_id, exit_status, term_sig);\n\t\t\t_prolog_error(req, rc);\n\t\t\trc = ESLURMD_PROLOG_FAILED;\n\t\t\tgoto done;\n\t\t}\n\t} else {\n\t\tslurm_mutex_unlock(&prolog_mutex);\n\t\t_wait_for_job_running_prolog(req->job_id);\n\t}\n\n\tif (_get_user_env(req) < 0) {\n\t\tbool requeue = _requeue_setup_env_fail();\n\t\tif (requeue) {\n\t\t\trc = ESLURMD_SETUP_ENVIRONMENT_ERROR;\n\t\t\tgoto done;\n\t\t}\n\t}\n\t_set_batch_job_limits(msg);\n\n\t/* Since job could have been killed while the prolog was\n\t * running (especially on BlueGene, which can take minutes\n\t * for partition booting). Test if the credential has since\n\t * been revoked and exit as needed. */\n\tif (slurm_cred_revoked(conf->vctx, req->cred)) {\n\t\tinfo(\"Job %u already killed, do not launch batch job\",\n\t\t     req->job_id);\n\t\trc = ESLURMD_CREDENTIAL_REVOKED;     /* job already ran */\n\t\tgoto done;\n\t}\n\n\tslurm_mutex_lock(&launch_mutex);\n\tif (req->step_id == SLURM_BATCH_SCRIPT)\n\t\tinfo(\"Launching batch job %u for UID %d\",\n\t\t     req->job_id, req->uid);\n\telse\n\t\tinfo(\"Launching batch job %u.%u for UID %d\",\n\t\t     req->job_id, req->step_id, req->uid);\n\n\tdebug3(\"_rpc_batch_job: call to _forkexec_slurmstepd\");\n\trc = _forkexec_slurmstepd(LAUNCH_BATCH_JOB, (void *)req, cli, NULL,\n\t\t\t\t  (hostset_t)NULL, SLURM_PROTOCOL_VERSION);\n\tdebug3(\"_rpc_batch_job: return from _forkexec_slurmstepd: %d\", rc);\n\n\tslurm_mutex_unlock(&launch_mutex);\n\t_launch_complete_add(req->job_id);\n\n\t/* On a busy system, slurmstepd may take a while to respond,\n\t * if the job was cancelled in the interim, run through the\n\t * abort logic below. */\n\trevoked = slurm_cred_revoked(conf->vctx, req->cred);\n\tif (revoked)\n\t\t_launch_complete_rm(req->job_id);\n\tif (revoked && _is_batch_job_finished(req->job_id)) {\n\t\t/* If configured with select/serial and the batch job already\n\t\t * completed, consider the job sucessfully launched and do\n\t\t * not repeat termination logic below, which in the worst case\n\t\t * just slows things down with another message. */\n\t\trevoked = false;\n\t}\n\tif (revoked) {\n\t\tinfo(\"Job %u killed while launch was in progress\",\n\t\t     req->job_id);\n\t\tsleep(1);\t/* give slurmstepd time to create\n\t\t\t\t * the communication socket */\n\t\t_terminate_all_steps(req->job_id, true);\n\t\trc = ESLURMD_CREDENTIAL_REVOKED;\n\t\tgoto done;\n\t}\n\ndone:\n\tif (!replied) {\n\t\tif (new_msg && (slurm_send_rc_msg(msg, rc) < 1)) {\n\t\t\t/* The slurmctld is no longer waiting for a reply.\n\t\t\t * This typically indicates that the slurmd was\n\t\t\t * blocked from memory and/or CPUs and the slurmctld\n\t\t\t * has requeued the batch job request. */\n\t\t\terror(\"Could not confirm batch launch for job %u, \"\n\t\t\t      \"aborting request\", req->job_id);\n\t\t\trc = SLURM_COMMUNICATIONS_SEND_ERROR;\n\t\t} else {\n\t\t\t/* No need to initiate separate reply below */\n\t\t\trc = SLURM_SUCCESS;\n\t\t}\n\t}\n\tif (rc != SLURM_SUCCESS) {\n\t\t/* prolog or job launch failure,\n\t\t * tell slurmctld that the job failed */\n\t\tif (req->step_id == SLURM_BATCH_SCRIPT)\n\t\t\t_launch_job_fail(req->job_id, rc);\n\t\telse\n\t\t\t_abort_step(req->job_id, req->step_id);\n\t}\n\n\t/*\n\t *  If job prolog failed or we could not reply,\n\t *  initiate message to slurmctld with current state\n\t */\n\tif ((rc == ESLURMD_PROLOG_FAILED)\n\t    || (rc == SLURM_COMMUNICATIONS_SEND_ERROR)\n\t    || (rc == ESLURMD_SETUP_ENVIRONMENT_ERROR)) {\n\t\tsend_registration_msg(rc, false);\n\t}\n}\n\n/*\n * Send notification message to batch job\n */\nstatic void\n_rpc_job_notify(slurm_msg_t *msg)\n{\n\tjob_notify_msg_t *req = msg->data;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tuid_t job_uid;\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd = NULL;\n\tint step_cnt  = 0;\n\tint fd;\n\n\tdebug(\"_rpc_job_notify, uid = %d, jobid = %u\", req_uid, req->job_id);\n\tjob_uid = _get_job_uid(req->job_id);\n\tif ((int)job_uid < 0)\n\t\tgoto no_job;\n\n\t/*\n\t * check that requesting user ID is the SLURM UID or root\n\t */\n\tif ((req_uid != job_uid) && (!_slurm_authorized_user(req_uid))) {\n\t\terror(\"Security violation: job_notify(%u) from uid %d\",\n\t\t      req->job_id, req_uid);\n\t\treturn;\n\t}\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif ((stepd->jobid  != req->job_id) ||\n\t\t    (stepd->stepid != SLURM_BATCH_SCRIPT)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tstep_cnt++;\n\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1) {\n\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tinfo(\"send notification to job %u.%u\",\n\t\t     stepd->jobid, stepd->stepid);\n\t\tif (stepd_notify_job(fd, stepd->protocol_version,\n\t\t\t\t     req->message) < 0)\n\t\t\tdebug(\"notify jobid=%u failed: %m\", stepd->jobid);\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\nno_job:\n\tif (step_cnt == 0) {\n\t\tdebug2(\"Can't find jobid %u to send notification message\",\n\t\t       req->job_id);\n\t}\n}\n\nstatic int\n_launch_job_fail(uint32_t job_id, uint32_t slurm_rc)\n{\n\tcomplete_batch_script_msg_t comp_msg;\n\tstruct requeue_msg req_msg;\n\tslurm_msg_t resp_msg;\n\tint rc = 0, rpc_rc;\n\tstatic time_t config_update = 0;\n\tstatic bool requeue_no_hold = false;\n\n\tif (config_update != conf->last_update) {\n\t\tchar *sched_params = slurm_get_sched_params();\n\t\trequeue_no_hold = (sched_params && strstr(\n\t\t\t\t\t   sched_params,\n\t\t\t\t\t   \"nohold_on_prolog_fail\"));\n\t\txfree(sched_params);\n\t\tconfig_update = conf->last_update;\n\t}\n\n\tslurm_msg_t_init(&resp_msg);\n\n\tif (slurm_rc == ESLURMD_CREDENTIAL_REVOKED) {\n\t\tcomp_msg.job_id = job_id;\n\t\tcomp_msg.job_rc = INFINITE;\n\t\tcomp_msg.slurm_rc = slurm_rc;\n\t\tcomp_msg.node_name = conf->node_name;\n\t\tcomp_msg.jobacct = NULL; /* unused */\n\t\tresp_msg.msg_type = REQUEST_COMPLETE_BATCH_SCRIPT;\n\t\tresp_msg.data = &comp_msg;\n\t} else {\n\t\treq_msg.job_id = job_id;\n\t\treq_msg.job_id_str = NULL;\n\t\tif (requeue_no_hold) {\n\t\t\treq_msg.state = JOB_PENDING;\n\t\t} else {\n\t\t\treq_msg.state = (JOB_REQUEUE_HOLD|JOB_LAUNCH_FAILED);\n\t\t}\n\t\tresp_msg.msg_type = REQUEST_JOB_REQUEUE;\n\t\tresp_msg.data = &req_msg;\n\t}\n\n\trpc_rc = slurm_send_recv_controller_rc_msg(&resp_msg, &rc);\n\tif ((resp_msg.msg_type == REQUEST_JOB_REQUEUE) &&\n\t    ((rc == ESLURM_DISABLED) || (rc == ESLURM_BATCH_ONLY))) {\n\t\tinfo(\"Could not launch job %u and not able to requeue it, \"\n\t\t     \"cancelling job\", job_id);\n\n\t\tif ((slurm_rc == ESLURMD_PROLOG_FAILED) &&\n\t\t    (rc == ESLURM_BATCH_ONLY)) {\n\t\t\tchar *buf = NULL;\n\t\t\txstrfmtcat(buf, \"Prolog failure on node %s\",\n\t\t\t\t   conf->node_name);\n\t\t\tslurm_notify_job(job_id, buf);\n\t\t\txfree(buf);\n\t\t}\n\n\t\tcomp_msg.job_id = job_id;\n\t\tcomp_msg.job_rc = INFINITE;\n\t\tcomp_msg.slurm_rc = slurm_rc;\n\t\tcomp_msg.node_name = conf->node_name;\n\t\tcomp_msg.jobacct = NULL; /* unused */\n\t\tresp_msg.msg_type = REQUEST_COMPLETE_BATCH_SCRIPT;\n\t\tresp_msg.data = &comp_msg;\n\t\trpc_rc = slurm_send_recv_controller_rc_msg(&resp_msg, &rc);\n\t}\n\n\treturn rpc_rc;\n}\n\nstatic int\n_abort_step(uint32_t job_id, uint32_t step_id)\n{\n\tstep_complete_msg_t resp;\n\tslurm_msg_t resp_msg;\n\tslurm_msg_t_init(&resp_msg);\n\tint rc, rc2;\n\n\tresp.job_id       = job_id;\n\tresp.job_step_id  = step_id;\n\tresp.range_first  = 0;\n\tresp.range_last   = 0;\n\tresp.step_rc      = 1;\n\tresp.jobacct      = jobacctinfo_create(NULL);\n\tresp_msg.msg_type = REQUEST_STEP_COMPLETE;\n\tresp_msg.data     = &resp;\n\trc2 = slurm_send_recv_controller_rc_msg(&resp_msg, &rc);\n\t/* Note: we are ignoring the RPC return code */\n\tjobacctinfo_destroy(resp.jobacct);\n\treturn rc2;\n}\n\nstatic void\n_rpc_reconfig(slurm_msg_t *msg)\n{\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\n\tif (!_slurm_authorized_user(req_uid))\n\t\terror(\"Security violation, reconfig RPC from uid %d\",\n\t\t      req_uid);\n\telse\n\t\tkill(conf->pid, SIGHUP);\n\tforward_wait(msg);\n\t/* Never return a message, slurmctld does not expect one */\n}\n\nstatic void\n_rpc_shutdown(slurm_msg_t *msg)\n{\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\n\tforward_wait(msg);\n\tif (!_slurm_authorized_user(req_uid))\n\t\terror(\"Security violation, shutdown RPC from uid %d\",\n\t\t      req_uid);\n\telse {\n\t\tif (kill(conf->pid, SIGTERM) != 0)\n\t\t\terror(\"kill(%u,SIGTERM): %m\", conf->pid);\n\t}\n\n\t/* Never return a message, slurmctld does not expect one */\n}\n\nstatic void\n_rpc_reboot(slurm_msg_t *msg)\n{\n\tchar *reboot_program, *cmd = NULL, *sp;\n\treboot_msg_t *reboot_msg;\n\tslurm_ctl_conf_t *cfg;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tint exit_code;\n\n\tif (!_slurm_authorized_user(req_uid))\n\t\terror(\"Security violation, reboot RPC from uid %d\",\n\t\t      req_uid);\n\telse {\n\t\tcfg = slurm_conf_lock();\n\t\treboot_program = cfg->reboot_program;\n\t\tif (reboot_program) {\n\t\t\tsp = strchr(reboot_program, ' ');\n\t\t\tif (sp)\n\t\t\t\tsp = xstrndup(reboot_program,\n\t\t\t\t\t      (sp - reboot_program));\n\t\t\telse\n\t\t\t\tsp = xstrdup(reboot_program);\n\t\t\treboot_msg = (reboot_msg_t *) msg->data;\n\t\t\tif (reboot_msg && reboot_msg->features) {\n\t\t\t\tinfo(\"Node reboot request with features %s being processed\",\n\t\t\t\t     reboot_msg->features);\n\t\t\t\t(void) node_features_g_node_set(\n\t\t\t\t\t\treboot_msg->features);\n\t\t\t\tif (reboot_msg->features[0]) {\n\t\t\t\t\txstrfmtcat(cmd, \"%s %s\",\n\t\t\t\t\t\t   sp, reboot_msg->features);\n\t\t\t\t} else {\n\t\t\t\t\tcmd = xstrdup(sp);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tcmd = xstrdup(sp);\n\t\t\t\tinfo(\"Node reboot request being processed\");\n\t\t\t}\n\t\t\tif (access(sp, R_OK | X_OK) < 0)\n\t\t\t\terror(\"Cannot run RebootProgram [%s]: %m\", sp);\n\t\t\telse if ((exit_code = system(cmd)))\n\t\t\t\terror(\"system(%s) returned %d\", reboot_program,\n\t\t\t\t      exit_code);\n\t\t\txfree(sp);\n\t\t\txfree(cmd);\n\t\t} else\n\t\t\terror(\"RebootProgram isn't defined in config\");\n\t\tslurm_conf_unlock();\n\t}\n\n\t/* Never return a message, slurmctld does not expect one */\n\t/* slurm_send_rc_msg(msg, rc); */\n}\n\nstatic void _job_limits_free(void *x)\n{\n\txfree(x);\n}\n\nstatic int _job_limits_match(void *x, void *key)\n{\n\tjob_mem_limits_t *job_limits_ptr = (job_mem_limits_t *) x;\n\tuint32_t *job_id = (uint32_t *) key;\n\tif (job_limits_ptr->job_id == *job_id)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int _step_limits_match(void *x, void *key)\n{\n\tjob_mem_limits_t *job_limits_ptr = (job_mem_limits_t *) x;\n\tstep_loc_t *step_ptr = (step_loc_t *) key;\n\n\tif ((job_limits_ptr->job_id  == step_ptr->jobid) &&\n\t    (job_limits_ptr->step_id == step_ptr->stepid))\n\t\treturn 1;\n\treturn 0;\n}\n\n/* Call only with job_limits_mutex locked */\nstatic void\n_load_job_limits(void)\n{\n\tList steps;\n\tListIterator step_iter;\n\tstep_loc_t *stepd;\n\tint fd;\n\tjob_mem_limits_t *job_limits_ptr;\n\tslurmstepd_mem_info_t stepd_mem_info;\n\n\tif (!job_limits_list)\n\t\tjob_limits_list = list_create(_job_limits_free);\n\tjob_limits_loaded = true;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\tstep_iter = list_iterator_create(steps);\n\twhile ((stepd = list_next(step_iter))) {\n\t\tjob_limits_ptr = list_find_first(job_limits_list,\n\t\t\t\t\t\t _step_limits_match, stepd);\n\t\tif (job_limits_ptr)\t/* already processed */\n\t\t\tcontinue;\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1)\n\t\t\tcontinue;\t/* step completed */\n\n\t\tif (stepd_get_mem_limits(fd, stepd->protocol_version,\n\t\t\t\t\t  &stepd_mem_info) != SLURM_SUCCESS) {\n\t\t\terror(\"Error reading step %u.%u memory limits from \"\n\t\t\t      \"slurmstepd\",\n\t\t\t      stepd->jobid, stepd->stepid);\n\t\t\tclose(fd);\n\t\t\tcontinue;\n\t\t}\n\n\n\t\tif ((stepd_mem_info.job_mem_limit\n\t\t     || stepd_mem_info.step_mem_limit)) {\n\t\t\t/* create entry for this job */\n\t\t\tjob_limits_ptr = xmalloc(sizeof(job_mem_limits_t));\n\t\t\tjob_limits_ptr->job_id   = stepd->jobid;\n\t\t\tjob_limits_ptr->step_id  = stepd->stepid;\n\t\t\tjob_limits_ptr->job_mem  =\n\t\t\t\tstepd_mem_info.job_mem_limit;\n\t\t\tjob_limits_ptr->step_mem =\n\t\t\t\tstepd_mem_info.step_mem_limit;\n#if _LIMIT_INFO\n\t\t\tinfo(\"RecLim step:%u.%u job_mem:%u step_mem:%u\",\n\t\t\t     job_limits_ptr->job_id, job_limits_ptr->step_id,\n\t\t\t     job_limits_ptr->job_mem,\n\t\t\t     job_limits_ptr->step_mem);\n#endif\n\t\t\tlist_append(job_limits_list, job_limits_ptr);\n\t\t}\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(step_iter);\n\tFREE_NULL_LIST(steps);\n}\n\nstatic void\n_cancel_step_mem_limit(uint32_t job_id, uint32_t step_id)\n{\n\tslurm_msg_t msg;\n\tjob_notify_msg_t notify_req;\n\tjob_step_kill_msg_t kill_req;\n\n\t/* NOTE: Batch jobs may have no srun to get this message */\n\tslurm_msg_t_init(&msg);\n\tnotify_req.job_id      = job_id;\n\tnotify_req.job_step_id = step_id;\n\tnotify_req.message     = \"Exceeded job memory limit\";\n\tmsg.msg_type    = REQUEST_JOB_NOTIFY;\n\tmsg.data        = &notify_req;\n\tslurm_send_only_controller_msg(&msg);\n\n\tmemset(&kill_req, 0, sizeof(job_step_kill_msg_t));\n\tkill_req.job_id      = job_id;\n\tkill_req.job_step_id = step_id;\n\tkill_req.signal      = SIGKILL;\n\tkill_req.flags       = (uint16_t) 0;\n\tmsg.msg_type    = REQUEST_CANCEL_JOB_STEP;\n\tmsg.data        = &kill_req;\n\tslurm_send_only_controller_msg(&msg);\n}\n\n/* Enforce job memory limits here in slurmd. Step memory limits are\n * enforced within slurmstepd (using jobacct_gather plugin). */\nstatic void\n_enforce_job_mem_limit(void)\n{\n\tList steps;\n\tListIterator step_iter, job_limits_iter;\n\tjob_mem_limits_t *job_limits_ptr;\n\tstep_loc_t *stepd;\n\tint fd, i, job_inx, job_cnt;\n\tuint16_t vsize_factor;\n\tuint64_t step_rss, step_vsize;\n\tjob_step_id_msg_t acct_req;\n\tjob_step_stat_t *resp = NULL;\n\tstruct job_mem_info {\n\t\tuint32_t job_id;\n\t\tuint32_t mem_limit;\t/* MB */\n\t\tuint32_t mem_used;\t/* MB */\n\t\tuint32_t vsize_limit;\t/* MB */\n\t\tuint32_t vsize_used;\t/* MB */\n\t};\n\tstruct job_mem_info *job_mem_info_ptr = NULL;\n\n\t/* If users have configured MemLimitEnforce=no\n\t * in their slurm.conf keep going.\n\t */\n\tif (conf->mem_limit_enforce == false)\n\t\treturn;\n\n\tslurm_mutex_lock(&job_limits_mutex);\n\tif (!job_limits_loaded)\n\t\t_load_job_limits();\n\tif (list_count(job_limits_list) == 0) {\n\t\tslurm_mutex_unlock(&job_limits_mutex);\n\t\treturn;\n\t}\n\n\t/* Build table of job limits, use highest mem limit recorded */\n\tjob_mem_info_ptr = xmalloc((list_count(job_limits_list) + 1) *\n\t\t\t\t   sizeof(struct job_mem_info));\n\tjob_cnt = 0;\n\tjob_limits_iter = list_iterator_create(job_limits_list);\n\twhile ((job_limits_ptr = list_next(job_limits_iter))) {\n\t\tif (job_limits_ptr->job_mem == 0) \t/* no job limit */\n\t\t\tcontinue;\n\t\tfor (i=0; i<job_cnt; i++) {\n\t\t\tif (job_mem_info_ptr[i].job_id !=\n\t\t\t    job_limits_ptr->job_id)\n\t\t\t\tcontinue;\n\t\t\tjob_mem_info_ptr[i].mem_limit = MAX(\n\t\t\t\tjob_mem_info_ptr[i].mem_limit,\n\t\t\t\tjob_limits_ptr->job_mem);\n\t\t\tbreak;\n\t\t}\n\t\tif (i < job_cnt)\t/* job already found & recorded */\n\t\t\tcontinue;\n\t\tjob_mem_info_ptr[job_cnt].job_id    = job_limits_ptr->job_id;\n\t\tjob_mem_info_ptr[job_cnt].mem_limit = job_limits_ptr->job_mem;\n\t\tjob_cnt++;\n\t}\n\tlist_iterator_destroy(job_limits_iter);\n\tslurm_mutex_unlock(&job_limits_mutex);\n\n\tvsize_factor = slurm_get_vsize_factor();\n\tfor (i=0; i<job_cnt; i++) {\n\t\tjob_mem_info_ptr[i].vsize_limit = job_mem_info_ptr[i].\n\t\t\tmem_limit;\n\t\tjob_mem_info_ptr[i].vsize_limit *= (vsize_factor / 100.0);\n\t}\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\tstep_iter = list_iterator_create(steps);\n\twhile ((stepd = list_next(step_iter))) {\n\t\tfor (job_inx=0; job_inx<job_cnt; job_inx++) {\n\t\t\tif (job_mem_info_ptr[job_inx].job_id == stepd->jobid)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (job_inx >= job_cnt)\n\t\t\tcontinue;\t/* job/step not being tracked */\n\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1)\n\t\t\tcontinue;\t/* step completed */\n\t\tacct_req.job_id  = stepd->jobid;\n\t\tacct_req.step_id = stepd->stepid;\n\t\tresp = xmalloc(sizeof(job_step_stat_t));\n\n\t\tif ((!stepd_stat_jobacct(\n\t\t\t     fd, stepd->protocol_version,\n\t\t\t     &acct_req, resp)) &&\n\t\t    (resp->jobacct)) {\n\t\t\t/* resp->jobacct is NULL if account is disabled */\n\t\t\tjobacctinfo_getinfo((struct jobacctinfo *)\n\t\t\t\t\t    resp->jobacct,\n\t\t\t\t\t    JOBACCT_DATA_TOT_RSS,\n\t\t\t\t\t    &step_rss,\n\t\t\t\t\t    stepd->protocol_version);\n\t\t\tjobacctinfo_getinfo((struct jobacctinfo *)\n\t\t\t\t\t    resp->jobacct,\n\t\t\t\t\t    JOBACCT_DATA_TOT_VSIZE,\n\t\t\t\t\t    &step_vsize,\n\t\t\t\t\t    stepd->protocol_version);\n#if _LIMIT_INFO\n\t\t\tinfo(\"Step:%u.%u RSS:%\"PRIu64\" KB VSIZE:%\"PRIu64\" KB\",\n\t\t\t     stepd->jobid, stepd->stepid,\n\t\t\t     step_rss, step_vsize);\n#endif\n\t\t\tstep_rss /= 1024;\t/* KB to MB */\n\t\t\tstep_rss = MAX(step_rss, 1);\n\t\t\tjob_mem_info_ptr[job_inx].mem_used += step_rss;\n\t\t\tstep_vsize /= 1024;\t/* KB to MB */\n\t\t\tstep_vsize = MAX(step_vsize, 1);\n\t\t\tjob_mem_info_ptr[job_inx].vsize_used += step_vsize;\n\t\t}\n\t\tslurm_free_job_step_stat(resp);\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(step_iter);\n\tFREE_NULL_LIST(steps);\n\n\tfor (i=0; i<job_cnt; i++) {\n\t\tif (job_mem_info_ptr[i].mem_used == 0) {\n\t\t\t/* no steps found,\n\t\t\t * purge records for all steps of this job */\n\t\t\tslurm_mutex_lock(&job_limits_mutex);\n\t\t\tlist_delete_all(job_limits_list, _job_limits_match,\n\t\t\t\t\t&job_mem_info_ptr[i].job_id);\n\t\t\tslurm_mutex_unlock(&job_limits_mutex);\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((job_mem_info_ptr[i].mem_limit != 0) &&\n\t\t    (job_mem_info_ptr[i].mem_used >\n\t\t     job_mem_info_ptr[i].mem_limit)) {\n\t\t\tinfo(\"Job %u exceeded memory limit (%u>%u), \"\n\t\t\t     \"cancelling it\", job_mem_info_ptr[i].job_id,\n\t\t\t     job_mem_info_ptr[i].mem_used,\n\t\t\t     job_mem_info_ptr[i].mem_limit);\n\t\t\t_cancel_step_mem_limit(job_mem_info_ptr[i].job_id,\n\t\t\t\t\t       NO_VAL);\n\t\t} else if ((job_mem_info_ptr[i].vsize_limit != 0) &&\n\t\t\t   (job_mem_info_ptr[i].vsize_used >\n\t\t\t    job_mem_info_ptr[i].vsize_limit)) {\n\t\t\tinfo(\"Job %u exceeded virtual memory limit (%u>%u), \"\n\t\t\t     \"cancelling it\", job_mem_info_ptr[i].job_id,\n\t\t\t     job_mem_info_ptr[i].vsize_used,\n\t\t\t     job_mem_info_ptr[i].vsize_limit);\n\t\t\t_cancel_step_mem_limit(job_mem_info_ptr[i].job_id,\n\t\t\t\t\t       NO_VAL);\n\t\t}\n\t}\n\txfree(job_mem_info_ptr);\n}\n\nstatic int\n_rpc_ping(slurm_msg_t *msg)\n{\n\tint        rc = SLURM_SUCCESS;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tstatic bool first_msg = true;\n\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"Security violation, ping RPC from uid %d\",\n\t\t      req_uid);\n\t\tif (first_msg) {\n\t\t\terror(\"Do you have SlurmUser configured as uid %d?\",\n\t\t\t      req_uid);\n\t\t}\n\t\trc = ESLURM_USER_ID_MISSING;\t/* or bad in this case */\n\t}\n\tfirst_msg = false;\n\n\tif (rc != SLURM_SUCCESS) {\n\t\t/* Return result. If the reply can't be sent this indicates\n\t\t * 1. The network is broken OR\n\t\t * 2. slurmctld has died    OR\n\t\t * 3. slurmd was paged out due to full memory\n\t\t * If the reply request fails, we send an registration message\n\t\t * to slurmctld in hopes of avoiding having the node set DOWN\n\t\t * due to slurmd paging and not being able to respond in a\n\t\t * timely fashion. */\n\t\tif (slurm_send_rc_msg(msg, rc) < 0) {\n\t\t\terror(\"Error responding to ping: %m\");\n\t\t\tsend_registration_msg(SLURM_SUCCESS, false);\n\t\t}\n\t} else {\n\t\tslurm_msg_t resp_msg;\n\t\tping_slurmd_resp_msg_t ping_resp;\n\t\tget_cpu_load(&ping_resp.cpu_load);\n\t\tget_free_mem(&ping_resp.free_mem);\n\t\tslurm_msg_t_copy(&resp_msg, msg);\n\t\tresp_msg.msg_type = RESPONSE_PING_SLURMD;\n\t\tresp_msg.data     = &ping_resp;\n\n\t\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\t}\n\n\t/* Take this opportunity to enforce any job memory limits */\n\t_enforce_job_mem_limit();\n\t/* Clear up any stalled file transfers as well */\n\t_file_bcast_cleanup();\n\treturn rc;\n}\n\nstatic int\n_rpc_health_check(slurm_msg_t *msg)\n{\n\tint        rc = SLURM_SUCCESS;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"Security violation, health check RPC from uid %d\",\n\t\t      req_uid);\n\t\trc = ESLURM_USER_ID_MISSING;\t/* or bad in this case */\n\t}\n\n\t/* Return result. If the reply can't be sent this indicates that\n\t * 1. The network is broken OR\n\t * 2. slurmctld has died    OR\n\t * 3. slurmd was paged out due to full memory\n\t * If the reply request fails, we send an registration message to\n\t * slurmctld in hopes of avoiding having the node set DOWN due to\n\t * slurmd paging and not being able to respond in a timely fashion. */\n\tif (slurm_send_rc_msg(msg, rc) < 0) {\n\t\terror(\"Error responding to health check: %m\");\n\t\tsend_registration_msg(SLURM_SUCCESS, false);\n\t}\n\n\tif (rc == SLURM_SUCCESS)\n\t\trc = run_script_health_check();\n\n\t/* Take this opportunity to enforce any job memory limits */\n\t_enforce_job_mem_limit();\n\t/* Clear up any stalled file transfers as well */\n\t_file_bcast_cleanup();\n\treturn rc;\n}\n\n\nstatic int\n_rpc_acct_gather_update(slurm_msg_t *msg)\n{\n\tint        rc = SLURM_SUCCESS;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tstatic bool first_msg = true;\n\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"Security violation, acct_gather_update RPC from uid %d\",\n\t\t      req_uid);\n\t\tif (first_msg) {\n\t\t\terror(\"Do you have SlurmUser configured as uid %d?\",\n\t\t\t      req_uid);\n\t\t}\n\t\trc = ESLURM_USER_ID_MISSING;\t/* or bad in this case */\n\t}\n\tfirst_msg = false;\n\n\tif (rc != SLURM_SUCCESS) {\n\t\t/* Return result. If the reply can't be sent this indicates\n\t\t * 1. The network is broken OR\n\t\t * 2. slurmctld has died    OR\n\t\t * 3. slurmd was paged out due to full memory\n\t\t * If the reply request fails, we send an registration message\n\t\t * to slurmctld in hopes of avoiding having the node set DOWN\n\t\t * due to slurmd paging and not being able to respond in a\n\t\t * timely fashion. */\n\t\tif (slurm_send_rc_msg(msg, rc) < 0) {\n\t\t\terror(\"Error responding to account gather: %m\");\n\t\t\tsend_registration_msg(SLURM_SUCCESS, false);\n\t\t}\n\t} else {\n\t\tslurm_msg_t resp_msg;\n\t\tacct_gather_node_resp_msg_t acct_msg;\n\n\t\t/* Update node energy usage data */\n\t\tacct_gather_energy_g_update_node_energy();\n\n\t\tmemset(&acct_msg, 0, sizeof(acct_gather_node_resp_msg_t));\n\t\tacct_msg.node_name = conf->node_name;\n\t\tacct_msg.sensor_cnt = 1;\n\t\tacct_msg.energy = acct_gather_energy_alloc(acct_msg.sensor_cnt);\n\t\tacct_gather_energy_g_get_data(\n\t\t\tENERGY_DATA_NODE_ENERGY, acct_msg.energy);\n\n\t\tslurm_msg_t_copy(&resp_msg, msg);\n\t\tresp_msg.msg_type = RESPONSE_ACCT_GATHER_UPDATE;\n\t\tresp_msg.data     = &acct_msg;\n\n\t\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\n\t\tacct_gather_energy_destroy(acct_msg.energy);\n\t}\n\treturn rc;\n}\n\nstatic int\n_rpc_acct_gather_energy(slurm_msg_t *msg)\n{\n\tint        rc = SLURM_SUCCESS;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tstatic bool first_msg = true;\n\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"Security violation, acct_gather_update RPC from uid %d\",\n\t\t      req_uid);\n\t\tif (first_msg) {\n\t\t\terror(\"Do you have SlurmUser configured as uid %d?\",\n\t\t\t      req_uid);\n\t\t}\n\t\trc = ESLURM_USER_ID_MISSING;\t/* or bad in this case */\n\t}\n\tfirst_msg = false;\n\n\tif (rc != SLURM_SUCCESS) {\n\t\tif (slurm_send_rc_msg(msg, rc) < 0)\n\t\t\terror(\"Error responding to energy request: %m\");\n\t} else {\n\t\tslurm_msg_t resp_msg;\n\t\tacct_gather_node_resp_msg_t acct_msg;\n\t\ttime_t now = time(NULL), last_poll = 0;\n\t\tint data_type = ENERGY_DATA_STRUCT;\n\t\tuint16_t sensor_cnt;\n\t\tacct_gather_energy_req_msg_t *req = msg->data;\n\n\t\tacct_gather_energy_g_get_data(ENERGY_DATA_LAST_POLL,\n\t\t\t\t\t      &last_poll);\n\t\tacct_gather_energy_g_get_data(ENERGY_DATA_SENSOR_CNT,\n\t\t\t\t\t      &sensor_cnt);\n\n\t\t/* If we polled later than delta seconds then force a\n\t\t   new poll.\n\t\t*/\n\t\tif ((now - last_poll) > req->delta)\n\t\t\tdata_type = ENERGY_DATA_JOULES_TASK;\n\n\t\tmemset(&acct_msg, 0, sizeof(acct_gather_node_resp_msg_t));\n\t\tacct_msg.sensor_cnt = sensor_cnt;\n\t\tacct_msg.energy = acct_gather_energy_alloc(acct_msg.sensor_cnt);\n\n\t\tacct_gather_energy_g_get_data(data_type, acct_msg.energy);\n\n\t\tslurm_msg_t_copy(&resp_msg, msg);\n\t\tresp_msg.msg_type = RESPONSE_ACCT_GATHER_ENERGY;\n\t\tresp_msg.data     = &acct_msg;\n\n\t\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\n\t\tacct_gather_energy_destroy(acct_msg.energy);\n\t}\n\treturn rc;\n}\n\nstatic int\n_signal_jobstep(uint32_t jobid, uint32_t stepid, uid_t req_uid,\n\t\tuint32_t signal)\n{\n\tint               fd, rc = SLURM_SUCCESS;\n\tuid_t uid;\n\tuint16_t protocol_version;\n\n\t/*  There will be no stepd if the prolog is still running\n\t *   Return failure so caller can retry.\n\t */\n\tif (_prolog_is_running (jobid)) {\n\t\tinfo (\"signal %d req for %u.%u while prolog is running.\"\n\t\t      \" Returning failure.\", signal, jobid, stepid);\n\t\treturn SLURM_FAILURE;\n\t}\n\n\tfd = stepd_connect(conf->spooldir, conf->node_name, jobid, stepid,\n\t\t\t   &protocol_version);\n\tif (fd == -1) {\n\t\tdebug(\"signal for nonexistent %u.%u stepd_connect failed: %m\",\n\t\t      jobid, stepid);\n\t\treturn ESLURM_INVALID_JOB_ID;\n\t}\n\n\tif ((int)(uid = stepd_get_uid(fd, protocol_version)) < 0) {\n\t\tdebug(\"_signal_jobstep: couldn't read from the step %u.%u: %m\",\n\t\t      jobid, stepid);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done2;\n\t}\n\n\tif ((req_uid != uid) && (!_slurm_authorized_user(req_uid))) {\n\t\tdebug(\"kill req from uid %ld for job %u.%u owned by uid %ld\",\n\t\t      (long) req_uid, jobid, stepid, (long) uid);\n\t\trc = ESLURM_USER_ID_MISSING;     /* or bad in this case */\n\t\tgoto done2;\n\t}\n\n#ifdef HAVE_AIX\n#  ifdef SIGMIGRATE\n#    ifdef SIGSOUND\n\t/* SIGMIGRATE and SIGSOUND are used to initiate job checkpoint on AIX.\n\t * These signals are not sent to the entire process group, but just a\n\t * single process, namely the PMD. */\n\tif (signal == SIGMIGRATE || signal == SIGSOUND) {\n\t\trc = stepd_signal_task_local(fd, protocol_version,\n\t\t\t\t\t     signal, 0);\n\t\tgoto done2;\n\t}\n#    endif\n#  endif\n#endif\n\n\trc = stepd_signal_container(fd, protocol_version, signal);\n\tif (rc == -1)\n\t\trc = ESLURMD_JOB_NOTRUNNING;\n\ndone2:\n\tclose(fd);\n\treturn rc;\n}\n\nstatic void\n_rpc_signal_tasks(slurm_msg_t *msg)\n{\n\tint               rc = SLURM_SUCCESS;\n\tuid_t             req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t\t conf->auth_info);\n\tkill_tasks_msg_t *req = (kill_tasks_msg_t *) msg->data;\n\tuint32_t flag;\n\tuint32_t sig;\n\n\tflag = req->signal >> 24;\n\tsig  = req->signal & 0xfff;\n\n\tif (flag & KILL_FULL_JOB) {\n\t\tdebug(\"%s: sending signal %u to entire job %u flag %u\",\n\t\t      __func__, sig, req->job_id, flag);\n\t\t_kill_all_active_steps(req->job_id, sig, true);\n\t} else if (flag & KILL_STEPS_ONLY) {\n\t\tdebug(\"%s: sending signal %u to all steps job %u flag %u\",\n\t\t      __func__, sig, req->job_id, flag);\n\t\t_kill_all_active_steps(req->job_id, sig, false);\n\t} else {\n\t\tdebug(\"%s: sending signal %u to step %u.%u flag %u\", __func__,\n\t\t      sig, req->job_id, req->job_step_id, flag);\n\t\trc = _signal_jobstep(req->job_id, req->job_step_id, req_uid,\n\t\t\t\t     req->signal);\n\t}\n\tslurm_send_rc_msg(msg, rc);\n}\n\nstatic void\n_rpc_checkpoint_tasks(slurm_msg_t *msg)\n{\n\tint               fd;\n\tint               rc = SLURM_SUCCESS;\n\tuid_t             req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t\t conf->auth_info);\n\tcheckpoint_tasks_msg_t *req = (checkpoint_tasks_msg_t *) msg->data;\n\tuint16_t protocol_version;\n\tuid_t uid;\n\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->job_step_id, &protocol_version);\n\tif (fd == -1) {\n\t\tdebug(\"checkpoint for nonexistent %u.%u stepd_connect \"\n\t\t      \"failed: %m\", req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done;\n\t}\n\n\tif ((int)(uid = stepd_get_uid(fd, protocol_version)) < 0) {\n\t\tdebug(\"_rpc_checkpoint_tasks: couldn't read from the \"\n\t\t      \"step %u.%u: %m\",\n\t\t      req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done2;\n\t}\n\n\tif ((req_uid != uid) && (!_slurm_authorized_user(req_uid))) {\n\t\tdebug(\"checkpoint req from uid %ld for job %u.%u owned by \"\n\t\t      \"uid %ld\", (long) req_uid, req->job_id, req->job_step_id,\n\t\t      (long) uid);\n\t\trc = ESLURM_USER_ID_MISSING;     /* or bad in this case */\n\t\tgoto done2;\n\t}\n\n\trc = stepd_checkpoint(fd, protocol_version,\n\t\t\t      req->timestamp, req->image_dir);\n\tif (rc == -1)\n\t\trc = ESLURMD_JOB_NOTRUNNING;\n\ndone2:\n\tclose(fd);\ndone:\n\tslurm_send_rc_msg(msg, rc);\n}\n\nstatic void\n_rpc_terminate_tasks(slurm_msg_t *msg)\n{\n\tkill_tasks_msg_t *req = (kill_tasks_msg_t *) msg->data;\n\tint               rc = SLURM_SUCCESS;\n\tint               fd;\n\tuid_t             req_uid, uid;\n\tuint16_t protocol_version;\n\n\tdebug3(\"Entering _rpc_terminate_tasks\");\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->job_step_id, &protocol_version);\n\tif (fd == -1) {\n\t\tdebug(\"kill for nonexistent job %u.%u stepd_connect \"\n\t\t      \"failed: %m\", req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done;\n\t}\n\n\tif ((int)(uid = stepd_get_uid(fd, protocol_version)) < 0) {\n\t\tdebug(\"terminate_tasks couldn't read from the step %u.%u: %m\",\n\t\t      req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done2;\n\t}\n\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tif ((req_uid != uid)\n\t    && (!_slurm_authorized_user(req_uid))) {\n\t\tdebug(\"kill req from uid %ld for job %u.%u owned by uid %ld\",\n\t\t      (long) req_uid, req->job_id, req->job_step_id,\n\t\t      (long) uid);\n\t\trc = ESLURM_USER_ID_MISSING;     /* or bad in this case */\n\t\tgoto done2;\n\t}\n\n\trc = stepd_terminate(fd, protocol_version);\n\tif (rc == -1)\n\t\trc = ESLURMD_JOB_NOTRUNNING;\n\ndone2:\n\tclose(fd);\ndone:\n\tslurm_send_rc_msg(msg, rc);\n}\n\nstatic int\n_rpc_step_complete(slurm_msg_t *msg)\n{\n\tstep_complete_msg_t *req = (step_complete_msg_t *)msg->data;\n\tint               rc = SLURM_SUCCESS;\n\tint               fd;\n\tuid_t             req_uid;\n\tuint16_t protocol_version;\n\n\tdebug3(\"Entering _rpc_step_complete\");\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->job_step_id, &protocol_version);\n\tif (fd == -1) {\n\t\terror(\"stepd_connect to %u.%u failed: %m\",\n\t\t      req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done;\n\t}\n\n\t/* step completion messages are only allowed from other slurmstepd,\n\t   so only root or SlurmUser is allowed here */\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\tdebug(\"step completion from uid %ld for job %u.%u\",\n\t\t      (long) req_uid, req->job_id, req->job_step_id);\n\t\trc = ESLURM_USER_ID_MISSING;     /* or bad in this case */\n\t\tgoto done2;\n\t}\n\n\trc = stepd_completion(fd, protocol_version, req);\n\tif (rc == -1)\n\t\trc = ESLURMD_JOB_NOTRUNNING;\n\ndone2:\n\tclose(fd);\ndone:\n\tslurm_send_rc_msg(msg, rc);\n\n\treturn rc;\n}\n\nstatic void _setup_step_complete_msg(slurm_msg_t *msg, void *data)\n{\n\tslurm_msg_t_init(msg);\n\tmsg->msg_type = REQUEST_STEP_COMPLETE;\n\tmsg->data = data;\n}\n\n/* This step_complete RPC came from slurmstepd because we are using\n * message aggregation configured and we are at the head of the tree.\n * This just adds the message to the list and goes on it's merry way. */\nstatic int\n_rpc_step_complete_aggr(slurm_msg_t *msg)\n{\n\tint rc;\n\tuid_t uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\n\tif (!_slurm_authorized_user(uid)) {\n\t\terror(\"Security violation: step_complete_aggr from uid %d\",\n\t\t      uid);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\treturn SLURM_ERROR;\n\t}\n\n\tif (conf->msg_aggr_window_msgs > 1) {\n\t\tslurm_msg_t *req = xmalloc_nz(sizeof(slurm_msg_t));\n\t\t_setup_step_complete_msg(req, msg->data);\n\t\tmsg->data = NULL;\n\n\t\tmsg_aggr_add_msg(req, 1, NULL);\n\t} else {\n\t\tslurm_msg_t req;\n\t\t_setup_step_complete_msg(&req, msg->data);\n\n\t\twhile (slurm_send_recv_controller_rc_msg(&req, &rc) < 0) {\n\t\t\terror(\"Unable to send step complete, \"\n\t\t\t      \"trying again in a minute: %m\");\n\t\t}\n\t}\n\n\t/* Finish communication with the stepd, we have to wait for\n\t * the message back from the slurmctld or we will cause a race\n\t * condition with srun.\n\t */\n\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\n\treturn SLURM_SUCCESS;\n}\n\n/* Get list of active jobs and steps, xfree returned value */\nstatic char *\n_get_step_list(void)\n{\n\tchar tmp[64];\n\tchar *step_list = NULL;\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tint fd;\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1)\n\t\t\tcontinue;\n\n\t\tif (stepd_state(fd, stepd->protocol_version)\n\t\t    == SLURMSTEPD_NOT_RUNNING) {\n\t\t\tdebug(\"stale domain socket for stepd %u.%u \",\n\t\t\t      stepd->jobid, stepd->stepid);\n\t\t\tclose(fd);\n\t\t\tcontinue;\n\t\t}\n\t\tclose(fd);\n\n\t\tif (step_list)\n\t\t\txstrcat(step_list, \", \");\n\t\tif (stepd->stepid == NO_VAL) {\n\t\t\tsnprintf(tmp, sizeof(tmp), \"%u\",\n\t\t\t\t stepd->jobid);\n\t\t\txstrcat(step_list, tmp);\n\t\t} else {\n\t\t\tsnprintf(tmp, sizeof(tmp), \"%u.%u\",\n\t\t\t\t stepd->jobid, stepd->stepid);\n\t\t\txstrcat(step_list, tmp);\n\t\t}\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\tif (step_list == NULL)\n\t\txstrcat(step_list, \"NONE\");\n\treturn step_list;\n}\n\nstatic int\n_rpc_daemon_status(slurm_msg_t *msg)\n{\n\tslurm_msg_t      resp_msg;\n\tslurmd_status_t *resp = NULL;\n\n\tresp = xmalloc(sizeof(slurmd_status_t));\n\tresp->actual_cpus        = conf->actual_cpus;\n\tresp->actual_boards      = conf->actual_boards;\n\tresp->actual_sockets     = conf->actual_sockets;\n\tresp->actual_cores       = conf->actual_cores;\n\tresp->actual_threads     = conf->actual_threads;\n\tresp->actual_real_mem    = conf->real_memory_size;\n\tresp->actual_tmp_disk    = conf->tmp_disk_space;\n\tresp->booted             = startup;\n\tresp->hostname           = xstrdup(conf->node_name);\n\tresp->step_list          = _get_step_list();\n\tresp->last_slurmctld_msg = last_slurmctld_msg;\n\tresp->pid                = conf->pid;\n\tresp->slurmd_debug       = conf->debug_level;\n\tresp->slurmd_logfile     = xstrdup(conf->logfile);\n\tresp->version            = xstrdup(SLURM_VERSION_STRING);\n\n\tslurm_msg_t_copy(&resp_msg, msg);\n\tresp_msg.msg_type = RESPONSE_SLURMD_STATUS;\n\tresp_msg.data     = resp;\n\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\tslurm_free_slurmd_status(resp);\n\treturn SLURM_SUCCESS;\n}\n\nstatic int\n_rpc_stat_jobacct(slurm_msg_t *msg)\n{\n\tjob_step_id_msg_t *req = (job_step_id_msg_t *)msg->data;\n\tslurm_msg_t        resp_msg;\n\tjob_step_stat_t *resp = NULL;\n\tint fd;\n\tuid_t req_uid, uid;\n\tuint16_t protocol_version;\n\n\tdebug3(\"Entering _rpc_stat_jobacct\");\n\t/* step completion messages are only allowed from other slurmstepd,\n\t   so only root or SlurmUser is allowed here */\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->step_id, &protocol_version);\n\tif (fd == -1) {\n\t\terror(\"stepd_connect to %u.%u failed: %m\",\n\t\t      req->job_id, req->step_id);\n\t\tslurm_send_rc_msg(msg, ESLURM_INVALID_JOB_ID);\n\t\treturn\tESLURM_INVALID_JOB_ID;\n\t}\n\n\tif ((int)(uid = stepd_get_uid(fd, protocol_version)) < 0) {\n\t\tdebug(\"stat_jobacct couldn't read from the step %u.%u: %m\",\n\t\t      req->job_id, req->step_id);\n\t\tclose(fd);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_INVALID_JOB_ID);\n\t\treturn\tESLURM_INVALID_JOB_ID;\n\t}\n\n\t/*\n\t * check that requesting user ID is the SLURM UID or root\n\t */\n\tif ((req_uid != uid) && (!_slurm_authorized_user(req_uid))) {\n\t\terror(\"stat_jobacct from uid %ld for job %u \"\n\t\t      \"owned by uid %ld\",\n\t\t      (long) req_uid, req->job_id, (long) uid);\n\n\t\tif (msg->conn_fd >= 0) {\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\t\tclose(fd);\n\t\t\treturn ESLURM_USER_ID_MISSING;/* or bad in this case */\n\t\t}\n\t}\n\n\tresp = xmalloc(sizeof(job_step_stat_t));\n\tresp->step_pids = xmalloc(sizeof(job_step_pids_t));\n\tresp->step_pids->node_name = xstrdup(conf->node_name);\n\tslurm_msg_t_copy(&resp_msg, msg);\n\tresp->return_code = SLURM_SUCCESS;\n\n\tif (stepd_stat_jobacct(fd, protocol_version, req, resp)\n\t    == SLURM_ERROR) {\n\t\tdebug(\"accounting for nonexistent job %u.%u requested\",\n\t\t      req->job_id, req->step_id);\n\t}\n\n\t/* FIX ME: This should probably happen in the\n\t   stepd_stat_jobacct to get more information about the pids.\n\t*/\n\tif (stepd_list_pids(fd, protocol_version, &resp->step_pids->pid,\n\t\t\t    &resp->step_pids->pid_cnt) == SLURM_ERROR) {\n\t\tdebug(\"No pids for nonexistent job %u.%u requested\",\n\t\t      req->job_id, req->step_id);\n\t}\n\n\tclose(fd);\n\n\tresp_msg.msg_type     = RESPONSE_JOB_STEP_STAT;\n\tresp_msg.data         = resp;\n\n\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\tslurm_free_job_step_stat(resp);\n\treturn SLURM_SUCCESS;\n}\n\nstatic int\n_callerid_find_job(callerid_conn_t conn, uint32_t *job_id)\n{\n\tino_t inode;\n\tpid_t pid;\n\tint rc;\n\n\trc = callerid_find_inode_by_conn(conn, &inode);\n\tif (rc != SLURM_SUCCESS) {\n\t\tdebug3(\"network_callerid inode not found\");\n\t\treturn ESLURM_INVALID_JOB_ID;\n\t}\n\tdebug3(\"network_callerid found inode %lu\", (long unsigned int)inode);\n\n\trc = find_pid_by_inode(&pid, inode);\n\tif (rc != SLURM_SUCCESS) {\n\t\tdebug3(\"network_callerid process not found\");\n\t\treturn ESLURM_INVALID_JOB_ID;\n\t}\n\tdebug3(\"network_callerid found process %d\", (pid_t)pid);\n\n\trc = slurm_pid2jobid(pid, job_id);\n\tif (rc != SLURM_SUCCESS) {\n\t\tdebug3(\"network_callerid job not found\");\n\t\treturn ESLURM_INVALID_JOB_ID;\n\t}\n\tdebug3(\"network_callerid found job %u\", *job_id);\n\treturn SLURM_SUCCESS;\n}\n\nstatic int\n_rpc_network_callerid(slurm_msg_t *msg)\n{\n\tnetwork_callerid_msg_t *req = (network_callerid_msg_t *)msg->data;\n\tslurm_msg_t resp_msg;\n\tnetwork_callerid_resp_t *resp = NULL;\n\n\tuid_t req_uid = -1;\n\tuid_t job_uid = -1;\n\tuint32_t job_id = (uint32_t)NO_VAL;\n\tcallerid_conn_t conn;\n\tint rc = ESLURM_INVALID_JOB_ID;\n\tchar ip_src_str[INET6_ADDRSTRLEN];\n\tchar ip_dst_str[INET6_ADDRSTRLEN];\n\n\tdebug3(\"Entering _rpc_network_callerid\");\n\n\tresp = xmalloc(sizeof(network_callerid_resp_t));\n\tslurm_msg_t_copy(&resp_msg, msg);\n\n\t/* Ideally this would be in an if block only when debug3 is enabled */\n\tinet_ntop(req->af, req->ip_src, ip_src_str, INET6_ADDRSTRLEN);\n\tinet_ntop(req->af, req->ip_dst, ip_dst_str, INET6_ADDRSTRLEN);\n\tdebug3(\"network_callerid checking %s:%u => %s:%u\",\n\t\tip_src_str, req->port_src, ip_dst_str, req->port_dst);\n\n\t/* My remote is the other's source */\n\tmemcpy((void*)&conn.ip_dst, (void*)&req->ip_src, 16);\n\tmemcpy((void*)&conn.ip_src, (void*)&req->ip_dst, 16);\n\tconn.port_src = req->port_dst;\n\tconn.port_dst = req->port_src;\n\tconn.af = req->af;\n\n\t/* Find the job id */\n\trc = _callerid_find_job(conn, &job_id);\n\tif (rc == SLURM_SUCCESS) {\n\t\t/* We found the job */\n\t\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\t\tif (!_slurm_authorized_user(req_uid)) {\n\t\t\t/* Requestor is not root or SlurmUser */\n\t\t\tjob_uid = _get_job_uid(job_id);\n\t\t\tif (job_uid != req_uid) {\n\t\t\t\t/* RPC call sent by non-root user who does not\n\t\t\t\t * own this job. Do not send them the job ID. */\n\t\t\t\terror(\"Security violation, REQUEST_NETWORK_CALLERID from uid=%d\",\n\t\t\t\t      req_uid);\n\t\t\t\tjob_id = NO_VAL;\n\t\t\t\trc = ESLURM_INVALID_JOB_ID;\n\t\t\t}\n\t\t}\n\t}\n\n\tresp->job_id = job_id;\n\tresp->node_name = xstrdup(conf->node_name);\n\n\tresp_msg.msg_type = RESPONSE_NETWORK_CALLERID;\n\tresp_msg.data     = resp;\n\n\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\tslurm_free_network_callerid_resp(resp);\n\treturn rc;\n}\n\nstatic int\n_rpc_list_pids(slurm_msg_t *msg)\n{\n\tjob_step_id_msg_t *req = (job_step_id_msg_t *)msg->data;\n\tslurm_msg_t        resp_msg;\n\tjob_step_pids_t *resp = NULL;\n\tint fd;\n\tuid_t req_uid;\n\tuid_t job_uid;\n\tuint16_t protocol_version = 0;\n\n\tdebug3(\"Entering _rpc_list_pids\");\n\t/* step completion messages are only allowed from other slurmstepd,\n\t * so only root or SlurmUser is allowed here */\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\n\tjob_uid = _get_job_uid(req->job_id);\n\n\tif ((int)job_uid < 0) {\n\t\terror(\"stat_pid for invalid job_id: %u\",\n\t\t      req->job_id);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_INVALID_JOB_ID);\n\t\treturn  ESLURM_INVALID_JOB_ID;\n\t}\n\n\t/*\n\t * check that requesting user ID is the SLURM UID or root\n\t */\n\tif ((req_uid != job_uid)\n\t    && (!_slurm_authorized_user(req_uid))) {\n\t\terror(\"stat_pid from uid %ld for job %u \"\n\t\t      \"owned by uid %ld\",\n\t\t      (long) req_uid, req->job_id, (long) job_uid);\n\n\t\tif (msg->conn_fd >= 0) {\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\t\treturn ESLURM_USER_ID_MISSING;/* or bad in this case */\n\t\t}\n\t}\n\n\tresp = xmalloc(sizeof(job_step_pids_t));\n\tslurm_msg_t_copy(&resp_msg, msg);\n\tresp->node_name = xstrdup(conf->node_name);\n\tresp->pid_cnt = 0;\n\tresp->pid = NULL;\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->step_id, &protocol_version);\n\tif (fd == -1) {\n\t\terror(\"stepd_connect to %u.%u failed: %m\",\n\t\t      req->job_id, req->step_id);\n\t\tslurm_send_rc_msg(msg, ESLURM_INVALID_JOB_ID);\n\t\tslurm_free_job_step_pids(resp);\n\t\treturn  ESLURM_INVALID_JOB_ID;\n\n\t}\n\n\tif (stepd_list_pids(fd, protocol_version,\n\t\t\t    &resp->pid, &resp->pid_cnt) == SLURM_ERROR) {\n\t\tdebug(\"No pids for nonexistent job %u.%u requested\",\n\t\t      req->job_id, req->step_id);\n\t}\n\n\tclose(fd);\n\n\tresp_msg.msg_type = RESPONSE_JOB_STEP_PIDS;\n\tresp_msg.data     = resp;\n\n\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\tslurm_free_job_step_pids(resp);\n\treturn SLURM_SUCCESS;\n}\n\n/*\n *  For the specified job_id: reply to slurmctld,\n *   sleep(configured kill_wait), then send SIGKILL\n */\nstatic void\n_rpc_timelimit(slurm_msg_t *msg)\n{\n\tuid_t           uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t   conf->auth_info);\n\tkill_job_msg_t *req = msg->data;\n\tint             nsteps, rc;\n\n\tif (!_slurm_authorized_user(uid)) {\n\t\terror (\"Security violation: rpc_timelimit req from uid %d\",\n\t\t       uid);\n\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\treturn;\n\t}\n\n\t/*\n\t *  Indicate to slurmctld that we've received the message\n\t */\n\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\tslurm_close(msg->conn_fd);\n\tmsg->conn_fd = -1;\n\n\tif (req->step_id != NO_VAL) {\n\t\tslurm_ctl_conf_t *cf;\n\t\tint delay;\n\t\t/* A jobstep has timed out:\n\t\t * - send the container a SIG_TIME_LIMIT or SIG_PREEMPTED\n\t\t *   to log the event\n\t\t * - send a SIGCONT to resume any suspended tasks\n\t\t * - send a SIGTERM to begin termination\n\t\t * - sleep KILL_WAIT\n\t\t * - send a SIGKILL to clean up\n\t\t */\n\t\tif (msg->msg_type == REQUEST_KILL_TIMELIMIT) {\n\t\t\trc = _signal_jobstep(req->job_id, req->step_id, uid,\n\t\t\t\t\t     SIG_TIME_LIMIT);\n\t\t} else {\n\t\t\trc = _signal_jobstep(req->job_id, req->step_id, uid,\n\t\t\t\t\t     SIG_PREEMPTED);\n\t\t}\n\t\tif (rc != SLURM_SUCCESS)\n\t\t\treturn;\n\t\trc = _signal_jobstep(req->job_id, req->step_id, uid, SIGCONT);\n\t\tif (rc != SLURM_SUCCESS)\n\t\t\treturn;\n\t\trc = _signal_jobstep(req->job_id, req->step_id, uid, SIGTERM);\n\t\tif (rc != SLURM_SUCCESS)\n\t\t\treturn;\n\t\tcf = slurm_conf_lock();\n\t\tdelay = MAX(cf->kill_wait, 5);\n\t\tslurm_conf_unlock();\n\t\tsleep(delay);\n\t\t_signal_jobstep(req->job_id, req->step_id, uid, SIGKILL);\n\t\treturn;\n\t}\n\n\tif (msg->msg_type == REQUEST_KILL_TIMELIMIT)\n\t\t_kill_all_active_steps(req->job_id, SIG_TIME_LIMIT, true);\n\telse /* (msg->type == REQUEST_KILL_PREEMPTED) */\n\t\t_kill_all_active_steps(req->job_id, SIG_PREEMPTED, true);\n\tnsteps = _kill_all_active_steps(req->job_id, SIGTERM, false);\n\tverbose( \"Job %u: timeout: sent SIGTERM to %d active steps\",\n\t\t req->job_id, nsteps );\n\n\t/* Revoke credential, send SIGKILL, run epilog, etc. */\n\t_rpc_terminate_job(msg);\n}\n\nstatic void  _rpc_pid2jid(slurm_msg_t *msg)\n{\n\tjob_id_request_msg_t *req = (job_id_request_msg_t *) msg->data;\n\tslurm_msg_t           resp_msg;\n\tjob_id_response_msg_t resp;\n\tbool         found = false;\n\tList         steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tint fd;\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1)\n\t\t\tcontinue;\n\n\t\tif (stepd_pid_in_container(\n\t\t\t    fd, stepd->protocol_version,\n\t\t\t    req->job_pid)\n\t\t    || req->job_pid == stepd_daemon_pid(\n\t\t\t    fd, stepd->protocol_version)) {\n\t\t\tslurm_msg_t_copy(&resp_msg, msg);\n\t\t\tresp.job_id = stepd->jobid;\n\t\t\tresp.return_code = SLURM_SUCCESS;\n\t\t\tfound = true;\n\t\t\tclose(fd);\n\t\t\tbreak;\n\t\t}\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\tif (found) {\n\t\tdebug3(\"_rpc_pid2jid: pid(%u) found in %u\",\n\t\t       req->job_pid, resp.job_id);\n\t\tresp_msg.address      = msg->address;\n\t\tresp_msg.msg_type     = RESPONSE_JOB_ID;\n\t\tresp_msg.data         = &resp;\n\n\t\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\t} else {\n\t\tdebug3(\"_rpc_pid2jid: pid(%u) not found\", req->job_pid);\n\t\tslurm_send_rc_msg(msg, ESLURM_INVALID_JOB_ID);\n\t}\n}\n\n/* Validate sbcast credential.\n * NOTE: We can only perform the full credential validation once with\n * Munge without generating a credential replay error\n * RET SLURM_SUCCESS or an error code */\nstatic int\n_valid_sbcast_cred(file_bcast_msg_t *req, uid_t req_uid, uint16_t block_no,\n\t\t   uint32_t *job_id)\n{\n\tint rc = SLURM_SUCCESS;\n\tchar *nodes = NULL;\n\thostset_t hset = NULL;\n\n\t*job_id = NO_VAL;\n\trc = extract_sbcast_cred(conf->vctx, req->cred, block_no,\n\t\t\t\t job_id, &nodes);\n\tif (rc != 0) {\n\t\terror(\"Security violation: Invalid sbcast_cred from uid %d\",\n\t\t      req_uid);\n\t\treturn ESLURMD_INVALID_JOB_CREDENTIAL;\n\t}\n\n\tif (!(hset = hostset_create(nodes))) {\n\t\terror(\"Unable to parse sbcast_cred hostlist %s\", nodes);\n\t\trc = ESLURMD_INVALID_JOB_CREDENTIAL;\n\t} else if (!hostset_within(hset, conf->node_name)) {\n\t\terror(\"Security violation: sbcast_cred from %d has \"\n\t\t      \"bad hostset %s\", req_uid, nodes);\n\t\trc = ESLURMD_INVALID_JOB_CREDENTIAL;\n\t}\n\tif (hset)\n\t\thostset_destroy(hset);\n\txfree(nodes);\n\n\t/* print_sbcast_cred(req->cred); */\n\n\treturn rc;\n}\n\nstatic void _fb_rdlock(void)\n{\n\tslurm_mutex_lock(&file_bcast_mutex);\n\twhile (1) {\n\t\tif ((fb_write_wait_lock == 0) && (fb_write_lock == 0)) {\n\t\t\tfb_read_lock++;\n\t\t\tbreak;\n\t\t} else {\t/* wait for state change and retry */\n\t\t\tpthread_cond_wait(&file_bcast_cond, &file_bcast_mutex);\n\t\t}\n\t}\n\tslurm_mutex_unlock(&file_bcast_mutex);\n}\n\nstatic void _fb_rdunlock(void)\n{\n\tslurm_mutex_lock(&file_bcast_mutex);\n\tfb_read_lock--;\n\tpthread_cond_broadcast(&file_bcast_cond);\n\tslurm_mutex_unlock(&file_bcast_mutex);\n}\n\nstatic void _fb_wrlock(void)\n{\n\tslurm_mutex_lock(&file_bcast_mutex);\n\tfb_write_wait_lock++;\n\twhile (1) {\n\t\tif ((fb_read_lock == 0) && (fb_write_lock == 0)) {\n\t\t\tfb_write_lock++;\n\t\t\tfb_write_wait_lock--;\n\t\t\tbreak;\n\t\t} else {\t/* wait for state change and retry */\n\t\t\tpthread_cond_wait(&file_bcast_cond, &file_bcast_mutex);\n\t\t}\n\t}\n\tslurm_mutex_unlock(&file_bcast_mutex);\n}\n\nstatic void _fb_wrunlock(void)\n{\n\tslurm_mutex_lock(&file_bcast_mutex);\n\tfb_write_lock--;\n\tpthread_cond_broadcast(&file_bcast_cond);\n\tslurm_mutex_unlock(&file_bcast_mutex);\n}\n\nstatic int _bcast_find_in_list(void *x, void *y)\n{\n\tfile_bcast_info_t *info = (file_bcast_info_t *)x;\n\tfile_bcast_info_t *key = (file_bcast_info_t *)y;\n\t/* uid, job_id, and fname must match */\n\treturn ((info->uid == key->uid)\n\t\t&& (info->job_id == key->job_id)\n\t\t&& (!xstrcmp(info->fname, key->fname)));\n}\n\n/* must have read lock */\nstatic file_bcast_info_t *_bcast_lookup_file(file_bcast_info_t *key)\n{\n\treturn list_find_first(file_bcast_list, _bcast_find_in_list, key);\n}\n\n/* must not have read lock, will get write lock */\nstatic void _file_bcast_close_file(file_bcast_info_t *key)\n{\n\t_fb_wrlock();\n\tlist_delete_all(file_bcast_list, _bcast_find_in_list, key);\n\t_fb_wrunlock();\n}\n\nstatic void _free_file_bcast_info_t(file_bcast_info_t *f)\n{\n\txfree(f->fname);\n\tif (f->fd)\n\t\tclose(f->fd);\n\txfree(f);\n}\n\nstatic int _bcast_find_in_list_to_remove(void *x, void *y)\n{\n\tfile_bcast_info_t *f = (file_bcast_info_t *)x;\n\ttime_t *now = (time_t *) y;\n\n\tif (f->last_update + FILE_BCAST_TIMEOUT < *now) {\n\t\terror(\"Removing stalled file_bcast transfer from uid \"\n\t\t      \"%u to file `%s`\", f->uid, f->fname);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/* remove transfers that have stalled */\nstatic void _file_bcast_cleanup(void)\n{\n\ttime_t now = time(NULL);\n\n\t_fb_wrlock();\n\tlist_delete_all(file_bcast_list, _bcast_find_in_list_to_remove, &now);\n\t_fb_wrunlock();\n}\n\nvoid file_bcast_init(void)\n{\n\t/* skip locks during slurmd init */\n\tfile_bcast_list = list_create((ListDelF) _free_file_bcast_info_t);\n}\n\nvoid file_bcast_purge(void)\n{\n\t_fb_wrlock();\n\tlist_destroy(file_bcast_list);\n\t/* destroying list before exit, no need to unlock */\n}\n\nstatic int _rpc_file_bcast(slurm_msg_t *msg)\n{\n\tint rc, offset, inx;\n\tfile_bcast_info_t *file_info;\n\tfile_bcast_msg_t *req = msg->data;\n\tfile_bcast_info_t key;\n\n\tkey.uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tkey.gid = g_slurm_auth_get_gid(msg->auth_cred, conf->auth_info);\n\tkey.fname = req->fname;\n\n\trc = _valid_sbcast_cred(req, key.uid, req->block_no, &key.job_id);\n\tif ((rc != SLURM_SUCCESS) && !_slurm_authorized_user(key.uid))\n\t\treturn rc;\n\n#if 0\n\tinfo(\"last_block=%u force=%u modes=%o\",\n\t     req->last_block, req->force, req->modes);\n\tinfo(\"uid=%u gid=%u atime=%lu mtime=%lu block_len[0]=%u\",\n\t     req->uid, req->gid, req->atime, req->mtime, req->block_len);\n#if 0\n\t/* when the file being transferred is binary, the following line\n\t * can break the terminal output for slurmd */\n\tinfo(\"req->block[0]=%s, @ %lu\", \\\n\t     req->block[0], (unsigned long) &req->block);\n#endif\n#endif\n\n\tif (req->block_no == 1) {\n\t\tinfo(\"sbcast req_uid=%u job_id=%u fname=%s block_no=%u\",\n\t\t     key.uid, key.job_id, key.fname, req->block_no);\n\t} else {\n\t\tdebug(\"sbcast req_uid=%u job_id=%u fname=%s block_no=%u\",\n\t\t      key.uid, key.job_id, key.fname, req->block_no);\n\t}\n\n\t/* first block must register the file and open fd/mmap */\n\tif (req->block_no == 1) {\n\t\tif ((rc = _file_bcast_register_file(msg, &key)))\n\t\t\treturn rc;\n\t}\n\n\t_fb_rdlock();\n\tif (!(file_info = _bcast_lookup_file(&key))) {\n\t\terror(\"No registered file transfer for uid %u file `%s`.\",\n\t\t      key.uid, key.fname);\n\t\t_fb_rdunlock();\n\t\treturn SLURM_ERROR;\n\t}\n\n\t/* now decompress file */\n\tif (bcast_decompress_data(req) < 0) {\n\t\terror(\"sbcast: data decompression error for UID %u, file %s\",\n\t\t      key.uid, key.fname);\n\t\t_fb_rdunlock();\n\t\treturn SLURM_FAILURE;\n\t}\n\n\toffset = 0;\n\twhile (req->block_len - offset) {\n\t\tinx = write(file_info->fd, &req->block[offset],\n\t\t\t    (req->block_len - offset));\n\t\tif (inx == -1) {\n\t\t\tif ((errno == EINTR) || (errno == EAGAIN))\n\t\t\t\tcontinue;\n\t\t\terror(\"sbcast: uid:%u can't write `%s`: %m\",\n\t\t\t      key.uid, key.fname);\n\t\t\t_fb_rdunlock();\n\t\t\treturn SLURM_FAILURE;\n\t\t}\n\t\toffset += inx;\n\t}\n\n\tfile_info->last_update = time(NULL);\n\n\tif (req->last_block && fchmod(file_info->fd, (req->modes & 0777))) {\n\t\terror(\"sbcast: uid:%u can't chmod `%s`: %m\",\n\t\t      key.uid, key.fname);\n\t}\n\tif (req->last_block && fchown(file_info->fd, key.uid, key.gid)) {\n\t\terror(\"sbcast: uid:%u gid:%u can't chown `%s`: %m\",\n\t\t      key.uid, key.gid, key.fname);\n\t}\n\tif (req->last_block && req->atime) {\n\t\tstruct utimbuf time_buf;\n\t\ttime_buf.actime  = req->atime;\n\t\ttime_buf.modtime = req->mtime;\n\t\tif (utime(key.fname, &time_buf)) {\n\t\t\terror(\"sbcast: uid:%u can't utime `%s`: %m\",\n\t\t\t      key.uid, key.fname);\n\t\t}\n\t}\n\n\t_fb_rdunlock();\n\n\tif (req->last_block) {\n\t\t_file_bcast_close_file(&key);\n\t}\n\treturn SLURM_SUCCESS;\n}\n\n/* pass an open file descriptor back to the parent process */\nstatic void _send_back_fd(int socket, int fd)\n{\n\tstruct msghdr msg = { 0 };\n\tstruct cmsghdr *cmsg;\n\tchar buf[CMSG_SPACE(sizeof(fd))];\n\tmemset(buf, '\\0', sizeof(buf));\n\n\tmsg.msg_iov = NULL;\n\tmsg.msg_iovlen = 0;\n\tmsg.msg_control = buf;\n\tmsg.msg_controllen = sizeof(buf);\n\n\tcmsg = CMSG_FIRSTHDR(&msg);\n\tcmsg->cmsg_level = SOL_SOCKET;\n\tcmsg->cmsg_type = SCM_RIGHTS;\n\tcmsg->cmsg_len = CMSG_LEN(sizeof(fd));\n\n\tmemmove(CMSG_DATA(cmsg), &fd, sizeof(fd));\n\tmsg.msg_controllen = cmsg->cmsg_len;\n\n\tif (sendmsg(socket, &msg, 0) < 0)\n\t\terror(\"%s: failed to send fd: %m\", __func__);\n}\n\n/* receive an open file descriptor from fork()'d child over unix socket */\nstatic int _receive_fd(int socket)\n{\n\tstruct msghdr msg = {0};\n\tstruct cmsghdr *cmsg;\n\tint fd;\n\tmsg.msg_iov = NULL;\n\tmsg.msg_iovlen = 0;\n\tchar c_buffer[256];\n\tmsg.msg_control = c_buffer;\n\tmsg.msg_controllen = sizeof(c_buffer);\n\n\tif (recvmsg(socket, &msg, 0) < 0) {\n\t\terror(\"%s: failed to receive fd: %m\", __func__);\n\t\treturn -1;\n\t}\n\n\tcmsg = CMSG_FIRSTHDR(&msg);\n\tmemmove(&fd, CMSG_DATA(cmsg), sizeof(fd));\n\treturn fd;\n}\n\nstatic int _file_bcast_register_file(slurm_msg_t *msg,\n\t\t\t\t     file_bcast_info_t *key)\n{\n\tfile_bcast_msg_t *req = msg->data;\n\tint fd, flags, rc;\n\tint pipe[2];\n\tgids_t *gids;\n\tpid_t child;\n\tfile_bcast_info_t *file_info;\n\n\tif (!(gids = _gids_cache_lookup(req->user_name, key->gid))) {\n\t\terror(\"sbcast: gids_cache_lookup for %s failed\", req->user_name);\n\t\treturn SLURM_ERROR;\n\t}\n\n\tif ((rc = container_g_create(key->job_id))) {\n\t\terror(\"sbcast: container_g_create(%u): %m\", key->job_id);\n\t\t_dealloc_gids(gids);\n\t\treturn rc;\n\t}\n\n\t/* child process will setuid to the user, register the process\n\t * with the container, and open the file for us. */\n\n\tif (socketpair(AF_UNIX, SOCK_DGRAM, 0, pipe) != 0) {\n\t\terror(\"%s: Failed to open pipe: %m\", __func__);\n\t\t_dealloc_gids(gids);\n\t\treturn SLURM_ERROR;\n\t}\n\n\tchild = fork();\n\tif (child == -1) {\n\t\terror(\"sbcast: fork failure\");\n\t\t_dealloc_gids(gids);\n\t\tclose(pipe[0]);\n\t\tclose(pipe[1]);\n\t\treturn errno;\n\t} else if (child > 0) {\n\t\t/* get fd back from pipe */\n\t\tclose(pipe[0]);\n\t\twaitpid(child, &rc, 0);\n\t\t_dealloc_gids(gids);\n\t\tif (rc) {\n\t\t\tclose(pipe[1]);\n\t\t\treturn WEXITSTATUS(rc);\n\t\t}\n\n\t\tfd = _receive_fd(pipe[1]);\n\t\tclose(pipe[1]);\n\n\t\tfile_info = xmalloc(sizeof(file_bcast_info_t));\n\t\tfile_info->fd = fd;\n\t\tfile_info->fname = xstrdup(req->fname);\n\t\tfile_info->uid = key->uid;\n\t\tfile_info->gid = key->gid;\n\t\tfile_info->job_id = key->job_id;\n\t\tfile_info->start_time = time(NULL);\n\n\t\t//TODO: mmap the file here\n\t\t_fb_wrlock();\n\t\tlist_append(file_bcast_list, file_info);\n\t\t_fb_wrunlock();\n\n\t\treturn SLURM_SUCCESS;\n\t}\n\n\t/* child process below here */\n\n\tclose(pipe[1]);\n\n\t/* container_g_add_pid needs to be called in the\n\t   forked process part of the fork to avoid a race\n\t   condition where if this process makes a file or\n\t   detacts itself from a child before we add the pid\n\t   to the container in the parent of the fork.\n\t*/\n\tif (container_g_add_pid(key->job_id, getpid(), key->uid)) {\n\t\terror(\"container_g_add_pid(%u): %m\", key->job_id);\n\t\texit(SLURM_ERROR);\n\t}\n\n\t/* The child actually performs the I/O and exits with\n\t * a return code, do not return! */\n\n\t/*********************************************************************\\\n\t * NOTE: It would be best to do an exec() immediately after the fork()\n\t * in order to help prevent a possible deadlock in the child process\n\t * due to locks being set at the time of the fork and being freed by\n\t * the parent process, but not freed by the child process. Performing\n\t * the work inline is done for simplicity. Note that the logging\n\t * performed by error() should be safe due to the use of\n\t * atfork_install_handlers() as defined in src/common/log.c.\n\t * Change the code below with caution.\n\t\\*********************************************************************/\n\n\tif (setgroups(gids->ngids, gids->gids) < 0) {\n\t\terror(\"sbcast: uid: %u setgroups failed: %m\", key->uid);\n\t\texit(errno);\n\t}\n\t_dealloc_gids(gids);\n\n\tif (setgid(key->gid) < 0) {\n\t\terror(\"sbcast: uid:%u setgid(%u): %m\", key->uid, key->gid);\n\t\texit(errno);\n\t}\n\tif (setuid(key->uid) < 0) {\n\t\terror(\"sbcast: getuid(%u): %m\", key->uid);\n\t\texit(errno);\n\t}\n\n\tflags = O_WRONLY | O_CREAT;\n\tif (req->force)\n\t\tflags |= O_TRUNC;\n\telse\n\t\tflags |= O_EXCL;\n\n\tfd = open(key->fname, flags, 0700);\n\tif (fd == -1) {\n\t\terror(\"sbcast: uid:%u can't open `%s`: %m\",\n\t\t      key->uid, key->fname);\n\t\texit(errno);\n\t}\n\t_send_back_fd(pipe[0], fd);\n\tclose(fd);\n\texit(SLURM_SUCCESS);\n}\n\nstatic void\n_rpc_reattach_tasks(slurm_msg_t *msg)\n{\n\treattach_tasks_request_msg_t  *req = msg->data;\n\treattach_tasks_response_msg_t *resp =\n\t\txmalloc(sizeof(reattach_tasks_response_msg_t));\n\tslurm_msg_t                    resp_msg;\n\tint          rc   = SLURM_SUCCESS;\n\tuint16_t     port = 0;\n\tchar         host[MAXHOSTNAMELEN];\n\tslurm_addr_t   ioaddr;\n\tvoid        *job_cred_sig;\n\tuint32_t     len;\n\tint               fd;\n\tuid_t             req_uid;\n\tslurm_addr_t *cli = &msg->orig_addr;\n\tuint32_t nodeid = (uint32_t)NO_VAL;\n\tuid_t uid = -1;\n\tuint16_t protocol_version;\n\n\tslurm_msg_t_copy(&resp_msg, msg);\n\tfd = stepd_connect(conf->spooldir, conf->node_name,\n\t\t\t   req->job_id, req->job_step_id, &protocol_version);\n\tif (fd == -1) {\n\t\tdebug(\"reattach for nonexistent job %u.%u stepd_connect\"\n\t\t      \" failed: %m\", req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done;\n\t}\n\n\tif ((int)(uid = stepd_get_uid(fd, protocol_version)) < 0) {\n\t\tdebug(\"_rpc_reattach_tasks couldn't read from the \"\n\t\t      \"step %u.%u: %m\",\n\t\t      req->job_id, req->job_step_id);\n\t\trc = ESLURM_INVALID_JOB_ID;\n\t\tgoto done2;\n\t}\n\n\tnodeid = stepd_get_nodeid(fd, protocol_version);\n\n\tdebug2(\"_rpc_reattach_tasks: nodeid %d in the job step\", nodeid);\n\n\treq_uid = g_slurm_auth_get_uid(msg->auth_cred, conf->auth_info);\n\tif ((req_uid != uid) && (!_slurm_authorized_user(req_uid))) {\n\t\terror(\"uid %ld attempt to attach to job %u.%u owned by %ld\",\n\t\t      (long) req_uid, req->job_id, req->job_step_id,\n\t\t      (long) uid);\n\t\trc = EPERM;\n\t\tgoto done2;\n\t}\n\n\tmemset(resp, 0, sizeof(reattach_tasks_response_msg_t));\n\tslurm_get_ip_str(cli, &port, host, sizeof(host));\n\n\t/*\n\t * Set response address by resp_port and client address\n\t */\n\tmemcpy(&resp_msg.address, cli, sizeof(slurm_addr_t));\n\tif (req->num_resp_port > 0) {\n\t\tport = req->resp_port[nodeid % req->num_resp_port];\n\t\tslurm_set_addr(&resp_msg.address, port, NULL);\n\t}\n\n\t/*\n\t * Set IO address by io_port and client address\n\t */\n\tmemcpy(&ioaddr, cli, sizeof(slurm_addr_t));\n\n\tif (req->num_io_port > 0) {\n\t\tport = req->io_port[nodeid % req->num_io_port];\n\t\tslurm_set_addr(&ioaddr, port, NULL);\n\t}\n\n\t/*\n\t * Get the signature of the job credential.  slurmstepd will need\n\t * this to prove its identity when it connects back to srun.\n\t */\n\tslurm_cred_get_signature(req->cred, (char **)(&job_cred_sig), &len);\n\tif (len != SLURM_IO_KEY_SIZE) {\n\t\terror(\"Incorrect slurm cred signature length\");\n\t\tgoto done2;\n\t}\n\n\tresp->gtids = NULL;\n\tresp->local_pids = NULL;\n\n\t /* NOTE: We need to use the protocol_version from\n\t  * sattach here since responses will be sent back to it. */\n\tif (msg->protocol_version < protocol_version)\n\t\tprotocol_version = msg->protocol_version;\n\n\t/* Following call fills in gtids and local_pids when successful. */\n\trc = stepd_attach(fd, protocol_version, &ioaddr,\n\t\t\t  &resp_msg.address, job_cred_sig, resp);\n\tif (rc != SLURM_SUCCESS) {\n\t\tdebug2(\"stepd_attach call failed\");\n\t\tgoto done2;\n\t}\n\ndone2:\n\tclose(fd);\ndone:\n\tdebug2(\"update step addrs rc = %d\", rc);\n\tresp_msg.data         = resp;\n\tresp_msg.msg_type     = RESPONSE_REATTACH_TASKS;\n\tresp->node_name       = xstrdup(conf->node_name);\n\tresp->return_code     = rc;\n\tdebug2(\"node %s sending rc = %d\", conf->node_name, rc);\n\n\tslurm_send_node_msg(msg->conn_fd, &resp_msg);\n\tslurm_free_reattach_tasks_response_msg(resp);\n}\n\nstatic uid_t _get_job_uid(uint32_t jobid)\n{\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\tuid_t uid = -1;\n\tint fd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif (stepd->jobid != jobid) {\n\t\t\t/* multiple jobs expected on shared nodes */\n\t\t\tcontinue;\n\t\t}\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1) {\n\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\t\tuid = stepd_get_uid(fd, stepd->protocol_version);\n\n\t\tclose(fd);\n\t\tif ((int)uid < 0) {\n\t\t\tdebug(\"stepd_get_uid failed %u.%u: %m\",\n\t\t\t      stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\t\tbreak;\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\treturn uid;\n}\n\n/*\n * _kill_all_active_steps - signals the container of all steps of a job\n * jobid IN - id of job to signal\n * sig   IN - signal to send\n * batch IN - if true signal batch script, otherwise skip it\n * RET count of signaled job steps (plus batch script, if applicable)\n */\nstatic int\n_kill_all_active_steps(uint32_t jobid, int sig, bool batch)\n{\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\tint step_cnt  = 0;\n\tint fd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif (stepd->jobid != jobid) {\n\t\t\t/* multiple jobs expected on shared nodes */\n\t\t\tdebug3(\"Step from other job: jobid=%u (this jobid=%u)\",\n\t\t\t       stepd->jobid, jobid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif ((stepd->stepid == SLURM_BATCH_SCRIPT) && (!batch))\n\t\t\tcontinue;\n\n\t\tstep_cnt++;\n\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1) {\n\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdebug2(\"container signal %d to job %u.%u\",\n\t\t       sig, jobid, stepd->stepid);\n\t\tif (stepd_signal_container(\n\t\t\t    fd, stepd->protocol_version, sig) < 0)\n\t\t\tdebug(\"kill jobid=%u failed: %m\", jobid);\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\tif (step_cnt == 0)\n\t\tdebug2(\"No steps in jobid %u to send signal %d\", jobid, sig);\n\treturn step_cnt;\n}\n\n/*\n * _terminate_all_steps - signals the container of all steps of a job\n * jobid IN - id of job to signal\n * batch IN - if true signal batch script, otherwise skip it\n * RET count of signaled job steps (plus batch script, if applicable)\n */\nstatic int\n_terminate_all_steps(uint32_t jobid, bool batch)\n{\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\tint step_cnt  = 0;\n\tint fd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif (stepd->jobid != jobid) {\n\t\t\t/* multiple jobs expected on shared nodes */\n\t\t\tdebug3(\"Step from other job: jobid=%u (this jobid=%u)\",\n\t\t\t       stepd->jobid, jobid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif ((stepd->stepid == SLURM_BATCH_SCRIPT) && (!batch))\n\t\t\tcontinue;\n\n\t\tstep_cnt++;\n\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1) {\n\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdebug2(\"terminate job step %u.%u\", jobid, stepd->stepid);\n\t\tif (stepd_terminate(fd, stepd->protocol_version) < 0)\n\t\t\tdebug(\"kill jobid=%u.%u failed: %m\", jobid,\n\t\t\t      stepd->stepid);\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\tif (step_cnt == 0)\n\t\tdebug2(\"No steps in job %u to terminate\", jobid);\n\treturn step_cnt;\n}\n\nstatic bool\n_job_still_running(uint32_t job_id)\n{\n\tbool         retval = false;\n\tList         steps;\n\tListIterator i;\n\tstep_loc_t  *s     = NULL;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((s = list_next(i))) {\n\t\tif (s->jobid == job_id) {\n\t\t\tint fd;\n\t\t\tfd = stepd_connect(s->directory, s->nodename,\n\t\t\t\t\t   s->jobid, s->stepid,\n\t\t\t\t\t   &s->protocol_version);\n\t\t\tif (fd == -1)\n\t\t\t\tcontinue;\n\n\t\t\tif (stepd_state(fd, s->protocol_version)\n\t\t\t    != SLURMSTEPD_NOT_RUNNING) {\n\t\t\t\tretval = true;\n\t\t\t\tclose(fd);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tclose(fd);\n\t\t}\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\treturn retval;\n}\n\n/*\n * Wait until all job steps are in SLURMSTEPD_NOT_RUNNING state.\n * This indicates that switch_g_job_postfini has completed and\n * freed the switch windows (as needed only for Federation switch).\n */\nstatic void\n_wait_state_completed(uint32_t jobid, int max_delay)\n{\n\tint i;\n\n\tfor (i=0; i<max_delay; i++) {\n\t\tif (_steps_completed_now(jobid))\n\t\t\tbreak;\n\t\tsleep(1);\n\t}\n\tif (i >= max_delay)\n\t\terror(\"timed out waiting for job %u to complete\", jobid);\n}\n\nstatic bool\n_steps_completed_now(uint32_t jobid)\n{\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\tbool rc = true;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif (stepd->jobid == jobid) {\n\t\t\tint fd;\n\t\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t\t   &stepd->protocol_version);\n\t\t\tif (fd == -1)\n\t\t\t\tcontinue;\n\n\t\t\tif (stepd_state(fd, stepd->protocol_version)\n\t\t\t    != SLURMSTEPD_NOT_RUNNING) {\n\t\t\t\trc = false;\n\t\t\t\tclose(fd);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tclose(fd);\n\t\t}\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\treturn rc;\n}\n\nstatic void _epilog_complete_msg_setup(\n\tslurm_msg_t *msg, epilog_complete_msg_t *req, uint32_t jobid, int rc)\n{\n\tslurm_msg_t_init(msg);\n\tmemset(req, 0, sizeof(epilog_complete_msg_t));\n\n\treq->job_id      = jobid;\n\treq->return_code = rc;\n\treq->node_name   = conf->node_name;\n\n\tmsg->msg_type    = MESSAGE_EPILOG_COMPLETE;\n\tmsg->data        = req;\n}\n\n/*\n *  Send epilog complete message to currently active controller.\n *  If enabled, use message aggregation.\n *   Returns SLURM_SUCCESS if message sent successfully,\n *           SLURM_FAILURE if epilog complete message fails to be sent.\n */\nstatic int\n_epilog_complete(uint32_t jobid, int rc)\n{\n\tint ret = SLURM_SUCCESS;\n\n\tif (conf->msg_aggr_window_msgs > 1) {\n\t\t/* message aggregation is enabled */\n\t\tslurm_msg_t *msg = xmalloc(sizeof(slurm_msg_t));\n\t\tepilog_complete_msg_t *req =\n\t\t\txmalloc(sizeof(epilog_complete_msg_t));\n\n\t\t_epilog_complete_msg_setup(msg, req, jobid, rc);\n\n\t\t/* we need to copy this symbol */\n\t\treq->node_name   = xstrdup(conf->node_name);\n\n\t\tmsg_aggr_add_msg(msg, 0, NULL);\n\t} else {\n\t\tslurm_msg_t msg;\n\t\tepilog_complete_msg_t req;\n\n\t\t_epilog_complete_msg_setup(&msg, &req, jobid, rc);\n\n\t\t/* Note: No return code to message, slurmctld will resend\n\t\t * TERMINATE_JOB request if message send fails */\n\t\tif (slurm_send_only_controller_msg(&msg) < 0) {\n\t\t\terror(\"Unable to send epilog complete message: %m\");\n\t\t\tret = SLURM_ERROR;\n\t\t} else {\n\t\t\tdebug(\"Job %u: sent epilog complete msg: rc = %d\",\n\t\t\t      jobid, rc);\n\t\t}\n\t}\n\treturn ret;\n}\n\n\n/*\n * Send a signal through the appropriate slurmstepds for each job step\n * belonging to a given job allocation.\n */\nstatic void\n_rpc_signal_job(slurm_msg_t *msg)\n{\n\tsignal_job_msg_t *req = msg->data;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tuid_t job_uid;\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd = NULL;\n\tint step_cnt  = 0;\n\tint fd;\n\n\tdebug(\"_rpc_signal_job, uid = %d, signal = %d\", req_uid, req->signal);\n\tjob_uid = _get_job_uid(req->job_id);\n\tif ((int)job_uid < 0)\n\t\tgoto no_job;\n\n\t/*\n\t * check that requesting user ID is the SLURM UID or root\n\t */\n\tif ((req_uid != job_uid) && (!_slurm_authorized_user(req_uid))) {\n\t\terror(\"Security violation: kill_job(%u) from uid %d\",\n\t\t      req->job_id, req_uid);\n\t\tif (msg->conn_fd >= 0) {\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\t\terror (\"_rpc_signal_job: close(%d): %m\",\n\t\t\t\t       msg->conn_fd);\n\t\t\tmsg->conn_fd = -1;\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Loop through all job steps for this job and signal the\n\t * step's process group through the slurmstepd.\n\t */\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\tif (stepd->jobid != req->job_id) {\n\t\t\t/* multiple jobs expected on shared nodes */\n\t\t\tdebug3(\"Step from other job: jobid=%u (this jobid=%u)\",\n\t\t\t       stepd->jobid, req->job_id);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (stepd->stepid == SLURM_BATCH_SCRIPT) {\n\t\t\tdebug2(\"batch script itself not signalled\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tstep_cnt++;\n\n\t\tfd = stepd_connect(stepd->directory, stepd->nodename,\n\t\t\t\t   stepd->jobid, stepd->stepid,\n\t\t\t\t   &stepd->protocol_version);\n\t\tif (fd == -1) {\n\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdebug2(\"  signal %d to job %u.%u\",\n\t\t       req->signal, stepd->jobid, stepd->stepid);\n\t\tif (stepd_signal_container(\n\t\t\t    fd, stepd->protocol_version, req->signal) < 0)\n\t\t\tdebug(\"signal jobid=%u failed: %m\", stepd->jobid);\n\t\tclose(fd);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\nno_job:\n\tif (step_cnt == 0) {\n\t\tdebug2(\"No steps in jobid %u to send signal %d\",\n\t\t       req->job_id, req->signal);\n\t}\n\n\t/*\n\t *  At this point, if connection still open, we send controller\n\t *   a \"success\" reply to indicate that we've recvd the msg.\n\t */\n\tif (msg->conn_fd >= 0) {\n\t\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\terror (\"_rpc_signal_job: close(%d): %m\", msg->conn_fd);\n\t\tmsg->conn_fd = -1;\n\t}\n}\n\n/* if a lock is granted to the job then return 1; else return 0 if\n * the lock for the job is already taken or there's no more locks */\nstatic int\n_get_suspend_job_lock(uint32_t job_id)\n{\n\tstatic bool logged = false;\n\tint i, empty_loc = -1, rc = 0;\n\n\tslurm_mutex_lock(&suspend_mutex);\n\tfor (i = 0; i < job_suspend_size; i++) {\n\t\tif (job_suspend_array[i] == 0) {\n\t\t\tempty_loc = i;\n\t\t\tcontinue;\n\t\t}\n\t\tif (job_suspend_array[i] == job_id) {\n\t\t\t/* another thread already a lock for this job ID */\n\t\t\tslurm_mutex_unlock(&suspend_mutex);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\tif (empty_loc != -1) {\n\t\t/* nobody has the lock and here's an available used lock */\n\t\tjob_suspend_array[empty_loc] = job_id;\n\t\trc = 1;\n\t} else if (job_suspend_size < NUM_PARALLEL_SUSP_JOBS) {\n\t\t/* a new lock is available */\n\t\tjob_suspend_array[job_suspend_size++] = job_id;\n\t\trc = 1;\n\t} else if (!logged) {\n\t\terror(\"Simultaneous job suspend/resume limit reached (%d). \"\n\t\t      \"Configure SchedulerTimeSlice higher.\",\n\t\t      NUM_PARALLEL_SUSP_JOBS);\n\t\tlogged = true;\n\t}\n\tslurm_mutex_unlock(&suspend_mutex);\n\treturn rc;\n}\n\nstatic void\n_unlock_suspend_job(uint32_t job_id)\n{\n\tint i;\n\tslurm_mutex_lock(&suspend_mutex);\n\tfor (i = 0; i < job_suspend_size; i++) {\n\t\tif (job_suspend_array[i] == job_id)\n\t\t\tjob_suspend_array[i] = 0;\n\t}\n\tslurm_mutex_unlock(&suspend_mutex);\n}\n\n/* Add record for every launched job so we know they are ready for suspend */\nextern void record_launched_jobs(void)\n{\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\twhile ((stepd = list_next(i))) {\n\t\t_launch_complete_add(stepd->jobid);\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n}\n\n/*\n * Send a job suspend/resume request through the appropriate slurmstepds for\n * each job step belonging to a given job allocation.\n */\nstatic void\n_rpc_suspend_job(slurm_msg_t *msg)\n{\n\tint time_slice = -1;\n\tsuspend_int_msg_t *req = msg->data;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\tList steps;\n\tListIterator i;\n\tstep_loc_t *stepd;\n\tint step_cnt  = 0;\n\tint rc = SLURM_SUCCESS;\n\tDEF_TIMERS;\n\n\tif (time_slice == -1)\n\t\ttime_slice = slurm_get_time_slice();\n\tif ((req->op != SUSPEND_JOB) && (req->op != RESUME_JOB)) {\n\t\terror(\"REQUEST_SUSPEND_INT: bad op code %u\", req->op);\n\t\trc = ESLURM_NOT_SUPPORTED;\n\t}\n\n\t/*\n\t * check that requesting user ID is the SLURM UID or root\n\t */\n\tif (!_slurm_authorized_user(req_uid)) {\n\t\terror(\"Security violation: suspend_job(%u) from uid %d\",\n\t\t      req->job_id, req_uid);\n\t\trc =  ESLURM_USER_ID_MISSING;\n\t}\n\n\t/* send a response now, which will include any errors\n\t * detected with the request */\n\tif (msg->conn_fd >= 0) {\n\t\tslurm_send_rc_msg(msg, rc);\n\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\terror(\"_rpc_suspend_job: close(%d): %m\",\n\t\t\t      msg->conn_fd);\n\t\tmsg->conn_fd = -1;\n\t}\n\tif (rc != SLURM_SUCCESS)\n\t\treturn;\n\n\t/* now we can focus on performing the requested action,\n\t * which could take a few seconds to complete */\n\tdebug(\"_rpc_suspend_job jobid=%u uid=%d action=%s\", req->job_id,\n\t      req_uid, req->op == SUSPEND_JOB ? \"suspend\" : \"resume\");\n\n\t/* Try to get a thread lock for this job. If the lock\n\t * is not available then sleep and try again */\n\twhile (!_get_suspend_job_lock(req->job_id)) {\n\t\tdebug3(\"suspend lock sleep for %u\", req->job_id);\n\t\tusleep(10000);\n\t}\n\tSTART_TIMER;\n\n\t/* Defer suspend until job prolog and launch complete */\n\tif (req->op == SUSPEND_JOB)\n\t\t_launch_complete_wait(req->job_id);\n\n\tif ((req->op == SUSPEND_JOB) && (req->indf_susp))\n\t\tswitch_g_job_suspend(req->switch_info, 5);\n\n\t/* Release or reclaim resources bound to these tasks (task affinity) */\n\tif (req->op == SUSPEND_JOB) {\n\t\t(void) task_g_slurmd_suspend_job(req->job_id);\n\t} else {\n\t\t(void) task_g_slurmd_resume_job(req->job_id);\n\t}\n\n\t/*\n\t * Loop through all job steps and call stepd_suspend or stepd_resume\n\t * as appropriate. Since the \"suspend\" action may contains a sleep\n\t * (if the launch is in progress) suspend multiple jobsteps in parallel.\n\t */\n\tsteps = stepd_available(conf->spooldir, conf->node_name);\n\ti = list_iterator_create(steps);\n\n\twhile (1) {\n\t\tint x, fdi, fd[NUM_PARALLEL_SUSP_STEPS];\n\t\tuint16_t protocol_version[NUM_PARALLEL_SUSP_STEPS];\n\n\t\tfdi = 0;\n\t\twhile ((stepd = list_next(i))) {\n\t\t\tif (stepd->jobid != req->job_id) {\n\t\t\t\t/* multiple jobs expected on shared nodes */\n\t\t\t\tdebug3(\"Step from other job: jobid=%u \"\n\t\t\t\t       \"(this jobid=%u)\",\n\t\t\t\t       stepd->jobid, req->job_id);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tstep_cnt++;\n\n\t\t\tfd[fdi] = stepd_connect(stepd->directory,\n\t\t\t\t\t\tstepd->nodename, stepd->jobid,\n\t\t\t\t\t\tstepd->stepid,\n\t\t\t\t\t\t&protocol_version[fdi]);\n\t\t\tif (fd[fdi] == -1) {\n\t\t\t\tdebug3(\"Unable to connect to step %u.%u\",\n\t\t\t\t       stepd->jobid, stepd->stepid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tfdi++;\n\t\t\tif (fdi >= NUM_PARALLEL_SUSP_STEPS)\n\t\t\t\tbreak;\n\t\t}\n\t\t/* check for open connections */\n\t\tif (fdi == 0)\n\t\t\tbreak;\n\n\t\tif (req->op == SUSPEND_JOB) {\n\t\t\tint susp_fail_count = 0;\n\t\t\t/* The suspend RPCs are processed in parallel for\n\t\t\t * every step in the job */\n\t\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t\t(void) stepd_suspend(fd[x],\n\t\t\t\t\t\t     protocol_version[x],\n\t\t\t\t\t\t     req, 0);\n\t\t\t}\n\t\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t\tif (stepd_suspend(fd[x],\n\t\t\t\t\t\t  protocol_version[x],\n\t\t\t\t\t\t  req, 1) < 0) {\n\t\t\t\t\tsusp_fail_count++;\n\t\t\t\t} else {\n\t\t\t\t\tclose(fd[x]);\n\t\t\t\t\tfd[x] = -1;\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Suspend RPCs can fail at step startup, so retry */\n\t\t\tif (susp_fail_count) {\n\t\t\t\tsleep(1);\n\t\t\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t\t\tif (fd[x] == -1)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t(void) stepd_suspend(\n\t\t\t\t\t\tfd[x],\n\t\t\t\t\t\tprotocol_version[x],\n\t\t\t\t\t\treq, 0);\n\t\t\t\t\tif (stepd_suspend(\n\t\t\t\t\t\t    fd[x],\n\t\t\t\t\t\t    protocol_version[x],\n\t\t\t\t\t\t    req, 1) >= 0)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tdebug(\"Suspend of job %u failed: %m\",\n\t\t\t\t\t      req->job_id);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t/* The resume RPCs are processed in parallel for\n\t\t\t * every step in the job */\n\t\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t\t(void) stepd_resume(fd[x],\n\t\t\t\t\t\t    protocol_version[x],\n\t\t\t\t\t\t    req, 0);\n\t\t\t}\n\t\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t\tif (stepd_resume(fd[x],\n\t\t\t\t\t\t protocol_version[x],\n\t\t\t\t\t\t req, 1) < 0) {\n\t\t\t\t\tdebug(\"Resume of job %u failed: %m\",\n\t\t\t\t\t      req->job_id);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor (x = 0; x < fdi; x++) {\n\t\t\t/* fd may have been closed by stepd_suspend */\n\t\t\tif (fd[x] != -1)\n\t\t\t\tclose(fd[x]);\n\t\t}\n\n\t\t/* check for no more jobs */\n\t\tif (fdi < NUM_PARALLEL_SUSP_STEPS)\n\t\t\tbreak;\n\t}\n\tlist_iterator_destroy(i);\n\tFREE_NULL_LIST(steps);\n\n\tif ((req->op == RESUME_JOB) && (req->indf_susp))\n\t\tswitch_g_job_resume(req->switch_info, 5);\n\n\t_unlock_suspend_job(req->job_id);\n\n\tEND_TIMER;\n\tif (DELTA_TIMER >= (time_slice * 1000000)) {\n\t\tif (req->op == SUSPEND_JOB) {\n\t\t\tinfo(\"Suspend time for job_id %u was %s. \"\n\t\t\t     \"Configure SchedulerTimeSlice higher.\",\n\t\t\t     req->job_id, TIME_STR);\n\t\t} else {\n\t\t\tinfo(\"Resume time for job_id %u was %s. \"\n\t\t\t     \"Configure SchedulerTimeSlice higher.\",\n\t\t\t     req->job_id, TIME_STR);\n\t\t}\n\t}\n\n\tif (step_cnt == 0) {\n\t\tdebug2(\"No steps in jobid %u to suspend/resume\", req->job_id);\n\t}\n}\n\n/* Job shouldn't even be running here, abort it immediately */\nstatic void\n_rpc_abort_job(slurm_msg_t *msg)\n{\n\tkill_job_msg_t *req    = msg->data;\n\tuid_t           uid    = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t      conf->auth_info);\n\tjob_env_t       job_env;\n\n\tdebug(\"_rpc_abort_job, uid = %d\", uid);\n\t/*\n\t * check that requesting user ID is the SLURM UID\n\t */\n\tif (!_slurm_authorized_user(uid)) {\n\t\terror(\"Security violation: abort_job(%u) from uid %d\",\n\t\t      req->job_id, uid);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\treturn;\n\t}\n\n\ttask_g_slurmd_release_resources(req->job_id);\n\n\t/*\n\t * \"revoke\" all future credentials for this jobid\n\t */\n\tif (slurm_cred_revoke(conf->vctx, req->job_id, req->time,\n\t\t\t      req->start_time) < 0) {\n\t\tdebug(\"revoking cred for job %u: %m\", req->job_id);\n\t} else {\n\t\tsave_cred_state(conf->vctx);\n\t\tdebug(\"credential for job %u revoked\", req->job_id);\n\t}\n\n\t/*\n\t *  At this point, if connection still open, we send controller\n\t *   a \"success\" reply to indicate that we've recvd the msg.\n\t */\n\tif (msg->conn_fd >= 0) {\n\t\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\terror (\"rpc_abort_job: close(%d): %m\", msg->conn_fd);\n\t\tmsg->conn_fd = -1;\n\t}\n\n\tif (_kill_all_active_steps(req->job_id, SIG_ABORT, true)) {\n\t\t/*\n\t\t *  Block until all user processes are complete.\n\t\t */\n\t\t_pause_for_job_completion (req->job_id, req->nodes, 0);\n\t}\n\n\t/*\n\t *  Begin expiration period for cached information about job.\n\t *   If expiration period has already begun, then do not run\n\t *   the epilog again, as that script has already been executed\n\t *   for this job.\n\t */\n\tif (slurm_cred_begin_expiration(conf->vctx, req->job_id) < 0) {\n\t\tdebug(\"Not running epilog for jobid %d: %m\", req->job_id);\n\t\treturn;\n\t}\n\n\tsave_cred_state(conf->vctx);\n\n\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\tjob_env.jobid = req->job_id;\n\tjob_env.node_list = req->nodes;\n\tjob_env.spank_job_env = req->spank_job_env;\n\tjob_env.spank_job_env_size = req->spank_job_env_size;\n\tjob_env.uid = req->job_uid;\n\n#if defined(HAVE_BG)\n\tselect_g_select_jobinfo_get(req->select_jobinfo,\n\t\t\t\t    SELECT_JOBDATA_BLOCK_ID,\n\t\t\t\t    &job_env.resv_id);\n#elif defined(HAVE_ALPS_CRAY)\n\tjob_env.resv_id = select_g_select_jobinfo_xstrdup(req->select_jobinfo,\n\t\t\t\t\t\t\t  SELECT_PRINT_RESV_ID);\n#endif\n\n\t_run_epilog(&job_env);\n\n\tif (container_g_delete(req->job_id))\n\t\terror(\"container_g_delete(%u): %m\", req->job_id);\n\t_launch_complete_rm(req->job_id);\n\n\txfree(job_env.resv_id);\n}\n\n/* This is a variant of _rpc_terminate_job for use with select/serial */\nstatic void\n_rpc_terminate_batch_job(uint32_t job_id, uint32_t user_id, char *node_name)\n{\n\tint             rc     = SLURM_SUCCESS;\n\tint             nsteps = 0;\n\tint\t\tdelay;\n\ttime_t\t\tnow = time(NULL);\n\tslurm_ctl_conf_t *cf;\n\tjob_env_t job_env;\n\n\ttask_g_slurmd_release_resources(job_id);\n\n\tif (_waiter_init(job_id) == SLURM_ERROR)\n\t\treturn;\n\n\t/*\n\t * \"revoke\" all future credentials for this jobid\n\t */\n\t_note_batch_job_finished(job_id);\n\tif (slurm_cred_revoke(conf->vctx, job_id, now, now) < 0) {\n\t\tdebug(\"revoking cred for job %u: %m\", job_id);\n\t} else {\n\t\tsave_cred_state(conf->vctx);\n\t\tdebug(\"credential for job %u revoked\", job_id);\n\t}\n\n\t/*\n\t * Tasks might be stopped (possibly by a debugger)\n\t * so send SIGCONT first.\n\t */\n\t_kill_all_active_steps(job_id, SIGCONT, true);\n\tif (errno == ESLURMD_STEP_SUSPENDED) {\n\t\t/*\n\t\t * If the job step is currently suspended, we don't\n\t\t * bother with a \"nice\" termination.\n\t\t */\n\t\tdebug2(\"Job is currently suspended, terminating\");\n\t\tnsteps = _terminate_all_steps(job_id, true);\n\t} else {\n\t\tnsteps = _kill_all_active_steps(job_id, SIGTERM, true);\n\t}\n\n#ifndef HAVE_AIX\n\tif ((nsteps == 0) && !conf->epilog) {\n\t\tslurm_cred_begin_expiration(conf->vctx, job_id);\n\t\tsave_cred_state(conf->vctx);\n\t\t_waiter_complete(job_id);\n\t\tif (container_g_delete(job_id))\n\t\t\terror(\"container_g_delete(%u): %m\", job_id);\n\t\t_launch_complete_rm(job_id);\n\t\treturn;\n\t}\n#endif\n\n\t/*\n\t *  Check for corpses\n\t */\n\tcf = slurm_conf_lock();\n\tdelay = MAX(cf->kill_wait, 5);\n\tslurm_conf_unlock();\n\tif (!_pause_for_job_completion(job_id, NULL, delay) &&\n\t     _terminate_all_steps(job_id, true) ) {\n\t\t/*\n\t\t *  Block until all user processes are complete.\n\t\t */\n\t\t_pause_for_job_completion(job_id, NULL, 0);\n\t}\n\n\t/*\n\t *  Begin expiration period for cached information about job.\n\t *   If expiration period has already begun, then do not run\n\t *   the epilog again, as that script has already been executed\n\t *   for this job.\n\t */\n\tif (slurm_cred_begin_expiration(conf->vctx, job_id) < 0) {\n\t\tdebug(\"Not running epilog for jobid %d: %m\", job_id);\n\t\tgoto done;\n\t}\n\n\tsave_cred_state(conf->vctx);\n\n\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\tjob_env.jobid = job_id;\n\tjob_env.node_list = node_name;\n\tjob_env.uid = (uid_t)user_id;\n\t/* NOTE: We lack the job's SPANK environment variables */\n\trc = _run_epilog(&job_env);\n\tif (rc) {\n\t\tint term_sig, exit_status;\n\t\tif (WIFSIGNALED(rc)) {\n\t\t\texit_status = 0;\n\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t} else {\n\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\tterm_sig    = 0;\n\t\t}\n\t\terror(\"[job %u] epilog failed status=%d:%d\",\n\t\t      job_id, exit_status, term_sig);\n\t} else\n\t\tdebug(\"completed epilog for jobid %u\", job_id);\n\tif (container_g_delete(job_id))\n\t\terror(\"container_g_delete(%u): %m\", job_id);\n\t_launch_complete_rm(job_id);\n\n    done:\n\t_wait_state_completed(job_id, 5);\n\t_waiter_complete(job_id);\n}\n\nstatic void _handle_old_batch_job_launch(slurm_msg_t *msg)\n{\n\tif (msg->msg_type != REQUEST_BATCH_JOB_LAUNCH) {\n\t\terror(\"_handle_batch_job_launch: \"\n\t\t      \"Invalid response msg_type (%u)\", msg->msg_type);\n\t\treturn;\n\t}\n\n\t/* (resp_msg.msg_type == REQUEST_BATCH_JOB_LAUNCH) */\n\tdebug2(\"Processing RPC: REQUEST_BATCH_JOB_LAUNCH\");\n\tlast_slurmctld_msg = time(NULL);\n\t_rpc_batch_job(msg, false);\n\tslurm_free_job_launch_msg(msg->data);\n\tmsg->data = NULL;\n\n}\n\n/* This complete batch RPC came from slurmstepd because we have select/serial\n * configured. Terminate the job here. Forward the batch completion RPC to\n * slurmctld and possible get a new batch launch RPC in response. */\nstatic void\n_rpc_complete_batch(slurm_msg_t *msg)\n{\n\tint\t\ti, rc, msg_rc;\n\tslurm_msg_t\tresp_msg;\n\tuid_t           uid    = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t      conf->auth_info);\n\tcomplete_batch_script_msg_t *req = msg->data;\n\tstatic int\trunning_serial = -1;\n\tuint16_t msg_type;\n\n\tif (running_serial == -1) {\n\t\tchar *select_type = slurm_get_select_type();\n\t\tif (!xstrcmp(select_type, \"select/serial\"))\n\t\t\trunning_serial = 1;\n\t\telse\n\t\t\trunning_serial = 0;\n\t\txfree(select_type);\n\t}\n\n\tif (!_slurm_authorized_user(uid)) {\n\t\terror(\"Security violation: complete_batch(%u) from uid %d\",\n\t\t      req->job_id, uid);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\treturn;\n\t}\n\n\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\n\tif (running_serial) {\n\t\t_rpc_terminate_batch_job(\n\t\t\treq->job_id, req->user_id, req->node_name);\n\t\tmsg_type = REQUEST_COMPLETE_BATCH_JOB;\n\t} else\n\t\tmsg_type = msg->msg_type;\n\n\tfor (i = 0; i <= MAX_RETRY; i++) {\n\t\tif (conf->msg_aggr_window_msgs > 1) {\n\t\t\tslurm_msg_t *req_msg =\n\t\t\t\txmalloc_nz(sizeof(slurm_msg_t));\n\t\t\tslurm_msg_t_init(req_msg);\n\t\t\treq_msg->msg_type = msg_type;\n\t\t\treq_msg->data = msg->data;\n\t\t\tmsg->data = NULL;\n\n\t\t\tmsg_aggr_add_msg(req_msg, 1,\n\t\t\t\t\t _handle_old_batch_job_launch);\n\t\t\treturn;\n\t\t} else {\n\t\t\tslurm_msg_t req_msg;\n\t\t\tslurm_msg_t_init(&req_msg);\n\t\t\treq_msg.msg_type = msg_type;\n\t\t\treq_msg.data\t = msg->data;\n\t\t\tmsg_rc = slurm_send_recv_controller_msg(\n\t\t\t\t&req_msg, &resp_msg);\n\n\t\t\tif (msg_rc == SLURM_SUCCESS)\n\t\t\t\tbreak;\n\t\t}\n\t\tinfo(\"Retrying job complete RPC for job %u\", req->job_id);\n\t\tsleep(RETRY_DELAY);\n\t}\n\tif (i > MAX_RETRY) {\n\t\terror(\"Unable to send job complete message: %m\");\n\t\treturn;\n\t}\n\n\tif (resp_msg.msg_type == RESPONSE_SLURM_RC) {\n\t\tlast_slurmctld_msg = time(NULL);\n\t\trc = ((return_code_msg_t *) resp_msg.data)->return_code;\n\t\tslurm_free_return_code_msg(resp_msg.data);\n\t\tif (rc) {\n\t\t\terror(\"complete_batch for job %u: %s\", req->job_id,\n\t\t\t      slurm_strerror(rc));\n\t\t}\n\t\treturn;\n\t}\n\n\t_handle_old_batch_job_launch(&resp_msg);\n}\n\nstatic void\n_rpc_terminate_job(slurm_msg_t *msg)\n{\n#ifndef HAVE_AIX\n\tbool\t\thave_spank = false;\n#endif\n\tint             rc     = SLURM_SUCCESS;\n\tkill_job_msg_t *req    = msg->data;\n\tuid_t           uid    = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t      conf->auth_info);\n\tint             nsteps = 0;\n\tint\t\tdelay;\n//\tslurm_ctl_conf_t *cf;\n//\tstruct stat\tstat_buf;\n\tjob_env_t       job_env;\n\n\tdebug(\"_rpc_terminate_job, uid = %d\", uid);\n\t/*\n\t * check that requesting user ID is the SLURM UID\n\t */\n\tif (!_slurm_authorized_user(uid)) {\n\t\terror(\"Security violation: kill_job(%u) from uid %d\",\n\t\t      req->job_id, uid);\n\t\tif (msg->conn_fd >= 0)\n\t\t\tslurm_send_rc_msg(msg, ESLURM_USER_ID_MISSING);\n\t\treturn;\n\t}\n\n\ttask_g_slurmd_release_resources(req->job_id);\n\n\t/*\n\t *  Initialize a \"waiter\" thread for this jobid. If another\n\t *   thread is already waiting on termination of this job,\n\t *   _waiter_init() will return SLURM_ERROR. In this case, just\n\t *   notify slurmctld that we recvd the message successfully,\n\t *   then exit this thread.\n\t */\n\tif (_waiter_init(req->job_id) == SLURM_ERROR) {\n\t\tif (msg->conn_fd >= 0) {\n\t\t\t/* No matter if the step hasn't started yet or\n\t\t\t * not just send a success to let the\n\t\t\t * controller know we got this request.\n\t\t\t */\n\t\t\tslurm_send_rc_msg (msg, SLURM_SUCCESS);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * \"revoke\" all future credentials for this jobid\n\t */\n\tif (slurm_cred_revoke(conf->vctx, req->job_id, req->time,\n\t\t\t      req->start_time) < 0) {\n\t\tdebug(\"revoking cred for job %u: %m\", req->job_id);\n\t} else {\n\t\tsave_cred_state(conf->vctx);\n\t\tdebug(\"credential for job %u revoked\", req->job_id);\n\t}\n\n\t/*\n\t * Before signalling steps, if the job has any steps that are still\n\t * in the process of fork/exec/check in with slurmd, wait on a condition\n\t * var for the start.  Otherwise a slow-starting step can miss the\n\t * job termination message and run indefinitely.\n\t */\n\tif (_step_is_starting(req->job_id, NO_VAL)) {\n\t\tif (msg->conn_fd >= 0) {\n\t\t\t/* If the step hasn't started yet just send a\n\t\t\t * success to let the controller know we got\n\t\t\t * this request.\n\t\t\t */\n\t\t\tdebug(\"sent SUCCESS, waiting for step to start\");\n\t\t\tslurm_send_rc_msg (msg, SLURM_SUCCESS);\n\t\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\t\terror ( \"rpc_kill_job: close(%d): %m\",\n\t\t\t\t\tmsg->conn_fd);\n\t\t\tmsg->conn_fd = -1;\n\t\t}\n\t\tif (_wait_for_starting_step(req->job_id, NO_VAL)) {\n\t\t\t/*\n\t\t\t * There's currently no case in which we enter this\n\t\t\t * error condition.  If there was, it's hard to say\n\t\t\t * whether to to proceed with the job termination.\n\t\t\t */\n\t\t\terror(\"Error in _wait_for_starting_step\");\n\t\t}\n\t}\n\tif (IS_JOB_NODE_FAILED(req))\n\t\t_kill_all_active_steps(req->job_id, SIG_NODE_FAIL, true);\n\tif (IS_JOB_PENDING(req))\n\t\t_kill_all_active_steps(req->job_id, SIG_REQUEUED, true);\n\telse if (IS_JOB_FAILED(req))\n\t\t_kill_all_active_steps(req->job_id, SIG_FAILURE, true);\n\n\t/*\n\t * Tasks might be stopped (possibly by a debugger)\n\t * so send SIGCONT first.\n\t */\n\t_kill_all_active_steps(req->job_id, SIGCONT, true);\n\tif (errno == ESLURMD_STEP_SUSPENDED) {\n\t\t/*\n\t\t * If the job step is currently suspended, we don't\n\t\t * bother with a \"nice\" termination.\n\t\t */\n\t\tdebug2(\"Job is currently suspended, terminating\");\n\t\tnsteps = _terminate_all_steps(req->job_id, true);\n\t} else {\n\t\tnsteps = _kill_all_active_steps(req->job_id, SIGTERM, true);\n\t}\n\n#ifndef HAVE_AIX\n\tif ((nsteps == 0) && !conf->epilog) {\n\t\tstruct stat stat_buf;\n\t\tif (conf->plugstack && (stat(conf->plugstack, &stat_buf) == 0))\n\t\t\thave_spank = true;\n\t}\n\t/*\n\t *  If there are currently no active job steps and no\n\t *    configured epilog to run, bypass asynchronous reply and\n\t *    notify slurmctld that we have already completed this\n\t *    request. We need to send current switch state on AIX\n\t *    systems, so this bypass can not be used.\n\t */\n\tif ((nsteps == 0) && !conf->epilog && !have_spank) {\n\t\tdebug4(\"sent ALREADY_COMPLETE\");\n\t\tif (msg->conn_fd >= 0) {\n\t\t\tslurm_send_rc_msg(msg,\n\t\t\t\t\t  ESLURMD_KILL_JOB_ALREADY_COMPLETE);\n\t\t}\n\t\tslurm_cred_begin_expiration(conf->vctx, req->job_id);\n\t\tsave_cred_state(conf->vctx);\n\t\t_waiter_complete(req->job_id);\n\n\t\t/*\n\t\t * The controller needs to get MESSAGE_EPILOG_COMPLETE to bring\n\t\t * the job out of \"completing\" state.  Otherwise, the job\n\t\t * could remain \"completing\" unnecessarily, until the request\n\t\t * to terminate is resent.\n\t\t */\n\t\t_sync_messages_kill(req);\n\t\tif (msg->conn_fd < 0) {\n\t\t\t/* The epilog complete message processing on\n\t\t\t * slurmctld is equivalent to that of a\n\t\t\t * ESLURMD_KILL_JOB_ALREADY_COMPLETE reply above */\n\t\t\t_epilog_complete(req->job_id, rc);\n\t\t}\n\t\tif (container_g_delete(req->job_id))\n\t\t\terror(\"container_g_delete(%u): %m\", req->job_id);\n\t\t_launch_complete_rm(req->job_id);\n\t\treturn;\n\t}\n#endif\n\n\t/*\n\t *  At this point, if connection still open, we send controller\n\t *   a \"success\" reply to indicate that we've recvd the msg.\n\t */\n\tif (msg->conn_fd >= 0) {\n\t\tdebug4(\"sent SUCCESS\");\n\t\tslurm_send_rc_msg(msg, SLURM_SUCCESS);\n\t\tif (slurm_close(msg->conn_fd) < 0)\n\t\t\terror (\"rpc_kill_job: close(%d): %m\", msg->conn_fd);\n\t\tmsg->conn_fd = -1;\n\t}\n\n\t/*\n\t *  Check for corpses\n\t */\n\tdelay = MAX(conf->kill_wait, 5);\n\tif ( !_pause_for_job_completion (req->job_id, req->nodes, delay) &&\n\t     _terminate_all_steps(req->job_id, true) ) {\n\t\t/*\n\t\t *  Block until all user processes are complete.\n\t\t */\n\t\t_pause_for_job_completion (req->job_id, req->nodes, 0);\n\t}\n\n\t/*\n\t *  Begin expiration period for cached information about job.\n\t *   If expiration period has already begun, then do not run\n\t *   the epilog again, as that script has already been executed\n\t *   for this job.\n\t */\n\tif (slurm_cred_begin_expiration(conf->vctx, req->job_id) < 0) {\n\t\tdebug(\"Not running epilog for jobid %d: %m\", req->job_id);\n\t\tgoto done;\n\t}\n\n\tsave_cred_state(conf->vctx);\n\n\tmemset(&job_env, 0, sizeof(job_env_t));\n\n\tjob_env.jobid = req->job_id;\n\tjob_env.node_list = req->nodes;\n\tjob_env.spank_job_env = req->spank_job_env;\n\tjob_env.spank_job_env_size = req->spank_job_env_size;\n\tjob_env.uid = req->job_uid;\n\n#if defined(HAVE_BG)\n\tselect_g_select_jobinfo_get(req->select_jobinfo,\n\t\t\t\t    SELECT_JOBDATA_BLOCK_ID,\n\t\t\t\t    &job_env.resv_id);\n#elif defined(HAVE_ALPS_CRAY)\n\tjob_env.resv_id = select_g_select_jobinfo_xstrdup(\n\t\treq->select_jobinfo, SELECT_PRINT_RESV_ID);\n#endif\n\trc = _run_epilog(&job_env);\n\txfree(job_env.resv_id);\n\n\tif (rc) {\n\t\tint term_sig, exit_status;\n\t\tif (WIFSIGNALED(rc)) {\n\t\t\texit_status = 0;\n\t\t\tterm_sig    = WTERMSIG(rc);\n\t\t} else {\n\t\t\texit_status = WEXITSTATUS(rc);\n\t\t\tterm_sig    = 0;\n\t\t}\n\t\terror(\"[job %u] epilog failed status=%d:%d\",\n\t\t      req->job_id, exit_status, term_sig);\n\t\trc = ESLURMD_EPILOG_FAILED;\n\t} else\n\t\tdebug(\"completed epilog for jobid %u\", req->job_id);\n\tif (container_g_delete(req->job_id))\n\t\terror(\"container_g_delete(%u): %m\", req->job_id);\n\t_launch_complete_rm(req->job_id);\n\n    done:\n\t_wait_state_completed(req->job_id, 5);\n\t_waiter_complete(req->job_id);\n\t_sync_messages_kill(req);\n\n\t_epilog_complete(req->job_id, rc);\n}\n\n/* On a parallel job, every slurmd may send the EPILOG_COMPLETE\n * message to the slurmctld at the same time, resulting in lost\n * messages. We add a delay here to spead out the message traffic\n * assuming synchronized clocks across the cluster.\n * Allow 10 msec processing time in slurmctld for each RPC. */\nstatic void _sync_messages_kill(kill_job_msg_t *req)\n{\n\tint host_cnt, host_inx;\n\tchar *host;\n\thostset_t hosts;\n\tint epilog_msg_time;\n\n\thosts = hostset_create(req->nodes);\n\thost_cnt = hostset_count(hosts);\n\tif (host_cnt <= 64)\n\t\tgoto fini;\n\tif (conf->hostname == NULL)\n\t\tgoto fini;\t/* should never happen */\n\n\tfor (host_inx=0; host_inx<host_cnt; host_inx++) {\n\t\thost = hostset_shift(hosts);\n\t\tif (host == NULL)\n\t\t\tbreak;\n\t\tif (xstrcmp(host, conf->node_name) == 0) {\n\t\t\tfree(host);\n\t\t\tbreak;\n\t\t}\n\t\tfree(host);\n\t}\n\tepilog_msg_time = slurm_get_epilog_msg_time();\n\t_delay_rpc(host_inx, host_cnt, epilog_msg_time);\n\n fini:\thostset_destroy(hosts);\n}\n\n/* Delay a message based upon the host index, total host count and RPC_TIME.\n * This logic depends upon synchronized clocks across the cluster. */\nstatic void _delay_rpc(int host_inx, int host_cnt, int usec_per_rpc)\n{\n\tstruct timeval tv1;\n\tuint32_t cur_time;\t/* current time in usec (just 9 digits) */\n\tuint32_t tot_time;\t/* total time expected for all RPCs */\n\tuint32_t offset_time;\t/* relative time within tot_time */\n\tuint32_t target_time;\t/* desired time to issue the RPC */\n\tuint32_t delta_time;\n\nagain:\tif (gettimeofday(&tv1, NULL)) {\n\t\tusleep(host_inx * usec_per_rpc);\n\t\treturn;\n\t}\n\n\tcur_time = ((tv1.tv_sec % 1000) * 1000000) + tv1.tv_usec;\n\ttot_time = host_cnt * usec_per_rpc;\n\toffset_time = cur_time % tot_time;\n\ttarget_time = host_inx * usec_per_rpc;\n\tif (target_time < offset_time)\n\t\tdelta_time = target_time - offset_time + tot_time;\n\telse\n\t\tdelta_time = target_time - offset_time;\n\tif (usleep(delta_time)) {\n\t\tif (errno == EINVAL) /* usleep for more than 1 sec */\n\t\t\tusleep(900000);\n\t\t/* errno == EINTR */\n\t\tgoto again;\n\t}\n}\n\n/*\n *  Returns true if \"uid\" is a \"slurm authorized user\" - i.e. uid == 0\n *   or uid == slurm user id at this time.\n */\nstatic bool\n_slurm_authorized_user(uid_t uid)\n{\n\treturn ((uid == (uid_t) 0) || (uid == conf->slurm_user_id));\n}\n\n\nstruct waiter {\n\tuint32_t jobid;\n\tpthread_t thd;\n};\n\n\nstatic struct waiter *\n_waiter_create(uint32_t jobid)\n{\n\tstruct waiter *wp = xmalloc(sizeof(struct waiter));\n\n\twp->jobid = jobid;\n\twp->thd   = pthread_self();\n\n\treturn wp;\n}\n\nstatic int _find_waiter(struct waiter *w, uint32_t *jp)\n{\n\treturn (w->jobid == *jp);\n}\n\nstatic void _waiter_destroy(struct waiter *wp)\n{\n\txfree(wp);\n}\n\nstatic int _waiter_init (uint32_t jobid)\n{\n\tif (!waiters)\n\t\twaiters = list_create((ListDelF) _waiter_destroy);\n\n\t/*\n\t *  Exit this thread if another thread is waiting on job\n\t */\n\tif (list_find_first (waiters, (ListFindF) _find_waiter, &jobid))\n\t\treturn SLURM_ERROR;\n\telse\n\t\tlist_append(waiters, _waiter_create(jobid));\n\n\treturn (SLURM_SUCCESS);\n}\n\nstatic int _waiter_complete (uint32_t jobid)\n{\n\treturn (list_delete_all (waiters, (ListFindF) _find_waiter, &jobid));\n}\n\n/*\n *  Like _wait_for_procs(), but only wait for up to max_time seconds\n *  if max_time == 0, send SIGKILL to tasks repeatedly\n *\n *  Returns true if all job processes are gone\n */\nstatic bool\n_pause_for_job_completion (uint32_t job_id, char *nodes, int max_time)\n{\n\tint sec = 0;\n\tint pause = 1;\n\tbool rc = false;\n\n\twhile ((sec < max_time) || (max_time == 0)) {\n\t\trc = _job_still_running (job_id);\n\t\tif (!rc)\n\t\t\tbreak;\n\t\tif ((max_time == 0) && (sec > 1)) {\n\t\t\t_terminate_all_steps(job_id, true);\n\t\t}\n\t\tif (sec > 10) {\n\t\t\t/* Reduce logging frequency about unkillable tasks */\n\t\t\tif (max_time)\n\t\t\t\tpause = MIN((max_time - sec), 10);\n\t\t\telse\n\t\t\t\tpause = 10;\n\t\t}\n\t\tsleep(pause);\n\t\tsec += pause;\n\t}\n\n\t/*\n\t * Return true if job is NOT running\n\t */\n\treturn (!rc);\n}\n\n/*\n * Does nothing and returns SLURM_SUCCESS (if uid authenticates).\n *\n * Timelimit is not currently used in the slurmd or slurmstepd.\n */\nstatic void\n_rpc_update_time(slurm_msg_t *msg)\n{\n\tint   rc      = SLURM_SUCCESS;\n\tuid_t req_uid = g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t     conf->auth_info);\n\n\tif ((req_uid != conf->slurm_user_id) && (req_uid != 0)) {\n\t\trc = ESLURM_USER_ID_MISSING;\n\t\terror(\"Security violation, uid %d can't update time limit\",\n\t\t      req_uid);\n\t\tgoto done;\n\t}\n\n/* \tif (shm_update_job_timelimit(req->job_id, req->expiration_time) < 0) { */\n/* \t\terror(\"updating lifetime for job %u: %m\", req->job_id); */\n/* \t\trc = ESLURM_INVALID_JOB_ID; */\n/* \t} else */\n/* \t\tdebug(\"reset job %u lifetime\", req->job_id); */\n\n    done:\n\tslurm_send_rc_msg(msg, rc);\n}\n\n/* NOTE: call _destroy_env() to free returned value */\nstatic char **\n_build_env(job_env_t *job_env)\n{\n\tchar **env = xmalloc(sizeof(char *));\n\tbool user_name_set = 0;\n\n\tenv[0]  = NULL;\n\tif (!valid_spank_job_env(job_env->spank_job_env,\n\t\t\t\t job_env->spank_job_env_size,\n\t\t\t\t job_env->uid)) {\n\t\t/* If SPANK job environment is bad, log it and do not use */\n\t\tjob_env->spank_job_env_size = 0;\n\t\tjob_env->spank_job_env = (char **) NULL;\n\t}\n\tif (job_env->spank_job_env_size) {\n\t\tenv_array_merge_spank(&env,\n\t\t\t\t      (const char **) job_env->spank_job_env);\n\t}\n\n\tslurm_mutex_lock(&conf->config_mutex);\n\tsetenvf(&env, \"SLURMD_NODENAME\", \"%s\", conf->node_name);\n\tsetenvf(&env, \"SLURM_CONF\", conf->conffile);\n\tslurm_mutex_unlock(&conf->config_mutex);\n\n\tsetenvf(&env, \"SLURM_CLUSTER_NAME\", \"%s\", conf->cluster_name);\n\tsetenvf(&env, \"SLURM_JOB_ID\", \"%u\", job_env->jobid);\n\tsetenvf(&env, \"SLURM_JOB_UID\",   \"%u\", job_env->uid);\n\n#ifndef HAVE_NATIVE_CRAY\n\t/* uid_to_string on a cray is a heavy call, so try to avoid it */\n\tif (!job_env->user_name) {\n\t\tjob_env->user_name = uid_to_string(job_env->uid);\n\t\tuser_name_set = 1;\n\t}\n#endif\n\n\tsetenvf(&env, \"SLURM_JOB_USER\", \"%s\", job_env->user_name);\n\tif (user_name_set)\n\t\txfree(job_env->user_name);\n\n\tsetenvf(&env, \"SLURM_JOBID\", \"%u\", job_env->jobid);\n\tsetenvf(&env, \"SLURM_UID\",   \"%u\", job_env->uid);\n\tif (job_env->node_list)\n\t\tsetenvf(&env, \"SLURM_NODELIST\", \"%s\", job_env->node_list);\n\n\tif (job_env->partition)\n\t\tsetenvf(&env, \"SLURM_JOB_PARTITION\", \"%s\", job_env->partition);\n\n\tif (job_env->resv_id) {\n#if defined(HAVE_BG)\n\t\tsetenvf(&env, \"MPIRUN_PARTITION\", \"%s\", job_env->resv_id);\n# ifdef HAVE_BGP\n\t\t/* Needed for HTC jobs */\n\t\tsetenvf(&env, \"SUBMIT_POOL\", \"%s\", job_env->resv_id);\n# endif\n#elif defined(HAVE_ALPS_CRAY)\n\t\tsetenvf(&env, \"BASIL_RESERVATION_ID\", \"%s\", job_env->resv_id);\n#endif\n\t}\n\treturn env;\n}\n\nstatic void\n_destroy_env(char **env)\n{\n\tint i=0;\n\n\tif (env) {\n\t\tfor(i=0; env[i]; i++) {\n\t\t\txfree(env[i]);\n\t\t}\n\t\txfree(env);\n\t}\n\n\treturn;\n}\n\n/* Trigger srun of spank prolog or epilog in slurmstepd */\nstatic int\n_run_spank_job_script (const char *mode, char **env, uint32_t job_id, uid_t uid)\n{\n\tpid_t cpid;\n\tint status = 0, timeout;\n\tint pfds[2];\n\n\tif (pipe (pfds) < 0) {\n\t\terror (\"_run_spank_job_script: pipe: %m\");\n\t\treturn (-1);\n\t}\n\n\tfd_set_close_on_exec (pfds[1]);\n\n\tdebug (\"Calling %s spank %s\", conf->stepd_loc, mode);\n\tif ((cpid = fork ()) < 0) {\n\t\terror (\"executing spank %s: %m\", mode);\n\t\treturn (-1);\n\t}\n\tif (cpid == 0) {\n\t\t/* Run slurmstepd spank [prolog|epilog] */\n\t\tchar *argv[4] = {\n\t\t\t(char *) conf->stepd_loc,\n\t\t\t\"spank\",\n\t\t\t(char *) mode,\n\t\t\tNULL };\n\n\t\t/* container_g_add_pid needs to be called in the\n\t\t   forked process part of the fork to avoid a race\n\t\t   condition where if this process makes a file or\n\t\t   detacts itself from a child before we add the pid\n\t\t   to the container in the parent of the fork.\n\t\t*/\n\t\tif (container_g_add_pid(job_id, getpid(), getuid())\n\t\t    != SLURM_SUCCESS)\n\t\t\terror(\"container_g_add_pid(%u): %m\", job_id);\n\n\t\tif (dup2 (pfds[0], STDIN_FILENO) < 0)\n\t\t\tfatal (\"dup2: %m\");\n#ifdef SETPGRP_TWO_ARGS\n\t\tsetpgrp(0, 0);\n#else\n\t\tsetpgrp();\n#endif\n\t\tif (conf->chos_loc && !access(conf->chos_loc, X_OK))\n\t\t\texecve(conf->chos_loc, argv, env);\n\t\telse\n\t\t\texecve(argv[0], argv, env);\n\t\terror (\"execve(%s): %m\", argv[0]);\n\t\texit (127);\n\t}\n\n\tclose (pfds[0]);\n\n\tif (_send_slurmd_conf_lite (pfds[1], conf) < 0)\n\t\terror (\"Failed to send slurmd conf to slurmstepd\\n\");\n\tclose (pfds[1]);\n\n\ttimeout = MAX(slurm_get_prolog_timeout(), 120); /* 120 secs in v15.08 */\n\tif (waitpid_timeout (mode, cpid, &status, timeout) < 0) {\n\t\terror (\"spank/%s timed out after %u secs\", mode, timeout);\n\t\treturn (-1);\n\t}\n\n\tif (status)\n\t\terror (\"spank/%s returned status 0x%04x\", mode, status);\n\n\t/*\n\t *  No longer need SPANK option env vars in environment\n\t */\n\tspank_clear_remote_options_env (env);\n\n\treturn (status);\n}\n\nstatic int _run_job_script(const char *name, const char *path,\n\t\t\t   uint32_t jobid, int timeout, char **env, uid_t uid)\n{\n\tstruct stat stat_buf;\n\tint status = 0, rc;\n\n\t/*\n\t *  Always run both spank prolog/epilog and real prolog/epilog script,\n\t *   even if spank plugins fail. (May want to alter this in the future)\n\t *   If both \"script\" mechanisms fail, prefer to return the \"real\"\n\t *   prolog/epilog status.\n\t */\n\tif (conf->plugstack && (stat(conf->plugstack, &stat_buf) == 0))\n\t\tstatus = _run_spank_job_script(name, env, jobid, uid);\n\tif ((rc = run_script(name, path, jobid, timeout, env, uid)))\n\t\tstatus = rc;\n\treturn (status);\n}\n\n#ifdef HAVE_BG\n/* a slow prolog is expected on bluegene systems */\nstatic int\n_run_prolog(job_env_t *job_env, slurm_cred_t *cred)\n{\n\tint rc;\n\tchar *my_prolog;\n\tchar **my_env;\n\n\tmy_env = _build_env(job_env);\n\tsetenvf(&my_env, \"SLURM_STEP_ID\", \"%u\", job_env->step_id);\n\n\tslurm_mutex_lock(&conf->config_mutex);\n\tmy_prolog = xstrdup(conf->prolog);\n\tslurm_mutex_unlock(&conf->config_mutex);\n\n\trc = _run_job_script(\"prolog\", my_prolog, job_env->jobid,\n\t\t\t     -1, my_env, job_env->uid);\n\t_remove_job_running_prolog(job_env->jobid);\n\txfree(my_prolog);\n\t_destroy_env(my_env);\n\n\treturn rc;\n}\n#else\nstatic void *_prolog_timer(void *x)\n{\n\tint delay_time, rc = SLURM_SUCCESS;\n\tstruct timespec ts;\n\tstruct timeval now;\n\tslurm_msg_t msg;\n\tjob_notify_msg_t notify_req;\n\tchar srun_msg[128];\n\ttimer_struct_t *timer_struct = (timer_struct_t *) x;\n\n\tdelay_time = MAX(2, (timer_struct->msg_timeout - 2));\n\tgettimeofday(&now, NULL);\n\tts.tv_sec = now.tv_sec + delay_time;\n\tts.tv_nsec = now.tv_usec * 1000;\n\tslurm_mutex_lock(timer_struct->timer_mutex);\n\tif (!timer_struct->prolog_fini) {\n\t\trc = pthread_cond_timedwait(timer_struct->timer_cond,\n\t\t\t\t\t    timer_struct->timer_mutex,\n\t\t\t\t\t    &ts);\n\t}\n\tslurm_mutex_unlock(timer_struct->timer_mutex);\n\n\tif (rc != ETIMEDOUT)\n\t\treturn NULL;\n\n\tslurm_msg_t_init(&msg);\n\tsnprintf(srun_msg, sizeof(srun_msg), \"Prolog hung on node %s\",\n\t\t conf->node_name);\n\tnotify_req.job_id\t= timer_struct->job_id;\n\tnotify_req.job_step_id\t= NO_VAL;\n\tnotify_req.message\t= srun_msg;\n\tmsg.msg_type\t= REQUEST_JOB_NOTIFY;\n\tmsg.data\t= &notify_req;\n\tslurm_send_only_controller_msg(&msg);\n\treturn NULL;\n}\n\nstatic int\n_run_prolog(job_env_t *job_env, slurm_cred_t *cred)\n{\n\tDEF_TIMERS;\n\tint rc, diff_time;\n\tchar *my_prolog;\n\ttime_t start_time = time(NULL);\n\tstatic uint16_t msg_timeout = 0;\n\tstatic uint16_t timeout;\n\tpthread_t       timer_id;\n\tpthread_attr_t  timer_attr;\n\tpthread_cond_t  timer_cond  = PTHREAD_COND_INITIALIZER;\n\tpthread_mutex_t timer_mutex = PTHREAD_MUTEX_INITIALIZER;\n\ttimer_struct_t  timer_struct;\n\tbool prolog_fini = false;\n\tchar **my_env;\n\n\tmy_env = _build_env(job_env);\n\tsetenvf(&my_env, \"SLURM_STEP_ID\", \"%u\", job_env->step_id);\n\tif (cred) {\n\t\tslurm_cred_arg_t cred_arg;\n\t\tslurm_cred_get_args(cred, &cred_arg);\n\t\tsetenvf(&my_env, \"SLURM_JOB_CONSTRAINTS\", \"%s\",\n\t\t\tcred_arg.job_constraints);\n\t\tgres_plugin_job_set_env(&my_env, cred_arg.job_gres_list);\n\t\tslurm_cred_free_args(&cred_arg);\n\t}\n\n\tif (msg_timeout == 0)\n\t\tmsg_timeout = slurm_get_msg_timeout();\n\n\tif (timeout == 0)\n\t\ttimeout = slurm_get_prolog_timeout();\n\n\tslurm_mutex_lock(&conf->config_mutex);\n\tmy_prolog = xstrdup(conf->prolog);\n\tslurm_mutex_unlock(&conf->config_mutex);\n\n\tslurm_attr_init(&timer_attr);\n\ttimer_struct.job_id      = job_env->jobid;\n\ttimer_struct.msg_timeout = msg_timeout;\n\ttimer_struct.prolog_fini = &prolog_fini;\n\ttimer_struct.timer_cond  = &timer_cond;\n\ttimer_struct.timer_mutex = &timer_mutex;\n\tpthread_create(&timer_id, &timer_attr, &_prolog_timer, &timer_struct);\n\tSTART_TIMER;\n\n\tif (timeout == (uint16_t)NO_VAL)\n\t\trc = _run_job_script(\"prolog\", my_prolog, job_env->jobid,\n\t\t\t\t     -1, my_env, job_env->uid);\n\telse\n\t\trc = _run_job_script(\"prolog\", my_prolog, job_env->jobid,\n\t\t\t\t     timeout, my_env, job_env->uid);\n\n\tEND_TIMER;\n\tinfo(\"%s: run job script took %s\", __func__, TIME_STR);\n\tslurm_mutex_lock(&timer_mutex);\n\tprolog_fini = true;\n\tpthread_cond_broadcast(&timer_cond);\n\tslurm_mutex_unlock(&timer_mutex);\n\n\tdiff_time = difftime(time(NULL), start_time);\n\tinfo(\"%s: prolog with lock for job %u ran for %d seconds\",\n\t     __func__, job_env->jobid, diff_time);\n\tif (diff_time >= (msg_timeout / 2)) {\n\t\tinfo(\"prolog for job %u ran for %d seconds\",\n\t\t     job_env->jobid, diff_time);\n\t}\n\n\t_remove_job_running_prolog(job_env->jobid);\n\txfree(my_prolog);\n\t_destroy_env(my_env);\n\n\tpthread_join(timer_id, NULL);\n\treturn rc;\n}\n#endif\n\nstatic int\n_run_epilog(job_env_t *job_env)\n{\n\ttime_t start_time = time(NULL);\n\tstatic uint16_t msg_timeout = 0;\n\tstatic uint16_t timeout;\n\tint error_code, diff_time;\n\tchar *my_epilog;\n\tchar **my_env = _build_env(job_env);\n\n\tif (msg_timeout == 0)\n\t\tmsg_timeout = slurm_get_msg_timeout();\n\n\tif (timeout == 0)\n\t\ttimeout = slurm_get_prolog_timeout();\n\n\tslurm_mutex_lock(&conf->config_mutex);\n\tmy_epilog = xstrdup(conf->epilog);\n\tslurm_mutex_unlock(&conf->config_mutex);\n\n\t_wait_for_job_running_prolog(job_env->jobid);\n\n\tif (timeout == (uint16_t)NO_VAL)\n\t\terror_code = _run_job_script(\"epilog\", my_epilog, job_env->jobid,\n\t\t\t\t\t     -1, my_env, job_env->uid);\n\telse\n\t\terror_code = _run_job_script(\"epilog\", my_epilog, job_env->jobid,\n\t\t\t\t\t     timeout, my_env, job_env->uid);\n\n\txfree(my_epilog);\n\t_destroy_env(my_env);\n\n\tdiff_time = difftime(time(NULL), start_time);\n\tif (diff_time >= (msg_timeout / 2)) {\n\t\tinfo(\"epilog for job %u ran for %d seconds\",\n\t\t     job_env->jobid, diff_time);\n\t}\n\n\treturn error_code;\n}\n\n\n/**********************************************************************/\n/* Because calling initgroups(2)/getgrouplist(3) can be expensive and */\n/* is not cached by sssd or nscd, we cache the group access list.     */\n/**********************************************************************/\n\ntypedef struct gid_cache_s {\n\tchar *user;\n\ttime_t timestamp;\n\tgid_t gid;\n\tgids_t *gids;\n\tstruct gid_cache_s *next;\n} gids_cache_t;\n\n#define GIDS_HASH_LEN 64\nstatic gids_cache_t *gids_hashtbl[GIDS_HASH_LEN] = {NULL};\nstatic pthread_mutex_t gids_mutex = PTHREAD_MUTEX_INITIALIZER;\n\nstatic gids_t *\n_alloc_gids(int n, gid_t *gids)\n{\n\tgids_t *new;\n\n\tnew = (gids_t *)xmalloc(sizeof(gids_t));\n\tnew->ngids = n;\n\tnew->gids = gids;\n\treturn new;\n}\n\nstatic void\n_dealloc_gids(gids_t *p)\n{\n\txfree(p->gids);\n\txfree(p);\n}\n\n/* Duplicate a gids_t struct.  */\nstatic gids_t *\n_gids_dup(gids_t *g)\n{\n\tint buf_size;\n\tgids_t *n = xmalloc(sizeof(gids_t));\n\tn->ngids = g->ngids;\n\tbuf_size = g->ngids * sizeof(gid_t);\n\tn->gids = xmalloc(buf_size);\n\tmemcpy(n->gids, g->gids, buf_size);\n\treturn n;\n}\n\nstatic gids_cache_t *\n_alloc_gids_cache(char *user, gid_t gid, gids_t *gids, gids_cache_t *next)\n{\n\tgids_cache_t *p;\n\n\tp = (gids_cache_t *)xmalloc(sizeof(gids_cache_t));\n\tp->user = xstrdup(user);\n\tp->timestamp = time(NULL);\n\tp->gid = gid;\n\tp->gids = gids;\n\tp->next = next;\n\treturn p;\n}\n\nstatic void\n_dealloc_gids_cache(gids_cache_t *p)\n{\n\txfree(p->user);\n\t_dealloc_gids(p->gids);\n\txfree(p);\n}\n\nstatic size_t\n_gids_hashtbl_idx(const char *user)\n{\n\tuint64_t x = siphash_str(user);\n\treturn x % GIDS_HASH_LEN;\n}\n\nvoid\ngids_cache_purge(void)\n{\n\tint i;\n\tgids_cache_t *p, *q;\n\n\tslurm_mutex_lock(&gids_mutex);\n\tfor (i=0; i<GIDS_HASH_LEN; i++) {\n\t\tp = gids_hashtbl[i];\n\t\twhile (p) {\n\t\t\tq = p->next;\n\t\t\t_dealloc_gids_cache(p);\n\t\t\tp = q;\n\t\t}\n\t\tgids_hashtbl[i] = NULL;\n\t}\n\tslurm_mutex_unlock(&gids_mutex);\n}\n\nstatic void\n_gids_cache_register(char *user, gid_t gid, gids_t *gids)\n{\n\tsize_t idx;\n\tgids_cache_t *p, *q;\n\n\tidx = _gids_hashtbl_idx(user);\n\tq = gids_hashtbl[idx];\n\tp = _alloc_gids_cache(user, gid, gids, q);\n\tgids_hashtbl[idx] = p;\n\tdebug2(\"Cached group access list for %s/%d\", user, gid);\n}\n\n/* how many groups to use by default to avoid repeated calls to getgrouplist */\n#define NGROUPS_START 64\n\nstatic gids_t *_gids_cache_lookup(char *user, gid_t gid)\n{\n\tsize_t idx;\n\tgids_cache_t *p;\n\tbool found_but_old = false;\n\ttime_t now = 0;\n\tint ngroups = NGROUPS_START;\n\tgid_t *groups;\n\tgids_t *ret_gids = NULL;\n\n\tidx = _gids_hashtbl_idx(user);\n\tslurm_mutex_lock(&gids_mutex);\n\tp = gids_hashtbl[idx];\n\twhile (p) {\n\t\tif (xstrcmp(p->user, user) == 0 && p->gid == gid) {\n\t\t\tslurm_ctl_conf_t *cf = slurm_conf_lock();\n\t\t\tint group_ttl = cf->group_info & GROUP_TIME_MASK;\n\t\t\tslurm_conf_unlock();\n\t\t\tif (!group_ttl) {\n\t\t\t\tret_gids = _gids_dup(p->gids);\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t\tnow = time(NULL);\n\t\t\tif (difftime(now, p->timestamp) < group_ttl) {\n\t\t\t\tret_gids = _gids_dup(p->gids);\n\t\t\t\tgoto done;\n\t\t\t} else {\n\t\t\t\tfound_but_old = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tp = p->next;\n\t}\n\t/* Cache lookup failed or cached value was too old, fetch new\n\t * value and insert it into cache.  */\n\tgroups = xmalloc(ngroups * sizeof(gid_t));\n\twhile (getgrouplist(user, gid, groups, &ngroups) == -1) {\n\t\t/* group list larger than array, resize array to fit */\n\t\tgroups = xrealloc(groups, ngroups * sizeof(gid_t));\n\t}\n\tif (found_but_old) {\n\t\txfree(p->gids->gids);\n\t\tp->gids->gids = groups;\n\t\tp->gids->ngids = ngroups;\n\t\tp->timestamp = now;\n\t\tret_gids = _gids_dup(p->gids);\n\t} else {\n\t\tgids_t *gids = _alloc_gids(ngroups, groups);\n\t\t_gids_cache_register(user, gid, gids);\n\t\tret_gids = _gids_dup(gids);\n\t}\ndone:\n\tslurm_mutex_unlock(&gids_mutex);\n\treturn ret_gids;\n}\n\n\nextern void\ndestroy_starting_step(void *x)\n{\n\txfree(x);\n}\n\n\nstatic int\n_add_starting_step(uint16_t type, void *req)\n{\n\tstarting_step_t *starting_step;\n\tint rc = SLURM_SUCCESS;\n\n\t/* Add the step info to a list of starting processes that\n\t   cannot reliably be contacted. */\n\tslurm_mutex_lock(&conf->starting_steps_lock);\n\tstarting_step = xmalloc(sizeof(starting_step_t));\n\tif (!starting_step) {\n\t\terror(\"%s failed to allocate memory\", __func__);\n\t\trc = SLURM_FAILURE;\n\t\tgoto fail;\n\t}\n\tswitch (type) {\n\tcase LAUNCH_BATCH_JOB:\n\t\tstarting_step->job_id =\n\t\t\t((batch_job_launch_msg_t *)req)->job_id;\n\t\tstarting_step->step_id =\n\t\t\t((batch_job_launch_msg_t *)req)->step_id;\n\t\tbreak;\n\tcase LAUNCH_TASKS:\n\t\tstarting_step->job_id =\n\t\t\t((launch_tasks_request_msg_t *)req)->job_id;\n\t\tstarting_step->step_id =\n\t\t\t((launch_tasks_request_msg_t *)req)->job_step_id;\n\t\tbreak;\n\tcase REQUEST_LAUNCH_PROLOG:\n\t\tstarting_step->job_id  = ((prolog_launch_msg_t *)req)->job_id;\n\t\tstarting_step->step_id = SLURM_EXTERN_CONT;\n\t\tbreak;\n\tdefault:\n\t\terror(\"%s called with an invalid type: %u\", __func__, type);\n\t\trc = SLURM_FAILURE;\n\t\txfree(starting_step);\n\t\tgoto fail;\n\t}\n\n\tif (!list_append(conf->starting_steps, starting_step)) {\n\t\terror(\"%s failed to allocate memory for list\", __func__);\n\t\trc = SLURM_FAILURE;\n\t\txfree(starting_step);\n\t\tgoto fail;\n\t}\n\nfail:\n\tslurm_mutex_unlock(&conf->starting_steps_lock);\n\treturn rc;\n}\n\n\nstatic int\n_remove_starting_step(uint16_t type, void *req)\n{\n\tuint32_t job_id, step_id;\n\tListIterator iter;\n\tstarting_step_t *starting_step;\n\tint rc = SLURM_SUCCESS;\n\tbool found = false;\n\n\tslurm_mutex_lock(&conf->starting_steps_lock);\n\n\tswitch(type) {\n\tcase LAUNCH_BATCH_JOB:\n\t\tjob_id =  ((batch_job_launch_msg_t *)req)->job_id;\n\t\tstep_id = ((batch_job_launch_msg_t *)req)->step_id;\n\t\tbreak;\n\tcase LAUNCH_TASKS:\n\t\tjob_id =  ((launch_tasks_request_msg_t *)req)->job_id;\n\t\tstep_id = ((launch_tasks_request_msg_t *)req)->job_step_id;\n\t\tbreak;\n\tdefault:\n\t\terror(\"%s called with an invalid type: %u\", __func__, type);\n\t\trc = SLURM_FAILURE;\n\t\tgoto fail;\n\t}\n\n\titer = list_iterator_create(conf->starting_steps);\n\twhile ((starting_step = list_next(iter))) {\n\t\tif (starting_step->job_id  == job_id &&\n\t\t    starting_step->step_id == step_id) {\n\t\t\tstarting_step = list_remove(iter);\n\t\t\txfree(starting_step);\n\n\t\t\tfound = true;\n\t\t\tpthread_cond_broadcast(&conf->starting_steps_cond);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\terror(\"%s: step %u.%u not found\", __func__, job_id, step_id);\n\t\trc = SLURM_FAILURE;\n\t}\nfail:\n\tslurm_mutex_unlock(&conf->starting_steps_lock);\n\treturn rc;\n}\n\n\n\nstatic int _compare_starting_steps(void *listentry, void *key)\n{\n\tstarting_step_t *step0 = (starting_step_t *)listentry;\n\tstarting_step_t *step1 = (starting_step_t *)key;\n\n\tif (step1->step_id != NO_VAL)\n\t\treturn (step0->job_id  == step1->job_id &&\n\t\t\tstep0->step_id == step1->step_id);\n\telse\n\t\treturn (step0->job_id  == step1->job_id);\n}\n\n\n/* Wait for a step to get far enough in the launch process to have\n   a socket open, ready to handle RPC calls.  Pass step_id = NO_VAL\n   to wait on any step for the given job. */\n\nstatic int _wait_for_starting_step(uint32_t job_id, uint32_t step_id)\n{\n\tstarting_step_t  starting_step;\n\tstarting_step.job_id  = job_id;\n\tstarting_step.step_id = step_id;\n\tint num_passes = 0;\n\n\tslurm_mutex_lock(&conf->starting_steps_lock);\n\n\twhile (list_find_first( conf->starting_steps,\n\t\t\t\t&_compare_starting_steps,\n\t\t\t\t&starting_step )) {\n\t\tif (num_passes == 0) {\n\t\t\tif (step_id != NO_VAL)\n\t\t\t\tdebug( \"Blocked waiting for step %d.%d\",\n\t\t\t\t\tjob_id, step_id);\n\t\t\telse\n\t\t\t\tdebug( \"Blocked waiting for job %d, all steps\",\n\t\t\t\t\tjob_id);\n\t\t}\n\t\tnum_passes++;\n\n\t\tpthread_cond_wait(&conf->starting_steps_cond,\n\t\t\t\t  &conf->starting_steps_lock);\n\t}\n\tif (num_passes > 0) {\n\t\tif (step_id != NO_VAL)\n\t\t\tdebug( \"Finished wait for step %d.%d\",\n\t\t\t\tjob_id, step_id);\n\t\telse\n\t\t\tdebug( \"Finished wait for job %d, all steps\",\n\t\t\t\tjob_id);\n\t}\n\tslurm_mutex_unlock(&conf->starting_steps_lock);\n\n\treturn SLURM_SUCCESS;\n}\n\n\n/* Return true if the step has not yet confirmed that its socket to\n   handle RPC calls has been created.  Pass step_id = NO_VAL\n   to return true if any of the job's steps are still starting. */\nstatic bool _step_is_starting(uint32_t job_id, uint32_t step_id)\n{\n\tstarting_step_t  starting_step;\n\tstarting_step.job_id  = job_id;\n\tstarting_step.step_id = step_id;\n\tbool ret = false;\n\n\tslurm_mutex_lock(&conf->starting_steps_lock);\n\n\tif (list_find_first( conf->starting_steps,\n\t\t\t     &_compare_starting_steps,\n\t\t\t     &starting_step )) {\n\t\tret = true;\n\t}\n\n\tslurm_mutex_unlock(&conf->starting_steps_lock);\n\treturn ret;\n}\n\n/* Add this job to the list of jobs currently running their prolog */\nstatic void _add_job_running_prolog(uint32_t job_id)\n{\n\tuint32_t *job_running_prolog;\n\n\t/* Add the job to a list of jobs whose prologs are running */\n\tslurm_mutex_lock(&conf->prolog_running_lock);\n\tjob_running_prolog = xmalloc(sizeof(uint32_t));\n\tif (!job_running_prolog) {\n\t\terror(\"_add_job_running_prolog failed to allocate memory\");\n\t\tgoto fail;\n\t}\n\n\t*job_running_prolog = job_id;\n\tif (!list_append(conf->prolog_running_jobs, job_running_prolog)) {\n\t\terror(\"_add_job_running_prolog failed to append job to list\");\n\t\txfree(job_running_prolog);\n\t}\n\nfail:\n\tslurm_mutex_unlock(&conf->prolog_running_lock);\n}\n\n/* Remove this job from the list of jobs currently running their prolog */\nstatic void _remove_job_running_prolog(uint32_t job_id)\n{\n\tListIterator iter;\n\tuint32_t *job_running_prolog;\n\tbool found = false;\n\n\tslurm_mutex_lock(&conf->prolog_running_lock);\n\n\titer = list_iterator_create(conf->prolog_running_jobs);\n\twhile ((job_running_prolog = list_next(iter))) {\n\t\tif (*job_running_prolog  == job_id) {\n\t\t\tjob_running_prolog = list_remove(iter);\n\t\t\txfree(job_running_prolog);\n\n\t\t\tfound = true;\n\t\t\tpthread_cond_broadcast(&conf->prolog_running_cond);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\terror(\"_remove_job_running_prolog: job not found\");\n\n\tslurm_mutex_unlock(&conf->prolog_running_lock);\n}\n\nstatic int _match_jobid(void *listentry, void *key)\n{\n\tuint32_t *job0 = (uint32_t *)listentry;\n\tuint32_t *job1 = (uint32_t *)key;\n\n\treturn (*job0 == *job1);\n}\n\nstatic int _prolog_is_running (uint32_t jobid)\n{\n\tint rc = 0;\n\tif (list_find_first (conf->prolog_running_jobs,\n\t                     (ListFindF) _match_jobid, &jobid))\n\t\trc = 1;\n\treturn (rc);\n}\n\n/* Wait for the job's prolog to complete */\nstatic void _wait_for_job_running_prolog(uint32_t job_id)\n{\n\tdebug( \"Waiting for job %d's prolog to complete\", job_id);\n\tslurm_mutex_lock(&conf->prolog_running_lock);\n\n\twhile (_prolog_is_running (job_id)) {\n\t\tpthread_cond_wait(&conf->prolog_running_cond,\n\t\t\t\t  &conf->prolog_running_lock);\n\t}\n\n\tslurm_mutex_unlock(&conf->prolog_running_lock);\n\tdebug( \"Finished wait for job %d's prolog to complete\", job_id);\n}\n\n\nstatic void\n_rpc_forward_data(slurm_msg_t *msg)\n{\n\tforward_data_msg_t *req = (forward_data_msg_t *)msg->data;\n\tuint32_t req_uid;\n\tstruct sockaddr_un sa;\n\tint fd = -1, rc = 0;\n\n\tdebug3(\"Entering _rpc_forward_data, address: %s, len: %u\",\n\t       req->address, req->len);\n\n\t/* sanity check */\n\tif (strlen(req->address) > sizeof(sa.sun_path) - 1) {\n\t\tslurm_seterrno(EINVAL);\n\t\trc = errno;\n\t\tgoto done;\n\t}\n\n\t/* connect to specified address */\n\tfd = socket(AF_UNIX, SOCK_STREAM, 0);\n\tif (fd < 0) {\n\t\trc = errno;\n\t\terror(\"failed creating UNIX domain socket: %m\");\n\t\tgoto done;\n\t}\n\tmemset(&sa, 0, sizeof(sa));\n\tsa.sun_family = AF_UNIX;\n\tstrcpy(sa.sun_path, req->address);\n\twhile (((rc = connect(fd, (struct sockaddr *)&sa, SUN_LEN(&sa))) < 0) &&\n\t       (errno == EINTR));\n\tif (rc < 0) {\n\t\trc = errno;\n\t\tdebug2(\"failed connecting to specified socket '%s': %m\",\n\t\t       req->address);\n\t\tgoto done;\n\t}\n\n\treq_uid = (uint32_t)g_slurm_auth_get_uid(msg->auth_cred,\n\t\t\t\t\t\t conf->auth_info);\n\t/*\n\t * although always in localhost, we still convert it to network\n\t * byte order, to make it consistent with pack/unpack.\n\t */\n\treq_uid = htonl(req_uid);\n\tsafe_write(fd, &req_uid, sizeof(uint32_t));\n\treq_uid = htonl(req->len);\n\tsafe_write(fd, &req_uid, sizeof(uint32_t));\n\tsafe_write(fd, req->data, req->len);\n\nrwfail:\ndone:\n\tif (fd >= 0){\n\t\tclose(fd);\n\t}\n\tslurm_send_rc_msg(msg, rc);\n}\n\nstatic void _launch_complete_add(uint32_t job_id)\n{\n\tint j, empty;\n\n\tslurm_mutex_lock(&job_state_mutex);\n\tempty = -1;\n\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\tif (job_id == active_job_id[j])\n\t\t\tbreak;\n\t\tif ((active_job_id[j] == 0) && (empty == -1))\n\t\t\tempty = j;\n\t}\n\tif (j >= JOB_STATE_CNT || job_id != active_job_id[j]) {\n\t\tif (empty == -1)\t/* Discard oldest job */\n\t\t\tempty = 0;\n\t\tfor (j = empty + 1; j < JOB_STATE_CNT; j++) {\n\t\t\tactive_job_id[j - 1] = active_job_id[j];\n\t\t}\n\t\tactive_job_id[JOB_STATE_CNT - 1] = 0;\n\t\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\t\tif (active_job_id[j] == 0) {\n\t\t\t\tactive_job_id[j] = job_id;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tpthread_cond_signal(&job_state_cond);\n\tslurm_mutex_unlock(&job_state_mutex);\n\t_launch_complete_log(\"job add\", job_id);\n}\n\nstatic void _launch_complete_log(char *type, uint32_t job_id)\n{\n#if 0\n\tint j;\n\n\tinfo(\"active %s %u\", type, job_id);\n\tslurm_mutex_lock(&job_state_mutex);\n\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\tif (active_job_id[j] != 0) {\n\t\t\tinfo(\"active_job_id[%d]=%u\", j, active_job_id[j]);\n\t\t}\n\t}\n\tslurm_mutex_unlock(&job_state_mutex);\n#endif\n}\n\n/* Test if we have a specific job ID still running */\nstatic bool _launch_job_test(uint32_t job_id)\n{\n\tbool found = false;\n\tint j;\n\n\tslurm_mutex_lock(&job_state_mutex);\n\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\tif (job_id == active_job_id[j]) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tslurm_mutex_unlock(&job_state_mutex);\n\treturn found;\n}\n\n\nstatic void _launch_complete_rm(uint32_t job_id)\n{\n\tint j;\n\n\tslurm_mutex_lock(&job_state_mutex);\n\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\tif (job_id == active_job_id[j])\n\t\t\tbreak;\n\t}\n\tif (j < JOB_STATE_CNT && job_id == active_job_id[j]) {\n\t\tfor (j = j + 1; j < JOB_STATE_CNT; j++) {\n\t\t\tactive_job_id[j - 1] = active_job_id[j];\n\t\t}\n\t\tactive_job_id[JOB_STATE_CNT - 1] = 0;\n\t}\n\tslurm_mutex_unlock(&job_state_mutex);\n\t_launch_complete_log(\"job remove\", job_id);\n}\n\nstatic void _launch_complete_wait(uint32_t job_id)\n{\n\tint i, j, empty;\n\ttime_t start = time(NULL);\n\tstruct timeval now;\n\tstruct timespec timeout;\n\n\tslurm_mutex_lock(&job_state_mutex);\n\tfor (i = 0; ; i++) {\n\t\tempty = -1;\n\t\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\t\tif (job_id == active_job_id[j])\n\t\t\t\tbreak;\n\t\t\tif ((active_job_id[j] == 0) && (empty == -1))\n\t\t\t\tempty = j;\n\t\t}\n\t\tif (j < JOB_STATE_CNT)\t/* Found job, ready to return */\n\t\t\tbreak;\n\t\tif (difftime(time(NULL), start) <= 9) {  /* Retry for 9 secs */\n\t\t\tdebug2(\"wait for launch of job %u before suspending it\",\n\t\t\t       job_id);\n\t\t\tgettimeofday(&now, NULL);\n\t\t\ttimeout.tv_sec  = now.tv_sec + 1;\n\t\t\ttimeout.tv_nsec = now.tv_usec * 1000;\n\t\t\tpthread_cond_timedwait(&job_state_cond,&job_state_mutex,\n\t\t\t\t\t       &timeout);\n\t\t\tcontinue;\n\t\t}\n\t\tif (empty == -1)\t/* Discard oldest job */\n\t\t\tempty = 0;\n\t\tfor (j = empty + 1; j < JOB_STATE_CNT; j++) {\n\t\t\tactive_job_id[j - 1] = active_job_id[j];\n\t\t}\n\t\tactive_job_id[JOB_STATE_CNT - 1] = 0;\n\t\tfor (j = 0; j < JOB_STATE_CNT; j++) {\n\t\t\tif (active_job_id[j] == 0) {\n\t\t\t\tactive_job_id[j] = job_id;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\tslurm_mutex_unlock(&job_state_mutex);\n\t_launch_complete_log(\"job wait\", job_id);\n}\n\nstatic bool\n_requeue_setup_env_fail(void)\n{\n\tstatic time_t config_update = 0;\n\tstatic bool requeue = false;\n\n\tif (config_update != conf->last_update) {\n\t\tchar *sched_params = slurm_get_sched_params();\n\t\trequeue = (sched_params &&\n\t\t\t   (strstr(sched_params, \"no_env_cache\") ||\n\t\t\t    strstr(sched_params, \"requeue_setup_env_fail\")));\n\t\txfree(sched_params);\n\t\tconfig_update = conf->last_update;\n\t}\n\n\treturn requeue;\n}\n"], "filenames": ["NEWS", "src/slurmd/slurmd/req.c"], "buggy_code_start_loc": [30, 170], "buggy_code_end_loc": [30, 1422], "fixing_code_start_loc": [31, 171], "fixing_code_end_loc": [34, 1527], "type": "CWE-284", "message": "The _prolog_error function in slurmd/req.c in Slurm before 15.08.13, 16.x before 16.05.7, and 17.x before 17.02.0-pre4 has a vulnerability in how the slurmd daemon informs users of a Prolog failure on a compute node. That vulnerability could allow a user to assume control of an arbitrary file on the system. Any exploitation of this is dependent on the user being able to cause or anticipate the failure (non-zero return code) of a Prolog script that their job would run on. This issue affects all Slurm versions from 0.6.0 (September 2005) to present. Workarounds to prevent exploitation of this are to either disable your Prolog script, or modify it such that it always returns 0 (\"success\") and adjust it to set the node as down using scontrol instead of relying on the slurmd to handle that automatically. If you do not have a Prolog set you are unaffected by this issue.", "other": {"cve": {"id": "CVE-2016-10030", "sourceIdentifier": "cve@mitre.org", "published": "2017-01-05T11:59:00.133", "lastModified": "2017-01-11T02:59:05.197", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The _prolog_error function in slurmd/req.c in Slurm before 15.08.13, 16.x before 16.05.7, and 17.x before 17.02.0-pre4 has a vulnerability in how the slurmd daemon informs users of a Prolog failure on a compute node. That vulnerability could allow a user to assume control of an arbitrary file on the system. Any exploitation of this is dependent on the user being able to cause or anticipate the failure (non-zero return code) of a Prolog script that their job would run on. This issue affects all Slurm versions from 0.6.0 (September 2005) to present. Workarounds to prevent exploitation of this are to either disable your Prolog script, or modify it such that it always returns 0 (\"success\") and adjust it to set the node as down using scontrol instead of relying on the slurmd to handle that automatically. If you do not have a Prolog set you are unaffected by this issue."}, {"lang": "es", "value": "La funci\u00f3n _prolog_error en slurmd/req.c en Slurm en versiones anteriores a 15.08.13, 16.x en versiones anteriores a 16.05.7 y 17.x en versiones anteriores a 17.02.0-pre4 tiene una vulnerabilidad en como el slurmd daemon informa a los usuarios de un fallo Prolog en un nodo de c\u00e1lculo. Esta vulnerabilidad podr\u00eda permitir a un usuario asumir el control de un archivo arbitrario en el sistema. Cualquier explotaci\u00f3n de esto depende de que el usuario pueda provocar o anticipar el fallo (c\u00f3digo de retorno distinto de cero) de una secuencia de comandos Prolog que ejecutar\u00eda su trabajo. Este problema afecta a todas las versiones de Slurm desde la 0.6.0 (septiembre de 2005) hasta el presente. Las soluciones para evitar la explotaci\u00f3n de esto son para deshabilitar su secuencia de comandos Prolog, o modificarlo de tal manera que siempre devuelva 0 (\"\u00e9xito\") y ajustarlo para establecer el nodo como ca\u00eddo utilizando scontrol en lugar de confiar en slurmd para manejarlo autom\u00e1ticamente. Si no tiene un conjunto Prolog, no se ver\u00e1 afectado por este problema."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.2, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:H/Au:N/C:C/I:C/A:C", "accessVector": "NETWORK", "accessComplexity": "HIGH", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.6}, "baseSeverity": "HIGH", "exploitabilityScore": 4.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-284"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:*:*:*:*:*:*:*:*", "versionEndIncluding": "15.08.12", "matchCriteriaId": "E0909E51-4336-4389-93EA-0B2F5497F572"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.0:*:*:*:*:*:*:*", "matchCriteriaId": "4CF0BB7E-6A58-44F1-B256-050B994337C7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.0:pre1:*:*:*:*:*:*", "matchCriteriaId": "BC0DE382-132A-4368-A5EE-E4C41356DFDC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.0:pre2:*:*:*:*:*:*", "matchCriteriaId": "FE5FDB9D-001B-40E0-A03C-FC6DC613A9AC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "91FFABC7-8770-4D73-901F-5DB67FE038FB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "7744D1CC-1C6A-48E3-9CEB-EC2F4ACCF5A8"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.1:*:*:*:*:*:*:*", "matchCriteriaId": "4D0908F7-FB4C-430A-A710-8436F02FCFEA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.2:*:*:*:*:*:*:*", "matchCriteriaId": "66F7F64E-CB09-440E-A54B-DDC7BBB0B3BF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.3:*:*:*:*:*:*:*", "matchCriteriaId": "985B2E2F-6215-4B7A-B6DD-3D1814E1F015"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.4:*:*:*:*:*:*:*", "matchCriteriaId": "6E18A9C0-A73F-4BAA-A7FF-9B2B30C61224"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.5:*:*:*:*:*:*:*", "matchCriteriaId": "7BE93BD8-C330-4504-B44A-C727D71D7EC4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:16.05.6:*:*:*:*:*:*:*", "matchCriteriaId": "1A05FDEE-FE60-45C0-8AA8-277DADF87926"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:17.02.0:pre1:*:*:*:*:*:*", "matchCriteriaId": "C2D03B41-F9D6-41EB-AEE7-B2673BECA338"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:17.02.0:pre2:*:*:*:*:*:*", "matchCriteriaId": "E2DD1E41-DE4C-4C0A-ABBD-3360AF9DECFB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:schedmd:slurm:17.02.0:pre3:*:*:*:*:*:*", "matchCriteriaId": "CE470D55-EB72-4C0E-BA6F-A7D9A5B34C13"}]}]}], "references": [{"url": "http://www.securityfocus.com/bid/95299", "source": "cve@mitre.org"}, {"url": "https://github.com/SchedMD/slurm/commit/92362a92fffe60187df61f99ab11c249d44120ee", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://www.schedmd.com/news.php?id=178", "source": "cve@mitre.org", "tags": ["Mitigation", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/SchedMD/slurm/commit/92362a92fffe60187df61f99ab11c249d44120ee"}}