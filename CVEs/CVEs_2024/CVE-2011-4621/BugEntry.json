{"buggy_code": ["/*\n *  linux/kernel/fork.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n/*\n *  'fork.c' contains the help-routines for the 'fork' system call\n * (see also entry.S and others).\n * Fork is rather simple, once you get the hang of it, but the memory\n * management can be a bitch. See 'mm/memory.c': 'copy_page_range()'\n */\n\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/unistd.h>\n#include <linux/module.h>\n#include <linux/vmalloc.h>\n#include <linux/completion.h>\n#include <linux/personality.h>\n#include <linux/mempolicy.h>\n#include <linux/sem.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/iocontext.h>\n#include <linux/key.h>\n#include <linux/binfmts.h>\n#include <linux/mman.h>\n#include <linux/mmu_notifier.h>\n#include <linux/fs.h>\n#include <linux/nsproxy.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/cgroup.h>\n#include <linux/security.h>\n#include <linux/hugetlb.h>\n#include <linux/swap.h>\n#include <linux/syscalls.h>\n#include <linux/jiffies.h>\n#include <linux/tracehook.h>\n#include <linux/futex.h>\n#include <linux/compat.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/rcupdate.h>\n#include <linux/ptrace.h>\n#include <linux/mount.h>\n#include <linux/audit.h>\n#include <linux/memcontrol.h>\n#include <linux/ftrace.h>\n#include <linux/profile.h>\n#include <linux/rmap.h>\n#include <linux/ksm.h>\n#include <linux/acct.h>\n#include <linux/tsacct_kern.h>\n#include <linux/cn_proc.h>\n#include <linux/freezer.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/random.h>\n#include <linux/tty.h>\n#include <linux/proc_fs.h>\n#include <linux/blkdev.h>\n#include <linux/fs_struct.h>\n#include <linux/magic.h>\n#include <linux/perf_event.h>\n#include <linux/posix-timers.h>\n#include <linux/user-return-notifier.h>\n#include <linux/oom.h>\n\n#include <asm/pgtable.h>\n#include <asm/pgalloc.h>\n#include <asm/uaccess.h>\n#include <asm/mmu_context.h>\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n\n#include <trace/events/sched.h>\n\n/*\n * Protected counters by write_lock_irq(&tasklist_lock)\n */\nunsigned long total_forks;\t/* Handle normal Linux uptimes. */\nint nr_threads; \t\t/* The idle threads do not count.. */\n\nint max_threads;\t\t/* tunable limit on nr_threads */\n\nDEFINE_PER_CPU(unsigned long, process_counts) = 0;\n\n__cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */\n\n#ifdef CONFIG_PROVE_RCU\nint lockdep_tasklist_lock_is_held(void)\n{\n\treturn lockdep_is_held(&tasklist_lock);\n}\nEXPORT_SYMBOL_GPL(lockdep_tasklist_lock_is_held);\n#endif /* #ifdef CONFIG_PROVE_RCU */\n\nint nr_processes(void)\n{\n\tint cpu;\n\tint total = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\ttotal += per_cpu(process_counts, cpu);\n\n\treturn total;\n}\n\n#ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR\n# define alloc_task_struct()\tkmem_cache_alloc(task_struct_cachep, GFP_KERNEL)\n# define free_task_struct(tsk)\tkmem_cache_free(task_struct_cachep, (tsk))\nstatic struct kmem_cache *task_struct_cachep;\n#endif\n\n#ifndef __HAVE_ARCH_THREAD_INFO_ALLOCATOR\nstatic inline struct thread_info *alloc_thread_info(struct task_struct *tsk)\n{\n#ifdef CONFIG_DEBUG_STACK_USAGE\n\tgfp_t mask = GFP_KERNEL | __GFP_ZERO;\n#else\n\tgfp_t mask = GFP_KERNEL;\n#endif\n\treturn (struct thread_info *)__get_free_pages(mask, THREAD_SIZE_ORDER);\n}\n\nstatic inline void free_thread_info(struct thread_info *ti)\n{\n\tfree_pages((unsigned long)ti, THREAD_SIZE_ORDER);\n}\n#endif\n\n/* SLAB cache for signal_struct structures (tsk->signal) */\nstatic struct kmem_cache *signal_cachep;\n\n/* SLAB cache for sighand_struct structures (tsk->sighand) */\nstruct kmem_cache *sighand_cachep;\n\n/* SLAB cache for files_struct structures (tsk->files) */\nstruct kmem_cache *files_cachep;\n\n/* SLAB cache for fs_struct structures (tsk->fs) */\nstruct kmem_cache *fs_cachep;\n\n/* SLAB cache for vm_area_struct structures */\nstruct kmem_cache *vm_area_cachep;\n\n/* SLAB cache for mm_struct structures (tsk->mm) */\nstatic struct kmem_cache *mm_cachep;\n\nstatic void account_kernel_stack(struct thread_info *ti, int account)\n{\n\tstruct zone *zone = page_zone(virt_to_page(ti));\n\n\tmod_zone_page_state(zone, NR_KERNEL_STACK, account);\n}\n\nvoid free_task(struct task_struct *tsk)\n{\n\tprop_local_destroy_single(&tsk->dirties);\n\taccount_kernel_stack(tsk->stack, -1);\n\tfree_thread_info(tsk->stack);\n\trt_mutex_debug_task_free(tsk);\n\tftrace_graph_exit_task(tsk);\n\tfree_task_struct(tsk);\n}\nEXPORT_SYMBOL(free_task);\n\nstatic inline void free_signal_struct(struct signal_struct *sig)\n{\n\ttaskstats_tgid_free(sig);\n\tkmem_cache_free(signal_cachep, sig);\n}\n\nstatic inline void put_signal_struct(struct signal_struct *sig)\n{\n\tif (atomic_dec_and_test(&sig->sigcnt))\n\t\tfree_signal_struct(sig);\n}\n\nvoid __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(atomic_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}\n\n/*\n * macro override instead of weak attribute alias, to workaround\n * gcc 4.1.0 and 4.1.1 bugs with weak attribute and empty functions.\n */\n#ifndef arch_task_cache_init\n#define arch_task_cache_init()\n#endif\n\nvoid __init fork_init(unsigned long mempages)\n{\n#ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR\n#ifndef ARCH_MIN_TASKALIGN\n#define ARCH_MIN_TASKALIGN\tL1_CACHE_BYTES\n#endif\n\t/* create a slab on which task_structs can be allocated */\n\ttask_struct_cachep =\n\t\tkmem_cache_create(\"task_struct\", sizeof(struct task_struct),\n\t\t\tARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);\n#endif\n\n\t/* do the arch specific task caches init */\n\tarch_task_cache_init();\n\n\t/*\n\t * The default maximum number of threads is set to a safe\n\t * value: the thread structures can take up at most half\n\t * of memory.\n\t */\n\tmax_threads = mempages / (8 * THREAD_SIZE / PAGE_SIZE);\n\n\t/*\n\t * we need to allow at least 20 threads to boot a system\n\t */\n\tif(max_threads < 20)\n\t\tmax_threads = 20;\n\n\tinit_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads/2;\n\tinit_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads/2;\n\tinit_task.signal->rlim[RLIMIT_SIGPENDING] =\n\t\tinit_task.signal->rlim[RLIMIT_NPROC];\n}\n\nint __attribute__((weak)) arch_dup_task_struct(struct task_struct *dst,\n\t\t\t\t\t       struct task_struct *src)\n{\n\t*dst = *src;\n\treturn 0;\n}\n\nstatic struct task_struct *dup_task_struct(struct task_struct *orig)\n{\n\tstruct task_struct *tsk;\n\tstruct thread_info *ti;\n\tunsigned long *stackend;\n\n\tint err;\n\n\tprepare_to_copy(orig);\n\n\ttsk = alloc_task_struct();\n\tif (!tsk)\n\t\treturn NULL;\n\n\tti = alloc_thread_info(tsk);\n\tif (!ti) {\n\t\tfree_task_struct(tsk);\n\t\treturn NULL;\n\t}\n\n \terr = arch_dup_task_struct(tsk, orig);\n\tif (err)\n\t\tgoto out;\n\n\ttsk->stack = ti;\n\n\terr = prop_local_init_single(&tsk->dirties);\n\tif (err)\n\t\tgoto out;\n\n\tsetup_thread_stack(tsk, orig);\n\tclear_user_return_notifier(tsk);\n\tstackend = end_of_stack(tsk);\n\t*stackend = STACK_END_MAGIC;\t/* for overflow detection */\n\n#ifdef CONFIG_CC_STACKPROTECTOR\n\ttsk->stack_canary = get_random_int();\n#endif\n\n\t/* One for us, one for whoever does the \"release_task()\" (usually parent) */\n\tatomic_set(&tsk->usage,2);\n\tatomic_set(&tsk->fs_excl, 0);\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\ttsk->btrace_seq = 0;\n#endif\n\ttsk->splice_pipe = NULL;\n\n\taccount_kernel_stack(ti, 1);\n\n\treturn tsk;\n\nout:\n\tfree_thread_info(ti);\n\tfree_task_struct(tsk);\n\treturn NULL;\n}\n\n#ifdef CONFIG_MMU\nstatic int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)\n{\n\tstruct vm_area_struct *mpnt, *tmp, *prev, **pprev;\n\tstruct rb_node **rb_link, *rb_parent;\n\tint retval;\n\tunsigned long charge;\n\tstruct mempolicy *pol;\n\n\tdown_write(&oldmm->mmap_sem);\n\tflush_cache_dup_mm(oldmm);\n\t/*\n\t * Not linked in yet - no deadlock potential:\n\t */\n\tdown_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);\n\n\tmm->locked_vm = 0;\n\tmm->mmap = NULL;\n\tmm->mmap_cache = NULL;\n\tmm->free_area_cache = oldmm->mmap_base;\n\tmm->cached_hole_size = ~0UL;\n\tmm->map_count = 0;\n\tcpumask_clear(mm_cpumask(mm));\n\tmm->mm_rb = RB_ROOT;\n\trb_link = &mm->mm_rb.rb_node;\n\trb_parent = NULL;\n\tpprev = &mm->mmap;\n\tretval = ksm_fork(mm, oldmm);\n\tif (retval)\n\t\tgoto out;\n\n\tprev = NULL;\n\tfor (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {\n\t\tstruct file *file;\n\n\t\tif (mpnt->vm_flags & VM_DONTCOPY) {\n\t\t\tlong pages = vma_pages(mpnt);\n\t\t\tmm->total_vm -= pages;\n\t\t\tvm_stat_account(mm, mpnt->vm_flags, mpnt->vm_file,\n\t\t\t\t\t\t\t\t-pages);\n\t\t\tcontinue;\n\t\t}\n\t\tcharge = 0;\n\t\tif (mpnt->vm_flags & VM_ACCOUNT) {\n\t\t\tunsigned int len = (mpnt->vm_end - mpnt->vm_start) >> PAGE_SHIFT;\n\t\t\tif (security_vm_enough_memory(len))\n\t\t\t\tgoto fail_nomem;\n\t\t\tcharge = len;\n\t\t}\n\t\ttmp = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);\n\t\tif (!tmp)\n\t\t\tgoto fail_nomem;\n\t\t*tmp = *mpnt;\n\t\tINIT_LIST_HEAD(&tmp->anon_vma_chain);\n\t\tpol = mpol_dup(vma_policy(mpnt));\n\t\tretval = PTR_ERR(pol);\n\t\tif (IS_ERR(pol))\n\t\t\tgoto fail_nomem_policy;\n\t\tvma_set_policy(tmp, pol);\n\t\ttmp->vm_mm = mm;\n\t\tif (anon_vma_fork(tmp, mpnt))\n\t\t\tgoto fail_nomem_anon_vma_fork;\n\t\ttmp->vm_flags &= ~VM_LOCKED;\n\t\ttmp->vm_next = tmp->vm_prev = NULL;\n\t\tfile = tmp->vm_file;\n\t\tif (file) {\n\t\t\tstruct inode *inode = file->f_path.dentry->d_inode;\n\t\t\tstruct address_space *mapping = file->f_mapping;\n\n\t\t\tget_file(file);\n\t\t\tif (tmp->vm_flags & VM_DENYWRITE)\n\t\t\t\tatomic_dec(&inode->i_writecount);\n\t\t\tspin_lock(&mapping->i_mmap_lock);\n\t\t\tif (tmp->vm_flags & VM_SHARED)\n\t\t\t\tmapping->i_mmap_writable++;\n\t\t\ttmp->vm_truncate_count = mpnt->vm_truncate_count;\n\t\t\tflush_dcache_mmap_lock(mapping);\n\t\t\t/* insert tmp into the share list, just after mpnt */\n\t\t\tvma_prio_tree_add(tmp, mpnt);\n\t\t\tflush_dcache_mmap_unlock(mapping);\n\t\t\tspin_unlock(&mapping->i_mmap_lock);\n\t\t}\n\n\t\t/*\n\t\t * Clear hugetlb-related page reserves for children. This only\n\t\t * affects MAP_PRIVATE mappings. Faults generated by the child\n\t\t * are not guaranteed to succeed, even if read-only\n\t\t */\n\t\tif (is_vm_hugetlb_page(tmp))\n\t\t\treset_vma_resv_huge_pages(tmp);\n\n\t\t/*\n\t\t * Link in the new vma and copy the page table entries.\n\t\t */\n\t\t*pprev = tmp;\n\t\tpprev = &tmp->vm_next;\n\t\ttmp->vm_prev = prev;\n\t\tprev = tmp;\n\n\t\t__vma_link_rb(mm, tmp, rb_link, rb_parent);\n\t\trb_link = &tmp->vm_rb.rb_right;\n\t\trb_parent = &tmp->vm_rb;\n\n\t\tmm->map_count++;\n\t\tretval = copy_page_range(mm, oldmm, mpnt);\n\n\t\tif (tmp->vm_ops && tmp->vm_ops->open)\n\t\t\ttmp->vm_ops->open(tmp);\n\n\t\tif (retval)\n\t\t\tgoto out;\n\t}\n\t/* a new mm has just been created */\n\tarch_dup_mmap(oldmm, mm);\n\tretval = 0;\nout:\n\tup_write(&mm->mmap_sem);\n\tflush_tlb_mm(oldmm);\n\tup_write(&oldmm->mmap_sem);\n\treturn retval;\nfail_nomem_anon_vma_fork:\n\tmpol_put(pol);\nfail_nomem_policy:\n\tkmem_cache_free(vm_area_cachep, tmp);\nfail_nomem:\n\tretval = -ENOMEM;\n\tvm_unacct_memory(charge);\n\tgoto out;\n}\n\nstatic inline int mm_alloc_pgd(struct mm_struct * mm)\n{\n\tmm->pgd = pgd_alloc(mm);\n\tif (unlikely(!mm->pgd))\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic inline void mm_free_pgd(struct mm_struct * mm)\n{\n\tpgd_free(mm, mm->pgd);\n}\n#else\n#define dup_mmap(mm, oldmm)\t(0)\n#define mm_alloc_pgd(mm)\t(0)\n#define mm_free_pgd(mm)\n#endif /* CONFIG_MMU */\n\n__cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);\n\n#define allocate_mm()\t(kmem_cache_alloc(mm_cachep, GFP_KERNEL))\n#define free_mm(mm)\t(kmem_cache_free(mm_cachep, (mm)))\n\nstatic unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;\n\nstatic int __init coredump_filter_setup(char *s)\n{\n\tdefault_dump_filter =\n\t\t(simple_strtoul(s, NULL, 0) << MMF_DUMP_FILTER_SHIFT) &\n\t\tMMF_DUMP_FILTER_MASK;\n\treturn 1;\n}\n\n__setup(\"coredump_filter=\", coredump_filter_setup);\n\n#include <linux/init_task.h>\n\nstatic void mm_init_aio(struct mm_struct *mm)\n{\n#ifdef CONFIG_AIO\n\tspin_lock_init(&mm->ioctx_lock);\n\tINIT_HLIST_HEAD(&mm->ioctx_list);\n#endif\n}\n\nstatic struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)\n{\n\tatomic_set(&mm->mm_users, 1);\n\tatomic_set(&mm->mm_count, 1);\n\tinit_rwsem(&mm->mmap_sem);\n\tINIT_LIST_HEAD(&mm->mmlist);\n\tmm->flags = (current->mm) ?\n\t\t(current->mm->flags & MMF_INIT_MASK) : default_dump_filter;\n\tmm->core_state = NULL;\n\tmm->nr_ptes = 0;\n\tmemset(&mm->rss_stat, 0, sizeof(mm->rss_stat));\n\tspin_lock_init(&mm->page_table_lock);\n\tmm->free_area_cache = TASK_UNMAPPED_BASE;\n\tmm->cached_hole_size = ~0UL;\n\tmm_init_aio(mm);\n\tmm_init_owner(mm, p);\n\tatomic_set(&mm->oom_disable_count, 0);\n\n\tif (likely(!mm_alloc_pgd(mm))) {\n\t\tmm->def_flags = 0;\n\t\tmmu_notifier_mm_init(mm);\n\t\treturn mm;\n\t}\n\n\tfree_mm(mm);\n\treturn NULL;\n}\n\n/*\n * Allocate and initialize an mm_struct.\n */\nstruct mm_struct * mm_alloc(void)\n{\n\tstruct mm_struct * mm;\n\n\tmm = allocate_mm();\n\tif (mm) {\n\t\tmemset(mm, 0, sizeof(*mm));\n\t\tmm = mm_init(mm, current);\n\t}\n\treturn mm;\n}\n\n/*\n * Called when the last reference to the mm\n * is dropped: either by a lazy thread or by\n * mmput. Free the page directory and the mm.\n */\nvoid __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tfree_mm(mm);\n}\nEXPORT_SYMBOL_GPL(__mmdrop);\n\n/*\n * Decrement the use count and release all resources for an mm.\n */\nvoid mmput(struct mm_struct *mm)\n{\n\tmight_sleep();\n\n\tif (atomic_dec_and_test(&mm->mm_users)) {\n\t\texit_aio(mm);\n\t\tksm_exit(mm);\n\t\texit_mmap(mm);\n\t\tset_mm_exe_file(mm, NULL);\n\t\tif (!list_empty(&mm->mmlist)) {\n\t\t\tspin_lock(&mmlist_lock);\n\t\t\tlist_del(&mm->mmlist);\n\t\t\tspin_unlock(&mmlist_lock);\n\t\t}\n\t\tput_swap_token(mm);\n\t\tif (mm->binfmt)\n\t\t\tmodule_put(mm->binfmt->module);\n\t\tmmdrop(mm);\n\t}\n}\nEXPORT_SYMBOL_GPL(mmput);\n\n/**\n * get_task_mm - acquire a reference to the task's mm\n *\n * Returns %NULL if the task has no mm.  Checks PF_KTHREAD (meaning\n * this kernel workthread has transiently adopted a user mm with use_mm,\n * to do its AIO) is not set and if so returns a reference to it, after\n * bumping up the use count.  User must release the mm via mmput()\n * after use.  Typically used by /proc and ptrace.\n */\nstruct mm_struct *get_task_mm(struct task_struct *task)\n{\n\tstruct mm_struct *mm;\n\n\ttask_lock(task);\n\tmm = task->mm;\n\tif (mm) {\n\t\tif (task->flags & PF_KTHREAD)\n\t\t\tmm = NULL;\n\t\telse\n\t\t\tatomic_inc(&mm->mm_users);\n\t}\n\ttask_unlock(task);\n\treturn mm;\n}\nEXPORT_SYMBOL_GPL(get_task_mm);\n\n/* Please note the differences between mmput and mm_release.\n * mmput is called whenever we stop holding onto a mm_struct,\n * error success whatever.\n *\n * mm_release is called after a mm_struct has been removed\n * from the current process.\n *\n * This difference is important for error handling, when we\n * only half set up a mm_struct for a new process and need to restore\n * the old one.  Because we mmput the new mm_struct before\n * restoring the old one. . .\n * Eric Biederman 10 January 1998\n */\nvoid mm_release(struct task_struct *tsk, struct mm_struct *mm)\n{\n\tstruct completion *vfork_done = tsk->vfork_done;\n\n\t/* Get rid of any futexes when releasing the mm */\n#ifdef CONFIG_FUTEX\n\tif (unlikely(tsk->robust_list)) {\n\t\texit_robust_list(tsk);\n\t\ttsk->robust_list = NULL;\n\t}\n#ifdef CONFIG_COMPAT\n\tif (unlikely(tsk->compat_robust_list)) {\n\t\tcompat_exit_robust_list(tsk);\n\t\ttsk->compat_robust_list = NULL;\n\t}\n#endif\n\tif (unlikely(!list_empty(&tsk->pi_state_list)))\n\t\texit_pi_state_list(tsk);\n#endif\n\n\t/* Get rid of any cached register state */\n\tdeactivate_mm(tsk, mm);\n\n\t/* notify parent sleeping on vfork() */\n\tif (vfork_done) {\n\t\ttsk->vfork_done = NULL;\n\t\tcomplete(vfork_done);\n\t}\n\n\t/*\n\t * If we're exiting normally, clear a user-space tid field if\n\t * requested.  We leave this alone when dying by signal, to leave\n\t * the value intact in a core dump, and to save the unnecessary\n\t * trouble otherwise.  Userland only wants this done for a sys_exit.\n\t */\n\tif (tsk->clear_child_tid) {\n\t\tif (!(tsk->flags & PF_SIGNALED) &&\n\t\t    atomic_read(&mm->mm_users) > 1) {\n\t\t\t/*\n\t\t\t * We don't check the error code - if userspace has\n\t\t\t * not set up a proper pointer then tough luck.\n\t\t\t */\n\t\t\tput_user(0, tsk->clear_child_tid);\n\t\t\tsys_futex(tsk->clear_child_tid, FUTEX_WAKE,\n\t\t\t\t\t1, NULL, NULL, 0);\n\t\t}\n\t\ttsk->clear_child_tid = NULL;\n\t}\n}\n\n/*\n * Allocate a new mm structure and copy contents from the\n * mm structure of the passed in task structure.\n */\nstruct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tif (!oldmm)\n\t\treturn NULL;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\t/* Initializing for Swap token stuff */\n\tmm->token_priority = 0;\n\tmm->last_interval = 0;\n\n\tif (!mm_init(mm, tsk))\n\t\tgoto fail_nomem;\n\n\tif (init_new_context(tsk, mm))\n\t\tgoto fail_nocontext;\n\n\tdup_mm_exe_file(oldmm, mm);\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n\nfail_nocontext:\n\t/*\n\t * If init_new_context() failed, we cannot use mmput() to free the mm\n\t * because it calls destroy_context()\n\t */\n\tmm_free_pgd(mm);\n\tfree_mm(mm);\n\treturn NULL;\n}\n\nstatic int copy_mm(unsigned long clone_flags, struct task_struct * tsk)\n{\n\tstruct mm_struct * mm, *oldmm;\n\tint retval;\n\n\ttsk->min_flt = tsk->maj_flt = 0;\n\ttsk->nvcsw = tsk->nivcsw = 0;\n#ifdef CONFIG_DETECT_HUNG_TASK\n\ttsk->last_switch_count = tsk->nvcsw + tsk->nivcsw;\n#endif\n\n\ttsk->mm = NULL;\n\ttsk->active_mm = NULL;\n\n\t/*\n\t * Are we cloning a kernel thread?\n\t *\n\t * We need to steal a active VM for that..\n\t */\n\toldmm = current->mm;\n\tif (!oldmm)\n\t\treturn 0;\n\n\tif (clone_flags & CLONE_VM) {\n\t\tatomic_inc(&oldmm->mm_users);\n\t\tmm = oldmm;\n\t\tgoto good_mm;\n\t}\n\n\tretval = -ENOMEM;\n\tmm = dup_mm(tsk);\n\tif (!mm)\n\t\tgoto fail_nomem;\n\ngood_mm:\n\t/* Initializing for Swap token stuff */\n\tmm->token_priority = 0;\n\tmm->last_interval = 0;\n\tif (tsk->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)\n\t\tatomic_inc(&mm->oom_disable_count);\n\n\ttsk->mm = mm;\n\ttsk->active_mm = mm;\n\treturn 0;\n\nfail_nomem:\n\treturn retval;\n}\n\nstatic int copy_fs(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct fs_struct *fs = current->fs;\n\tif (clone_flags & CLONE_FS) {\n\t\t/* tsk->fs is already what we want */\n\t\tspin_lock(&fs->lock);\n\t\tif (fs->in_exec) {\n\t\t\tspin_unlock(&fs->lock);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t\tfs->users++;\n\t\tspin_unlock(&fs->lock);\n\t\treturn 0;\n\t}\n\ttsk->fs = copy_fs_struct(fs);\n\tif (!tsk->fs)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic int copy_files(unsigned long clone_flags, struct task_struct * tsk)\n{\n\tstruct files_struct *oldf, *newf;\n\tint error = 0;\n\n\t/*\n\t * A background process may not have any files ...\n\t */\n\toldf = current->files;\n\tif (!oldf)\n\t\tgoto out;\n\n\tif (clone_flags & CLONE_FILES) {\n\t\tatomic_inc(&oldf->count);\n\t\tgoto out;\n\t}\n\n\tnewf = dup_fd(oldf, &error);\n\tif (!newf)\n\t\tgoto out;\n\n\ttsk->files = newf;\n\terror = 0;\nout:\n\treturn error;\n}\n\nstatic int copy_io(unsigned long clone_flags, struct task_struct *tsk)\n{\n#ifdef CONFIG_BLOCK\n\tstruct io_context *ioc = current->io_context;\n\n\tif (!ioc)\n\t\treturn 0;\n\t/*\n\t * Share io context with parent, if CLONE_IO is set\n\t */\n\tif (clone_flags & CLONE_IO) {\n\t\ttsk->io_context = ioc_task_link(ioc);\n\t\tif (unlikely(!tsk->io_context))\n\t\t\treturn -ENOMEM;\n\t} else if (ioprio_valid(ioc->ioprio)) {\n\t\ttsk->io_context = alloc_io_context(GFP_KERNEL, -1);\n\t\tif (unlikely(!tsk->io_context))\n\t\t\treturn -ENOMEM;\n\n\t\ttsk->io_context->ioprio = ioc->ioprio;\n\t}\n#endif\n\treturn 0;\n}\n\nstatic int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sighand_struct *sig;\n\n\tif (clone_flags & CLONE_SIGHAND) {\n\t\tatomic_inc(&current->sighand->count);\n\t\treturn 0;\n\t}\n\tsig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);\n\trcu_assign_pointer(tsk->sighand, sig);\n\tif (!sig)\n\t\treturn -ENOMEM;\n\tatomic_set(&sig->count, 1);\n\tmemcpy(sig->action, current->sighand->action, sizeof(sig->action));\n\treturn 0;\n}\n\nvoid __cleanup_sighand(struct sighand_struct *sighand)\n{\n\tif (atomic_dec_and_test(&sighand->count))\n\t\tkmem_cache_free(sighand_cachep, sighand);\n}\n\n\n/*\n * Initialize POSIX timer handling for a thread group.\n */\nstatic void posix_cpu_timers_init_group(struct signal_struct *sig)\n{\n\tunsigned long cpu_limit;\n\n\t/* Thread group counters. */\n\tthread_group_cputime_init(sig);\n\n\tcpu_limit = ACCESS_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);\n\tif (cpu_limit != RLIM_INFINITY) {\n\t\tsig->cputime_expires.prof_exp = secs_to_cputime(cpu_limit);\n\t\tsig->cputimer.running = 1;\n\t}\n\n\t/* The timer lists. */\n\tINIT_LIST_HEAD(&sig->cpu_timers[0]);\n\tINIT_LIST_HEAD(&sig->cpu_timers[1]);\n\tINIT_LIST_HEAD(&sig->cpu_timers[2]);\n}\n\nstatic int copy_signal(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct signal_struct *sig;\n\n\tif (clone_flags & CLONE_THREAD)\n\t\treturn 0;\n\n\tsig = kmem_cache_zalloc(signal_cachep, GFP_KERNEL);\n\ttsk->signal = sig;\n\tif (!sig)\n\t\treturn -ENOMEM;\n\n\tsig->nr_threads = 1;\n\tatomic_set(&sig->live, 1);\n\tatomic_set(&sig->sigcnt, 1);\n\tinit_waitqueue_head(&sig->wait_chldexit);\n\tif (clone_flags & CLONE_NEWPID)\n\t\tsig->flags |= SIGNAL_UNKILLABLE;\n\tsig->curr_target = tsk;\n\tinit_sigpending(&sig->shared_pending);\n\tINIT_LIST_HEAD(&sig->posix_timers);\n\n\thrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\tsig->real_timer.function = it_real_fn;\n\n\ttask_lock(current->group_leader);\n\tmemcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);\n\ttask_unlock(current->group_leader);\n\n\tposix_cpu_timers_init_group(sig);\n\n\ttty_audit_fork(sig);\n\n\tsig->oom_adj = current->signal->oom_adj;\n\tsig->oom_score_adj = current->signal->oom_score_adj;\n\n\tmutex_init(&sig->cred_guard_mutex);\n\n\treturn 0;\n}\n\nstatic void copy_flags(unsigned long clone_flags, struct task_struct *p)\n{\n\tunsigned long new_flags = p->flags;\n\n\tnew_flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);\n\tnew_flags |= PF_FORKNOEXEC;\n\tnew_flags |= PF_STARTING;\n\tp->flags = new_flags;\n\tclear_freeze_flag(p);\n}\n\nSYSCALL_DEFINE1(set_tid_address, int __user *, tidptr)\n{\n\tcurrent->clear_child_tid = tidptr;\n\n\treturn task_pid_vnr(current);\n}\n\nstatic void rt_mutex_init_task(struct task_struct *p)\n{\n\traw_spin_lock_init(&p->pi_lock);\n#ifdef CONFIG_RT_MUTEXES\n\tplist_head_init_raw(&p->pi_waiters, &p->pi_lock);\n\tp->pi_blocked_on = NULL;\n#endif\n}\n\n#ifdef CONFIG_MM_OWNER\nvoid mm_init_owner(struct mm_struct *mm, struct task_struct *p)\n{\n\tmm->owner = p;\n}\n#endif /* CONFIG_MM_OWNER */\n\n/*\n * Initialize POSIX timer handling for a single task.\n */\nstatic void posix_cpu_timers_init(struct task_struct *tsk)\n{\n\ttsk->cputime_expires.prof_exp = cputime_zero;\n\ttsk->cputime_expires.virt_exp = cputime_zero;\n\ttsk->cputime_expires.sched_exp = 0;\n\tINIT_LIST_HEAD(&tsk->cpu_timers[0]);\n\tINIT_LIST_HEAD(&tsk->cpu_timers[1]);\n\tINIT_LIST_HEAD(&tsk->cpu_timers[2]);\n}\n\n/*\n * This creates a new process as a copy of the old one,\n * but does not actually start it yet.\n *\n * It copies the registers, and all the appropriate\n * parts of the process environment (as per the clone\n * flags). The actual kick-off is left to the caller.\n */\nstatic struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tstruct pt_regs *regs,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\tint cgroup_callbacks_done = 0;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = cputime_zero;\n\tp->stime = cputime_zero;\n\tp->gtime = cputime_zero;\n\tp->utimescaled = cputime_zero;\n\tp->stimescaled = cputime_zero;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_utime = cputime_zero;\n\tp->prev_stime = cputime_zero;\n#endif\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tp->lock_depth = -1;\t\t/* -1 = no lock */\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n \tif (IS_ERR(p->mempolicy)) {\n \t\tretval = PTR_ERR(p->mempolicy);\n \t\tp->mempolicy = NULL;\n \t\tgoto bad_fork_cleanup_cgroup;\n \t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tp->hardirqs_enabled = 1;\n#else\n\tp->hardirqs_enabled = 0;\n#endif\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_CGROUP_MEM_RES_CTLR\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p, clone_flags);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tif ((retval = audit_alloc(p)))\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tif ((retval = copy_semundo(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_audit;\n\tif ((retval = copy_files(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_semundo;\n\tif ((retval = copy_fs(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_files;\n\tif ((retval = copy_sighand(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_fs;\n\tif ((retval = copy_signal(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_sighand;\n\tif ((retval = copy_mm(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_signal;\n\tif ((retval = copy_namespaces(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_mm;\n\tif ((retval = copy_io(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p, regs);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\n\t\tif (clone_flags & CLONE_NEWPID) {\n\t\t\tretval = pid_ns_prepare_proc(p->nsproxy->pid_ns);\n\t\t\tif (retval < 0)\n\t\t\t\tgoto bad_fork_free_pid;\n\t\t}\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tif (current->nsproxy != p->nsproxy) {\n\t\tretval = ns_cgroup_clone(p, pid);\n\t\tif (retval)\n\t\t\tgoto bad_fork_free_pid;\n\t}\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\n\t/* Now that the task is set up, run cgroup callbacks if\n\t * necessary. We need to run them before the task is visible\n\t * on the tasklist. */\n\tcgroup_fork_callbacks(p);\n\tcgroup_callbacks_done = 1;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n \t */\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\ttracehook_finish_clone(p, clone_flags, trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (clone_flags & CLONE_NEWPID)\n\t\t\t\tp->nsproxy->pid_ns->child_reaper = p;\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__get_cpu_var(process_counts)++;\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tperf_event_fork(p);\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm) {\n\t\ttask_lock(p);\n\t\tif (p->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)\n\t\t\tatomic_dec(&p->mm->oom_disable_count);\n\t\ttask_unlock(p);\n\t\tmmput(p->mm);\n\t}\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tcgroup_exit(p, cgroup_callbacks_done);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}\n\nnoinline struct pt_regs * __cpuinit __attribute__((weak)) idle_regs(struct pt_regs *regs)\n{\n\tmemset(regs, 0, sizeof(struct pt_regs));\n\treturn regs;\n}\n\nstatic inline void init_idle_pids(struct pid_link *links)\n{\n\tenum pid_type type;\n\n\tfor (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {\n\t\tINIT_HLIST_NODE(&links[type].node); /* not really needed */\n\t\tlinks[type].pid = &init_struct_pid;\n\t}\n}\n\nstruct task_struct * __cpuinit fork_idle(int cpu)\n{\n\tstruct task_struct *task;\n\tstruct pt_regs regs;\n\n\ttask = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL,\n\t\t\t    &init_struct_pid, 0);\n\tif (!IS_ERR(task)) {\n\t\tinit_idle_pids(task->pids);\n\t\tinit_idle(task, cpu);\n\t}\n\n\treturn task;\n}\n\n/*\n *  Ok, this is the main fork-routine.\n *\n * It copies the process, and if successful kick-starts\n * it and waits for it to finish using the VM if required.\n */\nlong do_fork(unsigned long clone_flags,\n\t      unsigned long stack_start,\n\t      struct pt_regs *regs,\n\t      unsigned long stack_size,\n\t      int __user *parent_tidptr,\n\t      int __user *child_tidptr)\n{\n\tstruct task_struct *p;\n\tint trace = 0;\n\tlong nr;\n\n\t/*\n\t * Do some preliminary argument and permissions checking before we\n\t * actually start allocating stuff\n\t */\n\tif (clone_flags & CLONE_NEWUSER) {\n\t\tif (clone_flags & CLONE_THREAD)\n\t\t\treturn -EINVAL;\n\t\t/* hopefully this check will go away when userns support is\n\t\t * complete\n\t\t */\n\t\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SETUID) ||\n\t\t\t\t!capable(CAP_SETGID))\n\t\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We hope to recycle these flags after 2.6.26\n\t */\n\tif (unlikely(clone_flags & CLONE_STOPPED)) {\n\t\tstatic int __read_mostly count = 100;\n\n\t\tif (count > 0 && printk_ratelimit()) {\n\t\t\tchar comm[TASK_COMM_LEN];\n\n\t\t\tcount--;\n\t\t\tprintk(KERN_INFO \"fork(): process `%s' used deprecated \"\n\t\t\t\t\t\"clone flags 0x%lx\\n\",\n\t\t\t\tget_task_comm(comm, current),\n\t\t\t\tclone_flags & CLONE_STOPPED);\n\t\t}\n\t}\n\n\t/*\n\t * When called from kernel_thread, don't do user tracing stuff.\n\t */\n\tif (likely(user_mode(regs)))\n\t\ttrace = tracehook_prepare_clone(clone_flags);\n\n\tp = copy_process(clone_flags, stack_start, regs, stack_size,\n\t\t\t child_tidptr, NULL, trace);\n\t/*\n\t * Do this prior waking up the new thread - the thread pointer\n\t * might get invalid after that point, if the thread exits quickly.\n\t */\n\tif (!IS_ERR(p)) {\n\t\tstruct completion vfork;\n\n\t\ttrace_sched_process_fork(current, p);\n\n\t\tnr = task_pid_vnr(p);\n\n\t\tif (clone_flags & CLONE_PARENT_SETTID)\n\t\t\tput_user(nr, parent_tidptr);\n\n\t\tif (clone_flags & CLONE_VFORK) {\n\t\t\tp->vfork_done = &vfork;\n\t\t\tinit_completion(&vfork);\n\t\t}\n\n\t\taudit_finish_fork(p);\n\t\ttracehook_report_clone(regs, clone_flags, nr, p);\n\n\t\t/*\n\t\t * We set PF_STARTING at creation in case tracing wants to\n\t\t * use this to distinguish a fully live task from one that\n\t\t * hasn't gotten to tracehook_report_clone() yet.  Now we\n\t\t * clear it and set the child going.\n\t\t */\n\t\tp->flags &= ~PF_STARTING;\n\n\t\tif (unlikely(clone_flags & CLONE_STOPPED)) {\n\t\t\t/*\n\t\t\t * We'll start up with an immediate SIGSTOP.\n\t\t\t */\n\t\t\tsigaddset(&p->pending.signal, SIGSTOP);\n\t\t\tset_tsk_thread_flag(p, TIF_SIGPENDING);\n\t\t\t__set_task_state(p, TASK_STOPPED);\n\t\t} else {\n\t\t\twake_up_new_task(p, clone_flags);\n\t\t}\n\n\t\ttracehook_report_clone_complete(trace, regs,\n\t\t\t\t\t\tclone_flags, nr, p);\n\n\t\tif (clone_flags & CLONE_VFORK) {\n\t\t\tfreezer_do_not_count();\n\t\t\twait_for_completion(&vfork);\n\t\t\tfreezer_count();\n\t\t\ttracehook_report_vfork_done(p, nr);\n\t\t}\n\t} else {\n\t\tnr = PTR_ERR(p);\n\t}\n\treturn nr;\n}\n\n#ifndef ARCH_MIN_MMSTRUCT_ALIGN\n#define ARCH_MIN_MMSTRUCT_ALIGN 0\n#endif\n\nstatic void sighand_ctor(void *data)\n{\n\tstruct sighand_struct *sighand = data;\n\n\tspin_lock_init(&sighand->siglock);\n\tinit_waitqueue_head(&sighand->signalfd_wqh);\n}\n\nvoid __init proc_caches_init(void)\n{\n\tsighand_cachep = kmem_cache_create(\"sighand_cache\",\n\t\t\tsizeof(struct sighand_struct), 0,\n\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_DESTROY_BY_RCU|\n\t\t\tSLAB_NOTRACK, sighand_ctor);\n\tsignal_cachep = kmem_cache_create(\"signal_cache\",\n\t\t\tsizeof(struct signal_struct), 0,\n\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);\n\tfiles_cachep = kmem_cache_create(\"files_cache\",\n\t\t\tsizeof(struct files_struct), 0,\n\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);\n\tfs_cachep = kmem_cache_create(\"fs_cache\",\n\t\t\tsizeof(struct fs_struct), 0,\n\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);\n\tmm_cachep = kmem_cache_create(\"mm_struct\",\n\t\t\tsizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,\n\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);\n\tvm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC);\n\tmmap_init();\n}\n\n/*\n * Check constraints on flags passed to the unshare system call and\n * force unsharing of additional process context as appropriate.\n */\nstatic void check_unshare_flags(unsigned long *flags_ptr)\n{\n\t/*\n\t * If unsharing a thread from a thread group, must also\n\t * unshare vm.\n\t */\n\tif (*flags_ptr & CLONE_THREAD)\n\t\t*flags_ptr |= CLONE_VM;\n\n\t/*\n\t * If unsharing vm, must also unshare signal handlers.\n\t */\n\tif (*flags_ptr & CLONE_VM)\n\t\t*flags_ptr |= CLONE_SIGHAND;\n\n\t/*\n\t * If unsharing namespace, must also unshare filesystem information.\n\t */\n\tif (*flags_ptr & CLONE_NEWNS)\n\t\t*flags_ptr |= CLONE_FS;\n}\n\n/*\n * Unsharing of tasks created with CLONE_THREAD is not supported yet\n */\nstatic int unshare_thread(unsigned long unshare_flags)\n{\n\tif (unshare_flags & CLONE_THREAD)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n/*\n * Unshare the filesystem structure if it is being shared\n */\nstatic int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)\n{\n\tstruct fs_struct *fs = current->fs;\n\n\tif (!(unshare_flags & CLONE_FS) || !fs)\n\t\treturn 0;\n\n\t/* don't need lock here; in the worst case we'll do useless copy */\n\tif (fs->users == 1)\n\t\treturn 0;\n\n\t*new_fsp = copy_fs_struct(fs);\n\tif (!*new_fsp)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n/*\n * Unsharing of sighand is not supported yet\n */\nstatic int unshare_sighand(unsigned long unshare_flags, struct sighand_struct **new_sighp)\n{\n\tstruct sighand_struct *sigh = current->sighand;\n\n\tif ((unshare_flags & CLONE_SIGHAND) && atomic_read(&sigh->count) > 1)\n\t\treturn -EINVAL;\n\telse\n\t\treturn 0;\n}\n\n/*\n * Unshare vm if it is being shared\n */\nstatic int unshare_vm(unsigned long unshare_flags, struct mm_struct **new_mmp)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tif ((unshare_flags & CLONE_VM) &&\n\t    (mm && atomic_read(&mm->mm_users) > 1)) {\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Unshare file descriptor table if it is being shared\n */\nstatic int unshare_fd(unsigned long unshare_flags, struct files_struct **new_fdp)\n{\n\tstruct files_struct *fd = current->files;\n\tint error = 0;\n\n\tif ((unshare_flags & CLONE_FILES) &&\n\t    (fd && atomic_read(&fd->count) > 1)) {\n\t\t*new_fdp = dup_fd(fd, &error);\n\t\tif (!*new_fdp)\n\t\t\treturn error;\n\t}\n\n\treturn 0;\n}\n\n/*\n * unshare allows a process to 'unshare' part of the process\n * context which was originally shared using clone.  copy_*\n * functions used by do_fork() cannot be used here directly\n * because they modify an inactive task_struct that is being\n * constructed. Here we are modifying the current, active,\n * task_struct.\n */\nSYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)\n{\n\tint err = 0;\n\tstruct fs_struct *fs, *new_fs = NULL;\n\tstruct sighand_struct *new_sigh = NULL;\n\tstruct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;\n\tstruct files_struct *fd, *new_fd = NULL;\n\tstruct nsproxy *new_nsproxy = NULL;\n\tint do_sysvsem = 0;\n\n\tcheck_unshare_flags(&unshare_flags);\n\n\t/* Return -EINVAL for all unsupported flags */\n\terr = -EINVAL;\n\tif (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|\n\t\t\t\tCLONE_VM|CLONE_FILES|CLONE_SYSVSEM|\n\t\t\t\tCLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET))\n\t\tgoto bad_unshare_out;\n\n\t/*\n\t * CLONE_NEWIPC must also detach from the undolist: after switching\n\t * to a new ipc namespace, the semaphore arrays from the old\n\t * namespace are unreachable.\n\t */\n\tif (unshare_flags & (CLONE_NEWIPC|CLONE_SYSVSEM))\n\t\tdo_sysvsem = 1;\n\tif ((err = unshare_thread(unshare_flags)))\n\t\tgoto bad_unshare_out;\n\tif ((err = unshare_fs(unshare_flags, &new_fs)))\n\t\tgoto bad_unshare_cleanup_thread;\n\tif ((err = unshare_sighand(unshare_flags, &new_sigh)))\n\t\tgoto bad_unshare_cleanup_fs;\n\tif ((err = unshare_vm(unshare_flags, &new_mm)))\n\t\tgoto bad_unshare_cleanup_sigh;\n\tif ((err = unshare_fd(unshare_flags, &new_fd)))\n\t\tgoto bad_unshare_cleanup_vm;\n\tif ((err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,\n\t\t\tnew_fs)))\n\t\tgoto bad_unshare_cleanup_fd;\n\n\tif (new_fs ||  new_mm || new_fd || do_sysvsem || new_nsproxy) {\n\t\tif (do_sysvsem) {\n\t\t\t/*\n\t\t\t * CLONE_SYSVSEM is equivalent to sys_exit().\n\t\t\t */\n\t\t\texit_sem(current);\n\t\t}\n\n\t\tif (new_nsproxy) {\n\t\t\tswitch_task_namespaces(current, new_nsproxy);\n\t\t\tnew_nsproxy = NULL;\n\t\t}\n\n\t\ttask_lock(current);\n\n\t\tif (new_fs) {\n\t\t\tfs = current->fs;\n\t\t\tspin_lock(&fs->lock);\n\t\t\tcurrent->fs = new_fs;\n\t\t\tif (--fs->users)\n\t\t\t\tnew_fs = NULL;\n\t\t\telse\n\t\t\t\tnew_fs = fs;\n\t\t\tspin_unlock(&fs->lock);\n\t\t}\n\n\t\tif (new_mm) {\n\t\t\tmm = current->mm;\n\t\t\tactive_mm = current->active_mm;\n\t\t\tcurrent->mm = new_mm;\n\t\t\tcurrent->active_mm = new_mm;\n\t\t\tif (current->signal->oom_score_adj == OOM_SCORE_ADJ_MIN) {\n\t\t\t\tatomic_dec(&mm->oom_disable_count);\n\t\t\t\tatomic_inc(&new_mm->oom_disable_count);\n\t\t\t}\n\t\t\tactivate_mm(active_mm, new_mm);\n\t\t\tnew_mm = mm;\n\t\t}\n\n\t\tif (new_fd) {\n\t\t\tfd = current->files;\n\t\t\tcurrent->files = new_fd;\n\t\t\tnew_fd = fd;\n\t\t}\n\n\t\ttask_unlock(current);\n\t}\n\n\tif (new_nsproxy)\n\t\tput_nsproxy(new_nsproxy);\n\nbad_unshare_cleanup_fd:\n\tif (new_fd)\n\t\tput_files_struct(new_fd);\n\nbad_unshare_cleanup_vm:\n\tif (new_mm)\n\t\tmmput(new_mm);\n\nbad_unshare_cleanup_sigh:\n\tif (new_sigh)\n\t\tif (atomic_dec_and_test(&new_sigh->count))\n\t\t\tkmem_cache_free(sighand_cachep, new_sigh);\n\nbad_unshare_cleanup_fs:\n\tif (new_fs)\n\t\tfree_fs_struct(new_fs);\n\nbad_unshare_cleanup_thread:\nbad_unshare_out:\n\treturn err;\n}\n\n/*\n *\tHelper to unshare the files of the current task.\n *\tWe don't want to expose copy_files internals to\n *\tthe exec layer of the kernel.\n */\n\nint unshare_files(struct files_struct **displaced)\n{\n\tstruct task_struct *task = current;\n\tstruct files_struct *copy = NULL;\n\tint error;\n\n\terror = unshare_fd(CLONE_FILES, &copy);\n\tif (error || !copy) {\n\t\t*displaced = NULL;\n\t\treturn error;\n\t}\n\t*displaced = task->files;\n\ttask_lock(task);\n\ttask->files = copy;\n\ttask_unlock(task);\n\treturn 0;\n}\n", "/*\n *  kernel/sched.c\n *\n *  Kernel scheduler and related syscalls\n *\n *  Copyright (C) 1991-2002  Linus Torvalds\n *\n *  1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and\n *\t\tmake semaphores SMP safe\n *  1998-11-19\tImplemented schedule_timeout() and related stuff\n *\t\tby Andrea Arcangeli\n *  2002-01-04\tNew ultra-scalable O(1) scheduler by Ingo Molnar:\n *\t\thybrid priority-list and round-robin design with\n *\t\tan array-switch method of distributing timeslices\n *\t\tand per-CPU runqueues.  Cleanups and useful suggestions\n *\t\tby Davide Libenzi, preemptible kernel bits by Robert Love.\n *  2003-09-03\tInteractivity tuning by Con Kolivas.\n *  2004-04-02\tScheduler domains code by Nick Piggin\n *  2007-04-15  Work begun on replacing all interactivity tuning with a\n *              fair scheduling design by Con Kolivas.\n *  2007-05-05  Load balancing (smp-nice) and other improvements\n *              by Peter Williams\n *  2007-05-06  Interactivity improvements to CFS by Mike Galbraith\n *  2007-07-01  Group scheduling enhancements by Srivatsa Vaddagiri\n *  2007-11-29  RT balancing improvements by Steven Rostedt, Gregory Haskins,\n *              Thomas Gleixner, Mike Kravetz\n */\n\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/nmi.h>\n#include <linux/init.h>\n#include <linux/uaccess.h>\n#include <linux/highmem.h>\n#include <linux/smp_lock.h>\n#include <asm/mmu_context.h>\n#include <linux/interrupt.h>\n#include <linux/capability.h>\n#include <linux/completion.h>\n#include <linux/kernel_stat.h>\n#include <linux/debug_locks.h>\n#include <linux/perf_event.h>\n#include <linux/security.h>\n#include <linux/notifier.h>\n#include <linux/profile.h>\n#include <linux/freezer.h>\n#include <linux/vmalloc.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/pid_namespace.h>\n#include <linux/smp.h>\n#include <linux/threads.h>\n#include <linux/timer.h>\n#include <linux/rcupdate.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/percpu.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/stop_machine.h>\n#include <linux/sysctl.h>\n#include <linux/syscalls.h>\n#include <linux/times.h>\n#include <linux/tsacct_kern.h>\n#include <linux/kprobes.h>\n#include <linux/delayacct.h>\n#include <linux/unistd.h>\n#include <linux/pagemap.h>\n#include <linux/hrtimer.h>\n#include <linux/tick.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/ftrace.h>\n#include <linux/slab.h>\n\n#include <asm/tlb.h>\n#include <asm/irq_regs.h>\n\n#include \"sched_cpupri.h\"\n#include \"workqueue_sched.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/sched.h>\n\n/*\n * Convert user-nice values [ -20 ... 0 ... 19 ]\n * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],\n * and back.\n */\n#define NICE_TO_PRIO(nice)\t(MAX_RT_PRIO + (nice) + 20)\n#define PRIO_TO_NICE(prio)\t((prio) - MAX_RT_PRIO - 20)\n#define TASK_NICE(p)\t\tPRIO_TO_NICE((p)->static_prio)\n\n/*\n * 'User priority' is the nice value converted to something we\n * can work with better when scaling various scheduler parameters,\n * it's a [ 0 ... 39 ] range.\n */\n#define USER_PRIO(p)\t\t((p)-MAX_RT_PRIO)\n#define TASK_USER_PRIO(p)\tUSER_PRIO((p)->static_prio)\n#define MAX_USER_PRIO\t\t(USER_PRIO(MAX_PRIO))\n\n/*\n * Helpers for converting nanosecond timing to jiffy resolution\n */\n#define NS_TO_JIFFIES(TIME)\t((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))\n\n#define NICE_0_LOAD\t\tSCHED_LOAD_SCALE\n#define NICE_0_SHIFT\t\tSCHED_LOAD_SHIFT\n\n/*\n * These are the 'tuning knobs' of the scheduler:\n *\n * default timeslice is 100 msecs (used only for SCHED_RR tasks).\n * Timeslices get refilled after they expire.\n */\n#define DEF_TIMESLICE\t\t(100 * HZ / 1000)\n\n/*\n * single value that denotes runtime == period, ie unlimited time.\n */\n#define RUNTIME_INF\t((u64)~0ULL)\n\nstatic inline int rt_policy(int policy)\n{\n\tif (unlikely(policy == SCHED_FIFO || policy == SCHED_RR))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline int task_has_rt_policy(struct task_struct *p)\n{\n\treturn rt_policy(p->policy);\n}\n\n/*\n * This is the priority-queue data structure of the RT scheduling class:\n */\nstruct rt_prio_array {\n\tDECLARE_BITMAP(bitmap, MAX_RT_PRIO+1); /* include 1 bit for delimiter */\n\tstruct list_head queue[MAX_RT_PRIO];\n};\n\nstruct rt_bandwidth {\n\t/* nests inside the rq lock: */\n\traw_spinlock_t\t\trt_runtime_lock;\n\tktime_t\t\t\trt_period;\n\tu64\t\t\trt_runtime;\n\tstruct hrtimer\t\trt_period_timer;\n};\n\nstatic struct rt_bandwidth def_rt_bandwidth;\n\nstatic int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun);\n\nstatic enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)\n{\n\tstruct rt_bandwidth *rt_b =\n\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);\n\tktime_t now;\n\tint overrun;\n\tint idle = 0;\n\n\tfor (;;) {\n\t\tnow = hrtimer_cb_get_time(timer);\n\t\toverrun = hrtimer_forward(timer, now, rt_b->rt_period);\n\n\t\tif (!overrun)\n\t\t\tbreak;\n\n\t\tidle = do_sched_rt_period_timer(rt_b, overrun);\n\t}\n\n\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\n}\n\nstatic\nvoid init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)\n{\n\trt_b->rt_period = ns_to_ktime(period);\n\trt_b->rt_runtime = runtime;\n\n\traw_spin_lock_init(&rt_b->rt_runtime_lock);\n\n\thrtimer_init(&rt_b->rt_period_timer,\n\t\t\tCLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\trt_b->rt_period_timer.function = sched_rt_period_timer;\n}\n\nstatic inline int rt_bandwidth_enabled(void)\n{\n\treturn sysctl_sched_rt_runtime >= 0;\n}\n\nstatic void start_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\tktime_t now;\n\n\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)\n\t\treturn;\n\n\tif (hrtimer_active(&rt_b->rt_period_timer))\n\t\treturn;\n\n\traw_spin_lock(&rt_b->rt_runtime_lock);\n\tfor (;;) {\n\t\tunsigned long delta;\n\t\tktime_t soft, hard;\n\n\t\tif (hrtimer_active(&rt_b->rt_period_timer))\n\t\t\tbreak;\n\n\t\tnow = hrtimer_cb_get_time(&rt_b->rt_period_timer);\n\t\thrtimer_forward(&rt_b->rt_period_timer, now, rt_b->rt_period);\n\n\t\tsoft = hrtimer_get_softexpires(&rt_b->rt_period_timer);\n\t\thard = hrtimer_get_expires(&rt_b->rt_period_timer);\n\t\tdelta = ktime_to_ns(ktime_sub(hard, soft));\n\t\t__hrtimer_start_range_ns(&rt_b->rt_period_timer, soft, delta,\n\t\t\t\tHRTIMER_MODE_ABS_PINNED, 0);\n\t}\n\traw_spin_unlock(&rt_b->rt_runtime_lock);\n}\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\thrtimer_cancel(&rt_b->rt_period_timer);\n}\n#endif\n\n/*\n * sched_domains_mutex serializes calls to arch_init_sched_domains,\n * detach_destroy_domains and partition_sched_domains.\n */\nstatic DEFINE_MUTEX(sched_domains_mutex);\n\n#ifdef CONFIG_CGROUP_SCHED\n\n#include <linux/cgroup.h>\n\nstruct cfs_rq;\n\nstatic LIST_HEAD(task_groups);\n\n/* task group related information */\nstruct task_group {\n\tstruct cgroup_subsys_state css;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/* schedulable entities of this group on each cpu */\n\tstruct sched_entity **se;\n\t/* runqueue \"owned\" by this group on each cpu */\n\tstruct cfs_rq **cfs_rq;\n\tunsigned long shares;\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct sched_rt_entity **rt_se;\n\tstruct rt_rq **rt_rq;\n\n\tstruct rt_bandwidth rt_bandwidth;\n#endif\n\n\tstruct rcu_head rcu;\n\tstruct list_head list;\n\n\tstruct task_group *parent;\n\tstruct list_head siblings;\n\tstruct list_head children;\n};\n\n#define root_task_group init_task_group\n\n/* task_group_lock serializes add/remove of task groups and also changes to\n * a task group's cpu shares.\n */\nstatic DEFINE_SPINLOCK(task_group_lock);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\n#ifdef CONFIG_SMP\nstatic int root_task_group_empty(void)\n{\n\treturn list_empty(&root_task_group.children);\n}\n#endif\n\n# define INIT_TASK_GROUP_LOAD\tNICE_0_LOAD\n\n/*\n * A weight of 0 or 1 can cause arithmetics problems.\n * A weight of a cfs_rq is the sum of weights of which entities\n * are queued on this cfs_rq, so a weight of a entity should not be\n * too large, so as the shares value of a task group.\n * (The default weight is 1024 - so there's no practical\n *  limitation from this.)\n */\n#define MIN_SHARES\t2\n#define MAX_SHARES\t(1UL << 18)\n\nstatic int init_task_group_load = INIT_TASK_GROUP_LOAD;\n#endif\n\n/* Default task group.\n *\tEvery task in system belong to this group at bootup.\n */\nstruct task_group init_task_group;\n\n#endif\t/* CONFIG_CGROUP_SCHED */\n\n/* CFS-related fields in a runqueue */\nstruct cfs_rq {\n\tstruct load_weight load;\n\tunsigned long nr_running;\n\n\tu64 exec_clock;\n\tu64 min_vruntime;\n\n\tstruct rb_root tasks_timeline;\n\tstruct rb_node *rb_leftmost;\n\n\tstruct list_head tasks;\n\tstruct list_head *balance_iterator;\n\n\t/*\n\t * 'curr' points to currently running entity on this cfs_rq.\n\t * It is set to NULL otherwise (i.e when none are currently running).\n\t */\n\tstruct sched_entity *curr, *next, *last;\n\n\tunsigned int nr_spread_over;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tstruct rq *rq;\t/* cpu runqueue to which this cfs_rq is attached */\n\n\t/*\n\t * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in\n\t * a hierarchy). Non-leaf lrqs hold other higher schedulable entities\n\t * (like users, containers etc.)\n\t *\n\t * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This\n\t * list is used during load balance.\n\t */\n\tstruct list_head leaf_cfs_rq_list;\n\tstruct task_group *tg;\t/* group that \"owns\" this runqueue */\n\n#ifdef CONFIG_SMP\n\t/*\n\t * the part of load.weight contributed by tasks\n\t */\n\tunsigned long task_weight;\n\n\t/*\n\t *   h_load = weight * f(tg)\n\t *\n\t * Where f(tg) is the recursive weight fraction assigned to\n\t * this group.\n\t */\n\tunsigned long h_load;\n\n\t/*\n\t * this cpu's part of tg->shares\n\t */\n\tunsigned long shares;\n\n\t/*\n\t * load.weight at the time we set shares\n\t */\n\tunsigned long rq_weight;\n#endif\n#endif\n};\n\n/* Real-Time classes' related field in a runqueue: */\nstruct rt_rq {\n\tstruct rt_prio_array active;\n\tunsigned long rt_nr_running;\n#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED\n\tstruct {\n\t\tint curr; /* highest queued rt task prio */\n#ifdef CONFIG_SMP\n\t\tint next; /* next highest */\n#endif\n\t} highest_prio;\n#endif\n#ifdef CONFIG_SMP\n\tunsigned long rt_nr_migratory;\n\tunsigned long rt_nr_total;\n\tint overloaded;\n\tstruct plist_head pushable_tasks;\n#endif\n\tint rt_throttled;\n\tu64 rt_time;\n\tu64 rt_runtime;\n\t/* Nests inside the rq lock: */\n\traw_spinlock_t rt_runtime_lock;\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tunsigned long rt_nr_boosted;\n\n\tstruct rq *rq;\n\tstruct list_head leaf_rt_rq_list;\n\tstruct task_group *tg;\n#endif\n};\n\n#ifdef CONFIG_SMP\n\n/*\n * We add the notion of a root-domain which will be used to define per-domain\n * variables. Each exclusive cpuset essentially defines an island domain by\n * fully partitioning the member cpus from any other cpuset. Whenever a new\n * exclusive cpuset is created, we also create and attach a new root-domain\n * object.\n *\n */\nstruct root_domain {\n\tatomic_t refcount;\n\tcpumask_var_t span;\n\tcpumask_var_t online;\n\n\t/*\n\t * The \"RT overload\" flag: it gets set if a CPU has more than\n\t * one runnable RT task.\n\t */\n\tcpumask_var_t rto_mask;\n\tatomic_t rto_count;\n\tstruct cpupri cpupri;\n};\n\n/*\n * By default the system creates a single root-domain with all cpus as\n * members (mimicking the global state we have today).\n */\nstatic struct root_domain def_root_domain;\n\n#endif /* CONFIG_SMP */\n\n/*\n * This is the main, per-CPU runqueue data structure.\n *\n * Locking rule: those places that want to lock multiple runqueues\n * (such as the load balancing or the thread migration code), lock\n * acquire operations must be ordered by ascending &runqueue.\n */\nstruct rq {\n\t/* runqueue lock: */\n\traw_spinlock_t lock;\n\n\t/*\n\t * nr_running and cpu_load should be in the same cacheline because\n\t * remote CPUs use both these fields when doing load calculation.\n\t */\n\tunsigned long nr_running;\n\t#define CPU_LOAD_IDX_MAX 5\n\tunsigned long cpu_load[CPU_LOAD_IDX_MAX];\n\tunsigned long last_load_update_tick;\n#ifdef CONFIG_NO_HZ\n\tu64 nohz_stamp;\n\tunsigned char nohz_balance_kick;\n#endif\n\tunsigned int skip_clock_update;\n\n\t/* capture load from *all* tasks on this cpu: */\n\tstruct load_weight load;\n\tunsigned long nr_load_updates;\n\tu64 nr_switches;\n\n\tstruct cfs_rq cfs;\n\tstruct rt_rq rt;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/* list of leaf cfs_rq on this cpu: */\n\tstruct list_head leaf_cfs_rq_list;\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct list_head leaf_rt_rq_list;\n#endif\n\n\t/*\n\t * This is part of a global counter where only the total sum\n\t * over all CPUs matters. A task can increase this counter on\n\t * one CPU and if it got migrated afterwards it may decrease\n\t * it on another CPU. Always updated under the runqueue lock:\n\t */\n\tunsigned long nr_uninterruptible;\n\n\tstruct task_struct *curr, *idle, *stop;\n\tunsigned long next_balance;\n\tstruct mm_struct *prev_mm;\n\n\tu64 clock;\n\tu64 clock_task;\n\n\tatomic_t nr_iowait;\n\n#ifdef CONFIG_SMP\n\tstruct root_domain *rd;\n\tstruct sched_domain *sd;\n\n\tunsigned long cpu_power;\n\n\tunsigned char idle_at_tick;\n\t/* For active balancing */\n\tint post_schedule;\n\tint active_balance;\n\tint push_cpu;\n\tstruct cpu_stop_work active_balance_work;\n\t/* cpu of this runqueue: */\n\tint cpu;\n\tint online;\n\n\tunsigned long avg_load_per_task;\n\n\tu64 rt_avg;\n\tu64 age_stamp;\n\tu64 idle_stamp;\n\tu64 avg_idle;\n#endif\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\tu64 prev_irq_time;\n#endif\n\n\t/* calc_load related fields */\n\tunsigned long calc_load_update;\n\tlong calc_load_active;\n\n#ifdef CONFIG_SCHED_HRTICK\n#ifdef CONFIG_SMP\n\tint hrtick_csd_pending;\n\tstruct call_single_data hrtick_csd;\n#endif\n\tstruct hrtimer hrtick_timer;\n#endif\n\n#ifdef CONFIG_SCHEDSTATS\n\t/* latency stats */\n\tstruct sched_info rq_sched_info;\n\tunsigned long long rq_cpu_time;\n\t/* could above be rq->cfs_rq.exec_clock + rq->rt_rq.rt_runtime ? */\n\n\t/* sys_sched_yield() stats */\n\tunsigned int yld_count;\n\n\t/* schedule() stats */\n\tunsigned int sched_switch;\n\tunsigned int sched_count;\n\tunsigned int sched_goidle;\n\n\t/* try_to_wake_up() stats */\n\tunsigned int ttwu_count;\n\tunsigned int ttwu_local;\n\n\t/* BKL stats */\n\tunsigned int bkl_count;\n#endif\n};\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);\n\n\nstatic void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}\n\n#define rcu_dereference_check_sched_domain(p) \\\n\trcu_dereference_check((p), \\\n\t\t\t      rcu_read_lock_sched_held() || \\\n\t\t\t      lockdep_is_held(&sched_domains_mutex))\n\n/*\n * The domain tree (rq->sd) is protected by RCU's quiescent state transition.\n * See detach_destroy_domains: synchronize_sched for details.\n *\n * The domain tree of any CPU may only be accessed from within\n * preempt-disabled sections.\n */\n#define for_each_domain(cpu, __sd) \\\n\tfor (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); __sd; __sd = __sd->parent)\n\n#define cpu_rq(cpu)\t\t(&per_cpu(runqueues, (cpu)))\n#define this_rq()\t\t(&__get_cpu_var(runqueues))\n#define task_rq(p)\t\tcpu_rq(task_cpu(p))\n#define cpu_curr(cpu)\t\t(cpu_rq(cpu)->curr)\n#define raw_rq()\t\t(&__raw_get_cpu_var(runqueues))\n\n#ifdef CONFIG_CGROUP_SCHED\n\n/*\n * Return the group to which this tasks belongs.\n *\n * We use task_subsys_state_check() and extend the RCU verification\n * with lockdep_is_held(&task_rq(p)->lock) because cpu_cgroup_attach()\n * holds that lock for each task it moves into the cgroup. Therefore\n * by holding that lock, we pin the task to the current cgroup.\n */\nstatic inline struct task_group *task_group(struct task_struct *p)\n{\n\tstruct cgroup_subsys_state *css;\n\n\tcss = task_subsys_state_check(p, cpu_cgroup_subsys_id,\n\t\t\tlockdep_is_held(&task_rq(p)->lock));\n\treturn container_of(css, struct task_group, css);\n}\n\n/* Change a task's cfs_rq and parent entity if it moves across CPUs/groups */\nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu)\n{\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tp->se.cfs_rq = task_group(p)->cfs_rq[cpu];\n\tp->se.parent = task_group(p)->se[cpu];\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tp->rt.rt_rq  = task_group(p)->rt_rq[cpu];\n\tp->rt.parent = task_group(p)->rt_se[cpu];\n#endif\n}\n\n#else /* CONFIG_CGROUP_SCHED */\n\nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu) { }\nstatic inline struct task_group *task_group(struct task_struct *p)\n{\n\treturn NULL;\n}\n\n#endif /* CONFIG_CGROUP_SCHED */\n\nstatic u64 irq_time_cpu(int cpu);\nstatic void sched_irq_time_avg_update(struct rq *rq, u64 irq_time);\n\ninline void update_rq_clock(struct rq *rq)\n{\n\tif (!rq->skip_clock_update) {\n\t\tint cpu = cpu_of(rq);\n\t\tu64 irq_time;\n\n\t\trq->clock = sched_clock_cpu(cpu);\n\t\tirq_time = irq_time_cpu(cpu);\n\t\tif (rq->clock - irq_time > rq->clock_task)\n\t\t\trq->clock_task = rq->clock - irq_time;\n\n\t\tsched_irq_time_avg_update(rq, irq_time);\n\t}\n}\n\n/*\n * Tunables that become constants when CONFIG_SCHED_DEBUG is off:\n */\n#ifdef CONFIG_SCHED_DEBUG\n# define const_debug __read_mostly\n#else\n# define const_debug static const\n#endif\n\n/**\n * runqueue_is_locked\n * @cpu: the processor in question.\n *\n * Returns true if the current cpu runqueue is locked.\n * This interface allows printk to be called with the runqueue lock\n * held and know whether or not it is OK to wake up the klogd.\n */\nint runqueue_is_locked(int cpu)\n{\n\treturn raw_spin_is_locked(&cpu_rq(cpu)->lock);\n}\n\n/*\n * Debugging: various feature bits\n */\n\n#define SCHED_FEAT(name, enabled)\t\\\n\t__SCHED_FEAT_##name ,\n\nenum {\n#include \"sched_features.h\"\n};\n\n#undef SCHED_FEAT\n\n#define SCHED_FEAT(name, enabled)\t\\\n\t(1UL << __SCHED_FEAT_##name) * enabled |\n\nconst_debug unsigned int sysctl_sched_features =\n#include \"sched_features.h\"\n\t0;\n\n#undef SCHED_FEAT\n\n#ifdef CONFIG_SCHED_DEBUG\n#define SCHED_FEAT(name, enabled)\t\\\n\t#name ,\n\nstatic __read_mostly char *sched_feat_names[] = {\n#include \"sched_features.h\"\n\tNULL\n};\n\n#undef SCHED_FEAT\n\nstatic int sched_feat_show(struct seq_file *m, void *v)\n{\n\tint i;\n\n\tfor (i = 0; sched_feat_names[i]; i++) {\n\t\tif (!(sysctl_sched_features & (1UL << i)))\n\t\t\tseq_puts(m, \"NO_\");\n\t\tseq_printf(m, \"%s \", sched_feat_names[i]);\n\t}\n\tseq_puts(m, \"\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t\nsched_feat_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tchar *cmp;\n\tint neg = 0;\n\tint i;\n\n\tif (cnt > 63)\n\t\tcnt = 63;\n\n\tif (copy_from_user(&buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\tcmp = strstrip(buf);\n\n\tif (strncmp(buf, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\tfor (i = 0; sched_feat_names[i]; i++) {\n\t\tif (strcmp(cmp, sched_feat_names[i]) == 0) {\n\t\t\tif (neg)\n\t\t\t\tsysctl_sched_features &= ~(1UL << i);\n\t\t\telse\n\t\t\t\tsysctl_sched_features |= (1UL << i);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!sched_feat_names[i])\n\t\treturn -EINVAL;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int sched_feat_open(struct inode *inode, struct file *filp)\n{\n\treturn single_open(filp, sched_feat_show, NULL);\n}\n\nstatic const struct file_operations sched_feat_fops = {\n\t.open\t\t= sched_feat_open,\n\t.write\t\t= sched_feat_write,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n\nstatic __init int sched_init_debug(void)\n{\n\tdebugfs_create_file(\"sched_features\", 0644, NULL, NULL,\n\t\t\t&sched_feat_fops);\n\n\treturn 0;\n}\nlate_initcall(sched_init_debug);\n\n#endif\n\n#define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))\n\n/*\n * Number of tasks to iterate in a single balance run.\n * Limited because this is done with IRQs disabled.\n */\nconst_debug unsigned int sysctl_sched_nr_migrate = 32;\n\n/*\n * ratelimit for updating the group shares.\n * default: 0.25ms\n */\nunsigned int sysctl_sched_shares_ratelimit = 250000;\nunsigned int normalized_sysctl_sched_shares_ratelimit = 250000;\n\n/*\n * Inject some fuzzyness into changing the per-cpu group shares\n * this avoids remote rq-locks at the expense of fairness.\n * default: 4\n */\nunsigned int sysctl_sched_shares_thresh = 4;\n\n/*\n * period over which we average the RT time consumption, measured\n * in ms.\n *\n * default: 1s\n */\nconst_debug unsigned int sysctl_sched_time_avg = MSEC_PER_SEC;\n\n/*\n * period over which we measure -rt task cpu usage in us.\n * default: 1s\n */\nunsigned int sysctl_sched_rt_period = 1000000;\n\nstatic __read_mostly int scheduler_running;\n\n/*\n * part of the period that we allow rt tasks to run in us.\n * default: 0.95s\n */\nint sysctl_sched_rt_runtime = 950000;\n\nstatic inline u64 global_rt_period(void)\n{\n\treturn (u64)sysctl_sched_rt_period * NSEC_PER_USEC;\n}\n\nstatic inline u64 global_rt_runtime(void)\n{\n\tif (sysctl_sched_rt_runtime < 0)\n\t\treturn RUNTIME_INF;\n\n\treturn (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;\n}\n\n#ifndef prepare_arch_switch\n# define prepare_arch_switch(next)\tdo { } while (0)\n#endif\n#ifndef finish_arch_switch\n# define finish_arch_switch(prev)\tdo { } while (0)\n#endif\n\nstatic inline int task_current(struct rq *rq, struct task_struct *p)\n{\n\treturn rq->curr == p;\n}\n\n#ifndef __ARCH_WANT_UNLOCKED_CTXSW\nstatic inline int task_running(struct rq *rq, struct task_struct *p)\n{\n\treturn task_current(rq, p);\n}\n\nstatic inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)\n{\n}\n\nstatic inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)\n{\n#ifdef CONFIG_DEBUG_SPINLOCK\n\t/* this is a valid case when another task releases the spinlock */\n\trq->lock.owner = current;\n#endif\n\t/*\n\t * If we are tracking spinlock dependencies then we have to\n\t * fix up the runqueue lock - which gets 'carried over' from\n\t * prev into current:\n\t */\n\tspin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);\n\n\traw_spin_unlock_irq(&rq->lock);\n}\n\n#else /* __ARCH_WANT_UNLOCKED_CTXSW */\nstatic inline int task_running(struct rq *rq, struct task_struct *p)\n{\n#ifdef CONFIG_SMP\n\treturn p->oncpu;\n#else\n\treturn task_current(rq, p);\n#endif\n}\n\nstatic inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * We can optimise this out completely for !SMP, because the\n\t * SMP rebalancing from interrupt is the only thing that cares\n\t * here.\n\t */\n\tnext->oncpu = 1;\n#endif\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\traw_spin_unlock_irq(&rq->lock);\n#else\n\traw_spin_unlock(&rq->lock);\n#endif\n}\n\nstatic inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * After ->oncpu is cleared, the task can be moved to a different CPU.\n\t * We must ensure this doesn't happen until the switch is completely\n\t * finished.\n\t */\n\tsmp_wmb();\n\tprev->oncpu = 0;\n#endif\n#ifndef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_enable();\n#endif\n}\n#endif /* __ARCH_WANT_UNLOCKED_CTXSW */\n\n/*\n * Check whether the task is waking, we use this to synchronize ->cpus_allowed\n * against ttwu().\n */\nstatic inline int task_is_waking(struct task_struct *p)\n{\n\treturn unlikely(p->state == TASK_WAKING);\n}\n\n/*\n * __task_rq_lock - lock the runqueue a given task resides on.\n * Must be called interrupts disabled.\n */\nstatic inline struct rq *__task_rq_lock(struct task_struct *p)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\tif (likely(rq == task_rq(p)))\n\t\t\treturn rq;\n\t\traw_spin_unlock(&rq->lock);\n\t}\n}\n\n/*\n * task_rq_lock - lock the runqueue a given task resides on and disable\n * interrupts. Note the ordering: we can safely lookup the task_rq without\n * explicitly disabling preemption.\n */\nstatic struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\tlocal_irq_save(*flags);\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\tif (likely(rq == task_rq(p)))\n\t\t\treturn rq;\n\t\traw_spin_unlock_irqrestore(&rq->lock, *flags);\n\t}\n}\n\nstatic void __task_rq_unlock(struct rq *rq)\n\t__releases(rq->lock)\n{\n\traw_spin_unlock(&rq->lock);\n}\n\nstatic inline void task_rq_unlock(struct rq *rq, unsigned long *flags)\n\t__releases(rq->lock)\n{\n\traw_spin_unlock_irqrestore(&rq->lock, *flags);\n}\n\n/*\n * this_rq_lock - lock this runqueue and disable interrupts.\n */\nstatic struct rq *this_rq_lock(void)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tlocal_irq_disable();\n\trq = this_rq();\n\traw_spin_lock(&rq->lock);\n\n\treturn rq;\n}\n\n#ifdef CONFIG_SCHED_HRTICK\n/*\n * Use HR-timers to deliver accurate preemption points.\n *\n * Its all a bit involved since we cannot program an hrt while holding the\n * rq->lock. So what we do is store a state in in rq->hrtick_* and ask for a\n * reschedule event.\n *\n * When we get rescheduled we reprogram the hrtick_timer outside of the\n * rq->lock.\n */\n\n/*\n * Use hrtick when:\n *  - enabled by features\n *  - hrtimer is actually high res\n */\nstatic inline int hrtick_enabled(struct rq *rq)\n{\n\tif (!sched_feat(HRTICK))\n\t\treturn 0;\n\tif (!cpu_active(cpu_of(rq)))\n\t\treturn 0;\n\treturn hrtimer_is_hres_active(&rq->hrtick_timer);\n}\n\nstatic void hrtick_clear(struct rq *rq)\n{\n\tif (hrtimer_active(&rq->hrtick_timer))\n\t\thrtimer_cancel(&rq->hrtick_timer);\n}\n\n/*\n * High-resolution timer tick.\n * Runs from hardirq context with interrupts disabled.\n */\nstatic enum hrtimer_restart hrtick(struct hrtimer *timer)\n{\n\tstruct rq *rq = container_of(timer, struct rq, hrtick_timer);\n\n\tWARN_ON_ONCE(cpu_of(rq) != smp_processor_id());\n\n\traw_spin_lock(&rq->lock);\n\tupdate_rq_clock(rq);\n\trq->curr->sched_class->task_tick(rq, rq->curr, 1);\n\traw_spin_unlock(&rq->lock);\n\n\treturn HRTIMER_NORESTART;\n}\n\n#ifdef CONFIG_SMP\n/*\n * called from hardirq (IPI) context\n */\nstatic void __hrtick_start(void *arg)\n{\n\tstruct rq *rq = arg;\n\n\traw_spin_lock(&rq->lock);\n\thrtimer_restart(&rq->hrtick_timer);\n\trq->hrtick_csd_pending = 0;\n\traw_spin_unlock(&rq->lock);\n}\n\n/*\n * Called to set the hrtick timer state.\n *\n * called with rq->lock held and irqs disabled\n */\nstatic void hrtick_start(struct rq *rq, u64 delay)\n{\n\tstruct hrtimer *timer = &rq->hrtick_timer;\n\tktime_t time = ktime_add_ns(timer->base->get_time(), delay);\n\n\thrtimer_set_expires(timer, time);\n\n\tif (rq == this_rq()) {\n\t\thrtimer_restart(timer);\n\t} else if (!rq->hrtick_csd_pending) {\n\t\t__smp_call_function_single(cpu_of(rq), &rq->hrtick_csd, 0);\n\t\trq->hrtick_csd_pending = 1;\n\t}\n}\n\nstatic int\nhotplug_hrtick(struct notifier_block *nfb, unsigned long action, void *hcpu)\n{\n\tint cpu = (int)(long)hcpu;\n\n\tswitch (action) {\n\tcase CPU_UP_CANCELED:\n\tcase CPU_UP_CANCELED_FROZEN:\n\tcase CPU_DOWN_PREPARE:\n\tcase CPU_DOWN_PREPARE_FROZEN:\n\tcase CPU_DEAD:\n\tcase CPU_DEAD_FROZEN:\n\t\thrtick_clear(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstatic __init void init_hrtick(void)\n{\n\thotcpu_notifier(hotplug_hrtick, 0);\n}\n#else\n/*\n * Called to set the hrtick timer state.\n *\n * called with rq->lock held and irqs disabled\n */\nstatic void hrtick_start(struct rq *rq, u64 delay)\n{\n\t__hrtimer_start_range_ns(&rq->hrtick_timer, ns_to_ktime(delay), 0,\n\t\t\tHRTIMER_MODE_REL_PINNED, 0);\n}\n\nstatic inline void init_hrtick(void)\n{\n}\n#endif /* CONFIG_SMP */\n\nstatic void init_rq_hrtick(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\trq->hrtick_csd_pending = 0;\n\n\trq->hrtick_csd.flags = 0;\n\trq->hrtick_csd.func = __hrtick_start;\n\trq->hrtick_csd.info = rq;\n#endif\n\n\thrtimer_init(&rq->hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\trq->hrtick_timer.function = hrtick;\n}\n#else\t/* CONFIG_SCHED_HRTICK */\nstatic inline void hrtick_clear(struct rq *rq)\n{\n}\n\nstatic inline void init_rq_hrtick(struct rq *rq)\n{\n}\n\nstatic inline void init_hrtick(void)\n{\n}\n#endif\t/* CONFIG_SCHED_HRTICK */\n\n/*\n * resched_task - mark a task 'to be rescheduled now'.\n *\n * On UP this means the setting of the need_resched flag, on SMP it\n * might also involve a cross-CPU call to trigger the scheduler on\n * the target CPU.\n */\n#ifdef CONFIG_SMP\n\n#ifndef tsk_is_polling\n#define tsk_is_polling(t) test_tsk_thread_flag(t, TIF_POLLING_NRFLAG)\n#endif\n\nstatic void resched_task(struct task_struct *p)\n{\n\tint cpu;\n\n\tassert_raw_spin_locked(&task_rq(p)->lock);\n\n\tif (test_tsk_need_resched(p))\n\t\treturn;\n\n\tset_tsk_need_resched(p);\n\n\tcpu = task_cpu(p);\n\tif (cpu == smp_processor_id())\n\t\treturn;\n\n\t/* NEED_RESCHED must be visible before we test polling */\n\tsmp_mb();\n\tif (!tsk_is_polling(p))\n\t\tsmp_send_reschedule(cpu);\n}\n\nstatic void resched_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\tif (!raw_spin_trylock_irqsave(&rq->lock, flags))\n\t\treturn;\n\tresched_task(cpu_curr(cpu));\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n}\n\n#ifdef CONFIG_NO_HZ\n/*\n * In the semi idle case, use the nearest busy cpu for migrating timers\n * from an idle cpu.  This is good for power-savings.\n *\n * We don't do similar optimization for completely idle system, as\n * selecting an idle cpu will add more delays to the timers than intended\n * (as that cpu's timer base may not be uptodate wrt jiffies etc).\n */\nint get_nohz_timer_target(void)\n{\n\tint cpu = smp_processor_id();\n\tint i;\n\tstruct sched_domain *sd;\n\n\tfor_each_domain(cpu, sd) {\n\t\tfor_each_cpu(i, sched_domain_span(sd))\n\t\t\tif (!idle_cpu(i))\n\t\t\t\treturn i;\n\t}\n\treturn cpu;\n}\n/*\n * When add_timer_on() enqueues a timer into the timer wheel of an\n * idle CPU then this timer might expire before the next timer event\n * which is scheduled to wake up that CPU. In case of a completely\n * idle system the next event might even be infinite time into the\n * future. wake_up_idle_cpu() ensures that the CPU is woken up and\n * leaves the inner idle loop so the newly added timer is taken into\n * account when the CPU goes back to idle and evaluates the timer\n * wheel for the next timer event.\n */\nvoid wake_up_idle_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tif (cpu == smp_processor_id())\n\t\treturn;\n\n\t/*\n\t * This is safe, as this function is called with the timer\n\t * wheel base lock of (cpu) held. When the CPU is on the way\n\t * to idle and has not yet set rq->curr to idle then it will\n\t * be serialized on the timer wheel base lock and take the new\n\t * timer into account automatically.\n\t */\n\tif (rq->curr != rq->idle)\n\t\treturn;\n\n\t/*\n\t * We can set TIF_RESCHED on the idle task of the other CPU\n\t * lockless. The worst case is that the other CPU runs the\n\t * idle task through an additional NOOP schedule()\n\t */\n\tset_tsk_need_resched(rq->idle);\n\n\t/* NEED_RESCHED must be visible before we test polling */\n\tsmp_mb();\n\tif (!tsk_is_polling(rq->idle))\n\t\tsmp_send_reschedule(cpu);\n}\n\n#endif /* CONFIG_NO_HZ */\n\nstatic u64 sched_avg_period(void)\n{\n\treturn (u64)sysctl_sched_time_avg * NSEC_PER_MSEC / 2;\n}\n\nstatic void sched_avg_update(struct rq *rq)\n{\n\ts64 period = sched_avg_period();\n\n\twhile ((s64)(rq->clock - rq->age_stamp) > period) {\n\t\t/*\n\t\t * Inline assembly required to prevent the compiler\n\t\t * optimising this loop into a divmod call.\n\t\t * See __iter_div_u64_rem() for another example of this.\n\t\t */\n\t\tasm(\"\" : \"+rm\" (rq->age_stamp));\n\t\trq->age_stamp += period;\n\t\trq->rt_avg /= 2;\n\t}\n}\n\nstatic void sched_rt_avg_update(struct rq *rq, u64 rt_delta)\n{\n\trq->rt_avg += rt_delta;\n\tsched_avg_update(rq);\n}\n\n#else /* !CONFIG_SMP */\nstatic void resched_task(struct task_struct *p)\n{\n\tassert_raw_spin_locked(&task_rq(p)->lock);\n\tset_tsk_need_resched(p);\n}\n\nstatic void sched_rt_avg_update(struct rq *rq, u64 rt_delta)\n{\n}\n\nstatic void sched_avg_update(struct rq *rq)\n{\n}\n#endif /* CONFIG_SMP */\n\n#if BITS_PER_LONG == 32\n# define WMULT_CONST\t(~0UL)\n#else\n# define WMULT_CONST\t(1UL << 32)\n#endif\n\n#define WMULT_SHIFT\t32\n\n/*\n * Shift right and round:\n */\n#define SRR(x, y) (((x) + (1UL << ((y) - 1))) >> (y))\n\n/*\n * delta *= weight / lw\n */\nstatic unsigned long\ncalc_delta_mine(unsigned long delta_exec, unsigned long weight,\n\t\tstruct load_weight *lw)\n{\n\tu64 tmp;\n\n\tif (!lw->inv_weight) {\n\t\tif (BITS_PER_LONG > 32 && unlikely(lw->weight >= WMULT_CONST))\n\t\t\tlw->inv_weight = 1;\n\t\telse\n\t\t\tlw->inv_weight = 1 + (WMULT_CONST-lw->weight/2)\n\t\t\t\t/ (lw->weight+1);\n\t}\n\n\ttmp = (u64)delta_exec * weight;\n\t/*\n\t * Check whether we'd overflow the 64-bit multiplication:\n\t */\n\tif (unlikely(tmp > WMULT_CONST))\n\t\ttmp = SRR(SRR(tmp, WMULT_SHIFT/2) * lw->inv_weight,\n\t\t\tWMULT_SHIFT/2);\n\telse\n\t\ttmp = SRR(tmp * lw->inv_weight, WMULT_SHIFT);\n\n\treturn (unsigned long)min(tmp, (u64)(unsigned long)LONG_MAX);\n}\n\nstatic inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}\n\n/*\n * To aid in avoiding the subversion of \"niceness\" due to uneven distribution\n * of tasks with abnormal \"nice\" values across CPUs the contribution that\n * each task makes to its run queue's load is weighted according to its\n * scheduling class and \"nice\" value. For SCHED_NORMAL tasks this is just a\n * scaled version of the new time slice allocation that they receive on time\n * slice expiry etc.\n */\n\n#define WEIGHT_IDLEPRIO                3\n#define WMULT_IDLEPRIO         1431655765\n\n/*\n * Nice levels are multiplicative, with a gentle 10% change for every\n * nice level changed. I.e. when a CPU-bound task goes from nice 0 to\n * nice 1, it will get ~10% less CPU time than another CPU-bound task\n * that remained on nice 0.\n *\n * The \"10% effect\" is relative and cumulative: from _any_ nice level,\n * if you go up 1 level, it's -10% CPU usage, if you go down 1 level\n * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.\n * If a task goes up by ~10% and another task goes down by ~10% then\n * the relative distance between them is ~25%.)\n */\nstatic const int prio_to_weight[40] = {\n /* -20 */     88761,     71755,     56483,     46273,     36291,\n /* -15 */     29154,     23254,     18705,     14949,     11916,\n /* -10 */      9548,      7620,      6100,      4904,      3906,\n /*  -5 */      3121,      2501,      1991,      1586,      1277,\n /*   0 */      1024,       820,       655,       526,       423,\n /*   5 */       335,       272,       215,       172,       137,\n /*  10 */       110,        87,        70,        56,        45,\n /*  15 */        36,        29,        23,        18,        15,\n};\n\n/*\n * Inverse (2^32/x) values of the prio_to_weight[] array, precalculated.\n *\n * In cases where the weight does not change often, we can use the\n * precalculated inverse to speed up arithmetics by turning divisions\n * into multiplications:\n */\nstatic const u32 prio_to_wmult[40] = {\n /* -20 */     48388,     59856,     76040,     92818,    118348,\n /* -15 */    147320,    184698,    229616,    287308,    360437,\n /* -10 */    449829,    563644,    704093,    875809,   1099582,\n /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,\n /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,\n /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,\n /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,\n /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,\n};\n\n/* Time spent by the tasks of the cpu accounting group executing in ... */\nenum cpuacct_stat_index {\n\tCPUACCT_STAT_USER,\t/* ... user mode */\n\tCPUACCT_STAT_SYSTEM,\t/* ... kernel mode */\n\n\tCPUACCT_STAT_NSTATS,\n};\n\n#ifdef CONFIG_CGROUP_CPUACCT\nstatic void cpuacct_charge(struct task_struct *tsk, u64 cputime);\nstatic void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val);\n#else\nstatic inline void cpuacct_charge(struct task_struct *tsk, u64 cputime) {}\nstatic inline void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val) {}\n#endif\n\nstatic inline void inc_cpu_load(struct rq *rq, unsigned long load)\n{\n\tupdate_load_add(&rq->load, load);\n}\n\nstatic inline void dec_cpu_load(struct rq *rq, unsigned long load)\n{\n\tupdate_load_sub(&rq->load, load);\n}\n\n#if (defined(CONFIG_SMP) && defined(CONFIG_FAIR_GROUP_SCHED)) || defined(CONFIG_RT_GROUP_SCHED)\ntypedef int (*tg_visitor)(struct task_group *, void *);\n\n/*\n * Iterate the full tree, calling @down when first entering a node and @up when\n * leaving it for the final time.\n */\nstatic int walk_tg_tree(tg_visitor down, tg_visitor up, void *data)\n{\n\tstruct task_group *parent, *child;\n\tint ret;\n\n\trcu_read_lock();\n\tparent = &root_task_group;\ndown:\n\tret = (*down)(parent, data);\n\tif (ret)\n\t\tgoto out_unlock;\n\tlist_for_each_entry_rcu(child, &parent->children, siblings) {\n\t\tparent = child;\n\t\tgoto down;\n\nup:\n\t\tcontinue;\n\t}\n\tret = (*up)(parent, data);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tchild = parent;\n\tparent = parent->parent;\n\tif (parent)\n\t\tgoto up;\nout_unlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int tg_nop(struct task_group *tg, void *data)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_SMP\n/* Used instead of source_load when we know the type == 0 */\nstatic unsigned long weighted_cpuload(const int cpu)\n{\n\treturn cpu_rq(cpu)->load.weight;\n}\n\n/*\n * Return a low guess at the load of a migration-source cpu weighted\n * according to the scheduling class and \"nice\" value.\n *\n * We want to under-estimate the load of migration sources, to\n * balance conservatively.\n */\nstatic unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(cpu);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}\n\n/*\n * Return a high guess at the load of a migration-target cpu weighted\n * according to the scheduling class and \"nice\" value.\n */\nstatic unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(cpu);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}\n\nstatic unsigned long power_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_power;\n}\n\nstatic int task_hot(struct task_struct *p, u64 now, struct sched_domain *sd);\n\nstatic unsigned long cpu_avg_load_per_task(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long nr_running = ACCESS_ONCE(rq->nr_running);\n\n\tif (nr_running)\n\t\trq->avg_load_per_task = rq->load.weight / nr_running;\n\telse\n\t\trq->avg_load_per_task = 0;\n\n\treturn rq->avg_load_per_task;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\nstatic __read_mostly unsigned long __percpu *update_shares_data;\n\nstatic void __set_se_shares(struct sched_entity *se, unsigned long shares);\n\n/*\n * Calculate and set the cpu's group shares.\n */\nstatic void update_group_shares_cpu(struct task_group *tg, int cpu,\n\t\t\t\t    unsigned long sd_shares,\n\t\t\t\t    unsigned long sd_rq_weight,\n\t\t\t\t    unsigned long *usd_rq_weight)\n{\n\tunsigned long shares, rq_weight;\n\tint boost = 0;\n\n\trq_weight = usd_rq_weight[cpu];\n\tif (!rq_weight) {\n\t\tboost = 1;\n\t\trq_weight = NICE_0_LOAD;\n\t}\n\n\t/*\n\t *             \\Sum_j shares_j * rq_weight_i\n\t * shares_i =  -----------------------------\n\t *                  \\Sum_j rq_weight_j\n\t */\n\tshares = (sd_shares * rq_weight) / sd_rq_weight;\n\tshares = clamp_t(unsigned long, shares, MIN_SHARES, MAX_SHARES);\n\n\tif (abs(shares - tg->se[cpu]->load.weight) >\n\t\t\tsysctl_sched_shares_thresh) {\n\t\tstruct rq *rq = cpu_rq(cpu);\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\ttg->cfs_rq[cpu]->rq_weight = boost ? 0 : rq_weight;\n\t\ttg->cfs_rq[cpu]->shares = boost ? 0 : shares;\n\t\t__set_se_shares(tg->se[cpu], shares);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t}\n}\n\n/*\n * Re-compute the task group their per cpu shares over the given domain.\n * This needs to be done in a bottom-up fashion because the rq weight of a\n * parent group depends on the shares of its child groups.\n */\nstatic int tg_shares_up(struct task_group *tg, void *data)\n{\n\tunsigned long weight, rq_weight = 0, sum_weight = 0, shares = 0;\n\tunsigned long *usd_rq_weight;\n\tstruct sched_domain *sd = data;\n\tunsigned long flags;\n\tint i;\n\n\tif (!tg->se[0])\n\t\treturn 0;\n\n\tlocal_irq_save(flags);\n\tusd_rq_weight = per_cpu_ptr(update_shares_data, smp_processor_id());\n\n\tfor_each_cpu(i, sched_domain_span(sd)) {\n\t\tweight = tg->cfs_rq[i]->load.weight;\n\t\tusd_rq_weight[i] = weight;\n\n\t\trq_weight += weight;\n\t\t/*\n\t\t * If there are currently no tasks on the cpu pretend there\n\t\t * is one of average load so that when a new task gets to\n\t\t * run here it will not get delayed by group starvation.\n\t\t */\n\t\tif (!weight)\n\t\t\tweight = NICE_0_LOAD;\n\n\t\tsum_weight += weight;\n\t\tshares += tg->cfs_rq[i]->shares;\n\t}\n\n\tif (!rq_weight)\n\t\trq_weight = sum_weight;\n\n\tif ((!shares && rq_weight) || shares > tg->shares)\n\t\tshares = tg->shares;\n\n\tif (!sd->parent || !(sd->parent->flags & SD_LOAD_BALANCE))\n\t\tshares = tg->shares;\n\n\tfor_each_cpu(i, sched_domain_span(sd))\n\t\tupdate_group_shares_cpu(tg, i, shares, rq_weight, usd_rq_weight);\n\n\tlocal_irq_restore(flags);\n\n\treturn 0;\n}\n\n/*\n * Compute the cpu's hierarchical load factor for each task group.\n * This needs to be done in a top-down fashion because the load of a child\n * group is a fraction of its parents load.\n */\nstatic int tg_load_down(struct task_group *tg, void *data)\n{\n\tunsigned long load;\n\tlong cpu = (long)data;\n\n\tif (!tg->parent) {\n\t\tload = cpu_rq(cpu)->load.weight;\n\t} else {\n\t\tload = tg->parent->cfs_rq[cpu]->h_load;\n\t\tload *= tg->cfs_rq[cpu]->shares;\n\t\tload /= tg->parent->cfs_rq[cpu]->load.weight + 1;\n\t}\n\n\ttg->cfs_rq[cpu]->h_load = load;\n\n\treturn 0;\n}\n\nstatic void update_shares(struct sched_domain *sd)\n{\n\ts64 elapsed;\n\tu64 now;\n\n\tif (root_task_group_empty())\n\t\treturn;\n\n\tnow = local_clock();\n\telapsed = now - sd->last_update;\n\n\tif (elapsed >= (s64)(u64)sysctl_sched_shares_ratelimit) {\n\t\tsd->last_update = now;\n\t\twalk_tg_tree(tg_nop, tg_shares_up, sd);\n\t}\n}\n\nstatic void update_h_load(long cpu)\n{\n\twalk_tg_tree(tg_load_down, tg_nop, (void *)cpu);\n}\n\n#else\n\nstatic inline void update_shares(struct sched_domain *sd)\n{\n}\n\n#endif\n\n#ifdef CONFIG_PREEMPT\n\nstatic void double_rq_lock(struct rq *rq1, struct rq *rq2);\n\n/*\n * fair double_lock_balance: Safely acquires both rq->locks in a fair\n * way at the expense of forcing extra atomic operations in all\n * invocations.  This assures that the double_lock is acquired using the\n * same underlying policy as the spinlock_t on this architecture, which\n * reduces latency compared to the unfair variant below.  However, it\n * also adds more overhead and therefore may reduce throughput.\n */\nstatic inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(this_rq->lock)\n\t__acquires(busiest->lock)\n\t__acquires(this_rq->lock)\n{\n\traw_spin_unlock(&this_rq->lock);\n\tdouble_rq_lock(this_rq, busiest);\n\n\treturn 1;\n}\n\n#else\n/*\n * Unfair double_lock_balance: Optimizes throughput at the expense of\n * latency by eliminating extra atomic operations when the locks are\n * already in proper order on entry.  This favors lower cpu-ids and will\n * grant the double lock to lower cpus over higher ids under contention,\n * regardless of entry order into the function.\n */\nstatic int _double_lock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(this_rq->lock)\n\t__acquires(busiest->lock)\n\t__acquires(this_rq->lock)\n{\n\tint ret = 0;\n\n\tif (unlikely(!raw_spin_trylock(&busiest->lock))) {\n\t\tif (busiest < this_rq) {\n\t\t\traw_spin_unlock(&this_rq->lock);\n\t\t\traw_spin_lock(&busiest->lock);\n\t\t\traw_spin_lock_nested(&this_rq->lock,\n\t\t\t\t\t      SINGLE_DEPTH_NESTING);\n\t\t\tret = 1;\n\t\t} else\n\t\t\traw_spin_lock_nested(&busiest->lock,\n\t\t\t\t\t      SINGLE_DEPTH_NESTING);\n\t}\n\treturn ret;\n}\n\n#endif /* CONFIG_PREEMPT */\n\n/*\n * double_lock_balance - lock the busiest runqueue, this_rq is locked already.\n */\nstatic int double_lock_balance(struct rq *this_rq, struct rq *busiest)\n{\n\tif (unlikely(!irqs_disabled())) {\n\t\t/* printk() doesn't work good under rq->lock */\n\t\traw_spin_unlock(&this_rq->lock);\n\t\tBUG_ON(1);\n\t}\n\n\treturn _double_lock_balance(this_rq, busiest);\n}\n\nstatic inline void double_unlock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(busiest->lock)\n{\n\traw_spin_unlock(&busiest->lock);\n\tlock_set_subclass(&this_rq->lock.dep_map, 0, _RET_IP_);\n}\n\n/*\n * double_rq_lock - safely lock two runqueues\n *\n * Note this does not disable interrupts like task_rq_lock,\n * you need to do so manually before calling.\n */\nstatic void double_rq_lock(struct rq *rq1, struct rq *rq2)\n\t__acquires(rq1->lock)\n\t__acquires(rq2->lock)\n{\n\tBUG_ON(!irqs_disabled());\n\tif (rq1 == rq2) {\n\t\traw_spin_lock(&rq1->lock);\n\t\t__acquire(rq2->lock);\t/* Fake it out ;) */\n\t} else {\n\t\tif (rq1 < rq2) {\n\t\t\traw_spin_lock(&rq1->lock);\n\t\t\traw_spin_lock_nested(&rq2->lock, SINGLE_DEPTH_NESTING);\n\t\t} else {\n\t\t\traw_spin_lock(&rq2->lock);\n\t\t\traw_spin_lock_nested(&rq1->lock, SINGLE_DEPTH_NESTING);\n\t\t}\n\t}\n}\n\n/*\n * double_rq_unlock - safely unlock two runqueues\n *\n * Note this does not restore interrupts like task_rq_unlock,\n * you need to do so manually after calling.\n */\nstatic void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\traw_spin_unlock(&rq1->lock);\n\tif (rq1 != rq2)\n\t\traw_spin_unlock(&rq2->lock);\n\telse\n\t\t__release(rq2->lock);\n}\n\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void cfs_rq_set_shares(struct cfs_rq *cfs_rq, unsigned long shares)\n{\n#ifdef CONFIG_SMP\n\tcfs_rq->shares = shares;\n#endif\n}\n#endif\n\nstatic void calc_load_account_idle(struct rq *this_rq);\nstatic void update_sysctl(void);\nstatic int get_update_sysctl_factor(void);\nstatic void update_cpu_load(struct rq *this_rq);\n\nstatic inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n\tset_task_rq(p, cpu);\n#ifdef CONFIG_SMP\n\t/*\n\t * After ->cpu is set up to a new value, task_rq_lock(p, ...) can be\n\t * successfuly executed on another CPU. We must ensure that updates of\n\t * per-task data have been completed by this moment.\n\t */\n\tsmp_wmb();\n\ttask_thread_info(p)->cpu = cpu;\n#endif\n}\n\nstatic const struct sched_class rt_sched_class;\n\n#define sched_class_highest (&stop_sched_class)\n#define for_each_class(class) \\\n   for (class = sched_class_highest; class; class = class->next)\n\n#include \"sched_stats.h\"\n\nstatic void inc_nr_running(struct rq *rq)\n{\n\trq->nr_running++;\n}\n\nstatic void dec_nr_running(struct rq *rq)\n{\n\trq->nr_running--;\n}\n\nstatic void set_load_weight(struct task_struct *p)\n{\n\t/*\n\t * SCHED_IDLE tasks get minimal weight:\n\t */\n\tif (p->policy == SCHED_IDLE) {\n\t\tp->se.load.weight = WEIGHT_IDLEPRIO;\n\t\tp->se.load.inv_weight = WMULT_IDLEPRIO;\n\t\treturn;\n\t}\n\n\tp->se.load.weight = prio_to_weight[p->static_prio - MAX_RT_PRIO];\n\tp->se.load.inv_weight = prio_to_wmult[p->static_prio - MAX_RT_PRIO];\n}\n\nstatic void enqueue_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tupdate_rq_clock(rq);\n\tsched_info_queued(p);\n\tp->sched_class->enqueue_task(rq, p, flags);\n\tp->se.on_rq = 1;\n}\n\nstatic void dequeue_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tupdate_rq_clock(rq);\n\tsched_info_dequeued(p);\n\tp->sched_class->dequeue_task(rq, p, flags);\n\tp->se.on_rq = 0;\n}\n\n/*\n * activate_task - move a task to the runqueue.\n */\nstatic void activate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_contributes_to_load(p))\n\t\trq->nr_uninterruptible--;\n\n\tenqueue_task(rq, p, flags);\n\tinc_nr_running(rq);\n}\n\n/*\n * deactivate_task - remove a task from the runqueue.\n */\nstatic void deactivate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_contributes_to_load(p))\n\t\trq->nr_uninterruptible++;\n\n\tdequeue_task(rq, p, flags);\n\tdec_nr_running(rq);\n}\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\n/*\n * There are no locks covering percpu hardirq/softirq time.\n * They are only modified in account_system_vtime, on corresponding CPU\n * with interrupts disabled. So, writes are safe.\n * They are read and saved off onto struct rq in update_rq_clock().\n * This may result in other CPU reading this CPU's irq time and can\n * race with irq/account_system_vtime on this CPU. We would either get old\n * or new value (or semi updated value on 32 bit) with a side effect of\n * accounting a slice of irq time to wrong task when irq is in progress\n * while we read rq->clock. That is a worthy compromise in place of having\n * locks on each irq in account_system_time.\n */\nstatic DEFINE_PER_CPU(u64, cpu_hardirq_time);\nstatic DEFINE_PER_CPU(u64, cpu_softirq_time);\n\nstatic DEFINE_PER_CPU(u64, irq_start_time);\nstatic int sched_clock_irqtime;\n\nvoid enable_sched_clock_irqtime(void)\n{\n\tsched_clock_irqtime = 1;\n}\n\nvoid disable_sched_clock_irqtime(void)\n{\n\tsched_clock_irqtime = 0;\n}\n\nstatic u64 irq_time_cpu(int cpu)\n{\n\tif (!sched_clock_irqtime)\n\t\treturn 0;\n\n\treturn per_cpu(cpu_softirq_time, cpu) + per_cpu(cpu_hardirq_time, cpu);\n}\n\nvoid account_system_vtime(struct task_struct *curr)\n{\n\tunsigned long flags;\n\tint cpu;\n\tu64 now, delta;\n\n\tif (!sched_clock_irqtime)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu = smp_processor_id();\n\tnow = sched_clock_cpu(cpu);\n\tdelta = now - per_cpu(irq_start_time, cpu);\n\tper_cpu(irq_start_time, cpu) = now;\n\t/*\n\t * We do not account for softirq time from ksoftirqd here.\n\t * We want to continue accounting softirq time to ksoftirqd thread\n\t * in that case, so as not to confuse scheduler with a special task\n\t * that do not consume any time, but still wants to run.\n\t */\n\tif (hardirq_count())\n\t\tper_cpu(cpu_hardirq_time, cpu) += delta;\n\telse if (in_serving_softirq() && !(curr->flags & PF_KSOFTIRQD))\n\t\tper_cpu(cpu_softirq_time, cpu) += delta;\n\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(account_system_vtime);\n\nstatic void sched_irq_time_avg_update(struct rq *rq, u64 curr_irq_time)\n{\n\tif (sched_clock_irqtime && sched_feat(NONIRQ_POWER)) {\n\t\tu64 delta_irq = curr_irq_time - rq->prev_irq_time;\n\t\trq->prev_irq_time = curr_irq_time;\n\t\tsched_rt_avg_update(rq, delta_irq);\n\t}\n}\n\n#else\n\nstatic u64 irq_time_cpu(int cpu)\n{\n\treturn 0;\n}\n\nstatic void sched_irq_time_avg_update(struct rq *rq, u64 curr_irq_time) { }\n\n#endif\n\n#include \"sched_idletask.c\"\n#include \"sched_fair.c\"\n#include \"sched_rt.c\"\n#include \"sched_stoptask.c\"\n#ifdef CONFIG_SCHED_DEBUG\n# include \"sched_debug.c\"\n#endif\n\nvoid sched_set_stop_task(int cpu, struct task_struct *stop)\n{\n\tstruct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };\n\tstruct task_struct *old_stop = cpu_rq(cpu)->stop;\n\n\tif (stop) {\n\t\t/*\n\t\t * Make it appear like a SCHED_FIFO task, its something\n\t\t * userspace knows about and won't get confused about.\n\t\t *\n\t\t * Also, it will make PI more or less work without too\n\t\t * much confusion -- but then, stop work should not\n\t\t * rely on PI working anyway.\n\t\t */\n\t\tsched_setscheduler_nocheck(stop, SCHED_FIFO, &param);\n\n\t\tstop->sched_class = &stop_sched_class;\n\t}\n\n\tcpu_rq(cpu)->stop = stop;\n\n\tif (old_stop) {\n\t\t/*\n\t\t * Reset it back to a normal scheduling class so that\n\t\t * it can die in pieces.\n\t\t */\n\t\told_stop->sched_class = &rt_sched_class;\n\t}\n}\n\n/*\n * __normal_prio - return the priority that is based on the static prio\n */\nstatic inline int __normal_prio(struct task_struct *p)\n{\n\treturn p->static_prio;\n}\n\n/*\n * Calculate the expected normal priority: i.e. priority\n * without taking RT-inheritance into account. Might be\n * boosted by interactivity modifiers. Changes upon fork,\n * setprio syscalls, and whenever the interactivity\n * estimator recalculates.\n */\nstatic inline int normal_prio(struct task_struct *p)\n{\n\tint prio;\n\n\tif (task_has_rt_policy(p))\n\t\tprio = MAX_RT_PRIO-1 - p->rt_priority;\n\telse\n\t\tprio = __normal_prio(p);\n\treturn prio;\n}\n\n/*\n * Calculate the current priority, i.e. the priority\n * taken into account by the scheduler. This value might\n * be boosted by RT tasks, or might be boosted by\n * interactivity modifiers. Will be RT if the task got\n * RT-boosted. If not then it returns p->normal_prio.\n */\nstatic int effective_prio(struct task_struct *p)\n{\n\tp->normal_prio = normal_prio(p);\n\t/*\n\t * If we are RT tasks or we were boosted to RT priority,\n\t * keep the priority unchanged. Otherwise, update priority\n\t * to the normal priority:\n\t */\n\tif (!rt_prio(p->prio))\n\t\treturn p->normal_prio;\n\treturn p->prio;\n}\n\n/**\n * task_curr - is this task currently executing on a CPU?\n * @p: the task in question.\n */\ninline int task_curr(const struct task_struct *p)\n{\n\treturn cpu_curr(task_cpu(p)) == p;\n}\n\nstatic inline void check_class_changed(struct rq *rq, struct task_struct *p,\n\t\t\t\t       const struct sched_class *prev_class,\n\t\t\t\t       int oldprio, int running)\n{\n\tif (prev_class != p->sched_class) {\n\t\tif (prev_class->switched_from)\n\t\t\tprev_class->switched_from(rq, p, running);\n\t\tp->sched_class->switched_to(rq, p, running);\n\t} else\n\t\tp->sched_class->prio_changed(rq, p, oldprio, running);\n}\n\nstatic void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)\n{\n\tconst struct sched_class *class;\n\n\tif (p->sched_class == rq->curr->sched_class) {\n\t\trq->curr->sched_class->check_preempt_curr(rq, p, flags);\n\t} else {\n\t\tfor_each_class(class) {\n\t\t\tif (class == rq->curr->sched_class)\n\t\t\t\tbreak;\n\t\t\tif (class == p->sched_class) {\n\t\t\t\tresched_task(rq->curr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * A queue event has occurred, and we're going to schedule.  In\n\t * this case, we can save a useless back to back clock update.\n\t */\n\tif (test_tsk_need_resched(rq->curr))\n\t\trq->skip_clock_update = 1;\n}\n\n#ifdef CONFIG_SMP\n/*\n * Is this task likely cache-hot:\n */\nstatic int\ntask_hot(struct task_struct *p, u64 now, struct sched_domain *sd)\n{\n\ts64 delta;\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn 0;\n\n\tif (unlikely(p->policy == SCHED_IDLE))\n\t\treturn 0;\n\n\t/*\n\t * Buddy candidates are cache hot:\n\t */\n\tif (sched_feat(CACHE_HOT_BUDDY) && this_rq()->nr_running &&\n\t\t\t(&p->se == cfs_rq_of(&p->se)->next ||\n\t\t\t &p->se == cfs_rq_of(&p->se)->last))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == -1)\n\t\treturn 1;\n\tif (sysctl_sched_migration_cost == 0)\n\t\treturn 0;\n\n\tdelta = now - p->se.exec_start;\n\n\treturn delta < (s64)sysctl_sched_migration_cost;\n}\n\nvoid set_task_cpu(struct task_struct *p, unsigned int new_cpu)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\t/*\n\t * We should never call set_task_cpu() on a blocked task,\n\t * ttwu() will sort out the placement.\n\t */\n\tWARN_ON_ONCE(p->state != TASK_RUNNING && p->state != TASK_WAKING &&\n\t\t\t!(task_thread_info(p)->preempt_count & PREEMPT_ACTIVE));\n#endif\n\n\ttrace_sched_migrate_task(p, new_cpu);\n\n\tif (task_cpu(p) != new_cpu) {\n\t\tp->se.nr_migrations++;\n\t\tperf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, 1, NULL, 0);\n\t}\n\n\t__set_task_cpu(p, new_cpu);\n}\n\nstruct migration_arg {\n\tstruct task_struct *task;\n\tint dest_cpu;\n};\n\nstatic int migration_cpu_stop(void *data);\n\n/*\n * The task's runqueue lock must be held.\n * Returns true if you have to wait for migration thread.\n */\nstatic bool migrate_task(struct task_struct *p, int dest_cpu)\n{\n\tstruct rq *rq = task_rq(p);\n\n\t/*\n\t * If the task is not on a runqueue (and not running), then\n\t * the next wake-up will properly place the task.\n\t */\n\treturn p->se.on_rq || task_running(rq, p);\n}\n\n/*\n * wait_task_inactive - wait for a thread to unschedule.\n *\n * If @match_state is nonzero, it's the @p->state value just checked and\n * not expected to change.  If it changes, i.e. @p might have woken up,\n * then return zero.  When we succeed in waiting for @p to be off its CPU,\n * we return a positive number (its total switch count).  If a second call\n * a short while later returns the same number, the caller can be sure that\n * @p has remained unscheduled the whole time.\n *\n * The caller must ensure that the task *will* unschedule sometime soon,\n * else this function might spin for a *long* time. This function can't\n * be called with interrupts off, or it may introduce deadlock with\n * smp_call_function() if an IPI is sent by the same process we are\n * waiting to become inactive.\n */\nunsigned long wait_task_inactive(struct task_struct *p, long match_state)\n{\n\tunsigned long flags;\n\tint running, on_rq;\n\tunsigned long ncsw;\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\t/*\n\t\t * We do the initial early heuristics without holding\n\t\t * any task-queue locks at all. We'll only try to get\n\t\t * the runqueue lock when things look like they will\n\t\t * work out!\n\t\t */\n\t\trq = task_rq(p);\n\n\t\t/*\n\t\t * If the task is actively running on another CPU\n\t\t * still, just relax and busy-wait without holding\n\t\t * any locks.\n\t\t *\n\t\t * NOTE! Since we don't hold any locks, it's not\n\t\t * even sure that \"rq\" stays as the right runqueue!\n\t\t * But we don't care, since \"task_running()\" will\n\t\t * return false if the runqueue has changed and p\n\t\t * is actually now running somewhere else!\n\t\t */\n\t\twhile (task_running(rq, p)) {\n\t\t\tif (match_state && unlikely(p->state != match_state))\n\t\t\t\treturn 0;\n\t\t\tcpu_relax();\n\t\t}\n\n\t\t/*\n\t\t * Ok, time to look more closely! We need the rq\n\t\t * lock now, to be *sure*. If we're wrong, we'll\n\t\t * just go back and repeat.\n\t\t */\n\t\trq = task_rq_lock(p, &flags);\n\t\ttrace_sched_wait_task(p);\n\t\trunning = task_running(rq, p);\n\t\ton_rq = p->se.on_rq;\n\t\tncsw = 0;\n\t\tif (!match_state || p->state == match_state)\n\t\t\tncsw = p->nvcsw | LONG_MIN; /* sets MSB */\n\t\ttask_rq_unlock(rq, &flags);\n\n\t\t/*\n\t\t * If it changed from the expected state, bail out now.\n\t\t */\n\t\tif (unlikely(!ncsw))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Was it really running after all now that we\n\t\t * checked with the proper locks actually held?\n\t\t *\n\t\t * Oops. Go back and try again..\n\t\t */\n\t\tif (unlikely(running)) {\n\t\t\tcpu_relax();\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * It's not enough that it's not actively running,\n\t\t * it must be off the runqueue _entirely_, and not\n\t\t * preempted!\n\t\t *\n\t\t * So if it was still runnable (but just not actively\n\t\t * running right now), it's preempted, and we should\n\t\t * yield - it could be a while.\n\t\t */\n\t\tif (unlikely(on_rq)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Ahh, all good. It wasn't running, and it wasn't\n\t\t * runnable, which means that it will never become\n\t\t * running in the future either. We're all done!\n\t\t */\n\t\tbreak;\n\t}\n\n\treturn ncsw;\n}\n\n/***\n * kick_process - kick a running thread to enter/exit the kernel\n * @p: the to-be-kicked thread\n *\n * Cause a process which is running on another CPU to enter\n * kernel-mode, without any delay. (to get signals handled.)\n *\n * NOTE: this function doesnt have to take the runqueue lock,\n * because all it wants to ensure is that the remote task enters\n * the kernel. If the IPI races and the task has been migrated\n * to another CPU then no harm is done and the purpose has been\n * achieved as well.\n */\nvoid kick_process(struct task_struct *p)\n{\n\tint cpu;\n\n\tpreempt_disable();\n\tcpu = task_cpu(p);\n\tif ((cpu != smp_processor_id()) && task_curr(p))\n\t\tsmp_send_reschedule(cpu);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(kick_process);\n#endif /* CONFIG_SMP */\n\n/**\n * task_oncpu_function_call - call a function on the cpu on which a task runs\n * @p:\t\tthe task to evaluate\n * @func:\tthe function to be called\n * @info:\tthe function call argument\n *\n * Calls the function @func when the task is currently running. This might\n * be on the current CPU, which just calls the function directly\n */\nvoid task_oncpu_function_call(struct task_struct *p,\n\t\t\t      void (*func) (void *info), void *info)\n{\n\tint cpu;\n\n\tpreempt_disable();\n\tcpu = task_cpu(p);\n\tif (task_curr(p))\n\t\tsmp_call_function_single(cpu, func, info, 1);\n\tpreempt_enable();\n}\n\n#ifdef CONFIG_SMP\n/*\n * ->cpus_allowed is protected by either TASK_WAKING or rq->lock held.\n */\nstatic int select_fallback_rq(int cpu, struct task_struct *p)\n{\n\tint dest_cpu;\n\tconst struct cpumask *nodemask = cpumask_of_node(cpu_to_node(cpu));\n\n\t/* Look for allowed, online CPU in same node. */\n\tfor_each_cpu_and(dest_cpu, nodemask, cpu_active_mask)\n\t\tif (cpumask_test_cpu(dest_cpu, &p->cpus_allowed))\n\t\t\treturn dest_cpu;\n\n\t/* Any allowed, online CPU? */\n\tdest_cpu = cpumask_any_and(&p->cpus_allowed, cpu_active_mask);\n\tif (dest_cpu < nr_cpu_ids)\n\t\treturn dest_cpu;\n\n\t/* No more Mr. Nice Guy. */\n\tif (unlikely(dest_cpu >= nr_cpu_ids)) {\n\t\tdest_cpu = cpuset_cpus_allowed_fallback(p);\n\t\t/*\n\t\t * Don't tell them about moving exiting tasks or\n\t\t * kernel threads (both mm NULL), since they never\n\t\t * leave kernel.\n\t\t */\n\t\tif (p->mm && printk_ratelimit()) {\n\t\t\tprintk(KERN_INFO \"process %d (%s) no \"\n\t\t\t       \"longer affine to cpu%d\\n\",\n\t\t\t       task_pid_nr(p), p->comm, cpu);\n\t\t}\n\t}\n\n\treturn dest_cpu;\n}\n\n/*\n * The caller (fork, wakeup) owns TASK_WAKING, ->cpus_allowed is stable.\n */\nstatic inline\nint select_task_rq(struct rq *rq, struct task_struct *p, int sd_flags, int wake_flags)\n{\n\tint cpu = p->sched_class->select_task_rq(rq, p, sd_flags, wake_flags);\n\n\t/*\n\t * In order not to call set_task_cpu() on a blocking task we need\n\t * to rely on ttwu() to place the task on a valid ->cpus_allowed\n\t * cpu.\n\t *\n\t * Since this is common to all placement strategies, this lives here.\n\t *\n\t * [ this allows ->select_task() to simply return task_cpu(p) and\n\t *   not worry about this generic constraint ]\n\t */\n\tif (unlikely(!cpumask_test_cpu(cpu, &p->cpus_allowed) ||\n\t\t     !cpu_online(cpu)))\n\t\tcpu = select_fallback_rq(task_cpu(p), p);\n\n\treturn cpu;\n}\n\nstatic void update_avg(u64 *avg, u64 sample)\n{\n\ts64 diff = sample - *avg;\n\t*avg += diff >> 3;\n}\n#endif\n\nstatic inline void ttwu_activate(struct task_struct *p, struct rq *rq,\n\t\t\t\t bool is_sync, bool is_migrate, bool is_local,\n\t\t\t\t unsigned long en_flags)\n{\n\tschedstat_inc(p, se.statistics.nr_wakeups);\n\tif (is_sync)\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_sync);\n\tif (is_migrate)\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_migrate);\n\tif (is_local)\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_local);\n\telse\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_remote);\n\n\tactivate_task(rq, p, en_flags);\n}\n\nstatic inline void ttwu_post_activation(struct task_struct *p, struct rq *rq,\n\t\t\t\t\tint wake_flags, bool success)\n{\n\ttrace_sched_wakeup(p, success);\n\tcheck_preempt_curr(rq, p, wake_flags);\n\n\tp->state = TASK_RUNNING;\n#ifdef CONFIG_SMP\n\tif (p->sched_class->task_woken)\n\t\tp->sched_class->task_woken(rq, p);\n\n\tif (unlikely(rq->idle_stamp)) {\n\t\tu64 delta = rq->clock - rq->idle_stamp;\n\t\tu64 max = 2*sysctl_sched_migration_cost;\n\n\t\tif (delta > max)\n\t\t\trq->avg_idle = max;\n\t\telse\n\t\t\tupdate_avg(&rq->avg_idle, delta);\n\t\trq->idle_stamp = 0;\n\t}\n#endif\n\t/* if a worker is waking up, notify workqueue */\n\tif ((p->flags & PF_WQ_WORKER) && success)\n\t\twq_worker_waking_up(p, cpu_of(rq));\n}\n\n/**\n * try_to_wake_up - wake up a thread\n * @p: the thread to be awakened\n * @state: the mask of task states that can be woken\n * @wake_flags: wake modifier flags (WF_*)\n *\n * Put it on the run-queue if it's not already there. The \"current\"\n * thread is always on the run-queue (except when the actual\n * re-schedule is in progress), and as such you're allowed to do\n * the simpler \"current->state = TASK_RUNNING\" to mark yourself\n * runnable without the overhead of this.\n *\n * Returns %true if @p was woken up, %false if it was already running\n * or @state didn't match @p's state.\n */\nstatic int try_to_wake_up(struct task_struct *p, unsigned int state,\n\t\t\t  int wake_flags)\n{\n\tint cpu, orig_cpu, this_cpu, success = 0;\n\tunsigned long flags;\n\tunsigned long en_flags = ENQUEUE_WAKEUP;\n\tstruct rq *rq;\n\n\tthis_cpu = get_cpu();\n\n\tsmp_wmb();\n\trq = task_rq_lock(p, &flags);\n\tif (!(p->state & state))\n\t\tgoto out;\n\n\tif (p->se.on_rq)\n\t\tgoto out_running;\n\n\tcpu = task_cpu(p);\n\torig_cpu = cpu;\n\n#ifdef CONFIG_SMP\n\tif (unlikely(task_running(rq, p)))\n\t\tgoto out_activate;\n\n\t/*\n\t * In order to handle concurrent wakeups and release the rq->lock\n\t * we put the task in TASK_WAKING state.\n\t *\n\t * First fix up the nr_uninterruptible count:\n\t */\n\tif (task_contributes_to_load(p)) {\n\t\tif (likely(cpu_online(orig_cpu)))\n\t\t\trq->nr_uninterruptible--;\n\t\telse\n\t\t\tthis_rq()->nr_uninterruptible--;\n\t}\n\tp->state = TASK_WAKING;\n\n\tif (p->sched_class->task_waking) {\n\t\tp->sched_class->task_waking(rq, p);\n\t\ten_flags |= ENQUEUE_WAKING;\n\t}\n\n\tcpu = select_task_rq(rq, p, SD_BALANCE_WAKE, wake_flags);\n\tif (cpu != orig_cpu)\n\t\tset_task_cpu(p, cpu);\n\t__task_rq_unlock(rq);\n\n\trq = cpu_rq(cpu);\n\traw_spin_lock(&rq->lock);\n\n\t/*\n\t * We migrated the task without holding either rq->lock, however\n\t * since the task is not on the task list itself, nobody else\n\t * will try and migrate the task, hence the rq should match the\n\t * cpu we just moved it to.\n\t */\n\tWARN_ON(task_cpu(p) != cpu);\n\tWARN_ON(p->state != TASK_WAKING);\n\n#ifdef CONFIG_SCHEDSTATS\n\tschedstat_inc(rq, ttwu_count);\n\tif (cpu == this_cpu)\n\t\tschedstat_inc(rq, ttwu_local);\n\telse {\n\t\tstruct sched_domain *sd;\n\t\tfor_each_domain(this_cpu, sd) {\n\t\t\tif (cpumask_test_cpu(cpu, sched_domain_span(sd))) {\n\t\t\t\tschedstat_inc(sd, ttwu_wake_remote);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n#endif /* CONFIG_SCHEDSTATS */\n\nout_activate:\n#endif /* CONFIG_SMP */\n\tttwu_activate(p, rq, wake_flags & WF_SYNC, orig_cpu != cpu,\n\t\t      cpu == this_cpu, en_flags);\n\tsuccess = 1;\nout_running:\n\tttwu_post_activation(p, rq, wake_flags, success);\nout:\n\ttask_rq_unlock(rq, &flags);\n\tput_cpu();\n\n\treturn success;\n}\n\n/**\n * try_to_wake_up_local - try to wake up a local task with rq lock held\n * @p: the thread to be awakened\n *\n * Put @p on the run-queue if it's not alredy there.  The caller must\n * ensure that this_rq() is locked, @p is bound to this_rq() and not\n * the current task.  this_rq() stays locked over invocation.\n */\nstatic void try_to_wake_up_local(struct task_struct *p)\n{\n\tstruct rq *rq = task_rq(p);\n\tbool success = false;\n\n\tBUG_ON(rq != this_rq());\n\tBUG_ON(p == current);\n\tlockdep_assert_held(&rq->lock);\n\n\tif (!(p->state & TASK_NORMAL))\n\t\treturn;\n\n\tif (!p->se.on_rq) {\n\t\tif (likely(!task_running(rq, p))) {\n\t\t\tschedstat_inc(rq, ttwu_count);\n\t\t\tschedstat_inc(rq, ttwu_local);\n\t\t}\n\t\tttwu_activate(p, rq, false, false, true, ENQUEUE_WAKEUP);\n\t\tsuccess = true;\n\t}\n\tttwu_post_activation(p, rq, 0, success);\n}\n\n/**\n * wake_up_process - Wake up a specific process\n * @p: The process to be woken up.\n *\n * Attempt to wake up the nominated process and move it to the set of runnable\n * processes.  Returns 1 if the process was woken up, 0 if it was already\n * running.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nint wake_up_process(struct task_struct *p)\n{\n\treturn try_to_wake_up(p, TASK_ALL, 0);\n}\nEXPORT_SYMBOL(wake_up_process);\n\nint wake_up_state(struct task_struct *p, unsigned int state)\n{\n\treturn try_to_wake_up(p, state, 0);\n}\n\n/*\n * Perform scheduler related setup for a newly forked process p.\n * p is forked by current.\n *\n * __sched_fork() is basic setup used by init_idle() too:\n */\nstatic void __sched_fork(struct task_struct *p)\n{\n\tp->se.exec_start\t\t= 0;\n\tp->se.sum_exec_runtime\t\t= 0;\n\tp->se.prev_sum_exec_runtime\t= 0;\n\tp->se.nr_migrations\t\t= 0;\n\n#ifdef CONFIG_SCHEDSTATS\n\tmemset(&p->se.statistics, 0, sizeof(p->se.statistics));\n#endif\n\n\tINIT_LIST_HEAD(&p->rt.run_list);\n\tp->se.on_rq = 0;\n\tINIT_LIST_HEAD(&p->se.group_node);\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\tINIT_HLIST_HEAD(&p->preempt_notifiers);\n#endif\n}\n\n/*\n * fork()/clone()-time setup:\n */\nvoid sched_fork(struct task_struct *p, int clone_flags)\n{\n\tint cpu = get_cpu();\n\n\t__sched_fork(p);\n\t/*\n\t * We mark the process as running here. This guarantees that\n\t * nobody will actually run it, and a signal or other external\n\t * event cannot wake it up and insert it on the runqueue either.\n\t */\n\tp->state = TASK_RUNNING;\n\n\t/*\n\t * Revert to default priority/policy on fork if requested.\n\t */\n\tif (unlikely(p->sched_reset_on_fork)) {\n\t\tif (p->policy == SCHED_FIFO || p->policy == SCHED_RR) {\n\t\t\tp->policy = SCHED_NORMAL;\n\t\t\tp->normal_prio = p->static_prio;\n\t\t}\n\n\t\tif (PRIO_TO_NICE(p->static_prio) < 0) {\n\t\t\tp->static_prio = NICE_TO_PRIO(0);\n\t\t\tp->normal_prio = p->static_prio;\n\t\t\tset_load_weight(p);\n\t\t}\n\n\t\t/*\n\t\t * We don't need the reset flag anymore after the fork. It has\n\t\t * fulfilled its duty:\n\t\t */\n\t\tp->sched_reset_on_fork = 0;\n\t}\n\n\t/*\n\t * Make sure we do not leak PI boosting priority to the child.\n\t */\n\tp->prio = current->normal_prio;\n\n\tif (!rt_prio(p->prio))\n\t\tp->sched_class = &fair_sched_class;\n\n\tif (p->sched_class->task_fork)\n\t\tp->sched_class->task_fork(p);\n\n\t/*\n\t * The child is not yet in the pid-hash so no cgroup attach races,\n\t * and the cgroup is pinned to this child due to cgroup_fork()\n\t * is ran before sched_fork().\n\t *\n\t * Silence PROVE_RCU.\n\t */\n\trcu_read_lock();\n\tset_task_cpu(p, cpu);\n\trcu_read_unlock();\n\n#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)\n\tif (likely(sched_info_on()))\n\t\tmemset(&p->sched_info, 0, sizeof(p->sched_info));\n#endif\n#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)\n\tp->oncpu = 0;\n#endif\n#ifdef CONFIG_PREEMPT\n\t/* Want to start with kernel preemption disabled. */\n\ttask_thread_info(p)->preempt_count = 1;\n#endif\n\tplist_node_init(&p->pushable_tasks, MAX_PRIO);\n\n\tput_cpu();\n}\n\n/*\n * wake_up_new_task - wake up a newly created task for the first time.\n *\n * This function will do some initial scheduler statistics housekeeping\n * that must be done for every newly created context, then puts the task\n * on the runqueue and wakes it.\n */\nvoid wake_up_new_task(struct task_struct *p, unsigned long clone_flags)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint cpu __maybe_unused = get_cpu();\n\n#ifdef CONFIG_SMP\n\trq = task_rq_lock(p, &flags);\n\tp->state = TASK_WAKING;\n\n\t/*\n\t * Fork balancing, do it here and not earlier because:\n\t *  - cpus_allowed can change in the fork path\n\t *  - any previously selected cpu might disappear through hotplug\n\t *\n\t * We set TASK_WAKING so that select_task_rq() can drop rq->lock\n\t * without people poking at ->cpus_allowed.\n\t */\n\tcpu = select_task_rq(rq, p, SD_BALANCE_FORK, 0);\n\tset_task_cpu(p, cpu);\n\n\tp->state = TASK_RUNNING;\n\ttask_rq_unlock(rq, &flags);\n#endif\n\n\trq = task_rq_lock(p, &flags);\n\tactivate_task(rq, p, 0);\n\ttrace_sched_wakeup_new(p, 1);\n\tcheck_preempt_curr(rq, p, WF_FORK);\n#ifdef CONFIG_SMP\n\tif (p->sched_class->task_woken)\n\t\tp->sched_class->task_woken(rq, p);\n#endif\n\ttask_rq_unlock(rq, &flags);\n\tput_cpu();\n}\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\n/**\n * preempt_notifier_register - tell me when current is being preempted & rescheduled\n * @notifier: notifier struct to register\n */\nvoid preempt_notifier_register(struct preempt_notifier *notifier)\n{\n\thlist_add_head(&notifier->link, &current->preempt_notifiers);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_register);\n\n/**\n * preempt_notifier_unregister - no longer interested in preemption notifications\n * @notifier: notifier struct to unregister\n *\n * This is safe to call from within a preemption notifier.\n */\nvoid preempt_notifier_unregister(struct preempt_notifier *notifier)\n{\n\thlist_del(&notifier->link);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_unregister);\n\nstatic void fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n\tstruct preempt_notifier *notifier;\n\tstruct hlist_node *node;\n\n\thlist_for_each_entry(notifier, node, &curr->preempt_notifiers, link)\n\t\tnotifier->ops->sched_in(notifier, raw_smp_processor_id());\n}\n\nstatic void\nfire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t struct task_struct *next)\n{\n\tstruct preempt_notifier *notifier;\n\tstruct hlist_node *node;\n\n\thlist_for_each_entry(notifier, node, &curr->preempt_notifiers, link)\n\t\tnotifier->ops->sched_out(notifier, next);\n}\n\n#else /* !CONFIG_PREEMPT_NOTIFIERS */\n\nstatic void fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n}\n\nstatic void\nfire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t struct task_struct *next)\n{\n}\n\n#endif /* CONFIG_PREEMPT_NOTIFIERS */\n\n/**\n * prepare_task_switch - prepare to switch tasks\n * @rq: the runqueue preparing to switch\n * @prev: the current task that is being switched out\n * @next: the task we are going to switch to.\n *\n * This is called with the rq lock held and interrupts off. It must\n * be paired with a subsequent finish_task_switch after the context\n * switch.\n *\n * prepare_task_switch sets up locking and calls architecture specific\n * hooks.\n */\nstatic inline void\nprepare_task_switch(struct rq *rq, struct task_struct *prev,\n\t\t    struct task_struct *next)\n{\n\tfire_sched_out_preempt_notifiers(prev, next);\n\tprepare_lock_switch(rq, next);\n\tprepare_arch_switch(next);\n}\n\n/**\n * finish_task_switch - clean up after a task-switch\n * @rq: runqueue associated with task-switch\n * @prev: the thread we just switched away from.\n *\n * finish_task_switch must be called after the context switch, paired\n * with a prepare_task_switch call before the context switch.\n * finish_task_switch will reconcile locking set up by prepare_task_switch,\n * and do any other architecture-specific cleanup actions.\n *\n * Note that we may have delayed dropping an mm in context_switch(). If\n * so, we finish that here outside of the runqueue lock. (Doing it\n * with the lock held can cause deadlocks; see schedule() for\n * details.)\n */\nstatic void finish_task_switch(struct rq *rq, struct task_struct *prev)\n\t__releases(rq->lock)\n{\n\tstruct mm_struct *mm = rq->prev_mm;\n\tlong prev_state;\n\n\trq->prev_mm = NULL;\n\n\t/*\n\t * A task struct has one reference for the use as \"current\".\n\t * If a task dies, then it sets TASK_DEAD in tsk->state and calls\n\t * schedule one last time. The schedule call will never return, and\n\t * the scheduled task must drop that reference.\n\t * The test for TASK_DEAD must occur while the runqueue locks are\n\t * still held, otherwise prev could be scheduled on another cpu, die\n\t * there before we look at prev->state, and then the reference would\n\t * be dropped twice.\n\t *\t\tManfred Spraul <manfred@colorfullife.com>\n\t */\n\tprev_state = prev->state;\n\tfinish_arch_switch(prev);\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_disable();\n#endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */\n\tperf_event_task_sched_in(current);\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_enable();\n#endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */\n\tfinish_lock_switch(rq, prev);\n\n\tfire_sched_in_preempt_notifiers(current);\n\tif (mm)\n\t\tmmdrop(mm);\n\tif (unlikely(prev_state == TASK_DEAD)) {\n\t\t/*\n\t\t * Remove function-return probe instances associated with this\n\t\t * task and put them back on the free list.\n\t\t */\n\t\tkprobe_flush_task(prev);\n\t\tput_task_struct(prev);\n\t}\n}\n\n#ifdef CONFIG_SMP\n\n/* assumes rq->lock is held */\nstatic inline void pre_schedule(struct rq *rq, struct task_struct *prev)\n{\n\tif (prev->sched_class->pre_schedule)\n\t\tprev->sched_class->pre_schedule(rq, prev);\n}\n\n/* rq->lock is NOT held, but preemption is disabled */\nstatic inline void post_schedule(struct rq *rq)\n{\n\tif (rq->post_schedule) {\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->curr->sched_class->post_schedule)\n\t\t\trq->curr->sched_class->post_schedule(rq);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\t\trq->post_schedule = 0;\n\t}\n}\n\n#else\n\nstatic inline void pre_schedule(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void post_schedule(struct rq *rq)\n{\n}\n\n#endif\n\n/**\n * schedule_tail - first thing a freshly forked thread must call.\n * @prev: the thread we just switched away from.\n */\nasmlinkage void schedule_tail(struct task_struct *prev)\n\t__releases(rq->lock)\n{\n\tstruct rq *rq = this_rq();\n\n\tfinish_task_switch(rq, prev);\n\n\t/*\n\t * FIXME: do we need to worry about rq being invalidated by the\n\t * task_switch?\n\t */\n\tpost_schedule(rq);\n\n#ifdef __ARCH_WANT_UNLOCKED_CTXSW\n\t/* In this case, finish_task_switch does not reenable preemption */\n\tpreempt_enable();\n#endif\n\tif (current->set_child_tid)\n\t\tput_user(task_pid_vnr(current), current->set_child_tid);\n}\n\n/*\n * context_switch - switch to the new MM and the new\n * thread's register state.\n */\nstatic inline void\ncontext_switch(struct rq *rq, struct task_struct *prev,\n\t       struct task_struct *next)\n{\n\tstruct mm_struct *mm, *oldmm;\n\n\tprepare_task_switch(rq, prev, next);\n\ttrace_sched_switch(prev, next);\n\tmm = next->mm;\n\toldmm = prev->active_mm;\n\t/*\n\t * For paravirt, this is coupled with an exit in switch_to to\n\t * combine the page table reload and the switch backend into\n\t * one hypercall.\n\t */\n\tarch_start_context_switch(prev);\n\n\tif (!mm) {\n\t\tnext->active_mm = oldmm;\n\t\tatomic_inc(&oldmm->mm_count);\n\t\tenter_lazy_tlb(oldmm, next);\n\t} else\n\t\tswitch_mm(oldmm, mm, next);\n\n\tif (!prev->mm) {\n\t\tprev->active_mm = NULL;\n\t\trq->prev_mm = oldmm;\n\t}\n\t/*\n\t * Since the runqueue lock will be released by the next\n\t * task (which is an invalid locking op but in the case\n\t * of the scheduler it's an obvious special-case), so we\n\t * do an early lockdep release here:\n\t */\n#ifndef __ARCH_WANT_UNLOCKED_CTXSW\n\tspin_release(&rq->lock.dep_map, 1, _THIS_IP_);\n#endif\n\n\t/* Here we just switch the register state and the stack. */\n\tswitch_to(prev, next, prev);\n\n\tbarrier();\n\t/*\n\t * this_rq must be evaluated again because prev may have moved\n\t * CPUs since it called schedule(), thus the 'rq' on its stack\n\t * frame will be invalid.\n\t */\n\tfinish_task_switch(this_rq(), prev);\n}\n\n/*\n * nr_running, nr_uninterruptible and nr_context_switches:\n *\n * externally visible scheduler statistics: current number of runnable\n * threads, current number of uninterruptible-sleeping threads, total\n * number of context switches performed since bootup.\n */\nunsigned long nr_running(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_online_cpu(i)\n\t\tsum += cpu_rq(i)->nr_running;\n\n\treturn sum;\n}\n\nunsigned long nr_uninterruptible(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += cpu_rq(i)->nr_uninterruptible;\n\n\t/*\n\t * Since we read the counters lockless, it might be slightly\n\t * inaccurate. Do not allow it to go below zero though:\n\t */\n\tif (unlikely((long)sum < 0))\n\t\tsum = 0;\n\n\treturn sum;\n}\n\nunsigned long long nr_context_switches(void)\n{\n\tint i;\n\tunsigned long long sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += cpu_rq(i)->nr_switches;\n\n\treturn sum;\n}\n\nunsigned long nr_iowait(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += atomic_read(&cpu_rq(i)->nr_iowait);\n\n\treturn sum;\n}\n\nunsigned long nr_iowait_cpu(int cpu)\n{\n\tstruct rq *this = cpu_rq(cpu);\n\treturn atomic_read(&this->nr_iowait);\n}\n\nunsigned long this_cpu_load(void)\n{\n\tstruct rq *this = this_rq();\n\treturn this->cpu_load[0];\n}\n\n\n/* Variables and functions for calc_load */\nstatic atomic_long_t calc_load_tasks;\nstatic unsigned long calc_load_update;\nunsigned long avenrun[3];\nEXPORT_SYMBOL(avenrun);\n\nstatic long calc_load_fold_active(struct rq *this_rq)\n{\n\tlong nr_active, delta = 0;\n\n\tnr_active = this_rq->nr_running;\n\tnr_active += (long) this_rq->nr_uninterruptible;\n\n\tif (nr_active != this_rq->calc_load_active) {\n\t\tdelta = nr_active - this_rq->calc_load_active;\n\t\tthis_rq->calc_load_active = nr_active;\n\t}\n\n\treturn delta;\n}\n\nstatic unsigned long\ncalc_load(unsigned long load, unsigned long exp, unsigned long active)\n{\n\tload *= exp;\n\tload += active * (FIXED_1 - exp);\n\tload += 1UL << (FSHIFT - 1);\n\treturn load >> FSHIFT;\n}\n\n#ifdef CONFIG_NO_HZ\n/*\n * For NO_HZ we delay the active fold to the next LOAD_FREQ update.\n *\n * When making the ILB scale, we should try to pull this in as well.\n */\nstatic atomic_long_t calc_load_tasks_idle;\n\nstatic void calc_load_account_idle(struct rq *this_rq)\n{\n\tlong delta;\n\n\tdelta = calc_load_fold_active(this_rq);\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks_idle);\n}\n\nstatic long calc_load_fold_idle(void)\n{\n\tlong delta = 0;\n\n\t/*\n\t * Its got a race, we don't care...\n\t */\n\tif (atomic_long_read(&calc_load_tasks_idle))\n\t\tdelta = atomic_long_xchg(&calc_load_tasks_idle, 0);\n\n\treturn delta;\n}\n\n/**\n * fixed_power_int - compute: x^n, in O(log n) time\n *\n * @x:         base of the power\n * @frac_bits: fractional bits of @x\n * @n:         power to raise @x to.\n *\n * By exploiting the relation between the definition of the natural power\n * function: x^n := x*x*...*x (x multiplied by itself for n times), and\n * the binary encoding of numbers used by computers: n := \\Sum n_i * 2^i,\n * (where: n_i \\elem {0, 1}, the binary vector representing n),\n * we find: x^n := x^(\\Sum n_i * 2^i) := \\Prod x^(n_i * 2^i), which is\n * of course trivially computable in O(log_2 n), the length of our binary\n * vector.\n */\nstatic unsigned long\nfixed_power_int(unsigned long x, unsigned int frac_bits, unsigned int n)\n{\n\tunsigned long result = 1UL << frac_bits;\n\n\tif (n) for (;;) {\n\t\tif (n & 1) {\n\t\t\tresult *= x;\n\t\t\tresult += 1UL << (frac_bits - 1);\n\t\t\tresult >>= frac_bits;\n\t\t}\n\t\tn >>= 1;\n\t\tif (!n)\n\t\t\tbreak;\n\t\tx *= x;\n\t\tx += 1UL << (frac_bits - 1);\n\t\tx >>= frac_bits;\n\t}\n\n\treturn result;\n}\n\n/*\n * a1 = a0 * e + a * (1 - e)\n *\n * a2 = a1 * e + a * (1 - e)\n *    = (a0 * e + a * (1 - e)) * e + a * (1 - e)\n *    = a0 * e^2 + a * (1 - e) * (1 + e)\n *\n * a3 = a2 * e + a * (1 - e)\n *    = (a0 * e^2 + a * (1 - e) * (1 + e)) * e + a * (1 - e)\n *    = a0 * e^3 + a * (1 - e) * (1 + e + e^2)\n *\n *  ...\n *\n * an = a0 * e^n + a * (1 - e) * (1 + e + ... + e^n-1) [1]\n *    = a0 * e^n + a * (1 - e) * (1 - e^n)/(1 - e)\n *    = a0 * e^n + a * (1 - e^n)\n *\n * [1] application of the geometric series:\n *\n *              n         1 - x^(n+1)\n *     S_n := \\Sum x^i = -------------\n *             i=0          1 - x\n */\nstatic unsigned long\ncalc_load_n(unsigned long load, unsigned long exp,\n\t    unsigned long active, unsigned int n)\n{\n\n\treturn calc_load(load, fixed_power_int(exp, FSHIFT, n), active);\n}\n\n/*\n * NO_HZ can leave us missing all per-cpu ticks calling\n * calc_load_account_active(), but since an idle CPU folds its delta into\n * calc_load_tasks_idle per calc_load_account_idle(), all we need to do is fold\n * in the pending idle delta if our idle period crossed a load cycle boundary.\n *\n * Once we've updated the global active value, we need to apply the exponential\n * weights adjusted to the number of cycles missed.\n */\nstatic void calc_global_nohz(unsigned long ticks)\n{\n\tlong delta, active, n;\n\n\tif (time_before(jiffies, calc_load_update))\n\t\treturn;\n\n\t/*\n\t * If we crossed a calc_load_update boundary, make sure to fold\n\t * any pending idle changes, the respective CPUs might have\n\t * missed the tick driven calc_load_account_active() update\n\t * due to NO_HZ.\n\t */\n\tdelta = calc_load_fold_idle();\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks);\n\n\t/*\n\t * If we were idle for multiple load cycles, apply them.\n\t */\n\tif (ticks >= LOAD_FREQ) {\n\t\tn = ticks / LOAD_FREQ;\n\n\t\tactive = atomic_long_read(&calc_load_tasks);\n\t\tactive = active > 0 ? active * FIXED_1 : 0;\n\n\t\tavenrun[0] = calc_load_n(avenrun[0], EXP_1, active, n);\n\t\tavenrun[1] = calc_load_n(avenrun[1], EXP_5, active, n);\n\t\tavenrun[2] = calc_load_n(avenrun[2], EXP_15, active, n);\n\n\t\tcalc_load_update += n * LOAD_FREQ;\n\t}\n\n\t/*\n\t * Its possible the remainder of the above division also crosses\n\t * a LOAD_FREQ period, the regular check in calc_global_load()\n\t * which comes after this will take care of that.\n\t *\n\t * Consider us being 11 ticks before a cycle completion, and us\n\t * sleeping for 4*LOAD_FREQ + 22 ticks, then the above code will\n\t * age us 4 cycles, and the test in calc_global_load() will\n\t * pick up the final one.\n\t */\n}\n#else\nstatic void calc_load_account_idle(struct rq *this_rq)\n{\n}\n\nstatic inline long calc_load_fold_idle(void)\n{\n\treturn 0;\n}\n\nstatic void calc_global_nohz(unsigned long ticks)\n{\n}\n#endif\n\n/**\n * get_avenrun - get the load average array\n * @loads:\tpointer to dest load array\n * @offset:\toffset to add\n * @shift:\tshift count to shift the result left\n *\n * These values are estimates at best, so no need for locking.\n */\nvoid get_avenrun(unsigned long *loads, unsigned long offset, int shift)\n{\n\tloads[0] = (avenrun[0] + offset) << shift;\n\tloads[1] = (avenrun[1] + offset) << shift;\n\tloads[2] = (avenrun[2] + offset) << shift;\n}\n\n/*\n * calc_load - update the avenrun load estimates 10 ticks after the\n * CPUs have updated calc_load_tasks.\n */\nvoid calc_global_load(unsigned long ticks)\n{\n\tlong active;\n\n\tcalc_global_nohz(ticks);\n\n\tif (time_before(jiffies, calc_load_update + 10))\n\t\treturn;\n\n\tactive = atomic_long_read(&calc_load_tasks);\n\tactive = active > 0 ? active * FIXED_1 : 0;\n\n\tavenrun[0] = calc_load(avenrun[0], EXP_1, active);\n\tavenrun[1] = calc_load(avenrun[1], EXP_5, active);\n\tavenrun[2] = calc_load(avenrun[2], EXP_15, active);\n\n\tcalc_load_update += LOAD_FREQ;\n}\n\n/*\n * Called from update_cpu_load() to periodically update this CPU's\n * active count.\n */\nstatic void calc_load_account_active(struct rq *this_rq)\n{\n\tlong delta;\n\n\tif (time_before(jiffies, this_rq->calc_load_update))\n\t\treturn;\n\n\tdelta  = calc_load_fold_active(this_rq);\n\tdelta += calc_load_fold_idle();\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks);\n\n\tthis_rq->calc_load_update += LOAD_FREQ;\n}\n\n/*\n * The exact cpuload at various idx values, calculated at every tick would be\n * load = (2^idx - 1) / 2^idx * load + 1 / 2^idx * cur_load\n *\n * If a cpu misses updates for n-1 ticks (as it was idle) and update gets called\n * on nth tick when cpu may be busy, then we have:\n * load = ((2^idx - 1) / 2^idx)^(n-1) * load\n * load = (2^idx - 1) / 2^idx) * load + 1 / 2^idx * cur_load\n *\n * decay_load_missed() below does efficient calculation of\n * load = ((2^idx - 1) / 2^idx)^(n-1) * load\n * avoiding 0..n-1 loop doing load = ((2^idx - 1) / 2^idx) * load\n *\n * The calculation is approximated on a 128 point scale.\n * degrade_zero_ticks is the number of ticks after which load at any\n * particular idx is approximated to be zero.\n * degrade_factor is a precomputed table, a row for each load idx.\n * Each column corresponds to degradation factor for a power of two ticks,\n * based on 128 point scale.\n * Example:\n * row 2, col 3 (=12) says that the degradation at load idx 2 after\n * 8 ticks is 12/128 (which is an approximation of exact factor 3^8/4^8).\n *\n * With this power of 2 load factors, we can degrade the load n times\n * by looking at 1 bits in n and doing as many mult/shift instead of\n * n mult/shifts needed by the exact degradation.\n */\n#define DEGRADE_SHIFT\t\t7\nstatic const unsigned char\n\t\tdegrade_zero_ticks[CPU_LOAD_IDX_MAX] = {0, 8, 32, 64, 128};\nstatic const unsigned char\n\t\tdegrade_factor[CPU_LOAD_IDX_MAX][DEGRADE_SHIFT + 1] = {\n\t\t\t\t\t{0, 0, 0, 0, 0, 0, 0, 0},\n\t\t\t\t\t{64, 32, 8, 0, 0, 0, 0, 0},\n\t\t\t\t\t{96, 72, 40, 12, 1, 0, 0},\n\t\t\t\t\t{112, 98, 75, 43, 15, 1, 0},\n\t\t\t\t\t{120, 112, 98, 76, 45, 16, 2} };\n\n/*\n * Update cpu_load for any missed ticks, due to tickless idle. The backlog\n * would be when CPU is idle and so we just decay the old load without\n * adding any new load.\n */\nstatic unsigned long\ndecay_load_missed(unsigned long load, unsigned long missed_updates, int idx)\n{\n\tint j = 0;\n\n\tif (!missed_updates)\n\t\treturn load;\n\n\tif (missed_updates >= degrade_zero_ticks[idx])\n\t\treturn 0;\n\n\tif (idx == 1)\n\t\treturn load >> missed_updates;\n\n\twhile (missed_updates) {\n\t\tif (missed_updates % 2)\n\t\t\tload = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;\n\n\t\tmissed_updates >>= 1;\n\t\tj++;\n\t}\n\treturn load;\n}\n\n/*\n * Update rq->cpu_load[] statistics. This function is usually called every\n * scheduler tick (TICK_NSEC). With tickless idle this will not be called\n * every tick. We fix it up based on jiffies.\n */\nstatic void update_cpu_load(struct rq *this_rq)\n{\n\tunsigned long this_load = this_rq->load.weight;\n\tunsigned long curr_jiffies = jiffies;\n\tunsigned long pending_updates;\n\tint i, scale;\n\n\tthis_rq->nr_load_updates++;\n\n\t/* Avoid repeated calls on same jiffy, when moving in and out of idle */\n\tif (curr_jiffies == this_rq->last_load_update_tick)\n\t\treturn;\n\n\tpending_updates = curr_jiffies - this_rq->last_load_update_tick;\n\tthis_rq->last_load_update_tick = curr_jiffies;\n\n\t/* Update our load: */\n\tthis_rq->cpu_load[0] = this_load; /* Fasttrack for idx 0 */\n\tfor (i = 1, scale = 2; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {\n\t\tunsigned long old_load, new_load;\n\n\t\t/* scale is effectively 1 << i now, and >> i divides by scale */\n\n\t\told_load = this_rq->cpu_load[i];\n\t\told_load = decay_load_missed(old_load, pending_updates - 1, i);\n\t\tnew_load = this_load;\n\t\t/*\n\t\t * Round up the averaging division if load is increasing. This\n\t\t * prevents us from getting stuck on 9 if the load is 10, for\n\t\t * example.\n\t\t */\n\t\tif (new_load > old_load)\n\t\t\tnew_load += scale - 1;\n\n\t\tthis_rq->cpu_load[i] = (old_load * (scale - 1) + new_load) >> i;\n\t}\n\n\tsched_avg_update(this_rq);\n}\n\nstatic void update_cpu_load_active(struct rq *this_rq)\n{\n\tupdate_cpu_load(this_rq);\n\n\tcalc_load_account_active(this_rq);\n}\n\n#ifdef CONFIG_SMP\n\n/*\n * sched_exec - execve() is a valuable balancing opportunity, because at\n * this point the task has the smallest effective memory and cache footprint.\n */\nvoid sched_exec(void)\n{\n\tstruct task_struct *p = current;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint dest_cpu;\n\n\trq = task_rq_lock(p, &flags);\n\tdest_cpu = p->sched_class->select_task_rq(rq, p, SD_BALANCE_EXEC, 0);\n\tif (dest_cpu == smp_processor_id())\n\t\tgoto unlock;\n\n\t/*\n\t * select_task_rq() can race against ->cpus_allowed\n\t */\n\tif (cpumask_test_cpu(dest_cpu, &p->cpus_allowed) &&\n\t    likely(cpu_active(dest_cpu)) && migrate_task(p, dest_cpu)) {\n\t\tstruct migration_arg arg = { p, dest_cpu };\n\n\t\ttask_rq_unlock(rq, &flags);\n\t\tstop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);\n\t\treturn;\n\t}\nunlock:\n\ttask_rq_unlock(rq, &flags);\n}\n\n#endif\n\nDEFINE_PER_CPU(struct kernel_stat, kstat);\n\nEXPORT_PER_CPU_SYMBOL(kstat);\n\n/*\n * Return any ns on the sched_clock that have not yet been accounted in\n * @p in case that task is currently running.\n *\n * Called with task_rq_lock() held on @rq.\n */\nstatic u64 do_task_delta_exec(struct task_struct *p, struct rq *rq)\n{\n\tu64 ns = 0;\n\n\tif (task_current(rq, p)) {\n\t\tupdate_rq_clock(rq);\n\t\tns = rq->clock_task - p->se.exec_start;\n\t\tif ((s64)ns < 0)\n\t\t\tns = 0;\n\t}\n\n\treturn ns;\n}\n\nunsigned long long task_delta_exec(struct task_struct *p)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns = 0;\n\n\trq = task_rq_lock(p, &flags);\n\tns = do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, &flags);\n\n\treturn ns;\n}\n\n/*\n * Return accounted runtime for the task.\n * In case the task is currently running, return the runtime plus current's\n * pending runtime that have not been accounted yet.\n */\nunsigned long long task_sched_runtime(struct task_struct *p)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns = 0;\n\n\trq = task_rq_lock(p, &flags);\n\tns = p->se.sum_exec_runtime + do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, &flags);\n\n\treturn ns;\n}\n\n/*\n * Return sum_exec_runtime for the thread group.\n * In case the task is currently running, return the sum plus current's\n * pending runtime that have not been accounted yet.\n *\n * Note that the thread group might have other running tasks as well,\n * so the return value not includes other pending runtime that other\n * running tasks might have.\n */\nunsigned long long thread_group_sched_runtime(struct task_struct *p)\n{\n\tstruct task_cputime totals;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns;\n\n\trq = task_rq_lock(p, &flags);\n\tthread_group_cputime(p, &totals);\n\tns = totals.sum_exec_runtime + do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, &flags);\n\n\treturn ns;\n}\n\n/*\n * Account user cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @cputime: the cpu time spent in user space since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nvoid account_user_time(struct task_struct *p, cputime_t cputime,\n\t\t       cputime_t cputime_scaled)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t tmp;\n\n\t/* Add user time to process. */\n\tp->utime = cputime_add(p->utime, cputime);\n\tp->utimescaled = cputime_add(p->utimescaled, cputime_scaled);\n\taccount_group_user_time(p, cputime);\n\n\t/* Add user time to cpustat. */\n\ttmp = cputime_to_cputime64(cputime);\n\tif (TASK_NICE(p) > 0)\n\t\tcpustat->nice = cputime64_add(cpustat->nice, tmp);\n\telse\n\t\tcpustat->user = cputime64_add(cpustat->user, tmp);\n\n\tcpuacct_update_stats(p, CPUACCT_STAT_USER, cputime);\n\t/* Account for user time used */\n\tacct_update_integrals(p);\n}\n\n/*\n * Account guest cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @cputime: the cpu time spent in virtual machine since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nstatic void account_guest_time(struct task_struct *p, cputime_t cputime,\n\t\t\t       cputime_t cputime_scaled)\n{\n\tcputime64_t tmp;\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\n\ttmp = cputime_to_cputime64(cputime);\n\n\t/* Add guest time to process. */\n\tp->utime = cputime_add(p->utime, cputime);\n\tp->utimescaled = cputime_add(p->utimescaled, cputime_scaled);\n\taccount_group_user_time(p, cputime);\n\tp->gtime = cputime_add(p->gtime, cputime);\n\n\t/* Add guest time to cpustat. */\n\tif (TASK_NICE(p) > 0) {\n\t\tcpustat->nice = cputime64_add(cpustat->nice, tmp);\n\t\tcpustat->guest_nice = cputime64_add(cpustat->guest_nice, tmp);\n\t} else {\n\t\tcpustat->user = cputime64_add(cpustat->user, tmp);\n\t\tcpustat->guest = cputime64_add(cpustat->guest, tmp);\n\t}\n}\n\n/*\n * Account system cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @hardirq_offset: the offset to subtract from hardirq_count()\n * @cputime: the cpu time spent in kernel space since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nvoid account_system_time(struct task_struct *p, int hardirq_offset,\n\t\t\t cputime_t cputime, cputime_t cputime_scaled)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t tmp;\n\n\tif ((p->flags & PF_VCPU) && (irq_count() - hardirq_offset == 0)) {\n\t\taccount_guest_time(p, cputime, cputime_scaled);\n\t\treturn;\n\t}\n\n\t/* Add system time to process. */\n\tp->stime = cputime_add(p->stime, cputime);\n\tp->stimescaled = cputime_add(p->stimescaled, cputime_scaled);\n\taccount_group_system_time(p, cputime);\n\n\t/* Add system time to cpustat. */\n\ttmp = cputime_to_cputime64(cputime);\n\tif (hardirq_count() - hardirq_offset)\n\t\tcpustat->irq = cputime64_add(cpustat->irq, tmp);\n\telse if (in_serving_softirq())\n\t\tcpustat->softirq = cputime64_add(cpustat->softirq, tmp);\n\telse\n\t\tcpustat->system = cputime64_add(cpustat->system, tmp);\n\n\tcpuacct_update_stats(p, CPUACCT_STAT_SYSTEM, cputime);\n\n\t/* Account for system time used */\n\tacct_update_integrals(p);\n}\n\n/*\n * Account for involuntary wait time.\n * @steal: the cpu time spent in involuntary wait\n */\nvoid account_steal_time(cputime_t cputime)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t cputime64 = cputime_to_cputime64(cputime);\n\n\tcpustat->steal = cputime64_add(cpustat->steal, cputime64);\n}\n\n/*\n * Account for idle time.\n * @cputime: the cpu time spent in idle wait\n */\nvoid account_idle_time(cputime_t cputime)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t cputime64 = cputime_to_cputime64(cputime);\n\tstruct rq *rq = this_rq();\n\n\tif (atomic_read(&rq->nr_iowait) > 0)\n\t\tcpustat->iowait = cputime64_add(cpustat->iowait, cputime64);\n\telse\n\t\tcpustat->idle = cputime64_add(cpustat->idle, cputime64);\n}\n\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\n/*\n * Account a single tick of cpu time.\n * @p: the process that the cpu time gets accounted to\n * @user_tick: indicates if the tick is a user or a system tick\n */\nvoid account_process_tick(struct task_struct *p, int user_tick)\n{\n\tcputime_t one_jiffy_scaled = cputime_to_scaled(cputime_one_jiffy);\n\tstruct rq *rq = this_rq();\n\n\tif (user_tick)\n\t\taccount_user_time(p, cputime_one_jiffy, one_jiffy_scaled);\n\telse if ((p != rq->idle) || (irq_count() != HARDIRQ_OFFSET))\n\t\taccount_system_time(p, HARDIRQ_OFFSET, cputime_one_jiffy,\n\t\t\t\t    one_jiffy_scaled);\n\telse\n\t\taccount_idle_time(cputime_one_jiffy);\n}\n\n/*\n * Account multiple ticks of steal time.\n * @p: the process from which the cpu time has been stolen\n * @ticks: number of stolen ticks\n */\nvoid account_steal_ticks(unsigned long ticks)\n{\n\taccount_steal_time(jiffies_to_cputime(ticks));\n}\n\n/*\n * Account multiple ticks of idle time.\n * @ticks: number of stolen ticks\n */\nvoid account_idle_ticks(unsigned long ticks)\n{\n\taccount_idle_time(jiffies_to_cputime(ticks));\n}\n\n#endif\n\n/*\n * Use precise platform statistics if available:\n */\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING\nvoid task_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\t*ut = p->utime;\n\t*st = p->stime;\n}\n\nvoid thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tstruct task_cputime cputime;\n\n\tthread_group_cputime(p, &cputime);\n\n\t*ut = cputime.utime;\n\t*st = cputime.stime;\n}\n#else\n\n#ifndef nsecs_to_cputime\n# define nsecs_to_cputime(__nsecs)\tnsecs_to_jiffies(__nsecs)\n#endif\n\nvoid task_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tcputime_t rtime, utime = p->utime, total = cputime_add(utime, p->stime);\n\n\t/*\n\t * Use CFS's precise accounting:\n\t */\n\trtime = nsecs_to_cputime(p->se.sum_exec_runtime);\n\n\tif (total) {\n\t\tu64 temp = rtime;\n\n\t\ttemp *= utime;\n\t\tdo_div(temp, total);\n\t\tutime = (cputime_t)temp;\n\t} else\n\t\tutime = rtime;\n\n\t/*\n\t * Compare with previous values, to keep monotonicity:\n\t */\n\tp->prev_utime = max(p->prev_utime, utime);\n\tp->prev_stime = max(p->prev_stime, cputime_sub(rtime, p->prev_utime));\n\n\t*ut = p->prev_utime;\n\t*st = p->prev_stime;\n}\n\n/*\n * Must be called with siglock held.\n */\nvoid thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tstruct signal_struct *sig = p->signal;\n\tstruct task_cputime cputime;\n\tcputime_t rtime, utime, total;\n\n\tthread_group_cputime(p, &cputime);\n\n\ttotal = cputime_add(cputime.utime, cputime.stime);\n\trtime = nsecs_to_cputime(cputime.sum_exec_runtime);\n\n\tif (total) {\n\t\tu64 temp = rtime;\n\n\t\ttemp *= cputime.utime;\n\t\tdo_div(temp, total);\n\t\tutime = (cputime_t)temp;\n\t} else\n\t\tutime = rtime;\n\n\tsig->prev_utime = max(sig->prev_utime, utime);\n\tsig->prev_stime = max(sig->prev_stime,\n\t\t\t      cputime_sub(rtime, sig->prev_utime));\n\n\t*ut = sig->prev_utime;\n\t*st = sig->prev_stime;\n}\n#endif\n\n/*\n * This function gets called by the timer code, with HZ frequency.\n * We call it with interrupts disabled.\n *\n * It also gets called by the fork code, when changing the parent's\n * timeslices.\n */\nvoid scheduler_tick(void)\n{\n\tint cpu = smp_processor_id();\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct task_struct *curr = rq->curr;\n\n\tsched_clock_tick();\n\n\traw_spin_lock(&rq->lock);\n\tupdate_rq_clock(rq);\n\tupdate_cpu_load_active(rq);\n\tcurr->sched_class->task_tick(rq, curr, 0);\n\traw_spin_unlock(&rq->lock);\n\n\tperf_event_task_tick();\n\n#ifdef CONFIG_SMP\n\trq->idle_at_tick = idle_cpu(cpu);\n\ttrigger_load_balance(rq, cpu);\n#endif\n}\n\nnotrace unsigned long get_parent_ip(unsigned long addr)\n{\n\tif (in_lock_functions(addr)) {\n\t\taddr = CALLER_ADDR2;\n\t\tif (in_lock_functions(addr))\n\t\t\taddr = CALLER_ADDR3;\n\t}\n\treturn addr;\n}\n\n#if defined(CONFIG_PREEMPT) && (defined(CONFIG_DEBUG_PREEMPT) || \\\n\t\t\t\tdefined(CONFIG_PREEMPT_TRACER))\n\nvoid __kprobes add_preempt_count(int val)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Underflow?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))\n\t\treturn;\n#endif\n\tpreempt_count() += val;\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Spinlock count overflowing soon?\n\t */\n\tDEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >=\n\t\t\t\tPREEMPT_MASK - 10);\n#endif\n\tif (preempt_count() == val)\n\t\ttrace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));\n}\nEXPORT_SYMBOL(add_preempt_count);\n\nvoid __kprobes sub_preempt_count(int val)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Underflow?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON(val > preempt_count()))\n\t\treturn;\n\t/*\n\t * Is the spinlock portion underflowing?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&\n\t\t\t!(preempt_count() & PREEMPT_MASK)))\n\t\treturn;\n#endif\n\n\tif (preempt_count() == val)\n\t\ttrace_preempt_on(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));\n\tpreempt_count() -= val;\n}\nEXPORT_SYMBOL(sub_preempt_count);\n\n#endif\n\n/*\n * Print scheduling while atomic bug:\n */\nstatic noinline void __schedule_bug(struct task_struct *prev)\n{\n\tstruct pt_regs *regs = get_irq_regs();\n\n\tprintk(KERN_ERR \"BUG: scheduling while atomic: %s/%d/0x%08x\\n\",\n\t\tprev->comm, prev->pid, preempt_count());\n\n\tdebug_show_held_locks(prev);\n\tprint_modules();\n\tif (irqs_disabled())\n\t\tprint_irqtrace_events(prev);\n\n\tif (regs)\n\t\tshow_regs(regs);\n\telse\n\t\tdump_stack();\n}\n\n/*\n * Various schedule()-time debugging checks and statistics:\n */\nstatic inline void schedule_debug(struct task_struct *prev)\n{\n\t/*\n\t * Test if we are atomic. Since do_exit() needs to call into\n\t * schedule() atomically, we ignore that path for now.\n\t * Otherwise, whine if we are scheduling when we should not be.\n\t */\n\tif (unlikely(in_atomic_preempt_off() && !prev->exit_state))\n\t\t__schedule_bug(prev);\n\n\tprofile_hit(SCHED_PROFILING, __builtin_return_address(0));\n\n\tschedstat_inc(this_rq(), sched_count);\n#ifdef CONFIG_SCHEDSTATS\n\tif (unlikely(prev->lock_depth >= 0)) {\n\t\tschedstat_inc(this_rq(), bkl_count);\n\t\tschedstat_inc(prev, sched_info.bkl_count);\n\t}\n#endif\n}\n\nstatic void put_prev_task(struct rq *rq, struct task_struct *prev)\n{\n\tif (prev->se.on_rq)\n\t\tupdate_rq_clock(rq);\n\trq->skip_clock_update = 0;\n\tprev->sched_class->put_prev_task(rq, prev);\n}\n\n/*\n * Pick up the highest-prio task:\n */\nstatic inline struct task_struct *\npick_next_task(struct rq *rq)\n{\n\tconst struct sched_class *class;\n\tstruct task_struct *p;\n\n\t/*\n\t * Optimization: we know that if all tasks are in\n\t * the fair class we can call that function directly:\n\t */\n\tif (likely(rq->nr_running == rq->cfs.nr_running)) {\n\t\tp = fair_sched_class.pick_next_task(rq);\n\t\tif (likely(p))\n\t\t\treturn p;\n\t}\n\n\tfor_each_class(class) {\n\t\tp = class->pick_next_task(rq);\n\t\tif (p)\n\t\t\treturn p;\n\t}\n\n\tBUG(); /* the idle class will always have a runnable task */\n}\n\n/*\n * schedule() is the main scheduler function.\n */\nasmlinkage void __sched schedule(void)\n{\n\tstruct task_struct *prev, *next;\n\tunsigned long *switch_count;\n\tstruct rq *rq;\n\tint cpu;\n\nneed_resched:\n\tpreempt_disable();\n\tcpu = smp_processor_id();\n\trq = cpu_rq(cpu);\n\trcu_note_context_switch(cpu);\n\tprev = rq->curr;\n\n\trelease_kernel_lock(prev);\nneed_resched_nonpreemptible:\n\n\tschedule_debug(prev);\n\n\tif (sched_feat(HRTICK))\n\t\thrtick_clear(rq);\n\n\traw_spin_lock_irq(&rq->lock);\n\tclear_tsk_need_resched(prev);\n\n\tswitch_count = &prev->nivcsw;\n\tif (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {\n\t\tif (unlikely(signal_pending_state(prev->state, prev))) {\n\t\t\tprev->state = TASK_RUNNING;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If a worker is going to sleep, notify and\n\t\t\t * ask workqueue whether it wants to wake up a\n\t\t\t * task to maintain concurrency.  If so, wake\n\t\t\t * up the task.\n\t\t\t */\n\t\t\tif (prev->flags & PF_WQ_WORKER) {\n\t\t\t\tstruct task_struct *to_wakeup;\n\n\t\t\t\tto_wakeup = wq_worker_sleeping(prev, cpu);\n\t\t\t\tif (to_wakeup)\n\t\t\t\t\ttry_to_wake_up_local(to_wakeup);\n\t\t\t}\n\t\t\tdeactivate_task(rq, prev, DEQUEUE_SLEEP);\n\t\t}\n\t\tswitch_count = &prev->nvcsw;\n\t}\n\n\tpre_schedule(rq, prev);\n\n\tif (unlikely(!rq->nr_running))\n\t\tidle_balance(cpu, rq);\n\n\tput_prev_task(rq, prev);\n\tnext = pick_next_task(rq);\n\n\tif (likely(prev != next)) {\n\t\tsched_info_switch(prev, next);\n\t\tperf_event_task_sched_out(prev, next);\n\n\t\trq->nr_switches++;\n\t\trq->curr = next;\n\t\t++*switch_count;\n\n\t\tcontext_switch(rq, prev, next); /* unlocks the rq */\n\t\t/*\n\t\t * The context switch have flipped the stack from under us\n\t\t * and restored the local variables which were saved when\n\t\t * this task called schedule() in the past. prev == current\n\t\t * is still correct, but it can be moved to another cpu/rq.\n\t\t */\n\t\tcpu = smp_processor_id();\n\t\trq = cpu_rq(cpu);\n\t} else\n\t\traw_spin_unlock_irq(&rq->lock);\n\n\tpost_schedule(rq);\n\n\tif (unlikely(reacquire_kernel_lock(prev)))\n\t\tgoto need_resched_nonpreemptible;\n\n\tpreempt_enable_no_resched();\n\tif (need_resched())\n\t\tgoto need_resched;\n}\nEXPORT_SYMBOL(schedule);\n\n#ifdef CONFIG_MUTEX_SPIN_ON_OWNER\n/*\n * Look out! \"owner\" is an entirely speculative pointer\n * access and not reliable.\n */\nint mutex_spin_on_owner(struct mutex *lock, struct thread_info *owner)\n{\n\tunsigned int cpu;\n\tstruct rq *rq;\n\n\tif (!sched_feat(OWNER_SPIN))\n\t\treturn 0;\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\n\t/*\n\t * Need to access the cpu field knowing that\n\t * DEBUG_PAGEALLOC could have unmapped it if\n\t * the mutex owner just released it and exited.\n\t */\n\tif (probe_kernel_address(&owner->cpu, cpu))\n\t\treturn 0;\n#else\n\tcpu = owner->cpu;\n#endif\n\n\t/*\n\t * Even if the access succeeded (likely case),\n\t * the cpu field may no longer be valid.\n\t */\n\tif (cpu >= nr_cpumask_bits)\n\t\treturn 0;\n\n\t/*\n\t * We need to validate that we can do a\n\t * get_cpu() and that we have the percpu area.\n\t */\n\tif (!cpu_online(cpu))\n\t\treturn 0;\n\n\trq = cpu_rq(cpu);\n\n\tfor (;;) {\n\t\t/*\n\t\t * Owner changed, break to re-assess state.\n\t\t */\n\t\tif (lock->owner != owner) {\n\t\t\t/*\n\t\t\t * If the lock has switched to a different owner,\n\t\t\t * we likely have heavy contention. Return 0 to quit\n\t\t\t * optimistic spinning and not contend further:\n\t\t\t */\n\t\t\tif (lock->owner)\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Is that owner really running on that cpu?\n\t\t */\n\t\tif (task_thread_info(rq->curr) != owner || need_resched())\n\t\t\treturn 0;\n\n\t\tcpu_relax();\n\t}\n\n\treturn 1;\n}\n#endif\n\n#ifdef CONFIG_PREEMPT\n/*\n * this is the entry point to schedule() from in-kernel preemption\n * off of preempt_enable. Kernel preemptions off return from interrupt\n * occur there and call schedule directly.\n */\nasmlinkage void __sched notrace preempt_schedule(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\n\t/*\n\t * If there is a non-zero preempt_count or interrupts are disabled,\n\t * we do not want to preempt the current task. Just return..\n\t */\n\tif (likely(ti->preempt_count || irqs_disabled()))\n\t\treturn;\n\n\tdo {\n\t\tadd_preempt_count_notrace(PREEMPT_ACTIVE);\n\t\tschedule();\n\t\tsub_preempt_count_notrace(PREEMPT_ACTIVE);\n\n\t\t/*\n\t\t * Check again in case we missed a preemption opportunity\n\t\t * between schedule and now.\n\t\t */\n\t\tbarrier();\n\t} while (need_resched());\n}\nEXPORT_SYMBOL(preempt_schedule);\n\n/*\n * this is the entry point to schedule() from kernel preemption\n * off of irq context.\n * Note, that this is called and return with irqs disabled. This will\n * protect us against recursive calling from irq.\n */\nasmlinkage void __sched preempt_schedule_irq(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\n\t/* Catch callers which need to be fixed */\n\tBUG_ON(ti->preempt_count || !irqs_disabled());\n\n\tdo {\n\t\tadd_preempt_count(PREEMPT_ACTIVE);\n\t\tlocal_irq_enable();\n\t\tschedule();\n\t\tlocal_irq_disable();\n\t\tsub_preempt_count(PREEMPT_ACTIVE);\n\n\t\t/*\n\t\t * Check again in case we missed a preemption opportunity\n\t\t * between schedule and now.\n\t\t */\n\t\tbarrier();\n\t} while (need_resched());\n}\n\n#endif /* CONFIG_PREEMPT */\n\nint default_wake_function(wait_queue_t *curr, unsigned mode, int wake_flags,\n\t\t\t  void *key)\n{\n\treturn try_to_wake_up(curr->private, mode, wake_flags);\n}\nEXPORT_SYMBOL(default_wake_function);\n\n/*\n * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just\n * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve\n * number) then we wake all the non-exclusive tasks and one exclusive task.\n *\n * There are circumstances in which we can try to wake a task which has already\n * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns\n * zero in this (rare) case, and we handle it by continuing to scan the queue.\n */\nstatic void __wake_up_common(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, int wake_flags, void *key)\n{\n\twait_queue_t *curr, *next;\n\n\tlist_for_each_entry_safe(curr, next, &q->task_list, task_list) {\n\t\tunsigned flags = curr->flags;\n\n\t\tif (curr->func(curr, mode, wake_flags, key) &&\n\t\t\t\t(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)\n\t\t\tbreak;\n\t}\n}\n\n/**\n * __wake_up - wake up threads blocked on a waitqueue.\n * @q: the waitqueue\n * @mode: which threads\n * @nr_exclusive: how many wake-one or wake-many threads to wake up\n * @key: is directly passed to the wakeup function\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid __wake_up(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, void *key)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__wake_up_common(q, mode, nr_exclusive, 0, key);\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\nEXPORT_SYMBOL(__wake_up);\n\n/*\n * Same as __wake_up but called with the spinlock in wait_queue_head_t held.\n */\nvoid __wake_up_locked(wait_queue_head_t *q, unsigned int mode)\n{\n\t__wake_up_common(q, mode, 1, 0, NULL);\n}\nEXPORT_SYMBOL_GPL(__wake_up_locked);\n\nvoid __wake_up_locked_key(wait_queue_head_t *q, unsigned int mode, void *key)\n{\n\t__wake_up_common(q, mode, 1, 0, key);\n}\n\n/**\n * __wake_up_sync_key - wake up threads blocked on a waitqueue.\n * @q: the waitqueue\n * @mode: which threads\n * @nr_exclusive: how many wake-one or wake-many threads to wake up\n * @key: opaque value to be passed to wakeup targets\n *\n * The sync wakeup differs that the waker knows that it will schedule\n * away soon, so while the target thread will be woken up, it will not\n * be migrated to another CPU - ie. the two threads are 'synchronized'\n * with each other. This can prevent needless bouncing between CPUs.\n *\n * On UP it can prevent extra preemption.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, void *key)\n{\n\tunsigned long flags;\n\tint wake_flags = WF_SYNC;\n\n\tif (unlikely(!q))\n\t\treturn;\n\n\tif (unlikely(!nr_exclusive))\n\t\twake_flags = 0;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__wake_up_common(q, mode, nr_exclusive, wake_flags, key);\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\nEXPORT_SYMBOL_GPL(__wake_up_sync_key);\n\n/*\n * __wake_up_sync - see __wake_up_sync_key()\n */\nvoid __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)\n{\n\t__wake_up_sync_key(q, mode, nr_exclusive, NULL);\n}\nEXPORT_SYMBOL_GPL(__wake_up_sync);\t/* For internal use only */\n\n/**\n * complete: - signals a single thread waiting on this completion\n * @x:  holds the state of this particular completion\n *\n * This will wake up a single thread waiting on this completion. Threads will be\n * awakened in the same order in which they were queued.\n *\n * See also complete_all(), wait_for_completion() and related routines.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid complete(struct completion *x)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tx->done++;\n\t__wake_up_common(&x->wait, TASK_NORMAL, 1, 0, NULL);\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n}\nEXPORT_SYMBOL(complete);\n\n/**\n * complete_all: - signals all threads waiting on this completion\n * @x:  holds the state of this particular completion\n *\n * This will wake up all threads waiting on this particular completion event.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid complete_all(struct completion *x)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tx->done += UINT_MAX/2;\n\t__wake_up_common(&x->wait, TASK_NORMAL, 0, 0, NULL);\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n}\nEXPORT_SYMBOL(complete_all);\n\nstatic inline long __sched\ndo_wait_for_common(struct completion *x, long timeout, int state)\n{\n\tif (!x->done) {\n\t\tDECLARE_WAITQUEUE(wait, current);\n\n\t\t__add_wait_queue_tail_exclusive(&x->wait, &wait);\n\t\tdo {\n\t\t\tif (signal_pending_state(state, current)) {\n\t\t\t\ttimeout = -ERESTARTSYS;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t__set_current_state(state);\n\t\t\tspin_unlock_irq(&x->wait.lock);\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tspin_lock_irq(&x->wait.lock);\n\t\t} while (!x->done && timeout);\n\t\t__remove_wait_queue(&x->wait, &wait);\n\t\tif (!x->done)\n\t\t\treturn timeout;\n\t}\n\tx->done--;\n\treturn timeout ?: 1;\n}\n\nstatic long __sched\nwait_for_common(struct completion *x, long timeout, int state)\n{\n\tmight_sleep();\n\n\tspin_lock_irq(&x->wait.lock);\n\ttimeout = do_wait_for_common(x, timeout, state);\n\tspin_unlock_irq(&x->wait.lock);\n\treturn timeout;\n}\n\n/**\n * wait_for_completion: - waits for completion of a task\n * @x:  holds the state of this particular completion\n *\n * This waits to be signaled for completion of a specific task. It is NOT\n * interruptible and there is no timeout.\n *\n * See also similar routines (i.e. wait_for_completion_timeout()) with timeout\n * and interrupt capability. Also see complete().\n */\nvoid __sched wait_for_completion(struct completion *x)\n{\n\twait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion);\n\n/**\n * wait_for_completion_timeout: - waits for completion of a task (w/timeout)\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be signaled or for a\n * specified timeout to expire. The timeout is in jiffies. It is not\n * interruptible.\n */\nunsigned long __sched\nwait_for_completion_timeout(struct completion *x, unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion_timeout);\n\n/**\n * wait_for_completion_interruptible: - waits for completion of a task (w/intr)\n * @x:  holds the state of this particular completion\n *\n * This waits for completion of a specific task to be signaled. It is\n * interruptible.\n */\nint __sched wait_for_completion_interruptible(struct completion *x)\n{\n\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_INTERRUPTIBLE);\n\tif (t == -ERESTARTSYS)\n\t\treturn t;\n\treturn 0;\n}\nEXPORT_SYMBOL(wait_for_completion_interruptible);\n\n/**\n * wait_for_completion_interruptible_timeout: - waits for completion (w/(to,intr))\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be signaled or for a\n * specified timeout to expire. It is interruptible. The timeout is in jiffies.\n */\nunsigned long __sched\nwait_for_completion_interruptible_timeout(struct completion *x,\n\t\t\t\t\t  unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_INTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion_interruptible_timeout);\n\n/**\n * wait_for_completion_killable: - waits for completion of a task (killable)\n * @x:  holds the state of this particular completion\n *\n * This waits to be signaled for completion of a specific task. It can be\n * interrupted by a kill signal.\n */\nint __sched wait_for_completion_killable(struct completion *x)\n{\n\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_KILLABLE);\n\tif (t == -ERESTARTSYS)\n\t\treturn t;\n\treturn 0;\n}\nEXPORT_SYMBOL(wait_for_completion_killable);\n\n/**\n * wait_for_completion_killable_timeout: - waits for completion of a task (w/(to,killable))\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be\n * signaled or for a specified timeout to expire. It can be\n * interrupted by a kill signal. The timeout is in jiffies.\n */\nunsigned long __sched\nwait_for_completion_killable_timeout(struct completion *x,\n\t\t\t\t     unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_KILLABLE);\n}\nEXPORT_SYMBOL(wait_for_completion_killable_timeout);\n\n/**\n *\ttry_wait_for_completion - try to decrement a completion without blocking\n *\t@x:\tcompletion structure\n *\n *\tReturns: 0 if a decrement cannot be done without blocking\n *\t\t 1 if a decrement succeeded.\n *\n *\tIf a completion is being used as a counting completion,\n *\tattempt to decrement the counter without blocking. This\n *\tenables us to avoid waiting if the resource the completion\n *\tis protecting is not available.\n */\nbool try_wait_for_completion(struct completion *x)\n{\n\tunsigned long flags;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tif (!x->done)\n\t\tret = 0;\n\telse\n\t\tx->done--;\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(try_wait_for_completion);\n\n/**\n *\tcompletion_done - Test to see if a completion has any waiters\n *\t@x:\tcompletion structure\n *\n *\tReturns: 0 if there are waiters (wait_for_completion() in progress)\n *\t\t 1 if there are no waiters.\n *\n */\nbool completion_done(struct completion *x)\n{\n\tunsigned long flags;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tif (!x->done)\n\t\tret = 0;\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(completion_done);\n\nstatic long __sched\nsleep_on_common(wait_queue_head_t *q, int state, long timeout)\n{\n\tunsigned long flags;\n\twait_queue_t wait;\n\n\tinit_waitqueue_entry(&wait, current);\n\n\t__set_current_state(state);\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__add_wait_queue(q, &wait);\n\tspin_unlock(&q->lock);\n\ttimeout = schedule_timeout(timeout);\n\tspin_lock_irq(&q->lock);\n\t__remove_wait_queue(q, &wait);\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\treturn timeout;\n}\n\nvoid __sched interruptible_sleep_on(wait_queue_head_t *q)\n{\n\tsleep_on_common(q, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n}\nEXPORT_SYMBOL(interruptible_sleep_on);\n\nlong __sched\ninterruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)\n{\n\treturn sleep_on_common(q, TASK_INTERRUPTIBLE, timeout);\n}\nEXPORT_SYMBOL(interruptible_sleep_on_timeout);\n\nvoid __sched sleep_on(wait_queue_head_t *q)\n{\n\tsleep_on_common(q, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n}\nEXPORT_SYMBOL(sleep_on);\n\nlong __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)\n{\n\treturn sleep_on_common(q, TASK_UNINTERRUPTIBLE, timeout);\n}\nEXPORT_SYMBOL(sleep_on_timeout);\n\n#ifdef CONFIG_RT_MUTEXES\n\n/*\n * rt_mutex_setprio - set the current priority of a task\n * @p: task\n * @prio: prio value (kernel-internal form)\n *\n * This function changes the 'effective' priority of a task. It does\n * not touch ->normal_prio like __setscheduler().\n *\n * Used by the rt_mutex code to implement priority inheritance logic.\n */\nvoid rt_mutex_setprio(struct task_struct *p, int prio)\n{\n\tunsigned long flags;\n\tint oldprio, on_rq, running;\n\tstruct rq *rq;\n\tconst struct sched_class *prev_class;\n\n\tBUG_ON(prio < 0 || prio > MAX_PRIO);\n\n\trq = task_rq_lock(p, &flags);\n\n\ttrace_sched_pi_setprio(p, prio);\n\toldprio = p->prio;\n\tprev_class = p->sched_class;\n\ton_rq = p->se.on_rq;\n\trunning = task_current(rq, p);\n\tif (on_rq)\n\t\tdequeue_task(rq, p, 0);\n\tif (running)\n\t\tp->sched_class->put_prev_task(rq, p);\n\n\tif (rt_prio(prio))\n\t\tp->sched_class = &rt_sched_class;\n\telse\n\t\tp->sched_class = &fair_sched_class;\n\n\tp->prio = prio;\n\n\tif (running)\n\t\tp->sched_class->set_curr_task(rq);\n\tif (on_rq) {\n\t\tenqueue_task(rq, p, oldprio < prio ? ENQUEUE_HEAD : 0);\n\n\t\tcheck_class_changed(rq, p, prev_class, oldprio, running);\n\t}\n\ttask_rq_unlock(rq, &flags);\n}\n\n#endif\n\nvoid set_user_nice(struct task_struct *p, long nice)\n{\n\tint old_prio, delta, on_rq;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\tif (TASK_NICE(p) == nice || nice < -20 || nice > 19)\n\t\treturn;\n\t/*\n\t * We have to be careful, if called from sys_setpriority(),\n\t * the task might be in the middle of scheduling on another CPU.\n\t */\n\trq = task_rq_lock(p, &flags);\n\t/*\n\t * The RT priorities are set via sched_setscheduler(), but we still\n\t * allow the 'normal' nice value to be set - but as expected\n\t * it wont have any effect on scheduling until the task is\n\t * SCHED_FIFO/SCHED_RR:\n\t */\n\tif (task_has_rt_policy(p)) {\n\t\tp->static_prio = NICE_TO_PRIO(nice);\n\t\tgoto out_unlock;\n\t}\n\ton_rq = p->se.on_rq;\n\tif (on_rq)\n\t\tdequeue_task(rq, p, 0);\n\n\tp->static_prio = NICE_TO_PRIO(nice);\n\tset_load_weight(p);\n\told_prio = p->prio;\n\tp->prio = effective_prio(p);\n\tdelta = p->prio - old_prio;\n\n\tif (on_rq) {\n\t\tenqueue_task(rq, p, 0);\n\t\t/*\n\t\t * If the task increased its priority or is running and\n\t\t * lowered its priority, then reschedule its CPU:\n\t\t */\n\t\tif (delta < 0 || (delta > 0 && task_running(rq, p)))\n\t\t\tresched_task(rq->curr);\n\t}\nout_unlock:\n\ttask_rq_unlock(rq, &flags);\n}\nEXPORT_SYMBOL(set_user_nice);\n\n/*\n * can_nice - check if a task can reduce its nice value\n * @p: task\n * @nice: nice value\n */\nint can_nice(const struct task_struct *p, const int nice)\n{\n\t/* convert nice value [19,-20] to rlimit style value [1,40] */\n\tint nice_rlim = 20 - nice;\n\n\treturn (nice_rlim <= task_rlimit(p, RLIMIT_NICE) ||\n\t\tcapable(CAP_SYS_NICE));\n}\n\n#ifdef __ARCH_WANT_SYS_NICE\n\n/*\n * sys_nice - change the priority of the current process.\n * @increment: priority increment\n *\n * sys_setpriority is a more generic, but much slower function that\n * does similar things.\n */\nSYSCALL_DEFINE1(nice, int, increment)\n{\n\tlong nice, retval;\n\n\t/*\n\t * Setpriority might change our priority at the same moment.\n\t * We don't have to worry. Conceptually one call occurs first\n\t * and we have a single winner.\n\t */\n\tif (increment < -40)\n\t\tincrement = -40;\n\tif (increment > 40)\n\t\tincrement = 40;\n\n\tnice = TASK_NICE(current) + increment;\n\tif (nice < -20)\n\t\tnice = -20;\n\tif (nice > 19)\n\t\tnice = 19;\n\n\tif (increment < 0 && !can_nice(current, nice))\n\t\treturn -EPERM;\n\n\tretval = security_task_setnice(current, nice);\n\tif (retval)\n\t\treturn retval;\n\n\tset_user_nice(current, nice);\n\treturn 0;\n}\n\n#endif\n\n/**\n * task_prio - return the priority value of a given task.\n * @p: the task in question.\n *\n * This is the priority value as seen by users in /proc.\n * RT tasks are offset by -200. Normal tasks are centered\n * around 0, value goes from -16 to +15.\n */\nint task_prio(const struct task_struct *p)\n{\n\treturn p->prio - MAX_RT_PRIO;\n}\n\n/**\n * task_nice - return the nice value of a given task.\n * @p: the task in question.\n */\nint task_nice(const struct task_struct *p)\n{\n\treturn TASK_NICE(p);\n}\nEXPORT_SYMBOL(task_nice);\n\n/**\n * idle_cpu - is a given cpu idle currently?\n * @cpu: the processor in question.\n */\nint idle_cpu(int cpu)\n{\n\treturn cpu_curr(cpu) == cpu_rq(cpu)->idle;\n}\n\n/**\n * idle_task - return the idle task for a given cpu.\n * @cpu: the processor in question.\n */\nstruct task_struct *idle_task(int cpu)\n{\n\treturn cpu_rq(cpu)->idle;\n}\n\n/**\n * find_process_by_pid - find a process with a matching PID value.\n * @pid: the pid in question.\n */\nstatic struct task_struct *find_process_by_pid(pid_t pid)\n{\n\treturn pid ? find_task_by_vpid(pid) : current;\n}\n\n/* Actually do priority change: must hold rq lock. */\nstatic void\n__setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)\n{\n\tBUG_ON(p->se.on_rq);\n\n\tp->policy = policy;\n\tp->rt_priority = prio;\n\tp->normal_prio = normal_prio(p);\n\t/* we are holding p->pi_lock already */\n\tp->prio = rt_mutex_getprio(p);\n\tif (rt_prio(p->prio))\n\t\tp->sched_class = &rt_sched_class;\n\telse\n\t\tp->sched_class = &fair_sched_class;\n\tset_load_weight(p);\n}\n\n/*\n * check the target process has a UID that matches the current process's\n */\nstatic bool check_same_owner(struct task_struct *p)\n{\n\tconst struct cred *cred = current_cred(), *pcred;\n\tbool match;\n\n\trcu_read_lock();\n\tpcred = __task_cred(p);\n\tmatch = (cred->euid == pcred->euid ||\n\t\t cred->euid == pcred->uid);\n\trcu_read_unlock();\n\treturn match;\n}\n\nstatic int __sched_setscheduler(struct task_struct *p, int policy,\n\t\t\t\tstruct sched_param *param, bool user)\n{\n\tint retval, oldprio, oldpolicy = -1, on_rq, running;\n\tunsigned long flags;\n\tconst struct sched_class *prev_class;\n\tstruct rq *rq;\n\tint reset_on_fork;\n\n\t/* may grab non-irq protected spin_locks */\n\tBUG_ON(in_interrupt());\nrecheck:\n\t/* double check policy once rq lock held */\n\tif (policy < 0) {\n\t\treset_on_fork = p->sched_reset_on_fork;\n\t\tpolicy = oldpolicy = p->policy;\n\t} else {\n\t\treset_on_fork = !!(policy & SCHED_RESET_ON_FORK);\n\t\tpolicy &= ~SCHED_RESET_ON_FORK;\n\n\t\tif (policy != SCHED_FIFO && policy != SCHED_RR &&\n\t\t\t\tpolicy != SCHED_NORMAL && policy != SCHED_BATCH &&\n\t\t\t\tpolicy != SCHED_IDLE)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Valid priorities for SCHED_FIFO and SCHED_RR are\n\t * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL,\n\t * SCHED_BATCH and SCHED_IDLE is 0.\n\t */\n\tif (param->sched_priority < 0 ||\n\t    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||\n\t    (!p->mm && param->sched_priority > MAX_RT_PRIO-1))\n\t\treturn -EINVAL;\n\tif (rt_policy(policy) != (param->sched_priority != 0))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Allow unprivileged RT tasks to decrease priority:\n\t */\n\tif (user && !capable(CAP_SYS_NICE)) {\n\t\tif (rt_policy(policy)) {\n\t\t\tunsigned long rlim_rtprio =\n\t\t\t\t\ttask_rlimit(p, RLIMIT_RTPRIO);\n\n\t\t\t/* can't set/change the rt policy */\n\t\t\tif (policy != p->policy && !rlim_rtprio)\n\t\t\t\treturn -EPERM;\n\n\t\t\t/* can't increase priority */\n\t\t\tif (param->sched_priority > p->rt_priority &&\n\t\t\t    param->sched_priority > rlim_rtprio)\n\t\t\t\treturn -EPERM;\n\t\t}\n\t\t/*\n\t\t * Like positive nice levels, dont allow tasks to\n\t\t * move out of SCHED_IDLE either:\n\t\t */\n\t\tif (p->policy == SCHED_IDLE && policy != SCHED_IDLE)\n\t\t\treturn -EPERM;\n\n\t\t/* can't change other user's priorities */\n\t\tif (!check_same_owner(p))\n\t\t\treturn -EPERM;\n\n\t\t/* Normal users shall not reset the sched_reset_on_fork flag */\n\t\tif (p->sched_reset_on_fork && !reset_on_fork)\n\t\t\treturn -EPERM;\n\t}\n\n\tif (user) {\n\t\tretval = security_task_setscheduler(p);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\t/*\n\t * make sure no PI-waiters arrive (or leave) while we are\n\t * changing the priority of the task:\n\t */\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\t/*\n\t * To be able to change p->policy safely, the apropriate\n\t * runqueue lock must be held.\n\t */\n\trq = __task_rq_lock(p);\n\n\t/*\n\t * Changing the policy of the stop threads its a very bad idea\n\t */\n\tif (p == rq->stop) {\n\t\t__task_rq_unlock(rq);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\t\treturn -EINVAL;\n\t}\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tif (user) {\n\t\t/*\n\t\t * Do not allow realtime tasks into groups that have no runtime\n\t\t * assigned.\n\t\t */\n\t\tif (rt_bandwidth_enabled() && rt_policy(policy) &&\n\t\t\t\ttask_group(p)->rt_bandwidth.rt_runtime == 0) {\n\t\t\t__task_rq_unlock(rq);\n\t\t\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n#endif\n\n\t/* recheck policy now with rq lock held */\n\tif (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {\n\t\tpolicy = oldpolicy = -1;\n\t\t__task_rq_unlock(rq);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\t\tgoto recheck;\n\t}\n\ton_rq = p->se.on_rq;\n\trunning = task_current(rq, p);\n\tif (on_rq)\n\t\tdeactivate_task(rq, p, 0);\n\tif (running)\n\t\tp->sched_class->put_prev_task(rq, p);\n\n\tp->sched_reset_on_fork = reset_on_fork;\n\n\toldprio = p->prio;\n\tprev_class = p->sched_class;\n\t__setscheduler(rq, p, policy, param->sched_priority);\n\n\tif (running)\n\t\tp->sched_class->set_curr_task(rq);\n\tif (on_rq) {\n\t\tactivate_task(rq, p, 0);\n\n\t\tcheck_class_changed(rq, p, prev_class, oldprio, running);\n\t}\n\t__task_rq_unlock(rq);\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\n\trt_mutex_adjust_pi(p);\n\n\treturn 0;\n}\n\n/**\n * sched_setscheduler - change the scheduling policy and/or RT priority of a thread.\n * @p: the task in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n *\n * NOTE that the task may be already dead.\n */\nint sched_setscheduler(struct task_struct *p, int policy,\n\t\t       struct sched_param *param)\n{\n\treturn __sched_setscheduler(p, policy, param, true);\n}\nEXPORT_SYMBOL_GPL(sched_setscheduler);\n\n/**\n * sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace.\n * @p: the task in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n *\n * Just like sched_setscheduler, only don't bother checking if the\n * current context has permission.  For example, this is needed in\n * stop_machine(): we create temporary high priority worker threads,\n * but our caller might not have that capability.\n */\nint sched_setscheduler_nocheck(struct task_struct *p, int policy,\n\t\t\t       struct sched_param *param)\n{\n\treturn __sched_setscheduler(p, policy, param, false);\n}\n\nstatic int\ndo_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)\n{\n\tstruct sched_param lparam;\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!param || pid < 0)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&lparam, param, sizeof(struct sched_param)))\n\t\treturn -EFAULT;\n\n\trcu_read_lock();\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (p != NULL)\n\t\tretval = sched_setscheduler(p, policy, &lparam);\n\trcu_read_unlock();\n\n\treturn retval;\n}\n\n/**\n * sys_sched_setscheduler - set/change the scheduler policy and RT priority\n * @pid: the pid in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n */\nSYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy,\n\t\tstruct sched_param __user *, param)\n{\n\t/* negative values for policy are not valid */\n\tif (policy < 0)\n\t\treturn -EINVAL;\n\n\treturn do_sched_setscheduler(pid, policy, param);\n}\n\n/**\n * sys_sched_setparam - set/change the RT priority of a thread\n * @pid: the pid in question.\n * @param: structure containing the new RT priority.\n */\nSYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param)\n{\n\treturn do_sched_setscheduler(pid, -1, param);\n}\n\n/**\n * sys_sched_getscheduler - get the policy (scheduling class) of a thread\n * @pid: the pid in question.\n */\nSYSCALL_DEFINE1(sched_getscheduler, pid_t, pid)\n{\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (pid < 0)\n\t\treturn -EINVAL;\n\n\tretval = -ESRCH;\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tif (p) {\n\t\tretval = security_task_getscheduler(p);\n\t\tif (!retval)\n\t\t\tretval = p->policy\n\t\t\t\t| (p->sched_reset_on_fork ? SCHED_RESET_ON_FORK : 0);\n\t}\n\trcu_read_unlock();\n\treturn retval;\n}\n\n/**\n * sys_sched_getparam - get the RT priority of a thread\n * @pid: the pid in question.\n * @param: structure containing the RT priority.\n */\nSYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param)\n{\n\tstruct sched_param lp;\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!param || pid < 0)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tretval = -ESRCH;\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tlp.sched_priority = p->rt_priority;\n\trcu_read_unlock();\n\n\t/*\n\t * This one might sleep, we cannot do it with a spinlock held ...\n\t */\n\tretval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;\n\n\treturn retval;\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\nlong sched_setaffinity(pid_t pid, const struct cpumask *in_mask)\n{\n\tcpumask_var_t cpus_allowed, new_mask;\n\tstruct task_struct *p;\n\tint retval;\n\n\tget_online_cpus();\n\trcu_read_lock();\n\n\tp = find_process_by_pid(pid);\n\tif (!p) {\n\t\trcu_read_unlock();\n\t\tput_online_cpus();\n\t\treturn -ESRCH;\n\t}\n\n\t/* Prevent p going away */\n\tget_task_struct(p);\n\trcu_read_unlock();\n\n\tif (!alloc_cpumask_var(&cpus_allowed, GFP_KERNEL)) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_put_task;\n\t}\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_free_cpus_allowed;\n\t}\n\tretval = -EPERM;\n\tif (!check_same_owner(p) && !capable(CAP_SYS_NICE))\n\t\tgoto out_unlock;\n\n\tretval = security_task_setscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tcpuset_cpus_allowed(p, cpus_allowed);\n\tcpumask_and(new_mask, in_mask, cpus_allowed);\nagain:\n\tretval = set_cpus_allowed_ptr(p, new_mask);\n\n\tif (!retval) {\n\t\tcpuset_cpus_allowed(p, cpus_allowed);\n\t\tif (!cpumask_subset(new_mask, cpus_allowed)) {\n\t\t\t/*\n\t\t\t * We must have raced with a concurrent cpuset\n\t\t\t * update. Just reset the cpus_allowed to the\n\t\t\t * cpuset's cpus_allowed\n\t\t\t */\n\t\t\tcpumask_copy(new_mask, cpus_allowed);\n\t\t\tgoto again;\n\t\t}\n\t}\nout_unlock:\n\tfree_cpumask_var(new_mask);\nout_free_cpus_allowed:\n\tfree_cpumask_var(cpus_allowed);\nout_put_task:\n\tput_task_struct(p);\n\tput_online_cpus();\n\treturn retval;\n}\n\nstatic int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,\n\t\t\t     struct cpumask *new_mask)\n{\n\tif (len < cpumask_size())\n\t\tcpumask_clear(new_mask);\n\telse if (len > cpumask_size())\n\t\tlen = cpumask_size();\n\n\treturn copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;\n}\n\n/**\n * sys_sched_setaffinity - set the cpu affinity of a process\n * @pid: pid of the process\n * @len: length in bytes of the bitmask pointed to by user_mask_ptr\n * @user_mask_ptr: user-space pointer to the new cpu mask\n */\nSYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len,\n\t\tunsigned long __user *, user_mask_ptr)\n{\n\tcpumask_var_t new_mask;\n\tint retval;\n\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tretval = get_user_cpu_mask(user_mask_ptr, len, new_mask);\n\tif (retval == 0)\n\t\tretval = sched_setaffinity(pid, new_mask);\n\tfree_cpumask_var(new_mask);\n\treturn retval;\n}\n\nlong sched_getaffinity(pid_t pid, struct cpumask *mask)\n{\n\tstruct task_struct *p;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint retval;\n\n\tget_online_cpus();\n\trcu_read_lock();\n\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\trq = task_rq_lock(p, &flags);\n\tcpumask_and(mask, &p->cpus_allowed, cpu_online_mask);\n\ttask_rq_unlock(rq, &flags);\n\nout_unlock:\n\trcu_read_unlock();\n\tput_online_cpus();\n\n\treturn retval;\n}\n\n/**\n * sys_sched_getaffinity - get the cpu affinity of a process\n * @pid: pid of the process\n * @len: length in bytes of the bitmask pointed to by user_mask_ptr\n * @user_mask_ptr: user-space pointer to hold the current cpu mask\n */\nSYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len,\n\t\tunsigned long __user *, user_mask_ptr)\n{\n\tint ret;\n\tcpumask_var_t mask;\n\n\tif ((len * BITS_PER_BYTE) < nr_cpu_ids)\n\t\treturn -EINVAL;\n\tif (len & (sizeof(unsigned long)-1))\n\t\treturn -EINVAL;\n\n\tif (!alloc_cpumask_var(&mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tret = sched_getaffinity(pid, mask);\n\tif (ret == 0) {\n\t\tsize_t retlen = min_t(size_t, len, cpumask_size());\n\n\t\tif (copy_to_user(user_mask_ptr, mask, retlen))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = retlen;\n\t}\n\tfree_cpumask_var(mask);\n\n\treturn ret;\n}\n\n/**\n * sys_sched_yield - yield the current processor to other threads.\n *\n * This function yields the current CPU to other tasks. If there are no\n * other threads running on this CPU then this function will return.\n */\nSYSCALL_DEFINE0(sched_yield)\n{\n\tstruct rq *rq = this_rq_lock();\n\n\tschedstat_inc(rq, yld_count);\n\tcurrent->sched_class->yield_task(rq);\n\n\t/*\n\t * Since we are going to call schedule() anyway, there's\n\t * no need to preempt or enable interrupts:\n\t */\n\t__release(rq->lock);\n\tspin_release(&rq->lock.dep_map, 1, _THIS_IP_);\n\tdo_raw_spin_unlock(&rq->lock);\n\tpreempt_enable_no_resched();\n\n\tschedule();\n\n\treturn 0;\n}\n\nstatic inline int should_resched(void)\n{\n\treturn need_resched() && !(preempt_count() & PREEMPT_ACTIVE);\n}\n\nstatic void __cond_resched(void)\n{\n\tadd_preempt_count(PREEMPT_ACTIVE);\n\tschedule();\n\tsub_preempt_count(PREEMPT_ACTIVE);\n}\n\nint __sched _cond_resched(void)\n{\n\tif (should_resched()) {\n\t\t__cond_resched();\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(_cond_resched);\n\n/*\n * __cond_resched_lock() - if a reschedule is pending, drop the given lock,\n * call schedule, and on return reacquire the lock.\n *\n * This works OK both with and without CONFIG_PREEMPT. We do strange low-level\n * operations here to prevent schedule() from being called twice (once via\n * spin_unlock(), once by hand).\n */\nint __cond_resched_lock(spinlock_t *lock)\n{\n\tint resched = should_resched();\n\tint ret = 0;\n\n\tlockdep_assert_held(lock);\n\n\tif (spin_needbreak(lock) || resched) {\n\t\tspin_unlock(lock);\n\t\tif (resched)\n\t\t\t__cond_resched();\n\t\telse\n\t\t\tcpu_relax();\n\t\tret = 1;\n\t\tspin_lock(lock);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__cond_resched_lock);\n\nint __sched __cond_resched_softirq(void)\n{\n\tBUG_ON(!in_softirq());\n\n\tif (should_resched()) {\n\t\tlocal_bh_enable();\n\t\t__cond_resched();\n\t\tlocal_bh_disable();\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__cond_resched_softirq);\n\n/**\n * yield - yield the current processor to other threads.\n *\n * This is a shortcut for kernel-space yielding - it marks the\n * thread runnable and calls sys_sched_yield().\n */\nvoid __sched yield(void)\n{\n\tset_current_state(TASK_RUNNING);\n\tsys_sched_yield();\n}\nEXPORT_SYMBOL(yield);\n\n/*\n * This task is about to go to sleep on IO. Increment rq->nr_iowait so\n * that process accounting knows that this is a task in IO wait state.\n */\nvoid __sched io_schedule(void)\n{\n\tstruct rq *rq = raw_rq();\n\n\tdelayacct_blkio_start();\n\tatomic_inc(&rq->nr_iowait);\n\tcurrent->in_iowait = 1;\n\tschedule();\n\tcurrent->in_iowait = 0;\n\tatomic_dec(&rq->nr_iowait);\n\tdelayacct_blkio_end();\n}\nEXPORT_SYMBOL(io_schedule);\n\nlong __sched io_schedule_timeout(long timeout)\n{\n\tstruct rq *rq = raw_rq();\n\tlong ret;\n\n\tdelayacct_blkio_start();\n\tatomic_inc(&rq->nr_iowait);\n\tcurrent->in_iowait = 1;\n\tret = schedule_timeout(timeout);\n\tcurrent->in_iowait = 0;\n\tatomic_dec(&rq->nr_iowait);\n\tdelayacct_blkio_end();\n\treturn ret;\n}\n\n/**\n * sys_sched_get_priority_max - return maximum RT priority.\n * @policy: scheduling class.\n *\n * this syscall returns the maximum rt_priority that can be used\n * by a given scheduling class.\n */\nSYSCALL_DEFINE1(sched_get_priority_max, int, policy)\n{\n\tint ret = -EINVAL;\n\n\tswitch (policy) {\n\tcase SCHED_FIFO:\n\tcase SCHED_RR:\n\t\tret = MAX_USER_RT_PRIO-1;\n\t\tbreak;\n\tcase SCHED_NORMAL:\n\tcase SCHED_BATCH:\n\tcase SCHED_IDLE:\n\t\tret = 0;\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/**\n * sys_sched_get_priority_min - return minimum RT priority.\n * @policy: scheduling class.\n *\n * this syscall returns the minimum rt_priority that can be used\n * by a given scheduling class.\n */\nSYSCALL_DEFINE1(sched_get_priority_min, int, policy)\n{\n\tint ret = -EINVAL;\n\n\tswitch (policy) {\n\tcase SCHED_FIFO:\n\tcase SCHED_RR:\n\t\tret = 1;\n\t\tbreak;\n\tcase SCHED_NORMAL:\n\tcase SCHED_BATCH:\n\tcase SCHED_IDLE:\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\n/**\n * sys_sched_rr_get_interval - return the default timeslice of a process.\n * @pid: pid of the process.\n * @interval: userspace pointer to the timeslice value.\n *\n * this syscall writes the default timeslice value of a given process\n * into the user-space timespec buffer. A value of '0' means infinity.\n */\nSYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid,\n\t\tstruct timespec __user *, interval)\n{\n\tstruct task_struct *p;\n\tunsigned int time_slice;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint retval;\n\tstruct timespec t;\n\n\tif (pid < 0)\n\t\treturn -EINVAL;\n\n\tretval = -ESRCH;\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\trq = task_rq_lock(p, &flags);\n\ttime_slice = p->sched_class->get_rr_interval(rq, p);\n\ttask_rq_unlock(rq, &flags);\n\n\trcu_read_unlock();\n\tjiffies_to_timespec(time_slice, &t);\n\tretval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;\n\treturn retval;\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\nstatic const char stat_nam[] = TASK_STATE_TO_CHAR_STR;\n\nvoid sched_show_task(struct task_struct *p)\n{\n\tunsigned long free = 0;\n\tunsigned state;\n\n\tstate = p->state ? __ffs(p->state) + 1 : 0;\n\tprintk(KERN_INFO \"%-13.13s %c\", p->comm,\n\t\tstate < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');\n#if BITS_PER_LONG == 32\n\tif (state == TASK_RUNNING)\n\t\tprintk(KERN_CONT \" running  \");\n\telse\n\t\tprintk(KERN_CONT \" %08lx \", thread_saved_pc(p));\n#else\n\tif (state == TASK_RUNNING)\n\t\tprintk(KERN_CONT \"  running task    \");\n\telse\n\t\tprintk(KERN_CONT \" %016lx \", thread_saved_pc(p));\n#endif\n#ifdef CONFIG_DEBUG_STACK_USAGE\n\tfree = stack_not_used(p);\n#endif\n\tprintk(KERN_CONT \"%5lu %5d %6d 0x%08lx\\n\", free,\n\t\ttask_pid_nr(p), task_pid_nr(p->real_parent),\n\t\t(unsigned long)task_thread_info(p)->flags);\n\n\tshow_stack(p, NULL);\n}\n\nvoid show_state_filter(unsigned long state_filter)\n{\n\tstruct task_struct *g, *p;\n\n#if BITS_PER_LONG == 32\n\tprintk(KERN_INFO\n\t\t\"  task                PC stack   pid father\\n\");\n#else\n\tprintk(KERN_INFO\n\t\t\"  task                        PC stack   pid father\\n\");\n#endif\n\tread_lock(&tasklist_lock);\n\tdo_each_thread(g, p) {\n\t\t/*\n\t\t * reset the NMI-timeout, listing all files on a slow\n\t\t * console might take alot of time:\n\t\t */\n\t\ttouch_nmi_watchdog();\n\t\tif (!state_filter || (p->state & state_filter))\n\t\t\tsched_show_task(p);\n\t} while_each_thread(g, p);\n\n\ttouch_all_softlockup_watchdogs();\n\n#ifdef CONFIG_SCHED_DEBUG\n\tsysrq_sched_debug_show();\n#endif\n\tread_unlock(&tasklist_lock);\n\t/*\n\t * Only show locks if all tasks are dumped:\n\t */\n\tif (!state_filter)\n\t\tdebug_show_all_locks();\n}\n\nvoid __cpuinit init_idle_bootup_task(struct task_struct *idle)\n{\n\tidle->sched_class = &idle_sched_class;\n}\n\n/**\n * init_idle - set up an idle thread for a given CPU\n * @idle: task in question\n * @cpu: cpu the idle task belongs to\n *\n * NOTE: this function does not set the idle thread's NEED_RESCHED\n * flag, to make booting more robust.\n */\nvoid __cpuinit init_idle(struct task_struct *idle, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\n\t__sched_fork(idle);\n\tidle->state = TASK_RUNNING;\n\tidle->se.exec_start = sched_clock();\n\n\tcpumask_copy(&idle->cpus_allowed, cpumask_of(cpu));\n\t/*\n\t * We're having a chicken and egg problem, even though we are\n\t * holding rq->lock, the cpu isn't yet set to this cpu so the\n\t * lockdep check in task_group() will fail.\n\t *\n\t * Similar case to sched_fork(). / Alternatively we could\n\t * use task_rq_lock() here and obtain the other rq->lock.\n\t *\n\t * Silence PROVE_RCU\n\t */\n\trcu_read_lock();\n\t__set_task_cpu(idle, cpu);\n\trcu_read_unlock();\n\n\trq->curr = rq->idle = idle;\n#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)\n\tidle->oncpu = 1;\n#endif\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\t/* Set the preempt count _outside_ the spinlocks! */\n#if defined(CONFIG_PREEMPT)\n\ttask_thread_info(idle)->preempt_count = (idle->lock_depth >= 0);\n#else\n\ttask_thread_info(idle)->preempt_count = 0;\n#endif\n\t/*\n\t * The idle tasks have their own, simple scheduling class:\n\t */\n\tidle->sched_class = &idle_sched_class;\n\tftrace_graph_init_task(idle);\n}\n\n/*\n * In a system that switches off the HZ timer nohz_cpu_mask\n * indicates which cpus entered this state. This is used\n * in the rcu update to wait only for active cpus. For system\n * which do not switch off the HZ timer nohz_cpu_mask should\n * always be CPU_BITS_NONE.\n */\ncpumask_var_t nohz_cpu_mask;\n\n/*\n * Increase the granularity value when there are more CPUs,\n * because with more CPUs the 'effective latency' as visible\n * to users decreases. But the relationship is not linear,\n * so pick a second-best guess by going with the log2 of the\n * number of CPUs.\n *\n * This idea comes from the SD scheduler of Con Kolivas:\n */\nstatic int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}\n\nstatic void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n\tSET_SYSCTL(sched_shares_ratelimit);\n#undef SET_SYSCTL\n}\n\nstatic inline void sched_init_granularity(void)\n{\n\tupdate_sysctl();\n}\n\n#ifdef CONFIG_SMP\n/*\n * This is how migration works:\n *\n * 1) we invoke migration_cpu_stop() on the target CPU using\n *    stop_one_cpu().\n * 2) stopper starts to run (implicitly forcing the migrated thread\n *    off the CPU)\n * 3) it checks whether the migrated task is still in the wrong runqueue.\n * 4) if it's in the wrong runqueue then the migration thread removes\n *    it and puts it into the right queue.\n * 5) stopper completes and stop_one_cpu() returns and the migration\n *    is done.\n */\n\n/*\n * Change a given task's CPU affinity. Migrate the thread to a\n * proper CPU and schedule it away if the CPU it's executing on\n * is removed from the allowed bitmask.\n *\n * NOTE: the caller must have a valid reference to the task, the\n * task must not exit() & deallocate itself prematurely. The\n * call is not atomic; no spinlocks may be held.\n */\nint set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tunsigned int dest_cpu;\n\tint ret = 0;\n\n\t/*\n\t * Serialize against TASK_WAKING so that ttwu() and wunt() can\n\t * drop the rq->lock and still rely on ->cpus_allowed.\n\t */\nagain:\n\twhile (task_is_waking(p))\n\t\tcpu_relax();\n\trq = task_rq_lock(p, &flags);\n\tif (task_is_waking(p)) {\n\t\ttask_rq_unlock(rq, &flags);\n\t\tgoto again;\n\t}\n\n\tif (!cpumask_intersects(new_mask, cpu_active_mask)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (unlikely((p->flags & PF_THREAD_BOUND) && p != current &&\n\t\t     !cpumask_equal(&p->cpus_allowed, new_mask))) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (p->sched_class->set_cpus_allowed)\n\t\tp->sched_class->set_cpus_allowed(p, new_mask);\n\telse {\n\t\tcpumask_copy(&p->cpus_allowed, new_mask);\n\t\tp->rt.nr_cpus_allowed = cpumask_weight(new_mask);\n\t}\n\n\t/* Can the task run on the task's current CPU? If so, we're done */\n\tif (cpumask_test_cpu(task_cpu(p), new_mask))\n\t\tgoto out;\n\n\tdest_cpu = cpumask_any_and(cpu_active_mask, new_mask);\n\tif (migrate_task(p, dest_cpu)) {\n\t\tstruct migration_arg arg = { p, dest_cpu };\n\t\t/* Need help from migration thread: drop lock and wait. */\n\t\ttask_rq_unlock(rq, &flags);\n\t\tstop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);\n\t\ttlb_migrate_finish(p->mm);\n\t\treturn 0;\n\t}\nout:\n\ttask_rq_unlock(rq, &flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);\n\n/*\n * Move (not current) task off this cpu, onto dest cpu. We're doing\n * this because either it can't run here any more (set_cpus_allowed()\n * away from this CPU, or CPU going down), or because we're\n * attempting to rebalance this task on exec (sched_exec).\n *\n * So we race with normal scheduler movements, but that's OK, as long\n * as the task is no longer on this CPU.\n *\n * Returns non-zero if task was successfully migrated.\n */\nstatic int __migrate_task(struct task_struct *p, int src_cpu, int dest_cpu)\n{\n\tstruct rq *rq_dest, *rq_src;\n\tint ret = 0;\n\n\tif (unlikely(!cpu_active(dest_cpu)))\n\t\treturn ret;\n\n\trq_src = cpu_rq(src_cpu);\n\trq_dest = cpu_rq(dest_cpu);\n\n\tdouble_rq_lock(rq_src, rq_dest);\n\t/* Already moved. */\n\tif (task_cpu(p) != src_cpu)\n\t\tgoto done;\n\t/* Affinity changed (again). */\n\tif (!cpumask_test_cpu(dest_cpu, &p->cpus_allowed))\n\t\tgoto fail;\n\n\t/*\n\t * If we're not on a rq, the next wake-up will ensure we're\n\t * placed properly.\n\t */\n\tif (p->se.on_rq) {\n\t\tdeactivate_task(rq_src, p, 0);\n\t\tset_task_cpu(p, dest_cpu);\n\t\tactivate_task(rq_dest, p, 0);\n\t\tcheck_preempt_curr(rq_dest, p, 0);\n\t}\ndone:\n\tret = 1;\nfail:\n\tdouble_rq_unlock(rq_src, rq_dest);\n\treturn ret;\n}\n\n/*\n * migration_cpu_stop - this will be executed by a highprio stopper thread\n * and performs thread migration by bumping thread off CPU then\n * 'pushing' onto another runqueue.\n */\nstatic int migration_cpu_stop(void *data)\n{\n\tstruct migration_arg *arg = data;\n\n\t/*\n\t * The original target cpu might have gone down and we might\n\t * be on another cpu but it doesn't matter.\n\t */\n\tlocal_irq_disable();\n\t__migrate_task(arg->task, raw_smp_processor_id(), arg->dest_cpu);\n\tlocal_irq_enable();\n\treturn 0;\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n/*\n * Figure out where task on dead CPU should go, use force if necessary.\n */\nvoid move_task_off_dead_cpu(int dead_cpu, struct task_struct *p)\n{\n\tstruct rq *rq = cpu_rq(dead_cpu);\n\tint needs_cpu, uninitialized_var(dest_cpu);\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\n\traw_spin_lock(&rq->lock);\n\tneeds_cpu = (task_cpu(p) == dead_cpu) && (p->state != TASK_WAKING);\n\tif (needs_cpu)\n\t\tdest_cpu = select_fallback_rq(dead_cpu, p);\n\traw_spin_unlock(&rq->lock);\n\t/*\n\t * It can only fail if we race with set_cpus_allowed(),\n\t * in the racer should migrate the task anyway.\n\t */\n\tif (needs_cpu)\n\t\t__migrate_task(p, dead_cpu, dest_cpu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * While a dead CPU has no uninterruptible tasks queued at this point,\n * it might still have a nonzero ->nr_uninterruptible counter, because\n * for performance reasons the counter is not stricly tracking tasks to\n * their home CPUs. So we just add the counter to another CPU's counter,\n * to keep the global sum constant after CPU-down:\n */\nstatic void migrate_nr_uninterruptible(struct rq *rq_src)\n{\n\tstruct rq *rq_dest = cpu_rq(cpumask_any(cpu_active_mask));\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tdouble_rq_lock(rq_src, rq_dest);\n\trq_dest->nr_uninterruptible += rq_src->nr_uninterruptible;\n\trq_src->nr_uninterruptible = 0;\n\tdouble_rq_unlock(rq_src, rq_dest);\n\tlocal_irq_restore(flags);\n}\n\n/* Run through task list and migrate tasks from the dead cpu. */\nstatic void migrate_live_tasks(int src_cpu)\n{\n\tstruct task_struct *p, *t;\n\n\tread_lock(&tasklist_lock);\n\n\tdo_each_thread(t, p) {\n\t\tif (p == current)\n\t\t\tcontinue;\n\n\t\tif (task_cpu(p) == src_cpu)\n\t\t\tmove_task_off_dead_cpu(src_cpu, p);\n\t} while_each_thread(t, p);\n\n\tread_unlock(&tasklist_lock);\n}\n\n/*\n * Schedules idle task to be the next runnable task on current CPU.\n * It does so by boosting its priority to highest possible.\n * Used by CPU offline code.\n */\nvoid sched_idle_next(void)\n{\n\tint this_cpu = smp_processor_id();\n\tstruct rq *rq = cpu_rq(this_cpu);\n\tstruct task_struct *p = rq->idle;\n\tunsigned long flags;\n\n\t/* cpu has to be offline */\n\tBUG_ON(cpu_online(this_cpu));\n\n\t/*\n\t * Strictly not necessary since rest of the CPUs are stopped by now\n\t * and interrupts disabled on the current cpu.\n\t */\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\n\t__setscheduler(rq, p, SCHED_FIFO, MAX_RT_PRIO-1);\n\n\tactivate_task(rq, p, 0);\n\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n}\n\n/*\n * Ensures that the idle task is using init_mm right before its cpu goes\n * offline.\n */\nvoid idle_task_exit(void)\n{\n\tstruct mm_struct *mm = current->active_mm;\n\n\tBUG_ON(cpu_online(smp_processor_id()));\n\n\tif (mm != &init_mm)\n\t\tswitch_mm(mm, &init_mm, current);\n\tmmdrop(mm);\n}\n\n/* called under rq->lock with disabled interrupts */\nstatic void migrate_dead(unsigned int dead_cpu, struct task_struct *p)\n{\n\tstruct rq *rq = cpu_rq(dead_cpu);\n\n\t/* Must be exiting, otherwise would be on tasklist. */\n\tBUG_ON(!p->exit_state);\n\n\t/* Cannot have done final schedule yet: would have vanished. */\n\tBUG_ON(p->state == TASK_DEAD);\n\n\tget_task_struct(p);\n\n\t/*\n\t * Drop lock around migration; if someone else moves it,\n\t * that's OK. No task can be added to this CPU, so iteration is\n\t * fine.\n\t */\n\traw_spin_unlock_irq(&rq->lock);\n\tmove_task_off_dead_cpu(dead_cpu, p);\n\traw_spin_lock_irq(&rq->lock);\n\n\tput_task_struct(p);\n}\n\n/* release_task() removes task from tasklist, so we won't find dead tasks. */\nstatic void migrate_dead_tasks(unsigned int dead_cpu)\n{\n\tstruct rq *rq = cpu_rq(dead_cpu);\n\tstruct task_struct *next;\n\n\tfor ( ; ; ) {\n\t\tif (!rq->nr_running)\n\t\t\tbreak;\n\t\tnext = pick_next_task(rq);\n\t\tif (!next)\n\t\t\tbreak;\n\t\tnext->sched_class->put_prev_task(rq, next);\n\t\tmigrate_dead(dead_cpu, next);\n\n\t}\n}\n\n/*\n * remove the tasks which were accounted by rq from calc_load_tasks.\n */\nstatic void calc_global_load_remove(struct rq *rq)\n{\n\tatomic_long_sub(rq->calc_load_active, &calc_load_tasks);\n\trq->calc_load_active = 0;\n}\n#endif /* CONFIG_HOTPLUG_CPU */\n\n#if defined(CONFIG_SCHED_DEBUG) && defined(CONFIG_SYSCTL)\n\nstatic struct ctl_table sd_ctl_dir[] = {\n\t{\n\t\t.procname\t= \"sched_domain\",\n\t\t.mode\t\t= 0555,\n\t},\n\t{}\n};\n\nstatic struct ctl_table sd_ctl_root[] = {\n\t{\n\t\t.procname\t= \"kernel\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= sd_ctl_dir,\n\t},\n\t{}\n};\n\nstatic struct ctl_table *sd_alloc_ctl_entry(int n)\n{\n\tstruct ctl_table *entry =\n\t\tkcalloc(n, sizeof(struct ctl_table), GFP_KERNEL);\n\n\treturn entry;\n}\n\nstatic void sd_free_ctl_entry(struct ctl_table **tablep)\n{\n\tstruct ctl_table *entry;\n\n\t/*\n\t * In the intermediate directories, both the child directory and\n\t * procname are dynamically allocated and could fail but the mode\n\t * will always be set. In the lowest directory the names are\n\t * static strings and all have proc handlers.\n\t */\n\tfor (entry = *tablep; entry->mode; entry++) {\n\t\tif (entry->child)\n\t\t\tsd_free_ctl_entry(&entry->child);\n\t\tif (entry->proc_handler == NULL)\n\t\t\tkfree(entry->procname);\n\t}\n\n\tkfree(*tablep);\n\t*tablep = NULL;\n}\n\nstatic void\nset_table_entry(struct ctl_table *entry,\n\t\tconst char *procname, void *data, int maxlen,\n\t\tmode_t mode, proc_handler *proc_handler)\n{\n\tentry->procname = procname;\n\tentry->data = data;\n\tentry->maxlen = maxlen;\n\tentry->mode = mode;\n\tentry->proc_handler = proc_handler;\n}\n\nstatic struct ctl_table *\nsd_alloc_ctl_domain_table(struct sched_domain *sd)\n{\n\tstruct ctl_table *table = sd_alloc_ctl_entry(13);\n\n\tif (table == NULL)\n\t\treturn NULL;\n\n\tset_table_entry(&table[0], \"min_interval\", &sd->min_interval,\n\t\tsizeof(long), 0644, proc_doulongvec_minmax);\n\tset_table_entry(&table[1], \"max_interval\", &sd->max_interval,\n\t\tsizeof(long), 0644, proc_doulongvec_minmax);\n\tset_table_entry(&table[2], \"busy_idx\", &sd->busy_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[3], \"idle_idx\", &sd->idle_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[4], \"newidle_idx\", &sd->newidle_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[5], \"wake_idx\", &sd->wake_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[6], \"forkexec_idx\", &sd->forkexec_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[7], \"busy_factor\", &sd->busy_factor,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[8], \"imbalance_pct\", &sd->imbalance_pct,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[9], \"cache_nice_tries\",\n\t\t&sd->cache_nice_tries,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[10], \"flags\", &sd->flags,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[11], \"name\", sd->name,\n\t\tCORENAME_MAX_SIZE, 0444, proc_dostring);\n\t/* &table[12] is terminator */\n\n\treturn table;\n}\n\nstatic ctl_table *sd_alloc_ctl_cpu_table(int cpu)\n{\n\tstruct ctl_table *entry, *table;\n\tstruct sched_domain *sd;\n\tint domain_num = 0, i;\n\tchar buf[32];\n\n\tfor_each_domain(cpu, sd)\n\t\tdomain_num++;\n\tentry = table = sd_alloc_ctl_entry(domain_num + 1);\n\tif (table == NULL)\n\t\treturn NULL;\n\n\ti = 0;\n\tfor_each_domain(cpu, sd) {\n\t\tsnprintf(buf, 32, \"domain%d\", i);\n\t\tentry->procname = kstrdup(buf, GFP_KERNEL);\n\t\tentry->mode = 0555;\n\t\tentry->child = sd_alloc_ctl_domain_table(sd);\n\t\tentry++;\n\t\ti++;\n\t}\n\treturn table;\n}\n\nstatic struct ctl_table_header *sd_sysctl_header;\nstatic void register_sched_domain_sysctl(void)\n{\n\tint i, cpu_num = num_possible_cpus();\n\tstruct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);\n\tchar buf[32];\n\n\tWARN_ON(sd_ctl_dir[0].child);\n\tsd_ctl_dir[0].child = entry;\n\n\tif (entry == NULL)\n\t\treturn;\n\n\tfor_each_possible_cpu(i) {\n\t\tsnprintf(buf, 32, \"cpu%d\", i);\n\t\tentry->procname = kstrdup(buf, GFP_KERNEL);\n\t\tentry->mode = 0555;\n\t\tentry->child = sd_alloc_ctl_cpu_table(i);\n\t\tentry++;\n\t}\n\n\tWARN_ON(sd_sysctl_header);\n\tsd_sysctl_header = register_sysctl_table(sd_ctl_root);\n}\n\n/* may be called multiple times per register */\nstatic void unregister_sched_domain_sysctl(void)\n{\n\tif (sd_sysctl_header)\n\t\tunregister_sysctl_table(sd_sysctl_header);\n\tsd_sysctl_header = NULL;\n\tif (sd_ctl_dir[0].child)\n\t\tsd_free_ctl_entry(&sd_ctl_dir[0].child);\n}\n#else\nstatic void register_sched_domain_sysctl(void)\n{\n}\nstatic void unregister_sched_domain_sysctl(void)\n{\n}\n#endif\n\nstatic void set_rq_online(struct rq *rq)\n{\n\tif (!rq->online) {\n\t\tconst struct sched_class *class;\n\n\t\tcpumask_set_cpu(rq->cpu, rq->rd->online);\n\t\trq->online = 1;\n\n\t\tfor_each_class(class) {\n\t\t\tif (class->rq_online)\n\t\t\t\tclass->rq_online(rq);\n\t\t}\n\t}\n}\n\nstatic void set_rq_offline(struct rq *rq)\n{\n\tif (rq->online) {\n\t\tconst struct sched_class *class;\n\n\t\tfor_each_class(class) {\n\t\t\tif (class->rq_offline)\n\t\t\t\tclass->rq_offline(rq);\n\t\t}\n\n\t\tcpumask_clear_cpu(rq->cpu, rq->rd->online);\n\t\trq->online = 0;\n\t}\n}\n\n/*\n * migration_call - callback that gets triggered when a CPU is added.\n * Here we can start up the necessary migration thread for the new CPU.\n */\nstatic int __cpuinit\nmigration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)\n{\n\tint cpu = (long)hcpu;\n\tunsigned long flags;\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tswitch (action) {\n\n\tcase CPU_UP_PREPARE:\n\tcase CPU_UP_PREPARE_FROZEN:\n\t\trq->calc_load_update = calc_load_update;\n\t\tbreak;\n\n\tcase CPU_ONLINE:\n\tcase CPU_ONLINE_FROZEN:\n\t\t/* Update our root-domain */\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->rd) {\n\t\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));\n\n\t\t\tset_rq_online(rq);\n\t\t}\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t\tbreak;\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tcase CPU_DEAD:\n\tcase CPU_DEAD_FROZEN:\n\t\tmigrate_live_tasks(cpu);\n\t\t/* Idle task back to normal (off runqueue, low prio) */\n\t\traw_spin_lock_irq(&rq->lock);\n\t\tdeactivate_task(rq, rq->idle, 0);\n\t\t__setscheduler(rq, rq->idle, SCHED_NORMAL, 0);\n\t\trq->idle->sched_class = &idle_sched_class;\n\t\tmigrate_dead_tasks(cpu);\n\t\traw_spin_unlock_irq(&rq->lock);\n\t\tmigrate_nr_uninterruptible(rq);\n\t\tBUG_ON(rq->nr_running != 0);\n\t\tcalc_global_load_remove(rq);\n\t\tbreak;\n\n\tcase CPU_DYING:\n\tcase CPU_DYING_FROZEN:\n\t\t/* Update our root-domain */\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->rd) {\n\t\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));\n\t\t\tset_rq_offline(rq);\n\t\t}\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t\tbreak;\n#endif\n\t}\n\treturn NOTIFY_OK;\n}\n\n/*\n * Register at high priority so that task migration (migrate_all_tasks)\n * happens before everything else.  This has to be lower priority than\n * the notifier in the perf_event subsystem, though.\n */\nstatic struct notifier_block __cpuinitdata migration_notifier = {\n\t.notifier_call = migration_call,\n\t.priority = CPU_PRI_MIGRATION,\n};\n\nstatic int __cpuinit sched_cpu_active(struct notifier_block *nfb,\n\t\t\t\t      unsigned long action, void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_ONLINE:\n\tcase CPU_DOWN_FAILED:\n\t\tset_cpu_active((long)hcpu, true);\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int __cpuinit sched_cpu_inactive(struct notifier_block *nfb,\n\t\t\t\t\tunsigned long action, void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_DOWN_PREPARE:\n\t\tset_cpu_active((long)hcpu, false);\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int __init migration_init(void)\n{\n\tvoid *cpu = (void *)(long)smp_processor_id();\n\tint err;\n\n\t/* Initialize migration for the boot CPU */\n\terr = migration_call(&migration_notifier, CPU_UP_PREPARE, cpu);\n\tBUG_ON(err == NOTIFY_BAD);\n\tmigration_call(&migration_notifier, CPU_ONLINE, cpu);\n\tregister_cpu_notifier(&migration_notifier);\n\n\t/* Register cpu active notifiers */\n\tcpu_notifier(sched_cpu_active, CPU_PRI_SCHED_ACTIVE);\n\tcpu_notifier(sched_cpu_inactive, CPU_PRI_SCHED_INACTIVE);\n\n\treturn 0;\n}\nearly_initcall(migration_init);\n#endif\n\n#ifdef CONFIG_SMP\n\n#ifdef CONFIG_SCHED_DEBUG\n\nstatic __read_mostly int sched_domain_debug_enabled;\n\nstatic int __init sched_domain_debug_setup(char *str)\n{\n\tsched_domain_debug_enabled = 1;\n\n\treturn 0;\n}\nearly_param(\"sched_debug\", sched_domain_debug_setup);\n\nstatic int sched_domain_debug_one(struct sched_domain *sd, int cpu, int level,\n\t\t\t\t  struct cpumask *groupmask)\n{\n\tstruct sched_group *group = sd->groups;\n\tchar str[256];\n\n\tcpulist_scnprintf(str, sizeof(str), sched_domain_span(sd));\n\tcpumask_clear(groupmask);\n\n\tprintk(KERN_DEBUG \"%*s domain %d: \", level, \"\", level);\n\n\tif (!(sd->flags & SD_LOAD_BALANCE)) {\n\t\tprintk(\"does not load-balance\\n\");\n\t\tif (sd->parent)\n\t\t\tprintk(KERN_ERR \"ERROR: !SD_LOAD_BALANCE domain\"\n\t\t\t\t\t\" has parent\");\n\t\treturn -1;\n\t}\n\n\tprintk(KERN_CONT \"span %s level %s\\n\", str, sd->name);\n\n\tif (!cpumask_test_cpu(cpu, sched_domain_span(sd))) {\n\t\tprintk(KERN_ERR \"ERROR: domain->span does not contain \"\n\t\t\t\t\"CPU%d\\n\", cpu);\n\t}\n\tif (!cpumask_test_cpu(cpu, sched_group_cpus(group))) {\n\t\tprintk(KERN_ERR \"ERROR: domain->groups does not contain\"\n\t\t\t\t\" CPU%d\\n\", cpu);\n\t}\n\n\tprintk(KERN_DEBUG \"%*s groups:\", level + 1, \"\");\n\tdo {\n\t\tif (!group) {\n\t\t\tprintk(\"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: group is NULL\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!group->cpu_power) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: domain->cpu_power not \"\n\t\t\t\t\t\"set\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!cpumask_weight(sched_group_cpus(group))) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: empty group\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cpumask_intersects(groupmask, sched_group_cpus(group))) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: repeated CPUs\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tcpumask_or(groupmask, groupmask, sched_group_cpus(group));\n\n\t\tcpulist_scnprintf(str, sizeof(str), sched_group_cpus(group));\n\n\t\tprintk(KERN_CONT \" %s\", str);\n\t\tif (group->cpu_power != SCHED_LOAD_SCALE) {\n\t\t\tprintk(KERN_CONT \" (cpu_power = %d)\",\n\t\t\t\tgroup->cpu_power);\n\t\t}\n\n\t\tgroup = group->next;\n\t} while (group != sd->groups);\n\tprintk(KERN_CONT \"\\n\");\n\n\tif (!cpumask_equal(sched_domain_span(sd), groupmask))\n\t\tprintk(KERN_ERR \"ERROR: groups don't span domain->span\\n\");\n\n\tif (sd->parent &&\n\t    !cpumask_subset(groupmask, sched_domain_span(sd->parent)))\n\t\tprintk(KERN_ERR \"ERROR: parent span is not a superset \"\n\t\t\t\"of domain->span\\n\");\n\treturn 0;\n}\n\nstatic void sched_domain_debug(struct sched_domain *sd, int cpu)\n{\n\tcpumask_var_t groupmask;\n\tint level = 0;\n\n\tif (!sched_domain_debug_enabled)\n\t\treturn;\n\n\tif (!sd) {\n\t\tprintk(KERN_DEBUG \"CPU%d attaching NULL sched-domain.\\n\", cpu);\n\t\treturn;\n\t}\n\n\tprintk(KERN_DEBUG \"CPU%d attaching sched-domain:\\n\", cpu);\n\n\tif (!alloc_cpumask_var(&groupmask, GFP_KERNEL)) {\n\t\tprintk(KERN_DEBUG \"Cannot load-balance (out of memory)\\n\");\n\t\treturn;\n\t}\n\n\tfor (;;) {\n\t\tif (sched_domain_debug_one(sd, cpu, level, groupmask))\n\t\t\tbreak;\n\t\tlevel++;\n\t\tsd = sd->parent;\n\t\tif (!sd)\n\t\t\tbreak;\n\t}\n\tfree_cpumask_var(groupmask);\n}\n#else /* !CONFIG_SCHED_DEBUG */\n# define sched_domain_debug(sd, cpu) do { } while (0)\n#endif /* CONFIG_SCHED_DEBUG */\n\nstatic int sd_degenerate(struct sched_domain *sd)\n{\n\tif (cpumask_weight(sched_domain_span(sd)) == 1)\n\t\treturn 1;\n\n\t/* Following flags need at least 2 groups */\n\tif (sd->flags & (SD_LOAD_BALANCE |\n\t\t\t SD_BALANCE_NEWIDLE |\n\t\t\t SD_BALANCE_FORK |\n\t\t\t SD_BALANCE_EXEC |\n\t\t\t SD_SHARE_CPUPOWER |\n\t\t\t SD_SHARE_PKG_RESOURCES)) {\n\t\tif (sd->groups != sd->groups->next)\n\t\t\treturn 0;\n\t}\n\n\t/* Following flags don't use groups */\n\tif (sd->flags & (SD_WAKE_AFFINE))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int\nsd_parent_degenerate(struct sched_domain *sd, struct sched_domain *parent)\n{\n\tunsigned long cflags = sd->flags, pflags = parent->flags;\n\n\tif (sd_degenerate(parent))\n\t\treturn 1;\n\n\tif (!cpumask_equal(sched_domain_span(sd), sched_domain_span(parent)))\n\t\treturn 0;\n\n\t/* Flags needing groups don't count if only 1 group in parent */\n\tif (parent->groups == parent->groups->next) {\n\t\tpflags &= ~(SD_LOAD_BALANCE |\n\t\t\t\tSD_BALANCE_NEWIDLE |\n\t\t\t\tSD_BALANCE_FORK |\n\t\t\t\tSD_BALANCE_EXEC |\n\t\t\t\tSD_SHARE_CPUPOWER |\n\t\t\t\tSD_SHARE_PKG_RESOURCES);\n\t\tif (nr_node_ids == 1)\n\t\t\tpflags &= ~SD_SERIALIZE;\n\t}\n\tif (~cflags & pflags)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic void free_rootdomain(struct root_domain *rd)\n{\n\tsynchronize_sched();\n\n\tcpupri_cleanup(&rd->cpupri);\n\n\tfree_cpumask_var(rd->rto_mask);\n\tfree_cpumask_var(rd->online);\n\tfree_cpumask_var(rd->span);\n\tkfree(rd);\n}\n\nstatic void rq_attach_root(struct rq *rq, struct root_domain *rd)\n{\n\tstruct root_domain *old_rd = NULL;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\n\tif (rq->rd) {\n\t\told_rd = rq->rd;\n\n\t\tif (cpumask_test_cpu(rq->cpu, old_rd->online))\n\t\t\tset_rq_offline(rq);\n\n\t\tcpumask_clear_cpu(rq->cpu, old_rd->span);\n\n\t\t/*\n\t\t * If we dont want to free the old_rt yet then\n\t\t * set old_rd to NULL to skip the freeing later\n\t\t * in this function:\n\t\t */\n\t\tif (!atomic_dec_and_test(&old_rd->refcount))\n\t\t\told_rd = NULL;\n\t}\n\n\tatomic_inc(&rd->refcount);\n\trq->rd = rd;\n\n\tcpumask_set_cpu(rq->cpu, rd->span);\n\tif (cpumask_test_cpu(rq->cpu, cpu_active_mask))\n\t\tset_rq_online(rq);\n\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\tif (old_rd)\n\t\tfree_rootdomain(old_rd);\n}\n\nstatic int init_rootdomain(struct root_domain *rd)\n{\n\tmemset(rd, 0, sizeof(*rd));\n\n\tif (!alloc_cpumask_var(&rd->span, GFP_KERNEL))\n\t\tgoto out;\n\tif (!alloc_cpumask_var(&rd->online, GFP_KERNEL))\n\t\tgoto free_span;\n\tif (!alloc_cpumask_var(&rd->rto_mask, GFP_KERNEL))\n\t\tgoto free_online;\n\n\tif (cpupri_init(&rd->cpupri) != 0)\n\t\tgoto free_rto_mask;\n\treturn 0;\n\nfree_rto_mask:\n\tfree_cpumask_var(rd->rto_mask);\nfree_online:\n\tfree_cpumask_var(rd->online);\nfree_span:\n\tfree_cpumask_var(rd->span);\nout:\n\treturn -ENOMEM;\n}\n\nstatic void init_defrootdomain(void)\n{\n\tinit_rootdomain(&def_root_domain);\n\n\tatomic_set(&def_root_domain.refcount, 1);\n}\n\nstatic struct root_domain *alloc_rootdomain(void)\n{\n\tstruct root_domain *rd;\n\n\trd = kmalloc(sizeof(*rd), GFP_KERNEL);\n\tif (!rd)\n\t\treturn NULL;\n\n\tif (init_rootdomain(rd) != 0) {\n\t\tkfree(rd);\n\t\treturn NULL;\n\t}\n\n\treturn rd;\n}\n\n/*\n * Attach the domain 'sd' to 'cpu' as its base domain. Callers must\n * hold the hotplug lock.\n */\nstatic void\ncpu_attach_domain(struct sched_domain *sd, struct root_domain *rd, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct sched_domain *tmp;\n\n\tfor (tmp = sd; tmp; tmp = tmp->parent)\n\t\ttmp->span_weight = cpumask_weight(sched_domain_span(tmp));\n\n\t/* Remove the sched domains which do not contribute to scheduling. */\n\tfor (tmp = sd; tmp; ) {\n\t\tstruct sched_domain *parent = tmp->parent;\n\t\tif (!parent)\n\t\t\tbreak;\n\n\t\tif (sd_parent_degenerate(tmp, parent)) {\n\t\t\ttmp->parent = parent->parent;\n\t\t\tif (parent->parent)\n\t\t\t\tparent->parent->child = tmp;\n\t\t} else\n\t\t\ttmp = tmp->parent;\n\t}\n\n\tif (sd && sd_degenerate(sd)) {\n\t\tsd = sd->parent;\n\t\tif (sd)\n\t\t\tsd->child = NULL;\n\t}\n\n\tsched_domain_debug(sd, cpu);\n\n\trq_attach_root(rq, rd);\n\trcu_assign_pointer(rq->sd, sd);\n}\n\n/* cpus with isolated domains */\nstatic cpumask_var_t cpu_isolated_map;\n\n/* Setup the mask of cpus configured for isolated domains */\nstatic int __init isolated_cpu_setup(char *str)\n{\n\talloc_bootmem_cpumask_var(&cpu_isolated_map);\n\tcpulist_parse(str, cpu_isolated_map);\n\treturn 1;\n}\n\n__setup(\"isolcpus=\", isolated_cpu_setup);\n\n/*\n * init_sched_build_groups takes the cpumask we wish to span, and a pointer\n * to a function which identifies what group(along with sched group) a CPU\n * belongs to. The return value of group_fn must be a >= 0 and < nr_cpu_ids\n * (due to the fact that we keep track of groups covered with a struct cpumask).\n *\n * init_sched_build_groups will build a circular linked list of the groups\n * covered by the given span, and will set each group's ->cpumask correctly,\n * and ->cpu_power to 0.\n */\nstatic void\ninit_sched_build_groups(const struct cpumask *span,\n\t\t\tconst struct cpumask *cpu_map,\n\t\t\tint (*group_fn)(int cpu, const struct cpumask *cpu_map,\n\t\t\t\t\tstruct sched_group **sg,\n\t\t\t\t\tstruct cpumask *tmpmask),\n\t\t\tstruct cpumask *covered, struct cpumask *tmpmask)\n{\n\tstruct sched_group *first = NULL, *last = NULL;\n\tint i;\n\n\tcpumask_clear(covered);\n\n\tfor_each_cpu(i, span) {\n\t\tstruct sched_group *sg;\n\t\tint group = group_fn(i, cpu_map, &sg, tmpmask);\n\t\tint j;\n\n\t\tif (cpumask_test_cpu(i, covered))\n\t\t\tcontinue;\n\n\t\tcpumask_clear(sched_group_cpus(sg));\n\t\tsg->cpu_power = 0;\n\n\t\tfor_each_cpu(j, span) {\n\t\t\tif (group_fn(j, cpu_map, NULL, tmpmask) != group)\n\t\t\t\tcontinue;\n\n\t\t\tcpumask_set_cpu(j, covered);\n\t\t\tcpumask_set_cpu(j, sched_group_cpus(sg));\n\t\t}\n\t\tif (!first)\n\t\t\tfirst = sg;\n\t\tif (last)\n\t\t\tlast->next = sg;\n\t\tlast = sg;\n\t}\n\tlast->next = first;\n}\n\n#define SD_NODES_PER_DOMAIN 16\n\n#ifdef CONFIG_NUMA\n\n/**\n * find_next_best_node - find the next node to include in a sched_domain\n * @node: node whose sched_domain we're building\n * @used_nodes: nodes already in the sched_domain\n *\n * Find the next node to include in a given scheduling domain. Simply\n * finds the closest node not already in the @used_nodes map.\n *\n * Should use nodemask_t.\n */\nstatic int find_next_best_node(int node, nodemask_t *used_nodes)\n{\n\tint i, n, val, min_val, best_node = 0;\n\n\tmin_val = INT_MAX;\n\n\tfor (i = 0; i < nr_node_ids; i++) {\n\t\t/* Start at @node */\n\t\tn = (node + i) % nr_node_ids;\n\n\t\tif (!nr_cpus_node(n))\n\t\t\tcontinue;\n\n\t\t/* Skip already used nodes */\n\t\tif (node_isset(n, *used_nodes))\n\t\t\tcontinue;\n\n\t\t/* Simple min distance search */\n\t\tval = node_distance(node, n);\n\n\t\tif (val < min_val) {\n\t\t\tmin_val = val;\n\t\t\tbest_node = n;\n\t\t}\n\t}\n\n\tnode_set(best_node, *used_nodes);\n\treturn best_node;\n}\n\n/**\n * sched_domain_node_span - get a cpumask for a node's sched_domain\n * @node: node whose cpumask we're constructing\n * @span: resulting cpumask\n *\n * Given a node, construct a good cpumask for its sched_domain to span. It\n * should be one that prevents unnecessary balancing, but also spreads tasks\n * out optimally.\n */\nstatic void sched_domain_node_span(int node, struct cpumask *span)\n{\n\tnodemask_t used_nodes;\n\tint i;\n\n\tcpumask_clear(span);\n\tnodes_clear(used_nodes);\n\n\tcpumask_or(span, span, cpumask_of_node(node));\n\tnode_set(node, used_nodes);\n\n\tfor (i = 1; i < SD_NODES_PER_DOMAIN; i++) {\n\t\tint next_node = find_next_best_node(node, &used_nodes);\n\n\t\tcpumask_or(span, span, cpumask_of_node(next_node));\n\t}\n}\n#endif /* CONFIG_NUMA */\n\nint sched_smt_power_savings = 0, sched_mc_power_savings = 0;\n\n/*\n * The cpus mask in sched_group and sched_domain hangs off the end.\n *\n * ( See the the comments in include/linux/sched.h:struct sched_group\n *   and struct sched_domain. )\n */\nstruct static_sched_group {\n\tstruct sched_group sg;\n\tDECLARE_BITMAP(cpus, CONFIG_NR_CPUS);\n};\n\nstruct static_sched_domain {\n\tstruct sched_domain sd;\n\tDECLARE_BITMAP(span, CONFIG_NR_CPUS);\n};\n\nstruct s_data {\n#ifdef CONFIG_NUMA\n\tint\t\t\tsd_allnodes;\n\tcpumask_var_t\t\tdomainspan;\n\tcpumask_var_t\t\tcovered;\n\tcpumask_var_t\t\tnotcovered;\n#endif\n\tcpumask_var_t\t\tnodemask;\n\tcpumask_var_t\t\tthis_sibling_map;\n\tcpumask_var_t\t\tthis_core_map;\n\tcpumask_var_t\t\tthis_book_map;\n\tcpumask_var_t\t\tsend_covered;\n\tcpumask_var_t\t\ttmpmask;\n\tstruct sched_group\t**sched_group_nodes;\n\tstruct root_domain\t*rd;\n};\n\nenum s_alloc {\n\tsa_sched_groups = 0,\n\tsa_rootdomain,\n\tsa_tmpmask,\n\tsa_send_covered,\n\tsa_this_book_map,\n\tsa_this_core_map,\n\tsa_this_sibling_map,\n\tsa_nodemask,\n\tsa_sched_group_nodes,\n#ifdef CONFIG_NUMA\n\tsa_notcovered,\n\tsa_covered,\n\tsa_domainspan,\n#endif\n\tsa_none,\n};\n\n/*\n * SMT sched-domains:\n */\n#ifdef CONFIG_SCHED_SMT\nstatic DEFINE_PER_CPU(struct static_sched_domain, cpu_domains);\nstatic DEFINE_PER_CPU(struct static_sched_group, sched_groups);\n\nstatic int\ncpu_to_cpu_group(int cpu, const struct cpumask *cpu_map,\n\t\t struct sched_group **sg, struct cpumask *unused)\n{\n\tif (sg)\n\t\t*sg = &per_cpu(sched_groups, cpu).sg;\n\treturn cpu;\n}\n#endif /* CONFIG_SCHED_SMT */\n\n/*\n * multi-core sched-domains:\n */\n#ifdef CONFIG_SCHED_MC\nstatic DEFINE_PER_CPU(struct static_sched_domain, core_domains);\nstatic DEFINE_PER_CPU(struct static_sched_group, sched_group_core);\n\nstatic int\ncpu_to_core_group(int cpu, const struct cpumask *cpu_map,\n\t\t  struct sched_group **sg, struct cpumask *mask)\n{\n\tint group;\n#ifdef CONFIG_SCHED_SMT\n\tcpumask_and(mask, topology_thread_cpumask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#else\n\tgroup = cpu;\n#endif\n\tif (sg)\n\t\t*sg = &per_cpu(sched_group_core, group).sg;\n\treturn group;\n}\n#endif /* CONFIG_SCHED_MC */\n\n/*\n * book sched-domains:\n */\n#ifdef CONFIG_SCHED_BOOK\nstatic DEFINE_PER_CPU(struct static_sched_domain, book_domains);\nstatic DEFINE_PER_CPU(struct static_sched_group, sched_group_book);\n\nstatic int\ncpu_to_book_group(int cpu, const struct cpumask *cpu_map,\n\t\t  struct sched_group **sg, struct cpumask *mask)\n{\n\tint group = cpu;\n#ifdef CONFIG_SCHED_MC\n\tcpumask_and(mask, cpu_coregroup_mask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#elif defined(CONFIG_SCHED_SMT)\n\tcpumask_and(mask, topology_thread_cpumask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#endif\n\tif (sg)\n\t\t*sg = &per_cpu(sched_group_book, group).sg;\n\treturn group;\n}\n#endif /* CONFIG_SCHED_BOOK */\n\nstatic DEFINE_PER_CPU(struct static_sched_domain, phys_domains);\nstatic DEFINE_PER_CPU(struct static_sched_group, sched_group_phys);\n\nstatic int\ncpu_to_phys_group(int cpu, const struct cpumask *cpu_map,\n\t\t  struct sched_group **sg, struct cpumask *mask)\n{\n\tint group;\n#ifdef CONFIG_SCHED_BOOK\n\tcpumask_and(mask, cpu_book_mask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#elif defined(CONFIG_SCHED_MC)\n\tcpumask_and(mask, cpu_coregroup_mask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#elif defined(CONFIG_SCHED_SMT)\n\tcpumask_and(mask, topology_thread_cpumask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#else\n\tgroup = cpu;\n#endif\n\tif (sg)\n\t\t*sg = &per_cpu(sched_group_phys, group).sg;\n\treturn group;\n}\n\n#ifdef CONFIG_NUMA\n/*\n * The init_sched_build_groups can't handle what we want to do with node\n * groups, so roll our own. Now each node has its own list of groups which\n * gets dynamically allocated.\n */\nstatic DEFINE_PER_CPU(struct static_sched_domain, node_domains);\nstatic struct sched_group ***sched_group_nodes_bycpu;\n\nstatic DEFINE_PER_CPU(struct static_sched_domain, allnodes_domains);\nstatic DEFINE_PER_CPU(struct static_sched_group, sched_group_allnodes);\n\nstatic int cpu_to_allnodes_group(int cpu, const struct cpumask *cpu_map,\n\t\t\t\t struct sched_group **sg,\n\t\t\t\t struct cpumask *nodemask)\n{\n\tint group;\n\n\tcpumask_and(nodemask, cpumask_of_node(cpu_to_node(cpu)), cpu_map);\n\tgroup = cpumask_first(nodemask);\n\n\tif (sg)\n\t\t*sg = &per_cpu(sched_group_allnodes, group).sg;\n\treturn group;\n}\n\nstatic void init_numa_sched_groups_power(struct sched_group *group_head)\n{\n\tstruct sched_group *sg = group_head;\n\tint j;\n\n\tif (!sg)\n\t\treturn;\n\tdo {\n\t\tfor_each_cpu(j, sched_group_cpus(sg)) {\n\t\t\tstruct sched_domain *sd;\n\n\t\t\tsd = &per_cpu(phys_domains, j).sd;\n\t\t\tif (j != group_first_cpu(sd->groups)) {\n\t\t\t\t/*\n\t\t\t\t * Only add \"power\" once for each\n\t\t\t\t * physical package.\n\t\t\t\t */\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tsg->cpu_power += sd->groups->cpu_power;\n\t\t}\n\t\tsg = sg->next;\n\t} while (sg != group_head);\n}\n\nstatic int build_numa_sched_groups(struct s_data *d,\n\t\t\t\t   const struct cpumask *cpu_map, int num)\n{\n\tstruct sched_domain *sd;\n\tstruct sched_group *sg, *prev;\n\tint n, j;\n\n\tcpumask_clear(d->covered);\n\tcpumask_and(d->nodemask, cpumask_of_node(num), cpu_map);\n\tif (cpumask_empty(d->nodemask)) {\n\t\td->sched_group_nodes[num] = NULL;\n\t\tgoto out;\n\t}\n\n\tsched_domain_node_span(num, d->domainspan);\n\tcpumask_and(d->domainspan, d->domainspan, cpu_map);\n\n\tsg = kmalloc_node(sizeof(struct sched_group) + cpumask_size(),\n\t\t\t  GFP_KERNEL, num);\n\tif (!sg) {\n\t\tprintk(KERN_WARNING \"Can not alloc domain group for node %d\\n\",\n\t\t       num);\n\t\treturn -ENOMEM;\n\t}\n\td->sched_group_nodes[num] = sg;\n\n\tfor_each_cpu(j, d->nodemask) {\n\t\tsd = &per_cpu(node_domains, j).sd;\n\t\tsd->groups = sg;\n\t}\n\n\tsg->cpu_power = 0;\n\tcpumask_copy(sched_group_cpus(sg), d->nodemask);\n\tsg->next = sg;\n\tcpumask_or(d->covered, d->covered, d->nodemask);\n\n\tprev = sg;\n\tfor (j = 0; j < nr_node_ids; j++) {\n\t\tn = (num + j) % nr_node_ids;\n\t\tcpumask_complement(d->notcovered, d->covered);\n\t\tcpumask_and(d->tmpmask, d->notcovered, cpu_map);\n\t\tcpumask_and(d->tmpmask, d->tmpmask, d->domainspan);\n\t\tif (cpumask_empty(d->tmpmask))\n\t\t\tbreak;\n\t\tcpumask_and(d->tmpmask, d->tmpmask, cpumask_of_node(n));\n\t\tif (cpumask_empty(d->tmpmask))\n\t\t\tcontinue;\n\t\tsg = kmalloc_node(sizeof(struct sched_group) + cpumask_size(),\n\t\t\t\t  GFP_KERNEL, num);\n\t\tif (!sg) {\n\t\t\tprintk(KERN_WARNING\n\t\t\t       \"Can not alloc domain group for node %d\\n\", j);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tsg->cpu_power = 0;\n\t\tcpumask_copy(sched_group_cpus(sg), d->tmpmask);\n\t\tsg->next = prev->next;\n\t\tcpumask_or(d->covered, d->covered, d->tmpmask);\n\t\tprev->next = sg;\n\t\tprev = sg;\n\t}\nout:\n\treturn 0;\n}\n#endif /* CONFIG_NUMA */\n\n#ifdef CONFIG_NUMA\n/* Free memory allocated for various sched_group structures */\nstatic void free_sched_groups(const struct cpumask *cpu_map,\n\t\t\t      struct cpumask *nodemask)\n{\n\tint cpu, i;\n\n\tfor_each_cpu(cpu, cpu_map) {\n\t\tstruct sched_group **sched_group_nodes\n\t\t\t= sched_group_nodes_bycpu[cpu];\n\n\t\tif (!sched_group_nodes)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < nr_node_ids; i++) {\n\t\t\tstruct sched_group *oldsg, *sg = sched_group_nodes[i];\n\n\t\t\tcpumask_and(nodemask, cpumask_of_node(i), cpu_map);\n\t\t\tif (cpumask_empty(nodemask))\n\t\t\t\tcontinue;\n\n\t\t\tif (sg == NULL)\n\t\t\t\tcontinue;\n\t\t\tsg = sg->next;\nnext_sg:\n\t\t\toldsg = sg;\n\t\t\tsg = sg->next;\n\t\t\tkfree(oldsg);\n\t\t\tif (oldsg != sched_group_nodes[i])\n\t\t\t\tgoto next_sg;\n\t\t}\n\t\tkfree(sched_group_nodes);\n\t\tsched_group_nodes_bycpu[cpu] = NULL;\n\t}\n}\n#else /* !CONFIG_NUMA */\nstatic void free_sched_groups(const struct cpumask *cpu_map,\n\t\t\t      struct cpumask *nodemask)\n{\n}\n#endif /* CONFIG_NUMA */\n\n/*\n * Initialize sched groups cpu_power.\n *\n * cpu_power indicates the capacity of sched group, which is used while\n * distributing the load between different sched groups in a sched domain.\n * Typically cpu_power for all the groups in a sched domain will be same unless\n * there are asymmetries in the topology. If there are asymmetries, group\n * having more cpu_power will pickup more load compared to the group having\n * less cpu_power.\n */\nstatic void init_sched_groups_power(int cpu, struct sched_domain *sd)\n{\n\tstruct sched_domain *child;\n\tstruct sched_group *group;\n\tlong power;\n\tint weight;\n\n\tWARN_ON(!sd || !sd->groups);\n\n\tif (cpu != group_first_cpu(sd->groups))\n\t\treturn;\n\n\tsd->groups->group_weight = cpumask_weight(sched_group_cpus(sd->groups));\n\n\tchild = sd->child;\n\n\tsd->groups->cpu_power = 0;\n\n\tif (!child) {\n\t\tpower = SCHED_LOAD_SCALE;\n\t\tweight = cpumask_weight(sched_domain_span(sd));\n\t\t/*\n\t\t * SMT siblings share the power of a single core.\n\t\t * Usually multiple threads get a better yield out of\n\t\t * that one core than a single thread would have,\n\t\t * reflect that in sd->smt_gain.\n\t\t */\n\t\tif ((sd->flags & SD_SHARE_CPUPOWER) && weight > 1) {\n\t\t\tpower *= sd->smt_gain;\n\t\t\tpower /= weight;\n\t\t\tpower >>= SCHED_LOAD_SHIFT;\n\t\t}\n\t\tsd->groups->cpu_power += power;\n\t\treturn;\n\t}\n\n\t/*\n\t * Add cpu_power of each child group to this groups cpu_power.\n\t */\n\tgroup = child->groups;\n\tdo {\n\t\tsd->groups->cpu_power += group->cpu_power;\n\t\tgroup = group->next;\n\t} while (group != child->groups);\n}\n\n/*\n * Initializers for schedule domains\n * Non-inlined to reduce accumulated stack pressure in build_sched_domains()\n */\n\n#ifdef CONFIG_SCHED_DEBUG\n# define SD_INIT_NAME(sd, type)\t\tsd->name = #type\n#else\n# define SD_INIT_NAME(sd, type)\t\tdo { } while (0)\n#endif\n\n#define\tSD_INIT(sd, type)\tsd_init_##type(sd)\n\n#define SD_INIT_FUNC(type)\t\\\nstatic noinline void sd_init_##type(struct sched_domain *sd)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tmemset(sd, 0, sizeof(*sd));\t\t\t\t\\\n\t*sd = SD_##type##_INIT;\t\t\t\t\t\\\n\tsd->level = SD_LV_##type;\t\t\t\t\\\n\tSD_INIT_NAME(sd, type);\t\t\t\t\t\\\n}\n\nSD_INIT_FUNC(CPU)\n#ifdef CONFIG_NUMA\n SD_INIT_FUNC(ALLNODES)\n SD_INIT_FUNC(NODE)\n#endif\n#ifdef CONFIG_SCHED_SMT\n SD_INIT_FUNC(SIBLING)\n#endif\n#ifdef CONFIG_SCHED_MC\n SD_INIT_FUNC(MC)\n#endif\n#ifdef CONFIG_SCHED_BOOK\n SD_INIT_FUNC(BOOK)\n#endif\n\nstatic int default_relax_domain_level = -1;\n\nstatic int __init setup_relax_domain_level(char *str)\n{\n\tunsigned long val;\n\n\tval = simple_strtoul(str, NULL, 0);\n\tif (val < SD_LV_MAX)\n\t\tdefault_relax_domain_level = val;\n\n\treturn 1;\n}\n__setup(\"relax_domain_level=\", setup_relax_domain_level);\n\nstatic void set_domain_attribute(struct sched_domain *sd,\n\t\t\t\t struct sched_domain_attr *attr)\n{\n\tint request;\n\n\tif (!attr || attr->relax_domain_level < 0) {\n\t\tif (default_relax_domain_level < 0)\n\t\t\treturn;\n\t\telse\n\t\t\trequest = default_relax_domain_level;\n\t} else\n\t\trequest = attr->relax_domain_level;\n\tif (request < sd->level) {\n\t\t/* turn off idle balance on this domain */\n\t\tsd->flags &= ~(SD_BALANCE_WAKE|SD_BALANCE_NEWIDLE);\n\t} else {\n\t\t/* turn on idle balance on this domain */\n\t\tsd->flags |= (SD_BALANCE_WAKE|SD_BALANCE_NEWIDLE);\n\t}\n}\n\nstatic void __free_domain_allocs(struct s_data *d, enum s_alloc what,\n\t\t\t\t const struct cpumask *cpu_map)\n{\n\tswitch (what) {\n\tcase sa_sched_groups:\n\t\tfree_sched_groups(cpu_map, d->tmpmask); /* fall through */\n\t\td->sched_group_nodes = NULL;\n\tcase sa_rootdomain:\n\t\tfree_rootdomain(d->rd); /* fall through */\n\tcase sa_tmpmask:\n\t\tfree_cpumask_var(d->tmpmask); /* fall through */\n\tcase sa_send_covered:\n\t\tfree_cpumask_var(d->send_covered); /* fall through */\n\tcase sa_this_book_map:\n\t\tfree_cpumask_var(d->this_book_map); /* fall through */\n\tcase sa_this_core_map:\n\t\tfree_cpumask_var(d->this_core_map); /* fall through */\n\tcase sa_this_sibling_map:\n\t\tfree_cpumask_var(d->this_sibling_map); /* fall through */\n\tcase sa_nodemask:\n\t\tfree_cpumask_var(d->nodemask); /* fall through */\n\tcase sa_sched_group_nodes:\n#ifdef CONFIG_NUMA\n\t\tkfree(d->sched_group_nodes); /* fall through */\n\tcase sa_notcovered:\n\t\tfree_cpumask_var(d->notcovered); /* fall through */\n\tcase sa_covered:\n\t\tfree_cpumask_var(d->covered); /* fall through */\n\tcase sa_domainspan:\n\t\tfree_cpumask_var(d->domainspan); /* fall through */\n#endif\n\tcase sa_none:\n\t\tbreak;\n\t}\n}\n\nstatic enum s_alloc __visit_domain_allocation_hell(struct s_data *d,\n\t\t\t\t\t\t   const struct cpumask *cpu_map)\n{\n#ifdef CONFIG_NUMA\n\tif (!alloc_cpumask_var(&d->domainspan, GFP_KERNEL))\n\t\treturn sa_none;\n\tif (!alloc_cpumask_var(&d->covered, GFP_KERNEL))\n\t\treturn sa_domainspan;\n\tif (!alloc_cpumask_var(&d->notcovered, GFP_KERNEL))\n\t\treturn sa_covered;\n\t/* Allocate the per-node list of sched groups */\n\td->sched_group_nodes = kcalloc(nr_node_ids,\n\t\t\t\t      sizeof(struct sched_group *), GFP_KERNEL);\n\tif (!d->sched_group_nodes) {\n\t\tprintk(KERN_WARNING \"Can not alloc sched group node list\\n\");\n\t\treturn sa_notcovered;\n\t}\n\tsched_group_nodes_bycpu[cpumask_first(cpu_map)] = d->sched_group_nodes;\n#endif\n\tif (!alloc_cpumask_var(&d->nodemask, GFP_KERNEL))\n\t\treturn sa_sched_group_nodes;\n\tif (!alloc_cpumask_var(&d->this_sibling_map, GFP_KERNEL))\n\t\treturn sa_nodemask;\n\tif (!alloc_cpumask_var(&d->this_core_map, GFP_KERNEL))\n\t\treturn sa_this_sibling_map;\n\tif (!alloc_cpumask_var(&d->this_book_map, GFP_KERNEL))\n\t\treturn sa_this_core_map;\n\tif (!alloc_cpumask_var(&d->send_covered, GFP_KERNEL))\n\t\treturn sa_this_book_map;\n\tif (!alloc_cpumask_var(&d->tmpmask, GFP_KERNEL))\n\t\treturn sa_send_covered;\n\td->rd = alloc_rootdomain();\n\tif (!d->rd) {\n\t\tprintk(KERN_WARNING \"Cannot alloc root domain\\n\");\n\t\treturn sa_tmpmask;\n\t}\n\treturn sa_rootdomain;\n}\n\nstatic struct sched_domain *__build_numa_sched_domains(struct s_data *d,\n\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr, int i)\n{\n\tstruct sched_domain *sd = NULL;\n#ifdef CONFIG_NUMA\n\tstruct sched_domain *parent;\n\n\td->sd_allnodes = 0;\n\tif (cpumask_weight(cpu_map) >\n\t    SD_NODES_PER_DOMAIN * cpumask_weight(d->nodemask)) {\n\t\tsd = &per_cpu(allnodes_domains, i).sd;\n\t\tSD_INIT(sd, ALLNODES);\n\t\tset_domain_attribute(sd, attr);\n\t\tcpumask_copy(sched_domain_span(sd), cpu_map);\n\t\tcpu_to_allnodes_group(i, cpu_map, &sd->groups, d->tmpmask);\n\t\td->sd_allnodes = 1;\n\t}\n\tparent = sd;\n\n\tsd = &per_cpu(node_domains, i).sd;\n\tSD_INIT(sd, NODE);\n\tset_domain_attribute(sd, attr);\n\tsched_domain_node_span(cpu_to_node(i), sched_domain_span(sd));\n\tsd->parent = parent;\n\tif (parent)\n\t\tparent->child = sd;\n\tcpumask_and(sched_domain_span(sd), sched_domain_span(sd), cpu_map);\n#endif\n\treturn sd;\n}\n\nstatic struct sched_domain *__build_cpu_sched_domain(struct s_data *d,\n\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr,\n\tstruct sched_domain *parent, int i)\n{\n\tstruct sched_domain *sd;\n\tsd = &per_cpu(phys_domains, i).sd;\n\tSD_INIT(sd, CPU);\n\tset_domain_attribute(sd, attr);\n\tcpumask_copy(sched_domain_span(sd), d->nodemask);\n\tsd->parent = parent;\n\tif (parent)\n\t\tparent->child = sd;\n\tcpu_to_phys_group(i, cpu_map, &sd->groups, d->tmpmask);\n\treturn sd;\n}\n\nstatic struct sched_domain *__build_book_sched_domain(struct s_data *d,\n\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr,\n\tstruct sched_domain *parent, int i)\n{\n\tstruct sched_domain *sd = parent;\n#ifdef CONFIG_SCHED_BOOK\n\tsd = &per_cpu(book_domains, i).sd;\n\tSD_INIT(sd, BOOK);\n\tset_domain_attribute(sd, attr);\n\tcpumask_and(sched_domain_span(sd), cpu_map, cpu_book_mask(i));\n\tsd->parent = parent;\n\tparent->child = sd;\n\tcpu_to_book_group(i, cpu_map, &sd->groups, d->tmpmask);\n#endif\n\treturn sd;\n}\n\nstatic struct sched_domain *__build_mc_sched_domain(struct s_data *d,\n\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr,\n\tstruct sched_domain *parent, int i)\n{\n\tstruct sched_domain *sd = parent;\n#ifdef CONFIG_SCHED_MC\n\tsd = &per_cpu(core_domains, i).sd;\n\tSD_INIT(sd, MC);\n\tset_domain_attribute(sd, attr);\n\tcpumask_and(sched_domain_span(sd), cpu_map, cpu_coregroup_mask(i));\n\tsd->parent = parent;\n\tparent->child = sd;\n\tcpu_to_core_group(i, cpu_map, &sd->groups, d->tmpmask);\n#endif\n\treturn sd;\n}\n\nstatic struct sched_domain *__build_smt_sched_domain(struct s_data *d,\n\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr,\n\tstruct sched_domain *parent, int i)\n{\n\tstruct sched_domain *sd = parent;\n#ifdef CONFIG_SCHED_SMT\n\tsd = &per_cpu(cpu_domains, i).sd;\n\tSD_INIT(sd, SIBLING);\n\tset_domain_attribute(sd, attr);\n\tcpumask_and(sched_domain_span(sd), cpu_map, topology_thread_cpumask(i));\n\tsd->parent = parent;\n\tparent->child = sd;\n\tcpu_to_cpu_group(i, cpu_map, &sd->groups, d->tmpmask);\n#endif\n\treturn sd;\n}\n\nstatic void build_sched_groups(struct s_data *d, enum sched_domain_level l,\n\t\t\t       const struct cpumask *cpu_map, int cpu)\n{\n\tswitch (l) {\n#ifdef CONFIG_SCHED_SMT\n\tcase SD_LV_SIBLING: /* set up CPU (sibling) groups */\n\t\tcpumask_and(d->this_sibling_map, cpu_map,\n\t\t\t    topology_thread_cpumask(cpu));\n\t\tif (cpu == cpumask_first(d->this_sibling_map))\n\t\t\tinit_sched_build_groups(d->this_sibling_map, cpu_map,\n\t\t\t\t\t\t&cpu_to_cpu_group,\n\t\t\t\t\t\td->send_covered, d->tmpmask);\n\t\tbreak;\n#endif\n#ifdef CONFIG_SCHED_MC\n\tcase SD_LV_MC: /* set up multi-core groups */\n\t\tcpumask_and(d->this_core_map, cpu_map, cpu_coregroup_mask(cpu));\n\t\tif (cpu == cpumask_first(d->this_core_map))\n\t\t\tinit_sched_build_groups(d->this_core_map, cpu_map,\n\t\t\t\t\t\t&cpu_to_core_group,\n\t\t\t\t\t\td->send_covered, d->tmpmask);\n\t\tbreak;\n#endif\n#ifdef CONFIG_SCHED_BOOK\n\tcase SD_LV_BOOK: /* set up book groups */\n\t\tcpumask_and(d->this_book_map, cpu_map, cpu_book_mask(cpu));\n\t\tif (cpu == cpumask_first(d->this_book_map))\n\t\t\tinit_sched_build_groups(d->this_book_map, cpu_map,\n\t\t\t\t\t\t&cpu_to_book_group,\n\t\t\t\t\t\td->send_covered, d->tmpmask);\n\t\tbreak;\n#endif\n\tcase SD_LV_CPU: /* set up physical groups */\n\t\tcpumask_and(d->nodemask, cpumask_of_node(cpu), cpu_map);\n\t\tif (!cpumask_empty(d->nodemask))\n\t\t\tinit_sched_build_groups(d->nodemask, cpu_map,\n\t\t\t\t\t\t&cpu_to_phys_group,\n\t\t\t\t\t\td->send_covered, d->tmpmask);\n\t\tbreak;\n#ifdef CONFIG_NUMA\n\tcase SD_LV_ALLNODES:\n\t\tinit_sched_build_groups(cpu_map, cpu_map, &cpu_to_allnodes_group,\n\t\t\t\t\td->send_covered, d->tmpmask);\n\t\tbreak;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n}\n\n/*\n * Build sched domains for a given set of cpus and attach the sched domains\n * to the individual cpus\n */\nstatic int __build_sched_domains(const struct cpumask *cpu_map,\n\t\t\t\t struct sched_domain_attr *attr)\n{\n\tenum s_alloc alloc_state = sa_none;\n\tstruct s_data d;\n\tstruct sched_domain *sd;\n\tint i;\n#ifdef CONFIG_NUMA\n\td.sd_allnodes = 0;\n#endif\n\n\talloc_state = __visit_domain_allocation_hell(&d, cpu_map);\n\tif (alloc_state != sa_rootdomain)\n\t\tgoto error;\n\talloc_state = sa_sched_groups;\n\n\t/*\n\t * Set up domains for cpus specified by the cpu_map.\n\t */\n\tfor_each_cpu(i, cpu_map) {\n\t\tcpumask_and(d.nodemask, cpumask_of_node(cpu_to_node(i)),\n\t\t\t    cpu_map);\n\n\t\tsd = __build_numa_sched_domains(&d, cpu_map, attr, i);\n\t\tsd = __build_cpu_sched_domain(&d, cpu_map, attr, sd, i);\n\t\tsd = __build_book_sched_domain(&d, cpu_map, attr, sd, i);\n\t\tsd = __build_mc_sched_domain(&d, cpu_map, attr, sd, i);\n\t\tsd = __build_smt_sched_domain(&d, cpu_map, attr, sd, i);\n\t}\n\n\tfor_each_cpu(i, cpu_map) {\n\t\tbuild_sched_groups(&d, SD_LV_SIBLING, cpu_map, i);\n\t\tbuild_sched_groups(&d, SD_LV_BOOK, cpu_map, i);\n\t\tbuild_sched_groups(&d, SD_LV_MC, cpu_map, i);\n\t}\n\n\t/* Set up physical groups */\n\tfor (i = 0; i < nr_node_ids; i++)\n\t\tbuild_sched_groups(&d, SD_LV_CPU, cpu_map, i);\n\n#ifdef CONFIG_NUMA\n\t/* Set up node groups */\n\tif (d.sd_allnodes)\n\t\tbuild_sched_groups(&d, SD_LV_ALLNODES, cpu_map, 0);\n\n\tfor (i = 0; i < nr_node_ids; i++)\n\t\tif (build_numa_sched_groups(&d, cpu_map, i))\n\t\t\tgoto error;\n#endif\n\n\t/* Calculate CPU power for physical packages and nodes */\n#ifdef CONFIG_SCHED_SMT\n\tfor_each_cpu(i, cpu_map) {\n\t\tsd = &per_cpu(cpu_domains, i).sd;\n\t\tinit_sched_groups_power(i, sd);\n\t}\n#endif\n#ifdef CONFIG_SCHED_MC\n\tfor_each_cpu(i, cpu_map) {\n\t\tsd = &per_cpu(core_domains, i).sd;\n\t\tinit_sched_groups_power(i, sd);\n\t}\n#endif\n#ifdef CONFIG_SCHED_BOOK\n\tfor_each_cpu(i, cpu_map) {\n\t\tsd = &per_cpu(book_domains, i).sd;\n\t\tinit_sched_groups_power(i, sd);\n\t}\n#endif\n\n\tfor_each_cpu(i, cpu_map) {\n\t\tsd = &per_cpu(phys_domains, i).sd;\n\t\tinit_sched_groups_power(i, sd);\n\t}\n\n#ifdef CONFIG_NUMA\n\tfor (i = 0; i < nr_node_ids; i++)\n\t\tinit_numa_sched_groups_power(d.sched_group_nodes[i]);\n\n\tif (d.sd_allnodes) {\n\t\tstruct sched_group *sg;\n\n\t\tcpu_to_allnodes_group(cpumask_first(cpu_map), cpu_map, &sg,\n\t\t\t\t\t\t\t\td.tmpmask);\n\t\tinit_numa_sched_groups_power(sg);\n\t}\n#endif\n\n\t/* Attach the domains */\n\tfor_each_cpu(i, cpu_map) {\n#ifdef CONFIG_SCHED_SMT\n\t\tsd = &per_cpu(cpu_domains, i).sd;\n#elif defined(CONFIG_SCHED_MC)\n\t\tsd = &per_cpu(core_domains, i).sd;\n#elif defined(CONFIG_SCHED_BOOK)\n\t\tsd = &per_cpu(book_domains, i).sd;\n#else\n\t\tsd = &per_cpu(phys_domains, i).sd;\n#endif\n\t\tcpu_attach_domain(sd, d.rd, i);\n\t}\n\n\td.sched_group_nodes = NULL; /* don't free this we still need it */\n\t__free_domain_allocs(&d, sa_tmpmask, cpu_map);\n\treturn 0;\n\nerror:\n\t__free_domain_allocs(&d, alloc_state, cpu_map);\n\treturn -ENOMEM;\n}\n\nstatic int build_sched_domains(const struct cpumask *cpu_map)\n{\n\treturn __build_sched_domains(cpu_map, NULL);\n}\n\nstatic cpumask_var_t *doms_cur;\t/* current sched domains */\nstatic int ndoms_cur;\t\t/* number of sched domains in 'doms_cur' */\nstatic struct sched_domain_attr *dattr_cur;\n\t\t\t\t/* attribues of custom domains in 'doms_cur' */\n\n/*\n * Special case: If a kmalloc of a doms_cur partition (array of\n * cpumask) fails, then fallback to a single sched domain,\n * as determined by the single cpumask fallback_doms.\n */\nstatic cpumask_var_t fallback_doms;\n\n/*\n * arch_update_cpu_topology lets virtualized architectures update the\n * cpu core maps. It is supposed to return 1 if the topology changed\n * or 0 if it stayed the same.\n */\nint __attribute__((weak)) arch_update_cpu_topology(void)\n{\n\treturn 0;\n}\n\ncpumask_var_t *alloc_sched_domains(unsigned int ndoms)\n{\n\tint i;\n\tcpumask_var_t *doms;\n\n\tdoms = kmalloc(sizeof(*doms) * ndoms, GFP_KERNEL);\n\tif (!doms)\n\t\treturn NULL;\n\tfor (i = 0; i < ndoms; i++) {\n\t\tif (!alloc_cpumask_var(&doms[i], GFP_KERNEL)) {\n\t\t\tfree_sched_domains(doms, i);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\treturn doms;\n}\n\nvoid free_sched_domains(cpumask_var_t doms[], unsigned int ndoms)\n{\n\tunsigned int i;\n\tfor (i = 0; i < ndoms; i++)\n\t\tfree_cpumask_var(doms[i]);\n\tkfree(doms);\n}\n\n/*\n * Set up scheduler domains and groups. Callers must hold the hotplug lock.\n * For now this just excludes isolated cpus, but could be used to\n * exclude other special cases in the future.\n */\nstatic int arch_init_sched_domains(const struct cpumask *cpu_map)\n{\n\tint err;\n\n\tarch_update_cpu_topology();\n\tndoms_cur = 1;\n\tdoms_cur = alloc_sched_domains(ndoms_cur);\n\tif (!doms_cur)\n\t\tdoms_cur = &fallback_doms;\n\tcpumask_andnot(doms_cur[0], cpu_map, cpu_isolated_map);\n\tdattr_cur = NULL;\n\terr = build_sched_domains(doms_cur[0]);\n\tregister_sched_domain_sysctl();\n\n\treturn err;\n}\n\nstatic void arch_destroy_sched_domains(const struct cpumask *cpu_map,\n\t\t\t\t       struct cpumask *tmpmask)\n{\n\tfree_sched_groups(cpu_map, tmpmask);\n}\n\n/*\n * Detach sched domains from a group of cpus specified in cpu_map\n * These cpus will now be attached to the NULL domain\n */\nstatic void detach_destroy_domains(const struct cpumask *cpu_map)\n{\n\t/* Save because hotplug lock held. */\n\tstatic DECLARE_BITMAP(tmpmask, CONFIG_NR_CPUS);\n\tint i;\n\n\tfor_each_cpu(i, cpu_map)\n\t\tcpu_attach_domain(NULL, &def_root_domain, i);\n\tsynchronize_sched();\n\tarch_destroy_sched_domains(cpu_map, to_cpumask(tmpmask));\n}\n\n/* handle null as \"default\" */\nstatic int dattrs_equal(struct sched_domain_attr *cur, int idx_cur,\n\t\t\tstruct sched_domain_attr *new, int idx_new)\n{\n\tstruct sched_domain_attr tmp;\n\n\t/* fast path */\n\tif (!new && !cur)\n\t\treturn 1;\n\n\ttmp = SD_ATTR_INIT;\n\treturn !memcmp(cur ? (cur + idx_cur) : &tmp,\n\t\t\tnew ? (new + idx_new) : &tmp,\n\t\t\tsizeof(struct sched_domain_attr));\n}\n\n/*\n * Partition sched domains as specified by the 'ndoms_new'\n * cpumasks in the array doms_new[] of cpumasks. This compares\n * doms_new[] to the current sched domain partitioning, doms_cur[].\n * It destroys each deleted domain and builds each new domain.\n *\n * 'doms_new' is an array of cpumask_var_t's of length 'ndoms_new'.\n * The masks don't intersect (don't overlap.) We should setup one\n * sched domain for each mask. CPUs not in any of the cpumasks will\n * not be load balanced. If the same cpumask appears both in the\n * current 'doms_cur' domains and in the new 'doms_new', we can leave\n * it as it is.\n *\n * The passed in 'doms_new' should be allocated using\n * alloc_sched_domains.  This routine takes ownership of it and will\n * free_sched_domains it when done with it. If the caller failed the\n * alloc call, then it can pass in doms_new == NULL && ndoms_new == 1,\n * and partition_sched_domains() will fallback to the single partition\n * 'fallback_doms', it also forces the domains to be rebuilt.\n *\n * If doms_new == NULL it will be replaced with cpu_online_mask.\n * ndoms_new == 0 is a special case for destroying existing domains,\n * and it will not create the default domain.\n *\n * Call with hotplug lock held\n */\nvoid partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t     struct sched_domain_attr *dattr_new)\n{\n\tint i, j, n;\n\tint new_topology;\n\n\tmutex_lock(&sched_domains_mutex);\n\n\t/* always unregister in case we don't destroy any domains */\n\tunregister_sched_domain_sysctl();\n\n\t/* Let architecture update cpu core mappings. */\n\tnew_topology = arch_update_cpu_topology();\n\n\tn = doms_new ? ndoms_new : 0;\n\n\t/* Destroy deleted domains */\n\tfor (i = 0; i < ndoms_cur; i++) {\n\t\tfor (j = 0; j < n && !new_topology; j++) {\n\t\t\tif (cpumask_equal(doms_cur[i], doms_new[j])\n\t\t\t    && dattrs_equal(dattr_cur, i, dattr_new, j))\n\t\t\t\tgoto match1;\n\t\t}\n\t\t/* no match - a current sched domain not in new doms_new[] */\n\t\tdetach_destroy_domains(doms_cur[i]);\nmatch1:\n\t\t;\n\t}\n\n\tif (doms_new == NULL) {\n\t\tndoms_cur = 0;\n\t\tdoms_new = &fallback_doms;\n\t\tcpumask_andnot(doms_new[0], cpu_active_mask, cpu_isolated_map);\n\t\tWARN_ON_ONCE(dattr_new);\n\t}\n\n\t/* Build new domains */\n\tfor (i = 0; i < ndoms_new; i++) {\n\t\tfor (j = 0; j < ndoms_cur && !new_topology; j++) {\n\t\t\tif (cpumask_equal(doms_new[i], doms_cur[j])\n\t\t\t    && dattrs_equal(dattr_new, i, dattr_cur, j))\n\t\t\t\tgoto match2;\n\t\t}\n\t\t/* no match - add a new doms_new */\n\t\t__build_sched_domains(doms_new[i],\n\t\t\t\t\tdattr_new ? dattr_new + i : NULL);\nmatch2:\n\t\t;\n\t}\n\n\t/* Remember the new sched domains */\n\tif (doms_cur != &fallback_doms)\n\t\tfree_sched_domains(doms_cur, ndoms_cur);\n\tkfree(dattr_cur);\t/* kfree(NULL) is safe */\n\tdoms_cur = doms_new;\n\tdattr_cur = dattr_new;\n\tndoms_cur = ndoms_new;\n\n\tregister_sched_domain_sysctl();\n\n\tmutex_unlock(&sched_domains_mutex);\n}\n\n#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)\nstatic void arch_reinit_sched_domains(void)\n{\n\tget_online_cpus();\n\n\t/* Destroy domains first to force the rebuild */\n\tpartition_sched_domains(0, NULL, NULL);\n\n\trebuild_sched_domains();\n\tput_online_cpus();\n}\n\nstatic ssize_t sched_power_savings_store(const char *buf, size_t count, int smt)\n{\n\tunsigned int level = 0;\n\n\tif (sscanf(buf, \"%u\", &level) != 1)\n\t\treturn -EINVAL;\n\n\t/*\n\t * level is always be positive so don't check for\n\t * level < POWERSAVINGS_BALANCE_NONE which is 0\n\t * What happens on 0 or 1 byte write,\n\t * need to check for count as well?\n\t */\n\n\tif (level >= MAX_POWERSAVINGS_BALANCE_LEVELS)\n\t\treturn -EINVAL;\n\n\tif (smt)\n\t\tsched_smt_power_savings = level;\n\telse\n\t\tsched_mc_power_savings = level;\n\n\tarch_reinit_sched_domains();\n\n\treturn count;\n}\n\n#ifdef CONFIG_SCHED_MC\nstatic ssize_t sched_mc_power_savings_show(struct sysdev_class *class,\n\t\t\t\t\t   struct sysdev_class_attribute *attr,\n\t\t\t\t\t   char *page)\n{\n\treturn sprintf(page, \"%u\\n\", sched_mc_power_savings);\n}\nstatic ssize_t sched_mc_power_savings_store(struct sysdev_class *class,\n\t\t\t\t\t    struct sysdev_class_attribute *attr,\n\t\t\t\t\t    const char *buf, size_t count)\n{\n\treturn sched_power_savings_store(buf, count, 0);\n}\nstatic SYSDEV_CLASS_ATTR(sched_mc_power_savings, 0644,\n\t\t\t sched_mc_power_savings_show,\n\t\t\t sched_mc_power_savings_store);\n#endif\n\n#ifdef CONFIG_SCHED_SMT\nstatic ssize_t sched_smt_power_savings_show(struct sysdev_class *dev,\n\t\t\t\t\t    struct sysdev_class_attribute *attr,\n\t\t\t\t\t    char *page)\n{\n\treturn sprintf(page, \"%u\\n\", sched_smt_power_savings);\n}\nstatic ssize_t sched_smt_power_savings_store(struct sysdev_class *dev,\n\t\t\t\t\t     struct sysdev_class_attribute *attr,\n\t\t\t\t\t     const char *buf, size_t count)\n{\n\treturn sched_power_savings_store(buf, count, 1);\n}\nstatic SYSDEV_CLASS_ATTR(sched_smt_power_savings, 0644,\n\t\t   sched_smt_power_savings_show,\n\t\t   sched_smt_power_savings_store);\n#endif\n\nint __init sched_create_sysfs_power_savings_entries(struct sysdev_class *cls)\n{\n\tint err = 0;\n\n#ifdef CONFIG_SCHED_SMT\n\tif (smt_capable())\n\t\terr = sysfs_create_file(&cls->kset.kobj,\n\t\t\t\t\t&attr_sched_smt_power_savings.attr);\n#endif\n#ifdef CONFIG_SCHED_MC\n\tif (!err && mc_capable())\n\t\terr = sysfs_create_file(&cls->kset.kobj,\n\t\t\t\t\t&attr_sched_mc_power_savings.attr);\n#endif\n\treturn err;\n}\n#endif /* CONFIG_SCHED_MC || CONFIG_SCHED_SMT */\n\n/*\n * Update cpusets according to cpu_active mask.  If cpusets are\n * disabled, cpuset_update_active_cpus() becomes a simple wrapper\n * around partition_sched_domains().\n */\nstatic int cpuset_cpu_active(struct notifier_block *nfb, unsigned long action,\n\t\t\t     void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_ONLINE:\n\tcase CPU_DOWN_FAILED:\n\t\tcpuset_update_active_cpus();\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int cpuset_cpu_inactive(struct notifier_block *nfb, unsigned long action,\n\t\t\t       void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_DOWN_PREPARE:\n\t\tcpuset_update_active_cpus();\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int update_runtime(struct notifier_block *nfb,\n\t\t\t\tunsigned long action, void *hcpu)\n{\n\tint cpu = (int)(long)hcpu;\n\n\tswitch (action) {\n\tcase CPU_DOWN_PREPARE:\n\tcase CPU_DOWN_PREPARE_FROZEN:\n\t\tdisable_runtime(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\n\tcase CPU_DOWN_FAILED:\n\tcase CPU_DOWN_FAILED_FROZEN:\n\tcase CPU_ONLINE:\n\tcase CPU_ONLINE_FROZEN:\n\t\tenable_runtime(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nvoid __init sched_init_smp(void)\n{\n\tcpumask_var_t non_isolated_cpus;\n\n\talloc_cpumask_var(&non_isolated_cpus, GFP_KERNEL);\n\talloc_cpumask_var(&fallback_doms, GFP_KERNEL);\n\n#if defined(CONFIG_NUMA)\n\tsched_group_nodes_bycpu = kzalloc(nr_cpu_ids * sizeof(void **),\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\tBUG_ON(sched_group_nodes_bycpu == NULL);\n#endif\n\tget_online_cpus();\n\tmutex_lock(&sched_domains_mutex);\n\tarch_init_sched_domains(cpu_active_mask);\n\tcpumask_andnot(non_isolated_cpus, cpu_possible_mask, cpu_isolated_map);\n\tif (cpumask_empty(non_isolated_cpus))\n\t\tcpumask_set_cpu(smp_processor_id(), non_isolated_cpus);\n\tmutex_unlock(&sched_domains_mutex);\n\tput_online_cpus();\n\n\thotcpu_notifier(cpuset_cpu_active, CPU_PRI_CPUSET_ACTIVE);\n\thotcpu_notifier(cpuset_cpu_inactive, CPU_PRI_CPUSET_INACTIVE);\n\n\t/* RT runtime code needs to handle some hotplug events */\n\thotcpu_notifier(update_runtime, 0);\n\n\tinit_hrtick();\n\n\t/* Move init over to a non-isolated CPU */\n\tif (set_cpus_allowed_ptr(current, non_isolated_cpus) < 0)\n\t\tBUG();\n\tsched_init_granularity();\n\tfree_cpumask_var(non_isolated_cpus);\n\n\tinit_sched_rt_class();\n}\n#else\nvoid __init sched_init_smp(void)\n{\n\tsched_init_granularity();\n}\n#endif /* CONFIG_SMP */\n\nconst_debug unsigned int sysctl_timer_migration = 1;\n\nint in_sched_functions(unsigned long addr)\n{\n\treturn in_lock_functions(addr) ||\n\t\t(addr >= (unsigned long)__sched_text_start\n\t\t&& addr < (unsigned long)__sched_text_end);\n}\n\nstatic void init_cfs_rq(struct cfs_rq *cfs_rq, struct rq *rq)\n{\n\tcfs_rq->tasks_timeline = RB_ROOT;\n\tINIT_LIST_HEAD(&cfs_rq->tasks);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tcfs_rq->rq = rq;\n#endif\n\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));\n}\n\nstatic void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq)\n{\n\tstruct rt_prio_array *array;\n\tint i;\n\n\tarray = &rt_rq->active;\n\tfor (i = 0; i < MAX_RT_PRIO; i++) {\n\t\tINIT_LIST_HEAD(array->queue + i);\n\t\t__clear_bit(i, array->bitmap);\n\t}\n\t/* delimiter for bitsearch: */\n\t__set_bit(MAX_RT_PRIO, array->bitmap);\n\n#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED\n\trt_rq->highest_prio.curr = MAX_RT_PRIO;\n#ifdef CONFIG_SMP\n\trt_rq->highest_prio.next = MAX_RT_PRIO;\n#endif\n#endif\n#ifdef CONFIG_SMP\n\trt_rq->rt_nr_migratory = 0;\n\trt_rq->overloaded = 0;\n\tplist_head_init_raw(&rt_rq->pushable_tasks, &rq->lock);\n#endif\n\n\trt_rq->rt_time = 0;\n\trt_rq->rt_throttled = 0;\n\trt_rq->rt_runtime = 0;\n\traw_spin_lock_init(&rt_rq->rt_runtime_lock);\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\trt_rq->rt_nr_boosted = 0;\n\trt_rq->rq = rq;\n#endif\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\t\tstruct sched_entity *se, int cpu, int add,\n\t\t\t\tstruct sched_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\ttg->cfs_rq[cpu] = cfs_rq;\n\tinit_cfs_rq(cfs_rq, rq);\n\tcfs_rq->tg = tg;\n\tif (add)\n\t\tlist_add(&cfs_rq->leaf_cfs_rq_list, &rq->leaf_cfs_rq_list);\n\n\ttg->se[cpu] = se;\n\t/* se could be NULL for init_task_group */\n\tif (!se)\n\t\treturn;\n\n\tif (!parent)\n\t\tse->cfs_rq = &rq->cfs;\n\telse\n\t\tse->cfs_rq = parent->my_q;\n\n\tse->my_q = cfs_rq;\n\tse->load.weight = tg->shares;\n\tse->load.inv_weight = 0;\n\tse->parent = parent;\n}\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,\n\t\tstruct sched_rt_entity *rt_se, int cpu, int add,\n\t\tstruct sched_rt_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\ttg->rt_rq[cpu] = rt_rq;\n\tinit_rt_rq(rt_rq, rq);\n\trt_rq->tg = tg;\n\trt_rq->rt_runtime = tg->rt_bandwidth.rt_runtime;\n\tif (add)\n\t\tlist_add(&rt_rq->leaf_rt_rq_list, &rq->leaf_rt_rq_list);\n\n\ttg->rt_se[cpu] = rt_se;\n\tif (!rt_se)\n\t\treturn;\n\n\tif (!parent)\n\t\trt_se->rt_rq = &rq->rt;\n\telse\n\t\trt_se->rt_rq = parent->my_q;\n\n\trt_se->my_q = rt_rq;\n\trt_se->parent = parent;\n\tINIT_LIST_HEAD(&rt_se->run_list);\n}\n#endif\n\nvoid __init sched_init(void)\n{\n\tint i, j;\n\tunsigned long alloc_size = 0, ptr;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\talloc_size += 2 * nr_cpu_ids * sizeof(void **);\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\talloc_size += 2 * nr_cpu_ids * sizeof(void **);\n#endif\n#ifdef CONFIG_CPUMASK_OFFSTACK\n\talloc_size += num_possible_cpus() * cpumask_size();\n#endif\n\tif (alloc_size) {\n\t\tptr = (unsigned long)kzalloc(alloc_size, GFP_NOWAIT);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t\tinit_task_group.se = (struct sched_entity **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\tinit_task_group.cfs_rq = (struct cfs_rq **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n#ifdef CONFIG_RT_GROUP_SCHED\n\t\tinit_task_group.rt_se = (struct sched_rt_entity **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\tinit_task_group.rt_rq = (struct rt_rq **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n#endif /* CONFIG_RT_GROUP_SCHED */\n#ifdef CONFIG_CPUMASK_OFFSTACK\n\t\tfor_each_possible_cpu(i) {\n\t\t\tper_cpu(load_balance_tmpmask, i) = (void *)ptr;\n\t\t\tptr += cpumask_size();\n\t\t}\n#endif /* CONFIG_CPUMASK_OFFSTACK */\n\t}\n\n#ifdef CONFIG_SMP\n\tinit_defrootdomain();\n#endif\n\n\tinit_rt_bandwidth(&def_rt_bandwidth,\n\t\t\tglobal_rt_period(), global_rt_runtime());\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tinit_rt_bandwidth(&init_task_group.rt_bandwidth,\n\t\t\tglobal_rt_period(), global_rt_runtime());\n#endif /* CONFIG_RT_GROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_SCHED\n\tlist_add(&init_task_group.list, &task_groups);\n\tINIT_LIST_HEAD(&init_task_group.children);\n\n#endif /* CONFIG_CGROUP_SCHED */\n\n#if defined CONFIG_FAIR_GROUP_SCHED && defined CONFIG_SMP\n\tupdate_shares_data = __alloc_percpu(nr_cpu_ids * sizeof(unsigned long),\n\t\t\t\t\t    __alignof__(unsigned long));\n#endif\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq;\n\n\t\trq = cpu_rq(i);\n\t\traw_spin_lock_init(&rq->lock);\n\t\trq->nr_running = 0;\n\t\trq->calc_load_active = 0;\n\t\trq->calc_load_update = jiffies + LOAD_FREQ;\n\t\tinit_cfs_rq(&rq->cfs, rq);\n\t\tinit_rt_rq(&rq->rt, rq);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t\tinit_task_group.shares = init_task_group_load;\n\t\tINIT_LIST_HEAD(&rq->leaf_cfs_rq_list);\n#ifdef CONFIG_CGROUP_SCHED\n\t\t/*\n\t\t * How much cpu bandwidth does init_task_group get?\n\t\t *\n\t\t * In case of task-groups formed thr' the cgroup filesystem, it\n\t\t * gets 100% of the cpu resources in the system. This overall\n\t\t * system cpu resource is divided among the tasks of\n\t\t * init_task_group and its child task-groups in a fair manner,\n\t\t * based on each entity's (task or task-group's) weight\n\t\t * (se->load.weight).\n\t\t *\n\t\t * In other words, if init_task_group has 10 tasks of weight\n\t\t * 1024) and two child groups A0 and A1 (of weight 1024 each),\n\t\t * then A0's share of the cpu resource is:\n\t\t *\n\t\t *\tA0's bandwidth = 1024 / (10*1024 + 1024 + 1024) = 8.33%\n\t\t *\n\t\t * We achieve this by letting init_task_group's tasks sit\n\t\t * directly in rq->cfs (i.e init_task_group->se[] = NULL).\n\t\t */\n\t\tinit_tg_cfs_entry(&init_task_group, &rq->cfs, NULL, i, 1, NULL);\n#endif\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n\t\trq->rt.rt_runtime = def_rt_bandwidth.rt_runtime;\n#ifdef CONFIG_RT_GROUP_SCHED\n\t\tINIT_LIST_HEAD(&rq->leaf_rt_rq_list);\n#ifdef CONFIG_CGROUP_SCHED\n\t\tinit_tg_rt_entry(&init_task_group, &rq->rt, NULL, i, 1, NULL);\n#endif\n#endif\n\n\t\tfor (j = 0; j < CPU_LOAD_IDX_MAX; j++)\n\t\t\trq->cpu_load[j] = 0;\n\n\t\trq->last_load_update_tick = jiffies;\n\n#ifdef CONFIG_SMP\n\t\trq->sd = NULL;\n\t\trq->rd = NULL;\n\t\trq->cpu_power = SCHED_LOAD_SCALE;\n\t\trq->post_schedule = 0;\n\t\trq->active_balance = 0;\n\t\trq->next_balance = jiffies;\n\t\trq->push_cpu = 0;\n\t\trq->cpu = i;\n\t\trq->online = 0;\n\t\trq->idle_stamp = 0;\n\t\trq->avg_idle = 2*sysctl_sched_migration_cost;\n\t\trq_attach_root(rq, &def_root_domain);\n#ifdef CONFIG_NO_HZ\n\t\trq->nohz_balance_kick = 0;\n\t\tinit_sched_softirq_csd(&per_cpu(remote_sched_softirq_cb, i));\n#endif\n#endif\n\t\tinit_rq_hrtick(rq);\n\t\tatomic_set(&rq->nr_iowait, 0);\n\t}\n\n\tset_load_weight(&init_task);\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\tINIT_HLIST_HEAD(&init_task.preempt_notifiers);\n#endif\n\n#ifdef CONFIG_SMP\n\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains);\n#endif\n\n#ifdef CONFIG_RT_MUTEXES\n\tplist_head_init_raw(&init_task.pi_waiters, &init_task.pi_lock);\n#endif\n\n\t/*\n\t * The boot idle thread does lazy MMU switching as well:\n\t */\n\tatomic_inc(&init_mm.mm_count);\n\tenter_lazy_tlb(&init_mm, current);\n\n\t/*\n\t * Make us the idle thread. Technically, schedule() should not be\n\t * called from this thread, however somewhere below it might be,\n\t * but because we are the idle thread, we just pick up running again\n\t * when this runqueue becomes \"idle\".\n\t */\n\tinit_idle(current, smp_processor_id());\n\n\tcalc_load_update = jiffies + LOAD_FREQ;\n\n\t/*\n\t * During early bootup we pretend to be a normal task:\n\t */\n\tcurrent->sched_class = &fair_sched_class;\n\n\t/* Allocate the nohz_cpu_mask if CONFIG_CPUMASK_OFFSTACK */\n\tzalloc_cpumask_var(&nohz_cpu_mask, GFP_NOWAIT);\n#ifdef CONFIG_SMP\n#ifdef CONFIG_NO_HZ\n\tzalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);\n\talloc_cpumask_var(&nohz.grp_idle_mask, GFP_NOWAIT);\n\tatomic_set(&nohz.load_balancer, nr_cpu_ids);\n\tatomic_set(&nohz.first_pick_cpu, nr_cpu_ids);\n\tatomic_set(&nohz.second_pick_cpu, nr_cpu_ids);\n#endif\n\t/* May be allocated at isolcpus cmdline parse time */\n\tif (cpu_isolated_map == NULL)\n\t\tzalloc_cpumask_var(&cpu_isolated_map, GFP_NOWAIT);\n#endif /* SMP */\n\n\tperf_event_init();\n\n\tscheduler_running = 1;\n}\n\n#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP\nstatic inline int preempt_count_equals(int preempt_offset)\n{\n\tint nested = (preempt_count() & ~PREEMPT_ACTIVE) + rcu_preempt_depth();\n\n\treturn (nested == PREEMPT_INATOMIC_BASE + preempt_offset);\n}\n\nvoid __might_sleep(const char *file, int line, int preempt_offset)\n{\n#ifdef in_atomic\n\tstatic unsigned long prev_jiffy;\t/* ratelimiting */\n\n\tif ((preempt_count_equals(preempt_offset) && !irqs_disabled()) ||\n\t    system_state != SYSTEM_RUNNING || oops_in_progress)\n\t\treturn;\n\tif (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)\n\t\treturn;\n\tprev_jiffy = jiffies;\n\n\tprintk(KERN_ERR\n\t\t\"BUG: sleeping function called from invalid context at %s:%d\\n\",\n\t\t\tfile, line);\n\tprintk(KERN_ERR\n\t\t\"in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\\n\",\n\t\t\tin_atomic(), irqs_disabled(),\n\t\t\tcurrent->pid, current->comm);\n\n\tdebug_show_held_locks(current);\n\tif (irqs_disabled())\n\t\tprint_irqtrace_events(current);\n\tdump_stack();\n#endif\n}\nEXPORT_SYMBOL(__might_sleep);\n#endif\n\n#ifdef CONFIG_MAGIC_SYSRQ\nstatic void normalize_task(struct rq *rq, struct task_struct *p)\n{\n\tint on_rq;\n\n\ton_rq = p->se.on_rq;\n\tif (on_rq)\n\t\tdeactivate_task(rq, p, 0);\n\t__setscheduler(rq, p, SCHED_NORMAL, 0);\n\tif (on_rq) {\n\t\tactivate_task(rq, p, 0);\n\t\tresched_task(rq->curr);\n\t}\n}\n\nvoid normalize_rt_tasks(void)\n{\n\tstruct task_struct *g, *p;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\tread_lock_irqsave(&tasklist_lock, flags);\n\tdo_each_thread(g, p) {\n\t\t/*\n\t\t * Only normalize user tasks:\n\t\t */\n\t\tif (!p->mm)\n\t\t\tcontinue;\n\n\t\tp->se.exec_start\t\t= 0;\n#ifdef CONFIG_SCHEDSTATS\n\t\tp->se.statistics.wait_start\t= 0;\n\t\tp->se.statistics.sleep_start\t= 0;\n\t\tp->se.statistics.block_start\t= 0;\n#endif\n\n\t\tif (!rt_task(p)) {\n\t\t\t/*\n\t\t\t * Renice negative nice level userspace\n\t\t\t * tasks back to 0:\n\t\t\t */\n\t\t\tif (TASK_NICE(p) < 0 && p->mm)\n\t\t\t\tset_user_nice(p, 0);\n\t\t\tcontinue;\n\t\t}\n\n\t\traw_spin_lock(&p->pi_lock);\n\t\trq = __task_rq_lock(p);\n\n\t\tnormalize_task(rq, p);\n\n\t\t__task_rq_unlock(rq);\n\t\traw_spin_unlock(&p->pi_lock);\n\t} while_each_thread(g, p);\n\n\tread_unlock_irqrestore(&tasklist_lock, flags);\n}\n\n#endif /* CONFIG_MAGIC_SYSRQ */\n\n#if defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB)\n/*\n * These functions are only useful for the IA64 MCA handling, or kdb.\n *\n * They can only be called when the whole system has been\n * stopped - every CPU needs to be quiescent, and no scheduling\n * activity can take place. Using them for anything else would\n * be a serious bug, and as a result, they aren't even visible\n * under any other configuration.\n */\n\n/**\n * curr_task - return the current task for a given cpu.\n * @cpu: the processor in question.\n *\n * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!\n */\nstruct task_struct *curr_task(int cpu)\n{\n\treturn cpu_curr(cpu);\n}\n\n#endif /* defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) */\n\n#ifdef CONFIG_IA64\n/**\n * set_curr_task - set the current task for a given cpu.\n * @cpu: the processor in question.\n * @p: the task pointer to set.\n *\n * Description: This function must only be used when non-maskable interrupts\n * are serviced on a separate stack. It allows the architecture to switch the\n * notion of the current task on a cpu in a non-blocking manner. This function\n * must be called with all CPU's synchronized, and interrupts disabled, the\n * and caller must save the original value of the current task (see\n * curr_task() above) and restore that value before reenabling interrupts and\n * re-starting the system.\n *\n * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!\n */\nvoid set_curr_task(int cpu, struct task_struct *p)\n{\n\tcpu_curr(cpu) = p;\n}\n\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void free_fair_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->cfs_rq)\n\t\t\tkfree(tg->cfs_rq[i]);\n\t\tif (tg->se)\n\t\t\tkfree(tg->se[i]);\n\t}\n\n\tkfree(tg->cfs_rq);\n\tkfree(tg->se);\n}\n\nstatic\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se;\n\tstruct rq *rq;\n\tint i;\n\n\ttg->cfs_rq = kzalloc(sizeof(cfs_rq) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->cfs_rq)\n\t\tgoto err;\n\ttg->se = kzalloc(sizeof(se) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->se)\n\t\tgoto err;\n\n\ttg->shares = NICE_0_LOAD;\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\n\t\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(i));\n\t\tif (!cfs_rq)\n\t\t\tgoto err;\n\n\t\tse = kzalloc_node(sizeof(struct sched_entity),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(i));\n\t\tif (!se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_tg_cfs_entry(tg, cfs_rq, se, i, 0, parent->se[i]);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(cfs_rq);\nerr:\n\treturn 0;\n}\n\nstatic inline void register_fair_sched_group(struct task_group *tg, int cpu)\n{\n\tlist_add_rcu(&tg->cfs_rq[cpu]->leaf_cfs_rq_list,\n\t\t\t&cpu_rq(cpu)->leaf_cfs_rq_list);\n}\n\nstatic inline void unregister_fair_sched_group(struct task_group *tg, int cpu)\n{\n\tlist_del_rcu(&tg->cfs_rq[cpu]->leaf_cfs_rq_list);\n}\n#else /* !CONFG_FAIR_GROUP_SCHED */\nstatic inline void free_fair_sched_group(struct task_group *tg)\n{\n}\n\nstatic inline\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nstatic inline void register_fair_sched_group(struct task_group *tg, int cpu)\n{\n}\n\nstatic inline void unregister_fair_sched_group(struct task_group *tg, int cpu)\n{\n}\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void free_rt_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tdestroy_rt_bandwidth(&tg->rt_bandwidth);\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->rt_rq)\n\t\t\tkfree(tg->rt_rq[i]);\n\t\tif (tg->rt_se)\n\t\t\tkfree(tg->rt_se[i]);\n\t}\n\n\tkfree(tg->rt_rq);\n\tkfree(tg->rt_se);\n}\n\nstatic\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct rt_rq *rt_rq;\n\tstruct sched_rt_entity *rt_se;\n\tstruct rq *rq;\n\tint i;\n\n\ttg->rt_rq = kzalloc(sizeof(rt_rq) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->rt_rq)\n\t\tgoto err;\n\ttg->rt_se = kzalloc(sizeof(rt_se) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->rt_se)\n\t\tgoto err;\n\n\tinit_rt_bandwidth(&tg->rt_bandwidth,\n\t\t\tktime_to_ns(def_rt_bandwidth.rt_period), 0);\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\n\t\trt_rq = kzalloc_node(sizeof(struct rt_rq),\n\t\t\t\t     GFP_KERNEL, cpu_to_node(i));\n\t\tif (!rt_rq)\n\t\t\tgoto err;\n\n\t\trt_se = kzalloc_node(sizeof(struct sched_rt_entity),\n\t\t\t\t     GFP_KERNEL, cpu_to_node(i));\n\t\tif (!rt_se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_tg_rt_entry(tg, rt_rq, rt_se, i, 0, parent->rt_se[i]);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(rt_rq);\nerr:\n\treturn 0;\n}\n\nstatic inline void register_rt_sched_group(struct task_group *tg, int cpu)\n{\n\tlist_add_rcu(&tg->rt_rq[cpu]->leaf_rt_rq_list,\n\t\t\t&cpu_rq(cpu)->leaf_rt_rq_list);\n}\n\nstatic inline void unregister_rt_sched_group(struct task_group *tg, int cpu)\n{\n\tlist_del_rcu(&tg->rt_rq[cpu]->leaf_rt_rq_list);\n}\n#else /* !CONFIG_RT_GROUP_SCHED */\nstatic inline void free_rt_sched_group(struct task_group *tg)\n{\n}\n\nstatic inline\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nstatic inline void register_rt_sched_group(struct task_group *tg, int cpu)\n{\n}\n\nstatic inline void unregister_rt_sched_group(struct task_group *tg, int cpu)\n{\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_SCHED\nstatic void free_sched_group(struct task_group *tg)\n{\n\tfree_fair_sched_group(tg);\n\tfree_rt_sched_group(tg);\n\tkfree(tg);\n}\n\n/* allocate runqueue etc for a new task group */\nstruct task_group *sched_create_group(struct task_group *parent)\n{\n\tstruct task_group *tg;\n\tunsigned long flags;\n\tint i;\n\n\ttg = kzalloc(sizeof(*tg), GFP_KERNEL);\n\tif (!tg)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!alloc_fair_sched_group(tg, parent))\n\t\tgoto err;\n\n\tif (!alloc_rt_sched_group(tg, parent))\n\t\tgoto err;\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tfor_each_possible_cpu(i) {\n\t\tregister_fair_sched_group(tg, i);\n\t\tregister_rt_sched_group(tg, i);\n\t}\n\tlist_add_rcu(&tg->list, &task_groups);\n\n\tWARN_ON(!parent); /* root should already exist */\n\n\ttg->parent = parent;\n\tINIT_LIST_HEAD(&tg->children);\n\tlist_add_rcu(&tg->siblings, &parent->children);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\treturn tg;\n\nerr:\n\tfree_sched_group(tg);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/* rcu callback to free various structures associated with a task group */\nstatic void free_sched_group_rcu(struct rcu_head *rhp)\n{\n\t/* now it should be safe to free those cfs_rqs */\n\tfree_sched_group(container_of(rhp, struct task_group, rcu));\n}\n\n/* Destroy runqueue etc associated with a task group */\nvoid sched_destroy_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\tint i;\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tfor_each_possible_cpu(i) {\n\t\tunregister_fair_sched_group(tg, i);\n\t\tunregister_rt_sched_group(tg, i);\n\t}\n\tlist_del_rcu(&tg->list);\n\tlist_del_rcu(&tg->siblings);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\t/* wait for possible concurrent references to cfs_rqs complete */\n\tcall_rcu(&tg->rcu, free_sched_group_rcu);\n}\n\n/* change task's runqueue when it moves between groups.\n *\tThe caller of this function should have put the task in its new group\n *\tby now. This function just updates tsk->se.cfs_rq and tsk->se.parent to\n *\treflect its new group.\n */\nvoid sched_move_task(struct task_struct *tsk)\n{\n\tint on_rq, running;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(tsk, &flags);\n\n\trunning = task_current(rq, tsk);\n\ton_rq = tsk->se.on_rq;\n\n\tif (on_rq)\n\t\tdequeue_task(rq, tsk, 0);\n\tif (unlikely(running))\n\t\ttsk->sched_class->put_prev_task(rq, tsk);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (tsk->sched_class->task_move_group)\n\t\ttsk->sched_class->task_move_group(tsk, on_rq);\n\telse\n#endif\n\t\tset_task_rq(tsk, task_cpu(tsk));\n\n\tif (unlikely(running))\n\t\ttsk->sched_class->set_curr_task(rq);\n\tif (on_rq)\n\t\tenqueue_task(rq, tsk, 0);\n\n\ttask_rq_unlock(rq, &flags);\n}\n#endif /* CONFIG_CGROUP_SCHED */\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void __set_se_shares(struct sched_entity *se, unsigned long shares)\n{\n\tstruct cfs_rq *cfs_rq = se->cfs_rq;\n\tint on_rq;\n\n\ton_rq = se->on_rq;\n\tif (on_rq)\n\t\tdequeue_entity(cfs_rq, se, 0);\n\n\tse->load.weight = shares;\n\tse->load.inv_weight = 0;\n\n\tif (on_rq)\n\t\tenqueue_entity(cfs_rq, se, 0);\n}\n\nstatic void set_se_shares(struct sched_entity *se, unsigned long shares)\n{\n\tstruct cfs_rq *cfs_rq = se->cfs_rq;\n\tstruct rq *rq = cfs_rq->rq;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\t__set_se_shares(se, shares);\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n}\n\nstatic DEFINE_MUTEX(shares_mutex);\n\nint sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\tunsigned long flags;\n\n\t/*\n\t * We can't change the weight of the root cgroup.\n\t */\n\tif (!tg->se[0])\n\t\treturn -EINVAL;\n\n\tif (shares < MIN_SHARES)\n\t\tshares = MIN_SHARES;\n\telse if (shares > MAX_SHARES)\n\t\tshares = MAX_SHARES;\n\n\tmutex_lock(&shares_mutex);\n\tif (tg->shares == shares)\n\t\tgoto done;\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tfor_each_possible_cpu(i)\n\t\tunregister_fair_sched_group(tg, i);\n\tlist_del_rcu(&tg->siblings);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\t/* wait for any ongoing reference to this group to finish */\n\tsynchronize_sched();\n\n\t/*\n\t * Now we are free to modify the group's share on each cpu\n\t * w/o tripping rebalance_share or load_balance_fair.\n\t */\n\ttg->shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\t/*\n\t\t * force a rebalance\n\t\t */\n\t\tcfs_rq_set_shares(tg->cfs_rq[i], 0);\n\t\tset_se_shares(tg->se[i], shares);\n\t}\n\n\t/*\n\t * Enable load balance activity on this group, by inserting it back on\n\t * each cpu's rq->leaf_cfs_rq_list.\n\t */\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tfor_each_possible_cpu(i)\n\t\tregister_fair_sched_group(tg, i);\n\tlist_add_rcu(&tg->siblings, &tg->parent->children);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\ndone:\n\tmutex_unlock(&shares_mutex);\n\treturn 0;\n}\n\nunsigned long sched_group_shares(struct task_group *tg)\n{\n\treturn tg->shares;\n}\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n/*\n * Ensure that the real time constraints are schedulable.\n */\nstatic DEFINE_MUTEX(rt_constraints_mutex);\n\nstatic unsigned long to_ratio(u64 period, u64 runtime)\n{\n\tif (runtime == RUNTIME_INF)\n\t\treturn 1ULL << 20;\n\n\treturn div64_u64(runtime << 20, period);\n}\n\n/* Must be called with tasklist_lock held */\nstatic inline int tg_has_rt_tasks(struct task_group *tg)\n{\n\tstruct task_struct *g, *p;\n\n\tdo_each_thread(g, p) {\n\t\tif (rt_task(p) && rt_rq_of_se(&p->rt)->tg == tg)\n\t\t\treturn 1;\n\t} while_each_thread(g, p);\n\n\treturn 0;\n}\n\nstruct rt_schedulable_data {\n\tstruct task_group *tg;\n\tu64 rt_period;\n\tu64 rt_runtime;\n};\n\nstatic int tg_schedulable(struct task_group *tg, void *data)\n{\n\tstruct rt_schedulable_data *d = data;\n\tstruct task_group *child;\n\tunsigned long total, sum = 0;\n\tu64 period, runtime;\n\n\tperiod = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\truntime = tg->rt_bandwidth.rt_runtime;\n\n\tif (tg == d->tg) {\n\t\tperiod = d->rt_period;\n\t\truntime = d->rt_runtime;\n\t}\n\n\t/*\n\t * Cannot have more runtime than the period.\n\t */\n\tif (runtime > period && runtime != RUNTIME_INF)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Ensure we don't starve existing RT tasks.\n\t */\n\tif (rt_bandwidth_enabled() && !runtime && tg_has_rt_tasks(tg))\n\t\treturn -EBUSY;\n\n\ttotal = to_ratio(period, runtime);\n\n\t/*\n\t * Nobody can have more than the global setting allows.\n\t */\n\tif (total > to_ratio(global_rt_period(), global_rt_runtime()))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The sum of our children's runtime should not exceed our own.\n\t */\n\tlist_for_each_entry_rcu(child, &tg->children, siblings) {\n\t\tperiod = ktime_to_ns(child->rt_bandwidth.rt_period);\n\t\truntime = child->rt_bandwidth.rt_runtime;\n\n\t\tif (child == d->tg) {\n\t\t\tperiod = d->rt_period;\n\t\t\truntime = d->rt_runtime;\n\t\t}\n\n\t\tsum += to_ratio(period, runtime);\n\t}\n\n\tif (sum > total)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int __rt_schedulable(struct task_group *tg, u64 period, u64 runtime)\n{\n\tstruct rt_schedulable_data data = {\n\t\t.tg = tg,\n\t\t.rt_period = period,\n\t\t.rt_runtime = runtime,\n\t};\n\n\treturn walk_tg_tree(tg_schedulable, tg_nop, &data);\n}\n\nstatic int tg_set_bandwidth(struct task_group *tg,\n\t\tu64 rt_period, u64 rt_runtime)\n{\n\tint i, err = 0;\n\n\tmutex_lock(&rt_constraints_mutex);\n\tread_lock(&tasklist_lock);\n\terr = __rt_schedulable(tg, rt_period, rt_runtime);\n\tif (err)\n\t\tgoto unlock;\n\n\traw_spin_lock_irq(&tg->rt_bandwidth.rt_runtime_lock);\n\ttg->rt_bandwidth.rt_period = ns_to_ktime(rt_period);\n\ttg->rt_bandwidth.rt_runtime = rt_runtime;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rt_rq *rt_rq = tg->rt_rq[i];\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = rt_runtime;\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t}\n\traw_spin_unlock_irq(&tg->rt_bandwidth.rt_runtime_lock);\nunlock:\n\tread_unlock(&tasklist_lock);\n\tmutex_unlock(&rt_constraints_mutex);\n\n\treturn err;\n}\n\nint sched_group_set_rt_runtime(struct task_group *tg, long rt_runtime_us)\n{\n\tu64 rt_runtime, rt_period;\n\n\trt_period = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\trt_runtime = (u64)rt_runtime_us * NSEC_PER_USEC;\n\tif (rt_runtime_us < 0)\n\t\trt_runtime = RUNTIME_INF;\n\n\treturn tg_set_bandwidth(tg, rt_period, rt_runtime);\n}\n\nlong sched_group_rt_runtime(struct task_group *tg)\n{\n\tu64 rt_runtime_us;\n\n\tif (tg->rt_bandwidth.rt_runtime == RUNTIME_INF)\n\t\treturn -1;\n\n\trt_runtime_us = tg->rt_bandwidth.rt_runtime;\n\tdo_div(rt_runtime_us, NSEC_PER_USEC);\n\treturn rt_runtime_us;\n}\n\nint sched_group_set_rt_period(struct task_group *tg, long rt_period_us)\n{\n\tu64 rt_runtime, rt_period;\n\n\trt_period = (u64)rt_period_us * NSEC_PER_USEC;\n\trt_runtime = tg->rt_bandwidth.rt_runtime;\n\n\tif (rt_period == 0)\n\t\treturn -EINVAL;\n\n\treturn tg_set_bandwidth(tg, rt_period, rt_runtime);\n}\n\nlong sched_group_rt_period(struct task_group *tg)\n{\n\tu64 rt_period_us;\n\n\trt_period_us = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\tdo_div(rt_period_us, NSEC_PER_USEC);\n\treturn rt_period_us;\n}\n\nstatic int sched_rt_global_constraints(void)\n{\n\tu64 runtime, period;\n\tint ret = 0;\n\n\tif (sysctl_sched_rt_period <= 0)\n\t\treturn -EINVAL;\n\n\truntime = global_rt_runtime();\n\tperiod = global_rt_period();\n\n\t/*\n\t * Sanity check on the sysctl variables.\n\t */\n\tif (runtime > period && runtime != RUNTIME_INF)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rt_constraints_mutex);\n\tread_lock(&tasklist_lock);\n\tret = __rt_schedulable(NULL, 0, 0);\n\tread_unlock(&tasklist_lock);\n\tmutex_unlock(&rt_constraints_mutex);\n\n\treturn ret;\n}\n\nint sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk)\n{\n\t/* Don't accept realtime tasks when there is no way for them to run */\n\tif (rt_task(tsk) && tg->rt_bandwidth.rt_runtime == 0)\n\t\treturn 0;\n\n\treturn 1;\n}\n\n#else /* !CONFIG_RT_GROUP_SCHED */\nstatic int sched_rt_global_constraints(void)\n{\n\tunsigned long flags;\n\tint i;\n\n\tif (sysctl_sched_rt_period <= 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * There's always some RT tasks in the root group\n\t * -- migration, kstopmachine etc..\n\t */\n\tif (sysctl_sched_rt_runtime == 0)\n\t\treturn -EBUSY;\n\n\traw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);\n\tfor_each_possible_cpu(i) {\n\t\tstruct rt_rq *rt_rq = &cpu_rq(i)->rt;\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = global_rt_runtime();\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t}\n\traw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);\n\n\treturn 0;\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\nint sched_rt_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos)\n{\n\tint ret;\n\tint old_period, old_runtime;\n\tstatic DEFINE_MUTEX(mutex);\n\n\tmutex_lock(&mutex);\n\told_period = sysctl_sched_rt_period;\n\told_runtime = sysctl_sched_rt_runtime;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\tif (!ret && write) {\n\t\tret = sched_rt_global_constraints();\n\t\tif (ret) {\n\t\t\tsysctl_sched_rt_period = old_period;\n\t\t\tsysctl_sched_rt_runtime = old_runtime;\n\t\t} else {\n\t\t\tdef_rt_bandwidth.rt_runtime = global_rt_runtime();\n\t\t\tdef_rt_bandwidth.rt_period =\n\t\t\t\tns_to_ktime(global_rt_period());\n\t\t}\n\t}\n\tmutex_unlock(&mutex);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_CGROUP_SCHED\n\n/* return corresponding task_group object of a cgroup */\nstatic inline struct task_group *cgroup_tg(struct cgroup *cgrp)\n{\n\treturn container_of(cgroup_subsys_state(cgrp, cpu_cgroup_subsys_id),\n\t\t\t    struct task_group, css);\n}\n\nstatic struct cgroup_subsys_state *\ncpu_cgroup_create(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct task_group *tg, *parent;\n\n\tif (!cgrp->parent) {\n\t\t/* This is early initialization for the top cgroup */\n\t\treturn &init_task_group.css;\n\t}\n\n\tparent = cgroup_tg(cgrp->parent);\n\ttg = sched_create_group(parent);\n\tif (IS_ERR(tg))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn &tg->css;\n}\n\nstatic void\ncpu_cgroup_destroy(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct task_group *tg = cgroup_tg(cgrp);\n\n\tsched_destroy_group(tg);\n}\n\nstatic int\ncpu_cgroup_can_attach_task(struct cgroup *cgrp, struct task_struct *tsk)\n{\n#ifdef CONFIG_RT_GROUP_SCHED\n\tif (!sched_rt_can_attach(cgroup_tg(cgrp), tsk))\n\t\treturn -EINVAL;\n#else\n\t/* We don't support RT-tasks being in separate groups */\n\tif (tsk->sched_class != &fair_sched_class)\n\t\treturn -EINVAL;\n#endif\n\treturn 0;\n}\n\nstatic int\ncpu_cgroup_can_attach(struct cgroup_subsys *ss, struct cgroup *cgrp,\n\t\t      struct task_struct *tsk, bool threadgroup)\n{\n\tint retval = cpu_cgroup_can_attach_task(cgrp, tsk);\n\tif (retval)\n\t\treturn retval;\n\tif (threadgroup) {\n\t\tstruct task_struct *c;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(c, &tsk->thread_group, thread_group) {\n\t\t\tretval = cpu_cgroup_can_attach_task(cgrp, c);\n\t\t\tif (retval) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn retval;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\treturn 0;\n}\n\nstatic void\ncpu_cgroup_attach(struct cgroup_subsys *ss, struct cgroup *cgrp,\n\t\t  struct cgroup *old_cont, struct task_struct *tsk,\n\t\t  bool threadgroup)\n{\n\tsched_move_task(tsk);\n\tif (threadgroup) {\n\t\tstruct task_struct *c;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(c, &tsk->thread_group, thread_group) {\n\t\t\tsched_move_task(c);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic int cpu_shares_write_u64(struct cgroup *cgrp, struct cftype *cftype,\n\t\t\t\tu64 shareval)\n{\n\treturn sched_group_set_shares(cgroup_tg(cgrp), shareval);\n}\n\nstatic u64 cpu_shares_read_u64(struct cgroup *cgrp, struct cftype *cft)\n{\n\tstruct task_group *tg = cgroup_tg(cgrp);\n\n\treturn (u64) tg->shares;\n}\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic int cpu_rt_runtime_write(struct cgroup *cgrp, struct cftype *cft,\n\t\t\t\ts64 val)\n{\n\treturn sched_group_set_rt_runtime(cgroup_tg(cgrp), val);\n}\n\nstatic s64 cpu_rt_runtime_read(struct cgroup *cgrp, struct cftype *cft)\n{\n\treturn sched_group_rt_runtime(cgroup_tg(cgrp));\n}\n\nstatic int cpu_rt_period_write_uint(struct cgroup *cgrp, struct cftype *cftype,\n\t\tu64 rt_period_us)\n{\n\treturn sched_group_set_rt_period(cgroup_tg(cgrp), rt_period_us);\n}\n\nstatic u64 cpu_rt_period_read_uint(struct cgroup *cgrp, struct cftype *cft)\n{\n\treturn sched_group_rt_period(cgroup_tg(cgrp));\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\nstatic struct cftype cpu_files[] = {\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t{\n\t\t.name = \"shares\",\n\t\t.read_u64 = cpu_shares_read_u64,\n\t\t.write_u64 = cpu_shares_write_u64,\n\t},\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\t{\n\t\t.name = \"rt_runtime_us\",\n\t\t.read_s64 = cpu_rt_runtime_read,\n\t\t.write_s64 = cpu_rt_runtime_write,\n\t},\n\t{\n\t\t.name = \"rt_period_us\",\n\t\t.read_u64 = cpu_rt_period_read_uint,\n\t\t.write_u64 = cpu_rt_period_write_uint,\n\t},\n#endif\n};\n\nstatic int cpu_cgroup_populate(struct cgroup_subsys *ss, struct cgroup *cont)\n{\n\treturn cgroup_add_files(cont, ss, cpu_files, ARRAY_SIZE(cpu_files));\n}\n\nstruct cgroup_subsys cpu_cgroup_subsys = {\n\t.name\t\t= \"cpu\",\n\t.create\t\t= cpu_cgroup_create,\n\t.destroy\t= cpu_cgroup_destroy,\n\t.can_attach\t= cpu_cgroup_can_attach,\n\t.attach\t\t= cpu_cgroup_attach,\n\t.populate\t= cpu_cgroup_populate,\n\t.subsys_id\t= cpu_cgroup_subsys_id,\n\t.early_init\t= 1,\n};\n\n#endif\t/* CONFIG_CGROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_CPUACCT\n\n/*\n * CPU accounting code for task groups.\n *\n * Based on the work by Paul Menage (menage@google.com) and Balbir Singh\n * (balbir@in.ibm.com).\n */\n\n/* track cpu usage of a group of tasks and its child groups */\nstruct cpuacct {\n\tstruct cgroup_subsys_state css;\n\t/* cpuusage holds pointer to a u64-type object on every cpu */\n\tu64 __percpu *cpuusage;\n\tstruct percpu_counter cpustat[CPUACCT_STAT_NSTATS];\n\tstruct cpuacct *parent;\n};\n\nstruct cgroup_subsys cpuacct_subsys;\n\n/* return cpu accounting group corresponding to this container */\nstatic inline struct cpuacct *cgroup_ca(struct cgroup *cgrp)\n{\n\treturn container_of(cgroup_subsys_state(cgrp, cpuacct_subsys_id),\n\t\t\t    struct cpuacct, css);\n}\n\n/* return cpu accounting group to which this task belongs */\nstatic inline struct cpuacct *task_ca(struct task_struct *tsk)\n{\n\treturn container_of(task_subsys_state(tsk, cpuacct_subsys_id),\n\t\t\t    struct cpuacct, css);\n}\n\n/* create a new cpu accounting group */\nstatic struct cgroup_subsys_state *cpuacct_create(\n\tstruct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct cpuacct *ca = kzalloc(sizeof(*ca), GFP_KERNEL);\n\tint i;\n\n\tif (!ca)\n\t\tgoto out;\n\n\tca->cpuusage = alloc_percpu(u64);\n\tif (!ca->cpuusage)\n\t\tgoto out_free_ca;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++)\n\t\tif (percpu_counter_init(&ca->cpustat[i], 0))\n\t\t\tgoto out_free_counters;\n\n\tif (cgrp->parent)\n\t\tca->parent = cgroup_ca(cgrp->parent);\n\n\treturn &ca->css;\n\nout_free_counters:\n\twhile (--i >= 0)\n\t\tpercpu_counter_destroy(&ca->cpustat[i]);\n\tfree_percpu(ca->cpuusage);\nout_free_ca:\n\tkfree(ca);\nout:\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/* destroy an existing cpu accounting group */\nstatic void\ncpuacct_destroy(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint i;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++)\n\t\tpercpu_counter_destroy(&ca->cpustat[i]);\n\tfree_percpu(ca->cpuusage);\n\tkfree(ca);\n}\n\nstatic u64 cpuacct_cpuusage_read(struct cpuacct *ca, int cpu)\n{\n\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\tu64 data;\n\n#ifndef CONFIG_64BIT\n\t/*\n\t * Take rq->lock to make 64-bit read safe on 32-bit platforms.\n\t */\n\traw_spin_lock_irq(&cpu_rq(cpu)->lock);\n\tdata = *cpuusage;\n\traw_spin_unlock_irq(&cpu_rq(cpu)->lock);\n#else\n\tdata = *cpuusage;\n#endif\n\n\treturn data;\n}\n\nstatic void cpuacct_cpuusage_write(struct cpuacct *ca, int cpu, u64 val)\n{\n\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\n#ifndef CONFIG_64BIT\n\t/*\n\t * Take rq->lock to make 64-bit write safe on 32-bit platforms.\n\t */\n\traw_spin_lock_irq(&cpu_rq(cpu)->lock);\n\t*cpuusage = val;\n\traw_spin_unlock_irq(&cpu_rq(cpu)->lock);\n#else\n\t*cpuusage = val;\n#endif\n}\n\n/* return total cpu usage (in nanoseconds) of a group */\nstatic u64 cpuusage_read(struct cgroup *cgrp, struct cftype *cft)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tu64 totalcpuusage = 0;\n\tint i;\n\n\tfor_each_present_cpu(i)\n\t\ttotalcpuusage += cpuacct_cpuusage_read(ca, i);\n\n\treturn totalcpuusage;\n}\n\nstatic int cpuusage_write(struct cgroup *cgrp, struct cftype *cftype,\n\t\t\t\t\t\t\t\tu64 reset)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint err = 0;\n\tint i;\n\n\tif (reset) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tfor_each_present_cpu(i)\n\t\tcpuacct_cpuusage_write(ca, i, 0);\n\nout:\n\treturn err;\n}\n\nstatic int cpuacct_percpu_seq_read(struct cgroup *cgroup, struct cftype *cft,\n\t\t\t\t   struct seq_file *m)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgroup);\n\tu64 percpu;\n\tint i;\n\n\tfor_each_present_cpu(i) {\n\t\tpercpu = cpuacct_cpuusage_read(ca, i);\n\t\tseq_printf(m, \"%llu \", (unsigned long long) percpu);\n\t}\n\tseq_printf(m, \"\\n\");\n\treturn 0;\n}\n\nstatic const char *cpuacct_stat_desc[] = {\n\t[CPUACCT_STAT_USER] = \"user\",\n\t[CPUACCT_STAT_SYSTEM] = \"system\",\n};\n\nstatic int cpuacct_stats_show(struct cgroup *cgrp, struct cftype *cft,\n\t\tstruct cgroup_map_cb *cb)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint i;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++) {\n\t\ts64 val = percpu_counter_read(&ca->cpustat[i]);\n\t\tval = cputime64_to_clock_t(val);\n\t\tcb->fill(cb, cpuacct_stat_desc[i], val);\n\t}\n\treturn 0;\n}\n\nstatic struct cftype files[] = {\n\t{\n\t\t.name = \"usage\",\n\t\t.read_u64 = cpuusage_read,\n\t\t.write_u64 = cpuusage_write,\n\t},\n\t{\n\t\t.name = \"usage_percpu\",\n\t\t.read_seq_string = cpuacct_percpu_seq_read,\n\t},\n\t{\n\t\t.name = \"stat\",\n\t\t.read_map = cpuacct_stats_show,\n\t},\n};\n\nstatic int cpuacct_populate(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\treturn cgroup_add_files(cgrp, ss, files, ARRAY_SIZE(files));\n}\n\n/*\n * charge this task's execution time to its accounting group.\n *\n * called with rq->lock held.\n */\nstatic void cpuacct_charge(struct task_struct *tsk, u64 cputime)\n{\n\tstruct cpuacct *ca;\n\tint cpu;\n\n\tif (unlikely(!cpuacct_subsys.active))\n\t\treturn;\n\n\tcpu = task_cpu(tsk);\n\n\trcu_read_lock();\n\n\tca = task_ca(tsk);\n\n\tfor (; ca; ca = ca->parent) {\n\t\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\t\t*cpuusage += cputime;\n\t}\n\n\trcu_read_unlock();\n}\n\n/*\n * When CONFIG_VIRT_CPU_ACCOUNTING is enabled one jiffy can be very large\n * in cputime_t units. As a result, cpuacct_update_stats calls\n * percpu_counter_add with values large enough to always overflow the\n * per cpu batch limit causing bad SMP scalability.\n *\n * To fix this we scale percpu_counter_batch by cputime_one_jiffy so we\n * batch the same amount of time with CONFIG_VIRT_CPU_ACCOUNTING disabled\n * and enabled. We cap it at INT_MAX which is the largest allowed batch value.\n */\n#ifdef CONFIG_SMP\n#define CPUACCT_BATCH\t\\\n\tmin_t(long, percpu_counter_batch * cputime_one_jiffy, INT_MAX)\n#else\n#define CPUACCT_BATCH\t0\n#endif\n\n/*\n * Charge the system/user time to the task's accounting group.\n */\nstatic void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val)\n{\n\tstruct cpuacct *ca;\n\tint batch = CPUACCT_BATCH;\n\n\tif (unlikely(!cpuacct_subsys.active))\n\t\treturn;\n\n\trcu_read_lock();\n\tca = task_ca(tsk);\n\n\tdo {\n\t\t__percpu_counter_add(&ca->cpustat[idx], val, batch);\n\t\tca = ca->parent;\n\t} while (ca);\n\trcu_read_unlock();\n}\n\nstruct cgroup_subsys cpuacct_subsys = {\n\t.name = \"cpuacct\",\n\t.create = cpuacct_create,\n\t.destroy = cpuacct_destroy,\n\t.populate = cpuacct_populate,\n\t.subsys_id = cpuacct_subsys_id,\n};\n#endif\t/* CONFIG_CGROUP_CPUACCT */\n\n#ifndef CONFIG_SMP\n\nvoid synchronize_sched_expedited(void)\n{\n\tbarrier();\n}\nEXPORT_SYMBOL_GPL(synchronize_sched_expedited);\n\n#else /* #ifndef CONFIG_SMP */\n\nstatic atomic_t synchronize_sched_expedited_count = ATOMIC_INIT(0);\n\nstatic int synchronize_sched_expedited_cpu_stop(void *data)\n{\n\t/*\n\t * There must be a full memory barrier on each affected CPU\n\t * between the time that try_stop_cpus() is called and the\n\t * time that it returns.\n\t *\n\t * In the current initial implementation of cpu_stop, the\n\t * above condition is already met when the control reaches\n\t * this point and the following smp_mb() is not strictly\n\t * necessary.  Do smp_mb() anyway for documentation and\n\t * robustness against future implementation changes.\n\t */\n\tsmp_mb(); /* See above comment block. */\n\treturn 0;\n}\n\n/*\n * Wait for an rcu-sched grace period to elapse, but use \"big hammer\"\n * approach to force grace period to end quickly.  This consumes\n * significant time on all CPUs, and is thus not recommended for\n * any sort of common-case code.\n *\n * Note that it is illegal to call this function while holding any\n * lock that is acquired by a CPU-hotplug notifier.  Failing to\n * observe this restriction will result in deadlock.\n */\nvoid synchronize_sched_expedited(void)\n{\n\tint snap, trycount = 0;\n\n\tsmp_mb();  /* ensure prior mod happens before capturing snap. */\n\tsnap = atomic_read(&synchronize_sched_expedited_count) + 1;\n\tget_online_cpus();\n\twhile (try_stop_cpus(cpu_online_mask,\n\t\t\t     synchronize_sched_expedited_cpu_stop,\n\t\t\t     NULL) == -EAGAIN) {\n\t\tput_online_cpus();\n\t\tif (trycount++ < 10)\n\t\t\tudelay(trycount * num_online_cpus());\n\t\telse {\n\t\t\tsynchronize_sched();\n\t\t\treturn;\n\t\t}\n\t\tif (atomic_read(&synchronize_sched_expedited_count) - snap > 0) {\n\t\t\tsmp_mb(); /* ensure test happens before caller kfree */\n\t\t\treturn;\n\t\t}\n\t\tget_online_cpus();\n\t}\n\tatomic_inc(&synchronize_sched_expedited_count);\n\tsmp_mb__after_atomic_inc(); /* ensure post-GP actions seen after GP. */\n\tput_online_cpus();\n}\nEXPORT_SYMBOL_GPL(synchronize_sched_expedited);\n\n#endif /* #else #ifndef CONFIG_SMP */\n"], "fixing_code": ["/*\n *  linux/kernel/fork.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n/*\n *  'fork.c' contains the help-routines for the 'fork' system call\n * (see also entry.S and others).\n * Fork is rather simple, once you get the hang of it, but the memory\n * management can be a bitch. See 'mm/memory.c': 'copy_page_range()'\n */\n\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/unistd.h>\n#include <linux/module.h>\n#include <linux/vmalloc.h>\n#include <linux/completion.h>\n#include <linux/personality.h>\n#include <linux/mempolicy.h>\n#include <linux/sem.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/iocontext.h>\n#include <linux/key.h>\n#include <linux/binfmts.h>\n#include <linux/mman.h>\n#include <linux/mmu_notifier.h>\n#include <linux/fs.h>\n#include <linux/nsproxy.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/cgroup.h>\n#include <linux/security.h>\n#include <linux/hugetlb.h>\n#include <linux/swap.h>\n#include <linux/syscalls.h>\n#include <linux/jiffies.h>\n#include <linux/tracehook.h>\n#include <linux/futex.h>\n#include <linux/compat.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/rcupdate.h>\n#include <linux/ptrace.h>\n#include <linux/mount.h>\n#include <linux/audit.h>\n#include <linux/memcontrol.h>\n#include <linux/ftrace.h>\n#include <linux/profile.h>\n#include <linux/rmap.h>\n#include <linux/ksm.h>\n#include <linux/acct.h>\n#include <linux/tsacct_kern.h>\n#include <linux/cn_proc.h>\n#include <linux/freezer.h>\n#include <linux/delayacct.h>\n#include <linux/taskstats_kern.h>\n#include <linux/random.h>\n#include <linux/tty.h>\n#include <linux/proc_fs.h>\n#include <linux/blkdev.h>\n#include <linux/fs_struct.h>\n#include <linux/magic.h>\n#include <linux/perf_event.h>\n#include <linux/posix-timers.h>\n#include <linux/user-return-notifier.h>\n#include <linux/oom.h>\n\n#include <asm/pgtable.h>\n#include <asm/pgalloc.h>\n#include <asm/uaccess.h>\n#include <asm/mmu_context.h>\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n\n#include <trace/events/sched.h>\n\n/*\n * Protected counters by write_lock_irq(&tasklist_lock)\n */\nunsigned long total_forks;\t/* Handle normal Linux uptimes. */\nint nr_threads; \t\t/* The idle threads do not count.. */\n\nint max_threads;\t\t/* tunable limit on nr_threads */\n\nDEFINE_PER_CPU(unsigned long, process_counts) = 0;\n\n__cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */\n\n#ifdef CONFIG_PROVE_RCU\nint lockdep_tasklist_lock_is_held(void)\n{\n\treturn lockdep_is_held(&tasklist_lock);\n}\nEXPORT_SYMBOL_GPL(lockdep_tasklist_lock_is_held);\n#endif /* #ifdef CONFIG_PROVE_RCU */\n\nint nr_processes(void)\n{\n\tint cpu;\n\tint total = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\ttotal += per_cpu(process_counts, cpu);\n\n\treturn total;\n}\n\n#ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR\n# define alloc_task_struct()\tkmem_cache_alloc(task_struct_cachep, GFP_KERNEL)\n# define free_task_struct(tsk)\tkmem_cache_free(task_struct_cachep, (tsk))\nstatic struct kmem_cache *task_struct_cachep;\n#endif\n\n#ifndef __HAVE_ARCH_THREAD_INFO_ALLOCATOR\nstatic inline struct thread_info *alloc_thread_info(struct task_struct *tsk)\n{\n#ifdef CONFIG_DEBUG_STACK_USAGE\n\tgfp_t mask = GFP_KERNEL | __GFP_ZERO;\n#else\n\tgfp_t mask = GFP_KERNEL;\n#endif\n\treturn (struct thread_info *)__get_free_pages(mask, THREAD_SIZE_ORDER);\n}\n\nstatic inline void free_thread_info(struct thread_info *ti)\n{\n\tfree_pages((unsigned long)ti, THREAD_SIZE_ORDER);\n}\n#endif\n\n/* SLAB cache for signal_struct structures (tsk->signal) */\nstatic struct kmem_cache *signal_cachep;\n\n/* SLAB cache for sighand_struct structures (tsk->sighand) */\nstruct kmem_cache *sighand_cachep;\n\n/* SLAB cache for files_struct structures (tsk->files) */\nstruct kmem_cache *files_cachep;\n\n/* SLAB cache for fs_struct structures (tsk->fs) */\nstruct kmem_cache *fs_cachep;\n\n/* SLAB cache for vm_area_struct structures */\nstruct kmem_cache *vm_area_cachep;\n\n/* SLAB cache for mm_struct structures (tsk->mm) */\nstatic struct kmem_cache *mm_cachep;\n\nstatic void account_kernel_stack(struct thread_info *ti, int account)\n{\n\tstruct zone *zone = page_zone(virt_to_page(ti));\n\n\tmod_zone_page_state(zone, NR_KERNEL_STACK, account);\n}\n\nvoid free_task(struct task_struct *tsk)\n{\n\tprop_local_destroy_single(&tsk->dirties);\n\taccount_kernel_stack(tsk->stack, -1);\n\tfree_thread_info(tsk->stack);\n\trt_mutex_debug_task_free(tsk);\n\tftrace_graph_exit_task(tsk);\n\tfree_task_struct(tsk);\n}\nEXPORT_SYMBOL(free_task);\n\nstatic inline void free_signal_struct(struct signal_struct *sig)\n{\n\ttaskstats_tgid_free(sig);\n\tkmem_cache_free(signal_cachep, sig);\n}\n\nstatic inline void put_signal_struct(struct signal_struct *sig)\n{\n\tif (atomic_dec_and_test(&sig->sigcnt))\n\t\tfree_signal_struct(sig);\n}\n\nvoid __put_task_struct(struct task_struct *tsk)\n{\n\tWARN_ON(!tsk->exit_state);\n\tWARN_ON(atomic_read(&tsk->usage));\n\tWARN_ON(tsk == current);\n\n\texit_creds(tsk);\n\tdelayacct_tsk_free(tsk);\n\tput_signal_struct(tsk->signal);\n\n\tif (!profile_handoff_task(tsk))\n\t\tfree_task(tsk);\n}\n\n/*\n * macro override instead of weak attribute alias, to workaround\n * gcc 4.1.0 and 4.1.1 bugs with weak attribute and empty functions.\n */\n#ifndef arch_task_cache_init\n#define arch_task_cache_init()\n#endif\n\nvoid __init fork_init(unsigned long mempages)\n{\n#ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR\n#ifndef ARCH_MIN_TASKALIGN\n#define ARCH_MIN_TASKALIGN\tL1_CACHE_BYTES\n#endif\n\t/* create a slab on which task_structs can be allocated */\n\ttask_struct_cachep =\n\t\tkmem_cache_create(\"task_struct\", sizeof(struct task_struct),\n\t\t\tARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);\n#endif\n\n\t/* do the arch specific task caches init */\n\tarch_task_cache_init();\n\n\t/*\n\t * The default maximum number of threads is set to a safe\n\t * value: the thread structures can take up at most half\n\t * of memory.\n\t */\n\tmax_threads = mempages / (8 * THREAD_SIZE / PAGE_SIZE);\n\n\t/*\n\t * we need to allow at least 20 threads to boot a system\n\t */\n\tif(max_threads < 20)\n\t\tmax_threads = 20;\n\n\tinit_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads/2;\n\tinit_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads/2;\n\tinit_task.signal->rlim[RLIMIT_SIGPENDING] =\n\t\tinit_task.signal->rlim[RLIMIT_NPROC];\n}\n\nint __attribute__((weak)) arch_dup_task_struct(struct task_struct *dst,\n\t\t\t\t\t       struct task_struct *src)\n{\n\t*dst = *src;\n\treturn 0;\n}\n\nstatic struct task_struct *dup_task_struct(struct task_struct *orig)\n{\n\tstruct task_struct *tsk;\n\tstruct thread_info *ti;\n\tunsigned long *stackend;\n\n\tint err;\n\n\tprepare_to_copy(orig);\n\n\ttsk = alloc_task_struct();\n\tif (!tsk)\n\t\treturn NULL;\n\n\tti = alloc_thread_info(tsk);\n\tif (!ti) {\n\t\tfree_task_struct(tsk);\n\t\treturn NULL;\n\t}\n\n \terr = arch_dup_task_struct(tsk, orig);\n\tif (err)\n\t\tgoto out;\n\n\ttsk->stack = ti;\n\n\terr = prop_local_init_single(&tsk->dirties);\n\tif (err)\n\t\tgoto out;\n\n\tsetup_thread_stack(tsk, orig);\n\tclear_user_return_notifier(tsk);\n\tclear_tsk_need_resched(tsk);\n\tstackend = end_of_stack(tsk);\n\t*stackend = STACK_END_MAGIC;\t/* for overflow detection */\n\n#ifdef CONFIG_CC_STACKPROTECTOR\n\ttsk->stack_canary = get_random_int();\n#endif\n\n\t/* One for us, one for whoever does the \"release_task()\" (usually parent) */\n\tatomic_set(&tsk->usage,2);\n\tatomic_set(&tsk->fs_excl, 0);\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\ttsk->btrace_seq = 0;\n#endif\n\ttsk->splice_pipe = NULL;\n\n\taccount_kernel_stack(ti, 1);\n\n\treturn tsk;\n\nout:\n\tfree_thread_info(ti);\n\tfree_task_struct(tsk);\n\treturn NULL;\n}\n\n#ifdef CONFIG_MMU\nstatic int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)\n{\n\tstruct vm_area_struct *mpnt, *tmp, *prev, **pprev;\n\tstruct rb_node **rb_link, *rb_parent;\n\tint retval;\n\tunsigned long charge;\n\tstruct mempolicy *pol;\n\n\tdown_write(&oldmm->mmap_sem);\n\tflush_cache_dup_mm(oldmm);\n\t/*\n\t * Not linked in yet - no deadlock potential:\n\t */\n\tdown_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);\n\n\tmm->locked_vm = 0;\n\tmm->mmap = NULL;\n\tmm->mmap_cache = NULL;\n\tmm->free_area_cache = oldmm->mmap_base;\n\tmm->cached_hole_size = ~0UL;\n\tmm->map_count = 0;\n\tcpumask_clear(mm_cpumask(mm));\n\tmm->mm_rb = RB_ROOT;\n\trb_link = &mm->mm_rb.rb_node;\n\trb_parent = NULL;\n\tpprev = &mm->mmap;\n\tretval = ksm_fork(mm, oldmm);\n\tif (retval)\n\t\tgoto out;\n\n\tprev = NULL;\n\tfor (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {\n\t\tstruct file *file;\n\n\t\tif (mpnt->vm_flags & VM_DONTCOPY) {\n\t\t\tlong pages = vma_pages(mpnt);\n\t\t\tmm->total_vm -= pages;\n\t\t\tvm_stat_account(mm, mpnt->vm_flags, mpnt->vm_file,\n\t\t\t\t\t\t\t\t-pages);\n\t\t\tcontinue;\n\t\t}\n\t\tcharge = 0;\n\t\tif (mpnt->vm_flags & VM_ACCOUNT) {\n\t\t\tunsigned int len = (mpnt->vm_end - mpnt->vm_start) >> PAGE_SHIFT;\n\t\t\tif (security_vm_enough_memory(len))\n\t\t\t\tgoto fail_nomem;\n\t\t\tcharge = len;\n\t\t}\n\t\ttmp = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);\n\t\tif (!tmp)\n\t\t\tgoto fail_nomem;\n\t\t*tmp = *mpnt;\n\t\tINIT_LIST_HEAD(&tmp->anon_vma_chain);\n\t\tpol = mpol_dup(vma_policy(mpnt));\n\t\tretval = PTR_ERR(pol);\n\t\tif (IS_ERR(pol))\n\t\t\tgoto fail_nomem_policy;\n\t\tvma_set_policy(tmp, pol);\n\t\ttmp->vm_mm = mm;\n\t\tif (anon_vma_fork(tmp, mpnt))\n\t\t\tgoto fail_nomem_anon_vma_fork;\n\t\ttmp->vm_flags &= ~VM_LOCKED;\n\t\ttmp->vm_next = tmp->vm_prev = NULL;\n\t\tfile = tmp->vm_file;\n\t\tif (file) {\n\t\t\tstruct inode *inode = file->f_path.dentry->d_inode;\n\t\t\tstruct address_space *mapping = file->f_mapping;\n\n\t\t\tget_file(file);\n\t\t\tif (tmp->vm_flags & VM_DENYWRITE)\n\t\t\t\tatomic_dec(&inode->i_writecount);\n\t\t\tspin_lock(&mapping->i_mmap_lock);\n\t\t\tif (tmp->vm_flags & VM_SHARED)\n\t\t\t\tmapping->i_mmap_writable++;\n\t\t\ttmp->vm_truncate_count = mpnt->vm_truncate_count;\n\t\t\tflush_dcache_mmap_lock(mapping);\n\t\t\t/* insert tmp into the share list, just after mpnt */\n\t\t\tvma_prio_tree_add(tmp, mpnt);\n\t\t\tflush_dcache_mmap_unlock(mapping);\n\t\t\tspin_unlock(&mapping->i_mmap_lock);\n\t\t}\n\n\t\t/*\n\t\t * Clear hugetlb-related page reserves for children. This only\n\t\t * affects MAP_PRIVATE mappings. Faults generated by the child\n\t\t * are not guaranteed to succeed, even if read-only\n\t\t */\n\t\tif (is_vm_hugetlb_page(tmp))\n\t\t\treset_vma_resv_huge_pages(tmp);\n\n\t\t/*\n\t\t * Link in the new vma and copy the page table entries.\n\t\t */\n\t\t*pprev = tmp;\n\t\tpprev = &tmp->vm_next;\n\t\ttmp->vm_prev = prev;\n\t\tprev = tmp;\n\n\t\t__vma_link_rb(mm, tmp, rb_link, rb_parent);\n\t\trb_link = &tmp->vm_rb.rb_right;\n\t\trb_parent = &tmp->vm_rb;\n\n\t\tmm->map_count++;\n\t\tretval = copy_page_range(mm, oldmm, mpnt);\n\n\t\tif (tmp->vm_ops && tmp->vm_ops->open)\n\t\t\ttmp->vm_ops->open(tmp);\n\n\t\tif (retval)\n\t\t\tgoto out;\n\t}\n\t/* a new mm has just been created */\n\tarch_dup_mmap(oldmm, mm);\n\tretval = 0;\nout:\n\tup_write(&mm->mmap_sem);\n\tflush_tlb_mm(oldmm);\n\tup_write(&oldmm->mmap_sem);\n\treturn retval;\nfail_nomem_anon_vma_fork:\n\tmpol_put(pol);\nfail_nomem_policy:\n\tkmem_cache_free(vm_area_cachep, tmp);\nfail_nomem:\n\tretval = -ENOMEM;\n\tvm_unacct_memory(charge);\n\tgoto out;\n}\n\nstatic inline int mm_alloc_pgd(struct mm_struct * mm)\n{\n\tmm->pgd = pgd_alloc(mm);\n\tif (unlikely(!mm->pgd))\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic inline void mm_free_pgd(struct mm_struct * mm)\n{\n\tpgd_free(mm, mm->pgd);\n}\n#else\n#define dup_mmap(mm, oldmm)\t(0)\n#define mm_alloc_pgd(mm)\t(0)\n#define mm_free_pgd(mm)\n#endif /* CONFIG_MMU */\n\n__cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);\n\n#define allocate_mm()\t(kmem_cache_alloc(mm_cachep, GFP_KERNEL))\n#define free_mm(mm)\t(kmem_cache_free(mm_cachep, (mm)))\n\nstatic unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;\n\nstatic int __init coredump_filter_setup(char *s)\n{\n\tdefault_dump_filter =\n\t\t(simple_strtoul(s, NULL, 0) << MMF_DUMP_FILTER_SHIFT) &\n\t\tMMF_DUMP_FILTER_MASK;\n\treturn 1;\n}\n\n__setup(\"coredump_filter=\", coredump_filter_setup);\n\n#include <linux/init_task.h>\n\nstatic void mm_init_aio(struct mm_struct *mm)\n{\n#ifdef CONFIG_AIO\n\tspin_lock_init(&mm->ioctx_lock);\n\tINIT_HLIST_HEAD(&mm->ioctx_list);\n#endif\n}\n\nstatic struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)\n{\n\tatomic_set(&mm->mm_users, 1);\n\tatomic_set(&mm->mm_count, 1);\n\tinit_rwsem(&mm->mmap_sem);\n\tINIT_LIST_HEAD(&mm->mmlist);\n\tmm->flags = (current->mm) ?\n\t\t(current->mm->flags & MMF_INIT_MASK) : default_dump_filter;\n\tmm->core_state = NULL;\n\tmm->nr_ptes = 0;\n\tmemset(&mm->rss_stat, 0, sizeof(mm->rss_stat));\n\tspin_lock_init(&mm->page_table_lock);\n\tmm->free_area_cache = TASK_UNMAPPED_BASE;\n\tmm->cached_hole_size = ~0UL;\n\tmm_init_aio(mm);\n\tmm_init_owner(mm, p);\n\tatomic_set(&mm->oom_disable_count, 0);\n\n\tif (likely(!mm_alloc_pgd(mm))) {\n\t\tmm->def_flags = 0;\n\t\tmmu_notifier_mm_init(mm);\n\t\treturn mm;\n\t}\n\n\tfree_mm(mm);\n\treturn NULL;\n}\n\n/*\n * Allocate and initialize an mm_struct.\n */\nstruct mm_struct * mm_alloc(void)\n{\n\tstruct mm_struct * mm;\n\n\tmm = allocate_mm();\n\tif (mm) {\n\t\tmemset(mm, 0, sizeof(*mm));\n\t\tmm = mm_init(mm, current);\n\t}\n\treturn mm;\n}\n\n/*\n * Called when the last reference to the mm\n * is dropped: either by a lazy thread or by\n * mmput. Free the page directory and the mm.\n */\nvoid __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tfree_mm(mm);\n}\nEXPORT_SYMBOL_GPL(__mmdrop);\n\n/*\n * Decrement the use count and release all resources for an mm.\n */\nvoid mmput(struct mm_struct *mm)\n{\n\tmight_sleep();\n\n\tif (atomic_dec_and_test(&mm->mm_users)) {\n\t\texit_aio(mm);\n\t\tksm_exit(mm);\n\t\texit_mmap(mm);\n\t\tset_mm_exe_file(mm, NULL);\n\t\tif (!list_empty(&mm->mmlist)) {\n\t\t\tspin_lock(&mmlist_lock);\n\t\t\tlist_del(&mm->mmlist);\n\t\t\tspin_unlock(&mmlist_lock);\n\t\t}\n\t\tput_swap_token(mm);\n\t\tif (mm->binfmt)\n\t\t\tmodule_put(mm->binfmt->module);\n\t\tmmdrop(mm);\n\t}\n}\nEXPORT_SYMBOL_GPL(mmput);\n\n/**\n * get_task_mm - acquire a reference to the task's mm\n *\n * Returns %NULL if the task has no mm.  Checks PF_KTHREAD (meaning\n * this kernel workthread has transiently adopted a user mm with use_mm,\n * to do its AIO) is not set and if so returns a reference to it, after\n * bumping up the use count.  User must release the mm via mmput()\n * after use.  Typically used by /proc and ptrace.\n */\nstruct mm_struct *get_task_mm(struct task_struct *task)\n{\n\tstruct mm_struct *mm;\n\n\ttask_lock(task);\n\tmm = task->mm;\n\tif (mm) {\n\t\tif (task->flags & PF_KTHREAD)\n\t\t\tmm = NULL;\n\t\telse\n\t\t\tatomic_inc(&mm->mm_users);\n\t}\n\ttask_unlock(task);\n\treturn mm;\n}\nEXPORT_SYMBOL_GPL(get_task_mm);\n\n/* Please note the differences between mmput and mm_release.\n * mmput is called whenever we stop holding onto a mm_struct,\n * error success whatever.\n *\n * mm_release is called after a mm_struct has been removed\n * from the current process.\n *\n * This difference is important for error handling, when we\n * only half set up a mm_struct for a new process and need to restore\n * the old one.  Because we mmput the new mm_struct before\n * restoring the old one. . .\n * Eric Biederman 10 January 1998\n */\nvoid mm_release(struct task_struct *tsk, struct mm_struct *mm)\n{\n\tstruct completion *vfork_done = tsk->vfork_done;\n\n\t/* Get rid of any futexes when releasing the mm */\n#ifdef CONFIG_FUTEX\n\tif (unlikely(tsk->robust_list)) {\n\t\texit_robust_list(tsk);\n\t\ttsk->robust_list = NULL;\n\t}\n#ifdef CONFIG_COMPAT\n\tif (unlikely(tsk->compat_robust_list)) {\n\t\tcompat_exit_robust_list(tsk);\n\t\ttsk->compat_robust_list = NULL;\n\t}\n#endif\n\tif (unlikely(!list_empty(&tsk->pi_state_list)))\n\t\texit_pi_state_list(tsk);\n#endif\n\n\t/* Get rid of any cached register state */\n\tdeactivate_mm(tsk, mm);\n\n\t/* notify parent sleeping on vfork() */\n\tif (vfork_done) {\n\t\ttsk->vfork_done = NULL;\n\t\tcomplete(vfork_done);\n\t}\n\n\t/*\n\t * If we're exiting normally, clear a user-space tid field if\n\t * requested.  We leave this alone when dying by signal, to leave\n\t * the value intact in a core dump, and to save the unnecessary\n\t * trouble otherwise.  Userland only wants this done for a sys_exit.\n\t */\n\tif (tsk->clear_child_tid) {\n\t\tif (!(tsk->flags & PF_SIGNALED) &&\n\t\t    atomic_read(&mm->mm_users) > 1) {\n\t\t\t/*\n\t\t\t * We don't check the error code - if userspace has\n\t\t\t * not set up a proper pointer then tough luck.\n\t\t\t */\n\t\t\tput_user(0, tsk->clear_child_tid);\n\t\t\tsys_futex(tsk->clear_child_tid, FUTEX_WAKE,\n\t\t\t\t\t1, NULL, NULL, 0);\n\t\t}\n\t\ttsk->clear_child_tid = NULL;\n\t}\n}\n\n/*\n * Allocate a new mm structure and copy contents from the\n * mm structure of the passed in task structure.\n */\nstruct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tif (!oldmm)\n\t\treturn NULL;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\t/* Initializing for Swap token stuff */\n\tmm->token_priority = 0;\n\tmm->last_interval = 0;\n\n\tif (!mm_init(mm, tsk))\n\t\tgoto fail_nomem;\n\n\tif (init_new_context(tsk, mm))\n\t\tgoto fail_nocontext;\n\n\tdup_mm_exe_file(oldmm, mm);\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n\nfail_nocontext:\n\t/*\n\t * If init_new_context() failed, we cannot use mmput() to free the mm\n\t * because it calls destroy_context()\n\t */\n\tmm_free_pgd(mm);\n\tfree_mm(mm);\n\treturn NULL;\n}\n\nstatic int copy_mm(unsigned long clone_flags, struct task_struct * tsk)\n{\n\tstruct mm_struct * mm, *oldmm;\n\tint retval;\n\n\ttsk->min_flt = tsk->maj_flt = 0;\n\ttsk->nvcsw = tsk->nivcsw = 0;\n#ifdef CONFIG_DETECT_HUNG_TASK\n\ttsk->last_switch_count = tsk->nvcsw + tsk->nivcsw;\n#endif\n\n\ttsk->mm = NULL;\n\ttsk->active_mm = NULL;\n\n\t/*\n\t * Are we cloning a kernel thread?\n\t *\n\t * We need to steal a active VM for that..\n\t */\n\toldmm = current->mm;\n\tif (!oldmm)\n\t\treturn 0;\n\n\tif (clone_flags & CLONE_VM) {\n\t\tatomic_inc(&oldmm->mm_users);\n\t\tmm = oldmm;\n\t\tgoto good_mm;\n\t}\n\n\tretval = -ENOMEM;\n\tmm = dup_mm(tsk);\n\tif (!mm)\n\t\tgoto fail_nomem;\n\ngood_mm:\n\t/* Initializing for Swap token stuff */\n\tmm->token_priority = 0;\n\tmm->last_interval = 0;\n\tif (tsk->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)\n\t\tatomic_inc(&mm->oom_disable_count);\n\n\ttsk->mm = mm;\n\ttsk->active_mm = mm;\n\treturn 0;\n\nfail_nomem:\n\treturn retval;\n}\n\nstatic int copy_fs(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct fs_struct *fs = current->fs;\n\tif (clone_flags & CLONE_FS) {\n\t\t/* tsk->fs is already what we want */\n\t\tspin_lock(&fs->lock);\n\t\tif (fs->in_exec) {\n\t\t\tspin_unlock(&fs->lock);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t\tfs->users++;\n\t\tspin_unlock(&fs->lock);\n\t\treturn 0;\n\t}\n\ttsk->fs = copy_fs_struct(fs);\n\tif (!tsk->fs)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic int copy_files(unsigned long clone_flags, struct task_struct * tsk)\n{\n\tstruct files_struct *oldf, *newf;\n\tint error = 0;\n\n\t/*\n\t * A background process may not have any files ...\n\t */\n\toldf = current->files;\n\tif (!oldf)\n\t\tgoto out;\n\n\tif (clone_flags & CLONE_FILES) {\n\t\tatomic_inc(&oldf->count);\n\t\tgoto out;\n\t}\n\n\tnewf = dup_fd(oldf, &error);\n\tif (!newf)\n\t\tgoto out;\n\n\ttsk->files = newf;\n\terror = 0;\nout:\n\treturn error;\n}\n\nstatic int copy_io(unsigned long clone_flags, struct task_struct *tsk)\n{\n#ifdef CONFIG_BLOCK\n\tstruct io_context *ioc = current->io_context;\n\n\tif (!ioc)\n\t\treturn 0;\n\t/*\n\t * Share io context with parent, if CLONE_IO is set\n\t */\n\tif (clone_flags & CLONE_IO) {\n\t\ttsk->io_context = ioc_task_link(ioc);\n\t\tif (unlikely(!tsk->io_context))\n\t\t\treturn -ENOMEM;\n\t} else if (ioprio_valid(ioc->ioprio)) {\n\t\ttsk->io_context = alloc_io_context(GFP_KERNEL, -1);\n\t\tif (unlikely(!tsk->io_context))\n\t\t\treturn -ENOMEM;\n\n\t\ttsk->io_context->ioprio = ioc->ioprio;\n\t}\n#endif\n\treturn 0;\n}\n\nstatic int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct sighand_struct *sig;\n\n\tif (clone_flags & CLONE_SIGHAND) {\n\t\tatomic_inc(&current->sighand->count);\n\t\treturn 0;\n\t}\n\tsig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);\n\trcu_assign_pointer(tsk->sighand, sig);\n\tif (!sig)\n\t\treturn -ENOMEM;\n\tatomic_set(&sig->count, 1);\n\tmemcpy(sig->action, current->sighand->action, sizeof(sig->action));\n\treturn 0;\n}\n\nvoid __cleanup_sighand(struct sighand_struct *sighand)\n{\n\tif (atomic_dec_and_test(&sighand->count))\n\t\tkmem_cache_free(sighand_cachep, sighand);\n}\n\n\n/*\n * Initialize POSIX timer handling for a thread group.\n */\nstatic void posix_cpu_timers_init_group(struct signal_struct *sig)\n{\n\tunsigned long cpu_limit;\n\n\t/* Thread group counters. */\n\tthread_group_cputime_init(sig);\n\n\tcpu_limit = ACCESS_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);\n\tif (cpu_limit != RLIM_INFINITY) {\n\t\tsig->cputime_expires.prof_exp = secs_to_cputime(cpu_limit);\n\t\tsig->cputimer.running = 1;\n\t}\n\n\t/* The timer lists. */\n\tINIT_LIST_HEAD(&sig->cpu_timers[0]);\n\tINIT_LIST_HEAD(&sig->cpu_timers[1]);\n\tINIT_LIST_HEAD(&sig->cpu_timers[2]);\n}\n\nstatic int copy_signal(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct signal_struct *sig;\n\n\tif (clone_flags & CLONE_THREAD)\n\t\treturn 0;\n\n\tsig = kmem_cache_zalloc(signal_cachep, GFP_KERNEL);\n\ttsk->signal = sig;\n\tif (!sig)\n\t\treturn -ENOMEM;\n\n\tsig->nr_threads = 1;\n\tatomic_set(&sig->live, 1);\n\tatomic_set(&sig->sigcnt, 1);\n\tinit_waitqueue_head(&sig->wait_chldexit);\n\tif (clone_flags & CLONE_NEWPID)\n\t\tsig->flags |= SIGNAL_UNKILLABLE;\n\tsig->curr_target = tsk;\n\tinit_sigpending(&sig->shared_pending);\n\tINIT_LIST_HEAD(&sig->posix_timers);\n\n\thrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\tsig->real_timer.function = it_real_fn;\n\n\ttask_lock(current->group_leader);\n\tmemcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);\n\ttask_unlock(current->group_leader);\n\n\tposix_cpu_timers_init_group(sig);\n\n\ttty_audit_fork(sig);\n\n\tsig->oom_adj = current->signal->oom_adj;\n\tsig->oom_score_adj = current->signal->oom_score_adj;\n\n\tmutex_init(&sig->cred_guard_mutex);\n\n\treturn 0;\n}\n\nstatic void copy_flags(unsigned long clone_flags, struct task_struct *p)\n{\n\tunsigned long new_flags = p->flags;\n\n\tnew_flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);\n\tnew_flags |= PF_FORKNOEXEC;\n\tnew_flags |= PF_STARTING;\n\tp->flags = new_flags;\n\tclear_freeze_flag(p);\n}\n\nSYSCALL_DEFINE1(set_tid_address, int __user *, tidptr)\n{\n\tcurrent->clear_child_tid = tidptr;\n\n\treturn task_pid_vnr(current);\n}\n\nstatic void rt_mutex_init_task(struct task_struct *p)\n{\n\traw_spin_lock_init(&p->pi_lock);\n#ifdef CONFIG_RT_MUTEXES\n\tplist_head_init_raw(&p->pi_waiters, &p->pi_lock);\n\tp->pi_blocked_on = NULL;\n#endif\n}\n\n#ifdef CONFIG_MM_OWNER\nvoid mm_init_owner(struct mm_struct *mm, struct task_struct *p)\n{\n\tmm->owner = p;\n}\n#endif /* CONFIG_MM_OWNER */\n\n/*\n * Initialize POSIX timer handling for a single task.\n */\nstatic void posix_cpu_timers_init(struct task_struct *tsk)\n{\n\ttsk->cputime_expires.prof_exp = cputime_zero;\n\ttsk->cputime_expires.virt_exp = cputime_zero;\n\ttsk->cputime_expires.sched_exp = 0;\n\tINIT_LIST_HEAD(&tsk->cpu_timers[0]);\n\tINIT_LIST_HEAD(&tsk->cpu_timers[1]);\n\tINIT_LIST_HEAD(&tsk->cpu_timers[2]);\n}\n\n/*\n * This creates a new process as a copy of the old one,\n * but does not actually start it yet.\n *\n * It copies the registers, and all the appropriate\n * parts of the process environment (as per the clone\n * flags). The actual kick-off is left to the caller.\n */\nstatic struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tstruct pt_regs *regs,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\tint cgroup_callbacks_done = 0;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = cputime_zero;\n\tp->stime = cputime_zero;\n\tp->gtime = cputime_zero;\n\tp->utimescaled = cputime_zero;\n\tp->stimescaled = cputime_zero;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_utime = cputime_zero;\n\tp->prev_stime = cputime_zero;\n#endif\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tp->lock_depth = -1;\t\t/* -1 = no lock */\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n \tif (IS_ERR(p->mempolicy)) {\n \t\tretval = PTR_ERR(p->mempolicy);\n \t\tp->mempolicy = NULL;\n \t\tgoto bad_fork_cleanup_cgroup;\n \t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tp->hardirqs_enabled = 1;\n#else\n\tp->hardirqs_enabled = 0;\n#endif\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_CGROUP_MEM_RES_CTLR\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p, clone_flags);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tif ((retval = audit_alloc(p)))\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tif ((retval = copy_semundo(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_audit;\n\tif ((retval = copy_files(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_semundo;\n\tif ((retval = copy_fs(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_files;\n\tif ((retval = copy_sighand(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_fs;\n\tif ((retval = copy_signal(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_sighand;\n\tif ((retval = copy_mm(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_signal;\n\tif ((retval = copy_namespaces(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_mm;\n\tif ((retval = copy_io(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p, regs);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\n\t\tif (clone_flags & CLONE_NEWPID) {\n\t\t\tretval = pid_ns_prepare_proc(p->nsproxy->pid_ns);\n\t\t\tif (retval < 0)\n\t\t\t\tgoto bad_fork_free_pid;\n\t\t}\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tif (current->nsproxy != p->nsproxy) {\n\t\tretval = ns_cgroup_clone(p, pid);\n\t\tif (retval)\n\t\t\tgoto bad_fork_free_pid;\n\t}\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\n\t/* Now that the task is set up, run cgroup callbacks if\n\t * necessary. We need to run them before the task is visible\n\t * on the tasklist. */\n\tcgroup_fork_callbacks(p);\n\tcgroup_callbacks_done = 1;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n \t */\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\ttracehook_finish_clone(p, clone_flags, trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (clone_flags & CLONE_NEWPID)\n\t\t\t\tp->nsproxy->pid_ns->child_reaper = p;\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__get_cpu_var(process_counts)++;\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tperf_event_fork(p);\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm) {\n\t\ttask_lock(p);\n\t\tif (p->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)\n\t\t\tatomic_dec(&p->mm->oom_disable_count);\n\t\ttask_unlock(p);\n\t\tmmput(p->mm);\n\t}\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tcgroup_exit(p, cgroup_callbacks_done);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}\n\nnoinline struct pt_regs * __cpuinit __attribute__((weak)) idle_regs(struct pt_regs *regs)\n{\n\tmemset(regs, 0, sizeof(struct pt_regs));\n\treturn regs;\n}\n\nstatic inline void init_idle_pids(struct pid_link *links)\n{\n\tenum pid_type type;\n\n\tfor (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {\n\t\tINIT_HLIST_NODE(&links[type].node); /* not really needed */\n\t\tlinks[type].pid = &init_struct_pid;\n\t}\n}\n\nstruct task_struct * __cpuinit fork_idle(int cpu)\n{\n\tstruct task_struct *task;\n\tstruct pt_regs regs;\n\n\ttask = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL,\n\t\t\t    &init_struct_pid, 0);\n\tif (!IS_ERR(task)) {\n\t\tinit_idle_pids(task->pids);\n\t\tinit_idle(task, cpu);\n\t}\n\n\treturn task;\n}\n\n/*\n *  Ok, this is the main fork-routine.\n *\n * It copies the process, and if successful kick-starts\n * it and waits for it to finish using the VM if required.\n */\nlong do_fork(unsigned long clone_flags,\n\t      unsigned long stack_start,\n\t      struct pt_regs *regs,\n\t      unsigned long stack_size,\n\t      int __user *parent_tidptr,\n\t      int __user *child_tidptr)\n{\n\tstruct task_struct *p;\n\tint trace = 0;\n\tlong nr;\n\n\t/*\n\t * Do some preliminary argument and permissions checking before we\n\t * actually start allocating stuff\n\t */\n\tif (clone_flags & CLONE_NEWUSER) {\n\t\tif (clone_flags & CLONE_THREAD)\n\t\t\treturn -EINVAL;\n\t\t/* hopefully this check will go away when userns support is\n\t\t * complete\n\t\t */\n\t\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SETUID) ||\n\t\t\t\t!capable(CAP_SETGID))\n\t\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We hope to recycle these flags after 2.6.26\n\t */\n\tif (unlikely(clone_flags & CLONE_STOPPED)) {\n\t\tstatic int __read_mostly count = 100;\n\n\t\tif (count > 0 && printk_ratelimit()) {\n\t\t\tchar comm[TASK_COMM_LEN];\n\n\t\t\tcount--;\n\t\t\tprintk(KERN_INFO \"fork(): process `%s' used deprecated \"\n\t\t\t\t\t\"clone flags 0x%lx\\n\",\n\t\t\t\tget_task_comm(comm, current),\n\t\t\t\tclone_flags & CLONE_STOPPED);\n\t\t}\n\t}\n\n\t/*\n\t * When called from kernel_thread, don't do user tracing stuff.\n\t */\n\tif (likely(user_mode(regs)))\n\t\ttrace = tracehook_prepare_clone(clone_flags);\n\n\tp = copy_process(clone_flags, stack_start, regs, stack_size,\n\t\t\t child_tidptr, NULL, trace);\n\t/*\n\t * Do this prior waking up the new thread - the thread pointer\n\t * might get invalid after that point, if the thread exits quickly.\n\t */\n\tif (!IS_ERR(p)) {\n\t\tstruct completion vfork;\n\n\t\ttrace_sched_process_fork(current, p);\n\n\t\tnr = task_pid_vnr(p);\n\n\t\tif (clone_flags & CLONE_PARENT_SETTID)\n\t\t\tput_user(nr, parent_tidptr);\n\n\t\tif (clone_flags & CLONE_VFORK) {\n\t\t\tp->vfork_done = &vfork;\n\t\t\tinit_completion(&vfork);\n\t\t}\n\n\t\taudit_finish_fork(p);\n\t\ttracehook_report_clone(regs, clone_flags, nr, p);\n\n\t\t/*\n\t\t * We set PF_STARTING at creation in case tracing wants to\n\t\t * use this to distinguish a fully live task from one that\n\t\t * hasn't gotten to tracehook_report_clone() yet.  Now we\n\t\t * clear it and set the child going.\n\t\t */\n\t\tp->flags &= ~PF_STARTING;\n\n\t\tif (unlikely(clone_flags & CLONE_STOPPED)) {\n\t\t\t/*\n\t\t\t * We'll start up with an immediate SIGSTOP.\n\t\t\t */\n\t\t\tsigaddset(&p->pending.signal, SIGSTOP);\n\t\t\tset_tsk_thread_flag(p, TIF_SIGPENDING);\n\t\t\t__set_task_state(p, TASK_STOPPED);\n\t\t} else {\n\t\t\twake_up_new_task(p, clone_flags);\n\t\t}\n\n\t\ttracehook_report_clone_complete(trace, regs,\n\t\t\t\t\t\tclone_flags, nr, p);\n\n\t\tif (clone_flags & CLONE_VFORK) {\n\t\t\tfreezer_do_not_count();\n\t\t\twait_for_completion(&vfork);\n\t\t\tfreezer_count();\n\t\t\ttracehook_report_vfork_done(p, nr);\n\t\t}\n\t} else {\n\t\tnr = PTR_ERR(p);\n\t}\n\treturn nr;\n}\n\n#ifndef ARCH_MIN_MMSTRUCT_ALIGN\n#define ARCH_MIN_MMSTRUCT_ALIGN 0\n#endif\n\nstatic void sighand_ctor(void *data)\n{\n\tstruct sighand_struct *sighand = data;\n\n\tspin_lock_init(&sighand->siglock);\n\tinit_waitqueue_head(&sighand->signalfd_wqh);\n}\n\nvoid __init proc_caches_init(void)\n{\n\tsighand_cachep = kmem_cache_create(\"sighand_cache\",\n\t\t\tsizeof(struct sighand_struct), 0,\n\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_DESTROY_BY_RCU|\n\t\t\tSLAB_NOTRACK, sighand_ctor);\n\tsignal_cachep = kmem_cache_create(\"signal_cache\",\n\t\t\tsizeof(struct signal_struct), 0,\n\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);\n\tfiles_cachep = kmem_cache_create(\"files_cache\",\n\t\t\tsizeof(struct files_struct), 0,\n\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);\n\tfs_cachep = kmem_cache_create(\"fs_cache\",\n\t\t\tsizeof(struct fs_struct), 0,\n\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);\n\tmm_cachep = kmem_cache_create(\"mm_struct\",\n\t\t\tsizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,\n\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);\n\tvm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC);\n\tmmap_init();\n}\n\n/*\n * Check constraints on flags passed to the unshare system call and\n * force unsharing of additional process context as appropriate.\n */\nstatic void check_unshare_flags(unsigned long *flags_ptr)\n{\n\t/*\n\t * If unsharing a thread from a thread group, must also\n\t * unshare vm.\n\t */\n\tif (*flags_ptr & CLONE_THREAD)\n\t\t*flags_ptr |= CLONE_VM;\n\n\t/*\n\t * If unsharing vm, must also unshare signal handlers.\n\t */\n\tif (*flags_ptr & CLONE_VM)\n\t\t*flags_ptr |= CLONE_SIGHAND;\n\n\t/*\n\t * If unsharing namespace, must also unshare filesystem information.\n\t */\n\tif (*flags_ptr & CLONE_NEWNS)\n\t\t*flags_ptr |= CLONE_FS;\n}\n\n/*\n * Unsharing of tasks created with CLONE_THREAD is not supported yet\n */\nstatic int unshare_thread(unsigned long unshare_flags)\n{\n\tif (unshare_flags & CLONE_THREAD)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n/*\n * Unshare the filesystem structure if it is being shared\n */\nstatic int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)\n{\n\tstruct fs_struct *fs = current->fs;\n\n\tif (!(unshare_flags & CLONE_FS) || !fs)\n\t\treturn 0;\n\n\t/* don't need lock here; in the worst case we'll do useless copy */\n\tif (fs->users == 1)\n\t\treturn 0;\n\n\t*new_fsp = copy_fs_struct(fs);\n\tif (!*new_fsp)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n/*\n * Unsharing of sighand is not supported yet\n */\nstatic int unshare_sighand(unsigned long unshare_flags, struct sighand_struct **new_sighp)\n{\n\tstruct sighand_struct *sigh = current->sighand;\n\n\tif ((unshare_flags & CLONE_SIGHAND) && atomic_read(&sigh->count) > 1)\n\t\treturn -EINVAL;\n\telse\n\t\treturn 0;\n}\n\n/*\n * Unshare vm if it is being shared\n */\nstatic int unshare_vm(unsigned long unshare_flags, struct mm_struct **new_mmp)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tif ((unshare_flags & CLONE_VM) &&\n\t    (mm && atomic_read(&mm->mm_users) > 1)) {\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Unshare file descriptor table if it is being shared\n */\nstatic int unshare_fd(unsigned long unshare_flags, struct files_struct **new_fdp)\n{\n\tstruct files_struct *fd = current->files;\n\tint error = 0;\n\n\tif ((unshare_flags & CLONE_FILES) &&\n\t    (fd && atomic_read(&fd->count) > 1)) {\n\t\t*new_fdp = dup_fd(fd, &error);\n\t\tif (!*new_fdp)\n\t\t\treturn error;\n\t}\n\n\treturn 0;\n}\n\n/*\n * unshare allows a process to 'unshare' part of the process\n * context which was originally shared using clone.  copy_*\n * functions used by do_fork() cannot be used here directly\n * because they modify an inactive task_struct that is being\n * constructed. Here we are modifying the current, active,\n * task_struct.\n */\nSYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)\n{\n\tint err = 0;\n\tstruct fs_struct *fs, *new_fs = NULL;\n\tstruct sighand_struct *new_sigh = NULL;\n\tstruct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;\n\tstruct files_struct *fd, *new_fd = NULL;\n\tstruct nsproxy *new_nsproxy = NULL;\n\tint do_sysvsem = 0;\n\n\tcheck_unshare_flags(&unshare_flags);\n\n\t/* Return -EINVAL for all unsupported flags */\n\terr = -EINVAL;\n\tif (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|\n\t\t\t\tCLONE_VM|CLONE_FILES|CLONE_SYSVSEM|\n\t\t\t\tCLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET))\n\t\tgoto bad_unshare_out;\n\n\t/*\n\t * CLONE_NEWIPC must also detach from the undolist: after switching\n\t * to a new ipc namespace, the semaphore arrays from the old\n\t * namespace are unreachable.\n\t */\n\tif (unshare_flags & (CLONE_NEWIPC|CLONE_SYSVSEM))\n\t\tdo_sysvsem = 1;\n\tif ((err = unshare_thread(unshare_flags)))\n\t\tgoto bad_unshare_out;\n\tif ((err = unshare_fs(unshare_flags, &new_fs)))\n\t\tgoto bad_unshare_cleanup_thread;\n\tif ((err = unshare_sighand(unshare_flags, &new_sigh)))\n\t\tgoto bad_unshare_cleanup_fs;\n\tif ((err = unshare_vm(unshare_flags, &new_mm)))\n\t\tgoto bad_unshare_cleanup_sigh;\n\tif ((err = unshare_fd(unshare_flags, &new_fd)))\n\t\tgoto bad_unshare_cleanup_vm;\n\tif ((err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,\n\t\t\tnew_fs)))\n\t\tgoto bad_unshare_cleanup_fd;\n\n\tif (new_fs ||  new_mm || new_fd || do_sysvsem || new_nsproxy) {\n\t\tif (do_sysvsem) {\n\t\t\t/*\n\t\t\t * CLONE_SYSVSEM is equivalent to sys_exit().\n\t\t\t */\n\t\t\texit_sem(current);\n\t\t}\n\n\t\tif (new_nsproxy) {\n\t\t\tswitch_task_namespaces(current, new_nsproxy);\n\t\t\tnew_nsproxy = NULL;\n\t\t}\n\n\t\ttask_lock(current);\n\n\t\tif (new_fs) {\n\t\t\tfs = current->fs;\n\t\t\tspin_lock(&fs->lock);\n\t\t\tcurrent->fs = new_fs;\n\t\t\tif (--fs->users)\n\t\t\t\tnew_fs = NULL;\n\t\t\telse\n\t\t\t\tnew_fs = fs;\n\t\t\tspin_unlock(&fs->lock);\n\t\t}\n\n\t\tif (new_mm) {\n\t\t\tmm = current->mm;\n\t\t\tactive_mm = current->active_mm;\n\t\t\tcurrent->mm = new_mm;\n\t\t\tcurrent->active_mm = new_mm;\n\t\t\tif (current->signal->oom_score_adj == OOM_SCORE_ADJ_MIN) {\n\t\t\t\tatomic_dec(&mm->oom_disable_count);\n\t\t\t\tatomic_inc(&new_mm->oom_disable_count);\n\t\t\t}\n\t\t\tactivate_mm(active_mm, new_mm);\n\t\t\tnew_mm = mm;\n\t\t}\n\n\t\tif (new_fd) {\n\t\t\tfd = current->files;\n\t\t\tcurrent->files = new_fd;\n\t\t\tnew_fd = fd;\n\t\t}\n\n\t\ttask_unlock(current);\n\t}\n\n\tif (new_nsproxy)\n\t\tput_nsproxy(new_nsproxy);\n\nbad_unshare_cleanup_fd:\n\tif (new_fd)\n\t\tput_files_struct(new_fd);\n\nbad_unshare_cleanup_vm:\n\tif (new_mm)\n\t\tmmput(new_mm);\n\nbad_unshare_cleanup_sigh:\n\tif (new_sigh)\n\t\tif (atomic_dec_and_test(&new_sigh->count))\n\t\t\tkmem_cache_free(sighand_cachep, new_sigh);\n\nbad_unshare_cleanup_fs:\n\tif (new_fs)\n\t\tfree_fs_struct(new_fs);\n\nbad_unshare_cleanup_thread:\nbad_unshare_out:\n\treturn err;\n}\n\n/*\n *\tHelper to unshare the files of the current task.\n *\tWe don't want to expose copy_files internals to\n *\tthe exec layer of the kernel.\n */\n\nint unshare_files(struct files_struct **displaced)\n{\n\tstruct task_struct *task = current;\n\tstruct files_struct *copy = NULL;\n\tint error;\n\n\terror = unshare_fd(CLONE_FILES, &copy);\n\tif (error || !copy) {\n\t\t*displaced = NULL;\n\t\treturn error;\n\t}\n\t*displaced = task->files;\n\ttask_lock(task);\n\ttask->files = copy;\n\ttask_unlock(task);\n\treturn 0;\n}\n", "/*\n *  kernel/sched.c\n *\n *  Kernel scheduler and related syscalls\n *\n *  Copyright (C) 1991-2002  Linus Torvalds\n *\n *  1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and\n *\t\tmake semaphores SMP safe\n *  1998-11-19\tImplemented schedule_timeout() and related stuff\n *\t\tby Andrea Arcangeli\n *  2002-01-04\tNew ultra-scalable O(1) scheduler by Ingo Molnar:\n *\t\thybrid priority-list and round-robin design with\n *\t\tan array-switch method of distributing timeslices\n *\t\tand per-CPU runqueues.  Cleanups and useful suggestions\n *\t\tby Davide Libenzi, preemptible kernel bits by Robert Love.\n *  2003-09-03\tInteractivity tuning by Con Kolivas.\n *  2004-04-02\tScheduler domains code by Nick Piggin\n *  2007-04-15  Work begun on replacing all interactivity tuning with a\n *              fair scheduling design by Con Kolivas.\n *  2007-05-05  Load balancing (smp-nice) and other improvements\n *              by Peter Williams\n *  2007-05-06  Interactivity improvements to CFS by Mike Galbraith\n *  2007-07-01  Group scheduling enhancements by Srivatsa Vaddagiri\n *  2007-11-29  RT balancing improvements by Steven Rostedt, Gregory Haskins,\n *              Thomas Gleixner, Mike Kravetz\n */\n\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/nmi.h>\n#include <linux/init.h>\n#include <linux/uaccess.h>\n#include <linux/highmem.h>\n#include <linux/smp_lock.h>\n#include <asm/mmu_context.h>\n#include <linux/interrupt.h>\n#include <linux/capability.h>\n#include <linux/completion.h>\n#include <linux/kernel_stat.h>\n#include <linux/debug_locks.h>\n#include <linux/perf_event.h>\n#include <linux/security.h>\n#include <linux/notifier.h>\n#include <linux/profile.h>\n#include <linux/freezer.h>\n#include <linux/vmalloc.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/pid_namespace.h>\n#include <linux/smp.h>\n#include <linux/threads.h>\n#include <linux/timer.h>\n#include <linux/rcupdate.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/percpu.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/stop_machine.h>\n#include <linux/sysctl.h>\n#include <linux/syscalls.h>\n#include <linux/times.h>\n#include <linux/tsacct_kern.h>\n#include <linux/kprobes.h>\n#include <linux/delayacct.h>\n#include <linux/unistd.h>\n#include <linux/pagemap.h>\n#include <linux/hrtimer.h>\n#include <linux/tick.h>\n#include <linux/debugfs.h>\n#include <linux/ctype.h>\n#include <linux/ftrace.h>\n#include <linux/slab.h>\n\n#include <asm/tlb.h>\n#include <asm/irq_regs.h>\n\n#include \"sched_cpupri.h\"\n#include \"workqueue_sched.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/sched.h>\n\n/*\n * Convert user-nice values [ -20 ... 0 ... 19 ]\n * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],\n * and back.\n */\n#define NICE_TO_PRIO(nice)\t(MAX_RT_PRIO + (nice) + 20)\n#define PRIO_TO_NICE(prio)\t((prio) - MAX_RT_PRIO - 20)\n#define TASK_NICE(p)\t\tPRIO_TO_NICE((p)->static_prio)\n\n/*\n * 'User priority' is the nice value converted to something we\n * can work with better when scaling various scheduler parameters,\n * it's a [ 0 ... 39 ] range.\n */\n#define USER_PRIO(p)\t\t((p)-MAX_RT_PRIO)\n#define TASK_USER_PRIO(p)\tUSER_PRIO((p)->static_prio)\n#define MAX_USER_PRIO\t\t(USER_PRIO(MAX_PRIO))\n\n/*\n * Helpers for converting nanosecond timing to jiffy resolution\n */\n#define NS_TO_JIFFIES(TIME)\t((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))\n\n#define NICE_0_LOAD\t\tSCHED_LOAD_SCALE\n#define NICE_0_SHIFT\t\tSCHED_LOAD_SHIFT\n\n/*\n * These are the 'tuning knobs' of the scheduler:\n *\n * default timeslice is 100 msecs (used only for SCHED_RR tasks).\n * Timeslices get refilled after they expire.\n */\n#define DEF_TIMESLICE\t\t(100 * HZ / 1000)\n\n/*\n * single value that denotes runtime == period, ie unlimited time.\n */\n#define RUNTIME_INF\t((u64)~0ULL)\n\nstatic inline int rt_policy(int policy)\n{\n\tif (unlikely(policy == SCHED_FIFO || policy == SCHED_RR))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline int task_has_rt_policy(struct task_struct *p)\n{\n\treturn rt_policy(p->policy);\n}\n\n/*\n * This is the priority-queue data structure of the RT scheduling class:\n */\nstruct rt_prio_array {\n\tDECLARE_BITMAP(bitmap, MAX_RT_PRIO+1); /* include 1 bit for delimiter */\n\tstruct list_head queue[MAX_RT_PRIO];\n};\n\nstruct rt_bandwidth {\n\t/* nests inside the rq lock: */\n\traw_spinlock_t\t\trt_runtime_lock;\n\tktime_t\t\t\trt_period;\n\tu64\t\t\trt_runtime;\n\tstruct hrtimer\t\trt_period_timer;\n};\n\nstatic struct rt_bandwidth def_rt_bandwidth;\n\nstatic int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun);\n\nstatic enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)\n{\n\tstruct rt_bandwidth *rt_b =\n\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);\n\tktime_t now;\n\tint overrun;\n\tint idle = 0;\n\n\tfor (;;) {\n\t\tnow = hrtimer_cb_get_time(timer);\n\t\toverrun = hrtimer_forward(timer, now, rt_b->rt_period);\n\n\t\tif (!overrun)\n\t\t\tbreak;\n\n\t\tidle = do_sched_rt_period_timer(rt_b, overrun);\n\t}\n\n\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\n}\n\nstatic\nvoid init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)\n{\n\trt_b->rt_period = ns_to_ktime(period);\n\trt_b->rt_runtime = runtime;\n\n\traw_spin_lock_init(&rt_b->rt_runtime_lock);\n\n\thrtimer_init(&rt_b->rt_period_timer,\n\t\t\tCLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\trt_b->rt_period_timer.function = sched_rt_period_timer;\n}\n\nstatic inline int rt_bandwidth_enabled(void)\n{\n\treturn sysctl_sched_rt_runtime >= 0;\n}\n\nstatic void start_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\tktime_t now;\n\n\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)\n\t\treturn;\n\n\tif (hrtimer_active(&rt_b->rt_period_timer))\n\t\treturn;\n\n\traw_spin_lock(&rt_b->rt_runtime_lock);\n\tfor (;;) {\n\t\tunsigned long delta;\n\t\tktime_t soft, hard;\n\n\t\tif (hrtimer_active(&rt_b->rt_period_timer))\n\t\t\tbreak;\n\n\t\tnow = hrtimer_cb_get_time(&rt_b->rt_period_timer);\n\t\thrtimer_forward(&rt_b->rt_period_timer, now, rt_b->rt_period);\n\n\t\tsoft = hrtimer_get_softexpires(&rt_b->rt_period_timer);\n\t\thard = hrtimer_get_expires(&rt_b->rt_period_timer);\n\t\tdelta = ktime_to_ns(ktime_sub(hard, soft));\n\t\t__hrtimer_start_range_ns(&rt_b->rt_period_timer, soft, delta,\n\t\t\t\tHRTIMER_MODE_ABS_PINNED, 0);\n\t}\n\traw_spin_unlock(&rt_b->rt_runtime_lock);\n}\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\thrtimer_cancel(&rt_b->rt_period_timer);\n}\n#endif\n\n/*\n * sched_domains_mutex serializes calls to arch_init_sched_domains,\n * detach_destroy_domains and partition_sched_domains.\n */\nstatic DEFINE_MUTEX(sched_domains_mutex);\n\n#ifdef CONFIG_CGROUP_SCHED\n\n#include <linux/cgroup.h>\n\nstruct cfs_rq;\n\nstatic LIST_HEAD(task_groups);\n\n/* task group related information */\nstruct task_group {\n\tstruct cgroup_subsys_state css;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/* schedulable entities of this group on each cpu */\n\tstruct sched_entity **se;\n\t/* runqueue \"owned\" by this group on each cpu */\n\tstruct cfs_rq **cfs_rq;\n\tunsigned long shares;\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct sched_rt_entity **rt_se;\n\tstruct rt_rq **rt_rq;\n\n\tstruct rt_bandwidth rt_bandwidth;\n#endif\n\n\tstruct rcu_head rcu;\n\tstruct list_head list;\n\n\tstruct task_group *parent;\n\tstruct list_head siblings;\n\tstruct list_head children;\n};\n\n#define root_task_group init_task_group\n\n/* task_group_lock serializes add/remove of task groups and also changes to\n * a task group's cpu shares.\n */\nstatic DEFINE_SPINLOCK(task_group_lock);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\n#ifdef CONFIG_SMP\nstatic int root_task_group_empty(void)\n{\n\treturn list_empty(&root_task_group.children);\n}\n#endif\n\n# define INIT_TASK_GROUP_LOAD\tNICE_0_LOAD\n\n/*\n * A weight of 0 or 1 can cause arithmetics problems.\n * A weight of a cfs_rq is the sum of weights of which entities\n * are queued on this cfs_rq, so a weight of a entity should not be\n * too large, so as the shares value of a task group.\n * (The default weight is 1024 - so there's no practical\n *  limitation from this.)\n */\n#define MIN_SHARES\t2\n#define MAX_SHARES\t(1UL << 18)\n\nstatic int init_task_group_load = INIT_TASK_GROUP_LOAD;\n#endif\n\n/* Default task group.\n *\tEvery task in system belong to this group at bootup.\n */\nstruct task_group init_task_group;\n\n#endif\t/* CONFIG_CGROUP_SCHED */\n\n/* CFS-related fields in a runqueue */\nstruct cfs_rq {\n\tstruct load_weight load;\n\tunsigned long nr_running;\n\n\tu64 exec_clock;\n\tu64 min_vruntime;\n\n\tstruct rb_root tasks_timeline;\n\tstruct rb_node *rb_leftmost;\n\n\tstruct list_head tasks;\n\tstruct list_head *balance_iterator;\n\n\t/*\n\t * 'curr' points to currently running entity on this cfs_rq.\n\t * It is set to NULL otherwise (i.e when none are currently running).\n\t */\n\tstruct sched_entity *curr, *next, *last;\n\n\tunsigned int nr_spread_over;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tstruct rq *rq;\t/* cpu runqueue to which this cfs_rq is attached */\n\n\t/*\n\t * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in\n\t * a hierarchy). Non-leaf lrqs hold other higher schedulable entities\n\t * (like users, containers etc.)\n\t *\n\t * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This\n\t * list is used during load balance.\n\t */\n\tstruct list_head leaf_cfs_rq_list;\n\tstruct task_group *tg;\t/* group that \"owns\" this runqueue */\n\n#ifdef CONFIG_SMP\n\t/*\n\t * the part of load.weight contributed by tasks\n\t */\n\tunsigned long task_weight;\n\n\t/*\n\t *   h_load = weight * f(tg)\n\t *\n\t * Where f(tg) is the recursive weight fraction assigned to\n\t * this group.\n\t */\n\tunsigned long h_load;\n\n\t/*\n\t * this cpu's part of tg->shares\n\t */\n\tunsigned long shares;\n\n\t/*\n\t * load.weight at the time we set shares\n\t */\n\tunsigned long rq_weight;\n#endif\n#endif\n};\n\n/* Real-Time classes' related field in a runqueue: */\nstruct rt_rq {\n\tstruct rt_prio_array active;\n\tunsigned long rt_nr_running;\n#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED\n\tstruct {\n\t\tint curr; /* highest queued rt task prio */\n#ifdef CONFIG_SMP\n\t\tint next; /* next highest */\n#endif\n\t} highest_prio;\n#endif\n#ifdef CONFIG_SMP\n\tunsigned long rt_nr_migratory;\n\tunsigned long rt_nr_total;\n\tint overloaded;\n\tstruct plist_head pushable_tasks;\n#endif\n\tint rt_throttled;\n\tu64 rt_time;\n\tu64 rt_runtime;\n\t/* Nests inside the rq lock: */\n\traw_spinlock_t rt_runtime_lock;\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tunsigned long rt_nr_boosted;\n\n\tstruct rq *rq;\n\tstruct list_head leaf_rt_rq_list;\n\tstruct task_group *tg;\n#endif\n};\n\n#ifdef CONFIG_SMP\n\n/*\n * We add the notion of a root-domain which will be used to define per-domain\n * variables. Each exclusive cpuset essentially defines an island domain by\n * fully partitioning the member cpus from any other cpuset. Whenever a new\n * exclusive cpuset is created, we also create and attach a new root-domain\n * object.\n *\n */\nstruct root_domain {\n\tatomic_t refcount;\n\tcpumask_var_t span;\n\tcpumask_var_t online;\n\n\t/*\n\t * The \"RT overload\" flag: it gets set if a CPU has more than\n\t * one runnable RT task.\n\t */\n\tcpumask_var_t rto_mask;\n\tatomic_t rto_count;\n\tstruct cpupri cpupri;\n};\n\n/*\n * By default the system creates a single root-domain with all cpus as\n * members (mimicking the global state we have today).\n */\nstatic struct root_domain def_root_domain;\n\n#endif /* CONFIG_SMP */\n\n/*\n * This is the main, per-CPU runqueue data structure.\n *\n * Locking rule: those places that want to lock multiple runqueues\n * (such as the load balancing or the thread migration code), lock\n * acquire operations must be ordered by ascending &runqueue.\n */\nstruct rq {\n\t/* runqueue lock: */\n\traw_spinlock_t lock;\n\n\t/*\n\t * nr_running and cpu_load should be in the same cacheline because\n\t * remote CPUs use both these fields when doing load calculation.\n\t */\n\tunsigned long nr_running;\n\t#define CPU_LOAD_IDX_MAX 5\n\tunsigned long cpu_load[CPU_LOAD_IDX_MAX];\n\tunsigned long last_load_update_tick;\n#ifdef CONFIG_NO_HZ\n\tu64 nohz_stamp;\n\tunsigned char nohz_balance_kick;\n#endif\n\tunsigned int skip_clock_update;\n\n\t/* capture load from *all* tasks on this cpu: */\n\tstruct load_weight load;\n\tunsigned long nr_load_updates;\n\tu64 nr_switches;\n\n\tstruct cfs_rq cfs;\n\tstruct rt_rq rt;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/* list of leaf cfs_rq on this cpu: */\n\tstruct list_head leaf_cfs_rq_list;\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct list_head leaf_rt_rq_list;\n#endif\n\n\t/*\n\t * This is part of a global counter where only the total sum\n\t * over all CPUs matters. A task can increase this counter on\n\t * one CPU and if it got migrated afterwards it may decrease\n\t * it on another CPU. Always updated under the runqueue lock:\n\t */\n\tunsigned long nr_uninterruptible;\n\n\tstruct task_struct *curr, *idle, *stop;\n\tunsigned long next_balance;\n\tstruct mm_struct *prev_mm;\n\n\tu64 clock;\n\tu64 clock_task;\n\n\tatomic_t nr_iowait;\n\n#ifdef CONFIG_SMP\n\tstruct root_domain *rd;\n\tstruct sched_domain *sd;\n\n\tunsigned long cpu_power;\n\n\tunsigned char idle_at_tick;\n\t/* For active balancing */\n\tint post_schedule;\n\tint active_balance;\n\tint push_cpu;\n\tstruct cpu_stop_work active_balance_work;\n\t/* cpu of this runqueue: */\n\tint cpu;\n\tint online;\n\n\tunsigned long avg_load_per_task;\n\n\tu64 rt_avg;\n\tu64 age_stamp;\n\tu64 idle_stamp;\n\tu64 avg_idle;\n#endif\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\tu64 prev_irq_time;\n#endif\n\n\t/* calc_load related fields */\n\tunsigned long calc_load_update;\n\tlong calc_load_active;\n\n#ifdef CONFIG_SCHED_HRTICK\n#ifdef CONFIG_SMP\n\tint hrtick_csd_pending;\n\tstruct call_single_data hrtick_csd;\n#endif\n\tstruct hrtimer hrtick_timer;\n#endif\n\n#ifdef CONFIG_SCHEDSTATS\n\t/* latency stats */\n\tstruct sched_info rq_sched_info;\n\tunsigned long long rq_cpu_time;\n\t/* could above be rq->cfs_rq.exec_clock + rq->rt_rq.rt_runtime ? */\n\n\t/* sys_sched_yield() stats */\n\tunsigned int yld_count;\n\n\t/* schedule() stats */\n\tunsigned int sched_switch;\n\tunsigned int sched_count;\n\tunsigned int sched_goidle;\n\n\t/* try_to_wake_up() stats */\n\tunsigned int ttwu_count;\n\tunsigned int ttwu_local;\n\n\t/* BKL stats */\n\tunsigned int bkl_count;\n#endif\n};\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);\n\n\nstatic void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}\n\n#define rcu_dereference_check_sched_domain(p) \\\n\trcu_dereference_check((p), \\\n\t\t\t      rcu_read_lock_sched_held() || \\\n\t\t\t      lockdep_is_held(&sched_domains_mutex))\n\n/*\n * The domain tree (rq->sd) is protected by RCU's quiescent state transition.\n * See detach_destroy_domains: synchronize_sched for details.\n *\n * The domain tree of any CPU may only be accessed from within\n * preempt-disabled sections.\n */\n#define for_each_domain(cpu, __sd) \\\n\tfor (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); __sd; __sd = __sd->parent)\n\n#define cpu_rq(cpu)\t\t(&per_cpu(runqueues, (cpu)))\n#define this_rq()\t\t(&__get_cpu_var(runqueues))\n#define task_rq(p)\t\tcpu_rq(task_cpu(p))\n#define cpu_curr(cpu)\t\t(cpu_rq(cpu)->curr)\n#define raw_rq()\t\t(&__raw_get_cpu_var(runqueues))\n\n#ifdef CONFIG_CGROUP_SCHED\n\n/*\n * Return the group to which this tasks belongs.\n *\n * We use task_subsys_state_check() and extend the RCU verification\n * with lockdep_is_held(&task_rq(p)->lock) because cpu_cgroup_attach()\n * holds that lock for each task it moves into the cgroup. Therefore\n * by holding that lock, we pin the task to the current cgroup.\n */\nstatic inline struct task_group *task_group(struct task_struct *p)\n{\n\tstruct cgroup_subsys_state *css;\n\n\tcss = task_subsys_state_check(p, cpu_cgroup_subsys_id,\n\t\t\tlockdep_is_held(&task_rq(p)->lock));\n\treturn container_of(css, struct task_group, css);\n}\n\n/* Change a task's cfs_rq and parent entity if it moves across CPUs/groups */\nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu)\n{\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tp->se.cfs_rq = task_group(p)->cfs_rq[cpu];\n\tp->se.parent = task_group(p)->se[cpu];\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tp->rt.rt_rq  = task_group(p)->rt_rq[cpu];\n\tp->rt.parent = task_group(p)->rt_se[cpu];\n#endif\n}\n\n#else /* CONFIG_CGROUP_SCHED */\n\nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu) { }\nstatic inline struct task_group *task_group(struct task_struct *p)\n{\n\treturn NULL;\n}\n\n#endif /* CONFIG_CGROUP_SCHED */\n\nstatic u64 irq_time_cpu(int cpu);\nstatic void sched_irq_time_avg_update(struct rq *rq, u64 irq_time);\n\ninline void update_rq_clock(struct rq *rq)\n{\n\tint cpu = cpu_of(rq);\n\tu64 irq_time;\n\n\tif (rq->skip_clock_update)\n\t\treturn;\n\n\trq->clock = sched_clock_cpu(cpu);\n\tirq_time = irq_time_cpu(cpu);\n\tif (rq->clock - irq_time > rq->clock_task)\n\t\trq->clock_task = rq->clock - irq_time;\n\n\tsched_irq_time_avg_update(rq, irq_time);\n}\n\n/*\n * Tunables that become constants when CONFIG_SCHED_DEBUG is off:\n */\n#ifdef CONFIG_SCHED_DEBUG\n# define const_debug __read_mostly\n#else\n# define const_debug static const\n#endif\n\n/**\n * runqueue_is_locked\n * @cpu: the processor in question.\n *\n * Returns true if the current cpu runqueue is locked.\n * This interface allows printk to be called with the runqueue lock\n * held and know whether or not it is OK to wake up the klogd.\n */\nint runqueue_is_locked(int cpu)\n{\n\treturn raw_spin_is_locked(&cpu_rq(cpu)->lock);\n}\n\n/*\n * Debugging: various feature bits\n */\n\n#define SCHED_FEAT(name, enabled)\t\\\n\t__SCHED_FEAT_##name ,\n\nenum {\n#include \"sched_features.h\"\n};\n\n#undef SCHED_FEAT\n\n#define SCHED_FEAT(name, enabled)\t\\\n\t(1UL << __SCHED_FEAT_##name) * enabled |\n\nconst_debug unsigned int sysctl_sched_features =\n#include \"sched_features.h\"\n\t0;\n\n#undef SCHED_FEAT\n\n#ifdef CONFIG_SCHED_DEBUG\n#define SCHED_FEAT(name, enabled)\t\\\n\t#name ,\n\nstatic __read_mostly char *sched_feat_names[] = {\n#include \"sched_features.h\"\n\tNULL\n};\n\n#undef SCHED_FEAT\n\nstatic int sched_feat_show(struct seq_file *m, void *v)\n{\n\tint i;\n\n\tfor (i = 0; sched_feat_names[i]; i++) {\n\t\tif (!(sysctl_sched_features & (1UL << i)))\n\t\t\tseq_puts(m, \"NO_\");\n\t\tseq_printf(m, \"%s \", sched_feat_names[i]);\n\t}\n\tseq_puts(m, \"\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t\nsched_feat_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tchar *cmp;\n\tint neg = 0;\n\tint i;\n\n\tif (cnt > 63)\n\t\tcnt = 63;\n\n\tif (copy_from_user(&buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\tcmp = strstrip(buf);\n\n\tif (strncmp(buf, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\tfor (i = 0; sched_feat_names[i]; i++) {\n\t\tif (strcmp(cmp, sched_feat_names[i]) == 0) {\n\t\t\tif (neg)\n\t\t\t\tsysctl_sched_features &= ~(1UL << i);\n\t\t\telse\n\t\t\t\tsysctl_sched_features |= (1UL << i);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!sched_feat_names[i])\n\t\treturn -EINVAL;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int sched_feat_open(struct inode *inode, struct file *filp)\n{\n\treturn single_open(filp, sched_feat_show, NULL);\n}\n\nstatic const struct file_operations sched_feat_fops = {\n\t.open\t\t= sched_feat_open,\n\t.write\t\t= sched_feat_write,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n\nstatic __init int sched_init_debug(void)\n{\n\tdebugfs_create_file(\"sched_features\", 0644, NULL, NULL,\n\t\t\t&sched_feat_fops);\n\n\treturn 0;\n}\nlate_initcall(sched_init_debug);\n\n#endif\n\n#define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))\n\n/*\n * Number of tasks to iterate in a single balance run.\n * Limited because this is done with IRQs disabled.\n */\nconst_debug unsigned int sysctl_sched_nr_migrate = 32;\n\n/*\n * ratelimit for updating the group shares.\n * default: 0.25ms\n */\nunsigned int sysctl_sched_shares_ratelimit = 250000;\nunsigned int normalized_sysctl_sched_shares_ratelimit = 250000;\n\n/*\n * Inject some fuzzyness into changing the per-cpu group shares\n * this avoids remote rq-locks at the expense of fairness.\n * default: 4\n */\nunsigned int sysctl_sched_shares_thresh = 4;\n\n/*\n * period over which we average the RT time consumption, measured\n * in ms.\n *\n * default: 1s\n */\nconst_debug unsigned int sysctl_sched_time_avg = MSEC_PER_SEC;\n\n/*\n * period over which we measure -rt task cpu usage in us.\n * default: 1s\n */\nunsigned int sysctl_sched_rt_period = 1000000;\n\nstatic __read_mostly int scheduler_running;\n\n/*\n * part of the period that we allow rt tasks to run in us.\n * default: 0.95s\n */\nint sysctl_sched_rt_runtime = 950000;\n\nstatic inline u64 global_rt_period(void)\n{\n\treturn (u64)sysctl_sched_rt_period * NSEC_PER_USEC;\n}\n\nstatic inline u64 global_rt_runtime(void)\n{\n\tif (sysctl_sched_rt_runtime < 0)\n\t\treturn RUNTIME_INF;\n\n\treturn (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;\n}\n\n#ifndef prepare_arch_switch\n# define prepare_arch_switch(next)\tdo { } while (0)\n#endif\n#ifndef finish_arch_switch\n# define finish_arch_switch(prev)\tdo { } while (0)\n#endif\n\nstatic inline int task_current(struct rq *rq, struct task_struct *p)\n{\n\treturn rq->curr == p;\n}\n\n#ifndef __ARCH_WANT_UNLOCKED_CTXSW\nstatic inline int task_running(struct rq *rq, struct task_struct *p)\n{\n\treturn task_current(rq, p);\n}\n\nstatic inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)\n{\n}\n\nstatic inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)\n{\n#ifdef CONFIG_DEBUG_SPINLOCK\n\t/* this is a valid case when another task releases the spinlock */\n\trq->lock.owner = current;\n#endif\n\t/*\n\t * If we are tracking spinlock dependencies then we have to\n\t * fix up the runqueue lock - which gets 'carried over' from\n\t * prev into current:\n\t */\n\tspin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);\n\n\traw_spin_unlock_irq(&rq->lock);\n}\n\n#else /* __ARCH_WANT_UNLOCKED_CTXSW */\nstatic inline int task_running(struct rq *rq, struct task_struct *p)\n{\n#ifdef CONFIG_SMP\n\treturn p->oncpu;\n#else\n\treturn task_current(rq, p);\n#endif\n}\n\nstatic inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * We can optimise this out completely for !SMP, because the\n\t * SMP rebalancing from interrupt is the only thing that cares\n\t * here.\n\t */\n\tnext->oncpu = 1;\n#endif\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\traw_spin_unlock_irq(&rq->lock);\n#else\n\traw_spin_unlock(&rq->lock);\n#endif\n}\n\nstatic inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)\n{\n#ifdef CONFIG_SMP\n\t/*\n\t * After ->oncpu is cleared, the task can be moved to a different CPU.\n\t * We must ensure this doesn't happen until the switch is completely\n\t * finished.\n\t */\n\tsmp_wmb();\n\tprev->oncpu = 0;\n#endif\n#ifndef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_enable();\n#endif\n}\n#endif /* __ARCH_WANT_UNLOCKED_CTXSW */\n\n/*\n * Check whether the task is waking, we use this to synchronize ->cpus_allowed\n * against ttwu().\n */\nstatic inline int task_is_waking(struct task_struct *p)\n{\n\treturn unlikely(p->state == TASK_WAKING);\n}\n\n/*\n * __task_rq_lock - lock the runqueue a given task resides on.\n * Must be called interrupts disabled.\n */\nstatic inline struct rq *__task_rq_lock(struct task_struct *p)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\tif (likely(rq == task_rq(p)))\n\t\t\treturn rq;\n\t\traw_spin_unlock(&rq->lock);\n\t}\n}\n\n/*\n * task_rq_lock - lock the runqueue a given task resides on and disable\n * interrupts. Note the ordering: we can safely lookup the task_rq without\n * explicitly disabling preemption.\n */\nstatic struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\tlocal_irq_save(*flags);\n\t\trq = task_rq(p);\n\t\traw_spin_lock(&rq->lock);\n\t\tif (likely(rq == task_rq(p)))\n\t\t\treturn rq;\n\t\traw_spin_unlock_irqrestore(&rq->lock, *flags);\n\t}\n}\n\nstatic void __task_rq_unlock(struct rq *rq)\n\t__releases(rq->lock)\n{\n\traw_spin_unlock(&rq->lock);\n}\n\nstatic inline void task_rq_unlock(struct rq *rq, unsigned long *flags)\n\t__releases(rq->lock)\n{\n\traw_spin_unlock_irqrestore(&rq->lock, *flags);\n}\n\n/*\n * this_rq_lock - lock this runqueue and disable interrupts.\n */\nstatic struct rq *this_rq_lock(void)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tlocal_irq_disable();\n\trq = this_rq();\n\traw_spin_lock(&rq->lock);\n\n\treturn rq;\n}\n\n#ifdef CONFIG_SCHED_HRTICK\n/*\n * Use HR-timers to deliver accurate preemption points.\n *\n * Its all a bit involved since we cannot program an hrt while holding the\n * rq->lock. So what we do is store a state in in rq->hrtick_* and ask for a\n * reschedule event.\n *\n * When we get rescheduled we reprogram the hrtick_timer outside of the\n * rq->lock.\n */\n\n/*\n * Use hrtick when:\n *  - enabled by features\n *  - hrtimer is actually high res\n */\nstatic inline int hrtick_enabled(struct rq *rq)\n{\n\tif (!sched_feat(HRTICK))\n\t\treturn 0;\n\tif (!cpu_active(cpu_of(rq)))\n\t\treturn 0;\n\treturn hrtimer_is_hres_active(&rq->hrtick_timer);\n}\n\nstatic void hrtick_clear(struct rq *rq)\n{\n\tif (hrtimer_active(&rq->hrtick_timer))\n\t\thrtimer_cancel(&rq->hrtick_timer);\n}\n\n/*\n * High-resolution timer tick.\n * Runs from hardirq context with interrupts disabled.\n */\nstatic enum hrtimer_restart hrtick(struct hrtimer *timer)\n{\n\tstruct rq *rq = container_of(timer, struct rq, hrtick_timer);\n\n\tWARN_ON_ONCE(cpu_of(rq) != smp_processor_id());\n\n\traw_spin_lock(&rq->lock);\n\tupdate_rq_clock(rq);\n\trq->curr->sched_class->task_tick(rq, rq->curr, 1);\n\traw_spin_unlock(&rq->lock);\n\n\treturn HRTIMER_NORESTART;\n}\n\n#ifdef CONFIG_SMP\n/*\n * called from hardirq (IPI) context\n */\nstatic void __hrtick_start(void *arg)\n{\n\tstruct rq *rq = arg;\n\n\traw_spin_lock(&rq->lock);\n\thrtimer_restart(&rq->hrtick_timer);\n\trq->hrtick_csd_pending = 0;\n\traw_spin_unlock(&rq->lock);\n}\n\n/*\n * Called to set the hrtick timer state.\n *\n * called with rq->lock held and irqs disabled\n */\nstatic void hrtick_start(struct rq *rq, u64 delay)\n{\n\tstruct hrtimer *timer = &rq->hrtick_timer;\n\tktime_t time = ktime_add_ns(timer->base->get_time(), delay);\n\n\thrtimer_set_expires(timer, time);\n\n\tif (rq == this_rq()) {\n\t\thrtimer_restart(timer);\n\t} else if (!rq->hrtick_csd_pending) {\n\t\t__smp_call_function_single(cpu_of(rq), &rq->hrtick_csd, 0);\n\t\trq->hrtick_csd_pending = 1;\n\t}\n}\n\nstatic int\nhotplug_hrtick(struct notifier_block *nfb, unsigned long action, void *hcpu)\n{\n\tint cpu = (int)(long)hcpu;\n\n\tswitch (action) {\n\tcase CPU_UP_CANCELED:\n\tcase CPU_UP_CANCELED_FROZEN:\n\tcase CPU_DOWN_PREPARE:\n\tcase CPU_DOWN_PREPARE_FROZEN:\n\tcase CPU_DEAD:\n\tcase CPU_DEAD_FROZEN:\n\t\thrtick_clear(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstatic __init void init_hrtick(void)\n{\n\thotcpu_notifier(hotplug_hrtick, 0);\n}\n#else\n/*\n * Called to set the hrtick timer state.\n *\n * called with rq->lock held and irqs disabled\n */\nstatic void hrtick_start(struct rq *rq, u64 delay)\n{\n\t__hrtimer_start_range_ns(&rq->hrtick_timer, ns_to_ktime(delay), 0,\n\t\t\tHRTIMER_MODE_REL_PINNED, 0);\n}\n\nstatic inline void init_hrtick(void)\n{\n}\n#endif /* CONFIG_SMP */\n\nstatic void init_rq_hrtick(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\trq->hrtick_csd_pending = 0;\n\n\trq->hrtick_csd.flags = 0;\n\trq->hrtick_csd.func = __hrtick_start;\n\trq->hrtick_csd.info = rq;\n#endif\n\n\thrtimer_init(&rq->hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\trq->hrtick_timer.function = hrtick;\n}\n#else\t/* CONFIG_SCHED_HRTICK */\nstatic inline void hrtick_clear(struct rq *rq)\n{\n}\n\nstatic inline void init_rq_hrtick(struct rq *rq)\n{\n}\n\nstatic inline void init_hrtick(void)\n{\n}\n#endif\t/* CONFIG_SCHED_HRTICK */\n\n/*\n * resched_task - mark a task 'to be rescheduled now'.\n *\n * On UP this means the setting of the need_resched flag, on SMP it\n * might also involve a cross-CPU call to trigger the scheduler on\n * the target CPU.\n */\n#ifdef CONFIG_SMP\n\n#ifndef tsk_is_polling\n#define tsk_is_polling(t) test_tsk_thread_flag(t, TIF_POLLING_NRFLAG)\n#endif\n\nstatic void resched_task(struct task_struct *p)\n{\n\tint cpu;\n\n\tassert_raw_spin_locked(&task_rq(p)->lock);\n\n\tif (test_tsk_need_resched(p))\n\t\treturn;\n\n\tset_tsk_need_resched(p);\n\n\tcpu = task_cpu(p);\n\tif (cpu == smp_processor_id())\n\t\treturn;\n\n\t/* NEED_RESCHED must be visible before we test polling */\n\tsmp_mb();\n\tif (!tsk_is_polling(p))\n\t\tsmp_send_reschedule(cpu);\n}\n\nstatic void resched_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\tif (!raw_spin_trylock_irqsave(&rq->lock, flags))\n\t\treturn;\n\tresched_task(cpu_curr(cpu));\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n}\n\n#ifdef CONFIG_NO_HZ\n/*\n * In the semi idle case, use the nearest busy cpu for migrating timers\n * from an idle cpu.  This is good for power-savings.\n *\n * We don't do similar optimization for completely idle system, as\n * selecting an idle cpu will add more delays to the timers than intended\n * (as that cpu's timer base may not be uptodate wrt jiffies etc).\n */\nint get_nohz_timer_target(void)\n{\n\tint cpu = smp_processor_id();\n\tint i;\n\tstruct sched_domain *sd;\n\n\tfor_each_domain(cpu, sd) {\n\t\tfor_each_cpu(i, sched_domain_span(sd))\n\t\t\tif (!idle_cpu(i))\n\t\t\t\treturn i;\n\t}\n\treturn cpu;\n}\n/*\n * When add_timer_on() enqueues a timer into the timer wheel of an\n * idle CPU then this timer might expire before the next timer event\n * which is scheduled to wake up that CPU. In case of a completely\n * idle system the next event might even be infinite time into the\n * future. wake_up_idle_cpu() ensures that the CPU is woken up and\n * leaves the inner idle loop so the newly added timer is taken into\n * account when the CPU goes back to idle and evaluates the timer\n * wheel for the next timer event.\n */\nvoid wake_up_idle_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tif (cpu == smp_processor_id())\n\t\treturn;\n\n\t/*\n\t * This is safe, as this function is called with the timer\n\t * wheel base lock of (cpu) held. When the CPU is on the way\n\t * to idle and has not yet set rq->curr to idle then it will\n\t * be serialized on the timer wheel base lock and take the new\n\t * timer into account automatically.\n\t */\n\tif (rq->curr != rq->idle)\n\t\treturn;\n\n\t/*\n\t * We can set TIF_RESCHED on the idle task of the other CPU\n\t * lockless. The worst case is that the other CPU runs the\n\t * idle task through an additional NOOP schedule()\n\t */\n\tset_tsk_need_resched(rq->idle);\n\n\t/* NEED_RESCHED must be visible before we test polling */\n\tsmp_mb();\n\tif (!tsk_is_polling(rq->idle))\n\t\tsmp_send_reschedule(cpu);\n}\n\n#endif /* CONFIG_NO_HZ */\n\nstatic u64 sched_avg_period(void)\n{\n\treturn (u64)sysctl_sched_time_avg * NSEC_PER_MSEC / 2;\n}\n\nstatic void sched_avg_update(struct rq *rq)\n{\n\ts64 period = sched_avg_period();\n\n\twhile ((s64)(rq->clock - rq->age_stamp) > period) {\n\t\t/*\n\t\t * Inline assembly required to prevent the compiler\n\t\t * optimising this loop into a divmod call.\n\t\t * See __iter_div_u64_rem() for another example of this.\n\t\t */\n\t\tasm(\"\" : \"+rm\" (rq->age_stamp));\n\t\trq->age_stamp += period;\n\t\trq->rt_avg /= 2;\n\t}\n}\n\nstatic void sched_rt_avg_update(struct rq *rq, u64 rt_delta)\n{\n\trq->rt_avg += rt_delta;\n\tsched_avg_update(rq);\n}\n\n#else /* !CONFIG_SMP */\nstatic void resched_task(struct task_struct *p)\n{\n\tassert_raw_spin_locked(&task_rq(p)->lock);\n\tset_tsk_need_resched(p);\n}\n\nstatic void sched_rt_avg_update(struct rq *rq, u64 rt_delta)\n{\n}\n\nstatic void sched_avg_update(struct rq *rq)\n{\n}\n#endif /* CONFIG_SMP */\n\n#if BITS_PER_LONG == 32\n# define WMULT_CONST\t(~0UL)\n#else\n# define WMULT_CONST\t(1UL << 32)\n#endif\n\n#define WMULT_SHIFT\t32\n\n/*\n * Shift right and round:\n */\n#define SRR(x, y) (((x) + (1UL << ((y) - 1))) >> (y))\n\n/*\n * delta *= weight / lw\n */\nstatic unsigned long\ncalc_delta_mine(unsigned long delta_exec, unsigned long weight,\n\t\tstruct load_weight *lw)\n{\n\tu64 tmp;\n\n\tif (!lw->inv_weight) {\n\t\tif (BITS_PER_LONG > 32 && unlikely(lw->weight >= WMULT_CONST))\n\t\t\tlw->inv_weight = 1;\n\t\telse\n\t\t\tlw->inv_weight = 1 + (WMULT_CONST-lw->weight/2)\n\t\t\t\t/ (lw->weight+1);\n\t}\n\n\ttmp = (u64)delta_exec * weight;\n\t/*\n\t * Check whether we'd overflow the 64-bit multiplication:\n\t */\n\tif (unlikely(tmp > WMULT_CONST))\n\t\ttmp = SRR(SRR(tmp, WMULT_SHIFT/2) * lw->inv_weight,\n\t\t\tWMULT_SHIFT/2);\n\telse\n\t\ttmp = SRR(tmp * lw->inv_weight, WMULT_SHIFT);\n\n\treturn (unsigned long)min(tmp, (u64)(unsigned long)LONG_MAX);\n}\n\nstatic inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}\n\n/*\n * To aid in avoiding the subversion of \"niceness\" due to uneven distribution\n * of tasks with abnormal \"nice\" values across CPUs the contribution that\n * each task makes to its run queue's load is weighted according to its\n * scheduling class and \"nice\" value. For SCHED_NORMAL tasks this is just a\n * scaled version of the new time slice allocation that they receive on time\n * slice expiry etc.\n */\n\n#define WEIGHT_IDLEPRIO                3\n#define WMULT_IDLEPRIO         1431655765\n\n/*\n * Nice levels are multiplicative, with a gentle 10% change for every\n * nice level changed. I.e. when a CPU-bound task goes from nice 0 to\n * nice 1, it will get ~10% less CPU time than another CPU-bound task\n * that remained on nice 0.\n *\n * The \"10% effect\" is relative and cumulative: from _any_ nice level,\n * if you go up 1 level, it's -10% CPU usage, if you go down 1 level\n * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.\n * If a task goes up by ~10% and another task goes down by ~10% then\n * the relative distance between them is ~25%.)\n */\nstatic const int prio_to_weight[40] = {\n /* -20 */     88761,     71755,     56483,     46273,     36291,\n /* -15 */     29154,     23254,     18705,     14949,     11916,\n /* -10 */      9548,      7620,      6100,      4904,      3906,\n /*  -5 */      3121,      2501,      1991,      1586,      1277,\n /*   0 */      1024,       820,       655,       526,       423,\n /*   5 */       335,       272,       215,       172,       137,\n /*  10 */       110,        87,        70,        56,        45,\n /*  15 */        36,        29,        23,        18,        15,\n};\n\n/*\n * Inverse (2^32/x) values of the prio_to_weight[] array, precalculated.\n *\n * In cases where the weight does not change often, we can use the\n * precalculated inverse to speed up arithmetics by turning divisions\n * into multiplications:\n */\nstatic const u32 prio_to_wmult[40] = {\n /* -20 */     48388,     59856,     76040,     92818,    118348,\n /* -15 */    147320,    184698,    229616,    287308,    360437,\n /* -10 */    449829,    563644,    704093,    875809,   1099582,\n /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,\n /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,\n /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,\n /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,\n /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,\n};\n\n/* Time spent by the tasks of the cpu accounting group executing in ... */\nenum cpuacct_stat_index {\n\tCPUACCT_STAT_USER,\t/* ... user mode */\n\tCPUACCT_STAT_SYSTEM,\t/* ... kernel mode */\n\n\tCPUACCT_STAT_NSTATS,\n};\n\n#ifdef CONFIG_CGROUP_CPUACCT\nstatic void cpuacct_charge(struct task_struct *tsk, u64 cputime);\nstatic void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val);\n#else\nstatic inline void cpuacct_charge(struct task_struct *tsk, u64 cputime) {}\nstatic inline void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val) {}\n#endif\n\nstatic inline void inc_cpu_load(struct rq *rq, unsigned long load)\n{\n\tupdate_load_add(&rq->load, load);\n}\n\nstatic inline void dec_cpu_load(struct rq *rq, unsigned long load)\n{\n\tupdate_load_sub(&rq->load, load);\n}\n\n#if (defined(CONFIG_SMP) && defined(CONFIG_FAIR_GROUP_SCHED)) || defined(CONFIG_RT_GROUP_SCHED)\ntypedef int (*tg_visitor)(struct task_group *, void *);\n\n/*\n * Iterate the full tree, calling @down when first entering a node and @up when\n * leaving it for the final time.\n */\nstatic int walk_tg_tree(tg_visitor down, tg_visitor up, void *data)\n{\n\tstruct task_group *parent, *child;\n\tint ret;\n\n\trcu_read_lock();\n\tparent = &root_task_group;\ndown:\n\tret = (*down)(parent, data);\n\tif (ret)\n\t\tgoto out_unlock;\n\tlist_for_each_entry_rcu(child, &parent->children, siblings) {\n\t\tparent = child;\n\t\tgoto down;\n\nup:\n\t\tcontinue;\n\t}\n\tret = (*up)(parent, data);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tchild = parent;\n\tparent = parent->parent;\n\tif (parent)\n\t\tgoto up;\nout_unlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int tg_nop(struct task_group *tg, void *data)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_SMP\n/* Used instead of source_load when we know the type == 0 */\nstatic unsigned long weighted_cpuload(const int cpu)\n{\n\treturn cpu_rq(cpu)->load.weight;\n}\n\n/*\n * Return a low guess at the load of a migration-source cpu weighted\n * according to the scheduling class and \"nice\" value.\n *\n * We want to under-estimate the load of migration sources, to\n * balance conservatively.\n */\nstatic unsigned long source_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(cpu);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn min(rq->cpu_load[type-1], total);\n}\n\n/*\n * Return a high guess at the load of a migration-target cpu weighted\n * according to the scheduling class and \"nice\" value.\n */\nstatic unsigned long target_load(int cpu, int type)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long total = weighted_cpuload(cpu);\n\n\tif (type == 0 || !sched_feat(LB_BIAS))\n\t\treturn total;\n\n\treturn max(rq->cpu_load[type-1], total);\n}\n\nstatic unsigned long power_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_power;\n}\n\nstatic int task_hot(struct task_struct *p, u64 now, struct sched_domain *sd);\n\nstatic unsigned long cpu_avg_load_per_task(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long nr_running = ACCESS_ONCE(rq->nr_running);\n\n\tif (nr_running)\n\t\trq->avg_load_per_task = rq->load.weight / nr_running;\n\telse\n\t\trq->avg_load_per_task = 0;\n\n\treturn rq->avg_load_per_task;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\nstatic __read_mostly unsigned long __percpu *update_shares_data;\n\nstatic void __set_se_shares(struct sched_entity *se, unsigned long shares);\n\n/*\n * Calculate and set the cpu's group shares.\n */\nstatic void update_group_shares_cpu(struct task_group *tg, int cpu,\n\t\t\t\t    unsigned long sd_shares,\n\t\t\t\t    unsigned long sd_rq_weight,\n\t\t\t\t    unsigned long *usd_rq_weight)\n{\n\tunsigned long shares, rq_weight;\n\tint boost = 0;\n\n\trq_weight = usd_rq_weight[cpu];\n\tif (!rq_weight) {\n\t\tboost = 1;\n\t\trq_weight = NICE_0_LOAD;\n\t}\n\n\t/*\n\t *             \\Sum_j shares_j * rq_weight_i\n\t * shares_i =  -----------------------------\n\t *                  \\Sum_j rq_weight_j\n\t */\n\tshares = (sd_shares * rq_weight) / sd_rq_weight;\n\tshares = clamp_t(unsigned long, shares, MIN_SHARES, MAX_SHARES);\n\n\tif (abs(shares - tg->se[cpu]->load.weight) >\n\t\t\tsysctl_sched_shares_thresh) {\n\t\tstruct rq *rq = cpu_rq(cpu);\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\ttg->cfs_rq[cpu]->rq_weight = boost ? 0 : rq_weight;\n\t\ttg->cfs_rq[cpu]->shares = boost ? 0 : shares;\n\t\t__set_se_shares(tg->se[cpu], shares);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t}\n}\n\n/*\n * Re-compute the task group their per cpu shares over the given domain.\n * This needs to be done in a bottom-up fashion because the rq weight of a\n * parent group depends on the shares of its child groups.\n */\nstatic int tg_shares_up(struct task_group *tg, void *data)\n{\n\tunsigned long weight, rq_weight = 0, sum_weight = 0, shares = 0;\n\tunsigned long *usd_rq_weight;\n\tstruct sched_domain *sd = data;\n\tunsigned long flags;\n\tint i;\n\n\tif (!tg->se[0])\n\t\treturn 0;\n\n\tlocal_irq_save(flags);\n\tusd_rq_weight = per_cpu_ptr(update_shares_data, smp_processor_id());\n\n\tfor_each_cpu(i, sched_domain_span(sd)) {\n\t\tweight = tg->cfs_rq[i]->load.weight;\n\t\tusd_rq_weight[i] = weight;\n\n\t\trq_weight += weight;\n\t\t/*\n\t\t * If there are currently no tasks on the cpu pretend there\n\t\t * is one of average load so that when a new task gets to\n\t\t * run here it will not get delayed by group starvation.\n\t\t */\n\t\tif (!weight)\n\t\t\tweight = NICE_0_LOAD;\n\n\t\tsum_weight += weight;\n\t\tshares += tg->cfs_rq[i]->shares;\n\t}\n\n\tif (!rq_weight)\n\t\trq_weight = sum_weight;\n\n\tif ((!shares && rq_weight) || shares > tg->shares)\n\t\tshares = tg->shares;\n\n\tif (!sd->parent || !(sd->parent->flags & SD_LOAD_BALANCE))\n\t\tshares = tg->shares;\n\n\tfor_each_cpu(i, sched_domain_span(sd))\n\t\tupdate_group_shares_cpu(tg, i, shares, rq_weight, usd_rq_weight);\n\n\tlocal_irq_restore(flags);\n\n\treturn 0;\n}\n\n/*\n * Compute the cpu's hierarchical load factor for each task group.\n * This needs to be done in a top-down fashion because the load of a child\n * group is a fraction of its parents load.\n */\nstatic int tg_load_down(struct task_group *tg, void *data)\n{\n\tunsigned long load;\n\tlong cpu = (long)data;\n\n\tif (!tg->parent) {\n\t\tload = cpu_rq(cpu)->load.weight;\n\t} else {\n\t\tload = tg->parent->cfs_rq[cpu]->h_load;\n\t\tload *= tg->cfs_rq[cpu]->shares;\n\t\tload /= tg->parent->cfs_rq[cpu]->load.weight + 1;\n\t}\n\n\ttg->cfs_rq[cpu]->h_load = load;\n\n\treturn 0;\n}\n\nstatic void update_shares(struct sched_domain *sd)\n{\n\ts64 elapsed;\n\tu64 now;\n\n\tif (root_task_group_empty())\n\t\treturn;\n\n\tnow = local_clock();\n\telapsed = now - sd->last_update;\n\n\tif (elapsed >= (s64)(u64)sysctl_sched_shares_ratelimit) {\n\t\tsd->last_update = now;\n\t\twalk_tg_tree(tg_nop, tg_shares_up, sd);\n\t}\n}\n\nstatic void update_h_load(long cpu)\n{\n\twalk_tg_tree(tg_load_down, tg_nop, (void *)cpu);\n}\n\n#else\n\nstatic inline void update_shares(struct sched_domain *sd)\n{\n}\n\n#endif\n\n#ifdef CONFIG_PREEMPT\n\nstatic void double_rq_lock(struct rq *rq1, struct rq *rq2);\n\n/*\n * fair double_lock_balance: Safely acquires both rq->locks in a fair\n * way at the expense of forcing extra atomic operations in all\n * invocations.  This assures that the double_lock is acquired using the\n * same underlying policy as the spinlock_t on this architecture, which\n * reduces latency compared to the unfair variant below.  However, it\n * also adds more overhead and therefore may reduce throughput.\n */\nstatic inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(this_rq->lock)\n\t__acquires(busiest->lock)\n\t__acquires(this_rq->lock)\n{\n\traw_spin_unlock(&this_rq->lock);\n\tdouble_rq_lock(this_rq, busiest);\n\n\treturn 1;\n}\n\n#else\n/*\n * Unfair double_lock_balance: Optimizes throughput at the expense of\n * latency by eliminating extra atomic operations when the locks are\n * already in proper order on entry.  This favors lower cpu-ids and will\n * grant the double lock to lower cpus over higher ids under contention,\n * regardless of entry order into the function.\n */\nstatic int _double_lock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(this_rq->lock)\n\t__acquires(busiest->lock)\n\t__acquires(this_rq->lock)\n{\n\tint ret = 0;\n\n\tif (unlikely(!raw_spin_trylock(&busiest->lock))) {\n\t\tif (busiest < this_rq) {\n\t\t\traw_spin_unlock(&this_rq->lock);\n\t\t\traw_spin_lock(&busiest->lock);\n\t\t\traw_spin_lock_nested(&this_rq->lock,\n\t\t\t\t\t      SINGLE_DEPTH_NESTING);\n\t\t\tret = 1;\n\t\t} else\n\t\t\traw_spin_lock_nested(&busiest->lock,\n\t\t\t\t\t      SINGLE_DEPTH_NESTING);\n\t}\n\treturn ret;\n}\n\n#endif /* CONFIG_PREEMPT */\n\n/*\n * double_lock_balance - lock the busiest runqueue, this_rq is locked already.\n */\nstatic int double_lock_balance(struct rq *this_rq, struct rq *busiest)\n{\n\tif (unlikely(!irqs_disabled())) {\n\t\t/* printk() doesn't work good under rq->lock */\n\t\traw_spin_unlock(&this_rq->lock);\n\t\tBUG_ON(1);\n\t}\n\n\treturn _double_lock_balance(this_rq, busiest);\n}\n\nstatic inline void double_unlock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(busiest->lock)\n{\n\traw_spin_unlock(&busiest->lock);\n\tlock_set_subclass(&this_rq->lock.dep_map, 0, _RET_IP_);\n}\n\n/*\n * double_rq_lock - safely lock two runqueues\n *\n * Note this does not disable interrupts like task_rq_lock,\n * you need to do so manually before calling.\n */\nstatic void double_rq_lock(struct rq *rq1, struct rq *rq2)\n\t__acquires(rq1->lock)\n\t__acquires(rq2->lock)\n{\n\tBUG_ON(!irqs_disabled());\n\tif (rq1 == rq2) {\n\t\traw_spin_lock(&rq1->lock);\n\t\t__acquire(rq2->lock);\t/* Fake it out ;) */\n\t} else {\n\t\tif (rq1 < rq2) {\n\t\t\traw_spin_lock(&rq1->lock);\n\t\t\traw_spin_lock_nested(&rq2->lock, SINGLE_DEPTH_NESTING);\n\t\t} else {\n\t\t\traw_spin_lock(&rq2->lock);\n\t\t\traw_spin_lock_nested(&rq1->lock, SINGLE_DEPTH_NESTING);\n\t\t}\n\t}\n}\n\n/*\n * double_rq_unlock - safely unlock two runqueues\n *\n * Note this does not restore interrupts like task_rq_unlock,\n * you need to do so manually after calling.\n */\nstatic void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\traw_spin_unlock(&rq1->lock);\n\tif (rq1 != rq2)\n\t\traw_spin_unlock(&rq2->lock);\n\telse\n\t\t__release(rq2->lock);\n}\n\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void cfs_rq_set_shares(struct cfs_rq *cfs_rq, unsigned long shares)\n{\n#ifdef CONFIG_SMP\n\tcfs_rq->shares = shares;\n#endif\n}\n#endif\n\nstatic void calc_load_account_idle(struct rq *this_rq);\nstatic void update_sysctl(void);\nstatic int get_update_sysctl_factor(void);\nstatic void update_cpu_load(struct rq *this_rq);\n\nstatic inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n\tset_task_rq(p, cpu);\n#ifdef CONFIG_SMP\n\t/*\n\t * After ->cpu is set up to a new value, task_rq_lock(p, ...) can be\n\t * successfuly executed on another CPU. We must ensure that updates of\n\t * per-task data have been completed by this moment.\n\t */\n\tsmp_wmb();\n\ttask_thread_info(p)->cpu = cpu;\n#endif\n}\n\nstatic const struct sched_class rt_sched_class;\n\n#define sched_class_highest (&stop_sched_class)\n#define for_each_class(class) \\\n   for (class = sched_class_highest; class; class = class->next)\n\n#include \"sched_stats.h\"\n\nstatic void inc_nr_running(struct rq *rq)\n{\n\trq->nr_running++;\n}\n\nstatic void dec_nr_running(struct rq *rq)\n{\n\trq->nr_running--;\n}\n\nstatic void set_load_weight(struct task_struct *p)\n{\n\t/*\n\t * SCHED_IDLE tasks get minimal weight:\n\t */\n\tif (p->policy == SCHED_IDLE) {\n\t\tp->se.load.weight = WEIGHT_IDLEPRIO;\n\t\tp->se.load.inv_weight = WMULT_IDLEPRIO;\n\t\treturn;\n\t}\n\n\tp->se.load.weight = prio_to_weight[p->static_prio - MAX_RT_PRIO];\n\tp->se.load.inv_weight = prio_to_wmult[p->static_prio - MAX_RT_PRIO];\n}\n\nstatic void enqueue_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tupdate_rq_clock(rq);\n\tsched_info_queued(p);\n\tp->sched_class->enqueue_task(rq, p, flags);\n\tp->se.on_rq = 1;\n}\n\nstatic void dequeue_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tupdate_rq_clock(rq);\n\tsched_info_dequeued(p);\n\tp->sched_class->dequeue_task(rq, p, flags);\n\tp->se.on_rq = 0;\n}\n\n/*\n * activate_task - move a task to the runqueue.\n */\nstatic void activate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_contributes_to_load(p))\n\t\trq->nr_uninterruptible--;\n\n\tenqueue_task(rq, p, flags);\n\tinc_nr_running(rq);\n}\n\n/*\n * deactivate_task - remove a task from the runqueue.\n */\nstatic void deactivate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_contributes_to_load(p))\n\t\trq->nr_uninterruptible++;\n\n\tdequeue_task(rq, p, flags);\n\tdec_nr_running(rq);\n}\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\n/*\n * There are no locks covering percpu hardirq/softirq time.\n * They are only modified in account_system_vtime, on corresponding CPU\n * with interrupts disabled. So, writes are safe.\n * They are read and saved off onto struct rq in update_rq_clock().\n * This may result in other CPU reading this CPU's irq time and can\n * race with irq/account_system_vtime on this CPU. We would either get old\n * or new value (or semi updated value on 32 bit) with a side effect of\n * accounting a slice of irq time to wrong task when irq is in progress\n * while we read rq->clock. That is a worthy compromise in place of having\n * locks on each irq in account_system_time.\n */\nstatic DEFINE_PER_CPU(u64, cpu_hardirq_time);\nstatic DEFINE_PER_CPU(u64, cpu_softirq_time);\n\nstatic DEFINE_PER_CPU(u64, irq_start_time);\nstatic int sched_clock_irqtime;\n\nvoid enable_sched_clock_irqtime(void)\n{\n\tsched_clock_irqtime = 1;\n}\n\nvoid disable_sched_clock_irqtime(void)\n{\n\tsched_clock_irqtime = 0;\n}\n\nstatic u64 irq_time_cpu(int cpu)\n{\n\tif (!sched_clock_irqtime)\n\t\treturn 0;\n\n\treturn per_cpu(cpu_softirq_time, cpu) + per_cpu(cpu_hardirq_time, cpu);\n}\n\nvoid account_system_vtime(struct task_struct *curr)\n{\n\tunsigned long flags;\n\tint cpu;\n\tu64 now, delta;\n\n\tif (!sched_clock_irqtime)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tcpu = smp_processor_id();\n\tnow = sched_clock_cpu(cpu);\n\tdelta = now - per_cpu(irq_start_time, cpu);\n\tper_cpu(irq_start_time, cpu) = now;\n\t/*\n\t * We do not account for softirq time from ksoftirqd here.\n\t * We want to continue accounting softirq time to ksoftirqd thread\n\t * in that case, so as not to confuse scheduler with a special task\n\t * that do not consume any time, but still wants to run.\n\t */\n\tif (hardirq_count())\n\t\tper_cpu(cpu_hardirq_time, cpu) += delta;\n\telse if (in_serving_softirq() && !(curr->flags & PF_KSOFTIRQD))\n\t\tper_cpu(cpu_softirq_time, cpu) += delta;\n\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(account_system_vtime);\n\nstatic void sched_irq_time_avg_update(struct rq *rq, u64 curr_irq_time)\n{\n\tif (sched_clock_irqtime && sched_feat(NONIRQ_POWER)) {\n\t\tu64 delta_irq = curr_irq_time - rq->prev_irq_time;\n\t\trq->prev_irq_time = curr_irq_time;\n\t\tsched_rt_avg_update(rq, delta_irq);\n\t}\n}\n\n#else\n\nstatic u64 irq_time_cpu(int cpu)\n{\n\treturn 0;\n}\n\nstatic void sched_irq_time_avg_update(struct rq *rq, u64 curr_irq_time) { }\n\n#endif\n\n#include \"sched_idletask.c\"\n#include \"sched_fair.c\"\n#include \"sched_rt.c\"\n#include \"sched_stoptask.c\"\n#ifdef CONFIG_SCHED_DEBUG\n# include \"sched_debug.c\"\n#endif\n\nvoid sched_set_stop_task(int cpu, struct task_struct *stop)\n{\n\tstruct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };\n\tstruct task_struct *old_stop = cpu_rq(cpu)->stop;\n\n\tif (stop) {\n\t\t/*\n\t\t * Make it appear like a SCHED_FIFO task, its something\n\t\t * userspace knows about and won't get confused about.\n\t\t *\n\t\t * Also, it will make PI more or less work without too\n\t\t * much confusion -- but then, stop work should not\n\t\t * rely on PI working anyway.\n\t\t */\n\t\tsched_setscheduler_nocheck(stop, SCHED_FIFO, &param);\n\n\t\tstop->sched_class = &stop_sched_class;\n\t}\n\n\tcpu_rq(cpu)->stop = stop;\n\n\tif (old_stop) {\n\t\t/*\n\t\t * Reset it back to a normal scheduling class so that\n\t\t * it can die in pieces.\n\t\t */\n\t\told_stop->sched_class = &rt_sched_class;\n\t}\n}\n\n/*\n * __normal_prio - return the priority that is based on the static prio\n */\nstatic inline int __normal_prio(struct task_struct *p)\n{\n\treturn p->static_prio;\n}\n\n/*\n * Calculate the expected normal priority: i.e. priority\n * without taking RT-inheritance into account. Might be\n * boosted by interactivity modifiers. Changes upon fork,\n * setprio syscalls, and whenever the interactivity\n * estimator recalculates.\n */\nstatic inline int normal_prio(struct task_struct *p)\n{\n\tint prio;\n\n\tif (task_has_rt_policy(p))\n\t\tprio = MAX_RT_PRIO-1 - p->rt_priority;\n\telse\n\t\tprio = __normal_prio(p);\n\treturn prio;\n}\n\n/*\n * Calculate the current priority, i.e. the priority\n * taken into account by the scheduler. This value might\n * be boosted by RT tasks, or might be boosted by\n * interactivity modifiers. Will be RT if the task got\n * RT-boosted. If not then it returns p->normal_prio.\n */\nstatic int effective_prio(struct task_struct *p)\n{\n\tp->normal_prio = normal_prio(p);\n\t/*\n\t * If we are RT tasks or we were boosted to RT priority,\n\t * keep the priority unchanged. Otherwise, update priority\n\t * to the normal priority:\n\t */\n\tif (!rt_prio(p->prio))\n\t\treturn p->normal_prio;\n\treturn p->prio;\n}\n\n/**\n * task_curr - is this task currently executing on a CPU?\n * @p: the task in question.\n */\ninline int task_curr(const struct task_struct *p)\n{\n\treturn cpu_curr(task_cpu(p)) == p;\n}\n\nstatic inline void check_class_changed(struct rq *rq, struct task_struct *p,\n\t\t\t\t       const struct sched_class *prev_class,\n\t\t\t\t       int oldprio, int running)\n{\n\tif (prev_class != p->sched_class) {\n\t\tif (prev_class->switched_from)\n\t\t\tprev_class->switched_from(rq, p, running);\n\t\tp->sched_class->switched_to(rq, p, running);\n\t} else\n\t\tp->sched_class->prio_changed(rq, p, oldprio, running);\n}\n\nstatic void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)\n{\n\tconst struct sched_class *class;\n\n\tif (p->sched_class == rq->curr->sched_class) {\n\t\trq->curr->sched_class->check_preempt_curr(rq, p, flags);\n\t} else {\n\t\tfor_each_class(class) {\n\t\t\tif (class == rq->curr->sched_class)\n\t\t\t\tbreak;\n\t\t\tif (class == p->sched_class) {\n\t\t\t\tresched_task(rq->curr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * A queue event has occurred, and we're going to schedule.  In\n\t * this case, we can save a useless back to back clock update.\n\t */\n\tif (rq->curr->se.on_rq && test_tsk_need_resched(rq->curr))\n\t\trq->skip_clock_update = 1;\n}\n\n#ifdef CONFIG_SMP\n/*\n * Is this task likely cache-hot:\n */\nstatic int\ntask_hot(struct task_struct *p, u64 now, struct sched_domain *sd)\n{\n\ts64 delta;\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn 0;\n\n\tif (unlikely(p->policy == SCHED_IDLE))\n\t\treturn 0;\n\n\t/*\n\t * Buddy candidates are cache hot:\n\t */\n\tif (sched_feat(CACHE_HOT_BUDDY) && this_rq()->nr_running &&\n\t\t\t(&p->se == cfs_rq_of(&p->se)->next ||\n\t\t\t &p->se == cfs_rq_of(&p->se)->last))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == -1)\n\t\treturn 1;\n\tif (sysctl_sched_migration_cost == 0)\n\t\treturn 0;\n\n\tdelta = now - p->se.exec_start;\n\n\treturn delta < (s64)sysctl_sched_migration_cost;\n}\n\nvoid set_task_cpu(struct task_struct *p, unsigned int new_cpu)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\t/*\n\t * We should never call set_task_cpu() on a blocked task,\n\t * ttwu() will sort out the placement.\n\t */\n\tWARN_ON_ONCE(p->state != TASK_RUNNING && p->state != TASK_WAKING &&\n\t\t\t!(task_thread_info(p)->preempt_count & PREEMPT_ACTIVE));\n#endif\n\n\ttrace_sched_migrate_task(p, new_cpu);\n\n\tif (task_cpu(p) != new_cpu) {\n\t\tp->se.nr_migrations++;\n\t\tperf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, 1, NULL, 0);\n\t}\n\n\t__set_task_cpu(p, new_cpu);\n}\n\nstruct migration_arg {\n\tstruct task_struct *task;\n\tint dest_cpu;\n};\n\nstatic int migration_cpu_stop(void *data);\n\n/*\n * The task's runqueue lock must be held.\n * Returns true if you have to wait for migration thread.\n */\nstatic bool migrate_task(struct task_struct *p, int dest_cpu)\n{\n\tstruct rq *rq = task_rq(p);\n\n\t/*\n\t * If the task is not on a runqueue (and not running), then\n\t * the next wake-up will properly place the task.\n\t */\n\treturn p->se.on_rq || task_running(rq, p);\n}\n\n/*\n * wait_task_inactive - wait for a thread to unschedule.\n *\n * If @match_state is nonzero, it's the @p->state value just checked and\n * not expected to change.  If it changes, i.e. @p might have woken up,\n * then return zero.  When we succeed in waiting for @p to be off its CPU,\n * we return a positive number (its total switch count).  If a second call\n * a short while later returns the same number, the caller can be sure that\n * @p has remained unscheduled the whole time.\n *\n * The caller must ensure that the task *will* unschedule sometime soon,\n * else this function might spin for a *long* time. This function can't\n * be called with interrupts off, or it may introduce deadlock with\n * smp_call_function() if an IPI is sent by the same process we are\n * waiting to become inactive.\n */\nunsigned long wait_task_inactive(struct task_struct *p, long match_state)\n{\n\tunsigned long flags;\n\tint running, on_rq;\n\tunsigned long ncsw;\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\t/*\n\t\t * We do the initial early heuristics without holding\n\t\t * any task-queue locks at all. We'll only try to get\n\t\t * the runqueue lock when things look like they will\n\t\t * work out!\n\t\t */\n\t\trq = task_rq(p);\n\n\t\t/*\n\t\t * If the task is actively running on another CPU\n\t\t * still, just relax and busy-wait without holding\n\t\t * any locks.\n\t\t *\n\t\t * NOTE! Since we don't hold any locks, it's not\n\t\t * even sure that \"rq\" stays as the right runqueue!\n\t\t * But we don't care, since \"task_running()\" will\n\t\t * return false if the runqueue has changed and p\n\t\t * is actually now running somewhere else!\n\t\t */\n\t\twhile (task_running(rq, p)) {\n\t\t\tif (match_state && unlikely(p->state != match_state))\n\t\t\t\treturn 0;\n\t\t\tcpu_relax();\n\t\t}\n\n\t\t/*\n\t\t * Ok, time to look more closely! We need the rq\n\t\t * lock now, to be *sure*. If we're wrong, we'll\n\t\t * just go back and repeat.\n\t\t */\n\t\trq = task_rq_lock(p, &flags);\n\t\ttrace_sched_wait_task(p);\n\t\trunning = task_running(rq, p);\n\t\ton_rq = p->se.on_rq;\n\t\tncsw = 0;\n\t\tif (!match_state || p->state == match_state)\n\t\t\tncsw = p->nvcsw | LONG_MIN; /* sets MSB */\n\t\ttask_rq_unlock(rq, &flags);\n\n\t\t/*\n\t\t * If it changed from the expected state, bail out now.\n\t\t */\n\t\tif (unlikely(!ncsw))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Was it really running after all now that we\n\t\t * checked with the proper locks actually held?\n\t\t *\n\t\t * Oops. Go back and try again..\n\t\t */\n\t\tif (unlikely(running)) {\n\t\t\tcpu_relax();\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * It's not enough that it's not actively running,\n\t\t * it must be off the runqueue _entirely_, and not\n\t\t * preempted!\n\t\t *\n\t\t * So if it was still runnable (but just not actively\n\t\t * running right now), it's preempted, and we should\n\t\t * yield - it could be a while.\n\t\t */\n\t\tif (unlikely(on_rq)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Ahh, all good. It wasn't running, and it wasn't\n\t\t * runnable, which means that it will never become\n\t\t * running in the future either. We're all done!\n\t\t */\n\t\tbreak;\n\t}\n\n\treturn ncsw;\n}\n\n/***\n * kick_process - kick a running thread to enter/exit the kernel\n * @p: the to-be-kicked thread\n *\n * Cause a process which is running on another CPU to enter\n * kernel-mode, without any delay. (to get signals handled.)\n *\n * NOTE: this function doesnt have to take the runqueue lock,\n * because all it wants to ensure is that the remote task enters\n * the kernel. If the IPI races and the task has been migrated\n * to another CPU then no harm is done and the purpose has been\n * achieved as well.\n */\nvoid kick_process(struct task_struct *p)\n{\n\tint cpu;\n\n\tpreempt_disable();\n\tcpu = task_cpu(p);\n\tif ((cpu != smp_processor_id()) && task_curr(p))\n\t\tsmp_send_reschedule(cpu);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(kick_process);\n#endif /* CONFIG_SMP */\n\n/**\n * task_oncpu_function_call - call a function on the cpu on which a task runs\n * @p:\t\tthe task to evaluate\n * @func:\tthe function to be called\n * @info:\tthe function call argument\n *\n * Calls the function @func when the task is currently running. This might\n * be on the current CPU, which just calls the function directly\n */\nvoid task_oncpu_function_call(struct task_struct *p,\n\t\t\t      void (*func) (void *info), void *info)\n{\n\tint cpu;\n\n\tpreempt_disable();\n\tcpu = task_cpu(p);\n\tif (task_curr(p))\n\t\tsmp_call_function_single(cpu, func, info, 1);\n\tpreempt_enable();\n}\n\n#ifdef CONFIG_SMP\n/*\n * ->cpus_allowed is protected by either TASK_WAKING or rq->lock held.\n */\nstatic int select_fallback_rq(int cpu, struct task_struct *p)\n{\n\tint dest_cpu;\n\tconst struct cpumask *nodemask = cpumask_of_node(cpu_to_node(cpu));\n\n\t/* Look for allowed, online CPU in same node. */\n\tfor_each_cpu_and(dest_cpu, nodemask, cpu_active_mask)\n\t\tif (cpumask_test_cpu(dest_cpu, &p->cpus_allowed))\n\t\t\treturn dest_cpu;\n\n\t/* Any allowed, online CPU? */\n\tdest_cpu = cpumask_any_and(&p->cpus_allowed, cpu_active_mask);\n\tif (dest_cpu < nr_cpu_ids)\n\t\treturn dest_cpu;\n\n\t/* No more Mr. Nice Guy. */\n\tif (unlikely(dest_cpu >= nr_cpu_ids)) {\n\t\tdest_cpu = cpuset_cpus_allowed_fallback(p);\n\t\t/*\n\t\t * Don't tell them about moving exiting tasks or\n\t\t * kernel threads (both mm NULL), since they never\n\t\t * leave kernel.\n\t\t */\n\t\tif (p->mm && printk_ratelimit()) {\n\t\t\tprintk(KERN_INFO \"process %d (%s) no \"\n\t\t\t       \"longer affine to cpu%d\\n\",\n\t\t\t       task_pid_nr(p), p->comm, cpu);\n\t\t}\n\t}\n\n\treturn dest_cpu;\n}\n\n/*\n * The caller (fork, wakeup) owns TASK_WAKING, ->cpus_allowed is stable.\n */\nstatic inline\nint select_task_rq(struct rq *rq, struct task_struct *p, int sd_flags, int wake_flags)\n{\n\tint cpu = p->sched_class->select_task_rq(rq, p, sd_flags, wake_flags);\n\n\t/*\n\t * In order not to call set_task_cpu() on a blocking task we need\n\t * to rely on ttwu() to place the task on a valid ->cpus_allowed\n\t * cpu.\n\t *\n\t * Since this is common to all placement strategies, this lives here.\n\t *\n\t * [ this allows ->select_task() to simply return task_cpu(p) and\n\t *   not worry about this generic constraint ]\n\t */\n\tif (unlikely(!cpumask_test_cpu(cpu, &p->cpus_allowed) ||\n\t\t     !cpu_online(cpu)))\n\t\tcpu = select_fallback_rq(task_cpu(p), p);\n\n\treturn cpu;\n}\n\nstatic void update_avg(u64 *avg, u64 sample)\n{\n\ts64 diff = sample - *avg;\n\t*avg += diff >> 3;\n}\n#endif\n\nstatic inline void ttwu_activate(struct task_struct *p, struct rq *rq,\n\t\t\t\t bool is_sync, bool is_migrate, bool is_local,\n\t\t\t\t unsigned long en_flags)\n{\n\tschedstat_inc(p, se.statistics.nr_wakeups);\n\tif (is_sync)\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_sync);\n\tif (is_migrate)\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_migrate);\n\tif (is_local)\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_local);\n\telse\n\t\tschedstat_inc(p, se.statistics.nr_wakeups_remote);\n\n\tactivate_task(rq, p, en_flags);\n}\n\nstatic inline void ttwu_post_activation(struct task_struct *p, struct rq *rq,\n\t\t\t\t\tint wake_flags, bool success)\n{\n\ttrace_sched_wakeup(p, success);\n\tcheck_preempt_curr(rq, p, wake_flags);\n\n\tp->state = TASK_RUNNING;\n#ifdef CONFIG_SMP\n\tif (p->sched_class->task_woken)\n\t\tp->sched_class->task_woken(rq, p);\n\n\tif (unlikely(rq->idle_stamp)) {\n\t\tu64 delta = rq->clock - rq->idle_stamp;\n\t\tu64 max = 2*sysctl_sched_migration_cost;\n\n\t\tif (delta > max)\n\t\t\trq->avg_idle = max;\n\t\telse\n\t\t\tupdate_avg(&rq->avg_idle, delta);\n\t\trq->idle_stamp = 0;\n\t}\n#endif\n\t/* if a worker is waking up, notify workqueue */\n\tif ((p->flags & PF_WQ_WORKER) && success)\n\t\twq_worker_waking_up(p, cpu_of(rq));\n}\n\n/**\n * try_to_wake_up - wake up a thread\n * @p: the thread to be awakened\n * @state: the mask of task states that can be woken\n * @wake_flags: wake modifier flags (WF_*)\n *\n * Put it on the run-queue if it's not already there. The \"current\"\n * thread is always on the run-queue (except when the actual\n * re-schedule is in progress), and as such you're allowed to do\n * the simpler \"current->state = TASK_RUNNING\" to mark yourself\n * runnable without the overhead of this.\n *\n * Returns %true if @p was woken up, %false if it was already running\n * or @state didn't match @p's state.\n */\nstatic int try_to_wake_up(struct task_struct *p, unsigned int state,\n\t\t\t  int wake_flags)\n{\n\tint cpu, orig_cpu, this_cpu, success = 0;\n\tunsigned long flags;\n\tunsigned long en_flags = ENQUEUE_WAKEUP;\n\tstruct rq *rq;\n\n\tthis_cpu = get_cpu();\n\n\tsmp_wmb();\n\trq = task_rq_lock(p, &flags);\n\tif (!(p->state & state))\n\t\tgoto out;\n\n\tif (p->se.on_rq)\n\t\tgoto out_running;\n\n\tcpu = task_cpu(p);\n\torig_cpu = cpu;\n\n#ifdef CONFIG_SMP\n\tif (unlikely(task_running(rq, p)))\n\t\tgoto out_activate;\n\n\t/*\n\t * In order to handle concurrent wakeups and release the rq->lock\n\t * we put the task in TASK_WAKING state.\n\t *\n\t * First fix up the nr_uninterruptible count:\n\t */\n\tif (task_contributes_to_load(p)) {\n\t\tif (likely(cpu_online(orig_cpu)))\n\t\t\trq->nr_uninterruptible--;\n\t\telse\n\t\t\tthis_rq()->nr_uninterruptible--;\n\t}\n\tp->state = TASK_WAKING;\n\n\tif (p->sched_class->task_waking) {\n\t\tp->sched_class->task_waking(rq, p);\n\t\ten_flags |= ENQUEUE_WAKING;\n\t}\n\n\tcpu = select_task_rq(rq, p, SD_BALANCE_WAKE, wake_flags);\n\tif (cpu != orig_cpu)\n\t\tset_task_cpu(p, cpu);\n\t__task_rq_unlock(rq);\n\n\trq = cpu_rq(cpu);\n\traw_spin_lock(&rq->lock);\n\n\t/*\n\t * We migrated the task without holding either rq->lock, however\n\t * since the task is not on the task list itself, nobody else\n\t * will try and migrate the task, hence the rq should match the\n\t * cpu we just moved it to.\n\t */\n\tWARN_ON(task_cpu(p) != cpu);\n\tWARN_ON(p->state != TASK_WAKING);\n\n#ifdef CONFIG_SCHEDSTATS\n\tschedstat_inc(rq, ttwu_count);\n\tif (cpu == this_cpu)\n\t\tschedstat_inc(rq, ttwu_local);\n\telse {\n\t\tstruct sched_domain *sd;\n\t\tfor_each_domain(this_cpu, sd) {\n\t\t\tif (cpumask_test_cpu(cpu, sched_domain_span(sd))) {\n\t\t\t\tschedstat_inc(sd, ttwu_wake_remote);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n#endif /* CONFIG_SCHEDSTATS */\n\nout_activate:\n#endif /* CONFIG_SMP */\n\tttwu_activate(p, rq, wake_flags & WF_SYNC, orig_cpu != cpu,\n\t\t      cpu == this_cpu, en_flags);\n\tsuccess = 1;\nout_running:\n\tttwu_post_activation(p, rq, wake_flags, success);\nout:\n\ttask_rq_unlock(rq, &flags);\n\tput_cpu();\n\n\treturn success;\n}\n\n/**\n * try_to_wake_up_local - try to wake up a local task with rq lock held\n * @p: the thread to be awakened\n *\n * Put @p on the run-queue if it's not alredy there.  The caller must\n * ensure that this_rq() is locked, @p is bound to this_rq() and not\n * the current task.  this_rq() stays locked over invocation.\n */\nstatic void try_to_wake_up_local(struct task_struct *p)\n{\n\tstruct rq *rq = task_rq(p);\n\tbool success = false;\n\n\tBUG_ON(rq != this_rq());\n\tBUG_ON(p == current);\n\tlockdep_assert_held(&rq->lock);\n\n\tif (!(p->state & TASK_NORMAL))\n\t\treturn;\n\n\tif (!p->se.on_rq) {\n\t\tif (likely(!task_running(rq, p))) {\n\t\t\tschedstat_inc(rq, ttwu_count);\n\t\t\tschedstat_inc(rq, ttwu_local);\n\t\t}\n\t\tttwu_activate(p, rq, false, false, true, ENQUEUE_WAKEUP);\n\t\tsuccess = true;\n\t}\n\tttwu_post_activation(p, rq, 0, success);\n}\n\n/**\n * wake_up_process - Wake up a specific process\n * @p: The process to be woken up.\n *\n * Attempt to wake up the nominated process and move it to the set of runnable\n * processes.  Returns 1 if the process was woken up, 0 if it was already\n * running.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nint wake_up_process(struct task_struct *p)\n{\n\treturn try_to_wake_up(p, TASK_ALL, 0);\n}\nEXPORT_SYMBOL(wake_up_process);\n\nint wake_up_state(struct task_struct *p, unsigned int state)\n{\n\treturn try_to_wake_up(p, state, 0);\n}\n\n/*\n * Perform scheduler related setup for a newly forked process p.\n * p is forked by current.\n *\n * __sched_fork() is basic setup used by init_idle() too:\n */\nstatic void __sched_fork(struct task_struct *p)\n{\n\tp->se.exec_start\t\t= 0;\n\tp->se.sum_exec_runtime\t\t= 0;\n\tp->se.prev_sum_exec_runtime\t= 0;\n\tp->se.nr_migrations\t\t= 0;\n\n#ifdef CONFIG_SCHEDSTATS\n\tmemset(&p->se.statistics, 0, sizeof(p->se.statistics));\n#endif\n\n\tINIT_LIST_HEAD(&p->rt.run_list);\n\tp->se.on_rq = 0;\n\tINIT_LIST_HEAD(&p->se.group_node);\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\tINIT_HLIST_HEAD(&p->preempt_notifiers);\n#endif\n}\n\n/*\n * fork()/clone()-time setup:\n */\nvoid sched_fork(struct task_struct *p, int clone_flags)\n{\n\tint cpu = get_cpu();\n\n\t__sched_fork(p);\n\t/*\n\t * We mark the process as running here. This guarantees that\n\t * nobody will actually run it, and a signal or other external\n\t * event cannot wake it up and insert it on the runqueue either.\n\t */\n\tp->state = TASK_RUNNING;\n\n\t/*\n\t * Revert to default priority/policy on fork if requested.\n\t */\n\tif (unlikely(p->sched_reset_on_fork)) {\n\t\tif (p->policy == SCHED_FIFO || p->policy == SCHED_RR) {\n\t\t\tp->policy = SCHED_NORMAL;\n\t\t\tp->normal_prio = p->static_prio;\n\t\t}\n\n\t\tif (PRIO_TO_NICE(p->static_prio) < 0) {\n\t\t\tp->static_prio = NICE_TO_PRIO(0);\n\t\t\tp->normal_prio = p->static_prio;\n\t\t\tset_load_weight(p);\n\t\t}\n\n\t\t/*\n\t\t * We don't need the reset flag anymore after the fork. It has\n\t\t * fulfilled its duty:\n\t\t */\n\t\tp->sched_reset_on_fork = 0;\n\t}\n\n\t/*\n\t * Make sure we do not leak PI boosting priority to the child.\n\t */\n\tp->prio = current->normal_prio;\n\n\tif (!rt_prio(p->prio))\n\t\tp->sched_class = &fair_sched_class;\n\n\tif (p->sched_class->task_fork)\n\t\tp->sched_class->task_fork(p);\n\n\t/*\n\t * The child is not yet in the pid-hash so no cgroup attach races,\n\t * and the cgroup is pinned to this child due to cgroup_fork()\n\t * is ran before sched_fork().\n\t *\n\t * Silence PROVE_RCU.\n\t */\n\trcu_read_lock();\n\tset_task_cpu(p, cpu);\n\trcu_read_unlock();\n\n#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)\n\tif (likely(sched_info_on()))\n\t\tmemset(&p->sched_info, 0, sizeof(p->sched_info));\n#endif\n#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)\n\tp->oncpu = 0;\n#endif\n#ifdef CONFIG_PREEMPT\n\t/* Want to start with kernel preemption disabled. */\n\ttask_thread_info(p)->preempt_count = 1;\n#endif\n\tplist_node_init(&p->pushable_tasks, MAX_PRIO);\n\n\tput_cpu();\n}\n\n/*\n * wake_up_new_task - wake up a newly created task for the first time.\n *\n * This function will do some initial scheduler statistics housekeeping\n * that must be done for every newly created context, then puts the task\n * on the runqueue and wakes it.\n */\nvoid wake_up_new_task(struct task_struct *p, unsigned long clone_flags)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint cpu __maybe_unused = get_cpu();\n\n#ifdef CONFIG_SMP\n\trq = task_rq_lock(p, &flags);\n\tp->state = TASK_WAKING;\n\n\t/*\n\t * Fork balancing, do it here and not earlier because:\n\t *  - cpus_allowed can change in the fork path\n\t *  - any previously selected cpu might disappear through hotplug\n\t *\n\t * We set TASK_WAKING so that select_task_rq() can drop rq->lock\n\t * without people poking at ->cpus_allowed.\n\t */\n\tcpu = select_task_rq(rq, p, SD_BALANCE_FORK, 0);\n\tset_task_cpu(p, cpu);\n\n\tp->state = TASK_RUNNING;\n\ttask_rq_unlock(rq, &flags);\n#endif\n\n\trq = task_rq_lock(p, &flags);\n\tactivate_task(rq, p, 0);\n\ttrace_sched_wakeup_new(p, 1);\n\tcheck_preempt_curr(rq, p, WF_FORK);\n#ifdef CONFIG_SMP\n\tif (p->sched_class->task_woken)\n\t\tp->sched_class->task_woken(rq, p);\n#endif\n\ttask_rq_unlock(rq, &flags);\n\tput_cpu();\n}\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\n/**\n * preempt_notifier_register - tell me when current is being preempted & rescheduled\n * @notifier: notifier struct to register\n */\nvoid preempt_notifier_register(struct preempt_notifier *notifier)\n{\n\thlist_add_head(&notifier->link, &current->preempt_notifiers);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_register);\n\n/**\n * preempt_notifier_unregister - no longer interested in preemption notifications\n * @notifier: notifier struct to unregister\n *\n * This is safe to call from within a preemption notifier.\n */\nvoid preempt_notifier_unregister(struct preempt_notifier *notifier)\n{\n\thlist_del(&notifier->link);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_unregister);\n\nstatic void fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n\tstruct preempt_notifier *notifier;\n\tstruct hlist_node *node;\n\n\thlist_for_each_entry(notifier, node, &curr->preempt_notifiers, link)\n\t\tnotifier->ops->sched_in(notifier, raw_smp_processor_id());\n}\n\nstatic void\nfire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t struct task_struct *next)\n{\n\tstruct preempt_notifier *notifier;\n\tstruct hlist_node *node;\n\n\thlist_for_each_entry(notifier, node, &curr->preempt_notifiers, link)\n\t\tnotifier->ops->sched_out(notifier, next);\n}\n\n#else /* !CONFIG_PREEMPT_NOTIFIERS */\n\nstatic void fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n}\n\nstatic void\nfire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t struct task_struct *next)\n{\n}\n\n#endif /* CONFIG_PREEMPT_NOTIFIERS */\n\n/**\n * prepare_task_switch - prepare to switch tasks\n * @rq: the runqueue preparing to switch\n * @prev: the current task that is being switched out\n * @next: the task we are going to switch to.\n *\n * This is called with the rq lock held and interrupts off. It must\n * be paired with a subsequent finish_task_switch after the context\n * switch.\n *\n * prepare_task_switch sets up locking and calls architecture specific\n * hooks.\n */\nstatic inline void\nprepare_task_switch(struct rq *rq, struct task_struct *prev,\n\t\t    struct task_struct *next)\n{\n\tfire_sched_out_preempt_notifiers(prev, next);\n\tprepare_lock_switch(rq, next);\n\tprepare_arch_switch(next);\n}\n\n/**\n * finish_task_switch - clean up after a task-switch\n * @rq: runqueue associated with task-switch\n * @prev: the thread we just switched away from.\n *\n * finish_task_switch must be called after the context switch, paired\n * with a prepare_task_switch call before the context switch.\n * finish_task_switch will reconcile locking set up by prepare_task_switch,\n * and do any other architecture-specific cleanup actions.\n *\n * Note that we may have delayed dropping an mm in context_switch(). If\n * so, we finish that here outside of the runqueue lock. (Doing it\n * with the lock held can cause deadlocks; see schedule() for\n * details.)\n */\nstatic void finish_task_switch(struct rq *rq, struct task_struct *prev)\n\t__releases(rq->lock)\n{\n\tstruct mm_struct *mm = rq->prev_mm;\n\tlong prev_state;\n\n\trq->prev_mm = NULL;\n\n\t/*\n\t * A task struct has one reference for the use as \"current\".\n\t * If a task dies, then it sets TASK_DEAD in tsk->state and calls\n\t * schedule one last time. The schedule call will never return, and\n\t * the scheduled task must drop that reference.\n\t * The test for TASK_DEAD must occur while the runqueue locks are\n\t * still held, otherwise prev could be scheduled on another cpu, die\n\t * there before we look at prev->state, and then the reference would\n\t * be dropped twice.\n\t *\t\tManfred Spraul <manfred@colorfullife.com>\n\t */\n\tprev_state = prev->state;\n\tfinish_arch_switch(prev);\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_disable();\n#endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */\n\tperf_event_task_sched_in(current);\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tlocal_irq_enable();\n#endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */\n\tfinish_lock_switch(rq, prev);\n\n\tfire_sched_in_preempt_notifiers(current);\n\tif (mm)\n\t\tmmdrop(mm);\n\tif (unlikely(prev_state == TASK_DEAD)) {\n\t\t/*\n\t\t * Remove function-return probe instances associated with this\n\t\t * task and put them back on the free list.\n\t\t */\n\t\tkprobe_flush_task(prev);\n\t\tput_task_struct(prev);\n\t}\n}\n\n#ifdef CONFIG_SMP\n\n/* assumes rq->lock is held */\nstatic inline void pre_schedule(struct rq *rq, struct task_struct *prev)\n{\n\tif (prev->sched_class->pre_schedule)\n\t\tprev->sched_class->pre_schedule(rq, prev);\n}\n\n/* rq->lock is NOT held, but preemption is disabled */\nstatic inline void post_schedule(struct rq *rq)\n{\n\tif (rq->post_schedule) {\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->curr->sched_class->post_schedule)\n\t\t\trq->curr->sched_class->post_schedule(rq);\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\t\trq->post_schedule = 0;\n\t}\n}\n\n#else\n\nstatic inline void pre_schedule(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void post_schedule(struct rq *rq)\n{\n}\n\n#endif\n\n/**\n * schedule_tail - first thing a freshly forked thread must call.\n * @prev: the thread we just switched away from.\n */\nasmlinkage void schedule_tail(struct task_struct *prev)\n\t__releases(rq->lock)\n{\n\tstruct rq *rq = this_rq();\n\n\tfinish_task_switch(rq, prev);\n\n\t/*\n\t * FIXME: do we need to worry about rq being invalidated by the\n\t * task_switch?\n\t */\n\tpost_schedule(rq);\n\n#ifdef __ARCH_WANT_UNLOCKED_CTXSW\n\t/* In this case, finish_task_switch does not reenable preemption */\n\tpreempt_enable();\n#endif\n\tif (current->set_child_tid)\n\t\tput_user(task_pid_vnr(current), current->set_child_tid);\n}\n\n/*\n * context_switch - switch to the new MM and the new\n * thread's register state.\n */\nstatic inline void\ncontext_switch(struct rq *rq, struct task_struct *prev,\n\t       struct task_struct *next)\n{\n\tstruct mm_struct *mm, *oldmm;\n\n\tprepare_task_switch(rq, prev, next);\n\ttrace_sched_switch(prev, next);\n\tmm = next->mm;\n\toldmm = prev->active_mm;\n\t/*\n\t * For paravirt, this is coupled with an exit in switch_to to\n\t * combine the page table reload and the switch backend into\n\t * one hypercall.\n\t */\n\tarch_start_context_switch(prev);\n\n\tif (!mm) {\n\t\tnext->active_mm = oldmm;\n\t\tatomic_inc(&oldmm->mm_count);\n\t\tenter_lazy_tlb(oldmm, next);\n\t} else\n\t\tswitch_mm(oldmm, mm, next);\n\n\tif (!prev->mm) {\n\t\tprev->active_mm = NULL;\n\t\trq->prev_mm = oldmm;\n\t}\n\t/*\n\t * Since the runqueue lock will be released by the next\n\t * task (which is an invalid locking op but in the case\n\t * of the scheduler it's an obvious special-case), so we\n\t * do an early lockdep release here:\n\t */\n#ifndef __ARCH_WANT_UNLOCKED_CTXSW\n\tspin_release(&rq->lock.dep_map, 1, _THIS_IP_);\n#endif\n\n\t/* Here we just switch the register state and the stack. */\n\tswitch_to(prev, next, prev);\n\n\tbarrier();\n\t/*\n\t * this_rq must be evaluated again because prev may have moved\n\t * CPUs since it called schedule(), thus the 'rq' on its stack\n\t * frame will be invalid.\n\t */\n\tfinish_task_switch(this_rq(), prev);\n}\n\n/*\n * nr_running, nr_uninterruptible and nr_context_switches:\n *\n * externally visible scheduler statistics: current number of runnable\n * threads, current number of uninterruptible-sleeping threads, total\n * number of context switches performed since bootup.\n */\nunsigned long nr_running(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_online_cpu(i)\n\t\tsum += cpu_rq(i)->nr_running;\n\n\treturn sum;\n}\n\nunsigned long nr_uninterruptible(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += cpu_rq(i)->nr_uninterruptible;\n\n\t/*\n\t * Since we read the counters lockless, it might be slightly\n\t * inaccurate. Do not allow it to go below zero though:\n\t */\n\tif (unlikely((long)sum < 0))\n\t\tsum = 0;\n\n\treturn sum;\n}\n\nunsigned long long nr_context_switches(void)\n{\n\tint i;\n\tunsigned long long sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += cpu_rq(i)->nr_switches;\n\n\treturn sum;\n}\n\nunsigned long nr_iowait(void)\n{\n\tunsigned long i, sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += atomic_read(&cpu_rq(i)->nr_iowait);\n\n\treturn sum;\n}\n\nunsigned long nr_iowait_cpu(int cpu)\n{\n\tstruct rq *this = cpu_rq(cpu);\n\treturn atomic_read(&this->nr_iowait);\n}\n\nunsigned long this_cpu_load(void)\n{\n\tstruct rq *this = this_rq();\n\treturn this->cpu_load[0];\n}\n\n\n/* Variables and functions for calc_load */\nstatic atomic_long_t calc_load_tasks;\nstatic unsigned long calc_load_update;\nunsigned long avenrun[3];\nEXPORT_SYMBOL(avenrun);\n\nstatic long calc_load_fold_active(struct rq *this_rq)\n{\n\tlong nr_active, delta = 0;\n\n\tnr_active = this_rq->nr_running;\n\tnr_active += (long) this_rq->nr_uninterruptible;\n\n\tif (nr_active != this_rq->calc_load_active) {\n\t\tdelta = nr_active - this_rq->calc_load_active;\n\t\tthis_rq->calc_load_active = nr_active;\n\t}\n\n\treturn delta;\n}\n\nstatic unsigned long\ncalc_load(unsigned long load, unsigned long exp, unsigned long active)\n{\n\tload *= exp;\n\tload += active * (FIXED_1 - exp);\n\tload += 1UL << (FSHIFT - 1);\n\treturn load >> FSHIFT;\n}\n\n#ifdef CONFIG_NO_HZ\n/*\n * For NO_HZ we delay the active fold to the next LOAD_FREQ update.\n *\n * When making the ILB scale, we should try to pull this in as well.\n */\nstatic atomic_long_t calc_load_tasks_idle;\n\nstatic void calc_load_account_idle(struct rq *this_rq)\n{\n\tlong delta;\n\n\tdelta = calc_load_fold_active(this_rq);\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks_idle);\n}\n\nstatic long calc_load_fold_idle(void)\n{\n\tlong delta = 0;\n\n\t/*\n\t * Its got a race, we don't care...\n\t */\n\tif (atomic_long_read(&calc_load_tasks_idle))\n\t\tdelta = atomic_long_xchg(&calc_load_tasks_idle, 0);\n\n\treturn delta;\n}\n\n/**\n * fixed_power_int - compute: x^n, in O(log n) time\n *\n * @x:         base of the power\n * @frac_bits: fractional bits of @x\n * @n:         power to raise @x to.\n *\n * By exploiting the relation between the definition of the natural power\n * function: x^n := x*x*...*x (x multiplied by itself for n times), and\n * the binary encoding of numbers used by computers: n := \\Sum n_i * 2^i,\n * (where: n_i \\elem {0, 1}, the binary vector representing n),\n * we find: x^n := x^(\\Sum n_i * 2^i) := \\Prod x^(n_i * 2^i), which is\n * of course trivially computable in O(log_2 n), the length of our binary\n * vector.\n */\nstatic unsigned long\nfixed_power_int(unsigned long x, unsigned int frac_bits, unsigned int n)\n{\n\tunsigned long result = 1UL << frac_bits;\n\n\tif (n) for (;;) {\n\t\tif (n & 1) {\n\t\t\tresult *= x;\n\t\t\tresult += 1UL << (frac_bits - 1);\n\t\t\tresult >>= frac_bits;\n\t\t}\n\t\tn >>= 1;\n\t\tif (!n)\n\t\t\tbreak;\n\t\tx *= x;\n\t\tx += 1UL << (frac_bits - 1);\n\t\tx >>= frac_bits;\n\t}\n\n\treturn result;\n}\n\n/*\n * a1 = a0 * e + a * (1 - e)\n *\n * a2 = a1 * e + a * (1 - e)\n *    = (a0 * e + a * (1 - e)) * e + a * (1 - e)\n *    = a0 * e^2 + a * (1 - e) * (1 + e)\n *\n * a3 = a2 * e + a * (1 - e)\n *    = (a0 * e^2 + a * (1 - e) * (1 + e)) * e + a * (1 - e)\n *    = a0 * e^3 + a * (1 - e) * (1 + e + e^2)\n *\n *  ...\n *\n * an = a0 * e^n + a * (1 - e) * (1 + e + ... + e^n-1) [1]\n *    = a0 * e^n + a * (1 - e) * (1 - e^n)/(1 - e)\n *    = a0 * e^n + a * (1 - e^n)\n *\n * [1] application of the geometric series:\n *\n *              n         1 - x^(n+1)\n *     S_n := \\Sum x^i = -------------\n *             i=0          1 - x\n */\nstatic unsigned long\ncalc_load_n(unsigned long load, unsigned long exp,\n\t    unsigned long active, unsigned int n)\n{\n\n\treturn calc_load(load, fixed_power_int(exp, FSHIFT, n), active);\n}\n\n/*\n * NO_HZ can leave us missing all per-cpu ticks calling\n * calc_load_account_active(), but since an idle CPU folds its delta into\n * calc_load_tasks_idle per calc_load_account_idle(), all we need to do is fold\n * in the pending idle delta if our idle period crossed a load cycle boundary.\n *\n * Once we've updated the global active value, we need to apply the exponential\n * weights adjusted to the number of cycles missed.\n */\nstatic void calc_global_nohz(unsigned long ticks)\n{\n\tlong delta, active, n;\n\n\tif (time_before(jiffies, calc_load_update))\n\t\treturn;\n\n\t/*\n\t * If we crossed a calc_load_update boundary, make sure to fold\n\t * any pending idle changes, the respective CPUs might have\n\t * missed the tick driven calc_load_account_active() update\n\t * due to NO_HZ.\n\t */\n\tdelta = calc_load_fold_idle();\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks);\n\n\t/*\n\t * If we were idle for multiple load cycles, apply them.\n\t */\n\tif (ticks >= LOAD_FREQ) {\n\t\tn = ticks / LOAD_FREQ;\n\n\t\tactive = atomic_long_read(&calc_load_tasks);\n\t\tactive = active > 0 ? active * FIXED_1 : 0;\n\n\t\tavenrun[0] = calc_load_n(avenrun[0], EXP_1, active, n);\n\t\tavenrun[1] = calc_load_n(avenrun[1], EXP_5, active, n);\n\t\tavenrun[2] = calc_load_n(avenrun[2], EXP_15, active, n);\n\n\t\tcalc_load_update += n * LOAD_FREQ;\n\t}\n\n\t/*\n\t * Its possible the remainder of the above division also crosses\n\t * a LOAD_FREQ period, the regular check in calc_global_load()\n\t * which comes after this will take care of that.\n\t *\n\t * Consider us being 11 ticks before a cycle completion, and us\n\t * sleeping for 4*LOAD_FREQ + 22 ticks, then the above code will\n\t * age us 4 cycles, and the test in calc_global_load() will\n\t * pick up the final one.\n\t */\n}\n#else\nstatic void calc_load_account_idle(struct rq *this_rq)\n{\n}\n\nstatic inline long calc_load_fold_idle(void)\n{\n\treturn 0;\n}\n\nstatic void calc_global_nohz(unsigned long ticks)\n{\n}\n#endif\n\n/**\n * get_avenrun - get the load average array\n * @loads:\tpointer to dest load array\n * @offset:\toffset to add\n * @shift:\tshift count to shift the result left\n *\n * These values are estimates at best, so no need for locking.\n */\nvoid get_avenrun(unsigned long *loads, unsigned long offset, int shift)\n{\n\tloads[0] = (avenrun[0] + offset) << shift;\n\tloads[1] = (avenrun[1] + offset) << shift;\n\tloads[2] = (avenrun[2] + offset) << shift;\n}\n\n/*\n * calc_load - update the avenrun load estimates 10 ticks after the\n * CPUs have updated calc_load_tasks.\n */\nvoid calc_global_load(unsigned long ticks)\n{\n\tlong active;\n\n\tcalc_global_nohz(ticks);\n\n\tif (time_before(jiffies, calc_load_update + 10))\n\t\treturn;\n\n\tactive = atomic_long_read(&calc_load_tasks);\n\tactive = active > 0 ? active * FIXED_1 : 0;\n\n\tavenrun[0] = calc_load(avenrun[0], EXP_1, active);\n\tavenrun[1] = calc_load(avenrun[1], EXP_5, active);\n\tavenrun[2] = calc_load(avenrun[2], EXP_15, active);\n\n\tcalc_load_update += LOAD_FREQ;\n}\n\n/*\n * Called from update_cpu_load() to periodically update this CPU's\n * active count.\n */\nstatic void calc_load_account_active(struct rq *this_rq)\n{\n\tlong delta;\n\n\tif (time_before(jiffies, this_rq->calc_load_update))\n\t\treturn;\n\n\tdelta  = calc_load_fold_active(this_rq);\n\tdelta += calc_load_fold_idle();\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks);\n\n\tthis_rq->calc_load_update += LOAD_FREQ;\n}\n\n/*\n * The exact cpuload at various idx values, calculated at every tick would be\n * load = (2^idx - 1) / 2^idx * load + 1 / 2^idx * cur_load\n *\n * If a cpu misses updates for n-1 ticks (as it was idle) and update gets called\n * on nth tick when cpu may be busy, then we have:\n * load = ((2^idx - 1) / 2^idx)^(n-1) * load\n * load = (2^idx - 1) / 2^idx) * load + 1 / 2^idx * cur_load\n *\n * decay_load_missed() below does efficient calculation of\n * load = ((2^idx - 1) / 2^idx)^(n-1) * load\n * avoiding 0..n-1 loop doing load = ((2^idx - 1) / 2^idx) * load\n *\n * The calculation is approximated on a 128 point scale.\n * degrade_zero_ticks is the number of ticks after which load at any\n * particular idx is approximated to be zero.\n * degrade_factor is a precomputed table, a row for each load idx.\n * Each column corresponds to degradation factor for a power of two ticks,\n * based on 128 point scale.\n * Example:\n * row 2, col 3 (=12) says that the degradation at load idx 2 after\n * 8 ticks is 12/128 (which is an approximation of exact factor 3^8/4^8).\n *\n * With this power of 2 load factors, we can degrade the load n times\n * by looking at 1 bits in n and doing as many mult/shift instead of\n * n mult/shifts needed by the exact degradation.\n */\n#define DEGRADE_SHIFT\t\t7\nstatic const unsigned char\n\t\tdegrade_zero_ticks[CPU_LOAD_IDX_MAX] = {0, 8, 32, 64, 128};\nstatic const unsigned char\n\t\tdegrade_factor[CPU_LOAD_IDX_MAX][DEGRADE_SHIFT + 1] = {\n\t\t\t\t\t{0, 0, 0, 0, 0, 0, 0, 0},\n\t\t\t\t\t{64, 32, 8, 0, 0, 0, 0, 0},\n\t\t\t\t\t{96, 72, 40, 12, 1, 0, 0},\n\t\t\t\t\t{112, 98, 75, 43, 15, 1, 0},\n\t\t\t\t\t{120, 112, 98, 76, 45, 16, 2} };\n\n/*\n * Update cpu_load for any missed ticks, due to tickless idle. The backlog\n * would be when CPU is idle and so we just decay the old load without\n * adding any new load.\n */\nstatic unsigned long\ndecay_load_missed(unsigned long load, unsigned long missed_updates, int idx)\n{\n\tint j = 0;\n\n\tif (!missed_updates)\n\t\treturn load;\n\n\tif (missed_updates >= degrade_zero_ticks[idx])\n\t\treturn 0;\n\n\tif (idx == 1)\n\t\treturn load >> missed_updates;\n\n\twhile (missed_updates) {\n\t\tif (missed_updates % 2)\n\t\t\tload = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;\n\n\t\tmissed_updates >>= 1;\n\t\tj++;\n\t}\n\treturn load;\n}\n\n/*\n * Update rq->cpu_load[] statistics. This function is usually called every\n * scheduler tick (TICK_NSEC). With tickless idle this will not be called\n * every tick. We fix it up based on jiffies.\n */\nstatic void update_cpu_load(struct rq *this_rq)\n{\n\tunsigned long this_load = this_rq->load.weight;\n\tunsigned long curr_jiffies = jiffies;\n\tunsigned long pending_updates;\n\tint i, scale;\n\n\tthis_rq->nr_load_updates++;\n\n\t/* Avoid repeated calls on same jiffy, when moving in and out of idle */\n\tif (curr_jiffies == this_rq->last_load_update_tick)\n\t\treturn;\n\n\tpending_updates = curr_jiffies - this_rq->last_load_update_tick;\n\tthis_rq->last_load_update_tick = curr_jiffies;\n\n\t/* Update our load: */\n\tthis_rq->cpu_load[0] = this_load; /* Fasttrack for idx 0 */\n\tfor (i = 1, scale = 2; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {\n\t\tunsigned long old_load, new_load;\n\n\t\t/* scale is effectively 1 << i now, and >> i divides by scale */\n\n\t\told_load = this_rq->cpu_load[i];\n\t\told_load = decay_load_missed(old_load, pending_updates - 1, i);\n\t\tnew_load = this_load;\n\t\t/*\n\t\t * Round up the averaging division if load is increasing. This\n\t\t * prevents us from getting stuck on 9 if the load is 10, for\n\t\t * example.\n\t\t */\n\t\tif (new_load > old_load)\n\t\t\tnew_load += scale - 1;\n\n\t\tthis_rq->cpu_load[i] = (old_load * (scale - 1) + new_load) >> i;\n\t}\n\n\tsched_avg_update(this_rq);\n}\n\nstatic void update_cpu_load_active(struct rq *this_rq)\n{\n\tupdate_cpu_load(this_rq);\n\n\tcalc_load_account_active(this_rq);\n}\n\n#ifdef CONFIG_SMP\n\n/*\n * sched_exec - execve() is a valuable balancing opportunity, because at\n * this point the task has the smallest effective memory and cache footprint.\n */\nvoid sched_exec(void)\n{\n\tstruct task_struct *p = current;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint dest_cpu;\n\n\trq = task_rq_lock(p, &flags);\n\tdest_cpu = p->sched_class->select_task_rq(rq, p, SD_BALANCE_EXEC, 0);\n\tif (dest_cpu == smp_processor_id())\n\t\tgoto unlock;\n\n\t/*\n\t * select_task_rq() can race against ->cpus_allowed\n\t */\n\tif (cpumask_test_cpu(dest_cpu, &p->cpus_allowed) &&\n\t    likely(cpu_active(dest_cpu)) && migrate_task(p, dest_cpu)) {\n\t\tstruct migration_arg arg = { p, dest_cpu };\n\n\t\ttask_rq_unlock(rq, &flags);\n\t\tstop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);\n\t\treturn;\n\t}\nunlock:\n\ttask_rq_unlock(rq, &flags);\n}\n\n#endif\n\nDEFINE_PER_CPU(struct kernel_stat, kstat);\n\nEXPORT_PER_CPU_SYMBOL(kstat);\n\n/*\n * Return any ns on the sched_clock that have not yet been accounted in\n * @p in case that task is currently running.\n *\n * Called with task_rq_lock() held on @rq.\n */\nstatic u64 do_task_delta_exec(struct task_struct *p, struct rq *rq)\n{\n\tu64 ns = 0;\n\n\tif (task_current(rq, p)) {\n\t\tupdate_rq_clock(rq);\n\t\tns = rq->clock_task - p->se.exec_start;\n\t\tif ((s64)ns < 0)\n\t\t\tns = 0;\n\t}\n\n\treturn ns;\n}\n\nunsigned long long task_delta_exec(struct task_struct *p)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns = 0;\n\n\trq = task_rq_lock(p, &flags);\n\tns = do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, &flags);\n\n\treturn ns;\n}\n\n/*\n * Return accounted runtime for the task.\n * In case the task is currently running, return the runtime plus current's\n * pending runtime that have not been accounted yet.\n */\nunsigned long long task_sched_runtime(struct task_struct *p)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns = 0;\n\n\trq = task_rq_lock(p, &flags);\n\tns = p->se.sum_exec_runtime + do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, &flags);\n\n\treturn ns;\n}\n\n/*\n * Return sum_exec_runtime for the thread group.\n * In case the task is currently running, return the sum plus current's\n * pending runtime that have not been accounted yet.\n *\n * Note that the thread group might have other running tasks as well,\n * so the return value not includes other pending runtime that other\n * running tasks might have.\n */\nunsigned long long thread_group_sched_runtime(struct task_struct *p)\n{\n\tstruct task_cputime totals;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tu64 ns;\n\n\trq = task_rq_lock(p, &flags);\n\tthread_group_cputime(p, &totals);\n\tns = totals.sum_exec_runtime + do_task_delta_exec(p, rq);\n\ttask_rq_unlock(rq, &flags);\n\n\treturn ns;\n}\n\n/*\n * Account user cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @cputime: the cpu time spent in user space since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nvoid account_user_time(struct task_struct *p, cputime_t cputime,\n\t\t       cputime_t cputime_scaled)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t tmp;\n\n\t/* Add user time to process. */\n\tp->utime = cputime_add(p->utime, cputime);\n\tp->utimescaled = cputime_add(p->utimescaled, cputime_scaled);\n\taccount_group_user_time(p, cputime);\n\n\t/* Add user time to cpustat. */\n\ttmp = cputime_to_cputime64(cputime);\n\tif (TASK_NICE(p) > 0)\n\t\tcpustat->nice = cputime64_add(cpustat->nice, tmp);\n\telse\n\t\tcpustat->user = cputime64_add(cpustat->user, tmp);\n\n\tcpuacct_update_stats(p, CPUACCT_STAT_USER, cputime);\n\t/* Account for user time used */\n\tacct_update_integrals(p);\n}\n\n/*\n * Account guest cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @cputime: the cpu time spent in virtual machine since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nstatic void account_guest_time(struct task_struct *p, cputime_t cputime,\n\t\t\t       cputime_t cputime_scaled)\n{\n\tcputime64_t tmp;\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\n\ttmp = cputime_to_cputime64(cputime);\n\n\t/* Add guest time to process. */\n\tp->utime = cputime_add(p->utime, cputime);\n\tp->utimescaled = cputime_add(p->utimescaled, cputime_scaled);\n\taccount_group_user_time(p, cputime);\n\tp->gtime = cputime_add(p->gtime, cputime);\n\n\t/* Add guest time to cpustat. */\n\tif (TASK_NICE(p) > 0) {\n\t\tcpustat->nice = cputime64_add(cpustat->nice, tmp);\n\t\tcpustat->guest_nice = cputime64_add(cpustat->guest_nice, tmp);\n\t} else {\n\t\tcpustat->user = cputime64_add(cpustat->user, tmp);\n\t\tcpustat->guest = cputime64_add(cpustat->guest, tmp);\n\t}\n}\n\n/*\n * Account system cpu time to a process.\n * @p: the process that the cpu time gets accounted to\n * @hardirq_offset: the offset to subtract from hardirq_count()\n * @cputime: the cpu time spent in kernel space since the last update\n * @cputime_scaled: cputime scaled by cpu frequency\n */\nvoid account_system_time(struct task_struct *p, int hardirq_offset,\n\t\t\t cputime_t cputime, cputime_t cputime_scaled)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t tmp;\n\n\tif ((p->flags & PF_VCPU) && (irq_count() - hardirq_offset == 0)) {\n\t\taccount_guest_time(p, cputime, cputime_scaled);\n\t\treturn;\n\t}\n\n\t/* Add system time to process. */\n\tp->stime = cputime_add(p->stime, cputime);\n\tp->stimescaled = cputime_add(p->stimescaled, cputime_scaled);\n\taccount_group_system_time(p, cputime);\n\n\t/* Add system time to cpustat. */\n\ttmp = cputime_to_cputime64(cputime);\n\tif (hardirq_count() - hardirq_offset)\n\t\tcpustat->irq = cputime64_add(cpustat->irq, tmp);\n\telse if (in_serving_softirq())\n\t\tcpustat->softirq = cputime64_add(cpustat->softirq, tmp);\n\telse\n\t\tcpustat->system = cputime64_add(cpustat->system, tmp);\n\n\tcpuacct_update_stats(p, CPUACCT_STAT_SYSTEM, cputime);\n\n\t/* Account for system time used */\n\tacct_update_integrals(p);\n}\n\n/*\n * Account for involuntary wait time.\n * @steal: the cpu time spent in involuntary wait\n */\nvoid account_steal_time(cputime_t cputime)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t cputime64 = cputime_to_cputime64(cputime);\n\n\tcpustat->steal = cputime64_add(cpustat->steal, cputime64);\n}\n\n/*\n * Account for idle time.\n * @cputime: the cpu time spent in idle wait\n */\nvoid account_idle_time(cputime_t cputime)\n{\n\tstruct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;\n\tcputime64_t cputime64 = cputime_to_cputime64(cputime);\n\tstruct rq *rq = this_rq();\n\n\tif (atomic_read(&rq->nr_iowait) > 0)\n\t\tcpustat->iowait = cputime64_add(cpustat->iowait, cputime64);\n\telse\n\t\tcpustat->idle = cputime64_add(cpustat->idle, cputime64);\n}\n\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\n/*\n * Account a single tick of cpu time.\n * @p: the process that the cpu time gets accounted to\n * @user_tick: indicates if the tick is a user or a system tick\n */\nvoid account_process_tick(struct task_struct *p, int user_tick)\n{\n\tcputime_t one_jiffy_scaled = cputime_to_scaled(cputime_one_jiffy);\n\tstruct rq *rq = this_rq();\n\n\tif (user_tick)\n\t\taccount_user_time(p, cputime_one_jiffy, one_jiffy_scaled);\n\telse if ((p != rq->idle) || (irq_count() != HARDIRQ_OFFSET))\n\t\taccount_system_time(p, HARDIRQ_OFFSET, cputime_one_jiffy,\n\t\t\t\t    one_jiffy_scaled);\n\telse\n\t\taccount_idle_time(cputime_one_jiffy);\n}\n\n/*\n * Account multiple ticks of steal time.\n * @p: the process from which the cpu time has been stolen\n * @ticks: number of stolen ticks\n */\nvoid account_steal_ticks(unsigned long ticks)\n{\n\taccount_steal_time(jiffies_to_cputime(ticks));\n}\n\n/*\n * Account multiple ticks of idle time.\n * @ticks: number of stolen ticks\n */\nvoid account_idle_ticks(unsigned long ticks)\n{\n\taccount_idle_time(jiffies_to_cputime(ticks));\n}\n\n#endif\n\n/*\n * Use precise platform statistics if available:\n */\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING\nvoid task_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\t*ut = p->utime;\n\t*st = p->stime;\n}\n\nvoid thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tstruct task_cputime cputime;\n\n\tthread_group_cputime(p, &cputime);\n\n\t*ut = cputime.utime;\n\t*st = cputime.stime;\n}\n#else\n\n#ifndef nsecs_to_cputime\n# define nsecs_to_cputime(__nsecs)\tnsecs_to_jiffies(__nsecs)\n#endif\n\nvoid task_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tcputime_t rtime, utime = p->utime, total = cputime_add(utime, p->stime);\n\n\t/*\n\t * Use CFS's precise accounting:\n\t */\n\trtime = nsecs_to_cputime(p->se.sum_exec_runtime);\n\n\tif (total) {\n\t\tu64 temp = rtime;\n\n\t\ttemp *= utime;\n\t\tdo_div(temp, total);\n\t\tutime = (cputime_t)temp;\n\t} else\n\t\tutime = rtime;\n\n\t/*\n\t * Compare with previous values, to keep monotonicity:\n\t */\n\tp->prev_utime = max(p->prev_utime, utime);\n\tp->prev_stime = max(p->prev_stime, cputime_sub(rtime, p->prev_utime));\n\n\t*ut = p->prev_utime;\n\t*st = p->prev_stime;\n}\n\n/*\n * Must be called with siglock held.\n */\nvoid thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *st)\n{\n\tstruct signal_struct *sig = p->signal;\n\tstruct task_cputime cputime;\n\tcputime_t rtime, utime, total;\n\n\tthread_group_cputime(p, &cputime);\n\n\ttotal = cputime_add(cputime.utime, cputime.stime);\n\trtime = nsecs_to_cputime(cputime.sum_exec_runtime);\n\n\tif (total) {\n\t\tu64 temp = rtime;\n\n\t\ttemp *= cputime.utime;\n\t\tdo_div(temp, total);\n\t\tutime = (cputime_t)temp;\n\t} else\n\t\tutime = rtime;\n\n\tsig->prev_utime = max(sig->prev_utime, utime);\n\tsig->prev_stime = max(sig->prev_stime,\n\t\t\t      cputime_sub(rtime, sig->prev_utime));\n\n\t*ut = sig->prev_utime;\n\t*st = sig->prev_stime;\n}\n#endif\n\n/*\n * This function gets called by the timer code, with HZ frequency.\n * We call it with interrupts disabled.\n *\n * It also gets called by the fork code, when changing the parent's\n * timeslices.\n */\nvoid scheduler_tick(void)\n{\n\tint cpu = smp_processor_id();\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct task_struct *curr = rq->curr;\n\n\tsched_clock_tick();\n\n\traw_spin_lock(&rq->lock);\n\tupdate_rq_clock(rq);\n\tupdate_cpu_load_active(rq);\n\tcurr->sched_class->task_tick(rq, curr, 0);\n\traw_spin_unlock(&rq->lock);\n\n\tperf_event_task_tick();\n\n#ifdef CONFIG_SMP\n\trq->idle_at_tick = idle_cpu(cpu);\n\ttrigger_load_balance(rq, cpu);\n#endif\n}\n\nnotrace unsigned long get_parent_ip(unsigned long addr)\n{\n\tif (in_lock_functions(addr)) {\n\t\taddr = CALLER_ADDR2;\n\t\tif (in_lock_functions(addr))\n\t\t\taddr = CALLER_ADDR3;\n\t}\n\treturn addr;\n}\n\n#if defined(CONFIG_PREEMPT) && (defined(CONFIG_DEBUG_PREEMPT) || \\\n\t\t\t\tdefined(CONFIG_PREEMPT_TRACER))\n\nvoid __kprobes add_preempt_count(int val)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Underflow?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))\n\t\treturn;\n#endif\n\tpreempt_count() += val;\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Spinlock count overflowing soon?\n\t */\n\tDEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >=\n\t\t\t\tPREEMPT_MASK - 10);\n#endif\n\tif (preempt_count() == val)\n\t\ttrace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));\n}\nEXPORT_SYMBOL(add_preempt_count);\n\nvoid __kprobes sub_preempt_count(int val)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\t/*\n\t * Underflow?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON(val > preempt_count()))\n\t\treturn;\n\t/*\n\t * Is the spinlock portion underflowing?\n\t */\n\tif (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&\n\t\t\t!(preempt_count() & PREEMPT_MASK)))\n\t\treturn;\n#endif\n\n\tif (preempt_count() == val)\n\t\ttrace_preempt_on(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));\n\tpreempt_count() -= val;\n}\nEXPORT_SYMBOL(sub_preempt_count);\n\n#endif\n\n/*\n * Print scheduling while atomic bug:\n */\nstatic noinline void __schedule_bug(struct task_struct *prev)\n{\n\tstruct pt_regs *regs = get_irq_regs();\n\n\tprintk(KERN_ERR \"BUG: scheduling while atomic: %s/%d/0x%08x\\n\",\n\t\tprev->comm, prev->pid, preempt_count());\n\n\tdebug_show_held_locks(prev);\n\tprint_modules();\n\tif (irqs_disabled())\n\t\tprint_irqtrace_events(prev);\n\n\tif (regs)\n\t\tshow_regs(regs);\n\telse\n\t\tdump_stack();\n}\n\n/*\n * Various schedule()-time debugging checks and statistics:\n */\nstatic inline void schedule_debug(struct task_struct *prev)\n{\n\t/*\n\t * Test if we are atomic. Since do_exit() needs to call into\n\t * schedule() atomically, we ignore that path for now.\n\t * Otherwise, whine if we are scheduling when we should not be.\n\t */\n\tif (unlikely(in_atomic_preempt_off() && !prev->exit_state))\n\t\t__schedule_bug(prev);\n\n\tprofile_hit(SCHED_PROFILING, __builtin_return_address(0));\n\n\tschedstat_inc(this_rq(), sched_count);\n#ifdef CONFIG_SCHEDSTATS\n\tif (unlikely(prev->lock_depth >= 0)) {\n\t\tschedstat_inc(this_rq(), bkl_count);\n\t\tschedstat_inc(prev, sched_info.bkl_count);\n\t}\n#endif\n}\n\nstatic void put_prev_task(struct rq *rq, struct task_struct *prev)\n{\n\tif (prev->se.on_rq)\n\t\tupdate_rq_clock(rq);\n\tprev->sched_class->put_prev_task(rq, prev);\n}\n\n/*\n * Pick up the highest-prio task:\n */\nstatic inline struct task_struct *\npick_next_task(struct rq *rq)\n{\n\tconst struct sched_class *class;\n\tstruct task_struct *p;\n\n\t/*\n\t * Optimization: we know that if all tasks are in\n\t * the fair class we can call that function directly:\n\t */\n\tif (likely(rq->nr_running == rq->cfs.nr_running)) {\n\t\tp = fair_sched_class.pick_next_task(rq);\n\t\tif (likely(p))\n\t\t\treturn p;\n\t}\n\n\tfor_each_class(class) {\n\t\tp = class->pick_next_task(rq);\n\t\tif (p)\n\t\t\treturn p;\n\t}\n\n\tBUG(); /* the idle class will always have a runnable task */\n}\n\n/*\n * schedule() is the main scheduler function.\n */\nasmlinkage void __sched schedule(void)\n{\n\tstruct task_struct *prev, *next;\n\tunsigned long *switch_count;\n\tstruct rq *rq;\n\tint cpu;\n\nneed_resched:\n\tpreempt_disable();\n\tcpu = smp_processor_id();\n\trq = cpu_rq(cpu);\n\trcu_note_context_switch(cpu);\n\tprev = rq->curr;\n\n\trelease_kernel_lock(prev);\nneed_resched_nonpreemptible:\n\n\tschedule_debug(prev);\n\n\tif (sched_feat(HRTICK))\n\t\thrtick_clear(rq);\n\n\traw_spin_lock_irq(&rq->lock);\n\n\tswitch_count = &prev->nivcsw;\n\tif (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {\n\t\tif (unlikely(signal_pending_state(prev->state, prev))) {\n\t\t\tprev->state = TASK_RUNNING;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If a worker is going to sleep, notify and\n\t\t\t * ask workqueue whether it wants to wake up a\n\t\t\t * task to maintain concurrency.  If so, wake\n\t\t\t * up the task.\n\t\t\t */\n\t\t\tif (prev->flags & PF_WQ_WORKER) {\n\t\t\t\tstruct task_struct *to_wakeup;\n\n\t\t\t\tto_wakeup = wq_worker_sleeping(prev, cpu);\n\t\t\t\tif (to_wakeup)\n\t\t\t\t\ttry_to_wake_up_local(to_wakeup);\n\t\t\t}\n\t\t\tdeactivate_task(rq, prev, DEQUEUE_SLEEP);\n\t\t}\n\t\tswitch_count = &prev->nvcsw;\n\t}\n\n\tpre_schedule(rq, prev);\n\n\tif (unlikely(!rq->nr_running))\n\t\tidle_balance(cpu, rq);\n\n\tput_prev_task(rq, prev);\n\tnext = pick_next_task(rq);\n\tclear_tsk_need_resched(prev);\n\trq->skip_clock_update = 0;\n\n\tif (likely(prev != next)) {\n\t\tsched_info_switch(prev, next);\n\t\tperf_event_task_sched_out(prev, next);\n\n\t\trq->nr_switches++;\n\t\trq->curr = next;\n\t\t++*switch_count;\n\t\tWARN_ON_ONCE(test_tsk_need_resched(next));\n\n\t\tcontext_switch(rq, prev, next); /* unlocks the rq */\n\t\t/*\n\t\t * The context switch have flipped the stack from under us\n\t\t * and restored the local variables which were saved when\n\t\t * this task called schedule() in the past. prev == current\n\t\t * is still correct, but it can be moved to another cpu/rq.\n\t\t */\n\t\tcpu = smp_processor_id();\n\t\trq = cpu_rq(cpu);\n\t} else\n\t\traw_spin_unlock_irq(&rq->lock);\n\n\tpost_schedule(rq);\n\n\tif (unlikely(reacquire_kernel_lock(prev)))\n\t\tgoto need_resched_nonpreemptible;\n\n\tpreempt_enable_no_resched();\n\tif (need_resched())\n\t\tgoto need_resched;\n}\nEXPORT_SYMBOL(schedule);\n\n#ifdef CONFIG_MUTEX_SPIN_ON_OWNER\n/*\n * Look out! \"owner\" is an entirely speculative pointer\n * access and not reliable.\n */\nint mutex_spin_on_owner(struct mutex *lock, struct thread_info *owner)\n{\n\tunsigned int cpu;\n\tstruct rq *rq;\n\n\tif (!sched_feat(OWNER_SPIN))\n\t\treturn 0;\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\n\t/*\n\t * Need to access the cpu field knowing that\n\t * DEBUG_PAGEALLOC could have unmapped it if\n\t * the mutex owner just released it and exited.\n\t */\n\tif (probe_kernel_address(&owner->cpu, cpu))\n\t\treturn 0;\n#else\n\tcpu = owner->cpu;\n#endif\n\n\t/*\n\t * Even if the access succeeded (likely case),\n\t * the cpu field may no longer be valid.\n\t */\n\tif (cpu >= nr_cpumask_bits)\n\t\treturn 0;\n\n\t/*\n\t * We need to validate that we can do a\n\t * get_cpu() and that we have the percpu area.\n\t */\n\tif (!cpu_online(cpu))\n\t\treturn 0;\n\n\trq = cpu_rq(cpu);\n\n\tfor (;;) {\n\t\t/*\n\t\t * Owner changed, break to re-assess state.\n\t\t */\n\t\tif (lock->owner != owner) {\n\t\t\t/*\n\t\t\t * If the lock has switched to a different owner,\n\t\t\t * we likely have heavy contention. Return 0 to quit\n\t\t\t * optimistic spinning and not contend further:\n\t\t\t */\n\t\t\tif (lock->owner)\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Is that owner really running on that cpu?\n\t\t */\n\t\tif (task_thread_info(rq->curr) != owner || need_resched())\n\t\t\treturn 0;\n\n\t\tcpu_relax();\n\t}\n\n\treturn 1;\n}\n#endif\n\n#ifdef CONFIG_PREEMPT\n/*\n * this is the entry point to schedule() from in-kernel preemption\n * off of preempt_enable. Kernel preemptions off return from interrupt\n * occur there and call schedule directly.\n */\nasmlinkage void __sched notrace preempt_schedule(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\n\t/*\n\t * If there is a non-zero preempt_count or interrupts are disabled,\n\t * we do not want to preempt the current task. Just return..\n\t */\n\tif (likely(ti->preempt_count || irqs_disabled()))\n\t\treturn;\n\n\tdo {\n\t\tadd_preempt_count_notrace(PREEMPT_ACTIVE);\n\t\tschedule();\n\t\tsub_preempt_count_notrace(PREEMPT_ACTIVE);\n\n\t\t/*\n\t\t * Check again in case we missed a preemption opportunity\n\t\t * between schedule and now.\n\t\t */\n\t\tbarrier();\n\t} while (need_resched());\n}\nEXPORT_SYMBOL(preempt_schedule);\n\n/*\n * this is the entry point to schedule() from kernel preemption\n * off of irq context.\n * Note, that this is called and return with irqs disabled. This will\n * protect us against recursive calling from irq.\n */\nasmlinkage void __sched preempt_schedule_irq(void)\n{\n\tstruct thread_info *ti = current_thread_info();\n\n\t/* Catch callers which need to be fixed */\n\tBUG_ON(ti->preempt_count || !irqs_disabled());\n\n\tdo {\n\t\tadd_preempt_count(PREEMPT_ACTIVE);\n\t\tlocal_irq_enable();\n\t\tschedule();\n\t\tlocal_irq_disable();\n\t\tsub_preempt_count(PREEMPT_ACTIVE);\n\n\t\t/*\n\t\t * Check again in case we missed a preemption opportunity\n\t\t * between schedule and now.\n\t\t */\n\t\tbarrier();\n\t} while (need_resched());\n}\n\n#endif /* CONFIG_PREEMPT */\n\nint default_wake_function(wait_queue_t *curr, unsigned mode, int wake_flags,\n\t\t\t  void *key)\n{\n\treturn try_to_wake_up(curr->private, mode, wake_flags);\n}\nEXPORT_SYMBOL(default_wake_function);\n\n/*\n * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just\n * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve\n * number) then we wake all the non-exclusive tasks and one exclusive task.\n *\n * There are circumstances in which we can try to wake a task which has already\n * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns\n * zero in this (rare) case, and we handle it by continuing to scan the queue.\n */\nstatic void __wake_up_common(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, int wake_flags, void *key)\n{\n\twait_queue_t *curr, *next;\n\n\tlist_for_each_entry_safe(curr, next, &q->task_list, task_list) {\n\t\tunsigned flags = curr->flags;\n\n\t\tif (curr->func(curr, mode, wake_flags, key) &&\n\t\t\t\t(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)\n\t\t\tbreak;\n\t}\n}\n\n/**\n * __wake_up - wake up threads blocked on a waitqueue.\n * @q: the waitqueue\n * @mode: which threads\n * @nr_exclusive: how many wake-one or wake-many threads to wake up\n * @key: is directly passed to the wakeup function\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid __wake_up(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, void *key)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__wake_up_common(q, mode, nr_exclusive, 0, key);\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\nEXPORT_SYMBOL(__wake_up);\n\n/*\n * Same as __wake_up but called with the spinlock in wait_queue_head_t held.\n */\nvoid __wake_up_locked(wait_queue_head_t *q, unsigned int mode)\n{\n\t__wake_up_common(q, mode, 1, 0, NULL);\n}\nEXPORT_SYMBOL_GPL(__wake_up_locked);\n\nvoid __wake_up_locked_key(wait_queue_head_t *q, unsigned int mode, void *key)\n{\n\t__wake_up_common(q, mode, 1, 0, key);\n}\n\n/**\n * __wake_up_sync_key - wake up threads blocked on a waitqueue.\n * @q: the waitqueue\n * @mode: which threads\n * @nr_exclusive: how many wake-one or wake-many threads to wake up\n * @key: opaque value to be passed to wakeup targets\n *\n * The sync wakeup differs that the waker knows that it will schedule\n * away soon, so while the target thread will be woken up, it will not\n * be migrated to another CPU - ie. the two threads are 'synchronized'\n * with each other. This can prevent needless bouncing between CPUs.\n *\n * On UP it can prevent extra preemption.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode,\n\t\t\tint nr_exclusive, void *key)\n{\n\tunsigned long flags;\n\tint wake_flags = WF_SYNC;\n\n\tif (unlikely(!q))\n\t\treturn;\n\n\tif (unlikely(!nr_exclusive))\n\t\twake_flags = 0;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__wake_up_common(q, mode, nr_exclusive, wake_flags, key);\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\nEXPORT_SYMBOL_GPL(__wake_up_sync_key);\n\n/*\n * __wake_up_sync - see __wake_up_sync_key()\n */\nvoid __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)\n{\n\t__wake_up_sync_key(q, mode, nr_exclusive, NULL);\n}\nEXPORT_SYMBOL_GPL(__wake_up_sync);\t/* For internal use only */\n\n/**\n * complete: - signals a single thread waiting on this completion\n * @x:  holds the state of this particular completion\n *\n * This will wake up a single thread waiting on this completion. Threads will be\n * awakened in the same order in which they were queued.\n *\n * See also complete_all(), wait_for_completion() and related routines.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid complete(struct completion *x)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tx->done++;\n\t__wake_up_common(&x->wait, TASK_NORMAL, 1, 0, NULL);\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n}\nEXPORT_SYMBOL(complete);\n\n/**\n * complete_all: - signals all threads waiting on this completion\n * @x:  holds the state of this particular completion\n *\n * This will wake up all threads waiting on this particular completion event.\n *\n * It may be assumed that this function implies a write memory barrier before\n * changing the task state if and only if any tasks are woken up.\n */\nvoid complete_all(struct completion *x)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tx->done += UINT_MAX/2;\n\t__wake_up_common(&x->wait, TASK_NORMAL, 0, 0, NULL);\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n}\nEXPORT_SYMBOL(complete_all);\n\nstatic inline long __sched\ndo_wait_for_common(struct completion *x, long timeout, int state)\n{\n\tif (!x->done) {\n\t\tDECLARE_WAITQUEUE(wait, current);\n\n\t\t__add_wait_queue_tail_exclusive(&x->wait, &wait);\n\t\tdo {\n\t\t\tif (signal_pending_state(state, current)) {\n\t\t\t\ttimeout = -ERESTARTSYS;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t__set_current_state(state);\n\t\t\tspin_unlock_irq(&x->wait.lock);\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tspin_lock_irq(&x->wait.lock);\n\t\t} while (!x->done && timeout);\n\t\t__remove_wait_queue(&x->wait, &wait);\n\t\tif (!x->done)\n\t\t\treturn timeout;\n\t}\n\tx->done--;\n\treturn timeout ?: 1;\n}\n\nstatic long __sched\nwait_for_common(struct completion *x, long timeout, int state)\n{\n\tmight_sleep();\n\n\tspin_lock_irq(&x->wait.lock);\n\ttimeout = do_wait_for_common(x, timeout, state);\n\tspin_unlock_irq(&x->wait.lock);\n\treturn timeout;\n}\n\n/**\n * wait_for_completion: - waits for completion of a task\n * @x:  holds the state of this particular completion\n *\n * This waits to be signaled for completion of a specific task. It is NOT\n * interruptible and there is no timeout.\n *\n * See also similar routines (i.e. wait_for_completion_timeout()) with timeout\n * and interrupt capability. Also see complete().\n */\nvoid __sched wait_for_completion(struct completion *x)\n{\n\twait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion);\n\n/**\n * wait_for_completion_timeout: - waits for completion of a task (w/timeout)\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be signaled or for a\n * specified timeout to expire. The timeout is in jiffies. It is not\n * interruptible.\n */\nunsigned long __sched\nwait_for_completion_timeout(struct completion *x, unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion_timeout);\n\n/**\n * wait_for_completion_interruptible: - waits for completion of a task (w/intr)\n * @x:  holds the state of this particular completion\n *\n * This waits for completion of a specific task to be signaled. It is\n * interruptible.\n */\nint __sched wait_for_completion_interruptible(struct completion *x)\n{\n\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_INTERRUPTIBLE);\n\tif (t == -ERESTARTSYS)\n\t\treturn t;\n\treturn 0;\n}\nEXPORT_SYMBOL(wait_for_completion_interruptible);\n\n/**\n * wait_for_completion_interruptible_timeout: - waits for completion (w/(to,intr))\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be signaled or for a\n * specified timeout to expire. It is interruptible. The timeout is in jiffies.\n */\nunsigned long __sched\nwait_for_completion_interruptible_timeout(struct completion *x,\n\t\t\t\t\t  unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_INTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion_interruptible_timeout);\n\n/**\n * wait_for_completion_killable: - waits for completion of a task (killable)\n * @x:  holds the state of this particular completion\n *\n * This waits to be signaled for completion of a specific task. It can be\n * interrupted by a kill signal.\n */\nint __sched wait_for_completion_killable(struct completion *x)\n{\n\tlong t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_KILLABLE);\n\tif (t == -ERESTARTSYS)\n\t\treturn t;\n\treturn 0;\n}\nEXPORT_SYMBOL(wait_for_completion_killable);\n\n/**\n * wait_for_completion_killable_timeout: - waits for completion of a task (w/(to,killable))\n * @x:  holds the state of this particular completion\n * @timeout:  timeout value in jiffies\n *\n * This waits for either a completion of a specific task to be\n * signaled or for a specified timeout to expire. It can be\n * interrupted by a kill signal. The timeout is in jiffies.\n */\nunsigned long __sched\nwait_for_completion_killable_timeout(struct completion *x,\n\t\t\t\t     unsigned long timeout)\n{\n\treturn wait_for_common(x, timeout, TASK_KILLABLE);\n}\nEXPORT_SYMBOL(wait_for_completion_killable_timeout);\n\n/**\n *\ttry_wait_for_completion - try to decrement a completion without blocking\n *\t@x:\tcompletion structure\n *\n *\tReturns: 0 if a decrement cannot be done without blocking\n *\t\t 1 if a decrement succeeded.\n *\n *\tIf a completion is being used as a counting completion,\n *\tattempt to decrement the counter without blocking. This\n *\tenables us to avoid waiting if the resource the completion\n *\tis protecting is not available.\n */\nbool try_wait_for_completion(struct completion *x)\n{\n\tunsigned long flags;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tif (!x->done)\n\t\tret = 0;\n\telse\n\t\tx->done--;\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(try_wait_for_completion);\n\n/**\n *\tcompletion_done - Test to see if a completion has any waiters\n *\t@x:\tcompletion structure\n *\n *\tReturns: 0 if there are waiters (wait_for_completion() in progress)\n *\t\t 1 if there are no waiters.\n *\n */\nbool completion_done(struct completion *x)\n{\n\tunsigned long flags;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&x->wait.lock, flags);\n\tif (!x->done)\n\t\tret = 0;\n\tspin_unlock_irqrestore(&x->wait.lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(completion_done);\n\nstatic long __sched\nsleep_on_common(wait_queue_head_t *q, int state, long timeout)\n{\n\tunsigned long flags;\n\twait_queue_t wait;\n\n\tinit_waitqueue_entry(&wait, current);\n\n\t__set_current_state(state);\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__add_wait_queue(q, &wait);\n\tspin_unlock(&q->lock);\n\ttimeout = schedule_timeout(timeout);\n\tspin_lock_irq(&q->lock);\n\t__remove_wait_queue(q, &wait);\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\treturn timeout;\n}\n\nvoid __sched interruptible_sleep_on(wait_queue_head_t *q)\n{\n\tsleep_on_common(q, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n}\nEXPORT_SYMBOL(interruptible_sleep_on);\n\nlong __sched\ninterruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)\n{\n\treturn sleep_on_common(q, TASK_INTERRUPTIBLE, timeout);\n}\nEXPORT_SYMBOL(interruptible_sleep_on_timeout);\n\nvoid __sched sleep_on(wait_queue_head_t *q)\n{\n\tsleep_on_common(q, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n}\nEXPORT_SYMBOL(sleep_on);\n\nlong __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)\n{\n\treturn sleep_on_common(q, TASK_UNINTERRUPTIBLE, timeout);\n}\nEXPORT_SYMBOL(sleep_on_timeout);\n\n#ifdef CONFIG_RT_MUTEXES\n\n/*\n * rt_mutex_setprio - set the current priority of a task\n * @p: task\n * @prio: prio value (kernel-internal form)\n *\n * This function changes the 'effective' priority of a task. It does\n * not touch ->normal_prio like __setscheduler().\n *\n * Used by the rt_mutex code to implement priority inheritance logic.\n */\nvoid rt_mutex_setprio(struct task_struct *p, int prio)\n{\n\tunsigned long flags;\n\tint oldprio, on_rq, running;\n\tstruct rq *rq;\n\tconst struct sched_class *prev_class;\n\n\tBUG_ON(prio < 0 || prio > MAX_PRIO);\n\n\trq = task_rq_lock(p, &flags);\n\n\ttrace_sched_pi_setprio(p, prio);\n\toldprio = p->prio;\n\tprev_class = p->sched_class;\n\ton_rq = p->se.on_rq;\n\trunning = task_current(rq, p);\n\tif (on_rq)\n\t\tdequeue_task(rq, p, 0);\n\tif (running)\n\t\tp->sched_class->put_prev_task(rq, p);\n\n\tif (rt_prio(prio))\n\t\tp->sched_class = &rt_sched_class;\n\telse\n\t\tp->sched_class = &fair_sched_class;\n\n\tp->prio = prio;\n\n\tif (running)\n\t\tp->sched_class->set_curr_task(rq);\n\tif (on_rq) {\n\t\tenqueue_task(rq, p, oldprio < prio ? ENQUEUE_HEAD : 0);\n\n\t\tcheck_class_changed(rq, p, prev_class, oldprio, running);\n\t}\n\ttask_rq_unlock(rq, &flags);\n}\n\n#endif\n\nvoid set_user_nice(struct task_struct *p, long nice)\n{\n\tint old_prio, delta, on_rq;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\tif (TASK_NICE(p) == nice || nice < -20 || nice > 19)\n\t\treturn;\n\t/*\n\t * We have to be careful, if called from sys_setpriority(),\n\t * the task might be in the middle of scheduling on another CPU.\n\t */\n\trq = task_rq_lock(p, &flags);\n\t/*\n\t * The RT priorities are set via sched_setscheduler(), but we still\n\t * allow the 'normal' nice value to be set - but as expected\n\t * it wont have any effect on scheduling until the task is\n\t * SCHED_FIFO/SCHED_RR:\n\t */\n\tif (task_has_rt_policy(p)) {\n\t\tp->static_prio = NICE_TO_PRIO(nice);\n\t\tgoto out_unlock;\n\t}\n\ton_rq = p->se.on_rq;\n\tif (on_rq)\n\t\tdequeue_task(rq, p, 0);\n\n\tp->static_prio = NICE_TO_PRIO(nice);\n\tset_load_weight(p);\n\told_prio = p->prio;\n\tp->prio = effective_prio(p);\n\tdelta = p->prio - old_prio;\n\n\tif (on_rq) {\n\t\tenqueue_task(rq, p, 0);\n\t\t/*\n\t\t * If the task increased its priority or is running and\n\t\t * lowered its priority, then reschedule its CPU:\n\t\t */\n\t\tif (delta < 0 || (delta > 0 && task_running(rq, p)))\n\t\t\tresched_task(rq->curr);\n\t}\nout_unlock:\n\ttask_rq_unlock(rq, &flags);\n}\nEXPORT_SYMBOL(set_user_nice);\n\n/*\n * can_nice - check if a task can reduce its nice value\n * @p: task\n * @nice: nice value\n */\nint can_nice(const struct task_struct *p, const int nice)\n{\n\t/* convert nice value [19,-20] to rlimit style value [1,40] */\n\tint nice_rlim = 20 - nice;\n\n\treturn (nice_rlim <= task_rlimit(p, RLIMIT_NICE) ||\n\t\tcapable(CAP_SYS_NICE));\n}\n\n#ifdef __ARCH_WANT_SYS_NICE\n\n/*\n * sys_nice - change the priority of the current process.\n * @increment: priority increment\n *\n * sys_setpriority is a more generic, but much slower function that\n * does similar things.\n */\nSYSCALL_DEFINE1(nice, int, increment)\n{\n\tlong nice, retval;\n\n\t/*\n\t * Setpriority might change our priority at the same moment.\n\t * We don't have to worry. Conceptually one call occurs first\n\t * and we have a single winner.\n\t */\n\tif (increment < -40)\n\t\tincrement = -40;\n\tif (increment > 40)\n\t\tincrement = 40;\n\n\tnice = TASK_NICE(current) + increment;\n\tif (nice < -20)\n\t\tnice = -20;\n\tif (nice > 19)\n\t\tnice = 19;\n\n\tif (increment < 0 && !can_nice(current, nice))\n\t\treturn -EPERM;\n\n\tretval = security_task_setnice(current, nice);\n\tif (retval)\n\t\treturn retval;\n\n\tset_user_nice(current, nice);\n\treturn 0;\n}\n\n#endif\n\n/**\n * task_prio - return the priority value of a given task.\n * @p: the task in question.\n *\n * This is the priority value as seen by users in /proc.\n * RT tasks are offset by -200. Normal tasks are centered\n * around 0, value goes from -16 to +15.\n */\nint task_prio(const struct task_struct *p)\n{\n\treturn p->prio - MAX_RT_PRIO;\n}\n\n/**\n * task_nice - return the nice value of a given task.\n * @p: the task in question.\n */\nint task_nice(const struct task_struct *p)\n{\n\treturn TASK_NICE(p);\n}\nEXPORT_SYMBOL(task_nice);\n\n/**\n * idle_cpu - is a given cpu idle currently?\n * @cpu: the processor in question.\n */\nint idle_cpu(int cpu)\n{\n\treturn cpu_curr(cpu) == cpu_rq(cpu)->idle;\n}\n\n/**\n * idle_task - return the idle task for a given cpu.\n * @cpu: the processor in question.\n */\nstruct task_struct *idle_task(int cpu)\n{\n\treturn cpu_rq(cpu)->idle;\n}\n\n/**\n * find_process_by_pid - find a process with a matching PID value.\n * @pid: the pid in question.\n */\nstatic struct task_struct *find_process_by_pid(pid_t pid)\n{\n\treturn pid ? find_task_by_vpid(pid) : current;\n}\n\n/* Actually do priority change: must hold rq lock. */\nstatic void\n__setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)\n{\n\tBUG_ON(p->se.on_rq);\n\n\tp->policy = policy;\n\tp->rt_priority = prio;\n\tp->normal_prio = normal_prio(p);\n\t/* we are holding p->pi_lock already */\n\tp->prio = rt_mutex_getprio(p);\n\tif (rt_prio(p->prio))\n\t\tp->sched_class = &rt_sched_class;\n\telse\n\t\tp->sched_class = &fair_sched_class;\n\tset_load_weight(p);\n}\n\n/*\n * check the target process has a UID that matches the current process's\n */\nstatic bool check_same_owner(struct task_struct *p)\n{\n\tconst struct cred *cred = current_cred(), *pcred;\n\tbool match;\n\n\trcu_read_lock();\n\tpcred = __task_cred(p);\n\tmatch = (cred->euid == pcred->euid ||\n\t\t cred->euid == pcred->uid);\n\trcu_read_unlock();\n\treturn match;\n}\n\nstatic int __sched_setscheduler(struct task_struct *p, int policy,\n\t\t\t\tstruct sched_param *param, bool user)\n{\n\tint retval, oldprio, oldpolicy = -1, on_rq, running;\n\tunsigned long flags;\n\tconst struct sched_class *prev_class;\n\tstruct rq *rq;\n\tint reset_on_fork;\n\n\t/* may grab non-irq protected spin_locks */\n\tBUG_ON(in_interrupt());\nrecheck:\n\t/* double check policy once rq lock held */\n\tif (policy < 0) {\n\t\treset_on_fork = p->sched_reset_on_fork;\n\t\tpolicy = oldpolicy = p->policy;\n\t} else {\n\t\treset_on_fork = !!(policy & SCHED_RESET_ON_FORK);\n\t\tpolicy &= ~SCHED_RESET_ON_FORK;\n\n\t\tif (policy != SCHED_FIFO && policy != SCHED_RR &&\n\t\t\t\tpolicy != SCHED_NORMAL && policy != SCHED_BATCH &&\n\t\t\t\tpolicy != SCHED_IDLE)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Valid priorities for SCHED_FIFO and SCHED_RR are\n\t * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL,\n\t * SCHED_BATCH and SCHED_IDLE is 0.\n\t */\n\tif (param->sched_priority < 0 ||\n\t    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||\n\t    (!p->mm && param->sched_priority > MAX_RT_PRIO-1))\n\t\treturn -EINVAL;\n\tif (rt_policy(policy) != (param->sched_priority != 0))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Allow unprivileged RT tasks to decrease priority:\n\t */\n\tif (user && !capable(CAP_SYS_NICE)) {\n\t\tif (rt_policy(policy)) {\n\t\t\tunsigned long rlim_rtprio =\n\t\t\t\t\ttask_rlimit(p, RLIMIT_RTPRIO);\n\n\t\t\t/* can't set/change the rt policy */\n\t\t\tif (policy != p->policy && !rlim_rtprio)\n\t\t\t\treturn -EPERM;\n\n\t\t\t/* can't increase priority */\n\t\t\tif (param->sched_priority > p->rt_priority &&\n\t\t\t    param->sched_priority > rlim_rtprio)\n\t\t\t\treturn -EPERM;\n\t\t}\n\t\t/*\n\t\t * Like positive nice levels, dont allow tasks to\n\t\t * move out of SCHED_IDLE either:\n\t\t */\n\t\tif (p->policy == SCHED_IDLE && policy != SCHED_IDLE)\n\t\t\treturn -EPERM;\n\n\t\t/* can't change other user's priorities */\n\t\tif (!check_same_owner(p))\n\t\t\treturn -EPERM;\n\n\t\t/* Normal users shall not reset the sched_reset_on_fork flag */\n\t\tif (p->sched_reset_on_fork && !reset_on_fork)\n\t\t\treturn -EPERM;\n\t}\n\n\tif (user) {\n\t\tretval = security_task_setscheduler(p);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\t/*\n\t * make sure no PI-waiters arrive (or leave) while we are\n\t * changing the priority of the task:\n\t */\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\t/*\n\t * To be able to change p->policy safely, the apropriate\n\t * runqueue lock must be held.\n\t */\n\trq = __task_rq_lock(p);\n\n\t/*\n\t * Changing the policy of the stop threads its a very bad idea\n\t */\n\tif (p == rq->stop) {\n\t\t__task_rq_unlock(rq);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\t\treturn -EINVAL;\n\t}\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tif (user) {\n\t\t/*\n\t\t * Do not allow realtime tasks into groups that have no runtime\n\t\t * assigned.\n\t\t */\n\t\tif (rt_bandwidth_enabled() && rt_policy(policy) &&\n\t\t\t\ttask_group(p)->rt_bandwidth.rt_runtime == 0) {\n\t\t\t__task_rq_unlock(rq);\n\t\t\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n#endif\n\n\t/* recheck policy now with rq lock held */\n\tif (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {\n\t\tpolicy = oldpolicy = -1;\n\t\t__task_rq_unlock(rq);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\t\tgoto recheck;\n\t}\n\ton_rq = p->se.on_rq;\n\trunning = task_current(rq, p);\n\tif (on_rq)\n\t\tdeactivate_task(rq, p, 0);\n\tif (running)\n\t\tp->sched_class->put_prev_task(rq, p);\n\n\tp->sched_reset_on_fork = reset_on_fork;\n\n\toldprio = p->prio;\n\tprev_class = p->sched_class;\n\t__setscheduler(rq, p, policy, param->sched_priority);\n\n\tif (running)\n\t\tp->sched_class->set_curr_task(rq);\n\tif (on_rq) {\n\t\tactivate_task(rq, p, 0);\n\n\t\tcheck_class_changed(rq, p, prev_class, oldprio, running);\n\t}\n\t__task_rq_unlock(rq);\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\n\trt_mutex_adjust_pi(p);\n\n\treturn 0;\n}\n\n/**\n * sched_setscheduler - change the scheduling policy and/or RT priority of a thread.\n * @p: the task in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n *\n * NOTE that the task may be already dead.\n */\nint sched_setscheduler(struct task_struct *p, int policy,\n\t\t       struct sched_param *param)\n{\n\treturn __sched_setscheduler(p, policy, param, true);\n}\nEXPORT_SYMBOL_GPL(sched_setscheduler);\n\n/**\n * sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace.\n * @p: the task in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n *\n * Just like sched_setscheduler, only don't bother checking if the\n * current context has permission.  For example, this is needed in\n * stop_machine(): we create temporary high priority worker threads,\n * but our caller might not have that capability.\n */\nint sched_setscheduler_nocheck(struct task_struct *p, int policy,\n\t\t\t       struct sched_param *param)\n{\n\treturn __sched_setscheduler(p, policy, param, false);\n}\n\nstatic int\ndo_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)\n{\n\tstruct sched_param lparam;\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!param || pid < 0)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&lparam, param, sizeof(struct sched_param)))\n\t\treturn -EFAULT;\n\n\trcu_read_lock();\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (p != NULL)\n\t\tretval = sched_setscheduler(p, policy, &lparam);\n\trcu_read_unlock();\n\n\treturn retval;\n}\n\n/**\n * sys_sched_setscheduler - set/change the scheduler policy and RT priority\n * @pid: the pid in question.\n * @policy: new policy.\n * @param: structure containing the new RT priority.\n */\nSYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy,\n\t\tstruct sched_param __user *, param)\n{\n\t/* negative values for policy are not valid */\n\tif (policy < 0)\n\t\treturn -EINVAL;\n\n\treturn do_sched_setscheduler(pid, policy, param);\n}\n\n/**\n * sys_sched_setparam - set/change the RT priority of a thread\n * @pid: the pid in question.\n * @param: structure containing the new RT priority.\n */\nSYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param)\n{\n\treturn do_sched_setscheduler(pid, -1, param);\n}\n\n/**\n * sys_sched_getscheduler - get the policy (scheduling class) of a thread\n * @pid: the pid in question.\n */\nSYSCALL_DEFINE1(sched_getscheduler, pid_t, pid)\n{\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (pid < 0)\n\t\treturn -EINVAL;\n\n\tretval = -ESRCH;\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tif (p) {\n\t\tretval = security_task_getscheduler(p);\n\t\tif (!retval)\n\t\t\tretval = p->policy\n\t\t\t\t| (p->sched_reset_on_fork ? SCHED_RESET_ON_FORK : 0);\n\t}\n\trcu_read_unlock();\n\treturn retval;\n}\n\n/**\n * sys_sched_getparam - get the RT priority of a thread\n * @pid: the pid in question.\n * @param: structure containing the RT priority.\n */\nSYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param)\n{\n\tstruct sched_param lp;\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!param || pid < 0)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tretval = -ESRCH;\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tlp.sched_priority = p->rt_priority;\n\trcu_read_unlock();\n\n\t/*\n\t * This one might sleep, we cannot do it with a spinlock held ...\n\t */\n\tretval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;\n\n\treturn retval;\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\nlong sched_setaffinity(pid_t pid, const struct cpumask *in_mask)\n{\n\tcpumask_var_t cpus_allowed, new_mask;\n\tstruct task_struct *p;\n\tint retval;\n\n\tget_online_cpus();\n\trcu_read_lock();\n\n\tp = find_process_by_pid(pid);\n\tif (!p) {\n\t\trcu_read_unlock();\n\t\tput_online_cpus();\n\t\treturn -ESRCH;\n\t}\n\n\t/* Prevent p going away */\n\tget_task_struct(p);\n\trcu_read_unlock();\n\n\tif (!alloc_cpumask_var(&cpus_allowed, GFP_KERNEL)) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_put_task;\n\t}\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_free_cpus_allowed;\n\t}\n\tretval = -EPERM;\n\tif (!check_same_owner(p) && !capable(CAP_SYS_NICE))\n\t\tgoto out_unlock;\n\n\tretval = security_task_setscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tcpuset_cpus_allowed(p, cpus_allowed);\n\tcpumask_and(new_mask, in_mask, cpus_allowed);\nagain:\n\tretval = set_cpus_allowed_ptr(p, new_mask);\n\n\tif (!retval) {\n\t\tcpuset_cpus_allowed(p, cpus_allowed);\n\t\tif (!cpumask_subset(new_mask, cpus_allowed)) {\n\t\t\t/*\n\t\t\t * We must have raced with a concurrent cpuset\n\t\t\t * update. Just reset the cpus_allowed to the\n\t\t\t * cpuset's cpus_allowed\n\t\t\t */\n\t\t\tcpumask_copy(new_mask, cpus_allowed);\n\t\t\tgoto again;\n\t\t}\n\t}\nout_unlock:\n\tfree_cpumask_var(new_mask);\nout_free_cpus_allowed:\n\tfree_cpumask_var(cpus_allowed);\nout_put_task:\n\tput_task_struct(p);\n\tput_online_cpus();\n\treturn retval;\n}\n\nstatic int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,\n\t\t\t     struct cpumask *new_mask)\n{\n\tif (len < cpumask_size())\n\t\tcpumask_clear(new_mask);\n\telse if (len > cpumask_size())\n\t\tlen = cpumask_size();\n\n\treturn copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;\n}\n\n/**\n * sys_sched_setaffinity - set the cpu affinity of a process\n * @pid: pid of the process\n * @len: length in bytes of the bitmask pointed to by user_mask_ptr\n * @user_mask_ptr: user-space pointer to the new cpu mask\n */\nSYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len,\n\t\tunsigned long __user *, user_mask_ptr)\n{\n\tcpumask_var_t new_mask;\n\tint retval;\n\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tretval = get_user_cpu_mask(user_mask_ptr, len, new_mask);\n\tif (retval == 0)\n\t\tretval = sched_setaffinity(pid, new_mask);\n\tfree_cpumask_var(new_mask);\n\treturn retval;\n}\n\nlong sched_getaffinity(pid_t pid, struct cpumask *mask)\n{\n\tstruct task_struct *p;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint retval;\n\n\tget_online_cpus();\n\trcu_read_lock();\n\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\trq = task_rq_lock(p, &flags);\n\tcpumask_and(mask, &p->cpus_allowed, cpu_online_mask);\n\ttask_rq_unlock(rq, &flags);\n\nout_unlock:\n\trcu_read_unlock();\n\tput_online_cpus();\n\n\treturn retval;\n}\n\n/**\n * sys_sched_getaffinity - get the cpu affinity of a process\n * @pid: pid of the process\n * @len: length in bytes of the bitmask pointed to by user_mask_ptr\n * @user_mask_ptr: user-space pointer to hold the current cpu mask\n */\nSYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len,\n\t\tunsigned long __user *, user_mask_ptr)\n{\n\tint ret;\n\tcpumask_var_t mask;\n\n\tif ((len * BITS_PER_BYTE) < nr_cpu_ids)\n\t\treturn -EINVAL;\n\tif (len & (sizeof(unsigned long)-1))\n\t\treturn -EINVAL;\n\n\tif (!alloc_cpumask_var(&mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tret = sched_getaffinity(pid, mask);\n\tif (ret == 0) {\n\t\tsize_t retlen = min_t(size_t, len, cpumask_size());\n\n\t\tif (copy_to_user(user_mask_ptr, mask, retlen))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = retlen;\n\t}\n\tfree_cpumask_var(mask);\n\n\treturn ret;\n}\n\n/**\n * sys_sched_yield - yield the current processor to other threads.\n *\n * This function yields the current CPU to other tasks. If there are no\n * other threads running on this CPU then this function will return.\n */\nSYSCALL_DEFINE0(sched_yield)\n{\n\tstruct rq *rq = this_rq_lock();\n\n\tschedstat_inc(rq, yld_count);\n\tcurrent->sched_class->yield_task(rq);\n\n\t/*\n\t * Since we are going to call schedule() anyway, there's\n\t * no need to preempt or enable interrupts:\n\t */\n\t__release(rq->lock);\n\tspin_release(&rq->lock.dep_map, 1, _THIS_IP_);\n\tdo_raw_spin_unlock(&rq->lock);\n\tpreempt_enable_no_resched();\n\n\tschedule();\n\n\treturn 0;\n}\n\nstatic inline int should_resched(void)\n{\n\treturn need_resched() && !(preempt_count() & PREEMPT_ACTIVE);\n}\n\nstatic void __cond_resched(void)\n{\n\tadd_preempt_count(PREEMPT_ACTIVE);\n\tschedule();\n\tsub_preempt_count(PREEMPT_ACTIVE);\n}\n\nint __sched _cond_resched(void)\n{\n\tif (should_resched()) {\n\t\t__cond_resched();\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(_cond_resched);\n\n/*\n * __cond_resched_lock() - if a reschedule is pending, drop the given lock,\n * call schedule, and on return reacquire the lock.\n *\n * This works OK both with and without CONFIG_PREEMPT. We do strange low-level\n * operations here to prevent schedule() from being called twice (once via\n * spin_unlock(), once by hand).\n */\nint __cond_resched_lock(spinlock_t *lock)\n{\n\tint resched = should_resched();\n\tint ret = 0;\n\n\tlockdep_assert_held(lock);\n\n\tif (spin_needbreak(lock) || resched) {\n\t\tspin_unlock(lock);\n\t\tif (resched)\n\t\t\t__cond_resched();\n\t\telse\n\t\t\tcpu_relax();\n\t\tret = 1;\n\t\tspin_lock(lock);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__cond_resched_lock);\n\nint __sched __cond_resched_softirq(void)\n{\n\tBUG_ON(!in_softirq());\n\n\tif (should_resched()) {\n\t\tlocal_bh_enable();\n\t\t__cond_resched();\n\t\tlocal_bh_disable();\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__cond_resched_softirq);\n\n/**\n * yield - yield the current processor to other threads.\n *\n * This is a shortcut for kernel-space yielding - it marks the\n * thread runnable and calls sys_sched_yield().\n */\nvoid __sched yield(void)\n{\n\tset_current_state(TASK_RUNNING);\n\tsys_sched_yield();\n}\nEXPORT_SYMBOL(yield);\n\n/*\n * This task is about to go to sleep on IO. Increment rq->nr_iowait so\n * that process accounting knows that this is a task in IO wait state.\n */\nvoid __sched io_schedule(void)\n{\n\tstruct rq *rq = raw_rq();\n\n\tdelayacct_blkio_start();\n\tatomic_inc(&rq->nr_iowait);\n\tcurrent->in_iowait = 1;\n\tschedule();\n\tcurrent->in_iowait = 0;\n\tatomic_dec(&rq->nr_iowait);\n\tdelayacct_blkio_end();\n}\nEXPORT_SYMBOL(io_schedule);\n\nlong __sched io_schedule_timeout(long timeout)\n{\n\tstruct rq *rq = raw_rq();\n\tlong ret;\n\n\tdelayacct_blkio_start();\n\tatomic_inc(&rq->nr_iowait);\n\tcurrent->in_iowait = 1;\n\tret = schedule_timeout(timeout);\n\tcurrent->in_iowait = 0;\n\tatomic_dec(&rq->nr_iowait);\n\tdelayacct_blkio_end();\n\treturn ret;\n}\n\n/**\n * sys_sched_get_priority_max - return maximum RT priority.\n * @policy: scheduling class.\n *\n * this syscall returns the maximum rt_priority that can be used\n * by a given scheduling class.\n */\nSYSCALL_DEFINE1(sched_get_priority_max, int, policy)\n{\n\tint ret = -EINVAL;\n\n\tswitch (policy) {\n\tcase SCHED_FIFO:\n\tcase SCHED_RR:\n\t\tret = MAX_USER_RT_PRIO-1;\n\t\tbreak;\n\tcase SCHED_NORMAL:\n\tcase SCHED_BATCH:\n\tcase SCHED_IDLE:\n\t\tret = 0;\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/**\n * sys_sched_get_priority_min - return minimum RT priority.\n * @policy: scheduling class.\n *\n * this syscall returns the minimum rt_priority that can be used\n * by a given scheduling class.\n */\nSYSCALL_DEFINE1(sched_get_priority_min, int, policy)\n{\n\tint ret = -EINVAL;\n\n\tswitch (policy) {\n\tcase SCHED_FIFO:\n\tcase SCHED_RR:\n\t\tret = 1;\n\t\tbreak;\n\tcase SCHED_NORMAL:\n\tcase SCHED_BATCH:\n\tcase SCHED_IDLE:\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\n/**\n * sys_sched_rr_get_interval - return the default timeslice of a process.\n * @pid: pid of the process.\n * @interval: userspace pointer to the timeslice value.\n *\n * this syscall writes the default timeslice value of a given process\n * into the user-space timespec buffer. A value of '0' means infinity.\n */\nSYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid,\n\t\tstruct timespec __user *, interval)\n{\n\tstruct task_struct *p;\n\tunsigned int time_slice;\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint retval;\n\tstruct timespec t;\n\n\tif (pid < 0)\n\t\treturn -EINVAL;\n\n\tretval = -ESRCH;\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\trq = task_rq_lock(p, &flags);\n\ttime_slice = p->sched_class->get_rr_interval(rq, p);\n\ttask_rq_unlock(rq, &flags);\n\n\trcu_read_unlock();\n\tjiffies_to_timespec(time_slice, &t);\n\tretval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;\n\treturn retval;\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\nstatic const char stat_nam[] = TASK_STATE_TO_CHAR_STR;\n\nvoid sched_show_task(struct task_struct *p)\n{\n\tunsigned long free = 0;\n\tunsigned state;\n\n\tstate = p->state ? __ffs(p->state) + 1 : 0;\n\tprintk(KERN_INFO \"%-13.13s %c\", p->comm,\n\t\tstate < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');\n#if BITS_PER_LONG == 32\n\tif (state == TASK_RUNNING)\n\t\tprintk(KERN_CONT \" running  \");\n\telse\n\t\tprintk(KERN_CONT \" %08lx \", thread_saved_pc(p));\n#else\n\tif (state == TASK_RUNNING)\n\t\tprintk(KERN_CONT \"  running task    \");\n\telse\n\t\tprintk(KERN_CONT \" %016lx \", thread_saved_pc(p));\n#endif\n#ifdef CONFIG_DEBUG_STACK_USAGE\n\tfree = stack_not_used(p);\n#endif\n\tprintk(KERN_CONT \"%5lu %5d %6d 0x%08lx\\n\", free,\n\t\ttask_pid_nr(p), task_pid_nr(p->real_parent),\n\t\t(unsigned long)task_thread_info(p)->flags);\n\n\tshow_stack(p, NULL);\n}\n\nvoid show_state_filter(unsigned long state_filter)\n{\n\tstruct task_struct *g, *p;\n\n#if BITS_PER_LONG == 32\n\tprintk(KERN_INFO\n\t\t\"  task                PC stack   pid father\\n\");\n#else\n\tprintk(KERN_INFO\n\t\t\"  task                        PC stack   pid father\\n\");\n#endif\n\tread_lock(&tasklist_lock);\n\tdo_each_thread(g, p) {\n\t\t/*\n\t\t * reset the NMI-timeout, listing all files on a slow\n\t\t * console might take alot of time:\n\t\t */\n\t\ttouch_nmi_watchdog();\n\t\tif (!state_filter || (p->state & state_filter))\n\t\t\tsched_show_task(p);\n\t} while_each_thread(g, p);\n\n\ttouch_all_softlockup_watchdogs();\n\n#ifdef CONFIG_SCHED_DEBUG\n\tsysrq_sched_debug_show();\n#endif\n\tread_unlock(&tasklist_lock);\n\t/*\n\t * Only show locks if all tasks are dumped:\n\t */\n\tif (!state_filter)\n\t\tdebug_show_all_locks();\n}\n\nvoid __cpuinit init_idle_bootup_task(struct task_struct *idle)\n{\n\tidle->sched_class = &idle_sched_class;\n}\n\n/**\n * init_idle - set up an idle thread for a given CPU\n * @idle: task in question\n * @cpu: cpu the idle task belongs to\n *\n * NOTE: this function does not set the idle thread's NEED_RESCHED\n * flag, to make booting more robust.\n */\nvoid __cpuinit init_idle(struct task_struct *idle, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\n\t__sched_fork(idle);\n\tidle->state = TASK_RUNNING;\n\tidle->se.exec_start = sched_clock();\n\n\tcpumask_copy(&idle->cpus_allowed, cpumask_of(cpu));\n\t/*\n\t * We're having a chicken and egg problem, even though we are\n\t * holding rq->lock, the cpu isn't yet set to this cpu so the\n\t * lockdep check in task_group() will fail.\n\t *\n\t * Similar case to sched_fork(). / Alternatively we could\n\t * use task_rq_lock() here and obtain the other rq->lock.\n\t *\n\t * Silence PROVE_RCU\n\t */\n\trcu_read_lock();\n\t__set_task_cpu(idle, cpu);\n\trcu_read_unlock();\n\n\trq->curr = rq->idle = idle;\n#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)\n\tidle->oncpu = 1;\n#endif\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\t/* Set the preempt count _outside_ the spinlocks! */\n#if defined(CONFIG_PREEMPT)\n\ttask_thread_info(idle)->preempt_count = (idle->lock_depth >= 0);\n#else\n\ttask_thread_info(idle)->preempt_count = 0;\n#endif\n\t/*\n\t * The idle tasks have their own, simple scheduling class:\n\t */\n\tidle->sched_class = &idle_sched_class;\n\tftrace_graph_init_task(idle);\n}\n\n/*\n * In a system that switches off the HZ timer nohz_cpu_mask\n * indicates which cpus entered this state. This is used\n * in the rcu update to wait only for active cpus. For system\n * which do not switch off the HZ timer nohz_cpu_mask should\n * always be CPU_BITS_NONE.\n */\ncpumask_var_t nohz_cpu_mask;\n\n/*\n * Increase the granularity value when there are more CPUs,\n * because with more CPUs the 'effective latency' as visible\n * to users decreases. But the relationship is not linear,\n * so pick a second-best guess by going with the log2 of the\n * number of CPUs.\n *\n * This idea comes from the SD scheduler of Con Kolivas:\n */\nstatic int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}\n\nstatic void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_min_granularity);\n\tSET_SYSCTL(sched_latency);\n\tSET_SYSCTL(sched_wakeup_granularity);\n\tSET_SYSCTL(sched_shares_ratelimit);\n#undef SET_SYSCTL\n}\n\nstatic inline void sched_init_granularity(void)\n{\n\tupdate_sysctl();\n}\n\n#ifdef CONFIG_SMP\n/*\n * This is how migration works:\n *\n * 1) we invoke migration_cpu_stop() on the target CPU using\n *    stop_one_cpu().\n * 2) stopper starts to run (implicitly forcing the migrated thread\n *    off the CPU)\n * 3) it checks whether the migrated task is still in the wrong runqueue.\n * 4) if it's in the wrong runqueue then the migration thread removes\n *    it and puts it into the right queue.\n * 5) stopper completes and stop_one_cpu() returns and the migration\n *    is done.\n */\n\n/*\n * Change a given task's CPU affinity. Migrate the thread to a\n * proper CPU and schedule it away if the CPU it's executing on\n * is removed from the allowed bitmask.\n *\n * NOTE: the caller must have a valid reference to the task, the\n * task must not exit() & deallocate itself prematurely. The\n * call is not atomic; no spinlocks may be held.\n */\nint set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tunsigned int dest_cpu;\n\tint ret = 0;\n\n\t/*\n\t * Serialize against TASK_WAKING so that ttwu() and wunt() can\n\t * drop the rq->lock and still rely on ->cpus_allowed.\n\t */\nagain:\n\twhile (task_is_waking(p))\n\t\tcpu_relax();\n\trq = task_rq_lock(p, &flags);\n\tif (task_is_waking(p)) {\n\t\ttask_rq_unlock(rq, &flags);\n\t\tgoto again;\n\t}\n\n\tif (!cpumask_intersects(new_mask, cpu_active_mask)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (unlikely((p->flags & PF_THREAD_BOUND) && p != current &&\n\t\t     !cpumask_equal(&p->cpus_allowed, new_mask))) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (p->sched_class->set_cpus_allowed)\n\t\tp->sched_class->set_cpus_allowed(p, new_mask);\n\telse {\n\t\tcpumask_copy(&p->cpus_allowed, new_mask);\n\t\tp->rt.nr_cpus_allowed = cpumask_weight(new_mask);\n\t}\n\n\t/* Can the task run on the task's current CPU? If so, we're done */\n\tif (cpumask_test_cpu(task_cpu(p), new_mask))\n\t\tgoto out;\n\n\tdest_cpu = cpumask_any_and(cpu_active_mask, new_mask);\n\tif (migrate_task(p, dest_cpu)) {\n\t\tstruct migration_arg arg = { p, dest_cpu };\n\t\t/* Need help from migration thread: drop lock and wait. */\n\t\ttask_rq_unlock(rq, &flags);\n\t\tstop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);\n\t\ttlb_migrate_finish(p->mm);\n\t\treturn 0;\n\t}\nout:\n\ttask_rq_unlock(rq, &flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);\n\n/*\n * Move (not current) task off this cpu, onto dest cpu. We're doing\n * this because either it can't run here any more (set_cpus_allowed()\n * away from this CPU, or CPU going down), or because we're\n * attempting to rebalance this task on exec (sched_exec).\n *\n * So we race with normal scheduler movements, but that's OK, as long\n * as the task is no longer on this CPU.\n *\n * Returns non-zero if task was successfully migrated.\n */\nstatic int __migrate_task(struct task_struct *p, int src_cpu, int dest_cpu)\n{\n\tstruct rq *rq_dest, *rq_src;\n\tint ret = 0;\n\n\tif (unlikely(!cpu_active(dest_cpu)))\n\t\treturn ret;\n\n\trq_src = cpu_rq(src_cpu);\n\trq_dest = cpu_rq(dest_cpu);\n\n\tdouble_rq_lock(rq_src, rq_dest);\n\t/* Already moved. */\n\tif (task_cpu(p) != src_cpu)\n\t\tgoto done;\n\t/* Affinity changed (again). */\n\tif (!cpumask_test_cpu(dest_cpu, &p->cpus_allowed))\n\t\tgoto fail;\n\n\t/*\n\t * If we're not on a rq, the next wake-up will ensure we're\n\t * placed properly.\n\t */\n\tif (p->se.on_rq) {\n\t\tdeactivate_task(rq_src, p, 0);\n\t\tset_task_cpu(p, dest_cpu);\n\t\tactivate_task(rq_dest, p, 0);\n\t\tcheck_preempt_curr(rq_dest, p, 0);\n\t}\ndone:\n\tret = 1;\nfail:\n\tdouble_rq_unlock(rq_src, rq_dest);\n\treturn ret;\n}\n\n/*\n * migration_cpu_stop - this will be executed by a highprio stopper thread\n * and performs thread migration by bumping thread off CPU then\n * 'pushing' onto another runqueue.\n */\nstatic int migration_cpu_stop(void *data)\n{\n\tstruct migration_arg *arg = data;\n\n\t/*\n\t * The original target cpu might have gone down and we might\n\t * be on another cpu but it doesn't matter.\n\t */\n\tlocal_irq_disable();\n\t__migrate_task(arg->task, raw_smp_processor_id(), arg->dest_cpu);\n\tlocal_irq_enable();\n\treturn 0;\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n/*\n * Figure out where task on dead CPU should go, use force if necessary.\n */\nvoid move_task_off_dead_cpu(int dead_cpu, struct task_struct *p)\n{\n\tstruct rq *rq = cpu_rq(dead_cpu);\n\tint needs_cpu, uninitialized_var(dest_cpu);\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\n\traw_spin_lock(&rq->lock);\n\tneeds_cpu = (task_cpu(p) == dead_cpu) && (p->state != TASK_WAKING);\n\tif (needs_cpu)\n\t\tdest_cpu = select_fallback_rq(dead_cpu, p);\n\traw_spin_unlock(&rq->lock);\n\t/*\n\t * It can only fail if we race with set_cpus_allowed(),\n\t * in the racer should migrate the task anyway.\n\t */\n\tif (needs_cpu)\n\t\t__migrate_task(p, dead_cpu, dest_cpu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * While a dead CPU has no uninterruptible tasks queued at this point,\n * it might still have a nonzero ->nr_uninterruptible counter, because\n * for performance reasons the counter is not stricly tracking tasks to\n * their home CPUs. So we just add the counter to another CPU's counter,\n * to keep the global sum constant after CPU-down:\n */\nstatic void migrate_nr_uninterruptible(struct rq *rq_src)\n{\n\tstruct rq *rq_dest = cpu_rq(cpumask_any(cpu_active_mask));\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tdouble_rq_lock(rq_src, rq_dest);\n\trq_dest->nr_uninterruptible += rq_src->nr_uninterruptible;\n\trq_src->nr_uninterruptible = 0;\n\tdouble_rq_unlock(rq_src, rq_dest);\n\tlocal_irq_restore(flags);\n}\n\n/* Run through task list and migrate tasks from the dead cpu. */\nstatic void migrate_live_tasks(int src_cpu)\n{\n\tstruct task_struct *p, *t;\n\n\tread_lock(&tasklist_lock);\n\n\tdo_each_thread(t, p) {\n\t\tif (p == current)\n\t\t\tcontinue;\n\n\t\tif (task_cpu(p) == src_cpu)\n\t\t\tmove_task_off_dead_cpu(src_cpu, p);\n\t} while_each_thread(t, p);\n\n\tread_unlock(&tasklist_lock);\n}\n\n/*\n * Schedules idle task to be the next runnable task on current CPU.\n * It does so by boosting its priority to highest possible.\n * Used by CPU offline code.\n */\nvoid sched_idle_next(void)\n{\n\tint this_cpu = smp_processor_id();\n\tstruct rq *rq = cpu_rq(this_cpu);\n\tstruct task_struct *p = rq->idle;\n\tunsigned long flags;\n\n\t/* cpu has to be offline */\n\tBUG_ON(cpu_online(this_cpu));\n\n\t/*\n\t * Strictly not necessary since rest of the CPUs are stopped by now\n\t * and interrupts disabled on the current cpu.\n\t */\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\n\t__setscheduler(rq, p, SCHED_FIFO, MAX_RT_PRIO-1);\n\n\tactivate_task(rq, p, 0);\n\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n}\n\n/*\n * Ensures that the idle task is using init_mm right before its cpu goes\n * offline.\n */\nvoid idle_task_exit(void)\n{\n\tstruct mm_struct *mm = current->active_mm;\n\n\tBUG_ON(cpu_online(smp_processor_id()));\n\n\tif (mm != &init_mm)\n\t\tswitch_mm(mm, &init_mm, current);\n\tmmdrop(mm);\n}\n\n/* called under rq->lock with disabled interrupts */\nstatic void migrate_dead(unsigned int dead_cpu, struct task_struct *p)\n{\n\tstruct rq *rq = cpu_rq(dead_cpu);\n\n\t/* Must be exiting, otherwise would be on tasklist. */\n\tBUG_ON(!p->exit_state);\n\n\t/* Cannot have done final schedule yet: would have vanished. */\n\tBUG_ON(p->state == TASK_DEAD);\n\n\tget_task_struct(p);\n\n\t/*\n\t * Drop lock around migration; if someone else moves it,\n\t * that's OK. No task can be added to this CPU, so iteration is\n\t * fine.\n\t */\n\traw_spin_unlock_irq(&rq->lock);\n\tmove_task_off_dead_cpu(dead_cpu, p);\n\traw_spin_lock_irq(&rq->lock);\n\n\tput_task_struct(p);\n}\n\n/* release_task() removes task from tasklist, so we won't find dead tasks. */\nstatic void migrate_dead_tasks(unsigned int dead_cpu)\n{\n\tstruct rq *rq = cpu_rq(dead_cpu);\n\tstruct task_struct *next;\n\n\tfor ( ; ; ) {\n\t\tif (!rq->nr_running)\n\t\t\tbreak;\n\t\tnext = pick_next_task(rq);\n\t\tif (!next)\n\t\t\tbreak;\n\t\tnext->sched_class->put_prev_task(rq, next);\n\t\tmigrate_dead(dead_cpu, next);\n\n\t}\n}\n\n/*\n * remove the tasks which were accounted by rq from calc_load_tasks.\n */\nstatic void calc_global_load_remove(struct rq *rq)\n{\n\tatomic_long_sub(rq->calc_load_active, &calc_load_tasks);\n\trq->calc_load_active = 0;\n}\n#endif /* CONFIG_HOTPLUG_CPU */\n\n#if defined(CONFIG_SCHED_DEBUG) && defined(CONFIG_SYSCTL)\n\nstatic struct ctl_table sd_ctl_dir[] = {\n\t{\n\t\t.procname\t= \"sched_domain\",\n\t\t.mode\t\t= 0555,\n\t},\n\t{}\n};\n\nstatic struct ctl_table sd_ctl_root[] = {\n\t{\n\t\t.procname\t= \"kernel\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= sd_ctl_dir,\n\t},\n\t{}\n};\n\nstatic struct ctl_table *sd_alloc_ctl_entry(int n)\n{\n\tstruct ctl_table *entry =\n\t\tkcalloc(n, sizeof(struct ctl_table), GFP_KERNEL);\n\n\treturn entry;\n}\n\nstatic void sd_free_ctl_entry(struct ctl_table **tablep)\n{\n\tstruct ctl_table *entry;\n\n\t/*\n\t * In the intermediate directories, both the child directory and\n\t * procname are dynamically allocated and could fail but the mode\n\t * will always be set. In the lowest directory the names are\n\t * static strings and all have proc handlers.\n\t */\n\tfor (entry = *tablep; entry->mode; entry++) {\n\t\tif (entry->child)\n\t\t\tsd_free_ctl_entry(&entry->child);\n\t\tif (entry->proc_handler == NULL)\n\t\t\tkfree(entry->procname);\n\t}\n\n\tkfree(*tablep);\n\t*tablep = NULL;\n}\n\nstatic void\nset_table_entry(struct ctl_table *entry,\n\t\tconst char *procname, void *data, int maxlen,\n\t\tmode_t mode, proc_handler *proc_handler)\n{\n\tentry->procname = procname;\n\tentry->data = data;\n\tentry->maxlen = maxlen;\n\tentry->mode = mode;\n\tentry->proc_handler = proc_handler;\n}\n\nstatic struct ctl_table *\nsd_alloc_ctl_domain_table(struct sched_domain *sd)\n{\n\tstruct ctl_table *table = sd_alloc_ctl_entry(13);\n\n\tif (table == NULL)\n\t\treturn NULL;\n\n\tset_table_entry(&table[0], \"min_interval\", &sd->min_interval,\n\t\tsizeof(long), 0644, proc_doulongvec_minmax);\n\tset_table_entry(&table[1], \"max_interval\", &sd->max_interval,\n\t\tsizeof(long), 0644, proc_doulongvec_minmax);\n\tset_table_entry(&table[2], \"busy_idx\", &sd->busy_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[3], \"idle_idx\", &sd->idle_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[4], \"newidle_idx\", &sd->newidle_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[5], \"wake_idx\", &sd->wake_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[6], \"forkexec_idx\", &sd->forkexec_idx,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[7], \"busy_factor\", &sd->busy_factor,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[8], \"imbalance_pct\", &sd->imbalance_pct,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[9], \"cache_nice_tries\",\n\t\t&sd->cache_nice_tries,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[10], \"flags\", &sd->flags,\n\t\tsizeof(int), 0644, proc_dointvec_minmax);\n\tset_table_entry(&table[11], \"name\", sd->name,\n\t\tCORENAME_MAX_SIZE, 0444, proc_dostring);\n\t/* &table[12] is terminator */\n\n\treturn table;\n}\n\nstatic ctl_table *sd_alloc_ctl_cpu_table(int cpu)\n{\n\tstruct ctl_table *entry, *table;\n\tstruct sched_domain *sd;\n\tint domain_num = 0, i;\n\tchar buf[32];\n\n\tfor_each_domain(cpu, sd)\n\t\tdomain_num++;\n\tentry = table = sd_alloc_ctl_entry(domain_num + 1);\n\tif (table == NULL)\n\t\treturn NULL;\n\n\ti = 0;\n\tfor_each_domain(cpu, sd) {\n\t\tsnprintf(buf, 32, \"domain%d\", i);\n\t\tentry->procname = kstrdup(buf, GFP_KERNEL);\n\t\tentry->mode = 0555;\n\t\tentry->child = sd_alloc_ctl_domain_table(sd);\n\t\tentry++;\n\t\ti++;\n\t}\n\treturn table;\n}\n\nstatic struct ctl_table_header *sd_sysctl_header;\nstatic void register_sched_domain_sysctl(void)\n{\n\tint i, cpu_num = num_possible_cpus();\n\tstruct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);\n\tchar buf[32];\n\n\tWARN_ON(sd_ctl_dir[0].child);\n\tsd_ctl_dir[0].child = entry;\n\n\tif (entry == NULL)\n\t\treturn;\n\n\tfor_each_possible_cpu(i) {\n\t\tsnprintf(buf, 32, \"cpu%d\", i);\n\t\tentry->procname = kstrdup(buf, GFP_KERNEL);\n\t\tentry->mode = 0555;\n\t\tentry->child = sd_alloc_ctl_cpu_table(i);\n\t\tentry++;\n\t}\n\n\tWARN_ON(sd_sysctl_header);\n\tsd_sysctl_header = register_sysctl_table(sd_ctl_root);\n}\n\n/* may be called multiple times per register */\nstatic void unregister_sched_domain_sysctl(void)\n{\n\tif (sd_sysctl_header)\n\t\tunregister_sysctl_table(sd_sysctl_header);\n\tsd_sysctl_header = NULL;\n\tif (sd_ctl_dir[0].child)\n\t\tsd_free_ctl_entry(&sd_ctl_dir[0].child);\n}\n#else\nstatic void register_sched_domain_sysctl(void)\n{\n}\nstatic void unregister_sched_domain_sysctl(void)\n{\n}\n#endif\n\nstatic void set_rq_online(struct rq *rq)\n{\n\tif (!rq->online) {\n\t\tconst struct sched_class *class;\n\n\t\tcpumask_set_cpu(rq->cpu, rq->rd->online);\n\t\trq->online = 1;\n\n\t\tfor_each_class(class) {\n\t\t\tif (class->rq_online)\n\t\t\t\tclass->rq_online(rq);\n\t\t}\n\t}\n}\n\nstatic void set_rq_offline(struct rq *rq)\n{\n\tif (rq->online) {\n\t\tconst struct sched_class *class;\n\n\t\tfor_each_class(class) {\n\t\t\tif (class->rq_offline)\n\t\t\t\tclass->rq_offline(rq);\n\t\t}\n\n\t\tcpumask_clear_cpu(rq->cpu, rq->rd->online);\n\t\trq->online = 0;\n\t}\n}\n\n/*\n * migration_call - callback that gets triggered when a CPU is added.\n * Here we can start up the necessary migration thread for the new CPU.\n */\nstatic int __cpuinit\nmigration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)\n{\n\tint cpu = (long)hcpu;\n\tunsigned long flags;\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tswitch (action) {\n\n\tcase CPU_UP_PREPARE:\n\tcase CPU_UP_PREPARE_FROZEN:\n\t\trq->calc_load_update = calc_load_update;\n\t\tbreak;\n\n\tcase CPU_ONLINE:\n\tcase CPU_ONLINE_FROZEN:\n\t\t/* Update our root-domain */\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->rd) {\n\t\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));\n\n\t\t\tset_rq_online(rq);\n\t\t}\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t\tbreak;\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tcase CPU_DEAD:\n\tcase CPU_DEAD_FROZEN:\n\t\tmigrate_live_tasks(cpu);\n\t\t/* Idle task back to normal (off runqueue, low prio) */\n\t\traw_spin_lock_irq(&rq->lock);\n\t\tdeactivate_task(rq, rq->idle, 0);\n\t\t__setscheduler(rq, rq->idle, SCHED_NORMAL, 0);\n\t\trq->idle->sched_class = &idle_sched_class;\n\t\tmigrate_dead_tasks(cpu);\n\t\traw_spin_unlock_irq(&rq->lock);\n\t\tmigrate_nr_uninterruptible(rq);\n\t\tBUG_ON(rq->nr_running != 0);\n\t\tcalc_global_load_remove(rq);\n\t\tbreak;\n\n\tcase CPU_DYING:\n\tcase CPU_DYING_FROZEN:\n\t\t/* Update our root-domain */\n\t\traw_spin_lock_irqsave(&rq->lock, flags);\n\t\tif (rq->rd) {\n\t\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));\n\t\t\tset_rq_offline(rq);\n\t\t}\n\t\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\t\tbreak;\n#endif\n\t}\n\treturn NOTIFY_OK;\n}\n\n/*\n * Register at high priority so that task migration (migrate_all_tasks)\n * happens before everything else.  This has to be lower priority than\n * the notifier in the perf_event subsystem, though.\n */\nstatic struct notifier_block __cpuinitdata migration_notifier = {\n\t.notifier_call = migration_call,\n\t.priority = CPU_PRI_MIGRATION,\n};\n\nstatic int __cpuinit sched_cpu_active(struct notifier_block *nfb,\n\t\t\t\t      unsigned long action, void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_ONLINE:\n\tcase CPU_DOWN_FAILED:\n\t\tset_cpu_active((long)hcpu, true);\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int __cpuinit sched_cpu_inactive(struct notifier_block *nfb,\n\t\t\t\t\tunsigned long action, void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_DOWN_PREPARE:\n\t\tset_cpu_active((long)hcpu, false);\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int __init migration_init(void)\n{\n\tvoid *cpu = (void *)(long)smp_processor_id();\n\tint err;\n\n\t/* Initialize migration for the boot CPU */\n\terr = migration_call(&migration_notifier, CPU_UP_PREPARE, cpu);\n\tBUG_ON(err == NOTIFY_BAD);\n\tmigration_call(&migration_notifier, CPU_ONLINE, cpu);\n\tregister_cpu_notifier(&migration_notifier);\n\n\t/* Register cpu active notifiers */\n\tcpu_notifier(sched_cpu_active, CPU_PRI_SCHED_ACTIVE);\n\tcpu_notifier(sched_cpu_inactive, CPU_PRI_SCHED_INACTIVE);\n\n\treturn 0;\n}\nearly_initcall(migration_init);\n#endif\n\n#ifdef CONFIG_SMP\n\n#ifdef CONFIG_SCHED_DEBUG\n\nstatic __read_mostly int sched_domain_debug_enabled;\n\nstatic int __init sched_domain_debug_setup(char *str)\n{\n\tsched_domain_debug_enabled = 1;\n\n\treturn 0;\n}\nearly_param(\"sched_debug\", sched_domain_debug_setup);\n\nstatic int sched_domain_debug_one(struct sched_domain *sd, int cpu, int level,\n\t\t\t\t  struct cpumask *groupmask)\n{\n\tstruct sched_group *group = sd->groups;\n\tchar str[256];\n\n\tcpulist_scnprintf(str, sizeof(str), sched_domain_span(sd));\n\tcpumask_clear(groupmask);\n\n\tprintk(KERN_DEBUG \"%*s domain %d: \", level, \"\", level);\n\n\tif (!(sd->flags & SD_LOAD_BALANCE)) {\n\t\tprintk(\"does not load-balance\\n\");\n\t\tif (sd->parent)\n\t\t\tprintk(KERN_ERR \"ERROR: !SD_LOAD_BALANCE domain\"\n\t\t\t\t\t\" has parent\");\n\t\treturn -1;\n\t}\n\n\tprintk(KERN_CONT \"span %s level %s\\n\", str, sd->name);\n\n\tif (!cpumask_test_cpu(cpu, sched_domain_span(sd))) {\n\t\tprintk(KERN_ERR \"ERROR: domain->span does not contain \"\n\t\t\t\t\"CPU%d\\n\", cpu);\n\t}\n\tif (!cpumask_test_cpu(cpu, sched_group_cpus(group))) {\n\t\tprintk(KERN_ERR \"ERROR: domain->groups does not contain\"\n\t\t\t\t\" CPU%d\\n\", cpu);\n\t}\n\n\tprintk(KERN_DEBUG \"%*s groups:\", level + 1, \"\");\n\tdo {\n\t\tif (!group) {\n\t\t\tprintk(\"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: group is NULL\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!group->cpu_power) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: domain->cpu_power not \"\n\t\t\t\t\t\"set\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!cpumask_weight(sched_group_cpus(group))) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: empty group\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cpumask_intersects(groupmask, sched_group_cpus(group))) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: repeated CPUs\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tcpumask_or(groupmask, groupmask, sched_group_cpus(group));\n\n\t\tcpulist_scnprintf(str, sizeof(str), sched_group_cpus(group));\n\n\t\tprintk(KERN_CONT \" %s\", str);\n\t\tif (group->cpu_power != SCHED_LOAD_SCALE) {\n\t\t\tprintk(KERN_CONT \" (cpu_power = %d)\",\n\t\t\t\tgroup->cpu_power);\n\t\t}\n\n\t\tgroup = group->next;\n\t} while (group != sd->groups);\n\tprintk(KERN_CONT \"\\n\");\n\n\tif (!cpumask_equal(sched_domain_span(sd), groupmask))\n\t\tprintk(KERN_ERR \"ERROR: groups don't span domain->span\\n\");\n\n\tif (sd->parent &&\n\t    !cpumask_subset(groupmask, sched_domain_span(sd->parent)))\n\t\tprintk(KERN_ERR \"ERROR: parent span is not a superset \"\n\t\t\t\"of domain->span\\n\");\n\treturn 0;\n}\n\nstatic void sched_domain_debug(struct sched_domain *sd, int cpu)\n{\n\tcpumask_var_t groupmask;\n\tint level = 0;\n\n\tif (!sched_domain_debug_enabled)\n\t\treturn;\n\n\tif (!sd) {\n\t\tprintk(KERN_DEBUG \"CPU%d attaching NULL sched-domain.\\n\", cpu);\n\t\treturn;\n\t}\n\n\tprintk(KERN_DEBUG \"CPU%d attaching sched-domain:\\n\", cpu);\n\n\tif (!alloc_cpumask_var(&groupmask, GFP_KERNEL)) {\n\t\tprintk(KERN_DEBUG \"Cannot load-balance (out of memory)\\n\");\n\t\treturn;\n\t}\n\n\tfor (;;) {\n\t\tif (sched_domain_debug_one(sd, cpu, level, groupmask))\n\t\t\tbreak;\n\t\tlevel++;\n\t\tsd = sd->parent;\n\t\tif (!sd)\n\t\t\tbreak;\n\t}\n\tfree_cpumask_var(groupmask);\n}\n#else /* !CONFIG_SCHED_DEBUG */\n# define sched_domain_debug(sd, cpu) do { } while (0)\n#endif /* CONFIG_SCHED_DEBUG */\n\nstatic int sd_degenerate(struct sched_domain *sd)\n{\n\tif (cpumask_weight(sched_domain_span(sd)) == 1)\n\t\treturn 1;\n\n\t/* Following flags need at least 2 groups */\n\tif (sd->flags & (SD_LOAD_BALANCE |\n\t\t\t SD_BALANCE_NEWIDLE |\n\t\t\t SD_BALANCE_FORK |\n\t\t\t SD_BALANCE_EXEC |\n\t\t\t SD_SHARE_CPUPOWER |\n\t\t\t SD_SHARE_PKG_RESOURCES)) {\n\t\tif (sd->groups != sd->groups->next)\n\t\t\treturn 0;\n\t}\n\n\t/* Following flags don't use groups */\n\tif (sd->flags & (SD_WAKE_AFFINE))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int\nsd_parent_degenerate(struct sched_domain *sd, struct sched_domain *parent)\n{\n\tunsigned long cflags = sd->flags, pflags = parent->flags;\n\n\tif (sd_degenerate(parent))\n\t\treturn 1;\n\n\tif (!cpumask_equal(sched_domain_span(sd), sched_domain_span(parent)))\n\t\treturn 0;\n\n\t/* Flags needing groups don't count if only 1 group in parent */\n\tif (parent->groups == parent->groups->next) {\n\t\tpflags &= ~(SD_LOAD_BALANCE |\n\t\t\t\tSD_BALANCE_NEWIDLE |\n\t\t\t\tSD_BALANCE_FORK |\n\t\t\t\tSD_BALANCE_EXEC |\n\t\t\t\tSD_SHARE_CPUPOWER |\n\t\t\t\tSD_SHARE_PKG_RESOURCES);\n\t\tif (nr_node_ids == 1)\n\t\t\tpflags &= ~SD_SERIALIZE;\n\t}\n\tif (~cflags & pflags)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic void free_rootdomain(struct root_domain *rd)\n{\n\tsynchronize_sched();\n\n\tcpupri_cleanup(&rd->cpupri);\n\n\tfree_cpumask_var(rd->rto_mask);\n\tfree_cpumask_var(rd->online);\n\tfree_cpumask_var(rd->span);\n\tkfree(rd);\n}\n\nstatic void rq_attach_root(struct rq *rq, struct root_domain *rd)\n{\n\tstruct root_domain *old_rd = NULL;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\n\tif (rq->rd) {\n\t\told_rd = rq->rd;\n\n\t\tif (cpumask_test_cpu(rq->cpu, old_rd->online))\n\t\t\tset_rq_offline(rq);\n\n\t\tcpumask_clear_cpu(rq->cpu, old_rd->span);\n\n\t\t/*\n\t\t * If we dont want to free the old_rt yet then\n\t\t * set old_rd to NULL to skip the freeing later\n\t\t * in this function:\n\t\t */\n\t\tif (!atomic_dec_and_test(&old_rd->refcount))\n\t\t\told_rd = NULL;\n\t}\n\n\tatomic_inc(&rd->refcount);\n\trq->rd = rd;\n\n\tcpumask_set_cpu(rq->cpu, rd->span);\n\tif (cpumask_test_cpu(rq->cpu, cpu_active_mask))\n\t\tset_rq_online(rq);\n\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n\n\tif (old_rd)\n\t\tfree_rootdomain(old_rd);\n}\n\nstatic int init_rootdomain(struct root_domain *rd)\n{\n\tmemset(rd, 0, sizeof(*rd));\n\n\tif (!alloc_cpumask_var(&rd->span, GFP_KERNEL))\n\t\tgoto out;\n\tif (!alloc_cpumask_var(&rd->online, GFP_KERNEL))\n\t\tgoto free_span;\n\tif (!alloc_cpumask_var(&rd->rto_mask, GFP_KERNEL))\n\t\tgoto free_online;\n\n\tif (cpupri_init(&rd->cpupri) != 0)\n\t\tgoto free_rto_mask;\n\treturn 0;\n\nfree_rto_mask:\n\tfree_cpumask_var(rd->rto_mask);\nfree_online:\n\tfree_cpumask_var(rd->online);\nfree_span:\n\tfree_cpumask_var(rd->span);\nout:\n\treturn -ENOMEM;\n}\n\nstatic void init_defrootdomain(void)\n{\n\tinit_rootdomain(&def_root_domain);\n\n\tatomic_set(&def_root_domain.refcount, 1);\n}\n\nstatic struct root_domain *alloc_rootdomain(void)\n{\n\tstruct root_domain *rd;\n\n\trd = kmalloc(sizeof(*rd), GFP_KERNEL);\n\tif (!rd)\n\t\treturn NULL;\n\n\tif (init_rootdomain(rd) != 0) {\n\t\tkfree(rd);\n\t\treturn NULL;\n\t}\n\n\treturn rd;\n}\n\n/*\n * Attach the domain 'sd' to 'cpu' as its base domain. Callers must\n * hold the hotplug lock.\n */\nstatic void\ncpu_attach_domain(struct sched_domain *sd, struct root_domain *rd, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct sched_domain *tmp;\n\n\tfor (tmp = sd; tmp; tmp = tmp->parent)\n\t\ttmp->span_weight = cpumask_weight(sched_domain_span(tmp));\n\n\t/* Remove the sched domains which do not contribute to scheduling. */\n\tfor (tmp = sd; tmp; ) {\n\t\tstruct sched_domain *parent = tmp->parent;\n\t\tif (!parent)\n\t\t\tbreak;\n\n\t\tif (sd_parent_degenerate(tmp, parent)) {\n\t\t\ttmp->parent = parent->parent;\n\t\t\tif (parent->parent)\n\t\t\t\tparent->parent->child = tmp;\n\t\t} else\n\t\t\ttmp = tmp->parent;\n\t}\n\n\tif (sd && sd_degenerate(sd)) {\n\t\tsd = sd->parent;\n\t\tif (sd)\n\t\t\tsd->child = NULL;\n\t}\n\n\tsched_domain_debug(sd, cpu);\n\n\trq_attach_root(rq, rd);\n\trcu_assign_pointer(rq->sd, sd);\n}\n\n/* cpus with isolated domains */\nstatic cpumask_var_t cpu_isolated_map;\n\n/* Setup the mask of cpus configured for isolated domains */\nstatic int __init isolated_cpu_setup(char *str)\n{\n\talloc_bootmem_cpumask_var(&cpu_isolated_map);\n\tcpulist_parse(str, cpu_isolated_map);\n\treturn 1;\n}\n\n__setup(\"isolcpus=\", isolated_cpu_setup);\n\n/*\n * init_sched_build_groups takes the cpumask we wish to span, and a pointer\n * to a function which identifies what group(along with sched group) a CPU\n * belongs to. The return value of group_fn must be a >= 0 and < nr_cpu_ids\n * (due to the fact that we keep track of groups covered with a struct cpumask).\n *\n * init_sched_build_groups will build a circular linked list of the groups\n * covered by the given span, and will set each group's ->cpumask correctly,\n * and ->cpu_power to 0.\n */\nstatic void\ninit_sched_build_groups(const struct cpumask *span,\n\t\t\tconst struct cpumask *cpu_map,\n\t\t\tint (*group_fn)(int cpu, const struct cpumask *cpu_map,\n\t\t\t\t\tstruct sched_group **sg,\n\t\t\t\t\tstruct cpumask *tmpmask),\n\t\t\tstruct cpumask *covered, struct cpumask *tmpmask)\n{\n\tstruct sched_group *first = NULL, *last = NULL;\n\tint i;\n\n\tcpumask_clear(covered);\n\n\tfor_each_cpu(i, span) {\n\t\tstruct sched_group *sg;\n\t\tint group = group_fn(i, cpu_map, &sg, tmpmask);\n\t\tint j;\n\n\t\tif (cpumask_test_cpu(i, covered))\n\t\t\tcontinue;\n\n\t\tcpumask_clear(sched_group_cpus(sg));\n\t\tsg->cpu_power = 0;\n\n\t\tfor_each_cpu(j, span) {\n\t\t\tif (group_fn(j, cpu_map, NULL, tmpmask) != group)\n\t\t\t\tcontinue;\n\n\t\t\tcpumask_set_cpu(j, covered);\n\t\t\tcpumask_set_cpu(j, sched_group_cpus(sg));\n\t\t}\n\t\tif (!first)\n\t\t\tfirst = sg;\n\t\tif (last)\n\t\t\tlast->next = sg;\n\t\tlast = sg;\n\t}\n\tlast->next = first;\n}\n\n#define SD_NODES_PER_DOMAIN 16\n\n#ifdef CONFIG_NUMA\n\n/**\n * find_next_best_node - find the next node to include in a sched_domain\n * @node: node whose sched_domain we're building\n * @used_nodes: nodes already in the sched_domain\n *\n * Find the next node to include in a given scheduling domain. Simply\n * finds the closest node not already in the @used_nodes map.\n *\n * Should use nodemask_t.\n */\nstatic int find_next_best_node(int node, nodemask_t *used_nodes)\n{\n\tint i, n, val, min_val, best_node = 0;\n\n\tmin_val = INT_MAX;\n\n\tfor (i = 0; i < nr_node_ids; i++) {\n\t\t/* Start at @node */\n\t\tn = (node + i) % nr_node_ids;\n\n\t\tif (!nr_cpus_node(n))\n\t\t\tcontinue;\n\n\t\t/* Skip already used nodes */\n\t\tif (node_isset(n, *used_nodes))\n\t\t\tcontinue;\n\n\t\t/* Simple min distance search */\n\t\tval = node_distance(node, n);\n\n\t\tif (val < min_val) {\n\t\t\tmin_val = val;\n\t\t\tbest_node = n;\n\t\t}\n\t}\n\n\tnode_set(best_node, *used_nodes);\n\treturn best_node;\n}\n\n/**\n * sched_domain_node_span - get a cpumask for a node's sched_domain\n * @node: node whose cpumask we're constructing\n * @span: resulting cpumask\n *\n * Given a node, construct a good cpumask for its sched_domain to span. It\n * should be one that prevents unnecessary balancing, but also spreads tasks\n * out optimally.\n */\nstatic void sched_domain_node_span(int node, struct cpumask *span)\n{\n\tnodemask_t used_nodes;\n\tint i;\n\n\tcpumask_clear(span);\n\tnodes_clear(used_nodes);\n\n\tcpumask_or(span, span, cpumask_of_node(node));\n\tnode_set(node, used_nodes);\n\n\tfor (i = 1; i < SD_NODES_PER_DOMAIN; i++) {\n\t\tint next_node = find_next_best_node(node, &used_nodes);\n\n\t\tcpumask_or(span, span, cpumask_of_node(next_node));\n\t}\n}\n#endif /* CONFIG_NUMA */\n\nint sched_smt_power_savings = 0, sched_mc_power_savings = 0;\n\n/*\n * The cpus mask in sched_group and sched_domain hangs off the end.\n *\n * ( See the the comments in include/linux/sched.h:struct sched_group\n *   and struct sched_domain. )\n */\nstruct static_sched_group {\n\tstruct sched_group sg;\n\tDECLARE_BITMAP(cpus, CONFIG_NR_CPUS);\n};\n\nstruct static_sched_domain {\n\tstruct sched_domain sd;\n\tDECLARE_BITMAP(span, CONFIG_NR_CPUS);\n};\n\nstruct s_data {\n#ifdef CONFIG_NUMA\n\tint\t\t\tsd_allnodes;\n\tcpumask_var_t\t\tdomainspan;\n\tcpumask_var_t\t\tcovered;\n\tcpumask_var_t\t\tnotcovered;\n#endif\n\tcpumask_var_t\t\tnodemask;\n\tcpumask_var_t\t\tthis_sibling_map;\n\tcpumask_var_t\t\tthis_core_map;\n\tcpumask_var_t\t\tthis_book_map;\n\tcpumask_var_t\t\tsend_covered;\n\tcpumask_var_t\t\ttmpmask;\n\tstruct sched_group\t**sched_group_nodes;\n\tstruct root_domain\t*rd;\n};\n\nenum s_alloc {\n\tsa_sched_groups = 0,\n\tsa_rootdomain,\n\tsa_tmpmask,\n\tsa_send_covered,\n\tsa_this_book_map,\n\tsa_this_core_map,\n\tsa_this_sibling_map,\n\tsa_nodemask,\n\tsa_sched_group_nodes,\n#ifdef CONFIG_NUMA\n\tsa_notcovered,\n\tsa_covered,\n\tsa_domainspan,\n#endif\n\tsa_none,\n};\n\n/*\n * SMT sched-domains:\n */\n#ifdef CONFIG_SCHED_SMT\nstatic DEFINE_PER_CPU(struct static_sched_domain, cpu_domains);\nstatic DEFINE_PER_CPU(struct static_sched_group, sched_groups);\n\nstatic int\ncpu_to_cpu_group(int cpu, const struct cpumask *cpu_map,\n\t\t struct sched_group **sg, struct cpumask *unused)\n{\n\tif (sg)\n\t\t*sg = &per_cpu(sched_groups, cpu).sg;\n\treturn cpu;\n}\n#endif /* CONFIG_SCHED_SMT */\n\n/*\n * multi-core sched-domains:\n */\n#ifdef CONFIG_SCHED_MC\nstatic DEFINE_PER_CPU(struct static_sched_domain, core_domains);\nstatic DEFINE_PER_CPU(struct static_sched_group, sched_group_core);\n\nstatic int\ncpu_to_core_group(int cpu, const struct cpumask *cpu_map,\n\t\t  struct sched_group **sg, struct cpumask *mask)\n{\n\tint group;\n#ifdef CONFIG_SCHED_SMT\n\tcpumask_and(mask, topology_thread_cpumask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#else\n\tgroup = cpu;\n#endif\n\tif (sg)\n\t\t*sg = &per_cpu(sched_group_core, group).sg;\n\treturn group;\n}\n#endif /* CONFIG_SCHED_MC */\n\n/*\n * book sched-domains:\n */\n#ifdef CONFIG_SCHED_BOOK\nstatic DEFINE_PER_CPU(struct static_sched_domain, book_domains);\nstatic DEFINE_PER_CPU(struct static_sched_group, sched_group_book);\n\nstatic int\ncpu_to_book_group(int cpu, const struct cpumask *cpu_map,\n\t\t  struct sched_group **sg, struct cpumask *mask)\n{\n\tint group = cpu;\n#ifdef CONFIG_SCHED_MC\n\tcpumask_and(mask, cpu_coregroup_mask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#elif defined(CONFIG_SCHED_SMT)\n\tcpumask_and(mask, topology_thread_cpumask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#endif\n\tif (sg)\n\t\t*sg = &per_cpu(sched_group_book, group).sg;\n\treturn group;\n}\n#endif /* CONFIG_SCHED_BOOK */\n\nstatic DEFINE_PER_CPU(struct static_sched_domain, phys_domains);\nstatic DEFINE_PER_CPU(struct static_sched_group, sched_group_phys);\n\nstatic int\ncpu_to_phys_group(int cpu, const struct cpumask *cpu_map,\n\t\t  struct sched_group **sg, struct cpumask *mask)\n{\n\tint group;\n#ifdef CONFIG_SCHED_BOOK\n\tcpumask_and(mask, cpu_book_mask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#elif defined(CONFIG_SCHED_MC)\n\tcpumask_and(mask, cpu_coregroup_mask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#elif defined(CONFIG_SCHED_SMT)\n\tcpumask_and(mask, topology_thread_cpumask(cpu), cpu_map);\n\tgroup = cpumask_first(mask);\n#else\n\tgroup = cpu;\n#endif\n\tif (sg)\n\t\t*sg = &per_cpu(sched_group_phys, group).sg;\n\treturn group;\n}\n\n#ifdef CONFIG_NUMA\n/*\n * The init_sched_build_groups can't handle what we want to do with node\n * groups, so roll our own. Now each node has its own list of groups which\n * gets dynamically allocated.\n */\nstatic DEFINE_PER_CPU(struct static_sched_domain, node_domains);\nstatic struct sched_group ***sched_group_nodes_bycpu;\n\nstatic DEFINE_PER_CPU(struct static_sched_domain, allnodes_domains);\nstatic DEFINE_PER_CPU(struct static_sched_group, sched_group_allnodes);\n\nstatic int cpu_to_allnodes_group(int cpu, const struct cpumask *cpu_map,\n\t\t\t\t struct sched_group **sg,\n\t\t\t\t struct cpumask *nodemask)\n{\n\tint group;\n\n\tcpumask_and(nodemask, cpumask_of_node(cpu_to_node(cpu)), cpu_map);\n\tgroup = cpumask_first(nodemask);\n\n\tif (sg)\n\t\t*sg = &per_cpu(sched_group_allnodes, group).sg;\n\treturn group;\n}\n\nstatic void init_numa_sched_groups_power(struct sched_group *group_head)\n{\n\tstruct sched_group *sg = group_head;\n\tint j;\n\n\tif (!sg)\n\t\treturn;\n\tdo {\n\t\tfor_each_cpu(j, sched_group_cpus(sg)) {\n\t\t\tstruct sched_domain *sd;\n\n\t\t\tsd = &per_cpu(phys_domains, j).sd;\n\t\t\tif (j != group_first_cpu(sd->groups)) {\n\t\t\t\t/*\n\t\t\t\t * Only add \"power\" once for each\n\t\t\t\t * physical package.\n\t\t\t\t */\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tsg->cpu_power += sd->groups->cpu_power;\n\t\t}\n\t\tsg = sg->next;\n\t} while (sg != group_head);\n}\n\nstatic int build_numa_sched_groups(struct s_data *d,\n\t\t\t\t   const struct cpumask *cpu_map, int num)\n{\n\tstruct sched_domain *sd;\n\tstruct sched_group *sg, *prev;\n\tint n, j;\n\n\tcpumask_clear(d->covered);\n\tcpumask_and(d->nodemask, cpumask_of_node(num), cpu_map);\n\tif (cpumask_empty(d->nodemask)) {\n\t\td->sched_group_nodes[num] = NULL;\n\t\tgoto out;\n\t}\n\n\tsched_domain_node_span(num, d->domainspan);\n\tcpumask_and(d->domainspan, d->domainspan, cpu_map);\n\n\tsg = kmalloc_node(sizeof(struct sched_group) + cpumask_size(),\n\t\t\t  GFP_KERNEL, num);\n\tif (!sg) {\n\t\tprintk(KERN_WARNING \"Can not alloc domain group for node %d\\n\",\n\t\t       num);\n\t\treturn -ENOMEM;\n\t}\n\td->sched_group_nodes[num] = sg;\n\n\tfor_each_cpu(j, d->nodemask) {\n\t\tsd = &per_cpu(node_domains, j).sd;\n\t\tsd->groups = sg;\n\t}\n\n\tsg->cpu_power = 0;\n\tcpumask_copy(sched_group_cpus(sg), d->nodemask);\n\tsg->next = sg;\n\tcpumask_or(d->covered, d->covered, d->nodemask);\n\n\tprev = sg;\n\tfor (j = 0; j < nr_node_ids; j++) {\n\t\tn = (num + j) % nr_node_ids;\n\t\tcpumask_complement(d->notcovered, d->covered);\n\t\tcpumask_and(d->tmpmask, d->notcovered, cpu_map);\n\t\tcpumask_and(d->tmpmask, d->tmpmask, d->domainspan);\n\t\tif (cpumask_empty(d->tmpmask))\n\t\t\tbreak;\n\t\tcpumask_and(d->tmpmask, d->tmpmask, cpumask_of_node(n));\n\t\tif (cpumask_empty(d->tmpmask))\n\t\t\tcontinue;\n\t\tsg = kmalloc_node(sizeof(struct sched_group) + cpumask_size(),\n\t\t\t\t  GFP_KERNEL, num);\n\t\tif (!sg) {\n\t\t\tprintk(KERN_WARNING\n\t\t\t       \"Can not alloc domain group for node %d\\n\", j);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tsg->cpu_power = 0;\n\t\tcpumask_copy(sched_group_cpus(sg), d->tmpmask);\n\t\tsg->next = prev->next;\n\t\tcpumask_or(d->covered, d->covered, d->tmpmask);\n\t\tprev->next = sg;\n\t\tprev = sg;\n\t}\nout:\n\treturn 0;\n}\n#endif /* CONFIG_NUMA */\n\n#ifdef CONFIG_NUMA\n/* Free memory allocated for various sched_group structures */\nstatic void free_sched_groups(const struct cpumask *cpu_map,\n\t\t\t      struct cpumask *nodemask)\n{\n\tint cpu, i;\n\n\tfor_each_cpu(cpu, cpu_map) {\n\t\tstruct sched_group **sched_group_nodes\n\t\t\t= sched_group_nodes_bycpu[cpu];\n\n\t\tif (!sched_group_nodes)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < nr_node_ids; i++) {\n\t\t\tstruct sched_group *oldsg, *sg = sched_group_nodes[i];\n\n\t\t\tcpumask_and(nodemask, cpumask_of_node(i), cpu_map);\n\t\t\tif (cpumask_empty(nodemask))\n\t\t\t\tcontinue;\n\n\t\t\tif (sg == NULL)\n\t\t\t\tcontinue;\n\t\t\tsg = sg->next;\nnext_sg:\n\t\t\toldsg = sg;\n\t\t\tsg = sg->next;\n\t\t\tkfree(oldsg);\n\t\t\tif (oldsg != sched_group_nodes[i])\n\t\t\t\tgoto next_sg;\n\t\t}\n\t\tkfree(sched_group_nodes);\n\t\tsched_group_nodes_bycpu[cpu] = NULL;\n\t}\n}\n#else /* !CONFIG_NUMA */\nstatic void free_sched_groups(const struct cpumask *cpu_map,\n\t\t\t      struct cpumask *nodemask)\n{\n}\n#endif /* CONFIG_NUMA */\n\n/*\n * Initialize sched groups cpu_power.\n *\n * cpu_power indicates the capacity of sched group, which is used while\n * distributing the load between different sched groups in a sched domain.\n * Typically cpu_power for all the groups in a sched domain will be same unless\n * there are asymmetries in the topology. If there are asymmetries, group\n * having more cpu_power will pickup more load compared to the group having\n * less cpu_power.\n */\nstatic void init_sched_groups_power(int cpu, struct sched_domain *sd)\n{\n\tstruct sched_domain *child;\n\tstruct sched_group *group;\n\tlong power;\n\tint weight;\n\n\tWARN_ON(!sd || !sd->groups);\n\n\tif (cpu != group_first_cpu(sd->groups))\n\t\treturn;\n\n\tsd->groups->group_weight = cpumask_weight(sched_group_cpus(sd->groups));\n\n\tchild = sd->child;\n\n\tsd->groups->cpu_power = 0;\n\n\tif (!child) {\n\t\tpower = SCHED_LOAD_SCALE;\n\t\tweight = cpumask_weight(sched_domain_span(sd));\n\t\t/*\n\t\t * SMT siblings share the power of a single core.\n\t\t * Usually multiple threads get a better yield out of\n\t\t * that one core than a single thread would have,\n\t\t * reflect that in sd->smt_gain.\n\t\t */\n\t\tif ((sd->flags & SD_SHARE_CPUPOWER) && weight > 1) {\n\t\t\tpower *= sd->smt_gain;\n\t\t\tpower /= weight;\n\t\t\tpower >>= SCHED_LOAD_SHIFT;\n\t\t}\n\t\tsd->groups->cpu_power += power;\n\t\treturn;\n\t}\n\n\t/*\n\t * Add cpu_power of each child group to this groups cpu_power.\n\t */\n\tgroup = child->groups;\n\tdo {\n\t\tsd->groups->cpu_power += group->cpu_power;\n\t\tgroup = group->next;\n\t} while (group != child->groups);\n}\n\n/*\n * Initializers for schedule domains\n * Non-inlined to reduce accumulated stack pressure in build_sched_domains()\n */\n\n#ifdef CONFIG_SCHED_DEBUG\n# define SD_INIT_NAME(sd, type)\t\tsd->name = #type\n#else\n# define SD_INIT_NAME(sd, type)\t\tdo { } while (0)\n#endif\n\n#define\tSD_INIT(sd, type)\tsd_init_##type(sd)\n\n#define SD_INIT_FUNC(type)\t\\\nstatic noinline void sd_init_##type(struct sched_domain *sd)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tmemset(sd, 0, sizeof(*sd));\t\t\t\t\\\n\t*sd = SD_##type##_INIT;\t\t\t\t\t\\\n\tsd->level = SD_LV_##type;\t\t\t\t\\\n\tSD_INIT_NAME(sd, type);\t\t\t\t\t\\\n}\n\nSD_INIT_FUNC(CPU)\n#ifdef CONFIG_NUMA\n SD_INIT_FUNC(ALLNODES)\n SD_INIT_FUNC(NODE)\n#endif\n#ifdef CONFIG_SCHED_SMT\n SD_INIT_FUNC(SIBLING)\n#endif\n#ifdef CONFIG_SCHED_MC\n SD_INIT_FUNC(MC)\n#endif\n#ifdef CONFIG_SCHED_BOOK\n SD_INIT_FUNC(BOOK)\n#endif\n\nstatic int default_relax_domain_level = -1;\n\nstatic int __init setup_relax_domain_level(char *str)\n{\n\tunsigned long val;\n\n\tval = simple_strtoul(str, NULL, 0);\n\tif (val < SD_LV_MAX)\n\t\tdefault_relax_domain_level = val;\n\n\treturn 1;\n}\n__setup(\"relax_domain_level=\", setup_relax_domain_level);\n\nstatic void set_domain_attribute(struct sched_domain *sd,\n\t\t\t\t struct sched_domain_attr *attr)\n{\n\tint request;\n\n\tif (!attr || attr->relax_domain_level < 0) {\n\t\tif (default_relax_domain_level < 0)\n\t\t\treturn;\n\t\telse\n\t\t\trequest = default_relax_domain_level;\n\t} else\n\t\trequest = attr->relax_domain_level;\n\tif (request < sd->level) {\n\t\t/* turn off idle balance on this domain */\n\t\tsd->flags &= ~(SD_BALANCE_WAKE|SD_BALANCE_NEWIDLE);\n\t} else {\n\t\t/* turn on idle balance on this domain */\n\t\tsd->flags |= (SD_BALANCE_WAKE|SD_BALANCE_NEWIDLE);\n\t}\n}\n\nstatic void __free_domain_allocs(struct s_data *d, enum s_alloc what,\n\t\t\t\t const struct cpumask *cpu_map)\n{\n\tswitch (what) {\n\tcase sa_sched_groups:\n\t\tfree_sched_groups(cpu_map, d->tmpmask); /* fall through */\n\t\td->sched_group_nodes = NULL;\n\tcase sa_rootdomain:\n\t\tfree_rootdomain(d->rd); /* fall through */\n\tcase sa_tmpmask:\n\t\tfree_cpumask_var(d->tmpmask); /* fall through */\n\tcase sa_send_covered:\n\t\tfree_cpumask_var(d->send_covered); /* fall through */\n\tcase sa_this_book_map:\n\t\tfree_cpumask_var(d->this_book_map); /* fall through */\n\tcase sa_this_core_map:\n\t\tfree_cpumask_var(d->this_core_map); /* fall through */\n\tcase sa_this_sibling_map:\n\t\tfree_cpumask_var(d->this_sibling_map); /* fall through */\n\tcase sa_nodemask:\n\t\tfree_cpumask_var(d->nodemask); /* fall through */\n\tcase sa_sched_group_nodes:\n#ifdef CONFIG_NUMA\n\t\tkfree(d->sched_group_nodes); /* fall through */\n\tcase sa_notcovered:\n\t\tfree_cpumask_var(d->notcovered); /* fall through */\n\tcase sa_covered:\n\t\tfree_cpumask_var(d->covered); /* fall through */\n\tcase sa_domainspan:\n\t\tfree_cpumask_var(d->domainspan); /* fall through */\n#endif\n\tcase sa_none:\n\t\tbreak;\n\t}\n}\n\nstatic enum s_alloc __visit_domain_allocation_hell(struct s_data *d,\n\t\t\t\t\t\t   const struct cpumask *cpu_map)\n{\n#ifdef CONFIG_NUMA\n\tif (!alloc_cpumask_var(&d->domainspan, GFP_KERNEL))\n\t\treturn sa_none;\n\tif (!alloc_cpumask_var(&d->covered, GFP_KERNEL))\n\t\treturn sa_domainspan;\n\tif (!alloc_cpumask_var(&d->notcovered, GFP_KERNEL))\n\t\treturn sa_covered;\n\t/* Allocate the per-node list of sched groups */\n\td->sched_group_nodes = kcalloc(nr_node_ids,\n\t\t\t\t      sizeof(struct sched_group *), GFP_KERNEL);\n\tif (!d->sched_group_nodes) {\n\t\tprintk(KERN_WARNING \"Can not alloc sched group node list\\n\");\n\t\treturn sa_notcovered;\n\t}\n\tsched_group_nodes_bycpu[cpumask_first(cpu_map)] = d->sched_group_nodes;\n#endif\n\tif (!alloc_cpumask_var(&d->nodemask, GFP_KERNEL))\n\t\treturn sa_sched_group_nodes;\n\tif (!alloc_cpumask_var(&d->this_sibling_map, GFP_KERNEL))\n\t\treturn sa_nodemask;\n\tif (!alloc_cpumask_var(&d->this_core_map, GFP_KERNEL))\n\t\treturn sa_this_sibling_map;\n\tif (!alloc_cpumask_var(&d->this_book_map, GFP_KERNEL))\n\t\treturn sa_this_core_map;\n\tif (!alloc_cpumask_var(&d->send_covered, GFP_KERNEL))\n\t\treturn sa_this_book_map;\n\tif (!alloc_cpumask_var(&d->tmpmask, GFP_KERNEL))\n\t\treturn sa_send_covered;\n\td->rd = alloc_rootdomain();\n\tif (!d->rd) {\n\t\tprintk(KERN_WARNING \"Cannot alloc root domain\\n\");\n\t\treturn sa_tmpmask;\n\t}\n\treturn sa_rootdomain;\n}\n\nstatic struct sched_domain *__build_numa_sched_domains(struct s_data *d,\n\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr, int i)\n{\n\tstruct sched_domain *sd = NULL;\n#ifdef CONFIG_NUMA\n\tstruct sched_domain *parent;\n\n\td->sd_allnodes = 0;\n\tif (cpumask_weight(cpu_map) >\n\t    SD_NODES_PER_DOMAIN * cpumask_weight(d->nodemask)) {\n\t\tsd = &per_cpu(allnodes_domains, i).sd;\n\t\tSD_INIT(sd, ALLNODES);\n\t\tset_domain_attribute(sd, attr);\n\t\tcpumask_copy(sched_domain_span(sd), cpu_map);\n\t\tcpu_to_allnodes_group(i, cpu_map, &sd->groups, d->tmpmask);\n\t\td->sd_allnodes = 1;\n\t}\n\tparent = sd;\n\n\tsd = &per_cpu(node_domains, i).sd;\n\tSD_INIT(sd, NODE);\n\tset_domain_attribute(sd, attr);\n\tsched_domain_node_span(cpu_to_node(i), sched_domain_span(sd));\n\tsd->parent = parent;\n\tif (parent)\n\t\tparent->child = sd;\n\tcpumask_and(sched_domain_span(sd), sched_domain_span(sd), cpu_map);\n#endif\n\treturn sd;\n}\n\nstatic struct sched_domain *__build_cpu_sched_domain(struct s_data *d,\n\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr,\n\tstruct sched_domain *parent, int i)\n{\n\tstruct sched_domain *sd;\n\tsd = &per_cpu(phys_domains, i).sd;\n\tSD_INIT(sd, CPU);\n\tset_domain_attribute(sd, attr);\n\tcpumask_copy(sched_domain_span(sd), d->nodemask);\n\tsd->parent = parent;\n\tif (parent)\n\t\tparent->child = sd;\n\tcpu_to_phys_group(i, cpu_map, &sd->groups, d->tmpmask);\n\treturn sd;\n}\n\nstatic struct sched_domain *__build_book_sched_domain(struct s_data *d,\n\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr,\n\tstruct sched_domain *parent, int i)\n{\n\tstruct sched_domain *sd = parent;\n#ifdef CONFIG_SCHED_BOOK\n\tsd = &per_cpu(book_domains, i).sd;\n\tSD_INIT(sd, BOOK);\n\tset_domain_attribute(sd, attr);\n\tcpumask_and(sched_domain_span(sd), cpu_map, cpu_book_mask(i));\n\tsd->parent = parent;\n\tparent->child = sd;\n\tcpu_to_book_group(i, cpu_map, &sd->groups, d->tmpmask);\n#endif\n\treturn sd;\n}\n\nstatic struct sched_domain *__build_mc_sched_domain(struct s_data *d,\n\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr,\n\tstruct sched_domain *parent, int i)\n{\n\tstruct sched_domain *sd = parent;\n#ifdef CONFIG_SCHED_MC\n\tsd = &per_cpu(core_domains, i).sd;\n\tSD_INIT(sd, MC);\n\tset_domain_attribute(sd, attr);\n\tcpumask_and(sched_domain_span(sd), cpu_map, cpu_coregroup_mask(i));\n\tsd->parent = parent;\n\tparent->child = sd;\n\tcpu_to_core_group(i, cpu_map, &sd->groups, d->tmpmask);\n#endif\n\treturn sd;\n}\n\nstatic struct sched_domain *__build_smt_sched_domain(struct s_data *d,\n\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr,\n\tstruct sched_domain *parent, int i)\n{\n\tstruct sched_domain *sd = parent;\n#ifdef CONFIG_SCHED_SMT\n\tsd = &per_cpu(cpu_domains, i).sd;\n\tSD_INIT(sd, SIBLING);\n\tset_domain_attribute(sd, attr);\n\tcpumask_and(sched_domain_span(sd), cpu_map, topology_thread_cpumask(i));\n\tsd->parent = parent;\n\tparent->child = sd;\n\tcpu_to_cpu_group(i, cpu_map, &sd->groups, d->tmpmask);\n#endif\n\treturn sd;\n}\n\nstatic void build_sched_groups(struct s_data *d, enum sched_domain_level l,\n\t\t\t       const struct cpumask *cpu_map, int cpu)\n{\n\tswitch (l) {\n#ifdef CONFIG_SCHED_SMT\n\tcase SD_LV_SIBLING: /* set up CPU (sibling) groups */\n\t\tcpumask_and(d->this_sibling_map, cpu_map,\n\t\t\t    topology_thread_cpumask(cpu));\n\t\tif (cpu == cpumask_first(d->this_sibling_map))\n\t\t\tinit_sched_build_groups(d->this_sibling_map, cpu_map,\n\t\t\t\t\t\t&cpu_to_cpu_group,\n\t\t\t\t\t\td->send_covered, d->tmpmask);\n\t\tbreak;\n#endif\n#ifdef CONFIG_SCHED_MC\n\tcase SD_LV_MC: /* set up multi-core groups */\n\t\tcpumask_and(d->this_core_map, cpu_map, cpu_coregroup_mask(cpu));\n\t\tif (cpu == cpumask_first(d->this_core_map))\n\t\t\tinit_sched_build_groups(d->this_core_map, cpu_map,\n\t\t\t\t\t\t&cpu_to_core_group,\n\t\t\t\t\t\td->send_covered, d->tmpmask);\n\t\tbreak;\n#endif\n#ifdef CONFIG_SCHED_BOOK\n\tcase SD_LV_BOOK: /* set up book groups */\n\t\tcpumask_and(d->this_book_map, cpu_map, cpu_book_mask(cpu));\n\t\tif (cpu == cpumask_first(d->this_book_map))\n\t\t\tinit_sched_build_groups(d->this_book_map, cpu_map,\n\t\t\t\t\t\t&cpu_to_book_group,\n\t\t\t\t\t\td->send_covered, d->tmpmask);\n\t\tbreak;\n#endif\n\tcase SD_LV_CPU: /* set up physical groups */\n\t\tcpumask_and(d->nodemask, cpumask_of_node(cpu), cpu_map);\n\t\tif (!cpumask_empty(d->nodemask))\n\t\t\tinit_sched_build_groups(d->nodemask, cpu_map,\n\t\t\t\t\t\t&cpu_to_phys_group,\n\t\t\t\t\t\td->send_covered, d->tmpmask);\n\t\tbreak;\n#ifdef CONFIG_NUMA\n\tcase SD_LV_ALLNODES:\n\t\tinit_sched_build_groups(cpu_map, cpu_map, &cpu_to_allnodes_group,\n\t\t\t\t\td->send_covered, d->tmpmask);\n\t\tbreak;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n}\n\n/*\n * Build sched domains for a given set of cpus and attach the sched domains\n * to the individual cpus\n */\nstatic int __build_sched_domains(const struct cpumask *cpu_map,\n\t\t\t\t struct sched_domain_attr *attr)\n{\n\tenum s_alloc alloc_state = sa_none;\n\tstruct s_data d;\n\tstruct sched_domain *sd;\n\tint i;\n#ifdef CONFIG_NUMA\n\td.sd_allnodes = 0;\n#endif\n\n\talloc_state = __visit_domain_allocation_hell(&d, cpu_map);\n\tif (alloc_state != sa_rootdomain)\n\t\tgoto error;\n\talloc_state = sa_sched_groups;\n\n\t/*\n\t * Set up domains for cpus specified by the cpu_map.\n\t */\n\tfor_each_cpu(i, cpu_map) {\n\t\tcpumask_and(d.nodemask, cpumask_of_node(cpu_to_node(i)),\n\t\t\t    cpu_map);\n\n\t\tsd = __build_numa_sched_domains(&d, cpu_map, attr, i);\n\t\tsd = __build_cpu_sched_domain(&d, cpu_map, attr, sd, i);\n\t\tsd = __build_book_sched_domain(&d, cpu_map, attr, sd, i);\n\t\tsd = __build_mc_sched_domain(&d, cpu_map, attr, sd, i);\n\t\tsd = __build_smt_sched_domain(&d, cpu_map, attr, sd, i);\n\t}\n\n\tfor_each_cpu(i, cpu_map) {\n\t\tbuild_sched_groups(&d, SD_LV_SIBLING, cpu_map, i);\n\t\tbuild_sched_groups(&d, SD_LV_BOOK, cpu_map, i);\n\t\tbuild_sched_groups(&d, SD_LV_MC, cpu_map, i);\n\t}\n\n\t/* Set up physical groups */\n\tfor (i = 0; i < nr_node_ids; i++)\n\t\tbuild_sched_groups(&d, SD_LV_CPU, cpu_map, i);\n\n#ifdef CONFIG_NUMA\n\t/* Set up node groups */\n\tif (d.sd_allnodes)\n\t\tbuild_sched_groups(&d, SD_LV_ALLNODES, cpu_map, 0);\n\n\tfor (i = 0; i < nr_node_ids; i++)\n\t\tif (build_numa_sched_groups(&d, cpu_map, i))\n\t\t\tgoto error;\n#endif\n\n\t/* Calculate CPU power for physical packages and nodes */\n#ifdef CONFIG_SCHED_SMT\n\tfor_each_cpu(i, cpu_map) {\n\t\tsd = &per_cpu(cpu_domains, i).sd;\n\t\tinit_sched_groups_power(i, sd);\n\t}\n#endif\n#ifdef CONFIG_SCHED_MC\n\tfor_each_cpu(i, cpu_map) {\n\t\tsd = &per_cpu(core_domains, i).sd;\n\t\tinit_sched_groups_power(i, sd);\n\t}\n#endif\n#ifdef CONFIG_SCHED_BOOK\n\tfor_each_cpu(i, cpu_map) {\n\t\tsd = &per_cpu(book_domains, i).sd;\n\t\tinit_sched_groups_power(i, sd);\n\t}\n#endif\n\n\tfor_each_cpu(i, cpu_map) {\n\t\tsd = &per_cpu(phys_domains, i).sd;\n\t\tinit_sched_groups_power(i, sd);\n\t}\n\n#ifdef CONFIG_NUMA\n\tfor (i = 0; i < nr_node_ids; i++)\n\t\tinit_numa_sched_groups_power(d.sched_group_nodes[i]);\n\n\tif (d.sd_allnodes) {\n\t\tstruct sched_group *sg;\n\n\t\tcpu_to_allnodes_group(cpumask_first(cpu_map), cpu_map, &sg,\n\t\t\t\t\t\t\t\td.tmpmask);\n\t\tinit_numa_sched_groups_power(sg);\n\t}\n#endif\n\n\t/* Attach the domains */\n\tfor_each_cpu(i, cpu_map) {\n#ifdef CONFIG_SCHED_SMT\n\t\tsd = &per_cpu(cpu_domains, i).sd;\n#elif defined(CONFIG_SCHED_MC)\n\t\tsd = &per_cpu(core_domains, i).sd;\n#elif defined(CONFIG_SCHED_BOOK)\n\t\tsd = &per_cpu(book_domains, i).sd;\n#else\n\t\tsd = &per_cpu(phys_domains, i).sd;\n#endif\n\t\tcpu_attach_domain(sd, d.rd, i);\n\t}\n\n\td.sched_group_nodes = NULL; /* don't free this we still need it */\n\t__free_domain_allocs(&d, sa_tmpmask, cpu_map);\n\treturn 0;\n\nerror:\n\t__free_domain_allocs(&d, alloc_state, cpu_map);\n\treturn -ENOMEM;\n}\n\nstatic int build_sched_domains(const struct cpumask *cpu_map)\n{\n\treturn __build_sched_domains(cpu_map, NULL);\n}\n\nstatic cpumask_var_t *doms_cur;\t/* current sched domains */\nstatic int ndoms_cur;\t\t/* number of sched domains in 'doms_cur' */\nstatic struct sched_domain_attr *dattr_cur;\n\t\t\t\t/* attribues of custom domains in 'doms_cur' */\n\n/*\n * Special case: If a kmalloc of a doms_cur partition (array of\n * cpumask) fails, then fallback to a single sched domain,\n * as determined by the single cpumask fallback_doms.\n */\nstatic cpumask_var_t fallback_doms;\n\n/*\n * arch_update_cpu_topology lets virtualized architectures update the\n * cpu core maps. It is supposed to return 1 if the topology changed\n * or 0 if it stayed the same.\n */\nint __attribute__((weak)) arch_update_cpu_topology(void)\n{\n\treturn 0;\n}\n\ncpumask_var_t *alloc_sched_domains(unsigned int ndoms)\n{\n\tint i;\n\tcpumask_var_t *doms;\n\n\tdoms = kmalloc(sizeof(*doms) * ndoms, GFP_KERNEL);\n\tif (!doms)\n\t\treturn NULL;\n\tfor (i = 0; i < ndoms; i++) {\n\t\tif (!alloc_cpumask_var(&doms[i], GFP_KERNEL)) {\n\t\t\tfree_sched_domains(doms, i);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\treturn doms;\n}\n\nvoid free_sched_domains(cpumask_var_t doms[], unsigned int ndoms)\n{\n\tunsigned int i;\n\tfor (i = 0; i < ndoms; i++)\n\t\tfree_cpumask_var(doms[i]);\n\tkfree(doms);\n}\n\n/*\n * Set up scheduler domains and groups. Callers must hold the hotplug lock.\n * For now this just excludes isolated cpus, but could be used to\n * exclude other special cases in the future.\n */\nstatic int arch_init_sched_domains(const struct cpumask *cpu_map)\n{\n\tint err;\n\n\tarch_update_cpu_topology();\n\tndoms_cur = 1;\n\tdoms_cur = alloc_sched_domains(ndoms_cur);\n\tif (!doms_cur)\n\t\tdoms_cur = &fallback_doms;\n\tcpumask_andnot(doms_cur[0], cpu_map, cpu_isolated_map);\n\tdattr_cur = NULL;\n\terr = build_sched_domains(doms_cur[0]);\n\tregister_sched_domain_sysctl();\n\n\treturn err;\n}\n\nstatic void arch_destroy_sched_domains(const struct cpumask *cpu_map,\n\t\t\t\t       struct cpumask *tmpmask)\n{\n\tfree_sched_groups(cpu_map, tmpmask);\n}\n\n/*\n * Detach sched domains from a group of cpus specified in cpu_map\n * These cpus will now be attached to the NULL domain\n */\nstatic void detach_destroy_domains(const struct cpumask *cpu_map)\n{\n\t/* Save because hotplug lock held. */\n\tstatic DECLARE_BITMAP(tmpmask, CONFIG_NR_CPUS);\n\tint i;\n\n\tfor_each_cpu(i, cpu_map)\n\t\tcpu_attach_domain(NULL, &def_root_domain, i);\n\tsynchronize_sched();\n\tarch_destroy_sched_domains(cpu_map, to_cpumask(tmpmask));\n}\n\n/* handle null as \"default\" */\nstatic int dattrs_equal(struct sched_domain_attr *cur, int idx_cur,\n\t\t\tstruct sched_domain_attr *new, int idx_new)\n{\n\tstruct sched_domain_attr tmp;\n\n\t/* fast path */\n\tif (!new && !cur)\n\t\treturn 1;\n\n\ttmp = SD_ATTR_INIT;\n\treturn !memcmp(cur ? (cur + idx_cur) : &tmp,\n\t\t\tnew ? (new + idx_new) : &tmp,\n\t\t\tsizeof(struct sched_domain_attr));\n}\n\n/*\n * Partition sched domains as specified by the 'ndoms_new'\n * cpumasks in the array doms_new[] of cpumasks. This compares\n * doms_new[] to the current sched domain partitioning, doms_cur[].\n * It destroys each deleted domain and builds each new domain.\n *\n * 'doms_new' is an array of cpumask_var_t's of length 'ndoms_new'.\n * The masks don't intersect (don't overlap.) We should setup one\n * sched domain for each mask. CPUs not in any of the cpumasks will\n * not be load balanced. If the same cpumask appears both in the\n * current 'doms_cur' domains and in the new 'doms_new', we can leave\n * it as it is.\n *\n * The passed in 'doms_new' should be allocated using\n * alloc_sched_domains.  This routine takes ownership of it and will\n * free_sched_domains it when done with it. If the caller failed the\n * alloc call, then it can pass in doms_new == NULL && ndoms_new == 1,\n * and partition_sched_domains() will fallback to the single partition\n * 'fallback_doms', it also forces the domains to be rebuilt.\n *\n * If doms_new == NULL it will be replaced with cpu_online_mask.\n * ndoms_new == 0 is a special case for destroying existing domains,\n * and it will not create the default domain.\n *\n * Call with hotplug lock held\n */\nvoid partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t     struct sched_domain_attr *dattr_new)\n{\n\tint i, j, n;\n\tint new_topology;\n\n\tmutex_lock(&sched_domains_mutex);\n\n\t/* always unregister in case we don't destroy any domains */\n\tunregister_sched_domain_sysctl();\n\n\t/* Let architecture update cpu core mappings. */\n\tnew_topology = arch_update_cpu_topology();\n\n\tn = doms_new ? ndoms_new : 0;\n\n\t/* Destroy deleted domains */\n\tfor (i = 0; i < ndoms_cur; i++) {\n\t\tfor (j = 0; j < n && !new_topology; j++) {\n\t\t\tif (cpumask_equal(doms_cur[i], doms_new[j])\n\t\t\t    && dattrs_equal(dattr_cur, i, dattr_new, j))\n\t\t\t\tgoto match1;\n\t\t}\n\t\t/* no match - a current sched domain not in new doms_new[] */\n\t\tdetach_destroy_domains(doms_cur[i]);\nmatch1:\n\t\t;\n\t}\n\n\tif (doms_new == NULL) {\n\t\tndoms_cur = 0;\n\t\tdoms_new = &fallback_doms;\n\t\tcpumask_andnot(doms_new[0], cpu_active_mask, cpu_isolated_map);\n\t\tWARN_ON_ONCE(dattr_new);\n\t}\n\n\t/* Build new domains */\n\tfor (i = 0; i < ndoms_new; i++) {\n\t\tfor (j = 0; j < ndoms_cur && !new_topology; j++) {\n\t\t\tif (cpumask_equal(doms_new[i], doms_cur[j])\n\t\t\t    && dattrs_equal(dattr_new, i, dattr_cur, j))\n\t\t\t\tgoto match2;\n\t\t}\n\t\t/* no match - add a new doms_new */\n\t\t__build_sched_domains(doms_new[i],\n\t\t\t\t\tdattr_new ? dattr_new + i : NULL);\nmatch2:\n\t\t;\n\t}\n\n\t/* Remember the new sched domains */\n\tif (doms_cur != &fallback_doms)\n\t\tfree_sched_domains(doms_cur, ndoms_cur);\n\tkfree(dattr_cur);\t/* kfree(NULL) is safe */\n\tdoms_cur = doms_new;\n\tdattr_cur = dattr_new;\n\tndoms_cur = ndoms_new;\n\n\tregister_sched_domain_sysctl();\n\n\tmutex_unlock(&sched_domains_mutex);\n}\n\n#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)\nstatic void arch_reinit_sched_domains(void)\n{\n\tget_online_cpus();\n\n\t/* Destroy domains first to force the rebuild */\n\tpartition_sched_domains(0, NULL, NULL);\n\n\trebuild_sched_domains();\n\tput_online_cpus();\n}\n\nstatic ssize_t sched_power_savings_store(const char *buf, size_t count, int smt)\n{\n\tunsigned int level = 0;\n\n\tif (sscanf(buf, \"%u\", &level) != 1)\n\t\treturn -EINVAL;\n\n\t/*\n\t * level is always be positive so don't check for\n\t * level < POWERSAVINGS_BALANCE_NONE which is 0\n\t * What happens on 0 or 1 byte write,\n\t * need to check for count as well?\n\t */\n\n\tif (level >= MAX_POWERSAVINGS_BALANCE_LEVELS)\n\t\treturn -EINVAL;\n\n\tif (smt)\n\t\tsched_smt_power_savings = level;\n\telse\n\t\tsched_mc_power_savings = level;\n\n\tarch_reinit_sched_domains();\n\n\treturn count;\n}\n\n#ifdef CONFIG_SCHED_MC\nstatic ssize_t sched_mc_power_savings_show(struct sysdev_class *class,\n\t\t\t\t\t   struct sysdev_class_attribute *attr,\n\t\t\t\t\t   char *page)\n{\n\treturn sprintf(page, \"%u\\n\", sched_mc_power_savings);\n}\nstatic ssize_t sched_mc_power_savings_store(struct sysdev_class *class,\n\t\t\t\t\t    struct sysdev_class_attribute *attr,\n\t\t\t\t\t    const char *buf, size_t count)\n{\n\treturn sched_power_savings_store(buf, count, 0);\n}\nstatic SYSDEV_CLASS_ATTR(sched_mc_power_savings, 0644,\n\t\t\t sched_mc_power_savings_show,\n\t\t\t sched_mc_power_savings_store);\n#endif\n\n#ifdef CONFIG_SCHED_SMT\nstatic ssize_t sched_smt_power_savings_show(struct sysdev_class *dev,\n\t\t\t\t\t    struct sysdev_class_attribute *attr,\n\t\t\t\t\t    char *page)\n{\n\treturn sprintf(page, \"%u\\n\", sched_smt_power_savings);\n}\nstatic ssize_t sched_smt_power_savings_store(struct sysdev_class *dev,\n\t\t\t\t\t     struct sysdev_class_attribute *attr,\n\t\t\t\t\t     const char *buf, size_t count)\n{\n\treturn sched_power_savings_store(buf, count, 1);\n}\nstatic SYSDEV_CLASS_ATTR(sched_smt_power_savings, 0644,\n\t\t   sched_smt_power_savings_show,\n\t\t   sched_smt_power_savings_store);\n#endif\n\nint __init sched_create_sysfs_power_savings_entries(struct sysdev_class *cls)\n{\n\tint err = 0;\n\n#ifdef CONFIG_SCHED_SMT\n\tif (smt_capable())\n\t\terr = sysfs_create_file(&cls->kset.kobj,\n\t\t\t\t\t&attr_sched_smt_power_savings.attr);\n#endif\n#ifdef CONFIG_SCHED_MC\n\tif (!err && mc_capable())\n\t\terr = sysfs_create_file(&cls->kset.kobj,\n\t\t\t\t\t&attr_sched_mc_power_savings.attr);\n#endif\n\treturn err;\n}\n#endif /* CONFIG_SCHED_MC || CONFIG_SCHED_SMT */\n\n/*\n * Update cpusets according to cpu_active mask.  If cpusets are\n * disabled, cpuset_update_active_cpus() becomes a simple wrapper\n * around partition_sched_domains().\n */\nstatic int cpuset_cpu_active(struct notifier_block *nfb, unsigned long action,\n\t\t\t     void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_ONLINE:\n\tcase CPU_DOWN_FAILED:\n\t\tcpuset_update_active_cpus();\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int cpuset_cpu_inactive(struct notifier_block *nfb, unsigned long action,\n\t\t\t       void *hcpu)\n{\n\tswitch (action & ~CPU_TASKS_FROZEN) {\n\tcase CPU_DOWN_PREPARE:\n\t\tcpuset_update_active_cpus();\n\t\treturn NOTIFY_OK;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nstatic int update_runtime(struct notifier_block *nfb,\n\t\t\t\tunsigned long action, void *hcpu)\n{\n\tint cpu = (int)(long)hcpu;\n\n\tswitch (action) {\n\tcase CPU_DOWN_PREPARE:\n\tcase CPU_DOWN_PREPARE_FROZEN:\n\t\tdisable_runtime(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\n\tcase CPU_DOWN_FAILED:\n\tcase CPU_DOWN_FAILED_FROZEN:\n\tcase CPU_ONLINE:\n\tcase CPU_ONLINE_FROZEN:\n\t\tenable_runtime(cpu_rq(cpu));\n\t\treturn NOTIFY_OK;\n\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n}\n\nvoid __init sched_init_smp(void)\n{\n\tcpumask_var_t non_isolated_cpus;\n\n\talloc_cpumask_var(&non_isolated_cpus, GFP_KERNEL);\n\talloc_cpumask_var(&fallback_doms, GFP_KERNEL);\n\n#if defined(CONFIG_NUMA)\n\tsched_group_nodes_bycpu = kzalloc(nr_cpu_ids * sizeof(void **),\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\tBUG_ON(sched_group_nodes_bycpu == NULL);\n#endif\n\tget_online_cpus();\n\tmutex_lock(&sched_domains_mutex);\n\tarch_init_sched_domains(cpu_active_mask);\n\tcpumask_andnot(non_isolated_cpus, cpu_possible_mask, cpu_isolated_map);\n\tif (cpumask_empty(non_isolated_cpus))\n\t\tcpumask_set_cpu(smp_processor_id(), non_isolated_cpus);\n\tmutex_unlock(&sched_domains_mutex);\n\tput_online_cpus();\n\n\thotcpu_notifier(cpuset_cpu_active, CPU_PRI_CPUSET_ACTIVE);\n\thotcpu_notifier(cpuset_cpu_inactive, CPU_PRI_CPUSET_INACTIVE);\n\n\t/* RT runtime code needs to handle some hotplug events */\n\thotcpu_notifier(update_runtime, 0);\n\n\tinit_hrtick();\n\n\t/* Move init over to a non-isolated CPU */\n\tif (set_cpus_allowed_ptr(current, non_isolated_cpus) < 0)\n\t\tBUG();\n\tsched_init_granularity();\n\tfree_cpumask_var(non_isolated_cpus);\n\n\tinit_sched_rt_class();\n}\n#else\nvoid __init sched_init_smp(void)\n{\n\tsched_init_granularity();\n}\n#endif /* CONFIG_SMP */\n\nconst_debug unsigned int sysctl_timer_migration = 1;\n\nint in_sched_functions(unsigned long addr)\n{\n\treturn in_lock_functions(addr) ||\n\t\t(addr >= (unsigned long)__sched_text_start\n\t\t&& addr < (unsigned long)__sched_text_end);\n}\n\nstatic void init_cfs_rq(struct cfs_rq *cfs_rq, struct rq *rq)\n{\n\tcfs_rq->tasks_timeline = RB_ROOT;\n\tINIT_LIST_HEAD(&cfs_rq->tasks);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tcfs_rq->rq = rq;\n#endif\n\tcfs_rq->min_vruntime = (u64)(-(1LL << 20));\n}\n\nstatic void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq)\n{\n\tstruct rt_prio_array *array;\n\tint i;\n\n\tarray = &rt_rq->active;\n\tfor (i = 0; i < MAX_RT_PRIO; i++) {\n\t\tINIT_LIST_HEAD(array->queue + i);\n\t\t__clear_bit(i, array->bitmap);\n\t}\n\t/* delimiter for bitsearch: */\n\t__set_bit(MAX_RT_PRIO, array->bitmap);\n\n#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED\n\trt_rq->highest_prio.curr = MAX_RT_PRIO;\n#ifdef CONFIG_SMP\n\trt_rq->highest_prio.next = MAX_RT_PRIO;\n#endif\n#endif\n#ifdef CONFIG_SMP\n\trt_rq->rt_nr_migratory = 0;\n\trt_rq->overloaded = 0;\n\tplist_head_init_raw(&rt_rq->pushable_tasks, &rq->lock);\n#endif\n\n\trt_rq->rt_time = 0;\n\trt_rq->rt_throttled = 0;\n\trt_rq->rt_runtime = 0;\n\traw_spin_lock_init(&rt_rq->rt_runtime_lock);\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\trt_rq->rt_nr_boosted = 0;\n\trt_rq->rq = rq;\n#endif\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\t\tstruct sched_entity *se, int cpu, int add,\n\t\t\t\tstruct sched_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\ttg->cfs_rq[cpu] = cfs_rq;\n\tinit_cfs_rq(cfs_rq, rq);\n\tcfs_rq->tg = tg;\n\tif (add)\n\t\tlist_add(&cfs_rq->leaf_cfs_rq_list, &rq->leaf_cfs_rq_list);\n\n\ttg->se[cpu] = se;\n\t/* se could be NULL for init_task_group */\n\tif (!se)\n\t\treturn;\n\n\tif (!parent)\n\t\tse->cfs_rq = &rq->cfs;\n\telse\n\t\tse->cfs_rq = parent->my_q;\n\n\tse->my_q = cfs_rq;\n\tse->load.weight = tg->shares;\n\tse->load.inv_weight = 0;\n\tse->parent = parent;\n}\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,\n\t\tstruct sched_rt_entity *rt_se, int cpu, int add,\n\t\tstruct sched_rt_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\ttg->rt_rq[cpu] = rt_rq;\n\tinit_rt_rq(rt_rq, rq);\n\trt_rq->tg = tg;\n\trt_rq->rt_runtime = tg->rt_bandwidth.rt_runtime;\n\tif (add)\n\t\tlist_add(&rt_rq->leaf_rt_rq_list, &rq->leaf_rt_rq_list);\n\n\ttg->rt_se[cpu] = rt_se;\n\tif (!rt_se)\n\t\treturn;\n\n\tif (!parent)\n\t\trt_se->rt_rq = &rq->rt;\n\telse\n\t\trt_se->rt_rq = parent->my_q;\n\n\trt_se->my_q = rt_rq;\n\trt_se->parent = parent;\n\tINIT_LIST_HEAD(&rt_se->run_list);\n}\n#endif\n\nvoid __init sched_init(void)\n{\n\tint i, j;\n\tunsigned long alloc_size = 0, ptr;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\talloc_size += 2 * nr_cpu_ids * sizeof(void **);\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\talloc_size += 2 * nr_cpu_ids * sizeof(void **);\n#endif\n#ifdef CONFIG_CPUMASK_OFFSTACK\n\talloc_size += num_possible_cpus() * cpumask_size();\n#endif\n\tif (alloc_size) {\n\t\tptr = (unsigned long)kzalloc(alloc_size, GFP_NOWAIT);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t\tinit_task_group.se = (struct sched_entity **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\tinit_task_group.cfs_rq = (struct cfs_rq **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n#ifdef CONFIG_RT_GROUP_SCHED\n\t\tinit_task_group.rt_se = (struct sched_rt_entity **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\tinit_task_group.rt_rq = (struct rt_rq **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n#endif /* CONFIG_RT_GROUP_SCHED */\n#ifdef CONFIG_CPUMASK_OFFSTACK\n\t\tfor_each_possible_cpu(i) {\n\t\t\tper_cpu(load_balance_tmpmask, i) = (void *)ptr;\n\t\t\tptr += cpumask_size();\n\t\t}\n#endif /* CONFIG_CPUMASK_OFFSTACK */\n\t}\n\n#ifdef CONFIG_SMP\n\tinit_defrootdomain();\n#endif\n\n\tinit_rt_bandwidth(&def_rt_bandwidth,\n\t\t\tglobal_rt_period(), global_rt_runtime());\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tinit_rt_bandwidth(&init_task_group.rt_bandwidth,\n\t\t\tglobal_rt_period(), global_rt_runtime());\n#endif /* CONFIG_RT_GROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_SCHED\n\tlist_add(&init_task_group.list, &task_groups);\n\tINIT_LIST_HEAD(&init_task_group.children);\n\n#endif /* CONFIG_CGROUP_SCHED */\n\n#if defined CONFIG_FAIR_GROUP_SCHED && defined CONFIG_SMP\n\tupdate_shares_data = __alloc_percpu(nr_cpu_ids * sizeof(unsigned long),\n\t\t\t\t\t    __alignof__(unsigned long));\n#endif\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq;\n\n\t\trq = cpu_rq(i);\n\t\traw_spin_lock_init(&rq->lock);\n\t\trq->nr_running = 0;\n\t\trq->calc_load_active = 0;\n\t\trq->calc_load_update = jiffies + LOAD_FREQ;\n\t\tinit_cfs_rq(&rq->cfs, rq);\n\t\tinit_rt_rq(&rq->rt, rq);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t\tinit_task_group.shares = init_task_group_load;\n\t\tINIT_LIST_HEAD(&rq->leaf_cfs_rq_list);\n#ifdef CONFIG_CGROUP_SCHED\n\t\t/*\n\t\t * How much cpu bandwidth does init_task_group get?\n\t\t *\n\t\t * In case of task-groups formed thr' the cgroup filesystem, it\n\t\t * gets 100% of the cpu resources in the system. This overall\n\t\t * system cpu resource is divided among the tasks of\n\t\t * init_task_group and its child task-groups in a fair manner,\n\t\t * based on each entity's (task or task-group's) weight\n\t\t * (se->load.weight).\n\t\t *\n\t\t * In other words, if init_task_group has 10 tasks of weight\n\t\t * 1024) and two child groups A0 and A1 (of weight 1024 each),\n\t\t * then A0's share of the cpu resource is:\n\t\t *\n\t\t *\tA0's bandwidth = 1024 / (10*1024 + 1024 + 1024) = 8.33%\n\t\t *\n\t\t * We achieve this by letting init_task_group's tasks sit\n\t\t * directly in rq->cfs (i.e init_task_group->se[] = NULL).\n\t\t */\n\t\tinit_tg_cfs_entry(&init_task_group, &rq->cfs, NULL, i, 1, NULL);\n#endif\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n\t\trq->rt.rt_runtime = def_rt_bandwidth.rt_runtime;\n#ifdef CONFIG_RT_GROUP_SCHED\n\t\tINIT_LIST_HEAD(&rq->leaf_rt_rq_list);\n#ifdef CONFIG_CGROUP_SCHED\n\t\tinit_tg_rt_entry(&init_task_group, &rq->rt, NULL, i, 1, NULL);\n#endif\n#endif\n\n\t\tfor (j = 0; j < CPU_LOAD_IDX_MAX; j++)\n\t\t\trq->cpu_load[j] = 0;\n\n\t\trq->last_load_update_tick = jiffies;\n\n#ifdef CONFIG_SMP\n\t\trq->sd = NULL;\n\t\trq->rd = NULL;\n\t\trq->cpu_power = SCHED_LOAD_SCALE;\n\t\trq->post_schedule = 0;\n\t\trq->active_balance = 0;\n\t\trq->next_balance = jiffies;\n\t\trq->push_cpu = 0;\n\t\trq->cpu = i;\n\t\trq->online = 0;\n\t\trq->idle_stamp = 0;\n\t\trq->avg_idle = 2*sysctl_sched_migration_cost;\n\t\trq_attach_root(rq, &def_root_domain);\n#ifdef CONFIG_NO_HZ\n\t\trq->nohz_balance_kick = 0;\n\t\tinit_sched_softirq_csd(&per_cpu(remote_sched_softirq_cb, i));\n#endif\n#endif\n\t\tinit_rq_hrtick(rq);\n\t\tatomic_set(&rq->nr_iowait, 0);\n\t}\n\n\tset_load_weight(&init_task);\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\tINIT_HLIST_HEAD(&init_task.preempt_notifiers);\n#endif\n\n#ifdef CONFIG_SMP\n\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains);\n#endif\n\n#ifdef CONFIG_RT_MUTEXES\n\tplist_head_init_raw(&init_task.pi_waiters, &init_task.pi_lock);\n#endif\n\n\t/*\n\t * The boot idle thread does lazy MMU switching as well:\n\t */\n\tatomic_inc(&init_mm.mm_count);\n\tenter_lazy_tlb(&init_mm, current);\n\n\t/*\n\t * Make us the idle thread. Technically, schedule() should not be\n\t * called from this thread, however somewhere below it might be,\n\t * but because we are the idle thread, we just pick up running again\n\t * when this runqueue becomes \"idle\".\n\t */\n\tinit_idle(current, smp_processor_id());\n\n\tcalc_load_update = jiffies + LOAD_FREQ;\n\n\t/*\n\t * During early bootup we pretend to be a normal task:\n\t */\n\tcurrent->sched_class = &fair_sched_class;\n\n\t/* Allocate the nohz_cpu_mask if CONFIG_CPUMASK_OFFSTACK */\n\tzalloc_cpumask_var(&nohz_cpu_mask, GFP_NOWAIT);\n#ifdef CONFIG_SMP\n#ifdef CONFIG_NO_HZ\n\tzalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);\n\talloc_cpumask_var(&nohz.grp_idle_mask, GFP_NOWAIT);\n\tatomic_set(&nohz.load_balancer, nr_cpu_ids);\n\tatomic_set(&nohz.first_pick_cpu, nr_cpu_ids);\n\tatomic_set(&nohz.second_pick_cpu, nr_cpu_ids);\n#endif\n\t/* May be allocated at isolcpus cmdline parse time */\n\tif (cpu_isolated_map == NULL)\n\t\tzalloc_cpumask_var(&cpu_isolated_map, GFP_NOWAIT);\n#endif /* SMP */\n\n\tperf_event_init();\n\n\tscheduler_running = 1;\n}\n\n#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP\nstatic inline int preempt_count_equals(int preempt_offset)\n{\n\tint nested = (preempt_count() & ~PREEMPT_ACTIVE) + rcu_preempt_depth();\n\n\treturn (nested == PREEMPT_INATOMIC_BASE + preempt_offset);\n}\n\nvoid __might_sleep(const char *file, int line, int preempt_offset)\n{\n#ifdef in_atomic\n\tstatic unsigned long prev_jiffy;\t/* ratelimiting */\n\n\tif ((preempt_count_equals(preempt_offset) && !irqs_disabled()) ||\n\t    system_state != SYSTEM_RUNNING || oops_in_progress)\n\t\treturn;\n\tif (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)\n\t\treturn;\n\tprev_jiffy = jiffies;\n\n\tprintk(KERN_ERR\n\t\t\"BUG: sleeping function called from invalid context at %s:%d\\n\",\n\t\t\tfile, line);\n\tprintk(KERN_ERR\n\t\t\"in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\\n\",\n\t\t\tin_atomic(), irqs_disabled(),\n\t\t\tcurrent->pid, current->comm);\n\n\tdebug_show_held_locks(current);\n\tif (irqs_disabled())\n\t\tprint_irqtrace_events(current);\n\tdump_stack();\n#endif\n}\nEXPORT_SYMBOL(__might_sleep);\n#endif\n\n#ifdef CONFIG_MAGIC_SYSRQ\nstatic void normalize_task(struct rq *rq, struct task_struct *p)\n{\n\tint on_rq;\n\n\ton_rq = p->se.on_rq;\n\tif (on_rq)\n\t\tdeactivate_task(rq, p, 0);\n\t__setscheduler(rq, p, SCHED_NORMAL, 0);\n\tif (on_rq) {\n\t\tactivate_task(rq, p, 0);\n\t\tresched_task(rq->curr);\n\t}\n}\n\nvoid normalize_rt_tasks(void)\n{\n\tstruct task_struct *g, *p;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\tread_lock_irqsave(&tasklist_lock, flags);\n\tdo_each_thread(g, p) {\n\t\t/*\n\t\t * Only normalize user tasks:\n\t\t */\n\t\tif (!p->mm)\n\t\t\tcontinue;\n\n\t\tp->se.exec_start\t\t= 0;\n#ifdef CONFIG_SCHEDSTATS\n\t\tp->se.statistics.wait_start\t= 0;\n\t\tp->se.statistics.sleep_start\t= 0;\n\t\tp->se.statistics.block_start\t= 0;\n#endif\n\n\t\tif (!rt_task(p)) {\n\t\t\t/*\n\t\t\t * Renice negative nice level userspace\n\t\t\t * tasks back to 0:\n\t\t\t */\n\t\t\tif (TASK_NICE(p) < 0 && p->mm)\n\t\t\t\tset_user_nice(p, 0);\n\t\t\tcontinue;\n\t\t}\n\n\t\traw_spin_lock(&p->pi_lock);\n\t\trq = __task_rq_lock(p);\n\n\t\tnormalize_task(rq, p);\n\n\t\t__task_rq_unlock(rq);\n\t\traw_spin_unlock(&p->pi_lock);\n\t} while_each_thread(g, p);\n\n\tread_unlock_irqrestore(&tasklist_lock, flags);\n}\n\n#endif /* CONFIG_MAGIC_SYSRQ */\n\n#if defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB)\n/*\n * These functions are only useful for the IA64 MCA handling, or kdb.\n *\n * They can only be called when the whole system has been\n * stopped - every CPU needs to be quiescent, and no scheduling\n * activity can take place. Using them for anything else would\n * be a serious bug, and as a result, they aren't even visible\n * under any other configuration.\n */\n\n/**\n * curr_task - return the current task for a given cpu.\n * @cpu: the processor in question.\n *\n * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!\n */\nstruct task_struct *curr_task(int cpu)\n{\n\treturn cpu_curr(cpu);\n}\n\n#endif /* defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) */\n\n#ifdef CONFIG_IA64\n/**\n * set_curr_task - set the current task for a given cpu.\n * @cpu: the processor in question.\n * @p: the task pointer to set.\n *\n * Description: This function must only be used when non-maskable interrupts\n * are serviced on a separate stack. It allows the architecture to switch the\n * notion of the current task on a cpu in a non-blocking manner. This function\n * must be called with all CPU's synchronized, and interrupts disabled, the\n * and caller must save the original value of the current task (see\n * curr_task() above) and restore that value before reenabling interrupts and\n * re-starting the system.\n *\n * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!\n */\nvoid set_curr_task(int cpu, struct task_struct *p)\n{\n\tcpu_curr(cpu) = p;\n}\n\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void free_fair_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->cfs_rq)\n\t\t\tkfree(tg->cfs_rq[i]);\n\t\tif (tg->se)\n\t\t\tkfree(tg->se[i]);\n\t}\n\n\tkfree(tg->cfs_rq);\n\tkfree(tg->se);\n}\n\nstatic\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se;\n\tstruct rq *rq;\n\tint i;\n\n\ttg->cfs_rq = kzalloc(sizeof(cfs_rq) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->cfs_rq)\n\t\tgoto err;\n\ttg->se = kzalloc(sizeof(se) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->se)\n\t\tgoto err;\n\n\ttg->shares = NICE_0_LOAD;\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\n\t\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(i));\n\t\tif (!cfs_rq)\n\t\t\tgoto err;\n\n\t\tse = kzalloc_node(sizeof(struct sched_entity),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(i));\n\t\tif (!se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_tg_cfs_entry(tg, cfs_rq, se, i, 0, parent->se[i]);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(cfs_rq);\nerr:\n\treturn 0;\n}\n\nstatic inline void register_fair_sched_group(struct task_group *tg, int cpu)\n{\n\tlist_add_rcu(&tg->cfs_rq[cpu]->leaf_cfs_rq_list,\n\t\t\t&cpu_rq(cpu)->leaf_cfs_rq_list);\n}\n\nstatic inline void unregister_fair_sched_group(struct task_group *tg, int cpu)\n{\n\tlist_del_rcu(&tg->cfs_rq[cpu]->leaf_cfs_rq_list);\n}\n#else /* !CONFG_FAIR_GROUP_SCHED */\nstatic inline void free_fair_sched_group(struct task_group *tg)\n{\n}\n\nstatic inline\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nstatic inline void register_fair_sched_group(struct task_group *tg, int cpu)\n{\n}\n\nstatic inline void unregister_fair_sched_group(struct task_group *tg, int cpu)\n{\n}\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void free_rt_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tdestroy_rt_bandwidth(&tg->rt_bandwidth);\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->rt_rq)\n\t\t\tkfree(tg->rt_rq[i]);\n\t\tif (tg->rt_se)\n\t\t\tkfree(tg->rt_se[i]);\n\t}\n\n\tkfree(tg->rt_rq);\n\tkfree(tg->rt_se);\n}\n\nstatic\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct rt_rq *rt_rq;\n\tstruct sched_rt_entity *rt_se;\n\tstruct rq *rq;\n\tint i;\n\n\ttg->rt_rq = kzalloc(sizeof(rt_rq) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->rt_rq)\n\t\tgoto err;\n\ttg->rt_se = kzalloc(sizeof(rt_se) * nr_cpu_ids, GFP_KERNEL);\n\tif (!tg->rt_se)\n\t\tgoto err;\n\n\tinit_rt_bandwidth(&tg->rt_bandwidth,\n\t\t\tktime_to_ns(def_rt_bandwidth.rt_period), 0);\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\n\t\trt_rq = kzalloc_node(sizeof(struct rt_rq),\n\t\t\t\t     GFP_KERNEL, cpu_to_node(i));\n\t\tif (!rt_rq)\n\t\t\tgoto err;\n\n\t\trt_se = kzalloc_node(sizeof(struct sched_rt_entity),\n\t\t\t\t     GFP_KERNEL, cpu_to_node(i));\n\t\tif (!rt_se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_tg_rt_entry(tg, rt_rq, rt_se, i, 0, parent->rt_se[i]);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(rt_rq);\nerr:\n\treturn 0;\n}\n\nstatic inline void register_rt_sched_group(struct task_group *tg, int cpu)\n{\n\tlist_add_rcu(&tg->rt_rq[cpu]->leaf_rt_rq_list,\n\t\t\t&cpu_rq(cpu)->leaf_rt_rq_list);\n}\n\nstatic inline void unregister_rt_sched_group(struct task_group *tg, int cpu)\n{\n\tlist_del_rcu(&tg->rt_rq[cpu]->leaf_rt_rq_list);\n}\n#else /* !CONFIG_RT_GROUP_SCHED */\nstatic inline void free_rt_sched_group(struct task_group *tg)\n{\n}\n\nstatic inline\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nstatic inline void register_rt_sched_group(struct task_group *tg, int cpu)\n{\n}\n\nstatic inline void unregister_rt_sched_group(struct task_group *tg, int cpu)\n{\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_SCHED\nstatic void free_sched_group(struct task_group *tg)\n{\n\tfree_fair_sched_group(tg);\n\tfree_rt_sched_group(tg);\n\tkfree(tg);\n}\n\n/* allocate runqueue etc for a new task group */\nstruct task_group *sched_create_group(struct task_group *parent)\n{\n\tstruct task_group *tg;\n\tunsigned long flags;\n\tint i;\n\n\ttg = kzalloc(sizeof(*tg), GFP_KERNEL);\n\tif (!tg)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!alloc_fair_sched_group(tg, parent))\n\t\tgoto err;\n\n\tif (!alloc_rt_sched_group(tg, parent))\n\t\tgoto err;\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tfor_each_possible_cpu(i) {\n\t\tregister_fair_sched_group(tg, i);\n\t\tregister_rt_sched_group(tg, i);\n\t}\n\tlist_add_rcu(&tg->list, &task_groups);\n\n\tWARN_ON(!parent); /* root should already exist */\n\n\ttg->parent = parent;\n\tINIT_LIST_HEAD(&tg->children);\n\tlist_add_rcu(&tg->siblings, &parent->children);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\treturn tg;\n\nerr:\n\tfree_sched_group(tg);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/* rcu callback to free various structures associated with a task group */\nstatic void free_sched_group_rcu(struct rcu_head *rhp)\n{\n\t/* now it should be safe to free those cfs_rqs */\n\tfree_sched_group(container_of(rhp, struct task_group, rcu));\n}\n\n/* Destroy runqueue etc associated with a task group */\nvoid sched_destroy_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\tint i;\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tfor_each_possible_cpu(i) {\n\t\tunregister_fair_sched_group(tg, i);\n\t\tunregister_rt_sched_group(tg, i);\n\t}\n\tlist_del_rcu(&tg->list);\n\tlist_del_rcu(&tg->siblings);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\t/* wait for possible concurrent references to cfs_rqs complete */\n\tcall_rcu(&tg->rcu, free_sched_group_rcu);\n}\n\n/* change task's runqueue when it moves between groups.\n *\tThe caller of this function should have put the task in its new group\n *\tby now. This function just updates tsk->se.cfs_rq and tsk->se.parent to\n *\treflect its new group.\n */\nvoid sched_move_task(struct task_struct *tsk)\n{\n\tint on_rq, running;\n\tunsigned long flags;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(tsk, &flags);\n\n\trunning = task_current(rq, tsk);\n\ton_rq = tsk->se.on_rq;\n\n\tif (on_rq)\n\t\tdequeue_task(rq, tsk, 0);\n\tif (unlikely(running))\n\t\ttsk->sched_class->put_prev_task(rq, tsk);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (tsk->sched_class->task_move_group)\n\t\ttsk->sched_class->task_move_group(tsk, on_rq);\n\telse\n#endif\n\t\tset_task_rq(tsk, task_cpu(tsk));\n\n\tif (unlikely(running))\n\t\ttsk->sched_class->set_curr_task(rq);\n\tif (on_rq)\n\t\tenqueue_task(rq, tsk, 0);\n\n\ttask_rq_unlock(rq, &flags);\n}\n#endif /* CONFIG_CGROUP_SCHED */\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void __set_se_shares(struct sched_entity *se, unsigned long shares)\n{\n\tstruct cfs_rq *cfs_rq = se->cfs_rq;\n\tint on_rq;\n\n\ton_rq = se->on_rq;\n\tif (on_rq)\n\t\tdequeue_entity(cfs_rq, se, 0);\n\n\tse->load.weight = shares;\n\tse->load.inv_weight = 0;\n\n\tif (on_rq)\n\t\tenqueue_entity(cfs_rq, se, 0);\n}\n\nstatic void set_se_shares(struct sched_entity *se, unsigned long shares)\n{\n\tstruct cfs_rq *cfs_rq = se->cfs_rq;\n\tstruct rq *rq = cfs_rq->rq;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&rq->lock, flags);\n\t__set_se_shares(se, shares);\n\traw_spin_unlock_irqrestore(&rq->lock, flags);\n}\n\nstatic DEFINE_MUTEX(shares_mutex);\n\nint sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\tunsigned long flags;\n\n\t/*\n\t * We can't change the weight of the root cgroup.\n\t */\n\tif (!tg->se[0])\n\t\treturn -EINVAL;\n\n\tif (shares < MIN_SHARES)\n\t\tshares = MIN_SHARES;\n\telse if (shares > MAX_SHARES)\n\t\tshares = MAX_SHARES;\n\n\tmutex_lock(&shares_mutex);\n\tif (tg->shares == shares)\n\t\tgoto done;\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tfor_each_possible_cpu(i)\n\t\tunregister_fair_sched_group(tg, i);\n\tlist_del_rcu(&tg->siblings);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\t/* wait for any ongoing reference to this group to finish */\n\tsynchronize_sched();\n\n\t/*\n\t * Now we are free to modify the group's share on each cpu\n\t * w/o tripping rebalance_share or load_balance_fair.\n\t */\n\ttg->shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\t/*\n\t\t * force a rebalance\n\t\t */\n\t\tcfs_rq_set_shares(tg->cfs_rq[i], 0);\n\t\tset_se_shares(tg->se[i], shares);\n\t}\n\n\t/*\n\t * Enable load balance activity on this group, by inserting it back on\n\t * each cpu's rq->leaf_cfs_rq_list.\n\t */\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tfor_each_possible_cpu(i)\n\t\tregister_fair_sched_group(tg, i);\n\tlist_add_rcu(&tg->siblings, &tg->parent->children);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\ndone:\n\tmutex_unlock(&shares_mutex);\n\treturn 0;\n}\n\nunsigned long sched_group_shares(struct task_group *tg)\n{\n\treturn tg->shares;\n}\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n/*\n * Ensure that the real time constraints are schedulable.\n */\nstatic DEFINE_MUTEX(rt_constraints_mutex);\n\nstatic unsigned long to_ratio(u64 period, u64 runtime)\n{\n\tif (runtime == RUNTIME_INF)\n\t\treturn 1ULL << 20;\n\n\treturn div64_u64(runtime << 20, period);\n}\n\n/* Must be called with tasklist_lock held */\nstatic inline int tg_has_rt_tasks(struct task_group *tg)\n{\n\tstruct task_struct *g, *p;\n\n\tdo_each_thread(g, p) {\n\t\tif (rt_task(p) && rt_rq_of_se(&p->rt)->tg == tg)\n\t\t\treturn 1;\n\t} while_each_thread(g, p);\n\n\treturn 0;\n}\n\nstruct rt_schedulable_data {\n\tstruct task_group *tg;\n\tu64 rt_period;\n\tu64 rt_runtime;\n};\n\nstatic int tg_schedulable(struct task_group *tg, void *data)\n{\n\tstruct rt_schedulable_data *d = data;\n\tstruct task_group *child;\n\tunsigned long total, sum = 0;\n\tu64 period, runtime;\n\n\tperiod = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\truntime = tg->rt_bandwidth.rt_runtime;\n\n\tif (tg == d->tg) {\n\t\tperiod = d->rt_period;\n\t\truntime = d->rt_runtime;\n\t}\n\n\t/*\n\t * Cannot have more runtime than the period.\n\t */\n\tif (runtime > period && runtime != RUNTIME_INF)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Ensure we don't starve existing RT tasks.\n\t */\n\tif (rt_bandwidth_enabled() && !runtime && tg_has_rt_tasks(tg))\n\t\treturn -EBUSY;\n\n\ttotal = to_ratio(period, runtime);\n\n\t/*\n\t * Nobody can have more than the global setting allows.\n\t */\n\tif (total > to_ratio(global_rt_period(), global_rt_runtime()))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The sum of our children's runtime should not exceed our own.\n\t */\n\tlist_for_each_entry_rcu(child, &tg->children, siblings) {\n\t\tperiod = ktime_to_ns(child->rt_bandwidth.rt_period);\n\t\truntime = child->rt_bandwidth.rt_runtime;\n\n\t\tif (child == d->tg) {\n\t\t\tperiod = d->rt_period;\n\t\t\truntime = d->rt_runtime;\n\t\t}\n\n\t\tsum += to_ratio(period, runtime);\n\t}\n\n\tif (sum > total)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int __rt_schedulable(struct task_group *tg, u64 period, u64 runtime)\n{\n\tstruct rt_schedulable_data data = {\n\t\t.tg = tg,\n\t\t.rt_period = period,\n\t\t.rt_runtime = runtime,\n\t};\n\n\treturn walk_tg_tree(tg_schedulable, tg_nop, &data);\n}\n\nstatic int tg_set_bandwidth(struct task_group *tg,\n\t\tu64 rt_period, u64 rt_runtime)\n{\n\tint i, err = 0;\n\n\tmutex_lock(&rt_constraints_mutex);\n\tread_lock(&tasklist_lock);\n\terr = __rt_schedulable(tg, rt_period, rt_runtime);\n\tif (err)\n\t\tgoto unlock;\n\n\traw_spin_lock_irq(&tg->rt_bandwidth.rt_runtime_lock);\n\ttg->rt_bandwidth.rt_period = ns_to_ktime(rt_period);\n\ttg->rt_bandwidth.rt_runtime = rt_runtime;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rt_rq *rt_rq = tg->rt_rq[i];\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = rt_runtime;\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t}\n\traw_spin_unlock_irq(&tg->rt_bandwidth.rt_runtime_lock);\nunlock:\n\tread_unlock(&tasklist_lock);\n\tmutex_unlock(&rt_constraints_mutex);\n\n\treturn err;\n}\n\nint sched_group_set_rt_runtime(struct task_group *tg, long rt_runtime_us)\n{\n\tu64 rt_runtime, rt_period;\n\n\trt_period = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\trt_runtime = (u64)rt_runtime_us * NSEC_PER_USEC;\n\tif (rt_runtime_us < 0)\n\t\trt_runtime = RUNTIME_INF;\n\n\treturn tg_set_bandwidth(tg, rt_period, rt_runtime);\n}\n\nlong sched_group_rt_runtime(struct task_group *tg)\n{\n\tu64 rt_runtime_us;\n\n\tif (tg->rt_bandwidth.rt_runtime == RUNTIME_INF)\n\t\treturn -1;\n\n\trt_runtime_us = tg->rt_bandwidth.rt_runtime;\n\tdo_div(rt_runtime_us, NSEC_PER_USEC);\n\treturn rt_runtime_us;\n}\n\nint sched_group_set_rt_period(struct task_group *tg, long rt_period_us)\n{\n\tu64 rt_runtime, rt_period;\n\n\trt_period = (u64)rt_period_us * NSEC_PER_USEC;\n\trt_runtime = tg->rt_bandwidth.rt_runtime;\n\n\tif (rt_period == 0)\n\t\treturn -EINVAL;\n\n\treturn tg_set_bandwidth(tg, rt_period, rt_runtime);\n}\n\nlong sched_group_rt_period(struct task_group *tg)\n{\n\tu64 rt_period_us;\n\n\trt_period_us = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\tdo_div(rt_period_us, NSEC_PER_USEC);\n\treturn rt_period_us;\n}\n\nstatic int sched_rt_global_constraints(void)\n{\n\tu64 runtime, period;\n\tint ret = 0;\n\n\tif (sysctl_sched_rt_period <= 0)\n\t\treturn -EINVAL;\n\n\truntime = global_rt_runtime();\n\tperiod = global_rt_period();\n\n\t/*\n\t * Sanity check on the sysctl variables.\n\t */\n\tif (runtime > period && runtime != RUNTIME_INF)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rt_constraints_mutex);\n\tread_lock(&tasklist_lock);\n\tret = __rt_schedulable(NULL, 0, 0);\n\tread_unlock(&tasklist_lock);\n\tmutex_unlock(&rt_constraints_mutex);\n\n\treturn ret;\n}\n\nint sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk)\n{\n\t/* Don't accept realtime tasks when there is no way for them to run */\n\tif (rt_task(tsk) && tg->rt_bandwidth.rt_runtime == 0)\n\t\treturn 0;\n\n\treturn 1;\n}\n\n#else /* !CONFIG_RT_GROUP_SCHED */\nstatic int sched_rt_global_constraints(void)\n{\n\tunsigned long flags;\n\tint i;\n\n\tif (sysctl_sched_rt_period <= 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * There's always some RT tasks in the root group\n\t * -- migration, kstopmachine etc..\n\t */\n\tif (sysctl_sched_rt_runtime == 0)\n\t\treturn -EBUSY;\n\n\traw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);\n\tfor_each_possible_cpu(i) {\n\t\tstruct rt_rq *rt_rq = &cpu_rq(i)->rt;\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = global_rt_runtime();\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t}\n\traw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);\n\n\treturn 0;\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\nint sched_rt_handler(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp,\n\t\tloff_t *ppos)\n{\n\tint ret;\n\tint old_period, old_runtime;\n\tstatic DEFINE_MUTEX(mutex);\n\n\tmutex_lock(&mutex);\n\told_period = sysctl_sched_rt_period;\n\told_runtime = sysctl_sched_rt_runtime;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\tif (!ret && write) {\n\t\tret = sched_rt_global_constraints();\n\t\tif (ret) {\n\t\t\tsysctl_sched_rt_period = old_period;\n\t\t\tsysctl_sched_rt_runtime = old_runtime;\n\t\t} else {\n\t\t\tdef_rt_bandwidth.rt_runtime = global_rt_runtime();\n\t\t\tdef_rt_bandwidth.rt_period =\n\t\t\t\tns_to_ktime(global_rt_period());\n\t\t}\n\t}\n\tmutex_unlock(&mutex);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_CGROUP_SCHED\n\n/* return corresponding task_group object of a cgroup */\nstatic inline struct task_group *cgroup_tg(struct cgroup *cgrp)\n{\n\treturn container_of(cgroup_subsys_state(cgrp, cpu_cgroup_subsys_id),\n\t\t\t    struct task_group, css);\n}\n\nstatic struct cgroup_subsys_state *\ncpu_cgroup_create(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct task_group *tg, *parent;\n\n\tif (!cgrp->parent) {\n\t\t/* This is early initialization for the top cgroup */\n\t\treturn &init_task_group.css;\n\t}\n\n\tparent = cgroup_tg(cgrp->parent);\n\ttg = sched_create_group(parent);\n\tif (IS_ERR(tg))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn &tg->css;\n}\n\nstatic void\ncpu_cgroup_destroy(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct task_group *tg = cgroup_tg(cgrp);\n\n\tsched_destroy_group(tg);\n}\n\nstatic int\ncpu_cgroup_can_attach_task(struct cgroup *cgrp, struct task_struct *tsk)\n{\n#ifdef CONFIG_RT_GROUP_SCHED\n\tif (!sched_rt_can_attach(cgroup_tg(cgrp), tsk))\n\t\treturn -EINVAL;\n#else\n\t/* We don't support RT-tasks being in separate groups */\n\tif (tsk->sched_class != &fair_sched_class)\n\t\treturn -EINVAL;\n#endif\n\treturn 0;\n}\n\nstatic int\ncpu_cgroup_can_attach(struct cgroup_subsys *ss, struct cgroup *cgrp,\n\t\t      struct task_struct *tsk, bool threadgroup)\n{\n\tint retval = cpu_cgroup_can_attach_task(cgrp, tsk);\n\tif (retval)\n\t\treturn retval;\n\tif (threadgroup) {\n\t\tstruct task_struct *c;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(c, &tsk->thread_group, thread_group) {\n\t\t\tretval = cpu_cgroup_can_attach_task(cgrp, c);\n\t\t\tif (retval) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn retval;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\treturn 0;\n}\n\nstatic void\ncpu_cgroup_attach(struct cgroup_subsys *ss, struct cgroup *cgrp,\n\t\t  struct cgroup *old_cont, struct task_struct *tsk,\n\t\t  bool threadgroup)\n{\n\tsched_move_task(tsk);\n\tif (threadgroup) {\n\t\tstruct task_struct *c;\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(c, &tsk->thread_group, thread_group) {\n\t\t\tsched_move_task(c);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic int cpu_shares_write_u64(struct cgroup *cgrp, struct cftype *cftype,\n\t\t\t\tu64 shareval)\n{\n\treturn sched_group_set_shares(cgroup_tg(cgrp), shareval);\n}\n\nstatic u64 cpu_shares_read_u64(struct cgroup *cgrp, struct cftype *cft)\n{\n\tstruct task_group *tg = cgroup_tg(cgrp);\n\n\treturn (u64) tg->shares;\n}\n#endif /* CONFIG_FAIR_GROUP_SCHED */\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic int cpu_rt_runtime_write(struct cgroup *cgrp, struct cftype *cft,\n\t\t\t\ts64 val)\n{\n\treturn sched_group_set_rt_runtime(cgroup_tg(cgrp), val);\n}\n\nstatic s64 cpu_rt_runtime_read(struct cgroup *cgrp, struct cftype *cft)\n{\n\treturn sched_group_rt_runtime(cgroup_tg(cgrp));\n}\n\nstatic int cpu_rt_period_write_uint(struct cgroup *cgrp, struct cftype *cftype,\n\t\tu64 rt_period_us)\n{\n\treturn sched_group_set_rt_period(cgroup_tg(cgrp), rt_period_us);\n}\n\nstatic u64 cpu_rt_period_read_uint(struct cgroup *cgrp, struct cftype *cft)\n{\n\treturn sched_group_rt_period(cgroup_tg(cgrp));\n}\n#endif /* CONFIG_RT_GROUP_SCHED */\n\nstatic struct cftype cpu_files[] = {\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t{\n\t\t.name = \"shares\",\n\t\t.read_u64 = cpu_shares_read_u64,\n\t\t.write_u64 = cpu_shares_write_u64,\n\t},\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\t{\n\t\t.name = \"rt_runtime_us\",\n\t\t.read_s64 = cpu_rt_runtime_read,\n\t\t.write_s64 = cpu_rt_runtime_write,\n\t},\n\t{\n\t\t.name = \"rt_period_us\",\n\t\t.read_u64 = cpu_rt_period_read_uint,\n\t\t.write_u64 = cpu_rt_period_write_uint,\n\t},\n#endif\n};\n\nstatic int cpu_cgroup_populate(struct cgroup_subsys *ss, struct cgroup *cont)\n{\n\treturn cgroup_add_files(cont, ss, cpu_files, ARRAY_SIZE(cpu_files));\n}\n\nstruct cgroup_subsys cpu_cgroup_subsys = {\n\t.name\t\t= \"cpu\",\n\t.create\t\t= cpu_cgroup_create,\n\t.destroy\t= cpu_cgroup_destroy,\n\t.can_attach\t= cpu_cgroup_can_attach,\n\t.attach\t\t= cpu_cgroup_attach,\n\t.populate\t= cpu_cgroup_populate,\n\t.subsys_id\t= cpu_cgroup_subsys_id,\n\t.early_init\t= 1,\n};\n\n#endif\t/* CONFIG_CGROUP_SCHED */\n\n#ifdef CONFIG_CGROUP_CPUACCT\n\n/*\n * CPU accounting code for task groups.\n *\n * Based on the work by Paul Menage (menage@google.com) and Balbir Singh\n * (balbir@in.ibm.com).\n */\n\n/* track cpu usage of a group of tasks and its child groups */\nstruct cpuacct {\n\tstruct cgroup_subsys_state css;\n\t/* cpuusage holds pointer to a u64-type object on every cpu */\n\tu64 __percpu *cpuusage;\n\tstruct percpu_counter cpustat[CPUACCT_STAT_NSTATS];\n\tstruct cpuacct *parent;\n};\n\nstruct cgroup_subsys cpuacct_subsys;\n\n/* return cpu accounting group corresponding to this container */\nstatic inline struct cpuacct *cgroup_ca(struct cgroup *cgrp)\n{\n\treturn container_of(cgroup_subsys_state(cgrp, cpuacct_subsys_id),\n\t\t\t    struct cpuacct, css);\n}\n\n/* return cpu accounting group to which this task belongs */\nstatic inline struct cpuacct *task_ca(struct task_struct *tsk)\n{\n\treturn container_of(task_subsys_state(tsk, cpuacct_subsys_id),\n\t\t\t    struct cpuacct, css);\n}\n\n/* create a new cpu accounting group */\nstatic struct cgroup_subsys_state *cpuacct_create(\n\tstruct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct cpuacct *ca = kzalloc(sizeof(*ca), GFP_KERNEL);\n\tint i;\n\n\tif (!ca)\n\t\tgoto out;\n\n\tca->cpuusage = alloc_percpu(u64);\n\tif (!ca->cpuusage)\n\t\tgoto out_free_ca;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++)\n\t\tif (percpu_counter_init(&ca->cpustat[i], 0))\n\t\t\tgoto out_free_counters;\n\n\tif (cgrp->parent)\n\t\tca->parent = cgroup_ca(cgrp->parent);\n\n\treturn &ca->css;\n\nout_free_counters:\n\twhile (--i >= 0)\n\t\tpercpu_counter_destroy(&ca->cpustat[i]);\n\tfree_percpu(ca->cpuusage);\nout_free_ca:\n\tkfree(ca);\nout:\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/* destroy an existing cpu accounting group */\nstatic void\ncpuacct_destroy(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint i;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++)\n\t\tpercpu_counter_destroy(&ca->cpustat[i]);\n\tfree_percpu(ca->cpuusage);\n\tkfree(ca);\n}\n\nstatic u64 cpuacct_cpuusage_read(struct cpuacct *ca, int cpu)\n{\n\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\tu64 data;\n\n#ifndef CONFIG_64BIT\n\t/*\n\t * Take rq->lock to make 64-bit read safe on 32-bit platforms.\n\t */\n\traw_spin_lock_irq(&cpu_rq(cpu)->lock);\n\tdata = *cpuusage;\n\traw_spin_unlock_irq(&cpu_rq(cpu)->lock);\n#else\n\tdata = *cpuusage;\n#endif\n\n\treturn data;\n}\n\nstatic void cpuacct_cpuusage_write(struct cpuacct *ca, int cpu, u64 val)\n{\n\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\n#ifndef CONFIG_64BIT\n\t/*\n\t * Take rq->lock to make 64-bit write safe on 32-bit platforms.\n\t */\n\traw_spin_lock_irq(&cpu_rq(cpu)->lock);\n\t*cpuusage = val;\n\traw_spin_unlock_irq(&cpu_rq(cpu)->lock);\n#else\n\t*cpuusage = val;\n#endif\n}\n\n/* return total cpu usage (in nanoseconds) of a group */\nstatic u64 cpuusage_read(struct cgroup *cgrp, struct cftype *cft)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tu64 totalcpuusage = 0;\n\tint i;\n\n\tfor_each_present_cpu(i)\n\t\ttotalcpuusage += cpuacct_cpuusage_read(ca, i);\n\n\treturn totalcpuusage;\n}\n\nstatic int cpuusage_write(struct cgroup *cgrp, struct cftype *cftype,\n\t\t\t\t\t\t\t\tu64 reset)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint err = 0;\n\tint i;\n\n\tif (reset) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tfor_each_present_cpu(i)\n\t\tcpuacct_cpuusage_write(ca, i, 0);\n\nout:\n\treturn err;\n}\n\nstatic int cpuacct_percpu_seq_read(struct cgroup *cgroup, struct cftype *cft,\n\t\t\t\t   struct seq_file *m)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgroup);\n\tu64 percpu;\n\tint i;\n\n\tfor_each_present_cpu(i) {\n\t\tpercpu = cpuacct_cpuusage_read(ca, i);\n\t\tseq_printf(m, \"%llu \", (unsigned long long) percpu);\n\t}\n\tseq_printf(m, \"\\n\");\n\treturn 0;\n}\n\nstatic const char *cpuacct_stat_desc[] = {\n\t[CPUACCT_STAT_USER] = \"user\",\n\t[CPUACCT_STAT_SYSTEM] = \"system\",\n};\n\nstatic int cpuacct_stats_show(struct cgroup *cgrp, struct cftype *cft,\n\t\tstruct cgroup_map_cb *cb)\n{\n\tstruct cpuacct *ca = cgroup_ca(cgrp);\n\tint i;\n\n\tfor (i = 0; i < CPUACCT_STAT_NSTATS; i++) {\n\t\ts64 val = percpu_counter_read(&ca->cpustat[i]);\n\t\tval = cputime64_to_clock_t(val);\n\t\tcb->fill(cb, cpuacct_stat_desc[i], val);\n\t}\n\treturn 0;\n}\n\nstatic struct cftype files[] = {\n\t{\n\t\t.name = \"usage\",\n\t\t.read_u64 = cpuusage_read,\n\t\t.write_u64 = cpuusage_write,\n\t},\n\t{\n\t\t.name = \"usage_percpu\",\n\t\t.read_seq_string = cpuacct_percpu_seq_read,\n\t},\n\t{\n\t\t.name = \"stat\",\n\t\t.read_map = cpuacct_stats_show,\n\t},\n};\n\nstatic int cpuacct_populate(struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\treturn cgroup_add_files(cgrp, ss, files, ARRAY_SIZE(files));\n}\n\n/*\n * charge this task's execution time to its accounting group.\n *\n * called with rq->lock held.\n */\nstatic void cpuacct_charge(struct task_struct *tsk, u64 cputime)\n{\n\tstruct cpuacct *ca;\n\tint cpu;\n\n\tif (unlikely(!cpuacct_subsys.active))\n\t\treturn;\n\n\tcpu = task_cpu(tsk);\n\n\trcu_read_lock();\n\n\tca = task_ca(tsk);\n\n\tfor (; ca; ca = ca->parent) {\n\t\tu64 *cpuusage = per_cpu_ptr(ca->cpuusage, cpu);\n\t\t*cpuusage += cputime;\n\t}\n\n\trcu_read_unlock();\n}\n\n/*\n * When CONFIG_VIRT_CPU_ACCOUNTING is enabled one jiffy can be very large\n * in cputime_t units. As a result, cpuacct_update_stats calls\n * percpu_counter_add with values large enough to always overflow the\n * per cpu batch limit causing bad SMP scalability.\n *\n * To fix this we scale percpu_counter_batch by cputime_one_jiffy so we\n * batch the same amount of time with CONFIG_VIRT_CPU_ACCOUNTING disabled\n * and enabled. We cap it at INT_MAX which is the largest allowed batch value.\n */\n#ifdef CONFIG_SMP\n#define CPUACCT_BATCH\t\\\n\tmin_t(long, percpu_counter_batch * cputime_one_jiffy, INT_MAX)\n#else\n#define CPUACCT_BATCH\t0\n#endif\n\n/*\n * Charge the system/user time to the task's accounting group.\n */\nstatic void cpuacct_update_stats(struct task_struct *tsk,\n\t\tenum cpuacct_stat_index idx, cputime_t val)\n{\n\tstruct cpuacct *ca;\n\tint batch = CPUACCT_BATCH;\n\n\tif (unlikely(!cpuacct_subsys.active))\n\t\treturn;\n\n\trcu_read_lock();\n\tca = task_ca(tsk);\n\n\tdo {\n\t\t__percpu_counter_add(&ca->cpustat[idx], val, batch);\n\t\tca = ca->parent;\n\t} while (ca);\n\trcu_read_unlock();\n}\n\nstruct cgroup_subsys cpuacct_subsys = {\n\t.name = \"cpuacct\",\n\t.create = cpuacct_create,\n\t.destroy = cpuacct_destroy,\n\t.populate = cpuacct_populate,\n\t.subsys_id = cpuacct_subsys_id,\n};\n#endif\t/* CONFIG_CGROUP_CPUACCT */\n\n#ifndef CONFIG_SMP\n\nvoid synchronize_sched_expedited(void)\n{\n\tbarrier();\n}\nEXPORT_SYMBOL_GPL(synchronize_sched_expedited);\n\n#else /* #ifndef CONFIG_SMP */\n\nstatic atomic_t synchronize_sched_expedited_count = ATOMIC_INIT(0);\n\nstatic int synchronize_sched_expedited_cpu_stop(void *data)\n{\n\t/*\n\t * There must be a full memory barrier on each affected CPU\n\t * between the time that try_stop_cpus() is called and the\n\t * time that it returns.\n\t *\n\t * In the current initial implementation of cpu_stop, the\n\t * above condition is already met when the control reaches\n\t * this point and the following smp_mb() is not strictly\n\t * necessary.  Do smp_mb() anyway for documentation and\n\t * robustness against future implementation changes.\n\t */\n\tsmp_mb(); /* See above comment block. */\n\treturn 0;\n}\n\n/*\n * Wait for an rcu-sched grace period to elapse, but use \"big hammer\"\n * approach to force grace period to end quickly.  This consumes\n * significant time on all CPUs, and is thus not recommended for\n * any sort of common-case code.\n *\n * Note that it is illegal to call this function while holding any\n * lock that is acquired by a CPU-hotplug notifier.  Failing to\n * observe this restriction will result in deadlock.\n */\nvoid synchronize_sched_expedited(void)\n{\n\tint snap, trycount = 0;\n\n\tsmp_mb();  /* ensure prior mod happens before capturing snap. */\n\tsnap = atomic_read(&synchronize_sched_expedited_count) + 1;\n\tget_online_cpus();\n\twhile (try_stop_cpus(cpu_online_mask,\n\t\t\t     synchronize_sched_expedited_cpu_stop,\n\t\t\t     NULL) == -EAGAIN) {\n\t\tput_online_cpus();\n\t\tif (trycount++ < 10)\n\t\t\tudelay(trycount * num_online_cpus());\n\t\telse {\n\t\t\tsynchronize_sched();\n\t\t\treturn;\n\t\t}\n\t\tif (atomic_read(&synchronize_sched_expedited_count) - snap > 0) {\n\t\t\tsmp_mb(); /* ensure test happens before caller kfree */\n\t\t\treturn;\n\t\t}\n\t\tget_online_cpus();\n\t}\n\tatomic_inc(&synchronize_sched_expedited_count);\n\tsmp_mb__after_atomic_inc(); /* ensure post-GP actions seen after GP. */\n\tput_online_cpus();\n}\nEXPORT_SYMBOL_GPL(synchronize_sched_expedited);\n\n#endif /* #else #ifndef CONFIG_SMP */\n"], "filenames": ["kernel/fork.c", "kernel/sched.c"], "buggy_code_start_loc": [275, 644], "buggy_code_end_loc": [275, 4073], "fixing_code_start_loc": [276, 644], "fixing_code_end_loc": [277, 4076], "type": "CWE-835", "message": "The Linux kernel before 2.6.37 does not properly implement a certain clock-update optimization, which allows local users to cause a denial of service (system hang) via an application that executes code in a loop.", "other": {"cve": {"id": "CVE-2011-4621", "sourceIdentifier": "secalert@redhat.com", "published": "2012-05-17T11:00:35.617", "lastModified": "2023-02-13T03:23:59.627", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Linux kernel before 2.6.37 does not properly implement a certain clock-update optimization, which allows local users to cause a denial of service (system hang) via an application that executes code in a loop."}, {"lang": "es", "value": "El kernel de Linux antes de v2.6.37 no aplica la optimizaci\u00f3n de una actualizaci\u00f3n de reloj, lo que permite a usuarios locales provocar una denegaci\u00f3n de servicio (bloqueo del sistema) a trav\u00e9s de una aplicaci\u00f3n que ejecuta c\u00f3digo en un bucle."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-835"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.37", "matchCriteriaId": "76630B45-B590-4651-972E-F938A83010C0"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=f26f9aff6aaf67e9a430d16c266f91b13a5bff64", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/12/21/6", "source": "secalert@redhat.com", "tags": ["Exploit", "Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=769711", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/f26f9aff6aaf67e9a430d16c266f91b13a5bff64", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/f26f9aff6aaf67e9a430d16c266f91b13a5bff64"}}