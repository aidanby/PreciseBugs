{"buggy_code": ["__author__ = \"Gina H\u00e4u\u00dfge <osd@foosel.net>\"\n__license__ = \"GNU Affero General Public License http://www.gnu.org/licenses/agpl.html\"\n__copyright__ = \"Copyright (C) 2014 The OctoPrint Project - Released under terms of the AGPLv3 License\"\n\nimport copy\nimport logging\nimport os\nimport shutil\nfrom contextlib import contextmanager\nfrom os import scandir, walk\n\nimport pylru\n\nimport octoprint.filemanager\nfrom octoprint.util import (\n    atomic_write,\n    is_hidden_path,\n    time_this,\n    to_bytes,\n    to_unicode,\n    yaml,\n)\nfrom octoprint.util.files import sanitize_filename\n\n\nclass StorageInterface:\n    \"\"\"\n    Interface of storage adapters for OctoPrint.\n    \"\"\"\n\n    # noinspection PyUnreachableCode\n    @property\n    def analysis_backlog(self):\n        \"\"\"\n        Get an iterator over all items stored in the storage that need to be analysed by the :class:`~octoprint.filemanager.AnalysisQueue`.\n\n        The yielded elements are expected as storage specific absolute paths to the respective files. Don't forget\n        to recurse into folders if your storage adapter supports those.\n\n        :return: an iterator yielding all un-analysed files in the storage\n        \"\"\"\n        # empty generator pattern, yield is intentionally unreachable\n        return\n        yield\n\n    # noinspection PyUnreachableCode\n    def analysis_backlog_for_path(self, path=None):\n        # empty generator pattern, yield is intentionally unreachable\n        return\n        yield\n\n    def last_modified(self, path=None, recursive=False):\n        \"\"\"\n        Get the last modification date of the specified ``path`` or ``path``'s subtree.\n\n        Args:\n            path (str or None): Path for which to determine the subtree's last modification date. If left out or\n                set to None, defatuls to storage root.\n            recursive (bool): Whether to determine only the date of the specified ``path`` (False, default) or\n                the whole ``path``'s subtree (True).\n\n        Returns: (float) The last modification date of the indicated subtree\n        \"\"\"\n        raise NotImplementedError()\n\n    def file_in_path(self, path, filepath):\n        \"\"\"\n        Returns whether the file indicated by ``file`` is inside ``path`` or not.\n        :param string path: the path to check\n        :param string filepath: path to the file\n        :return: ``True`` if the file is inside the path, ``False`` otherwise\n        \"\"\"\n        return NotImplementedError()\n\n    def file_exists(self, path):\n        \"\"\"\n        Returns whether the file indicated by ``path`` exists or not.\n        :param string path: the path to check for existence\n        :return: ``True`` if the file exists, ``False`` otherwise\n        \"\"\"\n        raise NotImplementedError()\n\n    def folder_exists(self, path):\n        \"\"\"\n        Returns whether the folder indicated by ``path`` exists or not.\n        :param string path: the path to check for existence\n        :return: ``True`` if the folder exists, ``False`` otherwise\n        \"\"\"\n        raise NotImplementedError()\n\n    def list_files(\n        self, path=None, filter=None, recursive=True, level=0, force_refresh=False\n    ):\n        \"\"\"\n        List all files in storage starting at ``path``. If ``recursive`` is set to True (the default), also dives into\n        subfolders.\n\n        An optional filter function can be supplied which will be called with a file name and file data and which has\n        to return True if the file is to be included in the result or False if not.\n\n        The data structure of the returned result will be a dictionary mapping from file names to entry data. File nodes\n        will contain their metadata here, folder nodes will contain their contained files and folders. Example::\n\n           {\n             \"some_folder\": {\n               \"name\": \"some_folder\",\n               \"path\": \"some_folder\",\n               \"type\": \"folder\",\n               \"children\": {\n                 \"some_sub_folder\": {\n                   \"name\": \"some_sub_folder\",\n                   \"path\": \"some_folder/some_sub_folder\",\n                   \"type\": \"folder\",\n                   \"typePath\": [\"folder\"],\n                   \"children\": { ... }\n                 },\n                 \"some_file.gcode\": {\n                   \"name\": \"some_file.gcode\",\n                   \"path\": \"some_folder/some_file.gcode\",\n                   \"type\": \"machinecode\",\n                   \"typePath\": [\"machinecode\", \"gcode\"],\n                   \"hash\": \"<sha1 hash>\",\n                   \"links\": [ ... ],\n                   ...\n                 },\n                 ...\n               }\n             \"test.gcode\": {\n               \"name\": \"test.gcode\",\n               \"path\": \"test.gcode\",\n               \"type\": \"machinecode\",\n               \"typePath\": [\"machinecode\", \"gcode\"],\n               \"hash\": \"<sha1 hash>\",\n               \"links\": [...],\n               ...\n             },\n             \"test.stl\": {\n               \"name\": \"test.stl\",\n               \"path\": \"test.stl\",\n               \"type\": \"model\",\n               \"typePath\": [\"model\", \"stl\"],\n               \"hash\": \"<sha1 hash>\",\n               \"links\": [...],\n               ...\n             },\n             ...\n           }\n\n        :param string path:     base path from which to recursively list all files, optional, if not supplied listing will start\n                                from root of base folder\n        :param function filter: a filter that matches the files that are to be returned, may be left out in which case no\n                                filtering will take place\n        :param bool recursive:  will also step into sub folders for building the complete list if set to True, otherwise will only\n                                do one step down into sub folders to be able to populate the ``children``.\n        :return: a dictionary mapping entry names to entry data that represents the whole file list\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_folder(self, path, ignore_existing=True, display=None):\n        \"\"\"\n        Adds a folder as ``path``\n\n        The ``path`` will be sanitized.\n\n        :param string path:          the path of the new folder\n        :param bool ignore_existing: if set to True, no error will be raised if the folder to be added already exists\n        :param unicode display:      display name of the folder\n        :return: the sanitized name of the new folder to be used for future references to the folder\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_folder(self, path, recursive=True):\n        \"\"\"\n        Removes the folder at ``path``\n\n        :param string path:    the path of the folder to remove\n        :param bool recursive: if set to True, contained folders and files will also be removed, otherwise an error will\n                               be raised if the folder is not empty (apart from any metadata files) when it's to be removed\n        \"\"\"\n        raise NotImplementedError()\n\n    def copy_folder(self, source, destination):\n        \"\"\"\n        Copies the folder ``source`` to ``destination``\n\n        :param string source: path to the source folder\n        :param string destination: path to destination\n\n        :return: the path in the storage to the copy of the folder\n        \"\"\"\n        raise NotImplementedError()\n\n    def move_folder(self, source, destination):\n        \"\"\"\n        Moves the folder ``source`` to ``destination``\n\n        :param string source: path to the source folder\n        :param string destination: path to destination\n\n        :return: the new path in the storage to the folder\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_file(\n        self,\n        path,\n        file_object,\n        printer_profile=None,\n        links=None,\n        allow_overwrite=False,\n        display=None,\n    ):\n        \"\"\"\n        Adds the file ``file_object`` as ``path``\n\n        :param string path:            the file's new path, will be sanitized\n        :param object file_object:     a file object that provides a ``save`` method which will be called with the destination path\n                                       where the object should then store its contents\n        :param object printer_profile: the printer profile associated with this file (if any)\n        :param list links:             any links to add with the file\n        :param bool allow_overwrite:   if set to True no error will be raised if the file already exists and the existing file\n                                       and its metadata will just be silently overwritten\n        :param unicode display:        display name of the file\n        :return: the sanitized name of the file to be used for future references to it\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_file(self, path):\n        \"\"\"\n        Removes the file at ``path``\n\n        Will also take care of deleting the corresponding entries\n        in the metadata and deleting all links pointing to the file.\n\n        :param string path: path of the file to remove\n        \"\"\"\n        raise NotImplementedError()\n\n    def copy_file(self, source, destination):\n        \"\"\"\n        Copies the file ``source`` to ``destination``\n\n        :param string source: path to the source file\n        :param string destination: path to destination\n\n        :return: the path in the storage to the copy of the file\n        \"\"\"\n        raise NotImplementedError()\n\n    def move_file(self, source, destination):\n        \"\"\"\n        Moves the file ``source`` to ``destination``\n\n        :param string source: path to the source file\n        :param string destination: path to destination\n\n        :return: the new path in the storage to the file\n        \"\"\"\n        raise NotImplementedError()\n\n    def has_analysis(self, path):\n        \"\"\"\n        Returns whether the file at path has been analysed yet\n\n        :param path: virtual path to the file for which to retrieve the metadata\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_metadata(self, path):\n        \"\"\"\n        Retrieves the metadata for the file ``path``.\n\n        :param path: virtual path to the file for which to retrieve the metadata\n        :return: the metadata associated with the file\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_link(self, path, rel, data):\n        \"\"\"\n        Adds a link of relation ``rel`` to file ``path`` with the given ``data``.\n\n        The following relation types are currently supported:\n\n          * ``model``: adds a link to a model from which the file was created/sliced, expected additional data is the ``name``\n            and optionally the ``hash`` of the file to link to. If the link can be resolved against another file on the\n            current ``path``, not only will it be added to the links of ``name`` but a reverse link of type ``machinecode``\n            referring to ``name`` and its hash will also be added to the linked ``model`` file\n          * ``machinecode``: adds a link to a file containing machine code created from the current file (model), expected\n            additional data is the ``name`` and optionally the ``hash`` of the file to link to. If the link can be resolved\n            against another file on the current ``path``, not only will it be added to the links of ``name`` but a reverse\n            link of type ``model`` referring to ``name`` and its hash will also be added to the linked ``model`` file.\n          * ``web``: adds a location on the web associated with this file (e.g. a website where to download a model),\n            expected additional data is a ``href`` attribute holding the website's URL and optionally a ``retrieved``\n            attribute describing when the content was retrieved\n\n        Note that adding ``model`` links to files identifying as models or ``machinecode`` links to files identifying\n        as machine code will be refused.\n\n        :param path: path of the file for which to add a link\n        :param rel: type of relation of the link to add (currently ``model``, ``machinecode`` and ``web`` are supported)\n        :param data: additional data of the link to add\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_link(self, path, rel, data):\n        \"\"\"\n        Removes the link consisting of ``rel`` and ``data`` from file ``name`` on ``path``.\n\n        :param path: path of the file from which to remove the link\n        :param rel: type of relation of the link to remove (currently ``model``, ``machinecode`` and ``web`` are supported)\n        :param data: additional data of the link to remove, must match existing link\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_additional_metadata(self, path, key):\n        \"\"\"\n        Fetches additional metadata at ``key`` from the metadata of ``path``.\n\n        :param path: the virtual path to the file for which to fetch additional metadata\n        :param key: key of metadata to fetch\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_additional_metadata(self, path, key, data, overwrite=False, merge=False):\n        \"\"\"\n        Adds additional metadata to the metadata of ``path``. Metadata in ``data`` will be saved under ``key``.\n\n        If ``overwrite`` is set and ``key`` already exists in ``name``'s metadata, the current value will be overwritten.\n\n        If ``merge`` is set and ``key`` already exists and both ``data`` and the existing data under ``key`` are dictionaries,\n        the two dictionaries will be merged recursively.\n\n        :param path: the virtual path to the file for which to add additional metadata\n        :param key: key of metadata to add\n        :param data: metadata to add\n        :param overwrite: if True and ``key`` already exists, it will be overwritten\n        :param merge: if True and ``key`` already exists and both ``data`` and the existing data are dictionaries, they\n                      will be merged\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_additional_metadata(self, path, key):\n        \"\"\"\n        Removes additional metadata under ``key`` for ``name`` on ``path``\n\n        :param path: the virtual path to the file for which to remove the metadata under ``key``\n        :param key: the key to remove\n        \"\"\"\n        raise NotImplementedError()\n\n    def canonicalize(self, path):\n        \"\"\"\n        Canonicalizes the given ``path``. The ``path`` may consist of both folder and file name, the underlying\n        implementation must separate those if necessary.\n\n        By default, this calls :func:`~octoprint.filemanager.StorageInterface.sanitize`, which also takes care\n        of stripping any invalid characters.\n\n        Args:\n                path: the path to canonicalize\n\n        Returns:\n                a 2-tuple containing the canonicalized path and file name\n\n        \"\"\"\n        return self.sanitize(path)\n\n    def sanitize(self, path):\n        \"\"\"\n        Sanitizes the given ``path``, stripping it of all invalid characters. The ``path`` may consist of both\n        folder and file name, the underlying implementation must separate those if necessary and sanitize individually.\n\n        :param string path: the path to sanitize\n        :return: a 2-tuple containing the sanitized path and file name\n        \"\"\"\n        raise NotImplementedError()\n\n    def sanitize_path(self, path):\n        \"\"\"\n        Sanitizes the given folder-only ``path``, stripping it of all invalid characters.\n        :param string path: the path to sanitize\n        :return: the sanitized path\n        \"\"\"\n        raise NotImplementedError()\n\n    def sanitize_name(self, name):\n        \"\"\"\n        Sanitizes the given file ``name``, stripping it of all invalid characters.\n        :param string name: the file name to sanitize\n        :return: the sanitized name\n        \"\"\"\n        raise NotImplementedError()\n\n    def split_path(self, path):\n        \"\"\"\n        Split ``path`` into base directory and file name.\n        :param path: the path to split\n        :return: a tuple (base directory, file name)\n        \"\"\"\n        raise NotImplementedError()\n\n    def join_path(self, *path):\n        \"\"\"\n        Join path elements together\n        :param path: path elements to join\n        :return: joined representation of the path to be usable as fully qualified path for further operations\n        \"\"\"\n        raise NotImplementedError()\n\n    def path_on_disk(self, path):\n        \"\"\"\n        Retrieves the path on disk for ``path``.\n\n        Note: if the storage is not on disk and there exists no path on disk to refer to it, this method should\n        raise an :class:`io.UnsupportedOperation`\n\n        Opposite of :func:`path_in_storage`.\n\n        :param string path: the virtual path for which to retrieve the path on disk\n        :return: the path on disk to ``path``\n        \"\"\"\n        raise NotImplementedError()\n\n    def path_in_storage(self, path):\n        \"\"\"\n        Retrieves the equivalent in the storage adapter for ``path``.\n\n        Opposite of :func:`path_on_disk`.\n\n        :param string path: the path for which to retrieve the storage path\n        :return: the path in storage to ``path``\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass StorageError(Exception):\n    UNKNOWN = \"unknown\"\n    INVALID_DIRECTORY = \"invalid_directory\"\n    INVALID_FILE = \"invalid_file\"\n    INVALID_SOURCE = \"invalid_source\"\n    INVALID_DESTINATION = \"invalid_destination\"\n    DOES_NOT_EXIST = \"does_not_exist\"\n    ALREADY_EXISTS = \"already_exists\"\n    SOURCE_EQUALS_DESTINATION = \"source_equals_destination\"\n    NOT_EMPTY = \"not_empty\"\n\n    def __init__(self, message, code=None, cause=None):\n        BaseException.__init__(self)\n        self.message = message\n        self.cause = cause\n\n        if code is None:\n            code = StorageError.UNKNOWN\n        self.code = code\n\n\nclass LocalFileStorage(StorageInterface):\n    \"\"\"\n    The ``LocalFileStorage`` is a storage implementation which holds all files, folders and metadata on disk.\n\n    Metadata is managed inside ``.metadata.json`` files in the respective folders, indexed by the sanitized filenames\n    stored within the folder. Metadata access is managed through an LRU cache to minimize access overhead.\n\n    This storage type implements :func:`path_on_disk`.\n    \"\"\"\n\n    def __init__(self, basefolder, create=False, really_universal=False):\n        \"\"\"\n        Initializes a ``LocalFileStorage`` instance under the given ``basefolder``, creating the necessary folder\n        if necessary and ``create`` is set to ``True``.\n\n        :param string basefolder:     the path to the folder under which to create the storage\n        :param bool create:           ``True`` if the folder should be created if it doesn't exist yet, ``False`` otherwise\n        :param bool really_universal: ``True`` if the file names should be forced to really universal, ``False`` otherwise\n        \"\"\"\n        self._logger = logging.getLogger(__name__)\n\n        self.basefolder = os.path.realpath(os.path.abspath(to_unicode(basefolder)))\n        if not os.path.exists(self.basefolder) and create:\n            os.makedirs(self.basefolder)\n        if not os.path.exists(self.basefolder) or not os.path.isdir(self.basefolder):\n            raise StorageError(\n                f\"{basefolder} is not a valid directory\",\n                code=StorageError.INVALID_DIRECTORY,\n            )\n\n        self._really_universal = really_universal\n\n        import threading\n\n        self._metadata_lock_mutex = threading.RLock()\n        self._metadata_locks = {}\n        self._persisted_metadata_lock_mutex = threading.RLock()\n        self._persisted_metadata_locks = {}\n\n        self._metadata_cache = pylru.lrucache(100)\n        self._filelist_cache = {}\n        self._filelist_cache_mutex = threading.RLock()\n\n        self._old_metadata = None\n        self._initialize_metadata()\n\n    def _initialize_metadata(self):\n        self._logger.info(f\"Initializing the file metadata for {self.basefolder}...\")\n\n        old_metadata_path = os.path.join(self.basefolder, \"metadata.yaml\")\n        backup_path = os.path.join(self.basefolder, \"metadata.yaml.backup\")\n\n        if os.path.exists(old_metadata_path):\n            # load the old metadata file\n            try:\n                self._old_metadata = yaml.load_from_file(path=old_metadata_path)\n            except Exception:\n                self._logger.exception(\"Error while loading old metadata file\")\n\n            # make sure the metadata is initialized as far as possible\n            self._list_folder(self.basefolder)\n\n            # rename the old metadata file\n            self._old_metadata = None\n            try:\n                import shutil\n\n                shutil.move(old_metadata_path, backup_path)\n            except Exception:\n                self._logger.exception(\"Could not rename old metadata.yaml file\")\n\n        else:\n            # make sure the metadata is initialized as far as possible\n            self._list_folder(self.basefolder)\n\n        self._logger.info(\n            f\"... file metadata for {self.basefolder} initialized successfully.\"\n        )\n\n    @property\n    def analysis_backlog(self):\n        return self.analysis_backlog_for_path()\n\n    def analysis_backlog_for_path(self, path=None):\n        if path:\n            path = self.sanitize_path(path)\n\n        yield from self._analysis_backlog_generator(path)\n\n    def _analysis_backlog_generator(self, path=None):\n        if path is None:\n            path = self.basefolder\n\n        metadata = self._get_metadata(path)\n        if not metadata:\n            metadata = {}\n        for entry in scandir(path):\n            if is_hidden_path(entry.name):\n                continue\n\n            if entry.is_file() and octoprint.filemanager.valid_file_type(entry.name):\n                if (\n                    entry.name not in metadata\n                    or not isinstance(metadata[entry.name], dict)\n                    or \"analysis\" not in metadata[entry.name]\n                ):\n                    printer_profile_rels = self.get_link(entry.path, \"printerprofile\")\n                    if printer_profile_rels:\n                        printer_profile_id = printer_profile_rels[0][\"id\"]\n                    else:\n                        printer_profile_id = None\n\n                    yield entry.name, entry.path, printer_profile_id\n            elif os.path.isdir(entry.path):\n                for sub_entry in self._analysis_backlog_generator(entry.path):\n                    yield self.join_path(entry.name, sub_entry[0]), sub_entry[\n                        1\n                    ], sub_entry[2]\n\n    def last_modified(self, path=None, recursive=False):\n        if path is None:\n            path = self.basefolder\n        else:\n            path = os.path.join(self.basefolder, path)\n\n        def last_modified_for_path(p):\n            metadata = os.path.join(p, \".metadata.json\")\n            if os.path.exists(metadata):\n                return max(os.stat(p).st_mtime, os.stat(metadata).st_mtime)\n            else:\n                return os.stat(p).st_mtime\n\n        if recursive:\n            return max(last_modified_for_path(root) for root, _, _ in walk(path))\n        else:\n            return last_modified_for_path(path)\n\n    def file_in_path(self, path, filepath):\n        filepath = self.sanitize_path(filepath)\n        path = self.sanitize_path(path)\n\n        return filepath == path or filepath.startswith(path + os.sep)\n\n    def file_exists(self, path):\n        path, name = self.sanitize(path)\n        file_path = os.path.join(path, name)\n        return os.path.exists(file_path) and os.path.isfile(file_path)\n\n    def folder_exists(self, path):\n        path, name = self.sanitize(path)\n        folder_path = os.path.join(path, name)\n        return os.path.exists(folder_path) and os.path.isdir(folder_path)\n\n    def list_files(\n        self, path=None, filter=None, recursive=True, level=0, force_refresh=False\n    ):\n        if path:\n            path = self.sanitize_path(to_unicode(path))\n            base = self.path_in_storage(path)\n            if base:\n                base += \"/\"\n        else:\n            path = self.basefolder\n            base = \"\"\n\n        def strip_children(nodes):\n            result = {}\n            for key, node in nodes.items():\n                if node[\"type\"] == \"folder\":\n                    node = copy.copy(node)\n                    node[\"children\"] = {}\n                result[key] = node\n            return result\n\n        def strip_grandchildren(nodes):\n            result = {}\n            for key, node in nodes.items():\n                if node[\"type\"] == \"folder\":\n                    node = copy.copy(node)\n                    node[\"children\"] = strip_children(node[\"children\"])\n                result[key] = node\n            return result\n\n        def apply_filter(nodes, filter_func):\n            result = {}\n            for key, node in nodes.items():\n                if filter_func(node) or node[\"type\"] == \"folder\":\n                    if node[\"type\"] == \"folder\":\n                        node = copy.copy(node)\n                        node[\"children\"] = apply_filter(\n                            node.get(\"children\", {}), filter_func\n                        )\n                    result[key] = node\n            return result\n\n        result = self._list_folder(path, base=base, force_refresh=force_refresh)\n        if not recursive:\n            if level > 0:\n                result = strip_grandchildren(result)\n            else:\n                result = strip_children(result)\n        if callable(filter):\n            result = apply_filter(result, filter)\n        return result\n\n    def add_folder(self, path, ignore_existing=True, display=None):\n        display_path, display_name = self.canonicalize(path)\n        path = self.sanitize_path(display_path)\n        name = self.sanitize_name(display_name)\n\n        if display is not None:\n            display_name = display\n\n        folder_path = os.path.join(path, name)\n        if os.path.exists(folder_path):\n            if not ignore_existing:\n                raise StorageError(\n                    f\"{name} does already exist in {path}\",\n                    code=StorageError.ALREADY_EXISTS,\n                )\n        else:\n            os.mkdir(folder_path)\n\n        if display_name != name:\n            metadata = self._get_metadata_entry(path, name, default={})\n            metadata[\"display\"] = display_name\n            self._update_metadata_entry(path, name, metadata)\n\n        return self.path_in_storage((path, name))\n\n    def remove_folder(self, path, recursive=True):\n        path, name = self.sanitize(path)\n\n        folder_path = os.path.join(path, name)\n        if not os.path.exists(folder_path):\n            return\n\n        empty = True\n        for entry in scandir(folder_path):\n            if entry.name == \".metadata.json\" or entry.name == \".metadata.yaml\":\n                continue\n            empty = False\n            break\n\n        if not empty and not recursive:\n            raise StorageError(\n                f\"{name} in {path} is not empty\",\n                code=StorageError.NOT_EMPTY,\n            )\n\n        import shutil\n\n        shutil.rmtree(folder_path)\n\n        self._remove_metadata_entry(path, name)\n\n    def _get_source_destination_data(self, source, destination, must_not_equal=False):\n        \"\"\"Prepares data dicts about source and destination for copy/move.\"\"\"\n        source_path, source_name = self.sanitize(source)\n\n        destination_canon_path, destination_canon_name = self.canonicalize(destination)\n        destination_path = self.sanitize_path(destination_canon_path)\n        destination_name = self.sanitize_name(destination_canon_name)\n\n        source_fullpath = os.path.join(source_path, source_name)\n        destination_fullpath = os.path.join(destination_path, destination_name)\n\n        if not os.path.exists(source_fullpath):\n            raise StorageError(\n                f\"{source_name} in {source_path} does not exist\",\n                code=StorageError.INVALID_SOURCE,\n            )\n\n        if not os.path.isdir(destination_path):\n            raise StorageError(\n                \"Destination path {} does not exist or is not a folder\".format(\n                    destination_path\n                ),\n                code=StorageError.INVALID_DESTINATION,\n            )\n        if (\n            os.path.exists(destination_fullpath)\n            and source_fullpath != destination_fullpath\n        ):\n            raise StorageError(\n                f\"{destination_name} does already exist in {destination_path}\",\n                code=StorageError.INVALID_DESTINATION,\n            )\n\n        source_meta = self._get_metadata_entry(source_path, source_name)\n        if source_meta:\n            source_display = source_meta.get(\"display\", source_name)\n        else:\n            source_display = source_name\n\n        if (\n            must_not_equal or source_display == destination_canon_name\n        ) and source_fullpath == destination_fullpath:\n            raise StorageError(\n                \"Source {} and destination {} are the same folder\".format(\n                    source_path, destination_path\n                ),\n                code=StorageError.SOURCE_EQUALS_DESTINATION,\n            )\n\n        source_data = {\n            \"path\": source_path,\n            \"name\": source_name,\n            \"display\": source_display,\n            \"fullpath\": source_fullpath,\n        }\n        destination_data = {\n            \"path\": destination_path,\n            \"name\": destination_name,\n            \"display\": destination_canon_name,\n            \"fullpath\": destination_fullpath,\n        }\n        return source_data, destination_data\n\n    def _set_display_metadata(self, destination_data, source_data=None):\n        if (\n            source_data\n            and destination_data[\"name\"] == source_data[\"name\"]\n            and source_data[\"name\"] != source_data[\"display\"]\n        ):\n            display = source_data[\"display\"]\n        elif destination_data[\"name\"] != destination_data[\"display\"]:\n            display = destination_data[\"display\"]\n        else:\n            display = None\n\n        destination_meta = self._get_metadata_entry(\n            destination_data[\"path\"], destination_data[\"name\"], default={}\n        )\n        if display:\n            destination_meta[\"display\"] = display\n            self._update_metadata_entry(\n                destination_data[\"path\"], destination_data[\"name\"], destination_meta\n            )\n        elif \"display\" in destination_meta:\n            del destination_meta[\"display\"]\n            self._update_metadata_entry(\n                destination_data[\"path\"], destination_data[\"name\"], destination_meta\n            )\n\n    def copy_folder(self, source, destination):\n        source_data, destination_data = self._get_source_destination_data(\n            source, destination, must_not_equal=True\n        )\n\n        try:\n            shutil.copytree(source_data[\"fullpath\"], destination_data[\"fullpath\"])\n        except Exception as e:\n            raise StorageError(\n                \"Could not copy %s in %s to %s in %s\"\n                % (\n                    source_data[\"name\"],\n                    source_data[\"path\"],\n                    destination_data[\"name\"],\n                    destination_data[\"path\"],\n                ),\n                cause=e,\n            )\n\n        self._set_display_metadata(destination_data, source_data=source_data)\n\n        return self.path_in_storage(destination_data[\"fullpath\"])\n\n    def move_folder(self, source, destination):\n        source_data, destination_data = self._get_source_destination_data(\n            source, destination\n        )\n\n        # only a display rename? Update that and bail early\n        if source_data[\"fullpath\"] == destination_data[\"fullpath\"]:\n            self._set_display_metadata(destination_data)\n            return self.path_in_storage(destination_data[\"fullpath\"])\n\n        try:\n            shutil.move(source_data[\"fullpath\"], destination_data[\"fullpath\"])\n        except Exception as e:\n            raise StorageError(\n                \"Could not move %s in %s to %s in %s\"\n                % (\n                    source_data[\"name\"],\n                    source_data[\"path\"],\n                    destination_data[\"name\"],\n                    destination_data[\"path\"],\n                ),\n                cause=e,\n            )\n\n        self._set_display_metadata(destination_data, source_data=source_data)\n        self._remove_metadata_entry(source_data[\"path\"], source_data[\"name\"])\n        self._delete_metadata(source_data[\"fullpath\"])\n\n        return self.path_in_storage(destination_data[\"fullpath\"])\n\n    def add_file(\n        self,\n        path,\n        file_object,\n        printer_profile=None,\n        links=None,\n        allow_overwrite=False,\n        display=None,\n    ):\n        display_path, display_name = self.canonicalize(path)\n        path = self.sanitize_path(display_path)\n        name = self.sanitize_name(display_name)\n\n        if display:\n            display_name = display\n\n        if not octoprint.filemanager.valid_file_type(name):\n            raise StorageError(\n                f\"{name} is an unrecognized file type\",\n                code=StorageError.INVALID_FILE,\n            )\n\n        file_path = os.path.join(path, name)\n        if os.path.exists(file_path) and not os.path.isfile(file_path):\n            raise StorageError(\n                f\"{name} does already exist in {path} and is not a file\",\n                code=StorageError.ALREADY_EXISTS,\n            )\n        if os.path.exists(file_path) and not allow_overwrite:\n            raise StorageError(\n                f\"{name} does already exist in {path} and overwriting is prohibited\",\n                code=StorageError.ALREADY_EXISTS,\n            )\n\n        # make sure folders exist\n        if not os.path.exists(path):\n            # TODO persist display names of path segments!\n            os.makedirs(path)\n\n        # save the file\n        file_object.save(file_path)\n\n        # save the file's hash to the metadata of the folder\n        file_hash = self._create_hash(file_path)\n        metadata = self._get_metadata_entry(path, name, default={})\n        metadata_dirty = False\n        if \"hash\" not in metadata or metadata[\"hash\"] != file_hash:\n            # hash changed -> throw away old metadata\n            metadata = {\"hash\": file_hash}\n            metadata_dirty = True\n\n        if \"display\" not in metadata and display_name != name:\n            # display name is not the same as file name -> store in metadata\n            metadata[\"display\"] = display_name\n            metadata_dirty = True\n\n        if metadata_dirty:\n            self._update_metadata_entry(path, name, metadata)\n\n        # process any links that were also provided for adding to the file\n        if not links:\n            links = []\n\n        if printer_profile is not None:\n            links.append(\n                (\n                    \"printerprofile\",\n                    {\"id\": printer_profile[\"id\"], \"name\": printer_profile[\"name\"]},\n                )\n            )\n\n        self._add_links(name, path, links)\n\n        # touch the file to set last access and modification time to now\n        os.utime(file_path, None)\n\n        return self.path_in_storage((path, name))\n\n    def remove_file(self, path):\n        path, name = self.sanitize(path)\n\n        file_path = os.path.join(path, name)\n        if not os.path.exists(file_path):\n            return\n        if not os.path.isfile(file_path):\n            raise StorageError(\n                f\"{name} in {path} is not a file\",\n                code=StorageError.INVALID_FILE,\n            )\n\n        try:\n            os.remove(file_path)\n        except Exception as e:\n            raise StorageError(f\"Could not delete {name} in {path}\", cause=e)\n\n        self._remove_metadata_entry(path, name)\n\n    def copy_file(self, source, destination):\n        source_data, destination_data = self._get_source_destination_data(\n            source, destination, must_not_equal=True\n        )\n\n        try:\n            shutil.copy2(source_data[\"fullpath\"], destination_data[\"fullpath\"])\n        except Exception as e:\n            raise StorageError(\n                \"Could not copy %s in %s to %s in %s\"\n                % (\n                    source_data[\"name\"],\n                    source_data[\"path\"],\n                    destination_data[\"name\"],\n                    destination_data[\"path\"],\n                ),\n                cause=e,\n            )\n\n        self._copy_metadata_entry(\n            source_data[\"path\"],\n            source_data[\"name\"],\n            destination_data[\"path\"],\n            destination_data[\"name\"],\n        )\n        self._set_display_metadata(destination_data, source_data=source_data)\n\n        return self.path_in_storage(destination_data[\"fullpath\"])\n\n    def move_file(self, source, destination, allow_overwrite=False):\n        source_data, destination_data = self._get_source_destination_data(\n            source, destination\n        )\n\n        # only a display rename? Update that and bail early\n        if source_data[\"fullpath\"] == destination_data[\"fullpath\"]:\n            self._set_display_metadata(destination_data)\n            return self.path_in_storage(destination_data[\"fullpath\"])\n\n        try:\n            shutil.move(source_data[\"fullpath\"], destination_data[\"fullpath\"])\n        except Exception as e:\n            raise StorageError(\n                \"Could not move %s in %s to %s in %s\"\n                % (\n                    source_data[\"name\"],\n                    source_data[\"path\"],\n                    destination_data[\"name\"],\n                    destination_data[\"path\"],\n                ),\n                cause=e,\n            )\n\n        self._copy_metadata_entry(\n            source_data[\"path\"],\n            source_data[\"name\"],\n            destination_data[\"path\"],\n            destination_data[\"name\"],\n            delete_source=True,\n        )\n        self._set_display_metadata(destination_data, source_data=source_data)\n\n        return self.path_in_storage(destination_data[\"fullpath\"])\n\n    def has_analysis(self, path):\n        metadata = self.get_metadata(path)\n        return \"analysis\" in metadata\n\n    def get_metadata(self, path):\n        path, name = self.sanitize(path)\n        return self._get_metadata_entry(path, name)\n\n    def get_link(self, path, rel):\n        path, name = self.sanitize(path)\n        return self._get_links(name, path, rel)\n\n    def add_link(self, path, rel, data):\n        path, name = self.sanitize(path)\n        self._add_links(name, path, [(rel, data)])\n\n    def remove_link(self, path, rel, data):\n        path, name = self.sanitize(path)\n        self._remove_links(name, path, [(rel, data)])\n\n    def add_history(self, path, data):\n        path, name = self.sanitize(path)\n        self._add_history(name, path, data)\n\n    def update_history(self, path, index, data):\n        path, name = self.sanitize(path)\n        self._update_history(name, path, index, data)\n\n    def remove_history(self, path, index):\n        path, name = self.sanitize(path)\n        self._delete_history(name, path, index)\n\n    def get_additional_metadata(self, path, key):\n        path, name = self.sanitize(path)\n        metadata = self._get_metadata(path)\n\n        if name not in metadata:\n            return\n\n        return metadata[name].get(key)\n\n    def set_additional_metadata(self, path, key, data, overwrite=False, merge=False):\n        path, name = self.sanitize(path)\n        metadata = self._get_metadata(path)\n        metadata_dirty = False\n\n        if name not in metadata:\n            return\n\n        metadata = self._copied_metadata(metadata, name)\n\n        if key not in metadata[name] or overwrite:\n            metadata[name][key] = data\n            metadata_dirty = True\n        elif (\n            key in metadata[name]\n            and isinstance(metadata[name][key], dict)\n            and isinstance(data, dict)\n            and merge\n        ):\n            import octoprint.util\n\n            metadata[name][key] = octoprint.util.dict_merge(\n                metadata[name][key], data, in_place=True\n            )\n            metadata_dirty = True\n\n        if metadata_dirty:\n            self._save_metadata(path, metadata)\n\n    def remove_additional_metadata(self, path, key):\n        path, name = self.sanitize(path)\n        metadata = self._get_metadata(path)\n\n        if name not in metadata:\n            return\n\n        if key not in metadata[name]:\n            return\n\n        metadata = self._copied_metadata(metadata, name)\n        del metadata[name][key]\n        self._save_metadata(path, metadata)\n\n    def split_path(self, path):\n        path = to_unicode(path)\n        split = path.split(\"/\")\n        if len(split) == 1:\n            return \"\", split[0]\n        else:\n            return self.join_path(*split[:-1]), split[-1]\n\n    def join_path(self, *path):\n        return \"/\".join(map(to_unicode, path))\n\n    def sanitize(self, path):\n        \"\"\"\n        Returns a ``(path, name)`` tuple derived from the provided ``path``.\n\n        ``path`` may be:\n          * a storage path\n          * an absolute file system path\n          * a tuple or list containing all individual path elements\n          * a string representation of the path\n          * with or without a file name\n\n        Note that for a ``path`` without a trailing slash the last part will be considered a file name and\n        hence be returned at second position. If you only need to convert a folder path, be sure to\n        include a trailing slash for a string ``path`` or an empty last element for a list ``path``.\n        \"\"\"\n\n        path, name = self.canonicalize(path)\n        name = self.sanitize_name(name)\n        path = self.sanitize_path(path)\n        return path, name\n\n    def canonicalize(self, path):\n        name = None\n        if isinstance(path, str):\n            path = to_unicode(path)\n            if path.startswith(self.basefolder):\n                path = path[len(self.basefolder) :]\n            path = path.replace(os.path.sep, \"/\")\n            path = path.split(\"/\")\n        if isinstance(path, (list, tuple)):\n            if len(path) == 1:\n                name = to_unicode(path[0])\n                path = \"\"\n            else:\n                name = to_unicode(path[-1])\n                path = self.join_path(*map(to_unicode, path[:-1]))\n        if not path:\n            path = \"\"\n\n        return path, name\n\n    def sanitize_name(self, name):\n        \"\"\"\n        Raises a :class:`ValueError` for a ``name`` containing ``/`` or ``\\\\``. Otherwise\n        sanitizes the given ``name`` using ``octoprint.files.sanitize_filename``. Also\n        strips any leading ``.``.\n        \"\"\"\n        return sanitize_filename(name, really_universal=self._really_universal)\n\n    def sanitize_path(self, path):\n        \"\"\"\n        Ensures that the on disk representation of ``path`` is located under the configured basefolder. Resolves all\n        relative path elements (e.g. ``..``) and sanitizes folder names using :func:`sanitize_name`. Final path is the\n        absolute path including leading ``basefolder`` path.\n        \"\"\"\n        path = to_unicode(path)\n\n        if len(path):\n            if path[0] == \"/\":\n                path = path[1:]\n            elif path[0] == \".\" and path[1] == \"/\":\n                path = path[2:]\n\n        path_elements = path.split(\"/\")\n        joined_path = self.basefolder\n        for path_element in path_elements:\n            if path_element == \"..\" or path_element == \".\":\n                joined_path = os.path.join(joined_path, path_element)\n            else:\n                joined_path = os.path.join(joined_path, self.sanitize_name(path_element))\n        path = os.path.realpath(joined_path)\n        if not path.startswith(self.basefolder):\n            raise ValueError(f\"path not contained in base folder: {path}\")\n        return path\n\n    def _sanitize_entry(self, entry, path, entry_path):\n        entry = to_unicode(entry)\n        sanitized = self.sanitize_name(entry)\n        if sanitized != entry:\n            # entry is not sanitized yet, let's take care of that\n            sanitized_path = os.path.join(path, sanitized)\n            sanitized_name, sanitized_ext = os.path.splitext(sanitized)\n\n            counter = 1\n            while os.path.exists(sanitized_path):\n                counter += 1\n                sanitized = self.sanitize_name(\n                    f\"{sanitized_name}_({counter}){sanitized_ext}\"\n                )\n                sanitized_path = os.path.join(path, sanitized)\n\n            try:\n                shutil.move(entry_path, sanitized_path)\n\n                self._logger.info(f'Sanitized \"{entry_path}\" to \"{sanitized_path}\"')\n                return sanitized, sanitized_path\n            except Exception:\n                self._logger.exception(\n                    'Error while trying to rename \"{}\" to \"{}\", ignoring file'.format(\n                        entry_path, sanitized_path\n                    )\n                )\n                raise\n\n        return entry, entry_path\n\n    def path_in_storage(self, path):\n        if isinstance(path, (tuple, list)):\n            path = self.join_path(*path)\n        if isinstance(path, str):\n            path = to_unicode(path)\n            if path.startswith(self.basefolder):\n                path = path[len(self.basefolder) :]\n            path = path.replace(os.path.sep, \"/\")\n        if path.startswith(\"/\"):\n            path = path[1:]\n\n        return path\n\n    def path_on_disk(self, path):\n        path, name = self.sanitize(path)\n        return os.path.join(path, name)\n\n    ##~~ internals\n\n    def _add_history(self, name, path, data):\n        metadata = self._copied_metadata(self._get_metadata(path), name)\n\n        if \"hash\" not in metadata[name]:\n            metadata[name][\"hash\"] = self._create_hash(os.path.join(path, name))\n\n        if \"history\" not in metadata[name]:\n            metadata[name][\"history\"] = []\n\n        metadata[name][\"history\"].append(data)\n        self._calculate_stats_from_history(name, path, metadata=metadata, save=False)\n        self._save_metadata(path, metadata)\n\n    def _update_history(self, name, path, index, data):\n        metadata = self._get_metadata(path)\n\n        if name not in metadata or \"history\" not in metadata[name]:\n            return\n\n        metadata = self._copied_metadata(metadata, name)\n\n        try:\n            metadata[name][\"history\"][index].update(data)\n            self._calculate_stats_from_history(name, path, metadata=metadata, save=False)\n            self._save_metadata(path, metadata)\n        except IndexError:\n            pass\n\n    def _delete_history(self, name, path, index):\n        metadata = self._get_metadata(path)\n\n        if name not in metadata or \"history\" not in metadata[name]:\n            return\n\n        metadata = self._copied_metadata(metadata, name)\n\n        try:\n            del metadata[name][\"history\"][index]\n            self._calculate_stats_from_history(name, path, metadata=metadata, save=False)\n            self._save_metadata(path, metadata)\n        except IndexError:\n            pass\n\n    def _calculate_stats_from_history(self, name, path, metadata=None, save=True):\n        if metadata is None:\n            metadata = self._copied_metadata(self._get_metadata(path), name)\n\n        if \"history\" not in metadata[name]:\n            return\n\n        # collect data from history\n        former_print_times = {}\n        last_print = {}\n\n        for history_entry in metadata[name][\"history\"]:\n            if (\n                \"printTime\" not in history_entry\n                or \"success\" not in history_entry\n                or not history_entry[\"success\"]\n                or \"printerProfile\" not in history_entry\n            ):\n                continue\n\n            printer_profile = history_entry[\"printerProfile\"]\n            if not printer_profile:\n                continue\n\n            print_time = history_entry[\"printTime\"]\n            try:\n                print_time = float(print_time)\n            except Exception:\n                self._logger.warning(\n                    \"Invalid print time value found in print history for {} in {}/.metadata.json: {!r}\".format(\n                        name, path, print_time\n                    )\n                )\n                continue\n\n            if printer_profile not in former_print_times:\n                former_print_times[printer_profile] = []\n            former_print_times[printer_profile].append(print_time)\n\n            if (\n                printer_profile not in last_print\n                or last_print[printer_profile] is None\n                or (\n                    \"timestamp\" in history_entry\n                    and history_entry[\"timestamp\"]\n                    > last_print[printer_profile][\"timestamp\"]\n                )\n            ):\n                last_print[printer_profile] = history_entry\n\n        # calculate stats\n        statistics = {\"averagePrintTime\": {}, \"lastPrintTime\": {}}\n\n        for printer_profile in former_print_times:\n            if not former_print_times[printer_profile]:\n                continue\n            statistics[\"averagePrintTime\"][printer_profile] = sum(\n                former_print_times[printer_profile]\n            ) / len(former_print_times[printer_profile])\n\n        for printer_profile in last_print:\n            if not last_print[printer_profile]:\n                continue\n            statistics[\"lastPrintTime\"][printer_profile] = last_print[printer_profile][\n                \"printTime\"\n            ]\n\n        metadata[name][\"statistics\"] = statistics\n\n        if save:\n            self._save_metadata(path, metadata)\n\n    def _get_links(self, name, path, searched_rel):\n        metadata = self._get_metadata(path)\n        result = []\n\n        if name not in metadata:\n            return result\n\n        if \"links\" not in metadata[name]:\n            return result\n\n        for data in metadata[name][\"links\"]:\n            if \"rel\" not in data or not data[\"rel\"] == searched_rel:\n                continue\n            result.append(data)\n        return result\n\n    def _add_links(self, name, path, links):\n        file_type = octoprint.filemanager.get_file_type(name)\n        if file_type:\n            file_type = file_type[0]\n\n        metadata = self._copied_metadata(self._get_metadata(path), name)\n        metadata_dirty = False\n\n        if \"hash\" not in metadata[name]:\n            metadata[name][\"hash\"] = self._create_hash(os.path.join(path, name))\n\n        if \"links\" not in metadata[name]:\n            metadata[name][\"links\"] = []\n\n        for rel, data in links:\n            if (rel == \"model\" or rel == \"machinecode\") and \"name\" in data:\n                if file_type == \"model\" and rel == \"model\":\n                    # adding a model link to a model doesn't make sense\n                    return\n                elif file_type == \"machinecode\" and rel == \"machinecode\":\n                    # adding a machinecode link to a machinecode doesn't make sense\n                    return\n\n                ref_path = os.path.join(path, data[\"name\"])\n                if not os.path.exists(ref_path):\n                    # file doesn't exist, we won't create the link\n                    continue\n\n                # fetch hash of target file\n                if data[\"name\"] in metadata and \"hash\" in metadata[data[\"name\"]]:\n                    hash = metadata[data[\"name\"]][\"hash\"]\n                else:\n                    hash = self._create_hash(ref_path)\n                    if data[\"name\"] not in metadata:\n                        metadata[data[\"name\"]] = {\"hash\": hash, \"links\": []}\n                    else:\n                        metadata[data[\"name\"]][\"hash\"] = hash\n\n                if \"hash\" in data and not data[\"hash\"] == hash:\n                    # file doesn't have the correct hash, we won't create the link\n                    continue\n\n                if \"links\" not in metadata[data[\"name\"]]:\n                    metadata[data[\"name\"]][\"links\"] = []\n\n                # add reverse link to link target file\n                metadata[data[\"name\"]][\"links\"].append(\n                    {\n                        \"rel\": \"machinecode\" if rel == \"model\" else \"model\",\n                        \"name\": name,\n                        \"hash\": metadata[name][\"hash\"],\n                    }\n                )\n                metadata_dirty = True\n\n                link_dict = {\"rel\": rel, \"name\": data[\"name\"], \"hash\": hash}\n\n            elif rel == \"web\" and \"href\" in data:\n                link_dict = {\"rel\": rel, \"href\": data[\"href\"]}\n                if \"retrieved\" in data:\n                    link_dict[\"retrieved\"] = data[\"retrieved\"]\n\n            else:\n                continue\n\n            if link_dict:\n                metadata[name][\"links\"].append(link_dict)\n                metadata_dirty = True\n\n        if metadata_dirty:\n            self._save_metadata(path, metadata)\n\n    def _remove_links(self, name, path, links):\n        metadata = self._copied_metadata(self._get_metadata(path), name)\n        metadata_dirty = False\n\n        hash = metadata[name].get(\"hash\", self._create_hash(os.path.join(path, name)))\n\n        for rel, data in links:\n            if (rel == \"model\" or rel == \"machinecode\") and \"name\" in data:\n                if data[\"name\"] in metadata and \"links\" in metadata[data[\"name\"]]:\n                    ref_rel = \"model\" if rel == \"machinecode\" else \"machinecode\"\n                    for link in metadata[data[\"name\"]][\"links\"]:\n                        if (\n                            link[\"rel\"] == ref_rel\n                            and \"name\" in link\n                            and link[\"name\"] == name\n                            and \"hash\" in link\n                            and link[\"hash\"] == hash\n                        ):\n                            metadata[data[\"name\"]] = copy.deepcopy(metadata[data[\"name\"]])\n                            metadata[data[\"name\"]][\"links\"].remove(link)\n                            metadata_dirty = True\n\n            if \"links\" in metadata[name]:\n                for link in metadata[name][\"links\"]:\n                    if not link[\"rel\"] == rel:\n                        continue\n\n                    matches = True\n                    for k, v in data.items():\n                        if k not in link or not link[k] == v:\n                            matches = False\n                            break\n\n                    if not matches:\n                        continue\n\n                    metadata[name][\"links\"].remove(link)\n                    metadata_dirty = True\n\n        if metadata_dirty:\n            self._save_metadata(path, metadata)\n\n    @time_this(\n        logtarget=__name__ + \".timings\",\n        message=\"{func}({func_args},{func_kwargs}) took {timing:.2f}ms\",\n        incl_func_args=True,\n        log_enter=True,\n    )\n    def _list_folder(self, path, base=\"\", force_refresh=False, **kwargs):\n        def get_size(nodes):\n            total_size = 0\n            for node in nodes.values():\n                if \"size\" in node:\n                    total_size += node[\"size\"]\n            return total_size\n\n        def enrich_folders(nodes):\n            nodes = copy.copy(nodes)\n            for key, value in nodes.items():\n                if value[\"type\"] == \"folder\":\n                    value = copy.copy(value)\n                    value[\"children\"] = self._list_folder(\n                        os.path.join(path, key),\n                        base=value[\"path\"] + \"/\",\n                        force_refresh=force_refresh,\n                    )\n                    value[\"size\"] = get_size(value[\"children\"])\n                    nodes[key] = value\n            return nodes\n\n        metadata_dirty = False\n        try:\n            with self._filelist_cache_mutex:\n                cache = self._filelist_cache.get(path)\n                lm = self.last_modified(path, recursive=True)\n                if not force_refresh and cache and cache[0] >= lm:\n                    return enrich_folders(cache[1])\n\n                metadata = self._get_metadata(path)\n                if not metadata:\n                    metadata = {}\n\n                result = {}\n\n                for entry in scandir(path):\n                    if is_hidden_path(entry.name):\n                        # no hidden files and folders\n                        continue\n\n                    try:\n                        entry_name = entry_display = entry.name\n                        entry_path = entry.path\n                        entry_is_file = entry.is_file()\n                        entry_is_dir = entry.is_dir()\n                        entry_stat = entry.stat()\n                    except Exception:\n                        # error while trying to fetch file metadata, that might be thanks to file already having\n                        # been moved or deleted - ignore it and continue\n                        continue\n\n                    try:\n                        new_entry_name, new_entry_path = self._sanitize_entry(\n                            entry_name, path, entry_path\n                        )\n                        if entry_name != new_entry_name or entry_path != new_entry_path:\n                            entry_display = to_unicode(entry_name)\n                            entry_name = new_entry_name\n                            entry_path = new_entry_path\n                            entry_stat = os.stat(entry_path)\n                    except Exception:\n                        # error while trying to rename the file, we'll continue here and ignore it\n                        continue\n\n                    path_in_location = entry_name if not base else base + entry_name\n\n                    try:\n                        # file handling\n                        if entry_is_file:\n                            type_path = octoprint.filemanager.get_file_type(entry_name)\n                            if not type_path:\n                                # only supported extensions\n                                continue\n                            else:\n                                file_type = type_path[0]\n\n                            if entry_name in metadata and isinstance(\n                                metadata[entry_name], dict\n                            ):\n                                entry_metadata = metadata[entry_name]\n                                if (\n                                    \"display\" not in entry_metadata\n                                    and entry_display != entry_name\n                                ):\n                                    if not metadata_dirty:\n                                        metadata = self._copied_metadata(\n                                            metadata, entry_name\n                                        )\n                                    metadata[entry_name][\"display\"] = entry_display\n                                    entry_metadata[\"display\"] = entry_display\n                                    metadata_dirty = True\n                            else:\n                                if not metadata_dirty:\n                                    metadata = self._copied_metadata(metadata, entry_name)\n                                entry_metadata = self._add_basic_metadata(\n                                    path,\n                                    entry_name,\n                                    display_name=entry_display,\n                                    save=False,\n                                    metadata=metadata,\n                                )\n                                metadata_dirty = True\n\n                            extended_entry_data = {}\n                            extended_entry_data.update(entry_metadata)\n                            extended_entry_data[\"name\"] = entry_name\n                            extended_entry_data[\"display\"] = entry_metadata.get(\n                                \"display\", entry_name\n                            )\n                            extended_entry_data[\"path\"] = path_in_location\n                            extended_entry_data[\"type\"] = file_type\n                            extended_entry_data[\"typePath\"] = type_path\n                            stat = entry_stat\n                            if stat:\n                                extended_entry_data[\"size\"] = stat.st_size\n                                extended_entry_data[\"date\"] = int(stat.st_mtime)\n\n                            result[entry_name] = extended_entry_data\n\n                        # folder recursion\n                        elif entry_is_dir:\n                            if entry_name in metadata and isinstance(\n                                metadata[entry_name], dict\n                            ):\n                                entry_metadata = metadata[entry_name]\n                                if (\n                                    \"display\" not in entry_metadata\n                                    and entry_display != entry_name\n                                ):\n                                    if not metadata_dirty:\n                                        metadata = self._copied_metadata(\n                                            metadata, entry_name\n                                        )\n                                    metadata[entry_name][\"display\"] = entry_display\n                                    entry_metadata[\"display\"] = entry_display\n                                    metadata_dirty = True\n                            elif entry_name != entry_display:\n                                if not metadata_dirty:\n                                    metadata = self._copied_metadata(metadata, entry_name)\n                                entry_metadata = self._add_basic_metadata(\n                                    path,\n                                    entry_name,\n                                    display_name=entry_display,\n                                    save=False,\n                                    metadata=metadata,\n                                )\n                                metadata_dirty = True\n                            else:\n                                entry_metadata = {}\n\n                            entry_data = {\n                                \"name\": entry_name,\n                                \"display\": entry_metadata.get(\"display\", entry_name),\n                                \"path\": path_in_location,\n                                \"type\": \"folder\",\n                                \"typePath\": [\"folder\"],\n                            }\n\n                            result[entry_name] = entry_data\n                    except Exception:\n                        # So something went wrong somewhere while processing this file entry - log that and continue\n                        self._logger.exception(\n                            f\"Error while processing entry {entry_path}\"\n                        )\n                        continue\n\n                self._filelist_cache[path] = (\n                    lm,\n                    result,\n                )\n                return enrich_folders(result)\n        finally:\n            # save metadata\n            if metadata_dirty:\n                self._save_metadata(path, metadata)\n\n    def _add_basic_metadata(\n        self,\n        path,\n        entry,\n        display_name=None,\n        additional_metadata=None,\n        save=True,\n        metadata=None,\n    ):\n        if additional_metadata is None:\n            additional_metadata = {}\n\n        if metadata is None:\n            metadata = self._get_metadata(path)\n\n        entry_path = os.path.join(path, entry)\n\n        if os.path.isfile(entry_path):\n            entry_data = {\n                \"hash\": self._create_hash(os.path.join(path, entry)),\n                \"links\": [],\n                \"notes\": [],\n            }\n            if (\n                path == self.basefolder\n                and self._old_metadata is not None\n                and entry in self._old_metadata\n                and \"gcodeAnalysis\" in self._old_metadata[entry]\n            ):\n                # if there is still old metadata available and that contains an analysis for this file, use it!\n                entry_data[\"analysis\"] = self._old_metadata[entry][\"gcodeAnalysis\"]\n\n        elif os.path.isdir(entry_path):\n            entry_data = {}\n\n        else:\n            return\n\n        if display_name is not None and not display_name == entry:\n            entry_data[\"display\"] = display_name\n\n        entry_data.update(additional_metadata)\n\n        metadata = copy.copy(metadata)\n        metadata[entry] = entry_data\n\n        if save:\n            self._save_metadata(path, metadata)\n\n        return entry_data\n\n    def _create_hash(self, path):\n        import hashlib\n\n        blocksize = 65536\n        hash = hashlib.sha1()\n        with open(path, \"rb\") as f:\n            buffer = f.read(blocksize)\n            while len(buffer) > 0:\n                hash.update(buffer)\n                buffer = f.read(blocksize)\n\n        return hash.hexdigest()\n\n    def _get_metadata_entry(self, path, name, default=None):\n        with self._get_metadata_lock(path):\n            metadata = self._get_metadata(path)\n            return metadata.get(name, default)\n\n    def _remove_metadata_entry(self, path, name):\n        with self._get_metadata_lock(path):\n            metadata = self._get_metadata(path)\n            if name not in metadata:\n                return\n\n            metadata = copy.copy(metadata)\n\n            if \"hash\" in metadata[name]:\n                hash = metadata[name][\"hash\"]\n                for m in metadata.values():\n                    if \"links\" not in m:\n                        continue\n                    links_hash = (\n                        lambda link: \"hash\" in link\n                        and link[\"hash\"] == hash\n                        and \"rel\" in link\n                        and (link[\"rel\"] == \"model\" or link[\"rel\"] == \"machinecode\")\n                    )\n                    m[\"links\"] = [link for link in m[\"links\"] if not links_hash(link)]\n\n            del metadata[name]\n            self._save_metadata(path, metadata)\n\n    def _update_metadata_entry(self, path, name, data):\n        with self._get_metadata_lock(path):\n            metadata = copy.copy(self._get_metadata(path))\n            metadata[name] = data\n            self._save_metadata(path, metadata)\n\n    def _copy_metadata_entry(\n        self,\n        source_path,\n        source_name,\n        destination_path,\n        destination_name,\n        delete_source=False,\n        updates=None,\n    ):\n        with self._get_metadata_lock(source_path):\n            source_data = self._get_metadata_entry(source_path, source_name, default={})\n            if not source_data:\n                return\n\n            if delete_source:\n                self._remove_metadata_entry(source_path, source_name)\n\n        if updates is not None:\n            source_data.update(updates)\n\n        with self._get_metadata_lock(destination_path):\n            self._update_metadata_entry(destination_path, destination_name, source_data)\n\n    def _get_metadata(self, path, force=False):\n        import json\n\n        if not force:\n            metadata = self._metadata_cache.get(path)\n            if metadata:\n                return metadata\n\n        self._migrate_metadata(path)\n\n        metadata_path = os.path.join(path, \".metadata.json\")\n\n        metadata = None\n        with self._get_persisted_metadata_lock(path):\n            if os.path.exists(metadata_path):\n                with open(metadata_path, encoding=\"utf-8\") as f:\n                    try:\n                        metadata = json.load(f)\n                    except Exception:\n                        self._logger.exception(\n                            f\"Error while reading .metadata.json from {path}\"\n                        )\n\n        def valid_json(value):\n            try:\n                json.dumps(value, allow_nan=False)\n                return True\n            except Exception:\n                return False\n\n        if isinstance(metadata, dict):\n            old_size = len(metadata)\n            metadata = {k: v for k, v in metadata.items() if valid_json(v)}\n            metadata = {\n                k: v for k, v in metadata.items() if os.path.exists(os.path.join(path, k))\n            }\n            new_size = len(metadata)\n            if new_size != old_size:\n                self._logger.info(\n                    \"Deleted {} stale or invalid entries from metadata for path {}\".format(\n                        old_size - new_size, path\n                    )\n                )\n                self._save_metadata(path, metadata)\n            else:\n                with self._get_metadata_lock(path):\n                    self._metadata_cache[path] = metadata\n            return metadata\n        else:\n            return {}\n\n    def _save_metadata(self, path, metadata):\n        import json\n\n        with self._get_metadata_lock(path):\n            self._metadata_cache[path] = metadata\n\n        with self._get_persisted_metadata_lock(path):\n            metadata_path = os.path.join(path, \".metadata.json\")\n            try:\n                with atomic_write(metadata_path, mode=\"wb\") as f:\n                    f.write(\n                        to_bytes(json.dumps(metadata, indent=2, separators=(\",\", \": \")))\n                    )\n            except Exception:\n                self._logger.exception(f\"Error while writing .metadata.json to {path}\")\n\n    def _delete_metadata(self, path):\n        with self._get_metadata_lock(path):\n            if path in self._metadata_cache:\n                del self._metadata_cache[path]\n\n        with self._get_persisted_metadata_lock(path):\n            metadata_files = (\".metadata.json\", \".metadata.yaml\")\n            for metadata_file in metadata_files:\n                metadata_path = os.path.join(path, metadata_file)\n                if os.path.exists(metadata_path):\n                    try:\n                        os.remove(metadata_path)\n                    except Exception:\n                        self._logger.exception(\n                            f\"Error while deleting {metadata_file} from {path}\"\n                        )\n\n    @staticmethod\n    def _copied_metadata(metadata, name):\n        metadata = copy.copy(metadata)\n        metadata[name] = copy.deepcopy(metadata.get(name, {}))\n        return metadata\n\n    def _migrate_metadata(self, path):\n        # we switched to json in 1.3.9 - if we still have yaml here, migrate it now\n        import json\n\n        with self._get_persisted_metadata_lock(path):\n            metadata_path_yaml = os.path.join(path, \".metadata.yaml\")\n            metadata_path_json = os.path.join(path, \".metadata.json\")\n\n            if not os.path.exists(metadata_path_yaml):\n                # nothing to migrate\n                return\n\n            if os.path.exists(metadata_path_json):\n                # already migrated\n                try:\n                    os.remove(metadata_path_yaml)\n                except Exception:\n                    self._logger.exception(\n                        f\"Error while removing .metadata.yaml from {path}\"\n                    )\n                return\n\n            try:\n                metadata = yaml.load_from_file(path=metadata_path_yaml)\n            except Exception:\n                self._logger.exception(f\"Error while reading .metadata.yaml from {path}\")\n                return\n\n            if not isinstance(metadata, dict):\n                # looks invalid, ignore it\n                return\n\n            with atomic_write(metadata_path_json, mode=\"wb\") as f:\n                f.write(to_bytes(json.dumps(metadata, indent=2, separators=(\",\", \": \"))))\n\n            try:\n                os.remove(metadata_path_yaml)\n            except Exception:\n                self._logger.exception(f\"Error while removing .metadata.yaml from {path}\")\n\n    @contextmanager\n    def _get_metadata_lock(self, path):\n        with self._metadata_lock_mutex:\n            if path not in self._metadata_locks:\n                import threading\n\n                self._metadata_locks[path] = (0, threading.RLock())\n\n            counter, lock = self._metadata_locks[path]\n            counter += 1\n            self._metadata_locks[path] = (counter, lock)\n\n        yield lock\n\n        with self._metadata_lock_mutex:\n            counter = self._metadata_locks[path][0]\n            counter -= 1\n            if counter <= 0:\n                del self._metadata_locks[path]\n            else:\n                self._metadata_locks[path] = (counter, lock)\n\n    @contextmanager\n    def _get_persisted_metadata_lock(self, path):\n        with self._persisted_metadata_lock_mutex:\n            if path not in self._persisted_metadata_locks:\n                import threading\n\n                self._persisted_metadata_locks[path] = (0, threading.RLock())\n\n            counter, lock = self._persisted_metadata_locks[path]\n            counter += 1\n            self._persisted_metadata_locks[path] = (counter, lock)\n\n        yield lock\n\n        with self._persisted_metadata_lock_mutex:\n            counter = self._persisted_metadata_locks[path][0]\n            counter -= 1\n            if counter <= 0:\n                del self._persisted_metadata_locks[path]\n            else:\n                self._persisted_metadata_locks[path] = (counter, lock)\n", "__author__ = \"Gina H\u00e4u\u00dfge <osd@foosel.net>\"\n__license__ = \"GNU Affero General Public License http://www.gnu.org/licenses/agpl.html\"\n__copyright__ = \"Copyright (C) 2014 The OctoPrint Project - Released under terms of the AGPLv3 License\"\n\nimport atexit\nimport base64\nimport functools\nimport logging\nimport logging.config\nimport mimetypes\nimport os\nimport re\nimport signal\nimport sys\nimport time\nimport uuid  # noqa: F401\nfrom collections import OrderedDict, defaultdict\n\nfrom babel import Locale\nfrom flask import (  # noqa: F401\n    Blueprint,\n    Flask,\n    Request,\n    Response,\n    current_app,\n    g,\n    make_response,\n    request,\n    session,\n)\nfrom flask_assets import Bundle, Environment\nfrom flask_babel import Babel, gettext, ngettext  # noqa: F401\nfrom flask_login import (  # noqa: F401\n    LoginManager,\n    current_user,\n    session_protected,\n    user_logged_out,\n)\nfrom watchdog.observers import Observer\nfrom watchdog.observers.polling import PollingObserver\nfrom werkzeug.exceptions import HTTPException\n\nimport octoprint.util\nimport octoprint.util.net\nfrom octoprint.server import util\nfrom octoprint.systemcommands import system_command_manager\nfrom octoprint.util.json import JsonEncoding\nfrom octoprint.vendor.flask_principal import (  # noqa: F401\n    AnonymousIdentity,\n    Identity,\n    Permission,\n    Principal,\n    RoleNeed,\n    UserNeed,\n    identity_changed,\n    identity_loaded,\n)\nfrom octoprint.vendor.sockjs.tornado import SockJSRouter\n\ntry:\n    import fcntl\nexcept ImportError:\n    fcntl = None\n\nSUCCESS = {}\nNO_CONTENT = (\"\", 204, {\"Content-Type\": \"text/plain\"})\nNOT_MODIFIED = (\"Not Modified\", 304, {\"Content-Type\": \"text/plain\"})\n\napp = Flask(\"octoprint\")\n\nassets = None\nbabel = None\nlimiter = None\ndebug = False\nsafe_mode = False\n\nprinter = None\nprinterProfileManager = None\nfileManager = None\nslicingManager = None\nanalysisQueue = None\nuserManager = None\npermissionManager = None\ngroupManager = None\neventManager = None\nloginManager = None\npluginManager = None\npluginLifecycleManager = None\npreemptiveCache = None\njsonEncoder = None\njsonDecoder = None\nconnectivityChecker = None\nenvironmentDetector = None\n\nprincipals = Principal(app)\n\nimport octoprint.access.groups as groups  # noqa: E402\nimport octoprint.access.permissions as permissions  # noqa: E402\n\n# we set admin_permission to a GroupPermission with the default admin group\nadmin_permission = octoprint.util.variable_deprecated(\n    \"admin_permission has been deprecated, \" \"please use individual Permissions instead\",\n    since=\"1.4.0\",\n)(groups.GroupPermission(groups.ADMIN_GROUP))\n\n# we set user_permission to a GroupPermission with the default user group\nuser_permission = octoprint.util.variable_deprecated(\n    \"user_permission has been deprecated, \" \"please use individual Permissions instead\",\n    since=\"1.4.0\",\n)(groups.GroupPermission(groups.USER_GROUP))\n\nimport octoprint._version  # noqa: E402\nimport octoprint.access.groups as groups  # noqa: E402\nimport octoprint.access.users as users  # noqa: E402\nimport octoprint.events as events  # noqa: E402\nimport octoprint.filemanager.analysis  # noqa: E402\nimport octoprint.filemanager.storage  # noqa: E402\nimport octoprint.plugin  # noqa: E402\nimport octoprint.slicing  # noqa: E402\nimport octoprint.timelapse  # noqa: E402\n\n# only import further octoprint stuff down here, as it might depend on things defined above to be initialized already\nfrom octoprint import __branch__, __display_version__, __revision__, __version__\nfrom octoprint.printer.profile import PrinterProfileManager\nfrom octoprint.printer.standard import Printer\nfrom octoprint.server.util import (\n    corsRequestHandler,\n    corsResponseHandler,\n    loginFromApiKeyRequestHandler,\n    requireLoginRequestHandler,\n)\nfrom octoprint.server.util.flask import PreemptiveCache\nfrom octoprint.settings import settings\n\nVERSION = __version__\nBRANCH = __branch__\nDISPLAY_VERSION = __display_version__\nREVISION = __revision__\n\nLOCALES = []\nLANGUAGES = set()\n\n\n@identity_loaded.connect_via(app)\ndef on_identity_loaded(sender, identity):\n    user = load_user(identity.id)\n    if user is None:\n        user = userManager.anonymous_user_factory()\n\n    identity.provides.add(UserNeed(user.get_id()))\n    for need in user.needs:\n        identity.provides.add(need)\n\n\ndef _clear_identity(sender):\n    # Remove session keys set by Flask-Principal\n    for key in (\"identity.id\", \"identity.name\", \"identity.auth_type\"):\n        session.pop(key, None)\n\n    # switch to anonymous identity\n    identity_changed.send(sender, identity=AnonymousIdentity())\n\n\n@session_protected.connect_via(app)\ndef on_session_protected(sender):\n    # session was deleted by session protection, that means the user is no more and we need to clear our identity\n    if session.get(\"remember\", None) == \"clear\":\n        _clear_identity(sender)\n\n\n@user_logged_out.connect_via(app)\ndef on_user_logged_out(sender, user=None):\n    # user was logged out, clear identity\n    _clear_identity(sender)\n\n\ndef load_user(id):\n    if id is None:\n        return None\n\n    if id == \"_api\":\n        return userManager.api_user_factory()\n\n    if session and \"usersession.id\" in session:\n        sessionid = session[\"usersession.id\"]\n    else:\n        sessionid = None\n\n    if sessionid:\n        user = userManager.find_user(userid=id, session=sessionid)\n    else:\n        user = userManager.find_user(userid=id)\n\n    if user and user.is_active:\n        return user\n\n    return None\n\n\ndef load_user_from_request(request):\n    user = None\n\n    if settings().getBoolean([\"accessControl\", \"trustBasicAuthentication\"]):\n        # Basic Authentication?\n        user = util.get_user_for_authorization_header(\n            request.headers.get(\"Authorization\")\n        )\n\n    if settings().getBoolean([\"accessControl\", \"trustRemoteUser\"]):\n        # Remote user header?\n        user = util.get_user_for_remote_user_header(request)\n\n    return user\n\n\ndef unauthorized_user():\n    from flask import abort\n\n    abort(403)\n\n\n# ~~ startup code\n\n\nclass Server:\n    def __init__(\n        self,\n        settings=None,\n        plugin_manager=None,\n        connectivity_checker=None,\n        environment_detector=None,\n        event_manager=None,\n        host=None,\n        port=None,\n        v6_only=False,\n        debug=False,\n        safe_mode=False,\n        allow_root=False,\n        octoprint_daemon=None,\n    ):\n        self._settings = settings\n        self._plugin_manager = plugin_manager\n        self._connectivity_checker = connectivity_checker\n        self._environment_detector = environment_detector\n        self._event_manager = event_manager\n        self._host = host\n        self._port = port\n        self._v6_only = v6_only\n        self._debug = debug\n        self._safe_mode = safe_mode\n        self._allow_root = allow_root\n        self._octoprint_daemon = octoprint_daemon\n        self._server = None\n\n        self._logger = None\n\n        self._lifecycle_callbacks = defaultdict(list)\n\n        self._intermediary_server = None\n\n    def run(self):\n        if not self._allow_root:\n            self._check_for_root()\n\n        if self._settings is None:\n            self._settings = settings()\n\n        if not self._settings.getBoolean([\"server\", \"ignoreIncompleteStartup\"]):\n            self._settings.setBoolean([\"server\", \"incompleteStartup\"], True)\n            self._settings.save()\n\n        if self._plugin_manager is None:\n            self._plugin_manager = octoprint.plugin.plugin_manager()\n\n        global app\n        global babel\n\n        global printer\n        global printerProfileManager\n        global fileManager\n        global slicingManager\n        global analysisQueue\n        global userManager\n        global permissionManager\n        global groupManager\n        global eventManager\n        global loginManager\n        global pluginManager\n        global pluginLifecycleManager\n        global preemptiveCache\n        global jsonEncoder\n        global jsonDecoder\n        global connectivityChecker\n        global environmentDetector\n        global debug\n        global safe_mode\n\n        from tornado.ioloop import IOLoop\n        from tornado.web import Application\n\n        debug = self._debug\n        safe_mode = self._safe_mode\n\n        if safe_mode:\n            self._log_safe_mode_start(safe_mode)\n\n        if self._v6_only and not octoprint.util.net.HAS_V6:\n            raise RuntimeError(\n                \"IPv6 only mode configured but system doesn't support IPv6\"\n            )\n\n        if self._host is None:\n            host = self._settings.get([\"server\", \"host\"])\n            if host is None:\n                if octoprint.util.net.HAS_V6:\n                    host = \"::\"\n                else:\n                    host = \"0.0.0.0\"\n\n            self._host = host\n\n        if \":\" in self._host and not octoprint.util.net.HAS_V6:\n            raise RuntimeError(\n                \"IPv6 host address {!r} configured but system doesn't support IPv6\".format(\n                    self._host\n                )\n            )\n\n        if self._port is None:\n            self._port = self._settings.getInt([\"server\", \"port\"])\n            if self._port is None:\n                self._port = 5000\n\n        self._logger = logging.getLogger(__name__)\n        self._setup_heartbeat_logging()\n        pluginManager = self._plugin_manager\n\n        # monkey patch/fix some stuff\n        util.tornado.fix_json_encode()\n        util.tornado.fix_websocket_check_origin()\n        util.tornado.enable_per_message_deflate_extension()\n        util.flask.fix_flask_jsonify()\n\n        self._setup_mimetypes()\n\n        additional_translation_folders = []\n        if not safe_mode:\n            additional_translation_folders += [\n                self._settings.getBaseFolder(\"translations\")\n            ]\n        util.flask.enable_additional_translations(\n            additional_folders=additional_translation_folders\n        )\n\n        # setup app\n        self._setup_app(app)\n\n        # setup i18n\n        self._setup_i18n(app)\n\n        if self._settings.getBoolean([\"serial\", \"log\"]):\n            # enable debug logging to serial.log\n            logging.getLogger(\"SERIAL\").setLevel(logging.DEBUG)\n\n        if self._settings.getBoolean([\"devel\", \"pluginTimings\"]):\n            # enable plugin timings log\n            logging.getLogger(\"PLUGIN_TIMINGS\").setLevel(logging.DEBUG)\n\n        # start the intermediary server\n        self._start_intermediary_server()\n\n        ### IMPORTANT!\n        ###\n        ### Best do not start any subprocesses until the intermediary server shuts down again or they MIGHT inherit the\n        ### open port and prevent us from firing up Tornado later.\n        ###\n        ### The intermediary server's socket should have the CLOSE_EXEC flag (or its equivalent) set where possible, but\n        ### we can only do that if fcntl is available or we are on Windows, so better safe than sorry.\n        ###\n        ### See also issues #2035 and #2090\n\n        systemCommandManager = system_command_manager()\n        printerProfileManager = PrinterProfileManager()\n        eventManager = self._event_manager\n\n        analysis_queue_factories = {\n            \"gcode\": octoprint.filemanager.analysis.GcodeAnalysisQueue\n        }\n        analysis_queue_hooks = pluginManager.get_hooks(\n            \"octoprint.filemanager.analysis.factory\"\n        )\n        for name, hook in analysis_queue_hooks.items():\n            try:\n                additional_factories = hook()\n                analysis_queue_factories.update(**additional_factories)\n            except Exception:\n                self._logger.exception(\n                    f\"Error while processing analysis queues from {name}\",\n                    extra={\"plugin\": name},\n                )\n        analysisQueue = octoprint.filemanager.analysis.AnalysisQueue(\n            analysis_queue_factories\n        )\n\n        slicingManager = octoprint.slicing.SlicingManager(\n            self._settings.getBaseFolder(\"slicingProfiles\"), printerProfileManager\n        )\n\n        storage_managers = {}\n        storage_managers[\n            octoprint.filemanager.FileDestinations.LOCAL\n        ] = octoprint.filemanager.storage.LocalFileStorage(\n            self._settings.getBaseFolder(\"uploads\"),\n            really_universal=self._settings.getBoolean(\n                [\"feature\", \"enforceReallyUniversalFilenames\"]\n            ),\n        )\n\n        fileManager = octoprint.filemanager.FileManager(\n            analysisQueue,\n            slicingManager,\n            printerProfileManager,\n            initial_storage_managers=storage_managers,\n        )\n        pluginLifecycleManager = LifecycleManager(pluginManager)\n        preemptiveCache = PreemptiveCache(\n            os.path.join(\n                self._settings.getBaseFolder(\"data\"), \"preemptive_cache_config.yaml\"\n            )\n        )\n\n        JsonEncoding.add_encoder(users.User, lambda obj: obj.as_dict())\n        JsonEncoding.add_encoder(groups.Group, lambda obj: obj.as_dict())\n        JsonEncoding.add_encoder(\n            permissions.OctoPrintPermission, lambda obj: obj.as_dict()\n        )\n\n        # start regular check if we are connected to the internet\n        def on_connectivity_change(old_value, new_value):\n            eventManager.fire(\n                events.Events.CONNECTIVITY_CHANGED,\n                payload={\"old\": old_value, \"new\": new_value},\n            )\n\n        connectivityChecker = self._connectivity_checker\n        environmentDetector = self._environment_detector\n\n        def on_settings_update(*args, **kwargs):\n            # make sure our connectivity checker runs with the latest settings\n            connectivityEnabled = self._settings.getBoolean(\n                [\"server\", \"onlineCheck\", \"enabled\"]\n            )\n            connectivityInterval = self._settings.getInt(\n                [\"server\", \"onlineCheck\", \"interval\"]\n            )\n            connectivityHost = self._settings.get([\"server\", \"onlineCheck\", \"host\"])\n            connectivityPort = self._settings.getInt([\"server\", \"onlineCheck\", \"port\"])\n            connectivityName = self._settings.get([\"server\", \"onlineCheck\", \"name\"])\n\n            if (\n                connectivityChecker.enabled != connectivityEnabled\n                or connectivityChecker.interval != connectivityInterval\n                or connectivityChecker.host != connectivityHost\n                or connectivityChecker.port != connectivityPort\n                or connectivityChecker.name != connectivityName\n            ):\n                connectivityChecker.enabled = connectivityEnabled\n                connectivityChecker.interval = connectivityInterval\n                connectivityChecker.host = connectivityHost\n                connectivityChecker.port = connectivityPort\n                connectivityChecker.name = connectivityName\n                connectivityChecker.check_immediately()\n\n        eventManager.subscribe(events.Events.SETTINGS_UPDATED, on_settings_update)\n\n        components = {\n            \"plugin_manager\": pluginManager,\n            \"printer_profile_manager\": printerProfileManager,\n            \"event_bus\": eventManager,\n            \"analysis_queue\": analysisQueue,\n            \"slicing_manager\": slicingManager,\n            \"file_manager\": fileManager,\n            \"plugin_lifecycle_manager\": pluginLifecycleManager,\n            \"preemptive_cache\": preemptiveCache,\n            \"json_encoder\": jsonEncoder,\n            \"json_decoder\": jsonDecoder,\n            \"connectivity_checker\": connectivityChecker,\n            \"environment_detector\": self._environment_detector,\n            \"system_commands\": systemCommandManager,\n        }\n\n        # ~~ setup access control\n\n        # get additional permissions from plugins\n        self._setup_plugin_permissions()\n\n        # create group manager instance\n        group_manager_factories = pluginManager.get_hooks(\n            \"octoprint.access.groups.factory\"\n        )\n        for name, factory in group_manager_factories.items():\n            try:\n                groupManager = factory(components, self._settings)\n                if groupManager is not None:\n                    self._logger.debug(\n                        f\"Created group manager instance from factory {name}\"\n                    )\n                    break\n            except Exception:\n                self._logger.exception(\n                    \"Error while creating group manager instance from factory {}\".format(\n                        name\n                    )\n                )\n        else:\n            group_manager_name = self._settings.get([\"accessControl\", \"groupManager\"])\n            try:\n                clazz = octoprint.util.get_class(group_manager_name)\n                groupManager = clazz()\n            except AttributeError:\n                self._logger.exception(\n                    \"Could not instantiate group manager {}, \"\n                    \"falling back to FilebasedGroupManager!\".format(group_manager_name)\n                )\n                groupManager = octoprint.access.groups.FilebasedGroupManager()\n        components.update({\"group_manager\": groupManager})\n\n        # create user manager instance\n        user_manager_factories = pluginManager.get_hooks(\n            \"octoprint.users.factory\"\n        )  # legacy, set first so that new wins\n        user_manager_factories.update(\n            pluginManager.get_hooks(\"octoprint.access.users.factory\")\n        )\n        for name, factory in user_manager_factories.items():\n            try:\n                userManager = factory(components, self._settings)\n                if userManager is not None:\n                    self._logger.debug(\n                        f\"Created user manager instance from factory {name}\"\n                    )\n                    break\n            except Exception:\n                self._logger.exception(\n                    \"Error while creating user manager instance from factory {}\".format(\n                        name\n                    ),\n                    extra={\"plugin\": name},\n                )\n        else:\n            user_manager_name = self._settings.get([\"accessControl\", \"userManager\"])\n            try:\n                clazz = octoprint.util.get_class(user_manager_name)\n                userManager = clazz(groupManager)\n            except octoprint.access.users.CorruptUserStorage:\n                raise\n            except Exception:\n                self._logger.exception(\n                    \"Could not instantiate user manager {}, \"\n                    \"falling back to FilebasedUserManager!\".format(user_manager_name)\n                )\n                userManager = octoprint.access.users.FilebasedUserManager(groupManager)\n        components.update({\"user_manager\": userManager})\n\n        # create printer instance\n        printer_factories = pluginManager.get_hooks(\"octoprint.printer.factory\")\n        for name, factory in printer_factories.items():\n            try:\n                printer = factory(components)\n                if printer is not None:\n                    self._logger.debug(f\"Created printer instance from factory {name}\")\n                    break\n            except Exception:\n                self._logger.exception(\n                    f\"Error while creating printer instance from factory {name}\",\n                    extra={\"plugin\": name},\n                )\n        else:\n            printer = Printer(fileManager, analysisQueue, printerProfileManager)\n        components.update({\"printer\": printer})\n\n        from octoprint import (\n            init_custom_events,\n            init_settings_plugin_config_migration_and_cleanup,\n        )\n        from octoprint import octoprint_plugin_inject_factory as opif\n        from octoprint import settings_plugin_inject_factory as spif\n\n        init_custom_events(pluginManager)\n\n        octoprint_plugin_inject_factory = opif(self._settings, components)\n        settings_plugin_inject_factory = spif(self._settings)\n\n        pluginManager.implementation_inject_factories = [\n            octoprint_plugin_inject_factory,\n            settings_plugin_inject_factory,\n        ]\n        pluginManager.initialize_implementations()\n\n        init_settings_plugin_config_migration_and_cleanup(pluginManager)\n\n        pluginManager.log_all_plugins()\n\n        # log environment data now\n        self._environment_detector.log_detected_environment()\n\n        # initialize file manager and register it for changes in the registered plugins\n        fileManager.initialize()\n        pluginLifecycleManager.add_callback(\n            [\"enabled\", \"disabled\"], lambda name, plugin: fileManager.reload_plugins()\n        )\n\n        # initialize slicing manager and register it for changes in the registered plugins\n        slicingManager.initialize()\n        pluginLifecycleManager.add_callback(\n            [\"enabled\", \"disabled\"], lambda name, plugin: slicingManager.reload_slicers()\n        )\n\n        # setup jinja2\n        self._setup_jinja2()\n\n        # setup assets\n        self._setup_assets()\n\n        # configure timelapse\n        octoprint.timelapse.valid_timelapse(\"test\")\n        octoprint.timelapse.configure_timelapse()\n\n        # setup command triggers\n        events.CommandTrigger(printer)\n        if self._debug:\n            events.DebugEventListener()\n\n        # setup login manager\n        self._setup_login_manager()\n\n        # register API blueprint\n        self._setup_blueprints()\n\n        ## Tornado initialization starts here\n\n        ioloop = IOLoop()\n        ioloop.install()\n\n        enable_cors = settings().getBoolean([\"api\", \"allowCrossOrigin\"])\n\n        self._router = SockJSRouter(\n            self._create_socket_connection,\n            \"/sockjs\",\n            session_kls=util.sockjs.ThreadSafeSession,\n            user_settings={\n                \"websocket_allow_origin\": \"*\" if enable_cors else \"\",\n                \"jsessionid\": False,\n                \"sockjs_url\": \"../../static/js/lib/sockjs.min.js\",\n            },\n        )\n\n        upload_suffixes = {\n            \"name\": self._settings.get([\"server\", \"uploads\", \"nameSuffix\"]),\n            \"path\": self._settings.get([\"server\", \"uploads\", \"pathSuffix\"]),\n        }\n\n        def mime_type_guesser(path):\n            from octoprint.filemanager import get_mime_type\n\n            return get_mime_type(path)\n\n        def download_name_generator(path):\n            metadata = fileManager.get_metadata(\"local\", path)\n            if metadata and \"display\" in metadata:\n                return metadata[\"display\"]\n\n        download_handler_kwargs = {\"as_attachment\": True, \"allow_client_caching\": False}\n\n        additional_mime_types = {\"mime_type_guesser\": mime_type_guesser}\n\n        ##~~ Permission validators\n\n        access_validators_from_plugins = []\n        for plugin, hook in pluginManager.get_hooks(\n            \"octoprint.server.http.access_validator\"\n        ).items():\n            try:\n                access_validators_from_plugins.append(\n                    util.tornado.access_validation_factory(app, hook)\n                )\n            except Exception:\n                self._logger.exception(\n                    \"Error while adding tornado access validator from plugin {}\".format(\n                        plugin\n                    ),\n                    extra={\"plugin\": plugin},\n                )\n\n        timelapse_validators = [\n            util.tornado.access_validation_factory(\n                app,\n                util.flask.permission_validator,\n                permissions.Permissions.TIMELAPSE_LIST,\n            ),\n        ] + access_validators_from_plugins\n        download_validators = [\n            util.tornado.access_validation_factory(\n                app,\n                util.flask.permission_validator,\n                permissions.Permissions.FILES_DOWNLOAD,\n            ),\n        ] + access_validators_from_plugins\n        log_validators = [\n            util.tornado.access_validation_factory(\n                app,\n                util.flask.permission_validator,\n                permissions.Permissions.PLUGIN_LOGGING_MANAGE,\n            ),\n        ] + access_validators_from_plugins\n        camera_validators = [\n            util.tornado.access_validation_factory(\n                app, util.flask.permission_validator, permissions.Permissions.WEBCAM\n            ),\n        ] + access_validators_from_plugins\n        systeminfo_validators = [\n            util.tornado.access_validation_factory(\n                app, util.flask.permission_validator, permissions.Permissions.SYSTEM\n            )\n        ] + access_validators_from_plugins\n\n        timelapse_permission_validator = {\n            \"access_validation\": util.tornado.validation_chain(*timelapse_validators)\n        }\n        download_permission_validator = {\n            \"access_validation\": util.tornado.validation_chain(*download_validators)\n        }\n        log_permission_validator = {\n            \"access_validation\": util.tornado.validation_chain(*log_validators)\n        }\n        camera_permission_validator = {\n            \"access_validation\": util.tornado.validation_chain(*camera_validators)\n        }\n        systeminfo_permission_validator = {\n            \"access_validation\": util.tornado.validation_chain(*systeminfo_validators)\n        }\n\n        no_hidden_files_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                lambda path: not octoprint.util.is_hidden_path(path), status_code=404\n            )\n        }\n\n        valid_timelapse = lambda path: not octoprint.util.is_hidden_path(path) and (\n            octoprint.timelapse.valid_timelapse(path)\n            or octoprint.timelapse.valid_timelapse_thumbnail(path)\n        )\n        timelapse_path_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                valid_timelapse,\n                status_code=404,\n            )\n        }\n        timelapses_path_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                lambda path: valid_timelapse(path)\n                and os.path.realpath(os.path.abspath(path)).startswith(\n                    settings().getBaseFolder(\"timelapse\")\n                ),\n                status_code=400,\n            )\n        }\n\n        valid_log = lambda path: not octoprint.util.is_hidden_path(\n            path\n        ) and path.endswith(\".log\")\n        log_path_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                valid_log,\n                status_code=404,\n            )\n        }\n        logs_path_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                lambda path: valid_log(path)\n                and os.path.realpath(os.path.abspath(path)).startswith(\n                    settings().getBaseFolder(\"logs\")\n                ),\n                status_code=400,\n            )\n        }\n\n        def joined_dict(*dicts):\n            if not len(dicts):\n                return {}\n\n            joined = {}\n            for d in dicts:\n                joined.update(d)\n            return joined\n\n        util.tornado.RequestlessExceptionLoggingMixin.LOG_REQUEST = debug\n        util.tornado.CorsSupportMixin.ENABLE_CORS = enable_cors\n\n        server_routes = self._router.urls + [\n            # various downloads\n            # .mpg and .mp4 timelapses:\n            (\n                r\"/downloads/timelapse/(.*)\",\n                util.tornado.LargeResponseHandler,\n                joined_dict(\n                    {\"path\": self._settings.getBaseFolder(\"timelapse\")},\n                    timelapse_permission_validator,\n                    download_handler_kwargs,\n                    timelapse_path_validator,\n                ),\n            ),\n            # zipped timelapse bundles\n            (\n                r\"/downloads/timelapses\",\n                util.tornado.DynamicZipBundleHandler,\n                joined_dict(\n                    {\n                        \"as_attachment\": True,\n                        \"attachment_name\": \"octoprint-timelapses.zip\",\n                        \"path_processor\": lambda x: (\n                            x,\n                            os.path.join(self._settings.getBaseFolder(\"timelapse\"), x),\n                        ),\n                    },\n                    timelapse_permission_validator,\n                    timelapses_path_validator,\n                ),\n            ),\n            # uploaded printables\n            (\n                r\"/downloads/files/local/(.*)\",\n                util.tornado.LargeResponseHandler,\n                joined_dict(\n                    {\n                        \"path\": self._settings.getBaseFolder(\"uploads\"),\n                        \"as_attachment\": True,\n                        \"name_generator\": download_name_generator,\n                    },\n                    download_permission_validator,\n                    download_handler_kwargs,\n                    no_hidden_files_validator,\n                    additional_mime_types,\n                ),\n            ),\n            # log files\n            (\n                r\"/downloads/logs/([^/]*)\",\n                util.tornado.LargeResponseHandler,\n                joined_dict(\n                    {\n                        \"path\": self._settings.getBaseFolder(\"logs\"),\n                        \"mime_type_guesser\": lambda *args, **kwargs: \"text/plain\",\n                        \"stream_body\": True,\n                    },\n                    download_handler_kwargs,\n                    log_permission_validator,\n                    log_path_validator,\n                ),\n            ),\n            # zipped log file bundles\n            (\n                r\"/downloads/logs\",\n                util.tornado.DynamicZipBundleHandler,\n                joined_dict(\n                    {\n                        \"as_attachment\": True,\n                        \"attachment_name\": \"octoprint-logs.zip\",\n                        \"path_processor\": lambda x: (\n                            x,\n                            os.path.join(self._settings.getBaseFolder(\"logs\"), x),\n                        ),\n                    },\n                    log_permission_validator,\n                    logs_path_validator,\n                ),\n            ),\n            # system info bundle\n            (\n                r\"/downloads/systeminfo.zip\",\n                util.tornado.SystemInfoBundleHandler,\n                systeminfo_permission_validator,\n            ),\n            # camera snapshot\n            (\n                r\"/downloads/camera/current\",\n                util.tornado.UrlProxyHandler,\n                joined_dict(\n                    {\n                        \"url\": self._settings.get([\"webcam\", \"snapshot\"]),\n                        \"as_attachment\": True,\n                    },\n                    camera_permission_validator,\n                ),\n            ),\n            # generated webassets\n            (\n                r\"/static/webassets/(.*)\",\n                util.tornado.LargeResponseHandler,\n                {\n                    \"path\": os.path.join(\n                        self._settings.getBaseFolder(\"generated\"), \"webassets\"\n                    ),\n                    \"is_pre_compressed\": True,\n                },\n            ),\n            # online indicators - text file with \"online\" as content and a transparent gif\n            (r\"/online.txt\", util.tornado.StaticDataHandler, {\"data\": \"online\\n\"}),\n            (\n                r\"/online.gif\",\n                util.tornado.StaticDataHandler,\n                {\n                    \"data\": bytes(\n                        base64.b64decode(\n                            \"R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"\n                        )\n                    ),\n                    \"content_type\": \"image/gif\",\n                },\n            ),\n            # deprecated endpoints\n            (\n                r\"/api/logs\",\n                util.tornado.DeprecatedEndpointHandler,\n                {\"url\": \"/plugin/logging/logs\"},\n            ),\n            (\n                r\"/api/logs/(.*)\",\n                util.tornado.DeprecatedEndpointHandler,\n                {\"url\": \"/plugin/logging/logs/{0}\"},\n            ),\n        ]\n\n        # fetch additional routes from plugins\n        for name, hook in pluginManager.get_hooks(\"octoprint.server.http.routes\").items():\n            try:\n                result = hook(list(server_routes))\n            except Exception:\n                self._logger.exception(\n                    f\"There was an error while retrieving additional \"\n                    f\"server routes from plugin hook {name}\",\n                    extra={\"plugin\": name},\n                )\n            else:\n                if isinstance(result, (list, tuple)):\n                    for entry in result:\n                        if not isinstance(entry, tuple) or not len(entry) == 3:\n                            continue\n                        if not isinstance(entry[0], str):\n                            continue\n                        if not isinstance(entry[2], dict):\n                            continue\n\n                        route, handler, kwargs = entry\n                        route = r\"/plugin/{name}/{route}\".format(\n                            name=name,\n                            route=route if not route.startswith(\"/\") else route[1:],\n                        )\n\n                        self._logger.debug(\n                            f\"Adding additional route {route} handled by handler {handler} and with additional arguments {kwargs!r}\"\n                        )\n                        server_routes.append((route, handler, kwargs))\n\n        headers = {\n            \"X-Robots-Tag\": \"noindex, nofollow, noimageindex\",\n            \"X-Content-Type-Options\": \"nosniff\",\n        }\n        if not settings().getBoolean([\"server\", \"allowFraming\"]):\n            headers[\"X-Frame-Options\"] = \"sameorigin\"\n\n        removed_headers = [\"Server\"]\n\n        server_routes.append(\n            (\n                r\".*\",\n                util.tornado.UploadStorageFallbackHandler,\n                {\n                    \"fallback\": util.tornado.WsgiInputContainer(\n                        app.wsgi_app, headers=headers, removed_headers=removed_headers\n                    ),\n                    \"file_prefix\": \"octoprint-file-upload-\",\n                    \"file_suffix\": \".tmp\",\n                    \"suffixes\": upload_suffixes,\n                },\n            )\n        )\n\n        transforms = [\n            util.tornado.GlobalHeaderTransform.for_headers(\n                \"OctoPrintGlobalHeaderTransform\",\n                headers=headers,\n                removed_headers=removed_headers,\n            )\n        ]\n\n        self._tornado_app = Application(handlers=server_routes, transforms=transforms)\n        max_body_sizes = [\n            (\n                \"POST\",\n                r\"/api/files/([^/]*)\",\n                self._settings.getInt([\"server\", \"uploads\", \"maxSize\"]),\n            ),\n            (\"POST\", r\"/api/languages\", 5 * 1024 * 1024),\n        ]\n\n        # allow plugins to extend allowed maximum body sizes\n        for name, hook in pluginManager.get_hooks(\n            \"octoprint.server.http.bodysize\"\n        ).items():\n            try:\n                result = hook(list(max_body_sizes))\n            except Exception:\n                self._logger.exception(\n                    f\"There was an error while retrieving additional \"\n                    f\"upload sizes from plugin hook {name}\",\n                    extra={\"plugin\": name},\n                )\n            else:\n                if isinstance(result, (list, tuple)):\n                    for entry in result:\n                        if not isinstance(entry, tuple) or not len(entry) == 3:\n                            continue\n                        if (\n                            entry[0]\n                            not in util.tornado.UploadStorageFallbackHandler.BODY_METHODS\n                        ):\n                            continue\n                        if not isinstance(entry[2], int):\n                            continue\n\n                        method, route, size = entry\n                        route = r\"/plugin/{name}/{route}\".format(\n                            name=name,\n                            route=route if not route.startswith(\"/\") else route[1:],\n                        )\n\n                        self._logger.debug(\n                            f\"Adding maximum body size of {size}B for {method} requests to {route})\"\n                        )\n                        max_body_sizes.append((method, route, size))\n\n        self._stop_intermediary_server()\n\n        # initialize and bind the server\n        trusted_downstream = self._settings.get(\n            [\"server\", \"reverseProxy\", \"trustedDownstream\"]\n        )\n        if not isinstance(trusted_downstream, list):\n            self._logger.warning(\n                \"server.reverseProxy.trustedDownstream is not a list, skipping\"\n            )\n            trusted_downstream = []\n\n        server_kwargs = {\n            \"max_body_sizes\": max_body_sizes,\n            \"default_max_body_size\": self._settings.getInt([\"server\", \"maxSize\"]),\n            \"xheaders\": True,\n            \"trusted_downstream\": trusted_downstream,\n        }\n        if sys.platform == \"win32\":\n            # set 10min idle timeout under windows to hopefully make #2916 less likely\n            server_kwargs.update({\"idle_connection_timeout\": 600})\n\n        self._server = util.tornado.CustomHTTPServer(self._tornado_app, **server_kwargs)\n\n        listening_address = self._host\n        if self._host == \"::\" and not self._v6_only:\n            # special case - tornado only listens on v4 _and_ v6 if we use None as address\n            listening_address = None\n\n        self._server.listen(self._port, address=listening_address)\n\n        ### From now on it's ok to launch subprocesses again\n\n        eventManager.fire(events.Events.STARTUP)\n\n        # analysis backlog\n        fileManager.process_backlog()\n\n        # auto connect\n        if self._settings.getBoolean([\"serial\", \"autoconnect\"]):\n            self._logger.info(\n                \"Autoconnect on startup is configured, trying to connect to the printer...\"\n            )\n            try:\n                (port, baudrate) = (\n                    self._settings.get([\"serial\", \"port\"]),\n                    self._settings.getInt([\"serial\", \"baudrate\"]),\n                )\n                printer_profile = printerProfileManager.get_default()\n                connectionOptions = printer.__class__.get_connection_options()\n                if port in connectionOptions[\"ports\"] or port == \"AUTO\" or port is None:\n                    self._logger.info(\n                        f\"Trying to connect to configured serial port {port}\"\n                    )\n                    printer.connect(\n                        port=port,\n                        baudrate=baudrate,\n                        profile=printer_profile[\"id\"]\n                        if \"id\" in printer_profile\n                        else \"_default\",\n                    )\n                else:\n                    self._logger.info(\n                        \"Could not find configured serial port {} in the system, cannot automatically connect to a non existing printer. Is it plugged in and booted up yet?\"\n                    )\n            except Exception:\n                self._logger.exception(\n                    \"Something went wrong while attempting to automatically connect to the printer\"\n                )\n\n        # start up watchdogs\n        try:\n            watched = self._settings.getBaseFolder(\"watched\")\n            watchdog_handler = util.watchdog.GcodeWatchdogHandler(fileManager, printer)\n            watchdog_handler.initial_scan(watched)\n\n            if self._settings.getBoolean([\"feature\", \"pollWatched\"]):\n                # use less performant polling observer if explicitly configured\n                observer = PollingObserver()\n            else:\n                # use os default\n                observer = Observer()\n\n            observer.schedule(watchdog_handler, watched, recursive=True)\n            observer.start()\n        except Exception:\n            self._logger.exception(\"Error starting watched folder observer\")\n\n        # run our startup plugins\n        octoprint.plugin.call_plugin(\n            octoprint.plugin.StartupPlugin,\n            \"on_startup\",\n            args=(self._host, self._port),\n            sorting_context=\"StartupPlugin.on_startup\",\n        )\n\n        def call_on_startup(name, plugin):\n            implementation = plugin.get_implementation(octoprint.plugin.StartupPlugin)\n            if implementation is None:\n                return\n            implementation.on_startup(self._host, self._port)\n\n        pluginLifecycleManager.add_callback(\"enabled\", call_on_startup)\n\n        # prepare our after startup function\n        def on_after_startup():\n            if self._host == \"::\":\n                if self._v6_only:\n                    # only v6\n                    self._logger.info(f\"Listening on http://[::]:{self._port}\")\n                else:\n                    # all v4 and v6\n                    self._logger.info(\n                        \"Listening on http://0.0.0.0:{port} and http://[::]:{port}\".format(\n                            port=self._port\n                        )\n                    )\n            else:\n                self._logger.info(\n                    \"Listening on http://{}:{}\".format(\n                        self._host if \":\" not in self._host else \"[\" + self._host + \"]\",\n                        self._port,\n                    )\n                )\n\n            if safe_mode and self._settings.getBoolean([\"server\", \"startOnceInSafeMode\"]):\n                self._logger.info(\n                    \"Server started successfully in safe mode as requested from config, removing flag\"\n                )\n                self._settings.setBoolean([\"server\", \"startOnceInSafeMode\"], False)\n                self._settings.save()\n\n            # now this is somewhat ugly, but the issue is the following: startup plugins might want to do things for\n            # which they need the server to be already alive (e.g. for being able to resolve urls, such as favicons\n            # or service xmls or the like). While they are working though the ioloop would block. Therefore we'll\n            # create a single use thread in which to perform our after-startup-tasks, start that and hand back\n            # control to the ioloop\n            def work():\n                octoprint.plugin.call_plugin(\n                    octoprint.plugin.StartupPlugin,\n                    \"on_after_startup\",\n                    sorting_context=\"StartupPlugin.on_after_startup\",\n                )\n\n                def call_on_after_startup(name, plugin):\n                    implementation = plugin.get_implementation(\n                        octoprint.plugin.StartupPlugin\n                    )\n                    if implementation is None:\n                        return\n                    implementation.on_after_startup()\n\n                pluginLifecycleManager.add_callback(\"enabled\", call_on_after_startup)\n\n                # if there was a rogue plugin we wouldn't even have made it here, so remove startup triggered safe mode\n                # flag again...\n                self._settings.setBoolean([\"server\", \"incompleteStartup\"], False)\n                self._settings.save()\n\n                # make a backup of the current config\n                self._settings.backup(ext=\"backup\")\n\n                # when we are through with that we also run our preemptive cache\n                if settings().getBoolean([\"devel\", \"cache\", \"preemptive\"]):\n                    self._execute_preemptive_flask_caching(preemptiveCache)\n\n            import threading\n\n            threading.Thread(target=work).start()\n\n        ioloop.add_callback(on_after_startup)\n\n        # prepare our shutdown function\n        def on_shutdown():\n            # will be called on clean system exit and shutdown the watchdog observer and call the on_shutdown methods\n            # on all registered ShutdownPlugins\n            self._logger.info(\"Shutting down...\")\n            observer.stop()\n            observer.join()\n            eventManager.fire(events.Events.SHUTDOWN)\n\n            self._logger.info(\"Calling on_shutdown on plugins\")\n            octoprint.plugin.call_plugin(\n                octoprint.plugin.ShutdownPlugin,\n                \"on_shutdown\",\n                sorting_context=\"ShutdownPlugin.on_shutdown\",\n            )\n\n            # wait for shutdown event to be processed, but maximally for 15s\n            event_timeout = 15.0\n            if eventManager.join(timeout=event_timeout):\n                self._logger.warning(\n                    \"Event loop was still busy processing after {}s, shutting down anyhow\".format(\n                        event_timeout\n                    )\n                )\n\n            if self._octoprint_daemon is not None:\n                self._logger.info(\"Cleaning up daemon pidfile\")\n                self._octoprint_daemon.terminated()\n\n            self._logger.info(\"Goodbye!\")\n\n        atexit.register(on_shutdown)\n\n        def sigterm_handler(*args, **kwargs):\n            # will stop tornado on SIGTERM, making the program exit cleanly\n            def shutdown_tornado():\n                self._logger.debug(\"Shutting down tornado's IOLoop...\")\n                ioloop.stop()\n\n            self._logger.debug(\"SIGTERM received...\")\n            ioloop.add_callback_from_signal(shutdown_tornado)\n\n        signal.signal(signal.SIGTERM, sigterm_handler)\n\n        try:\n            # this is the main loop - as long as tornado is running, OctoPrint is running\n            ioloop.start()\n            self._logger.debug(\"Tornado's IOLoop stopped\")\n        except (KeyboardInterrupt, SystemExit):\n            pass\n        except Exception:\n            self._logger.fatal(\n                \"Now that is embarrassing... Something really really went wrong here. Please report this including the stacktrace below in OctoPrint's bugtracker. Thanks!\"\n            )\n            self._logger.exception(\"Stacktrace follows:\")\n\n    def _log_safe_mode_start(self, self_mode):\n        self_mode_file = os.path.join(\n            self._settings.getBaseFolder(\"data\"), \"last_safe_mode\"\n        )\n        try:\n            with open(self_mode_file, \"w+\", encoding=\"utf-8\") as f:\n                f.write(self_mode)\n        except Exception as ex:\n            self._logger.warn(f\"Could not write safe mode file {self_mode_file}: {ex}\")\n\n    def _create_socket_connection(self, session):\n        global printer, fileManager, analysisQueue, userManager, eventManager, connectivityChecker\n        return util.sockjs.PrinterStateConnection(\n            printer,\n            fileManager,\n            analysisQueue,\n            userManager,\n            groupManager,\n            eventManager,\n            pluginManager,\n            connectivityChecker,\n            session,\n        )\n\n    def _check_for_root(self):\n        if \"geteuid\" in dir(os) and os.geteuid() == 0:\n            exit(\"You should not run OctoPrint as root!\")\n\n    def _get_locale(self):\n        global LANGUAGES\n\n        if \"l10n\" in request.values:\n            return Locale.negotiate([request.values[\"l10n\"]], LANGUAGES)\n\n        if \"X-Locale\" in request.headers:\n            return Locale.negotiate([request.headers[\"X-Locale\"]], LANGUAGES)\n\n        if hasattr(g, \"identity\") and g.identity:\n            userid = g.identity.id\n            try:\n                user_language = userManager.get_user_setting(\n                    userid, (\"interface\", \"language\")\n                )\n                if user_language is not None and not user_language == \"_default\":\n                    return Locale.negotiate([user_language], LANGUAGES)\n            except octoprint.access.users.UnknownUser:\n                pass\n\n        default_language = self._settings.get([\"appearance\", \"defaultLanguage\"])\n        if (\n            default_language is not None\n            and not default_language == \"_default\"\n            and default_language in LANGUAGES\n        ):\n            return Locale.negotiate([default_language], LANGUAGES)\n\n        return Locale.parse(request.accept_languages.best_match(LANGUAGES))\n\n    def _setup_heartbeat_logging(self):\n        logger = logging.getLogger(__name__ + \".heartbeat\")\n\n        def log_heartbeat():\n            logger.info(\"Server heartbeat <3\")\n\n        interval = settings().getFloat([\"server\", \"heartbeat\"])\n        logger.info(f\"Starting server heartbeat, {interval}s interval\")\n\n        timer = octoprint.util.RepeatedTimer(interval, log_heartbeat)\n        timer.start()\n\n    def _setup_app(self, app):\n        global limiter\n\n        from octoprint.server.util.flask import (\n            OctoPrintFlaskRequest,\n            OctoPrintFlaskResponse,\n            OctoPrintJsonEncoder,\n            OctoPrintSessionInterface,\n            PrefixAwareJinjaEnvironment,\n            ReverseProxiedEnvironment,\n        )\n\n        # we must set this here because setting app.debug will access app.jinja_env\n        app.jinja_environment = PrefixAwareJinjaEnvironment\n\n        app.config[\"TEMPLATES_AUTO_RELOAD\"] = True\n        app.config[\"JSONIFY_PRETTYPRINT_REGULAR\"] = False\n        app.config[\"REMEMBER_COOKIE_HTTPONLY\"] = True\n\n        # we must not set this before TEMPLATES_AUTO_RELOAD is set to True or that won't take\n        app.debug = self._debug\n\n        # setup octoprint's flask json serialization/deserialization\n        app.json_encoder = OctoPrintJsonEncoder\n\n        s = settings()\n\n        secret_key = s.get([\"server\", \"secretKey\"])\n        if not secret_key:\n            import string\n            from random import choice\n\n            chars = string.ascii_lowercase + string.ascii_uppercase + string.digits\n            secret_key = \"\".join(choice(chars) for _ in range(32))\n            s.set([\"server\", \"secretKey\"], secret_key)\n            s.save()\n\n        app.secret_key = secret_key\n\n        reverse_proxied = ReverseProxiedEnvironment(\n            header_prefix=s.get([\"server\", \"reverseProxy\", \"prefixHeader\"]),\n            header_scheme=s.get([\"server\", \"reverseProxy\", \"schemeHeader\"]),\n            header_host=s.get([\"server\", \"reverseProxy\", \"hostHeader\"]),\n            header_server=s.get([\"server\", \"reverseProxy\", \"serverHeader\"]),\n            header_port=s.get([\"server\", \"reverseProxy\", \"portHeader\"]),\n            prefix=s.get([\"server\", \"reverseProxy\", \"prefixFallback\"]),\n            scheme=s.get([\"server\", \"reverseProxy\", \"schemeFallback\"]),\n            host=s.get([\"server\", \"reverseProxy\", \"hostFallback\"]),\n            server=s.get([\"server\", \"reverseProxy\", \"serverFallback\"]),\n            port=s.get([\"server\", \"reverseProxy\", \"portFallback\"]),\n        )\n\n        OctoPrintFlaskRequest.environment_wrapper = reverse_proxied\n        app.request_class = OctoPrintFlaskRequest\n        app.response_class = OctoPrintFlaskResponse\n        app.session_interface = OctoPrintSessionInterface()\n\n        @app.before_request\n        def before_request():\n            g.locale = self._get_locale()\n\n            # used for performance measurement\n            g.start_time = time.monotonic()\n\n            if self._debug and \"perfprofile\" in request.args:\n                try:\n                    from pyinstrument import Profiler\n\n                    g.perfprofiler = Profiler()\n                    g.perfprofiler.start()\n                except ImportError:\n                    # profiler dependency not installed, ignore\n                    pass\n\n        @app.after_request\n        def after_request(response):\n            # send no-cache headers with all POST responses\n            if request.method == \"POST\":\n                response.cache_control.no_cache = True\n\n            response.headers.add(\"X-Clacks-Overhead\", \"GNU Terry Pratchett\")\n\n            if hasattr(g, \"perfprofiler\"):\n                g.perfprofiler.stop()\n                output_html = g.perfprofiler.output_html()\n                return make_response(output_html)\n\n            if hasattr(g, \"start_time\"):\n                end_time = time.monotonic()\n                duration_ms = int((end_time - g.start_time) * 1000)\n                response.headers.add(\"Server-Timing\", f\"app;dur={duration_ms}\")\n\n            return response\n\n        from octoprint.util.jinja import MarkdownFilter\n\n        MarkdownFilter(app)\n\n        from flask_limiter import Limiter\n        from flask_limiter.util import get_remote_address\n\n        app.config[\"RATELIMIT_STRATEGY\"] = \"fixed-window-elastic-expiry\"\n\n        limiter = Limiter(app, key_func=get_remote_address)\n\n    def _setup_i18n(self, app):\n        global babel\n        global LOCALES\n        global LANGUAGES\n\n        babel = Babel(app)\n\n        def get_available_locale_identifiers(locales):\n            result = set()\n\n            # add available translations\n            for locale in locales:\n                result.add(locale.language)\n                if locale.territory:\n                    # if a territory is specified, add that too\n                    result.add(f\"{locale.language}_{locale.territory}\")\n\n            return result\n\n        LOCALES = babel.list_translations()\n        LANGUAGES = get_available_locale_identifiers(LOCALES)\n\n        @babel.localeselector\n        def get_locale():\n            return self._get_locale()\n\n    def _setup_jinja2(self):\n        import re\n\n        app.jinja_env.add_extension(\"jinja2.ext.do\")\n        app.jinja_env.add_extension(\"octoprint.util.jinja.trycatch\")\n\n        def regex_replace(s, find, replace):\n            return re.sub(find, replace, s)\n\n        html_header_regex = re.compile(\n            r\"<h(?P<number>[1-6])>(?P<content>.*?)</h(?P=number)>\"\n        )\n\n        def offset_html_headers(s, offset):\n            def repl(match):\n                number = int(match.group(\"number\"))\n                number += offset\n                if number > 6:\n                    number = 6\n                elif number < 1:\n                    number = 1\n                return \"<h{number}>{content}</h{number}>\".format(\n                    number=number, content=match.group(\"content\")\n                )\n\n            return html_header_regex.sub(repl, s)\n\n        markdown_header_regex = re.compile(\n            r\"^(?P<hashs>#+)\\s+(?P<content>.*)$\", flags=re.MULTILINE\n        )\n\n        def offset_markdown_headers(s, offset):\n            def repl(match):\n                number = len(match.group(\"hashs\"))\n                number += offset\n                if number > 6:\n                    number = 6\n                elif number < 1:\n                    number = 1\n                return \"{hashs} {content}\".format(\n                    hashs=\"#\" * number, content=match.group(\"content\")\n                )\n\n            return markdown_header_regex.sub(repl, s)\n\n        html_link_regex = re.compile(r\"<(?P<tag>a.*?)>(?P<content>.*?)</a>\")\n\n        def externalize_links(text):\n            def repl(match):\n                tag = match.group(\"tag\")\n                if \"href\" not in tag:\n                    return match.group(0)\n\n                if \"target=\" not in tag and \"rel=\" not in tag:\n                    tag += ' target=\"_blank\" rel=\"noreferrer noopener\"'\n\n                content = match.group(\"content\")\n                return f\"<{tag}>{content}</a>\"\n\n            return html_link_regex.sub(repl, text)\n\n        single_quote_regex = re.compile(\"(?<!\\\\\\\\)'\")\n\n        def escape_single_quote(text):\n            return single_quote_regex.sub(\"\\\\'\", text)\n\n        double_quote_regex = re.compile('(?<!\\\\\\\\)\"')\n\n        def escape_double_quote(text):\n            return double_quote_regex.sub('\\\\\"', text)\n\n        app.jinja_env.filters[\"regex_replace\"] = regex_replace\n        app.jinja_env.filters[\"offset_html_headers\"] = offset_html_headers\n        app.jinja_env.filters[\"offset_markdown_headers\"] = offset_markdown_headers\n        app.jinja_env.filters[\"externalize_links\"] = externalize_links\n        app.jinja_env.filters[\"escape_single_quote\"] = app.jinja_env.filters[\n            \"esq\"\n        ] = escape_single_quote\n        app.jinja_env.filters[\"escape_double_quote\"] = app.jinja_env.filters[\n            \"edq\"\n        ] = escape_double_quote\n\n        # configure additional template folders for jinja2\n        import jinja2\n\n        import octoprint.util.jinja\n\n        app.jinja_env.prefix_loader = jinja2.PrefixLoader({})\n\n        loaders = [app.jinja_loader, app.jinja_env.prefix_loader]\n        if octoprint.util.is_running_from_source():\n            root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../..\"))\n            allowed = [\"AUTHORS.md\", \"SUPPORTERS.md\", \"THIRDPARTYLICENSES.md\"]\n            files = {\"_data/\" + name: os.path.join(root, name) for name in allowed}\n            loaders.append(octoprint.util.jinja.SelectedFilesWithConversionLoader(files))\n\n        # TODO: Remove this in 2.0.0\n        warning_message = \"Loading plugin template '{template}' from '{filename}' without plugin prefix, this is deprecated and will soon no longer be supported.\"\n        loaders.append(\n            octoprint.util.jinja.WarningLoader(\n                octoprint.util.jinja.PrefixChoiceLoader(app.jinja_env.prefix_loader),\n                warning_message,\n            )\n        )\n\n        app.jinja_loader = jinja2.ChoiceLoader(loaders)\n\n        self._register_template_plugins()\n\n        # make sure plugin lifecycle events relevant for jinja2 are taken care of\n        def template_enabled(name, plugin):\n            if plugin.implementation is None or not isinstance(\n                plugin.implementation, octoprint.plugin.TemplatePlugin\n            ):\n                return\n            self._register_additional_template_plugin(plugin.implementation)\n\n        def template_disabled(name, plugin):\n            if plugin.implementation is None or not isinstance(\n                plugin.implementation, octoprint.plugin.TemplatePlugin\n            ):\n                return\n            self._unregister_additional_template_plugin(plugin.implementation)\n\n        pluginLifecycleManager.add_callback(\"enabled\", template_enabled)\n        pluginLifecycleManager.add_callback(\"disabled\", template_disabled)\n\n    def _execute_preemptive_flask_caching(self, preemptive_cache):\n        import time\n\n        from werkzeug.test import EnvironBuilder\n\n        # we clean up entries from our preemptive cache settings that haven't been\n        # accessed longer than server.preemptiveCache.until days\n        preemptive_cache_timeout = settings().getInt(\n            [\"server\", \"preemptiveCache\", \"until\"]\n        )\n        cutoff_timestamp = time.time() - preemptive_cache_timeout * 24 * 60 * 60\n\n        def filter_current_entries(entry):\n            \"\"\"Returns True for entries younger than the cutoff date\"\"\"\n            return \"_timestamp\" in entry and entry[\"_timestamp\"] > cutoff_timestamp\n\n        def filter_http_entries(entry):\n            \"\"\"Returns True for entries targeting http or https.\"\"\"\n            return (\n                \"base_url\" in entry\n                and entry[\"base_url\"]\n                and (\n                    entry[\"base_url\"].startswith(\"http://\")\n                    or entry[\"base_url\"].startswith(\"https://\")\n                )\n            )\n\n        def filter_entries(entry):\n            \"\"\"Combined filter.\"\"\"\n            filters = (filter_current_entries, filter_http_entries)\n            return all([f(entry) for f in filters])\n\n        # filter out all old and non-http entries\n        cache_data = preemptive_cache.clean_all_data(\n            lambda root, entries: list(filter(filter_entries, entries))\n        )\n        if not cache_data:\n            return\n\n        def execute_caching():\n            logger = logging.getLogger(__name__ + \".preemptive_cache\")\n\n            for route in sorted(cache_data.keys(), key=lambda x: (x.count(\"/\"), x)):\n                entries = reversed(\n                    sorted(cache_data[route], key=lambda x: x.get(\"_count\", 0))\n                )\n                for kwargs in entries:\n                    plugin = kwargs.get(\"plugin\", None)\n                    if plugin:\n                        try:\n                            plugin_info = pluginManager.get_plugin_info(\n                                plugin, require_enabled=True\n                            )\n                            if plugin_info is None:\n                                logger.info(\n                                    \"About to preemptively cache plugin {} but it is not installed or enabled, preemptive caching makes no sense\".format(\n                                        plugin\n                                    )\n                                )\n                                continue\n\n                            implementation = plugin_info.implementation\n                            if implementation is None or not isinstance(\n                                implementation, octoprint.plugin.UiPlugin\n                            ):\n                                logger.info(\n                                    \"About to preemptively cache plugin {} but it is not a UiPlugin, preemptive caching makes no sense\".format(\n                                        plugin\n                                    )\n                                )\n                                continue\n                            if not implementation.get_ui_preemptive_caching_enabled():\n                                logger.info(\n                                    \"About to preemptively cache plugin {} but it has disabled preemptive caching\".format(\n                                        plugin\n                                    )\n                                )\n                                continue\n                        except Exception:\n                            logger.exception(\n                                \"Error while trying to check if plugin {} has preemptive caching enabled, skipping entry\"\n                            )\n                            continue\n\n                    additional_request_data = kwargs.get(\"_additional_request_data\", {})\n                    kwargs = {\n                        k: v\n                        for k, v in kwargs.items()\n                        if not k.startswith(\"_\") and not k == \"plugin\"\n                    }\n                    kwargs.update(additional_request_data)\n\n                    try:\n                        start = time.monotonic()\n                        if plugin:\n                            logger.info(\n                                \"Preemptively caching {} (ui {}) for {!r}\".format(\n                                    route, plugin, kwargs\n                                )\n                            )\n                        else:\n                            logger.info(\n                                \"Preemptively caching {} (ui _default) for {!r}\".format(\n                                    route, kwargs\n                                )\n                            )\n\n                        headers = kwargs.get(\"headers\", {})\n                        headers[\"X-Force-View\"] = plugin if plugin else \"_default\"\n                        headers[\"X-Preemptive-Recording\"] = \"yes\"\n                        kwargs[\"headers\"] = headers\n\n                        builder = EnvironBuilder(**kwargs)\n                        app(builder.get_environ(), lambda *a, **kw: None)\n\n                        logger.info(f\"... done in {time.monotonic() - start:.2f}s\")\n                    except Exception:\n                        logger.exception(\n                            \"Error while trying to preemptively cache {} for {!r}\".format(\n                                route, kwargs\n                            )\n                        )\n\n        # asynchronous caching\n        import threading\n\n        cache_thread = threading.Thread(\n            target=execute_caching, name=\"Preemptive Cache Worker\"\n        )\n        cache_thread.daemon = True\n        cache_thread.start()\n\n    def _register_template_plugins(self):\n        template_plugins = pluginManager.get_implementations(\n            octoprint.plugin.TemplatePlugin\n        )\n        for plugin in template_plugins:\n            try:\n                self._register_additional_template_plugin(plugin)\n            except Exception:\n                self._logger.exception(\n                    \"Error while trying to register templates of plugin {}, ignoring it\".format(\n                        plugin._identifier\n                    )\n                )\n\n    def _register_additional_template_plugin(self, plugin):\n        import octoprint.util.jinja\n\n        folder = plugin.get_template_folder()\n        if (\n            folder is not None\n            and plugin.template_folder_key not in app.jinja_env.prefix_loader.mapping\n        ):\n            loader = octoprint.util.jinja.FilteredFileSystemLoader(\n                [plugin.get_template_folder()],\n                path_filter=lambda x: not octoprint.util.is_hidden_path(x),\n            )\n\n            app.jinja_env.prefix_loader.mapping[plugin.template_folder_key] = loader\n\n    def _unregister_additional_template_plugin(self, plugin):\n        folder = plugin.get_template_folder()\n        if (\n            folder is not None\n            and plugin.template_folder_key in app.jinja_env.prefix_loader.mapping\n        ):\n            del app.jinja_env.prefix_loader.mapping[plugin.template_folder_key]\n\n    def _setup_blueprints(self):\n        # do not remove or the index view won't be found\n        import octoprint.server.views  # noqa: F401\n        from octoprint.server.api import api\n        from octoprint.server.util.flask import make_api_error\n\n        blueprints = [api]\n        api_endpoints = [\"/api\"]\n        registrators = [functools.partial(app.register_blueprint, api, url_prefix=\"/api\")]\n\n        # also register any blueprints defined in BlueprintPlugins\n        (\n            blueprints_from_plugins,\n            api_endpoints_from_plugins,\n            registrators_from_plugins,\n        ) = self._prepare_blueprint_plugins()\n        blueprints += blueprints_from_plugins\n        api_endpoints += api_endpoints_from_plugins\n        registrators += registrators_from_plugins\n\n        # and register a blueprint for serving the static files of asset plugins which are not blueprint plugins themselves\n        (blueprints_from_assets, registrators_from_assets) = self._prepare_asset_plugins()\n        blueprints += blueprints_from_assets\n        registrators += registrators_from_assets\n\n        # make sure all before/after_request hook results are attached as well\n        self._add_plugin_request_handlers_to_blueprints(*blueprints)\n\n        # register everything with the system\n        for registrator in registrators:\n            registrator()\n\n        @app.errorhandler(HTTPException)\n        def _handle_api_error(ex):\n            if any(map(lambda x: request.path.startswith(x), api_endpoints)):\n                return make_api_error(ex.description, ex.code)\n            else:\n                return ex\n\n    def _prepare_blueprint_plugins(self):\n        def register_plugin_blueprint(plugin, blueprint, url_prefix):\n            try:\n                app.register_blueprint(\n                    blueprint, url_prefix=url_prefix, name_prefix=\"plugin\"\n                )\n                self._logger.debug(\n                    f\"Registered API of plugin {plugin} under URL prefix {url_prefix}\"\n                )\n            except Exception:\n                self._logger.exception(\n                    f\"Error while registering blueprint of plugin {plugin}, ignoring it\",\n                    extra={\"plugin\": plugin},\n                )\n\n        blueprints = []\n        api_endpoints = []\n        registrators = []\n\n        blueprint_plugins = octoprint.plugin.plugin_manager().get_implementations(\n            octoprint.plugin.BlueprintPlugin\n        )\n        for plugin in blueprint_plugins:\n            blueprint, prefix = self._prepare_blueprint_plugin(plugin)\n\n            blueprints.append(blueprint)\n            api_endpoints += map(\n                lambda x: prefix + x, plugin.get_blueprint_api_prefixes()\n            )\n            registrators.append(\n                functools.partial(\n                    register_plugin_blueprint, plugin._identifier, blueprint, prefix\n                )\n            )\n\n        return blueprints, api_endpoints, registrators\n\n    def _prepare_asset_plugins(self):\n        def register_asset_blueprint(plugin, blueprint, url_prefix):\n            try:\n                app.register_blueprint(\n                    blueprint, url_prefix=url_prefix, name_prefix=\"plugin\"\n                )\n                self._logger.debug(\n                    f\"Registered assets of plugin {plugin} under URL prefix {url_prefix}\"\n                )\n            except Exception:\n                self._logger.exception(\n                    f\"Error while registering blueprint of plugin {plugin}, ignoring it\",\n                    extra={\"plugin\": plugin},\n                )\n\n        blueprints = []\n        registrators = []\n\n        asset_plugins = octoprint.plugin.plugin_manager().get_implementations(\n            octoprint.plugin.AssetPlugin\n        )\n        for plugin in asset_plugins:\n            if isinstance(plugin, octoprint.plugin.BlueprintPlugin):\n                continue\n            blueprint, prefix = self._prepare_asset_plugin(plugin)\n\n            blueprints.append(blueprint)\n            registrators.append(\n                functools.partial(\n                    register_asset_blueprint, plugin._identifier, blueprint, prefix\n                )\n            )\n\n        return blueprints, registrators\n\n    def _prepare_blueprint_plugin(self, plugin):\n        name = plugin._identifier\n        blueprint = plugin.get_blueprint()\n        if blueprint is None:\n            return\n\n        blueprint.before_request(corsRequestHandler)\n        blueprint.before_request(loginFromApiKeyRequestHandler)\n        blueprint.after_request(corsResponseHandler)\n\n        if plugin.is_blueprint_protected():\n            blueprint.before_request(requireLoginRequestHandler)\n\n        url_prefix = f\"/plugin/{name}\"\n        return blueprint, url_prefix\n\n    def _prepare_asset_plugin(self, plugin):\n        name = plugin._identifier\n\n        url_prefix = f\"/plugin/{name}\"\n        blueprint = Blueprint(name, name, static_folder=plugin.get_asset_folder())\n        return blueprint, url_prefix\n\n    def _add_plugin_request_handlers_to_blueprints(self, *blueprints):\n        before_hooks = octoprint.plugin.plugin_manager().get_hooks(\n            \"octoprint.server.api.before_request\"\n        )\n        after_hooks = octoprint.plugin.plugin_manager().get_hooks(\n            \"octoprint.server.api.after_request\"\n        )\n\n        for name, hook in before_hooks.items():\n            plugin = octoprint.plugin.plugin_manager().get_plugin(name)\n            for blueprint in blueprints:\n                try:\n                    result = hook(plugin=plugin)\n                    if isinstance(result, (list, tuple)):\n                        for h in result:\n                            blueprint.before_request(h)\n                except Exception:\n                    self._logger.exception(\n                        \"Error processing before_request hooks from plugin {}\".format(\n                            plugin\n                        ),\n                        extra={\"plugin\": name},\n                    )\n\n        for name, hook in after_hooks.items():\n            plugin = octoprint.plugin.plugin_manager().get_plugin(name)\n            for blueprint in blueprints:\n                try:\n                    result = hook(plugin=plugin)\n                    if isinstance(result, (list, tuple)):\n                        for h in result:\n                            blueprint.after_request(h)\n                except Exception:\n                    self._logger.exception(\n                        \"Error processing after_request hooks from plugin {}\".format(\n                            plugin\n                        ),\n                        extra={\"plugin\": name},\n                    )\n\n    def _setup_mimetypes(self):\n        # Safety measures for Windows... apparently the mimetypes module takes its translation from the windows\n        # registry, and if for some weird reason that gets borked the reported MIME types can be all over the place.\n        # Since at least in Chrome that can cause hilarious issues with JS files (refusal to run them and thus a\n        # borked UI) we make sure that .js always maps to the correct application/javascript, and also throw in a\n        # .css -> text/css for good measure.\n        #\n        # See #3367\n        mimetypes.add_type(\"application/javascript\", \".js\")\n        mimetypes.add_type(\"text/css\", \".css\")\n\n    def _setup_assets(self):\n        global app\n        global assets\n        global pluginManager\n\n        from octoprint.server.util.webassets import MemoryManifest  # noqa: F401\n\n        util.flask.fix_webassets_filtertool()\n\n        base_folder = self._settings.getBaseFolder(\"generated\")\n\n        # clean the folder\n        if self._settings.getBoolean([\"devel\", \"webassets\", \"clean_on_startup\"]):\n            import errno\n            import shutil\n\n            for entry, recreate in (\n                (\"webassets\", True),\n                # no longer used, but clean up just in case\n                (\".webassets-cache\", False),\n                (\".webassets-manifest.json\", False),\n            ):\n                path = os.path.join(base_folder, entry)\n\n                # delete path if it exists\n                if os.path.exists(path):\n                    try:\n                        self._logger.debug(f\"Deleting {path}...\")\n                        if os.path.isdir(path):\n                            shutil.rmtree(path)\n                        else:\n                            os.remove(path)\n                    except Exception:\n                        self._logger.exception(\n                            f\"Error while trying to delete {path}, \" f\"leaving it alone\"\n                        )\n                        continue\n\n                # re-create path if necessary\n                if recreate:\n                    self._logger.debug(f\"Creating {path}...\")\n                    error_text = (\n                        f\"Error while trying to re-create {path}, that might cause \"\n                        f\"errors with the webassets cache\"\n                    )\n                    try:\n                        os.makedirs(path)\n                    except OSError as e:\n                        if e.errno == errno.EACCES:\n                            # that might be caused by the user still having the folder open somewhere, let's try again after\n                            # waiting a bit\n                            import time\n\n                            for n in range(3):\n                                time.sleep(0.5)\n                                self._logger.debug(\n                                    \"Creating {path}: Retry #{retry} after {time}s\".format(\n                                        path=path, retry=n + 1, time=(n + 1) * 0.5\n                                    )\n                                )\n                                try:\n                                    os.makedirs(path)\n                                    break\n                                except Exception:\n                                    if self._logger.isEnabledFor(logging.DEBUG):\n                                        self._logger.exception(\n                                            f\"Ignored error while creating \"\n                                            f\"directory {path}\"\n                                        )\n                                    pass\n                            else:\n                                # this will only get executed if we never did\n                                # successfully execute makedirs above\n                                self._logger.exception(error_text)\n                                continue\n                        else:\n                            # not an access error, so something we don't understand\n                            # went wrong -> log an error and stop\n                            self._logger.exception(error_text)\n                            continue\n                    except Exception:\n                        # not an OSError, so something we don't understand\n                        # went wrong -> log an error and stop\n                        self._logger.exception(error_text)\n                        continue\n\n                self._logger.info(f\"Reset webasset folder {path}...\")\n\n        AdjustedEnvironment = type(Environment)(\n            Environment.__name__,\n            (Environment,),\n            {\"resolver_class\": util.flask.PluginAssetResolver},\n        )\n\n        class CustomDirectoryEnvironment(AdjustedEnvironment):\n            @property\n            def directory(self):\n                return base_folder\n\n        assets = CustomDirectoryEnvironment(app)\n        assets.debug = not self._settings.getBoolean([\"devel\", \"webassets\", \"bundle\"])\n\n        # we should rarely if ever regenerate the webassets in production and can wait a\n        # few seconds for regeneration in development, if it means we can get rid of\n        # a whole monkey patch and in internal use of pickle with non-tamperproof files\n        assets.cache = False\n        assets.manifest = \"memory\"\n\n        UpdaterType = type(util.flask.SettingsCheckUpdater)(\n            util.flask.SettingsCheckUpdater.__name__,\n            (util.flask.SettingsCheckUpdater,),\n            {\"updater\": assets.updater},\n        )\n        assets.updater = UpdaterType\n\n        preferred_stylesheet = self._settings.get([\"devel\", \"stylesheet\"])\n\n        dynamic_core_assets = util.flask.collect_core_assets()\n        dynamic_plugin_assets = util.flask.collect_plugin_assets(\n            preferred_stylesheet=preferred_stylesheet\n        )\n\n        js_libs = [\n            \"js/lib/babel-polyfill.min.js\",\n            \"js/lib/jquery/jquery.min.js\",\n            \"js/lib/modernizr.custom.js\",\n            \"js/lib/lodash.min.js\",\n            \"js/lib/sprintf.min.js\",\n            \"js/lib/knockout.js\",\n            \"js/lib/knockout.mapping-latest.js\",\n            \"js/lib/babel.js\",\n            \"js/lib/bootstrap/bootstrap.js\",\n            \"js/lib/bootstrap/bootstrap-modalmanager.js\",\n            \"js/lib/bootstrap/bootstrap-modal.js\",\n            \"js/lib/bootstrap/bootstrap-slider.js\",\n            \"js/lib/bootstrap/bootstrap-tabdrop.js\",\n            \"js/lib/jquery/jquery-ui.js\",\n            \"js/lib/jquery/jquery.flot.js\",\n            \"js/lib/jquery/jquery.flot.time.js\",\n            \"js/lib/jquery/jquery.flot.crosshair.js\",\n            \"js/lib/jquery/jquery.flot.resize.js\",\n            \"js/lib/jquery/jquery.iframe-transport.js\",\n            \"js/lib/jquery/jquery.fileupload.js\",\n            \"js/lib/jquery/jquery.slimscroll.min.js\",\n            \"js/lib/jquery/jquery.qrcode.min.js\",\n            \"js/lib/jquery/jquery.bootstrap.wizard.js\",\n            \"js/lib/pnotify/pnotify.core.min.js\",\n            \"js/lib/pnotify/pnotify.buttons.min.js\",\n            \"js/lib/pnotify/pnotify.callbacks.min.js\",\n            \"js/lib/pnotify/pnotify.confirm.min.js\",\n            \"js/lib/pnotify/pnotify.desktop.min.js\",\n            \"js/lib/pnotify/pnotify.history.min.js\",\n            \"js/lib/pnotify/pnotify.mobile.min.js\",\n            \"js/lib/pnotify/pnotify.nonblock.min.js\",\n            \"js/lib/pnotify/pnotify.reference.min.js\",\n            \"js/lib/pnotify/pnotify.tooltip.min.js\",\n            \"js/lib/pnotify/pnotify.maxheight.js\",\n            \"js/lib/moment-with-locales.min.js\",\n            \"js/lib/pusher.color.min.js\",\n            \"js/lib/detectmobilebrowser.js\",\n            \"js/lib/ua-parser.min.js\",\n            \"js/lib/md5.min.js\",\n            \"js/lib/bootstrap-slider-knockout-binding.js\",\n            \"js/lib/loglevel.min.js\",\n            \"js/lib/sockjs.min.js\",\n            \"js/lib/hls.js\",\n            \"js/lib/less.js\",\n        ]\n\n        css_libs = [\n            \"css/bootstrap.min.css\",\n            \"css/bootstrap-modal.css\",\n            \"css/bootstrap-slider.css\",\n            \"css/bootstrap-tabdrop.css\",\n            \"vendor/font-awesome-3.2.1/css/font-awesome.min.css\",\n            \"vendor/font-awesome-5.15.1/css/all.min.css\",\n            \"vendor/font-awesome-5.15.1/css/v4-shims.min.css\",\n            \"css/jquery.fileupload-ui.css\",\n            \"css/pnotify.core.min.css\",\n            \"css/pnotify.buttons.min.css\",\n            \"css/pnotify.history.min.css\",\n        ]\n\n        # a couple of custom filters\n        from webassets.filter import register_filter\n\n        from octoprint.server.util.webassets import (\n            GzipFile,\n            JsDelimiterBundler,\n            JsPluginBundle,\n            LessImportRewrite,\n            RJSMinExtended,\n            SourceMapRemove,\n            SourceMapRewrite,\n        )\n\n        register_filter(LessImportRewrite)\n        register_filter(SourceMapRewrite)\n        register_filter(SourceMapRemove)\n        register_filter(JsDelimiterBundler)\n        register_filter(GzipFile)\n        register_filter(RJSMinExtended)\n\n        def all_assets_for_plugins(collection):\n            \"\"\"Gets all plugin assets for a dict of plugin->assets\"\"\"\n            result = []\n            for assets in collection.values():\n                result += assets\n            return result\n\n        # -- JS --------------------------------------------------------------------------------------------------------\n\n        filters = [\"sourcemap_remove\"]\n        if self._settings.getBoolean([\"devel\", \"webassets\", \"minify\"]):\n            filters += [\"rjsmin_extended\"]\n        filters += [\"js_delimiter_bundler\", \"gzip\"]\n\n        js_filters = filters\n        if self._settings.getBoolean([\"devel\", \"webassets\", \"minify_plugins\"]):\n            js_plugin_filters = js_filters\n        else:\n            js_plugin_filters = [x for x in js_filters if x not in (\"rjsmin_extended\",)]\n\n        def js_bundles_for_plugins(collection, filters=None):\n            \"\"\"Produces JsPluginBundle instances that output IIFE wrapped assets\"\"\"\n            result = OrderedDict()\n            for plugin, assets in collection.items():\n                if len(assets):\n                    result[plugin] = JsPluginBundle(plugin, *assets, filters=filters)\n            return result\n\n        js_core = (\n            dynamic_core_assets[\"js\"]\n            + all_assets_for_plugins(dynamic_plugin_assets[\"bundled\"][\"js\"])\n            + [\"js/app/dataupdater.js\", \"js/app/helpers.js\", \"js/app/main.js\"]\n        )\n        js_plugins = js_bundles_for_plugins(\n            dynamic_plugin_assets[\"external\"][\"js\"], filters=\"js_delimiter_bundler\"\n        )\n\n        clientjs_core = dynamic_core_assets[\"clientjs\"] + all_assets_for_plugins(\n            dynamic_plugin_assets[\"bundled\"][\"clientjs\"]\n        )\n        clientjs_plugins = js_bundles_for_plugins(\n            dynamic_plugin_assets[\"external\"][\"clientjs\"], filters=\"js_delimiter_bundler\"\n        )\n\n        js_libs_bundle = Bundle(\n            *js_libs, output=\"webassets/packed_libs.js\", filters=\",\".join(js_filters)\n        )\n\n        js_core_bundle = Bundle(\n            *js_core, output=\"webassets/packed_core.js\", filters=\",\".join(js_filters)\n        )\n\n        if len(js_plugins) == 0:\n            js_plugins_bundle = Bundle(*[])\n        else:\n            js_plugins_bundle = Bundle(\n                *js_plugins.values(),\n                output=\"webassets/packed_plugins.js\",\n                filters=\",\".join(js_plugin_filters),\n            )\n\n        js_app_bundle = Bundle(\n            js_plugins_bundle,\n            js_core_bundle,\n            output=\"webassets/packed_app.js\",\n            filters=\",\".join(js_plugin_filters),\n        )\n\n        js_client_core_bundle = Bundle(\n            *clientjs_core,\n            output=\"webassets/packed_client_core.js\",\n            filters=\",\".join(js_filters),\n        )\n\n        if len(clientjs_plugins) == 0:\n            js_client_plugins_bundle = Bundle(*[])\n        else:\n            js_client_plugins_bundle = Bundle(\n                *clientjs_plugins.values(),\n                output=\"webassets/packed_client_plugins.js\",\n                filters=\",\".join(js_plugin_filters),\n            )\n\n        js_client_bundle = Bundle(\n            js_client_core_bundle,\n            js_client_plugins_bundle,\n            output=\"webassets/packed_client.js\",\n            filters=\",\".join(js_plugin_filters),\n        )\n\n        # -- CSS -------------------------------------------------------------------------------------------------------\n\n        css_filters = [\"cssrewrite\", \"gzip\"]\n\n        css_core = list(dynamic_core_assets[\"css\"]) + all_assets_for_plugins(\n            dynamic_plugin_assets[\"bundled\"][\"css\"]\n        )\n        css_plugins = list(\n            all_assets_for_plugins(dynamic_plugin_assets[\"external\"][\"css\"])\n        )\n\n        css_libs_bundle = Bundle(\n            *css_libs, output=\"webassets/packed_libs.css\", filters=\",\".join(css_filters)\n        )\n\n        if len(css_core) == 0:\n            css_core_bundle = Bundle(*[])\n        else:\n            css_core_bundle = Bundle(\n                *css_core,\n                output=\"webassets/packed_core.css\",\n                filters=\",\".join(css_filters),\n            )\n\n        if len(css_plugins) == 0:\n            css_plugins_bundle = Bundle(*[])\n        else:\n            css_plugins_bundle = Bundle(\n                *css_plugins,\n                output=\"webassets/packed_plugins.css\",\n                filters=\",\".join(css_filters),\n            )\n\n        css_app_bundle = Bundle(\n            css_core,\n            css_plugins,\n            output=\"webassets/packed_app.css\",\n            filters=\",\".join(css_filters),\n        )\n\n        # -- LESS ------------------------------------------------------------------------------------------------------\n\n        less_filters = [\"cssrewrite\", \"less_importrewrite\", \"gzip\"]\n\n        less_core = list(dynamic_core_assets[\"less\"]) + all_assets_for_plugins(\n            dynamic_plugin_assets[\"bundled\"][\"less\"]\n        )\n        less_plugins = all_assets_for_plugins(dynamic_plugin_assets[\"external\"][\"less\"])\n\n        if len(less_core) == 0:\n            less_core_bundle = Bundle(*[])\n        else:\n            less_core_bundle = Bundle(\n                *less_core,\n                output=\"webassets/packed_core.less\",\n                filters=\",\".join(less_filters),\n            )\n\n        if len(less_plugins) == 0:\n            less_plugins_bundle = Bundle(*[])\n        else:\n            less_plugins_bundle = Bundle(\n                *less_plugins,\n                output=\"webassets/packed_plugins.less\",\n                filters=\",\".join(less_filters),\n            )\n\n        less_app_bundle = Bundle(\n            less_core,\n            less_plugins,\n            output=\"webassets/packed_app.less\",\n            filters=\",\".join(less_filters),\n        )\n\n        # -- asset registration ----------------------------------------------------------------------------------------\n\n        assets.register(\"js_libs\", js_libs_bundle)\n        assets.register(\"js_client_core\", js_client_core_bundle)\n        for plugin, bundle in clientjs_plugins.items():\n            # register our collected clientjs plugin bundles so that they are bound to the environment\n            assets.register(f\"js_client_plugin_{plugin}\", bundle)\n        assets.register(\"js_client_plugins\", js_client_plugins_bundle)\n        assets.register(\"js_client\", js_client_bundle)\n        assets.register(\"js_core\", js_core_bundle)\n        for plugin, bundle in js_plugins.items():\n            # register our collected plugin bundles so that they are bound to the environment\n            assets.register(f\"js_plugin_{plugin}\", bundle)\n        assets.register(\"js_plugins\", js_plugins_bundle)\n        assets.register(\"js_app\", js_app_bundle)\n        assets.register(\"css_libs\", css_libs_bundle)\n        assets.register(\"css_core\", css_core_bundle)\n        assets.register(\"css_plugins\", css_plugins_bundle)\n        assets.register(\"css_app\", css_app_bundle)\n        assets.register(\"less_core\", less_core_bundle)\n        assets.register(\"less_plugins\", less_plugins_bundle)\n        assets.register(\"less_app\", less_app_bundle)\n\n    def _setup_login_manager(self):\n        global loginManager\n\n        loginManager = LoginManager()\n\n        # \"strong\" is incompatible to remember me, see maxcountryman/flask-login#156. It also causes issues with\n        # clients toggling between IPv4 and IPv6 client addresses due to names being resolved one way or the other as\n        # at least observed on a Win10 client targeting \"localhost\", resolved as both \"127.0.0.1\" and \"::1\"\n        loginManager.session_protection = \"basic\"\n\n        loginManager.user_loader(load_user)\n        loginManager.unauthorized_handler(unauthorized_user)\n        loginManager.anonymous_user = userManager.anonymous_user_factory\n        loginManager.request_loader(load_user_from_request)\n\n        loginManager.init_app(app, add_context_processor=False)\n\n    def _start_intermediary_server(self):\n        import socket\n        import threading\n        from http.server import BaseHTTPRequestHandler, HTTPServer\n\n        host = self._host\n        port = self._port\n\n        class IntermediaryServerHandler(BaseHTTPRequestHandler):\n            def __init__(self, rules=None, *args, **kwargs):\n                if rules is None:\n                    rules = []\n                self.rules = rules\n                BaseHTTPRequestHandler.__init__(self, *args, **kwargs)\n\n            def do_GET(self):\n                request_path = self.path\n                if \"?\" in request_path:\n                    request_path = request_path[0 : request_path.find(\"?\")]\n\n                for rule in self.rules:\n                    path, data, content_type = rule\n                    if request_path == path:\n                        self.send_response(200)\n                        if content_type:\n                            self.send_header(\"Content-Type\", content_type)\n                        self.end_headers()\n                        if isinstance(data, str):\n                            data = data.encode(\"utf-8\")\n                        self.wfile.write(data)\n                        break\n                else:\n                    self.send_response(404)\n                    self.wfile.write(b\"Not found\")\n\n        base_path = os.path.realpath(\n            os.path.join(os.path.dirname(__file__), \"..\", \"static\")\n        )\n        rules = [\n            (\n                \"/\",\n                [\n                    \"intermediary.html\",\n                ],\n                \"text/html\",\n            ),\n            (\"/favicon.ico\", [\"img\", \"tentacle-20x20.png\"], \"image/png\"),\n            (\n                \"/intermediary.gif\",\n                bytes(\n                    base64.b64decode(\n                        \"R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"\n                    )\n                ),\n                \"image/gif\",\n            ),\n        ]\n\n        def contents(args):\n            path = os.path.join(base_path, *args)\n            if not os.path.isfile(path):\n                return \"\"\n\n            with open(path, \"rb\") as f:\n                data = f.read()\n            return data\n\n        def process(rule):\n            if len(rule) == 2:\n                path, data = rule\n                content_type = None\n            else:\n                path, data, content_type = rule\n\n            if isinstance(data, (list, tuple)):\n                data = contents(data)\n\n            return path, data, content_type\n\n        rules = list(\n            map(process, filter(lambda rule: len(rule) == 2 or len(rule) == 3, rules))\n        )\n\n        HTTPServerV4 = HTTPServer\n\n        class HTTPServerV6(HTTPServer):\n            address_family = socket.AF_INET6\n\n        class HTTPServerV6SingleStack(HTTPServerV6):\n            def __init__(self, *args, **kwargs):\n                HTTPServerV6.__init__(self, *args, **kwargs)\n\n                # explicitly set V6ONLY flag - seems to be the default, but just to make sure...\n                self.socket.setsockopt(\n                    octoprint.util.net.IPPROTO_IPV6, octoprint.util.net.IPV6_V6ONLY, 1\n                )\n\n        class HTTPServerV6DualStack(HTTPServerV6):\n            def __init__(self, *args, **kwargs):\n                HTTPServerV6.__init__(self, *args, **kwargs)\n\n                # explicitly unset V6ONLY flag\n                self.socket.setsockopt(\n                    octoprint.util.net.IPPROTO_IPV6, octoprint.util.net.IPV6_V6ONLY, 0\n                )\n\n        if \":\" in host:\n            # v6\n            if host == \"::\" and not self._v6_only:\n                ServerClass = HTTPServerV6DualStack\n            else:\n                ServerClass = HTTPServerV6SingleStack\n        else:\n            # v4\n            ServerClass = HTTPServerV4\n\n        if host == \"::\":\n            if self._v6_only:\n                self._logger.debug(f\"Starting intermediary server on http://[::]:{port}\")\n            else:\n                self._logger.debug(\n                    \"Starting intermediary server on http://0.0.0.0:{port} and http://[::]:{port}\".format(\n                        port=port\n                    )\n                )\n        else:\n            self._logger.debug(\n                \"Starting intermediary server on http://{}:{}\".format(\n                    host if \":\" not in host else \"[\" + host + \"]\", port\n                )\n            )\n\n        self._intermediary_server = ServerClass(\n            (host, port),\n            lambda *args, **kwargs: IntermediaryServerHandler(rules, *args, **kwargs),\n            bind_and_activate=False,\n        )\n\n        # if possible, make sure our socket's port descriptor isn't handed over to subprocesses\n        from octoprint.util.platform import set_close_exec\n\n        try:\n            set_close_exec(self._intermediary_server.fileno())\n        except Exception:\n            self._logger.exception(\n                \"Error while attempting to set_close_exec on intermediary server socket\"\n            )\n\n        # then bind the server and have it serve our handler until stopped\n        try:\n            self._intermediary_server.server_bind()\n            self._intermediary_server.server_activate()\n        except Exception as exc:\n            self._intermediary_server.server_close()\n\n            if isinstance(exc, UnicodeDecodeError) and sys.platform == \"win32\":\n                # we end up here if the hostname contains non-ASCII characters due to\n                # https://bugs.python.org/issue26227 - tell the user they need\n                # to either change their hostname or read up other options in\n                # https://github.com/OctoPrint/OctoPrint/issues/3963\n                raise CannotStartServerException(\n                    \"OctoPrint cannot start due to a Python bug \"\n                    \"(https://bugs.python.org/issue26227). Your \"\n                    \"computer's host name contains non-ASCII characters. \"\n                    \"Please either change your computer's host name to \"\n                    \"contain only ASCII characters, or take a look at \"\n                    \"https://github.com/OctoPrint/OctoPrint/issues/3963 for \"\n                    \"other options.\"\n                )\n            else:\n                raise\n\n        def serve():\n            try:\n                self._intermediary_server.serve_forever()\n            except Exception:\n                self._logger.exception(\"Error in intermediary server\")\n\n        thread = threading.Thread(target=serve)\n        thread.daemon = True\n        thread.start()\n\n        self._logger.info(\"Intermediary server started\")\n\n    def _stop_intermediary_server(self):\n        if self._intermediary_server is None:\n            return\n        self._logger.info(\"Shutting down intermediary server...\")\n        self._intermediary_server.shutdown()\n        self._intermediary_server.server_close()\n        self._logger.info(\"Intermediary server shut down\")\n\n    def _setup_plugin_permissions(self):\n        global pluginManager\n\n        from octoprint.access.permissions import PluginOctoPrintPermission\n\n        key_whitelist = re.compile(r\"[A-Za-z0-9_]*\")\n\n        def permission_key(plugin, definition):\n            return \"PLUGIN_{}_{}\".format(plugin.upper(), definition[\"key\"].upper())\n\n        def permission_name(plugin, definition):\n            return \"{}: {}\".format(plugin, definition[\"name\"])\n\n        def permission_role(plugin, role):\n            return f\"plugin_{plugin}_{role}\"\n\n        def process_regular_permission(plugin_info, definition):\n            permissions = []\n            for key in definition.get(\"permissions\", []):\n                permission = octoprint.access.permissions.Permissions.find(key)\n\n                if permission is None:\n                    # if there is still no permission found, postpone this - maybe it is a permission from\n                    # another plugin that hasn't been loaded yet\n                    return False\n\n                permissions.append(permission)\n\n            roles = definition.get(\"roles\", [])\n            description = definition.get(\"description\", \"\")\n            dangerous = definition.get(\"dangerous\", False)\n            default_groups = definition.get(\"default_groups\", [])\n\n            roles_and_permissions = [\n                permission_role(plugin_info.key, role) for role in roles\n            ] + permissions\n\n            key = permission_key(plugin_info.key, definition)\n            permission = PluginOctoPrintPermission(\n                permission_name(plugin_info.name, definition),\n                description,\n                plugin=plugin_info.key,\n                dangerous=dangerous,\n                default_groups=default_groups,\n                *roles_and_permissions,\n            )\n            setattr(\n                octoprint.access.permissions.Permissions,\n                key,\n                PluginOctoPrintPermission(\n                    permission_name(plugin_info.name, definition),\n                    description,\n                    plugin=plugin_info.key,\n                    dangerous=dangerous,\n                    default_groups=default_groups,\n                    *roles_and_permissions,\n                ),\n            )\n\n            self._logger.info(\n                \"Added new permission from plugin {}: {} (needs: {!r})\".format(\n                    plugin_info.key, key, \", \".join(map(repr, permission.needs))\n                )\n            )\n            return True\n\n        postponed = []\n\n        hooks = pluginManager.get_hooks(\"octoprint.access.permissions\")\n        for name, factory in hooks.items():\n            try:\n                if isinstance(factory, (tuple, list)):\n                    additional_permissions = list(factory)\n                elif callable(factory):\n                    additional_permissions = factory()\n                else:\n                    raise ValueError(\"factory must be either a callable, tuple or list\")\n\n                if not isinstance(additional_permissions, (tuple, list)):\n                    raise ValueError(\n                        \"factory result must be either a tuple or a list of permission definition dicts\"\n                    )\n\n                plugin_info = pluginManager.get_plugin_info(name)\n                for p in additional_permissions:\n                    if not isinstance(p, dict):\n                        continue\n\n                    if \"key\" not in p or \"name\" not in p:\n                        continue\n\n                    if not key_whitelist.match(p[\"key\"]):\n                        self._logger.warning(\n                            \"Got permission with invalid key from plugin {}: {}\".format(\n                                name, p[\"key\"]\n                            )\n                        )\n                        continue\n\n                    if not process_regular_permission(plugin_info, p):\n                        postponed.append((plugin_info, p))\n            except Exception:\n                self._logger.exception(\n                    f\"Error while creating permission instance/s from {name}\"\n                )\n\n        # final resolution passes\n        pass_number = 1\n        still_postponed = []\n        while len(postponed):\n            start_length = len(postponed)\n            self._logger.debug(\n                \"Plugin permission resolution pass #{}, \"\n                \"{} unresolved permissions...\".format(pass_number, start_length)\n            )\n\n            for plugin_info, definition in postponed:\n                if not process_regular_permission(plugin_info, definition):\n                    still_postponed.append((plugin_info, definition))\n\n            self._logger.debug(\n                \"... pass #{} done, {} permissions left to resolve\".format(\n                    pass_number, len(still_postponed)\n                )\n            )\n\n            if len(still_postponed) == start_length:\n                # no change, looks like some stuff is unresolvable - let's bail\n                for plugin_info, definition in still_postponed:\n                    self._logger.warning(\n                        \"Unable to resolve permission from {}: {!r}\".format(\n                            plugin_info.key, definition\n                        )\n                    )\n                break\n\n            postponed = still_postponed\n            still_postponed = []\n            pass_number += 1\n\n\nclass LifecycleManager:\n    def __init__(self, plugin_manager):\n        self._plugin_manager = plugin_manager\n\n        self._plugin_lifecycle_callbacks = defaultdict(list)\n        self._logger = logging.getLogger(__name__)\n\n        def wrap_plugin_event(lifecycle_event, new_handler):\n            orig_handler = getattr(self._plugin_manager, \"on_plugin_\" + lifecycle_event)\n\n            def handler(*args, **kwargs):\n                if callable(orig_handler):\n                    orig_handler(*args, **kwargs)\n                if callable(new_handler):\n                    new_handler(*args, **kwargs)\n\n            return handler\n\n        def on_plugin_event_factory(lifecycle_event):\n            def on_plugin_event(name, plugin):\n                self.on_plugin_event(lifecycle_event, name, plugin)\n\n            return on_plugin_event\n\n        for event in (\"loaded\", \"unloaded\", \"enabled\", \"disabled\"):\n            wrap_plugin_event(event, on_plugin_event_factory(event))\n\n    def on_plugin_event(self, event, name, plugin):\n        for lifecycle_callback in self._plugin_lifecycle_callbacks[event]:\n            lifecycle_callback(name, plugin)\n\n    def add_callback(self, events, callback):\n        if isinstance(events, str):\n            events = [events]\n\n        for event in events:\n            self._plugin_lifecycle_callbacks[event].append(callback)\n\n    def remove_callback(self, callback, events=None):\n        if events is None:\n            for event in self._plugin_lifecycle_callbacks:\n                if callback in self._plugin_lifecycle_callbacks[event]:\n                    self._plugin_lifecycle_callbacks[event].remove(callback)\n        else:\n            if isinstance(events, str):\n                events = [events]\n\n            for event in events:\n                if callback in self._plugin_lifecycle_callbacks[event]:\n                    self._plugin_lifecycle_callbacks[event].remove(callback)\n\n\nclass CannotStartServerException(Exception):\n    pass\n", "__author__ = \"Gina H\u00e4u\u00dfge <osd@foosel.net>\"\n__license__ = \"GNU Affero General Public License http://www.gnu.org/licenses/agpl.html\"\n__copyright__ = \"Copyright (C) 2014 The OctoPrint Project - Released under terms of the AGPLv3 License\"\n\nimport hashlib\nimport logging\nimport os\nimport threading\nfrom urllib.parse import quote as urlquote\n\nimport psutil\nfrom flask import abort, jsonify, make_response, request, url_for\n\nimport octoprint.filemanager\nimport octoprint.filemanager.storage\nimport octoprint.filemanager.util\nimport octoprint.slicing\nfrom octoprint.access.permissions import Permissions\nfrom octoprint.events import Events\nfrom octoprint.filemanager.destinations import FileDestinations\nfrom octoprint.server import (\n    NO_CONTENT,\n    current_user,\n    eventManager,\n    fileManager,\n    printer,\n    slicingManager,\n)\nfrom octoprint.server.api import api\nfrom octoprint.server.util.flask import (\n    get_json_command_from_request,\n    no_firstrun_access,\n    with_revalidation_checking,\n)\nfrom octoprint.settings import settings, valid_boolean_trues\nfrom octoprint.util import sv, time_this\n\n# ~~ GCODE file handling\n\n_file_cache = {}\n_file_cache_mutex = threading.RLock()\n\n_DATA_FORMAT_VERSION = \"v2\"\n\n\ndef _clear_file_cache():\n    with _file_cache_mutex:\n        _file_cache.clear()\n\n\ndef _create_lastmodified(path, recursive):\n    path = path[len(\"/api/files\") :]\n    if path.startswith(\"/\"):\n        path = path[1:]\n\n    if path == \"\":\n        # all storages involved\n        lms = [0]\n        for storage in fileManager.registered_storages:\n            try:\n                lms.append(fileManager.last_modified(storage, recursive=recursive))\n            except Exception:\n                logging.getLogger(__name__).exception(\n                    \"There was an error retrieving the last modified data from storage {}\".format(\n                        storage\n                    )\n                )\n                lms.append(None)\n\n        if any(filter(lambda x: x is None, lms)):\n            # we return None if ANY of the involved storages returned None\n            return None\n\n        # if we reach this point, we return the maximum of all dates\n        return max(lms)\n\n    else:\n        if \"/\" in path:\n            storage, path_in_storage = path.split(\"/\", 1)\n        else:\n            storage = path\n            path_in_storage = None\n\n        try:\n            return fileManager.last_modified(\n                storage, path=path_in_storage, recursive=recursive\n            )\n        except Exception:\n            logging.getLogger(__name__).exception(\n                \"There was an error retrieving the last modified data from storage {} and path {}\".format(\n                    storage, path_in_storage\n                )\n            )\n            return None\n\n\ndef _create_etag(path, filter, recursive, lm=None):\n    if lm is None:\n        lm = _create_lastmodified(path, recursive)\n\n    if lm is None:\n        return None\n\n    hash = hashlib.sha1()\n\n    def hash_update(value):\n        value = value.encode(\"utf-8\")\n        hash.update(value)\n\n    hash_update(str(lm))\n    hash_update(str(filter))\n    hash_update(str(recursive))\n\n    path = path[len(\"/api/files\") :]\n    if path.startswith(\"/\"):\n        path = path[1:]\n\n    if \"/\" in path:\n        storage, _ = path.split(\"/\", 1)\n    else:\n        storage = path\n\n    if path == \"\" or storage == FileDestinations.SDCARD:\n        # include sd data in etag\n        hash_update(repr(sorted(printer.get_sd_files(), key=lambda x: sv(x[\"name\"]))))\n\n    hash_update(_DATA_FORMAT_VERSION)  # increment version if we change the API format\n\n    return hash.hexdigest()\n\n\n@api.route(\"/files\", methods=[\"GET\"])\n@Permissions.FILES_LIST.require(403)\n@with_revalidation_checking(\n    etag_factory=lambda lm=None: _create_etag(\n        request.path,\n        request.values.get(\"filter\", False),\n        request.values.get(\"recursive\", False),\n        lm=lm,\n    ),\n    lastmodified_factory=lambda: _create_lastmodified(\n        request.path, request.values.get(\"recursive\", False)\n    ),\n    unless=lambda: request.values.get(\"force\", False)\n    or request.values.get(\"_refresh\", False),\n)\ndef readGcodeFiles():\n    filter = request.values.get(\"filter\", False)\n    recursive = request.values.get(\"recursive\", \"false\") in valid_boolean_trues\n    force = request.values.get(\"force\", \"false\") in valid_boolean_trues\n\n    files = _getFileList(\n        FileDestinations.LOCAL,\n        filter=filter,\n        recursive=recursive,\n        allow_from_cache=not force,\n    )\n    files.extend(_getFileList(FileDestinations.SDCARD, allow_from_cache=not force))\n\n    usage = psutil.disk_usage(settings().getBaseFolder(\"uploads\", check_writable=False))\n    return jsonify(files=files, free=usage.free, total=usage.total)\n\n\n@api.route(\"/files/test\", methods=[\"POST\"])\n@Permissions.FILES_LIST.require(403)\ndef runFilesTest():\n    valid_commands = {\n        \"sanitize\": [\"storage\", \"path\", \"filename\"],\n        \"exists\": [\"storage\", \"path\", \"filename\"],\n    }\n\n    command, data, response = get_json_command_from_request(request, valid_commands)\n    if response is not None:\n        return response\n\n    def sanitize(storage, path, filename):\n        sanitized_path = fileManager.sanitize_path(storage, path)\n        sanitized_name = fileManager.sanitize_name(storage, filename)\n        joined = fileManager.join_path(storage, sanitized_path, sanitized_name)\n        return sanitized_path, sanitized_name, joined\n\n    if command == \"sanitize\":\n        _, _, sanitized = sanitize(data[\"storage\"], data[\"path\"], data[\"filename\"])\n        return jsonify(sanitized=sanitized)\n    elif command == \"exists\":\n        storage = data[\"storage\"]\n        path = data[\"path\"]\n        filename = data[\"filename\"]\n\n        sanitized_path, _, sanitized = sanitize(storage, path, filename)\n\n        exists = fileManager.file_exists(storage, sanitized)\n        if exists:\n            suggestion = filename\n            name, ext = os.path.splitext(filename)\n            counter = 0\n            while fileManager.file_exists(\n                storage,\n                fileManager.join_path(\n                    storage,\n                    sanitized_path,\n                    fileManager.sanitize_name(storage, suggestion),\n                ),\n            ):\n                counter += 1\n                suggestion = f\"{name}_{counter}{ext}\"\n            return jsonify(exists=True, suggestion=suggestion)\n        else:\n            return jsonify(exists=False)\n\n\n@api.route(\"/files/<string:origin>\", methods=[\"GET\"])\n@Permissions.FILES_LIST.require(403)\n@with_revalidation_checking(\n    etag_factory=lambda lm=None: _create_etag(\n        request.path,\n        request.values.get(\"filter\", False),\n        request.values.get(\"recursive\", False),\n        lm=lm,\n    ),\n    lastmodified_factory=lambda: _create_lastmodified(\n        request.path, request.values.get(\"recursive\", False)\n    ),\n    unless=lambda: request.values.get(\"force\", False)\n    or request.values.get(\"_refresh\", False),\n)\ndef readGcodeFilesForOrigin(origin):\n    if origin not in [FileDestinations.LOCAL, FileDestinations.SDCARD]:\n        abort(404)\n\n    filter = request.values.get(\"filter\", False)\n    recursive = request.values.get(\"recursive\", \"false\") in valid_boolean_trues\n    force = request.values.get(\"force\", \"false\") in valid_boolean_trues\n\n    files = _getFileList(\n        origin, filter=filter, recursive=recursive, allow_from_cache=not force\n    )\n\n    if origin == FileDestinations.LOCAL:\n        usage = psutil.disk_usage(\n            settings().getBaseFolder(\"uploads\", check_writable=False)\n        )\n        return jsonify(files=files, free=usage.free, total=usage.total)\n    else:\n        return jsonify(files=files)\n\n\n@api.route(\"/files/<string:target>/<path:filename>\", methods=[\"GET\"])\n@Permissions.FILES_LIST.require(403)\n@with_revalidation_checking(\n    etag_factory=lambda lm=None: _create_etag(\n        request.path,\n        request.values.get(\"filter\", False),\n        request.values.get(\"recursive\", False),\n        lm=lm,\n    ),\n    lastmodified_factory=lambda: _create_lastmodified(\n        request.path, request.values.get(\"recursive\", False)\n    ),\n    unless=lambda: request.values.get(\"force\", False)\n    or request.values.get(\"_refresh\", False),\n)\ndef readGcodeFile(target, filename):\n    if target not in [FileDestinations.LOCAL, FileDestinations.SDCARD]:\n        abort(404)\n\n    if not _validate(target, filename):\n        abort(404)\n\n    recursive = False\n    if \"recursive\" in request.values:\n        recursive = request.values[\"recursive\"] in valid_boolean_trues\n\n    file = _getFileDetails(target, filename, recursive=recursive)\n    if not file:\n        abort(404)\n\n    return jsonify(file)\n\n\ndef _getFileDetails(origin, path, recursive=True):\n    parent, path = os.path.split(path)\n    files = _getFileList(origin, path=parent, recursive=recursive, level=1)\n\n    for f in files:\n        if f[\"name\"] == path:\n            return f\n    else:\n        return None\n\n\n@time_this(\n    logtarget=__name__ + \".timings\",\n    message=\"{func}({func_args},{func_kwargs}) took {timing:.2f}ms\",\n    incl_func_args=True,\n    log_enter=True,\n    message_enter=\"Entering {func}({func_args},{func_kwargs})...\",\n)\ndef _getFileList(\n    origin, path=None, filter=None, recursive=False, level=0, allow_from_cache=True\n):\n    if origin == FileDestinations.SDCARD:\n        sdFileList = printer.get_sd_files(refresh=not allow_from_cache)\n\n        files = []\n        if sdFileList is not None:\n            for f in sdFileList:\n                type_path = octoprint.filemanager.get_file_type(f[\"name\"])\n                if not type_path:\n                    # only supported extensions\n                    continue\n                else:\n                    file_type = type_path[0]\n\n                file = {\n                    \"type\": file_type,\n                    \"typePath\": type_path,\n                    \"name\": f[\"name\"],\n                    \"display\": f[\"display\"] if f[\"display\"] else f[\"name\"],\n                    \"path\": f[\"name\"],\n                    \"origin\": FileDestinations.SDCARD,\n                    \"refs\": {\n                        \"resource\": url_for(\n                            \".readGcodeFile\",\n                            target=FileDestinations.SDCARD,\n                            filename=f[\"name\"],\n                            _external=True,\n                        )\n                    },\n                }\n                if f[\"size\"] is not None:\n                    file.update({\"size\": f[\"size\"]})\n                files.append(file)\n    else:\n        filter_func = None\n        if filter:\n            filter_func = lambda entry, entry_data: octoprint.filemanager.valid_file_type(\n                entry, type=filter\n            )\n\n        with _file_cache_mutex:\n            cache_key = f\"{origin}:{path}:{recursive}:{filter}\"\n            files, lastmodified = _file_cache.get(cache_key, ([], None))\n            # recursive needs to be True for lastmodified queries so we get lastmodified of whole subtree - #3422\n            if (\n                not allow_from_cache\n                or lastmodified is None\n                or lastmodified\n                < fileManager.last_modified(origin, path=path, recursive=True)\n            ):\n                files = list(\n                    fileManager.list_files(\n                        origin,\n                        path=path,\n                        filter=filter_func,\n                        recursive=recursive,\n                        level=level,\n                        force_refresh=not allow_from_cache,\n                    )[origin].values()\n                )\n                lastmodified = fileManager.last_modified(\n                    origin, path=path, recursive=True\n                )\n                _file_cache[cache_key] = (files, lastmodified)\n\n        def analyse_recursively(files, path=None):\n            if path is None:\n                path = \"\"\n\n            result = []\n            for file_or_folder in files:\n                # make a shallow copy in order to not accidentally modify the cached data\n                file_or_folder = dict(file_or_folder)\n\n                file_or_folder[\"origin\"] = FileDestinations.LOCAL\n\n                if file_or_folder[\"type\"] == \"folder\":\n                    if \"children\" in file_or_folder:\n                        file_or_folder[\"children\"] = analyse_recursively(\n                            file_or_folder[\"children\"].values(),\n                            path + file_or_folder[\"name\"] + \"/\",\n                        )\n\n                    file_or_folder[\"refs\"] = {\n                        \"resource\": url_for(\n                            \".readGcodeFile\",\n                            target=FileDestinations.LOCAL,\n                            filename=path + file_or_folder[\"name\"],\n                            _external=True,\n                        )\n                    }\n                else:\n                    if (\n                        \"analysis\" in file_or_folder\n                        and octoprint.filemanager.valid_file_type(\n                            file_or_folder[\"name\"], type=\"gcode\"\n                        )\n                    ):\n                        file_or_folder[\"gcodeAnalysis\"] = file_or_folder[\"analysis\"]\n                        del file_or_folder[\"analysis\"]\n\n                    if (\n                        \"history\" in file_or_folder\n                        and octoprint.filemanager.valid_file_type(\n                            file_or_folder[\"name\"], type=\"gcode\"\n                        )\n                    ):\n                        # convert print log\n                        history = file_or_folder[\"history\"]\n                        del file_or_folder[\"history\"]\n                        success = 0\n                        failure = 0\n                        last = None\n                        for entry in history:\n                            success += 1 if \"success\" in entry and entry[\"success\"] else 0\n                            failure += (\n                                1 if \"success\" in entry and not entry[\"success\"] else 0\n                            )\n                            if not last or (\n                                \"timestamp\" in entry\n                                and \"timestamp\" in last\n                                and entry[\"timestamp\"] > last[\"timestamp\"]\n                            ):\n                                last = entry\n                        if last:\n                            prints = {\n                                \"success\": success,\n                                \"failure\": failure,\n                                \"last\": {\n                                    \"success\": last[\"success\"],\n                                    \"date\": last[\"timestamp\"],\n                                },\n                            }\n                            if \"printTime\" in last:\n                                prints[\"last\"][\"printTime\"] = last[\"printTime\"]\n                            file_or_folder[\"prints\"] = prints\n\n                    file_or_folder[\"refs\"] = {\n                        \"resource\": url_for(\n                            \".readGcodeFile\",\n                            target=FileDestinations.LOCAL,\n                            filename=file_or_folder[\"path\"],\n                            _external=True,\n                        ),\n                        \"download\": url_for(\"index\", _external=True)\n                        + \"downloads/files/\"\n                        + FileDestinations.LOCAL\n                        + \"/\"\n                        + urlquote(file_or_folder[\"path\"]),\n                    }\n\n                result.append(file_or_folder)\n\n            return result\n\n        files = analyse_recursively(files)\n\n    return files\n\n\ndef _verifyFileExists(origin, filename):\n    if origin == FileDestinations.SDCARD:\n        return filename in (x[\"name\"] for x in printer.get_sd_files())\n    else:\n        return fileManager.file_exists(origin, filename)\n\n\ndef _verifyFolderExists(origin, foldername):\n    if origin == FileDestinations.SDCARD:\n        return False\n    else:\n        return fileManager.folder_exists(origin, foldername)\n\n\ndef _isBusy(target, path):\n    currentOrigin, currentPath = _getCurrentFile()\n    if (\n        currentPath is not None\n        and currentOrigin == target\n        and fileManager.file_in_path(FileDestinations.LOCAL, path, currentPath)\n        and (printer.is_printing() or printer.is_paused())\n    ):\n        return True\n\n    return any(\n        target == x[0] and fileManager.file_in_path(FileDestinations.LOCAL, path, x[1])\n        for x in fileManager.get_busy_files()\n    )\n\n\n@api.route(\"/files/<string:target>\", methods=[\"POST\"])\n@no_firstrun_access\n@Permissions.FILES_UPLOAD.require(403)\ndef uploadGcodeFile(target):\n    input_name = \"file\"\n    input_upload_name = (\n        input_name + \".\" + settings().get([\"server\", \"uploads\", \"nameSuffix\"])\n    )\n    input_upload_path = (\n        input_name + \".\" + settings().get([\"server\", \"uploads\", \"pathSuffix\"])\n    )\n    if input_upload_name in request.values and input_upload_path in request.values:\n        if target not in [FileDestinations.LOCAL, FileDestinations.SDCARD]:\n            abort(404)\n\n        upload = octoprint.filemanager.util.DiskFileWrapper(\n            request.values[input_upload_name], request.values[input_upload_path]\n        )\n\n        # Store any additional user data the caller may have passed.\n        userdata = None\n        if \"userdata\" in request.values:\n            import json\n\n            try:\n                userdata = json.loads(request.values[\"userdata\"])\n            except Exception:\n                abort(400, description=\"userdata contains invalid JSON\")\n\n        # check preconditions for SD upload\n        if target == FileDestinations.SDCARD and not settings().getBoolean(\n            [\"feature\", \"sdSupport\"]\n        ):\n            abort(404)\n\n        sd = target == FileDestinations.SDCARD\n        if sd:\n            # validate that all preconditions for SD upload are met before attempting it\n            if not (\n                printer.is_operational()\n                and not (printer.is_printing() or printer.is_paused())\n            ):\n                abort(\n                    409,\n                    description=\"Can not upload to SD card, printer is either not operational or already busy\",\n                )\n            if not printer.is_sd_ready():\n                abort(409, description=\"Can not upload to SD card, not yet initialized\")\n\n        # evaluate select and print parameter and if set check permissions & preconditions\n        # and adjust as necessary\n        #\n        # we do NOT abort(409) here since this would be a backwards incompatible behaviour change\n        # on the API, but instead return the actually effective select and print flags in the response\n        #\n        # note that this behaviour might change in a future API version\n        select_request = (\n            \"select\" in request.values\n            and request.values[\"select\"] in valid_boolean_trues\n            and Permissions.FILES_SELECT.can()\n        )\n        print_request = (\n            \"print\" in request.values\n            and request.values[\"print\"] in valid_boolean_trues\n            and Permissions.PRINT.can()\n        )\n\n        to_select = select_request\n        to_print = print_request\n        if (to_select or to_print) and not (\n            printer.is_operational()\n            and not (printer.is_printing() or printer.is_paused())\n        ):\n            # can't select or print files if not operational or ready\n            to_select = to_print = False\n\n        # determine future filename of file to be uploaded, abort if it can't be uploaded\n        try:\n            # FileDestinations.LOCAL = should normally be target, but can't because SDCard handling isn't implemented yet\n            canonPath, canonFilename = fileManager.canonicalize(\n                FileDestinations.LOCAL, upload.filename\n            )\n            if request.values.get(\"path\"):\n                canonPath = request.values.get(\"path\")\n            if request.values.get(\"filename\"):\n                canonFilename = request.values.get(\"filename\")\n\n            futurePath = fileManager.sanitize_path(FileDestinations.LOCAL, canonPath)\n            futureFilename = fileManager.sanitize_name(\n                FileDestinations.LOCAL, canonFilename\n            )\n        except Exception:\n            canonFilename = None\n            futurePath = None\n            futureFilename = None\n\n        if futureFilename is None:\n            abort(400, description=\"Can not upload file, invalid file name\")\n\n        # prohibit overwriting currently selected file while it's being printed\n        futureFullPath = fileManager.join_path(\n            FileDestinations.LOCAL, futurePath, futureFilename\n        )\n        futureFullPathInStorage = fileManager.path_in_storage(\n            FileDestinations.LOCAL, futureFullPath\n        )\n\n        if not printer.can_modify_file(futureFullPathInStorage, sd):\n            abort(\n                409,\n                description=\"Trying to overwrite file that is currently being printed\",\n            )\n\n        if (\n            fileManager.file_exists(FileDestinations.LOCAL, futureFullPathInStorage)\n            and request.values.get(\"noOverwrite\") in valid_boolean_trues\n        ):\n            abort(409, description=\"File already exists and noOverwrite was set\")\n\n        reselect = printer.is_current_file(futureFullPathInStorage, sd)\n\n        user = current_user.get_name()\n\n        def fileProcessingFinished(filename, absFilename, destination):\n            \"\"\"\n            Callback for when the file processing (upload, optional slicing, addition to analysis queue) has\n            finished.\n\n            Depending on the file's destination triggers either streaming to SD card or directly calls to_select.\n            \"\"\"\n\n            if (\n                destination == FileDestinations.SDCARD\n                and octoprint.filemanager.valid_file_type(filename, \"machinecode\")\n            ):\n                return filename, printer.add_sd_file(\n                    filename,\n                    absFilename,\n                    on_success=selectAndOrPrint,\n                    tags={\"source:api\", \"api:files.sd\"},\n                )\n            else:\n                selectAndOrPrint(filename, absFilename, destination)\n                return filename\n\n        def selectAndOrPrint(filename, absFilename, destination):\n            \"\"\"\n            Callback for when the file is ready to be selected and optionally printed. For SD file uploads this is only\n            the case after they have finished streaming to the printer, which is why this callback is also used\n            for the corresponding call to addSdFile.\n\n            Selects the just uploaded file if either to_select or to_print are True, or if the\n            exact file is already selected, such reloading it.\n            \"\"\"\n            if octoprint.filemanager.valid_file_type(added_file, \"gcode\") and (\n                to_select or to_print or reselect\n            ):\n                printer.select_file(\n                    absFilename,\n                    destination == FileDestinations.SDCARD,\n                    to_print,\n                    user,\n                )\n\n        try:\n            added_file = fileManager.add_file(\n                FileDestinations.LOCAL,\n                futureFullPathInStorage,\n                upload,\n                allow_overwrite=True,\n                display=canonFilename,\n            )\n        except octoprint.filemanager.storage.StorageError as e:\n            if e.code == octoprint.filemanager.storage.StorageError.INVALID_FILE:\n                abort(415, description=\"Could not upload file, invalid type\")\n            else:\n                abort(500, description=\"Could not upload file\")\n        else:\n            filename = fileProcessingFinished(\n                added_file,\n                fileManager.path_on_disk(FileDestinations.LOCAL, added_file),\n                target,\n            )\n            done = not sd\n\n        if userdata is not None:\n            # upload included userdata, add this now to the metadata\n            fileManager.set_additional_metadata(\n                FileDestinations.LOCAL, added_file, \"userdata\", userdata\n            )\n\n        sdFilename = None\n        if isinstance(filename, tuple):\n            filename, sdFilename = filename\n\n        payload = {\n            \"name\": futureFilename,\n            \"path\": filename,\n            \"target\": target,\n            \"select\": select_request,\n            \"print\": print_request,\n            \"effective_select\": to_select,\n            \"effective_print\": to_print,\n        }\n        if userdata is not None:\n            payload[\"userdata\"] = userdata\n        eventManager.fire(Events.UPLOAD, payload)\n\n        files = {}\n        location = url_for(\n            \".readGcodeFile\",\n            target=FileDestinations.LOCAL,\n            filename=filename,\n            _external=True,\n        )\n        files.update(\n            {\n                FileDestinations.LOCAL: {\n                    \"name\": futureFilename,\n                    \"path\": filename,\n                    \"origin\": FileDestinations.LOCAL,\n                    \"refs\": {\n                        \"resource\": location,\n                        \"download\": url_for(\"index\", _external=True)\n                        + \"downloads/files/\"\n                        + FileDestinations.LOCAL\n                        + \"/\"\n                        + urlquote(filename),\n                    },\n                }\n            }\n        )\n\n        if sd and sdFilename:\n            location = url_for(\n                \".readGcodeFile\",\n                target=FileDestinations.SDCARD,\n                filename=sdFilename,\n                _external=True,\n            )\n            files.update(\n                {\n                    FileDestinations.SDCARD: {\n                        \"name\": sdFilename,\n                        \"path\": sdFilename,\n                        \"origin\": FileDestinations.SDCARD,\n                        \"refs\": {\"resource\": location},\n                    }\n                }\n            )\n\n        r = make_response(\n            jsonify(\n                files=files,\n                done=done,\n                effectiveSelect=to_select,\n                effectivePrint=to_print,\n            ),\n            201,\n        )\n        r.headers[\"Location\"] = location\n        return r\n\n    elif \"foldername\" in request.values:\n        foldername = request.values[\"foldername\"]\n\n        if target not in [FileDestinations.LOCAL]:\n            abort(400, description=\"target is invalid\")\n\n        canonPath, canonName = fileManager.canonicalize(target, foldername)\n        futurePath = fileManager.sanitize_path(target, canonPath)\n        futureName = fileManager.sanitize_name(target, canonName)\n        if not futureName or not futurePath:\n            abort(400, description=\"folder name is empty\")\n\n        if \"path\" in request.values and request.values[\"path\"]:\n            futurePath = fileManager.sanitize_path(\n                FileDestinations.LOCAL, request.values[\"path\"]\n            )\n\n        futureFullPath = fileManager.join_path(target, futurePath, futureName)\n        if octoprint.filemanager.valid_file_type(futureName):\n            abort(409, description=\"Can't create folder, please try another name\")\n\n        try:\n            added_folder = fileManager.add_folder(\n                target, futureFullPath, display=canonName\n            )\n        except octoprint.filemanager.storage.StorageError as e:\n            if e.code == octoprint.filemanager.storage.StorageError.INVALID_DIRECTORY:\n                abort(400, description=\"Could not create folder, invalid directory\")\n            else:\n                abort(500, description=\"Could not create folder\")\n\n        location = url_for(\n            \".readGcodeFile\",\n            target=FileDestinations.LOCAL,\n            filename=added_folder,\n            _external=True,\n        )\n        folder = {\n            \"name\": futureName,\n            \"path\": added_folder,\n            \"origin\": target,\n            \"refs\": {\"resource\": location},\n        }\n\n        r = make_response(jsonify(folder=folder, done=True), 201)\n        r.headers[\"Location\"] = location\n        return r\n\n    else:\n        abort(400, description=\"No file to upload and no folder to create\")\n\n\n@api.route(\"/files/<string:target>/<path:filename>\", methods=[\"POST\"])\n@no_firstrun_access\ndef gcodeFileCommand(filename, target):\n    if target not in [FileDestinations.LOCAL, FileDestinations.SDCARD]:\n        abort(404)\n\n    if not _validate(target, filename):\n        abort(404)\n\n    # valid file commands, dict mapping command name to mandatory parameters\n    valid_commands = {\n        \"select\": [],\n        \"unselect\": [],\n        \"slice\": [],\n        \"analyse\": [],\n        \"copy\": [\"destination\"],\n        \"move\": [\"destination\"],\n    }\n\n    command, data, response = get_json_command_from_request(request, valid_commands)\n    if response is not None:\n        return response\n\n    user = current_user.get_name()\n\n    if command == \"select\":\n        with Permissions.FILES_SELECT.require(403):\n            if not _verifyFileExists(target, filename):\n                abort(404)\n\n            # selects/loads a file\n            if not octoprint.filemanager.valid_file_type(filename, type=\"machinecode\"):\n                abort(\n                    415,\n                    description=\"Cannot select file for printing, not a machinecode file\",\n                )\n\n            if not printer.is_ready():\n                abort(\n                    409,\n                    description=\"Printer is already printing, cannot select a new file\",\n                )\n\n            printAfterLoading = False\n            if \"print\" in data and data[\"print\"] in valid_boolean_trues:\n                with Permissions.PRINT.require(403):\n                    if not printer.is_operational():\n                        abort(\n                            409,\n                            description=\"Printer is not operational, cannot directly start printing\",\n                        )\n                    printAfterLoading = True\n\n            sd = False\n            if target == FileDestinations.SDCARD:\n                filenameToSelect = filename\n                sd = True\n            else:\n                filenameToSelect = fileManager.path_on_disk(target, filename)\n            printer.select_file(filenameToSelect, sd, printAfterLoading, user)\n\n    elif command == \"unselect\":\n        with Permissions.FILES_SELECT.require(403):\n            if not printer.is_ready():\n                return make_response(\n                    \"Printer is already printing, cannot unselect current file\", 409\n                )\n\n            _, currentFilename = _getCurrentFile()\n            if currentFilename is None:\n                return make_response(\n                    \"Cannot unselect current file when there is no file selected\", 409\n                )\n\n            if filename != currentFilename and filename != \"current\":\n                return make_response(\n                    \"Only the currently selected file can be unselected\", 400\n                )\n\n            printer.unselect_file()\n\n    elif command == \"slice\":\n        with Permissions.SLICE.require(403):\n            if not _verifyFileExists(target, filename):\n                abort(404)\n\n            try:\n                if \"slicer\" in data:\n                    slicer = data[\"slicer\"]\n                    del data[\"slicer\"]\n                    slicer_instance = slicingManager.get_slicer(slicer)\n\n                elif \"cura\" in slicingManager.registered_slicers:\n                    slicer = \"cura\"\n                    slicer_instance = slicingManager.get_slicer(\"cura\")\n\n                else:\n                    abort(415, description=\"Cannot slice file, no slicer available\")\n            except octoprint.slicing.UnknownSlicer:\n                abort(404)\n\n            if not any(\n                [\n                    octoprint.filemanager.valid_file_type(filename, type=source_file_type)\n                    for source_file_type in slicer_instance.get_slicer_properties().get(\n                        \"source_file_types\", [\"model\"]\n                    )\n                ]\n            ):\n                abort(415, description=\"Cannot slice file, not a model file\")\n\n            cores = psutil.cpu_count()\n            if (\n                slicer_instance.get_slicer_properties().get(\"same_device\", True)\n                and (printer.is_printing() or printer.is_paused())\n                and (cores is None or cores < 2)\n            ):\n                # slicer runs on same device as OctoPrint, slicing while printing is hence disabled\n                abort(\n                    409,\n                    description=\"Cannot slice on this slicer while printing on single core systems or systems of unknown core count due to performance reasons\",\n                )\n\n            if \"destination\" in data and data[\"destination\"]:\n                destination = data[\"destination\"]\n                del data[\"destination\"]\n            elif \"gcode\" in data and data[\"gcode\"]:\n                destination = data[\"gcode\"]\n                del data[\"gcode\"]\n            else:\n                import os\n\n                name, _ = os.path.splitext(filename)\n                destination = (\n                    name\n                    + \".\"\n                    + slicer_instance.get_slicer_properties().get(\n                        \"destination_extensions\", [\"gco\", \"gcode\", \"g\"]\n                    )[0]\n                )\n\n            full_path = destination\n            if \"path\" in data and data[\"path\"]:\n                full_path = fileManager.join_path(target, data[\"path\"], destination)\n            else:\n                path, _ = fileManager.split_path(target, filename)\n                if path:\n                    full_path = fileManager.join_path(target, path, destination)\n\n            canon_path, canon_name = fileManager.canonicalize(target, full_path)\n            sanitized_name = fileManager.sanitize_name(target, canon_name)\n\n            if canon_path:\n                full_path = fileManager.join_path(target, canon_path, sanitized_name)\n            else:\n                full_path = sanitized_name\n\n            # prohibit overwriting the file that is currently being printed\n            currentOrigin, currentFilename = _getCurrentFile()\n            if (\n                currentFilename == full_path\n                and currentOrigin == target\n                and (printer.is_printing() or printer.is_paused())\n            ):\n                abort(\n                    409,\n                    description=\"Trying to slice into file that is currently being printed\",\n                )\n\n            if \"profile\" in data and data[\"profile\"]:\n                profile = data[\"profile\"]\n                del data[\"profile\"]\n            else:\n                profile = None\n\n            if \"printerProfile\" in data and data[\"printerProfile\"]:\n                printerProfile = data[\"printerProfile\"]\n                del data[\"printerProfile\"]\n            else:\n                printerProfile = None\n\n            if (\n                \"position\" in data\n                and data[\"position\"]\n                and isinstance(data[\"position\"], dict)\n                and \"x\" in data[\"position\"]\n                and \"y\" in data[\"position\"]\n            ):\n                position = data[\"position\"]\n                del data[\"position\"]\n            else:\n                position = None\n\n            select_after_slicing = False\n            if \"select\" in data and data[\"select\"] in valid_boolean_trues:\n                if not printer.is_operational():\n                    abort(\n                        409,\n                        description=\"Printer is not operational, cannot directly select for printing\",\n                    )\n                select_after_slicing = True\n\n            print_after_slicing = False\n            if \"print\" in data and data[\"print\"] in valid_boolean_trues:\n                if not printer.is_operational():\n                    abort(\n                        409,\n                        description=\"Printer is not operational, cannot directly start printing\",\n                    )\n                select_after_slicing = print_after_slicing = True\n\n            override_keys = [\n                k for k in data if k.startswith(\"profile.\") and data[k] is not None\n            ]\n            overrides = {}\n            for key in override_keys:\n                overrides[key[len(\"profile.\") :]] = data[key]\n\n            def slicing_done(target, path, select_after_slicing, print_after_slicing):\n                if select_after_slicing or print_after_slicing:\n                    sd = False\n                    if target == FileDestinations.SDCARD:\n                        filenameToSelect = path\n                        sd = True\n                    else:\n                        filenameToSelect = fileManager.path_on_disk(target, path)\n                    printer.select_file(filenameToSelect, sd, print_after_slicing, user)\n\n            try:\n                fileManager.slice(\n                    slicer,\n                    target,\n                    filename,\n                    target,\n                    full_path,\n                    profile=profile,\n                    printer_profile_id=printerProfile,\n                    position=position,\n                    overrides=overrides,\n                    display=canon_name,\n                    callback=slicing_done,\n                    callback_args=(\n                        target,\n                        full_path,\n                        select_after_slicing,\n                        print_after_slicing,\n                    ),\n                )\n            except octoprint.slicing.UnknownProfile:\n                abort(404, description=\"Unknown profile\")\n\n            location = url_for(\n                \".readGcodeFile\",\n                target=target,\n                filename=full_path,\n                _external=True,\n            )\n            result = {\n                \"name\": destination,\n                \"path\": full_path,\n                \"display\": canon_name,\n                \"origin\": FileDestinations.LOCAL,\n                \"refs\": {\n                    \"resource\": location,\n                    \"download\": url_for(\"index\", _external=True)\n                    + \"downloads/files/\"\n                    + target\n                    + \"/\"\n                    + urlquote(full_path),\n                },\n            }\n\n            r = make_response(jsonify(result), 202)\n            r.headers[\"Location\"] = location\n            return r\n\n    elif command == \"analyse\":\n        with Permissions.FILES_UPLOAD.require(403):\n            if not _verifyFileExists(target, filename):\n                abort(404)\n\n            printer_profile = None\n            if \"printerProfile\" in data and data[\"printerProfile\"]:\n                printer_profile = data[\"printerProfile\"]\n\n            if not fileManager.analyse(\n                target, filename, printer_profile_id=printer_profile\n            ):\n                abort(400, description=\"No analysis possible\")\n\n    elif command == \"copy\" or command == \"move\":\n        with Permissions.FILES_UPLOAD.require(403):\n            # Copy and move are only possible on local storage\n            if target not in [FileDestinations.LOCAL]:\n                abort(400, description=f\"Unsupported target for {command}\")\n\n            if not _verifyFileExists(target, filename) and not _verifyFolderExists(\n                target, filename\n            ):\n                abort(404)\n\n            path, name = fileManager.split_path(target, filename)\n\n            destination = data[\"destination\"]\n            dst_path, dst_name = fileManager.split_path(target, destination)\n            sanitized_destination = fileManager.join_path(\n                target, dst_path, fileManager.sanitize_name(target, dst_name)\n            )\n\n            # Check for exception thrown by _verifyFolderExists, if outside the root directory\n            try:\n                if (\n                    _verifyFolderExists(target, destination)\n                    and sanitized_destination != filename\n                ):\n                    # destination is an existing folder and not ourselves (= display rename), we'll assume we are supposed\n                    # to move filename to this folder under the same name\n                    destination = fileManager.join_path(target, destination, name)\n\n                if _verifyFileExists(target, destination) or _verifyFolderExists(\n                    target, destination\n                ):\n                    abort(409, description=\"File or folder does already exist\")\n\n            except Exception:\n                abort(\n                    409, description=\"Exception thrown by storage, bad folder/file name?\"\n                )\n\n            is_file = fileManager.file_exists(target, filename)\n            is_folder = fileManager.folder_exists(target, filename)\n\n            if not (is_file or is_folder):\n                abort(400, description=f\"Neither file nor folder, can't {command}\")\n\n            if command == \"copy\":\n                # destination already there? error...\n                if _verifyFileExists(target, destination) or _verifyFolderExists(\n                    target, destination\n                ):\n                    abort(409, description=\"File or folder does already exist\")\n\n                if is_file:\n                    fileManager.copy_file(target, filename, destination)\n                else:\n                    fileManager.copy_folder(target, filename, destination)\n\n            elif command == \"move\":\n                with Permissions.FILES_DELETE.require(403):\n                    if _isBusy(target, filename):\n                        abort(\n                            409,\n                            description=\"Trying to move a file or folder that is currently in use\",\n                        )\n\n                    # destination already there AND not ourselves (= display rename)? error...\n                    if (\n                        _verifyFileExists(target, destination)\n                        or _verifyFolderExists(target, destination)\n                    ) and sanitized_destination != filename:\n                        abort(409, description=\"File or folder does already exist\")\n\n                    # deselect the file if it's currently selected\n                    currentOrigin, currentFilename = _getCurrentFile()\n                    if currentFilename is not None and filename == currentFilename:\n                        printer.unselect_file()\n\n                    if is_file:\n                        fileManager.move_file(target, filename, destination)\n                    else:\n                        fileManager.move_folder(target, filename, destination)\n\n            location = url_for(\n                \".readGcodeFile\",\n                target=target,\n                filename=destination,\n                _external=True,\n            )\n            result = {\n                \"name\": name,\n                \"path\": destination,\n                \"origin\": FileDestinations.LOCAL,\n                \"refs\": {\"resource\": location},\n            }\n            if is_file:\n                result[\"refs\"][\"download\"] = (\n                    url_for(\"index\", _external=True)\n                    + \"downloads/files/\"\n                    + target\n                    + \"/\"\n                    + urlquote(destination)\n                )\n\n            r = make_response(jsonify(result), 201)\n            r.headers[\"Location\"] = location\n            return r\n\n    return NO_CONTENT\n\n\n@api.route(\"/files/<string:target>/<path:filename>\", methods=[\"DELETE\"])\n@no_firstrun_access\n@Permissions.FILES_DELETE.require(403)\ndef deleteGcodeFile(filename, target):\n    if not _validate(target, filename):\n        abort(404)\n\n    if not _verifyFileExists(target, filename) and not _verifyFolderExists(\n        target, filename\n    ):\n        abort(404)\n\n    if target not in [FileDestinations.LOCAL, FileDestinations.SDCARD]:\n        abort(404)\n\n    if _verifyFileExists(target, filename):\n        if _isBusy(target, filename):\n            abort(409, description=\"Trying to delete a file that is currently in use\")\n\n        # deselect the file if it's currently selected\n        currentOrigin, currentPath = _getCurrentFile()\n        if (\n            currentPath is not None\n            and currentOrigin == target\n            and filename == currentPath\n        ):\n            printer.unselect_file()\n\n        # delete it\n        if target == FileDestinations.SDCARD:\n            printer.delete_sd_file(filename, tags={\"source:api\", \"api:files.sd\"})\n        else:\n            fileManager.remove_file(target, filename)\n\n    elif _verifyFolderExists(target, filename):\n        if _isBusy(target, filename):\n            abort(\n                409,\n                description=\"Trying to delete a folder that contains a file that is currently in use\",\n            )\n\n        # deselect the file if it's currently selected\n        currentOrigin, currentPath = _getCurrentFile()\n        if (\n            currentPath is not None\n            and currentOrigin == target\n            and fileManager.file_in_path(target, filename, currentPath)\n        ):\n            printer.unselect_file()\n\n        # delete it\n        fileManager.remove_folder(target, filename, recursive=True)\n\n    return NO_CONTENT\n\n\ndef _getCurrentFile():\n    currentJob = printer.get_current_job()\n    if (\n        currentJob is not None\n        and \"file\" in currentJob\n        and \"path\" in currentJob[\"file\"]\n        and \"origin\" in currentJob[\"file\"]\n    ):\n        return currentJob[\"file\"][\"origin\"], currentJob[\"file\"][\"path\"]\n    else:\n        return None, None\n\n\ndef _validate(target, filename):\n    if target == FileDestinations.SDCARD:\n        # we make no assumptions about the shape of valid SDCard file names\n        return True\n    else:\n        return filename == \"/\".join(\n            map(lambda x: fileManager.sanitize_name(target, x), filename.split(\"/\"))\n        )\n\n\nclass WerkzeugFileWrapper(octoprint.filemanager.util.AbstractFileWrapper):\n    \"\"\"\n    A wrapper around a Werkzeug ``FileStorage`` object.\n\n    Arguments:\n        file_obj (werkzeug.datastructures.FileStorage): The Werkzeug ``FileStorage`` instance to wrap.\n\n    .. seealso::\n\n       `werkzeug.datastructures.FileStorage <http://werkzeug.pocoo.org/docs/0.10/datastructures/#werkzeug.datastructures.FileStorage>`_\n            The documentation of Werkzeug's ``FileStorage`` class.\n    \"\"\"\n\n    def __init__(self, file_obj):\n        octoprint.filemanager.util.AbstractFileWrapper.__init__(self, file_obj.filename)\n        self.file_obj = file_obj\n\n    def save(self, path):\n        \"\"\"\n        Delegates to ``werkzeug.datastructures.FileStorage.save``\n        \"\"\"\n        self.file_obj.save(path)\n\n    def stream(self):\n        \"\"\"\n        Returns ``werkzeug.datastructures.FileStorage.stream``\n        \"\"\"\n        return self.file_obj.stream\n"], "fixing_code": ["__author__ = \"Gina H\u00e4u\u00dfge <osd@foosel.net>\"\n__license__ = \"GNU Affero General Public License http://www.gnu.org/licenses/agpl.html\"\n__copyright__ = \"Copyright (C) 2014 The OctoPrint Project - Released under terms of the AGPLv3 License\"\n\nimport copy\nimport logging\nimport os\nimport shutil\nfrom contextlib import contextmanager\nfrom os import scandir, walk\n\nimport pylru\n\nimport octoprint.filemanager\nfrom octoprint.util import (\n    atomic_write,\n    is_hidden_path,\n    time_this,\n    to_bytes,\n    to_unicode,\n    yaml,\n)\nfrom octoprint.util.files import sanitize_filename\n\n\nclass StorageInterface:\n    \"\"\"\n    Interface of storage adapters for OctoPrint.\n    \"\"\"\n\n    # noinspection PyUnreachableCode\n    @property\n    def analysis_backlog(self):\n        \"\"\"\n        Get an iterator over all items stored in the storage that need to be analysed by the :class:`~octoprint.filemanager.AnalysisQueue`.\n\n        The yielded elements are expected as storage specific absolute paths to the respective files. Don't forget\n        to recurse into folders if your storage adapter supports those.\n\n        :return: an iterator yielding all un-analysed files in the storage\n        \"\"\"\n        # empty generator pattern, yield is intentionally unreachable\n        return\n        yield\n\n    # noinspection PyUnreachableCode\n    def analysis_backlog_for_path(self, path=None):\n        # empty generator pattern, yield is intentionally unreachable\n        return\n        yield\n\n    def last_modified(self, path=None, recursive=False):\n        \"\"\"\n        Get the last modification date of the specified ``path`` or ``path``'s subtree.\n\n        Args:\n            path (str or None): Path for which to determine the subtree's last modification date. If left out or\n                set to None, defatuls to storage root.\n            recursive (bool): Whether to determine only the date of the specified ``path`` (False, default) or\n                the whole ``path``'s subtree (True).\n\n        Returns: (float) The last modification date of the indicated subtree\n        \"\"\"\n        raise NotImplementedError()\n\n    def file_in_path(self, path, filepath):\n        \"\"\"\n        Returns whether the file indicated by ``file`` is inside ``path`` or not.\n        :param string path: the path to check\n        :param string filepath: path to the file\n        :return: ``True`` if the file is inside the path, ``False`` otherwise\n        \"\"\"\n        return NotImplementedError()\n\n    def file_exists(self, path):\n        \"\"\"\n        Returns whether the file indicated by ``path`` exists or not.\n        :param string path: the path to check for existence\n        :return: ``True`` if the file exists, ``False`` otherwise\n        \"\"\"\n        raise NotImplementedError()\n\n    def folder_exists(self, path):\n        \"\"\"\n        Returns whether the folder indicated by ``path`` exists or not.\n        :param string path: the path to check for existence\n        :return: ``True`` if the folder exists, ``False`` otherwise\n        \"\"\"\n        raise NotImplementedError()\n\n    def list_files(\n        self, path=None, filter=None, recursive=True, level=0, force_refresh=False\n    ):\n        \"\"\"\n        List all files in storage starting at ``path``. If ``recursive`` is set to True (the default), also dives into\n        subfolders.\n\n        An optional filter function can be supplied which will be called with a file name and file data and which has\n        to return True if the file is to be included in the result or False if not.\n\n        The data structure of the returned result will be a dictionary mapping from file names to entry data. File nodes\n        will contain their metadata here, folder nodes will contain their contained files and folders. Example::\n\n           {\n             \"some_folder\": {\n               \"name\": \"some_folder\",\n               \"path\": \"some_folder\",\n               \"type\": \"folder\",\n               \"children\": {\n                 \"some_sub_folder\": {\n                   \"name\": \"some_sub_folder\",\n                   \"path\": \"some_folder/some_sub_folder\",\n                   \"type\": \"folder\",\n                   \"typePath\": [\"folder\"],\n                   \"children\": { ... }\n                 },\n                 \"some_file.gcode\": {\n                   \"name\": \"some_file.gcode\",\n                   \"path\": \"some_folder/some_file.gcode\",\n                   \"type\": \"machinecode\",\n                   \"typePath\": [\"machinecode\", \"gcode\"],\n                   \"hash\": \"<sha1 hash>\",\n                   \"links\": [ ... ],\n                   ...\n                 },\n                 ...\n               }\n             \"test.gcode\": {\n               \"name\": \"test.gcode\",\n               \"path\": \"test.gcode\",\n               \"type\": \"machinecode\",\n               \"typePath\": [\"machinecode\", \"gcode\"],\n               \"hash\": \"<sha1 hash>\",\n               \"links\": [...],\n               ...\n             },\n             \"test.stl\": {\n               \"name\": \"test.stl\",\n               \"path\": \"test.stl\",\n               \"type\": \"model\",\n               \"typePath\": [\"model\", \"stl\"],\n               \"hash\": \"<sha1 hash>\",\n               \"links\": [...],\n               ...\n             },\n             ...\n           }\n\n        :param string path:     base path from which to recursively list all files, optional, if not supplied listing will start\n                                from root of base folder\n        :param function filter: a filter that matches the files that are to be returned, may be left out in which case no\n                                filtering will take place\n        :param bool recursive:  will also step into sub folders for building the complete list if set to True, otherwise will only\n                                do one step down into sub folders to be able to populate the ``children``.\n        :return: a dictionary mapping entry names to entry data that represents the whole file list\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_folder(self, path, ignore_existing=True, display=None):\n        \"\"\"\n        Adds a folder as ``path``\n\n        The ``path`` will be sanitized.\n\n        :param string path:          the path of the new folder\n        :param bool ignore_existing: if set to True, no error will be raised if the folder to be added already exists\n        :param unicode display:      display name of the folder\n        :return: the sanitized name of the new folder to be used for future references to the folder\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_folder(self, path, recursive=True):\n        \"\"\"\n        Removes the folder at ``path``\n\n        :param string path:    the path of the folder to remove\n        :param bool recursive: if set to True, contained folders and files will also be removed, otherwise an error will\n                               be raised if the folder is not empty (apart from any metadata files) when it's to be removed\n        \"\"\"\n        raise NotImplementedError()\n\n    def copy_folder(self, source, destination):\n        \"\"\"\n        Copies the folder ``source`` to ``destination``\n\n        :param string source: path to the source folder\n        :param string destination: path to destination\n\n        :return: the path in the storage to the copy of the folder\n        \"\"\"\n        raise NotImplementedError()\n\n    def move_folder(self, source, destination):\n        \"\"\"\n        Moves the folder ``source`` to ``destination``\n\n        :param string source: path to the source folder\n        :param string destination: path to destination\n\n        :return: the new path in the storage to the folder\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_file(\n        self,\n        path,\n        file_object,\n        printer_profile=None,\n        links=None,\n        allow_overwrite=False,\n        display=None,\n    ):\n        \"\"\"\n        Adds the file ``file_object`` as ``path``\n\n        :param string path:            the file's new path, will be sanitized\n        :param object file_object:     a file object that provides a ``save`` method which will be called with the destination path\n                                       where the object should then store its contents\n        :param object printer_profile: the printer profile associated with this file (if any)\n        :param list links:             any links to add with the file\n        :param bool allow_overwrite:   if set to True no error will be raised if the file already exists and the existing file\n                                       and its metadata will just be silently overwritten\n        :param unicode display:        display name of the file\n        :return: the sanitized name of the file to be used for future references to it\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_file(self, path):\n        \"\"\"\n        Removes the file at ``path``\n\n        Will also take care of deleting the corresponding entries\n        in the metadata and deleting all links pointing to the file.\n\n        :param string path: path of the file to remove\n        \"\"\"\n        raise NotImplementedError()\n\n    def copy_file(self, source, destination):\n        \"\"\"\n        Copies the file ``source`` to ``destination``\n\n        :param string source: path to the source file\n        :param string destination: path to destination\n\n        :return: the path in the storage to the copy of the file\n        \"\"\"\n        raise NotImplementedError()\n\n    def move_file(self, source, destination):\n        \"\"\"\n        Moves the file ``source`` to ``destination``\n\n        :param string source: path to the source file\n        :param string destination: path to destination\n\n        :return: the new path in the storage to the file\n        \"\"\"\n        raise NotImplementedError()\n\n    def has_analysis(self, path):\n        \"\"\"\n        Returns whether the file at path has been analysed yet\n\n        :param path: virtual path to the file for which to retrieve the metadata\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_metadata(self, path):\n        \"\"\"\n        Retrieves the metadata for the file ``path``.\n\n        :param path: virtual path to the file for which to retrieve the metadata\n        :return: the metadata associated with the file\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_link(self, path, rel, data):\n        \"\"\"\n        Adds a link of relation ``rel`` to file ``path`` with the given ``data``.\n\n        The following relation types are currently supported:\n\n          * ``model``: adds a link to a model from which the file was created/sliced, expected additional data is the ``name``\n            and optionally the ``hash`` of the file to link to. If the link can be resolved against another file on the\n            current ``path``, not only will it be added to the links of ``name`` but a reverse link of type ``machinecode``\n            referring to ``name`` and its hash will also be added to the linked ``model`` file\n          * ``machinecode``: adds a link to a file containing machine code created from the current file (model), expected\n            additional data is the ``name`` and optionally the ``hash`` of the file to link to. If the link can be resolved\n            against another file on the current ``path``, not only will it be added to the links of ``name`` but a reverse\n            link of type ``model`` referring to ``name`` and its hash will also be added to the linked ``model`` file.\n          * ``web``: adds a location on the web associated with this file (e.g. a website where to download a model),\n            expected additional data is a ``href`` attribute holding the website's URL and optionally a ``retrieved``\n            attribute describing when the content was retrieved\n\n        Note that adding ``model`` links to files identifying as models or ``machinecode`` links to files identifying\n        as machine code will be refused.\n\n        :param path: path of the file for which to add a link\n        :param rel: type of relation of the link to add (currently ``model``, ``machinecode`` and ``web`` are supported)\n        :param data: additional data of the link to add\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_link(self, path, rel, data):\n        \"\"\"\n        Removes the link consisting of ``rel`` and ``data`` from file ``name`` on ``path``.\n\n        :param path: path of the file from which to remove the link\n        :param rel: type of relation of the link to remove (currently ``model``, ``machinecode`` and ``web`` are supported)\n        :param data: additional data of the link to remove, must match existing link\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_additional_metadata(self, path, key):\n        \"\"\"\n        Fetches additional metadata at ``key`` from the metadata of ``path``.\n\n        :param path: the virtual path to the file for which to fetch additional metadata\n        :param key: key of metadata to fetch\n        \"\"\"\n        raise NotImplementedError()\n\n    def set_additional_metadata(self, path, key, data, overwrite=False, merge=False):\n        \"\"\"\n        Adds additional metadata to the metadata of ``path``. Metadata in ``data`` will be saved under ``key``.\n\n        If ``overwrite`` is set and ``key`` already exists in ``name``'s metadata, the current value will be overwritten.\n\n        If ``merge`` is set and ``key`` already exists and both ``data`` and the existing data under ``key`` are dictionaries,\n        the two dictionaries will be merged recursively.\n\n        :param path: the virtual path to the file for which to add additional metadata\n        :param key: key of metadata to add\n        :param data: metadata to add\n        :param overwrite: if True and ``key`` already exists, it will be overwritten\n        :param merge: if True and ``key`` already exists and both ``data`` and the existing data are dictionaries, they\n                      will be merged\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_additional_metadata(self, path, key):\n        \"\"\"\n        Removes additional metadata under ``key`` for ``name`` on ``path``\n\n        :param path: the virtual path to the file for which to remove the metadata under ``key``\n        :param key: the key to remove\n        \"\"\"\n        raise NotImplementedError()\n\n    def canonicalize(self, path):\n        \"\"\"\n        Canonicalizes the given ``path``. The ``path`` may consist of both folder and file name, the underlying\n        implementation must separate those if necessary.\n\n        By default, this calls :func:`~octoprint.filemanager.StorageInterface.sanitize`, which also takes care\n        of stripping any invalid characters.\n\n        Args:\n                path: the path to canonicalize\n\n        Returns:\n                a 2-tuple containing the canonicalized path and file name\n\n        \"\"\"\n        return self.sanitize(path)\n\n    def sanitize(self, path):\n        \"\"\"\n        Sanitizes the given ``path``, stripping it of all invalid characters. The ``path`` may consist of both\n        folder and file name, the underlying implementation must separate those if necessary and sanitize individually.\n\n        :param string path: the path to sanitize\n        :return: a 2-tuple containing the sanitized path and file name\n        \"\"\"\n        raise NotImplementedError()\n\n    def sanitize_path(self, path):\n        \"\"\"\n        Sanitizes the given folder-only ``path``, stripping it of all invalid characters.\n        :param string path: the path to sanitize\n        :return: the sanitized path\n        \"\"\"\n        raise NotImplementedError()\n\n    def sanitize_name(self, name):\n        \"\"\"\n        Sanitizes the given file ``name``, stripping it of all invalid characters.\n        :param string name: the file name to sanitize\n        :return: the sanitized name\n        \"\"\"\n        raise NotImplementedError()\n\n    def split_path(self, path):\n        \"\"\"\n        Split ``path`` into base directory and file name.\n        :param path: the path to split\n        :return: a tuple (base directory, file name)\n        \"\"\"\n        raise NotImplementedError()\n\n    def join_path(self, *path):\n        \"\"\"\n        Join path elements together\n        :param path: path elements to join\n        :return: joined representation of the path to be usable as fully qualified path for further operations\n        \"\"\"\n        raise NotImplementedError()\n\n    def path_on_disk(self, path):\n        \"\"\"\n        Retrieves the path on disk for ``path``.\n\n        Note: if the storage is not on disk and there exists no path on disk to refer to it, this method should\n        raise an :class:`io.UnsupportedOperation`\n\n        Opposite of :func:`path_in_storage`.\n\n        :param string path: the virtual path for which to retrieve the path on disk\n        :return: the path on disk to ``path``\n        \"\"\"\n        raise NotImplementedError()\n\n    def path_in_storage(self, path):\n        \"\"\"\n        Retrieves the equivalent in the storage adapter for ``path``.\n\n        Opposite of :func:`path_on_disk`.\n\n        :param string path: the path for which to retrieve the storage path\n        :return: the path in storage to ``path``\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass StorageError(Exception):\n    UNKNOWN = \"unknown\"\n    INVALID_DIRECTORY = \"invalid_directory\"\n    INVALID_FILE = \"invalid_file\"\n    INVALID_SOURCE = \"invalid_source\"\n    INVALID_DESTINATION = \"invalid_destination\"\n    DOES_NOT_EXIST = \"does_not_exist\"\n    ALREADY_EXISTS = \"already_exists\"\n    SOURCE_EQUALS_DESTINATION = \"source_equals_destination\"\n    NOT_EMPTY = \"not_empty\"\n\n    def __init__(self, message, code=None, cause=None):\n        BaseException.__init__(self)\n        self.message = message\n        self.cause = cause\n\n        if code is None:\n            code = StorageError.UNKNOWN\n        self.code = code\n\n\nclass LocalFileStorage(StorageInterface):\n    \"\"\"\n    The ``LocalFileStorage`` is a storage implementation which holds all files, folders and metadata on disk.\n\n    Metadata is managed inside ``.metadata.json`` files in the respective folders, indexed by the sanitized filenames\n    stored within the folder. Metadata access is managed through an LRU cache to minimize access overhead.\n\n    This storage type implements :func:`path_on_disk`.\n    \"\"\"\n\n    def __init__(self, basefolder, create=False, really_universal=False):\n        \"\"\"\n        Initializes a ``LocalFileStorage`` instance under the given ``basefolder``, creating the necessary folder\n        if necessary and ``create`` is set to ``True``.\n\n        :param string basefolder:     the path to the folder under which to create the storage\n        :param bool create:           ``True`` if the folder should be created if it doesn't exist yet, ``False`` otherwise\n        :param bool really_universal: ``True`` if the file names should be forced to really universal, ``False`` otherwise\n        \"\"\"\n        self._logger = logging.getLogger(__name__)\n\n        self.basefolder = os.path.realpath(os.path.abspath(to_unicode(basefolder)))\n        if not os.path.exists(self.basefolder) and create:\n            os.makedirs(self.basefolder)\n        if not os.path.exists(self.basefolder) or not os.path.isdir(self.basefolder):\n            raise StorageError(\n                f\"{basefolder} is not a valid directory\",\n                code=StorageError.INVALID_DIRECTORY,\n            )\n\n        self._really_universal = really_universal\n\n        import threading\n\n        self._metadata_lock_mutex = threading.RLock()\n        self._metadata_locks = {}\n        self._persisted_metadata_lock_mutex = threading.RLock()\n        self._persisted_metadata_locks = {}\n\n        self._metadata_cache = pylru.lrucache(100)\n        self._filelist_cache = {}\n        self._filelist_cache_mutex = threading.RLock()\n\n        self._old_metadata = None\n        self._initialize_metadata()\n\n    def _initialize_metadata(self):\n        self._logger.info(f\"Initializing the file metadata for {self.basefolder}...\")\n\n        old_metadata_path = os.path.join(self.basefolder, \"metadata.yaml\")\n        backup_path = os.path.join(self.basefolder, \"metadata.yaml.backup\")\n\n        if os.path.exists(old_metadata_path):\n            # load the old metadata file\n            try:\n                self._old_metadata = yaml.load_from_file(path=old_metadata_path)\n            except Exception:\n                self._logger.exception(\"Error while loading old metadata file\")\n\n            # make sure the metadata is initialized as far as possible\n            self._list_folder(self.basefolder)\n\n            # rename the old metadata file\n            self._old_metadata = None\n            try:\n                import shutil\n\n                shutil.move(old_metadata_path, backup_path)\n            except Exception:\n                self._logger.exception(\"Could not rename old metadata.yaml file\")\n\n        else:\n            # make sure the metadata is initialized as far as possible\n            self._list_folder(self.basefolder)\n\n        self._logger.info(\n            f\"... file metadata for {self.basefolder} initialized successfully.\"\n        )\n\n    @property\n    def analysis_backlog(self):\n        return self.analysis_backlog_for_path()\n\n    def analysis_backlog_for_path(self, path=None):\n        if path:\n            path = self.sanitize_path(path)\n\n        yield from self._analysis_backlog_generator(path)\n\n    def _analysis_backlog_generator(self, path=None):\n        if path is None:\n            path = self.basefolder\n\n        metadata = self._get_metadata(path)\n        if not metadata:\n            metadata = {}\n        for entry in scandir(path):\n            if is_hidden_path(entry.name):\n                continue\n\n            if entry.is_file() and octoprint.filemanager.valid_file_type(entry.name):\n                if (\n                    entry.name not in metadata\n                    or not isinstance(metadata[entry.name], dict)\n                    or \"analysis\" not in metadata[entry.name]\n                ):\n                    printer_profile_rels = self.get_link(entry.path, \"printerprofile\")\n                    if printer_profile_rels:\n                        printer_profile_id = printer_profile_rels[0][\"id\"]\n                    else:\n                        printer_profile_id = None\n\n                    yield entry.name, entry.path, printer_profile_id\n            elif os.path.isdir(entry.path):\n                for sub_entry in self._analysis_backlog_generator(entry.path):\n                    yield self.join_path(entry.name, sub_entry[0]), sub_entry[\n                        1\n                    ], sub_entry[2]\n\n    def last_modified(self, path=None, recursive=False):\n        if path is None:\n            path = self.basefolder\n        else:\n            path = os.path.join(self.basefolder, path)\n\n        def last_modified_for_path(p):\n            metadata = os.path.join(p, \".metadata.json\")\n            if os.path.exists(metadata):\n                return max(os.stat(p).st_mtime, os.stat(metadata).st_mtime)\n            else:\n                return os.stat(p).st_mtime\n\n        if recursive:\n            return max(last_modified_for_path(root) for root, _, _ in walk(path))\n        else:\n            return last_modified_for_path(path)\n\n    def file_in_path(self, path, filepath):\n        filepath = self.sanitize_path(filepath)\n        path = self.sanitize_path(path)\n\n        return filepath == path or filepath.startswith(path + os.sep)\n\n    def file_exists(self, path):\n        path, name = self.sanitize(path)\n        file_path = os.path.join(path, name)\n        return os.path.exists(file_path) and os.path.isfile(file_path)\n\n    def folder_exists(self, path):\n        path, name = self.sanitize(path)\n        folder_path = os.path.join(path, name)\n        return os.path.exists(folder_path) and os.path.isdir(folder_path)\n\n    def list_files(\n        self, path=None, filter=None, recursive=True, level=0, force_refresh=False\n    ):\n        if path:\n            path = self.sanitize_path(to_unicode(path))\n            base = self.path_in_storage(path)\n            if base:\n                base += \"/\"\n        else:\n            path = self.basefolder\n            base = \"\"\n\n        def strip_children(nodes):\n            result = {}\n            for key, node in nodes.items():\n                if node[\"type\"] == \"folder\":\n                    node = copy.copy(node)\n                    node[\"children\"] = {}\n                result[key] = node\n            return result\n\n        def strip_grandchildren(nodes):\n            result = {}\n            for key, node in nodes.items():\n                if node[\"type\"] == \"folder\":\n                    node = copy.copy(node)\n                    node[\"children\"] = strip_children(node[\"children\"])\n                result[key] = node\n            return result\n\n        def apply_filter(nodes, filter_func):\n            result = {}\n            for key, node in nodes.items():\n                if filter_func(node) or node[\"type\"] == \"folder\":\n                    if node[\"type\"] == \"folder\":\n                        node = copy.copy(node)\n                        node[\"children\"] = apply_filter(\n                            node.get(\"children\", {}), filter_func\n                        )\n                    result[key] = node\n            return result\n\n        result = self._list_folder(path, base=base, force_refresh=force_refresh)\n        if not recursive:\n            if level > 0:\n                result = strip_grandchildren(result)\n            else:\n                result = strip_children(result)\n        if callable(filter):\n            result = apply_filter(result, filter)\n        return result\n\n    def add_folder(self, path, ignore_existing=True, display=None):\n        display_path, display_name = self.canonicalize(path)\n        path = self.sanitize_path(display_path)\n        name = self.sanitize_name(display_name)\n\n        if display is not None:\n            display_name = display\n\n        folder_path = os.path.join(path, name)\n        if os.path.exists(folder_path):\n            if not ignore_existing:\n                raise StorageError(\n                    f\"{name} does already exist in {path}\",\n                    code=StorageError.ALREADY_EXISTS,\n                )\n        else:\n            os.mkdir(folder_path)\n\n        if display_name != name:\n            metadata = self._get_metadata_entry(path, name, default={})\n            metadata[\"display\"] = display_name\n            self._update_metadata_entry(path, name, metadata)\n\n        return self.path_in_storage((path, name))\n\n    def remove_folder(self, path, recursive=True):\n        path, name = self.sanitize(path)\n\n        folder_path = os.path.join(path, name)\n        if not os.path.exists(folder_path):\n            return\n\n        empty = True\n        for entry in scandir(folder_path):\n            if entry.name == \".metadata.json\" or entry.name == \".metadata.yaml\":\n                continue\n            empty = False\n            break\n\n        if not empty and not recursive:\n            raise StorageError(\n                f\"{name} in {path} is not empty\",\n                code=StorageError.NOT_EMPTY,\n            )\n\n        import shutil\n\n        shutil.rmtree(folder_path)\n\n        self._remove_metadata_entry(path, name)\n\n    def _get_source_destination_data(self, source, destination, must_not_equal=False):\n        \"\"\"Prepares data dicts about source and destination for copy/move.\"\"\"\n        source_path, source_name = self.sanitize(source)\n\n        destination_canon_path, destination_canon_name = self.canonicalize(destination)\n        destination_path = self.sanitize_path(destination_canon_path)\n        destination_name = self.sanitize_name(destination_canon_name)\n\n        source_fullpath = os.path.join(source_path, source_name)\n        destination_fullpath = os.path.join(destination_path, destination_name)\n\n        if not os.path.exists(source_fullpath):\n            raise StorageError(\n                f\"{source_name} in {source_path} does not exist\",\n                code=StorageError.INVALID_SOURCE,\n            )\n\n        if not os.path.isdir(destination_path):\n            raise StorageError(\n                \"Destination path {} does not exist or is not a folder\".format(\n                    destination_path\n                ),\n                code=StorageError.INVALID_DESTINATION,\n            )\n        if (\n            os.path.exists(destination_fullpath)\n            and source_fullpath != destination_fullpath\n        ):\n            raise StorageError(\n                f\"{destination_name} does already exist in {destination_path}\",\n                code=StorageError.INVALID_DESTINATION,\n            )\n\n        source_meta = self._get_metadata_entry(source_path, source_name)\n        if source_meta:\n            source_display = source_meta.get(\"display\", source_name)\n        else:\n            source_display = source_name\n\n        if (\n            must_not_equal or source_display == destination_canon_name\n        ) and source_fullpath == destination_fullpath:\n            raise StorageError(\n                \"Source {} and destination {} are the same folder\".format(\n                    source_path, destination_path\n                ),\n                code=StorageError.SOURCE_EQUALS_DESTINATION,\n            )\n\n        source_data = {\n            \"path\": source_path,\n            \"name\": source_name,\n            \"display\": source_display,\n            \"fullpath\": source_fullpath,\n        }\n        destination_data = {\n            \"path\": destination_path,\n            \"name\": destination_name,\n            \"display\": destination_canon_name,\n            \"fullpath\": destination_fullpath,\n        }\n        return source_data, destination_data\n\n    def _set_display_metadata(self, destination_data, source_data=None):\n        if (\n            source_data\n            and destination_data[\"name\"] == source_data[\"name\"]\n            and source_data[\"name\"] != source_data[\"display\"]\n        ):\n            display = source_data[\"display\"]\n        elif destination_data[\"name\"] != destination_data[\"display\"]:\n            display = destination_data[\"display\"]\n        else:\n            display = None\n\n        destination_meta = self._get_metadata_entry(\n            destination_data[\"path\"], destination_data[\"name\"], default={}\n        )\n        if display:\n            destination_meta[\"display\"] = display\n            self._update_metadata_entry(\n                destination_data[\"path\"], destination_data[\"name\"], destination_meta\n            )\n        elif \"display\" in destination_meta:\n            del destination_meta[\"display\"]\n            self._update_metadata_entry(\n                destination_data[\"path\"], destination_data[\"name\"], destination_meta\n            )\n\n    def copy_folder(self, source, destination):\n        source_data, destination_data = self._get_source_destination_data(\n            source, destination, must_not_equal=True\n        )\n\n        try:\n            shutil.copytree(source_data[\"fullpath\"], destination_data[\"fullpath\"])\n        except Exception as e:\n            raise StorageError(\n                \"Could not copy %s in %s to %s in %s\"\n                % (\n                    source_data[\"name\"],\n                    source_data[\"path\"],\n                    destination_data[\"name\"],\n                    destination_data[\"path\"],\n                ),\n                cause=e,\n            )\n\n        self._set_display_metadata(destination_data, source_data=source_data)\n\n        return self.path_in_storage(destination_data[\"fullpath\"])\n\n    def move_folder(self, source, destination):\n        source_data, destination_data = self._get_source_destination_data(\n            source, destination\n        )\n\n        # only a display rename? Update that and bail early\n        if source_data[\"fullpath\"] == destination_data[\"fullpath\"]:\n            self._set_display_metadata(destination_data)\n            return self.path_in_storage(destination_data[\"fullpath\"])\n\n        try:\n            shutil.move(source_data[\"fullpath\"], destination_data[\"fullpath\"])\n        except Exception as e:\n            raise StorageError(\n                \"Could not move %s in %s to %s in %s\"\n                % (\n                    source_data[\"name\"],\n                    source_data[\"path\"],\n                    destination_data[\"name\"],\n                    destination_data[\"path\"],\n                ),\n                cause=e,\n            )\n\n        self._set_display_metadata(destination_data, source_data=source_data)\n        self._remove_metadata_entry(source_data[\"path\"], source_data[\"name\"])\n        self._delete_metadata(source_data[\"fullpath\"])\n\n        return self.path_in_storage(destination_data[\"fullpath\"])\n\n    def add_file(\n        self,\n        path,\n        file_object,\n        printer_profile=None,\n        links=None,\n        allow_overwrite=False,\n        display=None,\n    ):\n        display_path, display_name = self.canonicalize(path)\n        path = self.sanitize_path(display_path)\n        name = self.sanitize_name(display_name)\n\n        if display:\n            display_name = display\n\n        if not octoprint.filemanager.valid_file_type(name):\n            raise StorageError(\n                f\"{name} is an unrecognized file type\",\n                code=StorageError.INVALID_FILE,\n            )\n\n        file_path = os.path.join(path, name)\n        if os.path.exists(file_path) and not os.path.isfile(file_path):\n            raise StorageError(\n                f\"{name} does already exist in {path} and is not a file\",\n                code=StorageError.ALREADY_EXISTS,\n            )\n        if os.path.exists(file_path) and not allow_overwrite:\n            raise StorageError(\n                f\"{name} does already exist in {path} and overwriting is prohibited\",\n                code=StorageError.ALREADY_EXISTS,\n            )\n\n        # make sure folders exist\n        if not os.path.exists(path):\n            # TODO persist display names of path segments!\n            os.makedirs(path)\n\n        # save the file\n        file_object.save(file_path)\n\n        # save the file's hash to the metadata of the folder\n        file_hash = self._create_hash(file_path)\n        metadata = self._get_metadata_entry(path, name, default={})\n        metadata_dirty = False\n        if \"hash\" not in metadata or metadata[\"hash\"] != file_hash:\n            # hash changed -> throw away old metadata\n            metadata = {\"hash\": file_hash}\n            metadata_dirty = True\n\n        if \"display\" not in metadata and display_name != name:\n            # display name is not the same as file name -> store in metadata\n            metadata[\"display\"] = display_name\n            metadata_dirty = True\n\n        if metadata_dirty:\n            self._update_metadata_entry(path, name, metadata)\n\n        # process any links that were also provided for adding to the file\n        if not links:\n            links = []\n\n        if printer_profile is not None:\n            links.append(\n                (\n                    \"printerprofile\",\n                    {\"id\": printer_profile[\"id\"], \"name\": printer_profile[\"name\"]},\n                )\n            )\n\n        self._add_links(name, path, links)\n\n        # touch the file to set last access and modification time to now\n        os.utime(file_path, None)\n\n        return self.path_in_storage((path, name))\n\n    def remove_file(self, path):\n        path, name = self.sanitize(path)\n\n        file_path = os.path.join(path, name)\n        if not os.path.exists(file_path):\n            return\n        if not os.path.isfile(file_path):\n            raise StorageError(\n                f\"{name} in {path} is not a file\",\n                code=StorageError.INVALID_FILE,\n            )\n\n        try:\n            os.remove(file_path)\n        except Exception as e:\n            raise StorageError(f\"Could not delete {name} in {path}\", cause=e)\n\n        self._remove_metadata_entry(path, name)\n\n    def copy_file(self, source, destination):\n        source_data, destination_data = self._get_source_destination_data(\n            source, destination, must_not_equal=True\n        )\n\n        if not octoprint.filemanager.valid_file_type(destination_data[\"name\"]):\n            raise StorageError(\n                f\"{destination_data['name']} is an unrecognized file type\",\n                code=StorageError.INVALID_FILE,\n            )\n\n        try:\n            shutil.copy2(source_data[\"fullpath\"], destination_data[\"fullpath\"])\n        except Exception as e:\n            raise StorageError(\n                \"Could not copy %s in %s to %s in %s\"\n                % (\n                    source_data[\"name\"],\n                    source_data[\"path\"],\n                    destination_data[\"name\"],\n                    destination_data[\"path\"],\n                ),\n                cause=e,\n            )\n\n        self._copy_metadata_entry(\n            source_data[\"path\"],\n            source_data[\"name\"],\n            destination_data[\"path\"],\n            destination_data[\"name\"],\n        )\n        self._set_display_metadata(destination_data, source_data=source_data)\n\n        return self.path_in_storage(destination_data[\"fullpath\"])\n\n    def move_file(self, source, destination, allow_overwrite=False):\n        source_data, destination_data = self._get_source_destination_data(\n            source, destination\n        )\n\n        if not octoprint.filemanager.valid_file_type(destination_data[\"name\"]):\n            raise StorageError(\n                f\"{destination_data['name']} is an unrecognized file type\",\n                code=StorageError.INVALID_FILE,\n            )\n\n        # only a display rename? Update that and bail early\n        if source_data[\"fullpath\"] == destination_data[\"fullpath\"]:\n            self._set_display_metadata(destination_data)\n            return self.path_in_storage(destination_data[\"fullpath\"])\n\n        try:\n            shutil.move(source_data[\"fullpath\"], destination_data[\"fullpath\"])\n        except Exception as e:\n            raise StorageError(\n                \"Could not move %s in %s to %s in %s\"\n                % (\n                    source_data[\"name\"],\n                    source_data[\"path\"],\n                    destination_data[\"name\"],\n                    destination_data[\"path\"],\n                ),\n                cause=e,\n            )\n\n        self._copy_metadata_entry(\n            source_data[\"path\"],\n            source_data[\"name\"],\n            destination_data[\"path\"],\n            destination_data[\"name\"],\n            delete_source=True,\n        )\n        self._set_display_metadata(destination_data, source_data=source_data)\n\n        return self.path_in_storage(destination_data[\"fullpath\"])\n\n    def has_analysis(self, path):\n        metadata = self.get_metadata(path)\n        return \"analysis\" in metadata\n\n    def get_metadata(self, path):\n        path, name = self.sanitize(path)\n        return self._get_metadata_entry(path, name)\n\n    def get_link(self, path, rel):\n        path, name = self.sanitize(path)\n        return self._get_links(name, path, rel)\n\n    def add_link(self, path, rel, data):\n        path, name = self.sanitize(path)\n        self._add_links(name, path, [(rel, data)])\n\n    def remove_link(self, path, rel, data):\n        path, name = self.sanitize(path)\n        self._remove_links(name, path, [(rel, data)])\n\n    def add_history(self, path, data):\n        path, name = self.sanitize(path)\n        self._add_history(name, path, data)\n\n    def update_history(self, path, index, data):\n        path, name = self.sanitize(path)\n        self._update_history(name, path, index, data)\n\n    def remove_history(self, path, index):\n        path, name = self.sanitize(path)\n        self._delete_history(name, path, index)\n\n    def get_additional_metadata(self, path, key):\n        path, name = self.sanitize(path)\n        metadata = self._get_metadata(path)\n\n        if name not in metadata:\n            return\n\n        return metadata[name].get(key)\n\n    def set_additional_metadata(self, path, key, data, overwrite=False, merge=False):\n        path, name = self.sanitize(path)\n        metadata = self._get_metadata(path)\n        metadata_dirty = False\n\n        if name not in metadata:\n            return\n\n        metadata = self._copied_metadata(metadata, name)\n\n        if key not in metadata[name] or overwrite:\n            metadata[name][key] = data\n            metadata_dirty = True\n        elif (\n            key in metadata[name]\n            and isinstance(metadata[name][key], dict)\n            and isinstance(data, dict)\n            and merge\n        ):\n            import octoprint.util\n\n            metadata[name][key] = octoprint.util.dict_merge(\n                metadata[name][key], data, in_place=True\n            )\n            metadata_dirty = True\n\n        if metadata_dirty:\n            self._save_metadata(path, metadata)\n\n    def remove_additional_metadata(self, path, key):\n        path, name = self.sanitize(path)\n        metadata = self._get_metadata(path)\n\n        if name not in metadata:\n            return\n\n        if key not in metadata[name]:\n            return\n\n        metadata = self._copied_metadata(metadata, name)\n        del metadata[name][key]\n        self._save_metadata(path, metadata)\n\n    def split_path(self, path):\n        path = to_unicode(path)\n        split = path.split(\"/\")\n        if len(split) == 1:\n            return \"\", split[0]\n        else:\n            return self.join_path(*split[:-1]), split[-1]\n\n    def join_path(self, *path):\n        return \"/\".join(map(to_unicode, path))\n\n    def sanitize(self, path):\n        \"\"\"\n        Returns a ``(path, name)`` tuple derived from the provided ``path``.\n\n        ``path`` may be:\n          * a storage path\n          * an absolute file system path\n          * a tuple or list containing all individual path elements\n          * a string representation of the path\n          * with or without a file name\n\n        Note that for a ``path`` without a trailing slash the last part will be considered a file name and\n        hence be returned at second position. If you only need to convert a folder path, be sure to\n        include a trailing slash for a string ``path`` or an empty last element for a list ``path``.\n        \"\"\"\n\n        path, name = self.canonicalize(path)\n        name = self.sanitize_name(name)\n        path = self.sanitize_path(path)\n        return path, name\n\n    def canonicalize(self, path):\n        name = None\n        if isinstance(path, str):\n            path = to_unicode(path)\n            if path.startswith(self.basefolder):\n                path = path[len(self.basefolder) :]\n            path = path.replace(os.path.sep, \"/\")\n            path = path.split(\"/\")\n        if isinstance(path, (list, tuple)):\n            if len(path) == 1:\n                name = to_unicode(path[0])\n                path = \"\"\n            else:\n                name = to_unicode(path[-1])\n                path = self.join_path(*map(to_unicode, path[:-1]))\n        if not path:\n            path = \"\"\n\n        return path, name\n\n    def sanitize_name(self, name):\n        \"\"\"\n        Raises a :class:`ValueError` for a ``name`` containing ``/`` or ``\\\\``. Otherwise\n        sanitizes the given ``name`` using ``octoprint.files.sanitize_filename``. Also\n        strips any leading ``.``.\n        \"\"\"\n        return sanitize_filename(name, really_universal=self._really_universal)\n\n    def sanitize_path(self, path):\n        \"\"\"\n        Ensures that the on disk representation of ``path`` is located under the configured basefolder. Resolves all\n        relative path elements (e.g. ``..``) and sanitizes folder names using :func:`sanitize_name`. Final path is the\n        absolute path including leading ``basefolder`` path.\n        \"\"\"\n        path = to_unicode(path)\n\n        if len(path):\n            if path[0] == \"/\":\n                path = path[1:]\n            elif path[0] == \".\" and path[1] == \"/\":\n                path = path[2:]\n\n        path_elements = path.split(\"/\")\n        joined_path = self.basefolder\n        for path_element in path_elements:\n            if path_element == \"..\" or path_element == \".\":\n                joined_path = os.path.join(joined_path, path_element)\n            else:\n                joined_path = os.path.join(joined_path, self.sanitize_name(path_element))\n        path = os.path.realpath(joined_path)\n        if not path.startswith(self.basefolder):\n            raise ValueError(f\"path not contained in base folder: {path}\")\n        return path\n\n    def _sanitize_entry(self, entry, path, entry_path):\n        entry = to_unicode(entry)\n        sanitized = self.sanitize_name(entry)\n        if sanitized != entry:\n            # entry is not sanitized yet, let's take care of that\n            sanitized_path = os.path.join(path, sanitized)\n            sanitized_name, sanitized_ext = os.path.splitext(sanitized)\n\n            counter = 1\n            while os.path.exists(sanitized_path):\n                counter += 1\n                sanitized = self.sanitize_name(\n                    f\"{sanitized_name}_({counter}){sanitized_ext}\"\n                )\n                sanitized_path = os.path.join(path, sanitized)\n\n            try:\n                shutil.move(entry_path, sanitized_path)\n\n                self._logger.info(f'Sanitized \"{entry_path}\" to \"{sanitized_path}\"')\n                return sanitized, sanitized_path\n            except Exception:\n                self._logger.exception(\n                    'Error while trying to rename \"{}\" to \"{}\", ignoring file'.format(\n                        entry_path, sanitized_path\n                    )\n                )\n                raise\n\n        return entry, entry_path\n\n    def path_in_storage(self, path):\n        if isinstance(path, (tuple, list)):\n            path = self.join_path(*path)\n        if isinstance(path, str):\n            path = to_unicode(path)\n            if path.startswith(self.basefolder):\n                path = path[len(self.basefolder) :]\n            path = path.replace(os.path.sep, \"/\")\n        if path.startswith(\"/\"):\n            path = path[1:]\n\n        return path\n\n    def path_on_disk(self, path):\n        path, name = self.sanitize(path)\n        return os.path.join(path, name)\n\n    ##~~ internals\n\n    def _add_history(self, name, path, data):\n        metadata = self._copied_metadata(self._get_metadata(path), name)\n\n        if \"hash\" not in metadata[name]:\n            metadata[name][\"hash\"] = self._create_hash(os.path.join(path, name))\n\n        if \"history\" not in metadata[name]:\n            metadata[name][\"history\"] = []\n\n        metadata[name][\"history\"].append(data)\n        self._calculate_stats_from_history(name, path, metadata=metadata, save=False)\n        self._save_metadata(path, metadata)\n\n    def _update_history(self, name, path, index, data):\n        metadata = self._get_metadata(path)\n\n        if name not in metadata or \"history\" not in metadata[name]:\n            return\n\n        metadata = self._copied_metadata(metadata, name)\n\n        try:\n            metadata[name][\"history\"][index].update(data)\n            self._calculate_stats_from_history(name, path, metadata=metadata, save=False)\n            self._save_metadata(path, metadata)\n        except IndexError:\n            pass\n\n    def _delete_history(self, name, path, index):\n        metadata = self._get_metadata(path)\n\n        if name not in metadata or \"history\" not in metadata[name]:\n            return\n\n        metadata = self._copied_metadata(metadata, name)\n\n        try:\n            del metadata[name][\"history\"][index]\n            self._calculate_stats_from_history(name, path, metadata=metadata, save=False)\n            self._save_metadata(path, metadata)\n        except IndexError:\n            pass\n\n    def _calculate_stats_from_history(self, name, path, metadata=None, save=True):\n        if metadata is None:\n            metadata = self._copied_metadata(self._get_metadata(path), name)\n\n        if \"history\" not in metadata[name]:\n            return\n\n        # collect data from history\n        former_print_times = {}\n        last_print = {}\n\n        for history_entry in metadata[name][\"history\"]:\n            if (\n                \"printTime\" not in history_entry\n                or \"success\" not in history_entry\n                or not history_entry[\"success\"]\n                or \"printerProfile\" not in history_entry\n            ):\n                continue\n\n            printer_profile = history_entry[\"printerProfile\"]\n            if not printer_profile:\n                continue\n\n            print_time = history_entry[\"printTime\"]\n            try:\n                print_time = float(print_time)\n            except Exception:\n                self._logger.warning(\n                    \"Invalid print time value found in print history for {} in {}/.metadata.json: {!r}\".format(\n                        name, path, print_time\n                    )\n                )\n                continue\n\n            if printer_profile not in former_print_times:\n                former_print_times[printer_profile] = []\n            former_print_times[printer_profile].append(print_time)\n\n            if (\n                printer_profile not in last_print\n                or last_print[printer_profile] is None\n                or (\n                    \"timestamp\" in history_entry\n                    and history_entry[\"timestamp\"]\n                    > last_print[printer_profile][\"timestamp\"]\n                )\n            ):\n                last_print[printer_profile] = history_entry\n\n        # calculate stats\n        statistics = {\"averagePrintTime\": {}, \"lastPrintTime\": {}}\n\n        for printer_profile in former_print_times:\n            if not former_print_times[printer_profile]:\n                continue\n            statistics[\"averagePrintTime\"][printer_profile] = sum(\n                former_print_times[printer_profile]\n            ) / len(former_print_times[printer_profile])\n\n        for printer_profile in last_print:\n            if not last_print[printer_profile]:\n                continue\n            statistics[\"lastPrintTime\"][printer_profile] = last_print[printer_profile][\n                \"printTime\"\n            ]\n\n        metadata[name][\"statistics\"] = statistics\n\n        if save:\n            self._save_metadata(path, metadata)\n\n    def _get_links(self, name, path, searched_rel):\n        metadata = self._get_metadata(path)\n        result = []\n\n        if name not in metadata:\n            return result\n\n        if \"links\" not in metadata[name]:\n            return result\n\n        for data in metadata[name][\"links\"]:\n            if \"rel\" not in data or not data[\"rel\"] == searched_rel:\n                continue\n            result.append(data)\n        return result\n\n    def _add_links(self, name, path, links):\n        file_type = octoprint.filemanager.get_file_type(name)\n        if file_type:\n            file_type = file_type[0]\n\n        metadata = self._copied_metadata(self._get_metadata(path), name)\n        metadata_dirty = False\n\n        if \"hash\" not in metadata[name]:\n            metadata[name][\"hash\"] = self._create_hash(os.path.join(path, name))\n\n        if \"links\" not in metadata[name]:\n            metadata[name][\"links\"] = []\n\n        for rel, data in links:\n            if (rel == \"model\" or rel == \"machinecode\") and \"name\" in data:\n                if file_type == \"model\" and rel == \"model\":\n                    # adding a model link to a model doesn't make sense\n                    return\n                elif file_type == \"machinecode\" and rel == \"machinecode\":\n                    # adding a machinecode link to a machinecode doesn't make sense\n                    return\n\n                ref_path = os.path.join(path, data[\"name\"])\n                if not os.path.exists(ref_path):\n                    # file doesn't exist, we won't create the link\n                    continue\n\n                # fetch hash of target file\n                if data[\"name\"] in metadata and \"hash\" in metadata[data[\"name\"]]:\n                    hash = metadata[data[\"name\"]][\"hash\"]\n                else:\n                    hash = self._create_hash(ref_path)\n                    if data[\"name\"] not in metadata:\n                        metadata[data[\"name\"]] = {\"hash\": hash, \"links\": []}\n                    else:\n                        metadata[data[\"name\"]][\"hash\"] = hash\n\n                if \"hash\" in data and not data[\"hash\"] == hash:\n                    # file doesn't have the correct hash, we won't create the link\n                    continue\n\n                if \"links\" not in metadata[data[\"name\"]]:\n                    metadata[data[\"name\"]][\"links\"] = []\n\n                # add reverse link to link target file\n                metadata[data[\"name\"]][\"links\"].append(\n                    {\n                        \"rel\": \"machinecode\" if rel == \"model\" else \"model\",\n                        \"name\": name,\n                        \"hash\": metadata[name][\"hash\"],\n                    }\n                )\n                metadata_dirty = True\n\n                link_dict = {\"rel\": rel, \"name\": data[\"name\"], \"hash\": hash}\n\n            elif rel == \"web\" and \"href\" in data:\n                link_dict = {\"rel\": rel, \"href\": data[\"href\"]}\n                if \"retrieved\" in data:\n                    link_dict[\"retrieved\"] = data[\"retrieved\"]\n\n            else:\n                continue\n\n            if link_dict:\n                metadata[name][\"links\"].append(link_dict)\n                metadata_dirty = True\n\n        if metadata_dirty:\n            self._save_metadata(path, metadata)\n\n    def _remove_links(self, name, path, links):\n        metadata = self._copied_metadata(self._get_metadata(path), name)\n        metadata_dirty = False\n\n        hash = metadata[name].get(\"hash\", self._create_hash(os.path.join(path, name)))\n\n        for rel, data in links:\n            if (rel == \"model\" or rel == \"machinecode\") and \"name\" in data:\n                if data[\"name\"] in metadata and \"links\" in metadata[data[\"name\"]]:\n                    ref_rel = \"model\" if rel == \"machinecode\" else \"machinecode\"\n                    for link in metadata[data[\"name\"]][\"links\"]:\n                        if (\n                            link[\"rel\"] == ref_rel\n                            and \"name\" in link\n                            and link[\"name\"] == name\n                            and \"hash\" in link\n                            and link[\"hash\"] == hash\n                        ):\n                            metadata[data[\"name\"]] = copy.deepcopy(metadata[data[\"name\"]])\n                            metadata[data[\"name\"]][\"links\"].remove(link)\n                            metadata_dirty = True\n\n            if \"links\" in metadata[name]:\n                for link in metadata[name][\"links\"]:\n                    if not link[\"rel\"] == rel:\n                        continue\n\n                    matches = True\n                    for k, v in data.items():\n                        if k not in link or not link[k] == v:\n                            matches = False\n                            break\n\n                    if not matches:\n                        continue\n\n                    metadata[name][\"links\"].remove(link)\n                    metadata_dirty = True\n\n        if metadata_dirty:\n            self._save_metadata(path, metadata)\n\n    @time_this(\n        logtarget=__name__ + \".timings\",\n        message=\"{func}({func_args},{func_kwargs}) took {timing:.2f}ms\",\n        incl_func_args=True,\n        log_enter=True,\n    )\n    def _list_folder(self, path, base=\"\", force_refresh=False, **kwargs):\n        def get_size(nodes):\n            total_size = 0\n            for node in nodes.values():\n                if \"size\" in node:\n                    total_size += node[\"size\"]\n            return total_size\n\n        def enrich_folders(nodes):\n            nodes = copy.copy(nodes)\n            for key, value in nodes.items():\n                if value[\"type\"] == \"folder\":\n                    value = copy.copy(value)\n                    value[\"children\"] = self._list_folder(\n                        os.path.join(path, key),\n                        base=value[\"path\"] + \"/\",\n                        force_refresh=force_refresh,\n                    )\n                    value[\"size\"] = get_size(value[\"children\"])\n                    nodes[key] = value\n            return nodes\n\n        metadata_dirty = False\n        try:\n            with self._filelist_cache_mutex:\n                cache = self._filelist_cache.get(path)\n                lm = self.last_modified(path, recursive=True)\n                if not force_refresh and cache and cache[0] >= lm:\n                    return enrich_folders(cache[1])\n\n                metadata = self._get_metadata(path)\n                if not metadata:\n                    metadata = {}\n\n                result = {}\n\n                for entry in scandir(path):\n                    if is_hidden_path(entry.name):\n                        # no hidden files and folders\n                        continue\n\n                    try:\n                        entry_name = entry_display = entry.name\n                        entry_path = entry.path\n                        entry_is_file = entry.is_file()\n                        entry_is_dir = entry.is_dir()\n                        entry_stat = entry.stat()\n                    except Exception:\n                        # error while trying to fetch file metadata, that might be thanks to file already having\n                        # been moved or deleted - ignore it and continue\n                        continue\n\n                    try:\n                        new_entry_name, new_entry_path = self._sanitize_entry(\n                            entry_name, path, entry_path\n                        )\n                        if entry_name != new_entry_name or entry_path != new_entry_path:\n                            entry_display = to_unicode(entry_name)\n                            entry_name = new_entry_name\n                            entry_path = new_entry_path\n                            entry_stat = os.stat(entry_path)\n                    except Exception:\n                        # error while trying to rename the file, we'll continue here and ignore it\n                        continue\n\n                    path_in_location = entry_name if not base else base + entry_name\n\n                    try:\n                        # file handling\n                        if entry_is_file:\n                            type_path = octoprint.filemanager.get_file_type(entry_name)\n                            if not type_path:\n                                # only supported extensions\n                                continue\n                            else:\n                                file_type = type_path[0]\n\n                            if entry_name in metadata and isinstance(\n                                metadata[entry_name], dict\n                            ):\n                                entry_metadata = metadata[entry_name]\n                                if (\n                                    \"display\" not in entry_metadata\n                                    and entry_display != entry_name\n                                ):\n                                    if not metadata_dirty:\n                                        metadata = self._copied_metadata(\n                                            metadata, entry_name\n                                        )\n                                    metadata[entry_name][\"display\"] = entry_display\n                                    entry_metadata[\"display\"] = entry_display\n                                    metadata_dirty = True\n                            else:\n                                if not metadata_dirty:\n                                    metadata = self._copied_metadata(metadata, entry_name)\n                                entry_metadata = self._add_basic_metadata(\n                                    path,\n                                    entry_name,\n                                    display_name=entry_display,\n                                    save=False,\n                                    metadata=metadata,\n                                )\n                                metadata_dirty = True\n\n                            extended_entry_data = {}\n                            extended_entry_data.update(entry_metadata)\n                            extended_entry_data[\"name\"] = entry_name\n                            extended_entry_data[\"display\"] = entry_metadata.get(\n                                \"display\", entry_name\n                            )\n                            extended_entry_data[\"path\"] = path_in_location\n                            extended_entry_data[\"type\"] = file_type\n                            extended_entry_data[\"typePath\"] = type_path\n                            stat = entry_stat\n                            if stat:\n                                extended_entry_data[\"size\"] = stat.st_size\n                                extended_entry_data[\"date\"] = int(stat.st_mtime)\n\n                            result[entry_name] = extended_entry_data\n\n                        # folder recursion\n                        elif entry_is_dir:\n                            if entry_name in metadata and isinstance(\n                                metadata[entry_name], dict\n                            ):\n                                entry_metadata = metadata[entry_name]\n                                if (\n                                    \"display\" not in entry_metadata\n                                    and entry_display != entry_name\n                                ):\n                                    if not metadata_dirty:\n                                        metadata = self._copied_metadata(\n                                            metadata, entry_name\n                                        )\n                                    metadata[entry_name][\"display\"] = entry_display\n                                    entry_metadata[\"display\"] = entry_display\n                                    metadata_dirty = True\n                            elif entry_name != entry_display:\n                                if not metadata_dirty:\n                                    metadata = self._copied_metadata(metadata, entry_name)\n                                entry_metadata = self._add_basic_metadata(\n                                    path,\n                                    entry_name,\n                                    display_name=entry_display,\n                                    save=False,\n                                    metadata=metadata,\n                                )\n                                metadata_dirty = True\n                            else:\n                                entry_metadata = {}\n\n                            entry_data = {\n                                \"name\": entry_name,\n                                \"display\": entry_metadata.get(\"display\", entry_name),\n                                \"path\": path_in_location,\n                                \"type\": \"folder\",\n                                \"typePath\": [\"folder\"],\n                            }\n\n                            result[entry_name] = entry_data\n                    except Exception:\n                        # So something went wrong somewhere while processing this file entry - log that and continue\n                        self._logger.exception(\n                            f\"Error while processing entry {entry_path}\"\n                        )\n                        continue\n\n                self._filelist_cache[path] = (\n                    lm,\n                    result,\n                )\n                return enrich_folders(result)\n        finally:\n            # save metadata\n            if metadata_dirty:\n                self._save_metadata(path, metadata)\n\n    def _add_basic_metadata(\n        self,\n        path,\n        entry,\n        display_name=None,\n        additional_metadata=None,\n        save=True,\n        metadata=None,\n    ):\n        if additional_metadata is None:\n            additional_metadata = {}\n\n        if metadata is None:\n            metadata = self._get_metadata(path)\n\n        entry_path = os.path.join(path, entry)\n\n        if os.path.isfile(entry_path):\n            entry_data = {\n                \"hash\": self._create_hash(os.path.join(path, entry)),\n                \"links\": [],\n                \"notes\": [],\n            }\n            if (\n                path == self.basefolder\n                and self._old_metadata is not None\n                and entry in self._old_metadata\n                and \"gcodeAnalysis\" in self._old_metadata[entry]\n            ):\n                # if there is still old metadata available and that contains an analysis for this file, use it!\n                entry_data[\"analysis\"] = self._old_metadata[entry][\"gcodeAnalysis\"]\n\n        elif os.path.isdir(entry_path):\n            entry_data = {}\n\n        else:\n            return\n\n        if display_name is not None and not display_name == entry:\n            entry_data[\"display\"] = display_name\n\n        entry_data.update(additional_metadata)\n\n        metadata = copy.copy(metadata)\n        metadata[entry] = entry_data\n\n        if save:\n            self._save_metadata(path, metadata)\n\n        return entry_data\n\n    def _create_hash(self, path):\n        import hashlib\n\n        blocksize = 65536\n        hash = hashlib.sha1()\n        with open(path, \"rb\") as f:\n            buffer = f.read(blocksize)\n            while len(buffer) > 0:\n                hash.update(buffer)\n                buffer = f.read(blocksize)\n\n        return hash.hexdigest()\n\n    def _get_metadata_entry(self, path, name, default=None):\n        with self._get_metadata_lock(path):\n            metadata = self._get_metadata(path)\n            return metadata.get(name, default)\n\n    def _remove_metadata_entry(self, path, name):\n        with self._get_metadata_lock(path):\n            metadata = self._get_metadata(path)\n            if name not in metadata:\n                return\n\n            metadata = copy.copy(metadata)\n\n            if \"hash\" in metadata[name]:\n                hash = metadata[name][\"hash\"]\n                for m in metadata.values():\n                    if \"links\" not in m:\n                        continue\n                    links_hash = (\n                        lambda link: \"hash\" in link\n                        and link[\"hash\"] == hash\n                        and \"rel\" in link\n                        and (link[\"rel\"] == \"model\" or link[\"rel\"] == \"machinecode\")\n                    )\n                    m[\"links\"] = [link for link in m[\"links\"] if not links_hash(link)]\n\n            del metadata[name]\n            self._save_metadata(path, metadata)\n\n    def _update_metadata_entry(self, path, name, data):\n        with self._get_metadata_lock(path):\n            metadata = copy.copy(self._get_metadata(path))\n            metadata[name] = data\n            self._save_metadata(path, metadata)\n\n    def _copy_metadata_entry(\n        self,\n        source_path,\n        source_name,\n        destination_path,\n        destination_name,\n        delete_source=False,\n        updates=None,\n    ):\n        with self._get_metadata_lock(source_path):\n            source_data = self._get_metadata_entry(source_path, source_name, default={})\n            if not source_data:\n                return\n\n            if delete_source:\n                self._remove_metadata_entry(source_path, source_name)\n\n        if updates is not None:\n            source_data.update(updates)\n\n        with self._get_metadata_lock(destination_path):\n            self._update_metadata_entry(destination_path, destination_name, source_data)\n\n    def _get_metadata(self, path, force=False):\n        import json\n\n        if not force:\n            metadata = self._metadata_cache.get(path)\n            if metadata:\n                return metadata\n\n        self._migrate_metadata(path)\n\n        metadata_path = os.path.join(path, \".metadata.json\")\n\n        metadata = None\n        with self._get_persisted_metadata_lock(path):\n            if os.path.exists(metadata_path):\n                with open(metadata_path, encoding=\"utf-8\") as f:\n                    try:\n                        metadata = json.load(f)\n                    except Exception:\n                        self._logger.exception(\n                            f\"Error while reading .metadata.json from {path}\"\n                        )\n\n        def valid_json(value):\n            try:\n                json.dumps(value, allow_nan=False)\n                return True\n            except Exception:\n                return False\n\n        if isinstance(metadata, dict):\n            old_size = len(metadata)\n            metadata = {k: v for k, v in metadata.items() if valid_json(v)}\n            metadata = {\n                k: v for k, v in metadata.items() if os.path.exists(os.path.join(path, k))\n            }\n            new_size = len(metadata)\n            if new_size != old_size:\n                self._logger.info(\n                    \"Deleted {} stale or invalid entries from metadata for path {}\".format(\n                        old_size - new_size, path\n                    )\n                )\n                self._save_metadata(path, metadata)\n            else:\n                with self._get_metadata_lock(path):\n                    self._metadata_cache[path] = metadata\n            return metadata\n        else:\n            return {}\n\n    def _save_metadata(self, path, metadata):\n        import json\n\n        with self._get_metadata_lock(path):\n            self._metadata_cache[path] = metadata\n\n        with self._get_persisted_metadata_lock(path):\n            metadata_path = os.path.join(path, \".metadata.json\")\n            try:\n                with atomic_write(metadata_path, mode=\"wb\") as f:\n                    f.write(\n                        to_bytes(json.dumps(metadata, indent=2, separators=(\",\", \": \")))\n                    )\n            except Exception:\n                self._logger.exception(f\"Error while writing .metadata.json to {path}\")\n\n    def _delete_metadata(self, path):\n        with self._get_metadata_lock(path):\n            if path in self._metadata_cache:\n                del self._metadata_cache[path]\n\n        with self._get_persisted_metadata_lock(path):\n            metadata_files = (\".metadata.json\", \".metadata.yaml\")\n            for metadata_file in metadata_files:\n                metadata_path = os.path.join(path, metadata_file)\n                if os.path.exists(metadata_path):\n                    try:\n                        os.remove(metadata_path)\n                    except Exception:\n                        self._logger.exception(\n                            f\"Error while deleting {metadata_file} from {path}\"\n                        )\n\n    @staticmethod\n    def _copied_metadata(metadata, name):\n        metadata = copy.copy(metadata)\n        metadata[name] = copy.deepcopy(metadata.get(name, {}))\n        return metadata\n\n    def _migrate_metadata(self, path):\n        # we switched to json in 1.3.9 - if we still have yaml here, migrate it now\n        import json\n\n        with self._get_persisted_metadata_lock(path):\n            metadata_path_yaml = os.path.join(path, \".metadata.yaml\")\n            metadata_path_json = os.path.join(path, \".metadata.json\")\n\n            if not os.path.exists(metadata_path_yaml):\n                # nothing to migrate\n                return\n\n            if os.path.exists(metadata_path_json):\n                # already migrated\n                try:\n                    os.remove(metadata_path_yaml)\n                except Exception:\n                    self._logger.exception(\n                        f\"Error while removing .metadata.yaml from {path}\"\n                    )\n                return\n\n            try:\n                metadata = yaml.load_from_file(path=metadata_path_yaml)\n            except Exception:\n                self._logger.exception(f\"Error while reading .metadata.yaml from {path}\")\n                return\n\n            if not isinstance(metadata, dict):\n                # looks invalid, ignore it\n                return\n\n            with atomic_write(metadata_path_json, mode=\"wb\") as f:\n                f.write(to_bytes(json.dumps(metadata, indent=2, separators=(\",\", \": \"))))\n\n            try:\n                os.remove(metadata_path_yaml)\n            except Exception:\n                self._logger.exception(f\"Error while removing .metadata.yaml from {path}\")\n\n    @contextmanager\n    def _get_metadata_lock(self, path):\n        with self._metadata_lock_mutex:\n            if path not in self._metadata_locks:\n                import threading\n\n                self._metadata_locks[path] = (0, threading.RLock())\n\n            counter, lock = self._metadata_locks[path]\n            counter += 1\n            self._metadata_locks[path] = (counter, lock)\n\n        yield lock\n\n        with self._metadata_lock_mutex:\n            counter = self._metadata_locks[path][0]\n            counter -= 1\n            if counter <= 0:\n                del self._metadata_locks[path]\n            else:\n                self._metadata_locks[path] = (counter, lock)\n\n    @contextmanager\n    def _get_persisted_metadata_lock(self, path):\n        with self._persisted_metadata_lock_mutex:\n            if path not in self._persisted_metadata_locks:\n                import threading\n\n                self._persisted_metadata_locks[path] = (0, threading.RLock())\n\n            counter, lock = self._persisted_metadata_locks[path]\n            counter += 1\n            self._persisted_metadata_locks[path] = (counter, lock)\n\n        yield lock\n\n        with self._persisted_metadata_lock_mutex:\n            counter = self._persisted_metadata_locks[path][0]\n            counter -= 1\n            if counter <= 0:\n                del self._persisted_metadata_locks[path]\n            else:\n                self._persisted_metadata_locks[path] = (counter, lock)\n", "__author__ = \"Gina H\u00e4u\u00dfge <osd@foosel.net>\"\n__license__ = \"GNU Affero General Public License http://www.gnu.org/licenses/agpl.html\"\n__copyright__ = \"Copyright (C) 2014 The OctoPrint Project - Released under terms of the AGPLv3 License\"\n\nimport atexit\nimport base64\nimport functools\nimport logging\nimport logging.config\nimport mimetypes\nimport os\nimport re\nimport signal\nimport sys\nimport time\nimport uuid  # noqa: F401\nfrom collections import OrderedDict, defaultdict\n\nfrom babel import Locale\nfrom flask import (  # noqa: F401\n    Blueprint,\n    Flask,\n    Request,\n    Response,\n    current_app,\n    g,\n    make_response,\n    request,\n    session,\n)\nfrom flask_assets import Bundle, Environment\nfrom flask_babel import Babel, gettext, ngettext  # noqa: F401\nfrom flask_login import (  # noqa: F401\n    LoginManager,\n    current_user,\n    session_protected,\n    user_logged_out,\n)\nfrom watchdog.observers import Observer\nfrom watchdog.observers.polling import PollingObserver\nfrom werkzeug.exceptions import HTTPException\n\nimport octoprint.filemanager\nimport octoprint.util\nimport octoprint.util.net\nfrom octoprint.server import util\nfrom octoprint.systemcommands import system_command_manager\nfrom octoprint.util.json import JsonEncoding\nfrom octoprint.vendor.flask_principal import (  # noqa: F401\n    AnonymousIdentity,\n    Identity,\n    Permission,\n    Principal,\n    RoleNeed,\n    UserNeed,\n    identity_changed,\n    identity_loaded,\n)\nfrom octoprint.vendor.sockjs.tornado import SockJSRouter\n\ntry:\n    import fcntl\nexcept ImportError:\n    fcntl = None\n\nSUCCESS = {}\nNO_CONTENT = (\"\", 204, {\"Content-Type\": \"text/plain\"})\nNOT_MODIFIED = (\"Not Modified\", 304, {\"Content-Type\": \"text/plain\"})\n\napp = Flask(\"octoprint\")\n\nassets = None\nbabel = None\nlimiter = None\ndebug = False\nsafe_mode = False\n\nprinter = None\nprinterProfileManager = None\nfileManager = None\nslicingManager = None\nanalysisQueue = None\nuserManager = None\npermissionManager = None\ngroupManager = None\neventManager = None\nloginManager = None\npluginManager = None\npluginLifecycleManager = None\npreemptiveCache = None\njsonEncoder = None\njsonDecoder = None\nconnectivityChecker = None\nenvironmentDetector = None\n\nprincipals = Principal(app)\n\nimport octoprint.access.groups as groups  # noqa: E402\nimport octoprint.access.permissions as permissions  # noqa: E402\n\n# we set admin_permission to a GroupPermission with the default admin group\nadmin_permission = octoprint.util.variable_deprecated(\n    \"admin_permission has been deprecated, \" \"please use individual Permissions instead\",\n    since=\"1.4.0\",\n)(groups.GroupPermission(groups.ADMIN_GROUP))\n\n# we set user_permission to a GroupPermission with the default user group\nuser_permission = octoprint.util.variable_deprecated(\n    \"user_permission has been deprecated, \" \"please use individual Permissions instead\",\n    since=\"1.4.0\",\n)(groups.GroupPermission(groups.USER_GROUP))\n\nimport octoprint._version  # noqa: E402\nimport octoprint.access.groups as groups  # noqa: E402\nimport octoprint.access.users as users  # noqa: E402\nimport octoprint.events as events  # noqa: E402\nimport octoprint.filemanager.analysis  # noqa: E402\nimport octoprint.filemanager.storage  # noqa: E402\nimport octoprint.plugin  # noqa: E402\nimport octoprint.slicing  # noqa: E402\nimport octoprint.timelapse  # noqa: E402\n\n# only import further octoprint stuff down here, as it might depend on things defined above to be initialized already\nfrom octoprint import __branch__, __display_version__, __revision__, __version__\nfrom octoprint.printer.profile import PrinterProfileManager\nfrom octoprint.printer.standard import Printer\nfrom octoprint.server.util import (\n    corsRequestHandler,\n    corsResponseHandler,\n    loginFromApiKeyRequestHandler,\n    requireLoginRequestHandler,\n)\nfrom octoprint.server.util.flask import PreemptiveCache\nfrom octoprint.settings import settings\n\nVERSION = __version__\nBRANCH = __branch__\nDISPLAY_VERSION = __display_version__\nREVISION = __revision__\n\nLOCALES = []\nLANGUAGES = set()\n\n\n@identity_loaded.connect_via(app)\ndef on_identity_loaded(sender, identity):\n    user = load_user(identity.id)\n    if user is None:\n        user = userManager.anonymous_user_factory()\n\n    identity.provides.add(UserNeed(user.get_id()))\n    for need in user.needs:\n        identity.provides.add(need)\n\n\ndef _clear_identity(sender):\n    # Remove session keys set by Flask-Principal\n    for key in (\"identity.id\", \"identity.name\", \"identity.auth_type\"):\n        session.pop(key, None)\n\n    # switch to anonymous identity\n    identity_changed.send(sender, identity=AnonymousIdentity())\n\n\n@session_protected.connect_via(app)\ndef on_session_protected(sender):\n    # session was deleted by session protection, that means the user is no more and we need to clear our identity\n    if session.get(\"remember\", None) == \"clear\":\n        _clear_identity(sender)\n\n\n@user_logged_out.connect_via(app)\ndef on_user_logged_out(sender, user=None):\n    # user was logged out, clear identity\n    _clear_identity(sender)\n\n\ndef load_user(id):\n    if id is None:\n        return None\n\n    if id == \"_api\":\n        return userManager.api_user_factory()\n\n    if session and \"usersession.id\" in session:\n        sessionid = session[\"usersession.id\"]\n    else:\n        sessionid = None\n\n    if sessionid:\n        user = userManager.find_user(userid=id, session=sessionid)\n    else:\n        user = userManager.find_user(userid=id)\n\n    if user and user.is_active:\n        return user\n\n    return None\n\n\ndef load_user_from_request(request):\n    user = None\n\n    if settings().getBoolean([\"accessControl\", \"trustBasicAuthentication\"]):\n        # Basic Authentication?\n        user = util.get_user_for_authorization_header(\n            request.headers.get(\"Authorization\")\n        )\n\n    if settings().getBoolean([\"accessControl\", \"trustRemoteUser\"]):\n        # Remote user header?\n        user = util.get_user_for_remote_user_header(request)\n\n    return user\n\n\ndef unauthorized_user():\n    from flask import abort\n\n    abort(403)\n\n\n# ~~ startup code\n\n\nclass Server:\n    def __init__(\n        self,\n        settings=None,\n        plugin_manager=None,\n        connectivity_checker=None,\n        environment_detector=None,\n        event_manager=None,\n        host=None,\n        port=None,\n        v6_only=False,\n        debug=False,\n        safe_mode=False,\n        allow_root=False,\n        octoprint_daemon=None,\n    ):\n        self._settings = settings\n        self._plugin_manager = plugin_manager\n        self._connectivity_checker = connectivity_checker\n        self._environment_detector = environment_detector\n        self._event_manager = event_manager\n        self._host = host\n        self._port = port\n        self._v6_only = v6_only\n        self._debug = debug\n        self._safe_mode = safe_mode\n        self._allow_root = allow_root\n        self._octoprint_daemon = octoprint_daemon\n        self._server = None\n\n        self._logger = None\n\n        self._lifecycle_callbacks = defaultdict(list)\n\n        self._intermediary_server = None\n\n    def run(self):\n        if not self._allow_root:\n            self._check_for_root()\n\n        if self._settings is None:\n            self._settings = settings()\n\n        if not self._settings.getBoolean([\"server\", \"ignoreIncompleteStartup\"]):\n            self._settings.setBoolean([\"server\", \"incompleteStartup\"], True)\n            self._settings.save()\n\n        if self._plugin_manager is None:\n            self._plugin_manager = octoprint.plugin.plugin_manager()\n\n        global app\n        global babel\n\n        global printer\n        global printerProfileManager\n        global fileManager\n        global slicingManager\n        global analysisQueue\n        global userManager\n        global permissionManager\n        global groupManager\n        global eventManager\n        global loginManager\n        global pluginManager\n        global pluginLifecycleManager\n        global preemptiveCache\n        global jsonEncoder\n        global jsonDecoder\n        global connectivityChecker\n        global environmentDetector\n        global debug\n        global safe_mode\n\n        from tornado.ioloop import IOLoop\n        from tornado.web import Application\n\n        debug = self._debug\n        safe_mode = self._safe_mode\n\n        if safe_mode:\n            self._log_safe_mode_start(safe_mode)\n\n        if self._v6_only and not octoprint.util.net.HAS_V6:\n            raise RuntimeError(\n                \"IPv6 only mode configured but system doesn't support IPv6\"\n            )\n\n        if self._host is None:\n            host = self._settings.get([\"server\", \"host\"])\n            if host is None:\n                if octoprint.util.net.HAS_V6:\n                    host = \"::\"\n                else:\n                    host = \"0.0.0.0\"\n\n            self._host = host\n\n        if \":\" in self._host and not octoprint.util.net.HAS_V6:\n            raise RuntimeError(\n                \"IPv6 host address {!r} configured but system doesn't support IPv6\".format(\n                    self._host\n                )\n            )\n\n        if self._port is None:\n            self._port = self._settings.getInt([\"server\", \"port\"])\n            if self._port is None:\n                self._port = 5000\n\n        self._logger = logging.getLogger(__name__)\n        self._setup_heartbeat_logging()\n        pluginManager = self._plugin_manager\n\n        # monkey patch/fix some stuff\n        util.tornado.fix_json_encode()\n        util.tornado.fix_websocket_check_origin()\n        util.tornado.enable_per_message_deflate_extension()\n        util.flask.fix_flask_jsonify()\n\n        self._setup_mimetypes()\n\n        additional_translation_folders = []\n        if not safe_mode:\n            additional_translation_folders += [\n                self._settings.getBaseFolder(\"translations\")\n            ]\n        util.flask.enable_additional_translations(\n            additional_folders=additional_translation_folders\n        )\n\n        # setup app\n        self._setup_app(app)\n\n        # setup i18n\n        self._setup_i18n(app)\n\n        if self._settings.getBoolean([\"serial\", \"log\"]):\n            # enable debug logging to serial.log\n            logging.getLogger(\"SERIAL\").setLevel(logging.DEBUG)\n\n        if self._settings.getBoolean([\"devel\", \"pluginTimings\"]):\n            # enable plugin timings log\n            logging.getLogger(\"PLUGIN_TIMINGS\").setLevel(logging.DEBUG)\n\n        # start the intermediary server\n        self._start_intermediary_server()\n\n        ### IMPORTANT!\n        ###\n        ### Best do not start any subprocesses until the intermediary server shuts down again or they MIGHT inherit the\n        ### open port and prevent us from firing up Tornado later.\n        ###\n        ### The intermediary server's socket should have the CLOSE_EXEC flag (or its equivalent) set where possible, but\n        ### we can only do that if fcntl is available or we are on Windows, so better safe than sorry.\n        ###\n        ### See also issues #2035 and #2090\n\n        systemCommandManager = system_command_manager()\n        printerProfileManager = PrinterProfileManager()\n        eventManager = self._event_manager\n\n        analysis_queue_factories = {\n            \"gcode\": octoprint.filemanager.analysis.GcodeAnalysisQueue\n        }\n        analysis_queue_hooks = pluginManager.get_hooks(\n            \"octoprint.filemanager.analysis.factory\"\n        )\n        for name, hook in analysis_queue_hooks.items():\n            try:\n                additional_factories = hook()\n                analysis_queue_factories.update(**additional_factories)\n            except Exception:\n                self._logger.exception(\n                    f\"Error while processing analysis queues from {name}\",\n                    extra={\"plugin\": name},\n                )\n        analysisQueue = octoprint.filemanager.analysis.AnalysisQueue(\n            analysis_queue_factories\n        )\n\n        slicingManager = octoprint.slicing.SlicingManager(\n            self._settings.getBaseFolder(\"slicingProfiles\"), printerProfileManager\n        )\n\n        storage_managers = {}\n        storage_managers[\n            octoprint.filemanager.FileDestinations.LOCAL\n        ] = octoprint.filemanager.storage.LocalFileStorage(\n            self._settings.getBaseFolder(\"uploads\"),\n            really_universal=self._settings.getBoolean(\n                [\"feature\", \"enforceReallyUniversalFilenames\"]\n            ),\n        )\n\n        fileManager = octoprint.filemanager.FileManager(\n            analysisQueue,\n            slicingManager,\n            printerProfileManager,\n            initial_storage_managers=storage_managers,\n        )\n        pluginLifecycleManager = LifecycleManager(pluginManager)\n        preemptiveCache = PreemptiveCache(\n            os.path.join(\n                self._settings.getBaseFolder(\"data\"), \"preemptive_cache_config.yaml\"\n            )\n        )\n\n        JsonEncoding.add_encoder(users.User, lambda obj: obj.as_dict())\n        JsonEncoding.add_encoder(groups.Group, lambda obj: obj.as_dict())\n        JsonEncoding.add_encoder(\n            permissions.OctoPrintPermission, lambda obj: obj.as_dict()\n        )\n\n        # start regular check if we are connected to the internet\n        def on_connectivity_change(old_value, new_value):\n            eventManager.fire(\n                events.Events.CONNECTIVITY_CHANGED,\n                payload={\"old\": old_value, \"new\": new_value},\n            )\n\n        connectivityChecker = self._connectivity_checker\n        environmentDetector = self._environment_detector\n\n        def on_settings_update(*args, **kwargs):\n            # make sure our connectivity checker runs with the latest settings\n            connectivityEnabled = self._settings.getBoolean(\n                [\"server\", \"onlineCheck\", \"enabled\"]\n            )\n            connectivityInterval = self._settings.getInt(\n                [\"server\", \"onlineCheck\", \"interval\"]\n            )\n            connectivityHost = self._settings.get([\"server\", \"onlineCheck\", \"host\"])\n            connectivityPort = self._settings.getInt([\"server\", \"onlineCheck\", \"port\"])\n            connectivityName = self._settings.get([\"server\", \"onlineCheck\", \"name\"])\n\n            if (\n                connectivityChecker.enabled != connectivityEnabled\n                or connectivityChecker.interval != connectivityInterval\n                or connectivityChecker.host != connectivityHost\n                or connectivityChecker.port != connectivityPort\n                or connectivityChecker.name != connectivityName\n            ):\n                connectivityChecker.enabled = connectivityEnabled\n                connectivityChecker.interval = connectivityInterval\n                connectivityChecker.host = connectivityHost\n                connectivityChecker.port = connectivityPort\n                connectivityChecker.name = connectivityName\n                connectivityChecker.check_immediately()\n\n        eventManager.subscribe(events.Events.SETTINGS_UPDATED, on_settings_update)\n\n        components = {\n            \"plugin_manager\": pluginManager,\n            \"printer_profile_manager\": printerProfileManager,\n            \"event_bus\": eventManager,\n            \"analysis_queue\": analysisQueue,\n            \"slicing_manager\": slicingManager,\n            \"file_manager\": fileManager,\n            \"plugin_lifecycle_manager\": pluginLifecycleManager,\n            \"preemptive_cache\": preemptiveCache,\n            \"json_encoder\": jsonEncoder,\n            \"json_decoder\": jsonDecoder,\n            \"connectivity_checker\": connectivityChecker,\n            \"environment_detector\": self._environment_detector,\n            \"system_commands\": systemCommandManager,\n        }\n\n        # ~~ setup access control\n\n        # get additional permissions from plugins\n        self._setup_plugin_permissions()\n\n        # create group manager instance\n        group_manager_factories = pluginManager.get_hooks(\n            \"octoprint.access.groups.factory\"\n        )\n        for name, factory in group_manager_factories.items():\n            try:\n                groupManager = factory(components, self._settings)\n                if groupManager is not None:\n                    self._logger.debug(\n                        f\"Created group manager instance from factory {name}\"\n                    )\n                    break\n            except Exception:\n                self._logger.exception(\n                    \"Error while creating group manager instance from factory {}\".format(\n                        name\n                    )\n                )\n        else:\n            group_manager_name = self._settings.get([\"accessControl\", \"groupManager\"])\n            try:\n                clazz = octoprint.util.get_class(group_manager_name)\n                groupManager = clazz()\n            except AttributeError:\n                self._logger.exception(\n                    \"Could not instantiate group manager {}, \"\n                    \"falling back to FilebasedGroupManager!\".format(group_manager_name)\n                )\n                groupManager = octoprint.access.groups.FilebasedGroupManager()\n        components.update({\"group_manager\": groupManager})\n\n        # create user manager instance\n        user_manager_factories = pluginManager.get_hooks(\n            \"octoprint.users.factory\"\n        )  # legacy, set first so that new wins\n        user_manager_factories.update(\n            pluginManager.get_hooks(\"octoprint.access.users.factory\")\n        )\n        for name, factory in user_manager_factories.items():\n            try:\n                userManager = factory(components, self._settings)\n                if userManager is not None:\n                    self._logger.debug(\n                        f\"Created user manager instance from factory {name}\"\n                    )\n                    break\n            except Exception:\n                self._logger.exception(\n                    \"Error while creating user manager instance from factory {}\".format(\n                        name\n                    ),\n                    extra={\"plugin\": name},\n                )\n        else:\n            user_manager_name = self._settings.get([\"accessControl\", \"userManager\"])\n            try:\n                clazz = octoprint.util.get_class(user_manager_name)\n                userManager = clazz(groupManager)\n            except octoprint.access.users.CorruptUserStorage:\n                raise\n            except Exception:\n                self._logger.exception(\n                    \"Could not instantiate user manager {}, \"\n                    \"falling back to FilebasedUserManager!\".format(user_manager_name)\n                )\n                userManager = octoprint.access.users.FilebasedUserManager(groupManager)\n        components.update({\"user_manager\": userManager})\n\n        # create printer instance\n        printer_factories = pluginManager.get_hooks(\"octoprint.printer.factory\")\n        for name, factory in printer_factories.items():\n            try:\n                printer = factory(components)\n                if printer is not None:\n                    self._logger.debug(f\"Created printer instance from factory {name}\")\n                    break\n            except Exception:\n                self._logger.exception(\n                    f\"Error while creating printer instance from factory {name}\",\n                    extra={\"plugin\": name},\n                )\n        else:\n            printer = Printer(fileManager, analysisQueue, printerProfileManager)\n        components.update({\"printer\": printer})\n\n        from octoprint import (\n            init_custom_events,\n            init_settings_plugin_config_migration_and_cleanup,\n        )\n        from octoprint import octoprint_plugin_inject_factory as opif\n        from octoprint import settings_plugin_inject_factory as spif\n\n        init_custom_events(pluginManager)\n\n        octoprint_plugin_inject_factory = opif(self._settings, components)\n        settings_plugin_inject_factory = spif(self._settings)\n\n        pluginManager.implementation_inject_factories = [\n            octoprint_plugin_inject_factory,\n            settings_plugin_inject_factory,\n        ]\n        pluginManager.initialize_implementations()\n\n        init_settings_plugin_config_migration_and_cleanup(pluginManager)\n\n        pluginManager.log_all_plugins()\n\n        # log environment data now\n        self._environment_detector.log_detected_environment()\n\n        # initialize file manager and register it for changes in the registered plugins\n        fileManager.initialize()\n        pluginLifecycleManager.add_callback(\n            [\"enabled\", \"disabled\"], lambda name, plugin: fileManager.reload_plugins()\n        )\n\n        # initialize slicing manager and register it for changes in the registered plugins\n        slicingManager.initialize()\n        pluginLifecycleManager.add_callback(\n            [\"enabled\", \"disabled\"], lambda name, plugin: slicingManager.reload_slicers()\n        )\n\n        # setup jinja2\n        self._setup_jinja2()\n\n        # setup assets\n        self._setup_assets()\n\n        # configure timelapse\n        octoprint.timelapse.valid_timelapse(\"test\")\n        octoprint.timelapse.configure_timelapse()\n\n        # setup command triggers\n        events.CommandTrigger(printer)\n        if self._debug:\n            events.DebugEventListener()\n\n        # setup login manager\n        self._setup_login_manager()\n\n        # register API blueprint\n        self._setup_blueprints()\n\n        ## Tornado initialization starts here\n\n        ioloop = IOLoop()\n        ioloop.install()\n\n        enable_cors = settings().getBoolean([\"api\", \"allowCrossOrigin\"])\n\n        self._router = SockJSRouter(\n            self._create_socket_connection,\n            \"/sockjs\",\n            session_kls=util.sockjs.ThreadSafeSession,\n            user_settings={\n                \"websocket_allow_origin\": \"*\" if enable_cors else \"\",\n                \"jsessionid\": False,\n                \"sockjs_url\": \"../../static/js/lib/sockjs.min.js\",\n            },\n        )\n\n        upload_suffixes = {\n            \"name\": self._settings.get([\"server\", \"uploads\", \"nameSuffix\"]),\n            \"path\": self._settings.get([\"server\", \"uploads\", \"pathSuffix\"]),\n        }\n\n        def mime_type_guesser(path):\n            from octoprint.filemanager import get_mime_type\n\n            return get_mime_type(path)\n\n        def download_name_generator(path):\n            metadata = fileManager.get_metadata(\"local\", path)\n            if metadata and \"display\" in metadata:\n                return metadata[\"display\"]\n\n        download_handler_kwargs = {\"as_attachment\": True, \"allow_client_caching\": False}\n\n        additional_mime_types = {\"mime_type_guesser\": mime_type_guesser}\n\n        ##~~ Permission validators\n\n        access_validators_from_plugins = []\n        for plugin, hook in pluginManager.get_hooks(\n            \"octoprint.server.http.access_validator\"\n        ).items():\n            try:\n                access_validators_from_plugins.append(\n                    util.tornado.access_validation_factory(app, hook)\n                )\n            except Exception:\n                self._logger.exception(\n                    \"Error while adding tornado access validator from plugin {}\".format(\n                        plugin\n                    ),\n                    extra={\"plugin\": plugin},\n                )\n\n        timelapse_validators = [\n            util.tornado.access_validation_factory(\n                app,\n                util.flask.permission_validator,\n                permissions.Permissions.TIMELAPSE_LIST,\n            ),\n        ] + access_validators_from_plugins\n        download_validators = [\n            util.tornado.access_validation_factory(\n                app,\n                util.flask.permission_validator,\n                permissions.Permissions.FILES_DOWNLOAD,\n            ),\n        ] + access_validators_from_plugins\n        log_validators = [\n            util.tornado.access_validation_factory(\n                app,\n                util.flask.permission_validator,\n                permissions.Permissions.PLUGIN_LOGGING_MANAGE,\n            ),\n        ] + access_validators_from_plugins\n        camera_validators = [\n            util.tornado.access_validation_factory(\n                app, util.flask.permission_validator, permissions.Permissions.WEBCAM\n            ),\n        ] + access_validators_from_plugins\n        systeminfo_validators = [\n            util.tornado.access_validation_factory(\n                app, util.flask.permission_validator, permissions.Permissions.SYSTEM\n            )\n        ] + access_validators_from_plugins\n\n        timelapse_permission_validator = {\n            \"access_validation\": util.tornado.validation_chain(*timelapse_validators)\n        }\n        download_permission_validator = {\n            \"access_validation\": util.tornado.validation_chain(*download_validators)\n        }\n        log_permission_validator = {\n            \"access_validation\": util.tornado.validation_chain(*log_validators)\n        }\n        camera_permission_validator = {\n            \"access_validation\": util.tornado.validation_chain(*camera_validators)\n        }\n        systeminfo_permission_validator = {\n            \"access_validation\": util.tornado.validation_chain(*systeminfo_validators)\n        }\n\n        no_hidden_files_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                lambda path: not octoprint.util.is_hidden_path(path), status_code=404\n            )\n        }\n\n        only_known_types_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                lambda path: octoprint.filemanager.valid_file_type(\n                    os.path.basename(path)\n                ),\n                status_code=404,\n            )\n        }\n\n        valid_timelapse = lambda path: not octoprint.util.is_hidden_path(path) and (\n            octoprint.timelapse.valid_timelapse(path)\n            or octoprint.timelapse.valid_timelapse_thumbnail(path)\n        )\n        timelapse_path_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                valid_timelapse,\n                status_code=404,\n            )\n        }\n        timelapses_path_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                lambda path: valid_timelapse(path)\n                and os.path.realpath(os.path.abspath(path)).startswith(\n                    settings().getBaseFolder(\"timelapse\")\n                ),\n                status_code=400,\n            )\n        }\n\n        valid_log = lambda path: not octoprint.util.is_hidden_path(\n            path\n        ) and path.endswith(\".log\")\n        log_path_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                valid_log,\n                status_code=404,\n            )\n        }\n        logs_path_validator = {\n            \"path_validation\": util.tornado.path_validation_factory(\n                lambda path: valid_log(path)\n                and os.path.realpath(os.path.abspath(path)).startswith(\n                    settings().getBaseFolder(\"logs\")\n                ),\n                status_code=400,\n            )\n        }\n\n        def joined_dict(*dicts):\n            if not len(dicts):\n                return {}\n\n            joined = {}\n            for d in dicts:\n                joined.update(d)\n            return joined\n\n        util.tornado.RequestlessExceptionLoggingMixin.LOG_REQUEST = debug\n        util.tornado.CorsSupportMixin.ENABLE_CORS = enable_cors\n\n        server_routes = self._router.urls + [\n            # various downloads\n            # .mpg and .mp4 timelapses:\n            (\n                r\"/downloads/timelapse/(.*)\",\n                util.tornado.LargeResponseHandler,\n                joined_dict(\n                    {\"path\": self._settings.getBaseFolder(\"timelapse\")},\n                    timelapse_permission_validator,\n                    download_handler_kwargs,\n                    timelapse_path_validator,\n                ),\n            ),\n            # zipped timelapse bundles\n            (\n                r\"/downloads/timelapses\",\n                util.tornado.DynamicZipBundleHandler,\n                joined_dict(\n                    {\n                        \"as_attachment\": True,\n                        \"attachment_name\": \"octoprint-timelapses.zip\",\n                        \"path_processor\": lambda x: (\n                            x,\n                            os.path.join(self._settings.getBaseFolder(\"timelapse\"), x),\n                        ),\n                    },\n                    timelapse_permission_validator,\n                    timelapses_path_validator,\n                ),\n            ),\n            # uploaded printables\n            (\n                r\"/downloads/files/local/(.*)\",\n                util.tornado.LargeResponseHandler,\n                joined_dict(\n                    {\n                        \"path\": self._settings.getBaseFolder(\"uploads\"),\n                        \"as_attachment\": True,\n                        \"name_generator\": download_name_generator,\n                    },\n                    download_permission_validator,\n                    download_handler_kwargs,\n                    no_hidden_files_validator,\n                    only_known_types_validator,\n                    additional_mime_types,\n                ),\n            ),\n            # log files\n            (\n                r\"/downloads/logs/([^/]*)\",\n                util.tornado.LargeResponseHandler,\n                joined_dict(\n                    {\n                        \"path\": self._settings.getBaseFolder(\"logs\"),\n                        \"mime_type_guesser\": lambda *args, **kwargs: \"text/plain\",\n                        \"stream_body\": True,\n                    },\n                    download_handler_kwargs,\n                    log_permission_validator,\n                    log_path_validator,\n                ),\n            ),\n            # zipped log file bundles\n            (\n                r\"/downloads/logs\",\n                util.tornado.DynamicZipBundleHandler,\n                joined_dict(\n                    {\n                        \"as_attachment\": True,\n                        \"attachment_name\": \"octoprint-logs.zip\",\n                        \"path_processor\": lambda x: (\n                            x,\n                            os.path.join(self._settings.getBaseFolder(\"logs\"), x),\n                        ),\n                    },\n                    log_permission_validator,\n                    logs_path_validator,\n                ),\n            ),\n            # system info bundle\n            (\n                r\"/downloads/systeminfo.zip\",\n                util.tornado.SystemInfoBundleHandler,\n                systeminfo_permission_validator,\n            ),\n            # camera snapshot\n            (\n                r\"/downloads/camera/current\",\n                util.tornado.UrlProxyHandler,\n                joined_dict(\n                    {\n                        \"url\": self._settings.get([\"webcam\", \"snapshot\"]),\n                        \"as_attachment\": True,\n                    },\n                    camera_permission_validator,\n                ),\n            ),\n            # generated webassets\n            (\n                r\"/static/webassets/(.*)\",\n                util.tornado.LargeResponseHandler,\n                {\n                    \"path\": os.path.join(\n                        self._settings.getBaseFolder(\"generated\"), \"webassets\"\n                    ),\n                    \"is_pre_compressed\": True,\n                },\n            ),\n            # online indicators - text file with \"online\" as content and a transparent gif\n            (r\"/online.txt\", util.tornado.StaticDataHandler, {\"data\": \"online\\n\"}),\n            (\n                r\"/online.gif\",\n                util.tornado.StaticDataHandler,\n                {\n                    \"data\": bytes(\n                        base64.b64decode(\n                            \"R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"\n                        )\n                    ),\n                    \"content_type\": \"image/gif\",\n                },\n            ),\n            # deprecated endpoints\n            (\n                r\"/api/logs\",\n                util.tornado.DeprecatedEndpointHandler,\n                {\"url\": \"/plugin/logging/logs\"},\n            ),\n            (\n                r\"/api/logs/(.*)\",\n                util.tornado.DeprecatedEndpointHandler,\n                {\"url\": \"/plugin/logging/logs/{0}\"},\n            ),\n        ]\n\n        # fetch additional routes from plugins\n        for name, hook in pluginManager.get_hooks(\"octoprint.server.http.routes\").items():\n            try:\n                result = hook(list(server_routes))\n            except Exception:\n                self._logger.exception(\n                    f\"There was an error while retrieving additional \"\n                    f\"server routes from plugin hook {name}\",\n                    extra={\"plugin\": name},\n                )\n            else:\n                if isinstance(result, (list, tuple)):\n                    for entry in result:\n                        if not isinstance(entry, tuple) or not len(entry) == 3:\n                            continue\n                        if not isinstance(entry[0], str):\n                            continue\n                        if not isinstance(entry[2], dict):\n                            continue\n\n                        route, handler, kwargs = entry\n                        route = r\"/plugin/{name}/{route}\".format(\n                            name=name,\n                            route=route if not route.startswith(\"/\") else route[1:],\n                        )\n\n                        self._logger.debug(\n                            f\"Adding additional route {route} handled by handler {handler} and with additional arguments {kwargs!r}\"\n                        )\n                        server_routes.append((route, handler, kwargs))\n\n        headers = {\n            \"X-Robots-Tag\": \"noindex, nofollow, noimageindex\",\n            \"X-Content-Type-Options\": \"nosniff\",\n        }\n        if not settings().getBoolean([\"server\", \"allowFraming\"]):\n            headers[\"X-Frame-Options\"] = \"sameorigin\"\n\n        removed_headers = [\"Server\"]\n\n        server_routes.append(\n            (\n                r\".*\",\n                util.tornado.UploadStorageFallbackHandler,\n                {\n                    \"fallback\": util.tornado.WsgiInputContainer(\n                        app.wsgi_app, headers=headers, removed_headers=removed_headers\n                    ),\n                    \"file_prefix\": \"octoprint-file-upload-\",\n                    \"file_suffix\": \".tmp\",\n                    \"suffixes\": upload_suffixes,\n                },\n            )\n        )\n\n        transforms = [\n            util.tornado.GlobalHeaderTransform.for_headers(\n                \"OctoPrintGlobalHeaderTransform\",\n                headers=headers,\n                removed_headers=removed_headers,\n            )\n        ]\n\n        self._tornado_app = Application(handlers=server_routes, transforms=transforms)\n        max_body_sizes = [\n            (\n                \"POST\",\n                r\"/api/files/([^/]*)\",\n                self._settings.getInt([\"server\", \"uploads\", \"maxSize\"]),\n            ),\n            (\"POST\", r\"/api/languages\", 5 * 1024 * 1024),\n        ]\n\n        # allow plugins to extend allowed maximum body sizes\n        for name, hook in pluginManager.get_hooks(\n            \"octoprint.server.http.bodysize\"\n        ).items():\n            try:\n                result = hook(list(max_body_sizes))\n            except Exception:\n                self._logger.exception(\n                    f\"There was an error while retrieving additional \"\n                    f\"upload sizes from plugin hook {name}\",\n                    extra={\"plugin\": name},\n                )\n            else:\n                if isinstance(result, (list, tuple)):\n                    for entry in result:\n                        if not isinstance(entry, tuple) or not len(entry) == 3:\n                            continue\n                        if (\n                            entry[0]\n                            not in util.tornado.UploadStorageFallbackHandler.BODY_METHODS\n                        ):\n                            continue\n                        if not isinstance(entry[2], int):\n                            continue\n\n                        method, route, size = entry\n                        route = r\"/plugin/{name}/{route}\".format(\n                            name=name,\n                            route=route if not route.startswith(\"/\") else route[1:],\n                        )\n\n                        self._logger.debug(\n                            f\"Adding maximum body size of {size}B for {method} requests to {route})\"\n                        )\n                        max_body_sizes.append((method, route, size))\n\n        self._stop_intermediary_server()\n\n        # initialize and bind the server\n        trusted_downstream = self._settings.get(\n            [\"server\", \"reverseProxy\", \"trustedDownstream\"]\n        )\n        if not isinstance(trusted_downstream, list):\n            self._logger.warning(\n                \"server.reverseProxy.trustedDownstream is not a list, skipping\"\n            )\n            trusted_downstream = []\n\n        server_kwargs = {\n            \"max_body_sizes\": max_body_sizes,\n            \"default_max_body_size\": self._settings.getInt([\"server\", \"maxSize\"]),\n            \"xheaders\": True,\n            \"trusted_downstream\": trusted_downstream,\n        }\n        if sys.platform == \"win32\":\n            # set 10min idle timeout under windows to hopefully make #2916 less likely\n            server_kwargs.update({\"idle_connection_timeout\": 600})\n\n        self._server = util.tornado.CustomHTTPServer(self._tornado_app, **server_kwargs)\n\n        listening_address = self._host\n        if self._host == \"::\" and not self._v6_only:\n            # special case - tornado only listens on v4 _and_ v6 if we use None as address\n            listening_address = None\n\n        self._server.listen(self._port, address=listening_address)\n\n        ### From now on it's ok to launch subprocesses again\n\n        eventManager.fire(events.Events.STARTUP)\n\n        # analysis backlog\n        fileManager.process_backlog()\n\n        # auto connect\n        if self._settings.getBoolean([\"serial\", \"autoconnect\"]):\n            self._logger.info(\n                \"Autoconnect on startup is configured, trying to connect to the printer...\"\n            )\n            try:\n                (port, baudrate) = (\n                    self._settings.get([\"serial\", \"port\"]),\n                    self._settings.getInt([\"serial\", \"baudrate\"]),\n                )\n                printer_profile = printerProfileManager.get_default()\n                connectionOptions = printer.__class__.get_connection_options()\n                if port in connectionOptions[\"ports\"] or port == \"AUTO\" or port is None:\n                    self._logger.info(\n                        f\"Trying to connect to configured serial port {port}\"\n                    )\n                    printer.connect(\n                        port=port,\n                        baudrate=baudrate,\n                        profile=printer_profile[\"id\"]\n                        if \"id\" in printer_profile\n                        else \"_default\",\n                    )\n                else:\n                    self._logger.info(\n                        \"Could not find configured serial port {} in the system, cannot automatically connect to a non existing printer. Is it plugged in and booted up yet?\"\n                    )\n            except Exception:\n                self._logger.exception(\n                    \"Something went wrong while attempting to automatically connect to the printer\"\n                )\n\n        # start up watchdogs\n        try:\n            watched = self._settings.getBaseFolder(\"watched\")\n            watchdog_handler = util.watchdog.GcodeWatchdogHandler(fileManager, printer)\n            watchdog_handler.initial_scan(watched)\n\n            if self._settings.getBoolean([\"feature\", \"pollWatched\"]):\n                # use less performant polling observer if explicitly configured\n                observer = PollingObserver()\n            else:\n                # use os default\n                observer = Observer()\n\n            observer.schedule(watchdog_handler, watched, recursive=True)\n            observer.start()\n        except Exception:\n            self._logger.exception(\"Error starting watched folder observer\")\n\n        # run our startup plugins\n        octoprint.plugin.call_plugin(\n            octoprint.plugin.StartupPlugin,\n            \"on_startup\",\n            args=(self._host, self._port),\n            sorting_context=\"StartupPlugin.on_startup\",\n        )\n\n        def call_on_startup(name, plugin):\n            implementation = plugin.get_implementation(octoprint.plugin.StartupPlugin)\n            if implementation is None:\n                return\n            implementation.on_startup(self._host, self._port)\n\n        pluginLifecycleManager.add_callback(\"enabled\", call_on_startup)\n\n        # prepare our after startup function\n        def on_after_startup():\n            if self._host == \"::\":\n                if self._v6_only:\n                    # only v6\n                    self._logger.info(f\"Listening on http://[::]:{self._port}\")\n                else:\n                    # all v4 and v6\n                    self._logger.info(\n                        \"Listening on http://0.0.0.0:{port} and http://[::]:{port}\".format(\n                            port=self._port\n                        )\n                    )\n            else:\n                self._logger.info(\n                    \"Listening on http://{}:{}\".format(\n                        self._host if \":\" not in self._host else \"[\" + self._host + \"]\",\n                        self._port,\n                    )\n                )\n\n            if safe_mode and self._settings.getBoolean([\"server\", \"startOnceInSafeMode\"]):\n                self._logger.info(\n                    \"Server started successfully in safe mode as requested from config, removing flag\"\n                )\n                self._settings.setBoolean([\"server\", \"startOnceInSafeMode\"], False)\n                self._settings.save()\n\n            # now this is somewhat ugly, but the issue is the following: startup plugins might want to do things for\n            # which they need the server to be already alive (e.g. for being able to resolve urls, such as favicons\n            # or service xmls or the like). While they are working though the ioloop would block. Therefore we'll\n            # create a single use thread in which to perform our after-startup-tasks, start that and hand back\n            # control to the ioloop\n            def work():\n                octoprint.plugin.call_plugin(\n                    octoprint.plugin.StartupPlugin,\n                    \"on_after_startup\",\n                    sorting_context=\"StartupPlugin.on_after_startup\",\n                )\n\n                def call_on_after_startup(name, plugin):\n                    implementation = plugin.get_implementation(\n                        octoprint.plugin.StartupPlugin\n                    )\n                    if implementation is None:\n                        return\n                    implementation.on_after_startup()\n\n                pluginLifecycleManager.add_callback(\"enabled\", call_on_after_startup)\n\n                # if there was a rogue plugin we wouldn't even have made it here, so remove startup triggered safe mode\n                # flag again...\n                self._settings.setBoolean([\"server\", \"incompleteStartup\"], False)\n                self._settings.save()\n\n                # make a backup of the current config\n                self._settings.backup(ext=\"backup\")\n\n                # when we are through with that we also run our preemptive cache\n                if settings().getBoolean([\"devel\", \"cache\", \"preemptive\"]):\n                    self._execute_preemptive_flask_caching(preemptiveCache)\n\n            import threading\n\n            threading.Thread(target=work).start()\n\n        ioloop.add_callback(on_after_startup)\n\n        # prepare our shutdown function\n        def on_shutdown():\n            # will be called on clean system exit and shutdown the watchdog observer and call the on_shutdown methods\n            # on all registered ShutdownPlugins\n            self._logger.info(\"Shutting down...\")\n            observer.stop()\n            observer.join()\n            eventManager.fire(events.Events.SHUTDOWN)\n\n            self._logger.info(\"Calling on_shutdown on plugins\")\n            octoprint.plugin.call_plugin(\n                octoprint.plugin.ShutdownPlugin,\n                \"on_shutdown\",\n                sorting_context=\"ShutdownPlugin.on_shutdown\",\n            )\n\n            # wait for shutdown event to be processed, but maximally for 15s\n            event_timeout = 15.0\n            if eventManager.join(timeout=event_timeout):\n                self._logger.warning(\n                    \"Event loop was still busy processing after {}s, shutting down anyhow\".format(\n                        event_timeout\n                    )\n                )\n\n            if self._octoprint_daemon is not None:\n                self._logger.info(\"Cleaning up daemon pidfile\")\n                self._octoprint_daemon.terminated()\n\n            self._logger.info(\"Goodbye!\")\n\n        atexit.register(on_shutdown)\n\n        def sigterm_handler(*args, **kwargs):\n            # will stop tornado on SIGTERM, making the program exit cleanly\n            def shutdown_tornado():\n                self._logger.debug(\"Shutting down tornado's IOLoop...\")\n                ioloop.stop()\n\n            self._logger.debug(\"SIGTERM received...\")\n            ioloop.add_callback_from_signal(shutdown_tornado)\n\n        signal.signal(signal.SIGTERM, sigterm_handler)\n\n        try:\n            # this is the main loop - as long as tornado is running, OctoPrint is running\n            ioloop.start()\n            self._logger.debug(\"Tornado's IOLoop stopped\")\n        except (KeyboardInterrupt, SystemExit):\n            pass\n        except Exception:\n            self._logger.fatal(\n                \"Now that is embarrassing... Something really really went wrong here. Please report this including the stacktrace below in OctoPrint's bugtracker. Thanks!\"\n            )\n            self._logger.exception(\"Stacktrace follows:\")\n\n    def _log_safe_mode_start(self, self_mode):\n        self_mode_file = os.path.join(\n            self._settings.getBaseFolder(\"data\"), \"last_safe_mode\"\n        )\n        try:\n            with open(self_mode_file, \"w+\", encoding=\"utf-8\") as f:\n                f.write(self_mode)\n        except Exception as ex:\n            self._logger.warn(f\"Could not write safe mode file {self_mode_file}: {ex}\")\n\n    def _create_socket_connection(self, session):\n        global printer, fileManager, analysisQueue, userManager, eventManager, connectivityChecker\n        return util.sockjs.PrinterStateConnection(\n            printer,\n            fileManager,\n            analysisQueue,\n            userManager,\n            groupManager,\n            eventManager,\n            pluginManager,\n            connectivityChecker,\n            session,\n        )\n\n    def _check_for_root(self):\n        if \"geteuid\" in dir(os) and os.geteuid() == 0:\n            exit(\"You should not run OctoPrint as root!\")\n\n    def _get_locale(self):\n        global LANGUAGES\n\n        if \"l10n\" in request.values:\n            return Locale.negotiate([request.values[\"l10n\"]], LANGUAGES)\n\n        if \"X-Locale\" in request.headers:\n            return Locale.negotiate([request.headers[\"X-Locale\"]], LANGUAGES)\n\n        if hasattr(g, \"identity\") and g.identity:\n            userid = g.identity.id\n            try:\n                user_language = userManager.get_user_setting(\n                    userid, (\"interface\", \"language\")\n                )\n                if user_language is not None and not user_language == \"_default\":\n                    return Locale.negotiate([user_language], LANGUAGES)\n            except octoprint.access.users.UnknownUser:\n                pass\n\n        default_language = self._settings.get([\"appearance\", \"defaultLanguage\"])\n        if (\n            default_language is not None\n            and not default_language == \"_default\"\n            and default_language in LANGUAGES\n        ):\n            return Locale.negotiate([default_language], LANGUAGES)\n\n        return Locale.parse(request.accept_languages.best_match(LANGUAGES))\n\n    def _setup_heartbeat_logging(self):\n        logger = logging.getLogger(__name__ + \".heartbeat\")\n\n        def log_heartbeat():\n            logger.info(\"Server heartbeat <3\")\n\n        interval = settings().getFloat([\"server\", \"heartbeat\"])\n        logger.info(f\"Starting server heartbeat, {interval}s interval\")\n\n        timer = octoprint.util.RepeatedTimer(interval, log_heartbeat)\n        timer.start()\n\n    def _setup_app(self, app):\n        global limiter\n\n        from octoprint.server.util.flask import (\n            OctoPrintFlaskRequest,\n            OctoPrintFlaskResponse,\n            OctoPrintJsonEncoder,\n            OctoPrintSessionInterface,\n            PrefixAwareJinjaEnvironment,\n            ReverseProxiedEnvironment,\n        )\n\n        # we must set this here because setting app.debug will access app.jinja_env\n        app.jinja_environment = PrefixAwareJinjaEnvironment\n\n        app.config[\"TEMPLATES_AUTO_RELOAD\"] = True\n        app.config[\"JSONIFY_PRETTYPRINT_REGULAR\"] = False\n        app.config[\"REMEMBER_COOKIE_HTTPONLY\"] = True\n\n        # we must not set this before TEMPLATES_AUTO_RELOAD is set to True or that won't take\n        app.debug = self._debug\n\n        # setup octoprint's flask json serialization/deserialization\n        app.json_encoder = OctoPrintJsonEncoder\n\n        s = settings()\n\n        secret_key = s.get([\"server\", \"secretKey\"])\n        if not secret_key:\n            import string\n            from random import choice\n\n            chars = string.ascii_lowercase + string.ascii_uppercase + string.digits\n            secret_key = \"\".join(choice(chars) for _ in range(32))\n            s.set([\"server\", \"secretKey\"], secret_key)\n            s.save()\n\n        app.secret_key = secret_key\n\n        reverse_proxied = ReverseProxiedEnvironment(\n            header_prefix=s.get([\"server\", \"reverseProxy\", \"prefixHeader\"]),\n            header_scheme=s.get([\"server\", \"reverseProxy\", \"schemeHeader\"]),\n            header_host=s.get([\"server\", \"reverseProxy\", \"hostHeader\"]),\n            header_server=s.get([\"server\", \"reverseProxy\", \"serverHeader\"]),\n            header_port=s.get([\"server\", \"reverseProxy\", \"portHeader\"]),\n            prefix=s.get([\"server\", \"reverseProxy\", \"prefixFallback\"]),\n            scheme=s.get([\"server\", \"reverseProxy\", \"schemeFallback\"]),\n            host=s.get([\"server\", \"reverseProxy\", \"hostFallback\"]),\n            server=s.get([\"server\", \"reverseProxy\", \"serverFallback\"]),\n            port=s.get([\"server\", \"reverseProxy\", \"portFallback\"]),\n        )\n\n        OctoPrintFlaskRequest.environment_wrapper = reverse_proxied\n        app.request_class = OctoPrintFlaskRequest\n        app.response_class = OctoPrintFlaskResponse\n        app.session_interface = OctoPrintSessionInterface()\n\n        @app.before_request\n        def before_request():\n            g.locale = self._get_locale()\n\n            # used for performance measurement\n            g.start_time = time.monotonic()\n\n            if self._debug and \"perfprofile\" in request.args:\n                try:\n                    from pyinstrument import Profiler\n\n                    g.perfprofiler = Profiler()\n                    g.perfprofiler.start()\n                except ImportError:\n                    # profiler dependency not installed, ignore\n                    pass\n\n        @app.after_request\n        def after_request(response):\n            # send no-cache headers with all POST responses\n            if request.method == \"POST\":\n                response.cache_control.no_cache = True\n\n            response.headers.add(\"X-Clacks-Overhead\", \"GNU Terry Pratchett\")\n\n            if hasattr(g, \"perfprofiler\"):\n                g.perfprofiler.stop()\n                output_html = g.perfprofiler.output_html()\n                return make_response(output_html)\n\n            if hasattr(g, \"start_time\"):\n                end_time = time.monotonic()\n                duration_ms = int((end_time - g.start_time) * 1000)\n                response.headers.add(\"Server-Timing\", f\"app;dur={duration_ms}\")\n\n            return response\n\n        from octoprint.util.jinja import MarkdownFilter\n\n        MarkdownFilter(app)\n\n        from flask_limiter import Limiter\n        from flask_limiter.util import get_remote_address\n\n        app.config[\"RATELIMIT_STRATEGY\"] = \"fixed-window-elastic-expiry\"\n\n        limiter = Limiter(app, key_func=get_remote_address)\n\n    def _setup_i18n(self, app):\n        global babel\n        global LOCALES\n        global LANGUAGES\n\n        babel = Babel(app)\n\n        def get_available_locale_identifiers(locales):\n            result = set()\n\n            # add available translations\n            for locale in locales:\n                result.add(locale.language)\n                if locale.territory:\n                    # if a territory is specified, add that too\n                    result.add(f\"{locale.language}_{locale.territory}\")\n\n            return result\n\n        LOCALES = babel.list_translations()\n        LANGUAGES = get_available_locale_identifiers(LOCALES)\n\n        @babel.localeselector\n        def get_locale():\n            return self._get_locale()\n\n    def _setup_jinja2(self):\n        import re\n\n        app.jinja_env.add_extension(\"jinja2.ext.do\")\n        app.jinja_env.add_extension(\"octoprint.util.jinja.trycatch\")\n\n        def regex_replace(s, find, replace):\n            return re.sub(find, replace, s)\n\n        html_header_regex = re.compile(\n            r\"<h(?P<number>[1-6])>(?P<content>.*?)</h(?P=number)>\"\n        )\n\n        def offset_html_headers(s, offset):\n            def repl(match):\n                number = int(match.group(\"number\"))\n                number += offset\n                if number > 6:\n                    number = 6\n                elif number < 1:\n                    number = 1\n                return \"<h{number}>{content}</h{number}>\".format(\n                    number=number, content=match.group(\"content\")\n                )\n\n            return html_header_regex.sub(repl, s)\n\n        markdown_header_regex = re.compile(\n            r\"^(?P<hashs>#+)\\s+(?P<content>.*)$\", flags=re.MULTILINE\n        )\n\n        def offset_markdown_headers(s, offset):\n            def repl(match):\n                number = len(match.group(\"hashs\"))\n                number += offset\n                if number > 6:\n                    number = 6\n                elif number < 1:\n                    number = 1\n                return \"{hashs} {content}\".format(\n                    hashs=\"#\" * number, content=match.group(\"content\")\n                )\n\n            return markdown_header_regex.sub(repl, s)\n\n        html_link_regex = re.compile(r\"<(?P<tag>a.*?)>(?P<content>.*?)</a>\")\n\n        def externalize_links(text):\n            def repl(match):\n                tag = match.group(\"tag\")\n                if \"href\" not in tag:\n                    return match.group(0)\n\n                if \"target=\" not in tag and \"rel=\" not in tag:\n                    tag += ' target=\"_blank\" rel=\"noreferrer noopener\"'\n\n                content = match.group(\"content\")\n                return f\"<{tag}>{content}</a>\"\n\n            return html_link_regex.sub(repl, text)\n\n        single_quote_regex = re.compile(\"(?<!\\\\\\\\)'\")\n\n        def escape_single_quote(text):\n            return single_quote_regex.sub(\"\\\\'\", text)\n\n        double_quote_regex = re.compile('(?<!\\\\\\\\)\"')\n\n        def escape_double_quote(text):\n            return double_quote_regex.sub('\\\\\"', text)\n\n        app.jinja_env.filters[\"regex_replace\"] = regex_replace\n        app.jinja_env.filters[\"offset_html_headers\"] = offset_html_headers\n        app.jinja_env.filters[\"offset_markdown_headers\"] = offset_markdown_headers\n        app.jinja_env.filters[\"externalize_links\"] = externalize_links\n        app.jinja_env.filters[\"escape_single_quote\"] = app.jinja_env.filters[\n            \"esq\"\n        ] = escape_single_quote\n        app.jinja_env.filters[\"escape_double_quote\"] = app.jinja_env.filters[\n            \"edq\"\n        ] = escape_double_quote\n\n        # configure additional template folders for jinja2\n        import jinja2\n\n        import octoprint.util.jinja\n\n        app.jinja_env.prefix_loader = jinja2.PrefixLoader({})\n\n        loaders = [app.jinja_loader, app.jinja_env.prefix_loader]\n        if octoprint.util.is_running_from_source():\n            root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../..\"))\n            allowed = [\"AUTHORS.md\", \"SUPPORTERS.md\", \"THIRDPARTYLICENSES.md\"]\n            files = {\"_data/\" + name: os.path.join(root, name) for name in allowed}\n            loaders.append(octoprint.util.jinja.SelectedFilesWithConversionLoader(files))\n\n        # TODO: Remove this in 2.0.0\n        warning_message = \"Loading plugin template '{template}' from '{filename}' without plugin prefix, this is deprecated and will soon no longer be supported.\"\n        loaders.append(\n            octoprint.util.jinja.WarningLoader(\n                octoprint.util.jinja.PrefixChoiceLoader(app.jinja_env.prefix_loader),\n                warning_message,\n            )\n        )\n\n        app.jinja_loader = jinja2.ChoiceLoader(loaders)\n\n        self._register_template_plugins()\n\n        # make sure plugin lifecycle events relevant for jinja2 are taken care of\n        def template_enabled(name, plugin):\n            if plugin.implementation is None or not isinstance(\n                plugin.implementation, octoprint.plugin.TemplatePlugin\n            ):\n                return\n            self._register_additional_template_plugin(plugin.implementation)\n\n        def template_disabled(name, plugin):\n            if plugin.implementation is None or not isinstance(\n                plugin.implementation, octoprint.plugin.TemplatePlugin\n            ):\n                return\n            self._unregister_additional_template_plugin(plugin.implementation)\n\n        pluginLifecycleManager.add_callback(\"enabled\", template_enabled)\n        pluginLifecycleManager.add_callback(\"disabled\", template_disabled)\n\n    def _execute_preemptive_flask_caching(self, preemptive_cache):\n        import time\n\n        from werkzeug.test import EnvironBuilder\n\n        # we clean up entries from our preemptive cache settings that haven't been\n        # accessed longer than server.preemptiveCache.until days\n        preemptive_cache_timeout = settings().getInt(\n            [\"server\", \"preemptiveCache\", \"until\"]\n        )\n        cutoff_timestamp = time.time() - preemptive_cache_timeout * 24 * 60 * 60\n\n        def filter_current_entries(entry):\n            \"\"\"Returns True for entries younger than the cutoff date\"\"\"\n            return \"_timestamp\" in entry and entry[\"_timestamp\"] > cutoff_timestamp\n\n        def filter_http_entries(entry):\n            \"\"\"Returns True for entries targeting http or https.\"\"\"\n            return (\n                \"base_url\" in entry\n                and entry[\"base_url\"]\n                and (\n                    entry[\"base_url\"].startswith(\"http://\")\n                    or entry[\"base_url\"].startswith(\"https://\")\n                )\n            )\n\n        def filter_entries(entry):\n            \"\"\"Combined filter.\"\"\"\n            filters = (filter_current_entries, filter_http_entries)\n            return all([f(entry) for f in filters])\n\n        # filter out all old and non-http entries\n        cache_data = preemptive_cache.clean_all_data(\n            lambda root, entries: list(filter(filter_entries, entries))\n        )\n        if not cache_data:\n            return\n\n        def execute_caching():\n            logger = logging.getLogger(__name__ + \".preemptive_cache\")\n\n            for route in sorted(cache_data.keys(), key=lambda x: (x.count(\"/\"), x)):\n                entries = reversed(\n                    sorted(cache_data[route], key=lambda x: x.get(\"_count\", 0))\n                )\n                for kwargs in entries:\n                    plugin = kwargs.get(\"plugin\", None)\n                    if plugin:\n                        try:\n                            plugin_info = pluginManager.get_plugin_info(\n                                plugin, require_enabled=True\n                            )\n                            if plugin_info is None:\n                                logger.info(\n                                    \"About to preemptively cache plugin {} but it is not installed or enabled, preemptive caching makes no sense\".format(\n                                        plugin\n                                    )\n                                )\n                                continue\n\n                            implementation = plugin_info.implementation\n                            if implementation is None or not isinstance(\n                                implementation, octoprint.plugin.UiPlugin\n                            ):\n                                logger.info(\n                                    \"About to preemptively cache plugin {} but it is not a UiPlugin, preemptive caching makes no sense\".format(\n                                        plugin\n                                    )\n                                )\n                                continue\n                            if not implementation.get_ui_preemptive_caching_enabled():\n                                logger.info(\n                                    \"About to preemptively cache plugin {} but it has disabled preemptive caching\".format(\n                                        plugin\n                                    )\n                                )\n                                continue\n                        except Exception:\n                            logger.exception(\n                                \"Error while trying to check if plugin {} has preemptive caching enabled, skipping entry\"\n                            )\n                            continue\n\n                    additional_request_data = kwargs.get(\"_additional_request_data\", {})\n                    kwargs = {\n                        k: v\n                        for k, v in kwargs.items()\n                        if not k.startswith(\"_\") and not k == \"plugin\"\n                    }\n                    kwargs.update(additional_request_data)\n\n                    try:\n                        start = time.monotonic()\n                        if plugin:\n                            logger.info(\n                                \"Preemptively caching {} (ui {}) for {!r}\".format(\n                                    route, plugin, kwargs\n                                )\n                            )\n                        else:\n                            logger.info(\n                                \"Preemptively caching {} (ui _default) for {!r}\".format(\n                                    route, kwargs\n                                )\n                            )\n\n                        headers = kwargs.get(\"headers\", {})\n                        headers[\"X-Force-View\"] = plugin if plugin else \"_default\"\n                        headers[\"X-Preemptive-Recording\"] = \"yes\"\n                        kwargs[\"headers\"] = headers\n\n                        builder = EnvironBuilder(**kwargs)\n                        app(builder.get_environ(), lambda *a, **kw: None)\n\n                        logger.info(f\"... done in {time.monotonic() - start:.2f}s\")\n                    except Exception:\n                        logger.exception(\n                            \"Error while trying to preemptively cache {} for {!r}\".format(\n                                route, kwargs\n                            )\n                        )\n\n        # asynchronous caching\n        import threading\n\n        cache_thread = threading.Thread(\n            target=execute_caching, name=\"Preemptive Cache Worker\"\n        )\n        cache_thread.daemon = True\n        cache_thread.start()\n\n    def _register_template_plugins(self):\n        template_plugins = pluginManager.get_implementations(\n            octoprint.plugin.TemplatePlugin\n        )\n        for plugin in template_plugins:\n            try:\n                self._register_additional_template_plugin(plugin)\n            except Exception:\n                self._logger.exception(\n                    \"Error while trying to register templates of plugin {}, ignoring it\".format(\n                        plugin._identifier\n                    )\n                )\n\n    def _register_additional_template_plugin(self, plugin):\n        import octoprint.util.jinja\n\n        folder = plugin.get_template_folder()\n        if (\n            folder is not None\n            and plugin.template_folder_key not in app.jinja_env.prefix_loader.mapping\n        ):\n            loader = octoprint.util.jinja.FilteredFileSystemLoader(\n                [plugin.get_template_folder()],\n                path_filter=lambda x: not octoprint.util.is_hidden_path(x),\n            )\n\n            app.jinja_env.prefix_loader.mapping[plugin.template_folder_key] = loader\n\n    def _unregister_additional_template_plugin(self, plugin):\n        folder = plugin.get_template_folder()\n        if (\n            folder is not None\n            and plugin.template_folder_key in app.jinja_env.prefix_loader.mapping\n        ):\n            del app.jinja_env.prefix_loader.mapping[plugin.template_folder_key]\n\n    def _setup_blueprints(self):\n        # do not remove or the index view won't be found\n        import octoprint.server.views  # noqa: F401\n        from octoprint.server.api import api\n        from octoprint.server.util.flask import make_api_error\n\n        blueprints = [api]\n        api_endpoints = [\"/api\"]\n        registrators = [functools.partial(app.register_blueprint, api, url_prefix=\"/api\")]\n\n        # also register any blueprints defined in BlueprintPlugins\n        (\n            blueprints_from_plugins,\n            api_endpoints_from_plugins,\n            registrators_from_plugins,\n        ) = self._prepare_blueprint_plugins()\n        blueprints += blueprints_from_plugins\n        api_endpoints += api_endpoints_from_plugins\n        registrators += registrators_from_plugins\n\n        # and register a blueprint for serving the static files of asset plugins which are not blueprint plugins themselves\n        (blueprints_from_assets, registrators_from_assets) = self._prepare_asset_plugins()\n        blueprints += blueprints_from_assets\n        registrators += registrators_from_assets\n\n        # make sure all before/after_request hook results are attached as well\n        self._add_plugin_request_handlers_to_blueprints(*blueprints)\n\n        # register everything with the system\n        for registrator in registrators:\n            registrator()\n\n        @app.errorhandler(HTTPException)\n        def _handle_api_error(ex):\n            if any(map(lambda x: request.path.startswith(x), api_endpoints)):\n                return make_api_error(ex.description, ex.code)\n            else:\n                return ex\n\n    def _prepare_blueprint_plugins(self):\n        def register_plugin_blueprint(plugin, blueprint, url_prefix):\n            try:\n                app.register_blueprint(\n                    blueprint, url_prefix=url_prefix, name_prefix=\"plugin\"\n                )\n                self._logger.debug(\n                    f\"Registered API of plugin {plugin} under URL prefix {url_prefix}\"\n                )\n            except Exception:\n                self._logger.exception(\n                    f\"Error while registering blueprint of plugin {plugin}, ignoring it\",\n                    extra={\"plugin\": plugin},\n                )\n\n        blueprints = []\n        api_endpoints = []\n        registrators = []\n\n        blueprint_plugins = octoprint.plugin.plugin_manager().get_implementations(\n            octoprint.plugin.BlueprintPlugin\n        )\n        for plugin in blueprint_plugins:\n            blueprint, prefix = self._prepare_blueprint_plugin(plugin)\n\n            blueprints.append(blueprint)\n            api_endpoints += map(\n                lambda x: prefix + x, plugin.get_blueprint_api_prefixes()\n            )\n            registrators.append(\n                functools.partial(\n                    register_plugin_blueprint, plugin._identifier, blueprint, prefix\n                )\n            )\n\n        return blueprints, api_endpoints, registrators\n\n    def _prepare_asset_plugins(self):\n        def register_asset_blueprint(plugin, blueprint, url_prefix):\n            try:\n                app.register_blueprint(\n                    blueprint, url_prefix=url_prefix, name_prefix=\"plugin\"\n                )\n                self._logger.debug(\n                    f\"Registered assets of plugin {plugin} under URL prefix {url_prefix}\"\n                )\n            except Exception:\n                self._logger.exception(\n                    f\"Error while registering blueprint of plugin {plugin}, ignoring it\",\n                    extra={\"plugin\": plugin},\n                )\n\n        blueprints = []\n        registrators = []\n\n        asset_plugins = octoprint.plugin.plugin_manager().get_implementations(\n            octoprint.plugin.AssetPlugin\n        )\n        for plugin in asset_plugins:\n            if isinstance(plugin, octoprint.plugin.BlueprintPlugin):\n                continue\n            blueprint, prefix = self._prepare_asset_plugin(plugin)\n\n            blueprints.append(blueprint)\n            registrators.append(\n                functools.partial(\n                    register_asset_blueprint, plugin._identifier, blueprint, prefix\n                )\n            )\n\n        return blueprints, registrators\n\n    def _prepare_blueprint_plugin(self, plugin):\n        name = plugin._identifier\n        blueprint = plugin.get_blueprint()\n        if blueprint is None:\n            return\n\n        blueprint.before_request(corsRequestHandler)\n        blueprint.before_request(loginFromApiKeyRequestHandler)\n        blueprint.after_request(corsResponseHandler)\n\n        if plugin.is_blueprint_protected():\n            blueprint.before_request(requireLoginRequestHandler)\n\n        url_prefix = f\"/plugin/{name}\"\n        return blueprint, url_prefix\n\n    def _prepare_asset_plugin(self, plugin):\n        name = plugin._identifier\n\n        url_prefix = f\"/plugin/{name}\"\n        blueprint = Blueprint(name, name, static_folder=plugin.get_asset_folder())\n        return blueprint, url_prefix\n\n    def _add_plugin_request_handlers_to_blueprints(self, *blueprints):\n        before_hooks = octoprint.plugin.plugin_manager().get_hooks(\n            \"octoprint.server.api.before_request\"\n        )\n        after_hooks = octoprint.plugin.plugin_manager().get_hooks(\n            \"octoprint.server.api.after_request\"\n        )\n\n        for name, hook in before_hooks.items():\n            plugin = octoprint.plugin.plugin_manager().get_plugin(name)\n            for blueprint in blueprints:\n                try:\n                    result = hook(plugin=plugin)\n                    if isinstance(result, (list, tuple)):\n                        for h in result:\n                            blueprint.before_request(h)\n                except Exception:\n                    self._logger.exception(\n                        \"Error processing before_request hooks from plugin {}\".format(\n                            plugin\n                        ),\n                        extra={\"plugin\": name},\n                    )\n\n        for name, hook in after_hooks.items():\n            plugin = octoprint.plugin.plugin_manager().get_plugin(name)\n            for blueprint in blueprints:\n                try:\n                    result = hook(plugin=plugin)\n                    if isinstance(result, (list, tuple)):\n                        for h in result:\n                            blueprint.after_request(h)\n                except Exception:\n                    self._logger.exception(\n                        \"Error processing after_request hooks from plugin {}\".format(\n                            plugin\n                        ),\n                        extra={\"plugin\": name},\n                    )\n\n    def _setup_mimetypes(self):\n        # Safety measures for Windows... apparently the mimetypes module takes its translation from the windows\n        # registry, and if for some weird reason that gets borked the reported MIME types can be all over the place.\n        # Since at least in Chrome that can cause hilarious issues with JS files (refusal to run them and thus a\n        # borked UI) we make sure that .js always maps to the correct application/javascript, and also throw in a\n        # .css -> text/css for good measure.\n        #\n        # See #3367\n        mimetypes.add_type(\"application/javascript\", \".js\")\n        mimetypes.add_type(\"text/css\", \".css\")\n\n    def _setup_assets(self):\n        global app\n        global assets\n        global pluginManager\n\n        from octoprint.server.util.webassets import MemoryManifest  # noqa: F401\n\n        util.flask.fix_webassets_filtertool()\n\n        base_folder = self._settings.getBaseFolder(\"generated\")\n\n        # clean the folder\n        if self._settings.getBoolean([\"devel\", \"webassets\", \"clean_on_startup\"]):\n            import errno\n            import shutil\n\n            for entry, recreate in (\n                (\"webassets\", True),\n                # no longer used, but clean up just in case\n                (\".webassets-cache\", False),\n                (\".webassets-manifest.json\", False),\n            ):\n                path = os.path.join(base_folder, entry)\n\n                # delete path if it exists\n                if os.path.exists(path):\n                    try:\n                        self._logger.debug(f\"Deleting {path}...\")\n                        if os.path.isdir(path):\n                            shutil.rmtree(path)\n                        else:\n                            os.remove(path)\n                    except Exception:\n                        self._logger.exception(\n                            f\"Error while trying to delete {path}, \" f\"leaving it alone\"\n                        )\n                        continue\n\n                # re-create path if necessary\n                if recreate:\n                    self._logger.debug(f\"Creating {path}...\")\n                    error_text = (\n                        f\"Error while trying to re-create {path}, that might cause \"\n                        f\"errors with the webassets cache\"\n                    )\n                    try:\n                        os.makedirs(path)\n                    except OSError as e:\n                        if e.errno == errno.EACCES:\n                            # that might be caused by the user still having the folder open somewhere, let's try again after\n                            # waiting a bit\n                            import time\n\n                            for n in range(3):\n                                time.sleep(0.5)\n                                self._logger.debug(\n                                    \"Creating {path}: Retry #{retry} after {time}s\".format(\n                                        path=path, retry=n + 1, time=(n + 1) * 0.5\n                                    )\n                                )\n                                try:\n                                    os.makedirs(path)\n                                    break\n                                except Exception:\n                                    if self._logger.isEnabledFor(logging.DEBUG):\n                                        self._logger.exception(\n                                            f\"Ignored error while creating \"\n                                            f\"directory {path}\"\n                                        )\n                                    pass\n                            else:\n                                # this will only get executed if we never did\n                                # successfully execute makedirs above\n                                self._logger.exception(error_text)\n                                continue\n                        else:\n                            # not an access error, so something we don't understand\n                            # went wrong -> log an error and stop\n                            self._logger.exception(error_text)\n                            continue\n                    except Exception:\n                        # not an OSError, so something we don't understand\n                        # went wrong -> log an error and stop\n                        self._logger.exception(error_text)\n                        continue\n\n                self._logger.info(f\"Reset webasset folder {path}...\")\n\n        AdjustedEnvironment = type(Environment)(\n            Environment.__name__,\n            (Environment,),\n            {\"resolver_class\": util.flask.PluginAssetResolver},\n        )\n\n        class CustomDirectoryEnvironment(AdjustedEnvironment):\n            @property\n            def directory(self):\n                return base_folder\n\n        assets = CustomDirectoryEnvironment(app)\n        assets.debug = not self._settings.getBoolean([\"devel\", \"webassets\", \"bundle\"])\n\n        # we should rarely if ever regenerate the webassets in production and can wait a\n        # few seconds for regeneration in development, if it means we can get rid of\n        # a whole monkey patch and in internal use of pickle with non-tamperproof files\n        assets.cache = False\n        assets.manifest = \"memory\"\n\n        UpdaterType = type(util.flask.SettingsCheckUpdater)(\n            util.flask.SettingsCheckUpdater.__name__,\n            (util.flask.SettingsCheckUpdater,),\n            {\"updater\": assets.updater},\n        )\n        assets.updater = UpdaterType\n\n        preferred_stylesheet = self._settings.get([\"devel\", \"stylesheet\"])\n\n        dynamic_core_assets = util.flask.collect_core_assets()\n        dynamic_plugin_assets = util.flask.collect_plugin_assets(\n            preferred_stylesheet=preferred_stylesheet\n        )\n\n        js_libs = [\n            \"js/lib/babel-polyfill.min.js\",\n            \"js/lib/jquery/jquery.min.js\",\n            \"js/lib/modernizr.custom.js\",\n            \"js/lib/lodash.min.js\",\n            \"js/lib/sprintf.min.js\",\n            \"js/lib/knockout.js\",\n            \"js/lib/knockout.mapping-latest.js\",\n            \"js/lib/babel.js\",\n            \"js/lib/bootstrap/bootstrap.js\",\n            \"js/lib/bootstrap/bootstrap-modalmanager.js\",\n            \"js/lib/bootstrap/bootstrap-modal.js\",\n            \"js/lib/bootstrap/bootstrap-slider.js\",\n            \"js/lib/bootstrap/bootstrap-tabdrop.js\",\n            \"js/lib/jquery/jquery-ui.js\",\n            \"js/lib/jquery/jquery.flot.js\",\n            \"js/lib/jquery/jquery.flot.time.js\",\n            \"js/lib/jquery/jquery.flot.crosshair.js\",\n            \"js/lib/jquery/jquery.flot.resize.js\",\n            \"js/lib/jquery/jquery.iframe-transport.js\",\n            \"js/lib/jquery/jquery.fileupload.js\",\n            \"js/lib/jquery/jquery.slimscroll.min.js\",\n            \"js/lib/jquery/jquery.qrcode.min.js\",\n            \"js/lib/jquery/jquery.bootstrap.wizard.js\",\n            \"js/lib/pnotify/pnotify.core.min.js\",\n            \"js/lib/pnotify/pnotify.buttons.min.js\",\n            \"js/lib/pnotify/pnotify.callbacks.min.js\",\n            \"js/lib/pnotify/pnotify.confirm.min.js\",\n            \"js/lib/pnotify/pnotify.desktop.min.js\",\n            \"js/lib/pnotify/pnotify.history.min.js\",\n            \"js/lib/pnotify/pnotify.mobile.min.js\",\n            \"js/lib/pnotify/pnotify.nonblock.min.js\",\n            \"js/lib/pnotify/pnotify.reference.min.js\",\n            \"js/lib/pnotify/pnotify.tooltip.min.js\",\n            \"js/lib/pnotify/pnotify.maxheight.js\",\n            \"js/lib/moment-with-locales.min.js\",\n            \"js/lib/pusher.color.min.js\",\n            \"js/lib/detectmobilebrowser.js\",\n            \"js/lib/ua-parser.min.js\",\n            \"js/lib/md5.min.js\",\n            \"js/lib/bootstrap-slider-knockout-binding.js\",\n            \"js/lib/loglevel.min.js\",\n            \"js/lib/sockjs.min.js\",\n            \"js/lib/hls.js\",\n            \"js/lib/less.js\",\n        ]\n\n        css_libs = [\n            \"css/bootstrap.min.css\",\n            \"css/bootstrap-modal.css\",\n            \"css/bootstrap-slider.css\",\n            \"css/bootstrap-tabdrop.css\",\n            \"vendor/font-awesome-3.2.1/css/font-awesome.min.css\",\n            \"vendor/font-awesome-5.15.1/css/all.min.css\",\n            \"vendor/font-awesome-5.15.1/css/v4-shims.min.css\",\n            \"css/jquery.fileupload-ui.css\",\n            \"css/pnotify.core.min.css\",\n            \"css/pnotify.buttons.min.css\",\n            \"css/pnotify.history.min.css\",\n        ]\n\n        # a couple of custom filters\n        from webassets.filter import register_filter\n\n        from octoprint.server.util.webassets import (\n            GzipFile,\n            JsDelimiterBundler,\n            JsPluginBundle,\n            LessImportRewrite,\n            RJSMinExtended,\n            SourceMapRemove,\n            SourceMapRewrite,\n        )\n\n        register_filter(LessImportRewrite)\n        register_filter(SourceMapRewrite)\n        register_filter(SourceMapRemove)\n        register_filter(JsDelimiterBundler)\n        register_filter(GzipFile)\n        register_filter(RJSMinExtended)\n\n        def all_assets_for_plugins(collection):\n            \"\"\"Gets all plugin assets for a dict of plugin->assets\"\"\"\n            result = []\n            for assets in collection.values():\n                result += assets\n            return result\n\n        # -- JS --------------------------------------------------------------------------------------------------------\n\n        filters = [\"sourcemap_remove\"]\n        if self._settings.getBoolean([\"devel\", \"webassets\", \"minify\"]):\n            filters += [\"rjsmin_extended\"]\n        filters += [\"js_delimiter_bundler\", \"gzip\"]\n\n        js_filters = filters\n        if self._settings.getBoolean([\"devel\", \"webassets\", \"minify_plugins\"]):\n            js_plugin_filters = js_filters\n        else:\n            js_plugin_filters = [x for x in js_filters if x not in (\"rjsmin_extended\",)]\n\n        def js_bundles_for_plugins(collection, filters=None):\n            \"\"\"Produces JsPluginBundle instances that output IIFE wrapped assets\"\"\"\n            result = OrderedDict()\n            for plugin, assets in collection.items():\n                if len(assets):\n                    result[plugin] = JsPluginBundle(plugin, *assets, filters=filters)\n            return result\n\n        js_core = (\n            dynamic_core_assets[\"js\"]\n            + all_assets_for_plugins(dynamic_plugin_assets[\"bundled\"][\"js\"])\n            + [\"js/app/dataupdater.js\", \"js/app/helpers.js\", \"js/app/main.js\"]\n        )\n        js_plugins = js_bundles_for_plugins(\n            dynamic_plugin_assets[\"external\"][\"js\"], filters=\"js_delimiter_bundler\"\n        )\n\n        clientjs_core = dynamic_core_assets[\"clientjs\"] + all_assets_for_plugins(\n            dynamic_plugin_assets[\"bundled\"][\"clientjs\"]\n        )\n        clientjs_plugins = js_bundles_for_plugins(\n            dynamic_plugin_assets[\"external\"][\"clientjs\"], filters=\"js_delimiter_bundler\"\n        )\n\n        js_libs_bundle = Bundle(\n            *js_libs, output=\"webassets/packed_libs.js\", filters=\",\".join(js_filters)\n        )\n\n        js_core_bundle = Bundle(\n            *js_core, output=\"webassets/packed_core.js\", filters=\",\".join(js_filters)\n        )\n\n        if len(js_plugins) == 0:\n            js_plugins_bundle = Bundle(*[])\n        else:\n            js_plugins_bundle = Bundle(\n                *js_plugins.values(),\n                output=\"webassets/packed_plugins.js\",\n                filters=\",\".join(js_plugin_filters),\n            )\n\n        js_app_bundle = Bundle(\n            js_plugins_bundle,\n            js_core_bundle,\n            output=\"webassets/packed_app.js\",\n            filters=\",\".join(js_plugin_filters),\n        )\n\n        js_client_core_bundle = Bundle(\n            *clientjs_core,\n            output=\"webassets/packed_client_core.js\",\n            filters=\",\".join(js_filters),\n        )\n\n        if len(clientjs_plugins) == 0:\n            js_client_plugins_bundle = Bundle(*[])\n        else:\n            js_client_plugins_bundle = Bundle(\n                *clientjs_plugins.values(),\n                output=\"webassets/packed_client_plugins.js\",\n                filters=\",\".join(js_plugin_filters),\n            )\n\n        js_client_bundle = Bundle(\n            js_client_core_bundle,\n            js_client_plugins_bundle,\n            output=\"webassets/packed_client.js\",\n            filters=\",\".join(js_plugin_filters),\n        )\n\n        # -- CSS -------------------------------------------------------------------------------------------------------\n\n        css_filters = [\"cssrewrite\", \"gzip\"]\n\n        css_core = list(dynamic_core_assets[\"css\"]) + all_assets_for_plugins(\n            dynamic_plugin_assets[\"bundled\"][\"css\"]\n        )\n        css_plugins = list(\n            all_assets_for_plugins(dynamic_plugin_assets[\"external\"][\"css\"])\n        )\n\n        css_libs_bundle = Bundle(\n            *css_libs, output=\"webassets/packed_libs.css\", filters=\",\".join(css_filters)\n        )\n\n        if len(css_core) == 0:\n            css_core_bundle = Bundle(*[])\n        else:\n            css_core_bundle = Bundle(\n                *css_core,\n                output=\"webassets/packed_core.css\",\n                filters=\",\".join(css_filters),\n            )\n\n        if len(css_plugins) == 0:\n            css_plugins_bundle = Bundle(*[])\n        else:\n            css_plugins_bundle = Bundle(\n                *css_plugins,\n                output=\"webassets/packed_plugins.css\",\n                filters=\",\".join(css_filters),\n            )\n\n        css_app_bundle = Bundle(\n            css_core,\n            css_plugins,\n            output=\"webassets/packed_app.css\",\n            filters=\",\".join(css_filters),\n        )\n\n        # -- LESS ------------------------------------------------------------------------------------------------------\n\n        less_filters = [\"cssrewrite\", \"less_importrewrite\", \"gzip\"]\n\n        less_core = list(dynamic_core_assets[\"less\"]) + all_assets_for_plugins(\n            dynamic_plugin_assets[\"bundled\"][\"less\"]\n        )\n        less_plugins = all_assets_for_plugins(dynamic_plugin_assets[\"external\"][\"less\"])\n\n        if len(less_core) == 0:\n            less_core_bundle = Bundle(*[])\n        else:\n            less_core_bundle = Bundle(\n                *less_core,\n                output=\"webassets/packed_core.less\",\n                filters=\",\".join(less_filters),\n            )\n\n        if len(less_plugins) == 0:\n            less_plugins_bundle = Bundle(*[])\n        else:\n            less_plugins_bundle = Bundle(\n                *less_plugins,\n                output=\"webassets/packed_plugins.less\",\n                filters=\",\".join(less_filters),\n            )\n\n        less_app_bundle = Bundle(\n            less_core,\n            less_plugins,\n            output=\"webassets/packed_app.less\",\n            filters=\",\".join(less_filters),\n        )\n\n        # -- asset registration ----------------------------------------------------------------------------------------\n\n        assets.register(\"js_libs\", js_libs_bundle)\n        assets.register(\"js_client_core\", js_client_core_bundle)\n        for plugin, bundle in clientjs_plugins.items():\n            # register our collected clientjs plugin bundles so that they are bound to the environment\n            assets.register(f\"js_client_plugin_{plugin}\", bundle)\n        assets.register(\"js_client_plugins\", js_client_plugins_bundle)\n        assets.register(\"js_client\", js_client_bundle)\n        assets.register(\"js_core\", js_core_bundle)\n        for plugin, bundle in js_plugins.items():\n            # register our collected plugin bundles so that they are bound to the environment\n            assets.register(f\"js_plugin_{plugin}\", bundle)\n        assets.register(\"js_plugins\", js_plugins_bundle)\n        assets.register(\"js_app\", js_app_bundle)\n        assets.register(\"css_libs\", css_libs_bundle)\n        assets.register(\"css_core\", css_core_bundle)\n        assets.register(\"css_plugins\", css_plugins_bundle)\n        assets.register(\"css_app\", css_app_bundle)\n        assets.register(\"less_core\", less_core_bundle)\n        assets.register(\"less_plugins\", less_plugins_bundle)\n        assets.register(\"less_app\", less_app_bundle)\n\n    def _setup_login_manager(self):\n        global loginManager\n\n        loginManager = LoginManager()\n\n        # \"strong\" is incompatible to remember me, see maxcountryman/flask-login#156. It also causes issues with\n        # clients toggling between IPv4 and IPv6 client addresses due to names being resolved one way or the other as\n        # at least observed on a Win10 client targeting \"localhost\", resolved as both \"127.0.0.1\" and \"::1\"\n        loginManager.session_protection = \"basic\"\n\n        loginManager.user_loader(load_user)\n        loginManager.unauthorized_handler(unauthorized_user)\n        loginManager.anonymous_user = userManager.anonymous_user_factory\n        loginManager.request_loader(load_user_from_request)\n\n        loginManager.init_app(app, add_context_processor=False)\n\n    def _start_intermediary_server(self):\n        import socket\n        import threading\n        from http.server import BaseHTTPRequestHandler, HTTPServer\n\n        host = self._host\n        port = self._port\n\n        class IntermediaryServerHandler(BaseHTTPRequestHandler):\n            def __init__(self, rules=None, *args, **kwargs):\n                if rules is None:\n                    rules = []\n                self.rules = rules\n                BaseHTTPRequestHandler.__init__(self, *args, **kwargs)\n\n            def do_GET(self):\n                request_path = self.path\n                if \"?\" in request_path:\n                    request_path = request_path[0 : request_path.find(\"?\")]\n\n                for rule in self.rules:\n                    path, data, content_type = rule\n                    if request_path == path:\n                        self.send_response(200)\n                        if content_type:\n                            self.send_header(\"Content-Type\", content_type)\n                        self.end_headers()\n                        if isinstance(data, str):\n                            data = data.encode(\"utf-8\")\n                        self.wfile.write(data)\n                        break\n                else:\n                    self.send_response(404)\n                    self.wfile.write(b\"Not found\")\n\n        base_path = os.path.realpath(\n            os.path.join(os.path.dirname(__file__), \"..\", \"static\")\n        )\n        rules = [\n            (\n                \"/\",\n                [\n                    \"intermediary.html\",\n                ],\n                \"text/html\",\n            ),\n            (\"/favicon.ico\", [\"img\", \"tentacle-20x20.png\"], \"image/png\"),\n            (\n                \"/intermediary.gif\",\n                bytes(\n                    base64.b64decode(\n                        \"R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\"\n                    )\n                ),\n                \"image/gif\",\n            ),\n        ]\n\n        def contents(args):\n            path = os.path.join(base_path, *args)\n            if not os.path.isfile(path):\n                return \"\"\n\n            with open(path, \"rb\") as f:\n                data = f.read()\n            return data\n\n        def process(rule):\n            if len(rule) == 2:\n                path, data = rule\n                content_type = None\n            else:\n                path, data, content_type = rule\n\n            if isinstance(data, (list, tuple)):\n                data = contents(data)\n\n            return path, data, content_type\n\n        rules = list(\n            map(process, filter(lambda rule: len(rule) == 2 or len(rule) == 3, rules))\n        )\n\n        HTTPServerV4 = HTTPServer\n\n        class HTTPServerV6(HTTPServer):\n            address_family = socket.AF_INET6\n\n        class HTTPServerV6SingleStack(HTTPServerV6):\n            def __init__(self, *args, **kwargs):\n                HTTPServerV6.__init__(self, *args, **kwargs)\n\n                # explicitly set V6ONLY flag - seems to be the default, but just to make sure...\n                self.socket.setsockopt(\n                    octoprint.util.net.IPPROTO_IPV6, octoprint.util.net.IPV6_V6ONLY, 1\n                )\n\n        class HTTPServerV6DualStack(HTTPServerV6):\n            def __init__(self, *args, **kwargs):\n                HTTPServerV6.__init__(self, *args, **kwargs)\n\n                # explicitly unset V6ONLY flag\n                self.socket.setsockopt(\n                    octoprint.util.net.IPPROTO_IPV6, octoprint.util.net.IPV6_V6ONLY, 0\n                )\n\n        if \":\" in host:\n            # v6\n            if host == \"::\" and not self._v6_only:\n                ServerClass = HTTPServerV6DualStack\n            else:\n                ServerClass = HTTPServerV6SingleStack\n        else:\n            # v4\n            ServerClass = HTTPServerV4\n\n        if host == \"::\":\n            if self._v6_only:\n                self._logger.debug(f\"Starting intermediary server on http://[::]:{port}\")\n            else:\n                self._logger.debug(\n                    \"Starting intermediary server on http://0.0.0.0:{port} and http://[::]:{port}\".format(\n                        port=port\n                    )\n                )\n        else:\n            self._logger.debug(\n                \"Starting intermediary server on http://{}:{}\".format(\n                    host if \":\" not in host else \"[\" + host + \"]\", port\n                )\n            )\n\n        self._intermediary_server = ServerClass(\n            (host, port),\n            lambda *args, **kwargs: IntermediaryServerHandler(rules, *args, **kwargs),\n            bind_and_activate=False,\n        )\n\n        # if possible, make sure our socket's port descriptor isn't handed over to subprocesses\n        from octoprint.util.platform import set_close_exec\n\n        try:\n            set_close_exec(self._intermediary_server.fileno())\n        except Exception:\n            self._logger.exception(\n                \"Error while attempting to set_close_exec on intermediary server socket\"\n            )\n\n        # then bind the server and have it serve our handler until stopped\n        try:\n            self._intermediary_server.server_bind()\n            self._intermediary_server.server_activate()\n        except Exception as exc:\n            self._intermediary_server.server_close()\n\n            if isinstance(exc, UnicodeDecodeError) and sys.platform == \"win32\":\n                # we end up here if the hostname contains non-ASCII characters due to\n                # https://bugs.python.org/issue26227 - tell the user they need\n                # to either change their hostname or read up other options in\n                # https://github.com/OctoPrint/OctoPrint/issues/3963\n                raise CannotStartServerException(\n                    \"OctoPrint cannot start due to a Python bug \"\n                    \"(https://bugs.python.org/issue26227). Your \"\n                    \"computer's host name contains non-ASCII characters. \"\n                    \"Please either change your computer's host name to \"\n                    \"contain only ASCII characters, or take a look at \"\n                    \"https://github.com/OctoPrint/OctoPrint/issues/3963 for \"\n                    \"other options.\"\n                )\n            else:\n                raise\n\n        def serve():\n            try:\n                self._intermediary_server.serve_forever()\n            except Exception:\n                self._logger.exception(\"Error in intermediary server\")\n\n        thread = threading.Thread(target=serve)\n        thread.daemon = True\n        thread.start()\n\n        self._logger.info(\"Intermediary server started\")\n\n    def _stop_intermediary_server(self):\n        if self._intermediary_server is None:\n            return\n        self._logger.info(\"Shutting down intermediary server...\")\n        self._intermediary_server.shutdown()\n        self._intermediary_server.server_close()\n        self._logger.info(\"Intermediary server shut down\")\n\n    def _setup_plugin_permissions(self):\n        global pluginManager\n\n        from octoprint.access.permissions import PluginOctoPrintPermission\n\n        key_whitelist = re.compile(r\"[A-Za-z0-9_]*\")\n\n        def permission_key(plugin, definition):\n            return \"PLUGIN_{}_{}\".format(plugin.upper(), definition[\"key\"].upper())\n\n        def permission_name(plugin, definition):\n            return \"{}: {}\".format(plugin, definition[\"name\"])\n\n        def permission_role(plugin, role):\n            return f\"plugin_{plugin}_{role}\"\n\n        def process_regular_permission(plugin_info, definition):\n            permissions = []\n            for key in definition.get(\"permissions\", []):\n                permission = octoprint.access.permissions.Permissions.find(key)\n\n                if permission is None:\n                    # if there is still no permission found, postpone this - maybe it is a permission from\n                    # another plugin that hasn't been loaded yet\n                    return False\n\n                permissions.append(permission)\n\n            roles = definition.get(\"roles\", [])\n            description = definition.get(\"description\", \"\")\n            dangerous = definition.get(\"dangerous\", False)\n            default_groups = definition.get(\"default_groups\", [])\n\n            roles_and_permissions = [\n                permission_role(plugin_info.key, role) for role in roles\n            ] + permissions\n\n            key = permission_key(plugin_info.key, definition)\n            permission = PluginOctoPrintPermission(\n                permission_name(plugin_info.name, definition),\n                description,\n                plugin=plugin_info.key,\n                dangerous=dangerous,\n                default_groups=default_groups,\n                *roles_and_permissions,\n            )\n            setattr(\n                octoprint.access.permissions.Permissions,\n                key,\n                PluginOctoPrintPermission(\n                    permission_name(plugin_info.name, definition),\n                    description,\n                    plugin=plugin_info.key,\n                    dangerous=dangerous,\n                    default_groups=default_groups,\n                    *roles_and_permissions,\n                ),\n            )\n\n            self._logger.info(\n                \"Added new permission from plugin {}: {} (needs: {!r})\".format(\n                    plugin_info.key, key, \", \".join(map(repr, permission.needs))\n                )\n            )\n            return True\n\n        postponed = []\n\n        hooks = pluginManager.get_hooks(\"octoprint.access.permissions\")\n        for name, factory in hooks.items():\n            try:\n                if isinstance(factory, (tuple, list)):\n                    additional_permissions = list(factory)\n                elif callable(factory):\n                    additional_permissions = factory()\n                else:\n                    raise ValueError(\"factory must be either a callable, tuple or list\")\n\n                if not isinstance(additional_permissions, (tuple, list)):\n                    raise ValueError(\n                        \"factory result must be either a tuple or a list of permission definition dicts\"\n                    )\n\n                plugin_info = pluginManager.get_plugin_info(name)\n                for p in additional_permissions:\n                    if not isinstance(p, dict):\n                        continue\n\n                    if \"key\" not in p or \"name\" not in p:\n                        continue\n\n                    if not key_whitelist.match(p[\"key\"]):\n                        self._logger.warning(\n                            \"Got permission with invalid key from plugin {}: {}\".format(\n                                name, p[\"key\"]\n                            )\n                        )\n                        continue\n\n                    if not process_regular_permission(plugin_info, p):\n                        postponed.append((plugin_info, p))\n            except Exception:\n                self._logger.exception(\n                    f\"Error while creating permission instance/s from {name}\"\n                )\n\n        # final resolution passes\n        pass_number = 1\n        still_postponed = []\n        while len(postponed):\n            start_length = len(postponed)\n            self._logger.debug(\n                \"Plugin permission resolution pass #{}, \"\n                \"{} unresolved permissions...\".format(pass_number, start_length)\n            )\n\n            for plugin_info, definition in postponed:\n                if not process_regular_permission(plugin_info, definition):\n                    still_postponed.append((plugin_info, definition))\n\n            self._logger.debug(\n                \"... pass #{} done, {} permissions left to resolve\".format(\n                    pass_number, len(still_postponed)\n                )\n            )\n\n            if len(still_postponed) == start_length:\n                # no change, looks like some stuff is unresolvable - let's bail\n                for plugin_info, definition in still_postponed:\n                    self._logger.warning(\n                        \"Unable to resolve permission from {}: {!r}\".format(\n                            plugin_info.key, definition\n                        )\n                    )\n                break\n\n            postponed = still_postponed\n            still_postponed = []\n            pass_number += 1\n\n\nclass LifecycleManager:\n    def __init__(self, plugin_manager):\n        self._plugin_manager = plugin_manager\n\n        self._plugin_lifecycle_callbacks = defaultdict(list)\n        self._logger = logging.getLogger(__name__)\n\n        def wrap_plugin_event(lifecycle_event, new_handler):\n            orig_handler = getattr(self._plugin_manager, \"on_plugin_\" + lifecycle_event)\n\n            def handler(*args, **kwargs):\n                if callable(orig_handler):\n                    orig_handler(*args, **kwargs)\n                if callable(new_handler):\n                    new_handler(*args, **kwargs)\n\n            return handler\n\n        def on_plugin_event_factory(lifecycle_event):\n            def on_plugin_event(name, plugin):\n                self.on_plugin_event(lifecycle_event, name, plugin)\n\n            return on_plugin_event\n\n        for event in (\"loaded\", \"unloaded\", \"enabled\", \"disabled\"):\n            wrap_plugin_event(event, on_plugin_event_factory(event))\n\n    def on_plugin_event(self, event, name, plugin):\n        for lifecycle_callback in self._plugin_lifecycle_callbacks[event]:\n            lifecycle_callback(name, plugin)\n\n    def add_callback(self, events, callback):\n        if isinstance(events, str):\n            events = [events]\n\n        for event in events:\n            self._plugin_lifecycle_callbacks[event].append(callback)\n\n    def remove_callback(self, callback, events=None):\n        if events is None:\n            for event in self._plugin_lifecycle_callbacks:\n                if callback in self._plugin_lifecycle_callbacks[event]:\n                    self._plugin_lifecycle_callbacks[event].remove(callback)\n        else:\n            if isinstance(events, str):\n                events = [events]\n\n            for event in events:\n                if callback in self._plugin_lifecycle_callbacks[event]:\n                    self._plugin_lifecycle_callbacks[event].remove(callback)\n\n\nclass CannotStartServerException(Exception):\n    pass\n", "__author__ = \"Gina H\u00e4u\u00dfge <osd@foosel.net>\"\n__license__ = \"GNU Affero General Public License http://www.gnu.org/licenses/agpl.html\"\n__copyright__ = \"Copyright (C) 2014 The OctoPrint Project - Released under terms of the AGPLv3 License\"\n\nimport hashlib\nimport logging\nimport os\nimport threading\nfrom urllib.parse import quote as urlquote\n\nimport psutil\nfrom flask import abort, jsonify, make_response, request, url_for\n\nimport octoprint.filemanager\nimport octoprint.filemanager.storage\nimport octoprint.filemanager.util\nimport octoprint.slicing\nfrom octoprint.access.permissions import Permissions\nfrom octoprint.events import Events\nfrom octoprint.filemanager.destinations import FileDestinations\nfrom octoprint.server import (\n    NO_CONTENT,\n    current_user,\n    eventManager,\n    fileManager,\n    printer,\n    slicingManager,\n)\nfrom octoprint.server.api import api\nfrom octoprint.server.util.flask import (\n    get_json_command_from_request,\n    no_firstrun_access,\n    with_revalidation_checking,\n)\nfrom octoprint.settings import settings, valid_boolean_trues\nfrom octoprint.util import sv, time_this\n\n# ~~ GCODE file handling\n\n_file_cache = {}\n_file_cache_mutex = threading.RLock()\n\n_DATA_FORMAT_VERSION = \"v2\"\n\n\ndef _clear_file_cache():\n    with _file_cache_mutex:\n        _file_cache.clear()\n\n\ndef _create_lastmodified(path, recursive):\n    path = path[len(\"/api/files\") :]\n    if path.startswith(\"/\"):\n        path = path[1:]\n\n    if path == \"\":\n        # all storages involved\n        lms = [0]\n        for storage in fileManager.registered_storages:\n            try:\n                lms.append(fileManager.last_modified(storage, recursive=recursive))\n            except Exception:\n                logging.getLogger(__name__).exception(\n                    \"There was an error retrieving the last modified data from storage {}\".format(\n                        storage\n                    )\n                )\n                lms.append(None)\n\n        if any(filter(lambda x: x is None, lms)):\n            # we return None if ANY of the involved storages returned None\n            return None\n\n        # if we reach this point, we return the maximum of all dates\n        return max(lms)\n\n    else:\n        if \"/\" in path:\n            storage, path_in_storage = path.split(\"/\", 1)\n        else:\n            storage = path\n            path_in_storage = None\n\n        try:\n            return fileManager.last_modified(\n                storage, path=path_in_storage, recursive=recursive\n            )\n        except Exception:\n            logging.getLogger(__name__).exception(\n                \"There was an error retrieving the last modified data from storage {} and path {}\".format(\n                    storage, path_in_storage\n                )\n            )\n            return None\n\n\ndef _create_etag(path, filter, recursive, lm=None):\n    if lm is None:\n        lm = _create_lastmodified(path, recursive)\n\n    if lm is None:\n        return None\n\n    hash = hashlib.sha1()\n\n    def hash_update(value):\n        value = value.encode(\"utf-8\")\n        hash.update(value)\n\n    hash_update(str(lm))\n    hash_update(str(filter))\n    hash_update(str(recursive))\n\n    path = path[len(\"/api/files\") :]\n    if path.startswith(\"/\"):\n        path = path[1:]\n\n    if \"/\" in path:\n        storage, _ = path.split(\"/\", 1)\n    else:\n        storage = path\n\n    if path == \"\" or storage == FileDestinations.SDCARD:\n        # include sd data in etag\n        hash_update(repr(sorted(printer.get_sd_files(), key=lambda x: sv(x[\"name\"]))))\n\n    hash_update(_DATA_FORMAT_VERSION)  # increment version if we change the API format\n\n    return hash.hexdigest()\n\n\n@api.route(\"/files\", methods=[\"GET\"])\n@Permissions.FILES_LIST.require(403)\n@with_revalidation_checking(\n    etag_factory=lambda lm=None: _create_etag(\n        request.path,\n        request.values.get(\"filter\", False),\n        request.values.get(\"recursive\", False),\n        lm=lm,\n    ),\n    lastmodified_factory=lambda: _create_lastmodified(\n        request.path, request.values.get(\"recursive\", False)\n    ),\n    unless=lambda: request.values.get(\"force\", False)\n    or request.values.get(\"_refresh\", False),\n)\ndef readGcodeFiles():\n    filter = request.values.get(\"filter\", False)\n    recursive = request.values.get(\"recursive\", \"false\") in valid_boolean_trues\n    force = request.values.get(\"force\", \"false\") in valid_boolean_trues\n\n    files = _getFileList(\n        FileDestinations.LOCAL,\n        filter=filter,\n        recursive=recursive,\n        allow_from_cache=not force,\n    )\n    files.extend(_getFileList(FileDestinations.SDCARD, allow_from_cache=not force))\n\n    usage = psutil.disk_usage(settings().getBaseFolder(\"uploads\", check_writable=False))\n    return jsonify(files=files, free=usage.free, total=usage.total)\n\n\n@api.route(\"/files/test\", methods=[\"POST\"])\n@Permissions.FILES_LIST.require(403)\ndef runFilesTest():\n    valid_commands = {\n        \"sanitize\": [\"storage\", \"path\", \"filename\"],\n        \"exists\": [\"storage\", \"path\", \"filename\"],\n    }\n\n    command, data, response = get_json_command_from_request(request, valid_commands)\n    if response is not None:\n        return response\n\n    def sanitize(storage, path, filename):\n        sanitized_path = fileManager.sanitize_path(storage, path)\n        sanitized_name = fileManager.sanitize_name(storage, filename)\n        joined = fileManager.join_path(storage, sanitized_path, sanitized_name)\n        return sanitized_path, sanitized_name, joined\n\n    if command == \"sanitize\":\n        _, _, sanitized = sanitize(data[\"storage\"], data[\"path\"], data[\"filename\"])\n        return jsonify(sanitized=sanitized)\n    elif command == \"exists\":\n        storage = data[\"storage\"]\n        path = data[\"path\"]\n        filename = data[\"filename\"]\n\n        sanitized_path, _, sanitized = sanitize(storage, path, filename)\n\n        exists = fileManager.file_exists(storage, sanitized)\n        if exists:\n            suggestion = filename\n            name, ext = os.path.splitext(filename)\n            counter = 0\n            while fileManager.file_exists(\n                storage,\n                fileManager.join_path(\n                    storage,\n                    sanitized_path,\n                    fileManager.sanitize_name(storage, suggestion),\n                ),\n            ):\n                counter += 1\n                suggestion = f\"{name}_{counter}{ext}\"\n            return jsonify(exists=True, suggestion=suggestion)\n        else:\n            return jsonify(exists=False)\n\n\n@api.route(\"/files/<string:origin>\", methods=[\"GET\"])\n@Permissions.FILES_LIST.require(403)\n@with_revalidation_checking(\n    etag_factory=lambda lm=None: _create_etag(\n        request.path,\n        request.values.get(\"filter\", False),\n        request.values.get(\"recursive\", False),\n        lm=lm,\n    ),\n    lastmodified_factory=lambda: _create_lastmodified(\n        request.path, request.values.get(\"recursive\", False)\n    ),\n    unless=lambda: request.values.get(\"force\", False)\n    or request.values.get(\"_refresh\", False),\n)\ndef readGcodeFilesForOrigin(origin):\n    if origin not in [FileDestinations.LOCAL, FileDestinations.SDCARD]:\n        abort(404)\n\n    filter = request.values.get(\"filter\", False)\n    recursive = request.values.get(\"recursive\", \"false\") in valid_boolean_trues\n    force = request.values.get(\"force\", \"false\") in valid_boolean_trues\n\n    files = _getFileList(\n        origin, filter=filter, recursive=recursive, allow_from_cache=not force\n    )\n\n    if origin == FileDestinations.LOCAL:\n        usage = psutil.disk_usage(\n            settings().getBaseFolder(\"uploads\", check_writable=False)\n        )\n        return jsonify(files=files, free=usage.free, total=usage.total)\n    else:\n        return jsonify(files=files)\n\n\n@api.route(\"/files/<string:target>/<path:filename>\", methods=[\"GET\"])\n@Permissions.FILES_LIST.require(403)\n@with_revalidation_checking(\n    etag_factory=lambda lm=None: _create_etag(\n        request.path,\n        request.values.get(\"filter\", False),\n        request.values.get(\"recursive\", False),\n        lm=lm,\n    ),\n    lastmodified_factory=lambda: _create_lastmodified(\n        request.path, request.values.get(\"recursive\", False)\n    ),\n    unless=lambda: request.values.get(\"force\", False)\n    or request.values.get(\"_refresh\", False),\n)\ndef readGcodeFile(target, filename):\n    if target not in [FileDestinations.LOCAL, FileDestinations.SDCARD]:\n        abort(404)\n\n    if not _validate(target, filename):\n        abort(404)\n\n    recursive = False\n    if \"recursive\" in request.values:\n        recursive = request.values[\"recursive\"] in valid_boolean_trues\n\n    file = _getFileDetails(target, filename, recursive=recursive)\n    if not file:\n        abort(404)\n\n    return jsonify(file)\n\n\ndef _getFileDetails(origin, path, recursive=True):\n    parent, path = os.path.split(path)\n    files = _getFileList(origin, path=parent, recursive=recursive, level=1)\n\n    for f in files:\n        if f[\"name\"] == path:\n            return f\n    else:\n        return None\n\n\n@time_this(\n    logtarget=__name__ + \".timings\",\n    message=\"{func}({func_args},{func_kwargs}) took {timing:.2f}ms\",\n    incl_func_args=True,\n    log_enter=True,\n    message_enter=\"Entering {func}({func_args},{func_kwargs})...\",\n)\ndef _getFileList(\n    origin, path=None, filter=None, recursive=False, level=0, allow_from_cache=True\n):\n    if origin == FileDestinations.SDCARD:\n        sdFileList = printer.get_sd_files(refresh=not allow_from_cache)\n\n        files = []\n        if sdFileList is not None:\n            for f in sdFileList:\n                type_path = octoprint.filemanager.get_file_type(f[\"name\"])\n                if not type_path:\n                    # only supported extensions\n                    continue\n                else:\n                    file_type = type_path[0]\n\n                file = {\n                    \"type\": file_type,\n                    \"typePath\": type_path,\n                    \"name\": f[\"name\"],\n                    \"display\": f[\"display\"] if f[\"display\"] else f[\"name\"],\n                    \"path\": f[\"name\"],\n                    \"origin\": FileDestinations.SDCARD,\n                    \"refs\": {\n                        \"resource\": url_for(\n                            \".readGcodeFile\",\n                            target=FileDestinations.SDCARD,\n                            filename=f[\"name\"],\n                            _external=True,\n                        )\n                    },\n                }\n                if f[\"size\"] is not None:\n                    file.update({\"size\": f[\"size\"]})\n                files.append(file)\n    else:\n        filter_func = None\n        if filter:\n            filter_func = lambda entry, entry_data: octoprint.filemanager.valid_file_type(\n                entry, type=filter\n            )\n\n        with _file_cache_mutex:\n            cache_key = f\"{origin}:{path}:{recursive}:{filter}\"\n            files, lastmodified = _file_cache.get(cache_key, ([], None))\n            # recursive needs to be True for lastmodified queries so we get lastmodified of whole subtree - #3422\n            if (\n                not allow_from_cache\n                or lastmodified is None\n                or lastmodified\n                < fileManager.last_modified(origin, path=path, recursive=True)\n            ):\n                files = list(\n                    fileManager.list_files(\n                        origin,\n                        path=path,\n                        filter=filter_func,\n                        recursive=recursive,\n                        level=level,\n                        force_refresh=not allow_from_cache,\n                    )[origin].values()\n                )\n                lastmodified = fileManager.last_modified(\n                    origin, path=path, recursive=True\n                )\n                _file_cache[cache_key] = (files, lastmodified)\n\n        def analyse_recursively(files, path=None):\n            if path is None:\n                path = \"\"\n\n            result = []\n            for file_or_folder in files:\n                # make a shallow copy in order to not accidentally modify the cached data\n                file_or_folder = dict(file_or_folder)\n\n                file_or_folder[\"origin\"] = FileDestinations.LOCAL\n\n                if file_or_folder[\"type\"] == \"folder\":\n                    if \"children\" in file_or_folder:\n                        file_or_folder[\"children\"] = analyse_recursively(\n                            file_or_folder[\"children\"].values(),\n                            path + file_or_folder[\"name\"] + \"/\",\n                        )\n\n                    file_or_folder[\"refs\"] = {\n                        \"resource\": url_for(\n                            \".readGcodeFile\",\n                            target=FileDestinations.LOCAL,\n                            filename=path + file_or_folder[\"name\"],\n                            _external=True,\n                        )\n                    }\n                else:\n                    if (\n                        \"analysis\" in file_or_folder\n                        and octoprint.filemanager.valid_file_type(\n                            file_or_folder[\"name\"], type=\"gcode\"\n                        )\n                    ):\n                        file_or_folder[\"gcodeAnalysis\"] = file_or_folder[\"analysis\"]\n                        del file_or_folder[\"analysis\"]\n\n                    if (\n                        \"history\" in file_or_folder\n                        and octoprint.filemanager.valid_file_type(\n                            file_or_folder[\"name\"], type=\"gcode\"\n                        )\n                    ):\n                        # convert print log\n                        history = file_or_folder[\"history\"]\n                        del file_or_folder[\"history\"]\n                        success = 0\n                        failure = 0\n                        last = None\n                        for entry in history:\n                            success += 1 if \"success\" in entry and entry[\"success\"] else 0\n                            failure += (\n                                1 if \"success\" in entry and not entry[\"success\"] else 0\n                            )\n                            if not last or (\n                                \"timestamp\" in entry\n                                and \"timestamp\" in last\n                                and entry[\"timestamp\"] > last[\"timestamp\"]\n                            ):\n                                last = entry\n                        if last:\n                            prints = {\n                                \"success\": success,\n                                \"failure\": failure,\n                                \"last\": {\n                                    \"success\": last[\"success\"],\n                                    \"date\": last[\"timestamp\"],\n                                },\n                            }\n                            if \"printTime\" in last:\n                                prints[\"last\"][\"printTime\"] = last[\"printTime\"]\n                            file_or_folder[\"prints\"] = prints\n\n                    file_or_folder[\"refs\"] = {\n                        \"resource\": url_for(\n                            \".readGcodeFile\",\n                            target=FileDestinations.LOCAL,\n                            filename=file_or_folder[\"path\"],\n                            _external=True,\n                        ),\n                        \"download\": url_for(\"index\", _external=True)\n                        + \"downloads/files/\"\n                        + FileDestinations.LOCAL\n                        + \"/\"\n                        + urlquote(file_or_folder[\"path\"]),\n                    }\n\n                result.append(file_or_folder)\n\n            return result\n\n        files = analyse_recursively(files)\n\n    return files\n\n\ndef _verifyFileExists(origin, filename):\n    if origin == FileDestinations.SDCARD:\n        return filename in (x[\"name\"] for x in printer.get_sd_files())\n    else:\n        return fileManager.file_exists(origin, filename)\n\n\ndef _verifyFolderExists(origin, foldername):\n    if origin == FileDestinations.SDCARD:\n        return False\n    else:\n        return fileManager.folder_exists(origin, foldername)\n\n\ndef _isBusy(target, path):\n    currentOrigin, currentPath = _getCurrentFile()\n    if (\n        currentPath is not None\n        and currentOrigin == target\n        and fileManager.file_in_path(FileDestinations.LOCAL, path, currentPath)\n        and (printer.is_printing() or printer.is_paused())\n    ):\n        return True\n\n    return any(\n        target == x[0] and fileManager.file_in_path(FileDestinations.LOCAL, path, x[1])\n        for x in fileManager.get_busy_files()\n    )\n\n\n@api.route(\"/files/<string:target>\", methods=[\"POST\"])\n@no_firstrun_access\n@Permissions.FILES_UPLOAD.require(403)\ndef uploadGcodeFile(target):\n    input_name = \"file\"\n    input_upload_name = (\n        input_name + \".\" + settings().get([\"server\", \"uploads\", \"nameSuffix\"])\n    )\n    input_upload_path = (\n        input_name + \".\" + settings().get([\"server\", \"uploads\", \"pathSuffix\"])\n    )\n    if input_upload_name in request.values and input_upload_path in request.values:\n        if target not in [FileDestinations.LOCAL, FileDestinations.SDCARD]:\n            abort(404)\n\n        upload = octoprint.filemanager.util.DiskFileWrapper(\n            request.values[input_upload_name], request.values[input_upload_path]\n        )\n\n        # Store any additional user data the caller may have passed.\n        userdata = None\n        if \"userdata\" in request.values:\n            import json\n\n            try:\n                userdata = json.loads(request.values[\"userdata\"])\n            except Exception:\n                abort(400, description=\"userdata contains invalid JSON\")\n\n        # check preconditions for SD upload\n        if target == FileDestinations.SDCARD and not settings().getBoolean(\n            [\"feature\", \"sdSupport\"]\n        ):\n            abort(404)\n\n        sd = target == FileDestinations.SDCARD\n        if sd:\n            # validate that all preconditions for SD upload are met before attempting it\n            if not (\n                printer.is_operational()\n                and not (printer.is_printing() or printer.is_paused())\n            ):\n                abort(\n                    409,\n                    description=\"Can not upload to SD card, printer is either not operational or already busy\",\n                )\n            if not printer.is_sd_ready():\n                abort(409, description=\"Can not upload to SD card, not yet initialized\")\n\n        # evaluate select and print parameter and if set check permissions & preconditions\n        # and adjust as necessary\n        #\n        # we do NOT abort(409) here since this would be a backwards incompatible behaviour change\n        # on the API, but instead return the actually effective select and print flags in the response\n        #\n        # note that this behaviour might change in a future API version\n        select_request = (\n            \"select\" in request.values\n            and request.values[\"select\"] in valid_boolean_trues\n            and Permissions.FILES_SELECT.can()\n        )\n        print_request = (\n            \"print\" in request.values\n            and request.values[\"print\"] in valid_boolean_trues\n            and Permissions.PRINT.can()\n        )\n\n        to_select = select_request\n        to_print = print_request\n        if (to_select or to_print) and not (\n            printer.is_operational()\n            and not (printer.is_printing() or printer.is_paused())\n        ):\n            # can't select or print files if not operational or ready\n            to_select = to_print = False\n\n        # determine future filename of file to be uploaded, abort if it can't be uploaded\n        try:\n            # FileDestinations.LOCAL = should normally be target, but can't because SDCard handling isn't implemented yet\n            canonPath, canonFilename = fileManager.canonicalize(\n                FileDestinations.LOCAL, upload.filename\n            )\n            if request.values.get(\"path\"):\n                canonPath = request.values.get(\"path\")\n            if request.values.get(\"filename\"):\n                canonFilename = request.values.get(\"filename\")\n\n            futurePath = fileManager.sanitize_path(FileDestinations.LOCAL, canonPath)\n            futureFilename = fileManager.sanitize_name(\n                FileDestinations.LOCAL, canonFilename\n            )\n        except Exception:\n            canonFilename = None\n            futurePath = None\n            futureFilename = None\n\n        if futureFilename is None:\n            abort(400, description=\"Can not upload file, invalid file name\")\n\n        # prohibit overwriting currently selected file while it's being printed\n        futureFullPath = fileManager.join_path(\n            FileDestinations.LOCAL, futurePath, futureFilename\n        )\n        futureFullPathInStorage = fileManager.path_in_storage(\n            FileDestinations.LOCAL, futureFullPath\n        )\n\n        if not printer.can_modify_file(futureFullPathInStorage, sd):\n            abort(\n                409,\n                description=\"Trying to overwrite file that is currently being printed\",\n            )\n\n        if (\n            fileManager.file_exists(FileDestinations.LOCAL, futureFullPathInStorage)\n            and request.values.get(\"noOverwrite\") in valid_boolean_trues\n        ):\n            abort(409, description=\"File already exists and noOverwrite was set\")\n\n        reselect = printer.is_current_file(futureFullPathInStorage, sd)\n\n        user = current_user.get_name()\n\n        def fileProcessingFinished(filename, absFilename, destination):\n            \"\"\"\n            Callback for when the file processing (upload, optional slicing, addition to analysis queue) has\n            finished.\n\n            Depending on the file's destination triggers either streaming to SD card or directly calls to_select.\n            \"\"\"\n\n            if (\n                destination == FileDestinations.SDCARD\n                and octoprint.filemanager.valid_file_type(filename, \"machinecode\")\n            ):\n                return filename, printer.add_sd_file(\n                    filename,\n                    absFilename,\n                    on_success=selectAndOrPrint,\n                    tags={\"source:api\", \"api:files.sd\"},\n                )\n            else:\n                selectAndOrPrint(filename, absFilename, destination)\n                return filename\n\n        def selectAndOrPrint(filename, absFilename, destination):\n            \"\"\"\n            Callback for when the file is ready to be selected and optionally printed. For SD file uploads this is only\n            the case after they have finished streaming to the printer, which is why this callback is also used\n            for the corresponding call to addSdFile.\n\n            Selects the just uploaded file if either to_select or to_print are True, or if the\n            exact file is already selected, such reloading it.\n            \"\"\"\n            if octoprint.filemanager.valid_file_type(added_file, \"gcode\") and (\n                to_select or to_print or reselect\n            ):\n                printer.select_file(\n                    absFilename,\n                    destination == FileDestinations.SDCARD,\n                    to_print,\n                    user,\n                )\n\n        try:\n            added_file = fileManager.add_file(\n                FileDestinations.LOCAL,\n                futureFullPathInStorage,\n                upload,\n                allow_overwrite=True,\n                display=canonFilename,\n            )\n        except octoprint.filemanager.storage.StorageError as e:\n            if e.code == octoprint.filemanager.storage.StorageError.INVALID_FILE:\n                abort(415, description=\"Could not upload file, invalid type\")\n            else:\n                abort(500, description=\"Could not upload file\")\n        else:\n            filename = fileProcessingFinished(\n                added_file,\n                fileManager.path_on_disk(FileDestinations.LOCAL, added_file),\n                target,\n            )\n            done = not sd\n\n        if userdata is not None:\n            # upload included userdata, add this now to the metadata\n            fileManager.set_additional_metadata(\n                FileDestinations.LOCAL, added_file, \"userdata\", userdata\n            )\n\n        sdFilename = None\n        if isinstance(filename, tuple):\n            filename, sdFilename = filename\n\n        payload = {\n            \"name\": futureFilename,\n            \"path\": filename,\n            \"target\": target,\n            \"select\": select_request,\n            \"print\": print_request,\n            \"effective_select\": to_select,\n            \"effective_print\": to_print,\n        }\n        if userdata is not None:\n            payload[\"userdata\"] = userdata\n        eventManager.fire(Events.UPLOAD, payload)\n\n        files = {}\n        location = url_for(\n            \".readGcodeFile\",\n            target=FileDestinations.LOCAL,\n            filename=filename,\n            _external=True,\n        )\n        files.update(\n            {\n                FileDestinations.LOCAL: {\n                    \"name\": futureFilename,\n                    \"path\": filename,\n                    \"origin\": FileDestinations.LOCAL,\n                    \"refs\": {\n                        \"resource\": location,\n                        \"download\": url_for(\"index\", _external=True)\n                        + \"downloads/files/\"\n                        + FileDestinations.LOCAL\n                        + \"/\"\n                        + urlquote(filename),\n                    },\n                }\n            }\n        )\n\n        if sd and sdFilename:\n            location = url_for(\n                \".readGcodeFile\",\n                target=FileDestinations.SDCARD,\n                filename=sdFilename,\n                _external=True,\n            )\n            files.update(\n                {\n                    FileDestinations.SDCARD: {\n                        \"name\": sdFilename,\n                        \"path\": sdFilename,\n                        \"origin\": FileDestinations.SDCARD,\n                        \"refs\": {\"resource\": location},\n                    }\n                }\n            )\n\n        r = make_response(\n            jsonify(\n                files=files,\n                done=done,\n                effectiveSelect=to_select,\n                effectivePrint=to_print,\n            ),\n            201,\n        )\n        r.headers[\"Location\"] = location\n        return r\n\n    elif \"foldername\" in request.values:\n        foldername = request.values[\"foldername\"]\n\n        if target not in [FileDestinations.LOCAL]:\n            abort(400, description=\"target is invalid\")\n\n        canonPath, canonName = fileManager.canonicalize(target, foldername)\n        futurePath = fileManager.sanitize_path(target, canonPath)\n        futureName = fileManager.sanitize_name(target, canonName)\n        if not futureName or not futurePath:\n            abort(400, description=\"folder name is empty\")\n\n        if \"path\" in request.values and request.values[\"path\"]:\n            futurePath = fileManager.sanitize_path(\n                FileDestinations.LOCAL, request.values[\"path\"]\n            )\n\n        futureFullPath = fileManager.join_path(target, futurePath, futureName)\n        if octoprint.filemanager.valid_file_type(futureName):\n            abort(409, description=\"Can't create folder, please try another name\")\n\n        try:\n            added_folder = fileManager.add_folder(\n                target, futureFullPath, display=canonName\n            )\n        except octoprint.filemanager.storage.StorageError as e:\n            if e.code == octoprint.filemanager.storage.StorageError.INVALID_DIRECTORY:\n                abort(400, description=\"Could not create folder, invalid directory\")\n            else:\n                abort(500, description=\"Could not create folder\")\n\n        location = url_for(\n            \".readGcodeFile\",\n            target=FileDestinations.LOCAL,\n            filename=added_folder,\n            _external=True,\n        )\n        folder = {\n            \"name\": futureName,\n            \"path\": added_folder,\n            \"origin\": target,\n            \"refs\": {\"resource\": location},\n        }\n\n        r = make_response(jsonify(folder=folder, done=True), 201)\n        r.headers[\"Location\"] = location\n        return r\n\n    else:\n        abort(400, description=\"No file to upload and no folder to create\")\n\n\n@api.route(\"/files/<string:target>/<path:filename>\", methods=[\"POST\"])\n@no_firstrun_access\ndef gcodeFileCommand(filename, target):\n    if target not in [FileDestinations.LOCAL, FileDestinations.SDCARD]:\n        abort(404)\n\n    if not _validate(target, filename):\n        abort(404)\n\n    # valid file commands, dict mapping command name to mandatory parameters\n    valid_commands = {\n        \"select\": [],\n        \"unselect\": [],\n        \"slice\": [],\n        \"analyse\": [],\n        \"copy\": [\"destination\"],\n        \"move\": [\"destination\"],\n    }\n\n    command, data, response = get_json_command_from_request(request, valid_commands)\n    if response is not None:\n        return response\n\n    user = current_user.get_name()\n\n    if command == \"select\":\n        with Permissions.FILES_SELECT.require(403):\n            if not _verifyFileExists(target, filename):\n                abort(404)\n\n            # selects/loads a file\n            if not octoprint.filemanager.valid_file_type(filename, type=\"machinecode\"):\n                abort(\n                    415,\n                    description=\"Cannot select file for printing, not a machinecode file\",\n                )\n\n            if not printer.is_ready():\n                abort(\n                    409,\n                    description=\"Printer is already printing, cannot select a new file\",\n                )\n\n            printAfterLoading = False\n            if \"print\" in data and data[\"print\"] in valid_boolean_trues:\n                with Permissions.PRINT.require(403):\n                    if not printer.is_operational():\n                        abort(\n                            409,\n                            description=\"Printer is not operational, cannot directly start printing\",\n                        )\n                    printAfterLoading = True\n\n            sd = False\n            if target == FileDestinations.SDCARD:\n                filenameToSelect = filename\n                sd = True\n            else:\n                filenameToSelect = fileManager.path_on_disk(target, filename)\n            printer.select_file(filenameToSelect, sd, printAfterLoading, user)\n\n    elif command == \"unselect\":\n        with Permissions.FILES_SELECT.require(403):\n            if not printer.is_ready():\n                return make_response(\n                    \"Printer is already printing, cannot unselect current file\", 409\n                )\n\n            _, currentFilename = _getCurrentFile()\n            if currentFilename is None:\n                return make_response(\n                    \"Cannot unselect current file when there is no file selected\", 409\n                )\n\n            if filename != currentFilename and filename != \"current\":\n                return make_response(\n                    \"Only the currently selected file can be unselected\", 400\n                )\n\n            printer.unselect_file()\n\n    elif command == \"slice\":\n        with Permissions.SLICE.require(403):\n            if not _verifyFileExists(target, filename):\n                abort(404)\n\n            try:\n                if \"slicer\" in data:\n                    slicer = data[\"slicer\"]\n                    del data[\"slicer\"]\n                    slicer_instance = slicingManager.get_slicer(slicer)\n\n                elif \"cura\" in slicingManager.registered_slicers:\n                    slicer = \"cura\"\n                    slicer_instance = slicingManager.get_slicer(\"cura\")\n\n                else:\n                    abort(415, description=\"Cannot slice file, no slicer available\")\n            except octoprint.slicing.UnknownSlicer:\n                abort(404)\n\n            if not any(\n                [\n                    octoprint.filemanager.valid_file_type(filename, type=source_file_type)\n                    for source_file_type in slicer_instance.get_slicer_properties().get(\n                        \"source_file_types\", [\"model\"]\n                    )\n                ]\n            ):\n                abort(415, description=\"Cannot slice file, not a model file\")\n\n            cores = psutil.cpu_count()\n            if (\n                slicer_instance.get_slicer_properties().get(\"same_device\", True)\n                and (printer.is_printing() or printer.is_paused())\n                and (cores is None or cores < 2)\n            ):\n                # slicer runs on same device as OctoPrint, slicing while printing is hence disabled\n                abort(\n                    409,\n                    description=\"Cannot slice on this slicer while printing on single core systems or systems of unknown core count due to performance reasons\",\n                )\n\n            if \"destination\" in data and data[\"destination\"]:\n                destination = data[\"destination\"]\n                del data[\"destination\"]\n            elif \"gcode\" in data and data[\"gcode\"]:\n                destination = data[\"gcode\"]\n                del data[\"gcode\"]\n            else:\n                import os\n\n                name, _ = os.path.splitext(filename)\n                destination = (\n                    name\n                    + \".\"\n                    + slicer_instance.get_slicer_properties().get(\n                        \"destination_extensions\", [\"gco\", \"gcode\", \"g\"]\n                    )[0]\n                )\n\n            full_path = destination\n            if \"path\" in data and data[\"path\"]:\n                full_path = fileManager.join_path(target, data[\"path\"], destination)\n            else:\n                path, _ = fileManager.split_path(target, filename)\n                if path:\n                    full_path = fileManager.join_path(target, path, destination)\n\n            canon_path, canon_name = fileManager.canonicalize(target, full_path)\n            sanitized_name = fileManager.sanitize_name(target, canon_name)\n\n            if canon_path:\n                full_path = fileManager.join_path(target, canon_path, sanitized_name)\n            else:\n                full_path = sanitized_name\n\n            # prohibit overwriting the file that is currently being printed\n            currentOrigin, currentFilename = _getCurrentFile()\n            if (\n                currentFilename == full_path\n                and currentOrigin == target\n                and (printer.is_printing() or printer.is_paused())\n            ):\n                abort(\n                    409,\n                    description=\"Trying to slice into file that is currently being printed\",\n                )\n\n            if \"profile\" in data and data[\"profile\"]:\n                profile = data[\"profile\"]\n                del data[\"profile\"]\n            else:\n                profile = None\n\n            if \"printerProfile\" in data and data[\"printerProfile\"]:\n                printerProfile = data[\"printerProfile\"]\n                del data[\"printerProfile\"]\n            else:\n                printerProfile = None\n\n            if (\n                \"position\" in data\n                and data[\"position\"]\n                and isinstance(data[\"position\"], dict)\n                and \"x\" in data[\"position\"]\n                and \"y\" in data[\"position\"]\n            ):\n                position = data[\"position\"]\n                del data[\"position\"]\n            else:\n                position = None\n\n            select_after_slicing = False\n            if \"select\" in data and data[\"select\"] in valid_boolean_trues:\n                if not printer.is_operational():\n                    abort(\n                        409,\n                        description=\"Printer is not operational, cannot directly select for printing\",\n                    )\n                select_after_slicing = True\n\n            print_after_slicing = False\n            if \"print\" in data and data[\"print\"] in valid_boolean_trues:\n                if not printer.is_operational():\n                    abort(\n                        409,\n                        description=\"Printer is not operational, cannot directly start printing\",\n                    )\n                select_after_slicing = print_after_slicing = True\n\n            override_keys = [\n                k for k in data if k.startswith(\"profile.\") and data[k] is not None\n            ]\n            overrides = {}\n            for key in override_keys:\n                overrides[key[len(\"profile.\") :]] = data[key]\n\n            def slicing_done(target, path, select_after_slicing, print_after_slicing):\n                if select_after_slicing or print_after_slicing:\n                    sd = False\n                    if target == FileDestinations.SDCARD:\n                        filenameToSelect = path\n                        sd = True\n                    else:\n                        filenameToSelect = fileManager.path_on_disk(target, path)\n                    printer.select_file(filenameToSelect, sd, print_after_slicing, user)\n\n            try:\n                fileManager.slice(\n                    slicer,\n                    target,\n                    filename,\n                    target,\n                    full_path,\n                    profile=profile,\n                    printer_profile_id=printerProfile,\n                    position=position,\n                    overrides=overrides,\n                    display=canon_name,\n                    callback=slicing_done,\n                    callback_args=(\n                        target,\n                        full_path,\n                        select_after_slicing,\n                        print_after_slicing,\n                    ),\n                )\n            except octoprint.slicing.UnknownProfile:\n                abort(404, description=\"Unknown profile\")\n\n            location = url_for(\n                \".readGcodeFile\",\n                target=target,\n                filename=full_path,\n                _external=True,\n            )\n            result = {\n                \"name\": destination,\n                \"path\": full_path,\n                \"display\": canon_name,\n                \"origin\": FileDestinations.LOCAL,\n                \"refs\": {\n                    \"resource\": location,\n                    \"download\": url_for(\"index\", _external=True)\n                    + \"downloads/files/\"\n                    + target\n                    + \"/\"\n                    + urlquote(full_path),\n                },\n            }\n\n            r = make_response(jsonify(result), 202)\n            r.headers[\"Location\"] = location\n            return r\n\n    elif command == \"analyse\":\n        with Permissions.FILES_UPLOAD.require(403):\n            if not _verifyFileExists(target, filename):\n                abort(404)\n\n            printer_profile = None\n            if \"printerProfile\" in data and data[\"printerProfile\"]:\n                printer_profile = data[\"printerProfile\"]\n\n            if not fileManager.analyse(\n                target, filename, printer_profile_id=printer_profile\n            ):\n                abort(400, description=\"No analysis possible\")\n\n    elif command == \"copy\" or command == \"move\":\n        with Permissions.FILES_UPLOAD.require(403):\n            # Copy and move are only possible on local storage\n            if target not in [FileDestinations.LOCAL]:\n                abort(400, description=f\"Unsupported target for {command}\")\n\n            if not _verifyFileExists(target, filename) and not _verifyFolderExists(\n                target, filename\n            ):\n                abort(404)\n\n            path, name = fileManager.split_path(target, filename)\n\n            destination = data[\"destination\"]\n            dst_path, dst_name = fileManager.split_path(target, destination)\n            sanitized_destination = fileManager.join_path(\n                target, dst_path, fileManager.sanitize_name(target, dst_name)\n            )\n\n            # Check for exception thrown by _verifyFolderExists, if outside the root directory\n            try:\n                if (\n                    _verifyFolderExists(target, destination)\n                    and sanitized_destination != filename\n                ):\n                    # destination is an existing folder and not ourselves (= display rename), we'll assume we are supposed\n                    # to move filename to this folder under the same name\n                    destination = fileManager.join_path(target, destination, name)\n\n                if _verifyFileExists(target, destination) or _verifyFolderExists(\n                    target, destination\n                ):\n                    abort(409, description=\"File or folder does already exist\")\n\n            except Exception:\n                abort(\n                    409, description=\"Exception thrown by storage, bad folder/file name?\"\n                )\n\n            is_file = fileManager.file_exists(target, filename)\n            is_folder = fileManager.folder_exists(target, filename)\n\n            if not (is_file or is_folder):\n                abort(400, description=f\"Neither file nor folder, can't {command}\")\n\n            try:\n                if command == \"copy\":\n                    # destination already there? error...\n                    if _verifyFileExists(target, destination) or _verifyFolderExists(\n                        target, destination\n                    ):\n                        abort(409, description=\"File or folder does already exist\")\n\n                    if is_file:\n                        fileManager.copy_file(target, filename, destination)\n                    else:\n                        fileManager.copy_folder(target, filename, destination)\n\n                elif command == \"move\":\n                    with Permissions.FILES_DELETE.require(403):\n                        if _isBusy(target, filename):\n                            abort(\n                                409,\n                                description=\"Trying to move a file or folder that is currently in use\",\n                            )\n\n                        # destination already there AND not ourselves (= display rename)? error...\n                        if (\n                            _verifyFileExists(target, destination)\n                            or _verifyFolderExists(target, destination)\n                        ) and sanitized_destination != filename:\n                            abort(409, description=\"File or folder does already exist\")\n\n                        # deselect the file if it's currently selected\n                        currentOrigin, currentFilename = _getCurrentFile()\n                        if currentFilename is not None and filename == currentFilename:\n                            printer.unselect_file()\n\n                        if is_file:\n                            fileManager.move_file(target, filename, destination)\n                        else:\n                            fileManager.move_folder(target, filename, destination)\n\n            except octoprint.filemanager.storage.StorageError as e:\n                if e.code == octoprint.filemanager.storage.StorageError.INVALID_FILE:\n                    abort(\n                        415,\n                        description=f\"Could not {command} {filename} to {destination}, invalid type\",\n                    )\n                else:\n                    abort(\n                        500,\n                        description=f\"Could not {command} {filename} to {destination}\",\n                    )\n\n            location = url_for(\n                \".readGcodeFile\",\n                target=target,\n                filename=destination,\n                _external=True,\n            )\n            result = {\n                \"name\": name,\n                \"path\": destination,\n                \"origin\": FileDestinations.LOCAL,\n                \"refs\": {\"resource\": location},\n            }\n            if is_file:\n                result[\"refs\"][\"download\"] = (\n                    url_for(\"index\", _external=True)\n                    + \"downloads/files/\"\n                    + target\n                    + \"/\"\n                    + urlquote(destination)\n                )\n\n            r = make_response(jsonify(result), 201)\n            r.headers[\"Location\"] = location\n            return r\n\n    return NO_CONTENT\n\n\n@api.route(\"/files/<string:target>/<path:filename>\", methods=[\"DELETE\"])\n@no_firstrun_access\n@Permissions.FILES_DELETE.require(403)\ndef deleteGcodeFile(filename, target):\n    if not _validate(target, filename):\n        abort(404)\n\n    if not _verifyFileExists(target, filename) and not _verifyFolderExists(\n        target, filename\n    ):\n        abort(404)\n\n    if target not in [FileDestinations.LOCAL, FileDestinations.SDCARD]:\n        abort(404)\n\n    if _verifyFileExists(target, filename):\n        if _isBusy(target, filename):\n            abort(409, description=\"Trying to delete a file that is currently in use\")\n\n        # deselect the file if it's currently selected\n        currentOrigin, currentPath = _getCurrentFile()\n        if (\n            currentPath is not None\n            and currentOrigin == target\n            and filename == currentPath\n        ):\n            printer.unselect_file()\n\n        # delete it\n        if target == FileDestinations.SDCARD:\n            printer.delete_sd_file(filename, tags={\"source:api\", \"api:files.sd\"})\n        else:\n            fileManager.remove_file(target, filename)\n\n    elif _verifyFolderExists(target, filename):\n        if _isBusy(target, filename):\n            abort(\n                409,\n                description=\"Trying to delete a folder that contains a file that is currently in use\",\n            )\n\n        # deselect the file if it's currently selected\n        currentOrigin, currentPath = _getCurrentFile()\n        if (\n            currentPath is not None\n            and currentOrigin == target\n            and fileManager.file_in_path(target, filename, currentPath)\n        ):\n            printer.unselect_file()\n\n        # delete it\n        fileManager.remove_folder(target, filename, recursive=True)\n\n    return NO_CONTENT\n\n\ndef _getCurrentFile():\n    currentJob = printer.get_current_job()\n    if (\n        currentJob is not None\n        and \"file\" in currentJob\n        and \"path\" in currentJob[\"file\"]\n        and \"origin\" in currentJob[\"file\"]\n    ):\n        return currentJob[\"file\"][\"origin\"], currentJob[\"file\"][\"path\"]\n    else:\n        return None, None\n\n\ndef _validate(target, filename):\n    if target == FileDestinations.SDCARD:\n        # we make no assumptions about the shape of valid SDCard file names\n        return True\n    else:\n        return filename == \"/\".join(\n            map(lambda x: fileManager.sanitize_name(target, x), filename.split(\"/\"))\n        )\n\n\nclass WerkzeugFileWrapper(octoprint.filemanager.util.AbstractFileWrapper):\n    \"\"\"\n    A wrapper around a Werkzeug ``FileStorage`` object.\n\n    Arguments:\n        file_obj (werkzeug.datastructures.FileStorage): The Werkzeug ``FileStorage`` instance to wrap.\n\n    .. seealso::\n\n       `werkzeug.datastructures.FileStorage <http://werkzeug.pocoo.org/docs/0.10/datastructures/#werkzeug.datastructures.FileStorage>`_\n            The documentation of Werkzeug's ``FileStorage`` class.\n    \"\"\"\n\n    def __init__(self, file_obj):\n        octoprint.filemanager.util.AbstractFileWrapper.__init__(self, file_obj.filename)\n        self.file_obj = file_obj\n\n    def save(self, path):\n        \"\"\"\n        Delegates to ``werkzeug.datastructures.FileStorage.save``\n        \"\"\"\n        self.file_obj.save(path)\n\n    def stream(self):\n        \"\"\"\n        Returns ``werkzeug.datastructures.FileStorage.stream``\n        \"\"\"\n        return self.file_obj.stream\n"], "filenames": ["src/octoprint/filemanager/storage.py", "src/octoprint/server/__init__.py", "src/octoprint/server/api/files.py"], "buggy_code_start_loc": [956, 42, 1141], "buggy_code_end_loc": [984, 842, 1177], "fixing_code_start_loc": [957, 43, 1141], "fixing_code_end_loc": [997, 854, 1190], "type": "CWE-434", "message": "Unrestricted Upload of File with Dangerous Type in GitHub repository octoprint/octoprint prior to 1.8.3.", "other": {"cve": {"id": "CVE-2022-2872", "sourceIdentifier": "security@huntr.dev", "published": "2022-09-21T10:15:09.327", "lastModified": "2022-09-23T17:58:22.120", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Unrestricted Upload of File with Dangerous Type in GitHub repository octoprint/octoprint prior to 1.8.3."}, {"lang": "es", "value": "Una Descarga sin Restricciones de Archivos de Tipo Peligroso en el repositorio GitHub octoprint/octoprint versiones anteriores a 1.8.3"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:R/S:C/C:L/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 5.4, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.3, "impactScore": 2.7}], "cvssMetricV30": [{"source": "security@huntr.dev", "type": "Secondary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:H/PR:L/UI:R/S:U/C:L/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 3.7, "baseSeverity": "LOW"}, "exploitabilityScore": 1.2, "impactScore": 2.5}]}, "weaknesses": [{"source": "security@huntr.dev", "type": "Primary", "description": [{"lang": "en", "value": "CWE-434"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:octoprint:octoprint:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.8.3", "matchCriteriaId": "900F81F7-9FC4-44CE-ABD6-1E82DC120B4B"}]}]}], "references": [{"url": "https://github.com/octoprint/octoprint/commit/3e3c11811e216fb371a33e28412df83f9701e5b0", "source": "security@huntr.dev", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://huntr.dev/bounties/b966c74d-6f3f-49fe-b40a-eaf25e362c56", "source": "security@huntr.dev", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/octoprint/octoprint/commit/3e3c11811e216fb371a33e28412df83f9701e5b0"}}