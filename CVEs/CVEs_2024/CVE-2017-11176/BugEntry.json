{"buggy_code": ["/*\n * POSIX message queues filesystem for Linux.\n *\n * Copyright (C) 2003,2004  Krzysztof Benedyczak    (golbi@mat.uni.torun.pl)\n *                          Michal Wronski          (michal.wronski@gmail.com)\n *\n * Spinlocks:               Mohamed Abbas           (abbas.mohamed@intel.com)\n * Lockless receive & send, fd based notify:\n *\t\t\t    Manfred Spraul\t    (manfred@colorfullife.com)\n *\n * Audit:                   George Wilson           (ltcgcw@us.ibm.com)\n *\n * This file is released under the GPL.\n */\n\n#include <linux/capability.h>\n#include <linux/init.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/sysctl.h>\n#include <linux/poll.h>\n#include <linux/mqueue.h>\n#include <linux/msg.h>\n#include <linux/skbuff.h>\n#include <linux/vmalloc.h>\n#include <linux/netlink.h>\n#include <linux/syscalls.h>\n#include <linux/audit.h>\n#include <linux/signal.h>\n#include <linux/mutex.h>\n#include <linux/nsproxy.h>\n#include <linux/pid.h>\n#include <linux/ipc_namespace.h>\n#include <linux/user_namespace.h>\n#include <linux/slab.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/user.h>\n\n#include <net/sock.h>\n#include \"util.h\"\n\n#define MQUEUE_MAGIC\t0x19800202\n#define DIRENT_SIZE\t20\n#define FILENT_SIZE\t80\n\n#define SEND\t\t0\n#define RECV\t\t1\n\n#define STATE_NONE\t0\n#define STATE_READY\t1\n\nstruct posix_msg_tree_node {\n\tstruct rb_node\t\trb_node;\n\tstruct list_head\tmsg_list;\n\tint\t\t\tpriority;\n};\n\nstruct ext_wait_queue {\t\t/* queue of sleeping tasks */\n\tstruct task_struct *task;\n\tstruct list_head list;\n\tstruct msg_msg *msg;\t/* ptr of loaded message */\n\tint state;\t\t/* one of STATE_* values */\n};\n\nstruct mqueue_inode_info {\n\tspinlock_t lock;\n\tstruct inode vfs_inode;\n\twait_queue_head_t wait_q;\n\n\tstruct rb_root msg_tree;\n\tstruct posix_msg_tree_node *node_cache;\n\tstruct mq_attr attr;\n\n\tstruct sigevent notify;\n\tstruct pid *notify_owner;\n\tstruct user_namespace *notify_user_ns;\n\tstruct user_struct *user;\t/* user who created, for accounting */\n\tstruct sock *notify_sock;\n\tstruct sk_buff *notify_cookie;\n\n\t/* for tasks waiting for free space and messages, respectively */\n\tstruct ext_wait_queue e_wait_q[2];\n\n\tunsigned long qsize; /* size of queue in memory (sum of all msgs) */\n};\n\nstatic const struct inode_operations mqueue_dir_inode_operations;\nstatic const struct file_operations mqueue_file_operations;\nstatic const struct super_operations mqueue_super_ops;\nstatic void remove_notification(struct mqueue_inode_info *info);\n\nstatic struct kmem_cache *mqueue_inode_cachep;\n\nstatic struct ctl_table_header *mq_sysctl_table;\n\nstatic inline struct mqueue_inode_info *MQUEUE_I(struct inode *inode)\n{\n\treturn container_of(inode, struct mqueue_inode_info, vfs_inode);\n}\n\n/*\n * This routine should be called with the mq_lock held.\n */\nstatic inline struct ipc_namespace *__get_ns_from_inode(struct inode *inode)\n{\n\treturn get_ipc_ns(inode->i_sb->s_fs_info);\n}\n\nstatic struct ipc_namespace *get_ns_from_inode(struct inode *inode)\n{\n\tstruct ipc_namespace *ns;\n\n\tspin_lock(&mq_lock);\n\tns = __get_ns_from_inode(inode);\n\tspin_unlock(&mq_lock);\n\treturn ns;\n}\n\n/* Auxiliary functions to manipulate messages' list */\nstatic int msg_insert(struct msg_msg *msg, struct mqueue_inode_info *info)\n{\n\tstruct rb_node **p, *parent = NULL;\n\tstruct posix_msg_tree_node *leaf;\n\n\tp = &info->msg_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tleaf = rb_entry(parent, struct posix_msg_tree_node, rb_node);\n\n\t\tif (likely(leaf->priority == msg->m_type))\n\t\t\tgoto insert_msg;\n\t\telse if (msg->m_type < leaf->priority)\n\t\t\tp = &(*p)->rb_left;\n\t\telse\n\t\t\tp = &(*p)->rb_right;\n\t}\n\tif (info->node_cache) {\n\t\tleaf = info->node_cache;\n\t\tinfo->node_cache = NULL;\n\t} else {\n\t\tleaf = kmalloc(sizeof(*leaf), GFP_ATOMIC);\n\t\tif (!leaf)\n\t\t\treturn -ENOMEM;\n\t\tINIT_LIST_HEAD(&leaf->msg_list);\n\t}\n\tleaf->priority = msg->m_type;\n\trb_link_node(&leaf->rb_node, parent, p);\n\trb_insert_color(&leaf->rb_node, &info->msg_tree);\ninsert_msg:\n\tinfo->attr.mq_curmsgs++;\n\tinfo->qsize += msg->m_ts;\n\tlist_add_tail(&msg->m_list, &leaf->msg_list);\n\treturn 0;\n}\n\nstatic inline struct msg_msg *msg_get(struct mqueue_inode_info *info)\n{\n\tstruct rb_node **p, *parent = NULL;\n\tstruct posix_msg_tree_node *leaf;\n\tstruct msg_msg *msg;\n\ntry_again:\n\tp = &info->msg_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\t/*\n\t\t * During insert, low priorities go to the left and high to the\n\t\t * right.  On receive, we want the highest priorities first, so\n\t\t * walk all the way to the right.\n\t\t */\n\t\tp = &(*p)->rb_right;\n\t}\n\tif (!parent) {\n\t\tif (info->attr.mq_curmsgs) {\n\t\t\tpr_warn_once(\"Inconsistency in POSIX message queue, \"\n\t\t\t\t     \"no tree element, but supposedly messages \"\n\t\t\t\t     \"should exist!\\n\");\n\t\t\tinfo->attr.mq_curmsgs = 0;\n\t\t}\n\t\treturn NULL;\n\t}\n\tleaf = rb_entry(parent, struct posix_msg_tree_node, rb_node);\n\tif (unlikely(list_empty(&leaf->msg_list))) {\n\t\tpr_warn_once(\"Inconsistency in POSIX message queue, \"\n\t\t\t     \"empty leaf node but we haven't implemented \"\n\t\t\t     \"lazy leaf delete!\\n\");\n\t\trb_erase(&leaf->rb_node, &info->msg_tree);\n\t\tif (info->node_cache) {\n\t\t\tkfree(leaf);\n\t\t} else {\n\t\t\tinfo->node_cache = leaf;\n\t\t}\n\t\tgoto try_again;\n\t} else {\n\t\tmsg = list_first_entry(&leaf->msg_list,\n\t\t\t\t       struct msg_msg, m_list);\n\t\tlist_del(&msg->m_list);\n\t\tif (list_empty(&leaf->msg_list)) {\n\t\t\trb_erase(&leaf->rb_node, &info->msg_tree);\n\t\t\tif (info->node_cache) {\n\t\t\t\tkfree(leaf);\n\t\t\t} else {\n\t\t\t\tinfo->node_cache = leaf;\n\t\t\t}\n\t\t}\n\t}\n\tinfo->attr.mq_curmsgs--;\n\tinfo->qsize -= msg->m_ts;\n\treturn msg;\n}\n\nstatic struct inode *mqueue_get_inode(struct super_block *sb,\n\t\tstruct ipc_namespace *ipc_ns, umode_t mode,\n\t\tstruct mq_attr *attr)\n{\n\tstruct user_struct *u = current_user();\n\tstruct inode *inode;\n\tint ret = -ENOMEM;\n\n\tinode = new_inode(sb);\n\tif (!inode)\n\t\tgoto err;\n\n\tinode->i_ino = get_next_ino();\n\tinode->i_mode = mode;\n\tinode->i_uid = current_fsuid();\n\tinode->i_gid = current_fsgid();\n\tinode->i_mtime = inode->i_ctime = inode->i_atime = current_time(inode);\n\n\tif (S_ISREG(mode)) {\n\t\tstruct mqueue_inode_info *info;\n\t\tunsigned long mq_bytes, mq_treesize;\n\n\t\tinode->i_fop = &mqueue_file_operations;\n\t\tinode->i_size = FILENT_SIZE;\n\t\t/* mqueue specific info */\n\t\tinfo = MQUEUE_I(inode);\n\t\tspin_lock_init(&info->lock);\n\t\tinit_waitqueue_head(&info->wait_q);\n\t\tINIT_LIST_HEAD(&info->e_wait_q[0].list);\n\t\tINIT_LIST_HEAD(&info->e_wait_q[1].list);\n\t\tinfo->notify_owner = NULL;\n\t\tinfo->notify_user_ns = NULL;\n\t\tinfo->qsize = 0;\n\t\tinfo->user = NULL;\t/* set when all is ok */\n\t\tinfo->msg_tree = RB_ROOT;\n\t\tinfo->node_cache = NULL;\n\t\tmemset(&info->attr, 0, sizeof(info->attr));\n\t\tinfo->attr.mq_maxmsg = min(ipc_ns->mq_msg_max,\n\t\t\t\t\t   ipc_ns->mq_msg_default);\n\t\tinfo->attr.mq_msgsize = min(ipc_ns->mq_msgsize_max,\n\t\t\t\t\t    ipc_ns->mq_msgsize_default);\n\t\tif (attr) {\n\t\t\tinfo->attr.mq_maxmsg = attr->mq_maxmsg;\n\t\t\tinfo->attr.mq_msgsize = attr->mq_msgsize;\n\t\t}\n\t\t/*\n\t\t * We used to allocate a static array of pointers and account\n\t\t * the size of that array as well as one msg_msg struct per\n\t\t * possible message into the queue size. That's no longer\n\t\t * accurate as the queue is now an rbtree and will grow and\n\t\t * shrink depending on usage patterns.  We can, however, still\n\t\t * account one msg_msg struct per message, but the nodes are\n\t\t * allocated depending on priority usage, and most programs\n\t\t * only use one, or a handful, of priorities.  However, since\n\t\t * this is pinned memory, we need to assume worst case, so\n\t\t * that means the min(mq_maxmsg, max_priorities) * struct\n\t\t * posix_msg_tree_node.\n\t\t */\n\t\tmq_treesize = info->attr.mq_maxmsg * sizeof(struct msg_msg) +\n\t\t\tmin_t(unsigned int, info->attr.mq_maxmsg, MQ_PRIO_MAX) *\n\t\t\tsizeof(struct posix_msg_tree_node);\n\n\t\tmq_bytes = mq_treesize + (info->attr.mq_maxmsg *\n\t\t\t\t\t  info->attr.mq_msgsize);\n\n\t\tspin_lock(&mq_lock);\n\t\tif (u->mq_bytes + mq_bytes < u->mq_bytes ||\n\t\t    u->mq_bytes + mq_bytes > rlimit(RLIMIT_MSGQUEUE)) {\n\t\t\tspin_unlock(&mq_lock);\n\t\t\t/* mqueue_evict_inode() releases info->messages */\n\t\t\tret = -EMFILE;\n\t\t\tgoto out_inode;\n\t\t}\n\t\tu->mq_bytes += mq_bytes;\n\t\tspin_unlock(&mq_lock);\n\n\t\t/* all is ok */\n\t\tinfo->user = get_uid(u);\n\t} else if (S_ISDIR(mode)) {\n\t\tinc_nlink(inode);\n\t\t/* Some things misbehave if size == 0 on a directory */\n\t\tinode->i_size = 2 * DIRENT_SIZE;\n\t\tinode->i_op = &mqueue_dir_inode_operations;\n\t\tinode->i_fop = &simple_dir_operations;\n\t}\n\n\treturn inode;\nout_inode:\n\tiput(inode);\nerr:\n\treturn ERR_PTR(ret);\n}\n\nstatic int mqueue_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct inode *inode;\n\tstruct ipc_namespace *ns = sb->s_fs_info;\n\n\tsb->s_iflags |= SB_I_NOEXEC | SB_I_NODEV;\n\tsb->s_blocksize = PAGE_SIZE;\n\tsb->s_blocksize_bits = PAGE_SHIFT;\n\tsb->s_magic = MQUEUE_MAGIC;\n\tsb->s_op = &mqueue_super_ops;\n\n\tinode = mqueue_get_inode(sb, ns, S_IFDIR | S_ISVTX | S_IRWXUGO, NULL);\n\tif (IS_ERR(inode))\n\t\treturn PTR_ERR(inode);\n\n\tsb->s_root = d_make_root(inode);\n\tif (!sb->s_root)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic struct dentry *mqueue_mount(struct file_system_type *fs_type,\n\t\t\t int flags, const char *dev_name,\n\t\t\t void *data)\n{\n\tstruct ipc_namespace *ns;\n\tif (flags & MS_KERNMOUNT) {\n\t\tns = data;\n\t\tdata = NULL;\n\t} else {\n\t\tns = current->nsproxy->ipc_ns;\n\t}\n\treturn mount_ns(fs_type, flags, data, ns, ns->user_ns, mqueue_fill_super);\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct mqueue_inode_info *p = (struct mqueue_inode_info *) foo;\n\n\tinode_init_once(&p->vfs_inode);\n}\n\nstatic struct inode *mqueue_alloc_inode(struct super_block *sb)\n{\n\tstruct mqueue_inode_info *ei;\n\n\tei = kmem_cache_alloc(mqueue_inode_cachep, GFP_KERNEL);\n\tif (!ei)\n\t\treturn NULL;\n\treturn &ei->vfs_inode;\n}\n\nstatic void mqueue_i_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tkmem_cache_free(mqueue_inode_cachep, MQUEUE_I(inode));\n}\n\nstatic void mqueue_destroy_inode(struct inode *inode)\n{\n\tcall_rcu(&inode->i_rcu, mqueue_i_callback);\n}\n\nstatic void mqueue_evict_inode(struct inode *inode)\n{\n\tstruct mqueue_inode_info *info;\n\tstruct user_struct *user;\n\tunsigned long mq_bytes, mq_treesize;\n\tstruct ipc_namespace *ipc_ns;\n\tstruct msg_msg *msg;\n\n\tclear_inode(inode);\n\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn;\n\n\tipc_ns = get_ns_from_inode(inode);\n\tinfo = MQUEUE_I(inode);\n\tspin_lock(&info->lock);\n\twhile ((msg = msg_get(info)) != NULL)\n\t\tfree_msg(msg);\n\tkfree(info->node_cache);\n\tspin_unlock(&info->lock);\n\n\t/* Total amount of bytes accounted for the mqueue */\n\tmq_treesize = info->attr.mq_maxmsg * sizeof(struct msg_msg) +\n\t\tmin_t(unsigned int, info->attr.mq_maxmsg, MQ_PRIO_MAX) *\n\t\tsizeof(struct posix_msg_tree_node);\n\n\tmq_bytes = mq_treesize + (info->attr.mq_maxmsg *\n\t\t\t\t  info->attr.mq_msgsize);\n\n\tuser = info->user;\n\tif (user) {\n\t\tspin_lock(&mq_lock);\n\t\tuser->mq_bytes -= mq_bytes;\n\t\t/*\n\t\t * get_ns_from_inode() ensures that the\n\t\t * (ipc_ns = sb->s_fs_info) is either a valid ipc_ns\n\t\t * to which we now hold a reference, or it is NULL.\n\t\t * We can't put it here under mq_lock, though.\n\t\t */\n\t\tif (ipc_ns)\n\t\t\tipc_ns->mq_queues_count--;\n\t\tspin_unlock(&mq_lock);\n\t\tfree_uid(user);\n\t}\n\tif (ipc_ns)\n\t\tput_ipc_ns(ipc_ns);\n}\n\nstatic int mqueue_create(struct inode *dir, struct dentry *dentry,\n\t\t\t\tumode_t mode, bool excl)\n{\n\tstruct inode *inode;\n\tstruct mq_attr *attr = dentry->d_fsdata;\n\tint error;\n\tstruct ipc_namespace *ipc_ns;\n\n\tspin_lock(&mq_lock);\n\tipc_ns = __get_ns_from_inode(dir);\n\tif (!ipc_ns) {\n\t\terror = -EACCES;\n\t\tgoto out_unlock;\n\t}\n\n\tif (ipc_ns->mq_queues_count >= ipc_ns->mq_queues_max &&\n\t    !capable(CAP_SYS_RESOURCE)) {\n\t\terror = -ENOSPC;\n\t\tgoto out_unlock;\n\t}\n\tipc_ns->mq_queues_count++;\n\tspin_unlock(&mq_lock);\n\n\tinode = mqueue_get_inode(dir->i_sb, ipc_ns, mode, attr);\n\tif (IS_ERR(inode)) {\n\t\terror = PTR_ERR(inode);\n\t\tspin_lock(&mq_lock);\n\t\tipc_ns->mq_queues_count--;\n\t\tgoto out_unlock;\n\t}\n\n\tput_ipc_ns(ipc_ns);\n\tdir->i_size += DIRENT_SIZE;\n\tdir->i_ctime = dir->i_mtime = dir->i_atime = current_time(dir);\n\n\td_instantiate(dentry, inode);\n\tdget(dentry);\n\treturn 0;\nout_unlock:\n\tspin_unlock(&mq_lock);\n\tif (ipc_ns)\n\t\tput_ipc_ns(ipc_ns);\n\treturn error;\n}\n\nstatic int mqueue_unlink(struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode = d_inode(dentry);\n\n\tdir->i_ctime = dir->i_mtime = dir->i_atime = current_time(dir);\n\tdir->i_size -= DIRENT_SIZE;\n\tdrop_nlink(inode);\n\tdput(dentry);\n\treturn 0;\n}\n\n/*\n*\tThis is routine for system read from queue file.\n*\tTo avoid mess with doing here some sort of mq_receive we allow\n*\tto read only queue size & notification info (the only values\n*\tthat are interesting from user point of view and aren't accessible\n*\tthrough std routines)\n*/\nstatic ssize_t mqueue_read_file(struct file *filp, char __user *u_data,\n\t\t\t\tsize_t count, loff_t *off)\n{\n\tstruct mqueue_inode_info *info = MQUEUE_I(file_inode(filp));\n\tchar buffer[FILENT_SIZE];\n\tssize_t ret;\n\n\tspin_lock(&info->lock);\n\tsnprintf(buffer, sizeof(buffer),\n\t\t\t\"QSIZE:%-10lu NOTIFY:%-5d SIGNO:%-5d NOTIFY_PID:%-6d\\n\",\n\t\t\tinfo->qsize,\n\t\t\tinfo->notify_owner ? info->notify.sigev_notify : 0,\n\t\t\t(info->notify_owner &&\n\t\t\t info->notify.sigev_notify == SIGEV_SIGNAL) ?\n\t\t\t\tinfo->notify.sigev_signo : 0,\n\t\t\tpid_vnr(info->notify_owner));\n\tspin_unlock(&info->lock);\n\tbuffer[sizeof(buffer)-1] = '\\0';\n\n\tret = simple_read_from_buffer(u_data, count, off, buffer,\n\t\t\t\tstrlen(buffer));\n\tif (ret <= 0)\n\t\treturn ret;\n\n\tfile_inode(filp)->i_atime = file_inode(filp)->i_ctime = current_time(file_inode(filp));\n\treturn ret;\n}\n\nstatic int mqueue_flush_file(struct file *filp, fl_owner_t id)\n{\n\tstruct mqueue_inode_info *info = MQUEUE_I(file_inode(filp));\n\n\tspin_lock(&info->lock);\n\tif (task_tgid(current) == info->notify_owner)\n\t\tremove_notification(info);\n\n\tspin_unlock(&info->lock);\n\treturn 0;\n}\n\nstatic unsigned int mqueue_poll_file(struct file *filp, struct poll_table_struct *poll_tab)\n{\n\tstruct mqueue_inode_info *info = MQUEUE_I(file_inode(filp));\n\tint retval = 0;\n\n\tpoll_wait(filp, &info->wait_q, poll_tab);\n\n\tspin_lock(&info->lock);\n\tif (info->attr.mq_curmsgs)\n\t\tretval = POLLIN | POLLRDNORM;\n\n\tif (info->attr.mq_curmsgs < info->attr.mq_maxmsg)\n\t\tretval |= POLLOUT | POLLWRNORM;\n\tspin_unlock(&info->lock);\n\n\treturn retval;\n}\n\n/* Adds current to info->e_wait_q[sr] before element with smaller prio */\nstatic void wq_add(struct mqueue_inode_info *info, int sr,\n\t\t\tstruct ext_wait_queue *ewp)\n{\n\tstruct ext_wait_queue *walk;\n\n\tewp->task = current;\n\n\tlist_for_each_entry(walk, &info->e_wait_q[sr].list, list) {\n\t\tif (walk->task->static_prio <= current->static_prio) {\n\t\t\tlist_add_tail(&ewp->list, &walk->list);\n\t\t\treturn;\n\t\t}\n\t}\n\tlist_add_tail(&ewp->list, &info->e_wait_q[sr].list);\n}\n\n/*\n * Puts current task to sleep. Caller must hold queue lock. After return\n * lock isn't held.\n * sr: SEND or RECV\n */\nstatic int wq_sleep(struct mqueue_inode_info *info, int sr,\n\t\t    ktime_t *timeout, struct ext_wait_queue *ewp)\n\t__releases(&info->lock)\n{\n\tint retval;\n\tsigned long time;\n\n\twq_add(info, sr, ewp);\n\n\tfor (;;) {\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\n\t\tspin_unlock(&info->lock);\n\t\ttime = schedule_hrtimeout_range_clock(timeout, 0,\n\t\t\tHRTIMER_MODE_ABS, CLOCK_REALTIME);\n\n\t\tif (ewp->state == STATE_READY) {\n\t\t\tretval = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tspin_lock(&info->lock);\n\t\tif (ewp->state == STATE_READY) {\n\t\t\tretval = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tretval = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (time == 0) {\n\t\t\tretval = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t}\n\tlist_del(&ewp->list);\nout_unlock:\n\tspin_unlock(&info->lock);\nout:\n\treturn retval;\n}\n\n/*\n * Returns waiting task that should be serviced first or NULL if none exists\n */\nstatic struct ext_wait_queue *wq_get_first_waiter(\n\t\tstruct mqueue_inode_info *info, int sr)\n{\n\tstruct list_head *ptr;\n\n\tptr = info->e_wait_q[sr].list.prev;\n\tif (ptr == &info->e_wait_q[sr].list)\n\t\treturn NULL;\n\treturn list_entry(ptr, struct ext_wait_queue, list);\n}\n\n\nstatic inline void set_cookie(struct sk_buff *skb, char code)\n{\n\t((char *)skb->data)[NOTIFY_COOKIE_LEN-1] = code;\n}\n\n/*\n * The next function is only to split too long sys_mq_timedsend\n */\nstatic void __do_notify(struct mqueue_inode_info *info)\n{\n\t/* notification\n\t * invoked when there is registered process and there isn't process\n\t * waiting synchronously for message AND state of queue changed from\n\t * empty to not empty. Here we are sure that no one is waiting\n\t * synchronously. */\n\tif (info->notify_owner &&\n\t    info->attr.mq_curmsgs == 1) {\n\t\tstruct siginfo sig_i;\n\t\tswitch (info->notify.sigev_notify) {\n\t\tcase SIGEV_NONE:\n\t\t\tbreak;\n\t\tcase SIGEV_SIGNAL:\n\t\t\t/* sends signal */\n\n\t\t\tsig_i.si_signo = info->notify.sigev_signo;\n\t\t\tsig_i.si_errno = 0;\n\t\t\tsig_i.si_code = SI_MESGQ;\n\t\t\tsig_i.si_value = info->notify.sigev_value;\n\t\t\t/* map current pid/uid into info->owner's namespaces */\n\t\t\trcu_read_lock();\n\t\t\tsig_i.si_pid = task_tgid_nr_ns(current,\n\t\t\t\t\t\tns_of_pid(info->notify_owner));\n\t\t\tsig_i.si_uid = from_kuid_munged(info->notify_user_ns, current_uid());\n\t\t\trcu_read_unlock();\n\n\t\t\tkill_pid_info(info->notify.sigev_signo,\n\t\t\t\t      &sig_i, info->notify_owner);\n\t\t\tbreak;\n\t\tcase SIGEV_THREAD:\n\t\t\tset_cookie(info->notify_cookie, NOTIFY_WOKENUP);\n\t\t\tnetlink_sendskb(info->notify_sock, info->notify_cookie);\n\t\t\tbreak;\n\t\t}\n\t\t/* after notification unregisters process */\n\t\tput_pid(info->notify_owner);\n\t\tput_user_ns(info->notify_user_ns);\n\t\tinfo->notify_owner = NULL;\n\t\tinfo->notify_user_ns = NULL;\n\t}\n\twake_up(&info->wait_q);\n}\n\nstatic int prepare_timeout(const struct timespec __user *u_abs_timeout,\n\t\t\t   struct timespec *ts)\n{\n\tif (copy_from_user(ts, u_abs_timeout, sizeof(struct timespec)))\n\t\treturn -EFAULT;\n\tif (!timespec_valid(ts))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic void remove_notification(struct mqueue_inode_info *info)\n{\n\tif (info->notify_owner != NULL &&\n\t    info->notify.sigev_notify == SIGEV_THREAD) {\n\t\tset_cookie(info->notify_cookie, NOTIFY_REMOVED);\n\t\tnetlink_sendskb(info->notify_sock, info->notify_cookie);\n\t}\n\tput_pid(info->notify_owner);\n\tput_user_ns(info->notify_user_ns);\n\tinfo->notify_owner = NULL;\n\tinfo->notify_user_ns = NULL;\n}\n\nstatic int mq_attr_ok(struct ipc_namespace *ipc_ns, struct mq_attr *attr)\n{\n\tint mq_treesize;\n\tunsigned long total_size;\n\n\tif (attr->mq_maxmsg <= 0 || attr->mq_msgsize <= 0)\n\t\treturn -EINVAL;\n\tif (capable(CAP_SYS_RESOURCE)) {\n\t\tif (attr->mq_maxmsg > HARD_MSGMAX ||\n\t\t    attr->mq_msgsize > HARD_MSGSIZEMAX)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr->mq_maxmsg > ipc_ns->mq_msg_max ||\n\t\t\t\tattr->mq_msgsize > ipc_ns->mq_msgsize_max)\n\t\t\treturn -EINVAL;\n\t}\n\t/* check for overflow */\n\tif (attr->mq_msgsize > ULONG_MAX/attr->mq_maxmsg)\n\t\treturn -EOVERFLOW;\n\tmq_treesize = attr->mq_maxmsg * sizeof(struct msg_msg) +\n\t\tmin_t(unsigned int, attr->mq_maxmsg, MQ_PRIO_MAX) *\n\t\tsizeof(struct posix_msg_tree_node);\n\ttotal_size = attr->mq_maxmsg * attr->mq_msgsize;\n\tif (total_size + mq_treesize < total_size)\n\t\treturn -EOVERFLOW;\n\treturn 0;\n}\n\n/*\n * Invoked when creating a new queue via sys_mq_open\n */\nstatic struct file *do_create(struct ipc_namespace *ipc_ns, struct inode *dir,\n\t\t\tstruct path *path, int oflag, umode_t mode,\n\t\t\tstruct mq_attr *attr)\n{\n\tconst struct cred *cred = current_cred();\n\tint ret;\n\n\tif (attr) {\n\t\tret = mq_attr_ok(ipc_ns, attr);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\t/* store for use during create */\n\t\tpath->dentry->d_fsdata = attr;\n\t} else {\n\t\tstruct mq_attr def_attr;\n\n\t\tdef_attr.mq_maxmsg = min(ipc_ns->mq_msg_max,\n\t\t\t\t\t ipc_ns->mq_msg_default);\n\t\tdef_attr.mq_msgsize = min(ipc_ns->mq_msgsize_max,\n\t\t\t\t\t  ipc_ns->mq_msgsize_default);\n\t\tret = mq_attr_ok(ipc_ns, &def_attr);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t}\n\n\tmode &= ~current_umask();\n\tret = vfs_create(dir, path->dentry, mode, true);\n\tpath->dentry->d_fsdata = NULL;\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\treturn dentry_open(path, oflag, cred);\n}\n\n/* Opens existing queue */\nstatic struct file *do_open(struct path *path, int oflag)\n{\n\tstatic const int oflag2acc[O_ACCMODE] = { MAY_READ, MAY_WRITE,\n\t\t\t\t\t\t  MAY_READ | MAY_WRITE };\n\tint acc;\n\tif ((oflag & O_ACCMODE) == (O_RDWR | O_WRONLY))\n\t\treturn ERR_PTR(-EINVAL);\n\tacc = oflag2acc[oflag & O_ACCMODE];\n\tif (inode_permission(d_inode(path->dentry), acc))\n\t\treturn ERR_PTR(-EACCES);\n\treturn dentry_open(path, oflag, current_cred());\n}\n\nstatic int do_mq_open(const char __user *u_name, int oflag, umode_t mode,\n\t\t      struct mq_attr *attr)\n{\n\tstruct path path;\n\tstruct file *filp;\n\tstruct filename *name;\n\tint fd, error;\n\tstruct ipc_namespace *ipc_ns = current->nsproxy->ipc_ns;\n\tstruct vfsmount *mnt = ipc_ns->mq_mnt;\n\tstruct dentry *root = mnt->mnt_root;\n\tint ro;\n\n\taudit_mq_open(oflag, mode, attr);\n\n\tif (IS_ERR(name = getname(u_name)))\n\t\treturn PTR_ERR(name);\n\n\tfd = get_unused_fd_flags(O_CLOEXEC);\n\tif (fd < 0)\n\t\tgoto out_putname;\n\n\tro = mnt_want_write(mnt);\t/* we'll drop it in any case */\n\terror = 0;\n\tinode_lock(d_inode(root));\n\tpath.dentry = lookup_one_len(name->name, root, strlen(name->name));\n\tif (IS_ERR(path.dentry)) {\n\t\terror = PTR_ERR(path.dentry);\n\t\tgoto out_putfd;\n\t}\n\tpath.mnt = mntget(mnt);\n\n\tif (oflag & O_CREAT) {\n\t\tif (d_really_is_positive(path.dentry)) {\t/* entry already exists */\n\t\t\taudit_inode(name, path.dentry, 0);\n\t\t\tif (oflag & O_EXCL) {\n\t\t\t\terror = -EEXIST;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tfilp = do_open(&path, oflag);\n\t\t} else {\n\t\t\tif (ro) {\n\t\t\t\terror = ro;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\taudit_inode_parent_hidden(name, root);\n\t\t\tfilp = do_create(ipc_ns, d_inode(root), &path,\n\t\t\t\t\t oflag, mode, attr);\n\t\t}\n\t} else {\n\t\tif (d_really_is_negative(path.dentry)) {\n\t\t\terror = -ENOENT;\n\t\t\tgoto out;\n\t\t}\n\t\taudit_inode(name, path.dentry, 0);\n\t\tfilp = do_open(&path, oflag);\n\t}\n\n\tif (!IS_ERR(filp))\n\t\tfd_install(fd, filp);\n\telse\n\t\terror = PTR_ERR(filp);\nout:\n\tpath_put(&path);\nout_putfd:\n\tif (error) {\n\t\tput_unused_fd(fd);\n\t\tfd = error;\n\t}\n\tinode_unlock(d_inode(root));\n\tif (!ro)\n\t\tmnt_drop_write(mnt);\nout_putname:\n\tputname(name);\n\treturn fd;\n}\n\nSYSCALL_DEFINE4(mq_open, const char __user *, u_name, int, oflag, umode_t, mode,\n\t\tstruct mq_attr __user *, u_attr)\n{\n\tstruct mq_attr attr;\n\tif (u_attr && copy_from_user(&attr, u_attr, sizeof(struct mq_attr)))\n\t\treturn -EFAULT;\n\n\treturn do_mq_open(u_name, oflag, mode, u_attr ? &attr : NULL);\n}\n\nSYSCALL_DEFINE1(mq_unlink, const char __user *, u_name)\n{\n\tint err;\n\tstruct filename *name;\n\tstruct dentry *dentry;\n\tstruct inode *inode = NULL;\n\tstruct ipc_namespace *ipc_ns = current->nsproxy->ipc_ns;\n\tstruct vfsmount *mnt = ipc_ns->mq_mnt;\n\n\tname = getname(u_name);\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\n\taudit_inode_parent_hidden(name, mnt->mnt_root);\n\terr = mnt_want_write(mnt);\n\tif (err)\n\t\tgoto out_name;\n\tinode_lock_nested(d_inode(mnt->mnt_root), I_MUTEX_PARENT);\n\tdentry = lookup_one_len(name->name, mnt->mnt_root,\n\t\t\t\tstrlen(name->name));\n\tif (IS_ERR(dentry)) {\n\t\terr = PTR_ERR(dentry);\n\t\tgoto out_unlock;\n\t}\n\n\tinode = d_inode(dentry);\n\tif (!inode) {\n\t\terr = -ENOENT;\n\t} else {\n\t\tihold(inode);\n\t\terr = vfs_unlink(d_inode(dentry->d_parent), dentry, NULL);\n\t}\n\tdput(dentry);\n\nout_unlock:\n\tinode_unlock(d_inode(mnt->mnt_root));\n\tif (inode)\n\t\tiput(inode);\n\tmnt_drop_write(mnt);\nout_name:\n\tputname(name);\n\n\treturn err;\n}\n\n/* Pipelined send and receive functions.\n *\n * If a receiver finds no waiting message, then it registers itself in the\n * list of waiting receivers. A sender checks that list before adding the new\n * message into the message array. If there is a waiting receiver, then it\n * bypasses the message array and directly hands the message over to the\n * receiver. The receiver accepts the message and returns without grabbing the\n * queue spinlock:\n *\n * - Set pointer to message.\n * - Queue the receiver task for later wakeup (without the info->lock).\n * - Update its state to STATE_READY. Now the receiver can continue.\n * - Wake up the process after the lock is dropped. Should the process wake up\n *   before this wakeup (due to a timeout or a signal) it will either see\n *   STATE_READY and continue or acquire the lock to check the state again.\n *\n * The same algorithm is used for senders.\n */\n\n/* pipelined_send() - send a message directly to the task waiting in\n * sys_mq_timedreceive() (without inserting message into a queue).\n */\nstatic inline void pipelined_send(struct wake_q_head *wake_q,\n\t\t\t\t  struct mqueue_inode_info *info,\n\t\t\t\t  struct msg_msg *message,\n\t\t\t\t  struct ext_wait_queue *receiver)\n{\n\treceiver->msg = message;\n\tlist_del(&receiver->list);\n\twake_q_add(wake_q, receiver->task);\n\t/*\n\t * Rely on the implicit cmpxchg barrier from wake_q_add such\n\t * that we can ensure that updating receiver->state is the last\n\t * write operation: As once set, the receiver can continue,\n\t * and if we don't have the reference count from the wake_q,\n\t * yet, at that point we can later have a use-after-free\n\t * condition and bogus wakeup.\n\t */\n\treceiver->state = STATE_READY;\n}\n\n/* pipelined_receive() - if there is task waiting in sys_mq_timedsend()\n * gets its message and put to the queue (we have one free place for sure). */\nstatic inline void pipelined_receive(struct wake_q_head *wake_q,\n\t\t\t\t     struct mqueue_inode_info *info)\n{\n\tstruct ext_wait_queue *sender = wq_get_first_waiter(info, SEND);\n\n\tif (!sender) {\n\t\t/* for poll */\n\t\twake_up_interruptible(&info->wait_q);\n\t\treturn;\n\t}\n\tif (msg_insert(sender->msg, info))\n\t\treturn;\n\n\tlist_del(&sender->list);\n\twake_q_add(wake_q, sender->task);\n\tsender->state = STATE_READY;\n}\n\nstatic int do_mq_timedsend(mqd_t mqdes, const char __user *u_msg_ptr,\n\t\tsize_t msg_len, unsigned int msg_prio,\n\t\tstruct timespec *ts)\n{\n\tstruct fd f;\n\tstruct inode *inode;\n\tstruct ext_wait_queue wait;\n\tstruct ext_wait_queue *receiver;\n\tstruct msg_msg *msg_ptr;\n\tstruct mqueue_inode_info *info;\n\tktime_t expires, *timeout = NULL;\n\tstruct posix_msg_tree_node *new_leaf = NULL;\n\tint ret = 0;\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (unlikely(msg_prio >= (unsigned long) MQ_PRIO_MAX))\n\t\treturn -EINVAL;\n\n\tif (ts) {\n\t\texpires = timespec_to_ktime(*ts);\n\t\ttimeout = &expires;\n\t}\n\n\taudit_mq_sendrecv(mqdes, msg_len, msg_prio, ts);\n\n\tf = fdget(mqdes);\n\tif (unlikely(!f.file)) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\tinode = file_inode(f.file);\n\tif (unlikely(f.file->f_op != &mqueue_file_operations)) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\tinfo = MQUEUE_I(inode);\n\taudit_file(f.file);\n\n\tif (unlikely(!(f.file->f_mode & FMODE_WRITE))) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\n\tif (unlikely(msg_len > info->attr.mq_msgsize)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out_fput;\n\t}\n\n\t/* First try to allocate memory, before doing anything with\n\t * existing queues. */\n\tmsg_ptr = load_msg(u_msg_ptr, msg_len);\n\tif (IS_ERR(msg_ptr)) {\n\t\tret = PTR_ERR(msg_ptr);\n\t\tgoto out_fput;\n\t}\n\tmsg_ptr->m_ts = msg_len;\n\tmsg_ptr->m_type = msg_prio;\n\n\t/*\n\t * msg_insert really wants us to have a valid, spare node struct so\n\t * it doesn't have to kmalloc a GFP_ATOMIC allocation, but it will\n\t * fall back to that if necessary.\n\t */\n\tif (!info->node_cache)\n\t\tnew_leaf = kmalloc(sizeof(*new_leaf), GFP_KERNEL);\n\n\tspin_lock(&info->lock);\n\n\tif (!info->node_cache && new_leaf) {\n\t\t/* Save our speculative allocation into the cache */\n\t\tINIT_LIST_HEAD(&new_leaf->msg_list);\n\t\tinfo->node_cache = new_leaf;\n\t\tnew_leaf = NULL;\n\t} else {\n\t\tkfree(new_leaf);\n\t}\n\n\tif (info->attr.mq_curmsgs == info->attr.mq_maxmsg) {\n\t\tif (f.file->f_flags & O_NONBLOCK) {\n\t\t\tret = -EAGAIN;\n\t\t} else {\n\t\t\twait.task = current;\n\t\t\twait.msg = (void *) msg_ptr;\n\t\t\twait.state = STATE_NONE;\n\t\t\tret = wq_sleep(info, SEND, timeout, &wait);\n\t\t\t/*\n\t\t\t * wq_sleep must be called with info->lock held, and\n\t\t\t * returns with the lock released\n\t\t\t */\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\treceiver = wq_get_first_waiter(info, RECV);\n\t\tif (receiver) {\n\t\t\tpipelined_send(&wake_q, info, msg_ptr, receiver);\n\t\t} else {\n\t\t\t/* adds message to the queue */\n\t\t\tret = msg_insert(msg_ptr, info);\n\t\t\tif (ret)\n\t\t\t\tgoto out_unlock;\n\t\t\t__do_notify(info);\n\t\t}\n\t\tinode->i_atime = inode->i_mtime = inode->i_ctime =\n\t\t\t\tcurrent_time(inode);\n\t}\nout_unlock:\n\tspin_unlock(&info->lock);\n\twake_up_q(&wake_q);\nout_free:\n\tif (ret)\n\t\tfree_msg(msg_ptr);\nout_fput:\n\tfdput(f);\nout:\n\treturn ret;\n}\n\nstatic int do_mq_timedreceive(mqd_t mqdes, char __user *u_msg_ptr,\n\t\tsize_t msg_len, unsigned int __user *u_msg_prio,\n\t\tstruct timespec *ts)\n{\n\tssize_t ret;\n\tstruct msg_msg *msg_ptr;\n\tstruct fd f;\n\tstruct inode *inode;\n\tstruct mqueue_inode_info *info;\n\tstruct ext_wait_queue wait;\n\tktime_t expires, *timeout = NULL;\n\tstruct posix_msg_tree_node *new_leaf = NULL;\n\n\tif (ts) {\n\t\texpires = timespec_to_ktime(*ts);\n\t\ttimeout = &expires;\n\t}\n\n\taudit_mq_sendrecv(mqdes, msg_len, 0, ts);\n\n\tf = fdget(mqdes);\n\tif (unlikely(!f.file)) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\tinode = file_inode(f.file);\n\tif (unlikely(f.file->f_op != &mqueue_file_operations)) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\tinfo = MQUEUE_I(inode);\n\taudit_file(f.file);\n\n\tif (unlikely(!(f.file->f_mode & FMODE_READ))) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\n\t/* checks if buffer is big enough */\n\tif (unlikely(msg_len < info->attr.mq_msgsize)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out_fput;\n\t}\n\n\t/*\n\t * msg_insert really wants us to have a valid, spare node struct so\n\t * it doesn't have to kmalloc a GFP_ATOMIC allocation, but it will\n\t * fall back to that if necessary.\n\t */\n\tif (!info->node_cache)\n\t\tnew_leaf = kmalloc(sizeof(*new_leaf), GFP_KERNEL);\n\n\tspin_lock(&info->lock);\n\n\tif (!info->node_cache && new_leaf) {\n\t\t/* Save our speculative allocation into the cache */\n\t\tINIT_LIST_HEAD(&new_leaf->msg_list);\n\t\tinfo->node_cache = new_leaf;\n\t} else {\n\t\tkfree(new_leaf);\n\t}\n\n\tif (info->attr.mq_curmsgs == 0) {\n\t\tif (f.file->f_flags & O_NONBLOCK) {\n\t\t\tspin_unlock(&info->lock);\n\t\t\tret = -EAGAIN;\n\t\t} else {\n\t\t\twait.task = current;\n\t\t\twait.state = STATE_NONE;\n\t\t\tret = wq_sleep(info, RECV, timeout, &wait);\n\t\t\tmsg_ptr = wait.msg;\n\t\t}\n\t} else {\n\t\tDEFINE_WAKE_Q(wake_q);\n\n\t\tmsg_ptr = msg_get(info);\n\n\t\tinode->i_atime = inode->i_mtime = inode->i_ctime =\n\t\t\t\tcurrent_time(inode);\n\n\t\t/* There is now free space in queue. */\n\t\tpipelined_receive(&wake_q, info);\n\t\tspin_unlock(&info->lock);\n\t\twake_up_q(&wake_q);\n\t\tret = 0;\n\t}\n\tif (ret == 0) {\n\t\tret = msg_ptr->m_ts;\n\n\t\tif ((u_msg_prio && put_user(msg_ptr->m_type, u_msg_prio)) ||\n\t\t\tstore_msg(u_msg_ptr, msg_ptr, msg_ptr->m_ts)) {\n\t\t\tret = -EFAULT;\n\t\t}\n\t\tfree_msg(msg_ptr);\n\t}\nout_fput:\n\tfdput(f);\nout:\n\treturn ret;\n}\n\nSYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,\n\t\tsize_t, msg_len, unsigned int, msg_prio,\n\t\tconst struct timespec __user *, u_abs_timeout)\n{\n\tstruct timespec ts, *p = NULL;\n\tif (u_abs_timeout) {\n\t\tint res = prepare_timeout(u_abs_timeout, &ts);\n\t\tif (res)\n\t\t\treturn res;\n\t\tp = &ts;\n\t}\n\treturn do_mq_timedsend(mqdes, u_msg_ptr, msg_len, msg_prio, p);\n}\n\nSYSCALL_DEFINE5(mq_timedreceive, mqd_t, mqdes, char __user *, u_msg_ptr,\n\t\tsize_t, msg_len, unsigned int __user *, u_msg_prio,\n\t\tconst struct timespec __user *, u_abs_timeout)\n{\n\tstruct timespec ts, *p = NULL;\n\tif (u_abs_timeout) {\n\t\tint res = prepare_timeout(u_abs_timeout, &ts);\n\t\tif (res)\n\t\t\treturn res;\n\t\tp = &ts;\n\t}\n\treturn do_mq_timedreceive(mqdes, u_msg_ptr, msg_len, u_msg_prio, p);\n}\n\n/*\n * Notes: the case when user wants us to deregister (with NULL as pointer)\n * and he isn't currently owner of notification, will be silently discarded.\n * It isn't explicitly defined in the POSIX.\n */\nstatic int do_mq_notify(mqd_t mqdes, const struct sigevent *notification)\n{\n\tint ret;\n\tstruct fd f;\n\tstruct sock *sock;\n\tstruct inode *inode;\n\tstruct mqueue_inode_info *info;\n\tstruct sk_buff *nc;\n\n\taudit_mq_notify(mqdes, notification);\n\n\tnc = NULL;\n\tsock = NULL;\n\tif (notification != NULL) {\n\t\tif (unlikely(notification->sigev_notify != SIGEV_NONE &&\n\t\t\t     notification->sigev_notify != SIGEV_SIGNAL &&\n\t\t\t     notification->sigev_notify != SIGEV_THREAD))\n\t\t\treturn -EINVAL;\n\t\tif (notification->sigev_notify == SIGEV_SIGNAL &&\n\t\t\t!valid_signal(notification->sigev_signo)) {\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (notification->sigev_notify == SIGEV_THREAD) {\n\t\t\tlong timeo;\n\n\t\t\t/* create the notify skb */\n\t\t\tnc = alloc_skb(NOTIFY_COOKIE_LEN, GFP_KERNEL);\n\t\t\tif (!nc) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (copy_from_user(nc->data,\n\t\t\t\t\tnotification->sigev_value.sival_ptr,\n\t\t\t\t\tNOTIFY_COOKIE_LEN)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/* TODO: add a header? */\n\t\t\tskb_put(nc, NOTIFY_COOKIE_LEN);\n\t\t\t/* and attach it to the socket */\nretry:\n\t\t\tf = fdget(notification->sigev_signo);\n\t\t\tif (!f.file) {\n\t\t\t\tret = -EBADF;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsock = netlink_getsockbyfilp(f.file);\n\t\t\tfdput(f);\n\t\t\tif (IS_ERR(sock)) {\n\t\t\t\tret = PTR_ERR(sock);\n\t\t\t\tsock = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\ttimeo = MAX_SCHEDULE_TIMEOUT;\n\t\t\tret = netlink_attachskb(sock, nc, &timeo, NULL);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto retry;\n\t\t\tif (ret) {\n\t\t\t\tsock = NULL;\n\t\t\t\tnc = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tf = fdget(mqdes);\n\tif (!f.file) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\tinode = file_inode(f.file);\n\tif (unlikely(f.file->f_op != &mqueue_file_operations)) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\tinfo = MQUEUE_I(inode);\n\n\tret = 0;\n\tspin_lock(&info->lock);\n\tif (notification == NULL) {\n\t\tif (info->notify_owner == task_tgid(current)) {\n\t\t\tremove_notification(info);\n\t\t\tinode->i_atime = inode->i_ctime = current_time(inode);\n\t\t}\n\t} else if (info->notify_owner != NULL) {\n\t\tret = -EBUSY;\n\t} else {\n\t\tswitch (notification->sigev_notify) {\n\t\tcase SIGEV_NONE:\n\t\t\tinfo->notify.sigev_notify = SIGEV_NONE;\n\t\t\tbreak;\n\t\tcase SIGEV_THREAD:\n\t\t\tinfo->notify_sock = sock;\n\t\t\tinfo->notify_cookie = nc;\n\t\t\tsock = NULL;\n\t\t\tnc = NULL;\n\t\t\tinfo->notify.sigev_notify = SIGEV_THREAD;\n\t\t\tbreak;\n\t\tcase SIGEV_SIGNAL:\n\t\t\tinfo->notify.sigev_signo = notification->sigev_signo;\n\t\t\tinfo->notify.sigev_value = notification->sigev_value;\n\t\t\tinfo->notify.sigev_notify = SIGEV_SIGNAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tinfo->notify_owner = get_pid(task_tgid(current));\n\t\tinfo->notify_user_ns = get_user_ns(current_user_ns());\n\t\tinode->i_atime = inode->i_ctime = current_time(inode);\n\t}\n\tspin_unlock(&info->lock);\nout_fput:\n\tfdput(f);\nout:\n\tif (sock)\n\t\tnetlink_detachskb(sock, nc);\n\telse if (nc)\n\t\tdev_kfree_skb(nc);\n\n\treturn ret;\n}\n\nSYSCALL_DEFINE2(mq_notify, mqd_t, mqdes,\n\t\tconst struct sigevent __user *, u_notification)\n{\n\tstruct sigevent n, *p = NULL;\n\tif (u_notification) {\n\t\tif (copy_from_user(&n, u_notification, sizeof(struct sigevent)))\n\t\t\treturn -EFAULT;\n\t\tp = &n;\n\t}\n\treturn do_mq_notify(mqdes, p);\n}\n\nstatic int do_mq_getsetattr(int mqdes, struct mq_attr *new, struct mq_attr *old)\n{\n\tstruct fd f;\n\tstruct inode *inode;\n\tstruct mqueue_inode_info *info;\n\n\tif (new && (new->mq_flags & (~O_NONBLOCK)))\n\t\treturn -EINVAL;\n\n\tf = fdget(mqdes);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tif (unlikely(f.file->f_op != &mqueue_file_operations)) {\n\t\tfdput(f);\n\t\treturn -EBADF;\n\t}\n\n\tinode = file_inode(f.file);\n\tinfo = MQUEUE_I(inode);\n\n\tspin_lock(&info->lock);\n\n\tif (old) {\n\t\t*old = info->attr;\n\t\told->mq_flags = f.file->f_flags & O_NONBLOCK;\n\t}\n\tif (new) {\n\t\taudit_mq_getsetattr(mqdes, new);\n\t\tspin_lock(&f.file->f_lock);\n\t\tif (new->mq_flags & O_NONBLOCK)\n\t\t\tf.file->f_flags |= O_NONBLOCK;\n\t\telse\n\t\t\tf.file->f_flags &= ~O_NONBLOCK;\n\t\tspin_unlock(&f.file->f_lock);\n\n\t\tinode->i_atime = inode->i_ctime = current_time(inode);\n\t}\n\n\tspin_unlock(&info->lock);\n\tfdput(f);\n\treturn 0;\n}\n\nSYSCALL_DEFINE3(mq_getsetattr, mqd_t, mqdes,\n\t\tconst struct mq_attr __user *, u_mqstat,\n\t\tstruct mq_attr __user *, u_omqstat)\n{\n\tint ret;\n\tstruct mq_attr mqstat, omqstat;\n\tstruct mq_attr *new = NULL, *old = NULL;\n\n\tif (u_mqstat) {\n\t\tnew = &mqstat;\n\t\tif (copy_from_user(new, u_mqstat, sizeof(struct mq_attr)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (u_omqstat)\n\t\told = &omqstat;\n\n\tret = do_mq_getsetattr(mqdes, new, old);\n\tif (ret || !old)\n\t\treturn ret;\n\n\tif (copy_to_user(u_omqstat, old, sizeof(struct mq_attr)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\n\nstruct compat_mq_attr {\n\tcompat_long_t mq_flags;      /* message queue flags\t\t     */\n\tcompat_long_t mq_maxmsg;     /* maximum number of messages\t     */\n\tcompat_long_t mq_msgsize;    /* maximum message size\t\t     */\n\tcompat_long_t mq_curmsgs;    /* number of messages currently queued  */\n\tcompat_long_t __reserved[4]; /* ignored for input, zeroed for output */\n};\n\nstatic inline int get_compat_mq_attr(struct mq_attr *attr,\n\t\t\tconst struct compat_mq_attr __user *uattr)\n{\n\tstruct compat_mq_attr v;\n\n\tif (copy_from_user(&v, uattr, sizeof(*uattr)))\n\t\treturn -EFAULT;\n\n\tmemset(attr, 0, sizeof(*attr));\n\tattr->mq_flags = v.mq_flags;\n\tattr->mq_maxmsg = v.mq_maxmsg;\n\tattr->mq_msgsize = v.mq_msgsize;\n\tattr->mq_curmsgs = v.mq_curmsgs;\n\treturn 0;\n}\n\nstatic inline int put_compat_mq_attr(const struct mq_attr *attr,\n\t\t\tstruct compat_mq_attr __user *uattr)\n{\n\tstruct compat_mq_attr v;\n\n\tmemset(&v, 0, sizeof(v));\n\tv.mq_flags = attr->mq_flags;\n\tv.mq_maxmsg = attr->mq_maxmsg;\n\tv.mq_msgsize = attr->mq_msgsize;\n\tv.mq_curmsgs = attr->mq_curmsgs;\n\tif (copy_to_user(uattr, &v, sizeof(*uattr)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nCOMPAT_SYSCALL_DEFINE4(mq_open, const char __user *, u_name,\n\t\t       int, oflag, compat_mode_t, mode,\n\t\t       struct compat_mq_attr __user *, u_attr)\n{\n\tstruct mq_attr attr, *p = NULL;\n\tif (u_attr && oflag & O_CREAT) {\n\t\tp = &attr;\n\t\tif (get_compat_mq_attr(&attr, u_attr))\n\t\t\treturn -EFAULT;\n\t}\n\treturn do_mq_open(u_name, oflag, mode, p);\n}\n\nstatic int compat_prepare_timeout(const struct compat_timespec __user *p,\n\t\t\t\t   struct timespec *ts)\n{\n\tif (compat_get_timespec(ts, p))\n\t\treturn -EFAULT;\n\tif (!timespec_valid(ts))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nCOMPAT_SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes,\n\t\t       const char __user *, u_msg_ptr,\n\t\t       compat_size_t, msg_len, unsigned int, msg_prio,\n\t\t       const struct compat_timespec __user *, u_abs_timeout)\n{\n\tstruct timespec ts, *p = NULL;\n\tif (u_abs_timeout) {\n\t\tint res = compat_prepare_timeout(u_abs_timeout, &ts);\n\t\tif (res)\n\t\t\treturn res;\n\t\tp = &ts;\n\t}\n\treturn do_mq_timedsend(mqdes, u_msg_ptr, msg_len, msg_prio, p);\n}\n\nCOMPAT_SYSCALL_DEFINE5(mq_timedreceive, mqd_t, mqdes,\n\t\t       char __user *, u_msg_ptr,\n\t\t       compat_size_t, msg_len, unsigned int __user *, u_msg_prio,\n\t\t       const struct compat_timespec __user *, u_abs_timeout)\n{\n\tstruct timespec ts, *p = NULL;\n\tif (u_abs_timeout) {\n\t\tint res = compat_prepare_timeout(u_abs_timeout, &ts);\n\t\tif (res)\n\t\t\treturn res;\n\t\tp = &ts;\n\t}\n\treturn do_mq_timedreceive(mqdes, u_msg_ptr, msg_len, u_msg_prio, p);\n}\n\nCOMPAT_SYSCALL_DEFINE2(mq_notify, mqd_t, mqdes,\n\t\t       const struct compat_sigevent __user *, u_notification)\n{\n\tstruct sigevent n, *p = NULL;\n\tif (u_notification) {\n\t\tif (get_compat_sigevent(&n, u_notification))\n\t\t\treturn -EFAULT;\n\t\tif (n.sigev_notify == SIGEV_THREAD)\n\t\t\tn.sigev_value.sival_ptr = compat_ptr(n.sigev_value.sival_int);\n\t\tp = &n;\n\t}\n\treturn do_mq_notify(mqdes, p);\n}\n\nCOMPAT_SYSCALL_DEFINE3(mq_getsetattr, mqd_t, mqdes,\n\t\t       const struct compat_mq_attr __user *, u_mqstat,\n\t\t       struct compat_mq_attr __user *, u_omqstat)\n{\n\tint ret;\n\tstruct mq_attr mqstat, omqstat;\n\tstruct mq_attr *new = NULL, *old = NULL;\n\n\tif (u_mqstat) {\n\t\tnew = &mqstat;\n\t\tif (get_compat_mq_attr(new, u_mqstat))\n\t\t\treturn -EFAULT;\n\t}\n\tif (u_omqstat)\n\t\told = &omqstat;\n\n\tret = do_mq_getsetattr(mqdes, new, old);\n\tif (ret || !old)\n\t\treturn ret;\n\n\tif (put_compat_mq_attr(old, u_omqstat))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n#endif\n\nstatic const struct inode_operations mqueue_dir_inode_operations = {\n\t.lookup = simple_lookup,\n\t.create = mqueue_create,\n\t.unlink = mqueue_unlink,\n};\n\nstatic const struct file_operations mqueue_file_operations = {\n\t.flush = mqueue_flush_file,\n\t.poll = mqueue_poll_file,\n\t.read = mqueue_read_file,\n\t.llseek = default_llseek,\n};\n\nstatic const struct super_operations mqueue_super_ops = {\n\t.alloc_inode = mqueue_alloc_inode,\n\t.destroy_inode = mqueue_destroy_inode,\n\t.evict_inode = mqueue_evict_inode,\n\t.statfs = simple_statfs,\n};\n\nstatic struct file_system_type mqueue_fs_type = {\n\t.name = \"mqueue\",\n\t.mount = mqueue_mount,\n\t.kill_sb = kill_litter_super,\n\t.fs_flags = FS_USERNS_MOUNT,\n};\n\nint mq_init_ns(struct ipc_namespace *ns)\n{\n\tns->mq_queues_count  = 0;\n\tns->mq_queues_max    = DFLT_QUEUESMAX;\n\tns->mq_msg_max       = DFLT_MSGMAX;\n\tns->mq_msgsize_max   = DFLT_MSGSIZEMAX;\n\tns->mq_msg_default   = DFLT_MSG;\n\tns->mq_msgsize_default  = DFLT_MSGSIZE;\n\n\tns->mq_mnt = kern_mount_data(&mqueue_fs_type, ns);\n\tif (IS_ERR(ns->mq_mnt)) {\n\t\tint err = PTR_ERR(ns->mq_mnt);\n\t\tns->mq_mnt = NULL;\n\t\treturn err;\n\t}\n\treturn 0;\n}\n\nvoid mq_clear_sbinfo(struct ipc_namespace *ns)\n{\n\tns->mq_mnt->mnt_sb->s_fs_info = NULL;\n}\n\nvoid mq_put_mnt(struct ipc_namespace *ns)\n{\n\tkern_unmount(ns->mq_mnt);\n}\n\nstatic int __init init_mqueue_fs(void)\n{\n\tint error;\n\n\tmqueue_inode_cachep = kmem_cache_create(\"mqueue_inode_cache\",\n\t\t\t\tsizeof(struct mqueue_inode_info), 0,\n\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_ACCOUNT, init_once);\n\tif (mqueue_inode_cachep == NULL)\n\t\treturn -ENOMEM;\n\n\t/* ignore failures - they are not fatal */\n\tmq_sysctl_table = mq_register_sysctl_table();\n\n\terror = register_filesystem(&mqueue_fs_type);\n\tif (error)\n\t\tgoto out_sysctl;\n\n\tspin_lock_init(&mq_lock);\n\n\terror = mq_init_ns(&init_ipc_ns);\n\tif (error)\n\t\tgoto out_filesystem;\n\n\treturn 0;\n\nout_filesystem:\n\tunregister_filesystem(&mqueue_fs_type);\nout_sysctl:\n\tif (mq_sysctl_table)\n\t\tunregister_sysctl_table(mq_sysctl_table);\n\tkmem_cache_destroy(mqueue_inode_cachep);\n\treturn error;\n}\n\ndevice_initcall(init_mqueue_fs);\n"], "fixing_code": ["/*\n * POSIX message queues filesystem for Linux.\n *\n * Copyright (C) 2003,2004  Krzysztof Benedyczak    (golbi@mat.uni.torun.pl)\n *                          Michal Wronski          (michal.wronski@gmail.com)\n *\n * Spinlocks:               Mohamed Abbas           (abbas.mohamed@intel.com)\n * Lockless receive & send, fd based notify:\n *\t\t\t    Manfred Spraul\t    (manfred@colorfullife.com)\n *\n * Audit:                   George Wilson           (ltcgcw@us.ibm.com)\n *\n * This file is released under the GPL.\n */\n\n#include <linux/capability.h>\n#include <linux/init.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/sysctl.h>\n#include <linux/poll.h>\n#include <linux/mqueue.h>\n#include <linux/msg.h>\n#include <linux/skbuff.h>\n#include <linux/vmalloc.h>\n#include <linux/netlink.h>\n#include <linux/syscalls.h>\n#include <linux/audit.h>\n#include <linux/signal.h>\n#include <linux/mutex.h>\n#include <linux/nsproxy.h>\n#include <linux/pid.h>\n#include <linux/ipc_namespace.h>\n#include <linux/user_namespace.h>\n#include <linux/slab.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/user.h>\n\n#include <net/sock.h>\n#include \"util.h\"\n\n#define MQUEUE_MAGIC\t0x19800202\n#define DIRENT_SIZE\t20\n#define FILENT_SIZE\t80\n\n#define SEND\t\t0\n#define RECV\t\t1\n\n#define STATE_NONE\t0\n#define STATE_READY\t1\n\nstruct posix_msg_tree_node {\n\tstruct rb_node\t\trb_node;\n\tstruct list_head\tmsg_list;\n\tint\t\t\tpriority;\n};\n\nstruct ext_wait_queue {\t\t/* queue of sleeping tasks */\n\tstruct task_struct *task;\n\tstruct list_head list;\n\tstruct msg_msg *msg;\t/* ptr of loaded message */\n\tint state;\t\t/* one of STATE_* values */\n};\n\nstruct mqueue_inode_info {\n\tspinlock_t lock;\n\tstruct inode vfs_inode;\n\twait_queue_head_t wait_q;\n\n\tstruct rb_root msg_tree;\n\tstruct posix_msg_tree_node *node_cache;\n\tstruct mq_attr attr;\n\n\tstruct sigevent notify;\n\tstruct pid *notify_owner;\n\tstruct user_namespace *notify_user_ns;\n\tstruct user_struct *user;\t/* user who created, for accounting */\n\tstruct sock *notify_sock;\n\tstruct sk_buff *notify_cookie;\n\n\t/* for tasks waiting for free space and messages, respectively */\n\tstruct ext_wait_queue e_wait_q[2];\n\n\tunsigned long qsize; /* size of queue in memory (sum of all msgs) */\n};\n\nstatic const struct inode_operations mqueue_dir_inode_operations;\nstatic const struct file_operations mqueue_file_operations;\nstatic const struct super_operations mqueue_super_ops;\nstatic void remove_notification(struct mqueue_inode_info *info);\n\nstatic struct kmem_cache *mqueue_inode_cachep;\n\nstatic struct ctl_table_header *mq_sysctl_table;\n\nstatic inline struct mqueue_inode_info *MQUEUE_I(struct inode *inode)\n{\n\treturn container_of(inode, struct mqueue_inode_info, vfs_inode);\n}\n\n/*\n * This routine should be called with the mq_lock held.\n */\nstatic inline struct ipc_namespace *__get_ns_from_inode(struct inode *inode)\n{\n\treturn get_ipc_ns(inode->i_sb->s_fs_info);\n}\n\nstatic struct ipc_namespace *get_ns_from_inode(struct inode *inode)\n{\n\tstruct ipc_namespace *ns;\n\n\tspin_lock(&mq_lock);\n\tns = __get_ns_from_inode(inode);\n\tspin_unlock(&mq_lock);\n\treturn ns;\n}\n\n/* Auxiliary functions to manipulate messages' list */\nstatic int msg_insert(struct msg_msg *msg, struct mqueue_inode_info *info)\n{\n\tstruct rb_node **p, *parent = NULL;\n\tstruct posix_msg_tree_node *leaf;\n\n\tp = &info->msg_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tleaf = rb_entry(parent, struct posix_msg_tree_node, rb_node);\n\n\t\tif (likely(leaf->priority == msg->m_type))\n\t\t\tgoto insert_msg;\n\t\telse if (msg->m_type < leaf->priority)\n\t\t\tp = &(*p)->rb_left;\n\t\telse\n\t\t\tp = &(*p)->rb_right;\n\t}\n\tif (info->node_cache) {\n\t\tleaf = info->node_cache;\n\t\tinfo->node_cache = NULL;\n\t} else {\n\t\tleaf = kmalloc(sizeof(*leaf), GFP_ATOMIC);\n\t\tif (!leaf)\n\t\t\treturn -ENOMEM;\n\t\tINIT_LIST_HEAD(&leaf->msg_list);\n\t}\n\tleaf->priority = msg->m_type;\n\trb_link_node(&leaf->rb_node, parent, p);\n\trb_insert_color(&leaf->rb_node, &info->msg_tree);\ninsert_msg:\n\tinfo->attr.mq_curmsgs++;\n\tinfo->qsize += msg->m_ts;\n\tlist_add_tail(&msg->m_list, &leaf->msg_list);\n\treturn 0;\n}\n\nstatic inline struct msg_msg *msg_get(struct mqueue_inode_info *info)\n{\n\tstruct rb_node **p, *parent = NULL;\n\tstruct posix_msg_tree_node *leaf;\n\tstruct msg_msg *msg;\n\ntry_again:\n\tp = &info->msg_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\t/*\n\t\t * During insert, low priorities go to the left and high to the\n\t\t * right.  On receive, we want the highest priorities first, so\n\t\t * walk all the way to the right.\n\t\t */\n\t\tp = &(*p)->rb_right;\n\t}\n\tif (!parent) {\n\t\tif (info->attr.mq_curmsgs) {\n\t\t\tpr_warn_once(\"Inconsistency in POSIX message queue, \"\n\t\t\t\t     \"no tree element, but supposedly messages \"\n\t\t\t\t     \"should exist!\\n\");\n\t\t\tinfo->attr.mq_curmsgs = 0;\n\t\t}\n\t\treturn NULL;\n\t}\n\tleaf = rb_entry(parent, struct posix_msg_tree_node, rb_node);\n\tif (unlikely(list_empty(&leaf->msg_list))) {\n\t\tpr_warn_once(\"Inconsistency in POSIX message queue, \"\n\t\t\t     \"empty leaf node but we haven't implemented \"\n\t\t\t     \"lazy leaf delete!\\n\");\n\t\trb_erase(&leaf->rb_node, &info->msg_tree);\n\t\tif (info->node_cache) {\n\t\t\tkfree(leaf);\n\t\t} else {\n\t\t\tinfo->node_cache = leaf;\n\t\t}\n\t\tgoto try_again;\n\t} else {\n\t\tmsg = list_first_entry(&leaf->msg_list,\n\t\t\t\t       struct msg_msg, m_list);\n\t\tlist_del(&msg->m_list);\n\t\tif (list_empty(&leaf->msg_list)) {\n\t\t\trb_erase(&leaf->rb_node, &info->msg_tree);\n\t\t\tif (info->node_cache) {\n\t\t\t\tkfree(leaf);\n\t\t\t} else {\n\t\t\t\tinfo->node_cache = leaf;\n\t\t\t}\n\t\t}\n\t}\n\tinfo->attr.mq_curmsgs--;\n\tinfo->qsize -= msg->m_ts;\n\treturn msg;\n}\n\nstatic struct inode *mqueue_get_inode(struct super_block *sb,\n\t\tstruct ipc_namespace *ipc_ns, umode_t mode,\n\t\tstruct mq_attr *attr)\n{\n\tstruct user_struct *u = current_user();\n\tstruct inode *inode;\n\tint ret = -ENOMEM;\n\n\tinode = new_inode(sb);\n\tif (!inode)\n\t\tgoto err;\n\n\tinode->i_ino = get_next_ino();\n\tinode->i_mode = mode;\n\tinode->i_uid = current_fsuid();\n\tinode->i_gid = current_fsgid();\n\tinode->i_mtime = inode->i_ctime = inode->i_atime = current_time(inode);\n\n\tif (S_ISREG(mode)) {\n\t\tstruct mqueue_inode_info *info;\n\t\tunsigned long mq_bytes, mq_treesize;\n\n\t\tinode->i_fop = &mqueue_file_operations;\n\t\tinode->i_size = FILENT_SIZE;\n\t\t/* mqueue specific info */\n\t\tinfo = MQUEUE_I(inode);\n\t\tspin_lock_init(&info->lock);\n\t\tinit_waitqueue_head(&info->wait_q);\n\t\tINIT_LIST_HEAD(&info->e_wait_q[0].list);\n\t\tINIT_LIST_HEAD(&info->e_wait_q[1].list);\n\t\tinfo->notify_owner = NULL;\n\t\tinfo->notify_user_ns = NULL;\n\t\tinfo->qsize = 0;\n\t\tinfo->user = NULL;\t/* set when all is ok */\n\t\tinfo->msg_tree = RB_ROOT;\n\t\tinfo->node_cache = NULL;\n\t\tmemset(&info->attr, 0, sizeof(info->attr));\n\t\tinfo->attr.mq_maxmsg = min(ipc_ns->mq_msg_max,\n\t\t\t\t\t   ipc_ns->mq_msg_default);\n\t\tinfo->attr.mq_msgsize = min(ipc_ns->mq_msgsize_max,\n\t\t\t\t\t    ipc_ns->mq_msgsize_default);\n\t\tif (attr) {\n\t\t\tinfo->attr.mq_maxmsg = attr->mq_maxmsg;\n\t\t\tinfo->attr.mq_msgsize = attr->mq_msgsize;\n\t\t}\n\t\t/*\n\t\t * We used to allocate a static array of pointers and account\n\t\t * the size of that array as well as one msg_msg struct per\n\t\t * possible message into the queue size. That's no longer\n\t\t * accurate as the queue is now an rbtree and will grow and\n\t\t * shrink depending on usage patterns.  We can, however, still\n\t\t * account one msg_msg struct per message, but the nodes are\n\t\t * allocated depending on priority usage, and most programs\n\t\t * only use one, or a handful, of priorities.  However, since\n\t\t * this is pinned memory, we need to assume worst case, so\n\t\t * that means the min(mq_maxmsg, max_priorities) * struct\n\t\t * posix_msg_tree_node.\n\t\t */\n\t\tmq_treesize = info->attr.mq_maxmsg * sizeof(struct msg_msg) +\n\t\t\tmin_t(unsigned int, info->attr.mq_maxmsg, MQ_PRIO_MAX) *\n\t\t\tsizeof(struct posix_msg_tree_node);\n\n\t\tmq_bytes = mq_treesize + (info->attr.mq_maxmsg *\n\t\t\t\t\t  info->attr.mq_msgsize);\n\n\t\tspin_lock(&mq_lock);\n\t\tif (u->mq_bytes + mq_bytes < u->mq_bytes ||\n\t\t    u->mq_bytes + mq_bytes > rlimit(RLIMIT_MSGQUEUE)) {\n\t\t\tspin_unlock(&mq_lock);\n\t\t\t/* mqueue_evict_inode() releases info->messages */\n\t\t\tret = -EMFILE;\n\t\t\tgoto out_inode;\n\t\t}\n\t\tu->mq_bytes += mq_bytes;\n\t\tspin_unlock(&mq_lock);\n\n\t\t/* all is ok */\n\t\tinfo->user = get_uid(u);\n\t} else if (S_ISDIR(mode)) {\n\t\tinc_nlink(inode);\n\t\t/* Some things misbehave if size == 0 on a directory */\n\t\tinode->i_size = 2 * DIRENT_SIZE;\n\t\tinode->i_op = &mqueue_dir_inode_operations;\n\t\tinode->i_fop = &simple_dir_operations;\n\t}\n\n\treturn inode;\nout_inode:\n\tiput(inode);\nerr:\n\treturn ERR_PTR(ret);\n}\n\nstatic int mqueue_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct inode *inode;\n\tstruct ipc_namespace *ns = sb->s_fs_info;\n\n\tsb->s_iflags |= SB_I_NOEXEC | SB_I_NODEV;\n\tsb->s_blocksize = PAGE_SIZE;\n\tsb->s_blocksize_bits = PAGE_SHIFT;\n\tsb->s_magic = MQUEUE_MAGIC;\n\tsb->s_op = &mqueue_super_ops;\n\n\tinode = mqueue_get_inode(sb, ns, S_IFDIR | S_ISVTX | S_IRWXUGO, NULL);\n\tif (IS_ERR(inode))\n\t\treturn PTR_ERR(inode);\n\n\tsb->s_root = d_make_root(inode);\n\tif (!sb->s_root)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic struct dentry *mqueue_mount(struct file_system_type *fs_type,\n\t\t\t int flags, const char *dev_name,\n\t\t\t void *data)\n{\n\tstruct ipc_namespace *ns;\n\tif (flags & MS_KERNMOUNT) {\n\t\tns = data;\n\t\tdata = NULL;\n\t} else {\n\t\tns = current->nsproxy->ipc_ns;\n\t}\n\treturn mount_ns(fs_type, flags, data, ns, ns->user_ns, mqueue_fill_super);\n}\n\nstatic void init_once(void *foo)\n{\n\tstruct mqueue_inode_info *p = (struct mqueue_inode_info *) foo;\n\n\tinode_init_once(&p->vfs_inode);\n}\n\nstatic struct inode *mqueue_alloc_inode(struct super_block *sb)\n{\n\tstruct mqueue_inode_info *ei;\n\n\tei = kmem_cache_alloc(mqueue_inode_cachep, GFP_KERNEL);\n\tif (!ei)\n\t\treturn NULL;\n\treturn &ei->vfs_inode;\n}\n\nstatic void mqueue_i_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tkmem_cache_free(mqueue_inode_cachep, MQUEUE_I(inode));\n}\n\nstatic void mqueue_destroy_inode(struct inode *inode)\n{\n\tcall_rcu(&inode->i_rcu, mqueue_i_callback);\n}\n\nstatic void mqueue_evict_inode(struct inode *inode)\n{\n\tstruct mqueue_inode_info *info;\n\tstruct user_struct *user;\n\tunsigned long mq_bytes, mq_treesize;\n\tstruct ipc_namespace *ipc_ns;\n\tstruct msg_msg *msg;\n\n\tclear_inode(inode);\n\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn;\n\n\tipc_ns = get_ns_from_inode(inode);\n\tinfo = MQUEUE_I(inode);\n\tspin_lock(&info->lock);\n\twhile ((msg = msg_get(info)) != NULL)\n\t\tfree_msg(msg);\n\tkfree(info->node_cache);\n\tspin_unlock(&info->lock);\n\n\t/* Total amount of bytes accounted for the mqueue */\n\tmq_treesize = info->attr.mq_maxmsg * sizeof(struct msg_msg) +\n\t\tmin_t(unsigned int, info->attr.mq_maxmsg, MQ_PRIO_MAX) *\n\t\tsizeof(struct posix_msg_tree_node);\n\n\tmq_bytes = mq_treesize + (info->attr.mq_maxmsg *\n\t\t\t\t  info->attr.mq_msgsize);\n\n\tuser = info->user;\n\tif (user) {\n\t\tspin_lock(&mq_lock);\n\t\tuser->mq_bytes -= mq_bytes;\n\t\t/*\n\t\t * get_ns_from_inode() ensures that the\n\t\t * (ipc_ns = sb->s_fs_info) is either a valid ipc_ns\n\t\t * to which we now hold a reference, or it is NULL.\n\t\t * We can't put it here under mq_lock, though.\n\t\t */\n\t\tif (ipc_ns)\n\t\t\tipc_ns->mq_queues_count--;\n\t\tspin_unlock(&mq_lock);\n\t\tfree_uid(user);\n\t}\n\tif (ipc_ns)\n\t\tput_ipc_ns(ipc_ns);\n}\n\nstatic int mqueue_create(struct inode *dir, struct dentry *dentry,\n\t\t\t\tumode_t mode, bool excl)\n{\n\tstruct inode *inode;\n\tstruct mq_attr *attr = dentry->d_fsdata;\n\tint error;\n\tstruct ipc_namespace *ipc_ns;\n\n\tspin_lock(&mq_lock);\n\tipc_ns = __get_ns_from_inode(dir);\n\tif (!ipc_ns) {\n\t\terror = -EACCES;\n\t\tgoto out_unlock;\n\t}\n\n\tif (ipc_ns->mq_queues_count >= ipc_ns->mq_queues_max &&\n\t    !capable(CAP_SYS_RESOURCE)) {\n\t\terror = -ENOSPC;\n\t\tgoto out_unlock;\n\t}\n\tipc_ns->mq_queues_count++;\n\tspin_unlock(&mq_lock);\n\n\tinode = mqueue_get_inode(dir->i_sb, ipc_ns, mode, attr);\n\tif (IS_ERR(inode)) {\n\t\terror = PTR_ERR(inode);\n\t\tspin_lock(&mq_lock);\n\t\tipc_ns->mq_queues_count--;\n\t\tgoto out_unlock;\n\t}\n\n\tput_ipc_ns(ipc_ns);\n\tdir->i_size += DIRENT_SIZE;\n\tdir->i_ctime = dir->i_mtime = dir->i_atime = current_time(dir);\n\n\td_instantiate(dentry, inode);\n\tdget(dentry);\n\treturn 0;\nout_unlock:\n\tspin_unlock(&mq_lock);\n\tif (ipc_ns)\n\t\tput_ipc_ns(ipc_ns);\n\treturn error;\n}\n\nstatic int mqueue_unlink(struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode = d_inode(dentry);\n\n\tdir->i_ctime = dir->i_mtime = dir->i_atime = current_time(dir);\n\tdir->i_size -= DIRENT_SIZE;\n\tdrop_nlink(inode);\n\tdput(dentry);\n\treturn 0;\n}\n\n/*\n*\tThis is routine for system read from queue file.\n*\tTo avoid mess with doing here some sort of mq_receive we allow\n*\tto read only queue size & notification info (the only values\n*\tthat are interesting from user point of view and aren't accessible\n*\tthrough std routines)\n*/\nstatic ssize_t mqueue_read_file(struct file *filp, char __user *u_data,\n\t\t\t\tsize_t count, loff_t *off)\n{\n\tstruct mqueue_inode_info *info = MQUEUE_I(file_inode(filp));\n\tchar buffer[FILENT_SIZE];\n\tssize_t ret;\n\n\tspin_lock(&info->lock);\n\tsnprintf(buffer, sizeof(buffer),\n\t\t\t\"QSIZE:%-10lu NOTIFY:%-5d SIGNO:%-5d NOTIFY_PID:%-6d\\n\",\n\t\t\tinfo->qsize,\n\t\t\tinfo->notify_owner ? info->notify.sigev_notify : 0,\n\t\t\t(info->notify_owner &&\n\t\t\t info->notify.sigev_notify == SIGEV_SIGNAL) ?\n\t\t\t\tinfo->notify.sigev_signo : 0,\n\t\t\tpid_vnr(info->notify_owner));\n\tspin_unlock(&info->lock);\n\tbuffer[sizeof(buffer)-1] = '\\0';\n\n\tret = simple_read_from_buffer(u_data, count, off, buffer,\n\t\t\t\tstrlen(buffer));\n\tif (ret <= 0)\n\t\treturn ret;\n\n\tfile_inode(filp)->i_atime = file_inode(filp)->i_ctime = current_time(file_inode(filp));\n\treturn ret;\n}\n\nstatic int mqueue_flush_file(struct file *filp, fl_owner_t id)\n{\n\tstruct mqueue_inode_info *info = MQUEUE_I(file_inode(filp));\n\n\tspin_lock(&info->lock);\n\tif (task_tgid(current) == info->notify_owner)\n\t\tremove_notification(info);\n\n\tspin_unlock(&info->lock);\n\treturn 0;\n}\n\nstatic unsigned int mqueue_poll_file(struct file *filp, struct poll_table_struct *poll_tab)\n{\n\tstruct mqueue_inode_info *info = MQUEUE_I(file_inode(filp));\n\tint retval = 0;\n\n\tpoll_wait(filp, &info->wait_q, poll_tab);\n\n\tspin_lock(&info->lock);\n\tif (info->attr.mq_curmsgs)\n\t\tretval = POLLIN | POLLRDNORM;\n\n\tif (info->attr.mq_curmsgs < info->attr.mq_maxmsg)\n\t\tretval |= POLLOUT | POLLWRNORM;\n\tspin_unlock(&info->lock);\n\n\treturn retval;\n}\n\n/* Adds current to info->e_wait_q[sr] before element with smaller prio */\nstatic void wq_add(struct mqueue_inode_info *info, int sr,\n\t\t\tstruct ext_wait_queue *ewp)\n{\n\tstruct ext_wait_queue *walk;\n\n\tewp->task = current;\n\n\tlist_for_each_entry(walk, &info->e_wait_q[sr].list, list) {\n\t\tif (walk->task->static_prio <= current->static_prio) {\n\t\t\tlist_add_tail(&ewp->list, &walk->list);\n\t\t\treturn;\n\t\t}\n\t}\n\tlist_add_tail(&ewp->list, &info->e_wait_q[sr].list);\n}\n\n/*\n * Puts current task to sleep. Caller must hold queue lock. After return\n * lock isn't held.\n * sr: SEND or RECV\n */\nstatic int wq_sleep(struct mqueue_inode_info *info, int sr,\n\t\t    ktime_t *timeout, struct ext_wait_queue *ewp)\n\t__releases(&info->lock)\n{\n\tint retval;\n\tsigned long time;\n\n\twq_add(info, sr, ewp);\n\n\tfor (;;) {\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\n\t\tspin_unlock(&info->lock);\n\t\ttime = schedule_hrtimeout_range_clock(timeout, 0,\n\t\t\tHRTIMER_MODE_ABS, CLOCK_REALTIME);\n\n\t\tif (ewp->state == STATE_READY) {\n\t\t\tretval = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tspin_lock(&info->lock);\n\t\tif (ewp->state == STATE_READY) {\n\t\t\tretval = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tretval = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (time == 0) {\n\t\t\tretval = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t}\n\tlist_del(&ewp->list);\nout_unlock:\n\tspin_unlock(&info->lock);\nout:\n\treturn retval;\n}\n\n/*\n * Returns waiting task that should be serviced first or NULL if none exists\n */\nstatic struct ext_wait_queue *wq_get_first_waiter(\n\t\tstruct mqueue_inode_info *info, int sr)\n{\n\tstruct list_head *ptr;\n\n\tptr = info->e_wait_q[sr].list.prev;\n\tif (ptr == &info->e_wait_q[sr].list)\n\t\treturn NULL;\n\treturn list_entry(ptr, struct ext_wait_queue, list);\n}\n\n\nstatic inline void set_cookie(struct sk_buff *skb, char code)\n{\n\t((char *)skb->data)[NOTIFY_COOKIE_LEN-1] = code;\n}\n\n/*\n * The next function is only to split too long sys_mq_timedsend\n */\nstatic void __do_notify(struct mqueue_inode_info *info)\n{\n\t/* notification\n\t * invoked when there is registered process and there isn't process\n\t * waiting synchronously for message AND state of queue changed from\n\t * empty to not empty. Here we are sure that no one is waiting\n\t * synchronously. */\n\tif (info->notify_owner &&\n\t    info->attr.mq_curmsgs == 1) {\n\t\tstruct siginfo sig_i;\n\t\tswitch (info->notify.sigev_notify) {\n\t\tcase SIGEV_NONE:\n\t\t\tbreak;\n\t\tcase SIGEV_SIGNAL:\n\t\t\t/* sends signal */\n\n\t\t\tsig_i.si_signo = info->notify.sigev_signo;\n\t\t\tsig_i.si_errno = 0;\n\t\t\tsig_i.si_code = SI_MESGQ;\n\t\t\tsig_i.si_value = info->notify.sigev_value;\n\t\t\t/* map current pid/uid into info->owner's namespaces */\n\t\t\trcu_read_lock();\n\t\t\tsig_i.si_pid = task_tgid_nr_ns(current,\n\t\t\t\t\t\tns_of_pid(info->notify_owner));\n\t\t\tsig_i.si_uid = from_kuid_munged(info->notify_user_ns, current_uid());\n\t\t\trcu_read_unlock();\n\n\t\t\tkill_pid_info(info->notify.sigev_signo,\n\t\t\t\t      &sig_i, info->notify_owner);\n\t\t\tbreak;\n\t\tcase SIGEV_THREAD:\n\t\t\tset_cookie(info->notify_cookie, NOTIFY_WOKENUP);\n\t\t\tnetlink_sendskb(info->notify_sock, info->notify_cookie);\n\t\t\tbreak;\n\t\t}\n\t\t/* after notification unregisters process */\n\t\tput_pid(info->notify_owner);\n\t\tput_user_ns(info->notify_user_ns);\n\t\tinfo->notify_owner = NULL;\n\t\tinfo->notify_user_ns = NULL;\n\t}\n\twake_up(&info->wait_q);\n}\n\nstatic int prepare_timeout(const struct timespec __user *u_abs_timeout,\n\t\t\t   struct timespec *ts)\n{\n\tif (copy_from_user(ts, u_abs_timeout, sizeof(struct timespec)))\n\t\treturn -EFAULT;\n\tif (!timespec_valid(ts))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic void remove_notification(struct mqueue_inode_info *info)\n{\n\tif (info->notify_owner != NULL &&\n\t    info->notify.sigev_notify == SIGEV_THREAD) {\n\t\tset_cookie(info->notify_cookie, NOTIFY_REMOVED);\n\t\tnetlink_sendskb(info->notify_sock, info->notify_cookie);\n\t}\n\tput_pid(info->notify_owner);\n\tput_user_ns(info->notify_user_ns);\n\tinfo->notify_owner = NULL;\n\tinfo->notify_user_ns = NULL;\n}\n\nstatic int mq_attr_ok(struct ipc_namespace *ipc_ns, struct mq_attr *attr)\n{\n\tint mq_treesize;\n\tunsigned long total_size;\n\n\tif (attr->mq_maxmsg <= 0 || attr->mq_msgsize <= 0)\n\t\treturn -EINVAL;\n\tif (capable(CAP_SYS_RESOURCE)) {\n\t\tif (attr->mq_maxmsg > HARD_MSGMAX ||\n\t\t    attr->mq_msgsize > HARD_MSGSIZEMAX)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr->mq_maxmsg > ipc_ns->mq_msg_max ||\n\t\t\t\tattr->mq_msgsize > ipc_ns->mq_msgsize_max)\n\t\t\treturn -EINVAL;\n\t}\n\t/* check for overflow */\n\tif (attr->mq_msgsize > ULONG_MAX/attr->mq_maxmsg)\n\t\treturn -EOVERFLOW;\n\tmq_treesize = attr->mq_maxmsg * sizeof(struct msg_msg) +\n\t\tmin_t(unsigned int, attr->mq_maxmsg, MQ_PRIO_MAX) *\n\t\tsizeof(struct posix_msg_tree_node);\n\ttotal_size = attr->mq_maxmsg * attr->mq_msgsize;\n\tif (total_size + mq_treesize < total_size)\n\t\treturn -EOVERFLOW;\n\treturn 0;\n}\n\n/*\n * Invoked when creating a new queue via sys_mq_open\n */\nstatic struct file *do_create(struct ipc_namespace *ipc_ns, struct inode *dir,\n\t\t\tstruct path *path, int oflag, umode_t mode,\n\t\t\tstruct mq_attr *attr)\n{\n\tconst struct cred *cred = current_cred();\n\tint ret;\n\n\tif (attr) {\n\t\tret = mq_attr_ok(ipc_ns, attr);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\t/* store for use during create */\n\t\tpath->dentry->d_fsdata = attr;\n\t} else {\n\t\tstruct mq_attr def_attr;\n\n\t\tdef_attr.mq_maxmsg = min(ipc_ns->mq_msg_max,\n\t\t\t\t\t ipc_ns->mq_msg_default);\n\t\tdef_attr.mq_msgsize = min(ipc_ns->mq_msgsize_max,\n\t\t\t\t\t  ipc_ns->mq_msgsize_default);\n\t\tret = mq_attr_ok(ipc_ns, &def_attr);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t}\n\n\tmode &= ~current_umask();\n\tret = vfs_create(dir, path->dentry, mode, true);\n\tpath->dentry->d_fsdata = NULL;\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\treturn dentry_open(path, oflag, cred);\n}\n\n/* Opens existing queue */\nstatic struct file *do_open(struct path *path, int oflag)\n{\n\tstatic const int oflag2acc[O_ACCMODE] = { MAY_READ, MAY_WRITE,\n\t\t\t\t\t\t  MAY_READ | MAY_WRITE };\n\tint acc;\n\tif ((oflag & O_ACCMODE) == (O_RDWR | O_WRONLY))\n\t\treturn ERR_PTR(-EINVAL);\n\tacc = oflag2acc[oflag & O_ACCMODE];\n\tif (inode_permission(d_inode(path->dentry), acc))\n\t\treturn ERR_PTR(-EACCES);\n\treturn dentry_open(path, oflag, current_cred());\n}\n\nstatic int do_mq_open(const char __user *u_name, int oflag, umode_t mode,\n\t\t      struct mq_attr *attr)\n{\n\tstruct path path;\n\tstruct file *filp;\n\tstruct filename *name;\n\tint fd, error;\n\tstruct ipc_namespace *ipc_ns = current->nsproxy->ipc_ns;\n\tstruct vfsmount *mnt = ipc_ns->mq_mnt;\n\tstruct dentry *root = mnt->mnt_root;\n\tint ro;\n\n\taudit_mq_open(oflag, mode, attr);\n\n\tif (IS_ERR(name = getname(u_name)))\n\t\treturn PTR_ERR(name);\n\n\tfd = get_unused_fd_flags(O_CLOEXEC);\n\tif (fd < 0)\n\t\tgoto out_putname;\n\n\tro = mnt_want_write(mnt);\t/* we'll drop it in any case */\n\terror = 0;\n\tinode_lock(d_inode(root));\n\tpath.dentry = lookup_one_len(name->name, root, strlen(name->name));\n\tif (IS_ERR(path.dentry)) {\n\t\terror = PTR_ERR(path.dentry);\n\t\tgoto out_putfd;\n\t}\n\tpath.mnt = mntget(mnt);\n\n\tif (oflag & O_CREAT) {\n\t\tif (d_really_is_positive(path.dentry)) {\t/* entry already exists */\n\t\t\taudit_inode(name, path.dentry, 0);\n\t\t\tif (oflag & O_EXCL) {\n\t\t\t\terror = -EEXIST;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tfilp = do_open(&path, oflag);\n\t\t} else {\n\t\t\tif (ro) {\n\t\t\t\terror = ro;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\taudit_inode_parent_hidden(name, root);\n\t\t\tfilp = do_create(ipc_ns, d_inode(root), &path,\n\t\t\t\t\t oflag, mode, attr);\n\t\t}\n\t} else {\n\t\tif (d_really_is_negative(path.dentry)) {\n\t\t\terror = -ENOENT;\n\t\t\tgoto out;\n\t\t}\n\t\taudit_inode(name, path.dentry, 0);\n\t\tfilp = do_open(&path, oflag);\n\t}\n\n\tif (!IS_ERR(filp))\n\t\tfd_install(fd, filp);\n\telse\n\t\terror = PTR_ERR(filp);\nout:\n\tpath_put(&path);\nout_putfd:\n\tif (error) {\n\t\tput_unused_fd(fd);\n\t\tfd = error;\n\t}\n\tinode_unlock(d_inode(root));\n\tif (!ro)\n\t\tmnt_drop_write(mnt);\nout_putname:\n\tputname(name);\n\treturn fd;\n}\n\nSYSCALL_DEFINE4(mq_open, const char __user *, u_name, int, oflag, umode_t, mode,\n\t\tstruct mq_attr __user *, u_attr)\n{\n\tstruct mq_attr attr;\n\tif (u_attr && copy_from_user(&attr, u_attr, sizeof(struct mq_attr)))\n\t\treturn -EFAULT;\n\n\treturn do_mq_open(u_name, oflag, mode, u_attr ? &attr : NULL);\n}\n\nSYSCALL_DEFINE1(mq_unlink, const char __user *, u_name)\n{\n\tint err;\n\tstruct filename *name;\n\tstruct dentry *dentry;\n\tstruct inode *inode = NULL;\n\tstruct ipc_namespace *ipc_ns = current->nsproxy->ipc_ns;\n\tstruct vfsmount *mnt = ipc_ns->mq_mnt;\n\n\tname = getname(u_name);\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\n\taudit_inode_parent_hidden(name, mnt->mnt_root);\n\terr = mnt_want_write(mnt);\n\tif (err)\n\t\tgoto out_name;\n\tinode_lock_nested(d_inode(mnt->mnt_root), I_MUTEX_PARENT);\n\tdentry = lookup_one_len(name->name, mnt->mnt_root,\n\t\t\t\tstrlen(name->name));\n\tif (IS_ERR(dentry)) {\n\t\terr = PTR_ERR(dentry);\n\t\tgoto out_unlock;\n\t}\n\n\tinode = d_inode(dentry);\n\tif (!inode) {\n\t\terr = -ENOENT;\n\t} else {\n\t\tihold(inode);\n\t\terr = vfs_unlink(d_inode(dentry->d_parent), dentry, NULL);\n\t}\n\tdput(dentry);\n\nout_unlock:\n\tinode_unlock(d_inode(mnt->mnt_root));\n\tif (inode)\n\t\tiput(inode);\n\tmnt_drop_write(mnt);\nout_name:\n\tputname(name);\n\n\treturn err;\n}\n\n/* Pipelined send and receive functions.\n *\n * If a receiver finds no waiting message, then it registers itself in the\n * list of waiting receivers. A sender checks that list before adding the new\n * message into the message array. If there is a waiting receiver, then it\n * bypasses the message array and directly hands the message over to the\n * receiver. The receiver accepts the message and returns without grabbing the\n * queue spinlock:\n *\n * - Set pointer to message.\n * - Queue the receiver task for later wakeup (without the info->lock).\n * - Update its state to STATE_READY. Now the receiver can continue.\n * - Wake up the process after the lock is dropped. Should the process wake up\n *   before this wakeup (due to a timeout or a signal) it will either see\n *   STATE_READY and continue or acquire the lock to check the state again.\n *\n * The same algorithm is used for senders.\n */\n\n/* pipelined_send() - send a message directly to the task waiting in\n * sys_mq_timedreceive() (without inserting message into a queue).\n */\nstatic inline void pipelined_send(struct wake_q_head *wake_q,\n\t\t\t\t  struct mqueue_inode_info *info,\n\t\t\t\t  struct msg_msg *message,\n\t\t\t\t  struct ext_wait_queue *receiver)\n{\n\treceiver->msg = message;\n\tlist_del(&receiver->list);\n\twake_q_add(wake_q, receiver->task);\n\t/*\n\t * Rely on the implicit cmpxchg barrier from wake_q_add such\n\t * that we can ensure that updating receiver->state is the last\n\t * write operation: As once set, the receiver can continue,\n\t * and if we don't have the reference count from the wake_q,\n\t * yet, at that point we can later have a use-after-free\n\t * condition and bogus wakeup.\n\t */\n\treceiver->state = STATE_READY;\n}\n\n/* pipelined_receive() - if there is task waiting in sys_mq_timedsend()\n * gets its message and put to the queue (we have one free place for sure). */\nstatic inline void pipelined_receive(struct wake_q_head *wake_q,\n\t\t\t\t     struct mqueue_inode_info *info)\n{\n\tstruct ext_wait_queue *sender = wq_get_first_waiter(info, SEND);\n\n\tif (!sender) {\n\t\t/* for poll */\n\t\twake_up_interruptible(&info->wait_q);\n\t\treturn;\n\t}\n\tif (msg_insert(sender->msg, info))\n\t\treturn;\n\n\tlist_del(&sender->list);\n\twake_q_add(wake_q, sender->task);\n\tsender->state = STATE_READY;\n}\n\nstatic int do_mq_timedsend(mqd_t mqdes, const char __user *u_msg_ptr,\n\t\tsize_t msg_len, unsigned int msg_prio,\n\t\tstruct timespec *ts)\n{\n\tstruct fd f;\n\tstruct inode *inode;\n\tstruct ext_wait_queue wait;\n\tstruct ext_wait_queue *receiver;\n\tstruct msg_msg *msg_ptr;\n\tstruct mqueue_inode_info *info;\n\tktime_t expires, *timeout = NULL;\n\tstruct posix_msg_tree_node *new_leaf = NULL;\n\tint ret = 0;\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (unlikely(msg_prio >= (unsigned long) MQ_PRIO_MAX))\n\t\treturn -EINVAL;\n\n\tif (ts) {\n\t\texpires = timespec_to_ktime(*ts);\n\t\ttimeout = &expires;\n\t}\n\n\taudit_mq_sendrecv(mqdes, msg_len, msg_prio, ts);\n\n\tf = fdget(mqdes);\n\tif (unlikely(!f.file)) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\tinode = file_inode(f.file);\n\tif (unlikely(f.file->f_op != &mqueue_file_operations)) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\tinfo = MQUEUE_I(inode);\n\taudit_file(f.file);\n\n\tif (unlikely(!(f.file->f_mode & FMODE_WRITE))) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\n\tif (unlikely(msg_len > info->attr.mq_msgsize)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out_fput;\n\t}\n\n\t/* First try to allocate memory, before doing anything with\n\t * existing queues. */\n\tmsg_ptr = load_msg(u_msg_ptr, msg_len);\n\tif (IS_ERR(msg_ptr)) {\n\t\tret = PTR_ERR(msg_ptr);\n\t\tgoto out_fput;\n\t}\n\tmsg_ptr->m_ts = msg_len;\n\tmsg_ptr->m_type = msg_prio;\n\n\t/*\n\t * msg_insert really wants us to have a valid, spare node struct so\n\t * it doesn't have to kmalloc a GFP_ATOMIC allocation, but it will\n\t * fall back to that if necessary.\n\t */\n\tif (!info->node_cache)\n\t\tnew_leaf = kmalloc(sizeof(*new_leaf), GFP_KERNEL);\n\n\tspin_lock(&info->lock);\n\n\tif (!info->node_cache && new_leaf) {\n\t\t/* Save our speculative allocation into the cache */\n\t\tINIT_LIST_HEAD(&new_leaf->msg_list);\n\t\tinfo->node_cache = new_leaf;\n\t\tnew_leaf = NULL;\n\t} else {\n\t\tkfree(new_leaf);\n\t}\n\n\tif (info->attr.mq_curmsgs == info->attr.mq_maxmsg) {\n\t\tif (f.file->f_flags & O_NONBLOCK) {\n\t\t\tret = -EAGAIN;\n\t\t} else {\n\t\t\twait.task = current;\n\t\t\twait.msg = (void *) msg_ptr;\n\t\t\twait.state = STATE_NONE;\n\t\t\tret = wq_sleep(info, SEND, timeout, &wait);\n\t\t\t/*\n\t\t\t * wq_sleep must be called with info->lock held, and\n\t\t\t * returns with the lock released\n\t\t\t */\n\t\t\tgoto out_free;\n\t\t}\n\t} else {\n\t\treceiver = wq_get_first_waiter(info, RECV);\n\t\tif (receiver) {\n\t\t\tpipelined_send(&wake_q, info, msg_ptr, receiver);\n\t\t} else {\n\t\t\t/* adds message to the queue */\n\t\t\tret = msg_insert(msg_ptr, info);\n\t\t\tif (ret)\n\t\t\t\tgoto out_unlock;\n\t\t\t__do_notify(info);\n\t\t}\n\t\tinode->i_atime = inode->i_mtime = inode->i_ctime =\n\t\t\t\tcurrent_time(inode);\n\t}\nout_unlock:\n\tspin_unlock(&info->lock);\n\twake_up_q(&wake_q);\nout_free:\n\tif (ret)\n\t\tfree_msg(msg_ptr);\nout_fput:\n\tfdput(f);\nout:\n\treturn ret;\n}\n\nstatic int do_mq_timedreceive(mqd_t mqdes, char __user *u_msg_ptr,\n\t\tsize_t msg_len, unsigned int __user *u_msg_prio,\n\t\tstruct timespec *ts)\n{\n\tssize_t ret;\n\tstruct msg_msg *msg_ptr;\n\tstruct fd f;\n\tstruct inode *inode;\n\tstruct mqueue_inode_info *info;\n\tstruct ext_wait_queue wait;\n\tktime_t expires, *timeout = NULL;\n\tstruct posix_msg_tree_node *new_leaf = NULL;\n\n\tif (ts) {\n\t\texpires = timespec_to_ktime(*ts);\n\t\ttimeout = &expires;\n\t}\n\n\taudit_mq_sendrecv(mqdes, msg_len, 0, ts);\n\n\tf = fdget(mqdes);\n\tif (unlikely(!f.file)) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\tinode = file_inode(f.file);\n\tif (unlikely(f.file->f_op != &mqueue_file_operations)) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\tinfo = MQUEUE_I(inode);\n\taudit_file(f.file);\n\n\tif (unlikely(!(f.file->f_mode & FMODE_READ))) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\n\t/* checks if buffer is big enough */\n\tif (unlikely(msg_len < info->attr.mq_msgsize)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out_fput;\n\t}\n\n\t/*\n\t * msg_insert really wants us to have a valid, spare node struct so\n\t * it doesn't have to kmalloc a GFP_ATOMIC allocation, but it will\n\t * fall back to that if necessary.\n\t */\n\tif (!info->node_cache)\n\t\tnew_leaf = kmalloc(sizeof(*new_leaf), GFP_KERNEL);\n\n\tspin_lock(&info->lock);\n\n\tif (!info->node_cache && new_leaf) {\n\t\t/* Save our speculative allocation into the cache */\n\t\tINIT_LIST_HEAD(&new_leaf->msg_list);\n\t\tinfo->node_cache = new_leaf;\n\t} else {\n\t\tkfree(new_leaf);\n\t}\n\n\tif (info->attr.mq_curmsgs == 0) {\n\t\tif (f.file->f_flags & O_NONBLOCK) {\n\t\t\tspin_unlock(&info->lock);\n\t\t\tret = -EAGAIN;\n\t\t} else {\n\t\t\twait.task = current;\n\t\t\twait.state = STATE_NONE;\n\t\t\tret = wq_sleep(info, RECV, timeout, &wait);\n\t\t\tmsg_ptr = wait.msg;\n\t\t}\n\t} else {\n\t\tDEFINE_WAKE_Q(wake_q);\n\n\t\tmsg_ptr = msg_get(info);\n\n\t\tinode->i_atime = inode->i_mtime = inode->i_ctime =\n\t\t\t\tcurrent_time(inode);\n\n\t\t/* There is now free space in queue. */\n\t\tpipelined_receive(&wake_q, info);\n\t\tspin_unlock(&info->lock);\n\t\twake_up_q(&wake_q);\n\t\tret = 0;\n\t}\n\tif (ret == 0) {\n\t\tret = msg_ptr->m_ts;\n\n\t\tif ((u_msg_prio && put_user(msg_ptr->m_type, u_msg_prio)) ||\n\t\t\tstore_msg(u_msg_ptr, msg_ptr, msg_ptr->m_ts)) {\n\t\t\tret = -EFAULT;\n\t\t}\n\t\tfree_msg(msg_ptr);\n\t}\nout_fput:\n\tfdput(f);\nout:\n\treturn ret;\n}\n\nSYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,\n\t\tsize_t, msg_len, unsigned int, msg_prio,\n\t\tconst struct timespec __user *, u_abs_timeout)\n{\n\tstruct timespec ts, *p = NULL;\n\tif (u_abs_timeout) {\n\t\tint res = prepare_timeout(u_abs_timeout, &ts);\n\t\tif (res)\n\t\t\treturn res;\n\t\tp = &ts;\n\t}\n\treturn do_mq_timedsend(mqdes, u_msg_ptr, msg_len, msg_prio, p);\n}\n\nSYSCALL_DEFINE5(mq_timedreceive, mqd_t, mqdes, char __user *, u_msg_ptr,\n\t\tsize_t, msg_len, unsigned int __user *, u_msg_prio,\n\t\tconst struct timespec __user *, u_abs_timeout)\n{\n\tstruct timespec ts, *p = NULL;\n\tif (u_abs_timeout) {\n\t\tint res = prepare_timeout(u_abs_timeout, &ts);\n\t\tif (res)\n\t\t\treturn res;\n\t\tp = &ts;\n\t}\n\treturn do_mq_timedreceive(mqdes, u_msg_ptr, msg_len, u_msg_prio, p);\n}\n\n/*\n * Notes: the case when user wants us to deregister (with NULL as pointer)\n * and he isn't currently owner of notification, will be silently discarded.\n * It isn't explicitly defined in the POSIX.\n */\nstatic int do_mq_notify(mqd_t mqdes, const struct sigevent *notification)\n{\n\tint ret;\n\tstruct fd f;\n\tstruct sock *sock;\n\tstruct inode *inode;\n\tstruct mqueue_inode_info *info;\n\tstruct sk_buff *nc;\n\n\taudit_mq_notify(mqdes, notification);\n\n\tnc = NULL;\n\tsock = NULL;\n\tif (notification != NULL) {\n\t\tif (unlikely(notification->sigev_notify != SIGEV_NONE &&\n\t\t\t     notification->sigev_notify != SIGEV_SIGNAL &&\n\t\t\t     notification->sigev_notify != SIGEV_THREAD))\n\t\t\treturn -EINVAL;\n\t\tif (notification->sigev_notify == SIGEV_SIGNAL &&\n\t\t\t!valid_signal(notification->sigev_signo)) {\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (notification->sigev_notify == SIGEV_THREAD) {\n\t\t\tlong timeo;\n\n\t\t\t/* create the notify skb */\n\t\t\tnc = alloc_skb(NOTIFY_COOKIE_LEN, GFP_KERNEL);\n\t\t\tif (!nc) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (copy_from_user(nc->data,\n\t\t\t\t\tnotification->sigev_value.sival_ptr,\n\t\t\t\t\tNOTIFY_COOKIE_LEN)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/* TODO: add a header? */\n\t\t\tskb_put(nc, NOTIFY_COOKIE_LEN);\n\t\t\t/* and attach it to the socket */\nretry:\n\t\t\tf = fdget(notification->sigev_signo);\n\t\t\tif (!f.file) {\n\t\t\t\tret = -EBADF;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsock = netlink_getsockbyfilp(f.file);\n\t\t\tfdput(f);\n\t\t\tif (IS_ERR(sock)) {\n\t\t\t\tret = PTR_ERR(sock);\n\t\t\t\tsock = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\ttimeo = MAX_SCHEDULE_TIMEOUT;\n\t\t\tret = netlink_attachskb(sock, nc, &timeo, NULL);\n\t\t\tif (ret == 1) {\n\t\t\t\tsock = NULL;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tsock = NULL;\n\t\t\t\tnc = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tf = fdget(mqdes);\n\tif (!f.file) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\tinode = file_inode(f.file);\n\tif (unlikely(f.file->f_op != &mqueue_file_operations)) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\tinfo = MQUEUE_I(inode);\n\n\tret = 0;\n\tspin_lock(&info->lock);\n\tif (notification == NULL) {\n\t\tif (info->notify_owner == task_tgid(current)) {\n\t\t\tremove_notification(info);\n\t\t\tinode->i_atime = inode->i_ctime = current_time(inode);\n\t\t}\n\t} else if (info->notify_owner != NULL) {\n\t\tret = -EBUSY;\n\t} else {\n\t\tswitch (notification->sigev_notify) {\n\t\tcase SIGEV_NONE:\n\t\t\tinfo->notify.sigev_notify = SIGEV_NONE;\n\t\t\tbreak;\n\t\tcase SIGEV_THREAD:\n\t\t\tinfo->notify_sock = sock;\n\t\t\tinfo->notify_cookie = nc;\n\t\t\tsock = NULL;\n\t\t\tnc = NULL;\n\t\t\tinfo->notify.sigev_notify = SIGEV_THREAD;\n\t\t\tbreak;\n\t\tcase SIGEV_SIGNAL:\n\t\t\tinfo->notify.sigev_signo = notification->sigev_signo;\n\t\t\tinfo->notify.sigev_value = notification->sigev_value;\n\t\t\tinfo->notify.sigev_notify = SIGEV_SIGNAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tinfo->notify_owner = get_pid(task_tgid(current));\n\t\tinfo->notify_user_ns = get_user_ns(current_user_ns());\n\t\tinode->i_atime = inode->i_ctime = current_time(inode);\n\t}\n\tspin_unlock(&info->lock);\nout_fput:\n\tfdput(f);\nout:\n\tif (sock)\n\t\tnetlink_detachskb(sock, nc);\n\telse if (nc)\n\t\tdev_kfree_skb(nc);\n\n\treturn ret;\n}\n\nSYSCALL_DEFINE2(mq_notify, mqd_t, mqdes,\n\t\tconst struct sigevent __user *, u_notification)\n{\n\tstruct sigevent n, *p = NULL;\n\tif (u_notification) {\n\t\tif (copy_from_user(&n, u_notification, sizeof(struct sigevent)))\n\t\t\treturn -EFAULT;\n\t\tp = &n;\n\t}\n\treturn do_mq_notify(mqdes, p);\n}\n\nstatic int do_mq_getsetattr(int mqdes, struct mq_attr *new, struct mq_attr *old)\n{\n\tstruct fd f;\n\tstruct inode *inode;\n\tstruct mqueue_inode_info *info;\n\n\tif (new && (new->mq_flags & (~O_NONBLOCK)))\n\t\treturn -EINVAL;\n\n\tf = fdget(mqdes);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tif (unlikely(f.file->f_op != &mqueue_file_operations)) {\n\t\tfdput(f);\n\t\treturn -EBADF;\n\t}\n\n\tinode = file_inode(f.file);\n\tinfo = MQUEUE_I(inode);\n\n\tspin_lock(&info->lock);\n\n\tif (old) {\n\t\t*old = info->attr;\n\t\told->mq_flags = f.file->f_flags & O_NONBLOCK;\n\t}\n\tif (new) {\n\t\taudit_mq_getsetattr(mqdes, new);\n\t\tspin_lock(&f.file->f_lock);\n\t\tif (new->mq_flags & O_NONBLOCK)\n\t\t\tf.file->f_flags |= O_NONBLOCK;\n\t\telse\n\t\t\tf.file->f_flags &= ~O_NONBLOCK;\n\t\tspin_unlock(&f.file->f_lock);\n\n\t\tinode->i_atime = inode->i_ctime = current_time(inode);\n\t}\n\n\tspin_unlock(&info->lock);\n\tfdput(f);\n\treturn 0;\n}\n\nSYSCALL_DEFINE3(mq_getsetattr, mqd_t, mqdes,\n\t\tconst struct mq_attr __user *, u_mqstat,\n\t\tstruct mq_attr __user *, u_omqstat)\n{\n\tint ret;\n\tstruct mq_attr mqstat, omqstat;\n\tstruct mq_attr *new = NULL, *old = NULL;\n\n\tif (u_mqstat) {\n\t\tnew = &mqstat;\n\t\tif (copy_from_user(new, u_mqstat, sizeof(struct mq_attr)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (u_omqstat)\n\t\told = &omqstat;\n\n\tret = do_mq_getsetattr(mqdes, new, old);\n\tif (ret || !old)\n\t\treturn ret;\n\n\tif (copy_to_user(u_omqstat, old, sizeof(struct mq_attr)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n#ifdef CONFIG_COMPAT\n\nstruct compat_mq_attr {\n\tcompat_long_t mq_flags;      /* message queue flags\t\t     */\n\tcompat_long_t mq_maxmsg;     /* maximum number of messages\t     */\n\tcompat_long_t mq_msgsize;    /* maximum message size\t\t     */\n\tcompat_long_t mq_curmsgs;    /* number of messages currently queued  */\n\tcompat_long_t __reserved[4]; /* ignored for input, zeroed for output */\n};\n\nstatic inline int get_compat_mq_attr(struct mq_attr *attr,\n\t\t\tconst struct compat_mq_attr __user *uattr)\n{\n\tstruct compat_mq_attr v;\n\n\tif (copy_from_user(&v, uattr, sizeof(*uattr)))\n\t\treturn -EFAULT;\n\n\tmemset(attr, 0, sizeof(*attr));\n\tattr->mq_flags = v.mq_flags;\n\tattr->mq_maxmsg = v.mq_maxmsg;\n\tattr->mq_msgsize = v.mq_msgsize;\n\tattr->mq_curmsgs = v.mq_curmsgs;\n\treturn 0;\n}\n\nstatic inline int put_compat_mq_attr(const struct mq_attr *attr,\n\t\t\tstruct compat_mq_attr __user *uattr)\n{\n\tstruct compat_mq_attr v;\n\n\tmemset(&v, 0, sizeof(v));\n\tv.mq_flags = attr->mq_flags;\n\tv.mq_maxmsg = attr->mq_maxmsg;\n\tv.mq_msgsize = attr->mq_msgsize;\n\tv.mq_curmsgs = attr->mq_curmsgs;\n\tif (copy_to_user(uattr, &v, sizeof(*uattr)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nCOMPAT_SYSCALL_DEFINE4(mq_open, const char __user *, u_name,\n\t\t       int, oflag, compat_mode_t, mode,\n\t\t       struct compat_mq_attr __user *, u_attr)\n{\n\tstruct mq_attr attr, *p = NULL;\n\tif (u_attr && oflag & O_CREAT) {\n\t\tp = &attr;\n\t\tif (get_compat_mq_attr(&attr, u_attr))\n\t\t\treturn -EFAULT;\n\t}\n\treturn do_mq_open(u_name, oflag, mode, p);\n}\n\nstatic int compat_prepare_timeout(const struct compat_timespec __user *p,\n\t\t\t\t   struct timespec *ts)\n{\n\tif (compat_get_timespec(ts, p))\n\t\treturn -EFAULT;\n\tif (!timespec_valid(ts))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nCOMPAT_SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes,\n\t\t       const char __user *, u_msg_ptr,\n\t\t       compat_size_t, msg_len, unsigned int, msg_prio,\n\t\t       const struct compat_timespec __user *, u_abs_timeout)\n{\n\tstruct timespec ts, *p = NULL;\n\tif (u_abs_timeout) {\n\t\tint res = compat_prepare_timeout(u_abs_timeout, &ts);\n\t\tif (res)\n\t\t\treturn res;\n\t\tp = &ts;\n\t}\n\treturn do_mq_timedsend(mqdes, u_msg_ptr, msg_len, msg_prio, p);\n}\n\nCOMPAT_SYSCALL_DEFINE5(mq_timedreceive, mqd_t, mqdes,\n\t\t       char __user *, u_msg_ptr,\n\t\t       compat_size_t, msg_len, unsigned int __user *, u_msg_prio,\n\t\t       const struct compat_timespec __user *, u_abs_timeout)\n{\n\tstruct timespec ts, *p = NULL;\n\tif (u_abs_timeout) {\n\t\tint res = compat_prepare_timeout(u_abs_timeout, &ts);\n\t\tif (res)\n\t\t\treturn res;\n\t\tp = &ts;\n\t}\n\treturn do_mq_timedreceive(mqdes, u_msg_ptr, msg_len, u_msg_prio, p);\n}\n\nCOMPAT_SYSCALL_DEFINE2(mq_notify, mqd_t, mqdes,\n\t\t       const struct compat_sigevent __user *, u_notification)\n{\n\tstruct sigevent n, *p = NULL;\n\tif (u_notification) {\n\t\tif (get_compat_sigevent(&n, u_notification))\n\t\t\treturn -EFAULT;\n\t\tif (n.sigev_notify == SIGEV_THREAD)\n\t\t\tn.sigev_value.sival_ptr = compat_ptr(n.sigev_value.sival_int);\n\t\tp = &n;\n\t}\n\treturn do_mq_notify(mqdes, p);\n}\n\nCOMPAT_SYSCALL_DEFINE3(mq_getsetattr, mqd_t, mqdes,\n\t\t       const struct compat_mq_attr __user *, u_mqstat,\n\t\t       struct compat_mq_attr __user *, u_omqstat)\n{\n\tint ret;\n\tstruct mq_attr mqstat, omqstat;\n\tstruct mq_attr *new = NULL, *old = NULL;\n\n\tif (u_mqstat) {\n\t\tnew = &mqstat;\n\t\tif (get_compat_mq_attr(new, u_mqstat))\n\t\t\treturn -EFAULT;\n\t}\n\tif (u_omqstat)\n\t\told = &omqstat;\n\n\tret = do_mq_getsetattr(mqdes, new, old);\n\tif (ret || !old)\n\t\treturn ret;\n\n\tif (put_compat_mq_attr(old, u_omqstat))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n#endif\n\nstatic const struct inode_operations mqueue_dir_inode_operations = {\n\t.lookup = simple_lookup,\n\t.create = mqueue_create,\n\t.unlink = mqueue_unlink,\n};\n\nstatic const struct file_operations mqueue_file_operations = {\n\t.flush = mqueue_flush_file,\n\t.poll = mqueue_poll_file,\n\t.read = mqueue_read_file,\n\t.llseek = default_llseek,\n};\n\nstatic const struct super_operations mqueue_super_ops = {\n\t.alloc_inode = mqueue_alloc_inode,\n\t.destroy_inode = mqueue_destroy_inode,\n\t.evict_inode = mqueue_evict_inode,\n\t.statfs = simple_statfs,\n};\n\nstatic struct file_system_type mqueue_fs_type = {\n\t.name = \"mqueue\",\n\t.mount = mqueue_mount,\n\t.kill_sb = kill_litter_super,\n\t.fs_flags = FS_USERNS_MOUNT,\n};\n\nint mq_init_ns(struct ipc_namespace *ns)\n{\n\tns->mq_queues_count  = 0;\n\tns->mq_queues_max    = DFLT_QUEUESMAX;\n\tns->mq_msg_max       = DFLT_MSGMAX;\n\tns->mq_msgsize_max   = DFLT_MSGSIZEMAX;\n\tns->mq_msg_default   = DFLT_MSG;\n\tns->mq_msgsize_default  = DFLT_MSGSIZE;\n\n\tns->mq_mnt = kern_mount_data(&mqueue_fs_type, ns);\n\tif (IS_ERR(ns->mq_mnt)) {\n\t\tint err = PTR_ERR(ns->mq_mnt);\n\t\tns->mq_mnt = NULL;\n\t\treturn err;\n\t}\n\treturn 0;\n}\n\nvoid mq_clear_sbinfo(struct ipc_namespace *ns)\n{\n\tns->mq_mnt->mnt_sb->s_fs_info = NULL;\n}\n\nvoid mq_put_mnt(struct ipc_namespace *ns)\n{\n\tkern_unmount(ns->mq_mnt);\n}\n\nstatic int __init init_mqueue_fs(void)\n{\n\tint error;\n\n\tmqueue_inode_cachep = kmem_cache_create(\"mqueue_inode_cache\",\n\t\t\t\tsizeof(struct mqueue_inode_info), 0,\n\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_ACCOUNT, init_once);\n\tif (mqueue_inode_cachep == NULL)\n\t\treturn -ENOMEM;\n\n\t/* ignore failures - they are not fatal */\n\tmq_sysctl_table = mq_register_sysctl_table();\n\n\terror = register_filesystem(&mqueue_fs_type);\n\tif (error)\n\t\tgoto out_sysctl;\n\n\tspin_lock_init(&mq_lock);\n\n\terror = mq_init_ns(&init_ipc_ns);\n\tif (error)\n\t\tgoto out_filesystem;\n\n\treturn 0;\n\nout_filesystem:\n\tunregister_filesystem(&mqueue_fs_type);\nout_sysctl:\n\tif (mq_sysctl_table)\n\t\tunregister_sysctl_table(mq_sysctl_table);\n\tkmem_cache_destroy(mqueue_inode_cachep);\n\treturn error;\n}\n\ndevice_initcall(init_mqueue_fs);\n"], "filenames": ["ipc/mqueue.c"], "buggy_code_start_loc": [1273], "buggy_code_end_loc": [1274], "fixing_code_start_loc": [1273], "fixing_code_end_loc": [1277], "type": "CWE-416", "message": "The mq_notify function in the Linux kernel through 4.11.9 does not set the sock pointer to NULL upon entry into the retry logic. During a user-space close of a Netlink socket, it allows attackers to cause a denial of service (use-after-free) or possibly have unspecified other impact.", "other": {"cve": {"id": "CVE-2017-11176", "sourceIdentifier": "cve@mitre.org", "published": "2017-07-11T23:29:00.213", "lastModified": "2023-01-17T21:01:46.437", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The mq_notify function in the Linux kernel through 4.11.9 does not set the sock pointer to NULL upon entry into the retry logic. During a user-space close of a Netlink socket, it allows attackers to cause a denial of service (use-after-free) or possibly have unspecified other impact."}, {"lang": "es", "value": "La funci\u00f3n mq_notify en el kernel de Linux hasta versi\u00f3n 4.11.9 no se ajusta el puntero sock a NULL al ingresar a la l\u00f3gica de reintento. Durante un cierre de espacio de usuario de un socket Netlink, permite que los atacantes causen una denegaci\u00f3n de servicio (uso de memoria previamente liberada) o posiblemente tengan otro impacto no especificado."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.2.92", "matchCriteriaId": "21888989-B8CA-4696-9FE4-9CDA1B3B92AE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.3", "versionEndExcluding": "3.16.47", "matchCriteriaId": "18369FD7-D135-4C78-BA5E-8FF5F4573485"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.61", "matchCriteriaId": "737CDAED-1476-433D-A1D3-7460323FF392"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.43", "matchCriteriaId": "5F6B255F-0933-4983-B3F6-AD5B128A8F04"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.77", "matchCriteriaId": "F09881FD-5BBD-4E0F-88C0-C795EE420DC5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.9.38", "matchCriteriaId": "E384283D-0BD4-4873-A961-1B6CD1A5F451"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.11.11", "matchCriteriaId": "034014AF-629B-4611-9567-8F875FB8A29D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.12", "versionEndExcluding": "4.12.2", "matchCriteriaId": "132D6674-8244-452B-ADBA-2B0136E4435E"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=f991af3daabaecff34684fd51fac80319d1baad1", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.debian.org/security/2017/dsa-3927", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.debian.org/security/2017/dsa-3945", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/99919", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2918", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2930", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2931", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:0169", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3822", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/f991af3daabaecff34684fd51fac80319d1baad1", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://help.ecostruxureit.com/display/public/UADCE725/Security+fixes+in+StruxureWare+Data+Center+Expert+v7.6.0", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.exploit-db.com/exploits/45553/", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/f991af3daabaecff34684fd51fac80319d1baad1"}}