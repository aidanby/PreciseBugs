{"buggy_code": ["from __future__ import annotations\n\nimport contextvars\nimport logging\nimport pprint\nimport typing as t\nfrom abc import ABCMeta, abstractmethod\n\nfrom piccolo.querystring import QueryString\nfrom piccolo.utils.sync import run_sync\nfrom piccolo.utils.warnings import Level, colored_string, colored_warning\n\nif t.TYPE_CHECKING:  # pragma: no cover\n    from piccolo.query.base import Query\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Batch:\n    pass\n\n\nTransactionClass = t.TypeVar(\"TransactionClass\")\n\n\nclass Engine(t.Generic[TransactionClass], metaclass=ABCMeta):\n\n    __slots__ = (\"query_id\",)\n\n    def __init__(self):\n        run_sync(self.check_version())\n        run_sync(self.prep_database())\n        self.query_id = 0\n\n    @property\n    @abstractmethod\n    def engine_type(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def min_version_number(self) -> float:\n        pass\n\n    @abstractmethod\n    async def get_version(self) -> float:\n        pass\n\n    @abstractmethod\n    def get_version_sync(self) -> float:\n        pass\n\n    @abstractmethod\n    async def prep_database(self):\n        pass\n\n    @abstractmethod\n    async def batch(\n        self,\n        query: Query,\n        batch_size: int = 100,\n        node: t.Optional[str] = None,\n    ) -> Batch:\n        pass\n\n    @abstractmethod\n    async def run_querystring(self, querystring: QueryString, in_pool: bool):\n        pass\n\n    @abstractmethod\n    async def run_ddl(self, ddl: str, in_pool: bool = True):\n        pass\n\n    @abstractmethod\n    def transaction(self):\n        pass\n\n    @abstractmethod\n    def atomic(self):\n        pass\n\n    async def check_version(self):\n        \"\"\"\n        Warn if the database version isn't supported.\n        \"\"\"\n        try:\n            version_number = await self.get_version()\n        except Exception as exception:\n            colored_warning(\n                f\"Unable to fetch server version: {exception}\",\n                level=Level.high,\n            )\n            return\n\n        engine_type = self.engine_type.capitalize()\n        logger.info(f\"Running {engine_type} version {version_number}\")\n        if version_number and (version_number < self.min_version_number):\n            message = (\n                f\"This version of {self.engine_type} isn't supported \"\n                f\"(< {self.min_version_number}) - some features might not be \"\n                \"available. For instructions on installing databases, see the \"\n                \"Piccolo docs.\"\n            )\n            colored_warning(message, stacklevel=3)\n\n    def _connection_pool_warning(self):\n        message = (\n            f\"Connection pooling is not supported for {self.engine_type}.\"\n        )\n        logger.warning(message)\n        colored_warning(message, stacklevel=3)\n\n    async def start_connection_pool(self):\n        \"\"\"\n        The database driver doesn't implement connection pooling.\n        \"\"\"\n        self._connection_pool_warning()\n\n    async def close_connection_pool(self):\n        \"\"\"\n        The database driver doesn't implement connection pooling.\n        \"\"\"\n        self._connection_pool_warning()\n\n    ###########################################################################\n\n    current_transaction: contextvars.ContextVar[t.Optional[TransactionClass]]\n\n    def transaction_exists(self) -> bool:\n        \"\"\"\n        Find out if a transaction is currently active.\n\n        :returns:\n            ``True`` if a transaction is already active for the current\n            asyncio task. This is useful to know, because nested transactions\n            aren't currently supported, so you can check if an existing\n            transaction is already active, before creating a new one.\n\n        \"\"\"\n        return self.current_transaction.get() is not None\n\n    ###########################################################################\n    # Logging queries and responses\n\n    def get_query_id(self) -> int:\n        self.query_id += 1\n        return self.query_id\n\n    def print_query(self, query_id: int, query: str):\n        print(colored_string(f\"\\nQuery {query_id}:\"))\n        print(query)\n\n    def print_response(self, query_id: int, response: t.List):\n        print(\n            colored_string(f\"\\nQuery {query_id} response:\", level=Level.high)\n        )\n        pprint.pprint(response)\n", "from __future__ import annotations\n\nimport contextvars\nimport typing as t\nfrom dataclasses import dataclass\n\nfrom piccolo.engine.base import Batch, Engine\nfrom piccolo.engine.exceptions import TransactionError\nfrom piccolo.query.base import DDL, Query\nfrom piccolo.querystring import QueryString\nfrom piccolo.utils.lazy_loader import LazyLoader\nfrom piccolo.utils.sync import run_sync\nfrom piccolo.utils.warnings import Level, colored_warning\n\nasyncpg = LazyLoader(\"asyncpg\", globals(), \"asyncpg\")\n\nif t.TYPE_CHECKING:  # pragma: no cover\n    from asyncpg.connection import Connection\n    from asyncpg.cursor import Cursor\n    from asyncpg.pool import Pool\n\n\n@dataclass\nclass AsyncBatch(Batch):\n\n    connection: Connection\n    query: Query\n    batch_size: int\n\n    # Set internally\n    _transaction = None\n    _cursor: t.Optional[Cursor] = None\n\n    @property\n    def cursor(self) -> Cursor:\n        if not self._cursor:\n            raise ValueError(\"_cursor not set\")\n        return self._cursor\n\n    async def next(self) -> t.List[t.Dict]:\n        data = await self.cursor.fetch(self.batch_size)\n        return await self.query._process_results(data)\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        response = await self.next()\n        if response == []:\n            raise StopAsyncIteration()\n        return response\n\n    async def __aenter__(self):\n        self._transaction = self.connection.transaction()\n        await self._transaction.start()\n        querystring = self.query.querystrings[0]\n        template, template_args = querystring.compile_string()\n\n        self._cursor = await self.connection.cursor(template, *template_args)\n        return self\n\n    async def __aexit__(self, exception_type, exception, traceback):\n        if exception:\n            await self._transaction.rollback()\n        else:\n            await self._transaction.commit()\n\n        await self.connection.close()\n\n        return exception is not None\n\n\n###############################################################################\n\n\nclass Atomic:\n    \"\"\"\n    This is useful if you want to build up a transaction programatically, by\n    adding queries to it.\n\n    Usage::\n\n        transaction = engine.atomic()\n        transaction.add(Foo.create_table())\n\n        # Either:\n        transaction.run_sync()\n        await transaction.run()\n\n    \"\"\"\n\n    __slots__ = (\"engine\", \"queries\")\n\n    def __init__(self, engine: PostgresEngine):\n        self.engine = engine\n        self.queries: t.List[Query] = []\n\n    def add(self, *query: Query):\n        self.queries += list(query)\n\n    async def run(self):\n        from piccolo.query.methods.objects import Create, GetOrCreate\n\n        try:\n            async with self.engine.transaction():\n                for query in self.queries:\n                    if isinstance(query, (Query, DDL, Create, GetOrCreate)):\n                        await query.run()\n                    else:\n                        raise ValueError(\"Unrecognised query\")\n            self.queries = []\n        except Exception as exception:\n            self.queries = []\n            raise exception from exception\n\n    def run_sync(self):\n        return run_sync(self.run())\n\n    def __await__(self):\n        return self.run().__await__()\n\n\n###############################################################################\n\n\nclass Savepoint:\n    def __init__(self, name: str, transaction: PostgresTransaction):\n        self.name = name\n        self.transaction = transaction\n\n    async def rollback_to(self):\n        await self.transaction.connection.execute(\n            f\"ROLLBACK TO SAVEPOINT {self.name}\"\n        )\n\n    async def release(self):\n        await self.transaction.connection.execute(\n            f\"RELEASE SAVEPOINT {self.name}\"\n        )\n\n\nclass PostgresTransaction:\n    \"\"\"\n    Used for wrapping queries in a transaction, using a context manager.\n    Currently it's async only.\n\n    Usage::\n\n        async with engine.transaction():\n            # Run some queries:\n            await Band.select().run()\n\n    \"\"\"\n\n    __slots__ = (\n        \"engine\",\n        \"transaction\",\n        \"context\",\n        \"connection\",\n        \"_savepoint_id\",\n        \"_parent\",\n        \"_committed\",\n        \"_rolled_back\",\n    )\n\n    def __init__(self, engine: PostgresEngine, allow_nested: bool = True):\n        \"\"\"\n        :param allow_nested:\n            If ``True`` then if we try creating a new transaction when another\n            is already active, we treat this as a no-op::\n\n                async with DB.transaction():\n                    async with DB.transaction():\n                        pass\n\n            If we want to disallow this behaviour, then setting\n            ``allow_nested=False`` will cause a ``TransactionError`` to be\n            raised.\n\n        \"\"\"\n        self.engine = engine\n        current_transaction = self.engine.current_transaction.get()\n\n        self._savepoint_id = 0\n        self._parent = None\n        self._committed = False\n        self._rolled_back = False\n\n        if current_transaction:\n            if allow_nested:\n                self._parent = current_transaction\n            else:\n                raise TransactionError(\n                    \"A transaction is already active - nested transactions \"\n                    \"aren't allowed.\"\n                )\n\n    async def __aenter__(self) -> PostgresTransaction:\n        if self._parent is not None:\n            return self._parent\n\n        self.connection = await self.get_connection()\n        self.transaction = self.connection.transaction()\n        await self.begin()\n        self.context = self.engine.current_transaction.set(self)\n        return self\n\n    async def get_connection(self):\n        if self.engine.pool:\n            return await self.engine.pool.acquire()\n        else:\n            return await self.engine.get_new_connection()\n\n    async def begin(self):\n        await self.transaction.start()\n\n    async def commit(self):\n        await self.transaction.commit()\n        self._committed = True\n\n    async def rollback(self):\n        await self.transaction.rollback()\n        self._rolled_back = True\n\n    async def rollback_to(self, savepoint_name: str):\n        \"\"\"\n        Used to rollback to a savepoint just using the name.\n        \"\"\"\n        await Savepoint(name=savepoint_name, transaction=self).rollback_to()\n\n    ###########################################################################\n\n    def get_savepoint_id(self) -> int:\n        self._savepoint_id += 1\n        return self._savepoint_id\n\n    async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:\n        name = name or f\"savepoint_{self.get_savepoint_id()}\"\n        await self.connection.execute(f\"SAVEPOINT {name}\")\n        return Savepoint(name=name, transaction=self)\n\n    ###########################################################################\n\n    async def __aexit__(self, exception_type, exception, traceback):\n        if self._parent:\n            return exception is None\n\n        if exception:\n            # The user may have manually rolled it back.\n            if not self._rolled_back:\n                await self.rollback()\n        else:\n            # The user may have manually committed it.\n            if not self._committed and not self._rolled_back:\n                await self.commit()\n\n        if self.engine.pool:\n            await self.engine.pool.release(self.connection)\n        else:\n            await self.connection.close()\n\n        self.engine.current_transaction.reset(self.context)\n\n        return exception is None\n\n\n###############################################################################\n\n\nclass PostgresEngine(Engine[t.Optional[PostgresTransaction]]):\n    \"\"\"\n    Used to connect to PostgreSQL.\n\n    :param config:\n        The config dictionary is passed to the underlying database adapter,\n        asyncpg. Common arguments you're likely to need are:\n\n        * host\n        * port\n        * user\n        * password\n        * database\n\n        For example, ``{'host': 'localhost', 'port': 5432}``.\n\n        See the `asyncpg docs <https://magicstack.github.io/asyncpg/current/api/index.html#connection>`_\n        for all available options.\n\n    :param extensions:\n        When the engine starts, it will try and create these extensions\n        in Postgres. If you're using a read only database, set this value to an\n        empty tuple ``()``.\n\n    :param log_queries:\n        If ``True``, all SQL and DDL statements are printed out before being\n        run. Useful for debugging.\n\n    :param log_responses:\n        If ``True``, the raw response from each query is printed out. Useful\n        for debugging.\n\n    :param extra_nodes:\n        If you have additional database nodes (e.g. read replicas) for the\n        server, you can specify them here. It's a mapping of a memorable name\n        to a ``PostgresEngine`` instance. For example::\n\n            DB = PostgresEngine(\n                config={'database': 'main_db'},\n                extra_nodes={\n                    'read_replica_1': PostgresEngine(\n                        config={\n                            'database': 'main_db',\n                            host: 'read_replicate.my_db.com'\n                        },\n                        extensions=()\n                    )\n                }\n            )\n\n        Note how we set ``extensions=()``, because it's a read only database.\n\n        When executing a query, you can specify one of these nodes instead\n        of the main database. For example::\n\n            >>> await MyTable.select().run(node=\"read_replica_1\")\n\n    \"\"\"  # noqa: E501\n\n    __slots__ = (\n        \"config\",\n        \"extensions\",\n        \"log_queries\",\n        \"log_responses\",\n        \"extra_nodes\",\n        \"pool\",\n        \"current_transaction\",\n    )\n\n    engine_type = \"postgres\"\n    min_version_number = 10\n\n    def __init__(\n        self,\n        config: t.Dict[str, t.Any],\n        extensions: t.Sequence[str] = (\"uuid-ossp\",),\n        log_queries: bool = False,\n        log_responses: bool = False,\n        extra_nodes: t.Mapping[str, PostgresEngine] = None,\n    ) -> None:\n        if extra_nodes is None:\n            extra_nodes = {}\n\n        self.config = config\n        self.extensions = extensions\n        self.log_queries = log_queries\n        self.log_responses = log_responses\n        self.extra_nodes = extra_nodes\n        self.pool: t.Optional[Pool] = None\n        database_name = config.get(\"database\", \"Unknown\")\n        self.current_transaction = contextvars.ContextVar(\n            f\"pg_current_transaction_{database_name}\", default=None\n        )\n        super().__init__()\n\n    @staticmethod\n    def _parse_raw_version_string(version_string: str) -> float:\n        \"\"\"\n        The format of the version string isn't always consistent. Sometimes\n        it's just the version number e.g. '9.6.18', and sometimes\n        it contains specific build information e.g.\n        '12.4 (Ubuntu 12.4-0ubuntu0.20.04.1)'. Just extract the major and\n        minor version numbers.\n        \"\"\"\n        version_segment = version_string.split(\" \")[0]\n        major, minor = version_segment.split(\".\")[:2]\n        return float(f\"{major}.{minor}\")\n\n    async def get_version(self) -> float:\n        \"\"\"\n        Returns the version of Postgres being run.\n        \"\"\"\n        try:\n            response: t.Sequence[t.Dict] = await self._run_in_new_connection(\n                \"SHOW server_version\"\n            )\n        except ConnectionRefusedError as exception:\n            # Suppressing the exception, otherwise importing piccolo_conf.py\n            # containing an engine will raise an ImportError.\n            colored_warning(f\"Unable to connect to database - {exception}\")\n            return 0.0\n        else:\n            version_string = response[0][\"server_version\"]\n            return self._parse_raw_version_string(\n                version_string=version_string\n            )\n\n    def get_version_sync(self) -> float:\n        return run_sync(self.get_version())\n\n    async def prep_database(self):\n        for extension in self.extensions:\n            try:\n                await self._run_in_new_connection(\n                    f'CREATE EXTENSION IF NOT EXISTS \"{extension}\"',\n                )\n            except asyncpg.exceptions.InsufficientPrivilegeError:\n                colored_warning(\n                    f\"=> Unable to create {extension} extension - some \"\n                    \"functionality may not behave as expected. Make sure \"\n                    \"your database user has permission to create \"\n                    \"extensions, or add it manually using \"\n                    f'`CREATE EXTENSION \"{extension}\";`',\n                    level=Level.medium,\n                )\n\n    ###########################################################################\n    # These typos existed in the codebase for a while, so leaving these proxy\n    # methods for now to ensure backwards compatibility.\n\n    async def start_connnection_pool(self, **kwargs) -> None:\n        colored_warning(\n            \"`start_connnection_pool` is a typo - please change it to \"\n            \"`start_connection_pool`.\",\n            category=DeprecationWarning,\n        )\n        return await self.start_connection_pool()\n\n    async def close_connnection_pool(self, **kwargs) -> None:\n        colored_warning(\n            \"`close_connnection_pool` is a typo - please change it to \"\n            \"`close_connection_pool`.\",\n            category=DeprecationWarning,\n        )\n        return await self.close_connection_pool()\n\n    ###########################################################################\n\n    async def start_connection_pool(self, **kwargs) -> None:\n        if self.pool:\n            colored_warning(\n                \"A pool already exists - close it first if you want to create \"\n                \"a new pool.\",\n            )\n        else:\n            config = dict(self.config)\n            config.update(**kwargs)\n            self.pool = await asyncpg.create_pool(**config)\n\n    async def close_connection_pool(self) -> None:\n        if self.pool:\n            await self.pool.close()\n            self.pool = None\n        else:\n            colored_warning(\"No pool is running.\")\n\n    ###########################################################################\n\n    async def get_new_connection(self) -> Connection:\n        \"\"\"\n        Returns a new connection - doesn't retrieve it from the pool.\n        \"\"\"\n        return await asyncpg.connect(**self.config)\n\n    ###########################################################################\n\n    async def batch(\n        self,\n        query: Query,\n        batch_size: int = 100,\n        node: t.Optional[str] = None,\n    ) -> AsyncBatch:\n        \"\"\"\n        :param query:\n            The database query to run.\n        :param batch_size:\n            How many rows to fetch on each iteration.\n        :param node:\n            Which node to run the query on (see ``extra_nodes``). If not\n            specified, it runs on the main Postgres node.\n        \"\"\"\n        engine: t.Any = self.extra_nodes.get(node) if node else self\n        connection = await engine.get_new_connection()\n        return AsyncBatch(\n            connection=connection, query=query, batch_size=batch_size\n        )\n\n    ###########################################################################\n\n    async def _run_in_pool(self, query: str, args: t.Sequence[t.Any] = None):\n        if args is None:\n            args = []\n        if not self.pool:\n            raise ValueError(\"A pool isn't currently running.\")\n\n        async with self.pool.acquire() as connection:\n            response = await connection.fetch(query, *args)\n\n        return response\n\n    async def _run_in_new_connection(\n        self, query: str, args: t.Sequence[t.Any] = None\n    ):\n        if args is None:\n            args = []\n        connection = await self.get_new_connection()\n\n        try:\n            results = await connection.fetch(query, *args)\n        except asyncpg.exceptions.PostgresError as exception:\n            await connection.close()\n            raise exception\n\n        await connection.close()\n        return results\n\n    async def run_querystring(\n        self, querystring: QueryString, in_pool: bool = True\n    ):\n        query, query_args = querystring.compile_string(\n            engine_type=self.engine_type\n        )\n\n        query_id = self.get_query_id()\n\n        if self.log_queries:\n            self.print_query(query_id=query_id, query=querystring.__str__())\n\n        # If running inside a transaction:\n        current_transaction = self.current_transaction.get()\n        if current_transaction:\n            response = await current_transaction.connection.fetch(\n                query, *query_args\n            )\n        elif in_pool and self.pool:\n            response = await self._run_in_pool(query, query_args)\n        else:\n            response = await self._run_in_new_connection(query, query_args)\n\n        if self.log_responses:\n            self.print_response(query_id=query_id, response=response)\n\n        return response\n\n    async def run_ddl(self, ddl: str, in_pool: bool = True):\n        query_id = self.get_query_id()\n\n        if self.log_queries:\n            self.print_query(query_id=query_id, query=ddl)\n\n        # If running inside a transaction:\n        current_transaction = self.current_transaction.get()\n        if current_transaction:\n            response = await current_transaction.connection.fetch(ddl)\n        elif in_pool and self.pool:\n            response = await self._run_in_pool(ddl)\n        else:\n            response = await self._run_in_new_connection(ddl)\n\n        if self.log_responses:\n            self.print_response(query_id=query_id, response=response)\n\n        return response\n\n    def atomic(self) -> Atomic:\n        return Atomic(engine=self)\n\n    def transaction(self, allow_nested: bool = True) -> PostgresTransaction:\n        return PostgresTransaction(engine=self, allow_nested=allow_nested)\n", "from __future__ import annotations\n\nimport contextvars\nimport datetime\nimport enum\nimport os\nimport sqlite3\nimport typing as t\nimport uuid\nfrom dataclasses import dataclass\nfrom decimal import Decimal\n\nfrom piccolo.engine.base import Batch, Engine\nfrom piccolo.engine.exceptions import TransactionError\nfrom piccolo.query.base import DDL, Query\nfrom piccolo.querystring import QueryString\nfrom piccolo.utils.encoding import dump_json, load_json\nfrom piccolo.utils.lazy_loader import LazyLoader\nfrom piccolo.utils.sync import run_sync\n\naiosqlite = LazyLoader(\"aiosqlite\", globals(), \"aiosqlite\")\n\n\nif t.TYPE_CHECKING:  # pragma: no cover\n    from aiosqlite import Connection, Cursor  # type: ignore\n\n    from piccolo.table import Table\n\n###############################################################################\n\n# We need to register some adapters so sqlite returns types which are more\n# consistent with the Postgres engine.\n\n\n# In\n\n\ndef convert_numeric_in(value):\n    \"\"\"\n    Convert any Decimal values into floats.\n    \"\"\"\n    return float(value)\n\n\ndef convert_uuid_in(value) -> str:\n    \"\"\"\n    Converts the UUID value being passed into sqlite.\n    \"\"\"\n    return str(value)\n\n\ndef convert_time_in(value: datetime.time) -> str:\n    \"\"\"\n    Converts the time value being passed into sqlite.\n    \"\"\"\n    return value.isoformat()\n\n\ndef convert_date_in(value: datetime.date):\n    \"\"\"\n    Converts the date value being passed into sqlite.\n    \"\"\"\n    return value.isoformat()\n\n\ndef convert_datetime_in(value: datetime.datetime) -> str:\n    \"\"\"\n    Converts the datetime into a string. If it's timezone aware, we want to\n    convert it to UTC first. This is to replicate Postgres, which stores\n    timezone aware datetimes in UTC.\n    \"\"\"\n    if value.tzinfo is not None:\n        value = value.astimezone(datetime.timezone.utc)\n    return str(value)\n\n\ndef convert_timedelta_in(value: datetime.timedelta):\n    \"\"\"\n    Converts the timedelta value being passed into sqlite.\n    \"\"\"\n    return value.total_seconds()\n\n\ndef convert_array_in(value: list):\n    \"\"\"\n    Converts a list value into a string.\n    \"\"\"\n    if value and type(value[0]) not in [str, int, float]:\n        raise ValueError(\"Can only serialise str, int and float.\")\n\n    return dump_json(value)\n\n\n# Out\n\n\ndef convert_numeric_out(value: bytes) -> Decimal:\n    \"\"\"\n    Convert float values into Decimals.\n    \"\"\"\n    return Decimal(value.decode(\"ascii\"))\n\n\ndef convert_int_out(value: bytes) -> int:\n    \"\"\"\n    Make sure Integer values are actually of type int.\n    \"\"\"\n    return int(float(value))\n\n\ndef convert_uuid_out(value: bytes) -> uuid.UUID:\n    \"\"\"\n    If the value is a uuid, convert it to a UUID instance.\n    \"\"\"\n    return uuid.UUID(value.decode(\"utf8\"))\n\n\ndef convert_date_out(value: bytes) -> datetime.date:\n    return datetime.date.fromisoformat(value.decode(\"utf8\"))\n\n\ndef convert_time_out(value: bytes) -> datetime.time:\n    \"\"\"\n    If the value is a time, convert it to a UUID instance.\n    \"\"\"\n    return datetime.time.fromisoformat(value.decode(\"utf8\"))\n\n\ndef convert_seconds_out(value: bytes) -> datetime.timedelta:\n    \"\"\"\n    If the value is from a seconds column, convert it to a timedelta instance.\n    \"\"\"\n    return datetime.timedelta(seconds=float(value.decode(\"utf8\")))\n\n\ndef convert_boolean_out(value: bytes) -> bool:\n    \"\"\"\n    If the value is from a boolean column, convert it to a bool value.\n    \"\"\"\n    _value = value.decode(\"utf8\")\n    return _value == \"1\"\n\n\ndef convert_timestamp_out(value: bytes) -> datetime.datetime:\n    \"\"\"\n    If the value is from a timestamp column, convert it to a datetime value.\n    \"\"\"\n    return datetime.datetime.fromisoformat(value.decode(\"utf8\"))\n\n\ndef convert_timestamptz_out(value: bytes) -> datetime.datetime:\n    \"\"\"\n    If the value is from a timestamptz column, convert it to a datetime value,\n    with a timezone of UTC.\n    \"\"\"\n    _value = datetime.datetime.fromisoformat(value.decode(\"utf8\"))\n    _value = _value.replace(tzinfo=datetime.timezone.utc)\n    return _value\n\n\ndef convert_array_out(value: bytes) -> t.List:\n    \"\"\"\n    If the value if from an array column, deserialise the string back into a\n    list.\n    \"\"\"\n    return load_json(value.decode(\"utf8\"))\n\n\ndef convert_M2M_out(value: bytes) -> t.List:\n    _value = value.decode(\"utf8\")\n    return _value.split(\",\")\n\n\nsqlite3.register_converter(\"Numeric\", convert_numeric_out)\nsqlite3.register_converter(\"Integer\", convert_int_out)\nsqlite3.register_converter(\"UUID\", convert_uuid_out)\nsqlite3.register_converter(\"Date\", convert_date_out)\nsqlite3.register_converter(\"Time\", convert_time_out)\nsqlite3.register_converter(\"Seconds\", convert_seconds_out)\nsqlite3.register_converter(\"Boolean\", convert_boolean_out)\nsqlite3.register_converter(\"Timestamp\", convert_timestamp_out)\nsqlite3.register_converter(\"Timestamptz\", convert_timestamptz_out)\nsqlite3.register_converter(\"Array\", convert_array_out)\nsqlite3.register_converter(\"M2M\", convert_M2M_out)\n\nsqlite3.register_adapter(Decimal, convert_numeric_in)\nsqlite3.register_adapter(uuid.UUID, convert_uuid_in)\nsqlite3.register_adapter(datetime.time, convert_time_in)\nsqlite3.register_adapter(datetime.date, convert_date_in)\nsqlite3.register_adapter(datetime.datetime, convert_datetime_in)\nsqlite3.register_adapter(datetime.timedelta, convert_timedelta_in)\nsqlite3.register_adapter(list, convert_array_in)\n\n###############################################################################\n\n\n@dataclass\nclass AsyncBatch(Batch):\n\n    connection: Connection\n    query: Query\n    batch_size: int\n\n    # Set internally\n    _cursor: t.Optional[Cursor] = None\n\n    @property\n    def cursor(self) -> Cursor:\n        if not self._cursor:\n            raise ValueError(\"_cursor not set\")\n        return self._cursor\n\n    async def next(self) -> t.List[t.Dict]:\n        data = await self.cursor.fetchmany(self.batch_size)\n        return await self.query._process_results(data)\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        response = await self.next()\n        if response == []:\n            raise StopAsyncIteration()\n        return response\n\n    async def __aenter__(self):\n        querystring = self.query.querystrings[0]\n        template, template_args = querystring.compile_string()\n\n        self._cursor = await self.connection.execute(template, *template_args)\n        return self\n\n    async def __aexit__(self, exception_type, exception, traceback):\n        await self._cursor.close()\n        await self.connection.close()\n        return exception is not None\n\n\n###############################################################################\n\n\nclass TransactionType(enum.Enum):\n    \"\"\"\n    See the `SQLite <https://www.sqlite.org/lang_transaction.html>`_ docs for\n    more info.\n    \"\"\"\n\n    deferred = \"DEFERRED\"\n    immediate = \"IMMEDIATE\"\n    exclusive = \"EXCLUSIVE\"\n\n\nclass Atomic:\n    \"\"\"\n    Usage:\n\n    transaction = engine.atomic()\n    transaction.add(Foo.create_table())\n\n    # Either:\n    transaction.run_sync()\n    await transaction.run()\n    \"\"\"\n\n    __slots__ = (\"engine\", \"queries\", \"transaction_type\")\n\n    def __init__(\n        self,\n        engine: SQLiteEngine,\n        transaction_type: TransactionType = TransactionType.deferred,\n    ):\n        self.engine = engine\n        self.transaction_type = transaction_type\n        self.queries: t.List[Query] = []\n\n    def add(self, *query: Query):\n        self.queries += list(query)\n\n    async def run(self):\n        from piccolo.query.methods.objects import Create, GetOrCreate\n\n        try:\n            async with self.engine.transaction(\n                transaction_type=self.transaction_type\n            ):\n                for query in self.queries:\n                    if isinstance(query, (Query, DDL, Create, GetOrCreate)):\n                        await query.run()\n                    else:\n                        raise ValueError(\"Unrecognised query\")\n            self.queries = []\n        except Exception as exception:\n            self.queries = []\n            raise exception from exception\n\n    def run_sync(self):\n        return run_sync(self.run())\n\n    def __await__(self):\n        return self.run().__await__()\n\n\n###############################################################################\n\n\nclass Savepoint:\n    def __init__(self, name: str, transaction: SQLiteTransaction):\n        self.name = name\n        self.transaction = transaction\n\n    async def rollback_to(self):\n        await self.transaction.connection.execute(\n            f\"ROLLBACK TO SAVEPOINT {self.name}\"\n        )\n\n    async def release(self):\n        await self.transaction.connection.execute(\n            f\"RELEASE SAVEPOINT {self.name}\"\n        )\n\n\nclass SQLiteTransaction:\n    \"\"\"\n    Used for wrapping queries in a transaction, using a context manager.\n    Currently it's async only.\n\n    Usage::\n\n        async with engine.transaction():\n            # Run some queries:\n            await Band.select().run()\n\n    \"\"\"\n\n    __slots__ = (\n        \"engine\",\n        \"context\",\n        \"connection\",\n        \"transaction_type\",\n        \"allow_nested\",\n        \"_savepoint_id\",\n        \"_parent\",\n        \"_committed\",\n        \"_rolled_back\",\n    )\n\n    def __init__(\n        self,\n        engine: SQLiteEngine,\n        transaction_type: TransactionType = TransactionType.deferred,\n        allow_nested: bool = True,\n    ):\n        \"\"\"\n        :param transaction_type:\n            If your transaction just contains ``SELECT`` queries, then use\n            ``TransactionType.deferred``. This will give you the best\n            performance. When performing ``INSERT``, ``UPDATE``, ``DELETE``\n            queries, we recommend using ``TransactionType.immediate`` to\n            avoid database locks.\n        \"\"\"\n        self.engine = engine\n        self.transaction_type = transaction_type\n        current_transaction = self.engine.current_transaction.get()\n\n        self._savepoint_id = 0\n        self._parent = None\n        self._committed = False\n        self._rolled_back = False\n\n        if current_transaction:\n            if allow_nested:\n                self._parent = current_transaction\n            else:\n                raise TransactionError(\n                    \"A transaction is already active - nested transactions \"\n                    \"aren't allowed.\"\n                )\n\n    async def __aenter__(self) -> SQLiteTransaction:\n        if self._parent is not None:\n            return self._parent\n\n        self.connection = await self.get_connection()\n        await self.begin()\n        self.context = self.engine.current_transaction.set(self)\n        return self\n\n    async def get_connection(self):\n        return await self.engine.get_connection()\n\n    async def begin(self):\n        await self.connection.execute(f\"BEGIN {self.transaction_type.value}\")\n\n    async def commit(self):\n        await self.connection.execute(\"COMMIT\")\n        self._committed = True\n\n    async def rollback(self):\n        await self.connection.execute(\"ROLLBACK\")\n        self._rolled_back = True\n\n    async def rollback_to(self, savepoint_name: str):\n        \"\"\"\n        Used to rollback to a savepoint just using the name.\n        \"\"\"\n        await Savepoint(name=savepoint_name, transaction=self).rollback_to()\n\n    ###########################################################################\n\n    def get_savepoint_id(self) -> int:\n        self._savepoint_id += 1\n        return self._savepoint_id\n\n    async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:\n        name = name or f\"savepoint_{self.get_savepoint_id()}\"\n        await self.connection.execute(f\"SAVEPOINT {name}\")\n        return Savepoint(name=name, transaction=self)\n\n    ###########################################################################\n\n    async def __aexit__(self, exception_type, exception, traceback):\n        if self._parent:\n            return exception is None\n\n        if exception:\n            # The user may have manually rolled it back.\n            if not self._rolled_back:\n                await self.rollback()\n        else:\n            # The user may have manually committed it.\n            if not self._committed and not self._rolled_back:\n                await self.commit()\n\n        await self.connection.close()\n        self.engine.current_transaction.reset(self.context)\n\n        return exception is None\n\n\n###############################################################################\n\n\ndef dict_factory(cursor, row) -> t.Dict:\n    return {col[0]: row[idx] for idx, col in enumerate(cursor.description)}\n\n\nclass SQLiteEngine(Engine[t.Optional[SQLiteTransaction]]):\n\n    __slots__ = (\n        \"connection_kwargs\",\n        \"current_transaction\",\n        \"log_queries\",\n        \"log_responses\",\n    )\n\n    engine_type = \"sqlite\"\n    min_version_number = 3.25\n\n    def __init__(\n        self,\n        path: str = \"piccolo.sqlite\",\n        log_queries: bool = False,\n        log_responses: bool = False,\n        **connection_kwargs,\n    ) -> None:\n        \"\"\"\n        :param path:\n            A relative or absolute path to the the SQLite database file (it\n            will be created if it doesn't already exist).\n        :param log_queries:\n            If ``True``, all SQL and DDL statements are printed out before\n            being run. Useful for debugging.\n        :param log_responses:\n            If ``True``, the raw response from each query is printed out.\n            Useful for debugging.\n        :param connection_kwargs:\n            These are passed directly to the database adapter. We recommend\n            setting ``timeout`` if you expect your application to process a\n            large number of concurrent writes, to prevent queries timing out.\n            See Python's `sqlite3 docs <https://docs.python.org/3/library/sqlite3.html#sqlite3.connect>`_\n            for more info.\n\n        \"\"\"  # noqa: E501\n        default_connection_kwargs = {\n            \"database\": path,\n            \"detect_types\": sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,\n            \"isolation_level\": None,\n        }\n\n        self.log_queries = log_queries\n        self.log_responses = log_responses\n        self.connection_kwargs = {\n            **default_connection_kwargs,\n            **connection_kwargs,\n        }\n\n        self.current_transaction = contextvars.ContextVar(\n            f\"sqlite_current_transaction_{path}\", default=None\n        )\n\n        super().__init__()\n\n    @property\n    def path(self):\n        return self.connection_kwargs[\"database\"]\n\n    @path.setter\n    def path(self, value: str):\n        self.connection_kwargs[\"database\"] = value\n\n    async def get_version(self) -> float:\n        return self.get_version_sync()\n\n    def get_version_sync(self) -> float:\n        major, minor, _ = sqlite3.sqlite_version_info\n        return float(f\"{major}.{minor}\")\n\n    async def prep_database(self):\n        pass\n\n    ###########################################################################\n\n    def remove_db_file(self):\n        \"\"\"\n        Use with caution - removes the SQLite file. Useful for testing\n        purposes.\n        \"\"\"\n        if os.path.exists(self.path):\n            os.unlink(self.path)\n\n    def create_db_file(self):\n        \"\"\"\n        Create the database file. Useful for testing purposes.\n        \"\"\"\n        if os.path.exists(self.path):\n            raise Exception(f\"Database at {self.path} already exists\")\n        with open(self.path, \"w\"):\n            pass\n\n    ###########################################################################\n\n    async def batch(\n        self, query: Query, batch_size: int = 100, node: t.Optional[str] = None\n    ) -> AsyncBatch:\n        \"\"\"\n        :param query:\n            The database query to run.\n        :param batch_size:\n            How many rows to fetch on each iteration.\n        :param node:\n            This is ignored currently, as SQLite runs off a single node. The\n            value is here so the API is consistent with Postgres.\n        \"\"\"\n        connection = await self.get_connection()\n        return AsyncBatch(\n            connection=connection, query=query, batch_size=batch_size\n        )\n\n    ###########################################################################\n\n    async def get_connection(self) -> Connection:\n        connection = await aiosqlite.connect(**self.connection_kwargs)\n        connection.row_factory = dict_factory  # type: ignore\n        await connection.execute(\"PRAGMA foreign_keys = 1\")\n        return connection\n\n    ###########################################################################\n\n    async def _get_inserted_pk(self, cursor, table: t.Type[Table]) -> t.Any:\n        \"\"\"\n        If the `pk` column is a non-integer then `ROWID` and `pk` will return\n        different types. Need to query by `lastrowid` to get `pk`s in SQLite\n        prior to 3.35.0.\n        \"\"\"\n        await cursor.execute(\n            f\"SELECT {table._meta.primary_key._meta.db_column_name} FROM \"\n            f\"{table._meta.tablename} WHERE ROWID = {cursor.lastrowid}\"\n        )\n        response = await cursor.fetchone()\n        return response[table._meta.primary_key._meta.db_column_name]\n\n    async def _run_in_new_connection(\n        self,\n        query: str,\n        args: t.List[t.Any] = None,\n        query_type: str = \"generic\",\n        table: t.Optional[t.Type[Table]] = None,\n    ):\n        if args is None:\n            args = []\n        async with aiosqlite.connect(**self.connection_kwargs) as connection:\n            await connection.execute(\"PRAGMA foreign_keys = 1\")\n\n            connection.row_factory = dict_factory  # type: ignore\n            async with connection.execute(query, args) as cursor:\n                await connection.commit()\n\n                if query_type == \"insert\" and self.get_version_sync() < 3.35:\n                    # We can't use the RETURNING clause on older versions\n                    # of SQLite.\n                    assert table is not None\n                    pk = await self._get_inserted_pk(cursor, table)\n                    return [{table._meta.primary_key._meta.db_column_name: pk}]\n                else:\n                    return await cursor.fetchall()\n\n    async def _run_in_existing_connection(\n        self,\n        connection,\n        query: str,\n        args: t.List[t.Any] = None,\n        query_type: str = \"generic\",\n        table: t.Optional[t.Type[Table]] = None,\n    ):\n        \"\"\"\n        This is used when a transaction is currently active.\n        \"\"\"\n        if args is None:\n            args = []\n        await connection.execute(\"PRAGMA foreign_keys = 1\")\n\n        connection.row_factory = dict_factory\n        async with connection.execute(query, args) as cursor:\n            response = await cursor.fetchall()\n\n            if query_type == \"insert\" and self.get_version_sync() < 3.35:\n                # We can't use the RETURNING clause on older versions\n                # of SQLite.\n                assert table is not None\n                pk = await self._get_inserted_pk(cursor, table)\n                return [{table._meta.primary_key._meta.db_column_name: pk}]\n            else:\n                return response\n\n    async def run_querystring(\n        self, querystring: QueryString, in_pool: bool = False\n    ):\n        \"\"\"\n        Connection pools aren't currently supported - the argument is there\n        for consistency with other engines.\n        \"\"\"\n        query_id = self.get_query_id()\n\n        if self.log_queries:\n            self.print_query(query_id=query_id, query=querystring.__str__())\n\n        query, query_args = querystring.compile_string(\n            engine_type=self.engine_type\n        )\n\n        # If running inside a transaction:\n        current_transaction = self.current_transaction.get()\n        if current_transaction:\n            response = await self._run_in_existing_connection(\n                connection=current_transaction.connection,\n                query=query,\n                args=query_args,\n                query_type=querystring.query_type,\n                table=querystring.table,\n            )\n        else:\n            response = await self._run_in_new_connection(\n                query=query,\n                args=query_args,\n                query_type=querystring.query_type,\n                table=querystring.table,\n            )\n\n        if self.log_responses:\n            self.print_response(query_id=query_id, response=response)\n\n        return response\n\n    async def run_ddl(self, ddl: str, in_pool: bool = False):\n        \"\"\"\n        Connection pools aren't currently supported - the argument is there\n        for consistency with other engines.\n        \"\"\"\n        query_id = self.get_query_id()\n\n        if self.log_queries:\n            self.print_query(query_id=query_id, query=ddl)\n\n        # If running inside a transaction:\n        current_transaction = self.current_transaction.get()\n        if current_transaction:\n            response = await self._run_in_existing_connection(\n                connection=current_transaction.connection,\n                query=ddl,\n            )\n        else:\n            response = await self._run_in_new_connection(\n                query=ddl,\n            )\n\n        if self.log_responses:\n            self.print_response(query_id=query_id, response=response)\n\n        return response\n\n    def atomic(\n        self, transaction_type: TransactionType = TransactionType.deferred\n    ) -> Atomic:\n        return Atomic(engine=self, transaction_type=transaction_type)\n\n    def transaction(\n        self,\n        transaction_type: TransactionType = TransactionType.deferred,\n        allow_nested: bool = True,\n    ) -> SQLiteTransaction:\n        \"\"\"\n        Create a new database transaction. See :class:`Transaction`.\n        \"\"\"\n        return SQLiteTransaction(\n            engine=self,\n            transaction_type=transaction_type,\n            allow_nested=allow_nested,\n        )\n", "import asyncio\nimport typing as t\nfrom unittest import TestCase\n\nfrom piccolo.engine.postgres import Atomic\nfrom piccolo.engine.sqlite import SQLiteEngine, TransactionType\nfrom piccolo.table import drop_db_tables_sync\nfrom piccolo.utils.sync import run_sync\nfrom tests.base import engines_only\nfrom tests.example_apps.music.tables import Band, Manager\n\n\nclass TestAtomic(TestCase):\n    def test_error(self):\n        \"\"\"\n        Make sure queries in a transaction aren't committed if a query fails.\n        \"\"\"\n        atomic = Band._meta.db.atomic()\n        atomic.add(\n            Manager.create_table(),\n            Band.create_table(),\n            Band.raw(\"MALFORMED QUERY ... SHOULD ERROR\"),\n        )\n        try:\n            atomic.run_sync()\n        except Exception:\n            pass\n        self.assertTrue(not Band.table_exists().run_sync())\n        self.assertTrue(not Manager.table_exists().run_sync())\n\n    def test_succeeds(self):\n        \"\"\"\n        Make sure that when atomic is run successfully the database is modified\n        accordingly.\n        \"\"\"\n        atomic = Band._meta.db.atomic()\n        atomic.add(Manager.create_table(), Band.create_table())\n        atomic.run_sync()\n\n        self.assertTrue(Band.table_exists().run_sync())\n        self.assertTrue(Manager.table_exists().run_sync())\n\n        drop_db_tables_sync(Band, Manager)\n\n    @engines_only(\"postgres\", \"cockroach\")\n    def test_pool(self):\n        \"\"\"\n        Make sure atomic works correctly when a connection pool is active.\n        \"\"\"\n\n        async def run():\n            \"\"\"\n            We have to run this async function, so we can use a connection\n            pool.\n            \"\"\"\n            engine = Band._meta.db\n            await engine.start_connection_pool()\n\n            atomic: Atomic = engine.atomic()\n            atomic.add(\n                Manager.create_table(),\n                Band.create_table(),\n            )\n\n            await atomic.run()\n            await engine.close_connection_pool()\n\n        run_sync(run())\n\n        self.assertTrue(Band.table_exists().run_sync())\n        self.assertTrue(Manager.table_exists().run_sync())\n\n        drop_db_tables_sync(Band, Manager)\n\n\nclass TestTransaction(TestCase):\n    def tearDown(self):\n        for table in (Band, Manager):\n            if table.table_exists().run_sync():\n                table.alter().drop_table().run_sync()\n\n    def test_error(self):\n        \"\"\"\n        Make sure queries in a transaction aren't committed if a query fails.\n        \"\"\"\n\n        async def run_transaction():\n            try:\n                async with Band._meta.db.transaction():\n                    Manager.create_table()\n                    Band.create_table()\n                    Band.raw(\"MALFORMED QUERY ... SHOULD ERROR\")\n            except Exception:\n                pass\n\n        asyncio.run(run_transaction())\n\n        self.assertTrue(not Band.table_exists().run_sync())\n        self.assertTrue(not Manager.table_exists().run_sync())\n\n    def test_succeeds(self):\n        async def run_transaction():\n            async with Band._meta.db.transaction():\n                await Manager.create_table().run()\n                await Band.create_table().run()\n\n        asyncio.run(run_transaction())\n\n        self.assertTrue(Band.table_exists().run_sync())\n        self.assertTrue(Manager.table_exists().run_sync())\n\n    def test_manual_commit(self):\n        \"\"\"\n        The context manager automatically commits changes, but we also\n        allow the user to do it manually.\n        \"\"\"\n\n        async def run_transaction():\n            async with Band._meta.db.transaction() as transaction:\n                await Manager.create_table()\n                await transaction.commit()\n\n        asyncio.run(run_transaction())\n        self.assertTrue(Manager.table_exists().run_sync())\n\n    def test_manual_rollback(self):\n        \"\"\"\n        The context manager will automatically rollback changes if an exception\n        is raised, but we also allow the user to do it manually.\n        \"\"\"\n\n        async def run_transaction():\n            async with Band._meta.db.transaction() as transaction:\n                await Manager.create_table()\n                await transaction.rollback()\n\n        asyncio.run(run_transaction())\n        self.assertFalse(Manager.table_exists().run_sync())\n\n    @engines_only(\"postgres\")\n    def test_transaction_id(self):\n        \"\"\"\n        An extra sanity check, that the transaction id is the same for each\n        query inside the transaction block.\n        \"\"\"\n\n        async def run_transaction():\n            responses = []\n            async with Band._meta.db.transaction():\n                responses.append(\n                    await Manager.raw(\"SELECT txid_current()\").run()\n                )\n                responses.append(\n                    await Manager.raw(\"SELECT txid_current()\").run()\n                )\n            return [i[0][\"txid_current\"] for i in responses]\n\n        txids = asyncio.run(run_transaction())\n        self.assertEqual(len(set(txids)), 1)\n\n        # Now run it again and make sure the transaction ids differ.\n        next_txids = asyncio.run(run_transaction())\n        self.assertNotEqual(txids, next_txids)\n\n\nclass TestTransactionExists(TestCase):\n    def test_exists(self):\n        \"\"\"\n        Make sure we can detect when code is within a transaction.\n        \"\"\"\n        engine = t.cast(SQLiteEngine, Manager._meta.db)\n\n        async def run_inside_transaction():\n            async with engine.transaction():\n                return engine.transaction_exists()\n\n        self.assertTrue(asyncio.run(run_inside_transaction()))\n\n        async def run_outside_transaction():\n            return engine.transaction_exists()\n\n        self.assertFalse(asyncio.run(run_outside_transaction()))\n\n\n@engines_only(\"sqlite\")\nclass TestTransactionType(TestCase):\n    def setUp(self):\n        Manager.create_table().run_sync()\n\n    def tearDown(self):\n        Manager.alter().drop_table().run_sync()\n\n    def test_transaction(self):\n        \"\"\"\n        With SQLite, we can specify the transaction type. This helps when\n        we want to do concurrent writes, to avoid locking the database.\n\n        https://github.com/piccolo-orm/piccolo/issues/687\n        \"\"\"\n        engine = t.cast(SQLiteEngine, Manager._meta.db)\n\n        async def run_transaction(name: str):\n            async with engine.transaction(\n                transaction_type=TransactionType.immediate\n            ):\n                # This does a SELECT followed by an INSERT, so is a good test.\n                # If using TransactionType.deferred it would fail because\n                # the database will become locked.\n                await Manager.objects().get_or_create(Manager.name == name)\n\n        manager_names = [f\"Manager_{i}\" for i in range(1, 10)]\n\n        async def run_all():\n            \"\"\"\n            Run all of the transactions concurrently.\n            \"\"\"\n            await asyncio.gather(\n                *[run_transaction(name=name) for name in manager_names]\n            )\n\n        asyncio.run(run_all())\n\n        # Make sure it all ran effectively.\n        self.assertListEqual(\n            Manager.select(Manager.name)\n            .order_by(Manager.name)\n            .output(as_list=True)\n            .run_sync(),\n            manager_names,\n        )\n\n    def test_atomic(self):\n        \"\"\"\n        Similar to above, but with ``Atomic``.\n        \"\"\"\n        engine = t.cast(SQLiteEngine, Manager._meta.db)\n\n        async def run_atomic(name: str):\n            atomic = engine.atomic(transaction_type=TransactionType.immediate)\n            atomic.add(Manager.objects().get_or_create(Manager.name == name))\n            await atomic.run()\n\n        manager_names = [f\"Manager_{i}\" for i in range(1, 10)]\n\n        async def run_all():\n            \"\"\"\n            Run all of the transactions concurrently.\n            \"\"\"\n            await asyncio.gather(\n                *[run_atomic(name=name) for name in manager_names]\n            )\n\n        asyncio.run(run_all())\n\n        # Make sure it all ran effectively.\n        self.assertListEqual(\n            Manager.select(Manager.name)\n            .order_by(Manager.name)\n            .output(as_list=True)\n            .run_sync(),\n            manager_names,\n        )\n\n\nclass TestSavepoint(TestCase):\n    def setUp(self):\n        Manager.create_table().run_sync()\n\n    def tearDown(self):\n        Manager.alter().drop_table().run_sync()\n\n    def test_savepoint(self):\n        async def run_test():\n            async with Manager._meta.db.transaction() as transaction:\n                await Manager.insert(Manager(name=\"Manager 1\"))\n                savepoint = await transaction.savepoint()\n                await Manager.insert(Manager(name=\"Manager 2\"))\n                await savepoint.rollback_to()\n\n        run_sync(run_test())\n\n        self.assertListEqual(\n            Manager.select(Manager.name).run_sync(), [{\"name\": \"Manager 1\"}]\n        )\n\n    def test_named_savepoint(self):\n        async def run_test():\n            async with Manager._meta.db.transaction() as transaction:\n                await Manager.insert(Manager(name=\"Manager 1\"))\n                await transaction.savepoint(\"my_savepoint\")\n                await Manager.insert(Manager(name=\"Manager 2\"))\n                await transaction.rollback_to(\"my_savepoint\")\n\n        run_sync(run_test())\n\n        self.assertListEqual(\n            Manager.select(Manager.name).run_sync(), [{\"name\": \"Manager 1\"}]\n        )\n"], "fixing_code": ["from __future__ import annotations\n\nimport contextvars\nimport logging\nimport pprint\nimport string\nimport typing as t\nfrom abc import ABCMeta, abstractmethod\n\nfrom piccolo.querystring import QueryString\nfrom piccolo.utils.sync import run_sync\nfrom piccolo.utils.warnings import Level, colored_string, colored_warning\n\nif t.TYPE_CHECKING:  # pragma: no cover\n    from piccolo.query.base import Query\n\n\nlogger = logging.getLogger(__name__)\n# This is a set to speed up lookups from O(n) when\n# using str vs O(1) when using set[str]\nVALID_SAVEPOINT_CHARACTERS: t.Final[set[str]] = set(\n    string.ascii_letters + string.digits + \"-\" + \"_\"\n)\n\n\ndef validate_savepoint_name(savepoint_name: str) -> None:\n    \"\"\"Validates a save point's name meets the required character set.\"\"\"\n    if not all(i in VALID_SAVEPOINT_CHARACTERS for i in savepoint_name):\n        raise ValueError(\n            \"Savepoint names can only contain the following characters:\"\n            f\" {VALID_SAVEPOINT_CHARACTERS}\"\n        )\n\n\nclass Batch:\n    pass\n\n\nTransactionClass = t.TypeVar(\"TransactionClass\")\n\n\nclass Engine(t.Generic[TransactionClass], metaclass=ABCMeta):\n\n    __slots__ = (\"query_id\",)\n\n    def __init__(self):\n        run_sync(self.check_version())\n        run_sync(self.prep_database())\n        self.query_id = 0\n\n    @property\n    @abstractmethod\n    def engine_type(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def min_version_number(self) -> float:\n        pass\n\n    @abstractmethod\n    async def get_version(self) -> float:\n        pass\n\n    @abstractmethod\n    def get_version_sync(self) -> float:\n        pass\n\n    @abstractmethod\n    async def prep_database(self):\n        pass\n\n    @abstractmethod\n    async def batch(\n        self,\n        query: Query,\n        batch_size: int = 100,\n        node: t.Optional[str] = None,\n    ) -> Batch:\n        pass\n\n    @abstractmethod\n    async def run_querystring(self, querystring: QueryString, in_pool: bool):\n        pass\n\n    @abstractmethod\n    async def run_ddl(self, ddl: str, in_pool: bool = True):\n        pass\n\n    @abstractmethod\n    def transaction(self):\n        pass\n\n    @abstractmethod\n    def atomic(self):\n        pass\n\n    async def check_version(self):\n        \"\"\"\n        Warn if the database version isn't supported.\n        \"\"\"\n        try:\n            version_number = await self.get_version()\n        except Exception as exception:\n            colored_warning(\n                f\"Unable to fetch server version: {exception}\",\n                level=Level.high,\n            )\n            return\n\n        engine_type = self.engine_type.capitalize()\n        logger.info(f\"Running {engine_type} version {version_number}\")\n        if version_number and (version_number < self.min_version_number):\n            message = (\n                f\"This version of {self.engine_type} isn't supported \"\n                f\"(< {self.min_version_number}) - some features might not be \"\n                \"available. For instructions on installing databases, see the \"\n                \"Piccolo docs.\"\n            )\n            colored_warning(message, stacklevel=3)\n\n    def _connection_pool_warning(self):\n        message = (\n            f\"Connection pooling is not supported for {self.engine_type}.\"\n        )\n        logger.warning(message)\n        colored_warning(message, stacklevel=3)\n\n    async def start_connection_pool(self):\n        \"\"\"\n        The database driver doesn't implement connection pooling.\n        \"\"\"\n        self._connection_pool_warning()\n\n    async def close_connection_pool(self):\n        \"\"\"\n        The database driver doesn't implement connection pooling.\n        \"\"\"\n        self._connection_pool_warning()\n\n    ###########################################################################\n\n    current_transaction: contextvars.ContextVar[t.Optional[TransactionClass]]\n\n    def transaction_exists(self) -> bool:\n        \"\"\"\n        Find out if a transaction is currently active.\n\n        :returns:\n            ``True`` if a transaction is already active for the current\n            asyncio task. This is useful to know, because nested transactions\n            aren't currently supported, so you can check if an existing\n            transaction is already active, before creating a new one.\n\n        \"\"\"\n        return self.current_transaction.get() is not None\n\n    ###########################################################################\n    # Logging queries and responses\n\n    def get_query_id(self) -> int:\n        self.query_id += 1\n        return self.query_id\n\n    def print_query(self, query_id: int, query: str):\n        print(colored_string(f\"\\nQuery {query_id}:\"))\n        print(query)\n\n    def print_response(self, query_id: int, response: t.List):\n        print(\n            colored_string(f\"\\nQuery {query_id} response:\", level=Level.high)\n        )\n        pprint.pprint(response)\n", "from __future__ import annotations\n\nimport contextvars\nimport typing as t\nfrom dataclasses import dataclass\n\nfrom piccolo.engine.base import Batch, Engine, validate_savepoint_name\nfrom piccolo.engine.exceptions import TransactionError\nfrom piccolo.query.base import DDL, Query\nfrom piccolo.querystring import QueryString\nfrom piccolo.utils.lazy_loader import LazyLoader\nfrom piccolo.utils.sync import run_sync\nfrom piccolo.utils.warnings import Level, colored_warning\n\nasyncpg = LazyLoader(\"asyncpg\", globals(), \"asyncpg\")\n\nif t.TYPE_CHECKING:  # pragma: no cover\n    from asyncpg.connection import Connection\n    from asyncpg.cursor import Cursor\n    from asyncpg.pool import Pool\n\n\n@dataclass\nclass AsyncBatch(Batch):\n\n    connection: Connection\n    query: Query\n    batch_size: int\n\n    # Set internally\n    _transaction = None\n    _cursor: t.Optional[Cursor] = None\n\n    @property\n    def cursor(self) -> Cursor:\n        if not self._cursor:\n            raise ValueError(\"_cursor not set\")\n        return self._cursor\n\n    async def next(self) -> t.List[t.Dict]:\n        data = await self.cursor.fetch(self.batch_size)\n        return await self.query._process_results(data)\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        response = await self.next()\n        if response == []:\n            raise StopAsyncIteration()\n        return response\n\n    async def __aenter__(self):\n        self._transaction = self.connection.transaction()\n        await self._transaction.start()\n        querystring = self.query.querystrings[0]\n        template, template_args = querystring.compile_string()\n\n        self._cursor = await self.connection.cursor(template, *template_args)\n        return self\n\n    async def __aexit__(self, exception_type, exception, traceback):\n        if exception:\n            await self._transaction.rollback()\n        else:\n            await self._transaction.commit()\n\n        await self.connection.close()\n\n        return exception is not None\n\n\n###############################################################################\n\n\nclass Atomic:\n    \"\"\"\n    This is useful if you want to build up a transaction programatically, by\n    adding queries to it.\n\n    Usage::\n\n        transaction = engine.atomic()\n        transaction.add(Foo.create_table())\n\n        # Either:\n        transaction.run_sync()\n        await transaction.run()\n\n    \"\"\"\n\n    __slots__ = (\"engine\", \"queries\")\n\n    def __init__(self, engine: PostgresEngine):\n        self.engine = engine\n        self.queries: t.List[Query] = []\n\n    def add(self, *query: Query):\n        self.queries += list(query)\n\n    async def run(self):\n        from piccolo.query.methods.objects import Create, GetOrCreate\n\n        try:\n            async with self.engine.transaction():\n                for query in self.queries:\n                    if isinstance(query, (Query, DDL, Create, GetOrCreate)):\n                        await query.run()\n                    else:\n                        raise ValueError(\"Unrecognised query\")\n            self.queries = []\n        except Exception as exception:\n            self.queries = []\n            raise exception from exception\n\n    def run_sync(self):\n        return run_sync(self.run())\n\n    def __await__(self):\n        return self.run().__await__()\n\n\n###############################################################################\n\n\nclass Savepoint:\n    def __init__(self, name: str, transaction: PostgresTransaction):\n        self.name = name\n        self.transaction = transaction\n\n    async def rollback_to(self):\n        validate_savepoint_name(self.name)\n        await self.transaction.connection.execute(\n            f\"ROLLBACK TO SAVEPOINT {self.name}\"\n        )\n\n    async def release(self):\n        validate_savepoint_name(self.name)\n        await self.transaction.connection.execute(\n            f\"RELEASE SAVEPOINT {self.name}\"\n        )\n\n\nclass PostgresTransaction:\n    \"\"\"\n    Used for wrapping queries in a transaction, using a context manager.\n    Currently it's async only.\n\n    Usage::\n\n        async with engine.transaction():\n            # Run some queries:\n            await Band.select().run()\n\n    \"\"\"\n\n    __slots__ = (\n        \"engine\",\n        \"transaction\",\n        \"context\",\n        \"connection\",\n        \"_savepoint_id\",\n        \"_parent\",\n        \"_committed\",\n        \"_rolled_back\",\n    )\n\n    def __init__(self, engine: PostgresEngine, allow_nested: bool = True):\n        \"\"\"\n        :param allow_nested:\n            If ``True`` then if we try creating a new transaction when another\n            is already active, we treat this as a no-op::\n\n                async with DB.transaction():\n                    async with DB.transaction():\n                        pass\n\n            If we want to disallow this behaviour, then setting\n            ``allow_nested=False`` will cause a ``TransactionError`` to be\n            raised.\n\n        \"\"\"\n        self.engine = engine\n        current_transaction = self.engine.current_transaction.get()\n\n        self._savepoint_id = 0\n        self._parent = None\n        self._committed = False\n        self._rolled_back = False\n\n        if current_transaction:\n            if allow_nested:\n                self._parent = current_transaction\n            else:\n                raise TransactionError(\n                    \"A transaction is already active - nested transactions \"\n                    \"aren't allowed.\"\n                )\n\n    async def __aenter__(self) -> PostgresTransaction:\n        if self._parent is not None:\n            return self._parent\n\n        self.connection = await self.get_connection()\n        self.transaction = self.connection.transaction()\n        await self.begin()\n        self.context = self.engine.current_transaction.set(self)\n        return self\n\n    async def get_connection(self):\n        if self.engine.pool:\n            return await self.engine.pool.acquire()\n        else:\n            return await self.engine.get_new_connection()\n\n    async def begin(self):\n        await self.transaction.start()\n\n    async def commit(self):\n        await self.transaction.commit()\n        self._committed = True\n\n    async def rollback(self):\n        await self.transaction.rollback()\n        self._rolled_back = True\n\n    async def rollback_to(self, savepoint_name: str):\n        \"\"\"\n        Used to rollback to a savepoint just using the name.\n        \"\"\"\n        await Savepoint(name=savepoint_name, transaction=self).rollback_to()\n\n    ###########################################################################\n\n    def get_savepoint_id(self) -> int:\n        self._savepoint_id += 1\n        return self._savepoint_id\n\n    async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:\n        name = name or f\"savepoint_{self.get_savepoint_id()}\"\n        validate_savepoint_name(name)\n        await self.connection.execute(f\"SAVEPOINT {name}\")\n        return Savepoint(name=name, transaction=self)\n\n    ###########################################################################\n\n    async def __aexit__(self, exception_type, exception, traceback):\n        if self._parent:\n            return exception is None\n\n        if exception:\n            # The user may have manually rolled it back.\n            if not self._rolled_back:\n                await self.rollback()\n        else:\n            # The user may have manually committed it.\n            if not self._committed and not self._rolled_back:\n                await self.commit()\n\n        if self.engine.pool:\n            await self.engine.pool.release(self.connection)\n        else:\n            await self.connection.close()\n\n        self.engine.current_transaction.reset(self.context)\n\n        return exception is None\n\n\n###############################################################################\n\n\nclass PostgresEngine(Engine[t.Optional[PostgresTransaction]]):\n    \"\"\"\n    Used to connect to PostgreSQL.\n\n    :param config:\n        The config dictionary is passed to the underlying database adapter,\n        asyncpg. Common arguments you're likely to need are:\n\n        * host\n        * port\n        * user\n        * password\n        * database\n\n        For example, ``{'host': 'localhost', 'port': 5432}``.\n\n        See the `asyncpg docs <https://magicstack.github.io/asyncpg/current/api/index.html#connection>`_\n        for all available options.\n\n    :param extensions:\n        When the engine starts, it will try and create these extensions\n        in Postgres. If you're using a read only database, set this value to an\n        empty tuple ``()``.\n\n    :param log_queries:\n        If ``True``, all SQL and DDL statements are printed out before being\n        run. Useful for debugging.\n\n    :param log_responses:\n        If ``True``, the raw response from each query is printed out. Useful\n        for debugging.\n\n    :param extra_nodes:\n        If you have additional database nodes (e.g. read replicas) for the\n        server, you can specify them here. It's a mapping of a memorable name\n        to a ``PostgresEngine`` instance. For example::\n\n            DB = PostgresEngine(\n                config={'database': 'main_db'},\n                extra_nodes={\n                    'read_replica_1': PostgresEngine(\n                        config={\n                            'database': 'main_db',\n                            host: 'read_replicate.my_db.com'\n                        },\n                        extensions=()\n                    )\n                }\n            )\n\n        Note how we set ``extensions=()``, because it's a read only database.\n\n        When executing a query, you can specify one of these nodes instead\n        of the main database. For example::\n\n            >>> await MyTable.select().run(node=\"read_replica_1\")\n\n    \"\"\"  # noqa: E501\n\n    __slots__ = (\n        \"config\",\n        \"extensions\",\n        \"log_queries\",\n        \"log_responses\",\n        \"extra_nodes\",\n        \"pool\",\n        \"current_transaction\",\n    )\n\n    engine_type = \"postgres\"\n    min_version_number = 10\n\n    def __init__(\n        self,\n        config: t.Dict[str, t.Any],\n        extensions: t.Sequence[str] = (\"uuid-ossp\",),\n        log_queries: bool = False,\n        log_responses: bool = False,\n        extra_nodes: t.Mapping[str, PostgresEngine] = None,\n    ) -> None:\n        if extra_nodes is None:\n            extra_nodes = {}\n\n        self.config = config\n        self.extensions = extensions\n        self.log_queries = log_queries\n        self.log_responses = log_responses\n        self.extra_nodes = extra_nodes\n        self.pool: t.Optional[Pool] = None\n        database_name = config.get(\"database\", \"Unknown\")\n        self.current_transaction = contextvars.ContextVar(\n            f\"pg_current_transaction_{database_name}\", default=None\n        )\n        super().__init__()\n\n    @staticmethod\n    def _parse_raw_version_string(version_string: str) -> float:\n        \"\"\"\n        The format of the version string isn't always consistent. Sometimes\n        it's just the version number e.g. '9.6.18', and sometimes\n        it contains specific build information e.g.\n        '12.4 (Ubuntu 12.4-0ubuntu0.20.04.1)'. Just extract the major and\n        minor version numbers.\n        \"\"\"\n        version_segment = version_string.split(\" \")[0]\n        major, minor = version_segment.split(\".\")[:2]\n        return float(f\"{major}.{minor}\")\n\n    async def get_version(self) -> float:\n        \"\"\"\n        Returns the version of Postgres being run.\n        \"\"\"\n        try:\n            response: t.Sequence[t.Dict] = await self._run_in_new_connection(\n                \"SHOW server_version\"\n            )\n        except ConnectionRefusedError as exception:\n            # Suppressing the exception, otherwise importing piccolo_conf.py\n            # containing an engine will raise an ImportError.\n            colored_warning(f\"Unable to connect to database - {exception}\")\n            return 0.0\n        else:\n            version_string = response[0][\"server_version\"]\n            return self._parse_raw_version_string(\n                version_string=version_string\n            )\n\n    def get_version_sync(self) -> float:\n        return run_sync(self.get_version())\n\n    async def prep_database(self):\n        for extension in self.extensions:\n            try:\n                await self._run_in_new_connection(\n                    f'CREATE EXTENSION IF NOT EXISTS \"{extension}\"',\n                )\n            except asyncpg.exceptions.InsufficientPrivilegeError:\n                colored_warning(\n                    f\"=> Unable to create {extension} extension - some \"\n                    \"functionality may not behave as expected. Make sure \"\n                    \"your database user has permission to create \"\n                    \"extensions, or add it manually using \"\n                    f'`CREATE EXTENSION \"{extension}\";`',\n                    level=Level.medium,\n                )\n\n    ###########################################################################\n    # These typos existed in the codebase for a while, so leaving these proxy\n    # methods for now to ensure backwards compatibility.\n\n    async def start_connnection_pool(self, **kwargs) -> None:\n        colored_warning(\n            \"`start_connnection_pool` is a typo - please change it to \"\n            \"`start_connection_pool`.\",\n            category=DeprecationWarning,\n        )\n        return await self.start_connection_pool()\n\n    async def close_connnection_pool(self, **kwargs) -> None:\n        colored_warning(\n            \"`close_connnection_pool` is a typo - please change it to \"\n            \"`close_connection_pool`.\",\n            category=DeprecationWarning,\n        )\n        return await self.close_connection_pool()\n\n    ###########################################################################\n\n    async def start_connection_pool(self, **kwargs) -> None:\n        if self.pool:\n            colored_warning(\n                \"A pool already exists - close it first if you want to create \"\n                \"a new pool.\",\n            )\n        else:\n            config = dict(self.config)\n            config.update(**kwargs)\n            self.pool = await asyncpg.create_pool(**config)\n\n    async def close_connection_pool(self) -> None:\n        if self.pool:\n            await self.pool.close()\n            self.pool = None\n        else:\n            colored_warning(\"No pool is running.\")\n\n    ###########################################################################\n\n    async def get_new_connection(self) -> Connection:\n        \"\"\"\n        Returns a new connection - doesn't retrieve it from the pool.\n        \"\"\"\n        return await asyncpg.connect(**self.config)\n\n    ###########################################################################\n\n    async def batch(\n        self,\n        query: Query,\n        batch_size: int = 100,\n        node: t.Optional[str] = None,\n    ) -> AsyncBatch:\n        \"\"\"\n        :param query:\n            The database query to run.\n        :param batch_size:\n            How many rows to fetch on each iteration.\n        :param node:\n            Which node to run the query on (see ``extra_nodes``). If not\n            specified, it runs on the main Postgres node.\n        \"\"\"\n        engine: t.Any = self.extra_nodes.get(node) if node else self\n        connection = await engine.get_new_connection()\n        return AsyncBatch(\n            connection=connection, query=query, batch_size=batch_size\n        )\n\n    ###########################################################################\n\n    async def _run_in_pool(self, query: str, args: t.Sequence[t.Any] = None):\n        if args is None:\n            args = []\n        if not self.pool:\n            raise ValueError(\"A pool isn't currently running.\")\n\n        async with self.pool.acquire() as connection:\n            response = await connection.fetch(query, *args)\n\n        return response\n\n    async def _run_in_new_connection(\n        self, query: str, args: t.Sequence[t.Any] = None\n    ):\n        if args is None:\n            args = []\n        connection = await self.get_new_connection()\n\n        try:\n            results = await connection.fetch(query, *args)\n        except asyncpg.exceptions.PostgresError as exception:\n            await connection.close()\n            raise exception\n\n        await connection.close()\n        return results\n\n    async def run_querystring(\n        self, querystring: QueryString, in_pool: bool = True\n    ):\n        query, query_args = querystring.compile_string(\n            engine_type=self.engine_type\n        )\n\n        query_id = self.get_query_id()\n\n        if self.log_queries:\n            self.print_query(query_id=query_id, query=querystring.__str__())\n\n        # If running inside a transaction:\n        current_transaction = self.current_transaction.get()\n        if current_transaction:\n            response = await current_transaction.connection.fetch(\n                query, *query_args\n            )\n        elif in_pool and self.pool:\n            response = await self._run_in_pool(query, query_args)\n        else:\n            response = await self._run_in_new_connection(query, query_args)\n\n        if self.log_responses:\n            self.print_response(query_id=query_id, response=response)\n\n        return response\n\n    async def run_ddl(self, ddl: str, in_pool: bool = True):\n        query_id = self.get_query_id()\n\n        if self.log_queries:\n            self.print_query(query_id=query_id, query=ddl)\n\n        # If running inside a transaction:\n        current_transaction = self.current_transaction.get()\n        if current_transaction:\n            response = await current_transaction.connection.fetch(ddl)\n        elif in_pool and self.pool:\n            response = await self._run_in_pool(ddl)\n        else:\n            response = await self._run_in_new_connection(ddl)\n\n        if self.log_responses:\n            self.print_response(query_id=query_id, response=response)\n\n        return response\n\n    def atomic(self) -> Atomic:\n        return Atomic(engine=self)\n\n    def transaction(self, allow_nested: bool = True) -> PostgresTransaction:\n        return PostgresTransaction(engine=self, allow_nested=allow_nested)\n", "from __future__ import annotations\n\nimport contextvars\nimport datetime\nimport enum\nimport os\nimport sqlite3\nimport typing as t\nimport uuid\nfrom dataclasses import dataclass\nfrom decimal import Decimal\n\nfrom piccolo.engine.base import Batch, Engine, validate_savepoint_name\nfrom piccolo.engine.exceptions import TransactionError\nfrom piccolo.query.base import DDL, Query\nfrom piccolo.querystring import QueryString\nfrom piccolo.utils.encoding import dump_json, load_json\nfrom piccolo.utils.lazy_loader import LazyLoader\nfrom piccolo.utils.sync import run_sync\n\naiosqlite = LazyLoader(\"aiosqlite\", globals(), \"aiosqlite\")\n\n\nif t.TYPE_CHECKING:  # pragma: no cover\n    from aiosqlite import Connection, Cursor  # type: ignore\n\n    from piccolo.table import Table\n\n###############################################################################\n\n# We need to register some adapters so sqlite returns types which are more\n# consistent with the Postgres engine.\n\n\n# In\n\n\ndef convert_numeric_in(value):\n    \"\"\"\n    Convert any Decimal values into floats.\n    \"\"\"\n    return float(value)\n\n\ndef convert_uuid_in(value) -> str:\n    \"\"\"\n    Converts the UUID value being passed into sqlite.\n    \"\"\"\n    return str(value)\n\n\ndef convert_time_in(value: datetime.time) -> str:\n    \"\"\"\n    Converts the time value being passed into sqlite.\n    \"\"\"\n    return value.isoformat()\n\n\ndef convert_date_in(value: datetime.date):\n    \"\"\"\n    Converts the date value being passed into sqlite.\n    \"\"\"\n    return value.isoformat()\n\n\ndef convert_datetime_in(value: datetime.datetime) -> str:\n    \"\"\"\n    Converts the datetime into a string. If it's timezone aware, we want to\n    convert it to UTC first. This is to replicate Postgres, which stores\n    timezone aware datetimes in UTC.\n    \"\"\"\n    if value.tzinfo is not None:\n        value = value.astimezone(datetime.timezone.utc)\n    return str(value)\n\n\ndef convert_timedelta_in(value: datetime.timedelta):\n    \"\"\"\n    Converts the timedelta value being passed into sqlite.\n    \"\"\"\n    return value.total_seconds()\n\n\ndef convert_array_in(value: list):\n    \"\"\"\n    Converts a list value into a string.\n    \"\"\"\n    if value and type(value[0]) not in [str, int, float]:\n        raise ValueError(\"Can only serialise str, int and float.\")\n\n    return dump_json(value)\n\n\n# Out\n\n\ndef convert_numeric_out(value: bytes) -> Decimal:\n    \"\"\"\n    Convert float values into Decimals.\n    \"\"\"\n    return Decimal(value.decode(\"ascii\"))\n\n\ndef convert_int_out(value: bytes) -> int:\n    \"\"\"\n    Make sure Integer values are actually of type int.\n    \"\"\"\n    return int(float(value))\n\n\ndef convert_uuid_out(value: bytes) -> uuid.UUID:\n    \"\"\"\n    If the value is a uuid, convert it to a UUID instance.\n    \"\"\"\n    return uuid.UUID(value.decode(\"utf8\"))\n\n\ndef convert_date_out(value: bytes) -> datetime.date:\n    return datetime.date.fromisoformat(value.decode(\"utf8\"))\n\n\ndef convert_time_out(value: bytes) -> datetime.time:\n    \"\"\"\n    If the value is a time, convert it to a UUID instance.\n    \"\"\"\n    return datetime.time.fromisoformat(value.decode(\"utf8\"))\n\n\ndef convert_seconds_out(value: bytes) -> datetime.timedelta:\n    \"\"\"\n    If the value is from a seconds column, convert it to a timedelta instance.\n    \"\"\"\n    return datetime.timedelta(seconds=float(value.decode(\"utf8\")))\n\n\ndef convert_boolean_out(value: bytes) -> bool:\n    \"\"\"\n    If the value is from a boolean column, convert it to a bool value.\n    \"\"\"\n    _value = value.decode(\"utf8\")\n    return _value == \"1\"\n\n\ndef convert_timestamp_out(value: bytes) -> datetime.datetime:\n    \"\"\"\n    If the value is from a timestamp column, convert it to a datetime value.\n    \"\"\"\n    return datetime.datetime.fromisoformat(value.decode(\"utf8\"))\n\n\ndef convert_timestamptz_out(value: bytes) -> datetime.datetime:\n    \"\"\"\n    If the value is from a timestamptz column, convert it to a datetime value,\n    with a timezone of UTC.\n    \"\"\"\n    _value = datetime.datetime.fromisoformat(value.decode(\"utf8\"))\n    _value = _value.replace(tzinfo=datetime.timezone.utc)\n    return _value\n\n\ndef convert_array_out(value: bytes) -> t.List:\n    \"\"\"\n    If the value if from an array column, deserialise the string back into a\n    list.\n    \"\"\"\n    return load_json(value.decode(\"utf8\"))\n\n\ndef convert_M2M_out(value: bytes) -> t.List:\n    _value = value.decode(\"utf8\")\n    return _value.split(\",\")\n\n\nsqlite3.register_converter(\"Numeric\", convert_numeric_out)\nsqlite3.register_converter(\"Integer\", convert_int_out)\nsqlite3.register_converter(\"UUID\", convert_uuid_out)\nsqlite3.register_converter(\"Date\", convert_date_out)\nsqlite3.register_converter(\"Time\", convert_time_out)\nsqlite3.register_converter(\"Seconds\", convert_seconds_out)\nsqlite3.register_converter(\"Boolean\", convert_boolean_out)\nsqlite3.register_converter(\"Timestamp\", convert_timestamp_out)\nsqlite3.register_converter(\"Timestamptz\", convert_timestamptz_out)\nsqlite3.register_converter(\"Array\", convert_array_out)\nsqlite3.register_converter(\"M2M\", convert_M2M_out)\n\nsqlite3.register_adapter(Decimal, convert_numeric_in)\nsqlite3.register_adapter(uuid.UUID, convert_uuid_in)\nsqlite3.register_adapter(datetime.time, convert_time_in)\nsqlite3.register_adapter(datetime.date, convert_date_in)\nsqlite3.register_adapter(datetime.datetime, convert_datetime_in)\nsqlite3.register_adapter(datetime.timedelta, convert_timedelta_in)\nsqlite3.register_adapter(list, convert_array_in)\n\n###############################################################################\n\n\n@dataclass\nclass AsyncBatch(Batch):\n\n    connection: Connection\n    query: Query\n    batch_size: int\n\n    # Set internally\n    _cursor: t.Optional[Cursor] = None\n\n    @property\n    def cursor(self) -> Cursor:\n        if not self._cursor:\n            raise ValueError(\"_cursor not set\")\n        return self._cursor\n\n    async def next(self) -> t.List[t.Dict]:\n        data = await self.cursor.fetchmany(self.batch_size)\n        return await self.query._process_results(data)\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        response = await self.next()\n        if response == []:\n            raise StopAsyncIteration()\n        return response\n\n    async def __aenter__(self):\n        querystring = self.query.querystrings[0]\n        template, template_args = querystring.compile_string()\n\n        self._cursor = await self.connection.execute(template, *template_args)\n        return self\n\n    async def __aexit__(self, exception_type, exception, traceback):\n        await self._cursor.close()\n        await self.connection.close()\n        return exception is not None\n\n\n###############################################################################\n\n\nclass TransactionType(enum.Enum):\n    \"\"\"\n    See the `SQLite <https://www.sqlite.org/lang_transaction.html>`_ docs for\n    more info.\n    \"\"\"\n\n    deferred = \"DEFERRED\"\n    immediate = \"IMMEDIATE\"\n    exclusive = \"EXCLUSIVE\"\n\n\nclass Atomic:\n    \"\"\"\n    Usage:\n\n    transaction = engine.atomic()\n    transaction.add(Foo.create_table())\n\n    # Either:\n    transaction.run_sync()\n    await transaction.run()\n    \"\"\"\n\n    __slots__ = (\"engine\", \"queries\", \"transaction_type\")\n\n    def __init__(\n        self,\n        engine: SQLiteEngine,\n        transaction_type: TransactionType = TransactionType.deferred,\n    ):\n        self.engine = engine\n        self.transaction_type = transaction_type\n        self.queries: t.List[Query] = []\n\n    def add(self, *query: Query):\n        self.queries += list(query)\n\n    async def run(self):\n        from piccolo.query.methods.objects import Create, GetOrCreate\n\n        try:\n            async with self.engine.transaction(\n                transaction_type=self.transaction_type\n            ):\n                for query in self.queries:\n                    if isinstance(query, (Query, DDL, Create, GetOrCreate)):\n                        await query.run()\n                    else:\n                        raise ValueError(\"Unrecognised query\")\n            self.queries = []\n        except Exception as exception:\n            self.queries = []\n            raise exception from exception\n\n    def run_sync(self):\n        return run_sync(self.run())\n\n    def __await__(self):\n        return self.run().__await__()\n\n\n###############################################################################\n\n\nclass Savepoint:\n    def __init__(self, name: str, transaction: SQLiteTransaction):\n        self.name = name\n        self.transaction = transaction\n\n    async def rollback_to(self):\n        validate_savepoint_name(self.name)\n        await self.transaction.connection.execute(\n            f\"ROLLBACK TO SAVEPOINT {self.name}\"\n        )\n\n    async def release(self):\n        validate_savepoint_name(self.name)\n        await self.transaction.connection.execute(\n            f\"RELEASE SAVEPOINT {self.name}\"\n        )\n\n\nclass SQLiteTransaction:\n    \"\"\"\n    Used for wrapping queries in a transaction, using a context manager.\n    Currently it's async only.\n\n    Usage::\n\n        async with engine.transaction():\n            # Run some queries:\n            await Band.select().run()\n\n    \"\"\"\n\n    __slots__ = (\n        \"engine\",\n        \"context\",\n        \"connection\",\n        \"transaction_type\",\n        \"allow_nested\",\n        \"_savepoint_id\",\n        \"_parent\",\n        \"_committed\",\n        \"_rolled_back\",\n    )\n\n    def __init__(\n        self,\n        engine: SQLiteEngine,\n        transaction_type: TransactionType = TransactionType.deferred,\n        allow_nested: bool = True,\n    ):\n        \"\"\"\n        :param transaction_type:\n            If your transaction just contains ``SELECT`` queries, then use\n            ``TransactionType.deferred``. This will give you the best\n            performance. When performing ``INSERT``, ``UPDATE``, ``DELETE``\n            queries, we recommend using ``TransactionType.immediate`` to\n            avoid database locks.\n        \"\"\"\n        self.engine = engine\n        self.transaction_type = transaction_type\n        current_transaction = self.engine.current_transaction.get()\n\n        self._savepoint_id = 0\n        self._parent = None\n        self._committed = False\n        self._rolled_back = False\n\n        if current_transaction:\n            if allow_nested:\n                self._parent = current_transaction\n            else:\n                raise TransactionError(\n                    \"A transaction is already active - nested transactions \"\n                    \"aren't allowed.\"\n                )\n\n    async def __aenter__(self) -> SQLiteTransaction:\n        if self._parent is not None:\n            return self._parent\n\n        self.connection = await self.get_connection()\n        await self.begin()\n        self.context = self.engine.current_transaction.set(self)\n        return self\n\n    async def get_connection(self):\n        return await self.engine.get_connection()\n\n    async def begin(self):\n        await self.connection.execute(f\"BEGIN {self.transaction_type.value}\")\n\n    async def commit(self):\n        await self.connection.execute(\"COMMIT\")\n        self._committed = True\n\n    async def rollback(self):\n        await self.connection.execute(\"ROLLBACK\")\n        self._rolled_back = True\n\n    async def rollback_to(self, savepoint_name: str):\n        \"\"\"\n        Used to rollback to a savepoint just using the name.\n        \"\"\"\n        await Savepoint(name=savepoint_name, transaction=self).rollback_to()\n\n    ###########################################################################\n\n    def get_savepoint_id(self) -> int:\n        self._savepoint_id += 1\n        return self._savepoint_id\n\n    async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:\n        name = name or f\"savepoint_{self.get_savepoint_id()}\"\n        validate_savepoint_name(name)\n        await self.connection.execute(f\"SAVEPOINT {name}\")\n        return Savepoint(name=name, transaction=self)\n\n    ###########################################################################\n\n    async def __aexit__(self, exception_type, exception, traceback):\n        if self._parent:\n            return exception is None\n\n        if exception:\n            # The user may have manually rolled it back.\n            if not self._rolled_back:\n                await self.rollback()\n        else:\n            # The user may have manually committed it.\n            if not self._committed and not self._rolled_back:\n                await self.commit()\n\n        await self.connection.close()\n        self.engine.current_transaction.reset(self.context)\n\n        return exception is None\n\n\n###############################################################################\n\n\ndef dict_factory(cursor, row) -> t.Dict:\n    return {col[0]: row[idx] for idx, col in enumerate(cursor.description)}\n\n\nclass SQLiteEngine(Engine[t.Optional[SQLiteTransaction]]):\n\n    __slots__ = (\n        \"connection_kwargs\",\n        \"current_transaction\",\n        \"log_queries\",\n        \"log_responses\",\n    )\n\n    engine_type = \"sqlite\"\n    min_version_number = 3.25\n\n    def __init__(\n        self,\n        path: str = \"piccolo.sqlite\",\n        log_queries: bool = False,\n        log_responses: bool = False,\n        **connection_kwargs,\n    ) -> None:\n        \"\"\"\n        :param path:\n            A relative or absolute path to the the SQLite database file (it\n            will be created if it doesn't already exist).\n        :param log_queries:\n            If ``True``, all SQL and DDL statements are printed out before\n            being run. Useful for debugging.\n        :param log_responses:\n            If ``True``, the raw response from each query is printed out.\n            Useful for debugging.\n        :param connection_kwargs:\n            These are passed directly to the database adapter. We recommend\n            setting ``timeout`` if you expect your application to process a\n            large number of concurrent writes, to prevent queries timing out.\n            See Python's `sqlite3 docs <https://docs.python.org/3/library/sqlite3.html#sqlite3.connect>`_\n            for more info.\n\n        \"\"\"  # noqa: E501\n        default_connection_kwargs = {\n            \"database\": path,\n            \"detect_types\": sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,\n            \"isolation_level\": None,\n        }\n\n        self.log_queries = log_queries\n        self.log_responses = log_responses\n        self.connection_kwargs = {\n            **default_connection_kwargs,\n            **connection_kwargs,\n        }\n\n        self.current_transaction = contextvars.ContextVar(\n            f\"sqlite_current_transaction_{path}\", default=None\n        )\n\n        super().__init__()\n\n    @property\n    def path(self):\n        return self.connection_kwargs[\"database\"]\n\n    @path.setter\n    def path(self, value: str):\n        self.connection_kwargs[\"database\"] = value\n\n    async def get_version(self) -> float:\n        return self.get_version_sync()\n\n    def get_version_sync(self) -> float:\n        major, minor, _ = sqlite3.sqlite_version_info\n        return float(f\"{major}.{minor}\")\n\n    async def prep_database(self):\n        pass\n\n    ###########################################################################\n\n    def remove_db_file(self):\n        \"\"\"\n        Use with caution - removes the SQLite file. Useful for testing\n        purposes.\n        \"\"\"\n        if os.path.exists(self.path):\n            os.unlink(self.path)\n\n    def create_db_file(self):\n        \"\"\"\n        Create the database file. Useful for testing purposes.\n        \"\"\"\n        if os.path.exists(self.path):\n            raise Exception(f\"Database at {self.path} already exists\")\n        with open(self.path, \"w\"):\n            pass\n\n    ###########################################################################\n\n    async def batch(\n        self, query: Query, batch_size: int = 100, node: t.Optional[str] = None\n    ) -> AsyncBatch:\n        \"\"\"\n        :param query:\n            The database query to run.\n        :param batch_size:\n            How many rows to fetch on each iteration.\n        :param node:\n            This is ignored currently, as SQLite runs off a single node. The\n            value is here so the API is consistent with Postgres.\n        \"\"\"\n        connection = await self.get_connection()\n        return AsyncBatch(\n            connection=connection, query=query, batch_size=batch_size\n        )\n\n    ###########################################################################\n\n    async def get_connection(self) -> Connection:\n        connection = await aiosqlite.connect(**self.connection_kwargs)\n        connection.row_factory = dict_factory  # type: ignore\n        await connection.execute(\"PRAGMA foreign_keys = 1\")\n        return connection\n\n    ###########################################################################\n\n    async def _get_inserted_pk(self, cursor, table: t.Type[Table]) -> t.Any:\n        \"\"\"\n        If the `pk` column is a non-integer then `ROWID` and `pk` will return\n        different types. Need to query by `lastrowid` to get `pk`s in SQLite\n        prior to 3.35.0.\n        \"\"\"\n        await cursor.execute(\n            f\"SELECT {table._meta.primary_key._meta.db_column_name} FROM \"\n            f\"{table._meta.tablename} WHERE ROWID = {cursor.lastrowid}\"\n        )\n        response = await cursor.fetchone()\n        return response[table._meta.primary_key._meta.db_column_name]\n\n    async def _run_in_new_connection(\n        self,\n        query: str,\n        args: t.List[t.Any] = None,\n        query_type: str = \"generic\",\n        table: t.Optional[t.Type[Table]] = None,\n    ):\n        if args is None:\n            args = []\n        async with aiosqlite.connect(**self.connection_kwargs) as connection:\n            await connection.execute(\"PRAGMA foreign_keys = 1\")\n\n            connection.row_factory = dict_factory  # type: ignore\n            async with connection.execute(query, args) as cursor:\n                await connection.commit()\n\n                if query_type == \"insert\" and self.get_version_sync() < 3.35:\n                    # We can't use the RETURNING clause on older versions\n                    # of SQLite.\n                    assert table is not None\n                    pk = await self._get_inserted_pk(cursor, table)\n                    return [{table._meta.primary_key._meta.db_column_name: pk}]\n                else:\n                    return await cursor.fetchall()\n\n    async def _run_in_existing_connection(\n        self,\n        connection,\n        query: str,\n        args: t.List[t.Any] = None,\n        query_type: str = \"generic\",\n        table: t.Optional[t.Type[Table]] = None,\n    ):\n        \"\"\"\n        This is used when a transaction is currently active.\n        \"\"\"\n        if args is None:\n            args = []\n        await connection.execute(\"PRAGMA foreign_keys = 1\")\n\n        connection.row_factory = dict_factory\n        async with connection.execute(query, args) as cursor:\n            response = await cursor.fetchall()\n\n            if query_type == \"insert\" and self.get_version_sync() < 3.35:\n                # We can't use the RETURNING clause on older versions\n                # of SQLite.\n                assert table is not None\n                pk = await self._get_inserted_pk(cursor, table)\n                return [{table._meta.primary_key._meta.db_column_name: pk}]\n            else:\n                return response\n\n    async def run_querystring(\n        self, querystring: QueryString, in_pool: bool = False\n    ):\n        \"\"\"\n        Connection pools aren't currently supported - the argument is there\n        for consistency with other engines.\n        \"\"\"\n        query_id = self.get_query_id()\n\n        if self.log_queries:\n            self.print_query(query_id=query_id, query=querystring.__str__())\n\n        query, query_args = querystring.compile_string(\n            engine_type=self.engine_type\n        )\n\n        # If running inside a transaction:\n        current_transaction = self.current_transaction.get()\n        if current_transaction:\n            response = await self._run_in_existing_connection(\n                connection=current_transaction.connection,\n                query=query,\n                args=query_args,\n                query_type=querystring.query_type,\n                table=querystring.table,\n            )\n        else:\n            response = await self._run_in_new_connection(\n                query=query,\n                args=query_args,\n                query_type=querystring.query_type,\n                table=querystring.table,\n            )\n\n        if self.log_responses:\n            self.print_response(query_id=query_id, response=response)\n\n        return response\n\n    async def run_ddl(self, ddl: str, in_pool: bool = False):\n        \"\"\"\n        Connection pools aren't currently supported - the argument is there\n        for consistency with other engines.\n        \"\"\"\n        query_id = self.get_query_id()\n\n        if self.log_queries:\n            self.print_query(query_id=query_id, query=ddl)\n\n        # If running inside a transaction:\n        current_transaction = self.current_transaction.get()\n        if current_transaction:\n            response = await self._run_in_existing_connection(\n                connection=current_transaction.connection,\n                query=ddl,\n            )\n        else:\n            response = await self._run_in_new_connection(\n                query=ddl,\n            )\n\n        if self.log_responses:\n            self.print_response(query_id=query_id, response=response)\n\n        return response\n\n    def atomic(\n        self, transaction_type: TransactionType = TransactionType.deferred\n    ) -> Atomic:\n        return Atomic(engine=self, transaction_type=transaction_type)\n\n    def transaction(\n        self,\n        transaction_type: TransactionType = TransactionType.deferred,\n        allow_nested: bool = True,\n    ) -> SQLiteTransaction:\n        \"\"\"\n        Create a new database transaction. See :class:`Transaction`.\n        \"\"\"\n        return SQLiteTransaction(\n            engine=self,\n            transaction_type=transaction_type,\n            allow_nested=allow_nested,\n        )\n", "import asyncio\nimport typing as t\nfrom unittest import TestCase\n\nimport pytest\n\nfrom piccolo.engine.postgres import Atomic\nfrom piccolo.engine.sqlite import SQLiteEngine, TransactionType\nfrom piccolo.table import drop_db_tables_sync\nfrom piccolo.utils.sync import run_sync\nfrom tests.base import engines_only\nfrom tests.example_apps.music.tables import Band, Manager\n\n\nclass TestAtomic(TestCase):\n    def test_error(self):\n        \"\"\"\n        Make sure queries in a transaction aren't committed if a query fails.\n        \"\"\"\n        atomic = Band._meta.db.atomic()\n        atomic.add(\n            Manager.create_table(),\n            Band.create_table(),\n            Band.raw(\"MALFORMED QUERY ... SHOULD ERROR\"),\n        )\n        try:\n            atomic.run_sync()\n        except Exception:\n            pass\n        self.assertTrue(not Band.table_exists().run_sync())\n        self.assertTrue(not Manager.table_exists().run_sync())\n\n    def test_succeeds(self):\n        \"\"\"\n        Make sure that when atomic is run successfully the database is modified\n        accordingly.\n        \"\"\"\n        atomic = Band._meta.db.atomic()\n        atomic.add(Manager.create_table(), Band.create_table())\n        atomic.run_sync()\n\n        self.assertTrue(Band.table_exists().run_sync())\n        self.assertTrue(Manager.table_exists().run_sync())\n\n        drop_db_tables_sync(Band, Manager)\n\n    @engines_only(\"postgres\", \"cockroach\")\n    def test_pool(self):\n        \"\"\"\n        Make sure atomic works correctly when a connection pool is active.\n        \"\"\"\n\n        async def run():\n            \"\"\"\n            We have to run this async function, so we can use a connection\n            pool.\n            \"\"\"\n            engine = Band._meta.db\n            await engine.start_connection_pool()\n\n            atomic: Atomic = engine.atomic()\n            atomic.add(\n                Manager.create_table(),\n                Band.create_table(),\n            )\n\n            await atomic.run()\n            await engine.close_connection_pool()\n\n        run_sync(run())\n\n        self.assertTrue(Band.table_exists().run_sync())\n        self.assertTrue(Manager.table_exists().run_sync())\n\n        drop_db_tables_sync(Band, Manager)\n\n\nclass TestTransaction(TestCase):\n    def tearDown(self):\n        for table in (Band, Manager):\n            if table.table_exists().run_sync():\n                table.alter().drop_table().run_sync()\n\n    def test_error(self):\n        \"\"\"\n        Make sure queries in a transaction aren't committed if a query fails.\n        \"\"\"\n\n        async def run_transaction():\n            try:\n                async with Band._meta.db.transaction():\n                    Manager.create_table()\n                    Band.create_table()\n                    Band.raw(\"MALFORMED QUERY ... SHOULD ERROR\")\n            except Exception:\n                pass\n\n        asyncio.run(run_transaction())\n\n        self.assertTrue(not Band.table_exists().run_sync())\n        self.assertTrue(not Manager.table_exists().run_sync())\n\n    def test_succeeds(self):\n        async def run_transaction():\n            async with Band._meta.db.transaction():\n                await Manager.create_table().run()\n                await Band.create_table().run()\n\n        asyncio.run(run_transaction())\n\n        self.assertTrue(Band.table_exists().run_sync())\n        self.assertTrue(Manager.table_exists().run_sync())\n\n    def test_manual_commit(self):\n        \"\"\"\n        The context manager automatically commits changes, but we also\n        allow the user to do it manually.\n        \"\"\"\n\n        async def run_transaction():\n            async with Band._meta.db.transaction() as transaction:\n                await Manager.create_table()\n                await transaction.commit()\n\n        asyncio.run(run_transaction())\n        self.assertTrue(Manager.table_exists().run_sync())\n\n    def test_manual_rollback(self):\n        \"\"\"\n        The context manager will automatically rollback changes if an exception\n        is raised, but we also allow the user to do it manually.\n        \"\"\"\n\n        async def run_transaction():\n            async with Band._meta.db.transaction() as transaction:\n                await Manager.create_table()\n                await transaction.rollback()\n\n        asyncio.run(run_transaction())\n        self.assertFalse(Manager.table_exists().run_sync())\n\n    @engines_only(\"postgres\")\n    def test_transaction_id(self):\n        \"\"\"\n        An extra sanity check, that the transaction id is the same for each\n        query inside the transaction block.\n        \"\"\"\n\n        async def run_transaction():\n            responses = []\n            async with Band._meta.db.transaction():\n                responses.append(\n                    await Manager.raw(\"SELECT txid_current()\").run()\n                )\n                responses.append(\n                    await Manager.raw(\"SELECT txid_current()\").run()\n                )\n            return [i[0][\"txid_current\"] for i in responses]\n\n        txids = asyncio.run(run_transaction())\n        self.assertEqual(len(set(txids)), 1)\n\n        # Now run it again and make sure the transaction ids differ.\n        next_txids = asyncio.run(run_transaction())\n        self.assertNotEqual(txids, next_txids)\n\n\nclass TestTransactionExists(TestCase):\n    def test_exists(self):\n        \"\"\"\n        Make sure we can detect when code is within a transaction.\n        \"\"\"\n        engine = t.cast(SQLiteEngine, Manager._meta.db)\n\n        async def run_inside_transaction():\n            async with engine.transaction():\n                return engine.transaction_exists()\n\n        self.assertTrue(asyncio.run(run_inside_transaction()))\n\n        async def run_outside_transaction():\n            return engine.transaction_exists()\n\n        self.assertFalse(asyncio.run(run_outside_transaction()))\n\n\n@engines_only(\"sqlite\")\nclass TestTransactionType(TestCase):\n    def setUp(self):\n        Manager.create_table().run_sync()\n\n    def tearDown(self):\n        Manager.alter().drop_table().run_sync()\n\n    def test_transaction(self):\n        \"\"\"\n        With SQLite, we can specify the transaction type. This helps when\n        we want to do concurrent writes, to avoid locking the database.\n\n        https://github.com/piccolo-orm/piccolo/issues/687\n        \"\"\"\n        engine = t.cast(SQLiteEngine, Manager._meta.db)\n\n        async def run_transaction(name: str):\n            async with engine.transaction(\n                transaction_type=TransactionType.immediate\n            ):\n                # This does a SELECT followed by an INSERT, so is a good test.\n                # If using TransactionType.deferred it would fail because\n                # the database will become locked.\n                await Manager.objects().get_or_create(Manager.name == name)\n\n        manager_names = [f\"Manager_{i}\" for i in range(1, 10)]\n\n        async def run_all():\n            \"\"\"\n            Run all of the transactions concurrently.\n            \"\"\"\n            await asyncio.gather(\n                *[run_transaction(name=name) for name in manager_names]\n            )\n\n        asyncio.run(run_all())\n\n        # Make sure it all ran effectively.\n        self.assertListEqual(\n            Manager.select(Manager.name)\n            .order_by(Manager.name)\n            .output(as_list=True)\n            .run_sync(),\n            manager_names,\n        )\n\n    def test_atomic(self):\n        \"\"\"\n        Similar to above, but with ``Atomic``.\n        \"\"\"\n        engine = t.cast(SQLiteEngine, Manager._meta.db)\n\n        async def run_atomic(name: str):\n            atomic = engine.atomic(transaction_type=TransactionType.immediate)\n            atomic.add(Manager.objects().get_or_create(Manager.name == name))\n            await atomic.run()\n\n        manager_names = [f\"Manager_{i}\" for i in range(1, 10)]\n\n        async def run_all():\n            \"\"\"\n            Run all of the transactions concurrently.\n            \"\"\"\n            await asyncio.gather(\n                *[run_atomic(name=name) for name in manager_names]\n            )\n\n        asyncio.run(run_all())\n\n        # Make sure it all ran effectively.\n        self.assertListEqual(\n            Manager.select(Manager.name)\n            .order_by(Manager.name)\n            .output(as_list=True)\n            .run_sync(),\n            manager_names,\n        )\n\n\nclass TestSavepoint(TestCase):\n    def setUp(self):\n        Manager.create_table().run_sync()\n\n    def tearDown(self):\n        Manager.alter().drop_table().run_sync()\n\n    def test_savepoint(self):\n        async def run_test():\n            async with Manager._meta.db.transaction() as transaction:\n                await Manager.insert(Manager(name=\"Manager 1\"))\n                savepoint = await transaction.savepoint()\n                await Manager.insert(Manager(name=\"Manager 2\"))\n                await savepoint.rollback_to()\n\n        run_sync(run_test())\n\n        self.assertListEqual(\n            Manager.select(Manager.name).run_sync(), [{\"name\": \"Manager 1\"}]\n        )\n\n    def test_named_savepoint(self):\n        async def run_test():\n            async with Manager._meta.db.transaction() as transaction:\n                await Manager.insert(Manager(name=\"Manager 1\"))\n                await transaction.savepoint(\"my_savepoint\")\n                await Manager.insert(Manager(name=\"Manager 2\"))\n                await transaction.rollback_to(\"my_savepoint\")\n\n        run_sync(run_test())\n\n        self.assertListEqual(\n            Manager.select(Manager.name).run_sync(), [{\"name\": \"Manager 1\"}]\n        )\n\n    def test_savepoint_sqli_checks(self):\n        # Added to test the fix for GHSA-xq59-7jf3-rjc6\n        async def run_test():\n            async with Manager._meta.db.transaction() as transaction:\n                await transaction.savepoint(\n                    \"my_savepoint; SELECT * FROM Manager\"\n                )\n\n        with pytest.raises(ValueError):\n            run_sync(run_test())\n"], "filenames": ["piccolo/engine/base.py", "piccolo/engine/postgres.py", "piccolo/engine/sqlite.py", "tests/engine/test_transaction.py"], "buggy_code_start_loc": [5, 7, 13, 3], "buggy_code_end_loc": [17, 238, 415, 298], "fixing_code_start_loc": [6, 7, 13, 4], "fixing_code_end_loc": [33, 242, 419, 312], "type": "CWE-89", "message": "Piccolo is an object-relational mapping and query builder which supports asyncio. Prior to version 1.1.1, the handling of named transaction `savepoints` in all database implementations is vulnerable to SQL Injection via f-strings. While the likelihood of an end developer exposing a `savepoints` `name` parameter to a user is highly unlikely, it would not be unheard of. If a malicious user was able to abuse this functionality they would have essentially direct access to the database and the ability to modify data to the level of permissions associated with the database user. A non exhaustive list of actions possible based on database permissions is: Read all data stored in the database, including usernames and password hashes; insert arbitrary data into the database, including modifying existing records; and gain a shell on the underlying server. Version 1.1.1 fixes this issue.", "other": {"cve": {"id": "CVE-2023-47128", "sourceIdentifier": "security-advisories@github.com", "published": "2023-11-10T18:15:09.870", "lastModified": "2023-11-20T19:35:23.087", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Piccolo is an object-relational mapping and query builder which supports asyncio. Prior to version 1.1.1, the handling of named transaction `savepoints` in all database implementations is vulnerable to SQL Injection via f-strings. While the likelihood of an end developer exposing a `savepoints` `name` parameter to a user is highly unlikely, it would not be unheard of. If a malicious user was able to abuse this functionality they would have essentially direct access to the database and the ability to modify data to the level of permissions associated with the database user. A non exhaustive list of actions possible based on database permissions is: Read all data stored in the database, including usernames and password hashes; insert arbitrary data into the database, including modifying existing records; and gain a shell on the underlying server. Version 1.1.1 fixes this issue."}, {"lang": "es", "value": "Piccolo es un generador de consultas y mapeo relacional de objetos que admite asyncio. Antes de la versi\u00f3n 1.1.1, el manejo de \"savepoints\" de transacciones con nombre en todas las implementaciones de bases de datos es vulnerable a la inyecci\u00f3n SQL a trav\u00e9s de cadenas f. Si bien la probabilidad de que un desarrollador final exponga un par\u00e1metro de \"name\" de \"savepoints\" a un usuario es muy poco probable, no ser\u00eda algo inaudito. Si un usuario malintencionado pudiera abusar de esta funcionalidad, tendr\u00eda esencialmente acceso directo a la base de datos y la capacidad de modificar los datos al nivel de permisos asociados con el usuario de la base de datos. Una lista no exhaustiva de acciones posibles seg\u00fan los permisos de la base de datos es: Leer todos los datos almacenados en la base de datos, incluidos los nombres de usuario y los hashes de contrase\u00f1as; insertar datos arbitrarios en la base de datos, incluida la modificaci\u00f3n de registros existentes; y obtener un shell en el servidor subyacente. La versi\u00f3n 1.1.1 soluciona este problema."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 9.1, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.2}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 9.1, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.2}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-89"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:piccolo-orm:piccolo:1.1.0:*:*:*:*:*:*:*", "matchCriteriaId": "355EE2C8-1D3F-458B-8A51-4A6904485DD1"}]}]}], "references": [{"url": "https://github.com/piccolo-orm/piccolo/commit/82679eb8cd1449cf31d87c9914a072e70168b6eb", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/piccolo-orm/piccolo/security/advisories/GHSA-xq59-7jf3-rjc6", "source": "security-advisories@github.com", "tags": ["Exploit", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/piccolo-orm/piccolo/commit/82679eb8cd1449cf31d87c9914a072e70168b6eb"}}