{"buggy_code": ["import logging\n\nfrom contextlib import ExitStack\nfrom dateutil import parser\nfrom functools import wraps\nimport socket\nimport ipaddress\nfrom urllib.parse import urlparse\n\nfrom six import text_type\nfrom sshtunnel import open_tunnel\nfrom redash import settings, utils\nfrom redash.utils import json_loads, query_is_select_no_limit, add_limit_to_query\nfrom rq.timeouts import JobTimeoutException\n\nfrom redash.utils.requests_session import requests, requests_session\n\nlogger = logging.getLogger(__name__)\n\n__all__ = [\n    \"BaseQueryRunner\",\n    \"BaseHTTPQueryRunner\",\n    \"InterruptException\",\n    \"JobTimeoutException\",\n    \"BaseSQLQueryRunner\",\n    \"TYPE_DATETIME\",\n    \"TYPE_BOOLEAN\",\n    \"TYPE_INTEGER\",\n    \"TYPE_STRING\",\n    \"TYPE_DATE\",\n    \"TYPE_FLOAT\",\n    \"SUPPORTED_COLUMN_TYPES\",\n    \"register\",\n    \"get_query_runner\",\n    \"import_query_runners\",\n    \"guess_type\",\n]\n\n# Valid types of columns returned in results:\nTYPE_INTEGER = \"integer\"\nTYPE_FLOAT = \"float\"\nTYPE_BOOLEAN = \"boolean\"\nTYPE_STRING = \"string\"\nTYPE_DATETIME = \"datetime\"\nTYPE_DATE = \"date\"\n\nSUPPORTED_COLUMN_TYPES = set(\n    [TYPE_INTEGER, TYPE_FLOAT, TYPE_BOOLEAN, TYPE_STRING, TYPE_DATETIME, TYPE_DATE]\n)\n\n\nclass InterruptException(Exception):\n    pass\n\n\nclass NotSupported(Exception):\n    pass\n\n\nclass BaseQueryRunner(object):\n    deprecated = False\n    should_annotate_query = True\n    noop_query = None\n\n    def __init__(self, configuration):\n        self.syntax = \"sql\"\n        self.configuration = configuration\n\n    @classmethod\n    def name(cls):\n        return cls.__name__\n\n    @classmethod\n    def type(cls):\n        return cls.__name__.lower()\n\n    @classmethod\n    def enabled(cls):\n        return True\n\n    @property\n    def host(self):\n        \"\"\"Returns this query runner's configured host.\n        This is used primarily for temporarily swapping endpoints when using SSH tunnels to connect to a data source.\n\n        `BaseQueryRunner`'s na\u00efve implementation supports query runner implementations that store endpoints using `host` and `port`\n        configuration values. If your query runner uses a different schema (e.g. a web address), you should override this function.\n        \"\"\"\n        if \"host\" in self.configuration:\n            return self.configuration[\"host\"]\n        else:\n            raise NotImplementedError()\n\n    @host.setter\n    def host(self, host):\n        \"\"\"Sets this query runner's configured host.\n        This is used primarily for temporarily swapping endpoints when using SSH tunnels to connect to a data source.\n\n        `BaseQueryRunner`'s na\u00efve implementation supports query runner implementations that store endpoints using `host` and `port`\n        configuration values. If your query runner uses a different schema (e.g. a web address), you should override this function.\n        \"\"\"\n        if \"host\" in self.configuration:\n            self.configuration[\"host\"] = host\n        else:\n            raise NotImplementedError()\n\n    @property\n    def port(self):\n        \"\"\"Returns this query runner's configured port.\n        This is used primarily for temporarily swapping endpoints when using SSH tunnels to connect to a data source.\n\n        `BaseQueryRunner`'s na\u00efve implementation supports query runner implementations that store endpoints using `host` and `port`\n        configuration values. If your query runner uses a different schema (e.g. a web address), you should override this function.\n        \"\"\"\n        if \"port\" in self.configuration:\n            return self.configuration[\"port\"]\n        else:\n            raise NotImplementedError()\n\n    @port.setter\n    def port(self, port):\n        \"\"\"Sets this query runner's configured port.\n        This is used primarily for temporarily swapping endpoints when using SSH tunnels to connect to a data source.\n\n        `BaseQueryRunner`'s na\u00efve implementation supports query runner implementations that store endpoints using `host` and `port`\n        configuration values. If your query runner uses a different schema (e.g. a web address), you should override this function.\n        \"\"\"\n        if \"port\" in self.configuration:\n            self.configuration[\"port\"] = port\n        else:\n            raise NotImplementedError()\n\n    @classmethod\n    def configuration_schema(cls):\n        return {}\n\n    def annotate_query(self, query, metadata):\n        if not self.should_annotate_query:\n            return query\n\n        annotation = \", \".join([\"{}: {}\".format(k, v) for k, v in metadata.items()])\n        annotated_query = \"/* {} */ {}\".format(annotation, query)\n        return annotated_query\n\n    def test_connection(self):\n        if self.noop_query is None:\n            raise NotImplementedError()\n        data, error = self.run_query(self.noop_query, None)\n\n        if error is not None:\n            raise Exception(error)\n\n    def run_query(self, query, user):\n        raise NotImplementedError()\n\n    def fetch_columns(self, columns):\n        column_names = []\n        duplicates_counter = 1\n        new_columns = []\n\n        for col in columns:\n            column_name = col[0]\n            if column_name in column_names:\n                column_name = \"{}{}\".format(column_name, duplicates_counter)\n                duplicates_counter += 1\n\n            column_names.append(column_name)\n            new_columns.append(\n                {\"name\": column_name, \"friendly_name\": column_name, \"type\": col[1]}\n            )\n\n        return new_columns\n\n    def get_schema(self, get_stats=False):\n        raise NotSupported()\n\n    def _run_query_internal(self, query):\n        results, error = self.run_query(query, None)\n\n        if error is not None:\n            raise Exception(\"Failed running query [%s].\" % query)\n        return json_loads(results)[\"rows\"]\n\n    @classmethod\n    def to_dict(cls):\n        return {\n            \"name\": cls.name(),\n            \"type\": cls.type(),\n            \"configuration_schema\": cls.configuration_schema(),\n            **({\"deprecated\": True} if cls.deprecated else {}),\n        }\n\n    @property\n    def supports_auto_limit(self):\n        return False\n\n    def apply_auto_limit(self, query_text, should_apply_auto_limit):\n        return query_text\n\n    def gen_query_hash(self, query_text, set_auto_limit=False):\n        query_text = self.apply_auto_limit(query_text, set_auto_limit)\n        return utils.gen_query_hash(query_text)\n\n\nclass BaseSQLQueryRunner(BaseQueryRunner):\n    def get_schema(self, get_stats=False):\n        schema_dict = {}\n        self._get_tables(schema_dict)\n        if settings.SCHEMA_RUN_TABLE_SIZE_CALCULATIONS and get_stats:\n            self._get_tables_stats(schema_dict)\n        return list(schema_dict.values())\n\n    def _get_tables(self, schema_dict):\n        return []\n\n    def _get_tables_stats(self, tables_dict):\n        for t in tables_dict.keys():\n            if type(tables_dict[t]) == dict:\n                res = self._run_query_internal(\"select count(*) as cnt from %s\" % t)\n                tables_dict[t][\"size\"] = res[0][\"cnt\"]\n\n    @property\n    def supports_auto_limit(self):\n        return True\n\n    def apply_auto_limit(self, query_text, should_apply_auto_limit):\n        if should_apply_auto_limit:\n            from redash.query_runner.databricks import split_sql_statements, combine_sql_statements\n            queries = split_sql_statements(query_text)\n            # we only check for last one in the list because it is the one that we show result\n            last_query = queries[-1]\n            if query_is_select_no_limit(last_query):\n                queries[-1] = add_limit_to_query(last_query)\n            return combine_sql_statements(queries)\n        else:\n            return query_text\n\n\ndef is_private_address(url):\n    hostname = urlparse(url).hostname\n    ip_address = socket.gethostbyname(hostname)\n    return ipaddress.ip_address(text_type(ip_address)).is_private\n\n\nclass BaseHTTPQueryRunner(BaseQueryRunner):\n    should_annotate_query = False\n    response_error = \"Endpoint returned unexpected status code\"\n    requires_authentication = False\n    requires_url = True\n    url_title = \"URL base path\"\n    username_title = \"HTTP Basic Auth Username\"\n    password_title = \"HTTP Basic Auth Password\"\n\n    @classmethod\n    def configuration_schema(cls):\n        schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"url\": {\"type\": \"string\", \"title\": cls.url_title},\n                \"username\": {\"type\": \"string\", \"title\": cls.username_title},\n                \"password\": {\"type\": \"string\", \"title\": cls.password_title},\n            },\n            \"secret\": [\"password\"],\n            \"order\": [\"url\", \"username\", \"password\"],\n        }\n\n        if cls.requires_url or cls.requires_authentication:\n            schema[\"required\"] = []\n\n        if cls.requires_url:\n            schema[\"required\"] += [\"url\"]\n\n        if cls.requires_authentication:\n            schema[\"required\"] += [\"username\", \"password\"]\n        return schema\n\n    def get_auth(self):\n        username = self.configuration.get(\"username\")\n        password = self.configuration.get(\"password\")\n        if username and password:\n            return (username, password)\n        if self.requires_authentication:\n            raise ValueError(\"Username and Password required\")\n        else:\n            return None\n\n    def get_response(self, url, auth=None, http_method=\"get\", **kwargs):\n        if is_private_address(url) and settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\n            raise Exception(\"Can't query private addresses.\")\n\n        # Get authentication values if not given\n        if auth is None:\n            auth = self.get_auth()\n\n        # Then call requests to get the response from the given endpoint\n        # URL optionally, with the additional requests parameters.\n        error = None\n        response = None\n        try:\n            response = requests_session.request(http_method, url, auth=auth, **kwargs)\n            # Raise a requests HTTP exception with the appropriate reason\n            # for 4xx and 5xx response status codes which is later caught\n            # and passed back.\n            response.raise_for_status()\n\n            # Any other responses (e.g. 2xx and 3xx):\n            if response.status_code != 200:\n                error = \"{} ({}).\".format(self.response_error, response.status_code)\n\n        except requests.HTTPError as exc:\n            logger.exception(exc)\n            error = \"Failed to execute query. \" \"Return Code: {} Reason: {}\".format(\n                response.status_code, response.text\n            )\n        except requests.RequestException as exc:\n            # Catch all other requests exceptions and return the error.\n            logger.exception(exc)\n            error = str(exc)\n\n        # Return response and error.\n        return response, error\n\n\nquery_runners = {}\n\n\ndef register(query_runner_class):\n    global query_runners\n    if query_runner_class.enabled():\n        logger.debug(\n            \"Registering %s (%s) query runner.\",\n            query_runner_class.name(),\n            query_runner_class.type(),\n        )\n        query_runners[query_runner_class.type()] = query_runner_class\n    else:\n        logger.debug(\n            \"%s query runner enabled but not supported, not registering. Either disable or install missing \"\n            \"dependencies.\",\n            query_runner_class.name(),\n        )\n\n\ndef get_query_runner(query_runner_type, configuration):\n    query_runner_class = query_runners.get(query_runner_type, None)\n    if query_runner_class is None:\n        return None\n\n    return query_runner_class(configuration)\n\n\ndef get_configuration_schema_for_query_runner_type(query_runner_type):\n    query_runner_class = query_runners.get(query_runner_type, None)\n    if query_runner_class is None:\n        return None\n\n    return query_runner_class.configuration_schema()\n\n\ndef import_query_runners(query_runner_imports):\n    for runner_import in query_runner_imports:\n        __import__(runner_import)\n\n\ndef guess_type(value):\n    if isinstance(value, bool):\n        return TYPE_BOOLEAN\n    elif isinstance(value, int):\n        return TYPE_INTEGER\n    elif isinstance(value, float):\n        return TYPE_FLOAT\n\n    return guess_type_from_string(value)\n\n\ndef guess_type_from_string(string_value):\n    if string_value == \"\" or string_value is None:\n        return TYPE_STRING\n\n    try:\n        int(string_value)\n        return TYPE_INTEGER\n    except (ValueError, OverflowError):\n        pass\n\n    try:\n        float(string_value)\n        return TYPE_FLOAT\n    except (ValueError, OverflowError):\n        pass\n\n    if str(string_value).lower() in (\"true\", \"false\"):\n        return TYPE_BOOLEAN\n\n    try:\n        parser.parse(string_value)\n        return TYPE_DATETIME\n    except (ValueError, OverflowError):\n        pass\n\n    return TYPE_STRING\n\n\ndef with_ssh_tunnel(query_runner, details):\n    def tunnel(f):\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            try:\n                remote_host, remote_port = query_runner.host, query_runner.port\n            except NotImplementedError:\n                raise NotImplementedError(\n                    \"SSH tunneling is not implemented for this query runner yet.\"\n                )\n\n            stack = ExitStack()\n            try:\n                bastion_address = (details[\"ssh_host\"], details.get(\"ssh_port\", 22))\n                remote_address = (remote_host, remote_port)\n                auth = {\n                    \"ssh_username\": details[\"ssh_username\"],\n                    **settings.dynamic_settings.ssh_tunnel_auth(),\n                }\n                server = stack.enter_context(\n                    open_tunnel(\n                        bastion_address, remote_bind_address=remote_address, **auth\n                    )\n                )\n            except Exception as error:\n                raise type(error)(\"SSH tunnel: {}\".format(str(error)))\n\n            with stack:\n                try:\n                    query_runner.host, query_runner.port = server.local_bind_address\n                    result = f(*args, **kwargs)\n                finally:\n                    query_runner.host, query_runner.port = remote_host, remote_port\n\n                return result\n\n        return wrapper\n\n    query_runner.run_query = tunnel(query_runner.run_query)\n\n    return query_runner\n", "import logging\nimport yaml\nimport requests\nimport io\n\nfrom redash import settings\nfrom redash.query_runner import *\nfrom redash.utils import json_dumps\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import pandas as pd\n    import numpy as np\n    enabled = True\nexcept ImportError:\n    enabled = False\n\n\nclass CSV(BaseQueryRunner):\n    should_annotate_query = False\n\n    @classmethod\n    def name(cls):\n        return \"CSV\"\n\n    @classmethod\n    def enabled(cls):\n        return enabled\n\n    @classmethod\n    def configuration_schema(cls):\n        return {\n            'type': 'object',\n            'properties': {},\n        }\n\n    def __init__(self, configuration):\n        super(CSV, self).__init__(configuration)\n        self.syntax = \"yaml\"\n\n    def test_connection(self):\n        pass\n\n    def run_query(self, query, user):\n        path = \"\"\n        ua = \"\"\n        args = {}\n        try:\n            args = yaml.safe_load(query)\n            path = args['url']\n            args.pop('url', None)\n            ua = args['user-agent']\n            args.pop('user-agent', None)\n\n            if is_private_address(path) and settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\n                raise Exception(\"Can't query private addresses.\")\n        except:\n            pass\n\n        try:\n            response = requests.get(url=path, headers={\"User-agent\": ua})\n            workbook = pd.read_csv(io.BytesIO(response.content),sep=\",\", **args)\n\n            df = workbook.copy()\n            data = {'columns': [], 'rows': []}\n            conversions = [\n                {'pandas_type': np.integer, 'redash_type': 'integer',},\n                {'pandas_type': np.inexact, 'redash_type': 'float',},\n                {'pandas_type': np.datetime64, 'redash_type': 'datetime', 'to_redash': lambda x: x.strftime('%Y-%m-%d %H:%M:%S')},\n                {'pandas_type': np.bool_, 'redash_type': 'boolean'},\n                {'pandas_type': np.object, 'redash_type': 'string'}\n            ]\n            labels = []\n            for dtype, label in zip(df.dtypes, df.columns):\n                for conversion in conversions:\n                    if issubclass(dtype.type, conversion['pandas_type']):\n                        data['columns'].append({'name': label, 'friendly_name': label, 'type': conversion['redash_type']})\n                        labels.append(label)\n                        func = conversion.get('to_redash')\n                        if func:\n                            df[label] = df[label].apply(func)\n                        break\n            data['rows'] = df[labels].replace({np.nan: None}).to_dict(orient='records')\n\n            json_data = json_dumps(data)\n            error = None\n        except KeyboardInterrupt:\n            error = \"Query cancelled by user.\"\n            json_data = None\n        except Exception as e:\n            error = \"Error reading {0}. {1}\".format(path, str(e))\n            json_data = None\n\n        return json_data, error\n\n    def get_schema(self):\n        raise NotSupported()\n\nregister(CSV)\n", "import logging\nimport yaml\nimport requests\n\nfrom redash import settings\nfrom redash.query_runner import *\nfrom redash.utils import json_dumps\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import pandas as pd\n    import xlrd\n    import openpyxl\n    import numpy as np\n    enabled = True\nexcept ImportError:\n    enabled = False\n\nclass Excel(BaseQueryRunner):\n    should_annotate_query = False\n\n    @classmethod\n    def enabled(cls):\n        return enabled\n\n    @classmethod\n    def configuration_schema(cls):\n        return {\n            'type': 'object',\n            'properties': {},\n        }\n\n    def __init__(self, configuration):\n        super(Excel, self).__init__(configuration)\n        self.syntax = \"yaml\"\n\n    def test_connection(self):\n        pass\n\n    def run_query(self, query, user):\n        path = \"\"\n        ua = \"\"\n        args = {}\n        try:\n            args = yaml.safe_load(query)\n            path = args['url']\n            args.pop('url', None)\n            ua = args['user-agent']\n            args.pop('user-agent', None)\n\n            if is_private_address(path) and settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\n                raise Exception(\"Can't query private addresses.\")\n        except:\n            pass\n\n        try:\n            response = requests.get(url=path, headers={\"User-agent\": ua})\n            workbook = pd.read_excel(response.content, **args)\n\n            df = workbook.copy()\n            data = {'columns': [], 'rows': []}\n            conversions = [\n                {'pandas_type': np.integer, 'redash_type': 'integer',},\n                {'pandas_type': np.inexact, 'redash_type': 'float',},\n                {'pandas_type': np.datetime64, 'redash_type': 'datetime', 'to_redash': lambda x: x.strftime('%Y-%m-%d %H:%M:%S')},\n                {'pandas_type': np.bool_, 'redash_type': 'boolean'},\n                {'pandas_type': np.object, 'redash_type': 'string'}\n            ]\n            labels = []\n            for dtype, label in zip(df.dtypes, df.columns):\n                for conversion in conversions:\n                    if issubclass(dtype.type, conversion['pandas_type']):\n                        data['columns'].append({'name': label, 'friendly_name': label, 'type': conversion['redash_type']})\n                        labels.append(label)\n                        func = conversion.get('to_redash')\n                        if func:\n                            df[label] = df[label].apply(func)\n                        break\n            data['rows'] = df[labels].replace({np.nan: None}).to_dict(orient='records')\n\n            json_data = json_dumps(data)\n            error = None\n        except KeyboardInterrupt:\n            error = \"Query cancelled by user.\"\n            json_data = None\n        except Exception as e:\n            error = \"Error reading {0}. {1}\".format(path, str(e))\n            json_data = None\n\n        return json_data, error\n\n    def get_schema(self):\n        raise NotSupported()\n\nregister(Excel)\n", "import logging\nimport yaml\nimport datetime\nfrom funcy import compact, project\nfrom redash import settings\nfrom redash.utils import json_dumps\nfrom redash.query_runner import (\n    BaseHTTPQueryRunner,\n    register,\n    TYPE_BOOLEAN,\n    TYPE_DATETIME,\n    TYPE_FLOAT,\n    TYPE_INTEGER,\n    TYPE_STRING,\n    is_private_address,\n)\n\n\nclass QueryParseError(Exception):\n    pass\n\n\ndef parse_query(query):\n    # TODO: copy paste from Metrica query runner, we should extract this into a utility\n    query = query.strip()\n    if query == \"\":\n        raise QueryParseError(\"Query is empty.\")\n    try:\n        params = yaml.safe_load(query)\n        return params\n    except ValueError as e:\n        logging.exception(e)\n        error = str(e)\n        raise QueryParseError(error)\n\n\nTYPES_MAP = {\n    str: TYPE_STRING,\n    bytes: TYPE_STRING,\n    int: TYPE_INTEGER,\n    float: TYPE_FLOAT,\n    bool: TYPE_BOOLEAN,\n    datetime.datetime: TYPE_DATETIME,\n}\n\n\ndef _get_column_by_name(columns, column_name):\n    for c in columns:\n        if \"name\" in c and c[\"name\"] == column_name:\n            return c\n\n    return None\n\n\ndef _get_type(value):\n    return TYPES_MAP.get(type(value), TYPE_STRING)\n\n\ndef add_column(columns, column_name, column_type):\n    if _get_column_by_name(columns, column_name) is None:\n        columns.append(\n            {\"name\": column_name, \"friendly_name\": column_name, \"type\": column_type}\n        )\n\n\ndef _apply_path_search(response, path):\n    if path is None:\n        return response\n\n    path_parts = path.split(\".\")\n    path_parts.reverse()\n    while len(path_parts) > 0:\n        current_path = path_parts.pop()\n        if current_path in response:\n            response = response[current_path]\n        else:\n            raise Exception(\"Couldn't find path {} in response.\".format(path))\n\n    return response\n\n\ndef _normalize_json(data, path):\n    data = _apply_path_search(data, path)\n\n    if isinstance(data, dict):\n        data = [data]\n\n    return data\n\n\ndef _sort_columns_with_fields(columns, fields):\n    if fields:\n        columns = compact([_get_column_by_name(columns, field) for field in fields])\n\n    return columns\n\n\n# TODO: merge the logic here with the one in MongoDB's queyr runner\ndef parse_json(data, path, fields):\n    data = _normalize_json(data, path)\n\n    rows = []\n    columns = []\n\n    for row in data:\n        parsed_row = {}\n\n        for key in row:\n            if isinstance(row[key], dict):\n                for inner_key in row[key]:\n                    column_name = \"{}.{}\".format(key, inner_key)\n                    if fields and key not in fields and column_name not in fields:\n                        continue\n\n                    value = row[key][inner_key]\n                    add_column(columns, column_name, _get_type(value))\n                    parsed_row[column_name] = value\n            else:\n                if fields and key not in fields:\n                    continue\n\n                value = row[key]\n                add_column(columns, key, _get_type(value))\n                parsed_row[key] = row[key]\n\n        rows.append(parsed_row)\n\n    columns = _sort_columns_with_fields(columns, fields)\n\n    return {\"rows\": rows, \"columns\": columns}\n\n\nclass JSON(BaseHTTPQueryRunner):\n    requires_url = False\n\n    @classmethod\n    def configuration_schema(cls):\n        return {\n            \"type\": \"object\",\n            \"properties\": {\n                \"username\": {\"type\": \"string\", \"title\": cls.username_title},\n                \"password\": {\"type\": \"string\", \"title\": cls.password_title},\n            },\n            \"secret\": [\"password\"],\n            \"order\": [\"username\", \"password\"],\n        }\n\n    def __init__(self, configuration):\n        super(JSON, self).__init__(configuration)\n        self.syntax = \"yaml\"\n\n    def test_connection(self):\n        pass\n\n    def run_query(self, query, user):\n        query = parse_query(query)\n\n        if not isinstance(query, dict):\n            raise QueryParseError(\n                \"Query should be a YAML object describing the URL to query.\"\n            )\n\n        if \"url\" not in query:\n            raise QueryParseError(\"Query must include 'url' option.\")\n\n        if is_private_address(query[\"url\"]) and settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\n            raise Exception(\"Can't query private addresses.\")\n\n        method = query.get(\"method\", \"get\")\n        request_options = project(query, (\"params\", \"headers\", \"data\", \"auth\", \"json\"))\n\n        fields = query.get(\"fields\")\n        path = query.get(\"path\")\n\n        if isinstance(request_options.get(\"auth\", None), list):\n            request_options[\"auth\"] = tuple(request_options[\"auth\"])\n        elif self.configuration.get(\"username\") or self.configuration.get(\"password\"):\n            request_options[\"auth\"] = (\n                self.configuration.get(\"username\"),\n                self.configuration.get(\"password\"),\n            )\n\n        if method not in (\"get\", \"post\"):\n            raise QueryParseError(\"Only GET or POST methods are allowed.\")\n\n        if fields and not isinstance(fields, list):\n            raise QueryParseError(\"'fields' needs to be a list.\")\n\n        response, error = self.get_response(\n            query[\"url\"], http_method=method, **request_options\n        )\n\n        if error is not None:\n            return None, error\n\n        data = json_dumps(parse_json(response.json(), path, fields))\n\n        if data:\n            return data, None\n        else:\n            return None, \"Got empty response from '{}'.\".format(query[\"url\"])\n\n\nregister(JSON)\n", "import requests\nfrom redash import settings\n\n\nclass ConfiguredSession(requests.Session):\n    def request(self, *args, **kwargs):\n        if not settings.REQUESTS_ALLOW_REDIRECTS:\n            kwargs.update({\"allow_redirects\": False})\n        return super().request(*args, **kwargs)\n\n\nrequests_session = ConfiguredSession()\n", "google-api-python-client==1.7.11\nprotobuf==3.17.3\ngspread==3.1.0\nimpyla==0.16.0\ninfluxdb==5.2.3\nmysqlclient==1.3.14\noauth2client==4.1.3\npyhive==0.6.1\npymongo[tls,srv]==3.9.0\nvertica-python==0.9.5\ntd-client==1.0.0\npymssql==2.1.4\ndql==0.5.26\ndynamo3==0.4.10\nboto3>=1.10.0,<1.11.0\nbotocore>=1.13,<1.14.0\nsasl>=0.1.3\nthrift>=0.8.0\nthrift_sasl>=0.1.0\ncassandra-driver==3.21.0\nmemsql==3.0.0\natsd_client==3.0.5\nsimple_salesforce==0.74.3\nPyAthena>=1.5.0\npymapd==0.19.0\nqds-sdk>=1.9.6\nibm-db>=2.0.9\npydruid==0.5.7\nrequests_aws_sign==0.1.5\nsnowflake-connector-python==2.1.3\nphoenixdb==0.7\n# certifi is needed to support MongoDB and SSL:\ncertifi>=2019.9.11\npydgraph==2.0.2\nazure-kusto-data==0.0.35\npyexasol==0.12.0\npython-rapidjson==0.8.0\npyodbc==4.0.28\ntrino~=0.305\ncmem-cmempy==21.2.3\nxlrd==2.0.1\nopenpyxl==3.0.7\nfirebolt-sqlalchemy\n", "import mock\nfrom unittest import TestCase\n\nfrom redash.utils.requests_session import requests, ConfiguredSession\nfrom redash.query_runner import BaseHTTPQueryRunner\n\n\nclass RequiresAuthQueryRunner(BaseHTTPQueryRunner):\n    requires_authentication = True\n\n\nclass TestBaseHTTPQueryRunner(TestCase):\n    def test_requires_authentication_default(self):\n        self.assertFalse(BaseHTTPQueryRunner.requires_authentication)\n        schema = BaseHTTPQueryRunner.configuration_schema()\n        self.assertNotIn(\"username\", schema[\"required\"])\n        self.assertNotIn(\"password\", schema[\"required\"])\n\n    def test_requires_authentication_true(self):\n        schema = RequiresAuthQueryRunner.configuration_schema()\n        self.assertIn(\"username\", schema[\"required\"])\n        self.assertIn(\"password\", schema[\"required\"])\n\n    def test_get_auth_with_values(self):\n        query_runner = BaseHTTPQueryRunner(\n            {\"username\": \"username\", \"password\": \"password\"}\n        )\n        self.assertEqual(query_runner.get_auth(), (\"username\", \"password\"))\n\n    def test_get_auth_empty(self):\n        query_runner = BaseHTTPQueryRunner({})\n        self.assertIsNone(query_runner.get_auth())\n\n    def test_get_auth_empty_requires_authentication(self):\n        query_runner = RequiresAuthQueryRunner({})\n        self.assertRaisesRegex(\n            ValueError, \"Username and Password required\", query_runner.get_auth\n        )\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_success(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 200\n        mock_response.text = \"Success\"\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        response, error = query_runner.get_response(url)\n        mock_get.assert_called_once_with(\"get\", url, auth=None)\n        self.assertEqual(response.status_code, 200)\n        self.assertIsNone(error)\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_success_custom_auth(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 200\n        mock_response.text = \"Success\"\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        auth = (\"username\", \"password\")\n        response, error = query_runner.get_response(url, auth=auth)\n        mock_get.assert_called_once_with(\"get\", url, auth=auth)\n        self.assertEqual(response.status_code, 200)\n        self.assertIsNone(error)\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_failure(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 301\n        mock_response.text = \"Redirect\"\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        response, error = query_runner.get_response(url)\n        mock_get.assert_called_once_with(\"get\", url, auth=None)\n        self.assertIn(query_runner.response_error, error)\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_httperror_exception(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 500\n        mock_response.text = \"Server Error\"\n        http_error = requests.HTTPError()\n        mock_response.raise_for_status.side_effect = http_error\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        response, error = query_runner.get_response(url)\n        mock_get.assert_called_once_with(\"get\", url, auth=None)\n        self.assertIsNotNone(error)\n        self.assertIn(\"Failed to execute query\", error)\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_requests_exception(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 500\n        mock_response.text = \"Server Error\"\n        exception_message = \"Some requests exception\"\n        requests_exception = requests.RequestException(exception_message)\n        mock_response.raise_for_status.side_effect = requests_exception\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        response, error = query_runner.get_response(url)\n        mock_get.assert_called_once_with(\"get\", url, auth=None)\n        self.assertIsNotNone(error)\n        self.assertEqual(exception_message, error)\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_generic_exception(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 500\n        mock_response.text = \"Server Error\"\n        exception_message = \"Some generic exception\"\n        exception = ValueError(exception_message)\n        mock_response.raise_for_status.side_effect = exception\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        self.assertRaisesRegex(\n            ValueError, exception_message, query_runner.get_response, url\n        )\n"], "fixing_code": ["import logging\n\nfrom contextlib import ExitStack\nfrom dateutil import parser\nfrom functools import wraps\nimport socket\nimport ipaddress\nfrom urllib.parse import urlparse\n\nfrom six import text_type\nfrom sshtunnel import open_tunnel\nfrom redash import settings, utils\nfrom redash.utils import json_loads, query_is_select_no_limit, add_limit_to_query\nfrom rq.timeouts import JobTimeoutException\n\nfrom redash.utils.requests_session import requests_or_advocate, requests_session, UnacceptableAddressException\n\n\nlogger = logging.getLogger(__name__)\n\n__all__ = [\n    \"BaseQueryRunner\",\n    \"BaseHTTPQueryRunner\",\n    \"InterruptException\",\n    \"JobTimeoutException\",\n    \"BaseSQLQueryRunner\",\n    \"TYPE_DATETIME\",\n    \"TYPE_BOOLEAN\",\n    \"TYPE_INTEGER\",\n    \"TYPE_STRING\",\n    \"TYPE_DATE\",\n    \"TYPE_FLOAT\",\n    \"SUPPORTED_COLUMN_TYPES\",\n    \"register\",\n    \"get_query_runner\",\n    \"import_query_runners\",\n    \"guess_type\",\n]\n\n# Valid types of columns returned in results:\nTYPE_INTEGER = \"integer\"\nTYPE_FLOAT = \"float\"\nTYPE_BOOLEAN = \"boolean\"\nTYPE_STRING = \"string\"\nTYPE_DATETIME = \"datetime\"\nTYPE_DATE = \"date\"\n\nSUPPORTED_COLUMN_TYPES = set(\n    [TYPE_INTEGER, TYPE_FLOAT, TYPE_BOOLEAN, TYPE_STRING, TYPE_DATETIME, TYPE_DATE]\n)\n\n\nclass InterruptException(Exception):\n    pass\n\n\nclass NotSupported(Exception):\n    pass\n\n\nclass BaseQueryRunner(object):\n    deprecated = False\n    should_annotate_query = True\n    noop_query = None\n\n    def __init__(self, configuration):\n        self.syntax = \"sql\"\n        self.configuration = configuration\n\n    @classmethod\n    def name(cls):\n        return cls.__name__\n\n    @classmethod\n    def type(cls):\n        return cls.__name__.lower()\n\n    @classmethod\n    def enabled(cls):\n        return True\n\n    @property\n    def host(self):\n        \"\"\"Returns this query runner's configured host.\n        This is used primarily for temporarily swapping endpoints when using SSH tunnels to connect to a data source.\n\n        `BaseQueryRunner`'s na\u00efve implementation supports query runner implementations that store endpoints using `host` and `port`\n        configuration values. If your query runner uses a different schema (e.g. a web address), you should override this function.\n        \"\"\"\n        if \"host\" in self.configuration:\n            return self.configuration[\"host\"]\n        else:\n            raise NotImplementedError()\n\n    @host.setter\n    def host(self, host):\n        \"\"\"Sets this query runner's configured host.\n        This is used primarily for temporarily swapping endpoints when using SSH tunnels to connect to a data source.\n\n        `BaseQueryRunner`'s na\u00efve implementation supports query runner implementations that store endpoints using `host` and `port`\n        configuration values. If your query runner uses a different schema (e.g. a web address), you should override this function.\n        \"\"\"\n        if \"host\" in self.configuration:\n            self.configuration[\"host\"] = host\n        else:\n            raise NotImplementedError()\n\n    @property\n    def port(self):\n        \"\"\"Returns this query runner's configured port.\n        This is used primarily for temporarily swapping endpoints when using SSH tunnels to connect to a data source.\n\n        `BaseQueryRunner`'s na\u00efve implementation supports query runner implementations that store endpoints using `host` and `port`\n        configuration values. If your query runner uses a different schema (e.g. a web address), you should override this function.\n        \"\"\"\n        if \"port\" in self.configuration:\n            return self.configuration[\"port\"]\n        else:\n            raise NotImplementedError()\n\n    @port.setter\n    def port(self, port):\n        \"\"\"Sets this query runner's configured port.\n        This is used primarily for temporarily swapping endpoints when using SSH tunnels to connect to a data source.\n\n        `BaseQueryRunner`'s na\u00efve implementation supports query runner implementations that store endpoints using `host` and `port`\n        configuration values. If your query runner uses a different schema (e.g. a web address), you should override this function.\n        \"\"\"\n        if \"port\" in self.configuration:\n            self.configuration[\"port\"] = port\n        else:\n            raise NotImplementedError()\n\n    @classmethod\n    def configuration_schema(cls):\n        return {}\n\n    def annotate_query(self, query, metadata):\n        if not self.should_annotate_query:\n            return query\n\n        annotation = \", \".join([\"{}: {}\".format(k, v) for k, v in metadata.items()])\n        annotated_query = \"/* {} */ {}\".format(annotation, query)\n        return annotated_query\n\n    def test_connection(self):\n        if self.noop_query is None:\n            raise NotImplementedError()\n        data, error = self.run_query(self.noop_query, None)\n\n        if error is not None:\n            raise Exception(error)\n\n    def run_query(self, query, user):\n        raise NotImplementedError()\n\n    def fetch_columns(self, columns):\n        column_names = []\n        duplicates_counter = 1\n        new_columns = []\n\n        for col in columns:\n            column_name = col[0]\n            if column_name in column_names:\n                column_name = \"{}{}\".format(column_name, duplicates_counter)\n                duplicates_counter += 1\n\n            column_names.append(column_name)\n            new_columns.append(\n                {\"name\": column_name, \"friendly_name\": column_name, \"type\": col[1]}\n            )\n\n        return new_columns\n\n    def get_schema(self, get_stats=False):\n        raise NotSupported()\n\n    def _run_query_internal(self, query):\n        results, error = self.run_query(query, None)\n\n        if error is not None:\n            raise Exception(\"Failed running query [%s].\" % query)\n        return json_loads(results)[\"rows\"]\n\n    @classmethod\n    def to_dict(cls):\n        return {\n            \"name\": cls.name(),\n            \"type\": cls.type(),\n            \"configuration_schema\": cls.configuration_schema(),\n            **({\"deprecated\": True} if cls.deprecated else {}),\n        }\n\n    @property\n    def supports_auto_limit(self):\n        return False\n\n    def apply_auto_limit(self, query_text, should_apply_auto_limit):\n        return query_text\n\n    def gen_query_hash(self, query_text, set_auto_limit=False):\n        query_text = self.apply_auto_limit(query_text, set_auto_limit)\n        return utils.gen_query_hash(query_text)\n\n\nclass BaseSQLQueryRunner(BaseQueryRunner):\n    def get_schema(self, get_stats=False):\n        schema_dict = {}\n        self._get_tables(schema_dict)\n        if settings.SCHEMA_RUN_TABLE_SIZE_CALCULATIONS and get_stats:\n            self._get_tables_stats(schema_dict)\n        return list(schema_dict.values())\n\n    def _get_tables(self, schema_dict):\n        return []\n\n    def _get_tables_stats(self, tables_dict):\n        for t in tables_dict.keys():\n            if type(tables_dict[t]) == dict:\n                res = self._run_query_internal(\"select count(*) as cnt from %s\" % t)\n                tables_dict[t][\"size\"] = res[0][\"cnt\"]\n\n    @property\n    def supports_auto_limit(self):\n        return True\n\n    def apply_auto_limit(self, query_text, should_apply_auto_limit):\n        if should_apply_auto_limit:\n            from redash.query_runner.databricks import split_sql_statements, combine_sql_statements\n            queries = split_sql_statements(query_text)\n            # we only check for last one in the list because it is the one that we show result\n            last_query = queries[-1]\n            if query_is_select_no_limit(last_query):\n                queries[-1] = add_limit_to_query(last_query)\n            return combine_sql_statements(queries)\n        else:\n            return query_text\n\n\nclass BaseHTTPQueryRunner(BaseQueryRunner):\n    should_annotate_query = False\n    response_error = \"Endpoint returned unexpected status code\"\n    requires_authentication = False\n    requires_url = True\n    url_title = \"URL base path\"\n    username_title = \"HTTP Basic Auth Username\"\n    password_title = \"HTTP Basic Auth Password\"\n\n    @classmethod\n    def configuration_schema(cls):\n        schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"url\": {\"type\": \"string\", \"title\": cls.url_title},\n                \"username\": {\"type\": \"string\", \"title\": cls.username_title},\n                \"password\": {\"type\": \"string\", \"title\": cls.password_title},\n            },\n            \"secret\": [\"password\"],\n            \"order\": [\"url\", \"username\", \"password\"],\n        }\n\n        if cls.requires_url or cls.requires_authentication:\n            schema[\"required\"] = []\n\n        if cls.requires_url:\n            schema[\"required\"] += [\"url\"]\n\n        if cls.requires_authentication:\n            schema[\"required\"] += [\"username\", \"password\"]\n        return schema\n\n    def get_auth(self):\n        username = self.configuration.get(\"username\")\n        password = self.configuration.get(\"password\")\n        if username and password:\n            return (username, password)\n        if self.requires_authentication:\n            raise ValueError(\"Username and Password required\")\n        else:\n            return None\n\n    def get_response(self, url, auth=None, http_method=\"get\", **kwargs):\n\n        # Get authentication values if not given\n        if auth is None:\n            auth = self.get_auth()\n\n        # Then call requests to get the response from the given endpoint\n        # URL optionally, with the additional requests parameters.\n        error = None\n        response = None\n        try:\n            response = requests_session.request(http_method, url, auth=auth, **kwargs)\n            # Raise a requests HTTP exception with the appropriate reason\n            # for 4xx and 5xx response status codes which is later caught\n            # and passed back.\n            response.raise_for_status()\n\n            # Any other responses (e.g. 2xx and 3xx):\n            if response.status_code != 200:\n                error = \"{} ({}).\".format(self.response_error, response.status_code)\n\n        except requests_or_advocate.HTTPError as exc:\n            logger.exception(exc)\n            error = \"Failed to execute query. \" \"Return Code: {} Reason: {}\".format(\n                response.status_code, response.text\n            )\n        except UnacceptableAddressException as exc:\n            logger.exception(exc)\n            error = \"Can't query private addresses.\"\n        except requests_or_advocate.RequestException as exc:\n            # Catch all other requests exceptions and return the error.\n            logger.exception(exc)\n            error = str(exc)\n\n        # Return response and error.\n        return response, error\n\n\nquery_runners = {}\n\n\ndef register(query_runner_class):\n    global query_runners\n    if query_runner_class.enabled():\n        logger.debug(\n            \"Registering %s (%s) query runner.\",\n            query_runner_class.name(),\n            query_runner_class.type(),\n        )\n        query_runners[query_runner_class.type()] = query_runner_class\n    else:\n        logger.debug(\n            \"%s query runner enabled but not supported, not registering. Either disable or install missing \"\n            \"dependencies.\",\n            query_runner_class.name(),\n        )\n\n\ndef get_query_runner(query_runner_type, configuration):\n    query_runner_class = query_runners.get(query_runner_type, None)\n    if query_runner_class is None:\n        return None\n\n    return query_runner_class(configuration)\n\n\ndef get_configuration_schema_for_query_runner_type(query_runner_type):\n    query_runner_class = query_runners.get(query_runner_type, None)\n    if query_runner_class is None:\n        return None\n\n    return query_runner_class.configuration_schema()\n\n\ndef import_query_runners(query_runner_imports):\n    for runner_import in query_runner_imports:\n        __import__(runner_import)\n\n\ndef guess_type(value):\n    if isinstance(value, bool):\n        return TYPE_BOOLEAN\n    elif isinstance(value, int):\n        return TYPE_INTEGER\n    elif isinstance(value, float):\n        return TYPE_FLOAT\n\n    return guess_type_from_string(value)\n\n\ndef guess_type_from_string(string_value):\n    if string_value == \"\" or string_value is None:\n        return TYPE_STRING\n\n    try:\n        int(string_value)\n        return TYPE_INTEGER\n    except (ValueError, OverflowError):\n        pass\n\n    try:\n        float(string_value)\n        return TYPE_FLOAT\n    except (ValueError, OverflowError):\n        pass\n\n    if str(string_value).lower() in (\"true\", \"false\"):\n        return TYPE_BOOLEAN\n\n    try:\n        parser.parse(string_value)\n        return TYPE_DATETIME\n    except (ValueError, OverflowError):\n        pass\n\n    return TYPE_STRING\n\n\ndef with_ssh_tunnel(query_runner, details):\n    def tunnel(f):\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            try:\n                remote_host, remote_port = query_runner.host, query_runner.port\n            except NotImplementedError:\n                raise NotImplementedError(\n                    \"SSH tunneling is not implemented for this query runner yet.\"\n                )\n\n            stack = ExitStack()\n            try:\n                bastion_address = (details[\"ssh_host\"], details.get(\"ssh_port\", 22))\n                remote_address = (remote_host, remote_port)\n                auth = {\n                    \"ssh_username\": details[\"ssh_username\"],\n                    **settings.dynamic_settings.ssh_tunnel_auth(),\n                }\n                server = stack.enter_context(\n                    open_tunnel(\n                        bastion_address, remote_bind_address=remote_address, **auth\n                    )\n                )\n            except Exception as error:\n                raise type(error)(\"SSH tunnel: {}\".format(str(error)))\n\n            with stack:\n                try:\n                    query_runner.host, query_runner.port = server.local_bind_address\n                    result = f(*args, **kwargs)\n                finally:\n                    query_runner.host, query_runner.port = remote_host, remote_port\n\n                return result\n\n        return wrapper\n\n    query_runner.run_query = tunnel(query_runner.run_query)\n\n    return query_runner\n", "import logging\nimport yaml\nimport io\n\nfrom redash.utils.requests_session import requests_or_advocate, UnacceptableAddressException\n\nfrom redash.query_runner import *\nfrom redash.utils import json_dumps\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import pandas as pd\n    import numpy as np\n    enabled = True\nexcept ImportError:\n    enabled = False\n\n\nclass CSV(BaseQueryRunner):\n    should_annotate_query = False\n\n    @classmethod\n    def name(cls):\n        return \"CSV\"\n\n    @classmethod\n    def enabled(cls):\n        return enabled\n\n    @classmethod\n    def configuration_schema(cls):\n        return {\n            'type': 'object',\n            'properties': {},\n        }\n\n    def __init__(self, configuration):\n        super(CSV, self).__init__(configuration)\n        self.syntax = \"yaml\"\n\n    def test_connection(self):\n        pass\n\n    def run_query(self, query, user):\n        path = \"\"\n        ua = \"\"\n        args = {}\n        try:\n            args = yaml.safe_load(query)\n            path = args['url']\n            args.pop('url', None)\n            ua = args['user-agent']\n            args.pop('user-agent', None)\n        except:\n            pass\n\n        try:\n            response = requests_or_advocate.get(url=path, headers={\"User-agent\": ua})\n            workbook = pd.read_csv(io.BytesIO(response.content),sep=\",\", **args)\n\n            df = workbook.copy()\n            data = {'columns': [], 'rows': []}\n            conversions = [\n                {'pandas_type': np.integer, 'redash_type': 'integer',},\n                {'pandas_type': np.inexact, 'redash_type': 'float',},\n                {'pandas_type': np.datetime64, 'redash_type': 'datetime', 'to_redash': lambda x: x.strftime('%Y-%m-%d %H:%M:%S')},\n                {'pandas_type': np.bool_, 'redash_type': 'boolean'},\n                {'pandas_type': np.object, 'redash_type': 'string'}\n            ]\n            labels = []\n            for dtype, label in zip(df.dtypes, df.columns):\n                for conversion in conversions:\n                    if issubclass(dtype.type, conversion['pandas_type']):\n                        data['columns'].append({'name': label, 'friendly_name': label, 'type': conversion['redash_type']})\n                        labels.append(label)\n                        func = conversion.get('to_redash')\n                        if func:\n                            df[label] = df[label].apply(func)\n                        break\n            data['rows'] = df[labels].replace({np.nan: None}).to_dict(orient='records')\n\n            json_data = json_dumps(data)\n            error = None\n        except KeyboardInterrupt:\n            error = \"Query cancelled by user.\"\n            json_data = None\n        except UnacceptableAddressException:\n            error = \"Can't query private addresses.\"\n            json_data = None\n        except Exception as e:\n            error = \"Error reading {0}. {1}\".format(path, str(e))\n            json_data = None\n\n        return json_data, error\n\n    def get_schema(self):\n        raise NotSupported()\n\nregister(CSV)\n", "import logging\nimport yaml\n\nfrom redash.utils.requests_session import requests_or_advocate, UnacceptableAddressException\n\nfrom redash.query_runner import *\nfrom redash.utils import json_dumps\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import pandas as pd\n    import xlrd\n    import openpyxl\n    import numpy as np\n    enabled = True\nexcept ImportError:\n    enabled = False\n\nclass Excel(BaseQueryRunner):\n    should_annotate_query = False\n\n    @classmethod\n    def enabled(cls):\n        return enabled\n\n    @classmethod\n    def configuration_schema(cls):\n        return {\n            'type': 'object',\n            'properties': {},\n        }\n\n    def __init__(self, configuration):\n        super(Excel, self).__init__(configuration)\n        self.syntax = \"yaml\"\n\n    def test_connection(self):\n        pass\n\n    def run_query(self, query, user):\n        path = \"\"\n        ua = \"\"\n        args = {}\n        try:\n            args = yaml.safe_load(query)\n            path = args['url']\n            args.pop('url', None)\n            ua = args['user-agent']\n            args.pop('user-agent', None)\n\n        except:\n            pass\n\n        try:\n            response = requests_or_advocate.get(url=path, headers={\"User-agent\": ua})\n            workbook = pd.read_excel(response.content, **args)\n\n            df = workbook.copy()\n            data = {'columns': [], 'rows': []}\n            conversions = [\n                {'pandas_type': np.integer, 'redash_type': 'integer',},\n                {'pandas_type': np.inexact, 'redash_type': 'float',},\n                {'pandas_type': np.datetime64, 'redash_type': 'datetime', 'to_redash': lambda x: x.strftime('%Y-%m-%d %H:%M:%S')},\n                {'pandas_type': np.bool_, 'redash_type': 'boolean'},\n                {'pandas_type': np.object, 'redash_type': 'string'}\n            ]\n            labels = []\n            for dtype, label in zip(df.dtypes, df.columns):\n                for conversion in conversions:\n                    if issubclass(dtype.type, conversion['pandas_type']):\n                        data['columns'].append({'name': label, 'friendly_name': label, 'type': conversion['redash_type']})\n                        labels.append(label)\n                        func = conversion.get('to_redash')\n                        if func:\n                            df[label] = df[label].apply(func)\n                        break\n            data['rows'] = df[labels].replace({np.nan: None}).to_dict(orient='records')\n\n            json_data = json_dumps(data)\n            error = None\n        except KeyboardInterrupt:\n            error = \"Query cancelled by user.\"\n            json_data = None\n        except UnacceptableAddressException:\n            error = \"Can't query private addresses.\"\n            json_data = None\n        except Exception as e:\n            error = \"Error reading {0}. {1}\".format(path, str(e))\n            json_data = None\n\n        return json_data, error\n\n    def get_schema(self):\n        raise NotSupported()\n\nregister(Excel)\n", "import logging\nimport yaml\nimport datetime\nfrom funcy import compact, project\n\nfrom redash.utils.requests_session import requests_or_advocate, UnacceptableAddressException\n\nfrom redash.utils import json_dumps\nfrom redash.query_runner import (\n    BaseHTTPQueryRunner,\n    register,\n    TYPE_BOOLEAN,\n    TYPE_DATETIME,\n    TYPE_FLOAT,\n    TYPE_INTEGER,\n    TYPE_STRING,\n)\n\n\nclass QueryParseError(Exception):\n    pass\n\n\ndef parse_query(query):\n    # TODO: copy paste from Metrica query runner, we should extract this into a utility\n    query = query.strip()\n    if query == \"\":\n        raise QueryParseError(\"Query is empty.\")\n    try:\n        params = yaml.safe_load(query)\n        return params\n    except ValueError as e:\n        logging.exception(e)\n        error = str(e)\n        raise QueryParseError(error)\n\n\nTYPES_MAP = {\n    str: TYPE_STRING,\n    bytes: TYPE_STRING,\n    int: TYPE_INTEGER,\n    float: TYPE_FLOAT,\n    bool: TYPE_BOOLEAN,\n    datetime.datetime: TYPE_DATETIME,\n}\n\n\ndef _get_column_by_name(columns, column_name):\n    for c in columns:\n        if \"name\" in c and c[\"name\"] == column_name:\n            return c\n\n    return None\n\n\ndef _get_type(value):\n    return TYPES_MAP.get(type(value), TYPE_STRING)\n\n\ndef add_column(columns, column_name, column_type):\n    if _get_column_by_name(columns, column_name) is None:\n        columns.append(\n            {\"name\": column_name, \"friendly_name\": column_name, \"type\": column_type}\n        )\n\n\ndef _apply_path_search(response, path):\n    if path is None:\n        return response\n\n    path_parts = path.split(\".\")\n    path_parts.reverse()\n    while len(path_parts) > 0:\n        current_path = path_parts.pop()\n        if current_path in response:\n            response = response[current_path]\n        else:\n            raise Exception(\"Couldn't find path {} in response.\".format(path))\n\n    return response\n\n\ndef _normalize_json(data, path):\n    data = _apply_path_search(data, path)\n\n    if isinstance(data, dict):\n        data = [data]\n\n    return data\n\n\ndef _sort_columns_with_fields(columns, fields):\n    if fields:\n        columns = compact([_get_column_by_name(columns, field) for field in fields])\n\n    return columns\n\n\n# TODO: merge the logic here with the one in MongoDB's queyr runner\ndef parse_json(data, path, fields):\n    data = _normalize_json(data, path)\n\n    rows = []\n    columns = []\n\n    for row in data:\n        parsed_row = {}\n\n        for key in row:\n            if isinstance(row[key], dict):\n                for inner_key in row[key]:\n                    column_name = \"{}.{}\".format(key, inner_key)\n                    if fields and key not in fields and column_name not in fields:\n                        continue\n\n                    value = row[key][inner_key]\n                    add_column(columns, column_name, _get_type(value))\n                    parsed_row[column_name] = value\n            else:\n                if fields and key not in fields:\n                    continue\n\n                value = row[key]\n                add_column(columns, key, _get_type(value))\n                parsed_row[key] = row[key]\n\n        rows.append(parsed_row)\n\n    columns = _sort_columns_with_fields(columns, fields)\n\n    return {\"rows\": rows, \"columns\": columns}\n\n\nclass JSON(BaseHTTPQueryRunner):\n    requires_url = False\n\n    @classmethod\n    def configuration_schema(cls):\n        return {\n            \"type\": \"object\",\n            \"properties\": {\n                \"username\": {\"type\": \"string\", \"title\": cls.username_title},\n                \"password\": {\"type\": \"string\", \"title\": cls.password_title},\n            },\n            \"secret\": [\"password\"],\n            \"order\": [\"username\", \"password\"],\n        }\n\n    def __init__(self, configuration):\n        super(JSON, self).__init__(configuration)\n        self.syntax = \"yaml\"\n\n    def test_connection(self):\n        pass\n\n    def run_query(self, query, user):\n        query = parse_query(query)\n\n        if not isinstance(query, dict):\n            raise QueryParseError(\n                \"Query should be a YAML object describing the URL to query.\"\n            )\n\n        if \"url\" not in query:\n            raise QueryParseError(\"Query must include 'url' option.\")\n\n\n        method = query.get(\"method\", \"get\")\n        request_options = project(query, (\"params\", \"headers\", \"data\", \"auth\", \"json\"))\n\n        fields = query.get(\"fields\")\n        path = query.get(\"path\")\n\n        if isinstance(request_options.get(\"auth\", None), list):\n            request_options[\"auth\"] = tuple(request_options[\"auth\"])\n        elif self.configuration.get(\"username\") or self.configuration.get(\"password\"):\n            request_options[\"auth\"] = (\n                self.configuration.get(\"username\"),\n                self.configuration.get(\"password\"),\n            )\n\n        if method not in (\"get\", \"post\"):\n            raise QueryParseError(\"Only GET or POST methods are allowed.\")\n\n        if fields and not isinstance(fields, list):\n            raise QueryParseError(\"'fields' needs to be a list.\")\n\n        response, error = self.get_response(\n            query[\"url\"], http_method=method, **request_options\n        )\n\n        if error is not None:\n            return None, error\n\n        data = json_dumps(parse_json(response.json(), path, fields))\n\n        if data:\n            return data, None\n        else:\n            return None, \"Got empty response from '{}'.\".format(query[\"url\"])\n\n\nregister(JSON)\n", "from redash import settings\n\nfrom advocate.exceptions import UnacceptableAddressException\nif settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\n    import advocate as requests_or_advocate\nelse:\n    import requests as requests_or_advocate\n\n\n\nclass ConfiguredSession(requests_or_advocate.Session):\n    def request(self, *args, **kwargs):\n        if not settings.REQUESTS_ALLOW_REDIRECTS:\n            kwargs.update({\"allow_redirects\": False})\n        return super().request(*args, **kwargs)\n\n\nrequests_session = ConfiguredSession()\n", "google-api-python-client==1.7.11\nprotobuf==3.17.3\ngspread==3.1.0\nimpyla==0.16.0\ninfluxdb==5.2.3\nmysqlclient==1.3.14\noauth2client==4.1.3\npyhive==0.6.1\npymongo[tls,srv]==3.9.0\nvertica-python==0.9.5\ntd-client==1.0.0\npymssql==2.1.4\ndql==0.5.26\ndynamo3==0.4.10\nboto3>=1.10.0,<1.11.0\nbotocore>=1.13,<1.14.0\nsasl>=0.1.3\nthrift>=0.8.0\nthrift_sasl>=0.1.0\ncassandra-driver==3.21.0\nmemsql==3.0.0\natsd_client==3.0.5\nsimple_salesforce==0.74.3\nPyAthena>=1.5.0\npymapd==0.19.0\nqds-sdk>=1.9.6\nibm-db>=2.0.9\npydruid==0.5.7\nrequests_aws_sign==0.1.5\nsnowflake-connector-python==2.1.3\nphoenixdb==0.7\n# certifi is needed to support MongoDB and SSL:\ncertifi>=2019.9.11\npydgraph==2.0.2\nazure-kusto-data==0.0.35\npyexasol==0.12.0\npython-rapidjson==0.8.0\npyodbc==4.0.28\ntrino~=0.305\ncmem-cmempy==21.2.3\nxlrd==2.0.1\nopenpyxl==3.0.7\nfirebolt-sqlalchemy\nadvocate==1.0.0", "import mock\nfrom unittest import TestCase\n\nfrom redash.utils.requests_session import requests_or_advocate, ConfiguredSession\nfrom redash.query_runner import BaseHTTPQueryRunner\n\n\nclass RequiresAuthQueryRunner(BaseHTTPQueryRunner):\n    requires_authentication = True\n\n\nclass TestBaseHTTPQueryRunner(TestCase):\n    def test_requires_authentication_default(self):\n        self.assertFalse(BaseHTTPQueryRunner.requires_authentication)\n        schema = BaseHTTPQueryRunner.configuration_schema()\n        self.assertNotIn(\"username\", schema[\"required\"])\n        self.assertNotIn(\"password\", schema[\"required\"])\n\n    def test_requires_authentication_true(self):\n        schema = RequiresAuthQueryRunner.configuration_schema()\n        self.assertIn(\"username\", schema[\"required\"])\n        self.assertIn(\"password\", schema[\"required\"])\n\n    def test_get_auth_with_values(self):\n        query_runner = BaseHTTPQueryRunner(\n            {\"username\": \"username\", \"password\": \"password\"}\n        )\n        self.assertEqual(query_runner.get_auth(), (\"username\", \"password\"))\n\n    def test_get_auth_empty(self):\n        query_runner = BaseHTTPQueryRunner({})\n        self.assertIsNone(query_runner.get_auth())\n\n    def test_get_auth_empty_requires_authentication(self):\n        query_runner = RequiresAuthQueryRunner({})\n        self.assertRaisesRegex(\n            ValueError, \"Username and Password required\", query_runner.get_auth\n        )\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_success(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 200\n        mock_response.text = \"Success\"\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        response, error = query_runner.get_response(url)\n        mock_get.assert_called_once_with(\"get\", url, auth=None)\n        self.assertEqual(response.status_code, 200)\n        self.assertIsNone(error)\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_success_custom_auth(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 200\n        mock_response.text = \"Success\"\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        auth = (\"username\", \"password\")\n        response, error = query_runner.get_response(url, auth=auth)\n        mock_get.assert_called_once_with(\"get\", url, auth=auth)\n        self.assertEqual(response.status_code, 200)\n        self.assertIsNone(error)\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_failure(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 301\n        mock_response.text = \"Redirect\"\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        response, error = query_runner.get_response(url)\n        mock_get.assert_called_once_with(\"get\", url, auth=None)\n        self.assertIn(query_runner.response_error, error)\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_httperror_exception(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 500\n        mock_response.text = \"Server Error\"\n        http_error = requests_or_advocate.HTTPError()\n        mock_response.raise_for_status.side_effect = http_error\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        response, error = query_runner.get_response(url)\n        mock_get.assert_called_once_with(\"get\", url, auth=None)\n        self.assertIsNotNone(error)\n        self.assertIn(\"Failed to execute query\", error)\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_requests_exception(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 500\n        mock_response.text = \"Server Error\"\n        exception_message = \"Some requests exception\"\n        requests_exception = requests_or_advocate.RequestException(exception_message)\n        mock_response.raise_for_status.side_effect = requests_exception\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        response, error = query_runner.get_response(url)\n        mock_get.assert_called_once_with(\"get\", url, auth=None)\n        self.assertIsNotNone(error)\n        self.assertEqual(exception_message, error)\n\n    @mock.patch.object(ConfiguredSession, \"request\")\n    def test_get_response_generic_exception(self, mock_get):\n        mock_response = mock.Mock()\n        mock_response.status_code = 500\n        mock_response.text = \"Server Error\"\n        exception_message = \"Some generic exception\"\n        exception = ValueError(exception_message)\n        mock_response.raise_for_status.side_effect = exception\n        mock_get.return_value = mock_response\n\n        url = \"https://example.com/\"\n        query_runner = BaseHTTPQueryRunner({})\n        self.assertRaisesRegex(\n            ValueError, exception_message, query_runner.get_response, url\n        )\n"], "filenames": ["redash/query_runner/__init__.py", "redash/query_runner/csv.py", "redash/query_runner/excel.py", "redash/query_runner/json_ds.py", "redash/utils/requests_session.py", "requirements_all_ds.txt", "tests/query_runner/test_http.py"], "buggy_code_start_loc": [16, 3, 3, 5, 1, 43, 4], "buggy_code_end_loc": [316, 90, 86, 168, 6, 43, 105], "fixing_code_start_loc": [16, 2, 2, 5, 0, 44, 4], "fixing_code_end_loc": [312, 91, 88, 166, 12, 45, 105], "type": "CWE-918", "message": "Redash is a package for data visualization and sharing. In versions 10.0 and priorm the implementation of URL-loading data sources like JSON, CSV, or Excel is vulnerable to advanced methods of Server Side Request Forgery (SSRF). These vulnerabilities are only exploitable on installations where a URL-loading data source is enabled. As of time of publication, the `master` and `release/10.x.x` branches address this by applying the Advocate library for making http requests instead of the requests library directly. Users should upgrade to version 10.0.1 to receive this patch. There are a few workarounds for mitigating the vulnerability without upgrading. One can disable the vulnerable data sources entirely, by adding the following env variable to one's configuration, making them unavailable inside the webapp. One can switch any data source of certain types (viewable in the GitHub Security Advisory) to be `View Only` for all groups on the Settings > Groups > Data Sources screen. For users unable to update an admin may modify Redash's configuration through environment variables to mitigate this issue. Depending on the version of Redash, an admin may also need to run a CLI command to re-encrypt some fields in the database. The `master` and `release/10.x.x` branches as of time of publication have removed the default value for `REDASH_COOKIE_SECRET`. All future releases will also require this to be set explicitly. For existing installations, one will need to ensure that explicit values are set for the `REDASH_COOKIE_SECRET` and `REDASH_SECRET_KEY `variables.", "other": {"cve": {"id": "CVE-2021-43780", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-24T16:15:14.337", "lastModified": "2021-11-30T15:07:27.213", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Redash is a package for data visualization and sharing. In versions 10.0 and priorm the implementation of URL-loading data sources like JSON, CSV, or Excel is vulnerable to advanced methods of Server Side Request Forgery (SSRF). These vulnerabilities are only exploitable on installations where a URL-loading data source is enabled. As of time of publication, the `master` and `release/10.x.x` branches address this by applying the Advocate library for making http requests instead of the requests library directly. Users should upgrade to version 10.0.1 to receive this patch. There are a few workarounds for mitigating the vulnerability without upgrading. One can disable the vulnerable data sources entirely, by adding the following env variable to one's configuration, making them unavailable inside the webapp. One can switch any data source of certain types (viewable in the GitHub Security Advisory) to be `View Only` for all groups on the Settings > Groups > Data Sources screen. For users unable to update an admin may modify Redash's configuration through environment variables to mitigate this issue. Depending on the version of Redash, an admin may also need to run a CLI command to re-encrypt some fields in the database. The `master` and `release/10.x.x` branches as of time of publication have removed the default value for `REDASH_COOKIE_SECRET`. All future releases will also require this to be set explicitly. For existing installations, one will need to ensure that explicit values are set for the `REDASH_COOKIE_SECRET` and `REDASH_SECRET_KEY `variables."}, {"lang": "es", "value": "Redash es un paquete para visualizar y compartir datos. En las versiones 10.0 y anteriores, la implementaci\u00f3n de fuentes de datos de carga de URL como JSON, CSV o Excel es vulnerable a m\u00e9todos avanzados de falsificaci\u00f3n de peticiones del lado del servidor (SSRF). Estas vulnerabilidades s\u00f3lo pueden ser explotadas en instalaciones en las que est\u00e9 habilitada una fuente de datos de carga por URL. En el momento de la publicaci\u00f3n, las ramas \"master\" y \"release/10.x.x\" abordan esto aplicando la biblioteca Advocate para realizar peticiones http en lugar de la biblioteca requests directamente. Los usuarios deben actualizar a la versi\u00f3n 10.0.1 para recibir este parche. Se presentan algunas soluciones para mitigar la vulnerabilidad sin actualizar. Uno puede deshabilitar las fuentes de datos vulnerables por completo, a\u00f1adiendo la siguiente variable env a la configuraci\u00f3n, haciendo que no est\u00e9n disponibles dentro de la aplicaci\u00f3n web. Puede cambiarse cualquier fuente de datos de determinados tipos (visibles en el Aviso de Seguridad de GitHub) para que sea \"View Only\" para todos los grupos en la pantalla de Settings ) Groups ) Data Sources. Para usuarios que no pueden actualizar un administrador puede modificar la configuraci\u00f3n de Redash mediante variables de entorno para mitigar este problema. Dependiendo de la versi\u00f3n de Redash, un administrador tambi\u00e9n puede necesitar ejecutar un comando CLI para volver a cifrar algunos campos en la base de datos. Las ramas \"master\" y \"release/10.x.x\" en el momento de la publicaci\u00f3n han eliminado el valor por defecto de \"REDASH_COOKIE_SECRET\". Todas las versiones futuras tambi\u00e9n requerir\u00e1n que sea establecida expl\u00edcitamente. Para las instalaciones existentes, habr\u00e1 que asegurarse de que sean establecidos valores expl\u00edcitos para las variables \"REDASH_COOKIE_SECRET\" y \"REDASH_SECRET_KEY\""}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 6.8, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.6, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:S/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "SINGLE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 6.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 6.8, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-918"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-918"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redash:redash:*:*:*:*:*:*:*:*", "versionEndExcluding": "10.0.1", "matchCriteriaId": "AF2C7DBC-5166-4A5C-8691-ED5E0EE3B027"}]}]}], "references": [{"url": "https://github.com/getredash/redash/commit/61bbb5aa7a23a93f2f93710005f71bc972826099", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/getredash/redash/security/advisories/GHSA-fcpv-hgq6-87h7", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/getredash/redash/commit/61bbb5aa7a23a93f2f93710005f71bc972826099"}}