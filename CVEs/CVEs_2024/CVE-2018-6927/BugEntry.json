{"buggy_code": ["/*\n *  Fast Userspace Mutexes (which I call \"Futexes!\").\n *  (C) Rusty Russell, IBM 2002\n *\n *  Generalized futexes, futex requeueing, misc fixes by Ingo Molnar\n *  (C) Copyright 2003 Red Hat Inc, All Rights Reserved\n *\n *  Removed page pinning, fix privately mapped COW pages and other cleanups\n *  (C) Copyright 2003, 2004 Jamie Lokier\n *\n *  Robust futex support started by Ingo Molnar\n *  (C) Copyright 2006 Red Hat Inc, All Rights Reserved\n *  Thanks to Thomas Gleixner for suggestions, analysis and fixes.\n *\n *  PI-futex support started by Ingo Molnar and Thomas Gleixner\n *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>\n *  Copyright (C) 2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>\n *\n *  PRIVATE futexes by Eric Dumazet\n *  Copyright (C) 2007 Eric Dumazet <dada1@cosmosbay.com>\n *\n *  Requeue-PI support by Darren Hart <dvhltc@us.ibm.com>\n *  Copyright (C) IBM Corporation, 2009\n *  Thanks to Thomas Gleixner for conceptual design and careful reviews.\n *\n *  Thanks to Ben LaHaise for yelling \"hashed waitqueues\" loudly\n *  enough at me, Linus for the original (flawed) idea, Matthew\n *  Kirkwood for proof-of-concept implementation.\n *\n *  \"The futexes are also cursed.\"\n *  \"But they come in a choice of three flavours!\"\n *\n *  This program is free software; you can redistribute it and/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; either version 2 of the License, or\n *  (at your option) any later version.\n *\n *  This program is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *  GNU General Public License for more details.\n *\n *  You should have received a copy of the GNU General Public License\n *  along with this program; if not, write to the Free Software\n *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n */\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/jhash.h>\n#include <linux/init.h>\n#include <linux/futex.h>\n#include <linux/mount.h>\n#include <linux/pagemap.h>\n#include <linux/syscalls.h>\n#include <linux/signal.h>\n#include <linux/export.h>\n#include <linux/magic.h>\n#include <linux/pid.h>\n#include <linux/nsproxy.h>\n#include <linux/ptrace.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/mm.h>\n#include <linux/hugetlb.h>\n#include <linux/freezer.h>\n#include <linux/bootmem.h>\n#include <linux/fault-inject.h>\n\n#include <asm/futex.h>\n\n#include \"locking/rtmutex_common.h\"\n\n/*\n * READ this before attempting to hack on futexes!\n *\n * Basic futex operation and ordering guarantees\n * =============================================\n *\n * The waiter reads the futex value in user space and calls\n * futex_wait(). This function computes the hash bucket and acquires\n * the hash bucket lock. After that it reads the futex user space value\n * again and verifies that the data has not changed. If it has not changed\n * it enqueues itself into the hash bucket, releases the hash bucket lock\n * and schedules.\n *\n * The waker side modifies the user space value of the futex and calls\n * futex_wake(). This function computes the hash bucket and acquires the\n * hash bucket lock. Then it looks for waiters on that futex in the hash\n * bucket and wakes them.\n *\n * In futex wake up scenarios where no tasks are blocked on a futex, taking\n * the hb spinlock can be avoided and simply return. In order for this\n * optimization to work, ordering guarantees must exist so that the waiter\n * being added to the list is acknowledged when the list is concurrently being\n * checked by the waker, avoiding scenarios like the following:\n *\n * CPU 0                               CPU 1\n * val = *futex;\n * sys_futex(WAIT, futex, val);\n *   futex_wait(futex, val);\n *   uval = *futex;\n *                                     *futex = newval;\n *                                     sys_futex(WAKE, futex);\n *                                       futex_wake(futex);\n *                                       if (queue_empty())\n *                                         return;\n *   if (uval == val)\n *      lock(hash_bucket(futex));\n *      queue();\n *     unlock(hash_bucket(futex));\n *     schedule();\n *\n * This would cause the waiter on CPU 0 to wait forever because it\n * missed the transition of the user space value from val to newval\n * and the waker did not find the waiter in the hash bucket queue.\n *\n * The correct serialization ensures that a waiter either observes\n * the changed user space value before blocking or is woken by a\n * concurrent waker:\n *\n * CPU 0                                 CPU 1\n * val = *futex;\n * sys_futex(WAIT, futex, val);\n *   futex_wait(futex, val);\n *\n *   waiters++; (a)\n *   smp_mb(); (A) <-- paired with -.\n *                                  |\n *   lock(hash_bucket(futex));      |\n *                                  |\n *   uval = *futex;                 |\n *                                  |        *futex = newval;\n *                                  |        sys_futex(WAKE, futex);\n *                                  |          futex_wake(futex);\n *                                  |\n *                                  `--------> smp_mb(); (B)\n *   if (uval == val)\n *     queue();\n *     unlock(hash_bucket(futex));\n *     schedule();                         if (waiters)\n *                                           lock(hash_bucket(futex));\n *   else                                    wake_waiters(futex);\n *     waiters--; (b)                        unlock(hash_bucket(futex));\n *\n * Where (A) orders the waiters increment and the futex value read through\n * atomic operations (see hb_waiters_inc) and where (B) orders the write\n * to futex and the waiters read -- this is done by the barriers for both\n * shared and private futexes in get_futex_key_refs().\n *\n * This yields the following case (where X:=waiters, Y:=futex):\n *\n *\tX = Y = 0\n *\n *\tw[X]=1\t\tw[Y]=1\n *\tMB\t\tMB\n *\tr[Y]=y\t\tr[X]=x\n *\n * Which guarantees that x==0 && y==0 is impossible; which translates back into\n * the guarantee that we cannot both miss the futex variable change and the\n * enqueue.\n *\n * Note that a new waiter is accounted for in (a) even when it is possible that\n * the wait call can return error, in which case we backtrack from it in (b).\n * Refer to the comment in queue_lock().\n *\n * Similarly, in order to account for waiters being requeued on another\n * address we always increment the waiters for the destination bucket before\n * acquiring the lock. It then decrements them again  after releasing it -\n * the code that actually moves the futex(es) between hash buckets (requeue_futex)\n * will do the additional required waiter count housekeeping. This is done for\n * double_lock_hb() and double_unlock_hb(), respectively.\n */\n\n#ifndef CONFIG_HAVE_FUTEX_CMPXCHG\nint __read_mostly futex_cmpxchg_enabled;\n#endif\n\n/*\n * Futex flags used to encode options to functions and preserve them across\n * restarts.\n */\n#ifdef CONFIG_MMU\n# define FLAGS_SHARED\t\t0x01\n#else\n/*\n * NOMMU does not have per process address space. Let the compiler optimize\n * code away.\n */\n# define FLAGS_SHARED\t\t0x00\n#endif\n#define FLAGS_CLOCKRT\t\t0x02\n#define FLAGS_HAS_TIMEOUT\t0x04\n\n/*\n * Priority Inheritance state:\n */\nstruct futex_pi_state {\n\t/*\n\t * list of 'owned' pi_state instances - these have to be\n\t * cleaned up in do_exit() if the task exits prematurely:\n\t */\n\tstruct list_head list;\n\n\t/*\n\t * The PI object:\n\t */\n\tstruct rt_mutex pi_mutex;\n\n\tstruct task_struct *owner;\n\tatomic_t refcount;\n\n\tunion futex_key key;\n} __randomize_layout;\n\n/**\n * struct futex_q - The hashed futex queue entry, one per waiting task\n * @list:\t\tpriority-sorted list of tasks waiting on this futex\n * @task:\t\tthe task waiting on the futex\n * @lock_ptr:\t\tthe hash bucket lock\n * @key:\t\tthe key the futex is hashed on\n * @pi_state:\t\toptional priority inheritance state\n * @rt_waiter:\t\trt_waiter storage for use with requeue_pi\n * @requeue_pi_key:\tthe requeue_pi target futex key\n * @bitset:\t\tbitset for the optional bitmasked wakeup\n *\n * We use this hashed waitqueue, instead of a normal wait_queue_entry_t, so\n * we can wake only the relevant ones (hashed queues may be shared).\n *\n * A futex_q has a woken state, just like tasks have TASK_RUNNING.\n * It is considered woken when plist_node_empty(&q->list) || q->lock_ptr == 0.\n * The order of wakeup is always to make the first condition true, then\n * the second.\n *\n * PI futexes are typically woken before they are removed from the hash list via\n * the rt_mutex code. See unqueue_me_pi().\n */\nstruct futex_q {\n\tstruct plist_node list;\n\n\tstruct task_struct *task;\n\tspinlock_t *lock_ptr;\n\tunion futex_key key;\n\tstruct futex_pi_state *pi_state;\n\tstruct rt_mutex_waiter *rt_waiter;\n\tunion futex_key *requeue_pi_key;\n\tu32 bitset;\n} __randomize_layout;\n\nstatic const struct futex_q futex_q_init = {\n\t/* list gets initialized in queue_me()*/\n\t.key = FUTEX_KEY_INIT,\n\t.bitset = FUTEX_BITSET_MATCH_ANY\n};\n\n/*\n * Hash buckets are shared by all the futex_keys that hash to the same\n * location.  Each key may have multiple futex_q structures, one for each task\n * waiting on a futex.\n */\nstruct futex_hash_bucket {\n\tatomic_t waiters;\n\tspinlock_t lock;\n\tstruct plist_head chain;\n} ____cacheline_aligned_in_smp;\n\n/*\n * The base of the bucket array and its size are always used together\n * (after initialization only in hash_futex()), so ensure that they\n * reside in the same cacheline.\n */\nstatic struct {\n\tstruct futex_hash_bucket *queues;\n\tunsigned long            hashsize;\n} __futex_data __read_mostly __aligned(2*sizeof(long));\n#define futex_queues   (__futex_data.queues)\n#define futex_hashsize (__futex_data.hashsize)\n\n\n/*\n * Fault injections for futexes.\n */\n#ifdef CONFIG_FAIL_FUTEX\n\nstatic struct {\n\tstruct fault_attr attr;\n\n\tbool ignore_private;\n} fail_futex = {\n\t.attr = FAULT_ATTR_INITIALIZER,\n\t.ignore_private = false,\n};\n\nstatic int __init setup_fail_futex(char *str)\n{\n\treturn setup_fault_attr(&fail_futex.attr, str);\n}\n__setup(\"fail_futex=\", setup_fail_futex);\n\nstatic bool should_fail_futex(bool fshared)\n{\n\tif (fail_futex.ignore_private && !fshared)\n\t\treturn false;\n\n\treturn should_fail(&fail_futex.attr, 1);\n}\n\n#ifdef CONFIG_FAULT_INJECTION_DEBUG_FS\n\nstatic int __init fail_futex_debugfs(void)\n{\n\tumode_t mode = S_IFREG | S_IRUSR | S_IWUSR;\n\tstruct dentry *dir;\n\n\tdir = fault_create_debugfs_attr(\"fail_futex\", NULL,\n\t\t\t\t\t&fail_futex.attr);\n\tif (IS_ERR(dir))\n\t\treturn PTR_ERR(dir);\n\n\tif (!debugfs_create_bool(\"ignore-private\", mode, dir,\n\t\t\t\t &fail_futex.ignore_private)) {\n\t\tdebugfs_remove_recursive(dir);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nlate_initcall(fail_futex_debugfs);\n\n#endif /* CONFIG_FAULT_INJECTION_DEBUG_FS */\n\n#else\nstatic inline bool should_fail_futex(bool fshared)\n{\n\treturn false;\n}\n#endif /* CONFIG_FAIL_FUTEX */\n\nstatic inline void futex_get_mm(union futex_key *key)\n{\n\tmmgrab(key->private.mm);\n\t/*\n\t * Ensure futex_get_mm() implies a full barrier such that\n\t * get_futex_key() implies a full barrier. This is relied upon\n\t * as smp_mb(); (B), see the ordering comment above.\n\t */\n\tsmp_mb__after_atomic();\n}\n\n/*\n * Reflects a new waiter being added to the waitqueue.\n */\nstatic inline void hb_waiters_inc(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\tatomic_inc(&hb->waiters);\n\t/*\n\t * Full barrier (A), see the ordering comment above.\n\t */\n\tsmp_mb__after_atomic();\n#endif\n}\n\n/*\n * Reflects a waiter being removed from the waitqueue by wakeup\n * paths.\n */\nstatic inline void hb_waiters_dec(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\tatomic_dec(&hb->waiters);\n#endif\n}\n\nstatic inline int hb_waiters_pending(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\treturn atomic_read(&hb->waiters);\n#else\n\treturn 1;\n#endif\n}\n\n/**\n * hash_futex - Return the hash bucket in the global hash\n * @key:\tPointer to the futex key for which the hash is calculated\n *\n * We hash on the keys returned from get_futex_key (see below) and return the\n * corresponding hash bucket in the global hash.\n */\nstatic struct futex_hash_bucket *hash_futex(union futex_key *key)\n{\n\tu32 hash = jhash2((u32*)&key->both.word,\n\t\t\t  (sizeof(key->both.word)+sizeof(key->both.ptr))/4,\n\t\t\t  key->both.offset);\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}\n\n\n/**\n * match_futex - Check whether two futex keys are equal\n * @key1:\tPointer to key1\n * @key2:\tPointer to key2\n *\n * Return 1 if two futex_keys are equal, 0 otherwise.\n */\nstatic inline int match_futex(union futex_key *key1, union futex_key *key2)\n{\n\treturn (key1 && key2\n\t\t&& key1->both.word == key2->both.word\n\t\t&& key1->both.ptr == key2->both.ptr\n\t\t&& key1->both.offset == key2->both.offset);\n}\n\n/*\n * Take a reference to the resource addressed by a key.\n * Can be called while holding spinlocks.\n *\n */\nstatic void get_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr)\n\t\treturn;\n\n\t/*\n\t * On MMU less systems futexes are always \"private\" as there is no per\n\t * process address space. We need the smp wmb nevertheless - yes,\n\t * arch/blackfin has MMU less SMP ...\n\t */\n\tif (!IS_ENABLED(CONFIG_MMU)) {\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t\treturn;\n\t}\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tihold(key->shared.inode); /* implies smp_mb(); (B) */\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tfutex_get_mm(key); /* implies smp_mb(); (B) */\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * Private futexes do not hold reference on an inode or\n\t\t * mm, therefore the only purpose of calling get_futex_key_refs\n\t\t * is because we need the barrier for the lockless waiter check.\n\t\t */\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t}\n}\n\n/*\n * Drop a reference to the resource addressed by a key.\n * The hash bucket spinlock must not be held. This is\n * a no-op for private futexes, see comment in the get\n * counterpart.\n */\nstatic void drop_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr) {\n\t\t/* If we're here then we tried to put a key we failed to get */\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\n\tif (!IS_ENABLED(CONFIG_MMU))\n\t\treturn;\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tiput(key->shared.inode);\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tmmdrop(key->private.mm);\n\t\tbreak;\n\t}\n}\n\n/**\n * get_futex_key() - Get parameters which are the keys for a futex\n * @uaddr:\tvirtual address of the futex\n * @fshared:\t0 for a PROCESS_PRIVATE futex, 1 for PROCESS_SHARED\n * @key:\taddress where result is stored.\n * @rw:\t\tmapping needs to be read/write (values: VERIFY_READ,\n *              VERIFY_WRITE)\n *\n * Return: a negative error code or 0\n *\n * The key words are stored in @key on success.\n *\n * For shared mappings, it's (page->index, file_inode(vma->vm_file),\n * offset_within_page).  For private mappings, it's (uaddr, current->mm).\n * We can usually work out the index without swapping in the page.\n *\n * lock_page() might sleep, the caller should not hold a spinlock.\n */\nstatic int\nget_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(rw, uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\tget_futex_key_refs(key);  /* implies smp_mb(); (B) */\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, 1, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == VERIFY_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(fshared)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t\tget_futex_key_refs(key); /* implies smp_mb(); (B) */\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\t/*\n\t\t * Take a reference unless it is about to be freed. Previously\n\t\t * this reference was taken by ihold under the page lock\n\t\t * pinning the inode in place so i_lock was unnecessary. The\n\t\t * only way for this check to fail is if the inode was\n\t\t * truncated in parallel which is almost certainly an\n\t\t * application bug. In such a case, just retry.\n\t\t *\n\t\t * We are not calling into get_futex_key_refs() in file-backed\n\t\t * cases, therefore a successful atomic_inc return below will\n\t\t * guarantee that get_futex_key() will still imply smp_mb(); (B).\n\t\t */\n\t\tif (!atomic_inc_not_zero(&inode->i_count)) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\t/* Should be impossible but lets be paranoid for now */\n\t\tif (WARN_ON_ONCE(inode->i_mapping != mapping)) {\n\t\t\terr = -EFAULT;\n\t\t\trcu_read_unlock();\n\t\t\tiput(inode);\n\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.inode = inode;\n\t\tkey->shared.pgoff = basepage_index(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}\n\nstatic inline void put_futex_key(union futex_key *key)\n{\n\tdrop_futex_key_refs(key);\n}\n\n/**\n * fault_in_user_writeable() - Fault in user address and verify RW access\n * @uaddr:\tpointer to faulting user space address\n *\n * Slow path to fixup the fault we just took in the atomic write\n * access to @uaddr.\n *\n * We have no generic implementation of a non-destructive write to the\n * user address. We know that we faulted in the atomic pagefault\n * disabled section so we can as well avoid the #PF overhead by\n * calling get_user_pages() right away.\n */\nstatic int fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tdown_read(&mm->mmap_sem);\n\tret = fixup_user_fault(current, mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tup_read(&mm->mmap_sem);\n\n\treturn ret < 0 ? ret : 0;\n}\n\n/**\n * futex_top_waiter() - Return the highest priority waiter on a futex\n * @hb:\t\tthe hash bucket the futex_q's reside in\n * @key:\tthe futex key (to distinguish it from other futex futex_q's)\n *\n * Must be called with the hb lock held.\n */\nstatic struct futex_q *futex_top_waiter(struct futex_hash_bucket *hb,\n\t\t\t\t\tunion futex_key *key)\n{\n\tstruct futex_q *this;\n\n\tplist_for_each_entry(this, &hb->chain, list) {\n\t\tif (match_futex(&this->key, key))\n\t\t\treturn this;\n\t}\n\treturn NULL;\n}\n\nstatic int cmpxchg_futex_value_locked(u32 *curval, u32 __user *uaddr,\n\t\t\t\t      u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}\n\nstatic int get_futex_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}\n\n\n/*\n * PI code:\n */\nstatic int refill_pi_state_cache(void)\n{\n\tstruct futex_pi_state *pi_state;\n\n\tif (likely(current->pi_state_cache))\n\t\treturn 0;\n\n\tpi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);\n\n\tif (!pi_state)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pi_state->list);\n\t/* pi_mutex gets initialized later */\n\tpi_state->owner = NULL;\n\tatomic_set(&pi_state->refcount, 1);\n\tpi_state->key = FUTEX_KEY_INIT;\n\n\tcurrent->pi_state_cache = pi_state;\n\n\treturn 0;\n}\n\nstatic struct futex_pi_state *alloc_pi_state(void)\n{\n\tstruct futex_pi_state *pi_state = current->pi_state_cache;\n\n\tWARN_ON(!pi_state);\n\tcurrent->pi_state_cache = NULL;\n\n\treturn pi_state;\n}\n\nstatic void get_pi_state(struct futex_pi_state *pi_state)\n{\n\tWARN_ON_ONCE(!atomic_inc_not_zero(&pi_state->refcount));\n}\n\n/*\n * Drops a reference to the pi_state object and frees or caches it\n * when the last reference is gone.\n */\nstatic void put_pi_state(struct futex_pi_state *pi_state)\n{\n\tif (!pi_state)\n\t\treturn;\n\n\tif (!atomic_dec_and_test(&pi_state->refcount))\n\t\treturn;\n\n\t/*\n\t * If pi_state->owner is NULL, the owner is most probably dying\n\t * and has cleaned up the pi_state already\n\t */\n\tif (pi_state->owner) {\n\t\tstruct task_struct *owner;\n\n\t\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\t\towner = pi_state->owner;\n\t\tif (owner) {\n\t\t\traw_spin_lock(&owner->pi_lock);\n\t\t\tlist_del_init(&pi_state->list);\n\t\t\traw_spin_unlock(&owner->pi_lock);\n\t\t}\n\t\trt_mutex_proxy_unlock(&pi_state->pi_mutex, owner);\n\t\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t}\n\n\tif (current->pi_state_cache) {\n\t\tkfree(pi_state);\n\t} else {\n\t\t/*\n\t\t * pi_state->list is already empty.\n\t\t * clear pi_state->owner.\n\t\t * refcount is at 0 - put it back to 1.\n\t\t */\n\t\tpi_state->owner = NULL;\n\t\tatomic_set(&pi_state->refcount, 1);\n\t\tcurrent->pi_state_cache = pi_state;\n\t}\n}\n\n/*\n * Look up the task based on what TID userspace gave us.\n * We dont trust it.\n */\nstatic struct task_struct *futex_find_get_task(pid_t pid)\n{\n\tstruct task_struct *p;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (p)\n\t\tget_task_struct(p);\n\n\trcu_read_unlock();\n\n\treturn p;\n}\n\n#ifdef CONFIG_FUTEX_PI\n\n/*\n * This task is holding PI mutexes at exit time => bad.\n * Kernel cleans up PI-state, but userspace is likely hosed.\n * (Robust-futex cleanup is separate and might save the day for userspace.)\n */\nvoid exit_pi_state_list(struct task_struct *curr)\n{\n\tstruct list_head *next, *head = &curr->pi_state_list;\n\tstruct futex_pi_state *pi_state;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn;\n\t/*\n\t * We are a ZOMBIE and nobody can enqueue itself on\n\t * pi_state_list anymore, but we have to be careful\n\t * versus waiters unqueueing themselves:\n\t */\n\traw_spin_lock_irq(&curr->pi_lock);\n\twhile (!list_empty(head)) {\n\t\tnext = head->next;\n\t\tpi_state = list_entry(next, struct futex_pi_state, list);\n\t\tkey = pi_state->key;\n\t\thb = hash_futex(&key);\n\n\t\t/*\n\t\t * We can race against put_pi_state() removing itself from the\n\t\t * list (a waiter going away). put_pi_state() will first\n\t\t * decrement the reference count and then modify the list, so\n\t\t * its possible to see the list entry but fail this reference\n\t\t * acquire.\n\t\t *\n\t\t * In that case; drop the locks to let put_pi_state() make\n\t\t * progress and retry the loop.\n\t\t */\n\t\tif (!atomic_inc_not_zero(&pi_state->refcount)) {\n\t\t\traw_spin_unlock_irq(&curr->pi_lock);\n\t\t\tcpu_relax();\n\t\t\traw_spin_lock_irq(&curr->pi_lock);\n\t\t\tcontinue;\n\t\t}\n\t\traw_spin_unlock_irq(&curr->pi_lock);\n\n\t\tspin_lock(&hb->lock);\n\t\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\t\traw_spin_lock(&curr->pi_lock);\n\t\t/*\n\t\t * We dropped the pi-lock, so re-check whether this\n\t\t * task still owns the PI-state:\n\t\t */\n\t\tif (head->next != next) {\n\t\t\t/* retain curr->pi_lock for the loop invariant */\n\t\t\traw_spin_unlock(&pi_state->pi_mutex.wait_lock);\n\t\t\tspin_unlock(&hb->lock);\n\t\t\tput_pi_state(pi_state);\n\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON(pi_state->owner != curr);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\tpi_state->owner = NULL;\n\n\t\traw_spin_unlock(&curr->pi_lock);\n\t\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t\tspin_unlock(&hb->lock);\n\n\t\trt_mutex_futex_unlock(&pi_state->pi_mutex);\n\t\tput_pi_state(pi_state);\n\n\t\traw_spin_lock_irq(&curr->pi_lock);\n\t}\n\traw_spin_unlock_irq(&curr->pi_lock);\n}\n\n#endif\n\n/*\n * We need to check the following states:\n *\n *      Waiter | pi_state | pi->owner | uTID      | uODIED | ?\n *\n * [1]  NULL   | ---      | ---       | 0         | 0/1    | Valid\n * [2]  NULL   | ---      | ---       | >0        | 0/1    | Valid\n *\n * [3]  Found  | NULL     | --        | Any       | 0/1    | Invalid\n *\n * [4]  Found  | Found    | NULL      | 0         | 1      | Valid\n * [5]  Found  | Found    | NULL      | >0        | 1      | Invalid\n *\n * [6]  Found  | Found    | task      | 0         | 1      | Valid\n *\n * [7]  Found  | Found    | NULL      | Any       | 0      | Invalid\n *\n * [8]  Found  | Found    | task      | ==taskTID | 0/1    | Valid\n * [9]  Found  | Found    | task      | 0         | 0      | Invalid\n * [10] Found  | Found    | task      | !=taskTID | 0/1    | Invalid\n *\n * [1]\tIndicates that the kernel can acquire the futex atomically. We\n *\tcame came here due to a stale FUTEX_WAITERS/FUTEX_OWNER_DIED bit.\n *\n * [2]\tValid, if TID does not belong to a kernel thread. If no matching\n *      thread is found then it indicates that the owner TID has died.\n *\n * [3]\tInvalid. The waiter is queued on a non PI futex\n *\n * [4]\tValid state after exit_robust_list(), which sets the user space\n *\tvalue to FUTEX_WAITERS | FUTEX_OWNER_DIED.\n *\n * [5]\tThe user space value got manipulated between exit_robust_list()\n *\tand exit_pi_state_list()\n *\n * [6]\tValid state after exit_pi_state_list() which sets the new owner in\n *\tthe pi_state but cannot access the user space value.\n *\n * [7]\tpi_state->owner can only be NULL when the OWNER_DIED bit is set.\n *\n * [8]\tOwner and user space value match\n *\n * [9]\tThere is no transient state which sets the user space TID to 0\n *\texcept exit_robust_list(), but this is indicated by the\n *\tFUTEX_OWNER_DIED bit. See [4]\n *\n * [10] There is no transient state which leaves owner and user space\n *\tTID out of sync.\n *\n *\n * Serialization and lifetime rules:\n *\n * hb->lock:\n *\n *\thb -> futex_q, relation\n *\tfutex_q -> pi_state, relation\n *\n *\t(cannot be raw because hb can contain arbitrary amount\n *\t of futex_q's)\n *\n * pi_mutex->wait_lock:\n *\n *\t{uval, pi_state}\n *\n *\t(and pi_mutex 'obviously')\n *\n * p->pi_lock:\n *\n *\tp->pi_state_list -> pi_state->list, relation\n *\n * pi_state->refcount:\n *\n *\tpi_state lifetime\n *\n *\n * Lock order:\n *\n *   hb->lock\n *     pi_mutex->wait_lock\n *       p->pi_lock\n *\n */\n\n/*\n * Validate that the existing waiter has a pi_state and sanity check\n * the pi_state against the user space value. If correct, attach to\n * it.\n */\nstatic int attach_to_pi_state(u32 __user *uaddr, u32 uval,\n\t\t\t      struct futex_pi_state *pi_state,\n\t\t\t      struct futex_pi_state **ps)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tu32 uval2;\n\tint ret;\n\n\t/*\n\t * Userspace might have messed up non-PI and PI futexes [3]\n\t */\n\tif (unlikely(!pi_state))\n\t\treturn -EINVAL;\n\n\t/*\n\t * We get here with hb->lock held, and having found a\n\t * futex_top_waiter(). This means that futex_lock_pi() of said futex_q\n\t * has dropped the hb->lock in between queue_me() and unqueue_me_pi(),\n\t * which in turn means that futex_lock_pi() still has a reference on\n\t * our pi_state.\n\t *\n\t * The waiter holding a reference on @pi_state also protects against\n\t * the unlocked put_pi_state() in futex_unlock_pi(), futex_lock_pi()\n\t * and futex_wait_requeue_pi() as it cannot go to 0 and consequently\n\t * free pi_state before we can take a reference ourselves.\n\t */\n\tWARN_ON(!atomic_read(&pi_state->refcount));\n\n\t/*\n\t * Now that we have a pi_state, we can acquire wait_lock\n\t * and do the state validation.\n\t */\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Since {uval, pi_state} is serialized by wait_lock, and our current\n\t * uval was read without holding it, it can have changed. Verify it\n\t * still is what we expect it to be, otherwise retry the entire\n\t * operation.\n\t */\n\tif (get_futex_value_locked(&uval2, uaddr))\n\t\tgoto out_efault;\n\n\tif (uval != uval2)\n\t\tgoto out_eagain;\n\n\t/*\n\t * Handle the owner died case:\n\t */\n\tif (uval & FUTEX_OWNER_DIED) {\n\t\t/*\n\t\t * exit_pi_state_list sets owner to NULL and wakes the\n\t\t * topmost waiter. The task which acquires the\n\t\t * pi_state->rt_mutex will fixup owner.\n\t\t */\n\t\tif (!pi_state->owner) {\n\t\t\t/*\n\t\t\t * No pi state owner, but the user space TID\n\t\t\t * is not 0. Inconsistent state. [5]\n\t\t\t */\n\t\t\tif (pid)\n\t\t\t\tgoto out_einval;\n\t\t\t/*\n\t\t\t * Take a ref on the state and return success. [4]\n\t\t\t */\n\t\t\tgoto out_attach;\n\t\t}\n\n\t\t/*\n\t\t * If TID is 0, then either the dying owner has not\n\t\t * yet executed exit_pi_state_list() or some waiter\n\t\t * acquired the rtmutex in the pi state, but did not\n\t\t * yet fixup the TID in user space.\n\t\t *\n\t\t * Take a ref on the state and return success. [6]\n\t\t */\n\t\tif (!pid)\n\t\t\tgoto out_attach;\n\t} else {\n\t\t/*\n\t\t * If the owner died bit is not set, then the pi_state\n\t\t * must have an owner. [7]\n\t\t */\n\t\tif (!pi_state->owner)\n\t\t\tgoto out_einval;\n\t}\n\n\t/*\n\t * Bail out if user space manipulated the futex value. If pi\n\t * state exists then the owner TID must be the same as the\n\t * user space TID. [9/10]\n\t */\n\tif (pid != task_pid_vnr(pi_state->owner))\n\t\tgoto out_einval;\n\nout_attach:\n\tget_pi_state(pi_state);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t*ps = pi_state;\n\treturn 0;\n\nout_einval:\n\tret = -EINVAL;\n\tgoto out_error;\n\nout_eagain:\n\tret = -EAGAIN;\n\tgoto out_error;\n\nout_efault:\n\tret = -EFAULT;\n\tgoto out_error;\n\nout_error:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}\n\n/*\n * Lookup the task for the TID provided from user space and attach to\n * it after doing proper sanity checks.\n */\nstatic int attach_to_pi_owner(u32 uval, union futex_key *key,\n\t\t\t      struct futex_pi_state **ps)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tstruct futex_pi_state *pi_state;\n\tstruct task_struct *p;\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0 [1]\n\t */\n\tif (!pid)\n\t\treturn -ESRCH;\n\tp = futex_find_get_task(pid);\n\tif (!p)\n\t\treturn -ESRCH;\n\n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tput_task_struct(p);\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We need to look at the task state flags to figure out,\n\t * whether the task is exiting. To protect against the do_exit\n\t * change of the task flags, we do this protected by\n\t * p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->flags & PF_EXITING)) {\n\t\t/*\n\t\t * The task is on the way out. When PF_EXITPIDONE is\n\t\t * set, we know that the task has finished the\n\t\t * cleanup:\n\t\t */\n\t\tint ret = (p->flags & PF_EXITPIDONE) ? -ESRCH : -EAGAIN;\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * No existing pi state. First waiter. [2]\n\t *\n\t * This creates pi_state, we have hb->lock held, this means nothing can\n\t * observe this state, wait_lock is irrelevant.\n\t */\n\tpi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make @p\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\t/*\n\t * Assignment without holding pi_state->pi_mutex.wait_lock is safe\n\t * because there is no concurrency as the object is not published yet.\n\t */\n\tpi_state->owner = p;\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\t*ps = pi_state;\n\n\treturn 0;\n}\n\nstatic int lookup_pi_state(u32 __user *uaddr, u32 uval,\n\t\t\t   struct futex_hash_bucket *hb,\n\t\t\t   union futex_key *key, struct futex_pi_state **ps)\n{\n\tstruct futex_q *top_waiter = futex_top_waiter(hb, key);\n\n\t/*\n\t * If there is a waiter on that futex, validate it and\n\t * attach to the pi_state when the validation succeeds.\n\t */\n\tif (top_waiter)\n\t\treturn attach_to_pi_state(uaddr, uval, top_waiter->pi_state, ps);\n\n\t/*\n\t * We are the first waiter - try to look up the owner based on\n\t * @uval and attach to it.\n\t */\n\treturn attach_to_pi_owner(uval, key, ps);\n}\n\nstatic int lock_pi_update_atomic(u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tu32 uninitialized_var(curval);\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\tif (unlikely(cmpxchg_futex_value_locked(&curval, uaddr, uval, newval)))\n\t\treturn -EFAULT;\n\n\t/* If user space value changed, let the caller retry */\n\treturn curval != uval ? -EAGAIN : 0;\n}\n\n/**\n * futex_lock_pi_atomic() - Atomic work required to acquire a pi aware futex\n * @uaddr:\t\tthe pi futex user address\n * @hb:\t\t\tthe pi futex hash bucket\n * @key:\t\tthe futex key associated with uaddr and hb\n * @ps:\t\t\tthe pi_state pointer where we store the result of the\n *\t\t\tlookup\n * @task:\t\tthe task to perform the atomic lock work for.  This will\n *\t\t\tbe \"current\" except in the case of requeue pi.\n * @set_waiters:\tforce setting the FUTEX_WAITERS bit (1) or not (0)\n *\n * Return:\n *  -  0 - ready to wait;\n *  -  1 - acquired the lock;\n *  - <0 - error\n *\n * The hb->lock and futex_key refs shall be held by the caller.\n */\nstatic int futex_lock_pi_atomic(u32 __user *uaddr, struct futex_hash_bucket *hb,\n\t\t\t\tunion futex_key *key,\n\t\t\t\tstruct futex_pi_state **ps,\n\t\t\t\tstruct task_struct *task, int set_waiters)\n{\n\tu32 uval, newval, vpid = task_pid_vnr(task);\n\tstruct futex_q *top_waiter;\n\tint ret;\n\n\t/*\n\t * Read the user space value first so we can validate a few\n\t * things before proceeding further.\n\t */\n\tif (get_futex_value_locked(&uval, uaddr))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Detect deadlocks.\n\t */\n\tif ((unlikely((uval & FUTEX_TID_MASK) == vpid)))\n\t\treturn -EDEADLK;\n\n\tif ((unlikely(should_fail_futex(true))))\n\t\treturn -EDEADLK;\n\n\t/*\n\t * Lookup existing state first. If it exists, try to attach to\n\t * its pi_state.\n\t */\n\ttop_waiter = futex_top_waiter(hb, key);\n\tif (top_waiter)\n\t\treturn attach_to_pi_state(uaddr, uval, top_waiter->pi_state, ps);\n\n\t/*\n\t * No waiter and user TID is 0. We are here because the\n\t * waiters or the owner died bit is set or called from\n\t * requeue_cmp_pi or for whatever reason something took the\n\t * syscall.\n\t */\n\tif (!(uval & FUTEX_TID_MASK)) {\n\t\t/*\n\t\t * We take over the futex. No other waiters and the user space\n\t\t * TID is 0. We preserve the owner died bit.\n\t\t */\n\t\tnewval = uval & FUTEX_OWNER_DIED;\n\t\tnewval |= vpid;\n\n\t\t/* The futex requeue_pi code can enforce the waiters bit */\n\t\tif (set_waiters)\n\t\t\tnewval |= FUTEX_WAITERS;\n\n\t\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\t\t/* If the take over worked, return 1 */\n\t\treturn ret < 0 ? ret : 1;\n\t}\n\n\t/*\n\t * First waiter. Set the waiters bit before attaching ourself to\n\t * the owner. If owner tries to unlock, it will be forced into\n\t * the kernel and blocked on hb->lock.\n\t */\n\tnewval = uval | FUTEX_WAITERS;\n\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\tif (ret)\n\t\treturn ret;\n\t/*\n\t * If the update of the user space value succeeded, we try to\n\t * attach to the owner. If that fails, no harm done, we only\n\t * set the FUTEX_WAITERS bit in the user space variable.\n\t */\n\treturn attach_to_pi_owner(uval, key, ps);\n}\n\n/**\n * __unqueue_futex() - Remove the futex_q from its futex_hash_bucket\n * @q:\tThe futex_q to unqueue\n *\n * The q->lock_ptr must not be NULL and must be held by the caller.\n */\nstatic void __unqueue_futex(struct futex_q *q)\n{\n\tstruct futex_hash_bucket *hb;\n\n\tif (WARN_ON_SMP(!q->lock_ptr || !spin_is_locked(q->lock_ptr))\n\t    || WARN_ON(plist_node_empty(&q->list)))\n\t\treturn;\n\n\thb = container_of(q->lock_ptr, struct futex_hash_bucket, lock);\n\tplist_del(&q->list, &hb->chain);\n\thb_waiters_dec(hb);\n}\n\n/*\n * The hash bucket lock must be held when this is called.\n * Afterwards, the futex_q must not be accessed. Callers\n * must ensure to later call wake_up_q() for the actual\n * wakeups to occur.\n */\nstatic void mark_wake_futex(struct wake_q_head *wake_q, struct futex_q *q)\n{\n\tstruct task_struct *p = q->task;\n\n\tif (WARN(q->pi_state || q->rt_waiter, \"refusing to wake PI futex\\n\"))\n\t\treturn;\n\n\t/*\n\t * Queue the task for later wakeup for after we've released\n\t * the hb->lock. wake_q_add() grabs reference to p.\n\t */\n\twake_q_add(wake_q, p);\n\t__unqueue_futex(q);\n\t/*\n\t * The waiting task can free the futex_q as soon as q->lock_ptr = NULL\n\t * is written, without taking any locks. This is possible in the event\n\t * of a spurious wakeup, for example. A memory barrier is required here\n\t * to prevent the following store to lock_ptr from getting ahead of the\n\t * plist_del in __unqueue_futex().\n\t */\n\tsmp_store_release(&q->lock_ptr, NULL);\n}\n\n/*\n * Caller must hold a reference on @pi_state.\n */\nstatic int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_pi_state *pi_state)\n{\n\tu32 uninitialized_var(curval), newval;\n\tstruct task_struct *new_owner;\n\tbool postunlock = false;\n\tDEFINE_WAKE_Q(wake_q);\n\tint ret = 0;\n\n\tnew_owner = rt_mutex_next_owner(&pi_state->pi_mutex);\n\tif (WARN_ON_ONCE(!new_owner)) {\n\t\t/*\n\t\t * As per the comment in futex_unlock_pi() this should not happen.\n\t\t *\n\t\t * When this happens, give up our locks and try again, giving\n\t\t * the futex_lock_pi() instance time to complete, either by\n\t\t * waiting on the rtmutex or removing itself from the futex\n\t\t * queue.\n\t\t */\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * We pass it to the next owner. The WAITERS bit is always kept\n\t * enabled while there is PI state around. We cleanup the owner\n\t * died bit, because we are the owner.\n\t */\n\tnewval = FUTEX_WAITERS | task_pid_vnr(new_owner);\n\n\tif (unlikely(should_fail_futex(true)))\n\t\tret = -EFAULT;\n\n\tif (cmpxchg_futex_value_locked(&curval, uaddr, uval, newval)) {\n\t\tret = -EFAULT;\n\n\t} else if (curval != uval) {\n\t\t/*\n\t\t * If a unconditional UNLOCK_PI operation (user space did not\n\t\t * try the TID->0 transition) raced with a waiter setting the\n\t\t * FUTEX_WAITERS flag between get_user() and locking the hash\n\t\t * bucket lock, retry the operation.\n\t\t */\n\t\tif ((FUTEX_TID_MASK & curval) == uval)\n\t\t\tret = -EAGAIN;\n\t\telse\n\t\t\tret = -EINVAL;\n\t}\n\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * This is a point of no return; once we modify the uval there is no\n\t * going back and subsequent operations must not fail.\n\t */\n\n\traw_spin_lock(&pi_state->owner->pi_lock);\n\tWARN_ON(list_empty(&pi_state->list));\n\tlist_del_init(&pi_state->list);\n\traw_spin_unlock(&pi_state->owner->pi_lock);\n\n\traw_spin_lock(&new_owner->pi_lock);\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\tpi_state->owner = new_owner;\n\traw_spin_unlock(&new_owner->pi_lock);\n\n\tpostunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wake_q);\n\nout_unlock:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\n\tif (postunlock)\n\t\trt_mutex_postunlock(&wake_q);\n\n\treturn ret;\n}\n\n/*\n * Express the locking dependencies for lockdep:\n */\nstatic inline void\ndouble_lock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tif (hb1 <= hb2) {\n\t\tspin_lock(&hb1->lock);\n\t\tif (hb1 < hb2)\n\t\t\tspin_lock_nested(&hb2->lock, SINGLE_DEPTH_NESTING);\n\t} else { /* hb1 > hb2 */\n\t\tspin_lock(&hb2->lock);\n\t\tspin_lock_nested(&hb1->lock, SINGLE_DEPTH_NESTING);\n\t}\n}\n\nstatic inline void\ndouble_unlock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tspin_unlock(&hb1->lock);\n\tif (hb1 != hb2)\n\t\tspin_unlock(&hb2->lock);\n}\n\n/*\n * Wake up waiters matching bitset queued on this futex (uaddr).\n */\nstatic int\nfutex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)\n{\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *this, *next;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tint ret;\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\thb = hash_futex(&key);\n\n\t/* Make sure we really have tasks to wakeup */\n\tif (!hb_waiters_pending(hb))\n\t\tgoto out_put_key;\n\n\tspin_lock(&hb->lock);\n\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (match_futex (&this->key, &key)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check if one of the bits is set in both bitsets */\n\t\t\tif (!(this->bitset & bitset))\n\t\t\t\tcontinue;\n\n\t\t\tmark_wake_futex(&wake_q, this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock(&hb->lock);\n\twake_up_q(&wake_q);\nout_put_key:\n\tput_futex_key(&key);\nout:\n\treturn ret;\n}\n\nstatic int futex_atomic_op_inuser(unsigned int encoded_op, u32 __user *uaddr)\n{\n\tunsigned int op =\t  (encoded_op & 0x70000000) >> 28;\n\tunsigned int cmp =\t  (encoded_op & 0x0f000000) >> 24;\n\tint oparg = sign_extend32((encoded_op & 0x00fff000) >> 12, 11);\n\tint cmparg = sign_extend32(encoded_op & 0x00000fff, 11);\n\tint oldval, ret;\n\n\tif (encoded_op & (FUTEX_OP_OPARG_SHIFT << 28)) {\n\t\tif (oparg < 0 || oparg > 31) {\n\t\t\tchar comm[sizeof(current->comm)];\n\t\t\t/*\n\t\t\t * kill this print and return -EINVAL when userspace\n\t\t\t * is sane again\n\t\t\t */\n\t\t\tpr_info_ratelimited(\"futex_wake_op: %s tries to shift op by %d; fix this program\\n\",\n\t\t\t\t\tget_task_comm(comm, current), oparg);\n\t\t\toparg &= 31;\n\t\t}\n\t\toparg = 1 << oparg;\n\t}\n\n\tif (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))\n\t\treturn -EFAULT;\n\n\tret = arch_futex_atomic_op_inuser(op, oparg, &oldval, uaddr);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (cmp) {\n\tcase FUTEX_OP_CMP_EQ:\n\t\treturn oldval == cmparg;\n\tcase FUTEX_OP_CMP_NE:\n\t\treturn oldval != cmparg;\n\tcase FUTEX_OP_CMP_LT:\n\t\treturn oldval < cmparg;\n\tcase FUTEX_OP_CMP_GE:\n\t\treturn oldval >= cmparg;\n\tcase FUTEX_OP_CMP_LE:\n\t\treturn oldval <= cmparg;\n\tcase FUTEX_OP_CMP_GT:\n\t\treturn oldval > cmparg;\n\tdefault:\n\t\treturn -ENOSYS;\n\t}\n}\n\n/*\n * Wake up all waiters hashed on the physical page that is mapped\n * to this virtual address:\n */\nstatic int\nfutex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,\n\t      int nr_wake, int nr_wake2, int op)\n{\n\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;\n\tstruct futex_hash_bucket *hb1, *hb2;\n\tstruct futex_q *this, *next;\n\tint ret, op_ret;\n\tDEFINE_WAKE_Q(wake_q);\n\nretry:\n\tret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out_put_key1;\n\n\thb1 = hash_futex(&key1);\n\thb2 = hash_futex(&key2);\n\nretry_private:\n\tdouble_lock_hb(hb1, hb2);\n\top_ret = futex_atomic_op_inuser(op, uaddr2);\n\tif (unlikely(op_ret < 0)) {\n\n\t\tdouble_unlock_hb(hb1, hb2);\n\n#ifndef CONFIG_MMU\n\t\t/*\n\t\t * we don't get EFAULT from MMU faults if we don't have an MMU,\n\t\t * but we might get them from range checking\n\t\t */\n\t\tret = op_ret;\n\t\tgoto out_put_keys;\n#endif\n\n\t\tif (unlikely(op_ret != -EFAULT)) {\n\t\t\tret = op_ret;\n\t\t\tgoto out_put_keys;\n\t\t}\n\n\t\tret = fault_in_user_writeable(uaddr2);\n\t\tif (ret)\n\t\t\tgoto out_put_keys;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tput_futex_key(&key2);\n\t\tput_futex_key(&key1);\n\t\tgoto retry;\n\t}\n\n\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {\n\t\tif (match_futex (&this->key, &key1)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tmark_wake_futex(&wake_q, this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (op_ret > 0) {\n\t\top_ret = 0;\n\t\tplist_for_each_entry_safe(this, next, &hb2->chain, list) {\n\t\t\tif (match_futex (&this->key, &key2)) {\n\t\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t\tmark_wake_futex(&wake_q, this);\n\t\t\t\tif (++op_ret >= nr_wake2)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tret += op_ret;\n\t}\n\nout_unlock:\n\tdouble_unlock_hb(hb1, hb2);\n\twake_up_q(&wake_q);\nout_put_keys:\n\tput_futex_key(&key2);\nout_put_key1:\n\tput_futex_key(&key1);\nout:\n\treturn ret;\n}\n\n/**\n * requeue_futex() - Requeue a futex_q from one hb to another\n * @q:\t\tthe futex_q to requeue\n * @hb1:\tthe source hash_bucket\n * @hb2:\tthe target hash_bucket\n * @key2:\tthe new key for the requeued futex_q\n */\nstatic inline\nvoid requeue_futex(struct futex_q *q, struct futex_hash_bucket *hb1,\n\t\t   struct futex_hash_bucket *hb2, union futex_key *key2)\n{\n\n\t/*\n\t * If key1 and key2 hash to the same bucket, no need to\n\t * requeue.\n\t */\n\tif (likely(&hb1->chain != &hb2->chain)) {\n\t\tplist_del(&q->list, &hb1->chain);\n\t\thb_waiters_dec(hb1);\n\t\thb_waiters_inc(hb2);\n\t\tplist_add(&q->list, &hb2->chain);\n\t\tq->lock_ptr = &hb2->lock;\n\t}\n\tget_futex_key_refs(key2);\n\tq->key = *key2;\n}\n\n/**\n * requeue_pi_wake_futex() - Wake a task that acquired the lock during requeue\n * @q:\t\tthe futex_q\n * @key:\tthe key of the requeue target futex\n * @hb:\t\tthe hash_bucket of the requeue target futex\n *\n * During futex_requeue, with requeue_pi=1, it is possible to acquire the\n * target futex if it is uncontended or via a lock steal.  Set the futex_q key\n * to the requeue target futex so the waiter can detect the wakeup on the right\n * futex, but remove it from the hb and NULL the rt_waiter so it can detect\n * atomic lock acquisition.  Set the q->lock_ptr to the requeue target hb->lock\n * to protect access to the pi_state to fixup the owner later.  Must be called\n * with both q->lock_ptr and hb->lock held.\n */\nstatic inline\nvoid requeue_pi_wake_futex(struct futex_q *q, union futex_key *key,\n\t\t\t   struct futex_hash_bucket *hb)\n{\n\tget_futex_key_refs(key);\n\tq->key = *key;\n\n\t__unqueue_futex(q);\n\n\tWARN_ON(!q->rt_waiter);\n\tq->rt_waiter = NULL;\n\n\tq->lock_ptr = &hb->lock;\n\n\twake_up_state(q->task, TASK_NORMAL);\n}\n\n/**\n * futex_proxy_trylock_atomic() - Attempt an atomic lock for the top waiter\n * @pifutex:\t\tthe user address of the to futex\n * @hb1:\t\tthe from futex hash bucket, must be locked by the caller\n * @hb2:\t\tthe to futex hash bucket, must be locked by the caller\n * @key1:\t\tthe from futex key\n * @key2:\t\tthe to futex key\n * @ps:\t\t\taddress to store the pi_state pointer\n * @set_waiters:\tforce setting the FUTEX_WAITERS bit (1) or not (0)\n *\n * Try and get the lock on behalf of the top waiter if we can do it atomically.\n * Wake the top waiter if we succeed.  If the caller specified set_waiters,\n * then direct futex_lock_pi_atomic() to force setting the FUTEX_WAITERS bit.\n * hb1 and hb2 must be held by the caller.\n *\n * Return:\n *  -  0 - failed to acquire the lock atomically;\n *  - >0 - acquired the lock, return value is vpid of the top_waiter\n *  - <0 - error\n */\nstatic int futex_proxy_trylock_atomic(u32 __user *pifutex,\n\t\t\t\t struct futex_hash_bucket *hb1,\n\t\t\t\t struct futex_hash_bucket *hb2,\n\t\t\t\t union futex_key *key1, union futex_key *key2,\n\t\t\t\t struct futex_pi_state **ps, int set_waiters)\n{\n\tstruct futex_q *top_waiter = NULL;\n\tu32 curval;\n\tint ret, vpid;\n\n\tif (get_futex_value_locked(&curval, pifutex))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Find the top_waiter and determine if there are additional waiters.\n\t * If the caller intends to requeue more than 1 waiter to pifutex,\n\t * force futex_lock_pi_atomic() to set the FUTEX_WAITERS bit now,\n\t * as we have means to handle the possible fault.  If not, don't set\n\t * the bit unecessarily as it will force the subsequent unlock to enter\n\t * the kernel.\n\t */\n\ttop_waiter = futex_top_waiter(hb1, key1);\n\n\t/* There are no waiters, nothing for us to do. */\n\tif (!top_waiter)\n\t\treturn 0;\n\n\t/* Ensure we requeue to the expected futex. */\n\tif (!match_futex(top_waiter->requeue_pi_key, key2))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Try to take the lock for top_waiter.  Set the FUTEX_WAITERS bit in\n\t * the contended case or if set_waiters is 1.  The pi_state is returned\n\t * in ps in contended cases.\n\t */\n\tvpid = task_pid_vnr(top_waiter->task);\n\tret = futex_lock_pi_atomic(pifutex, hb2, key2, ps, top_waiter->task,\n\t\t\t\t   set_waiters);\n\tif (ret == 1) {\n\t\trequeue_pi_wake_futex(top_waiter, key2, hb2);\n\t\treturn vpid;\n\t}\n\treturn ret;\n}\n\n/**\n * futex_requeue() - Requeue waiters from uaddr1 to uaddr2\n * @uaddr1:\tsource futex user address\n * @flags:\tfutex flags (FLAGS_SHARED, etc.)\n * @uaddr2:\ttarget futex user address\n * @nr_wake:\tnumber of waiters to wake (must be 1 for requeue_pi)\n * @nr_requeue:\tnumber of waiters to requeue (0-INT_MAX)\n * @cmpval:\t@uaddr1 expected value (or %NULL)\n * @requeue_pi:\tif we are attempting to requeue from a non-pi futex to a\n *\t\tpi futex (pi to pi requeue is not supported)\n *\n * Requeue waiters on uaddr1 to uaddr2. In the requeue_pi case, try to acquire\n * uaddr2 atomically on behalf of the top waiter.\n *\n * Return:\n *  - >=0 - on success, the number of tasks requeued or woken;\n *  -  <0 - on error\n */\nstatic int futex_requeue(u32 __user *uaddr1, unsigned int flags,\n\t\t\t u32 __user *uaddr2, int nr_wake, int nr_requeue,\n\t\t\t u32 *cmpval, int requeue_pi)\n{\n\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;\n\tint drop_count = 0, task_count = 0, ret;\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct futex_hash_bucket *hb1, *hb2;\n\tstruct futex_q *this, *next;\n\tDEFINE_WAKE_Q(wake_q);\n\n\t/*\n\t * When PI not supported: return -ENOSYS if requeue_pi is true,\n\t * consequently the compiler knows requeue_pi is always false past\n\t * this point which will optimize away all the conditional code\n\t * further down.\n\t */\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI) && requeue_pi)\n\t\treturn -ENOSYS;\n\n\tif (requeue_pi) {\n\t\t/*\n\t\t * Requeue PI only works on two distinct uaddrs. This\n\t\t * check is only valid for private futexes. See below.\n\t\t */\n\t\tif (uaddr1 == uaddr2)\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * requeue_pi requires a pi_state, try to allocate it now\n\t\t * without any locks in case it fails.\n\t\t */\n\t\tif (refill_pi_state_cache())\n\t\t\treturn -ENOMEM;\n\t\t/*\n\t\t * requeue_pi must wake as many tasks as it can, up to nr_wake\n\t\t * + nr_requeue, since it acquires the rt_mutex prior to\n\t\t * returning to userspace, so as to not leave the rt_mutex with\n\t\t * waiters and no owner.  However, second and third wake-ups\n\t\t * cannot be predicted as they involve race conditions with the\n\t\t * first wake and a fault while looking up the pi_state.  Both\n\t\t * pthread_cond_signal() and pthread_cond_broadcast() should\n\t\t * use nr_wake=1.\n\t\t */\n\t\tif (nr_wake != 1)\n\t\t\treturn -EINVAL;\n\t}\n\nretry:\n\tret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2,\n\t\t\t    requeue_pi ? VERIFY_WRITE : VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out_put_key1;\n\n\t/*\n\t * The check above which compares uaddrs is not sufficient for\n\t * shared futexes. We need to compare the keys:\n\t */\n\tif (requeue_pi && match_futex(&key1, &key2)) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_keys;\n\t}\n\n\thb1 = hash_futex(&key1);\n\thb2 = hash_futex(&key2);\n\nretry_private:\n\thb_waiters_inc(hb2);\n\tdouble_lock_hb(hb1, hb2);\n\n\tif (likely(cmpval != NULL)) {\n\t\tu32 curval;\n\n\t\tret = get_futex_value_locked(&curval, uaddr1);\n\n\t\tif (unlikely(ret)) {\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\n\t\t\tret = get_user(curval, uaddr1);\n\t\t\tif (ret)\n\t\t\t\tgoto out_put_keys;\n\n\t\t\tif (!(flags & FLAGS_SHARED))\n\t\t\t\tgoto retry_private;\n\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tgoto retry;\n\t\t}\n\t\tif (curval != *cmpval) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (requeue_pi && (task_count - nr_wake < nr_requeue)) {\n\t\t/*\n\t\t * Attempt to acquire uaddr2 and wake the top waiter. If we\n\t\t * intend to requeue waiters, force setting the FUTEX_WAITERS\n\t\t * bit.  We force this here where we are able to easily handle\n\t\t * faults rather in the requeue loop below.\n\t\t */\n\t\tret = futex_proxy_trylock_atomic(uaddr2, hb1, hb2, &key1,\n\t\t\t\t\t\t &key2, &pi_state, nr_requeue);\n\n\t\t/*\n\t\t * At this point the top_waiter has either taken uaddr2 or is\n\t\t * waiting on it.  If the former, then the pi_state will not\n\t\t * exist yet, look it up one more time to ensure we have a\n\t\t * reference to it. If the lock was taken, ret contains the\n\t\t * vpid of the top waiter task.\n\t\t * If the lock was not taken, we have pi_state and an initial\n\t\t * refcount on it. In case of an error we have nothing.\n\t\t */\n\t\tif (ret > 0) {\n\t\t\tWARN_ON(pi_state);\n\t\t\tdrop_count++;\n\t\t\ttask_count++;\n\t\t\t/*\n\t\t\t * If we acquired the lock, then the user space value\n\t\t\t * of uaddr2 should be vpid. It cannot be changed by\n\t\t\t * the top waiter as it is blocked on hb2 lock if it\n\t\t\t * tries to do so. If something fiddled with it behind\n\t\t\t * our back the pi state lookup might unearth it. So\n\t\t\t * we rather use the known value than rereading and\n\t\t\t * handing potential crap to lookup_pi_state.\n\t\t\t *\n\t\t\t * If that call succeeds then we have pi_state and an\n\t\t\t * initial refcount on it.\n\t\t\t */\n\t\t\tret = lookup_pi_state(uaddr2, ret, hb2, &key2, &pi_state);\n\t\t}\n\n\t\tswitch (ret) {\n\t\tcase 0:\n\t\t\t/* We hold a reference on the pi state. */\n\t\t\tbreak;\n\n\t\t\t/* If the above failed, then pi_state is NULL */\n\t\tcase -EFAULT:\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tret = fault_in_user_writeable(uaddr2);\n\t\t\tif (!ret)\n\t\t\t\tgoto retry;\n\t\t\tgoto out;\n\t\tcase -EAGAIN:\n\t\t\t/*\n\t\t\t * Two reasons for this:\n\t\t\t * - Owner is exiting and we just wait for the\n\t\t\t *   exit to complete.\n\t\t\t * - The user space value changed.\n\t\t\t */\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\t\tdefault:\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {\n\t\tif (task_count - nr_wake >= nr_requeue)\n\t\t\tbreak;\n\n\t\tif (!match_futex(&this->key, &key1))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * FUTEX_WAIT_REQEUE_PI and FUTEX_CMP_REQUEUE_PI should always\n\t\t * be paired with each other and no other futex ops.\n\t\t *\n\t\t * We should never be requeueing a futex_q with a pi_state,\n\t\t * which is awaiting a futex_unlock_pi().\n\t\t */\n\t\tif ((requeue_pi && !this->rt_waiter) ||\n\t\t    (!requeue_pi && this->rt_waiter) ||\n\t\t    this->pi_state) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Wake nr_wake waiters.  For requeue_pi, if we acquired the\n\t\t * lock, we already woke the top_waiter.  If not, it will be\n\t\t * woken by futex_unlock_pi().\n\t\t */\n\t\tif (++task_count <= nr_wake && !requeue_pi) {\n\t\t\tmark_wake_futex(&wake_q, this);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Ensure we requeue to the expected futex for requeue_pi. */\n\t\tif (requeue_pi && !match_futex(this->requeue_pi_key, &key2)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Requeue nr_requeue waiters and possibly one more in the case\n\t\t * of requeue_pi if we couldn't acquire the lock atomically.\n\t\t */\n\t\tif (requeue_pi) {\n\t\t\t/*\n\t\t\t * Prepare the waiter to take the rt_mutex. Take a\n\t\t\t * refcount on the pi_state and store the pointer in\n\t\t\t * the futex_q object of the waiter.\n\t\t\t */\n\t\t\tget_pi_state(pi_state);\n\t\t\tthis->pi_state = pi_state;\n\t\t\tret = rt_mutex_start_proxy_lock(&pi_state->pi_mutex,\n\t\t\t\t\t\t\tthis->rt_waiter,\n\t\t\t\t\t\t\tthis->task);\n\t\t\tif (ret == 1) {\n\t\t\t\t/*\n\t\t\t\t * We got the lock. We do neither drop the\n\t\t\t\t * refcount on pi_state nor clear\n\t\t\t\t * this->pi_state because the waiter needs the\n\t\t\t\t * pi_state for cleaning up the user space\n\t\t\t\t * value. It will drop the refcount after\n\t\t\t\t * doing so.\n\t\t\t\t */\n\t\t\t\trequeue_pi_wake_futex(this, &key2, hb2);\n\t\t\t\tdrop_count++;\n\t\t\t\tcontinue;\n\t\t\t} else if (ret) {\n\t\t\t\t/*\n\t\t\t\t * rt_mutex_start_proxy_lock() detected a\n\t\t\t\t * potential deadlock when we tried to queue\n\t\t\t\t * that waiter. Drop the pi_state reference\n\t\t\t\t * which we took above and remove the pointer\n\t\t\t\t * to the state from the waiters futex_q\n\t\t\t\t * object.\n\t\t\t\t */\n\t\t\t\tthis->pi_state = NULL;\n\t\t\t\tput_pi_state(pi_state);\n\t\t\t\t/*\n\t\t\t\t * We stop queueing more waiters and let user\n\t\t\t\t * space deal with the mess.\n\t\t\t\t */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trequeue_futex(this, hb1, hb2, &key2);\n\t\tdrop_count++;\n\t}\n\n\t/*\n\t * We took an extra initial reference to the pi_state either\n\t * in futex_proxy_trylock_atomic() or in lookup_pi_state(). We\n\t * need to drop it here again.\n\t */\n\tput_pi_state(pi_state);\n\nout_unlock:\n\tdouble_unlock_hb(hb1, hb2);\n\twake_up_q(&wake_q);\n\thb_waiters_dec(hb2);\n\n\t/*\n\t * drop_futex_key_refs() must be called outside the spinlocks. During\n\t * the requeue we moved futex_q's from the hash bucket at key1 to the\n\t * one at key2 and updated their key pointer.  We no longer need to\n\t * hold the references to key1.\n\t */\n\twhile (--drop_count >= 0)\n\t\tdrop_futex_key_refs(&key1);\n\nout_put_keys:\n\tput_futex_key(&key2);\nout_put_key1:\n\tput_futex_key(&key1);\nout:\n\treturn ret ? ret : task_count;\n}\n\n/* The key must be already stored in q->key. */\nstatic inline struct futex_hash_bucket *queue_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = hash_futex(&q->key);\n\n\t/*\n\t * Increment the counter before taking the lock so that\n\t * a potential waker won't miss a to-be-slept task that is\n\t * waiting for the spinlock. This is safe as all queue_lock()\n\t * users end up calling queue_me(). Similarly, for housekeeping,\n\t * decrement the counter at queue_unlock() when some error has\n\t * occurred and we don't end up adding the task to the list.\n\t */\n\thb_waiters_inc(hb);\n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock); /* implies smp_mb(); (A) */\n\treturn hb;\n}\n\nstatic inline void\nqueue_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\thb_waiters_dec(hb);\n}\n\nstatic inline void __queue_me(struct futex_q *q, struct futex_hash_bucket *hb)\n{\n\tint prio;\n\n\t/*\n\t * The priority used to register this element is\n\t * - either the real thread-priority for the real-time threads\n\t * (i.e. threads with a priority lower than MAX_RT_PRIO)\n\t * - or MAX_RT_PRIO for non-RT threads.\n\t * Thus, all RT-threads are woken first in priority order, and\n\t * the others are woken last, in FIFO order.\n\t */\n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n}\n\n/**\n * queue_me() - Enqueue the futex_q on the futex_hash_bucket\n * @q:\tThe futex_q to enqueue\n * @hb:\tThe destination hash bucket\n *\n * The hb->lock must be held by the caller, and is released here. A call to\n * queue_me() is typically paired with exactly one call to unqueue_me().  The\n * exceptions involve the PI related operations, which may use unqueue_me_pi()\n * or nothing if the unqueue is done as part of the wake process and the unqueue\n * state is implicit in the state of woken task (see futex_wait_requeue_pi() for\n * an example).\n */\nstatic inline void queue_me(struct futex_q *q, struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\t__queue_me(q, hb);\n\tspin_unlock(&hb->lock);\n}\n\n/**\n * unqueue_me() - Remove the futex_q from its futex_hash_bucket\n * @q:\tThe futex_q to unqueue\n *\n * The q->lock_ptr must not be held by the caller. A call to unqueue_me() must\n * be paired with exactly one earlier call to queue_me().\n *\n * Return:\n *  - 1 - if the futex_q was still queued (and we removed unqueued it);\n *  - 0 - if the futex_q was already removed by the waking thread\n */\nstatic int unqueue_me(struct futex_q *q)\n{\n\tspinlock_t *lock_ptr;\n\tint ret = 0;\n\n\t/* In the common case we don't take the spinlock, which is nice. */\nretry:\n\t/*\n\t * q->lock_ptr can change between this read and the following spin_lock.\n\t * Use READ_ONCE to forbid the compiler from reloading q->lock_ptr and\n\t * optimizing lock_ptr out of the logic below.\n\t */\n\tlock_ptr = READ_ONCE(q->lock_ptr);\n\tif (lock_ptr != NULL) {\n\t\tspin_lock(lock_ptr);\n\t\t/*\n\t\t * q->lock_ptr can change between reading it and\n\t\t * spin_lock(), causing us to take the wrong lock.  This\n\t\t * corrects the race condition.\n\t\t *\n\t\t * Reasoning goes like this: if we have the wrong lock,\n\t\t * q->lock_ptr must have changed (maybe several times)\n\t\t * between reading it and the spin_lock().  It can\n\t\t * change again after the spin_lock() but only if it was\n\t\t * already changed before the spin_lock().  It cannot,\n\t\t * however, change back to the original value.  Therefore\n\t\t * we can detect whether we acquired the correct lock.\n\t\t */\n\t\tif (unlikely(lock_ptr != q->lock_ptr)) {\n\t\t\tspin_unlock(lock_ptr);\n\t\t\tgoto retry;\n\t\t}\n\t\t__unqueue_futex(q);\n\n\t\tBUG_ON(q->pi_state);\n\n\t\tspin_unlock(lock_ptr);\n\t\tret = 1;\n\t}\n\n\tdrop_futex_key_refs(&q->key);\n\treturn ret;\n}\n\n/*\n * PI futexes can not be requeued and must remove themself from the\n * hash bucket. The hash bucket lock (i.e. lock_ptr) is held on entry\n * and dropped here.\n */\nstatic void unqueue_me_pi(struct futex_q *q)\n\t__releases(q->lock_ptr)\n{\n\t__unqueue_futex(q);\n\n\tBUG_ON(!q->pi_state);\n\tput_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n\n\tspin_unlock(q->lock_ptr);\n}\n\nstatic int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\tstruct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tu32 uval, uninitialized_var(curval), newval;\n\tstruct task_struct *oldowner, *newowner;\n\tu32 newtid;\n\tint ret;\n\n\tlockdep_assert_held(q->lock_ptr);\n\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\toldowner = pi_state->owner;\n\t/* Owner died? */\n\tif (!pi_state->owner)\n\t\tnewtid |= FUTEX_OWNER_DIED;\n\n\t/*\n\t * We are here because either:\n\t *\n\t *  - we stole the lock and pi_state->owner needs updating to reflect\n\t *    that (@argowner == current),\n\t *\n\t * or:\n\t *\n\t *  - someone stole our lock and we need to fix things to point to the\n\t *    new owner (@argowner == NULL).\n\t *\n\t * Either way, we have to replace the TID in the user space variable.\n\t * This must be atomic as we have to preserve the owner died bit here.\n\t *\n\t * Note: We write the user space value _before_ changing the pi_state\n\t * because we can fault here. Imagine swapped out pages or a fork\n\t * that marked all the anonymous memory readonly for cow.\n\t *\n\t * Modifying pi_state _before_ the user space value would leave the\n\t * pi_state in an inconsistent state when we fault here, because we\n\t * need to drop the locks to handle the fault. This might be observed\n\t * in the PID check in lookup_pi_state.\n\t */\nretry:\n\tif (!argowner) {\n\t\tif (oldowner != current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (__rt_mutex_futex_trylock(&pi_state->pi_mutex)) {\n\t\t\t/* We got the lock after all, nothing to fix. */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Since we just failed the trylock; there must be an owner.\n\t\t */\n\t\tnewowner = rt_mutex_owner(&pi_state->pi_mutex);\n\t\tBUG_ON(!newowner);\n\t} else {\n\t\tWARN_ON_ONCE(argowner != current);\n\t\tif (oldowner == current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tnewowner = argowner;\n\t}\n\n\tnewtid = task_pid_vnr(newowner) | FUTEX_WAITERS;\n\n\tif (get_futex_value_locked(&uval, uaddr))\n\t\tgoto handle_fault;\n\n\tfor (;;) {\n\t\tnewval = (uval & FUTEX_OWNER_DIED) | newtid;\n\n\t\tif (cmpxchg_futex_value_locked(&curval, uaddr, uval, newval))\n\t\t\tgoto handle_fault;\n\t\tif (curval == uval)\n\t\t\tbreak;\n\t\tuval = curval;\n\t}\n\n\t/*\n\t * We fixed up user space. Now we need to fix the pi_state\n\t * itself.\n\t */\n\tif (pi_state->owner != NULL) {\n\t\traw_spin_lock(&pi_state->owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&pi_state->owner->pi_lock);\n\t}\n\n\tpi_state->owner = newowner;\n\n\traw_spin_lock(&newowner->pi_lock);\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &newowner->pi_state_list);\n\traw_spin_unlock(&newowner->pi_lock);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\n\treturn 0;\n\n\t/*\n\t * To handle the page fault we need to drop the locks here. That gives\n\t * the other task (either the highest priority waiter itself or the\n\t * task which stole the rtmutex) the chance to try the fixup of the\n\t * pi_state. So once we are back from handling the fault we need to\n\t * check the pi_state after reacquiring the locks and before trying to\n\t * do another fixup. When the fixup has been done already we simply\n\t * return.\n\t *\n\t * Note: we hold both hb->lock and pi_mutex->wait_lock. We can safely\n\t * drop hb->lock since the caller owns the hb -> futex_q relation.\n\t * Dropping the pi_mutex->wait_lock requires the state revalidate.\n\t */\nhandle_fault:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q->lock_ptr);\n\n\tret = fault_in_user_writeable(uaddr);\n\n\tspin_lock(q->lock_ptr);\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Check if someone else fixed it for us:\n\t */\n\tif (pi_state->owner != oldowner) {\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tgoto retry;\n\nout_unlock:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}\n\nstatic long futex_wait_restart(struct restart_block *restart);\n\n/**\n * fixup_owner() - Post lock pi_state and corner case management\n * @uaddr:\tuser address of the futex\n * @q:\t\tfutex_q (contains pi_state and access to the rt_mutex)\n * @locked:\tif the attempt to take the rt_mutex succeeded (1) or not (0)\n *\n * After attempting to lock an rt_mutex, this function is called to cleanup\n * the pi_state owner as well as handle race conditions that may allow us to\n * acquire the lock. Must be called with the hb lock held.\n *\n * Return:\n *  -  1 - success, lock taken;\n *  -  0 - success, lock not taken;\n *  - <0 - on error (-EFAULT)\n */\nstatic int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tint ret = 0;\n\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\tret = fixup_pi_state_owner(uaddr, q, current);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current) {\n\t\tret = fixup_pi_state_owner(uaddr, q, NULL);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex.\n\t */\n\tif (rt_mutex_owner(&q->pi_state->pi_mutex) == current) {\n\t\tprintk(KERN_ERR \"fixup_owner: ret = %d pi-mutex: %p \"\n\t\t\t\t\"pi-state %p\\n\", ret,\n\t\t\t\tq->pi_state->pi_mutex.owner,\n\t\t\t\tq->pi_state->owner);\n\t}\n\nout:\n\treturn ret ? ret : locked;\n}\n\n/**\n * futex_wait_queue_me() - queue_me() and wait for wakeup, timeout, or signal\n * @hb:\t\tthe futex hash bucket, must be locked by the caller\n * @q:\t\tthe futex_q to queue up on\n * @timeout:\tthe prepared hrtimer_sleeper, or null for no timeout\n */\nstatic void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,\n\t\t\t\tstruct hrtimer_sleeper *timeout)\n{\n\t/*\n\t * The task state is guaranteed to be set before another task can\n\t * wake it. set_current_state() is implemented using smp_store_mb() and\n\t * queue_me() calls spin_unlock() upon completion, both serializing\n\t * access to the hash list and forcing another memory barrier.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tqueue_me(q, hb);\n\n\t/* Arm the timer */\n\tif (timeout)\n\t\thrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);\n\n\t/*\n\t * If we have been removed from the hash list, then another task\n\t * has tried to wake us, and we can skip the call to schedule().\n\t */\n\tif (likely(!plist_node_empty(&q->list))) {\n\t\t/*\n\t\t * If the timer has already expired, current will already be\n\t\t * flagged for rescheduling. Only call schedule if there\n\t\t * is no timeout, or if it has yet to expire.\n\t\t */\n\t\tif (!timeout || timeout->task)\n\t\t\tfreezable_schedule();\n\t}\n\t__set_current_state(TASK_RUNNING);\n}\n\n/**\n * futex_wait_setup() - Prepare to wait on a futex\n * @uaddr:\tthe futex userspace address\n * @val:\tthe expected value\n * @flags:\tfutex flags (FLAGS_SHARED, etc.)\n * @q:\t\tthe associated futex_q\n * @hb:\t\tstorage for hash_bucket pointer to be returned to caller\n *\n * Setup the futex_q and locate the hash_bucket.  Get the futex value and\n * compare it with the expected value.  Handle atomic faults internally.\n * Return with the hb lock held and a q.key reference on success, and unlocked\n * with no q.key reference on failure.\n *\n * Return:\n *  -  0 - uaddr contains val and hb has been locked;\n *  - <1 - -EFAULT or -EWOULDBLOCK (uaddr does not contain val) and hb is unlocked\n */\nstatic int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,\n\t\t\t   struct futex_q *q, struct futex_hash_bucket **hb)\n{\n\tu32 uval;\n\tint ret;\n\n\t/*\n\t * Access the page AFTER the hash-bucket is locked.\n\t * Order is important:\n\t *\n\t *   Userspace waiter: val = var; if (cond(val)) futex_wait(&var, val);\n\t *   Userspace waker:  if (cond(var)) { var = new; futex_wake(&var); }\n\t *\n\t * The basic logical guarantee of a futex is that it blocks ONLY\n\t * if cond(var) is known to be true at the time of blocking, for\n\t * any cond.  If we locked the hash-bucket after testing *uaddr, that\n\t * would open a race condition where we could block indefinitely with\n\t * cond(var) false, which would violate the guarantee.\n\t *\n\t * On the other hand, we insert q and release the hash-bucket only\n\t * after testing *uaddr.  This guarantees that futex_wait() will NOT\n\t * absorb a wakeup if *uaddr does not match the desired values\n\t * while the syscall executes.\n\t */\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\nretry_private:\n\t*hb = queue_lock(q);\n\n\tret = get_futex_value_locked(&uval, uaddr);\n\n\tif (ret) {\n\t\tqueue_unlock(*hb);\n\n\t\tret = get_user(uval, uaddr);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tput_futex_key(&q->key);\n\t\tgoto retry;\n\t}\n\n\tif (uval != val) {\n\t\tqueue_unlock(*hb);\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout:\n\tif (ret)\n\t\tput_futex_key(&q->key);\n\treturn ret;\n}\n\nstatic int futex_wait(u32 __user *uaddr, unsigned int flags, u32 val,\n\t\t      ktime_t *abs_time, u32 bitset)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct restart_block *restart;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\tq.bitset = bitset;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\nretry:\n\t/*\n\t * Prepare to wait on uaddr. On success, holds hb lock and increments\n\t * q.key refs.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/* queue_me and wait for wakeup, timeout, or a signal. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\t/* If we were woken (and unqueued), we succeeded, whatever. */\n\tret = 0;\n\t/* unqueue_me() drops q.key ref */\n\tif (!unqueue_me(&q))\n\t\tgoto out;\n\tret = -ETIMEDOUT;\n\tif (to && !to->task)\n\t\tgoto out;\n\n\t/*\n\t * We expect signal_pending(current), but we might be the\n\t * victim of a spurious wakeup as well.\n\t */\n\tif (!signal_pending(current))\n\t\tgoto retry;\n\n\tret = -ERESTARTSYS;\n\tif (!abs_time)\n\t\tgoto out;\n\n\trestart = &current->restart_block;\n\trestart->fn = futex_wait_restart;\n\trestart->futex.uaddr = uaddr;\n\trestart->futex.val = val;\n\trestart->futex.time = *abs_time;\n\trestart->futex.bitset = bitset;\n\trestart->futex.flags = flags | FLAGS_HAS_TIMEOUT;\n\n\tret = -ERESTART_RESTARTBLOCK;\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}\n\n\nstatic long futex_wait_restart(struct restart_block *restart)\n{\n\tu32 __user *uaddr = restart->futex.uaddr;\n\tktime_t t, *tp = NULL;\n\n\tif (restart->futex.flags & FLAGS_HAS_TIMEOUT) {\n\t\tt = restart->futex.time;\n\t\ttp = &t;\n\t}\n\trestart->fn = do_no_restart_syscall;\n\n\treturn (long)futex_wait(uaddr, restart->futex.flags,\n\t\t\t\trestart->futex.val, tp, restart->futex.bitset);\n}\n\n\n/*\n * Userspace tried a 0 -> TID atomic transition of the futex value\n * and failed. The kernel side here does the whole locking operation:\n * if there are waiters then it will block as a consequence of relying\n * on rt-mutexes, it does PI, etc. (Due to races the kernel might see\n * a 0 value of the futex too.).\n *\n * Also serves as futex trylock_pi()'ing, and due semantics.\n */\nstatic int futex_lock_pi(u32 __user *uaddr, unsigned int flags,\n\t\t\t ktime_t *time, int trylock)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\n\tif (refill_pi_state_cache())\n\t\treturn -ENOMEM;\n\n\tif (time) {\n\t\tto = &timeout;\n\t\thrtimer_init_on_stack(&to->timer, CLOCK_REALTIME,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires(&to->timer, *time);\n\t}\n\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q.key, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\nretry_private:\n\thb = queue_lock(&q);\n\n\tret = futex_lock_pi_atomic(uaddr, hb, &q.key, &q.pi_state, current, 0);\n\tif (unlikely(ret)) {\n\t\t/*\n\t\t * Atomic work succeeded and we got the lock,\n\t\t * or failed. Either way, we do _not_ block.\n\t\t */\n\t\tswitch (ret) {\n\t\tcase 1:\n\t\t\t/* We got the lock. */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock_put_key;\n\t\tcase -EFAULT:\n\t\t\tgoto uaddr_faulted;\n\t\tcase -EAGAIN:\n\t\t\t/*\n\t\t\t * Two reasons for this:\n\t\t\t * - Task is exiting and we just wait for the\n\t\t\t *   exit to complete.\n\t\t\t * - The user space value changed.\n\t\t\t */\n\t\t\tqueue_unlock(hb);\n\t\t\tput_futex_key(&q.key);\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\t\tdefault:\n\t\t\tgoto out_unlock_put_key;\n\t\t}\n\t}\n\n\tWARN_ON(!q.pi_state);\n\n\t/*\n\t * Only actually queue now that the atomic ops are done:\n\t */\n\t__queue_me(&q, hb);\n\n\tif (trylock) {\n\t\tret = rt_mutex_futex_trylock(&q.pi_state->pi_mutex);\n\t\t/* Fixup the trylock return value: */\n\t\tret = ret ? 0 : -EWOULDBLOCK;\n\t\tgoto no_block;\n\t}\n\n\trt_mutex_init_waiter(&rt_waiter);\n\n\t/*\n\t * On PREEMPT_RT_FULL, when hb->lock becomes an rt_mutex, we must not\n\t * hold it while doing rt_mutex_start_proxy(), because then it will\n\t * include hb->lock in the blocking chain, even through we'll not in\n\t * fact hold it while blocking. This will lead it to report -EDEADLK\n\t * and BUG when futex_unlock_pi() interleaves with this.\n\t *\n\t * Therefore acquire wait_lock while holding hb->lock, but drop the\n\t * latter before calling rt_mutex_start_proxy_lock(). This still fully\n\t * serializes against futex_unlock_pi() as that does the exact same\n\t * lock handoff sequence.\n\t */\n\traw_spin_lock_irq(&q.pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q.lock_ptr);\n\tret = __rt_mutex_start_proxy_lock(&q.pi_state->pi_mutex, &rt_waiter, current);\n\traw_spin_unlock_irq(&q.pi_state->pi_mutex.wait_lock);\n\n\tif (ret) {\n\t\tif (ret == 1)\n\t\t\tret = 0;\n\n\t\tspin_lock(q.lock_ptr);\n\t\tgoto no_block;\n\t}\n\n\n\tif (unlikely(to))\n\t\thrtimer_start_expires(&to->timer, HRTIMER_MODE_ABS);\n\n\tret = rt_mutex_wait_proxy_lock(&q.pi_state->pi_mutex, to, &rt_waiter);\n\n\tspin_lock(q.lock_ptr);\n\t/*\n\t * If we failed to acquire the lock (signal/timeout), we must\n\t * first acquire the hb->lock before removing the lock from the\n\t * rt_mutex waitqueue, such that we can keep the hb and rt_mutex\n\t * wait lists consistent.\n\t *\n\t * In particular; it is important that futex_unlock_pi() can not\n\t * observe this inconsistency.\n\t */\n\tif (ret && !rt_mutex_cleanup_proxy_lock(&q.pi_state->pi_mutex, &rt_waiter))\n\t\tret = 0;\n\nno_block:\n\t/*\n\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t * haven't already.\n\t */\n\tres = fixup_owner(uaddr, &q, !ret);\n\t/*\n\t * If fixup_owner() returned an error, proprogate that.  If it acquired\n\t * the lock, clear our -ETIMEDOUT or -EINTR.\n\t */\n\tif (res)\n\t\tret = (res < 0) ? res : 0;\n\n\t/*\n\t * If fixup_owner() faulted and was unable to handle the fault, unlock\n\t * it and return the fault to userspace.\n\t */\n\tif (ret && (rt_mutex_owner(&q.pi_state->pi_mutex) == current)) {\n\t\tpi_state = q.pi_state;\n\t\tget_pi_state(pi_state);\n\t}\n\n\t/* Unqueue and drop the lock */\n\tunqueue_me_pi(&q);\n\n\tif (pi_state) {\n\t\trt_mutex_futex_unlock(&pi_state->pi_mutex);\n\t\tput_pi_state(pi_state);\n\t}\n\n\tgoto out_put_key;\n\nout_unlock_put_key:\n\tqueue_unlock(hb);\n\nout_put_key:\n\tput_futex_key(&q.key);\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret != -EINTR ? ret : -ERESTARTNOINTR;\n\nuaddr_faulted:\n\tqueue_unlock(hb);\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (ret)\n\t\tgoto out_put_key;\n\n\tif (!(flags & FLAGS_SHARED))\n\t\tgoto retry_private;\n\n\tput_futex_key(&q.key);\n\tgoto retry;\n}\n\n/*\n * Userspace attempted a TID -> 0 atomic transition, and failed.\n * This is the in-kernel slowpath: we look up the PI state (if any),\n * and do the rt-mutex unlock.\n */\nstatic int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)\n{\n\tu32 uninitialized_var(curval), uval, vpid = task_pid_vnr(current);\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *top_waiter;\n\tint ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -EFAULT;\n\t/*\n\t * We release only a lock we actually own:\n\t */\n\tif ((uval & FUTEX_TID_MASK) != vpid)\n\t\treturn -EPERM;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_WRITE);\n\tif (ret)\n\t\treturn ret;\n\n\thb = hash_futex(&key);\n\tspin_lock(&hb->lock);\n\n\t/*\n\t * Check waiters first. We do not trust user space values at\n\t * all and we at least want to know if user space fiddled\n\t * with the futex value instead of blindly unlocking.\n\t */\n\ttop_waiter = futex_top_waiter(hb, &key);\n\tif (top_waiter) {\n\t\tstruct futex_pi_state *pi_state = top_waiter->pi_state;\n\n\t\tret = -EINVAL;\n\t\tif (!pi_state)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If current does not own the pi_state then the futex is\n\t\t * inconsistent and user space fiddled with the futex value.\n\t\t */\n\t\tif (pi_state->owner != current)\n\t\t\tgoto out_unlock;\n\n\t\tget_pi_state(pi_state);\n\t\t/*\n\t\t * By taking wait_lock while still holding hb->lock, we ensure\n\t\t * there is no point where we hold neither; and therefore\n\t\t * wake_futex_pi() must observe a state consistent with what we\n\t\t * observed.\n\t\t */\n\t\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\t\tspin_unlock(&hb->lock);\n\n\t\t/* drops pi_state->pi_mutex.wait_lock */\n\t\tret = wake_futex_pi(uaddr, uval, pi_state);\n\n\t\tput_pi_state(pi_state);\n\n\t\t/*\n\t\t * Success, we're done! No tricky corner cases.\n\t\t */\n\t\tif (!ret)\n\t\t\tgoto out_putkey;\n\t\t/*\n\t\t * The atomic access to the futex value generated a\n\t\t * pagefault, so retry the user-access and the wakeup:\n\t\t */\n\t\tif (ret == -EFAULT)\n\t\t\tgoto pi_faulted;\n\t\t/*\n\t\t * A unconditional UNLOCK_PI op raced against a waiter\n\t\t * setting the FUTEX_WAITERS bit. Try again.\n\t\t */\n\t\tif (ret == -EAGAIN) {\n\t\t\tput_futex_key(&key);\n\t\t\tgoto retry;\n\t\t}\n\t\t/*\n\t\t * wake_futex_pi has detected invalid state. Tell user\n\t\t * space.\n\t\t */\n\t\tgoto out_putkey;\n\t}\n\n\t/*\n\t * We have no kernel internal state, i.e. no waiters in the\n\t * kernel. Waiters which are about to queue themselves are stuck\n\t * on hb->lock. So we can safely ignore them. We do neither\n\t * preserve the WAITERS bit not the OWNER_DIED one. We are the\n\t * owner.\n\t */\n\tif (cmpxchg_futex_value_locked(&curval, uaddr, uval, 0)) {\n\t\tspin_unlock(&hb->lock);\n\t\tgoto pi_faulted;\n\t}\n\n\t/*\n\t * If uval has changed, let user space handle it.\n\t */\n\tret = (curval == uval) ? 0 : -EAGAIN;\n\nout_unlock:\n\tspin_unlock(&hb->lock);\nout_putkey:\n\tput_futex_key(&key);\n\treturn ret;\n\npi_faulted:\n\tput_futex_key(&key);\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (!ret)\n\t\tgoto retry;\n\n\treturn ret;\n}\n\n/**\n * handle_early_requeue_pi_wakeup() - Detect early wakeup on the initial futex\n * @hb:\t\tthe hash_bucket futex_q was original enqueued on\n * @q:\t\tthe futex_q woken while waiting to be requeued\n * @key2:\tthe futex_key of the requeue target futex\n * @timeout:\tthe timeout associated with the wait (NULL if none)\n *\n * Detect if the task was woken on the initial futex as opposed to the requeue\n * target futex.  If so, determine if it was a timeout or a signal that caused\n * the wakeup and return the appropriate error code to the caller.  Must be\n * called with the hb lock held.\n *\n * Return:\n *  -  0 = no early wakeup detected;\n *  - <0 = -ETIMEDOUT or -ERESTARTNOINTR\n */\nstatic inline\nint handle_early_requeue_pi_wakeup(struct futex_hash_bucket *hb,\n\t\t\t\t   struct futex_q *q, union futex_key *key2,\n\t\t\t\t   struct hrtimer_sleeper *timeout)\n{\n\tint ret = 0;\n\n\t/*\n\t * With the hb lock held, we avoid races while we process the wakeup.\n\t * We only need to hold hb (and not hb2) to ensure atomicity as the\n\t * wakeup code can't change q.key from uaddr to uaddr2 if we hold hb.\n\t * It can't be requeued from uaddr2 to something else since we don't\n\t * support a PI aware source futex for requeue.\n\t */\n\tif (!match_futex(&q->key, key2)) {\n\t\tWARN_ON(q->lock_ptr && (&hb->lock != q->lock_ptr));\n\t\t/*\n\t\t * We were woken prior to requeue by a timeout or a signal.\n\t\t * Unqueue the futex_q and determine which it was.\n\t\t */\n\t\tplist_del(&q->list, &hb->chain);\n\t\thb_waiters_dec(hb);\n\n\t\t/* Handle spurious wakeups gracefully */\n\t\tret = -EWOULDBLOCK;\n\t\tif (timeout && !timeout->task)\n\t\t\tret = -ETIMEDOUT;\n\t\telse if (signal_pending(current))\n\t\t\tret = -ERESTARTNOINTR;\n\t}\n\treturn ret;\n}\n\n/**\n * futex_wait_requeue_pi() - Wait on uaddr and take uaddr2\n * @uaddr:\tthe futex we initially wait on (non-pi)\n * @flags:\tfutex flags (FLAGS_SHARED, FLAGS_CLOCKRT, etc.), they must be\n *\t\tthe same type, no requeueing from private to shared, etc.\n * @val:\tthe expected value of uaddr\n * @abs_time:\tabsolute timeout\n * @bitset:\t32 bit wakeup bitset set by userspace, defaults to all\n * @uaddr2:\tthe pi futex we will take prior to returning to user-space\n *\n * The caller will wait on uaddr and will be requeued by futex_requeue() to\n * uaddr2 which must be PI aware and unique from uaddr.  Normal wakeup will wake\n * on uaddr2 and complete the acquisition of the rt_mutex prior to returning to\n * userspace.  This ensures the rt_mutex maintains an owner when it has waiters;\n * without one, the pi logic would not know which task to boost/deboost, if\n * there was a need to.\n *\n * We call schedule in futex_wait_queue_me() when we enqueue and return there\n * via the following--\n * 1) wakeup on uaddr2 after an atomic lock acquisition by futex_requeue()\n * 2) wakeup on uaddr2 after a requeue\n * 3) signal\n * 4) timeout\n *\n * If 3, cleanup and return -ERESTARTNOINTR.\n *\n * If 2, we may then block on trying to take the rt_mutex and return via:\n * 5) successful lock\n * 6) signal\n * 7) timeout\n * 8) other lock acquisition failure\n *\n * If 6, return -EWOULDBLOCK (restarting the syscall would do the same).\n *\n * If 4 or 7, we cleanup and return with -ETIMEDOUT.\n *\n * Return:\n *  -  0 - On success;\n *  - <0 - On error\n */\nstatic int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,\n\t\t\t\t u32 val, ktime_t *abs_time, u32 bitset,\n\t\t\t\t u32 __user *uaddr2)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key2 = FUTEX_KEY_INIT;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\n\tif (uaddr == uaddr2)\n\t\treturn -EINVAL;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\n\t/*\n\t * The waiter is allocated on our stack, manipulated by the requeue\n\t * code while we sleep on uaddr.\n\t */\n\trt_mutex_init_waiter(&rt_waiter);\n\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\tq.bitset = bitset;\n\tq.rt_waiter = &rt_waiter;\n\tq.requeue_pi_key = &key2;\n\n\t/*\n\t * Prepare to wait on uaddr. On success, increments q.key (key1) ref\n\t * count.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out_key2;\n\n\t/*\n\t * The check above which compares uaddrs is not sufficient for\n\t * shared futexes. We need to compare the keys:\n\t */\n\tif (match_futex(&q.key, &key2)) {\n\t\tqueue_unlock(hb);\n\t\tret = -EINVAL;\n\t\tgoto out_put_keys;\n\t}\n\n\t/* Queue the futex_q, drop the hb lock, wait for wakeup. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\tspin_lock(&hb->lock);\n\tret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);\n\tspin_unlock(&hb->lock);\n\tif (ret)\n\t\tgoto out_put_keys;\n\n\t/*\n\t * In order for us to be here, we know our q.key == key2, and since\n\t * we took the hb->lock above, we also know that futex_requeue() has\n\t * completed and we no longer have to concern ourselves with a wakeup\n\t * race with the atomic proxy lock acquisition by the requeue code. The\n\t * futex_requeue dropped our key1 reference and incremented our key2\n\t * reference count.\n\t */\n\n\t/* Check if the requeue code acquired the second futex for us. */\n\tif (!q.rt_waiter) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case.\n\t\t */\n\t\tif (q.pi_state && (q.pi_state->owner != current)) {\n\t\t\tspin_lock(q.lock_ptr);\n\t\t\tret = fixup_pi_state_owner(uaddr2, &q, current);\n\t\t\tif (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {\n\t\t\t\tpi_state = q.pi_state;\n\t\t\t\tget_pi_state(pi_state);\n\t\t\t}\n\t\t\t/*\n\t\t\t * Drop the reference to the pi state which\n\t\t\t * the requeue_pi() code acquired for us.\n\t\t\t */\n\t\t\tput_pi_state(q.pi_state);\n\t\t\tspin_unlock(q.lock_ptr);\n\t\t}\n\t} else {\n\t\tstruct rt_mutex *pi_mutex;\n\n\t\t/*\n\t\t * We have been woken up by futex_unlock_pi(), a timeout, or a\n\t\t * signal.  futex_unlock_pi() will not destroy the lock_ptr nor\n\t\t * the pi_state.\n\t\t */\n\t\tWARN_ON(!q.pi_state);\n\t\tpi_mutex = &q.pi_state->pi_mutex;\n\t\tret = rt_mutex_wait_proxy_lock(pi_mutex, to, &rt_waiter);\n\n\t\tspin_lock(q.lock_ptr);\n\t\tif (ret && !rt_mutex_cleanup_proxy_lock(pi_mutex, &rt_waiter))\n\t\t\tret = 0;\n\n\t\tdebug_rt_mutex_free_waiter(&rt_waiter);\n\t\t/*\n\t\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t\t * haven't already.\n\t\t */\n\t\tres = fixup_owner(uaddr2, &q, !ret);\n\t\t/*\n\t\t * If fixup_owner() returned an error, proprogate that.  If it\n\t\t * acquired the lock, clear -ETIMEDOUT or -EINTR.\n\t\t */\n\t\tif (res)\n\t\t\tret = (res < 0) ? res : 0;\n\n\t\t/*\n\t\t * If fixup_pi_state_owner() faulted and was unable to handle\n\t\t * the fault, unlock the rt_mutex and return the fault to\n\t\t * userspace.\n\t\t */\n\t\tif (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {\n\t\t\tpi_state = q.pi_state;\n\t\t\tget_pi_state(pi_state);\n\t\t}\n\n\t\t/* Unqueue and drop the lock. */\n\t\tunqueue_me_pi(&q);\n\t}\n\n\tif (pi_state) {\n\t\trt_mutex_futex_unlock(&pi_state->pi_mutex);\n\t\tput_pi_state(pi_state);\n\t}\n\n\tif (ret == -EINTR) {\n\t\t/*\n\t\t * We've already been requeued, but cannot restart by calling\n\t\t * futex_lock_pi() directly. We could restart this syscall, but\n\t\t * it would detect that the user space \"val\" changed and return\n\t\t * -EWOULDBLOCK.  Save the overhead of the restart and return\n\t\t * -EWOULDBLOCK directly.\n\t\t */\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout_put_keys:\n\tput_futex_key(&q.key);\nout_key2:\n\tput_futex_key(&key2);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}\n\n/*\n * Support for robust futexes: the kernel cleans up held futexes at\n * thread exit time.\n *\n * Implementation: user-space maintains a per-thread list of locks it\n * is holding. Upon do_exit(), the kernel carefully walks this list,\n * and marks all locks that are owned by this thread with the\n * FUTEX_OWNER_DIED bit, and wakes up a waiter (if any). The list is\n * always manipulated with the lock held, so the list is private and\n * per-thread. Userspace also maintains a per-thread 'list_op_pending'\n * field, to allow the kernel to clean up if the thread dies after\n * acquiring the lock, but just before it could have added itself to\n * the list. There can only be one such pending lock.\n */\n\n/**\n * sys_set_robust_list() - Set the robust-futex list head of a task\n * @head:\tpointer to the list-head\n * @len:\tlength of the list-head, as userspace expects\n */\nSYSCALL_DEFINE2(set_robust_list, struct robust_list_head __user *, head,\n\t\tsize_t, len)\n{\n\tif (!futex_cmpxchg_enabled)\n\t\treturn -ENOSYS;\n\t/*\n\t * The kernel knows only one size for now:\n\t */\n\tif (unlikely(len != sizeof(*head)))\n\t\treturn -EINVAL;\n\n\tcurrent->robust_list = head;\n\n\treturn 0;\n}\n\n/**\n * sys_get_robust_list() - Get the robust-futex list head of a task\n * @pid:\tpid of the process [zero for current task]\n * @head_ptr:\tpointer to a list-head pointer, the kernel fills it in\n * @len_ptr:\tpointer to a length field, the kernel fills in the header size\n */\nSYSCALL_DEFINE3(get_robust_list, int, pid,\n\t\tstruct robust_list_head __user * __user *, head_ptr,\n\t\tsize_t __user *, len_ptr)\n{\n\tstruct robust_list_head __user *head;\n\tunsigned long ret;\n\tstruct task_struct *p;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn -ENOSYS;\n\n\trcu_read_lock();\n\n\tret = -ESRCH;\n\tif (!pid)\n\t\tp = current;\n\telse {\n\t\tp = find_task_by_vpid(pid);\n\t\tif (!p)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = -EPERM;\n\tif (!ptrace_may_access(p, PTRACE_MODE_READ_REALCREDS))\n\t\tgoto err_unlock;\n\n\thead = p->robust_list;\n\trcu_read_unlock();\n\n\tif (put_user(sizeof(*head), len_ptr))\n\t\treturn -EFAULT;\n\treturn put_user(head, head_ptr);\n\nerr_unlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * Process a futex-list entry, check whether it's owned by the\n * dying task, and do notification if so:\n */\nint handle_futex_death(u32 __user *uaddr, struct task_struct *curr, int pi)\n{\n\tu32 uval, uninitialized_var(nval), mval;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -1;\n\n\tif ((uval & FUTEX_TID_MASK) == task_pid_vnr(curr)) {\n\t\t/*\n\t\t * Ok, this dying thread is truly holding a futex\n\t\t * of interest. Set the OWNER_DIED bit atomically\n\t\t * via cmpxchg, and if the value had FUTEX_WAITERS\n\t\t * set, wake up a waiter (if any). (We have to do a\n\t\t * futex_wake() even if OWNER_DIED is already set -\n\t\t * to handle the rare but possible case of recursive\n\t\t * thread-death.) The rest of the cleanup is done in\n\t\t * userspace.\n\t\t */\n\t\tmval = (uval & FUTEX_WAITERS) | FUTEX_OWNER_DIED;\n\t\t/*\n\t\t * We are not holding a lock here, but we want to have\n\t\t * the pagefault_disable/enable() protection because\n\t\t * we want to handle the fault gracefully. If the\n\t\t * access fails we try to fault in the futex with R/W\n\t\t * verification via get_user_pages. get_user() above\n\t\t * does not guarantee R/W access. If that fails we\n\t\t * give up and leave the futex locked.\n\t\t */\n\t\tif (cmpxchg_futex_value_locked(&nval, uaddr, uval, mval)) {\n\t\t\tif (fault_in_user_writeable(uaddr))\n\t\t\t\treturn -1;\n\t\t\tgoto retry;\n\t\t}\n\t\tif (nval != uval)\n\t\t\tgoto retry;\n\n\t\t/*\n\t\t * Wake robust non-PI futexes here. The wakeup of\n\t\t * PI futexes happens in exit_pi_state():\n\t\t */\n\t\tif (!pi && (uval & FUTEX_WAITERS))\n\t\t\tfutex_wake(uaddr, 1, 1, FUTEX_BITSET_MATCH_ANY);\n\t}\n\treturn 0;\n}\n\n/*\n * Fetch a robust-list pointer. Bit 0 signals PI futexes:\n */\nstatic inline int fetch_robust_entry(struct robust_list __user **entry,\n\t\t\t\t     struct robust_list __user * __user *head,\n\t\t\t\t     unsigned int *pi)\n{\n\tunsigned long uentry;\n\n\tif (get_user(uentry, (unsigned long __user *)head))\n\t\treturn -EFAULT;\n\n\t*entry = (void __user *)(uentry & ~1UL);\n\t*pi = uentry & 1;\n\n\treturn 0;\n}\n\n/*\n * Walk curr->robust_list (very carefully, it's a userspace list!)\n * and mark any locks found there dead, and notify any waiters.\n *\n * We silently return on any sign of list-walking problem.\n */\nvoid exit_robust_list(struct task_struct *curr)\n{\n\tstruct robust_list_head __user *head = curr->robust_list;\n\tstruct robust_list __user *entry, *next_entry, *pending;\n\tunsigned int limit = ROBUST_LIST_LIMIT, pi, pip;\n\tunsigned int uninitialized_var(next_pi);\n\tunsigned long futex_offset;\n\tint rc;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn;\n\n\t/*\n\t * Fetch the list head (which was registered earlier, via\n\t * sys_set_robust_list()):\n\t */\n\tif (fetch_robust_entry(&entry, &head->list.next, &pi))\n\t\treturn;\n\t/*\n\t * Fetch the relative futex offset:\n\t */\n\tif (get_user(futex_offset, &head->futex_offset))\n\t\treturn;\n\t/*\n\t * Fetch any possibly pending lock-add first, and handle it\n\t * if it exists:\n\t */\n\tif (fetch_robust_entry(&pending, &head->list_op_pending, &pip))\n\t\treturn;\n\n\tnext_entry = NULL;\t/* avoid warning with gcc */\n\twhile (entry != &head->list) {\n\t\t/*\n\t\t * Fetch the next entry in the list before calling\n\t\t * handle_futex_death:\n\t\t */\n\t\trc = fetch_robust_entry(&next_entry, &entry->next, &next_pi);\n\t\t/*\n\t\t * A pending lock might already be on the list, so\n\t\t * don't process it twice:\n\t\t */\n\t\tif (entry != pending)\n\t\t\tif (handle_futex_death((void __user *)entry + futex_offset,\n\t\t\t\t\t\tcurr, pi))\n\t\t\t\treturn;\n\t\tif (rc)\n\t\t\treturn;\n\t\tentry = next_entry;\n\t\tpi = next_pi;\n\t\t/*\n\t\t * Avoid excessively long or circular lists:\n\t\t */\n\t\tif (!--limit)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (pending)\n\t\thandle_futex_death((void __user *)pending + futex_offset,\n\t\t\t\t   curr, pip);\n}\n\nlong do_futex(u32 __user *uaddr, int op, u32 val, ktime_t *timeout,\n\t\tu32 __user *uaddr2, u32 val2, u32 val3)\n{\n\tint cmd = op & FUTEX_CMD_MASK;\n\tunsigned int flags = 0;\n\n\tif (!(op & FUTEX_PRIVATE_FLAG))\n\t\tflags |= FLAGS_SHARED;\n\n\tif (op & FUTEX_CLOCK_REALTIME) {\n\t\tflags |= FLAGS_CLOCKRT;\n\t\tif (cmd != FUTEX_WAIT && cmd != FUTEX_WAIT_BITSET && \\\n\t\t    cmd != FUTEX_WAIT_REQUEUE_PI)\n\t\t\treturn -ENOSYS;\n\t}\n\n\tswitch (cmd) {\n\tcase FUTEX_LOCK_PI:\n\tcase FUTEX_UNLOCK_PI:\n\tcase FUTEX_TRYLOCK_PI:\n\tcase FUTEX_WAIT_REQUEUE_PI:\n\tcase FUTEX_CMP_REQUEUE_PI:\n\t\tif (!futex_cmpxchg_enabled)\n\t\t\treturn -ENOSYS;\n\t}\n\n\tswitch (cmd) {\n\tcase FUTEX_WAIT:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\tcase FUTEX_WAIT_BITSET:\n\t\treturn futex_wait(uaddr, flags, val, timeout, val3);\n\tcase FUTEX_WAKE:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\tcase FUTEX_WAKE_BITSET:\n\t\treturn futex_wake(uaddr, flags, val, val3);\n\tcase FUTEX_REQUEUE:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, NULL, 0);\n\tcase FUTEX_CMP_REQUEUE:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 0);\n\tcase FUTEX_WAKE_OP:\n\t\treturn futex_wake_op(uaddr, flags, uaddr2, val, val2, val3);\n\tcase FUTEX_LOCK_PI:\n\t\treturn futex_lock_pi(uaddr, flags, timeout, 0);\n\tcase FUTEX_UNLOCK_PI:\n\t\treturn futex_unlock_pi(uaddr, flags);\n\tcase FUTEX_TRYLOCK_PI:\n\t\treturn futex_lock_pi(uaddr, flags, NULL, 1);\n\tcase FUTEX_WAIT_REQUEUE_PI:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\t\treturn futex_wait_requeue_pi(uaddr, flags, val, timeout, val3,\n\t\t\t\t\t     uaddr2);\n\tcase FUTEX_CMP_REQUEUE_PI:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 1);\n\t}\n\treturn -ENOSYS;\n}\n\n\nSYSCALL_DEFINE6(futex, u32 __user *, uaddr, int, op, u32, val,\n\t\tstruct timespec __user *, utime, u32 __user *, uaddr2,\n\t\tu32, val3)\n{\n\tstruct timespec ts;\n\tktime_t t, *tp = NULL;\n\tu32 val2 = 0;\n\tint cmd = op & FUTEX_CMD_MASK;\n\n\tif (utime && (cmd == FUTEX_WAIT || cmd == FUTEX_LOCK_PI ||\n\t\t      cmd == FUTEX_WAIT_BITSET ||\n\t\t      cmd == FUTEX_WAIT_REQUEUE_PI)) {\n\t\tif (unlikely(should_fail_futex(!(op & FUTEX_PRIVATE_FLAG))))\n\t\t\treturn -EFAULT;\n\t\tif (copy_from_user(&ts, utime, sizeof(ts)) != 0)\n\t\t\treturn -EFAULT;\n\t\tif (!timespec_valid(&ts))\n\t\t\treturn -EINVAL;\n\n\t\tt = timespec_to_ktime(ts);\n\t\tif (cmd == FUTEX_WAIT)\n\t\t\tt = ktime_add_safe(ktime_get(), t);\n\t\ttp = &t;\n\t}\n\t/*\n\t * requeue parameter in 'utime' if cmd == FUTEX_*_REQUEUE_*.\n\t * number of waiters to wake in 'utime' if cmd == FUTEX_WAKE_OP.\n\t */\n\tif (cmd == FUTEX_REQUEUE || cmd == FUTEX_CMP_REQUEUE ||\n\t    cmd == FUTEX_CMP_REQUEUE_PI || cmd == FUTEX_WAKE_OP)\n\t\tval2 = (u32) (unsigned long) utime;\n\n\treturn do_futex(uaddr, op, val, tp, uaddr2, val2, val3);\n}\n\nstatic void __init futex_detect_cmpxchg(void)\n{\n#ifndef CONFIG_HAVE_FUTEX_CMPXCHG\n\tu32 curval;\n\n\t/*\n\t * This will fail and we want it. Some arch implementations do\n\t * runtime detection of the futex_atomic_cmpxchg_inatomic()\n\t * functionality. We want to know that before we call in any\n\t * of the complex code paths. Also we want to prevent\n\t * registration of robust lists in that case. NULL is\n\t * guaranteed to fault and we get -EFAULT on functional\n\t * implementation, the non-functional ones will return\n\t * -ENOSYS.\n\t */\n\tif (cmpxchg_futex_value_locked(&curval, NULL, 0, 0) == -EFAULT)\n\t\tfutex_cmpxchg_enabled = 1;\n#endif\n}\n\nstatic int __init futex_init(void)\n{\n\tunsigned int futex_shift;\n\tunsigned long i;\n\n#if CONFIG_BASE_SMALL\n\tfutex_hashsize = 16;\n#else\n\tfutex_hashsize = roundup_pow_of_two(256 * num_possible_cpus());\n#endif\n\n\tfutex_queues = alloc_large_system_hash(\"futex\", sizeof(*futex_queues),\n\t\t\t\t\t       futex_hashsize, 0,\n\t\t\t\t\t       futex_hashsize < 256 ? HASH_SMALL : 0,\n\t\t\t\t\t       &futex_shift, NULL,\n\t\t\t\t\t       futex_hashsize, futex_hashsize);\n\tfutex_hashsize = 1UL << futex_shift;\n\n\tfutex_detect_cmpxchg();\n\n\tfor (i = 0; i < futex_hashsize; i++) {\n\t\tatomic_set(&futex_queues[i].waiters, 0);\n\t\tplist_head_init(&futex_queues[i].chain);\n\t\tspin_lock_init(&futex_queues[i].lock);\n\t}\n\n\treturn 0;\n}\ncore_initcall(futex_init);\n"], "fixing_code": ["/*\n *  Fast Userspace Mutexes (which I call \"Futexes!\").\n *  (C) Rusty Russell, IBM 2002\n *\n *  Generalized futexes, futex requeueing, misc fixes by Ingo Molnar\n *  (C) Copyright 2003 Red Hat Inc, All Rights Reserved\n *\n *  Removed page pinning, fix privately mapped COW pages and other cleanups\n *  (C) Copyright 2003, 2004 Jamie Lokier\n *\n *  Robust futex support started by Ingo Molnar\n *  (C) Copyright 2006 Red Hat Inc, All Rights Reserved\n *  Thanks to Thomas Gleixner for suggestions, analysis and fixes.\n *\n *  PI-futex support started by Ingo Molnar and Thomas Gleixner\n *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>\n *  Copyright (C) 2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>\n *\n *  PRIVATE futexes by Eric Dumazet\n *  Copyright (C) 2007 Eric Dumazet <dada1@cosmosbay.com>\n *\n *  Requeue-PI support by Darren Hart <dvhltc@us.ibm.com>\n *  Copyright (C) IBM Corporation, 2009\n *  Thanks to Thomas Gleixner for conceptual design and careful reviews.\n *\n *  Thanks to Ben LaHaise for yelling \"hashed waitqueues\" loudly\n *  enough at me, Linus for the original (flawed) idea, Matthew\n *  Kirkwood for proof-of-concept implementation.\n *\n *  \"The futexes are also cursed.\"\n *  \"But they come in a choice of three flavours!\"\n *\n *  This program is free software; you can redistribute it and/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; either version 2 of the License, or\n *  (at your option) any later version.\n *\n *  This program is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *  GNU General Public License for more details.\n *\n *  You should have received a copy of the GNU General Public License\n *  along with this program; if not, write to the Free Software\n *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n */\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/jhash.h>\n#include <linux/init.h>\n#include <linux/futex.h>\n#include <linux/mount.h>\n#include <linux/pagemap.h>\n#include <linux/syscalls.h>\n#include <linux/signal.h>\n#include <linux/export.h>\n#include <linux/magic.h>\n#include <linux/pid.h>\n#include <linux/nsproxy.h>\n#include <linux/ptrace.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/mm.h>\n#include <linux/hugetlb.h>\n#include <linux/freezer.h>\n#include <linux/bootmem.h>\n#include <linux/fault-inject.h>\n\n#include <asm/futex.h>\n\n#include \"locking/rtmutex_common.h\"\n\n/*\n * READ this before attempting to hack on futexes!\n *\n * Basic futex operation and ordering guarantees\n * =============================================\n *\n * The waiter reads the futex value in user space and calls\n * futex_wait(). This function computes the hash bucket and acquires\n * the hash bucket lock. After that it reads the futex user space value\n * again and verifies that the data has not changed. If it has not changed\n * it enqueues itself into the hash bucket, releases the hash bucket lock\n * and schedules.\n *\n * The waker side modifies the user space value of the futex and calls\n * futex_wake(). This function computes the hash bucket and acquires the\n * hash bucket lock. Then it looks for waiters on that futex in the hash\n * bucket and wakes them.\n *\n * In futex wake up scenarios where no tasks are blocked on a futex, taking\n * the hb spinlock can be avoided and simply return. In order for this\n * optimization to work, ordering guarantees must exist so that the waiter\n * being added to the list is acknowledged when the list is concurrently being\n * checked by the waker, avoiding scenarios like the following:\n *\n * CPU 0                               CPU 1\n * val = *futex;\n * sys_futex(WAIT, futex, val);\n *   futex_wait(futex, val);\n *   uval = *futex;\n *                                     *futex = newval;\n *                                     sys_futex(WAKE, futex);\n *                                       futex_wake(futex);\n *                                       if (queue_empty())\n *                                         return;\n *   if (uval == val)\n *      lock(hash_bucket(futex));\n *      queue();\n *     unlock(hash_bucket(futex));\n *     schedule();\n *\n * This would cause the waiter on CPU 0 to wait forever because it\n * missed the transition of the user space value from val to newval\n * and the waker did not find the waiter in the hash bucket queue.\n *\n * The correct serialization ensures that a waiter either observes\n * the changed user space value before blocking or is woken by a\n * concurrent waker:\n *\n * CPU 0                                 CPU 1\n * val = *futex;\n * sys_futex(WAIT, futex, val);\n *   futex_wait(futex, val);\n *\n *   waiters++; (a)\n *   smp_mb(); (A) <-- paired with -.\n *                                  |\n *   lock(hash_bucket(futex));      |\n *                                  |\n *   uval = *futex;                 |\n *                                  |        *futex = newval;\n *                                  |        sys_futex(WAKE, futex);\n *                                  |          futex_wake(futex);\n *                                  |\n *                                  `--------> smp_mb(); (B)\n *   if (uval == val)\n *     queue();\n *     unlock(hash_bucket(futex));\n *     schedule();                         if (waiters)\n *                                           lock(hash_bucket(futex));\n *   else                                    wake_waiters(futex);\n *     waiters--; (b)                        unlock(hash_bucket(futex));\n *\n * Where (A) orders the waiters increment and the futex value read through\n * atomic operations (see hb_waiters_inc) and where (B) orders the write\n * to futex and the waiters read -- this is done by the barriers for both\n * shared and private futexes in get_futex_key_refs().\n *\n * This yields the following case (where X:=waiters, Y:=futex):\n *\n *\tX = Y = 0\n *\n *\tw[X]=1\t\tw[Y]=1\n *\tMB\t\tMB\n *\tr[Y]=y\t\tr[X]=x\n *\n * Which guarantees that x==0 && y==0 is impossible; which translates back into\n * the guarantee that we cannot both miss the futex variable change and the\n * enqueue.\n *\n * Note that a new waiter is accounted for in (a) even when it is possible that\n * the wait call can return error, in which case we backtrack from it in (b).\n * Refer to the comment in queue_lock().\n *\n * Similarly, in order to account for waiters being requeued on another\n * address we always increment the waiters for the destination bucket before\n * acquiring the lock. It then decrements them again  after releasing it -\n * the code that actually moves the futex(es) between hash buckets (requeue_futex)\n * will do the additional required waiter count housekeeping. This is done for\n * double_lock_hb() and double_unlock_hb(), respectively.\n */\n\n#ifndef CONFIG_HAVE_FUTEX_CMPXCHG\nint __read_mostly futex_cmpxchg_enabled;\n#endif\n\n/*\n * Futex flags used to encode options to functions and preserve them across\n * restarts.\n */\n#ifdef CONFIG_MMU\n# define FLAGS_SHARED\t\t0x01\n#else\n/*\n * NOMMU does not have per process address space. Let the compiler optimize\n * code away.\n */\n# define FLAGS_SHARED\t\t0x00\n#endif\n#define FLAGS_CLOCKRT\t\t0x02\n#define FLAGS_HAS_TIMEOUT\t0x04\n\n/*\n * Priority Inheritance state:\n */\nstruct futex_pi_state {\n\t/*\n\t * list of 'owned' pi_state instances - these have to be\n\t * cleaned up in do_exit() if the task exits prematurely:\n\t */\n\tstruct list_head list;\n\n\t/*\n\t * The PI object:\n\t */\n\tstruct rt_mutex pi_mutex;\n\n\tstruct task_struct *owner;\n\tatomic_t refcount;\n\n\tunion futex_key key;\n} __randomize_layout;\n\n/**\n * struct futex_q - The hashed futex queue entry, one per waiting task\n * @list:\t\tpriority-sorted list of tasks waiting on this futex\n * @task:\t\tthe task waiting on the futex\n * @lock_ptr:\t\tthe hash bucket lock\n * @key:\t\tthe key the futex is hashed on\n * @pi_state:\t\toptional priority inheritance state\n * @rt_waiter:\t\trt_waiter storage for use with requeue_pi\n * @requeue_pi_key:\tthe requeue_pi target futex key\n * @bitset:\t\tbitset for the optional bitmasked wakeup\n *\n * We use this hashed waitqueue, instead of a normal wait_queue_entry_t, so\n * we can wake only the relevant ones (hashed queues may be shared).\n *\n * A futex_q has a woken state, just like tasks have TASK_RUNNING.\n * It is considered woken when plist_node_empty(&q->list) || q->lock_ptr == 0.\n * The order of wakeup is always to make the first condition true, then\n * the second.\n *\n * PI futexes are typically woken before they are removed from the hash list via\n * the rt_mutex code. See unqueue_me_pi().\n */\nstruct futex_q {\n\tstruct plist_node list;\n\n\tstruct task_struct *task;\n\tspinlock_t *lock_ptr;\n\tunion futex_key key;\n\tstruct futex_pi_state *pi_state;\n\tstruct rt_mutex_waiter *rt_waiter;\n\tunion futex_key *requeue_pi_key;\n\tu32 bitset;\n} __randomize_layout;\n\nstatic const struct futex_q futex_q_init = {\n\t/* list gets initialized in queue_me()*/\n\t.key = FUTEX_KEY_INIT,\n\t.bitset = FUTEX_BITSET_MATCH_ANY\n};\n\n/*\n * Hash buckets are shared by all the futex_keys that hash to the same\n * location.  Each key may have multiple futex_q structures, one for each task\n * waiting on a futex.\n */\nstruct futex_hash_bucket {\n\tatomic_t waiters;\n\tspinlock_t lock;\n\tstruct plist_head chain;\n} ____cacheline_aligned_in_smp;\n\n/*\n * The base of the bucket array and its size are always used together\n * (after initialization only in hash_futex()), so ensure that they\n * reside in the same cacheline.\n */\nstatic struct {\n\tstruct futex_hash_bucket *queues;\n\tunsigned long            hashsize;\n} __futex_data __read_mostly __aligned(2*sizeof(long));\n#define futex_queues   (__futex_data.queues)\n#define futex_hashsize (__futex_data.hashsize)\n\n\n/*\n * Fault injections for futexes.\n */\n#ifdef CONFIG_FAIL_FUTEX\n\nstatic struct {\n\tstruct fault_attr attr;\n\n\tbool ignore_private;\n} fail_futex = {\n\t.attr = FAULT_ATTR_INITIALIZER,\n\t.ignore_private = false,\n};\n\nstatic int __init setup_fail_futex(char *str)\n{\n\treturn setup_fault_attr(&fail_futex.attr, str);\n}\n__setup(\"fail_futex=\", setup_fail_futex);\n\nstatic bool should_fail_futex(bool fshared)\n{\n\tif (fail_futex.ignore_private && !fshared)\n\t\treturn false;\n\n\treturn should_fail(&fail_futex.attr, 1);\n}\n\n#ifdef CONFIG_FAULT_INJECTION_DEBUG_FS\n\nstatic int __init fail_futex_debugfs(void)\n{\n\tumode_t mode = S_IFREG | S_IRUSR | S_IWUSR;\n\tstruct dentry *dir;\n\n\tdir = fault_create_debugfs_attr(\"fail_futex\", NULL,\n\t\t\t\t\t&fail_futex.attr);\n\tif (IS_ERR(dir))\n\t\treturn PTR_ERR(dir);\n\n\tif (!debugfs_create_bool(\"ignore-private\", mode, dir,\n\t\t\t\t &fail_futex.ignore_private)) {\n\t\tdebugfs_remove_recursive(dir);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nlate_initcall(fail_futex_debugfs);\n\n#endif /* CONFIG_FAULT_INJECTION_DEBUG_FS */\n\n#else\nstatic inline bool should_fail_futex(bool fshared)\n{\n\treturn false;\n}\n#endif /* CONFIG_FAIL_FUTEX */\n\nstatic inline void futex_get_mm(union futex_key *key)\n{\n\tmmgrab(key->private.mm);\n\t/*\n\t * Ensure futex_get_mm() implies a full barrier such that\n\t * get_futex_key() implies a full barrier. This is relied upon\n\t * as smp_mb(); (B), see the ordering comment above.\n\t */\n\tsmp_mb__after_atomic();\n}\n\n/*\n * Reflects a new waiter being added to the waitqueue.\n */\nstatic inline void hb_waiters_inc(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\tatomic_inc(&hb->waiters);\n\t/*\n\t * Full barrier (A), see the ordering comment above.\n\t */\n\tsmp_mb__after_atomic();\n#endif\n}\n\n/*\n * Reflects a waiter being removed from the waitqueue by wakeup\n * paths.\n */\nstatic inline void hb_waiters_dec(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\tatomic_dec(&hb->waiters);\n#endif\n}\n\nstatic inline int hb_waiters_pending(struct futex_hash_bucket *hb)\n{\n#ifdef CONFIG_SMP\n\treturn atomic_read(&hb->waiters);\n#else\n\treturn 1;\n#endif\n}\n\n/**\n * hash_futex - Return the hash bucket in the global hash\n * @key:\tPointer to the futex key for which the hash is calculated\n *\n * We hash on the keys returned from get_futex_key (see below) and return the\n * corresponding hash bucket in the global hash.\n */\nstatic struct futex_hash_bucket *hash_futex(union futex_key *key)\n{\n\tu32 hash = jhash2((u32*)&key->both.word,\n\t\t\t  (sizeof(key->both.word)+sizeof(key->both.ptr))/4,\n\t\t\t  key->both.offset);\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}\n\n\n/**\n * match_futex - Check whether two futex keys are equal\n * @key1:\tPointer to key1\n * @key2:\tPointer to key2\n *\n * Return 1 if two futex_keys are equal, 0 otherwise.\n */\nstatic inline int match_futex(union futex_key *key1, union futex_key *key2)\n{\n\treturn (key1 && key2\n\t\t&& key1->both.word == key2->both.word\n\t\t&& key1->both.ptr == key2->both.ptr\n\t\t&& key1->both.offset == key2->both.offset);\n}\n\n/*\n * Take a reference to the resource addressed by a key.\n * Can be called while holding spinlocks.\n *\n */\nstatic void get_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr)\n\t\treturn;\n\n\t/*\n\t * On MMU less systems futexes are always \"private\" as there is no per\n\t * process address space. We need the smp wmb nevertheless - yes,\n\t * arch/blackfin has MMU less SMP ...\n\t */\n\tif (!IS_ENABLED(CONFIG_MMU)) {\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t\treturn;\n\t}\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tihold(key->shared.inode); /* implies smp_mb(); (B) */\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tfutex_get_mm(key); /* implies smp_mb(); (B) */\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * Private futexes do not hold reference on an inode or\n\t\t * mm, therefore the only purpose of calling get_futex_key_refs\n\t\t * is because we need the barrier for the lockless waiter check.\n\t\t */\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t}\n}\n\n/*\n * Drop a reference to the resource addressed by a key.\n * The hash bucket spinlock must not be held. This is\n * a no-op for private futexes, see comment in the get\n * counterpart.\n */\nstatic void drop_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr) {\n\t\t/* If we're here then we tried to put a key we failed to get */\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\n\tif (!IS_ENABLED(CONFIG_MMU))\n\t\treturn;\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tiput(key->shared.inode);\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tmmdrop(key->private.mm);\n\t\tbreak;\n\t}\n}\n\n/**\n * get_futex_key() - Get parameters which are the keys for a futex\n * @uaddr:\tvirtual address of the futex\n * @fshared:\t0 for a PROCESS_PRIVATE futex, 1 for PROCESS_SHARED\n * @key:\taddress where result is stored.\n * @rw:\t\tmapping needs to be read/write (values: VERIFY_READ,\n *              VERIFY_WRITE)\n *\n * Return: a negative error code or 0\n *\n * The key words are stored in @key on success.\n *\n * For shared mappings, it's (page->index, file_inode(vma->vm_file),\n * offset_within_page).  For private mappings, it's (uaddr, current->mm).\n * We can usually work out the index without swapping in the page.\n *\n * lock_page() might sleep, the caller should not hold a spinlock.\n */\nstatic int\nget_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t/*\n\t * The futex address must be \"naturally\" aligned.\n\t */\n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(rw, uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * PROCESS_PRIVATE futexes are fast.\n\t * As the mm cannot disappear under us and the 'key' only needs\n\t * virtual address, we dont even have to find the underlying vma.\n\t * Note : We do have to check 'uaddr' is a valid user address,\n\t *        but access_ok() should be faster than find_vma()\n\t */\n\tif (!fshared) {\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\t\tget_futex_key_refs(key);  /* implies smp_mb(); (B) */\n\t\treturn 0;\n\t}\n\nagain:\n\t/* Ignore any VERIFY_READ mapping (futex common case) */\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, 1, &page);\n\t/*\n\t * If write access is not required (eg. FUTEX_WAIT), try\n\t * and get read-only access.\n\t */\n\tif (err == -EFAULT && rw == VERIFY_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t/*\n\t * The treatment of mapping from this point on is critical. The page\n\t * lock protects many things but in this context the page lock\n\t * stabilizes mapping, prevents inode freeing in the shared\n\t * file-backed region case and guards against movement to swap cache.\n\t *\n\t * Strictly speaking the page lock is not needed in all cases being\n\t * considered here and page lock forces unnecessarily serialization\n\t * From this point on, mapping will be re-verified if necessary and\n\t * page lock will be acquired only if it is unavoidable\n\t *\n\t * Mapping checks require the head page for any compound page so the\n\t * head page and mapping is looked up now. For anonymous pages, it\n\t * does not matter if the page splits in the future as the key is\n\t * based on the address. For filesystem-backed pages, the tail is\n\t * required as the index of the page determines the key. For\n\t * base pages, there is no tail page and tail == page.\n\t */\n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t/*\n\t * If page->mapping is NULL, then it cannot be a PageAnon\n\t * page; but it might be the ZERO_PAGE or in the gate area or\n\t * in a special mapping (all cases which we are happy to fail);\n\t * or it may have been a good file page when get_user_pages_fast\n\t * found it, but truncated or holepunched or subjected to\n\t * invalidate_complete_page2 before we got the page lock (also\n\t * cases which we are happy to fail).  And we hold a reference,\n\t * so refcount care in invalidate_complete_page's remove_mapping\n\t * prevents drop_caches from setting mapping to NULL beneath us.\n\t *\n\t * The case we do have to guard against is when memory pressure made\n\t * shmem_writepage move it from filecache to swapcache beneath us:\n\t * an unlikely race, but we do need to retry for page->mapping.\n\t */\n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t/*\n\t\t * Page lock is required to identify which special case above\n\t\t * applies. If this is really a shmem page then the page lock\n\t\t * will prevent unexpected transitions.\n\t\t */\n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t/*\n\t * Private mappings are handled in a simple way.\n\t *\n\t * If the futex key is stored on an anonymous page, then the associated\n\t * object is the mm which is implicitly pinned by the calling process.\n\t *\n\t * NOTE: When userspace waits on a MAP_SHARED mapping, even if\n\t * it's a read-only handle, it's expected that futexes attach to\n\t * the object not the particular process.\n\t */\n\tif (PageAnon(page)) {\n\t\t/*\n\t\t * A RO anonymous page will never change and thus doesn't make\n\t\t * sense for futex operations.\n\t\t */\n\t\tif (unlikely(should_fail_futex(fshared)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED; /* ref taken on mm */\n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t\tget_futex_key_refs(key); /* implies smp_mb(); (B) */\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * The associated futex object in this case is the inode and\n\t\t * the page->mapping must be traversed. Ordinarily this should\n\t\t * be stabilised under page lock but it's not strictly\n\t\t * necessary in this case as we just want to pin the inode, not\n\t\t * update the radix tree or anything like that.\n\t\t *\n\t\t * The RCU read lock is taken as the inode is finally freed\n\t\t * under RCU. If the mapping still matches expectations then the\n\t\t * mapping->host can be safely accessed as being a valid inode.\n\t\t */\n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\t/*\n\t\t * Take a reference unless it is about to be freed. Previously\n\t\t * this reference was taken by ihold under the page lock\n\t\t * pinning the inode in place so i_lock was unnecessary. The\n\t\t * only way for this check to fail is if the inode was\n\t\t * truncated in parallel which is almost certainly an\n\t\t * application bug. In such a case, just retry.\n\t\t *\n\t\t * We are not calling into get_futex_key_refs() in file-backed\n\t\t * cases, therefore a successful atomic_inc return below will\n\t\t * guarantee that get_futex_key() will still imply smp_mb(); (B).\n\t\t */\n\t\tif (!atomic_inc_not_zero(&inode->i_count)) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\t/* Should be impossible but lets be paranoid for now */\n\t\tif (WARN_ON_ONCE(inode->i_mapping != mapping)) {\n\t\t\terr = -EFAULT;\n\t\t\trcu_read_unlock();\n\t\t\tiput(inode);\n\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE; /* inode-based key */\n\t\tkey->shared.inode = inode;\n\t\tkey->shared.pgoff = basepage_index(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}\n\nstatic inline void put_futex_key(union futex_key *key)\n{\n\tdrop_futex_key_refs(key);\n}\n\n/**\n * fault_in_user_writeable() - Fault in user address and verify RW access\n * @uaddr:\tpointer to faulting user space address\n *\n * Slow path to fixup the fault we just took in the atomic write\n * access to @uaddr.\n *\n * We have no generic implementation of a non-destructive write to the\n * user address. We know that we faulted in the atomic pagefault\n * disabled section so we can as well avoid the #PF overhead by\n * calling get_user_pages() right away.\n */\nstatic int fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tdown_read(&mm->mmap_sem);\n\tret = fixup_user_fault(current, mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tup_read(&mm->mmap_sem);\n\n\treturn ret < 0 ? ret : 0;\n}\n\n/**\n * futex_top_waiter() - Return the highest priority waiter on a futex\n * @hb:\t\tthe hash bucket the futex_q's reside in\n * @key:\tthe futex key (to distinguish it from other futex futex_q's)\n *\n * Must be called with the hb lock held.\n */\nstatic struct futex_q *futex_top_waiter(struct futex_hash_bucket *hb,\n\t\t\t\t\tunion futex_key *key)\n{\n\tstruct futex_q *this;\n\n\tplist_for_each_entry(this, &hb->chain, list) {\n\t\tif (match_futex(&this->key, key))\n\t\t\treturn this;\n\t}\n\treturn NULL;\n}\n\nstatic int cmpxchg_futex_value_locked(u32 *curval, u32 __user *uaddr,\n\t\t\t\t      u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}\n\nstatic int get_futex_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}\n\n\n/*\n * PI code:\n */\nstatic int refill_pi_state_cache(void)\n{\n\tstruct futex_pi_state *pi_state;\n\n\tif (likely(current->pi_state_cache))\n\t\treturn 0;\n\n\tpi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);\n\n\tif (!pi_state)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pi_state->list);\n\t/* pi_mutex gets initialized later */\n\tpi_state->owner = NULL;\n\tatomic_set(&pi_state->refcount, 1);\n\tpi_state->key = FUTEX_KEY_INIT;\n\n\tcurrent->pi_state_cache = pi_state;\n\n\treturn 0;\n}\n\nstatic struct futex_pi_state *alloc_pi_state(void)\n{\n\tstruct futex_pi_state *pi_state = current->pi_state_cache;\n\n\tWARN_ON(!pi_state);\n\tcurrent->pi_state_cache = NULL;\n\n\treturn pi_state;\n}\n\nstatic void get_pi_state(struct futex_pi_state *pi_state)\n{\n\tWARN_ON_ONCE(!atomic_inc_not_zero(&pi_state->refcount));\n}\n\n/*\n * Drops a reference to the pi_state object and frees or caches it\n * when the last reference is gone.\n */\nstatic void put_pi_state(struct futex_pi_state *pi_state)\n{\n\tif (!pi_state)\n\t\treturn;\n\n\tif (!atomic_dec_and_test(&pi_state->refcount))\n\t\treturn;\n\n\t/*\n\t * If pi_state->owner is NULL, the owner is most probably dying\n\t * and has cleaned up the pi_state already\n\t */\n\tif (pi_state->owner) {\n\t\tstruct task_struct *owner;\n\n\t\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\t\towner = pi_state->owner;\n\t\tif (owner) {\n\t\t\traw_spin_lock(&owner->pi_lock);\n\t\t\tlist_del_init(&pi_state->list);\n\t\t\traw_spin_unlock(&owner->pi_lock);\n\t\t}\n\t\trt_mutex_proxy_unlock(&pi_state->pi_mutex, owner);\n\t\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t}\n\n\tif (current->pi_state_cache) {\n\t\tkfree(pi_state);\n\t} else {\n\t\t/*\n\t\t * pi_state->list is already empty.\n\t\t * clear pi_state->owner.\n\t\t * refcount is at 0 - put it back to 1.\n\t\t */\n\t\tpi_state->owner = NULL;\n\t\tatomic_set(&pi_state->refcount, 1);\n\t\tcurrent->pi_state_cache = pi_state;\n\t}\n}\n\n/*\n * Look up the task based on what TID userspace gave us.\n * We dont trust it.\n */\nstatic struct task_struct *futex_find_get_task(pid_t pid)\n{\n\tstruct task_struct *p;\n\n\trcu_read_lock();\n\tp = find_task_by_vpid(pid);\n\tif (p)\n\t\tget_task_struct(p);\n\n\trcu_read_unlock();\n\n\treturn p;\n}\n\n#ifdef CONFIG_FUTEX_PI\n\n/*\n * This task is holding PI mutexes at exit time => bad.\n * Kernel cleans up PI-state, but userspace is likely hosed.\n * (Robust-futex cleanup is separate and might save the day for userspace.)\n */\nvoid exit_pi_state_list(struct task_struct *curr)\n{\n\tstruct list_head *next, *head = &curr->pi_state_list;\n\tstruct futex_pi_state *pi_state;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn;\n\t/*\n\t * We are a ZOMBIE and nobody can enqueue itself on\n\t * pi_state_list anymore, but we have to be careful\n\t * versus waiters unqueueing themselves:\n\t */\n\traw_spin_lock_irq(&curr->pi_lock);\n\twhile (!list_empty(head)) {\n\t\tnext = head->next;\n\t\tpi_state = list_entry(next, struct futex_pi_state, list);\n\t\tkey = pi_state->key;\n\t\thb = hash_futex(&key);\n\n\t\t/*\n\t\t * We can race against put_pi_state() removing itself from the\n\t\t * list (a waiter going away). put_pi_state() will first\n\t\t * decrement the reference count and then modify the list, so\n\t\t * its possible to see the list entry but fail this reference\n\t\t * acquire.\n\t\t *\n\t\t * In that case; drop the locks to let put_pi_state() make\n\t\t * progress and retry the loop.\n\t\t */\n\t\tif (!atomic_inc_not_zero(&pi_state->refcount)) {\n\t\t\traw_spin_unlock_irq(&curr->pi_lock);\n\t\t\tcpu_relax();\n\t\t\traw_spin_lock_irq(&curr->pi_lock);\n\t\t\tcontinue;\n\t\t}\n\t\traw_spin_unlock_irq(&curr->pi_lock);\n\n\t\tspin_lock(&hb->lock);\n\t\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\t\traw_spin_lock(&curr->pi_lock);\n\t\t/*\n\t\t * We dropped the pi-lock, so re-check whether this\n\t\t * task still owns the PI-state:\n\t\t */\n\t\tif (head->next != next) {\n\t\t\t/* retain curr->pi_lock for the loop invariant */\n\t\t\traw_spin_unlock(&pi_state->pi_mutex.wait_lock);\n\t\t\tspin_unlock(&hb->lock);\n\t\t\tput_pi_state(pi_state);\n\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON(pi_state->owner != curr);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\tpi_state->owner = NULL;\n\n\t\traw_spin_unlock(&curr->pi_lock);\n\t\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t\tspin_unlock(&hb->lock);\n\n\t\trt_mutex_futex_unlock(&pi_state->pi_mutex);\n\t\tput_pi_state(pi_state);\n\n\t\traw_spin_lock_irq(&curr->pi_lock);\n\t}\n\traw_spin_unlock_irq(&curr->pi_lock);\n}\n\n#endif\n\n/*\n * We need to check the following states:\n *\n *      Waiter | pi_state | pi->owner | uTID      | uODIED | ?\n *\n * [1]  NULL   | ---      | ---       | 0         | 0/1    | Valid\n * [2]  NULL   | ---      | ---       | >0        | 0/1    | Valid\n *\n * [3]  Found  | NULL     | --        | Any       | 0/1    | Invalid\n *\n * [4]  Found  | Found    | NULL      | 0         | 1      | Valid\n * [5]  Found  | Found    | NULL      | >0        | 1      | Invalid\n *\n * [6]  Found  | Found    | task      | 0         | 1      | Valid\n *\n * [7]  Found  | Found    | NULL      | Any       | 0      | Invalid\n *\n * [8]  Found  | Found    | task      | ==taskTID | 0/1    | Valid\n * [9]  Found  | Found    | task      | 0         | 0      | Invalid\n * [10] Found  | Found    | task      | !=taskTID | 0/1    | Invalid\n *\n * [1]\tIndicates that the kernel can acquire the futex atomically. We\n *\tcame came here due to a stale FUTEX_WAITERS/FUTEX_OWNER_DIED bit.\n *\n * [2]\tValid, if TID does not belong to a kernel thread. If no matching\n *      thread is found then it indicates that the owner TID has died.\n *\n * [3]\tInvalid. The waiter is queued on a non PI futex\n *\n * [4]\tValid state after exit_robust_list(), which sets the user space\n *\tvalue to FUTEX_WAITERS | FUTEX_OWNER_DIED.\n *\n * [5]\tThe user space value got manipulated between exit_robust_list()\n *\tand exit_pi_state_list()\n *\n * [6]\tValid state after exit_pi_state_list() which sets the new owner in\n *\tthe pi_state but cannot access the user space value.\n *\n * [7]\tpi_state->owner can only be NULL when the OWNER_DIED bit is set.\n *\n * [8]\tOwner and user space value match\n *\n * [9]\tThere is no transient state which sets the user space TID to 0\n *\texcept exit_robust_list(), but this is indicated by the\n *\tFUTEX_OWNER_DIED bit. See [4]\n *\n * [10] There is no transient state which leaves owner and user space\n *\tTID out of sync.\n *\n *\n * Serialization and lifetime rules:\n *\n * hb->lock:\n *\n *\thb -> futex_q, relation\n *\tfutex_q -> pi_state, relation\n *\n *\t(cannot be raw because hb can contain arbitrary amount\n *\t of futex_q's)\n *\n * pi_mutex->wait_lock:\n *\n *\t{uval, pi_state}\n *\n *\t(and pi_mutex 'obviously')\n *\n * p->pi_lock:\n *\n *\tp->pi_state_list -> pi_state->list, relation\n *\n * pi_state->refcount:\n *\n *\tpi_state lifetime\n *\n *\n * Lock order:\n *\n *   hb->lock\n *     pi_mutex->wait_lock\n *       p->pi_lock\n *\n */\n\n/*\n * Validate that the existing waiter has a pi_state and sanity check\n * the pi_state against the user space value. If correct, attach to\n * it.\n */\nstatic int attach_to_pi_state(u32 __user *uaddr, u32 uval,\n\t\t\t      struct futex_pi_state *pi_state,\n\t\t\t      struct futex_pi_state **ps)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tu32 uval2;\n\tint ret;\n\n\t/*\n\t * Userspace might have messed up non-PI and PI futexes [3]\n\t */\n\tif (unlikely(!pi_state))\n\t\treturn -EINVAL;\n\n\t/*\n\t * We get here with hb->lock held, and having found a\n\t * futex_top_waiter(). This means that futex_lock_pi() of said futex_q\n\t * has dropped the hb->lock in between queue_me() and unqueue_me_pi(),\n\t * which in turn means that futex_lock_pi() still has a reference on\n\t * our pi_state.\n\t *\n\t * The waiter holding a reference on @pi_state also protects against\n\t * the unlocked put_pi_state() in futex_unlock_pi(), futex_lock_pi()\n\t * and futex_wait_requeue_pi() as it cannot go to 0 and consequently\n\t * free pi_state before we can take a reference ourselves.\n\t */\n\tWARN_ON(!atomic_read(&pi_state->refcount));\n\n\t/*\n\t * Now that we have a pi_state, we can acquire wait_lock\n\t * and do the state validation.\n\t */\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Since {uval, pi_state} is serialized by wait_lock, and our current\n\t * uval was read without holding it, it can have changed. Verify it\n\t * still is what we expect it to be, otherwise retry the entire\n\t * operation.\n\t */\n\tif (get_futex_value_locked(&uval2, uaddr))\n\t\tgoto out_efault;\n\n\tif (uval != uval2)\n\t\tgoto out_eagain;\n\n\t/*\n\t * Handle the owner died case:\n\t */\n\tif (uval & FUTEX_OWNER_DIED) {\n\t\t/*\n\t\t * exit_pi_state_list sets owner to NULL and wakes the\n\t\t * topmost waiter. The task which acquires the\n\t\t * pi_state->rt_mutex will fixup owner.\n\t\t */\n\t\tif (!pi_state->owner) {\n\t\t\t/*\n\t\t\t * No pi state owner, but the user space TID\n\t\t\t * is not 0. Inconsistent state. [5]\n\t\t\t */\n\t\t\tif (pid)\n\t\t\t\tgoto out_einval;\n\t\t\t/*\n\t\t\t * Take a ref on the state and return success. [4]\n\t\t\t */\n\t\t\tgoto out_attach;\n\t\t}\n\n\t\t/*\n\t\t * If TID is 0, then either the dying owner has not\n\t\t * yet executed exit_pi_state_list() or some waiter\n\t\t * acquired the rtmutex in the pi state, but did not\n\t\t * yet fixup the TID in user space.\n\t\t *\n\t\t * Take a ref on the state and return success. [6]\n\t\t */\n\t\tif (!pid)\n\t\t\tgoto out_attach;\n\t} else {\n\t\t/*\n\t\t * If the owner died bit is not set, then the pi_state\n\t\t * must have an owner. [7]\n\t\t */\n\t\tif (!pi_state->owner)\n\t\t\tgoto out_einval;\n\t}\n\n\t/*\n\t * Bail out if user space manipulated the futex value. If pi\n\t * state exists then the owner TID must be the same as the\n\t * user space TID. [9/10]\n\t */\n\tif (pid != task_pid_vnr(pi_state->owner))\n\t\tgoto out_einval;\n\nout_attach:\n\tget_pi_state(pi_state);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t*ps = pi_state;\n\treturn 0;\n\nout_einval:\n\tret = -EINVAL;\n\tgoto out_error;\n\nout_eagain:\n\tret = -EAGAIN;\n\tgoto out_error;\n\nout_efault:\n\tret = -EFAULT;\n\tgoto out_error;\n\nout_error:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}\n\n/*\n * Lookup the task for the TID provided from user space and attach to\n * it after doing proper sanity checks.\n */\nstatic int attach_to_pi_owner(u32 uval, union futex_key *key,\n\t\t\t      struct futex_pi_state **ps)\n{\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\tstruct futex_pi_state *pi_state;\n\tstruct task_struct *p;\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0 [1]\n\t */\n\tif (!pid)\n\t\treturn -ESRCH;\n\tp = futex_find_get_task(pid);\n\tif (!p)\n\t\treturn -ESRCH;\n\n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tput_task_struct(p);\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We need to look at the task state flags to figure out,\n\t * whether the task is exiting. To protect against the do_exit\n\t * change of the task flags, we do this protected by\n\t * p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->flags & PF_EXITING)) {\n\t\t/*\n\t\t * The task is on the way out. When PF_EXITPIDONE is\n\t\t * set, we know that the task has finished the\n\t\t * cleanup:\n\t\t */\n\t\tint ret = (p->flags & PF_EXITPIDONE) ? -ESRCH : -EAGAIN;\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * No existing pi state. First waiter. [2]\n\t *\n\t * This creates pi_state, we have hb->lock held, this means nothing can\n\t * observe this state, wait_lock is irrelevant.\n\t */\n\tpi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make @p\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\t/*\n\t * Assignment without holding pi_state->pi_mutex.wait_lock is safe\n\t * because there is no concurrency as the object is not published yet.\n\t */\n\tpi_state->owner = p;\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\t*ps = pi_state;\n\n\treturn 0;\n}\n\nstatic int lookup_pi_state(u32 __user *uaddr, u32 uval,\n\t\t\t   struct futex_hash_bucket *hb,\n\t\t\t   union futex_key *key, struct futex_pi_state **ps)\n{\n\tstruct futex_q *top_waiter = futex_top_waiter(hb, key);\n\n\t/*\n\t * If there is a waiter on that futex, validate it and\n\t * attach to the pi_state when the validation succeeds.\n\t */\n\tif (top_waiter)\n\t\treturn attach_to_pi_state(uaddr, uval, top_waiter->pi_state, ps);\n\n\t/*\n\t * We are the first waiter - try to look up the owner based on\n\t * @uval and attach to it.\n\t */\n\treturn attach_to_pi_owner(uval, key, ps);\n}\n\nstatic int lock_pi_update_atomic(u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tu32 uninitialized_var(curval);\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\tif (unlikely(cmpxchg_futex_value_locked(&curval, uaddr, uval, newval)))\n\t\treturn -EFAULT;\n\n\t/* If user space value changed, let the caller retry */\n\treturn curval != uval ? -EAGAIN : 0;\n}\n\n/**\n * futex_lock_pi_atomic() - Atomic work required to acquire a pi aware futex\n * @uaddr:\t\tthe pi futex user address\n * @hb:\t\t\tthe pi futex hash bucket\n * @key:\t\tthe futex key associated with uaddr and hb\n * @ps:\t\t\tthe pi_state pointer where we store the result of the\n *\t\t\tlookup\n * @task:\t\tthe task to perform the atomic lock work for.  This will\n *\t\t\tbe \"current\" except in the case of requeue pi.\n * @set_waiters:\tforce setting the FUTEX_WAITERS bit (1) or not (0)\n *\n * Return:\n *  -  0 - ready to wait;\n *  -  1 - acquired the lock;\n *  - <0 - error\n *\n * The hb->lock and futex_key refs shall be held by the caller.\n */\nstatic int futex_lock_pi_atomic(u32 __user *uaddr, struct futex_hash_bucket *hb,\n\t\t\t\tunion futex_key *key,\n\t\t\t\tstruct futex_pi_state **ps,\n\t\t\t\tstruct task_struct *task, int set_waiters)\n{\n\tu32 uval, newval, vpid = task_pid_vnr(task);\n\tstruct futex_q *top_waiter;\n\tint ret;\n\n\t/*\n\t * Read the user space value first so we can validate a few\n\t * things before proceeding further.\n\t */\n\tif (get_futex_value_locked(&uval, uaddr))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Detect deadlocks.\n\t */\n\tif ((unlikely((uval & FUTEX_TID_MASK) == vpid)))\n\t\treturn -EDEADLK;\n\n\tif ((unlikely(should_fail_futex(true))))\n\t\treturn -EDEADLK;\n\n\t/*\n\t * Lookup existing state first. If it exists, try to attach to\n\t * its pi_state.\n\t */\n\ttop_waiter = futex_top_waiter(hb, key);\n\tif (top_waiter)\n\t\treturn attach_to_pi_state(uaddr, uval, top_waiter->pi_state, ps);\n\n\t/*\n\t * No waiter and user TID is 0. We are here because the\n\t * waiters or the owner died bit is set or called from\n\t * requeue_cmp_pi or for whatever reason something took the\n\t * syscall.\n\t */\n\tif (!(uval & FUTEX_TID_MASK)) {\n\t\t/*\n\t\t * We take over the futex. No other waiters and the user space\n\t\t * TID is 0. We preserve the owner died bit.\n\t\t */\n\t\tnewval = uval & FUTEX_OWNER_DIED;\n\t\tnewval |= vpid;\n\n\t\t/* The futex requeue_pi code can enforce the waiters bit */\n\t\tif (set_waiters)\n\t\t\tnewval |= FUTEX_WAITERS;\n\n\t\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\t\t/* If the take over worked, return 1 */\n\t\treturn ret < 0 ? ret : 1;\n\t}\n\n\t/*\n\t * First waiter. Set the waiters bit before attaching ourself to\n\t * the owner. If owner tries to unlock, it will be forced into\n\t * the kernel and blocked on hb->lock.\n\t */\n\tnewval = uval | FUTEX_WAITERS;\n\tret = lock_pi_update_atomic(uaddr, uval, newval);\n\tif (ret)\n\t\treturn ret;\n\t/*\n\t * If the update of the user space value succeeded, we try to\n\t * attach to the owner. If that fails, no harm done, we only\n\t * set the FUTEX_WAITERS bit in the user space variable.\n\t */\n\treturn attach_to_pi_owner(uval, key, ps);\n}\n\n/**\n * __unqueue_futex() - Remove the futex_q from its futex_hash_bucket\n * @q:\tThe futex_q to unqueue\n *\n * The q->lock_ptr must not be NULL and must be held by the caller.\n */\nstatic void __unqueue_futex(struct futex_q *q)\n{\n\tstruct futex_hash_bucket *hb;\n\n\tif (WARN_ON_SMP(!q->lock_ptr || !spin_is_locked(q->lock_ptr))\n\t    || WARN_ON(plist_node_empty(&q->list)))\n\t\treturn;\n\n\thb = container_of(q->lock_ptr, struct futex_hash_bucket, lock);\n\tplist_del(&q->list, &hb->chain);\n\thb_waiters_dec(hb);\n}\n\n/*\n * The hash bucket lock must be held when this is called.\n * Afterwards, the futex_q must not be accessed. Callers\n * must ensure to later call wake_up_q() for the actual\n * wakeups to occur.\n */\nstatic void mark_wake_futex(struct wake_q_head *wake_q, struct futex_q *q)\n{\n\tstruct task_struct *p = q->task;\n\n\tif (WARN(q->pi_state || q->rt_waiter, \"refusing to wake PI futex\\n\"))\n\t\treturn;\n\n\t/*\n\t * Queue the task for later wakeup for after we've released\n\t * the hb->lock. wake_q_add() grabs reference to p.\n\t */\n\twake_q_add(wake_q, p);\n\t__unqueue_futex(q);\n\t/*\n\t * The waiting task can free the futex_q as soon as q->lock_ptr = NULL\n\t * is written, without taking any locks. This is possible in the event\n\t * of a spurious wakeup, for example. A memory barrier is required here\n\t * to prevent the following store to lock_ptr from getting ahead of the\n\t * plist_del in __unqueue_futex().\n\t */\n\tsmp_store_release(&q->lock_ptr, NULL);\n}\n\n/*\n * Caller must hold a reference on @pi_state.\n */\nstatic int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_pi_state *pi_state)\n{\n\tu32 uninitialized_var(curval), newval;\n\tstruct task_struct *new_owner;\n\tbool postunlock = false;\n\tDEFINE_WAKE_Q(wake_q);\n\tint ret = 0;\n\n\tnew_owner = rt_mutex_next_owner(&pi_state->pi_mutex);\n\tif (WARN_ON_ONCE(!new_owner)) {\n\t\t/*\n\t\t * As per the comment in futex_unlock_pi() this should not happen.\n\t\t *\n\t\t * When this happens, give up our locks and try again, giving\n\t\t * the futex_lock_pi() instance time to complete, either by\n\t\t * waiting on the rtmutex or removing itself from the futex\n\t\t * queue.\n\t\t */\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * We pass it to the next owner. The WAITERS bit is always kept\n\t * enabled while there is PI state around. We cleanup the owner\n\t * died bit, because we are the owner.\n\t */\n\tnewval = FUTEX_WAITERS | task_pid_vnr(new_owner);\n\n\tif (unlikely(should_fail_futex(true)))\n\t\tret = -EFAULT;\n\n\tif (cmpxchg_futex_value_locked(&curval, uaddr, uval, newval)) {\n\t\tret = -EFAULT;\n\n\t} else if (curval != uval) {\n\t\t/*\n\t\t * If a unconditional UNLOCK_PI operation (user space did not\n\t\t * try the TID->0 transition) raced with a waiter setting the\n\t\t * FUTEX_WAITERS flag between get_user() and locking the hash\n\t\t * bucket lock, retry the operation.\n\t\t */\n\t\tif ((FUTEX_TID_MASK & curval) == uval)\n\t\t\tret = -EAGAIN;\n\t\telse\n\t\t\tret = -EINVAL;\n\t}\n\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * This is a point of no return; once we modify the uval there is no\n\t * going back and subsequent operations must not fail.\n\t */\n\n\traw_spin_lock(&pi_state->owner->pi_lock);\n\tWARN_ON(list_empty(&pi_state->list));\n\tlist_del_init(&pi_state->list);\n\traw_spin_unlock(&pi_state->owner->pi_lock);\n\n\traw_spin_lock(&new_owner->pi_lock);\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &new_owner->pi_state_list);\n\tpi_state->owner = new_owner;\n\traw_spin_unlock(&new_owner->pi_lock);\n\n\tpostunlock = __rt_mutex_futex_unlock(&pi_state->pi_mutex, &wake_q);\n\nout_unlock:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\n\tif (postunlock)\n\t\trt_mutex_postunlock(&wake_q);\n\n\treturn ret;\n}\n\n/*\n * Express the locking dependencies for lockdep:\n */\nstatic inline void\ndouble_lock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tif (hb1 <= hb2) {\n\t\tspin_lock(&hb1->lock);\n\t\tif (hb1 < hb2)\n\t\t\tspin_lock_nested(&hb2->lock, SINGLE_DEPTH_NESTING);\n\t} else { /* hb1 > hb2 */\n\t\tspin_lock(&hb2->lock);\n\t\tspin_lock_nested(&hb1->lock, SINGLE_DEPTH_NESTING);\n\t}\n}\n\nstatic inline void\ndouble_unlock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)\n{\n\tspin_unlock(&hb1->lock);\n\tif (hb1 != hb2)\n\t\tspin_unlock(&hb2->lock);\n}\n\n/*\n * Wake up waiters matching bitset queued on this futex (uaddr).\n */\nstatic int\nfutex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)\n{\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *this, *next;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tint ret;\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\thb = hash_futex(&key);\n\n\t/* Make sure we really have tasks to wakeup */\n\tif (!hb_waiters_pending(hb))\n\t\tgoto out_put_key;\n\n\tspin_lock(&hb->lock);\n\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (match_futex (&this->key, &key)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check if one of the bits is set in both bitsets */\n\t\t\tif (!(this->bitset & bitset))\n\t\t\t\tcontinue;\n\n\t\t\tmark_wake_futex(&wake_q, this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock(&hb->lock);\n\twake_up_q(&wake_q);\nout_put_key:\n\tput_futex_key(&key);\nout:\n\treturn ret;\n}\n\nstatic int futex_atomic_op_inuser(unsigned int encoded_op, u32 __user *uaddr)\n{\n\tunsigned int op =\t  (encoded_op & 0x70000000) >> 28;\n\tunsigned int cmp =\t  (encoded_op & 0x0f000000) >> 24;\n\tint oparg = sign_extend32((encoded_op & 0x00fff000) >> 12, 11);\n\tint cmparg = sign_extend32(encoded_op & 0x00000fff, 11);\n\tint oldval, ret;\n\n\tif (encoded_op & (FUTEX_OP_OPARG_SHIFT << 28)) {\n\t\tif (oparg < 0 || oparg > 31) {\n\t\t\tchar comm[sizeof(current->comm)];\n\t\t\t/*\n\t\t\t * kill this print and return -EINVAL when userspace\n\t\t\t * is sane again\n\t\t\t */\n\t\t\tpr_info_ratelimited(\"futex_wake_op: %s tries to shift op by %d; fix this program\\n\",\n\t\t\t\t\tget_task_comm(comm, current), oparg);\n\t\t\toparg &= 31;\n\t\t}\n\t\toparg = 1 << oparg;\n\t}\n\n\tif (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))\n\t\treturn -EFAULT;\n\n\tret = arch_futex_atomic_op_inuser(op, oparg, &oldval, uaddr);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (cmp) {\n\tcase FUTEX_OP_CMP_EQ:\n\t\treturn oldval == cmparg;\n\tcase FUTEX_OP_CMP_NE:\n\t\treturn oldval != cmparg;\n\tcase FUTEX_OP_CMP_LT:\n\t\treturn oldval < cmparg;\n\tcase FUTEX_OP_CMP_GE:\n\t\treturn oldval >= cmparg;\n\tcase FUTEX_OP_CMP_LE:\n\t\treturn oldval <= cmparg;\n\tcase FUTEX_OP_CMP_GT:\n\t\treturn oldval > cmparg;\n\tdefault:\n\t\treturn -ENOSYS;\n\t}\n}\n\n/*\n * Wake up all waiters hashed on the physical page that is mapped\n * to this virtual address:\n */\nstatic int\nfutex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,\n\t      int nr_wake, int nr_wake2, int op)\n{\n\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;\n\tstruct futex_hash_bucket *hb1, *hb2;\n\tstruct futex_q *this, *next;\n\tint ret, op_ret;\n\tDEFINE_WAKE_Q(wake_q);\n\nretry:\n\tret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out_put_key1;\n\n\thb1 = hash_futex(&key1);\n\thb2 = hash_futex(&key2);\n\nretry_private:\n\tdouble_lock_hb(hb1, hb2);\n\top_ret = futex_atomic_op_inuser(op, uaddr2);\n\tif (unlikely(op_ret < 0)) {\n\n\t\tdouble_unlock_hb(hb1, hb2);\n\n#ifndef CONFIG_MMU\n\t\t/*\n\t\t * we don't get EFAULT from MMU faults if we don't have an MMU,\n\t\t * but we might get them from range checking\n\t\t */\n\t\tret = op_ret;\n\t\tgoto out_put_keys;\n#endif\n\n\t\tif (unlikely(op_ret != -EFAULT)) {\n\t\t\tret = op_ret;\n\t\t\tgoto out_put_keys;\n\t\t}\n\n\t\tret = fault_in_user_writeable(uaddr2);\n\t\tif (ret)\n\t\t\tgoto out_put_keys;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tput_futex_key(&key2);\n\t\tput_futex_key(&key1);\n\t\tgoto retry;\n\t}\n\n\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {\n\t\tif (match_futex (&this->key, &key1)) {\n\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tmark_wake_futex(&wake_q, this);\n\t\t\tif (++ret >= nr_wake)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (op_ret > 0) {\n\t\top_ret = 0;\n\t\tplist_for_each_entry_safe(this, next, &hb2->chain, list) {\n\t\t\tif (match_futex (&this->key, &key2)) {\n\t\t\t\tif (this->pi_state || this->rt_waiter) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t\tmark_wake_futex(&wake_q, this);\n\t\t\t\tif (++op_ret >= nr_wake2)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tret += op_ret;\n\t}\n\nout_unlock:\n\tdouble_unlock_hb(hb1, hb2);\n\twake_up_q(&wake_q);\nout_put_keys:\n\tput_futex_key(&key2);\nout_put_key1:\n\tput_futex_key(&key1);\nout:\n\treturn ret;\n}\n\n/**\n * requeue_futex() - Requeue a futex_q from one hb to another\n * @q:\t\tthe futex_q to requeue\n * @hb1:\tthe source hash_bucket\n * @hb2:\tthe target hash_bucket\n * @key2:\tthe new key for the requeued futex_q\n */\nstatic inline\nvoid requeue_futex(struct futex_q *q, struct futex_hash_bucket *hb1,\n\t\t   struct futex_hash_bucket *hb2, union futex_key *key2)\n{\n\n\t/*\n\t * If key1 and key2 hash to the same bucket, no need to\n\t * requeue.\n\t */\n\tif (likely(&hb1->chain != &hb2->chain)) {\n\t\tplist_del(&q->list, &hb1->chain);\n\t\thb_waiters_dec(hb1);\n\t\thb_waiters_inc(hb2);\n\t\tplist_add(&q->list, &hb2->chain);\n\t\tq->lock_ptr = &hb2->lock;\n\t}\n\tget_futex_key_refs(key2);\n\tq->key = *key2;\n}\n\n/**\n * requeue_pi_wake_futex() - Wake a task that acquired the lock during requeue\n * @q:\t\tthe futex_q\n * @key:\tthe key of the requeue target futex\n * @hb:\t\tthe hash_bucket of the requeue target futex\n *\n * During futex_requeue, with requeue_pi=1, it is possible to acquire the\n * target futex if it is uncontended or via a lock steal.  Set the futex_q key\n * to the requeue target futex so the waiter can detect the wakeup on the right\n * futex, but remove it from the hb and NULL the rt_waiter so it can detect\n * atomic lock acquisition.  Set the q->lock_ptr to the requeue target hb->lock\n * to protect access to the pi_state to fixup the owner later.  Must be called\n * with both q->lock_ptr and hb->lock held.\n */\nstatic inline\nvoid requeue_pi_wake_futex(struct futex_q *q, union futex_key *key,\n\t\t\t   struct futex_hash_bucket *hb)\n{\n\tget_futex_key_refs(key);\n\tq->key = *key;\n\n\t__unqueue_futex(q);\n\n\tWARN_ON(!q->rt_waiter);\n\tq->rt_waiter = NULL;\n\n\tq->lock_ptr = &hb->lock;\n\n\twake_up_state(q->task, TASK_NORMAL);\n}\n\n/**\n * futex_proxy_trylock_atomic() - Attempt an atomic lock for the top waiter\n * @pifutex:\t\tthe user address of the to futex\n * @hb1:\t\tthe from futex hash bucket, must be locked by the caller\n * @hb2:\t\tthe to futex hash bucket, must be locked by the caller\n * @key1:\t\tthe from futex key\n * @key2:\t\tthe to futex key\n * @ps:\t\t\taddress to store the pi_state pointer\n * @set_waiters:\tforce setting the FUTEX_WAITERS bit (1) or not (0)\n *\n * Try and get the lock on behalf of the top waiter if we can do it atomically.\n * Wake the top waiter if we succeed.  If the caller specified set_waiters,\n * then direct futex_lock_pi_atomic() to force setting the FUTEX_WAITERS bit.\n * hb1 and hb2 must be held by the caller.\n *\n * Return:\n *  -  0 - failed to acquire the lock atomically;\n *  - >0 - acquired the lock, return value is vpid of the top_waiter\n *  - <0 - error\n */\nstatic int futex_proxy_trylock_atomic(u32 __user *pifutex,\n\t\t\t\t struct futex_hash_bucket *hb1,\n\t\t\t\t struct futex_hash_bucket *hb2,\n\t\t\t\t union futex_key *key1, union futex_key *key2,\n\t\t\t\t struct futex_pi_state **ps, int set_waiters)\n{\n\tstruct futex_q *top_waiter = NULL;\n\tu32 curval;\n\tint ret, vpid;\n\n\tif (get_futex_value_locked(&curval, pifutex))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * Find the top_waiter and determine if there are additional waiters.\n\t * If the caller intends to requeue more than 1 waiter to pifutex,\n\t * force futex_lock_pi_atomic() to set the FUTEX_WAITERS bit now,\n\t * as we have means to handle the possible fault.  If not, don't set\n\t * the bit unecessarily as it will force the subsequent unlock to enter\n\t * the kernel.\n\t */\n\ttop_waiter = futex_top_waiter(hb1, key1);\n\n\t/* There are no waiters, nothing for us to do. */\n\tif (!top_waiter)\n\t\treturn 0;\n\n\t/* Ensure we requeue to the expected futex. */\n\tif (!match_futex(top_waiter->requeue_pi_key, key2))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Try to take the lock for top_waiter.  Set the FUTEX_WAITERS bit in\n\t * the contended case or if set_waiters is 1.  The pi_state is returned\n\t * in ps in contended cases.\n\t */\n\tvpid = task_pid_vnr(top_waiter->task);\n\tret = futex_lock_pi_atomic(pifutex, hb2, key2, ps, top_waiter->task,\n\t\t\t\t   set_waiters);\n\tif (ret == 1) {\n\t\trequeue_pi_wake_futex(top_waiter, key2, hb2);\n\t\treturn vpid;\n\t}\n\treturn ret;\n}\n\n/**\n * futex_requeue() - Requeue waiters from uaddr1 to uaddr2\n * @uaddr1:\tsource futex user address\n * @flags:\tfutex flags (FLAGS_SHARED, etc.)\n * @uaddr2:\ttarget futex user address\n * @nr_wake:\tnumber of waiters to wake (must be 1 for requeue_pi)\n * @nr_requeue:\tnumber of waiters to requeue (0-INT_MAX)\n * @cmpval:\t@uaddr1 expected value (or %NULL)\n * @requeue_pi:\tif we are attempting to requeue from a non-pi futex to a\n *\t\tpi futex (pi to pi requeue is not supported)\n *\n * Requeue waiters on uaddr1 to uaddr2. In the requeue_pi case, try to acquire\n * uaddr2 atomically on behalf of the top waiter.\n *\n * Return:\n *  - >=0 - on success, the number of tasks requeued or woken;\n *  -  <0 - on error\n */\nstatic int futex_requeue(u32 __user *uaddr1, unsigned int flags,\n\t\t\t u32 __user *uaddr2, int nr_wake, int nr_requeue,\n\t\t\t u32 *cmpval, int requeue_pi)\n{\n\tunion futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;\n\tint drop_count = 0, task_count = 0, ret;\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct futex_hash_bucket *hb1, *hb2;\n\tstruct futex_q *this, *next;\n\tDEFINE_WAKE_Q(wake_q);\n\n\tif (nr_wake < 0 || nr_requeue < 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * When PI not supported: return -ENOSYS if requeue_pi is true,\n\t * consequently the compiler knows requeue_pi is always false past\n\t * this point which will optimize away all the conditional code\n\t * further down.\n\t */\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI) && requeue_pi)\n\t\treturn -ENOSYS;\n\n\tif (requeue_pi) {\n\t\t/*\n\t\t * Requeue PI only works on two distinct uaddrs. This\n\t\t * check is only valid for private futexes. See below.\n\t\t */\n\t\tif (uaddr1 == uaddr2)\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * requeue_pi requires a pi_state, try to allocate it now\n\t\t * without any locks in case it fails.\n\t\t */\n\t\tif (refill_pi_state_cache())\n\t\t\treturn -ENOMEM;\n\t\t/*\n\t\t * requeue_pi must wake as many tasks as it can, up to nr_wake\n\t\t * + nr_requeue, since it acquires the rt_mutex prior to\n\t\t * returning to userspace, so as to not leave the rt_mutex with\n\t\t * waiters and no owner.  However, second and third wake-ups\n\t\t * cannot be predicted as they involve race conditions with the\n\t\t * first wake and a fault while looking up the pi_state.  Both\n\t\t * pthread_cond_signal() and pthread_cond_broadcast() should\n\t\t * use nr_wake=1.\n\t\t */\n\t\tif (nr_wake != 1)\n\t\t\treturn -EINVAL;\n\t}\n\nretry:\n\tret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2,\n\t\t\t    requeue_pi ? VERIFY_WRITE : VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\tgoto out_put_key1;\n\n\t/*\n\t * The check above which compares uaddrs is not sufficient for\n\t * shared futexes. We need to compare the keys:\n\t */\n\tif (requeue_pi && match_futex(&key1, &key2)) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_keys;\n\t}\n\n\thb1 = hash_futex(&key1);\n\thb2 = hash_futex(&key2);\n\nretry_private:\n\thb_waiters_inc(hb2);\n\tdouble_lock_hb(hb1, hb2);\n\n\tif (likely(cmpval != NULL)) {\n\t\tu32 curval;\n\n\t\tret = get_futex_value_locked(&curval, uaddr1);\n\n\t\tif (unlikely(ret)) {\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\n\t\t\tret = get_user(curval, uaddr1);\n\t\t\tif (ret)\n\t\t\t\tgoto out_put_keys;\n\n\t\t\tif (!(flags & FLAGS_SHARED))\n\t\t\t\tgoto retry_private;\n\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tgoto retry;\n\t\t}\n\t\tif (curval != *cmpval) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (requeue_pi && (task_count - nr_wake < nr_requeue)) {\n\t\t/*\n\t\t * Attempt to acquire uaddr2 and wake the top waiter. If we\n\t\t * intend to requeue waiters, force setting the FUTEX_WAITERS\n\t\t * bit.  We force this here where we are able to easily handle\n\t\t * faults rather in the requeue loop below.\n\t\t */\n\t\tret = futex_proxy_trylock_atomic(uaddr2, hb1, hb2, &key1,\n\t\t\t\t\t\t &key2, &pi_state, nr_requeue);\n\n\t\t/*\n\t\t * At this point the top_waiter has either taken uaddr2 or is\n\t\t * waiting on it.  If the former, then the pi_state will not\n\t\t * exist yet, look it up one more time to ensure we have a\n\t\t * reference to it. If the lock was taken, ret contains the\n\t\t * vpid of the top waiter task.\n\t\t * If the lock was not taken, we have pi_state and an initial\n\t\t * refcount on it. In case of an error we have nothing.\n\t\t */\n\t\tif (ret > 0) {\n\t\t\tWARN_ON(pi_state);\n\t\t\tdrop_count++;\n\t\t\ttask_count++;\n\t\t\t/*\n\t\t\t * If we acquired the lock, then the user space value\n\t\t\t * of uaddr2 should be vpid. It cannot be changed by\n\t\t\t * the top waiter as it is blocked on hb2 lock if it\n\t\t\t * tries to do so. If something fiddled with it behind\n\t\t\t * our back the pi state lookup might unearth it. So\n\t\t\t * we rather use the known value than rereading and\n\t\t\t * handing potential crap to lookup_pi_state.\n\t\t\t *\n\t\t\t * If that call succeeds then we have pi_state and an\n\t\t\t * initial refcount on it.\n\t\t\t */\n\t\t\tret = lookup_pi_state(uaddr2, ret, hb2, &key2, &pi_state);\n\t\t}\n\n\t\tswitch (ret) {\n\t\tcase 0:\n\t\t\t/* We hold a reference on the pi state. */\n\t\t\tbreak;\n\n\t\t\t/* If the above failed, then pi_state is NULL */\n\t\tcase -EFAULT:\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tret = fault_in_user_writeable(uaddr2);\n\t\t\tif (!ret)\n\t\t\t\tgoto retry;\n\t\t\tgoto out;\n\t\tcase -EAGAIN:\n\t\t\t/*\n\t\t\t * Two reasons for this:\n\t\t\t * - Owner is exiting and we just wait for the\n\t\t\t *   exit to complete.\n\t\t\t * - The user space value changed.\n\t\t\t */\n\t\t\tdouble_unlock_hb(hb1, hb2);\n\t\t\thb_waiters_dec(hb2);\n\t\t\tput_futex_key(&key2);\n\t\t\tput_futex_key(&key1);\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\t\tdefault:\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tplist_for_each_entry_safe(this, next, &hb1->chain, list) {\n\t\tif (task_count - nr_wake >= nr_requeue)\n\t\t\tbreak;\n\n\t\tif (!match_futex(&this->key, &key1))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * FUTEX_WAIT_REQEUE_PI and FUTEX_CMP_REQUEUE_PI should always\n\t\t * be paired with each other and no other futex ops.\n\t\t *\n\t\t * We should never be requeueing a futex_q with a pi_state,\n\t\t * which is awaiting a futex_unlock_pi().\n\t\t */\n\t\tif ((requeue_pi && !this->rt_waiter) ||\n\t\t    (!requeue_pi && this->rt_waiter) ||\n\t\t    this->pi_state) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Wake nr_wake waiters.  For requeue_pi, if we acquired the\n\t\t * lock, we already woke the top_waiter.  If not, it will be\n\t\t * woken by futex_unlock_pi().\n\t\t */\n\t\tif (++task_count <= nr_wake && !requeue_pi) {\n\t\t\tmark_wake_futex(&wake_q, this);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Ensure we requeue to the expected futex for requeue_pi. */\n\t\tif (requeue_pi && !match_futex(this->requeue_pi_key, &key2)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Requeue nr_requeue waiters and possibly one more in the case\n\t\t * of requeue_pi if we couldn't acquire the lock atomically.\n\t\t */\n\t\tif (requeue_pi) {\n\t\t\t/*\n\t\t\t * Prepare the waiter to take the rt_mutex. Take a\n\t\t\t * refcount on the pi_state and store the pointer in\n\t\t\t * the futex_q object of the waiter.\n\t\t\t */\n\t\t\tget_pi_state(pi_state);\n\t\t\tthis->pi_state = pi_state;\n\t\t\tret = rt_mutex_start_proxy_lock(&pi_state->pi_mutex,\n\t\t\t\t\t\t\tthis->rt_waiter,\n\t\t\t\t\t\t\tthis->task);\n\t\t\tif (ret == 1) {\n\t\t\t\t/*\n\t\t\t\t * We got the lock. We do neither drop the\n\t\t\t\t * refcount on pi_state nor clear\n\t\t\t\t * this->pi_state because the waiter needs the\n\t\t\t\t * pi_state for cleaning up the user space\n\t\t\t\t * value. It will drop the refcount after\n\t\t\t\t * doing so.\n\t\t\t\t */\n\t\t\t\trequeue_pi_wake_futex(this, &key2, hb2);\n\t\t\t\tdrop_count++;\n\t\t\t\tcontinue;\n\t\t\t} else if (ret) {\n\t\t\t\t/*\n\t\t\t\t * rt_mutex_start_proxy_lock() detected a\n\t\t\t\t * potential deadlock when we tried to queue\n\t\t\t\t * that waiter. Drop the pi_state reference\n\t\t\t\t * which we took above and remove the pointer\n\t\t\t\t * to the state from the waiters futex_q\n\t\t\t\t * object.\n\t\t\t\t */\n\t\t\t\tthis->pi_state = NULL;\n\t\t\t\tput_pi_state(pi_state);\n\t\t\t\t/*\n\t\t\t\t * We stop queueing more waiters and let user\n\t\t\t\t * space deal with the mess.\n\t\t\t\t */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trequeue_futex(this, hb1, hb2, &key2);\n\t\tdrop_count++;\n\t}\n\n\t/*\n\t * We took an extra initial reference to the pi_state either\n\t * in futex_proxy_trylock_atomic() or in lookup_pi_state(). We\n\t * need to drop it here again.\n\t */\n\tput_pi_state(pi_state);\n\nout_unlock:\n\tdouble_unlock_hb(hb1, hb2);\n\twake_up_q(&wake_q);\n\thb_waiters_dec(hb2);\n\n\t/*\n\t * drop_futex_key_refs() must be called outside the spinlocks. During\n\t * the requeue we moved futex_q's from the hash bucket at key1 to the\n\t * one at key2 and updated their key pointer.  We no longer need to\n\t * hold the references to key1.\n\t */\n\twhile (--drop_count >= 0)\n\t\tdrop_futex_key_refs(&key1);\n\nout_put_keys:\n\tput_futex_key(&key2);\nout_put_key1:\n\tput_futex_key(&key1);\nout:\n\treturn ret ? ret : task_count;\n}\n\n/* The key must be already stored in q->key. */\nstatic inline struct futex_hash_bucket *queue_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = hash_futex(&q->key);\n\n\t/*\n\t * Increment the counter before taking the lock so that\n\t * a potential waker won't miss a to-be-slept task that is\n\t * waiting for the spinlock. This is safe as all queue_lock()\n\t * users end up calling queue_me(). Similarly, for housekeeping,\n\t * decrement the counter at queue_unlock() when some error has\n\t * occurred and we don't end up adding the task to the list.\n\t */\n\thb_waiters_inc(hb);\n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock); /* implies smp_mb(); (A) */\n\treturn hb;\n}\n\nstatic inline void\nqueue_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\thb_waiters_dec(hb);\n}\n\nstatic inline void __queue_me(struct futex_q *q, struct futex_hash_bucket *hb)\n{\n\tint prio;\n\n\t/*\n\t * The priority used to register this element is\n\t * - either the real thread-priority for the real-time threads\n\t * (i.e. threads with a priority lower than MAX_RT_PRIO)\n\t * - or MAX_RT_PRIO for non-RT threads.\n\t * Thus, all RT-threads are woken first in priority order, and\n\t * the others are woken last, in FIFO order.\n\t */\n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n}\n\n/**\n * queue_me() - Enqueue the futex_q on the futex_hash_bucket\n * @q:\tThe futex_q to enqueue\n * @hb:\tThe destination hash bucket\n *\n * The hb->lock must be held by the caller, and is released here. A call to\n * queue_me() is typically paired with exactly one call to unqueue_me().  The\n * exceptions involve the PI related operations, which may use unqueue_me_pi()\n * or nothing if the unqueue is done as part of the wake process and the unqueue\n * state is implicit in the state of woken task (see futex_wait_requeue_pi() for\n * an example).\n */\nstatic inline void queue_me(struct futex_q *q, struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\t__queue_me(q, hb);\n\tspin_unlock(&hb->lock);\n}\n\n/**\n * unqueue_me() - Remove the futex_q from its futex_hash_bucket\n * @q:\tThe futex_q to unqueue\n *\n * The q->lock_ptr must not be held by the caller. A call to unqueue_me() must\n * be paired with exactly one earlier call to queue_me().\n *\n * Return:\n *  - 1 - if the futex_q was still queued (and we removed unqueued it);\n *  - 0 - if the futex_q was already removed by the waking thread\n */\nstatic int unqueue_me(struct futex_q *q)\n{\n\tspinlock_t *lock_ptr;\n\tint ret = 0;\n\n\t/* In the common case we don't take the spinlock, which is nice. */\nretry:\n\t/*\n\t * q->lock_ptr can change between this read and the following spin_lock.\n\t * Use READ_ONCE to forbid the compiler from reloading q->lock_ptr and\n\t * optimizing lock_ptr out of the logic below.\n\t */\n\tlock_ptr = READ_ONCE(q->lock_ptr);\n\tif (lock_ptr != NULL) {\n\t\tspin_lock(lock_ptr);\n\t\t/*\n\t\t * q->lock_ptr can change between reading it and\n\t\t * spin_lock(), causing us to take the wrong lock.  This\n\t\t * corrects the race condition.\n\t\t *\n\t\t * Reasoning goes like this: if we have the wrong lock,\n\t\t * q->lock_ptr must have changed (maybe several times)\n\t\t * between reading it and the spin_lock().  It can\n\t\t * change again after the spin_lock() but only if it was\n\t\t * already changed before the spin_lock().  It cannot,\n\t\t * however, change back to the original value.  Therefore\n\t\t * we can detect whether we acquired the correct lock.\n\t\t */\n\t\tif (unlikely(lock_ptr != q->lock_ptr)) {\n\t\t\tspin_unlock(lock_ptr);\n\t\t\tgoto retry;\n\t\t}\n\t\t__unqueue_futex(q);\n\n\t\tBUG_ON(q->pi_state);\n\n\t\tspin_unlock(lock_ptr);\n\t\tret = 1;\n\t}\n\n\tdrop_futex_key_refs(&q->key);\n\treturn ret;\n}\n\n/*\n * PI futexes can not be requeued and must remove themself from the\n * hash bucket. The hash bucket lock (i.e. lock_ptr) is held on entry\n * and dropped here.\n */\nstatic void unqueue_me_pi(struct futex_q *q)\n\t__releases(q->lock_ptr)\n{\n\t__unqueue_futex(q);\n\n\tBUG_ON(!q->pi_state);\n\tput_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n\n\tspin_unlock(q->lock_ptr);\n}\n\nstatic int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,\n\t\t\t\tstruct task_struct *argowner)\n{\n\tstruct futex_pi_state *pi_state = q->pi_state;\n\tu32 uval, uninitialized_var(curval), newval;\n\tstruct task_struct *oldowner, *newowner;\n\tu32 newtid;\n\tint ret;\n\n\tlockdep_assert_held(q->lock_ptr);\n\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\toldowner = pi_state->owner;\n\t/* Owner died? */\n\tif (!pi_state->owner)\n\t\tnewtid |= FUTEX_OWNER_DIED;\n\n\t/*\n\t * We are here because either:\n\t *\n\t *  - we stole the lock and pi_state->owner needs updating to reflect\n\t *    that (@argowner == current),\n\t *\n\t * or:\n\t *\n\t *  - someone stole our lock and we need to fix things to point to the\n\t *    new owner (@argowner == NULL).\n\t *\n\t * Either way, we have to replace the TID in the user space variable.\n\t * This must be atomic as we have to preserve the owner died bit here.\n\t *\n\t * Note: We write the user space value _before_ changing the pi_state\n\t * because we can fault here. Imagine swapped out pages or a fork\n\t * that marked all the anonymous memory readonly for cow.\n\t *\n\t * Modifying pi_state _before_ the user space value would leave the\n\t * pi_state in an inconsistent state when we fault here, because we\n\t * need to drop the locks to handle the fault. This might be observed\n\t * in the PID check in lookup_pi_state.\n\t */\nretry:\n\tif (!argowner) {\n\t\tif (oldowner != current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (__rt_mutex_futex_trylock(&pi_state->pi_mutex)) {\n\t\t\t/* We got the lock after all, nothing to fix. */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Since we just failed the trylock; there must be an owner.\n\t\t */\n\t\tnewowner = rt_mutex_owner(&pi_state->pi_mutex);\n\t\tBUG_ON(!newowner);\n\t} else {\n\t\tWARN_ON_ONCE(argowner != current);\n\t\tif (oldowner == current) {\n\t\t\t/*\n\t\t\t * We raced against a concurrent self; things are\n\t\t\t * already fixed up. Nothing to do.\n\t\t\t */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tnewowner = argowner;\n\t}\n\n\tnewtid = task_pid_vnr(newowner) | FUTEX_WAITERS;\n\n\tif (get_futex_value_locked(&uval, uaddr))\n\t\tgoto handle_fault;\n\n\tfor (;;) {\n\t\tnewval = (uval & FUTEX_OWNER_DIED) | newtid;\n\n\t\tif (cmpxchg_futex_value_locked(&curval, uaddr, uval, newval))\n\t\t\tgoto handle_fault;\n\t\tif (curval == uval)\n\t\t\tbreak;\n\t\tuval = curval;\n\t}\n\n\t/*\n\t * We fixed up user space. Now we need to fix the pi_state\n\t * itself.\n\t */\n\tif (pi_state->owner != NULL) {\n\t\traw_spin_lock(&pi_state->owner->pi_lock);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock(&pi_state->owner->pi_lock);\n\t}\n\n\tpi_state->owner = newowner;\n\n\traw_spin_lock(&newowner->pi_lock);\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &newowner->pi_state_list);\n\traw_spin_unlock(&newowner->pi_lock);\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\n\treturn 0;\n\n\t/*\n\t * To handle the page fault we need to drop the locks here. That gives\n\t * the other task (either the highest priority waiter itself or the\n\t * task which stole the rtmutex) the chance to try the fixup of the\n\t * pi_state. So once we are back from handling the fault we need to\n\t * check the pi_state after reacquiring the locks and before trying to\n\t * do another fixup. When the fixup has been done already we simply\n\t * return.\n\t *\n\t * Note: we hold both hb->lock and pi_mutex->wait_lock. We can safely\n\t * drop hb->lock since the caller owns the hb -> futex_q relation.\n\t * Dropping the pi_mutex->wait_lock requires the state revalidate.\n\t */\nhandle_fault:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q->lock_ptr);\n\n\tret = fault_in_user_writeable(uaddr);\n\n\tspin_lock(q->lock_ptr);\n\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\n\t/*\n\t * Check if someone else fixed it for us:\n\t */\n\tif (pi_state->owner != oldowner) {\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tgoto retry;\n\nout_unlock:\n\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\treturn ret;\n}\n\nstatic long futex_wait_restart(struct restart_block *restart);\n\n/**\n * fixup_owner() - Post lock pi_state and corner case management\n * @uaddr:\tuser address of the futex\n * @q:\t\tfutex_q (contains pi_state and access to the rt_mutex)\n * @locked:\tif the attempt to take the rt_mutex succeeded (1) or not (0)\n *\n * After attempting to lock an rt_mutex, this function is called to cleanup\n * the pi_state owner as well as handle race conditions that may allow us to\n * acquire the lock. Must be called with the hb lock held.\n *\n * Return:\n *  -  1 - success, lock taken;\n *  -  0 - success, lock not taken;\n *  - <0 - on error (-EFAULT)\n */\nstatic int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tint ret = 0;\n\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\tret = fixup_pi_state_owner(uaddr, q, current);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current) {\n\t\tret = fixup_pi_state_owner(uaddr, q, NULL);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex.\n\t */\n\tif (rt_mutex_owner(&q->pi_state->pi_mutex) == current) {\n\t\tprintk(KERN_ERR \"fixup_owner: ret = %d pi-mutex: %p \"\n\t\t\t\t\"pi-state %p\\n\", ret,\n\t\t\t\tq->pi_state->pi_mutex.owner,\n\t\t\t\tq->pi_state->owner);\n\t}\n\nout:\n\treturn ret ? ret : locked;\n}\n\n/**\n * futex_wait_queue_me() - queue_me() and wait for wakeup, timeout, or signal\n * @hb:\t\tthe futex hash bucket, must be locked by the caller\n * @q:\t\tthe futex_q to queue up on\n * @timeout:\tthe prepared hrtimer_sleeper, or null for no timeout\n */\nstatic void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,\n\t\t\t\tstruct hrtimer_sleeper *timeout)\n{\n\t/*\n\t * The task state is guaranteed to be set before another task can\n\t * wake it. set_current_state() is implemented using smp_store_mb() and\n\t * queue_me() calls spin_unlock() upon completion, both serializing\n\t * access to the hash list and forcing another memory barrier.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tqueue_me(q, hb);\n\n\t/* Arm the timer */\n\tif (timeout)\n\t\thrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);\n\n\t/*\n\t * If we have been removed from the hash list, then another task\n\t * has tried to wake us, and we can skip the call to schedule().\n\t */\n\tif (likely(!plist_node_empty(&q->list))) {\n\t\t/*\n\t\t * If the timer has already expired, current will already be\n\t\t * flagged for rescheduling. Only call schedule if there\n\t\t * is no timeout, or if it has yet to expire.\n\t\t */\n\t\tif (!timeout || timeout->task)\n\t\t\tfreezable_schedule();\n\t}\n\t__set_current_state(TASK_RUNNING);\n}\n\n/**\n * futex_wait_setup() - Prepare to wait on a futex\n * @uaddr:\tthe futex userspace address\n * @val:\tthe expected value\n * @flags:\tfutex flags (FLAGS_SHARED, etc.)\n * @q:\t\tthe associated futex_q\n * @hb:\t\tstorage for hash_bucket pointer to be returned to caller\n *\n * Setup the futex_q and locate the hash_bucket.  Get the futex value and\n * compare it with the expected value.  Handle atomic faults internally.\n * Return with the hb lock held and a q.key reference on success, and unlocked\n * with no q.key reference on failure.\n *\n * Return:\n *  -  0 - uaddr contains val and hb has been locked;\n *  - <1 - -EFAULT or -EWOULDBLOCK (uaddr does not contain val) and hb is unlocked\n */\nstatic int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,\n\t\t\t   struct futex_q *q, struct futex_hash_bucket **hb)\n{\n\tu32 uval;\n\tint ret;\n\n\t/*\n\t * Access the page AFTER the hash-bucket is locked.\n\t * Order is important:\n\t *\n\t *   Userspace waiter: val = var; if (cond(val)) futex_wait(&var, val);\n\t *   Userspace waker:  if (cond(var)) { var = new; futex_wake(&var); }\n\t *\n\t * The basic logical guarantee of a futex is that it blocks ONLY\n\t * if cond(var) is known to be true at the time of blocking, for\n\t * any cond.  If we locked the hash-bucket after testing *uaddr, that\n\t * would open a race condition where we could block indefinitely with\n\t * cond(var) false, which would violate the guarantee.\n\t *\n\t * On the other hand, we insert q and release the hash-bucket only\n\t * after testing *uaddr.  This guarantees that futex_wait() will NOT\n\t * absorb a wakeup if *uaddr does not match the desired values\n\t * while the syscall executes.\n\t */\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, VERIFY_READ);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\nretry_private:\n\t*hb = queue_lock(q);\n\n\tret = get_futex_value_locked(&uval, uaddr);\n\n\tif (ret) {\n\t\tqueue_unlock(*hb);\n\n\t\tret = get_user(uval, uaddr);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (!(flags & FLAGS_SHARED))\n\t\t\tgoto retry_private;\n\n\t\tput_futex_key(&q->key);\n\t\tgoto retry;\n\t}\n\n\tif (uval != val) {\n\t\tqueue_unlock(*hb);\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout:\n\tif (ret)\n\t\tput_futex_key(&q->key);\n\treturn ret;\n}\n\nstatic int futex_wait(u32 __user *uaddr, unsigned int flags, u32 val,\n\t\t      ktime_t *abs_time, u32 bitset)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct restart_block *restart;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\tq.bitset = bitset;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\nretry:\n\t/*\n\t * Prepare to wait on uaddr. On success, holds hb lock and increments\n\t * q.key refs.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/* queue_me and wait for wakeup, timeout, or a signal. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\t/* If we were woken (and unqueued), we succeeded, whatever. */\n\tret = 0;\n\t/* unqueue_me() drops q.key ref */\n\tif (!unqueue_me(&q))\n\t\tgoto out;\n\tret = -ETIMEDOUT;\n\tif (to && !to->task)\n\t\tgoto out;\n\n\t/*\n\t * We expect signal_pending(current), but we might be the\n\t * victim of a spurious wakeup as well.\n\t */\n\tif (!signal_pending(current))\n\t\tgoto retry;\n\n\tret = -ERESTARTSYS;\n\tif (!abs_time)\n\t\tgoto out;\n\n\trestart = &current->restart_block;\n\trestart->fn = futex_wait_restart;\n\trestart->futex.uaddr = uaddr;\n\trestart->futex.val = val;\n\trestart->futex.time = *abs_time;\n\trestart->futex.bitset = bitset;\n\trestart->futex.flags = flags | FLAGS_HAS_TIMEOUT;\n\n\tret = -ERESTART_RESTARTBLOCK;\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}\n\n\nstatic long futex_wait_restart(struct restart_block *restart)\n{\n\tu32 __user *uaddr = restart->futex.uaddr;\n\tktime_t t, *tp = NULL;\n\n\tif (restart->futex.flags & FLAGS_HAS_TIMEOUT) {\n\t\tt = restart->futex.time;\n\t\ttp = &t;\n\t}\n\trestart->fn = do_no_restart_syscall;\n\n\treturn (long)futex_wait(uaddr, restart->futex.flags,\n\t\t\t\trestart->futex.val, tp, restart->futex.bitset);\n}\n\n\n/*\n * Userspace tried a 0 -> TID atomic transition of the futex value\n * and failed. The kernel side here does the whole locking operation:\n * if there are waiters then it will block as a consequence of relying\n * on rt-mutexes, it does PI, etc. (Due to races the kernel might see\n * a 0 value of the futex too.).\n *\n * Also serves as futex trylock_pi()'ing, and due semantics.\n */\nstatic int futex_lock_pi(u32 __user *uaddr, unsigned int flags,\n\t\t\t ktime_t *time, int trylock)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\n\tif (refill_pi_state_cache())\n\t\treturn -ENOMEM;\n\n\tif (time) {\n\t\tto = &timeout;\n\t\thrtimer_init_on_stack(&to->timer, CLOCK_REALTIME,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires(&to->timer, *time);\n\t}\n\nretry:\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q.key, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\nretry_private:\n\thb = queue_lock(&q);\n\n\tret = futex_lock_pi_atomic(uaddr, hb, &q.key, &q.pi_state, current, 0);\n\tif (unlikely(ret)) {\n\t\t/*\n\t\t * Atomic work succeeded and we got the lock,\n\t\t * or failed. Either way, we do _not_ block.\n\t\t */\n\t\tswitch (ret) {\n\t\tcase 1:\n\t\t\t/* We got the lock. */\n\t\t\tret = 0;\n\t\t\tgoto out_unlock_put_key;\n\t\tcase -EFAULT:\n\t\t\tgoto uaddr_faulted;\n\t\tcase -EAGAIN:\n\t\t\t/*\n\t\t\t * Two reasons for this:\n\t\t\t * - Task is exiting and we just wait for the\n\t\t\t *   exit to complete.\n\t\t\t * - The user space value changed.\n\t\t\t */\n\t\t\tqueue_unlock(hb);\n\t\t\tput_futex_key(&q.key);\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\t\tdefault:\n\t\t\tgoto out_unlock_put_key;\n\t\t}\n\t}\n\n\tWARN_ON(!q.pi_state);\n\n\t/*\n\t * Only actually queue now that the atomic ops are done:\n\t */\n\t__queue_me(&q, hb);\n\n\tif (trylock) {\n\t\tret = rt_mutex_futex_trylock(&q.pi_state->pi_mutex);\n\t\t/* Fixup the trylock return value: */\n\t\tret = ret ? 0 : -EWOULDBLOCK;\n\t\tgoto no_block;\n\t}\n\n\trt_mutex_init_waiter(&rt_waiter);\n\n\t/*\n\t * On PREEMPT_RT_FULL, when hb->lock becomes an rt_mutex, we must not\n\t * hold it while doing rt_mutex_start_proxy(), because then it will\n\t * include hb->lock in the blocking chain, even through we'll not in\n\t * fact hold it while blocking. This will lead it to report -EDEADLK\n\t * and BUG when futex_unlock_pi() interleaves with this.\n\t *\n\t * Therefore acquire wait_lock while holding hb->lock, but drop the\n\t * latter before calling rt_mutex_start_proxy_lock(). This still fully\n\t * serializes against futex_unlock_pi() as that does the exact same\n\t * lock handoff sequence.\n\t */\n\traw_spin_lock_irq(&q.pi_state->pi_mutex.wait_lock);\n\tspin_unlock(q.lock_ptr);\n\tret = __rt_mutex_start_proxy_lock(&q.pi_state->pi_mutex, &rt_waiter, current);\n\traw_spin_unlock_irq(&q.pi_state->pi_mutex.wait_lock);\n\n\tif (ret) {\n\t\tif (ret == 1)\n\t\t\tret = 0;\n\n\t\tspin_lock(q.lock_ptr);\n\t\tgoto no_block;\n\t}\n\n\n\tif (unlikely(to))\n\t\thrtimer_start_expires(&to->timer, HRTIMER_MODE_ABS);\n\n\tret = rt_mutex_wait_proxy_lock(&q.pi_state->pi_mutex, to, &rt_waiter);\n\n\tspin_lock(q.lock_ptr);\n\t/*\n\t * If we failed to acquire the lock (signal/timeout), we must\n\t * first acquire the hb->lock before removing the lock from the\n\t * rt_mutex waitqueue, such that we can keep the hb and rt_mutex\n\t * wait lists consistent.\n\t *\n\t * In particular; it is important that futex_unlock_pi() can not\n\t * observe this inconsistency.\n\t */\n\tif (ret && !rt_mutex_cleanup_proxy_lock(&q.pi_state->pi_mutex, &rt_waiter))\n\t\tret = 0;\n\nno_block:\n\t/*\n\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t * haven't already.\n\t */\n\tres = fixup_owner(uaddr, &q, !ret);\n\t/*\n\t * If fixup_owner() returned an error, proprogate that.  If it acquired\n\t * the lock, clear our -ETIMEDOUT or -EINTR.\n\t */\n\tif (res)\n\t\tret = (res < 0) ? res : 0;\n\n\t/*\n\t * If fixup_owner() faulted and was unable to handle the fault, unlock\n\t * it and return the fault to userspace.\n\t */\n\tif (ret && (rt_mutex_owner(&q.pi_state->pi_mutex) == current)) {\n\t\tpi_state = q.pi_state;\n\t\tget_pi_state(pi_state);\n\t}\n\n\t/* Unqueue and drop the lock */\n\tunqueue_me_pi(&q);\n\n\tif (pi_state) {\n\t\trt_mutex_futex_unlock(&pi_state->pi_mutex);\n\t\tput_pi_state(pi_state);\n\t}\n\n\tgoto out_put_key;\n\nout_unlock_put_key:\n\tqueue_unlock(hb);\n\nout_put_key:\n\tput_futex_key(&q.key);\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret != -EINTR ? ret : -ERESTARTNOINTR;\n\nuaddr_faulted:\n\tqueue_unlock(hb);\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (ret)\n\t\tgoto out_put_key;\n\n\tif (!(flags & FLAGS_SHARED))\n\t\tgoto retry_private;\n\n\tput_futex_key(&q.key);\n\tgoto retry;\n}\n\n/*\n * Userspace attempted a TID -> 0 atomic transition, and failed.\n * This is the in-kernel slowpath: we look up the PI state (if any),\n * and do the rt-mutex unlock.\n */\nstatic int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)\n{\n\tu32 uninitialized_var(curval), uval, vpid = task_pid_vnr(current);\n\tunion futex_key key = FUTEX_KEY_INIT;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q *top_waiter;\n\tint ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -EFAULT;\n\t/*\n\t * We release only a lock we actually own:\n\t */\n\tif ((uval & FUTEX_TID_MASK) != vpid)\n\t\treturn -EPERM;\n\n\tret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_WRITE);\n\tif (ret)\n\t\treturn ret;\n\n\thb = hash_futex(&key);\n\tspin_lock(&hb->lock);\n\n\t/*\n\t * Check waiters first. We do not trust user space values at\n\t * all and we at least want to know if user space fiddled\n\t * with the futex value instead of blindly unlocking.\n\t */\n\ttop_waiter = futex_top_waiter(hb, &key);\n\tif (top_waiter) {\n\t\tstruct futex_pi_state *pi_state = top_waiter->pi_state;\n\n\t\tret = -EINVAL;\n\t\tif (!pi_state)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If current does not own the pi_state then the futex is\n\t\t * inconsistent and user space fiddled with the futex value.\n\t\t */\n\t\tif (pi_state->owner != current)\n\t\t\tgoto out_unlock;\n\n\t\tget_pi_state(pi_state);\n\t\t/*\n\t\t * By taking wait_lock while still holding hb->lock, we ensure\n\t\t * there is no point where we hold neither; and therefore\n\t\t * wake_futex_pi() must observe a state consistent with what we\n\t\t * observed.\n\t\t */\n\t\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\t\tspin_unlock(&hb->lock);\n\n\t\t/* drops pi_state->pi_mutex.wait_lock */\n\t\tret = wake_futex_pi(uaddr, uval, pi_state);\n\n\t\tput_pi_state(pi_state);\n\n\t\t/*\n\t\t * Success, we're done! No tricky corner cases.\n\t\t */\n\t\tif (!ret)\n\t\t\tgoto out_putkey;\n\t\t/*\n\t\t * The atomic access to the futex value generated a\n\t\t * pagefault, so retry the user-access and the wakeup:\n\t\t */\n\t\tif (ret == -EFAULT)\n\t\t\tgoto pi_faulted;\n\t\t/*\n\t\t * A unconditional UNLOCK_PI op raced against a waiter\n\t\t * setting the FUTEX_WAITERS bit. Try again.\n\t\t */\n\t\tif (ret == -EAGAIN) {\n\t\t\tput_futex_key(&key);\n\t\t\tgoto retry;\n\t\t}\n\t\t/*\n\t\t * wake_futex_pi has detected invalid state. Tell user\n\t\t * space.\n\t\t */\n\t\tgoto out_putkey;\n\t}\n\n\t/*\n\t * We have no kernel internal state, i.e. no waiters in the\n\t * kernel. Waiters which are about to queue themselves are stuck\n\t * on hb->lock. So we can safely ignore them. We do neither\n\t * preserve the WAITERS bit not the OWNER_DIED one. We are the\n\t * owner.\n\t */\n\tif (cmpxchg_futex_value_locked(&curval, uaddr, uval, 0)) {\n\t\tspin_unlock(&hb->lock);\n\t\tgoto pi_faulted;\n\t}\n\n\t/*\n\t * If uval has changed, let user space handle it.\n\t */\n\tret = (curval == uval) ? 0 : -EAGAIN;\n\nout_unlock:\n\tspin_unlock(&hb->lock);\nout_putkey:\n\tput_futex_key(&key);\n\treturn ret;\n\npi_faulted:\n\tput_futex_key(&key);\n\n\tret = fault_in_user_writeable(uaddr);\n\tif (!ret)\n\t\tgoto retry;\n\n\treturn ret;\n}\n\n/**\n * handle_early_requeue_pi_wakeup() - Detect early wakeup on the initial futex\n * @hb:\t\tthe hash_bucket futex_q was original enqueued on\n * @q:\t\tthe futex_q woken while waiting to be requeued\n * @key2:\tthe futex_key of the requeue target futex\n * @timeout:\tthe timeout associated with the wait (NULL if none)\n *\n * Detect if the task was woken on the initial futex as opposed to the requeue\n * target futex.  If so, determine if it was a timeout or a signal that caused\n * the wakeup and return the appropriate error code to the caller.  Must be\n * called with the hb lock held.\n *\n * Return:\n *  -  0 = no early wakeup detected;\n *  - <0 = -ETIMEDOUT or -ERESTARTNOINTR\n */\nstatic inline\nint handle_early_requeue_pi_wakeup(struct futex_hash_bucket *hb,\n\t\t\t\t   struct futex_q *q, union futex_key *key2,\n\t\t\t\t   struct hrtimer_sleeper *timeout)\n{\n\tint ret = 0;\n\n\t/*\n\t * With the hb lock held, we avoid races while we process the wakeup.\n\t * We only need to hold hb (and not hb2) to ensure atomicity as the\n\t * wakeup code can't change q.key from uaddr to uaddr2 if we hold hb.\n\t * It can't be requeued from uaddr2 to something else since we don't\n\t * support a PI aware source futex for requeue.\n\t */\n\tif (!match_futex(&q->key, key2)) {\n\t\tWARN_ON(q->lock_ptr && (&hb->lock != q->lock_ptr));\n\t\t/*\n\t\t * We were woken prior to requeue by a timeout or a signal.\n\t\t * Unqueue the futex_q and determine which it was.\n\t\t */\n\t\tplist_del(&q->list, &hb->chain);\n\t\thb_waiters_dec(hb);\n\n\t\t/* Handle spurious wakeups gracefully */\n\t\tret = -EWOULDBLOCK;\n\t\tif (timeout && !timeout->task)\n\t\t\tret = -ETIMEDOUT;\n\t\telse if (signal_pending(current))\n\t\t\tret = -ERESTARTNOINTR;\n\t}\n\treturn ret;\n}\n\n/**\n * futex_wait_requeue_pi() - Wait on uaddr and take uaddr2\n * @uaddr:\tthe futex we initially wait on (non-pi)\n * @flags:\tfutex flags (FLAGS_SHARED, FLAGS_CLOCKRT, etc.), they must be\n *\t\tthe same type, no requeueing from private to shared, etc.\n * @val:\tthe expected value of uaddr\n * @abs_time:\tabsolute timeout\n * @bitset:\t32 bit wakeup bitset set by userspace, defaults to all\n * @uaddr2:\tthe pi futex we will take prior to returning to user-space\n *\n * The caller will wait on uaddr and will be requeued by futex_requeue() to\n * uaddr2 which must be PI aware and unique from uaddr.  Normal wakeup will wake\n * on uaddr2 and complete the acquisition of the rt_mutex prior to returning to\n * userspace.  This ensures the rt_mutex maintains an owner when it has waiters;\n * without one, the pi logic would not know which task to boost/deboost, if\n * there was a need to.\n *\n * We call schedule in futex_wait_queue_me() when we enqueue and return there\n * via the following--\n * 1) wakeup on uaddr2 after an atomic lock acquisition by futex_requeue()\n * 2) wakeup on uaddr2 after a requeue\n * 3) signal\n * 4) timeout\n *\n * If 3, cleanup and return -ERESTARTNOINTR.\n *\n * If 2, we may then block on trying to take the rt_mutex and return via:\n * 5) successful lock\n * 6) signal\n * 7) timeout\n * 8) other lock acquisition failure\n *\n * If 6, return -EWOULDBLOCK (restarting the syscall would do the same).\n *\n * If 4 or 7, we cleanup and return with -ETIMEDOUT.\n *\n * Return:\n *  -  0 - On success;\n *  - <0 - On error\n */\nstatic int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,\n\t\t\t\t u32 val, ktime_t *abs_time, u32 bitset,\n\t\t\t\t u32 __user *uaddr2)\n{\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct rt_mutex_waiter rt_waiter;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key2 = FUTEX_KEY_INIT;\n\tstruct futex_q q = futex_q_init;\n\tint res, ret;\n\n\tif (!IS_ENABLED(CONFIG_FUTEX_PI))\n\t\treturn -ENOSYS;\n\n\tif (uaddr == uaddr2)\n\t\treturn -EINVAL;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\n\t/*\n\t * The waiter is allocated on our stack, manipulated by the requeue\n\t * code while we sleep on uaddr.\n\t */\n\trt_mutex_init_waiter(&rt_waiter);\n\n\tret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\tq.bitset = bitset;\n\tq.rt_waiter = &rt_waiter;\n\tq.requeue_pi_key = &key2;\n\n\t/*\n\t * Prepare to wait on uaddr. On success, increments q.key (key1) ref\n\t * count.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out_key2;\n\n\t/*\n\t * The check above which compares uaddrs is not sufficient for\n\t * shared futexes. We need to compare the keys:\n\t */\n\tif (match_futex(&q.key, &key2)) {\n\t\tqueue_unlock(hb);\n\t\tret = -EINVAL;\n\t\tgoto out_put_keys;\n\t}\n\n\t/* Queue the futex_q, drop the hb lock, wait for wakeup. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\tspin_lock(&hb->lock);\n\tret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);\n\tspin_unlock(&hb->lock);\n\tif (ret)\n\t\tgoto out_put_keys;\n\n\t/*\n\t * In order for us to be here, we know our q.key == key2, and since\n\t * we took the hb->lock above, we also know that futex_requeue() has\n\t * completed and we no longer have to concern ourselves with a wakeup\n\t * race with the atomic proxy lock acquisition by the requeue code. The\n\t * futex_requeue dropped our key1 reference and incremented our key2\n\t * reference count.\n\t */\n\n\t/* Check if the requeue code acquired the second futex for us. */\n\tif (!q.rt_waiter) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case.\n\t\t */\n\t\tif (q.pi_state && (q.pi_state->owner != current)) {\n\t\t\tspin_lock(q.lock_ptr);\n\t\t\tret = fixup_pi_state_owner(uaddr2, &q, current);\n\t\t\tif (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {\n\t\t\t\tpi_state = q.pi_state;\n\t\t\t\tget_pi_state(pi_state);\n\t\t\t}\n\t\t\t/*\n\t\t\t * Drop the reference to the pi state which\n\t\t\t * the requeue_pi() code acquired for us.\n\t\t\t */\n\t\t\tput_pi_state(q.pi_state);\n\t\t\tspin_unlock(q.lock_ptr);\n\t\t}\n\t} else {\n\t\tstruct rt_mutex *pi_mutex;\n\n\t\t/*\n\t\t * We have been woken up by futex_unlock_pi(), a timeout, or a\n\t\t * signal.  futex_unlock_pi() will not destroy the lock_ptr nor\n\t\t * the pi_state.\n\t\t */\n\t\tWARN_ON(!q.pi_state);\n\t\tpi_mutex = &q.pi_state->pi_mutex;\n\t\tret = rt_mutex_wait_proxy_lock(pi_mutex, to, &rt_waiter);\n\n\t\tspin_lock(q.lock_ptr);\n\t\tif (ret && !rt_mutex_cleanup_proxy_lock(pi_mutex, &rt_waiter))\n\t\t\tret = 0;\n\n\t\tdebug_rt_mutex_free_waiter(&rt_waiter);\n\t\t/*\n\t\t * Fixup the pi_state owner and possibly acquire the lock if we\n\t\t * haven't already.\n\t\t */\n\t\tres = fixup_owner(uaddr2, &q, !ret);\n\t\t/*\n\t\t * If fixup_owner() returned an error, proprogate that.  If it\n\t\t * acquired the lock, clear -ETIMEDOUT or -EINTR.\n\t\t */\n\t\tif (res)\n\t\t\tret = (res < 0) ? res : 0;\n\n\t\t/*\n\t\t * If fixup_pi_state_owner() faulted and was unable to handle\n\t\t * the fault, unlock the rt_mutex and return the fault to\n\t\t * userspace.\n\t\t */\n\t\tif (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current) {\n\t\t\tpi_state = q.pi_state;\n\t\t\tget_pi_state(pi_state);\n\t\t}\n\n\t\t/* Unqueue and drop the lock. */\n\t\tunqueue_me_pi(&q);\n\t}\n\n\tif (pi_state) {\n\t\trt_mutex_futex_unlock(&pi_state->pi_mutex);\n\t\tput_pi_state(pi_state);\n\t}\n\n\tif (ret == -EINTR) {\n\t\t/*\n\t\t * We've already been requeued, but cannot restart by calling\n\t\t * futex_lock_pi() directly. We could restart this syscall, but\n\t\t * it would detect that the user space \"val\" changed and return\n\t\t * -EWOULDBLOCK.  Save the overhead of the restart and return\n\t\t * -EWOULDBLOCK directly.\n\t\t */\n\t\tret = -EWOULDBLOCK;\n\t}\n\nout_put_keys:\n\tput_futex_key(&q.key);\nout_key2:\n\tput_futex_key(&key2);\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}\n\n/*\n * Support for robust futexes: the kernel cleans up held futexes at\n * thread exit time.\n *\n * Implementation: user-space maintains a per-thread list of locks it\n * is holding. Upon do_exit(), the kernel carefully walks this list,\n * and marks all locks that are owned by this thread with the\n * FUTEX_OWNER_DIED bit, and wakes up a waiter (if any). The list is\n * always manipulated with the lock held, so the list is private and\n * per-thread. Userspace also maintains a per-thread 'list_op_pending'\n * field, to allow the kernel to clean up if the thread dies after\n * acquiring the lock, but just before it could have added itself to\n * the list. There can only be one such pending lock.\n */\n\n/**\n * sys_set_robust_list() - Set the robust-futex list head of a task\n * @head:\tpointer to the list-head\n * @len:\tlength of the list-head, as userspace expects\n */\nSYSCALL_DEFINE2(set_robust_list, struct robust_list_head __user *, head,\n\t\tsize_t, len)\n{\n\tif (!futex_cmpxchg_enabled)\n\t\treturn -ENOSYS;\n\t/*\n\t * The kernel knows only one size for now:\n\t */\n\tif (unlikely(len != sizeof(*head)))\n\t\treturn -EINVAL;\n\n\tcurrent->robust_list = head;\n\n\treturn 0;\n}\n\n/**\n * sys_get_robust_list() - Get the robust-futex list head of a task\n * @pid:\tpid of the process [zero for current task]\n * @head_ptr:\tpointer to a list-head pointer, the kernel fills it in\n * @len_ptr:\tpointer to a length field, the kernel fills in the header size\n */\nSYSCALL_DEFINE3(get_robust_list, int, pid,\n\t\tstruct robust_list_head __user * __user *, head_ptr,\n\t\tsize_t __user *, len_ptr)\n{\n\tstruct robust_list_head __user *head;\n\tunsigned long ret;\n\tstruct task_struct *p;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn -ENOSYS;\n\n\trcu_read_lock();\n\n\tret = -ESRCH;\n\tif (!pid)\n\t\tp = current;\n\telse {\n\t\tp = find_task_by_vpid(pid);\n\t\tif (!p)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = -EPERM;\n\tif (!ptrace_may_access(p, PTRACE_MODE_READ_REALCREDS))\n\t\tgoto err_unlock;\n\n\thead = p->robust_list;\n\trcu_read_unlock();\n\n\tif (put_user(sizeof(*head), len_ptr))\n\t\treturn -EFAULT;\n\treturn put_user(head, head_ptr);\n\nerr_unlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * Process a futex-list entry, check whether it's owned by the\n * dying task, and do notification if so:\n */\nint handle_futex_death(u32 __user *uaddr, struct task_struct *curr, int pi)\n{\n\tu32 uval, uninitialized_var(nval), mval;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -1;\n\n\tif ((uval & FUTEX_TID_MASK) == task_pid_vnr(curr)) {\n\t\t/*\n\t\t * Ok, this dying thread is truly holding a futex\n\t\t * of interest. Set the OWNER_DIED bit atomically\n\t\t * via cmpxchg, and if the value had FUTEX_WAITERS\n\t\t * set, wake up a waiter (if any). (We have to do a\n\t\t * futex_wake() even if OWNER_DIED is already set -\n\t\t * to handle the rare but possible case of recursive\n\t\t * thread-death.) The rest of the cleanup is done in\n\t\t * userspace.\n\t\t */\n\t\tmval = (uval & FUTEX_WAITERS) | FUTEX_OWNER_DIED;\n\t\t/*\n\t\t * We are not holding a lock here, but we want to have\n\t\t * the pagefault_disable/enable() protection because\n\t\t * we want to handle the fault gracefully. If the\n\t\t * access fails we try to fault in the futex with R/W\n\t\t * verification via get_user_pages. get_user() above\n\t\t * does not guarantee R/W access. If that fails we\n\t\t * give up and leave the futex locked.\n\t\t */\n\t\tif (cmpxchg_futex_value_locked(&nval, uaddr, uval, mval)) {\n\t\t\tif (fault_in_user_writeable(uaddr))\n\t\t\t\treturn -1;\n\t\t\tgoto retry;\n\t\t}\n\t\tif (nval != uval)\n\t\t\tgoto retry;\n\n\t\t/*\n\t\t * Wake robust non-PI futexes here. The wakeup of\n\t\t * PI futexes happens in exit_pi_state():\n\t\t */\n\t\tif (!pi && (uval & FUTEX_WAITERS))\n\t\t\tfutex_wake(uaddr, 1, 1, FUTEX_BITSET_MATCH_ANY);\n\t}\n\treturn 0;\n}\n\n/*\n * Fetch a robust-list pointer. Bit 0 signals PI futexes:\n */\nstatic inline int fetch_robust_entry(struct robust_list __user **entry,\n\t\t\t\t     struct robust_list __user * __user *head,\n\t\t\t\t     unsigned int *pi)\n{\n\tunsigned long uentry;\n\n\tif (get_user(uentry, (unsigned long __user *)head))\n\t\treturn -EFAULT;\n\n\t*entry = (void __user *)(uentry & ~1UL);\n\t*pi = uentry & 1;\n\n\treturn 0;\n}\n\n/*\n * Walk curr->robust_list (very carefully, it's a userspace list!)\n * and mark any locks found there dead, and notify any waiters.\n *\n * We silently return on any sign of list-walking problem.\n */\nvoid exit_robust_list(struct task_struct *curr)\n{\n\tstruct robust_list_head __user *head = curr->robust_list;\n\tstruct robust_list __user *entry, *next_entry, *pending;\n\tunsigned int limit = ROBUST_LIST_LIMIT, pi, pip;\n\tunsigned int uninitialized_var(next_pi);\n\tunsigned long futex_offset;\n\tint rc;\n\n\tif (!futex_cmpxchg_enabled)\n\t\treturn;\n\n\t/*\n\t * Fetch the list head (which was registered earlier, via\n\t * sys_set_robust_list()):\n\t */\n\tif (fetch_robust_entry(&entry, &head->list.next, &pi))\n\t\treturn;\n\t/*\n\t * Fetch the relative futex offset:\n\t */\n\tif (get_user(futex_offset, &head->futex_offset))\n\t\treturn;\n\t/*\n\t * Fetch any possibly pending lock-add first, and handle it\n\t * if it exists:\n\t */\n\tif (fetch_robust_entry(&pending, &head->list_op_pending, &pip))\n\t\treturn;\n\n\tnext_entry = NULL;\t/* avoid warning with gcc */\n\twhile (entry != &head->list) {\n\t\t/*\n\t\t * Fetch the next entry in the list before calling\n\t\t * handle_futex_death:\n\t\t */\n\t\trc = fetch_robust_entry(&next_entry, &entry->next, &next_pi);\n\t\t/*\n\t\t * A pending lock might already be on the list, so\n\t\t * don't process it twice:\n\t\t */\n\t\tif (entry != pending)\n\t\t\tif (handle_futex_death((void __user *)entry + futex_offset,\n\t\t\t\t\t\tcurr, pi))\n\t\t\t\treturn;\n\t\tif (rc)\n\t\t\treturn;\n\t\tentry = next_entry;\n\t\tpi = next_pi;\n\t\t/*\n\t\t * Avoid excessively long or circular lists:\n\t\t */\n\t\tif (!--limit)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (pending)\n\t\thandle_futex_death((void __user *)pending + futex_offset,\n\t\t\t\t   curr, pip);\n}\n\nlong do_futex(u32 __user *uaddr, int op, u32 val, ktime_t *timeout,\n\t\tu32 __user *uaddr2, u32 val2, u32 val3)\n{\n\tint cmd = op & FUTEX_CMD_MASK;\n\tunsigned int flags = 0;\n\n\tif (!(op & FUTEX_PRIVATE_FLAG))\n\t\tflags |= FLAGS_SHARED;\n\n\tif (op & FUTEX_CLOCK_REALTIME) {\n\t\tflags |= FLAGS_CLOCKRT;\n\t\tif (cmd != FUTEX_WAIT && cmd != FUTEX_WAIT_BITSET && \\\n\t\t    cmd != FUTEX_WAIT_REQUEUE_PI)\n\t\t\treturn -ENOSYS;\n\t}\n\n\tswitch (cmd) {\n\tcase FUTEX_LOCK_PI:\n\tcase FUTEX_UNLOCK_PI:\n\tcase FUTEX_TRYLOCK_PI:\n\tcase FUTEX_WAIT_REQUEUE_PI:\n\tcase FUTEX_CMP_REQUEUE_PI:\n\t\tif (!futex_cmpxchg_enabled)\n\t\t\treturn -ENOSYS;\n\t}\n\n\tswitch (cmd) {\n\tcase FUTEX_WAIT:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\tcase FUTEX_WAIT_BITSET:\n\t\treturn futex_wait(uaddr, flags, val, timeout, val3);\n\tcase FUTEX_WAKE:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\tcase FUTEX_WAKE_BITSET:\n\t\treturn futex_wake(uaddr, flags, val, val3);\n\tcase FUTEX_REQUEUE:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, NULL, 0);\n\tcase FUTEX_CMP_REQUEUE:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 0);\n\tcase FUTEX_WAKE_OP:\n\t\treturn futex_wake_op(uaddr, flags, uaddr2, val, val2, val3);\n\tcase FUTEX_LOCK_PI:\n\t\treturn futex_lock_pi(uaddr, flags, timeout, 0);\n\tcase FUTEX_UNLOCK_PI:\n\t\treturn futex_unlock_pi(uaddr, flags);\n\tcase FUTEX_TRYLOCK_PI:\n\t\treturn futex_lock_pi(uaddr, flags, NULL, 1);\n\tcase FUTEX_WAIT_REQUEUE_PI:\n\t\tval3 = FUTEX_BITSET_MATCH_ANY;\n\t\treturn futex_wait_requeue_pi(uaddr, flags, val, timeout, val3,\n\t\t\t\t\t     uaddr2);\n\tcase FUTEX_CMP_REQUEUE_PI:\n\t\treturn futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 1);\n\t}\n\treturn -ENOSYS;\n}\n\n\nSYSCALL_DEFINE6(futex, u32 __user *, uaddr, int, op, u32, val,\n\t\tstruct timespec __user *, utime, u32 __user *, uaddr2,\n\t\tu32, val3)\n{\n\tstruct timespec ts;\n\tktime_t t, *tp = NULL;\n\tu32 val2 = 0;\n\tint cmd = op & FUTEX_CMD_MASK;\n\n\tif (utime && (cmd == FUTEX_WAIT || cmd == FUTEX_LOCK_PI ||\n\t\t      cmd == FUTEX_WAIT_BITSET ||\n\t\t      cmd == FUTEX_WAIT_REQUEUE_PI)) {\n\t\tif (unlikely(should_fail_futex(!(op & FUTEX_PRIVATE_FLAG))))\n\t\t\treturn -EFAULT;\n\t\tif (copy_from_user(&ts, utime, sizeof(ts)) != 0)\n\t\t\treturn -EFAULT;\n\t\tif (!timespec_valid(&ts))\n\t\t\treturn -EINVAL;\n\n\t\tt = timespec_to_ktime(ts);\n\t\tif (cmd == FUTEX_WAIT)\n\t\t\tt = ktime_add_safe(ktime_get(), t);\n\t\ttp = &t;\n\t}\n\t/*\n\t * requeue parameter in 'utime' if cmd == FUTEX_*_REQUEUE_*.\n\t * number of waiters to wake in 'utime' if cmd == FUTEX_WAKE_OP.\n\t */\n\tif (cmd == FUTEX_REQUEUE || cmd == FUTEX_CMP_REQUEUE ||\n\t    cmd == FUTEX_CMP_REQUEUE_PI || cmd == FUTEX_WAKE_OP)\n\t\tval2 = (u32) (unsigned long) utime;\n\n\treturn do_futex(uaddr, op, val, tp, uaddr2, val2, val3);\n}\n\nstatic void __init futex_detect_cmpxchg(void)\n{\n#ifndef CONFIG_HAVE_FUTEX_CMPXCHG\n\tu32 curval;\n\n\t/*\n\t * This will fail and we want it. Some arch implementations do\n\t * runtime detection of the futex_atomic_cmpxchg_inatomic()\n\t * functionality. We want to know that before we call in any\n\t * of the complex code paths. Also we want to prevent\n\t * registration of robust lists in that case. NULL is\n\t * guaranteed to fault and we get -EFAULT on functional\n\t * implementation, the non-functional ones will return\n\t * -ENOSYS.\n\t */\n\tif (cmpxchg_futex_value_locked(&curval, NULL, 0, 0) == -EFAULT)\n\t\tfutex_cmpxchg_enabled = 1;\n#endif\n}\n\nstatic int __init futex_init(void)\n{\n\tunsigned int futex_shift;\n\tunsigned long i;\n\n#if CONFIG_BASE_SMALL\n\tfutex_hashsize = 16;\n#else\n\tfutex_hashsize = roundup_pow_of_two(256 * num_possible_cpus());\n#endif\n\n\tfutex_queues = alloc_large_system_hash(\"futex\", sizeof(*futex_queues),\n\t\t\t\t\t       futex_hashsize, 0,\n\t\t\t\t\t       futex_hashsize < 256 ? HASH_SMALL : 0,\n\t\t\t\t\t       &futex_shift, NULL,\n\t\t\t\t\t       futex_hashsize, futex_hashsize);\n\tfutex_hashsize = 1UL << futex_shift;\n\n\tfutex_detect_cmpxchg();\n\n\tfor (i = 0; i < futex_hashsize; i++) {\n\t\tatomic_set(&futex_queues[i].waiters, 0);\n\t\tplist_head_init(&futex_queues[i].chain);\n\t\tspin_lock_init(&futex_queues[i].lock);\n\t}\n\n\treturn 0;\n}\ncore_initcall(futex_init);\n"], "filenames": ["kernel/futex.c"], "buggy_code_start_loc": [1879], "buggy_code_end_loc": [1879], "fixing_code_start_loc": [1880], "fixing_code_end_loc": [1883], "type": "CWE-190", "message": "The futex_requeue function in kernel/futex.c in the Linux kernel before 4.14.15 might allow attackers to cause a denial of service (integer overflow) or possibly have unspecified other impact by triggering a negative wake or requeue value.", "other": {"cve": {"id": "CVE-2018-6927", "sourceIdentifier": "cve@mitre.org", "published": "2018-02-12T19:29:01.180", "lastModified": "2019-03-06T21:38:42.047", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The futex_requeue function in kernel/futex.c in the Linux kernel before 4.14.15 might allow attackers to cause a denial of service (integer overflow) or possibly have unspecified other impact by triggering a negative wake or requeue value."}, {"lang": "es", "value": "La funci\u00f3n futex_requeue en kernel/futex.c en el kernel de Linux, en versiones anteriores a la 4.14.15, podr\u00eda permitir que atacantes provoquen una denegaci\u00f3n de servicio (desbordamiento de enteros) o que puedan causar otro tipo de impacto sin especificar desencadenando un valor wake o requeue negativo."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": true, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.14.15", "matchCriteriaId": "65746862-9FE5-44AF-9622-50BBE586E601"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:esm:*:*:*", "matchCriteriaId": "8D305F7A-D159-4716-AB26-5E38BB5CD991"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:17.10:*:*:*:*:*:*:*", "matchCriteriaId": "9070C9D8-A14A-467F-8253-33B966C16886"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "16F59A04-14CF-49E2-9973-645477EA09DA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:virtualization_host:4.0:*:*:*:*:*:*:*", "matchCriteriaId": "BB28F9AF-3D06-4532-B397-96D7E4792503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_desktop:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "33C068A4-3780-4EAB-A937-6082DF847564"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "51EF4996-72F4-4FA4-814F-F5991E7A8318"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "B353CE99-D57C-465B-AAB0-73EF581127D1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_eus:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "BF77CDCF-B9C9-427D-B2BF-36650FB2148C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "B76AA310-FEC7-497F-AF04-C3EC1E76C4CC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_workstation:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "825ECE2D-E232-46E0-A047-074B34DB1E97"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=fbe0e839d1e22d88810f3ee3e2f1479be4c0aa4a", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/103023", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:0654", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:0676", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:1062", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/fbe0e839d1e22d88810f3ee3e2f1479be4c0aa4a", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2018/05/msg00000.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3619-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3619-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3697-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3697-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3698-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3698-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2018/dsa-4187", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.14.15", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/fbe0e839d1e22d88810f3ee3e2f1479be4c0aa4a"}}