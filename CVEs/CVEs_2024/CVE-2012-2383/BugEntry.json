{"buggy_code": ["/*\n * Copyright \u00a9 2008,2010 Intel Corporation\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n *\n * Authors:\n *    Eric Anholt <eric@anholt.net>\n *    Chris Wilson <chris@chris-wilson.co.uk>\n *\n */\n\n#include \"drmP.h\"\n#include \"drm.h\"\n#include \"i915_drm.h\"\n#include \"i915_drv.h\"\n#include \"i915_trace.h\"\n#include \"intel_drv.h\"\n#include <linux/dma_remapping.h>\n\nstruct change_domains {\n\tuint32_t invalidate_domains;\n\tuint32_t flush_domains;\n\tuint32_t flush_rings;\n\tuint32_t flips;\n};\n\n/*\n * Set the next domain for the specified object. This\n * may not actually perform the necessary flushing/invaliding though,\n * as that may want to be batched with other set_domain operations\n *\n * This is (we hope) the only really tricky part of gem. The goal\n * is fairly simple -- track which caches hold bits of the object\n * and make sure they remain coherent. A few concrete examples may\n * help to explain how it works. For shorthand, we use the notation\n * (read_domains, write_domain), e.g. (CPU, CPU) to indicate the\n * a pair of read and write domain masks.\n *\n * Case 1: the batch buffer\n *\n *\t1. Allocated\n *\t2. Written by CPU\n *\t3. Mapped to GTT\n *\t4. Read by GPU\n *\t5. Unmapped from GTT\n *\t6. Freed\n *\n *\tLet's take these a step at a time\n *\n *\t1. Allocated\n *\t\tPages allocated from the kernel may still have\n *\t\tcache contents, so we set them to (CPU, CPU) always.\n *\t2. Written by CPU (using pwrite)\n *\t\tThe pwrite function calls set_domain (CPU, CPU) and\n *\t\tthis function does nothing (as nothing changes)\n *\t3. Mapped by GTT\n *\t\tThis function asserts that the object is not\n *\t\tcurrently in any GPU-based read or write domains\n *\t4. Read by GPU\n *\t\ti915_gem_execbuffer calls set_domain (COMMAND, 0).\n *\t\tAs write_domain is zero, this function adds in the\n *\t\tcurrent read domains (CPU+COMMAND, 0).\n *\t\tflush_domains is set to CPU.\n *\t\tinvalidate_domains is set to COMMAND\n *\t\tclflush is run to get data out of the CPU caches\n *\t\tthen i915_dev_set_domain calls i915_gem_flush to\n *\t\temit an MI_FLUSH and drm_agp_chipset_flush\n *\t5. Unmapped from GTT\n *\t\ti915_gem_object_unbind calls set_domain (CPU, CPU)\n *\t\tflush_domains and invalidate_domains end up both zero\n *\t\tso no flushing/invalidating happens\n *\t6. Freed\n *\t\tyay, done\n *\n * Case 2: The shared render buffer\n *\n *\t1. Allocated\n *\t2. Mapped to GTT\n *\t3. Read/written by GPU\n *\t4. set_domain to (CPU,CPU)\n *\t5. Read/written by CPU\n *\t6. Read/written by GPU\n *\n *\t1. Allocated\n *\t\tSame as last example, (CPU, CPU)\n *\t2. Mapped to GTT\n *\t\tNothing changes (assertions find that it is not in the GPU)\n *\t3. Read/written by GPU\n *\t\texecbuffer calls set_domain (RENDER, RENDER)\n *\t\tflush_domains gets CPU\n *\t\tinvalidate_domains gets GPU\n *\t\tclflush (obj)\n *\t\tMI_FLUSH and drm_agp_chipset_flush\n *\t4. set_domain (CPU, CPU)\n *\t\tflush_domains gets GPU\n *\t\tinvalidate_domains gets CPU\n *\t\twait_rendering (obj) to make sure all drawing is complete.\n *\t\tThis will include an MI_FLUSH to get the data from GPU\n *\t\tto memory\n *\t\tclflush (obj) to invalidate the CPU cache\n *\t\tAnother MI_FLUSH in i915_gem_flush (eliminate this somehow?)\n *\t5. Read/written by CPU\n *\t\tcache lines are loaded and dirtied\n *\t6. Read written by GPU\n *\t\tSame as last GPU access\n *\n * Case 3: The constant buffer\n *\n *\t1. Allocated\n *\t2. Written by CPU\n *\t3. Read by GPU\n *\t4. Updated (written) by CPU again\n *\t5. Read by GPU\n *\n *\t1. Allocated\n *\t\t(CPU, CPU)\n *\t2. Written by CPU\n *\t\t(CPU, CPU)\n *\t3. Read by GPU\n *\t\t(CPU+RENDER, 0)\n *\t\tflush_domains = CPU\n *\t\tinvalidate_domains = RENDER\n *\t\tclflush (obj)\n *\t\tMI_FLUSH\n *\t\tdrm_agp_chipset_flush\n *\t4. Updated (written) by CPU again\n *\t\t(CPU, CPU)\n *\t\tflush_domains = 0 (no previous write domain)\n *\t\tinvalidate_domains = 0 (no new read domains)\n *\t5. Read by GPU\n *\t\t(CPU+RENDER, 0)\n *\t\tflush_domains = CPU\n *\t\tinvalidate_domains = RENDER\n *\t\tclflush (obj)\n *\t\tMI_FLUSH\n *\t\tdrm_agp_chipset_flush\n */\nstatic void\ni915_gem_object_set_to_gpu_domain(struct drm_i915_gem_object *obj,\n\t\t\t\t  struct intel_ring_buffer *ring,\n\t\t\t\t  struct change_domains *cd)\n{\n\tuint32_t invalidate_domains = 0, flush_domains = 0;\n\n\t/*\n\t * If the object isn't moving to a new write domain,\n\t * let the object stay in multiple read domains\n\t */\n\tif (obj->base.pending_write_domain == 0)\n\t\tobj->base.pending_read_domains |= obj->base.read_domains;\n\n\t/*\n\t * Flush the current write domain if\n\t * the new read domains don't match. Invalidate\n\t * any read domains which differ from the old\n\t * write domain\n\t */\n\tif (obj->base.write_domain &&\n\t    (((obj->base.write_domain != obj->base.pending_read_domains ||\n\t       obj->ring != ring)) ||\n\t     (obj->fenced_gpu_access && !obj->pending_fenced_gpu_access))) {\n\t\tflush_domains |= obj->base.write_domain;\n\t\tinvalidate_domains |=\n\t\t\tobj->base.pending_read_domains & ~obj->base.write_domain;\n\t}\n\t/*\n\t * Invalidate any read caches which may have\n\t * stale data. That is, any new read domains.\n\t */\n\tinvalidate_domains |= obj->base.pending_read_domains & ~obj->base.read_domains;\n\tif ((flush_domains | invalidate_domains) & I915_GEM_DOMAIN_CPU)\n\t\ti915_gem_clflush_object(obj);\n\n\tif (obj->base.pending_write_domain)\n\t\tcd->flips |= atomic_read(&obj->pending_flip);\n\n\t/* The actual obj->write_domain will be updated with\n\t * pending_write_domain after we emit the accumulated flush for all\n\t * of our domain changes in execbuffers (which clears objects'\n\t * write_domains).  So if we have a current write domain that we\n\t * aren't changing, set pending_write_domain to that.\n\t */\n\tif (flush_domains == 0 && obj->base.pending_write_domain == 0)\n\t\tobj->base.pending_write_domain = obj->base.write_domain;\n\n\tcd->invalidate_domains |= invalidate_domains;\n\tcd->flush_domains |= flush_domains;\n\tif (flush_domains & I915_GEM_GPU_DOMAINS)\n\t\tcd->flush_rings |= intel_ring_flag(obj->ring);\n\tif (invalidate_domains & I915_GEM_GPU_DOMAINS)\n\t\tcd->flush_rings |= intel_ring_flag(ring);\n}\n\nstruct eb_objects {\n\tint and;\n\tstruct hlist_head buckets[0];\n};\n\nstatic struct eb_objects *\neb_create(int size)\n{\n\tstruct eb_objects *eb;\n\tint count = PAGE_SIZE / sizeof(struct hlist_head) / 2;\n\twhile (count > size)\n\t\tcount >>= 1;\n\teb = kzalloc(count*sizeof(struct hlist_head) +\n\t\t     sizeof(struct eb_objects),\n\t\t     GFP_KERNEL);\n\tif (eb == NULL)\n\t\treturn eb;\n\n\teb->and = count - 1;\n\treturn eb;\n}\n\nstatic void\neb_reset(struct eb_objects *eb)\n{\n\tmemset(eb->buckets, 0, (eb->and+1)*sizeof(struct hlist_head));\n}\n\nstatic void\neb_add_object(struct eb_objects *eb, struct drm_i915_gem_object *obj)\n{\n\thlist_add_head(&obj->exec_node,\n\t\t       &eb->buckets[obj->exec_handle & eb->and]);\n}\n\nstatic struct drm_i915_gem_object *\neb_get_object(struct eb_objects *eb, unsigned long handle)\n{\n\tstruct hlist_head *head;\n\tstruct hlist_node *node;\n\tstruct drm_i915_gem_object *obj;\n\n\thead = &eb->buckets[handle & eb->and];\n\thlist_for_each(node, head) {\n\t\tobj = hlist_entry(node, struct drm_i915_gem_object, exec_node);\n\t\tif (obj->exec_handle == handle)\n\t\t\treturn obj;\n\t}\n\n\treturn NULL;\n}\n\nstatic void\neb_destroy(struct eb_objects *eb)\n{\n\tkfree(eb);\n}\n\nstatic int\ni915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,\n\t\t\t\t   struct eb_objects *eb,\n\t\t\t\t   struct drm_i915_gem_relocation_entry *reloc)\n{\n\tstruct drm_device *dev = obj->base.dev;\n\tstruct drm_gem_object *target_obj;\n\tuint32_t target_offset;\n\tint ret = -EINVAL;\n\n\t/* we've already hold a reference to all valid objects */\n\ttarget_obj = &eb_get_object(eb, reloc->target_handle)->base;\n\tif (unlikely(target_obj == NULL))\n\t\treturn -ENOENT;\n\n\ttarget_offset = to_intel_bo(target_obj)->gtt_offset;\n\n\t/* The target buffer should have appeared before us in the\n\t * exec_object list, so it should have a GTT space bound by now.\n\t */\n\tif (unlikely(target_offset == 0)) {\n\t\tDRM_DEBUG(\"No GTT space found for object %d\\n\",\n\t\t\t  reloc->target_handle);\n\t\treturn ret;\n\t}\n\n\t/* Validate that the target is in a valid r/w GPU domain */\n\tif (unlikely(reloc->write_domain & (reloc->write_domain - 1))) {\n\t\tDRM_DEBUG(\"reloc with multiple write domains: \"\n\t\t\t  \"obj %p target %d offset %d \"\n\t\t\t  \"read %08x write %08x\",\n\t\t\t  obj, reloc->target_handle,\n\t\t\t  (int) reloc->offset,\n\t\t\t  reloc->read_domains,\n\t\t\t  reloc->write_domain);\n\t\treturn ret;\n\t}\n\tif (unlikely((reloc->write_domain | reloc->read_domains)\n\t\t     & ~I915_GEM_GPU_DOMAINS)) {\n\t\tDRM_DEBUG(\"reloc with read/write non-GPU domains: \"\n\t\t\t  \"obj %p target %d offset %d \"\n\t\t\t  \"read %08x write %08x\",\n\t\t\t  obj, reloc->target_handle,\n\t\t\t  (int) reloc->offset,\n\t\t\t  reloc->read_domains,\n\t\t\t  reloc->write_domain);\n\t\treturn ret;\n\t}\n\tif (unlikely(reloc->write_domain && target_obj->pending_write_domain &&\n\t\t     reloc->write_domain != target_obj->pending_write_domain)) {\n\t\tDRM_DEBUG(\"Write domain conflict: \"\n\t\t\t  \"obj %p target %d offset %d \"\n\t\t\t  \"new %08x old %08x\\n\",\n\t\t\t  obj, reloc->target_handle,\n\t\t\t  (int) reloc->offset,\n\t\t\t  reloc->write_domain,\n\t\t\t  target_obj->pending_write_domain);\n\t\treturn ret;\n\t}\n\n\ttarget_obj->pending_read_domains |= reloc->read_domains;\n\ttarget_obj->pending_write_domain |= reloc->write_domain;\n\n\t/* If the relocation already has the right value in it, no\n\t * more work needs to be done.\n\t */\n\tif (target_offset == reloc->presumed_offset)\n\t\treturn 0;\n\n\t/* Check that the relocation address is valid... */\n\tif (unlikely(reloc->offset > obj->base.size - 4)) {\n\t\tDRM_DEBUG(\"Relocation beyond object bounds: \"\n\t\t\t  \"obj %p target %d offset %d size %d.\\n\",\n\t\t\t  obj, reloc->target_handle,\n\t\t\t  (int) reloc->offset,\n\t\t\t  (int) obj->base.size);\n\t\treturn ret;\n\t}\n\tif (unlikely(reloc->offset & 3)) {\n\t\tDRM_DEBUG(\"Relocation not 4-byte aligned: \"\n\t\t\t  \"obj %p target %d offset %d.\\n\",\n\t\t\t  obj, reloc->target_handle,\n\t\t\t  (int) reloc->offset);\n\t\treturn ret;\n\t}\n\n\treloc->delta += target_offset;\n\tif (obj->base.write_domain == I915_GEM_DOMAIN_CPU) {\n\t\tuint32_t page_offset = reloc->offset & ~PAGE_MASK;\n\t\tchar *vaddr;\n\n\t\tvaddr = kmap_atomic(obj->pages[reloc->offset >> PAGE_SHIFT]);\n\t\t*(uint32_t *)(vaddr + page_offset) = reloc->delta;\n\t\tkunmap_atomic(vaddr);\n\t} else {\n\t\tstruct drm_i915_private *dev_priv = dev->dev_private;\n\t\tuint32_t __iomem *reloc_entry;\n\t\tvoid __iomem *reloc_page;\n\n\t\t/* We can't wait for rendering with pagefaults disabled */\n\t\tif (obj->active && in_atomic())\n\t\t\treturn -EFAULT;\n\n\t\tret = i915_gem_object_set_to_gtt_domain(obj, 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/* Map the page containing the relocation we're going to perform.  */\n\t\treloc->offset += obj->gtt_offset;\n\t\treloc_page = io_mapping_map_atomic_wc(dev_priv->mm.gtt_mapping,\n\t\t\t\t\t\t      reloc->offset & PAGE_MASK);\n\t\treloc_entry = (uint32_t __iomem *)\n\t\t\t(reloc_page + (reloc->offset & ~PAGE_MASK));\n\t\tiowrite32(reloc->delta, reloc_entry);\n\t\tio_mapping_unmap_atomic(reloc_page);\n\t}\n\n\t/* and update the user's relocation entry */\n\treloc->presumed_offset = target_offset;\n\n\treturn 0;\n}\n\nstatic int\ni915_gem_execbuffer_relocate_object(struct drm_i915_gem_object *obj,\n\t\t\t\t    struct eb_objects *eb)\n{\n\tstruct drm_i915_gem_relocation_entry __user *user_relocs;\n\tstruct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\n\tint i, ret;\n\n\tuser_relocs = (void __user *)(uintptr_t)entry->relocs_ptr;\n\tfor (i = 0; i < entry->relocation_count; i++) {\n\t\tstruct drm_i915_gem_relocation_entry reloc;\n\n\t\tif (__copy_from_user_inatomic(&reloc,\n\t\t\t\t\t      user_relocs+i,\n\t\t\t\t\t      sizeof(reloc)))\n\t\t\treturn -EFAULT;\n\n\t\tret = i915_gem_execbuffer_relocate_entry(obj, eb, &reloc);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (__copy_to_user_inatomic(&user_relocs[i].presumed_offset,\n\t\t\t\t\t    &reloc.presumed_offset,\n\t\t\t\t\t    sizeof(reloc.presumed_offset)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nstatic int\ni915_gem_execbuffer_relocate_object_slow(struct drm_i915_gem_object *obj,\n\t\t\t\t\t struct eb_objects *eb,\n\t\t\t\t\t struct drm_i915_gem_relocation_entry *relocs)\n{\n\tconst struct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\n\tint i, ret;\n\n\tfor (i = 0; i < entry->relocation_count; i++) {\n\t\tret = i915_gem_execbuffer_relocate_entry(obj, eb, &relocs[i]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int\ni915_gem_execbuffer_relocate(struct drm_device *dev,\n\t\t\t     struct eb_objects *eb,\n\t\t\t     struct list_head *objects)\n{\n\tstruct drm_i915_gem_object *obj;\n\tint ret = 0;\n\n\t/* This is the fast path and we cannot handle a pagefault whilst\n\t * holding the struct mutex lest the user pass in the relocations\n\t * contained within a mmaped bo. For in such a case we, the page\n\t * fault handler would call i915_gem_fault() and we would try to\n\t * acquire the struct mutex again. Obviously this is bad and so\n\t * lockdep complains vehemently.\n\t */\n\tpagefault_disable();\n\tlist_for_each_entry(obj, objects, exec_list) {\n\t\tret = i915_gem_execbuffer_relocate_object(obj, eb);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tpagefault_enable();\n\n\treturn ret;\n}\n\n#define  __EXEC_OBJECT_HAS_FENCE (1<<31)\n\nstatic int\npin_and_fence_object(struct drm_i915_gem_object *obj,\n\t\t     struct intel_ring_buffer *ring)\n{\n\tstruct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\n\tbool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;\n\tbool need_fence, need_mappable;\n\tint ret;\n\n\tneed_fence =\n\t\thas_fenced_gpu_access &&\n\t\tentry->flags & EXEC_OBJECT_NEEDS_FENCE &&\n\t\tobj->tiling_mode != I915_TILING_NONE;\n\tneed_mappable =\n\t\tentry->relocation_count ? true : need_fence;\n\n\tret = i915_gem_object_pin(obj, entry->alignment, need_mappable);\n\tif (ret)\n\t\treturn ret;\n\n\tif (has_fenced_gpu_access) {\n\t\tif (entry->flags & EXEC_OBJECT_NEEDS_FENCE) {\n\t\t\tif (obj->tiling_mode) {\n\t\t\t\tret = i915_gem_object_get_fence(obj, ring);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto err_unpin;\n\n\t\t\t\tentry->flags |= __EXEC_OBJECT_HAS_FENCE;\n\t\t\t\ti915_gem_object_pin_fence(obj);\n\t\t\t} else {\n\t\t\t\tret = i915_gem_object_put_fence(obj);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto err_unpin;\n\t\t\t}\n\t\t\tobj->pending_fenced_gpu_access = true;\n\t\t}\n\t}\n\n\tentry->offset = obj->gtt_offset;\n\treturn 0;\n\nerr_unpin:\n\ti915_gem_object_unpin(obj);\n\treturn ret;\n}\n\nstatic int\ni915_gem_execbuffer_reserve(struct intel_ring_buffer *ring,\n\t\t\t    struct drm_file *file,\n\t\t\t    struct list_head *objects)\n{\n\tdrm_i915_private_t *dev_priv = ring->dev->dev_private;\n\tstruct drm_i915_gem_object *obj;\n\tint ret, retry;\n\tbool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;\n\tstruct list_head ordered_objects;\n\n\tINIT_LIST_HEAD(&ordered_objects);\n\twhile (!list_empty(objects)) {\n\t\tstruct drm_i915_gem_exec_object2 *entry;\n\t\tbool need_fence, need_mappable;\n\n\t\tobj = list_first_entry(objects,\n\t\t\t\t       struct drm_i915_gem_object,\n\t\t\t\t       exec_list);\n\t\tentry = obj->exec_entry;\n\n\t\tneed_fence =\n\t\t\thas_fenced_gpu_access &&\n\t\t\tentry->flags & EXEC_OBJECT_NEEDS_FENCE &&\n\t\t\tobj->tiling_mode != I915_TILING_NONE;\n\t\tneed_mappable =\n\t\t\tentry->relocation_count ? true : need_fence;\n\n\t\tif (need_mappable)\n\t\t\tlist_move(&obj->exec_list, &ordered_objects);\n\t\telse\n\t\t\tlist_move_tail(&obj->exec_list, &ordered_objects);\n\n\t\tobj->base.pending_read_domains = 0;\n\t\tobj->base.pending_write_domain = 0;\n\t}\n\tlist_splice(&ordered_objects, objects);\n\n\t/* Attempt to pin all of the buffers into the GTT.\n\t * This is done in 3 phases:\n\t *\n\t * 1a. Unbind all objects that do not match the GTT constraints for\n\t *     the execbuffer (fenceable, mappable, alignment etc).\n\t * 1b. Increment pin count for already bound objects.\n\t * 2.  Bind new objects.\n\t * 3.  Decrement pin count.\n\t *\n\t * This avoid unnecessary unbinding of later objects in order to makr\n\t * room for the earlier objects *unless* we need to defragment.\n\t */\n\tretry = 0;\n\tdo {\n\t\tret = 0;\n\n\t\t/* Unbind any ill-fitting objects or pin. */\n\t\tlist_for_each_entry(obj, objects, exec_list) {\n\t\t\tstruct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\n\t\t\tbool need_fence, need_mappable;\n\n\t\t\tif (!obj->gtt_space)\n\t\t\t\tcontinue;\n\n\t\t\tneed_fence =\n\t\t\t\thas_fenced_gpu_access &&\n\t\t\t\tentry->flags & EXEC_OBJECT_NEEDS_FENCE &&\n\t\t\t\tobj->tiling_mode != I915_TILING_NONE;\n\t\t\tneed_mappable =\n\t\t\t\tentry->relocation_count ? true : need_fence;\n\n\t\t\tif ((entry->alignment && obj->gtt_offset & (entry->alignment - 1)) ||\n\t\t\t    (need_mappable && !obj->map_and_fenceable))\n\t\t\t\tret = i915_gem_object_unbind(obj);\n\t\t\telse\n\t\t\t\tret = pin_and_fence_object(obj, ring);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t}\n\n\t\t/* Bind fresh objects */\n\t\tlist_for_each_entry(obj, objects, exec_list) {\n\t\t\tif (obj->gtt_space)\n\t\t\t\tcontinue;\n\n\t\t\tret = pin_and_fence_object(obj, ring);\n\t\t\tif (ret) {\n\t\t\t\tint ret_ignore;\n\n\t\t\t\t/* This can potentially raise a harmless\n\t\t\t\t * -EINVAL if we failed to bind in the above\n\t\t\t\t * call. It cannot raise -EINTR since we know\n\t\t\t\t * that the bo is freshly bound and so will\n\t\t\t\t * not need to be flushed or waited upon.\n\t\t\t\t */\n\t\t\t\tret_ignore = i915_gem_object_unbind(obj);\n\t\t\t\t(void)ret_ignore;\n\t\t\t\tWARN_ON(obj->gtt_space);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Decrement pin count for bound objects */\n\t\tlist_for_each_entry(obj, objects, exec_list) {\n\t\t\tstruct drm_i915_gem_exec_object2 *entry;\n\n\t\t\tif (!obj->gtt_space)\n\t\t\t\tcontinue;\n\n\t\t\tentry = obj->exec_entry;\n\t\t\tif (entry->flags & __EXEC_OBJECT_HAS_FENCE) {\n\t\t\t\ti915_gem_object_unpin_fence(obj);\n\t\t\t\tentry->flags &= ~__EXEC_OBJECT_HAS_FENCE;\n\t\t\t}\n\n\t\t\ti915_gem_object_unpin(obj);\n\n\t\t\t/* ... and ensure ppgtt mapping exist if needed. */\n\t\t\tif (dev_priv->mm.aliasing_ppgtt && !obj->has_aliasing_ppgtt_mapping) {\n\t\t\t\ti915_ppgtt_bind_object(dev_priv->mm.aliasing_ppgtt,\n\t\t\t\t\t\t       obj, obj->cache_level);\n\n\t\t\t\tobj->has_aliasing_ppgtt_mapping = 1;\n\t\t\t}\n\t\t}\n\n\t\tif (ret != -ENOSPC || retry > 1)\n\t\t\treturn ret;\n\n\t\t/* First attempt, just clear anything that is purgeable.\n\t\t * Second attempt, clear the entire GTT.\n\t\t */\n\t\tret = i915_gem_evict_everything(ring->dev, retry == 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tretry++;\n\t} while (1);\n\nerr:\n\tlist_for_each_entry_continue_reverse(obj, objects, exec_list) {\n\t\tstruct drm_i915_gem_exec_object2 *entry;\n\n\t\tif (!obj->gtt_space)\n\t\t\tcontinue;\n\n\t\tentry = obj->exec_entry;\n\t\tif (entry->flags & __EXEC_OBJECT_HAS_FENCE) {\n\t\t\ti915_gem_object_unpin_fence(obj);\n\t\t\tentry->flags &= ~__EXEC_OBJECT_HAS_FENCE;\n\t\t}\n\n\t\ti915_gem_object_unpin(obj);\n\t}\n\n\treturn ret;\n}\n\nstatic int\ni915_gem_execbuffer_relocate_slow(struct drm_device *dev,\n\t\t\t\t  struct drm_file *file,\n\t\t\t\t  struct intel_ring_buffer *ring,\n\t\t\t\t  struct list_head *objects,\n\t\t\t\t  struct eb_objects *eb,\n\t\t\t\t  struct drm_i915_gem_exec_object2 *exec,\n\t\t\t\t  int count)\n{\n\tstruct drm_i915_gem_relocation_entry *reloc;\n\tstruct drm_i915_gem_object *obj;\n\tint *reloc_offset;\n\tint i, total, ret;\n\n\t/* We may process another execbuffer during the unlock... */\n\twhile (!list_empty(objects)) {\n\t\tobj = list_first_entry(objects,\n\t\t\t\t       struct drm_i915_gem_object,\n\t\t\t\t       exec_list);\n\t\tlist_del_init(&obj->exec_list);\n\t\tdrm_gem_object_unreference(&obj->base);\n\t}\n\n\tmutex_unlock(&dev->struct_mutex);\n\n\ttotal = 0;\n\tfor (i = 0; i < count; i++)\n\t\ttotal += exec[i].relocation_count;\n\n\treloc_offset = drm_malloc_ab(count, sizeof(*reloc_offset));\n\treloc = drm_malloc_ab(total, sizeof(*reloc));\n\tif (reloc == NULL || reloc_offset == NULL) {\n\t\tdrm_free_large(reloc);\n\t\tdrm_free_large(reloc_offset);\n\t\tmutex_lock(&dev->struct_mutex);\n\t\treturn -ENOMEM;\n\t}\n\n\ttotal = 0;\n\tfor (i = 0; i < count; i++) {\n\t\tstruct drm_i915_gem_relocation_entry __user *user_relocs;\n\n\t\tuser_relocs = (void __user *)(uintptr_t)exec[i].relocs_ptr;\n\n\t\tif (copy_from_user(reloc+total, user_relocs,\n\t\t\t\t   exec[i].relocation_count * sizeof(*reloc))) {\n\t\t\tret = -EFAULT;\n\t\t\tmutex_lock(&dev->struct_mutex);\n\t\t\tgoto err;\n\t\t}\n\n\t\treloc_offset[i] = total;\n\t\ttotal += exec[i].relocation_count;\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret) {\n\t\tmutex_lock(&dev->struct_mutex);\n\t\tgoto err;\n\t}\n\n\t/* reacquire the objects */\n\teb_reset(eb);\n\tfor (i = 0; i < count; i++) {\n\t\tobj = to_intel_bo(drm_gem_object_lookup(dev, file,\n\t\t\t\t\t\t\texec[i].handle));\n\t\tif (&obj->base == NULL) {\n\t\t\tDRM_DEBUG(\"Invalid object handle %d at index %d\\n\",\n\t\t\t\t   exec[i].handle, i);\n\t\t\tret = -ENOENT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tlist_add_tail(&obj->exec_list, objects);\n\t\tobj->exec_handle = exec[i].handle;\n\t\tobj->exec_entry = &exec[i];\n\t\teb_add_object(eb, obj);\n\t}\n\n\tret = i915_gem_execbuffer_reserve(ring, file, objects);\n\tif (ret)\n\t\tgoto err;\n\n\tlist_for_each_entry(obj, objects, exec_list) {\n\t\tint offset = obj->exec_entry - exec;\n\t\tret = i915_gem_execbuffer_relocate_object_slow(obj, eb,\n\t\t\t\t\t\t\t       reloc + reloc_offset[offset]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\t/* Leave the user relocations as are, this is the painfully slow path,\n\t * and we want to avoid the complication of dropping the lock whilst\n\t * having buffers reserved in the aperture and so causing spurious\n\t * ENOSPC for random operations.\n\t */\n\nerr:\n\tdrm_free_large(reloc);\n\tdrm_free_large(reloc_offset);\n\treturn ret;\n}\n\nstatic int\ni915_gem_execbuffer_flush(struct drm_device *dev,\n\t\t\t  uint32_t invalidate_domains,\n\t\t\t  uint32_t flush_domains,\n\t\t\t  uint32_t flush_rings)\n{\n\tdrm_i915_private_t *dev_priv = dev->dev_private;\n\tint i, ret;\n\n\tif (flush_domains & I915_GEM_DOMAIN_CPU)\n\t\tintel_gtt_chipset_flush();\n\n\tif (flush_domains & I915_GEM_DOMAIN_GTT)\n\t\twmb();\n\n\tif ((flush_domains | invalidate_domains) & I915_GEM_GPU_DOMAINS) {\n\t\tfor (i = 0; i < I915_NUM_RINGS; i++)\n\t\t\tif (flush_rings & (1 << i)) {\n\t\t\t\tret = i915_gem_flush_ring(&dev_priv->ring[i],\n\t\t\t\t\t\t\t  invalidate_domains,\n\t\t\t\t\t\t\t  flush_domains);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic bool\nintel_enable_semaphores(struct drm_device *dev)\n{\n\tif (INTEL_INFO(dev)->gen < 6)\n\t\treturn 0;\n\n\tif (i915_semaphores >= 0)\n\t\treturn i915_semaphores;\n\n\t/* Disable semaphores on SNB */\n\tif (INTEL_INFO(dev)->gen == 6)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int\ni915_gem_execbuffer_sync_rings(struct drm_i915_gem_object *obj,\n\t\t\t       struct intel_ring_buffer *to)\n{\n\tstruct intel_ring_buffer *from = obj->ring;\n\tu32 seqno;\n\tint ret, idx;\n\n\tif (from == NULL || to == from)\n\t\treturn 0;\n\n\t/* XXX gpu semaphores are implicated in various hard hangs on SNB */\n\tif (!intel_enable_semaphores(obj->base.dev))\n\t\treturn i915_gem_object_wait_rendering(obj);\n\n\tidx = intel_ring_sync_index(from, to);\n\n\tseqno = obj->last_rendering_seqno;\n\tif (seqno <= from->sync_seqno[idx])\n\t\treturn 0;\n\n\tif (seqno == from->outstanding_lazy_request) {\n\t\tstruct drm_i915_gem_request *request;\n\n\t\trequest = kzalloc(sizeof(*request), GFP_KERNEL);\n\t\tif (request == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tret = i915_add_request(from, NULL, request);\n\t\tif (ret) {\n\t\t\tkfree(request);\n\t\t\treturn ret;\n\t\t}\n\n\t\tseqno = request->seqno;\n\t}\n\n\tfrom->sync_seqno[idx] = seqno;\n\n\treturn to->sync_to(to, from, seqno - 1);\n}\n\nstatic int\ni915_gem_execbuffer_wait_for_flips(struct intel_ring_buffer *ring, u32 flips)\n{\n\tu32 plane, flip_mask;\n\tint ret;\n\n\t/* Check for any pending flips. As we only maintain a flip queue depth\n\t * of 1, we can simply insert a WAIT for the next display flip prior\n\t * to executing the batch and avoid stalling the CPU.\n\t */\n\n\tfor (plane = 0; flips >> plane; plane++) {\n\t\tif (((flips >> plane) & 1) == 0)\n\t\t\tcontinue;\n\n\t\tif (plane)\n\t\t\tflip_mask = MI_WAIT_FOR_PLANE_B_FLIP;\n\t\telse\n\t\t\tflip_mask = MI_WAIT_FOR_PLANE_A_FLIP;\n\n\t\tret = intel_ring_begin(ring, 2);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tintel_ring_emit(ring, MI_WAIT_FOR_EVENT | flip_mask);\n\t\tintel_ring_emit(ring, MI_NOOP);\n\t\tintel_ring_advance(ring);\n\t}\n\n\treturn 0;\n}\n\n\nstatic int\ni915_gem_execbuffer_move_to_gpu(struct intel_ring_buffer *ring,\n\t\t\t\tstruct list_head *objects)\n{\n\tstruct drm_i915_gem_object *obj;\n\tstruct change_domains cd;\n\tint ret;\n\n\tmemset(&cd, 0, sizeof(cd));\n\tlist_for_each_entry(obj, objects, exec_list)\n\t\ti915_gem_object_set_to_gpu_domain(obj, ring, &cd);\n\n\tif (cd.invalidate_domains | cd.flush_domains) {\n\t\tret = i915_gem_execbuffer_flush(ring->dev,\n\t\t\t\t\t\tcd.invalidate_domains,\n\t\t\t\t\t\tcd.flush_domains,\n\t\t\t\t\t\tcd.flush_rings);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (cd.flips) {\n\t\tret = i915_gem_execbuffer_wait_for_flips(ring, cd.flips);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tlist_for_each_entry(obj, objects, exec_list) {\n\t\tret = i915_gem_execbuffer_sync_rings(obj, ring);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic bool\ni915_gem_check_execbuffer(struct drm_i915_gem_execbuffer2 *exec)\n{\n\treturn ((exec->batch_start_offset | exec->batch_len) & 0x7) == 0;\n}\n\nstatic int\nvalidate_exec_list(struct drm_i915_gem_exec_object2 *exec,\n\t\t   int count)\n{\n\tint i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tchar __user *ptr = (char __user *)(uintptr_t)exec[i].relocs_ptr;\n\t\tint length; /* limited by fault_in_pages_readable() */\n\n\t\t/* First check for malicious input causing overflow */\n\t\tif (exec[i].relocation_count >\n\t\t    INT_MAX / sizeof(struct drm_i915_gem_relocation_entry))\n\t\t\treturn -EINVAL;\n\n\t\tlength = exec[i].relocation_count *\n\t\t\tsizeof(struct drm_i915_gem_relocation_entry);\n\t\tif (!access_ok(VERIFY_READ, ptr, length))\n\t\t\treturn -EFAULT;\n\n\t\t/* we may also need to update the presumed offsets */\n\t\tif (!access_ok(VERIFY_WRITE, ptr, length))\n\t\t\treturn -EFAULT;\n\n\t\tif (fault_in_pages_readable(ptr, length))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nstatic void\ni915_gem_execbuffer_move_to_active(struct list_head *objects,\n\t\t\t\t   struct intel_ring_buffer *ring,\n\t\t\t\t   u32 seqno)\n{\n\tstruct drm_i915_gem_object *obj;\n\n\tlist_for_each_entry(obj, objects, exec_list) {\n\t\t  u32 old_read = obj->base.read_domains;\n\t\t  u32 old_write = obj->base.write_domain;\n\n\n\t\tobj->base.read_domains = obj->base.pending_read_domains;\n\t\tobj->base.write_domain = obj->base.pending_write_domain;\n\t\tobj->fenced_gpu_access = obj->pending_fenced_gpu_access;\n\n\t\ti915_gem_object_move_to_active(obj, ring, seqno);\n\t\tif (obj->base.write_domain) {\n\t\t\tobj->dirty = 1;\n\t\t\tobj->pending_gpu_write = true;\n\t\t\tlist_move_tail(&obj->gpu_write_list,\n\t\t\t\t       &ring->gpu_write_list);\n\t\t\tintel_mark_busy(ring->dev, obj);\n\t\t}\n\n\t\ttrace_i915_gem_object_change_domain(obj, old_read, old_write);\n\t}\n}\n\nstatic void\ni915_gem_execbuffer_retire_commands(struct drm_device *dev,\n\t\t\t\t    struct drm_file *file,\n\t\t\t\t    struct intel_ring_buffer *ring)\n{\n\tstruct drm_i915_gem_request *request;\n\tu32 invalidate;\n\n\t/*\n\t * Ensure that the commands in the batch buffer are\n\t * finished before the interrupt fires.\n\t *\n\t * The sampler always gets flushed on i965 (sigh).\n\t */\n\tinvalidate = I915_GEM_DOMAIN_COMMAND;\n\tif (INTEL_INFO(dev)->gen >= 4)\n\t\tinvalidate |= I915_GEM_DOMAIN_SAMPLER;\n\tif (ring->flush(ring, invalidate, 0)) {\n\t\ti915_gem_next_request_seqno(ring);\n\t\treturn;\n\t}\n\n\t/* Add a breadcrumb for the completion of the batch buffer */\n\trequest = kzalloc(sizeof(*request), GFP_KERNEL);\n\tif (request == NULL || i915_add_request(ring, file, request)) {\n\t\ti915_gem_next_request_seqno(ring);\n\t\tkfree(request);\n\t}\n}\n\nstatic int\ni915_reset_gen7_sol_offsets(struct drm_device *dev,\n\t\t\t    struct intel_ring_buffer *ring)\n{\n\tdrm_i915_private_t *dev_priv = dev->dev_private;\n\tint ret, i;\n\n\tif (!IS_GEN7(dev) || ring != &dev_priv->ring[RCS])\n\t\treturn 0;\n\n\tret = intel_ring_begin(ring, 4 * 3);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < 4; i++) {\n\t\tintel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));\n\t\tintel_ring_emit(ring, GEN7_SO_WRITE_OFFSET(i));\n\t\tintel_ring_emit(ring, 0);\n\t}\n\n\tintel_ring_advance(ring);\n\n\treturn 0;\n}\n\nstatic int\ni915_gem_do_execbuffer(struct drm_device *dev, void *data,\n\t\t       struct drm_file *file,\n\t\t       struct drm_i915_gem_execbuffer2 *args,\n\t\t       struct drm_i915_gem_exec_object2 *exec)\n{\n\tdrm_i915_private_t *dev_priv = dev->dev_private;\n\tstruct list_head objects;\n\tstruct eb_objects *eb;\n\tstruct drm_i915_gem_object *batch_obj;\n\tstruct drm_clip_rect *cliprects = NULL;\n\tstruct intel_ring_buffer *ring;\n\tu32 exec_start, exec_len;\n\tu32 seqno;\n\tu32 mask;\n\tint ret, mode, i;\n\n\tif (!i915_gem_check_execbuffer(args)) {\n\t\tDRM_DEBUG(\"execbuf with invalid offset/length\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = validate_exec_list(exec, args->buffer_count);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (args->flags & I915_EXEC_RING_MASK) {\n\tcase I915_EXEC_DEFAULT:\n\tcase I915_EXEC_RENDER:\n\t\tring = &dev_priv->ring[RCS];\n\t\tbreak;\n\tcase I915_EXEC_BSD:\n\t\tif (!HAS_BSD(dev)) {\n\t\t\tDRM_DEBUG(\"execbuf with invalid ring (BSD)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tring = &dev_priv->ring[VCS];\n\t\tbreak;\n\tcase I915_EXEC_BLT:\n\t\tif (!HAS_BLT(dev)) {\n\t\t\tDRM_DEBUG(\"execbuf with invalid ring (BLT)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tring = &dev_priv->ring[BCS];\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"execbuf with unknown ring: %d\\n\",\n\t\t\t  (int)(args->flags & I915_EXEC_RING_MASK));\n\t\treturn -EINVAL;\n\t}\n\n\tmode = args->flags & I915_EXEC_CONSTANTS_MASK;\n\tmask = I915_EXEC_CONSTANTS_MASK;\n\tswitch (mode) {\n\tcase I915_EXEC_CONSTANTS_REL_GENERAL:\n\tcase I915_EXEC_CONSTANTS_ABSOLUTE:\n\tcase I915_EXEC_CONSTANTS_REL_SURFACE:\n\t\tif (ring == &dev_priv->ring[RCS] &&\n\t\t    mode != dev_priv->relative_constants_mode) {\n\t\t\tif (INTEL_INFO(dev)->gen < 4)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (INTEL_INFO(dev)->gen > 5 &&\n\t\t\t    mode == I915_EXEC_CONSTANTS_REL_SURFACE)\n\t\t\t\treturn -EINVAL;\n\n\t\t\t/* The HW changed the meaning on this bit on gen6 */\n\t\t\tif (INTEL_INFO(dev)->gen >= 6)\n\t\t\t\tmask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"execbuf with unknown constants: %d\\n\", mode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->num_cliprects != 0) {\n\t\tif (ring != &dev_priv->ring[RCS]) {\n\t\t\tDRM_DEBUG(\"clip rectangles are only valid with the render ring\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tcliprects = kmalloc(args->num_cliprects * sizeof(*cliprects),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (cliprects == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto pre_mutex_err;\n\t\t}\n\n\t\tif (copy_from_user(cliprects,\n\t\t\t\t     (struct drm_clip_rect __user *)(uintptr_t)\n\t\t\t\t     args->cliprects_ptr,\n\t\t\t\t     sizeof(*cliprects)*args->num_cliprects)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto pre_mutex_err;\n\t\t}\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret)\n\t\tgoto pre_mutex_err;\n\n\tif (dev_priv->mm.suspended) {\n\t\tmutex_unlock(&dev->struct_mutex);\n\t\tret = -EBUSY;\n\t\tgoto pre_mutex_err;\n\t}\n\n\teb = eb_create(args->buffer_count);\n\tif (eb == NULL) {\n\t\tmutex_unlock(&dev->struct_mutex);\n\t\tret = -ENOMEM;\n\t\tgoto pre_mutex_err;\n\t}\n\n\t/* Look up object handles */\n\tINIT_LIST_HEAD(&objects);\n\tfor (i = 0; i < args->buffer_count; i++) {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = to_intel_bo(drm_gem_object_lookup(dev, file,\n\t\t\t\t\t\t\texec[i].handle));\n\t\tif (&obj->base == NULL) {\n\t\t\tDRM_DEBUG(\"Invalid object handle %d at index %d\\n\",\n\t\t\t\t   exec[i].handle, i);\n\t\t\t/* prevent error path from reading uninitialized data */\n\t\t\tret = -ENOENT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (!list_empty(&obj->exec_list)) {\n\t\t\tDRM_DEBUG(\"Object %p [handle %d, index %d] appears more than once in object list\\n\",\n\t\t\t\t   obj, exec[i].handle, i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\tlist_add_tail(&obj->exec_list, &objects);\n\t\tobj->exec_handle = exec[i].handle;\n\t\tobj->exec_entry = &exec[i];\n\t\teb_add_object(eb, obj);\n\t}\n\n\t/* take note of the batch buffer before we might reorder the lists */\n\tbatch_obj = list_entry(objects.prev,\n\t\t\t       struct drm_i915_gem_object,\n\t\t\t       exec_list);\n\n\t/* Move the objects en-masse into the GTT, evicting if necessary. */\n\tret = i915_gem_execbuffer_reserve(ring, file, &objects);\n\tif (ret)\n\t\tgoto err;\n\n\t/* The objects are in their final locations, apply the relocations. */\n\tret = i915_gem_execbuffer_relocate(dev, eb, &objects);\n\tif (ret) {\n\t\tif (ret == -EFAULT) {\n\t\t\tret = i915_gem_execbuffer_relocate_slow(dev, file, ring,\n\t\t\t\t\t\t\t\t&objects, eb,\n\t\t\t\t\t\t\t\texec,\n\t\t\t\t\t\t\t\targs->buffer_count);\n\t\t\tBUG_ON(!mutex_is_locked(&dev->struct_mutex));\n\t\t}\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\t/* Set the pending read domains for the batch buffer to COMMAND */\n\tif (batch_obj->base.pending_write_domain) {\n\t\tDRM_DEBUG(\"Attempting to use self-modifying batch buffer\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\tbatch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;\n\n\tret = i915_gem_execbuffer_move_to_gpu(ring, &objects);\n\tif (ret)\n\t\tgoto err;\n\n\tseqno = i915_gem_next_request_seqno(ring);\n\tfor (i = 0; i < ARRAY_SIZE(ring->sync_seqno); i++) {\n\t\tif (seqno < ring->sync_seqno[i]) {\n\t\t\t/* The GPU can not handle its semaphore value wrapping,\n\t\t\t * so every billion or so execbuffers, we need to stall\n\t\t\t * the GPU in order to reset the counters.\n\t\t\t */\n\t\t\tret = i915_gpu_idle(dev, true);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tBUG_ON(ring->sync_seqno[i]);\n\t\t}\n\t}\n\n\tif (ring == &dev_priv->ring[RCS] &&\n\t    mode != dev_priv->relative_constants_mode) {\n\t\tret = intel_ring_begin(ring, 4);\n\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\tintel_ring_emit(ring, MI_NOOP);\n\t\tintel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));\n\t\tintel_ring_emit(ring, INSTPM);\n\t\tintel_ring_emit(ring, mask << 16 | mode);\n\t\tintel_ring_advance(ring);\n\n\t\tdev_priv->relative_constants_mode = mode;\n\t}\n\n\tif (args->flags & I915_EXEC_GEN7_SOL_RESET) {\n\t\tret = i915_reset_gen7_sol_offsets(dev, ring);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ttrace_i915_gem_ring_dispatch(ring, seqno);\n\n\texec_start = batch_obj->gtt_offset + args->batch_start_offset;\n\texec_len = args->batch_len;\n\tif (cliprects) {\n\t\tfor (i = 0; i < args->num_cliprects; i++) {\n\t\t\tret = i915_emit_box(dev, &cliprects[i],\n\t\t\t\t\t    args->DR1, args->DR4);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tret = ring->dispatch_execbuffer(ring,\n\t\t\t\t\t\t\texec_start, exec_len);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t}\n\t} else {\n\t\tret = ring->dispatch_execbuffer(ring, exec_start, exec_len);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ti915_gem_execbuffer_move_to_active(&objects, ring, seqno);\n\ti915_gem_execbuffer_retire_commands(dev, file, ring);\n\nerr:\n\teb_destroy(eb);\n\twhile (!list_empty(&objects)) {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = list_first_entry(&objects,\n\t\t\t\t       struct drm_i915_gem_object,\n\t\t\t\t       exec_list);\n\t\tlist_del_init(&obj->exec_list);\n\t\tdrm_gem_object_unreference(&obj->base);\n\t}\n\n\tmutex_unlock(&dev->struct_mutex);\n\npre_mutex_err:\n\tkfree(cliprects);\n\treturn ret;\n}\n\n/*\n * Legacy execbuffer just creates an exec2 list from the original exec object\n * list array and passes it to the real function.\n */\nint\ni915_gem_execbuffer(struct drm_device *dev, void *data,\n\t\t    struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer *args = data;\n\tstruct drm_i915_gem_execbuffer2 exec2;\n\tstruct drm_i915_gem_exec_object *exec_list = NULL;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret, i;\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Copy in the exec list from userland */\n\texec_list = drm_malloc_ab(sizeof(*exec_list), args->buffer_count);\n\texec2_list = drm_malloc_ab(sizeof(*exec2_list), args->buffer_count);\n\tif (exec_list == NULL || exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\tdrm_free_large(exec_list);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec_list);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tfor (i = 0; i < args->buffer_count; i++) {\n\t\texec2_list[i].handle = exec_list[i].handle;\n\t\texec2_list[i].relocation_count = exec_list[i].relocation_count;\n\t\texec2_list[i].relocs_ptr = exec_list[i].relocs_ptr;\n\t\texec2_list[i].alignment = exec_list[i].alignment;\n\t\texec2_list[i].offset = exec_list[i].offset;\n\t\tif (INTEL_INFO(dev)->gen < 4)\n\t\t\texec2_list[i].flags = EXEC_OBJECT_NEEDS_FENCE;\n\t\telse\n\t\t\texec2_list[i].flags = 0;\n\t}\n\n\texec2.buffers_ptr = args->buffers_ptr;\n\texec2.buffer_count = args->buffer_count;\n\texec2.batch_start_offset = args->batch_start_offset;\n\texec2.batch_len = args->batch_len;\n\texec2.DR1 = args->DR1;\n\texec2.DR4 = args->DR4;\n\texec2.num_cliprects = args->num_cliprects;\n\texec2.cliprects_ptr = args->cliprects_ptr;\n\texec2.flags = I915_EXEC_RENDER;\n\n\tret = i915_gem_do_execbuffer(dev, data, file, &exec2, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tfor (i = 0; i < args->buffer_count; i++)\n\t\t\texec_list[i].offset = exec2_list[i].offset;\n\t\t/* ... and back out to userspace */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec_list,\n\t\t\t\t   sizeof(*exec_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec_list);\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}\n\nint\ni915_gem_execbuffer2(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret;\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf2 with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\texec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,\n\t\t\t     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (exec2_list == NULL)\n\t\texec2_list = drm_malloc_ab(sizeof(*exec2_list),\n\t\t\t\t\t   args->buffer_count);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec2_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec2_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec2_list,\n\t\t\t\t   sizeof(*exec2_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}\n"], "fixing_code": ["/*\n * Copyright \u00a9 2008,2010 Intel Corporation\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the next\n * paragraph) shall be included in all copies or substantial portions of the\n * Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n *\n * Authors:\n *    Eric Anholt <eric@anholt.net>\n *    Chris Wilson <chris@chris-wilson.co.uk>\n *\n */\n\n#include \"drmP.h\"\n#include \"drm.h\"\n#include \"i915_drm.h\"\n#include \"i915_drv.h\"\n#include \"i915_trace.h\"\n#include \"intel_drv.h\"\n#include <linux/dma_remapping.h>\n\nstruct change_domains {\n\tuint32_t invalidate_domains;\n\tuint32_t flush_domains;\n\tuint32_t flush_rings;\n\tuint32_t flips;\n};\n\n/*\n * Set the next domain for the specified object. This\n * may not actually perform the necessary flushing/invaliding though,\n * as that may want to be batched with other set_domain operations\n *\n * This is (we hope) the only really tricky part of gem. The goal\n * is fairly simple -- track which caches hold bits of the object\n * and make sure they remain coherent. A few concrete examples may\n * help to explain how it works. For shorthand, we use the notation\n * (read_domains, write_domain), e.g. (CPU, CPU) to indicate the\n * a pair of read and write domain masks.\n *\n * Case 1: the batch buffer\n *\n *\t1. Allocated\n *\t2. Written by CPU\n *\t3. Mapped to GTT\n *\t4. Read by GPU\n *\t5. Unmapped from GTT\n *\t6. Freed\n *\n *\tLet's take these a step at a time\n *\n *\t1. Allocated\n *\t\tPages allocated from the kernel may still have\n *\t\tcache contents, so we set them to (CPU, CPU) always.\n *\t2. Written by CPU (using pwrite)\n *\t\tThe pwrite function calls set_domain (CPU, CPU) and\n *\t\tthis function does nothing (as nothing changes)\n *\t3. Mapped by GTT\n *\t\tThis function asserts that the object is not\n *\t\tcurrently in any GPU-based read or write domains\n *\t4. Read by GPU\n *\t\ti915_gem_execbuffer calls set_domain (COMMAND, 0).\n *\t\tAs write_domain is zero, this function adds in the\n *\t\tcurrent read domains (CPU+COMMAND, 0).\n *\t\tflush_domains is set to CPU.\n *\t\tinvalidate_domains is set to COMMAND\n *\t\tclflush is run to get data out of the CPU caches\n *\t\tthen i915_dev_set_domain calls i915_gem_flush to\n *\t\temit an MI_FLUSH and drm_agp_chipset_flush\n *\t5. Unmapped from GTT\n *\t\ti915_gem_object_unbind calls set_domain (CPU, CPU)\n *\t\tflush_domains and invalidate_domains end up both zero\n *\t\tso no flushing/invalidating happens\n *\t6. Freed\n *\t\tyay, done\n *\n * Case 2: The shared render buffer\n *\n *\t1. Allocated\n *\t2. Mapped to GTT\n *\t3. Read/written by GPU\n *\t4. set_domain to (CPU,CPU)\n *\t5. Read/written by CPU\n *\t6. Read/written by GPU\n *\n *\t1. Allocated\n *\t\tSame as last example, (CPU, CPU)\n *\t2. Mapped to GTT\n *\t\tNothing changes (assertions find that it is not in the GPU)\n *\t3. Read/written by GPU\n *\t\texecbuffer calls set_domain (RENDER, RENDER)\n *\t\tflush_domains gets CPU\n *\t\tinvalidate_domains gets GPU\n *\t\tclflush (obj)\n *\t\tMI_FLUSH and drm_agp_chipset_flush\n *\t4. set_domain (CPU, CPU)\n *\t\tflush_domains gets GPU\n *\t\tinvalidate_domains gets CPU\n *\t\twait_rendering (obj) to make sure all drawing is complete.\n *\t\tThis will include an MI_FLUSH to get the data from GPU\n *\t\tto memory\n *\t\tclflush (obj) to invalidate the CPU cache\n *\t\tAnother MI_FLUSH in i915_gem_flush (eliminate this somehow?)\n *\t5. Read/written by CPU\n *\t\tcache lines are loaded and dirtied\n *\t6. Read written by GPU\n *\t\tSame as last GPU access\n *\n * Case 3: The constant buffer\n *\n *\t1. Allocated\n *\t2. Written by CPU\n *\t3. Read by GPU\n *\t4. Updated (written) by CPU again\n *\t5. Read by GPU\n *\n *\t1. Allocated\n *\t\t(CPU, CPU)\n *\t2. Written by CPU\n *\t\t(CPU, CPU)\n *\t3. Read by GPU\n *\t\t(CPU+RENDER, 0)\n *\t\tflush_domains = CPU\n *\t\tinvalidate_domains = RENDER\n *\t\tclflush (obj)\n *\t\tMI_FLUSH\n *\t\tdrm_agp_chipset_flush\n *\t4. Updated (written) by CPU again\n *\t\t(CPU, CPU)\n *\t\tflush_domains = 0 (no previous write domain)\n *\t\tinvalidate_domains = 0 (no new read domains)\n *\t5. Read by GPU\n *\t\t(CPU+RENDER, 0)\n *\t\tflush_domains = CPU\n *\t\tinvalidate_domains = RENDER\n *\t\tclflush (obj)\n *\t\tMI_FLUSH\n *\t\tdrm_agp_chipset_flush\n */\nstatic void\ni915_gem_object_set_to_gpu_domain(struct drm_i915_gem_object *obj,\n\t\t\t\t  struct intel_ring_buffer *ring,\n\t\t\t\t  struct change_domains *cd)\n{\n\tuint32_t invalidate_domains = 0, flush_domains = 0;\n\n\t/*\n\t * If the object isn't moving to a new write domain,\n\t * let the object stay in multiple read domains\n\t */\n\tif (obj->base.pending_write_domain == 0)\n\t\tobj->base.pending_read_domains |= obj->base.read_domains;\n\n\t/*\n\t * Flush the current write domain if\n\t * the new read domains don't match. Invalidate\n\t * any read domains which differ from the old\n\t * write domain\n\t */\n\tif (obj->base.write_domain &&\n\t    (((obj->base.write_domain != obj->base.pending_read_domains ||\n\t       obj->ring != ring)) ||\n\t     (obj->fenced_gpu_access && !obj->pending_fenced_gpu_access))) {\n\t\tflush_domains |= obj->base.write_domain;\n\t\tinvalidate_domains |=\n\t\t\tobj->base.pending_read_domains & ~obj->base.write_domain;\n\t}\n\t/*\n\t * Invalidate any read caches which may have\n\t * stale data. That is, any new read domains.\n\t */\n\tinvalidate_domains |= obj->base.pending_read_domains & ~obj->base.read_domains;\n\tif ((flush_domains | invalidate_domains) & I915_GEM_DOMAIN_CPU)\n\t\ti915_gem_clflush_object(obj);\n\n\tif (obj->base.pending_write_domain)\n\t\tcd->flips |= atomic_read(&obj->pending_flip);\n\n\t/* The actual obj->write_domain will be updated with\n\t * pending_write_domain after we emit the accumulated flush for all\n\t * of our domain changes in execbuffers (which clears objects'\n\t * write_domains).  So if we have a current write domain that we\n\t * aren't changing, set pending_write_domain to that.\n\t */\n\tif (flush_domains == 0 && obj->base.pending_write_domain == 0)\n\t\tobj->base.pending_write_domain = obj->base.write_domain;\n\n\tcd->invalidate_domains |= invalidate_domains;\n\tcd->flush_domains |= flush_domains;\n\tif (flush_domains & I915_GEM_GPU_DOMAINS)\n\t\tcd->flush_rings |= intel_ring_flag(obj->ring);\n\tif (invalidate_domains & I915_GEM_GPU_DOMAINS)\n\t\tcd->flush_rings |= intel_ring_flag(ring);\n}\n\nstruct eb_objects {\n\tint and;\n\tstruct hlist_head buckets[0];\n};\n\nstatic struct eb_objects *\neb_create(int size)\n{\n\tstruct eb_objects *eb;\n\tint count = PAGE_SIZE / sizeof(struct hlist_head) / 2;\n\twhile (count > size)\n\t\tcount >>= 1;\n\teb = kzalloc(count*sizeof(struct hlist_head) +\n\t\t     sizeof(struct eb_objects),\n\t\t     GFP_KERNEL);\n\tif (eb == NULL)\n\t\treturn eb;\n\n\teb->and = count - 1;\n\treturn eb;\n}\n\nstatic void\neb_reset(struct eb_objects *eb)\n{\n\tmemset(eb->buckets, 0, (eb->and+1)*sizeof(struct hlist_head));\n}\n\nstatic void\neb_add_object(struct eb_objects *eb, struct drm_i915_gem_object *obj)\n{\n\thlist_add_head(&obj->exec_node,\n\t\t       &eb->buckets[obj->exec_handle & eb->and]);\n}\n\nstatic struct drm_i915_gem_object *\neb_get_object(struct eb_objects *eb, unsigned long handle)\n{\n\tstruct hlist_head *head;\n\tstruct hlist_node *node;\n\tstruct drm_i915_gem_object *obj;\n\n\thead = &eb->buckets[handle & eb->and];\n\thlist_for_each(node, head) {\n\t\tobj = hlist_entry(node, struct drm_i915_gem_object, exec_node);\n\t\tif (obj->exec_handle == handle)\n\t\t\treturn obj;\n\t}\n\n\treturn NULL;\n}\n\nstatic void\neb_destroy(struct eb_objects *eb)\n{\n\tkfree(eb);\n}\n\nstatic int\ni915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,\n\t\t\t\t   struct eb_objects *eb,\n\t\t\t\t   struct drm_i915_gem_relocation_entry *reloc)\n{\n\tstruct drm_device *dev = obj->base.dev;\n\tstruct drm_gem_object *target_obj;\n\tuint32_t target_offset;\n\tint ret = -EINVAL;\n\n\t/* we've already hold a reference to all valid objects */\n\ttarget_obj = &eb_get_object(eb, reloc->target_handle)->base;\n\tif (unlikely(target_obj == NULL))\n\t\treturn -ENOENT;\n\n\ttarget_offset = to_intel_bo(target_obj)->gtt_offset;\n\n\t/* The target buffer should have appeared before us in the\n\t * exec_object list, so it should have a GTT space bound by now.\n\t */\n\tif (unlikely(target_offset == 0)) {\n\t\tDRM_DEBUG(\"No GTT space found for object %d\\n\",\n\t\t\t  reloc->target_handle);\n\t\treturn ret;\n\t}\n\n\t/* Validate that the target is in a valid r/w GPU domain */\n\tif (unlikely(reloc->write_domain & (reloc->write_domain - 1))) {\n\t\tDRM_DEBUG(\"reloc with multiple write domains: \"\n\t\t\t  \"obj %p target %d offset %d \"\n\t\t\t  \"read %08x write %08x\",\n\t\t\t  obj, reloc->target_handle,\n\t\t\t  (int) reloc->offset,\n\t\t\t  reloc->read_domains,\n\t\t\t  reloc->write_domain);\n\t\treturn ret;\n\t}\n\tif (unlikely((reloc->write_domain | reloc->read_domains)\n\t\t     & ~I915_GEM_GPU_DOMAINS)) {\n\t\tDRM_DEBUG(\"reloc with read/write non-GPU domains: \"\n\t\t\t  \"obj %p target %d offset %d \"\n\t\t\t  \"read %08x write %08x\",\n\t\t\t  obj, reloc->target_handle,\n\t\t\t  (int) reloc->offset,\n\t\t\t  reloc->read_domains,\n\t\t\t  reloc->write_domain);\n\t\treturn ret;\n\t}\n\tif (unlikely(reloc->write_domain && target_obj->pending_write_domain &&\n\t\t     reloc->write_domain != target_obj->pending_write_domain)) {\n\t\tDRM_DEBUG(\"Write domain conflict: \"\n\t\t\t  \"obj %p target %d offset %d \"\n\t\t\t  \"new %08x old %08x\\n\",\n\t\t\t  obj, reloc->target_handle,\n\t\t\t  (int) reloc->offset,\n\t\t\t  reloc->write_domain,\n\t\t\t  target_obj->pending_write_domain);\n\t\treturn ret;\n\t}\n\n\ttarget_obj->pending_read_domains |= reloc->read_domains;\n\ttarget_obj->pending_write_domain |= reloc->write_domain;\n\n\t/* If the relocation already has the right value in it, no\n\t * more work needs to be done.\n\t */\n\tif (target_offset == reloc->presumed_offset)\n\t\treturn 0;\n\n\t/* Check that the relocation address is valid... */\n\tif (unlikely(reloc->offset > obj->base.size - 4)) {\n\t\tDRM_DEBUG(\"Relocation beyond object bounds: \"\n\t\t\t  \"obj %p target %d offset %d size %d.\\n\",\n\t\t\t  obj, reloc->target_handle,\n\t\t\t  (int) reloc->offset,\n\t\t\t  (int) obj->base.size);\n\t\treturn ret;\n\t}\n\tif (unlikely(reloc->offset & 3)) {\n\t\tDRM_DEBUG(\"Relocation not 4-byte aligned: \"\n\t\t\t  \"obj %p target %d offset %d.\\n\",\n\t\t\t  obj, reloc->target_handle,\n\t\t\t  (int) reloc->offset);\n\t\treturn ret;\n\t}\n\n\treloc->delta += target_offset;\n\tif (obj->base.write_domain == I915_GEM_DOMAIN_CPU) {\n\t\tuint32_t page_offset = reloc->offset & ~PAGE_MASK;\n\t\tchar *vaddr;\n\n\t\tvaddr = kmap_atomic(obj->pages[reloc->offset >> PAGE_SHIFT]);\n\t\t*(uint32_t *)(vaddr + page_offset) = reloc->delta;\n\t\tkunmap_atomic(vaddr);\n\t} else {\n\t\tstruct drm_i915_private *dev_priv = dev->dev_private;\n\t\tuint32_t __iomem *reloc_entry;\n\t\tvoid __iomem *reloc_page;\n\n\t\t/* We can't wait for rendering with pagefaults disabled */\n\t\tif (obj->active && in_atomic())\n\t\t\treturn -EFAULT;\n\n\t\tret = i915_gem_object_set_to_gtt_domain(obj, 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/* Map the page containing the relocation we're going to perform.  */\n\t\treloc->offset += obj->gtt_offset;\n\t\treloc_page = io_mapping_map_atomic_wc(dev_priv->mm.gtt_mapping,\n\t\t\t\t\t\t      reloc->offset & PAGE_MASK);\n\t\treloc_entry = (uint32_t __iomem *)\n\t\t\t(reloc_page + (reloc->offset & ~PAGE_MASK));\n\t\tiowrite32(reloc->delta, reloc_entry);\n\t\tio_mapping_unmap_atomic(reloc_page);\n\t}\n\n\t/* and update the user's relocation entry */\n\treloc->presumed_offset = target_offset;\n\n\treturn 0;\n}\n\nstatic int\ni915_gem_execbuffer_relocate_object(struct drm_i915_gem_object *obj,\n\t\t\t\t    struct eb_objects *eb)\n{\n\tstruct drm_i915_gem_relocation_entry __user *user_relocs;\n\tstruct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\n\tint i, ret;\n\n\tuser_relocs = (void __user *)(uintptr_t)entry->relocs_ptr;\n\tfor (i = 0; i < entry->relocation_count; i++) {\n\t\tstruct drm_i915_gem_relocation_entry reloc;\n\n\t\tif (__copy_from_user_inatomic(&reloc,\n\t\t\t\t\t      user_relocs+i,\n\t\t\t\t\t      sizeof(reloc)))\n\t\t\treturn -EFAULT;\n\n\t\tret = i915_gem_execbuffer_relocate_entry(obj, eb, &reloc);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (__copy_to_user_inatomic(&user_relocs[i].presumed_offset,\n\t\t\t\t\t    &reloc.presumed_offset,\n\t\t\t\t\t    sizeof(reloc.presumed_offset)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nstatic int\ni915_gem_execbuffer_relocate_object_slow(struct drm_i915_gem_object *obj,\n\t\t\t\t\t struct eb_objects *eb,\n\t\t\t\t\t struct drm_i915_gem_relocation_entry *relocs)\n{\n\tconst struct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\n\tint i, ret;\n\n\tfor (i = 0; i < entry->relocation_count; i++) {\n\t\tret = i915_gem_execbuffer_relocate_entry(obj, eb, &relocs[i]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int\ni915_gem_execbuffer_relocate(struct drm_device *dev,\n\t\t\t     struct eb_objects *eb,\n\t\t\t     struct list_head *objects)\n{\n\tstruct drm_i915_gem_object *obj;\n\tint ret = 0;\n\n\t/* This is the fast path and we cannot handle a pagefault whilst\n\t * holding the struct mutex lest the user pass in the relocations\n\t * contained within a mmaped bo. For in such a case we, the page\n\t * fault handler would call i915_gem_fault() and we would try to\n\t * acquire the struct mutex again. Obviously this is bad and so\n\t * lockdep complains vehemently.\n\t */\n\tpagefault_disable();\n\tlist_for_each_entry(obj, objects, exec_list) {\n\t\tret = i915_gem_execbuffer_relocate_object(obj, eb);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tpagefault_enable();\n\n\treturn ret;\n}\n\n#define  __EXEC_OBJECT_HAS_FENCE (1<<31)\n\nstatic int\npin_and_fence_object(struct drm_i915_gem_object *obj,\n\t\t     struct intel_ring_buffer *ring)\n{\n\tstruct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\n\tbool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;\n\tbool need_fence, need_mappable;\n\tint ret;\n\n\tneed_fence =\n\t\thas_fenced_gpu_access &&\n\t\tentry->flags & EXEC_OBJECT_NEEDS_FENCE &&\n\t\tobj->tiling_mode != I915_TILING_NONE;\n\tneed_mappable =\n\t\tentry->relocation_count ? true : need_fence;\n\n\tret = i915_gem_object_pin(obj, entry->alignment, need_mappable);\n\tif (ret)\n\t\treturn ret;\n\n\tif (has_fenced_gpu_access) {\n\t\tif (entry->flags & EXEC_OBJECT_NEEDS_FENCE) {\n\t\t\tif (obj->tiling_mode) {\n\t\t\t\tret = i915_gem_object_get_fence(obj, ring);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto err_unpin;\n\n\t\t\t\tentry->flags |= __EXEC_OBJECT_HAS_FENCE;\n\t\t\t\ti915_gem_object_pin_fence(obj);\n\t\t\t} else {\n\t\t\t\tret = i915_gem_object_put_fence(obj);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto err_unpin;\n\t\t\t}\n\t\t\tobj->pending_fenced_gpu_access = true;\n\t\t}\n\t}\n\n\tentry->offset = obj->gtt_offset;\n\treturn 0;\n\nerr_unpin:\n\ti915_gem_object_unpin(obj);\n\treturn ret;\n}\n\nstatic int\ni915_gem_execbuffer_reserve(struct intel_ring_buffer *ring,\n\t\t\t    struct drm_file *file,\n\t\t\t    struct list_head *objects)\n{\n\tdrm_i915_private_t *dev_priv = ring->dev->dev_private;\n\tstruct drm_i915_gem_object *obj;\n\tint ret, retry;\n\tbool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;\n\tstruct list_head ordered_objects;\n\n\tINIT_LIST_HEAD(&ordered_objects);\n\twhile (!list_empty(objects)) {\n\t\tstruct drm_i915_gem_exec_object2 *entry;\n\t\tbool need_fence, need_mappable;\n\n\t\tobj = list_first_entry(objects,\n\t\t\t\t       struct drm_i915_gem_object,\n\t\t\t\t       exec_list);\n\t\tentry = obj->exec_entry;\n\n\t\tneed_fence =\n\t\t\thas_fenced_gpu_access &&\n\t\t\tentry->flags & EXEC_OBJECT_NEEDS_FENCE &&\n\t\t\tobj->tiling_mode != I915_TILING_NONE;\n\t\tneed_mappable =\n\t\t\tentry->relocation_count ? true : need_fence;\n\n\t\tif (need_mappable)\n\t\t\tlist_move(&obj->exec_list, &ordered_objects);\n\t\telse\n\t\t\tlist_move_tail(&obj->exec_list, &ordered_objects);\n\n\t\tobj->base.pending_read_domains = 0;\n\t\tobj->base.pending_write_domain = 0;\n\t}\n\tlist_splice(&ordered_objects, objects);\n\n\t/* Attempt to pin all of the buffers into the GTT.\n\t * This is done in 3 phases:\n\t *\n\t * 1a. Unbind all objects that do not match the GTT constraints for\n\t *     the execbuffer (fenceable, mappable, alignment etc).\n\t * 1b. Increment pin count for already bound objects.\n\t * 2.  Bind new objects.\n\t * 3.  Decrement pin count.\n\t *\n\t * This avoid unnecessary unbinding of later objects in order to makr\n\t * room for the earlier objects *unless* we need to defragment.\n\t */\n\tretry = 0;\n\tdo {\n\t\tret = 0;\n\n\t\t/* Unbind any ill-fitting objects or pin. */\n\t\tlist_for_each_entry(obj, objects, exec_list) {\n\t\t\tstruct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\n\t\t\tbool need_fence, need_mappable;\n\n\t\t\tif (!obj->gtt_space)\n\t\t\t\tcontinue;\n\n\t\t\tneed_fence =\n\t\t\t\thas_fenced_gpu_access &&\n\t\t\t\tentry->flags & EXEC_OBJECT_NEEDS_FENCE &&\n\t\t\t\tobj->tiling_mode != I915_TILING_NONE;\n\t\t\tneed_mappable =\n\t\t\t\tentry->relocation_count ? true : need_fence;\n\n\t\t\tif ((entry->alignment && obj->gtt_offset & (entry->alignment - 1)) ||\n\t\t\t    (need_mappable && !obj->map_and_fenceable))\n\t\t\t\tret = i915_gem_object_unbind(obj);\n\t\t\telse\n\t\t\t\tret = pin_and_fence_object(obj, ring);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t}\n\n\t\t/* Bind fresh objects */\n\t\tlist_for_each_entry(obj, objects, exec_list) {\n\t\t\tif (obj->gtt_space)\n\t\t\t\tcontinue;\n\n\t\t\tret = pin_and_fence_object(obj, ring);\n\t\t\tif (ret) {\n\t\t\t\tint ret_ignore;\n\n\t\t\t\t/* This can potentially raise a harmless\n\t\t\t\t * -EINVAL if we failed to bind in the above\n\t\t\t\t * call. It cannot raise -EINTR since we know\n\t\t\t\t * that the bo is freshly bound and so will\n\t\t\t\t * not need to be flushed or waited upon.\n\t\t\t\t */\n\t\t\t\tret_ignore = i915_gem_object_unbind(obj);\n\t\t\t\t(void)ret_ignore;\n\t\t\t\tWARN_ON(obj->gtt_space);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Decrement pin count for bound objects */\n\t\tlist_for_each_entry(obj, objects, exec_list) {\n\t\t\tstruct drm_i915_gem_exec_object2 *entry;\n\n\t\t\tif (!obj->gtt_space)\n\t\t\t\tcontinue;\n\n\t\t\tentry = obj->exec_entry;\n\t\t\tif (entry->flags & __EXEC_OBJECT_HAS_FENCE) {\n\t\t\t\ti915_gem_object_unpin_fence(obj);\n\t\t\t\tentry->flags &= ~__EXEC_OBJECT_HAS_FENCE;\n\t\t\t}\n\n\t\t\ti915_gem_object_unpin(obj);\n\n\t\t\t/* ... and ensure ppgtt mapping exist if needed. */\n\t\t\tif (dev_priv->mm.aliasing_ppgtt && !obj->has_aliasing_ppgtt_mapping) {\n\t\t\t\ti915_ppgtt_bind_object(dev_priv->mm.aliasing_ppgtt,\n\t\t\t\t\t\t       obj, obj->cache_level);\n\n\t\t\t\tobj->has_aliasing_ppgtt_mapping = 1;\n\t\t\t}\n\t\t}\n\n\t\tif (ret != -ENOSPC || retry > 1)\n\t\t\treturn ret;\n\n\t\t/* First attempt, just clear anything that is purgeable.\n\t\t * Second attempt, clear the entire GTT.\n\t\t */\n\t\tret = i915_gem_evict_everything(ring->dev, retry == 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tretry++;\n\t} while (1);\n\nerr:\n\tlist_for_each_entry_continue_reverse(obj, objects, exec_list) {\n\t\tstruct drm_i915_gem_exec_object2 *entry;\n\n\t\tif (!obj->gtt_space)\n\t\t\tcontinue;\n\n\t\tentry = obj->exec_entry;\n\t\tif (entry->flags & __EXEC_OBJECT_HAS_FENCE) {\n\t\t\ti915_gem_object_unpin_fence(obj);\n\t\t\tentry->flags &= ~__EXEC_OBJECT_HAS_FENCE;\n\t\t}\n\n\t\ti915_gem_object_unpin(obj);\n\t}\n\n\treturn ret;\n}\n\nstatic int\ni915_gem_execbuffer_relocate_slow(struct drm_device *dev,\n\t\t\t\t  struct drm_file *file,\n\t\t\t\t  struct intel_ring_buffer *ring,\n\t\t\t\t  struct list_head *objects,\n\t\t\t\t  struct eb_objects *eb,\n\t\t\t\t  struct drm_i915_gem_exec_object2 *exec,\n\t\t\t\t  int count)\n{\n\tstruct drm_i915_gem_relocation_entry *reloc;\n\tstruct drm_i915_gem_object *obj;\n\tint *reloc_offset;\n\tint i, total, ret;\n\n\t/* We may process another execbuffer during the unlock... */\n\twhile (!list_empty(objects)) {\n\t\tobj = list_first_entry(objects,\n\t\t\t\t       struct drm_i915_gem_object,\n\t\t\t\t       exec_list);\n\t\tlist_del_init(&obj->exec_list);\n\t\tdrm_gem_object_unreference(&obj->base);\n\t}\n\n\tmutex_unlock(&dev->struct_mutex);\n\n\ttotal = 0;\n\tfor (i = 0; i < count; i++)\n\t\ttotal += exec[i].relocation_count;\n\n\treloc_offset = drm_malloc_ab(count, sizeof(*reloc_offset));\n\treloc = drm_malloc_ab(total, sizeof(*reloc));\n\tif (reloc == NULL || reloc_offset == NULL) {\n\t\tdrm_free_large(reloc);\n\t\tdrm_free_large(reloc_offset);\n\t\tmutex_lock(&dev->struct_mutex);\n\t\treturn -ENOMEM;\n\t}\n\n\ttotal = 0;\n\tfor (i = 0; i < count; i++) {\n\t\tstruct drm_i915_gem_relocation_entry __user *user_relocs;\n\n\t\tuser_relocs = (void __user *)(uintptr_t)exec[i].relocs_ptr;\n\n\t\tif (copy_from_user(reloc+total, user_relocs,\n\t\t\t\t   exec[i].relocation_count * sizeof(*reloc))) {\n\t\t\tret = -EFAULT;\n\t\t\tmutex_lock(&dev->struct_mutex);\n\t\t\tgoto err;\n\t\t}\n\n\t\treloc_offset[i] = total;\n\t\ttotal += exec[i].relocation_count;\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret) {\n\t\tmutex_lock(&dev->struct_mutex);\n\t\tgoto err;\n\t}\n\n\t/* reacquire the objects */\n\teb_reset(eb);\n\tfor (i = 0; i < count; i++) {\n\t\tobj = to_intel_bo(drm_gem_object_lookup(dev, file,\n\t\t\t\t\t\t\texec[i].handle));\n\t\tif (&obj->base == NULL) {\n\t\t\tDRM_DEBUG(\"Invalid object handle %d at index %d\\n\",\n\t\t\t\t   exec[i].handle, i);\n\t\t\tret = -ENOENT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tlist_add_tail(&obj->exec_list, objects);\n\t\tobj->exec_handle = exec[i].handle;\n\t\tobj->exec_entry = &exec[i];\n\t\teb_add_object(eb, obj);\n\t}\n\n\tret = i915_gem_execbuffer_reserve(ring, file, objects);\n\tif (ret)\n\t\tgoto err;\n\n\tlist_for_each_entry(obj, objects, exec_list) {\n\t\tint offset = obj->exec_entry - exec;\n\t\tret = i915_gem_execbuffer_relocate_object_slow(obj, eb,\n\t\t\t\t\t\t\t       reloc + reloc_offset[offset]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\t/* Leave the user relocations as are, this is the painfully slow path,\n\t * and we want to avoid the complication of dropping the lock whilst\n\t * having buffers reserved in the aperture and so causing spurious\n\t * ENOSPC for random operations.\n\t */\n\nerr:\n\tdrm_free_large(reloc);\n\tdrm_free_large(reloc_offset);\n\treturn ret;\n}\n\nstatic int\ni915_gem_execbuffer_flush(struct drm_device *dev,\n\t\t\t  uint32_t invalidate_domains,\n\t\t\t  uint32_t flush_domains,\n\t\t\t  uint32_t flush_rings)\n{\n\tdrm_i915_private_t *dev_priv = dev->dev_private;\n\tint i, ret;\n\n\tif (flush_domains & I915_GEM_DOMAIN_CPU)\n\t\tintel_gtt_chipset_flush();\n\n\tif (flush_domains & I915_GEM_DOMAIN_GTT)\n\t\twmb();\n\n\tif ((flush_domains | invalidate_domains) & I915_GEM_GPU_DOMAINS) {\n\t\tfor (i = 0; i < I915_NUM_RINGS; i++)\n\t\t\tif (flush_rings & (1 << i)) {\n\t\t\t\tret = i915_gem_flush_ring(&dev_priv->ring[i],\n\t\t\t\t\t\t\t  invalidate_domains,\n\t\t\t\t\t\t\t  flush_domains);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic bool\nintel_enable_semaphores(struct drm_device *dev)\n{\n\tif (INTEL_INFO(dev)->gen < 6)\n\t\treturn 0;\n\n\tif (i915_semaphores >= 0)\n\t\treturn i915_semaphores;\n\n\t/* Disable semaphores on SNB */\n\tif (INTEL_INFO(dev)->gen == 6)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int\ni915_gem_execbuffer_sync_rings(struct drm_i915_gem_object *obj,\n\t\t\t       struct intel_ring_buffer *to)\n{\n\tstruct intel_ring_buffer *from = obj->ring;\n\tu32 seqno;\n\tint ret, idx;\n\n\tif (from == NULL || to == from)\n\t\treturn 0;\n\n\t/* XXX gpu semaphores are implicated in various hard hangs on SNB */\n\tif (!intel_enable_semaphores(obj->base.dev))\n\t\treturn i915_gem_object_wait_rendering(obj);\n\n\tidx = intel_ring_sync_index(from, to);\n\n\tseqno = obj->last_rendering_seqno;\n\tif (seqno <= from->sync_seqno[idx])\n\t\treturn 0;\n\n\tif (seqno == from->outstanding_lazy_request) {\n\t\tstruct drm_i915_gem_request *request;\n\n\t\trequest = kzalloc(sizeof(*request), GFP_KERNEL);\n\t\tif (request == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tret = i915_add_request(from, NULL, request);\n\t\tif (ret) {\n\t\t\tkfree(request);\n\t\t\treturn ret;\n\t\t}\n\n\t\tseqno = request->seqno;\n\t}\n\n\tfrom->sync_seqno[idx] = seqno;\n\n\treturn to->sync_to(to, from, seqno - 1);\n}\n\nstatic int\ni915_gem_execbuffer_wait_for_flips(struct intel_ring_buffer *ring, u32 flips)\n{\n\tu32 plane, flip_mask;\n\tint ret;\n\n\t/* Check for any pending flips. As we only maintain a flip queue depth\n\t * of 1, we can simply insert a WAIT for the next display flip prior\n\t * to executing the batch and avoid stalling the CPU.\n\t */\n\n\tfor (plane = 0; flips >> plane; plane++) {\n\t\tif (((flips >> plane) & 1) == 0)\n\t\t\tcontinue;\n\n\t\tif (plane)\n\t\t\tflip_mask = MI_WAIT_FOR_PLANE_B_FLIP;\n\t\telse\n\t\t\tflip_mask = MI_WAIT_FOR_PLANE_A_FLIP;\n\n\t\tret = intel_ring_begin(ring, 2);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tintel_ring_emit(ring, MI_WAIT_FOR_EVENT | flip_mask);\n\t\tintel_ring_emit(ring, MI_NOOP);\n\t\tintel_ring_advance(ring);\n\t}\n\n\treturn 0;\n}\n\n\nstatic int\ni915_gem_execbuffer_move_to_gpu(struct intel_ring_buffer *ring,\n\t\t\t\tstruct list_head *objects)\n{\n\tstruct drm_i915_gem_object *obj;\n\tstruct change_domains cd;\n\tint ret;\n\n\tmemset(&cd, 0, sizeof(cd));\n\tlist_for_each_entry(obj, objects, exec_list)\n\t\ti915_gem_object_set_to_gpu_domain(obj, ring, &cd);\n\n\tif (cd.invalidate_domains | cd.flush_domains) {\n\t\tret = i915_gem_execbuffer_flush(ring->dev,\n\t\t\t\t\t\tcd.invalidate_domains,\n\t\t\t\t\t\tcd.flush_domains,\n\t\t\t\t\t\tcd.flush_rings);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (cd.flips) {\n\t\tret = i915_gem_execbuffer_wait_for_flips(ring, cd.flips);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tlist_for_each_entry(obj, objects, exec_list) {\n\t\tret = i915_gem_execbuffer_sync_rings(obj, ring);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic bool\ni915_gem_check_execbuffer(struct drm_i915_gem_execbuffer2 *exec)\n{\n\treturn ((exec->batch_start_offset | exec->batch_len) & 0x7) == 0;\n}\n\nstatic int\nvalidate_exec_list(struct drm_i915_gem_exec_object2 *exec,\n\t\t   int count)\n{\n\tint i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tchar __user *ptr = (char __user *)(uintptr_t)exec[i].relocs_ptr;\n\t\tint length; /* limited by fault_in_pages_readable() */\n\n\t\t/* First check for malicious input causing overflow */\n\t\tif (exec[i].relocation_count >\n\t\t    INT_MAX / sizeof(struct drm_i915_gem_relocation_entry))\n\t\t\treturn -EINVAL;\n\n\t\tlength = exec[i].relocation_count *\n\t\t\tsizeof(struct drm_i915_gem_relocation_entry);\n\t\tif (!access_ok(VERIFY_READ, ptr, length))\n\t\t\treturn -EFAULT;\n\n\t\t/* we may also need to update the presumed offsets */\n\t\tif (!access_ok(VERIFY_WRITE, ptr, length))\n\t\t\treturn -EFAULT;\n\n\t\tif (fault_in_pages_readable(ptr, length))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nstatic void\ni915_gem_execbuffer_move_to_active(struct list_head *objects,\n\t\t\t\t   struct intel_ring_buffer *ring,\n\t\t\t\t   u32 seqno)\n{\n\tstruct drm_i915_gem_object *obj;\n\n\tlist_for_each_entry(obj, objects, exec_list) {\n\t\t  u32 old_read = obj->base.read_domains;\n\t\t  u32 old_write = obj->base.write_domain;\n\n\n\t\tobj->base.read_domains = obj->base.pending_read_domains;\n\t\tobj->base.write_domain = obj->base.pending_write_domain;\n\t\tobj->fenced_gpu_access = obj->pending_fenced_gpu_access;\n\n\t\ti915_gem_object_move_to_active(obj, ring, seqno);\n\t\tif (obj->base.write_domain) {\n\t\t\tobj->dirty = 1;\n\t\t\tobj->pending_gpu_write = true;\n\t\t\tlist_move_tail(&obj->gpu_write_list,\n\t\t\t\t       &ring->gpu_write_list);\n\t\t\tintel_mark_busy(ring->dev, obj);\n\t\t}\n\n\t\ttrace_i915_gem_object_change_domain(obj, old_read, old_write);\n\t}\n}\n\nstatic void\ni915_gem_execbuffer_retire_commands(struct drm_device *dev,\n\t\t\t\t    struct drm_file *file,\n\t\t\t\t    struct intel_ring_buffer *ring)\n{\n\tstruct drm_i915_gem_request *request;\n\tu32 invalidate;\n\n\t/*\n\t * Ensure that the commands in the batch buffer are\n\t * finished before the interrupt fires.\n\t *\n\t * The sampler always gets flushed on i965 (sigh).\n\t */\n\tinvalidate = I915_GEM_DOMAIN_COMMAND;\n\tif (INTEL_INFO(dev)->gen >= 4)\n\t\tinvalidate |= I915_GEM_DOMAIN_SAMPLER;\n\tif (ring->flush(ring, invalidate, 0)) {\n\t\ti915_gem_next_request_seqno(ring);\n\t\treturn;\n\t}\n\n\t/* Add a breadcrumb for the completion of the batch buffer */\n\trequest = kzalloc(sizeof(*request), GFP_KERNEL);\n\tif (request == NULL || i915_add_request(ring, file, request)) {\n\t\ti915_gem_next_request_seqno(ring);\n\t\tkfree(request);\n\t}\n}\n\nstatic int\ni915_reset_gen7_sol_offsets(struct drm_device *dev,\n\t\t\t    struct intel_ring_buffer *ring)\n{\n\tdrm_i915_private_t *dev_priv = dev->dev_private;\n\tint ret, i;\n\n\tif (!IS_GEN7(dev) || ring != &dev_priv->ring[RCS])\n\t\treturn 0;\n\n\tret = intel_ring_begin(ring, 4 * 3);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < 4; i++) {\n\t\tintel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));\n\t\tintel_ring_emit(ring, GEN7_SO_WRITE_OFFSET(i));\n\t\tintel_ring_emit(ring, 0);\n\t}\n\n\tintel_ring_advance(ring);\n\n\treturn 0;\n}\n\nstatic int\ni915_gem_do_execbuffer(struct drm_device *dev, void *data,\n\t\t       struct drm_file *file,\n\t\t       struct drm_i915_gem_execbuffer2 *args,\n\t\t       struct drm_i915_gem_exec_object2 *exec)\n{\n\tdrm_i915_private_t *dev_priv = dev->dev_private;\n\tstruct list_head objects;\n\tstruct eb_objects *eb;\n\tstruct drm_i915_gem_object *batch_obj;\n\tstruct drm_clip_rect *cliprects = NULL;\n\tstruct intel_ring_buffer *ring;\n\tu32 exec_start, exec_len;\n\tu32 seqno;\n\tu32 mask;\n\tint ret, mode, i;\n\n\tif (!i915_gem_check_execbuffer(args)) {\n\t\tDRM_DEBUG(\"execbuf with invalid offset/length\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = validate_exec_list(exec, args->buffer_count);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (args->flags & I915_EXEC_RING_MASK) {\n\tcase I915_EXEC_DEFAULT:\n\tcase I915_EXEC_RENDER:\n\t\tring = &dev_priv->ring[RCS];\n\t\tbreak;\n\tcase I915_EXEC_BSD:\n\t\tif (!HAS_BSD(dev)) {\n\t\t\tDRM_DEBUG(\"execbuf with invalid ring (BSD)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tring = &dev_priv->ring[VCS];\n\t\tbreak;\n\tcase I915_EXEC_BLT:\n\t\tif (!HAS_BLT(dev)) {\n\t\t\tDRM_DEBUG(\"execbuf with invalid ring (BLT)\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tring = &dev_priv->ring[BCS];\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"execbuf with unknown ring: %d\\n\",\n\t\t\t  (int)(args->flags & I915_EXEC_RING_MASK));\n\t\treturn -EINVAL;\n\t}\n\n\tmode = args->flags & I915_EXEC_CONSTANTS_MASK;\n\tmask = I915_EXEC_CONSTANTS_MASK;\n\tswitch (mode) {\n\tcase I915_EXEC_CONSTANTS_REL_GENERAL:\n\tcase I915_EXEC_CONSTANTS_ABSOLUTE:\n\tcase I915_EXEC_CONSTANTS_REL_SURFACE:\n\t\tif (ring == &dev_priv->ring[RCS] &&\n\t\t    mode != dev_priv->relative_constants_mode) {\n\t\t\tif (INTEL_INFO(dev)->gen < 4)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (INTEL_INFO(dev)->gen > 5 &&\n\t\t\t    mode == I915_EXEC_CONSTANTS_REL_SURFACE)\n\t\t\t\treturn -EINVAL;\n\n\t\t\t/* The HW changed the meaning on this bit on gen6 */\n\t\t\tif (INTEL_INFO(dev)->gen >= 6)\n\t\t\t\tmask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"execbuf with unknown constants: %d\\n\", mode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->num_cliprects != 0) {\n\t\tif (ring != &dev_priv->ring[RCS]) {\n\t\t\tDRM_DEBUG(\"clip rectangles are only valid with the render ring\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tcliprects = kmalloc(args->num_cliprects * sizeof(*cliprects),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (cliprects == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto pre_mutex_err;\n\t\t}\n\n\t\tif (copy_from_user(cliprects,\n\t\t\t\t     (struct drm_clip_rect __user *)(uintptr_t)\n\t\t\t\t     args->cliprects_ptr,\n\t\t\t\t     sizeof(*cliprects)*args->num_cliprects)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto pre_mutex_err;\n\t\t}\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret)\n\t\tgoto pre_mutex_err;\n\n\tif (dev_priv->mm.suspended) {\n\t\tmutex_unlock(&dev->struct_mutex);\n\t\tret = -EBUSY;\n\t\tgoto pre_mutex_err;\n\t}\n\n\teb = eb_create(args->buffer_count);\n\tif (eb == NULL) {\n\t\tmutex_unlock(&dev->struct_mutex);\n\t\tret = -ENOMEM;\n\t\tgoto pre_mutex_err;\n\t}\n\n\t/* Look up object handles */\n\tINIT_LIST_HEAD(&objects);\n\tfor (i = 0; i < args->buffer_count; i++) {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = to_intel_bo(drm_gem_object_lookup(dev, file,\n\t\t\t\t\t\t\texec[i].handle));\n\t\tif (&obj->base == NULL) {\n\t\t\tDRM_DEBUG(\"Invalid object handle %d at index %d\\n\",\n\t\t\t\t   exec[i].handle, i);\n\t\t\t/* prevent error path from reading uninitialized data */\n\t\t\tret = -ENOENT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (!list_empty(&obj->exec_list)) {\n\t\t\tDRM_DEBUG(\"Object %p [handle %d, index %d] appears more than once in object list\\n\",\n\t\t\t\t   obj, exec[i].handle, i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\tlist_add_tail(&obj->exec_list, &objects);\n\t\tobj->exec_handle = exec[i].handle;\n\t\tobj->exec_entry = &exec[i];\n\t\teb_add_object(eb, obj);\n\t}\n\n\t/* take note of the batch buffer before we might reorder the lists */\n\tbatch_obj = list_entry(objects.prev,\n\t\t\t       struct drm_i915_gem_object,\n\t\t\t       exec_list);\n\n\t/* Move the objects en-masse into the GTT, evicting if necessary. */\n\tret = i915_gem_execbuffer_reserve(ring, file, &objects);\n\tif (ret)\n\t\tgoto err;\n\n\t/* The objects are in their final locations, apply the relocations. */\n\tret = i915_gem_execbuffer_relocate(dev, eb, &objects);\n\tif (ret) {\n\t\tif (ret == -EFAULT) {\n\t\t\tret = i915_gem_execbuffer_relocate_slow(dev, file, ring,\n\t\t\t\t\t\t\t\t&objects, eb,\n\t\t\t\t\t\t\t\texec,\n\t\t\t\t\t\t\t\targs->buffer_count);\n\t\t\tBUG_ON(!mutex_is_locked(&dev->struct_mutex));\n\t\t}\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\t/* Set the pending read domains for the batch buffer to COMMAND */\n\tif (batch_obj->base.pending_write_domain) {\n\t\tDRM_DEBUG(\"Attempting to use self-modifying batch buffer\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\tbatch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;\n\n\tret = i915_gem_execbuffer_move_to_gpu(ring, &objects);\n\tif (ret)\n\t\tgoto err;\n\n\tseqno = i915_gem_next_request_seqno(ring);\n\tfor (i = 0; i < ARRAY_SIZE(ring->sync_seqno); i++) {\n\t\tif (seqno < ring->sync_seqno[i]) {\n\t\t\t/* The GPU can not handle its semaphore value wrapping,\n\t\t\t * so every billion or so execbuffers, we need to stall\n\t\t\t * the GPU in order to reset the counters.\n\t\t\t */\n\t\t\tret = i915_gpu_idle(dev, true);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tBUG_ON(ring->sync_seqno[i]);\n\t\t}\n\t}\n\n\tif (ring == &dev_priv->ring[RCS] &&\n\t    mode != dev_priv->relative_constants_mode) {\n\t\tret = intel_ring_begin(ring, 4);\n\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\tintel_ring_emit(ring, MI_NOOP);\n\t\tintel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));\n\t\tintel_ring_emit(ring, INSTPM);\n\t\tintel_ring_emit(ring, mask << 16 | mode);\n\t\tintel_ring_advance(ring);\n\n\t\tdev_priv->relative_constants_mode = mode;\n\t}\n\n\tif (args->flags & I915_EXEC_GEN7_SOL_RESET) {\n\t\tret = i915_reset_gen7_sol_offsets(dev, ring);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ttrace_i915_gem_ring_dispatch(ring, seqno);\n\n\texec_start = batch_obj->gtt_offset + args->batch_start_offset;\n\texec_len = args->batch_len;\n\tif (cliprects) {\n\t\tfor (i = 0; i < args->num_cliprects; i++) {\n\t\t\tret = i915_emit_box(dev, &cliprects[i],\n\t\t\t\t\t    args->DR1, args->DR4);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tret = ring->dispatch_execbuffer(ring,\n\t\t\t\t\t\t\texec_start, exec_len);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t}\n\t} else {\n\t\tret = ring->dispatch_execbuffer(ring, exec_start, exec_len);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ti915_gem_execbuffer_move_to_active(&objects, ring, seqno);\n\ti915_gem_execbuffer_retire_commands(dev, file, ring);\n\nerr:\n\teb_destroy(eb);\n\twhile (!list_empty(&objects)) {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = list_first_entry(&objects,\n\t\t\t\t       struct drm_i915_gem_object,\n\t\t\t\t       exec_list);\n\t\tlist_del_init(&obj->exec_list);\n\t\tdrm_gem_object_unreference(&obj->base);\n\t}\n\n\tmutex_unlock(&dev->struct_mutex);\n\npre_mutex_err:\n\tkfree(cliprects);\n\treturn ret;\n}\n\n/*\n * Legacy execbuffer just creates an exec2 list from the original exec object\n * list array and passes it to the real function.\n */\nint\ni915_gem_execbuffer(struct drm_device *dev, void *data,\n\t\t    struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer *args = data;\n\tstruct drm_i915_gem_execbuffer2 exec2;\n\tstruct drm_i915_gem_exec_object *exec_list = NULL;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret, i;\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Copy in the exec list from userland */\n\texec_list = drm_malloc_ab(sizeof(*exec_list), args->buffer_count);\n\texec2_list = drm_malloc_ab(sizeof(*exec2_list), args->buffer_count);\n\tif (exec_list == NULL || exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\tdrm_free_large(exec_list);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec_list);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tfor (i = 0; i < args->buffer_count; i++) {\n\t\texec2_list[i].handle = exec_list[i].handle;\n\t\texec2_list[i].relocation_count = exec_list[i].relocation_count;\n\t\texec2_list[i].relocs_ptr = exec_list[i].relocs_ptr;\n\t\texec2_list[i].alignment = exec_list[i].alignment;\n\t\texec2_list[i].offset = exec_list[i].offset;\n\t\tif (INTEL_INFO(dev)->gen < 4)\n\t\t\texec2_list[i].flags = EXEC_OBJECT_NEEDS_FENCE;\n\t\telse\n\t\t\texec2_list[i].flags = 0;\n\t}\n\n\texec2.buffers_ptr = args->buffers_ptr;\n\texec2.buffer_count = args->buffer_count;\n\texec2.batch_start_offset = args->batch_start_offset;\n\texec2.batch_len = args->batch_len;\n\texec2.DR1 = args->DR1;\n\texec2.DR4 = args->DR4;\n\texec2.num_cliprects = args->num_cliprects;\n\texec2.cliprects_ptr = args->cliprects_ptr;\n\texec2.flags = I915_EXEC_RENDER;\n\n\tret = i915_gem_do_execbuffer(dev, data, file, &exec2, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tfor (i = 0; i < args->buffer_count; i++)\n\t\t\texec_list[i].offset = exec2_list[i].offset;\n\t\t/* ... and back out to userspace */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec_list,\n\t\t\t\t   sizeof(*exec_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec_list);\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}\n\nint\ni915_gem_execbuffer2(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret;\n\n\tif (args->buffer_count < 1 ||\n\t    args->buffer_count > UINT_MAX / sizeof(*exec2_list)) {\n\t\tDRM_DEBUG(\"execbuf2 with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\texec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,\n\t\t\t     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (exec2_list == NULL)\n\t\texec2_list = drm_malloc_ab(sizeof(*exec2_list),\n\t\t\t\t\t   args->buffer_count);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec2_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec2_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec2_list,\n\t\t\t\t   sizeof(*exec2_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}\n"], "filenames": ["drivers/gpu/drm/i915/i915_gem_execbuffer.c"], "buggy_code_start_loc": [1407], "buggy_code_end_loc": [1408], "fixing_code_start_loc": [1407], "fixing_code_end_loc": [1409], "type": "CWE-189", "message": "Integer overflow in the i915_gem_execbuffer2 function in drivers/gpu/drm/i915/i915_gem_execbuffer.c in the Direct Rendering Manager (DRM) subsystem in the Linux kernel before 3.3.5 on 32-bit platforms allows local users to cause a denial of service (out-of-bounds write) or possibly have unspecified other impact via a crafted ioctl call.", "other": {"cve": {"id": "CVE-2012-2383", "sourceIdentifier": "secalert@redhat.com", "published": "2012-06-13T10:24:56.013", "lastModified": "2023-02-13T00:25:05.843", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Integer overflow in the i915_gem_execbuffer2 function in drivers/gpu/drm/i915/i915_gem_execbuffer.c in the Direct Rendering Manager (DRM) subsystem in the Linux kernel before 3.3.5 on 32-bit platforms allows local users to cause a denial of service (out-of-bounds write) or possibly have unspecified other impact via a crafted ioctl call."}, {"lang": "es", "value": "Desbordamiento de entero en la funci\u00f3n i915_gem_execbuffer2 de drivers/gpu/drm/i915/i915_gem_execbuffer.c del subsistema Direct Rendering Manager (DRM) del kernel de Linux en versiones anteriores a la 3.3.5 en plataformas de 32-bit. Permite a usuarios locales provocar una denegaci\u00f3n de servicio (escritura fuera del l\u00edmite) o posiblemente tener otros inpactos sin especificar a trav\u00e9s de una llamada ioctl modificada."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-189"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.3.4", "matchCriteriaId": "E0CE28E0-37FE-4029-8E27-1D252D4E3B79"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:*:*:*:*:*:*:*", "matchCriteriaId": "AFB76697-1C2F-48C0-9B14-517EC053D4B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc1:*:*:*:*:*:*", "matchCriteriaId": "BED88DFD-1DC5-4505-A441-44ECDEF0252D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc2:*:*:*:*:*:*", "matchCriteriaId": "DBFD2ACD-728A-4082-BB6A-A1EF6E58E47D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc3:*:*:*:*:*:*", "matchCriteriaId": "C31B0E51-F62D-4053-B04F-FC4D5BC373D2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc4:*:*:*:*:*:*", "matchCriteriaId": "A914303E-1CB6-4AAD-9F5F-DE5433C4E814"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc5:*:*:*:*:*:*", "matchCriteriaId": "203BBA69-90B2-4C5E-8023-C14180742421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc6:*:*:*:*:*:*", "matchCriteriaId": "0DBFAB53-B889-4028-AC0E-7E165B152A18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc7:*:*:*:*:*:*", "matchCriteriaId": "FE409AEC-F677-4DEF-8EB7-2C35809043CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.1:*:*:*:*:*:*:*", "matchCriteriaId": "578EC12B-402F-4AD4-B8F8-C9B2CAB06891"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.2:*:*:*:*:*:*:*", "matchCriteriaId": "877002ED-8097-4BB4-BB88-6FC6306C38B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.3:*:*:*:*:*:*:*", "matchCriteriaId": "76294CE3-D72C-41D5-9E0F-B693D0042699"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=ed8cd3b2cd61004cab85380c52b1817aca1ca49b", "source": "secalert@redhat.com"}, {"url": "http://marc.info/?l=bugtraq&m=139447903326211&w=2", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2012-1156.html", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.3.5", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2012/05/22/8", "source": "secalert@redhat.com"}, {"url": "http://www.securityfocus.com/bid/53971", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=824176", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/ed8cd3b2cd61004cab85380c52b1817aca1ca49b", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/ed8cd3b2cd61004cab85380c52b1817aca1ca49b"}}