{"buggy_code": ["/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n *\n * Architecture independence:\n *   Copyright (c) 2005, Bull S.A.\n *   Written by Pierre Peiffer <pierre.peiffer@bull.net>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n */\n\n/*\n * Extents support for EXT4\n *\n * TODO:\n *   - ext4*_error() should be used in some situations\n *   - analyze all BUG()/BUG_ON(), use -EIO where appropriate\n *   - smart tree reduction\n */\n\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/falloc.h>\n#include <asm/uaccess.h>\n#include <linux/fiemap.h>\n#include \"ext4_jbd2.h\"\n\n#include <trace/events/ext4.h>\n\n/*\n * used by extent splitting.\n */\n#define EXT4_EXT_MAY_ZEROOUT\t0x1  /* safe to zeroout if split fails \\\n\t\t\t\t\tdue to ENOSPC */\n#define EXT4_EXT_MARK_UNINIT1\t0x2  /* mark first half uninitialized */\n#define EXT4_EXT_MARK_UNINIT2\t0x4  /* mark second half uninitialized */\n\nstatic __le32 ext4_extent_block_csum(struct inode *inode,\n\t\t\t\t     struct ext4_extent_header *eh)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t__u32 csum;\n\n\tcsum = ext4_chksum(sbi, ei->i_csum_seed, (__u8 *)eh,\n\t\t\t   EXT4_EXTENT_TAIL_OFFSET(eh));\n\treturn cpu_to_le32(csum);\n}\n\nstatic int ext4_extent_block_csum_verify(struct inode *inode,\n\t\t\t\t\t struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb,\n\t\tEXT4_FEATURE_RO_COMPAT_METADATA_CSUM))\n\t\treturn 1;\n\n\tet = find_ext4_extent_tail(eh);\n\tif (et->et_checksum != ext4_extent_block_csum(inode, eh))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic void ext4_extent_block_csum_set(struct inode *inode,\n\t\t\t\t       struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb,\n\t\tEXT4_FEATURE_RO_COMPAT_METADATA_CSUM))\n\t\treturn;\n\n\tet = find_ext4_extent_tail(eh);\n\tet->et_checksum = ext4_extent_block_csum(inode, eh);\n}\n\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t\tstruct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\tint split_flag,\n\t\t\t\tint flags);\n\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags);\n\nstatic int ext4_ext_truncate_extend_restart(handle_t *handle,\n\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t    int needed)\n{\n\tint err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (handle->h_buffer_credits > needed)\n\t\treturn 0;\n\terr = ext4_journal_extend(handle, needed);\n\tif (err <= 0)\n\t\treturn err;\n\terr = ext4_truncate_restart_trans(handle, inode, needed);\n\tif (err == 0)\n\t\terr = -EAGAIN;\n\n\treturn err;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n */\nstatic int ext4_ext_get_access(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\treturn ext4_journal_get_write_access(handle, path->p_bh);\n\t}\n\t/* path points to leaf/index in inode body */\n\t/* we use in-core data, no need to protect them */\n\treturn 0;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n *  - EIO\n */\n#define ext4_ext_dirty(handle, inode, path) \\\n\t\t__ext4_ext_dirty(__func__, __LINE__, (handle), (inode), (path))\nstatic int __ext4_ext_dirty(const char *where, unsigned int line,\n\t\t\t    handle_t *handle, struct inode *inode,\n\t\t\t    struct ext4_ext_path *path)\n{\n\tint err;\n\tif (path->p_bh) {\n\t\text4_extent_block_csum_set(inode, ext_block_hdr(path->p_bh));\n\t\t/* path points to block */\n\t\terr = __ext4_handle_dirty_metadata(where, line, handle,\n\t\t\t\t\t\t   inode, path->p_bh);\n\t} else {\n\t\t/* path points to leaf/index in inode body */\n\t\terr = ext4_mark_inode_dirty(handle, inode);\n\t}\n\treturn err;\n}\n\nstatic ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      ext4_lblk_t block)\n{\n\tif (path) {\n\t\tint depth = path->p_depth;\n\t\tstruct ext4_extent *ex;\n\n\t\t/*\n\t\t * Try to predict block placement assuming that we are\n\t\t * filling in a file which will eventually be\n\t\t * non-sparse --- i.e., in the case of libbfd writing\n\t\t * an ELF object sections out-of-order but in a way\n\t\t * the eventually results in a contiguous object or\n\t\t * executable file, or some database extending a table\n\t\t * space file.  However, this is actually somewhat\n\t\t * non-ideal if we are writing a sparse file such as\n\t\t * qemu or KVM writing a raw image file that is going\n\t\t * to stay fairly sparse, since it will end up\n\t\t * fragmenting the file system's free space.  Maybe we\n\t\t * should have some hueristics or some way to allow\n\t\t * userspace to pass a hint to file system,\n\t\t * especially if the latter case turns out to be\n\t\t * common.\n\t\t */\n\t\tex = path[depth].p_ext;\n\t\tif (ex) {\n\t\t\text4_fsblk_t ext_pblk = ext4_ext_pblock(ex);\n\t\t\text4_lblk_t ext_block = le32_to_cpu(ex->ee_block);\n\n\t\t\tif (block > ext_block)\n\t\t\t\treturn ext_pblk + (block - ext_block);\n\t\t\telse\n\t\t\t\treturn ext_pblk - (ext_block - block);\n\t\t}\n\n\t\t/* it looks like index is empty;\n\t\t * try to find starting block from index itself */\n\t\tif (path[depth].p_bh)\n\t\t\treturn path[depth].p_bh->b_blocknr;\n\t}\n\n\t/* OK. use inode's group */\n\treturn ext4_inode_to_goal_block(inode);\n}\n\n/*\n * Allocation for a meta data block\n */\nstatic ext4_fsblk_t\next4_ext_new_meta_block(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path,\n\t\t\tstruct ext4_extent *ex, int *err, unsigned int flags)\n{\n\text4_fsblk_t goal, newblock;\n\n\tgoal = ext4_ext_find_goal(inode, path, le32_to_cpu(ex->ee_block));\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, flags,\n\t\t\t\t\tNULL, err);\n\treturn newblock;\n}\n\nstatic inline int ext4_ext_space_block(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 6)\n\t\tsize = 6;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_block_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 5)\n\t\tsize = 5;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 3)\n\t\tsize = 3;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 4)\n\t\tsize = 4;\n#endif\n\treturn size;\n}\n\n/*\n * Calculate the number of metadata blocks needed\n * to allocate @blocks\n * Worse case is one block per extent\n */\nint ext4_ext_calc_metadata_amount(struct inode *inode, ext4_lblk_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint idxs;\n\n\tidxs = ((inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t/ sizeof(struct ext4_extent_idx));\n\n\t/*\n\t * If the new delayed allocation block is contiguous with the\n\t * previous da block, it can share index blocks with the\n\t * previous block, so we only need to allocate a new index\n\t * block every idxs leaf blocks.  At ldxs**2 blocks, we need\n\t * an additional index block, and at ldxs**3 blocks, yet\n\t * another index blocks.\n\t */\n\tif (ei->i_da_metadata_calc_len &&\n\t    ei->i_da_metadata_calc_last_lblock+1 == lblock) {\n\t\tint num = 0;\n\n\t\tif ((ei->i_da_metadata_calc_len % idxs) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {\n\t\t\tnum++;\n\t\t\tei->i_da_metadata_calc_len = 0;\n\t\t} else\n\t\t\tei->i_da_metadata_calc_len++;\n\t\tei->i_da_metadata_calc_last_lblock++;\n\t\treturn num;\n\t}\n\n\t/*\n\t * In the worst case we need a new set of index blocks at\n\t * every level of the inode's extent tree.\n\t */\n\tei->i_da_metadata_calc_len = 1;\n\tei->i_da_metadata_calc_last_lblock = lblock;\n\treturn ext_depth(inode) + 1;\n}\n\nstatic int\next4_ext_max_entries(struct inode *inode, int depth)\n{\n\tint max;\n\n\tif (depth == ext_depth(inode)) {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_root(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_root_idx(inode, 1);\n\t} else {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_block(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_block_idx(inode, 1);\n\t}\n\n\treturn max;\n}\n\nstatic int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)\n{\n\text4_fsblk_t block = ext4_ext_pblock(ext);\n\tint len = ext4_ext_get_actual_len(ext);\n\n\tif (len == 0)\n\t\treturn 0;\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, len);\n}\n\nstatic int ext4_valid_extent_idx(struct inode *inode,\n\t\t\t\tstruct ext4_extent_idx *ext_idx)\n{\n\text4_fsblk_t block = ext4_idx_pblock(ext_idx);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, 1);\n}\n\nstatic int ext4_valid_extent_entries(struct inode *inode,\n\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\tint depth)\n{\n\tunsigned short entries;\n\tif (eh->eh_entries == 0)\n\t\treturn 1;\n\n\tentries = le16_to_cpu(eh->eh_entries);\n\n\tif (depth == 0) {\n\t\t/* leaf entries */\n\t\tstruct ext4_extent *ext = EXT_FIRST_EXTENT(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent(inode, ext))\n\t\t\t\treturn 0;\n\t\t\text++;\n\t\t\tentries--;\n\t\t}\n\t} else {\n\t\tstruct ext4_extent_idx *ext_idx = EXT_FIRST_INDEX(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent_idx(inode, ext_idx))\n\t\t\t\treturn 0;\n\t\t\text_idx++;\n\t\t\tentries--;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __ext4_ext_check(const char *function, unsigned int line,\n\t\t\t    struct inode *inode, struct ext4_extent_header *eh,\n\t\t\t    int depth)\n{\n\tconst char *error_msg;\n\tint max = 0;\n\n\tif (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) {\n\t\terror_msg = \"invalid magic\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_depth) != depth)) {\n\t\terror_msg = \"unexpected eh_depth\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(eh->eh_max == 0)) {\n\t\terror_msg = \"invalid eh_max\";\n\t\tgoto corrupted;\n\t}\n\tmax = ext4_ext_max_entries(inode, depth);\n\tif (unlikely(le16_to_cpu(eh->eh_max) > max)) {\n\t\terror_msg = \"too large eh_max\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) {\n\t\terror_msg = \"invalid eh_entries\";\n\t\tgoto corrupted;\n\t}\n\tif (!ext4_valid_extent_entries(inode, eh, depth)) {\n\t\terror_msg = \"invalid extent entries\";\n\t\tgoto corrupted;\n\t}\n\t/* Verify checksum on non-root extent tree nodes */\n\tif (ext_depth(inode) != depth &&\n\t    !ext4_extent_block_csum_verify(inode, eh)) {\n\t\terror_msg = \"extent tree corrupted\";\n\t\tgoto corrupted;\n\t}\n\treturn 0;\n\ncorrupted:\n\text4_error_inode(inode, function, line, 0,\n\t\t\t\"bad header/extent: %s - magic %x, \"\n\t\t\t\"entries %u, max %u(%u), depth %u(%u)\",\n\t\t\terror_msg, le16_to_cpu(eh->eh_magic),\n\t\t\tle16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max),\n\t\t\tmax, le16_to_cpu(eh->eh_depth), depth);\n\n\treturn -EIO;\n}\n\n#define ext4_ext_check(inode, eh, depth)\t\\\n\t__ext4_ext_check(__func__, __LINE__, inode, eh, depth)\n\nint ext4_ext_check_inode(struct inode *inode)\n{\n\treturn ext4_ext_check(inode, ext_inode_hdr(inode), ext_depth(inode));\n}\n\nstatic int __ext4_ext_check_block(const char *function, unsigned int line,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct ext4_extent_header *eh,\n\t\t\t\t  int depth,\n\t\t\t\t  struct buffer_head *bh)\n{\n\tint ret;\n\n\tif (buffer_verified(bh))\n\t\treturn 0;\n\tret = ext4_ext_check(inode, eh, depth);\n\tif (ret)\n\t\treturn ret;\n\tset_buffer_verified(bh);\n\treturn ret;\n}\n\n#define ext4_ext_check_block(inode, eh, depth, bh)\t\\\n\t__ext4_ext_check_block(__func__, __LINE__, inode, eh, depth, bh)\n\n#ifdef EXT_DEBUG\nstatic void ext4_ext_show_path(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint k, l = path->p_depth;\n\n\text_debug(\"path:\");\n\tfor (k = 0; k <= l; k++, path++) {\n\t\tif (path->p_idx) {\n\t\t  ext_debug(\"  %d->%llu\", le32_to_cpu(path->p_idx->ei_block),\n\t\t\t    ext4_idx_pblock(path->p_idx));\n\t\t} else if (path->p_ext) {\n\t\t\text_debug(\"  %d:[%d]%d:%llu \",\n\t\t\t\t  le32_to_cpu(path->p_ext->ee_block),\n\t\t\t\t  ext4_ext_is_uninitialized(path->p_ext),\n\t\t\t\t  ext4_ext_get_actual_len(path->p_ext),\n\t\t\t\t  ext4_ext_pblock(path->p_ext));\n\t\t} else\n\t\t\text_debug(\"  []\");\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_leaf(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex;\n\tint i;\n\n\tif (!path)\n\t\treturn;\n\n\teh = path[depth].p_hdr;\n\tex = EXT_FIRST_EXTENT(eh);\n\n\text_debug(\"Displaying leaf extents for inode %lu\\n\", inode->i_ino);\n\n\tfor (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ex++) {\n\t\text_debug(\"%d:[%d]%d:%llu \", le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex), ext4_ext_pblock(ex));\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_move(struct inode *inode, struct ext4_ext_path *path,\n\t\t\text4_fsblk_t newblock, int level)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\n\tif (depth != level) {\n\t\tstruct ext4_extent_idx *idx;\n\t\tidx = path[level].p_idx;\n\t\twhile (idx <= EXT_MAX_INDEX(path[level].p_hdr)) {\n\t\t\text_debug(\"%d: move %d:%llu in new index %llu\\n\", level,\n\t\t\t\t\tle32_to_cpu(idx->ei_block),\n\t\t\t\t\text4_idx_pblock(idx),\n\t\t\t\t\tnewblock);\n\t\t\tidx++;\n\t\t}\n\n\t\treturn;\n\t}\n\n\tex = path[depth].p_ext;\n\twhile (ex <= EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\text_debug(\"move %d:%llu:[%d]%d in new leaf %llu\\n\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_pblock(ex),\n\t\t\t\text4_ext_is_uninitialized(ex),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tnewblock);\n\t\tex++;\n\t}\n}\n\n#else\n#define ext4_ext_show_path(inode, path)\n#define ext4_ext_show_leaf(inode, path)\n#define ext4_ext_show_move(inode, path, newblock, level)\n#endif\n\nvoid ext4_ext_drop_refs(struct ext4_ext_path *path)\n{\n\tint depth = path->p_depth;\n\tint i;\n\n\tfor (i = 0; i <= depth; i++, path++)\n\t\tif (path->p_bh) {\n\t\t\tbrelse(path->p_bh);\n\t\t\tpath->p_bh = NULL;\n\t\t}\n}\n\n/*\n * ext4_ext_binsearch_idx:\n * binary search for the closest index of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch_idx(struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent_idx *r, *l, *m;\n\n\n\text_debug(\"binsearch for %u(idx):  \", block);\n\n\tl = EXT_FIRST_INDEX(eh) + 1;\n\tr = EXT_LAST_INDEX(eh);\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ei_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ei_block),\n\t\t\t\tm, le32_to_cpu(m->ei_block),\n\t\t\t\tr, le32_to_cpu(r->ei_block));\n\t}\n\n\tpath->p_idx = l - 1;\n\text_debug(\"  -> %u->%lld \", le32_to_cpu(path->p_idx->ei_block),\n\t\t  ext4_idx_pblock(path->p_idx));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent_idx *chix, *ix;\n\t\tint k;\n\n\t\tchix = ix = EXT_FIRST_INDEX(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) {\n\t\t  if (k != 0 &&\n\t\t      le32_to_cpu(ix->ei_block) <= le32_to_cpu(ix[-1].ei_block)) {\n\t\t\t\tprintk(KERN_DEBUG \"k=%d, ix=0x%p, \"\n\t\t\t\t       \"first=0x%p\\n\", k,\n\t\t\t\t       ix, EXT_FIRST_INDEX(eh));\n\t\t\t\tprintk(KERN_DEBUG \"%u <= %u\\n\",\n\t\t\t\t       le32_to_cpu(ix->ei_block),\n\t\t\t\t       le32_to_cpu(ix[-1].ei_block));\n\t\t\t}\n\t\t\tBUG_ON(k && le32_to_cpu(ix->ei_block)\n\t\t\t\t\t   <= le32_to_cpu(ix[-1].ei_block));\n\t\t\tif (block < le32_to_cpu(ix->ei_block))\n\t\t\t\tbreak;\n\t\t\tchix = ix;\n\t\t}\n\t\tBUG_ON(chix != path->p_idx);\n\t}\n#endif\n\n}\n\n/*\n * ext4_ext_binsearch:\n * binary search for closest extent of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch(struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent *r, *l, *m;\n\n\tif (eh->eh_entries == 0) {\n\t\t/*\n\t\t * this leaf is empty:\n\t\t * we get such a leaf in split/add case\n\t\t */\n\t\treturn;\n\t}\n\n\text_debug(\"binsearch for %u:  \", block);\n\n\tl = EXT_FIRST_EXTENT(eh) + 1;\n\tr = EXT_LAST_EXTENT(eh);\n\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ee_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ee_block),\n\t\t\t\tm, le32_to_cpu(m->ee_block),\n\t\t\t\tr, le32_to_cpu(r->ee_block));\n\t}\n\n\tpath->p_ext = l - 1;\n\text_debug(\"  -> %d:%llu:[%d]%d \",\n\t\t\tle32_to_cpu(path->p_ext->ee_block),\n\t\t\text4_ext_pblock(path->p_ext),\n\t\t\text4_ext_is_uninitialized(path->p_ext),\n\t\t\text4_ext_get_actual_len(path->p_ext));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent *chex, *ex;\n\t\tint k;\n\n\t\tchex = ex = EXT_FIRST_EXTENT(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) {\n\t\t\tBUG_ON(k && le32_to_cpu(ex->ee_block)\n\t\t\t\t\t  <= le32_to_cpu(ex[-1].ee_block));\n\t\t\tif (block < le32_to_cpu(ex->ee_block))\n\t\t\t\tbreak;\n\t\t\tchex = ex;\n\t\t}\n\t\tBUG_ON(chex != path->p_ext);\n\t}\n#endif\n\n}\n\nint ext4_ext_tree_init(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_extent_header *eh;\n\n\teh = ext_inode_hdr(inode);\n\teh->eh_depth = 0;\n\teh->eh_entries = 0;\n\teh->eh_magic = EXT4_EXT_MAGIC;\n\teh->eh_max = cpu_to_le16(ext4_ext_space_root(inode, 0));\n\text4_mark_inode_dirty(handle, inode);\n\text4_ext_invalidate_cache(inode);\n\treturn 0;\n}\n\nstruct ext4_ext_path *\next4_ext_find_extent(struct inode *inode, ext4_lblk_t block,\n\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct buffer_head *bh;\n\tshort int depth, i, ppos = 0, alloc = 0;\n\n\teh = ext_inode_hdr(inode);\n\tdepth = ext_depth(inode);\n\n\t/* account possible depth increase */\n\tif (!path) {\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 2),\n\t\t\t\tGFP_NOFS);\n\t\tif (!path)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\talloc = 1;\n\t}\n\tpath[0].p_hdr = eh;\n\tpath[0].p_bh = NULL;\n\n\ti = depth;\n\t/* walk through the tree */\n\twhile (i) {\n\t\text_debug(\"depth %d: num %d, max %d\\n\",\n\t\t\t  ppos, le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\n\t\text4_ext_binsearch_idx(inode, path + ppos, block);\n\t\tpath[ppos].p_block = ext4_idx_pblock(path[ppos].p_idx);\n\t\tpath[ppos].p_depth = i;\n\t\tpath[ppos].p_ext = NULL;\n\n\t\tbh = sb_getblk(inode->i_sb, path[ppos].p_block);\n\t\tif (unlikely(!bh))\n\t\t\tgoto err;\n\t\tif (!bh_uptodate_or_lock(bh)) {\n\t\t\ttrace_ext4_ext_load_extent(inode, block,\n\t\t\t\t\t\tpath[ppos].p_block);\n\t\t\tif (bh_submit_read(bh) < 0) {\n\t\t\t\tput_bh(bh);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\t\teh = ext_block_hdr(bh);\n\t\tppos++;\n\t\tif (unlikely(ppos > depth)) {\n\t\t\tput_bh(bh);\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"ppos %d > depth %d\", ppos, depth);\n\t\t\tgoto err;\n\t\t}\n\t\tpath[ppos].p_bh = bh;\n\t\tpath[ppos].p_hdr = eh;\n\t\ti--;\n\n\t\tif (ext4_ext_check_block(inode, eh, i, bh))\n\t\t\tgoto err;\n\t}\n\n\tpath[ppos].p_depth = i;\n\tpath[ppos].p_ext = NULL;\n\tpath[ppos].p_idx = NULL;\n\n\t/* find extent */\n\text4_ext_binsearch(inode, path + ppos, block);\n\t/* if not an empty leaf */\n\tif (path[ppos].p_ext)\n\t\tpath[ppos].p_block = ext4_ext_pblock(path[ppos].p_ext);\n\n\text4_ext_show_path(inode, path);\n\n\treturn path;\n\nerr:\n\text4_ext_drop_refs(path);\n\tif (alloc)\n\t\tkfree(path);\n\treturn ERR_PTR(-EIO);\n}\n\n/*\n * ext4_ext_insert_index:\n * insert new index [@logical;@ptr] into the block at @curp;\n * check where to insert: before @curp or after @curp\n */\nstatic int ext4_ext_insert_index(handle_t *handle, struct inode *inode,\n\t\t\t\t struct ext4_ext_path *curp,\n\t\t\t\t int logical, ext4_fsblk_t ptr)\n{\n\tstruct ext4_extent_idx *ix;\n\tint len, err;\n\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\treturn err;\n\n\tif (unlikely(logical == le32_to_cpu(curp->p_idx->ei_block))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d == ei_block %d!\",\n\t\t\t\t logical, le32_to_cpu(curp->p_idx->ei_block));\n\t\treturn -EIO;\n\t}\n\n\tif (unlikely(le16_to_cpu(curp->p_hdr->eh_entries)\n\t\t\t     >= le16_to_cpu(curp->p_hdr->eh_max))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"eh_entries %d >= eh_max %d!\",\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_entries),\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_max));\n\t\treturn -EIO;\n\t}\n\n\tif (logical > le32_to_cpu(curp->p_idx->ei_block)) {\n\t\t/* insert after */\n\t\text_debug(\"insert new index %d after: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx + 1;\n\t} else {\n\t\t/* insert before */\n\t\text_debug(\"insert new index %d before: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx;\n\t}\n\n\tlen = EXT_LAST_INDEX(curp->p_hdr) - ix + 1;\n\tBUG_ON(len < 0);\n\tif (len > 0) {\n\t\text_debug(\"insert new index %d: \"\n\t\t\t\t\"move %d indices from 0x%p to 0x%p\\n\",\n\t\t\t\tlogical, len, ix, ix + 1);\n\t\tmemmove(ix + 1, ix, len * sizeof(struct ext4_extent_idx));\n\t}\n\n\tif (unlikely(ix > EXT_MAX_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_MAX_INDEX!\");\n\t\treturn -EIO;\n\t}\n\n\tix->ei_block = cpu_to_le32(logical);\n\text4_idx_store_pblock(ix, ptr);\n\tle16_add_cpu(&curp->p_hdr->eh_entries, 1);\n\n\tif (unlikely(ix > EXT_LAST_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_LAST_INDEX!\");\n\t\treturn -EIO;\n\t}\n\n\terr = ext4_ext_dirty(handle, inode, curp);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_split:\n * inserts new subtree into the path, using free index entry\n * at depth @at:\n * - allocates all needed blocks (new leaf and all intermediate index blocks)\n * - makes decision where to split\n * - moves remaining extents and index entries (right to the split point)\n *   into the newly allocated blocks\n * - initializes subtree\n */\nstatic int ext4_ext_split(handle_t *handle, struct inode *inode,\n\t\t\t  unsigned int flags,\n\t\t\t  struct ext4_ext_path *path,\n\t\t\t  struct ext4_extent *newext, int at)\n{\n\tstruct buffer_head *bh = NULL;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tint i = at, k, m, a;\n\text4_fsblk_t newblock, oldblock;\n\t__le32 border;\n\text4_fsblk_t *ablocks = NULL; /* array of allocated blocks */\n\tint err = 0;\n\n\t/* make decision: where to split? */\n\t/* FIXME: now decision is simplest: at current extent */\n\n\t/* if current leaf will be split, then we should use\n\t * border from split point */\n\tif (unlikely(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"p_ext > EXT_MAX_EXTENT!\");\n\t\treturn -EIO;\n\t}\n\tif (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\tborder = path[depth].p_ext[1].ee_block;\n\t\text_debug(\"leaf will be split.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\t  le32_to_cpu(border));\n\t} else {\n\t\tborder = newext->ee_block;\n\t\text_debug(\"leaf will be added.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\tle32_to_cpu(border));\n\t}\n\n\t/*\n\t * If error occurs, then we break processing\n\t * and mark filesystem read-only. index won't\n\t * be inserted and tree will be in consistent\n\t * state. Next mount will repair buffers too.\n\t */\n\n\t/*\n\t * Get array to track all allocated blocks.\n\t * We need this to handle errors and free blocks\n\t * upon them.\n\t */\n\tablocks = kzalloc(sizeof(ext4_fsblk_t) * depth, GFP_NOFS);\n\tif (!ablocks)\n\t\treturn -ENOMEM;\n\n\t/* allocate all needed blocks */\n\text_debug(\"allocate %d blocks for indexes/leaf\\n\", depth - at);\n\tfor (a = 0; a < depth - at; a++) {\n\t\tnewblock = ext4_ext_new_meta_block(handle, inode, path,\n\t\t\t\t\t\t   newext, &err, flags);\n\t\tif (newblock == 0)\n\t\t\tgoto cleanup;\n\t\tablocks[a] = newblock;\n\t}\n\n\t/* initialize new leaf */\n\tnewblock = ablocks[--a];\n\tif (unlikely(newblock == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"newblock == 0!\");\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err)\n\t\tgoto cleanup;\n\n\tneh = ext_block_hdr(bh);\n\tneh->eh_entries = 0;\n\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tneh->eh_depth = 0;\n\n\t/* move remainder of path[depth] to the new leaf */\n\tif (unlikely(path[depth].p_hdr->eh_entries !=\n\t\t     path[depth].p_hdr->eh_max)) {\n\t\tEXT4_ERROR_INODE(inode, \"eh_entries %d != eh_max %d!\",\n\t\t\t\t path[depth].p_hdr->eh_entries,\n\t\t\t\t path[depth].p_hdr->eh_max);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\t/* start copy from next extent */\n\tm = EXT_MAX_EXTENT(path[depth].p_hdr) - path[depth].p_ext++;\n\text4_ext_show_move(inode, path, newblock, depth);\n\tif (m) {\n\t\tstruct ext4_extent *ex;\n\t\tex = EXT_FIRST_EXTENT(neh);\n\t\tmemmove(ex, path[depth].p_ext, sizeof(struct ext4_extent) * m);\n\t\tle16_add_cpu(&neh->eh_entries, m);\n\t}\n\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto cleanup;\n\tbrelse(bh);\n\tbh = NULL;\n\n\t/* correct old leaf */\n\tif (m) {\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tle16_add_cpu(&path[depth].p_hdr->eh_entries, -m);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t}\n\n\t/* create intermediate indexes */\n\tk = depth - at - 1;\n\tif (unlikely(k < 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"k %d < 0!\", k);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tif (k)\n\t\text_debug(\"create %d intermediate indices\\n\", k);\n\t/* insert new index into current index block */\n\t/* current depth stored in i var */\n\ti = depth - 1;\n\twhile (k--) {\n\t\toldblock = newblock;\n\t\tnewblock = ablocks[--a];\n\t\tbh = sb_getblk(inode->i_sb, newblock);\n\t\tif (!bh) {\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tlock_buffer(bh);\n\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\tneh = ext_block_hdr(bh);\n\t\tneh->eh_entries = cpu_to_le16(1);\n\t\tneh->eh_magic = EXT4_EXT_MAGIC;\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\t\tneh->eh_depth = cpu_to_le16(depth - i);\n\t\tfidx = EXT_FIRST_INDEX(neh);\n\t\tfidx->ei_block = border;\n\t\text4_idx_store_pblock(fidx, oldblock);\n\n\t\text_debug(\"int.index at %d (block %llu): %u -> %llu\\n\",\n\t\t\t\ti, newblock, le32_to_cpu(border), oldblock);\n\n\t\t/* move remainder of path[i] to the new index block */\n\t\tif (unlikely(EXT_MAX_INDEX(path[i].p_hdr) !=\n\t\t\t\t\tEXT_LAST_INDEX(path[i].p_hdr))) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_MAX_INDEX != EXT_LAST_INDEX ee_block %d!\",\n\t\t\t\t\t le32_to_cpu(path[i].p_ext->ee_block));\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\t/* start copy indexes */\n\t\tm = EXT_MAX_INDEX(path[i].p_hdr) - path[i].p_idx++;\n\t\text_debug(\"cur 0x%p, last 0x%p\\n\", path[i].p_idx,\n\t\t\t\tEXT_MAX_INDEX(path[i].p_hdr));\n\t\text4_ext_show_move(inode, path, newblock, i);\n\t\tif (m) {\n\t\t\tmemmove(++fidx, path[i].p_idx,\n\t\t\t\tsizeof(struct ext4_extent_idx) * m);\n\t\t\tle16_add_cpu(&neh->eh_entries, m);\n\t\t}\n\t\text4_extent_block_csum_set(inode, neh);\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tbrelse(bh);\n\t\tbh = NULL;\n\n\t\t/* correct old index */\n\t\tif (m) {\n\t\t\terr = ext4_ext_get_access(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t\tle16_add_cpu(&path[i].p_hdr->eh_entries, -m);\n\t\t\terr = ext4_ext_dirty(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t}\n\n\t\ti--;\n\t}\n\n\t/* insert new index */\n\terr = ext4_ext_insert_index(handle, inode, path + at,\n\t\t\t\t    le32_to_cpu(border), newblock);\n\ncleanup:\n\tif (bh) {\n\t\tif (buffer_locked(bh))\n\t\t\tunlock_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\tif (err) {\n\t\t/* free all allocated blocks in error case */\n\t\tfor (i = 0; i < depth; i++) {\n\t\t\tif (!ablocks[i])\n\t\t\t\tcontinue;\n\t\t\text4_free_blocks(handle, inode, NULL, ablocks[i], 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\t\t}\n\t}\n\tkfree(ablocks);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_grow_indepth:\n * implements tree growing procedure:\n * - allocates new block\n * - moves top-level data (index block or leaf) into the new block\n * - initializes new top-level, creating index that points to the\n *   just created block\n */\nstatic int ext4_ext_grow_indepth(handle_t *handle, struct inode *inode,\n\t\t\t\t unsigned int flags,\n\t\t\t\t struct ext4_extent *newext)\n{\n\tstruct ext4_extent_header *neh;\n\tstruct buffer_head *bh;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\n\tnewblock = ext4_ext_new_meta_block(handle, inode, NULL,\n\t\tnewext, &err, flags);\n\tif (newblock == 0)\n\t\treturn err;\n\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\text4_std_error(inode->i_sb, err);\n\t\treturn err;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err) {\n\t\tunlock_buffer(bh);\n\t\tgoto out;\n\t}\n\n\t/* move top-level index/leaf into new block */\n\tmemmove(bh->b_data, EXT4_I(inode)->i_data,\n\t\tsizeof(EXT4_I(inode)->i_data));\n\n\t/* set size of new block */\n\tneh = ext_block_hdr(bh);\n\t/* old root could have indexes or leaves\n\t * so calculate e_max right way */\n\tif (ext_depth(inode))\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\telse\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto out;\n\n\t/* Update top-level index: num,max,pointer */\n\tneh = ext_inode_hdr(inode);\n\tneh->eh_entries = cpu_to_le16(1);\n\text4_idx_store_pblock(EXT_FIRST_INDEX(neh), newblock);\n\tif (neh->eh_depth == 0) {\n\t\t/* Root extent block becomes index block */\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_root_idx(inode, 0));\n\t\tEXT_FIRST_INDEX(neh)->ei_block =\n\t\t\tEXT_FIRST_EXTENT(neh)->ee_block;\n\t}\n\text_debug(\"new root: num %d(%d), lblock %d, ptr %llu\\n\",\n\t\t  le16_to_cpu(neh->eh_entries), le16_to_cpu(neh->eh_max),\n\t\t  le32_to_cpu(EXT_FIRST_INDEX(neh)->ei_block),\n\t\t  ext4_idx_pblock(EXT_FIRST_INDEX(neh)));\n\n\tle16_add_cpu(&neh->eh_depth, 1);\n\text4_mark_inode_dirty(handle, inode);\nout:\n\tbrelse(bh);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_create_new_leaf:\n * finds empty index and adds new leaf.\n * if no free index is found, then it requests in-depth growing.\n */\nstatic int ext4_ext_create_new_leaf(handle_t *handle, struct inode *inode,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct ext4_ext_path *path,\n\t\t\t\t    struct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *curp;\n\tint depth, i, err = 0;\n\nrepeat:\n\ti = depth = ext_depth(inode);\n\n\t/* walk up to the tree and look for free index entry */\n\tcurp = path + depth;\n\twhile (i > 0 && !EXT_HAS_FREE_INDEX(curp)) {\n\t\ti--;\n\t\tcurp--;\n\t}\n\n\t/* we use already allocated block for index block,\n\t * so subsequent data blocks should be contiguous */\n\tif (EXT_HAS_FREE_INDEX(curp)) {\n\t\t/* if we found index with free entry, then use that\n\t\t * entry: create all needed subtree and add new leaf */\n\t\terr = ext4_ext_split(handle, inode, flags, path, newext, i);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t    (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path))\n\t\t\terr = PTR_ERR(path);\n\t} else {\n\t\t/* tree is full, time to grow in depth */\n\t\terr = ext4_ext_grow_indepth(handle, inode, flags, newext);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t   (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * only first (depth 0 -> 1) produces free space;\n\t\t * in all other cases we have to split the grown tree\n\t\t */\n\t\tdepth = ext_depth(inode);\n\t\tif (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) {\n\t\t\t/* now we need to split */\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * search the closest allocated block to the left for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_left(struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\tint depth, ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_FIRST_EXTENT != ex *logical %d ee_block %d!\",\n\t\t\t\t\t *logical, le32_to_cpu(ex->ee_block));\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t  \"ix (%d) != EXT_FIRST_INDEX (%d) (depth %d)!\",\n\t\t\t\t  ix != NULL ? le32_to_cpu(ix->ei_block) : 0,\n\t\t\t\t  EXT_FIRST_INDEX(path[depth].p_hdr) != NULL ?\n\t\tle32_to_cpu(EXT_FIRST_INDEX(path[depth].p_hdr)->ei_block) : 0,\n\t\t\t\t  depth);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\t*logical = le32_to_cpu(ex->ee_block) + ee_len - 1;\n\t*phys = ext4_ext_pblock(ex) + ee_len - 1;\n\treturn 0;\n}\n\n/*\n * search the closest allocated block to the right for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the largest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t ext4_lblk_t *logical, ext4_fsblk_t *phys,\n\t\t\t\t struct ext4_extent **ret_ex)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t block;\n\tint depth;\t/* Note, NOT eh_depth; depth from top of tree */\n\tint ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"first_extent(path[%d].p_hdr) != ex\",\n\t\t\t\t\t depth);\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"ix != EXT_FIRST_INDEX *logical %d!\",\n\t\t\t\t\t\t *logical);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\tgoto found_extent;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\tif (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t/* next allocated block in this leaf */\n\t\tex++;\n\t\tgoto found_extent;\n\t}\n\n\t/* go up and search for index to the right */\n\twhile (--depth >= 0) {\n\t\tix = path[depth].p_idx;\n\t\tif (ix != EXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\tgoto got_index;\n\t}\n\n\t/* we've gone up to the root and found no index to the right */\n\treturn 0;\n\ngot_index:\n\t/* we've found index to the right, let's\n\t * follow it and find the closest allocated\n\t * block to the right */\n\tix++;\n\tblock = ext4_idx_pblock(ix);\n\twhile (++depth < path->p_depth) {\n\t\tbh = sb_bread(inode->i_sb, block);\n\t\tif (bh == NULL)\n\t\t\treturn -EIO;\n\t\teh = ext_block_hdr(bh);\n\t\t/* subtract from p_depth to get proper eh_depth */\n\t\tif (ext4_ext_check_block(inode, eh,\n\t\t\t\t\t path->p_depth - depth, bh)) {\n\t\t\tput_bh(bh);\n\t\t\treturn -EIO;\n\t\t}\n\t\tix = EXT_FIRST_INDEX(eh);\n\t\tblock = ext4_idx_pblock(ix);\n\t\tput_bh(bh);\n\t}\n\n\tbh = sb_bread(inode->i_sb, block);\n\tif (bh == NULL)\n\t\treturn -EIO;\n\teh = ext_block_hdr(bh);\n\tif (ext4_ext_check_block(inode, eh, path->p_depth - depth, bh)) {\n\t\tput_bh(bh);\n\t\treturn -EIO;\n\t}\n\tex = EXT_FIRST_EXTENT(eh);\nfound_extent:\n\t*logical = le32_to_cpu(ex->ee_block);\n\t*phys = ext4_ext_pblock(ex);\n\t*ret_ex = ex;\n\tif (bh)\n\t\tput_bh(bh);\n\treturn 0;\n}\n\n/*\n * ext4_ext_next_allocated_block:\n * returns allocated block in subsequent extent or EXT_MAX_BLOCKS.\n * NOTE: it considers block number from index entry as\n * allocated block. Thus, index entries have to be consistent\n * with leaves.\n */\nstatic ext4_lblk_t\next4_ext_next_allocated_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn EXT_MAX_BLOCKS;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\t/* leaf */\n\t\t\tif (path[depth].p_ext &&\n\t\t\t\tpath[depth].p_ext !=\n\t\t\t\t\tEXT_LAST_EXTENT(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_ext[1].ee_block);\n\t\t} else {\n\t\t\t/* index */\n\t\t\tif (path[depth].p_idx !=\n\t\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\t}\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_next_leaf_block:\n * returns first allocated block from next leaf or EXT_MAX_BLOCKS\n */\nstatic ext4_lblk_t ext4_ext_next_leaf_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\t/* zero-tree has no leaf blocks at all */\n\tif (depth == 0)\n\t\treturn EXT_MAX_BLOCKS;\n\n\t/* go to index block */\n\tdepth--;\n\n\twhile (depth >= 0) {\n\t\tif (path[depth].p_idx !=\n\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\treturn (ext4_lblk_t)\n\t\t\t\tle32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_correct_indexes:\n * if leaf gets modified and modified extent is first in the leaf,\n * then we have to correct all indexes above.\n * TODO: do we need to correct tree in all cases?\n */\nstatic int ext4_ext_correct_indexes(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\t__le32 border;\n\tint k, err = 0;\n\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\tif (unlikely(ex == NULL || eh == NULL)) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"ex %p == NULL or eh %p == NULL\", ex, eh);\n\t\treturn -EIO;\n\t}\n\n\tif (depth == 0) {\n\t\t/* there is no tree at all */\n\t\treturn 0;\n\t}\n\n\tif (ex != EXT_FIRST_EXTENT(eh)) {\n\t\t/* we correct tree if first leaf got modified only */\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: we need correction if border is smaller than current one\n\t */\n\tk = depth - 1;\n\tborder = path[depth].p_ext->ee_block;\n\terr = ext4_ext_get_access(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\tpath[k].p_idx->ei_block = border;\n\terr = ext4_ext_dirty(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\n\twhile (k--) {\n\t\t/* change all left-side indexes */\n\t\tif (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr))\n\t\t\tbreak;\n\t\terr = ext4_ext_get_access(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath[k].p_idx->ei_block = border;\n\t\terr = ext4_ext_dirty(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint\next4_can_extents_be_merged(struct inode *inode, struct ext4_extent *ex1,\n\t\t\t\tstruct ext4_extent *ex2)\n{\n\tunsigned short ext1_ee_len, ext2_ee_len, max_len;\n\n\t/*\n\t * Make sure that either both extents are uninitialized, or\n\t * both are _not_.\n\t */\n\tif (ext4_ext_is_uninitialized(ex1) ^ ext4_ext_is_uninitialized(ex2))\n\t\treturn 0;\n\n\tif (ext4_ext_is_uninitialized(ex1))\n\t\tmax_len = EXT_UNINIT_MAX_LEN;\n\telse\n\t\tmax_len = EXT_INIT_MAX_LEN;\n\n\text1_ee_len = ext4_ext_get_actual_len(ex1);\n\text2_ee_len = ext4_ext_get_actual_len(ex2);\n\n\tif (le32_to_cpu(ex1->ee_block) + ext1_ee_len !=\n\t\t\tle32_to_cpu(ex2->ee_block))\n\t\treturn 0;\n\n\t/*\n\t * To allow future support for preallocated extents to be added\n\t * as an RO_COMPAT feature, refuse to merge to extents if\n\t * this can result in the top bit of ee_len being set.\n\t */\n\tif (ext1_ee_len + ext2_ee_len > max_len)\n\t\treturn 0;\n#ifdef AGGRESSIVE_TEST\n\tif (ext1_ee_len >= 4)\n\t\treturn 0;\n#endif\n\n\tif (ext4_ext_pblock(ex1) + ext1_ee_len == ext4_ext_pblock(ex2))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function tries to merge the \"ex\" extent to the next extent in the tree.\n * It always tries to merge towards right. If you want to merge towards\n * left, pass \"ex - 1\" as argument instead of \"ex\".\n * Returns 0 if the extents (ex and ex+1) were _not_ merged and returns\n * 1 if they got merged.\n */\nstatic int ext4_ext_try_to_merge_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t struct ext4_extent *ex)\n{\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth, len;\n\tint merge_done = 0;\n\tint uninitialized = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\twhile (ex < EXT_LAST_EXTENT(eh)) {\n\t\tif (!ext4_can_extents_be_merged(inode, ex, ex + 1))\n\t\t\tbreak;\n\t\t/* merge with next extent! */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t+ ext4_ext_get_actual_len(ex + 1));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\n\t\tif (ex + 1 < EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = (EXT_LAST_EXTENT(eh) - ex - 1)\n\t\t\t\t* sizeof(struct ext4_extent);\n\t\t\tmemmove(ex + 1, ex + 2, len);\n\t\t}\n\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\tmerge_done = 1;\n\t\tWARN_ON(eh->eh_entries == 0);\n\t\tif (!eh->eh_entries)\n\t\t\tEXT4_ERROR_INODE(inode, \"eh->eh_entries = 0!\");\n\t}\n\n\treturn merge_done;\n}\n\n/*\n * This function does a very simple check to see if we can collapse\n * an extent tree with a single extent tree leaf block into the inode.\n */\nstatic void ext4_ext_try_to_merge_up(handle_t *handle,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tsize_t s;\n\tunsigned max_root = ext4_ext_space_root(inode, 0);\n\text4_fsblk_t blk;\n\n\tif ((path[0].p_depth != 1) ||\n\t    (le16_to_cpu(path[0].p_hdr->eh_entries) != 1) ||\n\t    (le16_to_cpu(path[1].p_hdr->eh_entries) > max_root))\n\t\treturn;\n\n\t/*\n\t * We need to modify the block allocation bitmap and the block\n\t * group descriptor to release the extent tree block.  If we\n\t * can't get the journal credits, give up.\n\t */\n\tif (ext4_journal_extend(handle, 2))\n\t\treturn;\n\n\t/*\n\t * Copy the extent data up to the inode\n\t */\n\tblk = ext4_idx_pblock(path[0].p_idx);\n\ts = le16_to_cpu(path[1].p_hdr->eh_entries) *\n\t\tsizeof(struct ext4_extent_idx);\n\ts += sizeof(struct ext4_extent_header);\n\n\tmemcpy(path[0].p_hdr, path[1].p_hdr, s);\n\tpath[0].p_depth = 0;\n\tpath[0].p_ext = EXT_FIRST_EXTENT(path[0].p_hdr) +\n\t\t(path[1].p_ext - EXT_FIRST_EXTENT(path[1].p_hdr));\n\tpath[0].p_hdr->eh_max = cpu_to_le16(max_root);\n\n\tbrelse(path[1].p_bh);\n\text4_free_blocks(handle, inode, NULL, blk, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n}\n\n/*\n * This function tries to merge the @ex extent to neighbours in the tree.\n * return 1 if merge left else 0.\n */\nstatic void ext4_ext_try_to_merge(handle_t *handle,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct ext4_ext_path *path,\n\t\t\t\t  struct ext4_extent *ex) {\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth;\n\tint merge_done = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\tif (ex > EXT_FIRST_EXTENT(eh))\n\t\tmerge_done = ext4_ext_try_to_merge_right(inode, path, ex - 1);\n\n\tif (!merge_done)\n\t\t(void) ext4_ext_try_to_merge_right(inode, path, ex);\n\n\text4_ext_try_to_merge_up(handle, inode, path);\n}\n\n/*\n * check if a portion of the \"newext\" extent overlaps with an\n * existing extent.\n *\n * If there is an overlap discovered, it updates the length of the newext\n * such that there will be no overlap, and then returns 1.\n * If there is no overlap found, it returns 0.\n */\nstatic unsigned int ext4_ext_check_overlap(struct ext4_sb_info *sbi,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_extent *newext,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\text4_lblk_t b1, b2;\n\tunsigned int depth, len1;\n\tunsigned int ret = 0;\n\n\tb1 = le32_to_cpu(newext->ee_block);\n\tlen1 = ext4_ext_get_actual_len(newext);\n\tdepth = ext_depth(inode);\n\tif (!path[depth].p_ext)\n\t\tgoto out;\n\tb2 = le32_to_cpu(path[depth].p_ext->ee_block);\n\tb2 &= ~(sbi->s_cluster_ratio - 1);\n\n\t/*\n\t * get the next allocated block if the extent in the path\n\t * is before the requested block(s)\n\t */\n\tif (b2 < b1) {\n\t\tb2 = ext4_ext_next_allocated_block(path);\n\t\tif (b2 == EXT_MAX_BLOCKS)\n\t\t\tgoto out;\n\t\tb2 &= ~(sbi->s_cluster_ratio - 1);\n\t}\n\n\t/* check for wrap through zero on extent logical start block*/\n\tif (b1 + len1 < b1) {\n\t\tlen1 = EXT_MAX_BLOCKS - b1;\n\t\tnewext->ee_len = cpu_to_le16(len1);\n\t\tret = 1;\n\t}\n\n\t/* check for overlap */\n\tif (b1 + len1 > b2) {\n\t\tnewext->ee_len = cpu_to_le16(b2 - b1);\n\t\tret = 1;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * ext4_ext_insert_extent:\n * tries to merge requsted extent into the existing extent or\n * inserts requested extent as new one into the tree,\n * creating new leaf in the no-space case.\n */\nint ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int flag)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tunsigned uninitialized = 0;\n\tint flags = 0;\n\n\tif (unlikely(ext4_ext_get_actual_len(newext) == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"ext4_ext_get_actual_len(newext) == 0\");\n\t\treturn -EIO;\n\t}\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(flag & EXT4_GET_BLOCKS_PRE_IO)\n\t\t&& ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\text_debug(\"append [%d]%d block to %u:[%d]%d (from %llu)\\n\",\n\t\t\t  ext4_ext_is_uninitialized(newext),\n\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t  ext4_ext_pblock(ex));\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t/*\n\t\t * ext4_can_extents_be_merged should have checked that either\n\t\t * both extents are uninitialized, or both aren't. Thus we\n\t\t * need to check only one of them here.\n\t\t */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\teh = path[depth].p_hdr;\n\t\tnearex = ex;\n\t\tgoto merge;\n\t}\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = EXT_MAX_BLOCKS;\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block))\n\t\tnext = ext4_ext_next_leaf_block(path);\n\tif (next != EXT_MAX_BLOCKS) {\n\t\text_debug(\"next leaf block - %u\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_ext_find_extent(inode, next, NULL);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isn't full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto has_space;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\tif (flag & EXT4_GET_BLOCKS_PUNCH_OUT_EXT)\n\t\tflags = EXT4_MB_USE_ROOT_BLOCKS;\n\terr = ext4_ext_create_new_leaf(handle, inode, flags, path, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %u:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tnearex = EXT_FIRST_EXTENT(eh);\n\t} else {\n\t\tif (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n\t\t\t/* Insert after */\n\t\t\text_debug(\"insert %u:%llu:[%d]%d before: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t\tnearex++;\n\t\t} else {\n\t\t\t/* Insert before */\n\t\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\t\text_debug(\"insert %u:%llu:[%d]%d after: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t}\n\t\tlen = EXT_LAST_EXTENT(eh) - nearex + 1;\n\t\tif (len > 0) {\n\t\t\text_debug(\"insert %u:%llu:[%d]%d: \"\n\t\t\t\t\t\"move %d extents from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tlen, nearex, nearex + 1);\n\t\t\tmemmove(nearex + 1, nearex,\n\t\t\t\tlen * sizeof(struct ext4_extent));\n\t\t}\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tpath[depth].p_ext = nearex;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext4_ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents */\n\tif (!(flag & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(handle, inode, path, nearex);\n\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\ncleanup:\n\tif (npath) {\n\t\text4_ext_drop_refs(npath);\n\t\tkfree(npath);\n\t}\n\text4_ext_invalidate_cache(inode);\n\treturn err;\n}\n\nstatic int ext4_ext_walk_space(struct inode *inode, ext4_lblk_t block,\n\t\t\t       ext4_lblk_t num, ext_prepare_callback func,\n\t\t\t       void *cbdata)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_ext_cache cbex;\n\tstruct ext4_extent *ex;\n\text4_lblk_t next, start = 0, end = 0;\n\text4_lblk_t last = block + num;\n\tint depth, exists, err = 0;\n\n\tBUG_ON(func == NULL);\n\tBUG_ON(inode == NULL);\n\n\twhile (block < last && block != EXT_MAX_BLOCKS) {\n\t\tnum = last - block;\n\t\t/* find extent for this block */\n\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\t\tpath = ext4_ext_find_extent(inode, block, path);\n\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tpath = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdepth = ext_depth(inode);\n\t\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tex = path[depth].p_ext;\n\t\tnext = ext4_ext_next_allocated_block(path);\n\n\t\texists = 0;\n\t\tif (!ex) {\n\t\t\t/* there is no extent yet, so try to allocate\n\t\t\t * all requested space */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t} else if (le32_to_cpu(ex->ee_block) > block) {\n\t\t\t/* need to allocate space before found extent */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\t\t/* need to allocate space after found extent */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t\tif (end >= next)\n\t\t\t\tend = next;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)) {\n\t\t\t/*\n\t\t\t * some part of requested space is covered\n\t\t\t * by found extent\n\t\t\t */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block)\n\t\t\t\t+ ext4_ext_get_actual_len(ex);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t\texists = 1;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t\tBUG_ON(end <= start);\n\n\t\tif (!exists) {\n\t\t\tcbex.ec_block = start;\n\t\t\tcbex.ec_len = end - start;\n\t\t\tcbex.ec_start = 0;\n\t\t} else {\n\t\t\tcbex.ec_block = le32_to_cpu(ex->ee_block);\n\t\t\tcbex.ec_len = ext4_ext_get_actual_len(ex);\n\t\t\tcbex.ec_start = ext4_ext_pblock(ex);\n\t\t}\n\n\t\tif (unlikely(cbex.ec_len == 0)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"cbex.ec_len == 0\");\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\terr = func(inode, next, &cbex, ex, cbdata);\n\t\text4_ext_drop_refs(path);\n\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t\tif (err == EXT_REPEAT)\n\t\t\tcontinue;\n\t\telse if (err == EXT_BREAK) {\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ext_depth(inode) != depth) {\n\t\t\t/* depth was changed. we have to realloc path */\n\t\t\tkfree(path);\n\t\t\tpath = NULL;\n\t\t}\n\n\t\tblock = cbex.ec_block + cbex.ec_len;\n\t}\n\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\n\treturn err;\n}\n\nstatic void\next4_ext_put_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t\t__u32 len, ext4_fsblk_t start)\n{\n\tstruct ext4_ext_cache *cex;\n\tBUG_ON(len == 0);\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\ttrace_ext4_ext_put_in_cache(inode, block, len, start);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\tcex->ec_block = block;\n\tcex->ec_len = len;\n\tcex->ec_start = start;\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n}\n\n/*\n * ext4_ext_put_gap_in_cache:\n * calculate boundaries of the gap that the requested block fits into\n * and cache this gap\n */\nstatic void\next4_ext_put_gap_in_cache(struct inode *inode, struct ext4_ext_path *path,\n\t\t\t\text4_lblk_t block)\n{\n\tint depth = ext_depth(inode);\n\tunsigned long len;\n\text4_lblk_t lblock;\n\tstruct ext4_extent *ex;\n\n\tex = path[depth].p_ext;\n\tif (ex == NULL) {\n\t\t/* there is no extent yet, so gap is [0;-] */\n\t\tlblock = 0;\n\t\tlen = EXT_MAX_BLOCKS;\n\t\text_debug(\"cache gap(whole file):\");\n\t} else if (block < le32_to_cpu(ex->ee_block)) {\n\t\tlblock = block;\n\t\tlen = le32_to_cpu(ex->ee_block) - block;\n\t\text_debug(\"cache gap(before): %u [%u:%u]\",\n\t\t\t\tblock,\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\t ext4_ext_get_actual_len(ex));\n\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\text4_lblk_t next;\n\t\tlblock = le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex);\n\n\t\tnext = ext4_ext_next_allocated_block(path);\n\t\text_debug(\"cache gap(after): [%u:%u] %u\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tblock);\n\t\tBUG_ON(next == lblock);\n\t\tlen = next - lblock;\n\t} else {\n\t\tlblock = len = 0;\n\t\tBUG();\n\t}\n\n\text_debug(\" -> %u:%lu\\n\", lblock, len);\n\text4_ext_put_in_cache(inode, lblock, len, 0);\n}\n\n/*\n * ext4_ext_in_cache()\n * Checks to see if the given block is in the cache.\n * If it is, the cached extent is stored in the given\n * cache extent pointer.\n *\n * @inode: The files inode\n * @block: The block to look for in the cache\n * @ex:    Pointer where the cached extent will be stored\n *         if it contains block\n *\n * Return 0 if cache is invalid; 1 if the cache is valid\n */\nstatic int\next4_ext_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t  struct ext4_extent *ex)\n{\n\tstruct ext4_ext_cache *cex;\n\tstruct ext4_sb_info *sbi;\n\tint ret = 0;\n\n\t/*\n\t * We borrow i_block_reservation_lock to protect i_cached_extent\n\t */\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\tsbi = EXT4_SB(inode->i_sb);\n\n\t/* has cache valid data? */\n\tif (cex->ec_len == 0)\n\t\tgoto errout;\n\n\tif (in_range(block, cex->ec_block, cex->ec_len)) {\n\t\tex->ee_block = cpu_to_le32(cex->ec_block);\n\t\text4_ext_store_pblock(ex, cex->ec_start);\n\t\tex->ee_len = cpu_to_le16(cex->ec_len);\n\t\text_debug(\"%u cached by %u:%u:%llu\\n\",\n\t\t\t\tblock,\n\t\t\t\tcex->ec_block, cex->ec_len, cex->ec_start);\n\t\tret = 1;\n\t}\nerrout:\n\ttrace_ext4_ext_in_cache(inode, block, ret);\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\treturn ret;\n}\n\n/*\n * ext4_ext_rm_idx:\n * removes index from the index block.\n */\nstatic int ext4_ext_rm_idx(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path)\n{\n\tint err;\n\text4_fsblk_t leaf;\n\n\t/* free index block */\n\tpath--;\n\tleaf = ext4_idx_pblock(path->p_idx);\n\tif (unlikely(path->p_hdr->eh_entries == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"path->p_hdr->eh_entries == 0\");\n\t\treturn -EIO;\n\t}\n\terr = ext4_ext_get_access(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\n\tif (path->p_idx != EXT_LAST_INDEX(path->p_hdr)) {\n\t\tint len = EXT_LAST_INDEX(path->p_hdr) - path->p_idx;\n\t\tlen *= sizeof(struct ext4_extent_idx);\n\t\tmemmove(path->p_idx, path->p_idx + 1, len);\n\t}\n\n\tle16_add_cpu(&path->p_hdr->eh_entries, -1);\n\terr = ext4_ext_dirty(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\text_debug(\"index is empty, remove it, free block %llu\\n\", leaf);\n\ttrace_ext4_ext_rm_idx(inode, leaf);\n\n\text4_free_blocks(handle, inode, NULL, leaf, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n\treturn err;\n}\n\n/*\n * ext4_ext_calc_credits_for_single_extent:\n * This routine returns max. credits that needed to insert an extent\n * to the extent tree.\n * When pass the actual path, the caller should calculate credits\n * under i_data_sem.\n */\nint ext4_ext_calc_credits_for_single_extent(struct inode *inode, int nrblocks,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path) {\n\t\tint depth = ext_depth(inode);\n\t\tint ret = 0;\n\n\t\t/* probably there is space in leaf? */\n\t\tif (le16_to_cpu(path[depth].p_hdr->eh_entries)\n\t\t\t\t< le16_to_cpu(path[depth].p_hdr->eh_max)) {\n\n\t\t\t/*\n\t\t\t *  There are some space in the leaf tree, no\n\t\t\t *  need to account for leaf block credit\n\t\t\t *\n\t\t\t *  bitmaps and block group descriptor blocks\n\t\t\t *  and other metadata blocks still need to be\n\t\t\t *  accounted.\n\t\t\t */\n\t\t\t/* 1 bitmap, 1 block group descriptor */\n\t\t\tret = 2 + EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ext4_chunk_trans_blocks(inode, nrblocks);\n}\n\n/*\n * How many index/leaf blocks need to change/allocate to modify nrblocks?\n *\n * if nrblocks are fit in a single extent (chunk flag is 1), then\n * in the worse case, each tree level index/leaf need to be changed\n * if the tree split due to insert a new extent, then the old tree\n * index/leaf need to be updated too\n *\n * If the nrblocks are discontiguous, they could cause\n * the whole tree split more than once, but this is really rare.\n */\nint ext4_ext_index_trans_blocks(struct inode *inode, int nrblocks, int chunk)\n{\n\tint index;\n\tint depth = ext_depth(inode);\n\n\tif (chunk)\n\t\tindex = depth * 2;\n\telse\n\t\tindex = depth * 3;\n\n\treturn index;\n}\n\nstatic int ext4_remove_blocks(handle_t *handle, struct inode *inode,\n\t\t\t      struct ext4_extent *ex,\n\t\t\t      ext4_fsblk_t *partial_cluster,\n\t\t\t      ext4_lblk_t from, ext4_lblk_t to)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tunsigned short ee_len =  ext4_ext_get_actual_len(ex);\n\text4_fsblk_t pblk;\n\tint flags = 0;\n\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\tflags |= EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET;\n\telse if (ext4_should_journal_data(inode))\n\t\tflags |= EXT4_FREE_BLOCKS_FORGET;\n\n\t/*\n\t * For bigalloc file systems, we never free a partial cluster\n\t * at the beginning of the extent.  Instead, we make a note\n\t * that we tried freeing the cluster, and check to see if we\n\t * need to free it on a subsequent call to ext4_remove_blocks,\n\t * or at the end of the ext4_truncate() operation.\n\t */\n\tflags |= EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER;\n\n\ttrace_ext4_remove_blocks(inode, ex, from, to, *partial_cluster);\n\t/*\n\t * If we have a partial cluster, and it's different from the\n\t * cluster of the last block, we need to explicitly free the\n\t * partial cluster here.\n\t */\n\tpblk = ext4_ext_pblock(ex) + ee_len - 1;\n\tif (*partial_cluster && (EXT4_B2C(sbi, pblk) != *partial_cluster)) {\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(sbi, *partial_cluster),\n\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\t*partial_cluster = 0;\n\t}\n\n#ifdef EXTENTS_STATS\n\t{\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\tspin_lock(&sbi->s_ext_stats_lock);\n\t\tsbi->s_ext_blocks += ee_len;\n\t\tsbi->s_ext_extents++;\n\t\tif (ee_len < sbi->s_ext_min)\n\t\t\tsbi->s_ext_min = ee_len;\n\t\tif (ee_len > sbi->s_ext_max)\n\t\t\tsbi->s_ext_max = ee_len;\n\t\tif (ext_depth(inode) > sbi->s_depth_max)\n\t\t\tsbi->s_depth_max = ext_depth(inode);\n\t\tspin_unlock(&sbi->s_ext_stats_lock);\n\t}\n#endif\n\tif (from >= le32_to_cpu(ex->ee_block)\n\t    && to == le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\t/* tail removal */\n\t\text4_lblk_t num;\n\n\t\tnum = le32_to_cpu(ex->ee_block) + ee_len - from;\n\t\tpblk = ext4_ext_pblock(ex) + ee_len - num;\n\t\text_debug(\"free last %u blocks starting %llu\\n\", num, pblk);\n\t\text4_free_blocks(handle, inode, NULL, pblk, num, flags);\n\t\t/*\n\t\t * If the block range to be freed didn't start at the\n\t\t * beginning of a cluster, and we removed the entire\n\t\t * extent, save the partial cluster here, since we\n\t\t * might need to delete if we determine that the\n\t\t * truncate operation has removed all of the blocks in\n\t\t * the cluster.\n\t\t */\n\t\tif (pblk & (sbi->s_cluster_ratio - 1) &&\n\t\t    (ee_len == num))\n\t\t\t*partial_cluster = EXT4_B2C(sbi, pblk);\n\t\telse\n\t\t\t*partial_cluster = 0;\n\t} else if (from == le32_to_cpu(ex->ee_block)\n\t\t   && to <= le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\t/* head removal */\n\t\text4_lblk_t num;\n\t\text4_fsblk_t start;\n\n\t\tnum = to - from;\n\t\tstart = ext4_ext_pblock(ex);\n\n\t\text_debug(\"free first %u blocks starting %llu\\n\", num, start);\n\t\text4_free_blocks(handle, inode, NULL, start, num, flags);\n\n\t} else {\n\t\tprintk(KERN_INFO \"strange request: removal(2) \"\n\t\t\t\t\"%u-%u from %u:%u\\n\",\n\t\t\t\tfrom, to, le32_to_cpu(ex->ee_block), ee_len);\n\t}\n\treturn 0;\n}\n\n\n/*\n * ext4_ext_rm_leaf() Removes the extents associated with the\n * blocks appearing between \"start\" and \"end\", and splits the extents\n * if \"start\" and \"end\" appear in the same extent\n *\n * @handle: The journal handle\n * @inode:  The files inode\n * @path:   The path to the leaf\n * @start:  The first block to remove\n * @end:   The last block to remove\n */\nstatic int\next4_ext_rm_leaf(handle_t *handle, struct inode *inode,\n\t\t struct ext4_ext_path *path, ext4_fsblk_t *partial_cluster,\n\t\t ext4_lblk_t start, ext4_lblk_t end)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint err = 0, correct_index = 0;\n\tint depth = ext_depth(inode), credits;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t a, b;\n\tunsigned num;\n\text4_lblk_t ex_ee_block;\n\tunsigned short ex_ee_len;\n\tunsigned uninitialized = 0;\n\tstruct ext4_extent *ex;\n\n\t/* the header must be checked already in ext4_ext_remove_space() */\n\text_debug(\"truncate since %u in leaf to %u\\n\", start, end);\n\tif (!path[depth].p_hdr)\n\t\tpath[depth].p_hdr = ext_block_hdr(path[depth].p_bh);\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\t/* find where to start removing */\n\tex = EXT_LAST_EXTENT(eh);\n\n\tex_ee_block = le32_to_cpu(ex->ee_block);\n\tex_ee_len = ext4_ext_get_actual_len(ex);\n\n\ttrace_ext4_ext_rm_leaf(inode, start, ex, *partial_cluster);\n\n\twhile (ex >= EXT_FIRST_EXTENT(eh) &&\n\t\t\tex_ee_block + ex_ee_len > start) {\n\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\telse\n\t\t\tuninitialized = 0;\n\n\t\text_debug(\"remove ext %u:[%d]%d\\n\", ex_ee_block,\n\t\t\t uninitialized, ex_ee_len);\n\t\tpath[depth].p_ext = ex;\n\n\t\ta = ex_ee_block > start ? ex_ee_block : start;\n\t\tb = ex_ee_block+ex_ee_len - 1 < end ?\n\t\t\tex_ee_block+ex_ee_len - 1 : end;\n\n\t\text_debug(\"  border %u:%u\\n\", a, b);\n\n\t\t/* If this extent is beyond the end of the hole, skip it */\n\t\tif (end < ex_ee_block) {\n\t\t\tex--;\n\t\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t\t\tcontinue;\n\t\t} else if (b != ex_ee_block + ex_ee_len - 1) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"can not handle truncate %u:%u \"\n\t\t\t\t\t \"on extent %u:%u\",\n\t\t\t\t\t start, end, ex_ee_block,\n\t\t\t\t\t ex_ee_block + ex_ee_len - 1);\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t} else if (a != ex_ee_block) {\n\t\t\t/* remove tail of the extent */\n\t\t\tnum = a - ex_ee_block;\n\t\t} else {\n\t\t\t/* remove whole extent: excellent! */\n\t\t\tnum = 0;\n\t\t}\n\t\t/*\n\t\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t\t * descriptor) for each block group; assume two block\n\t\t * groups plus ex_ee_len/blocks_per_block_group for\n\t\t * the worst case\n\t\t */\n\t\tcredits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));\n\t\tif (ex == EXT_FIRST_EXTENT(eh)) {\n\t\t\tcorrect_index = 1;\n\t\t\tcredits += (ext_depth(inode)) + 1;\n\t\t}\n\t\tcredits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);\n\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_remove_blocks(handle, inode, ex, partial_cluster,\n\t\t\t\t\t a, b);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (num == 0)\n\t\t\t/* this extent is removed; mark slot entirely unused */\n\t\t\text4_ext_store_pblock(ex, 0);\n\n\t\tex->ee_len = cpu_to_le16(num);\n\t\t/*\n\t\t * Do not mark uninitialized if all the blocks in the\n\t\t * extent have been removed.\n\t\t */\n\t\tif (uninitialized && num)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\t/*\n\t\t * If the extent was completely released,\n\t\t * we need to remove it from the leaf\n\t\t */\n\t\tif (num == 0) {\n\t\t\tif (end != EXT_MAX_BLOCKS - 1) {\n\t\t\t\t/*\n\t\t\t\t * For hole punching, we need to scoot all the\n\t\t\t\t * extents up when an extent is removed so that\n\t\t\t\t * we dont have blank extents in the middle\n\t\t\t\t */\n\t\t\t\tmemmove(ex, ex+1, (EXT_LAST_EXTENT(eh) - ex) *\n\t\t\t\t\tsizeof(struct ext4_extent));\n\n\t\t\t\t/* Now get rid of the one at the end */\n\t\t\t\tmemset(EXT_LAST_EXTENT(eh), 0,\n\t\t\t\t\tsizeof(struct ext4_extent));\n\t\t\t}\n\t\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\t} else\n\t\t\t*partial_cluster = 0;\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\text_debug(\"new extent: %u:%u:%llu\\n\", ex_ee_block, num,\n\t\t\t\text4_ext_pblock(ex));\n\t\tex--;\n\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t}\n\n\tif (correct_index && eh->eh_entries)\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\n\t/*\n\t * If there is still a entry in the leaf node, check to see if\n\t * it references the partial cluster.  This is the only place\n\t * where it could; if it doesn't, we can free the cluster.\n\t */\n\tif (*partial_cluster && ex >= EXT_FIRST_EXTENT(eh) &&\n\t    (EXT4_B2C(sbi, ext4_ext_pblock(ex) + ex_ee_len - 1) !=\n\t     *partial_cluster)) {\n\t\tint flags = EXT4_FREE_BLOCKS_FORGET;\n\n\t\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\t\tflags |= EXT4_FREE_BLOCKS_METADATA;\n\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(sbi, *partial_cluster),\n\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\t*partial_cluster = 0;\n\t}\n\n\t/* if this leaf is free, then we should\n\t * remove it from index block above */\n\tif (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)\n\t\terr = ext4_ext_rm_idx(handle, inode, path + depth);\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_more_to_rm:\n * returns 1 if current index has to be freed (even partial)\n */\nstatic int\next4_ext_more_to_rm(struct ext4_ext_path *path)\n{\n\tBUG_ON(path->p_idx == NULL);\n\n\tif (path->p_idx < EXT_FIRST_INDEX(path->p_hdr))\n\t\treturn 0;\n\n\t/*\n\t * if truncate on deeper level happened, it wasn't partial,\n\t * so we have to consider current index for truncation\n\t */\n\tif (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,\n\t\t\t\t ext4_lblk_t end)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tint depth = ext_depth(inode);\n\tstruct ext4_ext_path *path = NULL;\n\text4_fsblk_t partial_cluster = 0;\n\thandle_t *handle;\n\tint i = 0, err = 0;\n\n\text_debug(\"truncate since %u to %u\\n\", start, end);\n\n\t/* probably first extent we're gonna free will be last in block */\n\thandle = ext4_journal_start(inode, depth + 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\nagain:\n\text4_ext_invalidate_cache(inode);\n\n\ttrace_ext4_ext_remove_space(inode, start, depth);\n\n\t/*\n\t * Check if we are removing extents inside the extent tree. If that\n\t * is the case, we are going to punch a hole inside the extent tree\n\t * so we have to check whether we need to split the extent covering\n\t * the last block to remove so we can easily remove the part of it\n\t * in ext4_ext_rm_leaf().\n\t */\n\tif (end < EXT_MAX_BLOCKS - 1) {\n\t\tstruct ext4_extent *ex;\n\t\text4_lblk_t ee_block;\n\n\t\t/* find extent for this block */\n\t\tpath = ext4_ext_find_extent(inode, end, NULL);\n\t\tif (IS_ERR(path)) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn PTR_ERR(path);\n\t\t}\n\t\tdepth = ext_depth(inode);\n\t\t/* Leaf not may not exist only if inode has no blocks at all */\n\t\tex = path[depth].p_ext;\n\t\tif (!ex) {\n\t\t\tif (depth) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"path[%d].p_hdr == NULL\",\n\t\t\t\t\t\t depth);\n\t\t\t\terr = -EIO;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\tee_block = le32_to_cpu(ex->ee_block);\n\n\t\t/*\n\t\t * See if the last block is inside the extent, if so split\n\t\t * the extent at 'end' block so we can easily remove the\n\t\t * tail of the first part of the split extent in\n\t\t * ext4_ext_rm_leaf().\n\t\t */\n\t\tif (end >= ee_block &&\n\t\t    end < ee_block + ext4_ext_get_actual_len(ex) - 1) {\n\t\t\tint split_flag = 0;\n\n\t\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\t\tsplit_flag = EXT4_EXT_MARK_UNINIT1 |\n\t\t\t\t\t     EXT4_EXT_MARK_UNINIT2;\n\n\t\t\t/*\n\t\t\t * Split the extent in two so that 'end' is the last\n\t\t\t * block in the first new extent\n\t\t\t */\n\t\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\t\t\tend + 1, split_flag,\n\t\t\t\t\t\tEXT4_GET_BLOCKS_PRE_IO |\n\t\t\t\t\t\tEXT4_GET_BLOCKS_PUNCH_OUT_EXT);\n\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\t/*\n\t * We start scanning from right side, freeing all the blocks\n\t * after i_size and walking into the tree depth-wise.\n\t */\n\tdepth = ext_depth(inode);\n\tif (path) {\n\t\tint k = i = depth;\n\t\twhile (--k > 0)\n\t\t\tpath[k].p_block =\n\t\t\t\tle16_to_cpu(path[k].p_hdr->eh_entries)+1;\n\t} else {\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 1),\n\t\t\t       GFP_NOFS);\n\t\tif (path == NULL) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tpath[0].p_depth = depth;\n\t\tpath[0].p_hdr = ext_inode_hdr(inode);\n\t\ti = 0;\n\n\t\tif (ext4_ext_check(inode, path[0].p_hdr, depth)) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\terr = 0;\n\n\twhile (i >= 0 && err == 0) {\n\t\tif (i == depth) {\n\t\t\t/* this is leaf block */\n\t\t\terr = ext4_ext_rm_leaf(handle, inode, path,\n\t\t\t\t\t       &partial_cluster, start,\n\t\t\t\t\t       end);\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* this is index block */\n\t\tif (!path[i].p_hdr) {\n\t\t\text_debug(\"initialize header\\n\");\n\t\t\tpath[i].p_hdr = ext_block_hdr(path[i].p_bh);\n\t\t}\n\n\t\tif (!path[i].p_idx) {\n\t\t\t/* this level hasn't been touched yet */\n\t\t\tpath[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;\n\t\t\text_debug(\"init index ptr: hdr 0x%p, num %d\\n\",\n\t\t\t\t  path[i].p_hdr,\n\t\t\t\t  le16_to_cpu(path[i].p_hdr->eh_entries));\n\t\t} else {\n\t\t\t/* we were already here, see at next index */\n\t\t\tpath[i].p_idx--;\n\t\t}\n\n\t\text_debug(\"level %d - index, first 0x%p, cur 0x%p\\n\",\n\t\t\t\ti, EXT_FIRST_INDEX(path[i].p_hdr),\n\t\t\t\tpath[i].p_idx);\n\t\tif (ext4_ext_more_to_rm(path + i)) {\n\t\t\tstruct buffer_head *bh;\n\t\t\t/* go to the next level */\n\t\t\text_debug(\"move to level %d (block %llu)\\n\",\n\t\t\t\t  i + 1, ext4_idx_pblock(path[i].p_idx));\n\t\t\tmemset(path + i + 1, 0, sizeof(*path));\n\t\t\tbh = sb_bread(sb, ext4_idx_pblock(path[i].p_idx));\n\t\t\tif (!bh) {\n\t\t\t\t/* should we reset i_size? */\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (WARN_ON(i + 1 > depth)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ext4_ext_check_block(inode, ext_block_hdr(bh),\n\t\t\t\t\t\t\tdepth - i - 1, bh)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpath[i + 1].p_bh = bh;\n\n\t\t\t/* save actual number of indexes since this\n\t\t\t * number is changed at the next iteration */\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);\n\t\t\ti++;\n\t\t} else {\n\t\t\t/* we finished processing this index, go up */\n\t\t\tif (path[i].p_hdr->eh_entries == 0 && i > 0) {\n\t\t\t\t/* index is empty, remove it;\n\t\t\t\t * handle must be already prepared by the\n\t\t\t\t * truncatei_leaf() */\n\t\t\t\terr = ext4_ext_rm_idx(handle, inode, path + i);\n\t\t\t}\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\text_debug(\"return to level %d\\n\", i);\n\t\t}\n\t}\n\n\ttrace_ext4_ext_remove_space_done(inode, start, depth, partial_cluster,\n\t\t\tpath->p_hdr->eh_entries);\n\n\t/* If we still have something in the partial cluster and we have removed\n\t * even the first extent, then we should free the blocks in the partial\n\t * cluster as well. */\n\tif (partial_cluster && path->p_hdr->eh_entries == 0) {\n\t\tint flags = EXT4_FREE_BLOCKS_FORGET;\n\n\t\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\t\tflags |= EXT4_FREE_BLOCKS_METADATA;\n\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(EXT4_SB(sb), partial_cluster),\n\t\t\t\t EXT4_SB(sb)->s_cluster_ratio, flags);\n\t\tpartial_cluster = 0;\n\t}\n\n\t/* TODO: flexible tree reduction should be here */\n\tif (path->p_hdr->eh_entries == 0) {\n\t\t/*\n\t\t * truncate to zero freed all the tree,\n\t\t * so we need to correct eh_depth\n\t\t */\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err == 0) {\n\t\t\text_inode_hdr(inode)->eh_depth = 0;\n\t\t\text_inode_hdr(inode)->eh_max =\n\t\t\t\tcpu_to_le16(ext4_ext_space_root(inode, 0));\n\t\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\t}\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tif (err == -EAGAIN) {\n\t\tpath = NULL;\n\t\tgoto again;\n\t}\n\text4_journal_stop(handle);\n\n\treturn err;\n}\n\n/*\n * called at mount time\n */\nvoid ext4_ext_init(struct super_block *sb)\n{\n\t/*\n\t * possible initialization would be here\n\t */\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) {\n#if defined(AGGRESSIVE_TEST) || defined(CHECK_BINSEARCH) || defined(EXTENTS_STATS)\n\t\tprintk(KERN_INFO \"EXT4-fs: file extents enabled\"\n#ifdef AGGRESSIVE_TEST\n\t\t       \", aggressive tests\"\n#endif\n#ifdef CHECK_BINSEARCH\n\t\t       \", check binsearch\"\n#endif\n#ifdef EXTENTS_STATS\n\t\t       \", stats\"\n#endif\n\t\t       \"\\n\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tspin_lock_init(&EXT4_SB(sb)->s_ext_stats_lock);\n\t\tEXT4_SB(sb)->s_ext_min = 1 << 30;\n\t\tEXT4_SB(sb)->s_ext_max = 0;\n#endif\n\t}\n}\n\n/*\n * called at umount time\n */\nvoid ext4_ext_release(struct super_block *sb)\n{\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS))\n\t\treturn;\n\n#ifdef EXTENTS_STATS\n\tif (EXT4_SB(sb)->s_ext_blocks && EXT4_SB(sb)->s_ext_extents) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\tprintk(KERN_ERR \"EXT4-fs: %lu blocks in %lu extents (%lu ave)\\n\",\n\t\t\tsbi->s_ext_blocks, sbi->s_ext_extents,\n\t\t\tsbi->s_ext_blocks / sbi->s_ext_extents);\n\t\tprintk(KERN_ERR \"EXT4-fs: extents: %lu min, %lu max, max depth %lu\\n\",\n\t\t\tsbi->s_ext_min, sbi->s_ext_max, sbi->s_depth_max);\n\t}\n#endif\n}\n\n/* FIXME!! we need to try to merge to left or right after zero-out  */\nstatic int ext4_ext_zeroout(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\tint ret;\n\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\n\tret = sb_issue_zeroout(inode->i_sb, ee_pblock, ee_len, GFP_NOFS);\n\tif (ret > 0)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * ext4_split_extent_at() splits an extent at given block.\n *\n * @handle: the journal handle\n * @inode: the file inode\n * @path: the path to the extent\n * @split: the logical block where the extent is splitted.\n * @split_flags: indicates if the extent could be zeroout if split fails, and\n *\t\t the states(init or uninit) of new extents.\n * @flags: flags used to insert new extent to extent tree.\n *\n *\n * Splits extent [a, b] into two extents [a, @split) and [@split, b], states\n * of which are deterimined by split_flag.\n *\n * There are two cases:\n *  a> the extent are splitted into two extent.\n *  b> split is not needed, and just mark the extent.\n *\n * return 0 on success.\n */\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT1)\n\t\text4_ext_mark_uninitialized(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\text4_ext_mark_uninitialized(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}\n\n/*\n * ext4_split_extents() splits an extent and mark extent which is covered\n * by @map as split_flags indicates\n *\n * It may result in splitting the extent into multiple extents (upto three)\n * There are three possibilities:\n *   a> There is no split required\n *   b> Splits in two extents: Split is happening at either end of the extent\n *   c> Splits in three extents: Somone is splitting in middle of the extent\n *\n */\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint uninitialized;\n\tint split_flag1, flags1;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tuninitialized = ext4_ext_is_uninitialized(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_drop_refs(path);\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1;\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : map->m_len;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() if someone tries to write\n * to an uninitialized extent. It may result in splitting the uninitialized\n * extent into multiple extents (up to three - one initialized and two\n * uninitialized).\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be initialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * Pre-conditions:\n *  - The extent pointed to by 'path' is uninitialized.\n *  - The extent pointed to by 'path' contains a superset\n *    of the logical span [map->m_lblk, map->m_lblk + map->m_len).\n *\n * Post-conditions on success:\n *  - the returned value is the number of blocks beyond map->l_lblk\n *    that are allocated and initialized.\n *    It is guaranteed to be >= map->m_len.\n */\nstatic int ext4_ext_convert_to_initialized(handle_t *handle,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_map_blocks *map,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\tstruct ext4_sb_info *sbi;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_map_blocks split_map;\n\tstruct ext4_extent zero_ex;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block, eof_block;\n\tunsigned int ee_len, depth;\n\tint allocated, max_zeroout = 0;\n\tint err = 0;\n\tint split_flag = 0;\n\n\text_debug(\"ext4_ext_convert_to_initialized: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\tsbi = EXT4_SB(inode->i_sb);\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tallocated = ee_len - (map->m_lblk - ee_block);\n\n\ttrace_ext4_ext_convert_to_initialized_enter(inode, map, ex);\n\n\t/* Pre-conditions */\n\tBUG_ON(!ext4_ext_is_uninitialized(ex));\n\tBUG_ON(!in_range(map->m_lblk, ee_block, ee_len));\n\n\t/*\n\t * Attempt to transfer newly initialized blocks from the currently\n\t * uninitialized extent to its left neighbor. This is much cheaper\n\t * than an insertion followed by a merge as those involve costly\n\t * memmove() calls. This is the common case in steady state for\n\t * workloads doing fallocate(FALLOC_FL_KEEP_SIZE) followed by append\n\t * writes.\n\t *\n\t * Limitations of the current logic:\n\t *  - L1: we only deal with writes at the start of the extent.\n\t *    The approach could be extended to writes at the end\n\t *    of the extent but this scenario was deemed less common.\n\t *  - L2: we do not deal with writes covering the whole extent.\n\t *    This would require removing the extent if the transfer\n\t *    is possible.\n\t *  - L3: we only attempt to merge with an extent stored in the\n\t *    same extent tree node.\n\t */\n\tif ((map->m_lblk == ee_block) &&\t/*L1*/\n\t\t(map->m_len < ee_len) &&\t/*L2*/\n\t\t(ex > EXT_FIRST_EXTENT(eh))) {\t/*L3*/\n\t\tstruct ext4_extent *prev_ex;\n\t\text4_lblk_t prev_lblk;\n\t\text4_fsblk_t prev_pblk, ee_pblk;\n\t\tunsigned int prev_len, write_len;\n\n\t\tprev_ex = ex - 1;\n\t\tprev_lblk = le32_to_cpu(prev_ex->ee_block);\n\t\tprev_len = ext4_ext_get_actual_len(prev_ex);\n\t\tprev_pblk = ext4_ext_pblock(prev_ex);\n\t\tee_pblk = ext4_ext_pblock(ex);\n\t\twrite_len = map->m_len;\n\n\t\t/*\n\t\t * A transfer of blocks from 'ex' to 'prev_ex' is allowed\n\t\t * upon those conditions:\n\t\t * - C1: prev_ex is initialized,\n\t\t * - C2: prev_ex is logically abutting ex,\n\t\t * - C3: prev_ex is physically abutting ex,\n\t\t * - C4: prev_ex can receive the additional blocks without\n\t\t *   overflowing the (initialized) length limit.\n\t\t */\n\t\tif ((!ext4_ext_is_uninitialized(prev_ex)) &&\t\t/*C1*/\n\t\t\t((prev_lblk + prev_len) == ee_block) &&\t\t/*C2*/\n\t\t\t((prev_pblk + prev_len) == ee_pblk) &&\t\t/*C3*/\n\t\t\t(prev_len < (EXT_INIT_MAX_LEN - write_len))) {\t/*C4*/\n\t\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\ttrace_ext4_ext_convert_to_initialized_fastpath(inode,\n\t\t\t\tmap, ex, prev_ex);\n\n\t\t\t/* Shift the start of ex by 'write_len' blocks */\n\t\t\tex->ee_block = cpu_to_le32(ee_block + write_len);\n\t\t\text4_ext_store_pblock(ex, ee_pblk + write_len);\n\t\t\tex->ee_len = cpu_to_le16(ee_len - write_len);\n\t\t\text4_ext_mark_uninitialized(ex); /* Restore the flag */\n\n\t\t\t/* Extend prev_ex by 'write_len' blocks */\n\t\t\tprev_ex->ee_len = cpu_to_le16(prev_len + write_len);\n\n\t\t\t/* Mark the block containing both extents as dirty */\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\n\t\t\t/* Update path to point to the right extent */\n\t\t\tpath[depth].p_ext = prev_ex;\n\n\t\t\t/* Result: number of initialized blocks past m_lblk */\n\t\t\tallocated = write_len;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tWARN_ON(map->m_lblk < ee_block);\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\n\tif (EXT4_EXT_MAY_ZEROOUT & split_flag)\n\t\tmax_zeroout = sbi->s_extent_max_zeroout_kb >>\n\t\t\tinode->i_sb->s_blocksize_bits;\n\n\t/* If extent is less than s_max_zeroout_kb, zeroout directly */\n\tif (max_zeroout && (ee_len <= max_zeroout)) {\n\t\terr = ext4_ext_zeroout(inode, ex);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\t\text4_ext_mark_initialized(ex);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * four cases:\n\t * 1. split the extent into three extents.\n\t * 2. split the extent into two extents, zeroout the first half.\n\t * 3. split the extent into two extents, zeroout the second half.\n\t * 4. split the extent into two extents with out zeroout.\n\t */\n\tsplit_map.m_lblk = map->m_lblk;\n\tsplit_map.m_len = map->m_len;\n\n\tif (max_zeroout && (allocated > map->m_len)) {\n\t\tif (allocated <= max_zeroout) {\n\t\t\t/* case 3 */\n\t\t\tzero_ex.ee_block =\n\t\t\t\t\t cpu_to_le32(map->m_lblk);\n\t\t\tzero_ex.ee_len = cpu_to_le16(allocated);\n\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\text4_ext_pblock(ex) + map->m_lblk - ee_block);\n\t\t\terr = ext4_ext_zeroout(inode, &zero_ex);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tsplit_map.m_lblk = map->m_lblk;\n\t\t\tsplit_map.m_len = allocated;\n\t\t} else if (map->m_lblk - ee_block + map->m_len < max_zeroout) {\n\t\t\t/* case 2 */\n\t\t\tif (map->m_lblk != ee_block) {\n\t\t\t\tzero_ex.ee_block = ex->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(map->m_lblk -\n\t\t\t\t\t\t\tee_block);\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t\terr = ext4_ext_zeroout(inode, &zero_ex);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tsplit_map.m_lblk = ee_block;\n\t\t\tsplit_map.m_len = map->m_lblk - ee_block + map->m_len;\n\t\t\tallocated = map->m_len;\n\t\t}\n\t}\n\n\tallocated = ext4_split_extent(handle, inode, path,\n\t\t\t\t      &split_map, split_flag, 0);\n\tif (allocated < 0)\n\t\terr = allocated;\n\nout:\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() from\n * ext4_get_blocks_dio_write() when DIO to write\n * to an uninitialized extent.\n *\n * Writing to an uninitialized extent may result in splitting the uninitialized\n * extent into multiple initialized/uninitialized extents (up to three)\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be uninitialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * One of more index blocks maybe needed if the extent tree grow after\n * the uninitialized extent split. To prevent ENOSPC occur at the IO\n * complete, we need to split the uninitialized extent before DIO submit\n * the IO. The uninitialized extent called at this time will be split\n * into three uninitialized extent(at most). After IO complete, the part\n * being filled will be convert to initialized by the end_io callback function\n * via ext4_convert_unwritten_extents().\n *\n * Returns the size of uninitialized extent to be written on success.\n */\nstatic int ext4_split_unwritten_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tint flags)\n{\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"ext4_split_unwritten_extents: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\tsplit_flag |= EXT4_EXT_MARK_UNINIT2;\n\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, path, map, split_flag, flags);\n}\n\nstatic int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t      struct inode *inode,\n\t\t\t\t\t      struct ext4_ext_path *path)\n{\n\tstruct ext4_extent *ex;\n\tint depth;\n\tint err = 0;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\n\text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)le32_to_cpu(ex->ee_block),\n\t\text4_ext_get_actual_len(ex));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}\n\nstatic void unmap_underlying_metadata_blocks(struct block_device *bdev,\n\t\t\tsector_t block, int count)\n{\n\tint i;\n\tfor (i = 0; i < count; i++)\n                unmap_underlying_metadata(bdev, block + i);\n}\n\n/*\n * Handle EOFBLOCKS_FL flag, clearing it if necessary\n */\nstatic int check_eofblocks_fl(handle_t *handle, struct inode *inode,\n\t\t\t      ext4_lblk_t lblk,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      unsigned int len)\n{\n\tint i, depth;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *last_ex;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS))\n\t\treturn 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\n\t/*\n\t * We're going to remove EOFBLOCKS_FL entirely in future so we\n\t * do not care for this case anymore. Simply remove the flag\n\t * if there are no extents.\n\t */\n\tif (unlikely(!eh->eh_entries))\n\t\tgoto out;\n\tlast_ex = EXT_LAST_EXTENT(eh);\n\t/*\n\t * We should clear the EOFBLOCKS_FL flag if we are writing the\n\t * last block in the last extent in the file.  We test this by\n\t * first checking to see if the caller to\n\t * ext4_ext_get_blocks() was interested in the last block (or\n\t * a block beyond the last block) in the current extent.  If\n\t * this turns out to be false, we can bail out from this\n\t * function immediately.\n\t */\n\tif (lblk + len < le32_to_cpu(last_ex->ee_block) +\n\t    ext4_ext_get_actual_len(last_ex))\n\t\treturn 0;\n\t/*\n\t * If the caller does appear to be planning to write at or\n\t * beyond the end of the current extent, we then test to see\n\t * if the current extent is the last extent in the file, by\n\t * checking to make sure it was reached via the rightmost node\n\t * at each level of the tree.\n\t */\n\tfor (i = depth-1; i >= 0; i--)\n\t\tif (path[i].p_idx != EXT_LAST_INDEX(path[i].p_hdr))\n\t\t\treturn 0;\nout:\n\text4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\treturn ext4_mark_inode_dirty(handle, inode);\n}\n\n/**\n * ext4_find_delalloc_range: find delayed allocated block in the given range.\n *\n * Goes through the buffer heads in the range [lblk_start, lblk_end] and returns\n * whether there are any buffers marked for delayed allocation. It returns '1'\n * on the first delalloc'ed buffer head found. If no buffer head in the given\n * range is marked for delalloc, it returns 0.\n * lblk_start should always be <= lblk_end.\n * search_hint_reverse is to indicate that searching in reverse from lblk_end to\n * lblk_start might be more efficient (i.e., we will likely hit the delalloc'ed\n * block sooner). This is useful when blocks are truncated sequentially from\n * lblk_start towards lblk_end.\n */\nstatic int ext4_find_delalloc_range(struct inode *inode,\n\t\t\t\t    ext4_lblk_t lblk_start,\n\t\t\t\t    ext4_lblk_t lblk_end,\n\t\t\t\t    int search_hint_reverse)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct buffer_head *head, *bh = NULL;\n\tstruct page *page;\n\text4_lblk_t i, pg_lblk;\n\tpgoff_t index;\n\n\tif (!test_opt(inode->i_sb, DELALLOC))\n\t\treturn 0;\n\n\t/* reverse search wont work if fs block size is less than page size */\n\tif (inode->i_blkbits < PAGE_CACHE_SHIFT)\n\t\tsearch_hint_reverse = 0;\n\n\tif (search_hint_reverse)\n\t\ti = lblk_end;\n\telse\n\t\ti = lblk_start;\n\n\tindex = i >> (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\n\twhile ((i >= lblk_start) && (i <= lblk_end)) {\n\t\tpage = find_get_page(mapping, index);\n\t\tif (!page)\n\t\t\tgoto nextpage;\n\n\t\tif (!page_has_buffers(page))\n\t\t\tgoto nextpage;\n\n\t\thead = page_buffers(page);\n\t\tif (!head)\n\t\t\tgoto nextpage;\n\n\t\tbh = head;\n\t\tpg_lblk = index << (PAGE_CACHE_SHIFT -\n\t\t\t\t\t\tinode->i_blkbits);\n\t\tdo {\n\t\t\tif (unlikely(pg_lblk < lblk_start)) {\n\t\t\t\t/*\n\t\t\t\t * This is possible when fs block size is less\n\t\t\t\t * than page size and our cluster starts/ends in\n\t\t\t\t * middle of the page. So we need to skip the\n\t\t\t\t * initial few blocks till we reach the 'lblk'\n\t\t\t\t */\n\t\t\t\tpg_lblk++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* Check if the buffer is delayed allocated and that it\n\t\t\t * is not yet mapped. (when da-buffers are mapped during\n\t\t\t * their writeout, their da_mapped bit is set.)\n\t\t\t */\n\t\t\tif (buffer_delay(bh) && !buffer_da_mapped(bh)) {\n\t\t\t\tpage_cache_release(page);\n\t\t\t\ttrace_ext4_find_delalloc_range(inode,\n\t\t\t\t\t\tlblk_start, lblk_end,\n\t\t\t\t\t\tsearch_hint_reverse,\n\t\t\t\t\t\t1, i);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (search_hint_reverse)\n\t\t\t\ti--;\n\t\t\telse\n\t\t\t\ti++;\n\t\t} while ((i >= lblk_start) && (i <= lblk_end) &&\n\t\t\t\t((bh = bh->b_this_page) != head));\nnextpage:\n\t\tif (page)\n\t\t\tpage_cache_release(page);\n\t\t/*\n\t\t * Move to next page. 'i' will be the first lblk in the next\n\t\t * page.\n\t\t */\n\t\tif (search_hint_reverse)\n\t\t\tindex--;\n\t\telse\n\t\t\tindex++;\n\t\ti = index << (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\t}\n\n\ttrace_ext4_find_delalloc_range(inode, lblk_start, lblk_end,\n\t\t\t\t\tsearch_hint_reverse, 0, 0);\n\treturn 0;\n}\n\nint ext4_find_delalloc_cluster(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t       int search_hint_reverse)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t lblk_start, lblk_end;\n\tlblk_start = lblk & (~(sbi->s_cluster_ratio - 1));\n\tlblk_end = lblk_start + sbi->s_cluster_ratio - 1;\n\n\treturn ext4_find_delalloc_range(inode, lblk_start, lblk_end,\n\t\t\t\t\tsearch_hint_reverse);\n}\n\n/**\n * Determines how many complete clusters (out of those specified by the 'map')\n * are under delalloc and were reserved quota for.\n * This function is called when we are writing out the blocks that were\n * originally written with their allocation delayed, but then the space was\n * allocated using fallocate() before the delayed allocation could be resolved.\n * The cases to look for are:\n * ('=' indicated delayed allocated blocks\n *  '-' indicates non-delayed allocated blocks)\n * (a) partial clusters towards beginning and/or end outside of allocated range\n *     are not delalloc'ed.\n *\tEx:\n *\t|----c---=|====c====|====c====|===-c----|\n *\t         |++++++ allocated ++++++|\n *\t==> 4 complete clusters in above example\n *\n * (b) partial cluster (outside of allocated range) towards either end is\n *     marked for delayed allocation. In this case, we will exclude that\n *     cluster.\n *\tEx:\n *\t|----====c========|========c========|\n *\t     |++++++ allocated ++++++|\n *\t==> 1 complete clusters in above example\n *\n *\tEx:\n *\t|================c================|\n *            |++++++ allocated ++++++|\n *\t==> 0 complete clusters in above example\n *\n * The ext4_da_update_reserve_space will be called only if we\n * determine here that there were some \"entire\" clusters that span\n * this 'allocated' range.\n * In the non-bigalloc case, this function will just end up returning num_blks\n * without ever calling ext4_find_delalloc_range.\n */\nstatic unsigned int\nget_reserved_cluster_alloc(struct inode *inode, ext4_lblk_t lblk_start,\n\t\t\t   unsigned int num_blks)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t alloc_cluster_start, alloc_cluster_end;\n\text4_lblk_t lblk_from, lblk_to, c_offset;\n\tunsigned int allocated_clusters = 0;\n\n\talloc_cluster_start = EXT4_B2C(sbi, lblk_start);\n\talloc_cluster_end = EXT4_B2C(sbi, lblk_start + num_blks - 1);\n\n\t/* max possible clusters for this allocation */\n\tallocated_clusters = alloc_cluster_end - alloc_cluster_start + 1;\n\n\ttrace_ext4_get_reserved_cluster_alloc(inode, lblk_start, num_blks);\n\n\t/* Check towards left side */\n\tc_offset = lblk_start & (sbi->s_cluster_ratio - 1);\n\tif (c_offset) {\n\t\tlblk_from = lblk_start & (~(sbi->s_cluster_ratio - 1));\n\t\tlblk_to = lblk_from + c_offset - 1;\n\n\t\tif (ext4_find_delalloc_range(inode, lblk_from, lblk_to, 0))\n\t\t\tallocated_clusters--;\n\t}\n\n\t/* Now check towards right. */\n\tc_offset = (lblk_start + num_blks) & (sbi->s_cluster_ratio - 1);\n\tif (allocated_clusters && c_offset) {\n\t\tlblk_from = lblk_start + num_blks;\n\t\tlblk_to = lblk_from + (sbi->s_cluster_ratio - c_offset) - 1;\n\n\t\tif (ext4_find_delalloc_range(inode, lblk_from, lblk_to, 0))\n\t\t\tallocated_clusters--;\n\t}\n\n\treturn allocated_clusters;\n}\n\nstatic int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\ttrace_ext4_ext_handle_uninitialized_extents(inode, map, allocated,\n\t\t\t\t\t\t    newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle, inode, map,\n\t\t\t\t\t\t   path, flags);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, path);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\tmap->m_lblk, map->m_len);\n\t\tif (reserved_clusters)\n\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\t     reserved_clusters,\n\t\t\t\t\t\t     0);\n\t}\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}\n\n/*\n * get_implied_cluster_alloc - check to see if the requested\n * allocation (in the map structure) overlaps with a cluster already\n * allocated in an extent.\n *\t@sb\tThe filesystem superblock structure\n *\t@map\tThe requested lblk->pblk mapping\n *\t@ex\tThe extent structure which might contain an implied\n *\t\t\tcluster allocation\n *\n * This function is called by ext4_ext_map_blocks() after we failed to\n * find blocks that were already in the inode's extent tree.  Hence,\n * we know that the beginning of the requested region cannot overlap\n * the extent from the inode's extent tree.  There are three cases we\n * want to catch.  The first is this case:\n *\n *\t\t |--- cluster # N--|\n *    |--- extent ---|\t|---- requested region ---|\n *\t\t\t|==========|\n *\n * The second case that we need to test for is this one:\n *\n *   |--------- cluster # N ----------------|\n *\t   |--- requested region --|   |------- extent ----|\n *\t   |=======================|\n *\n * The third case is when the requested region lies between two extents\n * within the same cluster:\n *          |------------- cluster # N-------------|\n * |----- ex -----|                  |---- ex_right ----|\n *                  |------ requested region ------|\n *                  |================|\n *\n * In each of the above cases, we need to set the map->m_pblk and\n * map->m_len so it corresponds to the return the extent labelled as\n * \"|====|\" from cluster #N, since it is already in use for data in\n * cluster EXT4_B2C(sbi, map->m_lblk).\tWe will then return 1 to\n * signal to ext4_ext_map_blocks() that map->m_pblk should be treated\n * as a new \"allocated\" block region.  Otherwise, we will return 0 and\n * ext4_ext_map_blocks() will then allocate one or more new clusters\n * by calling ext4_mb_new_blocks().\n */\nstatic int get_implied_cluster_alloc(struct super_block *sb,\n\t\t\t\t     struct ext4_map_blocks *map,\n\t\t\t\t     struct ext4_extent *ex,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_lblk_t c_offset = map->m_lblk & (sbi->s_cluster_ratio-1);\n\text4_lblk_t ex_cluster_start, ex_cluster_end;\n\text4_lblk_t rr_cluster_start;\n\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\tunsigned short ee_len = ext4_ext_get_actual_len(ex);\n\n\t/* The extent passed in that we are trying to match */\n\tex_cluster_start = EXT4_B2C(sbi, ee_block);\n\tex_cluster_end = EXT4_B2C(sbi, ee_block + ee_len - 1);\n\n\t/* The requested region passed into ext4_map_blocks() */\n\trr_cluster_start = EXT4_B2C(sbi, map->m_lblk);\n\n\tif ((rr_cluster_start == ex_cluster_end) ||\n\t    (rr_cluster_start == ex_cluster_start)) {\n\t\tif (rr_cluster_start == ex_cluster_end)\n\t\t\tee_start += ee_len - 1;\n\t\tmap->m_pblk = (ee_start & ~(sbi->s_cluster_ratio - 1)) +\n\t\t\tc_offset;\n\t\tmap->m_len = min(map->m_len,\n\t\t\t\t (unsigned) sbi->s_cluster_ratio - c_offset);\n\t\t/*\n\t\t * Check for and handle this case:\n\t\t *\n\t\t *   |--------- cluster # N-------------|\n\t\t *\t\t       |------- extent ----|\n\t\t *\t   |--- requested region ---|\n\t\t *\t   |===========|\n\t\t */\n\n\t\tif (map->m_lblk < ee_block)\n\t\t\tmap->m_len = min(map->m_len, ee_block - map->m_lblk);\n\n\t\t/*\n\t\t * Check for the case where there is already another allocated\n\t\t * block to the right of 'ex' but before the end of the cluster.\n\t\t *\n\t\t *          |------------- cluster # N-------------|\n\t\t * |----- ex -----|                  |---- ex_right ----|\n\t\t *                  |------ requested region ------|\n\t\t *                  |================|\n\t\t */\n\t\tif (map->m_lblk > ee_block) {\n\t\t\text4_lblk_t next = ext4_ext_next_allocated_block(path);\n\t\t\tmap->m_len = min(map->m_len, next - map->m_lblk);\n\t\t}\n\n\t\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 1);\n\t\treturn 1;\n\t}\n\n\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 0);\n\treturn 0;\n}\n\n\n/*\n * Block allocation/map/preallocation routine for extents based files\n *\n *\n * Need to be called with\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system block\n * (ie, create is zero). Otherwise down_write(&EXT4_I(inode)->i_data_sem)\n *\n * return > 0, number of of blocks already mapped/allocated\n *          if create == 0 and these are pre-allocated blocks\n *          \tbuffer head is unmapped\n *          otherwise blocks are mapped\n *\n * return = 0, if plain look up failed (blocks have not been allocated)\n *          buffer head is unmapped\n *\n * return < 0, error case.\n */\nint ext4_ext_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map, int flags)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent newex, *ex, *ex2;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_fsblk_t newblock = 0;\n\tint free_on_err = 0, err = 0, depth, ret;\n\tunsigned int allocated = 0, offset = 0;\n\tunsigned int allocated_clusters = 0;\n\tstruct ext4_allocation_request ar;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\text4_lblk_t cluster_offset;\n\tint set_unwritten = 0;\n\n\text_debug(\"blocks %u/%u requested for inode %lu\\n\",\n\t\t  map->m_lblk, map->m_len, inode->i_ino);\n\ttrace_ext4_ext_map_blocks_enter(inode, map->m_lblk, map->m_len, flags);\n\n\t/* check in cache */\n\tif (ext4_ext_in_cache(inode, map->m_lblk, &newex)) {\n\t\tif (!newex.ee_start_lo && !newex.ee_start_hi) {\n\t\t\tif ((sbi->s_cluster_ratio > 1) &&\n\t\t\t    ext4_find_delalloc_cluster(inode, map->m_lblk, 0))\n\t\t\t\tmap->m_flags |= EXT4_MAP_FROM_CLUSTER;\n\n\t\t\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t\t\t/*\n\t\t\t\t * block isn't allocated yet and\n\t\t\t\t * user doesn't want to allocate it\n\t\t\t\t */\n\t\t\t\tgoto out2;\n\t\t\t}\n\t\t\t/* we should allocate requested block */\n\t\t} else {\n\t\t\t/* block is already allocated */\n\t\t\tif (sbi->s_cluster_ratio > 1)\n\t\t\t\tmap->m_flags |= EXT4_MAP_FROM_CLUSTER;\n\t\t\tnewblock = map->m_lblk\n\t\t\t\t   - le32_to_cpu(newex.ee_block)\n\t\t\t\t   + ext4_ext_pblock(&newex);\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ext4_ext_get_actual_len(&newex) -\n\t\t\t\t(map->m_lblk - le32_to_cpu(newex.ee_block));\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* find extent for this block */\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, NULL);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out2;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * consistent leaf must not be empty;\n\t * this situation is possible, though, _during_ tree modification;\n\t * this is why assert can't be put in ext4_ext_find_extent()\n\t */\n\tif (unlikely(path[depth].p_ext == NULL && depth != 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extent address \"\n\t\t\t\t \"lblock: %lu, depth: %d pblock %lld\",\n\t\t\t\t (unsigned long) map->m_lblk, depth,\n\t\t\t\t path[depth].p_block);\n\t\terr = -EIO;\n\t\tgoto out2;\n\t}\n\n\tex = path[depth].p_ext;\n\tif (ex) {\n\t\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\t\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\t\tunsigned short ee_len;\n\n\t\t/*\n\t\t * Uninitialized extents are treated as holes, except that\n\t\t * we split out initialized portions during a write.\n\t\t */\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\n\t\ttrace_ext4_ext_show_extent(inode, ee_block, ee_start, ee_len);\n\n\t\t/* if found extent covers block, simply return it */\n\t\tif (in_range(map->m_lblk, ee_block, ee_len)) {\n\t\t\tnewblock = map->m_lblk - ee_block + ee_start;\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t\t\text_debug(\"%u fit into %u:%d -> %llu\\n\", map->m_lblk,\n\t\t\t\t  ee_block, ee_len, newblock);\n\n\t\t\t/*\n\t\t\t * Do not put uninitialized extent\n\t\t\t * in the cache\n\t\t\t */\n\t\t\tif (!ext4_ext_is_uninitialized(ex)) {\n\t\t\t\text4_ext_put_in_cache(inode, ee_block,\n\t\t\t\t\tee_len, ee_start);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = ext4_ext_handle_uninitialized_extents(\n\t\t\t\thandle, inode, map, path, flags,\n\t\t\t\tallocated, newblock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif ((sbi->s_cluster_ratio > 1) &&\n\t    ext4_find_delalloc_cluster(inode, map->m_lblk, 0))\n\t\tmap->m_flags |= EXT4_MAP_FROM_CLUSTER;\n\n\t/*\n\t * requested block isn't allocated yet;\n\t * we couldn't try to create block if create flag is zero\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * put just found gap into cache to speed up\n\t\t * subsequent requests\n\t\t */\n\t\text4_ext_put_gap_in_cache(inode, path, map->m_lblk);\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * Okay, we need to do block allocation.\n\t */\n\tmap->m_flags &= ~EXT4_MAP_FROM_CLUSTER;\n\tnewex.ee_block = cpu_to_le32(map->m_lblk);\n\tcluster_offset = map->m_lblk & (sbi->s_cluster_ratio-1);\n\n\t/*\n\t * If we are doing bigalloc, check to see if the extent returned\n\t * by ext4_ext_find_extent() implies a cluster we can use.\n\t */\n\tif (cluster_offset && ex &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap->m_flags |= EXT4_MAP_FROM_CLUSTER;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/* find neighbour allocated blocks */\n\tar.lleft = map->m_lblk;\n\terr = ext4_ext_search_left(inode, path, &ar.lleft, &ar.pleft);\n\tif (err)\n\t\tgoto out2;\n\tar.lright = map->m_lblk;\n\tex2 = NULL;\n\terr = ext4_ext_search_right(inode, path, &ar.lright, &ar.pright, &ex2);\n\tif (err)\n\t\tgoto out2;\n\n\t/* Check if the extent after searching to the right implies a\n\t * cluster we can use. */\n\tif ((sbi->s_cluster_ratio > 1) && ex2 &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex2, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap->m_flags |= EXT4_MAP_FROM_CLUSTER;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/*\n\t * See if request is beyond maximum number of blocks we can have in\n\t * a single extent. For an initialized extent this limit is\n\t * EXT_INIT_MAX_LEN and for an uninitialized extent this limit is\n\t * EXT_UNINIT_MAX_LEN.\n\t */\n\tif (map->m_len > EXT_INIT_MAX_LEN &&\n\t    !(flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmap->m_len = EXT_INIT_MAX_LEN;\n\telse if (map->m_len > EXT_UNINIT_MAX_LEN &&\n\t\t (flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmap->m_len = EXT_UNINIT_MAX_LEN;\n\n\t/* Check if we can really insert (m_lblk)::(m_lblk + m_len) extent */\n\tnewex.ee_len = cpu_to_le16(map->m_len);\n\terr = ext4_ext_check_overlap(sbi, inode, &newex, path);\n\tif (err)\n\t\tallocated = ext4_ext_get_actual_len(&newex);\n\telse\n\t\tallocated = map->m_len;\n\n\t/* allocate new block */\n\tar.inode = inode;\n\tar.goal = ext4_ext_find_goal(inode, path, map->m_lblk);\n\tar.logical = map->m_lblk;\n\t/*\n\t * We calculate the offset from the beginning of the cluster\n\t * for the logical block number, since when we allocate a\n\t * physical cluster, the physical block should start at the\n\t * same offset from the beginning of the cluster.  This is\n\t * needed so that future calls to get_implied_cluster_alloc()\n\t * work correctly.\n\t */\n\toffset = map->m_lblk & (sbi->s_cluster_ratio - 1);\n\tar.len = EXT4_NUM_B2C(sbi, offset+allocated);\n\tar.goal -= offset;\n\tar.logical -= offset;\n\tif (S_ISREG(inode->i_mode))\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\telse\n\t\t/* disable in-core preallocation for non-regular files */\n\t\tar.flags = 0;\n\tif (flags & EXT4_GET_BLOCKS_NO_NORMALIZE)\n\t\tar.flags |= EXT4_MB_HINT_NOPREALLOC;\n\tnewblock = ext4_mb_new_blocks(handle, &ar, &err);\n\tif (!newblock)\n\t\tgoto out2;\n\text_debug(\"allocate new block: goal %llu, found %llu/%u\\n\",\n\t\t  ar.goal, newblock, allocated);\n\tfree_on_err = 1;\n\tallocated_clusters = ar.len;\n\tar.len = EXT4_C2B(sbi, ar.len) - offset;\n\tif (ar.len > allocated)\n\t\tar.len = allocated;\n\ngot_allocated_blocks:\n\t/* try to insert new extent into found leaf and return */\n\text4_ext_store_pblock(&newex, newblock + offset);\n\tnewex.ee_len = cpu_to_le16(ar.len);\n\t/* Mark uninitialized */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT){\n\t\text4_ext_mark_uninitialized(&newex);\n\t\t/*\n\t\t * io_end structure was created for every IO write to an\n\t\t * uninitialized extent. To avoid unnecessary conversion,\n\t\t * here we flag the IO that really needs the conversion.\n\t\t * For non asycn direct IO case, flag the inode state\n\t\t * that we need to perform conversion when IO is done.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\tset_unwritten = 1;\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t}\n\n\terr = 0;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0)\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t path, ar.len);\n\tif (!err)\n\t\terr = ext4_ext_insert_extent(handle, inode, path,\n\t\t\t\t\t     &newex, flags);\n\n\tif (!err && set_unwritten) {\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode,\n\t\t\t\t\t     EXT4_STATE_DIO_UNWRITTEN);\n\t}\n\n\tif (err && free_on_err) {\n\t\tint fb_flags = flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE ?\n\t\t\tEXT4_FREE_BLOCKS_NO_QUOT_UPDATE : 0;\n\t\t/* free data blocks we just allocated */\n\t\t/* not a good idea to call discard here directly,\n\t\t * but otherwise we'd need to call it every free() */\n\t\text4_discard_preallocations(inode);\n\t\text4_free_blocks(handle, inode, NULL, ext4_ext_pblock(&newex),\n\t\t\t\t ext4_ext_get_actual_len(&newex), fb_flags);\n\t\tgoto out2;\n\t}\n\n\t/* previous routine could use block we allocated */\n\tnewblock = ext4_ext_pblock(&newex);\n\tallocated = ext4_ext_get_actual_len(&newex);\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\n\t/*\n\t * Update reserved blocks/metadata blocks after successful\n\t * block allocation which had been deferred till now.\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\t/*\n\t\t * Check how many clusters we had reserved this allocated range\n\t\t */\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\t\t\tmap->m_lblk, allocated);\n\t\tif (map->m_flags & EXT4_MAP_FROM_CLUSTER) {\n\t\t\tif (reserved_clusters) {\n\t\t\t\t/*\n\t\t\t\t * We have clusters reserved for this range.\n\t\t\t\t * But since we are not doing actual allocation\n\t\t\t\t * and are simply using blocks from previously\n\t\t\t\t * allocated cluster, we should release the\n\t\t\t\t * reservation and not claim quota.\n\t\t\t\t */\n\t\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\treserved_clusters, 0);\n\t\t\t}\n\t\t} else {\n\t\t\tBUG_ON(allocated_clusters < reserved_clusters);\n\t\t\t/* We will claim quota for all newly allocated blocks.*/\n\t\t\text4_da_update_reserve_space(inode, allocated_clusters,\n\t\t\t\t\t\t\t1);\n\t\t\tif (reserved_clusters < allocated_clusters) {\n\t\t\t\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\t\t\t\tint reservation = allocated_clusters -\n\t\t\t\t\t\t  reserved_clusters;\n\t\t\t\t/*\n\t\t\t\t * It seems we claimed few clusters outside of\n\t\t\t\t * the range of this allocation. We should give\n\t\t\t\t * it back to the reservation pool. This can\n\t\t\t\t * happen in the following case:\n\t\t\t\t *\n\t\t\t\t * * Suppose s_cluster_ratio is 4 (i.e., each\n\t\t\t\t *   cluster has 4 blocks. Thus, the clusters\n\t\t\t\t *   are [0-3],[4-7],[8-11]...\n\t\t\t\t * * First comes delayed allocation write for\n\t\t\t\t *   logical blocks 10 & 11. Since there were no\n\t\t\t\t *   previous delayed allocated blocks in the\n\t\t\t\t *   range [8-11], we would reserve 1 cluster\n\t\t\t\t *   for this write.\n\t\t\t\t * * Next comes write for logical blocks 3 to 8.\n\t\t\t\t *   In this case, we will reserve 2 clusters\n\t\t\t\t *   (for [0-3] and [4-7]; and not for [8-11] as\n\t\t\t\t *   that range has a delayed allocated blocks.\n\t\t\t\t *   Thus total reserved clusters now becomes 3.\n\t\t\t\t * * Now, during the delayed allocation writeout\n\t\t\t\t *   time, we will first write blocks [3-8] and\n\t\t\t\t *   allocate 3 clusters for writing these\n\t\t\t\t *   blocks. Also, we would claim all these\n\t\t\t\t *   three clusters above.\n\t\t\t\t * * Now when we come here to writeout the\n\t\t\t\t *   blocks [10-11], we would expect to claim\n\t\t\t\t *   the reservation of 1 cluster we had made\n\t\t\t\t *   (and we would claim it since there are no\n\t\t\t\t *   more delayed allocated blocks in the range\n\t\t\t\t *   [8-11]. But our reserved cluster count had\n\t\t\t\t *   already gone to 0.\n\t\t\t\t *\n\t\t\t\t *   Thus, at the step 4 above when we determine\n\t\t\t\t *   that there are still some unwritten delayed\n\t\t\t\t *   allocated blocks outside of our current\n\t\t\t\t *   block range, we should increment the\n\t\t\t\t *   reserved clusters count so that when the\n\t\t\t\t *   remaining blocks finally gets written, we\n\t\t\t\t *   could claim them.\n\t\t\t\t */\n\t\t\t\tdquot_reserve_block(inode,\n\t\t\t\t\t\tEXT4_C2B(sbi, reservation));\n\t\t\t\tspin_lock(&ei->i_block_reservation_lock);\n\t\t\t\tei->i_reserved_data_blocks += reservation;\n\t\t\t\tspin_unlock(&ei->i_block_reservation_lock);\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Cache the extent and update transaction to commit on fdatasync only\n\t * when it is _not_ an uninitialized extent.\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_UNINIT_EXT) == 0) {\n\t\text4_ext_put_in_cache(inode, map->m_lblk, allocated, newblock);\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t} else\n\t\text4_update_inode_fsync_trans(handle, inode, 0);\nout:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\n\ttrace_ext4_ext_map_blocks_exit(inode, map->m_lblk,\n\t\tnewblock, map->m_len, err ? err : allocated);\n\n\treturn err ? err : allocated;\n}\n\nvoid ext4_ext_truncate(struct inode *inode)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t last_block;\n\thandle_t *handle;\n\tloff_t page_len;\n\tint err = 0;\n\n\t/*\n\t * finish any pending end_io work so we won't run the risk of\n\t * converting any truncated blocks to initialized later\n\t */\n\text4_flush_unwritten_io(inode);\n\n\t/*\n\t * probably first extent we're gonna free will be last in block\n\t */\n\terr = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, err);\n\tif (IS_ERR(handle))\n\t\treturn;\n\n\tif (inode->i_size % PAGE_CACHE_SIZE != 0) {\n\t\tpage_len = PAGE_CACHE_SIZE -\n\t\t\t(inode->i_size & (PAGE_CACHE_SIZE - 1));\n\n\t\terr = ext4_discard_partial_page_buffers(handle,\n\t\t\tmapping, inode->i_size, page_len, 0);\n\n\t\tif (err)\n\t\t\tgoto out_stop;\n\t}\n\n\tif (ext4_orphan_add(handle, inode))\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_ext_invalidate_cache(inode);\n\n\text4_discard_preallocations(inode);\n\n\t/*\n\t * TODO: optimization is possible here.\n\t * Probably we need not scan at all,\n\t * because page truncation is enough.\n\t */\n\n\t/* we have to know where to truncate from in crash case */\n\tEXT4_I(inode)->i_disksize = inode->i_size;\n\text4_mark_inode_dirty(handle, inode);\n\n\tlast_block = (inode->i_size + sb->s_blocksize - 1)\n\t\t\t>> EXT4_BLOCK_SIZE_BITS(sb);\n\terr = ext4_ext_remove_space(inode, last_block, EXT_MAX_BLOCKS - 1);\n\n\t/* In a multi-transaction truncate, we only make the final\n\t * transaction synchronous.\n\t */\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\nout_stop:\n\t/*\n\t * If this was a simple ftruncate() and the file will remain alive,\n\t * then we need to clear up the orphan record which we created above.\n\t * However, if this was a real unlink then we were called by\n\t * ext4_delete_inode(), and we allow that function to clean up the\n\t * orphan info for us.\n\t */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(handle, inode);\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\n}\n\nstatic void ext4_falloc_update_inode(struct inode *inode,\n\t\t\t\tint mode, loff_t new_size, int update_ctime)\n{\n\tstruct timespec now;\n\n\tif (update_ctime) {\n\t\tnow = current_fs_time(inode->i_sb);\n\t\tif (!timespec_equal(&inode->i_ctime, &now))\n\t\t\tinode->i_ctime = now;\n\t}\n\t/*\n\t * Update only when preallocation was requested beyond\n\t * the file size.\n\t */\n\tif (!(mode & FALLOC_FL_KEEP_SIZE)) {\n\t\tif (new_size > i_size_read(inode))\n\t\t\ti_size_write(inode, new_size);\n\t\tif (new_size > EXT4_I(inode)->i_disksize)\n\t\t\text4_update_i_disksize(inode, new_size);\n\t} else {\n\t\t/*\n\t\t * Mark that we allocate beyond EOF so the subsequent truncate\n\t\t * can proceed even if the new size is the same as i_size.\n\t\t */\n\t\tif (new_size > i_size_read(inode))\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\t}\n\n}\n\n/*\n * preallocate space for a file. This implements ext4's fallocate file\n * operation, which gets called from sys_fallocate system call.\n * For block-mapped files, posix_fallocate should fall back to the method\n * of writing zeroes to the required new blocks (the same behavior which is\n * expected for file systems which do not support fallocate() system call).\n */\nlong ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\thandle_t *handle;\n\tloff_t new_size;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint retries = 0;\n\tint flags;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\t/*\n\t * currently supporting (pre)allocate mode for extent-based\n\t * files _only_\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Return error if mode is not supported */\n\tif (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))\n\t\treturn -EOPNOTSUPP;\n\n\tif (mode & FALLOC_FL_PUNCH_HOLE)\n\t\treturn ext4_punch_hole(file, offset, len);\n\n\ttrace_ext4_fallocate_enter(inode, offset, len, mode);\n\tmap.m_lblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)\n\t\t- map.m_lblk;\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\tmutex_lock(&inode->i_mutex);\n\tret = inode_newsize_ok(inode, (len + offset));\n\tif (ret) {\n\t\tmutex_unlock(&inode->i_mutex);\n\t\ttrace_ext4_fallocate_exit(inode, offset, max_blocks, ret);\n\t\treturn ret;\n\t}\n\tflags = EXT4_GET_BLOCKS_CREATE_UNINIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\t/*\n\t * Don't normalize the request if it can fit in one extent so\n\t * that it doesn't get unnecessarily split into multiple\n\t * extents.\n\t */\n\tif (len <= EXT_UNINIT_MAX_LEN << blkbits)\n\t\tflags |= EXT4_GET_BLOCKS_NO_NORMALIZE;\n\n\t/* Prevent race condition between unwritten */\n\text4_flush_unwritten_io(inode);\nretry:\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk = map.m_lblk + ret;\n\t\tmap.m_len = max_blocks = max_blocks - ret;\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map, flags);\n\t\tif (ret <= 0) {\n#ifdef EXT4FS_DEBUG\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tprintk(KERN_ERR \"%s: ext4_ext_map_blocks \"\n\t\t\t\t    \"returned error inode#%lu, block=%u, \"\n\t\t\t\t    \"max_blocks=%u\", __func__,\n\t\t\t\t    inode->i_ino, map.m_lblk, max_blocks);\n#endif\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\t\tbreak;\n\t\t}\n\t\tif ((map.m_lblk + ret) >= (EXT4_BLOCK_ALIGN(offset + len,\n\t\t\t\t\t\tblkbits) >> blkbits))\n\t\t\tnew_size = offset + len;\n\t\telse\n\t\t\tnew_size = ((loff_t) map.m_lblk + ret) << blkbits;\n\n\t\text4_falloc_update_inode(inode, mode, new_size,\n\t\t\t\t\t (map.m_flags & EXT4_MAP_NEW));\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tif ((file->f_flags & O_SYNC) && ret >= max_blocks)\n\t\t\text4_handle_sync(handle);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret2)\n\t\t\tbreak;\n\t}\n\tif (ret == -ENOSPC &&\n\t\t\text4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tmutex_unlock(&inode->i_mutex);\n\ttrace_ext4_fallocate_exit(inode, offset, max_blocks,\n\t\t\t\tret > 0 ? ret2 : ret);\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * This function convert a range of blocks to written extents\n * The caller of this function will pass the start offset and the size.\n * all unwritten extents within this range will be converted to\n * written extents.\n *\n * This function is called from the direct IO end io call back\n * function, to convert the fallocated extents after IO is completed.\n * Returns 0 on success.\n */\nint ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,\n\t\t\t\t    ssize_t len)\n{\n\thandle_t *handle;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\tmap.m_lblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = ((EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits) -\n\t\t      map.m_lblk);\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = (max_blocks -= ret);\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map,\n\t\t\t\t      EXT4_GET_BLOCKS_IO_CONVERT_EXT);\n\t\tif (ret <= 0) {\n\t\t\tWARN_ON(ret <= 0);\n\t\t\text4_msg(inode->i_sb, KERN_ERR,\n\t\t\t\t \"%s:%d: inode #%lu: block %u: len %u: \"\n\t\t\t\t \"ext4_ext_map_blocks returned %d\",\n\t\t\t\t __func__, __LINE__, inode->i_ino, map.m_lblk,\n\t\t\t\t map.m_len, ret);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret <= 0 || ret2 )\n\t\t\tbreak;\n\t}\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * Callback function called for each extent to gather FIEMAP information.\n */\nstatic int ext4_ext_fiemap_cb(struct inode *inode, ext4_lblk_t next,\n\t\t       struct ext4_ext_cache *newex, struct ext4_extent *ex,\n\t\t       void *data)\n{\n\t__u64\tlogical;\n\t__u64\tphysical;\n\t__u64\tlength;\n\t__u32\tflags = 0;\n\tint\t\tret = 0;\n\tstruct fiemap_extent_info *fieinfo = data;\n\tunsigned char blksize_bits;\n\n\tblksize_bits = inode->i_sb->s_blocksize_bits;\n\tlogical = (__u64)newex->ec_block << blksize_bits;\n\n\tif (newex->ec_start == 0) {\n\t\t/*\n\t\t * No extent in extent-tree contains block @newex->ec_start,\n\t\t * then the block may stay in 1)a hole or 2)delayed-extent.\n\t\t *\n\t\t * Holes or delayed-extents are processed as follows.\n\t\t * 1. lookup dirty pages with specified range in pagecache.\n\t\t *    If no page is got, then there is no delayed-extent and\n\t\t *    return with EXT_CONTINUE.\n\t\t * 2. find the 1st mapped buffer,\n\t\t * 3. check if the mapped buffer is both in the request range\n\t\t *    and a delayed buffer. If not, there is no delayed-extent,\n\t\t *    then return.\n\t\t * 4. a delayed-extent is found, the extent will be collected.\n\t\t */\n\t\text4_lblk_t\tend = 0;\n\t\tpgoff_t\t\tlast_offset;\n\t\tpgoff_t\t\toffset;\n\t\tpgoff_t\t\tindex;\n\t\tpgoff_t\t\tstart_index = 0;\n\t\tstruct page\t**pages = NULL;\n\t\tstruct buffer_head *bh = NULL;\n\t\tstruct buffer_head *head = NULL;\n\t\tunsigned int nr_pages = PAGE_SIZE / sizeof(struct page *);\n\n\t\tpages = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\t\tif (pages == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\toffset = logical >> PAGE_SHIFT;\nrepeat:\n\t\tlast_offset = offset;\n\t\thead = NULL;\n\t\tret = find_get_pages_tag(inode->i_mapping, &offset,\n\t\t\t\t\tPAGECACHE_TAG_DIRTY, nr_pages, pages);\n\n\t\tif (!(flags & FIEMAP_EXTENT_DELALLOC)) {\n\t\t\t/* First time, try to find a mapped buffer. */\n\t\t\tif (ret == 0) {\nout:\n\t\t\t\tfor (index = 0; index < ret; index++)\n\t\t\t\t\tpage_cache_release(pages[index]);\n\t\t\t\t/* just a hole. */\n\t\t\t\tkfree(pages);\n\t\t\t\treturn EXT_CONTINUE;\n\t\t\t}\n\t\t\tindex = 0;\n\nnext_page:\n\t\t\t/* Try to find the 1st mapped buffer. */\n\t\t\tend = ((__u64)pages[index]->index << PAGE_SHIFT) >>\n\t\t\t\t  blksize_bits;\n\t\t\tif (!page_has_buffers(pages[index]))\n\t\t\t\tgoto out;\n\t\t\thead = page_buffers(pages[index]);\n\t\t\tif (!head)\n\t\t\t\tgoto out;\n\n\t\t\tindex++;\n\t\t\tbh = head;\n\t\t\tdo {\n\t\t\t\tif (end >= newex->ec_block +\n\t\t\t\t\tnewex->ec_len)\n\t\t\t\t\t/* The buffer is out of\n\t\t\t\t\t * the request range.\n\t\t\t\t\t */\n\t\t\t\t\tgoto out;\n\n\t\t\t\tif (buffer_mapped(bh) &&\n\t\t\t\t    end >= newex->ec_block) {\n\t\t\t\t\tstart_index = index - 1;\n\t\t\t\t\t/* get the 1st mapped buffer. */\n\t\t\t\t\tgoto found_mapped_buffer;\n\t\t\t\t}\n\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t\tend++;\n\t\t\t} while (bh != head);\n\n\t\t\t/* No mapped buffer in the range found in this page,\n\t\t\t * We need to look up next page.\n\t\t\t */\n\t\t\tif (index >= ret) {\n\t\t\t\t/* There is no page left, but we need to limit\n\t\t\t\t * newex->ec_len.\n\t\t\t\t */\n\t\t\t\tnewex->ec_len = end - newex->ec_block;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tgoto next_page;\n\t\t} else {\n\t\t\t/*Find contiguous delayed buffers. */\n\t\t\tif (ret > 0 && pages[0]->index == last_offset)\n\t\t\t\thead = page_buffers(pages[0]);\n\t\t\tbh = head;\n\t\t\tindex = 1;\n\t\t\tstart_index = 0;\n\t\t}\n\nfound_mapped_buffer:\n\t\tif (bh != NULL && buffer_delay(bh)) {\n\t\t\t/* 1st or contiguous delayed buffer found. */\n\t\t\tif (!(flags & FIEMAP_EXTENT_DELALLOC)) {\n\t\t\t\t/*\n\t\t\t\t * 1st delayed buffer found, record\n\t\t\t\t * the start of extent.\n\t\t\t\t */\n\t\t\t\tflags |= FIEMAP_EXTENT_DELALLOC;\n\t\t\t\tnewex->ec_block = end;\n\t\t\t\tlogical = (__u64)end << blksize_bits;\n\t\t\t}\n\t\t\t/* Find contiguous delayed buffers. */\n\t\t\tdo {\n\t\t\t\tif (!buffer_delay(bh))\n\t\t\t\t\tgoto found_delayed_extent;\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t\tend++;\n\t\t\t} while (bh != head);\n\n\t\t\tfor (; index < ret; index++) {\n\t\t\t\tif (!page_has_buffers(pages[index])) {\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\thead = page_buffers(pages[index]);\n\t\t\t\tif (!head) {\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (pages[index]->index !=\n\t\t\t\t    pages[start_index]->index + index\n\t\t\t\t    - start_index) {\n\t\t\t\t\t/* Blocks are not contiguous. */\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbh = head;\n\t\t\t\tdo {\n\t\t\t\t\tif (!buffer_delay(bh))\n\t\t\t\t\t\t/* Delayed-extent ends. */\n\t\t\t\t\t\tgoto found_delayed_extent;\n\t\t\t\t\tbh = bh->b_this_page;\n\t\t\t\t\tend++;\n\t\t\t\t} while (bh != head);\n\t\t\t}\n\t\t} else if (!(flags & FIEMAP_EXTENT_DELALLOC))\n\t\t\t/* a hole found. */\n\t\t\tgoto out;\n\nfound_delayed_extent:\n\t\tnewex->ec_len = min(end - newex->ec_block,\n\t\t\t\t\t\t(ext4_lblk_t)EXT_INIT_MAX_LEN);\n\t\tif (ret == nr_pages && bh != NULL &&\n\t\t\tnewex->ec_len < EXT_INIT_MAX_LEN &&\n\t\t\tbuffer_delay(bh)) {\n\t\t\t/* Have not collected an extent and continue. */\n\t\t\tfor (index = 0; index < ret; index++)\n\t\t\t\tpage_cache_release(pages[index]);\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tfor (index = 0; index < ret; index++)\n\t\t\tpage_cache_release(pages[index]);\n\t\tkfree(pages);\n\t}\n\n\tphysical = (__u64)newex->ec_start << blksize_bits;\n\tlength =   (__u64)newex->ec_len << blksize_bits;\n\n\tif (ex && ext4_ext_is_uninitialized(ex))\n\t\tflags |= FIEMAP_EXTENT_UNWRITTEN;\n\n\tif (next == EXT_MAX_BLOCKS)\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\n\tret = fiemap_fill_next_extent(fieinfo, logical, physical,\n\t\t\t\t\tlength, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret == 1)\n\t\treturn EXT_BREAK;\n\treturn EXT_CONTINUE;\n}\n/* fiemap flags we can handle specified here */\n#define EXT4_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)\n\nstatic int ext4_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\t__u64 physical = 0;\n\t__u64 length;\n\t__u32 flags = FIEMAP_EXTENT_LAST;\n\tint blockbits = inode->i_sb->s_blocksize_bits;\n\tint error = 0;\n\n\t/* in-inode? */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tstruct ext4_iloc iloc;\n\t\tint offset;\t/* offset of xattr in inode */\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error)\n\t\t\treturn error;\n\t\tphysical = iloc.bh->b_blocknr << blockbits;\n\t\toffset = EXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\tEXT4_I(inode)->i_extra_isize;\n\t\tphysical += offset;\n\t\tlength = EXT4_SB(inode->i_sb)->s_inode_size - offset;\n\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t\tbrelse(iloc.bh);\n\t} else { /* external block */\n\t\tphysical = EXT4_I(inode)->i_file_acl << blockbits;\n\t\tlength = inode->i_sb->s_blocksize;\n\t}\n\n\tif (physical)\n\t\terror = fiemap_fill_next_extent(fieinfo, 0, physical,\n\t\t\t\t\t\tlength, flags);\n\treturn (error < 0 ? error : 0);\n}\n\n/*\n * ext4_ext_punch_hole\n *\n * Punches a hole of \"length\" bytes in a file starting\n * at byte \"offset\"\n *\n * @inode:  The inode of the file to punch a hole in\n * @offset: The starting byte offset of the hole\n * @length: The length of the hole\n *\n * Returns the number of blocks removed or negative on err\n */\nint ext4_ext_punch_hole(struct file *file, loff_t offset, loff_t length)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t first_block, stop_block;\n\tstruct address_space *mapping = inode->i_mapping;\n\thandle_t *handle;\n\tloff_t first_page, last_page, page_len;\n\tloff_t first_page_offset, last_page_offset;\n\tint credits, err = 0;\n\n\t/*\n\t * Write out all dirty pages to avoid race conditions\n\t * Then release them.\n\t */\n\tif (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {\n\t\terr = filemap_write_and_wait_range(mapping,\n\t\t\toffset, offset + length - 1);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tmutex_lock(&inode->i_mutex);\n\t/* It's not possible punch hole on append only file */\n\tif (IS_APPEND(inode) || IS_IMMUTABLE(inode)) {\n\t\terr = -EPERM;\n\t\tgoto out_mutex;\n\t}\n\tif (IS_SWAPFILE(inode)) {\n\t\terr = -ETXTBSY;\n\t\tgoto out_mutex;\n\t}\n\n\t/* No need to punch hole beyond i_size */\n\tif (offset >= inode->i_size)\n\t\tgoto out_mutex;\n\n\t/*\n\t * If the hole extends beyond i_size, set the hole\n\t * to end after the page that contains i_size\n\t */\n\tif (offset + length > inode->i_size) {\n\t\tlength = inode->i_size +\n\t\t   PAGE_CACHE_SIZE - (inode->i_size & (PAGE_CACHE_SIZE - 1)) -\n\t\t   offset;\n\t}\n\n\tfirst_page = (offset + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\tlast_page = (offset + length) >> PAGE_CACHE_SHIFT;\n\n\tfirst_page_offset = first_page << PAGE_CACHE_SHIFT;\n\tlast_page_offset = last_page << PAGE_CACHE_SHIFT;\n\n\t/* Now release the pages */\n\tif (last_page_offset > first_page_offset) {\n\t\ttruncate_pagecache_range(inode, first_page_offset,\n\t\t\t\t\t last_page_offset - 1);\n\t}\n\n\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\text4_inode_block_unlocked_dio(inode);\n\terr = ext4_flush_unwritten_io(inode);\n\tif (err)\n\t\tgoto out_dio;\n\tinode_dio_wait(inode);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, credits);\n\tif (IS_ERR(handle)) {\n\t\terr = PTR_ERR(handle);\n\t\tgoto out_dio;\n\t}\n\n\n\t/*\n\t * Now we need to zero out the non-page-aligned data in the\n\t * pages at the start and tail of the hole, and unmap the buffer\n\t * heads for the block aligned regions of the page that were\n\t * completely zeroed.\n\t */\n\tif (first_page > last_page) {\n\t\t/*\n\t\t * If the file space being truncated is contained within a page\n\t\t * just zero out and unmap the middle of that page\n\t\t */\n\t\terr = ext4_discard_partial_page_buffers(handle,\n\t\t\tmapping, offset, length, 0);\n\n\t\tif (err)\n\t\t\tgoto out;\n\t} else {\n\t\t/*\n\t\t * zero out and unmap the partial page that contains\n\t\t * the start of the hole\n\t\t */\n\t\tpage_len  = first_page_offset - offset;\n\t\tif (page_len > 0) {\n\t\t\terr = ext4_discard_partial_page_buffers(handle, mapping,\n\t\t\t\t\t\t   offset, page_len, 0);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * zero out and unmap the partial page that contains\n\t\t * the end of the hole\n\t\t */\n\t\tpage_len = offset + length - last_page_offset;\n\t\tif (page_len > 0) {\n\t\t\terr = ext4_discard_partial_page_buffers(handle, mapping,\n\t\t\t\t\tlast_page_offset, page_len, 0);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * If i_size is contained in the last page, we need to\n\t * unmap and zero the partial page after i_size\n\t */\n\tif (inode->i_size >> PAGE_CACHE_SHIFT == last_page &&\n\t   inode->i_size % PAGE_CACHE_SIZE != 0) {\n\n\t\tpage_len = PAGE_CACHE_SIZE -\n\t\t\t(inode->i_size & (PAGE_CACHE_SIZE - 1));\n\n\t\tif (page_len > 0) {\n\t\t\terr = ext4_discard_partial_page_buffers(handle,\n\t\t\t  mapping, inode->i_size, page_len, 0);\n\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfirst_block = (offset + sb->s_blocksize - 1) >>\n\t\tEXT4_BLOCK_SIZE_BITS(sb);\n\tstop_block = (offset + length) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* If there are no blocks to remove, return now */\n\tif (first_block >= stop_block)\n\t\tgoto out;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_ext_invalidate_cache(inode);\n\text4_discard_preallocations(inode);\n\n\terr = ext4_ext_remove_space(inode, first_block, stop_block - 1);\n\n\text4_ext_invalidate_cache(inode);\n\text4_discard_preallocations(inode);\n\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\nout:\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn err;\n}\nint ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\text4_lblk_t start_blk;\n\tint error = 0;\n\n\t/* fallback to generic here if not in extents fmt */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn generic_block_fiemap(inode, fieinfo, start, len,\n\t\t\text4_get_block);\n\n\tif (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS))\n\t\treturn -EBADR;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\terror = ext4_xattr_fiemap(inode, fieinfo);\n\t} else {\n\t\text4_lblk_t len_blks;\n\t\t__u64 last_blk;\n\n\t\tstart_blk = start >> inode->i_sb->s_blocksize_bits;\n\t\tlast_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;\n\t\tif (last_blk >= EXT_MAX_BLOCKS)\n\t\t\tlast_blk = EXT_MAX_BLOCKS-1;\n\t\tlen_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;\n\n\t\t/*\n\t\t * Walk the extent tree gathering extent information.\n\t\t * ext4_ext_fiemap_cb will push extents back to user.\n\t\t */\n\t\terror = ext4_ext_walk_space(inode, start_blk, len_blks,\n\t\t\t\t\t  ext4_ext_fiemap_cb, fieinfo);\n\t}\n\n\treturn error;\n}\n"], "fixing_code": ["/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n *\n * Architecture independence:\n *   Copyright (c) 2005, Bull S.A.\n *   Written by Pierre Peiffer <pierre.peiffer@bull.net>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n */\n\n/*\n * Extents support for EXT4\n *\n * TODO:\n *   - ext4*_error() should be used in some situations\n *   - analyze all BUG()/BUG_ON(), use -EIO where appropriate\n *   - smart tree reduction\n */\n\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/falloc.h>\n#include <asm/uaccess.h>\n#include <linux/fiemap.h>\n#include \"ext4_jbd2.h\"\n\n#include <trace/events/ext4.h>\n\n/*\n * used by extent splitting.\n */\n#define EXT4_EXT_MAY_ZEROOUT\t0x1  /* safe to zeroout if split fails \\\n\t\t\t\t\tdue to ENOSPC */\n#define EXT4_EXT_MARK_UNINIT1\t0x2  /* mark first half uninitialized */\n#define EXT4_EXT_MARK_UNINIT2\t0x4  /* mark second half uninitialized */\n\n#define EXT4_EXT_DATA_VALID1\t0x8  /* first half contains valid data */\n#define EXT4_EXT_DATA_VALID2\t0x10 /* second half contains valid data */\n\nstatic __le32 ext4_extent_block_csum(struct inode *inode,\n\t\t\t\t     struct ext4_extent_header *eh)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t__u32 csum;\n\n\tcsum = ext4_chksum(sbi, ei->i_csum_seed, (__u8 *)eh,\n\t\t\t   EXT4_EXTENT_TAIL_OFFSET(eh));\n\treturn cpu_to_le32(csum);\n}\n\nstatic int ext4_extent_block_csum_verify(struct inode *inode,\n\t\t\t\t\t struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb,\n\t\tEXT4_FEATURE_RO_COMPAT_METADATA_CSUM))\n\t\treturn 1;\n\n\tet = find_ext4_extent_tail(eh);\n\tif (et->et_checksum != ext4_extent_block_csum(inode, eh))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic void ext4_extent_block_csum_set(struct inode *inode,\n\t\t\t\t       struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb,\n\t\tEXT4_FEATURE_RO_COMPAT_METADATA_CSUM))\n\t\treturn;\n\n\tet = find_ext4_extent_tail(eh);\n\tet->et_checksum = ext4_extent_block_csum(inode, eh);\n}\n\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t\tstruct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\tint split_flag,\n\t\t\t\tint flags);\n\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags);\n\nstatic int ext4_ext_truncate_extend_restart(handle_t *handle,\n\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t    int needed)\n{\n\tint err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (handle->h_buffer_credits > needed)\n\t\treturn 0;\n\terr = ext4_journal_extend(handle, needed);\n\tif (err <= 0)\n\t\treturn err;\n\terr = ext4_truncate_restart_trans(handle, inode, needed);\n\tif (err == 0)\n\t\terr = -EAGAIN;\n\n\treturn err;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n */\nstatic int ext4_ext_get_access(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\treturn ext4_journal_get_write_access(handle, path->p_bh);\n\t}\n\t/* path points to leaf/index in inode body */\n\t/* we use in-core data, no need to protect them */\n\treturn 0;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n *  - EIO\n */\n#define ext4_ext_dirty(handle, inode, path) \\\n\t\t__ext4_ext_dirty(__func__, __LINE__, (handle), (inode), (path))\nstatic int __ext4_ext_dirty(const char *where, unsigned int line,\n\t\t\t    handle_t *handle, struct inode *inode,\n\t\t\t    struct ext4_ext_path *path)\n{\n\tint err;\n\tif (path->p_bh) {\n\t\text4_extent_block_csum_set(inode, ext_block_hdr(path->p_bh));\n\t\t/* path points to block */\n\t\terr = __ext4_handle_dirty_metadata(where, line, handle,\n\t\t\t\t\t\t   inode, path->p_bh);\n\t} else {\n\t\t/* path points to leaf/index in inode body */\n\t\terr = ext4_mark_inode_dirty(handle, inode);\n\t}\n\treturn err;\n}\n\nstatic ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      ext4_lblk_t block)\n{\n\tif (path) {\n\t\tint depth = path->p_depth;\n\t\tstruct ext4_extent *ex;\n\n\t\t/*\n\t\t * Try to predict block placement assuming that we are\n\t\t * filling in a file which will eventually be\n\t\t * non-sparse --- i.e., in the case of libbfd writing\n\t\t * an ELF object sections out-of-order but in a way\n\t\t * the eventually results in a contiguous object or\n\t\t * executable file, or some database extending a table\n\t\t * space file.  However, this is actually somewhat\n\t\t * non-ideal if we are writing a sparse file such as\n\t\t * qemu or KVM writing a raw image file that is going\n\t\t * to stay fairly sparse, since it will end up\n\t\t * fragmenting the file system's free space.  Maybe we\n\t\t * should have some hueristics or some way to allow\n\t\t * userspace to pass a hint to file system,\n\t\t * especially if the latter case turns out to be\n\t\t * common.\n\t\t */\n\t\tex = path[depth].p_ext;\n\t\tif (ex) {\n\t\t\text4_fsblk_t ext_pblk = ext4_ext_pblock(ex);\n\t\t\text4_lblk_t ext_block = le32_to_cpu(ex->ee_block);\n\n\t\t\tif (block > ext_block)\n\t\t\t\treturn ext_pblk + (block - ext_block);\n\t\t\telse\n\t\t\t\treturn ext_pblk - (ext_block - block);\n\t\t}\n\n\t\t/* it looks like index is empty;\n\t\t * try to find starting block from index itself */\n\t\tif (path[depth].p_bh)\n\t\t\treturn path[depth].p_bh->b_blocknr;\n\t}\n\n\t/* OK. use inode's group */\n\treturn ext4_inode_to_goal_block(inode);\n}\n\n/*\n * Allocation for a meta data block\n */\nstatic ext4_fsblk_t\next4_ext_new_meta_block(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path,\n\t\t\tstruct ext4_extent *ex, int *err, unsigned int flags)\n{\n\text4_fsblk_t goal, newblock;\n\n\tgoal = ext4_ext_find_goal(inode, path, le32_to_cpu(ex->ee_block));\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, flags,\n\t\t\t\t\tNULL, err);\n\treturn newblock;\n}\n\nstatic inline int ext4_ext_space_block(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 6)\n\t\tsize = 6;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_block_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 5)\n\t\tsize = 5;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 3)\n\t\tsize = 3;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 4)\n\t\tsize = 4;\n#endif\n\treturn size;\n}\n\n/*\n * Calculate the number of metadata blocks needed\n * to allocate @blocks\n * Worse case is one block per extent\n */\nint ext4_ext_calc_metadata_amount(struct inode *inode, ext4_lblk_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint idxs;\n\n\tidxs = ((inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t/ sizeof(struct ext4_extent_idx));\n\n\t/*\n\t * If the new delayed allocation block is contiguous with the\n\t * previous da block, it can share index blocks with the\n\t * previous block, so we only need to allocate a new index\n\t * block every idxs leaf blocks.  At ldxs**2 blocks, we need\n\t * an additional index block, and at ldxs**3 blocks, yet\n\t * another index blocks.\n\t */\n\tif (ei->i_da_metadata_calc_len &&\n\t    ei->i_da_metadata_calc_last_lblock+1 == lblock) {\n\t\tint num = 0;\n\n\t\tif ((ei->i_da_metadata_calc_len % idxs) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {\n\t\t\tnum++;\n\t\t\tei->i_da_metadata_calc_len = 0;\n\t\t} else\n\t\t\tei->i_da_metadata_calc_len++;\n\t\tei->i_da_metadata_calc_last_lblock++;\n\t\treturn num;\n\t}\n\n\t/*\n\t * In the worst case we need a new set of index blocks at\n\t * every level of the inode's extent tree.\n\t */\n\tei->i_da_metadata_calc_len = 1;\n\tei->i_da_metadata_calc_last_lblock = lblock;\n\treturn ext_depth(inode) + 1;\n}\n\nstatic int\next4_ext_max_entries(struct inode *inode, int depth)\n{\n\tint max;\n\n\tif (depth == ext_depth(inode)) {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_root(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_root_idx(inode, 1);\n\t} else {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_block(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_block_idx(inode, 1);\n\t}\n\n\treturn max;\n}\n\nstatic int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)\n{\n\text4_fsblk_t block = ext4_ext_pblock(ext);\n\tint len = ext4_ext_get_actual_len(ext);\n\n\tif (len == 0)\n\t\treturn 0;\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, len);\n}\n\nstatic int ext4_valid_extent_idx(struct inode *inode,\n\t\t\t\tstruct ext4_extent_idx *ext_idx)\n{\n\text4_fsblk_t block = ext4_idx_pblock(ext_idx);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, 1);\n}\n\nstatic int ext4_valid_extent_entries(struct inode *inode,\n\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\tint depth)\n{\n\tunsigned short entries;\n\tif (eh->eh_entries == 0)\n\t\treturn 1;\n\n\tentries = le16_to_cpu(eh->eh_entries);\n\n\tif (depth == 0) {\n\t\t/* leaf entries */\n\t\tstruct ext4_extent *ext = EXT_FIRST_EXTENT(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent(inode, ext))\n\t\t\t\treturn 0;\n\t\t\text++;\n\t\t\tentries--;\n\t\t}\n\t} else {\n\t\tstruct ext4_extent_idx *ext_idx = EXT_FIRST_INDEX(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent_idx(inode, ext_idx))\n\t\t\t\treturn 0;\n\t\t\text_idx++;\n\t\t\tentries--;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __ext4_ext_check(const char *function, unsigned int line,\n\t\t\t    struct inode *inode, struct ext4_extent_header *eh,\n\t\t\t    int depth)\n{\n\tconst char *error_msg;\n\tint max = 0;\n\n\tif (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) {\n\t\terror_msg = \"invalid magic\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_depth) != depth)) {\n\t\terror_msg = \"unexpected eh_depth\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(eh->eh_max == 0)) {\n\t\terror_msg = \"invalid eh_max\";\n\t\tgoto corrupted;\n\t}\n\tmax = ext4_ext_max_entries(inode, depth);\n\tif (unlikely(le16_to_cpu(eh->eh_max) > max)) {\n\t\terror_msg = \"too large eh_max\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) {\n\t\terror_msg = \"invalid eh_entries\";\n\t\tgoto corrupted;\n\t}\n\tif (!ext4_valid_extent_entries(inode, eh, depth)) {\n\t\terror_msg = \"invalid extent entries\";\n\t\tgoto corrupted;\n\t}\n\t/* Verify checksum on non-root extent tree nodes */\n\tif (ext_depth(inode) != depth &&\n\t    !ext4_extent_block_csum_verify(inode, eh)) {\n\t\terror_msg = \"extent tree corrupted\";\n\t\tgoto corrupted;\n\t}\n\treturn 0;\n\ncorrupted:\n\text4_error_inode(inode, function, line, 0,\n\t\t\t\"bad header/extent: %s - magic %x, \"\n\t\t\t\"entries %u, max %u(%u), depth %u(%u)\",\n\t\t\terror_msg, le16_to_cpu(eh->eh_magic),\n\t\t\tle16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max),\n\t\t\tmax, le16_to_cpu(eh->eh_depth), depth);\n\n\treturn -EIO;\n}\n\n#define ext4_ext_check(inode, eh, depth)\t\\\n\t__ext4_ext_check(__func__, __LINE__, inode, eh, depth)\n\nint ext4_ext_check_inode(struct inode *inode)\n{\n\treturn ext4_ext_check(inode, ext_inode_hdr(inode), ext_depth(inode));\n}\n\nstatic int __ext4_ext_check_block(const char *function, unsigned int line,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct ext4_extent_header *eh,\n\t\t\t\t  int depth,\n\t\t\t\t  struct buffer_head *bh)\n{\n\tint ret;\n\n\tif (buffer_verified(bh))\n\t\treturn 0;\n\tret = ext4_ext_check(inode, eh, depth);\n\tif (ret)\n\t\treturn ret;\n\tset_buffer_verified(bh);\n\treturn ret;\n}\n\n#define ext4_ext_check_block(inode, eh, depth, bh)\t\\\n\t__ext4_ext_check_block(__func__, __LINE__, inode, eh, depth, bh)\n\n#ifdef EXT_DEBUG\nstatic void ext4_ext_show_path(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint k, l = path->p_depth;\n\n\text_debug(\"path:\");\n\tfor (k = 0; k <= l; k++, path++) {\n\t\tif (path->p_idx) {\n\t\t  ext_debug(\"  %d->%llu\", le32_to_cpu(path->p_idx->ei_block),\n\t\t\t    ext4_idx_pblock(path->p_idx));\n\t\t} else if (path->p_ext) {\n\t\t\text_debug(\"  %d:[%d]%d:%llu \",\n\t\t\t\t  le32_to_cpu(path->p_ext->ee_block),\n\t\t\t\t  ext4_ext_is_uninitialized(path->p_ext),\n\t\t\t\t  ext4_ext_get_actual_len(path->p_ext),\n\t\t\t\t  ext4_ext_pblock(path->p_ext));\n\t\t} else\n\t\t\text_debug(\"  []\");\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_leaf(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex;\n\tint i;\n\n\tif (!path)\n\t\treturn;\n\n\teh = path[depth].p_hdr;\n\tex = EXT_FIRST_EXTENT(eh);\n\n\text_debug(\"Displaying leaf extents for inode %lu\\n\", inode->i_ino);\n\n\tfor (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ex++) {\n\t\text_debug(\"%d:[%d]%d:%llu \", le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex), ext4_ext_pblock(ex));\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_move(struct inode *inode, struct ext4_ext_path *path,\n\t\t\text4_fsblk_t newblock, int level)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\n\tif (depth != level) {\n\t\tstruct ext4_extent_idx *idx;\n\t\tidx = path[level].p_idx;\n\t\twhile (idx <= EXT_MAX_INDEX(path[level].p_hdr)) {\n\t\t\text_debug(\"%d: move %d:%llu in new index %llu\\n\", level,\n\t\t\t\t\tle32_to_cpu(idx->ei_block),\n\t\t\t\t\text4_idx_pblock(idx),\n\t\t\t\t\tnewblock);\n\t\t\tidx++;\n\t\t}\n\n\t\treturn;\n\t}\n\n\tex = path[depth].p_ext;\n\twhile (ex <= EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\text_debug(\"move %d:%llu:[%d]%d in new leaf %llu\\n\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_pblock(ex),\n\t\t\t\text4_ext_is_uninitialized(ex),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tnewblock);\n\t\tex++;\n\t}\n}\n\n#else\n#define ext4_ext_show_path(inode, path)\n#define ext4_ext_show_leaf(inode, path)\n#define ext4_ext_show_move(inode, path, newblock, level)\n#endif\n\nvoid ext4_ext_drop_refs(struct ext4_ext_path *path)\n{\n\tint depth = path->p_depth;\n\tint i;\n\n\tfor (i = 0; i <= depth; i++, path++)\n\t\tif (path->p_bh) {\n\t\t\tbrelse(path->p_bh);\n\t\t\tpath->p_bh = NULL;\n\t\t}\n}\n\n/*\n * ext4_ext_binsearch_idx:\n * binary search for the closest index of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch_idx(struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent_idx *r, *l, *m;\n\n\n\text_debug(\"binsearch for %u(idx):  \", block);\n\n\tl = EXT_FIRST_INDEX(eh) + 1;\n\tr = EXT_LAST_INDEX(eh);\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ei_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ei_block),\n\t\t\t\tm, le32_to_cpu(m->ei_block),\n\t\t\t\tr, le32_to_cpu(r->ei_block));\n\t}\n\n\tpath->p_idx = l - 1;\n\text_debug(\"  -> %u->%lld \", le32_to_cpu(path->p_idx->ei_block),\n\t\t  ext4_idx_pblock(path->p_idx));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent_idx *chix, *ix;\n\t\tint k;\n\n\t\tchix = ix = EXT_FIRST_INDEX(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) {\n\t\t  if (k != 0 &&\n\t\t      le32_to_cpu(ix->ei_block) <= le32_to_cpu(ix[-1].ei_block)) {\n\t\t\t\tprintk(KERN_DEBUG \"k=%d, ix=0x%p, \"\n\t\t\t\t       \"first=0x%p\\n\", k,\n\t\t\t\t       ix, EXT_FIRST_INDEX(eh));\n\t\t\t\tprintk(KERN_DEBUG \"%u <= %u\\n\",\n\t\t\t\t       le32_to_cpu(ix->ei_block),\n\t\t\t\t       le32_to_cpu(ix[-1].ei_block));\n\t\t\t}\n\t\t\tBUG_ON(k && le32_to_cpu(ix->ei_block)\n\t\t\t\t\t   <= le32_to_cpu(ix[-1].ei_block));\n\t\t\tif (block < le32_to_cpu(ix->ei_block))\n\t\t\t\tbreak;\n\t\t\tchix = ix;\n\t\t}\n\t\tBUG_ON(chix != path->p_idx);\n\t}\n#endif\n\n}\n\n/*\n * ext4_ext_binsearch:\n * binary search for closest extent of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch(struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent *r, *l, *m;\n\n\tif (eh->eh_entries == 0) {\n\t\t/*\n\t\t * this leaf is empty:\n\t\t * we get such a leaf in split/add case\n\t\t */\n\t\treturn;\n\t}\n\n\text_debug(\"binsearch for %u:  \", block);\n\n\tl = EXT_FIRST_EXTENT(eh) + 1;\n\tr = EXT_LAST_EXTENT(eh);\n\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ee_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ee_block),\n\t\t\t\tm, le32_to_cpu(m->ee_block),\n\t\t\t\tr, le32_to_cpu(r->ee_block));\n\t}\n\n\tpath->p_ext = l - 1;\n\text_debug(\"  -> %d:%llu:[%d]%d \",\n\t\t\tle32_to_cpu(path->p_ext->ee_block),\n\t\t\text4_ext_pblock(path->p_ext),\n\t\t\text4_ext_is_uninitialized(path->p_ext),\n\t\t\text4_ext_get_actual_len(path->p_ext));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent *chex, *ex;\n\t\tint k;\n\n\t\tchex = ex = EXT_FIRST_EXTENT(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) {\n\t\t\tBUG_ON(k && le32_to_cpu(ex->ee_block)\n\t\t\t\t\t  <= le32_to_cpu(ex[-1].ee_block));\n\t\t\tif (block < le32_to_cpu(ex->ee_block))\n\t\t\t\tbreak;\n\t\t\tchex = ex;\n\t\t}\n\t\tBUG_ON(chex != path->p_ext);\n\t}\n#endif\n\n}\n\nint ext4_ext_tree_init(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_extent_header *eh;\n\n\teh = ext_inode_hdr(inode);\n\teh->eh_depth = 0;\n\teh->eh_entries = 0;\n\teh->eh_magic = EXT4_EXT_MAGIC;\n\teh->eh_max = cpu_to_le16(ext4_ext_space_root(inode, 0));\n\text4_mark_inode_dirty(handle, inode);\n\text4_ext_invalidate_cache(inode);\n\treturn 0;\n}\n\nstruct ext4_ext_path *\next4_ext_find_extent(struct inode *inode, ext4_lblk_t block,\n\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct buffer_head *bh;\n\tshort int depth, i, ppos = 0, alloc = 0;\n\n\teh = ext_inode_hdr(inode);\n\tdepth = ext_depth(inode);\n\n\t/* account possible depth increase */\n\tif (!path) {\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 2),\n\t\t\t\tGFP_NOFS);\n\t\tif (!path)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\talloc = 1;\n\t}\n\tpath[0].p_hdr = eh;\n\tpath[0].p_bh = NULL;\n\n\ti = depth;\n\t/* walk through the tree */\n\twhile (i) {\n\t\text_debug(\"depth %d: num %d, max %d\\n\",\n\t\t\t  ppos, le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\n\t\text4_ext_binsearch_idx(inode, path + ppos, block);\n\t\tpath[ppos].p_block = ext4_idx_pblock(path[ppos].p_idx);\n\t\tpath[ppos].p_depth = i;\n\t\tpath[ppos].p_ext = NULL;\n\n\t\tbh = sb_getblk(inode->i_sb, path[ppos].p_block);\n\t\tif (unlikely(!bh))\n\t\t\tgoto err;\n\t\tif (!bh_uptodate_or_lock(bh)) {\n\t\t\ttrace_ext4_ext_load_extent(inode, block,\n\t\t\t\t\t\tpath[ppos].p_block);\n\t\t\tif (bh_submit_read(bh) < 0) {\n\t\t\t\tput_bh(bh);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\t\teh = ext_block_hdr(bh);\n\t\tppos++;\n\t\tif (unlikely(ppos > depth)) {\n\t\t\tput_bh(bh);\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"ppos %d > depth %d\", ppos, depth);\n\t\t\tgoto err;\n\t\t}\n\t\tpath[ppos].p_bh = bh;\n\t\tpath[ppos].p_hdr = eh;\n\t\ti--;\n\n\t\tif (ext4_ext_check_block(inode, eh, i, bh))\n\t\t\tgoto err;\n\t}\n\n\tpath[ppos].p_depth = i;\n\tpath[ppos].p_ext = NULL;\n\tpath[ppos].p_idx = NULL;\n\n\t/* find extent */\n\text4_ext_binsearch(inode, path + ppos, block);\n\t/* if not an empty leaf */\n\tif (path[ppos].p_ext)\n\t\tpath[ppos].p_block = ext4_ext_pblock(path[ppos].p_ext);\n\n\text4_ext_show_path(inode, path);\n\n\treturn path;\n\nerr:\n\text4_ext_drop_refs(path);\n\tif (alloc)\n\t\tkfree(path);\n\treturn ERR_PTR(-EIO);\n}\n\n/*\n * ext4_ext_insert_index:\n * insert new index [@logical;@ptr] into the block at @curp;\n * check where to insert: before @curp or after @curp\n */\nstatic int ext4_ext_insert_index(handle_t *handle, struct inode *inode,\n\t\t\t\t struct ext4_ext_path *curp,\n\t\t\t\t int logical, ext4_fsblk_t ptr)\n{\n\tstruct ext4_extent_idx *ix;\n\tint len, err;\n\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\treturn err;\n\n\tif (unlikely(logical == le32_to_cpu(curp->p_idx->ei_block))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d == ei_block %d!\",\n\t\t\t\t logical, le32_to_cpu(curp->p_idx->ei_block));\n\t\treturn -EIO;\n\t}\n\n\tif (unlikely(le16_to_cpu(curp->p_hdr->eh_entries)\n\t\t\t     >= le16_to_cpu(curp->p_hdr->eh_max))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"eh_entries %d >= eh_max %d!\",\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_entries),\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_max));\n\t\treturn -EIO;\n\t}\n\n\tif (logical > le32_to_cpu(curp->p_idx->ei_block)) {\n\t\t/* insert after */\n\t\text_debug(\"insert new index %d after: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx + 1;\n\t} else {\n\t\t/* insert before */\n\t\text_debug(\"insert new index %d before: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx;\n\t}\n\n\tlen = EXT_LAST_INDEX(curp->p_hdr) - ix + 1;\n\tBUG_ON(len < 0);\n\tif (len > 0) {\n\t\text_debug(\"insert new index %d: \"\n\t\t\t\t\"move %d indices from 0x%p to 0x%p\\n\",\n\t\t\t\tlogical, len, ix, ix + 1);\n\t\tmemmove(ix + 1, ix, len * sizeof(struct ext4_extent_idx));\n\t}\n\n\tif (unlikely(ix > EXT_MAX_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_MAX_INDEX!\");\n\t\treturn -EIO;\n\t}\n\n\tix->ei_block = cpu_to_le32(logical);\n\text4_idx_store_pblock(ix, ptr);\n\tle16_add_cpu(&curp->p_hdr->eh_entries, 1);\n\n\tif (unlikely(ix > EXT_LAST_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_LAST_INDEX!\");\n\t\treturn -EIO;\n\t}\n\n\terr = ext4_ext_dirty(handle, inode, curp);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_split:\n * inserts new subtree into the path, using free index entry\n * at depth @at:\n * - allocates all needed blocks (new leaf and all intermediate index blocks)\n * - makes decision where to split\n * - moves remaining extents and index entries (right to the split point)\n *   into the newly allocated blocks\n * - initializes subtree\n */\nstatic int ext4_ext_split(handle_t *handle, struct inode *inode,\n\t\t\t  unsigned int flags,\n\t\t\t  struct ext4_ext_path *path,\n\t\t\t  struct ext4_extent *newext, int at)\n{\n\tstruct buffer_head *bh = NULL;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tint i = at, k, m, a;\n\text4_fsblk_t newblock, oldblock;\n\t__le32 border;\n\text4_fsblk_t *ablocks = NULL; /* array of allocated blocks */\n\tint err = 0;\n\n\t/* make decision: where to split? */\n\t/* FIXME: now decision is simplest: at current extent */\n\n\t/* if current leaf will be split, then we should use\n\t * border from split point */\n\tif (unlikely(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"p_ext > EXT_MAX_EXTENT!\");\n\t\treturn -EIO;\n\t}\n\tif (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\tborder = path[depth].p_ext[1].ee_block;\n\t\text_debug(\"leaf will be split.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\t  le32_to_cpu(border));\n\t} else {\n\t\tborder = newext->ee_block;\n\t\text_debug(\"leaf will be added.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\tle32_to_cpu(border));\n\t}\n\n\t/*\n\t * If error occurs, then we break processing\n\t * and mark filesystem read-only. index won't\n\t * be inserted and tree will be in consistent\n\t * state. Next mount will repair buffers too.\n\t */\n\n\t/*\n\t * Get array to track all allocated blocks.\n\t * We need this to handle errors and free blocks\n\t * upon them.\n\t */\n\tablocks = kzalloc(sizeof(ext4_fsblk_t) * depth, GFP_NOFS);\n\tif (!ablocks)\n\t\treturn -ENOMEM;\n\n\t/* allocate all needed blocks */\n\text_debug(\"allocate %d blocks for indexes/leaf\\n\", depth - at);\n\tfor (a = 0; a < depth - at; a++) {\n\t\tnewblock = ext4_ext_new_meta_block(handle, inode, path,\n\t\t\t\t\t\t   newext, &err, flags);\n\t\tif (newblock == 0)\n\t\t\tgoto cleanup;\n\t\tablocks[a] = newblock;\n\t}\n\n\t/* initialize new leaf */\n\tnewblock = ablocks[--a];\n\tif (unlikely(newblock == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"newblock == 0!\");\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err)\n\t\tgoto cleanup;\n\n\tneh = ext_block_hdr(bh);\n\tneh->eh_entries = 0;\n\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tneh->eh_depth = 0;\n\n\t/* move remainder of path[depth] to the new leaf */\n\tif (unlikely(path[depth].p_hdr->eh_entries !=\n\t\t     path[depth].p_hdr->eh_max)) {\n\t\tEXT4_ERROR_INODE(inode, \"eh_entries %d != eh_max %d!\",\n\t\t\t\t path[depth].p_hdr->eh_entries,\n\t\t\t\t path[depth].p_hdr->eh_max);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\t/* start copy from next extent */\n\tm = EXT_MAX_EXTENT(path[depth].p_hdr) - path[depth].p_ext++;\n\text4_ext_show_move(inode, path, newblock, depth);\n\tif (m) {\n\t\tstruct ext4_extent *ex;\n\t\tex = EXT_FIRST_EXTENT(neh);\n\t\tmemmove(ex, path[depth].p_ext, sizeof(struct ext4_extent) * m);\n\t\tle16_add_cpu(&neh->eh_entries, m);\n\t}\n\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto cleanup;\n\tbrelse(bh);\n\tbh = NULL;\n\n\t/* correct old leaf */\n\tif (m) {\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tle16_add_cpu(&path[depth].p_hdr->eh_entries, -m);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t}\n\n\t/* create intermediate indexes */\n\tk = depth - at - 1;\n\tif (unlikely(k < 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"k %d < 0!\", k);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tif (k)\n\t\text_debug(\"create %d intermediate indices\\n\", k);\n\t/* insert new index into current index block */\n\t/* current depth stored in i var */\n\ti = depth - 1;\n\twhile (k--) {\n\t\toldblock = newblock;\n\t\tnewblock = ablocks[--a];\n\t\tbh = sb_getblk(inode->i_sb, newblock);\n\t\tif (!bh) {\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tlock_buffer(bh);\n\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\tneh = ext_block_hdr(bh);\n\t\tneh->eh_entries = cpu_to_le16(1);\n\t\tneh->eh_magic = EXT4_EXT_MAGIC;\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\t\tneh->eh_depth = cpu_to_le16(depth - i);\n\t\tfidx = EXT_FIRST_INDEX(neh);\n\t\tfidx->ei_block = border;\n\t\text4_idx_store_pblock(fidx, oldblock);\n\n\t\text_debug(\"int.index at %d (block %llu): %u -> %llu\\n\",\n\t\t\t\ti, newblock, le32_to_cpu(border), oldblock);\n\n\t\t/* move remainder of path[i] to the new index block */\n\t\tif (unlikely(EXT_MAX_INDEX(path[i].p_hdr) !=\n\t\t\t\t\tEXT_LAST_INDEX(path[i].p_hdr))) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_MAX_INDEX != EXT_LAST_INDEX ee_block %d!\",\n\t\t\t\t\t le32_to_cpu(path[i].p_ext->ee_block));\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\t/* start copy indexes */\n\t\tm = EXT_MAX_INDEX(path[i].p_hdr) - path[i].p_idx++;\n\t\text_debug(\"cur 0x%p, last 0x%p\\n\", path[i].p_idx,\n\t\t\t\tEXT_MAX_INDEX(path[i].p_hdr));\n\t\text4_ext_show_move(inode, path, newblock, i);\n\t\tif (m) {\n\t\t\tmemmove(++fidx, path[i].p_idx,\n\t\t\t\tsizeof(struct ext4_extent_idx) * m);\n\t\t\tle16_add_cpu(&neh->eh_entries, m);\n\t\t}\n\t\text4_extent_block_csum_set(inode, neh);\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tbrelse(bh);\n\t\tbh = NULL;\n\n\t\t/* correct old index */\n\t\tif (m) {\n\t\t\terr = ext4_ext_get_access(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t\tle16_add_cpu(&path[i].p_hdr->eh_entries, -m);\n\t\t\terr = ext4_ext_dirty(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t}\n\n\t\ti--;\n\t}\n\n\t/* insert new index */\n\terr = ext4_ext_insert_index(handle, inode, path + at,\n\t\t\t\t    le32_to_cpu(border), newblock);\n\ncleanup:\n\tif (bh) {\n\t\tif (buffer_locked(bh))\n\t\t\tunlock_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\tif (err) {\n\t\t/* free all allocated blocks in error case */\n\t\tfor (i = 0; i < depth; i++) {\n\t\t\tif (!ablocks[i])\n\t\t\t\tcontinue;\n\t\t\text4_free_blocks(handle, inode, NULL, ablocks[i], 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\t\t}\n\t}\n\tkfree(ablocks);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_grow_indepth:\n * implements tree growing procedure:\n * - allocates new block\n * - moves top-level data (index block or leaf) into the new block\n * - initializes new top-level, creating index that points to the\n *   just created block\n */\nstatic int ext4_ext_grow_indepth(handle_t *handle, struct inode *inode,\n\t\t\t\t unsigned int flags,\n\t\t\t\t struct ext4_extent *newext)\n{\n\tstruct ext4_extent_header *neh;\n\tstruct buffer_head *bh;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\n\tnewblock = ext4_ext_new_meta_block(handle, inode, NULL,\n\t\tnewext, &err, flags);\n\tif (newblock == 0)\n\t\treturn err;\n\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\text4_std_error(inode->i_sb, err);\n\t\treturn err;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err) {\n\t\tunlock_buffer(bh);\n\t\tgoto out;\n\t}\n\n\t/* move top-level index/leaf into new block */\n\tmemmove(bh->b_data, EXT4_I(inode)->i_data,\n\t\tsizeof(EXT4_I(inode)->i_data));\n\n\t/* set size of new block */\n\tneh = ext_block_hdr(bh);\n\t/* old root could have indexes or leaves\n\t * so calculate e_max right way */\n\tif (ext_depth(inode))\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\telse\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto out;\n\n\t/* Update top-level index: num,max,pointer */\n\tneh = ext_inode_hdr(inode);\n\tneh->eh_entries = cpu_to_le16(1);\n\text4_idx_store_pblock(EXT_FIRST_INDEX(neh), newblock);\n\tif (neh->eh_depth == 0) {\n\t\t/* Root extent block becomes index block */\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_root_idx(inode, 0));\n\t\tEXT_FIRST_INDEX(neh)->ei_block =\n\t\t\tEXT_FIRST_EXTENT(neh)->ee_block;\n\t}\n\text_debug(\"new root: num %d(%d), lblock %d, ptr %llu\\n\",\n\t\t  le16_to_cpu(neh->eh_entries), le16_to_cpu(neh->eh_max),\n\t\t  le32_to_cpu(EXT_FIRST_INDEX(neh)->ei_block),\n\t\t  ext4_idx_pblock(EXT_FIRST_INDEX(neh)));\n\n\tle16_add_cpu(&neh->eh_depth, 1);\n\text4_mark_inode_dirty(handle, inode);\nout:\n\tbrelse(bh);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_create_new_leaf:\n * finds empty index and adds new leaf.\n * if no free index is found, then it requests in-depth growing.\n */\nstatic int ext4_ext_create_new_leaf(handle_t *handle, struct inode *inode,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct ext4_ext_path *path,\n\t\t\t\t    struct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *curp;\n\tint depth, i, err = 0;\n\nrepeat:\n\ti = depth = ext_depth(inode);\n\n\t/* walk up to the tree and look for free index entry */\n\tcurp = path + depth;\n\twhile (i > 0 && !EXT_HAS_FREE_INDEX(curp)) {\n\t\ti--;\n\t\tcurp--;\n\t}\n\n\t/* we use already allocated block for index block,\n\t * so subsequent data blocks should be contiguous */\n\tif (EXT_HAS_FREE_INDEX(curp)) {\n\t\t/* if we found index with free entry, then use that\n\t\t * entry: create all needed subtree and add new leaf */\n\t\terr = ext4_ext_split(handle, inode, flags, path, newext, i);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t    (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path))\n\t\t\terr = PTR_ERR(path);\n\t} else {\n\t\t/* tree is full, time to grow in depth */\n\t\terr = ext4_ext_grow_indepth(handle, inode, flags, newext);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t   (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * only first (depth 0 -> 1) produces free space;\n\t\t * in all other cases we have to split the grown tree\n\t\t */\n\t\tdepth = ext_depth(inode);\n\t\tif (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) {\n\t\t\t/* now we need to split */\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * search the closest allocated block to the left for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_left(struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\tint depth, ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_FIRST_EXTENT != ex *logical %d ee_block %d!\",\n\t\t\t\t\t *logical, le32_to_cpu(ex->ee_block));\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t  \"ix (%d) != EXT_FIRST_INDEX (%d) (depth %d)!\",\n\t\t\t\t  ix != NULL ? le32_to_cpu(ix->ei_block) : 0,\n\t\t\t\t  EXT_FIRST_INDEX(path[depth].p_hdr) != NULL ?\n\t\tle32_to_cpu(EXT_FIRST_INDEX(path[depth].p_hdr)->ei_block) : 0,\n\t\t\t\t  depth);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\t*logical = le32_to_cpu(ex->ee_block) + ee_len - 1;\n\t*phys = ext4_ext_pblock(ex) + ee_len - 1;\n\treturn 0;\n}\n\n/*\n * search the closest allocated block to the right for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the largest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t ext4_lblk_t *logical, ext4_fsblk_t *phys,\n\t\t\t\t struct ext4_extent **ret_ex)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t block;\n\tint depth;\t/* Note, NOT eh_depth; depth from top of tree */\n\tint ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"first_extent(path[%d].p_hdr) != ex\",\n\t\t\t\t\t depth);\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"ix != EXT_FIRST_INDEX *logical %d!\",\n\t\t\t\t\t\t *logical);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\tgoto found_extent;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\tif (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t/* next allocated block in this leaf */\n\t\tex++;\n\t\tgoto found_extent;\n\t}\n\n\t/* go up and search for index to the right */\n\twhile (--depth >= 0) {\n\t\tix = path[depth].p_idx;\n\t\tif (ix != EXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\tgoto got_index;\n\t}\n\n\t/* we've gone up to the root and found no index to the right */\n\treturn 0;\n\ngot_index:\n\t/* we've found index to the right, let's\n\t * follow it and find the closest allocated\n\t * block to the right */\n\tix++;\n\tblock = ext4_idx_pblock(ix);\n\twhile (++depth < path->p_depth) {\n\t\tbh = sb_bread(inode->i_sb, block);\n\t\tif (bh == NULL)\n\t\t\treturn -EIO;\n\t\teh = ext_block_hdr(bh);\n\t\t/* subtract from p_depth to get proper eh_depth */\n\t\tif (ext4_ext_check_block(inode, eh,\n\t\t\t\t\t path->p_depth - depth, bh)) {\n\t\t\tput_bh(bh);\n\t\t\treturn -EIO;\n\t\t}\n\t\tix = EXT_FIRST_INDEX(eh);\n\t\tblock = ext4_idx_pblock(ix);\n\t\tput_bh(bh);\n\t}\n\n\tbh = sb_bread(inode->i_sb, block);\n\tif (bh == NULL)\n\t\treturn -EIO;\n\teh = ext_block_hdr(bh);\n\tif (ext4_ext_check_block(inode, eh, path->p_depth - depth, bh)) {\n\t\tput_bh(bh);\n\t\treturn -EIO;\n\t}\n\tex = EXT_FIRST_EXTENT(eh);\nfound_extent:\n\t*logical = le32_to_cpu(ex->ee_block);\n\t*phys = ext4_ext_pblock(ex);\n\t*ret_ex = ex;\n\tif (bh)\n\t\tput_bh(bh);\n\treturn 0;\n}\n\n/*\n * ext4_ext_next_allocated_block:\n * returns allocated block in subsequent extent or EXT_MAX_BLOCKS.\n * NOTE: it considers block number from index entry as\n * allocated block. Thus, index entries have to be consistent\n * with leaves.\n */\nstatic ext4_lblk_t\next4_ext_next_allocated_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn EXT_MAX_BLOCKS;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\t/* leaf */\n\t\t\tif (path[depth].p_ext &&\n\t\t\t\tpath[depth].p_ext !=\n\t\t\t\t\tEXT_LAST_EXTENT(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_ext[1].ee_block);\n\t\t} else {\n\t\t\t/* index */\n\t\t\tif (path[depth].p_idx !=\n\t\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\t}\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_next_leaf_block:\n * returns first allocated block from next leaf or EXT_MAX_BLOCKS\n */\nstatic ext4_lblk_t ext4_ext_next_leaf_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\t/* zero-tree has no leaf blocks at all */\n\tif (depth == 0)\n\t\treturn EXT_MAX_BLOCKS;\n\n\t/* go to index block */\n\tdepth--;\n\n\twhile (depth >= 0) {\n\t\tif (path[depth].p_idx !=\n\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\treturn (ext4_lblk_t)\n\t\t\t\tle32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_correct_indexes:\n * if leaf gets modified and modified extent is first in the leaf,\n * then we have to correct all indexes above.\n * TODO: do we need to correct tree in all cases?\n */\nstatic int ext4_ext_correct_indexes(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\t__le32 border;\n\tint k, err = 0;\n\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\tif (unlikely(ex == NULL || eh == NULL)) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"ex %p == NULL or eh %p == NULL\", ex, eh);\n\t\treturn -EIO;\n\t}\n\n\tif (depth == 0) {\n\t\t/* there is no tree at all */\n\t\treturn 0;\n\t}\n\n\tif (ex != EXT_FIRST_EXTENT(eh)) {\n\t\t/* we correct tree if first leaf got modified only */\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: we need correction if border is smaller than current one\n\t */\n\tk = depth - 1;\n\tborder = path[depth].p_ext->ee_block;\n\terr = ext4_ext_get_access(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\tpath[k].p_idx->ei_block = border;\n\terr = ext4_ext_dirty(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\n\twhile (k--) {\n\t\t/* change all left-side indexes */\n\t\tif (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr))\n\t\t\tbreak;\n\t\terr = ext4_ext_get_access(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath[k].p_idx->ei_block = border;\n\t\terr = ext4_ext_dirty(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint\next4_can_extents_be_merged(struct inode *inode, struct ext4_extent *ex1,\n\t\t\t\tstruct ext4_extent *ex2)\n{\n\tunsigned short ext1_ee_len, ext2_ee_len, max_len;\n\n\t/*\n\t * Make sure that either both extents are uninitialized, or\n\t * both are _not_.\n\t */\n\tif (ext4_ext_is_uninitialized(ex1) ^ ext4_ext_is_uninitialized(ex2))\n\t\treturn 0;\n\n\tif (ext4_ext_is_uninitialized(ex1))\n\t\tmax_len = EXT_UNINIT_MAX_LEN;\n\telse\n\t\tmax_len = EXT_INIT_MAX_LEN;\n\n\text1_ee_len = ext4_ext_get_actual_len(ex1);\n\text2_ee_len = ext4_ext_get_actual_len(ex2);\n\n\tif (le32_to_cpu(ex1->ee_block) + ext1_ee_len !=\n\t\t\tle32_to_cpu(ex2->ee_block))\n\t\treturn 0;\n\n\t/*\n\t * To allow future support for preallocated extents to be added\n\t * as an RO_COMPAT feature, refuse to merge to extents if\n\t * this can result in the top bit of ee_len being set.\n\t */\n\tif (ext1_ee_len + ext2_ee_len > max_len)\n\t\treturn 0;\n#ifdef AGGRESSIVE_TEST\n\tif (ext1_ee_len >= 4)\n\t\treturn 0;\n#endif\n\n\tif (ext4_ext_pblock(ex1) + ext1_ee_len == ext4_ext_pblock(ex2))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function tries to merge the \"ex\" extent to the next extent in the tree.\n * It always tries to merge towards right. If you want to merge towards\n * left, pass \"ex - 1\" as argument instead of \"ex\".\n * Returns 0 if the extents (ex and ex+1) were _not_ merged and returns\n * 1 if they got merged.\n */\nstatic int ext4_ext_try_to_merge_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t struct ext4_extent *ex)\n{\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth, len;\n\tint merge_done = 0;\n\tint uninitialized = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\twhile (ex < EXT_LAST_EXTENT(eh)) {\n\t\tif (!ext4_can_extents_be_merged(inode, ex, ex + 1))\n\t\t\tbreak;\n\t\t/* merge with next extent! */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t+ ext4_ext_get_actual_len(ex + 1));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\n\t\tif (ex + 1 < EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = (EXT_LAST_EXTENT(eh) - ex - 1)\n\t\t\t\t* sizeof(struct ext4_extent);\n\t\t\tmemmove(ex + 1, ex + 2, len);\n\t\t}\n\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\tmerge_done = 1;\n\t\tWARN_ON(eh->eh_entries == 0);\n\t\tif (!eh->eh_entries)\n\t\t\tEXT4_ERROR_INODE(inode, \"eh->eh_entries = 0!\");\n\t}\n\n\treturn merge_done;\n}\n\n/*\n * This function does a very simple check to see if we can collapse\n * an extent tree with a single extent tree leaf block into the inode.\n */\nstatic void ext4_ext_try_to_merge_up(handle_t *handle,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tsize_t s;\n\tunsigned max_root = ext4_ext_space_root(inode, 0);\n\text4_fsblk_t blk;\n\n\tif ((path[0].p_depth != 1) ||\n\t    (le16_to_cpu(path[0].p_hdr->eh_entries) != 1) ||\n\t    (le16_to_cpu(path[1].p_hdr->eh_entries) > max_root))\n\t\treturn;\n\n\t/*\n\t * We need to modify the block allocation bitmap and the block\n\t * group descriptor to release the extent tree block.  If we\n\t * can't get the journal credits, give up.\n\t */\n\tif (ext4_journal_extend(handle, 2))\n\t\treturn;\n\n\t/*\n\t * Copy the extent data up to the inode\n\t */\n\tblk = ext4_idx_pblock(path[0].p_idx);\n\ts = le16_to_cpu(path[1].p_hdr->eh_entries) *\n\t\tsizeof(struct ext4_extent_idx);\n\ts += sizeof(struct ext4_extent_header);\n\n\tmemcpy(path[0].p_hdr, path[1].p_hdr, s);\n\tpath[0].p_depth = 0;\n\tpath[0].p_ext = EXT_FIRST_EXTENT(path[0].p_hdr) +\n\t\t(path[1].p_ext - EXT_FIRST_EXTENT(path[1].p_hdr));\n\tpath[0].p_hdr->eh_max = cpu_to_le16(max_root);\n\n\tbrelse(path[1].p_bh);\n\text4_free_blocks(handle, inode, NULL, blk, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n}\n\n/*\n * This function tries to merge the @ex extent to neighbours in the tree.\n * return 1 if merge left else 0.\n */\nstatic void ext4_ext_try_to_merge(handle_t *handle,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct ext4_ext_path *path,\n\t\t\t\t  struct ext4_extent *ex) {\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth;\n\tint merge_done = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\tif (ex > EXT_FIRST_EXTENT(eh))\n\t\tmerge_done = ext4_ext_try_to_merge_right(inode, path, ex - 1);\n\n\tif (!merge_done)\n\t\t(void) ext4_ext_try_to_merge_right(inode, path, ex);\n\n\text4_ext_try_to_merge_up(handle, inode, path);\n}\n\n/*\n * check if a portion of the \"newext\" extent overlaps with an\n * existing extent.\n *\n * If there is an overlap discovered, it updates the length of the newext\n * such that there will be no overlap, and then returns 1.\n * If there is no overlap found, it returns 0.\n */\nstatic unsigned int ext4_ext_check_overlap(struct ext4_sb_info *sbi,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_extent *newext,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\text4_lblk_t b1, b2;\n\tunsigned int depth, len1;\n\tunsigned int ret = 0;\n\n\tb1 = le32_to_cpu(newext->ee_block);\n\tlen1 = ext4_ext_get_actual_len(newext);\n\tdepth = ext_depth(inode);\n\tif (!path[depth].p_ext)\n\t\tgoto out;\n\tb2 = le32_to_cpu(path[depth].p_ext->ee_block);\n\tb2 &= ~(sbi->s_cluster_ratio - 1);\n\n\t/*\n\t * get the next allocated block if the extent in the path\n\t * is before the requested block(s)\n\t */\n\tif (b2 < b1) {\n\t\tb2 = ext4_ext_next_allocated_block(path);\n\t\tif (b2 == EXT_MAX_BLOCKS)\n\t\t\tgoto out;\n\t\tb2 &= ~(sbi->s_cluster_ratio - 1);\n\t}\n\n\t/* check for wrap through zero on extent logical start block*/\n\tif (b1 + len1 < b1) {\n\t\tlen1 = EXT_MAX_BLOCKS - b1;\n\t\tnewext->ee_len = cpu_to_le16(len1);\n\t\tret = 1;\n\t}\n\n\t/* check for overlap */\n\tif (b1 + len1 > b2) {\n\t\tnewext->ee_len = cpu_to_le16(b2 - b1);\n\t\tret = 1;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * ext4_ext_insert_extent:\n * tries to merge requsted extent into the existing extent or\n * inserts requested extent as new one into the tree,\n * creating new leaf in the no-space case.\n */\nint ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int flag)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tunsigned uninitialized = 0;\n\tint flags = 0;\n\n\tif (unlikely(ext4_ext_get_actual_len(newext) == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"ext4_ext_get_actual_len(newext) == 0\");\n\t\treturn -EIO;\n\t}\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(flag & EXT4_GET_BLOCKS_PRE_IO)\n\t\t&& ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\text_debug(\"append [%d]%d block to %u:[%d]%d (from %llu)\\n\",\n\t\t\t  ext4_ext_is_uninitialized(newext),\n\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t  ext4_ext_pblock(ex));\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t/*\n\t\t * ext4_can_extents_be_merged should have checked that either\n\t\t * both extents are uninitialized, or both aren't. Thus we\n\t\t * need to check only one of them here.\n\t\t */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\teh = path[depth].p_hdr;\n\t\tnearex = ex;\n\t\tgoto merge;\n\t}\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = EXT_MAX_BLOCKS;\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block))\n\t\tnext = ext4_ext_next_leaf_block(path);\n\tif (next != EXT_MAX_BLOCKS) {\n\t\text_debug(\"next leaf block - %u\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_ext_find_extent(inode, next, NULL);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isn't full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto has_space;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\tif (flag & EXT4_GET_BLOCKS_PUNCH_OUT_EXT)\n\t\tflags = EXT4_MB_USE_ROOT_BLOCKS;\n\terr = ext4_ext_create_new_leaf(handle, inode, flags, path, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %u:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tnearex = EXT_FIRST_EXTENT(eh);\n\t} else {\n\t\tif (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n\t\t\t/* Insert after */\n\t\t\text_debug(\"insert %u:%llu:[%d]%d before: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t\tnearex++;\n\t\t} else {\n\t\t\t/* Insert before */\n\t\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\t\text_debug(\"insert %u:%llu:[%d]%d after: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t}\n\t\tlen = EXT_LAST_EXTENT(eh) - nearex + 1;\n\t\tif (len > 0) {\n\t\t\text_debug(\"insert %u:%llu:[%d]%d: \"\n\t\t\t\t\t\"move %d extents from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tlen, nearex, nearex + 1);\n\t\t\tmemmove(nearex + 1, nearex,\n\t\t\t\tlen * sizeof(struct ext4_extent));\n\t\t}\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tpath[depth].p_ext = nearex;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext4_ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents */\n\tif (!(flag & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(handle, inode, path, nearex);\n\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\ncleanup:\n\tif (npath) {\n\t\text4_ext_drop_refs(npath);\n\t\tkfree(npath);\n\t}\n\text4_ext_invalidate_cache(inode);\n\treturn err;\n}\n\nstatic int ext4_ext_walk_space(struct inode *inode, ext4_lblk_t block,\n\t\t\t       ext4_lblk_t num, ext_prepare_callback func,\n\t\t\t       void *cbdata)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_ext_cache cbex;\n\tstruct ext4_extent *ex;\n\text4_lblk_t next, start = 0, end = 0;\n\text4_lblk_t last = block + num;\n\tint depth, exists, err = 0;\n\n\tBUG_ON(func == NULL);\n\tBUG_ON(inode == NULL);\n\n\twhile (block < last && block != EXT_MAX_BLOCKS) {\n\t\tnum = last - block;\n\t\t/* find extent for this block */\n\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\t\tpath = ext4_ext_find_extent(inode, block, path);\n\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tpath = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdepth = ext_depth(inode);\n\t\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tex = path[depth].p_ext;\n\t\tnext = ext4_ext_next_allocated_block(path);\n\n\t\texists = 0;\n\t\tif (!ex) {\n\t\t\t/* there is no extent yet, so try to allocate\n\t\t\t * all requested space */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t} else if (le32_to_cpu(ex->ee_block) > block) {\n\t\t\t/* need to allocate space before found extent */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\t\t/* need to allocate space after found extent */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t\tif (end >= next)\n\t\t\t\tend = next;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)) {\n\t\t\t/*\n\t\t\t * some part of requested space is covered\n\t\t\t * by found extent\n\t\t\t */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block)\n\t\t\t\t+ ext4_ext_get_actual_len(ex);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t\texists = 1;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t\tBUG_ON(end <= start);\n\n\t\tif (!exists) {\n\t\t\tcbex.ec_block = start;\n\t\t\tcbex.ec_len = end - start;\n\t\t\tcbex.ec_start = 0;\n\t\t} else {\n\t\t\tcbex.ec_block = le32_to_cpu(ex->ee_block);\n\t\t\tcbex.ec_len = ext4_ext_get_actual_len(ex);\n\t\t\tcbex.ec_start = ext4_ext_pblock(ex);\n\t\t}\n\n\t\tif (unlikely(cbex.ec_len == 0)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"cbex.ec_len == 0\");\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\terr = func(inode, next, &cbex, ex, cbdata);\n\t\text4_ext_drop_refs(path);\n\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t\tif (err == EXT_REPEAT)\n\t\t\tcontinue;\n\t\telse if (err == EXT_BREAK) {\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ext_depth(inode) != depth) {\n\t\t\t/* depth was changed. we have to realloc path */\n\t\t\tkfree(path);\n\t\t\tpath = NULL;\n\t\t}\n\n\t\tblock = cbex.ec_block + cbex.ec_len;\n\t}\n\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\n\treturn err;\n}\n\nstatic void\next4_ext_put_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t\t__u32 len, ext4_fsblk_t start)\n{\n\tstruct ext4_ext_cache *cex;\n\tBUG_ON(len == 0);\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\ttrace_ext4_ext_put_in_cache(inode, block, len, start);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\tcex->ec_block = block;\n\tcex->ec_len = len;\n\tcex->ec_start = start;\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n}\n\n/*\n * ext4_ext_put_gap_in_cache:\n * calculate boundaries of the gap that the requested block fits into\n * and cache this gap\n */\nstatic void\next4_ext_put_gap_in_cache(struct inode *inode, struct ext4_ext_path *path,\n\t\t\t\text4_lblk_t block)\n{\n\tint depth = ext_depth(inode);\n\tunsigned long len;\n\text4_lblk_t lblock;\n\tstruct ext4_extent *ex;\n\n\tex = path[depth].p_ext;\n\tif (ex == NULL) {\n\t\t/* there is no extent yet, so gap is [0;-] */\n\t\tlblock = 0;\n\t\tlen = EXT_MAX_BLOCKS;\n\t\text_debug(\"cache gap(whole file):\");\n\t} else if (block < le32_to_cpu(ex->ee_block)) {\n\t\tlblock = block;\n\t\tlen = le32_to_cpu(ex->ee_block) - block;\n\t\text_debug(\"cache gap(before): %u [%u:%u]\",\n\t\t\t\tblock,\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\t ext4_ext_get_actual_len(ex));\n\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\text4_lblk_t next;\n\t\tlblock = le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex);\n\n\t\tnext = ext4_ext_next_allocated_block(path);\n\t\text_debug(\"cache gap(after): [%u:%u] %u\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tblock);\n\t\tBUG_ON(next == lblock);\n\t\tlen = next - lblock;\n\t} else {\n\t\tlblock = len = 0;\n\t\tBUG();\n\t}\n\n\text_debug(\" -> %u:%lu\\n\", lblock, len);\n\text4_ext_put_in_cache(inode, lblock, len, 0);\n}\n\n/*\n * ext4_ext_in_cache()\n * Checks to see if the given block is in the cache.\n * If it is, the cached extent is stored in the given\n * cache extent pointer.\n *\n * @inode: The files inode\n * @block: The block to look for in the cache\n * @ex:    Pointer where the cached extent will be stored\n *         if it contains block\n *\n * Return 0 if cache is invalid; 1 if the cache is valid\n */\nstatic int\next4_ext_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t  struct ext4_extent *ex)\n{\n\tstruct ext4_ext_cache *cex;\n\tstruct ext4_sb_info *sbi;\n\tint ret = 0;\n\n\t/*\n\t * We borrow i_block_reservation_lock to protect i_cached_extent\n\t */\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\tsbi = EXT4_SB(inode->i_sb);\n\n\t/* has cache valid data? */\n\tif (cex->ec_len == 0)\n\t\tgoto errout;\n\n\tif (in_range(block, cex->ec_block, cex->ec_len)) {\n\t\tex->ee_block = cpu_to_le32(cex->ec_block);\n\t\text4_ext_store_pblock(ex, cex->ec_start);\n\t\tex->ee_len = cpu_to_le16(cex->ec_len);\n\t\text_debug(\"%u cached by %u:%u:%llu\\n\",\n\t\t\t\tblock,\n\t\t\t\tcex->ec_block, cex->ec_len, cex->ec_start);\n\t\tret = 1;\n\t}\nerrout:\n\ttrace_ext4_ext_in_cache(inode, block, ret);\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\treturn ret;\n}\n\n/*\n * ext4_ext_rm_idx:\n * removes index from the index block.\n */\nstatic int ext4_ext_rm_idx(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path)\n{\n\tint err;\n\text4_fsblk_t leaf;\n\n\t/* free index block */\n\tpath--;\n\tleaf = ext4_idx_pblock(path->p_idx);\n\tif (unlikely(path->p_hdr->eh_entries == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"path->p_hdr->eh_entries == 0\");\n\t\treturn -EIO;\n\t}\n\terr = ext4_ext_get_access(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\n\tif (path->p_idx != EXT_LAST_INDEX(path->p_hdr)) {\n\t\tint len = EXT_LAST_INDEX(path->p_hdr) - path->p_idx;\n\t\tlen *= sizeof(struct ext4_extent_idx);\n\t\tmemmove(path->p_idx, path->p_idx + 1, len);\n\t}\n\n\tle16_add_cpu(&path->p_hdr->eh_entries, -1);\n\terr = ext4_ext_dirty(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\text_debug(\"index is empty, remove it, free block %llu\\n\", leaf);\n\ttrace_ext4_ext_rm_idx(inode, leaf);\n\n\text4_free_blocks(handle, inode, NULL, leaf, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n\treturn err;\n}\n\n/*\n * ext4_ext_calc_credits_for_single_extent:\n * This routine returns max. credits that needed to insert an extent\n * to the extent tree.\n * When pass the actual path, the caller should calculate credits\n * under i_data_sem.\n */\nint ext4_ext_calc_credits_for_single_extent(struct inode *inode, int nrblocks,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path) {\n\t\tint depth = ext_depth(inode);\n\t\tint ret = 0;\n\n\t\t/* probably there is space in leaf? */\n\t\tif (le16_to_cpu(path[depth].p_hdr->eh_entries)\n\t\t\t\t< le16_to_cpu(path[depth].p_hdr->eh_max)) {\n\n\t\t\t/*\n\t\t\t *  There are some space in the leaf tree, no\n\t\t\t *  need to account for leaf block credit\n\t\t\t *\n\t\t\t *  bitmaps and block group descriptor blocks\n\t\t\t *  and other metadata blocks still need to be\n\t\t\t *  accounted.\n\t\t\t */\n\t\t\t/* 1 bitmap, 1 block group descriptor */\n\t\t\tret = 2 + EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ext4_chunk_trans_blocks(inode, nrblocks);\n}\n\n/*\n * How many index/leaf blocks need to change/allocate to modify nrblocks?\n *\n * if nrblocks are fit in a single extent (chunk flag is 1), then\n * in the worse case, each tree level index/leaf need to be changed\n * if the tree split due to insert a new extent, then the old tree\n * index/leaf need to be updated too\n *\n * If the nrblocks are discontiguous, they could cause\n * the whole tree split more than once, but this is really rare.\n */\nint ext4_ext_index_trans_blocks(struct inode *inode, int nrblocks, int chunk)\n{\n\tint index;\n\tint depth = ext_depth(inode);\n\n\tif (chunk)\n\t\tindex = depth * 2;\n\telse\n\t\tindex = depth * 3;\n\n\treturn index;\n}\n\nstatic int ext4_remove_blocks(handle_t *handle, struct inode *inode,\n\t\t\t      struct ext4_extent *ex,\n\t\t\t      ext4_fsblk_t *partial_cluster,\n\t\t\t      ext4_lblk_t from, ext4_lblk_t to)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tunsigned short ee_len =  ext4_ext_get_actual_len(ex);\n\text4_fsblk_t pblk;\n\tint flags = 0;\n\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\tflags |= EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET;\n\telse if (ext4_should_journal_data(inode))\n\t\tflags |= EXT4_FREE_BLOCKS_FORGET;\n\n\t/*\n\t * For bigalloc file systems, we never free a partial cluster\n\t * at the beginning of the extent.  Instead, we make a note\n\t * that we tried freeing the cluster, and check to see if we\n\t * need to free it on a subsequent call to ext4_remove_blocks,\n\t * or at the end of the ext4_truncate() operation.\n\t */\n\tflags |= EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER;\n\n\ttrace_ext4_remove_blocks(inode, ex, from, to, *partial_cluster);\n\t/*\n\t * If we have a partial cluster, and it's different from the\n\t * cluster of the last block, we need to explicitly free the\n\t * partial cluster here.\n\t */\n\tpblk = ext4_ext_pblock(ex) + ee_len - 1;\n\tif (*partial_cluster && (EXT4_B2C(sbi, pblk) != *partial_cluster)) {\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(sbi, *partial_cluster),\n\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\t*partial_cluster = 0;\n\t}\n\n#ifdef EXTENTS_STATS\n\t{\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\tspin_lock(&sbi->s_ext_stats_lock);\n\t\tsbi->s_ext_blocks += ee_len;\n\t\tsbi->s_ext_extents++;\n\t\tif (ee_len < sbi->s_ext_min)\n\t\t\tsbi->s_ext_min = ee_len;\n\t\tif (ee_len > sbi->s_ext_max)\n\t\t\tsbi->s_ext_max = ee_len;\n\t\tif (ext_depth(inode) > sbi->s_depth_max)\n\t\t\tsbi->s_depth_max = ext_depth(inode);\n\t\tspin_unlock(&sbi->s_ext_stats_lock);\n\t}\n#endif\n\tif (from >= le32_to_cpu(ex->ee_block)\n\t    && to == le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\t/* tail removal */\n\t\text4_lblk_t num;\n\n\t\tnum = le32_to_cpu(ex->ee_block) + ee_len - from;\n\t\tpblk = ext4_ext_pblock(ex) + ee_len - num;\n\t\text_debug(\"free last %u blocks starting %llu\\n\", num, pblk);\n\t\text4_free_blocks(handle, inode, NULL, pblk, num, flags);\n\t\t/*\n\t\t * If the block range to be freed didn't start at the\n\t\t * beginning of a cluster, and we removed the entire\n\t\t * extent, save the partial cluster here, since we\n\t\t * might need to delete if we determine that the\n\t\t * truncate operation has removed all of the blocks in\n\t\t * the cluster.\n\t\t */\n\t\tif (pblk & (sbi->s_cluster_ratio - 1) &&\n\t\t    (ee_len == num))\n\t\t\t*partial_cluster = EXT4_B2C(sbi, pblk);\n\t\telse\n\t\t\t*partial_cluster = 0;\n\t} else if (from == le32_to_cpu(ex->ee_block)\n\t\t   && to <= le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\t/* head removal */\n\t\text4_lblk_t num;\n\t\text4_fsblk_t start;\n\n\t\tnum = to - from;\n\t\tstart = ext4_ext_pblock(ex);\n\n\t\text_debug(\"free first %u blocks starting %llu\\n\", num, start);\n\t\text4_free_blocks(handle, inode, NULL, start, num, flags);\n\n\t} else {\n\t\tprintk(KERN_INFO \"strange request: removal(2) \"\n\t\t\t\t\"%u-%u from %u:%u\\n\",\n\t\t\t\tfrom, to, le32_to_cpu(ex->ee_block), ee_len);\n\t}\n\treturn 0;\n}\n\n\n/*\n * ext4_ext_rm_leaf() Removes the extents associated with the\n * blocks appearing between \"start\" and \"end\", and splits the extents\n * if \"start\" and \"end\" appear in the same extent\n *\n * @handle: The journal handle\n * @inode:  The files inode\n * @path:   The path to the leaf\n * @start:  The first block to remove\n * @end:   The last block to remove\n */\nstatic int\next4_ext_rm_leaf(handle_t *handle, struct inode *inode,\n\t\t struct ext4_ext_path *path, ext4_fsblk_t *partial_cluster,\n\t\t ext4_lblk_t start, ext4_lblk_t end)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint err = 0, correct_index = 0;\n\tint depth = ext_depth(inode), credits;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t a, b;\n\tunsigned num;\n\text4_lblk_t ex_ee_block;\n\tunsigned short ex_ee_len;\n\tunsigned uninitialized = 0;\n\tstruct ext4_extent *ex;\n\n\t/* the header must be checked already in ext4_ext_remove_space() */\n\text_debug(\"truncate since %u in leaf to %u\\n\", start, end);\n\tif (!path[depth].p_hdr)\n\t\tpath[depth].p_hdr = ext_block_hdr(path[depth].p_bh);\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\t/* find where to start removing */\n\tex = EXT_LAST_EXTENT(eh);\n\n\tex_ee_block = le32_to_cpu(ex->ee_block);\n\tex_ee_len = ext4_ext_get_actual_len(ex);\n\n\ttrace_ext4_ext_rm_leaf(inode, start, ex, *partial_cluster);\n\n\twhile (ex >= EXT_FIRST_EXTENT(eh) &&\n\t\t\tex_ee_block + ex_ee_len > start) {\n\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\telse\n\t\t\tuninitialized = 0;\n\n\t\text_debug(\"remove ext %u:[%d]%d\\n\", ex_ee_block,\n\t\t\t uninitialized, ex_ee_len);\n\t\tpath[depth].p_ext = ex;\n\n\t\ta = ex_ee_block > start ? ex_ee_block : start;\n\t\tb = ex_ee_block+ex_ee_len - 1 < end ?\n\t\t\tex_ee_block+ex_ee_len - 1 : end;\n\n\t\text_debug(\"  border %u:%u\\n\", a, b);\n\n\t\t/* If this extent is beyond the end of the hole, skip it */\n\t\tif (end < ex_ee_block) {\n\t\t\tex--;\n\t\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t\t\tcontinue;\n\t\t} else if (b != ex_ee_block + ex_ee_len - 1) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"can not handle truncate %u:%u \"\n\t\t\t\t\t \"on extent %u:%u\",\n\t\t\t\t\t start, end, ex_ee_block,\n\t\t\t\t\t ex_ee_block + ex_ee_len - 1);\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t} else if (a != ex_ee_block) {\n\t\t\t/* remove tail of the extent */\n\t\t\tnum = a - ex_ee_block;\n\t\t} else {\n\t\t\t/* remove whole extent: excellent! */\n\t\t\tnum = 0;\n\t\t}\n\t\t/*\n\t\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t\t * descriptor) for each block group; assume two block\n\t\t * groups plus ex_ee_len/blocks_per_block_group for\n\t\t * the worst case\n\t\t */\n\t\tcredits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));\n\t\tif (ex == EXT_FIRST_EXTENT(eh)) {\n\t\t\tcorrect_index = 1;\n\t\t\tcredits += (ext_depth(inode)) + 1;\n\t\t}\n\t\tcredits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);\n\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_remove_blocks(handle, inode, ex, partial_cluster,\n\t\t\t\t\t a, b);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (num == 0)\n\t\t\t/* this extent is removed; mark slot entirely unused */\n\t\t\text4_ext_store_pblock(ex, 0);\n\n\t\tex->ee_len = cpu_to_le16(num);\n\t\t/*\n\t\t * Do not mark uninitialized if all the blocks in the\n\t\t * extent have been removed.\n\t\t */\n\t\tif (uninitialized && num)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\t/*\n\t\t * If the extent was completely released,\n\t\t * we need to remove it from the leaf\n\t\t */\n\t\tif (num == 0) {\n\t\t\tif (end != EXT_MAX_BLOCKS - 1) {\n\t\t\t\t/*\n\t\t\t\t * For hole punching, we need to scoot all the\n\t\t\t\t * extents up when an extent is removed so that\n\t\t\t\t * we dont have blank extents in the middle\n\t\t\t\t */\n\t\t\t\tmemmove(ex, ex+1, (EXT_LAST_EXTENT(eh) - ex) *\n\t\t\t\t\tsizeof(struct ext4_extent));\n\n\t\t\t\t/* Now get rid of the one at the end */\n\t\t\t\tmemset(EXT_LAST_EXTENT(eh), 0,\n\t\t\t\t\tsizeof(struct ext4_extent));\n\t\t\t}\n\t\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\t} else\n\t\t\t*partial_cluster = 0;\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\text_debug(\"new extent: %u:%u:%llu\\n\", ex_ee_block, num,\n\t\t\t\text4_ext_pblock(ex));\n\t\tex--;\n\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t}\n\n\tif (correct_index && eh->eh_entries)\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\n\t/*\n\t * If there is still a entry in the leaf node, check to see if\n\t * it references the partial cluster.  This is the only place\n\t * where it could; if it doesn't, we can free the cluster.\n\t */\n\tif (*partial_cluster && ex >= EXT_FIRST_EXTENT(eh) &&\n\t    (EXT4_B2C(sbi, ext4_ext_pblock(ex) + ex_ee_len - 1) !=\n\t     *partial_cluster)) {\n\t\tint flags = EXT4_FREE_BLOCKS_FORGET;\n\n\t\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\t\tflags |= EXT4_FREE_BLOCKS_METADATA;\n\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(sbi, *partial_cluster),\n\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\t*partial_cluster = 0;\n\t}\n\n\t/* if this leaf is free, then we should\n\t * remove it from index block above */\n\tif (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)\n\t\terr = ext4_ext_rm_idx(handle, inode, path + depth);\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_more_to_rm:\n * returns 1 if current index has to be freed (even partial)\n */\nstatic int\next4_ext_more_to_rm(struct ext4_ext_path *path)\n{\n\tBUG_ON(path->p_idx == NULL);\n\n\tif (path->p_idx < EXT_FIRST_INDEX(path->p_hdr))\n\t\treturn 0;\n\n\t/*\n\t * if truncate on deeper level happened, it wasn't partial,\n\t * so we have to consider current index for truncation\n\t */\n\tif (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,\n\t\t\t\t ext4_lblk_t end)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tint depth = ext_depth(inode);\n\tstruct ext4_ext_path *path = NULL;\n\text4_fsblk_t partial_cluster = 0;\n\thandle_t *handle;\n\tint i = 0, err = 0;\n\n\text_debug(\"truncate since %u to %u\\n\", start, end);\n\n\t/* probably first extent we're gonna free will be last in block */\n\thandle = ext4_journal_start(inode, depth + 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\nagain:\n\text4_ext_invalidate_cache(inode);\n\n\ttrace_ext4_ext_remove_space(inode, start, depth);\n\n\t/*\n\t * Check if we are removing extents inside the extent tree. If that\n\t * is the case, we are going to punch a hole inside the extent tree\n\t * so we have to check whether we need to split the extent covering\n\t * the last block to remove so we can easily remove the part of it\n\t * in ext4_ext_rm_leaf().\n\t */\n\tif (end < EXT_MAX_BLOCKS - 1) {\n\t\tstruct ext4_extent *ex;\n\t\text4_lblk_t ee_block;\n\n\t\t/* find extent for this block */\n\t\tpath = ext4_ext_find_extent(inode, end, NULL);\n\t\tif (IS_ERR(path)) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn PTR_ERR(path);\n\t\t}\n\t\tdepth = ext_depth(inode);\n\t\t/* Leaf not may not exist only if inode has no blocks at all */\n\t\tex = path[depth].p_ext;\n\t\tif (!ex) {\n\t\t\tif (depth) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"path[%d].p_hdr == NULL\",\n\t\t\t\t\t\t depth);\n\t\t\t\terr = -EIO;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\tee_block = le32_to_cpu(ex->ee_block);\n\n\t\t/*\n\t\t * See if the last block is inside the extent, if so split\n\t\t * the extent at 'end' block so we can easily remove the\n\t\t * tail of the first part of the split extent in\n\t\t * ext4_ext_rm_leaf().\n\t\t */\n\t\tif (end >= ee_block &&\n\t\t    end < ee_block + ext4_ext_get_actual_len(ex) - 1) {\n\t\t\tint split_flag = 0;\n\n\t\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\t\tsplit_flag = EXT4_EXT_MARK_UNINIT1 |\n\t\t\t\t\t     EXT4_EXT_MARK_UNINIT2;\n\n\t\t\t/*\n\t\t\t * Split the extent in two so that 'end' is the last\n\t\t\t * block in the first new extent\n\t\t\t */\n\t\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\t\t\tend + 1, split_flag,\n\t\t\t\t\t\tEXT4_GET_BLOCKS_PRE_IO |\n\t\t\t\t\t\tEXT4_GET_BLOCKS_PUNCH_OUT_EXT);\n\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\t/*\n\t * We start scanning from right side, freeing all the blocks\n\t * after i_size and walking into the tree depth-wise.\n\t */\n\tdepth = ext_depth(inode);\n\tif (path) {\n\t\tint k = i = depth;\n\t\twhile (--k > 0)\n\t\t\tpath[k].p_block =\n\t\t\t\tle16_to_cpu(path[k].p_hdr->eh_entries)+1;\n\t} else {\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 1),\n\t\t\t       GFP_NOFS);\n\t\tif (path == NULL) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tpath[0].p_depth = depth;\n\t\tpath[0].p_hdr = ext_inode_hdr(inode);\n\t\ti = 0;\n\n\t\tif (ext4_ext_check(inode, path[0].p_hdr, depth)) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\terr = 0;\n\n\twhile (i >= 0 && err == 0) {\n\t\tif (i == depth) {\n\t\t\t/* this is leaf block */\n\t\t\terr = ext4_ext_rm_leaf(handle, inode, path,\n\t\t\t\t\t       &partial_cluster, start,\n\t\t\t\t\t       end);\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* this is index block */\n\t\tif (!path[i].p_hdr) {\n\t\t\text_debug(\"initialize header\\n\");\n\t\t\tpath[i].p_hdr = ext_block_hdr(path[i].p_bh);\n\t\t}\n\n\t\tif (!path[i].p_idx) {\n\t\t\t/* this level hasn't been touched yet */\n\t\t\tpath[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;\n\t\t\text_debug(\"init index ptr: hdr 0x%p, num %d\\n\",\n\t\t\t\t  path[i].p_hdr,\n\t\t\t\t  le16_to_cpu(path[i].p_hdr->eh_entries));\n\t\t} else {\n\t\t\t/* we were already here, see at next index */\n\t\t\tpath[i].p_idx--;\n\t\t}\n\n\t\text_debug(\"level %d - index, first 0x%p, cur 0x%p\\n\",\n\t\t\t\ti, EXT_FIRST_INDEX(path[i].p_hdr),\n\t\t\t\tpath[i].p_idx);\n\t\tif (ext4_ext_more_to_rm(path + i)) {\n\t\t\tstruct buffer_head *bh;\n\t\t\t/* go to the next level */\n\t\t\text_debug(\"move to level %d (block %llu)\\n\",\n\t\t\t\t  i + 1, ext4_idx_pblock(path[i].p_idx));\n\t\t\tmemset(path + i + 1, 0, sizeof(*path));\n\t\t\tbh = sb_bread(sb, ext4_idx_pblock(path[i].p_idx));\n\t\t\tif (!bh) {\n\t\t\t\t/* should we reset i_size? */\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (WARN_ON(i + 1 > depth)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ext4_ext_check_block(inode, ext_block_hdr(bh),\n\t\t\t\t\t\t\tdepth - i - 1, bh)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpath[i + 1].p_bh = bh;\n\n\t\t\t/* save actual number of indexes since this\n\t\t\t * number is changed at the next iteration */\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);\n\t\t\ti++;\n\t\t} else {\n\t\t\t/* we finished processing this index, go up */\n\t\t\tif (path[i].p_hdr->eh_entries == 0 && i > 0) {\n\t\t\t\t/* index is empty, remove it;\n\t\t\t\t * handle must be already prepared by the\n\t\t\t\t * truncatei_leaf() */\n\t\t\t\terr = ext4_ext_rm_idx(handle, inode, path + i);\n\t\t\t}\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\text_debug(\"return to level %d\\n\", i);\n\t\t}\n\t}\n\n\ttrace_ext4_ext_remove_space_done(inode, start, depth, partial_cluster,\n\t\t\tpath->p_hdr->eh_entries);\n\n\t/* If we still have something in the partial cluster and we have removed\n\t * even the first extent, then we should free the blocks in the partial\n\t * cluster as well. */\n\tif (partial_cluster && path->p_hdr->eh_entries == 0) {\n\t\tint flags = EXT4_FREE_BLOCKS_FORGET;\n\n\t\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\t\tflags |= EXT4_FREE_BLOCKS_METADATA;\n\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(EXT4_SB(sb), partial_cluster),\n\t\t\t\t EXT4_SB(sb)->s_cluster_ratio, flags);\n\t\tpartial_cluster = 0;\n\t}\n\n\t/* TODO: flexible tree reduction should be here */\n\tif (path->p_hdr->eh_entries == 0) {\n\t\t/*\n\t\t * truncate to zero freed all the tree,\n\t\t * so we need to correct eh_depth\n\t\t */\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err == 0) {\n\t\t\text_inode_hdr(inode)->eh_depth = 0;\n\t\t\text_inode_hdr(inode)->eh_max =\n\t\t\t\tcpu_to_le16(ext4_ext_space_root(inode, 0));\n\t\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\t}\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tif (err == -EAGAIN) {\n\t\tpath = NULL;\n\t\tgoto again;\n\t}\n\text4_journal_stop(handle);\n\n\treturn err;\n}\n\n/*\n * called at mount time\n */\nvoid ext4_ext_init(struct super_block *sb)\n{\n\t/*\n\t * possible initialization would be here\n\t */\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) {\n#if defined(AGGRESSIVE_TEST) || defined(CHECK_BINSEARCH) || defined(EXTENTS_STATS)\n\t\tprintk(KERN_INFO \"EXT4-fs: file extents enabled\"\n#ifdef AGGRESSIVE_TEST\n\t\t       \", aggressive tests\"\n#endif\n#ifdef CHECK_BINSEARCH\n\t\t       \", check binsearch\"\n#endif\n#ifdef EXTENTS_STATS\n\t\t       \", stats\"\n#endif\n\t\t       \"\\n\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tspin_lock_init(&EXT4_SB(sb)->s_ext_stats_lock);\n\t\tEXT4_SB(sb)->s_ext_min = 1 << 30;\n\t\tEXT4_SB(sb)->s_ext_max = 0;\n#endif\n\t}\n}\n\n/*\n * called at umount time\n */\nvoid ext4_ext_release(struct super_block *sb)\n{\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS))\n\t\treturn;\n\n#ifdef EXTENTS_STATS\n\tif (EXT4_SB(sb)->s_ext_blocks && EXT4_SB(sb)->s_ext_extents) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\tprintk(KERN_ERR \"EXT4-fs: %lu blocks in %lu extents (%lu ave)\\n\",\n\t\t\tsbi->s_ext_blocks, sbi->s_ext_extents,\n\t\t\tsbi->s_ext_blocks / sbi->s_ext_extents);\n\t\tprintk(KERN_ERR \"EXT4-fs: extents: %lu min, %lu max, max depth %lu\\n\",\n\t\t\tsbi->s_ext_min, sbi->s_ext_max, sbi->s_depth_max);\n\t}\n#endif\n}\n\n/* FIXME!! we need to try to merge to left or right after zero-out  */\nstatic int ext4_ext_zeroout(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\tint ret;\n\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\n\tret = sb_issue_zeroout(inode->i_sb, ee_pblock, ee_len, GFP_NOFS);\n\tif (ret > 0)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * ext4_split_extent_at() splits an extent at given block.\n *\n * @handle: the journal handle\n * @inode: the file inode\n * @path: the path to the extent\n * @split: the logical block where the extent is splitted.\n * @split_flags: indicates if the extent could be zeroout if split fails, and\n *\t\t the states(init or uninit) of new extents.\n * @flags: flags used to insert new extent to extent tree.\n *\n *\n * Splits extent [a, b] into two extents [a, @split) and [@split, b], states\n * of which are deterimined by split_flag.\n *\n * There are two cases:\n *  a> the extent are splitted into two extent.\n *  b> split is not needed, and just mark the extent.\n *\n * return 0 on success.\n */\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==\n\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT1)\n\t\text4_ext_mark_uninitialized(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\text4_ext_mark_uninitialized(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {\n\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1)\n\t\t\t\terr = ext4_ext_zeroout(inode, ex2);\n\t\t\telse\n\t\t\t\terr = ext4_ext_zeroout(inode, ex);\n\t\t} else\n\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}\n\n/*\n * ext4_split_extents() splits an extent and mark extent which is covered\n * by @map as split_flags indicates\n *\n * It may result in splitting the extent into multiple extents (upto three)\n * There are three possibilities:\n *   a> There is no split required\n *   b> Splits in two extents: Split is happening at either end of the extent\n *   c> Splits in three extents: Somone is splitting in middle of the extent\n *\n */\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint uninitialized;\n\tint split_flag1, flags1;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tuninitialized = ext4_ext_is_uninitialized(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNINIT2;\n\t\tif (split_flag & EXT4_EXT_DATA_VALID2)\n\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_drop_refs(path);\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t\t\t    EXT4_EXT_DATA_VALID2);\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1;\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : map->m_len;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() if someone tries to write\n * to an uninitialized extent. It may result in splitting the uninitialized\n * extent into multiple extents (up to three - one initialized and two\n * uninitialized).\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be initialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * Pre-conditions:\n *  - The extent pointed to by 'path' is uninitialized.\n *  - The extent pointed to by 'path' contains a superset\n *    of the logical span [map->m_lblk, map->m_lblk + map->m_len).\n *\n * Post-conditions on success:\n *  - the returned value is the number of blocks beyond map->l_lblk\n *    that are allocated and initialized.\n *    It is guaranteed to be >= map->m_len.\n */\nstatic int ext4_ext_convert_to_initialized(handle_t *handle,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_map_blocks *map,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\tstruct ext4_sb_info *sbi;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_map_blocks split_map;\n\tstruct ext4_extent zero_ex;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block, eof_block;\n\tunsigned int ee_len, depth;\n\tint allocated, max_zeroout = 0;\n\tint err = 0;\n\tint split_flag = 0;\n\n\text_debug(\"ext4_ext_convert_to_initialized: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\tsbi = EXT4_SB(inode->i_sb);\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tallocated = ee_len - (map->m_lblk - ee_block);\n\n\ttrace_ext4_ext_convert_to_initialized_enter(inode, map, ex);\n\n\t/* Pre-conditions */\n\tBUG_ON(!ext4_ext_is_uninitialized(ex));\n\tBUG_ON(!in_range(map->m_lblk, ee_block, ee_len));\n\n\t/*\n\t * Attempt to transfer newly initialized blocks from the currently\n\t * uninitialized extent to its left neighbor. This is much cheaper\n\t * than an insertion followed by a merge as those involve costly\n\t * memmove() calls. This is the common case in steady state for\n\t * workloads doing fallocate(FALLOC_FL_KEEP_SIZE) followed by append\n\t * writes.\n\t *\n\t * Limitations of the current logic:\n\t *  - L1: we only deal with writes at the start of the extent.\n\t *    The approach could be extended to writes at the end\n\t *    of the extent but this scenario was deemed less common.\n\t *  - L2: we do not deal with writes covering the whole extent.\n\t *    This would require removing the extent if the transfer\n\t *    is possible.\n\t *  - L3: we only attempt to merge with an extent stored in the\n\t *    same extent tree node.\n\t */\n\tif ((map->m_lblk == ee_block) &&\t/*L1*/\n\t\t(map->m_len < ee_len) &&\t/*L2*/\n\t\t(ex > EXT_FIRST_EXTENT(eh))) {\t/*L3*/\n\t\tstruct ext4_extent *prev_ex;\n\t\text4_lblk_t prev_lblk;\n\t\text4_fsblk_t prev_pblk, ee_pblk;\n\t\tunsigned int prev_len, write_len;\n\n\t\tprev_ex = ex - 1;\n\t\tprev_lblk = le32_to_cpu(prev_ex->ee_block);\n\t\tprev_len = ext4_ext_get_actual_len(prev_ex);\n\t\tprev_pblk = ext4_ext_pblock(prev_ex);\n\t\tee_pblk = ext4_ext_pblock(ex);\n\t\twrite_len = map->m_len;\n\n\t\t/*\n\t\t * A transfer of blocks from 'ex' to 'prev_ex' is allowed\n\t\t * upon those conditions:\n\t\t * - C1: prev_ex is initialized,\n\t\t * - C2: prev_ex is logically abutting ex,\n\t\t * - C3: prev_ex is physically abutting ex,\n\t\t * - C4: prev_ex can receive the additional blocks without\n\t\t *   overflowing the (initialized) length limit.\n\t\t */\n\t\tif ((!ext4_ext_is_uninitialized(prev_ex)) &&\t\t/*C1*/\n\t\t\t((prev_lblk + prev_len) == ee_block) &&\t\t/*C2*/\n\t\t\t((prev_pblk + prev_len) == ee_pblk) &&\t\t/*C3*/\n\t\t\t(prev_len < (EXT_INIT_MAX_LEN - write_len))) {\t/*C4*/\n\t\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\ttrace_ext4_ext_convert_to_initialized_fastpath(inode,\n\t\t\t\tmap, ex, prev_ex);\n\n\t\t\t/* Shift the start of ex by 'write_len' blocks */\n\t\t\tex->ee_block = cpu_to_le32(ee_block + write_len);\n\t\t\text4_ext_store_pblock(ex, ee_pblk + write_len);\n\t\t\tex->ee_len = cpu_to_le16(ee_len - write_len);\n\t\t\text4_ext_mark_uninitialized(ex); /* Restore the flag */\n\n\t\t\t/* Extend prev_ex by 'write_len' blocks */\n\t\t\tprev_ex->ee_len = cpu_to_le16(prev_len + write_len);\n\n\t\t\t/* Mark the block containing both extents as dirty */\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\n\t\t\t/* Update path to point to the right extent */\n\t\t\tpath[depth].p_ext = prev_ex;\n\n\t\t\t/* Result: number of initialized blocks past m_lblk */\n\t\t\tallocated = write_len;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tWARN_ON(map->m_lblk < ee_block);\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\n\tif (EXT4_EXT_MAY_ZEROOUT & split_flag)\n\t\tmax_zeroout = sbi->s_extent_max_zeroout_kb >>\n\t\t\tinode->i_sb->s_blocksize_bits;\n\n\t/* If extent is less than s_max_zeroout_kb, zeroout directly */\n\tif (max_zeroout && (ee_len <= max_zeroout)) {\n\t\terr = ext4_ext_zeroout(inode, ex);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\t\text4_ext_mark_initialized(ex);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * four cases:\n\t * 1. split the extent into three extents.\n\t * 2. split the extent into two extents, zeroout the first half.\n\t * 3. split the extent into two extents, zeroout the second half.\n\t * 4. split the extent into two extents with out zeroout.\n\t */\n\tsplit_map.m_lblk = map->m_lblk;\n\tsplit_map.m_len = map->m_len;\n\n\tif (max_zeroout && (allocated > map->m_len)) {\n\t\tif (allocated <= max_zeroout) {\n\t\t\t/* case 3 */\n\t\t\tzero_ex.ee_block =\n\t\t\t\t\t cpu_to_le32(map->m_lblk);\n\t\t\tzero_ex.ee_len = cpu_to_le16(allocated);\n\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\text4_ext_pblock(ex) + map->m_lblk - ee_block);\n\t\t\terr = ext4_ext_zeroout(inode, &zero_ex);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tsplit_map.m_lblk = map->m_lblk;\n\t\t\tsplit_map.m_len = allocated;\n\t\t} else if (map->m_lblk - ee_block + map->m_len < max_zeroout) {\n\t\t\t/* case 2 */\n\t\t\tif (map->m_lblk != ee_block) {\n\t\t\t\tzero_ex.ee_block = ex->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(map->m_lblk -\n\t\t\t\t\t\t\tee_block);\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t\terr = ext4_ext_zeroout(inode, &zero_ex);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tsplit_map.m_lblk = ee_block;\n\t\t\tsplit_map.m_len = map->m_lblk - ee_block + map->m_len;\n\t\t\tallocated = map->m_len;\n\t\t}\n\t}\n\n\tallocated = ext4_split_extent(handle, inode, path,\n\t\t\t\t      &split_map, split_flag, 0);\n\tif (allocated < 0)\n\t\terr = allocated;\n\nout:\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() from\n * ext4_get_blocks_dio_write() when DIO to write\n * to an uninitialized extent.\n *\n * Writing to an uninitialized extent may result in splitting the uninitialized\n * extent into multiple initialized/uninitialized extents (up to three)\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be uninitialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * One of more index blocks maybe needed if the extent tree grow after\n * the uninitialized extent split. To prevent ENOSPC occur at the IO\n * complete, we need to split the uninitialized extent before DIO submit\n * the IO. The uninitialized extent called at this time will be split\n * into three uninitialized extent(at most). After IO complete, the part\n * being filled will be convert to initialized by the end_io callback function\n * via ext4_convert_unwritten_extents().\n *\n * Returns the size of uninitialized extent to be written on success.\n */\nstatic int ext4_split_unwritten_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tint flags)\n{\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"ext4_split_unwritten_extents: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\tsplit_flag |= EXT4_EXT_MARK_UNINIT2;\n\tif (flags & EXT4_GET_BLOCKS_CONVERT)\n\t\tsplit_flag |= EXT4_EXT_DATA_VALID2;\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, path, map, split_flag, flags);\n}\n\nstatic int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\tunsigned int ee_len;\n\tint depth;\n\tint err = 0;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t  (unsigned long long)ee_block, ee_len);\n\n\t/* If extent is larger than requested then split is required */\n\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n\t\terr = ext4_split_unwritten_extents(handle, inode, map, path,\n\t\t\t\t\t\t   EXT4_GET_BLOCKS_CONVERT);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\t\tdepth = ext_depth(inode);\n\t\tex = path[depth].p_ext;\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}\n\nstatic void unmap_underlying_metadata_blocks(struct block_device *bdev,\n\t\t\tsector_t block, int count)\n{\n\tint i;\n\tfor (i = 0; i < count; i++)\n                unmap_underlying_metadata(bdev, block + i);\n}\n\n/*\n * Handle EOFBLOCKS_FL flag, clearing it if necessary\n */\nstatic int check_eofblocks_fl(handle_t *handle, struct inode *inode,\n\t\t\t      ext4_lblk_t lblk,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      unsigned int len)\n{\n\tint i, depth;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *last_ex;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS))\n\t\treturn 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\n\t/*\n\t * We're going to remove EOFBLOCKS_FL entirely in future so we\n\t * do not care for this case anymore. Simply remove the flag\n\t * if there are no extents.\n\t */\n\tif (unlikely(!eh->eh_entries))\n\t\tgoto out;\n\tlast_ex = EXT_LAST_EXTENT(eh);\n\t/*\n\t * We should clear the EOFBLOCKS_FL flag if we are writing the\n\t * last block in the last extent in the file.  We test this by\n\t * first checking to see if the caller to\n\t * ext4_ext_get_blocks() was interested in the last block (or\n\t * a block beyond the last block) in the current extent.  If\n\t * this turns out to be false, we can bail out from this\n\t * function immediately.\n\t */\n\tif (lblk + len < le32_to_cpu(last_ex->ee_block) +\n\t    ext4_ext_get_actual_len(last_ex))\n\t\treturn 0;\n\t/*\n\t * If the caller does appear to be planning to write at or\n\t * beyond the end of the current extent, we then test to see\n\t * if the current extent is the last extent in the file, by\n\t * checking to make sure it was reached via the rightmost node\n\t * at each level of the tree.\n\t */\n\tfor (i = depth-1; i >= 0; i--)\n\t\tif (path[i].p_idx != EXT_LAST_INDEX(path[i].p_hdr))\n\t\t\treturn 0;\nout:\n\text4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\treturn ext4_mark_inode_dirty(handle, inode);\n}\n\n/**\n * ext4_find_delalloc_range: find delayed allocated block in the given range.\n *\n * Goes through the buffer heads in the range [lblk_start, lblk_end] and returns\n * whether there are any buffers marked for delayed allocation. It returns '1'\n * on the first delalloc'ed buffer head found. If no buffer head in the given\n * range is marked for delalloc, it returns 0.\n * lblk_start should always be <= lblk_end.\n * search_hint_reverse is to indicate that searching in reverse from lblk_end to\n * lblk_start might be more efficient (i.e., we will likely hit the delalloc'ed\n * block sooner). This is useful when blocks are truncated sequentially from\n * lblk_start towards lblk_end.\n */\nstatic int ext4_find_delalloc_range(struct inode *inode,\n\t\t\t\t    ext4_lblk_t lblk_start,\n\t\t\t\t    ext4_lblk_t lblk_end,\n\t\t\t\t    int search_hint_reverse)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct buffer_head *head, *bh = NULL;\n\tstruct page *page;\n\text4_lblk_t i, pg_lblk;\n\tpgoff_t index;\n\n\tif (!test_opt(inode->i_sb, DELALLOC))\n\t\treturn 0;\n\n\t/* reverse search wont work if fs block size is less than page size */\n\tif (inode->i_blkbits < PAGE_CACHE_SHIFT)\n\t\tsearch_hint_reverse = 0;\n\n\tif (search_hint_reverse)\n\t\ti = lblk_end;\n\telse\n\t\ti = lblk_start;\n\n\tindex = i >> (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\n\twhile ((i >= lblk_start) && (i <= lblk_end)) {\n\t\tpage = find_get_page(mapping, index);\n\t\tif (!page)\n\t\t\tgoto nextpage;\n\n\t\tif (!page_has_buffers(page))\n\t\t\tgoto nextpage;\n\n\t\thead = page_buffers(page);\n\t\tif (!head)\n\t\t\tgoto nextpage;\n\n\t\tbh = head;\n\t\tpg_lblk = index << (PAGE_CACHE_SHIFT -\n\t\t\t\t\t\tinode->i_blkbits);\n\t\tdo {\n\t\t\tif (unlikely(pg_lblk < lblk_start)) {\n\t\t\t\t/*\n\t\t\t\t * This is possible when fs block size is less\n\t\t\t\t * than page size and our cluster starts/ends in\n\t\t\t\t * middle of the page. So we need to skip the\n\t\t\t\t * initial few blocks till we reach the 'lblk'\n\t\t\t\t */\n\t\t\t\tpg_lblk++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* Check if the buffer is delayed allocated and that it\n\t\t\t * is not yet mapped. (when da-buffers are mapped during\n\t\t\t * their writeout, their da_mapped bit is set.)\n\t\t\t */\n\t\t\tif (buffer_delay(bh) && !buffer_da_mapped(bh)) {\n\t\t\t\tpage_cache_release(page);\n\t\t\t\ttrace_ext4_find_delalloc_range(inode,\n\t\t\t\t\t\tlblk_start, lblk_end,\n\t\t\t\t\t\tsearch_hint_reverse,\n\t\t\t\t\t\t1, i);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (search_hint_reverse)\n\t\t\t\ti--;\n\t\t\telse\n\t\t\t\ti++;\n\t\t} while ((i >= lblk_start) && (i <= lblk_end) &&\n\t\t\t\t((bh = bh->b_this_page) != head));\nnextpage:\n\t\tif (page)\n\t\t\tpage_cache_release(page);\n\t\t/*\n\t\t * Move to next page. 'i' will be the first lblk in the next\n\t\t * page.\n\t\t */\n\t\tif (search_hint_reverse)\n\t\t\tindex--;\n\t\telse\n\t\t\tindex++;\n\t\ti = index << (PAGE_CACHE_SHIFT - inode->i_blkbits);\n\t}\n\n\ttrace_ext4_find_delalloc_range(inode, lblk_start, lblk_end,\n\t\t\t\t\tsearch_hint_reverse, 0, 0);\n\treturn 0;\n}\n\nint ext4_find_delalloc_cluster(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t       int search_hint_reverse)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t lblk_start, lblk_end;\n\tlblk_start = lblk & (~(sbi->s_cluster_ratio - 1));\n\tlblk_end = lblk_start + sbi->s_cluster_ratio - 1;\n\n\treturn ext4_find_delalloc_range(inode, lblk_start, lblk_end,\n\t\t\t\t\tsearch_hint_reverse);\n}\n\n/**\n * Determines how many complete clusters (out of those specified by the 'map')\n * are under delalloc and were reserved quota for.\n * This function is called when we are writing out the blocks that were\n * originally written with their allocation delayed, but then the space was\n * allocated using fallocate() before the delayed allocation could be resolved.\n * The cases to look for are:\n * ('=' indicated delayed allocated blocks\n *  '-' indicates non-delayed allocated blocks)\n * (a) partial clusters towards beginning and/or end outside of allocated range\n *     are not delalloc'ed.\n *\tEx:\n *\t|----c---=|====c====|====c====|===-c----|\n *\t         |++++++ allocated ++++++|\n *\t==> 4 complete clusters in above example\n *\n * (b) partial cluster (outside of allocated range) towards either end is\n *     marked for delayed allocation. In this case, we will exclude that\n *     cluster.\n *\tEx:\n *\t|----====c========|========c========|\n *\t     |++++++ allocated ++++++|\n *\t==> 1 complete clusters in above example\n *\n *\tEx:\n *\t|================c================|\n *            |++++++ allocated ++++++|\n *\t==> 0 complete clusters in above example\n *\n * The ext4_da_update_reserve_space will be called only if we\n * determine here that there were some \"entire\" clusters that span\n * this 'allocated' range.\n * In the non-bigalloc case, this function will just end up returning num_blks\n * without ever calling ext4_find_delalloc_range.\n */\nstatic unsigned int\nget_reserved_cluster_alloc(struct inode *inode, ext4_lblk_t lblk_start,\n\t\t\t   unsigned int num_blks)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t alloc_cluster_start, alloc_cluster_end;\n\text4_lblk_t lblk_from, lblk_to, c_offset;\n\tunsigned int allocated_clusters = 0;\n\n\talloc_cluster_start = EXT4_B2C(sbi, lblk_start);\n\talloc_cluster_end = EXT4_B2C(sbi, lblk_start + num_blks - 1);\n\n\t/* max possible clusters for this allocation */\n\tallocated_clusters = alloc_cluster_end - alloc_cluster_start + 1;\n\n\ttrace_ext4_get_reserved_cluster_alloc(inode, lblk_start, num_blks);\n\n\t/* Check towards left side */\n\tc_offset = lblk_start & (sbi->s_cluster_ratio - 1);\n\tif (c_offset) {\n\t\tlblk_from = lblk_start & (~(sbi->s_cluster_ratio - 1));\n\t\tlblk_to = lblk_from + c_offset - 1;\n\n\t\tif (ext4_find_delalloc_range(inode, lblk_from, lblk_to, 0))\n\t\t\tallocated_clusters--;\n\t}\n\n\t/* Now check towards right. */\n\tc_offset = (lblk_start + num_blks) & (sbi->s_cluster_ratio - 1);\n\tif (allocated_clusters && c_offset) {\n\t\tlblk_from = lblk_start + num_blks;\n\t\tlblk_to = lblk_from + (sbi->s_cluster_ratio - c_offset) - 1;\n\n\t\tif (ext4_find_delalloc_range(inode, lblk_from, lblk_to, 0))\n\t\t\tallocated_clusters--;\n\t}\n\n\treturn allocated_clusters;\n}\n\nstatic int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\ttrace_ext4_ext_handle_uninitialized_extents(inode, map, allocated,\n\t\t\t\t\t\t    newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle, inode, map,\n\t\t\t\t\t\t   path, flags);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, path);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\tmap->m_lblk, map->m_len);\n\t\tif (reserved_clusters)\n\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\t     reserved_clusters,\n\t\t\t\t\t\t     0);\n\t}\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}\n\n/*\n * get_implied_cluster_alloc - check to see if the requested\n * allocation (in the map structure) overlaps with a cluster already\n * allocated in an extent.\n *\t@sb\tThe filesystem superblock structure\n *\t@map\tThe requested lblk->pblk mapping\n *\t@ex\tThe extent structure which might contain an implied\n *\t\t\tcluster allocation\n *\n * This function is called by ext4_ext_map_blocks() after we failed to\n * find blocks that were already in the inode's extent tree.  Hence,\n * we know that the beginning of the requested region cannot overlap\n * the extent from the inode's extent tree.  There are three cases we\n * want to catch.  The first is this case:\n *\n *\t\t |--- cluster # N--|\n *    |--- extent ---|\t|---- requested region ---|\n *\t\t\t|==========|\n *\n * The second case that we need to test for is this one:\n *\n *   |--------- cluster # N ----------------|\n *\t   |--- requested region --|   |------- extent ----|\n *\t   |=======================|\n *\n * The third case is when the requested region lies between two extents\n * within the same cluster:\n *          |------------- cluster # N-------------|\n * |----- ex -----|                  |---- ex_right ----|\n *                  |------ requested region ------|\n *                  |================|\n *\n * In each of the above cases, we need to set the map->m_pblk and\n * map->m_len so it corresponds to the return the extent labelled as\n * \"|====|\" from cluster #N, since it is already in use for data in\n * cluster EXT4_B2C(sbi, map->m_lblk).\tWe will then return 1 to\n * signal to ext4_ext_map_blocks() that map->m_pblk should be treated\n * as a new \"allocated\" block region.  Otherwise, we will return 0 and\n * ext4_ext_map_blocks() will then allocate one or more new clusters\n * by calling ext4_mb_new_blocks().\n */\nstatic int get_implied_cluster_alloc(struct super_block *sb,\n\t\t\t\t     struct ext4_map_blocks *map,\n\t\t\t\t     struct ext4_extent *ex,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_lblk_t c_offset = map->m_lblk & (sbi->s_cluster_ratio-1);\n\text4_lblk_t ex_cluster_start, ex_cluster_end;\n\text4_lblk_t rr_cluster_start;\n\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\tunsigned short ee_len = ext4_ext_get_actual_len(ex);\n\n\t/* The extent passed in that we are trying to match */\n\tex_cluster_start = EXT4_B2C(sbi, ee_block);\n\tex_cluster_end = EXT4_B2C(sbi, ee_block + ee_len - 1);\n\n\t/* The requested region passed into ext4_map_blocks() */\n\trr_cluster_start = EXT4_B2C(sbi, map->m_lblk);\n\n\tif ((rr_cluster_start == ex_cluster_end) ||\n\t    (rr_cluster_start == ex_cluster_start)) {\n\t\tif (rr_cluster_start == ex_cluster_end)\n\t\t\tee_start += ee_len - 1;\n\t\tmap->m_pblk = (ee_start & ~(sbi->s_cluster_ratio - 1)) +\n\t\t\tc_offset;\n\t\tmap->m_len = min(map->m_len,\n\t\t\t\t (unsigned) sbi->s_cluster_ratio - c_offset);\n\t\t/*\n\t\t * Check for and handle this case:\n\t\t *\n\t\t *   |--------- cluster # N-------------|\n\t\t *\t\t       |------- extent ----|\n\t\t *\t   |--- requested region ---|\n\t\t *\t   |===========|\n\t\t */\n\n\t\tif (map->m_lblk < ee_block)\n\t\t\tmap->m_len = min(map->m_len, ee_block - map->m_lblk);\n\n\t\t/*\n\t\t * Check for the case where there is already another allocated\n\t\t * block to the right of 'ex' but before the end of the cluster.\n\t\t *\n\t\t *          |------------- cluster # N-------------|\n\t\t * |----- ex -----|                  |---- ex_right ----|\n\t\t *                  |------ requested region ------|\n\t\t *                  |================|\n\t\t */\n\t\tif (map->m_lblk > ee_block) {\n\t\t\text4_lblk_t next = ext4_ext_next_allocated_block(path);\n\t\t\tmap->m_len = min(map->m_len, next - map->m_lblk);\n\t\t}\n\n\t\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 1);\n\t\treturn 1;\n\t}\n\n\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 0);\n\treturn 0;\n}\n\n\n/*\n * Block allocation/map/preallocation routine for extents based files\n *\n *\n * Need to be called with\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system block\n * (ie, create is zero). Otherwise down_write(&EXT4_I(inode)->i_data_sem)\n *\n * return > 0, number of of blocks already mapped/allocated\n *          if create == 0 and these are pre-allocated blocks\n *          \tbuffer head is unmapped\n *          otherwise blocks are mapped\n *\n * return = 0, if plain look up failed (blocks have not been allocated)\n *          buffer head is unmapped\n *\n * return < 0, error case.\n */\nint ext4_ext_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map, int flags)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent newex, *ex, *ex2;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_fsblk_t newblock = 0;\n\tint free_on_err = 0, err = 0, depth, ret;\n\tunsigned int allocated = 0, offset = 0;\n\tunsigned int allocated_clusters = 0;\n\tstruct ext4_allocation_request ar;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\text4_lblk_t cluster_offset;\n\tint set_unwritten = 0;\n\n\text_debug(\"blocks %u/%u requested for inode %lu\\n\",\n\t\t  map->m_lblk, map->m_len, inode->i_ino);\n\ttrace_ext4_ext_map_blocks_enter(inode, map->m_lblk, map->m_len, flags);\n\n\t/* check in cache */\n\tif (ext4_ext_in_cache(inode, map->m_lblk, &newex)) {\n\t\tif (!newex.ee_start_lo && !newex.ee_start_hi) {\n\t\t\tif ((sbi->s_cluster_ratio > 1) &&\n\t\t\t    ext4_find_delalloc_cluster(inode, map->m_lblk, 0))\n\t\t\t\tmap->m_flags |= EXT4_MAP_FROM_CLUSTER;\n\n\t\t\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t\t\t/*\n\t\t\t\t * block isn't allocated yet and\n\t\t\t\t * user doesn't want to allocate it\n\t\t\t\t */\n\t\t\t\tgoto out2;\n\t\t\t}\n\t\t\t/* we should allocate requested block */\n\t\t} else {\n\t\t\t/* block is already allocated */\n\t\t\tif (sbi->s_cluster_ratio > 1)\n\t\t\t\tmap->m_flags |= EXT4_MAP_FROM_CLUSTER;\n\t\t\tnewblock = map->m_lblk\n\t\t\t\t   - le32_to_cpu(newex.ee_block)\n\t\t\t\t   + ext4_ext_pblock(&newex);\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ext4_ext_get_actual_len(&newex) -\n\t\t\t\t(map->m_lblk - le32_to_cpu(newex.ee_block));\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* find extent for this block */\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, NULL);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out2;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * consistent leaf must not be empty;\n\t * this situation is possible, though, _during_ tree modification;\n\t * this is why assert can't be put in ext4_ext_find_extent()\n\t */\n\tif (unlikely(path[depth].p_ext == NULL && depth != 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extent address \"\n\t\t\t\t \"lblock: %lu, depth: %d pblock %lld\",\n\t\t\t\t (unsigned long) map->m_lblk, depth,\n\t\t\t\t path[depth].p_block);\n\t\terr = -EIO;\n\t\tgoto out2;\n\t}\n\n\tex = path[depth].p_ext;\n\tif (ex) {\n\t\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\t\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\t\tunsigned short ee_len;\n\n\t\t/*\n\t\t * Uninitialized extents are treated as holes, except that\n\t\t * we split out initialized portions during a write.\n\t\t */\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\n\t\ttrace_ext4_ext_show_extent(inode, ee_block, ee_start, ee_len);\n\n\t\t/* if found extent covers block, simply return it */\n\t\tif (in_range(map->m_lblk, ee_block, ee_len)) {\n\t\t\tnewblock = map->m_lblk - ee_block + ee_start;\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t\t\text_debug(\"%u fit into %u:%d -> %llu\\n\", map->m_lblk,\n\t\t\t\t  ee_block, ee_len, newblock);\n\n\t\t\t/*\n\t\t\t * Do not put uninitialized extent\n\t\t\t * in the cache\n\t\t\t */\n\t\t\tif (!ext4_ext_is_uninitialized(ex)) {\n\t\t\t\text4_ext_put_in_cache(inode, ee_block,\n\t\t\t\t\tee_len, ee_start);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = ext4_ext_handle_uninitialized_extents(\n\t\t\t\thandle, inode, map, path, flags,\n\t\t\t\tallocated, newblock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif ((sbi->s_cluster_ratio > 1) &&\n\t    ext4_find_delalloc_cluster(inode, map->m_lblk, 0))\n\t\tmap->m_flags |= EXT4_MAP_FROM_CLUSTER;\n\n\t/*\n\t * requested block isn't allocated yet;\n\t * we couldn't try to create block if create flag is zero\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * put just found gap into cache to speed up\n\t\t * subsequent requests\n\t\t */\n\t\text4_ext_put_gap_in_cache(inode, path, map->m_lblk);\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * Okay, we need to do block allocation.\n\t */\n\tmap->m_flags &= ~EXT4_MAP_FROM_CLUSTER;\n\tnewex.ee_block = cpu_to_le32(map->m_lblk);\n\tcluster_offset = map->m_lblk & (sbi->s_cluster_ratio-1);\n\n\t/*\n\t * If we are doing bigalloc, check to see if the extent returned\n\t * by ext4_ext_find_extent() implies a cluster we can use.\n\t */\n\tif (cluster_offset && ex &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap->m_flags |= EXT4_MAP_FROM_CLUSTER;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/* find neighbour allocated blocks */\n\tar.lleft = map->m_lblk;\n\terr = ext4_ext_search_left(inode, path, &ar.lleft, &ar.pleft);\n\tif (err)\n\t\tgoto out2;\n\tar.lright = map->m_lblk;\n\tex2 = NULL;\n\terr = ext4_ext_search_right(inode, path, &ar.lright, &ar.pright, &ex2);\n\tif (err)\n\t\tgoto out2;\n\n\t/* Check if the extent after searching to the right implies a\n\t * cluster we can use. */\n\tif ((sbi->s_cluster_ratio > 1) && ex2 &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex2, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap->m_flags |= EXT4_MAP_FROM_CLUSTER;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/*\n\t * See if request is beyond maximum number of blocks we can have in\n\t * a single extent. For an initialized extent this limit is\n\t * EXT_INIT_MAX_LEN and for an uninitialized extent this limit is\n\t * EXT_UNINIT_MAX_LEN.\n\t */\n\tif (map->m_len > EXT_INIT_MAX_LEN &&\n\t    !(flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmap->m_len = EXT_INIT_MAX_LEN;\n\telse if (map->m_len > EXT_UNINIT_MAX_LEN &&\n\t\t (flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmap->m_len = EXT_UNINIT_MAX_LEN;\n\n\t/* Check if we can really insert (m_lblk)::(m_lblk + m_len) extent */\n\tnewex.ee_len = cpu_to_le16(map->m_len);\n\terr = ext4_ext_check_overlap(sbi, inode, &newex, path);\n\tif (err)\n\t\tallocated = ext4_ext_get_actual_len(&newex);\n\telse\n\t\tallocated = map->m_len;\n\n\t/* allocate new block */\n\tar.inode = inode;\n\tar.goal = ext4_ext_find_goal(inode, path, map->m_lblk);\n\tar.logical = map->m_lblk;\n\t/*\n\t * We calculate the offset from the beginning of the cluster\n\t * for the logical block number, since when we allocate a\n\t * physical cluster, the physical block should start at the\n\t * same offset from the beginning of the cluster.  This is\n\t * needed so that future calls to get_implied_cluster_alloc()\n\t * work correctly.\n\t */\n\toffset = map->m_lblk & (sbi->s_cluster_ratio - 1);\n\tar.len = EXT4_NUM_B2C(sbi, offset+allocated);\n\tar.goal -= offset;\n\tar.logical -= offset;\n\tif (S_ISREG(inode->i_mode))\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\telse\n\t\t/* disable in-core preallocation for non-regular files */\n\t\tar.flags = 0;\n\tif (flags & EXT4_GET_BLOCKS_NO_NORMALIZE)\n\t\tar.flags |= EXT4_MB_HINT_NOPREALLOC;\n\tnewblock = ext4_mb_new_blocks(handle, &ar, &err);\n\tif (!newblock)\n\t\tgoto out2;\n\text_debug(\"allocate new block: goal %llu, found %llu/%u\\n\",\n\t\t  ar.goal, newblock, allocated);\n\tfree_on_err = 1;\n\tallocated_clusters = ar.len;\n\tar.len = EXT4_C2B(sbi, ar.len) - offset;\n\tif (ar.len > allocated)\n\t\tar.len = allocated;\n\ngot_allocated_blocks:\n\t/* try to insert new extent into found leaf and return */\n\text4_ext_store_pblock(&newex, newblock + offset);\n\tnewex.ee_len = cpu_to_le16(ar.len);\n\t/* Mark uninitialized */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT){\n\t\text4_ext_mark_uninitialized(&newex);\n\t\t/*\n\t\t * io_end structure was created for every IO write to an\n\t\t * uninitialized extent. To avoid unnecessary conversion,\n\t\t * here we flag the IO that really needs the conversion.\n\t\t * For non asycn direct IO case, flag the inode state\n\t\t * that we need to perform conversion when IO is done.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\tset_unwritten = 1;\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t}\n\n\terr = 0;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0)\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t path, ar.len);\n\tif (!err)\n\t\terr = ext4_ext_insert_extent(handle, inode, path,\n\t\t\t\t\t     &newex, flags);\n\n\tif (!err && set_unwritten) {\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode,\n\t\t\t\t\t     EXT4_STATE_DIO_UNWRITTEN);\n\t}\n\n\tif (err && free_on_err) {\n\t\tint fb_flags = flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE ?\n\t\t\tEXT4_FREE_BLOCKS_NO_QUOT_UPDATE : 0;\n\t\t/* free data blocks we just allocated */\n\t\t/* not a good idea to call discard here directly,\n\t\t * but otherwise we'd need to call it every free() */\n\t\text4_discard_preallocations(inode);\n\t\text4_free_blocks(handle, inode, NULL, ext4_ext_pblock(&newex),\n\t\t\t\t ext4_ext_get_actual_len(&newex), fb_flags);\n\t\tgoto out2;\n\t}\n\n\t/* previous routine could use block we allocated */\n\tnewblock = ext4_ext_pblock(&newex);\n\tallocated = ext4_ext_get_actual_len(&newex);\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\n\t/*\n\t * Update reserved blocks/metadata blocks after successful\n\t * block allocation which had been deferred till now.\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\t/*\n\t\t * Check how many clusters we had reserved this allocated range\n\t\t */\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\t\t\tmap->m_lblk, allocated);\n\t\tif (map->m_flags & EXT4_MAP_FROM_CLUSTER) {\n\t\t\tif (reserved_clusters) {\n\t\t\t\t/*\n\t\t\t\t * We have clusters reserved for this range.\n\t\t\t\t * But since we are not doing actual allocation\n\t\t\t\t * and are simply using blocks from previously\n\t\t\t\t * allocated cluster, we should release the\n\t\t\t\t * reservation and not claim quota.\n\t\t\t\t */\n\t\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\treserved_clusters, 0);\n\t\t\t}\n\t\t} else {\n\t\t\tBUG_ON(allocated_clusters < reserved_clusters);\n\t\t\t/* We will claim quota for all newly allocated blocks.*/\n\t\t\text4_da_update_reserve_space(inode, allocated_clusters,\n\t\t\t\t\t\t\t1);\n\t\t\tif (reserved_clusters < allocated_clusters) {\n\t\t\t\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\t\t\t\tint reservation = allocated_clusters -\n\t\t\t\t\t\t  reserved_clusters;\n\t\t\t\t/*\n\t\t\t\t * It seems we claimed few clusters outside of\n\t\t\t\t * the range of this allocation. We should give\n\t\t\t\t * it back to the reservation pool. This can\n\t\t\t\t * happen in the following case:\n\t\t\t\t *\n\t\t\t\t * * Suppose s_cluster_ratio is 4 (i.e., each\n\t\t\t\t *   cluster has 4 blocks. Thus, the clusters\n\t\t\t\t *   are [0-3],[4-7],[8-11]...\n\t\t\t\t * * First comes delayed allocation write for\n\t\t\t\t *   logical blocks 10 & 11. Since there were no\n\t\t\t\t *   previous delayed allocated blocks in the\n\t\t\t\t *   range [8-11], we would reserve 1 cluster\n\t\t\t\t *   for this write.\n\t\t\t\t * * Next comes write for logical blocks 3 to 8.\n\t\t\t\t *   In this case, we will reserve 2 clusters\n\t\t\t\t *   (for [0-3] and [4-7]; and not for [8-11] as\n\t\t\t\t *   that range has a delayed allocated blocks.\n\t\t\t\t *   Thus total reserved clusters now becomes 3.\n\t\t\t\t * * Now, during the delayed allocation writeout\n\t\t\t\t *   time, we will first write blocks [3-8] and\n\t\t\t\t *   allocate 3 clusters for writing these\n\t\t\t\t *   blocks. Also, we would claim all these\n\t\t\t\t *   three clusters above.\n\t\t\t\t * * Now when we come here to writeout the\n\t\t\t\t *   blocks [10-11], we would expect to claim\n\t\t\t\t *   the reservation of 1 cluster we had made\n\t\t\t\t *   (and we would claim it since there are no\n\t\t\t\t *   more delayed allocated blocks in the range\n\t\t\t\t *   [8-11]. But our reserved cluster count had\n\t\t\t\t *   already gone to 0.\n\t\t\t\t *\n\t\t\t\t *   Thus, at the step 4 above when we determine\n\t\t\t\t *   that there are still some unwritten delayed\n\t\t\t\t *   allocated blocks outside of our current\n\t\t\t\t *   block range, we should increment the\n\t\t\t\t *   reserved clusters count so that when the\n\t\t\t\t *   remaining blocks finally gets written, we\n\t\t\t\t *   could claim them.\n\t\t\t\t */\n\t\t\t\tdquot_reserve_block(inode,\n\t\t\t\t\t\tEXT4_C2B(sbi, reservation));\n\t\t\t\tspin_lock(&ei->i_block_reservation_lock);\n\t\t\t\tei->i_reserved_data_blocks += reservation;\n\t\t\t\tspin_unlock(&ei->i_block_reservation_lock);\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Cache the extent and update transaction to commit on fdatasync only\n\t * when it is _not_ an uninitialized extent.\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_UNINIT_EXT) == 0) {\n\t\text4_ext_put_in_cache(inode, map->m_lblk, allocated, newblock);\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t} else\n\t\text4_update_inode_fsync_trans(handle, inode, 0);\nout:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\n\ttrace_ext4_ext_map_blocks_exit(inode, map->m_lblk,\n\t\tnewblock, map->m_len, err ? err : allocated);\n\n\treturn err ? err : allocated;\n}\n\nvoid ext4_ext_truncate(struct inode *inode)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t last_block;\n\thandle_t *handle;\n\tloff_t page_len;\n\tint err = 0;\n\n\t/*\n\t * finish any pending end_io work so we won't run the risk of\n\t * converting any truncated blocks to initialized later\n\t */\n\text4_flush_unwritten_io(inode);\n\n\t/*\n\t * probably first extent we're gonna free will be last in block\n\t */\n\terr = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, err);\n\tif (IS_ERR(handle))\n\t\treturn;\n\n\tif (inode->i_size % PAGE_CACHE_SIZE != 0) {\n\t\tpage_len = PAGE_CACHE_SIZE -\n\t\t\t(inode->i_size & (PAGE_CACHE_SIZE - 1));\n\n\t\terr = ext4_discard_partial_page_buffers(handle,\n\t\t\tmapping, inode->i_size, page_len, 0);\n\n\t\tif (err)\n\t\t\tgoto out_stop;\n\t}\n\n\tif (ext4_orphan_add(handle, inode))\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_ext_invalidate_cache(inode);\n\n\text4_discard_preallocations(inode);\n\n\t/*\n\t * TODO: optimization is possible here.\n\t * Probably we need not scan at all,\n\t * because page truncation is enough.\n\t */\n\n\t/* we have to know where to truncate from in crash case */\n\tEXT4_I(inode)->i_disksize = inode->i_size;\n\text4_mark_inode_dirty(handle, inode);\n\n\tlast_block = (inode->i_size + sb->s_blocksize - 1)\n\t\t\t>> EXT4_BLOCK_SIZE_BITS(sb);\n\terr = ext4_ext_remove_space(inode, last_block, EXT_MAX_BLOCKS - 1);\n\n\t/* In a multi-transaction truncate, we only make the final\n\t * transaction synchronous.\n\t */\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\nout_stop:\n\t/*\n\t * If this was a simple ftruncate() and the file will remain alive,\n\t * then we need to clear up the orphan record which we created above.\n\t * However, if this was a real unlink then we were called by\n\t * ext4_delete_inode(), and we allow that function to clean up the\n\t * orphan info for us.\n\t */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(handle, inode);\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\n}\n\nstatic void ext4_falloc_update_inode(struct inode *inode,\n\t\t\t\tint mode, loff_t new_size, int update_ctime)\n{\n\tstruct timespec now;\n\n\tif (update_ctime) {\n\t\tnow = current_fs_time(inode->i_sb);\n\t\tif (!timespec_equal(&inode->i_ctime, &now))\n\t\t\tinode->i_ctime = now;\n\t}\n\t/*\n\t * Update only when preallocation was requested beyond\n\t * the file size.\n\t */\n\tif (!(mode & FALLOC_FL_KEEP_SIZE)) {\n\t\tif (new_size > i_size_read(inode))\n\t\t\ti_size_write(inode, new_size);\n\t\tif (new_size > EXT4_I(inode)->i_disksize)\n\t\t\text4_update_i_disksize(inode, new_size);\n\t} else {\n\t\t/*\n\t\t * Mark that we allocate beyond EOF so the subsequent truncate\n\t\t * can proceed even if the new size is the same as i_size.\n\t\t */\n\t\tif (new_size > i_size_read(inode))\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\t}\n\n}\n\n/*\n * preallocate space for a file. This implements ext4's fallocate file\n * operation, which gets called from sys_fallocate system call.\n * For block-mapped files, posix_fallocate should fall back to the method\n * of writing zeroes to the required new blocks (the same behavior which is\n * expected for file systems which do not support fallocate() system call).\n */\nlong ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\thandle_t *handle;\n\tloff_t new_size;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint retries = 0;\n\tint flags;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\t/*\n\t * currently supporting (pre)allocate mode for extent-based\n\t * files _only_\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Return error if mode is not supported */\n\tif (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))\n\t\treturn -EOPNOTSUPP;\n\n\tif (mode & FALLOC_FL_PUNCH_HOLE)\n\t\treturn ext4_punch_hole(file, offset, len);\n\n\ttrace_ext4_fallocate_enter(inode, offset, len, mode);\n\tmap.m_lblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)\n\t\t- map.m_lblk;\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\tmutex_lock(&inode->i_mutex);\n\tret = inode_newsize_ok(inode, (len + offset));\n\tif (ret) {\n\t\tmutex_unlock(&inode->i_mutex);\n\t\ttrace_ext4_fallocate_exit(inode, offset, max_blocks, ret);\n\t\treturn ret;\n\t}\n\tflags = EXT4_GET_BLOCKS_CREATE_UNINIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\t/*\n\t * Don't normalize the request if it can fit in one extent so\n\t * that it doesn't get unnecessarily split into multiple\n\t * extents.\n\t */\n\tif (len <= EXT_UNINIT_MAX_LEN << blkbits)\n\t\tflags |= EXT4_GET_BLOCKS_NO_NORMALIZE;\n\n\t/* Prevent race condition between unwritten */\n\text4_flush_unwritten_io(inode);\nretry:\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk = map.m_lblk + ret;\n\t\tmap.m_len = max_blocks = max_blocks - ret;\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map, flags);\n\t\tif (ret <= 0) {\n#ifdef EXT4FS_DEBUG\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tprintk(KERN_ERR \"%s: ext4_ext_map_blocks \"\n\t\t\t\t    \"returned error inode#%lu, block=%u, \"\n\t\t\t\t    \"max_blocks=%u\", __func__,\n\t\t\t\t    inode->i_ino, map.m_lblk, max_blocks);\n#endif\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\t\tbreak;\n\t\t}\n\t\tif ((map.m_lblk + ret) >= (EXT4_BLOCK_ALIGN(offset + len,\n\t\t\t\t\t\tblkbits) >> blkbits))\n\t\t\tnew_size = offset + len;\n\t\telse\n\t\t\tnew_size = ((loff_t) map.m_lblk + ret) << blkbits;\n\n\t\text4_falloc_update_inode(inode, mode, new_size,\n\t\t\t\t\t (map.m_flags & EXT4_MAP_NEW));\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tif ((file->f_flags & O_SYNC) && ret >= max_blocks)\n\t\t\text4_handle_sync(handle);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret2)\n\t\t\tbreak;\n\t}\n\tif (ret == -ENOSPC &&\n\t\t\text4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tmutex_unlock(&inode->i_mutex);\n\ttrace_ext4_fallocate_exit(inode, offset, max_blocks,\n\t\t\t\tret > 0 ? ret2 : ret);\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * This function convert a range of blocks to written extents\n * The caller of this function will pass the start offset and the size.\n * all unwritten extents within this range will be converted to\n * written extents.\n *\n * This function is called from the direct IO end io call back\n * function, to convert the fallocated extents after IO is completed.\n * Returns 0 on success.\n */\nint ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,\n\t\t\t\t    ssize_t len)\n{\n\thandle_t *handle;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\tmap.m_lblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = ((EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits) -\n\t\t      map.m_lblk);\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = (max_blocks -= ret);\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map,\n\t\t\t\t      EXT4_GET_BLOCKS_IO_CONVERT_EXT);\n\t\tif (ret <= 0) {\n\t\t\tWARN_ON(ret <= 0);\n\t\t\text4_msg(inode->i_sb, KERN_ERR,\n\t\t\t\t \"%s:%d: inode #%lu: block %u: len %u: \"\n\t\t\t\t \"ext4_ext_map_blocks returned %d\",\n\t\t\t\t __func__, __LINE__, inode->i_ino, map.m_lblk,\n\t\t\t\t map.m_len, ret);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret <= 0 || ret2 )\n\t\t\tbreak;\n\t}\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * Callback function called for each extent to gather FIEMAP information.\n */\nstatic int ext4_ext_fiemap_cb(struct inode *inode, ext4_lblk_t next,\n\t\t       struct ext4_ext_cache *newex, struct ext4_extent *ex,\n\t\t       void *data)\n{\n\t__u64\tlogical;\n\t__u64\tphysical;\n\t__u64\tlength;\n\t__u32\tflags = 0;\n\tint\t\tret = 0;\n\tstruct fiemap_extent_info *fieinfo = data;\n\tunsigned char blksize_bits;\n\n\tblksize_bits = inode->i_sb->s_blocksize_bits;\n\tlogical = (__u64)newex->ec_block << blksize_bits;\n\n\tif (newex->ec_start == 0) {\n\t\t/*\n\t\t * No extent in extent-tree contains block @newex->ec_start,\n\t\t * then the block may stay in 1)a hole or 2)delayed-extent.\n\t\t *\n\t\t * Holes or delayed-extents are processed as follows.\n\t\t * 1. lookup dirty pages with specified range in pagecache.\n\t\t *    If no page is got, then there is no delayed-extent and\n\t\t *    return with EXT_CONTINUE.\n\t\t * 2. find the 1st mapped buffer,\n\t\t * 3. check if the mapped buffer is both in the request range\n\t\t *    and a delayed buffer. If not, there is no delayed-extent,\n\t\t *    then return.\n\t\t * 4. a delayed-extent is found, the extent will be collected.\n\t\t */\n\t\text4_lblk_t\tend = 0;\n\t\tpgoff_t\t\tlast_offset;\n\t\tpgoff_t\t\toffset;\n\t\tpgoff_t\t\tindex;\n\t\tpgoff_t\t\tstart_index = 0;\n\t\tstruct page\t**pages = NULL;\n\t\tstruct buffer_head *bh = NULL;\n\t\tstruct buffer_head *head = NULL;\n\t\tunsigned int nr_pages = PAGE_SIZE / sizeof(struct page *);\n\n\t\tpages = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\t\tif (pages == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\toffset = logical >> PAGE_SHIFT;\nrepeat:\n\t\tlast_offset = offset;\n\t\thead = NULL;\n\t\tret = find_get_pages_tag(inode->i_mapping, &offset,\n\t\t\t\t\tPAGECACHE_TAG_DIRTY, nr_pages, pages);\n\n\t\tif (!(flags & FIEMAP_EXTENT_DELALLOC)) {\n\t\t\t/* First time, try to find a mapped buffer. */\n\t\t\tif (ret == 0) {\nout:\n\t\t\t\tfor (index = 0; index < ret; index++)\n\t\t\t\t\tpage_cache_release(pages[index]);\n\t\t\t\t/* just a hole. */\n\t\t\t\tkfree(pages);\n\t\t\t\treturn EXT_CONTINUE;\n\t\t\t}\n\t\t\tindex = 0;\n\nnext_page:\n\t\t\t/* Try to find the 1st mapped buffer. */\n\t\t\tend = ((__u64)pages[index]->index << PAGE_SHIFT) >>\n\t\t\t\t  blksize_bits;\n\t\t\tif (!page_has_buffers(pages[index]))\n\t\t\t\tgoto out;\n\t\t\thead = page_buffers(pages[index]);\n\t\t\tif (!head)\n\t\t\t\tgoto out;\n\n\t\t\tindex++;\n\t\t\tbh = head;\n\t\t\tdo {\n\t\t\t\tif (end >= newex->ec_block +\n\t\t\t\t\tnewex->ec_len)\n\t\t\t\t\t/* The buffer is out of\n\t\t\t\t\t * the request range.\n\t\t\t\t\t */\n\t\t\t\t\tgoto out;\n\n\t\t\t\tif (buffer_mapped(bh) &&\n\t\t\t\t    end >= newex->ec_block) {\n\t\t\t\t\tstart_index = index - 1;\n\t\t\t\t\t/* get the 1st mapped buffer. */\n\t\t\t\t\tgoto found_mapped_buffer;\n\t\t\t\t}\n\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t\tend++;\n\t\t\t} while (bh != head);\n\n\t\t\t/* No mapped buffer in the range found in this page,\n\t\t\t * We need to look up next page.\n\t\t\t */\n\t\t\tif (index >= ret) {\n\t\t\t\t/* There is no page left, but we need to limit\n\t\t\t\t * newex->ec_len.\n\t\t\t\t */\n\t\t\t\tnewex->ec_len = end - newex->ec_block;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tgoto next_page;\n\t\t} else {\n\t\t\t/*Find contiguous delayed buffers. */\n\t\t\tif (ret > 0 && pages[0]->index == last_offset)\n\t\t\t\thead = page_buffers(pages[0]);\n\t\t\tbh = head;\n\t\t\tindex = 1;\n\t\t\tstart_index = 0;\n\t\t}\n\nfound_mapped_buffer:\n\t\tif (bh != NULL && buffer_delay(bh)) {\n\t\t\t/* 1st or contiguous delayed buffer found. */\n\t\t\tif (!(flags & FIEMAP_EXTENT_DELALLOC)) {\n\t\t\t\t/*\n\t\t\t\t * 1st delayed buffer found, record\n\t\t\t\t * the start of extent.\n\t\t\t\t */\n\t\t\t\tflags |= FIEMAP_EXTENT_DELALLOC;\n\t\t\t\tnewex->ec_block = end;\n\t\t\t\tlogical = (__u64)end << blksize_bits;\n\t\t\t}\n\t\t\t/* Find contiguous delayed buffers. */\n\t\t\tdo {\n\t\t\t\tif (!buffer_delay(bh))\n\t\t\t\t\tgoto found_delayed_extent;\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t\tend++;\n\t\t\t} while (bh != head);\n\n\t\t\tfor (; index < ret; index++) {\n\t\t\t\tif (!page_has_buffers(pages[index])) {\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\thead = page_buffers(pages[index]);\n\t\t\t\tif (!head) {\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (pages[index]->index !=\n\t\t\t\t    pages[start_index]->index + index\n\t\t\t\t    - start_index) {\n\t\t\t\t\t/* Blocks are not contiguous. */\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbh = head;\n\t\t\t\tdo {\n\t\t\t\t\tif (!buffer_delay(bh))\n\t\t\t\t\t\t/* Delayed-extent ends. */\n\t\t\t\t\t\tgoto found_delayed_extent;\n\t\t\t\t\tbh = bh->b_this_page;\n\t\t\t\t\tend++;\n\t\t\t\t} while (bh != head);\n\t\t\t}\n\t\t} else if (!(flags & FIEMAP_EXTENT_DELALLOC))\n\t\t\t/* a hole found. */\n\t\t\tgoto out;\n\nfound_delayed_extent:\n\t\tnewex->ec_len = min(end - newex->ec_block,\n\t\t\t\t\t\t(ext4_lblk_t)EXT_INIT_MAX_LEN);\n\t\tif (ret == nr_pages && bh != NULL &&\n\t\t\tnewex->ec_len < EXT_INIT_MAX_LEN &&\n\t\t\tbuffer_delay(bh)) {\n\t\t\t/* Have not collected an extent and continue. */\n\t\t\tfor (index = 0; index < ret; index++)\n\t\t\t\tpage_cache_release(pages[index]);\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tfor (index = 0; index < ret; index++)\n\t\t\tpage_cache_release(pages[index]);\n\t\tkfree(pages);\n\t}\n\n\tphysical = (__u64)newex->ec_start << blksize_bits;\n\tlength =   (__u64)newex->ec_len << blksize_bits;\n\n\tif (ex && ext4_ext_is_uninitialized(ex))\n\t\tflags |= FIEMAP_EXTENT_UNWRITTEN;\n\n\tif (next == EXT_MAX_BLOCKS)\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\n\tret = fiemap_fill_next_extent(fieinfo, logical, physical,\n\t\t\t\t\tlength, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret == 1)\n\t\treturn EXT_BREAK;\n\treturn EXT_CONTINUE;\n}\n/* fiemap flags we can handle specified here */\n#define EXT4_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)\n\nstatic int ext4_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\t__u64 physical = 0;\n\t__u64 length;\n\t__u32 flags = FIEMAP_EXTENT_LAST;\n\tint blockbits = inode->i_sb->s_blocksize_bits;\n\tint error = 0;\n\n\t/* in-inode? */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tstruct ext4_iloc iloc;\n\t\tint offset;\t/* offset of xattr in inode */\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error)\n\t\t\treturn error;\n\t\tphysical = iloc.bh->b_blocknr << blockbits;\n\t\toffset = EXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\tEXT4_I(inode)->i_extra_isize;\n\t\tphysical += offset;\n\t\tlength = EXT4_SB(inode->i_sb)->s_inode_size - offset;\n\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t\tbrelse(iloc.bh);\n\t} else { /* external block */\n\t\tphysical = EXT4_I(inode)->i_file_acl << blockbits;\n\t\tlength = inode->i_sb->s_blocksize;\n\t}\n\n\tif (physical)\n\t\terror = fiemap_fill_next_extent(fieinfo, 0, physical,\n\t\t\t\t\t\tlength, flags);\n\treturn (error < 0 ? error : 0);\n}\n\n/*\n * ext4_ext_punch_hole\n *\n * Punches a hole of \"length\" bytes in a file starting\n * at byte \"offset\"\n *\n * @inode:  The inode of the file to punch a hole in\n * @offset: The starting byte offset of the hole\n * @length: The length of the hole\n *\n * Returns the number of blocks removed or negative on err\n */\nint ext4_ext_punch_hole(struct file *file, loff_t offset, loff_t length)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t first_block, stop_block;\n\tstruct address_space *mapping = inode->i_mapping;\n\thandle_t *handle;\n\tloff_t first_page, last_page, page_len;\n\tloff_t first_page_offset, last_page_offset;\n\tint credits, err = 0;\n\n\t/*\n\t * Write out all dirty pages to avoid race conditions\n\t * Then release them.\n\t */\n\tif (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {\n\t\terr = filemap_write_and_wait_range(mapping,\n\t\t\toffset, offset + length - 1);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tmutex_lock(&inode->i_mutex);\n\t/* It's not possible punch hole on append only file */\n\tif (IS_APPEND(inode) || IS_IMMUTABLE(inode)) {\n\t\terr = -EPERM;\n\t\tgoto out_mutex;\n\t}\n\tif (IS_SWAPFILE(inode)) {\n\t\terr = -ETXTBSY;\n\t\tgoto out_mutex;\n\t}\n\n\t/* No need to punch hole beyond i_size */\n\tif (offset >= inode->i_size)\n\t\tgoto out_mutex;\n\n\t/*\n\t * If the hole extends beyond i_size, set the hole\n\t * to end after the page that contains i_size\n\t */\n\tif (offset + length > inode->i_size) {\n\t\tlength = inode->i_size +\n\t\t   PAGE_CACHE_SIZE - (inode->i_size & (PAGE_CACHE_SIZE - 1)) -\n\t\t   offset;\n\t}\n\n\tfirst_page = (offset + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\tlast_page = (offset + length) >> PAGE_CACHE_SHIFT;\n\n\tfirst_page_offset = first_page << PAGE_CACHE_SHIFT;\n\tlast_page_offset = last_page << PAGE_CACHE_SHIFT;\n\n\t/* Now release the pages */\n\tif (last_page_offset > first_page_offset) {\n\t\ttruncate_pagecache_range(inode, first_page_offset,\n\t\t\t\t\t last_page_offset - 1);\n\t}\n\n\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\text4_inode_block_unlocked_dio(inode);\n\terr = ext4_flush_unwritten_io(inode);\n\tif (err)\n\t\tgoto out_dio;\n\tinode_dio_wait(inode);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, credits);\n\tif (IS_ERR(handle)) {\n\t\terr = PTR_ERR(handle);\n\t\tgoto out_dio;\n\t}\n\n\n\t/*\n\t * Now we need to zero out the non-page-aligned data in the\n\t * pages at the start and tail of the hole, and unmap the buffer\n\t * heads for the block aligned regions of the page that were\n\t * completely zeroed.\n\t */\n\tif (first_page > last_page) {\n\t\t/*\n\t\t * If the file space being truncated is contained within a page\n\t\t * just zero out and unmap the middle of that page\n\t\t */\n\t\terr = ext4_discard_partial_page_buffers(handle,\n\t\t\tmapping, offset, length, 0);\n\n\t\tif (err)\n\t\t\tgoto out;\n\t} else {\n\t\t/*\n\t\t * zero out and unmap the partial page that contains\n\t\t * the start of the hole\n\t\t */\n\t\tpage_len  = first_page_offset - offset;\n\t\tif (page_len > 0) {\n\t\t\terr = ext4_discard_partial_page_buffers(handle, mapping,\n\t\t\t\t\t\t   offset, page_len, 0);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * zero out and unmap the partial page that contains\n\t\t * the end of the hole\n\t\t */\n\t\tpage_len = offset + length - last_page_offset;\n\t\tif (page_len > 0) {\n\t\t\terr = ext4_discard_partial_page_buffers(handle, mapping,\n\t\t\t\t\tlast_page_offset, page_len, 0);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * If i_size is contained in the last page, we need to\n\t * unmap and zero the partial page after i_size\n\t */\n\tif (inode->i_size >> PAGE_CACHE_SHIFT == last_page &&\n\t   inode->i_size % PAGE_CACHE_SIZE != 0) {\n\n\t\tpage_len = PAGE_CACHE_SIZE -\n\t\t\t(inode->i_size & (PAGE_CACHE_SIZE - 1));\n\n\t\tif (page_len > 0) {\n\t\t\terr = ext4_discard_partial_page_buffers(handle,\n\t\t\t  mapping, inode->i_size, page_len, 0);\n\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfirst_block = (offset + sb->s_blocksize - 1) >>\n\t\tEXT4_BLOCK_SIZE_BITS(sb);\n\tstop_block = (offset + length) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* If there are no blocks to remove, return now */\n\tif (first_block >= stop_block)\n\t\tgoto out;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_ext_invalidate_cache(inode);\n\text4_discard_preallocations(inode);\n\n\terr = ext4_ext_remove_space(inode, first_block, stop_block - 1);\n\n\text4_ext_invalidate_cache(inode);\n\text4_discard_preallocations(inode);\n\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\nout:\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn err;\n}\nint ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\text4_lblk_t start_blk;\n\tint error = 0;\n\n\t/* fallback to generic here if not in extents fmt */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn generic_block_fiemap(inode, fieinfo, start, len,\n\t\t\text4_get_block);\n\n\tif (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS))\n\t\treturn -EBADR;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\terror = ext4_xattr_fiemap(inode, fieinfo);\n\t} else {\n\t\text4_lblk_t len_blks;\n\t\t__u64 last_blk;\n\n\t\tstart_blk = start >> inode->i_sb->s_blocksize_bits;\n\t\tlast_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;\n\t\tif (last_blk >= EXT_MAX_BLOCKS)\n\t\t\tlast_blk = EXT_MAX_BLOCKS-1;\n\t\tlen_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;\n\n\t\t/*\n\t\t * Walk the extent tree gathering extent information.\n\t\t * ext4_ext_fiemap_cb will push extents back to user.\n\t\t */\n\t\terror = ext4_ext_walk_space(inode, start_blk, len_blks,\n\t\t\t\t\t  ext4_ext_fiemap_cb, fieinfo);\n\t}\n\n\treturn error;\n}\n"], "filenames": ["fs/ext4/extents.c"], "buggy_code_start_loc": [53], "buggy_code_end_loc": [3656], "fixing_code_start_loc": [54], "fixing_code_end_loc": [3691], "type": "CWE-362", "message": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized.", "other": {"cve": {"id": "CVE-2012-4508", "sourceIdentifier": "secalert@redhat.com", "published": "2012-12-21T11:47:36.580", "lastModified": "2023-02-13T04:34:36.667", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Race condition in fs/ext4/extents.c in the Linux kernel before 3.4.16 allows local users to obtain sensitive information from a deleted file by reading an extent that was not properly marked as uninitialized."}, {"lang": "es", "value": "Condici\u00f3n de carrera en fs/ext4/extents.c. En el kernel Linux antes de v3.4.16 permite a usuarios locales obtener informaci\u00f3n sensible de un archivo eliminado mediante la lectura de un 'extent' que no fue correctamente marcado como 'no inicializado' .\r\n"}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 1.9}, "baseSeverity": "LOW", "exploitabilityScore": 3.4, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.4.15", "matchCriteriaId": "2F399128-7646-4F7C-83D5-1C9461024AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "D30AEC07-3CBD-4F4F-9646-BEAA1D98750B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C2AA8E68-691B-499C-AEDD-3C0BFFE70044"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "9440475B-5960-4066-A204-F30AAFC87846"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "53BCFBFB-6AF0-4525-8623-7633CC5E17DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "6ED4E86A-74F0-436A-BEB4-3F4EE93A5421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "BF0365B0-8E16-4F30-BD92-5DD538CC8135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "079505E8-2942-4C33-93D1-35ADA4C39E72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "38989541-2360-4E0A-AE5A-3D6144AA6114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "4E51646B-7A0E-40F3-B8C9-239C1DA81DD1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.3:*:*:*:*:*:*:*", "matchCriteriaId": "42A8A507-F8E2-491C-A144-B2448A1DB26E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.4:*:*:*:*:*:*:*", "matchCriteriaId": "901FC6F3-2C2A-4112-AE27-AB102BBE8DEE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.5:*:*:*:*:*:*:*", "matchCriteriaId": "203AD334-DB9F-41B0-A4D1-A6C158EF8C40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.6:*:*:*:*:*:*:*", "matchCriteriaId": "B3611753-E440-410F-8250-600C996A4B8E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.7:*:*:*:*:*:*:*", "matchCriteriaId": "9739BB47-EEAF-42F1-A557-2AE2EA9526A3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.8:*:*:*:*:*:*:*", "matchCriteriaId": "5A95E3BB-0AFC-4C2E-B9BE-C975E902A266"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.9:*:*:*:*:*:*:*", "matchCriteriaId": "482A6C9A-9B8E-4D1C-917A-F16370745E7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.10:*:*:*:*:*:*:*", "matchCriteriaId": "C6D87357-63E0-41D0-9F02-1BCBF9A77E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.11:*:*:*:*:*:*:*", "matchCriteriaId": "3765A2D6-2D78-4FB1-989E-D5106BFA3F5E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.12:*:*:*:*:*:*:*", "matchCriteriaId": "F54257DB-7023-43C4-AC4D-9590B815CD92"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.13:*:*:*:*:*:*:*", "matchCriteriaId": "61FF5FCD-A4A1-4803-AC53-320A4C838AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.14:*:*:*:*:*:*:*", "matchCriteriaId": "9F096553-064F-46A2-877B-F32F163A0F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.15:*:*:*:*:*:*:*", "matchCriteriaId": "C0D762D1-E3AD-40EA-8D39-83EEB51B5E85"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.16:*:*:*:*:*:*:*", "matchCriteriaId": "A6187D19-7148-4B87-AD7E-244FF9EE0FA6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.17:*:*:*:*:*:*:*", "matchCriteriaId": "99AC64C2-E391-485C-9CD7-BA09C8FA5E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.18:*:*:*:*:*:*:*", "matchCriteriaId": "8CDA5E95-7805-441B-BEF7-4448EA45E964"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.19:*:*:*:*:*:*:*", "matchCriteriaId": "51561053-6C28-4F38-BC9B-3F7A7508EB72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.20:*:*:*:*:*:*:*", "matchCriteriaId": "118F4A5B-C498-4FC3-BE28-50D18EBE4F22"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "BD38EBE6-FE1A-4B55-9FB5-07952253B7A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.22:*:*:*:*:*:*:*", "matchCriteriaId": "3A491E47-82AD-4055-9444-2EC0D6715326"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.23:*:*:*:*:*:*:*", "matchCriteriaId": "13C5FD16-23B6-467F-9438-5B554922F974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.24:*:*:*:*:*:*:*", "matchCriteriaId": "9C67235F-5B51-4BF7-89EC-4810F720246F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.25:*:*:*:*:*:*:*", "matchCriteriaId": "08405DEF-05F4-45F0-AC95-DBF914A36D93"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.26:*:*:*:*:*:*:*", "matchCriteriaId": "1A7B9C4B-4A41-4175-9F07-191C1EE98C1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.27:*:*:*:*:*:*:*", "matchCriteriaId": "B306E0A8-4D4A-4895-8128-A500D30A7E0C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.28:*:*:*:*:*:*:*", "matchCriteriaId": "295C839A-F34E-4853-A926-55EABC639412"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.29:*:*:*:*:*:*:*", "matchCriteriaId": "2AFD5F49-7EF9-4CFE-95BD-8FD19B500B0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.30:*:*:*:*:*:*:*", "matchCriteriaId": "00B3DDDD-B2F6-4753-BA38-65A24017857D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.31:*:*:*:*:*:*:*", "matchCriteriaId": "33FCD39E-F4BF-432D-9CF9-F195CF5844F3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.32:*:*:*:*:*:*:*", "matchCriteriaId": "C7308690-CB0D-4758-B80F-D2ADCD2A9D66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.33:*:*:*:*:*:*:*", "matchCriteriaId": "313A470B-8A2B-478A-82B5-B27D2718331C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.34:*:*:*:*:*:*:*", "matchCriteriaId": "83FF021E-07E3-41CC-AAE8-D99D7FF24B9D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.35:*:*:*:*:*:*:*", "matchCriteriaId": "F72412E3-8DA9-4CC9-A426-B534202ADBA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.36:*:*:*:*:*:*:*", "matchCriteriaId": "FCAA9D7A-3C3E-4C0B-9D38-EA80E68C2E46"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.37:*:*:*:*:*:*:*", "matchCriteriaId": "4A9E3AE5-3FCF-4CBB-A30B-082BCFBFB0CB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.38:*:*:*:*:*:*:*", "matchCriteriaId": "CF715657-4C3A-4392-B85D-1BBF4DE45D89"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.39:*:*:*:*:*:*:*", "matchCriteriaId": "4B63C618-AC3D-4EF7-AFDF-27B9BF482B78"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.40:*:*:*:*:*:*:*", "matchCriteriaId": "C33DA5A9-5E40-4365-9602-82FB4DCD15B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.41:*:*:*:*:*:*:*", "matchCriteriaId": "EFAFDB74-40BD-46FA-89AC-617EB2C7160B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.42:*:*:*:*:*:*:*", "matchCriteriaId": "CF5F17DA-30A7-40CF-BD7C-CEDF06D64617"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.43:*:*:*:*:*:*:*", "matchCriteriaId": "71A276F5-BD9D-4C1B-90DF-9B0C15B6F7DF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.44:*:*:*:*:*:*:*", "matchCriteriaId": "F8F6EBEC-3C29-444B-BB85-6EF239B59EC1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "3DFFE5A6-6A67-4992-84A3-C0F05FACDEAD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "13BBD2A3-AE10-48B9-8776-4FB1CAC37D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "B25680CC-8918-4F27-8D7E-A6579215450B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "92C48B4C-410C-4BA8-A28A-B2E928320FCC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "CB447523-855B-461E-8197-95169BE86EB0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "B155BBDF-6DF6-4FF5-9C41-D8A5266DCC67"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "28476DEC-9630-4B40-9D4D-9BC151DC4CA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "5646880A-2355-4BDD-89E7-825863A0311F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.4:*:*:*:*:*:*:*", "matchCriteriaId": "7FF99148-267A-46F8-9927-A9082269BAF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.5:*:*:*:*:*:*:*", "matchCriteriaId": "A783C083-5D9C-48F9-B5A6-A97A9604FB19"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.6:*:*:*:*:*:*:*", "matchCriteriaId": "2B817A24-03AC-46CD-BEFA-505457FD2A5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.7:*:*:*:*:*:*:*", "matchCriteriaId": "51CF1BCE-090E-4B70-BA16-ACB74411293B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.8:*:*:*:*:*:*:*", "matchCriteriaId": "187AAD67-10D7-4B57-B4C6-00443E246AF3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.9:*:*:*:*:*:*:*", "matchCriteriaId": "F341CE88-C5BC-4CDD-9CB5-B6BAD7152E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.10:*:*:*:*:*:*:*", "matchCriteriaId": "37ACE2A6-C229-4236-8E9F-235F008F3AA0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "D3220B70-917F-4F9F-8A3B-2BF581281E8D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:x86:*", "matchCriteriaId": "7D47A395-821D-4BFF-996E-E849D9A40217"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc2:*:*:*:*:*:*", "matchCriteriaId": "99372D07-C06A-41FA-9843-6D57F99AB5AF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc3:*:*:*:*:*:*", "matchCriteriaId": "2B9DC110-D260-4DB4-B8B0-EF1D160ADA07"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc4:*:*:*:*:*:*", "matchCriteriaId": "6192FE84-4D53-40D4-AF61-78CE7136141A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc5:*:*:*:*:*:*", "matchCriteriaId": "42FEF3CF-1302-45EB-89CC-3786FE4BAC1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc6:*:*:*:*:*:*", "matchCriteriaId": "AE6A6B58-2C89-4DE4-BA57-78100818095C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:rc7:*:*:*:*:*:*", "matchCriteriaId": "1D467F87-2F13-4D26-9A93-E0BA526FEA24"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.1:*:*:*:*:*:*:*", "matchCriteriaId": "FE348F7B-02DE-47D5-8011-F83DA9426021"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.1:*:*:*:*:*:x86:*", "matchCriteriaId": "8A603291-33B4-4195-B52D-D2A9938089C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.2:*:*:*:*:*:*:*", "matchCriteriaId": "E91594EA-F0A3-41B3-A9C6-F7864FC2F229"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.3:*:*:*:*:*:*:*", "matchCriteriaId": "9E1ECCDB-0208-48F6-B44F-16CC0ECE3503"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.4:*:*:*:*:*:*:*", "matchCriteriaId": "FBA8B5DE-372E-47E0-A0F6-BE286D509CC3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.5:*:*:*:*:*:*:*", "matchCriteriaId": "9A1CA083-2CF8-45AE-9E15-1AA3A8352E3B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.6:*:*:*:*:*:*:*", "matchCriteriaId": "19D69A49-5290-4C5F-8157-719AD58D253D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.7:*:*:*:*:*:*:*", "matchCriteriaId": "290BD969-42E7-47B0-B21B-06DE4865432C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.8:*:*:*:*:*:*:*", "matchCriteriaId": "23A9E29E-DE78-4C73-9FBD-C2410F5FC8B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.9:*:*:*:*:*:*:*", "matchCriteriaId": "018434C9-E75F-45CB-A169-DAB4B1D864D7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.10:*:*:*:*:*:*:*", "matchCriteriaId": "DC0AC68F-EC58-4C4F-8CBC-A59ECC00CCDE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.11:*:*:*:*:*:*:*", "matchCriteriaId": "C123C844-F6D7-471E-A62E-F756042FB1CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.12:*:*:*:*:*:*:*", "matchCriteriaId": "A11C38BB-7FA2-49B0-AAC9-83DB387A06DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.13:*:*:*:*:*:*:*", "matchCriteriaId": "61F3733C-E5F6-4855-B471-DF3FB823613B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.14:*:*:*:*:*:*:*", "matchCriteriaId": "1DDCA75F-9A06-4457-9A45-38A38E7F7086"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.15:*:*:*:*:*:*:*", "matchCriteriaId": "7AEA837E-7864-4003-8DB7-111ED710A7E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.16:*:*:*:*:*:*:*", "matchCriteriaId": "B6FE471F-2D1F-4A1D-A197-7E46B75787E1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.17:*:*:*:*:*:*:*", "matchCriteriaId": "FDA9E6AB-58DC-4EC5-A25C-11F9D0B38BF7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.18:*:*:*:*:*:*:*", "matchCriteriaId": "DC6B8DB3-B05B-41A2-B091-342D66AAE8F5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.19:*:*:*:*:*:*:*", "matchCriteriaId": "958F0FF8-33EF-4A71-A0BD-572C85211DBA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.20:*:*:*:*:*:*:*", "matchCriteriaId": "FBA39F48-B02F-4C48-B304-DA9CCA055244"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.21:*:*:*:*:*:*:*", "matchCriteriaId": "1FF841F3-48A7-41D7-9C45-A8170435A5EB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.22:*:*:*:*:*:*:*", "matchCriteriaId": "EF506916-A6DC-4B1E-90E5-959492AF55F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.23:*:*:*:*:*:*:*", "matchCriteriaId": "B3CDAD1F-2C6A-48C0-8FAB-C2659373FA25"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.24:*:*:*:*:*:*:*", "matchCriteriaId": "4FFE4B22-C96A-43D0-B993-F51EDD9C5E0E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.25:*:*:*:*:*:*:*", "matchCriteriaId": "F571CC8B-B212-4553-B463-1DB01D616E8A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.26:*:*:*:*:*:*:*", "matchCriteriaId": "84E3E151-D437-48ED-A529-731EEFF88567"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.27:*:*:*:*:*:*:*", "matchCriteriaId": "E9E3EA3C-CCA5-4433-86E0-3D02C4757A0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.28:*:*:*:*:*:*:*", "matchCriteriaId": "F7AC4F7D-9FA6-4CF1-B2E9-70BF7D4D177C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.29:*:*:*:*:*:*:*", "matchCriteriaId": "3CE3A80D-9648-43CC-8F99-D741ED6552BF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2.30:*:*:*:*:*:*:*", "matchCriteriaId": "C8A98C03-A465-41B4-A551-A26FEC7FFD94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:*:*:*:*:*:*:*", "matchCriteriaId": "AFB76697-1C2F-48C0-9B14-517EC053D4B3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc1:*:*:*:*:*:*", "matchCriteriaId": "BED88DFD-1DC5-4505-A441-44ECDEF0252D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc2:*:*:*:*:*:*", "matchCriteriaId": "DBFD2ACD-728A-4082-BB6A-A1EF6E58E47D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc3:*:*:*:*:*:*", "matchCriteriaId": "C31B0E51-F62D-4053-B04F-FC4D5BC373D2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc4:*:*:*:*:*:*", "matchCriteriaId": "A914303E-1CB6-4AAD-9F5F-DE5433C4E814"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc5:*:*:*:*:*:*", "matchCriteriaId": "203BBA69-90B2-4C5E-8023-C14180742421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc6:*:*:*:*:*:*", "matchCriteriaId": "0DBFAB53-B889-4028-AC0E-7E165B152A18"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3:rc7:*:*:*:*:*:*", "matchCriteriaId": "FE409AEC-F677-4DEF-8EB7-2C35809043CE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.1:*:*:*:*:*:*:*", "matchCriteriaId": "578EC12B-402F-4AD4-B8F8-C9B2CAB06891"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.2:*:*:*:*:*:*:*", "matchCriteriaId": "877002ED-8097-4BB4-BB88-6FC6306C38B2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.3:*:*:*:*:*:*:*", "matchCriteriaId": "76294CE3-D72C-41D5-9E0F-B693D0042699"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.4:*:*:*:*:*:*:*", "matchCriteriaId": "916E97D4-1FAB-42F5-826B-653B1C0909A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.5:*:*:*:*:*:*:*", "matchCriteriaId": "33FD2217-C5D0-48C1-AD74-3527127FEF9C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.6:*:*:*:*:*:*:*", "matchCriteriaId": "2E92971F-B629-4E0A-9A50-8B235F9704B8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.7:*:*:*:*:*:*:*", "matchCriteriaId": "EDD3A069-3829-4EE2-9D5A-29459F29D4C1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.3.8:*:*:*:*:*:*:*", "matchCriteriaId": "A4A0964C-CEB2-41D7-A69C-1599B05B6171"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:*:*:*:*:*:*:*", "matchCriteriaId": "0F960FA6-F904-4A4E-B483-44C70090E9A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:*:*:*:*:*:x86:*", "matchCriteriaId": "8C3D9C66-933A-469E-9073-75015A8AD17D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc1:*:*:*:*:*:*", "matchCriteriaId": "261C1B41-C9E0-414F-8368-51C0C0B8AD38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc1:*:*:*:*:x86:*", "matchCriteriaId": "C92F29A0-DEFF-49E4-AE86-5DBDAD51C677"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc2:*:*:*:*:*:*", "matchCriteriaId": "5CCA261D-2B97-492F-89A0-5F209A804350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc2:*:*:*:*:x86:*", "matchCriteriaId": "5690A703-390D-4D8A-9258-2F47116DAB4F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc3:*:*:*:*:*:*", "matchCriteriaId": "1B1C0C68-9194-473F-BE5E-EC7F184899FA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc3:*:*:*:*:x86:*", "matchCriteriaId": "AB1EDDA7-15AF-4B45-A931-DFCBB1EEB701"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc4:*:*:*:*:*:*", "matchCriteriaId": "D7A6AC9E-BEA6-44B0-B3B3-F0F94E32424A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc4:*:*:*:*:x86:*", "matchCriteriaId": "952FE0DC-B2ED-4080-BF29-A2C265E83FEF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc5:*:*:*:*:*:*", "matchCriteriaId": "16038328-9399-4B85-B777-BA4757D02C9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc5:*:*:*:*:x86:*", "matchCriteriaId": "1CE7ABDB-6572-40E8-B952-CBE52C999858"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc6:*:*:*:*:*:*", "matchCriteriaId": "16CA2757-FA8D-43D9-96E8-D3C0EB6E1DEF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc6:*:*:*:*:x86:*", "matchCriteriaId": "0F417186-D1ED-4A31-92B2-83DEDA1AF272"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc7:*:*:*:*:*:*", "matchCriteriaId": "E8CB5481-5EAE-401E-BD7E-D3095CCA9E94"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc7:*:*:*:*:x86:*", "matchCriteriaId": "3D4FCFAE-918C-4886-9A17-08A5B94D35F4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.1:*:*:*:*:*:*:*", "matchCriteriaId": "A0F36FAC-141D-476D-84C5-A558C199F904"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.1:*:*:*:*:*:x86:*", "matchCriteriaId": "830D2914-C9FE-406F-AFCE-7A04BB9E2896"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.2:*:*:*:*:*:*:*", "matchCriteriaId": "51D64824-25F6-4761-BD6A-29038A143744"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.2:*:*:*:*:*:x86:*", "matchCriteriaId": "F4B791B5-2EB5-403A-90CC-B219F6277D1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.3:*:*:*:*:*:*:*", "matchCriteriaId": "E284C8A1-740F-454D-A774-99CD3A21B594"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.3:*:*:*:*:*:x86:*", "matchCriteriaId": "2BA5F34D-7490-4B2B-A7E6-8450F9C1FC31"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.4:*:*:*:*:*:*:*", "matchCriteriaId": "C70D72AE-0CBF-4324-9935-57E28EC6279C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.4:*:*:*:*:*:x86:*", "matchCriteriaId": "B803FE64-FC8D-4650-9FB9-FEEED4340416"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.5:*:*:*:*:*:*:*", "matchCriteriaId": "F674B06B-7E86-4E41-9126-8152D0DDABAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.5:*:*:*:*:*:x86:*", "matchCriteriaId": "4C560A9A-2388-4980-9E88-118C5EB806B7"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.10:*:*:*:*:*:*:*", "matchCriteriaId": "7DA94F50-2A62-4300-BF4D-A342AAE35629"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.11:*:*:*:*:*:*:*", "matchCriteriaId": "252D937B-50DC-444F-AE73-5FCF6203DF27"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.12:*:*:*:*:*:*:*", "matchCriteriaId": "F6D8EE51-02C1-47BC-A92C-0A8ABEFD28FF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.13:*:*:*:*:*:*:*", "matchCriteriaId": "7F20A5D7-3B38-4911-861A-04C8310D5916"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4.14:*:*:*:*:*:*:*", "matchCriteriaId": "D472DE3A-71D8-4F40-9DDE-85929A2B047D"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=dee1f973ca341c266229faa5a1a5bb268bed3531", "source": "secalert@redhat.com"}, {"url": "http://lists.fedoraproject.org/pipermail/package-announce/2012-November/091110.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2012-1540.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-0496.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-1519.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2013-1783.html", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.4.16", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2012/10/25/1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1645-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1899-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1900-1", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=869904", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/dee1f973ca341c266229faa5a1a5bb268bed3531", "source": "secalert@redhat.com", "tags": ["Patch"]}, {"url": "https://www.suse.com/support/update/announcement/2012/suse-su-20121679-1.html", "source": "secalert@redhat.com"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/dee1f973ca341c266229faa5a1a5bb268bed3531"}}