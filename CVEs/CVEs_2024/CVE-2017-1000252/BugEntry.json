{"buggy_code": ["/*\n * kvm eventfd support - use eventfd objects to signal various KVM events\n *\n * Copyright 2009 Novell.  All Rights Reserved.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Author:\n *\tGregory Haskins <ghaskins@novell.com>\n *\n * This file is free software; you can redistribute it and/or modify\n * it under the terms of version 2 of the GNU General Public License\n * as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\t See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software Foundation,\n * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA.\n */\n\n#include <linux/kvm_host.h>\n#include <linux/kvm.h>\n#include <linux/kvm_irqfd.h>\n#include <linux/workqueue.h>\n#include <linux/syscalls.h>\n#include <linux/wait.h>\n#include <linux/poll.h>\n#include <linux/file.h>\n#include <linux/list.h>\n#include <linux/eventfd.h>\n#include <linux/kernel.h>\n#include <linux/srcu.h>\n#include <linux/slab.h>\n#include <linux/seqlock.h>\n#include <linux/irqbypass.h>\n#include <trace/events/kvm.h>\n\n#include <kvm/iodev.h>\n\n#ifdef CONFIG_HAVE_KVM_IRQFD\n\nstatic struct workqueue_struct *irqfd_cleanup_wq;\n\nstatic void\nirqfd_inject(struct work_struct *work)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(work, struct kvm_kernel_irqfd, inject);\n\tstruct kvm *kvm = irqfd->kvm;\n\n\tif (!irqfd->resampler) {\n\t\tkvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 1,\n\t\t\t\tfalse);\n\t\tkvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 0,\n\t\t\t\tfalse);\n\t} else\n\t\tkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t\t    irqfd->gsi, 1, false);\n}\n\n/*\n * Since resampler irqfds share an IRQ source ID, we de-assert once\n * then notify all of the resampler irqfds using this GSI.  We can't\n * do multiple de-asserts or we risk racing with incoming re-asserts.\n */\nstatic void\nirqfd_resampler_ack(struct kvm_irq_ack_notifier *kian)\n{\n\tstruct kvm_kernel_irqfd_resampler *resampler;\n\tstruct kvm *kvm;\n\tstruct kvm_kernel_irqfd *irqfd;\n\tint idx;\n\n\tresampler = container_of(kian,\n\t\t\tstruct kvm_kernel_irqfd_resampler, notifier);\n\tkvm = resampler->kvm;\n\n\tkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t    resampler->notifier.gsi, 0, false);\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\n\tlist_for_each_entry_rcu(irqfd, &resampler->list, resampler_link)\n\t\teventfd_signal(irqfd->resamplefd, 1);\n\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n}\n\nstatic void\nirqfd_resampler_shutdown(struct kvm_kernel_irqfd *irqfd)\n{\n\tstruct kvm_kernel_irqfd_resampler *resampler = irqfd->resampler;\n\tstruct kvm *kvm = resampler->kvm;\n\n\tmutex_lock(&kvm->irqfds.resampler_lock);\n\n\tlist_del_rcu(&irqfd->resampler_link);\n\tsynchronize_srcu(&kvm->irq_srcu);\n\n\tif (list_empty(&resampler->list)) {\n\t\tlist_del(&resampler->link);\n\t\tkvm_unregister_irq_ack_notifier(kvm, &resampler->notifier);\n\t\tkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t\t    resampler->notifier.gsi, 0, false);\n\t\tkfree(resampler);\n\t}\n\n\tmutex_unlock(&kvm->irqfds.resampler_lock);\n}\n\n/*\n * Race-free decouple logic (ordering is critical)\n */\nstatic void\nirqfd_shutdown(struct work_struct *work)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(work, struct kvm_kernel_irqfd, shutdown);\n\tu64 cnt;\n\n\t/*\n\t * Synchronize with the wait-queue and unhook ourselves to prevent\n\t * further events.\n\t */\n\teventfd_ctx_remove_wait_queue(irqfd->eventfd, &irqfd->wait, &cnt);\n\n\t/*\n\t * We know no new events will be scheduled at this point, so block\n\t * until all previously outstanding events have completed\n\t */\n\tflush_work(&irqfd->inject);\n\n\tif (irqfd->resampler) {\n\t\tirqfd_resampler_shutdown(irqfd);\n\t\teventfd_ctx_put(irqfd->resamplefd);\n\t}\n\n\t/*\n\t * It is now safe to release the object's resources\n\t */\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\n\tirq_bypass_unregister_consumer(&irqfd->consumer);\n#endif\n\teventfd_ctx_put(irqfd->eventfd);\n\tkfree(irqfd);\n}\n\n\n/* assumes kvm->irqfds.lock is held */\nstatic bool\nirqfd_is_active(struct kvm_kernel_irqfd *irqfd)\n{\n\treturn list_empty(&irqfd->list) ? false : true;\n}\n\n/*\n * Mark the irqfd as inactive and schedule it for removal\n *\n * assumes kvm->irqfds.lock is held\n */\nstatic void\nirqfd_deactivate(struct kvm_kernel_irqfd *irqfd)\n{\n\tBUG_ON(!irqfd_is_active(irqfd));\n\n\tlist_del_init(&irqfd->list);\n\n\tqueue_work(irqfd_cleanup_wq, &irqfd->shutdown);\n}\n\nint __attribute__((weak)) kvm_arch_set_irq_inatomic(\n\t\t\t\tstruct kvm_kernel_irq_routing_entry *irq,\n\t\t\t\tstruct kvm *kvm, int irq_source_id,\n\t\t\t\tint level,\n\t\t\t\tbool line_status)\n{\n\treturn -EWOULDBLOCK;\n}\n\n/*\n * Called with wqh->lock held and interrupts disabled\n */\nstatic int\nirqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(wait, struct kvm_kernel_irqfd, wait);\n\tunsigned long flags = (unsigned long)key;\n\tstruct kvm_kernel_irq_routing_entry irq;\n\tstruct kvm *kvm = irqfd->kvm;\n\tunsigned seq;\n\tint idx;\n\n\tif (flags & POLLIN) {\n\t\tidx = srcu_read_lock(&kvm->irq_srcu);\n\t\tdo {\n\t\t\tseq = read_seqcount_begin(&irqfd->irq_entry_sc);\n\t\t\tirq = irqfd->irq_entry;\n\t\t} while (read_seqcount_retry(&irqfd->irq_entry_sc, seq));\n\t\t/* An event has been signaled, inject an interrupt */\n\t\tif (kvm_arch_set_irq_inatomic(&irq, kvm,\n\t\t\t\t\t      KVM_USERSPACE_IRQ_SOURCE_ID, 1,\n\t\t\t\t\t      false) == -EWOULDBLOCK)\n\t\t\tschedule_work(&irqfd->inject);\n\t\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\t}\n\n\tif (flags & POLLHUP) {\n\t\t/* The eventfd is closing, detach from KVM */\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&kvm->irqfds.lock, flags);\n\n\t\t/*\n\t\t * We must check if someone deactivated the irqfd before\n\t\t * we could acquire the irqfds.lock since the item is\n\t\t * deactivated from the KVM side before it is unhooked from\n\t\t * the wait-queue.  If it is already deactivated, we can\n\t\t * simply return knowing the other side will cleanup for us.\n\t\t * We cannot race against the irqfd going away since the\n\t\t * other side is required to acquire wqh->lock, which we hold\n\t\t */\n\t\tif (irqfd_is_active(irqfd))\n\t\t\tirqfd_deactivate(irqfd);\n\n\t\tspin_unlock_irqrestore(&kvm->irqfds.lock, flags);\n\t}\n\n\treturn 0;\n}\n\nstatic void\nirqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh,\n\t\t\tpoll_table *pt)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(pt, struct kvm_kernel_irqfd, pt);\n\tadd_wait_queue(wqh, &irqfd->wait);\n}\n\n/* Must be called under irqfds.lock */\nstatic void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)\n{\n\tstruct kvm_kernel_irq_routing_entry *e;\n\tstruct kvm_kernel_irq_routing_entry entries[KVM_NR_IRQCHIPS];\n\tint n_entries;\n\n\tn_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);\n\n\twrite_seqcount_begin(&irqfd->irq_entry_sc);\n\n\te = entries;\n\tif (n_entries == 1)\n\t\tirqfd->irq_entry = *e;\n\telse\n\t\tirqfd->irq_entry.type = 0;\n\n\twrite_seqcount_end(&irqfd->irq_entry_sc);\n}\n\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\nvoid __attribute__((weak)) kvm_arch_irq_bypass_stop(\n\t\t\t\tstruct irq_bypass_consumer *cons)\n{\n}\n\nvoid __attribute__((weak)) kvm_arch_irq_bypass_start(\n\t\t\t\tstruct irq_bypass_consumer *cons)\n{\n}\n\nint  __attribute__((weak)) kvm_arch_update_irqfd_routing(\n\t\t\t\tstruct kvm *kvm, unsigned int host_irq,\n\t\t\t\tuint32_t guest_irq, bool set)\n{\n\treturn 0;\n}\n#endif\n\nstatic int\nkvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\tstruct kvm_kernel_irqfd *irqfd, *tmp;\n\tstruct fd f;\n\tstruct eventfd_ctx *eventfd = NULL, *resamplefd = NULL;\n\tint ret;\n\tunsigned int events;\n\tint idx;\n\n\tif (!kvm_arch_intc_initialized(kvm))\n\t\treturn -EAGAIN;\n\n\tirqfd = kzalloc(sizeof(*irqfd), GFP_KERNEL);\n\tif (!irqfd)\n\t\treturn -ENOMEM;\n\n\tirqfd->kvm = kvm;\n\tirqfd->gsi = args->gsi;\n\tINIT_LIST_HEAD(&irqfd->list);\n\tINIT_WORK(&irqfd->inject, irqfd_inject);\n\tINIT_WORK(&irqfd->shutdown, irqfd_shutdown);\n\tseqcount_init(&irqfd->irq_entry_sc);\n\n\tf = fdget(args->fd);\n\tif (!f.file) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\teventfd = eventfd_ctx_fileget(f.file);\n\tif (IS_ERR(eventfd)) {\n\t\tret = PTR_ERR(eventfd);\n\t\tgoto fail;\n\t}\n\n\tirqfd->eventfd = eventfd;\n\n\tif (args->flags & KVM_IRQFD_FLAG_RESAMPLE) {\n\t\tstruct kvm_kernel_irqfd_resampler *resampler;\n\n\t\tresamplefd = eventfd_ctx_fdget(args->resamplefd);\n\t\tif (IS_ERR(resamplefd)) {\n\t\t\tret = PTR_ERR(resamplefd);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tirqfd->resamplefd = resamplefd;\n\t\tINIT_LIST_HEAD(&irqfd->resampler_link);\n\n\t\tmutex_lock(&kvm->irqfds.resampler_lock);\n\n\t\tlist_for_each_entry(resampler,\n\t\t\t\t    &kvm->irqfds.resampler_list, link) {\n\t\t\tif (resampler->notifier.gsi == irqfd->gsi) {\n\t\t\t\tirqfd->resampler = resampler;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!irqfd->resampler) {\n\t\t\tresampler = kzalloc(sizeof(*resampler), GFP_KERNEL);\n\t\t\tif (!resampler) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tmutex_unlock(&kvm->irqfds.resampler_lock);\n\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tresampler->kvm = kvm;\n\t\t\tINIT_LIST_HEAD(&resampler->list);\n\t\t\tresampler->notifier.gsi = irqfd->gsi;\n\t\t\tresampler->notifier.irq_acked = irqfd_resampler_ack;\n\t\t\tINIT_LIST_HEAD(&resampler->link);\n\n\t\t\tlist_add(&resampler->link, &kvm->irqfds.resampler_list);\n\t\t\tkvm_register_irq_ack_notifier(kvm,\n\t\t\t\t\t\t      &resampler->notifier);\n\t\t\tirqfd->resampler = resampler;\n\t\t}\n\n\t\tlist_add_rcu(&irqfd->resampler_link, &irqfd->resampler->list);\n\t\tsynchronize_srcu(&kvm->irq_srcu);\n\n\t\tmutex_unlock(&kvm->irqfds.resampler_lock);\n\t}\n\n\t/*\n\t * Install our own custom wake-up handling so we are notified via\n\t * a callback whenever someone signals the underlying eventfd\n\t */\n\tinit_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup);\n\tinit_poll_funcptr(&irqfd->pt, irqfd_ptable_queue_proc);\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tret = 0;\n\tlist_for_each_entry(tmp, &kvm->irqfds.items, list) {\n\t\tif (irqfd->eventfd != tmp->eventfd)\n\t\t\tcontinue;\n\t\t/* This fd is used for another irq already. */\n\t\tret = -EBUSY;\n\t\tspin_unlock_irq(&kvm->irqfds.lock);\n\t\tgoto fail;\n\t}\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tirqfd_update(kvm, irqfd);\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\n\tlist_add_tail(&irqfd->list, &kvm->irqfds.items);\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n\n\t/*\n\t * Check if there was an event already pending on the eventfd\n\t * before we registered, and trigger it as if we didn't miss it.\n\t */\n\tevents = f.file->f_op->poll(f.file, &irqfd->pt);\n\n\tif (events & POLLIN)\n\t\tschedule_work(&irqfd->inject);\n\n\t/*\n\t * do not drop the file until the irqfd is fully initialized, otherwise\n\t * we might race against the POLLHUP\n\t */\n\tfdput(f);\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\n\tif (kvm_arch_has_irq_bypass()) {\n\t\tirqfd->consumer.token = (void *)irqfd->eventfd;\n\t\tirqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;\n\t\tirqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;\n\t\tirqfd->consumer.stop = kvm_arch_irq_bypass_stop;\n\t\tirqfd->consumer.start = kvm_arch_irq_bypass_start;\n\t\tret = irq_bypass_register_consumer(&irqfd->consumer);\n\t\tif (ret)\n\t\t\tpr_info(\"irq bypass consumer (token %p) registration fails: %d\\n\",\n\t\t\t\tirqfd->consumer.token, ret);\n\t}\n#endif\n\n\treturn 0;\n\nfail:\n\tif (irqfd->resampler)\n\t\tirqfd_resampler_shutdown(irqfd);\n\n\tif (resamplefd && !IS_ERR(resamplefd))\n\t\teventfd_ctx_put(resamplefd);\n\n\tif (eventfd && !IS_ERR(eventfd))\n\t\teventfd_ctx_put(eventfd);\n\n\tfdput(f);\n\nout:\n\tkfree(irqfd);\n\treturn ret;\n}\n\nbool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin)\n{\n\tstruct kvm_irq_ack_notifier *kian;\n\tint gsi, idx;\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tgsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);\n\tif (gsi != -1)\n\t\thlist_for_each_entry_rcu(kian, &kvm->irq_ack_notifier_list,\n\t\t\t\t\t link)\n\t\t\tif (kian->gsi == gsi) {\n\t\t\t\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\t\t\t\treturn true;\n\t\t\t}\n\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(kvm_irq_has_notifier);\n\nvoid kvm_notify_acked_gsi(struct kvm *kvm, int gsi)\n{\n\tstruct kvm_irq_ack_notifier *kian;\n\n\thlist_for_each_entry_rcu(kian, &kvm->irq_ack_notifier_list,\n\t\t\t\t link)\n\t\tif (kian->gsi == gsi)\n\t\t\tkian->irq_acked(kian);\n}\n\nvoid kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin)\n{\n\tint gsi, idx;\n\n\ttrace_kvm_ack_irq(irqchip, pin);\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tgsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);\n\tif (gsi != -1)\n\t\tkvm_notify_acked_gsi(kvm, gsi);\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n}\n\nvoid kvm_register_irq_ack_notifier(struct kvm *kvm,\n\t\t\t\t   struct kvm_irq_ack_notifier *kian)\n{\n\tmutex_lock(&kvm->irq_lock);\n\thlist_add_head_rcu(&kian->link, &kvm->irq_ack_notifier_list);\n\tmutex_unlock(&kvm->irq_lock);\n\tkvm_arch_post_irq_ack_notifier_list_update(kvm);\n}\n\nvoid kvm_unregister_irq_ack_notifier(struct kvm *kvm,\n\t\t\t\t    struct kvm_irq_ack_notifier *kian)\n{\n\tmutex_lock(&kvm->irq_lock);\n\thlist_del_init_rcu(&kian->link);\n\tmutex_unlock(&kvm->irq_lock);\n\tsynchronize_srcu(&kvm->irq_srcu);\n\tkvm_arch_post_irq_ack_notifier_list_update(kvm);\n}\n#endif\n\nvoid\nkvm_eventfd_init(struct kvm *kvm)\n{\n#ifdef CONFIG_HAVE_KVM_IRQFD\n\tspin_lock_init(&kvm->irqfds.lock);\n\tINIT_LIST_HEAD(&kvm->irqfds.items);\n\tINIT_LIST_HEAD(&kvm->irqfds.resampler_list);\n\tmutex_init(&kvm->irqfds.resampler_lock);\n#endif\n\tINIT_LIST_HEAD(&kvm->ioeventfds);\n}\n\n#ifdef CONFIG_HAVE_KVM_IRQFD\n/*\n * shutdown any irqfd's that match fd+gsi\n */\nstatic int\nkvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\tstruct kvm_kernel_irqfd *irqfd, *tmp;\n\tstruct eventfd_ctx *eventfd;\n\n\teventfd = eventfd_ctx_fdget(args->fd);\n\tif (IS_ERR(eventfd))\n\t\treturn PTR_ERR(eventfd);\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tlist_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list) {\n\t\tif (irqfd->eventfd == eventfd && irqfd->gsi == args->gsi) {\n\t\t\t/*\n\t\t\t * This clearing of irq_entry.type is needed for when\n\t\t\t * another thread calls kvm_irq_routing_update before\n\t\t\t * we flush workqueue below (we synchronize with\n\t\t\t * kvm_irq_routing_update using irqfds.lock).\n\t\t\t */\n\t\t\twrite_seqcount_begin(&irqfd->irq_entry_sc);\n\t\t\tirqfd->irq_entry.type = 0;\n\t\t\twrite_seqcount_end(&irqfd->irq_entry_sc);\n\t\t\tirqfd_deactivate(irqfd);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n\teventfd_ctx_put(eventfd);\n\n\t/*\n\t * Block until we know all outstanding shutdown jobs have completed\n\t * so that we guarantee there will not be any more interrupts on this\n\t * gsi once this deassign function returns.\n\t */\n\tflush_workqueue(irqfd_cleanup_wq);\n\n\treturn 0;\n}\n\nint\nkvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\tif (args->flags & ~(KVM_IRQFD_FLAG_DEASSIGN | KVM_IRQFD_FLAG_RESAMPLE))\n\t\treturn -EINVAL;\n\n\tif (args->flags & KVM_IRQFD_FLAG_DEASSIGN)\n\t\treturn kvm_irqfd_deassign(kvm, args);\n\n\treturn kvm_irqfd_assign(kvm, args);\n}\n\n/*\n * This function is called as the kvm VM fd is being released. Shutdown all\n * irqfds that still remain open\n */\nvoid\nkvm_irqfd_release(struct kvm *kvm)\n{\n\tstruct kvm_kernel_irqfd *irqfd, *tmp;\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tlist_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list)\n\t\tirqfd_deactivate(irqfd);\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n\n\t/*\n\t * Block until we know all outstanding shutdown jobs have completed\n\t * since we do not take a kvm* reference.\n\t */\n\tflush_workqueue(irqfd_cleanup_wq);\n\n}\n\n/*\n * Take note of a change in irq routing.\n * Caller must invoke synchronize_srcu(&kvm->irq_srcu) afterwards.\n */\nvoid kvm_irq_routing_update(struct kvm *kvm)\n{\n\tstruct kvm_kernel_irqfd *irqfd;\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tlist_for_each_entry(irqfd, &kvm->irqfds.items, list) {\n\t\tirqfd_update(kvm, irqfd);\n\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\n\t\tif (irqfd->producer) {\n\t\t\tint ret = kvm_arch_update_irqfd_routing(\n\t\t\t\t\tirqfd->kvm, irqfd->producer->irq,\n\t\t\t\t\tirqfd->gsi, 1);\n\t\t\tWARN_ON(ret);\n\t\t}\n#endif\n\t}\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n}\n\n/*\n * create a host-wide workqueue for issuing deferred shutdown requests\n * aggregated from all vm* instances. We need our own isolated\n * queue to ease flushing work items when a VM exits.\n */\nint kvm_irqfd_init(void)\n{\n\tirqfd_cleanup_wq = alloc_workqueue(\"kvm-irqfd-cleanup\", 0, 0);\n\tif (!irqfd_cleanup_wq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid kvm_irqfd_exit(void)\n{\n\tdestroy_workqueue(irqfd_cleanup_wq);\n}\n#endif\n\n/*\n * --------------------------------------------------------------------\n * ioeventfd: translate a PIO/MMIO memory write to an eventfd signal.\n *\n * userspace can register a PIO/MMIO address with an eventfd for receiving\n * notification when the memory has been touched.\n * --------------------------------------------------------------------\n */\n\nstruct _ioeventfd {\n\tstruct list_head     list;\n\tu64                  addr;\n\tint                  length;\n\tstruct eventfd_ctx  *eventfd;\n\tu64                  datamatch;\n\tstruct kvm_io_device dev;\n\tu8                   bus_idx;\n\tbool                 wildcard;\n};\n\nstatic inline struct _ioeventfd *\nto_ioeventfd(struct kvm_io_device *dev)\n{\n\treturn container_of(dev, struct _ioeventfd, dev);\n}\n\nstatic void\nioeventfd_release(struct _ioeventfd *p)\n{\n\teventfd_ctx_put(p->eventfd);\n\tlist_del(&p->list);\n\tkfree(p);\n}\n\nstatic bool\nioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)\n{\n\tu64 _val;\n\n\tif (addr != p->addr)\n\t\t/* address must be precise for a hit */\n\t\treturn false;\n\n\tif (!p->length)\n\t\t/* length = 0 means only look at the address, so always a hit */\n\t\treturn true;\n\n\tif (len != p->length)\n\t\t/* address-range must be precise for a hit */\n\t\treturn false;\n\n\tif (p->wildcard)\n\t\t/* all else equal, wildcard is always a hit */\n\t\treturn true;\n\n\t/* otherwise, we have to actually compare the data */\n\n\tBUG_ON(!IS_ALIGNED((unsigned long)val, len));\n\n\tswitch (len) {\n\tcase 1:\n\t\t_val = *(u8 *)val;\n\t\tbreak;\n\tcase 2:\n\t\t_val = *(u16 *)val;\n\t\tbreak;\n\tcase 4:\n\t\t_val = *(u32 *)val;\n\t\tbreak;\n\tcase 8:\n\t\t_val = *(u64 *)val;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn _val == p->datamatch ? true : false;\n}\n\n/* MMIO/PIO writes trigger an event if the addr/val match */\nstatic int\nioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,\n\t\tint len, const void *val)\n{\n\tstruct _ioeventfd *p = to_ioeventfd(this);\n\n\tif (!ioeventfd_in_range(p, addr, len, val))\n\t\treturn -EOPNOTSUPP;\n\n\teventfd_signal(p->eventfd, 1);\n\treturn 0;\n}\n\n/*\n * This function is called as KVM is completely shutting down.  We do not\n * need to worry about locking just nuke anything we have as quickly as possible\n */\nstatic void\nioeventfd_destructor(struct kvm_io_device *this)\n{\n\tstruct _ioeventfd *p = to_ioeventfd(this);\n\n\tioeventfd_release(p);\n}\n\nstatic const struct kvm_io_device_ops ioeventfd_ops = {\n\t.write      = ioeventfd_write,\n\t.destructor = ioeventfd_destructor,\n};\n\n/* assumes kvm->slots_lock held */\nstatic bool\nioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)\n{\n\tstruct _ioeventfd *_p;\n\n\tlist_for_each_entry(_p, &kvm->ioeventfds, list)\n\t\tif (_p->bus_idx == p->bus_idx &&\n\t\t    _p->addr == p->addr &&\n\t\t    (!_p->length || !p->length ||\n\t\t     (_p->length == p->length &&\n\t\t      (_p->wildcard || p->wildcard ||\n\t\t       _p->datamatch == p->datamatch))))\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)\n{\n\tif (flags & KVM_IOEVENTFD_FLAG_PIO)\n\t\treturn KVM_PIO_BUS;\n\tif (flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY)\n\t\treturn KVM_VIRTIO_CCW_NOTIFY_BUS;\n\treturn KVM_MMIO_BUS;\n}\n\nstatic int kvm_assign_ioeventfd_idx(struct kvm *kvm,\n\t\t\t\tenum kvm_bus bus_idx,\n\t\t\t\tstruct kvm_ioeventfd *args)\n{\n\n\tstruct eventfd_ctx *eventfd;\n\tstruct _ioeventfd *p;\n\tint ret;\n\n\teventfd = eventfd_ctx_fdget(args->fd);\n\tif (IS_ERR(eventfd))\n\t\treturn PTR_ERR(eventfd);\n\n\tp = kzalloc(sizeof(*p), GFP_KERNEL);\n\tif (!p) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\tINIT_LIST_HEAD(&p->list);\n\tp->addr    = args->addr;\n\tp->bus_idx = bus_idx;\n\tp->length  = args->len;\n\tp->eventfd = eventfd;\n\n\t/* The datamatch feature is optional, otherwise this is a wildcard */\n\tif (args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH)\n\t\tp->datamatch = args->datamatch;\n\telse\n\t\tp->wildcard = true;\n\n\tmutex_lock(&kvm->slots_lock);\n\n\t/* Verify that there isn't a match already */\n\tif (ioeventfd_check_collision(kvm, p)) {\n\t\tret = -EEXIST;\n\t\tgoto unlock_fail;\n\t}\n\n\tkvm_iodevice_init(&p->dev, &ioeventfd_ops);\n\n\tret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,\n\t\t\t\t      &p->dev);\n\tif (ret < 0)\n\t\tgoto unlock_fail;\n\n\tkvm_get_bus(kvm, bus_idx)->ioeventfd_count++;\n\tlist_add_tail(&p->list, &kvm->ioeventfds);\n\n\tmutex_unlock(&kvm->slots_lock);\n\n\treturn 0;\n\nunlock_fail:\n\tmutex_unlock(&kvm->slots_lock);\n\nfail:\n\tkfree(p);\n\teventfd_ctx_put(eventfd);\n\n\treturn ret;\n}\n\nstatic int\nkvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t   struct kvm_ioeventfd *args)\n{\n\tstruct _ioeventfd        *p, *tmp;\n\tstruct eventfd_ctx       *eventfd;\n\tstruct kvm_io_bus\t *bus;\n\tint                       ret = -ENOENT;\n\n\teventfd = eventfd_ctx_fdget(args->fd);\n\tif (IS_ERR(eventfd))\n\t\treturn PTR_ERR(eventfd);\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tlist_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {\n\t\tbool wildcard = !(args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH);\n\n\t\tif (p->bus_idx != bus_idx ||\n\t\t    p->eventfd != eventfd  ||\n\t\t    p->addr != args->addr  ||\n\t\t    p->length != args->len ||\n\t\t    p->wildcard != wildcard)\n\t\t\tcontinue;\n\n\t\tif (!p->wildcard && p->datamatch != args->datamatch)\n\t\t\tcontinue;\n\n\t\tkvm_io_bus_unregister_dev(kvm, bus_idx, &p->dev);\n\t\tbus = kvm_get_bus(kvm, bus_idx);\n\t\tif (bus)\n\t\t\tbus->ioeventfd_count--;\n\t\tioeventfd_release(p);\n\t\tret = 0;\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&kvm->slots_lock);\n\n\teventfd_ctx_put(eventfd);\n\n\treturn ret;\n}\n\nstatic int kvm_deassign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\n{\n\tenum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);\n\tint ret = kvm_deassign_ioeventfd_idx(kvm, bus_idx, args);\n\n\tif (!args->len && bus_idx == KVM_MMIO_BUS)\n\t\tkvm_deassign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);\n\n\treturn ret;\n}\n\nstatic int\nkvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\n{\n\tenum kvm_bus              bus_idx;\n\tint ret;\n\n\tbus_idx = ioeventfd_bus_from_flags(args->flags);\n\t/* must be natural-word sized, or 0 to ignore length */\n\tswitch (args->len) {\n\tcase 0:\n\tcase 1:\n\tcase 2:\n\tcase 4:\n\tcase 8:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* check for range overflow */\n\tif (args->addr + args->len < args->addr)\n\t\treturn -EINVAL;\n\n\t/* check for extra flags that we don't understand */\n\tif (args->flags & ~KVM_IOEVENTFD_VALID_FLAG_MASK)\n\t\treturn -EINVAL;\n\n\t/* ioeventfd with no length can't be combined with DATAMATCH */\n\tif (!args->len && (args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH))\n\t\treturn -EINVAL;\n\n\tret = kvm_assign_ioeventfd_idx(kvm, bus_idx, args);\n\tif (ret)\n\t\tgoto fail;\n\n\t/* When length is ignored, MMIO is also put on a separate bus, for\n\t * faster lookups.\n\t */\n\tif (!args->len && bus_idx == KVM_MMIO_BUS) {\n\t\tret = kvm_assign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);\n\t\tif (ret < 0)\n\t\t\tgoto fast_fail;\n\t}\n\n\treturn 0;\n\nfast_fail:\n\tkvm_deassign_ioeventfd_idx(kvm, bus_idx, args);\nfail:\n\treturn ret;\n}\n\nint\nkvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\n{\n\tif (args->flags & KVM_IOEVENTFD_FLAG_DEASSIGN)\n\t\treturn kvm_deassign_ioeventfd(kvm, args);\n\n\treturn kvm_assign_ioeventfd(kvm, args);\n}\n"], "fixing_code": ["/*\n * kvm eventfd support - use eventfd objects to signal various KVM events\n *\n * Copyright 2009 Novell.  All Rights Reserved.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Author:\n *\tGregory Haskins <ghaskins@novell.com>\n *\n * This file is free software; you can redistribute it and/or modify\n * it under the terms of version 2 of the GNU General Public License\n * as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\t See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software Foundation,\n * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA.\n */\n\n#include <linux/kvm_host.h>\n#include <linux/kvm.h>\n#include <linux/kvm_irqfd.h>\n#include <linux/workqueue.h>\n#include <linux/syscalls.h>\n#include <linux/wait.h>\n#include <linux/poll.h>\n#include <linux/file.h>\n#include <linux/list.h>\n#include <linux/eventfd.h>\n#include <linux/kernel.h>\n#include <linux/srcu.h>\n#include <linux/slab.h>\n#include <linux/seqlock.h>\n#include <linux/irqbypass.h>\n#include <trace/events/kvm.h>\n\n#include <kvm/iodev.h>\n\n#ifdef CONFIG_HAVE_KVM_IRQFD\n\nstatic struct workqueue_struct *irqfd_cleanup_wq;\n\nstatic void\nirqfd_inject(struct work_struct *work)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(work, struct kvm_kernel_irqfd, inject);\n\tstruct kvm *kvm = irqfd->kvm;\n\n\tif (!irqfd->resampler) {\n\t\tkvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 1,\n\t\t\t\tfalse);\n\t\tkvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 0,\n\t\t\t\tfalse);\n\t} else\n\t\tkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t\t    irqfd->gsi, 1, false);\n}\n\n/*\n * Since resampler irqfds share an IRQ source ID, we de-assert once\n * then notify all of the resampler irqfds using this GSI.  We can't\n * do multiple de-asserts or we risk racing with incoming re-asserts.\n */\nstatic void\nirqfd_resampler_ack(struct kvm_irq_ack_notifier *kian)\n{\n\tstruct kvm_kernel_irqfd_resampler *resampler;\n\tstruct kvm *kvm;\n\tstruct kvm_kernel_irqfd *irqfd;\n\tint idx;\n\n\tresampler = container_of(kian,\n\t\t\tstruct kvm_kernel_irqfd_resampler, notifier);\n\tkvm = resampler->kvm;\n\n\tkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t    resampler->notifier.gsi, 0, false);\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\n\tlist_for_each_entry_rcu(irqfd, &resampler->list, resampler_link)\n\t\teventfd_signal(irqfd->resamplefd, 1);\n\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n}\n\nstatic void\nirqfd_resampler_shutdown(struct kvm_kernel_irqfd *irqfd)\n{\n\tstruct kvm_kernel_irqfd_resampler *resampler = irqfd->resampler;\n\tstruct kvm *kvm = resampler->kvm;\n\n\tmutex_lock(&kvm->irqfds.resampler_lock);\n\n\tlist_del_rcu(&irqfd->resampler_link);\n\tsynchronize_srcu(&kvm->irq_srcu);\n\n\tif (list_empty(&resampler->list)) {\n\t\tlist_del(&resampler->link);\n\t\tkvm_unregister_irq_ack_notifier(kvm, &resampler->notifier);\n\t\tkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t\t    resampler->notifier.gsi, 0, false);\n\t\tkfree(resampler);\n\t}\n\n\tmutex_unlock(&kvm->irqfds.resampler_lock);\n}\n\n/*\n * Race-free decouple logic (ordering is critical)\n */\nstatic void\nirqfd_shutdown(struct work_struct *work)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(work, struct kvm_kernel_irqfd, shutdown);\n\tu64 cnt;\n\n\t/*\n\t * Synchronize with the wait-queue and unhook ourselves to prevent\n\t * further events.\n\t */\n\teventfd_ctx_remove_wait_queue(irqfd->eventfd, &irqfd->wait, &cnt);\n\n\t/*\n\t * We know no new events will be scheduled at this point, so block\n\t * until all previously outstanding events have completed\n\t */\n\tflush_work(&irqfd->inject);\n\n\tif (irqfd->resampler) {\n\t\tirqfd_resampler_shutdown(irqfd);\n\t\teventfd_ctx_put(irqfd->resamplefd);\n\t}\n\n\t/*\n\t * It is now safe to release the object's resources\n\t */\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\n\tirq_bypass_unregister_consumer(&irqfd->consumer);\n#endif\n\teventfd_ctx_put(irqfd->eventfd);\n\tkfree(irqfd);\n}\n\n\n/* assumes kvm->irqfds.lock is held */\nstatic bool\nirqfd_is_active(struct kvm_kernel_irqfd *irqfd)\n{\n\treturn list_empty(&irqfd->list) ? false : true;\n}\n\n/*\n * Mark the irqfd as inactive and schedule it for removal\n *\n * assumes kvm->irqfds.lock is held\n */\nstatic void\nirqfd_deactivate(struct kvm_kernel_irqfd *irqfd)\n{\n\tBUG_ON(!irqfd_is_active(irqfd));\n\n\tlist_del_init(&irqfd->list);\n\n\tqueue_work(irqfd_cleanup_wq, &irqfd->shutdown);\n}\n\nint __attribute__((weak)) kvm_arch_set_irq_inatomic(\n\t\t\t\tstruct kvm_kernel_irq_routing_entry *irq,\n\t\t\t\tstruct kvm *kvm, int irq_source_id,\n\t\t\t\tint level,\n\t\t\t\tbool line_status)\n{\n\treturn -EWOULDBLOCK;\n}\n\n/*\n * Called with wqh->lock held and interrupts disabled\n */\nstatic int\nirqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(wait, struct kvm_kernel_irqfd, wait);\n\tunsigned long flags = (unsigned long)key;\n\tstruct kvm_kernel_irq_routing_entry irq;\n\tstruct kvm *kvm = irqfd->kvm;\n\tunsigned seq;\n\tint idx;\n\n\tif (flags & POLLIN) {\n\t\tidx = srcu_read_lock(&kvm->irq_srcu);\n\t\tdo {\n\t\t\tseq = read_seqcount_begin(&irqfd->irq_entry_sc);\n\t\t\tirq = irqfd->irq_entry;\n\t\t} while (read_seqcount_retry(&irqfd->irq_entry_sc, seq));\n\t\t/* An event has been signaled, inject an interrupt */\n\t\tif (kvm_arch_set_irq_inatomic(&irq, kvm,\n\t\t\t\t\t      KVM_USERSPACE_IRQ_SOURCE_ID, 1,\n\t\t\t\t\t      false) == -EWOULDBLOCK)\n\t\t\tschedule_work(&irqfd->inject);\n\t\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\t}\n\n\tif (flags & POLLHUP) {\n\t\t/* The eventfd is closing, detach from KVM */\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&kvm->irqfds.lock, flags);\n\n\t\t/*\n\t\t * We must check if someone deactivated the irqfd before\n\t\t * we could acquire the irqfds.lock since the item is\n\t\t * deactivated from the KVM side before it is unhooked from\n\t\t * the wait-queue.  If it is already deactivated, we can\n\t\t * simply return knowing the other side will cleanup for us.\n\t\t * We cannot race against the irqfd going away since the\n\t\t * other side is required to acquire wqh->lock, which we hold\n\t\t */\n\t\tif (irqfd_is_active(irqfd))\n\t\t\tirqfd_deactivate(irqfd);\n\n\t\tspin_unlock_irqrestore(&kvm->irqfds.lock, flags);\n\t}\n\n\treturn 0;\n}\n\nstatic void\nirqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh,\n\t\t\tpoll_table *pt)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(pt, struct kvm_kernel_irqfd, pt);\n\tadd_wait_queue(wqh, &irqfd->wait);\n}\n\n/* Must be called under irqfds.lock */\nstatic void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)\n{\n\tstruct kvm_kernel_irq_routing_entry *e;\n\tstruct kvm_kernel_irq_routing_entry entries[KVM_NR_IRQCHIPS];\n\tint n_entries;\n\n\tn_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);\n\n\twrite_seqcount_begin(&irqfd->irq_entry_sc);\n\n\te = entries;\n\tif (n_entries == 1)\n\t\tirqfd->irq_entry = *e;\n\telse\n\t\tirqfd->irq_entry.type = 0;\n\n\twrite_seqcount_end(&irqfd->irq_entry_sc);\n}\n\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\nvoid __attribute__((weak)) kvm_arch_irq_bypass_stop(\n\t\t\t\tstruct irq_bypass_consumer *cons)\n{\n}\n\nvoid __attribute__((weak)) kvm_arch_irq_bypass_start(\n\t\t\t\tstruct irq_bypass_consumer *cons)\n{\n}\n\nint  __attribute__((weak)) kvm_arch_update_irqfd_routing(\n\t\t\t\tstruct kvm *kvm, unsigned int host_irq,\n\t\t\t\tuint32_t guest_irq, bool set)\n{\n\treturn 0;\n}\n#endif\n\nstatic int\nkvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\tstruct kvm_kernel_irqfd *irqfd, *tmp;\n\tstruct fd f;\n\tstruct eventfd_ctx *eventfd = NULL, *resamplefd = NULL;\n\tint ret;\n\tunsigned int events;\n\tint idx;\n\n\tif (!kvm_arch_intc_initialized(kvm))\n\t\treturn -EAGAIN;\n\n\tirqfd = kzalloc(sizeof(*irqfd), GFP_KERNEL);\n\tif (!irqfd)\n\t\treturn -ENOMEM;\n\n\tirqfd->kvm = kvm;\n\tirqfd->gsi = args->gsi;\n\tINIT_LIST_HEAD(&irqfd->list);\n\tINIT_WORK(&irqfd->inject, irqfd_inject);\n\tINIT_WORK(&irqfd->shutdown, irqfd_shutdown);\n\tseqcount_init(&irqfd->irq_entry_sc);\n\n\tf = fdget(args->fd);\n\tif (!f.file) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\teventfd = eventfd_ctx_fileget(f.file);\n\tif (IS_ERR(eventfd)) {\n\t\tret = PTR_ERR(eventfd);\n\t\tgoto fail;\n\t}\n\n\tirqfd->eventfd = eventfd;\n\n\tif (args->flags & KVM_IRQFD_FLAG_RESAMPLE) {\n\t\tstruct kvm_kernel_irqfd_resampler *resampler;\n\n\t\tresamplefd = eventfd_ctx_fdget(args->resamplefd);\n\t\tif (IS_ERR(resamplefd)) {\n\t\t\tret = PTR_ERR(resamplefd);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tirqfd->resamplefd = resamplefd;\n\t\tINIT_LIST_HEAD(&irqfd->resampler_link);\n\n\t\tmutex_lock(&kvm->irqfds.resampler_lock);\n\n\t\tlist_for_each_entry(resampler,\n\t\t\t\t    &kvm->irqfds.resampler_list, link) {\n\t\t\tif (resampler->notifier.gsi == irqfd->gsi) {\n\t\t\t\tirqfd->resampler = resampler;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!irqfd->resampler) {\n\t\t\tresampler = kzalloc(sizeof(*resampler), GFP_KERNEL);\n\t\t\tif (!resampler) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tmutex_unlock(&kvm->irqfds.resampler_lock);\n\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tresampler->kvm = kvm;\n\t\t\tINIT_LIST_HEAD(&resampler->list);\n\t\t\tresampler->notifier.gsi = irqfd->gsi;\n\t\t\tresampler->notifier.irq_acked = irqfd_resampler_ack;\n\t\t\tINIT_LIST_HEAD(&resampler->link);\n\n\t\t\tlist_add(&resampler->link, &kvm->irqfds.resampler_list);\n\t\t\tkvm_register_irq_ack_notifier(kvm,\n\t\t\t\t\t\t      &resampler->notifier);\n\t\t\tirqfd->resampler = resampler;\n\t\t}\n\n\t\tlist_add_rcu(&irqfd->resampler_link, &irqfd->resampler->list);\n\t\tsynchronize_srcu(&kvm->irq_srcu);\n\n\t\tmutex_unlock(&kvm->irqfds.resampler_lock);\n\t}\n\n\t/*\n\t * Install our own custom wake-up handling so we are notified via\n\t * a callback whenever someone signals the underlying eventfd\n\t */\n\tinit_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup);\n\tinit_poll_funcptr(&irqfd->pt, irqfd_ptable_queue_proc);\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tret = 0;\n\tlist_for_each_entry(tmp, &kvm->irqfds.items, list) {\n\t\tif (irqfd->eventfd != tmp->eventfd)\n\t\t\tcontinue;\n\t\t/* This fd is used for another irq already. */\n\t\tret = -EBUSY;\n\t\tspin_unlock_irq(&kvm->irqfds.lock);\n\t\tgoto fail;\n\t}\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tirqfd_update(kvm, irqfd);\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\n\tlist_add_tail(&irqfd->list, &kvm->irqfds.items);\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n\n\t/*\n\t * Check if there was an event already pending on the eventfd\n\t * before we registered, and trigger it as if we didn't miss it.\n\t */\n\tevents = f.file->f_op->poll(f.file, &irqfd->pt);\n\n\tif (events & POLLIN)\n\t\tschedule_work(&irqfd->inject);\n\n\t/*\n\t * do not drop the file until the irqfd is fully initialized, otherwise\n\t * we might race against the POLLHUP\n\t */\n\tfdput(f);\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\n\tif (kvm_arch_has_irq_bypass()) {\n\t\tirqfd->consumer.token = (void *)irqfd->eventfd;\n\t\tirqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;\n\t\tirqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;\n\t\tirqfd->consumer.stop = kvm_arch_irq_bypass_stop;\n\t\tirqfd->consumer.start = kvm_arch_irq_bypass_start;\n\t\tret = irq_bypass_register_consumer(&irqfd->consumer);\n\t\tif (ret)\n\t\t\tpr_info(\"irq bypass consumer (token %p) registration fails: %d\\n\",\n\t\t\t\tirqfd->consumer.token, ret);\n\t}\n#endif\n\n\treturn 0;\n\nfail:\n\tif (irqfd->resampler)\n\t\tirqfd_resampler_shutdown(irqfd);\n\n\tif (resamplefd && !IS_ERR(resamplefd))\n\t\teventfd_ctx_put(resamplefd);\n\n\tif (eventfd && !IS_ERR(eventfd))\n\t\teventfd_ctx_put(eventfd);\n\n\tfdput(f);\n\nout:\n\tkfree(irqfd);\n\treturn ret;\n}\n\nbool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin)\n{\n\tstruct kvm_irq_ack_notifier *kian;\n\tint gsi, idx;\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tgsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);\n\tif (gsi != -1)\n\t\thlist_for_each_entry_rcu(kian, &kvm->irq_ack_notifier_list,\n\t\t\t\t\t link)\n\t\t\tif (kian->gsi == gsi) {\n\t\t\t\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\t\t\t\treturn true;\n\t\t\t}\n\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(kvm_irq_has_notifier);\n\nvoid kvm_notify_acked_gsi(struct kvm *kvm, int gsi)\n{\n\tstruct kvm_irq_ack_notifier *kian;\n\n\thlist_for_each_entry_rcu(kian, &kvm->irq_ack_notifier_list,\n\t\t\t\t link)\n\t\tif (kian->gsi == gsi)\n\t\t\tkian->irq_acked(kian);\n}\n\nvoid kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin)\n{\n\tint gsi, idx;\n\n\ttrace_kvm_ack_irq(irqchip, pin);\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tgsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);\n\tif (gsi != -1)\n\t\tkvm_notify_acked_gsi(kvm, gsi);\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n}\n\nvoid kvm_register_irq_ack_notifier(struct kvm *kvm,\n\t\t\t\t   struct kvm_irq_ack_notifier *kian)\n{\n\tmutex_lock(&kvm->irq_lock);\n\thlist_add_head_rcu(&kian->link, &kvm->irq_ack_notifier_list);\n\tmutex_unlock(&kvm->irq_lock);\n\tkvm_arch_post_irq_ack_notifier_list_update(kvm);\n}\n\nvoid kvm_unregister_irq_ack_notifier(struct kvm *kvm,\n\t\t\t\t    struct kvm_irq_ack_notifier *kian)\n{\n\tmutex_lock(&kvm->irq_lock);\n\thlist_del_init_rcu(&kian->link);\n\tmutex_unlock(&kvm->irq_lock);\n\tsynchronize_srcu(&kvm->irq_srcu);\n\tkvm_arch_post_irq_ack_notifier_list_update(kvm);\n}\n#endif\n\nvoid\nkvm_eventfd_init(struct kvm *kvm)\n{\n#ifdef CONFIG_HAVE_KVM_IRQFD\n\tspin_lock_init(&kvm->irqfds.lock);\n\tINIT_LIST_HEAD(&kvm->irqfds.items);\n\tINIT_LIST_HEAD(&kvm->irqfds.resampler_list);\n\tmutex_init(&kvm->irqfds.resampler_lock);\n#endif\n\tINIT_LIST_HEAD(&kvm->ioeventfds);\n}\n\n#ifdef CONFIG_HAVE_KVM_IRQFD\n/*\n * shutdown any irqfd's that match fd+gsi\n */\nstatic int\nkvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\tstruct kvm_kernel_irqfd *irqfd, *tmp;\n\tstruct eventfd_ctx *eventfd;\n\n\teventfd = eventfd_ctx_fdget(args->fd);\n\tif (IS_ERR(eventfd))\n\t\treturn PTR_ERR(eventfd);\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tlist_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list) {\n\t\tif (irqfd->eventfd == eventfd && irqfd->gsi == args->gsi) {\n\t\t\t/*\n\t\t\t * This clearing of irq_entry.type is needed for when\n\t\t\t * another thread calls kvm_irq_routing_update before\n\t\t\t * we flush workqueue below (we synchronize with\n\t\t\t * kvm_irq_routing_update using irqfds.lock).\n\t\t\t */\n\t\t\twrite_seqcount_begin(&irqfd->irq_entry_sc);\n\t\t\tirqfd->irq_entry.type = 0;\n\t\t\twrite_seqcount_end(&irqfd->irq_entry_sc);\n\t\t\tirqfd_deactivate(irqfd);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n\teventfd_ctx_put(eventfd);\n\n\t/*\n\t * Block until we know all outstanding shutdown jobs have completed\n\t * so that we guarantee there will not be any more interrupts on this\n\t * gsi once this deassign function returns.\n\t */\n\tflush_workqueue(irqfd_cleanup_wq);\n\n\treturn 0;\n}\n\nint\nkvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\tif (args->flags & ~(KVM_IRQFD_FLAG_DEASSIGN | KVM_IRQFD_FLAG_RESAMPLE))\n\t\treturn -EINVAL;\n\tif (args->gsi >= KVM_MAX_IRQ_ROUTES)\n\t\treturn -EINVAL;\n\n\tif (args->flags & KVM_IRQFD_FLAG_DEASSIGN)\n\t\treturn kvm_irqfd_deassign(kvm, args);\n\n\treturn kvm_irqfd_assign(kvm, args);\n}\n\n/*\n * This function is called as the kvm VM fd is being released. Shutdown all\n * irqfds that still remain open\n */\nvoid\nkvm_irqfd_release(struct kvm *kvm)\n{\n\tstruct kvm_kernel_irqfd *irqfd, *tmp;\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tlist_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list)\n\t\tirqfd_deactivate(irqfd);\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n\n\t/*\n\t * Block until we know all outstanding shutdown jobs have completed\n\t * since we do not take a kvm* reference.\n\t */\n\tflush_workqueue(irqfd_cleanup_wq);\n\n}\n\n/*\n * Take note of a change in irq routing.\n * Caller must invoke synchronize_srcu(&kvm->irq_srcu) afterwards.\n */\nvoid kvm_irq_routing_update(struct kvm *kvm)\n{\n\tstruct kvm_kernel_irqfd *irqfd;\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tlist_for_each_entry(irqfd, &kvm->irqfds.items, list) {\n\t\tirqfd_update(kvm, irqfd);\n\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\n\t\tif (irqfd->producer) {\n\t\t\tint ret = kvm_arch_update_irqfd_routing(\n\t\t\t\t\tirqfd->kvm, irqfd->producer->irq,\n\t\t\t\t\tirqfd->gsi, 1);\n\t\t\tWARN_ON(ret);\n\t\t}\n#endif\n\t}\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n}\n\n/*\n * create a host-wide workqueue for issuing deferred shutdown requests\n * aggregated from all vm* instances. We need our own isolated\n * queue to ease flushing work items when a VM exits.\n */\nint kvm_irqfd_init(void)\n{\n\tirqfd_cleanup_wq = alloc_workqueue(\"kvm-irqfd-cleanup\", 0, 0);\n\tif (!irqfd_cleanup_wq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid kvm_irqfd_exit(void)\n{\n\tdestroy_workqueue(irqfd_cleanup_wq);\n}\n#endif\n\n/*\n * --------------------------------------------------------------------\n * ioeventfd: translate a PIO/MMIO memory write to an eventfd signal.\n *\n * userspace can register a PIO/MMIO address with an eventfd for receiving\n * notification when the memory has been touched.\n * --------------------------------------------------------------------\n */\n\nstruct _ioeventfd {\n\tstruct list_head     list;\n\tu64                  addr;\n\tint                  length;\n\tstruct eventfd_ctx  *eventfd;\n\tu64                  datamatch;\n\tstruct kvm_io_device dev;\n\tu8                   bus_idx;\n\tbool                 wildcard;\n};\n\nstatic inline struct _ioeventfd *\nto_ioeventfd(struct kvm_io_device *dev)\n{\n\treturn container_of(dev, struct _ioeventfd, dev);\n}\n\nstatic void\nioeventfd_release(struct _ioeventfd *p)\n{\n\teventfd_ctx_put(p->eventfd);\n\tlist_del(&p->list);\n\tkfree(p);\n}\n\nstatic bool\nioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)\n{\n\tu64 _val;\n\n\tif (addr != p->addr)\n\t\t/* address must be precise for a hit */\n\t\treturn false;\n\n\tif (!p->length)\n\t\t/* length = 0 means only look at the address, so always a hit */\n\t\treturn true;\n\n\tif (len != p->length)\n\t\t/* address-range must be precise for a hit */\n\t\treturn false;\n\n\tif (p->wildcard)\n\t\t/* all else equal, wildcard is always a hit */\n\t\treturn true;\n\n\t/* otherwise, we have to actually compare the data */\n\n\tBUG_ON(!IS_ALIGNED((unsigned long)val, len));\n\n\tswitch (len) {\n\tcase 1:\n\t\t_val = *(u8 *)val;\n\t\tbreak;\n\tcase 2:\n\t\t_val = *(u16 *)val;\n\t\tbreak;\n\tcase 4:\n\t\t_val = *(u32 *)val;\n\t\tbreak;\n\tcase 8:\n\t\t_val = *(u64 *)val;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn _val == p->datamatch ? true : false;\n}\n\n/* MMIO/PIO writes trigger an event if the addr/val match */\nstatic int\nioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,\n\t\tint len, const void *val)\n{\n\tstruct _ioeventfd *p = to_ioeventfd(this);\n\n\tif (!ioeventfd_in_range(p, addr, len, val))\n\t\treturn -EOPNOTSUPP;\n\n\teventfd_signal(p->eventfd, 1);\n\treturn 0;\n}\n\n/*\n * This function is called as KVM is completely shutting down.  We do not\n * need to worry about locking just nuke anything we have as quickly as possible\n */\nstatic void\nioeventfd_destructor(struct kvm_io_device *this)\n{\n\tstruct _ioeventfd *p = to_ioeventfd(this);\n\n\tioeventfd_release(p);\n}\n\nstatic const struct kvm_io_device_ops ioeventfd_ops = {\n\t.write      = ioeventfd_write,\n\t.destructor = ioeventfd_destructor,\n};\n\n/* assumes kvm->slots_lock held */\nstatic bool\nioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)\n{\n\tstruct _ioeventfd *_p;\n\n\tlist_for_each_entry(_p, &kvm->ioeventfds, list)\n\t\tif (_p->bus_idx == p->bus_idx &&\n\t\t    _p->addr == p->addr &&\n\t\t    (!_p->length || !p->length ||\n\t\t     (_p->length == p->length &&\n\t\t      (_p->wildcard || p->wildcard ||\n\t\t       _p->datamatch == p->datamatch))))\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)\n{\n\tif (flags & KVM_IOEVENTFD_FLAG_PIO)\n\t\treturn KVM_PIO_BUS;\n\tif (flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY)\n\t\treturn KVM_VIRTIO_CCW_NOTIFY_BUS;\n\treturn KVM_MMIO_BUS;\n}\n\nstatic int kvm_assign_ioeventfd_idx(struct kvm *kvm,\n\t\t\t\tenum kvm_bus bus_idx,\n\t\t\t\tstruct kvm_ioeventfd *args)\n{\n\n\tstruct eventfd_ctx *eventfd;\n\tstruct _ioeventfd *p;\n\tint ret;\n\n\teventfd = eventfd_ctx_fdget(args->fd);\n\tif (IS_ERR(eventfd))\n\t\treturn PTR_ERR(eventfd);\n\n\tp = kzalloc(sizeof(*p), GFP_KERNEL);\n\tif (!p) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\tINIT_LIST_HEAD(&p->list);\n\tp->addr    = args->addr;\n\tp->bus_idx = bus_idx;\n\tp->length  = args->len;\n\tp->eventfd = eventfd;\n\n\t/* The datamatch feature is optional, otherwise this is a wildcard */\n\tif (args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH)\n\t\tp->datamatch = args->datamatch;\n\telse\n\t\tp->wildcard = true;\n\n\tmutex_lock(&kvm->slots_lock);\n\n\t/* Verify that there isn't a match already */\n\tif (ioeventfd_check_collision(kvm, p)) {\n\t\tret = -EEXIST;\n\t\tgoto unlock_fail;\n\t}\n\n\tkvm_iodevice_init(&p->dev, &ioeventfd_ops);\n\n\tret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,\n\t\t\t\t      &p->dev);\n\tif (ret < 0)\n\t\tgoto unlock_fail;\n\n\tkvm_get_bus(kvm, bus_idx)->ioeventfd_count++;\n\tlist_add_tail(&p->list, &kvm->ioeventfds);\n\n\tmutex_unlock(&kvm->slots_lock);\n\n\treturn 0;\n\nunlock_fail:\n\tmutex_unlock(&kvm->slots_lock);\n\nfail:\n\tkfree(p);\n\teventfd_ctx_put(eventfd);\n\n\treturn ret;\n}\n\nstatic int\nkvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t   struct kvm_ioeventfd *args)\n{\n\tstruct _ioeventfd        *p, *tmp;\n\tstruct eventfd_ctx       *eventfd;\n\tstruct kvm_io_bus\t *bus;\n\tint                       ret = -ENOENT;\n\n\teventfd = eventfd_ctx_fdget(args->fd);\n\tif (IS_ERR(eventfd))\n\t\treturn PTR_ERR(eventfd);\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tlist_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {\n\t\tbool wildcard = !(args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH);\n\n\t\tif (p->bus_idx != bus_idx ||\n\t\t    p->eventfd != eventfd  ||\n\t\t    p->addr != args->addr  ||\n\t\t    p->length != args->len ||\n\t\t    p->wildcard != wildcard)\n\t\t\tcontinue;\n\n\t\tif (!p->wildcard && p->datamatch != args->datamatch)\n\t\t\tcontinue;\n\n\t\tkvm_io_bus_unregister_dev(kvm, bus_idx, &p->dev);\n\t\tbus = kvm_get_bus(kvm, bus_idx);\n\t\tif (bus)\n\t\t\tbus->ioeventfd_count--;\n\t\tioeventfd_release(p);\n\t\tret = 0;\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&kvm->slots_lock);\n\n\teventfd_ctx_put(eventfd);\n\n\treturn ret;\n}\n\nstatic int kvm_deassign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\n{\n\tenum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);\n\tint ret = kvm_deassign_ioeventfd_idx(kvm, bus_idx, args);\n\n\tif (!args->len && bus_idx == KVM_MMIO_BUS)\n\t\tkvm_deassign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);\n\n\treturn ret;\n}\n\nstatic int\nkvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\n{\n\tenum kvm_bus              bus_idx;\n\tint ret;\n\n\tbus_idx = ioeventfd_bus_from_flags(args->flags);\n\t/* must be natural-word sized, or 0 to ignore length */\n\tswitch (args->len) {\n\tcase 0:\n\tcase 1:\n\tcase 2:\n\tcase 4:\n\tcase 8:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* check for range overflow */\n\tif (args->addr + args->len < args->addr)\n\t\treturn -EINVAL;\n\n\t/* check for extra flags that we don't understand */\n\tif (args->flags & ~KVM_IOEVENTFD_VALID_FLAG_MASK)\n\t\treturn -EINVAL;\n\n\t/* ioeventfd with no length can't be combined with DATAMATCH */\n\tif (!args->len && (args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH))\n\t\treturn -EINVAL;\n\n\tret = kvm_assign_ioeventfd_idx(kvm, bus_idx, args);\n\tif (ret)\n\t\tgoto fail;\n\n\t/* When length is ignored, MMIO is also put on a separate bus, for\n\t * faster lookups.\n\t */\n\tif (!args->len && bus_idx == KVM_MMIO_BUS) {\n\t\tret = kvm_assign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);\n\t\tif (ret < 0)\n\t\t\tgoto fast_fail;\n\t}\n\n\treturn 0;\n\nfast_fail:\n\tkvm_deassign_ioeventfd_idx(kvm, bus_idx, args);\nfail:\n\treturn ret;\n}\n\nint\nkvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\n{\n\tif (args->flags & KVM_IOEVENTFD_FLAG_DEASSIGN)\n\t\treturn kvm_deassign_ioeventfd(kvm, args);\n\n\treturn kvm_assign_ioeventfd(kvm, args);\n}\n"], "filenames": ["virt/kvm/eventfd.c"], "buggy_code_start_loc": [567], "buggy_code_end_loc": [567], "fixing_code_start_loc": [568], "fixing_code_end_loc": [570], "type": "CWE-20", "message": "The KVM subsystem in the Linux kernel through 4.13.3 allows guest OS users to cause a denial of service (assertion failure, and hypervisor hang or crash) via an out-of bounds guest_irq value, related to arch/x86/kvm/vmx.c and virt/kvm/eventfd.c.", "other": {"cve": {"id": "CVE-2017-1000252", "sourceIdentifier": "cve@mitre.org", "published": "2017-09-26T05:29:00.213", "lastModified": "2019-10-03T00:03:26.223", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The KVM subsystem in the Linux kernel through 4.13.3 allows guest OS users to cause a denial of service (assertion failure, and hypervisor hang or crash) via an out-of bounds guest_irq value, related to arch/x86/kvm/vmx.c and virt/kvm/eventfd.c."}, {"lang": "es", "value": "El subsistema KVM en el kernel de Linux hasta la versi\u00f3n 4.13.3 permite que los usuarios invitados del sistema operativo provoquen una denegaci\u00f3n de servicio (fallo de aserci\u00f3n y bloqueo o cierre inesperado del hipervisor) mediante un valor guest_irq fuera de l\u00edmites, relacionado con arch/x86/kvm/vmx.c y virt/kvm/eventfd.c."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}, {"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.13.3", "matchCriteriaId": "FA1E2C32-ED0D-4E2B-A313-448CC0545ED2"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=36ae3c0a36b7456432fedce38ae2f7bd3e01a563", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=3a8b0677fc6180a467e26cc32ce6b0c09a32f9bb", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.debian.org/security/2017/dsa-3981", "source": "cve@mitre.org"}, {"url": "http://www.openwall.com/lists/oss-security/2017/09/15/4", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/101022", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2018:0676", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2018:1062", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2018:1130", "source": "cve@mitre.org"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1490781", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/36ae3c0a36b7456432fedce38ae2f7bd3e01a563", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/3a8b0677fc6180a467e26cc32ce6b0c09a32f9bb", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://marc.info/?l=kvm&m=150549145711115&w=2", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://marc.info/?l=kvm&m=150549146311117&w=2", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/36ae3c0a36b7456432fedce38ae2f7bd3e01a563"}}