{"buggy_code": ["/*\n * Copyright (c) 2006 Oracle.  All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/in.h>\n#include <linux/if.h>\n#include <linux/netdevice.h>\n#include <linux/inetdevice.h>\n#include <linux/if_arp.h>\n#include <linux/delay.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n\n#include \"rds.h\"\n#include \"ib.h\"\n\nstatic unsigned int fmr_pool_size = RDS_FMR_POOL_SIZE;\nunsigned int fmr_message_size = RDS_FMR_SIZE + 1; /* +1 allows for unaligned MRs */\nunsigned int rds_ib_retry_count = RDS_IB_DEFAULT_RETRY_COUNT;\n\nmodule_param(fmr_pool_size, int, 0444);\nMODULE_PARM_DESC(fmr_pool_size, \" Max number of fmr per HCA\");\nmodule_param(fmr_message_size, int, 0444);\nMODULE_PARM_DESC(fmr_message_size, \" Max size of a RDMA transfer\");\nmodule_param(rds_ib_retry_count, int, 0444);\nMODULE_PARM_DESC(rds_ib_retry_count, \" Number of hw retries before reporting an error\");\n\n/*\n * we have a clumsy combination of RCU and a rwsem protecting this list\n * because it is used both in the get_mr fast path and while blocking in\n * the FMR flushing path.\n */\nDECLARE_RWSEM(rds_ib_devices_lock);\nstruct list_head rds_ib_devices;\n\n/* NOTE: if also grabbing ibdev lock, grab this first */\nDEFINE_SPINLOCK(ib_nodev_conns_lock);\nLIST_HEAD(ib_nodev_conns);\n\nstatic void rds_ib_nodev_connect(void)\n{\n\tstruct rds_ib_connection *ic;\n\n\tspin_lock(&ib_nodev_conns_lock);\n\tlist_for_each_entry(ic, &ib_nodev_conns, ib_node)\n\t\trds_conn_connect_if_down(ic->conn);\n\tspin_unlock(&ib_nodev_conns_lock);\n}\n\nstatic void rds_ib_dev_shutdown(struct rds_ib_device *rds_ibdev)\n{\n\tstruct rds_ib_connection *ic;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rds_ibdev->spinlock, flags);\n\tlist_for_each_entry(ic, &rds_ibdev->conn_list, ib_node)\n\t\trds_conn_drop(ic->conn);\n\tspin_unlock_irqrestore(&rds_ibdev->spinlock, flags);\n}\n\n/*\n * rds_ib_destroy_mr_pool() blocks on a few things and mrs drop references\n * from interrupt context so we push freing off into a work struct in krdsd.\n */\nstatic void rds_ib_dev_free(struct work_struct *work)\n{\n\tstruct rds_ib_ipaddr *i_ipaddr, *i_next;\n\tstruct rds_ib_device *rds_ibdev = container_of(work,\n\t\t\t\t\tstruct rds_ib_device, free_work);\n\n\tif (rds_ibdev->mr_pool)\n\t\trds_ib_destroy_mr_pool(rds_ibdev->mr_pool);\n\tif (rds_ibdev->mr)\n\t\tib_dereg_mr(rds_ibdev->mr);\n\tif (rds_ibdev->pd)\n\t\tib_dealloc_pd(rds_ibdev->pd);\n\n\tlist_for_each_entry_safe(i_ipaddr, i_next, &rds_ibdev->ipaddr_list, list) {\n\t\tlist_del(&i_ipaddr->list);\n\t\tkfree(i_ipaddr);\n\t}\n\n\tkfree(rds_ibdev);\n}\n\nvoid rds_ib_dev_put(struct rds_ib_device *rds_ibdev)\n{\n\tBUG_ON(atomic_read(&rds_ibdev->refcount) <= 0);\n\tif (atomic_dec_and_test(&rds_ibdev->refcount))\n\t\tqueue_work(rds_wq, &rds_ibdev->free_work);\n}\n\nstatic void rds_ib_add_one(struct ib_device *device)\n{\n\tstruct rds_ib_device *rds_ibdev;\n\tstruct ib_device_attr *dev_attr;\n\n\t/* Only handle IB (no iWARP) devices */\n\tif (device->node_type != RDMA_NODE_IB_CA)\n\t\treturn;\n\n\tdev_attr = kmalloc(sizeof *dev_attr, GFP_KERNEL);\n\tif (!dev_attr)\n\t\treturn;\n\n\tif (ib_query_device(device, dev_attr)) {\n\t\trdsdebug(\"Query device failed for %s\\n\", device->name);\n\t\tgoto free_attr;\n\t}\n\n\trds_ibdev = kzalloc_node(sizeof(struct rds_ib_device), GFP_KERNEL,\n\t\t\t\t ibdev_to_node(device));\n\tif (!rds_ibdev)\n\t\tgoto free_attr;\n\n\tspin_lock_init(&rds_ibdev->spinlock);\n\tatomic_set(&rds_ibdev->refcount, 1);\n\tINIT_WORK(&rds_ibdev->free_work, rds_ib_dev_free);\n\n\trds_ibdev->max_wrs = dev_attr->max_qp_wr;\n\trds_ibdev->max_sge = min(dev_attr->max_sge, RDS_IB_MAX_SGE);\n\n\trds_ibdev->fmr_max_remaps = dev_attr->max_map_per_fmr?: 32;\n\trds_ibdev->max_fmrs = dev_attr->max_fmr ?\n\t\t\tmin_t(unsigned int, dev_attr->max_fmr, fmr_pool_size) :\n\t\t\tfmr_pool_size;\n\n\trds_ibdev->max_initiator_depth = dev_attr->max_qp_init_rd_atom;\n\trds_ibdev->max_responder_resources = dev_attr->max_qp_rd_atom;\n\n\trds_ibdev->dev = device;\n\trds_ibdev->pd = ib_alloc_pd(device);\n\tif (IS_ERR(rds_ibdev->pd)) {\n\t\trds_ibdev->pd = NULL;\n\t\tgoto put_dev;\n\t}\n\n\trds_ibdev->mr = ib_get_dma_mr(rds_ibdev->pd, IB_ACCESS_LOCAL_WRITE);\n\tif (IS_ERR(rds_ibdev->mr)) {\n\t\trds_ibdev->mr = NULL;\n\t\tgoto put_dev;\n\t}\n\n\trds_ibdev->mr_pool = rds_ib_create_mr_pool(rds_ibdev);\n\tif (IS_ERR(rds_ibdev->mr_pool)) {\n\t\trds_ibdev->mr_pool = NULL;\n\t\tgoto put_dev;\n\t}\n\n\tINIT_LIST_HEAD(&rds_ibdev->ipaddr_list);\n\tINIT_LIST_HEAD(&rds_ibdev->conn_list);\n\n\tdown_write(&rds_ib_devices_lock);\n\tlist_add_tail_rcu(&rds_ibdev->list, &rds_ib_devices);\n\tup_write(&rds_ib_devices_lock);\n\tatomic_inc(&rds_ibdev->refcount);\n\n\tib_set_client_data(device, &rds_ib_client, rds_ibdev);\n\tatomic_inc(&rds_ibdev->refcount);\n\n\trds_ib_nodev_connect();\n\nput_dev:\n\trds_ib_dev_put(rds_ibdev);\nfree_attr:\n\tkfree(dev_attr);\n}\n\n/*\n * New connections use this to find the device to associate with the\n * connection.  It's not in the fast path so we're not concerned about the\n * performance of the IB call.  (As of this writing, it uses an interrupt\n * blocking spinlock to serialize walking a per-device list of all registered\n * clients.)\n *\n * RCU is used to handle incoming connections racing with device teardown.\n * Rather than use a lock to serialize removal from the client_data and\n * getting a new reference, we use an RCU grace period.  The destruction\n * path removes the device from client_data and then waits for all RCU\n * readers to finish.\n *\n * A new connection can get NULL from this if its arriving on a\n * device that is in the process of being removed.\n */\nstruct rds_ib_device *rds_ib_get_client_data(struct ib_device *device)\n{\n\tstruct rds_ib_device *rds_ibdev;\n\n\trcu_read_lock();\n\trds_ibdev = ib_get_client_data(device, &rds_ib_client);\n\tif (rds_ibdev)\n\t\tatomic_inc(&rds_ibdev->refcount);\n\trcu_read_unlock();\n\treturn rds_ibdev;\n}\n\n/*\n * The IB stack is letting us know that a device is going away.  This can\n * happen if the underlying HCA driver is removed or if PCI hotplug is removing\n * the pci function, for example.\n *\n * This can be called at any time and can be racing with any other RDS path.\n */\nstatic void rds_ib_remove_one(struct ib_device *device)\n{\n\tstruct rds_ib_device *rds_ibdev;\n\n\trds_ibdev = ib_get_client_data(device, &rds_ib_client);\n\tif (!rds_ibdev)\n\t\treturn;\n\n\trds_ib_dev_shutdown(rds_ibdev);\n\n\t/* stop connection attempts from getting a reference to this device. */\n\tib_set_client_data(device, &rds_ib_client, NULL);\n\n\tdown_write(&rds_ib_devices_lock);\n\tlist_del_rcu(&rds_ibdev->list);\n\tup_write(&rds_ib_devices_lock);\n\n\t/*\n\t * This synchronize rcu is waiting for readers of both the ib\n\t * client data and the devices list to finish before we drop\n\t * both of those references.\n\t */\n\tsynchronize_rcu();\n\trds_ib_dev_put(rds_ibdev);\n\trds_ib_dev_put(rds_ibdev);\n}\n\nstruct ib_client rds_ib_client = {\n\t.name   = \"rds_ib\",\n\t.add    = rds_ib_add_one,\n\t.remove = rds_ib_remove_one\n};\n\nstatic int rds_ib_conn_info_visitor(struct rds_connection *conn,\n\t\t\t\t    void *buffer)\n{\n\tstruct rds_info_rdma_connection *iinfo = buffer;\n\tstruct rds_ib_connection *ic;\n\n\t/* We will only ever look at IB transports */\n\tif (conn->c_trans != &rds_ib_transport)\n\t\treturn 0;\n\n\tiinfo->src_addr = conn->c_laddr;\n\tiinfo->dst_addr = conn->c_faddr;\n\n\tmemset(&iinfo->src_gid, 0, sizeof(iinfo->src_gid));\n\tmemset(&iinfo->dst_gid, 0, sizeof(iinfo->dst_gid));\n\tif (rds_conn_state(conn) == RDS_CONN_UP) {\n\t\tstruct rds_ib_device *rds_ibdev;\n\t\tstruct rdma_dev_addr *dev_addr;\n\n\t\tic = conn->c_transport_data;\n\t\tdev_addr = &ic->i_cm_id->route.addr.dev_addr;\n\n\t\trdma_addr_get_sgid(dev_addr, (union ib_gid *) &iinfo->src_gid);\n\t\trdma_addr_get_dgid(dev_addr, (union ib_gid *) &iinfo->dst_gid);\n\n\t\trds_ibdev = ic->rds_ibdev;\n\t\tiinfo->max_send_wr = ic->i_send_ring.w_nr;\n\t\tiinfo->max_recv_wr = ic->i_recv_ring.w_nr;\n\t\tiinfo->max_send_sge = rds_ibdev->max_sge;\n\t\trds_ib_get_mr_info(rds_ibdev, iinfo);\n\t}\n\treturn 1;\n}\n\nstatic void rds_ib_ic_info(struct socket *sock, unsigned int len,\n\t\t\t   struct rds_info_iterator *iter,\n\t\t\t   struct rds_info_lengths *lens)\n{\n\trds_for_each_conn_info(sock, len, iter, lens,\n\t\t\t\trds_ib_conn_info_visitor,\n\t\t\t\tsizeof(struct rds_info_rdma_connection));\n}\n\n\n/*\n * Early RDS/IB was built to only bind to an address if there is an IPoIB\n * device with that address set.\n *\n * If it were me, I'd advocate for something more flexible.  Sending and\n * receiving should be device-agnostic.  Transports would try and maintain\n * connections between peers who have messages queued.  Userspace would be\n * allowed to influence which paths have priority.  We could call userspace\n * asserting this policy \"routing\".\n */\nstatic int rds_ib_laddr_check(__be32 addr)\n{\n\tint ret;\n\tstruct rdma_cm_id *cm_id;\n\tstruct sockaddr_in sin;\n\n\t/* Create a CMA ID and try to bind it. This catches both\n\t * IB and iWARP capable NICs.\n\t */\n\tcm_id = rdma_create_id(NULL, NULL, RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(cm_id))\n\t\treturn PTR_ERR(cm_id);\n\n\tmemset(&sin, 0, sizeof(sin));\n\tsin.sin_family = AF_INET;\n\tsin.sin_addr.s_addr = addr;\n\n\t/* rdma_bind_addr will only succeed for IB & iWARP devices */\n\tret = rdma_bind_addr(cm_id, (struct sockaddr *)&sin);\n\t/* due to this, we will claim to support iWARP devices unless we\n\t   check node_type. */\n\tif (ret || cm_id->device->node_type != RDMA_NODE_IB_CA)\n\t\tret = -EADDRNOTAVAIL;\n\n\trdsdebug(\"addr %pI4 ret %d node type %d\\n\",\n\t\t&addr, ret,\n\t\tcm_id->device ? cm_id->device->node_type : -1);\n\n\trdma_destroy_id(cm_id);\n\n\treturn ret;\n}\n\nstatic void rds_ib_unregister_client(void)\n{\n\tib_unregister_client(&rds_ib_client);\n\t/* wait for rds_ib_dev_free() to complete */\n\tflush_workqueue(rds_wq);\n}\n\nvoid rds_ib_exit(void)\n{\n\trds_info_deregister_func(RDS_INFO_IB_CONNECTIONS, rds_ib_ic_info);\n\trds_ib_unregister_client();\n\trds_ib_destroy_nodev_conns();\n\trds_ib_sysctl_exit();\n\trds_ib_recv_exit();\n\trds_trans_unregister(&rds_ib_transport);\n}\n\nstruct rds_transport rds_ib_transport = {\n\t.laddr_check\t\t= rds_ib_laddr_check,\n\t.xmit_complete\t\t= rds_ib_xmit_complete,\n\t.xmit\t\t\t= rds_ib_xmit,\n\t.xmit_rdma\t\t= rds_ib_xmit_rdma,\n\t.xmit_atomic\t\t= rds_ib_xmit_atomic,\n\t.recv\t\t\t= rds_ib_recv,\n\t.conn_alloc\t\t= rds_ib_conn_alloc,\n\t.conn_free\t\t= rds_ib_conn_free,\n\t.conn_connect\t\t= rds_ib_conn_connect,\n\t.conn_shutdown\t\t= rds_ib_conn_shutdown,\n\t.inc_copy_to_user\t= rds_ib_inc_copy_to_user,\n\t.inc_free\t\t= rds_ib_inc_free,\n\t.cm_initiate_connect\t= rds_ib_cm_initiate_connect,\n\t.cm_handle_connect\t= rds_ib_cm_handle_connect,\n\t.cm_connect_complete\t= rds_ib_cm_connect_complete,\n\t.stats_info_copy\t= rds_ib_stats_info_copy,\n\t.exit\t\t\t= rds_ib_exit,\n\t.get_mr\t\t\t= rds_ib_get_mr,\n\t.sync_mr\t\t= rds_ib_sync_mr,\n\t.free_mr\t\t= rds_ib_free_mr,\n\t.flush_mrs\t\t= rds_ib_flush_mrs,\n\t.t_owner\t\t= THIS_MODULE,\n\t.t_name\t\t\t= \"infiniband\",\n\t.t_type\t\t\t= RDS_TRANS_IB\n};\n\nint rds_ib_init(void)\n{\n\tint ret;\n\n\tINIT_LIST_HEAD(&rds_ib_devices);\n\n\tret = ib_register_client(&rds_ib_client);\n\tif (ret)\n\t\tgoto out;\n\n\tret = rds_ib_sysctl_init();\n\tif (ret)\n\t\tgoto out_ibreg;\n\n\tret = rds_ib_recv_init();\n\tif (ret)\n\t\tgoto out_sysctl;\n\n\tret = rds_trans_register(&rds_ib_transport);\n\tif (ret)\n\t\tgoto out_recv;\n\n\trds_info_register_func(RDS_INFO_IB_CONNECTIONS, rds_ib_ic_info);\n\n\tgoto out;\n\nout_recv:\n\trds_ib_recv_exit();\nout_sysctl:\n\trds_ib_sysctl_exit();\nout_ibreg:\n\trds_ib_unregister_client();\nout:\n\treturn ret;\n}\n\nMODULE_LICENSE(\"GPL\");\n\n"], "fixing_code": ["/*\n * Copyright (c) 2006 Oracle.  All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/in.h>\n#include <linux/if.h>\n#include <linux/netdevice.h>\n#include <linux/inetdevice.h>\n#include <linux/if_arp.h>\n#include <linux/delay.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n\n#include \"rds.h\"\n#include \"ib.h\"\n\nstatic unsigned int fmr_pool_size = RDS_FMR_POOL_SIZE;\nunsigned int fmr_message_size = RDS_FMR_SIZE + 1; /* +1 allows for unaligned MRs */\nunsigned int rds_ib_retry_count = RDS_IB_DEFAULT_RETRY_COUNT;\n\nmodule_param(fmr_pool_size, int, 0444);\nMODULE_PARM_DESC(fmr_pool_size, \" Max number of fmr per HCA\");\nmodule_param(fmr_message_size, int, 0444);\nMODULE_PARM_DESC(fmr_message_size, \" Max size of a RDMA transfer\");\nmodule_param(rds_ib_retry_count, int, 0444);\nMODULE_PARM_DESC(rds_ib_retry_count, \" Number of hw retries before reporting an error\");\n\n/*\n * we have a clumsy combination of RCU and a rwsem protecting this list\n * because it is used both in the get_mr fast path and while blocking in\n * the FMR flushing path.\n */\nDECLARE_RWSEM(rds_ib_devices_lock);\nstruct list_head rds_ib_devices;\n\n/* NOTE: if also grabbing ibdev lock, grab this first */\nDEFINE_SPINLOCK(ib_nodev_conns_lock);\nLIST_HEAD(ib_nodev_conns);\n\nstatic void rds_ib_nodev_connect(void)\n{\n\tstruct rds_ib_connection *ic;\n\n\tspin_lock(&ib_nodev_conns_lock);\n\tlist_for_each_entry(ic, &ib_nodev_conns, ib_node)\n\t\trds_conn_connect_if_down(ic->conn);\n\tspin_unlock(&ib_nodev_conns_lock);\n}\n\nstatic void rds_ib_dev_shutdown(struct rds_ib_device *rds_ibdev)\n{\n\tstruct rds_ib_connection *ic;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rds_ibdev->spinlock, flags);\n\tlist_for_each_entry(ic, &rds_ibdev->conn_list, ib_node)\n\t\trds_conn_drop(ic->conn);\n\tspin_unlock_irqrestore(&rds_ibdev->spinlock, flags);\n}\n\n/*\n * rds_ib_destroy_mr_pool() blocks on a few things and mrs drop references\n * from interrupt context so we push freing off into a work struct in krdsd.\n */\nstatic void rds_ib_dev_free(struct work_struct *work)\n{\n\tstruct rds_ib_ipaddr *i_ipaddr, *i_next;\n\tstruct rds_ib_device *rds_ibdev = container_of(work,\n\t\t\t\t\tstruct rds_ib_device, free_work);\n\n\tif (rds_ibdev->mr_pool)\n\t\trds_ib_destroy_mr_pool(rds_ibdev->mr_pool);\n\tif (rds_ibdev->mr)\n\t\tib_dereg_mr(rds_ibdev->mr);\n\tif (rds_ibdev->pd)\n\t\tib_dealloc_pd(rds_ibdev->pd);\n\n\tlist_for_each_entry_safe(i_ipaddr, i_next, &rds_ibdev->ipaddr_list, list) {\n\t\tlist_del(&i_ipaddr->list);\n\t\tkfree(i_ipaddr);\n\t}\n\n\tkfree(rds_ibdev);\n}\n\nvoid rds_ib_dev_put(struct rds_ib_device *rds_ibdev)\n{\n\tBUG_ON(atomic_read(&rds_ibdev->refcount) <= 0);\n\tif (atomic_dec_and_test(&rds_ibdev->refcount))\n\t\tqueue_work(rds_wq, &rds_ibdev->free_work);\n}\n\nstatic void rds_ib_add_one(struct ib_device *device)\n{\n\tstruct rds_ib_device *rds_ibdev;\n\tstruct ib_device_attr *dev_attr;\n\n\t/* Only handle IB (no iWARP) devices */\n\tif (device->node_type != RDMA_NODE_IB_CA)\n\t\treturn;\n\n\tdev_attr = kmalloc(sizeof *dev_attr, GFP_KERNEL);\n\tif (!dev_attr)\n\t\treturn;\n\n\tif (ib_query_device(device, dev_attr)) {\n\t\trdsdebug(\"Query device failed for %s\\n\", device->name);\n\t\tgoto free_attr;\n\t}\n\n\trds_ibdev = kzalloc_node(sizeof(struct rds_ib_device), GFP_KERNEL,\n\t\t\t\t ibdev_to_node(device));\n\tif (!rds_ibdev)\n\t\tgoto free_attr;\n\n\tspin_lock_init(&rds_ibdev->spinlock);\n\tatomic_set(&rds_ibdev->refcount, 1);\n\tINIT_WORK(&rds_ibdev->free_work, rds_ib_dev_free);\n\n\trds_ibdev->max_wrs = dev_attr->max_qp_wr;\n\trds_ibdev->max_sge = min(dev_attr->max_sge, RDS_IB_MAX_SGE);\n\n\trds_ibdev->fmr_max_remaps = dev_attr->max_map_per_fmr?: 32;\n\trds_ibdev->max_fmrs = dev_attr->max_fmr ?\n\t\t\tmin_t(unsigned int, dev_attr->max_fmr, fmr_pool_size) :\n\t\t\tfmr_pool_size;\n\n\trds_ibdev->max_initiator_depth = dev_attr->max_qp_init_rd_atom;\n\trds_ibdev->max_responder_resources = dev_attr->max_qp_rd_atom;\n\n\trds_ibdev->dev = device;\n\trds_ibdev->pd = ib_alloc_pd(device);\n\tif (IS_ERR(rds_ibdev->pd)) {\n\t\trds_ibdev->pd = NULL;\n\t\tgoto put_dev;\n\t}\n\n\trds_ibdev->mr = ib_get_dma_mr(rds_ibdev->pd, IB_ACCESS_LOCAL_WRITE);\n\tif (IS_ERR(rds_ibdev->mr)) {\n\t\trds_ibdev->mr = NULL;\n\t\tgoto put_dev;\n\t}\n\n\trds_ibdev->mr_pool = rds_ib_create_mr_pool(rds_ibdev);\n\tif (IS_ERR(rds_ibdev->mr_pool)) {\n\t\trds_ibdev->mr_pool = NULL;\n\t\tgoto put_dev;\n\t}\n\n\tINIT_LIST_HEAD(&rds_ibdev->ipaddr_list);\n\tINIT_LIST_HEAD(&rds_ibdev->conn_list);\n\n\tdown_write(&rds_ib_devices_lock);\n\tlist_add_tail_rcu(&rds_ibdev->list, &rds_ib_devices);\n\tup_write(&rds_ib_devices_lock);\n\tatomic_inc(&rds_ibdev->refcount);\n\n\tib_set_client_data(device, &rds_ib_client, rds_ibdev);\n\tatomic_inc(&rds_ibdev->refcount);\n\n\trds_ib_nodev_connect();\n\nput_dev:\n\trds_ib_dev_put(rds_ibdev);\nfree_attr:\n\tkfree(dev_attr);\n}\n\n/*\n * New connections use this to find the device to associate with the\n * connection.  It's not in the fast path so we're not concerned about the\n * performance of the IB call.  (As of this writing, it uses an interrupt\n * blocking spinlock to serialize walking a per-device list of all registered\n * clients.)\n *\n * RCU is used to handle incoming connections racing with device teardown.\n * Rather than use a lock to serialize removal from the client_data and\n * getting a new reference, we use an RCU grace period.  The destruction\n * path removes the device from client_data and then waits for all RCU\n * readers to finish.\n *\n * A new connection can get NULL from this if its arriving on a\n * device that is in the process of being removed.\n */\nstruct rds_ib_device *rds_ib_get_client_data(struct ib_device *device)\n{\n\tstruct rds_ib_device *rds_ibdev;\n\n\trcu_read_lock();\n\trds_ibdev = ib_get_client_data(device, &rds_ib_client);\n\tif (rds_ibdev)\n\t\tatomic_inc(&rds_ibdev->refcount);\n\trcu_read_unlock();\n\treturn rds_ibdev;\n}\n\n/*\n * The IB stack is letting us know that a device is going away.  This can\n * happen if the underlying HCA driver is removed or if PCI hotplug is removing\n * the pci function, for example.\n *\n * This can be called at any time and can be racing with any other RDS path.\n */\nstatic void rds_ib_remove_one(struct ib_device *device)\n{\n\tstruct rds_ib_device *rds_ibdev;\n\n\trds_ibdev = ib_get_client_data(device, &rds_ib_client);\n\tif (!rds_ibdev)\n\t\treturn;\n\n\trds_ib_dev_shutdown(rds_ibdev);\n\n\t/* stop connection attempts from getting a reference to this device. */\n\tib_set_client_data(device, &rds_ib_client, NULL);\n\n\tdown_write(&rds_ib_devices_lock);\n\tlist_del_rcu(&rds_ibdev->list);\n\tup_write(&rds_ib_devices_lock);\n\n\t/*\n\t * This synchronize rcu is waiting for readers of both the ib\n\t * client data and the devices list to finish before we drop\n\t * both of those references.\n\t */\n\tsynchronize_rcu();\n\trds_ib_dev_put(rds_ibdev);\n\trds_ib_dev_put(rds_ibdev);\n}\n\nstruct ib_client rds_ib_client = {\n\t.name   = \"rds_ib\",\n\t.add    = rds_ib_add_one,\n\t.remove = rds_ib_remove_one\n};\n\nstatic int rds_ib_conn_info_visitor(struct rds_connection *conn,\n\t\t\t\t    void *buffer)\n{\n\tstruct rds_info_rdma_connection *iinfo = buffer;\n\tstruct rds_ib_connection *ic;\n\n\t/* We will only ever look at IB transports */\n\tif (conn->c_trans != &rds_ib_transport)\n\t\treturn 0;\n\n\tiinfo->src_addr = conn->c_laddr;\n\tiinfo->dst_addr = conn->c_faddr;\n\n\tmemset(&iinfo->src_gid, 0, sizeof(iinfo->src_gid));\n\tmemset(&iinfo->dst_gid, 0, sizeof(iinfo->dst_gid));\n\tif (rds_conn_state(conn) == RDS_CONN_UP) {\n\t\tstruct rds_ib_device *rds_ibdev;\n\t\tstruct rdma_dev_addr *dev_addr;\n\n\t\tic = conn->c_transport_data;\n\t\tdev_addr = &ic->i_cm_id->route.addr.dev_addr;\n\n\t\trdma_addr_get_sgid(dev_addr, (union ib_gid *) &iinfo->src_gid);\n\t\trdma_addr_get_dgid(dev_addr, (union ib_gid *) &iinfo->dst_gid);\n\n\t\trds_ibdev = ic->rds_ibdev;\n\t\tiinfo->max_send_wr = ic->i_send_ring.w_nr;\n\t\tiinfo->max_recv_wr = ic->i_recv_ring.w_nr;\n\t\tiinfo->max_send_sge = rds_ibdev->max_sge;\n\t\trds_ib_get_mr_info(rds_ibdev, iinfo);\n\t}\n\treturn 1;\n}\n\nstatic void rds_ib_ic_info(struct socket *sock, unsigned int len,\n\t\t\t   struct rds_info_iterator *iter,\n\t\t\t   struct rds_info_lengths *lens)\n{\n\trds_for_each_conn_info(sock, len, iter, lens,\n\t\t\t\trds_ib_conn_info_visitor,\n\t\t\t\tsizeof(struct rds_info_rdma_connection));\n}\n\n\n/*\n * Early RDS/IB was built to only bind to an address if there is an IPoIB\n * device with that address set.\n *\n * If it were me, I'd advocate for something more flexible.  Sending and\n * receiving should be device-agnostic.  Transports would try and maintain\n * connections between peers who have messages queued.  Userspace would be\n * allowed to influence which paths have priority.  We could call userspace\n * asserting this policy \"routing\".\n */\nstatic int rds_ib_laddr_check(__be32 addr)\n{\n\tint ret;\n\tstruct rdma_cm_id *cm_id;\n\tstruct sockaddr_in sin;\n\n\t/* Create a CMA ID and try to bind it. This catches both\n\t * IB and iWARP capable NICs.\n\t */\n\tcm_id = rdma_create_id(NULL, NULL, RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(cm_id))\n\t\treturn PTR_ERR(cm_id);\n\n\tmemset(&sin, 0, sizeof(sin));\n\tsin.sin_family = AF_INET;\n\tsin.sin_addr.s_addr = addr;\n\n\t/* rdma_bind_addr will only succeed for IB & iWARP devices */\n\tret = rdma_bind_addr(cm_id, (struct sockaddr *)&sin);\n\t/* due to this, we will claim to support iWARP devices unless we\n\t   check node_type. */\n\tif (ret || !cm_id->device ||\n\t    cm_id->device->node_type != RDMA_NODE_IB_CA)\n\t\tret = -EADDRNOTAVAIL;\n\n\trdsdebug(\"addr %pI4 ret %d node type %d\\n\",\n\t\t&addr, ret,\n\t\tcm_id->device ? cm_id->device->node_type : -1);\n\n\trdma_destroy_id(cm_id);\n\n\treturn ret;\n}\n\nstatic void rds_ib_unregister_client(void)\n{\n\tib_unregister_client(&rds_ib_client);\n\t/* wait for rds_ib_dev_free() to complete */\n\tflush_workqueue(rds_wq);\n}\n\nvoid rds_ib_exit(void)\n{\n\trds_info_deregister_func(RDS_INFO_IB_CONNECTIONS, rds_ib_ic_info);\n\trds_ib_unregister_client();\n\trds_ib_destroy_nodev_conns();\n\trds_ib_sysctl_exit();\n\trds_ib_recv_exit();\n\trds_trans_unregister(&rds_ib_transport);\n}\n\nstruct rds_transport rds_ib_transport = {\n\t.laddr_check\t\t= rds_ib_laddr_check,\n\t.xmit_complete\t\t= rds_ib_xmit_complete,\n\t.xmit\t\t\t= rds_ib_xmit,\n\t.xmit_rdma\t\t= rds_ib_xmit_rdma,\n\t.xmit_atomic\t\t= rds_ib_xmit_atomic,\n\t.recv\t\t\t= rds_ib_recv,\n\t.conn_alloc\t\t= rds_ib_conn_alloc,\n\t.conn_free\t\t= rds_ib_conn_free,\n\t.conn_connect\t\t= rds_ib_conn_connect,\n\t.conn_shutdown\t\t= rds_ib_conn_shutdown,\n\t.inc_copy_to_user\t= rds_ib_inc_copy_to_user,\n\t.inc_free\t\t= rds_ib_inc_free,\n\t.cm_initiate_connect\t= rds_ib_cm_initiate_connect,\n\t.cm_handle_connect\t= rds_ib_cm_handle_connect,\n\t.cm_connect_complete\t= rds_ib_cm_connect_complete,\n\t.stats_info_copy\t= rds_ib_stats_info_copy,\n\t.exit\t\t\t= rds_ib_exit,\n\t.get_mr\t\t\t= rds_ib_get_mr,\n\t.sync_mr\t\t= rds_ib_sync_mr,\n\t.free_mr\t\t= rds_ib_free_mr,\n\t.flush_mrs\t\t= rds_ib_flush_mrs,\n\t.t_owner\t\t= THIS_MODULE,\n\t.t_name\t\t\t= \"infiniband\",\n\t.t_type\t\t\t= RDS_TRANS_IB\n};\n\nint rds_ib_init(void)\n{\n\tint ret;\n\n\tINIT_LIST_HEAD(&rds_ib_devices);\n\n\tret = ib_register_client(&rds_ib_client);\n\tif (ret)\n\t\tgoto out;\n\n\tret = rds_ib_sysctl_init();\n\tif (ret)\n\t\tgoto out_ibreg;\n\n\tret = rds_ib_recv_init();\n\tif (ret)\n\t\tgoto out_sysctl;\n\n\tret = rds_trans_register(&rds_ib_transport);\n\tif (ret)\n\t\tgoto out_recv;\n\n\trds_info_register_func(RDS_INFO_IB_CONNECTIONS, rds_ib_ic_info);\n\n\tgoto out;\n\nout_recv:\n\trds_ib_recv_exit();\nout_sysctl:\n\trds_ib_sysctl_exit();\nout_ibreg:\n\trds_ib_unregister_client();\nout:\n\treturn ret;\n}\n\nMODULE_LICENSE(\"GPL\");\n\n"], "filenames": ["net/rds/ib.c"], "buggy_code_start_loc": [341], "buggy_code_end_loc": [342], "fixing_code_start_loc": [341], "fixing_code_end_loc": [343], "type": "CWE-476", "message": "The rds_ib_laddr_check function in net/rds/ib.c in the Linux kernel before 3.12.8 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact via a bind system call for an RDS socket on a system that lacks RDS transports.", "other": {"cve": {"id": "CVE-2013-7339", "sourceIdentifier": "cve@mitre.org", "published": "2014-03-24T16:40:43.140", "lastModified": "2020-08-28T14:35:20.633", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The rds_ib_laddr_check function in net/rds/ib.c in the Linux kernel before 3.12.8 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact via a bind system call for an RDS socket on a system that lacks RDS transports."}, {"lang": "es", "value": "La funci\u00f3n rds_ib_laddr_check en net/rds/ib.c en el kernel de Linux anterior a 3.12.8 permite a usuarios locales causar una denegaci\u00f3n de servicio (referencia de puntero nulo y ca\u00edda de sistema) o posiblemente tener otro impacto no especificado a trav\u00e9s de una llamada de sistema bind para un socket RDS en un sistema que carece de transportes RDS."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.7}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.12.8", "matchCriteriaId": "6B3D58B5-3A44-4506-B657-3BC3C83BE522"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=c2349758acf1874e4c2b93fe41d072336f1a31d0", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.12.8", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2014/03/20/14", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/66351", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1079214", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/c2349758acf1874e4c2b93fe41d072336f1a31d0", "source": "cve@mitre.org", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/c2349758acf1874e4c2b93fe41d072336f1a31d0"}}