{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/* binder.c\n *\n * Android IPC Subsystem\n *\n * Copyright (C) 2007-2008 Google, Inc.\n */\n\n/*\n * Locking overview\n *\n * There are 3 main spinlocks which must be acquired in the\n * order shown:\n *\n * 1) proc->outer_lock : protects binder_ref\n *    binder_proc_lock() and binder_proc_unlock() are\n *    used to acq/rel.\n * 2) node->lock : protects most fields of binder_node.\n *    binder_node_lock() and binder_node_unlock() are\n *    used to acq/rel\n * 3) proc->inner_lock : protects the thread and node lists\n *    (proc->threads, proc->waiting_threads, proc->nodes)\n *    and all todo lists associated with the binder_proc\n *    (proc->todo, thread->todo, proc->delivered_death and\n *    node->async_todo), as well as thread->transaction_stack\n *    binder_inner_proc_lock() and binder_inner_proc_unlock()\n *    are used to acq/rel\n *\n * Any lock under procA must never be nested under any lock at the same\n * level or below on procB.\n *\n * Functions that require a lock held on entry indicate which lock\n * in the suffix of the function name:\n *\n * foo_olocked() : requires node->outer_lock\n * foo_nlocked() : requires node->lock\n * foo_ilocked() : requires proc->inner_lock\n * foo_oilocked(): requires proc->outer_lock and proc->inner_lock\n * foo_nilocked(): requires node->lock and proc->inner_lock\n * ...\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/freezer.h>\n#include <linux/fs.h>\n#include <linux/list.h>\n#include <linux/miscdevice.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/nsproxy.h>\n#include <linux/poll.h>\n#include <linux/debugfs.h>\n#include <linux/rbtree.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/seq_file.h>\n#include <linux/string.h>\n#include <linux/uaccess.h>\n#include <linux/pid_namespace.h>\n#include <linux/security.h>\n#include <linux/spinlock.h>\n#include <linux/ratelimit.h>\n#include <linux/syscalls.h>\n#include <linux/task_work.h>\n\n#include <uapi/linux/android/binder.h>\n#include <uapi/linux/android/binderfs.h>\n\n#include <asm/cacheflush.h>\n\n#include \"binder_alloc.h\"\n#include \"binder_internal.h\"\n#include \"binder_trace.h\"\n\nstatic HLIST_HEAD(binder_deferred_list);\nstatic DEFINE_MUTEX(binder_deferred_lock);\n\nstatic HLIST_HEAD(binder_devices);\nstatic HLIST_HEAD(binder_procs);\nstatic DEFINE_MUTEX(binder_procs_lock);\n\nstatic HLIST_HEAD(binder_dead_nodes);\nstatic DEFINE_SPINLOCK(binder_dead_nodes_lock);\n\nstatic struct dentry *binder_debugfs_dir_entry_root;\nstatic struct dentry *binder_debugfs_dir_entry_proc;\nstatic atomic_t binder_last_id;\n\nstatic int proc_show(struct seq_file *m, void *unused);\nDEFINE_SHOW_ATTRIBUTE(proc);\n\n/* This is only defined in include/asm-arm/sizes.h */\n#ifndef SZ_1K\n#define SZ_1K                               0x400\n#endif\n\n#define FORBIDDEN_MMAP_FLAGS                (VM_WRITE)\n\nenum {\n\tBINDER_DEBUG_USER_ERROR             = 1U << 0,\n\tBINDER_DEBUG_FAILED_TRANSACTION     = 1U << 1,\n\tBINDER_DEBUG_DEAD_TRANSACTION       = 1U << 2,\n\tBINDER_DEBUG_OPEN_CLOSE             = 1U << 3,\n\tBINDER_DEBUG_DEAD_BINDER            = 1U << 4,\n\tBINDER_DEBUG_DEATH_NOTIFICATION     = 1U << 5,\n\tBINDER_DEBUG_READ_WRITE             = 1U << 6,\n\tBINDER_DEBUG_USER_REFS              = 1U << 7,\n\tBINDER_DEBUG_THREADS                = 1U << 8,\n\tBINDER_DEBUG_TRANSACTION            = 1U << 9,\n\tBINDER_DEBUG_TRANSACTION_COMPLETE   = 1U << 10,\n\tBINDER_DEBUG_FREE_BUFFER            = 1U << 11,\n\tBINDER_DEBUG_INTERNAL_REFS          = 1U << 12,\n\tBINDER_DEBUG_PRIORITY_CAP           = 1U << 13,\n\tBINDER_DEBUG_SPINLOCKS              = 1U << 14,\n};\nstatic uint32_t binder_debug_mask = BINDER_DEBUG_USER_ERROR |\n\tBINDER_DEBUG_FAILED_TRANSACTION | BINDER_DEBUG_DEAD_TRANSACTION;\nmodule_param_named(debug_mask, binder_debug_mask, uint, 0644);\n\nchar *binder_devices_param = CONFIG_ANDROID_BINDER_DEVICES;\nmodule_param_named(devices, binder_devices_param, charp, 0444);\n\nstatic DECLARE_WAIT_QUEUE_HEAD(binder_user_error_wait);\nstatic int binder_stop_on_user_error;\n\nstatic int binder_set_stop_on_user_error(const char *val,\n\t\t\t\t\t const struct kernel_param *kp)\n{\n\tint ret;\n\n\tret = param_set_int(val, kp);\n\tif (binder_stop_on_user_error < 2)\n\t\twake_up(&binder_user_error_wait);\n\treturn ret;\n}\nmodule_param_call(stop_on_user_error, binder_set_stop_on_user_error,\n\tparam_get_int, &binder_stop_on_user_error, 0644);\n\n#define binder_debug(mask, x...) \\\n\tdo { \\\n\t\tif (binder_debug_mask & mask) \\\n\t\t\tpr_info_ratelimited(x); \\\n\t} while (0)\n\n#define binder_user_error(x...) \\\n\tdo { \\\n\t\tif (binder_debug_mask & BINDER_DEBUG_USER_ERROR) \\\n\t\t\tpr_info_ratelimited(x); \\\n\t\tif (binder_stop_on_user_error) \\\n\t\t\tbinder_stop_on_user_error = 2; \\\n\t} while (0)\n\n#define to_flat_binder_object(hdr) \\\n\tcontainer_of(hdr, struct flat_binder_object, hdr)\n\n#define to_binder_fd_object(hdr) container_of(hdr, struct binder_fd_object, hdr)\n\n#define to_binder_buffer_object(hdr) \\\n\tcontainer_of(hdr, struct binder_buffer_object, hdr)\n\n#define to_binder_fd_array_object(hdr) \\\n\tcontainer_of(hdr, struct binder_fd_array_object, hdr)\n\nenum binder_stat_types {\n\tBINDER_STAT_PROC,\n\tBINDER_STAT_THREAD,\n\tBINDER_STAT_NODE,\n\tBINDER_STAT_REF,\n\tBINDER_STAT_DEATH,\n\tBINDER_STAT_TRANSACTION,\n\tBINDER_STAT_TRANSACTION_COMPLETE,\n\tBINDER_STAT_COUNT\n};\n\nstruct binder_stats {\n\tatomic_t br[_IOC_NR(BR_FAILED_REPLY) + 1];\n\tatomic_t bc[_IOC_NR(BC_REPLY_SG) + 1];\n\tatomic_t obj_created[BINDER_STAT_COUNT];\n\tatomic_t obj_deleted[BINDER_STAT_COUNT];\n};\n\nstatic struct binder_stats binder_stats;\n\nstatic inline void binder_stats_deleted(enum binder_stat_types type)\n{\n\tatomic_inc(&binder_stats.obj_deleted[type]);\n}\n\nstatic inline void binder_stats_created(enum binder_stat_types type)\n{\n\tatomic_inc(&binder_stats.obj_created[type]);\n}\n\nstruct binder_transaction_log binder_transaction_log;\nstruct binder_transaction_log binder_transaction_log_failed;\n\nstatic struct binder_transaction_log_entry *binder_transaction_log_add(\n\tstruct binder_transaction_log *log)\n{\n\tstruct binder_transaction_log_entry *e;\n\tunsigned int cur = atomic_inc_return(&log->cur);\n\n\tif (cur >= ARRAY_SIZE(log->entry))\n\t\tlog->full = true;\n\te = &log->entry[cur % ARRAY_SIZE(log->entry)];\n\tWRITE_ONCE(e->debug_id_done, 0);\n\t/*\n\t * write-barrier to synchronize access to e->debug_id_done.\n\t * We make sure the initialized 0 value is seen before\n\t * memset() other fields are zeroed by memset.\n\t */\n\tsmp_wmb();\n\tmemset(e, 0, sizeof(*e));\n\treturn e;\n}\n\n/**\n * struct binder_work - work enqueued on a worklist\n * @entry:             node enqueued on list\n * @type:              type of work to be performed\n *\n * There are separate work lists for proc, thread, and node (async).\n */\nstruct binder_work {\n\tstruct list_head entry;\n\n\tenum binder_work_type {\n\t\tBINDER_WORK_TRANSACTION = 1,\n\t\tBINDER_WORK_TRANSACTION_COMPLETE,\n\t\tBINDER_WORK_RETURN_ERROR,\n\t\tBINDER_WORK_NODE,\n\t\tBINDER_WORK_DEAD_BINDER,\n\t\tBINDER_WORK_DEAD_BINDER_AND_CLEAR,\n\t\tBINDER_WORK_CLEAR_DEATH_NOTIFICATION,\n\t} type;\n};\n\nstruct binder_error {\n\tstruct binder_work work;\n\tuint32_t cmd;\n};\n\n/**\n * struct binder_node - binder node bookkeeping\n * @debug_id:             unique ID for debugging\n *                        (invariant after initialized)\n * @lock:                 lock for node fields\n * @work:                 worklist element for node work\n *                        (protected by @proc->inner_lock)\n * @rb_node:              element for proc->nodes tree\n *                        (protected by @proc->inner_lock)\n * @dead_node:            element for binder_dead_nodes list\n *                        (protected by binder_dead_nodes_lock)\n * @proc:                 binder_proc that owns this node\n *                        (invariant after initialized)\n * @refs:                 list of references on this node\n *                        (protected by @lock)\n * @internal_strong_refs: used to take strong references when\n *                        initiating a transaction\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @local_weak_refs:      weak user refs from local process\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @local_strong_refs:    strong user refs from local process\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @tmp_refs:             temporary kernel refs\n *                        (protected by @proc->inner_lock while @proc\n *                        is valid, and by binder_dead_nodes_lock\n *                        if @proc is NULL. During inc/dec and node release\n *                        it is also protected by @lock to provide safety\n *                        as the node dies and @proc becomes NULL)\n * @ptr:                  userspace pointer for node\n *                        (invariant, no lock needed)\n * @cookie:               userspace cookie for node\n *                        (invariant, no lock needed)\n * @has_strong_ref:       userspace notified of strong ref\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @pending_strong_ref:   userspace has acked notification of strong ref\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @has_weak_ref:         userspace notified of weak ref\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @pending_weak_ref:     userspace has acked notification of weak ref\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @has_async_transaction: async transaction to node in progress\n *                        (protected by @lock)\n * @accept_fds:           file descriptor operations supported for node\n *                        (invariant after initialized)\n * @min_priority:         minimum scheduling priority\n *                        (invariant after initialized)\n * @txn_security_ctx:     require sender's security context\n *                        (invariant after initialized)\n * @async_todo:           list of async work items\n *                        (protected by @proc->inner_lock)\n *\n * Bookkeeping structure for binder nodes.\n */\nstruct binder_node {\n\tint debug_id;\n\tspinlock_t lock;\n\tstruct binder_work work;\n\tunion {\n\t\tstruct rb_node rb_node;\n\t\tstruct hlist_node dead_node;\n\t};\n\tstruct binder_proc *proc;\n\tstruct hlist_head refs;\n\tint internal_strong_refs;\n\tint local_weak_refs;\n\tint local_strong_refs;\n\tint tmp_refs;\n\tbinder_uintptr_t ptr;\n\tbinder_uintptr_t cookie;\n\tstruct {\n\t\t/*\n\t\t * bitfield elements protected by\n\t\t * proc inner_lock\n\t\t */\n\t\tu8 has_strong_ref:1;\n\t\tu8 pending_strong_ref:1;\n\t\tu8 has_weak_ref:1;\n\t\tu8 pending_weak_ref:1;\n\t};\n\tstruct {\n\t\t/*\n\t\t * invariant after initialization\n\t\t */\n\t\tu8 accept_fds:1;\n\t\tu8 txn_security_ctx:1;\n\t\tu8 min_priority;\n\t};\n\tbool has_async_transaction;\n\tstruct list_head async_todo;\n};\n\nstruct binder_ref_death {\n\t/**\n\t * @work: worklist element for death notifications\n\t *        (protected by inner_lock of the proc that\n\t *        this ref belongs to)\n\t */\n\tstruct binder_work work;\n\tbinder_uintptr_t cookie;\n};\n\n/**\n * struct binder_ref_data - binder_ref counts and id\n * @debug_id:        unique ID for the ref\n * @desc:            unique userspace handle for ref\n * @strong:          strong ref count (debugging only if not locked)\n * @weak:            weak ref count (debugging only if not locked)\n *\n * Structure to hold ref count and ref id information. Since\n * the actual ref can only be accessed with a lock, this structure\n * is used to return information about the ref to callers of\n * ref inc/dec functions.\n */\nstruct binder_ref_data {\n\tint debug_id;\n\tuint32_t desc;\n\tint strong;\n\tint weak;\n};\n\n/**\n * struct binder_ref - struct to track references on nodes\n * @data:        binder_ref_data containing id, handle, and current refcounts\n * @rb_node_desc: node for lookup by @data.desc in proc's rb_tree\n * @rb_node_node: node for lookup by @node in proc's rb_tree\n * @node_entry:  list entry for node->refs list in target node\n *               (protected by @node->lock)\n * @proc:        binder_proc containing ref\n * @node:        binder_node of target node. When cleaning up a\n *               ref for deletion in binder_cleanup_ref, a non-NULL\n *               @node indicates the node must be freed\n * @death:       pointer to death notification (ref_death) if requested\n *               (protected by @node->lock)\n *\n * Structure to track references from procA to target node (on procB). This\n * structure is unsafe to access without holding @proc->outer_lock.\n */\nstruct binder_ref {\n\t/* Lookups needed: */\n\t/*   node + proc => ref (transaction) */\n\t/*   desc + proc => ref (transaction, inc/dec ref) */\n\t/*   node => refs + procs (proc exit) */\n\tstruct binder_ref_data data;\n\tstruct rb_node rb_node_desc;\n\tstruct rb_node rb_node_node;\n\tstruct hlist_node node_entry;\n\tstruct binder_proc *proc;\n\tstruct binder_node *node;\n\tstruct binder_ref_death *death;\n};\n\nenum binder_deferred_state {\n\tBINDER_DEFERRED_FLUSH        = 0x01,\n\tBINDER_DEFERRED_RELEASE      = 0x02,\n};\n\n/**\n * struct binder_proc - binder process bookkeeping\n * @proc_node:            element for binder_procs list\n * @threads:              rbtree of binder_threads in this proc\n *                        (protected by @inner_lock)\n * @nodes:                rbtree of binder nodes associated with\n *                        this proc ordered by node->ptr\n *                        (protected by @inner_lock)\n * @refs_by_desc:         rbtree of refs ordered by ref->desc\n *                        (protected by @outer_lock)\n * @refs_by_node:         rbtree of refs ordered by ref->node\n *                        (protected by @outer_lock)\n * @waiting_threads:      threads currently waiting for proc work\n *                        (protected by @inner_lock)\n * @pid                   PID of group_leader of process\n *                        (invariant after initialized)\n * @tsk                   task_struct for group_leader of process\n *                        (invariant after initialized)\n * @cred                  struct cred associated with the `struct file`\n *                        in binder_open()\n *                        (invariant after initialized)\n * @deferred_work_node:   element for binder_deferred_list\n *                        (protected by binder_deferred_lock)\n * @deferred_work:        bitmap of deferred work to perform\n *                        (protected by binder_deferred_lock)\n * @is_dead:              process is dead and awaiting free\n *                        when outstanding transactions are cleaned up\n *                        (protected by @inner_lock)\n * @todo:                 list of work for this process\n *                        (protected by @inner_lock)\n * @stats:                per-process binder statistics\n *                        (atomics, no lock needed)\n * @delivered_death:      list of delivered death notification\n *                        (protected by @inner_lock)\n * @max_threads:          cap on number of binder threads\n *                        (protected by @inner_lock)\n * @requested_threads:    number of binder threads requested but not\n *                        yet started. In current implementation, can\n *                        only be 0 or 1.\n *                        (protected by @inner_lock)\n * @requested_threads_started: number binder threads started\n *                        (protected by @inner_lock)\n * @tmp_ref:              temporary reference to indicate proc is in use\n *                        (protected by @inner_lock)\n * @default_priority:     default scheduler priority\n *                        (invariant after initialized)\n * @debugfs_entry:        debugfs node\n * @alloc:                binder allocator bookkeeping\n * @context:              binder_context for this proc\n *                        (invariant after initialized)\n * @inner_lock:           can nest under outer_lock and/or node lock\n * @outer_lock:           no nesting under innor or node lock\n *                        Lock order: 1) outer, 2) node, 3) inner\n * @binderfs_entry:       process-specific binderfs log file\n *\n * Bookkeeping structure for binder processes\n */\nstruct binder_proc {\n\tstruct hlist_node proc_node;\n\tstruct rb_root threads;\n\tstruct rb_root nodes;\n\tstruct rb_root refs_by_desc;\n\tstruct rb_root refs_by_node;\n\tstruct list_head waiting_threads;\n\tint pid;\n\tstruct task_struct *tsk;\n\tconst struct cred *cred;\n\tstruct hlist_node deferred_work_node;\n\tint deferred_work;\n\tbool is_dead;\n\n\tstruct list_head todo;\n\tstruct binder_stats stats;\n\tstruct list_head delivered_death;\n\tint max_threads;\n\tint requested_threads;\n\tint requested_threads_started;\n\tint tmp_ref;\n\tlong default_priority;\n\tstruct dentry *debugfs_entry;\n\tstruct binder_alloc alloc;\n\tstruct binder_context *context;\n\tspinlock_t inner_lock;\n\tspinlock_t outer_lock;\n\tstruct dentry *binderfs_entry;\n};\n\nenum {\n\tBINDER_LOOPER_STATE_REGISTERED  = 0x01,\n\tBINDER_LOOPER_STATE_ENTERED     = 0x02,\n\tBINDER_LOOPER_STATE_EXITED      = 0x04,\n\tBINDER_LOOPER_STATE_INVALID     = 0x08,\n\tBINDER_LOOPER_STATE_WAITING     = 0x10,\n\tBINDER_LOOPER_STATE_POLL        = 0x20,\n};\n\n/**\n * struct binder_thread - binder thread bookkeeping\n * @proc:                 binder process for this thread\n *                        (invariant after initialization)\n * @rb_node:              element for proc->threads rbtree\n *                        (protected by @proc->inner_lock)\n * @waiting_thread_node:  element for @proc->waiting_threads list\n *                        (protected by @proc->inner_lock)\n * @pid:                  PID for this thread\n *                        (invariant after initialization)\n * @looper:               bitmap of looping state\n *                        (only accessed by this thread)\n * @looper_needs_return:  looping thread needs to exit driver\n *                        (no lock needed)\n * @transaction_stack:    stack of in-progress transactions for this thread\n *                        (protected by @proc->inner_lock)\n * @todo:                 list of work to do for this thread\n *                        (protected by @proc->inner_lock)\n * @process_todo:         whether work in @todo should be processed\n *                        (protected by @proc->inner_lock)\n * @return_error:         transaction errors reported by this thread\n *                        (only accessed by this thread)\n * @reply_error:          transaction errors reported by target thread\n *                        (protected by @proc->inner_lock)\n * @wait:                 wait queue for thread work\n * @stats:                per-thread statistics\n *                        (atomics, no lock needed)\n * @tmp_ref:              temporary reference to indicate thread is in use\n *                        (atomic since @proc->inner_lock cannot\n *                        always be acquired)\n * @is_dead:              thread is dead and awaiting free\n *                        when outstanding transactions are cleaned up\n *                        (protected by @proc->inner_lock)\n *\n * Bookkeeping structure for binder threads.\n */\nstruct binder_thread {\n\tstruct binder_proc *proc;\n\tstruct rb_node rb_node;\n\tstruct list_head waiting_thread_node;\n\tint pid;\n\tint looper;              /* only modified by this thread */\n\tbool looper_need_return; /* can be written by other thread */\n\tstruct binder_transaction *transaction_stack;\n\tstruct list_head todo;\n\tbool process_todo;\n\tstruct binder_error return_error;\n\tstruct binder_error reply_error;\n\twait_queue_head_t wait;\n\tstruct binder_stats stats;\n\tatomic_t tmp_ref;\n\tbool is_dead;\n};\n\n/**\n * struct binder_txn_fd_fixup - transaction fd fixup list element\n * @fixup_entry:          list entry\n * @file:                 struct file to be associated with new fd\n * @offset:               offset in buffer data to this fixup\n *\n * List element for fd fixups in a transaction. Since file\n * descriptors need to be allocated in the context of the\n * target process, we pass each fd to be processed in this\n * struct.\n */\nstruct binder_txn_fd_fixup {\n\tstruct list_head fixup_entry;\n\tstruct file *file;\n\tsize_t offset;\n};\n\nstruct binder_transaction {\n\tint debug_id;\n\tstruct binder_work work;\n\tstruct binder_thread *from;\n\tstruct binder_transaction *from_parent;\n\tstruct binder_proc *to_proc;\n\tstruct binder_thread *to_thread;\n\tstruct binder_transaction *to_parent;\n\tunsigned need_reply:1;\n\t/* unsigned is_dead:1; */\t/* not used at the moment */\n\n\tstruct binder_buffer *buffer;\n\tunsigned int\tcode;\n\tunsigned int\tflags;\n\tlong\tpriority;\n\tlong\tsaved_priority;\n\tkuid_t\tsender_euid;\n\tstruct list_head fd_fixups;\n\tbinder_uintptr_t security_ctx;\n\t/**\n\t * @lock:  protects @from, @to_proc, and @to_thread\n\t *\n\t * @from, @to_proc, and @to_thread can be set to NULL\n\t * during thread teardown\n\t */\n\tspinlock_t lock;\n};\n\n/**\n * struct binder_object - union of flat binder object types\n * @hdr:   generic object header\n * @fbo:   binder object (nodes and refs)\n * @fdo:   file descriptor object\n * @bbo:   binder buffer pointer\n * @fdao:  file descriptor array\n *\n * Used for type-independent object copies\n */\nstruct binder_object {\n\tunion {\n\t\tstruct binder_object_header hdr;\n\t\tstruct flat_binder_object fbo;\n\t\tstruct binder_fd_object fdo;\n\t\tstruct binder_buffer_object bbo;\n\t\tstruct binder_fd_array_object fdao;\n\t};\n};\n\n/**\n * binder_proc_lock() - Acquire outer lock for given binder_proc\n * @proc:         struct binder_proc to acquire\n *\n * Acquires proc->outer_lock. Used to protect binder_ref\n * structures associated with the given proc.\n */\n#define binder_proc_lock(proc) _binder_proc_lock(proc, __LINE__)\nstatic void\n_binder_proc_lock(struct binder_proc *proc, int line)\n\t__acquires(&proc->outer_lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_lock(&proc->outer_lock);\n}\n\n/**\n * binder_proc_unlock() - Release spinlock for given binder_proc\n * @proc:         struct binder_proc to acquire\n *\n * Release lock acquired via binder_proc_lock()\n */\n#define binder_proc_unlock(_proc) _binder_proc_unlock(_proc, __LINE__)\nstatic void\n_binder_proc_unlock(struct binder_proc *proc, int line)\n\t__releases(&proc->outer_lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_unlock(&proc->outer_lock);\n}\n\n/**\n * binder_inner_proc_lock() - Acquire inner lock for given binder_proc\n * @proc:         struct binder_proc to acquire\n *\n * Acquires proc->inner_lock. Used to protect todo lists\n */\n#define binder_inner_proc_lock(proc) _binder_inner_proc_lock(proc, __LINE__)\nstatic void\n_binder_inner_proc_lock(struct binder_proc *proc, int line)\n\t__acquires(&proc->inner_lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_lock(&proc->inner_lock);\n}\n\n/**\n * binder_inner_proc_unlock() - Release inner lock for given binder_proc\n * @proc:         struct binder_proc to acquire\n *\n * Release lock acquired via binder_inner_proc_lock()\n */\n#define binder_inner_proc_unlock(proc) _binder_inner_proc_unlock(proc, __LINE__)\nstatic void\n_binder_inner_proc_unlock(struct binder_proc *proc, int line)\n\t__releases(&proc->inner_lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_unlock(&proc->inner_lock);\n}\n\n/**\n * binder_node_lock() - Acquire spinlock for given binder_node\n * @node:         struct binder_node to acquire\n *\n * Acquires node->lock. Used to protect binder_node fields\n */\n#define binder_node_lock(node) _binder_node_lock(node, __LINE__)\nstatic void\n_binder_node_lock(struct binder_node *node, int line)\n\t__acquires(&node->lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_lock(&node->lock);\n}\n\n/**\n * binder_node_unlock() - Release spinlock for given binder_proc\n * @node:         struct binder_node to acquire\n *\n * Release lock acquired via binder_node_lock()\n */\n#define binder_node_unlock(node) _binder_node_unlock(node, __LINE__)\nstatic void\n_binder_node_unlock(struct binder_node *node, int line)\n\t__releases(&node->lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_unlock(&node->lock);\n}\n\n/**\n * binder_node_inner_lock() - Acquire node and inner locks\n * @node:         struct binder_node to acquire\n *\n * Acquires node->lock. If node->proc also acquires\n * proc->inner_lock. Used to protect binder_node fields\n */\n#define binder_node_inner_lock(node) _binder_node_inner_lock(node, __LINE__)\nstatic void\n_binder_node_inner_lock(struct binder_node *node, int line)\n\t__acquires(&node->lock) __acquires(&node->proc->inner_lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_lock(&node->lock);\n\tif (node->proc)\n\t\tbinder_inner_proc_lock(node->proc);\n\telse\n\t\t/* annotation for sparse */\n\t\t__acquire(&node->proc->inner_lock);\n}\n\n/**\n * binder_node_unlock() - Release node and inner locks\n * @node:         struct binder_node to acquire\n *\n * Release lock acquired via binder_node_lock()\n */\n#define binder_node_inner_unlock(node) _binder_node_inner_unlock(node, __LINE__)\nstatic void\n_binder_node_inner_unlock(struct binder_node *node, int line)\n\t__releases(&node->lock) __releases(&node->proc->inner_lock)\n{\n\tstruct binder_proc *proc = node->proc;\n\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tif (proc)\n\t\tbinder_inner_proc_unlock(proc);\n\telse\n\t\t/* annotation for sparse */\n\t\t__release(&node->proc->inner_lock);\n\tspin_unlock(&node->lock);\n}\n\nstatic bool binder_worklist_empty_ilocked(struct list_head *list)\n{\n\treturn list_empty(list);\n}\n\n/**\n * binder_worklist_empty() - Check if no items on the work list\n * @proc:       binder_proc associated with list\n * @list:\tlist to check\n *\n * Return: true if there are no items on list, else false\n */\nstatic bool binder_worklist_empty(struct binder_proc *proc,\n\t\t\t\t  struct list_head *list)\n{\n\tbool ret;\n\n\tbinder_inner_proc_lock(proc);\n\tret = binder_worklist_empty_ilocked(list);\n\tbinder_inner_proc_unlock(proc);\n\treturn ret;\n}\n\n/**\n * binder_enqueue_work_ilocked() - Add an item to the work list\n * @work:         struct binder_work to add to list\n * @target_list:  list to add work to\n *\n * Adds the work to the specified list. Asserts that work\n * is not already on a list.\n *\n * Requires the proc->inner_lock to be held.\n */\nstatic void\nbinder_enqueue_work_ilocked(struct binder_work *work,\n\t\t\t   struct list_head *target_list)\n{\n\tBUG_ON(target_list == NULL);\n\tBUG_ON(work->entry.next && !list_empty(&work->entry));\n\tlist_add_tail(&work->entry, target_list);\n}\n\n/**\n * binder_enqueue_deferred_thread_work_ilocked() - Add deferred thread work\n * @thread:       thread to queue work to\n * @work:         struct binder_work to add to list\n *\n * Adds the work to the todo list of the thread. Doesn't set the process_todo\n * flag, which means that (if it wasn't already set) the thread will go to\n * sleep without handling this work when it calls read.\n *\n * Requires the proc->inner_lock to be held.\n */\nstatic void\nbinder_enqueue_deferred_thread_work_ilocked(struct binder_thread *thread,\n\t\t\t\t\t    struct binder_work *work)\n{\n\tWARN_ON(!list_empty(&thread->waiting_thread_node));\n\tbinder_enqueue_work_ilocked(work, &thread->todo);\n}\n\n/**\n * binder_enqueue_thread_work_ilocked() - Add an item to the thread work list\n * @thread:       thread to queue work to\n * @work:         struct binder_work to add to list\n *\n * Adds the work to the todo list of the thread, and enables processing\n * of the todo queue.\n *\n * Requires the proc->inner_lock to be held.\n */\nstatic void\nbinder_enqueue_thread_work_ilocked(struct binder_thread *thread,\n\t\t\t\t   struct binder_work *work)\n{\n\tWARN_ON(!list_empty(&thread->waiting_thread_node));\n\tbinder_enqueue_work_ilocked(work, &thread->todo);\n\tthread->process_todo = true;\n}\n\n/**\n * binder_enqueue_thread_work() - Add an item to the thread work list\n * @thread:       thread to queue work to\n * @work:         struct binder_work to add to list\n *\n * Adds the work to the todo list of the thread, and enables processing\n * of the todo queue.\n */\nstatic void\nbinder_enqueue_thread_work(struct binder_thread *thread,\n\t\t\t   struct binder_work *work)\n{\n\tbinder_inner_proc_lock(thread->proc);\n\tbinder_enqueue_thread_work_ilocked(thread, work);\n\tbinder_inner_proc_unlock(thread->proc);\n}\n\nstatic void\nbinder_dequeue_work_ilocked(struct binder_work *work)\n{\n\tlist_del_init(&work->entry);\n}\n\n/**\n * binder_dequeue_work() - Removes an item from the work list\n * @proc:         binder_proc associated with list\n * @work:         struct binder_work to remove from list\n *\n * Removes the specified work item from whatever list it is on.\n * Can safely be called if work is not on any list.\n */\nstatic void\nbinder_dequeue_work(struct binder_proc *proc, struct binder_work *work)\n{\n\tbinder_inner_proc_lock(proc);\n\tbinder_dequeue_work_ilocked(work);\n\tbinder_inner_proc_unlock(proc);\n}\n\nstatic struct binder_work *binder_dequeue_work_head_ilocked(\n\t\t\t\t\tstruct list_head *list)\n{\n\tstruct binder_work *w;\n\n\tw = list_first_entry_or_null(list, struct binder_work, entry);\n\tif (w)\n\t\tlist_del_init(&w->entry);\n\treturn w;\n}\n\nstatic void\nbinder_defer_work(struct binder_proc *proc, enum binder_deferred_state defer);\nstatic void binder_free_thread(struct binder_thread *thread);\nstatic void binder_free_proc(struct binder_proc *proc);\nstatic void binder_inc_node_tmpref_ilocked(struct binder_node *node);\n\nstatic bool binder_has_work_ilocked(struct binder_thread *thread,\n\t\t\t\t    bool do_proc_work)\n{\n\treturn thread->process_todo ||\n\t\tthread->looper_need_return ||\n\t\t(do_proc_work &&\n\t\t !binder_worklist_empty_ilocked(&thread->proc->todo));\n}\n\nstatic bool binder_has_work(struct binder_thread *thread, bool do_proc_work)\n{\n\tbool has_work;\n\n\tbinder_inner_proc_lock(thread->proc);\n\thas_work = binder_has_work_ilocked(thread, do_proc_work);\n\tbinder_inner_proc_unlock(thread->proc);\n\n\treturn has_work;\n}\n\nstatic bool binder_available_for_proc_work_ilocked(struct binder_thread *thread)\n{\n\treturn !thread->transaction_stack &&\n\t\tbinder_worklist_empty_ilocked(&thread->todo) &&\n\t\t(thread->looper & (BINDER_LOOPER_STATE_ENTERED |\n\t\t\t\t   BINDER_LOOPER_STATE_REGISTERED));\n}\n\nstatic void binder_wakeup_poll_threads_ilocked(struct binder_proc *proc,\n\t\t\t\t\t       bool sync)\n{\n\tstruct rb_node *n;\n\tstruct binder_thread *thread;\n\n\tfor (n = rb_first(&proc->threads); n != NULL; n = rb_next(n)) {\n\t\tthread = rb_entry(n, struct binder_thread, rb_node);\n\t\tif (thread->looper & BINDER_LOOPER_STATE_POLL &&\n\t\t    binder_available_for_proc_work_ilocked(thread)) {\n\t\t\tif (sync)\n\t\t\t\twake_up_interruptible_sync(&thread->wait);\n\t\t\telse\n\t\t\t\twake_up_interruptible(&thread->wait);\n\t\t}\n\t}\n}\n\n/**\n * binder_select_thread_ilocked() - selects a thread for doing proc work.\n * @proc:\tprocess to select a thread from\n *\n * Note that calling this function moves the thread off the waiting_threads\n * list, so it can only be woken up by the caller of this function, or a\n * signal. Therefore, callers *should* always wake up the thread this function\n * returns.\n *\n * Return:\tIf there's a thread currently waiting for process work,\n *\t\treturns that thread. Otherwise returns NULL.\n */\nstatic struct binder_thread *\nbinder_select_thread_ilocked(struct binder_proc *proc)\n{\n\tstruct binder_thread *thread;\n\n\tassert_spin_locked(&proc->inner_lock);\n\tthread = list_first_entry_or_null(&proc->waiting_threads,\n\t\t\t\t\t  struct binder_thread,\n\t\t\t\t\t  waiting_thread_node);\n\n\tif (thread)\n\t\tlist_del_init(&thread->waiting_thread_node);\n\n\treturn thread;\n}\n\n/**\n * binder_wakeup_thread_ilocked() - wakes up a thread for doing proc work.\n * @proc:\tprocess to wake up a thread in\n * @thread:\tspecific thread to wake-up (may be NULL)\n * @sync:\twhether to do a synchronous wake-up\n *\n * This function wakes up a thread in the @proc process.\n * The caller may provide a specific thread to wake-up in\n * the @thread parameter. If @thread is NULL, this function\n * will wake up threads that have called poll().\n *\n * Note that for this function to work as expected, callers\n * should first call binder_select_thread() to find a thread\n * to handle the work (if they don't have a thread already),\n * and pass the result into the @thread parameter.\n */\nstatic void binder_wakeup_thread_ilocked(struct binder_proc *proc,\n\t\t\t\t\t struct binder_thread *thread,\n\t\t\t\t\t bool sync)\n{\n\tassert_spin_locked(&proc->inner_lock);\n\n\tif (thread) {\n\t\tif (sync)\n\t\t\twake_up_interruptible_sync(&thread->wait);\n\t\telse\n\t\t\twake_up_interruptible(&thread->wait);\n\t\treturn;\n\t}\n\n\t/* Didn't find a thread waiting for proc work; this can happen\n\t * in two scenarios:\n\t * 1. All threads are busy handling transactions\n\t *    In that case, one of those threads should call back into\n\t *    the kernel driver soon and pick up this work.\n\t * 2. Threads are using the (e)poll interface, in which case\n\t *    they may be blocked on the waitqueue without having been\n\t *    added to waiting_threads. For this case, we just iterate\n\t *    over all threads not handling transaction work, and\n\t *    wake them all up. We wake all because we don't know whether\n\t *    a thread that called into (e)poll is handling non-binder\n\t *    work currently.\n\t */\n\tbinder_wakeup_poll_threads_ilocked(proc, sync);\n}\n\nstatic void binder_wakeup_proc_ilocked(struct binder_proc *proc)\n{\n\tstruct binder_thread *thread = binder_select_thread_ilocked(proc);\n\n\tbinder_wakeup_thread_ilocked(proc, thread, /* sync = */false);\n}\n\nstatic void binder_set_nice(long nice)\n{\n\tlong min_nice;\n\n\tif (can_nice(current, nice)) {\n\t\tset_user_nice(current, nice);\n\t\treturn;\n\t}\n\tmin_nice = rlimit_to_nice(rlimit(RLIMIT_NICE));\n\tbinder_debug(BINDER_DEBUG_PRIORITY_CAP,\n\t\t     \"%d: nice value %ld not allowed use %ld instead\\n\",\n\t\t      current->pid, nice, min_nice);\n\tset_user_nice(current, min_nice);\n\tif (min_nice <= MAX_NICE)\n\t\treturn;\n\tbinder_user_error(\"%d RLIMIT_NICE not set\\n\", current->pid);\n}\n\nstatic struct binder_node *binder_get_node_ilocked(struct binder_proc *proc,\n\t\t\t\t\t\t   binder_uintptr_t ptr)\n{\n\tstruct rb_node *n = proc->nodes.rb_node;\n\tstruct binder_node *node;\n\n\tassert_spin_locked(&proc->inner_lock);\n\n\twhile (n) {\n\t\tnode = rb_entry(n, struct binder_node, rb_node);\n\n\t\tif (ptr < node->ptr)\n\t\t\tn = n->rb_left;\n\t\telse if (ptr > node->ptr)\n\t\t\tn = n->rb_right;\n\t\telse {\n\t\t\t/*\n\t\t\t * take an implicit weak reference\n\t\t\t * to ensure node stays alive until\n\t\t\t * call to binder_put_node()\n\t\t\t */\n\t\t\tbinder_inc_node_tmpref_ilocked(node);\n\t\t\treturn node;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct binder_node *binder_get_node(struct binder_proc *proc,\n\t\t\t\t\t   binder_uintptr_t ptr)\n{\n\tstruct binder_node *node;\n\n\tbinder_inner_proc_lock(proc);\n\tnode = binder_get_node_ilocked(proc, ptr);\n\tbinder_inner_proc_unlock(proc);\n\treturn node;\n}\n\nstatic struct binder_node *binder_init_node_ilocked(\n\t\t\t\t\t\tstruct binder_proc *proc,\n\t\t\t\t\t\tstruct binder_node *new_node,\n\t\t\t\t\t\tstruct flat_binder_object *fp)\n{\n\tstruct rb_node **p = &proc->nodes.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct binder_node *node;\n\tbinder_uintptr_t ptr = fp ? fp->binder : 0;\n\tbinder_uintptr_t cookie = fp ? fp->cookie : 0;\n\t__u32 flags = fp ? fp->flags : 0;\n\n\tassert_spin_locked(&proc->inner_lock);\n\n\twhile (*p) {\n\n\t\tparent = *p;\n\t\tnode = rb_entry(parent, struct binder_node, rb_node);\n\n\t\tif (ptr < node->ptr)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (ptr > node->ptr)\n\t\t\tp = &(*p)->rb_right;\n\t\telse {\n\t\t\t/*\n\t\t\t * A matching node is already in\n\t\t\t * the rb tree. Abandon the init\n\t\t\t * and return it.\n\t\t\t */\n\t\t\tbinder_inc_node_tmpref_ilocked(node);\n\t\t\treturn node;\n\t\t}\n\t}\n\tnode = new_node;\n\tbinder_stats_created(BINDER_STAT_NODE);\n\tnode->tmp_refs++;\n\trb_link_node(&node->rb_node, parent, p);\n\trb_insert_color(&node->rb_node, &proc->nodes);\n\tnode->debug_id = atomic_inc_return(&binder_last_id);\n\tnode->proc = proc;\n\tnode->ptr = ptr;\n\tnode->cookie = cookie;\n\tnode->work.type = BINDER_WORK_NODE;\n\tnode->min_priority = flags & FLAT_BINDER_FLAG_PRIORITY_MASK;\n\tnode->accept_fds = !!(flags & FLAT_BINDER_FLAG_ACCEPTS_FDS);\n\tnode->txn_security_ctx = !!(flags & FLAT_BINDER_FLAG_TXN_SECURITY_CTX);\n\tspin_lock_init(&node->lock);\n\tINIT_LIST_HEAD(&node->work.entry);\n\tINIT_LIST_HEAD(&node->async_todo);\n\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t     \"%d:%d node %d u%016llx c%016llx created\\n\",\n\t\t     proc->pid, current->pid, node->debug_id,\n\t\t     (u64)node->ptr, (u64)node->cookie);\n\n\treturn node;\n}\n\nstatic struct binder_node *binder_new_node(struct binder_proc *proc,\n\t\t\t\t\t   struct flat_binder_object *fp)\n{\n\tstruct binder_node *node;\n\tstruct binder_node *new_node = kzalloc(sizeof(*node), GFP_KERNEL);\n\n\tif (!new_node)\n\t\treturn NULL;\n\tbinder_inner_proc_lock(proc);\n\tnode = binder_init_node_ilocked(proc, new_node, fp);\n\tbinder_inner_proc_unlock(proc);\n\tif (node != new_node)\n\t\t/*\n\t\t * The node was already added by another thread\n\t\t */\n\t\tkfree(new_node);\n\n\treturn node;\n}\n\nstatic void binder_free_node(struct binder_node *node)\n{\n\tkfree(node);\n\tbinder_stats_deleted(BINDER_STAT_NODE);\n}\n\nstatic int binder_inc_node_nilocked(struct binder_node *node, int strong,\n\t\t\t\t    int internal,\n\t\t\t\t    struct list_head *target_list)\n{\n\tstruct binder_proc *proc = node->proc;\n\n\tassert_spin_locked(&node->lock);\n\tif (proc)\n\t\tassert_spin_locked(&proc->inner_lock);\n\tif (strong) {\n\t\tif (internal) {\n\t\t\tif (target_list == NULL &&\n\t\t\t    node->internal_strong_refs == 0 &&\n\t\t\t    !(node->proc &&\n\t\t\t      node == node->proc->context->binder_context_mgr_node &&\n\t\t\t      node->has_strong_ref)) {\n\t\t\t\tpr_err(\"invalid inc strong node for %d\\n\",\n\t\t\t\t\tnode->debug_id);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnode->internal_strong_refs++;\n\t\t} else\n\t\t\tnode->local_strong_refs++;\n\t\tif (!node->has_strong_ref && target_list) {\n\t\t\tstruct binder_thread *thread = container_of(target_list,\n\t\t\t\t\t\t    struct binder_thread, todo);\n\t\t\tbinder_dequeue_work_ilocked(&node->work);\n\t\t\tBUG_ON(&thread->todo != target_list);\n\t\t\tbinder_enqueue_deferred_thread_work_ilocked(thread,\n\t\t\t\t\t\t\t\t   &node->work);\n\t\t}\n\t} else {\n\t\tif (!internal)\n\t\t\tnode->local_weak_refs++;\n\t\tif (!node->has_weak_ref && list_empty(&node->work.entry)) {\n\t\t\tif (target_list == NULL) {\n\t\t\t\tpr_err(\"invalid inc weak node for %d\\n\",\n\t\t\t\t\tnode->debug_id);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/*\n\t\t\t * See comment above\n\t\t\t */\n\t\t\tbinder_enqueue_work_ilocked(&node->work, target_list);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int binder_inc_node(struct binder_node *node, int strong, int internal,\n\t\t\t   struct list_head *target_list)\n{\n\tint ret;\n\n\tbinder_node_inner_lock(node);\n\tret = binder_inc_node_nilocked(node, strong, internal, target_list);\n\tbinder_node_inner_unlock(node);\n\n\treturn ret;\n}\n\nstatic bool binder_dec_node_nilocked(struct binder_node *node,\n\t\t\t\t     int strong, int internal)\n{\n\tstruct binder_proc *proc = node->proc;\n\n\tassert_spin_locked(&node->lock);\n\tif (proc)\n\t\tassert_spin_locked(&proc->inner_lock);\n\tif (strong) {\n\t\tif (internal)\n\t\t\tnode->internal_strong_refs--;\n\t\telse\n\t\t\tnode->local_strong_refs--;\n\t\tif (node->local_strong_refs || node->internal_strong_refs)\n\t\t\treturn false;\n\t} else {\n\t\tif (!internal)\n\t\t\tnode->local_weak_refs--;\n\t\tif (node->local_weak_refs || node->tmp_refs ||\n\t\t\t\t!hlist_empty(&node->refs))\n\t\t\treturn false;\n\t}\n\n\tif (proc && (node->has_strong_ref || node->has_weak_ref)) {\n\t\tif (list_empty(&node->work.entry)) {\n\t\t\tbinder_enqueue_work_ilocked(&node->work, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t} else {\n\t\tif (hlist_empty(&node->refs) && !node->local_strong_refs &&\n\t\t    !node->local_weak_refs && !node->tmp_refs) {\n\t\t\tif (proc) {\n\t\t\t\tbinder_dequeue_work_ilocked(&node->work);\n\t\t\t\trb_erase(&node->rb_node, &proc->nodes);\n\t\t\t\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t\t\t\t     \"refless node %d deleted\\n\",\n\t\t\t\t\t     node->debug_id);\n\t\t\t} else {\n\t\t\t\tBUG_ON(!list_empty(&node->work.entry));\n\t\t\t\tspin_lock(&binder_dead_nodes_lock);\n\t\t\t\t/*\n\t\t\t\t * tmp_refs could have changed so\n\t\t\t\t * check it again\n\t\t\t\t */\n\t\t\t\tif (node->tmp_refs) {\n\t\t\t\t\tspin_unlock(&binder_dead_nodes_lock);\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t\thlist_del(&node->dead_node);\n\t\t\t\tspin_unlock(&binder_dead_nodes_lock);\n\t\t\t\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t\t\t\t     \"dead node %d deleted\\n\",\n\t\t\t\t\t     node->debug_id);\n\t\t\t}\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic void binder_dec_node(struct binder_node *node, int strong, int internal)\n{\n\tbool free_node;\n\n\tbinder_node_inner_lock(node);\n\tfree_node = binder_dec_node_nilocked(node, strong, internal);\n\tbinder_node_inner_unlock(node);\n\tif (free_node)\n\t\tbinder_free_node(node);\n}\n\nstatic void binder_inc_node_tmpref_ilocked(struct binder_node *node)\n{\n\t/*\n\t * No call to binder_inc_node() is needed since we\n\t * don't need to inform userspace of any changes to\n\t * tmp_refs\n\t */\n\tnode->tmp_refs++;\n}\n\n/**\n * binder_inc_node_tmpref() - take a temporary reference on node\n * @node:\tnode to reference\n *\n * Take reference on node to prevent the node from being freed\n * while referenced only by a local variable. The inner lock is\n * needed to serialize with the node work on the queue (which\n * isn't needed after the node is dead). If the node is dead\n * (node->proc is NULL), use binder_dead_nodes_lock to protect\n * node->tmp_refs against dead-node-only cases where the node\n * lock cannot be acquired (eg traversing the dead node list to\n * print nodes)\n */\nstatic void binder_inc_node_tmpref(struct binder_node *node)\n{\n\tbinder_node_lock(node);\n\tif (node->proc)\n\t\tbinder_inner_proc_lock(node->proc);\n\telse\n\t\tspin_lock(&binder_dead_nodes_lock);\n\tbinder_inc_node_tmpref_ilocked(node);\n\tif (node->proc)\n\t\tbinder_inner_proc_unlock(node->proc);\n\telse\n\t\tspin_unlock(&binder_dead_nodes_lock);\n\tbinder_node_unlock(node);\n}\n\n/**\n * binder_dec_node_tmpref() - remove a temporary reference on node\n * @node:\tnode to reference\n *\n * Release temporary reference on node taken via binder_inc_node_tmpref()\n */\nstatic void binder_dec_node_tmpref(struct binder_node *node)\n{\n\tbool free_node;\n\n\tbinder_node_inner_lock(node);\n\tif (!node->proc)\n\t\tspin_lock(&binder_dead_nodes_lock);\n\telse\n\t\t__acquire(&binder_dead_nodes_lock);\n\tnode->tmp_refs--;\n\tBUG_ON(node->tmp_refs < 0);\n\tif (!node->proc)\n\t\tspin_unlock(&binder_dead_nodes_lock);\n\telse\n\t\t__release(&binder_dead_nodes_lock);\n\t/*\n\t * Call binder_dec_node() to check if all refcounts are 0\n\t * and cleanup is needed. Calling with strong=0 and internal=1\n\t * causes no actual reference to be released in binder_dec_node().\n\t * If that changes, a change is needed here too.\n\t */\n\tfree_node = binder_dec_node_nilocked(node, 0, 1);\n\tbinder_node_inner_unlock(node);\n\tif (free_node)\n\t\tbinder_free_node(node);\n}\n\nstatic void binder_put_node(struct binder_node *node)\n{\n\tbinder_dec_node_tmpref(node);\n}\n\nstatic struct binder_ref *binder_get_ref_olocked(struct binder_proc *proc,\n\t\t\t\t\t\t u32 desc, bool need_strong_ref)\n{\n\tstruct rb_node *n = proc->refs_by_desc.rb_node;\n\tstruct binder_ref *ref;\n\n\twhile (n) {\n\t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n\n\t\tif (desc < ref->data.desc) {\n\t\t\tn = n->rb_left;\n\t\t} else if (desc > ref->data.desc) {\n\t\t\tn = n->rb_right;\n\t\t} else if (need_strong_ref && !ref->data.strong) {\n\t\t\tbinder_user_error(\"tried to use weak ref as strong ref\\n\");\n\t\t\treturn NULL;\n\t\t} else {\n\t\t\treturn ref;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n/**\n * binder_get_ref_for_node_olocked() - get the ref associated with given node\n * @proc:\tbinder_proc that owns the ref\n * @node:\tbinder_node of target\n * @new_ref:\tnewly allocated binder_ref to be initialized or %NULL\n *\n * Look up the ref for the given node and return it if it exists\n *\n * If it doesn't exist and the caller provides a newly allocated\n * ref, initialize the fields of the newly allocated ref and insert\n * into the given proc rb_trees and node refs list.\n *\n * Return:\tthe ref for node. It is possible that another thread\n *\t\tallocated/initialized the ref first in which case the\n *\t\treturned ref would be different than the passed-in\n *\t\tnew_ref. new_ref must be kfree'd by the caller in\n *\t\tthis case.\n */\nstatic struct binder_ref *binder_get_ref_for_node_olocked(\n\t\t\t\t\tstruct binder_proc *proc,\n\t\t\t\t\tstruct binder_node *node,\n\t\t\t\t\tstruct binder_ref *new_ref)\n{\n\tstruct binder_context *context = proc->context;\n\tstruct rb_node **p = &proc->refs_by_node.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct binder_ref *ref;\n\tstruct rb_node *n;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tref = rb_entry(parent, struct binder_ref, rb_node_node);\n\n\t\tif (node < ref->node)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (node > ref->node)\n\t\t\tp = &(*p)->rb_right;\n\t\telse\n\t\t\treturn ref;\n\t}\n\tif (!new_ref)\n\t\treturn NULL;\n\n\tbinder_stats_created(BINDER_STAT_REF);\n\tnew_ref->data.debug_id = atomic_inc_return(&binder_last_id);\n\tnew_ref->proc = proc;\n\tnew_ref->node = node;\n\trb_link_node(&new_ref->rb_node_node, parent, p);\n\trb_insert_color(&new_ref->rb_node_node, &proc->refs_by_node);\n\n\tnew_ref->data.desc = (node == context->binder_context_mgr_node) ? 0 : 1;\n\tfor (n = rb_first(&proc->refs_by_desc); n != NULL; n = rb_next(n)) {\n\t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n\t\tif (ref->data.desc > new_ref->data.desc)\n\t\t\tbreak;\n\t\tnew_ref->data.desc = ref->data.desc + 1;\n\t}\n\n\tp = &proc->refs_by_desc.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tref = rb_entry(parent, struct binder_ref, rb_node_desc);\n\n\t\tif (new_ref->data.desc < ref->data.desc)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (new_ref->data.desc > ref->data.desc)\n\t\t\tp = &(*p)->rb_right;\n\t\telse\n\t\t\tBUG();\n\t}\n\trb_link_node(&new_ref->rb_node_desc, parent, p);\n\trb_insert_color(&new_ref->rb_node_desc, &proc->refs_by_desc);\n\n\tbinder_node_lock(node);\n\thlist_add_head(&new_ref->node_entry, &node->refs);\n\n\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t     \"%d new ref %d desc %d for node %d\\n\",\n\t\t      proc->pid, new_ref->data.debug_id, new_ref->data.desc,\n\t\t      node->debug_id);\n\tbinder_node_unlock(node);\n\treturn new_ref;\n}\n\nstatic void binder_cleanup_ref_olocked(struct binder_ref *ref)\n{\n\tbool delete_node = false;\n\n\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t     \"%d delete ref %d desc %d for node %d\\n\",\n\t\t      ref->proc->pid, ref->data.debug_id, ref->data.desc,\n\t\t      ref->node->debug_id);\n\n\trb_erase(&ref->rb_node_desc, &ref->proc->refs_by_desc);\n\trb_erase(&ref->rb_node_node, &ref->proc->refs_by_node);\n\n\tbinder_node_inner_lock(ref->node);\n\tif (ref->data.strong)\n\t\tbinder_dec_node_nilocked(ref->node, 1, 1);\n\n\thlist_del(&ref->node_entry);\n\tdelete_node = binder_dec_node_nilocked(ref->node, 0, 1);\n\tbinder_node_inner_unlock(ref->node);\n\t/*\n\t * Clear ref->node unless we want the caller to free the node\n\t */\n\tif (!delete_node) {\n\t\t/*\n\t\t * The caller uses ref->node to determine\n\t\t * whether the node needs to be freed. Clear\n\t\t * it since the node is still alive.\n\t\t */\n\t\tref->node = NULL;\n\t}\n\n\tif (ref->death) {\n\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t     \"%d delete ref %d desc %d has death notification\\n\",\n\t\t\t      ref->proc->pid, ref->data.debug_id,\n\t\t\t      ref->data.desc);\n\t\tbinder_dequeue_work(ref->proc, &ref->death->work);\n\t\tbinder_stats_deleted(BINDER_STAT_DEATH);\n\t}\n\tbinder_stats_deleted(BINDER_STAT_REF);\n}\n\n/**\n * binder_inc_ref_olocked() - increment the ref for given handle\n * @ref:         ref to be incremented\n * @strong:      if true, strong increment, else weak\n * @target_list: list to queue node work on\n *\n * Increment the ref. @ref->proc->outer_lock must be held on entry\n *\n * Return: 0, if successful, else errno\n */\nstatic int binder_inc_ref_olocked(struct binder_ref *ref, int strong,\n\t\t\t\t  struct list_head *target_list)\n{\n\tint ret;\n\n\tif (strong) {\n\t\tif (ref->data.strong == 0) {\n\t\t\tret = binder_inc_node(ref->node, 1, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->data.strong++;\n\t} else {\n\t\tif (ref->data.weak == 0) {\n\t\t\tret = binder_inc_node(ref->node, 0, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->data.weak++;\n\t}\n\treturn 0;\n}\n\n/**\n * binder_dec_ref() - dec the ref for given handle\n * @ref:\tref to be decremented\n * @strong:\tif true, strong decrement, else weak\n *\n * Decrement the ref.\n *\n * Return: true if ref is cleaned up and ready to be freed\n */\nstatic bool binder_dec_ref_olocked(struct binder_ref *ref, int strong)\n{\n\tif (strong) {\n\t\tif (ref->data.strong == 0) {\n\t\t\tbinder_user_error(\"%d invalid dec strong, ref %d desc %d s %d w %d\\n\",\n\t\t\t\t\t  ref->proc->pid, ref->data.debug_id,\n\t\t\t\t\t  ref->data.desc, ref->data.strong,\n\t\t\t\t\t  ref->data.weak);\n\t\t\treturn false;\n\t\t}\n\t\tref->data.strong--;\n\t\tif (ref->data.strong == 0)\n\t\t\tbinder_dec_node(ref->node, strong, 1);\n\t} else {\n\t\tif (ref->data.weak == 0) {\n\t\t\tbinder_user_error(\"%d invalid dec weak, ref %d desc %d s %d w %d\\n\",\n\t\t\t\t\t  ref->proc->pid, ref->data.debug_id,\n\t\t\t\t\t  ref->data.desc, ref->data.strong,\n\t\t\t\t\t  ref->data.weak);\n\t\t\treturn false;\n\t\t}\n\t\tref->data.weak--;\n\t}\n\tif (ref->data.strong == 0 && ref->data.weak == 0) {\n\t\tbinder_cleanup_ref_olocked(ref);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/**\n * binder_get_node_from_ref() - get the node from the given proc/desc\n * @proc:\tproc containing the ref\n * @desc:\tthe handle associated with the ref\n * @need_strong_ref: if true, only return node if ref is strong\n * @rdata:\tthe id/refcount data for the ref\n *\n * Given a proc and ref handle, return the associated binder_node\n *\n * Return: a binder_node or NULL if not found or not strong when strong required\n */\nstatic struct binder_node *binder_get_node_from_ref(\n\t\tstruct binder_proc *proc,\n\t\tu32 desc, bool need_strong_ref,\n\t\tstruct binder_ref_data *rdata)\n{\n\tstruct binder_node *node;\n\tstruct binder_ref *ref;\n\n\tbinder_proc_lock(proc);\n\tref = binder_get_ref_olocked(proc, desc, need_strong_ref);\n\tif (!ref)\n\t\tgoto err_no_ref;\n\tnode = ref->node;\n\t/*\n\t * Take an implicit reference on the node to ensure\n\t * it stays alive until the call to binder_put_node()\n\t */\n\tbinder_inc_node_tmpref(node);\n\tif (rdata)\n\t\t*rdata = ref->data;\n\tbinder_proc_unlock(proc);\n\n\treturn node;\n\nerr_no_ref:\n\tbinder_proc_unlock(proc);\n\treturn NULL;\n}\n\n/**\n * binder_free_ref() - free the binder_ref\n * @ref:\tref to free\n *\n * Free the binder_ref. Free the binder_node indicated by ref->node\n * (if non-NULL) and the binder_ref_death indicated by ref->death.\n */\nstatic void binder_free_ref(struct binder_ref *ref)\n{\n\tif (ref->node)\n\t\tbinder_free_node(ref->node);\n\tkfree(ref->death);\n\tkfree(ref);\n}\n\n/**\n * binder_update_ref_for_handle() - inc/dec the ref for given handle\n * @proc:\tproc containing the ref\n * @desc:\tthe handle associated with the ref\n * @increment:\ttrue=inc reference, false=dec reference\n * @strong:\ttrue=strong reference, false=weak reference\n * @rdata:\tthe id/refcount data for the ref\n *\n * Given a proc and ref handle, increment or decrement the ref\n * according to \"increment\" arg.\n *\n * Return: 0 if successful, else errno\n */\nstatic int binder_update_ref_for_handle(struct binder_proc *proc,\n\t\tuint32_t desc, bool increment, bool strong,\n\t\tstruct binder_ref_data *rdata)\n{\n\tint ret = 0;\n\tstruct binder_ref *ref;\n\tbool delete_ref = false;\n\n\tbinder_proc_lock(proc);\n\tref = binder_get_ref_olocked(proc, desc, strong);\n\tif (!ref) {\n\t\tret = -EINVAL;\n\t\tgoto err_no_ref;\n\t}\n\tif (increment)\n\t\tret = binder_inc_ref_olocked(ref, strong, NULL);\n\telse\n\t\tdelete_ref = binder_dec_ref_olocked(ref, strong);\n\n\tif (rdata)\n\t\t*rdata = ref->data;\n\tbinder_proc_unlock(proc);\n\n\tif (delete_ref)\n\t\tbinder_free_ref(ref);\n\treturn ret;\n\nerr_no_ref:\n\tbinder_proc_unlock(proc);\n\treturn ret;\n}\n\n/**\n * binder_dec_ref_for_handle() - dec the ref for given handle\n * @proc:\tproc containing the ref\n * @desc:\tthe handle associated with the ref\n * @strong:\ttrue=strong reference, false=weak reference\n * @rdata:\tthe id/refcount data for the ref\n *\n * Just calls binder_update_ref_for_handle() to decrement the ref.\n *\n * Return: 0 if successful, else errno\n */\nstatic int binder_dec_ref_for_handle(struct binder_proc *proc,\n\t\tuint32_t desc, bool strong, struct binder_ref_data *rdata)\n{\n\treturn binder_update_ref_for_handle(proc, desc, false, strong, rdata);\n}\n\n\n/**\n * binder_inc_ref_for_node() - increment the ref for given proc/node\n * @proc:\t proc containing the ref\n * @node:\t target node\n * @strong:\t true=strong reference, false=weak reference\n * @target_list: worklist to use if node is incremented\n * @rdata:\t the id/refcount data for the ref\n *\n * Given a proc and node, increment the ref. Create the ref if it\n * doesn't already exist\n *\n * Return: 0 if successful, else errno\n */\nstatic int binder_inc_ref_for_node(struct binder_proc *proc,\n\t\t\tstruct binder_node *node,\n\t\t\tbool strong,\n\t\t\tstruct list_head *target_list,\n\t\t\tstruct binder_ref_data *rdata)\n{\n\tstruct binder_ref *ref;\n\tstruct binder_ref *new_ref = NULL;\n\tint ret = 0;\n\n\tbinder_proc_lock(proc);\n\tref = binder_get_ref_for_node_olocked(proc, node, NULL);\n\tif (!ref) {\n\t\tbinder_proc_unlock(proc);\n\t\tnew_ref = kzalloc(sizeof(*ref), GFP_KERNEL);\n\t\tif (!new_ref)\n\t\t\treturn -ENOMEM;\n\t\tbinder_proc_lock(proc);\n\t\tref = binder_get_ref_for_node_olocked(proc, node, new_ref);\n\t}\n\tret = binder_inc_ref_olocked(ref, strong, target_list);\n\t*rdata = ref->data;\n\tbinder_proc_unlock(proc);\n\tif (new_ref && ref != new_ref)\n\t\t/*\n\t\t * Another thread created the ref first so\n\t\t * free the one we allocated\n\t\t */\n\t\tkfree(new_ref);\n\treturn ret;\n}\n\nstatic void binder_pop_transaction_ilocked(struct binder_thread *target_thread,\n\t\t\t\t\t   struct binder_transaction *t)\n{\n\tBUG_ON(!target_thread);\n\tassert_spin_locked(&target_thread->proc->inner_lock);\n\tBUG_ON(target_thread->transaction_stack != t);\n\tBUG_ON(target_thread->transaction_stack->from != target_thread);\n\ttarget_thread->transaction_stack =\n\t\ttarget_thread->transaction_stack->from_parent;\n\tt->from = NULL;\n}\n\n/**\n * binder_thread_dec_tmpref() - decrement thread->tmp_ref\n * @thread:\tthread to decrement\n *\n * A thread needs to be kept alive while being used to create or\n * handle a transaction. binder_get_txn_from() is used to safely\n * extract t->from from a binder_transaction and keep the thread\n * indicated by t->from from being freed. When done with that\n * binder_thread, this function is called to decrement the\n * tmp_ref and free if appropriate (thread has been released\n * and no transaction being processed by the driver)\n */\nstatic void binder_thread_dec_tmpref(struct binder_thread *thread)\n{\n\t/*\n\t * atomic is used to protect the counter value while\n\t * it cannot reach zero or thread->is_dead is false\n\t */\n\tbinder_inner_proc_lock(thread->proc);\n\tatomic_dec(&thread->tmp_ref);\n\tif (thread->is_dead && !atomic_read(&thread->tmp_ref)) {\n\t\tbinder_inner_proc_unlock(thread->proc);\n\t\tbinder_free_thread(thread);\n\t\treturn;\n\t}\n\tbinder_inner_proc_unlock(thread->proc);\n}\n\n/**\n * binder_proc_dec_tmpref() - decrement proc->tmp_ref\n * @proc:\tproc to decrement\n *\n * A binder_proc needs to be kept alive while being used to create or\n * handle a transaction. proc->tmp_ref is incremented when\n * creating a new transaction or the binder_proc is currently in-use\n * by threads that are being released. When done with the binder_proc,\n * this function is called to decrement the counter and free the\n * proc if appropriate (proc has been released, all threads have\n * been released and not currenly in-use to process a transaction).\n */\nstatic void binder_proc_dec_tmpref(struct binder_proc *proc)\n{\n\tbinder_inner_proc_lock(proc);\n\tproc->tmp_ref--;\n\tif (proc->is_dead && RB_EMPTY_ROOT(&proc->threads) &&\n\t\t\t!proc->tmp_ref) {\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_free_proc(proc);\n\t\treturn;\n\t}\n\tbinder_inner_proc_unlock(proc);\n}\n\n/**\n * binder_get_txn_from() - safely extract the \"from\" thread in transaction\n * @t:\tbinder transaction for t->from\n *\n * Atomically return the \"from\" thread and increment the tmp_ref\n * count for the thread to ensure it stays alive until\n * binder_thread_dec_tmpref() is called.\n *\n * Return: the value of t->from\n */\nstatic struct binder_thread *binder_get_txn_from(\n\t\tstruct binder_transaction *t)\n{\n\tstruct binder_thread *from;\n\n\tspin_lock(&t->lock);\n\tfrom = t->from;\n\tif (from)\n\t\tatomic_inc(&from->tmp_ref);\n\tspin_unlock(&t->lock);\n\treturn from;\n}\n\n/**\n * binder_get_txn_from_and_acq_inner() - get t->from and acquire inner lock\n * @t:\tbinder transaction for t->from\n *\n * Same as binder_get_txn_from() except it also acquires the proc->inner_lock\n * to guarantee that the thread cannot be released while operating on it.\n * The caller must call binder_inner_proc_unlock() to release the inner lock\n * as well as call binder_dec_thread_txn() to release the reference.\n *\n * Return: the value of t->from\n */\nstatic struct binder_thread *binder_get_txn_from_and_acq_inner(\n\t\tstruct binder_transaction *t)\n\t__acquires(&t->from->proc->inner_lock)\n{\n\tstruct binder_thread *from;\n\n\tfrom = binder_get_txn_from(t);\n\tif (!from) {\n\t\t__acquire(&from->proc->inner_lock);\n\t\treturn NULL;\n\t}\n\tbinder_inner_proc_lock(from->proc);\n\tif (t->from) {\n\t\tBUG_ON(from != t->from);\n\t\treturn from;\n\t}\n\tbinder_inner_proc_unlock(from->proc);\n\t__acquire(&from->proc->inner_lock);\n\tbinder_thread_dec_tmpref(from);\n\treturn NULL;\n}\n\n/**\n * binder_free_txn_fixups() - free unprocessed fd fixups\n * @t:\tbinder transaction for t->from\n *\n * If the transaction is being torn down prior to being\n * processed by the target process, free all of the\n * fd fixups and fput the file structs. It is safe to\n * call this function after the fixups have been\n * processed -- in that case, the list will be empty.\n */\nstatic void binder_free_txn_fixups(struct binder_transaction *t)\n{\n\tstruct binder_txn_fd_fixup *fixup, *tmp;\n\n\tlist_for_each_entry_safe(fixup, tmp, &t->fd_fixups, fixup_entry) {\n\t\tfput(fixup->file);\n\t\tlist_del(&fixup->fixup_entry);\n\t\tkfree(fixup);\n\t}\n}\n\nstatic void binder_free_transaction(struct binder_transaction *t)\n{\n\tstruct binder_proc *target_proc = t->to_proc;\n\n\tif (target_proc) {\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (t->buffer)\n\t\t\tt->buffer->transaction = NULL;\n\t\tbinder_inner_proc_unlock(target_proc);\n\t}\n\t/*\n\t * If the transaction has no target_proc, then\n\t * t->buffer->transaction has already been cleared.\n\t */\n\tbinder_free_txn_fixups(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n}\n\nstatic void binder_send_failed_reply(struct binder_transaction *t,\n\t\t\t\t     uint32_t error_code)\n{\n\tstruct binder_thread *target_thread;\n\tstruct binder_transaction *next;\n\n\tBUG_ON(t->flags & TF_ONE_WAY);\n\twhile (1) {\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(t);\n\t\tif (target_thread) {\n\t\t\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t     \"send failed reply for transaction %d to %d:%d\\n\",\n\t\t\t\t      t->debug_id,\n\t\t\t\t      target_thread->proc->pid,\n\t\t\t\t      target_thread->pid);\n\n\t\t\tbinder_pop_transaction_ilocked(target_thread, t);\n\t\t\tif (target_thread->reply_error.cmd == BR_OK) {\n\t\t\t\ttarget_thread->reply_error.cmd = error_code;\n\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\ttarget_thread,\n\t\t\t\t\t&target_thread->reply_error.work);\n\t\t\t\twake_up_interruptible(&target_thread->wait);\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * Cannot get here for normal operation, but\n\t\t\t\t * we can if multiple synchronous transactions\n\t\t\t\t * are sent without blocking for responses.\n\t\t\t\t * Just ignore the 2nd error in this case.\n\t\t\t\t */\n\t\t\t\tpr_warn(\"Unexpected reply error: %u\\n\",\n\t\t\t\t\ttarget_thread->reply_error.cmd);\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\tbinder_thread_dec_tmpref(target_thread);\n\t\t\tbinder_free_transaction(t);\n\t\t\treturn;\n\t\t} else {\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t}\n\t\tnext = t->from_parent;\n\n\t\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t     \"send failed reply for transaction %d, target dead\\n\",\n\t\t\t     t->debug_id);\n\n\t\tbinder_free_transaction(t);\n\t\tif (next == NULL) {\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"reply failed, no target thread at root\\n\");\n\t\t\treturn;\n\t\t}\n\t\tt = next;\n\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t     \"reply failed, no target thread -- retry %d\\n\",\n\t\t\t      t->debug_id);\n\t}\n}\n\n/**\n * binder_cleanup_transaction() - cleans up undelivered transaction\n * @t:\t\ttransaction that needs to be cleaned up\n * @reason:\treason the transaction wasn't delivered\n * @error_code:\terror to return to caller (if synchronous call)\n */\nstatic void binder_cleanup_transaction(struct binder_transaction *t,\n\t\t\t\t       const char *reason,\n\t\t\t\t       uint32_t error_code)\n{\n\tif (t->buffer->target_node && !(t->flags & TF_ONE_WAY)) {\n\t\tbinder_send_failed_reply(t, error_code);\n\t} else {\n\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\"undelivered transaction %d, %s\\n\",\n\t\t\tt->debug_id, reason);\n\t\tbinder_free_transaction(t);\n\t}\n}\n\n/**\n * binder_get_object() - gets object and checks for valid metadata\n * @proc:\tbinder_proc owning the buffer\n * @buffer:\tbinder_buffer that we're parsing.\n * @offset:\toffset in the @buffer at which to validate an object.\n * @object:\tstruct binder_object to read into\n *\n * Return:\tIf there's a valid metadata object at @offset in @buffer, the\n *\t\tsize of that object. Otherwise, it returns zero. The object\n *\t\tis read into the struct binder_object pointed to by @object.\n */\nstatic size_t binder_get_object(struct binder_proc *proc,\n\t\t\t\tstruct binder_buffer *buffer,\n\t\t\t\tunsigned long offset,\n\t\t\t\tstruct binder_object *object)\n{\n\tsize_t read_size;\n\tstruct binder_object_header *hdr;\n\tsize_t object_size = 0;\n\n\tread_size = min_t(size_t, sizeof(*object), buffer->data_size - offset);\n\tif (offset > buffer->data_size || read_size < sizeof(*hdr) ||\n\t    binder_alloc_copy_from_buffer(&proc->alloc, object, buffer,\n\t\t\t\t\t  offset, read_size))\n\t\treturn 0;\n\n\t/* Ok, now see if we read a complete object. */\n\thdr = &object->hdr;\n\tswitch (hdr->type) {\n\tcase BINDER_TYPE_BINDER:\n\tcase BINDER_TYPE_WEAK_BINDER:\n\tcase BINDER_TYPE_HANDLE:\n\tcase BINDER_TYPE_WEAK_HANDLE:\n\t\tobject_size = sizeof(struct flat_binder_object);\n\t\tbreak;\n\tcase BINDER_TYPE_FD:\n\t\tobject_size = sizeof(struct binder_fd_object);\n\t\tbreak;\n\tcase BINDER_TYPE_PTR:\n\t\tobject_size = sizeof(struct binder_buffer_object);\n\t\tbreak;\n\tcase BINDER_TYPE_FDA:\n\t\tobject_size = sizeof(struct binder_fd_array_object);\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\tif (offset <= buffer->data_size - object_size &&\n\t    buffer->data_size >= object_size)\n\t\treturn object_size;\n\telse\n\t\treturn 0;\n}\n\n/**\n * binder_validate_ptr() - validates binder_buffer_object in a binder_buffer.\n * @proc:\tbinder_proc owning the buffer\n * @b:\t\tbinder_buffer containing the object\n * @object:\tstruct binder_object to read into\n * @index:\tindex in offset array at which the binder_buffer_object is\n *\t\tlocated\n * @start_offset: points to the start of the offset array\n * @object_offsetp: offset of @object read from @b\n * @num_valid:\tthe number of valid offsets in the offset array\n *\n * Return:\tIf @index is within the valid range of the offset array\n *\t\tdescribed by @start and @num_valid, and if there's a valid\n *\t\tbinder_buffer_object at the offset found in index @index\n *\t\tof the offset array, that object is returned. Otherwise,\n *\t\t%NULL is returned.\n *\t\tNote that the offset found in index @index itself is not\n *\t\tverified; this function assumes that @num_valid elements\n *\t\tfrom @start were previously verified to have valid offsets.\n *\t\tIf @object_offsetp is non-NULL, then the offset within\n *\t\t@b is written to it.\n */\nstatic struct binder_buffer_object *binder_validate_ptr(\n\t\t\t\t\t\tstruct binder_proc *proc,\n\t\t\t\t\t\tstruct binder_buffer *b,\n\t\t\t\t\t\tstruct binder_object *object,\n\t\t\t\t\t\tbinder_size_t index,\n\t\t\t\t\t\tbinder_size_t start_offset,\n\t\t\t\t\t\tbinder_size_t *object_offsetp,\n\t\t\t\t\t\tbinder_size_t num_valid)\n{\n\tsize_t object_size;\n\tbinder_size_t object_offset;\n\tunsigned long buffer_offset;\n\n\tif (index >= num_valid)\n\t\treturn NULL;\n\n\tbuffer_offset = start_offset + sizeof(binder_size_t) * index;\n\tif (binder_alloc_copy_from_buffer(&proc->alloc, &object_offset,\n\t\t\t\t\t  b, buffer_offset,\n\t\t\t\t\t  sizeof(object_offset)))\n\t\treturn NULL;\n\tobject_size = binder_get_object(proc, b, object_offset, object);\n\tif (!object_size || object->hdr.type != BINDER_TYPE_PTR)\n\t\treturn NULL;\n\tif (object_offsetp)\n\t\t*object_offsetp = object_offset;\n\n\treturn &object->bbo;\n}\n\n/**\n * binder_validate_fixup() - validates pointer/fd fixups happen in order.\n * @proc:\t\tbinder_proc owning the buffer\n * @b:\t\t\ttransaction buffer\n * @objects_start_offset: offset to start of objects buffer\n * @buffer_obj_offset:\toffset to binder_buffer_object in which to fix up\n * @fixup_offset:\tstart offset in @buffer to fix up\n * @last_obj_offset:\toffset to last binder_buffer_object that we fixed\n * @last_min_offset:\tminimum fixup offset in object at @last_obj_offset\n *\n * Return:\t\t%true if a fixup in buffer @buffer at offset @offset is\n *\t\t\tallowed.\n *\n * For safety reasons, we only allow fixups inside a buffer to happen\n * at increasing offsets; additionally, we only allow fixup on the last\n * buffer object that was verified, or one of its parents.\n *\n * Example of what is allowed:\n *\n * A\n *   B (parent = A, offset = 0)\n *   C (parent = A, offset = 16)\n *     D (parent = C, offset = 0)\n *   E (parent = A, offset = 32) // min_offset is 16 (C.parent_offset)\n *\n * Examples of what is not allowed:\n *\n * Decreasing offsets within the same parent:\n * A\n *   C (parent = A, offset = 16)\n *   B (parent = A, offset = 0) // decreasing offset within A\n *\n * Referring to a parent that wasn't the last object or any of its parents:\n * A\n *   B (parent = A, offset = 0)\n *   C (parent = A, offset = 0)\n *   C (parent = A, offset = 16)\n *     D (parent = B, offset = 0) // B is not A or any of A's parents\n */\nstatic bool binder_validate_fixup(struct binder_proc *proc,\n\t\t\t\t  struct binder_buffer *b,\n\t\t\t\t  binder_size_t objects_start_offset,\n\t\t\t\t  binder_size_t buffer_obj_offset,\n\t\t\t\t  binder_size_t fixup_offset,\n\t\t\t\t  binder_size_t last_obj_offset,\n\t\t\t\t  binder_size_t last_min_offset)\n{\n\tif (!last_obj_offset) {\n\t\t/* Nothing to fix up in */\n\t\treturn false;\n\t}\n\n\twhile (last_obj_offset != buffer_obj_offset) {\n\t\tunsigned long buffer_offset;\n\t\tstruct binder_object last_object;\n\t\tstruct binder_buffer_object *last_bbo;\n\t\tsize_t object_size = binder_get_object(proc, b, last_obj_offset,\n\t\t\t\t\t\t       &last_object);\n\t\tif (object_size != sizeof(*last_bbo))\n\t\t\treturn false;\n\n\t\tlast_bbo = &last_object.bbo;\n\t\t/*\n\t\t * Safe to retrieve the parent of last_obj, since it\n\t\t * was already previously verified by the driver.\n\t\t */\n\t\tif ((last_bbo->flags & BINDER_BUFFER_FLAG_HAS_PARENT) == 0)\n\t\t\treturn false;\n\t\tlast_min_offset = last_bbo->parent_offset + sizeof(uintptr_t);\n\t\tbuffer_offset = objects_start_offset +\n\t\t\tsizeof(binder_size_t) * last_bbo->parent;\n\t\tif (binder_alloc_copy_from_buffer(&proc->alloc,\n\t\t\t\t\t\t  &last_obj_offset,\n\t\t\t\t\t\t  b, buffer_offset,\n\t\t\t\t\t\t  sizeof(last_obj_offset)))\n\t\t\treturn false;\n\t}\n\treturn (fixup_offset >= last_min_offset);\n}\n\n/**\n * struct binder_task_work_cb - for deferred close\n *\n * @twork:                callback_head for task work\n * @fd:                   fd to close\n *\n * Structure to pass task work to be handled after\n * returning from binder_ioctl() via task_work_add().\n */\nstruct binder_task_work_cb {\n\tstruct callback_head twork;\n\tstruct file *file;\n};\n\n/**\n * binder_do_fd_close() - close list of file descriptors\n * @twork:\tcallback head for task work\n *\n * It is not safe to call ksys_close() during the binder_ioctl()\n * function if there is a chance that binder's own file descriptor\n * might be closed. This is to meet the requirements for using\n * fdget() (see comments for __fget_light()). Therefore use\n * task_work_add() to schedule the close operation once we have\n * returned from binder_ioctl(). This function is a callback\n * for that mechanism and does the actual ksys_close() on the\n * given file descriptor.\n */\nstatic void binder_do_fd_close(struct callback_head *twork)\n{\n\tstruct binder_task_work_cb *twcb = container_of(twork,\n\t\t\tstruct binder_task_work_cb, twork);\n\n\tfput(twcb->file);\n\tkfree(twcb);\n}\n\n/**\n * binder_deferred_fd_close() - schedule a close for the given file-descriptor\n * @fd:\t\tfile-descriptor to close\n *\n * See comments in binder_do_fd_close(). This function is used to schedule\n * a file-descriptor to be closed after returning from binder_ioctl().\n */\nstatic void binder_deferred_fd_close(int fd)\n{\n\tstruct binder_task_work_cb *twcb;\n\n\ttwcb = kzalloc(sizeof(*twcb), GFP_KERNEL);\n\tif (!twcb)\n\t\treturn;\n\tinit_task_work(&twcb->twork, binder_do_fd_close);\n\t__close_fd_get_file(fd, &twcb->file);\n\tif (twcb->file)\n\t\ttask_work_add(current, &twcb->twork, TWA_RESUME);\n\telse\n\t\tkfree(twcb);\n}\n\nstatic void binder_transaction_buffer_release(struct binder_proc *proc,\n\t\t\t\t\t      struct binder_thread *thread,\n\t\t\t\t\t      struct binder_buffer *buffer,\n\t\t\t\t\t      binder_size_t failed_at,\n\t\t\t\t\t      bool is_failure)\n{\n\tint debug_id = buffer->debug_id;\n\tbinder_size_t off_start_offset, buffer_offset, off_end_offset;\n\n\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t     \"%d buffer release %d, size %zd-%zd, failed at %llx\\n\",\n\t\t     proc->pid, buffer->debug_id,\n\t\t     buffer->data_size, buffer->offsets_size,\n\t\t     (unsigned long long)failed_at);\n\n\tif (buffer->target_node)\n\t\tbinder_dec_node(buffer->target_node, 1, 0);\n\n\toff_start_offset = ALIGN(buffer->data_size, sizeof(void *));\n\toff_end_offset = is_failure && failed_at ? failed_at :\n\t\t\t\toff_start_offset + buffer->offsets_size;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size = 0;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (!binder_alloc_copy_from_buffer(&proc->alloc, &object_offset,\n\t\t\t\t\t\t   buffer, buffer_offset,\n\t\t\t\t\t\t   sizeof(object_offset)))\n\t\t\tobject_size = binder_get_object(proc, buffer,\n\t\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0) {\n\t\t\tpr_err(\"transaction release %d bad object at offset %lld, size %zd\\n\",\n\t\t\t       debug_id, (u64)object_offset, buffer->data_size);\n\t\t\tcontinue;\n\t\t}\n\t\thdr = &object.hdr;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_node *node;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tnode = binder_get_node(proc, fp->binder);\n\t\t\tif (node == NULL) {\n\t\t\t\tpr_err(\"transaction release %d bad node %016llx\\n\",\n\t\t\t\t       debug_id, (u64)fp->binder);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        node %d u%016llx\\n\",\n\t\t\t\t     node->debug_id, (u64)node->ptr);\n\t\t\tbinder_dec_node(node, hdr->type == BINDER_TYPE_BINDER,\n\t\t\t\t\t0);\n\t\t\tbinder_put_node(node);\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_ref_data rdata;\n\t\t\tint ret;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_dec_ref_for_handle(proc, fp->handle,\n\t\t\t\thdr->type == BINDER_TYPE_HANDLE, &rdata);\n\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"transaction release %d bad handle %d, ret = %d\\n\",\n\t\t\t\t debug_id, fp->handle, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        ref %d desc %d\\n\",\n\t\t\t\t     rdata.debug_id, rdata.desc);\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\t/*\n\t\t\t * No need to close the file here since user-space\n\t\t\t * closes it for for successfully delivered\n\t\t\t * transactions. For transactions that weren't\n\t\t\t * delivered, the new fd was never allocated so\n\t\t\t * there is no need to close and the fput on the\n\t\t\t * file is done when the transaction is torn\n\t\t\t * down.\n\t\t\t */\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR:\n\t\t\t/*\n\t\t\t * Nothing to do here, this will get cleaned up when the\n\t\t\t * transaction buffer gets freed\n\t\t\t */\n\t\t\tbreak;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_fd_array_object *fda;\n\t\t\tstruct binder_buffer_object *parent;\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t fda_offset;\n\t\t\tsize_t fd_index;\n\t\t\tbinder_size_t fd_buf_size;\n\t\t\tbinder_size_t num_valid;\n\n\t\t\tif (is_failure) {\n\t\t\t\t/*\n\t\t\t\t * The fd fixups have not been applied so no\n\t\t\t\t * fds need to be closed.\n\t\t\t\t */\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tfda = to_binder_fd_array_object(hdr);\n\t\t\tparent = binder_validate_ptr(proc, buffer, &ptr_object,\n\t\t\t\t\t\t     fda->parent,\n\t\t\t\t\t\t     off_start_offset,\n\t\t\t\t\t\t     NULL,\n\t\t\t\t\t\t     num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tpr_err(\"transaction release %d bad parent offset\\n\",\n\t\t\t\t       debug_id);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfd_buf_size = sizeof(u32) * fda->num_fds;\n\t\t\tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {\n\t\t\t\tpr_err(\"transaction release %d invalid number of fds (%lld)\\n\",\n\t\t\t\t       debug_id, (u64)fda->num_fds);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (fd_buf_size > parent->length ||\n\t\t\t    fda->parent_offset > parent->length - fd_buf_size) {\n\t\t\t\t/* No space for all file descriptors here. */\n\t\t\t\tpr_err(\"transaction release %d not enough space for %lld fds in buffer\\n\",\n\t\t\t\t       debug_id, (u64)fda->num_fds);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * the source data for binder_buffer_object is visible\n\t\t\t * to user-space and the @buffer element is the user\n\t\t\t * pointer to the buffer_object containing the fd_array.\n\t\t\t * Convert the address to an offset relative to\n\t\t\t * the base of the transaction buffer.\n\t\t\t */\n\t\t\tfda_offset =\n\t\t\t    (parent->buffer - (uintptr_t)buffer->user_data) +\n\t\t\t    fda->parent_offset;\n\t\t\tfor (fd_index = 0; fd_index < fda->num_fds;\n\t\t\t     fd_index++) {\n\t\t\t\tu32 fd;\n\t\t\t\tint err;\n\t\t\t\tbinder_size_t offset = fda_offset +\n\t\t\t\t\tfd_index * sizeof(fd);\n\n\t\t\t\terr = binder_alloc_copy_from_buffer(\n\t\t\t\t\t\t&proc->alloc, &fd, buffer,\n\t\t\t\t\t\toffset, sizeof(fd));\n\t\t\t\tWARN_ON(err);\n\t\t\t\tif (!err) {\n\t\t\t\t\tbinder_deferred_fd_close(fd);\n\t\t\t\t\t/*\n\t\t\t\t\t * Need to make sure the thread goes\n\t\t\t\t\t * back to userspace to complete the\n\t\t\t\t\t * deferred close\n\t\t\t\t\t */\n\t\t\t\t\tif (thread)\n\t\t\t\t\t\tthread->looper_need_return = true;\n\t\t\t\t}\n\t\t\t}\n\t\t} break;\n\t\tdefault:\n\t\t\tpr_err(\"transaction release %d bad object type %x\\n\",\n\t\t\t\tdebug_id, hdr->type);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int binder_translate_binder(struct flat_binder_object *fp,\n\t\t\t\t   struct binder_transaction *t,\n\t\t\t\t   struct binder_thread *thread)\n{\n\tstruct binder_node *node;\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\tstruct binder_ref_data rdata;\n\tint ret = 0;\n\n\tnode = binder_get_node(proc, fp->binder);\n\tif (!node) {\n\t\tnode = binder_new_node(proc, fp);\n\t\tif (!node)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (fp->cookie != node->cookie) {\n\t\tbinder_user_error(\"%d:%d sending u%016llx node %d, cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fp->binder,\n\t\t\t\t  node->debug_id, (u64)fp->cookie,\n\t\t\t\t  (u64)node->cookie);\n\t\tret = -EINVAL;\n\t\tgoto done;\n\t}\n\tif (security_binder_transfer_binder(proc->cred, target_proc->cred)) {\n\t\tret = -EPERM;\n\t\tgoto done;\n\t}\n\n\tret = binder_inc_ref_for_node(target_proc, node,\n\t\t\tfp->hdr.type == BINDER_TYPE_BINDER,\n\t\t\t&thread->todo, &rdata);\n\tif (ret)\n\t\tgoto done;\n\n\tif (fp->hdr.type == BINDER_TYPE_BINDER)\n\t\tfp->hdr.type = BINDER_TYPE_HANDLE;\n\telse\n\t\tfp->hdr.type = BINDER_TYPE_WEAK_HANDLE;\n\tfp->binder = 0;\n\tfp->handle = rdata.desc;\n\tfp->cookie = 0;\n\n\ttrace_binder_transaction_node_to_ref(t, node, &rdata);\n\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t     \"        node %d u%016llx -> ref %d desc %d\\n\",\n\t\t     node->debug_id, (u64)node->ptr,\n\t\t     rdata.debug_id, rdata.desc);\ndone:\n\tbinder_put_node(node);\n\treturn ret;\n}\n\nstatic int binder_translate_handle(struct flat_binder_object *fp,\n\t\t\t\t   struct binder_transaction *t,\n\t\t\t\t   struct binder_thread *thread)\n{\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\tstruct binder_node *node;\n\tstruct binder_ref_data src_rdata;\n\tint ret = 0;\n\n\tnode = binder_get_node_from_ref(proc, fp->handle,\n\t\t\tfp->hdr.type == BINDER_TYPE_HANDLE, &src_rdata);\n\tif (!node) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid handle, %d\\n\",\n\t\t\t\t  proc->pid, thread->pid, fp->handle);\n\t\treturn -EINVAL;\n\t}\n\tif (security_binder_transfer_binder(proc->cred, target_proc->cred)) {\n\t\tret = -EPERM;\n\t\tgoto done;\n\t}\n\n\tbinder_node_lock(node);\n\tif (node->proc == target_proc) {\n\t\tif (fp->hdr.type == BINDER_TYPE_HANDLE)\n\t\t\tfp->hdr.type = BINDER_TYPE_BINDER;\n\t\telse\n\t\t\tfp->hdr.type = BINDER_TYPE_WEAK_BINDER;\n\t\tfp->binder = node->ptr;\n\t\tfp->cookie = node->cookie;\n\t\tif (node->proc)\n\t\t\tbinder_inner_proc_lock(node->proc);\n\t\telse\n\t\t\t__acquire(&node->proc->inner_lock);\n\t\tbinder_inc_node_nilocked(node,\n\t\t\t\t\t fp->hdr.type == BINDER_TYPE_BINDER,\n\t\t\t\t\t 0, NULL);\n\t\tif (node->proc)\n\t\t\tbinder_inner_proc_unlock(node->proc);\n\t\telse\n\t\t\t__release(&node->proc->inner_lock);\n\t\ttrace_binder_transaction_ref_to_node(t, node, &src_rdata);\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"        ref %d desc %d -> node %d u%016llx\\n\",\n\t\t\t     src_rdata.debug_id, src_rdata.desc, node->debug_id,\n\t\t\t     (u64)node->ptr);\n\t\tbinder_node_unlock(node);\n\t} else {\n\t\tstruct binder_ref_data dest_rdata;\n\n\t\tbinder_node_unlock(node);\n\t\tret = binder_inc_ref_for_node(target_proc, node,\n\t\t\t\tfp->hdr.type == BINDER_TYPE_HANDLE,\n\t\t\t\tNULL, &dest_rdata);\n\t\tif (ret)\n\t\t\tgoto done;\n\n\t\tfp->binder = 0;\n\t\tfp->handle = dest_rdata.desc;\n\t\tfp->cookie = 0;\n\t\ttrace_binder_transaction_ref_to_ref(t, node, &src_rdata,\n\t\t\t\t\t\t    &dest_rdata);\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"        ref %d desc %d -> ref %d desc %d (node %d)\\n\",\n\t\t\t     src_rdata.debug_id, src_rdata.desc,\n\t\t\t     dest_rdata.debug_id, dest_rdata.desc,\n\t\t\t     node->debug_id);\n\t}\ndone:\n\tbinder_put_node(node);\n\treturn ret;\n}\n\nstatic int binder_translate_fd(u32 fd, binder_size_t fd_offset,\n\t\t\t       struct binder_transaction *t,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction *in_reply_to)\n{\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\tstruct binder_txn_fd_fixup *fixup;\n\tstruct file *file;\n\tint ret = 0;\n\tbool target_allows_fd;\n\n\tif (in_reply_to)\n\t\ttarget_allows_fd = !!(in_reply_to->flags & TF_ACCEPT_FDS);\n\telse\n\t\ttarget_allows_fd = t->buffer->target_node->accept_fds;\n\tif (!target_allows_fd) {\n\t\tbinder_user_error(\"%d:%d got %s with fd, %d, but target does not allow fds\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  in_reply_to ? \"reply\" : \"transaction\",\n\t\t\t\t  fd);\n\t\tret = -EPERM;\n\t\tgoto err_fd_not_accepted;\n\t}\n\n\tfile = fget(fd);\n\tif (!file) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid fd, %d\\n\",\n\t\t\t\t  proc->pid, thread->pid, fd);\n\t\tret = -EBADF;\n\t\tgoto err_fget;\n\t}\n\tret = security_binder_transfer_file(proc->cred, target_proc->cred, file);\n\tif (ret < 0) {\n\t\tret = -EPERM;\n\t\tgoto err_security;\n\t}\n\n\t/*\n\t * Add fixup record for this transaction. The allocation\n\t * of the fd in the target needs to be done from a\n\t * target thread.\n\t */\n\tfixup = kzalloc(sizeof(*fixup), GFP_KERNEL);\n\tif (!fixup) {\n\t\tret = -ENOMEM;\n\t\tgoto err_alloc;\n\t}\n\tfixup->file = file;\n\tfixup->offset = fd_offset;\n\ttrace_binder_transaction_fd_send(t, fd, fixup->offset);\n\tlist_add_tail(&fixup->fixup_entry, &t->fd_fixups);\n\n\treturn ret;\n\nerr_alloc:\nerr_security:\n\tfput(file);\nerr_fget:\nerr_fd_not_accepted:\n\treturn ret;\n}\n\nstatic int binder_translate_fd_array(struct binder_fd_array_object *fda,\n\t\t\t\t     struct binder_buffer_object *parent,\n\t\t\t\t     struct binder_transaction *t,\n\t\t\t\t     struct binder_thread *thread,\n\t\t\t\t     struct binder_transaction *in_reply_to)\n{\n\tbinder_size_t fdi, fd_buf_size;\n\tbinder_size_t fda_offset;\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\n\tfd_buf_size = sizeof(u32) * fda->num_fds;\n\tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid number of fds (%lld)\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fda->num_fds);\n\t\treturn -EINVAL;\n\t}\n\tif (fd_buf_size > parent->length ||\n\t    fda->parent_offset > parent->length - fd_buf_size) {\n\t\t/* No space for all file descriptors here. */\n\t\tbinder_user_error(\"%d:%d not enough space to store %lld fds in buffer\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fda->num_fds);\n\t\treturn -EINVAL;\n\t}\n\t/*\n\t * the source data for binder_buffer_object is visible\n\t * to user-space and the @buffer element is the user\n\t * pointer to the buffer_object containing the fd_array.\n\t * Convert the address to an offset relative to\n\t * the base of the transaction buffer.\n\t */\n\tfda_offset = (parent->buffer - (uintptr_t)t->buffer->user_data) +\n\t\tfda->parent_offset;\n\tif (!IS_ALIGNED((unsigned long)fda_offset, sizeof(u32))) {\n\t\tbinder_user_error(\"%d:%d parent offset not aligned correctly.\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\tfor (fdi = 0; fdi < fda->num_fds; fdi++) {\n\t\tu32 fd;\n\t\tint ret;\n\t\tbinder_size_t offset = fda_offset + fdi * sizeof(fd);\n\n\t\tret = binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t    &fd, t->buffer,\n\t\t\t\t\t\t    offset, sizeof(fd));\n\t\tif (!ret)\n\t\t\tret = binder_translate_fd(fd, offset, t, thread,\n\t\t\t\t\t\t  in_reply_to);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic int binder_fixup_parent(struct binder_transaction *t,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_buffer_object *bp,\n\t\t\t       binder_size_t off_start_offset,\n\t\t\t       binder_size_t num_valid,\n\t\t\t       binder_size_t last_fixup_obj_off,\n\t\t\t       binder_size_t last_fixup_min_off)\n{\n\tstruct binder_buffer_object *parent;\n\tstruct binder_buffer *b = t->buffer;\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\tstruct binder_object object;\n\tbinder_size_t buffer_offset;\n\tbinder_size_t parent_offset;\n\n\tif (!(bp->flags & BINDER_BUFFER_FLAG_HAS_PARENT))\n\t\treturn 0;\n\n\tparent = binder_validate_ptr(target_proc, b, &object, bp->parent,\n\t\t\t\t     off_start_offset, &parent_offset,\n\t\t\t\t     num_valid);\n\tif (!parent) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!binder_validate_fixup(target_proc, b, off_start_offset,\n\t\t\t\t   parent_offset, bp->parent_offset,\n\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t   last_fixup_min_off)) {\n\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\n\tif (parent->length < sizeof(binder_uintptr_t) ||\n\t    bp->parent_offset > parent->length - sizeof(binder_uintptr_t)) {\n\t\t/* No space for a pointer here! */\n\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\tbuffer_offset = bp->parent_offset +\n\t\t\t(uintptr_t)parent->buffer - (uintptr_t)b->user_data;\n\tif (binder_alloc_copy_to_buffer(&target_proc->alloc, b, buffer_offset,\n\t\t\t\t\t&bp->buffer, sizeof(bp->buffer))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n/**\n * binder_proc_transaction() - sends a transaction to a process and wakes it up\n * @t:\t\ttransaction to send\n * @proc:\tprocess to send the transaction to\n * @thread:\tthread in @proc to send the transaction to (may be NULL)\n *\n * This function queues a transaction to the specified process. It will try\n * to find a thread in the target process to handle the transaction and\n * wake it up. If no thread is found, the work is queued to the proc\n * waitqueue.\n *\n * If the @thread parameter is not NULL, the transaction is always queued\n * to the waitlist of that specific thread.\n *\n * Return:\ttrue if the transactions was successfully queued\n *\t\tfalse if the target process or thread is dead\n */\nstatic bool binder_proc_transaction(struct binder_transaction *t,\n\t\t\t\t    struct binder_proc *proc,\n\t\t\t\t    struct binder_thread *thread)\n{\n\tstruct binder_node *node = t->buffer->target_node;\n\tbool oneway = !!(t->flags & TF_ONE_WAY);\n\tbool pending_async = false;\n\n\tBUG_ON(!node);\n\tbinder_node_lock(node);\n\tif (oneway) {\n\t\tBUG_ON(thread);\n\t\tif (node->has_async_transaction) {\n\t\t\tpending_async = true;\n\t\t} else {\n\t\t\tnode->has_async_transaction = true;\n\t\t}\n\t}\n\n\tbinder_inner_proc_lock(proc);\n\n\tif (proc->is_dead || (thread && thread->is_dead)) {\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_node_unlock(node);\n\t\treturn false;\n\t}\n\n\tif (!thread && !pending_async)\n\t\tthread = binder_select_thread_ilocked(proc);\n\n\tif (thread)\n\t\tbinder_enqueue_thread_work_ilocked(thread, &t->work);\n\telse if (!pending_async)\n\t\tbinder_enqueue_work_ilocked(&t->work, &proc->todo);\n\telse\n\t\tbinder_enqueue_work_ilocked(&t->work, &node->async_todo);\n\n\tif (!pending_async)\n\t\tbinder_wakeup_thread_ilocked(proc, thread, !oneway /* sync */);\n\n\tbinder_inner_proc_unlock(proc);\n\tbinder_node_unlock(node);\n\n\treturn true;\n}\n\n/**\n * binder_get_node_refs_for_txn() - Get required refs on node for txn\n * @node:         struct binder_node for which to get refs\n * @proc:         returns @node->proc if valid\n * @error:        if no @proc then returns BR_DEAD_REPLY\n *\n * User-space normally keeps the node alive when creating a transaction\n * since it has a reference to the target. The local strong ref keeps it\n * alive if the sending process dies before the target process processes\n * the transaction. If the source process is malicious or has a reference\n * counting bug, relying on the local strong ref can fail.\n *\n * Since user-space can cause the local strong ref to go away, we also take\n * a tmpref on the node to ensure it survives while we are constructing\n * the transaction. We also need a tmpref on the proc while we are\n * constructing the transaction, so we take that here as well.\n *\n * Return: The target_node with refs taken or NULL if no @node->proc is NULL.\n * Also sets @proc if valid. If the @node->proc is NULL indicating that the\n * target proc has died, @error is set to BR_DEAD_REPLY\n */\nstatic struct binder_node *binder_get_node_refs_for_txn(\n\t\tstruct binder_node *node,\n\t\tstruct binder_proc **procp,\n\t\tuint32_t *error)\n{\n\tstruct binder_node *target_node = NULL;\n\n\tbinder_node_inner_lock(node);\n\tif (node->proc) {\n\t\ttarget_node = node;\n\t\tbinder_inc_node_nilocked(node, 1, 0, NULL);\n\t\tbinder_inc_node_tmpref_ilocked(node);\n\t\tnode->proc->tmp_ref++;\n\t\t*procp = node->proc;\n\t} else\n\t\t*error = BR_DEAD_REPLY;\n\tbinder_node_inner_unlock(node);\n\n\treturn target_node;\n}\n\nstatic void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\tstrscpy(e->context_name, proc->context->name, BINDERFS_MAX_NAME);\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc->pid == proc->pid) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (WARN_ON(proc == target_proc)) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tif (security_binder_transaction(proc->cred,\n\t\t\t\t\t\ttarget_proc->cred) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\tsecurity_cred_getsecid(proc->cred, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tint err;\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\terr = binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  t->buffer, buf_offset,\n\t\t\t\t\t\t  secctx, secctx_sz);\n\t\tif (err) {\n\t\t\tt->security_ctx = 0;\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\tt->buffer->clear_on_free = !!(t->flags & TF_CLEAR_BUF);\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size -\n\t\tALIGN(secctx_sz, sizeof(u64));\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  &object_offset,\n\t\t\t\t\t\t  t->buffer,\n\t\t\t\t\t\t  buffer_offset,\n\t\t\t\t\t\t  sizeof(object_offset))) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tfp->pad_binder = 0;\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tbp, sizeof(*bp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, NULL, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}\n\n/**\n * binder_free_buf() - free the specified buffer\n * @proc:\tbinder proc that owns buffer\n * @buffer:\tbuffer to be freed\n * @is_failure:\tfailed to send transaction\n *\n * If buffer for an async transaction, enqueue the next async\n * transaction from the node.\n *\n * Cleanup buffer and free it.\n */\nstatic void\nbinder_free_buf(struct binder_proc *proc,\n\t\tstruct binder_thread *thread,\n\t\tstruct binder_buffer *buffer, bool is_failure)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}\n\nstatic int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node) {\n\t\t\t\t\tif (ctx_mgr_node->proc == proc) {\n\t\t\t\t\t\tbinder_user_error(\"%d:%d context manager tried to acquire desc 0\\n\",\n\t\t\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\t}\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\t}\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\t\t\tbool free_node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbinder_put_node(node);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_node_inner_lock(node);\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tfree_node = binder_dec_node_nilocked(node,\n\t\t\t\t\tcmd == BC_ACQUIRE_DONE, 0);\n\t\t\tWARN_ON(free_node);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d tr %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs,\n\t\t\t\t     node->local_weak_refs, node->tmp_refs);\n\t\t\tbinder_node_inner_unlock(node);\n\t\t\tbinder_put_node(node);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (IS_ERR_OR_NULL(buffer)) {\n\t\t\t\tif (PTR_ERR(buffer) == -EPERM) {\n\t\t\t\t\tbinder_user_error(\n\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned or currently freeing buffer\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)data_ptr);\n\t\t\t\t} else {\n\t\t\t\t\tbinder_user_error(\n\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)data_ptr);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\t\t\tbinder_free_buf(proc, thread, buffer, false);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\t/*\n\t\t\t\t * Allocate memory for death notification\n\t\t\t\t * before taking lock\n\t\t\t\t */\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tbinder_enqueue_thread_work(\n\t\t\t\t\t\tthread,\n\t\t\t\t\t\t&thread->return_error.work);\n\t\t\t\t\tbinder_debug(\n\t\t\t\t\t\tBINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t\"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tbinder_node_lock(ref->node);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tkfree(death);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\n\t\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t&ref->death->work, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper &\n\t\t\t\t\t    (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t     BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\t\t\tthread,\n\t\t\t\t\t\t\t\t&death->work);\n\t\t\t\t\telse {\n\t\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\t\tbinder_wakeup_proc_ilocked(\n\t\t\t\t\t\t\t\tproc);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tbinder_node_unlock(ref->node);\n\t\t\tbinder_proc_unlock(proc);\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death,\n\t\t\t\t\t    entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death =\n\t\t\t\t\tcontainer_of(w,\n\t\t\t\t\t\t     struct binder_ref_death,\n\t\t\t\t\t\t     work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_dequeue_work_ilocked(&death->work);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper &\n\t\t\t\t\t(BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\tthread, &death->work);\n\t\t\t\telse {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}\n\nstatic void binder_stat_br(struct binder_proc *proc,\n\t\t\t   struct binder_thread *thread, uint32_t cmd)\n{\n\ttrace_binder_return(cmd);\n\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.br)) {\n\t\tatomic_inc(&binder_stats.br[_IOC_NR(cmd)]);\n\t\tatomic_inc(&proc->stats.br[_IOC_NR(cmd)]);\n\t\tatomic_inc(&thread->stats.br[_IOC_NR(cmd)]);\n\t}\n}\n\nstatic int binder_put_node_cmd(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       void __user **ptrp,\n\t\t\t       binder_uintptr_t node_ptr,\n\t\t\t       binder_uintptr_t node_cookie,\n\t\t\t       int node_debug_id,\n\t\t\t       uint32_t cmd, const char *cmd_name)\n{\n\tvoid __user *ptr = *ptrp;\n\n\tif (put_user(cmd, (uint32_t __user *)ptr))\n\t\treturn -EFAULT;\n\tptr += sizeof(uint32_t);\n\n\tif (put_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\treturn -EFAULT;\n\tptr += sizeof(binder_uintptr_t);\n\n\tif (put_user(node_cookie, (binder_uintptr_t __user *)ptr))\n\t\treturn -EFAULT;\n\tptr += sizeof(binder_uintptr_t);\n\n\tbinder_stat_br(proc, thread, cmd);\n\tbinder_debug(BINDER_DEBUG_USER_REFS, \"%d:%d %s %d u%016llx c%016llx\\n\",\n\t\t     proc->pid, thread->pid, cmd_name, node_debug_id,\n\t\t     (u64)node_ptr, (u64)node_cookie);\n\n\t*ptrp = ptr;\n\treturn 0;\n}\n\nstatic int binder_wait_for_work(struct binder_thread *thread,\n\t\t\t\tbool do_proc_work)\n{\n\tDEFINE_WAIT(wait);\n\tstruct binder_proc *proc = thread->proc;\n\tint ret = 0;\n\n\tfreezer_do_not_count();\n\tbinder_inner_proc_lock(proc);\n\tfor (;;) {\n\t\tprepare_to_wait(&thread->wait, &wait, TASK_INTERRUPTIBLE);\n\t\tif (binder_has_work_ilocked(thread, do_proc_work))\n\t\t\tbreak;\n\t\tif (do_proc_work)\n\t\t\tlist_add(&thread->waiting_thread_node,\n\t\t\t\t &proc->waiting_threads);\n\t\tbinder_inner_proc_unlock(proc);\n\t\tschedule();\n\t\tbinder_inner_proc_lock(proc);\n\t\tlist_del_init(&thread->waiting_thread_node);\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t}\n\tfinish_wait(&thread->wait, &wait);\n\tbinder_inner_proc_unlock(proc);\n\tfreezer_count();\n\n\treturn ret;\n}\n\n/**\n * binder_apply_fd_fixups() - finish fd translation\n * @proc:         binder_proc associated @t->buffer\n * @t:\tbinder transaction with list of fd fixups\n *\n * Now that we are in the context of the transaction target\n * process, we can allocate and install fds. Process the\n * list of fds to translate and fixup the buffer with the\n * new fds.\n *\n * If we fail to allocate an fd, then free the resources by\n * fput'ing files that have not been processed and ksys_close'ing\n * any fds that have already been allocated.\n */\nstatic int binder_apply_fd_fixups(struct binder_proc *proc,\n\t\t\t\t  struct binder_transaction *t)\n{\n\tstruct binder_txn_fd_fixup *fixup, *tmp;\n\tint ret = 0;\n\n\tlist_for_each_entry(fixup, &t->fd_fixups, fixup_entry) {\n\t\tint fd = get_unused_fd_flags(O_CLOEXEC);\n\n\t\tif (fd < 0) {\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"failed fd fixup txn %d fd %d\\n\",\n\t\t\t\t     t->debug_id, fd);\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"fd fixup txn %d fd %d\\n\",\n\t\t\t     t->debug_id, fd);\n\t\ttrace_binder_transaction_fd_recv(t, fd, fixup->offset);\n\t\tfd_install(fd, fixup->file);\n\t\tfixup->file = NULL;\n\t\tif (binder_alloc_copy_to_buffer(&proc->alloc, t->buffer,\n\t\t\t\t\t\tfixup->offset, &fd,\n\t\t\t\t\t\tsizeof(u32))) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\tlist_for_each_entry_safe(fixup, tmp, &t->fd_fixups, fixup_entry) {\n\t\tif (fixup->file) {\n\t\t\tfput(fixup->file);\n\t\t} else if (ret) {\n\t\t\tu32 fd;\n\t\t\tint err;\n\n\t\t\terr = binder_alloc_copy_from_buffer(&proc->alloc, &fd,\n\t\t\t\t\t\t\t    t->buffer,\n\t\t\t\t\t\t\t    fixup->offset,\n\t\t\t\t\t\t\t    sizeof(fd));\n\t\t\tWARN_ON(err);\n\t\t\tif (!err)\n\t\t\t\tbinder_deferred_fd_close(fd);\n\t\t}\n\t\tlist_del(&fixup->fixup_entry);\n\t\tkfree(fixup);\n\t}\n\n\treturn ret;\n}\n\nstatic int binder_thread_read(struct binder_proc *proc,\n\t\t\t      struct binder_thread *thread,\n\t\t\t      binder_uintptr_t binder_buffer, size_t size,\n\t\t\t      binder_size_t *consumed, int non_block)\n{\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\tint ret = 0;\n\tint wait_for_proc_work;\n\n\tif (*consumed == 0) {\n\t\tif (put_user(BR_NOOP, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t}\n\nretry:\n\tbinder_inner_proc_lock(proc);\n\twait_for_proc_work = binder_available_for_proc_work_ilocked(thread);\n\tbinder_inner_proc_unlock(proc);\n\n\tthread->looper |= BINDER_LOOPER_STATE_WAITING;\n\n\ttrace_binder_wait_for_work(wait_for_proc_work,\n\t\t\t\t   !!thread->transaction_stack,\n\t\t\t\t   !binder_worklist_empty(proc, &thread->todo));\n\tif (wait_for_proc_work) {\n\t\tif (!(thread->looper & (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\tBINDER_LOOPER_STATE_ENTERED))) {\n\t\t\tbinder_user_error(\"%d:%d ERROR: Thread waiting for process work before calling BC_REGISTER_LOOPER or BC_ENTER_LOOPER (state %x)\\n\",\n\t\t\t\tproc->pid, thread->pid, thread->looper);\n\t\t\twait_event_interruptible(binder_user_error_wait,\n\t\t\t\t\t\t binder_stop_on_user_error < 2);\n\t\t}\n\t\tbinder_set_nice(proc->default_priority);\n\t}\n\n\tif (non_block) {\n\t\tif (!binder_has_work(thread, wait_for_proc_work))\n\t\t\tret = -EAGAIN;\n\t} else {\n\t\tret = binder_wait_for_work(thread, wait_for_proc_work);\n\t}\n\n\tthread->looper &= ~BINDER_LOOPER_STATE_WAITING;\n\n\tif (ret)\n\t\treturn ret;\n\n\twhile (1) {\n\t\tuint32_t cmd;\n\t\tstruct binder_transaction_data_secctx tr;\n\t\tstruct binder_transaction_data *trd = &tr.transaction_data;\n\t\tstruct binder_work *w = NULL;\n\t\tstruct list_head *list = NULL;\n\t\tstruct binder_transaction *t = NULL;\n\t\tstruct binder_thread *t_from;\n\t\tsize_t trsize = sizeof(*trd);\n\n\t\tbinder_inner_proc_lock(proc);\n\t\tif (!binder_worklist_empty_ilocked(&thread->todo))\n\t\t\tlist = &thread->todo;\n\t\telse if (!binder_worklist_empty_ilocked(&proc->todo) &&\n\t\t\t   wait_for_proc_work)\n\t\t\tlist = &proc->todo;\n\t\telse {\n\t\t\tbinder_inner_proc_unlock(proc);\n\n\t\t\t/* no data added */\n\t\t\tif (ptr - buffer == 4 && !thread->looper_need_return)\n\t\t\t\tgoto retry;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (end - ptr < sizeof(tr) + 4) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\t}\n\t\tw = binder_dequeue_work_head_ilocked(list);\n\t\tif (binder_worklist_empty_ilocked(&thread->todo))\n\t\t\tthread->process_todo = false;\n\n\t\tswitch (w->type) {\n\t\tcase BINDER_WORK_TRANSACTION: {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tt = container_of(w, struct binder_transaction, work);\n\t\t} break;\n\t\tcase BINDER_WORK_RETURN_ERROR: {\n\t\t\tstruct binder_error *e = container_of(\n\t\t\t\t\tw, struct binder_error, work);\n\n\t\t\tWARN_ON(e->cmd == BR_OK);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tif (put_user(e->cmd, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tcmd = e->cmd;\n\t\t\te->cmd = BR_OK;\n\t\t\tptr += sizeof(uint32_t);\n\n\t\t\tbinder_stat_br(proc, thread, cmd);\n\t\t} break;\n\t\tcase BINDER_WORK_TRANSACTION_COMPLETE: {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tcmd = BR_TRANSACTION_COMPLETE;\n\t\t\tkfree(w);\n\t\t\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\n\t\t\tif (put_user(cmd, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\n\t\t\tbinder_stat_br(proc, thread, cmd);\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION_COMPLETE,\n\t\t\t\t     \"%d:%d BR_TRANSACTION_COMPLETE\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t} break;\n\t\tcase BINDER_WORK_NODE: {\n\t\t\tstruct binder_node *node = container_of(w, struct binder_node, work);\n\t\t\tint strong, weak;\n\t\t\tbinder_uintptr_t node_ptr = node->ptr;\n\t\t\tbinder_uintptr_t node_cookie = node->cookie;\n\t\t\tint node_debug_id = node->debug_id;\n\t\t\tint has_weak_ref;\n\t\t\tint has_strong_ref;\n\t\t\tvoid __user *orig_ptr = ptr;\n\n\t\t\tBUG_ON(proc != node->proc);\n\t\t\tstrong = node->internal_strong_refs ||\n\t\t\t\t\tnode->local_strong_refs;\n\t\t\tweak = !hlist_empty(&node->refs) ||\n\t\t\t\t\tnode->local_weak_refs ||\n\t\t\t\t\tnode->tmp_refs || strong;\n\t\t\thas_strong_ref = node->has_strong_ref;\n\t\t\thas_weak_ref = node->has_weak_ref;\n\n\t\t\tif (weak && !has_weak_ref) {\n\t\t\t\tnode->has_weak_ref = 1;\n\t\t\t\tnode->pending_weak_ref = 1;\n\t\t\t\tnode->local_weak_refs++;\n\t\t\t}\n\t\t\tif (strong && !has_strong_ref) {\n\t\t\t\tnode->has_strong_ref = 1;\n\t\t\t\tnode->pending_strong_ref = 1;\n\t\t\t\tnode->local_strong_refs++;\n\t\t\t}\n\t\t\tif (!strong && has_strong_ref)\n\t\t\t\tnode->has_strong_ref = 0;\n\t\t\tif (!weak && has_weak_ref)\n\t\t\t\tnode->has_weak_ref = 0;\n\t\t\tif (!weak && !strong) {\n\t\t\t\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t\t\t\t     \"%d:%d node %d u%016llx c%016llx deleted\\n\",\n\t\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t\t     node_debug_id,\n\t\t\t\t\t     (u64)node_ptr,\n\t\t\t\t\t     (u64)node_cookie);\n\t\t\t\trb_erase(&node->rb_node, &proc->nodes);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbinder_node_lock(node);\n\t\t\t\t/*\n\t\t\t\t * Acquire the node lock before freeing the\n\t\t\t\t * node to serialize with other threads that\n\t\t\t\t * may have been holding the node lock while\n\t\t\t\t * decrementing this node (avoids race where\n\t\t\t\t * this thread frees while the other thread\n\t\t\t\t * is unlocking the node after the final\n\t\t\t\t * decrement)\n\t\t\t\t */\n\t\t\t\tbinder_node_unlock(node);\n\t\t\t\tbinder_free_node(node);\n\t\t\t} else\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\n\t\t\tif (weak && !has_weak_ref)\n\t\t\t\tret = binder_put_node_cmd(\n\t\t\t\t\t\tproc, thread, &ptr, node_ptr,\n\t\t\t\t\t\tnode_cookie, node_debug_id,\n\t\t\t\t\t\tBR_INCREFS, \"BR_INCREFS\");\n\t\t\tif (!ret && strong && !has_strong_ref)\n\t\t\t\tret = binder_put_node_cmd(\n\t\t\t\t\t\tproc, thread, &ptr, node_ptr,\n\t\t\t\t\t\tnode_cookie, node_debug_id,\n\t\t\t\t\t\tBR_ACQUIRE, \"BR_ACQUIRE\");\n\t\t\tif (!ret && !strong && has_strong_ref)\n\t\t\t\tret = binder_put_node_cmd(\n\t\t\t\t\t\tproc, thread, &ptr, node_ptr,\n\t\t\t\t\t\tnode_cookie, node_debug_id,\n\t\t\t\t\t\tBR_RELEASE, \"BR_RELEASE\");\n\t\t\tif (!ret && !weak && has_weak_ref)\n\t\t\t\tret = binder_put_node_cmd(\n\t\t\t\t\t\tproc, thread, &ptr, node_ptr,\n\t\t\t\t\t\tnode_cookie, node_debug_id,\n\t\t\t\t\t\tBR_DECREFS, \"BR_DECREFS\");\n\t\t\tif (orig_ptr == ptr)\n\t\t\t\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t\t\t\t     \"%d:%d node %d u%016llx c%016llx state unchanged\\n\",\n\t\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t\t     node_debug_id,\n\t\t\t\t\t     (u64)node_ptr,\n\t\t\t\t\t     (u64)node_cookie);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t} break;\n\t\tcase BINDER_WORK_DEAD_BINDER:\n\t\tcase BINDER_WORK_DEAD_BINDER_AND_CLEAR:\n\t\tcase BINDER_WORK_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tstruct binder_ref_death *death;\n\t\t\tuint32_t cmd;\n\t\t\tbinder_uintptr_t cookie;\n\n\t\t\tdeath = container_of(w, struct binder_ref_death, work);\n\t\t\tif (w->type == BINDER_WORK_CLEAR_DEATH_NOTIFICATION)\n\t\t\t\tcmd = BR_CLEAR_DEATH_NOTIFICATION_DONE;\n\t\t\telse\n\t\t\t\tcmd = BR_DEAD_BINDER;\n\t\t\tcookie = death->cookie;\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx\\n\",\n\t\t\t\t      proc->pid, thread->pid,\n\t\t\t\t      cmd == BR_DEAD_BINDER ?\n\t\t\t\t      \"BR_DEAD_BINDER\" :\n\t\t\t\t      \"BR_CLEAR_DEATH_NOTIFICATION_DONE\",\n\t\t\t\t      (u64)cookie);\n\t\t\tif (w->type == BINDER_WORK_CLEAR_DEATH_NOTIFICATION) {\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbinder_stats_deleted(BINDER_STAT_DEATH);\n\t\t\t} else {\n\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\tw, &proc->delivered_death);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tif (put_user(cmd, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (put_user(cookie,\n\t\t\t\t     (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tbinder_stat_br(proc, thread, cmd);\n\t\t\tif (cmd == BR_DEAD_BINDER)\n\t\t\t\tgoto done; /* DEAD_BINDER notifications can cause transactions */\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tpr_err(\"%d:%d: bad work type %d\\n\",\n\t\t\t       proc->pid, thread->pid, w->type);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!t)\n\t\t\tcontinue;\n\n\t\tBUG_ON(t->buffer == NULL);\n\t\tif (t->buffer->target_node) {\n\t\t\tstruct binder_node *target_node = t->buffer->target_node;\n\n\t\t\ttrd->target.ptr = target_node->ptr;\n\t\t\ttrd->cookie =  target_node->cookie;\n\t\t\tt->saved_priority = task_nice(current);\n\t\t\tif (t->priority < target_node->min_priority &&\n\t\t\t    !(t->flags & TF_ONE_WAY))\n\t\t\t\tbinder_set_nice(t->priority);\n\t\t\telse if (!(t->flags & TF_ONE_WAY) ||\n\t\t\t\t t->saved_priority > target_node->min_priority)\n\t\t\t\tbinder_set_nice(target_node->min_priority);\n\t\t\tcmd = BR_TRANSACTION;\n\t\t} else {\n\t\t\ttrd->target.ptr = 0;\n\t\t\ttrd->cookie = 0;\n\t\t\tcmd = BR_REPLY;\n\t\t}\n\t\ttrd->code = t->code;\n\t\ttrd->flags = t->flags;\n\t\ttrd->sender_euid = from_kuid(current_user_ns(), t->sender_euid);\n\n\t\tt_from = binder_get_txn_from(t);\n\t\tif (t_from) {\n\t\t\tstruct task_struct *sender = t_from->proc->tsk;\n\n\t\t\ttrd->sender_pid =\n\t\t\t\ttask_tgid_nr_ns(sender,\n\t\t\t\t\t\ttask_active_pid_ns(current));\n\t\t} else {\n\t\t\ttrd->sender_pid = 0;\n\t\t}\n\n\t\tret = binder_apply_fd_fixups(proc, t);\n\t\tif (ret) {\n\t\t\tstruct binder_buffer *buffer = t->buffer;\n\t\t\tbool oneway = !!(t->flags & TF_ONE_WAY);\n\t\t\tint tid = t->debug_id;\n\n\t\t\tif (t_from)\n\t\t\t\tbinder_thread_dec_tmpref(t_from);\n\t\t\tbuffer->transaction = NULL;\n\t\t\tbinder_cleanup_transaction(t, \"fd fixups failed\",\n\t\t\t\t\t\t   BR_FAILED_REPLY);\n\t\t\tbinder_free_buf(proc, thread, buffer, true);\n\t\t\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t     \"%d:%d %stransaction %d fd fixups failed %d/%d, line %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     oneway ? \"async \" :\n\t\t\t\t\t(cmd == BR_REPLY ? \"reply \" : \"\"),\n\t\t\t\t     tid, BR_FAILED_REPLY, ret, __LINE__);\n\t\t\tif (cmd == BR_REPLY) {\n\t\t\t\tcmd = BR_FAILED_REPLY;\n\t\t\t\tif (put_user(cmd, (uint32_t __user *)ptr))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\tptr += sizeof(uint32_t);\n\t\t\t\tbinder_stat_br(proc, thread, cmd);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\ttrd->data_size = t->buffer->data_size;\n\t\ttrd->offsets_size = t->buffer->offsets_size;\n\t\ttrd->data.ptr.buffer = (uintptr_t)t->buffer->user_data;\n\t\ttrd->data.ptr.offsets = trd->data.ptr.buffer +\n\t\t\t\t\tALIGN(t->buffer->data_size,\n\t\t\t\t\t    sizeof(void *));\n\n\t\ttr.secctx = t->security_ctx;\n\t\tif (t->security_ctx) {\n\t\t\tcmd = BR_TRANSACTION_SEC_CTX;\n\t\t\ttrsize = sizeof(tr);\n\t\t}\n\t\tif (put_user(cmd, (uint32_t __user *)ptr)) {\n\t\t\tif (t_from)\n\t\t\t\tbinder_thread_dec_tmpref(t_from);\n\n\t\t\tbinder_cleanup_transaction(t, \"put_user failed\",\n\t\t\t\t\t\t   BR_FAILED_REPLY);\n\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tptr += sizeof(uint32_t);\n\t\tif (copy_to_user(ptr, &tr, trsize)) {\n\t\t\tif (t_from)\n\t\t\t\tbinder_thread_dec_tmpref(t_from);\n\n\t\t\tbinder_cleanup_transaction(t, \"copy_to_user failed\",\n\t\t\t\t\t\t   BR_FAILED_REPLY);\n\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tptr += trsize;\n\n\t\ttrace_binder_transaction_received(t);\n\t\tbinder_stat_br(proc, thread, cmd);\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d %s %d %d:%d, cmd %d size %zd-%zd ptr %016llx-%016llx\\n\",\n\t\t\t     proc->pid, thread->pid,\n\t\t\t     (cmd == BR_TRANSACTION) ? \"BR_TRANSACTION\" :\n\t\t\t\t(cmd == BR_TRANSACTION_SEC_CTX) ?\n\t\t\t\t     \"BR_TRANSACTION_SEC_CTX\" : \"BR_REPLY\",\n\t\t\t     t->debug_id, t_from ? t_from->proc->pid : 0,\n\t\t\t     t_from ? t_from->pid : 0, cmd,\n\t\t\t     t->buffer->data_size, t->buffer->offsets_size,\n\t\t\t     (u64)trd->data.ptr.buffer,\n\t\t\t     (u64)trd->data.ptr.offsets);\n\n\t\tif (t_from)\n\t\t\tbinder_thread_dec_tmpref(t_from);\n\t\tt->buffer->allow_user_free = 1;\n\t\tif (cmd != BR_REPLY && !(t->flags & TF_ONE_WAY)) {\n\t\t\tbinder_inner_proc_lock(thread->proc);\n\t\t\tt->to_parent = thread->transaction_stack;\n\t\t\tt->to_thread = thread;\n\t\t\tthread->transaction_stack = t;\n\t\t\tbinder_inner_proc_unlock(thread->proc);\n\t\t} else {\n\t\t\tbinder_free_transaction(t);\n\t\t}\n\t\tbreak;\n\t}\n\ndone:\n\n\t*consumed = ptr - buffer;\n\tbinder_inner_proc_lock(proc);\n\tif (proc->requested_threads == 0 &&\n\t    list_empty(&thread->proc->waiting_threads) &&\n\t    proc->requested_threads_started < proc->max_threads &&\n\t    (thread->looper & (BINDER_LOOPER_STATE_REGISTERED |\n\t     BINDER_LOOPER_STATE_ENTERED)) /* the user-space code fails to */\n\t     /*spawn a new thread if we leave this out */) {\n\t\tproc->requested_threads++;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t     \"%d:%d BR_SPAWN_LOOPER\\n\",\n\t\t\t     proc->pid, thread->pid);\n\t\tif (put_user(BR_SPAWN_LOOPER, (uint32_t __user *)buffer))\n\t\t\treturn -EFAULT;\n\t\tbinder_stat_br(proc, thread, BR_SPAWN_LOOPER);\n\t} else\n\t\tbinder_inner_proc_unlock(proc);\n\treturn 0;\n}\n\nstatic void binder_release_work(struct binder_proc *proc,\n\t\t\t\tstruct list_head *list)\n{\n\tstruct binder_work *w;\n\tenum binder_work_type wtype;\n\n\twhile (1) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tw = binder_dequeue_work_head_ilocked(list);\n\t\twtype = w ? w->type : 0;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!w)\n\t\t\treturn;\n\n\t\tswitch (wtype) {\n\t\tcase BINDER_WORK_TRANSACTION: {\n\t\t\tstruct binder_transaction *t;\n\n\t\t\tt = container_of(w, struct binder_transaction, work);\n\n\t\t\tbinder_cleanup_transaction(t, \"process died.\",\n\t\t\t\t\t\t   BR_DEAD_REPLY);\n\t\t} break;\n\t\tcase BINDER_WORK_RETURN_ERROR: {\n\t\t\tstruct binder_error *e = container_of(\n\t\t\t\t\tw, struct binder_error, work);\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered TRANSACTION_ERROR: %u\\n\",\n\t\t\t\te->cmd);\n\t\t} break;\n\t\tcase BINDER_WORK_TRANSACTION_COMPLETE: {\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered TRANSACTION_COMPLETE\\n\");\n\t\t\tkfree(w);\n\t\t\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\n\t\t} break;\n\t\tcase BINDER_WORK_DEAD_BINDER_AND_CLEAR:\n\t\tcase BINDER_WORK_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tstruct binder_ref_death *death;\n\n\t\t\tdeath = container_of(w, struct binder_ref_death, work);\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered death notification, %016llx\\n\",\n\t\t\t\t(u64)death->cookie);\n\t\t\tkfree(death);\n\t\t\tbinder_stats_deleted(BINDER_STAT_DEATH);\n\t\t} break;\n\t\tcase BINDER_WORK_NODE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"unexpected work type, %d, not freed\\n\",\n\t\t\t       wtype);\n\t\t\tbreak;\n\t\t}\n\t}\n\n}\n\nstatic struct binder_thread *binder_get_thread_ilocked(\n\t\tstruct binder_proc *proc, struct binder_thread *new_thread)\n{\n\tstruct binder_thread *thread = NULL;\n\tstruct rb_node *parent = NULL;\n\tstruct rb_node **p = &proc->threads.rb_node;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tthread = rb_entry(parent, struct binder_thread, rb_node);\n\n\t\tif (current->pid < thread->pid)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (current->pid > thread->pid)\n\t\t\tp = &(*p)->rb_right;\n\t\telse\n\t\t\treturn thread;\n\t}\n\tif (!new_thread)\n\t\treturn NULL;\n\tthread = new_thread;\n\tbinder_stats_created(BINDER_STAT_THREAD);\n\tthread->proc = proc;\n\tthread->pid = current->pid;\n\tatomic_set(&thread->tmp_ref, 0);\n\tinit_waitqueue_head(&thread->wait);\n\tINIT_LIST_HEAD(&thread->todo);\n\trb_link_node(&thread->rb_node, parent, p);\n\trb_insert_color(&thread->rb_node, &proc->threads);\n\tthread->looper_need_return = true;\n\tthread->return_error.work.type = BINDER_WORK_RETURN_ERROR;\n\tthread->return_error.cmd = BR_OK;\n\tthread->reply_error.work.type = BINDER_WORK_RETURN_ERROR;\n\tthread->reply_error.cmd = BR_OK;\n\tINIT_LIST_HEAD(&new_thread->waiting_thread_node);\n\treturn thread;\n}\n\nstatic struct binder_thread *binder_get_thread(struct binder_proc *proc)\n{\n\tstruct binder_thread *thread;\n\tstruct binder_thread *new_thread;\n\n\tbinder_inner_proc_lock(proc);\n\tthread = binder_get_thread_ilocked(proc, NULL);\n\tbinder_inner_proc_unlock(proc);\n\tif (!thread) {\n\t\tnew_thread = kzalloc(sizeof(*thread), GFP_KERNEL);\n\t\tif (new_thread == NULL)\n\t\t\treturn NULL;\n\t\tbinder_inner_proc_lock(proc);\n\t\tthread = binder_get_thread_ilocked(proc, new_thread);\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (thread != new_thread)\n\t\t\tkfree(new_thread);\n\t}\n\treturn thread;\n}\n\nstatic void binder_free_proc(struct binder_proc *proc)\n{\n\tstruct binder_device *device;\n\n\tBUG_ON(!list_empty(&proc->todo));\n\tBUG_ON(!list_empty(&proc->delivered_death));\n\tdevice = container_of(proc->context, struct binder_device, context);\n\tif (refcount_dec_and_test(&device->ref)) {\n\t\tkfree(proc->context->name);\n\t\tkfree(device);\n\t}\n\tbinder_alloc_deferred_release(&proc->alloc);\n\tput_task_struct(proc->tsk);\n\tput_cred(proc->cred);\n\tbinder_stats_deleted(BINDER_STAT_PROC);\n\tkfree(proc);\n}\n\nstatic void binder_free_thread(struct binder_thread *thread)\n{\n\tBUG_ON(!list_empty(&thread->todo));\n\tbinder_stats_deleted(BINDER_STAT_THREAD);\n\tbinder_proc_dec_tmpref(thread->proc);\n\tkfree(thread);\n}\n\nstatic int binder_thread_release(struct binder_proc *proc,\n\t\t\t\t struct binder_thread *thread)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_transaction *send_reply = NULL;\n\tint active_transactions = 0;\n\tstruct binder_transaction *last_t = NULL;\n\n\tbinder_inner_proc_lock(thread->proc);\n\t/*\n\t * take a ref on the proc so it survives\n\t * after we remove this thread from proc->threads.\n\t * The corresponding dec is when we actually\n\t * free the thread in binder_free_thread()\n\t */\n\tproc->tmp_ref++;\n\t/*\n\t * take a ref on this thread to ensure it\n\t * survives while we are releasing it\n\t */\n\tatomic_inc(&thread->tmp_ref);\n\trb_erase(&thread->rb_node, &proc->threads);\n\tt = thread->transaction_stack;\n\tif (t) {\n\t\tspin_lock(&t->lock);\n\t\tif (t->to_thread == thread)\n\t\t\tsend_reply = t;\n\t} else {\n\t\t__acquire(&t->lock);\n\t}\n\tthread->is_dead = true;\n\n\twhile (t) {\n\t\tlast_t = t;\n\t\tactive_transactions++;\n\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t     \"release %d:%d transaction %d %s, still active\\n\",\n\t\t\t      proc->pid, thread->pid,\n\t\t\t     t->debug_id,\n\t\t\t     (t->to_thread == thread) ? \"in\" : \"out\");\n\n\t\tif (t->to_thread == thread) {\n\t\t\tt->to_proc = NULL;\n\t\t\tt->to_thread = NULL;\n\t\t\tif (t->buffer) {\n\t\t\t\tt->buffer->transaction = NULL;\n\t\t\t\tt->buffer = NULL;\n\t\t\t}\n\t\t\tt = t->to_parent;\n\t\t} else if (t->from == thread) {\n\t\t\tt->from = NULL;\n\t\t\tt = t->from_parent;\n\t\t} else\n\t\t\tBUG();\n\t\tspin_unlock(&last_t->lock);\n\t\tif (t)\n\t\t\tspin_lock(&t->lock);\n\t\telse\n\t\t\t__acquire(&t->lock);\n\t}\n\t/* annotation for sparse, lock not acquired in last iteration above */\n\t__release(&t->lock);\n\n\t/*\n\t * If this thread used poll, make sure we remove the waitqueue\n\t * from any epoll data structures holding it with POLLFREE.\n\t * waitqueue_active() is safe to use here because we're holding\n\t * the inner lock.\n\t */\n\tif ((thread->looper & BINDER_LOOPER_STATE_POLL) &&\n\t    waitqueue_active(&thread->wait)) {\n\t\twake_up_poll(&thread->wait, EPOLLHUP | POLLFREE);\n\t}\n\n\tbinder_inner_proc_unlock(thread->proc);\n\n\t/*\n\t * This is needed to avoid races between wake_up_poll() above and\n\t * and ep_remove_waitqueue() called for other reasons (eg the epoll file\n\t * descriptor being closed); ep_remove_waitqueue() holds an RCU read\n\t * lock, so we can be sure it's done after calling synchronize_rcu().\n\t */\n\tif (thread->looper & BINDER_LOOPER_STATE_POLL)\n\t\tsynchronize_rcu();\n\n\tif (send_reply)\n\t\tbinder_send_failed_reply(send_reply, BR_DEAD_REPLY);\n\tbinder_release_work(proc, &thread->todo);\n\tbinder_thread_dec_tmpref(thread);\n\treturn active_transactions;\n}\n\nstatic __poll_t binder_poll(struct file *filp,\n\t\t\t\tstruct poll_table_struct *wait)\n{\n\tstruct binder_proc *proc = filp->private_data;\n\tstruct binder_thread *thread = NULL;\n\tbool wait_for_proc_work;\n\n\tthread = binder_get_thread(proc);\n\tif (!thread)\n\t\treturn POLLERR;\n\n\tbinder_inner_proc_lock(thread->proc);\n\tthread->looper |= BINDER_LOOPER_STATE_POLL;\n\twait_for_proc_work = binder_available_for_proc_work_ilocked(thread);\n\n\tbinder_inner_proc_unlock(thread->proc);\n\n\tpoll_wait(filp, &thread->wait, wait);\n\n\tif (binder_has_work(thread, wait_for_proc_work))\n\t\treturn EPOLLIN;\n\n\treturn 0;\n}\n\nstatic int binder_ioctl_write_read(struct file *filp,\n\t\t\t\tunsigned int cmd, unsigned long arg,\n\t\t\t\tstruct binder_thread *thread)\n{\n\tint ret = 0;\n\tstruct binder_proc *proc = filp->private_data;\n\tunsigned int size = _IOC_SIZE(cmd);\n\tvoid __user *ubuf = (void __user *)arg;\n\tstruct binder_write_read bwr;\n\n\tif (size != sizeof(struct binder_write_read)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (copy_from_user(&bwr, ubuf, sizeof(bwr))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\tbinder_debug(BINDER_DEBUG_READ_WRITE,\n\t\t     \"%d:%d write %lld at %016llx, read %lld at %016llx\\n\",\n\t\t     proc->pid, thread->pid,\n\t\t     (u64)bwr.write_size, (u64)bwr.write_buffer,\n\t\t     (u64)bwr.read_size, (u64)bwr.read_buffer);\n\n\tif (bwr.write_size > 0) {\n\t\tret = binder_thread_write(proc, thread,\n\t\t\t\t\t  bwr.write_buffer,\n\t\t\t\t\t  bwr.write_size,\n\t\t\t\t\t  &bwr.write_consumed);\n\t\ttrace_binder_write_done(ret);\n\t\tif (ret < 0) {\n\t\t\tbwr.read_consumed = 0;\n\t\t\tif (copy_to_user(ubuf, &bwr, sizeof(bwr)))\n\t\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif (bwr.read_size > 0) {\n\t\tret = binder_thread_read(proc, thread, bwr.read_buffer,\n\t\t\t\t\t bwr.read_size,\n\t\t\t\t\t &bwr.read_consumed,\n\t\t\t\t\t filp->f_flags & O_NONBLOCK);\n\t\ttrace_binder_read_done(ret);\n\t\tbinder_inner_proc_lock(proc);\n\t\tif (!binder_worklist_empty_ilocked(&proc->todo))\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (ret < 0) {\n\t\t\tif (copy_to_user(ubuf, &bwr, sizeof(bwr)))\n\t\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tbinder_debug(BINDER_DEBUG_READ_WRITE,\n\t\t     \"%d:%d wrote %lld of %lld, read return %lld of %lld\\n\",\n\t\t     proc->pid, thread->pid,\n\t\t     (u64)bwr.write_consumed, (u64)bwr.write_size,\n\t\t     (u64)bwr.read_consumed, (u64)bwr.read_size);\n\tif (copy_to_user(ubuf, &bwr, sizeof(bwr))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int binder_ioctl_set_ctx_mgr(struct file *filp,\n\t\t\t\t    struct flat_binder_object *fbo)\n{\n\tint ret = 0;\n\tstruct binder_proc *proc = filp->private_data;\n\tstruct binder_context *context = proc->context;\n\tstruct binder_node *new_node;\n\tkuid_t curr_euid = current_euid();\n\n\tmutex_lock(&context->context_mgr_node_lock);\n\tif (context->binder_context_mgr_node) {\n\t\tpr_err(\"BINDER_SET_CONTEXT_MGR already set\\n\");\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\tret = security_binder_set_context_mgr(proc->cred);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (uid_valid(context->binder_context_mgr_uid)) {\n\t\tif (!uid_eq(context->binder_context_mgr_uid, curr_euid)) {\n\t\t\tpr_err(\"BINDER_SET_CONTEXT_MGR bad uid %d != %d\\n\",\n\t\t\t       from_kuid(&init_user_ns, curr_euid),\n\t\t\t       from_kuid(&init_user_ns,\n\t\t\t\t\t context->binder_context_mgr_uid));\n\t\t\tret = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tcontext->binder_context_mgr_uid = curr_euid;\n\t}\n\tnew_node = binder_new_node(proc, fbo);\n\tif (!new_node) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tbinder_node_lock(new_node);\n\tnew_node->local_weak_refs++;\n\tnew_node->local_strong_refs++;\n\tnew_node->has_strong_ref = 1;\n\tnew_node->has_weak_ref = 1;\n\tcontext->binder_context_mgr_node = new_node;\n\tbinder_node_unlock(new_node);\n\tbinder_put_node(new_node);\nout:\n\tmutex_unlock(&context->context_mgr_node_lock);\n\treturn ret;\n}\n\nstatic int binder_ioctl_get_node_info_for_ref(struct binder_proc *proc,\n\t\tstruct binder_node_info_for_ref *info)\n{\n\tstruct binder_node *node;\n\tstruct binder_context *context = proc->context;\n\t__u32 handle = info->handle;\n\n\tif (info->strong_count || info->weak_count || info->reserved1 ||\n\t    info->reserved2 || info->reserved3) {\n\t\tbinder_user_error(\"%d BINDER_GET_NODE_INFO_FOR_REF: only handle may be non-zero.\",\n\t\t\t\t  proc->pid);\n\t\treturn -EINVAL;\n\t}\n\n\t/* This ioctl may only be used by the context manager */\n\tmutex_lock(&context->context_mgr_node_lock);\n\tif (!context->binder_context_mgr_node ||\n\t\tcontext->binder_context_mgr_node->proc != proc) {\n\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\treturn -EPERM;\n\t}\n\tmutex_unlock(&context->context_mgr_node_lock);\n\n\tnode = binder_get_node_from_ref(proc, handle, true, NULL);\n\tif (!node)\n\t\treturn -EINVAL;\n\n\tinfo->strong_count = node->local_strong_refs +\n\t\tnode->internal_strong_refs;\n\tinfo->weak_count = node->local_weak_refs;\n\n\tbinder_put_node(node);\n\n\treturn 0;\n}\n\nstatic int binder_ioctl_get_node_debug_info(struct binder_proc *proc,\n\t\t\t\tstruct binder_node_debug_info *info)\n{\n\tstruct rb_node *n;\n\tbinder_uintptr_t ptr = info->ptr;\n\n\tmemset(info, 0, sizeof(*info));\n\n\tbinder_inner_proc_lock(proc);\n\tfor (n = rb_first(&proc->nodes); n != NULL; n = rb_next(n)) {\n\t\tstruct binder_node *node = rb_entry(n, struct binder_node,\n\t\t\t\t\t\t    rb_node);\n\t\tif (node->ptr > ptr) {\n\t\t\tinfo->ptr = node->ptr;\n\t\t\tinfo->cookie = node->cookie;\n\t\t\tinfo->has_strong_ref = node->has_strong_ref;\n\t\t\tinfo->has_weak_ref = node->has_weak_ref;\n\t\t\tbreak;\n\t\t}\n\t}\n\tbinder_inner_proc_unlock(proc);\n\n\treturn 0;\n}\n\nstatic long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tint ret;\n\tstruct binder_proc *proc = filp->private_data;\n\tstruct binder_thread *thread;\n\tunsigned int size = _IOC_SIZE(cmd);\n\tvoid __user *ubuf = (void __user *)arg;\n\n\t/*pr_info(\"binder_ioctl: %d:%d %x %lx\\n\",\n\t\t\tproc->pid, current->pid, cmd, arg);*/\n\n\tbinder_selftest_alloc(&proc->alloc);\n\n\ttrace_binder_ioctl(cmd, arg);\n\n\tret = wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error < 2);\n\tif (ret)\n\t\tgoto err_unlocked;\n\n\tthread = binder_get_thread(proc);\n\tif (thread == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tswitch (cmd) {\n\tcase BINDER_WRITE_READ:\n\t\tret = binder_ioctl_write_read(filp, cmd, arg, thread);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tbreak;\n\tcase BINDER_SET_MAX_THREADS: {\n\t\tint max_threads;\n\n\t\tif (copy_from_user(&max_threads, ubuf,\n\t\t\t\t   sizeof(max_threads))) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\t\tproc->max_threads = max_threads;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbreak;\n\t}\n\tcase BINDER_SET_CONTEXT_MGR_EXT: {\n\t\tstruct flat_binder_object fbo;\n\n\t\tif (copy_from_user(&fbo, ubuf, sizeof(fbo))) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tret = binder_ioctl_set_ctx_mgr(filp, &fbo);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tbreak;\n\t}\n\tcase BINDER_SET_CONTEXT_MGR:\n\t\tret = binder_ioctl_set_ctx_mgr(filp, NULL);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tbreak;\n\tcase BINDER_THREAD_EXIT:\n\t\tbinder_debug(BINDER_DEBUG_THREADS, \"%d:%d exit\\n\",\n\t\t\t     proc->pid, thread->pid);\n\t\tbinder_thread_release(proc, thread);\n\t\tthread = NULL;\n\t\tbreak;\n\tcase BINDER_VERSION: {\n\t\tstruct binder_version __user *ver = ubuf;\n\n\t\tif (size != sizeof(struct binder_version)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tif (put_user(BINDER_CURRENT_PROTOCOL_VERSION,\n\t\t\t     &ver->protocol_version)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tbreak;\n\t}\n\tcase BINDER_GET_NODE_INFO_FOR_REF: {\n\t\tstruct binder_node_info_for_ref info;\n\n\t\tif (copy_from_user(&info, ubuf, sizeof(info))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tret = binder_ioctl_get_node_info_for_ref(proc, &info);\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\n\t\tif (copy_to_user(ubuf, &info, sizeof(info))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tbreak;\n\t}\n\tcase BINDER_GET_NODE_DEBUG_INFO: {\n\t\tstruct binder_node_debug_info info;\n\n\t\tif (copy_from_user(&info, ubuf, sizeof(info))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tret = binder_ioctl_get_node_debug_info(proc, &info);\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\n\t\tif (copy_to_user(ubuf, &info, sizeof(info))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err;\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\tret = 0;\nerr:\n\tif (thread)\n\t\tthread->looper_need_return = false;\n\twait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error < 2);\n\tif (ret && ret != -ERESTARTSYS)\n\t\tpr_info(\"%d:%d ioctl %x %lx returned %d\\n\", proc->pid, current->pid, cmd, arg, ret);\nerr_unlocked:\n\ttrace_binder_ioctl_done(ret);\n\treturn ret;\n}\n\nstatic void binder_vma_open(struct vm_area_struct *vma)\n{\n\tstruct binder_proc *proc = vma->vm_private_data;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%d open vm area %lx-%lx (%ld K) vma %lx pagep %lx\\n\",\n\t\t     proc->pid, vma->vm_start, vma->vm_end,\n\t\t     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,\n\t\t     (unsigned long)pgprot_val(vma->vm_page_prot));\n}\n\nstatic void binder_vma_close(struct vm_area_struct *vma)\n{\n\tstruct binder_proc *proc = vma->vm_private_data;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%d close vm area %lx-%lx (%ld K) vma %lx pagep %lx\\n\",\n\t\t     proc->pid, vma->vm_start, vma->vm_end,\n\t\t     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,\n\t\t     (unsigned long)pgprot_val(vma->vm_page_prot));\n\tbinder_alloc_vma_close(&proc->alloc);\n}\n\nstatic vm_fault_t binder_vm_fault(struct vm_fault *vmf)\n{\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic const struct vm_operations_struct binder_vm_ops = {\n\t.open = binder_vma_open,\n\t.close = binder_vma_close,\n\t.fault = binder_vm_fault,\n};\n\nstatic int binder_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tint ret;\n\tstruct binder_proc *proc = filp->private_data;\n\tconst char *failure_string;\n\n\tif (proc->tsk != current->group_leader)\n\t\treturn -EINVAL;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d %lx-%lx (%ld K) vma %lx pagep %lx\\n\",\n\t\t     __func__, proc->pid, vma->vm_start, vma->vm_end,\n\t\t     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,\n\t\t     (unsigned long)pgprot_val(vma->vm_page_prot));\n\n\tif (vma->vm_flags & FORBIDDEN_MMAP_FLAGS) {\n\t\tret = -EPERM;\n\t\tfailure_string = \"bad vm_flags\";\n\t\tgoto err_bad_arg;\n\t}\n\tvma->vm_flags |= VM_DONTCOPY | VM_MIXEDMAP;\n\tvma->vm_flags &= ~VM_MAYWRITE;\n\n\tvma->vm_ops = &binder_vm_ops;\n\tvma->vm_private_data = proc;\n\n\tret = binder_alloc_mmap_handler(&proc->alloc, vma);\n\tif (ret)\n\t\treturn ret;\n\treturn 0;\n\nerr_bad_arg:\n\tpr_err(\"%s: %d %lx-%lx %s failed %d\\n\", __func__,\n\t       proc->pid, vma->vm_start, vma->vm_end, failure_string, ret);\n\treturn ret;\n}\n\nstatic int binder_open(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc, *itr;\n\tstruct binder_device *binder_dev;\n\tstruct binderfs_info *info;\n\tstruct dentry *binder_binderfs_dir_entry_proc = NULL;\n\tbool existing_pid = false;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE, \"%s: %d:%d\\n\", __func__,\n\t\t     current->group_leader->pid, current->pid);\n\n\tproc = kzalloc(sizeof(*proc), GFP_KERNEL);\n\tif (proc == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&proc->inner_lock);\n\tspin_lock_init(&proc->outer_lock);\n\tget_task_struct(current->group_leader);\n\tproc->tsk = current->group_leader;\n\tproc->cred = get_cred(filp->f_cred);\n\tINIT_LIST_HEAD(&proc->todo);\n\tproc->default_priority = task_nice(current);\n\t/* binderfs stashes devices in i_private */\n\tif (is_binderfs_device(nodp)) {\n\t\tbinder_dev = nodp->i_private;\n\t\tinfo = nodp->i_sb->s_fs_info;\n\t\tbinder_binderfs_dir_entry_proc = info->proc_log_dir;\n\t} else {\n\t\tbinder_dev = container_of(filp->private_data,\n\t\t\t\t\t  struct binder_device, miscdev);\n\t}\n\trefcount_inc(&binder_dev->ref);\n\tproc->context = &binder_dev->context;\n\tbinder_alloc_init(&proc->alloc);\n\n\tbinder_stats_created(BINDER_STAT_PROC);\n\tproc->pid = current->group_leader->pid;\n\tINIT_LIST_HEAD(&proc->delivered_death);\n\tINIT_LIST_HEAD(&proc->waiting_threads);\n\tfilp->private_data = proc;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(itr, &binder_procs, proc_node) {\n\t\tif (itr->pid == proc->pid) {\n\t\t\texisting_pid = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\thlist_add_head(&proc->proc_node, &binder_procs);\n\tmutex_unlock(&binder_procs_lock);\n\n\tif (binder_debugfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * proc debug entries are shared between contexts.\n\t\t * Only create for the first PID to avoid debugfs log spamming\n\t\t * The printing code will anyway print all contexts for a given\n\t\t * PID so this is not a problem.\n\t\t */\n\t\tproc->debugfs_entry = debugfs_create_file(strbuf, 0444,\n\t\t\tbinder_debugfs_dir_entry_proc,\n\t\t\t(void *)(unsigned long)proc->pid,\n\t\t\t&proc_fops);\n\t}\n\n\tif (binder_binderfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\t\tstruct dentry *binderfs_entry;\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * Similar to debugfs, the process specific log file is shared\n\t\t * between contexts. Only create for the first PID.\n\t\t * This is ok since same as debugfs, the log file will contain\n\t\t * information on all contexts of a given PID.\n\t\t */\n\t\tbinderfs_entry = binderfs_create_file(binder_binderfs_dir_entry_proc,\n\t\t\tstrbuf, &proc_fops, (void *)(unsigned long)proc->pid);\n\t\tif (!IS_ERR(binderfs_entry)) {\n\t\t\tproc->binderfs_entry = binderfs_entry;\n\t\t} else {\n\t\t\tint error;\n\n\t\t\terror = PTR_ERR(binderfs_entry);\n\t\t\tpr_warn(\"Unable to create file %s in binderfs (error %d)\\n\",\n\t\t\t\tstrbuf, error);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int binder_flush(struct file *filp, fl_owner_t id)\n{\n\tstruct binder_proc *proc = filp->private_data;\n\n\tbinder_defer_work(proc, BINDER_DEFERRED_FLUSH);\n\n\treturn 0;\n}\n\nstatic void binder_deferred_flush(struct binder_proc *proc)\n{\n\tstruct rb_node *n;\n\tint wake_count = 0;\n\n\tbinder_inner_proc_lock(proc);\n\tfor (n = rb_first(&proc->threads); n != NULL; n = rb_next(n)) {\n\t\tstruct binder_thread *thread = rb_entry(n, struct binder_thread, rb_node);\n\n\t\tthread->looper_need_return = true;\n\t\tif (thread->looper & BINDER_LOOPER_STATE_WAITING) {\n\t\t\twake_up_interruptible(&thread->wait);\n\t\t\twake_count++;\n\t\t}\n\t}\n\tbinder_inner_proc_unlock(proc);\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"binder_flush: %d woke %d threads\\n\", proc->pid,\n\t\t     wake_count);\n}\n\nstatic int binder_release(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc = filp->private_data;\n\n\tdebugfs_remove(proc->debugfs_entry);\n\n\tif (proc->binderfs_entry) {\n\t\tbinderfs_remove_file(proc->binderfs_entry);\n\t\tproc->binderfs_entry = NULL;\n\t}\n\n\tbinder_defer_work(proc, BINDER_DEFERRED_RELEASE);\n\n\treturn 0;\n}\n\nstatic int binder_node_release(struct binder_node *node, int refs)\n{\n\tstruct binder_ref *ref;\n\tint death = 0;\n\tstruct binder_proc *proc = node->proc;\n\n\tbinder_release_work(proc, &node->async_todo);\n\n\tbinder_node_lock(node);\n\tbinder_inner_proc_lock(proc);\n\tbinder_dequeue_work_ilocked(&node->work);\n\t/*\n\t * The caller must have taken a temporary ref on the node,\n\t */\n\tBUG_ON(!node->tmp_refs);\n\tif (hlist_empty(&node->refs) && node->tmp_refs == 1) {\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_node_unlock(node);\n\t\tbinder_free_node(node);\n\n\t\treturn refs;\n\t}\n\n\tnode->proc = NULL;\n\tnode->local_strong_refs = 0;\n\tnode->local_weak_refs = 0;\n\tbinder_inner_proc_unlock(proc);\n\n\tspin_lock(&binder_dead_nodes_lock);\n\thlist_add_head(&node->dead_node, &binder_dead_nodes);\n\tspin_unlock(&binder_dead_nodes_lock);\n\n\thlist_for_each_entry(ref, &node->refs, node_entry) {\n\t\trefs++;\n\t\t/*\n\t\t * Need the node lock to synchronize\n\t\t * with new notification requests and the\n\t\t * inner lock to synchronize with queued\n\t\t * death notifications.\n\t\t */\n\t\tbinder_inner_proc_lock(ref->proc);\n\t\tif (!ref->death) {\n\t\t\tbinder_inner_proc_unlock(ref->proc);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdeath++;\n\n\t\tBUG_ON(!list_empty(&ref->death->work.entry));\n\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\t\tbinder_enqueue_work_ilocked(&ref->death->work,\n\t\t\t\t\t    &ref->proc->todo);\n\t\tbinder_wakeup_proc_ilocked(ref->proc);\n\t\tbinder_inner_proc_unlock(ref->proc);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t     \"node %d now dead, refs %d, death %d\\n\",\n\t\t     node->debug_id, refs, death);\n\tbinder_node_unlock(node);\n\tbinder_put_node(node);\n\n\treturn refs;\n}\n\nstatic void binder_deferred_release(struct binder_proc *proc)\n{\n\tstruct binder_context *context = proc->context;\n\tstruct rb_node *n;\n\tint threads, nodes, incoming_refs, outgoing_refs, active_transactions;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_del(&proc->proc_node);\n\tmutex_unlock(&binder_procs_lock);\n\n\tmutex_lock(&context->context_mgr_node_lock);\n\tif (context->binder_context_mgr_node &&\n\t    context->binder_context_mgr_node->proc == proc) {\n\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t     \"%s: %d context_mgr_node gone\\n\",\n\t\t\t     __func__, proc->pid);\n\t\tcontext->binder_context_mgr_node = NULL;\n\t}\n\tmutex_unlock(&context->context_mgr_node_lock);\n\tbinder_inner_proc_lock(proc);\n\t/*\n\t * Make sure proc stays alive after we\n\t * remove all the threads\n\t */\n\tproc->tmp_ref++;\n\n\tproc->is_dead = true;\n\tthreads = 0;\n\tactive_transactions = 0;\n\twhile ((n = rb_first(&proc->threads))) {\n\t\tstruct binder_thread *thread;\n\n\t\tthread = rb_entry(n, struct binder_thread, rb_node);\n\t\tbinder_inner_proc_unlock(proc);\n\t\tthreads++;\n\t\tactive_transactions += binder_thread_release(proc, thread);\n\t\tbinder_inner_proc_lock(proc);\n\t}\n\n\tnodes = 0;\n\tincoming_refs = 0;\n\twhile ((n = rb_first(&proc->nodes))) {\n\t\tstruct binder_node *node;\n\n\t\tnode = rb_entry(n, struct binder_node, rb_node);\n\t\tnodes++;\n\t\t/*\n\t\t * take a temporary ref on the node before\n\t\t * calling binder_node_release() which will either\n\t\t * kfree() the node or call binder_put_node()\n\t\t */\n\t\tbinder_inc_node_tmpref_ilocked(node);\n\t\trb_erase(&node->rb_node, &proc->nodes);\n\t\tbinder_inner_proc_unlock(proc);\n\t\tincoming_refs = binder_node_release(node, incoming_refs);\n\t\tbinder_inner_proc_lock(proc);\n\t}\n\tbinder_inner_proc_unlock(proc);\n\n\toutgoing_refs = 0;\n\tbinder_proc_lock(proc);\n\twhile ((n = rb_first(&proc->refs_by_desc))) {\n\t\tstruct binder_ref *ref;\n\n\t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n\t\toutgoing_refs++;\n\t\tbinder_cleanup_ref_olocked(ref);\n\t\tbinder_proc_unlock(proc);\n\t\tbinder_free_ref(ref);\n\t\tbinder_proc_lock(proc);\n\t}\n\tbinder_proc_unlock(proc);\n\n\tbinder_release_work(proc, &proc->todo);\n\tbinder_release_work(proc, &proc->delivered_death);\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d threads %d, nodes %d (ref %d), refs %d, active transactions %d\\n\",\n\t\t     __func__, proc->pid, threads, nodes, incoming_refs,\n\t\t     outgoing_refs, active_transactions);\n\n\tbinder_proc_dec_tmpref(proc);\n}\n\nstatic void binder_deferred_func(struct work_struct *work)\n{\n\tstruct binder_proc *proc;\n\n\tint defer;\n\n\tdo {\n\t\tmutex_lock(&binder_deferred_lock);\n\t\tif (!hlist_empty(&binder_deferred_list)) {\n\t\t\tproc = hlist_entry(binder_deferred_list.first,\n\t\t\t\t\tstruct binder_proc, deferred_work_node);\n\t\t\thlist_del_init(&proc->deferred_work_node);\n\t\t\tdefer = proc->deferred_work;\n\t\t\tproc->deferred_work = 0;\n\t\t} else {\n\t\t\tproc = NULL;\n\t\t\tdefer = 0;\n\t\t}\n\t\tmutex_unlock(&binder_deferred_lock);\n\n\t\tif (defer & BINDER_DEFERRED_FLUSH)\n\t\t\tbinder_deferred_flush(proc);\n\n\t\tif (defer & BINDER_DEFERRED_RELEASE)\n\t\t\tbinder_deferred_release(proc); /* frees proc */\n\t} while (proc);\n}\nstatic DECLARE_WORK(binder_deferred_work, binder_deferred_func);\n\nstatic void\nbinder_defer_work(struct binder_proc *proc, enum binder_deferred_state defer)\n{\n\tmutex_lock(&binder_deferred_lock);\n\tproc->deferred_work |= defer;\n\tif (hlist_unhashed(&proc->deferred_work_node)) {\n\t\thlist_add_head(&proc->deferred_work_node,\n\t\t\t\t&binder_deferred_list);\n\t\tschedule_work(&binder_deferred_work);\n\t}\n\tmutex_unlock(&binder_deferred_lock);\n}\n\nstatic void print_binder_transaction_ilocked(struct seq_file *m,\n\t\t\t\t\t     struct binder_proc *proc,\n\t\t\t\t\t     const char *prefix,\n\t\t\t\t\t     struct binder_transaction *t)\n{\n\tstruct binder_proc *to_proc;\n\tstruct binder_buffer *buffer = t->buffer;\n\n\tspin_lock(&t->lock);\n\tto_proc = t->to_proc;\n\tseq_printf(m,\n\t\t   \"%s %d: %pK from %d:%d to %d:%d code %x flags %x pri %ld r%d\",\n\t\t   prefix, t->debug_id, t,\n\t\t   t->from ? t->from->proc->pid : 0,\n\t\t   t->from ? t->from->pid : 0,\n\t\t   to_proc ? to_proc->pid : 0,\n\t\t   t->to_thread ? t->to_thread->pid : 0,\n\t\t   t->code, t->flags, t->priority, t->need_reply);\n\tspin_unlock(&t->lock);\n\n\tif (proc != to_proc) {\n\t\t/*\n\t\t * Can only safely deref buffer if we are holding the\n\t\t * correct proc inner lock for this node\n\t\t */\n\t\tseq_puts(m, \"\\n\");\n\t\treturn;\n\t}\n\n\tif (buffer == NULL) {\n\t\tseq_puts(m, \" buffer free\\n\");\n\t\treturn;\n\t}\n\tif (buffer->target_node)\n\t\tseq_printf(m, \" node %d\", buffer->target_node->debug_id);\n\tseq_printf(m, \" size %zd:%zd data %pK\\n\",\n\t\t   buffer->data_size, buffer->offsets_size,\n\t\t   buffer->user_data);\n}\n\nstatic void print_binder_work_ilocked(struct seq_file *m,\n\t\t\t\t     struct binder_proc *proc,\n\t\t\t\t     const char *prefix,\n\t\t\t\t     const char *transaction_prefix,\n\t\t\t\t     struct binder_work *w)\n{\n\tstruct binder_node *node;\n\tstruct binder_transaction *t;\n\n\tswitch (w->type) {\n\tcase BINDER_WORK_TRANSACTION:\n\t\tt = container_of(w, struct binder_transaction, work);\n\t\tprint_binder_transaction_ilocked(\n\t\t\t\tm, proc, transaction_prefix, t);\n\t\tbreak;\n\tcase BINDER_WORK_RETURN_ERROR: {\n\t\tstruct binder_error *e = container_of(\n\t\t\t\tw, struct binder_error, work);\n\n\t\tseq_printf(m, \"%stransaction error: %u\\n\",\n\t\t\t   prefix, e->cmd);\n\t} break;\n\tcase BINDER_WORK_TRANSACTION_COMPLETE:\n\t\tseq_printf(m, \"%stransaction complete\\n\", prefix);\n\t\tbreak;\n\tcase BINDER_WORK_NODE:\n\t\tnode = container_of(w, struct binder_node, work);\n\t\tseq_printf(m, \"%snode work %d: u%016llx c%016llx\\n\",\n\t\t\t   prefix, node->debug_id,\n\t\t\t   (u64)node->ptr, (u64)node->cookie);\n\t\tbreak;\n\tcase BINDER_WORK_DEAD_BINDER:\n\t\tseq_printf(m, \"%shas dead binder\\n\", prefix);\n\t\tbreak;\n\tcase BINDER_WORK_DEAD_BINDER_AND_CLEAR:\n\t\tseq_printf(m, \"%shas cleared dead binder\\n\", prefix);\n\t\tbreak;\n\tcase BINDER_WORK_CLEAR_DEATH_NOTIFICATION:\n\t\tseq_printf(m, \"%shas cleared death notification\\n\", prefix);\n\t\tbreak;\n\tdefault:\n\t\tseq_printf(m, \"%sunknown work: type %d\\n\", prefix, w->type);\n\t\tbreak;\n\t}\n}\n\nstatic void print_binder_thread_ilocked(struct seq_file *m,\n\t\t\t\t\tstruct binder_thread *thread,\n\t\t\t\t\tint print_always)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tsize_t start_pos = m->count;\n\tsize_t header_pos;\n\n\tseq_printf(m, \"  thread %d: l %02x need_return %d tr %d\\n\",\n\t\t\tthread->pid, thread->looper,\n\t\t\tthread->looper_need_return,\n\t\t\tatomic_read(&thread->tmp_ref));\n\theader_pos = m->count;\n\tt = thread->transaction_stack;\n\twhile (t) {\n\t\tif (t->from == thread) {\n\t\t\tprint_binder_transaction_ilocked(m, thread->proc,\n\t\t\t\t\t\"    outgoing transaction\", t);\n\t\t\tt = t->from_parent;\n\t\t} else if (t->to_thread == thread) {\n\t\t\tprint_binder_transaction_ilocked(m, thread->proc,\n\t\t\t\t\t\t \"    incoming transaction\", t);\n\t\t\tt = t->to_parent;\n\t\t} else {\n\t\t\tprint_binder_transaction_ilocked(m, thread->proc,\n\t\t\t\t\t\"    bad transaction\", t);\n\t\t\tt = NULL;\n\t\t}\n\t}\n\tlist_for_each_entry(w, &thread->todo, entry) {\n\t\tprint_binder_work_ilocked(m, thread->proc, \"    \",\n\t\t\t\t\t  \"    pending transaction\", w);\n\t}\n\tif (!print_always && m->count == header_pos)\n\t\tm->count = start_pos;\n}\n\nstatic void print_binder_node_nilocked(struct seq_file *m,\n\t\t\t\t       struct binder_node *node)\n{\n\tstruct binder_ref *ref;\n\tstruct binder_work *w;\n\tint count;\n\n\tcount = 0;\n\thlist_for_each_entry(ref, &node->refs, node_entry)\n\t\tcount++;\n\n\tseq_printf(m, \"  node %d: u%016llx c%016llx hs %d hw %d ls %d lw %d is %d iw %d tr %d\",\n\t\t   node->debug_id, (u64)node->ptr, (u64)node->cookie,\n\t\t   node->has_strong_ref, node->has_weak_ref,\n\t\t   node->local_strong_refs, node->local_weak_refs,\n\t\t   node->internal_strong_refs, count, node->tmp_refs);\n\tif (count) {\n\t\tseq_puts(m, \" proc\");\n\t\thlist_for_each_entry(ref, &node->refs, node_entry)\n\t\t\tseq_printf(m, \" %d\", ref->proc->pid);\n\t}\n\tseq_puts(m, \"\\n\");\n\tif (node->proc) {\n\t\tlist_for_each_entry(w, &node->async_todo, entry)\n\t\t\tprint_binder_work_ilocked(m, node->proc, \"    \",\n\t\t\t\t\t  \"    pending async transaction\", w);\n\t}\n}\n\nstatic void print_binder_ref_olocked(struct seq_file *m,\n\t\t\t\t     struct binder_ref *ref)\n{\n\tbinder_node_lock(ref->node);\n\tseq_printf(m, \"  ref %d: desc %d %snode %d s %d w %d d %pK\\n\",\n\t\t   ref->data.debug_id, ref->data.desc,\n\t\t   ref->node->proc ? \"\" : \"dead \",\n\t\t   ref->node->debug_id, ref->data.strong,\n\t\t   ref->data.weak, ref->death);\n\tbinder_node_unlock(ref->node);\n}\n\nstatic void print_binder_proc(struct seq_file *m,\n\t\t\t      struct binder_proc *proc, int print_all)\n{\n\tstruct binder_work *w;\n\tstruct rb_node *n;\n\tsize_t start_pos = m->count;\n\tsize_t header_pos;\n\tstruct binder_node *last_node = NULL;\n\n\tseq_printf(m, \"proc %d\\n\", proc->pid);\n\tseq_printf(m, \"context %s\\n\", proc->context->name);\n\theader_pos = m->count;\n\n\tbinder_inner_proc_lock(proc);\n\tfor (n = rb_first(&proc->threads); n != NULL; n = rb_next(n))\n\t\tprint_binder_thread_ilocked(m, rb_entry(n, struct binder_thread,\n\t\t\t\t\t\trb_node), print_all);\n\n\tfor (n = rb_first(&proc->nodes); n != NULL; n = rb_next(n)) {\n\t\tstruct binder_node *node = rb_entry(n, struct binder_node,\n\t\t\t\t\t\t    rb_node);\n\t\tif (!print_all && !node->has_async_transaction)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * take a temporary reference on the node so it\n\t\t * survives and isn't removed from the tree\n\t\t * while we print it.\n\t\t */\n\t\tbinder_inc_node_tmpref_ilocked(node);\n\t\t/* Need to drop inner lock to take node lock */\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (last_node)\n\t\t\tbinder_put_node(last_node);\n\t\tbinder_node_inner_lock(node);\n\t\tprint_binder_node_nilocked(m, node);\n\t\tbinder_node_inner_unlock(node);\n\t\tlast_node = node;\n\t\tbinder_inner_proc_lock(proc);\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (last_node)\n\t\tbinder_put_node(last_node);\n\n\tif (print_all) {\n\t\tbinder_proc_lock(proc);\n\t\tfor (n = rb_first(&proc->refs_by_desc);\n\t\t     n != NULL;\n\t\t     n = rb_next(n))\n\t\t\tprint_binder_ref_olocked(m, rb_entry(n,\n\t\t\t\t\t\t\t    struct binder_ref,\n\t\t\t\t\t\t\t    rb_node_desc));\n\t\tbinder_proc_unlock(proc);\n\t}\n\tbinder_alloc_print_allocated(m, &proc->alloc);\n\tbinder_inner_proc_lock(proc);\n\tlist_for_each_entry(w, &proc->todo, entry)\n\t\tprint_binder_work_ilocked(m, proc, \"  \",\n\t\t\t\t\t  \"  pending transaction\", w);\n\tlist_for_each_entry(w, &proc->delivered_death, entry) {\n\t\tseq_puts(m, \"  has delivered dead binder\\n\");\n\t\tbreak;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (!print_all && m->count == header_pos)\n\t\tm->count = start_pos;\n}\n\nstatic const char * const binder_return_strings[] = {\n\t\"BR_ERROR\",\n\t\"BR_OK\",\n\t\"BR_TRANSACTION\",\n\t\"BR_REPLY\",\n\t\"BR_ACQUIRE_RESULT\",\n\t\"BR_DEAD_REPLY\",\n\t\"BR_TRANSACTION_COMPLETE\",\n\t\"BR_INCREFS\",\n\t\"BR_ACQUIRE\",\n\t\"BR_RELEASE\",\n\t\"BR_DECREFS\",\n\t\"BR_ATTEMPT_ACQUIRE\",\n\t\"BR_NOOP\",\n\t\"BR_SPAWN_LOOPER\",\n\t\"BR_FINISHED\",\n\t\"BR_DEAD_BINDER\",\n\t\"BR_CLEAR_DEATH_NOTIFICATION_DONE\",\n\t\"BR_FAILED_REPLY\"\n};\n\nstatic const char * const binder_command_strings[] = {\n\t\"BC_TRANSACTION\",\n\t\"BC_REPLY\",\n\t\"BC_ACQUIRE_RESULT\",\n\t\"BC_FREE_BUFFER\",\n\t\"BC_INCREFS\",\n\t\"BC_ACQUIRE\",\n\t\"BC_RELEASE\",\n\t\"BC_DECREFS\",\n\t\"BC_INCREFS_DONE\",\n\t\"BC_ACQUIRE_DONE\",\n\t\"BC_ATTEMPT_ACQUIRE\",\n\t\"BC_REGISTER_LOOPER\",\n\t\"BC_ENTER_LOOPER\",\n\t\"BC_EXIT_LOOPER\",\n\t\"BC_REQUEST_DEATH_NOTIFICATION\",\n\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\"BC_DEAD_BINDER_DONE\",\n\t\"BC_TRANSACTION_SG\",\n\t\"BC_REPLY_SG\",\n};\n\nstatic const char * const binder_objstat_strings[] = {\n\t\"proc\",\n\t\"thread\",\n\t\"node\",\n\t\"ref\",\n\t\"death\",\n\t\"transaction\",\n\t\"transaction_complete\"\n};\n\nstatic void print_binder_stats(struct seq_file *m, const char *prefix,\n\t\t\t       struct binder_stats *stats)\n{\n\tint i;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(stats->bc) !=\n\t\t     ARRAY_SIZE(binder_command_strings));\n\tfor (i = 0; i < ARRAY_SIZE(stats->bc); i++) {\n\t\tint temp = atomic_read(&stats->bc[i]);\n\n\t\tif (temp)\n\t\t\tseq_printf(m, \"%s%s: %d\\n\", prefix,\n\t\t\t\t   binder_command_strings[i], temp);\n\t}\n\n\tBUILD_BUG_ON(ARRAY_SIZE(stats->br) !=\n\t\t     ARRAY_SIZE(binder_return_strings));\n\tfor (i = 0; i < ARRAY_SIZE(stats->br); i++) {\n\t\tint temp = atomic_read(&stats->br[i]);\n\n\t\tif (temp)\n\t\t\tseq_printf(m, \"%s%s: %d\\n\", prefix,\n\t\t\t\t   binder_return_strings[i], temp);\n\t}\n\n\tBUILD_BUG_ON(ARRAY_SIZE(stats->obj_created) !=\n\t\t     ARRAY_SIZE(binder_objstat_strings));\n\tBUILD_BUG_ON(ARRAY_SIZE(stats->obj_created) !=\n\t\t     ARRAY_SIZE(stats->obj_deleted));\n\tfor (i = 0; i < ARRAY_SIZE(stats->obj_created); i++) {\n\t\tint created = atomic_read(&stats->obj_created[i]);\n\t\tint deleted = atomic_read(&stats->obj_deleted[i]);\n\n\t\tif (created || deleted)\n\t\t\tseq_printf(m, \"%s%s: active %d total %d\\n\",\n\t\t\t\tprefix,\n\t\t\t\tbinder_objstat_strings[i],\n\t\t\t\tcreated - deleted,\n\t\t\t\tcreated);\n\t}\n}\n\nstatic void print_binder_proc_stats(struct seq_file *m,\n\t\t\t\t    struct binder_proc *proc)\n{\n\tstruct binder_work *w;\n\tstruct binder_thread *thread;\n\tstruct rb_node *n;\n\tint count, strong, weak, ready_threads;\n\tsize_t free_async_space =\n\t\tbinder_alloc_get_free_async_space(&proc->alloc);\n\n\tseq_printf(m, \"proc %d\\n\", proc->pid);\n\tseq_printf(m, \"context %s\\n\", proc->context->name);\n\tcount = 0;\n\tready_threads = 0;\n\tbinder_inner_proc_lock(proc);\n\tfor (n = rb_first(&proc->threads); n != NULL; n = rb_next(n))\n\t\tcount++;\n\n\tlist_for_each_entry(thread, &proc->waiting_threads, waiting_thread_node)\n\t\tready_threads++;\n\n\tseq_printf(m, \"  threads: %d\\n\", count);\n\tseq_printf(m, \"  requested threads: %d+%d/%d\\n\"\n\t\t\t\"  ready threads %d\\n\"\n\t\t\t\"  free async space %zd\\n\", proc->requested_threads,\n\t\t\tproc->requested_threads_started, proc->max_threads,\n\t\t\tready_threads,\n\t\t\tfree_async_space);\n\tcount = 0;\n\tfor (n = rb_first(&proc->nodes); n != NULL; n = rb_next(n))\n\t\tcount++;\n\tbinder_inner_proc_unlock(proc);\n\tseq_printf(m, \"  nodes: %d\\n\", count);\n\tcount = 0;\n\tstrong = 0;\n\tweak = 0;\n\tbinder_proc_lock(proc);\n\tfor (n = rb_first(&proc->refs_by_desc); n != NULL; n = rb_next(n)) {\n\t\tstruct binder_ref *ref = rb_entry(n, struct binder_ref,\n\t\t\t\t\t\t  rb_node_desc);\n\t\tcount++;\n\t\tstrong += ref->data.strong;\n\t\tweak += ref->data.weak;\n\t}\n\tbinder_proc_unlock(proc);\n\tseq_printf(m, \"  refs: %d s %d w %d\\n\", count, strong, weak);\n\n\tcount = binder_alloc_get_allocated_count(&proc->alloc);\n\tseq_printf(m, \"  buffers: %d\\n\", count);\n\n\tbinder_alloc_print_pages(m, &proc->alloc);\n\n\tcount = 0;\n\tbinder_inner_proc_lock(proc);\n\tlist_for_each_entry(w, &proc->todo, entry) {\n\t\tif (w->type == BINDER_WORK_TRANSACTION)\n\t\t\tcount++;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tseq_printf(m, \"  pending transactions: %d\\n\", count);\n\n\tprint_binder_stats(m, \"  \", &proc->stats);\n}\n\n\nint binder_state_show(struct seq_file *m, void *unused)\n{\n\tstruct binder_proc *proc;\n\tstruct binder_node *node;\n\tstruct binder_node *last_node = NULL;\n\n\tseq_puts(m, \"binder state:\\n\");\n\n\tspin_lock(&binder_dead_nodes_lock);\n\tif (!hlist_empty(&binder_dead_nodes))\n\t\tseq_puts(m, \"dead nodes:\\n\");\n\thlist_for_each_entry(node, &binder_dead_nodes, dead_node) {\n\t\t/*\n\t\t * take a temporary reference on the node so it\n\t\t * survives and isn't removed from the list\n\t\t * while we print it.\n\t\t */\n\t\tnode->tmp_refs++;\n\t\tspin_unlock(&binder_dead_nodes_lock);\n\t\tif (last_node)\n\t\t\tbinder_put_node(last_node);\n\t\tbinder_node_lock(node);\n\t\tprint_binder_node_nilocked(m, node);\n\t\tbinder_node_unlock(node);\n\t\tlast_node = node;\n\t\tspin_lock(&binder_dead_nodes_lock);\n\t}\n\tspin_unlock(&binder_dead_nodes_lock);\n\tif (last_node)\n\t\tbinder_put_node(last_node);\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(proc, &binder_procs, proc_node)\n\t\tprint_binder_proc(m, proc, 1);\n\tmutex_unlock(&binder_procs_lock);\n\n\treturn 0;\n}\n\nint binder_stats_show(struct seq_file *m, void *unused)\n{\n\tstruct binder_proc *proc;\n\n\tseq_puts(m, \"binder stats:\\n\");\n\n\tprint_binder_stats(m, \"\", &binder_stats);\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(proc, &binder_procs, proc_node)\n\t\tprint_binder_proc_stats(m, proc);\n\tmutex_unlock(&binder_procs_lock);\n\n\treturn 0;\n}\n\nint binder_transactions_show(struct seq_file *m, void *unused)\n{\n\tstruct binder_proc *proc;\n\n\tseq_puts(m, \"binder transactions:\\n\");\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(proc, &binder_procs, proc_node)\n\t\tprint_binder_proc(m, proc, 0);\n\tmutex_unlock(&binder_procs_lock);\n\n\treturn 0;\n}\n\nstatic int proc_show(struct seq_file *m, void *unused)\n{\n\tstruct binder_proc *itr;\n\tint pid = (unsigned long)m->private;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(itr, &binder_procs, proc_node) {\n\t\tif (itr->pid == pid) {\n\t\t\tseq_puts(m, \"binder proc state:\\n\");\n\t\t\tprint_binder_proc(m, itr, 1);\n\t\t}\n\t}\n\tmutex_unlock(&binder_procs_lock);\n\n\treturn 0;\n}\n\nstatic void print_binder_transaction_log_entry(struct seq_file *m,\n\t\t\t\t\tstruct binder_transaction_log_entry *e)\n{\n\tint debug_id = READ_ONCE(e->debug_id_done);\n\t/*\n\t * read barrier to guarantee debug_id_done read before\n\t * we print the log values\n\t */\n\tsmp_rmb();\n\tseq_printf(m,\n\t\t   \"%d: %s from %d:%d to %d:%d context %s node %d handle %d size %d:%d ret %d/%d l=%d\",\n\t\t   e->debug_id, (e->call_type == 2) ? \"reply\" :\n\t\t   ((e->call_type == 1) ? \"async\" : \"call \"), e->from_proc,\n\t\t   e->from_thread, e->to_proc, e->to_thread, e->context_name,\n\t\t   e->to_node, e->target_handle, e->data_size, e->offsets_size,\n\t\t   e->return_error, e->return_error_param,\n\t\t   e->return_error_line);\n\t/*\n\t * read-barrier to guarantee read of debug_id_done after\n\t * done printing the fields of the entry\n\t */\n\tsmp_rmb();\n\tseq_printf(m, debug_id && debug_id == READ_ONCE(e->debug_id_done) ?\n\t\t\t\"\\n\" : \" (incomplete)\\n\");\n}\n\nint binder_transaction_log_show(struct seq_file *m, void *unused)\n{\n\tstruct binder_transaction_log *log = m->private;\n\tunsigned int log_cur = atomic_read(&log->cur);\n\tunsigned int count;\n\tunsigned int cur;\n\tint i;\n\n\tcount = log_cur + 1;\n\tcur = count < ARRAY_SIZE(log->entry) && !log->full ?\n\t\t0 : count % ARRAY_SIZE(log->entry);\n\tif (count > ARRAY_SIZE(log->entry) || log->full)\n\t\tcount = ARRAY_SIZE(log->entry);\n\tfor (i = 0; i < count; i++) {\n\t\tunsigned int index = cur++ % ARRAY_SIZE(log->entry);\n\n\t\tprint_binder_transaction_log_entry(m, &log->entry[index]);\n\t}\n\treturn 0;\n}\n\nconst struct file_operations binder_fops = {\n\t.owner = THIS_MODULE,\n\t.poll = binder_poll,\n\t.unlocked_ioctl = binder_ioctl,\n\t.compat_ioctl = binder_ioctl,\n\t.mmap = binder_mmap,\n\t.open = binder_open,\n\t.flush = binder_flush,\n\t.release = binder_release,\n};\n\nstatic int __init init_binder_device(const char *name)\n{\n\tint ret;\n\tstruct binder_device *binder_device;\n\n\tbinder_device = kzalloc(sizeof(*binder_device), GFP_KERNEL);\n\tif (!binder_device)\n\t\treturn -ENOMEM;\n\n\tbinder_device->miscdev.fops = &binder_fops;\n\tbinder_device->miscdev.minor = MISC_DYNAMIC_MINOR;\n\tbinder_device->miscdev.name = name;\n\n\trefcount_set(&binder_device->ref, 1);\n\tbinder_device->context.binder_context_mgr_uid = INVALID_UID;\n\tbinder_device->context.name = name;\n\tmutex_init(&binder_device->context.context_mgr_node_lock);\n\n\tret = misc_register(&binder_device->miscdev);\n\tif (ret < 0) {\n\t\tkfree(binder_device);\n\t\treturn ret;\n\t}\n\n\thlist_add_head(&binder_device->hlist, &binder_devices);\n\n\treturn ret;\n}\n\nstatic int __init binder_init(void)\n{\n\tint ret;\n\tchar *device_name, *device_tmp;\n\tstruct binder_device *device;\n\tstruct hlist_node *tmp;\n\tchar *device_names = NULL;\n\n\tret = binder_alloc_shrinker_init();\n\tif (ret)\n\t\treturn ret;\n\n\tatomic_set(&binder_transaction_log.cur, ~0U);\n\tatomic_set(&binder_transaction_log_failed.cur, ~0U);\n\n\tbinder_debugfs_dir_entry_root = debugfs_create_dir(\"binder\", NULL);\n\tif (binder_debugfs_dir_entry_root)\n\t\tbinder_debugfs_dir_entry_proc = debugfs_create_dir(\"proc\",\n\t\t\t\t\t\t binder_debugfs_dir_entry_root);\n\n\tif (binder_debugfs_dir_entry_root) {\n\t\tdebugfs_create_file(\"state\",\n\t\t\t\t    0444,\n\t\t\t\t    binder_debugfs_dir_entry_root,\n\t\t\t\t    NULL,\n\t\t\t\t    &binder_state_fops);\n\t\tdebugfs_create_file(\"stats\",\n\t\t\t\t    0444,\n\t\t\t\t    binder_debugfs_dir_entry_root,\n\t\t\t\t    NULL,\n\t\t\t\t    &binder_stats_fops);\n\t\tdebugfs_create_file(\"transactions\",\n\t\t\t\t    0444,\n\t\t\t\t    binder_debugfs_dir_entry_root,\n\t\t\t\t    NULL,\n\t\t\t\t    &binder_transactions_fops);\n\t\tdebugfs_create_file(\"transaction_log\",\n\t\t\t\t    0444,\n\t\t\t\t    binder_debugfs_dir_entry_root,\n\t\t\t\t    &binder_transaction_log,\n\t\t\t\t    &binder_transaction_log_fops);\n\t\tdebugfs_create_file(\"failed_transaction_log\",\n\t\t\t\t    0444,\n\t\t\t\t    binder_debugfs_dir_entry_root,\n\t\t\t\t    &binder_transaction_log_failed,\n\t\t\t\t    &binder_transaction_log_fops);\n\t}\n\n\tif (!IS_ENABLED(CONFIG_ANDROID_BINDERFS) &&\n\t    strcmp(binder_devices_param, \"\") != 0) {\n\t\t/*\n\t\t* Copy the module_parameter string, because we don't want to\n\t\t* tokenize it in-place.\n\t\t */\n\t\tdevice_names = kstrdup(binder_devices_param, GFP_KERNEL);\n\t\tif (!device_names) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_alloc_device_names_failed;\n\t\t}\n\n\t\tdevice_tmp = device_names;\n\t\twhile ((device_name = strsep(&device_tmp, \",\"))) {\n\t\t\tret = init_binder_device(device_name);\n\t\t\tif (ret)\n\t\t\t\tgoto err_init_binder_device_failed;\n\t\t}\n\t}\n\n\tret = init_binderfs();\n\tif (ret)\n\t\tgoto err_init_binder_device_failed;\n\n\treturn ret;\n\nerr_init_binder_device_failed:\n\thlist_for_each_entry_safe(device, tmp, &binder_devices, hlist) {\n\t\tmisc_deregister(&device->miscdev);\n\t\thlist_del(&device->hlist);\n\t\tkfree(device);\n\t}\n\n\tkfree(device_names);\n\nerr_alloc_device_names_failed:\n\tdebugfs_remove_recursive(binder_debugfs_dir_entry_root);\n\n\treturn ret;\n}\n\ndevice_initcall(binder_init);\n\n#define CREATE_TRACE_POINTS\n#include \"binder_trace.h\"\n\nMODULE_LICENSE(\"GPL v2\");\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/file.c\n *\n *  Copyright (C) 1998-1999, Stephen Tweedie and Bill Hawes\n *\n *  Manage the dynamic fd arrays in the process files_struct.\n */\n\n#include <linux/syscalls.h>\n#include <linux/export.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/slab.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/bitops.h>\n#include <linux/spinlock.h>\n#include <linux/rcupdate.h>\n\n#include <linux/nospec.h>\n\nunsigned int sysctl_nr_open __read_mostly = 1024*1024;\nunsigned int sysctl_nr_open_min = BITS_PER_LONG;\n/* our min() is unusable in constant expressions ;-/ */\n#define __const_min(x, y) ((x) < (y) ? (x) : (y))\nunsigned int sysctl_nr_open_max =\n\t__const_min(INT_MAX, ~(size_t)0/sizeof(void *)) & -BITS_PER_LONG;\n\nstatic void __free_fdtable(struct fdtable *fdt)\n{\n\tkvfree(fdt->fd);\n\tkvfree(fdt->open_fds);\n\tkfree(fdt);\n}\n\nstatic void free_fdtable_rcu(struct rcu_head *rcu)\n{\n\t__free_fdtable(container_of(rcu, struct fdtable, rcu));\n}\n\n#define BITBIT_NR(nr)\tBITS_TO_LONGS(BITS_TO_LONGS(nr))\n#define BITBIT_SIZE(nr)\t(BITBIT_NR(nr) * sizeof(long))\n\n/*\n * Copy 'count' fd bits from the old table to the new table and clear the extra\n * space if any.  This does not copy the file pointers.  Called with the files\n * spinlock held for write.\n */\nstatic void copy_fd_bitmaps(struct fdtable *nfdt, struct fdtable *ofdt,\n\t\t\t    unsigned int count)\n{\n\tunsigned int cpy, set;\n\n\tcpy = count / BITS_PER_BYTE;\n\tset = (nfdt->max_fds - count) / BITS_PER_BYTE;\n\tmemcpy(nfdt->open_fds, ofdt->open_fds, cpy);\n\tmemset((char *)nfdt->open_fds + cpy, 0, set);\n\tmemcpy(nfdt->close_on_exec, ofdt->close_on_exec, cpy);\n\tmemset((char *)nfdt->close_on_exec + cpy, 0, set);\n\n\tcpy = BITBIT_SIZE(count);\n\tset = BITBIT_SIZE(nfdt->max_fds) - cpy;\n\tmemcpy(nfdt->full_fds_bits, ofdt->full_fds_bits, cpy);\n\tmemset((char *)nfdt->full_fds_bits + cpy, 0, set);\n}\n\n/*\n * Copy all file descriptors from the old table to the new, expanded table and\n * clear the extra space.  Called with the files spinlock held for write.\n */\nstatic void copy_fdtable(struct fdtable *nfdt, struct fdtable *ofdt)\n{\n\tsize_t cpy, set;\n\n\tBUG_ON(nfdt->max_fds < ofdt->max_fds);\n\n\tcpy = ofdt->max_fds * sizeof(struct file *);\n\tset = (nfdt->max_fds - ofdt->max_fds) * sizeof(struct file *);\n\tmemcpy(nfdt->fd, ofdt->fd, cpy);\n\tmemset((char *)nfdt->fd + cpy, 0, set);\n\n\tcopy_fd_bitmaps(nfdt, ofdt, ofdt->max_fds);\n}\n\nstatic struct fdtable * alloc_fdtable(unsigned int nr)\n{\n\tstruct fdtable *fdt;\n\tvoid *data;\n\n\t/*\n\t * Figure out how many fds we actually want to support in this fdtable.\n\t * Allocation steps are keyed to the size of the fdarray, since it\n\t * grows far faster than any of the other dynamic data. We try to fit\n\t * the fdarray into comfortable page-tuned chunks: starting at 1024B\n\t * and growing in powers of two from there on.\n\t */\n\tnr /= (1024 / sizeof(struct file *));\n\tnr = roundup_pow_of_two(nr + 1);\n\tnr *= (1024 / sizeof(struct file *));\n\t/*\n\t * Note that this can drive nr *below* what we had passed if sysctl_nr_open\n\t * had been set lower between the check in expand_files() and here.  Deal\n\t * with that in caller, it's cheaper that way.\n\t *\n\t * We make sure that nr remains a multiple of BITS_PER_LONG - otherwise\n\t * bitmaps handling below becomes unpleasant, to put it mildly...\n\t */\n\tif (unlikely(nr > sysctl_nr_open))\n\t\tnr = ((sysctl_nr_open - 1) | (BITS_PER_LONG - 1)) + 1;\n\n\tfdt = kmalloc(sizeof(struct fdtable), GFP_KERNEL_ACCOUNT);\n\tif (!fdt)\n\t\tgoto out;\n\tfdt->max_fds = nr;\n\tdata = kvmalloc_array(nr, sizeof(struct file *), GFP_KERNEL_ACCOUNT);\n\tif (!data)\n\t\tgoto out_fdt;\n\tfdt->fd = data;\n\n\tdata = kvmalloc(max_t(size_t,\n\t\t\t\t 2 * nr / BITS_PER_BYTE + BITBIT_SIZE(nr), L1_CACHE_BYTES),\n\t\t\t\t GFP_KERNEL_ACCOUNT);\n\tif (!data)\n\t\tgoto out_arr;\n\tfdt->open_fds = data;\n\tdata += nr / BITS_PER_BYTE;\n\tfdt->close_on_exec = data;\n\tdata += nr / BITS_PER_BYTE;\n\tfdt->full_fds_bits = data;\n\n\treturn fdt;\n\nout_arr:\n\tkvfree(fdt->fd);\nout_fdt:\n\tkfree(fdt);\nout:\n\treturn NULL;\n}\n\n/*\n * Expand the file descriptor table.\n * This function will allocate a new fdtable and both fd array and fdset, of\n * the given size.\n * Return <0 error code on error; 1 on successful completion.\n * The files->file_lock should be held on entry, and will be held on exit.\n */\nstatic int expand_fdtable(struct files_struct *files, unsigned int nr)\n\t__releases(files->file_lock)\n\t__acquires(files->file_lock)\n{\n\tstruct fdtable *new_fdt, *cur_fdt;\n\n\tspin_unlock(&files->file_lock);\n\tnew_fdt = alloc_fdtable(nr);\n\n\t/* make sure all __fd_install() have seen resize_in_progress\n\t * or have finished their rcu_read_lock_sched() section.\n\t */\n\tif (atomic_read(&files->count) > 1)\n\t\tsynchronize_rcu();\n\n\tspin_lock(&files->file_lock);\n\tif (!new_fdt)\n\t\treturn -ENOMEM;\n\t/*\n\t * extremely unlikely race - sysctl_nr_open decreased between the check in\n\t * caller and alloc_fdtable().  Cheaper to catch it here...\n\t */\n\tif (unlikely(new_fdt->max_fds <= nr)) {\n\t\t__free_fdtable(new_fdt);\n\t\treturn -EMFILE;\n\t}\n\tcur_fdt = files_fdtable(files);\n\tBUG_ON(nr < cur_fdt->max_fds);\n\tcopy_fdtable(new_fdt, cur_fdt);\n\trcu_assign_pointer(files->fdt, new_fdt);\n\tif (cur_fdt != &files->fdtab)\n\t\tcall_rcu(&cur_fdt->rcu, free_fdtable_rcu);\n\t/* coupled with smp_rmb() in __fd_install() */\n\tsmp_wmb();\n\treturn 1;\n}\n\n/*\n * Expand files.\n * This function will expand the file structures, if the requested size exceeds\n * the current capacity and there is room for expansion.\n * Return <0 error code on error; 0 when nothing done; 1 when files were\n * expanded and execution may have blocked.\n * The files->file_lock should be held on entry, and will be held on exit.\n */\nstatic int expand_files(struct files_struct *files, unsigned int nr)\n\t__releases(files->file_lock)\n\t__acquires(files->file_lock)\n{\n\tstruct fdtable *fdt;\n\tint expanded = 0;\n\nrepeat:\n\tfdt = files_fdtable(files);\n\n\t/* Do we need to expand? */\n\tif (nr < fdt->max_fds)\n\t\treturn expanded;\n\n\t/* Can we expand? */\n\tif (nr >= sysctl_nr_open)\n\t\treturn -EMFILE;\n\n\tif (unlikely(files->resize_in_progress)) {\n\t\tspin_unlock(&files->file_lock);\n\t\texpanded = 1;\n\t\twait_event(files->resize_wait, !files->resize_in_progress);\n\t\tspin_lock(&files->file_lock);\n\t\tgoto repeat;\n\t}\n\n\t/* All good, so we try */\n\tfiles->resize_in_progress = true;\n\texpanded = expand_fdtable(files, nr);\n\tfiles->resize_in_progress = false;\n\n\twake_up_all(&files->resize_wait);\n\treturn expanded;\n}\n\nstatic inline void __set_close_on_exec(unsigned int fd, struct fdtable *fdt)\n{\n\t__set_bit(fd, fdt->close_on_exec);\n}\n\nstatic inline void __clear_close_on_exec(unsigned int fd, struct fdtable *fdt)\n{\n\tif (test_bit(fd, fdt->close_on_exec))\n\t\t__clear_bit(fd, fdt->close_on_exec);\n}\n\nstatic inline void __set_open_fd(unsigned int fd, struct fdtable *fdt)\n{\n\t__set_bit(fd, fdt->open_fds);\n\tfd /= BITS_PER_LONG;\n\tif (!~fdt->open_fds[fd])\n\t\t__set_bit(fd, fdt->full_fds_bits);\n}\n\nstatic inline void __clear_open_fd(unsigned int fd, struct fdtable *fdt)\n{\n\t__clear_bit(fd, fdt->open_fds);\n\t__clear_bit(fd / BITS_PER_LONG, fdt->full_fds_bits);\n}\n\nstatic unsigned int count_open_files(struct fdtable *fdt)\n{\n\tunsigned int size = fdt->max_fds;\n\tunsigned int i;\n\n\t/* Find the last open fd */\n\tfor (i = size / BITS_PER_LONG; i > 0; ) {\n\t\tif (fdt->open_fds[--i])\n\t\t\tbreak;\n\t}\n\ti = (i + 1) * BITS_PER_LONG;\n\treturn i;\n}\n\n/*\n * Allocate a new files structure and copy contents from the\n * passed in files structure.\n * errorp will be valid only when the returned files_struct is NULL.\n */\nstruct files_struct *dup_fd(struct files_struct *oldf, int *errorp)\n{\n\tstruct files_struct *newf;\n\tstruct file **old_fds, **new_fds;\n\tunsigned int open_files, i;\n\tstruct fdtable *old_fdt, *new_fdt;\n\n\t*errorp = -ENOMEM;\n\tnewf = kmem_cache_alloc(files_cachep, GFP_KERNEL);\n\tif (!newf)\n\t\tgoto out;\n\n\tatomic_set(&newf->count, 1);\n\n\tspin_lock_init(&newf->file_lock);\n\tnewf->resize_in_progress = false;\n\tinit_waitqueue_head(&newf->resize_wait);\n\tnewf->next_fd = 0;\n\tnew_fdt = &newf->fdtab;\n\tnew_fdt->max_fds = NR_OPEN_DEFAULT;\n\tnew_fdt->close_on_exec = newf->close_on_exec_init;\n\tnew_fdt->open_fds = newf->open_fds_init;\n\tnew_fdt->full_fds_bits = newf->full_fds_bits_init;\n\tnew_fdt->fd = &newf->fd_array[0];\n\n\tspin_lock(&oldf->file_lock);\n\told_fdt = files_fdtable(oldf);\n\topen_files = count_open_files(old_fdt);\n\n\t/*\n\t * Check whether we need to allocate a larger fd array and fd set.\n\t */\n\twhile (unlikely(open_files > new_fdt->max_fds)) {\n\t\tspin_unlock(&oldf->file_lock);\n\n\t\tif (new_fdt != &newf->fdtab)\n\t\t\t__free_fdtable(new_fdt);\n\n\t\tnew_fdt = alloc_fdtable(open_files - 1);\n\t\tif (!new_fdt) {\n\t\t\t*errorp = -ENOMEM;\n\t\t\tgoto out_release;\n\t\t}\n\n\t\t/* beyond sysctl_nr_open; nothing to do */\n\t\tif (unlikely(new_fdt->max_fds < open_files)) {\n\t\t\t__free_fdtable(new_fdt);\n\t\t\t*errorp = -EMFILE;\n\t\t\tgoto out_release;\n\t\t}\n\n\t\t/*\n\t\t * Reacquire the oldf lock and a pointer to its fd table\n\t\t * who knows it may have a new bigger fd table. We need\n\t\t * the latest pointer.\n\t\t */\n\t\tspin_lock(&oldf->file_lock);\n\t\told_fdt = files_fdtable(oldf);\n\t\topen_files = count_open_files(old_fdt);\n\t}\n\n\tcopy_fd_bitmaps(new_fdt, old_fdt, open_files);\n\n\told_fds = old_fdt->fd;\n\tnew_fds = new_fdt->fd;\n\n\tfor (i = open_files; i != 0; i--) {\n\t\tstruct file *f = *old_fds++;\n\t\tif (f) {\n\t\t\tget_file(f);\n\t\t} else {\n\t\t\t/*\n\t\t\t * The fd may be claimed in the fd bitmap but not yet\n\t\t\t * instantiated in the files array if a sibling thread\n\t\t\t * is partway through open().  So make sure that this\n\t\t\t * fd is available to the new process.\n\t\t\t */\n\t\t\t__clear_open_fd(open_files - i, new_fdt);\n\t\t}\n\t\trcu_assign_pointer(*new_fds++, f);\n\t}\n\tspin_unlock(&oldf->file_lock);\n\n\t/* clear the remainder */\n\tmemset(new_fds, 0, (new_fdt->max_fds - open_files) * sizeof(struct file *));\n\n\trcu_assign_pointer(newf->fdt, new_fdt);\n\n\treturn newf;\n\nout_release:\n\tkmem_cache_free(files_cachep, newf);\nout:\n\treturn NULL;\n}\n\nstatic struct fdtable *close_files(struct files_struct * files)\n{\n\t/*\n\t * It is safe to dereference the fd table without RCU or\n\t * ->file_lock because this is the last reference to the\n\t * files structure.\n\t */\n\tstruct fdtable *fdt = rcu_dereference_raw(files->fdt);\n\tunsigned int i, j = 0;\n\n\tfor (;;) {\n\t\tunsigned long set;\n\t\ti = j * BITS_PER_LONG;\n\t\tif (i >= fdt->max_fds)\n\t\t\tbreak;\n\t\tset = fdt->open_fds[j++];\n\t\twhile (set) {\n\t\t\tif (set & 1) {\n\t\t\t\tstruct file * file = xchg(&fdt->fd[i], NULL);\n\t\t\t\tif (file) {\n\t\t\t\t\tfilp_close(file, files);\n\t\t\t\t\tcond_resched();\n\t\t\t\t}\n\t\t\t}\n\t\t\ti++;\n\t\t\tset >>= 1;\n\t\t}\n\t}\n\n\treturn fdt;\n}\n\nstruct files_struct *get_files_struct(struct task_struct *task)\n{\n\tstruct files_struct *files;\n\n\ttask_lock(task);\n\tfiles = task->files;\n\tif (files)\n\t\tatomic_inc(&files->count);\n\ttask_unlock(task);\n\n\treturn files;\n}\n\nvoid put_files_struct(struct files_struct *files)\n{\n\tif (atomic_dec_and_test(&files->count)) {\n\t\tstruct fdtable *fdt = close_files(files);\n\n\t\t/* free the arrays if they are not embedded */\n\t\tif (fdt != &files->fdtab)\n\t\t\t__free_fdtable(fdt);\n\t\tkmem_cache_free(files_cachep, files);\n\t}\n}\n\nvoid reset_files_struct(struct files_struct *files)\n{\n\tstruct task_struct *tsk = current;\n\tstruct files_struct *old;\n\n\told = tsk->files;\n\ttask_lock(tsk);\n\ttsk->files = files;\n\ttask_unlock(tsk);\n\tput_files_struct(old);\n}\n\nvoid exit_files(struct task_struct *tsk)\n{\n\tstruct files_struct * files = tsk->files;\n\n\tif (files) {\n\t\ttask_lock(tsk);\n\t\ttsk->files = NULL;\n\t\ttask_unlock(tsk);\n\t\tput_files_struct(files);\n\t}\n}\n\nstruct files_struct init_files = {\n\t.count\t\t= ATOMIC_INIT(1),\n\t.fdt\t\t= &init_files.fdtab,\n\t.fdtab\t\t= {\n\t\t.max_fds\t= NR_OPEN_DEFAULT,\n\t\t.fd\t\t= &init_files.fd_array[0],\n\t\t.close_on_exec\t= init_files.close_on_exec_init,\n\t\t.open_fds\t= init_files.open_fds_init,\n\t\t.full_fds_bits\t= init_files.full_fds_bits_init,\n\t},\n\t.file_lock\t= __SPIN_LOCK_UNLOCKED(init_files.file_lock),\n\t.resize_wait\t= __WAIT_QUEUE_HEAD_INITIALIZER(init_files.resize_wait),\n};\n\nstatic unsigned int find_next_fd(struct fdtable *fdt, unsigned int start)\n{\n\tunsigned int maxfd = fdt->max_fds;\n\tunsigned int maxbit = maxfd / BITS_PER_LONG;\n\tunsigned int bitbit = start / BITS_PER_LONG;\n\n\tbitbit = find_next_zero_bit(fdt->full_fds_bits, maxbit, bitbit) * BITS_PER_LONG;\n\tif (bitbit > maxfd)\n\t\treturn maxfd;\n\tif (bitbit > start)\n\t\tstart = bitbit;\n\treturn find_next_zero_bit(fdt->open_fds, maxfd, start);\n}\n\n/*\n * allocate a file descriptor, mark it busy.\n */\nint __alloc_fd(struct files_struct *files,\n\t       unsigned start, unsigned end, unsigned flags)\n{\n\tunsigned int fd;\n\tint error;\n\tstruct fdtable *fdt;\n\n\tspin_lock(&files->file_lock);\nrepeat:\n\tfdt = files_fdtable(files);\n\tfd = start;\n\tif (fd < files->next_fd)\n\t\tfd = files->next_fd;\n\n\tif (fd < fdt->max_fds)\n\t\tfd = find_next_fd(fdt, fd);\n\n\t/*\n\t * N.B. For clone tasks sharing a files structure, this test\n\t * will limit the total number of files that can be opened.\n\t */\n\terror = -EMFILE;\n\tif (fd >= end)\n\t\tgoto out;\n\n\terror = expand_files(files, fd);\n\tif (error < 0)\n\t\tgoto out;\n\n\t/*\n\t * If we needed to expand the fs array we\n\t * might have blocked - try again.\n\t */\n\tif (error)\n\t\tgoto repeat;\n\n\tif (start <= files->next_fd)\n\t\tfiles->next_fd = fd + 1;\n\n\t__set_open_fd(fd, fdt);\n\tif (flags & O_CLOEXEC)\n\t\t__set_close_on_exec(fd, fdt);\n\telse\n\t\t__clear_close_on_exec(fd, fdt);\n\terror = fd;\n#if 1\n\t/* Sanity check */\n\tif (rcu_access_pointer(fdt->fd[fd]) != NULL) {\n\t\tprintk(KERN_WARNING \"alloc_fd: slot %d not NULL!\\n\", fd);\n\t\trcu_assign_pointer(fdt->fd[fd], NULL);\n\t}\n#endif\n\nout:\n\tspin_unlock(&files->file_lock);\n\treturn error;\n}\n\nstatic int alloc_fd(unsigned start, unsigned flags)\n{\n\treturn __alloc_fd(current->files, start, rlimit(RLIMIT_NOFILE), flags);\n}\n\nint __get_unused_fd_flags(unsigned flags, unsigned long nofile)\n{\n\treturn __alloc_fd(current->files, 0, nofile, flags);\n}\n\nint get_unused_fd_flags(unsigned flags)\n{\n\treturn __get_unused_fd_flags(flags, rlimit(RLIMIT_NOFILE));\n}\nEXPORT_SYMBOL(get_unused_fd_flags);\n\nstatic void __put_unused_fd(struct files_struct *files, unsigned int fd)\n{\n\tstruct fdtable *fdt = files_fdtable(files);\n\t__clear_open_fd(fd, fdt);\n\tif (fd < files->next_fd)\n\t\tfiles->next_fd = fd;\n}\n\nvoid put_unused_fd(unsigned int fd)\n{\n\tstruct files_struct *files = current->files;\n\tspin_lock(&files->file_lock);\n\t__put_unused_fd(files, fd);\n\tspin_unlock(&files->file_lock);\n}\n\nEXPORT_SYMBOL(put_unused_fd);\n\n/*\n * Install a file pointer in the fd array.\n *\n * The VFS is full of places where we drop the files lock between\n * setting the open_fds bitmap and installing the file in the file\n * array.  At any such point, we are vulnerable to a dup2() race\n * installing a file in the array before us.  We need to detect this and\n * fput() the struct file we are about to overwrite in this case.\n *\n * It should never happen - if we allow dup2() do it, _really_ bad things\n * will follow.\n *\n * NOTE: __fd_install() variant is really, really low-level; don't\n * use it unless you are forced to by truly lousy API shoved down\n * your throat.  'files' *MUST* be either current->files or obtained\n * by get_files_struct(current) done by whoever had given it to you,\n * or really bad things will happen.  Normally you want to use\n * fd_install() instead.\n */\n\nvoid __fd_install(struct files_struct *files, unsigned int fd,\n\t\tstruct file *file)\n{\n\tstruct fdtable *fdt;\n\n\trcu_read_lock_sched();\n\n\tif (unlikely(files->resize_in_progress)) {\n\t\trcu_read_unlock_sched();\n\t\tspin_lock(&files->file_lock);\n\t\tfdt = files_fdtable(files);\n\t\tBUG_ON(fdt->fd[fd] != NULL);\n\t\trcu_assign_pointer(fdt->fd[fd], file);\n\t\tspin_unlock(&files->file_lock);\n\t\treturn;\n\t}\n\t/* coupled with smp_wmb() in expand_fdtable() */\n\tsmp_rmb();\n\tfdt = rcu_dereference_sched(files->fdt);\n\tBUG_ON(fdt->fd[fd] != NULL);\n\trcu_assign_pointer(fdt->fd[fd], file);\n\trcu_read_unlock_sched();\n}\n\nvoid fd_install(unsigned int fd, struct file *file)\n{\n\t__fd_install(current->files, fd, file);\n}\n\nEXPORT_SYMBOL(fd_install);\n\n/*\n * The same warnings as for __alloc_fd()/__fd_install() apply here...\n */\nint __close_fd(struct files_struct *files, unsigned fd)\n{\n\tstruct file *file;\n\tstruct fdtable *fdt;\n\n\tspin_lock(&files->file_lock);\n\tfdt = files_fdtable(files);\n\tif (fd >= fdt->max_fds)\n\t\tgoto out_unlock;\n\tfd = array_index_nospec(fd, fdt->max_fds);\n\tfile = fdt->fd[fd];\n\tif (!file)\n\t\tgoto out_unlock;\n\trcu_assign_pointer(fdt->fd[fd], NULL);\n\t__put_unused_fd(files, fd);\n\tspin_unlock(&files->file_lock);\n\treturn filp_close(file, files);\n\nout_unlock:\n\tspin_unlock(&files->file_lock);\n\treturn -EBADF;\n}\nEXPORT_SYMBOL(__close_fd); /* for ksys_close() */\n\n/*\n * variant of __close_fd that gets a ref on the file for later fput\n */\nint __close_fd_get_file(unsigned int fd, struct file **res)\n{\n\tstruct files_struct *files = current->files;\n\tstruct file *file;\n\tstruct fdtable *fdt;\n\n\tspin_lock(&files->file_lock);\n\tfdt = files_fdtable(files);\n\tif (fd >= fdt->max_fds)\n\t\tgoto out_unlock;\n\tfile = fdt->fd[fd];\n\tif (!file)\n\t\tgoto out_unlock;\n\trcu_assign_pointer(fdt->fd[fd], NULL);\n\t__put_unused_fd(files, fd);\n\tspin_unlock(&files->file_lock);\n\tget_file(file);\n\t*res = file;\n\treturn filp_close(file, files);\n\nout_unlock:\n\tspin_unlock(&files->file_lock);\n\t*res = NULL;\n\treturn -ENOENT;\n}\n\nvoid do_close_on_exec(struct files_struct *files)\n{\n\tunsigned i;\n\tstruct fdtable *fdt;\n\n\t/* exec unshares first */\n\tspin_lock(&files->file_lock);\n\tfor (i = 0; ; i++) {\n\t\tunsigned long set;\n\t\tunsigned fd = i * BITS_PER_LONG;\n\t\tfdt = files_fdtable(files);\n\t\tif (fd >= fdt->max_fds)\n\t\t\tbreak;\n\t\tset = fdt->close_on_exec[i];\n\t\tif (!set)\n\t\t\tcontinue;\n\t\tfdt->close_on_exec[i] = 0;\n\t\tfor ( ; set ; fd++, set >>= 1) {\n\t\t\tstruct file *file;\n\t\t\tif (!(set & 1))\n\t\t\t\tcontinue;\n\t\t\tfile = fdt->fd[fd];\n\t\t\tif (!file)\n\t\t\t\tcontinue;\n\t\t\trcu_assign_pointer(fdt->fd[fd], NULL);\n\t\t\t__put_unused_fd(files, fd);\n\t\t\tspin_unlock(&files->file_lock);\n\t\t\tfilp_close(file, files);\n\t\t\tcond_resched();\n\t\t\tspin_lock(&files->file_lock);\n\t\t}\n\n\t}\n\tspin_unlock(&files->file_lock);\n}\n\nstatic struct file *__fget(unsigned int fd, fmode_t mask, unsigned int refs)\n{\n\tstruct files_struct *files = current->files;\n\tstruct file *file;\n\n\trcu_read_lock();\nloop:\n\tfile = fcheck_files(files, fd);\n\tif (file) {\n\t\t/* File object ref couldn't be taken.\n\t\t * dup2() atomicity guarantee is the reason\n\t\t * we loop to catch the new file (or NULL pointer)\n\t\t */\n\t\tif (file->f_mode & mask)\n\t\t\tfile = NULL;\n\t\telse if (!get_file_rcu_many(file, refs))\n\t\t\tgoto loop;\n\t\telse if (__fcheck_files(files, fd) != file) {\n\t\t\tfput_many(file, refs);\n\t\t\tgoto loop;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn file;\n}\n\nstruct file *fget_many(unsigned int fd, unsigned int refs)\n{\n\treturn __fget(fd, FMODE_PATH, refs);\n}\n\nstruct file *fget(unsigned int fd)\n{\n\treturn __fget(fd, FMODE_PATH, 1);\n}\nEXPORT_SYMBOL(fget);\n\nstruct file *fget_raw(unsigned int fd)\n{\n\treturn __fget(fd, 0, 1);\n}\nEXPORT_SYMBOL(fget_raw);\n\n/*\n * Lightweight file lookup - no refcnt increment if fd table isn't shared.\n *\n * You can use this instead of fget if you satisfy all of the following\n * conditions:\n * 1) You must call fput_light before exiting the syscall and returning control\n *    to userspace (i.e. you cannot remember the returned struct file * after\n *    returning to userspace).\n * 2) You must not call filp_close on the returned struct file * in between\n *    calls to fget_light and fput_light.\n * 3) You must not clone the current task in between the calls to fget_light\n *    and fput_light.\n *\n * The fput_needed flag returned by fget_light should be passed to the\n * corresponding fput_light.\n */\nstatic unsigned long __fget_light(unsigned int fd, fmode_t mask)\n{\n\tstruct files_struct *files = current->files;\n\tstruct file *file;\n\n\tif (atomic_read(&files->count) == 1) {\n\t\tfile = __fcheck_files(files, fd);\n\t\tif (!file || unlikely(file->f_mode & mask))\n\t\t\treturn 0;\n\t\treturn (unsigned long)file;\n\t} else {\n\t\tfile = __fget(fd, mask, 1);\n\t\tif (!file)\n\t\t\treturn 0;\n\t\treturn FDPUT_FPUT | (unsigned long)file;\n\t}\n}\nunsigned long __fdget(unsigned int fd)\n{\n\treturn __fget_light(fd, FMODE_PATH);\n}\nEXPORT_SYMBOL(__fdget);\n\nunsigned long __fdget_raw(unsigned int fd)\n{\n\treturn __fget_light(fd, 0);\n}\n\nunsigned long __fdget_pos(unsigned int fd)\n{\n\tunsigned long v = __fdget(fd);\n\tstruct file *file = (struct file *)(v & ~3);\n\n\tif (file && (file->f_mode & FMODE_ATOMIC_POS)) {\n\t\tif (file_count(file) > 1) {\n\t\t\tv |= FDPUT_POS_UNLOCK;\n\t\t\tmutex_lock(&file->f_pos_lock);\n\t\t}\n\t}\n\treturn v;\n}\n\nvoid __f_unlock_pos(struct file *f)\n{\n\tmutex_unlock(&f->f_pos_lock);\n}\n\n/*\n * We only lock f_pos if we have threads or if the file might be\n * shared with another process. In both cases we'll have an elevated\n * file count (done either by fdget() or by fork()).\n */\n\nvoid set_close_on_exec(unsigned int fd, int flag)\n{\n\tstruct files_struct *files = current->files;\n\tstruct fdtable *fdt;\n\tspin_lock(&files->file_lock);\n\tfdt = files_fdtable(files);\n\tif (flag)\n\t\t__set_close_on_exec(fd, fdt);\n\telse\n\t\t__clear_close_on_exec(fd, fdt);\n\tspin_unlock(&files->file_lock);\n}\n\nbool get_close_on_exec(unsigned int fd)\n{\n\tstruct files_struct *files = current->files;\n\tstruct fdtable *fdt;\n\tbool res;\n\trcu_read_lock();\n\tfdt = files_fdtable(files);\n\tres = close_on_exec(fd, fdt);\n\trcu_read_unlock();\n\treturn res;\n}\n\nstatic int do_dup2(struct files_struct *files,\n\tstruct file *file, unsigned fd, unsigned flags)\n__releases(&files->file_lock)\n{\n\tstruct file *tofree;\n\tstruct fdtable *fdt;\n\n\t/*\n\t * We need to detect attempts to do dup2() over allocated but still\n\t * not finished descriptor.  NB: OpenBSD avoids that at the price of\n\t * extra work in their equivalent of fget() - they insert struct\n\t * file immediately after grabbing descriptor, mark it larval if\n\t * more work (e.g. actual opening) is needed and make sure that\n\t * fget() treats larval files as absent.  Potentially interesting,\n\t * but while extra work in fget() is trivial, locking implications\n\t * and amount of surgery on open()-related paths in VFS are not.\n\t * FreeBSD fails with -EBADF in the same situation, NetBSD \"solution\"\n\t * deadlocks in rather amusing ways, AFAICS.  All of that is out of\n\t * scope of POSIX or SUS, since neither considers shared descriptor\n\t * tables and this condition does not arise without those.\n\t */\n\tfdt = files_fdtable(files);\n\ttofree = fdt->fd[fd];\n\tif (!tofree && fd_is_open(fd, fdt))\n\t\tgoto Ebusy;\n\tget_file(file);\n\trcu_assign_pointer(fdt->fd[fd], file);\n\t__set_open_fd(fd, fdt);\n\tif (flags & O_CLOEXEC)\n\t\t__set_close_on_exec(fd, fdt);\n\telse\n\t\t__clear_close_on_exec(fd, fdt);\n\tspin_unlock(&files->file_lock);\n\n\tif (tofree)\n\t\tfilp_close(tofree, files);\n\n\treturn fd;\n\nEbusy:\n\tspin_unlock(&files->file_lock);\n\treturn -EBUSY;\n}\n\nint replace_fd(unsigned fd, struct file *file, unsigned flags)\n{\n\tint err;\n\tstruct files_struct *files = current->files;\n\n\tif (!file)\n\t\treturn __close_fd(files, fd);\n\n\tif (fd >= rlimit(RLIMIT_NOFILE))\n\t\treturn -EBADF;\n\n\tspin_lock(&files->file_lock);\n\terr = expand_files(files, fd);\n\tif (unlikely(err < 0))\n\t\tgoto out_unlock;\n\treturn do_dup2(files, file, fd, flags);\n\nout_unlock:\n\tspin_unlock(&files->file_lock);\n\treturn err;\n}\n\nstatic int ksys_dup3(unsigned int oldfd, unsigned int newfd, int flags)\n{\n\tint err = -EBADF;\n\tstruct file *file;\n\tstruct files_struct *files = current->files;\n\n\tif ((flags & ~O_CLOEXEC) != 0)\n\t\treturn -EINVAL;\n\n\tif (unlikely(oldfd == newfd))\n\t\treturn -EINVAL;\n\n\tif (newfd >= rlimit(RLIMIT_NOFILE))\n\t\treturn -EBADF;\n\n\tspin_lock(&files->file_lock);\n\terr = expand_files(files, newfd);\n\tfile = fcheck(oldfd);\n\tif (unlikely(!file))\n\t\tgoto Ebadf;\n\tif (unlikely(err < 0)) {\n\t\tif (err == -EMFILE)\n\t\t\tgoto Ebadf;\n\t\tgoto out_unlock;\n\t}\n\treturn do_dup2(files, file, newfd, flags);\n\nEbadf:\n\terr = -EBADF;\nout_unlock:\n\tspin_unlock(&files->file_lock);\n\treturn err;\n}\n\nSYSCALL_DEFINE3(dup3, unsigned int, oldfd, unsigned int, newfd, int, flags)\n{\n\treturn ksys_dup3(oldfd, newfd, flags);\n}\n\nSYSCALL_DEFINE2(dup2, unsigned int, oldfd, unsigned int, newfd)\n{\n\tif (unlikely(newfd == oldfd)) { /* corner case */\n\t\tstruct files_struct *files = current->files;\n\t\tint retval = oldfd;\n\n\t\trcu_read_lock();\n\t\tif (!fcheck_files(files, oldfd))\n\t\t\tretval = -EBADF;\n\t\trcu_read_unlock();\n\t\treturn retval;\n\t}\n\treturn ksys_dup3(oldfd, newfd, 0);\n}\n\nint ksys_dup(unsigned int fildes)\n{\n\tint ret = -EBADF;\n\tstruct file *file = fget_raw(fildes);\n\n\tif (file) {\n\t\tret = get_unused_fd_flags(0);\n\t\tif (ret >= 0)\n\t\t\tfd_install(ret, file);\n\t\telse\n\t\t\tfput(file);\n\t}\n\treturn ret;\n}\n\nSYSCALL_DEFINE1(dup, unsigned int, fildes)\n{\n\treturn ksys_dup(fildes);\n}\n\nint f_dupfd(unsigned int from, struct file *file, unsigned flags)\n{\n\tint err;\n\tif (from >= rlimit(RLIMIT_NOFILE))\n\t\treturn -EINVAL;\n\terr = alloc_fd(from, flags);\n\tif (err >= 0) {\n\t\tget_file(file);\n\t\tfd_install(err, file);\n\t}\n\treturn err;\n}\n\nint iterate_fd(struct files_struct *files, unsigned n,\n\t\tint (*f)(const void *, struct file *, unsigned),\n\t\tconst void *p)\n{\n\tstruct fdtable *fdt;\n\tint res = 0;\n\tif (!files)\n\t\treturn 0;\n\tspin_lock(&files->file_lock);\n\tfor (fdt = files_fdtable(files); n < fdt->max_fds; n++) {\n\t\tstruct file *file;\n\t\tfile = rcu_dereference_check_fdtable(files, fdt->fd[n]);\n\t\tif (!file)\n\t\t\tcontinue;\n\t\tres = f(p, file, n);\n\t\tif (res)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&files->file_lock);\n\treturn res;\n}\nEXPORT_SYMBOL(iterate_fd);\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/* binder.c\n *\n * Android IPC Subsystem\n *\n * Copyright (C) 2007-2008 Google, Inc.\n */\n\n/*\n * Locking overview\n *\n * There are 3 main spinlocks which must be acquired in the\n * order shown:\n *\n * 1) proc->outer_lock : protects binder_ref\n *    binder_proc_lock() and binder_proc_unlock() are\n *    used to acq/rel.\n * 2) node->lock : protects most fields of binder_node.\n *    binder_node_lock() and binder_node_unlock() are\n *    used to acq/rel\n * 3) proc->inner_lock : protects the thread and node lists\n *    (proc->threads, proc->waiting_threads, proc->nodes)\n *    and all todo lists associated with the binder_proc\n *    (proc->todo, thread->todo, proc->delivered_death and\n *    node->async_todo), as well as thread->transaction_stack\n *    binder_inner_proc_lock() and binder_inner_proc_unlock()\n *    are used to acq/rel\n *\n * Any lock under procA must never be nested under any lock at the same\n * level or below on procB.\n *\n * Functions that require a lock held on entry indicate which lock\n * in the suffix of the function name:\n *\n * foo_olocked() : requires node->outer_lock\n * foo_nlocked() : requires node->lock\n * foo_ilocked() : requires proc->inner_lock\n * foo_oilocked(): requires proc->outer_lock and proc->inner_lock\n * foo_nilocked(): requires node->lock and proc->inner_lock\n * ...\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/freezer.h>\n#include <linux/fs.h>\n#include <linux/list.h>\n#include <linux/miscdevice.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/nsproxy.h>\n#include <linux/poll.h>\n#include <linux/debugfs.h>\n#include <linux/rbtree.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/seq_file.h>\n#include <linux/string.h>\n#include <linux/uaccess.h>\n#include <linux/pid_namespace.h>\n#include <linux/security.h>\n#include <linux/spinlock.h>\n#include <linux/ratelimit.h>\n#include <linux/syscalls.h>\n#include <linux/task_work.h>\n\n#include <uapi/linux/android/binder.h>\n#include <uapi/linux/android/binderfs.h>\n\n#include <asm/cacheflush.h>\n\n#include \"binder_alloc.h\"\n#include \"binder_internal.h\"\n#include \"binder_trace.h\"\n\nstatic HLIST_HEAD(binder_deferred_list);\nstatic DEFINE_MUTEX(binder_deferred_lock);\n\nstatic HLIST_HEAD(binder_devices);\nstatic HLIST_HEAD(binder_procs);\nstatic DEFINE_MUTEX(binder_procs_lock);\n\nstatic HLIST_HEAD(binder_dead_nodes);\nstatic DEFINE_SPINLOCK(binder_dead_nodes_lock);\n\nstatic struct dentry *binder_debugfs_dir_entry_root;\nstatic struct dentry *binder_debugfs_dir_entry_proc;\nstatic atomic_t binder_last_id;\n\nstatic int proc_show(struct seq_file *m, void *unused);\nDEFINE_SHOW_ATTRIBUTE(proc);\n\n/* This is only defined in include/asm-arm/sizes.h */\n#ifndef SZ_1K\n#define SZ_1K                               0x400\n#endif\n\n#define FORBIDDEN_MMAP_FLAGS                (VM_WRITE)\n\nenum {\n\tBINDER_DEBUG_USER_ERROR             = 1U << 0,\n\tBINDER_DEBUG_FAILED_TRANSACTION     = 1U << 1,\n\tBINDER_DEBUG_DEAD_TRANSACTION       = 1U << 2,\n\tBINDER_DEBUG_OPEN_CLOSE             = 1U << 3,\n\tBINDER_DEBUG_DEAD_BINDER            = 1U << 4,\n\tBINDER_DEBUG_DEATH_NOTIFICATION     = 1U << 5,\n\tBINDER_DEBUG_READ_WRITE             = 1U << 6,\n\tBINDER_DEBUG_USER_REFS              = 1U << 7,\n\tBINDER_DEBUG_THREADS                = 1U << 8,\n\tBINDER_DEBUG_TRANSACTION            = 1U << 9,\n\tBINDER_DEBUG_TRANSACTION_COMPLETE   = 1U << 10,\n\tBINDER_DEBUG_FREE_BUFFER            = 1U << 11,\n\tBINDER_DEBUG_INTERNAL_REFS          = 1U << 12,\n\tBINDER_DEBUG_PRIORITY_CAP           = 1U << 13,\n\tBINDER_DEBUG_SPINLOCKS              = 1U << 14,\n};\nstatic uint32_t binder_debug_mask = BINDER_DEBUG_USER_ERROR |\n\tBINDER_DEBUG_FAILED_TRANSACTION | BINDER_DEBUG_DEAD_TRANSACTION;\nmodule_param_named(debug_mask, binder_debug_mask, uint, 0644);\n\nchar *binder_devices_param = CONFIG_ANDROID_BINDER_DEVICES;\nmodule_param_named(devices, binder_devices_param, charp, 0444);\n\nstatic DECLARE_WAIT_QUEUE_HEAD(binder_user_error_wait);\nstatic int binder_stop_on_user_error;\n\nstatic int binder_set_stop_on_user_error(const char *val,\n\t\t\t\t\t const struct kernel_param *kp)\n{\n\tint ret;\n\n\tret = param_set_int(val, kp);\n\tif (binder_stop_on_user_error < 2)\n\t\twake_up(&binder_user_error_wait);\n\treturn ret;\n}\nmodule_param_call(stop_on_user_error, binder_set_stop_on_user_error,\n\tparam_get_int, &binder_stop_on_user_error, 0644);\n\n#define binder_debug(mask, x...) \\\n\tdo { \\\n\t\tif (binder_debug_mask & mask) \\\n\t\t\tpr_info_ratelimited(x); \\\n\t} while (0)\n\n#define binder_user_error(x...) \\\n\tdo { \\\n\t\tif (binder_debug_mask & BINDER_DEBUG_USER_ERROR) \\\n\t\t\tpr_info_ratelimited(x); \\\n\t\tif (binder_stop_on_user_error) \\\n\t\t\tbinder_stop_on_user_error = 2; \\\n\t} while (0)\n\n#define to_flat_binder_object(hdr) \\\n\tcontainer_of(hdr, struct flat_binder_object, hdr)\n\n#define to_binder_fd_object(hdr) container_of(hdr, struct binder_fd_object, hdr)\n\n#define to_binder_buffer_object(hdr) \\\n\tcontainer_of(hdr, struct binder_buffer_object, hdr)\n\n#define to_binder_fd_array_object(hdr) \\\n\tcontainer_of(hdr, struct binder_fd_array_object, hdr)\n\nenum binder_stat_types {\n\tBINDER_STAT_PROC,\n\tBINDER_STAT_THREAD,\n\tBINDER_STAT_NODE,\n\tBINDER_STAT_REF,\n\tBINDER_STAT_DEATH,\n\tBINDER_STAT_TRANSACTION,\n\tBINDER_STAT_TRANSACTION_COMPLETE,\n\tBINDER_STAT_COUNT\n};\n\nstruct binder_stats {\n\tatomic_t br[_IOC_NR(BR_FAILED_REPLY) + 1];\n\tatomic_t bc[_IOC_NR(BC_REPLY_SG) + 1];\n\tatomic_t obj_created[BINDER_STAT_COUNT];\n\tatomic_t obj_deleted[BINDER_STAT_COUNT];\n};\n\nstatic struct binder_stats binder_stats;\n\nstatic inline void binder_stats_deleted(enum binder_stat_types type)\n{\n\tatomic_inc(&binder_stats.obj_deleted[type]);\n}\n\nstatic inline void binder_stats_created(enum binder_stat_types type)\n{\n\tatomic_inc(&binder_stats.obj_created[type]);\n}\n\nstruct binder_transaction_log binder_transaction_log;\nstruct binder_transaction_log binder_transaction_log_failed;\n\nstatic struct binder_transaction_log_entry *binder_transaction_log_add(\n\tstruct binder_transaction_log *log)\n{\n\tstruct binder_transaction_log_entry *e;\n\tunsigned int cur = atomic_inc_return(&log->cur);\n\n\tif (cur >= ARRAY_SIZE(log->entry))\n\t\tlog->full = true;\n\te = &log->entry[cur % ARRAY_SIZE(log->entry)];\n\tWRITE_ONCE(e->debug_id_done, 0);\n\t/*\n\t * write-barrier to synchronize access to e->debug_id_done.\n\t * We make sure the initialized 0 value is seen before\n\t * memset() other fields are zeroed by memset.\n\t */\n\tsmp_wmb();\n\tmemset(e, 0, sizeof(*e));\n\treturn e;\n}\n\n/**\n * struct binder_work - work enqueued on a worklist\n * @entry:             node enqueued on list\n * @type:              type of work to be performed\n *\n * There are separate work lists for proc, thread, and node (async).\n */\nstruct binder_work {\n\tstruct list_head entry;\n\n\tenum binder_work_type {\n\t\tBINDER_WORK_TRANSACTION = 1,\n\t\tBINDER_WORK_TRANSACTION_COMPLETE,\n\t\tBINDER_WORK_RETURN_ERROR,\n\t\tBINDER_WORK_NODE,\n\t\tBINDER_WORK_DEAD_BINDER,\n\t\tBINDER_WORK_DEAD_BINDER_AND_CLEAR,\n\t\tBINDER_WORK_CLEAR_DEATH_NOTIFICATION,\n\t} type;\n};\n\nstruct binder_error {\n\tstruct binder_work work;\n\tuint32_t cmd;\n};\n\n/**\n * struct binder_node - binder node bookkeeping\n * @debug_id:             unique ID for debugging\n *                        (invariant after initialized)\n * @lock:                 lock for node fields\n * @work:                 worklist element for node work\n *                        (protected by @proc->inner_lock)\n * @rb_node:              element for proc->nodes tree\n *                        (protected by @proc->inner_lock)\n * @dead_node:            element for binder_dead_nodes list\n *                        (protected by binder_dead_nodes_lock)\n * @proc:                 binder_proc that owns this node\n *                        (invariant after initialized)\n * @refs:                 list of references on this node\n *                        (protected by @lock)\n * @internal_strong_refs: used to take strong references when\n *                        initiating a transaction\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @local_weak_refs:      weak user refs from local process\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @local_strong_refs:    strong user refs from local process\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @tmp_refs:             temporary kernel refs\n *                        (protected by @proc->inner_lock while @proc\n *                        is valid, and by binder_dead_nodes_lock\n *                        if @proc is NULL. During inc/dec and node release\n *                        it is also protected by @lock to provide safety\n *                        as the node dies and @proc becomes NULL)\n * @ptr:                  userspace pointer for node\n *                        (invariant, no lock needed)\n * @cookie:               userspace cookie for node\n *                        (invariant, no lock needed)\n * @has_strong_ref:       userspace notified of strong ref\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @pending_strong_ref:   userspace has acked notification of strong ref\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @has_weak_ref:         userspace notified of weak ref\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @pending_weak_ref:     userspace has acked notification of weak ref\n *                        (protected by @proc->inner_lock if @proc\n *                        and by @lock)\n * @has_async_transaction: async transaction to node in progress\n *                        (protected by @lock)\n * @accept_fds:           file descriptor operations supported for node\n *                        (invariant after initialized)\n * @min_priority:         minimum scheduling priority\n *                        (invariant after initialized)\n * @txn_security_ctx:     require sender's security context\n *                        (invariant after initialized)\n * @async_todo:           list of async work items\n *                        (protected by @proc->inner_lock)\n *\n * Bookkeeping structure for binder nodes.\n */\nstruct binder_node {\n\tint debug_id;\n\tspinlock_t lock;\n\tstruct binder_work work;\n\tunion {\n\t\tstruct rb_node rb_node;\n\t\tstruct hlist_node dead_node;\n\t};\n\tstruct binder_proc *proc;\n\tstruct hlist_head refs;\n\tint internal_strong_refs;\n\tint local_weak_refs;\n\tint local_strong_refs;\n\tint tmp_refs;\n\tbinder_uintptr_t ptr;\n\tbinder_uintptr_t cookie;\n\tstruct {\n\t\t/*\n\t\t * bitfield elements protected by\n\t\t * proc inner_lock\n\t\t */\n\t\tu8 has_strong_ref:1;\n\t\tu8 pending_strong_ref:1;\n\t\tu8 has_weak_ref:1;\n\t\tu8 pending_weak_ref:1;\n\t};\n\tstruct {\n\t\t/*\n\t\t * invariant after initialization\n\t\t */\n\t\tu8 accept_fds:1;\n\t\tu8 txn_security_ctx:1;\n\t\tu8 min_priority;\n\t};\n\tbool has_async_transaction;\n\tstruct list_head async_todo;\n};\n\nstruct binder_ref_death {\n\t/**\n\t * @work: worklist element for death notifications\n\t *        (protected by inner_lock of the proc that\n\t *        this ref belongs to)\n\t */\n\tstruct binder_work work;\n\tbinder_uintptr_t cookie;\n};\n\n/**\n * struct binder_ref_data - binder_ref counts and id\n * @debug_id:        unique ID for the ref\n * @desc:            unique userspace handle for ref\n * @strong:          strong ref count (debugging only if not locked)\n * @weak:            weak ref count (debugging only if not locked)\n *\n * Structure to hold ref count and ref id information. Since\n * the actual ref can only be accessed with a lock, this structure\n * is used to return information about the ref to callers of\n * ref inc/dec functions.\n */\nstruct binder_ref_data {\n\tint debug_id;\n\tuint32_t desc;\n\tint strong;\n\tint weak;\n};\n\n/**\n * struct binder_ref - struct to track references on nodes\n * @data:        binder_ref_data containing id, handle, and current refcounts\n * @rb_node_desc: node for lookup by @data.desc in proc's rb_tree\n * @rb_node_node: node for lookup by @node in proc's rb_tree\n * @node_entry:  list entry for node->refs list in target node\n *               (protected by @node->lock)\n * @proc:        binder_proc containing ref\n * @node:        binder_node of target node. When cleaning up a\n *               ref for deletion in binder_cleanup_ref, a non-NULL\n *               @node indicates the node must be freed\n * @death:       pointer to death notification (ref_death) if requested\n *               (protected by @node->lock)\n *\n * Structure to track references from procA to target node (on procB). This\n * structure is unsafe to access without holding @proc->outer_lock.\n */\nstruct binder_ref {\n\t/* Lookups needed: */\n\t/*   node + proc => ref (transaction) */\n\t/*   desc + proc => ref (transaction, inc/dec ref) */\n\t/*   node => refs + procs (proc exit) */\n\tstruct binder_ref_data data;\n\tstruct rb_node rb_node_desc;\n\tstruct rb_node rb_node_node;\n\tstruct hlist_node node_entry;\n\tstruct binder_proc *proc;\n\tstruct binder_node *node;\n\tstruct binder_ref_death *death;\n};\n\nenum binder_deferred_state {\n\tBINDER_DEFERRED_FLUSH        = 0x01,\n\tBINDER_DEFERRED_RELEASE      = 0x02,\n};\n\n/**\n * struct binder_proc - binder process bookkeeping\n * @proc_node:            element for binder_procs list\n * @threads:              rbtree of binder_threads in this proc\n *                        (protected by @inner_lock)\n * @nodes:                rbtree of binder nodes associated with\n *                        this proc ordered by node->ptr\n *                        (protected by @inner_lock)\n * @refs_by_desc:         rbtree of refs ordered by ref->desc\n *                        (protected by @outer_lock)\n * @refs_by_node:         rbtree of refs ordered by ref->node\n *                        (protected by @outer_lock)\n * @waiting_threads:      threads currently waiting for proc work\n *                        (protected by @inner_lock)\n * @pid                   PID of group_leader of process\n *                        (invariant after initialized)\n * @tsk                   task_struct for group_leader of process\n *                        (invariant after initialized)\n * @cred                  struct cred associated with the `struct file`\n *                        in binder_open()\n *                        (invariant after initialized)\n * @deferred_work_node:   element for binder_deferred_list\n *                        (protected by binder_deferred_lock)\n * @deferred_work:        bitmap of deferred work to perform\n *                        (protected by binder_deferred_lock)\n * @is_dead:              process is dead and awaiting free\n *                        when outstanding transactions are cleaned up\n *                        (protected by @inner_lock)\n * @todo:                 list of work for this process\n *                        (protected by @inner_lock)\n * @stats:                per-process binder statistics\n *                        (atomics, no lock needed)\n * @delivered_death:      list of delivered death notification\n *                        (protected by @inner_lock)\n * @max_threads:          cap on number of binder threads\n *                        (protected by @inner_lock)\n * @requested_threads:    number of binder threads requested but not\n *                        yet started. In current implementation, can\n *                        only be 0 or 1.\n *                        (protected by @inner_lock)\n * @requested_threads_started: number binder threads started\n *                        (protected by @inner_lock)\n * @tmp_ref:              temporary reference to indicate proc is in use\n *                        (protected by @inner_lock)\n * @default_priority:     default scheduler priority\n *                        (invariant after initialized)\n * @debugfs_entry:        debugfs node\n * @alloc:                binder allocator bookkeeping\n * @context:              binder_context for this proc\n *                        (invariant after initialized)\n * @inner_lock:           can nest under outer_lock and/or node lock\n * @outer_lock:           no nesting under innor or node lock\n *                        Lock order: 1) outer, 2) node, 3) inner\n * @binderfs_entry:       process-specific binderfs log file\n *\n * Bookkeeping structure for binder processes\n */\nstruct binder_proc {\n\tstruct hlist_node proc_node;\n\tstruct rb_root threads;\n\tstruct rb_root nodes;\n\tstruct rb_root refs_by_desc;\n\tstruct rb_root refs_by_node;\n\tstruct list_head waiting_threads;\n\tint pid;\n\tstruct task_struct *tsk;\n\tconst struct cred *cred;\n\tstruct hlist_node deferred_work_node;\n\tint deferred_work;\n\tbool is_dead;\n\n\tstruct list_head todo;\n\tstruct binder_stats stats;\n\tstruct list_head delivered_death;\n\tint max_threads;\n\tint requested_threads;\n\tint requested_threads_started;\n\tint tmp_ref;\n\tlong default_priority;\n\tstruct dentry *debugfs_entry;\n\tstruct binder_alloc alloc;\n\tstruct binder_context *context;\n\tspinlock_t inner_lock;\n\tspinlock_t outer_lock;\n\tstruct dentry *binderfs_entry;\n};\n\nenum {\n\tBINDER_LOOPER_STATE_REGISTERED  = 0x01,\n\tBINDER_LOOPER_STATE_ENTERED     = 0x02,\n\tBINDER_LOOPER_STATE_EXITED      = 0x04,\n\tBINDER_LOOPER_STATE_INVALID     = 0x08,\n\tBINDER_LOOPER_STATE_WAITING     = 0x10,\n\tBINDER_LOOPER_STATE_POLL        = 0x20,\n};\n\n/**\n * struct binder_thread - binder thread bookkeeping\n * @proc:                 binder process for this thread\n *                        (invariant after initialization)\n * @rb_node:              element for proc->threads rbtree\n *                        (protected by @proc->inner_lock)\n * @waiting_thread_node:  element for @proc->waiting_threads list\n *                        (protected by @proc->inner_lock)\n * @pid:                  PID for this thread\n *                        (invariant after initialization)\n * @looper:               bitmap of looping state\n *                        (only accessed by this thread)\n * @looper_needs_return:  looping thread needs to exit driver\n *                        (no lock needed)\n * @transaction_stack:    stack of in-progress transactions for this thread\n *                        (protected by @proc->inner_lock)\n * @todo:                 list of work to do for this thread\n *                        (protected by @proc->inner_lock)\n * @process_todo:         whether work in @todo should be processed\n *                        (protected by @proc->inner_lock)\n * @return_error:         transaction errors reported by this thread\n *                        (only accessed by this thread)\n * @reply_error:          transaction errors reported by target thread\n *                        (protected by @proc->inner_lock)\n * @wait:                 wait queue for thread work\n * @stats:                per-thread statistics\n *                        (atomics, no lock needed)\n * @tmp_ref:              temporary reference to indicate thread is in use\n *                        (atomic since @proc->inner_lock cannot\n *                        always be acquired)\n * @is_dead:              thread is dead and awaiting free\n *                        when outstanding transactions are cleaned up\n *                        (protected by @proc->inner_lock)\n *\n * Bookkeeping structure for binder threads.\n */\nstruct binder_thread {\n\tstruct binder_proc *proc;\n\tstruct rb_node rb_node;\n\tstruct list_head waiting_thread_node;\n\tint pid;\n\tint looper;              /* only modified by this thread */\n\tbool looper_need_return; /* can be written by other thread */\n\tstruct binder_transaction *transaction_stack;\n\tstruct list_head todo;\n\tbool process_todo;\n\tstruct binder_error return_error;\n\tstruct binder_error reply_error;\n\twait_queue_head_t wait;\n\tstruct binder_stats stats;\n\tatomic_t tmp_ref;\n\tbool is_dead;\n};\n\n/**\n * struct binder_txn_fd_fixup - transaction fd fixup list element\n * @fixup_entry:          list entry\n * @file:                 struct file to be associated with new fd\n * @offset:               offset in buffer data to this fixup\n *\n * List element for fd fixups in a transaction. Since file\n * descriptors need to be allocated in the context of the\n * target process, we pass each fd to be processed in this\n * struct.\n */\nstruct binder_txn_fd_fixup {\n\tstruct list_head fixup_entry;\n\tstruct file *file;\n\tsize_t offset;\n};\n\nstruct binder_transaction {\n\tint debug_id;\n\tstruct binder_work work;\n\tstruct binder_thread *from;\n\tstruct binder_transaction *from_parent;\n\tstruct binder_proc *to_proc;\n\tstruct binder_thread *to_thread;\n\tstruct binder_transaction *to_parent;\n\tunsigned need_reply:1;\n\t/* unsigned is_dead:1; */\t/* not used at the moment */\n\n\tstruct binder_buffer *buffer;\n\tunsigned int\tcode;\n\tunsigned int\tflags;\n\tlong\tpriority;\n\tlong\tsaved_priority;\n\tkuid_t\tsender_euid;\n\tstruct list_head fd_fixups;\n\tbinder_uintptr_t security_ctx;\n\t/**\n\t * @lock:  protects @from, @to_proc, and @to_thread\n\t *\n\t * @from, @to_proc, and @to_thread can be set to NULL\n\t * during thread teardown\n\t */\n\tspinlock_t lock;\n};\n\n/**\n * struct binder_object - union of flat binder object types\n * @hdr:   generic object header\n * @fbo:   binder object (nodes and refs)\n * @fdo:   file descriptor object\n * @bbo:   binder buffer pointer\n * @fdao:  file descriptor array\n *\n * Used for type-independent object copies\n */\nstruct binder_object {\n\tunion {\n\t\tstruct binder_object_header hdr;\n\t\tstruct flat_binder_object fbo;\n\t\tstruct binder_fd_object fdo;\n\t\tstruct binder_buffer_object bbo;\n\t\tstruct binder_fd_array_object fdao;\n\t};\n};\n\n/**\n * binder_proc_lock() - Acquire outer lock for given binder_proc\n * @proc:         struct binder_proc to acquire\n *\n * Acquires proc->outer_lock. Used to protect binder_ref\n * structures associated with the given proc.\n */\n#define binder_proc_lock(proc) _binder_proc_lock(proc, __LINE__)\nstatic void\n_binder_proc_lock(struct binder_proc *proc, int line)\n\t__acquires(&proc->outer_lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_lock(&proc->outer_lock);\n}\n\n/**\n * binder_proc_unlock() - Release spinlock for given binder_proc\n * @proc:         struct binder_proc to acquire\n *\n * Release lock acquired via binder_proc_lock()\n */\n#define binder_proc_unlock(_proc) _binder_proc_unlock(_proc, __LINE__)\nstatic void\n_binder_proc_unlock(struct binder_proc *proc, int line)\n\t__releases(&proc->outer_lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_unlock(&proc->outer_lock);\n}\n\n/**\n * binder_inner_proc_lock() - Acquire inner lock for given binder_proc\n * @proc:         struct binder_proc to acquire\n *\n * Acquires proc->inner_lock. Used to protect todo lists\n */\n#define binder_inner_proc_lock(proc) _binder_inner_proc_lock(proc, __LINE__)\nstatic void\n_binder_inner_proc_lock(struct binder_proc *proc, int line)\n\t__acquires(&proc->inner_lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_lock(&proc->inner_lock);\n}\n\n/**\n * binder_inner_proc_unlock() - Release inner lock for given binder_proc\n * @proc:         struct binder_proc to acquire\n *\n * Release lock acquired via binder_inner_proc_lock()\n */\n#define binder_inner_proc_unlock(proc) _binder_inner_proc_unlock(proc, __LINE__)\nstatic void\n_binder_inner_proc_unlock(struct binder_proc *proc, int line)\n\t__releases(&proc->inner_lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_unlock(&proc->inner_lock);\n}\n\n/**\n * binder_node_lock() - Acquire spinlock for given binder_node\n * @node:         struct binder_node to acquire\n *\n * Acquires node->lock. Used to protect binder_node fields\n */\n#define binder_node_lock(node) _binder_node_lock(node, __LINE__)\nstatic void\n_binder_node_lock(struct binder_node *node, int line)\n\t__acquires(&node->lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_lock(&node->lock);\n}\n\n/**\n * binder_node_unlock() - Release spinlock for given binder_proc\n * @node:         struct binder_node to acquire\n *\n * Release lock acquired via binder_node_lock()\n */\n#define binder_node_unlock(node) _binder_node_unlock(node, __LINE__)\nstatic void\n_binder_node_unlock(struct binder_node *node, int line)\n\t__releases(&node->lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_unlock(&node->lock);\n}\n\n/**\n * binder_node_inner_lock() - Acquire node and inner locks\n * @node:         struct binder_node to acquire\n *\n * Acquires node->lock. If node->proc also acquires\n * proc->inner_lock. Used to protect binder_node fields\n */\n#define binder_node_inner_lock(node) _binder_node_inner_lock(node, __LINE__)\nstatic void\n_binder_node_inner_lock(struct binder_node *node, int line)\n\t__acquires(&node->lock) __acquires(&node->proc->inner_lock)\n{\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tspin_lock(&node->lock);\n\tif (node->proc)\n\t\tbinder_inner_proc_lock(node->proc);\n\telse\n\t\t/* annotation for sparse */\n\t\t__acquire(&node->proc->inner_lock);\n}\n\n/**\n * binder_node_unlock() - Release node and inner locks\n * @node:         struct binder_node to acquire\n *\n * Release lock acquired via binder_node_lock()\n */\n#define binder_node_inner_unlock(node) _binder_node_inner_unlock(node, __LINE__)\nstatic void\n_binder_node_inner_unlock(struct binder_node *node, int line)\n\t__releases(&node->lock) __releases(&node->proc->inner_lock)\n{\n\tstruct binder_proc *proc = node->proc;\n\n\tbinder_debug(BINDER_DEBUG_SPINLOCKS,\n\t\t     \"%s: line=%d\\n\", __func__, line);\n\tif (proc)\n\t\tbinder_inner_proc_unlock(proc);\n\telse\n\t\t/* annotation for sparse */\n\t\t__release(&node->proc->inner_lock);\n\tspin_unlock(&node->lock);\n}\n\nstatic bool binder_worklist_empty_ilocked(struct list_head *list)\n{\n\treturn list_empty(list);\n}\n\n/**\n * binder_worklist_empty() - Check if no items on the work list\n * @proc:       binder_proc associated with list\n * @list:\tlist to check\n *\n * Return: true if there are no items on list, else false\n */\nstatic bool binder_worklist_empty(struct binder_proc *proc,\n\t\t\t\t  struct list_head *list)\n{\n\tbool ret;\n\n\tbinder_inner_proc_lock(proc);\n\tret = binder_worklist_empty_ilocked(list);\n\tbinder_inner_proc_unlock(proc);\n\treturn ret;\n}\n\n/**\n * binder_enqueue_work_ilocked() - Add an item to the work list\n * @work:         struct binder_work to add to list\n * @target_list:  list to add work to\n *\n * Adds the work to the specified list. Asserts that work\n * is not already on a list.\n *\n * Requires the proc->inner_lock to be held.\n */\nstatic void\nbinder_enqueue_work_ilocked(struct binder_work *work,\n\t\t\t   struct list_head *target_list)\n{\n\tBUG_ON(target_list == NULL);\n\tBUG_ON(work->entry.next && !list_empty(&work->entry));\n\tlist_add_tail(&work->entry, target_list);\n}\n\n/**\n * binder_enqueue_deferred_thread_work_ilocked() - Add deferred thread work\n * @thread:       thread to queue work to\n * @work:         struct binder_work to add to list\n *\n * Adds the work to the todo list of the thread. Doesn't set the process_todo\n * flag, which means that (if it wasn't already set) the thread will go to\n * sleep without handling this work when it calls read.\n *\n * Requires the proc->inner_lock to be held.\n */\nstatic void\nbinder_enqueue_deferred_thread_work_ilocked(struct binder_thread *thread,\n\t\t\t\t\t    struct binder_work *work)\n{\n\tWARN_ON(!list_empty(&thread->waiting_thread_node));\n\tbinder_enqueue_work_ilocked(work, &thread->todo);\n}\n\n/**\n * binder_enqueue_thread_work_ilocked() - Add an item to the thread work list\n * @thread:       thread to queue work to\n * @work:         struct binder_work to add to list\n *\n * Adds the work to the todo list of the thread, and enables processing\n * of the todo queue.\n *\n * Requires the proc->inner_lock to be held.\n */\nstatic void\nbinder_enqueue_thread_work_ilocked(struct binder_thread *thread,\n\t\t\t\t   struct binder_work *work)\n{\n\tWARN_ON(!list_empty(&thread->waiting_thread_node));\n\tbinder_enqueue_work_ilocked(work, &thread->todo);\n\tthread->process_todo = true;\n}\n\n/**\n * binder_enqueue_thread_work() - Add an item to the thread work list\n * @thread:       thread to queue work to\n * @work:         struct binder_work to add to list\n *\n * Adds the work to the todo list of the thread, and enables processing\n * of the todo queue.\n */\nstatic void\nbinder_enqueue_thread_work(struct binder_thread *thread,\n\t\t\t   struct binder_work *work)\n{\n\tbinder_inner_proc_lock(thread->proc);\n\tbinder_enqueue_thread_work_ilocked(thread, work);\n\tbinder_inner_proc_unlock(thread->proc);\n}\n\nstatic void\nbinder_dequeue_work_ilocked(struct binder_work *work)\n{\n\tlist_del_init(&work->entry);\n}\n\n/**\n * binder_dequeue_work() - Removes an item from the work list\n * @proc:         binder_proc associated with list\n * @work:         struct binder_work to remove from list\n *\n * Removes the specified work item from whatever list it is on.\n * Can safely be called if work is not on any list.\n */\nstatic void\nbinder_dequeue_work(struct binder_proc *proc, struct binder_work *work)\n{\n\tbinder_inner_proc_lock(proc);\n\tbinder_dequeue_work_ilocked(work);\n\tbinder_inner_proc_unlock(proc);\n}\n\nstatic struct binder_work *binder_dequeue_work_head_ilocked(\n\t\t\t\t\tstruct list_head *list)\n{\n\tstruct binder_work *w;\n\n\tw = list_first_entry_or_null(list, struct binder_work, entry);\n\tif (w)\n\t\tlist_del_init(&w->entry);\n\treturn w;\n}\n\nstatic void\nbinder_defer_work(struct binder_proc *proc, enum binder_deferred_state defer);\nstatic void binder_free_thread(struct binder_thread *thread);\nstatic void binder_free_proc(struct binder_proc *proc);\nstatic void binder_inc_node_tmpref_ilocked(struct binder_node *node);\n\nstatic bool binder_has_work_ilocked(struct binder_thread *thread,\n\t\t\t\t    bool do_proc_work)\n{\n\treturn thread->process_todo ||\n\t\tthread->looper_need_return ||\n\t\t(do_proc_work &&\n\t\t !binder_worklist_empty_ilocked(&thread->proc->todo));\n}\n\nstatic bool binder_has_work(struct binder_thread *thread, bool do_proc_work)\n{\n\tbool has_work;\n\n\tbinder_inner_proc_lock(thread->proc);\n\thas_work = binder_has_work_ilocked(thread, do_proc_work);\n\tbinder_inner_proc_unlock(thread->proc);\n\n\treturn has_work;\n}\n\nstatic bool binder_available_for_proc_work_ilocked(struct binder_thread *thread)\n{\n\treturn !thread->transaction_stack &&\n\t\tbinder_worklist_empty_ilocked(&thread->todo) &&\n\t\t(thread->looper & (BINDER_LOOPER_STATE_ENTERED |\n\t\t\t\t   BINDER_LOOPER_STATE_REGISTERED));\n}\n\nstatic void binder_wakeup_poll_threads_ilocked(struct binder_proc *proc,\n\t\t\t\t\t       bool sync)\n{\n\tstruct rb_node *n;\n\tstruct binder_thread *thread;\n\n\tfor (n = rb_first(&proc->threads); n != NULL; n = rb_next(n)) {\n\t\tthread = rb_entry(n, struct binder_thread, rb_node);\n\t\tif (thread->looper & BINDER_LOOPER_STATE_POLL &&\n\t\t    binder_available_for_proc_work_ilocked(thread)) {\n\t\t\tif (sync)\n\t\t\t\twake_up_interruptible_sync(&thread->wait);\n\t\t\telse\n\t\t\t\twake_up_interruptible(&thread->wait);\n\t\t}\n\t}\n}\n\n/**\n * binder_select_thread_ilocked() - selects a thread for doing proc work.\n * @proc:\tprocess to select a thread from\n *\n * Note that calling this function moves the thread off the waiting_threads\n * list, so it can only be woken up by the caller of this function, or a\n * signal. Therefore, callers *should* always wake up the thread this function\n * returns.\n *\n * Return:\tIf there's a thread currently waiting for process work,\n *\t\treturns that thread. Otherwise returns NULL.\n */\nstatic struct binder_thread *\nbinder_select_thread_ilocked(struct binder_proc *proc)\n{\n\tstruct binder_thread *thread;\n\n\tassert_spin_locked(&proc->inner_lock);\n\tthread = list_first_entry_or_null(&proc->waiting_threads,\n\t\t\t\t\t  struct binder_thread,\n\t\t\t\t\t  waiting_thread_node);\n\n\tif (thread)\n\t\tlist_del_init(&thread->waiting_thread_node);\n\n\treturn thread;\n}\n\n/**\n * binder_wakeup_thread_ilocked() - wakes up a thread for doing proc work.\n * @proc:\tprocess to wake up a thread in\n * @thread:\tspecific thread to wake-up (may be NULL)\n * @sync:\twhether to do a synchronous wake-up\n *\n * This function wakes up a thread in the @proc process.\n * The caller may provide a specific thread to wake-up in\n * the @thread parameter. If @thread is NULL, this function\n * will wake up threads that have called poll().\n *\n * Note that for this function to work as expected, callers\n * should first call binder_select_thread() to find a thread\n * to handle the work (if they don't have a thread already),\n * and pass the result into the @thread parameter.\n */\nstatic void binder_wakeup_thread_ilocked(struct binder_proc *proc,\n\t\t\t\t\t struct binder_thread *thread,\n\t\t\t\t\t bool sync)\n{\n\tassert_spin_locked(&proc->inner_lock);\n\n\tif (thread) {\n\t\tif (sync)\n\t\t\twake_up_interruptible_sync(&thread->wait);\n\t\telse\n\t\t\twake_up_interruptible(&thread->wait);\n\t\treturn;\n\t}\n\n\t/* Didn't find a thread waiting for proc work; this can happen\n\t * in two scenarios:\n\t * 1. All threads are busy handling transactions\n\t *    In that case, one of those threads should call back into\n\t *    the kernel driver soon and pick up this work.\n\t * 2. Threads are using the (e)poll interface, in which case\n\t *    they may be blocked on the waitqueue without having been\n\t *    added to waiting_threads. For this case, we just iterate\n\t *    over all threads not handling transaction work, and\n\t *    wake them all up. We wake all because we don't know whether\n\t *    a thread that called into (e)poll is handling non-binder\n\t *    work currently.\n\t */\n\tbinder_wakeup_poll_threads_ilocked(proc, sync);\n}\n\nstatic void binder_wakeup_proc_ilocked(struct binder_proc *proc)\n{\n\tstruct binder_thread *thread = binder_select_thread_ilocked(proc);\n\n\tbinder_wakeup_thread_ilocked(proc, thread, /* sync = */false);\n}\n\nstatic void binder_set_nice(long nice)\n{\n\tlong min_nice;\n\n\tif (can_nice(current, nice)) {\n\t\tset_user_nice(current, nice);\n\t\treturn;\n\t}\n\tmin_nice = rlimit_to_nice(rlimit(RLIMIT_NICE));\n\tbinder_debug(BINDER_DEBUG_PRIORITY_CAP,\n\t\t     \"%d: nice value %ld not allowed use %ld instead\\n\",\n\t\t      current->pid, nice, min_nice);\n\tset_user_nice(current, min_nice);\n\tif (min_nice <= MAX_NICE)\n\t\treturn;\n\tbinder_user_error(\"%d RLIMIT_NICE not set\\n\", current->pid);\n}\n\nstatic struct binder_node *binder_get_node_ilocked(struct binder_proc *proc,\n\t\t\t\t\t\t   binder_uintptr_t ptr)\n{\n\tstruct rb_node *n = proc->nodes.rb_node;\n\tstruct binder_node *node;\n\n\tassert_spin_locked(&proc->inner_lock);\n\n\twhile (n) {\n\t\tnode = rb_entry(n, struct binder_node, rb_node);\n\n\t\tif (ptr < node->ptr)\n\t\t\tn = n->rb_left;\n\t\telse if (ptr > node->ptr)\n\t\t\tn = n->rb_right;\n\t\telse {\n\t\t\t/*\n\t\t\t * take an implicit weak reference\n\t\t\t * to ensure node stays alive until\n\t\t\t * call to binder_put_node()\n\t\t\t */\n\t\t\tbinder_inc_node_tmpref_ilocked(node);\n\t\t\treturn node;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct binder_node *binder_get_node(struct binder_proc *proc,\n\t\t\t\t\t   binder_uintptr_t ptr)\n{\n\tstruct binder_node *node;\n\n\tbinder_inner_proc_lock(proc);\n\tnode = binder_get_node_ilocked(proc, ptr);\n\tbinder_inner_proc_unlock(proc);\n\treturn node;\n}\n\nstatic struct binder_node *binder_init_node_ilocked(\n\t\t\t\t\t\tstruct binder_proc *proc,\n\t\t\t\t\t\tstruct binder_node *new_node,\n\t\t\t\t\t\tstruct flat_binder_object *fp)\n{\n\tstruct rb_node **p = &proc->nodes.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct binder_node *node;\n\tbinder_uintptr_t ptr = fp ? fp->binder : 0;\n\tbinder_uintptr_t cookie = fp ? fp->cookie : 0;\n\t__u32 flags = fp ? fp->flags : 0;\n\n\tassert_spin_locked(&proc->inner_lock);\n\n\twhile (*p) {\n\n\t\tparent = *p;\n\t\tnode = rb_entry(parent, struct binder_node, rb_node);\n\n\t\tif (ptr < node->ptr)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (ptr > node->ptr)\n\t\t\tp = &(*p)->rb_right;\n\t\telse {\n\t\t\t/*\n\t\t\t * A matching node is already in\n\t\t\t * the rb tree. Abandon the init\n\t\t\t * and return it.\n\t\t\t */\n\t\t\tbinder_inc_node_tmpref_ilocked(node);\n\t\t\treturn node;\n\t\t}\n\t}\n\tnode = new_node;\n\tbinder_stats_created(BINDER_STAT_NODE);\n\tnode->tmp_refs++;\n\trb_link_node(&node->rb_node, parent, p);\n\trb_insert_color(&node->rb_node, &proc->nodes);\n\tnode->debug_id = atomic_inc_return(&binder_last_id);\n\tnode->proc = proc;\n\tnode->ptr = ptr;\n\tnode->cookie = cookie;\n\tnode->work.type = BINDER_WORK_NODE;\n\tnode->min_priority = flags & FLAT_BINDER_FLAG_PRIORITY_MASK;\n\tnode->accept_fds = !!(flags & FLAT_BINDER_FLAG_ACCEPTS_FDS);\n\tnode->txn_security_ctx = !!(flags & FLAT_BINDER_FLAG_TXN_SECURITY_CTX);\n\tspin_lock_init(&node->lock);\n\tINIT_LIST_HEAD(&node->work.entry);\n\tINIT_LIST_HEAD(&node->async_todo);\n\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t     \"%d:%d node %d u%016llx c%016llx created\\n\",\n\t\t     proc->pid, current->pid, node->debug_id,\n\t\t     (u64)node->ptr, (u64)node->cookie);\n\n\treturn node;\n}\n\nstatic struct binder_node *binder_new_node(struct binder_proc *proc,\n\t\t\t\t\t   struct flat_binder_object *fp)\n{\n\tstruct binder_node *node;\n\tstruct binder_node *new_node = kzalloc(sizeof(*node), GFP_KERNEL);\n\n\tif (!new_node)\n\t\treturn NULL;\n\tbinder_inner_proc_lock(proc);\n\tnode = binder_init_node_ilocked(proc, new_node, fp);\n\tbinder_inner_proc_unlock(proc);\n\tif (node != new_node)\n\t\t/*\n\t\t * The node was already added by another thread\n\t\t */\n\t\tkfree(new_node);\n\n\treturn node;\n}\n\nstatic void binder_free_node(struct binder_node *node)\n{\n\tkfree(node);\n\tbinder_stats_deleted(BINDER_STAT_NODE);\n}\n\nstatic int binder_inc_node_nilocked(struct binder_node *node, int strong,\n\t\t\t\t    int internal,\n\t\t\t\t    struct list_head *target_list)\n{\n\tstruct binder_proc *proc = node->proc;\n\n\tassert_spin_locked(&node->lock);\n\tif (proc)\n\t\tassert_spin_locked(&proc->inner_lock);\n\tif (strong) {\n\t\tif (internal) {\n\t\t\tif (target_list == NULL &&\n\t\t\t    node->internal_strong_refs == 0 &&\n\t\t\t    !(node->proc &&\n\t\t\t      node == node->proc->context->binder_context_mgr_node &&\n\t\t\t      node->has_strong_ref)) {\n\t\t\t\tpr_err(\"invalid inc strong node for %d\\n\",\n\t\t\t\t\tnode->debug_id);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnode->internal_strong_refs++;\n\t\t} else\n\t\t\tnode->local_strong_refs++;\n\t\tif (!node->has_strong_ref && target_list) {\n\t\t\tstruct binder_thread *thread = container_of(target_list,\n\t\t\t\t\t\t    struct binder_thread, todo);\n\t\t\tbinder_dequeue_work_ilocked(&node->work);\n\t\t\tBUG_ON(&thread->todo != target_list);\n\t\t\tbinder_enqueue_deferred_thread_work_ilocked(thread,\n\t\t\t\t\t\t\t\t   &node->work);\n\t\t}\n\t} else {\n\t\tif (!internal)\n\t\t\tnode->local_weak_refs++;\n\t\tif (!node->has_weak_ref && list_empty(&node->work.entry)) {\n\t\t\tif (target_list == NULL) {\n\t\t\t\tpr_err(\"invalid inc weak node for %d\\n\",\n\t\t\t\t\tnode->debug_id);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/*\n\t\t\t * See comment above\n\t\t\t */\n\t\t\tbinder_enqueue_work_ilocked(&node->work, target_list);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int binder_inc_node(struct binder_node *node, int strong, int internal,\n\t\t\t   struct list_head *target_list)\n{\n\tint ret;\n\n\tbinder_node_inner_lock(node);\n\tret = binder_inc_node_nilocked(node, strong, internal, target_list);\n\tbinder_node_inner_unlock(node);\n\n\treturn ret;\n}\n\nstatic bool binder_dec_node_nilocked(struct binder_node *node,\n\t\t\t\t     int strong, int internal)\n{\n\tstruct binder_proc *proc = node->proc;\n\n\tassert_spin_locked(&node->lock);\n\tif (proc)\n\t\tassert_spin_locked(&proc->inner_lock);\n\tif (strong) {\n\t\tif (internal)\n\t\t\tnode->internal_strong_refs--;\n\t\telse\n\t\t\tnode->local_strong_refs--;\n\t\tif (node->local_strong_refs || node->internal_strong_refs)\n\t\t\treturn false;\n\t} else {\n\t\tif (!internal)\n\t\t\tnode->local_weak_refs--;\n\t\tif (node->local_weak_refs || node->tmp_refs ||\n\t\t\t\t!hlist_empty(&node->refs))\n\t\t\treturn false;\n\t}\n\n\tif (proc && (node->has_strong_ref || node->has_weak_ref)) {\n\t\tif (list_empty(&node->work.entry)) {\n\t\t\tbinder_enqueue_work_ilocked(&node->work, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t} else {\n\t\tif (hlist_empty(&node->refs) && !node->local_strong_refs &&\n\t\t    !node->local_weak_refs && !node->tmp_refs) {\n\t\t\tif (proc) {\n\t\t\t\tbinder_dequeue_work_ilocked(&node->work);\n\t\t\t\trb_erase(&node->rb_node, &proc->nodes);\n\t\t\t\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t\t\t\t     \"refless node %d deleted\\n\",\n\t\t\t\t\t     node->debug_id);\n\t\t\t} else {\n\t\t\t\tBUG_ON(!list_empty(&node->work.entry));\n\t\t\t\tspin_lock(&binder_dead_nodes_lock);\n\t\t\t\t/*\n\t\t\t\t * tmp_refs could have changed so\n\t\t\t\t * check it again\n\t\t\t\t */\n\t\t\t\tif (node->tmp_refs) {\n\t\t\t\t\tspin_unlock(&binder_dead_nodes_lock);\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t\thlist_del(&node->dead_node);\n\t\t\t\tspin_unlock(&binder_dead_nodes_lock);\n\t\t\t\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t\t\t\t     \"dead node %d deleted\\n\",\n\t\t\t\t\t     node->debug_id);\n\t\t\t}\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic void binder_dec_node(struct binder_node *node, int strong, int internal)\n{\n\tbool free_node;\n\n\tbinder_node_inner_lock(node);\n\tfree_node = binder_dec_node_nilocked(node, strong, internal);\n\tbinder_node_inner_unlock(node);\n\tif (free_node)\n\t\tbinder_free_node(node);\n}\n\nstatic void binder_inc_node_tmpref_ilocked(struct binder_node *node)\n{\n\t/*\n\t * No call to binder_inc_node() is needed since we\n\t * don't need to inform userspace of any changes to\n\t * tmp_refs\n\t */\n\tnode->tmp_refs++;\n}\n\n/**\n * binder_inc_node_tmpref() - take a temporary reference on node\n * @node:\tnode to reference\n *\n * Take reference on node to prevent the node from being freed\n * while referenced only by a local variable. The inner lock is\n * needed to serialize with the node work on the queue (which\n * isn't needed after the node is dead). If the node is dead\n * (node->proc is NULL), use binder_dead_nodes_lock to protect\n * node->tmp_refs against dead-node-only cases where the node\n * lock cannot be acquired (eg traversing the dead node list to\n * print nodes)\n */\nstatic void binder_inc_node_tmpref(struct binder_node *node)\n{\n\tbinder_node_lock(node);\n\tif (node->proc)\n\t\tbinder_inner_proc_lock(node->proc);\n\telse\n\t\tspin_lock(&binder_dead_nodes_lock);\n\tbinder_inc_node_tmpref_ilocked(node);\n\tif (node->proc)\n\t\tbinder_inner_proc_unlock(node->proc);\n\telse\n\t\tspin_unlock(&binder_dead_nodes_lock);\n\tbinder_node_unlock(node);\n}\n\n/**\n * binder_dec_node_tmpref() - remove a temporary reference on node\n * @node:\tnode to reference\n *\n * Release temporary reference on node taken via binder_inc_node_tmpref()\n */\nstatic void binder_dec_node_tmpref(struct binder_node *node)\n{\n\tbool free_node;\n\n\tbinder_node_inner_lock(node);\n\tif (!node->proc)\n\t\tspin_lock(&binder_dead_nodes_lock);\n\telse\n\t\t__acquire(&binder_dead_nodes_lock);\n\tnode->tmp_refs--;\n\tBUG_ON(node->tmp_refs < 0);\n\tif (!node->proc)\n\t\tspin_unlock(&binder_dead_nodes_lock);\n\telse\n\t\t__release(&binder_dead_nodes_lock);\n\t/*\n\t * Call binder_dec_node() to check if all refcounts are 0\n\t * and cleanup is needed. Calling with strong=0 and internal=1\n\t * causes no actual reference to be released in binder_dec_node().\n\t * If that changes, a change is needed here too.\n\t */\n\tfree_node = binder_dec_node_nilocked(node, 0, 1);\n\tbinder_node_inner_unlock(node);\n\tif (free_node)\n\t\tbinder_free_node(node);\n}\n\nstatic void binder_put_node(struct binder_node *node)\n{\n\tbinder_dec_node_tmpref(node);\n}\n\nstatic struct binder_ref *binder_get_ref_olocked(struct binder_proc *proc,\n\t\t\t\t\t\t u32 desc, bool need_strong_ref)\n{\n\tstruct rb_node *n = proc->refs_by_desc.rb_node;\n\tstruct binder_ref *ref;\n\n\twhile (n) {\n\t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n\n\t\tif (desc < ref->data.desc) {\n\t\t\tn = n->rb_left;\n\t\t} else if (desc > ref->data.desc) {\n\t\t\tn = n->rb_right;\n\t\t} else if (need_strong_ref && !ref->data.strong) {\n\t\t\tbinder_user_error(\"tried to use weak ref as strong ref\\n\");\n\t\t\treturn NULL;\n\t\t} else {\n\t\t\treturn ref;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n/**\n * binder_get_ref_for_node_olocked() - get the ref associated with given node\n * @proc:\tbinder_proc that owns the ref\n * @node:\tbinder_node of target\n * @new_ref:\tnewly allocated binder_ref to be initialized or %NULL\n *\n * Look up the ref for the given node and return it if it exists\n *\n * If it doesn't exist and the caller provides a newly allocated\n * ref, initialize the fields of the newly allocated ref and insert\n * into the given proc rb_trees and node refs list.\n *\n * Return:\tthe ref for node. It is possible that another thread\n *\t\tallocated/initialized the ref first in which case the\n *\t\treturned ref would be different than the passed-in\n *\t\tnew_ref. new_ref must be kfree'd by the caller in\n *\t\tthis case.\n */\nstatic struct binder_ref *binder_get_ref_for_node_olocked(\n\t\t\t\t\tstruct binder_proc *proc,\n\t\t\t\t\tstruct binder_node *node,\n\t\t\t\t\tstruct binder_ref *new_ref)\n{\n\tstruct binder_context *context = proc->context;\n\tstruct rb_node **p = &proc->refs_by_node.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct binder_ref *ref;\n\tstruct rb_node *n;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tref = rb_entry(parent, struct binder_ref, rb_node_node);\n\n\t\tif (node < ref->node)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (node > ref->node)\n\t\t\tp = &(*p)->rb_right;\n\t\telse\n\t\t\treturn ref;\n\t}\n\tif (!new_ref)\n\t\treturn NULL;\n\n\tbinder_stats_created(BINDER_STAT_REF);\n\tnew_ref->data.debug_id = atomic_inc_return(&binder_last_id);\n\tnew_ref->proc = proc;\n\tnew_ref->node = node;\n\trb_link_node(&new_ref->rb_node_node, parent, p);\n\trb_insert_color(&new_ref->rb_node_node, &proc->refs_by_node);\n\n\tnew_ref->data.desc = (node == context->binder_context_mgr_node) ? 0 : 1;\n\tfor (n = rb_first(&proc->refs_by_desc); n != NULL; n = rb_next(n)) {\n\t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n\t\tif (ref->data.desc > new_ref->data.desc)\n\t\t\tbreak;\n\t\tnew_ref->data.desc = ref->data.desc + 1;\n\t}\n\n\tp = &proc->refs_by_desc.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tref = rb_entry(parent, struct binder_ref, rb_node_desc);\n\n\t\tif (new_ref->data.desc < ref->data.desc)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (new_ref->data.desc > ref->data.desc)\n\t\t\tp = &(*p)->rb_right;\n\t\telse\n\t\t\tBUG();\n\t}\n\trb_link_node(&new_ref->rb_node_desc, parent, p);\n\trb_insert_color(&new_ref->rb_node_desc, &proc->refs_by_desc);\n\n\tbinder_node_lock(node);\n\thlist_add_head(&new_ref->node_entry, &node->refs);\n\n\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t     \"%d new ref %d desc %d for node %d\\n\",\n\t\t      proc->pid, new_ref->data.debug_id, new_ref->data.desc,\n\t\t      node->debug_id);\n\tbinder_node_unlock(node);\n\treturn new_ref;\n}\n\nstatic void binder_cleanup_ref_olocked(struct binder_ref *ref)\n{\n\tbool delete_node = false;\n\n\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t     \"%d delete ref %d desc %d for node %d\\n\",\n\t\t      ref->proc->pid, ref->data.debug_id, ref->data.desc,\n\t\t      ref->node->debug_id);\n\n\trb_erase(&ref->rb_node_desc, &ref->proc->refs_by_desc);\n\trb_erase(&ref->rb_node_node, &ref->proc->refs_by_node);\n\n\tbinder_node_inner_lock(ref->node);\n\tif (ref->data.strong)\n\t\tbinder_dec_node_nilocked(ref->node, 1, 1);\n\n\thlist_del(&ref->node_entry);\n\tdelete_node = binder_dec_node_nilocked(ref->node, 0, 1);\n\tbinder_node_inner_unlock(ref->node);\n\t/*\n\t * Clear ref->node unless we want the caller to free the node\n\t */\n\tif (!delete_node) {\n\t\t/*\n\t\t * The caller uses ref->node to determine\n\t\t * whether the node needs to be freed. Clear\n\t\t * it since the node is still alive.\n\t\t */\n\t\tref->node = NULL;\n\t}\n\n\tif (ref->death) {\n\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t     \"%d delete ref %d desc %d has death notification\\n\",\n\t\t\t      ref->proc->pid, ref->data.debug_id,\n\t\t\t      ref->data.desc);\n\t\tbinder_dequeue_work(ref->proc, &ref->death->work);\n\t\tbinder_stats_deleted(BINDER_STAT_DEATH);\n\t}\n\tbinder_stats_deleted(BINDER_STAT_REF);\n}\n\n/**\n * binder_inc_ref_olocked() - increment the ref for given handle\n * @ref:         ref to be incremented\n * @strong:      if true, strong increment, else weak\n * @target_list: list to queue node work on\n *\n * Increment the ref. @ref->proc->outer_lock must be held on entry\n *\n * Return: 0, if successful, else errno\n */\nstatic int binder_inc_ref_olocked(struct binder_ref *ref, int strong,\n\t\t\t\t  struct list_head *target_list)\n{\n\tint ret;\n\n\tif (strong) {\n\t\tif (ref->data.strong == 0) {\n\t\t\tret = binder_inc_node(ref->node, 1, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->data.strong++;\n\t} else {\n\t\tif (ref->data.weak == 0) {\n\t\t\tret = binder_inc_node(ref->node, 0, 1, target_list);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tref->data.weak++;\n\t}\n\treturn 0;\n}\n\n/**\n * binder_dec_ref() - dec the ref for given handle\n * @ref:\tref to be decremented\n * @strong:\tif true, strong decrement, else weak\n *\n * Decrement the ref.\n *\n * Return: true if ref is cleaned up and ready to be freed\n */\nstatic bool binder_dec_ref_olocked(struct binder_ref *ref, int strong)\n{\n\tif (strong) {\n\t\tif (ref->data.strong == 0) {\n\t\t\tbinder_user_error(\"%d invalid dec strong, ref %d desc %d s %d w %d\\n\",\n\t\t\t\t\t  ref->proc->pid, ref->data.debug_id,\n\t\t\t\t\t  ref->data.desc, ref->data.strong,\n\t\t\t\t\t  ref->data.weak);\n\t\t\treturn false;\n\t\t}\n\t\tref->data.strong--;\n\t\tif (ref->data.strong == 0)\n\t\t\tbinder_dec_node(ref->node, strong, 1);\n\t} else {\n\t\tif (ref->data.weak == 0) {\n\t\t\tbinder_user_error(\"%d invalid dec weak, ref %d desc %d s %d w %d\\n\",\n\t\t\t\t\t  ref->proc->pid, ref->data.debug_id,\n\t\t\t\t\t  ref->data.desc, ref->data.strong,\n\t\t\t\t\t  ref->data.weak);\n\t\t\treturn false;\n\t\t}\n\t\tref->data.weak--;\n\t}\n\tif (ref->data.strong == 0 && ref->data.weak == 0) {\n\t\tbinder_cleanup_ref_olocked(ref);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/**\n * binder_get_node_from_ref() - get the node from the given proc/desc\n * @proc:\tproc containing the ref\n * @desc:\tthe handle associated with the ref\n * @need_strong_ref: if true, only return node if ref is strong\n * @rdata:\tthe id/refcount data for the ref\n *\n * Given a proc and ref handle, return the associated binder_node\n *\n * Return: a binder_node or NULL if not found or not strong when strong required\n */\nstatic struct binder_node *binder_get_node_from_ref(\n\t\tstruct binder_proc *proc,\n\t\tu32 desc, bool need_strong_ref,\n\t\tstruct binder_ref_data *rdata)\n{\n\tstruct binder_node *node;\n\tstruct binder_ref *ref;\n\n\tbinder_proc_lock(proc);\n\tref = binder_get_ref_olocked(proc, desc, need_strong_ref);\n\tif (!ref)\n\t\tgoto err_no_ref;\n\tnode = ref->node;\n\t/*\n\t * Take an implicit reference on the node to ensure\n\t * it stays alive until the call to binder_put_node()\n\t */\n\tbinder_inc_node_tmpref(node);\n\tif (rdata)\n\t\t*rdata = ref->data;\n\tbinder_proc_unlock(proc);\n\n\treturn node;\n\nerr_no_ref:\n\tbinder_proc_unlock(proc);\n\treturn NULL;\n}\n\n/**\n * binder_free_ref() - free the binder_ref\n * @ref:\tref to free\n *\n * Free the binder_ref. Free the binder_node indicated by ref->node\n * (if non-NULL) and the binder_ref_death indicated by ref->death.\n */\nstatic void binder_free_ref(struct binder_ref *ref)\n{\n\tif (ref->node)\n\t\tbinder_free_node(ref->node);\n\tkfree(ref->death);\n\tkfree(ref);\n}\n\n/**\n * binder_update_ref_for_handle() - inc/dec the ref for given handle\n * @proc:\tproc containing the ref\n * @desc:\tthe handle associated with the ref\n * @increment:\ttrue=inc reference, false=dec reference\n * @strong:\ttrue=strong reference, false=weak reference\n * @rdata:\tthe id/refcount data for the ref\n *\n * Given a proc and ref handle, increment or decrement the ref\n * according to \"increment\" arg.\n *\n * Return: 0 if successful, else errno\n */\nstatic int binder_update_ref_for_handle(struct binder_proc *proc,\n\t\tuint32_t desc, bool increment, bool strong,\n\t\tstruct binder_ref_data *rdata)\n{\n\tint ret = 0;\n\tstruct binder_ref *ref;\n\tbool delete_ref = false;\n\n\tbinder_proc_lock(proc);\n\tref = binder_get_ref_olocked(proc, desc, strong);\n\tif (!ref) {\n\t\tret = -EINVAL;\n\t\tgoto err_no_ref;\n\t}\n\tif (increment)\n\t\tret = binder_inc_ref_olocked(ref, strong, NULL);\n\telse\n\t\tdelete_ref = binder_dec_ref_olocked(ref, strong);\n\n\tif (rdata)\n\t\t*rdata = ref->data;\n\tbinder_proc_unlock(proc);\n\n\tif (delete_ref)\n\t\tbinder_free_ref(ref);\n\treturn ret;\n\nerr_no_ref:\n\tbinder_proc_unlock(proc);\n\treturn ret;\n}\n\n/**\n * binder_dec_ref_for_handle() - dec the ref for given handle\n * @proc:\tproc containing the ref\n * @desc:\tthe handle associated with the ref\n * @strong:\ttrue=strong reference, false=weak reference\n * @rdata:\tthe id/refcount data for the ref\n *\n * Just calls binder_update_ref_for_handle() to decrement the ref.\n *\n * Return: 0 if successful, else errno\n */\nstatic int binder_dec_ref_for_handle(struct binder_proc *proc,\n\t\tuint32_t desc, bool strong, struct binder_ref_data *rdata)\n{\n\treturn binder_update_ref_for_handle(proc, desc, false, strong, rdata);\n}\n\n\n/**\n * binder_inc_ref_for_node() - increment the ref for given proc/node\n * @proc:\t proc containing the ref\n * @node:\t target node\n * @strong:\t true=strong reference, false=weak reference\n * @target_list: worklist to use if node is incremented\n * @rdata:\t the id/refcount data for the ref\n *\n * Given a proc and node, increment the ref. Create the ref if it\n * doesn't already exist\n *\n * Return: 0 if successful, else errno\n */\nstatic int binder_inc_ref_for_node(struct binder_proc *proc,\n\t\t\tstruct binder_node *node,\n\t\t\tbool strong,\n\t\t\tstruct list_head *target_list,\n\t\t\tstruct binder_ref_data *rdata)\n{\n\tstruct binder_ref *ref;\n\tstruct binder_ref *new_ref = NULL;\n\tint ret = 0;\n\n\tbinder_proc_lock(proc);\n\tref = binder_get_ref_for_node_olocked(proc, node, NULL);\n\tif (!ref) {\n\t\tbinder_proc_unlock(proc);\n\t\tnew_ref = kzalloc(sizeof(*ref), GFP_KERNEL);\n\t\tif (!new_ref)\n\t\t\treturn -ENOMEM;\n\t\tbinder_proc_lock(proc);\n\t\tref = binder_get_ref_for_node_olocked(proc, node, new_ref);\n\t}\n\tret = binder_inc_ref_olocked(ref, strong, target_list);\n\t*rdata = ref->data;\n\tbinder_proc_unlock(proc);\n\tif (new_ref && ref != new_ref)\n\t\t/*\n\t\t * Another thread created the ref first so\n\t\t * free the one we allocated\n\t\t */\n\t\tkfree(new_ref);\n\treturn ret;\n}\n\nstatic void binder_pop_transaction_ilocked(struct binder_thread *target_thread,\n\t\t\t\t\t   struct binder_transaction *t)\n{\n\tBUG_ON(!target_thread);\n\tassert_spin_locked(&target_thread->proc->inner_lock);\n\tBUG_ON(target_thread->transaction_stack != t);\n\tBUG_ON(target_thread->transaction_stack->from != target_thread);\n\ttarget_thread->transaction_stack =\n\t\ttarget_thread->transaction_stack->from_parent;\n\tt->from = NULL;\n}\n\n/**\n * binder_thread_dec_tmpref() - decrement thread->tmp_ref\n * @thread:\tthread to decrement\n *\n * A thread needs to be kept alive while being used to create or\n * handle a transaction. binder_get_txn_from() is used to safely\n * extract t->from from a binder_transaction and keep the thread\n * indicated by t->from from being freed. When done with that\n * binder_thread, this function is called to decrement the\n * tmp_ref and free if appropriate (thread has been released\n * and no transaction being processed by the driver)\n */\nstatic void binder_thread_dec_tmpref(struct binder_thread *thread)\n{\n\t/*\n\t * atomic is used to protect the counter value while\n\t * it cannot reach zero or thread->is_dead is false\n\t */\n\tbinder_inner_proc_lock(thread->proc);\n\tatomic_dec(&thread->tmp_ref);\n\tif (thread->is_dead && !atomic_read(&thread->tmp_ref)) {\n\t\tbinder_inner_proc_unlock(thread->proc);\n\t\tbinder_free_thread(thread);\n\t\treturn;\n\t}\n\tbinder_inner_proc_unlock(thread->proc);\n}\n\n/**\n * binder_proc_dec_tmpref() - decrement proc->tmp_ref\n * @proc:\tproc to decrement\n *\n * A binder_proc needs to be kept alive while being used to create or\n * handle a transaction. proc->tmp_ref is incremented when\n * creating a new transaction or the binder_proc is currently in-use\n * by threads that are being released. When done with the binder_proc,\n * this function is called to decrement the counter and free the\n * proc if appropriate (proc has been released, all threads have\n * been released and not currenly in-use to process a transaction).\n */\nstatic void binder_proc_dec_tmpref(struct binder_proc *proc)\n{\n\tbinder_inner_proc_lock(proc);\n\tproc->tmp_ref--;\n\tif (proc->is_dead && RB_EMPTY_ROOT(&proc->threads) &&\n\t\t\t!proc->tmp_ref) {\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_free_proc(proc);\n\t\treturn;\n\t}\n\tbinder_inner_proc_unlock(proc);\n}\n\n/**\n * binder_get_txn_from() - safely extract the \"from\" thread in transaction\n * @t:\tbinder transaction for t->from\n *\n * Atomically return the \"from\" thread and increment the tmp_ref\n * count for the thread to ensure it stays alive until\n * binder_thread_dec_tmpref() is called.\n *\n * Return: the value of t->from\n */\nstatic struct binder_thread *binder_get_txn_from(\n\t\tstruct binder_transaction *t)\n{\n\tstruct binder_thread *from;\n\n\tspin_lock(&t->lock);\n\tfrom = t->from;\n\tif (from)\n\t\tatomic_inc(&from->tmp_ref);\n\tspin_unlock(&t->lock);\n\treturn from;\n}\n\n/**\n * binder_get_txn_from_and_acq_inner() - get t->from and acquire inner lock\n * @t:\tbinder transaction for t->from\n *\n * Same as binder_get_txn_from() except it also acquires the proc->inner_lock\n * to guarantee that the thread cannot be released while operating on it.\n * The caller must call binder_inner_proc_unlock() to release the inner lock\n * as well as call binder_dec_thread_txn() to release the reference.\n *\n * Return: the value of t->from\n */\nstatic struct binder_thread *binder_get_txn_from_and_acq_inner(\n\t\tstruct binder_transaction *t)\n\t__acquires(&t->from->proc->inner_lock)\n{\n\tstruct binder_thread *from;\n\n\tfrom = binder_get_txn_from(t);\n\tif (!from) {\n\t\t__acquire(&from->proc->inner_lock);\n\t\treturn NULL;\n\t}\n\tbinder_inner_proc_lock(from->proc);\n\tif (t->from) {\n\t\tBUG_ON(from != t->from);\n\t\treturn from;\n\t}\n\tbinder_inner_proc_unlock(from->proc);\n\t__acquire(&from->proc->inner_lock);\n\tbinder_thread_dec_tmpref(from);\n\treturn NULL;\n}\n\n/**\n * binder_free_txn_fixups() - free unprocessed fd fixups\n * @t:\tbinder transaction for t->from\n *\n * If the transaction is being torn down prior to being\n * processed by the target process, free all of the\n * fd fixups and fput the file structs. It is safe to\n * call this function after the fixups have been\n * processed -- in that case, the list will be empty.\n */\nstatic void binder_free_txn_fixups(struct binder_transaction *t)\n{\n\tstruct binder_txn_fd_fixup *fixup, *tmp;\n\n\tlist_for_each_entry_safe(fixup, tmp, &t->fd_fixups, fixup_entry) {\n\t\tfput(fixup->file);\n\t\tlist_del(&fixup->fixup_entry);\n\t\tkfree(fixup);\n\t}\n}\n\nstatic void binder_free_transaction(struct binder_transaction *t)\n{\n\tstruct binder_proc *target_proc = t->to_proc;\n\n\tif (target_proc) {\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (t->buffer)\n\t\t\tt->buffer->transaction = NULL;\n\t\tbinder_inner_proc_unlock(target_proc);\n\t}\n\t/*\n\t * If the transaction has no target_proc, then\n\t * t->buffer->transaction has already been cleared.\n\t */\n\tbinder_free_txn_fixups(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n}\n\nstatic void binder_send_failed_reply(struct binder_transaction *t,\n\t\t\t\t     uint32_t error_code)\n{\n\tstruct binder_thread *target_thread;\n\tstruct binder_transaction *next;\n\n\tBUG_ON(t->flags & TF_ONE_WAY);\n\twhile (1) {\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(t);\n\t\tif (target_thread) {\n\t\t\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t     \"send failed reply for transaction %d to %d:%d\\n\",\n\t\t\t\t      t->debug_id,\n\t\t\t\t      target_thread->proc->pid,\n\t\t\t\t      target_thread->pid);\n\n\t\t\tbinder_pop_transaction_ilocked(target_thread, t);\n\t\t\tif (target_thread->reply_error.cmd == BR_OK) {\n\t\t\t\ttarget_thread->reply_error.cmd = error_code;\n\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\ttarget_thread,\n\t\t\t\t\t&target_thread->reply_error.work);\n\t\t\t\twake_up_interruptible(&target_thread->wait);\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * Cannot get here for normal operation, but\n\t\t\t\t * we can if multiple synchronous transactions\n\t\t\t\t * are sent without blocking for responses.\n\t\t\t\t * Just ignore the 2nd error in this case.\n\t\t\t\t */\n\t\t\t\tpr_warn(\"Unexpected reply error: %u\\n\",\n\t\t\t\t\ttarget_thread->reply_error.cmd);\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\tbinder_thread_dec_tmpref(target_thread);\n\t\t\tbinder_free_transaction(t);\n\t\t\treturn;\n\t\t} else {\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t}\n\t\tnext = t->from_parent;\n\n\t\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t     \"send failed reply for transaction %d, target dead\\n\",\n\t\t\t     t->debug_id);\n\n\t\tbinder_free_transaction(t);\n\t\tif (next == NULL) {\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"reply failed, no target thread at root\\n\");\n\t\t\treturn;\n\t\t}\n\t\tt = next;\n\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t     \"reply failed, no target thread -- retry %d\\n\",\n\t\t\t      t->debug_id);\n\t}\n}\n\n/**\n * binder_cleanup_transaction() - cleans up undelivered transaction\n * @t:\t\ttransaction that needs to be cleaned up\n * @reason:\treason the transaction wasn't delivered\n * @error_code:\terror to return to caller (if synchronous call)\n */\nstatic void binder_cleanup_transaction(struct binder_transaction *t,\n\t\t\t\t       const char *reason,\n\t\t\t\t       uint32_t error_code)\n{\n\tif (t->buffer->target_node && !(t->flags & TF_ONE_WAY)) {\n\t\tbinder_send_failed_reply(t, error_code);\n\t} else {\n\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\"undelivered transaction %d, %s\\n\",\n\t\t\tt->debug_id, reason);\n\t\tbinder_free_transaction(t);\n\t}\n}\n\n/**\n * binder_get_object() - gets object and checks for valid metadata\n * @proc:\tbinder_proc owning the buffer\n * @buffer:\tbinder_buffer that we're parsing.\n * @offset:\toffset in the @buffer at which to validate an object.\n * @object:\tstruct binder_object to read into\n *\n * Return:\tIf there's a valid metadata object at @offset in @buffer, the\n *\t\tsize of that object. Otherwise, it returns zero. The object\n *\t\tis read into the struct binder_object pointed to by @object.\n */\nstatic size_t binder_get_object(struct binder_proc *proc,\n\t\t\t\tstruct binder_buffer *buffer,\n\t\t\t\tunsigned long offset,\n\t\t\t\tstruct binder_object *object)\n{\n\tsize_t read_size;\n\tstruct binder_object_header *hdr;\n\tsize_t object_size = 0;\n\n\tread_size = min_t(size_t, sizeof(*object), buffer->data_size - offset);\n\tif (offset > buffer->data_size || read_size < sizeof(*hdr) ||\n\t    binder_alloc_copy_from_buffer(&proc->alloc, object, buffer,\n\t\t\t\t\t  offset, read_size))\n\t\treturn 0;\n\n\t/* Ok, now see if we read a complete object. */\n\thdr = &object->hdr;\n\tswitch (hdr->type) {\n\tcase BINDER_TYPE_BINDER:\n\tcase BINDER_TYPE_WEAK_BINDER:\n\tcase BINDER_TYPE_HANDLE:\n\tcase BINDER_TYPE_WEAK_HANDLE:\n\t\tobject_size = sizeof(struct flat_binder_object);\n\t\tbreak;\n\tcase BINDER_TYPE_FD:\n\t\tobject_size = sizeof(struct binder_fd_object);\n\t\tbreak;\n\tcase BINDER_TYPE_PTR:\n\t\tobject_size = sizeof(struct binder_buffer_object);\n\t\tbreak;\n\tcase BINDER_TYPE_FDA:\n\t\tobject_size = sizeof(struct binder_fd_array_object);\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\tif (offset <= buffer->data_size - object_size &&\n\t    buffer->data_size >= object_size)\n\t\treturn object_size;\n\telse\n\t\treturn 0;\n}\n\n/**\n * binder_validate_ptr() - validates binder_buffer_object in a binder_buffer.\n * @proc:\tbinder_proc owning the buffer\n * @b:\t\tbinder_buffer containing the object\n * @object:\tstruct binder_object to read into\n * @index:\tindex in offset array at which the binder_buffer_object is\n *\t\tlocated\n * @start_offset: points to the start of the offset array\n * @object_offsetp: offset of @object read from @b\n * @num_valid:\tthe number of valid offsets in the offset array\n *\n * Return:\tIf @index is within the valid range of the offset array\n *\t\tdescribed by @start and @num_valid, and if there's a valid\n *\t\tbinder_buffer_object at the offset found in index @index\n *\t\tof the offset array, that object is returned. Otherwise,\n *\t\t%NULL is returned.\n *\t\tNote that the offset found in index @index itself is not\n *\t\tverified; this function assumes that @num_valid elements\n *\t\tfrom @start were previously verified to have valid offsets.\n *\t\tIf @object_offsetp is non-NULL, then the offset within\n *\t\t@b is written to it.\n */\nstatic struct binder_buffer_object *binder_validate_ptr(\n\t\t\t\t\t\tstruct binder_proc *proc,\n\t\t\t\t\t\tstruct binder_buffer *b,\n\t\t\t\t\t\tstruct binder_object *object,\n\t\t\t\t\t\tbinder_size_t index,\n\t\t\t\t\t\tbinder_size_t start_offset,\n\t\t\t\t\t\tbinder_size_t *object_offsetp,\n\t\t\t\t\t\tbinder_size_t num_valid)\n{\n\tsize_t object_size;\n\tbinder_size_t object_offset;\n\tunsigned long buffer_offset;\n\n\tif (index >= num_valid)\n\t\treturn NULL;\n\n\tbuffer_offset = start_offset + sizeof(binder_size_t) * index;\n\tif (binder_alloc_copy_from_buffer(&proc->alloc, &object_offset,\n\t\t\t\t\t  b, buffer_offset,\n\t\t\t\t\t  sizeof(object_offset)))\n\t\treturn NULL;\n\tobject_size = binder_get_object(proc, b, object_offset, object);\n\tif (!object_size || object->hdr.type != BINDER_TYPE_PTR)\n\t\treturn NULL;\n\tif (object_offsetp)\n\t\t*object_offsetp = object_offset;\n\n\treturn &object->bbo;\n}\n\n/**\n * binder_validate_fixup() - validates pointer/fd fixups happen in order.\n * @proc:\t\tbinder_proc owning the buffer\n * @b:\t\t\ttransaction buffer\n * @objects_start_offset: offset to start of objects buffer\n * @buffer_obj_offset:\toffset to binder_buffer_object in which to fix up\n * @fixup_offset:\tstart offset in @buffer to fix up\n * @last_obj_offset:\toffset to last binder_buffer_object that we fixed\n * @last_min_offset:\tminimum fixup offset in object at @last_obj_offset\n *\n * Return:\t\t%true if a fixup in buffer @buffer at offset @offset is\n *\t\t\tallowed.\n *\n * For safety reasons, we only allow fixups inside a buffer to happen\n * at increasing offsets; additionally, we only allow fixup on the last\n * buffer object that was verified, or one of its parents.\n *\n * Example of what is allowed:\n *\n * A\n *   B (parent = A, offset = 0)\n *   C (parent = A, offset = 16)\n *     D (parent = C, offset = 0)\n *   E (parent = A, offset = 32) // min_offset is 16 (C.parent_offset)\n *\n * Examples of what is not allowed:\n *\n * Decreasing offsets within the same parent:\n * A\n *   C (parent = A, offset = 16)\n *   B (parent = A, offset = 0) // decreasing offset within A\n *\n * Referring to a parent that wasn't the last object or any of its parents:\n * A\n *   B (parent = A, offset = 0)\n *   C (parent = A, offset = 0)\n *   C (parent = A, offset = 16)\n *     D (parent = B, offset = 0) // B is not A or any of A's parents\n */\nstatic bool binder_validate_fixup(struct binder_proc *proc,\n\t\t\t\t  struct binder_buffer *b,\n\t\t\t\t  binder_size_t objects_start_offset,\n\t\t\t\t  binder_size_t buffer_obj_offset,\n\t\t\t\t  binder_size_t fixup_offset,\n\t\t\t\t  binder_size_t last_obj_offset,\n\t\t\t\t  binder_size_t last_min_offset)\n{\n\tif (!last_obj_offset) {\n\t\t/* Nothing to fix up in */\n\t\treturn false;\n\t}\n\n\twhile (last_obj_offset != buffer_obj_offset) {\n\t\tunsigned long buffer_offset;\n\t\tstruct binder_object last_object;\n\t\tstruct binder_buffer_object *last_bbo;\n\t\tsize_t object_size = binder_get_object(proc, b, last_obj_offset,\n\t\t\t\t\t\t       &last_object);\n\t\tif (object_size != sizeof(*last_bbo))\n\t\t\treturn false;\n\n\t\tlast_bbo = &last_object.bbo;\n\t\t/*\n\t\t * Safe to retrieve the parent of last_obj, since it\n\t\t * was already previously verified by the driver.\n\t\t */\n\t\tif ((last_bbo->flags & BINDER_BUFFER_FLAG_HAS_PARENT) == 0)\n\t\t\treturn false;\n\t\tlast_min_offset = last_bbo->parent_offset + sizeof(uintptr_t);\n\t\tbuffer_offset = objects_start_offset +\n\t\t\tsizeof(binder_size_t) * last_bbo->parent;\n\t\tif (binder_alloc_copy_from_buffer(&proc->alloc,\n\t\t\t\t\t\t  &last_obj_offset,\n\t\t\t\t\t\t  b, buffer_offset,\n\t\t\t\t\t\t  sizeof(last_obj_offset)))\n\t\t\treturn false;\n\t}\n\treturn (fixup_offset >= last_min_offset);\n}\n\n/**\n * struct binder_task_work_cb - for deferred close\n *\n * @twork:                callback_head for task work\n * @fd:                   fd to close\n *\n * Structure to pass task work to be handled after\n * returning from binder_ioctl() via task_work_add().\n */\nstruct binder_task_work_cb {\n\tstruct callback_head twork;\n\tstruct file *file;\n};\n\n/**\n * binder_do_fd_close() - close list of file descriptors\n * @twork:\tcallback head for task work\n *\n * It is not safe to call ksys_close() during the binder_ioctl()\n * function if there is a chance that binder's own file descriptor\n * might be closed. This is to meet the requirements for using\n * fdget() (see comments for __fget_light()). Therefore use\n * task_work_add() to schedule the close operation once we have\n * returned from binder_ioctl(). This function is a callback\n * for that mechanism and does the actual ksys_close() on the\n * given file descriptor.\n */\nstatic void binder_do_fd_close(struct callback_head *twork)\n{\n\tstruct binder_task_work_cb *twcb = container_of(twork,\n\t\t\tstruct binder_task_work_cb, twork);\n\n\tfput(twcb->file);\n\tkfree(twcb);\n}\n\n/**\n * binder_deferred_fd_close() - schedule a close for the given file-descriptor\n * @fd:\t\tfile-descriptor to close\n *\n * See comments in binder_do_fd_close(). This function is used to schedule\n * a file-descriptor to be closed after returning from binder_ioctl().\n */\nstatic void binder_deferred_fd_close(int fd)\n{\n\tstruct binder_task_work_cb *twcb;\n\n\ttwcb = kzalloc(sizeof(*twcb), GFP_KERNEL);\n\tif (!twcb)\n\t\treturn;\n\tinit_task_work(&twcb->twork, binder_do_fd_close);\n\t__close_fd_get_file(fd, &twcb->file);\n\tif (twcb->file) {\n\t\tfilp_close(twcb->file, current->files);\n\t\ttask_work_add(current, &twcb->twork, TWA_RESUME);\n\t} else {\n\t\tkfree(twcb);\n\t}\n}\n\nstatic void binder_transaction_buffer_release(struct binder_proc *proc,\n\t\t\t\t\t      struct binder_thread *thread,\n\t\t\t\t\t      struct binder_buffer *buffer,\n\t\t\t\t\t      binder_size_t failed_at,\n\t\t\t\t\t      bool is_failure)\n{\n\tint debug_id = buffer->debug_id;\n\tbinder_size_t off_start_offset, buffer_offset, off_end_offset;\n\n\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t     \"%d buffer release %d, size %zd-%zd, failed at %llx\\n\",\n\t\t     proc->pid, buffer->debug_id,\n\t\t     buffer->data_size, buffer->offsets_size,\n\t\t     (unsigned long long)failed_at);\n\n\tif (buffer->target_node)\n\t\tbinder_dec_node(buffer->target_node, 1, 0);\n\n\toff_start_offset = ALIGN(buffer->data_size, sizeof(void *));\n\toff_end_offset = is_failure && failed_at ? failed_at :\n\t\t\t\toff_start_offset + buffer->offsets_size;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size = 0;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (!binder_alloc_copy_from_buffer(&proc->alloc, &object_offset,\n\t\t\t\t\t\t   buffer, buffer_offset,\n\t\t\t\t\t\t   sizeof(object_offset)))\n\t\t\tobject_size = binder_get_object(proc, buffer,\n\t\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0) {\n\t\t\tpr_err(\"transaction release %d bad object at offset %lld, size %zd\\n\",\n\t\t\t       debug_id, (u64)object_offset, buffer->data_size);\n\t\t\tcontinue;\n\t\t}\n\t\thdr = &object.hdr;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_node *node;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tnode = binder_get_node(proc, fp->binder);\n\t\t\tif (node == NULL) {\n\t\t\t\tpr_err(\"transaction release %d bad node %016llx\\n\",\n\t\t\t\t       debug_id, (u64)fp->binder);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        node %d u%016llx\\n\",\n\t\t\t\t     node->debug_id, (u64)node->ptr);\n\t\t\tbinder_dec_node(node, hdr->type == BINDER_TYPE_BINDER,\n\t\t\t\t\t0);\n\t\t\tbinder_put_node(node);\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\t\t\tstruct binder_ref_data rdata;\n\t\t\tint ret;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_dec_ref_for_handle(proc, fp->handle,\n\t\t\t\thdr->type == BINDER_TYPE_HANDLE, &rdata);\n\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"transaction release %d bad handle %d, ret = %d\\n\",\n\t\t\t\t debug_id, fp->handle, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"        ref %d desc %d\\n\",\n\t\t\t\t     rdata.debug_id, rdata.desc);\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\t/*\n\t\t\t * No need to close the file here since user-space\n\t\t\t * closes it for for successfully delivered\n\t\t\t * transactions. For transactions that weren't\n\t\t\t * delivered, the new fd was never allocated so\n\t\t\t * there is no need to close and the fput on the\n\t\t\t * file is done when the transaction is torn\n\t\t\t * down.\n\t\t\t */\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR:\n\t\t\t/*\n\t\t\t * Nothing to do here, this will get cleaned up when the\n\t\t\t * transaction buffer gets freed\n\t\t\t */\n\t\t\tbreak;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_fd_array_object *fda;\n\t\t\tstruct binder_buffer_object *parent;\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t fda_offset;\n\t\t\tsize_t fd_index;\n\t\t\tbinder_size_t fd_buf_size;\n\t\t\tbinder_size_t num_valid;\n\n\t\t\tif (is_failure) {\n\t\t\t\t/*\n\t\t\t\t * The fd fixups have not been applied so no\n\t\t\t\t * fds need to be closed.\n\t\t\t\t */\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tfda = to_binder_fd_array_object(hdr);\n\t\t\tparent = binder_validate_ptr(proc, buffer, &ptr_object,\n\t\t\t\t\t\t     fda->parent,\n\t\t\t\t\t\t     off_start_offset,\n\t\t\t\t\t\t     NULL,\n\t\t\t\t\t\t     num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tpr_err(\"transaction release %d bad parent offset\\n\",\n\t\t\t\t       debug_id);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfd_buf_size = sizeof(u32) * fda->num_fds;\n\t\t\tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {\n\t\t\t\tpr_err(\"transaction release %d invalid number of fds (%lld)\\n\",\n\t\t\t\t       debug_id, (u64)fda->num_fds);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (fd_buf_size > parent->length ||\n\t\t\t    fda->parent_offset > parent->length - fd_buf_size) {\n\t\t\t\t/* No space for all file descriptors here. */\n\t\t\t\tpr_err(\"transaction release %d not enough space for %lld fds in buffer\\n\",\n\t\t\t\t       debug_id, (u64)fda->num_fds);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * the source data for binder_buffer_object is visible\n\t\t\t * to user-space and the @buffer element is the user\n\t\t\t * pointer to the buffer_object containing the fd_array.\n\t\t\t * Convert the address to an offset relative to\n\t\t\t * the base of the transaction buffer.\n\t\t\t */\n\t\t\tfda_offset =\n\t\t\t    (parent->buffer - (uintptr_t)buffer->user_data) +\n\t\t\t    fda->parent_offset;\n\t\t\tfor (fd_index = 0; fd_index < fda->num_fds;\n\t\t\t     fd_index++) {\n\t\t\t\tu32 fd;\n\t\t\t\tint err;\n\t\t\t\tbinder_size_t offset = fda_offset +\n\t\t\t\t\tfd_index * sizeof(fd);\n\n\t\t\t\terr = binder_alloc_copy_from_buffer(\n\t\t\t\t\t\t&proc->alloc, &fd, buffer,\n\t\t\t\t\t\toffset, sizeof(fd));\n\t\t\t\tWARN_ON(err);\n\t\t\t\tif (!err) {\n\t\t\t\t\tbinder_deferred_fd_close(fd);\n\t\t\t\t\t/*\n\t\t\t\t\t * Need to make sure the thread goes\n\t\t\t\t\t * back to userspace to complete the\n\t\t\t\t\t * deferred close\n\t\t\t\t\t */\n\t\t\t\t\tif (thread)\n\t\t\t\t\t\tthread->looper_need_return = true;\n\t\t\t\t}\n\t\t\t}\n\t\t} break;\n\t\tdefault:\n\t\t\tpr_err(\"transaction release %d bad object type %x\\n\",\n\t\t\t\tdebug_id, hdr->type);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int binder_translate_binder(struct flat_binder_object *fp,\n\t\t\t\t   struct binder_transaction *t,\n\t\t\t\t   struct binder_thread *thread)\n{\n\tstruct binder_node *node;\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\tstruct binder_ref_data rdata;\n\tint ret = 0;\n\n\tnode = binder_get_node(proc, fp->binder);\n\tif (!node) {\n\t\tnode = binder_new_node(proc, fp);\n\t\tif (!node)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (fp->cookie != node->cookie) {\n\t\tbinder_user_error(\"%d:%d sending u%016llx node %d, cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fp->binder,\n\t\t\t\t  node->debug_id, (u64)fp->cookie,\n\t\t\t\t  (u64)node->cookie);\n\t\tret = -EINVAL;\n\t\tgoto done;\n\t}\n\tif (security_binder_transfer_binder(proc->cred, target_proc->cred)) {\n\t\tret = -EPERM;\n\t\tgoto done;\n\t}\n\n\tret = binder_inc_ref_for_node(target_proc, node,\n\t\t\tfp->hdr.type == BINDER_TYPE_BINDER,\n\t\t\t&thread->todo, &rdata);\n\tif (ret)\n\t\tgoto done;\n\n\tif (fp->hdr.type == BINDER_TYPE_BINDER)\n\t\tfp->hdr.type = BINDER_TYPE_HANDLE;\n\telse\n\t\tfp->hdr.type = BINDER_TYPE_WEAK_HANDLE;\n\tfp->binder = 0;\n\tfp->handle = rdata.desc;\n\tfp->cookie = 0;\n\n\ttrace_binder_transaction_node_to_ref(t, node, &rdata);\n\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t     \"        node %d u%016llx -> ref %d desc %d\\n\",\n\t\t     node->debug_id, (u64)node->ptr,\n\t\t     rdata.debug_id, rdata.desc);\ndone:\n\tbinder_put_node(node);\n\treturn ret;\n}\n\nstatic int binder_translate_handle(struct flat_binder_object *fp,\n\t\t\t\t   struct binder_transaction *t,\n\t\t\t\t   struct binder_thread *thread)\n{\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\tstruct binder_node *node;\n\tstruct binder_ref_data src_rdata;\n\tint ret = 0;\n\n\tnode = binder_get_node_from_ref(proc, fp->handle,\n\t\t\tfp->hdr.type == BINDER_TYPE_HANDLE, &src_rdata);\n\tif (!node) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid handle, %d\\n\",\n\t\t\t\t  proc->pid, thread->pid, fp->handle);\n\t\treturn -EINVAL;\n\t}\n\tif (security_binder_transfer_binder(proc->cred, target_proc->cred)) {\n\t\tret = -EPERM;\n\t\tgoto done;\n\t}\n\n\tbinder_node_lock(node);\n\tif (node->proc == target_proc) {\n\t\tif (fp->hdr.type == BINDER_TYPE_HANDLE)\n\t\t\tfp->hdr.type = BINDER_TYPE_BINDER;\n\t\telse\n\t\t\tfp->hdr.type = BINDER_TYPE_WEAK_BINDER;\n\t\tfp->binder = node->ptr;\n\t\tfp->cookie = node->cookie;\n\t\tif (node->proc)\n\t\t\tbinder_inner_proc_lock(node->proc);\n\t\telse\n\t\t\t__acquire(&node->proc->inner_lock);\n\t\tbinder_inc_node_nilocked(node,\n\t\t\t\t\t fp->hdr.type == BINDER_TYPE_BINDER,\n\t\t\t\t\t 0, NULL);\n\t\tif (node->proc)\n\t\t\tbinder_inner_proc_unlock(node->proc);\n\t\telse\n\t\t\t__release(&node->proc->inner_lock);\n\t\ttrace_binder_transaction_ref_to_node(t, node, &src_rdata);\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"        ref %d desc %d -> node %d u%016llx\\n\",\n\t\t\t     src_rdata.debug_id, src_rdata.desc, node->debug_id,\n\t\t\t     (u64)node->ptr);\n\t\tbinder_node_unlock(node);\n\t} else {\n\t\tstruct binder_ref_data dest_rdata;\n\n\t\tbinder_node_unlock(node);\n\t\tret = binder_inc_ref_for_node(target_proc, node,\n\t\t\t\tfp->hdr.type == BINDER_TYPE_HANDLE,\n\t\t\t\tNULL, &dest_rdata);\n\t\tif (ret)\n\t\t\tgoto done;\n\n\t\tfp->binder = 0;\n\t\tfp->handle = dest_rdata.desc;\n\t\tfp->cookie = 0;\n\t\ttrace_binder_transaction_ref_to_ref(t, node, &src_rdata,\n\t\t\t\t\t\t    &dest_rdata);\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"        ref %d desc %d -> ref %d desc %d (node %d)\\n\",\n\t\t\t     src_rdata.debug_id, src_rdata.desc,\n\t\t\t     dest_rdata.debug_id, dest_rdata.desc,\n\t\t\t     node->debug_id);\n\t}\ndone:\n\tbinder_put_node(node);\n\treturn ret;\n}\n\nstatic int binder_translate_fd(u32 fd, binder_size_t fd_offset,\n\t\t\t       struct binder_transaction *t,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction *in_reply_to)\n{\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\tstruct binder_txn_fd_fixup *fixup;\n\tstruct file *file;\n\tint ret = 0;\n\tbool target_allows_fd;\n\n\tif (in_reply_to)\n\t\ttarget_allows_fd = !!(in_reply_to->flags & TF_ACCEPT_FDS);\n\telse\n\t\ttarget_allows_fd = t->buffer->target_node->accept_fds;\n\tif (!target_allows_fd) {\n\t\tbinder_user_error(\"%d:%d got %s with fd, %d, but target does not allow fds\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  in_reply_to ? \"reply\" : \"transaction\",\n\t\t\t\t  fd);\n\t\tret = -EPERM;\n\t\tgoto err_fd_not_accepted;\n\t}\n\n\tfile = fget(fd);\n\tif (!file) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid fd, %d\\n\",\n\t\t\t\t  proc->pid, thread->pid, fd);\n\t\tret = -EBADF;\n\t\tgoto err_fget;\n\t}\n\tret = security_binder_transfer_file(proc->cred, target_proc->cred, file);\n\tif (ret < 0) {\n\t\tret = -EPERM;\n\t\tgoto err_security;\n\t}\n\n\t/*\n\t * Add fixup record for this transaction. The allocation\n\t * of the fd in the target needs to be done from a\n\t * target thread.\n\t */\n\tfixup = kzalloc(sizeof(*fixup), GFP_KERNEL);\n\tif (!fixup) {\n\t\tret = -ENOMEM;\n\t\tgoto err_alloc;\n\t}\n\tfixup->file = file;\n\tfixup->offset = fd_offset;\n\ttrace_binder_transaction_fd_send(t, fd, fixup->offset);\n\tlist_add_tail(&fixup->fixup_entry, &t->fd_fixups);\n\n\treturn ret;\n\nerr_alloc:\nerr_security:\n\tfput(file);\nerr_fget:\nerr_fd_not_accepted:\n\treturn ret;\n}\n\nstatic int binder_translate_fd_array(struct binder_fd_array_object *fda,\n\t\t\t\t     struct binder_buffer_object *parent,\n\t\t\t\t     struct binder_transaction *t,\n\t\t\t\t     struct binder_thread *thread,\n\t\t\t\t     struct binder_transaction *in_reply_to)\n{\n\tbinder_size_t fdi, fd_buf_size;\n\tbinder_size_t fda_offset;\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\n\tfd_buf_size = sizeof(u32) * fda->num_fds;\n\tif (fda->num_fds >= SIZE_MAX / sizeof(u32)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid number of fds (%lld)\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fda->num_fds);\n\t\treturn -EINVAL;\n\t}\n\tif (fd_buf_size > parent->length ||\n\t    fda->parent_offset > parent->length - fd_buf_size) {\n\t\t/* No space for all file descriptors here. */\n\t\tbinder_user_error(\"%d:%d not enough space to store %lld fds in buffer\\n\",\n\t\t\t\t  proc->pid, thread->pid, (u64)fda->num_fds);\n\t\treturn -EINVAL;\n\t}\n\t/*\n\t * the source data for binder_buffer_object is visible\n\t * to user-space and the @buffer element is the user\n\t * pointer to the buffer_object containing the fd_array.\n\t * Convert the address to an offset relative to\n\t * the base of the transaction buffer.\n\t */\n\tfda_offset = (parent->buffer - (uintptr_t)t->buffer->user_data) +\n\t\tfda->parent_offset;\n\tif (!IS_ALIGNED((unsigned long)fda_offset, sizeof(u32))) {\n\t\tbinder_user_error(\"%d:%d parent offset not aligned correctly.\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\tfor (fdi = 0; fdi < fda->num_fds; fdi++) {\n\t\tu32 fd;\n\t\tint ret;\n\t\tbinder_size_t offset = fda_offset + fdi * sizeof(fd);\n\n\t\tret = binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t    &fd, t->buffer,\n\t\t\t\t\t\t    offset, sizeof(fd));\n\t\tif (!ret)\n\t\t\tret = binder_translate_fd(fd, offset, t, thread,\n\t\t\t\t\t\t  in_reply_to);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic int binder_fixup_parent(struct binder_transaction *t,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_buffer_object *bp,\n\t\t\t       binder_size_t off_start_offset,\n\t\t\t       binder_size_t num_valid,\n\t\t\t       binder_size_t last_fixup_obj_off,\n\t\t\t       binder_size_t last_fixup_min_off)\n{\n\tstruct binder_buffer_object *parent;\n\tstruct binder_buffer *b = t->buffer;\n\tstruct binder_proc *proc = thread->proc;\n\tstruct binder_proc *target_proc = t->to_proc;\n\tstruct binder_object object;\n\tbinder_size_t buffer_offset;\n\tbinder_size_t parent_offset;\n\n\tif (!(bp->flags & BINDER_BUFFER_FLAG_HAS_PARENT))\n\t\treturn 0;\n\n\tparent = binder_validate_ptr(target_proc, b, &object, bp->parent,\n\t\t\t\t     off_start_offset, &parent_offset,\n\t\t\t\t     num_valid);\n\tif (!parent) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!binder_validate_fixup(target_proc, b, off_start_offset,\n\t\t\t\t   parent_offset, bp->parent_offset,\n\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t   last_fixup_min_off)) {\n\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\n\tif (parent->length < sizeof(binder_uintptr_t) ||\n\t    bp->parent_offset > parent->length - sizeof(binder_uintptr_t)) {\n\t\t/* No space for a pointer here! */\n\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\tbuffer_offset = bp->parent_offset +\n\t\t\t(uintptr_t)parent->buffer - (uintptr_t)b->user_data;\n\tif (binder_alloc_copy_to_buffer(&target_proc->alloc, b, buffer_offset,\n\t\t\t\t\t&bp->buffer, sizeof(bp->buffer))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset\\n\",\n\t\t\t\t  proc->pid, thread->pid);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n/**\n * binder_proc_transaction() - sends a transaction to a process and wakes it up\n * @t:\t\ttransaction to send\n * @proc:\tprocess to send the transaction to\n * @thread:\tthread in @proc to send the transaction to (may be NULL)\n *\n * This function queues a transaction to the specified process. It will try\n * to find a thread in the target process to handle the transaction and\n * wake it up. If no thread is found, the work is queued to the proc\n * waitqueue.\n *\n * If the @thread parameter is not NULL, the transaction is always queued\n * to the waitlist of that specific thread.\n *\n * Return:\ttrue if the transactions was successfully queued\n *\t\tfalse if the target process or thread is dead\n */\nstatic bool binder_proc_transaction(struct binder_transaction *t,\n\t\t\t\t    struct binder_proc *proc,\n\t\t\t\t    struct binder_thread *thread)\n{\n\tstruct binder_node *node = t->buffer->target_node;\n\tbool oneway = !!(t->flags & TF_ONE_WAY);\n\tbool pending_async = false;\n\n\tBUG_ON(!node);\n\tbinder_node_lock(node);\n\tif (oneway) {\n\t\tBUG_ON(thread);\n\t\tif (node->has_async_transaction) {\n\t\t\tpending_async = true;\n\t\t} else {\n\t\t\tnode->has_async_transaction = true;\n\t\t}\n\t}\n\n\tbinder_inner_proc_lock(proc);\n\n\tif (proc->is_dead || (thread && thread->is_dead)) {\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_node_unlock(node);\n\t\treturn false;\n\t}\n\n\tif (!thread && !pending_async)\n\t\tthread = binder_select_thread_ilocked(proc);\n\n\tif (thread)\n\t\tbinder_enqueue_thread_work_ilocked(thread, &t->work);\n\telse if (!pending_async)\n\t\tbinder_enqueue_work_ilocked(&t->work, &proc->todo);\n\telse\n\t\tbinder_enqueue_work_ilocked(&t->work, &node->async_todo);\n\n\tif (!pending_async)\n\t\tbinder_wakeup_thread_ilocked(proc, thread, !oneway /* sync */);\n\n\tbinder_inner_proc_unlock(proc);\n\tbinder_node_unlock(node);\n\n\treturn true;\n}\n\n/**\n * binder_get_node_refs_for_txn() - Get required refs on node for txn\n * @node:         struct binder_node for which to get refs\n * @proc:         returns @node->proc if valid\n * @error:        if no @proc then returns BR_DEAD_REPLY\n *\n * User-space normally keeps the node alive when creating a transaction\n * since it has a reference to the target. The local strong ref keeps it\n * alive if the sending process dies before the target process processes\n * the transaction. If the source process is malicious or has a reference\n * counting bug, relying on the local strong ref can fail.\n *\n * Since user-space can cause the local strong ref to go away, we also take\n * a tmpref on the node to ensure it survives while we are constructing\n * the transaction. We also need a tmpref on the proc while we are\n * constructing the transaction, so we take that here as well.\n *\n * Return: The target_node with refs taken or NULL if no @node->proc is NULL.\n * Also sets @proc if valid. If the @node->proc is NULL indicating that the\n * target proc has died, @error is set to BR_DEAD_REPLY\n */\nstatic struct binder_node *binder_get_node_refs_for_txn(\n\t\tstruct binder_node *node,\n\t\tstruct binder_proc **procp,\n\t\tuint32_t *error)\n{\n\tstruct binder_node *target_node = NULL;\n\n\tbinder_node_inner_lock(node);\n\tif (node->proc) {\n\t\ttarget_node = node;\n\t\tbinder_inc_node_nilocked(node, 1, 0, NULL);\n\t\tbinder_inc_node_tmpref_ilocked(node);\n\t\tnode->proc->tmp_ref++;\n\t\t*procp = node->proc;\n\t} else\n\t\t*error = BR_DEAD_REPLY;\n\tbinder_node_inner_unlock(node);\n\n\treturn target_node;\n}\n\nstatic void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\tstrscpy(e->context_name, proc->context->name, BINDERFS_MAX_NAME);\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc->pid == proc->pid) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (WARN_ON(proc == target_proc)) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tif (security_binder_transaction(proc->cred,\n\t\t\t\t\t\ttarget_proc->cred) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\tsecurity_cred_getsecid(proc->cred, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tint err;\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\terr = binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  t->buffer, buf_offset,\n\t\t\t\t\t\t  secctx, secctx_sz);\n\t\tif (err) {\n\t\t\tt->security_ctx = 0;\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\tt->buffer->clear_on_free = !!(t->flags & TF_CLEAR_BUF);\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size -\n\t\tALIGN(secctx_sz, sizeof(u64));\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  &object_offset,\n\t\t\t\t\t\t  t->buffer,\n\t\t\t\t\t\t  buffer_offset,\n\t\t\t\t\t\t  sizeof(object_offset))) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tfp->pad_binder = 0;\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tbp, sizeof(*bp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, NULL, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}\n\n/**\n * binder_free_buf() - free the specified buffer\n * @proc:\tbinder proc that owns buffer\n * @buffer:\tbuffer to be freed\n * @is_failure:\tfailed to send transaction\n *\n * If buffer for an async transaction, enqueue the next async\n * transaction from the node.\n *\n * Cleanup buffer and free it.\n */\nstatic void\nbinder_free_buf(struct binder_proc *proc,\n\t\tstruct binder_thread *thread,\n\t\tstruct binder_buffer *buffer, bool is_failure)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}\n\nstatic int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node) {\n\t\t\t\t\tif (ctx_mgr_node->proc == proc) {\n\t\t\t\t\t\tbinder_user_error(\"%d:%d context manager tried to acquire desc 0\\n\",\n\t\t\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\t}\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\t}\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\t\t\tbool free_node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbinder_put_node(node);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_node_inner_lock(node);\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tfree_node = binder_dec_node_nilocked(node,\n\t\t\t\t\tcmd == BC_ACQUIRE_DONE, 0);\n\t\t\tWARN_ON(free_node);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d tr %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs,\n\t\t\t\t     node->local_weak_refs, node->tmp_refs);\n\t\t\tbinder_node_inner_unlock(node);\n\t\t\tbinder_put_node(node);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (IS_ERR_OR_NULL(buffer)) {\n\t\t\t\tif (PTR_ERR(buffer) == -EPERM) {\n\t\t\t\t\tbinder_user_error(\n\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned or currently freeing buffer\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)data_ptr);\n\t\t\t\t} else {\n\t\t\t\t\tbinder_user_error(\n\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)data_ptr);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\t\t\tbinder_free_buf(proc, thread, buffer, false);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\t/*\n\t\t\t\t * Allocate memory for death notification\n\t\t\t\t * before taking lock\n\t\t\t\t */\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tbinder_enqueue_thread_work(\n\t\t\t\t\t\tthread,\n\t\t\t\t\t\t&thread->return_error.work);\n\t\t\t\t\tbinder_debug(\n\t\t\t\t\t\tBINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t\"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tbinder_node_lock(ref->node);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tkfree(death);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\n\t\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t&ref->death->work, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper &\n\t\t\t\t\t    (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t     BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\t\t\tthread,\n\t\t\t\t\t\t\t\t&death->work);\n\t\t\t\t\telse {\n\t\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\t\tbinder_wakeup_proc_ilocked(\n\t\t\t\t\t\t\t\tproc);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tbinder_node_unlock(ref->node);\n\t\t\tbinder_proc_unlock(proc);\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death,\n\t\t\t\t\t    entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death =\n\t\t\t\t\tcontainer_of(w,\n\t\t\t\t\t\t     struct binder_ref_death,\n\t\t\t\t\t\t     work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_dequeue_work_ilocked(&death->work);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper &\n\t\t\t\t\t(BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\tthread, &death->work);\n\t\t\t\telse {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}\n\nstatic void binder_stat_br(struct binder_proc *proc,\n\t\t\t   struct binder_thread *thread, uint32_t cmd)\n{\n\ttrace_binder_return(cmd);\n\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.br)) {\n\t\tatomic_inc(&binder_stats.br[_IOC_NR(cmd)]);\n\t\tatomic_inc(&proc->stats.br[_IOC_NR(cmd)]);\n\t\tatomic_inc(&thread->stats.br[_IOC_NR(cmd)]);\n\t}\n}\n\nstatic int binder_put_node_cmd(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       void __user **ptrp,\n\t\t\t       binder_uintptr_t node_ptr,\n\t\t\t       binder_uintptr_t node_cookie,\n\t\t\t       int node_debug_id,\n\t\t\t       uint32_t cmd, const char *cmd_name)\n{\n\tvoid __user *ptr = *ptrp;\n\n\tif (put_user(cmd, (uint32_t __user *)ptr))\n\t\treturn -EFAULT;\n\tptr += sizeof(uint32_t);\n\n\tif (put_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\treturn -EFAULT;\n\tptr += sizeof(binder_uintptr_t);\n\n\tif (put_user(node_cookie, (binder_uintptr_t __user *)ptr))\n\t\treturn -EFAULT;\n\tptr += sizeof(binder_uintptr_t);\n\n\tbinder_stat_br(proc, thread, cmd);\n\tbinder_debug(BINDER_DEBUG_USER_REFS, \"%d:%d %s %d u%016llx c%016llx\\n\",\n\t\t     proc->pid, thread->pid, cmd_name, node_debug_id,\n\t\t     (u64)node_ptr, (u64)node_cookie);\n\n\t*ptrp = ptr;\n\treturn 0;\n}\n\nstatic int binder_wait_for_work(struct binder_thread *thread,\n\t\t\t\tbool do_proc_work)\n{\n\tDEFINE_WAIT(wait);\n\tstruct binder_proc *proc = thread->proc;\n\tint ret = 0;\n\n\tfreezer_do_not_count();\n\tbinder_inner_proc_lock(proc);\n\tfor (;;) {\n\t\tprepare_to_wait(&thread->wait, &wait, TASK_INTERRUPTIBLE);\n\t\tif (binder_has_work_ilocked(thread, do_proc_work))\n\t\t\tbreak;\n\t\tif (do_proc_work)\n\t\t\tlist_add(&thread->waiting_thread_node,\n\t\t\t\t &proc->waiting_threads);\n\t\tbinder_inner_proc_unlock(proc);\n\t\tschedule();\n\t\tbinder_inner_proc_lock(proc);\n\t\tlist_del_init(&thread->waiting_thread_node);\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t}\n\tfinish_wait(&thread->wait, &wait);\n\tbinder_inner_proc_unlock(proc);\n\tfreezer_count();\n\n\treturn ret;\n}\n\n/**\n * binder_apply_fd_fixups() - finish fd translation\n * @proc:         binder_proc associated @t->buffer\n * @t:\tbinder transaction with list of fd fixups\n *\n * Now that we are in the context of the transaction target\n * process, we can allocate and install fds. Process the\n * list of fds to translate and fixup the buffer with the\n * new fds.\n *\n * If we fail to allocate an fd, then free the resources by\n * fput'ing files that have not been processed and ksys_close'ing\n * any fds that have already been allocated.\n */\nstatic int binder_apply_fd_fixups(struct binder_proc *proc,\n\t\t\t\t  struct binder_transaction *t)\n{\n\tstruct binder_txn_fd_fixup *fixup, *tmp;\n\tint ret = 0;\n\n\tlist_for_each_entry(fixup, &t->fd_fixups, fixup_entry) {\n\t\tint fd = get_unused_fd_flags(O_CLOEXEC);\n\n\t\tif (fd < 0) {\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t     \"failed fd fixup txn %d fd %d\\n\",\n\t\t\t\t     t->debug_id, fd);\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"fd fixup txn %d fd %d\\n\",\n\t\t\t     t->debug_id, fd);\n\t\ttrace_binder_transaction_fd_recv(t, fd, fixup->offset);\n\t\tfd_install(fd, fixup->file);\n\t\tfixup->file = NULL;\n\t\tif (binder_alloc_copy_to_buffer(&proc->alloc, t->buffer,\n\t\t\t\t\t\tfixup->offset, &fd,\n\t\t\t\t\t\tsizeof(u32))) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\tlist_for_each_entry_safe(fixup, tmp, &t->fd_fixups, fixup_entry) {\n\t\tif (fixup->file) {\n\t\t\tfput(fixup->file);\n\t\t} else if (ret) {\n\t\t\tu32 fd;\n\t\t\tint err;\n\n\t\t\terr = binder_alloc_copy_from_buffer(&proc->alloc, &fd,\n\t\t\t\t\t\t\t    t->buffer,\n\t\t\t\t\t\t\t    fixup->offset,\n\t\t\t\t\t\t\t    sizeof(fd));\n\t\t\tWARN_ON(err);\n\t\t\tif (!err)\n\t\t\t\tbinder_deferred_fd_close(fd);\n\t\t}\n\t\tlist_del(&fixup->fixup_entry);\n\t\tkfree(fixup);\n\t}\n\n\treturn ret;\n}\n\nstatic int binder_thread_read(struct binder_proc *proc,\n\t\t\t      struct binder_thread *thread,\n\t\t\t      binder_uintptr_t binder_buffer, size_t size,\n\t\t\t      binder_size_t *consumed, int non_block)\n{\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\tint ret = 0;\n\tint wait_for_proc_work;\n\n\tif (*consumed == 0) {\n\t\tif (put_user(BR_NOOP, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t}\n\nretry:\n\tbinder_inner_proc_lock(proc);\n\twait_for_proc_work = binder_available_for_proc_work_ilocked(thread);\n\tbinder_inner_proc_unlock(proc);\n\n\tthread->looper |= BINDER_LOOPER_STATE_WAITING;\n\n\ttrace_binder_wait_for_work(wait_for_proc_work,\n\t\t\t\t   !!thread->transaction_stack,\n\t\t\t\t   !binder_worklist_empty(proc, &thread->todo));\n\tif (wait_for_proc_work) {\n\t\tif (!(thread->looper & (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\tBINDER_LOOPER_STATE_ENTERED))) {\n\t\t\tbinder_user_error(\"%d:%d ERROR: Thread waiting for process work before calling BC_REGISTER_LOOPER or BC_ENTER_LOOPER (state %x)\\n\",\n\t\t\t\tproc->pid, thread->pid, thread->looper);\n\t\t\twait_event_interruptible(binder_user_error_wait,\n\t\t\t\t\t\t binder_stop_on_user_error < 2);\n\t\t}\n\t\tbinder_set_nice(proc->default_priority);\n\t}\n\n\tif (non_block) {\n\t\tif (!binder_has_work(thread, wait_for_proc_work))\n\t\t\tret = -EAGAIN;\n\t} else {\n\t\tret = binder_wait_for_work(thread, wait_for_proc_work);\n\t}\n\n\tthread->looper &= ~BINDER_LOOPER_STATE_WAITING;\n\n\tif (ret)\n\t\treturn ret;\n\n\twhile (1) {\n\t\tuint32_t cmd;\n\t\tstruct binder_transaction_data_secctx tr;\n\t\tstruct binder_transaction_data *trd = &tr.transaction_data;\n\t\tstruct binder_work *w = NULL;\n\t\tstruct list_head *list = NULL;\n\t\tstruct binder_transaction *t = NULL;\n\t\tstruct binder_thread *t_from;\n\t\tsize_t trsize = sizeof(*trd);\n\n\t\tbinder_inner_proc_lock(proc);\n\t\tif (!binder_worklist_empty_ilocked(&thread->todo))\n\t\t\tlist = &thread->todo;\n\t\telse if (!binder_worklist_empty_ilocked(&proc->todo) &&\n\t\t\t   wait_for_proc_work)\n\t\t\tlist = &proc->todo;\n\t\telse {\n\t\t\tbinder_inner_proc_unlock(proc);\n\n\t\t\t/* no data added */\n\t\t\tif (ptr - buffer == 4 && !thread->looper_need_return)\n\t\t\t\tgoto retry;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (end - ptr < sizeof(tr) + 4) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\t}\n\t\tw = binder_dequeue_work_head_ilocked(list);\n\t\tif (binder_worklist_empty_ilocked(&thread->todo))\n\t\t\tthread->process_todo = false;\n\n\t\tswitch (w->type) {\n\t\tcase BINDER_WORK_TRANSACTION: {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tt = container_of(w, struct binder_transaction, work);\n\t\t} break;\n\t\tcase BINDER_WORK_RETURN_ERROR: {\n\t\t\tstruct binder_error *e = container_of(\n\t\t\t\t\tw, struct binder_error, work);\n\n\t\t\tWARN_ON(e->cmd == BR_OK);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tif (put_user(e->cmd, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tcmd = e->cmd;\n\t\t\te->cmd = BR_OK;\n\t\t\tptr += sizeof(uint32_t);\n\n\t\t\tbinder_stat_br(proc, thread, cmd);\n\t\t} break;\n\t\tcase BINDER_WORK_TRANSACTION_COMPLETE: {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tcmd = BR_TRANSACTION_COMPLETE;\n\t\t\tkfree(w);\n\t\t\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\n\t\t\tif (put_user(cmd, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\n\t\t\tbinder_stat_br(proc, thread, cmd);\n\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION_COMPLETE,\n\t\t\t\t     \"%d:%d BR_TRANSACTION_COMPLETE\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t} break;\n\t\tcase BINDER_WORK_NODE: {\n\t\t\tstruct binder_node *node = container_of(w, struct binder_node, work);\n\t\t\tint strong, weak;\n\t\t\tbinder_uintptr_t node_ptr = node->ptr;\n\t\t\tbinder_uintptr_t node_cookie = node->cookie;\n\t\t\tint node_debug_id = node->debug_id;\n\t\t\tint has_weak_ref;\n\t\t\tint has_strong_ref;\n\t\t\tvoid __user *orig_ptr = ptr;\n\n\t\t\tBUG_ON(proc != node->proc);\n\t\t\tstrong = node->internal_strong_refs ||\n\t\t\t\t\tnode->local_strong_refs;\n\t\t\tweak = !hlist_empty(&node->refs) ||\n\t\t\t\t\tnode->local_weak_refs ||\n\t\t\t\t\tnode->tmp_refs || strong;\n\t\t\thas_strong_ref = node->has_strong_ref;\n\t\t\thas_weak_ref = node->has_weak_ref;\n\n\t\t\tif (weak && !has_weak_ref) {\n\t\t\t\tnode->has_weak_ref = 1;\n\t\t\t\tnode->pending_weak_ref = 1;\n\t\t\t\tnode->local_weak_refs++;\n\t\t\t}\n\t\t\tif (strong && !has_strong_ref) {\n\t\t\t\tnode->has_strong_ref = 1;\n\t\t\t\tnode->pending_strong_ref = 1;\n\t\t\t\tnode->local_strong_refs++;\n\t\t\t}\n\t\t\tif (!strong && has_strong_ref)\n\t\t\t\tnode->has_strong_ref = 0;\n\t\t\tif (!weak && has_weak_ref)\n\t\t\t\tnode->has_weak_ref = 0;\n\t\t\tif (!weak && !strong) {\n\t\t\t\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t\t\t\t     \"%d:%d node %d u%016llx c%016llx deleted\\n\",\n\t\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t\t     node_debug_id,\n\t\t\t\t\t     (u64)node_ptr,\n\t\t\t\t\t     (u64)node_cookie);\n\t\t\t\trb_erase(&node->rb_node, &proc->nodes);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbinder_node_lock(node);\n\t\t\t\t/*\n\t\t\t\t * Acquire the node lock before freeing the\n\t\t\t\t * node to serialize with other threads that\n\t\t\t\t * may have been holding the node lock while\n\t\t\t\t * decrementing this node (avoids race where\n\t\t\t\t * this thread frees while the other thread\n\t\t\t\t * is unlocking the node after the final\n\t\t\t\t * decrement)\n\t\t\t\t */\n\t\t\t\tbinder_node_unlock(node);\n\t\t\t\tbinder_free_node(node);\n\t\t\t} else\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\n\t\t\tif (weak && !has_weak_ref)\n\t\t\t\tret = binder_put_node_cmd(\n\t\t\t\t\t\tproc, thread, &ptr, node_ptr,\n\t\t\t\t\t\tnode_cookie, node_debug_id,\n\t\t\t\t\t\tBR_INCREFS, \"BR_INCREFS\");\n\t\t\tif (!ret && strong && !has_strong_ref)\n\t\t\t\tret = binder_put_node_cmd(\n\t\t\t\t\t\tproc, thread, &ptr, node_ptr,\n\t\t\t\t\t\tnode_cookie, node_debug_id,\n\t\t\t\t\t\tBR_ACQUIRE, \"BR_ACQUIRE\");\n\t\t\tif (!ret && !strong && has_strong_ref)\n\t\t\t\tret = binder_put_node_cmd(\n\t\t\t\t\t\tproc, thread, &ptr, node_ptr,\n\t\t\t\t\t\tnode_cookie, node_debug_id,\n\t\t\t\t\t\tBR_RELEASE, \"BR_RELEASE\");\n\t\t\tif (!ret && !weak && has_weak_ref)\n\t\t\t\tret = binder_put_node_cmd(\n\t\t\t\t\t\tproc, thread, &ptr, node_ptr,\n\t\t\t\t\t\tnode_cookie, node_debug_id,\n\t\t\t\t\t\tBR_DECREFS, \"BR_DECREFS\");\n\t\t\tif (orig_ptr == ptr)\n\t\t\t\tbinder_debug(BINDER_DEBUG_INTERNAL_REFS,\n\t\t\t\t\t     \"%d:%d node %d u%016llx c%016llx state unchanged\\n\",\n\t\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t\t     node_debug_id,\n\t\t\t\t\t     (u64)node_ptr,\n\t\t\t\t\t     (u64)node_cookie);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t} break;\n\t\tcase BINDER_WORK_DEAD_BINDER:\n\t\tcase BINDER_WORK_DEAD_BINDER_AND_CLEAR:\n\t\tcase BINDER_WORK_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tstruct binder_ref_death *death;\n\t\t\tuint32_t cmd;\n\t\t\tbinder_uintptr_t cookie;\n\n\t\t\tdeath = container_of(w, struct binder_ref_death, work);\n\t\t\tif (w->type == BINDER_WORK_CLEAR_DEATH_NOTIFICATION)\n\t\t\t\tcmd = BR_CLEAR_DEATH_NOTIFICATION_DONE;\n\t\t\telse\n\t\t\t\tcmd = BR_DEAD_BINDER;\n\t\t\tcookie = death->cookie;\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx\\n\",\n\t\t\t\t      proc->pid, thread->pid,\n\t\t\t\t      cmd == BR_DEAD_BINDER ?\n\t\t\t\t      \"BR_DEAD_BINDER\" :\n\t\t\t\t      \"BR_CLEAR_DEATH_NOTIFICATION_DONE\",\n\t\t\t\t      (u64)cookie);\n\t\t\tif (w->type == BINDER_WORK_CLEAR_DEATH_NOTIFICATION) {\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbinder_stats_deleted(BINDER_STAT_DEATH);\n\t\t\t} else {\n\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\tw, &proc->delivered_death);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tif (put_user(cmd, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (put_user(cookie,\n\t\t\t\t     (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tbinder_stat_br(proc, thread, cmd);\n\t\t\tif (cmd == BR_DEAD_BINDER)\n\t\t\t\tgoto done; /* DEAD_BINDER notifications can cause transactions */\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tpr_err(\"%d:%d: bad work type %d\\n\",\n\t\t\t       proc->pid, thread->pid, w->type);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!t)\n\t\t\tcontinue;\n\n\t\tBUG_ON(t->buffer == NULL);\n\t\tif (t->buffer->target_node) {\n\t\t\tstruct binder_node *target_node = t->buffer->target_node;\n\n\t\t\ttrd->target.ptr = target_node->ptr;\n\t\t\ttrd->cookie =  target_node->cookie;\n\t\t\tt->saved_priority = task_nice(current);\n\t\t\tif (t->priority < target_node->min_priority &&\n\t\t\t    !(t->flags & TF_ONE_WAY))\n\t\t\t\tbinder_set_nice(t->priority);\n\t\t\telse if (!(t->flags & TF_ONE_WAY) ||\n\t\t\t\t t->saved_priority > target_node->min_priority)\n\t\t\t\tbinder_set_nice(target_node->min_priority);\n\t\t\tcmd = BR_TRANSACTION;\n\t\t} else {\n\t\t\ttrd->target.ptr = 0;\n\t\t\ttrd->cookie = 0;\n\t\t\tcmd = BR_REPLY;\n\t\t}\n\t\ttrd->code = t->code;\n\t\ttrd->flags = t->flags;\n\t\ttrd->sender_euid = from_kuid(current_user_ns(), t->sender_euid);\n\n\t\tt_from = binder_get_txn_from(t);\n\t\tif (t_from) {\n\t\t\tstruct task_struct *sender = t_from->proc->tsk;\n\n\t\t\ttrd->sender_pid =\n\t\t\t\ttask_tgid_nr_ns(sender,\n\t\t\t\t\t\ttask_active_pid_ns(current));\n\t\t} else {\n\t\t\ttrd->sender_pid = 0;\n\t\t}\n\n\t\tret = binder_apply_fd_fixups(proc, t);\n\t\tif (ret) {\n\t\t\tstruct binder_buffer *buffer = t->buffer;\n\t\t\tbool oneway = !!(t->flags & TF_ONE_WAY);\n\t\t\tint tid = t->debug_id;\n\n\t\t\tif (t_from)\n\t\t\t\tbinder_thread_dec_tmpref(t_from);\n\t\t\tbuffer->transaction = NULL;\n\t\t\tbinder_cleanup_transaction(t, \"fd fixups failed\",\n\t\t\t\t\t\t   BR_FAILED_REPLY);\n\t\t\tbinder_free_buf(proc, thread, buffer, true);\n\t\t\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t     \"%d:%d %stransaction %d fd fixups failed %d/%d, line %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     oneway ? \"async \" :\n\t\t\t\t\t(cmd == BR_REPLY ? \"reply \" : \"\"),\n\t\t\t\t     tid, BR_FAILED_REPLY, ret, __LINE__);\n\t\t\tif (cmd == BR_REPLY) {\n\t\t\t\tcmd = BR_FAILED_REPLY;\n\t\t\t\tif (put_user(cmd, (uint32_t __user *)ptr))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\tptr += sizeof(uint32_t);\n\t\t\t\tbinder_stat_br(proc, thread, cmd);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\ttrd->data_size = t->buffer->data_size;\n\t\ttrd->offsets_size = t->buffer->offsets_size;\n\t\ttrd->data.ptr.buffer = (uintptr_t)t->buffer->user_data;\n\t\ttrd->data.ptr.offsets = trd->data.ptr.buffer +\n\t\t\t\t\tALIGN(t->buffer->data_size,\n\t\t\t\t\t    sizeof(void *));\n\n\t\ttr.secctx = t->security_ctx;\n\t\tif (t->security_ctx) {\n\t\t\tcmd = BR_TRANSACTION_SEC_CTX;\n\t\t\ttrsize = sizeof(tr);\n\t\t}\n\t\tif (put_user(cmd, (uint32_t __user *)ptr)) {\n\t\t\tif (t_from)\n\t\t\t\tbinder_thread_dec_tmpref(t_from);\n\n\t\t\tbinder_cleanup_transaction(t, \"put_user failed\",\n\t\t\t\t\t\t   BR_FAILED_REPLY);\n\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tptr += sizeof(uint32_t);\n\t\tif (copy_to_user(ptr, &tr, trsize)) {\n\t\t\tif (t_from)\n\t\t\t\tbinder_thread_dec_tmpref(t_from);\n\n\t\t\tbinder_cleanup_transaction(t, \"copy_to_user failed\",\n\t\t\t\t\t\t   BR_FAILED_REPLY);\n\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tptr += trsize;\n\n\t\ttrace_binder_transaction_received(t);\n\t\tbinder_stat_br(proc, thread, cmd);\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d %s %d %d:%d, cmd %d size %zd-%zd ptr %016llx-%016llx\\n\",\n\t\t\t     proc->pid, thread->pid,\n\t\t\t     (cmd == BR_TRANSACTION) ? \"BR_TRANSACTION\" :\n\t\t\t\t(cmd == BR_TRANSACTION_SEC_CTX) ?\n\t\t\t\t     \"BR_TRANSACTION_SEC_CTX\" : \"BR_REPLY\",\n\t\t\t     t->debug_id, t_from ? t_from->proc->pid : 0,\n\t\t\t     t_from ? t_from->pid : 0, cmd,\n\t\t\t     t->buffer->data_size, t->buffer->offsets_size,\n\t\t\t     (u64)trd->data.ptr.buffer,\n\t\t\t     (u64)trd->data.ptr.offsets);\n\n\t\tif (t_from)\n\t\t\tbinder_thread_dec_tmpref(t_from);\n\t\tt->buffer->allow_user_free = 1;\n\t\tif (cmd != BR_REPLY && !(t->flags & TF_ONE_WAY)) {\n\t\t\tbinder_inner_proc_lock(thread->proc);\n\t\t\tt->to_parent = thread->transaction_stack;\n\t\t\tt->to_thread = thread;\n\t\t\tthread->transaction_stack = t;\n\t\t\tbinder_inner_proc_unlock(thread->proc);\n\t\t} else {\n\t\t\tbinder_free_transaction(t);\n\t\t}\n\t\tbreak;\n\t}\n\ndone:\n\n\t*consumed = ptr - buffer;\n\tbinder_inner_proc_lock(proc);\n\tif (proc->requested_threads == 0 &&\n\t    list_empty(&thread->proc->waiting_threads) &&\n\t    proc->requested_threads_started < proc->max_threads &&\n\t    (thread->looper & (BINDER_LOOPER_STATE_REGISTERED |\n\t     BINDER_LOOPER_STATE_ENTERED)) /* the user-space code fails to */\n\t     /*spawn a new thread if we leave this out */) {\n\t\tproc->requested_threads++;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t     \"%d:%d BR_SPAWN_LOOPER\\n\",\n\t\t\t     proc->pid, thread->pid);\n\t\tif (put_user(BR_SPAWN_LOOPER, (uint32_t __user *)buffer))\n\t\t\treturn -EFAULT;\n\t\tbinder_stat_br(proc, thread, BR_SPAWN_LOOPER);\n\t} else\n\t\tbinder_inner_proc_unlock(proc);\n\treturn 0;\n}\n\nstatic void binder_release_work(struct binder_proc *proc,\n\t\t\t\tstruct list_head *list)\n{\n\tstruct binder_work *w;\n\tenum binder_work_type wtype;\n\n\twhile (1) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tw = binder_dequeue_work_head_ilocked(list);\n\t\twtype = w ? w->type : 0;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!w)\n\t\t\treturn;\n\n\t\tswitch (wtype) {\n\t\tcase BINDER_WORK_TRANSACTION: {\n\t\t\tstruct binder_transaction *t;\n\n\t\t\tt = container_of(w, struct binder_transaction, work);\n\n\t\t\tbinder_cleanup_transaction(t, \"process died.\",\n\t\t\t\t\t\t   BR_DEAD_REPLY);\n\t\t} break;\n\t\tcase BINDER_WORK_RETURN_ERROR: {\n\t\t\tstruct binder_error *e = container_of(\n\t\t\t\t\tw, struct binder_error, work);\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered TRANSACTION_ERROR: %u\\n\",\n\t\t\t\te->cmd);\n\t\t} break;\n\t\tcase BINDER_WORK_TRANSACTION_COMPLETE: {\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered TRANSACTION_COMPLETE\\n\");\n\t\t\tkfree(w);\n\t\t\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\n\t\t} break;\n\t\tcase BINDER_WORK_DEAD_BINDER_AND_CLEAR:\n\t\tcase BINDER_WORK_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tstruct binder_ref_death *death;\n\n\t\t\tdeath = container_of(w, struct binder_ref_death, work);\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t\t\"undelivered death notification, %016llx\\n\",\n\t\t\t\t(u64)death->cookie);\n\t\t\tkfree(death);\n\t\t\tbinder_stats_deleted(BINDER_STAT_DEATH);\n\t\t} break;\n\t\tcase BINDER_WORK_NODE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"unexpected work type, %d, not freed\\n\",\n\t\t\t       wtype);\n\t\t\tbreak;\n\t\t}\n\t}\n\n}\n\nstatic struct binder_thread *binder_get_thread_ilocked(\n\t\tstruct binder_proc *proc, struct binder_thread *new_thread)\n{\n\tstruct binder_thread *thread = NULL;\n\tstruct rb_node *parent = NULL;\n\tstruct rb_node **p = &proc->threads.rb_node;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tthread = rb_entry(parent, struct binder_thread, rb_node);\n\n\t\tif (current->pid < thread->pid)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (current->pid > thread->pid)\n\t\t\tp = &(*p)->rb_right;\n\t\telse\n\t\t\treturn thread;\n\t}\n\tif (!new_thread)\n\t\treturn NULL;\n\tthread = new_thread;\n\tbinder_stats_created(BINDER_STAT_THREAD);\n\tthread->proc = proc;\n\tthread->pid = current->pid;\n\tatomic_set(&thread->tmp_ref, 0);\n\tinit_waitqueue_head(&thread->wait);\n\tINIT_LIST_HEAD(&thread->todo);\n\trb_link_node(&thread->rb_node, parent, p);\n\trb_insert_color(&thread->rb_node, &proc->threads);\n\tthread->looper_need_return = true;\n\tthread->return_error.work.type = BINDER_WORK_RETURN_ERROR;\n\tthread->return_error.cmd = BR_OK;\n\tthread->reply_error.work.type = BINDER_WORK_RETURN_ERROR;\n\tthread->reply_error.cmd = BR_OK;\n\tINIT_LIST_HEAD(&new_thread->waiting_thread_node);\n\treturn thread;\n}\n\nstatic struct binder_thread *binder_get_thread(struct binder_proc *proc)\n{\n\tstruct binder_thread *thread;\n\tstruct binder_thread *new_thread;\n\n\tbinder_inner_proc_lock(proc);\n\tthread = binder_get_thread_ilocked(proc, NULL);\n\tbinder_inner_proc_unlock(proc);\n\tif (!thread) {\n\t\tnew_thread = kzalloc(sizeof(*thread), GFP_KERNEL);\n\t\tif (new_thread == NULL)\n\t\t\treturn NULL;\n\t\tbinder_inner_proc_lock(proc);\n\t\tthread = binder_get_thread_ilocked(proc, new_thread);\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (thread != new_thread)\n\t\t\tkfree(new_thread);\n\t}\n\treturn thread;\n}\n\nstatic void binder_free_proc(struct binder_proc *proc)\n{\n\tstruct binder_device *device;\n\n\tBUG_ON(!list_empty(&proc->todo));\n\tBUG_ON(!list_empty(&proc->delivered_death));\n\tdevice = container_of(proc->context, struct binder_device, context);\n\tif (refcount_dec_and_test(&device->ref)) {\n\t\tkfree(proc->context->name);\n\t\tkfree(device);\n\t}\n\tbinder_alloc_deferred_release(&proc->alloc);\n\tput_task_struct(proc->tsk);\n\tput_cred(proc->cred);\n\tbinder_stats_deleted(BINDER_STAT_PROC);\n\tkfree(proc);\n}\n\nstatic void binder_free_thread(struct binder_thread *thread)\n{\n\tBUG_ON(!list_empty(&thread->todo));\n\tbinder_stats_deleted(BINDER_STAT_THREAD);\n\tbinder_proc_dec_tmpref(thread->proc);\n\tkfree(thread);\n}\n\nstatic int binder_thread_release(struct binder_proc *proc,\n\t\t\t\t struct binder_thread *thread)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_transaction *send_reply = NULL;\n\tint active_transactions = 0;\n\tstruct binder_transaction *last_t = NULL;\n\n\tbinder_inner_proc_lock(thread->proc);\n\t/*\n\t * take a ref on the proc so it survives\n\t * after we remove this thread from proc->threads.\n\t * The corresponding dec is when we actually\n\t * free the thread in binder_free_thread()\n\t */\n\tproc->tmp_ref++;\n\t/*\n\t * take a ref on this thread to ensure it\n\t * survives while we are releasing it\n\t */\n\tatomic_inc(&thread->tmp_ref);\n\trb_erase(&thread->rb_node, &proc->threads);\n\tt = thread->transaction_stack;\n\tif (t) {\n\t\tspin_lock(&t->lock);\n\t\tif (t->to_thread == thread)\n\t\t\tsend_reply = t;\n\t} else {\n\t\t__acquire(&t->lock);\n\t}\n\tthread->is_dead = true;\n\n\twhile (t) {\n\t\tlast_t = t;\n\t\tactive_transactions++;\n\t\tbinder_debug(BINDER_DEBUG_DEAD_TRANSACTION,\n\t\t\t     \"release %d:%d transaction %d %s, still active\\n\",\n\t\t\t      proc->pid, thread->pid,\n\t\t\t     t->debug_id,\n\t\t\t     (t->to_thread == thread) ? \"in\" : \"out\");\n\n\t\tif (t->to_thread == thread) {\n\t\t\tt->to_proc = NULL;\n\t\t\tt->to_thread = NULL;\n\t\t\tif (t->buffer) {\n\t\t\t\tt->buffer->transaction = NULL;\n\t\t\t\tt->buffer = NULL;\n\t\t\t}\n\t\t\tt = t->to_parent;\n\t\t} else if (t->from == thread) {\n\t\t\tt->from = NULL;\n\t\t\tt = t->from_parent;\n\t\t} else\n\t\t\tBUG();\n\t\tspin_unlock(&last_t->lock);\n\t\tif (t)\n\t\t\tspin_lock(&t->lock);\n\t\telse\n\t\t\t__acquire(&t->lock);\n\t}\n\t/* annotation for sparse, lock not acquired in last iteration above */\n\t__release(&t->lock);\n\n\t/*\n\t * If this thread used poll, make sure we remove the waitqueue\n\t * from any epoll data structures holding it with POLLFREE.\n\t * waitqueue_active() is safe to use here because we're holding\n\t * the inner lock.\n\t */\n\tif ((thread->looper & BINDER_LOOPER_STATE_POLL) &&\n\t    waitqueue_active(&thread->wait)) {\n\t\twake_up_poll(&thread->wait, EPOLLHUP | POLLFREE);\n\t}\n\n\tbinder_inner_proc_unlock(thread->proc);\n\n\t/*\n\t * This is needed to avoid races between wake_up_poll() above and\n\t * and ep_remove_waitqueue() called for other reasons (eg the epoll file\n\t * descriptor being closed); ep_remove_waitqueue() holds an RCU read\n\t * lock, so we can be sure it's done after calling synchronize_rcu().\n\t */\n\tif (thread->looper & BINDER_LOOPER_STATE_POLL)\n\t\tsynchronize_rcu();\n\n\tif (send_reply)\n\t\tbinder_send_failed_reply(send_reply, BR_DEAD_REPLY);\n\tbinder_release_work(proc, &thread->todo);\n\tbinder_thread_dec_tmpref(thread);\n\treturn active_transactions;\n}\n\nstatic __poll_t binder_poll(struct file *filp,\n\t\t\t\tstruct poll_table_struct *wait)\n{\n\tstruct binder_proc *proc = filp->private_data;\n\tstruct binder_thread *thread = NULL;\n\tbool wait_for_proc_work;\n\n\tthread = binder_get_thread(proc);\n\tif (!thread)\n\t\treturn POLLERR;\n\n\tbinder_inner_proc_lock(thread->proc);\n\tthread->looper |= BINDER_LOOPER_STATE_POLL;\n\twait_for_proc_work = binder_available_for_proc_work_ilocked(thread);\n\n\tbinder_inner_proc_unlock(thread->proc);\n\n\tpoll_wait(filp, &thread->wait, wait);\n\n\tif (binder_has_work(thread, wait_for_proc_work))\n\t\treturn EPOLLIN;\n\n\treturn 0;\n}\n\nstatic int binder_ioctl_write_read(struct file *filp,\n\t\t\t\tunsigned int cmd, unsigned long arg,\n\t\t\t\tstruct binder_thread *thread)\n{\n\tint ret = 0;\n\tstruct binder_proc *proc = filp->private_data;\n\tunsigned int size = _IOC_SIZE(cmd);\n\tvoid __user *ubuf = (void __user *)arg;\n\tstruct binder_write_read bwr;\n\n\tif (size != sizeof(struct binder_write_read)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (copy_from_user(&bwr, ubuf, sizeof(bwr))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\tbinder_debug(BINDER_DEBUG_READ_WRITE,\n\t\t     \"%d:%d write %lld at %016llx, read %lld at %016llx\\n\",\n\t\t     proc->pid, thread->pid,\n\t\t     (u64)bwr.write_size, (u64)bwr.write_buffer,\n\t\t     (u64)bwr.read_size, (u64)bwr.read_buffer);\n\n\tif (bwr.write_size > 0) {\n\t\tret = binder_thread_write(proc, thread,\n\t\t\t\t\t  bwr.write_buffer,\n\t\t\t\t\t  bwr.write_size,\n\t\t\t\t\t  &bwr.write_consumed);\n\t\ttrace_binder_write_done(ret);\n\t\tif (ret < 0) {\n\t\t\tbwr.read_consumed = 0;\n\t\t\tif (copy_to_user(ubuf, &bwr, sizeof(bwr)))\n\t\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif (bwr.read_size > 0) {\n\t\tret = binder_thread_read(proc, thread, bwr.read_buffer,\n\t\t\t\t\t bwr.read_size,\n\t\t\t\t\t &bwr.read_consumed,\n\t\t\t\t\t filp->f_flags & O_NONBLOCK);\n\t\ttrace_binder_read_done(ret);\n\t\tbinder_inner_proc_lock(proc);\n\t\tif (!binder_worklist_empty_ilocked(&proc->todo))\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (ret < 0) {\n\t\t\tif (copy_to_user(ubuf, &bwr, sizeof(bwr)))\n\t\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tbinder_debug(BINDER_DEBUG_READ_WRITE,\n\t\t     \"%d:%d wrote %lld of %lld, read return %lld of %lld\\n\",\n\t\t     proc->pid, thread->pid,\n\t\t     (u64)bwr.write_consumed, (u64)bwr.write_size,\n\t\t     (u64)bwr.read_consumed, (u64)bwr.read_size);\n\tif (copy_to_user(ubuf, &bwr, sizeof(bwr))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int binder_ioctl_set_ctx_mgr(struct file *filp,\n\t\t\t\t    struct flat_binder_object *fbo)\n{\n\tint ret = 0;\n\tstruct binder_proc *proc = filp->private_data;\n\tstruct binder_context *context = proc->context;\n\tstruct binder_node *new_node;\n\tkuid_t curr_euid = current_euid();\n\n\tmutex_lock(&context->context_mgr_node_lock);\n\tif (context->binder_context_mgr_node) {\n\t\tpr_err(\"BINDER_SET_CONTEXT_MGR already set\\n\");\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\tret = security_binder_set_context_mgr(proc->cred);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (uid_valid(context->binder_context_mgr_uid)) {\n\t\tif (!uid_eq(context->binder_context_mgr_uid, curr_euid)) {\n\t\t\tpr_err(\"BINDER_SET_CONTEXT_MGR bad uid %d != %d\\n\",\n\t\t\t       from_kuid(&init_user_ns, curr_euid),\n\t\t\t       from_kuid(&init_user_ns,\n\t\t\t\t\t context->binder_context_mgr_uid));\n\t\t\tret = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tcontext->binder_context_mgr_uid = curr_euid;\n\t}\n\tnew_node = binder_new_node(proc, fbo);\n\tif (!new_node) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tbinder_node_lock(new_node);\n\tnew_node->local_weak_refs++;\n\tnew_node->local_strong_refs++;\n\tnew_node->has_strong_ref = 1;\n\tnew_node->has_weak_ref = 1;\n\tcontext->binder_context_mgr_node = new_node;\n\tbinder_node_unlock(new_node);\n\tbinder_put_node(new_node);\nout:\n\tmutex_unlock(&context->context_mgr_node_lock);\n\treturn ret;\n}\n\nstatic int binder_ioctl_get_node_info_for_ref(struct binder_proc *proc,\n\t\tstruct binder_node_info_for_ref *info)\n{\n\tstruct binder_node *node;\n\tstruct binder_context *context = proc->context;\n\t__u32 handle = info->handle;\n\n\tif (info->strong_count || info->weak_count || info->reserved1 ||\n\t    info->reserved2 || info->reserved3) {\n\t\tbinder_user_error(\"%d BINDER_GET_NODE_INFO_FOR_REF: only handle may be non-zero.\",\n\t\t\t\t  proc->pid);\n\t\treturn -EINVAL;\n\t}\n\n\t/* This ioctl may only be used by the context manager */\n\tmutex_lock(&context->context_mgr_node_lock);\n\tif (!context->binder_context_mgr_node ||\n\t\tcontext->binder_context_mgr_node->proc != proc) {\n\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\treturn -EPERM;\n\t}\n\tmutex_unlock(&context->context_mgr_node_lock);\n\n\tnode = binder_get_node_from_ref(proc, handle, true, NULL);\n\tif (!node)\n\t\treturn -EINVAL;\n\n\tinfo->strong_count = node->local_strong_refs +\n\t\tnode->internal_strong_refs;\n\tinfo->weak_count = node->local_weak_refs;\n\n\tbinder_put_node(node);\n\n\treturn 0;\n}\n\nstatic int binder_ioctl_get_node_debug_info(struct binder_proc *proc,\n\t\t\t\tstruct binder_node_debug_info *info)\n{\n\tstruct rb_node *n;\n\tbinder_uintptr_t ptr = info->ptr;\n\n\tmemset(info, 0, sizeof(*info));\n\n\tbinder_inner_proc_lock(proc);\n\tfor (n = rb_first(&proc->nodes); n != NULL; n = rb_next(n)) {\n\t\tstruct binder_node *node = rb_entry(n, struct binder_node,\n\t\t\t\t\t\t    rb_node);\n\t\tif (node->ptr > ptr) {\n\t\t\tinfo->ptr = node->ptr;\n\t\t\tinfo->cookie = node->cookie;\n\t\t\tinfo->has_strong_ref = node->has_strong_ref;\n\t\t\tinfo->has_weak_ref = node->has_weak_ref;\n\t\t\tbreak;\n\t\t}\n\t}\n\tbinder_inner_proc_unlock(proc);\n\n\treturn 0;\n}\n\nstatic long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tint ret;\n\tstruct binder_proc *proc = filp->private_data;\n\tstruct binder_thread *thread;\n\tunsigned int size = _IOC_SIZE(cmd);\n\tvoid __user *ubuf = (void __user *)arg;\n\n\t/*pr_info(\"binder_ioctl: %d:%d %x %lx\\n\",\n\t\t\tproc->pid, current->pid, cmd, arg);*/\n\n\tbinder_selftest_alloc(&proc->alloc);\n\n\ttrace_binder_ioctl(cmd, arg);\n\n\tret = wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error < 2);\n\tif (ret)\n\t\tgoto err_unlocked;\n\n\tthread = binder_get_thread(proc);\n\tif (thread == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tswitch (cmd) {\n\tcase BINDER_WRITE_READ:\n\t\tret = binder_ioctl_write_read(filp, cmd, arg, thread);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tbreak;\n\tcase BINDER_SET_MAX_THREADS: {\n\t\tint max_threads;\n\n\t\tif (copy_from_user(&max_threads, ubuf,\n\t\t\t\t   sizeof(max_threads))) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\t\tproc->max_threads = max_threads;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbreak;\n\t}\n\tcase BINDER_SET_CONTEXT_MGR_EXT: {\n\t\tstruct flat_binder_object fbo;\n\n\t\tif (copy_from_user(&fbo, ubuf, sizeof(fbo))) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tret = binder_ioctl_set_ctx_mgr(filp, &fbo);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tbreak;\n\t}\n\tcase BINDER_SET_CONTEXT_MGR:\n\t\tret = binder_ioctl_set_ctx_mgr(filp, NULL);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tbreak;\n\tcase BINDER_THREAD_EXIT:\n\t\tbinder_debug(BINDER_DEBUG_THREADS, \"%d:%d exit\\n\",\n\t\t\t     proc->pid, thread->pid);\n\t\tbinder_thread_release(proc, thread);\n\t\tthread = NULL;\n\t\tbreak;\n\tcase BINDER_VERSION: {\n\t\tstruct binder_version __user *ver = ubuf;\n\n\t\tif (size != sizeof(struct binder_version)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tif (put_user(BINDER_CURRENT_PROTOCOL_VERSION,\n\t\t\t     &ver->protocol_version)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tbreak;\n\t}\n\tcase BINDER_GET_NODE_INFO_FOR_REF: {\n\t\tstruct binder_node_info_for_ref info;\n\n\t\tif (copy_from_user(&info, ubuf, sizeof(info))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tret = binder_ioctl_get_node_info_for_ref(proc, &info);\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\n\t\tif (copy_to_user(ubuf, &info, sizeof(info))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tbreak;\n\t}\n\tcase BINDER_GET_NODE_DEBUG_INFO: {\n\t\tstruct binder_node_debug_info info;\n\n\t\tif (copy_from_user(&info, ubuf, sizeof(info))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err;\n\t\t}\n\n\t\tret = binder_ioctl_get_node_debug_info(proc, &info);\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\n\t\tif (copy_to_user(ubuf, &info, sizeof(info))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err;\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\tret = 0;\nerr:\n\tif (thread)\n\t\tthread->looper_need_return = false;\n\twait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error < 2);\n\tif (ret && ret != -ERESTARTSYS)\n\t\tpr_info(\"%d:%d ioctl %x %lx returned %d\\n\", proc->pid, current->pid, cmd, arg, ret);\nerr_unlocked:\n\ttrace_binder_ioctl_done(ret);\n\treturn ret;\n}\n\nstatic void binder_vma_open(struct vm_area_struct *vma)\n{\n\tstruct binder_proc *proc = vma->vm_private_data;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%d open vm area %lx-%lx (%ld K) vma %lx pagep %lx\\n\",\n\t\t     proc->pid, vma->vm_start, vma->vm_end,\n\t\t     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,\n\t\t     (unsigned long)pgprot_val(vma->vm_page_prot));\n}\n\nstatic void binder_vma_close(struct vm_area_struct *vma)\n{\n\tstruct binder_proc *proc = vma->vm_private_data;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%d close vm area %lx-%lx (%ld K) vma %lx pagep %lx\\n\",\n\t\t     proc->pid, vma->vm_start, vma->vm_end,\n\t\t     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,\n\t\t     (unsigned long)pgprot_val(vma->vm_page_prot));\n\tbinder_alloc_vma_close(&proc->alloc);\n}\n\nstatic vm_fault_t binder_vm_fault(struct vm_fault *vmf)\n{\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic const struct vm_operations_struct binder_vm_ops = {\n\t.open = binder_vma_open,\n\t.close = binder_vma_close,\n\t.fault = binder_vm_fault,\n};\n\nstatic int binder_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tint ret;\n\tstruct binder_proc *proc = filp->private_data;\n\tconst char *failure_string;\n\n\tif (proc->tsk != current->group_leader)\n\t\treturn -EINVAL;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d %lx-%lx (%ld K) vma %lx pagep %lx\\n\",\n\t\t     __func__, proc->pid, vma->vm_start, vma->vm_end,\n\t\t     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,\n\t\t     (unsigned long)pgprot_val(vma->vm_page_prot));\n\n\tif (vma->vm_flags & FORBIDDEN_MMAP_FLAGS) {\n\t\tret = -EPERM;\n\t\tfailure_string = \"bad vm_flags\";\n\t\tgoto err_bad_arg;\n\t}\n\tvma->vm_flags |= VM_DONTCOPY | VM_MIXEDMAP;\n\tvma->vm_flags &= ~VM_MAYWRITE;\n\n\tvma->vm_ops = &binder_vm_ops;\n\tvma->vm_private_data = proc;\n\n\tret = binder_alloc_mmap_handler(&proc->alloc, vma);\n\tif (ret)\n\t\treturn ret;\n\treturn 0;\n\nerr_bad_arg:\n\tpr_err(\"%s: %d %lx-%lx %s failed %d\\n\", __func__,\n\t       proc->pid, vma->vm_start, vma->vm_end, failure_string, ret);\n\treturn ret;\n}\n\nstatic int binder_open(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc, *itr;\n\tstruct binder_device *binder_dev;\n\tstruct binderfs_info *info;\n\tstruct dentry *binder_binderfs_dir_entry_proc = NULL;\n\tbool existing_pid = false;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE, \"%s: %d:%d\\n\", __func__,\n\t\t     current->group_leader->pid, current->pid);\n\n\tproc = kzalloc(sizeof(*proc), GFP_KERNEL);\n\tif (proc == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&proc->inner_lock);\n\tspin_lock_init(&proc->outer_lock);\n\tget_task_struct(current->group_leader);\n\tproc->tsk = current->group_leader;\n\tproc->cred = get_cred(filp->f_cred);\n\tINIT_LIST_HEAD(&proc->todo);\n\tproc->default_priority = task_nice(current);\n\t/* binderfs stashes devices in i_private */\n\tif (is_binderfs_device(nodp)) {\n\t\tbinder_dev = nodp->i_private;\n\t\tinfo = nodp->i_sb->s_fs_info;\n\t\tbinder_binderfs_dir_entry_proc = info->proc_log_dir;\n\t} else {\n\t\tbinder_dev = container_of(filp->private_data,\n\t\t\t\t\t  struct binder_device, miscdev);\n\t}\n\trefcount_inc(&binder_dev->ref);\n\tproc->context = &binder_dev->context;\n\tbinder_alloc_init(&proc->alloc);\n\n\tbinder_stats_created(BINDER_STAT_PROC);\n\tproc->pid = current->group_leader->pid;\n\tINIT_LIST_HEAD(&proc->delivered_death);\n\tINIT_LIST_HEAD(&proc->waiting_threads);\n\tfilp->private_data = proc;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(itr, &binder_procs, proc_node) {\n\t\tif (itr->pid == proc->pid) {\n\t\t\texisting_pid = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\thlist_add_head(&proc->proc_node, &binder_procs);\n\tmutex_unlock(&binder_procs_lock);\n\n\tif (binder_debugfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * proc debug entries are shared between contexts.\n\t\t * Only create for the first PID to avoid debugfs log spamming\n\t\t * The printing code will anyway print all contexts for a given\n\t\t * PID so this is not a problem.\n\t\t */\n\t\tproc->debugfs_entry = debugfs_create_file(strbuf, 0444,\n\t\t\tbinder_debugfs_dir_entry_proc,\n\t\t\t(void *)(unsigned long)proc->pid,\n\t\t\t&proc_fops);\n\t}\n\n\tif (binder_binderfs_dir_entry_proc && !existing_pid) {\n\t\tchar strbuf[11];\n\t\tstruct dentry *binderfs_entry;\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * Similar to debugfs, the process specific log file is shared\n\t\t * between contexts. Only create for the first PID.\n\t\t * This is ok since same as debugfs, the log file will contain\n\t\t * information on all contexts of a given PID.\n\t\t */\n\t\tbinderfs_entry = binderfs_create_file(binder_binderfs_dir_entry_proc,\n\t\t\tstrbuf, &proc_fops, (void *)(unsigned long)proc->pid);\n\t\tif (!IS_ERR(binderfs_entry)) {\n\t\t\tproc->binderfs_entry = binderfs_entry;\n\t\t} else {\n\t\t\tint error;\n\n\t\t\terror = PTR_ERR(binderfs_entry);\n\t\t\tpr_warn(\"Unable to create file %s in binderfs (error %d)\\n\",\n\t\t\t\tstrbuf, error);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int binder_flush(struct file *filp, fl_owner_t id)\n{\n\tstruct binder_proc *proc = filp->private_data;\n\n\tbinder_defer_work(proc, BINDER_DEFERRED_FLUSH);\n\n\treturn 0;\n}\n\nstatic void binder_deferred_flush(struct binder_proc *proc)\n{\n\tstruct rb_node *n;\n\tint wake_count = 0;\n\n\tbinder_inner_proc_lock(proc);\n\tfor (n = rb_first(&proc->threads); n != NULL; n = rb_next(n)) {\n\t\tstruct binder_thread *thread = rb_entry(n, struct binder_thread, rb_node);\n\n\t\tthread->looper_need_return = true;\n\t\tif (thread->looper & BINDER_LOOPER_STATE_WAITING) {\n\t\t\twake_up_interruptible(&thread->wait);\n\t\t\twake_count++;\n\t\t}\n\t}\n\tbinder_inner_proc_unlock(proc);\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"binder_flush: %d woke %d threads\\n\", proc->pid,\n\t\t     wake_count);\n}\n\nstatic int binder_release(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc = filp->private_data;\n\n\tdebugfs_remove(proc->debugfs_entry);\n\n\tif (proc->binderfs_entry) {\n\t\tbinderfs_remove_file(proc->binderfs_entry);\n\t\tproc->binderfs_entry = NULL;\n\t}\n\n\tbinder_defer_work(proc, BINDER_DEFERRED_RELEASE);\n\n\treturn 0;\n}\n\nstatic int binder_node_release(struct binder_node *node, int refs)\n{\n\tstruct binder_ref *ref;\n\tint death = 0;\n\tstruct binder_proc *proc = node->proc;\n\n\tbinder_release_work(proc, &node->async_todo);\n\n\tbinder_node_lock(node);\n\tbinder_inner_proc_lock(proc);\n\tbinder_dequeue_work_ilocked(&node->work);\n\t/*\n\t * The caller must have taken a temporary ref on the node,\n\t */\n\tBUG_ON(!node->tmp_refs);\n\tif (hlist_empty(&node->refs) && node->tmp_refs == 1) {\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_node_unlock(node);\n\t\tbinder_free_node(node);\n\n\t\treturn refs;\n\t}\n\n\tnode->proc = NULL;\n\tnode->local_strong_refs = 0;\n\tnode->local_weak_refs = 0;\n\tbinder_inner_proc_unlock(proc);\n\n\tspin_lock(&binder_dead_nodes_lock);\n\thlist_add_head(&node->dead_node, &binder_dead_nodes);\n\tspin_unlock(&binder_dead_nodes_lock);\n\n\thlist_for_each_entry(ref, &node->refs, node_entry) {\n\t\trefs++;\n\t\t/*\n\t\t * Need the node lock to synchronize\n\t\t * with new notification requests and the\n\t\t * inner lock to synchronize with queued\n\t\t * death notifications.\n\t\t */\n\t\tbinder_inner_proc_lock(ref->proc);\n\t\tif (!ref->death) {\n\t\t\tbinder_inner_proc_unlock(ref->proc);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdeath++;\n\n\t\tBUG_ON(!list_empty(&ref->death->work.entry));\n\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\t\tbinder_enqueue_work_ilocked(&ref->death->work,\n\t\t\t\t\t    &ref->proc->todo);\n\t\tbinder_wakeup_proc_ilocked(ref->proc);\n\t\tbinder_inner_proc_unlock(ref->proc);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t     \"node %d now dead, refs %d, death %d\\n\",\n\t\t     node->debug_id, refs, death);\n\tbinder_node_unlock(node);\n\tbinder_put_node(node);\n\n\treturn refs;\n}\n\nstatic void binder_deferred_release(struct binder_proc *proc)\n{\n\tstruct binder_context *context = proc->context;\n\tstruct rb_node *n;\n\tint threads, nodes, incoming_refs, outgoing_refs, active_transactions;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_del(&proc->proc_node);\n\tmutex_unlock(&binder_procs_lock);\n\n\tmutex_lock(&context->context_mgr_node_lock);\n\tif (context->binder_context_mgr_node &&\n\t    context->binder_context_mgr_node->proc == proc) {\n\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t     \"%s: %d context_mgr_node gone\\n\",\n\t\t\t     __func__, proc->pid);\n\t\tcontext->binder_context_mgr_node = NULL;\n\t}\n\tmutex_unlock(&context->context_mgr_node_lock);\n\tbinder_inner_proc_lock(proc);\n\t/*\n\t * Make sure proc stays alive after we\n\t * remove all the threads\n\t */\n\tproc->tmp_ref++;\n\n\tproc->is_dead = true;\n\tthreads = 0;\n\tactive_transactions = 0;\n\twhile ((n = rb_first(&proc->threads))) {\n\t\tstruct binder_thread *thread;\n\n\t\tthread = rb_entry(n, struct binder_thread, rb_node);\n\t\tbinder_inner_proc_unlock(proc);\n\t\tthreads++;\n\t\tactive_transactions += binder_thread_release(proc, thread);\n\t\tbinder_inner_proc_lock(proc);\n\t}\n\n\tnodes = 0;\n\tincoming_refs = 0;\n\twhile ((n = rb_first(&proc->nodes))) {\n\t\tstruct binder_node *node;\n\n\t\tnode = rb_entry(n, struct binder_node, rb_node);\n\t\tnodes++;\n\t\t/*\n\t\t * take a temporary ref on the node before\n\t\t * calling binder_node_release() which will either\n\t\t * kfree() the node or call binder_put_node()\n\t\t */\n\t\tbinder_inc_node_tmpref_ilocked(node);\n\t\trb_erase(&node->rb_node, &proc->nodes);\n\t\tbinder_inner_proc_unlock(proc);\n\t\tincoming_refs = binder_node_release(node, incoming_refs);\n\t\tbinder_inner_proc_lock(proc);\n\t}\n\tbinder_inner_proc_unlock(proc);\n\n\toutgoing_refs = 0;\n\tbinder_proc_lock(proc);\n\twhile ((n = rb_first(&proc->refs_by_desc))) {\n\t\tstruct binder_ref *ref;\n\n\t\tref = rb_entry(n, struct binder_ref, rb_node_desc);\n\t\toutgoing_refs++;\n\t\tbinder_cleanup_ref_olocked(ref);\n\t\tbinder_proc_unlock(proc);\n\t\tbinder_free_ref(ref);\n\t\tbinder_proc_lock(proc);\n\t}\n\tbinder_proc_unlock(proc);\n\n\tbinder_release_work(proc, &proc->todo);\n\tbinder_release_work(proc, &proc->delivered_death);\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE,\n\t\t     \"%s: %d threads %d, nodes %d (ref %d), refs %d, active transactions %d\\n\",\n\t\t     __func__, proc->pid, threads, nodes, incoming_refs,\n\t\t     outgoing_refs, active_transactions);\n\n\tbinder_proc_dec_tmpref(proc);\n}\n\nstatic void binder_deferred_func(struct work_struct *work)\n{\n\tstruct binder_proc *proc;\n\n\tint defer;\n\n\tdo {\n\t\tmutex_lock(&binder_deferred_lock);\n\t\tif (!hlist_empty(&binder_deferred_list)) {\n\t\t\tproc = hlist_entry(binder_deferred_list.first,\n\t\t\t\t\tstruct binder_proc, deferred_work_node);\n\t\t\thlist_del_init(&proc->deferred_work_node);\n\t\t\tdefer = proc->deferred_work;\n\t\t\tproc->deferred_work = 0;\n\t\t} else {\n\t\t\tproc = NULL;\n\t\t\tdefer = 0;\n\t\t}\n\t\tmutex_unlock(&binder_deferred_lock);\n\n\t\tif (defer & BINDER_DEFERRED_FLUSH)\n\t\t\tbinder_deferred_flush(proc);\n\n\t\tif (defer & BINDER_DEFERRED_RELEASE)\n\t\t\tbinder_deferred_release(proc); /* frees proc */\n\t} while (proc);\n}\nstatic DECLARE_WORK(binder_deferred_work, binder_deferred_func);\n\nstatic void\nbinder_defer_work(struct binder_proc *proc, enum binder_deferred_state defer)\n{\n\tmutex_lock(&binder_deferred_lock);\n\tproc->deferred_work |= defer;\n\tif (hlist_unhashed(&proc->deferred_work_node)) {\n\t\thlist_add_head(&proc->deferred_work_node,\n\t\t\t\t&binder_deferred_list);\n\t\tschedule_work(&binder_deferred_work);\n\t}\n\tmutex_unlock(&binder_deferred_lock);\n}\n\nstatic void print_binder_transaction_ilocked(struct seq_file *m,\n\t\t\t\t\t     struct binder_proc *proc,\n\t\t\t\t\t     const char *prefix,\n\t\t\t\t\t     struct binder_transaction *t)\n{\n\tstruct binder_proc *to_proc;\n\tstruct binder_buffer *buffer = t->buffer;\n\n\tspin_lock(&t->lock);\n\tto_proc = t->to_proc;\n\tseq_printf(m,\n\t\t   \"%s %d: %pK from %d:%d to %d:%d code %x flags %x pri %ld r%d\",\n\t\t   prefix, t->debug_id, t,\n\t\t   t->from ? t->from->proc->pid : 0,\n\t\t   t->from ? t->from->pid : 0,\n\t\t   to_proc ? to_proc->pid : 0,\n\t\t   t->to_thread ? t->to_thread->pid : 0,\n\t\t   t->code, t->flags, t->priority, t->need_reply);\n\tspin_unlock(&t->lock);\n\n\tif (proc != to_proc) {\n\t\t/*\n\t\t * Can only safely deref buffer if we are holding the\n\t\t * correct proc inner lock for this node\n\t\t */\n\t\tseq_puts(m, \"\\n\");\n\t\treturn;\n\t}\n\n\tif (buffer == NULL) {\n\t\tseq_puts(m, \" buffer free\\n\");\n\t\treturn;\n\t}\n\tif (buffer->target_node)\n\t\tseq_printf(m, \" node %d\", buffer->target_node->debug_id);\n\tseq_printf(m, \" size %zd:%zd data %pK\\n\",\n\t\t   buffer->data_size, buffer->offsets_size,\n\t\t   buffer->user_data);\n}\n\nstatic void print_binder_work_ilocked(struct seq_file *m,\n\t\t\t\t     struct binder_proc *proc,\n\t\t\t\t     const char *prefix,\n\t\t\t\t     const char *transaction_prefix,\n\t\t\t\t     struct binder_work *w)\n{\n\tstruct binder_node *node;\n\tstruct binder_transaction *t;\n\n\tswitch (w->type) {\n\tcase BINDER_WORK_TRANSACTION:\n\t\tt = container_of(w, struct binder_transaction, work);\n\t\tprint_binder_transaction_ilocked(\n\t\t\t\tm, proc, transaction_prefix, t);\n\t\tbreak;\n\tcase BINDER_WORK_RETURN_ERROR: {\n\t\tstruct binder_error *e = container_of(\n\t\t\t\tw, struct binder_error, work);\n\n\t\tseq_printf(m, \"%stransaction error: %u\\n\",\n\t\t\t   prefix, e->cmd);\n\t} break;\n\tcase BINDER_WORK_TRANSACTION_COMPLETE:\n\t\tseq_printf(m, \"%stransaction complete\\n\", prefix);\n\t\tbreak;\n\tcase BINDER_WORK_NODE:\n\t\tnode = container_of(w, struct binder_node, work);\n\t\tseq_printf(m, \"%snode work %d: u%016llx c%016llx\\n\",\n\t\t\t   prefix, node->debug_id,\n\t\t\t   (u64)node->ptr, (u64)node->cookie);\n\t\tbreak;\n\tcase BINDER_WORK_DEAD_BINDER:\n\t\tseq_printf(m, \"%shas dead binder\\n\", prefix);\n\t\tbreak;\n\tcase BINDER_WORK_DEAD_BINDER_AND_CLEAR:\n\t\tseq_printf(m, \"%shas cleared dead binder\\n\", prefix);\n\t\tbreak;\n\tcase BINDER_WORK_CLEAR_DEATH_NOTIFICATION:\n\t\tseq_printf(m, \"%shas cleared death notification\\n\", prefix);\n\t\tbreak;\n\tdefault:\n\t\tseq_printf(m, \"%sunknown work: type %d\\n\", prefix, w->type);\n\t\tbreak;\n\t}\n}\n\nstatic void print_binder_thread_ilocked(struct seq_file *m,\n\t\t\t\t\tstruct binder_thread *thread,\n\t\t\t\t\tint print_always)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tsize_t start_pos = m->count;\n\tsize_t header_pos;\n\n\tseq_printf(m, \"  thread %d: l %02x need_return %d tr %d\\n\",\n\t\t\tthread->pid, thread->looper,\n\t\t\tthread->looper_need_return,\n\t\t\tatomic_read(&thread->tmp_ref));\n\theader_pos = m->count;\n\tt = thread->transaction_stack;\n\twhile (t) {\n\t\tif (t->from == thread) {\n\t\t\tprint_binder_transaction_ilocked(m, thread->proc,\n\t\t\t\t\t\"    outgoing transaction\", t);\n\t\t\tt = t->from_parent;\n\t\t} else if (t->to_thread == thread) {\n\t\t\tprint_binder_transaction_ilocked(m, thread->proc,\n\t\t\t\t\t\t \"    incoming transaction\", t);\n\t\t\tt = t->to_parent;\n\t\t} else {\n\t\t\tprint_binder_transaction_ilocked(m, thread->proc,\n\t\t\t\t\t\"    bad transaction\", t);\n\t\t\tt = NULL;\n\t\t}\n\t}\n\tlist_for_each_entry(w, &thread->todo, entry) {\n\t\tprint_binder_work_ilocked(m, thread->proc, \"    \",\n\t\t\t\t\t  \"    pending transaction\", w);\n\t}\n\tif (!print_always && m->count == header_pos)\n\t\tm->count = start_pos;\n}\n\nstatic void print_binder_node_nilocked(struct seq_file *m,\n\t\t\t\t       struct binder_node *node)\n{\n\tstruct binder_ref *ref;\n\tstruct binder_work *w;\n\tint count;\n\n\tcount = 0;\n\thlist_for_each_entry(ref, &node->refs, node_entry)\n\t\tcount++;\n\n\tseq_printf(m, \"  node %d: u%016llx c%016llx hs %d hw %d ls %d lw %d is %d iw %d tr %d\",\n\t\t   node->debug_id, (u64)node->ptr, (u64)node->cookie,\n\t\t   node->has_strong_ref, node->has_weak_ref,\n\t\t   node->local_strong_refs, node->local_weak_refs,\n\t\t   node->internal_strong_refs, count, node->tmp_refs);\n\tif (count) {\n\t\tseq_puts(m, \" proc\");\n\t\thlist_for_each_entry(ref, &node->refs, node_entry)\n\t\t\tseq_printf(m, \" %d\", ref->proc->pid);\n\t}\n\tseq_puts(m, \"\\n\");\n\tif (node->proc) {\n\t\tlist_for_each_entry(w, &node->async_todo, entry)\n\t\t\tprint_binder_work_ilocked(m, node->proc, \"    \",\n\t\t\t\t\t  \"    pending async transaction\", w);\n\t}\n}\n\nstatic void print_binder_ref_olocked(struct seq_file *m,\n\t\t\t\t     struct binder_ref *ref)\n{\n\tbinder_node_lock(ref->node);\n\tseq_printf(m, \"  ref %d: desc %d %snode %d s %d w %d d %pK\\n\",\n\t\t   ref->data.debug_id, ref->data.desc,\n\t\t   ref->node->proc ? \"\" : \"dead \",\n\t\t   ref->node->debug_id, ref->data.strong,\n\t\t   ref->data.weak, ref->death);\n\tbinder_node_unlock(ref->node);\n}\n\nstatic void print_binder_proc(struct seq_file *m,\n\t\t\t      struct binder_proc *proc, int print_all)\n{\n\tstruct binder_work *w;\n\tstruct rb_node *n;\n\tsize_t start_pos = m->count;\n\tsize_t header_pos;\n\tstruct binder_node *last_node = NULL;\n\n\tseq_printf(m, \"proc %d\\n\", proc->pid);\n\tseq_printf(m, \"context %s\\n\", proc->context->name);\n\theader_pos = m->count;\n\n\tbinder_inner_proc_lock(proc);\n\tfor (n = rb_first(&proc->threads); n != NULL; n = rb_next(n))\n\t\tprint_binder_thread_ilocked(m, rb_entry(n, struct binder_thread,\n\t\t\t\t\t\trb_node), print_all);\n\n\tfor (n = rb_first(&proc->nodes); n != NULL; n = rb_next(n)) {\n\t\tstruct binder_node *node = rb_entry(n, struct binder_node,\n\t\t\t\t\t\t    rb_node);\n\t\tif (!print_all && !node->has_async_transaction)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * take a temporary reference on the node so it\n\t\t * survives and isn't removed from the tree\n\t\t * while we print it.\n\t\t */\n\t\tbinder_inc_node_tmpref_ilocked(node);\n\t\t/* Need to drop inner lock to take node lock */\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (last_node)\n\t\t\tbinder_put_node(last_node);\n\t\tbinder_node_inner_lock(node);\n\t\tprint_binder_node_nilocked(m, node);\n\t\tbinder_node_inner_unlock(node);\n\t\tlast_node = node;\n\t\tbinder_inner_proc_lock(proc);\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (last_node)\n\t\tbinder_put_node(last_node);\n\n\tif (print_all) {\n\t\tbinder_proc_lock(proc);\n\t\tfor (n = rb_first(&proc->refs_by_desc);\n\t\t     n != NULL;\n\t\t     n = rb_next(n))\n\t\t\tprint_binder_ref_olocked(m, rb_entry(n,\n\t\t\t\t\t\t\t    struct binder_ref,\n\t\t\t\t\t\t\t    rb_node_desc));\n\t\tbinder_proc_unlock(proc);\n\t}\n\tbinder_alloc_print_allocated(m, &proc->alloc);\n\tbinder_inner_proc_lock(proc);\n\tlist_for_each_entry(w, &proc->todo, entry)\n\t\tprint_binder_work_ilocked(m, proc, \"  \",\n\t\t\t\t\t  \"  pending transaction\", w);\n\tlist_for_each_entry(w, &proc->delivered_death, entry) {\n\t\tseq_puts(m, \"  has delivered dead binder\\n\");\n\t\tbreak;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (!print_all && m->count == header_pos)\n\t\tm->count = start_pos;\n}\n\nstatic const char * const binder_return_strings[] = {\n\t\"BR_ERROR\",\n\t\"BR_OK\",\n\t\"BR_TRANSACTION\",\n\t\"BR_REPLY\",\n\t\"BR_ACQUIRE_RESULT\",\n\t\"BR_DEAD_REPLY\",\n\t\"BR_TRANSACTION_COMPLETE\",\n\t\"BR_INCREFS\",\n\t\"BR_ACQUIRE\",\n\t\"BR_RELEASE\",\n\t\"BR_DECREFS\",\n\t\"BR_ATTEMPT_ACQUIRE\",\n\t\"BR_NOOP\",\n\t\"BR_SPAWN_LOOPER\",\n\t\"BR_FINISHED\",\n\t\"BR_DEAD_BINDER\",\n\t\"BR_CLEAR_DEATH_NOTIFICATION_DONE\",\n\t\"BR_FAILED_REPLY\"\n};\n\nstatic const char * const binder_command_strings[] = {\n\t\"BC_TRANSACTION\",\n\t\"BC_REPLY\",\n\t\"BC_ACQUIRE_RESULT\",\n\t\"BC_FREE_BUFFER\",\n\t\"BC_INCREFS\",\n\t\"BC_ACQUIRE\",\n\t\"BC_RELEASE\",\n\t\"BC_DECREFS\",\n\t\"BC_INCREFS_DONE\",\n\t\"BC_ACQUIRE_DONE\",\n\t\"BC_ATTEMPT_ACQUIRE\",\n\t\"BC_REGISTER_LOOPER\",\n\t\"BC_ENTER_LOOPER\",\n\t\"BC_EXIT_LOOPER\",\n\t\"BC_REQUEST_DEATH_NOTIFICATION\",\n\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\"BC_DEAD_BINDER_DONE\",\n\t\"BC_TRANSACTION_SG\",\n\t\"BC_REPLY_SG\",\n};\n\nstatic const char * const binder_objstat_strings[] = {\n\t\"proc\",\n\t\"thread\",\n\t\"node\",\n\t\"ref\",\n\t\"death\",\n\t\"transaction\",\n\t\"transaction_complete\"\n};\n\nstatic void print_binder_stats(struct seq_file *m, const char *prefix,\n\t\t\t       struct binder_stats *stats)\n{\n\tint i;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(stats->bc) !=\n\t\t     ARRAY_SIZE(binder_command_strings));\n\tfor (i = 0; i < ARRAY_SIZE(stats->bc); i++) {\n\t\tint temp = atomic_read(&stats->bc[i]);\n\n\t\tif (temp)\n\t\t\tseq_printf(m, \"%s%s: %d\\n\", prefix,\n\t\t\t\t   binder_command_strings[i], temp);\n\t}\n\n\tBUILD_BUG_ON(ARRAY_SIZE(stats->br) !=\n\t\t     ARRAY_SIZE(binder_return_strings));\n\tfor (i = 0; i < ARRAY_SIZE(stats->br); i++) {\n\t\tint temp = atomic_read(&stats->br[i]);\n\n\t\tif (temp)\n\t\t\tseq_printf(m, \"%s%s: %d\\n\", prefix,\n\t\t\t\t   binder_return_strings[i], temp);\n\t}\n\n\tBUILD_BUG_ON(ARRAY_SIZE(stats->obj_created) !=\n\t\t     ARRAY_SIZE(binder_objstat_strings));\n\tBUILD_BUG_ON(ARRAY_SIZE(stats->obj_created) !=\n\t\t     ARRAY_SIZE(stats->obj_deleted));\n\tfor (i = 0; i < ARRAY_SIZE(stats->obj_created); i++) {\n\t\tint created = atomic_read(&stats->obj_created[i]);\n\t\tint deleted = atomic_read(&stats->obj_deleted[i]);\n\n\t\tif (created || deleted)\n\t\t\tseq_printf(m, \"%s%s: active %d total %d\\n\",\n\t\t\t\tprefix,\n\t\t\t\tbinder_objstat_strings[i],\n\t\t\t\tcreated - deleted,\n\t\t\t\tcreated);\n\t}\n}\n\nstatic void print_binder_proc_stats(struct seq_file *m,\n\t\t\t\t    struct binder_proc *proc)\n{\n\tstruct binder_work *w;\n\tstruct binder_thread *thread;\n\tstruct rb_node *n;\n\tint count, strong, weak, ready_threads;\n\tsize_t free_async_space =\n\t\tbinder_alloc_get_free_async_space(&proc->alloc);\n\n\tseq_printf(m, \"proc %d\\n\", proc->pid);\n\tseq_printf(m, \"context %s\\n\", proc->context->name);\n\tcount = 0;\n\tready_threads = 0;\n\tbinder_inner_proc_lock(proc);\n\tfor (n = rb_first(&proc->threads); n != NULL; n = rb_next(n))\n\t\tcount++;\n\n\tlist_for_each_entry(thread, &proc->waiting_threads, waiting_thread_node)\n\t\tready_threads++;\n\n\tseq_printf(m, \"  threads: %d\\n\", count);\n\tseq_printf(m, \"  requested threads: %d+%d/%d\\n\"\n\t\t\t\"  ready threads %d\\n\"\n\t\t\t\"  free async space %zd\\n\", proc->requested_threads,\n\t\t\tproc->requested_threads_started, proc->max_threads,\n\t\t\tready_threads,\n\t\t\tfree_async_space);\n\tcount = 0;\n\tfor (n = rb_first(&proc->nodes); n != NULL; n = rb_next(n))\n\t\tcount++;\n\tbinder_inner_proc_unlock(proc);\n\tseq_printf(m, \"  nodes: %d\\n\", count);\n\tcount = 0;\n\tstrong = 0;\n\tweak = 0;\n\tbinder_proc_lock(proc);\n\tfor (n = rb_first(&proc->refs_by_desc); n != NULL; n = rb_next(n)) {\n\t\tstruct binder_ref *ref = rb_entry(n, struct binder_ref,\n\t\t\t\t\t\t  rb_node_desc);\n\t\tcount++;\n\t\tstrong += ref->data.strong;\n\t\tweak += ref->data.weak;\n\t}\n\tbinder_proc_unlock(proc);\n\tseq_printf(m, \"  refs: %d s %d w %d\\n\", count, strong, weak);\n\n\tcount = binder_alloc_get_allocated_count(&proc->alloc);\n\tseq_printf(m, \"  buffers: %d\\n\", count);\n\n\tbinder_alloc_print_pages(m, &proc->alloc);\n\n\tcount = 0;\n\tbinder_inner_proc_lock(proc);\n\tlist_for_each_entry(w, &proc->todo, entry) {\n\t\tif (w->type == BINDER_WORK_TRANSACTION)\n\t\t\tcount++;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tseq_printf(m, \"  pending transactions: %d\\n\", count);\n\n\tprint_binder_stats(m, \"  \", &proc->stats);\n}\n\n\nint binder_state_show(struct seq_file *m, void *unused)\n{\n\tstruct binder_proc *proc;\n\tstruct binder_node *node;\n\tstruct binder_node *last_node = NULL;\n\n\tseq_puts(m, \"binder state:\\n\");\n\n\tspin_lock(&binder_dead_nodes_lock);\n\tif (!hlist_empty(&binder_dead_nodes))\n\t\tseq_puts(m, \"dead nodes:\\n\");\n\thlist_for_each_entry(node, &binder_dead_nodes, dead_node) {\n\t\t/*\n\t\t * take a temporary reference on the node so it\n\t\t * survives and isn't removed from the list\n\t\t * while we print it.\n\t\t */\n\t\tnode->tmp_refs++;\n\t\tspin_unlock(&binder_dead_nodes_lock);\n\t\tif (last_node)\n\t\t\tbinder_put_node(last_node);\n\t\tbinder_node_lock(node);\n\t\tprint_binder_node_nilocked(m, node);\n\t\tbinder_node_unlock(node);\n\t\tlast_node = node;\n\t\tspin_lock(&binder_dead_nodes_lock);\n\t}\n\tspin_unlock(&binder_dead_nodes_lock);\n\tif (last_node)\n\t\tbinder_put_node(last_node);\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(proc, &binder_procs, proc_node)\n\t\tprint_binder_proc(m, proc, 1);\n\tmutex_unlock(&binder_procs_lock);\n\n\treturn 0;\n}\n\nint binder_stats_show(struct seq_file *m, void *unused)\n{\n\tstruct binder_proc *proc;\n\n\tseq_puts(m, \"binder stats:\\n\");\n\n\tprint_binder_stats(m, \"\", &binder_stats);\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(proc, &binder_procs, proc_node)\n\t\tprint_binder_proc_stats(m, proc);\n\tmutex_unlock(&binder_procs_lock);\n\n\treturn 0;\n}\n\nint binder_transactions_show(struct seq_file *m, void *unused)\n{\n\tstruct binder_proc *proc;\n\n\tseq_puts(m, \"binder transactions:\\n\");\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(proc, &binder_procs, proc_node)\n\t\tprint_binder_proc(m, proc, 0);\n\tmutex_unlock(&binder_procs_lock);\n\n\treturn 0;\n}\n\nstatic int proc_show(struct seq_file *m, void *unused)\n{\n\tstruct binder_proc *itr;\n\tint pid = (unsigned long)m->private;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_for_each_entry(itr, &binder_procs, proc_node) {\n\t\tif (itr->pid == pid) {\n\t\t\tseq_puts(m, \"binder proc state:\\n\");\n\t\t\tprint_binder_proc(m, itr, 1);\n\t\t}\n\t}\n\tmutex_unlock(&binder_procs_lock);\n\n\treturn 0;\n}\n\nstatic void print_binder_transaction_log_entry(struct seq_file *m,\n\t\t\t\t\tstruct binder_transaction_log_entry *e)\n{\n\tint debug_id = READ_ONCE(e->debug_id_done);\n\t/*\n\t * read barrier to guarantee debug_id_done read before\n\t * we print the log values\n\t */\n\tsmp_rmb();\n\tseq_printf(m,\n\t\t   \"%d: %s from %d:%d to %d:%d context %s node %d handle %d size %d:%d ret %d/%d l=%d\",\n\t\t   e->debug_id, (e->call_type == 2) ? \"reply\" :\n\t\t   ((e->call_type == 1) ? \"async\" : \"call \"), e->from_proc,\n\t\t   e->from_thread, e->to_proc, e->to_thread, e->context_name,\n\t\t   e->to_node, e->target_handle, e->data_size, e->offsets_size,\n\t\t   e->return_error, e->return_error_param,\n\t\t   e->return_error_line);\n\t/*\n\t * read-barrier to guarantee read of debug_id_done after\n\t * done printing the fields of the entry\n\t */\n\tsmp_rmb();\n\tseq_printf(m, debug_id && debug_id == READ_ONCE(e->debug_id_done) ?\n\t\t\t\"\\n\" : \" (incomplete)\\n\");\n}\n\nint binder_transaction_log_show(struct seq_file *m, void *unused)\n{\n\tstruct binder_transaction_log *log = m->private;\n\tunsigned int log_cur = atomic_read(&log->cur);\n\tunsigned int count;\n\tunsigned int cur;\n\tint i;\n\n\tcount = log_cur + 1;\n\tcur = count < ARRAY_SIZE(log->entry) && !log->full ?\n\t\t0 : count % ARRAY_SIZE(log->entry);\n\tif (count > ARRAY_SIZE(log->entry) || log->full)\n\t\tcount = ARRAY_SIZE(log->entry);\n\tfor (i = 0; i < count; i++) {\n\t\tunsigned int index = cur++ % ARRAY_SIZE(log->entry);\n\n\t\tprint_binder_transaction_log_entry(m, &log->entry[index]);\n\t}\n\treturn 0;\n}\n\nconst struct file_operations binder_fops = {\n\t.owner = THIS_MODULE,\n\t.poll = binder_poll,\n\t.unlocked_ioctl = binder_ioctl,\n\t.compat_ioctl = binder_ioctl,\n\t.mmap = binder_mmap,\n\t.open = binder_open,\n\t.flush = binder_flush,\n\t.release = binder_release,\n};\n\nstatic int __init init_binder_device(const char *name)\n{\n\tint ret;\n\tstruct binder_device *binder_device;\n\n\tbinder_device = kzalloc(sizeof(*binder_device), GFP_KERNEL);\n\tif (!binder_device)\n\t\treturn -ENOMEM;\n\n\tbinder_device->miscdev.fops = &binder_fops;\n\tbinder_device->miscdev.minor = MISC_DYNAMIC_MINOR;\n\tbinder_device->miscdev.name = name;\n\n\trefcount_set(&binder_device->ref, 1);\n\tbinder_device->context.binder_context_mgr_uid = INVALID_UID;\n\tbinder_device->context.name = name;\n\tmutex_init(&binder_device->context.context_mgr_node_lock);\n\n\tret = misc_register(&binder_device->miscdev);\n\tif (ret < 0) {\n\t\tkfree(binder_device);\n\t\treturn ret;\n\t}\n\n\thlist_add_head(&binder_device->hlist, &binder_devices);\n\n\treturn ret;\n}\n\nstatic int __init binder_init(void)\n{\n\tint ret;\n\tchar *device_name, *device_tmp;\n\tstruct binder_device *device;\n\tstruct hlist_node *tmp;\n\tchar *device_names = NULL;\n\n\tret = binder_alloc_shrinker_init();\n\tif (ret)\n\t\treturn ret;\n\n\tatomic_set(&binder_transaction_log.cur, ~0U);\n\tatomic_set(&binder_transaction_log_failed.cur, ~0U);\n\n\tbinder_debugfs_dir_entry_root = debugfs_create_dir(\"binder\", NULL);\n\tif (binder_debugfs_dir_entry_root)\n\t\tbinder_debugfs_dir_entry_proc = debugfs_create_dir(\"proc\",\n\t\t\t\t\t\t binder_debugfs_dir_entry_root);\n\n\tif (binder_debugfs_dir_entry_root) {\n\t\tdebugfs_create_file(\"state\",\n\t\t\t\t    0444,\n\t\t\t\t    binder_debugfs_dir_entry_root,\n\t\t\t\t    NULL,\n\t\t\t\t    &binder_state_fops);\n\t\tdebugfs_create_file(\"stats\",\n\t\t\t\t    0444,\n\t\t\t\t    binder_debugfs_dir_entry_root,\n\t\t\t\t    NULL,\n\t\t\t\t    &binder_stats_fops);\n\t\tdebugfs_create_file(\"transactions\",\n\t\t\t\t    0444,\n\t\t\t\t    binder_debugfs_dir_entry_root,\n\t\t\t\t    NULL,\n\t\t\t\t    &binder_transactions_fops);\n\t\tdebugfs_create_file(\"transaction_log\",\n\t\t\t\t    0444,\n\t\t\t\t    binder_debugfs_dir_entry_root,\n\t\t\t\t    &binder_transaction_log,\n\t\t\t\t    &binder_transaction_log_fops);\n\t\tdebugfs_create_file(\"failed_transaction_log\",\n\t\t\t\t    0444,\n\t\t\t\t    binder_debugfs_dir_entry_root,\n\t\t\t\t    &binder_transaction_log_failed,\n\t\t\t\t    &binder_transaction_log_fops);\n\t}\n\n\tif (!IS_ENABLED(CONFIG_ANDROID_BINDERFS) &&\n\t    strcmp(binder_devices_param, \"\") != 0) {\n\t\t/*\n\t\t* Copy the module_parameter string, because we don't want to\n\t\t* tokenize it in-place.\n\t\t */\n\t\tdevice_names = kstrdup(binder_devices_param, GFP_KERNEL);\n\t\tif (!device_names) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_alloc_device_names_failed;\n\t\t}\n\n\t\tdevice_tmp = device_names;\n\t\twhile ((device_name = strsep(&device_tmp, \",\"))) {\n\t\t\tret = init_binder_device(device_name);\n\t\t\tif (ret)\n\t\t\t\tgoto err_init_binder_device_failed;\n\t\t}\n\t}\n\n\tret = init_binderfs();\n\tif (ret)\n\t\tgoto err_init_binder_device_failed;\n\n\treturn ret;\n\nerr_init_binder_device_failed:\n\thlist_for_each_entry_safe(device, tmp, &binder_devices, hlist) {\n\t\tmisc_deregister(&device->miscdev);\n\t\thlist_del(&device->hlist);\n\t\tkfree(device);\n\t}\n\n\tkfree(device_names);\n\nerr_alloc_device_names_failed:\n\tdebugfs_remove_recursive(binder_debugfs_dir_entry_root);\n\n\treturn ret;\n}\n\ndevice_initcall(binder_init);\n\n#define CREATE_TRACE_POINTS\n#include \"binder_trace.h\"\n\nMODULE_LICENSE(\"GPL v2\");\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/file.c\n *\n *  Copyright (C) 1998-1999, Stephen Tweedie and Bill Hawes\n *\n *  Manage the dynamic fd arrays in the process files_struct.\n */\n\n#include <linux/syscalls.h>\n#include <linux/export.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/slab.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/bitops.h>\n#include <linux/spinlock.h>\n#include <linux/rcupdate.h>\n\n#include <linux/nospec.h>\n\nunsigned int sysctl_nr_open __read_mostly = 1024*1024;\nunsigned int sysctl_nr_open_min = BITS_PER_LONG;\n/* our min() is unusable in constant expressions ;-/ */\n#define __const_min(x, y) ((x) < (y) ? (x) : (y))\nunsigned int sysctl_nr_open_max =\n\t__const_min(INT_MAX, ~(size_t)0/sizeof(void *)) & -BITS_PER_LONG;\n\nstatic void __free_fdtable(struct fdtable *fdt)\n{\n\tkvfree(fdt->fd);\n\tkvfree(fdt->open_fds);\n\tkfree(fdt);\n}\n\nstatic void free_fdtable_rcu(struct rcu_head *rcu)\n{\n\t__free_fdtable(container_of(rcu, struct fdtable, rcu));\n}\n\n#define BITBIT_NR(nr)\tBITS_TO_LONGS(BITS_TO_LONGS(nr))\n#define BITBIT_SIZE(nr)\t(BITBIT_NR(nr) * sizeof(long))\n\n/*\n * Copy 'count' fd bits from the old table to the new table and clear the extra\n * space if any.  This does not copy the file pointers.  Called with the files\n * spinlock held for write.\n */\nstatic void copy_fd_bitmaps(struct fdtable *nfdt, struct fdtable *ofdt,\n\t\t\t    unsigned int count)\n{\n\tunsigned int cpy, set;\n\n\tcpy = count / BITS_PER_BYTE;\n\tset = (nfdt->max_fds - count) / BITS_PER_BYTE;\n\tmemcpy(nfdt->open_fds, ofdt->open_fds, cpy);\n\tmemset((char *)nfdt->open_fds + cpy, 0, set);\n\tmemcpy(nfdt->close_on_exec, ofdt->close_on_exec, cpy);\n\tmemset((char *)nfdt->close_on_exec + cpy, 0, set);\n\n\tcpy = BITBIT_SIZE(count);\n\tset = BITBIT_SIZE(nfdt->max_fds) - cpy;\n\tmemcpy(nfdt->full_fds_bits, ofdt->full_fds_bits, cpy);\n\tmemset((char *)nfdt->full_fds_bits + cpy, 0, set);\n}\n\n/*\n * Copy all file descriptors from the old table to the new, expanded table and\n * clear the extra space.  Called with the files spinlock held for write.\n */\nstatic void copy_fdtable(struct fdtable *nfdt, struct fdtable *ofdt)\n{\n\tsize_t cpy, set;\n\n\tBUG_ON(nfdt->max_fds < ofdt->max_fds);\n\n\tcpy = ofdt->max_fds * sizeof(struct file *);\n\tset = (nfdt->max_fds - ofdt->max_fds) * sizeof(struct file *);\n\tmemcpy(nfdt->fd, ofdt->fd, cpy);\n\tmemset((char *)nfdt->fd + cpy, 0, set);\n\n\tcopy_fd_bitmaps(nfdt, ofdt, ofdt->max_fds);\n}\n\nstatic struct fdtable * alloc_fdtable(unsigned int nr)\n{\n\tstruct fdtable *fdt;\n\tvoid *data;\n\n\t/*\n\t * Figure out how many fds we actually want to support in this fdtable.\n\t * Allocation steps are keyed to the size of the fdarray, since it\n\t * grows far faster than any of the other dynamic data. We try to fit\n\t * the fdarray into comfortable page-tuned chunks: starting at 1024B\n\t * and growing in powers of two from there on.\n\t */\n\tnr /= (1024 / sizeof(struct file *));\n\tnr = roundup_pow_of_two(nr + 1);\n\tnr *= (1024 / sizeof(struct file *));\n\t/*\n\t * Note that this can drive nr *below* what we had passed if sysctl_nr_open\n\t * had been set lower between the check in expand_files() and here.  Deal\n\t * with that in caller, it's cheaper that way.\n\t *\n\t * We make sure that nr remains a multiple of BITS_PER_LONG - otherwise\n\t * bitmaps handling below becomes unpleasant, to put it mildly...\n\t */\n\tif (unlikely(nr > sysctl_nr_open))\n\t\tnr = ((sysctl_nr_open - 1) | (BITS_PER_LONG - 1)) + 1;\n\n\tfdt = kmalloc(sizeof(struct fdtable), GFP_KERNEL_ACCOUNT);\n\tif (!fdt)\n\t\tgoto out;\n\tfdt->max_fds = nr;\n\tdata = kvmalloc_array(nr, sizeof(struct file *), GFP_KERNEL_ACCOUNT);\n\tif (!data)\n\t\tgoto out_fdt;\n\tfdt->fd = data;\n\n\tdata = kvmalloc(max_t(size_t,\n\t\t\t\t 2 * nr / BITS_PER_BYTE + BITBIT_SIZE(nr), L1_CACHE_BYTES),\n\t\t\t\t GFP_KERNEL_ACCOUNT);\n\tif (!data)\n\t\tgoto out_arr;\n\tfdt->open_fds = data;\n\tdata += nr / BITS_PER_BYTE;\n\tfdt->close_on_exec = data;\n\tdata += nr / BITS_PER_BYTE;\n\tfdt->full_fds_bits = data;\n\n\treturn fdt;\n\nout_arr:\n\tkvfree(fdt->fd);\nout_fdt:\n\tkfree(fdt);\nout:\n\treturn NULL;\n}\n\n/*\n * Expand the file descriptor table.\n * This function will allocate a new fdtable and both fd array and fdset, of\n * the given size.\n * Return <0 error code on error; 1 on successful completion.\n * The files->file_lock should be held on entry, and will be held on exit.\n */\nstatic int expand_fdtable(struct files_struct *files, unsigned int nr)\n\t__releases(files->file_lock)\n\t__acquires(files->file_lock)\n{\n\tstruct fdtable *new_fdt, *cur_fdt;\n\n\tspin_unlock(&files->file_lock);\n\tnew_fdt = alloc_fdtable(nr);\n\n\t/* make sure all __fd_install() have seen resize_in_progress\n\t * or have finished their rcu_read_lock_sched() section.\n\t */\n\tif (atomic_read(&files->count) > 1)\n\t\tsynchronize_rcu();\n\n\tspin_lock(&files->file_lock);\n\tif (!new_fdt)\n\t\treturn -ENOMEM;\n\t/*\n\t * extremely unlikely race - sysctl_nr_open decreased between the check in\n\t * caller and alloc_fdtable().  Cheaper to catch it here...\n\t */\n\tif (unlikely(new_fdt->max_fds <= nr)) {\n\t\t__free_fdtable(new_fdt);\n\t\treturn -EMFILE;\n\t}\n\tcur_fdt = files_fdtable(files);\n\tBUG_ON(nr < cur_fdt->max_fds);\n\tcopy_fdtable(new_fdt, cur_fdt);\n\trcu_assign_pointer(files->fdt, new_fdt);\n\tif (cur_fdt != &files->fdtab)\n\t\tcall_rcu(&cur_fdt->rcu, free_fdtable_rcu);\n\t/* coupled with smp_rmb() in __fd_install() */\n\tsmp_wmb();\n\treturn 1;\n}\n\n/*\n * Expand files.\n * This function will expand the file structures, if the requested size exceeds\n * the current capacity and there is room for expansion.\n * Return <0 error code on error; 0 when nothing done; 1 when files were\n * expanded and execution may have blocked.\n * The files->file_lock should be held on entry, and will be held on exit.\n */\nstatic int expand_files(struct files_struct *files, unsigned int nr)\n\t__releases(files->file_lock)\n\t__acquires(files->file_lock)\n{\n\tstruct fdtable *fdt;\n\tint expanded = 0;\n\nrepeat:\n\tfdt = files_fdtable(files);\n\n\t/* Do we need to expand? */\n\tif (nr < fdt->max_fds)\n\t\treturn expanded;\n\n\t/* Can we expand? */\n\tif (nr >= sysctl_nr_open)\n\t\treturn -EMFILE;\n\n\tif (unlikely(files->resize_in_progress)) {\n\t\tspin_unlock(&files->file_lock);\n\t\texpanded = 1;\n\t\twait_event(files->resize_wait, !files->resize_in_progress);\n\t\tspin_lock(&files->file_lock);\n\t\tgoto repeat;\n\t}\n\n\t/* All good, so we try */\n\tfiles->resize_in_progress = true;\n\texpanded = expand_fdtable(files, nr);\n\tfiles->resize_in_progress = false;\n\n\twake_up_all(&files->resize_wait);\n\treturn expanded;\n}\n\nstatic inline void __set_close_on_exec(unsigned int fd, struct fdtable *fdt)\n{\n\t__set_bit(fd, fdt->close_on_exec);\n}\n\nstatic inline void __clear_close_on_exec(unsigned int fd, struct fdtable *fdt)\n{\n\tif (test_bit(fd, fdt->close_on_exec))\n\t\t__clear_bit(fd, fdt->close_on_exec);\n}\n\nstatic inline void __set_open_fd(unsigned int fd, struct fdtable *fdt)\n{\n\t__set_bit(fd, fdt->open_fds);\n\tfd /= BITS_PER_LONG;\n\tif (!~fdt->open_fds[fd])\n\t\t__set_bit(fd, fdt->full_fds_bits);\n}\n\nstatic inline void __clear_open_fd(unsigned int fd, struct fdtable *fdt)\n{\n\t__clear_bit(fd, fdt->open_fds);\n\t__clear_bit(fd / BITS_PER_LONG, fdt->full_fds_bits);\n}\n\nstatic unsigned int count_open_files(struct fdtable *fdt)\n{\n\tunsigned int size = fdt->max_fds;\n\tunsigned int i;\n\n\t/* Find the last open fd */\n\tfor (i = size / BITS_PER_LONG; i > 0; ) {\n\t\tif (fdt->open_fds[--i])\n\t\t\tbreak;\n\t}\n\ti = (i + 1) * BITS_PER_LONG;\n\treturn i;\n}\n\n/*\n * Allocate a new files structure and copy contents from the\n * passed in files structure.\n * errorp will be valid only when the returned files_struct is NULL.\n */\nstruct files_struct *dup_fd(struct files_struct *oldf, int *errorp)\n{\n\tstruct files_struct *newf;\n\tstruct file **old_fds, **new_fds;\n\tunsigned int open_files, i;\n\tstruct fdtable *old_fdt, *new_fdt;\n\n\t*errorp = -ENOMEM;\n\tnewf = kmem_cache_alloc(files_cachep, GFP_KERNEL);\n\tif (!newf)\n\t\tgoto out;\n\n\tatomic_set(&newf->count, 1);\n\n\tspin_lock_init(&newf->file_lock);\n\tnewf->resize_in_progress = false;\n\tinit_waitqueue_head(&newf->resize_wait);\n\tnewf->next_fd = 0;\n\tnew_fdt = &newf->fdtab;\n\tnew_fdt->max_fds = NR_OPEN_DEFAULT;\n\tnew_fdt->close_on_exec = newf->close_on_exec_init;\n\tnew_fdt->open_fds = newf->open_fds_init;\n\tnew_fdt->full_fds_bits = newf->full_fds_bits_init;\n\tnew_fdt->fd = &newf->fd_array[0];\n\n\tspin_lock(&oldf->file_lock);\n\told_fdt = files_fdtable(oldf);\n\topen_files = count_open_files(old_fdt);\n\n\t/*\n\t * Check whether we need to allocate a larger fd array and fd set.\n\t */\n\twhile (unlikely(open_files > new_fdt->max_fds)) {\n\t\tspin_unlock(&oldf->file_lock);\n\n\t\tif (new_fdt != &newf->fdtab)\n\t\t\t__free_fdtable(new_fdt);\n\n\t\tnew_fdt = alloc_fdtable(open_files - 1);\n\t\tif (!new_fdt) {\n\t\t\t*errorp = -ENOMEM;\n\t\t\tgoto out_release;\n\t\t}\n\n\t\t/* beyond sysctl_nr_open; nothing to do */\n\t\tif (unlikely(new_fdt->max_fds < open_files)) {\n\t\t\t__free_fdtable(new_fdt);\n\t\t\t*errorp = -EMFILE;\n\t\t\tgoto out_release;\n\t\t}\n\n\t\t/*\n\t\t * Reacquire the oldf lock and a pointer to its fd table\n\t\t * who knows it may have a new bigger fd table. We need\n\t\t * the latest pointer.\n\t\t */\n\t\tspin_lock(&oldf->file_lock);\n\t\told_fdt = files_fdtable(oldf);\n\t\topen_files = count_open_files(old_fdt);\n\t}\n\n\tcopy_fd_bitmaps(new_fdt, old_fdt, open_files);\n\n\told_fds = old_fdt->fd;\n\tnew_fds = new_fdt->fd;\n\n\tfor (i = open_files; i != 0; i--) {\n\t\tstruct file *f = *old_fds++;\n\t\tif (f) {\n\t\t\tget_file(f);\n\t\t} else {\n\t\t\t/*\n\t\t\t * The fd may be claimed in the fd bitmap but not yet\n\t\t\t * instantiated in the files array if a sibling thread\n\t\t\t * is partway through open().  So make sure that this\n\t\t\t * fd is available to the new process.\n\t\t\t */\n\t\t\t__clear_open_fd(open_files - i, new_fdt);\n\t\t}\n\t\trcu_assign_pointer(*new_fds++, f);\n\t}\n\tspin_unlock(&oldf->file_lock);\n\n\t/* clear the remainder */\n\tmemset(new_fds, 0, (new_fdt->max_fds - open_files) * sizeof(struct file *));\n\n\trcu_assign_pointer(newf->fdt, new_fdt);\n\n\treturn newf;\n\nout_release:\n\tkmem_cache_free(files_cachep, newf);\nout:\n\treturn NULL;\n}\n\nstatic struct fdtable *close_files(struct files_struct * files)\n{\n\t/*\n\t * It is safe to dereference the fd table without RCU or\n\t * ->file_lock because this is the last reference to the\n\t * files structure.\n\t */\n\tstruct fdtable *fdt = rcu_dereference_raw(files->fdt);\n\tunsigned int i, j = 0;\n\n\tfor (;;) {\n\t\tunsigned long set;\n\t\ti = j * BITS_PER_LONG;\n\t\tif (i >= fdt->max_fds)\n\t\t\tbreak;\n\t\tset = fdt->open_fds[j++];\n\t\twhile (set) {\n\t\t\tif (set & 1) {\n\t\t\t\tstruct file * file = xchg(&fdt->fd[i], NULL);\n\t\t\t\tif (file) {\n\t\t\t\t\tfilp_close(file, files);\n\t\t\t\t\tcond_resched();\n\t\t\t\t}\n\t\t\t}\n\t\t\ti++;\n\t\t\tset >>= 1;\n\t\t}\n\t}\n\n\treturn fdt;\n}\n\nstruct files_struct *get_files_struct(struct task_struct *task)\n{\n\tstruct files_struct *files;\n\n\ttask_lock(task);\n\tfiles = task->files;\n\tif (files)\n\t\tatomic_inc(&files->count);\n\ttask_unlock(task);\n\n\treturn files;\n}\n\nvoid put_files_struct(struct files_struct *files)\n{\n\tif (atomic_dec_and_test(&files->count)) {\n\t\tstruct fdtable *fdt = close_files(files);\n\n\t\t/* free the arrays if they are not embedded */\n\t\tif (fdt != &files->fdtab)\n\t\t\t__free_fdtable(fdt);\n\t\tkmem_cache_free(files_cachep, files);\n\t}\n}\n\nvoid reset_files_struct(struct files_struct *files)\n{\n\tstruct task_struct *tsk = current;\n\tstruct files_struct *old;\n\n\told = tsk->files;\n\ttask_lock(tsk);\n\ttsk->files = files;\n\ttask_unlock(tsk);\n\tput_files_struct(old);\n}\n\nvoid exit_files(struct task_struct *tsk)\n{\n\tstruct files_struct * files = tsk->files;\n\n\tif (files) {\n\t\ttask_lock(tsk);\n\t\ttsk->files = NULL;\n\t\ttask_unlock(tsk);\n\t\tput_files_struct(files);\n\t}\n}\n\nstruct files_struct init_files = {\n\t.count\t\t= ATOMIC_INIT(1),\n\t.fdt\t\t= &init_files.fdtab,\n\t.fdtab\t\t= {\n\t\t.max_fds\t= NR_OPEN_DEFAULT,\n\t\t.fd\t\t= &init_files.fd_array[0],\n\t\t.close_on_exec\t= init_files.close_on_exec_init,\n\t\t.open_fds\t= init_files.open_fds_init,\n\t\t.full_fds_bits\t= init_files.full_fds_bits_init,\n\t},\n\t.file_lock\t= __SPIN_LOCK_UNLOCKED(init_files.file_lock),\n\t.resize_wait\t= __WAIT_QUEUE_HEAD_INITIALIZER(init_files.resize_wait),\n};\n\nstatic unsigned int find_next_fd(struct fdtable *fdt, unsigned int start)\n{\n\tunsigned int maxfd = fdt->max_fds;\n\tunsigned int maxbit = maxfd / BITS_PER_LONG;\n\tunsigned int bitbit = start / BITS_PER_LONG;\n\n\tbitbit = find_next_zero_bit(fdt->full_fds_bits, maxbit, bitbit) * BITS_PER_LONG;\n\tif (bitbit > maxfd)\n\t\treturn maxfd;\n\tif (bitbit > start)\n\t\tstart = bitbit;\n\treturn find_next_zero_bit(fdt->open_fds, maxfd, start);\n}\n\n/*\n * allocate a file descriptor, mark it busy.\n */\nint __alloc_fd(struct files_struct *files,\n\t       unsigned start, unsigned end, unsigned flags)\n{\n\tunsigned int fd;\n\tint error;\n\tstruct fdtable *fdt;\n\n\tspin_lock(&files->file_lock);\nrepeat:\n\tfdt = files_fdtable(files);\n\tfd = start;\n\tif (fd < files->next_fd)\n\t\tfd = files->next_fd;\n\n\tif (fd < fdt->max_fds)\n\t\tfd = find_next_fd(fdt, fd);\n\n\t/*\n\t * N.B. For clone tasks sharing a files structure, this test\n\t * will limit the total number of files that can be opened.\n\t */\n\terror = -EMFILE;\n\tif (fd >= end)\n\t\tgoto out;\n\n\terror = expand_files(files, fd);\n\tif (error < 0)\n\t\tgoto out;\n\n\t/*\n\t * If we needed to expand the fs array we\n\t * might have blocked - try again.\n\t */\n\tif (error)\n\t\tgoto repeat;\n\n\tif (start <= files->next_fd)\n\t\tfiles->next_fd = fd + 1;\n\n\t__set_open_fd(fd, fdt);\n\tif (flags & O_CLOEXEC)\n\t\t__set_close_on_exec(fd, fdt);\n\telse\n\t\t__clear_close_on_exec(fd, fdt);\n\terror = fd;\n#if 1\n\t/* Sanity check */\n\tif (rcu_access_pointer(fdt->fd[fd]) != NULL) {\n\t\tprintk(KERN_WARNING \"alloc_fd: slot %d not NULL!\\n\", fd);\n\t\trcu_assign_pointer(fdt->fd[fd], NULL);\n\t}\n#endif\n\nout:\n\tspin_unlock(&files->file_lock);\n\treturn error;\n}\n\nstatic int alloc_fd(unsigned start, unsigned flags)\n{\n\treturn __alloc_fd(current->files, start, rlimit(RLIMIT_NOFILE), flags);\n}\n\nint __get_unused_fd_flags(unsigned flags, unsigned long nofile)\n{\n\treturn __alloc_fd(current->files, 0, nofile, flags);\n}\n\nint get_unused_fd_flags(unsigned flags)\n{\n\treturn __get_unused_fd_flags(flags, rlimit(RLIMIT_NOFILE));\n}\nEXPORT_SYMBOL(get_unused_fd_flags);\n\nstatic void __put_unused_fd(struct files_struct *files, unsigned int fd)\n{\n\tstruct fdtable *fdt = files_fdtable(files);\n\t__clear_open_fd(fd, fdt);\n\tif (fd < files->next_fd)\n\t\tfiles->next_fd = fd;\n}\n\nvoid put_unused_fd(unsigned int fd)\n{\n\tstruct files_struct *files = current->files;\n\tspin_lock(&files->file_lock);\n\t__put_unused_fd(files, fd);\n\tspin_unlock(&files->file_lock);\n}\n\nEXPORT_SYMBOL(put_unused_fd);\n\n/*\n * Install a file pointer in the fd array.\n *\n * The VFS is full of places where we drop the files lock between\n * setting the open_fds bitmap and installing the file in the file\n * array.  At any such point, we are vulnerable to a dup2() race\n * installing a file in the array before us.  We need to detect this and\n * fput() the struct file we are about to overwrite in this case.\n *\n * It should never happen - if we allow dup2() do it, _really_ bad things\n * will follow.\n *\n * NOTE: __fd_install() variant is really, really low-level; don't\n * use it unless you are forced to by truly lousy API shoved down\n * your throat.  'files' *MUST* be either current->files or obtained\n * by get_files_struct(current) done by whoever had given it to you,\n * or really bad things will happen.  Normally you want to use\n * fd_install() instead.\n */\n\nvoid __fd_install(struct files_struct *files, unsigned int fd,\n\t\tstruct file *file)\n{\n\tstruct fdtable *fdt;\n\n\trcu_read_lock_sched();\n\n\tif (unlikely(files->resize_in_progress)) {\n\t\trcu_read_unlock_sched();\n\t\tspin_lock(&files->file_lock);\n\t\tfdt = files_fdtable(files);\n\t\tBUG_ON(fdt->fd[fd] != NULL);\n\t\trcu_assign_pointer(fdt->fd[fd], file);\n\t\tspin_unlock(&files->file_lock);\n\t\treturn;\n\t}\n\t/* coupled with smp_wmb() in expand_fdtable() */\n\tsmp_rmb();\n\tfdt = rcu_dereference_sched(files->fdt);\n\tBUG_ON(fdt->fd[fd] != NULL);\n\trcu_assign_pointer(fdt->fd[fd], file);\n\trcu_read_unlock_sched();\n}\n\nvoid fd_install(unsigned int fd, struct file *file)\n{\n\t__fd_install(current->files, fd, file);\n}\n\nEXPORT_SYMBOL(fd_install);\n\n/*\n * The same warnings as for __alloc_fd()/__fd_install() apply here...\n */\nint __close_fd(struct files_struct *files, unsigned fd)\n{\n\tstruct file *file;\n\tstruct fdtable *fdt;\n\n\tspin_lock(&files->file_lock);\n\tfdt = files_fdtable(files);\n\tif (fd >= fdt->max_fds)\n\t\tgoto out_unlock;\n\tfd = array_index_nospec(fd, fdt->max_fds);\n\tfile = fdt->fd[fd];\n\tif (!file)\n\t\tgoto out_unlock;\n\trcu_assign_pointer(fdt->fd[fd], NULL);\n\t__put_unused_fd(files, fd);\n\tspin_unlock(&files->file_lock);\n\treturn filp_close(file, files);\n\nout_unlock:\n\tspin_unlock(&files->file_lock);\n\treturn -EBADF;\n}\nEXPORT_SYMBOL(__close_fd); /* for ksys_close() */\n\n/*\n * variant of __close_fd that gets a ref on the file for later fput.\n * The caller must ensure that filp_close() called on the file, and then\n * an fput().\n */\nint __close_fd_get_file(unsigned int fd, struct file **res)\n{\n\tstruct files_struct *files = current->files;\n\tstruct file *file;\n\tstruct fdtable *fdt;\n\n\tspin_lock(&files->file_lock);\n\tfdt = files_fdtable(files);\n\tif (fd >= fdt->max_fds)\n\t\tgoto out_unlock;\n\tfile = fdt->fd[fd];\n\tif (!file)\n\t\tgoto out_unlock;\n\trcu_assign_pointer(fdt->fd[fd], NULL);\n\t__put_unused_fd(files, fd);\n\tspin_unlock(&files->file_lock);\n\tget_file(file);\n\t*res = file;\n\treturn 0;\n\nout_unlock:\n\tspin_unlock(&files->file_lock);\n\t*res = NULL;\n\treturn -ENOENT;\n}\n\nvoid do_close_on_exec(struct files_struct *files)\n{\n\tunsigned i;\n\tstruct fdtable *fdt;\n\n\t/* exec unshares first */\n\tspin_lock(&files->file_lock);\n\tfor (i = 0; ; i++) {\n\t\tunsigned long set;\n\t\tunsigned fd = i * BITS_PER_LONG;\n\t\tfdt = files_fdtable(files);\n\t\tif (fd >= fdt->max_fds)\n\t\t\tbreak;\n\t\tset = fdt->close_on_exec[i];\n\t\tif (!set)\n\t\t\tcontinue;\n\t\tfdt->close_on_exec[i] = 0;\n\t\tfor ( ; set ; fd++, set >>= 1) {\n\t\t\tstruct file *file;\n\t\t\tif (!(set & 1))\n\t\t\t\tcontinue;\n\t\t\tfile = fdt->fd[fd];\n\t\t\tif (!file)\n\t\t\t\tcontinue;\n\t\t\trcu_assign_pointer(fdt->fd[fd], NULL);\n\t\t\t__put_unused_fd(files, fd);\n\t\t\tspin_unlock(&files->file_lock);\n\t\t\tfilp_close(file, files);\n\t\t\tcond_resched();\n\t\t\tspin_lock(&files->file_lock);\n\t\t}\n\n\t}\n\tspin_unlock(&files->file_lock);\n}\n\nstatic struct file *__fget(unsigned int fd, fmode_t mask, unsigned int refs)\n{\n\tstruct files_struct *files = current->files;\n\tstruct file *file;\n\n\trcu_read_lock();\nloop:\n\tfile = fcheck_files(files, fd);\n\tif (file) {\n\t\t/* File object ref couldn't be taken.\n\t\t * dup2() atomicity guarantee is the reason\n\t\t * we loop to catch the new file (or NULL pointer)\n\t\t */\n\t\tif (file->f_mode & mask)\n\t\t\tfile = NULL;\n\t\telse if (!get_file_rcu_many(file, refs))\n\t\t\tgoto loop;\n\t\telse if (__fcheck_files(files, fd) != file) {\n\t\t\tfput_many(file, refs);\n\t\t\tgoto loop;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn file;\n}\n\nstruct file *fget_many(unsigned int fd, unsigned int refs)\n{\n\treturn __fget(fd, FMODE_PATH, refs);\n}\n\nstruct file *fget(unsigned int fd)\n{\n\treturn __fget(fd, FMODE_PATH, 1);\n}\nEXPORT_SYMBOL(fget);\n\nstruct file *fget_raw(unsigned int fd)\n{\n\treturn __fget(fd, 0, 1);\n}\nEXPORT_SYMBOL(fget_raw);\n\n/*\n * Lightweight file lookup - no refcnt increment if fd table isn't shared.\n *\n * You can use this instead of fget if you satisfy all of the following\n * conditions:\n * 1) You must call fput_light before exiting the syscall and returning control\n *    to userspace (i.e. you cannot remember the returned struct file * after\n *    returning to userspace).\n * 2) You must not call filp_close on the returned struct file * in between\n *    calls to fget_light and fput_light.\n * 3) You must not clone the current task in between the calls to fget_light\n *    and fput_light.\n *\n * The fput_needed flag returned by fget_light should be passed to the\n * corresponding fput_light.\n */\nstatic unsigned long __fget_light(unsigned int fd, fmode_t mask)\n{\n\tstruct files_struct *files = current->files;\n\tstruct file *file;\n\n\tif (atomic_read(&files->count) == 1) {\n\t\tfile = __fcheck_files(files, fd);\n\t\tif (!file || unlikely(file->f_mode & mask))\n\t\t\treturn 0;\n\t\treturn (unsigned long)file;\n\t} else {\n\t\tfile = __fget(fd, mask, 1);\n\t\tif (!file)\n\t\t\treturn 0;\n\t\treturn FDPUT_FPUT | (unsigned long)file;\n\t}\n}\nunsigned long __fdget(unsigned int fd)\n{\n\treturn __fget_light(fd, FMODE_PATH);\n}\nEXPORT_SYMBOL(__fdget);\n\nunsigned long __fdget_raw(unsigned int fd)\n{\n\treturn __fget_light(fd, 0);\n}\n\nunsigned long __fdget_pos(unsigned int fd)\n{\n\tunsigned long v = __fdget(fd);\n\tstruct file *file = (struct file *)(v & ~3);\n\n\tif (file && (file->f_mode & FMODE_ATOMIC_POS)) {\n\t\tif (file_count(file) > 1) {\n\t\t\tv |= FDPUT_POS_UNLOCK;\n\t\t\tmutex_lock(&file->f_pos_lock);\n\t\t}\n\t}\n\treturn v;\n}\n\nvoid __f_unlock_pos(struct file *f)\n{\n\tmutex_unlock(&f->f_pos_lock);\n}\n\n/*\n * We only lock f_pos if we have threads or if the file might be\n * shared with another process. In both cases we'll have an elevated\n * file count (done either by fdget() or by fork()).\n */\n\nvoid set_close_on_exec(unsigned int fd, int flag)\n{\n\tstruct files_struct *files = current->files;\n\tstruct fdtable *fdt;\n\tspin_lock(&files->file_lock);\n\tfdt = files_fdtable(files);\n\tif (flag)\n\t\t__set_close_on_exec(fd, fdt);\n\telse\n\t\t__clear_close_on_exec(fd, fdt);\n\tspin_unlock(&files->file_lock);\n}\n\nbool get_close_on_exec(unsigned int fd)\n{\n\tstruct files_struct *files = current->files;\n\tstruct fdtable *fdt;\n\tbool res;\n\trcu_read_lock();\n\tfdt = files_fdtable(files);\n\tres = close_on_exec(fd, fdt);\n\trcu_read_unlock();\n\treturn res;\n}\n\nstatic int do_dup2(struct files_struct *files,\n\tstruct file *file, unsigned fd, unsigned flags)\n__releases(&files->file_lock)\n{\n\tstruct file *tofree;\n\tstruct fdtable *fdt;\n\n\t/*\n\t * We need to detect attempts to do dup2() over allocated but still\n\t * not finished descriptor.  NB: OpenBSD avoids that at the price of\n\t * extra work in their equivalent of fget() - they insert struct\n\t * file immediately after grabbing descriptor, mark it larval if\n\t * more work (e.g. actual opening) is needed and make sure that\n\t * fget() treats larval files as absent.  Potentially interesting,\n\t * but while extra work in fget() is trivial, locking implications\n\t * and amount of surgery on open()-related paths in VFS are not.\n\t * FreeBSD fails with -EBADF in the same situation, NetBSD \"solution\"\n\t * deadlocks in rather amusing ways, AFAICS.  All of that is out of\n\t * scope of POSIX or SUS, since neither considers shared descriptor\n\t * tables and this condition does not arise without those.\n\t */\n\tfdt = files_fdtable(files);\n\ttofree = fdt->fd[fd];\n\tif (!tofree && fd_is_open(fd, fdt))\n\t\tgoto Ebusy;\n\tget_file(file);\n\trcu_assign_pointer(fdt->fd[fd], file);\n\t__set_open_fd(fd, fdt);\n\tif (flags & O_CLOEXEC)\n\t\t__set_close_on_exec(fd, fdt);\n\telse\n\t\t__clear_close_on_exec(fd, fdt);\n\tspin_unlock(&files->file_lock);\n\n\tif (tofree)\n\t\tfilp_close(tofree, files);\n\n\treturn fd;\n\nEbusy:\n\tspin_unlock(&files->file_lock);\n\treturn -EBUSY;\n}\n\nint replace_fd(unsigned fd, struct file *file, unsigned flags)\n{\n\tint err;\n\tstruct files_struct *files = current->files;\n\n\tif (!file)\n\t\treturn __close_fd(files, fd);\n\n\tif (fd >= rlimit(RLIMIT_NOFILE))\n\t\treturn -EBADF;\n\n\tspin_lock(&files->file_lock);\n\terr = expand_files(files, fd);\n\tif (unlikely(err < 0))\n\t\tgoto out_unlock;\n\treturn do_dup2(files, file, fd, flags);\n\nout_unlock:\n\tspin_unlock(&files->file_lock);\n\treturn err;\n}\n\nstatic int ksys_dup3(unsigned int oldfd, unsigned int newfd, int flags)\n{\n\tint err = -EBADF;\n\tstruct file *file;\n\tstruct files_struct *files = current->files;\n\n\tif ((flags & ~O_CLOEXEC) != 0)\n\t\treturn -EINVAL;\n\n\tif (unlikely(oldfd == newfd))\n\t\treturn -EINVAL;\n\n\tif (newfd >= rlimit(RLIMIT_NOFILE))\n\t\treturn -EBADF;\n\n\tspin_lock(&files->file_lock);\n\terr = expand_files(files, newfd);\n\tfile = fcheck(oldfd);\n\tif (unlikely(!file))\n\t\tgoto Ebadf;\n\tif (unlikely(err < 0)) {\n\t\tif (err == -EMFILE)\n\t\t\tgoto Ebadf;\n\t\tgoto out_unlock;\n\t}\n\treturn do_dup2(files, file, newfd, flags);\n\nEbadf:\n\terr = -EBADF;\nout_unlock:\n\tspin_unlock(&files->file_lock);\n\treturn err;\n}\n\nSYSCALL_DEFINE3(dup3, unsigned int, oldfd, unsigned int, newfd, int, flags)\n{\n\treturn ksys_dup3(oldfd, newfd, flags);\n}\n\nSYSCALL_DEFINE2(dup2, unsigned int, oldfd, unsigned int, newfd)\n{\n\tif (unlikely(newfd == oldfd)) { /* corner case */\n\t\tstruct files_struct *files = current->files;\n\t\tint retval = oldfd;\n\n\t\trcu_read_lock();\n\t\tif (!fcheck_files(files, oldfd))\n\t\t\tretval = -EBADF;\n\t\trcu_read_unlock();\n\t\treturn retval;\n\t}\n\treturn ksys_dup3(oldfd, newfd, 0);\n}\n\nint ksys_dup(unsigned int fildes)\n{\n\tint ret = -EBADF;\n\tstruct file *file = fget_raw(fildes);\n\n\tif (file) {\n\t\tret = get_unused_fd_flags(0);\n\t\tif (ret >= 0)\n\t\t\tfd_install(ret, file);\n\t\telse\n\t\t\tfput(file);\n\t}\n\treturn ret;\n}\n\nSYSCALL_DEFINE1(dup, unsigned int, fildes)\n{\n\treturn ksys_dup(fildes);\n}\n\nint f_dupfd(unsigned int from, struct file *file, unsigned flags)\n{\n\tint err;\n\tif (from >= rlimit(RLIMIT_NOFILE))\n\t\treturn -EINVAL;\n\terr = alloc_fd(from, flags);\n\tif (err >= 0) {\n\t\tget_file(file);\n\t\tfd_install(err, file);\n\t}\n\treturn err;\n}\n\nint iterate_fd(struct files_struct *files, unsigned n,\n\t\tint (*f)(const void *, struct file *, unsigned),\n\t\tconst void *p)\n{\n\tstruct fdtable *fdt;\n\tint res = 0;\n\tif (!files)\n\t\treturn 0;\n\tspin_lock(&files->file_lock);\n\tfor (fdt = files_fdtable(files); n < fdt->max_fds; n++) {\n\t\tstruct file *file;\n\t\tfile = rcu_dereference_check_fdtable(files, fdt->fd[n]);\n\t\tif (!file)\n\t\t\tcontinue;\n\t\tres = f(p, file, n);\n\t\tif (res)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&files->file_lock);\n\treturn res;\n}\nEXPORT_SYMBOL(iterate_fd);\n"], "filenames": ["drivers/android/binder.c", "fs/file.c"], "buggy_code_start_loc": [2239, 653], "buggy_code_end_loc": [2242, 674], "fixing_code_start_loc": [2239, 653], "fixing_code_end_loc": [2245, 676], "type": "CWE-416", "message": "The code in UEK6 U3 was missing an appropiate file descriptor count to be missing. This resulted in a use count error that allowed a file descriptor to a socket to be closed and freed while it was still in use by another portion of the kernel. An attack with local access can operate on the socket, and cause a denial of service. CVSS 3.1 Base Score 5.5 (Availability impacts). CVSS Vector: (CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H).", "other": {"cve": {"id": "CVE-2022-21504", "sourceIdentifier": "secalert_us@oracle.com", "published": "2022-06-14T18:15:08.070", "lastModified": "2022-09-28T19:59:54.500", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The code in UEK6 U3 was missing an appropiate file descriptor count to be missing. This resulted in a use count error that allowed a file descriptor to a socket to be closed and freed while it was still in use by another portion of the kernel. An attack with local access can operate on the socket, and cause a denial of service. CVSS 3.1 Base Score 5.5 (Availability impacts). CVSS Vector: (CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H)."}, {"lang": "es", "value": "En el c\u00f3digo de UEK6 U3 faltaba un recuento de descriptor de archivo adecuado para ser liberado. Esto daba lugar a un error de recuento de uso que permit\u00eda cerrar y liberar un descriptor de archivo de un socket mientras segu\u00eda siendo utilizado por otra parte del kernel. Un ataque con acceso local puede operar sobre el socket y causar una denegaci\u00f3n de servicio. Puntuaci\u00f3n de base CVSS 3.1 5.5 (impactos en la disponibilidad). Vector CVSS: (CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H)"}], "metrics": {"cvssMetricV31": [{"source": "secalert_us@oracle.com", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "nvd@nist.gov", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:oracle:linux:7:-:*:*:*:*:*:*", "matchCriteriaId": "44B8FEDF-6CB0-46E9-9AD7-4445B001C158"}, {"vulnerable": true, "criteria": "cpe:2.3:o:oracle:linux:8:-:*:*:*:*:*:*", "matchCriteriaId": "CA9021D6-6027-42E9-A12D-7EA32C5C63F1"}]}]}], "references": [{"url": "https://github.com/oracle/linux-uek/commit/49c68f5f892d8c2be00e0a89ff2a035422c03b59", "source": "secalert_us@oracle.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/oracle/linux-uek/commit/49c68f5f892d8c2be00e0a89ff2a035422c03b59"}}