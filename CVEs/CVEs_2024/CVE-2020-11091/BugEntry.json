{"buggy_code": ["package net\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"path/filepath\"\n\t\"syscall\"\n\n\t\"github.com/coreos/go-iptables/iptables\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/vishvananda/netlink\"\n\t\"k8s.io/apimachinery/pkg/types\"\n\n\t\"github.com/weaveworks/weave/common\"\n\t\"github.com/weaveworks/weave/common/odp\"\n\t\"github.com/weaveworks/weave/ipam/tracker\"\n\t\"github.com/weaveworks/weave/net/address\"\n\t\"github.com/weaveworks/weave/net/ipset\"\n\t\"github.com/weaveworks/weave/npc\"\n)\n\n/* This code implements three possible configurations to connect\n   containers to the Weave Net overlay:\n\n1. Bridge\n                 +-------+\n(container-veth)-+ weave +-(vethwe-bridge)--(vethwe-pcap)\n                 +-------+\n\n\"weave\" is a Linux bridge. \"vethwe-pcap\" (end of veth pair) is used\nto capture and inject packets, by router/pcap.go.\n\n2. BridgedFastdp\n\n                 +-------+                                    /----------\\\n(container-veth)-+ weave +-(vethwe-bridge)--(vethwe-datapath)-+ datapath +\n                 +-------+                                    \\----------/\n\n\"weave\" is a Linux bridge and \"datapath\" is an Open vSwitch datapath;\nthey are connected via a veth pair. Packet capture and injection use\nthe \"datapath\" device, via \"router/fastdp.go:fastDatapathBridge\"\n\n3. Fastdp\n\n                 /-------\\\n(container-veth)-+ weave +\n                 \\-------/\n\n\"weave\" is an Open vSwitch datapath, and capture/injection are as in\nBridgedFastdp. Not used by default due to missing conntrack support in\ndatapath of old kernel versions (https://github.com/weaveworks/weave/issues/1577).\n*/\n\nconst (\n\tWeaveBridgeName  = \"weave\"\n\tDatapathName     = \"datapath\"\n\tDatapathIfName   = \"vethwe-datapath\"\n\tBridgeIfName     = \"vethwe-bridge\"\n\tPcapIfName       = \"vethwe-pcap\"\n\tNoMasqLocalIpset = ipset.Name(\"weaver-no-masq-local\")\n)\n\ntype Bridge interface {\n\tinit(config *BridgeConfig) error // create and initialise bridge device(s)\n\tattach(veth *netlink.Veth) error // attach veth to bridge\n\tIsFastdp() bool                  // does this bridge use fastdp?\n\tString() string                  // human-readable type string\n}\n\n// Used to indicate a fallback to the Bridge type\nvar errBridgeNotSupported = errors.New(\"bridge not supported\")\n\ntype bridgeImpl struct{ bridge netlink.Link }\ntype fastdpImpl struct{ datapathName string }\ntype bridgedFastdpImpl struct {\n\tbridgeImpl\n\tfastdpImpl\n}\n\n// Returns a string that is consistent with the weave script\nfunc (bridgeImpl) String() string        { return \"bridge\" }\nfunc (fastdpImpl) String() string        { return \"fastdp\" }\nfunc (bridgedFastdpImpl) String() string { return \"bridged_fastdp\" }\n\n// Used to decide whether to manage ODP tunnels\nfunc (bridgeImpl) IsFastdp() bool        { return false }\nfunc (fastdpImpl) IsFastdp() bool        { return true }\nfunc (bridgedFastdpImpl) IsFastdp() bool { return true }\n\nfunc ExistingBridgeType(weaveBridgeName, datapathName string) (Bridge, error) {\n\tbridge, _ := netlink.LinkByName(weaveBridgeName)\n\tdatapath, _ := netlink.LinkByName(datapathName)\n\n\tswitch {\n\tcase bridge == nil && datapath == nil:\n\t\treturn nil, nil\n\tcase isBridge(bridge) && datapath == nil:\n\t\treturn bridgeImpl{bridge: bridge}, nil\n\tcase isDatapath(bridge) && datapath == nil:\n\t\treturn fastdpImpl{datapathName: datapathName}, nil\n\tcase isDatapath(datapath) && isBridge(bridge):\n\t\treturn bridgedFastdpImpl{bridgeImpl{bridge: bridge}, fastdpImpl{datapathName: datapathName}}, nil\n\tdefault:\n\t\treturn nil, errors.New(\"Inconsistent bridge state detected. Please do 'weave reset' and try again\")\n\t}\n}\n\nfunc EnforceAddrAssignType(bridgeName string) (setAddr bool, err error) {\n\tsysctlFilename := filepath.Join(\"/sys/class/net/\", bridgeName, \"/addr_assign_type\")\n\taddrAssignType, err := ioutil.ReadFile(sysctlFilename)\n\tif err != nil {\n\t\treturn false, errors.Wrapf(err, \"reading %q\", sysctlFilename)\n\t}\n\n\t// From include/uapi/linux/netdevice.h\n\t// #define NET_ADDR_PERM       0   /* address is permanent (default) */\n\t// #define NET_ADDR_RANDOM     1   /* address is generated randomly */\n\t// #define NET_ADDR_STOLEN     2   /* address is stolen from other device */\n\t// #define NET_ADDR_SET        3   /* address is set using dev_set_mac_address() */\n\t// Note the file typically has a newline at the end, so we just look at the first char\n\tif addrAssignType[0] != '3' {\n\t\tlink, err := netlink.LinkByName(bridgeName)\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrapf(err, \"EnforceAddrAssignType finding bridge %s\", bridgeName)\n\t\t}\n\n\t\tmac, err := RandomMAC()\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrap(err, \"creating random MAC\")\n\t\t}\n\n\t\tif err := netlink.LinkSetHardwareAddr(link, mac); err != nil {\n\t\t\treturn false, errors.Wrapf(err, \"setting bridge %s address to %v\", bridgeName, mac)\n\t\t}\n\t\treturn true, nil\n\t}\n\n\treturn false, nil\n}\n\nfunc isBridge(link netlink.Link) bool {\n\t_, isBridge := link.(*netlink.Bridge)\n\treturn isBridge\n}\n\nfunc isDatapath(link netlink.Link) bool {\n\tswitch link.(type) {\n\tcase *netlink.GenericLink:\n\t\treturn link.Type() == \"openvswitch\"\n\tcase *netlink.Device:\n\t\t// Assume it's our openvswitch device, and the kernel has not been updated to report the kind.\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\nfunc DetectHairpin(portIfName string, log *logrus.Logger) error {\n\tlink, err := netlink.LinkByName(portIfName)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Unable to find link %q: %s\", portIfName, err)\n\t}\n\n\tch := make(chan netlink.LinkUpdate)\n\t// See EnsureInterface for why done channel is not passed\n\tif err := netlink.LinkSubscribe(ch, nil); err != nil {\n\t\treturn fmt.Errorf(\"Unable to subscribe to netlink updates: %s\", err)\n\t}\n\n\tpi, err := netlink.LinkGetProtinfo(link)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Unable to get link protinfo %q: %s\", portIfName, err)\n\t}\n\tif pi.Hairpin {\n\t\treturn fmt.Errorf(\"Hairpin mode enabled on %q\", portIfName)\n\t}\n\n\tgo func() {\n\t\tfor up := range ch {\n\t\t\tif up.Attrs().Name == portIfName && up.Attrs().Protinfo != nil &&\n\t\t\t\tup.Attrs().Protinfo.Hairpin {\n\t\t\t\tlog.Errorf(\"Hairpin mode enabled on %q\", portIfName)\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn nil\n}\n\nvar ErrBridgeNoIP = fmt.Errorf(\"Bridge has no IP address\")\n\nfunc FindBridgeIP(bridgeName string, subnet *net.IPNet) (net.IP, error) {\n\tnetdev, err := GetBridgeNetDev(bridgeName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Failed to get netdev for %q bridge: %s\", bridgeName, err)\n\t}\n\tif len(netdev.CIDRs) == 0 {\n\t\treturn nil, ErrBridgeNoIP\n\t}\n\tif subnet != nil {\n\t\tfor _, cidr := range netdev.CIDRs {\n\t\t\tif subnet.Contains(cidr.IP) {\n\t\t\t\treturn cidr.IP, nil\n\t\t\t}\n\t\t}\n\t}\n\t// No subnet, or none in the required subnet; just return the first one\n\treturn netdev.CIDRs[0].IP, nil\n}\n\ntype BridgeConfig struct {\n\tDockerBridgeName string\n\tWeaveBridgeName  string\n\tDatapathName     string\n\tNoFastdp         bool\n\tNoBridgedFastdp  bool\n\tAWSVPC           bool\n\tNPC              bool\n\tMTU              int\n\tMac              string\n\tPort             int\n\tNoMasqLocal      bool\n}\n\nfunc (config *BridgeConfig) configuredBridgeType() Bridge {\n\tswitch {\n\tcase config.NoFastdp:\n\t\treturn bridgeImpl{}\n\tcase config.NoBridgedFastdp:\n\t\treturn fastdpImpl{datapathName: config.WeaveBridgeName}\n\tdefault:\n\t\treturn bridgedFastdpImpl{fastdpImpl: fastdpImpl{datapathName: config.DatapathName}}\n\t}\n}\n\nfunc EnsureBridge(procPath string, config *BridgeConfig, log *logrus.Logger, ips ipset.Interface) (Bridge, error) {\n\texistingBridgeType, err := ExistingBridgeType(config.WeaveBridgeName, config.DatapathName)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbridgeType := config.configuredBridgeType()\n\n\tif existingBridgeType != nil && bridgeType.String() != existingBridgeType.String() {\n\t\treturn nil,\n\t\t\tfmt.Errorf(\"Existing bridge type %q is different than requested %q. Please do 'weave reset' and try again\",\n\t\t\t\texistingBridgeType, bridgeType)\n\t}\n\n\tfor {\n\t\tif err := bridgeType.init(config); err != nil {\n\t\t\tif errors.Cause(err) == errBridgeNotSupported {\n\t\t\t\tlog.Warnf(\"Skipping bridge creation of %q due to: %s\", bridgeType, err)\n\t\t\t\tbridgeType = bridgeImpl{}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tbreak\n\t}\n\n\tif err := configureIPTables(config, ips); err != nil {\n\t\treturn bridgeType, errors.Wrap(err, \"configuring iptables\")\n\t}\n\n\tif config.AWSVPC {\n\t\t// Set proxy_arp on the bridge, so that it could accept packets destined\n\t\t// to containers within the same subnet but running on remote hosts.\n\t\t// Without it, exact routes on each container are required.\n\t\tif err := sysctl(procPath, \"net/ipv4/conf/\"+config.WeaveBridgeName+\"/proxy_arp\", \"1\"); err != nil {\n\t\t\treturn bridgeType, errors.Wrap(err, \"setting proxy_arp\")\n\t\t}\n\t\t// Avoid delaying the first ARP request. Also, setting it to 0 avoids\n\t\t// placing the request into a bounded queue as it can be seen:\n\t\t// https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/net/ipv4/arp.c?id=refs/tags/v4.6.1#n819\n\t\tif err := sysctl(procPath, \"net/ipv4/neigh/\"+config.WeaveBridgeName+\"/proxy_delay\", \"0\"); err != nil {\n\t\t\treturn bridgeType, errors.Wrap(err, \"setting proxy_arp\")\n\t\t}\n\t}\n\n\tif err := linkSetUpByName(config.WeaveBridgeName); err != nil {\n\t\treturn bridgeType, err\n\t}\n\n\tif err := monitorInterface(config.WeaveBridgeName, log); err != nil {\n\t\treturn bridgeType, err\n\t}\n\n\tif err := configureARPCache(procPath, config.WeaveBridgeName); err != nil {\n\t\treturn bridgeType, errors.Wrapf(err, \"configuring ARP cache on bridge %q\", config.WeaveBridgeName)\n\t}\n\n\t// NB: No concurrent call to Expose is possible, as EnsureBridge is called\n\t// before any service has been started.\n\tif err := reexpose(config, log); err != nil {\n\t\treturn bridgeType, err\n\t}\n\n\treturn bridgeType, nil\n}\n\nfunc (b bridgeImpl) initPrep(config *BridgeConfig) error {\n\tmac, err := net.ParseMAC(config.Mac)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"parsing bridge MAC %q\", config.Mac)\n\t}\n\n\tlinkAttrs := netlink.NewLinkAttrs()\n\tlinkAttrs.Name = config.WeaveBridgeName\n\t// NB: Do not set MAC addr when creating the bridge, set it manually\n\t// afterwards instead. Otherwise, on an older than 3.14 kernel FDB\n\t// entry won't be created which results in containers not being able to\n\t// reach the bridge w/o promiscuous mode.\n\tif config.MTU == 0 {\n\t\tconfig.MTU = 65535\n\t}\n\tb.bridge = &netlink.Bridge{LinkAttrs: linkAttrs}\n\tif err := LinkAddIfNotExist(b.bridge); err != nil {\n\t\treturn errors.Wrapf(err, \"creating bridge %q\", config.WeaveBridgeName)\n\t}\n\tif err := netlink.LinkSetHardwareAddr(b.bridge, mac); err != nil {\n\t\treturn errors.Wrapf(err, \"setting bridge %q mac %v\", config.WeaveBridgeName, mac)\n\t}\n\t// Attempting to set the bridge MTU to a high value directly\n\t// fails. Bridges take the lowest MTU of their interfaces. So\n\t// instead we create a temporary interface with the desired MTU,\n\t// attach that to the bridge, and then remove it again.\n\tdummy := &netlink.Dummy{LinkAttrs: netlink.NewLinkAttrs()}\n\tdummy.LinkAttrs.Name = \"vethwedu\"\n\tif err = netlink.LinkAdd(dummy); err != nil {\n\t\treturn errors.Wrap(err, \"creating dummy interface\")\n\t}\n\tif err := netlink.LinkSetMTU(dummy, config.MTU); err != nil {\n\t\treturn errors.Wrapf(err, \"setting dummy interface mtu to %d\", config.MTU)\n\t}\n\tif err := netlink.LinkSetMasterByIndex(dummy, b.bridge.Attrs().Index); err != nil {\n\t\treturn errors.Wrap(err, \"setting dummy interface master\")\n\t}\n\tif err := netlink.LinkDel(dummy); err != nil {\n\t\treturn errors.Wrap(err, \"deleting dummy interface\")\n\t}\n\n\treturn nil\n}\n\nfunc (b bridgeImpl) init(config *BridgeConfig) error {\n\tif err := b.initPrep(config); err != nil {\n\t\treturn err\n\t}\n\tif _, err := CreateAndAttachVeth(BridgeIfName, PcapIfName, config.WeaveBridgeName, config.MTU, true, false, func(veth netlink.Link) error {\n\t\treturn netlink.LinkSetUp(veth)\n\t}); err != nil {\n\t\treturn errors.Wrap(err, \"creating pcap veth pair\")\n\t}\n\tif err := EthtoolTXOff(config.WeaveBridgeName); err != nil {\n\t\treturn errors.Wrap(err, \"setting tx off\")\n\t}\n\n\treturn nil\n}\n\nfunc (f fastdpImpl) init(config *BridgeConfig) error {\n\todpSupported, err := odp.CreateDatapath(f.datapathName)\n\tif !odpSupported {\n\t\tmsg := \"\"\n\t\tif err != nil {\n\t\t\tmsg = err.Error()\n\t\t}\n\t\treturn errors.Wrap(errBridgeNotSupported, msg)\n\t}\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"creating datapath %q\", f.datapathName)\n\t}\n\tdatapath, err := netlink.LinkByName(f.datapathName)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"finding datapath %q\", f.datapathName)\n\t}\n\tif config.MTU == 0 {\n\t\t/* GCE has the lowest underlay network MTU we're likely to encounter on\n\t\t   a local network, at 1460 bytes.  To get the overlay MTU from that we\n\t\t   subtract 20 bytes for the outer IPv4 header, 8 bytes for the outer\n\t\t   UDP header, 8 bytes for the vxlan header, and 14 bytes for the inner\n\t\t   ethernet header.  In addition, we subtract 34 bytes for the ESP overhead\n\t\t   which is needed for the vxlan encryption. */\n\t\tconfig.MTU = 1376\n\t}\n\tif err := netlink.LinkSetMTU(datapath, config.MTU); err != nil {\n\t\treturn errors.Wrapf(err, \"setting datapath %q mtu %d\", f.datapathName, config.MTU)\n\t}\n\treturn nil\n}\n\nfunc (bf bridgedFastdpImpl) init(config *BridgeConfig) error {\n\tif err := bf.fastdpImpl.init(config); err != nil {\n\t\treturn err\n\t}\n\tif err := bf.bridgeImpl.initPrep(config); err != nil {\n\t\treturn err\n\t}\n\tif _, err := CreateAndAttachVeth(BridgeIfName, DatapathIfName, config.WeaveBridgeName, config.MTU, true, false, func(veth netlink.Link) error {\n\t\tif err := netlink.LinkSetUp(veth); err != nil {\n\t\t\treturn errors.Wrapf(err, \"setting link up on %q\", veth.Attrs().Name)\n\t\t}\n\t\tif err := odp.AddDatapathInterfaceIfNotExist(bf.datapathName, veth.Attrs().Name); err != nil {\n\t\t\treturn errors.Wrapf(err, \"adding interface %q to datapath %q\", veth.Attrs().Name, bf.datapathName)\n\t\t}\n\t\treturn nil\n\t}); err != nil {\n\t\treturn errors.Wrap(err, \"creating bridged fastdp veth pair\")\n\t}\n\n\treturn linkSetUpByName(bf.datapathName)\n}\n\nfunc (b bridgeImpl) attach(veth *netlink.Veth) error {\n\treturn netlink.LinkSetMasterByIndex(veth, b.bridge.Attrs().Index)\n}\n\nfunc (bf bridgedFastdpImpl) attach(veth *netlink.Veth) error {\n\treturn bf.bridgeImpl.attach(veth)\n}\n\nfunc (f fastdpImpl) attach(veth *netlink.Veth) error {\n\treturn odp.AddDatapathInterfaceIfNotExist(f.datapathName, veth.Attrs().Name)\n}\n\nfunc configureIPTables(config *BridgeConfig, ips ipset.Interface) error {\n\tipt, err := iptables.New()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating iptables object\")\n\t}\n\n\t// The order among weave filter/FORWARD rules is important!\n\tfwdRules := make([][]string, 0)\n\n\tif config.DockerBridgeName != \"\" {\n\t\tif config.WeaveBridgeName != config.DockerBridgeName {\n\t\t\tfwdRules = append(fwdRules, []string{\"-i\", config.DockerBridgeName, \"-o\", config.WeaveBridgeName, \"-j\", \"DROP\"})\n\t\t}\n\n\t\tdockerBridgeIP, err := FindBridgeIP(config.DockerBridgeName, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// forbid traffic to the Weave port from other containers\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.DockerBridgeName, \"-p\", \"tcp\", \"--dst\", dockerBridgeIP.String(), \"--dport\", fmt.Sprint(config.Port), \"-j\", \"DROP\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.DockerBridgeName, \"-p\", \"udp\", \"--dst\", dockerBridgeIP.String(), \"--dport\", fmt.Sprint(config.Port), \"-j\", \"DROP\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.DockerBridgeName, \"-p\", \"udp\", \"--dst\", dockerBridgeIP.String(), \"--dport\", fmt.Sprint(config.Port+1), \"-j\", \"DROP\"); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// let DNS traffic to weaveDNS, since otherwise it might get blocked by the likes of UFW\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.DockerBridgeName, \"-p\", \"udp\", \"--dport\", \"53\", \"-j\", \"ACCEPT\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.DockerBridgeName, \"-p\", \"tcp\", \"--dport\", \"53\", \"-j\", \"ACCEPT\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif config.NPC {\n\t\t// Steer traffic via the NPC.\n\n\t\tif err = ensureChains(ipt, \"filter\", npc.MainChain, npc.EgressChain); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Steer egress traffic destined to local node.\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.WeaveBridgeName, \"-j\", npc.EgressChain); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfwdRules = append(fwdRules,\n\t\t\t[][]string{\n\t\t\t\t// Might include ingress traffic which is fine as long as we do not\n\t\t\t\t// ACCEPT in WEAVE-NPC-EGRESS chain\n\t\t\t\t{\"-i\", config.WeaveBridgeName,\n\t\t\t\t\t\"-m\", \"comment\", \"--comment\", \"NOTE: this must go before '-j KUBE-FORWARD'\",\n\t\t\t\t\t\"-j\", npc.EgressChain},\n\t\t\t\t// The following rules are for ingress NPC processing\n\t\t\t\t{\"-o\", config.WeaveBridgeName,\n\t\t\t\t\t\"-m\", \"comment\", \"--comment\", \"NOTE: this must go before '-j KUBE-FORWARD'\",\n\t\t\t\t\t\"-j\", npc.MainChain},\n\t\t\t\t{\"-o\", config.WeaveBridgeName, \"-m\", \"state\", \"--state\", \"NEW\", \"-j\", \"NFLOG\", \"--nflog-group\", \"86\"},\n\t\t\t\t{\"-o\", config.WeaveBridgeName, \"-j\", \"DROP\"},\n\t\t\t}...)\n\t} else {\n\t\t// Work around the situation where there are no rules allowing traffic\n\t\t// across our bridge. E.g. ufw\n\t\tfwdRules = append(fwdRules, []string{\"-i\", config.WeaveBridgeName, \"-o\", config.WeaveBridgeName, \"-j\", \"ACCEPT\"})\n\t}\n\n\tif !config.NPC {\n\t\t// Create/Flush a chain for allowing ingress traffic when the bridge is exposed\n\t\tif err := ipt.ClearChain(\"filter\", \"WEAVE-EXPOSE\"); err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to clear/create filter/WEAVE-EXPOSE chain\")\n\t\t}\n\n\t\tfwdRules = append(fwdRules, []string{\"-o\", config.WeaveBridgeName, \"-j\", \"WEAVE-EXPOSE\"})\n\t}\n\n\t// Forward from weave to the rest of the world\n\tfwdRules = append(fwdRules, []string{\"-i\", config.WeaveBridgeName, \"!\", \"-o\", config.WeaveBridgeName, \"-j\", \"ACCEPT\"})\n\t// and allow replies back\n\tfwdRules = append(fwdRules, []string{\"-o\", config.WeaveBridgeName, \"-m\", \"conntrack\", \"--ctstate\", \"RELATED,ESTABLISHED\", \"-j\", \"ACCEPT\"})\n\n\tif err := ensureRulesAtTop(\"filter\", \"FORWARD\", fwdRules, ipt); err != nil {\n\t\treturn err\n\t}\n\n\t// Create a chain for masquerading\n\tif err := ipt.ClearChain(\"nat\", \"WEAVE\"); err != nil {\n\t\treturn errors.Wrap(err, \"failed to clear/create nat/WEAVE chain\")\n\t}\n\tif err := ipt.AppendUnique(\"nat\", \"POSTROUTING\", \"-j\", \"WEAVE\"); err != nil {\n\t\treturn err\n\t}\n\n\t// For the cases where the weave bridge is the default gateway for\n\t// containers (e.g. Kubernetes): create the ipset to store CIDRs allocated\n\t// by IPAM for local containers. In the case of Kubernetes, external traffic\n\t// sent to these CIDRs avoids SNAT'ing so that NodePort with\n\t// `\"externalTrafficPolicy\":\"Local\"` would receive packets with correct\n\t// src IP addr.\n\tif config.NoMasqLocal {\n\t\tips := ipset.New(common.LogLogger(), 0)\n\t\t_ = ips.Destroy(NoMasqLocalIpset)\n\t\tif err := ips.Create(NoMasqLocalIpset, ipset.HashNet); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := ipt.Insert(\"nat\", \"WEAVE\", 1,\n\t\t\t\"-m\", \"set\", \"--match-set\", string(NoMasqLocalIpset), \"dst\",\n\t\t\t\"-m\", \"comment\", \"--comment\", \"Prevent SNAT to locally running containers\",\n\t\t\t\"-j\", \"RETURN\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\ntype NoMasqLocalTracker struct {\n\tips   ipset.Interface\n\towner types.UID\n}\n\nfunc NewNoMasqLocalTracker(ips ipset.Interface) *NoMasqLocalTracker {\n\treturn &NoMasqLocalTracker{\n\t\tips:   ips,\n\t\towner: types.UID(0), // dummy ipset owner\n\t}\n}\n\nfunc (t *NoMasqLocalTracker) String() string {\n\treturn \"no-masq-local\"\n}\n\nfunc (t *NoMasqLocalTracker) HandleUpdate(prevRanges, currRanges []address.Range, local bool) error {\n\tif !local {\n\t\treturn nil\n\t}\n\n\tprev, curr := tracker.RemoveCommon(\n\t\taddress.NewCIDRs(tracker.Merge(prevRanges)),\n\t\taddress.NewCIDRs(tracker.Merge(currRanges)))\n\n\tfor _, cidr := range curr {\n\t\tif err := t.ips.AddEntry(t.owner, NoMasqLocalIpset, cidr.String(), \"\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tfor _, cidr := range prev {\n\t\tif err := t.ips.DelEntry(t.owner, NoMasqLocalIpset, cidr.String()); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc linkSetUpByName(linkName string) error {\n\tlink, err := netlink.LinkByName(linkName)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"setting link up on %q\", linkName)\n\t}\n\treturn netlink.LinkSetUp(link)\n}\n\nfunc reexpose(config *BridgeConfig, log *logrus.Logger) error {\n\t// Get existing IP addrs of the weave bridge.\n\t// If the bridge hasn't been exposed, then this functions does nothing.\n\t//\n\t// Ideally, we should consult IPAM for IP addrs allocated to \"weave:expose\",\n\t// but we don't want to introduce dependency on IPAM, as weave should be able\n\t// to run w/o IPAM.\n\tlink, err := netlink.LinkByName(config.WeaveBridgeName)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"cannot find bridge %q\", config.WeaveBridgeName)\n\t}\n\taddrs, err := netlink.AddrList(link, netlink.FAMILY_V4)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"cannot list IPv4 addrs of bridge %q\", config.WeaveBridgeName)\n\t}\n\n\tfor _, addr := range addrs {\n\t\tlog.Infof(\"Re-exposing %s on bridge %q\", addr.IPNet, config.WeaveBridgeName)\n\t\tif err := Expose(config.WeaveBridgeName, addr.IPNet, config.AWSVPC, config.NPC, false); err != nil {\n\t\t\treturn errors.Wrapf(err, \"unable to re-expose %s on bridge: %q\", addr.IPNet, config.WeaveBridgeName)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc monitorInterface(ifaceName string, log *logrus.Logger) error {\n\t_, err := netlink.LinkByName(ifaceName)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Unable to find link %q: %s\", ifaceName, err)\n\t}\n\n\tupdatesChannel := make(chan netlink.LinkUpdate)\n\tif err := netlink.LinkSubscribe(updatesChannel, nil); err != nil {\n\t\treturn errors.Wrapf(err, \"error monitoring link %q for UP/DOWN notifications\", ifaceName)\n\t}\n\n\tgo func() {\n\t\tfor update := range updatesChannel {\n\t\t\tif update.Link.Attrs().Name == ifaceName && update.IfInfomsg.Flags&syscall.IFF_UP == 0 {\n\t\t\t\tlog.Errorf(\"Interface %q which needs to be in UP state for Weave functioning is found to be in DOWN state\", ifaceName)\n\t\t\t}\n\t\t}\n\t}()\n\treturn nil\n}\n", "// +build go1.10\n\npackage net\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"runtime\"\n\n\t\"github.com/vishvananda/netlink\"\n\t\"github.com/vishvananda/netns\"\n)\n\nvar ErrLinkNotFound = errors.New(\"Link not found\")\n\nfunc WithNetNS(ns netns.NsHandle, work func() error) error {\n\truntime.LockOSThread()\n\tdefer runtime.UnlockOSThread()\n\n\toldNs, err := netns.Get()\n\tif err == nil {\n\t\tdefer oldNs.Close()\n\n\t\terr = netns.Set(ns)\n\t\tif err == nil {\n\t\t\tdefer netns.Set(oldNs)\n\n\t\t\terr = work()\n\t\t}\n\t}\n\n\treturn err\n}\n\nfunc WithNetNSLink(ns netns.NsHandle, ifName string, work func(link netlink.Link) error) error {\n\treturn WithNetNS(ns, func() error {\n\t\tlink, err := netlink.LinkByName(ifName)\n\t\tif err != nil {\n\t\t\tif err.Error() == errors.New(\"Link not found\").Error() {\n\t\t\t\treturn ErrLinkNotFound\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\treturn work(link)\n\t})\n}\n\nfunc WithNetNSByPath(path string, work func() error) error {\n\tns, err := netns.GetFromPath(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn WithNetNS(ns, work)\n}\n\nfunc NSPathByPid(pid int) string {\n\treturn NSPathByPidWithRoot(\"/\", pid)\n}\n\nfunc NSPathByPidWithRoot(root string, pid int) string {\n\treturn filepath.Join(root, fmt.Sprintf(\"/proc/%d/ns/net\", pid))\n}\n", "package net\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/coreos/go-iptables/iptables\"\n\t\"github.com/j-keck/arping\"\n\t\"github.com/vishvananda/netlink\"\n\t\"github.com/vishvananda/netns\"\n)\n\n// create and attach a veth to the Weave bridge\nfunc CreateAndAttachVeth(name, peerName, bridgeName string, mtu int, keepTXOn bool, errIfLinkExist bool, init func(peer netlink.Link) error) (*netlink.Veth, error) {\n\tbridge, err := netlink.LinkByName(bridgeName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(`bridge \"%s\" not present; did you launch weave?`, bridgeName)\n\t}\n\n\tif mtu == 0 {\n\t\tmtu = bridge.Attrs().MTU\n\t}\n\tveth := &netlink.Veth{\n\t\tLinkAttrs: netlink.LinkAttrs{\n\t\t\tName: name,\n\t\t\tMTU:  mtu},\n\t\tPeerName: peerName,\n\t}\n\n\tlinkAdd := LinkAddIfNotExist\n\tif errIfLinkExist {\n\t\tlinkAdd = netlink.LinkAdd\n\t}\n\tif err := linkAdd(veth); err != nil {\n\t\treturn nil, fmt.Errorf(`could not create veth pair %s-%s: %s`, name, peerName, err)\n\t}\n\n\tcleanup := func(format string, a ...interface{}) (*netlink.Veth, error) {\n\t\tnetlink.LinkDel(veth)\n\t\treturn nil, fmt.Errorf(format, a...)\n\t}\n\n\tbridgeType, err := ExistingBridgeType(bridgeName, DatapathName)\n\tif err != nil {\n\t\treturn cleanup(\"detect bridge type: %s\", err)\n\t}\n\tif err := bridgeType.attach(veth); err != nil {\n\t\treturn cleanup(\"attaching veth %q to %q: %s\", name, bridgeName, err)\n\t}\n\tif !bridgeType.IsFastdp() && !keepTXOn {\n\t\tif err := EthtoolTXOff(veth.PeerName); err != nil {\n\t\t\treturn cleanup(`unable to set tx off on %q: %s`, peerName, err)\n\t\t}\n\t}\n\n\tif init != nil {\n\t\tpeer, err := netlink.LinkByName(peerName)\n\t\tif err != nil {\n\t\t\treturn cleanup(\"unable to find peer veth %s: %s\", peerName, err)\n\t\t}\n\t\tif err := init(peer); err != nil {\n\t\t\treturn cleanup(\"initializing veth: %s\", err)\n\t\t}\n\t}\n\n\tif err := netlink.LinkSetUp(veth); err != nil {\n\t\treturn cleanup(\"unable to bring veth up: %s\", err)\n\t}\n\n\treturn veth, nil\n}\n\nfunc AddAddresses(link netlink.Link, cidrs []*net.IPNet) (newAddrs []*net.IPNet, err error) {\n\texistingAddrs, err := netlink.AddrList(link, netlink.FAMILY_V4)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get IP address for %q: %v\", link.Attrs().Name, err)\n\t}\n\tfor _, ipnet := range cidrs {\n\t\tif contains(existingAddrs, ipnet) {\n\t\t\tcontinue\n\t\t}\n\t\tif err := netlink.AddrAdd(link, &netlink.Addr{IPNet: ipnet}); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to add IP address to %q: %v\", link.Attrs().Name, err)\n\t\t}\n\t\tnewAddrs = append(newAddrs, ipnet)\n\t}\n\treturn newAddrs, nil\n}\n\nfunc contains(addrs []netlink.Addr, addr *net.IPNet) bool {\n\tfor _, x := range addrs {\n\t\tif addr.IP.Equal(x.IPNet.IP) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nconst (\n\tVethName   = \"ethwe\"        // name inside container namespace\n\tvethPrefix = \"v\" + VethName // starts with \"veth\" to suppress UI notifications\n)\n\nfunc interfaceExistsInNamespace(netNSPath string, ifName string) bool {\n\terr := WithNetNSByPath(netNSPath, func() error {\n\t\t_, err := netlink.LinkByName(ifName)\n\t\treturn err\n\t})\n\treturn err == nil\n}\n\nfunc AttachContainer(netNSPath, id, ifName, bridgeName string, mtu int, withMulticastRoute bool, cidrs []*net.IPNet, keepTXOn bool, hairpinMode bool) error {\n\tns, err := netns.GetFromPath(netNSPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer ns.Close()\n\n\tif !interfaceExistsInNamespace(netNSPath, ifName) {\n\t\tmaxIDLen := IFNAMSIZ - 1 - len(vethPrefix+\"pl\")\n\t\tif len(id) > maxIDLen {\n\t\t\tid = id[:maxIDLen] // trim passed ID if too long\n\t\t}\n\t\tname, peerName := vethPrefix+\"pl\"+id, vethPrefix+\"pg\"+id\n\t\tveth, err := CreateAndAttachVeth(name, peerName, bridgeName, mtu, keepTXOn, true, func(veth netlink.Link) error {\n\t\t\tif err := netlink.LinkSetNsFd(veth, int(ns)); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to move veth to container netns: %s\", err)\n\t\t\t}\n\t\t\tif err := WithNetNS(ns, func() error {\n\t\t\t\treturn setupIface(peerName, ifName)\n\t\t\t}); err != nil {\n\t\t\t\treturn fmt.Errorf(\"error setting up interface: %s\", err)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err = netlink.LinkSetHairpin(veth, hairpinMode); err != nil {\n\t\t\treturn fmt.Errorf(\"unable to set hairpin mode to %t for bridge side of veth %s: %s\", hairpinMode, name, err)\n\t\t}\n\n\t}\n\n\tif err := WithNetNSLink(ns, ifName, func(veth netlink.Link) error {\n\t\treturn setupIfaceAddrs(veth, withMulticastRoute, cidrs)\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up interface addresses: %s\", err)\n\t}\n\treturn nil\n}\n\n// setupIfaceAddrs expects to be called in the container's netns\nfunc setupIfaceAddrs(veth netlink.Link, withMulticastRoute bool, cidrs []*net.IPNet) error {\n\tnewAddresses, err := AddAddresses(veth, cidrs)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tifName := veth.Attrs().Name\n\tipt, err := iptables.New()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Add multicast ACCEPT rules for new subnets\n\tfor _, ipnet := range newAddresses {\n\t\tacceptRule := []string{\"-i\", ifName, \"-s\", subnet(ipnet), \"-d\", \"224.0.0.0/4\", \"-j\", \"ACCEPT\"}\n\t\texists, err := ipt.Exists(\"filter\", \"INPUT\", acceptRule...)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif !exists {\n\t\t\tif err := ipt.Insert(\"filter\", \"INPUT\", 1, acceptRule...); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := netlink.LinkSetUp(veth); err != nil {\n\t\treturn err\n\t}\n\tfor _, ipnet := range newAddresses {\n\t\t// If we don't wait for a bit here, we see the arp fail to reach the bridge.\n\t\ttime.Sleep(1 * time.Millisecond)\n\t\tarping.GratuitousArpOverIfaceByName(ipnet.IP, ifName)\n\t}\n\tif withMulticastRoute {\n\t\t/* Route multicast packets across the weave network.\n\t\tThis must come last in 'attach'. If you change this, change weavewait to match.\n\n\t\tTODO: Add the MTU lock to prevent PMTU discovery for multicast\n\t\tdestinations. Without that, the kernel sets the DF flag on\n\t\tmulticast packets. Since RFC1122 prohibits sending of ICMP\n\t\terrors for packets with multicast destinations, that causes\n\t\tpackets larger than the PMTU to be dropped silently.  */\n\n\t\t_, multicast, _ := net.ParseCIDR(\"224.0.0.0/4\")\n\t\tif err := AddRoute(veth, netlink.SCOPE_LINK, multicast, nil); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// setupIface expects to be called in the container's netns\nfunc setupIface(ifaceName, newIfName string) error {\n\tipt, err := iptables.New()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlink, err := netlink.LinkByName(ifaceName)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := netlink.LinkSetName(link, newIfName); err != nil {\n\t\treturn err\n\t}\n\t// This is only called by AttachContainer which is only called in host pid namespace\n\tif err := configureARPCache(\"/proc\", newIfName); err != nil {\n\t\treturn err\n\t}\n\treturn ipt.Append(\"filter\", \"INPUT\", \"-i\", newIfName, \"-d\", \"224.0.0.0/4\", \"-j\", \"DROP\")\n}\n\n// configureARP is a helper for the Docker plugin which doesn't set the addresses itself\nfunc ConfigureARP(prefix, rootPath string) error {\n\tlinks, err := netlink.LinkList()\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor _, link := range links {\n\t\tifName := link.Attrs().Name\n\t\tif strings.HasPrefix(ifName, prefix) {\n\t\t\tconfigureARPCache(rootPath+\"/proc\", ifName)\n\t\t\tif addrs, err := netlink.AddrList(link, netlink.FAMILY_V4); err == nil {\n\t\t\t\tfor _, addr := range addrs {\n\t\t\t\t\tarping.GratuitousArpOverIfaceByName(addr.IPNet.IP, ifName)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc DetachContainer(netNSPath, id, ifName string, cidrs []*net.IPNet) error {\n\tns, err := netns.GetFromPath(netNSPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer ns.Close()\n\n\tipt, err := iptables.New()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn WithNetNSLink(ns, ifName, func(veth netlink.Link) error {\n\t\texistingAddrs, err := netlink.AddrList(veth, netlink.FAMILY_V4)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get IP address for %q: %v\", veth.Attrs().Name, err)\n\t\t}\n\t\tfor _, ipnet := range cidrs {\n\t\t\tif !contains(existingAddrs, ipnet) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := netlink.AddrDel(veth, &netlink.Addr{IPNet: ipnet}); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to remove IP address from %q: %v\", veth.Attrs().Name, err)\n\t\t\t}\n\t\t}\n\t\taddrs, err := netlink.AddrList(veth, netlink.FAMILY_V4)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get IP address for %q: %v\", veth.Attrs().Name, err)\n\t\t}\n\n\t\t// Remove multicast ACCEPT rules for subnets we no longer have addresses in\n\t\tsubnets := subnets(addrs)\n\t\trules, err := ipt.List(\"filter\", \"INPUT\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, rule := range rules {\n\t\t\tps := strings.Split(rule, \" \")\n\t\t\tif len(ps) == 10 &&\n\t\t\t\tps[0] == \"-A\" && ps[2] == \"-s\" && ps[4] == \"-d\" && ps[5] == \"224.0.0.0/4\" &&\n\t\t\t\tps[6] == \"-i\" && ps[7] == ifName && ps[8] == \"-j\" && ps[9] == \"ACCEPT\" {\n\n\t\t\t\tif _, found := subnets[ps[3]]; !found {\n\t\t\t\t\tif err := ipt.Delete(\"filter\", \"INPUT\", ps[2:]...); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif len(addrs) == 0 { // all addresses gone: remove the interface\n\t\t\tif err := ipt.Delete(\"filter\", \"INPUT\", \"-i\", ifName, \"-d\", \"224.0.0.0/4\", \"-j\", \"DROP\"); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := netlink.LinkDel(veth); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n}\n\nfunc subnet(ipn *net.IPNet) string {\n\tones, _ := ipn.Mask.Size()\n\treturn fmt.Sprintf(\"%s/%d\", ipn.IP.Mask(ipn.Mask).String(), ones)\n}\n\nfunc subnets(addrs []netlink.Addr) map[string]struct{} {\n\tsubnets := make(map[string]struct{})\n\tfor _, addr := range addrs {\n\t\tsubnets[subnet(addr.IPNet)] = struct{}{}\n\t}\n\treturn subnets\n}\n", "package plugin\n\nimport (\n\t\"fmt\"\n\t\"strconv\"\n\t\"sync\"\n\n\t\"github.com/docker/libnetwork/drivers/remote/api\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"golang.org/x/sys/unix\"\n\n\t\"github.com/vishvananda/netlink\"\n\tweaveapi \"github.com/weaveworks/weave/api\"\n\t\"github.com/weaveworks/weave/common\"\n\t\"github.com/weaveworks/weave/common/docker\"\n\tweavenet \"github.com/weaveworks/weave/net\"\n\t\"github.com/weaveworks/weave/plugin/skel\"\n)\n\nconst (\n\tMulticastOption = \"works.weave.multicast\"\n)\n\ntype network struct {\n\tisOurs            bool\n\thasMulticastRoute bool\n}\n\ntype driver struct {\n\tsync.RWMutex\n\tname  string\n\tscope string\n\t// Docker API is not available for plugin-v2\n\tdocker     *docker.Client\n\tdns        bool\n\tisPluginV2 bool\n\t// Enable multicast regardless whether multicast opt is passed to Docker;\n\t// used only by plugin-v2\n\tforceMulticast bool\n\tnetworks       map[string]network\n}\n\nfunc New(client *docker.Client, weave *weaveapi.Client, name, scope string, dns, isPluginV2, forceMulticast bool) (skel.Driver, error) {\n\tdriver := &driver{\n\t\tname:       name,\n\t\tscope:      scope,\n\t\tdocker:     client,\n\t\tdns:        dns,\n\t\tisPluginV2: isPluginV2,\n\t\t// make sure that it's used only by plugin-v2\n\t\tforceMulticast: isPluginV2 && forceMulticast,\n\t\tnetworks:       make(map[string]network),\n\t}\n\n\t// Do not start watcher in the case of plugin v2, which prevents us from\n\t// configuring arp settings of containers.\n\tif client != nil {\n\t\t_, err := NewWatcher(client, weave, driver)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn driver, nil\n}\n\n// === protocol handlers\n\nfunc (driver *driver) GetCapabilities() (*api.GetCapabilityResponse, error) {\n\tdriver.logReq(\"GetCapabilities\", nil, \"\")\n\tvar caps = &api.GetCapabilityResponse{\n\t\tScope: driver.scope,\n\t}\n\tdriver.logRes(\"GetCapabilities\", caps)\n\treturn caps, nil\n}\n\n// In Swarm mode, CreateNetwork is called on each Swarm node when a new service\n// is created.\nfunc (driver *driver) CreateNetwork(create *api.CreateNetworkRequest) error {\n\tdriver.logReq(\"CreateNetwork\", create, create.NetworkID)\n\t_, err := driver.setupNetworkInfo(create.NetworkID, true, stringOptions(create))\n\treturn err\n}\n\n// NetworkAllocate is called on a Swarm node (master) which creates the network.\n// The returned options are passed to CreateNetwork.\nfunc (driver *driver) NetworkAllocate(alloc *api.AllocateNetworkRequest) (*api.AllocateNetworkResponse, error) {\n\tdriver.logReq(\"NetworkAllocate\", alloc, alloc.NetworkID)\n\treturn &api.AllocateNetworkResponse{Options: alloc.Options}, nil\n}\n\n// NetworkFree is called on a Swarm master node which created the network.\nfunc (driver *driver) NetworkFree(free *api.FreeNetworkRequest) (*api.FreeNetworkResponse, error) {\n\tdriver.logReq(\"NetworkFree\", free, free.NetworkID)\n\treturn nil, nil\n}\n\n// Deal with excessively-generic way the options get decoded from JSON\nfunc stringOptions(create *api.CreateNetworkRequest) map[string]string {\n\tif create.Options != nil {\n\t\tif data, found := create.Options[netlabel.GenericData]; found {\n\t\t\tif options, ok := data.(map[string]interface{}); ok {\n\t\t\t\tout := make(map[string]string, len(options))\n\t\t\t\tfor key, value := range options {\n\t\t\t\t\tif str, ok := value.(string); ok {\n\t\t\t\t\t\tout[key] = str\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn out\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// In Swarm mode, DeleteNetwork is called after a service has been removed.\nfunc (driver *driver) DeleteNetwork(delreq *api.DeleteNetworkRequest) error {\n\tdriver.logReq(\"DeleteNetwork\", delreq, delreq.NetworkID)\n\tdriver.Lock()\n\tdelete(driver.networks, delreq.NetworkID)\n\tdriver.Unlock()\n\treturn nil\n}\n\nfunc (driver *driver) CreateEndpoint(create *api.CreateEndpointRequest) (*api.CreateEndpointResponse, error) {\n\tdriver.logReq(\"CreateEndpoint\", create, create.EndpointID)\n\tcommon.Log.Debugf(\"interface %+v\", create.Interface)\n\n\tif create.Interface == nil {\n\t\treturn nil, driver.error(\"CreateEndpoint\", \"Not supported: creating an interface from within CreateEndpoint\")\n\t}\n\n\t// create veths. note we assume endpoint IDs are unique in the first 9 chars\n\tname, peerName := vethPair(create.EndpointID)\n\tif _, err := weavenet.CreateAndAttachVeth(name, peerName, weavenet.WeaveBridgeName, 0, false, true, nil); err != nil {\n\t\treturn nil, driver.error(\"JoinEndpoint\", \"%s\", err)\n\t}\n\n\t// Send back the MAC address\n\tlink, _ := netlink.LinkByName(peerName)\n\tresp := &api.CreateEndpointResponse{Interface: &api.EndpointInterface{MacAddress: link.Attrs().HardwareAddr.String()}}\n\n\tdriver.logRes(\"CreateEndpoint\", resp)\n\treturn resp, nil\n}\n\nfunc (driver *driver) DeleteEndpoint(deleteReq *api.DeleteEndpointRequest) error {\n\tdriver.logReq(\"DeleteEndpoint\", deleteReq, deleteReq.EndpointID)\n\tname, _ := vethPair(deleteReq.EndpointID)\n\tveth := &netlink.Veth{LinkAttrs: netlink.LinkAttrs{Name: name}}\n\tif err := netlink.LinkDel(veth); err != nil {\n\t\t// Try again using the name construction from earlier plugin version,\n\t\t// in case user has upgraded with endpoints still extant\n\t\tveth.Name = \"vethwl\" + deleteReq.EndpointID[:5]\n\t\tif err2 := netlink.LinkDel(veth); err2 != nil {\n\t\t\t// Note we report the first error\n\t\t\tdriver.warn(\"LeaveEndpoint\", \"unable to delete veth %q: %s\", name, err)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (driver *driver) EndpointInfo(req *api.EndpointInfoRequest) (*api.EndpointInfoResponse, error) {\n\tdriver.logReq(\"EndpointInfo\", req, req.EndpointID)\n\treturn &api.EndpointInfoResponse{Value: map[string]interface{}{}}, nil\n}\n\nfunc (driver *driver) JoinEndpoint(j *api.JoinRequest) (*api.JoinResponse, error) {\n\tdriver.logReq(\"JoinEndpoint\", j, fmt.Sprintf(\"%s:%s to %s\", j.NetworkID, j.EndpointID, j.SandboxKey))\n\n\tnetwork, err := driver.findNetworkInfo(j.NetworkID)\n\tif err != nil {\n\t\treturn nil, driver.error(\"JoinEndpoint\", \"unable to find network info: %s\", err)\n\t}\n\n\t_, peerName := vethPair(j.EndpointID)\n\tresponse := &api.JoinResponse{\n\t\tInterfaceName: &api.InterfaceName{\n\t\t\tSrcName:   peerName,\n\t\t\tDstPrefix: weavenet.VethName,\n\t\t},\n\t}\n\tif network.hasMulticastRoute {\n\t\tmulticastRoute := api.StaticRoute{\n\t\t\tDestination: \"224.0.0.0/4\",\n\t\t\tRouteType:   types.CONNECTED,\n\t\t}\n\t\tresponse.StaticRoutes = append(response.StaticRoutes, multicastRoute)\n\t}\n\tdriver.logRes(\"JoinEndpoint\", response)\n\treturn response, nil\n}\n\nfunc (driver *driver) findNetworkInfo(id string) (network, error) {\n\tvar network network\n\n\t// plugin-v2 does not have access to docker.sock, so we cannot call Docker\n\t// API for the network info.\n\tif driver.isPluginV2 {\n\t\t// safe to set, as isOurs used only by the watcher which is disabled for plugin-v2\n\t\tnetwork.isOurs = true\n\t\tnetwork.hasMulticastRoute = driver.forceMulticast\n\n\t\treturn network, nil\n\t}\n\n\tdriver.Lock()\n\tnetwork, found := driver.networks[id]\n\tdriver.Unlock()\n\tif found {\n\t\treturn network, nil\n\t}\n\n\tif driver.docker == nil {\n\t\treturn network, fmt.Errorf(\"Docker client disabled; unable to get network info\")\n\t}\n\n\tinfo, err := driver.docker.NetworkInfo(id)\n\tif err != nil {\n\t\treturn network, err\n\t}\n\treturn driver.setupNetworkInfo(id, info.Driver == driver.name, info.Options)\n}\n\nfunc (driver *driver) setupNetworkInfo(id string, isOurs bool, options map[string]string) (network, error) {\n\tnetwork := network{isOurs: isOurs}\n\tif isOurs {\n\t\tfor key, value := range options {\n\t\t\tswitch key {\n\t\t\tcase MulticastOption:\n\t\t\t\tif value == \"\" { // interpret \"--opt works.weave.multicast\" as \"turn it on\"\n\t\t\t\t\tnetwork.hasMulticastRoute = true\n\t\t\t\t} else {\n\t\t\t\t\tvar err error\n\t\t\t\t\tif network.hasMulticastRoute, err = strconv.ParseBool(value); err != nil {\n\t\t\t\t\t\treturn network, fmt.Errorf(\"unrecognized value %q for option %s\", value, key)\n\t\t\t\t\t}\n\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tdriver.warn(\"setupNetworkInfo\", \"unrecognized option: %s\", key)\n\t\t\t}\n\t\t}\n\t}\n\tdriver.Lock()\n\tdriver.networks[id] = network\n\tdriver.Unlock()\n\treturn network, nil\n}\n\nfunc (driver *driver) LeaveEndpoint(leave *api.LeaveRequest) error {\n\tdriver.logReq(\"LeaveEndpoint\", leave, fmt.Sprintf(\"%s:%s\", leave.NetworkID, leave.EndpointID))\n\treturn nil\n}\n\nfunc (driver *driver) DiscoverNew(disco *api.DiscoveryNotification) error {\n\tdriver.logReq(\"DiscoverNew\", disco, \"\")\n\treturn nil\n}\n\nfunc (driver *driver) DiscoverDelete(disco *api.DiscoveryNotification) error {\n\tdriver.logReq(\"DiscoverDelete\", disco, \"\")\n\treturn nil\n}\n\nfunc vethPair(id string) (string, string) {\n\t// IFNAMSIZ is buffer length; subtract 6 for \"vethwl\" and 1 for terminating nul\n\treturn \"vethwl\" + id[:unix.IFNAMSIZ-7], \"vethwg\" + id[:unix.IFNAMSIZ-7]\n}\n\n// logging\n\nfunc (driver *driver) logReq(fun string, req interface{}, short string) {\n\tdriver.log(common.Log.Debugf, \" %+v\", fun, req)\n\tcommon.Log.Infof(\"[net] %s %s\", fun, short)\n}\n\nfunc (driver *driver) logRes(fun string, res interface{}) {\n\tdriver.log(common.Log.Debugf, \" %+v\", fun, res)\n}\n\nfunc (driver *driver) warn(fun string, format string, a ...interface{}) {\n\tdriver.log(common.Log.Warnf, \": \"+format, fun, a...)\n}\n\nfunc (driver *driver) debug(fun string, format string, a ...interface{}) {\n\tdriver.log(common.Log.Debugf, \": \"+format, fun, a...)\n}\n\nfunc (driver *driver) error(fun string, format string, a ...interface{}) error {\n\tdriver.log(common.Log.Errorf, \": \"+format, fun, a...)\n\treturn fmt.Errorf(format, a...)\n}\n\nfunc (driver *driver) log(f func(string, ...interface{}), format string, fun string, a ...interface{}) {\n\tf(\"[net] %s\"+format, append([]interface{}{fun}, a...)...)\n}\n", "package plugin\n\nimport (\n\t\"fmt\"\n\n\tweaveapi \"github.com/weaveworks/weave/api\"\n\t\"github.com/weaveworks/weave/common/docker\"\n\tweavenet \"github.com/weaveworks/weave/net\"\n)\n\ntype watcher struct {\n\tclient *docker.Client\n\tweave  *weaveapi.Client\n\tdriver *driver\n}\n\ntype Watcher interface {\n}\n\nfunc NewWatcher(client *docker.Client, weave *weaveapi.Client, driver *driver) (Watcher, error) {\n\tw := &watcher{client: client, weave: weave, driver: driver}\n\treturn w, client.AddObserver(w)\n}\n\nfunc (w *watcher) ContainerStarted(id string) {\n\tw.driver.debug(\"ContainerStarted\", \"%s\", id)\n\tinfo, err := w.client.InspectContainer(id)\n\tif err != nil {\n\t\tw.driver.warn(\"ContainerStarted\", \"error inspecting container %s: %s\", id, err)\n\t\treturn\n\t}\n\t// check that it's on our network\n\tfor _, net := range info.NetworkSettings.Networks {\n\t\tnetwork, err := w.driver.findNetworkInfo(net.NetworkID)\n\t\tif err != nil {\n\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to find network %s info: %s\", net.NetworkID, err)\n\t\t\tcontinue\n\t\t}\n\t\tif network.isOurs {\n\t\t\tif w.driver.dns {\n\t\t\t\tfqdn := fmt.Sprintf(\"%s.%s\", info.Config.Hostname, info.Config.Domainname)\n\t\t\t\tif err := w.weave.RegisterWithDNS(id, fqdn, net.IPAddress); err != nil {\n\t\t\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to register %s with weaveDNS: %s\", id, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\trootDir := \"/\"\n\t\t\tif w.driver.isPluginV2 {\n\t\t\t\t// We bind mount host's /proc to /host/proc for plugin-v2\n\t\t\t\trootDir = \"/host\"\n\t\t\t}\n\t\t\tnetNSPath := weavenet.NSPathByPidWithRoot(rootDir, info.State.Pid)\n\t\t\tif err := weavenet.WithNetNSByPath(netNSPath, func() error {\n\t\t\t\treturn weavenet.ConfigureARP(weavenet.VethName, rootDir)\n\t\t\t}); err != nil {\n\t\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to configure interfaces: %s\", err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (w *watcher) ContainerDied(id string) {\n\t// don't need to do this as WeaveDNS removes names on container died anyway\n\t// (note by the time we get this event we can't see the EndpointID)\n}\n\nfunc (w *watcher) ContainerDestroyed(id string) {}\n", "package plugin\n\nimport (\n\t\"net\"\n\t\"os\"\n\t\"path\"\n\t\"strings\"\n\n\t\"github.com/docker/libnetwork/ipamapi\"\n\tweaveapi \"github.com/weaveworks/weave/api\"\n\t\"github.com/weaveworks/weave/common\"\n\t\"github.com/weaveworks/weave/common/docker\"\n\tweavenet \"github.com/weaveworks/weave/net\"\n\tipamplugin \"github.com/weaveworks/weave/plugin/ipam\"\n\tnetplugin \"github.com/weaveworks/weave/plugin/net\"\n\t\"github.com/weaveworks/weave/plugin/skel\"\n)\n\nconst (\n\tpluginV2Name    = \"net-plugin\"\n\tdefaultNetwork  = \"weave\"\n\tMulticastOption = netplugin.MulticastOption\n)\n\nvar Log = common.Log\n\ntype Config struct {\n\tSocket            string\n\tMeshSocket        string\n\tEnable            bool\n\tEnableV2          bool\n\tEnableV2Multicast bool\n\tDNS               bool\n\tDefaultSubnet     string\n}\n\ntype Plugin struct {\n\tConfig\n}\n\nfunc NewPlugin(config Config) *Plugin {\n\tif !config.Enable && !config.EnableV2 {\n\t\treturn nil\n\t}\n\tplugin := &Plugin{Config: config}\n\treturn plugin\n}\n\nfunc (plugin *Plugin) Start(weaveAPIAddr string, dockerClient *docker.Client, ready func()) {\n\tweave := weaveapi.NewClient(weaveAPIAddr, Log)\n\n\tLog.Info(\"Waiting for Weave API Server...\")\n\tweave.WaitAPIServer(30)\n\tLog.Info(\"Finished waiting for Weave API Server\")\n\n\tif err := plugin.run(dockerClient, weave, ready); err != nil {\n\t\tLog.Fatal(err)\n\t}\n}\n\nfunc (plugin *Plugin) run(dockerClient *docker.Client, weave *weaveapi.Client, ready func()) error {\n\tendChan := make(chan error, 1)\n\n\tif plugin.Socket != \"\" {\n\t\tglobalListener, err := listenAndServe(dockerClient, weave, plugin.Socket, endChan, \"global\", false, plugin.DNS, plugin.EnableV2, plugin.EnableV2Multicast)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer os.Remove(plugin.Socket)\n\t\tdefer globalListener.Close()\n\t}\n\tif plugin.MeshSocket != \"\" {\n\t\tmeshListener, err := listenAndServe(dockerClient, weave, plugin.MeshSocket, endChan, \"local\", true, plugin.DNS, plugin.EnableV2, plugin.EnableV2Multicast)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer os.Remove(plugin.MeshSocket)\n\t\tdefer meshListener.Close()\n\t\tif !plugin.EnableV2 {\n\t\t\tLog.Printf(\"Creating default %q network\", defaultNetwork)\n\t\t\toptions := map[string]interface{}{MulticastOption: \"true\"}\n\t\t\tdockerClient.EnsureNetwork(defaultNetwork, pluginNameFromAddress(plugin.MeshSocket), plugin.DefaultSubnet, options)\n\t\t}\n\t}\n\tready()\n\n\treturn <-endChan\n}\n\nfunc listenAndServe(dockerClient *docker.Client, weave *weaveapi.Client, address string, endChan chan<- error, scope string, withIpam, dns bool, isPluginV2, forceMulticast bool) (net.Listener, error) {\n\tvar name string\n\tif isPluginV2 {\n\t\tname = pluginV2Name\n\t} else {\n\t\tname = pluginNameFromAddress(address)\n\t}\n\n\td, err := netplugin.New(dockerClient, weave, name, scope, dns, isPluginV2, forceMulticast)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar i ipamapi.Ipam\n\tif withIpam {\n\t\ti = ipamplugin.NewIpam(weave)\n\t}\n\n\tlistener, err := weavenet.ListenUnixSocket(address)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tLog.Printf(\"Listening on %s for %s scope\", address, scope)\n\n\tgo func() {\n\t\tendChan <- skel.Listen(listener, d, i)\n\t}()\n\n\treturn listener, nil\n}\n\n// Take a socket address like \"/run/docker/plugins/weavemesh.sock\" and extract the plugin name \"weavemesh\"\nfunc pluginNameFromAddress(address string) string {\n\treturn strings.TrimSuffix(path.Base(address), \".sock\")\n}\n", "package main\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"net/http\"\n\t_ \"net/http/pprof\"\n\t\"os\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/gorilla/mux\"\n\t\"github.com/pkg/profile\"\n\t\"github.com/weaveworks/common/mflag\"\n\t\"github.com/weaveworks/common/mflagext\"\n\t\"github.com/weaveworks/common/signals\"\n\t\"github.com/weaveworks/mesh\"\n\n\t\"github.com/weaveworks/weave/common\"\n\t\"github.com/weaveworks/weave/common/docker\"\n\t\"github.com/weaveworks/weave/db\"\n\t\"github.com/weaveworks/weave/ipam\"\n\t\"github.com/weaveworks/weave/ipam/tracker\"\n\t\"github.com/weaveworks/weave/nameserver\"\n\tweavenet \"github.com/weaveworks/weave/net\"\n\t\"github.com/weaveworks/weave/net/address\"\n\t\"github.com/weaveworks/weave/net/ipset\"\n\t\"github.com/weaveworks/weave/plugin\"\n\tweaveproxy \"github.com/weaveworks/weave/proxy\"\n\tweave \"github.com/weaveworks/weave/router\"\n)\n\nvar version = \"unreleased\"\n\nvar Log = common.Log\n\ntype ipamConfig struct {\n\tIPRangeCIDR   string\n\tIPSubnetCIDR  string\n\tPeerCount     int\n\tMode          string\n\tObserver      bool\n\tSeedPeerNames []mesh.PeerName\n}\n\ntype dnsConfig struct {\n\tDomain        string\n\tListenAddress string\n\tTTL           int\n\tClientTimeout time.Duration\n\tResolvConf    string\n}\n\n// return the address part of the DNS listen address, without \":53\" on the end\nfunc (c dnsConfig) addressOnly() string {\n\taddr, _, err := net.SplitHostPort(c.ListenAddress)\n\tif err != nil {\n\t\tLog.Fatalf(\"Error when parsing DNS listen address %q: %s\", c.ListenAddress, err)\n\t}\n\treturn addr\n}\n\nfunc (c *ipamConfig) Enabled() bool {\n\tvar (\n\t\thasPeerCount = c.PeerCount > 0\n\t\thasMode      = c.HasMode()\n\t\thasRange     = c.IPRangeCIDR != \"\"\n\t\thasSubnet    = c.IPSubnetCIDR != \"\"\n\t)\n\tswitch {\n\tcase !(hasPeerCount || hasMode || hasRange || hasSubnet):\n\t\treturn false\n\tcase !hasRange && hasSubnet:\n\t\tLog.Fatal(\"--ipalloc-default-subnet specified without --ipalloc-range.\")\n\tcase !hasRange:\n\t\tLog.Fatal(\"--ipalloc-init specified without --ipalloc-range.\")\n\t}\n\tif hasMode {\n\t\tif err := c.parseMode(); err != nil {\n\t\t\tLog.Fatalf(\"Unable to parse --ipalloc-init: %s\", err)\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (c ipamConfig) HasMode() bool {\n\treturn len(c.Mode) > 0\n}\n\nfunc (c *ipamConfig) parseMode() error {\n\tmodeAndParam := strings.SplitN(c.Mode, \"=\", 2)\n\n\tswitch modeAndParam[0] {\n\tcase \"consensus\":\n\t\tif len(modeAndParam) == 2 {\n\t\t\tpeerCount, err := strconv.Atoi(modeAndParam[1])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"bad consensus parameter: %s\", err)\n\t\t\t}\n\t\t\tc.PeerCount = peerCount\n\t\t}\n\tcase \"seed\":\n\t\tif len(modeAndParam) != 2 {\n\t\t\treturn fmt.Errorf(\"seed mode requires peer name list\")\n\t\t}\n\t\tseedPeerNames, err := parsePeerNames(modeAndParam[1])\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"bad seed parameter: %s\", err)\n\t\t}\n\t\tc.SeedPeerNames = seedPeerNames\n\tcase \"observer\":\n\t\tif len(modeAndParam) != 1 {\n\t\t\treturn fmt.Errorf(\"observer mode takes no parameter\")\n\t\t}\n\t\tc.Observer = true\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown mode: %s\", modeAndParam[0])\n\t}\n\n\treturn nil\n}\n\nfunc getenvOrDefault(key, defaultVal string) string {\n\tif v := os.Getenv(key); v != \"\" {\n\t\treturn v\n\t}\n\treturn defaultVal\n}\n\nfunc main() {\n\tprocs := runtime.NumCPU()\n\t// packet sniffing can block an OS thread, so we need one thread\n\t// for that plus at least one more.\n\tif procs < 2 {\n\t\tprocs = 2\n\t}\n\truntime.GOMAXPROCS(procs)\n\n\tvar (\n\t\tjustVersion        bool\n\t\tjustPeerName       bool\n\t\tconfig             mesh.Config\n\t\tbridgeConfig       weavenet.BridgeConfig\n\t\tnetworkConfig      weave.NetworkConfig\n\t\tprotocolMinVersion int\n\t\tresume             bool\n\t\trouterName         string\n\t\tnickName           string\n\t\tpassword           string\n\t\tpktdebug           bool\n\t\tlogLevel           = \"info\"\n\t\tprof               string\n\t\tbufSzMB            int\n\t\tnoDiscovery        bool\n\t\thttpAddr           string\n\t\tstatusAddr         string\n\t\tmetricsAddr        string\n\t\tipamConfig         ipamConfig\n\t\tdockerAPI          string\n\t\tpeers              []string\n\t\tnoDNS              bool\n\t\tdnsConfig          dnsConfig\n\t\ttrustedSubnetStr   string\n\t\tdbPrefix           string\n\t\thostRoot           string\n\t\tprocPath           string\n\t\tdiscoveryEndpoint  string\n\t\ttoken              string\n\t\tadvertiseAddress   string\n\t\tpluginConfig       plugin.Config\n\t\tdefaultDockerHost  = getenvOrDefault(\"DOCKER_HOST\", \"unix:///var/run/docker.sock\")\n\t)\n\n\tmflag.BoolVar(&justVersion, []string{\"-version\"}, false, \"print version and exit\")\n\tmflag.BoolVar(&justPeerName, []string{\"-print-peer-name\"}, false, \"print peer name and exit\")\n\tmflag.StringVar(&config.Host, []string{\"-host\"}, \"\", \"router host\")\n\tmflag.IntVar(&config.Port, []string{\"-port\"}, mesh.Port, \"router port\")\n\tmflag.IntVar(&protocolMinVersion, []string{\"-min-protocol-version\"}, mesh.ProtocolMinVersion, \"minimum weave protocol version\")\n\tmflag.BoolVar(&resume, []string{\"-resume\"}, false, \"resume connections to previous peers\")\n\tmflag.StringVar(&bridgeConfig.WeaveBridgeName, []string{\"-weave-bridge\"}, \"weave\", \"name of weave bridge\")\n\tmflag.StringVar(&bridgeConfig.DockerBridgeName, []string{\"-docker-bridge\"}, \"\", \"name of Docker bridge\")\n\tmflag.BoolVar(&bridgeConfig.NPC, []string{\"-expect-npc\"}, false, \"set up iptables rules for npc\")\n\tmflag.StringVar(&routerName, []string{\"-name\"}, \"\", \"name of router (defaults to MAC of interface)\")\n\tmflag.StringVar(&nickName, []string{\"-nickname\"}, \"\", \"nickname of peer (defaults to hostname)\")\n\tmflag.StringVar(&password, []string{\"-password\"}, \"\", \"network password\")\n\tmflag.StringVar(&logLevel, []string{\"-log-level\"}, \"info\", \"logging level (debug, info, warning, error)\")\n\tmflag.BoolVar(&pktdebug, []string{\"-pkt-debug\"}, false, \"enable per-packet debug logging\")\n\tmflag.StringVar(&prof, []string{\"-profile\"}, \"\", \"enable profiling and write profiles to given path\")\n\tmflag.IntVar(&config.ConnLimit, []string{\"-conn-limit\"}, 200, \"connection limit (0 for unlimited)\")\n\tmflag.BoolVar(&noDiscovery, []string{\"-no-discovery\"}, false, \"disable peer discovery\")\n\tmflag.IntVar(&bufSzMB, []string{\"-bufsz\"}, 8, \"capture buffer size in MB\")\n\tmflag.IntVar(&bridgeConfig.MTU, []string{\"-mtu\"}, 0, \"MTU size\")\n\tmflag.StringVar(&httpAddr, []string{\"-http-addr\"}, \"\", \"address to bind HTTP interface to (disabled if blank, absolute path indicates unix domain socket)\")\n\tmflag.StringVar(&statusAddr, []string{\"-status-addr\"}, \"\", \"address to bind status+metrics interface to (disabled if blank, absolute path indicates unix domain socket)\")\n\tmflag.StringVar(&metricsAddr, []string{\"-metrics-addr\"}, \"\", \"address to bind metrics interface to (disabled if blank, absolute path indicates unix domain socket)\")\n\tmflag.StringVar(&ipamConfig.Mode, []string{\"-ipalloc-init\"}, \"\", \"allocator initialisation strategy (consensus, seed or observer)\")\n\tmflag.StringVar(&ipamConfig.IPRangeCIDR, []string{\"-ipalloc-range\"}, \"\", \"IP address range reserved for automatic allocation, in CIDR notation\")\n\tmflag.StringVar(&ipamConfig.IPSubnetCIDR, []string{\"-ipalloc-default-subnet\"}, \"\", \"subnet to allocate within by default, in CIDR notation\")\n\tmflag.StringVar(&dockerAPI, []string{\"-docker-api\"}, defaultDockerHost, \"Docker API endpoint\")\n\tmflag.BoolVar(&noDNS, []string{\"-no-dns\"}, false, \"disable DNS server\")\n\tmflag.StringVar(&dnsConfig.Domain, []string{\"-dns-domain\"}, nameserver.DefaultDomain, \"local domain to server requests for\")\n\tmflag.StringVar(&dnsConfig.ListenAddress, []string{\"-dns-listen-address\"}, nameserver.DefaultListenAddress, \"address to listen on for DNS requests\")\n\tmflag.IntVar(&dnsConfig.TTL, []string{\"-dns-ttl\"}, nameserver.DefaultTTL, \"TTL for DNS request from our domain\")\n\tmflag.DurationVar(&dnsConfig.ClientTimeout, []string{\"-dns-fallback-timeout\"}, nameserver.DefaultClientTimeout, \"timeout for fallback DNS requests\")\n\tmflag.StringVar(&dnsConfig.ResolvConf, []string{\"-resolv-conf\"}, \"\", \"path to resolver configuration for fallback DNS lookups\")\n\tmflag.StringVar(&bridgeConfig.DatapathName, []string{\"-datapath\"}, \"\", \"ODP datapath name\")\n\tmflag.BoolVar(&bridgeConfig.NoFastdp, []string{\"-no-fastdp\"}, false, \"Disable Fast Datapath\")\n\tmflag.BoolVar(&bridgeConfig.NoBridgedFastdp, []string{\"-no-bridged-fastdp\"}, false, \"Disable Bridged Fast Datapath\")\n\tmflag.StringVar(&trustedSubnetStr, []string{\"-trusted-subnets\"}, \"\", \"comma-separated list of trusted subnets in CIDR notation\")\n\tmflag.StringVar(&dbPrefix, []string{\"-db-prefix\"}, \"/weavedb/weave\", \"pathname/prefix of filename to store data\")\n\tmflag.StringVar(&procPath, []string{\"-proc-path\"}, \"/proc\", \"path to reach host /proc filesystem\")\n\tmflag.BoolVar(&bridgeConfig.AWSVPC, []string{\"-awsvpc\"}, false, \"use AWS VPC for routing\")\n\tmflag.StringVar(&hostRoot, []string{\"-host-root\"}, \"\", \"path to reach host filesystem\")\n\tmflag.StringVar(&discoveryEndpoint, []string{\"-peer-discovery-url\"}, \"https://cloud.weave.works/api/net\", \"url for peer discovery\")\n\tmflag.StringVar(&token, []string{\"-token\"}, \"\", \"token for peer discovery\")\n\tmflag.StringVar(&advertiseAddress, []string{\"-advertise-address\"}, \"\", \"address to advertise for peer discovery\")\n\n\tmflag.BoolVar(&pluginConfig.Enable, []string{\"-plugin\"}, false, \"enable Docker plugin (v1)\")\n\tmflag.BoolVar(&pluginConfig.EnableV2, []string{\"-plugin-v2\"}, false, \"enable Docker plugin (v2)\")\n\tmflag.BoolVar(&pluginConfig.EnableV2Multicast, []string{\"-plugin-v2-multicast\"}, false, \"enable multicast for Docker plugin (v2)\")\n\tmflag.StringVar(&pluginConfig.Socket, []string{\"-plugin-socket\"}, \"/run/docker/plugins/weave.sock\", \"plugin socket on which to listen\")\n\tmflag.StringVar(&pluginConfig.MeshSocket, []string{\"-plugin-mesh-socket\"}, \"/run/docker/plugins/weavemesh.sock\", \"plugin socket on which to listen in mesh mode\")\n\tmflag.BoolVar(&bridgeConfig.NoMasqLocal, []string{\"-no-masq-local\"}, false, \"do not SNAT external traffic sent to containers running on this node\")\n\n\tproxyConfig := newProxyConfig()\n\n\t// crude way of detecting that we probably have been started in a\n\t// container, with `weave launch` --> suppress misleading paths in\n\t// mflags error messages.\n\tif os.Args[0] == \"/home/weave/weaver\" { // matches the Dockerfile ENTRYPOINT\n\t\tos.Args[0] = \"weave\"\n\t\tmflag.CommandLine.Init(\"weave\", mflag.ExitOnError)\n\t}\n\n\tmflag.Parse()\n\n\tif justVersion {\n\t\tfmt.Printf(\"weave %s\\n\", version)\n\t\tos.Exit(0)\n\t}\n\tname := peerName(routerName, bridgeConfig.WeaveBridgeName, dbPrefix, hostRoot)\n\tif justPeerName {\n\t\tfmt.Printf(\"%s\\n\", name)\n\t\tos.Exit(0)\n\t}\n\n\tproxyConfig.DockerHost = dockerAPI\n\tif bridgeConfig.AWSVPC {\n\t\tproxyConfig.NoMulticastRoute = true\n\t\tproxyConfig.KeepTXOn = true\n\t}\n\n\tpeers = mflag.Args()\n\tif resume && len(peers) > 0 {\n\t\tLog.Fatalf(\"You must not specify an initial peer list in conjunction with --resume\")\n\t}\n\n\tcommon.SetLogLevel(logLevel)\n\tLog.Println(\"Command line options:\", options())\n\tLog.Infoln(\"weave \", version)\n\n\tif prof != \"\" {\n\t\tdefer profile.Start(profile.CPUProfile, profile.ProfilePath(prof), profile.NoShutdownHook).Stop()\n\t}\n\n\tif protocolMinVersion < mesh.ProtocolMinVersion || protocolMinVersion > mesh.ProtocolMaxVersion {\n\t\tLog.Fatalf(\"--min-protocol-version must be in range [%d,%d]\", mesh.ProtocolMinVersion, mesh.ProtocolMaxVersion)\n\t}\n\tconfig.ProtocolMinVersion = byte(protocolMinVersion)\n\n\tvar waitReady common.WaitGroup\n\n\tvar proxy *weaveproxy.Proxy\n\tvar err error\n\tif proxyConfig.Enabled {\n\t\tproxyConfig.DNSListenAddress = dnsConfig.addressOnly()\n\t\tproxyConfig.DNSDomain = dnsConfig.Domain\n\t\tif noDNS {\n\t\t\tproxyConfig.WithoutDNS = true\n\t\t}\n\t\t// Start Weave Proxy:\n\t\tproxy, err = weaveproxy.NewProxy(*proxyConfig)\n\t\tif err != nil {\n\t\t\tLog.Fatalf(\"Could not start proxy: %s\", err)\n\t\t}\n\t\tdefer proxy.Stop()\n\t\tlisteners := proxy.Listen()\n\t\tproxy.AttachExistingContainers()\n\t\tgo proxy.Serve(listeners, waitReady.Add())\n\t}\n\n\tif pktdebug {\n\t\tnetworkConfig.PacketLogging = packetLogging{}\n\t} else {\n\t\tnetworkConfig.PacketLogging = nopPacketLogging{}\n\t}\n\n\tif bridgeConfig.DockerBridgeName != \"\" {\n\t\tif setAddr, err := weavenet.EnforceAddrAssignType(bridgeConfig.DockerBridgeName); err != nil {\n\t\t\tLog.Errorf(\"While checking address assignment type of %s: %s\", bridgeConfig.DockerBridgeName, err)\n\t\t} else if setAddr {\n\t\t\tLog.Warningf(\"Setting %s MAC (mitigate https://github.com/docker/docker/issues/14908)\", bridgeConfig.DockerBridgeName)\n\t\t}\n\t}\n\n\tbridgeConfig.Mac = name.String()\n\tbridgeConfig.Port = config.Port\n\tips := ipset.New(common.LogLogger(), 0)\n\tbridgeType, err := weavenet.EnsureBridge(procPath, &bridgeConfig, Log, ips)\n\tcheckFatal(err)\n\tLog.Println(\"Bridge type is\", bridgeType)\n\n\tconfig.Password = determinePassword(password)\n\n\toverlay, injectorConsumer := createOverlay(bridgeType, bridgeConfig, config.Host, config.Port, bufSzMB, config.Password != nil)\n\tnetworkConfig.InjectorConsumer = injectorConsumer\n\n\tif injectorConsumer != nil {\n\t\tif err := weavenet.DetectHairpin(\"vethwe-bridge\", Log); err != nil {\n\t\t\tLog.Errorf(\"Setting may cause connectivity issues : %s\", err)\n\t\t\tLog.Infof(\"Hairpin mode may have been enabled by other software on this machine\")\n\t\t}\n\t}\n\n\tif nickName == \"\" {\n\t\tvar err error\n\t\tnickName, err = os.Hostname()\n\t\tcheckFatal(err)\n\t}\n\n\tconfig.TrustedSubnets = parseTrustedSubnets(trustedSubnetStr)\n\tconfig.PeerDiscovery = !noDiscovery\n\n\tif bridgeConfig.AWSVPC && len(config.Password) > 0 {\n\t\tLog.Fatalf(\"--awsvpc mode is not compatible with the --password option\")\n\t}\n\tif bridgeConfig.AWSVPC && !ipamConfig.Enabled() {\n\t\tLog.Fatalf(\"--awsvpc mode requires IPAM enabled\")\n\t}\n\tif bridgeConfig.AWSVPC && bridgeConfig.NoMasqLocal {\n\t\tLog.Fatalf(\"--awsvpc mode is not compatible with the --no-masq-local option\")\n\t}\n\n\tdb, err := db.NewBoltDB(dbPrefix)\n\tcheckFatal(err)\n\tdefer db.Close()\n\n\trouter, err := weave.NewNetworkRouter(config, networkConfig, bridgeConfig, name, nickName, overlay, db)\n\tcheckFatal(err)\n\tLog.Println(\"Our name is\", router.Ourself)\n\n\tif token != \"\" {\n\t\tvar addresses []string\n\t\tif advertiseAddress == \"\" {\n\t\t\tlocalAddrs, err := weavenet.LocalAddresses()\n\t\t\tcheckFatal(err)\n\t\t\tfor _, addr := range localAddrs {\n\t\t\t\taddresses = append(addresses, addr.IP.String())\n\t\t\t}\n\t\t} else {\n\t\t\taddresses = strings.Split(advertiseAddress, \",\")\n\t\t}\n\t\tdiscoveredPeers, count, err := peerDiscoveryUpdate(discoveryEndpoint, token, name.String(), nickName, addresses)\n\t\tcheckFatal(err)\n\t\tif !ipamConfig.HasMode() {\n\t\t\tipamConfig.PeerCount = len(peers) + count\n\t\t}\n\t\tpeers = append(peers, discoveredPeers...)\n\t} else if peers, err = router.InitialPeers(resume, peers); err != nil {\n\t\tLog.Fatal(\"Unable to get initial peer set: \", err)\n\t}\n\n\tvar dockerCli *docker.Client\n\tdockerVersion := \"none\"\n\tif dockerAPI != \"\" {\n\t\tdc, err := docker.NewClient(dockerAPI)\n\t\tif err != nil {\n\t\t\tLog.Fatal(\"Unable to start docker client: \", err)\n\t\t} else {\n\t\t\tLog.Info(dc.Info())\n\t\t}\n\t\tdockerCli = dc\n\t\tdockerVersion = dockerCli.DockerVersion()\n\t}\n\n\tcheckForUpdates(dockerVersion, router, uint(len(peers)))\n\n\tobserveContainers := func(o docker.ContainerObserver) {\n\t\tif dockerCli != nil {\n\t\t\tif err := dockerCli.AddObserver(o); err != nil {\n\t\t\t\tLog.Fatal(\"Unable to start watcher\", err)\n\t\t\t}\n\t\t}\n\t}\n\tisKnownPeer := func(name mesh.PeerName) bool {\n\t\treturn router.Peers.Fetch(name) != nil\n\t}\n\n\tvar (\n\t\tallocator     *ipam.Allocator\n\t\tdefaultSubnet address.CIDR\n\t)\n\tif ipamConfig.Enabled() {\n\t\tvar t tracker.LocalRangeTracker\n\t\tif bridgeConfig.AWSVPC {\n\t\t\tt, err = tracker.NewAWSVPCTracker(bridgeConfig.WeaveBridgeName)\n\t\t\tif err != nil {\n\t\t\t\tLog.Fatalf(\"Cannot create AWSVPC LocalRangeTracker: %s\", err)\n\t\t\t}\n\t\t} else if bridgeConfig.NoMasqLocal {\n\t\t\tt = weavenet.NewNoMasqLocalTracker(ips)\n\t\t}\n\t\tif t != nil {\n\t\t\tLog.Infof(\"Using %q LocalRangeTracker\", t)\n\t\t}\n\n\t\tpreClaims, err := findExistingAddresses(dockerCli, bridgeConfig.WeaveBridgeName)\n\t\tcheckFatal(err)\n\n\t\tallocator, defaultSubnet = createAllocator(router, ipamConfig, preClaims, db, t, isKnownPeer)\n\t\tobserveContainers(allocator)\n\n\t\tif dockerCli != nil {\n\t\t\tallContainerIDs, err := dockerCli.RunningContainerIDs()\n\t\t\tcheckFatal(err)\n\t\t\tallocator.PruneOwned(allContainerIDs)\n\t\t}\n\t}\n\n\tvar (\n\t\tns        *nameserver.Nameserver\n\t\tdnsserver *nameserver.DNSServer\n\t)\n\tif !noDNS {\n\t\tns, dnsserver = createDNSServer(dnsConfig, router.Router, isKnownPeer)\n\t\tobserveContainers(ns)\n\t\tns.Start()\n\t\tdefer ns.Stop()\n\t\tdnsserver.ActivateAndServe()\n\t\tif dockerCli != nil {\n\t\t\tpopulateDNS(ns, dockerCli, name, bridgeConfig.WeaveBridgeName)\n\t\t}\n\t\tdefer dnsserver.Stop()\n\t}\n\n\trouter.Start()\n\tif errors := router.InitiateConnections(peers, false); len(errors) > 0 {\n\t\tLog.Fatal(common.ErrorMessages(errors))\n\t}\n\tcheckFatal(router.CreateRestartSentinel())\n\n\tpluginConfig.DNS = !noDNS\n\tpluginConfig.DefaultSubnet = defaultSubnet.String()\n\tplugin := plugin.NewPlugin(pluginConfig)\n\n\t// The weave script always waits for a status call to succeed,\n\t// so there is no point in doing \"weave launch --http-addr ''\".\n\t// This is here to support stand-alone use of weaver.\n\tif httpAddr != \"\" {\n\t\tmuxRouter := mux.NewRouter()\n\t\tif allocator != nil {\n\t\t\tallocator.HandleHTTP(muxRouter, defaultSubnet, dockerCli)\n\t\t}\n\t\tif ns != nil {\n\t\t\tns.HandleHTTP(muxRouter, dockerCli)\n\t\t}\n\t\trouter.HandleHTTP(muxRouter)\n\t\tHandleHTTP(muxRouter, version, router, allocator, defaultSubnet, ns, dnsserver, proxy, plugin, &waitReady)\n\t\tHandleHTTPPeer(muxRouter, allocator, discoveryEndpoint, token, name.String())\n\t\tmuxRouter.Methods(\"GET\").Path(\"/metrics\").Handler(metricsHandler(router, allocator, ns, dnsserver))\n\t\tif proxy != nil {\n\t\t\tmuxRouter.Methods(\"GET\").Path(\"/proxyaddrs\").HandlerFunc(proxy.StatusHTTP)\n\t\t}\n\t\thttp.Handle(\"/\", common.LoggingHTTPHandler(muxRouter))\n\t\tLog.Println(\"Listening for HTTP control messages on\", httpAddr)\n\t\tgo listenAndServeHTTP(httpAddr, nil)\n\t}\n\n\tif statusAddr != \"\" {\n\t\tmuxRouter := mux.NewRouter()\n\t\tHandleHTTP(muxRouter, version, router, allocator, defaultSubnet, ns, dnsserver, proxy, plugin, &waitReady)\n\t\tmuxRouter.Methods(\"GET\").Path(\"/metrics\").Handler(metricsHandler(router, allocator, ns, dnsserver))\n\t\tstatusMux := http.NewServeMux()\n\t\tstatusMux.Handle(\"/\", muxRouter)\n\t\tLog.Println(\"Listening for status+metrics requests on\", statusAddr)\n\t\tgo listenAndServeHTTP(statusAddr, statusMux)\n\t}\n\n\tif metricsAddr != \"\" {\n\t\tmetricsMux := http.NewServeMux()\n\t\tmetricsMux.Handle(\"/metrics\", metricsHandler(router, allocator, ns, dnsserver))\n\t\tLog.Println(\"Listening for metrics requests on\", metricsAddr)\n\t\tgo listenAndServeHTTP(metricsAddr, metricsMux)\n\t}\n\n\tif plugin != nil {\n\t\tgo plugin.Start(httpAddr, dockerCli, waitReady.Add())\n\t}\n\n\tif bridgeConfig.AWSVPC {\n\t\t// Run this on its own goroutine because the allocator can block\n\t\t// We remove the default route installed by the kernel,\n\t\t// because awsvpc has installed it as well\n\t\tgo exposeForAWSVPC(allocator, defaultSubnet, bridgeConfig.WeaveBridgeName, waitReady.Add())\n\t}\n\n\tsignals.SignalHandlerLoop(common.Log, router)\n}\n\nfunc exposeForAWSVPC(alloc *ipam.Allocator, subnet address.CIDR, bridgeName string, ready func()) {\n\taddr, err := alloc.Allocate(\"weave:expose\", subnet, false, func() bool { return false })\n\tcheckFatal(err)\n\tcidr := address.MakeCIDR(subnet, addr)\n\terr = weavenet.Expose(bridgeName, cidr.IPNet(), true, false, false)\n\tcheckFatal(err)\n\tLog.Printf(\"Bridge %q exposed on address %v\", bridgeName, cidr)\n\tready()\n}\n\nfunc options() map[string]string {\n\toptions := make(map[string]string)\n\tmflag.Visit(func(f *mflag.Flag) {\n\t\tvalue := f.Value.String()\n\t\tname := canonicalName(f)\n\t\tif name == \"password\" || name == \"token\" {\n\t\t\tvalue = \"<redacted>\"\n\t\t}\n\t\toptions[name] = value\n\t})\n\treturn options\n}\n\nfunc canonicalName(f *mflag.Flag) string {\n\tfor _, n := range f.Names {\n\t\tif n[0] != '#' {\n\t\t\treturn strings.TrimLeft(n, \"#-\")\n\t\t}\n\t}\n\treturn \"\"\n}\n\ntype packetLogging struct{}\n\nfunc (packetLogging) LogPacket(msg string, key weave.PacketKey) {\n\tLog.Println(msg, key.SrcMAC, \"->\", key.DstMAC)\n}\n\nfunc (packetLogging) LogForwardPacket(msg string, key weave.ForwardPacketKey) {\n\tLog.Println(msg, key.SrcPeer, key.SrcMAC, \"->\", key.DstPeer, key.DstMAC)\n}\n\ntype nopPacketLogging struct{}\n\nfunc (nopPacketLogging) LogPacket(string, weave.PacketKey) {\n}\n\nfunc (nopPacketLogging) LogForwardPacket(string, weave.ForwardPacketKey) {\n}\n\nfunc newProxyConfig() *weaveproxy.Config {\n\tproxyConfig := weaveproxy.Config{\n\t\tImage:        getenvOrDefault(\"EXEC_IMAGE\", \"weaveworks/weaveexec\"),\n\t\tDockerBridge: getenvOrDefault(\"DOCKER_BRIDGE\", \"docker0\"),\n\t}\n\tmflag.BoolVar(&proxyConfig.Enabled, []string{\"-proxy\"}, false, \"instruct Weave Net to start its Docker proxy\")\n\tmflagext.ListVar(&proxyConfig.ListenAddrs, []string{\"H\"}, nil, \"addresses on which to listen for Docker proxy\")\n\tmflag.StringVar(&proxyConfig.HostnameFromLabel, []string{\"-hostname-from-label\"}, \"\", \"Key of container label from which to obtain the container's hostname\")\n\tmflag.StringVar(&proxyConfig.HostnameMatch, []string{\"-hostname-match\"}, \"(.*)\", \"Regexp pattern to apply on container names (e.g. '^aws-[0-9]+-(.*)$')\")\n\tmflag.StringVar(&proxyConfig.HostnameReplacement, []string{\"-hostname-replacement\"}, \"$1\", \"Expression to generate hostnames based on matches from --hostname-match (e.g. 'my-app-$1')\")\n\tmflag.BoolVar(&proxyConfig.RewriteInspect, []string{\"-rewrite-inspect\"}, false, \"Rewrite 'inspect' calls to return the weave network settings (if attached)\")\n\tmflag.BoolVar(&proxyConfig.NoDefaultIPAM, []string{\"-no-default-ipalloc\"}, false, \"proxy: do not automatically allocate addresses for containers without a WEAVE_CIDR\")\n\tmflag.BoolVar(&proxyConfig.NoRewriteHosts, []string{\"-no-rewrite-hosts\"}, false, \"proxy: do not automatically rewrite /etc/hosts. Use if you need the docker IP to remain in /etc/hosts\")\n\tmflag.StringVar(&proxyConfig.TLSConfig.CACert, []string{\"-tlscacert\"}, \"\", \"Trust certs signed only by this CA\")\n\tmflag.StringVar(&proxyConfig.TLSConfig.Cert, []string{\"-tlscert\"}, \"\", \"Path to TLS certificate file\")\n\tmflag.BoolVar(&proxyConfig.TLSConfig.Enabled, []string{\"-tls\"}, false, \"Use TLS; implied by --tlsverify\")\n\tmflag.StringVar(&proxyConfig.TLSConfig.Key, []string{\"-tlskey\"}, \"\", \"Path to TLS key file\")\n\tmflag.BoolVar(&proxyConfig.TLSConfig.Verify, []string{\"-tlsverify\"}, false, \"Use TLS and verify the remote\")\n\tmflag.BoolVar(&proxyConfig.WithoutDNS, []string{\"-without-dns\"}, false, \"proxy: instruct created containers to never use weaveDNS as their nameserver\")\n\tmflag.BoolVar(&proxyConfig.NoMulticastRoute, []string{\"-no-multicast-route\"}, false, \"proxy: do not add a multicast route via the weave interface when attaching containers\")\n\treturn &proxyConfig\n}\n\nfunc createOverlay(bridgeType weavenet.Bridge, config weavenet.BridgeConfig, host string, port int, bufSzMB int, enableEncryption bool) (weave.NetworkOverlay, weave.InjectorConsumer) {\n\toverlay := weave.NewOverlaySwitch()\n\tvar injectorConsumer weave.InjectorConsumer\n\tvar ignoreSleeve bool\n\n\tswitch {\n\tcase config.AWSVPC:\n\t\tvpc := weave.NewAWSVPC()\n\t\toverlay.Add(\"awsvpc\", vpc)\n\t\tinjectorConsumer = weave.NullInjectorConsumer{}\n\t\t// Currently, we do not support any overlay with AWSVPC\n\t\tignoreSleeve = true\n\tcase bridgeType == nil:\n\t\tinjectorConsumer = weave.NullInjectorConsumer{}\n\tcase bridgeType.IsFastdp():\n\t\tiface, err := weavenet.EnsureInterface(config.DatapathName)\n\t\tcheckFatal(err)\n\t\tfastdp, err := weave.NewFastDatapath(iface, port, enableEncryption)\n\t\tcheckFatal(err)\n\t\tinjectorConsumer = fastdp.InjectorConsumer()\n\t\toverlay.Add(\"fastdp\", fastdp.Overlay())\n\tcase !bridgeType.IsFastdp():\n\t\tiface, err := weavenet.EnsureInterface(weavenet.PcapIfName)\n\t\tcheckFatal(err)\n\t\tinjectorConsumer, err = weave.NewPcap(iface, bufSzMB*1024*1024) // bufsz flag is in MB\n\t\tcheckFatal(err)\n\t}\n\n\tif !ignoreSleeve {\n\t\tsleeve := weave.NewSleeveOverlay(host, port)\n\t\toverlay.Add(\"sleeve\", sleeve)\n\t\toverlay.SetCompatOverlay(sleeve)\n\t}\n\n\treturn overlay, injectorConsumer\n}\n\nfunc createAllocator(router *weave.NetworkRouter, config ipamConfig, preClaims []ipam.PreClaim, db db.DB, track tracker.LocalRangeTracker, isKnownPeer func(mesh.PeerName) bool) (*ipam.Allocator, address.CIDR) {\n\tipRange, err := ipam.ParseCIDRSubnet(config.IPRangeCIDR)\n\tcheckFatal(err)\n\tdefaultSubnet := ipRange\n\tif config.IPSubnetCIDR != \"\" {\n\t\tdefaultSubnet, err = ipam.ParseCIDRSubnet(config.IPSubnetCIDR)\n\t\tcheckFatal(err)\n\t\tif !ipRange.Range().Overlaps(defaultSubnet.Range()) {\n\t\t\tLog.Fatalf(\"IP address allocation default subnet %s does not overlap with allocation range %s\", defaultSubnet, ipRange)\n\t\t}\n\t}\n\n\tc := ipam.Config{\n\t\tOurName:     router.Ourself.Peer.Name,\n\t\tOurUID:      router.Ourself.Peer.UID,\n\t\tOurNickname: router.Ourself.Peer.NickName,\n\t\tSeed:        config.SeedPeerNames,\n\t\tUniverse:    ipRange,\n\t\tIsObserver:  config.Observer,\n\t\tPreClaims:   preClaims,\n\t\tQuorum:      func() uint { return determineQuorum(config.PeerCount, router) },\n\t\tDb:          db,\n\t\tIsKnownPeer: isKnownPeer,\n\t\tTracker:     track,\n\t}\n\n\tallocator := ipam.NewAllocator(c)\n\n\tgossip, err := router.NewGossip(\"IPallocation\", allocator)\n\tcheckFatal(err)\n\tallocator.SetInterfaces(gossip)\n\tallocator.Start()\n\trouter.Peers.OnGC(func(peer *mesh.Peer) { allocator.PeerGone(peer.Name) })\n\n\treturn allocator, defaultSubnet\n}\n\nfunc createDNSServer(config dnsConfig, router *mesh.Router, isKnownPeer func(mesh.PeerName) bool) (*nameserver.Nameserver, *nameserver.DNSServer) {\n\tns := nameserver.New(router.Ourself.Peer.Name, config.Domain, isKnownPeer)\n\trouter.Peers.OnGC(func(peer *mesh.Peer) { ns.PeerGone(peer.Name) })\n\tgossip, err := router.NewGossip(\"nameserver\", ns)\n\tcheckFatal(err)\n\tns.SetGossip(gossip)\n\tupstream := nameserver.NewUpstream(config.ResolvConf, config.addressOnly())\n\tdnsserver, err := nameserver.NewDNSServer(ns, config.Domain, config.ListenAddress,\n\t\tupstream, uint32(config.TTL), config.ClientTimeout)\n\tif err != nil {\n\t\tLog.Fatal(\"Unable to start dns server: \", err)\n\t}\n\tLog.Println(\"Listening for DNS queries on\", config.ListenAddress)\n\treturn ns, dnsserver\n}\n\n// Pick a quorum size based on the number of peer addresses.\nfunc determineQuorum(initPeerCountFlag int, router *weave.NetworkRouter) uint {\n\tif initPeerCountFlag > 0 {\n\t\treturn uint(initPeerCountFlag/2 + 1)\n\t}\n\n\tpeers := router.ConnectionMaker.Targets(true)\n\n\t// Guess a suitable quorum size based on the list of peer\n\t// addresses.  The peer list might or might not contain an\n\t// address for this peer, so the conservative assumption is\n\t// that it doesn't.  The list might contain multiple addresses\n\t// that resolve to the same peer, in which case the quorum\n\t// might be larger than it needs to be.  But the user can\n\t// specify it explicitly if that becomes a problem.\n\tclusterSize := uint(len(peers) + 1)\n\tquorum := clusterSize/2 + 1\n\tLog.Println(\"Assuming quorum size of\", quorum)\n\treturn quorum\n}\n\nfunc determinePassword(password string) []byte {\n\tif password == \"\" {\n\t\tpassword = os.Getenv(\"WEAVE_PASSWORD\")\n\t}\n\tif password == \"\" {\n\t\tLog.Println(\"Communication between peers is unencrypted.\")\n\t\treturn nil\n\t}\n\tLog.Println(\"Communication between peers via untrusted networks is encrypted.\")\n\treturn []byte(password)\n}\n\nfunc peerName(routerName, bridgeName, dbPrefix, hostRoot string) mesh.PeerName {\n\tif routerName == \"\" {\n\t\tiface, err := net.InterfaceByName(bridgeName)\n\t\tif err == nil {\n\t\t\trouterName = iface.HardwareAddr.String()\n\t\t} else {\n\t\t\trouterName, err = weavenet.GetSystemPeerName(dbPrefix, hostRoot)\n\t\t\tcheckFatal(err)\n\t\t}\n\t}\n\tname, err := mesh.PeerNameFromUserInput(routerName)\n\tcheckFatal(err)\n\treturn name\n}\n\nfunc parseTrustedSubnets(trustedSubnetStr string) []*net.IPNet {\n\ttrustedSubnets := []*net.IPNet{}\n\tif trustedSubnetStr == \"\" {\n\t\treturn trustedSubnets\n\t}\n\n\tfor _, subnetStr := range strings.Split(trustedSubnetStr, \",\") {\n\t\t_, subnet, err := net.ParseCIDR(subnetStr)\n\t\tif err != nil {\n\t\t\tLog.Fatal(\"Unable to parse trusted subnets: \", err)\n\t\t}\n\t\ttrustedSubnets = append(trustedSubnets, subnet)\n\t}\n\n\treturn trustedSubnets\n}\n\nfunc parsePeerNames(s string) ([]mesh.PeerName, error) {\n\tpeerNames := []mesh.PeerName{}\n\tif s == \"\" {\n\t\treturn peerNames, nil\n\t}\n\n\tfor _, peerNameStr := range strings.Split(s, \",\") {\n\t\tpeerName, err := mesh.PeerNameFromUserInput(peerNameStr)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error parsing peer names: %s\", err)\n\t\t}\n\t\tpeerNames = append(peerNames, peerName)\n\t}\n\n\treturn peerNames, nil\n}\n\nfunc listenAndServeHTTP(httpAddr string, handler http.Handler) {\n\tprotocol := \"tcp\"\n\tif strings.HasPrefix(httpAddr, \"/\") {\n\t\tos.Remove(httpAddr) // in case it's there from last time\n\t\tprotocol = \"unix\"\n\t}\n\tl, err := net.Listen(protocol, httpAddr)\n\tif err != nil {\n\t\tLog.Fatal(\"Unable to create http listener socket: \", err)\n\t}\n\terr = http.Serve(l, handler)\n\tif err != nil {\n\t\tLog.Fatal(\"Unable to create http server\", err)\n\t}\n}\n\nfunc checkFatal(e error) {\n\tif e != nil {\n\t\tLog.Fatal(e)\n\t}\n}\n"], "fixing_code": ["package net\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"path/filepath\"\n\t\"syscall\"\n\n\t\"github.com/coreos/go-iptables/iptables\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/vishvananda/netlink\"\n\t\"k8s.io/apimachinery/pkg/types\"\n\n\t\"github.com/weaveworks/weave/common\"\n\t\"github.com/weaveworks/weave/common/odp\"\n\t\"github.com/weaveworks/weave/ipam/tracker\"\n\t\"github.com/weaveworks/weave/net/address\"\n\t\"github.com/weaveworks/weave/net/ipset\"\n\t\"github.com/weaveworks/weave/npc\"\n)\n\n/* This code implements three possible configurations to connect\n   containers to the Weave Net overlay:\n\n1. Bridge\n                 +-------+\n(container-veth)-+ weave +-(vethwe-bridge)--(vethwe-pcap)\n                 +-------+\n\n\"weave\" is a Linux bridge. \"vethwe-pcap\" (end of veth pair) is used\nto capture and inject packets, by router/pcap.go.\n\n2. BridgedFastdp\n\n                 +-------+                                    /----------\\\n(container-veth)-+ weave +-(vethwe-bridge)--(vethwe-datapath)-+ datapath +\n                 +-------+                                    \\----------/\n\n\"weave\" is a Linux bridge and \"datapath\" is an Open vSwitch datapath;\nthey are connected via a veth pair. Packet capture and injection use\nthe \"datapath\" device, via \"router/fastdp.go:fastDatapathBridge\"\n\n3. Fastdp\n\n                 /-------\\\n(container-veth)-+ weave +\n                 \\-------/\n\n\"weave\" is an Open vSwitch datapath, and capture/injection are as in\nBridgedFastdp. Not used by default due to missing conntrack support in\ndatapath of old kernel versions (https://github.com/weaveworks/weave/issues/1577).\n*/\n\nconst (\n\tWeaveBridgeName  = \"weave\"\n\tDatapathName     = \"datapath\"\n\tDatapathIfName   = \"vethwe-datapath\"\n\tBridgeIfName     = \"vethwe-bridge\"\n\tPcapIfName       = \"vethwe-pcap\"\n\tNoMasqLocalIpset = ipset.Name(\"weaver-no-masq-local\")\n)\n\ntype Bridge interface {\n\tinit(procPath string, config *BridgeConfig) error // create and initialise bridge device(s)\n\tattach(veth *netlink.Veth) error                  // attach veth to bridge\n\tIsFastdp() bool                                   // does this bridge use fastdp?\n\tString() string                                   // human-readable type string\n}\n\n// Used to indicate a fallback to the Bridge type\nvar errBridgeNotSupported = errors.New(\"bridge not supported\")\n\ntype bridgeImpl struct{ bridge netlink.Link }\ntype fastdpImpl struct{ datapathName string }\ntype bridgedFastdpImpl struct {\n\tbridgeImpl\n\tfastdpImpl\n}\n\n// Returns a string that is consistent with the weave script\nfunc (bridgeImpl) String() string        { return \"bridge\" }\nfunc (fastdpImpl) String() string        { return \"fastdp\" }\nfunc (bridgedFastdpImpl) String() string { return \"bridged_fastdp\" }\n\n// Used to decide whether to manage ODP tunnels\nfunc (bridgeImpl) IsFastdp() bool        { return false }\nfunc (fastdpImpl) IsFastdp() bool        { return true }\nfunc (bridgedFastdpImpl) IsFastdp() bool { return true }\n\nfunc ExistingBridgeType(weaveBridgeName, datapathName string) (Bridge, error) {\n\tbridge, _ := netlink.LinkByName(weaveBridgeName)\n\tdatapath, _ := netlink.LinkByName(datapathName)\n\n\tswitch {\n\tcase bridge == nil && datapath == nil:\n\t\treturn nil, nil\n\tcase isBridge(bridge) && datapath == nil:\n\t\treturn bridgeImpl{bridge: bridge}, nil\n\tcase isDatapath(bridge) && datapath == nil:\n\t\treturn fastdpImpl{datapathName: datapathName}, nil\n\tcase isDatapath(datapath) && isBridge(bridge):\n\t\treturn bridgedFastdpImpl{bridgeImpl{bridge: bridge}, fastdpImpl{datapathName: datapathName}}, nil\n\tdefault:\n\t\treturn nil, errors.New(\"Inconsistent bridge state detected. Please do 'weave reset' and try again\")\n\t}\n}\n\nfunc EnforceAddrAssignType(bridgeName string) (setAddr bool, err error) {\n\tsysctlFilename := filepath.Join(\"/sys/class/net/\", bridgeName, \"/addr_assign_type\")\n\taddrAssignType, err := ioutil.ReadFile(sysctlFilename)\n\tif err != nil {\n\t\treturn false, errors.Wrapf(err, \"reading %q\", sysctlFilename)\n\t}\n\n\t// From include/uapi/linux/netdevice.h\n\t// #define NET_ADDR_PERM       0   /* address is permanent (default) */\n\t// #define NET_ADDR_RANDOM     1   /* address is generated randomly */\n\t// #define NET_ADDR_STOLEN     2   /* address is stolen from other device */\n\t// #define NET_ADDR_SET        3   /* address is set using dev_set_mac_address() */\n\t// Note the file typically has a newline at the end, so we just look at the first char\n\tif addrAssignType[0] != '3' {\n\t\tlink, err := netlink.LinkByName(bridgeName)\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrapf(err, \"EnforceAddrAssignType finding bridge %s\", bridgeName)\n\t\t}\n\n\t\tmac, err := RandomMAC()\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrap(err, \"creating random MAC\")\n\t\t}\n\n\t\tif err := netlink.LinkSetHardwareAddr(link, mac); err != nil {\n\t\t\treturn false, errors.Wrapf(err, \"setting bridge %s address to %v\", bridgeName, mac)\n\t\t}\n\t\treturn true, nil\n\t}\n\n\treturn false, nil\n}\n\nfunc isBridge(link netlink.Link) bool {\n\t_, isBridge := link.(*netlink.Bridge)\n\treturn isBridge\n}\n\nfunc isDatapath(link netlink.Link) bool {\n\tswitch link.(type) {\n\tcase *netlink.GenericLink:\n\t\treturn link.Type() == \"openvswitch\"\n\tcase *netlink.Device:\n\t\t// Assume it's our openvswitch device, and the kernel has not been updated to report the kind.\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\nfunc DetectHairpin(portIfName string, log *logrus.Logger) error {\n\tlink, err := netlink.LinkByName(portIfName)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Unable to find link %q: %s\", portIfName, err)\n\t}\n\n\tch := make(chan netlink.LinkUpdate)\n\t// See EnsureInterface for why done channel is not passed\n\tif err := netlink.LinkSubscribe(ch, nil); err != nil {\n\t\treturn fmt.Errorf(\"Unable to subscribe to netlink updates: %s\", err)\n\t}\n\n\tpi, err := netlink.LinkGetProtinfo(link)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Unable to get link protinfo %q: %s\", portIfName, err)\n\t}\n\tif pi.Hairpin {\n\t\treturn fmt.Errorf(\"Hairpin mode enabled on %q\", portIfName)\n\t}\n\n\tgo func() {\n\t\tfor up := range ch {\n\t\t\tif up.Attrs().Name == portIfName && up.Attrs().Protinfo != nil &&\n\t\t\t\tup.Attrs().Protinfo.Hairpin {\n\t\t\t\tlog.Errorf(\"Hairpin mode enabled on %q\", portIfName)\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn nil\n}\n\nvar ErrBridgeNoIP = fmt.Errorf(\"Bridge has no IP address\")\n\nfunc FindBridgeIP(bridgeName string, subnet *net.IPNet) (net.IP, error) {\n\tnetdev, err := GetBridgeNetDev(bridgeName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Failed to get netdev for %q bridge: %s\", bridgeName, err)\n\t}\n\tif len(netdev.CIDRs) == 0 {\n\t\treturn nil, ErrBridgeNoIP\n\t}\n\tif subnet != nil {\n\t\tfor _, cidr := range netdev.CIDRs {\n\t\t\tif subnet.Contains(cidr.IP) {\n\t\t\t\treturn cidr.IP, nil\n\t\t\t}\n\t\t}\n\t}\n\t// No subnet, or none in the required subnet; just return the first one\n\treturn netdev.CIDRs[0].IP, nil\n}\n\ntype BridgeConfig struct {\n\tDockerBridgeName string\n\tWeaveBridgeName  string\n\tDatapathName     string\n\tNoFastdp         bool\n\tNoBridgedFastdp  bool\n\tAWSVPC           bool\n\tNPC              bool\n\tMTU              int\n\tMac              string\n\tPort             int\n\tNoMasqLocal      bool\n}\n\nfunc (config *BridgeConfig) configuredBridgeType() Bridge {\n\tswitch {\n\tcase config.NoFastdp:\n\t\treturn bridgeImpl{}\n\tcase config.NoBridgedFastdp:\n\t\treturn fastdpImpl{datapathName: config.WeaveBridgeName}\n\tdefault:\n\t\treturn bridgedFastdpImpl{fastdpImpl: fastdpImpl{datapathName: config.DatapathName}}\n\t}\n}\n\nfunc EnsureBridge(procPath string, config *BridgeConfig, log *logrus.Logger, ips ipset.Interface) (Bridge, error) {\n\texistingBridgeType, err := ExistingBridgeType(config.WeaveBridgeName, config.DatapathName)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbridgeType := config.configuredBridgeType()\n\n\tif existingBridgeType != nil && bridgeType.String() != existingBridgeType.String() {\n\t\treturn nil,\n\t\t\tfmt.Errorf(\"Existing bridge type %q is different than requested %q. Please do 'weave reset' and try again\",\n\t\t\t\texistingBridgeType, bridgeType)\n\t}\n\n\tfor {\n\t\tif err := bridgeType.init(procPath, config); err != nil {\n\t\t\tif errors.Cause(err) == errBridgeNotSupported {\n\t\t\t\tlog.Warnf(\"Skipping bridge creation of %q due to: %s\", bridgeType, err)\n\t\t\t\tbridgeType = bridgeImpl{}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tbreak\n\t}\n\n\tif err := configureIPTables(config, ips); err != nil {\n\t\treturn bridgeType, errors.Wrap(err, \"configuring iptables\")\n\t}\n\n\tif config.AWSVPC {\n\t\t// Set proxy_arp on the bridge, so that it could accept packets destined\n\t\t// to containers within the same subnet but running on remote hosts.\n\t\t// Without it, exact routes on each container are required.\n\t\tif err := sysctl(procPath, \"net/ipv4/conf/\"+config.WeaveBridgeName+\"/proxy_arp\", \"1\"); err != nil {\n\t\t\treturn bridgeType, errors.Wrap(err, \"setting proxy_arp\")\n\t\t}\n\t\t// Avoid delaying the first ARP request. Also, setting it to 0 avoids\n\t\t// placing the request into a bounded queue as it can be seen:\n\t\t// https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/net/ipv4/arp.c?id=refs/tags/v4.6.1#n819\n\t\tif err := sysctl(procPath, \"net/ipv4/neigh/\"+config.WeaveBridgeName+\"/proxy_delay\", \"0\"); err != nil {\n\t\t\treturn bridgeType, errors.Wrap(err, \"setting proxy_arp\")\n\t\t}\n\t}\n\t// No ipv6 router advertisments please\n\tif err := sysctl(procPath, \"net/ipv6/conf/\"+config.WeaveBridgeName+\"/accept_ra\", \"0\"); err != nil {\n\t\treturn bridgeType, errors.Wrap(err, \"setting accept_ra to 0\")\n\t}\n\n\tif err := linkSetUpByName(config.WeaveBridgeName); err != nil {\n\t\treturn bridgeType, err\n\t}\n\n\tif err := monitorInterface(config.WeaveBridgeName, log); err != nil {\n\t\treturn bridgeType, err\n\t}\n\n\tif err := configureARPCache(procPath, config.WeaveBridgeName); err != nil {\n\t\treturn bridgeType, errors.Wrapf(err, \"configuring ARP cache on bridge %q\", config.WeaveBridgeName)\n\t}\n\n\t// NB: No concurrent call to Expose is possible, as EnsureBridge is called\n\t// before any service has been started.\n\tif err := reexpose(config, log); err != nil {\n\t\treturn bridgeType, err\n\t}\n\n\treturn bridgeType, nil\n}\n\nfunc (b bridgeImpl) initPrep(config *BridgeConfig) error {\n\tmac, err := net.ParseMAC(config.Mac)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"parsing bridge MAC %q\", config.Mac)\n\t}\n\n\tlinkAttrs := netlink.NewLinkAttrs()\n\tlinkAttrs.Name = config.WeaveBridgeName\n\t// NB: Do not set MAC addr when creating the bridge, set it manually\n\t// afterwards instead. Otherwise, on an older than 3.14 kernel FDB\n\t// entry won't be created which results in containers not being able to\n\t// reach the bridge w/o promiscuous mode.\n\tif config.MTU == 0 {\n\t\tconfig.MTU = 65535\n\t}\n\tb.bridge = &netlink.Bridge{LinkAttrs: linkAttrs}\n\tif err := LinkAddIfNotExist(b.bridge); err != nil {\n\t\treturn errors.Wrapf(err, \"creating bridge %q\", config.WeaveBridgeName)\n\t}\n\tif err := netlink.LinkSetHardwareAddr(b.bridge, mac); err != nil {\n\t\treturn errors.Wrapf(err, \"setting bridge %q mac %v\", config.WeaveBridgeName, mac)\n\t}\n\t// Attempting to set the bridge MTU to a high value directly\n\t// fails. Bridges take the lowest MTU of their interfaces. So\n\t// instead we create a temporary interface with the desired MTU,\n\t// attach that to the bridge, and then remove it again.\n\tdummy := &netlink.Dummy{LinkAttrs: netlink.NewLinkAttrs()}\n\tdummy.LinkAttrs.Name = \"vethwedu\"\n\tif err = netlink.LinkAdd(dummy); err != nil {\n\t\treturn errors.Wrap(err, \"creating dummy interface\")\n\t}\n\tif err := netlink.LinkSetMTU(dummy, config.MTU); err != nil {\n\t\treturn errors.Wrapf(err, \"setting dummy interface mtu to %d\", config.MTU)\n\t}\n\tif err := netlink.LinkSetMasterByIndex(dummy, b.bridge.Attrs().Index); err != nil {\n\t\treturn errors.Wrap(err, \"setting dummy interface master\")\n\t}\n\tif err := netlink.LinkDel(dummy); err != nil {\n\t\treturn errors.Wrap(err, \"deleting dummy interface\")\n\t}\n\n\treturn nil\n}\n\nfunc (b bridgeImpl) init(procPath string, config *BridgeConfig) error {\n\tif err := b.initPrep(config); err != nil {\n\t\treturn err\n\t}\n\tif _, err := CreateAndAttachVeth(procPath, BridgeIfName, PcapIfName, config.WeaveBridgeName, config.MTU, true, false, func(veth netlink.Link) error {\n\t\treturn netlink.LinkSetUp(veth)\n\t}); err != nil {\n\t\treturn errors.Wrap(err, \"creating pcap veth pair\")\n\t}\n\tif err := EthtoolTXOff(config.WeaveBridgeName); err != nil {\n\t\treturn errors.Wrap(err, \"setting tx off\")\n\t}\n\n\treturn nil\n}\n\nfunc (f fastdpImpl) init(procPath string, config *BridgeConfig) error {\n\todpSupported, err := odp.CreateDatapath(f.datapathName)\n\tif !odpSupported {\n\t\tmsg := \"\"\n\t\tif err != nil {\n\t\t\tmsg = err.Error()\n\t\t}\n\t\treturn errors.Wrap(errBridgeNotSupported, msg)\n\t}\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"creating datapath %q\", f.datapathName)\n\t}\n\tdatapath, err := netlink.LinkByName(f.datapathName)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"finding datapath %q\", f.datapathName)\n\t}\n\tif config.MTU == 0 {\n\t\t/* GCE has the lowest underlay network MTU we're likely to encounter on\n\t\t   a local network, at 1460 bytes.  To get the overlay MTU from that we\n\t\t   subtract 20 bytes for the outer IPv4 header, 8 bytes for the outer\n\t\t   UDP header, 8 bytes for the vxlan header, and 14 bytes for the inner\n\t\t   ethernet header.  In addition, we subtract 34 bytes for the ESP overhead\n\t\t   which is needed for the vxlan encryption. */\n\t\tconfig.MTU = 1376\n\t}\n\tif err := netlink.LinkSetMTU(datapath, config.MTU); err != nil {\n\t\treturn errors.Wrapf(err, \"setting datapath %q mtu %d\", f.datapathName, config.MTU)\n\t}\n\treturn nil\n}\n\nfunc (bf bridgedFastdpImpl) init(procPath string, config *BridgeConfig) error {\n\tif err := bf.fastdpImpl.init(procPath, config); err != nil {\n\t\treturn err\n\t}\n\tif err := bf.bridgeImpl.initPrep(config); err != nil {\n\t\treturn err\n\t}\n\tif _, err := CreateAndAttachVeth(procPath, BridgeIfName, DatapathIfName, config.WeaveBridgeName, config.MTU, true, false, func(veth netlink.Link) error {\n\t\tif err := netlink.LinkSetUp(veth); err != nil {\n\t\t\treturn errors.Wrapf(err, \"setting link up on %q\", veth.Attrs().Name)\n\t\t}\n\t\tif err := odp.AddDatapathInterfaceIfNotExist(bf.datapathName, veth.Attrs().Name); err != nil {\n\t\t\treturn errors.Wrapf(err, \"adding interface %q to datapath %q\", veth.Attrs().Name, bf.datapathName)\n\t\t}\n\t\treturn nil\n\t}); err != nil {\n\t\treturn errors.Wrap(err, \"creating bridged fastdp veth pair\")\n\t}\n\n\treturn linkSetUpByName(bf.datapathName)\n}\n\nfunc (b bridgeImpl) attach(veth *netlink.Veth) error {\n\treturn netlink.LinkSetMasterByIndex(veth, b.bridge.Attrs().Index)\n}\n\nfunc (bf bridgedFastdpImpl) attach(veth *netlink.Veth) error {\n\treturn bf.bridgeImpl.attach(veth)\n}\n\nfunc (f fastdpImpl) attach(veth *netlink.Veth) error {\n\treturn odp.AddDatapathInterfaceIfNotExist(f.datapathName, veth.Attrs().Name)\n}\n\nfunc configureIPTables(config *BridgeConfig, ips ipset.Interface) error {\n\tipt, err := iptables.New()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating iptables object\")\n\t}\n\n\t// The order among weave filter/FORWARD rules is important!\n\tfwdRules := make([][]string, 0)\n\n\tif config.DockerBridgeName != \"\" {\n\t\tif config.WeaveBridgeName != config.DockerBridgeName {\n\t\t\tfwdRules = append(fwdRules, []string{\"-i\", config.DockerBridgeName, \"-o\", config.WeaveBridgeName, \"-j\", \"DROP\"})\n\t\t}\n\n\t\tdockerBridgeIP, err := FindBridgeIP(config.DockerBridgeName, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// forbid traffic to the Weave port from other containers\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.DockerBridgeName, \"-p\", \"tcp\", \"--dst\", dockerBridgeIP.String(), \"--dport\", fmt.Sprint(config.Port), \"-j\", \"DROP\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.DockerBridgeName, \"-p\", \"udp\", \"--dst\", dockerBridgeIP.String(), \"--dport\", fmt.Sprint(config.Port), \"-j\", \"DROP\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.DockerBridgeName, \"-p\", \"udp\", \"--dst\", dockerBridgeIP.String(), \"--dport\", fmt.Sprint(config.Port+1), \"-j\", \"DROP\"); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// let DNS traffic to weaveDNS, since otherwise it might get blocked by the likes of UFW\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.DockerBridgeName, \"-p\", \"udp\", \"--dport\", \"53\", \"-j\", \"ACCEPT\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.DockerBridgeName, \"-p\", \"tcp\", \"--dport\", \"53\", \"-j\", \"ACCEPT\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif config.NPC {\n\t\t// Steer traffic via the NPC.\n\n\t\tif err = ensureChains(ipt, \"filter\", npc.MainChain, npc.EgressChain); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Steer egress traffic destined to local node.\n\t\tif err = ipt.AppendUnique(\"filter\", \"INPUT\", \"-i\", config.WeaveBridgeName, \"-j\", npc.EgressChain); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfwdRules = append(fwdRules,\n\t\t\t[][]string{\n\t\t\t\t// Might include ingress traffic which is fine as long as we do not\n\t\t\t\t// ACCEPT in WEAVE-NPC-EGRESS chain\n\t\t\t\t{\"-i\", config.WeaveBridgeName,\n\t\t\t\t\t\"-m\", \"comment\", \"--comment\", \"NOTE: this must go before '-j KUBE-FORWARD'\",\n\t\t\t\t\t\"-j\", npc.EgressChain},\n\t\t\t\t// The following rules are for ingress NPC processing\n\t\t\t\t{\"-o\", config.WeaveBridgeName,\n\t\t\t\t\t\"-m\", \"comment\", \"--comment\", \"NOTE: this must go before '-j KUBE-FORWARD'\",\n\t\t\t\t\t\"-j\", npc.MainChain},\n\t\t\t\t{\"-o\", config.WeaveBridgeName, \"-m\", \"state\", \"--state\", \"NEW\", \"-j\", \"NFLOG\", \"--nflog-group\", \"86\"},\n\t\t\t\t{\"-o\", config.WeaveBridgeName, \"-j\", \"DROP\"},\n\t\t\t}...)\n\t} else {\n\t\t// Work around the situation where there are no rules allowing traffic\n\t\t// across our bridge. E.g. ufw\n\t\tfwdRules = append(fwdRules, []string{\"-i\", config.WeaveBridgeName, \"-o\", config.WeaveBridgeName, \"-j\", \"ACCEPT\"})\n\t}\n\n\tif !config.NPC {\n\t\t// Create/Flush a chain for allowing ingress traffic when the bridge is exposed\n\t\tif err := ipt.ClearChain(\"filter\", \"WEAVE-EXPOSE\"); err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to clear/create filter/WEAVE-EXPOSE chain\")\n\t\t}\n\n\t\tfwdRules = append(fwdRules, []string{\"-o\", config.WeaveBridgeName, \"-j\", \"WEAVE-EXPOSE\"})\n\t}\n\n\t// Forward from weave to the rest of the world\n\tfwdRules = append(fwdRules, []string{\"-i\", config.WeaveBridgeName, \"!\", \"-o\", config.WeaveBridgeName, \"-j\", \"ACCEPT\"})\n\t// and allow replies back\n\tfwdRules = append(fwdRules, []string{\"-o\", config.WeaveBridgeName, \"-m\", \"conntrack\", \"--ctstate\", \"RELATED,ESTABLISHED\", \"-j\", \"ACCEPT\"})\n\n\tif err := ensureRulesAtTop(\"filter\", \"FORWARD\", fwdRules, ipt); err != nil {\n\t\treturn err\n\t}\n\n\t// Create a chain for masquerading\n\tif err := ipt.ClearChain(\"nat\", \"WEAVE\"); err != nil {\n\t\treturn errors.Wrap(err, \"failed to clear/create nat/WEAVE chain\")\n\t}\n\tif err := ipt.AppendUnique(\"nat\", \"POSTROUTING\", \"-j\", \"WEAVE\"); err != nil {\n\t\treturn err\n\t}\n\n\t// For the cases where the weave bridge is the default gateway for\n\t// containers (e.g. Kubernetes): create the ipset to store CIDRs allocated\n\t// by IPAM for local containers. In the case of Kubernetes, external traffic\n\t// sent to these CIDRs avoids SNAT'ing so that NodePort with\n\t// `\"externalTrafficPolicy\":\"Local\"` would receive packets with correct\n\t// src IP addr.\n\tif config.NoMasqLocal {\n\t\tips := ipset.New(common.LogLogger(), 0)\n\t\t_ = ips.Destroy(NoMasqLocalIpset)\n\t\tif err := ips.Create(NoMasqLocalIpset, ipset.HashNet); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := ipt.Insert(\"nat\", \"WEAVE\", 1,\n\t\t\t\"-m\", \"set\", \"--match-set\", string(NoMasqLocalIpset), \"dst\",\n\t\t\t\"-m\", \"comment\", \"--comment\", \"Prevent SNAT to locally running containers\",\n\t\t\t\"-j\", \"RETURN\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\ntype NoMasqLocalTracker struct {\n\tips   ipset.Interface\n\towner types.UID\n}\n\nfunc NewNoMasqLocalTracker(ips ipset.Interface) *NoMasqLocalTracker {\n\treturn &NoMasqLocalTracker{\n\t\tips:   ips,\n\t\towner: types.UID(0), // dummy ipset owner\n\t}\n}\n\nfunc (t *NoMasqLocalTracker) String() string {\n\treturn \"no-masq-local\"\n}\n\nfunc (t *NoMasqLocalTracker) HandleUpdate(prevRanges, currRanges []address.Range, local bool) error {\n\tif !local {\n\t\treturn nil\n\t}\n\n\tprev, curr := tracker.RemoveCommon(\n\t\taddress.NewCIDRs(tracker.Merge(prevRanges)),\n\t\taddress.NewCIDRs(tracker.Merge(currRanges)))\n\n\tfor _, cidr := range curr {\n\t\tif err := t.ips.AddEntry(t.owner, NoMasqLocalIpset, cidr.String(), \"\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tfor _, cidr := range prev {\n\t\tif err := t.ips.DelEntry(t.owner, NoMasqLocalIpset, cidr.String()); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc linkSetUpByName(linkName string) error {\n\tlink, err := netlink.LinkByName(linkName)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"setting link up on %q\", linkName)\n\t}\n\treturn netlink.LinkSetUp(link)\n}\n\nfunc reexpose(config *BridgeConfig, log *logrus.Logger) error {\n\t// Get existing IP addrs of the weave bridge.\n\t// If the bridge hasn't been exposed, then this functions does nothing.\n\t//\n\t// Ideally, we should consult IPAM for IP addrs allocated to \"weave:expose\",\n\t// but we don't want to introduce dependency on IPAM, as weave should be able\n\t// to run w/o IPAM.\n\tlink, err := netlink.LinkByName(config.WeaveBridgeName)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"cannot find bridge %q\", config.WeaveBridgeName)\n\t}\n\taddrs, err := netlink.AddrList(link, netlink.FAMILY_V4)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"cannot list IPv4 addrs of bridge %q\", config.WeaveBridgeName)\n\t}\n\n\tfor _, addr := range addrs {\n\t\tlog.Infof(\"Re-exposing %s on bridge %q\", addr.IPNet, config.WeaveBridgeName)\n\t\tif err := Expose(config.WeaveBridgeName, addr.IPNet, config.AWSVPC, config.NPC, false); err != nil {\n\t\t\treturn errors.Wrapf(err, \"unable to re-expose %s on bridge: %q\", addr.IPNet, config.WeaveBridgeName)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc monitorInterface(ifaceName string, log *logrus.Logger) error {\n\t_, err := netlink.LinkByName(ifaceName)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Unable to find link %q: %s\", ifaceName, err)\n\t}\n\n\tupdatesChannel := make(chan netlink.LinkUpdate)\n\tif err := netlink.LinkSubscribe(updatesChannel, nil); err != nil {\n\t\treturn errors.Wrapf(err, \"error monitoring link %q for UP/DOWN notifications\", ifaceName)\n\t}\n\n\tgo func() {\n\t\tfor update := range updatesChannel {\n\t\t\tif update.Link.Attrs().Name == ifaceName && update.IfInfomsg.Flags&syscall.IFF_UP == 0 {\n\t\t\t\tlog.Errorf(\"Interface %q which needs to be in UP state for Weave functioning is found to be in DOWN state\", ifaceName)\n\t\t\t}\n\t\t}\n\t}()\n\treturn nil\n}\n", "// +build go1.10\n\npackage net\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"runtime\"\n\n\t\"github.com/vishvananda/netlink\"\n\t\"github.com/vishvananda/netns\"\n)\n\nvar ErrLinkNotFound = errors.New(\"Link not found\")\n\nfunc WithNetNS(ns netns.NsHandle, work func() error) error {\n\truntime.LockOSThread()\n\tdefer runtime.UnlockOSThread()\n\n\toldNs, err := netns.Get()\n\tif err == nil {\n\t\tdefer oldNs.Close()\n\n\t\terr = netns.Set(ns)\n\t\tif err == nil {\n\t\t\tdefer netns.Set(oldNs)\n\n\t\t\terr = work()\n\t\t}\n\t}\n\n\treturn err\n}\n\nfunc WithNetNSLink(ns netns.NsHandle, ifName string, work func(link netlink.Link) error) error {\n\treturn WithNetNS(ns, func() error {\n\t\tlink, err := netlink.LinkByName(ifName)\n\t\tif err != nil {\n\t\t\tif err.Error() == errors.New(\"Link not found\").Error() {\n\t\t\t\treturn ErrLinkNotFound\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\treturn work(link)\n\t})\n}\n\nfunc WithNetNSByPath(path string, work func() error) error {\n\tns, err := netns.GetFromPath(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn WithNetNS(ns, work)\n}\n\nfunc NSPathByPid(pid int) string {\n\treturn NSPathByPidWithProc(\"/proc\", pid)\n}\n\nfunc NSPathByPidWithProc(procPath string, pid int) string {\n\treturn filepath.Join(procPath, fmt.Sprint(pid), \"/ns/net\")\n}\n", "package net\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/coreos/go-iptables/iptables\"\n\t\"github.com/j-keck/arping\"\n\t\"github.com/vishvananda/netlink\"\n\t\"github.com/vishvananda/netns\"\n)\n\n// create and attach a veth to the Weave bridge\nfunc CreateAndAttachVeth(procPath, name, peerName, bridgeName string, mtu int, keepTXOn bool, errIfLinkExist bool, init func(peer netlink.Link) error) (*netlink.Veth, error) {\n\tbridge, err := netlink.LinkByName(bridgeName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(`bridge \"%s\" not present; did you launch weave?`, bridgeName)\n\t}\n\n\tif mtu == 0 {\n\t\tmtu = bridge.Attrs().MTU\n\t}\n\tveth := &netlink.Veth{\n\t\tLinkAttrs: netlink.LinkAttrs{\n\t\t\tName: name,\n\t\t\tMTU:  mtu},\n\t\tPeerName: peerName,\n\t}\n\n\tlinkAdd := LinkAddIfNotExist\n\tif errIfLinkExist {\n\t\tlinkAdd = netlink.LinkAdd\n\t}\n\tif err := linkAdd(veth); err != nil {\n\t\treturn nil, fmt.Errorf(`could not create veth pair %s-%s: %s`, name, peerName, err)\n\t}\n\n\tcleanup := func(format string, a ...interface{}) (*netlink.Veth, error) {\n\t\tnetlink.LinkDel(veth)\n\t\treturn nil, fmt.Errorf(format, a...)\n\t}\n\n\tbridgeType, err := ExistingBridgeType(bridgeName, DatapathName)\n\tif err != nil {\n\t\treturn cleanup(\"detect bridge type: %s\", err)\n\t}\n\tif err := bridgeType.attach(veth); err != nil {\n\t\treturn cleanup(\"attaching veth %q to %q: %s\", name, bridgeName, err)\n\t}\n\t// No ipv6 router advertisments please\n\tif err := sysctl(procPath, \"net/ipv6/conf/\"+name+\"/accept_ra\", \"0\"); err != nil {\n\t\treturn cleanup(\"setting accept_ra to 0: %s\", err)\n\t}\n\tif err := sysctl(procPath, \"net/ipv6/conf/\"+peerName+\"/accept_ra\", \"0\"); err != nil {\n\t\treturn cleanup(\"setting accept_ra to 0: %s\", err)\n\t}\n\tif !bridgeType.IsFastdp() && !keepTXOn {\n\t\tif err := EthtoolTXOff(veth.PeerName); err != nil {\n\t\t\treturn cleanup(`unable to set tx off on %q: %s`, peerName, err)\n\t\t}\n\t}\n\n\tif init != nil {\n\t\tpeer, err := netlink.LinkByName(peerName)\n\t\tif err != nil {\n\t\t\treturn cleanup(\"unable to find peer veth %s: %s\", peerName, err)\n\t\t}\n\t\tif err := init(peer); err != nil {\n\t\t\treturn cleanup(\"initializing veth: %s\", err)\n\t\t}\n\t}\n\n\tif err := netlink.LinkSetUp(veth); err != nil {\n\t\treturn cleanup(\"unable to bring veth up: %s\", err)\n\t}\n\n\treturn veth, nil\n}\n\nfunc AddAddresses(link netlink.Link, cidrs []*net.IPNet) (newAddrs []*net.IPNet, err error) {\n\texistingAddrs, err := netlink.AddrList(link, netlink.FAMILY_V4)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get IP address for %q: %v\", link.Attrs().Name, err)\n\t}\n\tfor _, ipnet := range cidrs {\n\t\tif contains(existingAddrs, ipnet) {\n\t\t\tcontinue\n\t\t}\n\t\tif err := netlink.AddrAdd(link, &netlink.Addr{IPNet: ipnet}); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to add IP address to %q: %v\", link.Attrs().Name, err)\n\t\t}\n\t\tnewAddrs = append(newAddrs, ipnet)\n\t}\n\treturn newAddrs, nil\n}\n\nfunc contains(addrs []netlink.Addr, addr *net.IPNet) bool {\n\tfor _, x := range addrs {\n\t\tif addr.IP.Equal(x.IPNet.IP) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nconst (\n\tVethName   = \"ethwe\"        // name inside container namespace\n\tvethPrefix = \"v\" + VethName // starts with \"veth\" to suppress UI notifications\n)\n\nfunc interfaceExistsInNamespace(netNSPath string, ifName string) bool {\n\terr := WithNetNSByPath(netNSPath, func() error {\n\t\t_, err := netlink.LinkByName(ifName)\n\t\treturn err\n\t})\n\treturn err == nil\n}\n\nfunc AttachContainer(netNSPath, id, ifName, bridgeName string, mtu int, withMulticastRoute bool, cidrs []*net.IPNet, keepTXOn bool, hairpinMode bool) error {\n\t// AttachContainer expects to be called in host pid namespace\n\tconst procPath = \"/proc\"\n\n\tns, err := netns.GetFromPath(netNSPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer ns.Close()\n\n\tif !interfaceExistsInNamespace(netNSPath, ifName) {\n\t\tmaxIDLen := IFNAMSIZ - 1 - len(vethPrefix+\"pl\")\n\t\tif len(id) > maxIDLen {\n\t\t\tid = id[:maxIDLen] // trim passed ID if too long\n\t\t}\n\t\tname, peerName := vethPrefix+\"pl\"+id, vethPrefix+\"pg\"+id\n\t\tveth, err := CreateAndAttachVeth(procPath, name, peerName, bridgeName, mtu, keepTXOn, true, func(veth netlink.Link) error {\n\t\t\tif err := netlink.LinkSetNsFd(veth, int(ns)); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to move veth to container netns: %s\", err)\n\t\t\t}\n\t\t\tif err := WithNetNS(ns, func() error {\n\t\t\t\treturn setupIface(procPath, peerName, ifName)\n\t\t\t}); err != nil {\n\t\t\t\treturn fmt.Errorf(\"error setting up interface: %s\", err)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err = netlink.LinkSetHairpin(veth, hairpinMode); err != nil {\n\t\t\treturn fmt.Errorf(\"unable to set hairpin mode to %t for bridge side of veth %s: %s\", hairpinMode, name, err)\n\t\t}\n\n\t}\n\n\tif err := WithNetNSLink(ns, ifName, func(veth netlink.Link) error {\n\t\treturn setupIfaceAddrs(veth, withMulticastRoute, cidrs)\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"error setting up interface addresses: %s\", err)\n\t}\n\treturn nil\n}\n\n// setupIfaceAddrs expects to be called in the container's netns\nfunc setupIfaceAddrs(veth netlink.Link, withMulticastRoute bool, cidrs []*net.IPNet) error {\n\tnewAddresses, err := AddAddresses(veth, cidrs)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tifName := veth.Attrs().Name\n\tipt, err := iptables.New()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Add multicast ACCEPT rules for new subnets\n\tfor _, ipnet := range newAddresses {\n\t\tacceptRule := []string{\"-i\", ifName, \"-s\", subnet(ipnet), \"-d\", \"224.0.0.0/4\", \"-j\", \"ACCEPT\"}\n\t\texists, err := ipt.Exists(\"filter\", \"INPUT\", acceptRule...)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif !exists {\n\t\t\tif err := ipt.Insert(\"filter\", \"INPUT\", 1, acceptRule...); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := netlink.LinkSetUp(veth); err != nil {\n\t\treturn err\n\t}\n\tfor _, ipnet := range newAddresses {\n\t\t// If we don't wait for a bit here, we see the arp fail to reach the bridge.\n\t\ttime.Sleep(1 * time.Millisecond)\n\t\tarping.GratuitousArpOverIfaceByName(ipnet.IP, ifName)\n\t}\n\tif withMulticastRoute {\n\t\t/* Route multicast packets across the weave network.\n\t\tThis must come last in 'attach'. If you change this, change weavewait to match.\n\n\t\tTODO: Add the MTU lock to prevent PMTU discovery for multicast\n\t\tdestinations. Without that, the kernel sets the DF flag on\n\t\tmulticast packets. Since RFC1122 prohibits sending of ICMP\n\t\terrors for packets with multicast destinations, that causes\n\t\tpackets larger than the PMTU to be dropped silently.  */\n\n\t\t_, multicast, _ := net.ParseCIDR(\"224.0.0.0/4\")\n\t\tif err := AddRoute(veth, netlink.SCOPE_LINK, multicast, nil); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// setupIface expects to be called in the container's netns\nfunc setupIface(procPath, ifaceName, newIfName string) error {\n\tipt, err := iptables.New()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlink, err := netlink.LinkByName(ifaceName)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := netlink.LinkSetName(link, newIfName); err != nil {\n\t\treturn err\n\t}\n\tif err := configureARPCache(procPath, newIfName); err != nil {\n\t\treturn err\n\t}\n\treturn ipt.Append(\"filter\", \"INPUT\", \"-i\", newIfName, \"-d\", \"224.0.0.0/4\", \"-j\", \"DROP\")\n}\n\n// configureARP is a helper for the Docker plugin which doesn't set the addresses itself\nfunc ConfigureARP(prefix, procPath string) error {\n\tlinks, err := netlink.LinkList()\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor _, link := range links {\n\t\tifName := link.Attrs().Name\n\t\tif strings.HasPrefix(ifName, prefix) {\n\t\t\tconfigureARPCache(procPath, ifName)\n\t\t\tif addrs, err := netlink.AddrList(link, netlink.FAMILY_V4); err == nil {\n\t\t\t\tfor _, addr := range addrs {\n\t\t\t\t\tarping.GratuitousArpOverIfaceByName(addr.IPNet.IP, ifName)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc DetachContainer(netNSPath, id, ifName string, cidrs []*net.IPNet) error {\n\tns, err := netns.GetFromPath(netNSPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer ns.Close()\n\n\tipt, err := iptables.New()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn WithNetNSLink(ns, ifName, func(veth netlink.Link) error {\n\t\texistingAddrs, err := netlink.AddrList(veth, netlink.FAMILY_V4)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get IP address for %q: %v\", veth.Attrs().Name, err)\n\t\t}\n\t\tfor _, ipnet := range cidrs {\n\t\t\tif !contains(existingAddrs, ipnet) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := netlink.AddrDel(veth, &netlink.Addr{IPNet: ipnet}); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to remove IP address from %q: %v\", veth.Attrs().Name, err)\n\t\t\t}\n\t\t}\n\t\taddrs, err := netlink.AddrList(veth, netlink.FAMILY_V4)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get IP address for %q: %v\", veth.Attrs().Name, err)\n\t\t}\n\n\t\t// Remove multicast ACCEPT rules for subnets we no longer have addresses in\n\t\tsubnets := subnets(addrs)\n\t\trules, err := ipt.List(\"filter\", \"INPUT\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, rule := range rules {\n\t\t\tps := strings.Split(rule, \" \")\n\t\t\tif len(ps) == 10 &&\n\t\t\t\tps[0] == \"-A\" && ps[2] == \"-s\" && ps[4] == \"-d\" && ps[5] == \"224.0.0.0/4\" &&\n\t\t\t\tps[6] == \"-i\" && ps[7] == ifName && ps[8] == \"-j\" && ps[9] == \"ACCEPT\" {\n\n\t\t\t\tif _, found := subnets[ps[3]]; !found {\n\t\t\t\t\tif err := ipt.Delete(\"filter\", \"INPUT\", ps[2:]...); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif len(addrs) == 0 { // all addresses gone: remove the interface\n\t\t\tif err := ipt.Delete(\"filter\", \"INPUT\", \"-i\", ifName, \"-d\", \"224.0.0.0/4\", \"-j\", \"DROP\"); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := netlink.LinkDel(veth); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n}\n\nfunc subnet(ipn *net.IPNet) string {\n\tones, _ := ipn.Mask.Size()\n\treturn fmt.Sprintf(\"%s/%d\", ipn.IP.Mask(ipn.Mask).String(), ones)\n}\n\nfunc subnets(addrs []netlink.Addr) map[string]struct{} {\n\tsubnets := make(map[string]struct{})\n\tfor _, addr := range addrs {\n\t\tsubnets[subnet(addr.IPNet)] = struct{}{}\n\t}\n\treturn subnets\n}\n", "package plugin\n\nimport (\n\t\"fmt\"\n\t\"strconv\"\n\t\"sync\"\n\n\t\"github.com/docker/libnetwork/drivers/remote/api\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/types\"\n\t\"golang.org/x/sys/unix\"\n\n\t\"github.com/vishvananda/netlink\"\n\tweaveapi \"github.com/weaveworks/weave/api\"\n\t\"github.com/weaveworks/weave/common\"\n\t\"github.com/weaveworks/weave/common/docker\"\n\tweavenet \"github.com/weaveworks/weave/net\"\n\t\"github.com/weaveworks/weave/plugin/skel\"\n)\n\nconst (\n\tMulticastOption = \"works.weave.multicast\"\n)\n\ntype network struct {\n\tisOurs            bool\n\thasMulticastRoute bool\n}\n\ntype driver struct {\n\tsync.RWMutex\n\tname  string\n\tscope string\n\t// Docker API is not available for plugin-v2\n\tdocker     *docker.Client\n\tdns        bool\n\tisPluginV2 bool\n\t// Enable multicast regardless whether multicast opt is passed to Docker;\n\t// used only by plugin-v2\n\tforceMulticast bool\n\tnetworks       map[string]network\n\tprocPath       string\n}\n\nfunc New(client *docker.Client, weave *weaveapi.Client, name, scope string, dns, isPluginV2, forceMulticast bool, procPath string) (skel.Driver, error) {\n\tdriver := &driver{\n\t\tname:       name,\n\t\tscope:      scope,\n\t\tdocker:     client,\n\t\tdns:        dns,\n\t\tisPluginV2: isPluginV2,\n\t\t// make sure that it's used only by plugin-v2\n\t\tforceMulticast: isPluginV2 && forceMulticast,\n\t\tnetworks:       make(map[string]network),\n\t\tprocPath:       procPath,\n\t}\n\n\t// Do not start watcher in the case of plugin v2, which prevents us from\n\t// configuring arp settings of containers.\n\tif client != nil {\n\t\t_, err := NewWatcher(client, weave, driver)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn driver, nil\n}\n\n// === protocol handlers\n\nfunc (driver *driver) GetCapabilities() (*api.GetCapabilityResponse, error) {\n\tdriver.logReq(\"GetCapabilities\", nil, \"\")\n\tvar caps = &api.GetCapabilityResponse{\n\t\tScope: driver.scope,\n\t}\n\tdriver.logRes(\"GetCapabilities\", caps)\n\treturn caps, nil\n}\n\n// In Swarm mode, CreateNetwork is called on each Swarm node when a new service\n// is created.\nfunc (driver *driver) CreateNetwork(create *api.CreateNetworkRequest) error {\n\tdriver.logReq(\"CreateNetwork\", create, create.NetworkID)\n\t_, err := driver.setupNetworkInfo(create.NetworkID, true, stringOptions(create))\n\treturn err\n}\n\n// NetworkAllocate is called on a Swarm node (master) which creates the network.\n// The returned options are passed to CreateNetwork.\nfunc (driver *driver) NetworkAllocate(alloc *api.AllocateNetworkRequest) (*api.AllocateNetworkResponse, error) {\n\tdriver.logReq(\"NetworkAllocate\", alloc, alloc.NetworkID)\n\treturn &api.AllocateNetworkResponse{Options: alloc.Options}, nil\n}\n\n// NetworkFree is called on a Swarm master node which created the network.\nfunc (driver *driver) NetworkFree(free *api.FreeNetworkRequest) (*api.FreeNetworkResponse, error) {\n\tdriver.logReq(\"NetworkFree\", free, free.NetworkID)\n\treturn nil, nil\n}\n\n// Deal with excessively-generic way the options get decoded from JSON\nfunc stringOptions(create *api.CreateNetworkRequest) map[string]string {\n\tif create.Options != nil {\n\t\tif data, found := create.Options[netlabel.GenericData]; found {\n\t\t\tif options, ok := data.(map[string]interface{}); ok {\n\t\t\t\tout := make(map[string]string, len(options))\n\t\t\t\tfor key, value := range options {\n\t\t\t\t\tif str, ok := value.(string); ok {\n\t\t\t\t\t\tout[key] = str\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn out\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// In Swarm mode, DeleteNetwork is called after a service has been removed.\nfunc (driver *driver) DeleteNetwork(delreq *api.DeleteNetworkRequest) error {\n\tdriver.logReq(\"DeleteNetwork\", delreq, delreq.NetworkID)\n\tdriver.Lock()\n\tdelete(driver.networks, delreq.NetworkID)\n\tdriver.Unlock()\n\treturn nil\n}\n\nfunc (driver *driver) CreateEndpoint(create *api.CreateEndpointRequest) (*api.CreateEndpointResponse, error) {\n\tdriver.logReq(\"CreateEndpoint\", create, create.EndpointID)\n\tcommon.Log.Debugf(\"interface %+v\", create.Interface)\n\n\tif create.Interface == nil {\n\t\treturn nil, driver.error(\"CreateEndpoint\", \"Not supported: creating an interface from within CreateEndpoint\")\n\t}\n\n\t// create veths. note we assume endpoint IDs are unique in the first 9 chars\n\tname, peerName := vethPair(create.EndpointID)\n\tif _, err := weavenet.CreateAndAttachVeth(driver.procPath, name, peerName, weavenet.WeaveBridgeName, 0, false, true, nil); err != nil {\n\t\treturn nil, driver.error(\"JoinEndpoint\", \"%s\", err)\n\t}\n\n\t// Send back the MAC address\n\tlink, _ := netlink.LinkByName(peerName)\n\tresp := &api.CreateEndpointResponse{Interface: &api.EndpointInterface{MacAddress: link.Attrs().HardwareAddr.String()}}\n\n\tdriver.logRes(\"CreateEndpoint\", resp)\n\treturn resp, nil\n}\n\nfunc (driver *driver) DeleteEndpoint(deleteReq *api.DeleteEndpointRequest) error {\n\tdriver.logReq(\"DeleteEndpoint\", deleteReq, deleteReq.EndpointID)\n\tname, _ := vethPair(deleteReq.EndpointID)\n\tveth := &netlink.Veth{LinkAttrs: netlink.LinkAttrs{Name: name}}\n\tif err := netlink.LinkDel(veth); err != nil {\n\t\t// Try again using the name construction from earlier plugin version,\n\t\t// in case user has upgraded with endpoints still extant\n\t\tveth.Name = \"vethwl\" + deleteReq.EndpointID[:5]\n\t\tif err2 := netlink.LinkDel(veth); err2 != nil {\n\t\t\t// Note we report the first error\n\t\t\tdriver.warn(\"LeaveEndpoint\", \"unable to delete veth %q: %s\", name, err)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (driver *driver) EndpointInfo(req *api.EndpointInfoRequest) (*api.EndpointInfoResponse, error) {\n\tdriver.logReq(\"EndpointInfo\", req, req.EndpointID)\n\treturn &api.EndpointInfoResponse{Value: map[string]interface{}{}}, nil\n}\n\nfunc (driver *driver) JoinEndpoint(j *api.JoinRequest) (*api.JoinResponse, error) {\n\tdriver.logReq(\"JoinEndpoint\", j, fmt.Sprintf(\"%s:%s to %s\", j.NetworkID, j.EndpointID, j.SandboxKey))\n\n\tnetwork, err := driver.findNetworkInfo(j.NetworkID)\n\tif err != nil {\n\t\treturn nil, driver.error(\"JoinEndpoint\", \"unable to find network info: %s\", err)\n\t}\n\n\t_, peerName := vethPair(j.EndpointID)\n\tresponse := &api.JoinResponse{\n\t\tInterfaceName: &api.InterfaceName{\n\t\t\tSrcName:   peerName,\n\t\t\tDstPrefix: weavenet.VethName,\n\t\t},\n\t}\n\tif network.hasMulticastRoute {\n\t\tmulticastRoute := api.StaticRoute{\n\t\t\tDestination: \"224.0.0.0/4\",\n\t\t\tRouteType:   types.CONNECTED,\n\t\t}\n\t\tresponse.StaticRoutes = append(response.StaticRoutes, multicastRoute)\n\t}\n\tdriver.logRes(\"JoinEndpoint\", response)\n\treturn response, nil\n}\n\nfunc (driver *driver) findNetworkInfo(id string) (network, error) {\n\tvar network network\n\n\t// plugin-v2 does not have access to docker.sock, so we cannot call Docker\n\t// API for the network info.\n\tif driver.isPluginV2 {\n\t\t// safe to set, as isOurs used only by the watcher which is disabled for plugin-v2\n\t\tnetwork.isOurs = true\n\t\tnetwork.hasMulticastRoute = driver.forceMulticast\n\n\t\treturn network, nil\n\t}\n\n\tdriver.Lock()\n\tnetwork, found := driver.networks[id]\n\tdriver.Unlock()\n\tif found {\n\t\treturn network, nil\n\t}\n\n\tif driver.docker == nil {\n\t\treturn network, fmt.Errorf(\"Docker client disabled; unable to get network info\")\n\t}\n\n\tinfo, err := driver.docker.NetworkInfo(id)\n\tif err != nil {\n\t\treturn network, err\n\t}\n\treturn driver.setupNetworkInfo(id, info.Driver == driver.name, info.Options)\n}\n\nfunc (driver *driver) setupNetworkInfo(id string, isOurs bool, options map[string]string) (network, error) {\n\tnetwork := network{isOurs: isOurs}\n\tif isOurs {\n\t\tfor key, value := range options {\n\t\t\tswitch key {\n\t\t\tcase MulticastOption:\n\t\t\t\tif value == \"\" { // interpret \"--opt works.weave.multicast\" as \"turn it on\"\n\t\t\t\t\tnetwork.hasMulticastRoute = true\n\t\t\t\t} else {\n\t\t\t\t\tvar err error\n\t\t\t\t\tif network.hasMulticastRoute, err = strconv.ParseBool(value); err != nil {\n\t\t\t\t\t\treturn network, fmt.Errorf(\"unrecognized value %q for option %s\", value, key)\n\t\t\t\t\t}\n\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tdriver.warn(\"setupNetworkInfo\", \"unrecognized option: %s\", key)\n\t\t\t}\n\t\t}\n\t}\n\tdriver.Lock()\n\tdriver.networks[id] = network\n\tdriver.Unlock()\n\treturn network, nil\n}\n\nfunc (driver *driver) LeaveEndpoint(leave *api.LeaveRequest) error {\n\tdriver.logReq(\"LeaveEndpoint\", leave, fmt.Sprintf(\"%s:%s\", leave.NetworkID, leave.EndpointID))\n\treturn nil\n}\n\nfunc (driver *driver) DiscoverNew(disco *api.DiscoveryNotification) error {\n\tdriver.logReq(\"DiscoverNew\", disco, \"\")\n\treturn nil\n}\n\nfunc (driver *driver) DiscoverDelete(disco *api.DiscoveryNotification) error {\n\tdriver.logReq(\"DiscoverDelete\", disco, \"\")\n\treturn nil\n}\n\nfunc vethPair(id string) (string, string) {\n\t// IFNAMSIZ is buffer length; subtract 6 for \"vethwl\" and 1 for terminating nul\n\treturn \"vethwl\" + id[:unix.IFNAMSIZ-7], \"vethwg\" + id[:unix.IFNAMSIZ-7]\n}\n\n// logging\n\nfunc (driver *driver) logReq(fun string, req interface{}, short string) {\n\tdriver.log(common.Log.Debugf, \" %+v\", fun, req)\n\tcommon.Log.Infof(\"[net] %s %s\", fun, short)\n}\n\nfunc (driver *driver) logRes(fun string, res interface{}) {\n\tdriver.log(common.Log.Debugf, \" %+v\", fun, res)\n}\n\nfunc (driver *driver) warn(fun string, format string, a ...interface{}) {\n\tdriver.log(common.Log.Warnf, \": \"+format, fun, a...)\n}\n\nfunc (driver *driver) debug(fun string, format string, a ...interface{}) {\n\tdriver.log(common.Log.Debugf, \": \"+format, fun, a...)\n}\n\nfunc (driver *driver) error(fun string, format string, a ...interface{}) error {\n\tdriver.log(common.Log.Errorf, \": \"+format, fun, a...)\n\treturn fmt.Errorf(format, a...)\n}\n\nfunc (driver *driver) log(f func(string, ...interface{}), format string, fun string, a ...interface{}) {\n\tf(\"[net] %s\"+format, append([]interface{}{fun}, a...)...)\n}\n", "package plugin\n\nimport (\n\t\"fmt\"\n\n\tweaveapi \"github.com/weaveworks/weave/api\"\n\t\"github.com/weaveworks/weave/common/docker\"\n\tweavenet \"github.com/weaveworks/weave/net\"\n)\n\ntype watcher struct {\n\tclient *docker.Client\n\tweave  *weaveapi.Client\n\tdriver *driver\n}\n\ntype Watcher interface {\n}\n\nfunc NewWatcher(client *docker.Client, weave *weaveapi.Client, driver *driver) (Watcher, error) {\n\tw := &watcher{client: client, weave: weave, driver: driver}\n\treturn w, client.AddObserver(w)\n}\n\nfunc (w *watcher) ContainerStarted(id string) {\n\tw.driver.debug(\"ContainerStarted\", \"%s\", id)\n\tinfo, err := w.client.InspectContainer(id)\n\tif err != nil {\n\t\tw.driver.warn(\"ContainerStarted\", \"error inspecting container %s: %s\", id, err)\n\t\treturn\n\t}\n\t// check that it's on our network\n\tfor _, net := range info.NetworkSettings.Networks {\n\t\tnetwork, err := w.driver.findNetworkInfo(net.NetworkID)\n\t\tif err != nil {\n\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to find network %s info: %s\", net.NetworkID, err)\n\t\t\tcontinue\n\t\t}\n\t\tif network.isOurs {\n\t\t\tif w.driver.dns {\n\t\t\t\tfqdn := fmt.Sprintf(\"%s.%s\", info.Config.Hostname, info.Config.Domainname)\n\t\t\t\tif err := w.weave.RegisterWithDNS(id, fqdn, net.IPAddress); err != nil {\n\t\t\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to register %s with weaveDNS: %s\", id, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tnetNSPath := weavenet.NSPathByPidWithProc(w.driver.procPath, info.State.Pid)\n\t\t\tif err := weavenet.WithNetNSByPath(netNSPath, func() error {\n\t\t\t\treturn weavenet.ConfigureARP(weavenet.VethName, w.driver.procPath)\n\t\t\t}); err != nil {\n\t\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to configure interfaces: %s\", err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (w *watcher) ContainerDied(id string) {\n\t// don't need to do this as WeaveDNS removes names on container died anyway\n\t// (note by the time we get this event we can't see the EndpointID)\n}\n\nfunc (w *watcher) ContainerDestroyed(id string) {}\n", "package plugin\n\nimport (\n\t\"net\"\n\t\"os\"\n\t\"path\"\n\t\"strings\"\n\n\t\"github.com/docker/libnetwork/ipamapi\"\n\tweaveapi \"github.com/weaveworks/weave/api\"\n\t\"github.com/weaveworks/weave/common\"\n\t\"github.com/weaveworks/weave/common/docker\"\n\tweavenet \"github.com/weaveworks/weave/net\"\n\tipamplugin \"github.com/weaveworks/weave/plugin/ipam\"\n\tnetplugin \"github.com/weaveworks/weave/plugin/net\"\n\t\"github.com/weaveworks/weave/plugin/skel\"\n)\n\nconst (\n\tpluginV2Name    = \"net-plugin\"\n\tdefaultNetwork  = \"weave\"\n\tMulticastOption = netplugin.MulticastOption\n)\n\nvar Log = common.Log\n\ntype Config struct {\n\tSocket            string\n\tMeshSocket        string\n\tEnable            bool\n\tEnableV2          bool\n\tEnableV2Multicast bool\n\tDNS               bool\n\tDefaultSubnet     string\n\tProcPath          string // path to reach host /proc filesystem\n}\n\ntype Plugin struct {\n\tConfig\n}\n\nfunc NewPlugin(config Config) *Plugin {\n\tif !config.Enable && !config.EnableV2 {\n\t\treturn nil\n\t}\n\tplugin := &Plugin{Config: config}\n\treturn plugin\n}\n\nfunc (plugin *Plugin) Start(weaveAPIAddr string, dockerClient *docker.Client, ready func()) {\n\tweave := weaveapi.NewClient(weaveAPIAddr, Log)\n\n\tLog.Info(\"Waiting for Weave API Server...\")\n\tweave.WaitAPIServer(30)\n\tLog.Info(\"Finished waiting for Weave API Server\")\n\n\tif err := plugin.run(dockerClient, weave, ready); err != nil {\n\t\tLog.Fatal(err)\n\t}\n}\n\nfunc (plugin *Plugin) run(dockerClient *docker.Client, weave *weaveapi.Client, ready func()) error {\n\tendChan := make(chan error, 1)\n\n\tif plugin.Socket != \"\" {\n\t\tglobalListener, err := listenAndServe(dockerClient, weave, plugin.Socket, endChan, \"global\", false, plugin.DNS, plugin.EnableV2, plugin.EnableV2Multicast, plugin.ProcPath)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer os.Remove(plugin.Socket)\n\t\tdefer globalListener.Close()\n\t}\n\tif plugin.MeshSocket != \"\" {\n\t\tmeshListener, err := listenAndServe(dockerClient, weave, plugin.MeshSocket, endChan, \"local\", true, plugin.DNS, plugin.EnableV2, plugin.EnableV2Multicast, plugin.ProcPath)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer os.Remove(plugin.MeshSocket)\n\t\tdefer meshListener.Close()\n\t\tif !plugin.EnableV2 {\n\t\t\tLog.Printf(\"Creating default %q network\", defaultNetwork)\n\t\t\toptions := map[string]interface{}{MulticastOption: \"true\"}\n\t\t\tdockerClient.EnsureNetwork(defaultNetwork, pluginNameFromAddress(plugin.MeshSocket), plugin.DefaultSubnet, options)\n\t\t}\n\t}\n\tready()\n\n\treturn <-endChan\n}\n\nfunc listenAndServe(dockerClient *docker.Client, weave *weaveapi.Client, address string, endChan chan<- error, scope string, withIpam, dns bool, isPluginV2, forceMulticast bool, procPath string) (net.Listener, error) {\n\tvar name string\n\tif isPluginV2 {\n\t\tname = pluginV2Name\n\t} else {\n\t\tname = pluginNameFromAddress(address)\n\t}\n\n\td, err := netplugin.New(dockerClient, weave, name, scope, dns, isPluginV2, forceMulticast, procPath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar i ipamapi.Ipam\n\tif withIpam {\n\t\ti = ipamplugin.NewIpam(weave)\n\t}\n\n\tlistener, err := weavenet.ListenUnixSocket(address)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tLog.Printf(\"Listening on %s for %s scope\", address, scope)\n\n\tgo func() {\n\t\tendChan <- skel.Listen(listener, d, i)\n\t}()\n\n\treturn listener, nil\n}\n\n// Take a socket address like \"/run/docker/plugins/weavemesh.sock\" and extract the plugin name \"weavemesh\"\nfunc pluginNameFromAddress(address string) string {\n\treturn strings.TrimSuffix(path.Base(address), \".sock\")\n}\n", "package main\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"net/http\"\n\t_ \"net/http/pprof\"\n\t\"os\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/gorilla/mux\"\n\t\"github.com/pkg/profile\"\n\t\"github.com/weaveworks/common/mflag\"\n\t\"github.com/weaveworks/common/mflagext\"\n\t\"github.com/weaveworks/common/signals\"\n\t\"github.com/weaveworks/mesh\"\n\n\t\"github.com/weaveworks/weave/common\"\n\t\"github.com/weaveworks/weave/common/docker\"\n\t\"github.com/weaveworks/weave/db\"\n\t\"github.com/weaveworks/weave/ipam\"\n\t\"github.com/weaveworks/weave/ipam/tracker\"\n\t\"github.com/weaveworks/weave/nameserver\"\n\tweavenet \"github.com/weaveworks/weave/net\"\n\t\"github.com/weaveworks/weave/net/address\"\n\t\"github.com/weaveworks/weave/net/ipset\"\n\t\"github.com/weaveworks/weave/plugin\"\n\tweaveproxy \"github.com/weaveworks/weave/proxy\"\n\tweave \"github.com/weaveworks/weave/router\"\n)\n\nvar version = \"unreleased\"\n\nvar Log = common.Log\n\ntype ipamConfig struct {\n\tIPRangeCIDR   string\n\tIPSubnetCIDR  string\n\tPeerCount     int\n\tMode          string\n\tObserver      bool\n\tSeedPeerNames []mesh.PeerName\n}\n\ntype dnsConfig struct {\n\tDomain        string\n\tListenAddress string\n\tTTL           int\n\tClientTimeout time.Duration\n\tResolvConf    string\n}\n\n// return the address part of the DNS listen address, without \":53\" on the end\nfunc (c dnsConfig) addressOnly() string {\n\taddr, _, err := net.SplitHostPort(c.ListenAddress)\n\tif err != nil {\n\t\tLog.Fatalf(\"Error when parsing DNS listen address %q: %s\", c.ListenAddress, err)\n\t}\n\treturn addr\n}\n\nfunc (c *ipamConfig) Enabled() bool {\n\tvar (\n\t\thasPeerCount = c.PeerCount > 0\n\t\thasMode      = c.HasMode()\n\t\thasRange     = c.IPRangeCIDR != \"\"\n\t\thasSubnet    = c.IPSubnetCIDR != \"\"\n\t)\n\tswitch {\n\tcase !(hasPeerCount || hasMode || hasRange || hasSubnet):\n\t\treturn false\n\tcase !hasRange && hasSubnet:\n\t\tLog.Fatal(\"--ipalloc-default-subnet specified without --ipalloc-range.\")\n\tcase !hasRange:\n\t\tLog.Fatal(\"--ipalloc-init specified without --ipalloc-range.\")\n\t}\n\tif hasMode {\n\t\tif err := c.parseMode(); err != nil {\n\t\t\tLog.Fatalf(\"Unable to parse --ipalloc-init: %s\", err)\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (c ipamConfig) HasMode() bool {\n\treturn len(c.Mode) > 0\n}\n\nfunc (c *ipamConfig) parseMode() error {\n\tmodeAndParam := strings.SplitN(c.Mode, \"=\", 2)\n\n\tswitch modeAndParam[0] {\n\tcase \"consensus\":\n\t\tif len(modeAndParam) == 2 {\n\t\t\tpeerCount, err := strconv.Atoi(modeAndParam[1])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"bad consensus parameter: %s\", err)\n\t\t\t}\n\t\t\tc.PeerCount = peerCount\n\t\t}\n\tcase \"seed\":\n\t\tif len(modeAndParam) != 2 {\n\t\t\treturn fmt.Errorf(\"seed mode requires peer name list\")\n\t\t}\n\t\tseedPeerNames, err := parsePeerNames(modeAndParam[1])\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"bad seed parameter: %s\", err)\n\t\t}\n\t\tc.SeedPeerNames = seedPeerNames\n\tcase \"observer\":\n\t\tif len(modeAndParam) != 1 {\n\t\t\treturn fmt.Errorf(\"observer mode takes no parameter\")\n\t\t}\n\t\tc.Observer = true\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown mode: %s\", modeAndParam[0])\n\t}\n\n\treturn nil\n}\n\nfunc getenvOrDefault(key, defaultVal string) string {\n\tif v := os.Getenv(key); v != \"\" {\n\t\treturn v\n\t}\n\treturn defaultVal\n}\n\nfunc main() {\n\tprocs := runtime.NumCPU()\n\t// packet sniffing can block an OS thread, so we need one thread\n\t// for that plus at least one more.\n\tif procs < 2 {\n\t\tprocs = 2\n\t}\n\truntime.GOMAXPROCS(procs)\n\n\tvar (\n\t\tjustVersion        bool\n\t\tjustPeerName       bool\n\t\tconfig             mesh.Config\n\t\tbridgeConfig       weavenet.BridgeConfig\n\t\tnetworkConfig      weave.NetworkConfig\n\t\tprotocolMinVersion int\n\t\tresume             bool\n\t\trouterName         string\n\t\tnickName           string\n\t\tpassword           string\n\t\tpktdebug           bool\n\t\tlogLevel           = \"info\"\n\t\tprof               string\n\t\tbufSzMB            int\n\t\tnoDiscovery        bool\n\t\thttpAddr           string\n\t\tstatusAddr         string\n\t\tmetricsAddr        string\n\t\tipamConfig         ipamConfig\n\t\tdockerAPI          string\n\t\tpeers              []string\n\t\tnoDNS              bool\n\t\tdnsConfig          dnsConfig\n\t\ttrustedSubnetStr   string\n\t\tdbPrefix           string\n\t\thostRoot           string\n\t\tprocPath           string\n\t\tdiscoveryEndpoint  string\n\t\ttoken              string\n\t\tadvertiseAddress   string\n\t\tpluginConfig       plugin.Config\n\t\tdefaultDockerHost  = getenvOrDefault(\"DOCKER_HOST\", \"unix:///var/run/docker.sock\")\n\t)\n\n\tmflag.BoolVar(&justVersion, []string{\"-version\"}, false, \"print version and exit\")\n\tmflag.BoolVar(&justPeerName, []string{\"-print-peer-name\"}, false, \"print peer name and exit\")\n\tmflag.StringVar(&config.Host, []string{\"-host\"}, \"\", \"router host\")\n\tmflag.IntVar(&config.Port, []string{\"-port\"}, mesh.Port, \"router port\")\n\tmflag.IntVar(&protocolMinVersion, []string{\"-min-protocol-version\"}, mesh.ProtocolMinVersion, \"minimum weave protocol version\")\n\tmflag.BoolVar(&resume, []string{\"-resume\"}, false, \"resume connections to previous peers\")\n\tmflag.StringVar(&bridgeConfig.WeaveBridgeName, []string{\"-weave-bridge\"}, \"weave\", \"name of weave bridge\")\n\tmflag.StringVar(&bridgeConfig.DockerBridgeName, []string{\"-docker-bridge\"}, \"\", \"name of Docker bridge\")\n\tmflag.BoolVar(&bridgeConfig.NPC, []string{\"-expect-npc\"}, false, \"set up iptables rules for npc\")\n\tmflag.StringVar(&routerName, []string{\"-name\"}, \"\", \"name of router (defaults to MAC of interface)\")\n\tmflag.StringVar(&nickName, []string{\"-nickname\"}, \"\", \"nickname of peer (defaults to hostname)\")\n\tmflag.StringVar(&password, []string{\"-password\"}, \"\", \"network password\")\n\tmflag.StringVar(&logLevel, []string{\"-log-level\"}, \"info\", \"logging level (debug, info, warning, error)\")\n\tmflag.BoolVar(&pktdebug, []string{\"-pkt-debug\"}, false, \"enable per-packet debug logging\")\n\tmflag.StringVar(&prof, []string{\"-profile\"}, \"\", \"enable profiling and write profiles to given path\")\n\tmflag.IntVar(&config.ConnLimit, []string{\"-conn-limit\"}, 200, \"connection limit (0 for unlimited)\")\n\tmflag.BoolVar(&noDiscovery, []string{\"-no-discovery\"}, false, \"disable peer discovery\")\n\tmflag.IntVar(&bufSzMB, []string{\"-bufsz\"}, 8, \"capture buffer size in MB\")\n\tmflag.IntVar(&bridgeConfig.MTU, []string{\"-mtu\"}, 0, \"MTU size\")\n\tmflag.StringVar(&httpAddr, []string{\"-http-addr\"}, \"\", \"address to bind HTTP interface to (disabled if blank, absolute path indicates unix domain socket)\")\n\tmflag.StringVar(&statusAddr, []string{\"-status-addr\"}, \"\", \"address to bind status+metrics interface to (disabled if blank, absolute path indicates unix domain socket)\")\n\tmflag.StringVar(&metricsAddr, []string{\"-metrics-addr\"}, \"\", \"address to bind metrics interface to (disabled if blank, absolute path indicates unix domain socket)\")\n\tmflag.StringVar(&ipamConfig.Mode, []string{\"-ipalloc-init\"}, \"\", \"allocator initialisation strategy (consensus, seed or observer)\")\n\tmflag.StringVar(&ipamConfig.IPRangeCIDR, []string{\"-ipalloc-range\"}, \"\", \"IP address range reserved for automatic allocation, in CIDR notation\")\n\tmflag.StringVar(&ipamConfig.IPSubnetCIDR, []string{\"-ipalloc-default-subnet\"}, \"\", \"subnet to allocate within by default, in CIDR notation\")\n\tmflag.StringVar(&dockerAPI, []string{\"-docker-api\"}, defaultDockerHost, \"Docker API endpoint\")\n\tmflag.BoolVar(&noDNS, []string{\"-no-dns\"}, false, \"disable DNS server\")\n\tmflag.StringVar(&dnsConfig.Domain, []string{\"-dns-domain\"}, nameserver.DefaultDomain, \"local domain to server requests for\")\n\tmflag.StringVar(&dnsConfig.ListenAddress, []string{\"-dns-listen-address\"}, nameserver.DefaultListenAddress, \"address to listen on for DNS requests\")\n\tmflag.IntVar(&dnsConfig.TTL, []string{\"-dns-ttl\"}, nameserver.DefaultTTL, \"TTL for DNS request from our domain\")\n\tmflag.DurationVar(&dnsConfig.ClientTimeout, []string{\"-dns-fallback-timeout\"}, nameserver.DefaultClientTimeout, \"timeout for fallback DNS requests\")\n\tmflag.StringVar(&dnsConfig.ResolvConf, []string{\"-resolv-conf\"}, \"\", \"path to resolver configuration for fallback DNS lookups\")\n\tmflag.StringVar(&bridgeConfig.DatapathName, []string{\"-datapath\"}, \"\", \"ODP datapath name\")\n\tmflag.BoolVar(&bridgeConfig.NoFastdp, []string{\"-no-fastdp\"}, false, \"Disable Fast Datapath\")\n\tmflag.BoolVar(&bridgeConfig.NoBridgedFastdp, []string{\"-no-bridged-fastdp\"}, false, \"Disable Bridged Fast Datapath\")\n\tmflag.StringVar(&trustedSubnetStr, []string{\"-trusted-subnets\"}, \"\", \"comma-separated list of trusted subnets in CIDR notation\")\n\tmflag.StringVar(&dbPrefix, []string{\"-db-prefix\"}, \"/weavedb/weave\", \"pathname/prefix of filename to store data\")\n\tmflag.StringVar(&procPath, []string{\"-proc-path\"}, \"/proc\", \"path to reach host /proc filesystem\")\n\tmflag.BoolVar(&bridgeConfig.AWSVPC, []string{\"-awsvpc\"}, false, \"use AWS VPC for routing\")\n\tmflag.StringVar(&hostRoot, []string{\"-host-root\"}, \"\", \"path to reach host filesystem\")\n\tmflag.StringVar(&discoveryEndpoint, []string{\"-peer-discovery-url\"}, \"https://cloud.weave.works/api/net\", \"url for peer discovery\")\n\tmflag.StringVar(&token, []string{\"-token\"}, \"\", \"token for peer discovery\")\n\tmflag.StringVar(&advertiseAddress, []string{\"-advertise-address\"}, \"\", \"address to advertise for peer discovery\")\n\n\tmflag.BoolVar(&pluginConfig.Enable, []string{\"-plugin\"}, false, \"enable Docker plugin (v1)\")\n\tmflag.BoolVar(&pluginConfig.EnableV2, []string{\"-plugin-v2\"}, false, \"enable Docker plugin (v2)\")\n\tmflag.BoolVar(&pluginConfig.EnableV2Multicast, []string{\"-plugin-v2-multicast\"}, false, \"enable multicast for Docker plugin (v2)\")\n\tmflag.StringVar(&pluginConfig.Socket, []string{\"-plugin-socket\"}, \"/run/docker/plugins/weave.sock\", \"plugin socket on which to listen\")\n\tmflag.StringVar(&pluginConfig.MeshSocket, []string{\"-plugin-mesh-socket\"}, \"/run/docker/plugins/weavemesh.sock\", \"plugin socket on which to listen in mesh mode\")\n\tmflag.BoolVar(&bridgeConfig.NoMasqLocal, []string{\"-no-masq-local\"}, false, \"do not SNAT external traffic sent to containers running on this node\")\n\n\tproxyConfig := newProxyConfig()\n\n\t// crude way of detecting that we probably have been started in a\n\t// container, with `weave launch` --> suppress misleading paths in\n\t// mflags error messages.\n\tif os.Args[0] == \"/home/weave/weaver\" { // matches the Dockerfile ENTRYPOINT\n\t\tos.Args[0] = \"weave\"\n\t\tmflag.CommandLine.Init(\"weave\", mflag.ExitOnError)\n\t}\n\n\tmflag.Parse()\n\n\tif justVersion {\n\t\tfmt.Printf(\"weave %s\\n\", version)\n\t\tos.Exit(0)\n\t}\n\tname := peerName(routerName, bridgeConfig.WeaveBridgeName, dbPrefix, hostRoot)\n\tif justPeerName {\n\t\tfmt.Printf(\"%s\\n\", name)\n\t\tos.Exit(0)\n\t}\n\n\tproxyConfig.DockerHost = dockerAPI\n\tif bridgeConfig.AWSVPC {\n\t\tproxyConfig.NoMulticastRoute = true\n\t\tproxyConfig.KeepTXOn = true\n\t}\n\n\tpeers = mflag.Args()\n\tif resume && len(peers) > 0 {\n\t\tLog.Fatalf(\"You must not specify an initial peer list in conjunction with --resume\")\n\t}\n\n\tcommon.SetLogLevel(logLevel)\n\tLog.Println(\"Command line options:\", options())\n\tLog.Infoln(\"weave \", version)\n\n\tif prof != \"\" {\n\t\tdefer profile.Start(profile.CPUProfile, profile.ProfilePath(prof), profile.NoShutdownHook).Stop()\n\t}\n\n\tif protocolMinVersion < mesh.ProtocolMinVersion || protocolMinVersion > mesh.ProtocolMaxVersion {\n\t\tLog.Fatalf(\"--min-protocol-version must be in range [%d,%d]\", mesh.ProtocolMinVersion, mesh.ProtocolMaxVersion)\n\t}\n\tconfig.ProtocolMinVersion = byte(protocolMinVersion)\n\n\tvar waitReady common.WaitGroup\n\n\tvar proxy *weaveproxy.Proxy\n\tvar err error\n\tif proxyConfig.Enabled {\n\t\tproxyConfig.DNSListenAddress = dnsConfig.addressOnly()\n\t\tproxyConfig.DNSDomain = dnsConfig.Domain\n\t\tif noDNS {\n\t\t\tproxyConfig.WithoutDNS = true\n\t\t}\n\t\t// Start Weave Proxy:\n\t\tproxy, err = weaveproxy.NewProxy(*proxyConfig)\n\t\tif err != nil {\n\t\t\tLog.Fatalf(\"Could not start proxy: %s\", err)\n\t\t}\n\t\tdefer proxy.Stop()\n\t\tlisteners := proxy.Listen()\n\t\tproxy.AttachExistingContainers()\n\t\tgo proxy.Serve(listeners, waitReady.Add())\n\t}\n\n\tif pktdebug {\n\t\tnetworkConfig.PacketLogging = packetLogging{}\n\t} else {\n\t\tnetworkConfig.PacketLogging = nopPacketLogging{}\n\t}\n\n\tif bridgeConfig.DockerBridgeName != \"\" {\n\t\tif setAddr, err := weavenet.EnforceAddrAssignType(bridgeConfig.DockerBridgeName); err != nil {\n\t\t\tLog.Errorf(\"While checking address assignment type of %s: %s\", bridgeConfig.DockerBridgeName, err)\n\t\t} else if setAddr {\n\t\t\tLog.Warningf(\"Setting %s MAC (mitigate https://github.com/docker/docker/issues/14908)\", bridgeConfig.DockerBridgeName)\n\t\t}\n\t}\n\n\tbridgeConfig.Mac = name.String()\n\tbridgeConfig.Port = config.Port\n\tips := ipset.New(common.LogLogger(), 0)\n\tbridgeType, err := weavenet.EnsureBridge(procPath, &bridgeConfig, Log, ips)\n\tcheckFatal(err)\n\tLog.Println(\"Bridge type is\", bridgeType)\n\n\tconfig.Password = determinePassword(password)\n\n\toverlay, injectorConsumer := createOverlay(bridgeType, bridgeConfig, config.Host, config.Port, bufSzMB, config.Password != nil)\n\tnetworkConfig.InjectorConsumer = injectorConsumer\n\n\tif injectorConsumer != nil {\n\t\tif err := weavenet.DetectHairpin(\"vethwe-bridge\", Log); err != nil {\n\t\t\tLog.Errorf(\"Setting may cause connectivity issues : %s\", err)\n\t\t\tLog.Infof(\"Hairpin mode may have been enabled by other software on this machine\")\n\t\t}\n\t}\n\n\tif nickName == \"\" {\n\t\tvar err error\n\t\tnickName, err = os.Hostname()\n\t\tcheckFatal(err)\n\t}\n\n\tconfig.TrustedSubnets = parseTrustedSubnets(trustedSubnetStr)\n\tconfig.PeerDiscovery = !noDiscovery\n\n\tif bridgeConfig.AWSVPC && len(config.Password) > 0 {\n\t\tLog.Fatalf(\"--awsvpc mode is not compatible with the --password option\")\n\t}\n\tif bridgeConfig.AWSVPC && !ipamConfig.Enabled() {\n\t\tLog.Fatalf(\"--awsvpc mode requires IPAM enabled\")\n\t}\n\tif bridgeConfig.AWSVPC && bridgeConfig.NoMasqLocal {\n\t\tLog.Fatalf(\"--awsvpc mode is not compatible with the --no-masq-local option\")\n\t}\n\n\tdb, err := db.NewBoltDB(dbPrefix)\n\tcheckFatal(err)\n\tdefer db.Close()\n\n\trouter, err := weave.NewNetworkRouter(config, networkConfig, bridgeConfig, name, nickName, overlay, db)\n\tcheckFatal(err)\n\tLog.Println(\"Our name is\", router.Ourself)\n\n\tif token != \"\" {\n\t\tvar addresses []string\n\t\tif advertiseAddress == \"\" {\n\t\t\tlocalAddrs, err := weavenet.LocalAddresses()\n\t\t\tcheckFatal(err)\n\t\t\tfor _, addr := range localAddrs {\n\t\t\t\taddresses = append(addresses, addr.IP.String())\n\t\t\t}\n\t\t} else {\n\t\t\taddresses = strings.Split(advertiseAddress, \",\")\n\t\t}\n\t\tdiscoveredPeers, count, err := peerDiscoveryUpdate(discoveryEndpoint, token, name.String(), nickName, addresses)\n\t\tcheckFatal(err)\n\t\tif !ipamConfig.HasMode() {\n\t\t\tipamConfig.PeerCount = len(peers) + count\n\t\t}\n\t\tpeers = append(peers, discoveredPeers...)\n\t} else if peers, err = router.InitialPeers(resume, peers); err != nil {\n\t\tLog.Fatal(\"Unable to get initial peer set: \", err)\n\t}\n\n\tvar dockerCli *docker.Client\n\tdockerVersion := \"none\"\n\tif dockerAPI != \"\" {\n\t\tdc, err := docker.NewClient(dockerAPI)\n\t\tif err != nil {\n\t\t\tLog.Fatal(\"Unable to start docker client: \", err)\n\t\t} else {\n\t\t\tLog.Info(dc.Info())\n\t\t}\n\t\tdockerCli = dc\n\t\tdockerVersion = dockerCli.DockerVersion()\n\t}\n\n\tcheckForUpdates(dockerVersion, router, uint(len(peers)))\n\n\tobserveContainers := func(o docker.ContainerObserver) {\n\t\tif dockerCli != nil {\n\t\t\tif err := dockerCli.AddObserver(o); err != nil {\n\t\t\t\tLog.Fatal(\"Unable to start watcher\", err)\n\t\t\t}\n\t\t}\n\t}\n\tisKnownPeer := func(name mesh.PeerName) bool {\n\t\treturn router.Peers.Fetch(name) != nil\n\t}\n\n\tvar (\n\t\tallocator     *ipam.Allocator\n\t\tdefaultSubnet address.CIDR\n\t)\n\tif ipamConfig.Enabled() {\n\t\tvar t tracker.LocalRangeTracker\n\t\tif bridgeConfig.AWSVPC {\n\t\t\tt, err = tracker.NewAWSVPCTracker(bridgeConfig.WeaveBridgeName)\n\t\t\tif err != nil {\n\t\t\t\tLog.Fatalf(\"Cannot create AWSVPC LocalRangeTracker: %s\", err)\n\t\t\t}\n\t\t} else if bridgeConfig.NoMasqLocal {\n\t\t\tt = weavenet.NewNoMasqLocalTracker(ips)\n\t\t}\n\t\tif t != nil {\n\t\t\tLog.Infof(\"Using %q LocalRangeTracker\", t)\n\t\t}\n\n\t\tpreClaims, err := findExistingAddresses(dockerCli, bridgeConfig.WeaveBridgeName)\n\t\tcheckFatal(err)\n\n\t\tallocator, defaultSubnet = createAllocator(router, ipamConfig, preClaims, db, t, isKnownPeer)\n\t\tobserveContainers(allocator)\n\n\t\tif dockerCli != nil {\n\t\t\tallContainerIDs, err := dockerCli.RunningContainerIDs()\n\t\t\tcheckFatal(err)\n\t\t\tallocator.PruneOwned(allContainerIDs)\n\t\t}\n\t}\n\n\tvar (\n\t\tns        *nameserver.Nameserver\n\t\tdnsserver *nameserver.DNSServer\n\t)\n\tif !noDNS {\n\t\tns, dnsserver = createDNSServer(dnsConfig, router.Router, isKnownPeer)\n\t\tobserveContainers(ns)\n\t\tns.Start()\n\t\tdefer ns.Stop()\n\t\tdnsserver.ActivateAndServe()\n\t\tif dockerCli != nil {\n\t\t\tpopulateDNS(ns, dockerCli, name, bridgeConfig.WeaveBridgeName)\n\t\t}\n\t\tdefer dnsserver.Stop()\n\t}\n\n\trouter.Start()\n\tif errors := router.InitiateConnections(peers, false); len(errors) > 0 {\n\t\tLog.Fatal(common.ErrorMessages(errors))\n\t}\n\tcheckFatal(router.CreateRestartSentinel())\n\n\tpluginConfig.DNS = !noDNS\n\tpluginConfig.DefaultSubnet = defaultSubnet.String()\n\tpluginConfig.ProcPath = procPath\n\tplugin := plugin.NewPlugin(pluginConfig)\n\n\t// The weave script always waits for a status call to succeed,\n\t// so there is no point in doing \"weave launch --http-addr ''\".\n\t// This is here to support stand-alone use of weaver.\n\tif httpAddr != \"\" {\n\t\tmuxRouter := mux.NewRouter()\n\t\tif allocator != nil {\n\t\t\tallocator.HandleHTTP(muxRouter, defaultSubnet, dockerCli)\n\t\t}\n\t\tif ns != nil {\n\t\t\tns.HandleHTTP(muxRouter, dockerCli)\n\t\t}\n\t\trouter.HandleHTTP(muxRouter)\n\t\tHandleHTTP(muxRouter, version, router, allocator, defaultSubnet, ns, dnsserver, proxy, plugin, &waitReady)\n\t\tHandleHTTPPeer(muxRouter, allocator, discoveryEndpoint, token, name.String())\n\t\tmuxRouter.Methods(\"GET\").Path(\"/metrics\").Handler(metricsHandler(router, allocator, ns, dnsserver))\n\t\tif proxy != nil {\n\t\t\tmuxRouter.Methods(\"GET\").Path(\"/proxyaddrs\").HandlerFunc(proxy.StatusHTTP)\n\t\t}\n\t\thttp.Handle(\"/\", common.LoggingHTTPHandler(muxRouter))\n\t\tLog.Println(\"Listening for HTTP control messages on\", httpAddr)\n\t\tgo listenAndServeHTTP(httpAddr, nil)\n\t}\n\n\tif statusAddr != \"\" {\n\t\tmuxRouter := mux.NewRouter()\n\t\tHandleHTTP(muxRouter, version, router, allocator, defaultSubnet, ns, dnsserver, proxy, plugin, &waitReady)\n\t\tmuxRouter.Methods(\"GET\").Path(\"/metrics\").Handler(metricsHandler(router, allocator, ns, dnsserver))\n\t\tstatusMux := http.NewServeMux()\n\t\tstatusMux.Handle(\"/\", muxRouter)\n\t\tLog.Println(\"Listening for status+metrics requests on\", statusAddr)\n\t\tgo listenAndServeHTTP(statusAddr, statusMux)\n\t}\n\n\tif metricsAddr != \"\" {\n\t\tmetricsMux := http.NewServeMux()\n\t\tmetricsMux.Handle(\"/metrics\", metricsHandler(router, allocator, ns, dnsserver))\n\t\tLog.Println(\"Listening for metrics requests on\", metricsAddr)\n\t\tgo listenAndServeHTTP(metricsAddr, metricsMux)\n\t}\n\n\tif plugin != nil {\n\t\tgo plugin.Start(httpAddr, dockerCli, waitReady.Add())\n\t}\n\n\tif bridgeConfig.AWSVPC {\n\t\t// Run this on its own goroutine because the allocator can block\n\t\t// We remove the default route installed by the kernel,\n\t\t// because awsvpc has installed it as well\n\t\tgo exposeForAWSVPC(allocator, defaultSubnet, bridgeConfig.WeaveBridgeName, waitReady.Add())\n\t}\n\n\tsignals.SignalHandlerLoop(common.Log, router)\n}\n\nfunc exposeForAWSVPC(alloc *ipam.Allocator, subnet address.CIDR, bridgeName string, ready func()) {\n\taddr, err := alloc.Allocate(\"weave:expose\", subnet, false, func() bool { return false })\n\tcheckFatal(err)\n\tcidr := address.MakeCIDR(subnet, addr)\n\terr = weavenet.Expose(bridgeName, cidr.IPNet(), true, false, false)\n\tcheckFatal(err)\n\tLog.Printf(\"Bridge %q exposed on address %v\", bridgeName, cidr)\n\tready()\n}\n\nfunc options() map[string]string {\n\toptions := make(map[string]string)\n\tmflag.Visit(func(f *mflag.Flag) {\n\t\tvalue := f.Value.String()\n\t\tname := canonicalName(f)\n\t\tif name == \"password\" || name == \"token\" {\n\t\t\tvalue = \"<redacted>\"\n\t\t}\n\t\toptions[name] = value\n\t})\n\treturn options\n}\n\nfunc canonicalName(f *mflag.Flag) string {\n\tfor _, n := range f.Names {\n\t\tif n[0] != '#' {\n\t\t\treturn strings.TrimLeft(n, \"#-\")\n\t\t}\n\t}\n\treturn \"\"\n}\n\ntype packetLogging struct{}\n\nfunc (packetLogging) LogPacket(msg string, key weave.PacketKey) {\n\tLog.Println(msg, key.SrcMAC, \"->\", key.DstMAC)\n}\n\nfunc (packetLogging) LogForwardPacket(msg string, key weave.ForwardPacketKey) {\n\tLog.Println(msg, key.SrcPeer, key.SrcMAC, \"->\", key.DstPeer, key.DstMAC)\n}\n\ntype nopPacketLogging struct{}\n\nfunc (nopPacketLogging) LogPacket(string, weave.PacketKey) {\n}\n\nfunc (nopPacketLogging) LogForwardPacket(string, weave.ForwardPacketKey) {\n}\n\nfunc newProxyConfig() *weaveproxy.Config {\n\tproxyConfig := weaveproxy.Config{\n\t\tImage:        getenvOrDefault(\"EXEC_IMAGE\", \"weaveworks/weaveexec\"),\n\t\tDockerBridge: getenvOrDefault(\"DOCKER_BRIDGE\", \"docker0\"),\n\t}\n\tmflag.BoolVar(&proxyConfig.Enabled, []string{\"-proxy\"}, false, \"instruct Weave Net to start its Docker proxy\")\n\tmflagext.ListVar(&proxyConfig.ListenAddrs, []string{\"H\"}, nil, \"addresses on which to listen for Docker proxy\")\n\tmflag.StringVar(&proxyConfig.HostnameFromLabel, []string{\"-hostname-from-label\"}, \"\", \"Key of container label from which to obtain the container's hostname\")\n\tmflag.StringVar(&proxyConfig.HostnameMatch, []string{\"-hostname-match\"}, \"(.*)\", \"Regexp pattern to apply on container names (e.g. '^aws-[0-9]+-(.*)$')\")\n\tmflag.StringVar(&proxyConfig.HostnameReplacement, []string{\"-hostname-replacement\"}, \"$1\", \"Expression to generate hostnames based on matches from --hostname-match (e.g. 'my-app-$1')\")\n\tmflag.BoolVar(&proxyConfig.RewriteInspect, []string{\"-rewrite-inspect\"}, false, \"Rewrite 'inspect' calls to return the weave network settings (if attached)\")\n\tmflag.BoolVar(&proxyConfig.NoDefaultIPAM, []string{\"-no-default-ipalloc\"}, false, \"proxy: do not automatically allocate addresses for containers without a WEAVE_CIDR\")\n\tmflag.BoolVar(&proxyConfig.NoRewriteHosts, []string{\"-no-rewrite-hosts\"}, false, \"proxy: do not automatically rewrite /etc/hosts. Use if you need the docker IP to remain in /etc/hosts\")\n\tmflag.StringVar(&proxyConfig.TLSConfig.CACert, []string{\"-tlscacert\"}, \"\", \"Trust certs signed only by this CA\")\n\tmflag.StringVar(&proxyConfig.TLSConfig.Cert, []string{\"-tlscert\"}, \"\", \"Path to TLS certificate file\")\n\tmflag.BoolVar(&proxyConfig.TLSConfig.Enabled, []string{\"-tls\"}, false, \"Use TLS; implied by --tlsverify\")\n\tmflag.StringVar(&proxyConfig.TLSConfig.Key, []string{\"-tlskey\"}, \"\", \"Path to TLS key file\")\n\tmflag.BoolVar(&proxyConfig.TLSConfig.Verify, []string{\"-tlsverify\"}, false, \"Use TLS and verify the remote\")\n\tmflag.BoolVar(&proxyConfig.WithoutDNS, []string{\"-without-dns\"}, false, \"proxy: instruct created containers to never use weaveDNS as their nameserver\")\n\tmflag.BoolVar(&proxyConfig.NoMulticastRoute, []string{\"-no-multicast-route\"}, false, \"proxy: do not add a multicast route via the weave interface when attaching containers\")\n\treturn &proxyConfig\n}\n\nfunc createOverlay(bridgeType weavenet.Bridge, config weavenet.BridgeConfig, host string, port int, bufSzMB int, enableEncryption bool) (weave.NetworkOverlay, weave.InjectorConsumer) {\n\toverlay := weave.NewOverlaySwitch()\n\tvar injectorConsumer weave.InjectorConsumer\n\tvar ignoreSleeve bool\n\n\tswitch {\n\tcase config.AWSVPC:\n\t\tvpc := weave.NewAWSVPC()\n\t\toverlay.Add(\"awsvpc\", vpc)\n\t\tinjectorConsumer = weave.NullInjectorConsumer{}\n\t\t// Currently, we do not support any overlay with AWSVPC\n\t\tignoreSleeve = true\n\tcase bridgeType == nil:\n\t\tinjectorConsumer = weave.NullInjectorConsumer{}\n\tcase bridgeType.IsFastdp():\n\t\tiface, err := weavenet.EnsureInterface(config.DatapathName)\n\t\tcheckFatal(err)\n\t\tfastdp, err := weave.NewFastDatapath(iface, port, enableEncryption)\n\t\tcheckFatal(err)\n\t\tinjectorConsumer = fastdp.InjectorConsumer()\n\t\toverlay.Add(\"fastdp\", fastdp.Overlay())\n\tcase !bridgeType.IsFastdp():\n\t\tiface, err := weavenet.EnsureInterface(weavenet.PcapIfName)\n\t\tcheckFatal(err)\n\t\tinjectorConsumer, err = weave.NewPcap(iface, bufSzMB*1024*1024) // bufsz flag is in MB\n\t\tcheckFatal(err)\n\t}\n\n\tif !ignoreSleeve {\n\t\tsleeve := weave.NewSleeveOverlay(host, port)\n\t\toverlay.Add(\"sleeve\", sleeve)\n\t\toverlay.SetCompatOverlay(sleeve)\n\t}\n\n\treturn overlay, injectorConsumer\n}\n\nfunc createAllocator(router *weave.NetworkRouter, config ipamConfig, preClaims []ipam.PreClaim, db db.DB, track tracker.LocalRangeTracker, isKnownPeer func(mesh.PeerName) bool) (*ipam.Allocator, address.CIDR) {\n\tipRange, err := ipam.ParseCIDRSubnet(config.IPRangeCIDR)\n\tcheckFatal(err)\n\tdefaultSubnet := ipRange\n\tif config.IPSubnetCIDR != \"\" {\n\t\tdefaultSubnet, err = ipam.ParseCIDRSubnet(config.IPSubnetCIDR)\n\t\tcheckFatal(err)\n\t\tif !ipRange.Range().Overlaps(defaultSubnet.Range()) {\n\t\t\tLog.Fatalf(\"IP address allocation default subnet %s does not overlap with allocation range %s\", defaultSubnet, ipRange)\n\t\t}\n\t}\n\n\tc := ipam.Config{\n\t\tOurName:     router.Ourself.Peer.Name,\n\t\tOurUID:      router.Ourself.Peer.UID,\n\t\tOurNickname: router.Ourself.Peer.NickName,\n\t\tSeed:        config.SeedPeerNames,\n\t\tUniverse:    ipRange,\n\t\tIsObserver:  config.Observer,\n\t\tPreClaims:   preClaims,\n\t\tQuorum:      func() uint { return determineQuorum(config.PeerCount, router) },\n\t\tDb:          db,\n\t\tIsKnownPeer: isKnownPeer,\n\t\tTracker:     track,\n\t}\n\n\tallocator := ipam.NewAllocator(c)\n\n\tgossip, err := router.NewGossip(\"IPallocation\", allocator)\n\tcheckFatal(err)\n\tallocator.SetInterfaces(gossip)\n\tallocator.Start()\n\trouter.Peers.OnGC(func(peer *mesh.Peer) { allocator.PeerGone(peer.Name) })\n\n\treturn allocator, defaultSubnet\n}\n\nfunc createDNSServer(config dnsConfig, router *mesh.Router, isKnownPeer func(mesh.PeerName) bool) (*nameserver.Nameserver, *nameserver.DNSServer) {\n\tns := nameserver.New(router.Ourself.Peer.Name, config.Domain, isKnownPeer)\n\trouter.Peers.OnGC(func(peer *mesh.Peer) { ns.PeerGone(peer.Name) })\n\tgossip, err := router.NewGossip(\"nameserver\", ns)\n\tcheckFatal(err)\n\tns.SetGossip(gossip)\n\tupstream := nameserver.NewUpstream(config.ResolvConf, config.addressOnly())\n\tdnsserver, err := nameserver.NewDNSServer(ns, config.Domain, config.ListenAddress,\n\t\tupstream, uint32(config.TTL), config.ClientTimeout)\n\tif err != nil {\n\t\tLog.Fatal(\"Unable to start dns server: \", err)\n\t}\n\tLog.Println(\"Listening for DNS queries on\", config.ListenAddress)\n\treturn ns, dnsserver\n}\n\n// Pick a quorum size based on the number of peer addresses.\nfunc determineQuorum(initPeerCountFlag int, router *weave.NetworkRouter) uint {\n\tif initPeerCountFlag > 0 {\n\t\treturn uint(initPeerCountFlag/2 + 1)\n\t}\n\n\tpeers := router.ConnectionMaker.Targets(true)\n\n\t// Guess a suitable quorum size based on the list of peer\n\t// addresses.  The peer list might or might not contain an\n\t// address for this peer, so the conservative assumption is\n\t// that it doesn't.  The list might contain multiple addresses\n\t// that resolve to the same peer, in which case the quorum\n\t// might be larger than it needs to be.  But the user can\n\t// specify it explicitly if that becomes a problem.\n\tclusterSize := uint(len(peers) + 1)\n\tquorum := clusterSize/2 + 1\n\tLog.Println(\"Assuming quorum size of\", quorum)\n\treturn quorum\n}\n\nfunc determinePassword(password string) []byte {\n\tif password == \"\" {\n\t\tpassword = os.Getenv(\"WEAVE_PASSWORD\")\n\t}\n\tif password == \"\" {\n\t\tLog.Println(\"Communication between peers is unencrypted.\")\n\t\treturn nil\n\t}\n\tLog.Println(\"Communication between peers via untrusted networks is encrypted.\")\n\treturn []byte(password)\n}\n\nfunc peerName(routerName, bridgeName, dbPrefix, hostRoot string) mesh.PeerName {\n\tif routerName == \"\" {\n\t\tiface, err := net.InterfaceByName(bridgeName)\n\t\tif err == nil {\n\t\t\trouterName = iface.HardwareAddr.String()\n\t\t} else {\n\t\t\trouterName, err = weavenet.GetSystemPeerName(dbPrefix, hostRoot)\n\t\t\tcheckFatal(err)\n\t\t}\n\t}\n\tname, err := mesh.PeerNameFromUserInput(routerName)\n\tcheckFatal(err)\n\treturn name\n}\n\nfunc parseTrustedSubnets(trustedSubnetStr string) []*net.IPNet {\n\ttrustedSubnets := []*net.IPNet{}\n\tif trustedSubnetStr == \"\" {\n\t\treturn trustedSubnets\n\t}\n\n\tfor _, subnetStr := range strings.Split(trustedSubnetStr, \",\") {\n\t\t_, subnet, err := net.ParseCIDR(subnetStr)\n\t\tif err != nil {\n\t\t\tLog.Fatal(\"Unable to parse trusted subnets: \", err)\n\t\t}\n\t\ttrustedSubnets = append(trustedSubnets, subnet)\n\t}\n\n\treturn trustedSubnets\n}\n\nfunc parsePeerNames(s string) ([]mesh.PeerName, error) {\n\tpeerNames := []mesh.PeerName{}\n\tif s == \"\" {\n\t\treturn peerNames, nil\n\t}\n\n\tfor _, peerNameStr := range strings.Split(s, \",\") {\n\t\tpeerName, err := mesh.PeerNameFromUserInput(peerNameStr)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error parsing peer names: %s\", err)\n\t\t}\n\t\tpeerNames = append(peerNames, peerName)\n\t}\n\n\treturn peerNames, nil\n}\n\nfunc listenAndServeHTTP(httpAddr string, handler http.Handler) {\n\tprotocol := \"tcp\"\n\tif strings.HasPrefix(httpAddr, \"/\") {\n\t\tos.Remove(httpAddr) // in case it's there from last time\n\t\tprotocol = \"unix\"\n\t}\n\tl, err := net.Listen(protocol, httpAddr)\n\tif err != nil {\n\t\tLog.Fatal(\"Unable to create http listener socket: \", err)\n\t}\n\terr = http.Serve(l, handler)\n\tif err != nil {\n\t\tLog.Fatal(\"Unable to create http server\", err)\n\t}\n}\n\nfunc checkFatal(e error) {\n\tif e != nil {\n\t\tLog.Fatal(e)\n\t}\n}\n"], "filenames": ["net/bridge.go", "net/netns.go", "net/veth.go", "plugin/net/driver.go", "plugin/net/watcher.go", "plugin/plugin.go", "prog/weaver/main.go"], "buggy_code_start_loc": [66, 58, 16, 42, 46, 34, 455], "buggy_code_end_loc": [403, 63, 239, 138, 54, 99, 455], "fixing_code_start_loc": [66, 58, 16, 42, 46, 35, 456], "fixing_code_end_loc": [407, 63, 248, 140, 49, 100, 457], "type": "CWE-350", "message": "In Weave Net before version 2.6.3, an attacker able to run a process as root in a container is able to respond to DNS requests from the host and thereby insert themselves as a fake service. In a cluster with an IPv4 internal network, if IPv6 is not totally disabled on the host (via ipv6.disable=1 on the kernel cmdline), it will be either unconfigured or configured on some interfaces, but it's pretty likely that ipv6 forwarding is disabled, ie /proc/sys/net/ipv6/conf//forwarding == 0. Also by default, /proc/sys/net/ipv6/conf//accept_ra == 1. The combination of these 2 sysctls means that the host accepts router advertisements and configure the IPv6 stack using them. By sending rogue router advertisements, an attacker can reconfigure the host to redirect part or all of the IPv6 traffic of the host to the attacker controlled container. Even if there was no IPv6 traffic before, if the DNS returns A (IPv4) and AAAA (IPv6) records, many HTTP libraries will try to connect via IPv6 first then fallback to IPv4, giving an opportunity to the attacker to respond. If by chance you also have on the host a vulnerability like last year's RCE in apt (CVE-2019-3462), you can now escalate to the host. Weave Net version 2.6.3 disables the accept_ra option on the veth devices that it creates.", "other": {"cve": {"id": "CVE-2020-11091", "sourceIdentifier": "security-advisories@github.com", "published": "2020-06-03T23:15:11.167", "lastModified": "2020-06-09T20:02:46.830", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In Weave Net before version 2.6.3, an attacker able to run a process as root in a container is able to respond to DNS requests from the host and thereby insert themselves as a fake service. In a cluster with an IPv4 internal network, if IPv6 is not totally disabled on the host (via ipv6.disable=1 on the kernel cmdline), it will be either unconfigured or configured on some interfaces, but it's pretty likely that ipv6 forwarding is disabled, ie /proc/sys/net/ipv6/conf//forwarding == 0. Also by default, /proc/sys/net/ipv6/conf//accept_ra == 1. The combination of these 2 sysctls means that the host accepts router advertisements and configure the IPv6 stack using them. By sending rogue router advertisements, an attacker can reconfigure the host to redirect part or all of the IPv6 traffic of the host to the attacker controlled container. Even if there was no IPv6 traffic before, if the DNS returns A (IPv4) and AAAA (IPv6) records, many HTTP libraries will try to connect via IPv6 first then fallback to IPv4, giving an opportunity to the attacker to respond. If by chance you also have on the host a vulnerability like last year's RCE in apt (CVE-2019-3462), you can now escalate to the host. Weave Net version 2.6.3 disables the accept_ra option on the veth devices that it creates."}, {"lang": "es", "value": "En Weave Net versiones anteriores a 2.6.3, un atacante capaz de ejecutar un proceso como root en un contenedor puede responder a las peticiones DNS del host y, por lo tanto, insertarse como un servicio falso. En un cl\u00faster con una red interna IPv4, si IPv6 no est\u00e1 totalmente deshabilitado en el host (por medio de ipv6.disable=1 en el cmdline del kernel), no estar\u00e1 desconfigurado o configurado en algunas interfaces, pero es muy probable que el reenv\u00edo ipv6 este deshabilitado, es decir, /proc/sys/net/ipv6/conf//forwarding == 0. Tambi\u00e9n por defecto, /proc/sys/net/ipv6/conf//accept_ra == 1. La combinaci\u00f3n de estos 2 sysctls significa que el host acepta anuncios de enrutadores y configura la pila IPv6 us\u00e1ndolos. Mediante el env\u00edo de anuncios de enrutadores maliciosos, un atacante puede reconfigurar el host para redireccionar parte o la totalidad del tr\u00e1fico IPv6 del host hacia un contenedor controlado por el atacante. Incluso si antes no hab\u00eda tr\u00e1fico IPv6, si el DNS devuelve registros A (IPv4) y AAAA (IPv6), muchas bibliotecas HTTP intentar\u00e1n conectarse primero por medio de IPv6 y luego recurrir\u00e1n a IPv4, dando una oportunidad al atacante para responder. Si por casualidad tambi\u00e9n se presentar\u00e1n en el host una vulnerabilidad como RCE del a\u00f1o pasado en apt (CVE-2019-3462), ahora puede escalar al host. Weave Net versi\u00f3n 2.6.3 deshabilita la opci\u00f3n accept_ra en los dispositivos veth que crea"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:C/C:N/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 5.8, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.3, "impactScore": 4.0}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:C/C:N/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 5.8, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.3, "impactScore": 4.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:S/C:N/I:P/A:N", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 3.5}, "baseSeverity": "LOW", "exploitabilityScore": 6.8, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-350"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:weave:weave_net:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.3", "matchCriteriaId": "7C177AA9-82EE-41B6-82E8-B83E18077046"}]}]}], "references": [{"url": "https://github.com/weaveworks/weave/commit/15f21f1899060f7716c70a8555a084e836f39a60", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/weaveworks/weave/security/advisories/GHSA-59qg-grp7-5r73", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/weaveworks/weave/commit/15f21f1899060f7716c70a8555a084e836f39a60"}}