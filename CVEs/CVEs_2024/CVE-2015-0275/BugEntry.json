{"buggy_code": ["/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n *\n * Architecture independence:\n *   Copyright (c) 2005, Bull S.A.\n *   Written by Pierre Peiffer <pierre.peiffer@bull.net>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n */\n\n/*\n * Extents support for EXT4\n *\n * TODO:\n *   - ext4*_error() should be used in some situations\n *   - analyze all BUG()/BUG_ON(), use -EIO where appropriate\n *   - smart tree reduction\n */\n\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/fiemap.h>\n#include \"ext4_jbd2.h\"\n#include \"ext4_extents.h\"\n#include \"xattr.h\"\n\n#include <trace/events/ext4.h>\n\n/*\n * used by extent splitting.\n */\n#define EXT4_EXT_MAY_ZEROOUT\t0x1  /* safe to zeroout if split fails \\\n\t\t\t\t\tdue to ENOSPC */\n#define EXT4_EXT_MARK_UNWRIT1\t0x2  /* mark first half unwritten */\n#define EXT4_EXT_MARK_UNWRIT2\t0x4  /* mark second half unwritten */\n\n#define EXT4_EXT_DATA_VALID1\t0x8  /* first half contains valid data */\n#define EXT4_EXT_DATA_VALID2\t0x10 /* second half contains valid data */\n\nstatic __le32 ext4_extent_block_csum(struct inode *inode,\n\t\t\t\t     struct ext4_extent_header *eh)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t__u32 csum;\n\n\tcsum = ext4_chksum(sbi, ei->i_csum_seed, (__u8 *)eh,\n\t\t\t   EXT4_EXTENT_TAIL_OFFSET(eh));\n\treturn cpu_to_le32(csum);\n}\n\nstatic int ext4_extent_block_csum_verify(struct inode *inode,\n\t\t\t\t\t struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!ext4_has_metadata_csum(inode->i_sb))\n\t\treturn 1;\n\n\tet = find_ext4_extent_tail(eh);\n\tif (et->et_checksum != ext4_extent_block_csum(inode, eh))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic void ext4_extent_block_csum_set(struct inode *inode,\n\t\t\t\t       struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!ext4_has_metadata_csum(inode->i_sb))\n\t\treturn;\n\n\tet = find_ext4_extent_tail(eh);\n\tet->et_checksum = ext4_extent_block_csum(inode, eh);\n}\n\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t\tstruct inode *inode,\n\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\tint split_flag,\n\t\t\t\tint flags);\n\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path **ppath,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags);\n\nstatic int ext4_find_delayed_extent(struct inode *inode,\n\t\t\t\t    struct extent_status *newes);\n\nstatic int ext4_ext_truncate_extend_restart(handle_t *handle,\n\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t    int needed)\n{\n\tint err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (handle->h_buffer_credits > needed)\n\t\treturn 0;\n\terr = ext4_journal_extend(handle, needed);\n\tif (err <= 0)\n\t\treturn err;\n\terr = ext4_truncate_restart_trans(handle, inode, needed);\n\tif (err == 0)\n\t\terr = -EAGAIN;\n\n\treturn err;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n */\nstatic int ext4_ext_get_access(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\tBUFFER_TRACE(path->p_bh, \"get_write_access\");\n\t\treturn ext4_journal_get_write_access(handle, path->p_bh);\n\t}\n\t/* path points to leaf/index in inode body */\n\t/* we use in-core data, no need to protect them */\n\treturn 0;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n *  - EIO\n */\nint __ext4_ext_dirty(const char *where, unsigned int line, handle_t *handle,\n\t\t     struct inode *inode, struct ext4_ext_path *path)\n{\n\tint err;\n\n\tWARN_ON(!rwsem_is_locked(&EXT4_I(inode)->i_data_sem));\n\tif (path->p_bh) {\n\t\text4_extent_block_csum_set(inode, ext_block_hdr(path->p_bh));\n\t\t/* path points to block */\n\t\terr = __ext4_handle_dirty_metadata(where, line, handle,\n\t\t\t\t\t\t   inode, path->p_bh);\n\t} else {\n\t\t/* path points to leaf/index in inode body */\n\t\terr = ext4_mark_inode_dirty(handle, inode);\n\t}\n\treturn err;\n}\n\nstatic ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      ext4_lblk_t block)\n{\n\tif (path) {\n\t\tint depth = path->p_depth;\n\t\tstruct ext4_extent *ex;\n\n\t\t/*\n\t\t * Try to predict block placement assuming that we are\n\t\t * filling in a file which will eventually be\n\t\t * non-sparse --- i.e., in the case of libbfd writing\n\t\t * an ELF object sections out-of-order but in a way\n\t\t * the eventually results in a contiguous object or\n\t\t * executable file, or some database extending a table\n\t\t * space file.  However, this is actually somewhat\n\t\t * non-ideal if we are writing a sparse file such as\n\t\t * qemu or KVM writing a raw image file that is going\n\t\t * to stay fairly sparse, since it will end up\n\t\t * fragmenting the file system's free space.  Maybe we\n\t\t * should have some hueristics or some way to allow\n\t\t * userspace to pass a hint to file system,\n\t\t * especially if the latter case turns out to be\n\t\t * common.\n\t\t */\n\t\tex = path[depth].p_ext;\n\t\tif (ex) {\n\t\t\text4_fsblk_t ext_pblk = ext4_ext_pblock(ex);\n\t\t\text4_lblk_t ext_block = le32_to_cpu(ex->ee_block);\n\n\t\t\tif (block > ext_block)\n\t\t\t\treturn ext_pblk + (block - ext_block);\n\t\t\telse\n\t\t\t\treturn ext_pblk - (ext_block - block);\n\t\t}\n\n\t\t/* it looks like index is empty;\n\t\t * try to find starting block from index itself */\n\t\tif (path[depth].p_bh)\n\t\t\treturn path[depth].p_bh->b_blocknr;\n\t}\n\n\t/* OK. use inode's group */\n\treturn ext4_inode_to_goal_block(inode);\n}\n\n/*\n * Allocation for a meta data block\n */\nstatic ext4_fsblk_t\next4_ext_new_meta_block(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path,\n\t\t\tstruct ext4_extent *ex, int *err, unsigned int flags)\n{\n\text4_fsblk_t goal, newblock;\n\n\tgoal = ext4_ext_find_goal(inode, path, le32_to_cpu(ex->ee_block));\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, flags,\n\t\t\t\t\tNULL, err);\n\treturn newblock;\n}\n\nstatic inline int ext4_ext_space_block(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 6)\n\t\tsize = 6;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_block_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 5)\n\t\tsize = 5;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 3)\n\t\tsize = 3;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 4)\n\t\tsize = 4;\n#endif\n\treturn size;\n}\n\nstatic inline int\next4_force_split_extent_at(handle_t *handle, struct inode *inode,\n\t\t\t   struct ext4_ext_path **ppath, ext4_lblk_t lblk,\n\t\t\t   int nofail)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tint unwritten = ext4_ext_is_unwritten(path[path->p_depth].p_ext);\n\n\treturn ext4_split_extent_at(handle, inode, ppath, lblk, unwritten ?\n\t\t\tEXT4_EXT_MARK_UNWRIT1|EXT4_EXT_MARK_UNWRIT2 : 0,\n\t\t\tEXT4_EX_NOCACHE | EXT4_GET_BLOCKS_PRE_IO |\n\t\t\t(nofail ? EXT4_GET_BLOCKS_METADATA_NOFAIL:0));\n}\n\n/*\n * Calculate the number of metadata blocks needed\n * to allocate @blocks\n * Worse case is one block per extent\n */\nint ext4_ext_calc_metadata_amount(struct inode *inode, ext4_lblk_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint idxs;\n\n\tidxs = ((inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t/ sizeof(struct ext4_extent_idx));\n\n\t/*\n\t * If the new delayed allocation block is contiguous with the\n\t * previous da block, it can share index blocks with the\n\t * previous block, so we only need to allocate a new index\n\t * block every idxs leaf blocks.  At ldxs**2 blocks, we need\n\t * an additional index block, and at ldxs**3 blocks, yet\n\t * another index blocks.\n\t */\n\tif (ei->i_da_metadata_calc_len &&\n\t    ei->i_da_metadata_calc_last_lblock+1 == lblock) {\n\t\tint num = 0;\n\n\t\tif ((ei->i_da_metadata_calc_len % idxs) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {\n\t\t\tnum++;\n\t\t\tei->i_da_metadata_calc_len = 0;\n\t\t} else\n\t\t\tei->i_da_metadata_calc_len++;\n\t\tei->i_da_metadata_calc_last_lblock++;\n\t\treturn num;\n\t}\n\n\t/*\n\t * In the worst case we need a new set of index blocks at\n\t * every level of the inode's extent tree.\n\t */\n\tei->i_da_metadata_calc_len = 1;\n\tei->i_da_metadata_calc_last_lblock = lblock;\n\treturn ext_depth(inode) + 1;\n}\n\nstatic int\next4_ext_max_entries(struct inode *inode, int depth)\n{\n\tint max;\n\n\tif (depth == ext_depth(inode)) {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_root(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_root_idx(inode, 1);\n\t} else {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_block(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_block_idx(inode, 1);\n\t}\n\n\treturn max;\n}\n\nstatic int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)\n{\n\text4_fsblk_t block = ext4_ext_pblock(ext);\n\tint len = ext4_ext_get_actual_len(ext);\n\text4_lblk_t lblock = le32_to_cpu(ext->ee_block);\n\text4_lblk_t last = lblock + len - 1;\n\n\tif (lblock > last)\n\t\treturn 0;\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, len);\n}\n\nstatic int ext4_valid_extent_idx(struct inode *inode,\n\t\t\t\tstruct ext4_extent_idx *ext_idx)\n{\n\text4_fsblk_t block = ext4_idx_pblock(ext_idx);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, 1);\n}\n\nstatic int ext4_valid_extent_entries(struct inode *inode,\n\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\tint depth)\n{\n\tunsigned short entries;\n\tif (eh->eh_entries == 0)\n\t\treturn 1;\n\n\tentries = le16_to_cpu(eh->eh_entries);\n\n\tif (depth == 0) {\n\t\t/* leaf entries */\n\t\tstruct ext4_extent *ext = EXT_FIRST_EXTENT(eh);\n\t\tstruct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;\n\t\text4_fsblk_t pblock = 0;\n\t\text4_lblk_t lblock = 0;\n\t\text4_lblk_t prev = 0;\n\t\tint len = 0;\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent(inode, ext))\n\t\t\t\treturn 0;\n\n\t\t\t/* Check for overlapping extents */\n\t\t\tlblock = le32_to_cpu(ext->ee_block);\n\t\t\tlen = ext4_ext_get_actual_len(ext);\n\t\t\tif ((lblock <= prev) && prev) {\n\t\t\t\tpblock = ext4_ext_pblock(ext);\n\t\t\t\tes->s_last_error_block = cpu_to_le64(pblock);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\text++;\n\t\t\tentries--;\n\t\t\tprev = lblock + len - 1;\n\t\t}\n\t} else {\n\t\tstruct ext4_extent_idx *ext_idx = EXT_FIRST_INDEX(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent_idx(inode, ext_idx))\n\t\t\t\treturn 0;\n\t\t\text_idx++;\n\t\t\tentries--;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __ext4_ext_check(const char *function, unsigned int line,\n\t\t\t    struct inode *inode, struct ext4_extent_header *eh,\n\t\t\t    int depth, ext4_fsblk_t pblk)\n{\n\tconst char *error_msg;\n\tint max = 0;\n\n\tif (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) {\n\t\terror_msg = \"invalid magic\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_depth) != depth)) {\n\t\terror_msg = \"unexpected eh_depth\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(eh->eh_max == 0)) {\n\t\terror_msg = \"invalid eh_max\";\n\t\tgoto corrupted;\n\t}\n\tmax = ext4_ext_max_entries(inode, depth);\n\tif (unlikely(le16_to_cpu(eh->eh_max) > max)) {\n\t\terror_msg = \"too large eh_max\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) {\n\t\terror_msg = \"invalid eh_entries\";\n\t\tgoto corrupted;\n\t}\n\tif (!ext4_valid_extent_entries(inode, eh, depth)) {\n\t\terror_msg = \"invalid extent entries\";\n\t\tgoto corrupted;\n\t}\n\t/* Verify checksum on non-root extent tree nodes */\n\tif (ext_depth(inode) != depth &&\n\t    !ext4_extent_block_csum_verify(inode, eh)) {\n\t\terror_msg = \"extent tree corrupted\";\n\t\tgoto corrupted;\n\t}\n\treturn 0;\n\ncorrupted:\n\text4_error_inode(inode, function, line, 0,\n\t\t\t \"pblk %llu bad header/extent: %s - magic %x, \"\n\t\t\t \"entries %u, max %u(%u), depth %u(%u)\",\n\t\t\t (unsigned long long) pblk, error_msg,\n\t\t\t le16_to_cpu(eh->eh_magic),\n\t\t\t le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max),\n\t\t\t max, le16_to_cpu(eh->eh_depth), depth);\n\treturn -EIO;\n}\n\n#define ext4_ext_check(inode, eh, depth, pblk)\t\t\t\\\n\t__ext4_ext_check(__func__, __LINE__, (inode), (eh), (depth), (pblk))\n\nint ext4_ext_check_inode(struct inode *inode)\n{\n\treturn ext4_ext_check(inode, ext_inode_hdr(inode), ext_depth(inode), 0);\n}\n\nstatic struct buffer_head *\n__read_extent_tree_block(const char *function, unsigned int line,\n\t\t\t struct inode *inode, ext4_fsblk_t pblk, int depth,\n\t\t\t int flags)\n{\n\tstruct buffer_head\t\t*bh;\n\tint\t\t\t\terr;\n\n\tbh = sb_getblk(inode->i_sb, pblk);\n\tif (unlikely(!bh))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!bh_uptodate_or_lock(bh)) {\n\t\ttrace_ext4_ext_load_extent(inode, pblk, _RET_IP_);\n\t\terr = bh_submit_read(bh);\n\t\tif (err < 0)\n\t\t\tgoto errout;\n\t}\n\tif (buffer_verified(bh) && !(flags & EXT4_EX_FORCE_CACHE))\n\t\treturn bh;\n\terr = __ext4_ext_check(function, line, inode,\n\t\t\t       ext_block_hdr(bh), depth, pblk);\n\tif (err)\n\t\tgoto errout;\n\tset_buffer_verified(bh);\n\t/*\n\t * If this is a leaf block, cache all of its entries\n\t */\n\tif (!(flags & EXT4_EX_NOCACHE) && depth == 0) {\n\t\tstruct ext4_extent_header *eh = ext_block_hdr(bh);\n\t\tstruct ext4_extent *ex = EXT_FIRST_EXTENT(eh);\n\t\text4_lblk_t prev = 0;\n\t\tint i;\n\n\t\tfor (i = le16_to_cpu(eh->eh_entries); i > 0; i--, ex++) {\n\t\t\tunsigned int status = EXTENT_STATUS_WRITTEN;\n\t\t\text4_lblk_t lblk = le32_to_cpu(ex->ee_block);\n\t\t\tint len = ext4_ext_get_actual_len(ex);\n\n\t\t\tif (prev && (prev != lblk))\n\t\t\t\text4_es_cache_extent(inode, prev,\n\t\t\t\t\t\t     lblk - prev, ~0,\n\t\t\t\t\t\t     EXTENT_STATUS_HOLE);\n\n\t\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\t\tstatus = EXTENT_STATUS_UNWRITTEN;\n\t\t\text4_es_cache_extent(inode, lblk, len,\n\t\t\t\t\t     ext4_ext_pblock(ex), status);\n\t\t\tprev = lblk + len;\n\t\t}\n\t}\n\treturn bh;\nerrout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n\n}\n\n#define read_extent_tree_block(inode, pblk, depth, flags)\t\t\\\n\t__read_extent_tree_block(__func__, __LINE__, (inode), (pblk),   \\\n\t\t\t\t (depth), (flags))\n\n/*\n * This function is called to cache a file's extent information in the\n * extent status tree\n */\nint ext4_ext_precache(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_ext_path *path = NULL;\n\tstruct buffer_head *bh;\n\tint i = 0, depth, ret = 0;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn 0;\t/* not an extent-mapped inode */\n\n\tdown_read(&ei->i_data_sem);\n\tdepth = ext_depth(inode);\n\n\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 1),\n\t\t       GFP_NOFS);\n\tif (path == NULL) {\n\t\tup_read(&ei->i_data_sem);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Don't cache anything if there are no external extent blocks */\n\tif (depth == 0)\n\t\tgoto out;\n\tpath[0].p_hdr = ext_inode_hdr(inode);\n\tret = ext4_ext_check(inode, path[0].p_hdr, depth, 0);\n\tif (ret)\n\t\tgoto out;\n\tpath[0].p_idx = EXT_FIRST_INDEX(path[0].p_hdr);\n\twhile (i >= 0) {\n\t\t/*\n\t\t * If this is a leaf block or we've reached the end of\n\t\t * the index block, go up\n\t\t */\n\t\tif ((i == depth) ||\n\t\t    path[i].p_idx > EXT_LAST_INDEX(path[i].p_hdr)) {\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\t\tbh = read_extent_tree_block(inode,\n\t\t\t\t\t    ext4_idx_pblock(path[i].p_idx++),\n\t\t\t\t\t    depth - i - 1,\n\t\t\t\t\t    EXT4_EX_FORCE_CACHE);\n\t\tif (IS_ERR(bh)) {\n\t\t\tret = PTR_ERR(bh);\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t\tpath[i].p_bh = bh;\n\t\tpath[i].p_hdr = ext_block_hdr(bh);\n\t\tpath[i].p_idx = EXT_FIRST_INDEX(path[i].p_hdr);\n\t}\n\text4_set_inode_state(inode, EXT4_STATE_EXT_PRECACHED);\nout:\n\tup_read(&ei->i_data_sem);\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn ret;\n}\n\n#ifdef EXT_DEBUG\nstatic void ext4_ext_show_path(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint k, l = path->p_depth;\n\n\text_debug(\"path:\");\n\tfor (k = 0; k <= l; k++, path++) {\n\t\tif (path->p_idx) {\n\t\t  ext_debug(\"  %d->%llu\", le32_to_cpu(path->p_idx->ei_block),\n\t\t\t    ext4_idx_pblock(path->p_idx));\n\t\t} else if (path->p_ext) {\n\t\t\text_debug(\"  %d:[%d]%d:%llu \",\n\t\t\t\t  le32_to_cpu(path->p_ext->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(path->p_ext),\n\t\t\t\t  ext4_ext_get_actual_len(path->p_ext),\n\t\t\t\t  ext4_ext_pblock(path->p_ext));\n\t\t} else\n\t\t\text_debug(\"  []\");\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_leaf(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex;\n\tint i;\n\n\tif (!path)\n\t\treturn;\n\n\teh = path[depth].p_hdr;\n\tex = EXT_FIRST_EXTENT(eh);\n\n\text_debug(\"Displaying leaf extents for inode %lu\\n\", inode->i_ino);\n\n\tfor (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ex++) {\n\t\text_debug(\"%d:[%d]%d:%llu \", le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t  ext4_ext_get_actual_len(ex), ext4_ext_pblock(ex));\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_move(struct inode *inode, struct ext4_ext_path *path,\n\t\t\text4_fsblk_t newblock, int level)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\n\tif (depth != level) {\n\t\tstruct ext4_extent_idx *idx;\n\t\tidx = path[level].p_idx;\n\t\twhile (idx <= EXT_MAX_INDEX(path[level].p_hdr)) {\n\t\t\text_debug(\"%d: move %d:%llu in new index %llu\\n\", level,\n\t\t\t\t\tle32_to_cpu(idx->ei_block),\n\t\t\t\t\text4_idx_pblock(idx),\n\t\t\t\t\tnewblock);\n\t\t\tidx++;\n\t\t}\n\n\t\treturn;\n\t}\n\n\tex = path[depth].p_ext;\n\twhile (ex <= EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\text_debug(\"move %d:%llu:[%d]%d in new leaf %llu\\n\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_pblock(ex),\n\t\t\t\text4_ext_is_unwritten(ex),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tnewblock);\n\t\tex++;\n\t}\n}\n\n#else\n#define ext4_ext_show_path(inode, path)\n#define ext4_ext_show_leaf(inode, path)\n#define ext4_ext_show_move(inode, path, newblock, level)\n#endif\n\nvoid ext4_ext_drop_refs(struct ext4_ext_path *path)\n{\n\tint depth, i;\n\n\tif (!path)\n\t\treturn;\n\tdepth = path->p_depth;\n\tfor (i = 0; i <= depth; i++, path++)\n\t\tif (path->p_bh) {\n\t\t\tbrelse(path->p_bh);\n\t\t\tpath->p_bh = NULL;\n\t\t}\n}\n\n/*\n * ext4_ext_binsearch_idx:\n * binary search for the closest index of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch_idx(struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent_idx *r, *l, *m;\n\n\n\text_debug(\"binsearch for %u(idx):  \", block);\n\n\tl = EXT_FIRST_INDEX(eh) + 1;\n\tr = EXT_LAST_INDEX(eh);\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ei_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ei_block),\n\t\t\t\tm, le32_to_cpu(m->ei_block),\n\t\t\t\tr, le32_to_cpu(r->ei_block));\n\t}\n\n\tpath->p_idx = l - 1;\n\text_debug(\"  -> %u->%lld \", le32_to_cpu(path->p_idx->ei_block),\n\t\t  ext4_idx_pblock(path->p_idx));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent_idx *chix, *ix;\n\t\tint k;\n\n\t\tchix = ix = EXT_FIRST_INDEX(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) {\n\t\t  if (k != 0 &&\n\t\t      le32_to_cpu(ix->ei_block) <= le32_to_cpu(ix[-1].ei_block)) {\n\t\t\t\tprintk(KERN_DEBUG \"k=%d, ix=0x%p, \"\n\t\t\t\t       \"first=0x%p\\n\", k,\n\t\t\t\t       ix, EXT_FIRST_INDEX(eh));\n\t\t\t\tprintk(KERN_DEBUG \"%u <= %u\\n\",\n\t\t\t\t       le32_to_cpu(ix->ei_block),\n\t\t\t\t       le32_to_cpu(ix[-1].ei_block));\n\t\t\t}\n\t\t\tBUG_ON(k && le32_to_cpu(ix->ei_block)\n\t\t\t\t\t   <= le32_to_cpu(ix[-1].ei_block));\n\t\t\tif (block < le32_to_cpu(ix->ei_block))\n\t\t\t\tbreak;\n\t\t\tchix = ix;\n\t\t}\n\t\tBUG_ON(chix != path->p_idx);\n\t}\n#endif\n\n}\n\n/*\n * ext4_ext_binsearch:\n * binary search for closest extent of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch(struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent *r, *l, *m;\n\n\tif (eh->eh_entries == 0) {\n\t\t/*\n\t\t * this leaf is empty:\n\t\t * we get such a leaf in split/add case\n\t\t */\n\t\treturn;\n\t}\n\n\text_debug(\"binsearch for %u:  \", block);\n\n\tl = EXT_FIRST_EXTENT(eh) + 1;\n\tr = EXT_LAST_EXTENT(eh);\n\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ee_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ee_block),\n\t\t\t\tm, le32_to_cpu(m->ee_block),\n\t\t\t\tr, le32_to_cpu(r->ee_block));\n\t}\n\n\tpath->p_ext = l - 1;\n\text_debug(\"  -> %d:%llu:[%d]%d \",\n\t\t\tle32_to_cpu(path->p_ext->ee_block),\n\t\t\text4_ext_pblock(path->p_ext),\n\t\t\text4_ext_is_unwritten(path->p_ext),\n\t\t\text4_ext_get_actual_len(path->p_ext));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent *chex, *ex;\n\t\tint k;\n\n\t\tchex = ex = EXT_FIRST_EXTENT(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) {\n\t\t\tBUG_ON(k && le32_to_cpu(ex->ee_block)\n\t\t\t\t\t  <= le32_to_cpu(ex[-1].ee_block));\n\t\t\tif (block < le32_to_cpu(ex->ee_block))\n\t\t\t\tbreak;\n\t\t\tchex = ex;\n\t\t}\n\t\tBUG_ON(chex != path->p_ext);\n\t}\n#endif\n\n}\n\nint ext4_ext_tree_init(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_extent_header *eh;\n\n\teh = ext_inode_hdr(inode);\n\teh->eh_depth = 0;\n\teh->eh_entries = 0;\n\teh->eh_magic = EXT4_EXT_MAGIC;\n\teh->eh_max = cpu_to_le16(ext4_ext_space_root(inode, 0));\n\text4_mark_inode_dirty(handle, inode);\n\treturn 0;\n}\n\nstruct ext4_ext_path *\next4_find_extent(struct inode *inode, ext4_lblk_t block,\n\t\t struct ext4_ext_path **orig_path, int flags)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct buffer_head *bh;\n\tstruct ext4_ext_path *path = orig_path ? *orig_path : NULL;\n\tshort int depth, i, ppos = 0;\n\tint ret;\n\n\teh = ext_inode_hdr(inode);\n\tdepth = ext_depth(inode);\n\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tif (depth > path[0].p_maxdepth) {\n\t\t\tkfree(path);\n\t\t\t*orig_path = path = NULL;\n\t\t}\n\t}\n\tif (!path) {\n\t\t/* account possible depth increase */\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 2),\n\t\t\t\tGFP_NOFS);\n\t\tif (unlikely(!path))\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tpath[0].p_maxdepth = depth + 1;\n\t}\n\tpath[0].p_hdr = eh;\n\tpath[0].p_bh = NULL;\n\n\ti = depth;\n\t/* walk through the tree */\n\twhile (i) {\n\t\text_debug(\"depth %d: num %d, max %d\\n\",\n\t\t\t  ppos, le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\n\t\text4_ext_binsearch_idx(inode, path + ppos, block);\n\t\tpath[ppos].p_block = ext4_idx_pblock(path[ppos].p_idx);\n\t\tpath[ppos].p_depth = i;\n\t\tpath[ppos].p_ext = NULL;\n\n\t\tbh = read_extent_tree_block(inode, path[ppos].p_block, --i,\n\t\t\t\t\t    flags);\n\t\tif (unlikely(IS_ERR(bh))) {\n\t\t\tret = PTR_ERR(bh);\n\t\t\tgoto err;\n\t\t}\n\n\t\teh = ext_block_hdr(bh);\n\t\tppos++;\n\t\tif (unlikely(ppos > depth)) {\n\t\t\tput_bh(bh);\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"ppos %d > depth %d\", ppos, depth);\n\t\t\tret = -EIO;\n\t\t\tgoto err;\n\t\t}\n\t\tpath[ppos].p_bh = bh;\n\t\tpath[ppos].p_hdr = eh;\n\t}\n\n\tpath[ppos].p_depth = i;\n\tpath[ppos].p_ext = NULL;\n\tpath[ppos].p_idx = NULL;\n\n\t/* find extent */\n\text4_ext_binsearch(inode, path + ppos, block);\n\t/* if not an empty leaf */\n\tif (path[ppos].p_ext)\n\t\tpath[ppos].p_block = ext4_ext_pblock(path[ppos].p_ext);\n\n\text4_ext_show_path(inode, path);\n\n\treturn path;\n\nerr:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tif (orig_path)\n\t\t*orig_path = NULL;\n\treturn ERR_PTR(ret);\n}\n\n/*\n * ext4_ext_insert_index:\n * insert new index [@logical;@ptr] into the block at @curp;\n * check where to insert: before @curp or after @curp\n */\nstatic int ext4_ext_insert_index(handle_t *handle, struct inode *inode,\n\t\t\t\t struct ext4_ext_path *curp,\n\t\t\t\t int logical, ext4_fsblk_t ptr)\n{\n\tstruct ext4_extent_idx *ix;\n\tint len, err;\n\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\treturn err;\n\n\tif (unlikely(logical == le32_to_cpu(curp->p_idx->ei_block))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d == ei_block %d!\",\n\t\t\t\t logical, le32_to_cpu(curp->p_idx->ei_block));\n\t\treturn -EIO;\n\t}\n\n\tif (unlikely(le16_to_cpu(curp->p_hdr->eh_entries)\n\t\t\t     >= le16_to_cpu(curp->p_hdr->eh_max))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"eh_entries %d >= eh_max %d!\",\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_entries),\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_max));\n\t\treturn -EIO;\n\t}\n\n\tif (logical > le32_to_cpu(curp->p_idx->ei_block)) {\n\t\t/* insert after */\n\t\text_debug(\"insert new index %d after: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx + 1;\n\t} else {\n\t\t/* insert before */\n\t\text_debug(\"insert new index %d before: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx;\n\t}\n\n\tlen = EXT_LAST_INDEX(curp->p_hdr) - ix + 1;\n\tBUG_ON(len < 0);\n\tif (len > 0) {\n\t\text_debug(\"insert new index %d: \"\n\t\t\t\t\"move %d indices from 0x%p to 0x%p\\n\",\n\t\t\t\tlogical, len, ix, ix + 1);\n\t\tmemmove(ix + 1, ix, len * sizeof(struct ext4_extent_idx));\n\t}\n\n\tif (unlikely(ix > EXT_MAX_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_MAX_INDEX!\");\n\t\treturn -EIO;\n\t}\n\n\tix->ei_block = cpu_to_le32(logical);\n\text4_idx_store_pblock(ix, ptr);\n\tle16_add_cpu(&curp->p_hdr->eh_entries, 1);\n\n\tif (unlikely(ix > EXT_LAST_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_LAST_INDEX!\");\n\t\treturn -EIO;\n\t}\n\n\terr = ext4_ext_dirty(handle, inode, curp);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_split:\n * inserts new subtree into the path, using free index entry\n * at depth @at:\n * - allocates all needed blocks (new leaf and all intermediate index blocks)\n * - makes decision where to split\n * - moves remaining extents and index entries (right to the split point)\n *   into the newly allocated blocks\n * - initializes subtree\n */\nstatic int ext4_ext_split(handle_t *handle, struct inode *inode,\n\t\t\t  unsigned int flags,\n\t\t\t  struct ext4_ext_path *path,\n\t\t\t  struct ext4_extent *newext, int at)\n{\n\tstruct buffer_head *bh = NULL;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tint i = at, k, m, a;\n\text4_fsblk_t newblock, oldblock;\n\t__le32 border;\n\text4_fsblk_t *ablocks = NULL; /* array of allocated blocks */\n\tint err = 0;\n\n\t/* make decision: where to split? */\n\t/* FIXME: now decision is simplest: at current extent */\n\n\t/* if current leaf will be split, then we should use\n\t * border from split point */\n\tif (unlikely(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"p_ext > EXT_MAX_EXTENT!\");\n\t\treturn -EIO;\n\t}\n\tif (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\tborder = path[depth].p_ext[1].ee_block;\n\t\text_debug(\"leaf will be split.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\t  le32_to_cpu(border));\n\t} else {\n\t\tborder = newext->ee_block;\n\t\text_debug(\"leaf will be added.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\tle32_to_cpu(border));\n\t}\n\n\t/*\n\t * If error occurs, then we break processing\n\t * and mark filesystem read-only. index won't\n\t * be inserted and tree will be in consistent\n\t * state. Next mount will repair buffers too.\n\t */\n\n\t/*\n\t * Get array to track all allocated blocks.\n\t * We need this to handle errors and free blocks\n\t * upon them.\n\t */\n\tablocks = kzalloc(sizeof(ext4_fsblk_t) * depth, GFP_NOFS);\n\tif (!ablocks)\n\t\treturn -ENOMEM;\n\n\t/* allocate all needed blocks */\n\text_debug(\"allocate %d blocks for indexes/leaf\\n\", depth - at);\n\tfor (a = 0; a < depth - at; a++) {\n\t\tnewblock = ext4_ext_new_meta_block(handle, inode, path,\n\t\t\t\t\t\t   newext, &err, flags);\n\t\tif (newblock == 0)\n\t\t\tgoto cleanup;\n\t\tablocks[a] = newblock;\n\t}\n\n\t/* initialize new leaf */\n\tnewblock = ablocks[--a];\n\tif (unlikely(newblock == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"newblock == 0!\");\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (unlikely(!bh)) {\n\t\terr = -ENOMEM;\n\t\tgoto cleanup;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err)\n\t\tgoto cleanup;\n\n\tneh = ext_block_hdr(bh);\n\tneh->eh_entries = 0;\n\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tneh->eh_depth = 0;\n\n\t/* move remainder of path[depth] to the new leaf */\n\tif (unlikely(path[depth].p_hdr->eh_entries !=\n\t\t     path[depth].p_hdr->eh_max)) {\n\t\tEXT4_ERROR_INODE(inode, \"eh_entries %d != eh_max %d!\",\n\t\t\t\t path[depth].p_hdr->eh_entries,\n\t\t\t\t path[depth].p_hdr->eh_max);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\t/* start copy from next extent */\n\tm = EXT_MAX_EXTENT(path[depth].p_hdr) - path[depth].p_ext++;\n\text4_ext_show_move(inode, path, newblock, depth);\n\tif (m) {\n\t\tstruct ext4_extent *ex;\n\t\tex = EXT_FIRST_EXTENT(neh);\n\t\tmemmove(ex, path[depth].p_ext, sizeof(struct ext4_extent) * m);\n\t\tle16_add_cpu(&neh->eh_entries, m);\n\t}\n\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto cleanup;\n\tbrelse(bh);\n\tbh = NULL;\n\n\t/* correct old leaf */\n\tif (m) {\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tle16_add_cpu(&path[depth].p_hdr->eh_entries, -m);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t}\n\n\t/* create intermediate indexes */\n\tk = depth - at - 1;\n\tif (unlikely(k < 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"k %d < 0!\", k);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tif (k)\n\t\text_debug(\"create %d intermediate indices\\n\", k);\n\t/* insert new index into current index block */\n\t/* current depth stored in i var */\n\ti = depth - 1;\n\twhile (k--) {\n\t\toldblock = newblock;\n\t\tnewblock = ablocks[--a];\n\t\tbh = sb_getblk(inode->i_sb, newblock);\n\t\tif (unlikely(!bh)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tlock_buffer(bh);\n\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\tneh = ext_block_hdr(bh);\n\t\tneh->eh_entries = cpu_to_le16(1);\n\t\tneh->eh_magic = EXT4_EXT_MAGIC;\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\t\tneh->eh_depth = cpu_to_le16(depth - i);\n\t\tfidx = EXT_FIRST_INDEX(neh);\n\t\tfidx->ei_block = border;\n\t\text4_idx_store_pblock(fidx, oldblock);\n\n\t\text_debug(\"int.index at %d (block %llu): %u -> %llu\\n\",\n\t\t\t\ti, newblock, le32_to_cpu(border), oldblock);\n\n\t\t/* move remainder of path[i] to the new index block */\n\t\tif (unlikely(EXT_MAX_INDEX(path[i].p_hdr) !=\n\t\t\t\t\tEXT_LAST_INDEX(path[i].p_hdr))) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_MAX_INDEX != EXT_LAST_INDEX ee_block %d!\",\n\t\t\t\t\t le32_to_cpu(path[i].p_ext->ee_block));\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\t/* start copy indexes */\n\t\tm = EXT_MAX_INDEX(path[i].p_hdr) - path[i].p_idx++;\n\t\text_debug(\"cur 0x%p, last 0x%p\\n\", path[i].p_idx,\n\t\t\t\tEXT_MAX_INDEX(path[i].p_hdr));\n\t\text4_ext_show_move(inode, path, newblock, i);\n\t\tif (m) {\n\t\t\tmemmove(++fidx, path[i].p_idx,\n\t\t\t\tsizeof(struct ext4_extent_idx) * m);\n\t\t\tle16_add_cpu(&neh->eh_entries, m);\n\t\t}\n\t\text4_extent_block_csum_set(inode, neh);\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tbrelse(bh);\n\t\tbh = NULL;\n\n\t\t/* correct old index */\n\t\tif (m) {\n\t\t\terr = ext4_ext_get_access(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t\tle16_add_cpu(&path[i].p_hdr->eh_entries, -m);\n\t\t\terr = ext4_ext_dirty(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t}\n\n\t\ti--;\n\t}\n\n\t/* insert new index */\n\terr = ext4_ext_insert_index(handle, inode, path + at,\n\t\t\t\t    le32_to_cpu(border), newblock);\n\ncleanup:\n\tif (bh) {\n\t\tif (buffer_locked(bh))\n\t\t\tunlock_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\tif (err) {\n\t\t/* free all allocated blocks in error case */\n\t\tfor (i = 0; i < depth; i++) {\n\t\t\tif (!ablocks[i])\n\t\t\t\tcontinue;\n\t\t\text4_free_blocks(handle, inode, NULL, ablocks[i], 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\t\t}\n\t}\n\tkfree(ablocks);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_grow_indepth:\n * implements tree growing procedure:\n * - allocates new block\n * - moves top-level data (index block or leaf) into the new block\n * - initializes new top-level, creating index that points to the\n *   just created block\n */\nstatic int ext4_ext_grow_indepth(handle_t *handle, struct inode *inode,\n\t\t\t\t unsigned int flags)\n{\n\tstruct ext4_extent_header *neh;\n\tstruct buffer_head *bh;\n\text4_fsblk_t newblock, goal = 0;\n\tstruct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;\n\tint err = 0;\n\n\t/* Try to prepend new index to old one */\n\tif (ext_depth(inode))\n\t\tgoal = ext4_idx_pblock(EXT_FIRST_INDEX(ext_inode_hdr(inode)));\n\tif (goal > le32_to_cpu(es->s_first_data_block)) {\n\t\tflags |= EXT4_MB_HINT_TRY_GOAL;\n\t\tgoal--;\n\t} else\n\t\tgoal = ext4_inode_to_goal_block(inode);\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, flags,\n\t\t\t\t\tNULL, &err);\n\tif (newblock == 0)\n\t\treturn err;\n\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (unlikely(!bh))\n\t\treturn -ENOMEM;\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err) {\n\t\tunlock_buffer(bh);\n\t\tgoto out;\n\t}\n\n\t/* move top-level index/leaf into new block */\n\tmemmove(bh->b_data, EXT4_I(inode)->i_data,\n\t\tsizeof(EXT4_I(inode)->i_data));\n\n\t/* set size of new block */\n\tneh = ext_block_hdr(bh);\n\t/* old root could have indexes or leaves\n\t * so calculate e_max right way */\n\tif (ext_depth(inode))\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\telse\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto out;\n\n\t/* Update top-level index: num,max,pointer */\n\tneh = ext_inode_hdr(inode);\n\tneh->eh_entries = cpu_to_le16(1);\n\text4_idx_store_pblock(EXT_FIRST_INDEX(neh), newblock);\n\tif (neh->eh_depth == 0) {\n\t\t/* Root extent block becomes index block */\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_root_idx(inode, 0));\n\t\tEXT_FIRST_INDEX(neh)->ei_block =\n\t\t\tEXT_FIRST_EXTENT(neh)->ee_block;\n\t}\n\text_debug(\"new root: num %d(%d), lblock %d, ptr %llu\\n\",\n\t\t  le16_to_cpu(neh->eh_entries), le16_to_cpu(neh->eh_max),\n\t\t  le32_to_cpu(EXT_FIRST_INDEX(neh)->ei_block),\n\t\t  ext4_idx_pblock(EXT_FIRST_INDEX(neh)));\n\n\tle16_add_cpu(&neh->eh_depth, 1);\n\text4_mark_inode_dirty(handle, inode);\nout:\n\tbrelse(bh);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_create_new_leaf:\n * finds empty index and adds new leaf.\n * if no free index is found, then it requests in-depth growing.\n */\nstatic int ext4_ext_create_new_leaf(handle_t *handle, struct inode *inode,\n\t\t\t\t    unsigned int mb_flags,\n\t\t\t\t    unsigned int gb_flags,\n\t\t\t\t    struct ext4_ext_path **ppath,\n\t\t\t\t    struct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_ext_path *curp;\n\tint depth, i, err = 0;\n\nrepeat:\n\ti = depth = ext_depth(inode);\n\n\t/* walk up to the tree and look for free index entry */\n\tcurp = path + depth;\n\twhile (i > 0 && !EXT_HAS_FREE_INDEX(curp)) {\n\t\ti--;\n\t\tcurp--;\n\t}\n\n\t/* we use already allocated block for index block,\n\t * so subsequent data blocks should be contiguous */\n\tif (EXT_HAS_FREE_INDEX(curp)) {\n\t\t/* if we found index with free entry, then use that\n\t\t * entry: create all needed subtree and add new leaf */\n\t\terr = ext4_ext_split(handle, inode, mb_flags, path, newext, i);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\tpath = ext4_find_extent(inode,\n\t\t\t\t    (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    ppath, gb_flags);\n\t\tif (IS_ERR(path))\n\t\t\terr = PTR_ERR(path);\n\t} else {\n\t\t/* tree is full, time to grow in depth */\n\t\terr = ext4_ext_grow_indepth(handle, inode, mb_flags);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\tpath = ext4_find_extent(inode,\n\t\t\t\t   (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    ppath, gb_flags);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * only first (depth 0 -> 1) produces free space;\n\t\t * in all other cases we have to split the grown tree\n\t\t */\n\t\tdepth = ext_depth(inode);\n\t\tif (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) {\n\t\t\t/* now we need to split */\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * search the closest allocated block to the left for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_left(struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\tint depth, ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_FIRST_EXTENT != ex *logical %d ee_block %d!\",\n\t\t\t\t\t *logical, le32_to_cpu(ex->ee_block));\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t  \"ix (%d) != EXT_FIRST_INDEX (%d) (depth %d)!\",\n\t\t\t\t  ix != NULL ? le32_to_cpu(ix->ei_block) : 0,\n\t\t\t\t  EXT_FIRST_INDEX(path[depth].p_hdr) != NULL ?\n\t\tle32_to_cpu(EXT_FIRST_INDEX(path[depth].p_hdr)->ei_block) : 0,\n\t\t\t\t  depth);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\t*logical = le32_to_cpu(ex->ee_block) + ee_len - 1;\n\t*phys = ext4_ext_pblock(ex) + ee_len - 1;\n\treturn 0;\n}\n\n/*\n * search the closest allocated block to the right for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the largest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t ext4_lblk_t *logical, ext4_fsblk_t *phys,\n\t\t\t\t struct ext4_extent **ret_ex)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t block;\n\tint depth;\t/* Note, NOT eh_depth; depth from top of tree */\n\tint ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"first_extent(path[%d].p_hdr) != ex\",\n\t\t\t\t\t depth);\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"ix != EXT_FIRST_INDEX *logical %d!\",\n\t\t\t\t\t\t *logical);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\tgoto found_extent;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\tif (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t/* next allocated block in this leaf */\n\t\tex++;\n\t\tgoto found_extent;\n\t}\n\n\t/* go up and search for index to the right */\n\twhile (--depth >= 0) {\n\t\tix = path[depth].p_idx;\n\t\tif (ix != EXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\tgoto got_index;\n\t}\n\n\t/* we've gone up to the root and found no index to the right */\n\treturn 0;\n\ngot_index:\n\t/* we've found index to the right, let's\n\t * follow it and find the closest allocated\n\t * block to the right */\n\tix++;\n\tblock = ext4_idx_pblock(ix);\n\twhile (++depth < path->p_depth) {\n\t\t/* subtract from p_depth to get proper eh_depth */\n\t\tbh = read_extent_tree_block(inode, block,\n\t\t\t\t\t    path->p_depth - depth, 0);\n\t\tif (IS_ERR(bh))\n\t\t\treturn PTR_ERR(bh);\n\t\teh = ext_block_hdr(bh);\n\t\tix = EXT_FIRST_INDEX(eh);\n\t\tblock = ext4_idx_pblock(ix);\n\t\tput_bh(bh);\n\t}\n\n\tbh = read_extent_tree_block(inode, block, path->p_depth - depth, 0);\n\tif (IS_ERR(bh))\n\t\treturn PTR_ERR(bh);\n\teh = ext_block_hdr(bh);\n\tex = EXT_FIRST_EXTENT(eh);\nfound_extent:\n\t*logical = le32_to_cpu(ex->ee_block);\n\t*phys = ext4_ext_pblock(ex);\n\t*ret_ex = ex;\n\tif (bh)\n\t\tput_bh(bh);\n\treturn 0;\n}\n\n/*\n * ext4_ext_next_allocated_block:\n * returns allocated block in subsequent extent or EXT_MAX_BLOCKS.\n * NOTE: it considers block number from index entry as\n * allocated block. Thus, index entries have to be consistent\n * with leaves.\n */\next4_lblk_t\next4_ext_next_allocated_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn EXT_MAX_BLOCKS;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\t/* leaf */\n\t\t\tif (path[depth].p_ext &&\n\t\t\t\tpath[depth].p_ext !=\n\t\t\t\t\tEXT_LAST_EXTENT(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_ext[1].ee_block);\n\t\t} else {\n\t\t\t/* index */\n\t\t\tif (path[depth].p_idx !=\n\t\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\t}\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_next_leaf_block:\n * returns first allocated block from next leaf or EXT_MAX_BLOCKS\n */\nstatic ext4_lblk_t ext4_ext_next_leaf_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\t/* zero-tree has no leaf blocks at all */\n\tif (depth == 0)\n\t\treturn EXT_MAX_BLOCKS;\n\n\t/* go to index block */\n\tdepth--;\n\n\twhile (depth >= 0) {\n\t\tif (path[depth].p_idx !=\n\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\treturn (ext4_lblk_t)\n\t\t\t\tle32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_correct_indexes:\n * if leaf gets modified and modified extent is first in the leaf,\n * then we have to correct all indexes above.\n * TODO: do we need to correct tree in all cases?\n */\nstatic int ext4_ext_correct_indexes(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\t__le32 border;\n\tint k, err = 0;\n\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\tif (unlikely(ex == NULL || eh == NULL)) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"ex %p == NULL or eh %p == NULL\", ex, eh);\n\t\treturn -EIO;\n\t}\n\n\tif (depth == 0) {\n\t\t/* there is no tree at all */\n\t\treturn 0;\n\t}\n\n\tif (ex != EXT_FIRST_EXTENT(eh)) {\n\t\t/* we correct tree if first leaf got modified only */\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: we need correction if border is smaller than current one\n\t */\n\tk = depth - 1;\n\tborder = path[depth].p_ext->ee_block;\n\terr = ext4_ext_get_access(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\tpath[k].p_idx->ei_block = border;\n\terr = ext4_ext_dirty(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\n\twhile (k--) {\n\t\t/* change all left-side indexes */\n\t\tif (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr))\n\t\t\tbreak;\n\t\terr = ext4_ext_get_access(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath[k].p_idx->ei_block = border;\n\t\terr = ext4_ext_dirty(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint\next4_can_extents_be_merged(struct inode *inode, struct ext4_extent *ex1,\n\t\t\t\tstruct ext4_extent *ex2)\n{\n\tunsigned short ext1_ee_len, ext2_ee_len;\n\n\tif (ext4_ext_is_unwritten(ex1) != ext4_ext_is_unwritten(ex2))\n\t\treturn 0;\n\n\text1_ee_len = ext4_ext_get_actual_len(ex1);\n\text2_ee_len = ext4_ext_get_actual_len(ex2);\n\n\tif (le32_to_cpu(ex1->ee_block) + ext1_ee_len !=\n\t\t\tle32_to_cpu(ex2->ee_block))\n\t\treturn 0;\n\n\t/*\n\t * To allow future support for preallocated extents to be added\n\t * as an RO_COMPAT feature, refuse to merge to extents if\n\t * this can result in the top bit of ee_len being set.\n\t */\n\tif (ext1_ee_len + ext2_ee_len > EXT_INIT_MAX_LEN)\n\t\treturn 0;\n\tif (ext4_ext_is_unwritten(ex1) &&\n\t    (ext4_test_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN) ||\n\t     atomic_read(&EXT4_I(inode)->i_unwritten) ||\n\t     (ext1_ee_len + ext2_ee_len > EXT_UNWRITTEN_MAX_LEN)))\n\t\treturn 0;\n#ifdef AGGRESSIVE_TEST\n\tif (ext1_ee_len >= 4)\n\t\treturn 0;\n#endif\n\n\tif (ext4_ext_pblock(ex1) + ext1_ee_len == ext4_ext_pblock(ex2))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function tries to merge the \"ex\" extent to the next extent in the tree.\n * It always tries to merge towards right. If you want to merge towards\n * left, pass \"ex - 1\" as argument instead of \"ex\".\n * Returns 0 if the extents (ex and ex+1) were _not_ merged and returns\n * 1 if they got merged.\n */\nstatic int ext4_ext_try_to_merge_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t struct ext4_extent *ex)\n{\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth, len;\n\tint merge_done = 0, unwritten;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\twhile (ex < EXT_LAST_EXTENT(eh)) {\n\t\tif (!ext4_can_extents_be_merged(inode, ex, ex + 1))\n\t\t\tbreak;\n\t\t/* merge with next extent! */\n\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t+ ext4_ext_get_actual_len(ex + 1));\n\t\tif (unwritten)\n\t\t\text4_ext_mark_unwritten(ex);\n\n\t\tif (ex + 1 < EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = (EXT_LAST_EXTENT(eh) - ex - 1)\n\t\t\t\t* sizeof(struct ext4_extent);\n\t\t\tmemmove(ex + 1, ex + 2, len);\n\t\t}\n\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\tmerge_done = 1;\n\t\tWARN_ON(eh->eh_entries == 0);\n\t\tif (!eh->eh_entries)\n\t\t\tEXT4_ERROR_INODE(inode, \"eh->eh_entries = 0!\");\n\t}\n\n\treturn merge_done;\n}\n\n/*\n * This function does a very simple check to see if we can collapse\n * an extent tree with a single extent tree leaf block into the inode.\n */\nstatic void ext4_ext_try_to_merge_up(handle_t *handle,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tsize_t s;\n\tunsigned max_root = ext4_ext_space_root(inode, 0);\n\text4_fsblk_t blk;\n\n\tif ((path[0].p_depth != 1) ||\n\t    (le16_to_cpu(path[0].p_hdr->eh_entries) != 1) ||\n\t    (le16_to_cpu(path[1].p_hdr->eh_entries) > max_root))\n\t\treturn;\n\n\t/*\n\t * We need to modify the block allocation bitmap and the block\n\t * group descriptor to release the extent tree block.  If we\n\t * can't get the journal credits, give up.\n\t */\n\tif (ext4_journal_extend(handle, 2))\n\t\treturn;\n\n\t/*\n\t * Copy the extent data up to the inode\n\t */\n\tblk = ext4_idx_pblock(path[0].p_idx);\n\ts = le16_to_cpu(path[1].p_hdr->eh_entries) *\n\t\tsizeof(struct ext4_extent_idx);\n\ts += sizeof(struct ext4_extent_header);\n\n\tpath[1].p_maxdepth = path[0].p_maxdepth;\n\tmemcpy(path[0].p_hdr, path[1].p_hdr, s);\n\tpath[0].p_depth = 0;\n\tpath[0].p_ext = EXT_FIRST_EXTENT(path[0].p_hdr) +\n\t\t(path[1].p_ext - EXT_FIRST_EXTENT(path[1].p_hdr));\n\tpath[0].p_hdr->eh_max = cpu_to_le16(max_root);\n\n\tbrelse(path[1].p_bh);\n\text4_free_blocks(handle, inode, NULL, blk, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n}\n\n/*\n * This function tries to merge the @ex extent to neighbours in the tree.\n * return 1 if merge left else 0.\n */\nstatic void ext4_ext_try_to_merge(handle_t *handle,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct ext4_ext_path *path,\n\t\t\t\t  struct ext4_extent *ex) {\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth;\n\tint merge_done = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\tif (ex > EXT_FIRST_EXTENT(eh))\n\t\tmerge_done = ext4_ext_try_to_merge_right(inode, path, ex - 1);\n\n\tif (!merge_done)\n\t\t(void) ext4_ext_try_to_merge_right(inode, path, ex);\n\n\text4_ext_try_to_merge_up(handle, inode, path);\n}\n\n/*\n * check if a portion of the \"newext\" extent overlaps with an\n * existing extent.\n *\n * If there is an overlap discovered, it updates the length of the newext\n * such that there will be no overlap, and then returns 1.\n * If there is no overlap found, it returns 0.\n */\nstatic unsigned int ext4_ext_check_overlap(struct ext4_sb_info *sbi,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_extent *newext,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\text4_lblk_t b1, b2;\n\tunsigned int depth, len1;\n\tunsigned int ret = 0;\n\n\tb1 = le32_to_cpu(newext->ee_block);\n\tlen1 = ext4_ext_get_actual_len(newext);\n\tdepth = ext_depth(inode);\n\tif (!path[depth].p_ext)\n\t\tgoto out;\n\tb2 = EXT4_LBLK_CMASK(sbi, le32_to_cpu(path[depth].p_ext->ee_block));\n\n\t/*\n\t * get the next allocated block if the extent in the path\n\t * is before the requested block(s)\n\t */\n\tif (b2 < b1) {\n\t\tb2 = ext4_ext_next_allocated_block(path);\n\t\tif (b2 == EXT_MAX_BLOCKS)\n\t\t\tgoto out;\n\t\tb2 = EXT4_LBLK_CMASK(sbi, b2);\n\t}\n\n\t/* check for wrap through zero on extent logical start block*/\n\tif (b1 + len1 < b1) {\n\t\tlen1 = EXT_MAX_BLOCKS - b1;\n\t\tnewext->ee_len = cpu_to_le16(len1);\n\t\tret = 1;\n\t}\n\n\t/* check for overlap */\n\tif (b1 + len1 > b2) {\n\t\tnewext->ee_len = cpu_to_le16(b2 - b1);\n\t\tret = 1;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * ext4_ext_insert_extent:\n * tries to merge requsted extent into the existing extent or\n * inserts requested extent as new one into the tree,\n * creating new leaf in the no-space case.\n */\nint ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\tstruct ext4_extent *newext, int gb_flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tint mb_flags = 0, unwritten;\n\n\tif (gb_flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tmb_flags |= EXT4_MB_DELALLOC_RESERVED;\n\tif (unlikely(ext4_ext_get_actual_len(newext) == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"ext4_ext_get_actual_len(newext) == 0\");\n\t\treturn -EIO;\n\t}\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(gb_flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\n\t\t/*\n\t\t * Try to see whether we should rather test the extent on\n\t\t * right from ex, or from the left of ex. This is because\n\t\t * ext4_find_extent() can return either extent on the\n\t\t * left, or on the right from the searched position. This\n\t\t * will make merging more effective.\n\t\t */\n\t\tif (ex < EXT_LAST_EXTENT(eh) &&\n\t\t    (le32_to_cpu(ex->ee_block) +\n\t\t    ext4_ext_get_actual_len(ex) <\n\t\t    le32_to_cpu(newext->ee_block))) {\n\t\t\tex += 1;\n\t\t\tgoto prepend;\n\t\t} else if ((ex > EXT_FIRST_EXTENT(eh)) &&\n\t\t\t   (le32_to_cpu(newext->ee_block) +\n\t\t\t   ext4_ext_get_actual_len(newext) <\n\t\t\t   le32_to_cpu(ex->ee_block)))\n\t\t\tex -= 1;\n\n\t\t/* Try to append newex to the ex */\n\t\tif (ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\t\text_debug(\"append [%d]%d block to %u:[%d]%d\"\n\t\t\t\t  \"(from %llu)\\n\",\n\t\t\t\t  ext4_ext_is_unwritten(newext),\n\t\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t\t  ext4_ext_pblock(ex));\n\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t  path + depth);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\t\tif (unwritten)\n\t\t\t\text4_ext_mark_unwritten(ex);\n\t\t\teh = path[depth].p_hdr;\n\t\t\tnearex = ex;\n\t\t\tgoto merge;\n\t\t}\n\nprepend:\n\t\t/* Try to prepend newex to the ex */\n\t\tif (ext4_can_extents_be_merged(inode, newext, ex)) {\n\t\t\text_debug(\"prepend %u[%d]%d block to %u:[%d]%d\"\n\t\t\t\t  \"(from %llu)\\n\",\n\t\t\t\t  le32_to_cpu(newext->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(newext),\n\t\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t\t  ext4_ext_pblock(ex));\n\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t  path + depth);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\t\tex->ee_block = newext->ee_block;\n\t\t\text4_ext_store_pblock(ex, ext4_ext_pblock(newext));\n\t\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\t\tif (unwritten)\n\t\t\t\text4_ext_mark_unwritten(ex);\n\t\t\teh = path[depth].p_hdr;\n\t\t\tnearex = ex;\n\t\t\tgoto merge;\n\t\t}\n\t}\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = EXT_MAX_BLOCKS;\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block))\n\t\tnext = ext4_ext_next_leaf_block(path);\n\tif (next != EXT_MAX_BLOCKS) {\n\t\text_debug(\"next leaf block - %u\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_find_extent(inode, next, NULL, 0);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isn't full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto has_space;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\tif (gb_flags & EXT4_GET_BLOCKS_METADATA_NOFAIL)\n\t\tmb_flags |= EXT4_MB_USE_RESERVED;\n\terr = ext4_ext_create_new_leaf(handle, inode, mb_flags, gb_flags,\n\t\t\t\t       ppath, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %u:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tnearex = EXT_FIRST_EXTENT(eh);\n\t} else {\n\t\tif (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n\t\t\t/* Insert after */\n\t\t\text_debug(\"insert %u:%llu:[%d]%d before: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t\tnearex++;\n\t\t} else {\n\t\t\t/* Insert before */\n\t\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\t\text_debug(\"insert %u:%llu:[%d]%d after: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t}\n\t\tlen = EXT_LAST_EXTENT(eh) - nearex + 1;\n\t\tif (len > 0) {\n\t\t\text_debug(\"insert %u:%llu:[%d]%d: \"\n\t\t\t\t\t\"move %d extents from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tlen, nearex, nearex + 1);\n\t\t\tmemmove(nearex + 1, nearex,\n\t\t\t\tlen * sizeof(struct ext4_extent));\n\t\t}\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tpath[depth].p_ext = nearex;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext4_ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents */\n\tif (!(gb_flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(handle, inode, path, nearex);\n\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\ncleanup:\n\text4_ext_drop_refs(npath);\n\tkfree(npath);\n\treturn err;\n}\n\nstatic int ext4_fill_fiemap_extents(struct inode *inode,\n\t\t\t\t    ext4_lblk_t block, ext4_lblk_t num,\n\t\t\t\t    struct fiemap_extent_info *fieinfo)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent *ex;\n\tstruct extent_status es;\n\text4_lblk_t next, next_del, start = 0, end = 0;\n\text4_lblk_t last = block + num;\n\tint exists, depth = 0, err = 0;\n\tunsigned int flags = 0;\n\tunsigned char blksize_bits = inode->i_sb->s_blocksize_bits;\n\n\twhile (block < last && block != EXT_MAX_BLOCKS) {\n\t\tnum = last - block;\n\t\t/* find extent for this block */\n\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\n\t\tpath = ext4_find_extent(inode, block, &path, 0);\n\t\tif (IS_ERR(path)) {\n\t\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\t\terr = PTR_ERR(path);\n\t\t\tpath = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdepth = ext_depth(inode);\n\t\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tex = path[depth].p_ext;\n\t\tnext = ext4_ext_next_allocated_block(path);\n\n\t\tflags = 0;\n\t\texists = 0;\n\t\tif (!ex) {\n\t\t\t/* there is no extent yet, so try to allocate\n\t\t\t * all requested space */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t} else if (le32_to_cpu(ex->ee_block) > block) {\n\t\t\t/* need to allocate space before found extent */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\t\t/* need to allocate space after found extent */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t\tif (end >= next)\n\t\t\t\tend = next;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)) {\n\t\t\t/*\n\t\t\t * some part of requested space is covered\n\t\t\t * by found extent\n\t\t\t */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block)\n\t\t\t\t+ ext4_ext_get_actual_len(ex);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t\texists = 1;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t\tBUG_ON(end <= start);\n\n\t\tif (!exists) {\n\t\t\tes.es_lblk = start;\n\t\t\tes.es_len = end - start;\n\t\t\tes.es_pblk = 0;\n\t\t} else {\n\t\t\tes.es_lblk = le32_to_cpu(ex->ee_block);\n\t\t\tes.es_len = ext4_ext_get_actual_len(ex);\n\t\t\tes.es_pblk = ext4_ext_pblock(ex);\n\t\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\t\tflags |= FIEMAP_EXTENT_UNWRITTEN;\n\t\t}\n\n\t\t/*\n\t\t * Find delayed extent and update es accordingly. We call\n\t\t * it even in !exists case to find out whether es is the\n\t\t * last existing extent or not.\n\t\t */\n\t\tnext_del = ext4_find_delayed_extent(inode, &es);\n\t\tif (!exists && next_del) {\n\t\t\texists = 1;\n\t\t\tflags |= (FIEMAP_EXTENT_DELALLOC |\n\t\t\t\t  FIEMAP_EXTENT_UNKNOWN);\n\t\t}\n\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\n\t\tif (unlikely(es.es_len == 0)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"es.es_len == 0\");\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * This is possible iff next == next_del == EXT_MAX_BLOCKS.\n\t\t * we need to check next == EXT_MAX_BLOCKS because it is\n\t\t * possible that an extent is with unwritten and delayed\n\t\t * status due to when an extent is delayed allocated and\n\t\t * is allocated by fallocate status tree will track both of\n\t\t * them in a extent.\n\t\t *\n\t\t * So we could return a unwritten and delayed extent, and\n\t\t * its block is equal to 'next'.\n\t\t */\n\t\tif (next == next_del && next == EXT_MAX_BLOCKS) {\n\t\t\tflags |= FIEMAP_EXTENT_LAST;\n\t\t\tif (unlikely(next_del != EXT_MAX_BLOCKS ||\n\t\t\t\t     next != EXT_MAX_BLOCKS)) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"next extent == %u, next \"\n\t\t\t\t\t\t \"delalloc extent = %u\",\n\t\t\t\t\t\t next, next_del);\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (exists) {\n\t\t\terr = fiemap_fill_next_extent(fieinfo,\n\t\t\t\t(__u64)es.es_lblk << blksize_bits,\n\t\t\t\t(__u64)es.es_pblk << blksize_bits,\n\t\t\t\t(__u64)es.es_len << blksize_bits,\n\t\t\t\tflags);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\tif (err == 1) {\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tblock = es.es_lblk + es.es_len;\n\t}\n\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn err;\n}\n\n/*\n * ext4_ext_put_gap_in_cache:\n * calculate boundaries of the gap that the requested block fits into\n * and cache this gap\n */\nstatic void\next4_ext_put_gap_in_cache(struct inode *inode, struct ext4_ext_path *path,\n\t\t\t\text4_lblk_t block)\n{\n\tint depth = ext_depth(inode);\n\text4_lblk_t len;\n\text4_lblk_t lblock;\n\tstruct ext4_extent *ex;\n\tstruct extent_status es;\n\n\tex = path[depth].p_ext;\n\tif (ex == NULL) {\n\t\t/* there is no extent yet, so gap is [0;-] */\n\t\tlblock = 0;\n\t\tlen = EXT_MAX_BLOCKS;\n\t\text_debug(\"cache gap(whole file):\");\n\t} else if (block < le32_to_cpu(ex->ee_block)) {\n\t\tlblock = block;\n\t\tlen = le32_to_cpu(ex->ee_block) - block;\n\t\text_debug(\"cache gap(before): %u [%u:%u]\",\n\t\t\t\tblock,\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\t ext4_ext_get_actual_len(ex));\n\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\text4_lblk_t next;\n\t\tlblock = le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex);\n\n\t\tnext = ext4_ext_next_allocated_block(path);\n\t\text_debug(\"cache gap(after): [%u:%u] %u\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tblock);\n\t\tBUG_ON(next == lblock);\n\t\tlen = next - lblock;\n\t} else {\n\t\tBUG();\n\t}\n\n\text4_es_find_delayed_extent_range(inode, lblock, lblock + len - 1, &es);\n\tif (es.es_len) {\n\t\t/* There's delayed extent containing lblock? */\n\t\tif (es.es_lblk <= lblock)\n\t\t\treturn;\n\t\tlen = min(es.es_lblk - lblock, len);\n\t}\n\text_debug(\" -> %u:%u\\n\", lblock, len);\n\text4_es_insert_extent(inode, lblock, len, ~0, EXTENT_STATUS_HOLE);\n}\n\n/*\n * ext4_ext_rm_idx:\n * removes index from the index block.\n */\nstatic int ext4_ext_rm_idx(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, int depth)\n{\n\tint err;\n\text4_fsblk_t leaf;\n\n\t/* free index block */\n\tdepth--;\n\tpath = path + depth;\n\tleaf = ext4_idx_pblock(path->p_idx);\n\tif (unlikely(path->p_hdr->eh_entries == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"path->p_hdr->eh_entries == 0\");\n\t\treturn -EIO;\n\t}\n\terr = ext4_ext_get_access(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\n\tif (path->p_idx != EXT_LAST_INDEX(path->p_hdr)) {\n\t\tint len = EXT_LAST_INDEX(path->p_hdr) - path->p_idx;\n\t\tlen *= sizeof(struct ext4_extent_idx);\n\t\tmemmove(path->p_idx, path->p_idx + 1, len);\n\t}\n\n\tle16_add_cpu(&path->p_hdr->eh_entries, -1);\n\terr = ext4_ext_dirty(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\text_debug(\"index is empty, remove it, free block %llu\\n\", leaf);\n\ttrace_ext4_ext_rm_idx(inode, leaf);\n\n\text4_free_blocks(handle, inode, NULL, leaf, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n\n\twhile (--depth >= 0) {\n\t\tif (path->p_idx != EXT_FIRST_INDEX(path->p_hdr))\n\t\t\tbreak;\n\t\tpath--;\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath->p_idx->ei_block = (path+1)->p_idx->ei_block;\n\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\treturn err;\n}\n\n/*\n * ext4_ext_calc_credits_for_single_extent:\n * This routine returns max. credits that needed to insert an extent\n * to the extent tree.\n * When pass the actual path, the caller should calculate credits\n * under i_data_sem.\n */\nint ext4_ext_calc_credits_for_single_extent(struct inode *inode, int nrblocks,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path) {\n\t\tint depth = ext_depth(inode);\n\t\tint ret = 0;\n\n\t\t/* probably there is space in leaf? */\n\t\tif (le16_to_cpu(path[depth].p_hdr->eh_entries)\n\t\t\t\t< le16_to_cpu(path[depth].p_hdr->eh_max)) {\n\n\t\t\t/*\n\t\t\t *  There are some space in the leaf tree, no\n\t\t\t *  need to account for leaf block credit\n\t\t\t *\n\t\t\t *  bitmaps and block group descriptor blocks\n\t\t\t *  and other metadata blocks still need to be\n\t\t\t *  accounted.\n\t\t\t */\n\t\t\t/* 1 bitmap, 1 block group descriptor */\n\t\t\tret = 2 + EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ext4_chunk_trans_blocks(inode, nrblocks);\n}\n\n/*\n * How many index/leaf blocks need to change/allocate to add @extents extents?\n *\n * If we add a single extent, then in the worse case, each tree level\n * index/leaf need to be changed in case of the tree split.\n *\n * If more extents are inserted, they could cause the whole tree split more\n * than once, but this is really rare.\n */\nint ext4_ext_index_trans_blocks(struct inode *inode, int extents)\n{\n\tint index;\n\tint depth;\n\n\t/* If we are converting the inline data, only one is needed here. */\n\tif (ext4_has_inline_data(inode))\n\t\treturn 1;\n\n\tdepth = ext_depth(inode);\n\n\tif (extents <= 1)\n\t\tindex = depth * 2;\n\telse\n\t\tindex = depth * 3;\n\n\treturn index;\n}\n\nstatic inline int get_default_free_blocks_flags(struct inode *inode)\n{\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\treturn EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET;\n\telse if (ext4_should_journal_data(inode))\n\t\treturn EXT4_FREE_BLOCKS_FORGET;\n\treturn 0;\n}\n\nstatic int ext4_remove_blocks(handle_t *handle, struct inode *inode,\n\t\t\t      struct ext4_extent *ex,\n\t\t\t      long long *partial_cluster,\n\t\t\t      ext4_lblk_t from, ext4_lblk_t to)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tunsigned short ee_len = ext4_ext_get_actual_len(ex);\n\text4_fsblk_t pblk;\n\tint flags = get_default_free_blocks_flags(inode);\n\n\t/*\n\t * For bigalloc file systems, we never free a partial cluster\n\t * at the beginning of the extent.  Instead, we make a note\n\t * that we tried freeing the cluster, and check to see if we\n\t * need to free it on a subsequent call to ext4_remove_blocks,\n\t * or at the end of ext4_ext_rm_leaf or ext4_ext_remove_space.\n\t */\n\tflags |= EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER;\n\n\ttrace_ext4_remove_blocks(inode, ex, from, to, *partial_cluster);\n\t/*\n\t * If we have a partial cluster, and it's different from the\n\t * cluster of the last block, we need to explicitly free the\n\t * partial cluster here.\n\t */\n\tpblk = ext4_ext_pblock(ex) + ee_len - 1;\n\tif (*partial_cluster > 0 &&\n\t    *partial_cluster != (long long) EXT4_B2C(sbi, pblk)) {\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(sbi, *partial_cluster),\n\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\t*partial_cluster = 0;\n\t}\n\n#ifdef EXTENTS_STATS\n\t{\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\tspin_lock(&sbi->s_ext_stats_lock);\n\t\tsbi->s_ext_blocks += ee_len;\n\t\tsbi->s_ext_extents++;\n\t\tif (ee_len < sbi->s_ext_min)\n\t\t\tsbi->s_ext_min = ee_len;\n\t\tif (ee_len > sbi->s_ext_max)\n\t\t\tsbi->s_ext_max = ee_len;\n\t\tif (ext_depth(inode) > sbi->s_depth_max)\n\t\t\tsbi->s_depth_max = ext_depth(inode);\n\t\tspin_unlock(&sbi->s_ext_stats_lock);\n\t}\n#endif\n\tif (from >= le32_to_cpu(ex->ee_block)\n\t    && to == le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\t/* tail removal */\n\t\text4_lblk_t num;\n\t\tlong long first_cluster;\n\n\t\tnum = le32_to_cpu(ex->ee_block) + ee_len - from;\n\t\tpblk = ext4_ext_pblock(ex) + ee_len - num;\n\t\t/*\n\t\t * Usually we want to free partial cluster at the end of the\n\t\t * extent, except for the situation when the cluster is still\n\t\t * used by any other extent (partial_cluster is negative).\n\t\t */\n\t\tif (*partial_cluster < 0 &&\n\t\t    *partial_cluster == -(long long) EXT4_B2C(sbi, pblk+num-1))\n\t\t\tflags |= EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER;\n\n\t\text_debug(\"free last %u blocks starting %llu partial %lld\\n\",\n\t\t\t  num, pblk, *partial_cluster);\n\t\text4_free_blocks(handle, inode, NULL, pblk, num, flags);\n\t\t/*\n\t\t * If the block range to be freed didn't start at the\n\t\t * beginning of a cluster, and we removed the entire\n\t\t * extent and the cluster is not used by any other extent,\n\t\t * save the partial cluster here, since we might need to\n\t\t * delete if we determine that the truncate or punch hole\n\t\t * operation has removed all of the blocks in the cluster.\n\t\t * If that cluster is used by another extent, preserve its\n\t\t * negative value so it isn't freed later on.\n\t\t *\n\t\t * If the whole extent wasn't freed, we've reached the\n\t\t * start of the truncated/punched region and have finished\n\t\t * removing blocks.  If there's a partial cluster here it's\n\t\t * shared with the remainder of the extent and is no longer\n\t\t * a candidate for removal.\n\t\t */\n\t\tif (EXT4_PBLK_COFF(sbi, pblk) && ee_len == num) {\n\t\t\tfirst_cluster = (long long) EXT4_B2C(sbi, pblk);\n\t\t\tif (first_cluster != -*partial_cluster)\n\t\t\t\t*partial_cluster = first_cluster;\n\t\t} else {\n\t\t\t*partial_cluster = 0;\n\t\t}\n\t} else\n\t\text4_error(sbi->s_sb, \"strange request: removal(2) \"\n\t\t\t   \"%u-%u from %u:%u\\n\",\n\t\t\t   from, to, le32_to_cpu(ex->ee_block), ee_len);\n\treturn 0;\n}\n\n\n/*\n * ext4_ext_rm_leaf() Removes the extents associated with the\n * blocks appearing between \"start\" and \"end\".  Both \"start\"\n * and \"end\" must appear in the same extent or EIO is returned.\n *\n * @handle: The journal handle\n * @inode:  The files inode\n * @path:   The path to the leaf\n * @partial_cluster: The cluster which we'll have to free if all extents\n *                   has been released from it.  However, if this value is\n *                   negative, it's a cluster just to the right of the\n *                   punched region and it must not be freed.\n * @start:  The first block to remove\n * @end:   The last block to remove\n */\nstatic int\next4_ext_rm_leaf(handle_t *handle, struct inode *inode,\n\t\t struct ext4_ext_path *path,\n\t\t long long *partial_cluster,\n\t\t ext4_lblk_t start, ext4_lblk_t end)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint err = 0, correct_index = 0;\n\tint depth = ext_depth(inode), credits;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t a, b;\n\tunsigned num;\n\text4_lblk_t ex_ee_block;\n\tunsigned short ex_ee_len;\n\tunsigned unwritten = 0;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t pblk;\n\n\t/* the header must be checked already in ext4_ext_remove_space() */\n\text_debug(\"truncate since %u in leaf to %u\\n\", start, end);\n\tif (!path[depth].p_hdr)\n\t\tpath[depth].p_hdr = ext_block_hdr(path[depth].p_bh);\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\t/* find where to start removing */\n\tex = path[depth].p_ext;\n\tif (!ex)\n\t\tex = EXT_LAST_EXTENT(eh);\n\n\tex_ee_block = le32_to_cpu(ex->ee_block);\n\tex_ee_len = ext4_ext_get_actual_len(ex);\n\n\ttrace_ext4_ext_rm_leaf(inode, start, ex, *partial_cluster);\n\n\twhile (ex >= EXT_FIRST_EXTENT(eh) &&\n\t\t\tex_ee_block + ex_ee_len > start) {\n\n\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\tunwritten = 1;\n\t\telse\n\t\t\tunwritten = 0;\n\n\t\text_debug(\"remove ext %u:[%d]%d\\n\", ex_ee_block,\n\t\t\t  unwritten, ex_ee_len);\n\t\tpath[depth].p_ext = ex;\n\n\t\ta = ex_ee_block > start ? ex_ee_block : start;\n\t\tb = ex_ee_block+ex_ee_len - 1 < end ?\n\t\t\tex_ee_block+ex_ee_len - 1 : end;\n\n\t\text_debug(\"  border %u:%u\\n\", a, b);\n\n\t\t/* If this extent is beyond the end of the hole, skip it */\n\t\tif (end < ex_ee_block) {\n\t\t\t/*\n\t\t\t * We're going to skip this extent and move to another,\n\t\t\t * so note that its first cluster is in use to avoid\n\t\t\t * freeing it when removing blocks.  Eventually, the\n\t\t\t * right edge of the truncated/punched region will\n\t\t\t * be just to the left.\n\t\t\t */\n\t\t\tif (sbi->s_cluster_ratio > 1) {\n\t\t\t\tpblk = ext4_ext_pblock(ex);\n\t\t\t\t*partial_cluster =\n\t\t\t\t\t-(long long) EXT4_B2C(sbi, pblk);\n\t\t\t}\n\t\t\tex--;\n\t\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t\t\tcontinue;\n\t\t} else if (b != ex_ee_block + ex_ee_len - 1) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"can not handle truncate %u:%u \"\n\t\t\t\t\t \"on extent %u:%u\",\n\t\t\t\t\t start, end, ex_ee_block,\n\t\t\t\t\t ex_ee_block + ex_ee_len - 1);\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t} else if (a != ex_ee_block) {\n\t\t\t/* remove tail of the extent */\n\t\t\tnum = a - ex_ee_block;\n\t\t} else {\n\t\t\t/* remove whole extent: excellent! */\n\t\t\tnum = 0;\n\t\t}\n\t\t/*\n\t\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t\t * descriptor) for each block group; assume two block\n\t\t * groups plus ex_ee_len/blocks_per_block_group for\n\t\t * the worst case\n\t\t */\n\t\tcredits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));\n\t\tif (ex == EXT_FIRST_EXTENT(eh)) {\n\t\t\tcorrect_index = 1;\n\t\t\tcredits += (ext_depth(inode)) + 1;\n\t\t}\n\t\tcredits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);\n\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_remove_blocks(handle, inode, ex, partial_cluster,\n\t\t\t\t\t a, b);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (num == 0)\n\t\t\t/* this extent is removed; mark slot entirely unused */\n\t\t\text4_ext_store_pblock(ex, 0);\n\n\t\tex->ee_len = cpu_to_le16(num);\n\t\t/*\n\t\t * Do not mark unwritten if all the blocks in the\n\t\t * extent have been removed.\n\t\t */\n\t\tif (unwritten && num)\n\t\t\text4_ext_mark_unwritten(ex);\n\t\t/*\n\t\t * If the extent was completely released,\n\t\t * we need to remove it from the leaf\n\t\t */\n\t\tif (num == 0) {\n\t\t\tif (end != EXT_MAX_BLOCKS - 1) {\n\t\t\t\t/*\n\t\t\t\t * For hole punching, we need to scoot all the\n\t\t\t\t * extents up when an extent is removed so that\n\t\t\t\t * we dont have blank extents in the middle\n\t\t\t\t */\n\t\t\t\tmemmove(ex, ex+1, (EXT_LAST_EXTENT(eh) - ex) *\n\t\t\t\t\tsizeof(struct ext4_extent));\n\n\t\t\t\t/* Now get rid of the one at the end */\n\t\t\t\tmemset(EXT_LAST_EXTENT(eh), 0,\n\t\t\t\t\tsizeof(struct ext4_extent));\n\t\t\t}\n\t\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\t}\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\text_debug(\"new extent: %u:%u:%llu\\n\", ex_ee_block, num,\n\t\t\t\text4_ext_pblock(ex));\n\t\tex--;\n\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t}\n\n\tif (correct_index && eh->eh_entries)\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\n\t/*\n\t * If there's a partial cluster and at least one extent remains in\n\t * the leaf, free the partial cluster if it isn't shared with the\n\t * current extent.  If it is shared with the current extent\n\t * we zero partial_cluster because we've reached the start of the\n\t * truncated/punched region and we're done removing blocks.\n\t */\n\tif (*partial_cluster > 0 && ex >= EXT_FIRST_EXTENT(eh)) {\n\t\tpblk = ext4_ext_pblock(ex) + ex_ee_len - 1;\n\t\tif (*partial_cluster != (long long) EXT4_B2C(sbi, pblk)) {\n\t\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t\t EXT4_C2B(sbi, *partial_cluster),\n\t\t\t\t\t sbi->s_cluster_ratio,\n\t\t\t\t\t get_default_free_blocks_flags(inode));\n\t\t}\n\t\t*partial_cluster = 0;\n\t}\n\n\t/* if this leaf is free, then we should\n\t * remove it from index block above */\n\tif (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)\n\t\terr = ext4_ext_rm_idx(handle, inode, path, depth);\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_more_to_rm:\n * returns 1 if current index has to be freed (even partial)\n */\nstatic int\next4_ext_more_to_rm(struct ext4_ext_path *path)\n{\n\tBUG_ON(path->p_idx == NULL);\n\n\tif (path->p_idx < EXT_FIRST_INDEX(path->p_hdr))\n\t\treturn 0;\n\n\t/*\n\t * if truncate on deeper level happened, it wasn't partial,\n\t * so we have to consider current index for truncation\n\t */\n\tif (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block)\n\t\treturn 0;\n\treturn 1;\n}\n\nint ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,\n\t\t\t  ext4_lblk_t end)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint depth = ext_depth(inode);\n\tstruct ext4_ext_path *path = NULL;\n\tlong long partial_cluster = 0;\n\thandle_t *handle;\n\tint i = 0, err = 0;\n\n\text_debug(\"truncate since %u to %u\\n\", start, end);\n\n\t/* probably first extent we're gonna free will be last in block */\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, depth + 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\nagain:\n\ttrace_ext4_ext_remove_space(inode, start, end, depth);\n\n\t/*\n\t * Check if we are removing extents inside the extent tree. If that\n\t * is the case, we are going to punch a hole inside the extent tree\n\t * so we have to check whether we need to split the extent covering\n\t * the last block to remove so we can easily remove the part of it\n\t * in ext4_ext_rm_leaf().\n\t */\n\tif (end < EXT_MAX_BLOCKS - 1) {\n\t\tstruct ext4_extent *ex;\n\t\text4_lblk_t ee_block, ex_end, lblk;\n\t\text4_fsblk_t pblk;\n\n\t\t/* find extent for or closest extent to this block */\n\t\tpath = ext4_find_extent(inode, end, NULL, EXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path)) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn PTR_ERR(path);\n\t\t}\n\t\tdepth = ext_depth(inode);\n\t\t/* Leaf not may not exist only if inode has no blocks at all */\n\t\tex = path[depth].p_ext;\n\t\tif (!ex) {\n\t\t\tif (depth) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"path[%d].p_hdr == NULL\",\n\t\t\t\t\t\t depth);\n\t\t\t\terr = -EIO;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\tee_block = le32_to_cpu(ex->ee_block);\n\t\tex_end = ee_block + ext4_ext_get_actual_len(ex) - 1;\n\n\t\t/*\n\t\t * See if the last block is inside the extent, if so split\n\t\t * the extent at 'end' block so we can easily remove the\n\t\t * tail of the first part of the split extent in\n\t\t * ext4_ext_rm_leaf().\n\t\t */\n\t\tif (end >= ee_block && end < ex_end) {\n\n\t\t\t/*\n\t\t\t * If we're going to split the extent, note that\n\t\t\t * the cluster containing the block after 'end' is\n\t\t\t * in use to avoid freeing it when removing blocks.\n\t\t\t */\n\t\t\tif (sbi->s_cluster_ratio > 1) {\n\t\t\t\tpblk = ext4_ext_pblock(ex) + end - ee_block + 2;\n\t\t\t\tpartial_cluster =\n\t\t\t\t\t-(long long) EXT4_B2C(sbi, pblk);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Split the extent in two so that 'end' is the last\n\t\t\t * block in the first new extent. Also we should not\n\t\t\t * fail removing space due to ENOSPC so try to use\n\t\t\t * reserved block if that happens.\n\t\t\t */\n\t\t\terr = ext4_force_split_extent_at(handle, inode, &path,\n\t\t\t\t\t\t\t end + 1, 1);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t} else if (sbi->s_cluster_ratio > 1 && end >= ex_end) {\n\t\t\t/*\n\t\t\t * If there's an extent to the right its first cluster\n\t\t\t * contains the immediate right boundary of the\n\t\t\t * truncated/punched region.  Set partial_cluster to\n\t\t\t * its negative value so it won't be freed if shared\n\t\t\t * with the current extent.  The end < ee_block case\n\t\t\t * is handled in ext4_ext_rm_leaf().\n\t\t\t */\n\t\t\tlblk = ex_end + 1;\n\t\t\terr = ext4_ext_search_right(inode, path, &lblk, &pblk,\n\t\t\t\t\t\t    &ex);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tif (pblk)\n\t\t\t\tpartial_cluster =\n\t\t\t\t\t-(long long) EXT4_B2C(sbi, pblk);\n\t\t}\n\t}\n\t/*\n\t * We start scanning from right side, freeing all the blocks\n\t * after i_size and walking into the tree depth-wise.\n\t */\n\tdepth = ext_depth(inode);\n\tif (path) {\n\t\tint k = i = depth;\n\t\twhile (--k > 0)\n\t\t\tpath[k].p_block =\n\t\t\t\tle16_to_cpu(path[k].p_hdr->eh_entries)+1;\n\t} else {\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 1),\n\t\t\t       GFP_NOFS);\n\t\tif (path == NULL) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tpath[0].p_maxdepth = path[0].p_depth = depth;\n\t\tpath[0].p_hdr = ext_inode_hdr(inode);\n\t\ti = 0;\n\n\t\tif (ext4_ext_check(inode, path[0].p_hdr, depth, 0)) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\terr = 0;\n\n\twhile (i >= 0 && err == 0) {\n\t\tif (i == depth) {\n\t\t\t/* this is leaf block */\n\t\t\terr = ext4_ext_rm_leaf(handle, inode, path,\n\t\t\t\t\t       &partial_cluster, start,\n\t\t\t\t\t       end);\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* this is index block */\n\t\tif (!path[i].p_hdr) {\n\t\t\text_debug(\"initialize header\\n\");\n\t\t\tpath[i].p_hdr = ext_block_hdr(path[i].p_bh);\n\t\t}\n\n\t\tif (!path[i].p_idx) {\n\t\t\t/* this level hasn't been touched yet */\n\t\t\tpath[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;\n\t\t\text_debug(\"init index ptr: hdr 0x%p, num %d\\n\",\n\t\t\t\t  path[i].p_hdr,\n\t\t\t\t  le16_to_cpu(path[i].p_hdr->eh_entries));\n\t\t} else {\n\t\t\t/* we were already here, see at next index */\n\t\t\tpath[i].p_idx--;\n\t\t}\n\n\t\text_debug(\"level %d - index, first 0x%p, cur 0x%p\\n\",\n\t\t\t\ti, EXT_FIRST_INDEX(path[i].p_hdr),\n\t\t\t\tpath[i].p_idx);\n\t\tif (ext4_ext_more_to_rm(path + i)) {\n\t\t\tstruct buffer_head *bh;\n\t\t\t/* go to the next level */\n\t\t\text_debug(\"move to level %d (block %llu)\\n\",\n\t\t\t\t  i + 1, ext4_idx_pblock(path[i].p_idx));\n\t\t\tmemset(path + i + 1, 0, sizeof(*path));\n\t\t\tbh = read_extent_tree_block(inode,\n\t\t\t\text4_idx_pblock(path[i].p_idx), depth - i - 1,\n\t\t\t\tEXT4_EX_NOCACHE);\n\t\t\tif (IS_ERR(bh)) {\n\t\t\t\t/* should we reset i_size? */\n\t\t\t\terr = PTR_ERR(bh);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* Yield here to deal with large extent trees.\n\t\t\t * Should be a no-op if we did IO above. */\n\t\t\tcond_resched();\n\t\t\tif (WARN_ON(i + 1 > depth)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpath[i + 1].p_bh = bh;\n\n\t\t\t/* save actual number of indexes since this\n\t\t\t * number is changed at the next iteration */\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);\n\t\t\ti++;\n\t\t} else {\n\t\t\t/* we finished processing this index, go up */\n\t\t\tif (path[i].p_hdr->eh_entries == 0 && i > 0) {\n\t\t\t\t/* index is empty, remove it;\n\t\t\t\t * handle must be already prepared by the\n\t\t\t\t * truncatei_leaf() */\n\t\t\t\terr = ext4_ext_rm_idx(handle, inode, path, i);\n\t\t\t}\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\text_debug(\"return to level %d\\n\", i);\n\t\t}\n\t}\n\n\ttrace_ext4_ext_remove_space_done(inode, start, end, depth,\n\t\t\tpartial_cluster, path->p_hdr->eh_entries);\n\n\t/*\n\t * If we still have something in the partial cluster and we have removed\n\t * even the first extent, then we should free the blocks in the partial\n\t * cluster as well.  (This code will only run when there are no leaves\n\t * to the immediate left of the truncated/punched region.)\n\t */\n\tif (partial_cluster > 0 && err == 0) {\n\t\t/* don't zero partial_cluster since it's not used afterwards */\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(sbi, partial_cluster),\n\t\t\t\t sbi->s_cluster_ratio,\n\t\t\t\t get_default_free_blocks_flags(inode));\n\t}\n\n\t/* TODO: flexible tree reduction should be here */\n\tif (path->p_hdr->eh_entries == 0) {\n\t\t/*\n\t\t * truncate to zero freed all the tree,\n\t\t * so we need to correct eh_depth\n\t\t */\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err == 0) {\n\t\t\text_inode_hdr(inode)->eh_depth = 0;\n\t\t\text_inode_hdr(inode)->eh_max =\n\t\t\t\tcpu_to_le16(ext4_ext_space_root(inode, 0));\n\t\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\t}\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tpath = NULL;\n\tif (err == -EAGAIN)\n\t\tgoto again;\n\text4_journal_stop(handle);\n\n\treturn err;\n}\n\n/*\n * called at mount time\n */\nvoid ext4_ext_init(struct super_block *sb)\n{\n\t/*\n\t * possible initialization would be here\n\t */\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) {\n#if defined(AGGRESSIVE_TEST) || defined(CHECK_BINSEARCH) || defined(EXTENTS_STATS)\n\t\tprintk(KERN_INFO \"EXT4-fs: file extents enabled\"\n#ifdef AGGRESSIVE_TEST\n\t\t       \", aggressive tests\"\n#endif\n#ifdef CHECK_BINSEARCH\n\t\t       \", check binsearch\"\n#endif\n#ifdef EXTENTS_STATS\n\t\t       \", stats\"\n#endif\n\t\t       \"\\n\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tspin_lock_init(&EXT4_SB(sb)->s_ext_stats_lock);\n\t\tEXT4_SB(sb)->s_ext_min = 1 << 30;\n\t\tEXT4_SB(sb)->s_ext_max = 0;\n#endif\n\t}\n}\n\n/*\n * called at umount time\n */\nvoid ext4_ext_release(struct super_block *sb)\n{\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS))\n\t\treturn;\n\n#ifdef EXTENTS_STATS\n\tif (EXT4_SB(sb)->s_ext_blocks && EXT4_SB(sb)->s_ext_extents) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\tprintk(KERN_ERR \"EXT4-fs: %lu blocks in %lu extents (%lu ave)\\n\",\n\t\t\tsbi->s_ext_blocks, sbi->s_ext_extents,\n\t\t\tsbi->s_ext_blocks / sbi->s_ext_extents);\n\t\tprintk(KERN_ERR \"EXT4-fs: extents: %lu min, %lu max, max depth %lu\\n\",\n\t\t\tsbi->s_ext_min, sbi->s_ext_max, sbi->s_depth_max);\n\t}\n#endif\n}\n\nstatic int ext4_zeroout_es(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_lblk_t  ee_block;\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\n\tee_block  = le32_to_cpu(ex->ee_block);\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\n\tif (ee_len == 0)\n\t\treturn 0;\n\n\treturn ext4_es_insert_extent(inode, ee_block, ee_len, ee_pblock,\n\t\t\t\t     EXTENT_STATUS_WRITTEN);\n}\n\n/* FIXME!! we need to try to merge to left or right after zero-out  */\nstatic int ext4_ext_zeroout(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\tint ret;\n\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\n\tret = sb_issue_zeroout(inode->i_sb, ee_pblock, ee_len, GFP_NOFS);\n\tif (ret > 0)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * ext4_split_extent_at() splits an extent at given block.\n *\n * @handle: the journal handle\n * @inode: the file inode\n * @path: the path to the extent\n * @split: the logical block where the extent is splitted.\n * @split_flags: indicates if the extent could be zeroout if split fails, and\n *\t\t the states(init or unwritten) of new extents.\n * @flags: flags used to insert new extent to extent tree.\n *\n *\n * Splits extent [a, b] into two extents [a, @split) and [@split, b], states\n * of which are deterimined by split_flag.\n *\n * There are two cases:\n *  a> the extent are splitted into two extent.\n *  b> split is not needed, and just mark the extent.\n *\n * return 0 on success.\n */\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path **ppath,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex, zero_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==\n\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\tBUG_ON(!ext4_ext_is_unwritten(ex) &&\n\t       split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t     EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t     EXT4_EXT_MARK_UNWRIT2));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNWRIT2)\n\t\t\text4_ext_mark_unwritten(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNWRIT1)\n\t\text4_ext_mark_unwritten(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNWRIT2)\n\t\text4_ext_mark_unwritten(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, ppath, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {\n\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1) {\n\t\t\t\terr = ext4_ext_zeroout(inode, ex2);\n\t\t\t\tzero_ex.ee_block = ex2->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(ex2));\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex2));\n\t\t\t} else {\n\t\t\t\terr = ext4_ext_zeroout(inode, ex);\n\t\t\t\tzero_ex.ee_block = ex->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(ex));\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t}\n\t\t} else {\n\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tzero_ex.ee_block = orig_ex.ee_block;\n\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(&orig_ex));\n\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t      ext4_ext_pblock(&orig_ex));\n\t\t}\n\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\n\t\t/* update extent status tree */\n\t\terr = ext4_zeroout_es(inode, &zero_ex);\n\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + path->p_depth);\n\treturn err;\n}\n\n/*\n * ext4_split_extents() splits an extent and mark extent which is covered\n * by @map as split_flags indicates\n *\n * It may result in splitting the extent into multiple extents (up to three)\n * There are three possibilities:\n *   a> There is no split required\n *   b> Splits in two extents: Split is happening at either end of the extent\n *   c> Splits in three extents: Somone is splitting in middle of the extent\n *\n */\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path **ppath,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint unwritten;\n\tint split_flag1, flags1;\n\tint allocated = map->m_len;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tunwritten = ext4_ext_is_unwritten(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (unwritten)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNWRIT2;\n\t\tif (split_flag & EXT4_EXT_DATA_VALID2)\n\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;\n\t\terr = ext4_split_extent_at(handle, inode, ppath,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else {\n\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t}\n\t/*\n\t * Update path is required because previous ext4_split_extent_at() may\n\t * result in split of original leaf or extent zeroout.\n\t */\n\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tif (!ex) {\n\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t (unsigned long) map->m_lblk);\n\t\treturn -EIO;\n\t}\n\tunwritten = ext4_ext_is_unwritten(ex);\n\tsplit_flag1 = 0;\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_DATA_VALID2;\n\t\tif (unwritten) {\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNWRIT1;\n\t\t\tsplit_flag1 |= split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t\t\t\t     EXT4_EXT_MARK_UNWRIT2);\n\t\t}\n\t\terr = ext4_split_extent_at(handle, inode, ppath,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() if someone tries to write\n * to an unwritten extent. It may result in splitting the unwritten\n * extent into multiple extents (up to three - one initialized and two\n * unwritten).\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be initialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * Pre-conditions:\n *  - The extent pointed to by 'path' is unwritten.\n *  - The extent pointed to by 'path' contains a superset\n *    of the logical span [map->m_lblk, map->m_lblk + map->m_len).\n *\n * Post-conditions on success:\n *  - the returned value is the number of blocks beyond map->l_lblk\n *    that are allocated and initialized.\n *    It is guaranteed to be >= map->m_len.\n */\nstatic int ext4_ext_convert_to_initialized(handle_t *handle,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_map_blocks *map,\n\t\t\t\t\t   struct ext4_ext_path **ppath,\n\t\t\t\t\t   int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_sb_info *sbi;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_map_blocks split_map;\n\tstruct ext4_extent zero_ex;\n\tstruct ext4_extent *ex, *abut_ex;\n\text4_lblk_t ee_block, eof_block;\n\tunsigned int ee_len, depth, map_len = map->m_len;\n\tint allocated = 0, max_zeroout = 0;\n\tint err = 0;\n\tint split_flag = 0;\n\n\text_debug(\"ext4_ext_convert_to_initialized: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map_len);\n\n\tsbi = EXT4_SB(inode->i_sb);\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map_len)\n\t\teof_block = map->m_lblk + map_len;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tzero_ex.ee_len = 0;\n\n\ttrace_ext4_ext_convert_to_initialized_enter(inode, map, ex);\n\n\t/* Pre-conditions */\n\tBUG_ON(!ext4_ext_is_unwritten(ex));\n\tBUG_ON(!in_range(map->m_lblk, ee_block, ee_len));\n\n\t/*\n\t * Attempt to transfer newly initialized blocks from the currently\n\t * unwritten extent to its neighbor. This is much cheaper\n\t * than an insertion followed by a merge as those involve costly\n\t * memmove() calls. Transferring to the left is the common case in\n\t * steady state for workloads doing fallocate(FALLOC_FL_KEEP_SIZE)\n\t * followed by append writes.\n\t *\n\t * Limitations of the current logic:\n\t *  - L1: we do not deal with writes covering the whole extent.\n\t *    This would require removing the extent if the transfer\n\t *    is possible.\n\t *  - L2: we only attempt to merge with an extent stored in the\n\t *    same extent tree node.\n\t */\n\tif ((map->m_lblk == ee_block) &&\n\t\t/* See if we can merge left */\n\t\t(map_len < ee_len) &&\t\t/*L1*/\n\t\t(ex > EXT_FIRST_EXTENT(eh))) {\t/*L2*/\n\t\text4_lblk_t prev_lblk;\n\t\text4_fsblk_t prev_pblk, ee_pblk;\n\t\tunsigned int prev_len;\n\n\t\tabut_ex = ex - 1;\n\t\tprev_lblk = le32_to_cpu(abut_ex->ee_block);\n\t\tprev_len = ext4_ext_get_actual_len(abut_ex);\n\t\tprev_pblk = ext4_ext_pblock(abut_ex);\n\t\tee_pblk = ext4_ext_pblock(ex);\n\n\t\t/*\n\t\t * A transfer of blocks from 'ex' to 'abut_ex' is allowed\n\t\t * upon those conditions:\n\t\t * - C1: abut_ex is initialized,\n\t\t * - C2: abut_ex is logically abutting ex,\n\t\t * - C3: abut_ex is physically abutting ex,\n\t\t * - C4: abut_ex can receive the additional blocks without\n\t\t *   overflowing the (initialized) length limit.\n\t\t */\n\t\tif ((!ext4_ext_is_unwritten(abut_ex)) &&\t\t/*C1*/\n\t\t\t((prev_lblk + prev_len) == ee_block) &&\t\t/*C2*/\n\t\t\t((prev_pblk + prev_len) == ee_pblk) &&\t\t/*C3*/\n\t\t\t(prev_len < (EXT_INIT_MAX_LEN - map_len))) {\t/*C4*/\n\t\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\ttrace_ext4_ext_convert_to_initialized_fastpath(inode,\n\t\t\t\tmap, ex, abut_ex);\n\n\t\t\t/* Shift the start of ex by 'map_len' blocks */\n\t\t\tex->ee_block = cpu_to_le32(ee_block + map_len);\n\t\t\text4_ext_store_pblock(ex, ee_pblk + map_len);\n\t\t\tex->ee_len = cpu_to_le16(ee_len - map_len);\n\t\t\text4_ext_mark_unwritten(ex); /* Restore the flag */\n\n\t\t\t/* Extend abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_len = cpu_to_le16(prev_len + map_len);\n\n\t\t\t/* Result: number of initialized blocks past m_lblk */\n\t\t\tallocated = map_len;\n\t\t}\n\t} else if (((map->m_lblk + map_len) == (ee_block + ee_len)) &&\n\t\t   (map_len < ee_len) &&\t/*L1*/\n\t\t   ex < EXT_LAST_EXTENT(eh)) {\t/*L2*/\n\t\t/* See if we can merge right */\n\t\text4_lblk_t next_lblk;\n\t\text4_fsblk_t next_pblk, ee_pblk;\n\t\tunsigned int next_len;\n\n\t\tabut_ex = ex + 1;\n\t\tnext_lblk = le32_to_cpu(abut_ex->ee_block);\n\t\tnext_len = ext4_ext_get_actual_len(abut_ex);\n\t\tnext_pblk = ext4_ext_pblock(abut_ex);\n\t\tee_pblk = ext4_ext_pblock(ex);\n\n\t\t/*\n\t\t * A transfer of blocks from 'ex' to 'abut_ex' is allowed\n\t\t * upon those conditions:\n\t\t * - C1: abut_ex is initialized,\n\t\t * - C2: abut_ex is logically abutting ex,\n\t\t * - C3: abut_ex is physically abutting ex,\n\t\t * - C4: abut_ex can receive the additional blocks without\n\t\t *   overflowing the (initialized) length limit.\n\t\t */\n\t\tif ((!ext4_ext_is_unwritten(abut_ex)) &&\t\t/*C1*/\n\t\t    ((map->m_lblk + map_len) == next_lblk) &&\t\t/*C2*/\n\t\t    ((ee_pblk + ee_len) == next_pblk) &&\t\t/*C3*/\n\t\t    (next_len < (EXT_INIT_MAX_LEN - map_len))) {\t/*C4*/\n\t\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\ttrace_ext4_ext_convert_to_initialized_fastpath(inode,\n\t\t\t\tmap, ex, abut_ex);\n\n\t\t\t/* Shift the start of abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_block = cpu_to_le32(next_lblk - map_len);\n\t\t\text4_ext_store_pblock(abut_ex, next_pblk - map_len);\n\t\t\tex->ee_len = cpu_to_le16(ee_len - map_len);\n\t\t\text4_ext_mark_unwritten(ex); /* Restore the flag */\n\n\t\t\t/* Extend abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_len = cpu_to_le16(next_len + map_len);\n\n\t\t\t/* Result: number of initialized blocks past m_lblk */\n\t\t\tallocated = map_len;\n\t\t}\n\t}\n\tif (allocated) {\n\t\t/* Mark the block containing both extents as dirty */\n\t\text4_ext_dirty(handle, inode, path + depth);\n\n\t\t/* Update path to point to the right extent */\n\t\tpath[depth].p_ext = abut_ex;\n\t\tgoto out;\n\t} else\n\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\n\tWARN_ON(map->m_lblk < ee_block);\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully inside i_size or new_size.\n\t */\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\n\tif (EXT4_EXT_MAY_ZEROOUT & split_flag)\n\t\tmax_zeroout = sbi->s_extent_max_zeroout_kb >>\n\t\t\t(inode->i_sb->s_blocksize_bits - 10);\n\n\t/* If extent is less than s_max_zeroout_kb, zeroout directly */\n\tif (max_zeroout && (ee_len <= max_zeroout)) {\n\t\terr = ext4_ext_zeroout(inode, ex);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tzero_ex.ee_block = ex->ee_block;\n\t\tzero_ex.ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex));\n\t\text4_ext_store_pblock(&zero_ex, ext4_ext_pblock(ex));\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\t\text4_ext_mark_initialized(ex);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * four cases:\n\t * 1. split the extent into three extents.\n\t * 2. split the extent into two extents, zeroout the first half.\n\t * 3. split the extent into two extents, zeroout the second half.\n\t * 4. split the extent into two extents with out zeroout.\n\t */\n\tsplit_map.m_lblk = map->m_lblk;\n\tsplit_map.m_len = map->m_len;\n\n\tif (max_zeroout && (allocated > map->m_len)) {\n\t\tif (allocated <= max_zeroout) {\n\t\t\t/* case 3 */\n\t\t\tzero_ex.ee_block =\n\t\t\t\t\t cpu_to_le32(map->m_lblk);\n\t\t\tzero_ex.ee_len = cpu_to_le16(allocated);\n\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\text4_ext_pblock(ex) + map->m_lblk - ee_block);\n\t\t\terr = ext4_ext_zeroout(inode, &zero_ex);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tsplit_map.m_lblk = map->m_lblk;\n\t\t\tsplit_map.m_len = allocated;\n\t\t} else if (map->m_lblk - ee_block + map->m_len < max_zeroout) {\n\t\t\t/* case 2 */\n\t\t\tif (map->m_lblk != ee_block) {\n\t\t\t\tzero_ex.ee_block = ex->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(map->m_lblk -\n\t\t\t\t\t\t\tee_block);\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t\terr = ext4_ext_zeroout(inode, &zero_ex);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tsplit_map.m_lblk = ee_block;\n\t\t\tsplit_map.m_len = map->m_lblk - ee_block + map->m_len;\n\t\t\tallocated = map->m_len;\n\t\t}\n\t}\n\n\terr = ext4_split_extent(handle, inode, ppath, &split_map, split_flag,\n\t\t\t\tflags);\n\tif (err > 0)\n\t\terr = 0;\nout:\n\t/* If we have gotten a failure, don't zero out status tree */\n\tif (!err)\n\t\terr = ext4_zeroout_es(inode, &zero_ex);\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() from\n * ext4_get_blocks_dio_write() when DIO to write\n * to an unwritten extent.\n *\n * Writing to an unwritten extent may result in splitting the unwritten\n * extent into multiple initialized/unwritten extents (up to three)\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be unwritten\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * This works the same way in the case of initialized -> unwritten conversion.\n *\n * One of more index blocks maybe needed if the extent tree grow after\n * the unwritten extent split. To prevent ENOSPC occur at the IO\n * complete, we need to split the unwritten extent before DIO submit\n * the IO. The unwritten extent called at this time will be split\n * into three unwritten extent(at most). After IO complete, the part\n * being filled will be convert to initialized by the end_io callback function\n * via ext4_convert_unwritten_extents().\n *\n * Returns the size of unwritten extent to be written on success.\n */\nstatic int ext4_split_convert_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\t\tint flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"%s: inode %lu, logical block %llu, max_blocks %u\\n\",\n\t\t  __func__, inode->i_ino,\n\t\t  (unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\t/* Convert to unwritten */\n\tif (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN) {\n\t\tsplit_flag |= EXT4_EXT_DATA_VALID1;\n\t/* Convert to initialized */\n\t} else if (flags & EXT4_GET_BLOCKS_CONVERT) {\n\t\tsplit_flag |= ee_block + ee_len <= eof_block ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tsplit_flag |= (EXT4_EXT_MARK_UNWRIT2 | EXT4_EXT_DATA_VALID2);\n\t}\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, ppath, map, split_flag, flags);\n}\n\nstatic int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\t\tstruct ext4_ext_path **ppath)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\tunsigned int ee_len;\n\tint depth;\n\tint err = 0;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t  (unsigned long long)ee_block, ee_len);\n\n\t/* If extent is larger than requested it is a clear sign that we still\n\t * have some extent state machine issues left. So extent_split is still\n\t * required.\n\t * TODO: Once all related issues will be fixed this situation should be\n\t * illegal.\n\t */\n\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n#ifdef EXT4_DEBUG\n\t\text4_warning(\"Inode (%ld) finished: extent logical block %llu,\"\n\t\t\t     \" len %u; IO logical block %llu, len %u\\n\",\n\t\t\t     inode->i_ino, (unsigned long long)ee_block, ee_len,\n\t\t\t     (unsigned long long)map->m_lblk, map->m_len);\n#endif\n\t\terr = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\t\t\t EXT4_GET_BLOCKS_CONVERT);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = ext_depth(inode);\n\t\tex = path[depth].p_ext;\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}\n\nstatic void unmap_underlying_metadata_blocks(struct block_device *bdev,\n\t\t\tsector_t block, int count)\n{\n\tint i;\n\tfor (i = 0; i < count; i++)\n                unmap_underlying_metadata(bdev, block + i);\n}\n\n/*\n * Handle EOFBLOCKS_FL flag, clearing it if necessary\n */\nstatic int check_eofblocks_fl(handle_t *handle, struct inode *inode,\n\t\t\t      ext4_lblk_t lblk,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      unsigned int len)\n{\n\tint i, depth;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *last_ex;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS))\n\t\treturn 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\n\t/*\n\t * We're going to remove EOFBLOCKS_FL entirely in future so we\n\t * do not care for this case anymore. Simply remove the flag\n\t * if there are no extents.\n\t */\n\tif (unlikely(!eh->eh_entries))\n\t\tgoto out;\n\tlast_ex = EXT_LAST_EXTENT(eh);\n\t/*\n\t * We should clear the EOFBLOCKS_FL flag if we are writing the\n\t * last block in the last extent in the file.  We test this by\n\t * first checking to see if the caller to\n\t * ext4_ext_get_blocks() was interested in the last block (or\n\t * a block beyond the last block) in the current extent.  If\n\t * this turns out to be false, we can bail out from this\n\t * function immediately.\n\t */\n\tif (lblk + len < le32_to_cpu(last_ex->ee_block) +\n\t    ext4_ext_get_actual_len(last_ex))\n\t\treturn 0;\n\t/*\n\t * If the caller does appear to be planning to write at or\n\t * beyond the end of the current extent, we then test to see\n\t * if the current extent is the last extent in the file, by\n\t * checking to make sure it was reached via the rightmost node\n\t * at each level of the tree.\n\t */\n\tfor (i = depth-1; i >= 0; i--)\n\t\tif (path[i].p_idx != EXT_LAST_INDEX(path[i].p_hdr))\n\t\t\treturn 0;\nout:\n\text4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\treturn ext4_mark_inode_dirty(handle, inode);\n}\n\n/**\n * ext4_find_delalloc_range: find delayed allocated block in the given range.\n *\n * Return 1 if there is a delalloc block in the range, otherwise 0.\n */\nint ext4_find_delalloc_range(struct inode *inode,\n\t\t\t     ext4_lblk_t lblk_start,\n\t\t\t     ext4_lblk_t lblk_end)\n{\n\tstruct extent_status es;\n\n\text4_es_find_delayed_extent_range(inode, lblk_start, lblk_end, &es);\n\tif (es.es_len == 0)\n\t\treturn 0; /* there is no delay extent in this tree */\n\telse if (es.es_lblk <= lblk_start &&\n\t\t lblk_start < es.es_lblk + es.es_len)\n\t\treturn 1;\n\telse if (lblk_start <= es.es_lblk && es.es_lblk <= lblk_end)\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n\nint ext4_find_delalloc_cluster(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t lblk_start, lblk_end;\n\tlblk_start = EXT4_LBLK_CMASK(sbi, lblk);\n\tlblk_end = lblk_start + sbi->s_cluster_ratio - 1;\n\n\treturn ext4_find_delalloc_range(inode, lblk_start, lblk_end);\n}\n\n/**\n * Determines how many complete clusters (out of those specified by the 'map')\n * are under delalloc and were reserved quota for.\n * This function is called when we are writing out the blocks that were\n * originally written with their allocation delayed, but then the space was\n * allocated using fallocate() before the delayed allocation could be resolved.\n * The cases to look for are:\n * ('=' indicated delayed allocated blocks\n *  '-' indicates non-delayed allocated blocks)\n * (a) partial clusters towards beginning and/or end outside of allocated range\n *     are not delalloc'ed.\n *\tEx:\n *\t|----c---=|====c====|====c====|===-c----|\n *\t         |++++++ allocated ++++++|\n *\t==> 4 complete clusters in above example\n *\n * (b) partial cluster (outside of allocated range) towards either end is\n *     marked for delayed allocation. In this case, we will exclude that\n *     cluster.\n *\tEx:\n *\t|----====c========|========c========|\n *\t     |++++++ allocated ++++++|\n *\t==> 1 complete clusters in above example\n *\n *\tEx:\n *\t|================c================|\n *            |++++++ allocated ++++++|\n *\t==> 0 complete clusters in above example\n *\n * The ext4_da_update_reserve_space will be called only if we\n * determine here that there were some \"entire\" clusters that span\n * this 'allocated' range.\n * In the non-bigalloc case, this function will just end up returning num_blks\n * without ever calling ext4_find_delalloc_range.\n */\nstatic unsigned int\nget_reserved_cluster_alloc(struct inode *inode, ext4_lblk_t lblk_start,\n\t\t\t   unsigned int num_blks)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t alloc_cluster_start, alloc_cluster_end;\n\text4_lblk_t lblk_from, lblk_to, c_offset;\n\tunsigned int allocated_clusters = 0;\n\n\talloc_cluster_start = EXT4_B2C(sbi, lblk_start);\n\talloc_cluster_end = EXT4_B2C(sbi, lblk_start + num_blks - 1);\n\n\t/* max possible clusters for this allocation */\n\tallocated_clusters = alloc_cluster_end - alloc_cluster_start + 1;\n\n\ttrace_ext4_get_reserved_cluster_alloc(inode, lblk_start, num_blks);\n\n\t/* Check towards left side */\n\tc_offset = EXT4_LBLK_COFF(sbi, lblk_start);\n\tif (c_offset) {\n\t\tlblk_from = EXT4_LBLK_CMASK(sbi, lblk_start);\n\t\tlblk_to = lblk_from + c_offset - 1;\n\n\t\tif (ext4_find_delalloc_range(inode, lblk_from, lblk_to))\n\t\t\tallocated_clusters--;\n\t}\n\n\t/* Now check towards right. */\n\tc_offset = EXT4_LBLK_COFF(sbi, lblk_start + num_blks);\n\tif (allocated_clusters && c_offset) {\n\t\tlblk_from = lblk_start + num_blks;\n\t\tlblk_to = lblk_from + (sbi->s_cluster_ratio - c_offset) - 1;\n\n\t\tif (ext4_find_delalloc_range(inode, lblk_from, lblk_to))\n\t\t\tallocated_clusters--;\n\t}\n\n\treturn allocated_clusters;\n}\n\nstatic int\nconvert_initialized_extent(handle_t *handle, struct inode *inode,\n\t\t\t   struct ext4_map_blocks *map,\n\t\t\t   struct ext4_ext_path **ppath, int flags,\n\t\t\t   unsigned int allocated, ext4_fsblk_t newblock)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\tunsigned int ee_len;\n\tint depth;\n\tint err = 0;\n\n\t/*\n\t * Make sure that the extent is no bigger than we support with\n\t * unwritten extent\n\t */\n\tif (map->m_len > EXT_UNWRITTEN_MAX_LEN)\n\t\tmap->m_len = EXT_UNWRITTEN_MAX_LEN / 2;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\text_debug(\"%s: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", __func__, inode->i_ino,\n\t\t  (unsigned long long)ee_block, ee_len);\n\n\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n\t\terr = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\tEXT4_GET_BLOCKS_CONVERT_UNWRITTEN);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = ext_depth(inode);\n\t\tex = path[depth].p_ext;\n\t\tif (!ex) {\n\t\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t\t (unsigned long) map->m_lblk);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\treturn err;\n\t/* first mark the extent as unwritten */\n\text4_ext_mark_unwritten(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\tif (err)\n\t\treturn err;\n\text4_ext_show_leaf(inode, path);\n\n\text4_update_inode_fsync_trans(handle, inode, 1);\n\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path, map->m_len);\n\tif (err)\n\t\treturn err;\n\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_len = allocated;\n\treturn allocated;\n}\n\nstatic int\next4_ext_handle_unwritten_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path **ppath, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\n\text_debug(\"ext4_ext_handle_unwritten_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\t/*\n\t * When writing into unwritten space, we should not fail to\n\t * allocate metadata blocks for the new extent block if needed.\n\t */\n\tflags |= EXT4_GET_BLOCKS_METADATA_NOFAIL;\n\n\ttrace_ext4_ext_handle_unwritten_extents(inode, map, flags,\n\t\t\t\t\t\t    allocated, newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif (flags & EXT4_GET_BLOCKS_PRE_IO) {\n\t\tret = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\t\t flags | EXT4_GET_BLOCKS_CONVERT);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif (flags & EXT4_GET_BLOCKS_CONVERT) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n\t\t\t\t\t\t\t   ppath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tmap->m_flags |= EXT4_MAP_MAPPED;\n\t\tmap->m_pblk = newblock;\n\t\tif (allocated > map->m_len)\n\t\t\tallocated = map->m_len;\n\t\tmap->m_len = allocated;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNWRIT_EXT) {\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto map_out;\n\t}\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, ppath, flags);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\tmap->m_len = allocated;\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\tmap->m_lblk, map->m_len);\n\t\tif (reserved_clusters)\n\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\t     reserved_clusters,\n\t\t\t\t\t\t     0);\n\t}\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\treturn err ? err : allocated;\n}\n\n/*\n * get_implied_cluster_alloc - check to see if the requested\n * allocation (in the map structure) overlaps with a cluster already\n * allocated in an extent.\n *\t@sb\tThe filesystem superblock structure\n *\t@map\tThe requested lblk->pblk mapping\n *\t@ex\tThe extent structure which might contain an implied\n *\t\t\tcluster allocation\n *\n * This function is called by ext4_ext_map_blocks() after we failed to\n * find blocks that were already in the inode's extent tree.  Hence,\n * we know that the beginning of the requested region cannot overlap\n * the extent from the inode's extent tree.  There are three cases we\n * want to catch.  The first is this case:\n *\n *\t\t |--- cluster # N--|\n *    |--- extent ---|\t|---- requested region ---|\n *\t\t\t|==========|\n *\n * The second case that we need to test for is this one:\n *\n *   |--------- cluster # N ----------------|\n *\t   |--- requested region --|   |------- extent ----|\n *\t   |=======================|\n *\n * The third case is when the requested region lies between two extents\n * within the same cluster:\n *          |------------- cluster # N-------------|\n * |----- ex -----|                  |---- ex_right ----|\n *                  |------ requested region ------|\n *                  |================|\n *\n * In each of the above cases, we need to set the map->m_pblk and\n * map->m_len so it corresponds to the return the extent labelled as\n * \"|====|\" from cluster #N, since it is already in use for data in\n * cluster EXT4_B2C(sbi, map->m_lblk).\tWe will then return 1 to\n * signal to ext4_ext_map_blocks() that map->m_pblk should be treated\n * as a new \"allocated\" block region.  Otherwise, we will return 0 and\n * ext4_ext_map_blocks() will then allocate one or more new clusters\n * by calling ext4_mb_new_blocks().\n */\nstatic int get_implied_cluster_alloc(struct super_block *sb,\n\t\t\t\t     struct ext4_map_blocks *map,\n\t\t\t\t     struct ext4_extent *ex,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_lblk_t c_offset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\text4_lblk_t ex_cluster_start, ex_cluster_end;\n\text4_lblk_t rr_cluster_start;\n\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\tunsigned short ee_len = ext4_ext_get_actual_len(ex);\n\n\t/* The extent passed in that we are trying to match */\n\tex_cluster_start = EXT4_B2C(sbi, ee_block);\n\tex_cluster_end = EXT4_B2C(sbi, ee_block + ee_len - 1);\n\n\t/* The requested region passed into ext4_map_blocks() */\n\trr_cluster_start = EXT4_B2C(sbi, map->m_lblk);\n\n\tif ((rr_cluster_start == ex_cluster_end) ||\n\t    (rr_cluster_start == ex_cluster_start)) {\n\t\tif (rr_cluster_start == ex_cluster_end)\n\t\t\tee_start += ee_len - 1;\n\t\tmap->m_pblk = EXT4_PBLK_CMASK(sbi, ee_start) + c_offset;\n\t\tmap->m_len = min(map->m_len,\n\t\t\t\t (unsigned) sbi->s_cluster_ratio - c_offset);\n\t\t/*\n\t\t * Check for and handle this case:\n\t\t *\n\t\t *   |--------- cluster # N-------------|\n\t\t *\t\t       |------- extent ----|\n\t\t *\t   |--- requested region ---|\n\t\t *\t   |===========|\n\t\t */\n\n\t\tif (map->m_lblk < ee_block)\n\t\t\tmap->m_len = min(map->m_len, ee_block - map->m_lblk);\n\n\t\t/*\n\t\t * Check for the case where there is already another allocated\n\t\t * block to the right of 'ex' but before the end of the cluster.\n\t\t *\n\t\t *          |------------- cluster # N-------------|\n\t\t * |----- ex -----|                  |---- ex_right ----|\n\t\t *                  |------ requested region ------|\n\t\t *                  |================|\n\t\t */\n\t\tif (map->m_lblk > ee_block) {\n\t\t\text4_lblk_t next = ext4_ext_next_allocated_block(path);\n\t\t\tmap->m_len = min(map->m_len, next - map->m_lblk);\n\t\t}\n\n\t\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 1);\n\t\treturn 1;\n\t}\n\n\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 0);\n\treturn 0;\n}\n\n\n/*\n * Block allocation/map/preallocation routine for extents based files\n *\n *\n * Need to be called with\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system block\n * (ie, create is zero). Otherwise down_write(&EXT4_I(inode)->i_data_sem)\n *\n * return > 0, number of of blocks already mapped/allocated\n *          if create == 0 and these are pre-allocated blocks\n *          \tbuffer head is unmapped\n *          otherwise blocks are mapped\n *\n * return = 0, if plain look up failed (blocks have not been allocated)\n *          buffer head is unmapped\n *\n * return < 0, error case.\n */\nint ext4_ext_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map, int flags)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent newex, *ex, *ex2;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_fsblk_t newblock = 0;\n\tint free_on_err = 0, err = 0, depth, ret;\n\tunsigned int allocated = 0, offset = 0;\n\tunsigned int allocated_clusters = 0;\n\tstruct ext4_allocation_request ar;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\text4_lblk_t cluster_offset;\n\tint set_unwritten = 0;\n\tbool map_from_cluster = false;\n\n\text_debug(\"blocks %u/%u requested for inode %lu\\n\",\n\t\t  map->m_lblk, map->m_len, inode->i_ino);\n\ttrace_ext4_ext_map_blocks_enter(inode, map->m_lblk, map->m_len, flags);\n\n\t/* find extent for this block */\n\tpath = ext4_find_extent(inode, map->m_lblk, NULL, 0);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out2;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * consistent leaf must not be empty;\n\t * this situation is possible, though, _during_ tree modification;\n\t * this is why assert can't be put in ext4_find_extent()\n\t */\n\tif (unlikely(path[depth].p_ext == NULL && depth != 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extent address \"\n\t\t\t\t \"lblock: %lu, depth: %d pblock %lld\",\n\t\t\t\t (unsigned long) map->m_lblk, depth,\n\t\t\t\t path[depth].p_block);\n\t\terr = -EIO;\n\t\tgoto out2;\n\t}\n\n\tex = path[depth].p_ext;\n\tif (ex) {\n\t\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\t\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\t\tunsigned short ee_len;\n\n\n\t\t/*\n\t\t * unwritten extents are treated as holes, except that\n\t\t * we split out initialized portions during a write.\n\t\t */\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\n\t\ttrace_ext4_ext_show_extent(inode, ee_block, ee_start, ee_len);\n\n\t\t/* if found extent covers block, simply return it */\n\t\tif (in_range(map->m_lblk, ee_block, ee_len)) {\n\t\t\tnewblock = map->m_lblk - ee_block + ee_start;\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t\t\text_debug(\"%u fit into %u:%d -> %llu\\n\", map->m_lblk,\n\t\t\t\t  ee_block, ee_len, newblock);\n\n\t\t\t/*\n\t\t\t * If the extent is initialized check whether the\n\t\t\t * caller wants to convert it to unwritten.\n\t\t\t */\n\t\t\tif ((!ext4_ext_is_unwritten(ex)) &&\n\t\t\t    (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN)) {\n\t\t\t\tallocated = convert_initialized_extent(\n\t\t\t\t\t\thandle, inode, map, &path,\n\t\t\t\t\t\tflags, allocated, newblock);\n\t\t\t\tgoto out2;\n\t\t\t} else if (!ext4_ext_is_unwritten(ex))\n\t\t\t\tgoto out;\n\n\t\t\tret = ext4_ext_handle_unwritten_extents(\n\t\t\t\thandle, inode, map, &path, flags,\n\t\t\t\tallocated, newblock);\n\t\t\tif (ret < 0)\n\t\t\t\terr = ret;\n\t\t\telse\n\t\t\t\tallocated = ret;\n\t\t\tgoto out2;\n\t\t}\n\t}\n\n\t/*\n\t * requested block isn't allocated yet;\n\t * we couldn't try to create block if create flag is zero\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * put just found gap into cache to speed up\n\t\t * subsequent requests\n\t\t */\n\t\text4_ext_put_gap_in_cache(inode, path, map->m_lblk);\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * Okay, we need to do block allocation.\n\t */\n\tnewex.ee_block = cpu_to_le32(map->m_lblk);\n\tcluster_offset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\n\t/*\n\t * If we are doing bigalloc, check to see if the extent returned\n\t * by ext4_find_extent() implies a cluster we can use.\n\t */\n\tif (cluster_offset && ex &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap_from_cluster = true;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/* find neighbour allocated blocks */\n\tar.lleft = map->m_lblk;\n\terr = ext4_ext_search_left(inode, path, &ar.lleft, &ar.pleft);\n\tif (err)\n\t\tgoto out2;\n\tar.lright = map->m_lblk;\n\tex2 = NULL;\n\terr = ext4_ext_search_right(inode, path, &ar.lright, &ar.pright, &ex2);\n\tif (err)\n\t\tgoto out2;\n\n\t/* Check if the extent after searching to the right implies a\n\t * cluster we can use. */\n\tif ((sbi->s_cluster_ratio > 1) && ex2 &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex2, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap_from_cluster = true;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/*\n\t * See if request is beyond maximum number of blocks we can have in\n\t * a single extent. For an initialized extent this limit is\n\t * EXT_INIT_MAX_LEN and for an unwritten extent this limit is\n\t * EXT_UNWRITTEN_MAX_LEN.\n\t */\n\tif (map->m_len > EXT_INIT_MAX_LEN &&\n\t    !(flags & EXT4_GET_BLOCKS_UNWRIT_EXT))\n\t\tmap->m_len = EXT_INIT_MAX_LEN;\n\telse if (map->m_len > EXT_UNWRITTEN_MAX_LEN &&\n\t\t (flags & EXT4_GET_BLOCKS_UNWRIT_EXT))\n\t\tmap->m_len = EXT_UNWRITTEN_MAX_LEN;\n\n\t/* Check if we can really insert (m_lblk)::(m_lblk + m_len) extent */\n\tnewex.ee_len = cpu_to_le16(map->m_len);\n\terr = ext4_ext_check_overlap(sbi, inode, &newex, path);\n\tif (err)\n\t\tallocated = ext4_ext_get_actual_len(&newex);\n\telse\n\t\tallocated = map->m_len;\n\n\t/* allocate new block */\n\tar.inode = inode;\n\tar.goal = ext4_ext_find_goal(inode, path, map->m_lblk);\n\tar.logical = map->m_lblk;\n\t/*\n\t * We calculate the offset from the beginning of the cluster\n\t * for the logical block number, since when we allocate a\n\t * physical cluster, the physical block should start at the\n\t * same offset from the beginning of the cluster.  This is\n\t * needed so that future calls to get_implied_cluster_alloc()\n\t * work correctly.\n\t */\n\toffset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\tar.len = EXT4_NUM_B2C(sbi, offset+allocated);\n\tar.goal -= offset;\n\tar.logical -= offset;\n\tif (S_ISREG(inode->i_mode))\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\telse\n\t\t/* disable in-core preallocation for non-regular files */\n\t\tar.flags = 0;\n\tif (flags & EXT4_GET_BLOCKS_NO_NORMALIZE)\n\t\tar.flags |= EXT4_MB_HINT_NOPREALLOC;\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tar.flags |= EXT4_MB_DELALLOC_RESERVED;\n\tnewblock = ext4_mb_new_blocks(handle, &ar, &err);\n\tif (!newblock)\n\t\tgoto out2;\n\text_debug(\"allocate new block: goal %llu, found %llu/%u\\n\",\n\t\t  ar.goal, newblock, allocated);\n\tfree_on_err = 1;\n\tallocated_clusters = ar.len;\n\tar.len = EXT4_C2B(sbi, ar.len) - offset;\n\tif (ar.len > allocated)\n\t\tar.len = allocated;\n\ngot_allocated_blocks:\n\t/* try to insert new extent into found leaf and return */\n\text4_ext_store_pblock(&newex, newblock + offset);\n\tnewex.ee_len = cpu_to_le16(ar.len);\n\t/* Mark unwritten */\n\tif (flags & EXT4_GET_BLOCKS_UNWRIT_EXT){\n\t\text4_ext_mark_unwritten(&newex);\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\t/*\n\t\t * io_end structure was created for every IO write to an\n\t\t * unwritten extent. To avoid unnecessary conversion,\n\t\t * here we flag the IO that really needs the conversion.\n\t\t * For non asycn direct IO case, flag the inode state\n\t\t * that we need to perform conversion when IO is done.\n\t\t */\n\t\tif (flags & EXT4_GET_BLOCKS_PRE_IO)\n\t\t\tset_unwritten = 1;\n\t}\n\n\terr = 0;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0)\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t path, ar.len);\n\tif (!err)\n\t\terr = ext4_ext_insert_extent(handle, inode, &path,\n\t\t\t\t\t     &newex, flags);\n\n\tif (!err && set_unwritten) {\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode,\n\t\t\t\t\t     EXT4_STATE_DIO_UNWRITTEN);\n\t}\n\n\tif (err && free_on_err) {\n\t\tint fb_flags = flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE ?\n\t\t\tEXT4_FREE_BLOCKS_NO_QUOT_UPDATE : 0;\n\t\t/* free data blocks we just allocated */\n\t\t/* not a good idea to call discard here directly,\n\t\t * but otherwise we'd need to call it every free() */\n\t\text4_discard_preallocations(inode);\n\t\text4_free_blocks(handle, inode, NULL, newblock,\n\t\t\t\t EXT4_C2B(sbi, allocated_clusters), fb_flags);\n\t\tgoto out2;\n\t}\n\n\t/* previous routine could use block we allocated */\n\tnewblock = ext4_ext_pblock(&newex);\n\tallocated = ext4_ext_get_actual_len(&newex);\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\n\t/*\n\t * Update reserved blocks/metadata blocks after successful\n\t * block allocation which had been deferred till now.\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\t/*\n\t\t * Check how many clusters we had reserved this allocated range\n\t\t */\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\t\t\tmap->m_lblk, allocated);\n\t\tif (map_from_cluster) {\n\t\t\tif (reserved_clusters) {\n\t\t\t\t/*\n\t\t\t\t * We have clusters reserved for this range.\n\t\t\t\t * But since we are not doing actual allocation\n\t\t\t\t * and are simply using blocks from previously\n\t\t\t\t * allocated cluster, we should release the\n\t\t\t\t * reservation and not claim quota.\n\t\t\t\t */\n\t\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\treserved_clusters, 0);\n\t\t\t}\n\t\t} else {\n\t\t\tBUG_ON(allocated_clusters < reserved_clusters);\n\t\t\tif (reserved_clusters < allocated_clusters) {\n\t\t\t\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\t\t\t\tint reservation = allocated_clusters -\n\t\t\t\t\t\t  reserved_clusters;\n\t\t\t\t/*\n\t\t\t\t * It seems we claimed few clusters outside of\n\t\t\t\t * the range of this allocation. We should give\n\t\t\t\t * it back to the reservation pool. This can\n\t\t\t\t * happen in the following case:\n\t\t\t\t *\n\t\t\t\t * * Suppose s_cluster_ratio is 4 (i.e., each\n\t\t\t\t *   cluster has 4 blocks. Thus, the clusters\n\t\t\t\t *   are [0-3],[4-7],[8-11]...\n\t\t\t\t * * First comes delayed allocation write for\n\t\t\t\t *   logical blocks 10 & 11. Since there were no\n\t\t\t\t *   previous delayed allocated blocks in the\n\t\t\t\t *   range [8-11], we would reserve 1 cluster\n\t\t\t\t *   for this write.\n\t\t\t\t * * Next comes write for logical blocks 3 to 8.\n\t\t\t\t *   In this case, we will reserve 2 clusters\n\t\t\t\t *   (for [0-3] and [4-7]; and not for [8-11] as\n\t\t\t\t *   that range has a delayed allocated blocks.\n\t\t\t\t *   Thus total reserved clusters now becomes 3.\n\t\t\t\t * * Now, during the delayed allocation writeout\n\t\t\t\t *   time, we will first write blocks [3-8] and\n\t\t\t\t *   allocate 3 clusters for writing these\n\t\t\t\t *   blocks. Also, we would claim all these\n\t\t\t\t *   three clusters above.\n\t\t\t\t * * Now when we come here to writeout the\n\t\t\t\t *   blocks [10-11], we would expect to claim\n\t\t\t\t *   the reservation of 1 cluster we had made\n\t\t\t\t *   (and we would claim it since there are no\n\t\t\t\t *   more delayed allocated blocks in the range\n\t\t\t\t *   [8-11]. But our reserved cluster count had\n\t\t\t\t *   already gone to 0.\n\t\t\t\t *\n\t\t\t\t *   Thus, at the step 4 above when we determine\n\t\t\t\t *   that there are still some unwritten delayed\n\t\t\t\t *   allocated blocks outside of our current\n\t\t\t\t *   block range, we should increment the\n\t\t\t\t *   reserved clusters count so that when the\n\t\t\t\t *   remaining blocks finally gets written, we\n\t\t\t\t *   could claim them.\n\t\t\t\t */\n\t\t\t\tdquot_reserve_block(inode,\n\t\t\t\t\t\tEXT4_C2B(sbi, reservation));\n\t\t\t\tspin_lock(&ei->i_block_reservation_lock);\n\t\t\t\tei->i_reserved_data_blocks += reservation;\n\t\t\t\tspin_unlock(&ei->i_block_reservation_lock);\n\t\t\t}\n\t\t\t/*\n\t\t\t * We will claim quota for all newly allocated blocks.\n\t\t\t * We're updating the reserved space *after* the\n\t\t\t * correction above so we do not accidentally free\n\t\t\t * all the metadata reservation because we might\n\t\t\t * actually need it later on.\n\t\t\t */\n\t\t\text4_da_update_reserve_space(inode, allocated_clusters,\n\t\t\t\t\t\t\t1);\n\t\t}\n\t}\n\n\t/*\n\t * Cache the extent and update transaction to commit on fdatasync only\n\t * when it is _not_ an unwritten extent.\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_UNWRIT_EXT) == 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\telse\n\t\text4_update_inode_fsync_trans(handle, inode, 0);\nout:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\n\ttrace_ext4_ext_map_blocks_exit(inode, flags, map,\n\t\t\t\t       err ? err : allocated);\n\treturn err ? err : allocated;\n}\n\nvoid ext4_ext_truncate(handle_t *handle, struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t last_block;\n\tint err = 0;\n\n\t/*\n\t * TODO: optimization is possible here.\n\t * Probably we need not scan at all,\n\t * because page truncation is enough.\n\t */\n\n\t/* we have to know where to truncate from in crash case */\n\tEXT4_I(inode)->i_disksize = inode->i_size;\n\text4_mark_inode_dirty(handle, inode);\n\n\tlast_block = (inode->i_size + sb->s_blocksize - 1)\n\t\t\t>> EXT4_BLOCK_SIZE_BITS(sb);\nretry:\n\terr = ext4_es_remove_extent(inode, last_block,\n\t\t\t\t    EXT_MAX_BLOCKS - last_block);\n\tif (err == -ENOMEM) {\n\t\tcond_resched();\n\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\tgoto retry;\n\t}\n\tif (err) {\n\t\text4_std_error(inode->i_sb, err);\n\t\treturn;\n\t}\n\terr = ext4_ext_remove_space(inode, last_block, EXT_MAX_BLOCKS - 1);\n\text4_std_error(inode->i_sb, err);\n}\n\nstatic int ext4_alloc_file_blocks(struct file *file, ext4_lblk_t offset,\n\t\t\t\t  ext4_lblk_t len, loff_t new_size,\n\t\t\t\t  int flags, int mode)\n{\n\tstruct inode *inode = file_inode(file);\n\thandle_t *handle;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint retries = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits;\n\tloff_t epos;\n\n\tmap.m_lblk = offset;\n\tmap.m_len = len;\n\t/*\n\t * Don't normalize the request if it can fit in one extent so\n\t * that it doesn't get unnecessarily split into multiple\n\t * extents.\n\t */\n\tif (len <= EXT_UNWRITTEN_MAX_LEN)\n\t\tflags |= EXT4_GET_BLOCKS_NO_NORMALIZE;\n\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, len);\n\nretry:\n\twhile (ret >= 0 && len) {\n\t\thandle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,\n\t\t\t\t\t    credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map, flags);\n\t\tif (ret <= 0) {\n\t\t\text4_debug(\"inode #%lu: block %u: len %u: \"\n\t\t\t\t   \"ext4_ext_map_blocks returned %d\",\n\t\t\t\t   inode->i_ino, map.m_lblk,\n\t\t\t\t   map.m_len, ret);\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\t\tbreak;\n\t\t}\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = len = len - ret;\n\t\tepos = (loff_t)map.m_lblk << inode->i_blkbits;\n\t\tinode->i_ctime = ext4_current_time(inode);\n\t\tif (new_size) {\n\t\t\tif (epos > new_size)\n\t\t\t\tepos = new_size;\n\t\t\tif (ext4_update_inode_size(inode, epos) & 0x1)\n\t\t\t\tinode->i_mtime = inode->i_ctime;\n\t\t} else {\n\t\t\tif (epos > inode->i_size)\n\t\t\t\text4_set_inode_flag(inode,\n\t\t\t\t\t\t    EXT4_INODE_EOFBLOCKS);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret2)\n\t\t\tbreak;\n\t}\n\tif (ret == -ENOSPC &&\n\t\t\text4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\n\treturn ret > 0 ? ret2 : ret;\n}\n\nstatic long ext4_zero_range(struct file *file, loff_t offset,\n\t\t\t    loff_t len, int mode)\n{\n\tstruct inode *inode = file_inode(file);\n\thandle_t *handle = NULL;\n\tunsigned int max_blocks;\n\tloff_t new_size = 0;\n\tint ret = 0;\n\tint flags;\n\tint credits;\n\tint partial_begin, partial_end;\n\tloff_t start, end;\n\text4_lblk_t lblk;\n\tstruct address_space *mapping = inode->i_mapping;\n\tunsigned int blkbits = inode->i_blkbits;\n\n\ttrace_ext4_zero_range(inode, offset, len, mode);\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Write out all dirty pages to avoid race conditions\n\t * Then release them.\n\t */\n\tif (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {\n\t\tret = filemap_write_and_wait_range(mapping, offset,\n\t\t\t\t\t\t   offset + len - 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Round up offset. This is not fallocate, we neet to zero out\n\t * blocks, so convert interior block aligned part of the range to\n\t * unwritten and possibly manually zero out unaligned parts of the\n\t * range.\n\t */\n\tstart = round_up(offset, 1 << blkbits);\n\tend = round_down((offset + len), 1 << blkbits);\n\n\tif (start < offset || end > offset + len)\n\t\treturn -EINVAL;\n\tpartial_begin = offset & ((1 << blkbits) - 1);\n\tpartial_end = (offset + len) & ((1 << blkbits) - 1);\n\n\tlblk = start >> blkbits;\n\tmax_blocks = (end >> blkbits);\n\tif (max_blocks < lblk)\n\t\tmax_blocks = 0;\n\telse\n\t\tmax_blocks -= lblk;\n\n\tflags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT |\n\t\tEXT4_GET_BLOCKS_CONVERT_UNWRITTEN |\n\t\tEXT4_EX_NOCACHE;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\n\tmutex_lock(&inode->i_mutex);\n\n\t/*\n\t * Indirect files do not support unwritten extnets\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t     offset + len > i_size_read(inode)) {\n\t\tnew_size = offset + len;\n\t\tret = inode_newsize_ok(inode, new_size);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\t\t/*\n\t\t * If we have a partial block after EOF we have to allocate\n\t\t * the entire block.\n\t\t */\n\t\tif (partial_end)\n\t\t\tmax_blocks += 1;\n\t}\n\n\tif (max_blocks > 0) {\n\n\t\t/* Now release the pages and zero block aligned part of pages*/\n\t\ttruncate_pagecache_range(inode, start, end - 1);\n\t\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\n\t\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\t\text4_inode_block_unlocked_dio(inode);\n\t\tinode_dio_wait(inode);\n\n\t\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,\n\t\t\t\t\t     flags, mode);\n\t\tif (ret)\n\t\t\tgoto out_dio;\n\t\t/*\n\t\t * Remove entire range from the extent status tree.\n\t\t *\n\t\t * ext4_es_remove_extent(inode, lblk, max_blocks) is\n\t\t * NOT sufficient.  I'm not sure why this is the case,\n\t\t * but let's be conservative and remove the extent\n\t\t * status tree for the entire inode.  There should be\n\t\t * no outstanding delalloc extents thanks to the\n\t\t * filemap_write_and_wait_range() call above.\n\t\t */\n\t\tret = ext4_es_remove_extent(inode, 0, EXT_MAX_BLOCKS);\n\t\tif (ret)\n\t\t\tgoto out_dio;\n\t}\n\tif (!partial_begin && !partial_end)\n\t\tgoto out_dio;\n\n\t/*\n\t * In worst case we have to writeout two nonadjacent unwritten\n\t * blocks and update the inode\n\t */\n\tcredits = (2 * ext4_ext_index_trans_blocks(inode, 2)) + 1;\n\tif (ext4_should_journal_data(inode))\n\t\tcredits += 2;\n\thandle = ext4_journal_start(inode, EXT4_HT_MISC, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\text4_std_error(inode->i_sb, ret);\n\t\tgoto out_dio;\n\t}\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\tif (new_size) {\n\t\text4_update_inode_size(inode, new_size);\n\t} else {\n\t\t/*\n\t\t* Mark that we allocate beyond EOF so the subsequent truncate\n\t\t* can proceed even if the new size is the same as i_size.\n\t\t*/\n\t\tif ((offset + len) > i_size_read(inode))\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\t}\n\text4_mark_inode_dirty(handle, inode);\n\n\t/* Zero out partial block at the edges of the range */\n\tret = ext4_zero_partial_blocks(handle, inode, offset, len);\n\n\tif (file->f_flags & O_SYNC)\n\t\text4_handle_sync(handle);\n\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}\n\n/*\n * preallocate space for a file. This implements ext4's fallocate file\n * operation, which gets called from sys_fallocate system call.\n * For block-mapped files, posix_fallocate should fall back to the method\n * of writing zeroes to the required new blocks (the same behavior which is\n * expected for file systems which do not support fallocate() system call).\n */\nlong ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)\n{\n\tstruct inode *inode = file_inode(file);\n\tloff_t new_size = 0;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint flags;\n\text4_lblk_t lblk;\n\tunsigned int blkbits = inode->i_blkbits;\n\n\t/* Return error if mode is not supported */\n\tif (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |\n\t\t     FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE))\n\t\treturn -EOPNOTSUPP;\n\n\tif (mode & FALLOC_FL_PUNCH_HOLE)\n\t\treturn ext4_punch_hole(inode, offset, len);\n\n\tret = ext4_convert_inline_data(inode);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * currently supporting (pre)allocate mode for extent-based\n\t * files _only_\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn -EOPNOTSUPP;\n\n\tif (mode & FALLOC_FL_COLLAPSE_RANGE)\n\t\treturn ext4_collapse_range(inode, offset, len);\n\n\tif (mode & FALLOC_FL_ZERO_RANGE)\n\t\treturn ext4_zero_range(file, offset, len, mode);\n\n\ttrace_ext4_fallocate_enter(inode, offset, len, mode);\n\tlblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)\n\t\t- lblk;\n\n\tflags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t     offset + len > i_size_read(inode)) {\n\t\tnew_size = offset + len;\n\t\tret = inode_newsize_ok(inode, new_size);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,\n\t\t\t\t     flags, mode);\n\tif (ret)\n\t\tgoto out;\n\n\tif (file->f_flags & O_SYNC && EXT4_SB(inode->i_sb)->s_journal) {\n\t\tret = jbd2_complete_transaction(EXT4_SB(inode->i_sb)->s_journal,\n\t\t\t\t\t\tEXT4_I(inode)->i_sync_tid);\n\t}\nout:\n\tmutex_unlock(&inode->i_mutex);\n\ttrace_ext4_fallocate_exit(inode, offset, max_blocks, ret);\n\treturn ret;\n}\n\n/*\n * This function convert a range of blocks to written extents\n * The caller of this function will pass the start offset and the size.\n * all unwritten extents within this range will be converted to\n * written extents.\n *\n * This function is called from the direct IO end io call back\n * function, to convert the fallocated extents after IO is completed.\n * Returns 0 on success.\n */\nint ext4_convert_unwritten_extents(handle_t *handle, struct inode *inode,\n\t\t\t\t   loff_t offset, ssize_t len)\n{\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\tmap.m_lblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = ((EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits) -\n\t\t      map.m_lblk);\n\t/*\n\t * This is somewhat ugly but the idea is clear: When transaction is\n\t * reserved, everything goes into it. Otherwise we rather start several\n\t * smaller transactions for conversion of each extent separately.\n\t */\n\tif (handle) {\n\t\thandle = ext4_journal_start_reserved(handle,\n\t\t\t\t\t\t     EXT4_HT_EXT_CONVERT);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tcredits = 0;\n\t} else {\n\t\t/*\n\t\t * credits to insert 1 extent into extent tree\n\t\t */\n\t\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\t}\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = (max_blocks -= ret);\n\t\tif (credits) {\n\t\t\thandle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,\n\t\t\t\t\t\t    credits);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\tret = PTR_ERR(handle);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map,\n\t\t\t\t      EXT4_GET_BLOCKS_IO_CONVERT_EXT);\n\t\tif (ret <= 0)\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"inode #%lu: block %u: len %u: \"\n\t\t\t\t     \"ext4_ext_map_blocks returned %d\",\n\t\t\t\t     inode->i_ino, map.m_lblk,\n\t\t\t\t     map.m_len, ret);\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tif (credits)\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret <= 0 || ret2)\n\t\t\tbreak;\n\t}\n\tif (!credits)\n\t\tret2 = ext4_journal_stop(handle);\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * If newes is not existing extent (newes->ec_pblk equals zero) find\n * delayed extent at start of newes and update newes accordingly and\n * return start of the next delayed extent.\n *\n * If newes is existing extent (newes->ec_pblk is not equal zero)\n * return start of next delayed extent or EXT_MAX_BLOCKS if no delayed\n * extent found. Leave newes unmodified.\n */\nstatic int ext4_find_delayed_extent(struct inode *inode,\n\t\t\t\t    struct extent_status *newes)\n{\n\tstruct extent_status es;\n\text4_lblk_t block, next_del;\n\n\tif (newes->es_pblk == 0) {\n\t\text4_es_find_delayed_extent_range(inode, newes->es_lblk,\n\t\t\t\tnewes->es_lblk + newes->es_len - 1, &es);\n\n\t\t/*\n\t\t * No extent in extent-tree contains block @newes->es_pblk,\n\t\t * then the block may stay in 1)a hole or 2)delayed-extent.\n\t\t */\n\t\tif (es.es_len == 0)\n\t\t\t/* A hole found. */\n\t\t\treturn 0;\n\n\t\tif (es.es_lblk > newes->es_lblk) {\n\t\t\t/* A hole found. */\n\t\t\tnewes->es_len = min(es.es_lblk - newes->es_lblk,\n\t\t\t\t\t    newes->es_len);\n\t\t\treturn 0;\n\t\t}\n\n\t\tnewes->es_len = es.es_lblk + es.es_len - newes->es_lblk;\n\t}\n\n\tblock = newes->es_lblk + newes->es_len;\n\text4_es_find_delayed_extent_range(inode, block, EXT_MAX_BLOCKS, &es);\n\tif (es.es_len == 0)\n\t\tnext_del = EXT_MAX_BLOCKS;\n\telse\n\t\tnext_del = es.es_lblk;\n\n\treturn next_del;\n}\n/* fiemap flags we can handle specified here */\n#define EXT4_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)\n\nstatic int ext4_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\t__u64 physical = 0;\n\t__u64 length;\n\t__u32 flags = FIEMAP_EXTENT_LAST;\n\tint blockbits = inode->i_sb->s_blocksize_bits;\n\tint error = 0;\n\n\t/* in-inode? */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tstruct ext4_iloc iloc;\n\t\tint offset;\t/* offset of xattr in inode */\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error)\n\t\t\treturn error;\n\t\tphysical = (__u64)iloc.bh->b_blocknr << blockbits;\n\t\toffset = EXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\tEXT4_I(inode)->i_extra_isize;\n\t\tphysical += offset;\n\t\tlength = EXT4_SB(inode->i_sb)->s_inode_size - offset;\n\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t\tbrelse(iloc.bh);\n\t} else { /* external block */\n\t\tphysical = (__u64)EXT4_I(inode)->i_file_acl << blockbits;\n\t\tlength = inode->i_sb->s_blocksize;\n\t}\n\n\tif (physical)\n\t\terror = fiemap_fill_next_extent(fieinfo, 0, physical,\n\t\t\t\t\t\tlength, flags);\n\treturn (error < 0 ? error : 0);\n}\n\nint ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\text4_lblk_t start_blk;\n\tint error = 0;\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tint has_inline = 1;\n\n\t\terror = ext4_inline_data_fiemap(inode, fieinfo, &has_inline,\n\t\t\t\t\t\tstart, len);\n\n\t\tif (has_inline)\n\t\t\treturn error;\n\t}\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) {\n\t\terror = ext4_ext_precache(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/* fallback to generic here if not in extents fmt */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn generic_block_fiemap(inode, fieinfo, start, len,\n\t\t\text4_get_block);\n\n\tif (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS))\n\t\treturn -EBADR;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\terror = ext4_xattr_fiemap(inode, fieinfo);\n\t} else {\n\t\text4_lblk_t len_blks;\n\t\t__u64 last_blk;\n\n\t\tstart_blk = start >> inode->i_sb->s_blocksize_bits;\n\t\tlast_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;\n\t\tif (last_blk >= EXT_MAX_BLOCKS)\n\t\t\tlast_blk = EXT_MAX_BLOCKS-1;\n\t\tlen_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;\n\n\t\t/*\n\t\t * Walk the extent tree gathering extent information\n\t\t * and pushing extents back to the user.\n\t\t */\n\t\terror = ext4_fill_fiemap_extents(inode, start_blk,\n\t\t\t\t\t\t len_blks, fieinfo);\n\t}\n\treturn error;\n}\n\n/*\n * ext4_access_path:\n * Function to access the path buffer for marking it dirty.\n * It also checks if there are sufficient credits left in the journal handle\n * to update path.\n */\nstatic int\next4_access_path(handle_t *handle, struct inode *inode,\n\t\tstruct ext4_ext_path *path)\n{\n\tint credits, err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\n\t/*\n\t * Check if need to extend journal credits\n\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t * descriptor) for each block group; assume two block\n\t * groups\n\t */\n\tif (handle->h_buffer_credits < 7) {\n\t\tcredits = ext4_writepage_trans_blocks(inode);\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\t/* EAGAIN is success */\n\t\tif (err && err != -EAGAIN)\n\t\t\treturn err;\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path);\n\treturn err;\n}\n\n/*\n * ext4_ext_shift_path_extents:\n * Shift the extents of a path structure lying between path[depth].p_ext\n * and EXT_LAST_EXTENT(path[depth].p_hdr) downwards, by subtracting shift\n * from starting block for each extent.\n */\nstatic int\next4_ext_shift_path_extents(struct ext4_ext_path *path, ext4_lblk_t shift,\n\t\t\t    struct inode *inode, handle_t *handle,\n\t\t\t    ext4_lblk_t *start)\n{\n\tint depth, err = 0;\n\tstruct ext4_extent *ex_start, *ex_last;\n\tbool update = 0;\n\tdepth = path->p_depth;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\tex_start = path[depth].p_ext;\n\t\t\tif (!ex_start)\n\t\t\t\treturn -EIO;\n\n\t\t\tex_last = EXT_LAST_EXTENT(path[depth].p_hdr);\n\n\t\t\terr = ext4_access_path(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tif (ex_start == EXT_FIRST_EXTENT(path[depth].p_hdr))\n\t\t\t\tupdate = 1;\n\n\t\t\t*start = le32_to_cpu(ex_last->ee_block) +\n\t\t\t\text4_ext_get_actual_len(ex_last);\n\n\t\t\twhile (ex_start <= ex_last) {\n\t\t\t\tle32_add_cpu(&ex_start->ee_block, -shift);\n\t\t\t\t/* Try to merge to the left. */\n\t\t\t\tif ((ex_start >\n\t\t\t\t     EXT_FIRST_EXTENT(path[depth].p_hdr)) &&\n\t\t\t\t    ext4_ext_try_to_merge_right(inode,\n\t\t\t\t\t\t\tpath, ex_start - 1))\n\t\t\t\t\tex_last--;\n\t\t\t\telse\n\t\t\t\t\tex_start++;\n\t\t\t}\n\t\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tif (--depth < 0 || !update)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/* Update index too */\n\t\terr = ext4_access_path(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tle32_add_cpu(&path[depth].p_idx->ei_block, -shift);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* we are done if current index is not a starting index */\n\t\tif (path[depth].p_idx != EXT_FIRST_INDEX(path[depth].p_hdr))\n\t\t\tbreak;\n\n\t\tdepth--;\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_shift_extents:\n * All the extents which lies in the range from start to the last allocated\n * block for the file are shifted downwards by shift blocks.\n * On success, 0 is returned, error otherwise.\n */\nstatic int\next4_ext_shift_extents(struct inode *inode, handle_t *handle,\n\t\t       ext4_lblk_t start, ext4_lblk_t shift)\n{\n\tstruct ext4_ext_path *path;\n\tint ret = 0, depth;\n\tstruct ext4_extent *extent;\n\text4_lblk_t stop_block;\n\text4_lblk_t ex_start, ex_end;\n\n\t/* Let path point to the last extent */\n\tpath = ext4_find_extent(inode, EXT_MAX_BLOCKS - 1, NULL, 0);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tdepth = path->p_depth;\n\textent = path[depth].p_ext;\n\tif (!extent)\n\t\tgoto out;\n\n\tstop_block = le32_to_cpu(extent->ee_block) +\n\t\t\text4_ext_get_actual_len(extent);\n\n\t/* Nothing to shift, if hole is at the end of file */\n\tif (start >= stop_block)\n\t\tgoto out;\n\n\t/*\n\t * Don't start shifting extents until we make sure the hole is big\n\t * enough to accomodate the shift.\n\t */\n\tpath = ext4_find_extent(inode, start - 1, &path, 0);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\tdepth = path->p_depth;\n\textent =  path[depth].p_ext;\n\tif (extent) {\n\t\tex_start = le32_to_cpu(extent->ee_block);\n\t\tex_end = le32_to_cpu(extent->ee_block) +\n\t\t\text4_ext_get_actual_len(extent);\n\t} else {\n\t\tex_start = 0;\n\t\tex_end = 0;\n\t}\n\n\tif ((start == ex_start && shift > ex_start) ||\n\t    (shift > start - ex_end))\n\t\treturn -EINVAL;\n\n\t/* Its safe to start updating extents */\n\twhile (start < stop_block) {\n\t\tpath = ext4_find_extent(inode, start, &path, 0);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = path->p_depth;\n\t\textent = path[depth].p_ext;\n\t\tif (!extent) {\n\t\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t\t (unsigned long) start);\n\t\t\treturn -EIO;\n\t\t}\n\t\tif (start > le32_to_cpu(extent->ee_block)) {\n\t\t\t/* Hole, move to the next extent */\n\t\t\tif (extent < EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t\t\tpath[depth].p_ext++;\n\t\t\t} else {\n\t\t\t\tstart = ext4_ext_next_allocated_block(path);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tret = ext4_ext_shift_path_extents(path, shift, inode,\n\t\t\t\thandle, &start);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn ret;\n}\n\n/*\n * ext4_collapse_range:\n * This implements the fallocate's collapse range functionality for ext4\n * Returns: 0 and non-zero on error.\n */\nint ext4_collapse_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t punch_start, punch_stop;\n\thandle_t *handle;\n\tunsigned int credits;\n\tloff_t new_size, ioffset;\n\tint ret;\n\n\t/* Collapse range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t    len & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\ttrace_ext4_collapse_range(inode, offset, len);\n\n\tpunch_start = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tpunch_stop = (offset + len) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Need to round down offset to be aligned with page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\t\t\t   LLONG_MAX);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Take mutex lock */\n\tmutex_lock(&inode->i_mutex);\n\n\t/*\n\t * There is no need to overlap collapse range with EOF, in which case\n\t * it is effectively a truncate operation\n\t */\n\tif (offset + len >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\ttruncate_pagecache(inode, ioffset);\n\n\t/* Wait for existing dio to complete */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_dio;\n\t}\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tret = ext4_es_remove_extent(inode, punch_start,\n\t\t\t\t    EXT_MAX_BLOCKS - punch_start);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tret = ext4_ext_remove_space(inode, punch_start, punch_stop - 1);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\text4_discard_preallocations(inode);\n\n\tret = ext4_ext_shift_extents(inode, handle, punch_stop,\n\t\t\t\t     punch_stop - punch_start);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tnew_size = i_size_read(inode) - len;\n\ti_size_write(inode, new_size);\n\tEXT4_I(inode)->i_disksize = new_size;\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\nout_stop:\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}\n\n/**\n * ext4_swap_extents - Swap extents between two inodes\n *\n * @inode1:\tFirst inode\n * @inode2:\tSecond inode\n * @lblk1:\tStart block for first inode\n * @lblk2:\tStart block for second inode\n * @count:\tNumber of blocks to swap\n * @mark_unwritten: Mark second inode's extents as unwritten after swap\n * @erp:\tPointer to save error value\n *\n * This helper routine does exactly what is promise \"swap extents\". All other\n * stuff such as page-cache locking consistency, bh mapping consistency or\n * extent's data copying must be performed by caller.\n * Locking:\n * \t\ti_mutex is held for both inodes\n * \t\ti_data_sem is locked for write for both inodes\n * Assumptions:\n *\t\tAll pages from requested range are locked for both inodes\n */\nint\next4_swap_extents(handle_t *handle, struct inode *inode1,\n\t\t     struct inode *inode2, ext4_lblk_t lblk1, ext4_lblk_t lblk2,\n\t\t  ext4_lblk_t count, int unwritten, int *erp)\n{\n\tstruct ext4_ext_path *path1 = NULL;\n\tstruct ext4_ext_path *path2 = NULL;\n\tint replaced_count = 0;\n\n\tBUG_ON(!rwsem_is_locked(&EXT4_I(inode1)->i_data_sem));\n\tBUG_ON(!rwsem_is_locked(&EXT4_I(inode2)->i_data_sem));\n\tBUG_ON(!mutex_is_locked(&inode1->i_mutex));\n\tBUG_ON(!mutex_is_locked(&inode1->i_mutex));\n\n\t*erp = ext4_es_remove_extent(inode1, lblk1, count);\n\tif (unlikely(*erp))\n\t\treturn 0;\n\t*erp = ext4_es_remove_extent(inode2, lblk2, count);\n\tif (unlikely(*erp))\n\t\treturn 0;\n\n\twhile (count) {\n\t\tstruct ext4_extent *ex1, *ex2, tmp_ex;\n\t\text4_lblk_t e1_blk, e2_blk;\n\t\tint e1_len, e2_len, len;\n\t\tint split = 0;\n\n\t\tpath1 = ext4_find_extent(inode1, lblk1, NULL, EXT4_EX_NOCACHE);\n\t\tif (unlikely(IS_ERR(path1))) {\n\t\t\t*erp = PTR_ERR(path1);\n\t\t\tpath1 = NULL;\n\t\tfinish:\n\t\t\tcount = 0;\n\t\t\tgoto repeat;\n\t\t}\n\t\tpath2 = ext4_find_extent(inode2, lblk2, NULL, EXT4_EX_NOCACHE);\n\t\tif (unlikely(IS_ERR(path2))) {\n\t\t\t*erp = PTR_ERR(path2);\n\t\t\tpath2 = NULL;\n\t\t\tgoto finish;\n\t\t}\n\t\tex1 = path1[path1->p_depth].p_ext;\n\t\tex2 = path2[path2->p_depth].p_ext;\n\t\t/* Do we have somthing to swap ? */\n\t\tif (unlikely(!ex2 || !ex1))\n\t\t\tgoto finish;\n\n\t\te1_blk = le32_to_cpu(ex1->ee_block);\n\t\te2_blk = le32_to_cpu(ex2->ee_block);\n\t\te1_len = ext4_ext_get_actual_len(ex1);\n\t\te2_len = ext4_ext_get_actual_len(ex2);\n\n\t\t/* Hole handling */\n\t\tif (!in_range(lblk1, e1_blk, e1_len) ||\n\t\t    !in_range(lblk2, e2_blk, e2_len)) {\n\t\t\text4_lblk_t next1, next2;\n\n\t\t\t/* if hole after extent, then go to next extent */\n\t\t\tnext1 = ext4_ext_next_allocated_block(path1);\n\t\t\tnext2 = ext4_ext_next_allocated_block(path2);\n\t\t\t/* If hole before extent, then shift to that extent */\n\t\t\tif (e1_blk > lblk1)\n\t\t\t\tnext1 = e1_blk;\n\t\t\tif (e2_blk > lblk2)\n\t\t\t\tnext2 = e1_blk;\n\t\t\t/* Do we have something to swap */\n\t\t\tif (next1 == EXT_MAX_BLOCKS || next2 == EXT_MAX_BLOCKS)\n\t\t\t\tgoto finish;\n\t\t\t/* Move to the rightest boundary */\n\t\t\tlen = next1 - lblk1;\n\t\t\tif (len < next2 - lblk2)\n\t\t\t\tlen = next2 - lblk2;\n\t\t\tif (len > count)\n\t\t\t\tlen = count;\n\t\t\tlblk1 += len;\n\t\t\tlblk2 += len;\n\t\t\tcount -= len;\n\t\t\tgoto repeat;\n\t\t}\n\n\t\t/* Prepare left boundary */\n\t\tif (e1_blk < lblk1) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode1,\n\t\t\t\t\t\t&path1, lblk1, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\tif (e2_blk < lblk2) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode2,\n\t\t\t\t\t\t&path2,  lblk2, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\t/* ext4_split_extent_at() may result in leaf extent split,\n\t\t * path must to be revalidated. */\n\t\tif (split)\n\t\t\tgoto repeat;\n\n\t\t/* Prepare right boundary */\n\t\tlen = count;\n\t\tif (len > e1_blk + e1_len - lblk1)\n\t\t\tlen = e1_blk + e1_len - lblk1;\n\t\tif (len > e2_blk + e2_len - lblk2)\n\t\t\tlen = e2_blk + e2_len - lblk2;\n\n\t\tif (len != e1_len) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode1,\n\t\t\t\t\t\t&path1, lblk1 + len, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\tif (len != e2_len) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode2,\n\t\t\t\t\t\t&path2, lblk2 + len, 0);\n\t\t\tif (*erp)\n\t\t\t\tgoto finish;\n\t\t}\n\t\t/* ext4_split_extent_at() may result in leaf extent split,\n\t\t * path must to be revalidated. */\n\t\tif (split)\n\t\t\tgoto repeat;\n\n\t\tBUG_ON(e2_len != e1_len);\n\t\t*erp = ext4_ext_get_access(handle, inode1, path1 + path1->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\t*erp = ext4_ext_get_access(handle, inode2, path2 + path2->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\n\t\t/* Both extents are fully inside boundaries. Swap it now */\n\t\ttmp_ex = *ex1;\n\t\text4_ext_store_pblock(ex1, ext4_ext_pblock(ex2));\n\t\text4_ext_store_pblock(ex2, ext4_ext_pblock(&tmp_ex));\n\t\tex1->ee_len = cpu_to_le16(e2_len);\n\t\tex2->ee_len = cpu_to_le16(e1_len);\n\t\tif (unwritten)\n\t\t\text4_ext_mark_unwritten(ex2);\n\t\tif (ext4_ext_is_unwritten(&tmp_ex))\n\t\t\text4_ext_mark_unwritten(ex1);\n\n\t\text4_ext_try_to_merge(handle, inode2, path2, ex2);\n\t\text4_ext_try_to_merge(handle, inode1, path1, ex1);\n\t\t*erp = ext4_ext_dirty(handle, inode2, path2 +\n\t\t\t\t      path2->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\t*erp = ext4_ext_dirty(handle, inode1, path1 +\n\t\t\t\t      path1->p_depth);\n\t\t/*\n\t\t * Looks scarry ah..? second inode already points to new blocks,\n\t\t * and it was successfully dirtied. But luckily error may happen\n\t\t * only due to journal error, so full transaction will be\n\t\t * aborted anyway.\n\t\t */\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\tlblk1 += len;\n\t\tlblk2 += len;\n\t\treplaced_count += len;\n\t\tcount -= len;\n\n\trepeat:\n\t\text4_ext_drop_refs(path1);\n\t\tkfree(path1);\n\t\text4_ext_drop_refs(path2);\n\t\tkfree(path2);\n\t\tpath1 = path2 = NULL;\n\t}\n\treturn replaced_count;\n}\n"], "fixing_code": ["/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n *\n * Architecture independence:\n *   Copyright (c) 2005, Bull S.A.\n *   Written by Pierre Peiffer <pierre.peiffer@bull.net>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n */\n\n/*\n * Extents support for EXT4\n *\n * TODO:\n *   - ext4*_error() should be used in some situations\n *   - analyze all BUG()/BUG_ON(), use -EIO where appropriate\n *   - smart tree reduction\n */\n\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/fiemap.h>\n#include \"ext4_jbd2.h\"\n#include \"ext4_extents.h\"\n#include \"xattr.h\"\n\n#include <trace/events/ext4.h>\n\n/*\n * used by extent splitting.\n */\n#define EXT4_EXT_MAY_ZEROOUT\t0x1  /* safe to zeroout if split fails \\\n\t\t\t\t\tdue to ENOSPC */\n#define EXT4_EXT_MARK_UNWRIT1\t0x2  /* mark first half unwritten */\n#define EXT4_EXT_MARK_UNWRIT2\t0x4  /* mark second half unwritten */\n\n#define EXT4_EXT_DATA_VALID1\t0x8  /* first half contains valid data */\n#define EXT4_EXT_DATA_VALID2\t0x10 /* second half contains valid data */\n\nstatic __le32 ext4_extent_block_csum(struct inode *inode,\n\t\t\t\t     struct ext4_extent_header *eh)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t__u32 csum;\n\n\tcsum = ext4_chksum(sbi, ei->i_csum_seed, (__u8 *)eh,\n\t\t\t   EXT4_EXTENT_TAIL_OFFSET(eh));\n\treturn cpu_to_le32(csum);\n}\n\nstatic int ext4_extent_block_csum_verify(struct inode *inode,\n\t\t\t\t\t struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!ext4_has_metadata_csum(inode->i_sb))\n\t\treturn 1;\n\n\tet = find_ext4_extent_tail(eh);\n\tif (et->et_checksum != ext4_extent_block_csum(inode, eh))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic void ext4_extent_block_csum_set(struct inode *inode,\n\t\t\t\t       struct ext4_extent_header *eh)\n{\n\tstruct ext4_extent_tail *et;\n\n\tif (!ext4_has_metadata_csum(inode->i_sb))\n\t\treturn;\n\n\tet = find_ext4_extent_tail(eh);\n\tet->et_checksum = ext4_extent_block_csum(inode, eh);\n}\n\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t\tstruct inode *inode,\n\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\tint split_flag,\n\t\t\t\tint flags);\n\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path **ppath,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags);\n\nstatic int ext4_find_delayed_extent(struct inode *inode,\n\t\t\t\t    struct extent_status *newes);\n\nstatic int ext4_ext_truncate_extend_restart(handle_t *handle,\n\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t    int needed)\n{\n\tint err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (handle->h_buffer_credits > needed)\n\t\treturn 0;\n\terr = ext4_journal_extend(handle, needed);\n\tif (err <= 0)\n\t\treturn err;\n\terr = ext4_truncate_restart_trans(handle, inode, needed);\n\tif (err == 0)\n\t\terr = -EAGAIN;\n\n\treturn err;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n */\nstatic int ext4_ext_get_access(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\tBUFFER_TRACE(path->p_bh, \"get_write_access\");\n\t\treturn ext4_journal_get_write_access(handle, path->p_bh);\n\t}\n\t/* path points to leaf/index in inode body */\n\t/* we use in-core data, no need to protect them */\n\treturn 0;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n *  - EIO\n */\nint __ext4_ext_dirty(const char *where, unsigned int line, handle_t *handle,\n\t\t     struct inode *inode, struct ext4_ext_path *path)\n{\n\tint err;\n\n\tWARN_ON(!rwsem_is_locked(&EXT4_I(inode)->i_data_sem));\n\tif (path->p_bh) {\n\t\text4_extent_block_csum_set(inode, ext_block_hdr(path->p_bh));\n\t\t/* path points to block */\n\t\terr = __ext4_handle_dirty_metadata(where, line, handle,\n\t\t\t\t\t\t   inode, path->p_bh);\n\t} else {\n\t\t/* path points to leaf/index in inode body */\n\t\terr = ext4_mark_inode_dirty(handle, inode);\n\t}\n\treturn err;\n}\n\nstatic ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      ext4_lblk_t block)\n{\n\tif (path) {\n\t\tint depth = path->p_depth;\n\t\tstruct ext4_extent *ex;\n\n\t\t/*\n\t\t * Try to predict block placement assuming that we are\n\t\t * filling in a file which will eventually be\n\t\t * non-sparse --- i.e., in the case of libbfd writing\n\t\t * an ELF object sections out-of-order but in a way\n\t\t * the eventually results in a contiguous object or\n\t\t * executable file, or some database extending a table\n\t\t * space file.  However, this is actually somewhat\n\t\t * non-ideal if we are writing a sparse file such as\n\t\t * qemu or KVM writing a raw image file that is going\n\t\t * to stay fairly sparse, since it will end up\n\t\t * fragmenting the file system's free space.  Maybe we\n\t\t * should have some hueristics or some way to allow\n\t\t * userspace to pass a hint to file system,\n\t\t * especially if the latter case turns out to be\n\t\t * common.\n\t\t */\n\t\tex = path[depth].p_ext;\n\t\tif (ex) {\n\t\t\text4_fsblk_t ext_pblk = ext4_ext_pblock(ex);\n\t\t\text4_lblk_t ext_block = le32_to_cpu(ex->ee_block);\n\n\t\t\tif (block > ext_block)\n\t\t\t\treturn ext_pblk + (block - ext_block);\n\t\t\telse\n\t\t\t\treturn ext_pblk - (ext_block - block);\n\t\t}\n\n\t\t/* it looks like index is empty;\n\t\t * try to find starting block from index itself */\n\t\tif (path[depth].p_bh)\n\t\t\treturn path[depth].p_bh->b_blocknr;\n\t}\n\n\t/* OK. use inode's group */\n\treturn ext4_inode_to_goal_block(inode);\n}\n\n/*\n * Allocation for a meta data block\n */\nstatic ext4_fsblk_t\next4_ext_new_meta_block(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path,\n\t\t\tstruct ext4_extent *ex, int *err, unsigned int flags)\n{\n\text4_fsblk_t goal, newblock;\n\n\tgoal = ext4_ext_find_goal(inode, path, le32_to_cpu(ex->ee_block));\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, flags,\n\t\t\t\t\tNULL, err);\n\treturn newblock;\n}\n\nstatic inline int ext4_ext_space_block(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 6)\n\t\tsize = 6;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_block_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 5)\n\t\tsize = 5;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 3)\n\t\tsize = 3;\n#endif\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent_idx);\n#ifdef AGGRESSIVE_TEST\n\tif (!check && size > 4)\n\t\tsize = 4;\n#endif\n\treturn size;\n}\n\nstatic inline int\next4_force_split_extent_at(handle_t *handle, struct inode *inode,\n\t\t\t   struct ext4_ext_path **ppath, ext4_lblk_t lblk,\n\t\t\t   int nofail)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tint unwritten = ext4_ext_is_unwritten(path[path->p_depth].p_ext);\n\n\treturn ext4_split_extent_at(handle, inode, ppath, lblk, unwritten ?\n\t\t\tEXT4_EXT_MARK_UNWRIT1|EXT4_EXT_MARK_UNWRIT2 : 0,\n\t\t\tEXT4_EX_NOCACHE | EXT4_GET_BLOCKS_PRE_IO |\n\t\t\t(nofail ? EXT4_GET_BLOCKS_METADATA_NOFAIL:0));\n}\n\n/*\n * Calculate the number of metadata blocks needed\n * to allocate @blocks\n * Worse case is one block per extent\n */\nint ext4_ext_calc_metadata_amount(struct inode *inode, ext4_lblk_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint idxs;\n\n\tidxs = ((inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t/ sizeof(struct ext4_extent_idx));\n\n\t/*\n\t * If the new delayed allocation block is contiguous with the\n\t * previous da block, it can share index blocks with the\n\t * previous block, so we only need to allocate a new index\n\t * block every idxs leaf blocks.  At ldxs**2 blocks, we need\n\t * an additional index block, and at ldxs**3 blocks, yet\n\t * another index blocks.\n\t */\n\tif (ei->i_da_metadata_calc_len &&\n\t    ei->i_da_metadata_calc_last_lblock+1 == lblock) {\n\t\tint num = 0;\n\n\t\tif ((ei->i_da_metadata_calc_len % idxs) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {\n\t\t\tnum++;\n\t\t\tei->i_da_metadata_calc_len = 0;\n\t\t} else\n\t\t\tei->i_da_metadata_calc_len++;\n\t\tei->i_da_metadata_calc_last_lblock++;\n\t\treturn num;\n\t}\n\n\t/*\n\t * In the worst case we need a new set of index blocks at\n\t * every level of the inode's extent tree.\n\t */\n\tei->i_da_metadata_calc_len = 1;\n\tei->i_da_metadata_calc_last_lblock = lblock;\n\treturn ext_depth(inode) + 1;\n}\n\nstatic int\next4_ext_max_entries(struct inode *inode, int depth)\n{\n\tint max;\n\n\tif (depth == ext_depth(inode)) {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_root(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_root_idx(inode, 1);\n\t} else {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_block(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_block_idx(inode, 1);\n\t}\n\n\treturn max;\n}\n\nstatic int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)\n{\n\text4_fsblk_t block = ext4_ext_pblock(ext);\n\tint len = ext4_ext_get_actual_len(ext);\n\text4_lblk_t lblock = le32_to_cpu(ext->ee_block);\n\text4_lblk_t last = lblock + len - 1;\n\n\tif (lblock > last)\n\t\treturn 0;\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, len);\n}\n\nstatic int ext4_valid_extent_idx(struct inode *inode,\n\t\t\t\tstruct ext4_extent_idx *ext_idx)\n{\n\text4_fsblk_t block = ext4_idx_pblock(ext_idx);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, 1);\n}\n\nstatic int ext4_valid_extent_entries(struct inode *inode,\n\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\tint depth)\n{\n\tunsigned short entries;\n\tif (eh->eh_entries == 0)\n\t\treturn 1;\n\n\tentries = le16_to_cpu(eh->eh_entries);\n\n\tif (depth == 0) {\n\t\t/* leaf entries */\n\t\tstruct ext4_extent *ext = EXT_FIRST_EXTENT(eh);\n\t\tstruct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;\n\t\text4_fsblk_t pblock = 0;\n\t\text4_lblk_t lblock = 0;\n\t\text4_lblk_t prev = 0;\n\t\tint len = 0;\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent(inode, ext))\n\t\t\t\treturn 0;\n\n\t\t\t/* Check for overlapping extents */\n\t\t\tlblock = le32_to_cpu(ext->ee_block);\n\t\t\tlen = ext4_ext_get_actual_len(ext);\n\t\t\tif ((lblock <= prev) && prev) {\n\t\t\t\tpblock = ext4_ext_pblock(ext);\n\t\t\t\tes->s_last_error_block = cpu_to_le64(pblock);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\text++;\n\t\t\tentries--;\n\t\t\tprev = lblock + len - 1;\n\t\t}\n\t} else {\n\t\tstruct ext4_extent_idx *ext_idx = EXT_FIRST_INDEX(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent_idx(inode, ext_idx))\n\t\t\t\treturn 0;\n\t\t\text_idx++;\n\t\t\tentries--;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __ext4_ext_check(const char *function, unsigned int line,\n\t\t\t    struct inode *inode, struct ext4_extent_header *eh,\n\t\t\t    int depth, ext4_fsblk_t pblk)\n{\n\tconst char *error_msg;\n\tint max = 0;\n\n\tif (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) {\n\t\terror_msg = \"invalid magic\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_depth) != depth)) {\n\t\terror_msg = \"unexpected eh_depth\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(eh->eh_max == 0)) {\n\t\terror_msg = \"invalid eh_max\";\n\t\tgoto corrupted;\n\t}\n\tmax = ext4_ext_max_entries(inode, depth);\n\tif (unlikely(le16_to_cpu(eh->eh_max) > max)) {\n\t\terror_msg = \"too large eh_max\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) {\n\t\terror_msg = \"invalid eh_entries\";\n\t\tgoto corrupted;\n\t}\n\tif (!ext4_valid_extent_entries(inode, eh, depth)) {\n\t\terror_msg = \"invalid extent entries\";\n\t\tgoto corrupted;\n\t}\n\t/* Verify checksum on non-root extent tree nodes */\n\tif (ext_depth(inode) != depth &&\n\t    !ext4_extent_block_csum_verify(inode, eh)) {\n\t\terror_msg = \"extent tree corrupted\";\n\t\tgoto corrupted;\n\t}\n\treturn 0;\n\ncorrupted:\n\text4_error_inode(inode, function, line, 0,\n\t\t\t \"pblk %llu bad header/extent: %s - magic %x, \"\n\t\t\t \"entries %u, max %u(%u), depth %u(%u)\",\n\t\t\t (unsigned long long) pblk, error_msg,\n\t\t\t le16_to_cpu(eh->eh_magic),\n\t\t\t le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max),\n\t\t\t max, le16_to_cpu(eh->eh_depth), depth);\n\treturn -EIO;\n}\n\n#define ext4_ext_check(inode, eh, depth, pblk)\t\t\t\\\n\t__ext4_ext_check(__func__, __LINE__, (inode), (eh), (depth), (pblk))\n\nint ext4_ext_check_inode(struct inode *inode)\n{\n\treturn ext4_ext_check(inode, ext_inode_hdr(inode), ext_depth(inode), 0);\n}\n\nstatic struct buffer_head *\n__read_extent_tree_block(const char *function, unsigned int line,\n\t\t\t struct inode *inode, ext4_fsblk_t pblk, int depth,\n\t\t\t int flags)\n{\n\tstruct buffer_head\t\t*bh;\n\tint\t\t\t\terr;\n\n\tbh = sb_getblk(inode->i_sb, pblk);\n\tif (unlikely(!bh))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!bh_uptodate_or_lock(bh)) {\n\t\ttrace_ext4_ext_load_extent(inode, pblk, _RET_IP_);\n\t\terr = bh_submit_read(bh);\n\t\tif (err < 0)\n\t\t\tgoto errout;\n\t}\n\tif (buffer_verified(bh) && !(flags & EXT4_EX_FORCE_CACHE))\n\t\treturn bh;\n\terr = __ext4_ext_check(function, line, inode,\n\t\t\t       ext_block_hdr(bh), depth, pblk);\n\tif (err)\n\t\tgoto errout;\n\tset_buffer_verified(bh);\n\t/*\n\t * If this is a leaf block, cache all of its entries\n\t */\n\tif (!(flags & EXT4_EX_NOCACHE) && depth == 0) {\n\t\tstruct ext4_extent_header *eh = ext_block_hdr(bh);\n\t\tstruct ext4_extent *ex = EXT_FIRST_EXTENT(eh);\n\t\text4_lblk_t prev = 0;\n\t\tint i;\n\n\t\tfor (i = le16_to_cpu(eh->eh_entries); i > 0; i--, ex++) {\n\t\t\tunsigned int status = EXTENT_STATUS_WRITTEN;\n\t\t\text4_lblk_t lblk = le32_to_cpu(ex->ee_block);\n\t\t\tint len = ext4_ext_get_actual_len(ex);\n\n\t\t\tif (prev && (prev != lblk))\n\t\t\t\text4_es_cache_extent(inode, prev,\n\t\t\t\t\t\t     lblk - prev, ~0,\n\t\t\t\t\t\t     EXTENT_STATUS_HOLE);\n\n\t\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\t\tstatus = EXTENT_STATUS_UNWRITTEN;\n\t\t\text4_es_cache_extent(inode, lblk, len,\n\t\t\t\t\t     ext4_ext_pblock(ex), status);\n\t\t\tprev = lblk + len;\n\t\t}\n\t}\n\treturn bh;\nerrout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n\n}\n\n#define read_extent_tree_block(inode, pblk, depth, flags)\t\t\\\n\t__read_extent_tree_block(__func__, __LINE__, (inode), (pblk),   \\\n\t\t\t\t (depth), (flags))\n\n/*\n * This function is called to cache a file's extent information in the\n * extent status tree\n */\nint ext4_ext_precache(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_ext_path *path = NULL;\n\tstruct buffer_head *bh;\n\tint i = 0, depth, ret = 0;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn 0;\t/* not an extent-mapped inode */\n\n\tdown_read(&ei->i_data_sem);\n\tdepth = ext_depth(inode);\n\n\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 1),\n\t\t       GFP_NOFS);\n\tif (path == NULL) {\n\t\tup_read(&ei->i_data_sem);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Don't cache anything if there are no external extent blocks */\n\tif (depth == 0)\n\t\tgoto out;\n\tpath[0].p_hdr = ext_inode_hdr(inode);\n\tret = ext4_ext_check(inode, path[0].p_hdr, depth, 0);\n\tif (ret)\n\t\tgoto out;\n\tpath[0].p_idx = EXT_FIRST_INDEX(path[0].p_hdr);\n\twhile (i >= 0) {\n\t\t/*\n\t\t * If this is a leaf block or we've reached the end of\n\t\t * the index block, go up\n\t\t */\n\t\tif ((i == depth) ||\n\t\t    path[i].p_idx > EXT_LAST_INDEX(path[i].p_hdr)) {\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\t\tbh = read_extent_tree_block(inode,\n\t\t\t\t\t    ext4_idx_pblock(path[i].p_idx++),\n\t\t\t\t\t    depth - i - 1,\n\t\t\t\t\t    EXT4_EX_FORCE_CACHE);\n\t\tif (IS_ERR(bh)) {\n\t\t\tret = PTR_ERR(bh);\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t\tpath[i].p_bh = bh;\n\t\tpath[i].p_hdr = ext_block_hdr(bh);\n\t\tpath[i].p_idx = EXT_FIRST_INDEX(path[i].p_hdr);\n\t}\n\text4_set_inode_state(inode, EXT4_STATE_EXT_PRECACHED);\nout:\n\tup_read(&ei->i_data_sem);\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn ret;\n}\n\n#ifdef EXT_DEBUG\nstatic void ext4_ext_show_path(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint k, l = path->p_depth;\n\n\text_debug(\"path:\");\n\tfor (k = 0; k <= l; k++, path++) {\n\t\tif (path->p_idx) {\n\t\t  ext_debug(\"  %d->%llu\", le32_to_cpu(path->p_idx->ei_block),\n\t\t\t    ext4_idx_pblock(path->p_idx));\n\t\t} else if (path->p_ext) {\n\t\t\text_debug(\"  %d:[%d]%d:%llu \",\n\t\t\t\t  le32_to_cpu(path->p_ext->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(path->p_ext),\n\t\t\t\t  ext4_ext_get_actual_len(path->p_ext),\n\t\t\t\t  ext4_ext_pblock(path->p_ext));\n\t\t} else\n\t\t\text_debug(\"  []\");\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_leaf(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex;\n\tint i;\n\n\tif (!path)\n\t\treturn;\n\n\teh = path[depth].p_hdr;\n\tex = EXT_FIRST_EXTENT(eh);\n\n\text_debug(\"Displaying leaf extents for inode %lu\\n\", inode->i_ino);\n\n\tfor (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ex++) {\n\t\text_debug(\"%d:[%d]%d:%llu \", le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t  ext4_ext_get_actual_len(ex), ext4_ext_pblock(ex));\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_move(struct inode *inode, struct ext4_ext_path *path,\n\t\t\text4_fsblk_t newblock, int level)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\n\tif (depth != level) {\n\t\tstruct ext4_extent_idx *idx;\n\t\tidx = path[level].p_idx;\n\t\twhile (idx <= EXT_MAX_INDEX(path[level].p_hdr)) {\n\t\t\text_debug(\"%d: move %d:%llu in new index %llu\\n\", level,\n\t\t\t\t\tle32_to_cpu(idx->ei_block),\n\t\t\t\t\text4_idx_pblock(idx),\n\t\t\t\t\tnewblock);\n\t\t\tidx++;\n\t\t}\n\n\t\treturn;\n\t}\n\n\tex = path[depth].p_ext;\n\twhile (ex <= EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\text_debug(\"move %d:%llu:[%d]%d in new leaf %llu\\n\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_pblock(ex),\n\t\t\t\text4_ext_is_unwritten(ex),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tnewblock);\n\t\tex++;\n\t}\n}\n\n#else\n#define ext4_ext_show_path(inode, path)\n#define ext4_ext_show_leaf(inode, path)\n#define ext4_ext_show_move(inode, path, newblock, level)\n#endif\n\nvoid ext4_ext_drop_refs(struct ext4_ext_path *path)\n{\n\tint depth, i;\n\n\tif (!path)\n\t\treturn;\n\tdepth = path->p_depth;\n\tfor (i = 0; i <= depth; i++, path++)\n\t\tif (path->p_bh) {\n\t\t\tbrelse(path->p_bh);\n\t\t\tpath->p_bh = NULL;\n\t\t}\n}\n\n/*\n * ext4_ext_binsearch_idx:\n * binary search for the closest index of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch_idx(struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent_idx *r, *l, *m;\n\n\n\text_debug(\"binsearch for %u(idx):  \", block);\n\n\tl = EXT_FIRST_INDEX(eh) + 1;\n\tr = EXT_LAST_INDEX(eh);\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ei_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ei_block),\n\t\t\t\tm, le32_to_cpu(m->ei_block),\n\t\t\t\tr, le32_to_cpu(r->ei_block));\n\t}\n\n\tpath->p_idx = l - 1;\n\text_debug(\"  -> %u->%lld \", le32_to_cpu(path->p_idx->ei_block),\n\t\t  ext4_idx_pblock(path->p_idx));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent_idx *chix, *ix;\n\t\tint k;\n\n\t\tchix = ix = EXT_FIRST_INDEX(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) {\n\t\t  if (k != 0 &&\n\t\t      le32_to_cpu(ix->ei_block) <= le32_to_cpu(ix[-1].ei_block)) {\n\t\t\t\tprintk(KERN_DEBUG \"k=%d, ix=0x%p, \"\n\t\t\t\t       \"first=0x%p\\n\", k,\n\t\t\t\t       ix, EXT_FIRST_INDEX(eh));\n\t\t\t\tprintk(KERN_DEBUG \"%u <= %u\\n\",\n\t\t\t\t       le32_to_cpu(ix->ei_block),\n\t\t\t\t       le32_to_cpu(ix[-1].ei_block));\n\t\t\t}\n\t\t\tBUG_ON(k && le32_to_cpu(ix->ei_block)\n\t\t\t\t\t   <= le32_to_cpu(ix[-1].ei_block));\n\t\t\tif (block < le32_to_cpu(ix->ei_block))\n\t\t\t\tbreak;\n\t\t\tchix = ix;\n\t\t}\n\t\tBUG_ON(chix != path->p_idx);\n\t}\n#endif\n\n}\n\n/*\n * ext4_ext_binsearch:\n * binary search for closest extent of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch(struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent *r, *l, *m;\n\n\tif (eh->eh_entries == 0) {\n\t\t/*\n\t\t * this leaf is empty:\n\t\t * we get such a leaf in split/add case\n\t\t */\n\t\treturn;\n\t}\n\n\text_debug(\"binsearch for %u:  \", block);\n\n\tl = EXT_FIRST_EXTENT(eh) + 1;\n\tr = EXT_LAST_EXTENT(eh);\n\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ee_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ee_block),\n\t\t\t\tm, le32_to_cpu(m->ee_block),\n\t\t\t\tr, le32_to_cpu(r->ee_block));\n\t}\n\n\tpath->p_ext = l - 1;\n\text_debug(\"  -> %d:%llu:[%d]%d \",\n\t\t\tle32_to_cpu(path->p_ext->ee_block),\n\t\t\text4_ext_pblock(path->p_ext),\n\t\t\text4_ext_is_unwritten(path->p_ext),\n\t\t\text4_ext_get_actual_len(path->p_ext));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent *chex, *ex;\n\t\tint k;\n\n\t\tchex = ex = EXT_FIRST_EXTENT(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) {\n\t\t\tBUG_ON(k && le32_to_cpu(ex->ee_block)\n\t\t\t\t\t  <= le32_to_cpu(ex[-1].ee_block));\n\t\t\tif (block < le32_to_cpu(ex->ee_block))\n\t\t\t\tbreak;\n\t\t\tchex = ex;\n\t\t}\n\t\tBUG_ON(chex != path->p_ext);\n\t}\n#endif\n\n}\n\nint ext4_ext_tree_init(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_extent_header *eh;\n\n\teh = ext_inode_hdr(inode);\n\teh->eh_depth = 0;\n\teh->eh_entries = 0;\n\teh->eh_magic = EXT4_EXT_MAGIC;\n\teh->eh_max = cpu_to_le16(ext4_ext_space_root(inode, 0));\n\text4_mark_inode_dirty(handle, inode);\n\treturn 0;\n}\n\nstruct ext4_ext_path *\next4_find_extent(struct inode *inode, ext4_lblk_t block,\n\t\t struct ext4_ext_path **orig_path, int flags)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct buffer_head *bh;\n\tstruct ext4_ext_path *path = orig_path ? *orig_path : NULL;\n\tshort int depth, i, ppos = 0;\n\tint ret;\n\n\teh = ext_inode_hdr(inode);\n\tdepth = ext_depth(inode);\n\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tif (depth > path[0].p_maxdepth) {\n\t\t\tkfree(path);\n\t\t\t*orig_path = path = NULL;\n\t\t}\n\t}\n\tif (!path) {\n\t\t/* account possible depth increase */\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 2),\n\t\t\t\tGFP_NOFS);\n\t\tif (unlikely(!path))\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tpath[0].p_maxdepth = depth + 1;\n\t}\n\tpath[0].p_hdr = eh;\n\tpath[0].p_bh = NULL;\n\n\ti = depth;\n\t/* walk through the tree */\n\twhile (i) {\n\t\text_debug(\"depth %d: num %d, max %d\\n\",\n\t\t\t  ppos, le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\n\t\text4_ext_binsearch_idx(inode, path + ppos, block);\n\t\tpath[ppos].p_block = ext4_idx_pblock(path[ppos].p_idx);\n\t\tpath[ppos].p_depth = i;\n\t\tpath[ppos].p_ext = NULL;\n\n\t\tbh = read_extent_tree_block(inode, path[ppos].p_block, --i,\n\t\t\t\t\t    flags);\n\t\tif (unlikely(IS_ERR(bh))) {\n\t\t\tret = PTR_ERR(bh);\n\t\t\tgoto err;\n\t\t}\n\n\t\teh = ext_block_hdr(bh);\n\t\tppos++;\n\t\tif (unlikely(ppos > depth)) {\n\t\t\tput_bh(bh);\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"ppos %d > depth %d\", ppos, depth);\n\t\t\tret = -EIO;\n\t\t\tgoto err;\n\t\t}\n\t\tpath[ppos].p_bh = bh;\n\t\tpath[ppos].p_hdr = eh;\n\t}\n\n\tpath[ppos].p_depth = i;\n\tpath[ppos].p_ext = NULL;\n\tpath[ppos].p_idx = NULL;\n\n\t/* find extent */\n\text4_ext_binsearch(inode, path + ppos, block);\n\t/* if not an empty leaf */\n\tif (path[ppos].p_ext)\n\t\tpath[ppos].p_block = ext4_ext_pblock(path[ppos].p_ext);\n\n\text4_ext_show_path(inode, path);\n\n\treturn path;\n\nerr:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tif (orig_path)\n\t\t*orig_path = NULL;\n\treturn ERR_PTR(ret);\n}\n\n/*\n * ext4_ext_insert_index:\n * insert new index [@logical;@ptr] into the block at @curp;\n * check where to insert: before @curp or after @curp\n */\nstatic int ext4_ext_insert_index(handle_t *handle, struct inode *inode,\n\t\t\t\t struct ext4_ext_path *curp,\n\t\t\t\t int logical, ext4_fsblk_t ptr)\n{\n\tstruct ext4_extent_idx *ix;\n\tint len, err;\n\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\treturn err;\n\n\tif (unlikely(logical == le32_to_cpu(curp->p_idx->ei_block))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d == ei_block %d!\",\n\t\t\t\t logical, le32_to_cpu(curp->p_idx->ei_block));\n\t\treturn -EIO;\n\t}\n\n\tif (unlikely(le16_to_cpu(curp->p_hdr->eh_entries)\n\t\t\t     >= le16_to_cpu(curp->p_hdr->eh_max))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"eh_entries %d >= eh_max %d!\",\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_entries),\n\t\t\t\t le16_to_cpu(curp->p_hdr->eh_max));\n\t\treturn -EIO;\n\t}\n\n\tif (logical > le32_to_cpu(curp->p_idx->ei_block)) {\n\t\t/* insert after */\n\t\text_debug(\"insert new index %d after: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx + 1;\n\t} else {\n\t\t/* insert before */\n\t\text_debug(\"insert new index %d before: %llu\\n\", logical, ptr);\n\t\tix = curp->p_idx;\n\t}\n\n\tlen = EXT_LAST_INDEX(curp->p_hdr) - ix + 1;\n\tBUG_ON(len < 0);\n\tif (len > 0) {\n\t\text_debug(\"insert new index %d: \"\n\t\t\t\t\"move %d indices from 0x%p to 0x%p\\n\",\n\t\t\t\tlogical, len, ix, ix + 1);\n\t\tmemmove(ix + 1, ix, len * sizeof(struct ext4_extent_idx));\n\t}\n\n\tif (unlikely(ix > EXT_MAX_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_MAX_INDEX!\");\n\t\treturn -EIO;\n\t}\n\n\tix->ei_block = cpu_to_le32(logical);\n\text4_idx_store_pblock(ix, ptr);\n\tle16_add_cpu(&curp->p_hdr->eh_entries, 1);\n\n\tif (unlikely(ix > EXT_LAST_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_LAST_INDEX!\");\n\t\treturn -EIO;\n\t}\n\n\terr = ext4_ext_dirty(handle, inode, curp);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_split:\n * inserts new subtree into the path, using free index entry\n * at depth @at:\n * - allocates all needed blocks (new leaf and all intermediate index blocks)\n * - makes decision where to split\n * - moves remaining extents and index entries (right to the split point)\n *   into the newly allocated blocks\n * - initializes subtree\n */\nstatic int ext4_ext_split(handle_t *handle, struct inode *inode,\n\t\t\t  unsigned int flags,\n\t\t\t  struct ext4_ext_path *path,\n\t\t\t  struct ext4_extent *newext, int at)\n{\n\tstruct buffer_head *bh = NULL;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tint i = at, k, m, a;\n\text4_fsblk_t newblock, oldblock;\n\t__le32 border;\n\text4_fsblk_t *ablocks = NULL; /* array of allocated blocks */\n\tint err = 0;\n\n\t/* make decision: where to split? */\n\t/* FIXME: now decision is simplest: at current extent */\n\n\t/* if current leaf will be split, then we should use\n\t * border from split point */\n\tif (unlikely(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"p_ext > EXT_MAX_EXTENT!\");\n\t\treturn -EIO;\n\t}\n\tif (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\tborder = path[depth].p_ext[1].ee_block;\n\t\text_debug(\"leaf will be split.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\t  le32_to_cpu(border));\n\t} else {\n\t\tborder = newext->ee_block;\n\t\text_debug(\"leaf will be added.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\tle32_to_cpu(border));\n\t}\n\n\t/*\n\t * If error occurs, then we break processing\n\t * and mark filesystem read-only. index won't\n\t * be inserted and tree will be in consistent\n\t * state. Next mount will repair buffers too.\n\t */\n\n\t/*\n\t * Get array to track all allocated blocks.\n\t * We need this to handle errors and free blocks\n\t * upon them.\n\t */\n\tablocks = kzalloc(sizeof(ext4_fsblk_t) * depth, GFP_NOFS);\n\tif (!ablocks)\n\t\treturn -ENOMEM;\n\n\t/* allocate all needed blocks */\n\text_debug(\"allocate %d blocks for indexes/leaf\\n\", depth - at);\n\tfor (a = 0; a < depth - at; a++) {\n\t\tnewblock = ext4_ext_new_meta_block(handle, inode, path,\n\t\t\t\t\t\t   newext, &err, flags);\n\t\tif (newblock == 0)\n\t\t\tgoto cleanup;\n\t\tablocks[a] = newblock;\n\t}\n\n\t/* initialize new leaf */\n\tnewblock = ablocks[--a];\n\tif (unlikely(newblock == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"newblock == 0!\");\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (unlikely(!bh)) {\n\t\terr = -ENOMEM;\n\t\tgoto cleanup;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err)\n\t\tgoto cleanup;\n\n\tneh = ext_block_hdr(bh);\n\tneh->eh_entries = 0;\n\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tneh->eh_depth = 0;\n\n\t/* move remainder of path[depth] to the new leaf */\n\tif (unlikely(path[depth].p_hdr->eh_entries !=\n\t\t     path[depth].p_hdr->eh_max)) {\n\t\tEXT4_ERROR_INODE(inode, \"eh_entries %d != eh_max %d!\",\n\t\t\t\t path[depth].p_hdr->eh_entries,\n\t\t\t\t path[depth].p_hdr->eh_max);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\t/* start copy from next extent */\n\tm = EXT_MAX_EXTENT(path[depth].p_hdr) - path[depth].p_ext++;\n\text4_ext_show_move(inode, path, newblock, depth);\n\tif (m) {\n\t\tstruct ext4_extent *ex;\n\t\tex = EXT_FIRST_EXTENT(neh);\n\t\tmemmove(ex, path[depth].p_ext, sizeof(struct ext4_extent) * m);\n\t\tle16_add_cpu(&neh->eh_entries, m);\n\t}\n\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto cleanup;\n\tbrelse(bh);\n\tbh = NULL;\n\n\t/* correct old leaf */\n\tif (m) {\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tle16_add_cpu(&path[depth].p_hdr->eh_entries, -m);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t}\n\n\t/* create intermediate indexes */\n\tk = depth - at - 1;\n\tif (unlikely(k < 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"k %d < 0!\", k);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tif (k)\n\t\text_debug(\"create %d intermediate indices\\n\", k);\n\t/* insert new index into current index block */\n\t/* current depth stored in i var */\n\ti = depth - 1;\n\twhile (k--) {\n\t\toldblock = newblock;\n\t\tnewblock = ablocks[--a];\n\t\tbh = sb_getblk(inode->i_sb, newblock);\n\t\tif (unlikely(!bh)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tlock_buffer(bh);\n\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\tneh = ext_block_hdr(bh);\n\t\tneh->eh_entries = cpu_to_le16(1);\n\t\tneh->eh_magic = EXT4_EXT_MAGIC;\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\t\tneh->eh_depth = cpu_to_le16(depth - i);\n\t\tfidx = EXT_FIRST_INDEX(neh);\n\t\tfidx->ei_block = border;\n\t\text4_idx_store_pblock(fidx, oldblock);\n\n\t\text_debug(\"int.index at %d (block %llu): %u -> %llu\\n\",\n\t\t\t\ti, newblock, le32_to_cpu(border), oldblock);\n\n\t\t/* move remainder of path[i] to the new index block */\n\t\tif (unlikely(EXT_MAX_INDEX(path[i].p_hdr) !=\n\t\t\t\t\tEXT_LAST_INDEX(path[i].p_hdr))) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_MAX_INDEX != EXT_LAST_INDEX ee_block %d!\",\n\t\t\t\t\t le32_to_cpu(path[i].p_ext->ee_block));\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\t/* start copy indexes */\n\t\tm = EXT_MAX_INDEX(path[i].p_hdr) - path[i].p_idx++;\n\t\text_debug(\"cur 0x%p, last 0x%p\\n\", path[i].p_idx,\n\t\t\t\tEXT_MAX_INDEX(path[i].p_hdr));\n\t\text4_ext_show_move(inode, path, newblock, i);\n\t\tif (m) {\n\t\t\tmemmove(++fidx, path[i].p_idx,\n\t\t\t\tsizeof(struct ext4_extent_idx) * m);\n\t\t\tle16_add_cpu(&neh->eh_entries, m);\n\t\t}\n\t\text4_extent_block_csum_set(inode, neh);\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tbrelse(bh);\n\t\tbh = NULL;\n\n\t\t/* correct old index */\n\t\tif (m) {\n\t\t\terr = ext4_ext_get_access(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t\tle16_add_cpu(&path[i].p_hdr->eh_entries, -m);\n\t\t\terr = ext4_ext_dirty(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t}\n\n\t\ti--;\n\t}\n\n\t/* insert new index */\n\terr = ext4_ext_insert_index(handle, inode, path + at,\n\t\t\t\t    le32_to_cpu(border), newblock);\n\ncleanup:\n\tif (bh) {\n\t\tif (buffer_locked(bh))\n\t\t\tunlock_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\tif (err) {\n\t\t/* free all allocated blocks in error case */\n\t\tfor (i = 0; i < depth; i++) {\n\t\t\tif (!ablocks[i])\n\t\t\t\tcontinue;\n\t\t\text4_free_blocks(handle, inode, NULL, ablocks[i], 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\t\t}\n\t}\n\tkfree(ablocks);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_grow_indepth:\n * implements tree growing procedure:\n * - allocates new block\n * - moves top-level data (index block or leaf) into the new block\n * - initializes new top-level, creating index that points to the\n *   just created block\n */\nstatic int ext4_ext_grow_indepth(handle_t *handle, struct inode *inode,\n\t\t\t\t unsigned int flags)\n{\n\tstruct ext4_extent_header *neh;\n\tstruct buffer_head *bh;\n\text4_fsblk_t newblock, goal = 0;\n\tstruct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;\n\tint err = 0;\n\n\t/* Try to prepend new index to old one */\n\tif (ext_depth(inode))\n\t\tgoal = ext4_idx_pblock(EXT_FIRST_INDEX(ext_inode_hdr(inode)));\n\tif (goal > le32_to_cpu(es->s_first_data_block)) {\n\t\tflags |= EXT4_MB_HINT_TRY_GOAL;\n\t\tgoal--;\n\t} else\n\t\tgoal = ext4_inode_to_goal_block(inode);\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, flags,\n\t\t\t\t\tNULL, &err);\n\tif (newblock == 0)\n\t\treturn err;\n\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (unlikely(!bh))\n\t\treturn -ENOMEM;\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err) {\n\t\tunlock_buffer(bh);\n\t\tgoto out;\n\t}\n\n\t/* move top-level index/leaf into new block */\n\tmemmove(bh->b_data, EXT4_I(inode)->i_data,\n\t\tsizeof(EXT4_I(inode)->i_data));\n\n\t/* set size of new block */\n\tneh = ext_block_hdr(bh);\n\t/* old root could have indexes or leaves\n\t * so calculate e_max right way */\n\tif (ext_depth(inode))\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\telse\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\text4_extent_block_csum_set(inode, neh);\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto out;\n\n\t/* Update top-level index: num,max,pointer */\n\tneh = ext_inode_hdr(inode);\n\tneh->eh_entries = cpu_to_le16(1);\n\text4_idx_store_pblock(EXT_FIRST_INDEX(neh), newblock);\n\tif (neh->eh_depth == 0) {\n\t\t/* Root extent block becomes index block */\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_root_idx(inode, 0));\n\t\tEXT_FIRST_INDEX(neh)->ei_block =\n\t\t\tEXT_FIRST_EXTENT(neh)->ee_block;\n\t}\n\text_debug(\"new root: num %d(%d), lblock %d, ptr %llu\\n\",\n\t\t  le16_to_cpu(neh->eh_entries), le16_to_cpu(neh->eh_max),\n\t\t  le32_to_cpu(EXT_FIRST_INDEX(neh)->ei_block),\n\t\t  ext4_idx_pblock(EXT_FIRST_INDEX(neh)));\n\n\tle16_add_cpu(&neh->eh_depth, 1);\n\text4_mark_inode_dirty(handle, inode);\nout:\n\tbrelse(bh);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_create_new_leaf:\n * finds empty index and adds new leaf.\n * if no free index is found, then it requests in-depth growing.\n */\nstatic int ext4_ext_create_new_leaf(handle_t *handle, struct inode *inode,\n\t\t\t\t    unsigned int mb_flags,\n\t\t\t\t    unsigned int gb_flags,\n\t\t\t\t    struct ext4_ext_path **ppath,\n\t\t\t\t    struct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_ext_path *curp;\n\tint depth, i, err = 0;\n\nrepeat:\n\ti = depth = ext_depth(inode);\n\n\t/* walk up to the tree and look for free index entry */\n\tcurp = path + depth;\n\twhile (i > 0 && !EXT_HAS_FREE_INDEX(curp)) {\n\t\ti--;\n\t\tcurp--;\n\t}\n\n\t/* we use already allocated block for index block,\n\t * so subsequent data blocks should be contiguous */\n\tif (EXT_HAS_FREE_INDEX(curp)) {\n\t\t/* if we found index with free entry, then use that\n\t\t * entry: create all needed subtree and add new leaf */\n\t\terr = ext4_ext_split(handle, inode, mb_flags, path, newext, i);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\tpath = ext4_find_extent(inode,\n\t\t\t\t    (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    ppath, gb_flags);\n\t\tif (IS_ERR(path))\n\t\t\terr = PTR_ERR(path);\n\t} else {\n\t\t/* tree is full, time to grow in depth */\n\t\terr = ext4_ext_grow_indepth(handle, inode, mb_flags);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\tpath = ext4_find_extent(inode,\n\t\t\t\t   (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    ppath, gb_flags);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * only first (depth 0 -> 1) produces free space;\n\t\t * in all other cases we have to split the grown tree\n\t\t */\n\t\tdepth = ext_depth(inode);\n\t\tif (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) {\n\t\t\t/* now we need to split */\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * search the closest allocated block to the left for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_left(struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\tint depth, ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_FIRST_EXTENT != ex *logical %d ee_block %d!\",\n\t\t\t\t\t *logical, le32_to_cpu(ex->ee_block));\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t  \"ix (%d) != EXT_FIRST_INDEX (%d) (depth %d)!\",\n\t\t\t\t  ix != NULL ? le32_to_cpu(ix->ei_block) : 0,\n\t\t\t\t  EXT_FIRST_INDEX(path[depth].p_hdr) != NULL ?\n\t\tle32_to_cpu(EXT_FIRST_INDEX(path[depth].p_hdr)->ei_block) : 0,\n\t\t\t\t  depth);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\t*logical = le32_to_cpu(ex->ee_block) + ee_len - 1;\n\t*phys = ext4_ext_pblock(ex) + ee_len - 1;\n\treturn 0;\n}\n\n/*\n * search the closest allocated block to the right for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the largest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t ext4_lblk_t *logical, ext4_fsblk_t *phys,\n\t\t\t\t struct ext4_extent **ret_ex)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t block;\n\tint depth;\t/* Note, NOT eh_depth; depth from top of tree */\n\tint ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"first_extent(path[%d].p_hdr) != ex\",\n\t\t\t\t\t depth);\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"ix != EXT_FIRST_INDEX *logical %d!\",\n\t\t\t\t\t\t *logical);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\tgoto found_extent;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\tif (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t/* next allocated block in this leaf */\n\t\tex++;\n\t\tgoto found_extent;\n\t}\n\n\t/* go up and search for index to the right */\n\twhile (--depth >= 0) {\n\t\tix = path[depth].p_idx;\n\t\tif (ix != EXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\tgoto got_index;\n\t}\n\n\t/* we've gone up to the root and found no index to the right */\n\treturn 0;\n\ngot_index:\n\t/* we've found index to the right, let's\n\t * follow it and find the closest allocated\n\t * block to the right */\n\tix++;\n\tblock = ext4_idx_pblock(ix);\n\twhile (++depth < path->p_depth) {\n\t\t/* subtract from p_depth to get proper eh_depth */\n\t\tbh = read_extent_tree_block(inode, block,\n\t\t\t\t\t    path->p_depth - depth, 0);\n\t\tif (IS_ERR(bh))\n\t\t\treturn PTR_ERR(bh);\n\t\teh = ext_block_hdr(bh);\n\t\tix = EXT_FIRST_INDEX(eh);\n\t\tblock = ext4_idx_pblock(ix);\n\t\tput_bh(bh);\n\t}\n\n\tbh = read_extent_tree_block(inode, block, path->p_depth - depth, 0);\n\tif (IS_ERR(bh))\n\t\treturn PTR_ERR(bh);\n\teh = ext_block_hdr(bh);\n\tex = EXT_FIRST_EXTENT(eh);\nfound_extent:\n\t*logical = le32_to_cpu(ex->ee_block);\n\t*phys = ext4_ext_pblock(ex);\n\t*ret_ex = ex;\n\tif (bh)\n\t\tput_bh(bh);\n\treturn 0;\n}\n\n/*\n * ext4_ext_next_allocated_block:\n * returns allocated block in subsequent extent or EXT_MAX_BLOCKS.\n * NOTE: it considers block number from index entry as\n * allocated block. Thus, index entries have to be consistent\n * with leaves.\n */\next4_lblk_t\next4_ext_next_allocated_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn EXT_MAX_BLOCKS;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\t/* leaf */\n\t\t\tif (path[depth].p_ext &&\n\t\t\t\tpath[depth].p_ext !=\n\t\t\t\t\tEXT_LAST_EXTENT(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_ext[1].ee_block);\n\t\t} else {\n\t\t\t/* index */\n\t\t\tif (path[depth].p_idx !=\n\t\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\t}\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_next_leaf_block:\n * returns first allocated block from next leaf or EXT_MAX_BLOCKS\n */\nstatic ext4_lblk_t ext4_ext_next_leaf_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\t/* zero-tree has no leaf blocks at all */\n\tif (depth == 0)\n\t\treturn EXT_MAX_BLOCKS;\n\n\t/* go to index block */\n\tdepth--;\n\n\twhile (depth >= 0) {\n\t\tif (path[depth].p_idx !=\n\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\treturn (ext4_lblk_t)\n\t\t\t\tle32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCKS;\n}\n\n/*\n * ext4_ext_correct_indexes:\n * if leaf gets modified and modified extent is first in the leaf,\n * then we have to correct all indexes above.\n * TODO: do we need to correct tree in all cases?\n */\nstatic int ext4_ext_correct_indexes(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\t__le32 border;\n\tint k, err = 0;\n\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\tif (unlikely(ex == NULL || eh == NULL)) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"ex %p == NULL or eh %p == NULL\", ex, eh);\n\t\treturn -EIO;\n\t}\n\n\tif (depth == 0) {\n\t\t/* there is no tree at all */\n\t\treturn 0;\n\t}\n\n\tif (ex != EXT_FIRST_EXTENT(eh)) {\n\t\t/* we correct tree if first leaf got modified only */\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: we need correction if border is smaller than current one\n\t */\n\tk = depth - 1;\n\tborder = path[depth].p_ext->ee_block;\n\terr = ext4_ext_get_access(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\tpath[k].p_idx->ei_block = border;\n\terr = ext4_ext_dirty(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\n\twhile (k--) {\n\t\t/* change all left-side indexes */\n\t\tif (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr))\n\t\t\tbreak;\n\t\terr = ext4_ext_get_access(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath[k].p_idx->ei_block = border;\n\t\terr = ext4_ext_dirty(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint\next4_can_extents_be_merged(struct inode *inode, struct ext4_extent *ex1,\n\t\t\t\tstruct ext4_extent *ex2)\n{\n\tunsigned short ext1_ee_len, ext2_ee_len;\n\n\tif (ext4_ext_is_unwritten(ex1) != ext4_ext_is_unwritten(ex2))\n\t\treturn 0;\n\n\text1_ee_len = ext4_ext_get_actual_len(ex1);\n\text2_ee_len = ext4_ext_get_actual_len(ex2);\n\n\tif (le32_to_cpu(ex1->ee_block) + ext1_ee_len !=\n\t\t\tle32_to_cpu(ex2->ee_block))\n\t\treturn 0;\n\n\t/*\n\t * To allow future support for preallocated extents to be added\n\t * as an RO_COMPAT feature, refuse to merge to extents if\n\t * this can result in the top bit of ee_len being set.\n\t */\n\tif (ext1_ee_len + ext2_ee_len > EXT_INIT_MAX_LEN)\n\t\treturn 0;\n\tif (ext4_ext_is_unwritten(ex1) &&\n\t    (ext4_test_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN) ||\n\t     atomic_read(&EXT4_I(inode)->i_unwritten) ||\n\t     (ext1_ee_len + ext2_ee_len > EXT_UNWRITTEN_MAX_LEN)))\n\t\treturn 0;\n#ifdef AGGRESSIVE_TEST\n\tif (ext1_ee_len >= 4)\n\t\treturn 0;\n#endif\n\n\tif (ext4_ext_pblock(ex1) + ext1_ee_len == ext4_ext_pblock(ex2))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function tries to merge the \"ex\" extent to the next extent in the tree.\n * It always tries to merge towards right. If you want to merge towards\n * left, pass \"ex - 1\" as argument instead of \"ex\".\n * Returns 0 if the extents (ex and ex+1) were _not_ merged and returns\n * 1 if they got merged.\n */\nstatic int ext4_ext_try_to_merge_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t struct ext4_extent *ex)\n{\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth, len;\n\tint merge_done = 0, unwritten;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\twhile (ex < EXT_LAST_EXTENT(eh)) {\n\t\tif (!ext4_can_extents_be_merged(inode, ex, ex + 1))\n\t\t\tbreak;\n\t\t/* merge with next extent! */\n\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t+ ext4_ext_get_actual_len(ex + 1));\n\t\tif (unwritten)\n\t\t\text4_ext_mark_unwritten(ex);\n\n\t\tif (ex + 1 < EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = (EXT_LAST_EXTENT(eh) - ex - 1)\n\t\t\t\t* sizeof(struct ext4_extent);\n\t\t\tmemmove(ex + 1, ex + 2, len);\n\t\t}\n\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\tmerge_done = 1;\n\t\tWARN_ON(eh->eh_entries == 0);\n\t\tif (!eh->eh_entries)\n\t\t\tEXT4_ERROR_INODE(inode, \"eh->eh_entries = 0!\");\n\t}\n\n\treturn merge_done;\n}\n\n/*\n * This function does a very simple check to see if we can collapse\n * an extent tree with a single extent tree leaf block into the inode.\n */\nstatic void ext4_ext_try_to_merge_up(handle_t *handle,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tsize_t s;\n\tunsigned max_root = ext4_ext_space_root(inode, 0);\n\text4_fsblk_t blk;\n\n\tif ((path[0].p_depth != 1) ||\n\t    (le16_to_cpu(path[0].p_hdr->eh_entries) != 1) ||\n\t    (le16_to_cpu(path[1].p_hdr->eh_entries) > max_root))\n\t\treturn;\n\n\t/*\n\t * We need to modify the block allocation bitmap and the block\n\t * group descriptor to release the extent tree block.  If we\n\t * can't get the journal credits, give up.\n\t */\n\tif (ext4_journal_extend(handle, 2))\n\t\treturn;\n\n\t/*\n\t * Copy the extent data up to the inode\n\t */\n\tblk = ext4_idx_pblock(path[0].p_idx);\n\ts = le16_to_cpu(path[1].p_hdr->eh_entries) *\n\t\tsizeof(struct ext4_extent_idx);\n\ts += sizeof(struct ext4_extent_header);\n\n\tpath[1].p_maxdepth = path[0].p_maxdepth;\n\tmemcpy(path[0].p_hdr, path[1].p_hdr, s);\n\tpath[0].p_depth = 0;\n\tpath[0].p_ext = EXT_FIRST_EXTENT(path[0].p_hdr) +\n\t\t(path[1].p_ext - EXT_FIRST_EXTENT(path[1].p_hdr));\n\tpath[0].p_hdr->eh_max = cpu_to_le16(max_root);\n\n\tbrelse(path[1].p_bh);\n\text4_free_blocks(handle, inode, NULL, blk, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n}\n\n/*\n * This function tries to merge the @ex extent to neighbours in the tree.\n * return 1 if merge left else 0.\n */\nstatic void ext4_ext_try_to_merge(handle_t *handle,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct ext4_ext_path *path,\n\t\t\t\t  struct ext4_extent *ex) {\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth;\n\tint merge_done = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\tif (ex > EXT_FIRST_EXTENT(eh))\n\t\tmerge_done = ext4_ext_try_to_merge_right(inode, path, ex - 1);\n\n\tif (!merge_done)\n\t\t(void) ext4_ext_try_to_merge_right(inode, path, ex);\n\n\text4_ext_try_to_merge_up(handle, inode, path);\n}\n\n/*\n * check if a portion of the \"newext\" extent overlaps with an\n * existing extent.\n *\n * If there is an overlap discovered, it updates the length of the newext\n * such that there will be no overlap, and then returns 1.\n * If there is no overlap found, it returns 0.\n */\nstatic unsigned int ext4_ext_check_overlap(struct ext4_sb_info *sbi,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_extent *newext,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\text4_lblk_t b1, b2;\n\tunsigned int depth, len1;\n\tunsigned int ret = 0;\n\n\tb1 = le32_to_cpu(newext->ee_block);\n\tlen1 = ext4_ext_get_actual_len(newext);\n\tdepth = ext_depth(inode);\n\tif (!path[depth].p_ext)\n\t\tgoto out;\n\tb2 = EXT4_LBLK_CMASK(sbi, le32_to_cpu(path[depth].p_ext->ee_block));\n\n\t/*\n\t * get the next allocated block if the extent in the path\n\t * is before the requested block(s)\n\t */\n\tif (b2 < b1) {\n\t\tb2 = ext4_ext_next_allocated_block(path);\n\t\tif (b2 == EXT_MAX_BLOCKS)\n\t\t\tgoto out;\n\t\tb2 = EXT4_LBLK_CMASK(sbi, b2);\n\t}\n\n\t/* check for wrap through zero on extent logical start block*/\n\tif (b1 + len1 < b1) {\n\t\tlen1 = EXT_MAX_BLOCKS - b1;\n\t\tnewext->ee_len = cpu_to_le16(len1);\n\t\tret = 1;\n\t}\n\n\t/* check for overlap */\n\tif (b1 + len1 > b2) {\n\t\tnewext->ee_len = cpu_to_le16(b2 - b1);\n\t\tret = 1;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * ext4_ext_insert_extent:\n * tries to merge requsted extent into the existing extent or\n * inserts requested extent as new one into the tree,\n * creating new leaf in the no-space case.\n */\nint ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\tstruct ext4_extent *newext, int gb_flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tint mb_flags = 0, unwritten;\n\n\tif (gb_flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tmb_flags |= EXT4_MB_DELALLOC_RESERVED;\n\tif (unlikely(ext4_ext_get_actual_len(newext) == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"ext4_ext_get_actual_len(newext) == 0\");\n\t\treturn -EIO;\n\t}\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(gb_flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\n\t\t/*\n\t\t * Try to see whether we should rather test the extent on\n\t\t * right from ex, or from the left of ex. This is because\n\t\t * ext4_find_extent() can return either extent on the\n\t\t * left, or on the right from the searched position. This\n\t\t * will make merging more effective.\n\t\t */\n\t\tif (ex < EXT_LAST_EXTENT(eh) &&\n\t\t    (le32_to_cpu(ex->ee_block) +\n\t\t    ext4_ext_get_actual_len(ex) <\n\t\t    le32_to_cpu(newext->ee_block))) {\n\t\t\tex += 1;\n\t\t\tgoto prepend;\n\t\t} else if ((ex > EXT_FIRST_EXTENT(eh)) &&\n\t\t\t   (le32_to_cpu(newext->ee_block) +\n\t\t\t   ext4_ext_get_actual_len(newext) <\n\t\t\t   le32_to_cpu(ex->ee_block)))\n\t\t\tex -= 1;\n\n\t\t/* Try to append newex to the ex */\n\t\tif (ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\t\text_debug(\"append [%d]%d block to %u:[%d]%d\"\n\t\t\t\t  \"(from %llu)\\n\",\n\t\t\t\t  ext4_ext_is_unwritten(newext),\n\t\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t\t  ext4_ext_pblock(ex));\n\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t  path + depth);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\t\tif (unwritten)\n\t\t\t\text4_ext_mark_unwritten(ex);\n\t\t\teh = path[depth].p_hdr;\n\t\t\tnearex = ex;\n\t\t\tgoto merge;\n\t\t}\n\nprepend:\n\t\t/* Try to prepend newex to the ex */\n\t\tif (ext4_can_extents_be_merged(inode, newext, ex)) {\n\t\t\text_debug(\"prepend %u[%d]%d block to %u:[%d]%d\"\n\t\t\t\t  \"(from %llu)\\n\",\n\t\t\t\t  le32_to_cpu(newext->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(newext),\n\t\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t\t  ext4_ext_is_unwritten(ex),\n\t\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t\t  ext4_ext_pblock(ex));\n\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t  path + depth);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tunwritten = ext4_ext_is_unwritten(ex);\n\t\t\tex->ee_block = newext->ee_block;\n\t\t\text4_ext_store_pblock(ex, ext4_ext_pblock(newext));\n\t\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\t\tif (unwritten)\n\t\t\t\text4_ext_mark_unwritten(ex);\n\t\t\teh = path[depth].p_hdr;\n\t\t\tnearex = ex;\n\t\t\tgoto merge;\n\t\t}\n\t}\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = EXT_MAX_BLOCKS;\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block))\n\t\tnext = ext4_ext_next_leaf_block(path);\n\tif (next != EXT_MAX_BLOCKS) {\n\t\text_debug(\"next leaf block - %u\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_find_extent(inode, next, NULL, 0);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isn't full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto has_space;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\tif (gb_flags & EXT4_GET_BLOCKS_METADATA_NOFAIL)\n\t\tmb_flags |= EXT4_MB_USE_RESERVED;\n\terr = ext4_ext_create_new_leaf(handle, inode, mb_flags, gb_flags,\n\t\t\t\t       ppath, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %u:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tnearex = EXT_FIRST_EXTENT(eh);\n\t} else {\n\t\tif (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n\t\t\t/* Insert after */\n\t\t\text_debug(\"insert %u:%llu:[%d]%d before: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t\tnearex++;\n\t\t} else {\n\t\t\t/* Insert before */\n\t\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\t\text_debug(\"insert %u:%llu:[%d]%d after: \"\n\t\t\t\t\t\"nearest %p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex);\n\t\t}\n\t\tlen = EXT_LAST_EXTENT(eh) - nearex + 1;\n\t\tif (len > 0) {\n\t\t\text_debug(\"insert %u:%llu:[%d]%d: \"\n\t\t\t\t\t\"move %d extents from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_unwritten(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tlen, nearex, nearex + 1);\n\t\t\tmemmove(nearex + 1, nearex,\n\t\t\t\tlen * sizeof(struct ext4_extent));\n\t\t}\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tpath[depth].p_ext = nearex;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext4_ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents */\n\tif (!(gb_flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(handle, inode, path, nearex);\n\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\ncleanup:\n\text4_ext_drop_refs(npath);\n\tkfree(npath);\n\treturn err;\n}\n\nstatic int ext4_fill_fiemap_extents(struct inode *inode,\n\t\t\t\t    ext4_lblk_t block, ext4_lblk_t num,\n\t\t\t\t    struct fiemap_extent_info *fieinfo)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent *ex;\n\tstruct extent_status es;\n\text4_lblk_t next, next_del, start = 0, end = 0;\n\text4_lblk_t last = block + num;\n\tint exists, depth = 0, err = 0;\n\tunsigned int flags = 0;\n\tunsigned char blksize_bits = inode->i_sb->s_blocksize_bits;\n\n\twhile (block < last && block != EXT_MAX_BLOCKS) {\n\t\tnum = last - block;\n\t\t/* find extent for this block */\n\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\n\t\tpath = ext4_find_extent(inode, block, &path, 0);\n\t\tif (IS_ERR(path)) {\n\t\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\t\terr = PTR_ERR(path);\n\t\t\tpath = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdepth = ext_depth(inode);\n\t\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tex = path[depth].p_ext;\n\t\tnext = ext4_ext_next_allocated_block(path);\n\n\t\tflags = 0;\n\t\texists = 0;\n\t\tif (!ex) {\n\t\t\t/* there is no extent yet, so try to allocate\n\t\t\t * all requested space */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t} else if (le32_to_cpu(ex->ee_block) > block) {\n\t\t\t/* need to allocate space before found extent */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\t\t/* need to allocate space after found extent */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t\tif (end >= next)\n\t\t\t\tend = next;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)) {\n\t\t\t/*\n\t\t\t * some part of requested space is covered\n\t\t\t * by found extent\n\t\t\t */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block)\n\t\t\t\t+ ext4_ext_get_actual_len(ex);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t\texists = 1;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t\tBUG_ON(end <= start);\n\n\t\tif (!exists) {\n\t\t\tes.es_lblk = start;\n\t\t\tes.es_len = end - start;\n\t\t\tes.es_pblk = 0;\n\t\t} else {\n\t\t\tes.es_lblk = le32_to_cpu(ex->ee_block);\n\t\t\tes.es_len = ext4_ext_get_actual_len(ex);\n\t\t\tes.es_pblk = ext4_ext_pblock(ex);\n\t\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\t\tflags |= FIEMAP_EXTENT_UNWRITTEN;\n\t\t}\n\n\t\t/*\n\t\t * Find delayed extent and update es accordingly. We call\n\t\t * it even in !exists case to find out whether es is the\n\t\t * last existing extent or not.\n\t\t */\n\t\tnext_del = ext4_find_delayed_extent(inode, &es);\n\t\tif (!exists && next_del) {\n\t\t\texists = 1;\n\t\t\tflags |= (FIEMAP_EXTENT_DELALLOC |\n\t\t\t\t  FIEMAP_EXTENT_UNKNOWN);\n\t\t}\n\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\n\t\tif (unlikely(es.es_len == 0)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"es.es_len == 0\");\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * This is possible iff next == next_del == EXT_MAX_BLOCKS.\n\t\t * we need to check next == EXT_MAX_BLOCKS because it is\n\t\t * possible that an extent is with unwritten and delayed\n\t\t * status due to when an extent is delayed allocated and\n\t\t * is allocated by fallocate status tree will track both of\n\t\t * them in a extent.\n\t\t *\n\t\t * So we could return a unwritten and delayed extent, and\n\t\t * its block is equal to 'next'.\n\t\t */\n\t\tif (next == next_del && next == EXT_MAX_BLOCKS) {\n\t\t\tflags |= FIEMAP_EXTENT_LAST;\n\t\t\tif (unlikely(next_del != EXT_MAX_BLOCKS ||\n\t\t\t\t     next != EXT_MAX_BLOCKS)) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"next extent == %u, next \"\n\t\t\t\t\t\t \"delalloc extent = %u\",\n\t\t\t\t\t\t next, next_del);\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (exists) {\n\t\t\terr = fiemap_fill_next_extent(fieinfo,\n\t\t\t\t(__u64)es.es_lblk << blksize_bits,\n\t\t\t\t(__u64)es.es_pblk << blksize_bits,\n\t\t\t\t(__u64)es.es_len << blksize_bits,\n\t\t\t\tflags);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\tif (err == 1) {\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tblock = es.es_lblk + es.es_len;\n\t}\n\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn err;\n}\n\n/*\n * ext4_ext_put_gap_in_cache:\n * calculate boundaries of the gap that the requested block fits into\n * and cache this gap\n */\nstatic void\next4_ext_put_gap_in_cache(struct inode *inode, struct ext4_ext_path *path,\n\t\t\t\text4_lblk_t block)\n{\n\tint depth = ext_depth(inode);\n\text4_lblk_t len;\n\text4_lblk_t lblock;\n\tstruct ext4_extent *ex;\n\tstruct extent_status es;\n\n\tex = path[depth].p_ext;\n\tif (ex == NULL) {\n\t\t/* there is no extent yet, so gap is [0;-] */\n\t\tlblock = 0;\n\t\tlen = EXT_MAX_BLOCKS;\n\t\text_debug(\"cache gap(whole file):\");\n\t} else if (block < le32_to_cpu(ex->ee_block)) {\n\t\tlblock = block;\n\t\tlen = le32_to_cpu(ex->ee_block) - block;\n\t\text_debug(\"cache gap(before): %u [%u:%u]\",\n\t\t\t\tblock,\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\t ext4_ext_get_actual_len(ex));\n\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\text4_lblk_t next;\n\t\tlblock = le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex);\n\n\t\tnext = ext4_ext_next_allocated_block(path);\n\t\text_debug(\"cache gap(after): [%u:%u] %u\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tblock);\n\t\tBUG_ON(next == lblock);\n\t\tlen = next - lblock;\n\t} else {\n\t\tBUG();\n\t}\n\n\text4_es_find_delayed_extent_range(inode, lblock, lblock + len - 1, &es);\n\tif (es.es_len) {\n\t\t/* There's delayed extent containing lblock? */\n\t\tif (es.es_lblk <= lblock)\n\t\t\treturn;\n\t\tlen = min(es.es_lblk - lblock, len);\n\t}\n\text_debug(\" -> %u:%u\\n\", lblock, len);\n\text4_es_insert_extent(inode, lblock, len, ~0, EXTENT_STATUS_HOLE);\n}\n\n/*\n * ext4_ext_rm_idx:\n * removes index from the index block.\n */\nstatic int ext4_ext_rm_idx(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, int depth)\n{\n\tint err;\n\text4_fsblk_t leaf;\n\n\t/* free index block */\n\tdepth--;\n\tpath = path + depth;\n\tleaf = ext4_idx_pblock(path->p_idx);\n\tif (unlikely(path->p_hdr->eh_entries == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"path->p_hdr->eh_entries == 0\");\n\t\treturn -EIO;\n\t}\n\terr = ext4_ext_get_access(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\n\tif (path->p_idx != EXT_LAST_INDEX(path->p_hdr)) {\n\t\tint len = EXT_LAST_INDEX(path->p_hdr) - path->p_idx;\n\t\tlen *= sizeof(struct ext4_extent_idx);\n\t\tmemmove(path->p_idx, path->p_idx + 1, len);\n\t}\n\n\tle16_add_cpu(&path->p_hdr->eh_entries, -1);\n\terr = ext4_ext_dirty(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\text_debug(\"index is empty, remove it, free block %llu\\n\", leaf);\n\ttrace_ext4_ext_rm_idx(inode, leaf);\n\n\text4_free_blocks(handle, inode, NULL, leaf, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n\n\twhile (--depth >= 0) {\n\t\tif (path->p_idx != EXT_FIRST_INDEX(path->p_hdr))\n\t\t\tbreak;\n\t\tpath--;\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath->p_idx->ei_block = (path+1)->p_idx->ei_block;\n\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\treturn err;\n}\n\n/*\n * ext4_ext_calc_credits_for_single_extent:\n * This routine returns max. credits that needed to insert an extent\n * to the extent tree.\n * When pass the actual path, the caller should calculate credits\n * under i_data_sem.\n */\nint ext4_ext_calc_credits_for_single_extent(struct inode *inode, int nrblocks,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path) {\n\t\tint depth = ext_depth(inode);\n\t\tint ret = 0;\n\n\t\t/* probably there is space in leaf? */\n\t\tif (le16_to_cpu(path[depth].p_hdr->eh_entries)\n\t\t\t\t< le16_to_cpu(path[depth].p_hdr->eh_max)) {\n\n\t\t\t/*\n\t\t\t *  There are some space in the leaf tree, no\n\t\t\t *  need to account for leaf block credit\n\t\t\t *\n\t\t\t *  bitmaps and block group descriptor blocks\n\t\t\t *  and other metadata blocks still need to be\n\t\t\t *  accounted.\n\t\t\t */\n\t\t\t/* 1 bitmap, 1 block group descriptor */\n\t\t\tret = 2 + EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ext4_chunk_trans_blocks(inode, nrblocks);\n}\n\n/*\n * How many index/leaf blocks need to change/allocate to add @extents extents?\n *\n * If we add a single extent, then in the worse case, each tree level\n * index/leaf need to be changed in case of the tree split.\n *\n * If more extents are inserted, they could cause the whole tree split more\n * than once, but this is really rare.\n */\nint ext4_ext_index_trans_blocks(struct inode *inode, int extents)\n{\n\tint index;\n\tint depth;\n\n\t/* If we are converting the inline data, only one is needed here. */\n\tif (ext4_has_inline_data(inode))\n\t\treturn 1;\n\n\tdepth = ext_depth(inode);\n\n\tif (extents <= 1)\n\t\tindex = depth * 2;\n\telse\n\t\tindex = depth * 3;\n\n\treturn index;\n}\n\nstatic inline int get_default_free_blocks_flags(struct inode *inode)\n{\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\treturn EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET;\n\telse if (ext4_should_journal_data(inode))\n\t\treturn EXT4_FREE_BLOCKS_FORGET;\n\treturn 0;\n}\n\nstatic int ext4_remove_blocks(handle_t *handle, struct inode *inode,\n\t\t\t      struct ext4_extent *ex,\n\t\t\t      long long *partial_cluster,\n\t\t\t      ext4_lblk_t from, ext4_lblk_t to)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tunsigned short ee_len = ext4_ext_get_actual_len(ex);\n\text4_fsblk_t pblk;\n\tint flags = get_default_free_blocks_flags(inode);\n\n\t/*\n\t * For bigalloc file systems, we never free a partial cluster\n\t * at the beginning of the extent.  Instead, we make a note\n\t * that we tried freeing the cluster, and check to see if we\n\t * need to free it on a subsequent call to ext4_remove_blocks,\n\t * or at the end of ext4_ext_rm_leaf or ext4_ext_remove_space.\n\t */\n\tflags |= EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER;\n\n\ttrace_ext4_remove_blocks(inode, ex, from, to, *partial_cluster);\n\t/*\n\t * If we have a partial cluster, and it's different from the\n\t * cluster of the last block, we need to explicitly free the\n\t * partial cluster here.\n\t */\n\tpblk = ext4_ext_pblock(ex) + ee_len - 1;\n\tif (*partial_cluster > 0 &&\n\t    *partial_cluster != (long long) EXT4_B2C(sbi, pblk)) {\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(sbi, *partial_cluster),\n\t\t\t\t sbi->s_cluster_ratio, flags);\n\t\t*partial_cluster = 0;\n\t}\n\n#ifdef EXTENTS_STATS\n\t{\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\tspin_lock(&sbi->s_ext_stats_lock);\n\t\tsbi->s_ext_blocks += ee_len;\n\t\tsbi->s_ext_extents++;\n\t\tif (ee_len < sbi->s_ext_min)\n\t\t\tsbi->s_ext_min = ee_len;\n\t\tif (ee_len > sbi->s_ext_max)\n\t\t\tsbi->s_ext_max = ee_len;\n\t\tif (ext_depth(inode) > sbi->s_depth_max)\n\t\t\tsbi->s_depth_max = ext_depth(inode);\n\t\tspin_unlock(&sbi->s_ext_stats_lock);\n\t}\n#endif\n\tif (from >= le32_to_cpu(ex->ee_block)\n\t    && to == le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\t/* tail removal */\n\t\text4_lblk_t num;\n\t\tlong long first_cluster;\n\n\t\tnum = le32_to_cpu(ex->ee_block) + ee_len - from;\n\t\tpblk = ext4_ext_pblock(ex) + ee_len - num;\n\t\t/*\n\t\t * Usually we want to free partial cluster at the end of the\n\t\t * extent, except for the situation when the cluster is still\n\t\t * used by any other extent (partial_cluster is negative).\n\t\t */\n\t\tif (*partial_cluster < 0 &&\n\t\t    *partial_cluster == -(long long) EXT4_B2C(sbi, pblk+num-1))\n\t\t\tflags |= EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER;\n\n\t\text_debug(\"free last %u blocks starting %llu partial %lld\\n\",\n\t\t\t  num, pblk, *partial_cluster);\n\t\text4_free_blocks(handle, inode, NULL, pblk, num, flags);\n\t\t/*\n\t\t * If the block range to be freed didn't start at the\n\t\t * beginning of a cluster, and we removed the entire\n\t\t * extent and the cluster is not used by any other extent,\n\t\t * save the partial cluster here, since we might need to\n\t\t * delete if we determine that the truncate or punch hole\n\t\t * operation has removed all of the blocks in the cluster.\n\t\t * If that cluster is used by another extent, preserve its\n\t\t * negative value so it isn't freed later on.\n\t\t *\n\t\t * If the whole extent wasn't freed, we've reached the\n\t\t * start of the truncated/punched region and have finished\n\t\t * removing blocks.  If there's a partial cluster here it's\n\t\t * shared with the remainder of the extent and is no longer\n\t\t * a candidate for removal.\n\t\t */\n\t\tif (EXT4_PBLK_COFF(sbi, pblk) && ee_len == num) {\n\t\t\tfirst_cluster = (long long) EXT4_B2C(sbi, pblk);\n\t\t\tif (first_cluster != -*partial_cluster)\n\t\t\t\t*partial_cluster = first_cluster;\n\t\t} else {\n\t\t\t*partial_cluster = 0;\n\t\t}\n\t} else\n\t\text4_error(sbi->s_sb, \"strange request: removal(2) \"\n\t\t\t   \"%u-%u from %u:%u\\n\",\n\t\t\t   from, to, le32_to_cpu(ex->ee_block), ee_len);\n\treturn 0;\n}\n\n\n/*\n * ext4_ext_rm_leaf() Removes the extents associated with the\n * blocks appearing between \"start\" and \"end\".  Both \"start\"\n * and \"end\" must appear in the same extent or EIO is returned.\n *\n * @handle: The journal handle\n * @inode:  The files inode\n * @path:   The path to the leaf\n * @partial_cluster: The cluster which we'll have to free if all extents\n *                   has been released from it.  However, if this value is\n *                   negative, it's a cluster just to the right of the\n *                   punched region and it must not be freed.\n * @start:  The first block to remove\n * @end:   The last block to remove\n */\nstatic int\next4_ext_rm_leaf(handle_t *handle, struct inode *inode,\n\t\t struct ext4_ext_path *path,\n\t\t long long *partial_cluster,\n\t\t ext4_lblk_t start, ext4_lblk_t end)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint err = 0, correct_index = 0;\n\tint depth = ext_depth(inode), credits;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t a, b;\n\tunsigned num;\n\text4_lblk_t ex_ee_block;\n\tunsigned short ex_ee_len;\n\tunsigned unwritten = 0;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t pblk;\n\n\t/* the header must be checked already in ext4_ext_remove_space() */\n\text_debug(\"truncate since %u in leaf to %u\\n\", start, end);\n\tif (!path[depth].p_hdr)\n\t\tpath[depth].p_hdr = ext_block_hdr(path[depth].p_bh);\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\t/* find where to start removing */\n\tex = path[depth].p_ext;\n\tif (!ex)\n\t\tex = EXT_LAST_EXTENT(eh);\n\n\tex_ee_block = le32_to_cpu(ex->ee_block);\n\tex_ee_len = ext4_ext_get_actual_len(ex);\n\n\ttrace_ext4_ext_rm_leaf(inode, start, ex, *partial_cluster);\n\n\twhile (ex >= EXT_FIRST_EXTENT(eh) &&\n\t\t\tex_ee_block + ex_ee_len > start) {\n\n\t\tif (ext4_ext_is_unwritten(ex))\n\t\t\tunwritten = 1;\n\t\telse\n\t\t\tunwritten = 0;\n\n\t\text_debug(\"remove ext %u:[%d]%d\\n\", ex_ee_block,\n\t\t\t  unwritten, ex_ee_len);\n\t\tpath[depth].p_ext = ex;\n\n\t\ta = ex_ee_block > start ? ex_ee_block : start;\n\t\tb = ex_ee_block+ex_ee_len - 1 < end ?\n\t\t\tex_ee_block+ex_ee_len - 1 : end;\n\n\t\text_debug(\"  border %u:%u\\n\", a, b);\n\n\t\t/* If this extent is beyond the end of the hole, skip it */\n\t\tif (end < ex_ee_block) {\n\t\t\t/*\n\t\t\t * We're going to skip this extent and move to another,\n\t\t\t * so note that its first cluster is in use to avoid\n\t\t\t * freeing it when removing blocks.  Eventually, the\n\t\t\t * right edge of the truncated/punched region will\n\t\t\t * be just to the left.\n\t\t\t */\n\t\t\tif (sbi->s_cluster_ratio > 1) {\n\t\t\t\tpblk = ext4_ext_pblock(ex);\n\t\t\t\t*partial_cluster =\n\t\t\t\t\t-(long long) EXT4_B2C(sbi, pblk);\n\t\t\t}\n\t\t\tex--;\n\t\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t\t\tcontinue;\n\t\t} else if (b != ex_ee_block + ex_ee_len - 1) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"can not handle truncate %u:%u \"\n\t\t\t\t\t \"on extent %u:%u\",\n\t\t\t\t\t start, end, ex_ee_block,\n\t\t\t\t\t ex_ee_block + ex_ee_len - 1);\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t} else if (a != ex_ee_block) {\n\t\t\t/* remove tail of the extent */\n\t\t\tnum = a - ex_ee_block;\n\t\t} else {\n\t\t\t/* remove whole extent: excellent! */\n\t\t\tnum = 0;\n\t\t}\n\t\t/*\n\t\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t\t * descriptor) for each block group; assume two block\n\t\t * groups plus ex_ee_len/blocks_per_block_group for\n\t\t * the worst case\n\t\t */\n\t\tcredits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));\n\t\tif (ex == EXT_FIRST_EXTENT(eh)) {\n\t\t\tcorrect_index = 1;\n\t\t\tcredits += (ext_depth(inode)) + 1;\n\t\t}\n\t\tcredits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);\n\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_remove_blocks(handle, inode, ex, partial_cluster,\n\t\t\t\t\t a, b);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (num == 0)\n\t\t\t/* this extent is removed; mark slot entirely unused */\n\t\t\text4_ext_store_pblock(ex, 0);\n\n\t\tex->ee_len = cpu_to_le16(num);\n\t\t/*\n\t\t * Do not mark unwritten if all the blocks in the\n\t\t * extent have been removed.\n\t\t */\n\t\tif (unwritten && num)\n\t\t\text4_ext_mark_unwritten(ex);\n\t\t/*\n\t\t * If the extent was completely released,\n\t\t * we need to remove it from the leaf\n\t\t */\n\t\tif (num == 0) {\n\t\t\tif (end != EXT_MAX_BLOCKS - 1) {\n\t\t\t\t/*\n\t\t\t\t * For hole punching, we need to scoot all the\n\t\t\t\t * extents up when an extent is removed so that\n\t\t\t\t * we dont have blank extents in the middle\n\t\t\t\t */\n\t\t\t\tmemmove(ex, ex+1, (EXT_LAST_EXTENT(eh) - ex) *\n\t\t\t\t\tsizeof(struct ext4_extent));\n\n\t\t\t\t/* Now get rid of the one at the end */\n\t\t\t\tmemset(EXT_LAST_EXTENT(eh), 0,\n\t\t\t\t\tsizeof(struct ext4_extent));\n\t\t\t}\n\t\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\t}\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\text_debug(\"new extent: %u:%u:%llu\\n\", ex_ee_block, num,\n\t\t\t\text4_ext_pblock(ex));\n\t\tex--;\n\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t}\n\n\tif (correct_index && eh->eh_entries)\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\n\t/*\n\t * If there's a partial cluster and at least one extent remains in\n\t * the leaf, free the partial cluster if it isn't shared with the\n\t * current extent.  If it is shared with the current extent\n\t * we zero partial_cluster because we've reached the start of the\n\t * truncated/punched region and we're done removing blocks.\n\t */\n\tif (*partial_cluster > 0 && ex >= EXT_FIRST_EXTENT(eh)) {\n\t\tpblk = ext4_ext_pblock(ex) + ex_ee_len - 1;\n\t\tif (*partial_cluster != (long long) EXT4_B2C(sbi, pblk)) {\n\t\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t\t EXT4_C2B(sbi, *partial_cluster),\n\t\t\t\t\t sbi->s_cluster_ratio,\n\t\t\t\t\t get_default_free_blocks_flags(inode));\n\t\t}\n\t\t*partial_cluster = 0;\n\t}\n\n\t/* if this leaf is free, then we should\n\t * remove it from index block above */\n\tif (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)\n\t\terr = ext4_ext_rm_idx(handle, inode, path, depth);\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_more_to_rm:\n * returns 1 if current index has to be freed (even partial)\n */\nstatic int\next4_ext_more_to_rm(struct ext4_ext_path *path)\n{\n\tBUG_ON(path->p_idx == NULL);\n\n\tif (path->p_idx < EXT_FIRST_INDEX(path->p_hdr))\n\t\treturn 0;\n\n\t/*\n\t * if truncate on deeper level happened, it wasn't partial,\n\t * so we have to consider current index for truncation\n\t */\n\tif (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block)\n\t\treturn 0;\n\treturn 1;\n}\n\nint ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,\n\t\t\t  ext4_lblk_t end)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint depth = ext_depth(inode);\n\tstruct ext4_ext_path *path = NULL;\n\tlong long partial_cluster = 0;\n\thandle_t *handle;\n\tint i = 0, err = 0;\n\n\text_debug(\"truncate since %u to %u\\n\", start, end);\n\n\t/* probably first extent we're gonna free will be last in block */\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, depth + 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\nagain:\n\ttrace_ext4_ext_remove_space(inode, start, end, depth);\n\n\t/*\n\t * Check if we are removing extents inside the extent tree. If that\n\t * is the case, we are going to punch a hole inside the extent tree\n\t * so we have to check whether we need to split the extent covering\n\t * the last block to remove so we can easily remove the part of it\n\t * in ext4_ext_rm_leaf().\n\t */\n\tif (end < EXT_MAX_BLOCKS - 1) {\n\t\tstruct ext4_extent *ex;\n\t\text4_lblk_t ee_block, ex_end, lblk;\n\t\text4_fsblk_t pblk;\n\n\t\t/* find extent for or closest extent to this block */\n\t\tpath = ext4_find_extent(inode, end, NULL, EXT4_EX_NOCACHE);\n\t\tif (IS_ERR(path)) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn PTR_ERR(path);\n\t\t}\n\t\tdepth = ext_depth(inode);\n\t\t/* Leaf not may not exist only if inode has no blocks at all */\n\t\tex = path[depth].p_ext;\n\t\tif (!ex) {\n\t\t\tif (depth) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"path[%d].p_hdr == NULL\",\n\t\t\t\t\t\t depth);\n\t\t\t\terr = -EIO;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\tee_block = le32_to_cpu(ex->ee_block);\n\t\tex_end = ee_block + ext4_ext_get_actual_len(ex) - 1;\n\n\t\t/*\n\t\t * See if the last block is inside the extent, if so split\n\t\t * the extent at 'end' block so we can easily remove the\n\t\t * tail of the first part of the split extent in\n\t\t * ext4_ext_rm_leaf().\n\t\t */\n\t\tif (end >= ee_block && end < ex_end) {\n\n\t\t\t/*\n\t\t\t * If we're going to split the extent, note that\n\t\t\t * the cluster containing the block after 'end' is\n\t\t\t * in use to avoid freeing it when removing blocks.\n\t\t\t */\n\t\t\tif (sbi->s_cluster_ratio > 1) {\n\t\t\t\tpblk = ext4_ext_pblock(ex) + end - ee_block + 2;\n\t\t\t\tpartial_cluster =\n\t\t\t\t\t-(long long) EXT4_B2C(sbi, pblk);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Split the extent in two so that 'end' is the last\n\t\t\t * block in the first new extent. Also we should not\n\t\t\t * fail removing space due to ENOSPC so try to use\n\t\t\t * reserved block if that happens.\n\t\t\t */\n\t\t\terr = ext4_force_split_extent_at(handle, inode, &path,\n\t\t\t\t\t\t\t end + 1, 1);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t} else if (sbi->s_cluster_ratio > 1 && end >= ex_end) {\n\t\t\t/*\n\t\t\t * If there's an extent to the right its first cluster\n\t\t\t * contains the immediate right boundary of the\n\t\t\t * truncated/punched region.  Set partial_cluster to\n\t\t\t * its negative value so it won't be freed if shared\n\t\t\t * with the current extent.  The end < ee_block case\n\t\t\t * is handled in ext4_ext_rm_leaf().\n\t\t\t */\n\t\t\tlblk = ex_end + 1;\n\t\t\terr = ext4_ext_search_right(inode, path, &lblk, &pblk,\n\t\t\t\t\t\t    &ex);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tif (pblk)\n\t\t\t\tpartial_cluster =\n\t\t\t\t\t-(long long) EXT4_B2C(sbi, pblk);\n\t\t}\n\t}\n\t/*\n\t * We start scanning from right side, freeing all the blocks\n\t * after i_size and walking into the tree depth-wise.\n\t */\n\tdepth = ext_depth(inode);\n\tif (path) {\n\t\tint k = i = depth;\n\t\twhile (--k > 0)\n\t\t\tpath[k].p_block =\n\t\t\t\tle16_to_cpu(path[k].p_hdr->eh_entries)+1;\n\t} else {\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 1),\n\t\t\t       GFP_NOFS);\n\t\tif (path == NULL) {\n\t\t\text4_journal_stop(handle);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tpath[0].p_maxdepth = path[0].p_depth = depth;\n\t\tpath[0].p_hdr = ext_inode_hdr(inode);\n\t\ti = 0;\n\n\t\tif (ext4_ext_check(inode, path[0].p_hdr, depth, 0)) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\terr = 0;\n\n\twhile (i >= 0 && err == 0) {\n\t\tif (i == depth) {\n\t\t\t/* this is leaf block */\n\t\t\terr = ext4_ext_rm_leaf(handle, inode, path,\n\t\t\t\t\t       &partial_cluster, start,\n\t\t\t\t\t       end);\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* this is index block */\n\t\tif (!path[i].p_hdr) {\n\t\t\text_debug(\"initialize header\\n\");\n\t\t\tpath[i].p_hdr = ext_block_hdr(path[i].p_bh);\n\t\t}\n\n\t\tif (!path[i].p_idx) {\n\t\t\t/* this level hasn't been touched yet */\n\t\t\tpath[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;\n\t\t\text_debug(\"init index ptr: hdr 0x%p, num %d\\n\",\n\t\t\t\t  path[i].p_hdr,\n\t\t\t\t  le16_to_cpu(path[i].p_hdr->eh_entries));\n\t\t} else {\n\t\t\t/* we were already here, see at next index */\n\t\t\tpath[i].p_idx--;\n\t\t}\n\n\t\text_debug(\"level %d - index, first 0x%p, cur 0x%p\\n\",\n\t\t\t\ti, EXT_FIRST_INDEX(path[i].p_hdr),\n\t\t\t\tpath[i].p_idx);\n\t\tif (ext4_ext_more_to_rm(path + i)) {\n\t\t\tstruct buffer_head *bh;\n\t\t\t/* go to the next level */\n\t\t\text_debug(\"move to level %d (block %llu)\\n\",\n\t\t\t\t  i + 1, ext4_idx_pblock(path[i].p_idx));\n\t\t\tmemset(path + i + 1, 0, sizeof(*path));\n\t\t\tbh = read_extent_tree_block(inode,\n\t\t\t\text4_idx_pblock(path[i].p_idx), depth - i - 1,\n\t\t\t\tEXT4_EX_NOCACHE);\n\t\t\tif (IS_ERR(bh)) {\n\t\t\t\t/* should we reset i_size? */\n\t\t\t\terr = PTR_ERR(bh);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* Yield here to deal with large extent trees.\n\t\t\t * Should be a no-op if we did IO above. */\n\t\t\tcond_resched();\n\t\t\tif (WARN_ON(i + 1 > depth)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpath[i + 1].p_bh = bh;\n\n\t\t\t/* save actual number of indexes since this\n\t\t\t * number is changed at the next iteration */\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);\n\t\t\ti++;\n\t\t} else {\n\t\t\t/* we finished processing this index, go up */\n\t\t\tif (path[i].p_hdr->eh_entries == 0 && i > 0) {\n\t\t\t\t/* index is empty, remove it;\n\t\t\t\t * handle must be already prepared by the\n\t\t\t\t * truncatei_leaf() */\n\t\t\t\terr = ext4_ext_rm_idx(handle, inode, path, i);\n\t\t\t}\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\text_debug(\"return to level %d\\n\", i);\n\t\t}\n\t}\n\n\ttrace_ext4_ext_remove_space_done(inode, start, end, depth,\n\t\t\tpartial_cluster, path->p_hdr->eh_entries);\n\n\t/*\n\t * If we still have something in the partial cluster and we have removed\n\t * even the first extent, then we should free the blocks in the partial\n\t * cluster as well.  (This code will only run when there are no leaves\n\t * to the immediate left of the truncated/punched region.)\n\t */\n\tif (partial_cluster > 0 && err == 0) {\n\t\t/* don't zero partial_cluster since it's not used afterwards */\n\t\text4_free_blocks(handle, inode, NULL,\n\t\t\t\t EXT4_C2B(sbi, partial_cluster),\n\t\t\t\t sbi->s_cluster_ratio,\n\t\t\t\t get_default_free_blocks_flags(inode));\n\t}\n\n\t/* TODO: flexible tree reduction should be here */\n\tif (path->p_hdr->eh_entries == 0) {\n\t\t/*\n\t\t * truncate to zero freed all the tree,\n\t\t * so we need to correct eh_depth\n\t\t */\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err == 0) {\n\t\t\text_inode_hdr(inode)->eh_depth = 0;\n\t\t\text_inode_hdr(inode)->eh_max =\n\t\t\t\tcpu_to_le16(ext4_ext_space_root(inode, 0));\n\t\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\t}\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tpath = NULL;\n\tif (err == -EAGAIN)\n\t\tgoto again;\n\text4_journal_stop(handle);\n\n\treturn err;\n}\n\n/*\n * called at mount time\n */\nvoid ext4_ext_init(struct super_block *sb)\n{\n\t/*\n\t * possible initialization would be here\n\t */\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) {\n#if defined(AGGRESSIVE_TEST) || defined(CHECK_BINSEARCH) || defined(EXTENTS_STATS)\n\t\tprintk(KERN_INFO \"EXT4-fs: file extents enabled\"\n#ifdef AGGRESSIVE_TEST\n\t\t       \", aggressive tests\"\n#endif\n#ifdef CHECK_BINSEARCH\n\t\t       \", check binsearch\"\n#endif\n#ifdef EXTENTS_STATS\n\t\t       \", stats\"\n#endif\n\t\t       \"\\n\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tspin_lock_init(&EXT4_SB(sb)->s_ext_stats_lock);\n\t\tEXT4_SB(sb)->s_ext_min = 1 << 30;\n\t\tEXT4_SB(sb)->s_ext_max = 0;\n#endif\n\t}\n}\n\n/*\n * called at umount time\n */\nvoid ext4_ext_release(struct super_block *sb)\n{\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS))\n\t\treturn;\n\n#ifdef EXTENTS_STATS\n\tif (EXT4_SB(sb)->s_ext_blocks && EXT4_SB(sb)->s_ext_extents) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\tprintk(KERN_ERR \"EXT4-fs: %lu blocks in %lu extents (%lu ave)\\n\",\n\t\t\tsbi->s_ext_blocks, sbi->s_ext_extents,\n\t\t\tsbi->s_ext_blocks / sbi->s_ext_extents);\n\t\tprintk(KERN_ERR \"EXT4-fs: extents: %lu min, %lu max, max depth %lu\\n\",\n\t\t\tsbi->s_ext_min, sbi->s_ext_max, sbi->s_depth_max);\n\t}\n#endif\n}\n\nstatic int ext4_zeroout_es(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_lblk_t  ee_block;\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\n\tee_block  = le32_to_cpu(ex->ee_block);\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\n\tif (ee_len == 0)\n\t\treturn 0;\n\n\treturn ext4_es_insert_extent(inode, ee_block, ee_len, ee_pblock,\n\t\t\t\t     EXTENT_STATUS_WRITTEN);\n}\n\n/* FIXME!! we need to try to merge to left or right after zero-out  */\nstatic int ext4_ext_zeroout(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\tint ret;\n\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\n\tret = sb_issue_zeroout(inode->i_sb, ee_pblock, ee_len, GFP_NOFS);\n\tif (ret > 0)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * ext4_split_extent_at() splits an extent at given block.\n *\n * @handle: the journal handle\n * @inode: the file inode\n * @path: the path to the extent\n * @split: the logical block where the extent is splitted.\n * @split_flags: indicates if the extent could be zeroout if split fails, and\n *\t\t the states(init or unwritten) of new extents.\n * @flags: flags used to insert new extent to extent tree.\n *\n *\n * Splits extent [a, b] into two extents [a, @split) and [@split, b], states\n * of which are deterimined by split_flag.\n *\n * There are two cases:\n *  a> the extent are splitted into two extent.\n *  b> split is not needed, and just mark the extent.\n *\n * return 0 on success.\n */\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path **ppath,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex, zero_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==\n\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\tBUG_ON(!ext4_ext_is_unwritten(ex) &&\n\t       split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t     EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t     EXT4_EXT_MARK_UNWRIT2));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNWRIT2)\n\t\t\text4_ext_mark_unwritten(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNWRIT1)\n\t\text4_ext_mark_unwritten(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNWRIT2)\n\t\text4_ext_mark_unwritten(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, ppath, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {\n\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1) {\n\t\t\t\terr = ext4_ext_zeroout(inode, ex2);\n\t\t\t\tzero_ex.ee_block = ex2->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(ex2));\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex2));\n\t\t\t} else {\n\t\t\t\terr = ext4_ext_zeroout(inode, ex);\n\t\t\t\tzero_ex.ee_block = ex->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(ex));\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t}\n\t\t} else {\n\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tzero_ex.ee_block = orig_ex.ee_block;\n\t\t\tzero_ex.ee_len = cpu_to_le16(\n\t\t\t\t\t\text4_ext_get_actual_len(&orig_ex));\n\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t      ext4_ext_pblock(&orig_ex));\n\t\t}\n\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\n\t\t/* update extent status tree */\n\t\terr = ext4_zeroout_es(inode, &zero_ex);\n\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + path->p_depth);\n\treturn err;\n}\n\n/*\n * ext4_split_extents() splits an extent and mark extent which is covered\n * by @map as split_flags indicates\n *\n * It may result in splitting the extent into multiple extents (up to three)\n * There are three possibilities:\n *   a> There is no split required\n *   b> Splits in two extents: Split is happening at either end of the extent\n *   c> Splits in three extents: Somone is splitting in middle of the extent\n *\n */\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path **ppath,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint unwritten;\n\tint split_flag1, flags1;\n\tint allocated = map->m_len;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tunwritten = ext4_ext_is_unwritten(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (unwritten)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNWRIT2;\n\t\tif (split_flag & EXT4_EXT_DATA_VALID2)\n\t\t\tsplit_flag1 |= EXT4_EXT_DATA_VALID1;\n\t\terr = ext4_split_extent_at(handle, inode, ppath,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else {\n\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t}\n\t/*\n\t * Update path is required because previous ext4_split_extent_at() may\n\t * result in split of original leaf or extent zeroout.\n\t */\n\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tif (!ex) {\n\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t (unsigned long) map->m_lblk);\n\t\treturn -EIO;\n\t}\n\tunwritten = ext4_ext_is_unwritten(ex);\n\tsplit_flag1 = 0;\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_DATA_VALID2;\n\t\tif (unwritten) {\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNWRIT1;\n\t\t\tsplit_flag1 |= split_flag & (EXT4_EXT_MAY_ZEROOUT |\n\t\t\t\t\t\t     EXT4_EXT_MARK_UNWRIT2);\n\t\t}\n\t\terr = ext4_split_extent_at(handle, inode, ppath,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() if someone tries to write\n * to an unwritten extent. It may result in splitting the unwritten\n * extent into multiple extents (up to three - one initialized and two\n * unwritten).\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be initialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * Pre-conditions:\n *  - The extent pointed to by 'path' is unwritten.\n *  - The extent pointed to by 'path' contains a superset\n *    of the logical span [map->m_lblk, map->m_lblk + map->m_len).\n *\n * Post-conditions on success:\n *  - the returned value is the number of blocks beyond map->l_lblk\n *    that are allocated and initialized.\n *    It is guaranteed to be >= map->m_len.\n */\nstatic int ext4_ext_convert_to_initialized(handle_t *handle,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_map_blocks *map,\n\t\t\t\t\t   struct ext4_ext_path **ppath,\n\t\t\t\t\t   int flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_sb_info *sbi;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_map_blocks split_map;\n\tstruct ext4_extent zero_ex;\n\tstruct ext4_extent *ex, *abut_ex;\n\text4_lblk_t ee_block, eof_block;\n\tunsigned int ee_len, depth, map_len = map->m_len;\n\tint allocated = 0, max_zeroout = 0;\n\tint err = 0;\n\tint split_flag = 0;\n\n\text_debug(\"ext4_ext_convert_to_initialized: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map_len);\n\n\tsbi = EXT4_SB(inode->i_sb);\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map_len)\n\t\teof_block = map->m_lblk + map_len;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tzero_ex.ee_len = 0;\n\n\ttrace_ext4_ext_convert_to_initialized_enter(inode, map, ex);\n\n\t/* Pre-conditions */\n\tBUG_ON(!ext4_ext_is_unwritten(ex));\n\tBUG_ON(!in_range(map->m_lblk, ee_block, ee_len));\n\n\t/*\n\t * Attempt to transfer newly initialized blocks from the currently\n\t * unwritten extent to its neighbor. This is much cheaper\n\t * than an insertion followed by a merge as those involve costly\n\t * memmove() calls. Transferring to the left is the common case in\n\t * steady state for workloads doing fallocate(FALLOC_FL_KEEP_SIZE)\n\t * followed by append writes.\n\t *\n\t * Limitations of the current logic:\n\t *  - L1: we do not deal with writes covering the whole extent.\n\t *    This would require removing the extent if the transfer\n\t *    is possible.\n\t *  - L2: we only attempt to merge with an extent stored in the\n\t *    same extent tree node.\n\t */\n\tif ((map->m_lblk == ee_block) &&\n\t\t/* See if we can merge left */\n\t\t(map_len < ee_len) &&\t\t/*L1*/\n\t\t(ex > EXT_FIRST_EXTENT(eh))) {\t/*L2*/\n\t\text4_lblk_t prev_lblk;\n\t\text4_fsblk_t prev_pblk, ee_pblk;\n\t\tunsigned int prev_len;\n\n\t\tabut_ex = ex - 1;\n\t\tprev_lblk = le32_to_cpu(abut_ex->ee_block);\n\t\tprev_len = ext4_ext_get_actual_len(abut_ex);\n\t\tprev_pblk = ext4_ext_pblock(abut_ex);\n\t\tee_pblk = ext4_ext_pblock(ex);\n\n\t\t/*\n\t\t * A transfer of blocks from 'ex' to 'abut_ex' is allowed\n\t\t * upon those conditions:\n\t\t * - C1: abut_ex is initialized,\n\t\t * - C2: abut_ex is logically abutting ex,\n\t\t * - C3: abut_ex is physically abutting ex,\n\t\t * - C4: abut_ex can receive the additional blocks without\n\t\t *   overflowing the (initialized) length limit.\n\t\t */\n\t\tif ((!ext4_ext_is_unwritten(abut_ex)) &&\t\t/*C1*/\n\t\t\t((prev_lblk + prev_len) == ee_block) &&\t\t/*C2*/\n\t\t\t((prev_pblk + prev_len) == ee_pblk) &&\t\t/*C3*/\n\t\t\t(prev_len < (EXT_INIT_MAX_LEN - map_len))) {\t/*C4*/\n\t\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\ttrace_ext4_ext_convert_to_initialized_fastpath(inode,\n\t\t\t\tmap, ex, abut_ex);\n\n\t\t\t/* Shift the start of ex by 'map_len' blocks */\n\t\t\tex->ee_block = cpu_to_le32(ee_block + map_len);\n\t\t\text4_ext_store_pblock(ex, ee_pblk + map_len);\n\t\t\tex->ee_len = cpu_to_le16(ee_len - map_len);\n\t\t\text4_ext_mark_unwritten(ex); /* Restore the flag */\n\n\t\t\t/* Extend abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_len = cpu_to_le16(prev_len + map_len);\n\n\t\t\t/* Result: number of initialized blocks past m_lblk */\n\t\t\tallocated = map_len;\n\t\t}\n\t} else if (((map->m_lblk + map_len) == (ee_block + ee_len)) &&\n\t\t   (map_len < ee_len) &&\t/*L1*/\n\t\t   ex < EXT_LAST_EXTENT(eh)) {\t/*L2*/\n\t\t/* See if we can merge right */\n\t\text4_lblk_t next_lblk;\n\t\text4_fsblk_t next_pblk, ee_pblk;\n\t\tunsigned int next_len;\n\n\t\tabut_ex = ex + 1;\n\t\tnext_lblk = le32_to_cpu(abut_ex->ee_block);\n\t\tnext_len = ext4_ext_get_actual_len(abut_ex);\n\t\tnext_pblk = ext4_ext_pblock(abut_ex);\n\t\tee_pblk = ext4_ext_pblock(ex);\n\n\t\t/*\n\t\t * A transfer of blocks from 'ex' to 'abut_ex' is allowed\n\t\t * upon those conditions:\n\t\t * - C1: abut_ex is initialized,\n\t\t * - C2: abut_ex is logically abutting ex,\n\t\t * - C3: abut_ex is physically abutting ex,\n\t\t * - C4: abut_ex can receive the additional blocks without\n\t\t *   overflowing the (initialized) length limit.\n\t\t */\n\t\tif ((!ext4_ext_is_unwritten(abut_ex)) &&\t\t/*C1*/\n\t\t    ((map->m_lblk + map_len) == next_lblk) &&\t\t/*C2*/\n\t\t    ((ee_pblk + ee_len) == next_pblk) &&\t\t/*C3*/\n\t\t    (next_len < (EXT_INIT_MAX_LEN - map_len))) {\t/*C4*/\n\t\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\ttrace_ext4_ext_convert_to_initialized_fastpath(inode,\n\t\t\t\tmap, ex, abut_ex);\n\n\t\t\t/* Shift the start of abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_block = cpu_to_le32(next_lblk - map_len);\n\t\t\text4_ext_store_pblock(abut_ex, next_pblk - map_len);\n\t\t\tex->ee_len = cpu_to_le16(ee_len - map_len);\n\t\t\text4_ext_mark_unwritten(ex); /* Restore the flag */\n\n\t\t\t/* Extend abut_ex by 'map_len' blocks */\n\t\t\tabut_ex->ee_len = cpu_to_le16(next_len + map_len);\n\n\t\t\t/* Result: number of initialized blocks past m_lblk */\n\t\t\tallocated = map_len;\n\t\t}\n\t}\n\tif (allocated) {\n\t\t/* Mark the block containing both extents as dirty */\n\t\text4_ext_dirty(handle, inode, path + depth);\n\n\t\t/* Update path to point to the right extent */\n\t\tpath[depth].p_ext = abut_ex;\n\t\tgoto out;\n\t} else\n\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\n\tWARN_ON(map->m_lblk < ee_block);\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully inside i_size or new_size.\n\t */\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\n\tif (EXT4_EXT_MAY_ZEROOUT & split_flag)\n\t\tmax_zeroout = sbi->s_extent_max_zeroout_kb >>\n\t\t\t(inode->i_sb->s_blocksize_bits - 10);\n\n\t/* If extent is less than s_max_zeroout_kb, zeroout directly */\n\tif (max_zeroout && (ee_len <= max_zeroout)) {\n\t\terr = ext4_ext_zeroout(inode, ex);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tzero_ex.ee_block = ex->ee_block;\n\t\tzero_ex.ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex));\n\t\text4_ext_store_pblock(&zero_ex, ext4_ext_pblock(ex));\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\t\text4_ext_mark_initialized(ex);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * four cases:\n\t * 1. split the extent into three extents.\n\t * 2. split the extent into two extents, zeroout the first half.\n\t * 3. split the extent into two extents, zeroout the second half.\n\t * 4. split the extent into two extents with out zeroout.\n\t */\n\tsplit_map.m_lblk = map->m_lblk;\n\tsplit_map.m_len = map->m_len;\n\n\tif (max_zeroout && (allocated > map->m_len)) {\n\t\tif (allocated <= max_zeroout) {\n\t\t\t/* case 3 */\n\t\t\tzero_ex.ee_block =\n\t\t\t\t\t cpu_to_le32(map->m_lblk);\n\t\t\tzero_ex.ee_len = cpu_to_le16(allocated);\n\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\text4_ext_pblock(ex) + map->m_lblk - ee_block);\n\t\t\terr = ext4_ext_zeroout(inode, &zero_ex);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tsplit_map.m_lblk = map->m_lblk;\n\t\t\tsplit_map.m_len = allocated;\n\t\t} else if (map->m_lblk - ee_block + map->m_len < max_zeroout) {\n\t\t\t/* case 2 */\n\t\t\tif (map->m_lblk != ee_block) {\n\t\t\t\tzero_ex.ee_block = ex->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(map->m_lblk -\n\t\t\t\t\t\t\tee_block);\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t\terr = ext4_ext_zeroout(inode, &zero_ex);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tsplit_map.m_lblk = ee_block;\n\t\t\tsplit_map.m_len = map->m_lblk - ee_block + map->m_len;\n\t\t\tallocated = map->m_len;\n\t\t}\n\t}\n\n\terr = ext4_split_extent(handle, inode, ppath, &split_map, split_flag,\n\t\t\t\tflags);\n\tif (err > 0)\n\t\terr = 0;\nout:\n\t/* If we have gotten a failure, don't zero out status tree */\n\tif (!err)\n\t\terr = ext4_zeroout_es(inode, &zero_ex);\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() from\n * ext4_get_blocks_dio_write() when DIO to write\n * to an unwritten extent.\n *\n * Writing to an unwritten extent may result in splitting the unwritten\n * extent into multiple initialized/unwritten extents (up to three)\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be unwritten\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * This works the same way in the case of initialized -> unwritten conversion.\n *\n * One of more index blocks maybe needed if the extent tree grow after\n * the unwritten extent split. To prevent ENOSPC occur at the IO\n * complete, we need to split the unwritten extent before DIO submit\n * the IO. The unwritten extent called at this time will be split\n * into three unwritten extent(at most). After IO complete, the part\n * being filled will be convert to initialized by the end_io callback function\n * via ext4_convert_unwritten_extents().\n *\n * Returns the size of unwritten extent to be written on success.\n */\nstatic int ext4_split_convert_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path **ppath,\n\t\t\t\t\tint flags)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"%s: inode %lu, logical block %llu, max_blocks %u\\n\",\n\t\t  __func__, inode->i_ino,\n\t\t  (unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\t/* Convert to unwritten */\n\tif (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN) {\n\t\tsplit_flag |= EXT4_EXT_DATA_VALID1;\n\t/* Convert to initialized */\n\t} else if (flags & EXT4_GET_BLOCKS_CONVERT) {\n\t\tsplit_flag |= ee_block + ee_len <= eof_block ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tsplit_flag |= (EXT4_EXT_MARK_UNWRIT2 | EXT4_EXT_DATA_VALID2);\n\t}\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, ppath, map, split_flag, flags);\n}\n\nstatic int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\t\tstruct ext4_ext_path **ppath)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\tunsigned int ee_len;\n\tint depth;\n\tint err = 0;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t  (unsigned long long)ee_block, ee_len);\n\n\t/* If extent is larger than requested it is a clear sign that we still\n\t * have some extent state machine issues left. So extent_split is still\n\t * required.\n\t * TODO: Once all related issues will be fixed this situation should be\n\t * illegal.\n\t */\n\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n#ifdef EXT4_DEBUG\n\t\text4_warning(\"Inode (%ld) finished: extent logical block %llu,\"\n\t\t\t     \" len %u; IO logical block %llu, len %u\\n\",\n\t\t\t     inode->i_ino, (unsigned long long)ee_block, ee_len,\n\t\t\t     (unsigned long long)map->m_lblk, map->m_len);\n#endif\n\t\terr = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\t\t\t EXT4_GET_BLOCKS_CONVERT);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = ext_depth(inode);\n\t\tex = path[depth].p_ext;\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}\n\nstatic void unmap_underlying_metadata_blocks(struct block_device *bdev,\n\t\t\tsector_t block, int count)\n{\n\tint i;\n\tfor (i = 0; i < count; i++)\n                unmap_underlying_metadata(bdev, block + i);\n}\n\n/*\n * Handle EOFBLOCKS_FL flag, clearing it if necessary\n */\nstatic int check_eofblocks_fl(handle_t *handle, struct inode *inode,\n\t\t\t      ext4_lblk_t lblk,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      unsigned int len)\n{\n\tint i, depth;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *last_ex;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS))\n\t\treturn 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\n\t/*\n\t * We're going to remove EOFBLOCKS_FL entirely in future so we\n\t * do not care for this case anymore. Simply remove the flag\n\t * if there are no extents.\n\t */\n\tif (unlikely(!eh->eh_entries))\n\t\tgoto out;\n\tlast_ex = EXT_LAST_EXTENT(eh);\n\t/*\n\t * We should clear the EOFBLOCKS_FL flag if we are writing the\n\t * last block in the last extent in the file.  We test this by\n\t * first checking to see if the caller to\n\t * ext4_ext_get_blocks() was interested in the last block (or\n\t * a block beyond the last block) in the current extent.  If\n\t * this turns out to be false, we can bail out from this\n\t * function immediately.\n\t */\n\tif (lblk + len < le32_to_cpu(last_ex->ee_block) +\n\t    ext4_ext_get_actual_len(last_ex))\n\t\treturn 0;\n\t/*\n\t * If the caller does appear to be planning to write at or\n\t * beyond the end of the current extent, we then test to see\n\t * if the current extent is the last extent in the file, by\n\t * checking to make sure it was reached via the rightmost node\n\t * at each level of the tree.\n\t */\n\tfor (i = depth-1; i >= 0; i--)\n\t\tif (path[i].p_idx != EXT_LAST_INDEX(path[i].p_hdr))\n\t\t\treturn 0;\nout:\n\text4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\treturn ext4_mark_inode_dirty(handle, inode);\n}\n\n/**\n * ext4_find_delalloc_range: find delayed allocated block in the given range.\n *\n * Return 1 if there is a delalloc block in the range, otherwise 0.\n */\nint ext4_find_delalloc_range(struct inode *inode,\n\t\t\t     ext4_lblk_t lblk_start,\n\t\t\t     ext4_lblk_t lblk_end)\n{\n\tstruct extent_status es;\n\n\text4_es_find_delayed_extent_range(inode, lblk_start, lblk_end, &es);\n\tif (es.es_len == 0)\n\t\treturn 0; /* there is no delay extent in this tree */\n\telse if (es.es_lblk <= lblk_start &&\n\t\t lblk_start < es.es_lblk + es.es_len)\n\t\treturn 1;\n\telse if (lblk_start <= es.es_lblk && es.es_lblk <= lblk_end)\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n\nint ext4_find_delalloc_cluster(struct inode *inode, ext4_lblk_t lblk)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t lblk_start, lblk_end;\n\tlblk_start = EXT4_LBLK_CMASK(sbi, lblk);\n\tlblk_end = lblk_start + sbi->s_cluster_ratio - 1;\n\n\treturn ext4_find_delalloc_range(inode, lblk_start, lblk_end);\n}\n\n/**\n * Determines how many complete clusters (out of those specified by the 'map')\n * are under delalloc and were reserved quota for.\n * This function is called when we are writing out the blocks that were\n * originally written with their allocation delayed, but then the space was\n * allocated using fallocate() before the delayed allocation could be resolved.\n * The cases to look for are:\n * ('=' indicated delayed allocated blocks\n *  '-' indicates non-delayed allocated blocks)\n * (a) partial clusters towards beginning and/or end outside of allocated range\n *     are not delalloc'ed.\n *\tEx:\n *\t|----c---=|====c====|====c====|===-c----|\n *\t         |++++++ allocated ++++++|\n *\t==> 4 complete clusters in above example\n *\n * (b) partial cluster (outside of allocated range) towards either end is\n *     marked for delayed allocation. In this case, we will exclude that\n *     cluster.\n *\tEx:\n *\t|----====c========|========c========|\n *\t     |++++++ allocated ++++++|\n *\t==> 1 complete clusters in above example\n *\n *\tEx:\n *\t|================c================|\n *            |++++++ allocated ++++++|\n *\t==> 0 complete clusters in above example\n *\n * The ext4_da_update_reserve_space will be called only if we\n * determine here that there were some \"entire\" clusters that span\n * this 'allocated' range.\n * In the non-bigalloc case, this function will just end up returning num_blks\n * without ever calling ext4_find_delalloc_range.\n */\nstatic unsigned int\nget_reserved_cluster_alloc(struct inode *inode, ext4_lblk_t lblk_start,\n\t\t\t   unsigned int num_blks)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_lblk_t alloc_cluster_start, alloc_cluster_end;\n\text4_lblk_t lblk_from, lblk_to, c_offset;\n\tunsigned int allocated_clusters = 0;\n\n\talloc_cluster_start = EXT4_B2C(sbi, lblk_start);\n\talloc_cluster_end = EXT4_B2C(sbi, lblk_start + num_blks - 1);\n\n\t/* max possible clusters for this allocation */\n\tallocated_clusters = alloc_cluster_end - alloc_cluster_start + 1;\n\n\ttrace_ext4_get_reserved_cluster_alloc(inode, lblk_start, num_blks);\n\n\t/* Check towards left side */\n\tc_offset = EXT4_LBLK_COFF(sbi, lblk_start);\n\tif (c_offset) {\n\t\tlblk_from = EXT4_LBLK_CMASK(sbi, lblk_start);\n\t\tlblk_to = lblk_from + c_offset - 1;\n\n\t\tif (ext4_find_delalloc_range(inode, lblk_from, lblk_to))\n\t\t\tallocated_clusters--;\n\t}\n\n\t/* Now check towards right. */\n\tc_offset = EXT4_LBLK_COFF(sbi, lblk_start + num_blks);\n\tif (allocated_clusters && c_offset) {\n\t\tlblk_from = lblk_start + num_blks;\n\t\tlblk_to = lblk_from + (sbi->s_cluster_ratio - c_offset) - 1;\n\n\t\tif (ext4_find_delalloc_range(inode, lblk_from, lblk_to))\n\t\t\tallocated_clusters--;\n\t}\n\n\treturn allocated_clusters;\n}\n\nstatic int\nconvert_initialized_extent(handle_t *handle, struct inode *inode,\n\t\t\t   struct ext4_map_blocks *map,\n\t\t\t   struct ext4_ext_path **ppath, int flags,\n\t\t\t   unsigned int allocated, ext4_fsblk_t newblock)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block;\n\tunsigned int ee_len;\n\tint depth;\n\tint err = 0;\n\n\t/*\n\t * Make sure that the extent is no bigger than we support with\n\t * unwritten extent\n\t */\n\tif (map->m_len > EXT_UNWRITTEN_MAX_LEN)\n\t\tmap->m_len = EXT_UNWRITTEN_MAX_LEN / 2;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\text_debug(\"%s: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", __func__, inode->i_ino,\n\t\t  (unsigned long long)ee_block, ee_len);\n\n\tif (ee_block != map->m_lblk || ee_len > map->m_len) {\n\t\terr = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\tEXT4_GET_BLOCKS_CONVERT_UNWRITTEN);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tpath = ext4_find_extent(inode, map->m_lblk, ppath, 0);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = ext_depth(inode);\n\t\tex = path[depth].p_ext;\n\t\tif (!ex) {\n\t\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t\t (unsigned long) map->m_lblk);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\treturn err;\n\t/* first mark the extent as unwritten */\n\text4_ext_mark_unwritten(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\tif (err)\n\t\treturn err;\n\text4_ext_show_leaf(inode, path);\n\n\text4_update_inode_fsync_trans(handle, inode, 1);\n\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path, map->m_len);\n\tif (err)\n\t\treturn err;\n\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_len = allocated;\n\treturn allocated;\n}\n\nstatic int\next4_ext_handle_unwritten_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path **ppath, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tstruct ext4_ext_path *path = *ppath;\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\n\text_debug(\"ext4_ext_handle_unwritten_extents: inode %lu, logical \"\n\t\t  \"block %llu, max_blocks %u, flags %x, allocated %u\\n\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\t/*\n\t * When writing into unwritten space, we should not fail to\n\t * allocate metadata blocks for the new extent block if needed.\n\t */\n\tflags |= EXT4_GET_BLOCKS_METADATA_NOFAIL;\n\n\ttrace_ext4_ext_handle_unwritten_extents(inode, map, flags,\n\t\t\t\t\t\t    allocated, newblock);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif (flags & EXT4_GET_BLOCKS_PRE_IO) {\n\t\tret = ext4_split_convert_extents(handle, inode, map, ppath,\n\t\t\t\t\t flags | EXT4_GET_BLOCKS_CONVERT);\n\t\tif (ret <= 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif (flags & EXT4_GET_BLOCKS_CONVERT) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode, map,\n\t\t\t\t\t\t\t   ppath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tmap->m_flags |= EXT4_MAP_MAPPED;\n\t\tmap->m_pblk = newblock;\n\t\tif (allocated > map->m_len)\n\t\t\tallocated = map->m_len;\n\t\tmap->m_len = allocated;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNWRIT_EXT) {\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto map_out;\n\t}\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, ppath, flags);\n\tif (ret >= 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\tmap->m_len = allocated;\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\tmap->m_lblk, map->m_len);\n\t\tif (reserved_clusters)\n\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\t     reserved_clusters,\n\t\t\t\t\t\t     0);\n\t}\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0) {\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\treturn err ? err : allocated;\n}\n\n/*\n * get_implied_cluster_alloc - check to see if the requested\n * allocation (in the map structure) overlaps with a cluster already\n * allocated in an extent.\n *\t@sb\tThe filesystem superblock structure\n *\t@map\tThe requested lblk->pblk mapping\n *\t@ex\tThe extent structure which might contain an implied\n *\t\t\tcluster allocation\n *\n * This function is called by ext4_ext_map_blocks() after we failed to\n * find blocks that were already in the inode's extent tree.  Hence,\n * we know that the beginning of the requested region cannot overlap\n * the extent from the inode's extent tree.  There are three cases we\n * want to catch.  The first is this case:\n *\n *\t\t |--- cluster # N--|\n *    |--- extent ---|\t|---- requested region ---|\n *\t\t\t|==========|\n *\n * The second case that we need to test for is this one:\n *\n *   |--------- cluster # N ----------------|\n *\t   |--- requested region --|   |------- extent ----|\n *\t   |=======================|\n *\n * The third case is when the requested region lies between two extents\n * within the same cluster:\n *          |------------- cluster # N-------------|\n * |----- ex -----|                  |---- ex_right ----|\n *                  |------ requested region ------|\n *                  |================|\n *\n * In each of the above cases, we need to set the map->m_pblk and\n * map->m_len so it corresponds to the return the extent labelled as\n * \"|====|\" from cluster #N, since it is already in use for data in\n * cluster EXT4_B2C(sbi, map->m_lblk).\tWe will then return 1 to\n * signal to ext4_ext_map_blocks() that map->m_pblk should be treated\n * as a new \"allocated\" block region.  Otherwise, we will return 0 and\n * ext4_ext_map_blocks() will then allocate one or more new clusters\n * by calling ext4_mb_new_blocks().\n */\nstatic int get_implied_cluster_alloc(struct super_block *sb,\n\t\t\t\t     struct ext4_map_blocks *map,\n\t\t\t\t     struct ext4_extent *ex,\n\t\t\t\t     struct ext4_ext_path *path)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_lblk_t c_offset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\text4_lblk_t ex_cluster_start, ex_cluster_end;\n\text4_lblk_t rr_cluster_start;\n\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\tunsigned short ee_len = ext4_ext_get_actual_len(ex);\n\n\t/* The extent passed in that we are trying to match */\n\tex_cluster_start = EXT4_B2C(sbi, ee_block);\n\tex_cluster_end = EXT4_B2C(sbi, ee_block + ee_len - 1);\n\n\t/* The requested region passed into ext4_map_blocks() */\n\trr_cluster_start = EXT4_B2C(sbi, map->m_lblk);\n\n\tif ((rr_cluster_start == ex_cluster_end) ||\n\t    (rr_cluster_start == ex_cluster_start)) {\n\t\tif (rr_cluster_start == ex_cluster_end)\n\t\t\tee_start += ee_len - 1;\n\t\tmap->m_pblk = EXT4_PBLK_CMASK(sbi, ee_start) + c_offset;\n\t\tmap->m_len = min(map->m_len,\n\t\t\t\t (unsigned) sbi->s_cluster_ratio - c_offset);\n\t\t/*\n\t\t * Check for and handle this case:\n\t\t *\n\t\t *   |--------- cluster # N-------------|\n\t\t *\t\t       |------- extent ----|\n\t\t *\t   |--- requested region ---|\n\t\t *\t   |===========|\n\t\t */\n\n\t\tif (map->m_lblk < ee_block)\n\t\t\tmap->m_len = min(map->m_len, ee_block - map->m_lblk);\n\n\t\t/*\n\t\t * Check for the case where there is already another allocated\n\t\t * block to the right of 'ex' but before the end of the cluster.\n\t\t *\n\t\t *          |------------- cluster # N-------------|\n\t\t * |----- ex -----|                  |---- ex_right ----|\n\t\t *                  |------ requested region ------|\n\t\t *                  |================|\n\t\t */\n\t\tif (map->m_lblk > ee_block) {\n\t\t\text4_lblk_t next = ext4_ext_next_allocated_block(path);\n\t\t\tmap->m_len = min(map->m_len, next - map->m_lblk);\n\t\t}\n\n\t\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 1);\n\t\treturn 1;\n\t}\n\n\ttrace_ext4_get_implied_cluster_alloc_exit(sb, map, 0);\n\treturn 0;\n}\n\n\n/*\n * Block allocation/map/preallocation routine for extents based files\n *\n *\n * Need to be called with\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system block\n * (ie, create is zero). Otherwise down_write(&EXT4_I(inode)->i_data_sem)\n *\n * return > 0, number of of blocks already mapped/allocated\n *          if create == 0 and these are pre-allocated blocks\n *          \tbuffer head is unmapped\n *          otherwise blocks are mapped\n *\n * return = 0, if plain look up failed (blocks have not been allocated)\n *          buffer head is unmapped\n *\n * return < 0, error case.\n */\nint ext4_ext_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map, int flags)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent newex, *ex, *ex2;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\text4_fsblk_t newblock = 0;\n\tint free_on_err = 0, err = 0, depth, ret;\n\tunsigned int allocated = 0, offset = 0;\n\tunsigned int allocated_clusters = 0;\n\tstruct ext4_allocation_request ar;\n\text4_io_end_t *io = ext4_inode_aio(inode);\n\text4_lblk_t cluster_offset;\n\tint set_unwritten = 0;\n\tbool map_from_cluster = false;\n\n\text_debug(\"blocks %u/%u requested for inode %lu\\n\",\n\t\t  map->m_lblk, map->m_len, inode->i_ino);\n\ttrace_ext4_ext_map_blocks_enter(inode, map->m_lblk, map->m_len, flags);\n\n\t/* find extent for this block */\n\tpath = ext4_find_extent(inode, map->m_lblk, NULL, 0);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out2;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * consistent leaf must not be empty;\n\t * this situation is possible, though, _during_ tree modification;\n\t * this is why assert can't be put in ext4_find_extent()\n\t */\n\tif (unlikely(path[depth].p_ext == NULL && depth != 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extent address \"\n\t\t\t\t \"lblock: %lu, depth: %d pblock %lld\",\n\t\t\t\t (unsigned long) map->m_lblk, depth,\n\t\t\t\t path[depth].p_block);\n\t\terr = -EIO;\n\t\tgoto out2;\n\t}\n\n\tex = path[depth].p_ext;\n\tif (ex) {\n\t\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\t\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\t\tunsigned short ee_len;\n\n\n\t\t/*\n\t\t * unwritten extents are treated as holes, except that\n\t\t * we split out initialized portions during a write.\n\t\t */\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\n\t\ttrace_ext4_ext_show_extent(inode, ee_block, ee_start, ee_len);\n\n\t\t/* if found extent covers block, simply return it */\n\t\tif (in_range(map->m_lblk, ee_block, ee_len)) {\n\t\t\tnewblock = map->m_lblk - ee_block + ee_start;\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t\t\text_debug(\"%u fit into %u:%d -> %llu\\n\", map->m_lblk,\n\t\t\t\t  ee_block, ee_len, newblock);\n\n\t\t\t/*\n\t\t\t * If the extent is initialized check whether the\n\t\t\t * caller wants to convert it to unwritten.\n\t\t\t */\n\t\t\tif ((!ext4_ext_is_unwritten(ex)) &&\n\t\t\t    (flags & EXT4_GET_BLOCKS_CONVERT_UNWRITTEN)) {\n\t\t\t\tallocated = convert_initialized_extent(\n\t\t\t\t\t\thandle, inode, map, &path,\n\t\t\t\t\t\tflags, allocated, newblock);\n\t\t\t\tgoto out2;\n\t\t\t} else if (!ext4_ext_is_unwritten(ex))\n\t\t\t\tgoto out;\n\n\t\t\tret = ext4_ext_handle_unwritten_extents(\n\t\t\t\thandle, inode, map, &path, flags,\n\t\t\t\tallocated, newblock);\n\t\t\tif (ret < 0)\n\t\t\t\terr = ret;\n\t\t\telse\n\t\t\t\tallocated = ret;\n\t\t\tgoto out2;\n\t\t}\n\t}\n\n\t/*\n\t * requested block isn't allocated yet;\n\t * we couldn't try to create block if create flag is zero\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * put just found gap into cache to speed up\n\t\t * subsequent requests\n\t\t */\n\t\text4_ext_put_gap_in_cache(inode, path, map->m_lblk);\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * Okay, we need to do block allocation.\n\t */\n\tnewex.ee_block = cpu_to_le32(map->m_lblk);\n\tcluster_offset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\n\t/*\n\t * If we are doing bigalloc, check to see if the extent returned\n\t * by ext4_find_extent() implies a cluster we can use.\n\t */\n\tif (cluster_offset && ex &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap_from_cluster = true;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/* find neighbour allocated blocks */\n\tar.lleft = map->m_lblk;\n\terr = ext4_ext_search_left(inode, path, &ar.lleft, &ar.pleft);\n\tif (err)\n\t\tgoto out2;\n\tar.lright = map->m_lblk;\n\tex2 = NULL;\n\terr = ext4_ext_search_right(inode, path, &ar.lright, &ar.pright, &ex2);\n\tif (err)\n\t\tgoto out2;\n\n\t/* Check if the extent after searching to the right implies a\n\t * cluster we can use. */\n\tif ((sbi->s_cluster_ratio > 1) && ex2 &&\n\t    get_implied_cluster_alloc(inode->i_sb, map, ex2, path)) {\n\t\tar.len = allocated = map->m_len;\n\t\tnewblock = map->m_pblk;\n\t\tmap_from_cluster = true;\n\t\tgoto got_allocated_blocks;\n\t}\n\n\t/*\n\t * See if request is beyond maximum number of blocks we can have in\n\t * a single extent. For an initialized extent this limit is\n\t * EXT_INIT_MAX_LEN and for an unwritten extent this limit is\n\t * EXT_UNWRITTEN_MAX_LEN.\n\t */\n\tif (map->m_len > EXT_INIT_MAX_LEN &&\n\t    !(flags & EXT4_GET_BLOCKS_UNWRIT_EXT))\n\t\tmap->m_len = EXT_INIT_MAX_LEN;\n\telse if (map->m_len > EXT_UNWRITTEN_MAX_LEN &&\n\t\t (flags & EXT4_GET_BLOCKS_UNWRIT_EXT))\n\t\tmap->m_len = EXT_UNWRITTEN_MAX_LEN;\n\n\t/* Check if we can really insert (m_lblk)::(m_lblk + m_len) extent */\n\tnewex.ee_len = cpu_to_le16(map->m_len);\n\terr = ext4_ext_check_overlap(sbi, inode, &newex, path);\n\tif (err)\n\t\tallocated = ext4_ext_get_actual_len(&newex);\n\telse\n\t\tallocated = map->m_len;\n\n\t/* allocate new block */\n\tar.inode = inode;\n\tar.goal = ext4_ext_find_goal(inode, path, map->m_lblk);\n\tar.logical = map->m_lblk;\n\t/*\n\t * We calculate the offset from the beginning of the cluster\n\t * for the logical block number, since when we allocate a\n\t * physical cluster, the physical block should start at the\n\t * same offset from the beginning of the cluster.  This is\n\t * needed so that future calls to get_implied_cluster_alloc()\n\t * work correctly.\n\t */\n\toffset = EXT4_LBLK_COFF(sbi, map->m_lblk);\n\tar.len = EXT4_NUM_B2C(sbi, offset+allocated);\n\tar.goal -= offset;\n\tar.logical -= offset;\n\tif (S_ISREG(inode->i_mode))\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\telse\n\t\t/* disable in-core preallocation for non-regular files */\n\t\tar.flags = 0;\n\tif (flags & EXT4_GET_BLOCKS_NO_NORMALIZE)\n\t\tar.flags |= EXT4_MB_HINT_NOPREALLOC;\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\tar.flags |= EXT4_MB_DELALLOC_RESERVED;\n\tnewblock = ext4_mb_new_blocks(handle, &ar, &err);\n\tif (!newblock)\n\t\tgoto out2;\n\text_debug(\"allocate new block: goal %llu, found %llu/%u\\n\",\n\t\t  ar.goal, newblock, allocated);\n\tfree_on_err = 1;\n\tallocated_clusters = ar.len;\n\tar.len = EXT4_C2B(sbi, ar.len) - offset;\n\tif (ar.len > allocated)\n\t\tar.len = allocated;\n\ngot_allocated_blocks:\n\t/* try to insert new extent into found leaf and return */\n\text4_ext_store_pblock(&newex, newblock + offset);\n\tnewex.ee_len = cpu_to_le16(ar.len);\n\t/* Mark unwritten */\n\tif (flags & EXT4_GET_BLOCKS_UNWRIT_EXT){\n\t\text4_ext_mark_unwritten(&newex);\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\t/*\n\t\t * io_end structure was created for every IO write to an\n\t\t * unwritten extent. To avoid unnecessary conversion,\n\t\t * here we flag the IO that really needs the conversion.\n\t\t * For non asycn direct IO case, flag the inode state\n\t\t * that we need to perform conversion when IO is done.\n\t\t */\n\t\tif (flags & EXT4_GET_BLOCKS_PRE_IO)\n\t\t\tset_unwritten = 1;\n\t}\n\n\terr = 0;\n\tif ((flags & EXT4_GET_BLOCKS_KEEP_SIZE) == 0)\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t path, ar.len);\n\tif (!err)\n\t\terr = ext4_ext_insert_extent(handle, inode, &path,\n\t\t\t\t\t     &newex, flags);\n\n\tif (!err && set_unwritten) {\n\t\tif (io)\n\t\t\text4_set_io_unwritten_flag(inode, io);\n\t\telse\n\t\t\text4_set_inode_state(inode,\n\t\t\t\t\t     EXT4_STATE_DIO_UNWRITTEN);\n\t}\n\n\tif (err && free_on_err) {\n\t\tint fb_flags = flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE ?\n\t\t\tEXT4_FREE_BLOCKS_NO_QUOT_UPDATE : 0;\n\t\t/* free data blocks we just allocated */\n\t\t/* not a good idea to call discard here directly,\n\t\t * but otherwise we'd need to call it every free() */\n\t\text4_discard_preallocations(inode);\n\t\text4_free_blocks(handle, inode, NULL, newblock,\n\t\t\t\t EXT4_C2B(sbi, allocated_clusters), fb_flags);\n\t\tgoto out2;\n\t}\n\n\t/* previous routine could use block we allocated */\n\tnewblock = ext4_ext_pblock(&newex);\n\tallocated = ext4_ext_get_actual_len(&newex);\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\n\t/*\n\t * Update reserved blocks/metadata blocks after successful\n\t * block allocation which had been deferred till now.\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE) {\n\t\tunsigned int reserved_clusters;\n\t\t/*\n\t\t * Check how many clusters we had reserved this allocated range\n\t\t */\n\t\treserved_clusters = get_reserved_cluster_alloc(inode,\n\t\t\t\t\t\tmap->m_lblk, allocated);\n\t\tif (map_from_cluster) {\n\t\t\tif (reserved_clusters) {\n\t\t\t\t/*\n\t\t\t\t * We have clusters reserved for this range.\n\t\t\t\t * But since we are not doing actual allocation\n\t\t\t\t * and are simply using blocks from previously\n\t\t\t\t * allocated cluster, we should release the\n\t\t\t\t * reservation and not claim quota.\n\t\t\t\t */\n\t\t\t\text4_da_update_reserve_space(inode,\n\t\t\t\t\t\treserved_clusters, 0);\n\t\t\t}\n\t\t} else {\n\t\t\tBUG_ON(allocated_clusters < reserved_clusters);\n\t\t\tif (reserved_clusters < allocated_clusters) {\n\t\t\t\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\t\t\t\tint reservation = allocated_clusters -\n\t\t\t\t\t\t  reserved_clusters;\n\t\t\t\t/*\n\t\t\t\t * It seems we claimed few clusters outside of\n\t\t\t\t * the range of this allocation. We should give\n\t\t\t\t * it back to the reservation pool. This can\n\t\t\t\t * happen in the following case:\n\t\t\t\t *\n\t\t\t\t * * Suppose s_cluster_ratio is 4 (i.e., each\n\t\t\t\t *   cluster has 4 blocks. Thus, the clusters\n\t\t\t\t *   are [0-3],[4-7],[8-11]...\n\t\t\t\t * * First comes delayed allocation write for\n\t\t\t\t *   logical blocks 10 & 11. Since there were no\n\t\t\t\t *   previous delayed allocated blocks in the\n\t\t\t\t *   range [8-11], we would reserve 1 cluster\n\t\t\t\t *   for this write.\n\t\t\t\t * * Next comes write for logical blocks 3 to 8.\n\t\t\t\t *   In this case, we will reserve 2 clusters\n\t\t\t\t *   (for [0-3] and [4-7]; and not for [8-11] as\n\t\t\t\t *   that range has a delayed allocated blocks.\n\t\t\t\t *   Thus total reserved clusters now becomes 3.\n\t\t\t\t * * Now, during the delayed allocation writeout\n\t\t\t\t *   time, we will first write blocks [3-8] and\n\t\t\t\t *   allocate 3 clusters for writing these\n\t\t\t\t *   blocks. Also, we would claim all these\n\t\t\t\t *   three clusters above.\n\t\t\t\t * * Now when we come here to writeout the\n\t\t\t\t *   blocks [10-11], we would expect to claim\n\t\t\t\t *   the reservation of 1 cluster we had made\n\t\t\t\t *   (and we would claim it since there are no\n\t\t\t\t *   more delayed allocated blocks in the range\n\t\t\t\t *   [8-11]. But our reserved cluster count had\n\t\t\t\t *   already gone to 0.\n\t\t\t\t *\n\t\t\t\t *   Thus, at the step 4 above when we determine\n\t\t\t\t *   that there are still some unwritten delayed\n\t\t\t\t *   allocated blocks outside of our current\n\t\t\t\t *   block range, we should increment the\n\t\t\t\t *   reserved clusters count so that when the\n\t\t\t\t *   remaining blocks finally gets written, we\n\t\t\t\t *   could claim them.\n\t\t\t\t */\n\t\t\t\tdquot_reserve_block(inode,\n\t\t\t\t\t\tEXT4_C2B(sbi, reservation));\n\t\t\t\tspin_lock(&ei->i_block_reservation_lock);\n\t\t\t\tei->i_reserved_data_blocks += reservation;\n\t\t\t\tspin_unlock(&ei->i_block_reservation_lock);\n\t\t\t}\n\t\t\t/*\n\t\t\t * We will claim quota for all newly allocated blocks.\n\t\t\t * We're updating the reserved space *after* the\n\t\t\t * correction above so we do not accidentally free\n\t\t\t * all the metadata reservation because we might\n\t\t\t * actually need it later on.\n\t\t\t */\n\t\t\text4_da_update_reserve_space(inode, allocated_clusters,\n\t\t\t\t\t\t\t1);\n\t\t}\n\t}\n\n\t/*\n\t * Cache the extent and update transaction to commit on fdatasync only\n\t * when it is _not_ an unwritten extent.\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_UNWRIT_EXT) == 0)\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\telse\n\t\text4_update_inode_fsync_trans(handle, inode, 0);\nout:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\n\ttrace_ext4_ext_map_blocks_exit(inode, flags, map,\n\t\t\t\t       err ? err : allocated);\n\treturn err ? err : allocated;\n}\n\nvoid ext4_ext_truncate(handle_t *handle, struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t last_block;\n\tint err = 0;\n\n\t/*\n\t * TODO: optimization is possible here.\n\t * Probably we need not scan at all,\n\t * because page truncation is enough.\n\t */\n\n\t/* we have to know where to truncate from in crash case */\n\tEXT4_I(inode)->i_disksize = inode->i_size;\n\text4_mark_inode_dirty(handle, inode);\n\n\tlast_block = (inode->i_size + sb->s_blocksize - 1)\n\t\t\t>> EXT4_BLOCK_SIZE_BITS(sb);\nretry:\n\terr = ext4_es_remove_extent(inode, last_block,\n\t\t\t\t    EXT_MAX_BLOCKS - last_block);\n\tif (err == -ENOMEM) {\n\t\tcond_resched();\n\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\tgoto retry;\n\t}\n\tif (err) {\n\t\text4_std_error(inode->i_sb, err);\n\t\treturn;\n\t}\n\terr = ext4_ext_remove_space(inode, last_block, EXT_MAX_BLOCKS - 1);\n\text4_std_error(inode->i_sb, err);\n}\n\nstatic int ext4_alloc_file_blocks(struct file *file, ext4_lblk_t offset,\n\t\t\t\t  ext4_lblk_t len, loff_t new_size,\n\t\t\t\t  int flags, int mode)\n{\n\tstruct inode *inode = file_inode(file);\n\thandle_t *handle;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint retries = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits;\n\tloff_t epos;\n\n\tmap.m_lblk = offset;\n\tmap.m_len = len;\n\t/*\n\t * Don't normalize the request if it can fit in one extent so\n\t * that it doesn't get unnecessarily split into multiple\n\t * extents.\n\t */\n\tif (len <= EXT_UNWRITTEN_MAX_LEN)\n\t\tflags |= EXT4_GET_BLOCKS_NO_NORMALIZE;\n\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, len);\n\nretry:\n\twhile (ret >= 0 && len) {\n\t\thandle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,\n\t\t\t\t\t    credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map, flags);\n\t\tif (ret <= 0) {\n\t\t\text4_debug(\"inode #%lu: block %u: len %u: \"\n\t\t\t\t   \"ext4_ext_map_blocks returned %d\",\n\t\t\t\t   inode->i_ino, map.m_lblk,\n\t\t\t\t   map.m_len, ret);\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\t\tbreak;\n\t\t}\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = len = len - ret;\n\t\tepos = (loff_t)map.m_lblk << inode->i_blkbits;\n\t\tinode->i_ctime = ext4_current_time(inode);\n\t\tif (new_size) {\n\t\t\tif (epos > new_size)\n\t\t\t\tepos = new_size;\n\t\t\tif (ext4_update_inode_size(inode, epos) & 0x1)\n\t\t\t\tinode->i_mtime = inode->i_ctime;\n\t\t} else {\n\t\t\tif (epos > inode->i_size)\n\t\t\t\text4_set_inode_flag(inode,\n\t\t\t\t\t\t    EXT4_INODE_EOFBLOCKS);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret2)\n\t\t\tbreak;\n\t}\n\tif (ret == -ENOSPC &&\n\t\t\text4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\n\treturn ret > 0 ? ret2 : ret;\n}\n\nstatic long ext4_zero_range(struct file *file, loff_t offset,\n\t\t\t    loff_t len, int mode)\n{\n\tstruct inode *inode = file_inode(file);\n\thandle_t *handle = NULL;\n\tunsigned int max_blocks;\n\tloff_t new_size = 0;\n\tint ret = 0;\n\tint flags;\n\tint credits;\n\tint partial_begin, partial_end;\n\tloff_t start, end;\n\text4_lblk_t lblk;\n\tstruct address_space *mapping = inode->i_mapping;\n\tunsigned int blkbits = inode->i_blkbits;\n\n\ttrace_ext4_zero_range(inode, offset, len, mode);\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Write out all dirty pages to avoid race conditions\n\t * Then release them.\n\t */\n\tif (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {\n\t\tret = filemap_write_and_wait_range(mapping, offset,\n\t\t\t\t\t\t   offset + len - 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Round up offset. This is not fallocate, we neet to zero out\n\t * blocks, so convert interior block aligned part of the range to\n\t * unwritten and possibly manually zero out unaligned parts of the\n\t * range.\n\t */\n\tstart = round_up(offset, 1 << blkbits);\n\tend = round_down((offset + len), 1 << blkbits);\n\n\tif (start < offset || end > offset + len)\n\t\treturn -EINVAL;\n\tpartial_begin = offset & ((1 << blkbits) - 1);\n\tpartial_end = (offset + len) & ((1 << blkbits) - 1);\n\n\tlblk = start >> blkbits;\n\tmax_blocks = (end >> blkbits);\n\tif (max_blocks < lblk)\n\t\tmax_blocks = 0;\n\telse\n\t\tmax_blocks -= lblk;\n\n\tmutex_lock(&inode->i_mutex);\n\n\t/*\n\t * Indirect files do not support unwritten extnets\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t     offset + len > i_size_read(inode)) {\n\t\tnew_size = offset + len;\n\t\tret = inode_newsize_ok(inode, new_size);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\t}\n\n\tflags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\n\t/* Preallocate the range including the unaligned edges */\n\tif (partial_begin || partial_end) {\n\t\tret = ext4_alloc_file_blocks(file,\n\t\t\t\tround_down(offset, 1 << blkbits) >> blkbits,\n\t\t\t\t(round_up((offset + len), 1 << blkbits) -\n\t\t\t\t round_down(offset, 1 << blkbits)) >> blkbits,\n\t\t\t\tnew_size, flags, mode);\n\t\tif (ret)\n\t\t\tgoto out_mutex;\n\n\t}\n\n\t/* Zero range excluding the unaligned edges */\n\tif (max_blocks > 0) {\n\t\tflags |= (EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |\n\t\t\t  EXT4_EX_NOCACHE);\n\n\t\t/* Now release the pages and zero block aligned part of pages*/\n\t\ttruncate_pagecache_range(inode, start, end - 1);\n\t\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\n\t\t/* Wait all existing dio workers, newcomers will block on i_mutex */\n\t\text4_inode_block_unlocked_dio(inode);\n\t\tinode_dio_wait(inode);\n\n\t\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,\n\t\t\t\t\t     flags, mode);\n\t\tif (ret)\n\t\t\tgoto out_dio;\n\t\t/*\n\t\t * Remove entire range from the extent status tree.\n\t\t *\n\t\t * ext4_es_remove_extent(inode, lblk, max_blocks) is\n\t\t * NOT sufficient.  I'm not sure why this is the case,\n\t\t * but let's be conservative and remove the extent\n\t\t * status tree for the entire inode.  There should be\n\t\t * no outstanding delalloc extents thanks to the\n\t\t * filemap_write_and_wait_range() call above.\n\t\t */\n\t\tret = ext4_es_remove_extent(inode, 0, EXT_MAX_BLOCKS);\n\t\tif (ret)\n\t\t\tgoto out_dio;\n\t}\n\tif (!partial_begin && !partial_end)\n\t\tgoto out_dio;\n\n\t/*\n\t * In worst case we have to writeout two nonadjacent unwritten\n\t * blocks and update the inode\n\t */\n\tcredits = (2 * ext4_ext_index_trans_blocks(inode, 2)) + 1;\n\tif (ext4_should_journal_data(inode))\n\t\tcredits += 2;\n\thandle = ext4_journal_start(inode, EXT4_HT_MISC, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\text4_std_error(inode->i_sb, ret);\n\t\tgoto out_dio;\n\t}\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\tif (new_size) {\n\t\text4_update_inode_size(inode, new_size);\n\t} else {\n\t\t/*\n\t\t* Mark that we allocate beyond EOF so the subsequent truncate\n\t\t* can proceed even if the new size is the same as i_size.\n\t\t*/\n\t\tif ((offset + len) > i_size_read(inode))\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\t}\n\text4_mark_inode_dirty(handle, inode);\n\n\t/* Zero out partial block at the edges of the range */\n\tret = ext4_zero_partial_blocks(handle, inode, offset, len);\n\n\tif (file->f_flags & O_SYNC)\n\t\text4_handle_sync(handle);\n\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}\n\n/*\n * preallocate space for a file. This implements ext4's fallocate file\n * operation, which gets called from sys_fallocate system call.\n * For block-mapped files, posix_fallocate should fall back to the method\n * of writing zeroes to the required new blocks (the same behavior which is\n * expected for file systems which do not support fallocate() system call).\n */\nlong ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)\n{\n\tstruct inode *inode = file_inode(file);\n\tloff_t new_size = 0;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint flags;\n\text4_lblk_t lblk;\n\tunsigned int blkbits = inode->i_blkbits;\n\n\t/* Return error if mode is not supported */\n\tif (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |\n\t\t     FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE))\n\t\treturn -EOPNOTSUPP;\n\n\tif (mode & FALLOC_FL_PUNCH_HOLE)\n\t\treturn ext4_punch_hole(inode, offset, len);\n\n\tret = ext4_convert_inline_data(inode);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * currently supporting (pre)allocate mode for extent-based\n\t * files _only_\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn -EOPNOTSUPP;\n\n\tif (mode & FALLOC_FL_COLLAPSE_RANGE)\n\t\treturn ext4_collapse_range(inode, offset, len);\n\n\tif (mode & FALLOC_FL_ZERO_RANGE)\n\t\treturn ext4_zero_range(file, offset, len, mode);\n\n\ttrace_ext4_fallocate_enter(inode, offset, len, mode);\n\tlblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)\n\t\t- lblk;\n\n\tflags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;\n\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\tflags |= EXT4_GET_BLOCKS_KEEP_SIZE;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) &&\n\t     offset + len > i_size_read(inode)) {\n\t\tnew_size = offset + len;\n\t\tret = inode_newsize_ok(inode, new_size);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,\n\t\t\t\t     flags, mode);\n\tif (ret)\n\t\tgoto out;\n\n\tif (file->f_flags & O_SYNC && EXT4_SB(inode->i_sb)->s_journal) {\n\t\tret = jbd2_complete_transaction(EXT4_SB(inode->i_sb)->s_journal,\n\t\t\t\t\t\tEXT4_I(inode)->i_sync_tid);\n\t}\nout:\n\tmutex_unlock(&inode->i_mutex);\n\ttrace_ext4_fallocate_exit(inode, offset, max_blocks, ret);\n\treturn ret;\n}\n\n/*\n * This function convert a range of blocks to written extents\n * The caller of this function will pass the start offset and the size.\n * all unwritten extents within this range will be converted to\n * written extents.\n *\n * This function is called from the direct IO end io call back\n * function, to convert the fallocated extents after IO is completed.\n * Returns 0 on success.\n */\nint ext4_convert_unwritten_extents(handle_t *handle, struct inode *inode,\n\t\t\t\t   loff_t offset, ssize_t len)\n{\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\tmap.m_lblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = ((EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits) -\n\t\t      map.m_lblk);\n\t/*\n\t * This is somewhat ugly but the idea is clear: When transaction is\n\t * reserved, everything goes into it. Otherwise we rather start several\n\t * smaller transactions for conversion of each extent separately.\n\t */\n\tif (handle) {\n\t\thandle = ext4_journal_start_reserved(handle,\n\t\t\t\t\t\t     EXT4_HT_EXT_CONVERT);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tcredits = 0;\n\t} else {\n\t\t/*\n\t\t * credits to insert 1 extent into extent tree\n\t\t */\n\t\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\t}\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = (max_blocks -= ret);\n\t\tif (credits) {\n\t\t\thandle = ext4_journal_start(inode, EXT4_HT_MAP_BLOCKS,\n\t\t\t\t\t\t    credits);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\tret = PTR_ERR(handle);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map,\n\t\t\t\t      EXT4_GET_BLOCKS_IO_CONVERT_EXT);\n\t\tif (ret <= 0)\n\t\t\text4_warning(inode->i_sb,\n\t\t\t\t     \"inode #%lu: block %u: len %u: \"\n\t\t\t\t     \"ext4_ext_map_blocks returned %d\",\n\t\t\t\t     inode->i_ino, map.m_lblk,\n\t\t\t\t     map.m_len, ret);\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tif (credits)\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret <= 0 || ret2)\n\t\t\tbreak;\n\t}\n\tif (!credits)\n\t\tret2 = ext4_journal_stop(handle);\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * If newes is not existing extent (newes->ec_pblk equals zero) find\n * delayed extent at start of newes and update newes accordingly and\n * return start of the next delayed extent.\n *\n * If newes is existing extent (newes->ec_pblk is not equal zero)\n * return start of next delayed extent or EXT_MAX_BLOCKS if no delayed\n * extent found. Leave newes unmodified.\n */\nstatic int ext4_find_delayed_extent(struct inode *inode,\n\t\t\t\t    struct extent_status *newes)\n{\n\tstruct extent_status es;\n\text4_lblk_t block, next_del;\n\n\tif (newes->es_pblk == 0) {\n\t\text4_es_find_delayed_extent_range(inode, newes->es_lblk,\n\t\t\t\tnewes->es_lblk + newes->es_len - 1, &es);\n\n\t\t/*\n\t\t * No extent in extent-tree contains block @newes->es_pblk,\n\t\t * then the block may stay in 1)a hole or 2)delayed-extent.\n\t\t */\n\t\tif (es.es_len == 0)\n\t\t\t/* A hole found. */\n\t\t\treturn 0;\n\n\t\tif (es.es_lblk > newes->es_lblk) {\n\t\t\t/* A hole found. */\n\t\t\tnewes->es_len = min(es.es_lblk - newes->es_lblk,\n\t\t\t\t\t    newes->es_len);\n\t\t\treturn 0;\n\t\t}\n\n\t\tnewes->es_len = es.es_lblk + es.es_len - newes->es_lblk;\n\t}\n\n\tblock = newes->es_lblk + newes->es_len;\n\text4_es_find_delayed_extent_range(inode, block, EXT_MAX_BLOCKS, &es);\n\tif (es.es_len == 0)\n\t\tnext_del = EXT_MAX_BLOCKS;\n\telse\n\t\tnext_del = es.es_lblk;\n\n\treturn next_del;\n}\n/* fiemap flags we can handle specified here */\n#define EXT4_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)\n\nstatic int ext4_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\t__u64 physical = 0;\n\t__u64 length;\n\t__u32 flags = FIEMAP_EXTENT_LAST;\n\tint blockbits = inode->i_sb->s_blocksize_bits;\n\tint error = 0;\n\n\t/* in-inode? */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tstruct ext4_iloc iloc;\n\t\tint offset;\t/* offset of xattr in inode */\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error)\n\t\t\treturn error;\n\t\tphysical = (__u64)iloc.bh->b_blocknr << blockbits;\n\t\toffset = EXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\tEXT4_I(inode)->i_extra_isize;\n\t\tphysical += offset;\n\t\tlength = EXT4_SB(inode->i_sb)->s_inode_size - offset;\n\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t\tbrelse(iloc.bh);\n\t} else { /* external block */\n\t\tphysical = (__u64)EXT4_I(inode)->i_file_acl << blockbits;\n\t\tlength = inode->i_sb->s_blocksize;\n\t}\n\n\tif (physical)\n\t\terror = fiemap_fill_next_extent(fieinfo, 0, physical,\n\t\t\t\t\t\tlength, flags);\n\treturn (error < 0 ? error : 0);\n}\n\nint ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\text4_lblk_t start_blk;\n\tint error = 0;\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tint has_inline = 1;\n\n\t\terror = ext4_inline_data_fiemap(inode, fieinfo, &has_inline,\n\t\t\t\t\t\tstart, len);\n\n\t\tif (has_inline)\n\t\t\treturn error;\n\t}\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) {\n\t\terror = ext4_ext_precache(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/* fallback to generic here if not in extents fmt */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn generic_block_fiemap(inode, fieinfo, start, len,\n\t\t\text4_get_block);\n\n\tif (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS))\n\t\treturn -EBADR;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\terror = ext4_xattr_fiemap(inode, fieinfo);\n\t} else {\n\t\text4_lblk_t len_blks;\n\t\t__u64 last_blk;\n\n\t\tstart_blk = start >> inode->i_sb->s_blocksize_bits;\n\t\tlast_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;\n\t\tif (last_blk >= EXT_MAX_BLOCKS)\n\t\t\tlast_blk = EXT_MAX_BLOCKS-1;\n\t\tlen_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;\n\n\t\t/*\n\t\t * Walk the extent tree gathering extent information\n\t\t * and pushing extents back to the user.\n\t\t */\n\t\terror = ext4_fill_fiemap_extents(inode, start_blk,\n\t\t\t\t\t\t len_blks, fieinfo);\n\t}\n\treturn error;\n}\n\n/*\n * ext4_access_path:\n * Function to access the path buffer for marking it dirty.\n * It also checks if there are sufficient credits left in the journal handle\n * to update path.\n */\nstatic int\next4_access_path(handle_t *handle, struct inode *inode,\n\t\tstruct ext4_ext_path *path)\n{\n\tint credits, err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\n\t/*\n\t * Check if need to extend journal credits\n\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t * descriptor) for each block group; assume two block\n\t * groups\n\t */\n\tif (handle->h_buffer_credits < 7) {\n\t\tcredits = ext4_writepage_trans_blocks(inode);\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\t/* EAGAIN is success */\n\t\tif (err && err != -EAGAIN)\n\t\t\treturn err;\n\t}\n\n\terr = ext4_ext_get_access(handle, inode, path);\n\treturn err;\n}\n\n/*\n * ext4_ext_shift_path_extents:\n * Shift the extents of a path structure lying between path[depth].p_ext\n * and EXT_LAST_EXTENT(path[depth].p_hdr) downwards, by subtracting shift\n * from starting block for each extent.\n */\nstatic int\next4_ext_shift_path_extents(struct ext4_ext_path *path, ext4_lblk_t shift,\n\t\t\t    struct inode *inode, handle_t *handle,\n\t\t\t    ext4_lblk_t *start)\n{\n\tint depth, err = 0;\n\tstruct ext4_extent *ex_start, *ex_last;\n\tbool update = 0;\n\tdepth = path->p_depth;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\tex_start = path[depth].p_ext;\n\t\t\tif (!ex_start)\n\t\t\t\treturn -EIO;\n\n\t\t\tex_last = EXT_LAST_EXTENT(path[depth].p_hdr);\n\n\t\t\terr = ext4_access_path(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tif (ex_start == EXT_FIRST_EXTENT(path[depth].p_hdr))\n\t\t\t\tupdate = 1;\n\n\t\t\t*start = le32_to_cpu(ex_last->ee_block) +\n\t\t\t\text4_ext_get_actual_len(ex_last);\n\n\t\t\twhile (ex_start <= ex_last) {\n\t\t\t\tle32_add_cpu(&ex_start->ee_block, -shift);\n\t\t\t\t/* Try to merge to the left. */\n\t\t\t\tif ((ex_start >\n\t\t\t\t     EXT_FIRST_EXTENT(path[depth].p_hdr)) &&\n\t\t\t\t    ext4_ext_try_to_merge_right(inode,\n\t\t\t\t\t\t\tpath, ex_start - 1))\n\t\t\t\t\tex_last--;\n\t\t\t\telse\n\t\t\t\t\tex_start++;\n\t\t\t}\n\t\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tif (--depth < 0 || !update)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/* Update index too */\n\t\terr = ext4_access_path(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tle32_add_cpu(&path[depth].p_idx->ei_block, -shift);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* we are done if current index is not a starting index */\n\t\tif (path[depth].p_idx != EXT_FIRST_INDEX(path[depth].p_hdr))\n\t\t\tbreak;\n\n\t\tdepth--;\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_shift_extents:\n * All the extents which lies in the range from start to the last allocated\n * block for the file are shifted downwards by shift blocks.\n * On success, 0 is returned, error otherwise.\n */\nstatic int\next4_ext_shift_extents(struct inode *inode, handle_t *handle,\n\t\t       ext4_lblk_t start, ext4_lblk_t shift)\n{\n\tstruct ext4_ext_path *path;\n\tint ret = 0, depth;\n\tstruct ext4_extent *extent;\n\text4_lblk_t stop_block;\n\text4_lblk_t ex_start, ex_end;\n\n\t/* Let path point to the last extent */\n\tpath = ext4_find_extent(inode, EXT_MAX_BLOCKS - 1, NULL, 0);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tdepth = path->p_depth;\n\textent = path[depth].p_ext;\n\tif (!extent)\n\t\tgoto out;\n\n\tstop_block = le32_to_cpu(extent->ee_block) +\n\t\t\text4_ext_get_actual_len(extent);\n\n\t/* Nothing to shift, if hole is at the end of file */\n\tif (start >= stop_block)\n\t\tgoto out;\n\n\t/*\n\t * Don't start shifting extents until we make sure the hole is big\n\t * enough to accomodate the shift.\n\t */\n\tpath = ext4_find_extent(inode, start - 1, &path, 0);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\tdepth = path->p_depth;\n\textent =  path[depth].p_ext;\n\tif (extent) {\n\t\tex_start = le32_to_cpu(extent->ee_block);\n\t\tex_end = le32_to_cpu(extent->ee_block) +\n\t\t\text4_ext_get_actual_len(extent);\n\t} else {\n\t\tex_start = 0;\n\t\tex_end = 0;\n\t}\n\n\tif ((start == ex_start && shift > ex_start) ||\n\t    (shift > start - ex_end))\n\t\treturn -EINVAL;\n\n\t/* Its safe to start updating extents */\n\twhile (start < stop_block) {\n\t\tpath = ext4_find_extent(inode, start, &path, 0);\n\t\tif (IS_ERR(path))\n\t\t\treturn PTR_ERR(path);\n\t\tdepth = path->p_depth;\n\t\textent = path[depth].p_ext;\n\t\tif (!extent) {\n\t\t\tEXT4_ERROR_INODE(inode, \"unexpected hole at %lu\",\n\t\t\t\t\t (unsigned long) start);\n\t\t\treturn -EIO;\n\t\t}\n\t\tif (start > le32_to_cpu(extent->ee_block)) {\n\t\t\t/* Hole, move to the next extent */\n\t\t\tif (extent < EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t\t\tpath[depth].p_ext++;\n\t\t\t} else {\n\t\t\t\tstart = ext4_ext_next_allocated_block(path);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tret = ext4_ext_shift_path_extents(path, shift, inode,\n\t\t\t\thandle, &start);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\treturn ret;\n}\n\n/*\n * ext4_collapse_range:\n * This implements the fallocate's collapse range functionality for ext4\n * Returns: 0 and non-zero on error.\n */\nint ext4_collapse_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t punch_start, punch_stop;\n\thandle_t *handle;\n\tunsigned int credits;\n\tloff_t new_size, ioffset;\n\tint ret;\n\n\t/* Collapse range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t    len & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\ttrace_ext4_collapse_range(inode, offset, len);\n\n\tpunch_start = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tpunch_stop = (offset + len) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Need to round down offset to be aligned with page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\t\t\t   LLONG_MAX);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Take mutex lock */\n\tmutex_lock(&inode->i_mutex);\n\n\t/*\n\t * There is no need to overlap collapse range with EOF, in which case\n\t * it is effectively a truncate operation\n\t */\n\tif (offset + len >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\ttruncate_pagecache(inode, ioffset);\n\n\t/* Wait for existing dio to complete */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_dio;\n\t}\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tret = ext4_es_remove_extent(inode, punch_start,\n\t\t\t\t    EXT_MAX_BLOCKS - punch_start);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tret = ext4_ext_remove_space(inode, punch_start, punch_stop - 1);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\text4_discard_preallocations(inode);\n\n\tret = ext4_ext_shift_extents(inode, handle, punch_stop,\n\t\t\t\t     punch_stop - punch_start);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tnew_size = i_size_read(inode) - len;\n\ti_size_write(inode, new_size);\n\tEXT4_I(inode)->i_disksize = new_size;\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\nout_stop:\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}\n\n/**\n * ext4_swap_extents - Swap extents between two inodes\n *\n * @inode1:\tFirst inode\n * @inode2:\tSecond inode\n * @lblk1:\tStart block for first inode\n * @lblk2:\tStart block for second inode\n * @count:\tNumber of blocks to swap\n * @mark_unwritten: Mark second inode's extents as unwritten after swap\n * @erp:\tPointer to save error value\n *\n * This helper routine does exactly what is promise \"swap extents\". All other\n * stuff such as page-cache locking consistency, bh mapping consistency or\n * extent's data copying must be performed by caller.\n * Locking:\n * \t\ti_mutex is held for both inodes\n * \t\ti_data_sem is locked for write for both inodes\n * Assumptions:\n *\t\tAll pages from requested range are locked for both inodes\n */\nint\next4_swap_extents(handle_t *handle, struct inode *inode1,\n\t\t     struct inode *inode2, ext4_lblk_t lblk1, ext4_lblk_t lblk2,\n\t\t  ext4_lblk_t count, int unwritten, int *erp)\n{\n\tstruct ext4_ext_path *path1 = NULL;\n\tstruct ext4_ext_path *path2 = NULL;\n\tint replaced_count = 0;\n\n\tBUG_ON(!rwsem_is_locked(&EXT4_I(inode1)->i_data_sem));\n\tBUG_ON(!rwsem_is_locked(&EXT4_I(inode2)->i_data_sem));\n\tBUG_ON(!mutex_is_locked(&inode1->i_mutex));\n\tBUG_ON(!mutex_is_locked(&inode1->i_mutex));\n\n\t*erp = ext4_es_remove_extent(inode1, lblk1, count);\n\tif (unlikely(*erp))\n\t\treturn 0;\n\t*erp = ext4_es_remove_extent(inode2, lblk2, count);\n\tif (unlikely(*erp))\n\t\treturn 0;\n\n\twhile (count) {\n\t\tstruct ext4_extent *ex1, *ex2, tmp_ex;\n\t\text4_lblk_t e1_blk, e2_blk;\n\t\tint e1_len, e2_len, len;\n\t\tint split = 0;\n\n\t\tpath1 = ext4_find_extent(inode1, lblk1, NULL, EXT4_EX_NOCACHE);\n\t\tif (unlikely(IS_ERR(path1))) {\n\t\t\t*erp = PTR_ERR(path1);\n\t\t\tpath1 = NULL;\n\t\tfinish:\n\t\t\tcount = 0;\n\t\t\tgoto repeat;\n\t\t}\n\t\tpath2 = ext4_find_extent(inode2, lblk2, NULL, EXT4_EX_NOCACHE);\n\t\tif (unlikely(IS_ERR(path2))) {\n\t\t\t*erp = PTR_ERR(path2);\n\t\t\tpath2 = NULL;\n\t\t\tgoto finish;\n\t\t}\n\t\tex1 = path1[path1->p_depth].p_ext;\n\t\tex2 = path2[path2->p_depth].p_ext;\n\t\t/* Do we have somthing to swap ? */\n\t\tif (unlikely(!ex2 || !ex1))\n\t\t\tgoto finish;\n\n\t\te1_blk = le32_to_cpu(ex1->ee_block);\n\t\te2_blk = le32_to_cpu(ex2->ee_block);\n\t\te1_len = ext4_ext_get_actual_len(ex1);\n\t\te2_len = ext4_ext_get_actual_len(ex2);\n\n\t\t/* Hole handling */\n\t\tif (!in_range(lblk1, e1_blk, e1_len) ||\n\t\t    !in_range(lblk2, e2_blk, e2_len)) {\n\t\t\text4_lblk_t next1, next2;\n\n\t\t\t/* if hole after extent, then go to next extent */\n\t\t\tnext1 = ext4_ext_next_allocated_block(path1);\n\t\t\tnext2 = ext4_ext_next_allocated_block(path2);\n\t\t\t/* If hole before extent, then shift to that extent */\n\t\t\tif (e1_blk > lblk1)\n\t\t\t\tnext1 = e1_blk;\n\t\t\tif (e2_blk > lblk2)\n\t\t\t\tnext2 = e1_blk;\n\t\t\t/* Do we have something to swap */\n\t\t\tif (next1 == EXT_MAX_BLOCKS || next2 == EXT_MAX_BLOCKS)\n\t\t\t\tgoto finish;\n\t\t\t/* Move to the rightest boundary */\n\t\t\tlen = next1 - lblk1;\n\t\t\tif (len < next2 - lblk2)\n\t\t\t\tlen = next2 - lblk2;\n\t\t\tif (len > count)\n\t\t\t\tlen = count;\n\t\t\tlblk1 += len;\n\t\t\tlblk2 += len;\n\t\t\tcount -= len;\n\t\t\tgoto repeat;\n\t\t}\n\n\t\t/* Prepare left boundary */\n\t\tif (e1_blk < lblk1) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode1,\n\t\t\t\t\t\t&path1, lblk1, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\tif (e2_blk < lblk2) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode2,\n\t\t\t\t\t\t&path2,  lblk2, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\t/* ext4_split_extent_at() may result in leaf extent split,\n\t\t * path must to be revalidated. */\n\t\tif (split)\n\t\t\tgoto repeat;\n\n\t\t/* Prepare right boundary */\n\t\tlen = count;\n\t\tif (len > e1_blk + e1_len - lblk1)\n\t\t\tlen = e1_blk + e1_len - lblk1;\n\t\tif (len > e2_blk + e2_len - lblk2)\n\t\t\tlen = e2_blk + e2_len - lblk2;\n\n\t\tif (len != e1_len) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode1,\n\t\t\t\t\t\t&path1, lblk1 + len, 0);\n\t\t\tif (unlikely(*erp))\n\t\t\t\tgoto finish;\n\t\t}\n\t\tif (len != e2_len) {\n\t\t\tsplit = 1;\n\t\t\t*erp = ext4_force_split_extent_at(handle, inode2,\n\t\t\t\t\t\t&path2, lblk2 + len, 0);\n\t\t\tif (*erp)\n\t\t\t\tgoto finish;\n\t\t}\n\t\t/* ext4_split_extent_at() may result in leaf extent split,\n\t\t * path must to be revalidated. */\n\t\tif (split)\n\t\t\tgoto repeat;\n\n\t\tBUG_ON(e2_len != e1_len);\n\t\t*erp = ext4_ext_get_access(handle, inode1, path1 + path1->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\t*erp = ext4_ext_get_access(handle, inode2, path2 + path2->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\n\t\t/* Both extents are fully inside boundaries. Swap it now */\n\t\ttmp_ex = *ex1;\n\t\text4_ext_store_pblock(ex1, ext4_ext_pblock(ex2));\n\t\text4_ext_store_pblock(ex2, ext4_ext_pblock(&tmp_ex));\n\t\tex1->ee_len = cpu_to_le16(e2_len);\n\t\tex2->ee_len = cpu_to_le16(e1_len);\n\t\tif (unwritten)\n\t\t\text4_ext_mark_unwritten(ex2);\n\t\tif (ext4_ext_is_unwritten(&tmp_ex))\n\t\t\text4_ext_mark_unwritten(ex1);\n\n\t\text4_ext_try_to_merge(handle, inode2, path2, ex2);\n\t\text4_ext_try_to_merge(handle, inode1, path1, ex1);\n\t\t*erp = ext4_ext_dirty(handle, inode2, path2 +\n\t\t\t\t      path2->p_depth);\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\t*erp = ext4_ext_dirty(handle, inode1, path1 +\n\t\t\t\t      path1->p_depth);\n\t\t/*\n\t\t * Looks scarry ah..? second inode already points to new blocks,\n\t\t * and it was successfully dirtied. But luckily error may happen\n\t\t * only due to journal error, so full transaction will be\n\t\t * aborted anyway.\n\t\t */\n\t\tif (unlikely(*erp))\n\t\t\tgoto finish;\n\t\tlblk1 += len;\n\t\tlblk2 += len;\n\t\treplaced_count += len;\n\t\tcount -= len;\n\n\trepeat:\n\t\text4_ext_drop_refs(path1);\n\t\tkfree(path1);\n\t\text4_ext_drop_refs(path2);\n\t\tkfree(path2);\n\t\tpath1 = path2 = NULL;\n\t}\n\treturn replaced_count;\n}\n"], "filenames": ["fs/ext4/extents.c"], "buggy_code_start_loc": [4800], "buggy_code_end_loc": [4830], "fixing_code_start_loc": [4799], "fixing_code_end_loc": [4838], "type": "CWE-17", "message": "The ext4_zero_range function in fs/ext4/extents.c in the Linux kernel before 4.1 allows local users to cause a denial of service (BUG) via a crafted fallocate zero-range request.", "other": {"cve": {"id": "CVE-2015-0275", "sourceIdentifier": "secalert@redhat.com", "published": "2015-10-19T10:59:00.113", "lastModified": "2019-12-27T16:08:55.810", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The ext4_zero_range function in fs/ext4/extents.c in the Linux kernel before 4.1 allows local users to cause a denial of service (BUG) via a crafted fallocate zero-range request."}, {"lang": "es", "value": "La funci\u00f3n ext4_zero_range en fs/ext4/extents.c en el kernel de Linux en versiones anteriores a 4.1 permite a usuarios locales provocar una denegaci\u00f3n de servicio (BUG) a trav\u00e9s de una petici\u00f3n de rango cero a fallocate manipulada."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-17"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.0.5", "matchCriteriaId": "18F25E63-5459-406C-B2B9-8359A3315ADD"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:oracle:linux:7:*:*:*:*:*:*:*", "matchCriteriaId": "104DA87B-DEE4-4262-AE50-8E6BC43B228B"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=0f2af21aae11972fa924374ddcf52e88347cf5a8", "source": "secalert@redhat.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2015-1778.html", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2015-1787.html", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2015/02/23/14", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.oracle.com/technetwork/topics/security/linuxbulletinoct2015-2719645.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/75139", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1034454", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.spinics.net/lists/linux-ext4/msg47193.html", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1193907", "source": "secalert@redhat.com", "tags": ["Issue Tracking"]}, {"url": "https://github.com/torvalds/linux/commit/0f2af21aae11972fa924374ddcf52e88347cf5a8", "source": "secalert@redhat.com", "tags": ["Patch"]}, {"url": "https://support.f5.com/csp/article/K05211147", "source": "secalert@redhat.com"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0f2af21aae11972fa924374ddcf52e88347cf5a8"}}